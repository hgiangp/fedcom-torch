LinearRegression(
  (linear): Linear(in_features=2, out_features=3, bias=True)
)
{'linear.weight': tensor([[-1.2280,  0.2068],
        [ 1.2158,  1.6265],
        [ 0.0764,  0.5310]]), 'linear.bias': tensor([-0.0121,  1.3694,  0.2501])}
Epoch 0: loss 1.5970773696899414
Epoch 1: loss 1.571554183959961
Epoch 2: loss 1.5468096733093262
Epoch 3: loss 1.5228195190429688
Epoch 4: loss 1.499560832977295
Epoch 5: loss 1.4770095348358154
Epoch 6: loss 1.4551446437835693
Epoch 7: loss 1.433943748474121
Epoch 8: loss 1.413386583328247
Epoch 9: loss 1.3934522867202759
Epoch 10: loss 1.3741216659545898
Epoch 11: loss 1.3553755283355713
Epoch 12: loss 1.337195634841919
Epoch 13: loss 1.3195632696151733
Epoch 14: loss 1.302462100982666
Epoch 15: loss 1.285874366760254
Epoch 16: loss 1.2697839736938477
Epoch 17: loss 1.2541749477386475
Epoch 18: loss 1.23903226852417
Epoch 19: loss 1.2243406772613525
Epoch 20: loss 1.2100856304168701
Epoch 21: loss 1.1962532997131348
Epoch 22: loss 1.1828300952911377
Epoch 23: loss 1.1698024272918701
Epoch 24: loss 1.1571576595306396
Epoch 25: loss 1.144883394241333
Epoch 26: loss 1.132967472076416
Epoch 27: loss 1.1213984489440918
Epoch 28: loss 1.1101648807525635
Epoch 29: loss 1.0992555618286133
Epoch 30: loss 1.0886602401733398
Epoch 31: loss 1.0783679485321045
Epoch 32: loss 1.0683693885803223
Epoch 33: loss 1.058654546737671
Epoch 34: loss 1.0492140054702759
Epoch 35: loss 1.0400387048721313
Epoch 36: loss 1.0311195850372314
Epoch 37: loss 1.0224485397338867
Epoch 38: loss 1.0140166282653809
Epoch 39: loss 1.0058164596557617
Epoch 40: loss 0.9978398084640503
Epoch 41: loss 0.9900789856910706
Epoch 42: loss 0.9825274348258972
Epoch 43: loss 0.9751771092414856
Epoch 44: loss 0.9680216312408447
Epoch 45: loss 0.9610544443130493
Epoch 46: loss 0.9542688727378845
Epoch 47: loss 0.9476587772369385
Epoch 48: loss 0.9412180185317993
Epoch 49: loss 0.9349406957626343
Epoch 50: loss 0.9288210868835449
Epoch 51: loss 0.922853946685791
Epoch 52: loss 0.9170335531234741
Epoch 53: loss 0.9113554358482361
Epoch 54: loss 0.9058138132095337
Epoch 55: loss 0.9004042148590088
Epoch 56: loss 0.8951219916343689
Epoch 57: loss 0.8899623155593872
Epoch 58: loss 0.8849211931228638
Epoch 59: loss 0.8799941539764404
Epoch 60: loss 0.8751771450042725
Epoch 61: loss 0.8704659938812256
Epoch 62: loss 0.8658571243286133
Epoch 63: loss 0.8613465428352356
Epoch 64: loss 0.8569309115409851
Epoch 65: loss 0.8526065349578857
Epoch 66: loss 0.8483701944351196
Epoch 67: loss 0.8442184329032898
Epoch 68: loss 0.8401480317115784
Epoch 69: loss 0.8361562490463257
Epoch 70: loss 0.8322399258613586
Epoch 71: loss 0.828396201133728
Epoch 72: loss 0.8246221542358398
Epoch 73: loss 0.8209155797958374
Epoch 74: loss 0.8172733187675476
Epoch 75: loss 0.8136932253837585
Epoch 76: loss 0.8101728558540344
Epoch 77: loss 0.8067097067832947
Epoch 78: loss 0.8033016920089722
Epoch 79: loss 0.7999465465545654
Epoch 80: loss 0.7966420650482178
Epoch 81: loss 0.7933862805366516
Epoch 82: loss 0.7901773452758789
Epoch 83: loss 0.7870131731033325
Epoch 84: loss 0.7838921546936035
Epoch 85: loss 0.7808124423027039
Epoch 86: loss 0.777772068977356
Epoch 87: loss 0.7747697830200195
Epoch 88: loss 0.7718033790588379
Epoch 89: loss 0.7688719034194946
Epoch 90: loss 0.7659737467765808
Epoch 91: loss 0.7631072998046875
Epoch 92: loss 0.760271430015564
Epoch 93: loss 0.7574644684791565
Epoch 94: loss 0.7546853423118591
Epoch 95: loss 0.7519328594207764
Epoch 96: loss 0.749205470085144
Epoch 97: loss 0.7465026378631592
Epoch 98: loss 0.7438226938247681
Epoch 99: loss 0.7411648035049438
{'linear.weight': tensor([[ 0.2818, -0.2851],
        [-0.2332,  0.2157],
        [ 0.2163,  0.6203]]), 'linear.bias': tensor([ 0.6174, -0.1287,  0.2949])}
