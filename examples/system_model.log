v = 22.697160301531774
a = 91.65029380654791
['f_00000', 'f_00001', 'f_00002', 'f_00003', 'f_00004', 'f_00005', 'f_00006', 'f_00007', 'f_00008', 'f_00009']
10
dict_keys(['x', 'y'])
id = f_00000, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 83
id = f_00001, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 129
id = f_00002, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 85
id = f_00003, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 80
id = f_00004, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 235
id = f_00005, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 100
id = f_00006, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 146
id = f_00007, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 93
id = f_00008, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 89
id = f_00009, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 86
BaseFederated generated!
num_samples = [ 83 129  85  80 235 100 146  93  89  86]
msize = 502400
xs = [  6.0943416  -20.79968212  15.00902392  18.81129433 -39.02070377
 -26.04359014   2.55680806  -6.32485185  -0.33602315 -17.06087855]
ys = [ 17.5879595   15.55583871   1.32061395  22.54482414   9.35018685
 -17.18584926   7.37501568 -19.17765202  17.56900603  -0.99851822]
dists_uav = [101.71763524 103.31800857 101.12870423 104.22156154 107.7499017
 104.75505717 100.304178   102.01855757 101.5321766  101.44984286]
dists_bs = [239.94522511 221.81113273 257.42563646 246.58750669 214.31339109
 243.15306433 244.18139867 257.20860434 235.14255819 236.47461703]
uav_gains = [9.58312886e-11 9.21631668e-11 9.72326274e-11 9.01785356e-11
 8.29761259e-11 8.90347214e-11 9.92432040e-11 9.51261523e-11
 9.62695134e-11 9.64649612e-11]
bs_gains = [2.39320649e-11 2.98222725e-11 1.96548195e-11 2.21704798e-11
 3.28364367e-11 2.30584863e-11 2.27876148e-11 1.97012919e-11
 2.53260008e-11 2.49285719e-11]
SystemModel __init__!
t_co_uav = [0.06351163 0.06396501 0.06334461 0.06422068 0.06521719 0.06437154
 0.0631106  0.06359693 0.06345904 0.0634357 ]
t_co_bs = [0.08476871 0.08051968 0.08895491 0.08634835 0.0787878  0.08552995
 0.08577462 0.08890235 0.0836346  0.08394849]
difference = [-0.02125708 -0.01655467 -0.0256103  -0.02212767 -0.01357061 -0.0211584
 -0.02266402 -0.02530541 -0.02017555 -0.0205128 ]
decs_opt = [1 0 1 1 0 0 1 1 0 0]
af = 16.896171935691484	bf = 66.9234999628387	zeta = 33.79234387138297	eta = 0.5
af = 16.896171935691484	bf = 66.9234999628387	zeta = 157.27006780463867	eta = 0.1074341238072076
af = 16.896171935691484	bf = 66.9234999628387	zeta = 117.20903882473088	eta = 0.1441541719402482
af = 16.896171935691484	bf = 66.9234999628387	zeta = 116.43361171638361	eta = 0.14511421304054584
af = 16.896171935691484	bf = 66.9234999628387	zeta = 116.4331772754501	eta = 0.14511475449750555
af = 16.896171935691484	bf = 66.9234999628387	zeta = 116.43317727531253	eta = 0.14511475449767702
eta_opt = 0.14511475449767702
solve_bound_eta tau = 65.0
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [5.82085955 7.37965242 5.80555201 5.88584406 7.22092485 7.83884461
 5.78410521 5.82867765 7.66513541 7.69390417]
solve_bound_eta tau = 37.5
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [5.82085955 7.37965242 5.80555201 5.88584406 7.22092485 7.83884461
 5.78410521 5.82867765 7.66513541 7.69390417]
solve_bound_eta tau = 23.75
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [5.82085955 7.37965242 5.80555201 5.88584406 7.22092485 7.83884461
 5.78410521 5.82867765 7.66513541 7.69390417]
solve_bound_eta tau = 16.875
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [5.82085955 7.37965242 5.80555201 5.88584406 7.22092485 7.83884461
 5.78410521 5.82867765 7.66513541 7.69390417]
solve_bound_eta tau = 13.4375
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [5.82085955 7.37965242 5.80555201 5.88584406 7.22092485 7.83884461
 5.78410521 5.82867765 7.66513541 7.69390417]
solve_bound_eta tau = 15.15625
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [5.82085955 7.37965242 5.80555201 5.88584406 7.22092485 7.83884461
 5.78410521 5.82867765 7.66513541 7.69390417]
solve_bound_eta tau = 16.015625
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [5.82085955 7.37965242 5.80555201 5.88584406 7.22092485 7.83884461
 5.78410521 5.82867765 7.66513541 7.69390417]
solve_bound_eta tau = 16.4453125
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [5.82085955 7.37965242 5.80555201 5.88584406 7.22092485 7.83884461
 5.78410521 5.82867765 7.66513541 7.69390417]
solve_bound_eta tau = 16.23046875
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [5.82085955 7.37965242 5.80555201 5.88584406 7.22092485 7.83884461
 5.78410521 5.82867765 7.66513541 7.69390417]
solve_bound_eta tau = 16.337890625
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [5.82085955 7.37965242 5.80555201 5.88584406 7.22092485 7.83884461
 5.78410521 5.82867765 7.66513541 7.69390417]
solve_bound_eta tau = 16.3916015625
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [5.82085955 7.37965242 5.80555201 5.88584406 7.22092485 7.83884461
 5.78410521 5.82867765 7.66513541 7.69390417]
solve_bound_eta tau = 16.41845703125
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [5.82085955 7.37965242 5.80555201 5.88584406 7.22092485 7.83884461
 5.78410521 5.82867765 7.66513541 7.69390417]
solve_bound_eta tau = 16.405029296875
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [5.82085955 7.37965242 5.80555201 5.88584406 7.22092485 7.83884461
 5.78410521 5.82867765 7.66513541 7.69390417]
solve_bound_eta tau = 16.4117431640625
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [5.82085955 7.37965242 5.80555201 5.88584406 7.22092485 7.83884461
 5.78410521 5.82867765 7.66513541 7.69390417]
solve_bound_eta tau = 16.40838623046875
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [5.82085955 7.37965242 5.80555201 5.88584406 7.22092485 7.83884461
 5.78410521 5.82867765 7.66513541 7.69390417]
solve_bound_eta tau = 16.410064697265625
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [5.82085955 7.37965242 5.80555201 5.88584406 7.22092485 7.83884461
 5.78410521 5.82867765 7.66513541 7.69390417]
solve_bound_eta tau = 16.409225463867188
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [5.82085955 7.37965242 5.80555201 5.88584406 7.22092485 7.83884461
 5.78410521 5.82867765 7.66513541 7.69390417]
initialize_feasible_solution eta = 0.14511475449767702, tau = 16.409225463867188
ti_comp = [0.0262303  0.04076758 0.02686236 0.02528222 0.07426652 0.03160278
 0.04614005 0.02939058 0.02812647 0.02717839]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [ 9.62102767 13.00293018  9.67088294  9.5954013  16.40860404 12.55753137
 11.71251303  9.96897901 11.98164868 11.91365904]
optimize_network_fake tau = 21	t_min = 16.409225463867188
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.25032247 0.38905541 0.25635434 0.24127467 0.70874434 0.30159334
 0.44032627 0.2804818  0.26841807 0.25937027]
ene_total = [23.76183224 36.76263384 24.31869245 22.9307867  66.34214127 28.71212373
 41.34792207 26.55465407 25.62334201 24.78863831]
obj_prev = 321.1427666854388
solve_bound_eta tau = 21
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [5.82085955 7.37965242 5.80555201 5.88584406 7.22092485 7.83884461
 5.78410521 5.82867765 7.66513541 7.69390417]
eta_min = 0.02304542908779013	eta_max = 0.5582623390540764
af = 67.58468774276595	bf = 6.6923499962838715	zeta = 135.1693754855319	eta = 0.5
af = 67.58468774276595	bf = 6.6923499962838715	zeta = 107.07697150841277	eta = 0.6311785511925502
af = 67.58468774276595	bf = 6.6923499962838715	zeta = 102.4684402827174	eta = 0.6595658873726896
af = 67.58468774276595	bf = 6.6923499962838715	zeta = 102.27911495535675	eta = 0.6607867869434109
af = 67.58468774276595	bf = 6.6923499962838715	zeta = 102.27877445458762	eta = 0.6607889867977833
af = 67.58468774276595	bf = 6.6923499962838715	zeta = 102.27877445348349	eta = 0.6607889868049167
eta = 0.6607889868049167
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.02252116 0.03500277 0.02306384 0.02170714 0.06376474 0.02713393
 0.03961554 0.02523456 0.0241492  0.02333518]
ene_total = [ 7.80091752 11.63281654  7.94302935  7.60013827 19.35712339  9.64213153
 12.40875492  8.53634495  8.78448685  8.57303114]
ti_comp = [0.03770453 0.02069648 0.03787155 0.03699548 0.02242836 0.01568622
 0.03810556 0.03761923 0.01758156 0.01726767]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [21. 21. 21. 21. 21. 21. 21. 21. 21. 21.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.00139861 0.01742711 0.00148895 0.00130083 0.08971345 0.0141323
 0.007453   0.00197642 0.00793057 0.00741783]
ene_total = [ 1.60789749  5.28631548  1.62317555  1.60232245 20.24811869  4.70667053
  2.85572206  1.72954873  3.38063131  3.28076434]
optimize_network_iter = 0 obj = 46.321166643300465
eta = 0.5582623390540764
freqs = [4.20188589e+08 1.18974237e+09 4.28415846e+08 4.12763225e+08
 2.00000000e+09 1.21686254e+09 7.31348244e+08 4.71881315e+08
 9.66256019e+08 9.50658428e+08]
solve_bound_eta tau = 21
af = [5.928076   3.2539937  5.95433583 5.81659621 3.5262881  2.4662574
 5.99112745 5.9146642  2.76425256 2.71490023]
bf = [5.82085955 7.37965242 5.80555201 5.88584406 7.22092485 7.83884461
 5.78410521 5.82867765 7.66513541 7.69390417]
eta_min = 0.12023805694888506	eta_max = 0.5582623390540764
af = 23.621260790044964	bf = 6.6923499962838715	zeta = 47.24252158008993	eta = 0.5
af = 23.621260790044964	bf = 6.6923499962838715	zeta = 46.13072062834945	eta = 0.5120505482745182
af = 23.621260790044964	bf = 6.6923499962838715	zeta = 46.11710092286252	eta = 0.5122017715197431
af = 23.621260790044964	bf = 6.6923499962838715	zeta = 46.11709881193507	eta = 0.5122017949648602
eta = 0.5122017949648602
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.00160521 0.02000145 0.0017089  0.00149299 0.10296599 0.01621993
 0.00855396 0.00226837 0.00910207 0.0085136 ]
ene_total = [ 1.49488897  5.2708356   1.51123173  1.48712663 20.8261441   4.6544777
  2.79292444  1.62109004  3.28152361  3.17685599]
ti_comp = [0.04825847 0.03125042 0.04842549 0.04754942 0.0329823  0.02624015
 0.0486595  0.04817316 0.0281355  0.0278216 ]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [21. 21. 21. 21. 21. 21. 21. 21. 21. 21.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.00129076 0.01155621 0.00137679 0.00119052 0.06271906 0.00763528
 0.00691005 0.0018222  0.00468186 0.00432005]
ene_total = [ 1.43580759  3.68409525  1.44883289  1.43029653 13.26432284  3.04154534
  2.48405717  1.53726076  2.45102893  2.38894761]
optimize_network_iter = 1 obj = 33.1661949001213
eta = 0.5122017949648602
freqs = [3.76790986e+08 9.04336036e+08 3.84539407e+08 3.68587592e+08
 1.56092800e+09 8.34890627e+08 6.57326577e+08 4.22935087e+08
 6.92996914e+08 6.77192651e+08]
solve_bound_eta tau = 21
af = [6.6108532  4.28094648 6.63373317 6.51372173 4.51819444 3.5945981
 6.66578942 6.59916759 3.85423891 3.81123862]
bf = [5.82085955 7.37965242 5.80555201 5.88584406 7.22092485 7.83884461
 5.78410521 5.82867765 7.66513541 7.69390417]
eta_min = 0.17966330823878449	eta_max = 0.5122017949648601
af = 14.178686685427108	bf = 6.6923499962838715	zeta = 28.357373370854216	eta = 0.5
af = 14.178686685427108	bf = 6.6923499962838715	zeta = 33.04053339266102	eta = 0.42913007840776823
af = 14.178686685427108	bf = 6.6923499962838715	zeta = 32.73504185028158	eta = 0.4331348269012567
af = 14.178686685427108	bf = 6.6923499962838715	zeta = 32.73396608414873	eta = 0.43314906140545767
af = 14.178686685427108	bf = 6.6923499962838715	zeta = 32.733966070641806	eta = 0.43314906158418676
eta = 0.43314906158418676
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.00161418 0.01445178 0.00172176 0.00148882 0.07843423 0.00954841
 0.00864146 0.00227878 0.00585497 0.0054025 ]
ene_total = [ 1.28786192  3.63847908  1.30255587  1.27905862 13.95536594  2.92669408
  2.41757224  1.39669631  2.29888159  2.23080041]
ti_comp = [0.06637197 0.04936391 0.06653899 0.06566292 0.0510958  0.04435365
 0.06677299 0.06628666 0.046249   0.0459351 ]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [21. 21. 21. 21. 21. 21. 21. 21. 21. 21.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.00133457 0.00905787 0.0014262  0.00122097 0.0511104  0.00522658
 0.00717683 0.00188223 0.00338876 0.00309943]
ene_total = [1.24265397 2.76637357 1.25476876 1.23575138 9.53756066 2.22792556
 2.18076633 1.33258074 1.90013761 1.8584325 ]
optimize_network_iter = 2 obj = 25.536951077960452
eta = 0.43314906158418676
freqs = [3.42606356e+08 7.15949056e+08 3.49981224e+08 3.33788844e+08
 1.26004102e+09 6.17692935e+08 5.99037501e+08 3.84378247e+08
 5.27217330e+08 5.12927255e+08]
solve_bound_eta tau = 21
af = [7.27047192 5.40738778 7.28876764 7.19280182 5.59710048 4.85855688
 7.31440106 7.26112766 5.06617578 5.03179108]
bf = [5.82085955 7.37965242 5.80555201 5.88584406 7.22092485 7.83884461
 5.78410521 5.82867765 7.66513541 7.69390417]
eta_min = 0.2753370547044945	eta_max = 0.4331490615841859
af = 9.302668263685922	bf = 6.6923499962838715	zeta = 18.605336527371843	eta = 0.5
af = 9.302668263685922	bf = 6.6923499962838715	zeta = 26.2809365498845	eta = 0.35397019607837393
af = 9.302668263685922	bf = 6.6923499962838715	zeta = 25.31395118638227	eta = 0.36749175169028236
af = 9.302668263685922	bf = 6.6923499962838715	zeta = 25.303744389596346	eta = 0.36763998720721824
af = 9.302668263685922	bf = 6.6923499962838715	zeta = 25.303743193437224	eta = 0.3676400045863041
eta = 0.3676400045863041
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.00159613 0.0108331  0.00170572 0.00146027 0.06112741 0.00625092
 0.00858341 0.00225112 0.00405292 0.00370688]
ene_total = [ 1.15183051  2.73708314  1.16529308  1.14241624 10.00132391  2.1455867
  2.15871043  1.24799779  1.79955233  1.75394906]
ti_comp = [0.08138218 0.06437413 0.0815492  0.08067313 0.06610601 0.05936386
 0.0817832  0.08129687 0.06125921 0.06094531]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [21. 21. 21. 21. 21. 21. 21. 21. 21. 21.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.00151856 0.00911177 0.00162433 0.00138378 0.05223696 0.00499128
 0.00818439 0.00214071 0.00330433 0.00301211]
ene_total = [1.14058828 2.48760427 1.15349689 1.13133126 8.71279876 1.96302221
 2.10088022 1.23199483 1.69105633 1.6532533 ]
optimize_network_iter = 3 obj = 23.266026344810328
eta = 0.3676400045863041
freqs = [3.34177743e+08 6.56609403e+08 3.41529296e+08 3.24930007e+08
 1.16481152e+09 5.51958707e+08 5.84948270e+08 3.74833016e+08
 4.76044278e+08 4.62367048e+08]
solve_bound_eta tau = 21
af = [7.45384739 5.89606874 7.46914497 7.38890516 6.05469303 5.43717532
 7.49057782 7.44603438 5.6107715  5.58202145]
bf = [5.82085955 7.37965242 5.80555201 5.88584406 7.22092485 7.83884461
 5.78410521 5.82867765 7.66513541 7.69390417]
eta_min = 0.34596780694746343	eta_max = 0.367640004586301
af = 8.014936047894297	bf = 6.6923499962838715	zeta = 16.029872095788594	eta = 0.5
af = 8.014936047894297	bf = 6.6923499962838715	zeta = 24.495760640500148	eta = 0.3271968633887929
af = 8.014936047894297	bf = 6.6923499962838715	zeta = 23.255810921595604	eta = 0.3446422949909495
af = 8.014936047894297	bf = 6.6923499962838715	zeta = 23.239593104525053	eta = 0.34488280461045095
af = 8.014936047894297	bf = 6.6923499962838715	zeta = 23.239590128237733	eta = 0.34488284877948805
eta = 0.34488284877948805
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.00161553 0.00969363 0.00172805 0.00147215 0.0555727  0.00531001
 0.00870703 0.00227741 0.00351534 0.00320446]
ene_total = [1.11453334 2.48259241 1.12793843 1.10439385 8.87680456 1.93942202
 2.10101736 1.20832271 1.66183294 1.62273251]
ti_comp = [0.08634797 0.06933992 0.08651499 0.08563892 0.0710718  0.06432965
 0.08674899 0.08626266 0.066225   0.0659111 ]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [21. 21. 21. 21. 21. 21. 21. 21. 21. 21.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.00160986 0.00937259 0.00172239 0.0014655  0.05393444 0.00507265
 0.00868134 0.00226913 0.0033743  0.00307352]
ene_total = [1.11558697 2.44172384 1.12901577 1.10529359 8.66195812 1.90937726
 2.10090272 1.20916775 1.64482549 1.60707532]
optimize_network_iter = 4 obj = 22.924926840287487
eta = 0.34596780694746343
freqs = [3.34083505e+08 6.46599638e+08 3.41473205e+08 3.24674269e+08
 1.14921057e+09 5.40278715e+08 5.84948270e+08 3.74704702e+08
 4.67086268e+08 4.53491272e+08]
solve_bound_eta tau = 21
af = [7.45594995 5.98734355 7.47037185 7.39472523 6.13688768 5.55471866
 7.49057782 7.44858419 5.71837764 5.69127333]
bf = [5.82085955 7.37965242 5.80555201 5.88584406 7.22092485 7.83884461
 5.78410521 5.82867765 7.66513541 7.69390417]
eta_min = 0.3459678069474702	eta_max = 0.3459678069474627
af = 7.821005724785099	bf = 6.6923499962838715	zeta = 15.642011449570198	eta = 0.5
af = 7.821005724785099	bf = 6.6923499962838715	zeta = 24.226916127123708	eta = 0.32282299916946267
af = 7.821005724785099	bf = 6.6923499962838715	zeta = 22.941090251059364	eta = 0.34091691542096364
af = 7.821005724785099	bf = 6.6923499962838715	zeta = 22.92376106991469	eta = 0.3411746310272551
af = 7.821005724785099	bf = 6.6923499962838715	zeta = 22.923757681402567	eta = 0.3411746814585321
eta = 0.3411746814585321
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.00163102 0.00949578 0.00174503 0.00148476 0.05464335 0.00513932
 0.00879545 0.00229896 0.00341865 0.00311391]
ene_total = [1.11041438 2.44109724 1.12395124 1.09993192 8.69755803 1.90476126
 2.10149175 1.2045198  1.63902878 1.60100328]
ti_comp = [0.08634797 0.06933992 0.08651499 0.08563892 0.0710718  0.06432965
 0.08674899 0.08626266 0.066225   0.0659111 ]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [21. 21. 21. 21. 21. 21. 21. 21. 21. 21.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.00160986 0.00937259 0.00172239 0.0014655  0.05393444 0.00507265
 0.00868134 0.00226913 0.0033743  0.00307352]
ene_total = [1.11558697 2.44172384 1.12901577 1.10529359 8.66195812 1.90937726
 2.10090272 1.20916775 1.64482549 1.60707532]
optimize_network_iter = 5 obj = 22.924926840287576
eta = 0.3459678069474702
freqs = [3.34083505e+08 6.46599638e+08 3.41473205e+08 3.24674269e+08
 1.14921057e+09 5.40278715e+08 5.84948270e+08 3.74704702e+08
 4.67086268e+08 4.53491272e+08]
Done!
optimize_network_fake num_local_rounds = 34.75594133176583	num_global_rounds = 140.1311659886241
iter = 0	num_local_rounds = 34.75594133176583	num_global_rounds = 140.1311659886241
Round 1
-------------------------------
gradient difference: 1.5472776889801025
At round 0 accuracy: 0.554006968641115
At round 0 training accuracy: 0.5550621669626998
At round 0 training loss: 1.0090730456107109
Round 2
-------------------------------
gradient difference: 0.446103036403656
At round 1 accuracy: 0.5958188153310104
At round 1 training accuracy: 0.5630550621669627
At round 1 training loss: 0.9408757225278922
Round 3
-------------------------------
gradient difference: 0.2924008071422577
At round 2 accuracy: 0.6097560975609756
At round 2 training accuracy: 0.566607460035524
At round 2 training loss: 0.909364850872487
Round 4
-------------------------------
gradient difference: 0.5282202959060669
At round 3 accuracy: 0.6167247386759582
At round 3 training accuracy: 0.5799289520426287
At round 3 training loss: 0.8793468540403142
Round 5
-------------------------------
gradient difference: 0.3765283226966858
At round 4 accuracy: 0.6236933797909407
At round 4 training accuracy: 0.5870337477797514
At round 4 training loss: 0.8772562523521902
Round 6
-------------------------------
gradient difference: 0.303139865398407
At round 5 accuracy: 0.627177700348432
At round 5 training accuracy: 0.5950266429840142
At round 5 training loss: 0.8418905279676878
Round 7
-------------------------------
gradient difference: 0.3470182716846466
At round 6 accuracy: 0.6236933797909407
At round 6 training accuracy: 0.6039076376554174
At round 6 training loss: 0.8540643897152392
Round 8
-------------------------------
gradient difference: 0.3476237654685974
At round 7 accuracy: 0.6202090592334495
At round 7 training accuracy: 0.6127886323268206
At round 7 training loss: 0.8476133038055498
Round 9
-------------------------------
gradient difference: 0.6224265098571777
At round 8 accuracy: 0.6341463414634146
At round 8 training accuracy: 0.6127886323268206
At round 8 training loss: 0.8180373245266819
Round 10
-------------------------------
gradient difference: 0.297849178314209
At round 9 accuracy: 0.6376306620209059
At round 9 training accuracy: 0.6207815275310835
At round 9 training loss: 0.8356754009220959
Round 11
-------------------------------
gradient difference: 0.2987867593765259
At round 10 accuracy: 0.6411149825783972
At round 10 training accuracy: 0.6225577264653641
At round 10 training loss: 0.8232753061311725
Round 12
-------------------------------
gradient difference: 0.49854809045791626
At round 11 accuracy: 0.6515679442508711
At round 11 training accuracy: 0.627886323268206
At round 11 training loss: 0.8334263650461141
Round 13
-------------------------------
gradient difference: 0.3163661062717438
At round 12 accuracy: 0.6759581881533101
At round 12 training accuracy: 0.6341030195381883
At round 12 training loss: 0.8295050239959117
Round 14
-------------------------------
gradient difference: 0.3169119656085968
At round 13 accuracy: 0.6724738675958188
At round 13 training accuracy: 0.6429840142095915
At round 13 training loss: 0.8173314048618061
Round 15
-------------------------------
gradient difference: 0.24771468341350555
At round 14 accuracy: 0.6794425087108014
At round 14 training accuracy: 0.6474245115452931
At round 14 training loss: 0.812295702476961
Round 16
-------------------------------
gradient difference: 0.312309592962265
At round 15 accuracy: 0.6724738675958188
At round 15 training accuracy: 0.6483126110124334
At round 15 training loss: 0.8121137189742259
Round 17
-------------------------------
gradient difference: 0.3673330545425415
At round 16 accuracy: 0.6724738675958188
At round 16 training accuracy: 0.6456483126110124
At round 16 training loss: 0.8084981577293348
Round 18
-------------------------------
gradient difference: 0.5780783295631409
At round 17 accuracy: 0.6724738675958188
At round 17 training accuracy: 0.6492007104795737
At round 17 training loss: 0.8060397507346031
Round 19
-------------------------------
gradient difference: 0.3255350589752197
At round 18 accuracy: 0.6724738675958188
At round 18 training accuracy: 0.6527531083481349
At round 18 training loss: 0.8250703991399038
Round 20
-------------------------------
gradient difference: 0.43268242478370667
At round 19 accuracy: 0.6794425087108014
At round 19 training accuracy: 0.6563055062166963
At round 19 training loss: 0.8249350481502858
Round 21
-------------------------------
gradient difference: 0.304849773645401
At round 20 accuracy: 0.6759581881533101
At round 20 training accuracy: 0.6571936056838366
At round 20 training loss: 0.8154295808512688
Round 22
-------------------------------
gradient difference: 0.5281792879104614
At round 21 accuracy: 0.6794425087108014
At round 21 training accuracy: 0.6616341030195382
At round 21 training loss: 0.8260916964917969
Round 23
-------------------------------
gradient difference: 0.3718172013759613
At round 22 accuracy: 0.6794425087108014
At round 22 training accuracy: 0.6625222024866785
At round 22 training loss: 0.810703193777902
Round 24
-------------------------------
gradient difference: 0.38985663652420044
At round 23 accuracy: 0.6794425087108014
At round 23 training accuracy: 0.6642984014209592
At round 23 training loss: 0.797968394219293
Round 25
-------------------------------
gradient difference: 0.31703782081604004
At round 24 accuracy: 0.6829268292682927
At round 24 training accuracy: 0.6642984014209592
At round 24 training loss: 0.8201823305572415
Round 26
-------------------------------
gradient difference: 0.90643310546875
At round 25 accuracy: 0.6794425087108014
At round 25 training accuracy: 0.6642984014209592
At round 25 training loss: 0.8368906558204009
Round 27
-------------------------------
gradient difference: 0.3100040555000305
At round 26 accuracy: 0.6794425087108014
At round 26 training accuracy: 0.6660746003552398
At round 26 training loss: 0.816428031814723
Round 28
-------------------------------
gradient difference: 0.6908268928527832
At round 27 accuracy: 0.6794425087108014
At round 27 training accuracy: 0.6651865008880995
At round 27 training loss: 0.7881321389802669
Round 29
-------------------------------
gradient difference: 0.27630850672721863
At round 28 accuracy: 0.6829268292682927
At round 28 training accuracy: 0.6651865008880995
At round 28 training loss: 0.8019651868906095
Round 30
-------------------------------
gradient difference: 0.3842616677284241
At round 29 accuracy: 0.6829268292682927
At round 29 training accuracy: 0.6669626998223801
At round 29 training loss: 0.8024109473874348
Round 31
-------------------------------
gradient difference: 0.3746352195739746
At round 30 accuracy: 0.6829268292682927
At round 30 training accuracy: 0.6678507992895204
At round 30 training loss: 0.7888471881495628
Round 32
-------------------------------
gradient difference: 0.3097536563873291
At round 31 accuracy: 0.6829268292682927
At round 31 training accuracy: 0.6660746003552398
At round 31 training loss: 0.8160033872716079
Round 33
-------------------------------
gradient difference: 0.30708277225494385
At round 32 accuracy: 0.6829268292682927
At round 32 training accuracy: 0.6651865008880995
At round 32 training loss: 0.789011006390311
Round 34
-------------------------------
gradient difference: 0.36538857221603394
At round 33 accuracy: 0.6794425087108014
At round 33 training accuracy: 0.6669626998223801
At round 33 training loss: 0.8071506088038233
Done!
update_location
xs = [-493.9056584   479.20031788  515.00902392   18.81129433  460.97929623
  473.95640986    2.55680806   -6.32485185  499.66397685  -17.06087855]
ys = [  17.5879595    15.55583871    1.32061395 -477.45517586    9.35018685
  -17.18584926 -492.62498432  480.82234798   17.56900603  499.00148178]
dists_uav = [504.23420721 489.77028163 524.62942992 488.17753917 471.79374471
 484.69581375 502.67873681 491.15184421 509.87523938 509.2087513 ]
dists_bs = [355.6393812  673.35020344 711.53143446 680.63248109 657.19823937
 676.81594437 690.83249763 349.25436881 692.78856769 360.44656508]
uav_gains = [3.64260914e-13 3.94348189e-13 3.27481438e-13 3.97896753e-13
 4.37529010e-13 4.05830733e-13 3.67322769e-13 3.91310096e-13
 3.53480040e-13 3.54728092e-13]
bs_gains = [7.95181956e-12 1.33112640e-12 1.14064534e-12 1.29163144e-12
 1.42476812e-12 1.31212877e-12 1.23894024e-12 8.36559649e-12
 1.22917039e-12 7.65842723e-12]
t_co_uav = [0.53786392 0.50759189 0.58221569 0.50431535 0.47123304 0.49719102
 0.53456273 0.51044297 0.54993092 0.54849738]
t_co_bs = [0.11459805 0.23769431 0.25824188 0.24149859 0.22944398 0.2394982
 0.24691706 0.11280305 0.24796828 0.11596273]
difference = [0.42326587 0.26989758 0.32397381 0.26281676 0.24178906 0.25769281
 0.28764568 0.39763992 0.30196264 0.43253465]
decs_opt = [0 0 0 0 0 0 0 0 0 0]
af = 16.896171935691484	bf = 187.39058616556858	zeta = 33.79234387138297	eta = 0.5
af = 16.896171935691484	bf = 187.39058616556858	zeta = 398.2042402100984	eta = 0.04243091918553357
af = 16.896171935691484	bf = 187.39058616556858	zeta = 251.4496679533076	eta = 0.0671950457251309
af = 16.896171935691484	bf = 187.39058616556858	zeta = 249.79807519939155	eta = 0.0676393199675569
af = 16.896171935691484	bf = 187.39058616556858	zeta = 249.79768256791135	eta = 0.06763942628289996
af = 16.896171935691484	bf = 187.39058616556858	zeta = 249.79768256788898	eta = 0.06763942628290602
eta_opt = 0.06763942628290602
solve_bound_eta tau = 65.0
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [10.50294522 21.78475322 23.66794456 22.13341711 21.02860811 21.95008049
 22.63002084 10.33843249 22.72636567 10.62801845]
solve_bound_eta tau = 37.5
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [10.50294522 21.78475322 23.66794456 22.13341711 21.02860811 21.95008049
 22.63002084 10.33843249 22.72636567 10.62801845]
solve_bound_eta tau = 23.75
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [10.50294522 21.78475322 23.66794456 22.13341711 21.02860811 21.95008049
 22.63002084 10.33843249 22.72636567 10.62801845]
solve_bound_eta tau = 30.625
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [10.50294522 21.78475322 23.66794456 22.13341711 21.02860811 21.95008049
 22.63002084 10.33843249 22.72636567 10.62801845]
solve_bound_eta tau = 34.0625
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [10.50294522 21.78475322 23.66794456 22.13341711 21.02860811 21.95008049
 22.63002084 10.33843249 22.72636567 10.62801845]
solve_bound_eta tau = 32.34375
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [10.50294522 21.78475322 23.66794456 22.13341711 21.02860811 21.95008049
 22.63002084 10.33843249 22.72636567 10.62801845]
solve_bound_eta tau = 33.203125
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [10.50294522 21.78475322 23.66794456 22.13341711 21.02860811 21.95008049
 22.63002084 10.33843249 22.72636567 10.62801845]
solve_bound_eta tau = 32.7734375
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [10.50294522 21.78475322 23.66794456 22.13341711 21.02860811 21.95008049
 22.63002084 10.33843249 22.72636567 10.62801845]
solve_bound_eta tau = 32.55859375
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [10.50294522 21.78475322 23.66794456 22.13341711 21.02860811 21.95008049
 22.63002084 10.33843249 22.72636567 10.62801845]
solve_bound_eta tau = 32.666015625
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [10.50294522 21.78475322 23.66794456 22.13341711 21.02860811 21.95008049
 22.63002084 10.33843249 22.72636567 10.62801845]
solve_bound_eta tau = 32.7197265625
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [10.50294522 21.78475322 23.66794456 22.13341711 21.02860811 21.95008049
 22.63002084 10.33843249 22.72636567 10.62801845]
solve_bound_eta tau = 32.74658203125
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [10.50294522 21.78475322 23.66794456 22.13341711 21.02860811 21.95008049
 22.63002084 10.33843249 22.72636567 10.62801845]
solve_bound_eta tau = 32.733154296875
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [10.50294522 21.78475322 23.66794456 22.13341711 21.02860811 21.95008049
 22.63002084 10.33843249 22.72636567 10.62801845]
solve_bound_eta tau = 32.7398681640625
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [10.50294522 21.78475322 23.66794456 22.13341711 21.02860811 21.95008049
 22.63002084 10.33843249 22.72636567 10.62801845]
solve_bound_eta tau = 32.74322509765625
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [10.50294522 21.78475322 23.66794456 22.13341711 21.02860811 21.95008049
 22.63002084 10.33843249 22.72636567 10.62801845]
solve_bound_eta tau = 32.741546630859375
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [10.50294522 21.78475322 23.66794456 22.13341711 21.02860811 21.95008049
 22.63002084 10.33843249 22.72636567 10.62801845]
initialize_feasible_solution eta = 0.06763942628290602, tau = 32.741546630859375
ti_comp = [0.03660341 0.05688963 0.03748542 0.03528039 0.10363615 0.04410049
 0.06438671 0.04101345 0.03924944 0.03792642]
ti_coms = [0.11459805 0.23769431 0.25824188 0.24149859 0.22944398 0.2394982
 0.24691706 0.11280305 0.24796828 0.11596273]
t_total = [14.86298168 28.9573641  29.06975551 27.20715144 32.74150848 27.87752292
 30.60091009 15.1200384  28.23327014 15.12717981]
optimize_network_fake tau = 42	t_min = 32.741546630859375
ene_coms = [0.01145981 0.02376943 0.02582419 0.02414986 0.0229444  0.02394982
 0.02469171 0.0112803  0.02479683 0.01159627]
ene_comp = [0.25032247 0.38905541 0.25635434 0.24127467 0.70874434 0.30159334
 0.44032627 0.2804818  0.26841807 0.25937027]
ene_total = [24.23477019 38.21769453 26.12297453 24.57195569 67.73685675 30.13750109
 43.04952971 27.01018487 27.1446784  25.08501349]
obj_prev = 333.3111592311691
solve_bound_eta tau = 42
af = [1.24545495 1.93570709 1.27546591 1.2004385  3.5262881  1.50054813
 2.19080027 1.39550976 1.33548784 1.29047139]
bf = [10.50294522 21.78475322 23.66794456 22.13341711 21.02860811 21.95008049
 22.63002084 10.33843249 22.72636567 10.62801845]
eta_min = 0.0026986170626016946	eta_max = 0.40935331101911426
af = 67.58468774276595	bf = 18.739058616556857	zeta = 135.1693754855319	eta = 0.5
af = 67.58468774276595	bf = 18.739058616556857	zeta = 131.17038874895874	eta = 0.5152434813021202
af = 67.58468774276595	bf = 18.739058616556857	zeta = 131.10814278493507	eta = 0.5154881024714794
af = 67.58468774276595	bf = 18.739058616556857	zeta = 131.10812707399847	eta = 0.5154881642434005
eta = 0.5154881642434005
ene_coms = [0.01145981 0.02376943 0.02582419 0.02414986 0.0229444  0.02394982
 0.02469171 0.0112803  0.02479683 0.01159627]
ene_comp = [0.03601906 0.05598144 0.03688699 0.03471717 0.10198168 0.04339646
 0.06335883 0.04035871 0.03862285 0.03732096]
ene_total = [ 8.98110619 15.08567965 11.8624515  11.13529141 23.63102686 12.73922752
 16.65564646  9.76803965 11.99647103  9.2531868 ]
ti_comp = [0.15607387 0.03297761 0.01243004 0.02917333 0.04122794 0.03117372
 0.02375486 0.15786887 0.02270364 0.15470919]
ti_coms = [0.11459805 0.23769431 0.25824188 0.24149859 0.22944398 0.2394982
 0.24691706 0.11280305 0.24796828 0.11596273]
t_total = [42. 42. 42. 42. 42. 42. 42. 42. 42. 42.]
ene_coms = [0.01145981 0.02376943 0.02582419 0.02414986 0.0229444  0.02394982
 0.02469171 0.0112803  0.02479683 0.01159627]
ene_comp = [0.00029363 0.0246917  0.04972014 0.00752521 0.09550819 0.01287191
 0.06898804 0.00040372 0.017108   0.00033242]
ene_total = [ 1.82377288  7.51968516 11.7221684   4.91500094 18.38021672  5.7136062
 14.53623056  1.81300255  6.50234764  1.85096775]
optimize_network_iter = 0 obj = 74.77699879722674
eta = 0.40935331101911426
freqs = [1.55536145e+08 1.14407356e+09 2.00000000e+09 8.02024214e+08
 1.66709233e+09 9.38198684e+08 1.79756154e+09 1.72293888e+08
 1.14651026e+09 1.62579496e+08]
solve_bound_eta tau = 42
af = [16.01499058  3.38388571  1.27546591  2.99352185  4.23046528  3.19878541
  2.43752464 16.19917893  2.32965702 15.87495867]
bf = [10.50294522 21.78475322 23.66794456 22.13341711 21.02860811 21.95008049
 22.63002084 10.33843249 22.72636567 10.62801845]
eta_min = 0.36296577646600475	eta_max = 0.409353311019114
af = 28.46886783346189	bf = 18.739058616556857	zeta = 56.93773566692378	eta = 0.5
af = 28.46886783346189	bf = 18.739058616556857	zeta = 76.94434817810937	eta = 0.36999296904254325
af = 28.46886783346189	bf = 18.739058616556857	zeta = 74.67350974687407	eta = 0.3812445394620497
af = 28.46886783346189	bf = 18.739058616556857	zeta = 74.65306922635729	eta = 0.38134892682229554
af = 28.46886783346189	bf = 18.739058616556857	zeta = 74.6530675020111	eta = 0.3813489356307423
eta = 0.3813489356307423
ene_coms = [0.01145981 0.02376943 0.02582419 0.02414986 0.0229444  0.02394982
 0.02469171 0.0112803  0.02479683 0.01159627]
ene_comp = [0.00031692 0.02665072 0.05366489 0.00812225 0.10308572 0.01389316
 0.07446149 0.00043575 0.01846534 0.00035879]
ene_total = [ 1.74466765  7.46951217 11.77593955  4.78096356 18.67077895  5.60626219
 14.68908754  1.73567883  6.40908963  1.77108744]
ti_comp = [0.16890725 0.045811   0.02526342 0.04200671 0.05406133 0.04400711
 0.03658825 0.17070226 0.03553703 0.16754258]
ti_coms = [0.11459805 0.23769431 0.25824188 0.24149859 0.22944398 0.2394982
 0.24691706 0.11280305 0.24796828 0.11596273]
t_total = [42. 42. 42. 42. 42. 42. 42. 42. 42. 42.]
ene_coms = [0.01145981 0.02376943 0.02582419 0.02414986 0.0229444  0.02394982
 0.02469171 0.0112803  0.02479683 0.01159627]
ene_comp = [0.00031523 0.0160888  0.01513444 0.0045638  0.06984321 0.00812173
 0.03656531 0.00043417 0.00878015 0.0003564 ]
ene_total = [ 1.74441746  5.90481238  6.06783158  4.25379578 13.74605458  4.75125136
  9.07494405  1.73544593  4.97427366  1.77073341]
optimize_network_iter = 1 obj = 54.023560182859335
eta = 0.3813489356307423
freqs = [1.55121189e+08 8.88917153e+08 1.06210650e+09 6.01191209e+08
 1.37221603e+09 7.17329222e+08 1.25965772e+09 1.71982800e+08
 7.90588041e+08 1.62037151e+08]
solve_bound_eta tau = 42
af = [16.05783137  4.35520246  2.40176651  3.99353313  5.13955243  4.18370835
  3.47840566 16.22848056  3.37846708 15.9280928 ]
bf = [10.50294522 21.78475322 23.66794456 22.13341711 21.02860811 21.95008049
 22.63002084 10.33843249 22.72636567 10.62801845]
eta_min = 0.3813489356307518	eta_max = 0.3813489356307163
af = 15.230350461815739	bf = 18.739058616556857	zeta = 30.460700923631478	eta = 0.5
af = 15.230350461815739	bf = 18.739058616556857	zeta = 58.5918661962086	eta = 0.25993967167410814
af = 15.230350461815739	bf = 18.739058616556857	zeta = 53.04837350335061	eta = 0.28710306190356183
af = 15.230350461815739	bf = 18.739058616556857	zeta = 52.94625993819006	eta = 0.28765677650500315
af = 15.230350461815739	bf = 18.739058616556857	zeta = 52.94622027651022	eta = 0.28765699198688105
eta = 0.28765699198688105
ene_coms = [0.01145981 0.02376943 0.02582419 0.02414986 0.0229444  0.02394982
 0.02469171 0.0112803  0.02479683 0.01159627]
ene_comp = [0.00040743 0.02079417 0.0195607  0.00589854 0.09026975 0.01049703
 0.04725931 0.00056115 0.01134801 0.00046064]
ene_total = [ 1.52684214  5.73356867  5.83923562  3.86603748 14.56617092  4.43194351
  9.25724205  1.52352617  4.65040778  1.55124593]
ti_comp = [0.16890725 0.045811   0.02526342 0.04200671 0.05406133 0.04400711
 0.03658825 0.17070226 0.03553703 0.16754258]
ti_coms = [0.11459805 0.23769431 0.25824188 0.24149859 0.22944398 0.2394982
 0.24691706 0.11280305 0.24796828 0.11596273]
t_total = [42. 42. 42. 42. 42. 42. 42. 42. 42. 42.]
ene_coms = [0.01145981 0.02376943 0.02582419 0.02414986 0.0229444  0.02394982
 0.02469171 0.0112803  0.02479683 0.01159627]
ene_comp = [0.00031523 0.0160888  0.01513444 0.0045638  0.06984321 0.00812173
 0.03656531 0.00043417 0.00878015 0.0003564 ]
ene_total = [ 1.74441746  5.90481238  6.06783158  4.25379578 13.74605458  4.75125136
  9.07494405  1.73544593  4.97427366  1.77073341]
optimize_network_iter = 2 obj = 54.02356018286326
eta = 0.3813489356307518
freqs = [1.55121189e+08 8.88917153e+08 1.06210650e+09 6.01191209e+08
 1.37221603e+09 7.17329222e+08 1.25965772e+09 1.71982800e+08
 7.90588041e+08 1.62037151e+08]
Done!
optimize_network_fake num_local_rounds = 31.56758327509822	num_global_rounds = 148.14537480831925
iter = 1	num_local_rounds = 31.56758327509822	num_global_rounds = 148.14537480831925
Round 1
-------------------------------
gradient difference: 0.2801562547683716
At round 0 accuracy: 0.686411149825784
At round 0 training accuracy: 0.6660746003552398
At round 0 training loss: 0.8194667508039355
Round 2
-------------------------------
gradient difference: 0.41836318373680115
At round 1 accuracy: 0.686411149825784
At round 1 training accuracy: 0.6651865008880995
At round 1 training loss: 0.8015262594857888
Round 3
-------------------------------
gradient difference: 0.3344730734825134
At round 2 accuracy: 0.686411149825784
At round 2 training accuracy: 0.6642984014209592
At round 2 training loss: 0.8253055378108722
Round 4
-------------------------------
gradient difference: 0.3321928381919861
At round 3 accuracy: 0.686411149825784
At round 3 training accuracy: 0.6642984014209592
At round 3 training loss: 0.8033391428254478
Round 5
-------------------------------
gradient difference: 0.9354168772697449
At round 4 accuracy: 0.686411149825784
At round 4 training accuracy: 0.6651865008880995
At round 4 training loss: 0.8137377640355958
Round 6
-------------------------------
gradient difference: 0.5181168913841248
At round 5 accuracy: 0.686411149825784
At round 5 training accuracy: 0.6669626998223801
At round 5 training loss: 0.8162292019703937
Round 7
-------------------------------
gradient difference: 0.3085506558418274
At round 6 accuracy: 0.686411149825784
At round 6 training accuracy: 0.6660746003552398
At round 6 training loss: 0.799896105378425
Round 8
-------------------------------
gradient difference: 0.3229132890701294
At round 7 accuracy: 0.686411149825784
At round 7 training accuracy: 0.6660746003552398
At round 7 training loss: 0.8082837068517139
Round 9
-------------------------------
gradient difference: 0.2664812207221985
At round 8 accuracy: 0.686411149825784
At round 8 training accuracy: 0.6687388987566607
At round 8 training loss: 0.7982252057383687
Round 10
-------------------------------
gradient difference: 0.3109627664089203
At round 9 accuracy: 0.6898954703832753
At round 9 training accuracy: 0.6687388987566607
At round 9 training loss: 0.7922140639886269
Round 11
-------------------------------
gradient difference: 0.7394644021987915
At round 10 accuracy: 0.686411149825784
At round 10 training accuracy: 0.6678507992895204
At round 10 training loss: 0.8061357230347261
Round 12
-------------------------------
gradient difference: 0.7933427095413208
At round 11 accuracy: 0.6898954703832753
At round 11 training accuracy: 0.6660746003552398
At round 11 training loss: 0.7926091375609097
Round 13
-------------------------------
gradient difference: 0.31370967626571655
At round 12 accuracy: 0.686411149825784
At round 12 training accuracy: 0.6669626998223801
At round 12 training loss: 0.8032379112328261
Round 14
-------------------------------
gradient difference: 0.34700947999954224
At round 13 accuracy: 0.6829268292682927
At round 13 training accuracy: 0.6678507992895204
At round 13 training loss: 0.7886903255416964
Round 15
-------------------------------
gradient difference: 0.30190494656562805
At round 14 accuracy: 0.6829268292682927
At round 14 training accuracy: 0.6678507992895204
At round 14 training loss: 0.8199638681209444
Round 16
-------------------------------
gradient difference: 0.4050561487674713
At round 15 accuracy: 0.6829268292682927
At round 15 training accuracy: 0.6660746003552398
At round 15 training loss: 0.816868296958387
Round 17
-------------------------------
gradient difference: 0.6666368246078491
At round 16 accuracy: 0.686411149825784
At round 16 training accuracy: 0.6669626998223801
At round 16 training loss: 0.7949129501308048
Round 18
-------------------------------
gradient difference: 0.36460843682289124
At round 17 accuracy: 0.6829268292682927
At round 17 training accuracy: 0.6687388987566607
At round 17 training loss: 0.7915321943863494
Round 19
-------------------------------
gradient difference: 0.47443756461143494
At round 18 accuracy: 0.6829268292682927
At round 18 training accuracy: 0.6678507992895204
At round 18 training loss: 0.8153301153786353
Round 20
-------------------------------
gradient difference: 0.31959739327430725
At round 19 accuracy: 0.686411149825784
At round 19 training accuracy: 0.6669626998223801
At round 19 training loss: 0.7941387693402663
Round 21
-------------------------------
gradient difference: 0.37038668990135193
At round 20 accuracy: 0.686411149825784
At round 20 training accuracy: 0.6660746003552398
At round 20 training loss: 0.8241545107149473
Round 22
-------------------------------
gradient difference: 0.2941916584968567
At round 21 accuracy: 0.686411149825784
At round 21 training accuracy: 0.6651865008880995
At round 21 training loss: 0.8011623755337802
Round 23
-------------------------------
gradient difference: 0.3037358820438385
At round 22 accuracy: 0.686411149825784
At round 22 training accuracy: 0.6651865008880995
At round 22 training loss: 0.8239289373194699
Round 24
-------------------------------
gradient difference: 0.25218063592910767
At round 23 accuracy: 0.686411149825784
At round 23 training accuracy: 0.6642984014209592
At round 23 training loss: 0.7905062380681876
Round 25
-------------------------------
gradient difference: 0.3072000741958618
At round 24 accuracy: 0.6829268292682927
At round 24 training accuracy: 0.6642984014209592
At round 24 training loss: 0.8016481384654821
Round 26
-------------------------------
gradient difference: 0.2946547865867615
At round 25 accuracy: 0.686411149825784
At round 25 training accuracy: 0.6642984014209592
At round 25 training loss: 0.8029126677545236
Round 27
-------------------------------
gradient difference: 0.2876405417919159
At round 26 accuracy: 0.6829268292682927
At round 26 training accuracy: 0.6669626998223801
At round 26 training loss: 0.8356524552433111
Round 28
-------------------------------
gradient difference: 0.3998390734195709
At round 27 accuracy: 0.686411149825784
At round 27 training accuracy: 0.6651865008880995
At round 27 training loss: 0.799453566809857
Round 29
-------------------------------
gradient difference: 0.7017857432365417
At round 28 accuracy: 0.6829268292682927
At round 28 training accuracy: 0.6651865008880995
At round 28 training loss: 0.7910345608263749
Round 30
-------------------------------
gradient difference: 0.2797163128852844
At round 29 accuracy: 0.686411149825784
At round 29 training accuracy: 0.6651865008880995
At round 29 training loss: 0.8025078852869936
Round 31
-------------------------------
gradient difference: 0.2847750782966614
At round 30 accuracy: 0.6829268292682927
At round 30 training accuracy: 0.6651865008880995
At round 30 training loss: 0.8238637067948822
Done!
update_location
xs = [-493.9056584   -20.79968212 1015.00902392   18.81129433  460.97929623
  -26.04359014  502.55680806 -506.32485185  999.66397685  482.93912145]
ys = [ 517.5879595    15.55583871    1.32061395 -977.45517586 -490.64981315
  -17.18584926 -492.62498432  480.82234798   17.56900603  499.00148178]
dists_uav = [ 722.38500484  103.31800857 1019.92404749  982.73724139  680.61674289
  104.75505717  710.80427721  705.37577639 1004.80681555  701.59295453]
dists_bs = [ 468.04628933  221.81113273 1202.61631709 1168.63841721  920.62985992
  243.15306433  951.21309276  450.89185619 1185.16664498  733.38996975]
uav_gains = [1.43676383e-13 9.21631668e-11 6.03478381e-14 6.62375393e-14
 1.67080995e-13 8.90347214e-11 1.49673614e-13 1.52607570e-13
 6.26498031e-14 1.54700579e-13]
bs_gains = [3.68540675e-12 2.98222725e-11 2.62384531e-13 2.84308367e-13
 5.54441751e-13 2.30584863e-11 5.05960008e-13 4.09158323e-12
 2.73345361e-13 1.04798769e-12]
Done!
