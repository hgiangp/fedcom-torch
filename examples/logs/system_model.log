v = 22.697160301531774
a_0 = 27.840057009285058	a_alpha = -0.3425458469693173
['f_00000', 'f_00001', 'f_00002', 'f_00003', 'f_00004', 'f_00005', 'f_00006', 'f_00007', 'f_00008', 'f_00009']
10
dict_keys(['x', 'y'])
id = f_00000, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 126
id = f_00001, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 265
id = f_00002, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 124
id = f_00003, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 43
id = f_00004, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 306
id = f_00005, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 146
id = f_00006, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 54
id = f_00007, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 179
id = f_00008, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 130
id = f_00009, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 118
BaseFederated generated!
num_samples = [126 265 124  43 306 146  54 179 130 118]
msize = 502400
xs = [  6.0943416  -20.79968212  15.00902392  18.81129433 -39.02070377
 -26.04359014   2.55680806  -6.32485185  -0.33602315 -17.06087855]
ys = [ 17.5879595   15.55583871   1.32061395  22.54482414   9.35018685
 -17.18584926   7.37501568 -19.17765202  17.56900603  -0.99851822]
dists_uav = [101.71763524 103.31800857 101.12870423 104.22156154 107.7499017
 104.75505717 100.304178   102.01855757 101.5321766  101.44984286]
dists_bs = [239.94522511 221.81113273 257.42563646 246.58750669 214.31339109
 243.15306433 244.18139867 257.20860434 235.14255819 236.47461703]
uav_gains = [9.58312886e-11 9.21631668e-11 9.72326274e-11 9.01785356e-11
 8.29761259e-11 8.90347214e-11 9.92432040e-11 9.51261523e-11
 9.62695134e-11 9.64649612e-11]
bs_gains = [2.39320649e-11 2.98222725e-11 1.96548195e-11 2.21704798e-11
 3.28364367e-11 2.30584863e-11 2.27876148e-11 1.97012919e-11
 2.53260008e-11 2.49285719e-11]
SystemModel __init__!
t_co_uav = [0.06351163 0.06396501 0.06334461 0.06422068 0.06521719 0.06437154
 0.0631106  0.06359693 0.06345904 0.0634357 ]
t_co_bs = [0.08476871 0.08051968 0.08895491 0.08634835 0.0787878  0.08552995
 0.08577462 0.08890235 0.0836346  0.08394849]
difference = [-0.02125708 -0.01655467 -0.0256103  -0.02212767 -0.01357061 -0.0211584
 -0.02266402 -0.02530541 -0.02017555 -0.0205128 ]
decs_opt = [1 0 1 1 0 0 1 1 0 0]
af = 6.796163711028166	bf = 20.32894796997589	zeta = 7.475780082130983	eta = 0.9090909090909091
af = 6.796163711028166	bf = 20.32894796997589	zeta = 230.74360710778967	eta = 0.02945331312192499
af = 6.796163711028166	bf = 20.32894796997589	zeta = 45.629002909774904	eta = 0.1489439452461113
af = 6.796163711028166	bf = 20.32894796997589	zeta = 39.092727771299245	eta = 0.173847262610764
af = 6.796163711028166	bf = 20.32894796997589	zeta = 38.999286881910415	eta = 0.17426379440236703
af = 6.796163711028166	bf = 20.32894796997589	zeta = 38.99926333319505	eta = 0.17426389962713645
eta_opt = 0.17426389962713645
initialize_feasible_solution eta = 0.17426389962713645, tau = 10.000839233398438
ti_comp = [0.03604337 0.0758055  0.03547125 0.01230051 0.0875339  0.04176454
 0.01544716 0.05120447 0.0371876  0.0337549 ]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [3.35654071 5.27057237 3.33162034 2.5799458  5.60760928 4.29178968
 2.64860954 3.87057992 4.07357378 3.96842191]
system_model train() tau = 30	t0 = 0.050004196166992185	t_min = 10.000839233398438
Round 0
-------------------------------
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.86489914 22.7015787  10.6948057   3.82750737 26.17399534 12.62305496
  4.75731633 15.36016845 11.26073498 10.24387519]
obj_prev = 128.5079361631131
eta_min = 2.1972428731623064e-09	eta_max = 0.9187482009227209
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 29.903120328523933	eta = 0.9090909090909091
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 50.862560519193266	eta = 0.5344727942639534
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 40.950101804889684	eta = 0.6638482847646219
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 39.18020327767913	eta = 0.6938364931761265
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 39.094800455198204	eta = 0.6953521830931377
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 39.094588160812094	eta = 0.6953559590470941
eta = 0.6953559590470941
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.02998124 0.06305577 0.02950534 0.01023169 0.07281157 0.03474016
 0.0128491  0.04259239 0.03093302 0.02807767]
ene_total = [3.3202555  6.49821862 3.27523947 1.52191262 7.37393057 3.95636804
 1.75096148 4.4735121  3.5911297  3.33306006]
ti_comp = [0.26476791 0.24775986 0.26493494 0.26405887 0.24949175 0.2427496
 0.26516894 0.26468261 0.24464495 0.24433105]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [30. 30. 30. 30. 30. 30. 30. 30. 30. 30.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.40269086e-05 2.55265831e-04 2.28719695e-05 9.60109636e-07
 3.87586715e-04 4.44691157e-05 1.88561461e-06 6.89326925e-05
 3.09082577e-05 2.31742490e-05]
ene_total = [0.58260011 0.75916097 0.58096823 0.58697181 0.75542628 0.78568377
 0.5769119  0.5874834  0.76712376 0.76928555]
optimize_network iter = 0 obj = 6.7516157792990965
eta = 0.6953559590470941
freqs = [5.66179550e+07 1.27251793e+08 5.56841306e+07 1.93738838e+07
 1.45919802e+08 7.15555497e+07 2.42281409e+07 8.04593666e+07
 6.32202327e+07 5.74582420e+07]
eta_min = 0.6595211103034527	eta_max = 0.6953559590470929
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 0.07249356690995133	eta = 0.909090909090909
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 22.43093651594189	eta = 0.002938051320263521
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 2.424233800512977	eta = 0.027185184296772417
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 2.333929777668074	eta = 0.028237028926918685
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 2.333881523150504	eta = 0.02823761274584649
eta = 0.02823761274584649
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.35890421e-04 2.50613865e-03 2.24551507e-04 9.42612594e-06
 3.80523333e-03 4.36587104e-04 1.85125116e-05 6.76764628e-04
 3.03449855e-04 2.27519213e-04]
ene_total = [0.18871274 0.3024796  0.18790939 0.18425611 0.33473573 0.25754286
 0.18133617 0.20158775 0.24829862 0.24702256]
ti_comp = [0.30338297 0.28637492 0.30355    0.30267393 0.28810681 0.28136466
 0.303784   0.30329767 0.28326001 0.28294611]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [30. 30. 30. 30. 30. 30. 30. 30. 30. 30.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.75152158e-05 2.87283940e-04 2.61968029e-05 1.09875040e-06
 4.37019626e-04 4.97694879e-05 2.16020605e-06 7.89340962e-05
 3.46659600e-05 2.59825795e-05]
ene_total = [0.52156762 0.68187855 0.52009413 0.52520532 0.6799609  0.70342523
 0.51621531 0.52646951 0.68669249 0.68854913]
optimize_network iter = 1 obj = 6.050058200424808
eta = 0.6595211103034527
freqs = [5.66070564e+07 1.26125273e+08 5.56778794e+07 1.93635364e+07
 1.44763522e+08 7.07252580e+07 2.42281409e+07 8.04405789e+07
 6.25531698e+07 5.68420211e+07]
eta_min = 0.6595211103034622	eta_max = 0.6595211103034516
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 0.07141177968017555	eta = 0.909090909090909
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 22.429905462588092	eta = 0.0028943412096646293
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 2.4193533145705937	eta = 0.026833534117679286
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 2.3303118142037405	eta = 0.02785884674898469
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 2.3302654452731733	eta = 0.02785940109996355
eta = 0.02785940109996355
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.36690987e-04 2.47126971e-03 2.25349755e-04 9.45165461e-06
 3.75932384e-03 4.28126361e-04 1.85824929e-05 6.79005729e-04
 2.98203014e-04 2.23506966e-04]
ene_total = [0.18866225 0.30136334 0.18785915 0.18418516 0.33329075 0.25720037
 0.18126762 0.2015735  0.24805176 0.24681156]
ti_comp = [0.30338297 0.28637492 0.30355    0.30267393 0.28810681 0.28136466
 0.303784   0.30329767 0.28326001 0.28294611]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [30. 30. 30. 30. 30. 30. 30. 30. 30. 30.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.75152158e-05 2.87283940e-04 2.61968029e-05 1.09875040e-06
 4.37019626e-04 4.97694879e-05 2.16020605e-06 7.89340962e-05
 3.46659600e-05 2.59825795e-05]
ene_total = [0.52156762 0.68187855 0.52009413 0.52520532 0.6799609  0.70342523
 0.51621531 0.52646951 0.68669249 0.68854913]
optimize_network iter = 2 obj = 6.0500582004249726
eta = 0.6595211103034622
freqs = [5.66070564e+07 1.26125273e+08 5.56778794e+07 1.93635364e+07
 1.44763522e+08 7.07252580e+07 2.42281409e+07 8.04405789e+07
 6.25531698e+07 5.68420211e+07]
Done!
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.62436988e-05 2.74008143e-04 2.49862116e-05 1.04797559e-06
 4.16824332e-04 4.74695695e-05 2.06037988e-06 7.52864401e-05
 3.30639970e-05 2.47818878e-05]
ene_total = [0.00637741 0.00832598 0.00635945 0.00642312 0.0082956  0.00860046
 0.00631312 0.00643498 0.00839652 0.00841963]
At round 0 energy consumption: 0.07394626955167252
At round 0 eta: 0.6595211103034622
At round 0 a_n: 27.840057009285058
At round 0 local rounds: 13.62985484868938
At round 0 global rounds: 81.76735137411414
gradient difference: 1.1764147281646729
train() client id: f_00000-0-0 loss: 0.992933  [   32/  126]
train() client id: f_00000-0-1 loss: 1.137095  [   64/  126]
train() client id: f_00000-0-2 loss: 1.062667  [   96/  126]
train() client id: f_00000-1-0 loss: 1.110384  [   32/  126]
train() client id: f_00000-1-1 loss: 1.164084  [   64/  126]
train() client id: f_00000-1-2 loss: 1.029892  [   96/  126]
train() client id: f_00000-2-0 loss: 1.016150  [   32/  126]
train() client id: f_00000-2-1 loss: 1.173880  [   64/  126]
train() client id: f_00000-2-2 loss: 1.111411  [   96/  126]
train() client id: f_00000-3-0 loss: 1.075733  [   32/  126]
train() client id: f_00000-3-1 loss: 1.136158  [   64/  126]
train() client id: f_00000-3-2 loss: 1.210745  [   96/  126]
train() client id: f_00000-4-0 loss: 1.267512  [   32/  126]
train() client id: f_00000-4-1 loss: 1.152882  [   64/  126]
train() client id: f_00000-4-2 loss: 1.233451  [   96/  126]
train() client id: f_00000-5-0 loss: 1.195190  [   32/  126]
train() client id: f_00000-5-1 loss: 1.264115  [   64/  126]
train() client id: f_00000-5-2 loss: 1.235015  [   96/  126]
train() client id: f_00000-6-0 loss: 1.275929  [   32/  126]
train() client id: f_00000-6-1 loss: 1.201609  [   64/  126]
train() client id: f_00000-6-2 loss: 1.388459  [   96/  126]
train() client id: f_00000-7-0 loss: 1.301342  [   32/  126]
train() client id: f_00000-7-1 loss: 1.268964  [   64/  126]
train() client id: f_00000-7-2 loss: 1.349419  [   96/  126]
train() client id: f_00000-8-0 loss: 1.370348  [   32/  126]
train() client id: f_00000-8-1 loss: 1.229059  [   64/  126]
train() client id: f_00000-8-2 loss: 1.473368  [   96/  126]
train() client id: f_00000-9-0 loss: 1.383001  [   32/  126]
train() client id: f_00000-9-1 loss: 1.380453  [   64/  126]
train() client id: f_00000-9-2 loss: 1.300084  [   96/  126]
train() client id: f_00000-10-0 loss: 1.341543  [   32/  126]
train() client id: f_00000-10-1 loss: 1.427392  [   64/  126]
train() client id: f_00000-10-2 loss: 1.475134  [   96/  126]
train() client id: f_00000-11-0 loss: 1.464432  [   32/  126]
train() client id: f_00000-11-1 loss: 1.452486  [   64/  126]
train() client id: f_00000-11-2 loss: 1.468946  [   96/  126]
train() client id: f_00000-12-0 loss: 1.474566  [   32/  126]
train() client id: f_00000-12-1 loss: 1.484766  [   64/  126]
train() client id: f_00000-12-2 loss: 1.487285  [   96/  126]
train() client id: f_00001-0-0 loss: 0.876875  [   32/  265]
train() client id: f_00001-0-1 loss: 0.874113  [   64/  265]
train() client id: f_00001-0-2 loss: 0.880879  [   96/  265]
train() client id: f_00001-0-3 loss: 0.854245  [  128/  265]
train() client id: f_00001-0-4 loss: 0.984689  [  160/  265]
train() client id: f_00001-0-5 loss: 0.853262  [  192/  265]
train() client id: f_00001-0-6 loss: 0.898285  [  224/  265]
train() client id: f_00001-0-7 loss: 0.954925  [  256/  265]
train() client id: f_00001-1-0 loss: 0.865491  [   32/  265]
train() client id: f_00001-1-1 loss: 0.854044  [   64/  265]
train() client id: f_00001-1-2 loss: 0.836368  [   96/  265]
train() client id: f_00001-1-3 loss: 0.891936  [  128/  265]
train() client id: f_00001-1-4 loss: 0.998952  [  160/  265]
train() client id: f_00001-1-5 loss: 0.834074  [  192/  265]
train() client id: f_00001-1-6 loss: 0.997031  [  224/  265]
train() client id: f_00001-1-7 loss: 0.869786  [  256/  265]
train() client id: f_00001-2-0 loss: 0.886342  [   32/  265]
train() client id: f_00001-2-1 loss: 0.862736  [   64/  265]
train() client id: f_00001-2-2 loss: 0.932628  [   96/  265]
train() client id: f_00001-2-3 loss: 0.888058  [  128/  265]
train() client id: f_00001-2-4 loss: 0.900974  [  160/  265]
train() client id: f_00001-2-5 loss: 0.853463  [  192/  265]
train() client id: f_00001-2-6 loss: 0.997464  [  224/  265]
train() client id: f_00001-2-7 loss: 0.860894  [  256/  265]
train() client id: f_00001-3-0 loss: 0.886621  [   32/  265]
train() client id: f_00001-3-1 loss: 0.969963  [   64/  265]
train() client id: f_00001-3-2 loss: 0.838769  [   96/  265]
train() client id: f_00001-3-3 loss: 0.911587  [  128/  265]
train() client id: f_00001-3-4 loss: 0.948964  [  160/  265]
train() client id: f_00001-3-5 loss: 0.934462  [  192/  265]
train() client id: f_00001-3-6 loss: 0.873210  [  224/  265]
train() client id: f_00001-3-7 loss: 0.954088  [  256/  265]
train() client id: f_00001-4-0 loss: 0.849595  [   32/  265]
train() client id: f_00001-4-1 loss: 0.911944  [   64/  265]
train() client id: f_00001-4-2 loss: 0.944597  [   96/  265]
train() client id: f_00001-4-3 loss: 1.025685  [  128/  265]
train() client id: f_00001-4-4 loss: 0.935703  [  160/  265]
train() client id: f_00001-4-5 loss: 0.874845  [  192/  265]
train() client id: f_00001-4-6 loss: 0.929152  [  224/  265]
train() client id: f_00001-4-7 loss: 0.938162  [  256/  265]
train() client id: f_00001-5-0 loss: 0.888222  [   32/  265]
train() client id: f_00001-5-1 loss: 0.957806  [   64/  265]
train() client id: f_00001-5-2 loss: 1.008259  [   96/  265]
train() client id: f_00001-5-3 loss: 0.906021  [  128/  265]
train() client id: f_00001-5-4 loss: 0.934079  [  160/  265]
train() client id: f_00001-5-5 loss: 0.875013  [  192/  265]
train() client id: f_00001-5-6 loss: 1.000315  [  224/  265]
train() client id: f_00001-5-7 loss: 0.882526  [  256/  265]
train() client id: f_00001-6-0 loss: 0.857397  [   32/  265]
train() client id: f_00001-6-1 loss: 0.898962  [   64/  265]
train() client id: f_00001-6-2 loss: 0.957768  [   96/  265]
train() client id: f_00001-6-3 loss: 0.926809  [  128/  265]
train() client id: f_00001-6-4 loss: 0.994787  [  160/  265]
train() client id: f_00001-6-5 loss: 0.968167  [  192/  265]
train() client id: f_00001-6-6 loss: 1.029824  [  224/  265]
train() client id: f_00001-6-7 loss: 0.920721  [  256/  265]
train() client id: f_00001-7-0 loss: 0.947755  [   32/  265]
train() client id: f_00001-7-1 loss: 0.920525  [   64/  265]
train() client id: f_00001-7-2 loss: 1.084916  [   96/  265]
train() client id: f_00001-7-3 loss: 0.952717  [  128/  265]
train() client id: f_00001-7-4 loss: 0.971811  [  160/  265]
train() client id: f_00001-7-5 loss: 0.870818  [  192/  265]
train() client id: f_00001-7-6 loss: 1.031160  [  224/  265]
train() client id: f_00001-7-7 loss: 0.940861  [  256/  265]
train() client id: f_00001-8-0 loss: 0.910938  [   32/  265]
train() client id: f_00001-8-1 loss: 1.031780  [   64/  265]
train() client id: f_00001-8-2 loss: 0.989954  [   96/  265]
train() client id: f_00001-8-3 loss: 1.030291  [  128/  265]
train() client id: f_00001-8-4 loss: 1.010747  [  160/  265]
train() client id: f_00001-8-5 loss: 0.889402  [  192/  265]
train() client id: f_00001-8-6 loss: 0.960850  [  224/  265]
train() client id: f_00001-8-7 loss: 1.002803  [  256/  265]
train() client id: f_00001-9-0 loss: 0.869451  [   32/  265]
train() client id: f_00001-9-1 loss: 0.909356  [   64/  265]
train() client id: f_00001-9-2 loss: 1.025133  [   96/  265]
train() client id: f_00001-9-3 loss: 0.921885  [  128/  265]
train() client id: f_00001-9-4 loss: 0.999378  [  160/  265]
train() client id: f_00001-9-5 loss: 0.983004  [  192/  265]
train() client id: f_00001-9-6 loss: 1.076020  [  224/  265]
train() client id: f_00001-9-7 loss: 1.147934  [  256/  265]
train() client id: f_00001-10-0 loss: 0.943544  [   32/  265]
train() client id: f_00001-10-1 loss: 1.100682  [   64/  265]
train() client id: f_00001-10-2 loss: 1.070783  [   96/  265]
train() client id: f_00001-10-3 loss: 0.924724  [  128/  265]
train() client id: f_00001-10-4 loss: 0.963088  [  160/  265]
train() client id: f_00001-10-5 loss: 0.938521  [  192/  265]
train() client id: f_00001-10-6 loss: 1.112303  [  224/  265]
train() client id: f_00001-10-7 loss: 0.985248  [  256/  265]
train() client id: f_00001-11-0 loss: 1.049104  [   32/  265]
train() client id: f_00001-11-1 loss: 0.978078  [   64/  265]
train() client id: f_00001-11-2 loss: 0.966487  [   96/  265]
train() client id: f_00001-11-3 loss: 1.003963  [  128/  265]
train() client id: f_00001-11-4 loss: 1.181476  [  160/  265]
train() client id: f_00001-11-5 loss: 0.975852  [  192/  265]
train() client id: f_00001-11-6 loss: 0.989696  [  224/  265]
train() client id: f_00001-11-7 loss: 0.977061  [  256/  265]
train() client id: f_00001-12-0 loss: 0.972288  [   32/  265]
train() client id: f_00001-12-1 loss: 1.005869  [   64/  265]
train() client id: f_00001-12-2 loss: 1.097464  [   96/  265]
train() client id: f_00001-12-3 loss: 0.939754  [  128/  265]
train() client id: f_00001-12-4 loss: 1.032710  [  160/  265]
train() client id: f_00001-12-5 loss: 1.116142  [  192/  265]
train() client id: f_00001-12-6 loss: 0.973223  [  224/  265]
train() client id: f_00001-12-7 loss: 1.094831  [  256/  265]
train() client id: f_00002-0-0 loss: 0.984837  [   32/  124]
train() client id: f_00002-0-1 loss: 0.980319  [   64/  124]
train() client id: f_00002-0-2 loss: 1.058805  [   96/  124]
train() client id: f_00002-1-0 loss: 1.014942  [   32/  124]
train() client id: f_00002-1-1 loss: 0.976365  [   64/  124]
train() client id: f_00002-1-2 loss: 0.904580  [   96/  124]
train() client id: f_00002-2-0 loss: 0.987060  [   32/  124]
train() client id: f_00002-2-1 loss: 0.992455  [   64/  124]
train() client id: f_00002-2-2 loss: 0.944918  [   96/  124]
train() client id: f_00002-3-0 loss: 0.982620  [   32/  124]
train() client id: f_00002-3-1 loss: 0.918063  [   64/  124]
train() client id: f_00002-3-2 loss: 1.059213  [   96/  124]
train() client id: f_00002-4-0 loss: 0.997171  [   32/  124]
train() client id: f_00002-4-1 loss: 1.014416  [   64/  124]
train() client id: f_00002-4-2 loss: 1.006115  [   96/  124]
train() client id: f_00002-5-0 loss: 1.031898  [   32/  124]
train() client id: f_00002-5-1 loss: 0.931187  [   64/  124]
train() client id: f_00002-5-2 loss: 0.999555  [   96/  124]
train() client id: f_00002-6-0 loss: 0.946184  [   32/  124]
train() client id: f_00002-6-1 loss: 1.036928  [   64/  124]
train() client id: f_00002-6-2 loss: 0.956517  [   96/  124]
train() client id: f_00002-7-0 loss: 0.975185  [   32/  124]
train() client id: f_00002-7-1 loss: 0.983793  [   64/  124]
train() client id: f_00002-7-2 loss: 1.010158  [   96/  124]
train() client id: f_00002-8-0 loss: 1.034379  [   32/  124]
train() client id: f_00002-8-1 loss: 1.087320  [   64/  124]
train() client id: f_00002-8-2 loss: 0.994224  [   96/  124]
train() client id: f_00002-9-0 loss: 1.001732  [   32/  124]
train() client id: f_00002-9-1 loss: 1.037435  [   64/  124]
train() client id: f_00002-9-2 loss: 1.084133  [   96/  124]
train() client id: f_00002-10-0 loss: 0.890080  [   32/  124]
train() client id: f_00002-10-1 loss: 1.051584  [   64/  124]
train() client id: f_00002-10-2 loss: 1.136568  [   96/  124]
train() client id: f_00002-11-0 loss: 1.061661  [   32/  124]
train() client id: f_00002-11-1 loss: 1.065522  [   64/  124]
train() client id: f_00002-11-2 loss: 0.999305  [   96/  124]
train() client id: f_00002-12-0 loss: 1.125593  [   32/  124]
train() client id: f_00002-12-1 loss: 1.097022  [   64/  124]
train() client id: f_00002-12-2 loss: 1.037362  [   96/  124]
train() client id: f_00003-0-0 loss: 1.054284  [   32/   43]
train() client id: f_00003-1-0 loss: 1.014959  [   32/   43]
train() client id: f_00003-2-0 loss: 1.062093  [   32/   43]
train() client id: f_00003-3-0 loss: 1.042250  [   32/   43]
train() client id: f_00003-4-0 loss: 0.998736  [   32/   43]
train() client id: f_00003-5-0 loss: 1.026125  [   32/   43]
train() client id: f_00003-6-0 loss: 0.979597  [   32/   43]
train() client id: f_00003-7-0 loss: 1.030290  [   32/   43]
train() client id: f_00003-8-0 loss: 0.978879  [   32/   43]
train() client id: f_00003-9-0 loss: 0.999079  [   32/   43]
train() client id: f_00003-10-0 loss: 1.045099  [   32/   43]
train() client id: f_00003-11-0 loss: 1.043642  [   32/   43]
train() client id: f_00003-12-0 loss: 1.018984  [   32/   43]
train() client id: f_00004-0-0 loss: 0.783499  [   32/  306]
train() client id: f_00004-0-1 loss: 0.998432  [   64/  306]
train() client id: f_00004-0-2 loss: 0.826727  [   96/  306]
train() client id: f_00004-0-3 loss: 1.004033  [  128/  306]
train() client id: f_00004-0-4 loss: 0.759155  [  160/  306]
train() client id: f_00004-0-5 loss: 1.033563  [  192/  306]
train() client id: f_00004-0-6 loss: 1.012229  [  224/  306]
train() client id: f_00004-0-7 loss: 0.950963  [  256/  306]
train() client id: f_00004-0-8 loss: 0.835725  [  288/  306]
train() client id: f_00004-1-0 loss: 0.798254  [   32/  306]
train() client id: f_00004-1-1 loss: 0.947565  [   64/  306]
train() client id: f_00004-1-2 loss: 0.943458  [   96/  306]
train() client id: f_00004-1-3 loss: 0.949432  [  128/  306]
train() client id: f_00004-1-4 loss: 0.940245  [  160/  306]
train() client id: f_00004-1-5 loss: 0.907656  [  192/  306]
train() client id: f_00004-1-6 loss: 0.763805  [  224/  306]
train() client id: f_00004-1-7 loss: 0.990174  [  256/  306]
train() client id: f_00004-1-8 loss: 0.831862  [  288/  306]
train() client id: f_00004-2-0 loss: 0.766463  [   32/  306]
train() client id: f_00004-2-1 loss: 1.021625  [   64/  306]
train() client id: f_00004-2-2 loss: 1.012362  [   96/  306]
train() client id: f_00004-2-3 loss: 0.927915  [  128/  306]
train() client id: f_00004-2-4 loss: 0.749653  [  160/  306]
train() client id: f_00004-2-5 loss: 1.055583  [  192/  306]
train() client id: f_00004-2-6 loss: 0.789107  [  224/  306]
train() client id: f_00004-2-7 loss: 0.910075  [  256/  306]
train() client id: f_00004-2-8 loss: 0.980063  [  288/  306]
train() client id: f_00004-3-0 loss: 1.021373  [   32/  306]
train() client id: f_00004-3-1 loss: 0.856917  [   64/  306]
train() client id: f_00004-3-2 loss: 1.074046  [   96/  306]
train() client id: f_00004-3-3 loss: 0.716247  [  128/  306]
train() client id: f_00004-3-4 loss: 0.950385  [  160/  306]
train() client id: f_00004-3-5 loss: 0.943706  [  192/  306]
train() client id: f_00004-3-6 loss: 1.066646  [  224/  306]
train() client id: f_00004-3-7 loss: 0.788971  [  256/  306]
train() client id: f_00004-3-8 loss: 0.811168  [  288/  306]
train() client id: f_00004-4-0 loss: 0.791549  [   32/  306]
train() client id: f_00004-4-1 loss: 0.786562  [   64/  306]
train() client id: f_00004-4-2 loss: 0.824707  [   96/  306]
train() client id: f_00004-4-3 loss: 0.982951  [  128/  306]
train() client id: f_00004-4-4 loss: 0.939649  [  160/  306]
train() client id: f_00004-4-5 loss: 0.957909  [  192/  306]
train() client id: f_00004-4-6 loss: 1.065031  [  224/  306]
train() client id: f_00004-4-7 loss: 0.785103  [  256/  306]
train() client id: f_00004-4-8 loss: 1.011307  [  288/  306]
train() client id: f_00004-5-0 loss: 1.096318  [   32/  306]
train() client id: f_00004-5-1 loss: 0.993494  [   64/  306]
train() client id: f_00004-5-2 loss: 0.825941  [   96/  306]
train() client id: f_00004-5-3 loss: 0.924938  [  128/  306]
train() client id: f_00004-5-4 loss: 0.816661  [  160/  306]
train() client id: f_00004-5-5 loss: 0.931476  [  192/  306]
train() client id: f_00004-5-6 loss: 0.908760  [  224/  306]
train() client id: f_00004-5-7 loss: 0.914277  [  256/  306]
train() client id: f_00004-5-8 loss: 0.886551  [  288/  306]
train() client id: f_00004-6-0 loss: 0.895106  [   32/  306]
train() client id: f_00004-6-1 loss: 0.905657  [   64/  306]
train() client id: f_00004-6-2 loss: 0.819267  [   96/  306]
train() client id: f_00004-6-3 loss: 0.819216  [  128/  306]
train() client id: f_00004-6-4 loss: 0.965589  [  160/  306]
train() client id: f_00004-6-5 loss: 0.978455  [  192/  306]
train() client id: f_00004-6-6 loss: 1.042182  [  224/  306]
train() client id: f_00004-6-7 loss: 0.879224  [  256/  306]
train() client id: f_00004-6-8 loss: 0.859812  [  288/  306]
train() client id: f_00004-7-0 loss: 0.857128  [   32/  306]
train() client id: f_00004-7-1 loss: 0.892267  [   64/  306]
train() client id: f_00004-7-2 loss: 0.896591  [   96/  306]
train() client id: f_00004-7-3 loss: 0.832916  [  128/  306]
train() client id: f_00004-7-4 loss: 0.991944  [  160/  306]
train() client id: f_00004-7-5 loss: 0.916345  [  192/  306]
train() client id: f_00004-7-6 loss: 0.875077  [  224/  306]
train() client id: f_00004-7-7 loss: 1.063636  [  256/  306]
train() client id: f_00004-7-8 loss: 0.890914  [  288/  306]
train() client id: f_00004-8-0 loss: 0.940808  [   32/  306]
train() client id: f_00004-8-1 loss: 0.876756  [   64/  306]
train() client id: f_00004-8-2 loss: 1.003398  [   96/  306]
train() client id: f_00004-8-3 loss: 0.899906  [  128/  306]
train() client id: f_00004-8-4 loss: 0.909031  [  160/  306]
train() client id: f_00004-8-5 loss: 0.833985  [  192/  306]
train() client id: f_00004-8-6 loss: 0.969333  [  224/  306]
train() client id: f_00004-8-7 loss: 0.843371  [  256/  306]
train() client id: f_00004-8-8 loss: 0.906071  [  288/  306]
train() client id: f_00004-9-0 loss: 0.905740  [   32/  306]
train() client id: f_00004-9-1 loss: 0.988973  [   64/  306]
train() client id: f_00004-9-2 loss: 0.847997  [   96/  306]
train() client id: f_00004-9-3 loss: 0.790951  [  128/  306]
train() client id: f_00004-9-4 loss: 0.899460  [  160/  306]
train() client id: f_00004-9-5 loss: 0.912524  [  192/  306]
train() client id: f_00004-9-6 loss: 1.022359  [  224/  306]
train() client id: f_00004-9-7 loss: 0.863519  [  256/  306]
train() client id: f_00004-9-8 loss: 0.969936  [  288/  306]
train() client id: f_00004-10-0 loss: 0.971257  [   32/  306]
train() client id: f_00004-10-1 loss: 0.820784  [   64/  306]
train() client id: f_00004-10-2 loss: 0.985046  [   96/  306]
train() client id: f_00004-10-3 loss: 0.832585  [  128/  306]
train() client id: f_00004-10-4 loss: 0.834722  [  160/  306]
train() client id: f_00004-10-5 loss: 0.812243  [  192/  306]
train() client id: f_00004-10-6 loss: 0.899225  [  224/  306]
train() client id: f_00004-10-7 loss: 0.992965  [  256/  306]
train() client id: f_00004-10-8 loss: 1.043072  [  288/  306]
train() client id: f_00004-11-0 loss: 1.083737  [   32/  306]
train() client id: f_00004-11-1 loss: 0.765854  [   64/  306]
train() client id: f_00004-11-2 loss: 0.821106  [   96/  306]
train() client id: f_00004-11-3 loss: 0.952750  [  128/  306]
train() client id: f_00004-11-4 loss: 0.953600  [  160/  306]
train() client id: f_00004-11-5 loss: 0.928246  [  192/  306]
train() client id: f_00004-11-6 loss: 0.983711  [  224/  306]
train() client id: f_00004-11-7 loss: 0.967431  [  256/  306]
train() client id: f_00004-11-8 loss: 0.810048  [  288/  306]
train() client id: f_00004-12-0 loss: 0.964334  [   32/  306]
train() client id: f_00004-12-1 loss: 0.895717  [   64/  306]
train() client id: f_00004-12-2 loss: 0.967519  [   96/  306]
train() client id: f_00004-12-3 loss: 0.976787  [  128/  306]
train() client id: f_00004-12-4 loss: 0.800330  [  160/  306]
train() client id: f_00004-12-5 loss: 0.929293  [  192/  306]
train() client id: f_00004-12-6 loss: 0.902943  [  224/  306]
train() client id: f_00004-12-7 loss: 1.039992  [  256/  306]
train() client id: f_00004-12-8 loss: 0.825341  [  288/  306]
train() client id: f_00005-0-0 loss: 1.190654  [   32/  146]
train() client id: f_00005-0-1 loss: 0.974900  [   64/  146]
train() client id: f_00005-0-2 loss: 1.092176  [   96/  146]
train() client id: f_00005-0-3 loss: 0.950175  [  128/  146]
train() client id: f_00005-1-0 loss: 1.056877  [   32/  146]
train() client id: f_00005-1-1 loss: 1.104999  [   64/  146]
train() client id: f_00005-1-2 loss: 1.033717  [   96/  146]
train() client id: f_00005-1-3 loss: 1.049927  [  128/  146]
train() client id: f_00005-2-0 loss: 1.102942  [   32/  146]
train() client id: f_00005-2-1 loss: 1.020844  [   64/  146]
train() client id: f_00005-2-2 loss: 1.030239  [   96/  146]
train() client id: f_00005-2-3 loss: 1.077156  [  128/  146]
train() client id: f_00005-3-0 loss: 1.157178  [   32/  146]
train() client id: f_00005-3-1 loss: 0.974244  [   64/  146]
train() client id: f_00005-3-2 loss: 1.058337  [   96/  146]
train() client id: f_00005-3-3 loss: 1.076566  [  128/  146]
train() client id: f_00005-4-0 loss: 1.148681  [   32/  146]
train() client id: f_00005-4-1 loss: 0.969920  [   64/  146]
train() client id: f_00005-4-2 loss: 1.071086  [   96/  146]
train() client id: f_00005-4-3 loss: 1.054749  [  128/  146]
train() client id: f_00005-5-0 loss: 1.148703  [   32/  146]
train() client id: f_00005-5-1 loss: 0.999365  [   64/  146]
train() client id: f_00005-5-2 loss: 1.124933  [   96/  146]
train() client id: f_00005-5-3 loss: 1.040834  [  128/  146]
train() client id: f_00005-6-0 loss: 1.109217  [   32/  146]
train() client id: f_00005-6-1 loss: 1.105443  [   64/  146]
train() client id: f_00005-6-2 loss: 1.059149  [   96/  146]
train() client id: f_00005-6-3 loss: 1.113152  [  128/  146]
train() client id: f_00005-7-0 loss: 1.078901  [   32/  146]
train() client id: f_00005-7-1 loss: 1.142058  [   64/  146]
train() client id: f_00005-7-2 loss: 1.152698  [   96/  146]
train() client id: f_00005-7-3 loss: 1.051196  [  128/  146]
train() client id: f_00005-8-0 loss: 0.950220  [   32/  146]
train() client id: f_00005-8-1 loss: 1.019703  [   64/  146]
train() client id: f_00005-8-2 loss: 1.284522  [   96/  146]
train() client id: f_00005-8-3 loss: 1.143730  [  128/  146]
train() client id: f_00005-9-0 loss: 0.977902  [   32/  146]
train() client id: f_00005-9-1 loss: 1.202197  [   64/  146]
train() client id: f_00005-9-2 loss: 1.185638  [   96/  146]
train() client id: f_00005-9-3 loss: 1.064913  [  128/  146]
train() client id: f_00005-10-0 loss: 1.168633  [   32/  146]
train() client id: f_00005-10-1 loss: 1.242658  [   64/  146]
train() client id: f_00005-10-2 loss: 1.109869  [   96/  146]
train() client id: f_00005-10-3 loss: 1.036668  [  128/  146]
train() client id: f_00005-11-0 loss: 1.260496  [   32/  146]
train() client id: f_00005-11-1 loss: 1.130465  [   64/  146]
train() client id: f_00005-11-2 loss: 1.080955  [   96/  146]
train() client id: f_00005-11-3 loss: 1.080367  [  128/  146]
train() client id: f_00005-12-0 loss: 1.163291  [   32/  146]
train() client id: f_00005-12-1 loss: 1.092836  [   64/  146]
train() client id: f_00005-12-2 loss: 1.281917  [   96/  146]
train() client id: f_00005-12-3 loss: 1.134515  [  128/  146]
train() client id: f_00006-0-0 loss: 1.052372  [   32/   54]
train() client id: f_00006-1-0 loss: 1.055321  [   32/   54]
train() client id: f_00006-2-0 loss: 1.030908  [   32/   54]
train() client id: f_00006-3-0 loss: 0.997023  [   32/   54]
train() client id: f_00006-4-0 loss: 1.069654  [   32/   54]
train() client id: f_00006-5-0 loss: 1.039829  [   32/   54]
train() client id: f_00006-6-0 loss: 1.039960  [   32/   54]
train() client id: f_00006-7-0 loss: 1.094336  [   32/   54]
train() client id: f_00006-8-0 loss: 1.057999  [   32/   54]
train() client id: f_00006-9-0 loss: 1.068925  [   32/   54]
train() client id: f_00006-10-0 loss: 1.069285  [   32/   54]
train() client id: f_00006-11-0 loss: 1.072410  [   32/   54]
train() client id: f_00006-12-0 loss: 1.080469  [   32/   54]
train() client id: f_00007-0-0 loss: 0.861418  [   32/  179]
train() client id: f_00007-0-1 loss: 0.800292  [   64/  179]
train() client id: f_00007-0-2 loss: 0.838125  [   96/  179]
train() client id: f_00007-0-3 loss: 0.846320  [  128/  179]
train() client id: f_00007-0-4 loss: 0.841684  [  160/  179]
train() client id: f_00007-1-0 loss: 0.884907  [   32/  179]
train() client id: f_00007-1-1 loss: 0.777848  [   64/  179]
train() client id: f_00007-1-2 loss: 0.841331  [   96/  179]
train() client id: f_00007-1-3 loss: 0.828140  [  128/  179]
train() client id: f_00007-1-4 loss: 0.830069  [  160/  179]
train() client id: f_00007-2-0 loss: 0.856377  [   32/  179]
train() client id: f_00007-2-1 loss: 0.847705  [   64/  179]
train() client id: f_00007-2-2 loss: 0.797401  [   96/  179]
train() client id: f_00007-2-3 loss: 0.831406  [  128/  179]
train() client id: f_00007-2-4 loss: 0.896546  [  160/  179]
train() client id: f_00007-3-0 loss: 0.876581  [   32/  179]
train() client id: f_00007-3-1 loss: 0.843164  [   64/  179]
train() client id: f_00007-3-2 loss: 0.831103  [   96/  179]
train() client id: f_00007-3-3 loss: 0.863367  [  128/  179]
train() client id: f_00007-3-4 loss: 0.818254  [  160/  179]
train() client id: f_00007-4-0 loss: 0.861657  [   32/  179]
train() client id: f_00007-4-1 loss: 0.841120  [   64/  179]
train() client id: f_00007-4-2 loss: 0.828606  [   96/  179]
train() client id: f_00007-4-3 loss: 0.856614  [  128/  179]
train() client id: f_00007-4-4 loss: 0.849253  [  160/  179]
train() client id: f_00007-5-0 loss: 0.801683  [   32/  179]
train() client id: f_00007-5-1 loss: 0.858414  [   64/  179]
train() client id: f_00007-5-2 loss: 0.805411  [   96/  179]
train() client id: f_00007-5-3 loss: 0.909267  [  128/  179]
train() client id: f_00007-5-4 loss: 0.875401  [  160/  179]
train() client id: f_00007-6-0 loss: 0.791559  [   32/  179]
train() client id: f_00007-6-1 loss: 0.875191  [   64/  179]
train() client id: f_00007-6-2 loss: 0.813669  [   96/  179]
train() client id: f_00007-6-3 loss: 0.893654  [  128/  179]
train() client id: f_00007-6-4 loss: 0.831599  [  160/  179]
train() client id: f_00007-7-0 loss: 0.787473  [   32/  179]
train() client id: f_00007-7-1 loss: 0.903515  [   64/  179]
train() client id: f_00007-7-2 loss: 0.879996  [   96/  179]
train() client id: f_00007-7-3 loss: 0.910101  [  128/  179]
train() client id: f_00007-7-4 loss: 0.827392  [  160/  179]
train() client id: f_00007-8-0 loss: 0.831524  [   32/  179]
train() client id: f_00007-8-1 loss: 0.792440  [   64/  179]
train() client id: f_00007-8-2 loss: 0.908270  [   96/  179]
train() client id: f_00007-8-3 loss: 0.950498  [  128/  179]
train() client id: f_00007-8-4 loss: 0.915264  [  160/  179]
train() client id: f_00007-9-0 loss: 0.916892  [   32/  179]
train() client id: f_00007-9-1 loss: 0.869345  [   64/  179]
train() client id: f_00007-9-2 loss: 0.836105  [   96/  179]
train() client id: f_00007-9-3 loss: 0.890086  [  128/  179]
train() client id: f_00007-9-4 loss: 0.922404  [  160/  179]
train() client id: f_00007-10-0 loss: 0.941772  [   32/  179]
train() client id: f_00007-10-1 loss: 0.791717  [   64/  179]
train() client id: f_00007-10-2 loss: 0.855931  [   96/  179]
train() client id: f_00007-10-3 loss: 0.879984  [  128/  179]
train() client id: f_00007-10-4 loss: 0.987320  [  160/  179]
train() client id: f_00007-11-0 loss: 0.871908  [   32/  179]
train() client id: f_00007-11-1 loss: 0.993227  [   64/  179]
train() client id: f_00007-11-2 loss: 0.870565  [   96/  179]
train() client id: f_00007-11-3 loss: 0.905112  [  128/  179]
train() client id: f_00007-11-4 loss: 0.805867  [  160/  179]
train() client id: f_00007-12-0 loss: 0.811901  [   32/  179]
train() client id: f_00007-12-1 loss: 0.891312  [   64/  179]
train() client id: f_00007-12-2 loss: 0.993170  [   96/  179]
train() client id: f_00007-12-3 loss: 0.871328  [  128/  179]
train() client id: f_00007-12-4 loss: 0.918458  [  160/  179]
train() client id: f_00008-0-0 loss: 0.897846  [   32/  130]
train() client id: f_00008-0-1 loss: 0.976443  [   64/  130]
train() client id: f_00008-0-2 loss: 0.955148  [   96/  130]
train() client id: f_00008-0-3 loss: 0.927334  [  128/  130]
train() client id: f_00008-1-0 loss: 0.885873  [   32/  130]
train() client id: f_00008-1-1 loss: 0.950169  [   64/  130]
train() client id: f_00008-1-2 loss: 0.934793  [   96/  130]
train() client id: f_00008-1-3 loss: 0.993201  [  128/  130]
train() client id: f_00008-2-0 loss: 0.899487  [   32/  130]
train() client id: f_00008-2-1 loss: 0.942654  [   64/  130]
train() client id: f_00008-2-2 loss: 1.023690  [   96/  130]
train() client id: f_00008-2-3 loss: 0.910464  [  128/  130]
train() client id: f_00008-3-0 loss: 1.029455  [   32/  130]
train() client id: f_00008-3-1 loss: 0.941951  [   64/  130]
train() client id: f_00008-3-2 loss: 0.911001  [   96/  130]
train() client id: f_00008-3-3 loss: 0.910925  [  128/  130]
train() client id: f_00008-4-0 loss: 1.008010  [   32/  130]
train() client id: f_00008-4-1 loss: 0.981230  [   64/  130]
train() client id: f_00008-4-2 loss: 0.934693  [   96/  130]
train() client id: f_00008-4-3 loss: 0.882525  [  128/  130]
train() client id: f_00008-5-0 loss: 1.003673  [   32/  130]
train() client id: f_00008-5-1 loss: 0.965149  [   64/  130]
train() client id: f_00008-5-2 loss: 0.900441  [   96/  130]
train() client id: f_00008-5-3 loss: 0.980607  [  128/  130]
train() client id: f_00008-6-0 loss: 0.935926  [   32/  130]
train() client id: f_00008-6-1 loss: 1.021642  [   64/  130]
train() client id: f_00008-6-2 loss: 0.991841  [   96/  130]
train() client id: f_00008-6-3 loss: 0.915577  [  128/  130]
train() client id: f_00008-7-0 loss: 1.007049  [   32/  130]
train() client id: f_00008-7-1 loss: 0.961485  [   64/  130]
train() client id: f_00008-7-2 loss: 1.003267  [   96/  130]
train() client id: f_00008-7-3 loss: 0.913459  [  128/  130]
train() client id: f_00008-8-0 loss: 0.933809  [   32/  130]
train() client id: f_00008-8-1 loss: 1.064852  [   64/  130]
train() client id: f_00008-8-2 loss: 0.953309  [   96/  130]
train() client id: f_00008-8-3 loss: 0.982900  [  128/  130]
train() client id: f_00008-9-0 loss: 0.997780  [   32/  130]
train() client id: f_00008-9-1 loss: 0.980389  [   64/  130]
train() client id: f_00008-9-2 loss: 0.926663  [   96/  130]
train() client id: f_00008-9-3 loss: 1.041844  [  128/  130]
train() client id: f_00008-10-0 loss: 1.008013  [   32/  130]
train() client id: f_00008-10-1 loss: 1.052328  [   64/  130]
train() client id: f_00008-10-2 loss: 1.016085  [   96/  130]
train() client id: f_00008-10-3 loss: 0.904573  [  128/  130]
train() client id: f_00008-11-0 loss: 1.012460  [   32/  130]
train() client id: f_00008-11-1 loss: 1.023495  [   64/  130]
train() client id: f_00008-11-2 loss: 0.899047  [   96/  130]
train() client id: f_00008-11-3 loss: 1.012146  [  128/  130]
train() client id: f_00008-12-0 loss: 1.033407  [   32/  130]
train() client id: f_00008-12-1 loss: 0.932629  [   64/  130]
train() client id: f_00008-12-2 loss: 1.091049  [   96/  130]
train() client id: f_00008-12-3 loss: 0.972004  [  128/  130]
train() client id: f_00009-0-0 loss: 0.933865  [   32/  118]
train() client id: f_00009-0-1 loss: 1.002421  [   64/  118]
train() client id: f_00009-0-2 loss: 1.079030  [   96/  118]
train() client id: f_00009-1-0 loss: 1.026714  [   32/  118]
train() client id: f_00009-1-1 loss: 1.044561  [   64/  118]
train() client id: f_00009-1-2 loss: 1.024467  [   96/  118]
train() client id: f_00009-2-0 loss: 1.037547  [   32/  118]
train() client id: f_00009-2-1 loss: 0.985782  [   64/  118]
train() client id: f_00009-2-2 loss: 1.055691  [   96/  118]
train() client id: f_00009-3-0 loss: 0.965109  [   32/  118]
train() client id: f_00009-3-1 loss: 0.968984  [   64/  118]
train() client id: f_00009-3-2 loss: 1.139822  [   96/  118]
train() client id: f_00009-4-0 loss: 0.896102  [   32/  118]
train() client id: f_00009-4-1 loss: 1.150468  [   64/  118]
train() client id: f_00009-4-2 loss: 1.173201  [   96/  118]
train() client id: f_00009-5-0 loss: 1.061794  [   32/  118]
train() client id: f_00009-5-1 loss: 0.983456  [   64/  118]
train() client id: f_00009-5-2 loss: 1.060688  [   96/  118]
train() client id: f_00009-6-0 loss: 1.100600  [   32/  118]
train() client id: f_00009-6-1 loss: 1.080638  [   64/  118]
train() client id: f_00009-6-2 loss: 1.059143  [   96/  118]
train() client id: f_00009-7-0 loss: 0.967320  [   32/  118]
train() client id: f_00009-7-1 loss: 0.999176  [   64/  118]
train() client id: f_00009-7-2 loss: 1.011371  [   96/  118]
train() client id: f_00009-8-0 loss: 0.996333  [   32/  118]
train() client id: f_00009-8-1 loss: 1.095940  [   64/  118]
train() client id: f_00009-8-2 loss: 1.062297  [   96/  118]
train() client id: f_00009-9-0 loss: 1.108214  [   32/  118]
train() client id: f_00009-9-1 loss: 1.089951  [   64/  118]
train() client id: f_00009-9-2 loss: 1.101260  [   96/  118]
train() client id: f_00009-10-0 loss: 1.167881  [   32/  118]
train() client id: f_00009-10-1 loss: 0.968781  [   64/  118]
train() client id: f_00009-10-2 loss: 1.039606  [   96/  118]
train() client id: f_00009-11-0 loss: 0.986772  [   32/  118]
train() client id: f_00009-11-1 loss: 1.103272  [   64/  118]
train() client id: f_00009-11-2 loss: 1.112028  [   96/  118]
train() client id: f_00009-12-0 loss: 1.184010  [   32/  118]
train() client id: f_00009-12-1 loss: 0.921790  [   64/  118]
train() client id: f_00009-12-2 loss: 1.089412  [   96/  118]
At round 0 accuracy: 0.5596816976127321
At round 0 training accuracy: 0.5130784708249497
At round 0 training loss: 1.0255942950512829
update_location
xs = [  1.0943416  -15.79968212  20.00902392  18.81129433 -34.02070377
 -21.04359014   2.55680806  -6.32485185   4.66397685 -17.06087855]
ys = [ 17.5879595   15.55583871   1.32061395  17.54482414   9.35018685
 -17.18584926   2.37501568 -14.17765202  17.56900603   4.00148178]
dists_uav = [101.5407992  102.42858035 101.99071065 103.25543883 106.04166294
 103.62521942 100.06087131 101.19787334 101.63868679 101.52381707]
dists_bs = [236.19434294 225.31573798 261.13798756 249.70973192 217.52016588
 246.24779551 247.64047589 253.45510377 238.88001683 232.77727406]
uav_gains = [9.62490767e-11 9.41770035e-11 9.51910990e-11 9.23028553e-11
 8.63584788e-11 9.14815864e-11 9.98476140e-11 9.70665613e-11
 9.60174958e-11 9.62893323e-11]
bs_gains = [2.50114868e-11 2.85415667e-11 1.88824323e-11 2.14030060e-11
 3.14989018e-11 2.22562249e-11 2.19075354e-11 2.05291596e-11
 2.42320740e-11 2.60531636e-11]
Round 1
-------------------------------
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.86475814 22.70386864 10.69549308  3.82673858 26.17607366 12.62512831
  4.75712203 15.35951406 11.26321529 10.24142851]
obj_prev = 128.51334030366516
eta_min = 2.311289869413348e-09	eta_max = 0.9179068358889176
af = 27.184654844112664	bf = 2.038244896144109	zeta = 29.903120328523933	eta = 0.9090909090909091
af = 27.184654844112664	bf = 2.038244896144109	zeta = 50.92141160980498	eta = 0.5338550913006942
af = 27.184654844112664	bf = 2.038244896144109	zeta = 40.97475341706718	eta = 0.6634488941863763
af = 27.184654844112664	bf = 2.038244896144109	zeta = 39.19821510905128	eta = 0.693517670855259
af = 27.184654844112664	bf = 2.038244896144109	zeta = 39.11235513228073	eta = 0.6950400903288041
af = 27.184654844112664	bf = 2.038244896144109	zeta = 39.11214097403846	eta = 0.6950438960157429
eta = 0.6950438960157429
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [0.03001828 0.06313368 0.0295418  0.01024433 0.07290153 0.03478308
 0.01286498 0.04264501 0.03097124 0.02811235]
ene_total = [3.31978164 6.5061149  3.27744721 1.51901348 7.38134405 3.96296862
 1.74998819 4.47161385 3.59899574 3.32487331]
ti_comp = [0.26460659 0.24673409 0.26447904 0.26412078 0.24854123 0.24180084
 0.26502657 0.26470385 0.24355148 0.24498963]
ti_coms = [0.06346149 0.08133399 0.06358904 0.06394729 0.07952685 0.08626724
 0.06304151 0.06336423 0.0845166  0.08307845]
t_total = [29.9499958 29.9499958 29.9499958 29.9499958 29.9499958 29.9499958
 29.9499958 29.9499958 29.9499958 29.9499958]
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.41454867e-05 2.58347910e-04 2.30360589e-05 9.63220790e-07
 3.92006301e-04 4.49850892e-05 1.89464606e-06 6.91773975e-05
 3.13021490e-05 2.31353594e-05]
ene_total = [0.58155701 0.76609949 0.58262017 0.58387567 0.76180371 0.79165776
 0.57569161 0.58478017 0.77442671 0.76055195]
optimize_network iter = 0 obj = 6.763064260913875
eta = 0.6950438960157429
freqs = [5.67224657e+07 1.27938699e+08 5.58490306e+07 1.93932720e+07
 1.46658825e+08 7.19250640e+07 2.42711055e+07 8.05523076e+07
 6.35825283e+07 5.73745792e+07]
eta_min = 0.6596845988039914	eta_max = 0.6950438960157337
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 0.07315436494122368	eta = 0.9090909090909091
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 22.49041741434538	eta = 0.0029569912778037635
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 2.432729834459089	eta = 0.02733717784291988
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 2.3416402766936963	eta = 0.028400591154114474
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 2.3415910621216036	eta = 0.028401188065745816
eta = 0.028401188065745816
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.36378699e-04 2.52916596e-03 2.25517659e-04 9.42970754e-06
 3.83765052e-03 4.40393562e-04 1.85481443e-05 6.77230634e-04
 3.06440760e-04 2.26489788e-04]
ene_total = [0.18861483 0.30552365 0.1886691  0.18350388 0.33783864 0.25980791
 0.18116974 0.20096826 0.2509534  0.24454163]
ti_comp = [0.30264569 0.28477319 0.30251814 0.30215989 0.28658033 0.27983995
 0.30306567 0.30274295 0.28159058 0.28302874]
ti_coms = [0.06346149 0.08133399 0.06358904 0.06394729 0.07952685 0.08626724
 0.06304151 0.06336423 0.0845166  0.08307845]
t_total = [29.9499958 29.9499958 29.9499958 29.9499958 29.9499958 29.9499958
 29.9499958 29.9499958 29.9499958 29.9499958]
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.76000760e-05 2.90006048e-04 2.63287260e-05 1.10052366e-06
 4.40899051e-04 5.02234785e-05 2.16658346e-06 7.90821828e-05
 3.50155897e-05 2.59210505e-05]
ene_total = [0.52141495 0.68909041 0.52235439 0.52322131 0.68665088 0.70983183
 0.51589861 0.52483088 0.69426636 0.68175732]
optimize_network iter = 1 obj = 6.069316940771649
eta = 0.6596845988039914
freqs = [5.67111680e+07 1.26759149e+08 5.58345224e+07 1.93849279e+07
 1.45447947e+08 7.10682616e+07 2.42711055e+07 8.05399831e+07
 6.28865503e+07 5.67915891e+07]
eta_min = 0.659684598803993	eta_max = 0.6596845988039878
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 0.072022402521563	eta = 0.9090909090909091
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 22.489338538927885	eta = 0.002911375595591902
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 2.427629129214501	eta = 0.026970722420201213
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 2.3378597367303393	eta = 0.028006347153576604
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 2.337812514296872	eta = 0.028006912865265216
eta = 0.028006912865265216
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.37212058e-04 2.49249066e-03 2.26285293e-04 9.45857837e-06
 3.78935809e-03 4.31651518e-04 1.86209531e-05 6.79681002e-04
 3.00945552e-04 2.22781478e-04]
ene_total = [0.18856219 0.30434926 0.18861456 0.18343028 0.3363184  0.25945213
 0.18109834 0.20095692 0.25069421 0.24433622]
ti_comp = [0.30264569 0.28477319 0.30251814 0.30215989 0.28658033 0.27983995
 0.30306567 0.30274295 0.28159058 0.28302874]
ti_coms = [0.06346149 0.08133399 0.06358904 0.06394729 0.07952685 0.08626724
 0.06304151 0.06336423 0.0845166  0.08307845]
t_total = [29.9499958 29.9499958 29.9499958 29.9499958 29.9499958 29.9499958
 29.9499958 29.9499958 29.9499958 29.9499958]
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.76000760e-05 2.90006048e-04 2.63287260e-05 1.10052366e-06
 4.40899051e-04 5.02234785e-05 2.16658346e-06 7.90821828e-05
 3.50155897e-05 2.59210505e-05]
ene_total = [0.52141495 0.68909041 0.52235439 0.52322131 0.68665088 0.70983183
 0.51589861 0.52483088 0.69426636 0.68175732]
optimize_network iter = 2 obj = 6.069316940771676
eta = 0.659684598803993
freqs = [5.67111680e+07 1.26759149e+08 5.58345224e+07 1.93849279e+07
 1.45447947e+08 7.10682616e+07 2.42711055e+07 8.05399831e+07
 6.28865503e+07 5.67915891e+07]
Done!
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.63403224e-05 2.76769267e-04 2.51270008e-05 1.05029232e-06
 4.20775042e-04 4.79311221e-05 2.06769383e-06 7.54726250e-05
 3.34173689e-05 2.47379328e-05]
ene_total = [0.00637249 0.00841017 0.00638403 0.00639578 0.00837346 0.00867465
 0.00630622 0.0064119  0.00848508 0.00833258]
At round 1 energy consumption: 0.07414635829145368
At round 1 eta: 0.659684598803993
At round 1 a_n: 27.49751116231574
At round 1 local rounds: 13.621738685881681
At round 1 global rounds: 81.80663264560978
gradient difference: 0.3534095287322998
train() client id: f_00000-0-0 loss: 1.784742  [   32/  126]
train() client id: f_00000-0-1 loss: 1.894145  [   64/  126]
train() client id: f_00000-0-2 loss: 1.822125  [   96/  126]
train() client id: f_00000-1-0 loss: 1.630972  [   32/  126]
train() client id: f_00000-1-1 loss: 1.736439  [   64/  126]
train() client id: f_00000-1-2 loss: 1.609939  [   96/  126]
train() client id: f_00000-2-0 loss: 1.531524  [   32/  126]
train() client id: f_00000-2-1 loss: 1.354409  [   64/  126]
train() client id: f_00000-2-2 loss: 1.376661  [   96/  126]
train() client id: f_00000-3-0 loss: 1.345213  [   32/  126]
train() client id: f_00000-3-1 loss: 1.301502  [   64/  126]
train() client id: f_00000-3-2 loss: 1.225606  [   96/  126]
train() client id: f_00000-4-0 loss: 1.233379  [   32/  126]
train() client id: f_00000-4-1 loss: 1.283376  [   64/  126]
train() client id: f_00000-4-2 loss: 1.164603  [   96/  126]
train() client id: f_00000-5-0 loss: 1.096669  [   32/  126]
train() client id: f_00000-5-1 loss: 1.140445  [   64/  126]
train() client id: f_00000-5-2 loss: 1.116421  [   96/  126]
train() client id: f_00000-6-0 loss: 1.101159  [   32/  126]
train() client id: f_00000-6-1 loss: 0.949583  [   64/  126]
train() client id: f_00000-6-2 loss: 1.020248  [   96/  126]
train() client id: f_00000-7-0 loss: 0.969286  [   32/  126]
train() client id: f_00000-7-1 loss: 0.976707  [   64/  126]
train() client id: f_00000-7-2 loss: 0.985969  [   96/  126]
train() client id: f_00000-8-0 loss: 0.913765  [   32/  126]
train() client id: f_00000-8-1 loss: 0.951041  [   64/  126]
train() client id: f_00000-8-2 loss: 0.944198  [   96/  126]
train() client id: f_00000-9-0 loss: 0.939475  [   32/  126]
train() client id: f_00000-9-1 loss: 0.864275  [   64/  126]
train() client id: f_00000-9-2 loss: 0.926690  [   96/  126]
train() client id: f_00000-10-0 loss: 0.863856  [   32/  126]
train() client id: f_00000-10-1 loss: 0.890977  [   64/  126]
train() client id: f_00000-10-2 loss: 0.934942  [   96/  126]
train() client id: f_00000-11-0 loss: 0.849531  [   32/  126]
train() client id: f_00000-11-1 loss: 0.950603  [   64/  126]
train() client id: f_00000-11-2 loss: 0.888977  [   96/  126]
train() client id: f_00000-12-0 loss: 0.898382  [   32/  126]
train() client id: f_00000-12-1 loss: 0.760693  [   64/  126]
train() client id: f_00000-12-2 loss: 0.952238  [   96/  126]
train() client id: f_00001-0-0 loss: 0.845083  [   32/  265]
train() client id: f_00001-0-1 loss: 0.862706  [   64/  265]
train() client id: f_00001-0-2 loss: 0.879844  [   96/  265]
train() client id: f_00001-0-3 loss: 0.834293  [  128/  265]
train() client id: f_00001-0-4 loss: 0.834549  [  160/  265]
train() client id: f_00001-0-5 loss: 0.782682  [  192/  265]
train() client id: f_00001-0-6 loss: 0.795488  [  224/  265]
train() client id: f_00001-0-7 loss: 0.811616  [  256/  265]
train() client id: f_00001-1-0 loss: 0.803592  [   32/  265]
train() client id: f_00001-1-1 loss: 0.707085  [   64/  265]
train() client id: f_00001-1-2 loss: 0.735153  [   96/  265]
train() client id: f_00001-1-3 loss: 0.745637  [  128/  265]
train() client id: f_00001-1-4 loss: 0.736911  [  160/  265]
train() client id: f_00001-1-5 loss: 0.686671  [  192/  265]
train() client id: f_00001-1-6 loss: 0.696841  [  224/  265]
train() client id: f_00001-1-7 loss: 0.668664  [  256/  265]
train() client id: f_00001-2-0 loss: 0.743616  [   32/  265]
train() client id: f_00001-2-1 loss: 0.691786  [   64/  265]
train() client id: f_00001-2-2 loss: 0.628767  [   96/  265]
train() client id: f_00001-2-3 loss: 0.595813  [  128/  265]
train() client id: f_00001-2-4 loss: 0.675370  [  160/  265]
train() client id: f_00001-2-5 loss: 0.600101  [  192/  265]
train() client id: f_00001-2-6 loss: 0.633989  [  224/  265]
train() client id: f_00001-2-7 loss: 0.595505  [  256/  265]
train() client id: f_00001-3-0 loss: 0.612733  [   32/  265]
train() client id: f_00001-3-1 loss: 0.639680  [   64/  265]
train() client id: f_00001-3-2 loss: 0.572353  [   96/  265]
train() client id: f_00001-3-3 loss: 0.597731  [  128/  265]
train() client id: f_00001-3-4 loss: 0.620962  [  160/  265]
train() client id: f_00001-3-5 loss: 0.601999  [  192/  265]
train() client id: f_00001-3-6 loss: 0.512526  [  224/  265]
train() client id: f_00001-3-7 loss: 0.573011  [  256/  265]
train() client id: f_00001-4-0 loss: 0.538404  [   32/  265]
train() client id: f_00001-4-1 loss: 0.544997  [   64/  265]
train() client id: f_00001-4-2 loss: 0.613259  [   96/  265]
train() client id: f_00001-4-3 loss: 0.577393  [  128/  265]
train() client id: f_00001-4-4 loss: 0.583634  [  160/  265]
train() client id: f_00001-4-5 loss: 0.511418  [  192/  265]
train() client id: f_00001-4-6 loss: 0.530353  [  224/  265]
train() client id: f_00001-4-7 loss: 0.523601  [  256/  265]
train() client id: f_00001-5-0 loss: 0.502489  [   32/  265]
train() client id: f_00001-5-1 loss: 0.614166  [   64/  265]
train() client id: f_00001-5-2 loss: 0.517100  [   96/  265]
train() client id: f_00001-5-3 loss: 0.528946  [  128/  265]
train() client id: f_00001-5-4 loss: 0.496370  [  160/  265]
train() client id: f_00001-5-5 loss: 0.480907  [  192/  265]
train() client id: f_00001-5-6 loss: 0.425690  [  224/  265]
train() client id: f_00001-5-7 loss: 0.520402  [  256/  265]
train() client id: f_00001-6-0 loss: 0.502198  [   32/  265]
train() client id: f_00001-6-1 loss: 0.477614  [   64/  265]
train() client id: f_00001-6-2 loss: 0.481441  [   96/  265]
train() client id: f_00001-6-3 loss: 0.497801  [  128/  265]
train() client id: f_00001-6-4 loss: 0.425592  [  160/  265]
train() client id: f_00001-6-5 loss: 0.552803  [  192/  265]
train() client id: f_00001-6-6 loss: 0.587483  [  224/  265]
train() client id: f_00001-6-7 loss: 0.484239  [  256/  265]
train() client id: f_00001-7-0 loss: 0.521338  [   32/  265]
train() client id: f_00001-7-1 loss: 0.417582  [   64/  265]
train() client id: f_00001-7-2 loss: 0.582343  [   96/  265]
train() client id: f_00001-7-3 loss: 0.561806  [  128/  265]
train() client id: f_00001-7-4 loss: 0.404797  [  160/  265]
train() client id: f_00001-7-5 loss: 0.413480  [  192/  265]
train() client id: f_00001-7-6 loss: 0.525874  [  224/  265]
train() client id: f_00001-7-7 loss: 0.454586  [  256/  265]
train() client id: f_00001-8-0 loss: 0.428292  [   32/  265]
train() client id: f_00001-8-1 loss: 0.526731  [   64/  265]
train() client id: f_00001-8-2 loss: 0.406956  [   96/  265]
train() client id: f_00001-8-3 loss: 0.449721  [  128/  265]
train() client id: f_00001-8-4 loss: 0.420543  [  160/  265]
train() client id: f_00001-8-5 loss: 0.538890  [  192/  265]
train() client id: f_00001-8-6 loss: 0.487979  [  224/  265]
train() client id: f_00001-8-7 loss: 0.475960  [  256/  265]
train() client id: f_00001-9-0 loss: 0.459240  [   32/  265]
train() client id: f_00001-9-1 loss: 0.380432  [   64/  265]
train() client id: f_00001-9-2 loss: 0.587640  [   96/  265]
train() client id: f_00001-9-3 loss: 0.424569  [  128/  265]
train() client id: f_00001-9-4 loss: 0.523414  [  160/  265]
train() client id: f_00001-9-5 loss: 0.499924  [  192/  265]
train() client id: f_00001-9-6 loss: 0.361591  [  224/  265]
train() client id: f_00001-9-7 loss: 0.427260  [  256/  265]
train() client id: f_00001-10-0 loss: 0.483170  [   32/  265]
train() client id: f_00001-10-1 loss: 0.417697  [   64/  265]
train() client id: f_00001-10-2 loss: 0.455271  [   96/  265]
train() client id: f_00001-10-3 loss: 0.425812  [  128/  265]
train() client id: f_00001-10-4 loss: 0.535302  [  160/  265]
train() client id: f_00001-10-5 loss: 0.413338  [  192/  265]
train() client id: f_00001-10-6 loss: 0.433054  [  224/  265]
train() client id: f_00001-10-7 loss: 0.420228  [  256/  265]
train() client id: f_00001-11-0 loss: 0.463656  [   32/  265]
train() client id: f_00001-11-1 loss: 0.468461  [   64/  265]
train() client id: f_00001-11-2 loss: 0.451548  [   96/  265]
train() client id: f_00001-11-3 loss: 0.423857  [  128/  265]
train() client id: f_00001-11-4 loss: 0.456956  [  160/  265]
train() client id: f_00001-11-5 loss: 0.405501  [  192/  265]
train() client id: f_00001-11-6 loss: 0.431906  [  224/  265]
train() client id: f_00001-11-7 loss: 0.505454  [  256/  265]
train() client id: f_00001-12-0 loss: 0.439899  [   32/  265]
train() client id: f_00001-12-1 loss: 0.421324  [   64/  265]
train() client id: f_00001-12-2 loss: 0.524886  [   96/  265]
train() client id: f_00001-12-3 loss: 0.465463  [  128/  265]
train() client id: f_00001-12-4 loss: 0.465293  [  160/  265]
train() client id: f_00001-12-5 loss: 0.410511  [  192/  265]
train() client id: f_00001-12-6 loss: 0.399070  [  224/  265]
train() client id: f_00001-12-7 loss: 0.443973  [  256/  265]
train() client id: f_00002-0-0 loss: 1.140317  [   32/  124]
train() client id: f_00002-0-1 loss: 1.084161  [   64/  124]
train() client id: f_00002-0-2 loss: 1.038637  [   96/  124]
train() client id: f_00002-1-0 loss: 1.078292  [   32/  124]
train() client id: f_00002-1-1 loss: 1.075215  [   64/  124]
train() client id: f_00002-1-2 loss: 1.006716  [   96/  124]
train() client id: f_00002-2-0 loss: 0.997963  [   32/  124]
train() client id: f_00002-2-1 loss: 1.041662  [   64/  124]
train() client id: f_00002-2-2 loss: 0.984825  [   96/  124]
train() client id: f_00002-3-0 loss: 1.040026  [   32/  124]
train() client id: f_00002-3-1 loss: 0.928884  [   64/  124]
train() client id: f_00002-3-2 loss: 0.980982  [   96/  124]
train() client id: f_00002-4-0 loss: 1.053364  [   32/  124]
train() client id: f_00002-4-1 loss: 0.973093  [   64/  124]
train() client id: f_00002-4-2 loss: 0.967363  [   96/  124]
train() client id: f_00002-5-0 loss: 1.103953  [   32/  124]
train() client id: f_00002-5-1 loss: 0.884789  [   64/  124]
train() client id: f_00002-5-2 loss: 0.964576  [   96/  124]
train() client id: f_00002-6-0 loss: 0.837995  [   32/  124]
train() client id: f_00002-6-1 loss: 0.959019  [   64/  124]
train() client id: f_00002-6-2 loss: 0.977450  [   96/  124]
train() client id: f_00002-7-0 loss: 1.056722  [   32/  124]
train() client id: f_00002-7-1 loss: 0.905172  [   64/  124]
train() client id: f_00002-7-2 loss: 0.879388  [   96/  124]
train() client id: f_00002-8-0 loss: 0.878127  [   32/  124]
train() client id: f_00002-8-1 loss: 0.924505  [   64/  124]
train() client id: f_00002-8-2 loss: 0.848952  [   96/  124]
train() client id: f_00002-9-0 loss: 0.988330  [   32/  124]
train() client id: f_00002-9-1 loss: 0.875207  [   64/  124]
train() client id: f_00002-9-2 loss: 0.906345  [   96/  124]
train() client id: f_00002-10-0 loss: 0.989493  [   32/  124]
train() client id: f_00002-10-1 loss: 0.851900  [   64/  124]
train() client id: f_00002-10-2 loss: 0.864336  [   96/  124]
train() client id: f_00002-11-0 loss: 0.853795  [   32/  124]
train() client id: f_00002-11-1 loss: 0.970254  [   64/  124]
train() client id: f_00002-11-2 loss: 0.898667  [   96/  124]
train() client id: f_00002-12-0 loss: 0.940856  [   32/  124]
train() client id: f_00002-12-1 loss: 0.830506  [   64/  124]
train() client id: f_00002-12-2 loss: 0.962995  [   96/  124]
train() client id: f_00003-0-0 loss: 1.132523  [   32/   43]
train() client id: f_00003-1-0 loss: 1.062270  [   32/   43]
train() client id: f_00003-2-0 loss: 1.085501  [   32/   43]
train() client id: f_00003-3-0 loss: 1.027711  [   32/   43]
train() client id: f_00003-4-0 loss: 0.989375  [   32/   43]
train() client id: f_00003-5-0 loss: 1.049089  [   32/   43]
train() client id: f_00003-6-0 loss: 0.989313  [   32/   43]
train() client id: f_00003-7-0 loss: 0.961648  [   32/   43]
train() client id: f_00003-8-0 loss: 1.076888  [   32/   43]
train() client id: f_00003-9-0 loss: 1.027534  [   32/   43]
train() client id: f_00003-10-0 loss: 1.034387  [   32/   43]
train() client id: f_00003-11-0 loss: 1.053737  [   32/   43]
train() client id: f_00003-12-0 loss: 1.099274  [   32/   43]
train() client id: f_00004-0-0 loss: 0.998977  [   32/  306]
train() client id: f_00004-0-1 loss: 1.078498  [   64/  306]
train() client id: f_00004-0-2 loss: 1.008930  [   96/  306]
train() client id: f_00004-0-3 loss: 0.822387  [  128/  306]
train() client id: f_00004-0-4 loss: 1.120313  [  160/  306]
train() client id: f_00004-0-5 loss: 1.022682  [  192/  306]
train() client id: f_00004-0-6 loss: 1.226280  [  224/  306]
train() client id: f_00004-0-7 loss: 1.081729  [  256/  306]
train() client id: f_00004-0-8 loss: 1.041057  [  288/  306]
train() client id: f_00004-1-0 loss: 0.905332  [   32/  306]
train() client id: f_00004-1-1 loss: 1.182013  [   64/  306]
train() client id: f_00004-1-2 loss: 0.989221  [   96/  306]
train() client id: f_00004-1-3 loss: 0.967941  [  128/  306]
train() client id: f_00004-1-4 loss: 1.191941  [  160/  306]
train() client id: f_00004-1-5 loss: 1.103413  [  192/  306]
train() client id: f_00004-1-6 loss: 1.001769  [  224/  306]
train() client id: f_00004-1-7 loss: 0.909310  [  256/  306]
train() client id: f_00004-1-8 loss: 1.125868  [  288/  306]
train() client id: f_00004-2-0 loss: 1.012544  [   32/  306]
train() client id: f_00004-2-1 loss: 1.081630  [   64/  306]
train() client id: f_00004-2-2 loss: 1.054437  [   96/  306]
train() client id: f_00004-2-3 loss: 0.861143  [  128/  306]
train() client id: f_00004-2-4 loss: 1.020481  [  160/  306]
train() client id: f_00004-2-5 loss: 1.049760  [  192/  306]
train() client id: f_00004-2-6 loss: 0.910016  [  224/  306]
train() client id: f_00004-2-7 loss: 1.116440  [  256/  306]
train() client id: f_00004-2-8 loss: 1.266476  [  288/  306]
train() client id: f_00004-3-0 loss: 1.025194  [   32/  306]
train() client id: f_00004-3-1 loss: 0.980022  [   64/  306]
train() client id: f_00004-3-2 loss: 1.058290  [   96/  306]
train() client id: f_00004-3-3 loss: 1.170256  [  128/  306]
train() client id: f_00004-3-4 loss: 1.011669  [  160/  306]
train() client id: f_00004-3-5 loss: 0.927758  [  192/  306]
train() client id: f_00004-3-6 loss: 1.140820  [  224/  306]
train() client id: f_00004-3-7 loss: 0.866083  [  256/  306]
train() client id: f_00004-3-8 loss: 0.996846  [  288/  306]
train() client id: f_00004-4-0 loss: 0.919696  [   32/  306]
train() client id: f_00004-4-1 loss: 1.225918  [   64/  306]
train() client id: f_00004-4-2 loss: 0.920361  [   96/  306]
train() client id: f_00004-4-3 loss: 1.096405  [  128/  306]
train() client id: f_00004-4-4 loss: 1.068042  [  160/  306]
train() client id: f_00004-4-5 loss: 1.016881  [  192/  306]
train() client id: f_00004-4-6 loss: 1.060663  [  224/  306]
train() client id: f_00004-4-7 loss: 0.814681  [  256/  306]
train() client id: f_00004-4-8 loss: 1.026289  [  288/  306]
train() client id: f_00004-5-0 loss: 0.972910  [   32/  306]
train() client id: f_00004-5-1 loss: 1.046342  [   64/  306]
train() client id: f_00004-5-2 loss: 1.026600  [   96/  306]
train() client id: f_00004-5-3 loss: 1.038969  [  128/  306]
train() client id: f_00004-5-4 loss: 1.107482  [  160/  306]
train() client id: f_00004-5-5 loss: 0.870769  [  192/  306]
train() client id: f_00004-5-6 loss: 1.091332  [  224/  306]
train() client id: f_00004-5-7 loss: 0.989178  [  256/  306]
train() client id: f_00004-5-8 loss: 1.029888  [  288/  306]
train() client id: f_00004-6-0 loss: 1.127483  [   32/  306]
train() client id: f_00004-6-1 loss: 1.076426  [   64/  306]
train() client id: f_00004-6-2 loss: 1.027683  [   96/  306]
train() client id: f_00004-6-3 loss: 0.974844  [  128/  306]
train() client id: f_00004-6-4 loss: 0.967183  [  160/  306]
train() client id: f_00004-6-5 loss: 0.991499  [  192/  306]
train() client id: f_00004-6-6 loss: 1.045877  [  224/  306]
train() client id: f_00004-6-7 loss: 0.983406  [  256/  306]
train() client id: f_00004-6-8 loss: 0.995298  [  288/  306]
train() client id: f_00004-7-0 loss: 0.986486  [   32/  306]
train() client id: f_00004-7-1 loss: 1.025489  [   64/  306]
train() client id: f_00004-7-2 loss: 1.042756  [   96/  306]
train() client id: f_00004-7-3 loss: 1.006474  [  128/  306]
train() client id: f_00004-7-4 loss: 0.981030  [  160/  306]
train() client id: f_00004-7-5 loss: 1.040465  [  192/  306]
train() client id: f_00004-7-6 loss: 1.032485  [  224/  306]
train() client id: f_00004-7-7 loss: 0.966402  [  256/  306]
train() client id: f_00004-7-8 loss: 0.963871  [  288/  306]
train() client id: f_00004-8-0 loss: 0.937210  [   32/  306]
train() client id: f_00004-8-1 loss: 0.948884  [   64/  306]
train() client id: f_00004-8-2 loss: 1.029453  [   96/  306]
train() client id: f_00004-8-3 loss: 0.935884  [  128/  306]
train() client id: f_00004-8-4 loss: 1.032926  [  160/  306]
train() client id: f_00004-8-5 loss: 1.003998  [  192/  306]
train() client id: f_00004-8-6 loss: 1.091087  [  224/  306]
train() client id: f_00004-8-7 loss: 0.999362  [  256/  306]
train() client id: f_00004-8-8 loss: 1.032307  [  288/  306]
train() client id: f_00004-9-0 loss: 1.132327  [   32/  306]
train() client id: f_00004-9-1 loss: 1.020619  [   64/  306]
train() client id: f_00004-9-2 loss: 0.783524  [   96/  306]
train() client id: f_00004-9-3 loss: 0.996339  [  128/  306]
train() client id: f_00004-9-4 loss: 0.961553  [  160/  306]
train() client id: f_00004-9-5 loss: 1.084797  [  192/  306]
train() client id: f_00004-9-6 loss: 1.006572  [  224/  306]
train() client id: f_00004-9-7 loss: 0.960637  [  256/  306]
train() client id: f_00004-9-8 loss: 1.134777  [  288/  306]
train() client id: f_00004-10-0 loss: 0.949442  [   32/  306]
train() client id: f_00004-10-1 loss: 1.056179  [   64/  306]
train() client id: f_00004-10-2 loss: 0.981481  [   96/  306]
train() client id: f_00004-10-3 loss: 1.112048  [  128/  306]
train() client id: f_00004-10-4 loss: 0.951439  [  160/  306]
train() client id: f_00004-10-5 loss: 0.972138  [  192/  306]
train() client id: f_00004-10-6 loss: 0.897362  [  224/  306]
train() client id: f_00004-10-7 loss: 0.986700  [  256/  306]
train() client id: f_00004-10-8 loss: 1.087626  [  288/  306]
train() client id: f_00004-11-0 loss: 0.874884  [   32/  306]
train() client id: f_00004-11-1 loss: 1.158665  [   64/  306]
train() client id: f_00004-11-2 loss: 1.160418  [   96/  306]
train() client id: f_00004-11-3 loss: 0.862740  [  128/  306]
train() client id: f_00004-11-4 loss: 0.966623  [  160/  306]
train() client id: f_00004-11-5 loss: 0.885029  [  192/  306]
train() client id: f_00004-11-6 loss: 1.012302  [  224/  306]
train() client id: f_00004-11-7 loss: 0.988136  [  256/  306]
train() client id: f_00004-11-8 loss: 1.086249  [  288/  306]
train() client id: f_00004-12-0 loss: 0.850820  [   32/  306]
train() client id: f_00004-12-1 loss: 0.983877  [   64/  306]
train() client id: f_00004-12-2 loss: 1.018030  [   96/  306]
train() client id: f_00004-12-3 loss: 0.876828  [  128/  306]
train() client id: f_00004-12-4 loss: 1.008643  [  160/  306]
train() client id: f_00004-12-5 loss: 1.156691  [  192/  306]
train() client id: f_00004-12-6 loss: 0.916844  [  224/  306]
train() client id: f_00004-12-7 loss: 1.119883  [  256/  306]
train() client id: f_00004-12-8 loss: 1.023457  [  288/  306]
train() client id: f_00005-0-0 loss: 0.965775  [   32/  146]
train() client id: f_00005-0-1 loss: 0.971406  [   64/  146]
train() client id: f_00005-0-2 loss: 0.984509  [   96/  146]
train() client id: f_00005-0-3 loss: 0.987075  [  128/  146]
train() client id: f_00005-1-0 loss: 0.928393  [   32/  146]
train() client id: f_00005-1-1 loss: 0.935674  [   64/  146]
train() client id: f_00005-1-2 loss: 0.986537  [   96/  146]
train() client id: f_00005-1-3 loss: 0.899503  [  128/  146]
train() client id: f_00005-2-0 loss: 0.980957  [   32/  146]
train() client id: f_00005-2-1 loss: 0.931508  [   64/  146]
train() client id: f_00005-2-2 loss: 0.865813  [   96/  146]
train() client id: f_00005-2-3 loss: 0.833278  [  128/  146]
train() client id: f_00005-3-0 loss: 0.866706  [   32/  146]
train() client id: f_00005-3-1 loss: 0.861703  [   64/  146]
train() client id: f_00005-3-2 loss: 0.842003  [   96/  146]
train() client id: f_00005-3-3 loss: 0.937210  [  128/  146]
train() client id: f_00005-4-0 loss: 0.906053  [   32/  146]
train() client id: f_00005-4-1 loss: 0.848902  [   64/  146]
train() client id: f_00005-4-2 loss: 0.832629  [   96/  146]
train() client id: f_00005-4-3 loss: 0.922974  [  128/  146]
train() client id: f_00005-5-0 loss: 1.017032  [   32/  146]
train() client id: f_00005-5-1 loss: 0.835958  [   64/  146]
train() client id: f_00005-5-2 loss: 0.764467  [   96/  146]
train() client id: f_00005-5-3 loss: 0.880338  [  128/  146]
train() client id: f_00005-6-0 loss: 0.836248  [   32/  146]
train() client id: f_00005-6-1 loss: 0.816554  [   64/  146]
train() client id: f_00005-6-2 loss: 0.933527  [   96/  146]
train() client id: f_00005-6-3 loss: 0.802336  [  128/  146]
train() client id: f_00005-7-0 loss: 0.805871  [   32/  146]
train() client id: f_00005-7-1 loss: 0.839819  [   64/  146]
train() client id: f_00005-7-2 loss: 0.799491  [   96/  146]
train() client id: f_00005-7-3 loss: 0.900348  [  128/  146]
train() client id: f_00005-8-0 loss: 0.988292  [   32/  146]
train() client id: f_00005-8-1 loss: 0.704464  [   64/  146]
train() client id: f_00005-8-2 loss: 0.865904  [   96/  146]
train() client id: f_00005-8-3 loss: 0.725899  [  128/  146]
train() client id: f_00005-9-0 loss: 0.848894  [   32/  146]
train() client id: f_00005-9-1 loss: 0.782708  [   64/  146]
train() client id: f_00005-9-2 loss: 0.826221  [   96/  146]
train() client id: f_00005-9-3 loss: 0.725598  [  128/  146]
train() client id: f_00005-10-0 loss: 0.727079  [   32/  146]
train() client id: f_00005-10-1 loss: 0.765053  [   64/  146]
train() client id: f_00005-10-2 loss: 0.821194  [   96/  146]
train() client id: f_00005-10-3 loss: 0.961110  [  128/  146]
train() client id: f_00005-11-0 loss: 0.774564  [   32/  146]
train() client id: f_00005-11-1 loss: 0.793784  [   64/  146]
train() client id: f_00005-11-2 loss: 0.804667  [   96/  146]
train() client id: f_00005-11-3 loss: 0.852182  [  128/  146]
train() client id: f_00005-12-0 loss: 0.757418  [   32/  146]
train() client id: f_00005-12-1 loss: 0.837653  [   64/  146]
train() client id: f_00005-12-2 loss: 0.804162  [   96/  146]
train() client id: f_00005-12-3 loss: 0.844793  [  128/  146]
train() client id: f_00006-0-0 loss: 1.075209  [   32/   54]
train() client id: f_00006-1-0 loss: 1.028260  [   32/   54]
train() client id: f_00006-2-0 loss: 1.066211  [   32/   54]
train() client id: f_00006-3-0 loss: 1.017432  [   32/   54]
train() client id: f_00006-4-0 loss: 1.014172  [   32/   54]
train() client id: f_00006-5-0 loss: 1.082789  [   32/   54]
train() client id: f_00006-6-0 loss: 1.036914  [   32/   54]
train() client id: f_00006-7-0 loss: 1.046920  [   32/   54]
train() client id: f_00006-8-0 loss: 1.020741  [   32/   54]
train() client id: f_00006-9-0 loss: 1.025570  [   32/   54]
train() client id: f_00006-10-0 loss: 1.051270  [   32/   54]
train() client id: f_00006-11-0 loss: 1.074561  [   32/   54]
train() client id: f_00006-12-0 loss: 1.060990  [   32/   54]
train() client id: f_00007-0-0 loss: 0.754073  [   32/  179]
train() client id: f_00007-0-1 loss: 0.787759  [   64/  179]
train() client id: f_00007-0-2 loss: 0.854469  [   96/  179]
train() client id: f_00007-0-3 loss: 0.683709  [  128/  179]
train() client id: f_00007-0-4 loss: 0.741544  [  160/  179]
train() client id: f_00007-1-0 loss: 0.722204  [   32/  179]
train() client id: f_00007-1-1 loss: 0.742914  [   64/  179]
train() client id: f_00007-1-2 loss: 0.772201  [   96/  179]
train() client id: f_00007-1-3 loss: 0.710842  [  128/  179]
train() client id: f_00007-1-4 loss: 0.702193  [  160/  179]
train() client id: f_00007-2-0 loss: 0.710152  [   32/  179]
train() client id: f_00007-2-1 loss: 0.662242  [   64/  179]
train() client id: f_00007-2-2 loss: 0.764257  [   96/  179]
train() client id: f_00007-2-3 loss: 0.688930  [  128/  179]
train() client id: f_00007-2-4 loss: 0.671440  [  160/  179]
train() client id: f_00007-3-0 loss: 0.717734  [   32/  179]
train() client id: f_00007-3-1 loss: 0.661505  [   64/  179]
train() client id: f_00007-3-2 loss: 0.601889  [   96/  179]
train() client id: f_00007-3-3 loss: 0.646179  [  128/  179]
train() client id: f_00007-3-4 loss: 0.684966  [  160/  179]
train() client id: f_00007-4-0 loss: 0.598187  [   32/  179]
train() client id: f_00007-4-1 loss: 0.603066  [   64/  179]
train() client id: f_00007-4-2 loss: 0.628939  [   96/  179]
train() client id: f_00007-4-3 loss: 0.708935  [  128/  179]
train() client id: f_00007-4-4 loss: 0.716958  [  160/  179]
train() client id: f_00007-5-0 loss: 0.614394  [   32/  179]
train() client id: f_00007-5-1 loss: 0.630111  [   64/  179]
train() client id: f_00007-5-2 loss: 0.699242  [   96/  179]
train() client id: f_00007-5-3 loss: 0.571414  [  128/  179]
train() client id: f_00007-5-4 loss: 0.649416  [  160/  179]
train() client id: f_00007-6-0 loss: 0.605075  [   32/  179]
train() client id: f_00007-6-1 loss: 0.645212  [   64/  179]
train() client id: f_00007-6-2 loss: 0.695349  [   96/  179]
train() client id: f_00007-6-3 loss: 0.552807  [  128/  179]
train() client id: f_00007-6-4 loss: 0.550316  [  160/  179]
train() client id: f_00007-7-0 loss: 0.616859  [   32/  179]
train() client id: f_00007-7-1 loss: 0.549063  [   64/  179]
train() client id: f_00007-7-2 loss: 0.678749  [   96/  179]
train() client id: f_00007-7-3 loss: 0.595543  [  128/  179]
train() client id: f_00007-7-4 loss: 0.596734  [  160/  179]
train() client id: f_00007-8-0 loss: 0.531503  [   32/  179]
train() client id: f_00007-8-1 loss: 0.625747  [   64/  179]
train() client id: f_00007-8-2 loss: 0.529818  [   96/  179]
train() client id: f_00007-8-3 loss: 0.686596  [  128/  179]
train() client id: f_00007-8-4 loss: 0.690853  [  160/  179]
train() client id: f_00007-9-0 loss: 0.682535  [   32/  179]
train() client id: f_00007-9-1 loss: 0.575177  [   64/  179]
train() client id: f_00007-9-2 loss: 0.568154  [   96/  179]
train() client id: f_00007-9-3 loss: 0.634762  [  128/  179]
train() client id: f_00007-9-4 loss: 0.580497  [  160/  179]
train() client id: f_00007-10-0 loss: 0.596317  [   32/  179]
train() client id: f_00007-10-1 loss: 0.578168  [   64/  179]
train() client id: f_00007-10-2 loss: 0.628371  [   96/  179]
train() client id: f_00007-10-3 loss: 0.712789  [  128/  179]
train() client id: f_00007-10-4 loss: 0.507734  [  160/  179]
train() client id: f_00007-11-0 loss: 0.645978  [   32/  179]
train() client id: f_00007-11-1 loss: 0.487722  [   64/  179]
train() client id: f_00007-11-2 loss: 0.508706  [   96/  179]
train() client id: f_00007-11-3 loss: 0.611786  [  128/  179]
train() client id: f_00007-11-4 loss: 0.641774  [  160/  179]
train() client id: f_00007-12-0 loss: 0.504320  [   32/  179]
train() client id: f_00007-12-1 loss: 0.571179  [   64/  179]
train() client id: f_00007-12-2 loss: 0.595818  [   96/  179]
train() client id: f_00007-12-3 loss: 0.523059  [  128/  179]
train() client id: f_00007-12-4 loss: 0.658891  [  160/  179]
train() client id: f_00008-0-0 loss: 0.877931  [   32/  130]
train() client id: f_00008-0-1 loss: 0.840443  [   64/  130]
train() client id: f_00008-0-2 loss: 0.930394  [   96/  130]
train() client id: f_00008-0-3 loss: 0.862926  [  128/  130]
train() client id: f_00008-1-0 loss: 0.904990  [   32/  130]
train() client id: f_00008-1-1 loss: 0.878609  [   64/  130]
train() client id: f_00008-1-2 loss: 0.855301  [   96/  130]
train() client id: f_00008-1-3 loss: 0.848159  [  128/  130]
train() client id: f_00008-2-0 loss: 0.884562  [   32/  130]
train() client id: f_00008-2-1 loss: 0.922487  [   64/  130]
train() client id: f_00008-2-2 loss: 0.833631  [   96/  130]
train() client id: f_00008-2-3 loss: 0.848021  [  128/  130]
train() client id: f_00008-3-0 loss: 0.851708  [   32/  130]
train() client id: f_00008-3-1 loss: 0.838330  [   64/  130]
train() client id: f_00008-3-2 loss: 0.850117  [   96/  130]
train() client id: f_00008-3-3 loss: 0.921025  [  128/  130]
train() client id: f_00008-4-0 loss: 0.950655  [   32/  130]
train() client id: f_00008-4-1 loss: 0.848096  [   64/  130]
train() client id: f_00008-4-2 loss: 0.821248  [   96/  130]
train() client id: f_00008-4-3 loss: 0.850020  [  128/  130]
train() client id: f_00008-5-0 loss: 0.834705  [   32/  130]
train() client id: f_00008-5-1 loss: 0.910311  [   64/  130]
train() client id: f_00008-5-2 loss: 0.830669  [   96/  130]
train() client id: f_00008-5-3 loss: 0.881885  [  128/  130]
train() client id: f_00008-6-0 loss: 0.835628  [   32/  130]
train() client id: f_00008-6-1 loss: 0.942741  [   64/  130]
train() client id: f_00008-6-2 loss: 0.845400  [   96/  130]
train() client id: f_00008-6-3 loss: 0.825135  [  128/  130]
train() client id: f_00008-7-0 loss: 0.832034  [   32/  130]
train() client id: f_00008-7-1 loss: 0.883860  [   64/  130]
train() client id: f_00008-7-2 loss: 0.883033  [   96/  130]
train() client id: f_00008-7-3 loss: 0.837029  [  128/  130]
train() client id: f_00008-8-0 loss: 0.883960  [   32/  130]
train() client id: f_00008-8-1 loss: 0.885131  [   64/  130]
train() client id: f_00008-8-2 loss: 0.776626  [   96/  130]
train() client id: f_00008-8-3 loss: 0.906976  [  128/  130]
train() client id: f_00008-9-0 loss: 0.851241  [   32/  130]
train() client id: f_00008-9-1 loss: 0.873308  [   64/  130]
train() client id: f_00008-9-2 loss: 0.848945  [   96/  130]
train() client id: f_00008-9-3 loss: 0.896577  [  128/  130]
train() client id: f_00008-10-0 loss: 0.908336  [   32/  130]
train() client id: f_00008-10-1 loss: 0.906947  [   64/  130]
train() client id: f_00008-10-2 loss: 0.775755  [   96/  130]
train() client id: f_00008-10-3 loss: 0.883170  [  128/  130]
train() client id: f_00008-11-0 loss: 0.908963  [   32/  130]
train() client id: f_00008-11-1 loss: 0.832126  [   64/  130]
train() client id: f_00008-11-2 loss: 0.854954  [   96/  130]
train() client id: f_00008-11-3 loss: 0.878005  [  128/  130]
train() client id: f_00008-12-0 loss: 0.827330  [   32/  130]
train() client id: f_00008-12-1 loss: 0.878449  [   64/  130]
train() client id: f_00008-12-2 loss: 0.840948  [   96/  130]
train() client id: f_00008-12-3 loss: 0.909158  [  128/  130]
train() client id: f_00009-0-0 loss: 0.993182  [   32/  118]
train() client id: f_00009-0-1 loss: 1.094898  [   64/  118]
train() client id: f_00009-0-2 loss: 0.932284  [   96/  118]
train() client id: f_00009-1-0 loss: 0.999636  [   32/  118]
train() client id: f_00009-1-1 loss: 0.905348  [   64/  118]
train() client id: f_00009-1-2 loss: 1.039450  [   96/  118]
train() client id: f_00009-2-0 loss: 1.026448  [   32/  118]
train() client id: f_00009-2-1 loss: 0.964059  [   64/  118]
train() client id: f_00009-2-2 loss: 0.977739  [   96/  118]
train() client id: f_00009-3-0 loss: 0.925640  [   32/  118]
train() client id: f_00009-3-1 loss: 0.994265  [   64/  118]
train() client id: f_00009-3-2 loss: 0.880268  [   96/  118]
train() client id: f_00009-4-0 loss: 0.961407  [   32/  118]
train() client id: f_00009-4-1 loss: 0.951701  [   64/  118]
train() client id: f_00009-4-2 loss: 0.855076  [   96/  118]
train() client id: f_00009-5-0 loss: 0.975180  [   32/  118]
train() client id: f_00009-5-1 loss: 0.879012  [   64/  118]
train() client id: f_00009-5-2 loss: 0.828890  [   96/  118]
train() client id: f_00009-6-0 loss: 0.840699  [   32/  118]
train() client id: f_00009-6-1 loss: 1.049454  [   64/  118]
train() client id: f_00009-6-2 loss: 0.901262  [   96/  118]
train() client id: f_00009-7-0 loss: 0.834595  [   32/  118]
train() client id: f_00009-7-1 loss: 0.875032  [   64/  118]
train() client id: f_00009-7-2 loss: 0.874100  [   96/  118]
train() client id: f_00009-8-0 loss: 0.869195  [   32/  118]
train() client id: f_00009-8-1 loss: 0.819654  [   64/  118]
train() client id: f_00009-8-2 loss: 0.918096  [   96/  118]
train() client id: f_00009-9-0 loss: 0.927147  [   32/  118]
train() client id: f_00009-9-1 loss: 0.800572  [   64/  118]
train() client id: f_00009-9-2 loss: 0.940090  [   96/  118]
train() client id: f_00009-10-0 loss: 0.810886  [   32/  118]
train() client id: f_00009-10-1 loss: 0.802909  [   64/  118]
train() client id: f_00009-10-2 loss: 0.888416  [   96/  118]
train() client id: f_00009-11-0 loss: 0.777896  [   32/  118]
train() client id: f_00009-11-1 loss: 0.860265  [   64/  118]
train() client id: f_00009-11-2 loss: 0.791867  [   96/  118]
train() client id: f_00009-12-0 loss: 0.893221  [   32/  118]
train() client id: f_00009-12-1 loss: 0.880888  [   64/  118]
train() client id: f_00009-12-2 loss: 0.749132  [   96/  118]
At round 1 accuracy: 0.6180371352785146
At round 1 training accuracy: 0.5620389000670691
At round 1 training loss: 0.9849614902278094
update_location
xs = [ -3.9056584  -10.79968212  25.00902392  18.81129433 -29.02070377
 -16.04359014   2.55680806  -6.32485185   9.66397685 -12.06087855]
ys = [ 17.5879595   15.55583871   1.32061395  12.54482414   9.35018685
 -17.18584926  -2.62498432  -9.17765202  17.56900603   4.00148178]
dists_uav = [101.60999206 101.77729242 103.08828885 102.52432593 104.54485756
 102.72657981 100.06711653 100.6192479  101.99050162 100.80414995]
dists_bs = [232.490482   228.87591608 264.89269296 252.89227345 220.7936039
 249.40437224 251.15145857 249.74529647 242.66293951 236.19832881]
uav_gains = [9.60853004e-11 9.56909168e-11 9.26774783e-11 9.39572756e-11
 8.94829513e-11 9.34954732e-11 9.98320356e-11 9.84681076e-11
 9.51915868e-11 9.80171753e-11]
bs_gains = [2.61432507e-11 2.73157933e-11 1.81425423e-11 2.06573456e-11
 3.02086918e-11 2.14764606e-11 2.10607635e-11 2.13944731e-11
 2.31891278e-11 2.50103050e-11]
Round 2
-------------------------------
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.73113195 22.42682549 10.56475866  3.77907907 25.85610411 12.47188502
  4.69859503 15.17007359 11.12712296 10.11765294]
obj_prev = 126.94322881666007
eta_min = 1.8621250529904804e-09	eta_max = 0.9180913765363099
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 29.535190418159765	eta = 0.9090909090909091
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 50.41602139462105	eta = 0.5325722332838158
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 40.52128855107027	eta = 0.6626189360582114
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 38.752941123532246	eta = 0.6928551054183996
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 38.66719520504412	eta = 0.6943915369355615
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 38.6669798224976	eta = 0.694395404830551
eta = 0.694395404830551
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [0.0300953  0.06329568 0.0296176  0.01027062 0.07308859 0.03487234
 0.01289799 0.04275444 0.03105071 0.02818449]
ene_total = [3.27908415 6.43447607 3.23987046 1.49764204 7.29869581 3.92072634
 1.72777469 4.41559147 3.56239059 3.2907282 ]
ti_comp = [0.26882441 0.25014105 0.26840555 0.26856527 0.25202169 0.24528326
 0.26926224 0.26910548 0.24689208 0.24842218]
ti_coms = [0.06348111 0.08216448 0.06389997 0.06374025 0.08028384 0.08702226
 0.06304328 0.06320005 0.08541344 0.08388335]
t_total = [29.89999161 29.89999161 29.89999161 29.89999161 29.89999161 29.89999161
 29.89999161 29.89999161 29.89999161 29.89999161]
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.35742980e-05 2.53298284e-04 2.25396212e-05 9.38793768e-07
 3.84196142e-04 4.40541962e-05 1.84967284e-06 6.74494733e-05
 3.06958933e-05 2.26740888e-05]
ene_total = [0.57330776 0.76208585 0.57698348 0.57360274 0.75694218 0.78696767
 0.5674136  0.57472659 0.77128999 0.75680081]
optimize_network iter = 0 obj = 6.700120669142431
eta = 0.694395404830551
freqs = [5.59757626e+07 1.26519974e+08 5.51732252e+07 1.91212720e+07
 1.45004571e+08 7.10858434e+07 2.39506050e+07 7.94380680e+07
 6.28831617e+07 5.67270011e+07]
eta_min = 0.6637595643039359	eta_max = 0.6943954048305468
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 0.07059948298279546	eta = 0.909090909090909
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 22.33326679569504	eta = 0.0028738002708385358
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 2.406693151543204	eta = 0.026667856733220745
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 2.3186248233022884	eta = 0.02768078195366155
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 2.3185795201975767	eta = 0.02768132281299047
eta = 0.02768132281299047
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.31855663e-04 2.49121488e-03 2.21679510e-04 9.23313400e-06
 3.77860887e-03 4.33277584e-04 1.81917241e-05 6.63372559e-04
 3.01897292e-04 2.23002013e-04]
ene_total = [0.18608375 0.30281643 0.18698052 0.18052076 0.33390587 0.25835522
 0.17880307 0.19749234 0.25008995 0.2435316 ]
ti_comp = [0.30213693 0.28345356 0.30171807 0.30187779 0.28533421 0.27859578
 0.30257476 0.302418   0.2802046  0.2817347 ]
ti_coms = [0.06348111 0.08216448 0.06389997 0.06374025 0.08028384 0.08702226
 0.06304328 0.06320005 0.08541344 0.08388335]
t_total = [29.89999161 29.89999161 29.89999161 29.89999161 29.89999161 29.89999161
 29.89999161 29.89999161 29.89999161 29.89999161]
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.64813595e-05 2.79904609e-04 2.53103811e-05 1.05433666e-06
 4.25297547e-04 4.84557995e-05 2.07851021e-06 7.57844495e-05
 3.38154992e-05 2.50150645e-05]
ene_total = [0.52130977 0.69482585 0.52463943 0.52134959 0.69133623 0.71562472
 0.51573361 0.52304321 0.70127064 0.68803794]
optimize_network iter = 1 obj = 6.097170992212053
eta = 0.6637595643039359
freqs = [5.59657273e+07 1.25464071e+08 5.51538439e+07 1.91158103e+07
 1.43920617e+08 7.03288863e+07 2.39506050e+07 7.94329736e+07
 6.22620637e+07 5.62078659e+07]
eta_min = 0.6637595643039447	eta_max = 0.6637595643039276
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 0.06961124159514927	eta = 0.909090909090909
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 22.33232490105157	eta = 0.0028336927384439636
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 2.4022158004882264	eta = 0.026343572834638552
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 2.3153036098319895	eta = 0.027332461555343614
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 2.315259971692997	eta = 0.02733297671898439
eta = 0.02733297671898439
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.32590817e-04 2.45845542e-03 2.22305891e-04 9.26043939e-06
 3.73546925e-03 4.25596504e-04 1.82559506e-05 6.65629235e-04
 2.97007961e-04 2.19712069e-04]
ene_total = [0.18603789 0.30178187 0.18693126 0.18045688 0.33256672 0.25804555
 0.17874085 0.19748541 0.24986216 0.24335138]
ti_comp = [0.30213693 0.28345356 0.30171807 0.30187779 0.28533421 0.27859578
 0.30257476 0.302418   0.2802046  0.2817347 ]
ti_coms = [0.06348111 0.08216448 0.06389997 0.06374025 0.08028384 0.08702226
 0.06304328 0.06320005 0.08541344 0.08388335]
t_total = [29.89999161 29.89999161 29.89999161 29.89999161 29.89999161 29.89999161
 29.89999161 29.89999161 29.89999161 29.89999161]
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.64813595e-05 2.79904609e-04 2.53103811e-05 1.05433666e-06
 4.25297547e-04 4.84557995e-05 2.07851021e-06 7.57844495e-05
 3.38154992e-05 2.50150645e-05]
ene_total = [0.52130977 0.69482585 0.52463943 0.52134959 0.69133623 0.71562472
 0.51573361 0.52304321 0.70127064 0.68803794]
optimize_network iter = 2 obj = 6.09717099221221
eta = 0.6637595643039447
freqs = [5.59657273e+07 1.25464071e+08 5.51538439e+07 1.91158103e+07
 1.43920617e+08 7.03288863e+07 2.39506050e+07 7.94329736e+07
 6.22620637e+07 5.62078659e+07]
Done!
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.56524120e-05 2.71142739e-04 2.45180888e-05 1.02133270e-06
 4.11984434e-04 4.69389849e-05 2.01344649e-06 7.34121646e-05
 3.27569707e-05 2.42320165e-05]
ene_total = [0.00637376 0.00848759 0.00641452 0.00637505 0.00844037 0.00874916
 0.00630634 0.00639342 0.0085741  0.00841257]
At round 2 energy consumption: 0.07452687465895509
At round 2 eta: 0.6637595643039447
At round 2 a_n: 27.154965315346423
At round 2 local rounds: 13.420089837852764
At round 2 global rounds: 81.77931100223806
gradient difference: 0.4040362238883972
train() client id: f_00000-0-0 loss: 1.995123  [   32/  126]
train() client id: f_00000-0-1 loss: 2.169363  [   64/  126]
train() client id: f_00000-0-2 loss: 1.622581  [   96/  126]
train() client id: f_00000-1-0 loss: 1.698465  [   32/  126]
train() client id: f_00000-1-1 loss: 1.739868  [   64/  126]
train() client id: f_00000-1-2 loss: 1.666119  [   96/  126]
train() client id: f_00000-2-0 loss: 1.617900  [   32/  126]
train() client id: f_00000-2-1 loss: 1.441971  [   64/  126]
train() client id: f_00000-2-2 loss: 1.429328  [   96/  126]
train() client id: f_00000-3-0 loss: 1.490403  [   32/  126]
train() client id: f_00000-3-1 loss: 1.241409  [   64/  126]
train() client id: f_00000-3-2 loss: 1.396773  [   96/  126]
train() client id: f_00000-4-0 loss: 1.386272  [   32/  126]
train() client id: f_00000-4-1 loss: 1.173489  [   64/  126]
train() client id: f_00000-4-2 loss: 1.252402  [   96/  126]
train() client id: f_00000-5-0 loss: 1.145010  [   32/  126]
train() client id: f_00000-5-1 loss: 1.234144  [   64/  126]
train() client id: f_00000-5-2 loss: 1.175242  [   96/  126]
train() client id: f_00000-6-0 loss: 1.052833  [   32/  126]
train() client id: f_00000-6-1 loss: 1.166502  [   64/  126]
train() client id: f_00000-6-2 loss: 1.031951  [   96/  126]
train() client id: f_00000-7-0 loss: 1.033122  [   32/  126]
train() client id: f_00000-7-1 loss: 1.026354  [   64/  126]
train() client id: f_00000-7-2 loss: 1.030771  [   96/  126]
train() client id: f_00000-8-0 loss: 1.030533  [   32/  126]
train() client id: f_00000-8-1 loss: 0.943595  [   64/  126]
train() client id: f_00000-8-2 loss: 0.948866  [   96/  126]
train() client id: f_00000-9-0 loss: 0.936148  [   32/  126]
train() client id: f_00000-9-1 loss: 1.014132  [   64/  126]
train() client id: f_00000-9-2 loss: 0.916249  [   96/  126]
train() client id: f_00000-10-0 loss: 0.940755  [   32/  126]
train() client id: f_00000-10-1 loss: 0.931686  [   64/  126]
train() client id: f_00000-10-2 loss: 0.899988  [   96/  126]
train() client id: f_00000-11-0 loss: 0.961148  [   32/  126]
train() client id: f_00000-11-1 loss: 0.917280  [   64/  126]
train() client id: f_00000-11-2 loss: 0.878497  [   96/  126]
train() client id: f_00000-12-0 loss: 0.874162  [   32/  126]
train() client id: f_00000-12-1 loss: 0.915480  [   64/  126]
train() client id: f_00000-12-2 loss: 0.878693  [   96/  126]
train() client id: f_00001-0-0 loss: 0.796210  [   32/  265]
train() client id: f_00001-0-1 loss: 0.819737  [   64/  265]
train() client id: f_00001-0-2 loss: 0.814855  [   96/  265]
train() client id: f_00001-0-3 loss: 0.770858  [  128/  265]
train() client id: f_00001-0-4 loss: 0.741599  [  160/  265]
train() client id: f_00001-0-5 loss: 0.791268  [  192/  265]
train() client id: f_00001-0-6 loss: 0.783457  [  224/  265]
train() client id: f_00001-0-7 loss: 0.722239  [  256/  265]
train() client id: f_00001-1-0 loss: 0.754662  [   32/  265]
train() client id: f_00001-1-1 loss: 0.746401  [   64/  265]
train() client id: f_00001-1-2 loss: 0.742032  [   96/  265]
train() client id: f_00001-1-3 loss: 0.681429  [  128/  265]
train() client id: f_00001-1-4 loss: 0.743640  [  160/  265]
train() client id: f_00001-1-5 loss: 0.663054  [  192/  265]
train() client id: f_00001-1-6 loss: 0.746528  [  224/  265]
train() client id: f_00001-1-7 loss: 0.647586  [  256/  265]
train() client id: f_00001-2-0 loss: 0.663510  [   32/  265]
train() client id: f_00001-2-1 loss: 0.713059  [   64/  265]
train() client id: f_00001-2-2 loss: 0.740415  [   96/  265]
train() client id: f_00001-2-3 loss: 0.613553  [  128/  265]
train() client id: f_00001-2-4 loss: 0.682755  [  160/  265]
train() client id: f_00001-2-5 loss: 0.644534  [  192/  265]
train() client id: f_00001-2-6 loss: 0.726257  [  224/  265]
train() client id: f_00001-2-7 loss: 0.604086  [  256/  265]
train() client id: f_00001-3-0 loss: 0.631050  [   32/  265]
train() client id: f_00001-3-1 loss: 0.653774  [   64/  265]
train() client id: f_00001-3-2 loss: 0.659909  [   96/  265]
train() client id: f_00001-3-3 loss: 0.628954  [  128/  265]
train() client id: f_00001-3-4 loss: 0.595278  [  160/  265]
train() client id: f_00001-3-5 loss: 0.559769  [  192/  265]
train() client id: f_00001-3-6 loss: 0.704514  [  224/  265]
train() client id: f_00001-3-7 loss: 0.715987  [  256/  265]
train() client id: f_00001-4-0 loss: 0.634579  [   32/  265]
train() client id: f_00001-4-1 loss: 0.660241  [   64/  265]
train() client id: f_00001-4-2 loss: 0.611012  [   96/  265]
train() client id: f_00001-4-3 loss: 0.650165  [  128/  265]
train() client id: f_00001-4-4 loss: 0.637439  [  160/  265]
train() client id: f_00001-4-5 loss: 0.639306  [  192/  265]
train() client id: f_00001-4-6 loss: 0.565253  [  224/  265]
train() client id: f_00001-4-7 loss: 0.596748  [  256/  265]
train() client id: f_00001-5-0 loss: 0.650764  [   32/  265]
train() client id: f_00001-5-1 loss: 0.556262  [   64/  265]
train() client id: f_00001-5-2 loss: 0.588517  [   96/  265]
train() client id: f_00001-5-3 loss: 0.547160  [  128/  265]
train() client id: f_00001-5-4 loss: 0.544814  [  160/  265]
train() client id: f_00001-5-5 loss: 0.705533  [  192/  265]
train() client id: f_00001-5-6 loss: 0.622431  [  224/  265]
train() client id: f_00001-5-7 loss: 0.673396  [  256/  265]
train() client id: f_00001-6-0 loss: 0.531356  [   32/  265]
train() client id: f_00001-6-1 loss: 0.652153  [   64/  265]
train() client id: f_00001-6-2 loss: 0.570283  [   96/  265]
train() client id: f_00001-6-3 loss: 0.520067  [  128/  265]
train() client id: f_00001-6-4 loss: 0.660846  [  160/  265]
train() client id: f_00001-6-5 loss: 0.580840  [  192/  265]
train() client id: f_00001-6-6 loss: 0.575176  [  224/  265]
train() client id: f_00001-6-7 loss: 0.699278  [  256/  265]
train() client id: f_00001-7-0 loss: 0.510422  [   32/  265]
train() client id: f_00001-7-1 loss: 0.612550  [   64/  265]
train() client id: f_00001-7-2 loss: 0.612139  [   96/  265]
train() client id: f_00001-7-3 loss: 0.573507  [  128/  265]
train() client id: f_00001-7-4 loss: 0.567048  [  160/  265]
train() client id: f_00001-7-5 loss: 0.562488  [  192/  265]
train() client id: f_00001-7-6 loss: 0.773447  [  224/  265]
train() client id: f_00001-7-7 loss: 0.524082  [  256/  265]
train() client id: f_00001-8-0 loss: 0.553447  [   32/  265]
train() client id: f_00001-8-1 loss: 0.499987  [   64/  265]
train() client id: f_00001-8-2 loss: 0.599085  [   96/  265]
train() client id: f_00001-8-3 loss: 0.661991  [  128/  265]
train() client id: f_00001-8-4 loss: 0.525502  [  160/  265]
train() client id: f_00001-8-5 loss: 0.674138  [  192/  265]
train() client id: f_00001-8-6 loss: 0.576934  [  224/  265]
train() client id: f_00001-8-7 loss: 0.615373  [  256/  265]
train() client id: f_00001-9-0 loss: 0.662069  [   32/  265]
train() client id: f_00001-9-1 loss: 0.603116  [   64/  265]
train() client id: f_00001-9-2 loss: 0.517750  [   96/  265]
train() client id: f_00001-9-3 loss: 0.660051  [  128/  265]
train() client id: f_00001-9-4 loss: 0.607443  [  160/  265]
train() client id: f_00001-9-5 loss: 0.556717  [  192/  265]
train() client id: f_00001-9-6 loss: 0.518455  [  224/  265]
train() client id: f_00001-9-7 loss: 0.509689  [  256/  265]
train() client id: f_00001-10-0 loss: 0.536361  [   32/  265]
train() client id: f_00001-10-1 loss: 0.503022  [   64/  265]
train() client id: f_00001-10-2 loss: 0.565252  [   96/  265]
train() client id: f_00001-10-3 loss: 0.629926  [  128/  265]
train() client id: f_00001-10-4 loss: 0.599317  [  160/  265]
train() client id: f_00001-10-5 loss: 0.542855  [  192/  265]
train() client id: f_00001-10-6 loss: 0.651268  [  224/  265]
train() client id: f_00001-10-7 loss: 0.610602  [  256/  265]
train() client id: f_00001-11-0 loss: 0.486736  [   32/  265]
train() client id: f_00001-11-1 loss: 0.561541  [   64/  265]
train() client id: f_00001-11-2 loss: 0.605809  [   96/  265]
train() client id: f_00001-11-3 loss: 0.580153  [  128/  265]
train() client id: f_00001-11-4 loss: 0.631883  [  160/  265]
train() client id: f_00001-11-5 loss: 0.470560  [  192/  265]
train() client id: f_00001-11-6 loss: 0.740412  [  224/  265]
train() client id: f_00001-11-7 loss: 0.607129  [  256/  265]
train() client id: f_00001-12-0 loss: 0.688239  [   32/  265]
train() client id: f_00001-12-1 loss: 0.475787  [   64/  265]
train() client id: f_00001-12-2 loss: 0.623236  [   96/  265]
train() client id: f_00001-12-3 loss: 0.545749  [  128/  265]
train() client id: f_00001-12-4 loss: 0.565998  [  160/  265]
train() client id: f_00001-12-5 loss: 0.542842  [  192/  265]
train() client id: f_00001-12-6 loss: 0.635321  [  224/  265]
train() client id: f_00001-12-7 loss: 0.603038  [  256/  265]
train() client id: f_00002-0-0 loss: 1.188903  [   32/  124]
train() client id: f_00002-0-1 loss: 1.090900  [   64/  124]
train() client id: f_00002-0-2 loss: 1.108573  [   96/  124]
train() client id: f_00002-1-0 loss: 1.171401  [   32/  124]
train() client id: f_00002-1-1 loss: 1.003593  [   64/  124]
train() client id: f_00002-1-2 loss: 1.092640  [   96/  124]
train() client id: f_00002-2-0 loss: 1.065040  [   32/  124]
train() client id: f_00002-2-1 loss: 1.057129  [   64/  124]
train() client id: f_00002-2-2 loss: 1.025943  [   96/  124]
train() client id: f_00002-3-0 loss: 1.024709  [   32/  124]
train() client id: f_00002-3-1 loss: 1.173989  [   64/  124]
train() client id: f_00002-3-2 loss: 0.965051  [   96/  124]
train() client id: f_00002-4-0 loss: 1.030567  [   32/  124]
train() client id: f_00002-4-1 loss: 1.110422  [   64/  124]
train() client id: f_00002-4-2 loss: 0.918196  [   96/  124]
train() client id: f_00002-5-0 loss: 0.993915  [   32/  124]
train() client id: f_00002-5-1 loss: 0.986144  [   64/  124]
train() client id: f_00002-5-2 loss: 0.925106  [   96/  124]
train() client id: f_00002-6-0 loss: 1.026245  [   32/  124]
train() client id: f_00002-6-1 loss: 0.891444  [   64/  124]
train() client id: f_00002-6-2 loss: 0.963417  [   96/  124]
train() client id: f_00002-7-0 loss: 0.896761  [   32/  124]
train() client id: f_00002-7-1 loss: 0.930416  [   64/  124]
train() client id: f_00002-7-2 loss: 0.945652  [   96/  124]
train() client id: f_00002-8-0 loss: 1.026586  [   32/  124]
train() client id: f_00002-8-1 loss: 0.887147  [   64/  124]
train() client id: f_00002-8-2 loss: 0.900655  [   96/  124]
train() client id: f_00002-9-0 loss: 0.881618  [   32/  124]
train() client id: f_00002-9-1 loss: 0.978992  [   64/  124]
train() client id: f_00002-9-2 loss: 0.890506  [   96/  124]
train() client id: f_00002-10-0 loss: 0.976692  [   32/  124]
train() client id: f_00002-10-1 loss: 0.929421  [   64/  124]
train() client id: f_00002-10-2 loss: 0.828454  [   96/  124]
train() client id: f_00002-11-0 loss: 0.928192  [   32/  124]
train() client id: f_00002-11-1 loss: 0.911423  [   64/  124]
train() client id: f_00002-11-2 loss: 0.870168  [   96/  124]
train() client id: f_00002-12-0 loss: 0.981732  [   32/  124]
train() client id: f_00002-12-1 loss: 0.865437  [   64/  124]
train() client id: f_00002-12-2 loss: 0.907994  [   96/  124]
train() client id: f_00003-0-0 loss: 0.858810  [   32/   43]
train() client id: f_00003-1-0 loss: 0.880965  [   32/   43]
train() client id: f_00003-2-0 loss: 0.951172  [   32/   43]
train() client id: f_00003-3-0 loss: 0.970793  [   32/   43]
train() client id: f_00003-4-0 loss: 0.878237  [   32/   43]
train() client id: f_00003-5-0 loss: 0.897444  [   32/   43]
train() client id: f_00003-6-0 loss: 0.922463  [   32/   43]
train() client id: f_00003-7-0 loss: 0.940660  [   32/   43]
train() client id: f_00003-8-0 loss: 0.922256  [   32/   43]
train() client id: f_00003-9-0 loss: 0.906899  [   32/   43]
train() client id: f_00003-10-0 loss: 0.863441  [   32/   43]
train() client id: f_00003-11-0 loss: 0.878619  [   32/   43]
train() client id: f_00003-12-0 loss: 0.939599  [   32/   43]
train() client id: f_00004-0-0 loss: 0.985553  [   32/  306]
train() client id: f_00004-0-1 loss: 0.781769  [   64/  306]
train() client id: f_00004-0-2 loss: 0.866942  [   96/  306]
train() client id: f_00004-0-3 loss: 0.894322  [  128/  306]
train() client id: f_00004-0-4 loss: 0.898749  [  160/  306]
train() client id: f_00004-0-5 loss: 1.018839  [  192/  306]
train() client id: f_00004-0-6 loss: 0.934282  [  224/  306]
train() client id: f_00004-0-7 loss: 0.857054  [  256/  306]
train() client id: f_00004-0-8 loss: 0.994799  [  288/  306]
train() client id: f_00004-1-0 loss: 1.003109  [   32/  306]
train() client id: f_00004-1-1 loss: 0.884819  [   64/  306]
train() client id: f_00004-1-2 loss: 0.974570  [   96/  306]
train() client id: f_00004-1-3 loss: 0.764505  [  128/  306]
train() client id: f_00004-1-4 loss: 0.945672  [  160/  306]
train() client id: f_00004-1-5 loss: 1.112468  [  192/  306]
train() client id: f_00004-1-6 loss: 0.895405  [  224/  306]
train() client id: f_00004-1-7 loss: 0.820615  [  256/  306]
train() client id: f_00004-1-8 loss: 0.905683  [  288/  306]
train() client id: f_00004-2-0 loss: 1.187557  [   32/  306]
train() client id: f_00004-2-1 loss: 0.861412  [   64/  306]
train() client id: f_00004-2-2 loss: 0.902752  [   96/  306]
train() client id: f_00004-2-3 loss: 0.879014  [  128/  306]
train() client id: f_00004-2-4 loss: 0.903127  [  160/  306]
train() client id: f_00004-2-5 loss: 0.930048  [  192/  306]
train() client id: f_00004-2-6 loss: 0.922875  [  224/  306]
train() client id: f_00004-2-7 loss: 0.729874  [  256/  306]
train() client id: f_00004-2-8 loss: 0.871864  [  288/  306]
train() client id: f_00004-3-0 loss: 0.921232  [   32/  306]
train() client id: f_00004-3-1 loss: 0.845433  [   64/  306]
train() client id: f_00004-3-2 loss: 1.027532  [   96/  306]
train() client id: f_00004-3-3 loss: 0.807785  [  128/  306]
train() client id: f_00004-3-4 loss: 0.913092  [  160/  306]
train() client id: f_00004-3-5 loss: 0.859993  [  192/  306]
train() client id: f_00004-3-6 loss: 0.963750  [  224/  306]
train() client id: f_00004-3-7 loss: 0.868596  [  256/  306]
train() client id: f_00004-3-8 loss: 0.766055  [  288/  306]
train() client id: f_00004-4-0 loss: 0.812374  [   32/  306]
train() client id: f_00004-4-1 loss: 0.902970  [   64/  306]
train() client id: f_00004-4-2 loss: 0.887667  [   96/  306]
train() client id: f_00004-4-3 loss: 0.767835  [  128/  306]
train() client id: f_00004-4-4 loss: 1.001571  [  160/  306]
train() client id: f_00004-4-5 loss: 0.898150  [  192/  306]
train() client id: f_00004-4-6 loss: 0.874567  [  224/  306]
train() client id: f_00004-4-7 loss: 0.811823  [  256/  306]
train() client id: f_00004-4-8 loss: 1.076722  [  288/  306]
train() client id: f_00004-5-0 loss: 0.970977  [   32/  306]
train() client id: f_00004-5-1 loss: 0.979344  [   64/  306]
train() client id: f_00004-5-2 loss: 0.880754  [   96/  306]
train() client id: f_00004-5-3 loss: 0.753804  [  128/  306]
train() client id: f_00004-5-4 loss: 0.858928  [  160/  306]
train() client id: f_00004-5-5 loss: 0.978420  [  192/  306]
train() client id: f_00004-5-6 loss: 0.797795  [  224/  306]
train() client id: f_00004-5-7 loss: 0.918725  [  256/  306]
train() client id: f_00004-5-8 loss: 0.826190  [  288/  306]
train() client id: f_00004-6-0 loss: 0.931854  [   32/  306]
train() client id: f_00004-6-1 loss: 0.917121  [   64/  306]
train() client id: f_00004-6-2 loss: 0.846140  [   96/  306]
train() client id: f_00004-6-3 loss: 0.835089  [  128/  306]
train() client id: f_00004-6-4 loss: 0.876327  [  160/  306]
train() client id: f_00004-6-5 loss: 0.910811  [  192/  306]
train() client id: f_00004-6-6 loss: 0.683902  [  224/  306]
train() client id: f_00004-6-7 loss: 0.893142  [  256/  306]
train() client id: f_00004-6-8 loss: 0.927922  [  288/  306]
train() client id: f_00004-7-0 loss: 0.811559  [   32/  306]
train() client id: f_00004-7-1 loss: 0.751594  [   64/  306]
train() client id: f_00004-7-2 loss: 0.865457  [   96/  306]
train() client id: f_00004-7-3 loss: 0.969440  [  128/  306]
train() client id: f_00004-7-4 loss: 0.754667  [  160/  306]
train() client id: f_00004-7-5 loss: 0.977174  [  192/  306]
train() client id: f_00004-7-6 loss: 1.036407  [  224/  306]
train() client id: f_00004-7-7 loss: 0.904238  [  256/  306]
train() client id: f_00004-7-8 loss: 0.911505  [  288/  306]
train() client id: f_00004-8-0 loss: 0.827823  [   32/  306]
train() client id: f_00004-8-1 loss: 0.799379  [   64/  306]
train() client id: f_00004-8-2 loss: 0.908976  [   96/  306]
train() client id: f_00004-8-3 loss: 0.939699  [  128/  306]
train() client id: f_00004-8-4 loss: 0.873530  [  160/  306]
train() client id: f_00004-8-5 loss: 0.998153  [  192/  306]
train() client id: f_00004-8-6 loss: 0.754749  [  224/  306]
train() client id: f_00004-8-7 loss: 0.980426  [  256/  306]
train() client id: f_00004-8-8 loss: 0.834943  [  288/  306]
train() client id: f_00004-9-0 loss: 0.826549  [   32/  306]
train() client id: f_00004-9-1 loss: 0.873568  [   64/  306]
train() client id: f_00004-9-2 loss: 0.821398  [   96/  306]
train() client id: f_00004-9-3 loss: 0.944545  [  128/  306]
train() client id: f_00004-9-4 loss: 0.786878  [  160/  306]
train() client id: f_00004-9-5 loss: 0.940167  [  192/  306]
train() client id: f_00004-9-6 loss: 0.999774  [  224/  306]
train() client id: f_00004-9-7 loss: 0.917756  [  256/  306]
train() client id: f_00004-9-8 loss: 0.797760  [  288/  306]
train() client id: f_00004-10-0 loss: 0.938801  [   32/  306]
train() client id: f_00004-10-1 loss: 0.972415  [   64/  306]
train() client id: f_00004-10-2 loss: 0.952234  [   96/  306]
train() client id: f_00004-10-3 loss: 0.817334  [  128/  306]
train() client id: f_00004-10-4 loss: 1.027476  [  160/  306]
train() client id: f_00004-10-5 loss: 0.858589  [  192/  306]
train() client id: f_00004-10-6 loss: 0.905437  [  224/  306]
train() client id: f_00004-10-7 loss: 0.680639  [  256/  306]
train() client id: f_00004-10-8 loss: 0.795397  [  288/  306]
train() client id: f_00004-11-0 loss: 0.811706  [   32/  306]
train() client id: f_00004-11-1 loss: 0.976435  [   64/  306]
train() client id: f_00004-11-2 loss: 0.885598  [   96/  306]
train() client id: f_00004-11-3 loss: 0.882421  [  128/  306]
train() client id: f_00004-11-4 loss: 0.920406  [  160/  306]
train() client id: f_00004-11-5 loss: 0.832694  [  192/  306]
train() client id: f_00004-11-6 loss: 1.001876  [  224/  306]
train() client id: f_00004-11-7 loss: 0.832499  [  256/  306]
train() client id: f_00004-11-8 loss: 0.826238  [  288/  306]
train() client id: f_00004-12-0 loss: 0.928290  [   32/  306]
train() client id: f_00004-12-1 loss: 0.952013  [   64/  306]
train() client id: f_00004-12-2 loss: 0.891456  [   96/  306]
train() client id: f_00004-12-3 loss: 0.859985  [  128/  306]
train() client id: f_00004-12-4 loss: 0.893534  [  160/  306]
train() client id: f_00004-12-5 loss: 0.810984  [  192/  306]
train() client id: f_00004-12-6 loss: 0.854726  [  224/  306]
train() client id: f_00004-12-7 loss: 1.037731  [  256/  306]
train() client id: f_00004-12-8 loss: 0.723567  [  288/  306]
train() client id: f_00005-0-0 loss: 0.953498  [   32/  146]
train() client id: f_00005-0-1 loss: 0.871673  [   64/  146]
train() client id: f_00005-0-2 loss: 0.898452  [   96/  146]
train() client id: f_00005-0-3 loss: 0.937887  [  128/  146]
train() client id: f_00005-1-0 loss: 0.896874  [   32/  146]
train() client id: f_00005-1-1 loss: 0.872338  [   64/  146]
train() client id: f_00005-1-2 loss: 0.861941  [   96/  146]
train() client id: f_00005-1-3 loss: 0.942007  [  128/  146]
train() client id: f_00005-2-0 loss: 0.908959  [   32/  146]
train() client id: f_00005-2-1 loss: 0.842933  [   64/  146]
train() client id: f_00005-2-2 loss: 0.814014  [   96/  146]
train() client id: f_00005-2-3 loss: 0.947972  [  128/  146]
train() client id: f_00005-3-0 loss: 0.898494  [   32/  146]
train() client id: f_00005-3-1 loss: 0.883855  [   64/  146]
train() client id: f_00005-3-2 loss: 0.866223  [   96/  146]
train() client id: f_00005-3-3 loss: 0.814458  [  128/  146]
train() client id: f_00005-4-0 loss: 0.780640  [   32/  146]
train() client id: f_00005-4-1 loss: 0.824699  [   64/  146]
train() client id: f_00005-4-2 loss: 0.893532  [   96/  146]
train() client id: f_00005-4-3 loss: 0.909768  [  128/  146]
train() client id: f_00005-5-0 loss: 0.720038  [   32/  146]
train() client id: f_00005-5-1 loss: 0.877914  [   64/  146]
train() client id: f_00005-5-2 loss: 0.905199  [   96/  146]
train() client id: f_00005-5-3 loss: 0.854344  [  128/  146]
train() client id: f_00005-6-0 loss: 0.876921  [   32/  146]
train() client id: f_00005-6-1 loss: 0.878832  [   64/  146]
train() client id: f_00005-6-2 loss: 0.779093  [   96/  146]
train() client id: f_00005-6-3 loss: 0.732225  [  128/  146]
train() client id: f_00005-7-0 loss: 0.883352  [   32/  146]
train() client id: f_00005-7-1 loss: 0.773217  [   64/  146]
train() client id: f_00005-7-2 loss: 0.782554  [   96/  146]
train() client id: f_00005-7-3 loss: 0.881436  [  128/  146]
train() client id: f_00005-8-0 loss: 0.768378  [   32/  146]
train() client id: f_00005-8-1 loss: 0.840849  [   64/  146]
train() client id: f_00005-8-2 loss: 0.835641  [   96/  146]
train() client id: f_00005-8-3 loss: 0.895427  [  128/  146]
train() client id: f_00005-9-0 loss: 0.682529  [   32/  146]
train() client id: f_00005-9-1 loss: 0.903379  [   64/  146]
train() client id: f_00005-9-2 loss: 0.772345  [   96/  146]
train() client id: f_00005-9-3 loss: 0.952793  [  128/  146]
train() client id: f_00005-10-0 loss: 0.848482  [   32/  146]
train() client id: f_00005-10-1 loss: 0.839843  [   64/  146]
train() client id: f_00005-10-2 loss: 0.905843  [   96/  146]
train() client id: f_00005-10-3 loss: 0.621351  [  128/  146]
train() client id: f_00005-11-0 loss: 0.734883  [   32/  146]
train() client id: f_00005-11-1 loss: 0.660954  [   64/  146]
train() client id: f_00005-11-2 loss: 0.795937  [   96/  146]
train() client id: f_00005-11-3 loss: 0.965548  [  128/  146]
train() client id: f_00005-12-0 loss: 0.724567  [   32/  146]
train() client id: f_00005-12-1 loss: 0.875508  [   64/  146]
train() client id: f_00005-12-2 loss: 0.783212  [   96/  146]
train() client id: f_00005-12-3 loss: 0.885108  [  128/  146]
train() client id: f_00006-0-0 loss: 1.032281  [   32/   54]
train() client id: f_00006-1-0 loss: 1.021681  [   32/   54]
train() client id: f_00006-2-0 loss: 0.991913  [   32/   54]
train() client id: f_00006-3-0 loss: 1.029612  [   32/   54]
train() client id: f_00006-4-0 loss: 1.036657  [   32/   54]
train() client id: f_00006-5-0 loss: 1.025757  [   32/   54]
train() client id: f_00006-6-0 loss: 1.036554  [   32/   54]
train() client id: f_00006-7-0 loss: 1.028897  [   32/   54]
train() client id: f_00006-8-0 loss: 0.986778  [   32/   54]
train() client id: f_00006-9-0 loss: 1.024639  [   32/   54]
train() client id: f_00006-10-0 loss: 1.061628  [   32/   54]
train() client id: f_00006-11-0 loss: 1.033501  [   32/   54]
train() client id: f_00006-12-0 loss: 1.032469  [   32/   54]
train() client id: f_00007-0-0 loss: 0.761013  [   32/  179]
train() client id: f_00007-0-1 loss: 0.656232  [   64/  179]
train() client id: f_00007-0-2 loss: 0.713879  [   96/  179]
train() client id: f_00007-0-3 loss: 0.729904  [  128/  179]
train() client id: f_00007-0-4 loss: 0.646910  [  160/  179]
train() client id: f_00007-1-0 loss: 0.731142  [   32/  179]
train() client id: f_00007-1-1 loss: 0.690582  [   64/  179]
train() client id: f_00007-1-2 loss: 0.676088  [   96/  179]
train() client id: f_00007-1-3 loss: 0.728844  [  128/  179]
train() client id: f_00007-1-4 loss: 0.598435  [  160/  179]
train() client id: f_00007-2-0 loss: 0.602466  [   32/  179]
train() client id: f_00007-2-1 loss: 0.590553  [   64/  179]
train() client id: f_00007-2-2 loss: 0.749069  [   96/  179]
train() client id: f_00007-2-3 loss: 0.748913  [  128/  179]
train() client id: f_00007-2-4 loss: 0.596711  [  160/  179]
train() client id: f_00007-3-0 loss: 0.572613  [   32/  179]
train() client id: f_00007-3-1 loss: 0.603456  [   64/  179]
train() client id: f_00007-3-2 loss: 0.689842  [   96/  179]
train() client id: f_00007-3-3 loss: 0.568032  [  128/  179]
train() client id: f_00007-3-4 loss: 0.658290  [  160/  179]
train() client id: f_00007-4-0 loss: 0.582804  [   32/  179]
train() client id: f_00007-4-1 loss: 0.593253  [   64/  179]
train() client id: f_00007-4-2 loss: 0.546235  [   96/  179]
train() client id: f_00007-4-3 loss: 0.640392  [  128/  179]
train() client id: f_00007-4-4 loss: 0.635814  [  160/  179]
train() client id: f_00007-5-0 loss: 0.644181  [   32/  179]
train() client id: f_00007-5-1 loss: 0.581919  [   64/  179]
train() client id: f_00007-5-2 loss: 0.512402  [   96/  179]
train() client id: f_00007-5-3 loss: 0.630942  [  128/  179]
train() client id: f_00007-5-4 loss: 0.618130  [  160/  179]
train() client id: f_00007-6-0 loss: 0.574874  [   32/  179]
train() client id: f_00007-6-1 loss: 0.571114  [   64/  179]
train() client id: f_00007-6-2 loss: 0.657970  [   96/  179]
train() client id: f_00007-6-3 loss: 0.505912  [  128/  179]
train() client id: f_00007-6-4 loss: 0.687362  [  160/  179]
train() client id: f_00007-7-0 loss: 0.569398  [   32/  179]
train() client id: f_00007-7-1 loss: 0.545309  [   64/  179]
train() client id: f_00007-7-2 loss: 0.628024  [   96/  179]
train() client id: f_00007-7-3 loss: 0.568516  [  128/  179]
train() client id: f_00007-7-4 loss: 0.599806  [  160/  179]
train() client id: f_00007-8-0 loss: 0.507404  [   32/  179]
train() client id: f_00007-8-1 loss: 0.614287  [   64/  179]
train() client id: f_00007-8-2 loss: 0.526717  [   96/  179]
train() client id: f_00007-8-3 loss: 0.650171  [  128/  179]
train() client id: f_00007-8-4 loss: 0.558965  [  160/  179]
train() client id: f_00007-9-0 loss: 0.481750  [   32/  179]
train() client id: f_00007-9-1 loss: 0.497075  [   64/  179]
train() client id: f_00007-9-2 loss: 0.549891  [   96/  179]
train() client id: f_00007-9-3 loss: 0.705311  [  128/  179]
train() client id: f_00007-9-4 loss: 0.564866  [  160/  179]
train() client id: f_00007-10-0 loss: 0.675328  [   32/  179]
train() client id: f_00007-10-1 loss: 0.494315  [   64/  179]
train() client id: f_00007-10-2 loss: 0.532781  [   96/  179]
train() client id: f_00007-10-3 loss: 0.484281  [  128/  179]
train() client id: f_00007-10-4 loss: 0.571865  [  160/  179]
train() client id: f_00007-11-0 loss: 0.545435  [   32/  179]
train() client id: f_00007-11-1 loss: 0.715898  [   64/  179]
train() client id: f_00007-11-2 loss: 0.533459  [   96/  179]
train() client id: f_00007-11-3 loss: 0.492357  [  128/  179]
train() client id: f_00007-11-4 loss: 0.477884  [  160/  179]
train() client id: f_00007-12-0 loss: 0.593123  [   32/  179]
train() client id: f_00007-12-1 loss: 0.481655  [   64/  179]
train() client id: f_00007-12-2 loss: 0.521719  [   96/  179]
train() client id: f_00007-12-3 loss: 0.667078  [  128/  179]
train() client id: f_00007-12-4 loss: 0.524958  [  160/  179]
train() client id: f_00008-0-0 loss: 0.877091  [   32/  130]
train() client id: f_00008-0-1 loss: 0.879670  [   64/  130]
train() client id: f_00008-0-2 loss: 0.849411  [   96/  130]
train() client id: f_00008-0-3 loss: 0.880368  [  128/  130]
train() client id: f_00008-1-0 loss: 0.844697  [   32/  130]
train() client id: f_00008-1-1 loss: 0.839084  [   64/  130]
train() client id: f_00008-1-2 loss: 0.825544  [   96/  130]
train() client id: f_00008-1-3 loss: 0.926511  [  128/  130]
train() client id: f_00008-2-0 loss: 0.872428  [   32/  130]
train() client id: f_00008-2-1 loss: 0.897051  [   64/  130]
train() client id: f_00008-2-2 loss: 0.810008  [   96/  130]
train() client id: f_00008-2-3 loss: 0.870186  [  128/  130]
train() client id: f_00008-3-0 loss: 0.862444  [   32/  130]
train() client id: f_00008-3-1 loss: 0.861929  [   64/  130]
train() client id: f_00008-3-2 loss: 0.902330  [   96/  130]
train() client id: f_00008-3-3 loss: 0.809905  [  128/  130]
train() client id: f_00008-4-0 loss: 0.876574  [   32/  130]
train() client id: f_00008-4-1 loss: 0.864353  [   64/  130]
train() client id: f_00008-4-2 loss: 0.783414  [   96/  130]
train() client id: f_00008-4-3 loss: 0.895824  [  128/  130]
train() client id: f_00008-5-0 loss: 0.866393  [   32/  130]
train() client id: f_00008-5-1 loss: 0.754174  [   64/  130]
train() client id: f_00008-5-2 loss: 0.974104  [   96/  130]
train() client id: f_00008-5-3 loss: 0.797028  [  128/  130]
train() client id: f_00008-6-0 loss: 0.821858  [   32/  130]
train() client id: f_00008-6-1 loss: 0.794361  [   64/  130]
train() client id: f_00008-6-2 loss: 0.944908  [   96/  130]
train() client id: f_00008-6-3 loss: 0.842603  [  128/  130]
train() client id: f_00008-7-0 loss: 0.849863  [   32/  130]
train() client id: f_00008-7-1 loss: 0.937916  [   64/  130]
train() client id: f_00008-7-2 loss: 0.821569  [   96/  130]
train() client id: f_00008-7-3 loss: 0.789485  [  128/  130]
train() client id: f_00008-8-0 loss: 0.913348  [   32/  130]
train() client id: f_00008-8-1 loss: 0.803379  [   64/  130]
train() client id: f_00008-8-2 loss: 0.859850  [   96/  130]
train() client id: f_00008-8-3 loss: 0.796993  [  128/  130]
train() client id: f_00008-9-0 loss: 0.808523  [   32/  130]
train() client id: f_00008-9-1 loss: 0.892712  [   64/  130]
train() client id: f_00008-9-2 loss: 0.807062  [   96/  130]
train() client id: f_00008-9-3 loss: 0.861135  [  128/  130]
train() client id: f_00008-10-0 loss: 0.784502  [   32/  130]
train() client id: f_00008-10-1 loss: 0.910790  [   64/  130]
train() client id: f_00008-10-2 loss: 0.882827  [   96/  130]
train() client id: f_00008-10-3 loss: 0.766950  [  128/  130]
train() client id: f_00008-11-0 loss: 0.852269  [   32/  130]
train() client id: f_00008-11-1 loss: 0.922696  [   64/  130]
train() client id: f_00008-11-2 loss: 0.778396  [   96/  130]
train() client id: f_00008-11-3 loss: 0.829883  [  128/  130]
train() client id: f_00008-12-0 loss: 0.764790  [   32/  130]
train() client id: f_00008-12-1 loss: 0.816823  [   64/  130]
train() client id: f_00008-12-2 loss: 0.937309  [   96/  130]
train() client id: f_00008-12-3 loss: 0.852237  [  128/  130]
train() client id: f_00009-0-0 loss: 1.119370  [   32/  118]
train() client id: f_00009-0-1 loss: 1.104561  [   64/  118]
train() client id: f_00009-0-2 loss: 1.054054  [   96/  118]
train() client id: f_00009-1-0 loss: 1.099258  [   32/  118]
train() client id: f_00009-1-1 loss: 1.004470  [   64/  118]
train() client id: f_00009-1-2 loss: 1.056773  [   96/  118]
train() client id: f_00009-2-0 loss: 1.035558  [   32/  118]
train() client id: f_00009-2-1 loss: 1.080428  [   64/  118]
train() client id: f_00009-2-2 loss: 1.026529  [   96/  118]
train() client id: f_00009-3-0 loss: 1.063722  [   32/  118]
train() client id: f_00009-3-1 loss: 1.020631  [   64/  118]
train() client id: f_00009-3-2 loss: 1.011583  [   96/  118]
train() client id: f_00009-4-0 loss: 1.001096  [   32/  118]
train() client id: f_00009-4-1 loss: 1.057541  [   64/  118]
train() client id: f_00009-4-2 loss: 1.049533  [   96/  118]
train() client id: f_00009-5-0 loss: 1.035617  [   32/  118]
train() client id: f_00009-5-1 loss: 1.067852  [   64/  118]
train() client id: f_00009-5-2 loss: 0.950855  [   96/  118]
train() client id: f_00009-6-0 loss: 0.967663  [   32/  118]
train() client id: f_00009-6-1 loss: 1.087423  [   64/  118]
train() client id: f_00009-6-2 loss: 0.996053  [   96/  118]
train() client id: f_00009-7-0 loss: 1.092119  [   32/  118]
train() client id: f_00009-7-1 loss: 0.959719  [   64/  118]
train() client id: f_00009-7-2 loss: 0.938965  [   96/  118]
train() client id: f_00009-8-0 loss: 0.947631  [   32/  118]
train() client id: f_00009-8-1 loss: 0.969819  [   64/  118]
train() client id: f_00009-8-2 loss: 1.025148  [   96/  118]
train() client id: f_00009-9-0 loss: 1.040009  [   32/  118]
train() client id: f_00009-9-1 loss: 0.932534  [   64/  118]
train() client id: f_00009-9-2 loss: 1.138384  [   96/  118]
train() client id: f_00009-10-0 loss: 1.066728  [   32/  118]
train() client id: f_00009-10-1 loss: 1.040159  [   64/  118]
train() client id: f_00009-10-2 loss: 0.923634  [   96/  118]
train() client id: f_00009-11-0 loss: 1.026677  [   32/  118]
train() client id: f_00009-11-1 loss: 0.956350  [   64/  118]
train() client id: f_00009-11-2 loss: 1.025497  [   96/  118]
train() client id: f_00009-12-0 loss: 0.994923  [   32/  118]
train() client id: f_00009-12-1 loss: 0.979084  [   64/  118]
train() client id: f_00009-12-2 loss: 1.021951  [   96/  118]
At round 2 accuracy: 0.6312997347480106
At round 2 training accuracy: 0.574111334674715
At round 2 training loss: 0.9437848447417484
update_location
xs = [ -3.9056584   -5.79968212  30.00902392  18.81129433 -24.02070377
 -11.04359014   2.55680806  -6.32485185  14.66397685  -7.06087855]
ys = [ 22.5879595   15.55583871   1.32061395   7.54482414   9.35018685
 -17.18584926   2.37501568  -4.17765202  17.56900603   4.00148178]
dists_uav = [102.59371366 101.36873498 104.41401026 102.0332748  103.26867968
 102.0652453  100.06087131 100.28687116 102.58509731 100.32879877]
dists_bs = [229.13468488 232.48911402 268.68797708 256.13288296 224.13078434
 252.62047619 247.64047589 246.08115854 246.48923299 239.67486674]
uav_gains = [9.37984848e-11 9.66580426e-11 8.97635667e-11 9.50918527e-11
 9.22732701e-11 9.50174026e-11 9.98476140e-11 9.92860273e-11
 9.38181825e-11 9.91823280e-11]
bs_gains = [2.72295051e-11 2.61436814e-11 1.74340799e-11 1.99338479e-11
 2.89660886e-11 2.07196372e-11 2.19075354e-11 2.22984497e-11
 2.21952385e-11 2.40077301e-11]
Round 3
-------------------------------
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.59821531 22.14976829 10.43417884  3.73162013 25.53613015 12.31863725
  4.64005817 14.98083601 10.99100861  9.99386679]
obj_prev = 125.37431955255585
eta_min = 1.4920442712219943e-09	eta_max = 0.9182779578592448
af = 26.51569137072327	bf = 2.011304995713791	zeta = 29.1672605077956	eta = 0.9090909090909091
af = 26.51569137072327	bf = 2.011304995713791	zeta = 49.923723386827525	eta = 0.531124074325744
af = 26.51569137072327	bf = 2.011304995713791	zeta = 40.07321095605609	eta = 0.6616812263883753
af = 26.51569137072327	bf = 2.011304995713791	zeta = 38.31157890911957	eta = 0.692106462999664
af = 26.51569137072327	bf = 2.011304995713791	zeta = 38.22584319738169	eta = 0.6936587699009733
af = 26.51569137072327	bf = 2.011304995713791	zeta = 38.22562613755516	eta = 0.6936627087626083
eta = 0.6936627087626083
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [0.03018242 0.0634789  0.02970333 0.01030035 0.07330016 0.03497328
 0.01293532 0.0428782  0.03114059 0.02826607]
ene_total = [3.24068386 6.36286468 3.20278272 1.47684991 7.21615316 3.87841894
 1.70546401 4.36028936 3.52564504 3.25647446]
ti_comp = [0.27298014 0.25372925 0.27246494 0.27313894 0.25568174 0.24894535
 0.27369854 0.27363436 0.25041517 0.25203535]
ti_coms = [0.06375991 0.0830108  0.06427511 0.0636011  0.08105831 0.0877947
 0.06304151 0.06310569 0.08632488 0.08470469]
t_total = [29.84998741 29.84998741 29.84998741 29.84998741 29.84998741 29.84998741
 29.84998741 29.84998741 29.84998741 29.84998741]
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.30610989e-05 2.48328765e-04 2.20634935e-05 9.15521600e-07
 3.76526319e-04 4.31400861e-05 1.80578817e-06 6.58033963e-05
 3.00981129e-05 2.22204868e-05]
ene_total = [0.56723758 0.75785384 0.57171611 0.56386684 0.75191015 0.7820715
 0.55898529 0.56522719 0.76788634 0.75282607]
optimize_network iter = 0 obj = 6.639580919785389
eta = 0.6936627087626083
freqs = [5.52831753e+07 1.25091797e+08 5.45085400e+07 1.88555117e+07
 1.43342577e+08 7.02428831e+07 2.36306014e+07 7.83494392e+07
 6.21779230e+07 5.60756135e+07]
eta_min = 0.6679443667432773	eta_max = 0.6936627087626027
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 0.06810536589768883	eta = 0.9090909090909091
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 22.189266299545174	eta = 0.002790266616394072
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 2.3821060289713065	eta = 0.025991273371083312
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 2.2969923954010767	eta = 0.026954363942109684
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 2.2969507848071644	eta = 0.02695485223602507
eta = 0.02695485223602507
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.27830406e-04 2.45334550e-03 2.17974637e-04 9.04482732e-06
 3.71986367e-03 4.26199261e-04 1.78401495e-05 6.50099743e-04
 2.97352060e-04 2.19525641e-04]
ene_total = [0.18429415 0.3001259  0.18545689 0.17774529 0.33002202 0.25690445
 0.17642907 0.19425277 0.24920683 0.24251343]
ti_comp = [0.30125093 0.28200003 0.30073572 0.30140973 0.28395252 0.27721613
 0.30196932 0.30190514 0.27868595 0.28030614]
ti_coms = [0.06375991 0.0830108  0.06427511 0.0636011  0.08105831 0.0877947
 0.06304151 0.06310569 0.08632488 0.08470469]
t_total = [29.84998741 29.84998741 29.84998741 29.84998741 29.84998741 29.84998741
 29.84998741 29.84998741 29.84998741 29.84998741]
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.54305861e-05 2.69985736e-04 2.43218458e-05 1.00970000e-06
 4.09991014e-04 4.67221780e-05 1.99231109e-06 7.25971523e-05
 3.26363375e-05 2.41258334e-05]
ene_total = [0.52349772 0.70092769 0.52762029 0.52020198 0.69640997 0.72179151
 0.51570607 0.52200485 0.70861965 0.69467407]
optimize_network iter = 1 obj = 6.13145379564688
eta = 0.6679443667432773
freqs = [5.52695580e+07 1.24176825e+08 5.44854450e+07 1.88518958e+07
 1.42403130e+08 6.95950244e+07 2.36306014e+07 7.83477189e+07
 6.16413456e+07 5.56279733e+07]
eta_min = 0.6679443667432772	eta_max = 0.6679443667432751
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 0.06726955400122223	eta = 0.9090909090909091
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 22.188469685723824	eta = 0.002756122475650405
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 2.378298126804526	eta = 0.025713403762074868
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 2.294165378733591	eta = 0.026656378205336263
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 2.2941251141107752	eta = 0.02665684605646063
eta = 0.02665684605646063
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.28418763e-04 2.42502502e-03 2.18460004e-04 9.06917459e-06
 3.68255925e-03 4.19660878e-04 1.78950352e-05 6.52071157e-04
 2.93141173e-04 2.16699410e-04]
ene_total = [0.18425414 0.2992439  0.18541365 0.17769155 0.32888024 0.25664338
 0.17637659 0.19424829 0.24901305 0.24236033]
ti_comp = [0.30125093 0.28200003 0.30073572 0.30140973 0.28395252 0.27721613
 0.30196932 0.30190514 0.27868595 0.28030614]
ti_coms = [0.06375991 0.0830108  0.06427511 0.0636011  0.08105831 0.0877947
 0.06304151 0.06310569 0.08632488 0.08470469]
t_total = [29.84998741 29.84998741 29.84998741 29.84998741 29.84998741 29.84998741
 29.84998741 29.84998741 29.84998741 29.84998741]
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.54305861e-05 2.69985736e-04 2.43218458e-05 1.00970000e-06
 4.09991014e-04 4.67221780e-05 1.99231109e-06 7.25971523e-05
 3.26363375e-05 2.41258334e-05]
ene_total = [0.52349772 0.70092769 0.52762029 0.52020198 0.69640997 0.72179151
 0.51570607 0.52200485 0.70861965 0.69467407]
optimize_network iter = 2 obj = 6.1314537956468795
eta = 0.6679443667432772
freqs = [5.52695580e+07 1.24176825e+08 5.44854450e+07 1.88518958e+07
 1.42403130e+08 6.95950244e+07 2.36306014e+07 7.83477189e+07
 6.16413456e+07 5.56279733e+07]
Done!
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.50181899e-05 2.65607500e-04 2.39274295e-05 9.93326158e-07
 4.03342377e-04 4.59645058e-05 1.96000269e-06 7.14198774e-05
 3.21070889e-05 2.37345958e-05]
ene_total = [0.00640101 0.00856669 0.00645144 0.0063611  0.00850917 0.00882543
 0.00630611 0.00638199 0.00866459 0.0084942 ]
At round 3 energy consumption: 0.07496174437238437
At round 3 eta: 0.6679443667432772
At round 3 a_n: 26.812419468377104
At round 3 local rounds: 13.214290123550223
At round 3 global rounds: 81.77836059884595
gradient difference: 0.42925339937210083
train() client id: f_00000-0-0 loss: 1.621903  [   32/  126]
train() client id: f_00000-0-1 loss: 1.641862  [   64/  126]
train() client id: f_00000-0-2 loss: 1.834103  [   96/  126]
train() client id: f_00000-1-0 loss: 1.515015  [   32/  126]
train() client id: f_00000-1-1 loss: 1.468307  [   64/  126]
train() client id: f_00000-1-2 loss: 1.399493  [   96/  126]
train() client id: f_00000-2-0 loss: 1.362345  [   32/  126]
train() client id: f_00000-2-1 loss: 1.420198  [   64/  126]
train() client id: f_00000-2-2 loss: 1.390805  [   96/  126]
train() client id: f_00000-3-0 loss: 1.203319  [   32/  126]
train() client id: f_00000-3-1 loss: 1.434057  [   64/  126]
train() client id: f_00000-3-2 loss: 1.147332  [   96/  126]
train() client id: f_00000-4-0 loss: 1.215340  [   32/  126]
train() client id: f_00000-4-1 loss: 1.144370  [   64/  126]
train() client id: f_00000-4-2 loss: 1.096934  [   96/  126]
train() client id: f_00000-5-0 loss: 1.070433  [   32/  126]
train() client id: f_00000-5-1 loss: 0.998969  [   64/  126]
train() client id: f_00000-5-2 loss: 1.118168  [   96/  126]
train() client id: f_00000-6-0 loss: 1.032562  [   32/  126]
train() client id: f_00000-6-1 loss: 1.054454  [   64/  126]
train() client id: f_00000-6-2 loss: 1.016280  [   96/  126]
train() client id: f_00000-7-0 loss: 0.971458  [   32/  126]
train() client id: f_00000-7-1 loss: 1.020727  [   64/  126]
train() client id: f_00000-7-2 loss: 1.019741  [   96/  126]
train() client id: f_00000-8-0 loss: 0.967145  [   32/  126]
train() client id: f_00000-8-1 loss: 0.981285  [   64/  126]
train() client id: f_00000-8-2 loss: 0.957053  [   96/  126]
train() client id: f_00000-9-0 loss: 0.932582  [   32/  126]
train() client id: f_00000-9-1 loss: 0.952939  [   64/  126]
train() client id: f_00000-9-2 loss: 0.992263  [   96/  126]
train() client id: f_00000-10-0 loss: 0.965250  [   32/  126]
train() client id: f_00000-10-1 loss: 0.955857  [   64/  126]
train() client id: f_00000-10-2 loss: 0.961795  [   96/  126]
train() client id: f_00000-11-0 loss: 0.917243  [   32/  126]
train() client id: f_00000-11-1 loss: 0.931183  [   64/  126]
train() client id: f_00000-11-2 loss: 0.964610  [   96/  126]
train() client id: f_00000-12-0 loss: 0.889273  [   32/  126]
train() client id: f_00000-12-1 loss: 0.953950  [   64/  126]
train() client id: f_00000-12-2 loss: 0.965939  [   96/  126]
train() client id: f_00001-0-0 loss: 0.763847  [   32/  265]
train() client id: f_00001-0-1 loss: 0.683677  [   64/  265]
train() client id: f_00001-0-2 loss: 0.646221  [   96/  265]
train() client id: f_00001-0-3 loss: 0.725001  [  128/  265]
train() client id: f_00001-0-4 loss: 0.663331  [  160/  265]
train() client id: f_00001-0-5 loss: 0.678608  [  192/  265]
train() client id: f_00001-0-6 loss: 0.719017  [  224/  265]
train() client id: f_00001-0-7 loss: 0.599522  [  256/  265]
train() client id: f_00001-1-0 loss: 0.666260  [   32/  265]
train() client id: f_00001-1-1 loss: 0.641704  [   64/  265]
train() client id: f_00001-1-2 loss: 0.594115  [   96/  265]
train() client id: f_00001-1-3 loss: 0.664524  [  128/  265]
train() client id: f_00001-1-4 loss: 0.602198  [  160/  265]
train() client id: f_00001-1-5 loss: 0.616555  [  192/  265]
train() client id: f_00001-1-6 loss: 0.700553  [  224/  265]
train() client id: f_00001-1-7 loss: 0.599850  [  256/  265]
train() client id: f_00001-2-0 loss: 0.692387  [   32/  265]
train() client id: f_00001-2-1 loss: 0.644742  [   64/  265]
train() client id: f_00001-2-2 loss: 0.594161  [   96/  265]
train() client id: f_00001-2-3 loss: 0.595160  [  128/  265]
train() client id: f_00001-2-4 loss: 0.567206  [  160/  265]
train() client id: f_00001-2-5 loss: 0.581854  [  192/  265]
train() client id: f_00001-2-6 loss: 0.552881  [  224/  265]
train() client id: f_00001-2-7 loss: 0.614785  [  256/  265]
train() client id: f_00001-3-0 loss: 0.565729  [   32/  265]
train() client id: f_00001-3-1 loss: 0.610249  [   64/  265]
train() client id: f_00001-3-2 loss: 0.514784  [   96/  265]
train() client id: f_00001-3-3 loss: 0.648843  [  128/  265]
train() client id: f_00001-3-4 loss: 0.597181  [  160/  265]
train() client id: f_00001-3-5 loss: 0.534699  [  192/  265]
train() client id: f_00001-3-6 loss: 0.605563  [  224/  265]
train() client id: f_00001-3-7 loss: 0.611812  [  256/  265]
train() client id: f_00001-4-0 loss: 0.544304  [   32/  265]
train() client id: f_00001-4-1 loss: 0.665756  [   64/  265]
train() client id: f_00001-4-2 loss: 0.560549  [   96/  265]
train() client id: f_00001-4-3 loss: 0.599569  [  128/  265]
train() client id: f_00001-4-4 loss: 0.550503  [  160/  265]
train() client id: f_00001-4-5 loss: 0.583160  [  192/  265]
train() client id: f_00001-4-6 loss: 0.473176  [  224/  265]
train() client id: f_00001-4-7 loss: 0.577268  [  256/  265]
train() client id: f_00001-5-0 loss: 0.594670  [   32/  265]
train() client id: f_00001-5-1 loss: 0.469210  [   64/  265]
train() client id: f_00001-5-2 loss: 0.489387  [   96/  265]
train() client id: f_00001-5-3 loss: 0.537369  [  128/  265]
train() client id: f_00001-5-4 loss: 0.510005  [  160/  265]
train() client id: f_00001-5-5 loss: 0.567654  [  192/  265]
train() client id: f_00001-5-6 loss: 0.656609  [  224/  265]
train() client id: f_00001-5-7 loss: 0.554854  [  256/  265]
train() client id: f_00001-6-0 loss: 0.462722  [   32/  265]
train() client id: f_00001-6-1 loss: 0.538360  [   64/  265]
train() client id: f_00001-6-2 loss: 0.476173  [   96/  265]
train() client id: f_00001-6-3 loss: 0.466167  [  128/  265]
train() client id: f_00001-6-4 loss: 0.639515  [  160/  265]
train() client id: f_00001-6-5 loss: 0.674609  [  192/  265]
train() client id: f_00001-6-6 loss: 0.482882  [  224/  265]
train() client id: f_00001-6-7 loss: 0.682978  [  256/  265]
train() client id: f_00001-7-0 loss: 0.569291  [   32/  265]
train() client id: f_00001-7-1 loss: 0.553927  [   64/  265]
train() client id: f_00001-7-2 loss: 0.549844  [   96/  265]
train() client id: f_00001-7-3 loss: 0.504974  [  128/  265]
train() client id: f_00001-7-4 loss: 0.596431  [  160/  265]
train() client id: f_00001-7-5 loss: 0.528261  [  192/  265]
train() client id: f_00001-7-6 loss: 0.470734  [  224/  265]
train() client id: f_00001-7-7 loss: 0.604110  [  256/  265]
train() client id: f_00001-8-0 loss: 0.503060  [   32/  265]
train() client id: f_00001-8-1 loss: 0.550953  [   64/  265]
train() client id: f_00001-8-2 loss: 0.576578  [   96/  265]
train() client id: f_00001-8-3 loss: 0.524963  [  128/  265]
train() client id: f_00001-8-4 loss: 0.594052  [  160/  265]
train() client id: f_00001-8-5 loss: 0.459531  [  192/  265]
train() client id: f_00001-8-6 loss: 0.442405  [  224/  265]
train() client id: f_00001-8-7 loss: 0.696093  [  256/  265]
train() client id: f_00001-9-0 loss: 0.529273  [   32/  265]
train() client id: f_00001-9-1 loss: 0.630211  [   64/  265]
train() client id: f_00001-9-2 loss: 0.498138  [   96/  265]
train() client id: f_00001-9-3 loss: 0.590101  [  128/  265]
train() client id: f_00001-9-4 loss: 0.508012  [  160/  265]
train() client id: f_00001-9-5 loss: 0.617292  [  192/  265]
train() client id: f_00001-9-6 loss: 0.456629  [  224/  265]
train() client id: f_00001-9-7 loss: 0.456255  [  256/  265]
train() client id: f_00001-10-0 loss: 0.455210  [   32/  265]
train() client id: f_00001-10-1 loss: 0.436367  [   64/  265]
train() client id: f_00001-10-2 loss: 0.703818  [   96/  265]
train() client id: f_00001-10-3 loss: 0.648210  [  128/  265]
train() client id: f_00001-10-4 loss: 0.497757  [  160/  265]
train() client id: f_00001-10-5 loss: 0.578450  [  192/  265]
train() client id: f_00001-10-6 loss: 0.526288  [  224/  265]
train() client id: f_00001-10-7 loss: 0.489153  [  256/  265]
train() client id: f_00001-11-0 loss: 0.482986  [   32/  265]
train() client id: f_00001-11-1 loss: 0.532509  [   64/  265]
train() client id: f_00001-11-2 loss: 0.523562  [   96/  265]
train() client id: f_00001-11-3 loss: 0.674458  [  128/  265]
train() client id: f_00001-11-4 loss: 0.502130  [  160/  265]
train() client id: f_00001-11-5 loss: 0.517634  [  192/  265]
train() client id: f_00001-11-6 loss: 0.563577  [  224/  265]
train() client id: f_00001-11-7 loss: 0.524270  [  256/  265]
train() client id: f_00001-12-0 loss: 0.507814  [   32/  265]
train() client id: f_00001-12-1 loss: 0.581484  [   64/  265]
train() client id: f_00001-12-2 loss: 0.461185  [   96/  265]
train() client id: f_00001-12-3 loss: 0.459493  [  128/  265]
train() client id: f_00001-12-4 loss: 0.621765  [  160/  265]
train() client id: f_00001-12-5 loss: 0.509085  [  192/  265]
train() client id: f_00001-12-6 loss: 0.558859  [  224/  265]
train() client id: f_00001-12-7 loss: 0.573138  [  256/  265]
train() client id: f_00002-0-0 loss: 1.178726  [   32/  124]
train() client id: f_00002-0-1 loss: 1.169698  [   64/  124]
train() client id: f_00002-0-2 loss: 1.029178  [   96/  124]
train() client id: f_00002-1-0 loss: 1.059039  [   32/  124]
train() client id: f_00002-1-1 loss: 1.198150  [   64/  124]
train() client id: f_00002-1-2 loss: 1.072109  [   96/  124]
train() client id: f_00002-2-0 loss: 1.013879  [   32/  124]
train() client id: f_00002-2-1 loss: 1.133767  [   64/  124]
train() client id: f_00002-2-2 loss: 0.961735  [   96/  124]
train() client id: f_00002-3-0 loss: 1.002311  [   32/  124]
train() client id: f_00002-3-1 loss: 0.990643  [   64/  124]
train() client id: f_00002-3-2 loss: 1.098333  [   96/  124]
train() client id: f_00002-4-0 loss: 1.011738  [   32/  124]
train() client id: f_00002-4-1 loss: 0.954762  [   64/  124]
train() client id: f_00002-4-2 loss: 1.054106  [   96/  124]
train() client id: f_00002-5-0 loss: 0.993084  [   32/  124]
train() client id: f_00002-5-1 loss: 0.957833  [   64/  124]
train() client id: f_00002-5-2 loss: 1.020433  [   96/  124]
train() client id: f_00002-6-0 loss: 1.089875  [   32/  124]
train() client id: f_00002-6-1 loss: 0.958754  [   64/  124]
train() client id: f_00002-6-2 loss: 0.923574  [   96/  124]
train() client id: f_00002-7-0 loss: 1.009982  [   32/  124]
train() client id: f_00002-7-1 loss: 0.993621  [   64/  124]
train() client id: f_00002-7-2 loss: 0.953078  [   96/  124]
train() client id: f_00002-8-0 loss: 0.970218  [   32/  124]
train() client id: f_00002-8-1 loss: 0.968982  [   64/  124]
train() client id: f_00002-8-2 loss: 0.909492  [   96/  124]
train() client id: f_00002-9-0 loss: 1.024026  [   32/  124]
train() client id: f_00002-9-1 loss: 1.008238  [   64/  124]
train() client id: f_00002-9-2 loss: 0.918576  [   96/  124]
train() client id: f_00002-10-0 loss: 0.992414  [   32/  124]
train() client id: f_00002-10-1 loss: 0.858703  [   64/  124]
train() client id: f_00002-10-2 loss: 0.972009  [   96/  124]
train() client id: f_00002-11-0 loss: 0.844811  [   32/  124]
train() client id: f_00002-11-1 loss: 1.168296  [   64/  124]
train() client id: f_00002-11-2 loss: 0.952819  [   96/  124]
train() client id: f_00002-12-0 loss: 0.881177  [   32/  124]
train() client id: f_00002-12-1 loss: 0.985925  [   64/  124]
train() client id: f_00002-12-2 loss: 0.931939  [   96/  124]
train() client id: f_00003-0-0 loss: 0.826272  [   32/   43]
train() client id: f_00003-1-0 loss: 0.881000  [   32/   43]
train() client id: f_00003-2-0 loss: 0.904075  [   32/   43]
train() client id: f_00003-3-0 loss: 0.907089  [   32/   43]
train() client id: f_00003-4-0 loss: 0.863326  [   32/   43]
train() client id: f_00003-5-0 loss: 1.021910  [   32/   43]
train() client id: f_00003-6-0 loss: 0.923757  [   32/   43]
train() client id: f_00003-7-0 loss: 0.894825  [   32/   43]
train() client id: f_00003-8-0 loss: 0.928617  [   32/   43]
train() client id: f_00003-9-0 loss: 1.008083  [   32/   43]
train() client id: f_00003-10-0 loss: 1.018220  [   32/   43]
train() client id: f_00003-11-0 loss: 0.904190  [   32/   43]
train() client id: f_00003-12-0 loss: 1.014970  [   32/   43]
train() client id: f_00004-0-0 loss: 1.154210  [   32/  306]
train() client id: f_00004-0-1 loss: 1.234276  [   64/  306]
train() client id: f_00004-0-2 loss: 1.233661  [   96/  306]
train() client id: f_00004-0-3 loss: 1.247424  [  128/  306]
train() client id: f_00004-0-4 loss: 1.061053  [  160/  306]
train() client id: f_00004-0-5 loss: 0.927783  [  192/  306]
train() client id: f_00004-0-6 loss: 0.962200  [  224/  306]
train() client id: f_00004-0-7 loss: 1.054883  [  256/  306]
train() client id: f_00004-0-8 loss: 1.114910  [  288/  306]
train() client id: f_00004-1-0 loss: 1.117843  [   32/  306]
train() client id: f_00004-1-1 loss: 0.988768  [   64/  306]
train() client id: f_00004-1-2 loss: 1.068342  [   96/  306]
train() client id: f_00004-1-3 loss: 1.135477  [  128/  306]
train() client id: f_00004-1-4 loss: 1.055506  [  160/  306]
train() client id: f_00004-1-5 loss: 1.098694  [  192/  306]
train() client id: f_00004-1-6 loss: 1.248694  [  224/  306]
train() client id: f_00004-1-7 loss: 1.068747  [  256/  306]
train() client id: f_00004-1-8 loss: 1.215392  [  288/  306]
train() client id: f_00004-2-0 loss: 1.131017  [   32/  306]
train() client id: f_00004-2-1 loss: 1.053330  [   64/  306]
train() client id: f_00004-2-2 loss: 1.054984  [   96/  306]
train() client id: f_00004-2-3 loss: 1.254691  [  128/  306]
train() client id: f_00004-2-4 loss: 1.127630  [  160/  306]
train() client id: f_00004-2-5 loss: 1.002337  [  192/  306]
train() client id: f_00004-2-6 loss: 1.069757  [  224/  306]
train() client id: f_00004-2-7 loss: 1.126660  [  256/  306]
train() client id: f_00004-2-8 loss: 1.022803  [  288/  306]
train() client id: f_00004-3-0 loss: 1.110165  [   32/  306]
train() client id: f_00004-3-1 loss: 1.108013  [   64/  306]
train() client id: f_00004-3-2 loss: 1.075219  [   96/  306]
train() client id: f_00004-3-3 loss: 1.114113  [  128/  306]
train() client id: f_00004-3-4 loss: 1.079620  [  160/  306]
train() client id: f_00004-3-5 loss: 1.171318  [  192/  306]
train() client id: f_00004-3-6 loss: 1.160367  [  224/  306]
train() client id: f_00004-3-7 loss: 1.042410  [  256/  306]
train() client id: f_00004-3-8 loss: 1.080572  [  288/  306]
train() client id: f_00004-4-0 loss: 1.018256  [   32/  306]
train() client id: f_00004-4-1 loss: 0.971453  [   64/  306]
train() client id: f_00004-4-2 loss: 1.064215  [   96/  306]
train() client id: f_00004-4-3 loss: 1.049698  [  128/  306]
train() client id: f_00004-4-4 loss: 1.218643  [  160/  306]
train() client id: f_00004-4-5 loss: 0.902708  [  192/  306]
train() client id: f_00004-4-6 loss: 1.103239  [  224/  306]
train() client id: f_00004-4-7 loss: 1.301725  [  256/  306]
train() client id: f_00004-4-8 loss: 1.034064  [  288/  306]
train() client id: f_00004-5-0 loss: 1.204606  [   32/  306]
train() client id: f_00004-5-1 loss: 1.071513  [   64/  306]
train() client id: f_00004-5-2 loss: 1.079702  [   96/  306]
train() client id: f_00004-5-3 loss: 1.000513  [  128/  306]
train() client id: f_00004-5-4 loss: 1.097651  [  160/  306]
train() client id: f_00004-5-5 loss: 1.007639  [  192/  306]
train() client id: f_00004-5-6 loss: 1.065329  [  224/  306]
train() client id: f_00004-5-7 loss: 1.129354  [  256/  306]
train() client id: f_00004-5-8 loss: 1.050480  [  288/  306]
train() client id: f_00004-6-0 loss: 1.102561  [   32/  306]
train() client id: f_00004-6-1 loss: 1.129026  [   64/  306]
train() client id: f_00004-6-2 loss: 1.147089  [   96/  306]
train() client id: f_00004-6-3 loss: 1.053195  [  128/  306]
train() client id: f_00004-6-4 loss: 1.065389  [  160/  306]
train() client id: f_00004-6-5 loss: 1.087609  [  192/  306]
train() client id: f_00004-6-6 loss: 0.979462  [  224/  306]
train() client id: f_00004-6-7 loss: 1.068417  [  256/  306]
train() client id: f_00004-6-8 loss: 1.125846  [  288/  306]
train() client id: f_00004-7-0 loss: 1.141132  [   32/  306]
train() client id: f_00004-7-1 loss: 1.185203  [   64/  306]
train() client id: f_00004-7-2 loss: 0.987116  [   96/  306]
train() client id: f_00004-7-3 loss: 0.984443  [  128/  306]
train() client id: f_00004-7-4 loss: 1.144608  [  160/  306]
train() client id: f_00004-7-5 loss: 1.045206  [  192/  306]
train() client id: f_00004-7-6 loss: 1.143365  [  224/  306]
train() client id: f_00004-7-7 loss: 0.901823  [  256/  306]
train() client id: f_00004-7-8 loss: 1.009339  [  288/  306]
train() client id: f_00004-8-0 loss: 1.048666  [   32/  306]
train() client id: f_00004-8-1 loss: 1.015436  [   64/  306]
train() client id: f_00004-8-2 loss: 1.036166  [   96/  306]
train() client id: f_00004-8-3 loss: 1.175911  [  128/  306]
train() client id: f_00004-8-4 loss: 0.984194  [  160/  306]
train() client id: f_00004-8-5 loss: 1.209207  [  192/  306]
train() client id: f_00004-8-6 loss: 1.054682  [  224/  306]
train() client id: f_00004-8-7 loss: 1.120517  [  256/  306]
train() client id: f_00004-8-8 loss: 0.902066  [  288/  306]
train() client id: f_00004-9-0 loss: 1.059227  [   32/  306]
train() client id: f_00004-9-1 loss: 1.092283  [   64/  306]
train() client id: f_00004-9-2 loss: 1.130969  [   96/  306]
train() client id: f_00004-9-3 loss: 1.085336  [  128/  306]
train() client id: f_00004-9-4 loss: 1.001357  [  160/  306]
train() client id: f_00004-9-5 loss: 1.116142  [  192/  306]
train() client id: f_00004-9-6 loss: 1.055842  [  224/  306]
train() client id: f_00004-9-7 loss: 0.932503  [  256/  306]
train() client id: f_00004-9-8 loss: 0.936737  [  288/  306]
train() client id: f_00004-10-0 loss: 1.139042  [   32/  306]
train() client id: f_00004-10-1 loss: 1.072280  [   64/  306]
train() client id: f_00004-10-2 loss: 0.900228  [   96/  306]
train() client id: f_00004-10-3 loss: 0.964805  [  128/  306]
train() client id: f_00004-10-4 loss: 1.058894  [  160/  306]
train() client id: f_00004-10-5 loss: 1.190532  [  192/  306]
train() client id: f_00004-10-6 loss: 1.003304  [  224/  306]
train() client id: f_00004-10-7 loss: 1.056477  [  256/  306]
train() client id: f_00004-10-8 loss: 1.073408  [  288/  306]
train() client id: f_00004-11-0 loss: 1.063308  [   32/  306]
train() client id: f_00004-11-1 loss: 1.012359  [   64/  306]
train() client id: f_00004-11-2 loss: 1.077641  [   96/  306]
train() client id: f_00004-11-3 loss: 1.098539  [  128/  306]
train() client id: f_00004-11-4 loss: 1.005245  [  160/  306]
train() client id: f_00004-11-5 loss: 0.989726  [  192/  306]
train() client id: f_00004-11-6 loss: 1.083760  [  224/  306]
train() client id: f_00004-11-7 loss: 1.062710  [  256/  306]
train() client id: f_00004-11-8 loss: 1.049476  [  288/  306]
train() client id: f_00004-12-0 loss: 1.081855  [   32/  306]
train() client id: f_00004-12-1 loss: 1.071532  [   64/  306]
train() client id: f_00004-12-2 loss: 1.036907  [   96/  306]
train() client id: f_00004-12-3 loss: 0.941085  [  128/  306]
train() client id: f_00004-12-4 loss: 1.011818  [  160/  306]
train() client id: f_00004-12-5 loss: 1.166398  [  192/  306]
train() client id: f_00004-12-6 loss: 0.987683  [  224/  306]
train() client id: f_00004-12-7 loss: 1.015375  [  256/  306]
train() client id: f_00004-12-8 loss: 1.066272  [  288/  306]
train() client id: f_00005-0-0 loss: 0.857188  [   32/  146]
train() client id: f_00005-0-1 loss: 0.912426  [   64/  146]
train() client id: f_00005-0-2 loss: 0.801139  [   96/  146]
train() client id: f_00005-0-3 loss: 0.828460  [  128/  146]
train() client id: f_00005-1-0 loss: 0.828397  [   32/  146]
train() client id: f_00005-1-1 loss: 0.901158  [   64/  146]
train() client id: f_00005-1-2 loss: 0.792028  [   96/  146]
train() client id: f_00005-1-3 loss: 0.840242  [  128/  146]
train() client id: f_00005-2-0 loss: 0.805477  [   32/  146]
train() client id: f_00005-2-1 loss: 0.838451  [   64/  146]
train() client id: f_00005-2-2 loss: 0.771166  [   96/  146]
train() client id: f_00005-2-3 loss: 0.794131  [  128/  146]
train() client id: f_00005-3-0 loss: 0.697689  [   32/  146]
train() client id: f_00005-3-1 loss: 0.802201  [   64/  146]
train() client id: f_00005-3-2 loss: 0.923347  [   96/  146]
train() client id: f_00005-3-3 loss: 0.830233  [  128/  146]
train() client id: f_00005-4-0 loss: 0.798077  [   32/  146]
train() client id: f_00005-4-1 loss: 0.685892  [   64/  146]
train() client id: f_00005-4-2 loss: 0.695551  [   96/  146]
train() client id: f_00005-4-3 loss: 1.003956  [  128/  146]
train() client id: f_00005-5-0 loss: 0.775364  [   32/  146]
train() client id: f_00005-5-1 loss: 0.662028  [   64/  146]
train() client id: f_00005-5-2 loss: 0.881955  [   96/  146]
train() client id: f_00005-5-3 loss: 0.789274  [  128/  146]
train() client id: f_00005-6-0 loss: 0.734470  [   32/  146]
train() client id: f_00005-6-1 loss: 0.816386  [   64/  146]
train() client id: f_00005-6-2 loss: 0.803259  [   96/  146]
train() client id: f_00005-6-3 loss: 0.729658  [  128/  146]
train() client id: f_00005-7-0 loss: 0.754967  [   32/  146]
train() client id: f_00005-7-1 loss: 0.829611  [   64/  146]
train() client id: f_00005-7-2 loss: 0.830324  [   96/  146]
train() client id: f_00005-7-3 loss: 0.728485  [  128/  146]
train() client id: f_00005-8-0 loss: 0.765337  [   32/  146]
train() client id: f_00005-8-1 loss: 0.909733  [   64/  146]
train() client id: f_00005-8-2 loss: 0.670257  [   96/  146]
train() client id: f_00005-8-3 loss: 0.744448  [  128/  146]
train() client id: f_00005-9-0 loss: 0.743450  [   32/  146]
train() client id: f_00005-9-1 loss: 0.851008  [   64/  146]
train() client id: f_00005-9-2 loss: 0.710497  [   96/  146]
train() client id: f_00005-9-3 loss: 0.809223  [  128/  146]
train() client id: f_00005-10-0 loss: 0.704043  [   32/  146]
train() client id: f_00005-10-1 loss: 0.790119  [   64/  146]
train() client id: f_00005-10-2 loss: 0.847050  [   96/  146]
train() client id: f_00005-10-3 loss: 0.680027  [  128/  146]
train() client id: f_00005-11-0 loss: 0.624920  [   32/  146]
train() client id: f_00005-11-1 loss: 0.789601  [   64/  146]
train() client id: f_00005-11-2 loss: 0.766971  [   96/  146]
train() client id: f_00005-11-3 loss: 0.882246  [  128/  146]
train() client id: f_00005-12-0 loss: 0.756415  [   32/  146]
train() client id: f_00005-12-1 loss: 0.716768  [   64/  146]
train() client id: f_00005-12-2 loss: 0.686634  [   96/  146]
train() client id: f_00005-12-3 loss: 0.777330  [  128/  146]
train() client id: f_00006-0-0 loss: 0.971310  [   32/   54]
train() client id: f_00006-1-0 loss: 0.987911  [   32/   54]
train() client id: f_00006-2-0 loss: 0.936636  [   32/   54]
train() client id: f_00006-3-0 loss: 0.976883  [   32/   54]
train() client id: f_00006-4-0 loss: 0.944340  [   32/   54]
train() client id: f_00006-5-0 loss: 0.951927  [   32/   54]
train() client id: f_00006-6-0 loss: 0.977617  [   32/   54]
train() client id: f_00006-7-0 loss: 0.982211  [   32/   54]
train() client id: f_00006-8-0 loss: 0.971834  [   32/   54]
train() client id: f_00006-9-0 loss: 0.933595  [   32/   54]
train() client id: f_00006-10-0 loss: 1.011896  [   32/   54]
train() client id: f_00006-11-0 loss: 0.999868  [   32/   54]
train() client id: f_00006-12-0 loss: 0.981085  [   32/   54]
train() client id: f_00007-0-0 loss: 0.796976  [   32/  179]
train() client id: f_00007-0-1 loss: 0.783859  [   64/  179]
train() client id: f_00007-0-2 loss: 0.810830  [   96/  179]
train() client id: f_00007-0-3 loss: 0.754020  [  128/  179]
train() client id: f_00007-0-4 loss: 0.751372  [  160/  179]
train() client id: f_00007-1-0 loss: 0.731396  [   32/  179]
train() client id: f_00007-1-1 loss: 0.834160  [   64/  179]
train() client id: f_00007-1-2 loss: 0.757213  [   96/  179]
train() client id: f_00007-1-3 loss: 0.749426  [  128/  179]
train() client id: f_00007-1-4 loss: 0.738097  [  160/  179]
train() client id: f_00007-2-0 loss: 0.764887  [   32/  179]
train() client id: f_00007-2-1 loss: 0.706264  [   64/  179]
train() client id: f_00007-2-2 loss: 0.718409  [   96/  179]
train() client id: f_00007-2-3 loss: 0.770675  [  128/  179]
train() client id: f_00007-2-4 loss: 0.759592  [  160/  179]
train() client id: f_00007-3-0 loss: 0.652407  [   32/  179]
train() client id: f_00007-3-1 loss: 0.786484  [   64/  179]
train() client id: f_00007-3-2 loss: 0.698538  [   96/  179]
train() client id: f_00007-3-3 loss: 0.716665  [  128/  179]
train() client id: f_00007-3-4 loss: 0.688544  [  160/  179]
train() client id: f_00007-4-0 loss: 0.721650  [   32/  179]
train() client id: f_00007-4-1 loss: 0.746553  [   64/  179]
train() client id: f_00007-4-2 loss: 0.640118  [   96/  179]
train() client id: f_00007-4-3 loss: 0.639884  [  128/  179]
train() client id: f_00007-4-4 loss: 0.797140  [  160/  179]
train() client id: f_00007-5-0 loss: 0.727644  [   32/  179]
train() client id: f_00007-5-1 loss: 0.686087  [   64/  179]
train() client id: f_00007-5-2 loss: 0.609537  [   96/  179]
train() client id: f_00007-5-3 loss: 0.692101  [  128/  179]
train() client id: f_00007-5-4 loss: 0.771968  [  160/  179]
train() client id: f_00007-6-0 loss: 0.720876  [   32/  179]
train() client id: f_00007-6-1 loss: 0.663907  [   64/  179]
train() client id: f_00007-6-2 loss: 0.627638  [   96/  179]
train() client id: f_00007-6-3 loss: 0.657725  [  128/  179]
train() client id: f_00007-6-4 loss: 0.835147  [  160/  179]
train() client id: f_00007-7-0 loss: 0.750674  [   32/  179]
train() client id: f_00007-7-1 loss: 0.650098  [   64/  179]
train() client id: f_00007-7-2 loss: 0.674679  [   96/  179]
train() client id: f_00007-7-3 loss: 0.677380  [  128/  179]
train() client id: f_00007-7-4 loss: 0.671854  [  160/  179]
train() client id: f_00007-8-0 loss: 0.727112  [   32/  179]
train() client id: f_00007-8-1 loss: 0.734356  [   64/  179]
train() client id: f_00007-8-2 loss: 0.594365  [   96/  179]
train() client id: f_00007-8-3 loss: 0.648251  [  128/  179]
train() client id: f_00007-8-4 loss: 0.659417  [  160/  179]
train() client id: f_00007-9-0 loss: 0.653845  [   32/  179]
train() client id: f_00007-9-1 loss: 0.672114  [   64/  179]
train() client id: f_00007-9-2 loss: 0.664155  [   96/  179]
train() client id: f_00007-9-3 loss: 0.710493  [  128/  179]
train() client id: f_00007-9-4 loss: 0.643059  [  160/  179]
train() client id: f_00007-10-0 loss: 0.649000  [   32/  179]
train() client id: f_00007-10-1 loss: 0.792871  [   64/  179]
train() client id: f_00007-10-2 loss: 0.596943  [   96/  179]
train() client id: f_00007-10-3 loss: 0.586679  [  128/  179]
train() client id: f_00007-10-4 loss: 0.795316  [  160/  179]
train() client id: f_00007-11-0 loss: 0.713299  [   32/  179]
train() client id: f_00007-11-1 loss: 0.653304  [   64/  179]
train() client id: f_00007-11-2 loss: 0.648215  [   96/  179]
train() client id: f_00007-11-3 loss: 0.665408  [  128/  179]
train() client id: f_00007-11-4 loss: 0.726532  [  160/  179]
train() client id: f_00007-12-0 loss: 0.718157  [   32/  179]
train() client id: f_00007-12-1 loss: 0.718465  [   64/  179]
train() client id: f_00007-12-2 loss: 0.588073  [   96/  179]
train() client id: f_00007-12-3 loss: 0.657338  [  128/  179]
train() client id: f_00007-12-4 loss: 0.696615  [  160/  179]
train() client id: f_00008-0-0 loss: 0.806932  [   32/  130]
train() client id: f_00008-0-1 loss: 0.861638  [   64/  130]
train() client id: f_00008-0-2 loss: 0.877145  [   96/  130]
train() client id: f_00008-0-3 loss: 0.835157  [  128/  130]
train() client id: f_00008-1-0 loss: 0.771907  [   32/  130]
train() client id: f_00008-1-1 loss: 0.983526  [   64/  130]
train() client id: f_00008-1-2 loss: 0.802617  [   96/  130]
train() client id: f_00008-1-3 loss: 0.835814  [  128/  130]
train() client id: f_00008-2-0 loss: 0.877447  [   32/  130]
train() client id: f_00008-2-1 loss: 0.812675  [   64/  130]
train() client id: f_00008-2-2 loss: 0.903803  [   96/  130]
train() client id: f_00008-2-3 loss: 0.795191  [  128/  130]
train() client id: f_00008-3-0 loss: 0.832815  [   32/  130]
train() client id: f_00008-3-1 loss: 0.821114  [   64/  130]
train() client id: f_00008-3-2 loss: 0.929812  [   96/  130]
train() client id: f_00008-3-3 loss: 0.805099  [  128/  130]
train() client id: f_00008-4-0 loss: 0.808137  [   32/  130]
train() client id: f_00008-4-1 loss: 0.885343  [   64/  130]
train() client id: f_00008-4-2 loss: 0.714337  [   96/  130]
train() client id: f_00008-4-3 loss: 0.966878  [  128/  130]
train() client id: f_00008-5-0 loss: 0.833137  [   32/  130]
train() client id: f_00008-5-1 loss: 0.860007  [   64/  130]
train() client id: f_00008-5-2 loss: 0.906232  [   96/  130]
train() client id: f_00008-5-3 loss: 0.746304  [  128/  130]
train() client id: f_00008-6-0 loss: 0.826873  [   32/  130]
train() client id: f_00008-6-1 loss: 0.886116  [   64/  130]
train() client id: f_00008-6-2 loss: 0.768849  [   96/  130]
train() client id: f_00008-6-3 loss: 0.913617  [  128/  130]
train() client id: f_00008-7-0 loss: 0.792010  [   32/  130]
train() client id: f_00008-7-1 loss: 0.816242  [   64/  130]
train() client id: f_00008-7-2 loss: 1.003095  [   96/  130]
train() client id: f_00008-7-3 loss: 0.764991  [  128/  130]
train() client id: f_00008-8-0 loss: 0.788987  [   32/  130]
train() client id: f_00008-8-1 loss: 0.806751  [   64/  130]
train() client id: f_00008-8-2 loss: 0.892144  [   96/  130]
train() client id: f_00008-8-3 loss: 0.910444  [  128/  130]
train() client id: f_00008-9-0 loss: 0.908247  [   32/  130]
train() client id: f_00008-9-1 loss: 0.949852  [   64/  130]
train() client id: f_00008-9-2 loss: 0.748243  [   96/  130]
train() client id: f_00008-9-3 loss: 0.774826  [  128/  130]
train() client id: f_00008-10-0 loss: 0.893359  [   32/  130]
train() client id: f_00008-10-1 loss: 0.882494  [   64/  130]
train() client id: f_00008-10-2 loss: 0.750369  [   96/  130]
train() client id: f_00008-10-3 loss: 0.875271  [  128/  130]
train() client id: f_00008-11-0 loss: 0.796244  [   32/  130]
train() client id: f_00008-11-1 loss: 0.844199  [   64/  130]
train() client id: f_00008-11-2 loss: 0.918809  [   96/  130]
train() client id: f_00008-11-3 loss: 0.823813  [  128/  130]
train() client id: f_00008-12-0 loss: 0.849393  [   32/  130]
train() client id: f_00008-12-1 loss: 0.889380  [   64/  130]
train() client id: f_00008-12-2 loss: 0.792880  [   96/  130]
train() client id: f_00008-12-3 loss: 0.853804  [  128/  130]
train() client id: f_00009-0-0 loss: 1.134189  [   32/  118]
train() client id: f_00009-0-1 loss: 0.985719  [   64/  118]
train() client id: f_00009-0-2 loss: 0.998244  [   96/  118]
train() client id: f_00009-1-0 loss: 0.952173  [   32/  118]
train() client id: f_00009-1-1 loss: 1.024918  [   64/  118]
train() client id: f_00009-1-2 loss: 0.977818  [   96/  118]
train() client id: f_00009-2-0 loss: 0.934527  [   32/  118]
train() client id: f_00009-2-1 loss: 0.969862  [   64/  118]
train() client id: f_00009-2-2 loss: 0.957886  [   96/  118]
train() client id: f_00009-3-0 loss: 0.986356  [   32/  118]
train() client id: f_00009-3-1 loss: 0.930717  [   64/  118]
train() client id: f_00009-3-2 loss: 0.961132  [   96/  118]
train() client id: f_00009-4-0 loss: 0.864241  [   32/  118]
train() client id: f_00009-4-1 loss: 0.957490  [   64/  118]
train() client id: f_00009-4-2 loss: 0.950032  [   96/  118]
train() client id: f_00009-5-0 loss: 0.921955  [   32/  118]
train() client id: f_00009-5-1 loss: 0.857350  [   64/  118]
train() client id: f_00009-5-2 loss: 0.921575  [   96/  118]
train() client id: f_00009-6-0 loss: 0.841950  [   32/  118]
train() client id: f_00009-6-1 loss: 0.858246  [   64/  118]
train() client id: f_00009-6-2 loss: 1.033140  [   96/  118]
train() client id: f_00009-7-0 loss: 0.795752  [   32/  118]
train() client id: f_00009-7-1 loss: 0.952110  [   64/  118]
train() client id: f_00009-7-2 loss: 0.906746  [   96/  118]
train() client id: f_00009-8-0 loss: 0.934460  [   32/  118]
train() client id: f_00009-8-1 loss: 0.943452  [   64/  118]
train() client id: f_00009-8-2 loss: 0.837351  [   96/  118]
train() client id: f_00009-9-0 loss: 0.848604  [   32/  118]
train() client id: f_00009-9-1 loss: 0.837909  [   64/  118]
train() client id: f_00009-9-2 loss: 0.991697  [   96/  118]
train() client id: f_00009-10-0 loss: 0.798578  [   32/  118]
train() client id: f_00009-10-1 loss: 0.879372  [   64/  118]
train() client id: f_00009-10-2 loss: 0.830732  [   96/  118]
train() client id: f_00009-11-0 loss: 0.954129  [   32/  118]
train() client id: f_00009-11-1 loss: 0.787550  [   64/  118]
train() client id: f_00009-11-2 loss: 0.959328  [   96/  118]
train() client id: f_00009-12-0 loss: 0.944942  [   32/  118]
train() client id: f_00009-12-1 loss: 0.803027  [   64/  118]
train() client id: f_00009-12-2 loss: 0.815270  [   96/  118]
At round 3 accuracy: 0.6312997347480106
At round 3 training accuracy: 0.5767940979208585
At round 3 training loss: 0.9171214323656592
update_location
xs = [ -3.9056584   -0.79968212  35.00902392  18.81129433 -19.02070377
  -6.04359014   2.55680806  -6.32485185  19.66397685  -2.06087855]
ys = [ 27.5879595   15.55583871   1.32061395   2.54482414   9.35018685
 -17.18584926  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [103.80919842 101.2058477  105.95931189 101.78575993 102.2213929
 101.64584791 100.06711653 100.2031936  103.41828638 100.10124413]
dists_bs = [225.83972948 236.15289818 272.52214454 259.4293844  227.52890245
 255.89386294 251.15145857 242.46476046 250.35690873 243.20450851]
uav_gains = [9.10767832e-11 9.70474414e-11 8.65263782e-11 9.56710163e-11
 9.46549499e-11 9.60005847e-11 9.98320356e-11 9.94934409e-11
 9.19399105e-11 9.97469659e-11]
bs_gains = [2.83565323e-11 2.50237793e-11 1.67559498e-11 1.92327073e-11
 2.77710106e-11 1.99860260e-11 2.10607635e-11 2.32422371e-11
 2.12484502e-11 2.30448319e-11]
Round 4
-------------------------------
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.46545628 21.87269455 10.30373985  3.68435765 25.21614886 12.16538239
  4.58153103 14.79179615 10.85487015  9.87006751]
obj_prev = 123.8060444419656
eta_min = 1.1887003308068872e-09	eta_max = 0.9184674772800265
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 28.799330597431425	eta = 0.9090909090909091
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 49.438331310801296	eta = 0.5295730850913346
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 39.62791236585137	eta = 0.660675974861339
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 37.87221877758381	eta = 0.6913038232004766
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 37.78643716203915	eta = 0.6928732000256066
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 37.78621816141451	eta = 0.6928772157665561
eta = 0.6928772157665561
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [0.03027591 0.06367553 0.02979534 0.01033226 0.07352722 0.03508161
 0.01297539 0.04301102 0.03123705 0.02835363]
ene_total = [3.202787   6.29122282 3.16613857 1.4566637  7.13363507 3.83605127
 1.68315721 4.30566359 3.48877145 3.22212748]
ti_comp = [0.2772398  0.25747119 0.276632   0.27781288 0.25949396 0.25275957
 0.27830054 0.2782619  0.25409312 0.25580165]
ti_coms = [0.06410402 0.08387264 0.06471183 0.06353094 0.08184987 0.08858426
 0.06304328 0.06308193 0.0872507  0.08554218]
t_total = [29.79998322 29.79998322 29.79998322 29.79998322 29.79998322 29.79998322
 29.79998322 29.79998322 29.79998322 29.79998322]
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.25663118e-05 2.43411145e-04 2.16033125e-05 8.93224701e-07
 3.68952017e-04 4.22380117e-05 1.76284161e-06 6.42261255e-05
 2.95055929e-05 2.17720570e-05]
ene_total = [0.56161072 0.75347478 0.56683292 0.55471554 0.74677556 0.77704532
 0.5505341  0.55632463 0.76429157 0.74870068]
optimize_network iter = 0 obj = 6.580305820795663
eta = 0.6928772157665561
freqs = [5.46023917e+07 1.23655643e+08 5.38537534e+07 1.85957107e+07
 1.41674235e+08 6.93972005e+07 2.33118325e+07 7.72851390e+07
 6.14677254e+07 5.54211297e+07]
eta_min = 0.6721669962857192	eta_max = 0.692877215766553
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 0.06566942832741904	eta = 0.9090909090909091
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 22.05222718616278	eta = 0.002707185981428379
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 2.358376629903991	eta = 0.02531380252868433
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 2.276155607764052	eta = 0.02622820693542063
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 2.276117460225635	eta = 0.02622864651797696
eta = 0.02622864651797696
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.23933498e-04 2.41545493e-03 2.14377315e-04 8.86378481e-06
 3.66124143e-03 4.19142738e-04 1.74933012e-05 6.37338573e-04
 2.92794440e-04 2.16051827e-04]
ene_total = [0.18267388 0.2974487  0.18408433 0.17517407 0.32618133 0.25545427
 0.17406893 0.19124253 0.24830343 0.241486  ]
ti_comp = [0.30025765 0.28048904 0.29964984 0.30083073 0.28251181 0.27577742
 0.30131839 0.30127974 0.27711097 0.27881949]
ti_coms = [0.06410402 0.08387264 0.06471183 0.06353094 0.08184987 0.08858426
 0.06304328 0.06308193 0.0872507  0.08554218]
t_total = [29.79998322 29.79998322 29.79998322 29.79998322 29.79998322 29.79998322
 29.79998322 29.79998322 29.79998322 29.79998322]
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.44184640e-05 2.60315910e-04 2.33685526e-05 9.66842755e-07
 3.95080819e-04 4.50335349e-05 1.90864377e-06 6.95367030e-05
 3.14859996e-05 2.32591919e-05]
ene_total = [0.52628353 0.70725804 0.53116872 0.51967847 0.70173644 0.72818562
 0.51576709 0.52161422 0.71617089 0.70152458]
optimize_network iter = 1 obj = 6.169387607456715
eta = 0.6721669962857192
freqs = [5.45864375e+07 1.22896148e+08 5.38289512e+07 1.85932175e+07
 1.40894199e+08 6.88656172e+07 2.33118325e+07 7.72843192e+07
 6.10236134e+07 5.50512470e+07]
eta_min = 0.6721669962857293	eta_max = 0.6721669962857161
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 0.06499207998561081	eta = 0.909090909090909
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 22.05158160424028	eta = 0.0026793411075088985
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 2.3552731564287246	eta = 0.02508571412048698
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 2.273849552272878	eta = 0.0259840010165886
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 2.2738124460945865	eta = 0.025984425047592612
eta = 0.025984425047592612
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.24377690e-04 2.39200478e-03 2.14730208e-04 8.88417650e-06
 3.63033979e-03 4.13806558e-04 1.75382481e-05 6.38962582e-04
 2.89320240e-04 2.13725309e-04]
ene_total = [0.1826403  0.29672859 0.18404788 0.17513071 0.32524889 0.25524333
 0.17402652 0.19123929 0.24814553 0.24136141]
ti_comp = [0.30025765 0.28048904 0.29964984 0.30083073 0.28251181 0.27577742
 0.30131839 0.30127974 0.27711097 0.27881949]
ti_coms = [0.06410402 0.08387264 0.06471183 0.06353094 0.08184987 0.08858426
 0.06304328 0.06308193 0.0872507  0.08554218]
t_total = [29.79998322 29.79998322 29.79998322 29.79998322 29.79998322 29.79998322
 29.79998322 29.79998322 29.79998322 29.79998322]
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.44184640e-05 2.60315910e-04 2.33685526e-05 9.66842755e-07
 3.95080819e-04 4.50335349e-05 1.90864377e-06 6.95367030e-05
 3.14859996e-05 2.32591919e-05]
ene_total = [0.52628353 0.70725804 0.53116872 0.51967847 0.70173644 0.72818562
 0.51576709 0.52161422 0.71617089 0.70152458]
optimize_network iter = 2 obj = 6.169387607456903
eta = 0.6721669962857293
freqs = [5.45864375e+07 1.22896148e+08 5.38289512e+07 1.85932175e+07
 1.40894199e+08 6.88656172e+07 2.33118325e+07 7.72843192e+07
 6.10236134e+07 5.50512470e+07]
Done!
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.44035723e-05 2.60157155e-04 2.33543012e-05 9.66253122e-07
 3.94839877e-04 4.50060710e-05 1.90747978e-06 6.94942958e-05
 3.14667978e-05 2.32450072e-05]
ene_total = [0.00643481 0.00864742 0.00649454 0.00635406 0.00857983 0.00890343
 0.00630624 0.00637769 0.00875654 0.00857746]
At round 4 energy consumption: 0.07543200489545869
At round 4 eta: 0.6721669962857293
At round 4 a_n: 26.46987362140779
At round 4 local rounds: 13.007932941342311
At round 4 global rounds: 81.78682184099438
gradient difference: 0.4422478675842285
train() client id: f_00000-0-0 loss: 1.656888  [   32/  126]
train() client id: f_00000-0-1 loss: 1.228974  [   64/  126]
train() client id: f_00000-0-2 loss: 1.793181  [   96/  126]
train() client id: f_00000-1-0 loss: 1.731303  [   32/  126]
train() client id: f_00000-1-1 loss: 1.178412  [   64/  126]
train() client id: f_00000-1-2 loss: 1.258840  [   96/  126]
train() client id: f_00000-2-0 loss: 1.496954  [   32/  126]
train() client id: f_00000-2-1 loss: 1.149615  [   64/  126]
train() client id: f_00000-2-2 loss: 1.241463  [   96/  126]
train() client id: f_00000-3-0 loss: 1.152716  [   32/  126]
train() client id: f_00000-3-1 loss: 1.247669  [   64/  126]
train() client id: f_00000-3-2 loss: 1.120332  [   96/  126]
train() client id: f_00000-4-0 loss: 1.225935  [   32/  126]
train() client id: f_00000-4-1 loss: 1.126706  [   64/  126]
train() client id: f_00000-4-2 loss: 1.005715  [   96/  126]
train() client id: f_00000-5-0 loss: 1.025275  [   32/  126]
train() client id: f_00000-5-1 loss: 1.019530  [   64/  126]
train() client id: f_00000-5-2 loss: 1.001881  [   96/  126]
train() client id: f_00000-6-0 loss: 0.988068  [   32/  126]
train() client id: f_00000-6-1 loss: 0.943121  [   64/  126]
train() client id: f_00000-6-2 loss: 1.024178  [   96/  126]
train() client id: f_00000-7-0 loss: 0.956941  [   32/  126]
train() client id: f_00000-7-1 loss: 0.972096  [   64/  126]
train() client id: f_00000-7-2 loss: 0.936128  [   96/  126]
train() client id: f_00000-8-0 loss: 0.972363  [   32/  126]
train() client id: f_00000-8-1 loss: 0.932079  [   64/  126]
train() client id: f_00000-8-2 loss: 0.923567  [   96/  126]
train() client id: f_00000-9-0 loss: 0.949224  [   32/  126]
train() client id: f_00000-9-1 loss: 0.891140  [   64/  126]
train() client id: f_00000-9-2 loss: 0.976046  [   96/  126]
train() client id: f_00000-10-0 loss: 0.881937  [   32/  126]
train() client id: f_00000-10-1 loss: 0.949752  [   64/  126]
train() client id: f_00000-10-2 loss: 0.990872  [   96/  126]
train() client id: f_00000-11-0 loss: 0.962280  [   32/  126]
train() client id: f_00000-11-1 loss: 0.941113  [   64/  126]
train() client id: f_00000-11-2 loss: 0.934373  [   96/  126]
train() client id: f_00000-12-0 loss: 0.932289  [   32/  126]
train() client id: f_00000-12-1 loss: 0.958030  [   64/  126]
train() client id: f_00000-12-2 loss: 0.931130  [   96/  126]
train() client id: f_00001-0-0 loss: 0.640558  [   32/  265]
train() client id: f_00001-0-1 loss: 0.621884  [   64/  265]
train() client id: f_00001-0-2 loss: 0.638440  [   96/  265]
train() client id: f_00001-0-3 loss: 0.663923  [  128/  265]
train() client id: f_00001-0-4 loss: 0.570197  [  160/  265]
train() client id: f_00001-0-5 loss: 0.737100  [  192/  265]
train() client id: f_00001-0-6 loss: 0.627580  [  224/  265]
train() client id: f_00001-0-7 loss: 0.615381  [  256/  265]
train() client id: f_00001-1-0 loss: 0.598131  [   32/  265]
train() client id: f_00001-1-1 loss: 0.639898  [   64/  265]
train() client id: f_00001-1-2 loss: 0.559110  [   96/  265]
train() client id: f_00001-1-3 loss: 0.532540  [  128/  265]
train() client id: f_00001-1-4 loss: 0.665322  [  160/  265]
train() client id: f_00001-1-5 loss: 0.539505  [  192/  265]
train() client id: f_00001-1-6 loss: 0.692272  [  224/  265]
train() client id: f_00001-1-7 loss: 0.603238  [  256/  265]
train() client id: f_00001-2-0 loss: 0.605921  [   32/  265]
train() client id: f_00001-2-1 loss: 0.563359  [   64/  265]
train() client id: f_00001-2-2 loss: 0.615254  [   96/  265]
train() client id: f_00001-2-3 loss: 0.605562  [  128/  265]
train() client id: f_00001-2-4 loss: 0.578294  [  160/  265]
train() client id: f_00001-2-5 loss: 0.513360  [  192/  265]
train() client id: f_00001-2-6 loss: 0.653619  [  224/  265]
train() client id: f_00001-2-7 loss: 0.505778  [  256/  265]
train() client id: f_00001-3-0 loss: 0.472763  [   32/  265]
train() client id: f_00001-3-1 loss: 0.553342  [   64/  265]
train() client id: f_00001-3-2 loss: 0.657738  [   96/  265]
train() client id: f_00001-3-3 loss: 0.477653  [  128/  265]
train() client id: f_00001-3-4 loss: 0.489343  [  160/  265]
train() client id: f_00001-3-5 loss: 0.600993  [  192/  265]
train() client id: f_00001-3-6 loss: 0.666482  [  224/  265]
train() client id: f_00001-3-7 loss: 0.588470  [  256/  265]
train() client id: f_00001-4-0 loss: 0.569032  [   32/  265]
train() client id: f_00001-4-1 loss: 0.577441  [   64/  265]
train() client id: f_00001-4-2 loss: 0.573709  [   96/  265]
train() client id: f_00001-4-3 loss: 0.470165  [  128/  265]
train() client id: f_00001-4-4 loss: 0.619947  [  160/  265]
train() client id: f_00001-4-5 loss: 0.511760  [  192/  265]
train() client id: f_00001-4-6 loss: 0.475648  [  224/  265]
train() client id: f_00001-4-7 loss: 0.590989  [  256/  265]
train() client id: f_00001-5-0 loss: 0.657567  [   32/  265]
train() client id: f_00001-5-1 loss: 0.458344  [   64/  265]
train() client id: f_00001-5-2 loss: 0.571110  [   96/  265]
train() client id: f_00001-5-3 loss: 0.478045  [  128/  265]
train() client id: f_00001-5-4 loss: 0.636559  [  160/  265]
train() client id: f_00001-5-5 loss: 0.548061  [  192/  265]
train() client id: f_00001-5-6 loss: 0.518072  [  224/  265]
train() client id: f_00001-5-7 loss: 0.452556  [  256/  265]
train() client id: f_00001-6-0 loss: 0.499304  [   32/  265]
train() client id: f_00001-6-1 loss: 0.499913  [   64/  265]
train() client id: f_00001-6-2 loss: 0.509251  [   96/  265]
train() client id: f_00001-6-3 loss: 0.461142  [  128/  265]
train() client id: f_00001-6-4 loss: 0.542599  [  160/  265]
train() client id: f_00001-6-5 loss: 0.565001  [  192/  265]
train() client id: f_00001-6-6 loss: 0.570282  [  224/  265]
train() client id: f_00001-6-7 loss: 0.558695  [  256/  265]
train() client id: f_00001-7-0 loss: 0.559908  [   32/  265]
train() client id: f_00001-7-1 loss: 0.544859  [   64/  265]
train() client id: f_00001-7-2 loss: 0.492146  [   96/  265]
train() client id: f_00001-7-3 loss: 0.437573  [  128/  265]
train() client id: f_00001-7-4 loss: 0.521394  [  160/  265]
train() client id: f_00001-7-5 loss: 0.530981  [  192/  265]
train() client id: f_00001-7-6 loss: 0.532377  [  224/  265]
train() client id: f_00001-7-7 loss: 0.617732  [  256/  265]
train() client id: f_00001-8-0 loss: 0.546001  [   32/  265]
train() client id: f_00001-8-1 loss: 0.506176  [   64/  265]
train() client id: f_00001-8-2 loss: 0.551369  [   96/  265]
train() client id: f_00001-8-3 loss: 0.532136  [  128/  265]
train() client id: f_00001-8-4 loss: 0.572935  [  160/  265]
train() client id: f_00001-8-5 loss: 0.498717  [  192/  265]
train() client id: f_00001-8-6 loss: 0.434442  [  224/  265]
train() client id: f_00001-8-7 loss: 0.565556  [  256/  265]
train() client id: f_00001-9-0 loss: 0.505220  [   32/  265]
train() client id: f_00001-9-1 loss: 0.534869  [   64/  265]
train() client id: f_00001-9-2 loss: 0.623550  [   96/  265]
train() client id: f_00001-9-3 loss: 0.437547  [  128/  265]
train() client id: f_00001-9-4 loss: 0.452416  [  160/  265]
train() client id: f_00001-9-5 loss: 0.433586  [  192/  265]
train() client id: f_00001-9-6 loss: 0.500822  [  224/  265]
train() client id: f_00001-9-7 loss: 0.651914  [  256/  265]
train() client id: f_00001-10-0 loss: 0.443303  [   32/  265]
train() client id: f_00001-10-1 loss: 0.593104  [   64/  265]
train() client id: f_00001-10-2 loss: 0.557431  [   96/  265]
train() client id: f_00001-10-3 loss: 0.422475  [  128/  265]
train() client id: f_00001-10-4 loss: 0.685254  [  160/  265]
train() client id: f_00001-10-5 loss: 0.444447  [  192/  265]
train() client id: f_00001-10-6 loss: 0.480039  [  224/  265]
train() client id: f_00001-10-7 loss: 0.546865  [  256/  265]
train() client id: f_00001-11-0 loss: 0.553613  [   32/  265]
train() client id: f_00001-11-1 loss: 0.542634  [   64/  265]
train() client id: f_00001-11-2 loss: 0.502541  [   96/  265]
train() client id: f_00001-11-3 loss: 0.447763  [  128/  265]
train() client id: f_00001-11-4 loss: 0.559215  [  160/  265]
train() client id: f_00001-11-5 loss: 0.545300  [  192/  265]
train() client id: f_00001-11-6 loss: 0.551937  [  224/  265]
train() client id: f_00001-11-7 loss: 0.433783  [  256/  265]
train() client id: f_00001-12-0 loss: 0.496771  [   32/  265]
train() client id: f_00001-12-1 loss: 0.507303  [   64/  265]
train() client id: f_00001-12-2 loss: 0.545980  [   96/  265]
train() client id: f_00001-12-3 loss: 0.611659  [  128/  265]
train() client id: f_00001-12-4 loss: 0.501202  [  160/  265]
train() client id: f_00001-12-5 loss: 0.493016  [  192/  265]
train() client id: f_00001-12-6 loss: 0.481011  [  224/  265]
train() client id: f_00001-12-7 loss: 0.540889  [  256/  265]
train() client id: f_00002-0-0 loss: 1.204008  [   32/  124]
train() client id: f_00002-0-1 loss: 1.268030  [   64/  124]
train() client id: f_00002-0-2 loss: 1.157349  [   96/  124]
train() client id: f_00002-1-0 loss: 1.116796  [   32/  124]
train() client id: f_00002-1-1 loss: 1.172537  [   64/  124]
train() client id: f_00002-1-2 loss: 1.136192  [   96/  124]
train() client id: f_00002-2-0 loss: 1.117872  [   32/  124]
train() client id: f_00002-2-1 loss: 1.111132  [   64/  124]
train() client id: f_00002-2-2 loss: 1.117757  [   96/  124]
train() client id: f_00002-3-0 loss: 1.141450  [   32/  124]
train() client id: f_00002-3-1 loss: 1.110913  [   64/  124]
train() client id: f_00002-3-2 loss: 0.998702  [   96/  124]
train() client id: f_00002-4-0 loss: 1.114227  [   32/  124]
train() client id: f_00002-4-1 loss: 1.070067  [   64/  124]
train() client id: f_00002-4-2 loss: 0.998122  [   96/  124]
train() client id: f_00002-5-0 loss: 1.008235  [   32/  124]
train() client id: f_00002-5-1 loss: 1.112398  [   64/  124]
train() client id: f_00002-5-2 loss: 1.031085  [   96/  124]
train() client id: f_00002-6-0 loss: 1.112754  [   32/  124]
train() client id: f_00002-6-1 loss: 1.004600  [   64/  124]
train() client id: f_00002-6-2 loss: 0.996794  [   96/  124]
train() client id: f_00002-7-0 loss: 0.954345  [   32/  124]
train() client id: f_00002-7-1 loss: 1.038389  [   64/  124]
train() client id: f_00002-7-2 loss: 1.031271  [   96/  124]
train() client id: f_00002-8-0 loss: 1.008567  [   32/  124]
train() client id: f_00002-8-1 loss: 0.975929  [   64/  124]
train() client id: f_00002-8-2 loss: 1.097811  [   96/  124]
train() client id: f_00002-9-0 loss: 1.066673  [   32/  124]
train() client id: f_00002-9-1 loss: 1.130726  [   64/  124]
train() client id: f_00002-9-2 loss: 0.854937  [   96/  124]
train() client id: f_00002-10-0 loss: 0.915075  [   32/  124]
train() client id: f_00002-10-1 loss: 1.136951  [   64/  124]
train() client id: f_00002-10-2 loss: 0.916950  [   96/  124]
train() client id: f_00002-11-0 loss: 1.170553  [   32/  124]
train() client id: f_00002-11-1 loss: 0.943544  [   64/  124]
train() client id: f_00002-11-2 loss: 0.946113  [   96/  124]
train() client id: f_00002-12-0 loss: 0.963883  [   32/  124]
train() client id: f_00002-12-1 loss: 1.067016  [   64/  124]
train() client id: f_00002-12-2 loss: 0.988642  [   96/  124]
train() client id: f_00003-0-0 loss: 0.898388  [   32/   43]
train() client id: f_00003-1-0 loss: 0.868946  [   32/   43]
train() client id: f_00003-2-0 loss: 0.813676  [   32/   43]
train() client id: f_00003-3-0 loss: 0.754420  [   32/   43]
train() client id: f_00003-4-0 loss: 0.860575  [   32/   43]
train() client id: f_00003-5-0 loss: 0.797520  [   32/   43]
train() client id: f_00003-6-0 loss: 0.768073  [   32/   43]
train() client id: f_00003-7-0 loss: 0.852484  [   32/   43]
train() client id: f_00003-8-0 loss: 0.805555  [   32/   43]
train() client id: f_00003-9-0 loss: 0.876953  [   32/   43]
train() client id: f_00003-10-0 loss: 0.832276  [   32/   43]
train() client id: f_00003-11-0 loss: 0.870693  [   32/   43]
train() client id: f_00003-12-0 loss: 0.911510  [   32/   43]
train() client id: f_00004-0-0 loss: 0.926081  [   32/  306]
train() client id: f_00004-0-1 loss: 0.790097  [   64/  306]
train() client id: f_00004-0-2 loss: 0.996083  [   96/  306]
train() client id: f_00004-0-3 loss: 0.915694  [  128/  306]
train() client id: f_00004-0-4 loss: 0.802895  [  160/  306]
train() client id: f_00004-0-5 loss: 0.773088  [  192/  306]
train() client id: f_00004-0-6 loss: 0.961657  [  224/  306]
train() client id: f_00004-0-7 loss: 0.819637  [  256/  306]
train() client id: f_00004-0-8 loss: 0.955484  [  288/  306]
train() client id: f_00004-1-0 loss: 0.893097  [   32/  306]
train() client id: f_00004-1-1 loss: 0.917269  [   64/  306]
train() client id: f_00004-1-2 loss: 0.943591  [   96/  306]
train() client id: f_00004-1-3 loss: 0.808479  [  128/  306]
train() client id: f_00004-1-4 loss: 0.797703  [  160/  306]
train() client id: f_00004-1-5 loss: 0.839831  [  192/  306]
train() client id: f_00004-1-6 loss: 0.946961  [  224/  306]
train() client id: f_00004-1-7 loss: 0.928783  [  256/  306]
train() client id: f_00004-1-8 loss: 0.818799  [  288/  306]
train() client id: f_00004-2-0 loss: 0.734057  [   32/  306]
train() client id: f_00004-2-1 loss: 0.871646  [   64/  306]
train() client id: f_00004-2-2 loss: 0.901586  [   96/  306]
train() client id: f_00004-2-3 loss: 0.799277  [  128/  306]
train() client id: f_00004-2-4 loss: 0.921671  [  160/  306]
train() client id: f_00004-2-5 loss: 0.809752  [  192/  306]
train() client id: f_00004-2-6 loss: 0.903385  [  224/  306]
train() client id: f_00004-2-7 loss: 1.028662  [  256/  306]
train() client id: f_00004-2-8 loss: 0.836632  [  288/  306]
train() client id: f_00004-3-0 loss: 0.893719  [   32/  306]
train() client id: f_00004-3-1 loss: 0.765565  [   64/  306]
train() client id: f_00004-3-2 loss: 0.902135  [   96/  306]
train() client id: f_00004-3-3 loss: 0.766215  [  128/  306]
train() client id: f_00004-3-4 loss: 0.921027  [  160/  306]
train() client id: f_00004-3-5 loss: 0.808837  [  192/  306]
train() client id: f_00004-3-6 loss: 0.915673  [  224/  306]
train() client id: f_00004-3-7 loss: 0.911769  [  256/  306]
train() client id: f_00004-3-8 loss: 1.010431  [  288/  306]
train() client id: f_00004-4-0 loss: 1.007264  [   32/  306]
train() client id: f_00004-4-1 loss: 0.719099  [   64/  306]
train() client id: f_00004-4-2 loss: 0.924248  [   96/  306]
train() client id: f_00004-4-3 loss: 0.890350  [  128/  306]
train() client id: f_00004-4-4 loss: 0.824835  [  160/  306]
train() client id: f_00004-4-5 loss: 0.990761  [  192/  306]
train() client id: f_00004-4-6 loss: 0.742470  [  224/  306]
train() client id: f_00004-4-7 loss: 0.884347  [  256/  306]
train() client id: f_00004-4-8 loss: 0.934353  [  288/  306]
train() client id: f_00004-5-0 loss: 0.943306  [   32/  306]
train() client id: f_00004-5-1 loss: 0.952536  [   64/  306]
train() client id: f_00004-5-2 loss: 0.724208  [   96/  306]
train() client id: f_00004-5-3 loss: 0.994169  [  128/  306]
train() client id: f_00004-5-4 loss: 0.880696  [  160/  306]
train() client id: f_00004-5-5 loss: 0.790493  [  192/  306]
train() client id: f_00004-5-6 loss: 0.803937  [  224/  306]
train() client id: f_00004-5-7 loss: 0.946124  [  256/  306]
train() client id: f_00004-5-8 loss: 0.890157  [  288/  306]
train() client id: f_00004-6-0 loss: 1.116520  [   32/  306]
train() client id: f_00004-6-1 loss: 0.922976  [   64/  306]
train() client id: f_00004-6-2 loss: 0.809030  [   96/  306]
train() client id: f_00004-6-3 loss: 0.951427  [  128/  306]
train() client id: f_00004-6-4 loss: 0.816466  [  160/  306]
train() client id: f_00004-6-5 loss: 0.763496  [  192/  306]
train() client id: f_00004-6-6 loss: 0.830285  [  224/  306]
train() client id: f_00004-6-7 loss: 0.790799  [  256/  306]
train() client id: f_00004-6-8 loss: 0.810982  [  288/  306]
train() client id: f_00004-7-0 loss: 0.776869  [   32/  306]
train() client id: f_00004-7-1 loss: 0.862346  [   64/  306]
train() client id: f_00004-7-2 loss: 0.914673  [   96/  306]
train() client id: f_00004-7-3 loss: 0.821456  [  128/  306]
train() client id: f_00004-7-4 loss: 0.870106  [  160/  306]
train() client id: f_00004-7-5 loss: 0.761146  [  192/  306]
train() client id: f_00004-7-6 loss: 0.840584  [  224/  306]
train() client id: f_00004-7-7 loss: 0.918538  [  256/  306]
train() client id: f_00004-7-8 loss: 0.998037  [  288/  306]
train() client id: f_00004-8-0 loss: 0.800881  [   32/  306]
train() client id: f_00004-8-1 loss: 0.705954  [   64/  306]
train() client id: f_00004-8-2 loss: 0.834927  [   96/  306]
train() client id: f_00004-8-3 loss: 0.977241  [  128/  306]
train() client id: f_00004-8-4 loss: 0.950930  [  160/  306]
train() client id: f_00004-8-5 loss: 0.981537  [  192/  306]
train() client id: f_00004-8-6 loss: 0.878543  [  224/  306]
train() client id: f_00004-8-7 loss: 0.850466  [  256/  306]
train() client id: f_00004-8-8 loss: 0.809985  [  288/  306]
train() client id: f_00004-9-0 loss: 1.016340  [   32/  306]
train() client id: f_00004-9-1 loss: 0.795342  [   64/  306]
train() client id: f_00004-9-2 loss: 0.895454  [   96/  306]
train() client id: f_00004-9-3 loss: 0.976229  [  128/  306]
train() client id: f_00004-9-4 loss: 0.714381  [  160/  306]
train() client id: f_00004-9-5 loss: 0.875468  [  192/  306]
train() client id: f_00004-9-6 loss: 0.907822  [  224/  306]
train() client id: f_00004-9-7 loss: 0.782986  [  256/  306]
train() client id: f_00004-9-8 loss: 0.902511  [  288/  306]
train() client id: f_00004-10-0 loss: 0.918383  [   32/  306]
train() client id: f_00004-10-1 loss: 0.928628  [   64/  306]
train() client id: f_00004-10-2 loss: 0.848179  [   96/  306]
train() client id: f_00004-10-3 loss: 0.814482  [  128/  306]
train() client id: f_00004-10-4 loss: 0.843744  [  160/  306]
train() client id: f_00004-10-5 loss: 0.825220  [  192/  306]
train() client id: f_00004-10-6 loss: 1.022477  [  224/  306]
train() client id: f_00004-10-7 loss: 0.746184  [  256/  306]
train() client id: f_00004-10-8 loss: 0.855224  [  288/  306]
train() client id: f_00004-11-0 loss: 0.931984  [   32/  306]
train() client id: f_00004-11-1 loss: 0.986729  [   64/  306]
train() client id: f_00004-11-2 loss: 0.857308  [   96/  306]
train() client id: f_00004-11-3 loss: 0.756952  [  128/  306]
train() client id: f_00004-11-4 loss: 0.903112  [  160/  306]
train() client id: f_00004-11-5 loss: 0.883424  [  192/  306]
train() client id: f_00004-11-6 loss: 0.797280  [  224/  306]
train() client id: f_00004-11-7 loss: 0.925808  [  256/  306]
train() client id: f_00004-11-8 loss: 0.892277  [  288/  306]
train() client id: f_00004-12-0 loss: 0.833176  [   32/  306]
train() client id: f_00004-12-1 loss: 0.895230  [   64/  306]
train() client id: f_00004-12-2 loss: 0.795468  [   96/  306]
train() client id: f_00004-12-3 loss: 0.826403  [  128/  306]
train() client id: f_00004-12-4 loss: 0.837865  [  160/  306]
train() client id: f_00004-12-5 loss: 0.965393  [  192/  306]
train() client id: f_00004-12-6 loss: 0.879266  [  224/  306]
train() client id: f_00004-12-7 loss: 0.882477  [  256/  306]
train() client id: f_00004-12-8 loss: 0.951999  [  288/  306]
train() client id: f_00005-0-0 loss: 0.830669  [   32/  146]
train() client id: f_00005-0-1 loss: 0.808278  [   64/  146]
train() client id: f_00005-0-2 loss: 0.877140  [   96/  146]
train() client id: f_00005-0-3 loss: 0.900879  [  128/  146]
train() client id: f_00005-1-0 loss: 0.874822  [   32/  146]
train() client id: f_00005-1-1 loss: 0.845943  [   64/  146]
train() client id: f_00005-1-2 loss: 0.871144  [   96/  146]
train() client id: f_00005-1-3 loss: 0.740673  [  128/  146]
train() client id: f_00005-2-0 loss: 0.721285  [   32/  146]
train() client id: f_00005-2-1 loss: 0.938383  [   64/  146]
train() client id: f_00005-2-2 loss: 0.815262  [   96/  146]
train() client id: f_00005-2-3 loss: 0.783177  [  128/  146]
train() client id: f_00005-3-0 loss: 0.753755  [   32/  146]
train() client id: f_00005-3-1 loss: 0.834023  [   64/  146]
train() client id: f_00005-3-2 loss: 0.809944  [   96/  146]
train() client id: f_00005-3-3 loss: 0.888743  [  128/  146]
train() client id: f_00005-4-0 loss: 0.723845  [   32/  146]
train() client id: f_00005-4-1 loss: 0.798751  [   64/  146]
train() client id: f_00005-4-2 loss: 0.915876  [   96/  146]
train() client id: f_00005-4-3 loss: 0.825719  [  128/  146]
train() client id: f_00005-5-0 loss: 0.894621  [   32/  146]
train() client id: f_00005-5-1 loss: 0.885998  [   64/  146]
train() client id: f_00005-5-2 loss: 0.678815  [   96/  146]
train() client id: f_00005-5-3 loss: 0.850729  [  128/  146]
train() client id: f_00005-6-0 loss: 0.826986  [   32/  146]
train() client id: f_00005-6-1 loss: 0.772818  [   64/  146]
train() client id: f_00005-6-2 loss: 0.800771  [   96/  146]
train() client id: f_00005-6-3 loss: 0.825537  [  128/  146]
train() client id: f_00005-7-0 loss: 0.742027  [   32/  146]
train() client id: f_00005-7-1 loss: 0.785220  [   64/  146]
train() client id: f_00005-7-2 loss: 0.762614  [   96/  146]
train() client id: f_00005-7-3 loss: 0.863075  [  128/  146]
train() client id: f_00005-8-0 loss: 0.792039  [   32/  146]
train() client id: f_00005-8-1 loss: 0.809098  [   64/  146]
train() client id: f_00005-8-2 loss: 0.723869  [   96/  146]
train() client id: f_00005-8-3 loss: 0.834566  [  128/  146]
train() client id: f_00005-9-0 loss: 0.790966  [   32/  146]
train() client id: f_00005-9-1 loss: 0.781574  [   64/  146]
train() client id: f_00005-9-2 loss: 0.806496  [   96/  146]
train() client id: f_00005-9-3 loss: 0.762929  [  128/  146]
train() client id: f_00005-10-0 loss: 0.895768  [   32/  146]
train() client id: f_00005-10-1 loss: 0.682438  [   64/  146]
train() client id: f_00005-10-2 loss: 0.733817  [   96/  146]
train() client id: f_00005-10-3 loss: 0.745376  [  128/  146]
train() client id: f_00005-11-0 loss: 0.765439  [   32/  146]
train() client id: f_00005-11-1 loss: 0.672755  [   64/  146]
train() client id: f_00005-11-2 loss: 0.961483  [   96/  146]
train() client id: f_00005-11-3 loss: 0.820650  [  128/  146]
train() client id: f_00005-12-0 loss: 0.852767  [   32/  146]
train() client id: f_00005-12-1 loss: 0.831541  [   64/  146]
train() client id: f_00005-12-2 loss: 0.818952  [   96/  146]
train() client id: f_00005-12-3 loss: 0.812051  [  128/  146]
train() client id: f_00006-0-0 loss: 0.948315  [   32/   54]
train() client id: f_00006-1-0 loss: 0.910731  [   32/   54]
train() client id: f_00006-2-0 loss: 0.924245  [   32/   54]
train() client id: f_00006-3-0 loss: 0.888714  [   32/   54]
train() client id: f_00006-4-0 loss: 0.951195  [   32/   54]
train() client id: f_00006-5-0 loss: 0.927944  [   32/   54]
train() client id: f_00006-6-0 loss: 0.936053  [   32/   54]
train() client id: f_00006-7-0 loss: 0.918002  [   32/   54]
train() client id: f_00006-8-0 loss: 0.923353  [   32/   54]
train() client id: f_00006-9-0 loss: 0.890756  [   32/   54]
train() client id: f_00006-10-0 loss: 0.904558  [   32/   54]
train() client id: f_00006-11-0 loss: 0.918231  [   32/   54]
train() client id: f_00006-12-0 loss: 0.886477  [   32/   54]
train() client id: f_00007-0-0 loss: 0.724965  [   32/  179]
train() client id: f_00007-0-1 loss: 0.654706  [   64/  179]
train() client id: f_00007-0-2 loss: 0.772846  [   96/  179]
train() client id: f_00007-0-3 loss: 0.680791  [  128/  179]
train() client id: f_00007-0-4 loss: 0.688722  [  160/  179]
train() client id: f_00007-1-0 loss: 0.652005  [   32/  179]
train() client id: f_00007-1-1 loss: 0.771329  [   64/  179]
train() client id: f_00007-1-2 loss: 0.687919  [   96/  179]
train() client id: f_00007-1-3 loss: 0.673880  [  128/  179]
train() client id: f_00007-1-4 loss: 0.610984  [  160/  179]
train() client id: f_00007-2-0 loss: 0.605622  [   32/  179]
train() client id: f_00007-2-1 loss: 0.709329  [   64/  179]
train() client id: f_00007-2-2 loss: 0.580508  [   96/  179]
train() client id: f_00007-2-3 loss: 0.694978  [  128/  179]
train() client id: f_00007-2-4 loss: 0.621106  [  160/  179]
train() client id: f_00007-3-0 loss: 0.573097  [   32/  179]
train() client id: f_00007-3-1 loss: 0.685244  [   64/  179]
train() client id: f_00007-3-2 loss: 0.674831  [   96/  179]
train() client id: f_00007-3-3 loss: 0.678371  [  128/  179]
train() client id: f_00007-3-4 loss: 0.676610  [  160/  179]
train() client id: f_00007-4-0 loss: 0.572003  [   32/  179]
train() client id: f_00007-4-1 loss: 0.728205  [   64/  179]
train() client id: f_00007-4-2 loss: 0.616704  [   96/  179]
train() client id: f_00007-4-3 loss: 0.604531  [  128/  179]
train() client id: f_00007-4-4 loss: 0.722879  [  160/  179]
train() client id: f_00007-5-0 loss: 0.661873  [   32/  179]
train() client id: f_00007-5-1 loss: 0.612587  [   64/  179]
train() client id: f_00007-5-2 loss: 0.550272  [   96/  179]
train() client id: f_00007-5-3 loss: 0.662807  [  128/  179]
train() client id: f_00007-5-4 loss: 0.712784  [  160/  179]
train() client id: f_00007-6-0 loss: 0.618953  [   32/  179]
train() client id: f_00007-6-1 loss: 0.550832  [   64/  179]
train() client id: f_00007-6-2 loss: 0.698954  [   96/  179]
train() client id: f_00007-6-3 loss: 0.578408  [  128/  179]
train() client id: f_00007-6-4 loss: 0.599665  [  160/  179]
train() client id: f_00007-7-0 loss: 0.630875  [   32/  179]
train() client id: f_00007-7-1 loss: 0.595532  [   64/  179]
train() client id: f_00007-7-2 loss: 0.537363  [   96/  179]
train() client id: f_00007-7-3 loss: 0.647276  [  128/  179]
train() client id: f_00007-7-4 loss: 0.716861  [  160/  179]
train() client id: f_00007-8-0 loss: 0.529245  [   32/  179]
train() client id: f_00007-8-1 loss: 0.654258  [   64/  179]
train() client id: f_00007-8-2 loss: 0.615020  [   96/  179]
train() client id: f_00007-8-3 loss: 0.532242  [  128/  179]
train() client id: f_00007-8-4 loss: 0.644149  [  160/  179]
train() client id: f_00007-9-0 loss: 0.639141  [   32/  179]
train() client id: f_00007-9-1 loss: 0.660226  [   64/  179]
train() client id: f_00007-9-2 loss: 0.502108  [   96/  179]
train() client id: f_00007-9-3 loss: 0.586056  [  128/  179]
train() client id: f_00007-9-4 loss: 0.579198  [  160/  179]
train() client id: f_00007-10-0 loss: 0.703965  [   32/  179]
train() client id: f_00007-10-1 loss: 0.554811  [   64/  179]
train() client id: f_00007-10-2 loss: 0.501370  [   96/  179]
train() client id: f_00007-10-3 loss: 0.584178  [  128/  179]
train() client id: f_00007-10-4 loss: 0.582197  [  160/  179]
train() client id: f_00007-11-0 loss: 0.849443  [   32/  179]
train() client id: f_00007-11-1 loss: 0.562363  [   64/  179]
train() client id: f_00007-11-2 loss: 0.639100  [   96/  179]
train() client id: f_00007-11-3 loss: 0.509156  [  128/  179]
train() client id: f_00007-11-4 loss: 0.523901  [  160/  179]
train() client id: f_00007-12-0 loss: 0.642726  [   32/  179]
train() client id: f_00007-12-1 loss: 0.630356  [   64/  179]
train() client id: f_00007-12-2 loss: 0.589530  [   96/  179]
train() client id: f_00007-12-3 loss: 0.561814  [  128/  179]
train() client id: f_00007-12-4 loss: 0.575759  [  160/  179]
train() client id: f_00008-0-0 loss: 0.935920  [   32/  130]
train() client id: f_00008-0-1 loss: 0.823462  [   64/  130]
train() client id: f_00008-0-2 loss: 0.787696  [   96/  130]
train() client id: f_00008-0-3 loss: 0.850072  [  128/  130]
train() client id: f_00008-1-0 loss: 0.785086  [   32/  130]
train() client id: f_00008-1-1 loss: 0.882938  [   64/  130]
train() client id: f_00008-1-2 loss: 0.887803  [   96/  130]
train() client id: f_00008-1-3 loss: 0.837715  [  128/  130]
train() client id: f_00008-2-0 loss: 0.812525  [   32/  130]
train() client id: f_00008-2-1 loss: 0.772092  [   64/  130]
train() client id: f_00008-2-2 loss: 0.943886  [   96/  130]
train() client id: f_00008-2-3 loss: 0.858230  [  128/  130]
train() client id: f_00008-3-0 loss: 0.811937  [   32/  130]
train() client id: f_00008-3-1 loss: 0.855673  [   64/  130]
train() client id: f_00008-3-2 loss: 0.853543  [   96/  130]
train() client id: f_00008-3-3 loss: 0.863492  [  128/  130]
train() client id: f_00008-4-0 loss: 0.849724  [   32/  130]
train() client id: f_00008-4-1 loss: 0.859480  [   64/  130]
train() client id: f_00008-4-2 loss: 0.869661  [   96/  130]
train() client id: f_00008-4-3 loss: 0.799228  [  128/  130]
train() client id: f_00008-5-0 loss: 0.771044  [   32/  130]
train() client id: f_00008-5-1 loss: 0.894422  [   64/  130]
train() client id: f_00008-5-2 loss: 0.821748  [   96/  130]
train() client id: f_00008-5-3 loss: 0.863559  [  128/  130]
train() client id: f_00008-6-0 loss: 0.892897  [   32/  130]
train() client id: f_00008-6-1 loss: 0.788129  [   64/  130]
train() client id: f_00008-6-2 loss: 0.886540  [   96/  130]
train() client id: f_00008-6-3 loss: 0.815350  [  128/  130]
train() client id: f_00008-7-0 loss: 0.927726  [   32/  130]
train() client id: f_00008-7-1 loss: 0.806007  [   64/  130]
train() client id: f_00008-7-2 loss: 0.829897  [   96/  130]
train() client id: f_00008-7-3 loss: 0.814250  [  128/  130]
train() client id: f_00008-8-0 loss: 0.800129  [   32/  130]
train() client id: f_00008-8-1 loss: 0.892175  [   64/  130]
train() client id: f_00008-8-2 loss: 0.823579  [   96/  130]
train() client id: f_00008-8-3 loss: 0.796055  [  128/  130]
train() client id: f_00008-9-0 loss: 0.809402  [   32/  130]
train() client id: f_00008-9-1 loss: 0.788421  [   64/  130]
train() client id: f_00008-9-2 loss: 0.803320  [   96/  130]
train() client id: f_00008-9-3 loss: 0.946896  [  128/  130]
train() client id: f_00008-10-0 loss: 0.734388  [   32/  130]
train() client id: f_00008-10-1 loss: 0.821365  [   64/  130]
train() client id: f_00008-10-2 loss: 0.842036  [   96/  130]
train() client id: f_00008-10-3 loss: 0.954429  [  128/  130]
train() client id: f_00008-11-0 loss: 0.846386  [   32/  130]
train() client id: f_00008-11-1 loss: 0.836566  [   64/  130]
train() client id: f_00008-11-2 loss: 0.826623  [   96/  130]
train() client id: f_00008-11-3 loss: 0.858629  [  128/  130]
train() client id: f_00008-12-0 loss: 0.893011  [   32/  130]
train() client id: f_00008-12-1 loss: 0.816794  [   64/  130]
train() client id: f_00008-12-2 loss: 0.760546  [   96/  130]
train() client id: f_00008-12-3 loss: 0.906519  [  128/  130]
train() client id: f_00009-0-0 loss: 1.204486  [   32/  118]
train() client id: f_00009-0-1 loss: 1.155261  [   64/  118]
train() client id: f_00009-0-2 loss: 1.165906  [   96/  118]
train() client id: f_00009-1-0 loss: 1.156959  [   32/  118]
train() client id: f_00009-1-1 loss: 1.160969  [   64/  118]
train() client id: f_00009-1-2 loss: 1.079328  [   96/  118]
train() client id: f_00009-2-0 loss: 1.127556  [   32/  118]
train() client id: f_00009-2-1 loss: 1.041499  [   64/  118]
train() client id: f_00009-2-2 loss: 1.100069  [   96/  118]
train() client id: f_00009-3-0 loss: 1.072605  [   32/  118]
train() client id: f_00009-3-1 loss: 1.077590  [   64/  118]
train() client id: f_00009-3-2 loss: 1.075838  [   96/  118]
train() client id: f_00009-4-0 loss: 1.124752  [   32/  118]
train() client id: f_00009-4-1 loss: 1.064196  [   64/  118]
train() client id: f_00009-4-2 loss: 1.020566  [   96/  118]
train() client id: f_00009-5-0 loss: 1.077434  [   32/  118]
train() client id: f_00009-5-1 loss: 1.071427  [   64/  118]
train() client id: f_00009-5-2 loss: 0.920713  [   96/  118]
train() client id: f_00009-6-0 loss: 0.966097  [   32/  118]
train() client id: f_00009-6-1 loss: 1.077193  [   64/  118]
train() client id: f_00009-6-2 loss: 0.988772  [   96/  118]
train() client id: f_00009-7-0 loss: 0.962023  [   32/  118]
train() client id: f_00009-7-1 loss: 0.962191  [   64/  118]
train() client id: f_00009-7-2 loss: 1.143374  [   96/  118]
train() client id: f_00009-8-0 loss: 0.969396  [   32/  118]
train() client id: f_00009-8-1 loss: 1.024461  [   64/  118]
train() client id: f_00009-8-2 loss: 0.962128  [   96/  118]
train() client id: f_00009-9-0 loss: 0.979992  [   32/  118]
train() client id: f_00009-9-1 loss: 1.012266  [   64/  118]
train() client id: f_00009-9-2 loss: 0.991502  [   96/  118]
train() client id: f_00009-10-0 loss: 1.054409  [   32/  118]
train() client id: f_00009-10-1 loss: 1.020875  [   64/  118]
train() client id: f_00009-10-2 loss: 0.949130  [   96/  118]
train() client id: f_00009-11-0 loss: 1.037950  [   32/  118]
train() client id: f_00009-11-1 loss: 1.040110  [   64/  118]
train() client id: f_00009-11-2 loss: 0.958872  [   96/  118]
train() client id: f_00009-12-0 loss: 0.901142  [   32/  118]
train() client id: f_00009-12-1 loss: 1.080673  [   64/  118]
train() client id: f_00009-12-2 loss: 0.970092  [   96/  118]
At round 4 accuracy: 0.6312997347480106
At round 4 training accuracy: 0.5774647887323944
At round 4 training loss: 0.8993246647713571
update_location
xs = [ -3.9056584    4.20031788  40.00902392  18.81129433 -14.02070377
  -1.04359014  -2.44319194  -6.32485185  24.66397685   2.93912145]
ys = [ 32.5879595   15.55583871   1.32061395  -2.45517586   9.35018685
 -17.18584926  -2.62498432  -4.17765202  17.56900603   4.00148178]
dists_uav = [105.24841696 101.28981582 107.71474373 101.78355802 101.41008889
 101.47138757 100.06427799 100.28687116 104.48436116 100.1231756 ]
dists_bs = [222.60831747 239.86495054 276.39357718 262.77967434 230.98526882
 259.22236244 247.64185241 246.08115854 254.2640783  246.78497559]
uav_gains = [8.79949509e-11 9.68464333e-11 8.30438549e-11 9.56761907e-11
 9.65595300e-11 9.64137637e-11 9.98391157e-11 9.92860273e-11
 8.96125391e-11 9.96923511e-11]
bs_gains = [2.95242043e-11 2.39544975e-11 1.61070436e-11 1.85539819e-11
 2.66230674e-11 1.92757456e-11 2.19071944e-11 2.22984497e-11
 2.03467974e-11 2.21208434e-11]
Round 5
-------------------------------
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.33284149 21.59560189 10.17342765  3.63728595 24.89615745 12.01211791
  4.52299688 14.602885   10.71870557  9.74625265]
obj_prev = 122.23827245410807
eta_min = 9.414027209246992e-10	eta_max = 0.9186608067299383
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 28.431400687067253	eta = 0.909090909090909
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 48.95841824133416	eta = 0.5279322499743717
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 39.18478015448109	eta = 0.6596114051281231
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 37.434409794184106	eta = 0.6904537306569067
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 37.34853700150222	eta = 0.6920412410342677
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 37.348315836191034	eta = 0.6920453390909805
eta = 0.6920453390909805
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [0.03037504 0.06388402 0.0298929  0.01036609 0.07376796 0.03519648
 0.01301788 0.04315185 0.03133933 0.02844647]
ene_total = [3.16534696 6.21953031 3.1298897  1.43707177 7.05111483 3.79361781
 1.66080998 4.25148272 3.45176733 3.18768443]
ti_comp = [0.28160493 0.26136622 0.28090865 0.2825856  0.2634578  0.25672527
 0.28307344 0.28301023 0.25792521 0.25972041]
ti_coms = [0.06451099 0.0847497  0.06520727 0.06353032 0.08265812 0.08939065
 0.06304248 0.06310569 0.08819071 0.08639551]
t_total = [29.74997902 29.74997902 29.74997902 29.74997902 29.74997902 29.74997902
 29.74997902 29.74997902 29.74997902 29.74997902]
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.20876875e-05 2.38538142e-04 2.11569937e-05 8.71815183e-07
 3.61460861e-04 4.13466617e-05 1.72068806e-06 6.27009232e-05
 2.89175573e-05 2.13281397e-05]
ene_total = [0.55639501 0.74895917 0.56229982 0.54614218 0.74154691 0.77190053
 0.54202196 0.54780676 0.76051829 0.74443548]
optimize_network iter = 0 obj = 6.522026104246858
eta = 0.6920453390909805
freqs = [5.39320171e+07 1.22211702e+08 5.32075115e+07 1.83414978e+07
 1.39999582e+08 6.85489188e+07 2.29938135e+07 7.62372573e+07
 6.07527495e+07 5.47636387e+07]
eta_min = 0.6764484296472902	eta_max = 0.6920453390909633
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 0.06328991478194519	eta = 0.9090909090909091
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 21.92072085717903	eta = 0.0026247442563715508
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 2.335363763739976	eta = 0.024636969648472683
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 2.25597545648311	eta = 0.025503950408706712
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 2.255940548856704	eta = 0.025504345047817744
eta = 0.025504345047817744
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.20148930e-04 2.37751991e-03 2.10872665e-04 8.68941936e-06
 3.60269593e-03 4.12103953e-04 1.71501718e-05 6.24942794e-04
 2.88222535e-04 2.12578485e-04]
ene_total = [0.1812087  0.29478227 0.18284802 0.17280114 0.32238    0.25400243
 0.17170585 0.1883868  0.24737814 0.2404472 ]
ti_comp = [0.29913458 0.27889587 0.2984383  0.30011526 0.28098745 0.27425492
 0.3006031  0.30053989 0.27545486 0.27725007]
ti_coms = [0.06451099 0.0847497  0.06520727 0.06353032 0.08265812 0.08939065
 0.06304248 0.06310569 0.08819071 0.08639551]
t_total = [29.74997902 29.74997902 29.74997902 29.74997902 29.74997902 29.74997902
 29.74997902 29.74997902 29.74997902 29.74997902]
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.34412342e-05 2.50873933e-04 2.24469855e-05 9.25616981e-07
 3.80533165e-04 4.33862178e-05 1.82724368e-06 6.65820168e-05
 3.03620624e-05 2.24132480e-05]
ene_total = [0.52968454 0.71386454 0.53529951 0.51981962 0.70736071 0.73485767
 0.51590233 0.52171707 0.72397543 0.70863849]
optimize_network iter = 1 obj = 6.211119913967213
eta = 0.6764484296472902
freqs = [5.39156214e+07 1.21622656e+08 5.31836109e+07 1.83396515e+07
 1.39394357e+08 6.81410966e+07 2.29938135e+07 7.62362643e+07
 6.04092728e+07 5.44779861e+07]
eta_min = 0.6764484296473243	eta_max = 0.6764484296472864
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 0.06277694578067981	eta = 0.909090909090909
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 21.92023194550156	eta = 0.002603528596394288
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 2.332999905489733	eta = 0.024462045872963308
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 2.2542173952325193	eta = 0.02531696846560011
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 2.2541832396466193	eta = 0.02531735207056884
eta = 0.02531735207056884
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.20456386e-04 2.35937920e-03 2.11105834e-04 8.70509514e-06
 3.57877769e-03 4.08031791e-04 1.71845703e-05 6.26179946e-04
 2.85544288e-04 2.10788545e-04]
ene_total = [0.18118229 0.29423306 0.18281927 0.17276841 0.32166859 0.25384311
 0.17167384 0.18838425 0.24725795 0.24035246]
ti_comp = [0.29913458 0.27889587 0.2984383  0.30011526 0.28098745 0.27425492
 0.3006031  0.30053989 0.27545486 0.27725007]
ti_coms = [0.06451099 0.0847497  0.06520727 0.06353032 0.08265812 0.08939065
 0.06304248 0.06310569 0.08819071 0.08639551]
t_total = [29.74997902 29.74997902 29.74997902 29.74997902 29.74997902 29.74997902
 29.74997902 29.74997902 29.74997902 29.74997902]
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.34412342e-05 2.50873933e-04 2.24469855e-05 9.25616981e-07
 3.80533165e-04 4.33862178e-05 1.82724368e-06 6.65820168e-05
 3.03620624e-05 2.24132480e-05]
ene_total = [0.52968454 0.71386454 0.53529951 0.51981962 0.70736071 0.73485767
 0.51590233 0.52171707 0.72397543 0.70863849]
optimize_network iter = 2 obj = 6.2111199139678615
eta = 0.6764484296473243
freqs = [5.39156214e+07 1.21622656e+08 5.31836109e+07 1.83396515e+07
 1.39394357e+08 6.81410966e+07 2.29938135e+07 7.62362643e+07
 6.04092728e+07 5.44779861e+07]
Done!
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.19761204e-05 2.35193919e-04 2.10440137e-05 8.67764468e-07
 3.56749245e-04 4.06745112e-05 1.71303809e-06 6.24205363e-05
 2.84643859e-05 2.10123849e-05]
ene_total = [0.00647308 0.00871016 0.00654177 0.0063539  0.00862256 0.00897974
 0.00630596 0.00637299 0.00884754 0.00866056]
At round 5 energy consumption: 0.07586826097698268
At round 5 eta: 0.6764484296473243
At round 5 a_n: 26.12732777443847
At round 5 local rounds: 12.80002140651856
At round 5 global rounds: 81.81036980458869
gradient difference: 0.4274906516075134
train() client id: f_00000-0-0 loss: 1.585705  [   32/  126]
train() client id: f_00000-0-1 loss: 1.769598  [   64/  126]
train() client id: f_00000-0-2 loss: 1.310725  [   96/  126]
train() client id: f_00000-1-0 loss: 1.184676  [   32/  126]
train() client id: f_00000-1-1 loss: 1.432229  [   64/  126]
train() client id: f_00000-1-2 loss: 1.561175  [   96/  126]
train() client id: f_00000-2-0 loss: 1.268597  [   32/  126]
train() client id: f_00000-2-1 loss: 1.316831  [   64/  126]
train() client id: f_00000-2-2 loss: 1.206980  [   96/  126]
train() client id: f_00000-3-0 loss: 1.215972  [   32/  126]
train() client id: f_00000-3-1 loss: 1.198032  [   64/  126]
train() client id: f_00000-3-2 loss: 1.245813  [   96/  126]
train() client id: f_00000-4-0 loss: 1.107866  [   32/  126]
train() client id: f_00000-4-1 loss: 1.176520  [   64/  126]
train() client id: f_00000-4-2 loss: 1.074389  [   96/  126]
train() client id: f_00000-5-0 loss: 1.081629  [   32/  126]
train() client id: f_00000-5-1 loss: 1.013491  [   64/  126]
train() client id: f_00000-5-2 loss: 1.031239  [   96/  126]
train() client id: f_00000-6-0 loss: 1.000421  [   32/  126]
train() client id: f_00000-6-1 loss: 1.057810  [   64/  126]
train() client id: f_00000-6-2 loss: 0.974400  [   96/  126]
train() client id: f_00000-7-0 loss: 1.044923  [   32/  126]
train() client id: f_00000-7-1 loss: 0.943759  [   64/  126]
train() client id: f_00000-7-2 loss: 0.928966  [   96/  126]
train() client id: f_00000-8-0 loss: 0.992524  [   32/  126]
train() client id: f_00000-8-1 loss: 0.970565  [   64/  126]
train() client id: f_00000-8-2 loss: 0.937412  [   96/  126]
train() client id: f_00000-9-0 loss: 0.940025  [   32/  126]
train() client id: f_00000-9-1 loss: 0.929742  [   64/  126]
train() client id: f_00000-9-2 loss: 1.019465  [   96/  126]
train() client id: f_00000-10-0 loss: 0.918608  [   32/  126]
train() client id: f_00000-10-1 loss: 0.994880  [   64/  126]
train() client id: f_00000-10-2 loss: 0.954886  [   96/  126]
train() client id: f_00000-11-0 loss: 0.931409  [   32/  126]
train() client id: f_00000-11-1 loss: 0.996350  [   64/  126]
train() client id: f_00000-11-2 loss: 0.900219  [   96/  126]
train() client id: f_00001-0-0 loss: 0.610262  [   32/  265]
train() client id: f_00001-0-1 loss: 0.696555  [   64/  265]
train() client id: f_00001-0-2 loss: 0.642468  [   96/  265]
train() client id: f_00001-0-3 loss: 0.678970  [  128/  265]
train() client id: f_00001-0-4 loss: 0.607583  [  160/  265]
train() client id: f_00001-0-5 loss: 0.586886  [  192/  265]
train() client id: f_00001-0-6 loss: 0.561440  [  224/  265]
train() client id: f_00001-0-7 loss: 0.575654  [  256/  265]
train() client id: f_00001-1-0 loss: 0.636202  [   32/  265]
train() client id: f_00001-1-1 loss: 0.646646  [   64/  265]
train() client id: f_00001-1-2 loss: 0.569598  [   96/  265]
train() client id: f_00001-1-3 loss: 0.664536  [  128/  265]
train() client id: f_00001-1-4 loss: 0.524908  [  160/  265]
train() client id: f_00001-1-5 loss: 0.614826  [  192/  265]
train() client id: f_00001-1-6 loss: 0.532298  [  224/  265]
train() client id: f_00001-1-7 loss: 0.569579  [  256/  265]
train() client id: f_00001-2-0 loss: 0.551808  [   32/  265]
train() client id: f_00001-2-1 loss: 0.729867  [   64/  265]
train() client id: f_00001-2-2 loss: 0.601864  [   96/  265]
train() client id: f_00001-2-3 loss: 0.566772  [  128/  265]
train() client id: f_00001-2-4 loss: 0.552571  [  160/  265]
train() client id: f_00001-2-5 loss: 0.503102  [  192/  265]
train() client id: f_00001-2-6 loss: 0.555495  [  224/  265]
train() client id: f_00001-2-7 loss: 0.542011  [  256/  265]
train() client id: f_00001-3-0 loss: 0.484443  [   32/  265]
train() client id: f_00001-3-1 loss: 0.603758  [   64/  265]
train() client id: f_00001-3-2 loss: 0.551211  [   96/  265]
train() client id: f_00001-3-3 loss: 0.465017  [  128/  265]
train() client id: f_00001-3-4 loss: 0.534965  [  160/  265]
train() client id: f_00001-3-5 loss: 0.577553  [  192/  265]
train() client id: f_00001-3-6 loss: 0.583537  [  224/  265]
train() client id: f_00001-3-7 loss: 0.645657  [  256/  265]
train() client id: f_00001-4-0 loss: 0.559355  [   32/  265]
train() client id: f_00001-4-1 loss: 0.543922  [   64/  265]
train() client id: f_00001-4-2 loss: 0.577715  [   96/  265]
train() client id: f_00001-4-3 loss: 0.525025  [  128/  265]
train() client id: f_00001-4-4 loss: 0.575111  [  160/  265]
train() client id: f_00001-4-5 loss: 0.514369  [  192/  265]
train() client id: f_00001-4-6 loss: 0.536558  [  224/  265]
train() client id: f_00001-4-7 loss: 0.536497  [  256/  265]
train() client id: f_00001-5-0 loss: 0.469257  [   32/  265]
train() client id: f_00001-5-1 loss: 0.526082  [   64/  265]
train() client id: f_00001-5-2 loss: 0.570082  [   96/  265]
train() client id: f_00001-5-3 loss: 0.467432  [  128/  265]
train() client id: f_00001-5-4 loss: 0.575099  [  160/  265]
train() client id: f_00001-5-5 loss: 0.580856  [  192/  265]
train() client id: f_00001-5-6 loss: 0.665636  [  224/  265]
train() client id: f_00001-5-7 loss: 0.463474  [  256/  265]
train() client id: f_00001-6-0 loss: 0.578536  [   32/  265]
train() client id: f_00001-6-1 loss: 0.516156  [   64/  265]
train() client id: f_00001-6-2 loss: 0.623501  [   96/  265]
train() client id: f_00001-6-3 loss: 0.515812  [  128/  265]
train() client id: f_00001-6-4 loss: 0.521803  [  160/  265]
train() client id: f_00001-6-5 loss: 0.518190  [  192/  265]
train() client id: f_00001-6-6 loss: 0.443635  [  224/  265]
train() client id: f_00001-6-7 loss: 0.555436  [  256/  265]
train() client id: f_00001-7-0 loss: 0.563882  [   32/  265]
train() client id: f_00001-7-1 loss: 0.627520  [   64/  265]
train() client id: f_00001-7-2 loss: 0.455321  [   96/  265]
train() client id: f_00001-7-3 loss: 0.497220  [  128/  265]
train() client id: f_00001-7-4 loss: 0.579257  [  160/  265]
train() client id: f_00001-7-5 loss: 0.571368  [  192/  265]
train() client id: f_00001-7-6 loss: 0.563566  [  224/  265]
train() client id: f_00001-7-7 loss: 0.448181  [  256/  265]
train() client id: f_00001-8-0 loss: 0.469987  [   32/  265]
train() client id: f_00001-8-1 loss: 0.559753  [   64/  265]
train() client id: f_00001-8-2 loss: 0.444465  [   96/  265]
train() client id: f_00001-8-3 loss: 0.619866  [  128/  265]
train() client id: f_00001-8-4 loss: 0.631320  [  160/  265]
train() client id: f_00001-8-5 loss: 0.489383  [  192/  265]
train() client id: f_00001-8-6 loss: 0.578153  [  224/  265]
train() client id: f_00001-8-7 loss: 0.503257  [  256/  265]
train() client id: f_00001-9-0 loss: 0.440282  [   32/  265]
train() client id: f_00001-9-1 loss: 0.684198  [   64/  265]
train() client id: f_00001-9-2 loss: 0.570475  [   96/  265]
train() client id: f_00001-9-3 loss: 0.505740  [  128/  265]
train() client id: f_00001-9-4 loss: 0.526788  [  160/  265]
train() client id: f_00001-9-5 loss: 0.493768  [  192/  265]
train() client id: f_00001-9-6 loss: 0.494946  [  224/  265]
train() client id: f_00001-9-7 loss: 0.504477  [  256/  265]
train() client id: f_00001-10-0 loss: 0.576272  [   32/  265]
train() client id: f_00001-10-1 loss: 0.511286  [   64/  265]
train() client id: f_00001-10-2 loss: 0.503947  [   96/  265]
train() client id: f_00001-10-3 loss: 0.666566  [  128/  265]
train() client id: f_00001-10-4 loss: 0.461239  [  160/  265]
train() client id: f_00001-10-5 loss: 0.512696  [  192/  265]
train() client id: f_00001-10-6 loss: 0.450940  [  224/  265]
train() client id: f_00001-10-7 loss: 0.601219  [  256/  265]
train() client id: f_00001-11-0 loss: 0.451990  [   32/  265]
train() client id: f_00001-11-1 loss: 0.529623  [   64/  265]
train() client id: f_00001-11-2 loss: 0.537387  [   96/  265]
train() client id: f_00001-11-3 loss: 0.484953  [  128/  265]
train() client id: f_00001-11-4 loss: 0.505244  [  160/  265]
train() client id: f_00001-11-5 loss: 0.548919  [  192/  265]
train() client id: f_00001-11-6 loss: 0.593154  [  224/  265]
train() client id: f_00001-11-7 loss: 0.632394  [  256/  265]
train() client id: f_00002-0-0 loss: 1.112913  [   32/  124]
train() client id: f_00002-0-1 loss: 1.227268  [   64/  124]
train() client id: f_00002-0-2 loss: 1.200027  [   96/  124]
train() client id: f_00002-1-0 loss: 1.093892  [   32/  124]
train() client id: f_00002-1-1 loss: 1.089889  [   64/  124]
train() client id: f_00002-1-2 loss: 1.156445  [   96/  124]
train() client id: f_00002-2-0 loss: 1.107574  [   32/  124]
train() client id: f_00002-2-1 loss: 1.159129  [   64/  124]
train() client id: f_00002-2-2 loss: 1.080056  [   96/  124]
train() client id: f_00002-3-0 loss: 1.206898  [   32/  124]
train() client id: f_00002-3-1 loss: 1.003350  [   64/  124]
train() client id: f_00002-3-2 loss: 1.069229  [   96/  124]
train() client id: f_00002-4-0 loss: 1.073813  [   32/  124]
train() client id: f_00002-4-1 loss: 1.035522  [   64/  124]
train() client id: f_00002-4-2 loss: 1.104492  [   96/  124]
train() client id: f_00002-5-0 loss: 0.992953  [   32/  124]
train() client id: f_00002-5-1 loss: 1.085495  [   64/  124]
train() client id: f_00002-5-2 loss: 0.977390  [   96/  124]
train() client id: f_00002-6-0 loss: 1.004840  [   32/  124]
train() client id: f_00002-6-1 loss: 1.056139  [   64/  124]
train() client id: f_00002-6-2 loss: 1.018421  [   96/  124]
train() client id: f_00002-7-0 loss: 1.054070  [   32/  124]
train() client id: f_00002-7-1 loss: 0.976830  [   64/  124]
train() client id: f_00002-7-2 loss: 0.950342  [   96/  124]
train() client id: f_00002-8-0 loss: 0.921820  [   32/  124]
train() client id: f_00002-8-1 loss: 1.049958  [   64/  124]
train() client id: f_00002-8-2 loss: 1.052244  [   96/  124]
train() client id: f_00002-9-0 loss: 0.984537  [   32/  124]
train() client id: f_00002-9-1 loss: 0.926527  [   64/  124]
train() client id: f_00002-9-2 loss: 0.868818  [   96/  124]
train() client id: f_00002-10-0 loss: 0.937356  [   32/  124]
train() client id: f_00002-10-1 loss: 1.059550  [   64/  124]
train() client id: f_00002-10-2 loss: 0.969898  [   96/  124]
train() client id: f_00002-11-0 loss: 1.018869  [   32/  124]
train() client id: f_00002-11-1 loss: 0.995669  [   64/  124]
train() client id: f_00002-11-2 loss: 0.963604  [   96/  124]
train() client id: f_00003-0-0 loss: 0.772531  [   32/   43]
train() client id: f_00003-1-0 loss: 0.941330  [   32/   43]
train() client id: f_00003-2-0 loss: 0.767574  [   32/   43]
train() client id: f_00003-3-0 loss: 0.852275  [   32/   43]
train() client id: f_00003-4-0 loss: 0.796723  [   32/   43]
train() client id: f_00003-5-0 loss: 0.852662  [   32/   43]
train() client id: f_00003-6-0 loss: 0.872339  [   32/   43]
train() client id: f_00003-7-0 loss: 0.916892  [   32/   43]
train() client id: f_00003-8-0 loss: 0.885911  [   32/   43]
train() client id: f_00003-9-0 loss: 0.878087  [   32/   43]
train() client id: f_00003-10-0 loss: 0.859284  [   32/   43]
train() client id: f_00003-11-0 loss: 0.846406  [   32/   43]
train() client id: f_00004-0-0 loss: 0.907032  [   32/  306]
train() client id: f_00004-0-1 loss: 0.616103  [   64/  306]
train() client id: f_00004-0-2 loss: 0.841624  [   96/  306]
train() client id: f_00004-0-3 loss: 0.836131  [  128/  306]
train() client id: f_00004-0-4 loss: 0.893246  [  160/  306]
train() client id: f_00004-0-5 loss: 0.865624  [  192/  306]
train() client id: f_00004-0-6 loss: 0.839538  [  224/  306]
train() client id: f_00004-0-7 loss: 0.788691  [  256/  306]
train() client id: f_00004-0-8 loss: 0.823537  [  288/  306]
train() client id: f_00004-1-0 loss: 0.988668  [   32/  306]
train() client id: f_00004-1-1 loss: 0.868989  [   64/  306]
train() client id: f_00004-1-2 loss: 0.894546  [   96/  306]
train() client id: f_00004-1-3 loss: 0.938968  [  128/  306]
train() client id: f_00004-1-4 loss: 0.722541  [  160/  306]
train() client id: f_00004-1-5 loss: 0.833679  [  192/  306]
train() client id: f_00004-1-6 loss: 0.730388  [  224/  306]
train() client id: f_00004-1-7 loss: 0.729831  [  256/  306]
train() client id: f_00004-1-8 loss: 0.837377  [  288/  306]
train() client id: f_00004-2-0 loss: 0.799464  [   32/  306]
train() client id: f_00004-2-1 loss: 0.815474  [   64/  306]
train() client id: f_00004-2-2 loss: 0.853845  [   96/  306]
train() client id: f_00004-2-3 loss: 0.823470  [  128/  306]
train() client id: f_00004-2-4 loss: 0.808672  [  160/  306]
train() client id: f_00004-2-5 loss: 0.756959  [  192/  306]
train() client id: f_00004-2-6 loss: 0.856919  [  224/  306]
train() client id: f_00004-2-7 loss: 0.898933  [  256/  306]
train() client id: f_00004-2-8 loss: 0.715426  [  288/  306]
train() client id: f_00004-3-0 loss: 0.819478  [   32/  306]
train() client id: f_00004-3-1 loss: 0.901725  [   64/  306]
train() client id: f_00004-3-2 loss: 0.704523  [   96/  306]
train() client id: f_00004-3-3 loss: 0.743366  [  128/  306]
train() client id: f_00004-3-4 loss: 0.923222  [  160/  306]
train() client id: f_00004-3-5 loss: 0.959536  [  192/  306]
train() client id: f_00004-3-6 loss: 0.748220  [  224/  306]
train() client id: f_00004-3-7 loss: 0.895704  [  256/  306]
train() client id: f_00004-3-8 loss: 0.784682  [  288/  306]
train() client id: f_00004-4-0 loss: 0.779909  [   32/  306]
train() client id: f_00004-4-1 loss: 0.899011  [   64/  306]
train() client id: f_00004-4-2 loss: 0.830911  [   96/  306]
train() client id: f_00004-4-3 loss: 0.801126  [  128/  306]
train() client id: f_00004-4-4 loss: 0.937010  [  160/  306]
train() client id: f_00004-4-5 loss: 0.742651  [  192/  306]
train() client id: f_00004-4-6 loss: 0.927871  [  224/  306]
train() client id: f_00004-4-7 loss: 0.874535  [  256/  306]
train() client id: f_00004-4-8 loss: 0.671608  [  288/  306]
train() client id: f_00004-5-0 loss: 0.763373  [   32/  306]
train() client id: f_00004-5-1 loss: 0.734951  [   64/  306]
train() client id: f_00004-5-2 loss: 0.948955  [   96/  306]
train() client id: f_00004-5-3 loss: 0.758468  [  128/  306]
train() client id: f_00004-5-4 loss: 0.875055  [  160/  306]
train() client id: f_00004-5-5 loss: 0.857489  [  192/  306]
train() client id: f_00004-5-6 loss: 0.761327  [  224/  306]
train() client id: f_00004-5-7 loss: 0.876196  [  256/  306]
train() client id: f_00004-5-8 loss: 0.929727  [  288/  306]
train() client id: f_00004-6-0 loss: 0.934392  [   32/  306]
train() client id: f_00004-6-1 loss: 0.728124  [   64/  306]
train() client id: f_00004-6-2 loss: 0.824328  [   96/  306]
train() client id: f_00004-6-3 loss: 0.849233  [  128/  306]
train() client id: f_00004-6-4 loss: 0.813848  [  160/  306]
train() client id: f_00004-6-5 loss: 0.926609  [  192/  306]
train() client id: f_00004-6-6 loss: 0.832208  [  224/  306]
train() client id: f_00004-6-7 loss: 0.796108  [  256/  306]
train() client id: f_00004-6-8 loss: 0.783301  [  288/  306]
train() client id: f_00004-7-0 loss: 0.782876  [   32/  306]
train() client id: f_00004-7-1 loss: 0.880755  [   64/  306]
train() client id: f_00004-7-2 loss: 0.769503  [   96/  306]
train() client id: f_00004-7-3 loss: 0.800514  [  128/  306]
train() client id: f_00004-7-4 loss: 0.957206  [  160/  306]
train() client id: f_00004-7-5 loss: 0.856156  [  192/  306]
train() client id: f_00004-7-6 loss: 0.864606  [  224/  306]
train() client id: f_00004-7-7 loss: 0.771189  [  256/  306]
train() client id: f_00004-7-8 loss: 0.862853  [  288/  306]
train() client id: f_00004-8-0 loss: 0.730448  [   32/  306]
train() client id: f_00004-8-1 loss: 0.849706  [   64/  306]
train() client id: f_00004-8-2 loss: 0.837584  [   96/  306]
train() client id: f_00004-8-3 loss: 0.782957  [  128/  306]
train() client id: f_00004-8-4 loss: 0.866579  [  160/  306]
train() client id: f_00004-8-5 loss: 0.861746  [  192/  306]
train() client id: f_00004-8-6 loss: 0.793341  [  224/  306]
train() client id: f_00004-8-7 loss: 0.832308  [  256/  306]
train() client id: f_00004-8-8 loss: 0.864790  [  288/  306]
train() client id: f_00004-9-0 loss: 1.060776  [   32/  306]
train() client id: f_00004-9-1 loss: 0.701749  [   64/  306]
train() client id: f_00004-9-2 loss: 0.876738  [   96/  306]
train() client id: f_00004-9-3 loss: 0.772033  [  128/  306]
train() client id: f_00004-9-4 loss: 0.778826  [  160/  306]
train() client id: f_00004-9-5 loss: 0.824045  [  192/  306]
train() client id: f_00004-9-6 loss: 0.829961  [  224/  306]
train() client id: f_00004-9-7 loss: 0.872972  [  256/  306]
train() client id: f_00004-9-8 loss: 0.769125  [  288/  306]
train() client id: f_00004-10-0 loss: 0.890571  [   32/  306]
train() client id: f_00004-10-1 loss: 0.737746  [   64/  306]
train() client id: f_00004-10-2 loss: 0.829260  [   96/  306]
train() client id: f_00004-10-3 loss: 0.908909  [  128/  306]
train() client id: f_00004-10-4 loss: 0.819730  [  160/  306]
train() client id: f_00004-10-5 loss: 0.924332  [  192/  306]
train() client id: f_00004-10-6 loss: 0.725902  [  224/  306]
train() client id: f_00004-10-7 loss: 0.755736  [  256/  306]
train() client id: f_00004-10-8 loss: 0.966715  [  288/  306]
train() client id: f_00004-11-0 loss: 0.870764  [   32/  306]
train() client id: f_00004-11-1 loss: 0.852429  [   64/  306]
train() client id: f_00004-11-2 loss: 0.764946  [   96/  306]
train() client id: f_00004-11-3 loss: 0.885089  [  128/  306]
train() client id: f_00004-11-4 loss: 0.641388  [  160/  306]
train() client id: f_00004-11-5 loss: 0.858202  [  192/  306]
train() client id: f_00004-11-6 loss: 0.902873  [  224/  306]
train() client id: f_00004-11-7 loss: 0.891992  [  256/  306]
train() client id: f_00004-11-8 loss: 0.800126  [  288/  306]
train() client id: f_00005-0-0 loss: 0.888366  [   32/  146]
train() client id: f_00005-0-1 loss: 0.854291  [   64/  146]
train() client id: f_00005-0-2 loss: 0.807469  [   96/  146]
train() client id: f_00005-0-3 loss: 0.833166  [  128/  146]
train() client id: f_00005-1-0 loss: 0.750878  [   32/  146]
train() client id: f_00005-1-1 loss: 0.903079  [   64/  146]
train() client id: f_00005-1-2 loss: 0.827342  [   96/  146]
train() client id: f_00005-1-3 loss: 0.785908  [  128/  146]
train() client id: f_00005-2-0 loss: 0.830164  [   32/  146]
train() client id: f_00005-2-1 loss: 0.769484  [   64/  146]
train() client id: f_00005-2-2 loss: 0.926158  [   96/  146]
train() client id: f_00005-2-3 loss: 0.643857  [  128/  146]
train() client id: f_00005-3-0 loss: 0.918404  [   32/  146]
train() client id: f_00005-3-1 loss: 0.794332  [   64/  146]
train() client id: f_00005-3-2 loss: 0.776795  [   96/  146]
train() client id: f_00005-3-3 loss: 0.782877  [  128/  146]
train() client id: f_00005-4-0 loss: 0.741259  [   32/  146]
train() client id: f_00005-4-1 loss: 0.812752  [   64/  146]
train() client id: f_00005-4-2 loss: 0.668078  [   96/  146]
train() client id: f_00005-4-3 loss: 0.840655  [  128/  146]
train() client id: f_00005-5-0 loss: 0.890532  [   32/  146]
train() client id: f_00005-5-1 loss: 0.692956  [   64/  146]
train() client id: f_00005-5-2 loss: 0.701367  [   96/  146]
train() client id: f_00005-5-3 loss: 0.962176  [  128/  146]
train() client id: f_00005-6-0 loss: 0.809511  [   32/  146]
train() client id: f_00005-6-1 loss: 0.662970  [   64/  146]
train() client id: f_00005-6-2 loss: 0.781726  [   96/  146]
train() client id: f_00005-6-3 loss: 0.896704  [  128/  146]
train() client id: f_00005-7-0 loss: 0.680531  [   32/  146]
train() client id: f_00005-7-1 loss: 0.655306  [   64/  146]
train() client id: f_00005-7-2 loss: 1.027159  [   96/  146]
train() client id: f_00005-7-3 loss: 0.811419  [  128/  146]
train() client id: f_00005-8-0 loss: 0.776697  [   32/  146]
train() client id: f_00005-8-1 loss: 0.707492  [   64/  146]
train() client id: f_00005-8-2 loss: 0.782530  [   96/  146]
train() client id: f_00005-8-3 loss: 0.788055  [  128/  146]
train() client id: f_00005-9-0 loss: 0.759667  [   32/  146]
train() client id: f_00005-9-1 loss: 0.842515  [   64/  146]
train() client id: f_00005-9-2 loss: 0.722978  [   96/  146]
train() client id: f_00005-9-3 loss: 0.865522  [  128/  146]
train() client id: f_00005-10-0 loss: 0.731468  [   32/  146]
train() client id: f_00005-10-1 loss: 0.865178  [   64/  146]
train() client id: f_00005-10-2 loss: 0.765777  [   96/  146]
train() client id: f_00005-10-3 loss: 0.706680  [  128/  146]
train() client id: f_00005-11-0 loss: 0.631369  [   32/  146]
train() client id: f_00005-11-1 loss: 0.849547  [   64/  146]
train() client id: f_00005-11-2 loss: 0.799011  [   96/  146]
train() client id: f_00005-11-3 loss: 0.854974  [  128/  146]
train() client id: f_00006-0-0 loss: 0.944485  [   32/   54]
train() client id: f_00006-1-0 loss: 0.941155  [   32/   54]
train() client id: f_00006-2-0 loss: 0.892628  [   32/   54]
train() client id: f_00006-3-0 loss: 0.904436  [   32/   54]
train() client id: f_00006-4-0 loss: 0.940969  [   32/   54]
train() client id: f_00006-5-0 loss: 0.947298  [   32/   54]
train() client id: f_00006-6-0 loss: 0.941031  [   32/   54]
train() client id: f_00006-7-0 loss: 0.902417  [   32/   54]
train() client id: f_00006-8-0 loss: 0.955605  [   32/   54]
train() client id: f_00006-9-0 loss: 0.904582  [   32/   54]
train() client id: f_00006-10-0 loss: 0.948456  [   32/   54]
train() client id: f_00006-11-0 loss: 0.957372  [   32/   54]
train() client id: f_00007-0-0 loss: 0.774875  [   32/  179]
train() client id: f_00007-0-1 loss: 0.734087  [   64/  179]
train() client id: f_00007-0-2 loss: 0.730422  [   96/  179]
train() client id: f_00007-0-3 loss: 0.707709  [  128/  179]
train() client id: f_00007-0-4 loss: 0.691948  [  160/  179]
train() client id: f_00007-1-0 loss: 0.647328  [   32/  179]
train() client id: f_00007-1-1 loss: 0.679318  [   64/  179]
train() client id: f_00007-1-2 loss: 0.697447  [   96/  179]
train() client id: f_00007-1-3 loss: 0.875836  [  128/  179]
train() client id: f_00007-1-4 loss: 0.686660  [  160/  179]
train() client id: f_00007-2-0 loss: 0.733588  [   32/  179]
train() client id: f_00007-2-1 loss: 0.740632  [   64/  179]
train() client id: f_00007-2-2 loss: 0.638184  [   96/  179]
train() client id: f_00007-2-3 loss: 0.739327  [  128/  179]
train() client id: f_00007-2-4 loss: 0.616080  [  160/  179]
train() client id: f_00007-3-0 loss: 0.796555  [   32/  179]
train() client id: f_00007-3-1 loss: 0.706295  [   64/  179]
train() client id: f_00007-3-2 loss: 0.639792  [   96/  179]
train() client id: f_00007-3-3 loss: 0.616283  [  128/  179]
train() client id: f_00007-3-4 loss: 0.646482  [  160/  179]
train() client id: f_00007-4-0 loss: 0.644757  [   32/  179]
train() client id: f_00007-4-1 loss: 0.779656  [   64/  179]
train() client id: f_00007-4-2 loss: 0.656081  [   96/  179]
train() client id: f_00007-4-3 loss: 0.648595  [  128/  179]
train() client id: f_00007-4-4 loss: 0.702771  [  160/  179]
train() client id: f_00007-5-0 loss: 0.639530  [   32/  179]
train() client id: f_00007-5-1 loss: 0.663237  [   64/  179]
train() client id: f_00007-5-2 loss: 0.639659  [   96/  179]
train() client id: f_00007-5-3 loss: 0.635165  [  128/  179]
train() client id: f_00007-5-4 loss: 0.752208  [  160/  179]
train() client id: f_00007-6-0 loss: 0.623555  [   32/  179]
train() client id: f_00007-6-1 loss: 0.641667  [   64/  179]
train() client id: f_00007-6-2 loss: 0.762144  [   96/  179]
train() client id: f_00007-6-3 loss: 0.641230  [  128/  179]
train() client id: f_00007-6-4 loss: 0.685176  [  160/  179]
train() client id: f_00007-7-0 loss: 0.697716  [   32/  179]
train() client id: f_00007-7-1 loss: 0.688002  [   64/  179]
train() client id: f_00007-7-2 loss: 0.569204  [   96/  179]
train() client id: f_00007-7-3 loss: 0.557837  [  128/  179]
train() client id: f_00007-7-4 loss: 0.744196  [  160/  179]
train() client id: f_00007-8-0 loss: 0.698831  [   32/  179]
train() client id: f_00007-8-1 loss: 0.560201  [   64/  179]
train() client id: f_00007-8-2 loss: 0.623923  [   96/  179]
train() client id: f_00007-8-3 loss: 0.632402  [  128/  179]
train() client id: f_00007-8-4 loss: 0.668566  [  160/  179]
train() client id: f_00007-9-0 loss: 0.618565  [   32/  179]
train() client id: f_00007-9-1 loss: 0.621150  [   64/  179]
train() client id: f_00007-9-2 loss: 0.550506  [   96/  179]
train() client id: f_00007-9-3 loss: 0.634344  [  128/  179]
train() client id: f_00007-9-4 loss: 0.821893  [  160/  179]
train() client id: f_00007-10-0 loss: 0.608212  [   32/  179]
train() client id: f_00007-10-1 loss: 0.609979  [   64/  179]
train() client id: f_00007-10-2 loss: 0.764569  [   96/  179]
train() client id: f_00007-10-3 loss: 0.626251  [  128/  179]
train() client id: f_00007-10-4 loss: 0.693197  [  160/  179]
train() client id: f_00007-11-0 loss: 0.557144  [   32/  179]
train() client id: f_00007-11-1 loss: 0.626245  [   64/  179]
train() client id: f_00007-11-2 loss: 0.743075  [   96/  179]
train() client id: f_00007-11-3 loss: 0.762971  [  128/  179]
train() client id: f_00007-11-4 loss: 0.543719  [  160/  179]
train() client id: f_00008-0-0 loss: 0.665848  [   32/  130]
train() client id: f_00008-0-1 loss: 0.847642  [   64/  130]
train() client id: f_00008-0-2 loss: 0.926138  [   96/  130]
train() client id: f_00008-0-3 loss: 0.714125  [  128/  130]
train() client id: f_00008-1-0 loss: 0.763227  [   32/  130]
train() client id: f_00008-1-1 loss: 0.840566  [   64/  130]
train() client id: f_00008-1-2 loss: 0.742503  [   96/  130]
train() client id: f_00008-1-3 loss: 0.803973  [  128/  130]
train() client id: f_00008-2-0 loss: 0.830933  [   32/  130]
train() client id: f_00008-2-1 loss: 0.826163  [   64/  130]
train() client id: f_00008-2-2 loss: 0.750615  [   96/  130]
train() client id: f_00008-2-3 loss: 0.727601  [  128/  130]
train() client id: f_00008-3-0 loss: 0.809783  [   32/  130]
train() client id: f_00008-3-1 loss: 0.808188  [   64/  130]
train() client id: f_00008-3-2 loss: 0.769134  [   96/  130]
train() client id: f_00008-3-3 loss: 0.740887  [  128/  130]
train() client id: f_00008-4-0 loss: 0.749513  [   32/  130]
train() client id: f_00008-4-1 loss: 0.751166  [   64/  130]
train() client id: f_00008-4-2 loss: 0.802647  [   96/  130]
train() client id: f_00008-4-3 loss: 0.818661  [  128/  130]
train() client id: f_00008-5-0 loss: 0.725465  [   32/  130]
train() client id: f_00008-5-1 loss: 0.755529  [   64/  130]
train() client id: f_00008-5-2 loss: 0.820356  [   96/  130]
train() client id: f_00008-5-3 loss: 0.787728  [  128/  130]
train() client id: f_00008-6-0 loss: 0.768279  [   32/  130]
train() client id: f_00008-6-1 loss: 0.830503  [   64/  130]
train() client id: f_00008-6-2 loss: 0.718107  [   96/  130]
train() client id: f_00008-6-3 loss: 0.801459  [  128/  130]
train() client id: f_00008-7-0 loss: 0.791117  [   32/  130]
train() client id: f_00008-7-1 loss: 0.687289  [   64/  130]
train() client id: f_00008-7-2 loss: 0.837273  [   96/  130]
train() client id: f_00008-7-3 loss: 0.796350  [  128/  130]
train() client id: f_00008-8-0 loss: 0.706791  [   32/  130]
train() client id: f_00008-8-1 loss: 0.933979  [   64/  130]
train() client id: f_00008-8-2 loss: 0.750899  [   96/  130]
train() client id: f_00008-8-3 loss: 0.720455  [  128/  130]
train() client id: f_00008-9-0 loss: 0.692630  [   32/  130]
train() client id: f_00008-9-1 loss: 0.880547  [   64/  130]
train() client id: f_00008-9-2 loss: 0.859811  [   96/  130]
train() client id: f_00008-9-3 loss: 0.682433  [  128/  130]
train() client id: f_00008-10-0 loss: 0.761200  [   32/  130]
train() client id: f_00008-10-1 loss: 0.674505  [   64/  130]
train() client id: f_00008-10-2 loss: 0.814286  [   96/  130]
train() client id: f_00008-10-3 loss: 0.829660  [  128/  130]
train() client id: f_00008-11-0 loss: 0.700869  [   32/  130]
train() client id: f_00008-11-1 loss: 0.862842  [   64/  130]
train() client id: f_00008-11-2 loss: 0.687343  [   96/  130]
train() client id: f_00008-11-3 loss: 0.859551  [  128/  130]
train() client id: f_00009-0-0 loss: 1.194329  [   32/  118]
train() client id: f_00009-0-1 loss: 1.066607  [   64/  118]
train() client id: f_00009-0-2 loss: 1.162841  [   96/  118]
train() client id: f_00009-1-0 loss: 1.070673  [   32/  118]
train() client id: f_00009-1-1 loss: 1.094843  [   64/  118]
train() client id: f_00009-1-2 loss: 1.156976  [   96/  118]
train() client id: f_00009-2-0 loss: 1.073125  [   32/  118]
train() client id: f_00009-2-1 loss: 1.048797  [   64/  118]
train() client id: f_00009-2-2 loss: 1.116362  [   96/  118]
train() client id: f_00009-3-0 loss: 1.033484  [   32/  118]
train() client id: f_00009-3-1 loss: 1.103418  [   64/  118]
train() client id: f_00009-3-2 loss: 1.037231  [   96/  118]
train() client id: f_00009-4-0 loss: 1.006088  [   32/  118]
train() client id: f_00009-4-1 loss: 1.033388  [   64/  118]
train() client id: f_00009-4-2 loss: 1.078200  [   96/  118]
train() client id: f_00009-5-0 loss: 0.961267  [   32/  118]
train() client id: f_00009-5-1 loss: 0.946134  [   64/  118]
train() client id: f_00009-5-2 loss: 1.104907  [   96/  118]
train() client id: f_00009-6-0 loss: 0.945833  [   32/  118]
train() client id: f_00009-6-1 loss: 0.988258  [   64/  118]
train() client id: f_00009-6-2 loss: 0.959578  [   96/  118]
train() client id: f_00009-7-0 loss: 1.001807  [   32/  118]
train() client id: f_00009-7-1 loss: 0.984466  [   64/  118]
train() client id: f_00009-7-2 loss: 0.980663  [   96/  118]
train() client id: f_00009-8-0 loss: 0.907449  [   32/  118]
train() client id: f_00009-8-1 loss: 1.069504  [   64/  118]
train() client id: f_00009-8-2 loss: 1.005766  [   96/  118]
train() client id: f_00009-9-0 loss: 0.900721  [   32/  118]
train() client id: f_00009-9-1 loss: 0.955026  [   64/  118]
train() client id: f_00009-9-2 loss: 1.104131  [   96/  118]
train() client id: f_00009-10-0 loss: 0.983233  [   32/  118]
train() client id: f_00009-10-1 loss: 0.960927  [   64/  118]
train() client id: f_00009-10-2 loss: 0.937197  [   96/  118]
train() client id: f_00009-11-0 loss: 1.018479  [   32/  118]
train() client id: f_00009-11-1 loss: 0.913118  [   64/  118]
train() client id: f_00009-11-2 loss: 0.979270  [   96/  118]
At round 5 accuracy: 0.6312997347480106
At round 5 training accuracy: 0.5700871898054997
At round 5 training loss: 0.9037254770856669
update_location
xs = [-3.9056584   4.20031788 45.00902392 18.81129433 -9.02070377  3.95640986
 -7.44319194 -1.32485185 29.66397685 -2.06087855]
ys = [ 37.5879595   20.55583871   1.32061395  -7.45517586   9.35018685
 -17.18584926  -2.62498432  -4.17765202  17.56900603   4.00148178]
dists_uav = [106.90233331 102.17722435 109.6702159  102.02668495 100.84046356
 101.54312677 100.31097472 100.09599397 105.7762804  100.10124413]
dists_bs = [219.44325599 236.57082002 280.3007309  266.18172178 234.49730782
 262.60387904 244.18419069 249.53494358 258.20894888 243.20450851]
uav_gains = [8.46306925e-11 9.47572782e-11 7.93911850e-11 9.51072087e-11
 9.79289548e-11 9.62435611e-11 9.92263936e-11 9.97600464e-11
 8.69011873e-11 9.97469659e-11]
bs_gains = [3.07320703e-11 2.49001977e-11 1.54862491e-11 1.78976100e-11
 2.55216104e-11 1.85887806e-11 2.27868853e-11 2.14450097e-11
 1.94883232e-11 2.30448319e-11]
Round 6
-------------------------------
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.20035717 21.31407924 10.04322819  3.59039783 24.57615325 11.85884139
  4.4646498  14.41376646 10.58251293  9.61787463]
obj_prev = 120.66186087163511
eta_min = 7.409354424179371e-10	eta_max = 0.91885879376386
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 28.063470776703085	eta = 0.909090909090909
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 48.384419986505044	eta = 0.5272822567213747
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 38.70245321762515	eta = 0.6591893805073021
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 36.96801720495558	eta = 0.6901167032896544
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 36.88278882583632	eta = 0.6917114180576249
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 36.88256856080987	eta = 0.6917155490018552
eta = 0.6917155490018552
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [0.03041438 0.06396674 0.02993161 0.01037951 0.07386349 0.03524206
 0.01303473 0.04320773 0.03137991 0.0284833 ]
ene_total = [3.12833392 6.13288912 3.09403261 1.41867893 6.96750532 3.75135634
 1.63958615 4.19625148 3.41497814 3.13895655]
ti_comp = [0.28546116 0.26646804 0.2846808  0.28683999 0.2669565  0.2602256
 0.28732669 0.28738774 0.26129447 0.26489704]
ti_coms = [0.06497807 0.08397118 0.06575843 0.06359924 0.08348272 0.09021363
 0.06311253 0.06305149 0.08914475 0.08554218]
t_total = [29.69997482 29.69997482 29.69997482 29.69997482 29.69997482 29.69997482
 29.69997482 29.69997482 29.69997482 29.69997482]
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.15785733e-05 2.30384101e-04 2.06801576e-05 8.49436889e-07
 3.53417801e-04 4.03983548e-05 1.67661934e-06 6.10418604e-05
 2.82862087e-05 2.05824434e-05]
ene_total = [0.55252256 0.73118701 0.55906002 0.53908008 0.73747448 0.76799073
 0.53502532 0.53953919 0.7579054  0.72672045]
optimize_network iter = 0 obj = 6.446505230599096
eta = 0.6917155490018552
freqs = [5.32723555e+07 1.20027048e+08 5.25704736e+07 1.80928566e+07
 1.38343675e+08 6.77144291e+07 2.26827740e+07 7.51732237e+07
 6.00470262e+07 5.37629723e+07]
eta_min = 0.6801612059033121	eta_max = 0.6917155490018475
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 0.06073437862890649	eta = 0.9090909090909091
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 21.69496157536947	eta = 0.002544972079761971
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 2.3026899172665374	eta = 0.023977640700475235
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 2.226369694175787	eta = 0.024799597131268097
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 2.2263378919337997	eta = 0.024799951382431865
eta = 0.024799951382431865
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.16436107e-04 2.31078474e-03 2.07424872e-04 8.51997075e-06
 3.54482996e-03 4.05201146e-04 1.68167264e-05 6.12258394e-04
 2.83714628e-04 2.06444785e-04]
ene_total = [0.17988639 0.28688359 0.18173569 0.17062183 0.31863717 0.25255426
 0.16954014 0.18532952 0.24643571 0.23471359]
ti_comp = [0.29859544 0.27960232 0.29781508 0.29997427 0.28009078 0.27335988
 0.30046097 0.30052202 0.27442876 0.27803133]
ti_coms = [0.06497807 0.08397118 0.06575843 0.06359924 0.08348272 0.09021363
 0.06311253 0.06305149 0.08914475 0.08554218]
t_total = [29.69997482 29.69997482 29.69997482 29.69997482 29.69997482 29.69997482
 29.69997482 29.69997482 29.69997482 29.69997482]
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.25514581e-05 2.39268435e-04 2.16073258e-05 8.88109865e-07
 3.67109805e-04 4.18618440e-05 1.75321215e-06 6.38316142e-05
 2.93224432e-05 2.13642617e-05]
ene_total = [0.53264186 0.70549825 0.53893943 0.51960869 0.71195131 0.74036622
 0.5157035  0.52027595 0.73061032 0.70053117]
optimize_network iter = 1 obj = 6.216126697450631
eta = 0.6801612059033121
freqs = [5.32566467e+07 1.19616625e+08 5.25486356e+07 1.80913467e+07
 1.37882470e+08 6.74069266e+07 2.26825634e+07 7.51732237e+07
 5.97860936e+07 5.35642116e+07]
eta_min = 0.6801612059033234	eta_max = 0.680161205903311
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 0.06036643683170785	eta = 0.909090909090909
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 21.694610889380957	eta = 0.0025295949864110306
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 2.300984648578189	eta = 0.023850041316801688
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 2.2251002879015918	eta = 0.024663418200206223
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 2.225068999906738	eta = 0.02466376500693525
eta = 0.02466376500693525
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.16630671e-04 2.29842707e-03 2.07561279e-04 8.53123712e-06
 3.52647900e-03 4.02127407e-04 1.68414620e-05 6.13170349e-04
 2.81673164e-04 2.05226390e-04]
ene_total = [0.17986649 0.2865125  0.18171397 0.17059831 0.31810109 0.25243665
 0.16951713 0.18532807 0.24634661 0.23464818]
ti_comp = [0.29859544 0.27960232 0.29781508 0.29997427 0.28009078 0.27335988
 0.30046097 0.30052202 0.27442876 0.27803133]
ti_coms = [0.06497807 0.08397118 0.06575843 0.06359924 0.08348272 0.09021363
 0.06311253 0.06305149 0.08914475 0.08554218]
t_total = [29.69997482 29.69997482 29.69997482 29.69997482 29.69997482 29.69997482
 29.69997482 29.69997482 29.69997482 29.69997482]
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.25514581e-05 2.39268435e-04 2.16073258e-05 8.88109865e-07
 3.67109805e-04 4.18618440e-05 1.75321215e-06 6.38316142e-05
 2.93224432e-05 2.13642617e-05]
ene_total = [0.53264186 0.70549825 0.53893943 0.51960869 0.71195131 0.74036622
 0.5157035  0.52027595 0.73061032 0.70053117]
optimize_network iter = 2 obj = 6.216126697450847
eta = 0.6801612059033234
freqs = [5.32566467e+07 1.19616625e+08 5.25486356e+07 1.80913467e+07
 1.37882470e+08 6.74069266e+07 2.26825634e+07 7.51732237e+07
 5.97860936e+07 5.35642116e+07]
Done!
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.14422044e-05 2.27499377e-04 2.05445117e-05 8.44425808e-07
 3.49052528e-04 3.98027573e-05 1.66697572e-06 6.06918856e-05
 2.78801405e-05 2.03134033e-05]
ene_total = [0.00651925 0.00862462 0.00659639 0.00636077 0.00869732 0.00906117
 0.00631292 0.00636584 0.00894236 0.00857453]
At round 6 energy consumption: 0.07605515902799734
At round 6 eta: 0.6801612059033234
At round 6 a_n: 25.784781927469155
At round 6 local rounds: 12.620787147614399
At round 6 global rounds: 81.68905166188517
gradient difference: 0.4401788115501404
train() client id: f_00000-0-0 loss: 1.857292  [   32/  126]
train() client id: f_00000-0-1 loss: 1.547045  [   64/  126]
train() client id: f_00000-0-2 loss: 1.346964  [   96/  126]
train() client id: f_00000-1-0 loss: 1.263884  [   32/  126]
train() client id: f_00000-1-1 loss: 1.425166  [   64/  126]
train() client id: f_00000-1-2 loss: 1.366213  [   96/  126]
train() client id: f_00000-2-0 loss: 1.254287  [   32/  126]
train() client id: f_00000-2-1 loss: 1.244642  [   64/  126]
train() client id: f_00000-2-2 loss: 1.233199  [   96/  126]
train() client id: f_00000-3-0 loss: 1.069181  [   32/  126]
train() client id: f_00000-3-1 loss: 1.150920  [   64/  126]
train() client id: f_00000-3-2 loss: 1.174033  [   96/  126]
train() client id: f_00000-4-0 loss: 1.182405  [   32/  126]
train() client id: f_00000-4-1 loss: 1.064220  [   64/  126]
train() client id: f_00000-4-2 loss: 1.151465  [   96/  126]
train() client id: f_00000-5-0 loss: 1.066004  [   32/  126]
train() client id: f_00000-5-1 loss: 1.021508  [   64/  126]
train() client id: f_00000-5-2 loss: 1.199245  [   96/  126]
train() client id: f_00000-6-0 loss: 1.044624  [   32/  126]
train() client id: f_00000-6-1 loss: 1.035949  [   64/  126]
train() client id: f_00000-6-2 loss: 1.066980  [   96/  126]
train() client id: f_00000-7-0 loss: 0.990263  [   32/  126]
train() client id: f_00000-7-1 loss: 0.998444  [   64/  126]
train() client id: f_00000-7-2 loss: 1.024286  [   96/  126]
train() client id: f_00000-8-0 loss: 0.991952  [   32/  126]
train() client id: f_00000-8-1 loss: 0.967129  [   64/  126]
train() client id: f_00000-8-2 loss: 1.031876  [   96/  126]
train() client id: f_00000-9-0 loss: 1.007086  [   32/  126]
train() client id: f_00000-9-1 loss: 0.995056  [   64/  126]
train() client id: f_00000-9-2 loss: 1.037947  [   96/  126]
train() client id: f_00000-10-0 loss: 0.998248  [   32/  126]
train() client id: f_00000-10-1 loss: 1.049450  [   64/  126]
train() client id: f_00000-10-2 loss: 0.990410  [   96/  126]
train() client id: f_00000-11-0 loss: 1.009145  [   32/  126]
train() client id: f_00000-11-1 loss: 0.971573  [   64/  126]
train() client id: f_00000-11-2 loss: 0.963240  [   96/  126]
train() client id: f_00001-0-0 loss: 0.565906  [   32/  265]
train() client id: f_00001-0-1 loss: 0.493702  [   64/  265]
train() client id: f_00001-0-2 loss: 0.532465  [   96/  265]
train() client id: f_00001-0-3 loss: 0.482759  [  128/  265]
train() client id: f_00001-0-4 loss: 0.525678  [  160/  265]
train() client id: f_00001-0-5 loss: 0.545876  [  192/  265]
train() client id: f_00001-0-6 loss: 0.546371  [  224/  265]
train() client id: f_00001-0-7 loss: 0.492563  [  256/  265]
train() client id: f_00001-1-0 loss: 0.480374  [   32/  265]
train() client id: f_00001-1-1 loss: 0.476439  [   64/  265]
train() client id: f_00001-1-2 loss: 0.488213  [   96/  265]
train() client id: f_00001-1-3 loss: 0.516551  [  128/  265]
train() client id: f_00001-1-4 loss: 0.532179  [  160/  265]
train() client id: f_00001-1-5 loss: 0.421184  [  192/  265]
train() client id: f_00001-1-6 loss: 0.462487  [  224/  265]
train() client id: f_00001-1-7 loss: 0.501327  [  256/  265]
train() client id: f_00001-2-0 loss: 0.483288  [   32/  265]
train() client id: f_00001-2-1 loss: 0.501246  [   64/  265]
train() client id: f_00001-2-2 loss: 0.419556  [   96/  265]
train() client id: f_00001-2-3 loss: 0.405123  [  128/  265]
train() client id: f_00001-2-4 loss: 0.495731  [  160/  265]
train() client id: f_00001-2-5 loss: 0.546841  [  192/  265]
train() client id: f_00001-2-6 loss: 0.445508  [  224/  265]
train() client id: f_00001-2-7 loss: 0.471442  [  256/  265]
train() client id: f_00001-3-0 loss: 0.450722  [   32/  265]
train() client id: f_00001-3-1 loss: 0.539306  [   64/  265]
train() client id: f_00001-3-2 loss: 0.424627  [   96/  265]
train() client id: f_00001-3-3 loss: 0.525002  [  128/  265]
train() client id: f_00001-3-4 loss: 0.430047  [  160/  265]
train() client id: f_00001-3-5 loss: 0.463423  [  192/  265]
train() client id: f_00001-3-6 loss: 0.419984  [  224/  265]
train() client id: f_00001-3-7 loss: 0.447835  [  256/  265]
train() client id: f_00001-4-0 loss: 0.503043  [   32/  265]
train() client id: f_00001-4-1 loss: 0.427039  [   64/  265]
train() client id: f_00001-4-2 loss: 0.478412  [   96/  265]
train() client id: f_00001-4-3 loss: 0.486524  [  128/  265]
train() client id: f_00001-4-4 loss: 0.427948  [  160/  265]
train() client id: f_00001-4-5 loss: 0.419963  [  192/  265]
train() client id: f_00001-4-6 loss: 0.429328  [  224/  265]
train() client id: f_00001-4-7 loss: 0.404904  [  256/  265]
train() client id: f_00001-5-0 loss: 0.424519  [   32/  265]
train() client id: f_00001-5-1 loss: 0.447346  [   64/  265]
train() client id: f_00001-5-2 loss: 0.413904  [   96/  265]
train() client id: f_00001-5-3 loss: 0.484583  [  128/  265]
train() client id: f_00001-5-4 loss: 0.426560  [  160/  265]
train() client id: f_00001-5-5 loss: 0.365188  [  192/  265]
train() client id: f_00001-5-6 loss: 0.585613  [  224/  265]
train() client id: f_00001-5-7 loss: 0.408949  [  256/  265]
train() client id: f_00001-6-0 loss: 0.411129  [   32/  265]
train() client id: f_00001-6-1 loss: 0.468175  [   64/  265]
train() client id: f_00001-6-2 loss: 0.523636  [   96/  265]
train() client id: f_00001-6-3 loss: 0.505409  [  128/  265]
train() client id: f_00001-6-4 loss: 0.350832  [  160/  265]
train() client id: f_00001-6-5 loss: 0.469501  [  192/  265]
train() client id: f_00001-6-6 loss: 0.422578  [  224/  265]
train() client id: f_00001-6-7 loss: 0.364947  [  256/  265]
train() client id: f_00001-7-0 loss: 0.413895  [   32/  265]
train() client id: f_00001-7-1 loss: 0.415250  [   64/  265]
train() client id: f_00001-7-2 loss: 0.351595  [   96/  265]
train() client id: f_00001-7-3 loss: 0.571060  [  128/  265]
train() client id: f_00001-7-4 loss: 0.458615  [  160/  265]
train() client id: f_00001-7-5 loss: 0.463291  [  192/  265]
train() client id: f_00001-7-6 loss: 0.460585  [  224/  265]
train() client id: f_00001-7-7 loss: 0.346215  [  256/  265]
train() client id: f_00001-8-0 loss: 0.479270  [   32/  265]
train() client id: f_00001-8-1 loss: 0.390392  [   64/  265]
train() client id: f_00001-8-2 loss: 0.403944  [   96/  265]
train() client id: f_00001-8-3 loss: 0.352817  [  128/  265]
train() client id: f_00001-8-4 loss: 0.612810  [  160/  265]
train() client id: f_00001-8-5 loss: 0.345806  [  192/  265]
train() client id: f_00001-8-6 loss: 0.398422  [  224/  265]
train() client id: f_00001-8-7 loss: 0.406140  [  256/  265]
train() client id: f_00001-9-0 loss: 0.442761  [   32/  265]
train() client id: f_00001-9-1 loss: 0.451380  [   64/  265]
train() client id: f_00001-9-2 loss: 0.404376  [   96/  265]
train() client id: f_00001-9-3 loss: 0.458168  [  128/  265]
train() client id: f_00001-9-4 loss: 0.478981  [  160/  265]
train() client id: f_00001-9-5 loss: 0.407815  [  192/  265]
train() client id: f_00001-9-6 loss: 0.403485  [  224/  265]
train() client id: f_00001-9-7 loss: 0.382958  [  256/  265]
train() client id: f_00001-10-0 loss: 0.415243  [   32/  265]
train() client id: f_00001-10-1 loss: 0.450995  [   64/  265]
train() client id: f_00001-10-2 loss: 0.322573  [   96/  265]
train() client id: f_00001-10-3 loss: 0.331929  [  128/  265]
train() client id: f_00001-10-4 loss: 0.430663  [  160/  265]
train() client id: f_00001-10-5 loss: 0.384374  [  192/  265]
train() client id: f_00001-10-6 loss: 0.567418  [  224/  265]
train() client id: f_00001-10-7 loss: 0.531305  [  256/  265]
train() client id: f_00001-11-0 loss: 0.408080  [   32/  265]
train() client id: f_00001-11-1 loss: 0.554651  [   64/  265]
train() client id: f_00001-11-2 loss: 0.387368  [   96/  265]
train() client id: f_00001-11-3 loss: 0.521446  [  128/  265]
train() client id: f_00001-11-4 loss: 0.331782  [  160/  265]
train() client id: f_00001-11-5 loss: 0.475935  [  192/  265]
train() client id: f_00001-11-6 loss: 0.396958  [  224/  265]
train() client id: f_00001-11-7 loss: 0.345145  [  256/  265]
train() client id: f_00002-0-0 loss: 1.181388  [   32/  124]
train() client id: f_00002-0-1 loss: 1.261680  [   64/  124]
train() client id: f_00002-0-2 loss: 1.104412  [   96/  124]
train() client id: f_00002-1-0 loss: 1.098899  [   32/  124]
train() client id: f_00002-1-1 loss: 1.143427  [   64/  124]
train() client id: f_00002-1-2 loss: 1.201225  [   96/  124]
train() client id: f_00002-2-0 loss: 1.065885  [   32/  124]
train() client id: f_00002-2-1 loss: 1.144884  [   64/  124]
train() client id: f_00002-2-2 loss: 1.109531  [   96/  124]
train() client id: f_00002-3-0 loss: 1.133834  [   32/  124]
train() client id: f_00002-3-1 loss: 1.066928  [   64/  124]
train() client id: f_00002-3-2 loss: 1.089447  [   96/  124]
train() client id: f_00002-4-0 loss: 1.162441  [   32/  124]
train() client id: f_00002-4-1 loss: 1.057599  [   64/  124]
train() client id: f_00002-4-2 loss: 0.950656  [   96/  124]
train() client id: f_00002-5-0 loss: 1.041422  [   32/  124]
train() client id: f_00002-5-1 loss: 1.102412  [   64/  124]
train() client id: f_00002-5-2 loss: 0.907007  [   96/  124]
train() client id: f_00002-6-0 loss: 1.045276  [   32/  124]
train() client id: f_00002-6-1 loss: 0.992716  [   64/  124]
train() client id: f_00002-6-2 loss: 1.058292  [   96/  124]
train() client id: f_00002-7-0 loss: 1.015626  [   32/  124]
train() client id: f_00002-7-1 loss: 1.047668  [   64/  124]
train() client id: f_00002-7-2 loss: 1.018755  [   96/  124]
train() client id: f_00002-8-0 loss: 1.037955  [   32/  124]
train() client id: f_00002-8-1 loss: 0.963649  [   64/  124]
train() client id: f_00002-8-2 loss: 1.021112  [   96/  124]
train() client id: f_00002-9-0 loss: 0.989278  [   32/  124]
train() client id: f_00002-9-1 loss: 1.070409  [   64/  124]
train() client id: f_00002-9-2 loss: 1.025931  [   96/  124]
train() client id: f_00002-10-0 loss: 1.072585  [   32/  124]
train() client id: f_00002-10-1 loss: 0.914495  [   64/  124]
train() client id: f_00002-10-2 loss: 1.099646  [   96/  124]
train() client id: f_00002-11-0 loss: 1.014409  [   32/  124]
train() client id: f_00002-11-1 loss: 0.963867  [   64/  124]
train() client id: f_00002-11-2 loss: 0.972775  [   96/  124]
train() client id: f_00003-0-0 loss: 0.820814  [   32/   43]
train() client id: f_00003-1-0 loss: 0.815397  [   32/   43]
train() client id: f_00003-2-0 loss: 0.847298  [   32/   43]
train() client id: f_00003-3-0 loss: 0.906333  [   32/   43]
train() client id: f_00003-4-0 loss: 0.863908  [   32/   43]
train() client id: f_00003-5-0 loss: 0.869766  [   32/   43]
train() client id: f_00003-6-0 loss: 0.972089  [   32/   43]
train() client id: f_00003-7-0 loss: 0.914246  [   32/   43]
train() client id: f_00003-8-0 loss: 0.799707  [   32/   43]
train() client id: f_00003-9-0 loss: 0.900803  [   32/   43]
train() client id: f_00003-10-0 loss: 0.893746  [   32/   43]
train() client id: f_00003-11-0 loss: 0.905800  [   32/   43]
train() client id: f_00004-0-0 loss: 0.895555  [   32/  306]
train() client id: f_00004-0-1 loss: 0.915021  [   64/  306]
train() client id: f_00004-0-2 loss: 1.028519  [   96/  306]
train() client id: f_00004-0-3 loss: 1.039926  [  128/  306]
train() client id: f_00004-0-4 loss: 1.144321  [  160/  306]
train() client id: f_00004-0-5 loss: 0.802417  [  192/  306]
train() client id: f_00004-0-6 loss: 0.940865  [  224/  306]
train() client id: f_00004-0-7 loss: 0.998846  [  256/  306]
train() client id: f_00004-0-8 loss: 0.951670  [  288/  306]
train() client id: f_00004-1-0 loss: 0.899655  [   32/  306]
train() client id: f_00004-1-1 loss: 0.879328  [   64/  306]
train() client id: f_00004-1-2 loss: 0.990416  [   96/  306]
train() client id: f_00004-1-3 loss: 0.885593  [  128/  306]
train() client id: f_00004-1-4 loss: 1.079501  [  160/  306]
train() client id: f_00004-1-5 loss: 0.982541  [  192/  306]
train() client id: f_00004-1-6 loss: 1.033304  [  224/  306]
train() client id: f_00004-1-7 loss: 0.866427  [  256/  306]
train() client id: f_00004-1-8 loss: 1.091865  [  288/  306]
train() client id: f_00004-2-0 loss: 1.012616  [   32/  306]
train() client id: f_00004-2-1 loss: 0.961644  [   64/  306]
train() client id: f_00004-2-2 loss: 0.790123  [   96/  306]
train() client id: f_00004-2-3 loss: 0.913445  [  128/  306]
train() client id: f_00004-2-4 loss: 1.082906  [  160/  306]
train() client id: f_00004-2-5 loss: 0.926870  [  192/  306]
train() client id: f_00004-2-6 loss: 1.012511  [  224/  306]
train() client id: f_00004-2-7 loss: 0.977384  [  256/  306]
train() client id: f_00004-2-8 loss: 0.937931  [  288/  306]
train() client id: f_00004-3-0 loss: 0.866607  [   32/  306]
train() client id: f_00004-3-1 loss: 0.994668  [   64/  306]
train() client id: f_00004-3-2 loss: 0.900701  [   96/  306]
train() client id: f_00004-3-3 loss: 0.847688  [  128/  306]
train() client id: f_00004-3-4 loss: 1.085617  [  160/  306]
train() client id: f_00004-3-5 loss: 0.837780  [  192/  306]
train() client id: f_00004-3-6 loss: 1.039758  [  224/  306]
train() client id: f_00004-3-7 loss: 1.152153  [  256/  306]
train() client id: f_00004-3-8 loss: 1.002011  [  288/  306]
train() client id: f_00004-4-0 loss: 0.796533  [   32/  306]
train() client id: f_00004-4-1 loss: 1.124497  [   64/  306]
train() client id: f_00004-4-2 loss: 1.001947  [   96/  306]
train() client id: f_00004-4-3 loss: 0.914288  [  128/  306]
train() client id: f_00004-4-4 loss: 0.972275  [  160/  306]
train() client id: f_00004-4-5 loss: 0.942392  [  192/  306]
train() client id: f_00004-4-6 loss: 0.895128  [  224/  306]
train() client id: f_00004-4-7 loss: 0.903237  [  256/  306]
train() client id: f_00004-4-8 loss: 1.115522  [  288/  306]
train() client id: f_00004-5-0 loss: 0.857785  [   32/  306]
train() client id: f_00004-5-1 loss: 1.037704  [   64/  306]
train() client id: f_00004-5-2 loss: 0.987412  [   96/  306]
train() client id: f_00004-5-3 loss: 0.923873  [  128/  306]
train() client id: f_00004-5-4 loss: 0.936771  [  160/  306]
train() client id: f_00004-5-5 loss: 0.945431  [  192/  306]
train() client id: f_00004-5-6 loss: 0.983931  [  224/  306]
train() client id: f_00004-5-7 loss: 0.892185  [  256/  306]
train() client id: f_00004-5-8 loss: 0.980120  [  288/  306]
train() client id: f_00004-6-0 loss: 1.057862  [   32/  306]
train() client id: f_00004-6-1 loss: 0.898468  [   64/  306]
train() client id: f_00004-6-2 loss: 0.883737  [   96/  306]
train() client id: f_00004-6-3 loss: 1.006662  [  128/  306]
train() client id: f_00004-6-4 loss: 0.876801  [  160/  306]
train() client id: f_00004-6-5 loss: 0.868401  [  192/  306]
train() client id: f_00004-6-6 loss: 1.075062  [  224/  306]
train() client id: f_00004-6-7 loss: 0.886522  [  256/  306]
train() client id: f_00004-6-8 loss: 0.976172  [  288/  306]
train() client id: f_00004-7-0 loss: 0.947889  [   32/  306]
train() client id: f_00004-7-1 loss: 1.008998  [   64/  306]
train() client id: f_00004-7-2 loss: 0.976655  [   96/  306]
train() client id: f_00004-7-3 loss: 0.965637  [  128/  306]
train() client id: f_00004-7-4 loss: 0.865720  [  160/  306]
train() client id: f_00004-7-5 loss: 0.805247  [  192/  306]
train() client id: f_00004-7-6 loss: 0.998639  [  224/  306]
train() client id: f_00004-7-7 loss: 0.975999  [  256/  306]
train() client id: f_00004-7-8 loss: 1.007721  [  288/  306]
train() client id: f_00004-8-0 loss: 0.931027  [   32/  306]
train() client id: f_00004-8-1 loss: 1.039819  [   64/  306]
train() client id: f_00004-8-2 loss: 0.970060  [   96/  306]
train() client id: f_00004-8-3 loss: 0.866476  [  128/  306]
train() client id: f_00004-8-4 loss: 0.965983  [  160/  306]
train() client id: f_00004-8-5 loss: 0.908168  [  192/  306]
train() client id: f_00004-8-6 loss: 0.888810  [  224/  306]
train() client id: f_00004-8-7 loss: 0.965839  [  256/  306]
train() client id: f_00004-8-8 loss: 0.991194  [  288/  306]
train() client id: f_00004-9-0 loss: 0.859113  [   32/  306]
train() client id: f_00004-9-1 loss: 0.912525  [   64/  306]
train() client id: f_00004-9-2 loss: 0.920983  [   96/  306]
train() client id: f_00004-9-3 loss: 1.065354  [  128/  306]
train() client id: f_00004-9-4 loss: 0.916760  [  160/  306]
train() client id: f_00004-9-5 loss: 0.962244  [  192/  306]
train() client id: f_00004-9-6 loss: 0.985725  [  224/  306]
train() client id: f_00004-9-7 loss: 0.959638  [  256/  306]
train() client id: f_00004-9-8 loss: 0.912192  [  288/  306]
train() client id: f_00004-10-0 loss: 0.995812  [   32/  306]
train() client id: f_00004-10-1 loss: 0.917880  [   64/  306]
train() client id: f_00004-10-2 loss: 0.971476  [   96/  306]
train() client id: f_00004-10-3 loss: 1.036111  [  128/  306]
train() client id: f_00004-10-4 loss: 0.979005  [  160/  306]
train() client id: f_00004-10-5 loss: 0.939256  [  192/  306]
train() client id: f_00004-10-6 loss: 0.825149  [  224/  306]
train() client id: f_00004-10-7 loss: 0.907033  [  256/  306]
train() client id: f_00004-10-8 loss: 0.900231  [  288/  306]
train() client id: f_00004-11-0 loss: 0.977537  [   32/  306]
train() client id: f_00004-11-1 loss: 0.933610  [   64/  306]
train() client id: f_00004-11-2 loss: 0.893877  [   96/  306]
train() client id: f_00004-11-3 loss: 0.901302  [  128/  306]
train() client id: f_00004-11-4 loss: 0.940359  [  160/  306]
train() client id: f_00004-11-5 loss: 0.925877  [  192/  306]
train() client id: f_00004-11-6 loss: 0.846861  [  224/  306]
train() client id: f_00004-11-7 loss: 0.944508  [  256/  306]
train() client id: f_00004-11-8 loss: 1.061604  [  288/  306]
train() client id: f_00005-0-0 loss: 0.869132  [   32/  146]
train() client id: f_00005-0-1 loss: 0.765645  [   64/  146]
train() client id: f_00005-0-2 loss: 0.699052  [   96/  146]
train() client id: f_00005-0-3 loss: 0.700760  [  128/  146]
train() client id: f_00005-1-0 loss: 0.587310  [   32/  146]
train() client id: f_00005-1-1 loss: 0.606468  [   64/  146]
train() client id: f_00005-1-2 loss: 0.847074  [   96/  146]
train() client id: f_00005-1-3 loss: 0.895488  [  128/  146]
train() client id: f_00005-2-0 loss: 0.747944  [   32/  146]
train() client id: f_00005-2-1 loss: 0.727466  [   64/  146]
train() client id: f_00005-2-2 loss: 0.698490  [   96/  146]
train() client id: f_00005-2-3 loss: 0.766410  [  128/  146]
train() client id: f_00005-3-0 loss: 0.782061  [   32/  146]
train() client id: f_00005-3-1 loss: 0.674948  [   64/  146]
train() client id: f_00005-3-2 loss: 0.713988  [   96/  146]
train() client id: f_00005-3-3 loss: 0.756134  [  128/  146]
train() client id: f_00005-4-0 loss: 0.580745  [   32/  146]
train() client id: f_00005-4-1 loss: 0.775202  [   64/  146]
train() client id: f_00005-4-2 loss: 0.694516  [   96/  146]
train() client id: f_00005-4-3 loss: 0.765041  [  128/  146]
train() client id: f_00005-5-0 loss: 0.847888  [   32/  146]
train() client id: f_00005-5-1 loss: 0.642697  [   64/  146]
train() client id: f_00005-5-2 loss: 0.703972  [   96/  146]
train() client id: f_00005-5-3 loss: 0.685998  [  128/  146]
train() client id: f_00005-6-0 loss: 0.780299  [   32/  146]
train() client id: f_00005-6-1 loss: 0.626306  [   64/  146]
train() client id: f_00005-6-2 loss: 0.814224  [   96/  146]
train() client id: f_00005-6-3 loss: 0.771895  [  128/  146]
train() client id: f_00005-7-0 loss: 0.759854  [   32/  146]
train() client id: f_00005-7-1 loss: 0.620758  [   64/  146]
train() client id: f_00005-7-2 loss: 0.629576  [   96/  146]
train() client id: f_00005-7-3 loss: 0.794981  [  128/  146]
train() client id: f_00005-8-0 loss: 0.693745  [   32/  146]
train() client id: f_00005-8-1 loss: 0.634423  [   64/  146]
train() client id: f_00005-8-2 loss: 0.705807  [   96/  146]
train() client id: f_00005-8-3 loss: 0.816347  [  128/  146]
train() client id: f_00005-9-0 loss: 0.777921  [   32/  146]
train() client id: f_00005-9-1 loss: 0.655092  [   64/  146]
train() client id: f_00005-9-2 loss: 0.714021  [   96/  146]
train() client id: f_00005-9-3 loss: 0.651531  [  128/  146]
train() client id: f_00005-10-0 loss: 0.577753  [   32/  146]
train() client id: f_00005-10-1 loss: 0.676833  [   64/  146]
train() client id: f_00005-10-2 loss: 0.928630  [   96/  146]
train() client id: f_00005-10-3 loss: 0.750198  [  128/  146]
train() client id: f_00005-11-0 loss: 0.696940  [   32/  146]
train() client id: f_00005-11-1 loss: 0.709780  [   64/  146]
train() client id: f_00005-11-2 loss: 0.809506  [   96/  146]
train() client id: f_00005-11-3 loss: 0.647648  [  128/  146]
train() client id: f_00006-0-0 loss: 0.835404  [   32/   54]
train() client id: f_00006-1-0 loss: 0.775656  [   32/   54]
train() client id: f_00006-2-0 loss: 0.792750  [   32/   54]
train() client id: f_00006-3-0 loss: 0.780122  [   32/   54]
train() client id: f_00006-4-0 loss: 0.792226  [   32/   54]
train() client id: f_00006-5-0 loss: 0.785310  [   32/   54]
train() client id: f_00006-6-0 loss: 0.789188  [   32/   54]
train() client id: f_00006-7-0 loss: 0.781173  [   32/   54]
train() client id: f_00006-8-0 loss: 0.848108  [   32/   54]
train() client id: f_00006-9-0 loss: 0.786725  [   32/   54]
train() client id: f_00006-10-0 loss: 0.781390  [   32/   54]
train() client id: f_00006-11-0 loss: 0.776636  [   32/   54]
train() client id: f_00007-0-0 loss: 0.789397  [   32/  179]
train() client id: f_00007-0-1 loss: 0.818068  [   64/  179]
train() client id: f_00007-0-2 loss: 0.744937  [   96/  179]
train() client id: f_00007-0-3 loss: 0.719983  [  128/  179]
train() client id: f_00007-0-4 loss: 0.751763  [  160/  179]
train() client id: f_00007-1-0 loss: 0.798378  [   32/  179]
train() client id: f_00007-1-1 loss: 0.700098  [   64/  179]
train() client id: f_00007-1-2 loss: 0.721860  [   96/  179]
train() client id: f_00007-1-3 loss: 0.782964  [  128/  179]
train() client id: f_00007-1-4 loss: 0.794446  [  160/  179]
train() client id: f_00007-2-0 loss: 0.736764  [   32/  179]
train() client id: f_00007-2-1 loss: 0.687788  [   64/  179]
train() client id: f_00007-2-2 loss: 0.845018  [   96/  179]
train() client id: f_00007-2-3 loss: 0.709466  [  128/  179]
train() client id: f_00007-2-4 loss: 0.714941  [  160/  179]
train() client id: f_00007-3-0 loss: 0.754830  [   32/  179]
train() client id: f_00007-3-1 loss: 0.708678  [   64/  179]
train() client id: f_00007-3-2 loss: 0.772575  [   96/  179]
train() client id: f_00007-3-3 loss: 0.642630  [  128/  179]
train() client id: f_00007-3-4 loss: 0.786926  [  160/  179]
train() client id: f_00007-4-0 loss: 0.822511  [   32/  179]
train() client id: f_00007-4-1 loss: 0.840869  [   64/  179]
train() client id: f_00007-4-2 loss: 0.638649  [   96/  179]
train() client id: f_00007-4-3 loss: 0.632550  [  128/  179]
train() client id: f_00007-4-4 loss: 0.710884  [  160/  179]
train() client id: f_00007-5-0 loss: 0.881673  [   32/  179]
train() client id: f_00007-5-1 loss: 0.696023  [   64/  179]
train() client id: f_00007-5-2 loss: 0.637291  [   96/  179]
train() client id: f_00007-5-3 loss: 0.650133  [  128/  179]
train() client id: f_00007-5-4 loss: 0.768108  [  160/  179]
train() client id: f_00007-6-0 loss: 0.648882  [   32/  179]
train() client id: f_00007-6-1 loss: 0.754679  [   64/  179]
train() client id: f_00007-6-2 loss: 0.768851  [   96/  179]
train() client id: f_00007-6-3 loss: 0.652726  [  128/  179]
train() client id: f_00007-6-4 loss: 0.749549  [  160/  179]
train() client id: f_00007-7-0 loss: 0.746641  [   32/  179]
train() client id: f_00007-7-1 loss: 0.756256  [   64/  179]
train() client id: f_00007-7-2 loss: 0.691340  [   96/  179]
train() client id: f_00007-7-3 loss: 0.790055  [  128/  179]
train() client id: f_00007-7-4 loss: 0.633780  [  160/  179]
train() client id: f_00007-8-0 loss: 0.762468  [   32/  179]
train() client id: f_00007-8-1 loss: 0.697558  [   64/  179]
train() client id: f_00007-8-2 loss: 0.819617  [   96/  179]
train() client id: f_00007-8-3 loss: 0.683449  [  128/  179]
train() client id: f_00007-8-4 loss: 0.641524  [  160/  179]
train() client id: f_00007-9-0 loss: 0.620515  [   32/  179]
train() client id: f_00007-9-1 loss: 0.849202  [   64/  179]
train() client id: f_00007-9-2 loss: 0.631707  [   96/  179]
train() client id: f_00007-9-3 loss: 0.746014  [  128/  179]
train() client id: f_00007-9-4 loss: 0.754888  [  160/  179]
train() client id: f_00007-10-0 loss: 0.767458  [   32/  179]
train() client id: f_00007-10-1 loss: 0.693199  [   64/  179]
train() client id: f_00007-10-2 loss: 0.766892  [   96/  179]
train() client id: f_00007-10-3 loss: 0.706753  [  128/  179]
train() client id: f_00007-10-4 loss: 0.614940  [  160/  179]
train() client id: f_00007-11-0 loss: 0.616256  [   32/  179]
train() client id: f_00007-11-1 loss: 0.632114  [   64/  179]
train() client id: f_00007-11-2 loss: 0.693715  [   96/  179]
train() client id: f_00007-11-3 loss: 0.895455  [  128/  179]
train() client id: f_00007-11-4 loss: 0.700983  [  160/  179]
train() client id: f_00008-0-0 loss: 0.680929  [   32/  130]
train() client id: f_00008-0-1 loss: 0.672268  [   64/  130]
train() client id: f_00008-0-2 loss: 0.729071  [   96/  130]
train() client id: f_00008-0-3 loss: 0.823815  [  128/  130]
train() client id: f_00008-1-0 loss: 0.682501  [   32/  130]
train() client id: f_00008-1-1 loss: 0.790913  [   64/  130]
train() client id: f_00008-1-2 loss: 0.721429  [   96/  130]
train() client id: f_00008-1-3 loss: 0.710649  [  128/  130]
train() client id: f_00008-2-0 loss: 0.664242  [   32/  130]
train() client id: f_00008-2-1 loss: 0.797161  [   64/  130]
train() client id: f_00008-2-2 loss: 0.703401  [   96/  130]
train() client id: f_00008-2-3 loss: 0.717272  [  128/  130]
train() client id: f_00008-3-0 loss: 0.727370  [   32/  130]
train() client id: f_00008-3-1 loss: 0.611706  [   64/  130]
train() client id: f_00008-3-2 loss: 0.776279  [   96/  130]
train() client id: f_00008-3-3 loss: 0.772370  [  128/  130]
train() client id: f_00008-4-0 loss: 0.794205  [   32/  130]
train() client id: f_00008-4-1 loss: 0.696784  [   64/  130]
train() client id: f_00008-4-2 loss: 0.663029  [   96/  130]
train() client id: f_00008-4-3 loss: 0.720303  [  128/  130]
train() client id: f_00008-5-0 loss: 0.674823  [   32/  130]
train() client id: f_00008-5-1 loss: 0.751317  [   64/  130]
train() client id: f_00008-5-2 loss: 0.625009  [   96/  130]
train() client id: f_00008-5-3 loss: 0.770182  [  128/  130]
train() client id: f_00008-6-0 loss: 0.734493  [   32/  130]
train() client id: f_00008-6-1 loss: 0.752097  [   64/  130]
train() client id: f_00008-6-2 loss: 0.633612  [   96/  130]
train() client id: f_00008-6-3 loss: 0.736366  [  128/  130]
train() client id: f_00008-7-0 loss: 0.655261  [   32/  130]
train() client id: f_00008-7-1 loss: 0.724322  [   64/  130]
train() client id: f_00008-7-2 loss: 0.746415  [   96/  130]
train() client id: f_00008-7-3 loss: 0.718087  [  128/  130]
train() client id: f_00008-8-0 loss: 0.673596  [   32/  130]
train() client id: f_00008-8-1 loss: 0.698751  [   64/  130]
train() client id: f_00008-8-2 loss: 0.860964  [   96/  130]
train() client id: f_00008-8-3 loss: 0.601093  [  128/  130]
train() client id: f_00008-9-0 loss: 0.718203  [   32/  130]
train() client id: f_00008-9-1 loss: 0.639569  [   64/  130]
train() client id: f_00008-9-2 loss: 0.716042  [   96/  130]
train() client id: f_00008-9-3 loss: 0.759895  [  128/  130]
train() client id: f_00008-10-0 loss: 0.665906  [   32/  130]
train() client id: f_00008-10-1 loss: 0.693255  [   64/  130]
train() client id: f_00008-10-2 loss: 0.705440  [   96/  130]
train() client id: f_00008-10-3 loss: 0.737880  [  128/  130]
train() client id: f_00008-11-0 loss: 0.690317  [   32/  130]
train() client id: f_00008-11-1 loss: 0.798941  [   64/  130]
train() client id: f_00008-11-2 loss: 0.665585  [   96/  130]
train() client id: f_00008-11-3 loss: 0.702319  [  128/  130]
train() client id: f_00009-0-0 loss: 1.154999  [   32/  118]
train() client id: f_00009-0-1 loss: 1.033749  [   64/  118]
train() client id: f_00009-0-2 loss: 1.234264  [   96/  118]
train() client id: f_00009-1-0 loss: 1.075995  [   32/  118]
train() client id: f_00009-1-1 loss: 1.031698  [   64/  118]
train() client id: f_00009-1-2 loss: 1.132642  [   96/  118]
train() client id: f_00009-2-0 loss: 1.030472  [   32/  118]
train() client id: f_00009-2-1 loss: 1.050646  [   64/  118]
train() client id: f_00009-2-2 loss: 0.963716  [   96/  118]
train() client id: f_00009-3-0 loss: 1.101542  [   32/  118]
train() client id: f_00009-3-1 loss: 0.980965  [   64/  118]
train() client id: f_00009-3-2 loss: 0.974336  [   96/  118]
train() client id: f_00009-4-0 loss: 0.965346  [   32/  118]
train() client id: f_00009-4-1 loss: 0.999508  [   64/  118]
train() client id: f_00009-4-2 loss: 1.058869  [   96/  118]
train() client id: f_00009-5-0 loss: 1.052016  [   32/  118]
train() client id: f_00009-5-1 loss: 0.929022  [   64/  118]
train() client id: f_00009-5-2 loss: 0.991329  [   96/  118]
train() client id: f_00009-6-0 loss: 0.916618  [   32/  118]
train() client id: f_00009-6-1 loss: 1.023998  [   64/  118]
train() client id: f_00009-6-2 loss: 0.920209  [   96/  118]
train() client id: f_00009-7-0 loss: 0.872124  [   32/  118]
train() client id: f_00009-7-1 loss: 0.961748  [   64/  118]
train() client id: f_00009-7-2 loss: 0.982123  [   96/  118]
train() client id: f_00009-8-0 loss: 0.992340  [   32/  118]
train() client id: f_00009-8-1 loss: 0.823639  [   64/  118]
train() client id: f_00009-8-2 loss: 0.951756  [   96/  118]
train() client id: f_00009-9-0 loss: 1.002346  [   32/  118]
train() client id: f_00009-9-1 loss: 0.889179  [   64/  118]
train() client id: f_00009-9-2 loss: 0.919847  [   96/  118]
train() client id: f_00009-10-0 loss: 0.851032  [   32/  118]
train() client id: f_00009-10-1 loss: 1.036002  [   64/  118]
train() client id: f_00009-10-2 loss: 0.953077  [   96/  118]
train() client id: f_00009-11-0 loss: 0.898028  [   32/  118]
train() client id: f_00009-11-1 loss: 1.007067  [   64/  118]
train() client id: f_00009-11-2 loss: 0.926031  [   96/  118]
At round 6 accuracy: 0.6312997347480106
At round 6 training accuracy: 0.5747820254862508
At round 6 training loss: 0.8842468196522422
update_location
xs = [ -3.9056584    4.20031788  50.00902392  18.81129433  -4.02070377
   3.95640986 -12.44319194   3.67514815  34.66397685   2.93912145]
ys = [ 42.5879595   25.55583871   1.32061395 -12.45517586   9.35018685
 -12.18584926  -2.62498432  -4.17765202  17.56900603   4.00148178]
dists_uav = [108.76115328 103.2992912  111.81523373 102.51339522 100.51662575
 100.81739979 100.8053747  100.15467783 107.28588567 100.1231756 ]
dists_bs = [216.34745711 233.33733364 284.24213267 269.63356758 238.06255551
 258.96706121 240.78071123 253.04039114 262.18981874 246.78497559]
uav_gains = [8.10606699e-11 9.22049228e-11 7.56379401e-11 9.39823243e-11
 9.87196326e-11 9.79849732e-11 9.80141981e-11 9.96139759e-11
 8.38762801e-11 9.96923511e-11]
bs_gains = [3.19793078e-11 2.58784487e-11 1.48924591e-11 1.72634266e-11
 2.44657779e-11 1.93290009e-11 2.37002728e-11 2.06235063e-11
 1.86710946e-11 2.21208434e-11]
Round 7
-------------------------------
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.06798939 21.0326556   9.91312762  3.54368464 24.2561337  11.70105956
  4.40648092 14.22483624 10.44629039  9.49400071]
obj_prev = 119.08625877018483
eta_min = 5.793901722590025e-10	eta_max = 0.9200929959618319
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 27.695540866338916	eta = 0.909090909090909
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 47.81923698001299	eta = 0.5265195769323551
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 38.22376664223015	eta = 0.6586939654483899
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 36.504271658621605	eta = 0.6897210457833629
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 36.41961812780936	eta = 0.6913242290346582
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 36.41939846351146	eta = 0.6913283987699586
eta = 0.6913283987699586
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [0.03046057 0.06406391 0.02997707 0.01039528 0.07397568 0.03529559
 0.01305453 0.04327336 0.03142758 0.02852657]
ene_total = [3.09168549 6.04664955 3.05847947 1.40079217 6.88392674 3.69460946
 1.61888686 4.14166343 3.37804549 3.10465979]
ti_comp = [0.28943977 0.27173208 0.28857988 0.29120492 0.27061872 0.2656134
 0.2916892  0.29187392 0.26482942 0.26854656]
ti_coms = [0.0655023  0.08320999 0.06636219 0.06373715 0.08432335 0.08932867
 0.06325287 0.06306815 0.09011265 0.08639551]
t_total = [29.64997063 29.64997063 29.64997063 29.64997063 29.64997063 29.64997063
 29.64997063 29.64997063 29.64997063 29.64997063]
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.10852063e-05 2.22555556e-04 2.02169483e-05 8.27924223e-07
 3.45486619e-04 3.89530298e-05 1.63426776e-06 5.94499024e-05
 2.76617870e-05 2.01182160e-05]
ene_total = [0.54893267 0.71368309 0.55604324 0.53249539 0.73325245 0.74945808
 0.52851731 0.53180388 0.75506382 0.72338265]
optimize_network iter = 0 obj = 6.3726325729552675
eta = 0.6913283987699586
freqs = [5.26198833e+07 1.17880646e+08 5.19389521e+07 1.78487289e+07
 1.36678791e+08 6.64416515e+07 2.23774684e+07 7.41302190e+07
 5.93355081e+07 5.31128927e+07]
eta_min = 0.6839075543866936	eta_max = 0.6913283987699287
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 0.058240867111970324	eta = 0.9090909090909091
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 21.478076657689165	eta = 0.0024651296143925966
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 2.271073836819586	eta = 0.023313307551114187
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 2.1977536965308997	eta = 0.02409107213089359
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 2.1977247993541873	eta = 0.0240913888966546
eta = 0.0240913888966546
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.12822560e-04 2.24635427e-03 2.04058838e-04 8.35661507e-06
 3.48715330e-03 3.93170615e-04 1.64954064e-05 6.00054856e-04
 2.79202975e-04 2.03062288e-04]
ene_total = [0.17868869 0.2792033  0.18072909 0.16862269 0.31492846 0.24640612
 0.16755818 0.18248854 0.24546632 0.23363341]
ti_comp = [0.29797302 0.28026532 0.29711312 0.29973816 0.27915197 0.27414664
 0.30022245 0.30040716 0.27336266 0.27707981]
ti_coms = [0.0655023  0.08320999 0.06636219 0.06373715 0.08432335 0.08932867
 0.06325287 0.06306815 0.09011265 0.08639551]
t_total = [29.64997063 29.64997063 29.64997063 29.64997063 29.64997063 29.64997063
 29.64997063 29.64997063 29.64997063 29.64997063]
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.16912899e-05 2.28100645e-04 2.07945251e-05 8.52018260e-07
 3.54005908e-04 3.98676209e-05 1.68198679e-06 6.11879833e-05
 2.83060505e-05 2.06045860e-05]
ene_total = [0.53609492 0.69738041 0.54303623 0.51999606 0.71673297 0.7319378
 0.5161133  0.5194606  0.73738988 0.7064396 ]
optimize_network iter = 1 obj = 6.224581758881931
eta = 0.6839075543866936
freqs = [5.26073161e+07 1.17632966e+08 5.19221168e+07 1.78475642e+07
 1.36374531e+08 6.62555801e+07 2.23770658e+07 7.41302190e+07
 5.91638860e+07 5.29821614e+07]
eta_min = 0.6839075543866936	eta_max = 0.6839075543866819
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 0.05801261670554691	eta = 0.9090909090909091
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 21.4778591118164	eta = 0.0024554934542136467
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 2.270009752860956	eta = 0.023232826375798606
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 2.196960837665713	eta = 0.024005317507445147
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 2.196932244064364	eta = 0.024005629942423928
eta = 0.024005629942423928
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.12924512e-04 2.23906549e-03 2.04121753e-04 8.36352164e-06
 3.47496787e-03 3.91345733e-04 1.65106003e-05 6.00629173e-04
 2.77855860e-04 2.02257286e-04]
ene_total = [0.17867568 0.27898621 0.18071488 0.16860805 0.31457886 0.24633625
 0.16754386 0.18248768 0.24540916 0.23359161]
ti_comp = [0.29797302 0.28026532 0.29711312 0.29973816 0.27915197 0.27414664
 0.30022245 0.30040716 0.27336266 0.27707981]
ti_coms = [0.0655023  0.08320999 0.06636219 0.06373715 0.08432335 0.08932867
 0.06325287 0.06306815 0.09011265 0.08639551]
t_total = [29.64997063 29.64997063 29.64997063 29.64997063 29.64997063 29.64997063
 29.64997063 29.64997063 29.64997063 29.64997063]
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.16912899e-05 2.28100645e-04 2.07945251e-05 8.52018260e-07
 3.54005908e-04 3.98676209e-05 1.68198679e-06 6.11879833e-05
 2.83060505e-05 2.06045860e-05]
ene_total = [0.53609492 0.69738041 0.54303623 0.51999606 0.71673297 0.7319378
 0.5161133  0.5194606  0.73738988 0.7064396 ]
optimize_network iter = 2 obj = 6.224581758881931
eta = 0.6839075543866936
freqs = [5.26073161e+07 1.17632966e+08 5.19221168e+07 1.78475642e+07
 1.36374531e+08 6.62555801e+07 2.23770658e+07 7.41302190e+07
 5.91638860e+07 5.29821614e+07]
Done!
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.09225246e-05 2.20016485e-04 2.00575422e-05 8.21821712e-07
 3.41459515e-04 3.84546646e-05 1.62237516e-06 5.90194079e-05
 2.73028502e-05 1.98743347e-05]
ene_total = [0.00657115 0.00854102 0.00665628 0.00637454 0.00877379 0.00897132
 0.00632691 0.00636583 0.00903857 0.00865943]
At round 7 energy consumption: 0.07627883514893437
At round 7 eta: 0.6839075543866936
At round 7 a_n: 25.442236080499836
At round 7 local rounds: 12.440921153911754
At round 7 global rounds: 81.57354686993413
gradient difference: 0.4832950234413147
train() client id: f_00000-0-0 loss: 1.487027  [   32/  126]
train() client id: f_00000-0-1 loss: 1.385683  [   64/  126]
train() client id: f_00000-0-2 loss: 1.388872  [   96/  126]
train() client id: f_00000-1-0 loss: 1.564387  [   32/  126]
train() client id: f_00000-1-1 loss: 1.295447  [   64/  126]
train() client id: f_00000-1-2 loss: 1.078044  [   96/  126]
train() client id: f_00000-2-0 loss: 1.091226  [   32/  126]
train() client id: f_00000-2-1 loss: 1.470563  [   64/  126]
train() client id: f_00000-2-2 loss: 1.086482  [   96/  126]
train() client id: f_00000-3-0 loss: 1.169219  [   32/  126]
train() client id: f_00000-3-1 loss: 1.132173  [   64/  126]
train() client id: f_00000-3-2 loss: 1.117285  [   96/  126]
train() client id: f_00000-4-0 loss: 1.080166  [   32/  126]
train() client id: f_00000-4-1 loss: 0.984522  [   64/  126]
train() client id: f_00000-4-2 loss: 1.089815  [   96/  126]
train() client id: f_00000-5-0 loss: 1.123515  [   32/  126]
train() client id: f_00000-5-1 loss: 1.012286  [   64/  126]
train() client id: f_00000-5-2 loss: 1.025597  [   96/  126]
train() client id: f_00000-6-0 loss: 0.994297  [   32/  126]
train() client id: f_00000-6-1 loss: 1.009509  [   64/  126]
train() client id: f_00000-6-2 loss: 1.008224  [   96/  126]
train() client id: f_00000-7-0 loss: 0.993824  [   32/  126]
train() client id: f_00000-7-1 loss: 1.017920  [   64/  126]
train() client id: f_00000-7-2 loss: 0.945443  [   96/  126]
train() client id: f_00000-8-0 loss: 1.018548  [   32/  126]
train() client id: f_00000-8-1 loss: 1.009200  [   64/  126]
train() client id: f_00000-8-2 loss: 0.958667  [   96/  126]
train() client id: f_00000-9-0 loss: 0.985532  [   32/  126]
train() client id: f_00000-9-1 loss: 0.966226  [   64/  126]
train() client id: f_00000-9-2 loss: 1.022411  [   96/  126]
train() client id: f_00000-10-0 loss: 0.967688  [   32/  126]
train() client id: f_00000-10-1 loss: 0.984843  [   64/  126]
train() client id: f_00000-10-2 loss: 1.053011  [   96/  126]
train() client id: f_00000-11-0 loss: 0.978764  [   32/  126]
train() client id: f_00000-11-1 loss: 1.004707  [   64/  126]
train() client id: f_00000-11-2 loss: 1.021092  [   96/  126]
train() client id: f_00001-0-0 loss: 0.647155  [   32/  265]
train() client id: f_00001-0-1 loss: 0.624518  [   64/  265]
train() client id: f_00001-0-2 loss: 0.709147  [   96/  265]
train() client id: f_00001-0-3 loss: 0.567409  [  128/  265]
train() client id: f_00001-0-4 loss: 0.618655  [  160/  265]
train() client id: f_00001-0-5 loss: 0.551904  [  192/  265]
train() client id: f_00001-0-6 loss: 0.657740  [  224/  265]
train() client id: f_00001-0-7 loss: 0.653928  [  256/  265]
train() client id: f_00001-1-0 loss: 0.556199  [   32/  265]
train() client id: f_00001-1-1 loss: 0.622408  [   64/  265]
train() client id: f_00001-1-2 loss: 0.532083  [   96/  265]
train() client id: f_00001-1-3 loss: 0.594087  [  128/  265]
train() client id: f_00001-1-4 loss: 0.603332  [  160/  265]
train() client id: f_00001-1-5 loss: 0.607610  [  192/  265]
train() client id: f_00001-1-6 loss: 0.618712  [  224/  265]
train() client id: f_00001-1-7 loss: 0.670223  [  256/  265]
train() client id: f_00001-2-0 loss: 0.639383  [   32/  265]
train() client id: f_00001-2-1 loss: 0.636272  [   64/  265]
train() client id: f_00001-2-2 loss: 0.621561  [   96/  265]
train() client id: f_00001-2-3 loss: 0.627371  [  128/  265]
train() client id: f_00001-2-4 loss: 0.531525  [  160/  265]
train() client id: f_00001-2-5 loss: 0.547911  [  192/  265]
train() client id: f_00001-2-6 loss: 0.574838  [  224/  265]
train() client id: f_00001-2-7 loss: 0.616794  [  256/  265]
train() client id: f_00001-3-0 loss: 0.516995  [   32/  265]
train() client id: f_00001-3-1 loss: 0.636279  [   64/  265]
train() client id: f_00001-3-2 loss: 0.627741  [   96/  265]
train() client id: f_00001-3-3 loss: 0.681423  [  128/  265]
train() client id: f_00001-3-4 loss: 0.621296  [  160/  265]
train() client id: f_00001-3-5 loss: 0.556347  [  192/  265]
train() client id: f_00001-3-6 loss: 0.602671  [  224/  265]
train() client id: f_00001-3-7 loss: 0.538000  [  256/  265]
train() client id: f_00001-4-0 loss: 0.542568  [   32/  265]
train() client id: f_00001-4-1 loss: 0.607278  [   64/  265]
train() client id: f_00001-4-2 loss: 0.506938  [   96/  265]
train() client id: f_00001-4-3 loss: 0.519333  [  128/  265]
train() client id: f_00001-4-4 loss: 0.729678  [  160/  265]
train() client id: f_00001-4-5 loss: 0.523514  [  192/  265]
train() client id: f_00001-4-6 loss: 0.684473  [  224/  265]
train() client id: f_00001-4-7 loss: 0.568476  [  256/  265]
train() client id: f_00001-5-0 loss: 0.517434  [   32/  265]
train() client id: f_00001-5-1 loss: 0.604277  [   64/  265]
train() client id: f_00001-5-2 loss: 0.611560  [   96/  265]
train() client id: f_00001-5-3 loss: 0.690531  [  128/  265]
train() client id: f_00001-5-4 loss: 0.545650  [  160/  265]
train() client id: f_00001-5-5 loss: 0.613285  [  192/  265]
train() client id: f_00001-5-6 loss: 0.549999  [  224/  265]
train() client id: f_00001-5-7 loss: 0.561318  [  256/  265]
train() client id: f_00001-6-0 loss: 0.843929  [   32/  265]
train() client id: f_00001-6-1 loss: 0.496307  [   64/  265]
train() client id: f_00001-6-2 loss: 0.626267  [   96/  265]
train() client id: f_00001-6-3 loss: 0.608386  [  128/  265]
train() client id: f_00001-6-4 loss: 0.555465  [  160/  265]
train() client id: f_00001-6-5 loss: 0.520159  [  192/  265]
train() client id: f_00001-6-6 loss: 0.553317  [  224/  265]
train() client id: f_00001-6-7 loss: 0.487324  [  256/  265]
train() client id: f_00001-7-0 loss: 0.567942  [   32/  265]
train() client id: f_00001-7-1 loss: 0.651066  [   64/  265]
train() client id: f_00001-7-2 loss: 0.556389  [   96/  265]
train() client id: f_00001-7-3 loss: 0.620408  [  128/  265]
train() client id: f_00001-7-4 loss: 0.613309  [  160/  265]
train() client id: f_00001-7-5 loss: 0.558814  [  192/  265]
train() client id: f_00001-7-6 loss: 0.502698  [  224/  265]
train() client id: f_00001-7-7 loss: 0.608859  [  256/  265]
train() client id: f_00001-8-0 loss: 0.617573  [   32/  265]
train() client id: f_00001-8-1 loss: 0.481358  [   64/  265]
train() client id: f_00001-8-2 loss: 0.666427  [   96/  265]
train() client id: f_00001-8-3 loss: 0.503620  [  128/  265]
train() client id: f_00001-8-4 loss: 0.558747  [  160/  265]
train() client id: f_00001-8-5 loss: 0.629889  [  192/  265]
train() client id: f_00001-8-6 loss: 0.537116  [  224/  265]
train() client id: f_00001-8-7 loss: 0.674005  [  256/  265]
train() client id: f_00001-9-0 loss: 0.515556  [   32/  265]
train() client id: f_00001-9-1 loss: 0.601193  [   64/  265]
train() client id: f_00001-9-2 loss: 0.623488  [   96/  265]
train() client id: f_00001-9-3 loss: 0.554658  [  128/  265]
train() client id: f_00001-9-4 loss: 0.501638  [  160/  265]
train() client id: f_00001-9-5 loss: 0.561481  [  192/  265]
train() client id: f_00001-9-6 loss: 0.539966  [  224/  265]
train() client id: f_00001-9-7 loss: 0.717869  [  256/  265]
train() client id: f_00001-10-0 loss: 0.655279  [   32/  265]
train() client id: f_00001-10-1 loss: 0.481340  [   64/  265]
train() client id: f_00001-10-2 loss: 0.497751  [   96/  265]
train() client id: f_00001-10-3 loss: 0.558544  [  128/  265]
train() client id: f_00001-10-4 loss: 0.573742  [  160/  265]
train() client id: f_00001-10-5 loss: 0.765129  [  192/  265]
train() client id: f_00001-10-6 loss: 0.546176  [  224/  265]
train() client id: f_00001-10-7 loss: 0.600583  [  256/  265]
train() client id: f_00001-11-0 loss: 0.572248  [   32/  265]
train() client id: f_00001-11-1 loss: 0.496978  [   64/  265]
train() client id: f_00001-11-2 loss: 0.607484  [   96/  265]
train() client id: f_00001-11-3 loss: 0.681418  [  128/  265]
train() client id: f_00001-11-4 loss: 0.555879  [  160/  265]
train() client id: f_00001-11-5 loss: 0.509261  [  192/  265]
train() client id: f_00001-11-6 loss: 0.539578  [  224/  265]
train() client id: f_00001-11-7 loss: 0.610830  [  256/  265]
train() client id: f_00002-0-0 loss: 1.183515  [   32/  124]
train() client id: f_00002-0-1 loss: 1.213729  [   64/  124]
train() client id: f_00002-0-2 loss: 1.150905  [   96/  124]
train() client id: f_00002-1-0 loss: 1.157130  [   32/  124]
train() client id: f_00002-1-1 loss: 1.143915  [   64/  124]
train() client id: f_00002-1-2 loss: 1.152712  [   96/  124]
train() client id: f_00002-2-0 loss: 1.110878  [   32/  124]
train() client id: f_00002-2-1 loss: 1.089238  [   64/  124]
train() client id: f_00002-2-2 loss: 1.132450  [   96/  124]
train() client id: f_00002-3-0 loss: 1.116145  [   32/  124]
train() client id: f_00002-3-1 loss: 1.135586  [   64/  124]
train() client id: f_00002-3-2 loss: 1.056333  [   96/  124]
train() client id: f_00002-4-0 loss: 0.979356  [   32/  124]
train() client id: f_00002-4-1 loss: 1.081022  [   64/  124]
train() client id: f_00002-4-2 loss: 1.115601  [   96/  124]
train() client id: f_00002-5-0 loss: 1.096940  [   32/  124]
train() client id: f_00002-5-1 loss: 1.104940  [   64/  124]
train() client id: f_00002-5-2 loss: 1.052825  [   96/  124]
train() client id: f_00002-6-0 loss: 1.025315  [   32/  124]
train() client id: f_00002-6-1 loss: 1.024565  [   64/  124]
train() client id: f_00002-6-2 loss: 1.085600  [   96/  124]
train() client id: f_00002-7-0 loss: 0.978781  [   32/  124]
train() client id: f_00002-7-1 loss: 1.064785  [   64/  124]
train() client id: f_00002-7-2 loss: 1.049716  [   96/  124]
train() client id: f_00002-8-0 loss: 1.049303  [   32/  124]
train() client id: f_00002-8-1 loss: 0.964812  [   64/  124]
train() client id: f_00002-8-2 loss: 1.037641  [   96/  124]
train() client id: f_00002-9-0 loss: 1.043617  [   32/  124]
train() client id: f_00002-9-1 loss: 1.067969  [   64/  124]
train() client id: f_00002-9-2 loss: 0.945448  [   96/  124]
train() client id: f_00002-10-0 loss: 1.026084  [   32/  124]
train() client id: f_00002-10-1 loss: 1.073583  [   64/  124]
train() client id: f_00002-10-2 loss: 0.953449  [   96/  124]
train() client id: f_00002-11-0 loss: 0.939531  [   32/  124]
train() client id: f_00002-11-1 loss: 1.119880  [   64/  124]
train() client id: f_00002-11-2 loss: 0.889301  [   96/  124]
train() client id: f_00003-0-0 loss: 0.759670  [   32/   43]
train() client id: f_00003-1-0 loss: 0.937151  [   32/   43]
train() client id: f_00003-2-0 loss: 1.004344  [   32/   43]
train() client id: f_00003-3-0 loss: 0.897896  [   32/   43]
train() client id: f_00003-4-0 loss: 0.801866  [   32/   43]
train() client id: f_00003-5-0 loss: 0.810298  [   32/   43]
train() client id: f_00003-6-0 loss: 0.768584  [   32/   43]
train() client id: f_00003-7-0 loss: 0.865696  [   32/   43]
train() client id: f_00003-8-0 loss: 0.821704  [   32/   43]
train() client id: f_00003-9-0 loss: 0.870958  [   32/   43]
train() client id: f_00003-10-0 loss: 0.793023  [   32/   43]
train() client id: f_00003-11-0 loss: 0.851236  [   32/   43]
train() client id: f_00004-0-0 loss: 0.656290  [   32/  306]
train() client id: f_00004-0-1 loss: 0.682176  [   64/  306]
train() client id: f_00004-0-2 loss: 0.839892  [   96/  306]
train() client id: f_00004-0-3 loss: 0.804865  [  128/  306]
train() client id: f_00004-0-4 loss: 0.748137  [  160/  306]
train() client id: f_00004-0-5 loss: 0.828128  [  192/  306]
train() client id: f_00004-0-6 loss: 0.856720  [  224/  306]
train() client id: f_00004-0-7 loss: 0.584937  [  256/  306]
train() client id: f_00004-0-8 loss: 0.738308  [  288/  306]
train() client id: f_00004-1-0 loss: 0.829202  [   32/  306]
train() client id: f_00004-1-1 loss: 0.818970  [   64/  306]
train() client id: f_00004-1-2 loss: 0.633176  [   96/  306]
train() client id: f_00004-1-3 loss: 0.814414  [  128/  306]
train() client id: f_00004-1-4 loss: 0.702747  [  160/  306]
train() client id: f_00004-1-5 loss: 0.862869  [  192/  306]
train() client id: f_00004-1-6 loss: 0.729851  [  224/  306]
train() client id: f_00004-1-7 loss: 0.665896  [  256/  306]
train() client id: f_00004-1-8 loss: 0.659779  [  288/  306]
train() client id: f_00004-2-0 loss: 0.740400  [   32/  306]
train() client id: f_00004-2-1 loss: 0.814767  [   64/  306]
train() client id: f_00004-2-2 loss: 0.765939  [   96/  306]
train() client id: f_00004-2-3 loss: 0.789475  [  128/  306]
train() client id: f_00004-2-4 loss: 0.742244  [  160/  306]
train() client id: f_00004-2-5 loss: 0.730860  [  192/  306]
train() client id: f_00004-2-6 loss: 0.656973  [  224/  306]
train() client id: f_00004-2-7 loss: 0.898167  [  256/  306]
train() client id: f_00004-2-8 loss: 0.605159  [  288/  306]
train() client id: f_00004-3-0 loss: 0.663662  [   32/  306]
train() client id: f_00004-3-1 loss: 0.785012  [   64/  306]
train() client id: f_00004-3-2 loss: 0.769509  [   96/  306]
train() client id: f_00004-3-3 loss: 0.715719  [  128/  306]
train() client id: f_00004-3-4 loss: 0.776985  [  160/  306]
train() client id: f_00004-3-5 loss: 0.794491  [  192/  306]
train() client id: f_00004-3-6 loss: 0.760555  [  224/  306]
train() client id: f_00004-3-7 loss: 0.658195  [  256/  306]
train() client id: f_00004-3-8 loss: 0.770112  [  288/  306]
train() client id: f_00004-4-0 loss: 0.948783  [   32/  306]
train() client id: f_00004-4-1 loss: 0.616490  [   64/  306]
train() client id: f_00004-4-2 loss: 0.799379  [   96/  306]
train() client id: f_00004-4-3 loss: 0.588913  [  128/  306]
train() client id: f_00004-4-4 loss: 0.741309  [  160/  306]
train() client id: f_00004-4-5 loss: 0.749754  [  192/  306]
train() client id: f_00004-4-6 loss: 0.831050  [  224/  306]
train() client id: f_00004-4-7 loss: 0.790958  [  256/  306]
train() client id: f_00004-4-8 loss: 0.790030  [  288/  306]
train() client id: f_00004-5-0 loss: 0.792036  [   32/  306]
train() client id: f_00004-5-1 loss: 0.797356  [   64/  306]
train() client id: f_00004-5-2 loss: 0.786840  [   96/  306]
train() client id: f_00004-5-3 loss: 0.829610  [  128/  306]
train() client id: f_00004-5-4 loss: 0.672631  [  160/  306]
train() client id: f_00004-5-5 loss: 0.661976  [  192/  306]
train() client id: f_00004-5-6 loss: 0.715245  [  224/  306]
train() client id: f_00004-5-7 loss: 0.701141  [  256/  306]
train() client id: f_00004-5-8 loss: 0.812087  [  288/  306]
train() client id: f_00004-6-0 loss: 0.836670  [   32/  306]
train() client id: f_00004-6-1 loss: 0.671587  [   64/  306]
train() client id: f_00004-6-2 loss: 0.777682  [   96/  306]
train() client id: f_00004-6-3 loss: 0.744204  [  128/  306]
train() client id: f_00004-6-4 loss: 0.680119  [  160/  306]
train() client id: f_00004-6-5 loss: 0.751891  [  192/  306]
train() client id: f_00004-6-6 loss: 0.841157  [  224/  306]
train() client id: f_00004-6-7 loss: 0.709062  [  256/  306]
train() client id: f_00004-6-8 loss: 0.919305  [  288/  306]
train() client id: f_00004-7-0 loss: 0.911449  [   32/  306]
train() client id: f_00004-7-1 loss: 0.684047  [   64/  306]
train() client id: f_00004-7-2 loss: 0.761078  [   96/  306]
train() client id: f_00004-7-3 loss: 0.766050  [  128/  306]
train() client id: f_00004-7-4 loss: 0.656958  [  160/  306]
train() client id: f_00004-7-5 loss: 0.807066  [  192/  306]
train() client id: f_00004-7-6 loss: 0.748388  [  224/  306]
train() client id: f_00004-7-7 loss: 0.856621  [  256/  306]
train() client id: f_00004-7-8 loss: 0.682961  [  288/  306]
train() client id: f_00004-8-0 loss: 0.722654  [   32/  306]
train() client id: f_00004-8-1 loss: 0.866574  [   64/  306]
train() client id: f_00004-8-2 loss: 0.747224  [   96/  306]
train() client id: f_00004-8-3 loss: 0.770053  [  128/  306]
train() client id: f_00004-8-4 loss: 0.725123  [  160/  306]
train() client id: f_00004-8-5 loss: 0.658663  [  192/  306]
train() client id: f_00004-8-6 loss: 0.842730  [  224/  306]
train() client id: f_00004-8-7 loss: 0.700191  [  256/  306]
train() client id: f_00004-8-8 loss: 0.829185  [  288/  306]
train() client id: f_00004-9-0 loss: 0.766851  [   32/  306]
train() client id: f_00004-9-1 loss: 0.743564  [   64/  306]
train() client id: f_00004-9-2 loss: 0.794797  [   96/  306]
train() client id: f_00004-9-3 loss: 0.808443  [  128/  306]
train() client id: f_00004-9-4 loss: 0.789172  [  160/  306]
train() client id: f_00004-9-5 loss: 0.713526  [  192/  306]
train() client id: f_00004-9-6 loss: 0.804697  [  224/  306]
train() client id: f_00004-9-7 loss: 0.800076  [  256/  306]
train() client id: f_00004-9-8 loss: 0.712660  [  288/  306]
train() client id: f_00004-10-0 loss: 0.815617  [   32/  306]
train() client id: f_00004-10-1 loss: 0.689255  [   64/  306]
train() client id: f_00004-10-2 loss: 0.768827  [   96/  306]
train() client id: f_00004-10-3 loss: 0.766082  [  128/  306]
train() client id: f_00004-10-4 loss: 0.687975  [  160/  306]
train() client id: f_00004-10-5 loss: 0.895447  [  192/  306]
train() client id: f_00004-10-6 loss: 0.651843  [  224/  306]
train() client id: f_00004-10-7 loss: 0.906729  [  256/  306]
train() client id: f_00004-10-8 loss: 0.751351  [  288/  306]
train() client id: f_00004-11-0 loss: 0.819625  [   32/  306]
train() client id: f_00004-11-1 loss: 0.652208  [   64/  306]
train() client id: f_00004-11-2 loss: 0.704097  [   96/  306]
train() client id: f_00004-11-3 loss: 0.807437  [  128/  306]
train() client id: f_00004-11-4 loss: 0.804183  [  160/  306]
train() client id: f_00004-11-5 loss: 0.789576  [  192/  306]
train() client id: f_00004-11-6 loss: 0.833711  [  224/  306]
train() client id: f_00004-11-7 loss: 0.750224  [  256/  306]
train() client id: f_00004-11-8 loss: 0.759239  [  288/  306]
train() client id: f_00005-0-0 loss: 0.815426  [   32/  146]
train() client id: f_00005-0-1 loss: 0.794949  [   64/  146]
train() client id: f_00005-0-2 loss: 0.850926  [   96/  146]
train() client id: f_00005-0-3 loss: 0.893982  [  128/  146]
train() client id: f_00005-1-0 loss: 0.868508  [   32/  146]
train() client id: f_00005-1-1 loss: 0.994327  [   64/  146]
train() client id: f_00005-1-2 loss: 0.669519  [   96/  146]
train() client id: f_00005-1-3 loss: 0.779460  [  128/  146]
train() client id: f_00005-2-0 loss: 0.919949  [   32/  146]
train() client id: f_00005-2-1 loss: 0.762978  [   64/  146]
train() client id: f_00005-2-2 loss: 0.847936  [   96/  146]
train() client id: f_00005-2-3 loss: 0.816866  [  128/  146]
train() client id: f_00005-3-0 loss: 0.800274  [   32/  146]
train() client id: f_00005-3-1 loss: 0.933367  [   64/  146]
train() client id: f_00005-3-2 loss: 0.874283  [   96/  146]
train() client id: f_00005-3-3 loss: 0.688495  [  128/  146]
train() client id: f_00005-4-0 loss: 0.782619  [   32/  146]
train() client id: f_00005-4-1 loss: 0.963423  [   64/  146]
train() client id: f_00005-4-2 loss: 0.745967  [   96/  146]
train() client id: f_00005-4-3 loss: 0.779759  [  128/  146]
train() client id: f_00005-5-0 loss: 0.744577  [   32/  146]
train() client id: f_00005-5-1 loss: 0.794091  [   64/  146]
train() client id: f_00005-5-2 loss: 0.887019  [   96/  146]
train() client id: f_00005-5-3 loss: 0.898318  [  128/  146]
train() client id: f_00005-6-0 loss: 0.761168  [   32/  146]
train() client id: f_00005-6-1 loss: 0.973757  [   64/  146]
train() client id: f_00005-6-2 loss: 0.852524  [   96/  146]
train() client id: f_00005-6-3 loss: 0.798715  [  128/  146]
train() client id: f_00005-7-0 loss: 0.728937  [   32/  146]
train() client id: f_00005-7-1 loss: 0.899674  [   64/  146]
train() client id: f_00005-7-2 loss: 0.838377  [   96/  146]
train() client id: f_00005-7-3 loss: 0.916456  [  128/  146]
train() client id: f_00005-8-0 loss: 0.962952  [   32/  146]
train() client id: f_00005-8-1 loss: 0.846486  [   64/  146]
train() client id: f_00005-8-2 loss: 0.766826  [   96/  146]
train() client id: f_00005-8-3 loss: 0.766950  [  128/  146]
train() client id: f_00005-9-0 loss: 0.916081  [   32/  146]
train() client id: f_00005-9-1 loss: 0.792908  [   64/  146]
train() client id: f_00005-9-2 loss: 0.966182  [   96/  146]
train() client id: f_00005-9-3 loss: 0.620037  [  128/  146]
train() client id: f_00005-10-0 loss: 0.812845  [   32/  146]
train() client id: f_00005-10-1 loss: 0.887427  [   64/  146]
train() client id: f_00005-10-2 loss: 0.791108  [   96/  146]
train() client id: f_00005-10-3 loss: 0.804947  [  128/  146]
train() client id: f_00005-11-0 loss: 0.912979  [   32/  146]
train() client id: f_00005-11-1 loss: 0.920076  [   64/  146]
train() client id: f_00005-11-2 loss: 0.828997  [   96/  146]
train() client id: f_00005-11-3 loss: 0.816761  [  128/  146]
train() client id: f_00006-0-0 loss: 0.790025  [   32/   54]
train() client id: f_00006-1-0 loss: 0.838243  [   32/   54]
train() client id: f_00006-2-0 loss: 0.756595  [   32/   54]
train() client id: f_00006-3-0 loss: 0.761225  [   32/   54]
train() client id: f_00006-4-0 loss: 0.834886  [   32/   54]
train() client id: f_00006-5-0 loss: 0.852492  [   32/   54]
train() client id: f_00006-6-0 loss: 0.792584  [   32/   54]
train() client id: f_00006-7-0 loss: 0.810441  [   32/   54]
train() client id: f_00006-8-0 loss: 0.851115  [   32/   54]
train() client id: f_00006-9-0 loss: 0.790747  [   32/   54]
train() client id: f_00006-10-0 loss: 0.844099  [   32/   54]
train() client id: f_00006-11-0 loss: 0.754151  [   32/   54]
train() client id: f_00007-0-0 loss: 0.629077  [   32/  179]
train() client id: f_00007-0-1 loss: 0.673377  [   64/  179]
train() client id: f_00007-0-2 loss: 0.730435  [   96/  179]
train() client id: f_00007-0-3 loss: 0.710426  [  128/  179]
train() client id: f_00007-0-4 loss: 0.593735  [  160/  179]
train() client id: f_00007-1-0 loss: 0.685398  [   32/  179]
train() client id: f_00007-1-1 loss: 0.697751  [   64/  179]
train() client id: f_00007-1-2 loss: 0.754253  [   96/  179]
train() client id: f_00007-1-3 loss: 0.567700  [  128/  179]
train() client id: f_00007-1-4 loss: 0.621408  [  160/  179]
train() client id: f_00007-2-0 loss: 0.692200  [   32/  179]
train() client id: f_00007-2-1 loss: 0.707211  [   64/  179]
train() client id: f_00007-2-2 loss: 0.638477  [   96/  179]
train() client id: f_00007-2-3 loss: 0.603545  [  128/  179]
train() client id: f_00007-2-4 loss: 0.678065  [  160/  179]
train() client id: f_00007-3-0 loss: 0.660287  [   32/  179]
train() client id: f_00007-3-1 loss: 0.683678  [   64/  179]
train() client id: f_00007-3-2 loss: 0.677239  [   96/  179]
train() client id: f_00007-3-3 loss: 0.642091  [  128/  179]
train() client id: f_00007-3-4 loss: 0.636449  [  160/  179]
train() client id: f_00007-4-0 loss: 0.606264  [   32/  179]
train() client id: f_00007-4-1 loss: 0.725187  [   64/  179]
train() client id: f_00007-4-2 loss: 0.584585  [   96/  179]
train() client id: f_00007-4-3 loss: 0.722173  [  128/  179]
train() client id: f_00007-4-4 loss: 0.594719  [  160/  179]
train() client id: f_00007-5-0 loss: 0.606678  [   32/  179]
train() client id: f_00007-5-1 loss: 0.677977  [   64/  179]
train() client id: f_00007-5-2 loss: 0.743537  [   96/  179]
train() client id: f_00007-5-3 loss: 0.624206  [  128/  179]
train() client id: f_00007-5-4 loss: 0.538683  [  160/  179]
train() client id: f_00007-6-0 loss: 0.796670  [   32/  179]
train() client id: f_00007-6-1 loss: 0.618944  [   64/  179]
train() client id: f_00007-6-2 loss: 0.609409  [   96/  179]
train() client id: f_00007-6-3 loss: 0.628805  [  128/  179]
train() client id: f_00007-6-4 loss: 0.591049  [  160/  179]
train() client id: f_00007-7-0 loss: 0.663867  [   32/  179]
train() client id: f_00007-7-1 loss: 0.755679  [   64/  179]
train() client id: f_00007-7-2 loss: 0.656873  [   96/  179]
train() client id: f_00007-7-3 loss: 0.549187  [  128/  179]
train() client id: f_00007-7-4 loss: 0.616008  [  160/  179]
train() client id: f_00007-8-0 loss: 0.596798  [   32/  179]
train() client id: f_00007-8-1 loss: 0.603521  [   64/  179]
train() client id: f_00007-8-2 loss: 0.599757  [   96/  179]
train() client id: f_00007-8-3 loss: 0.596043  [  128/  179]
train() client id: f_00007-8-4 loss: 0.799440  [  160/  179]
train() client id: f_00007-9-0 loss: 0.612451  [   32/  179]
train() client id: f_00007-9-1 loss: 0.675915  [   64/  179]
train() client id: f_00007-9-2 loss: 0.535116  [   96/  179]
train() client id: f_00007-9-3 loss: 0.665042  [  128/  179]
train() client id: f_00007-9-4 loss: 0.604154  [  160/  179]
train() client id: f_00007-10-0 loss: 0.596322  [   32/  179]
train() client id: f_00007-10-1 loss: 0.682513  [   64/  179]
train() client id: f_00007-10-2 loss: 0.680512  [   96/  179]
train() client id: f_00007-10-3 loss: 0.652215  [  128/  179]
train() client id: f_00007-10-4 loss: 0.537013  [  160/  179]
train() client id: f_00007-11-0 loss: 0.530447  [   32/  179]
train() client id: f_00007-11-1 loss: 0.722012  [   64/  179]
train() client id: f_00007-11-2 loss: 0.754260  [   96/  179]
train() client id: f_00007-11-3 loss: 0.686933  [  128/  179]
train() client id: f_00007-11-4 loss: 0.535048  [  160/  179]
train() client id: f_00008-0-0 loss: 0.868725  [   32/  130]
train() client id: f_00008-0-1 loss: 0.790137  [   64/  130]
train() client id: f_00008-0-2 loss: 0.892070  [   96/  130]
train() client id: f_00008-0-3 loss: 0.753225  [  128/  130]
train() client id: f_00008-1-0 loss: 0.759587  [   32/  130]
train() client id: f_00008-1-1 loss: 0.900102  [   64/  130]
train() client id: f_00008-1-2 loss: 0.838976  [   96/  130]
train() client id: f_00008-1-3 loss: 0.805671  [  128/  130]
train() client id: f_00008-2-0 loss: 0.909105  [   32/  130]
train() client id: f_00008-2-1 loss: 0.712820  [   64/  130]
train() client id: f_00008-2-2 loss: 0.797657  [   96/  130]
train() client id: f_00008-2-3 loss: 0.872872  [  128/  130]
train() client id: f_00008-3-0 loss: 0.717247  [   32/  130]
train() client id: f_00008-3-1 loss: 0.921964  [   64/  130]
train() client id: f_00008-3-2 loss: 0.858315  [   96/  130]
train() client id: f_00008-3-3 loss: 0.799273  [  128/  130]
train() client id: f_00008-4-0 loss: 0.783205  [   32/  130]
train() client id: f_00008-4-1 loss: 0.890544  [   64/  130]
train() client id: f_00008-4-2 loss: 0.859588  [   96/  130]
train() client id: f_00008-4-3 loss: 0.754107  [  128/  130]
train() client id: f_00008-5-0 loss: 0.858239  [   32/  130]
train() client id: f_00008-5-1 loss: 0.818774  [   64/  130]
train() client id: f_00008-5-2 loss: 0.759658  [   96/  130]
train() client id: f_00008-5-3 loss: 0.847926  [  128/  130]
train() client id: f_00008-6-0 loss: 0.767734  [   32/  130]
train() client id: f_00008-6-1 loss: 0.788611  [   64/  130]
train() client id: f_00008-6-2 loss: 0.888967  [   96/  130]
train() client id: f_00008-6-3 loss: 0.821008  [  128/  130]
train() client id: f_00008-7-0 loss: 0.832592  [   32/  130]
train() client id: f_00008-7-1 loss: 0.881950  [   64/  130]
train() client id: f_00008-7-2 loss: 0.755011  [   96/  130]
train() client id: f_00008-7-3 loss: 0.813787  [  128/  130]
train() client id: f_00008-8-0 loss: 0.770909  [   32/  130]
train() client id: f_00008-8-1 loss: 0.794411  [   64/  130]
train() client id: f_00008-8-2 loss: 0.792529  [   96/  130]
train() client id: f_00008-8-3 loss: 0.897816  [  128/  130]
train() client id: f_00008-9-0 loss: 0.962792  [   32/  130]
train() client id: f_00008-9-1 loss: 0.755161  [   64/  130]
train() client id: f_00008-9-2 loss: 0.810912  [   96/  130]
train() client id: f_00008-9-3 loss: 0.721012  [  128/  130]
train() client id: f_00008-10-0 loss: 0.758494  [   32/  130]
train() client id: f_00008-10-1 loss: 0.923979  [   64/  130]
train() client id: f_00008-10-2 loss: 0.808726  [   96/  130]
train() client id: f_00008-10-3 loss: 0.769246  [  128/  130]
train() client id: f_00008-11-0 loss: 0.784515  [   32/  130]
train() client id: f_00008-11-1 loss: 0.880186  [   64/  130]
train() client id: f_00008-11-2 loss: 0.761171  [   96/  130]
train() client id: f_00008-11-3 loss: 0.852491  [  128/  130]
train() client id: f_00009-0-0 loss: 1.151949  [   32/  118]
train() client id: f_00009-0-1 loss: 1.223922  [   64/  118]
train() client id: f_00009-0-2 loss: 1.111339  [   96/  118]
train() client id: f_00009-1-0 loss: 1.158689  [   32/  118]
train() client id: f_00009-1-1 loss: 1.089622  [   64/  118]
train() client id: f_00009-1-2 loss: 1.123886  [   96/  118]
train() client id: f_00009-2-0 loss: 1.069462  [   32/  118]
train() client id: f_00009-2-1 loss: 0.975173  [   64/  118]
train() client id: f_00009-2-2 loss: 1.161882  [   96/  118]
train() client id: f_00009-3-0 loss: 1.104705  [   32/  118]
train() client id: f_00009-3-1 loss: 0.990606  [   64/  118]
train() client id: f_00009-3-2 loss: 1.111254  [   96/  118]
train() client id: f_00009-4-0 loss: 0.895106  [   32/  118]
train() client id: f_00009-4-1 loss: 1.142178  [   64/  118]
train() client id: f_00009-4-2 loss: 1.017055  [   96/  118]
train() client id: f_00009-5-0 loss: 1.069480  [   32/  118]
train() client id: f_00009-5-1 loss: 0.936439  [   64/  118]
train() client id: f_00009-5-2 loss: 1.078524  [   96/  118]
train() client id: f_00009-6-0 loss: 1.010844  [   32/  118]
train() client id: f_00009-6-1 loss: 1.011594  [   64/  118]
train() client id: f_00009-6-2 loss: 0.942458  [   96/  118]
train() client id: f_00009-7-0 loss: 0.999862  [   32/  118]
train() client id: f_00009-7-1 loss: 1.060192  [   64/  118]
train() client id: f_00009-7-2 loss: 0.898866  [   96/  118]
train() client id: f_00009-8-0 loss: 0.899235  [   32/  118]
train() client id: f_00009-8-1 loss: 0.993456  [   64/  118]
train() client id: f_00009-8-2 loss: 1.064414  [   96/  118]
train() client id: f_00009-9-0 loss: 1.043803  [   32/  118]
train() client id: f_00009-9-1 loss: 1.036137  [   64/  118]
train() client id: f_00009-9-2 loss: 0.848686  [   96/  118]
train() client id: f_00009-10-0 loss: 0.909823  [   32/  118]
train() client id: f_00009-10-1 loss: 0.920031  [   64/  118]
train() client id: f_00009-10-2 loss: 1.020827  [   96/  118]
train() client id: f_00009-11-0 loss: 0.836954  [   32/  118]
train() client id: f_00009-11-1 loss: 1.144012  [   64/  118]
train() client id: f_00009-11-2 loss: 0.942728  [   96/  118]
At round 7 accuracy: 0.6312997347480106
At round 7 training accuracy: 0.5774647887323944
At round 7 training loss: 0.8702778538759391
update_location
xs = [ -3.9056584    4.20031788  55.00902392  18.81129433   0.97929623
   3.95640986 -17.44319194  -1.32485185  39.66397685  -2.06087855]
ys = [ 47.5879595   30.55583871   1.32061395 -17.45517586   9.35018685
  -7.18584926  -2.62498432  -4.17765202  17.56900603   4.00148178]
dists_uav = [110.81456609 104.64846845 114.13911132 103.24024389 100.44095288
 100.33588395 101.54385992 100.09599397 109.0041331  100.10124413]
dists_bs = [213.32393628 230.16704729 288.21637743 273.13332372 241.6786571
 255.37635032 237.43374407 249.53494358 266.20507287 243.20450851]
uav_gains = [7.73572375e-11 8.92616175e-11 7.18461074e-11 9.23368231e-11
 9.89056819e-11 9.91648192e-11 9.62418239e-11 9.97600464e-11
 8.06096645e-11 9.97469659e-11]
bs_gains = [3.32646713e-11 2.68889148e-11 1.43245785e-11 1.66511777e-11
 2.34545361e-11 2.00996360e-11 2.46476348e-11 2.14450097e-11
 1.78932145e-11 2.30448319e-11]
Round 8
-------------------------------
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.93572436 20.7513301   9.78311252  3.49713647 23.93609638 11.54337877
  4.34848004 14.03581921 10.31003617  9.36568175]
obj_prev = 117.50679575560986
eta_min = 4.500137844098843e-10	eta_max = 0.9201793140385103
af = 24.843282687249765	bf = 1.924183044657018	zeta = 27.327610955974745	eta = 0.909090909090909
af = 24.843282687249765	bf = 1.924183044657018	zeta = 47.21200862959305	eta = 0.5262068572883742
af = 24.843282687249765	bf = 1.924183044657018	zeta = 37.72761010314322	eta = 0.6584907609925703
af = 24.843282687249765	bf = 1.924183044657018	zeta = 36.02779691629381	eta = 0.689558752231509
af = 24.843282687249765	bf = 1.924183044657018	zeta = 35.944047891502834	eta = 0.6911654124832922
af = 24.843282687249765	bf = 1.924183044657018	zeta = 35.94383021535565	eta = 0.6911695981870181
eta = 0.6911695981870181
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [0.03047953 0.06410378 0.02999573 0.01040174 0.07402172 0.03531755
 0.01306266 0.04330029 0.03144714 0.02854432]
ene_total = [3.05537056 5.9604125  3.0232152  1.38370094 6.79982988 3.63829942
 1.59895401 4.0866222  3.34115314 3.05627237]
ti_comp = [0.29321835 0.27683249 0.29228351 0.29535599 0.2741193  0.27083978
 0.29583662 0.2962475  0.2682047  0.2737568 ]
ti_coms = [0.06608063 0.08246649 0.06701547 0.06394299 0.08517969 0.0884592
 0.06346236 0.06305149 0.09109429 0.08554218]
t_total = [29.59996643 29.59996643 29.59996643 29.59996643 29.59996643 29.59996643
 29.59996643 29.59996643 29.59996643 29.59996643]
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.05836579e-05 2.14830890e-04 1.97446596e-05 8.06319238e-07
 3.37348113e-04 3.75341750e-05 1.59173447e-06 5.78153199e-05
 2.70203220e-05 1.93958759e-05]
ene_total = [0.54608481 0.69707832 0.55371718 0.52684511 0.72952358 0.73184161
 0.52295022 0.52419719 0.75268396 0.70631616]
optimize_network iter = 0 obj = 6.291238143797367
eta = 0.6911695981870181
freqs = [5.19741187e+07 1.15780803e+08 5.13127289e+07 1.76088264e+07
 1.35017343e+08 6.52000814e+07 2.20774830e+07 7.30812690e+07
 5.86252507e+07 5.21344532e+07]
eta_min = 0.6875884070281042	eta_max = 0.6911695981870177
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 0.05581615321591653	eta = 0.909090909090909
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 21.219211967217113	eta = 0.002391321484860486
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 2.2358034144344674	eta = 0.022695178449689362
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 2.1654164527348314	eta = 0.02343288627225956
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 2.1653901507160183	eta = 0.02343317090097433
eta = 0.02343317090097433
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.09174714e-04 2.18314889e-03 2.00648668e-04 8.19395643e-06
 3.42819024e-03 3.81428819e-04 1.61754827e-05 5.87529343e-04
 2.74585215e-04 1.97104266e-04]
ene_total = [0.17760768 0.27172476 0.17982108 0.16680246 0.3112301  0.24039767
 0.16575821 0.17957311 0.24447922 0.22799587]
ti_comp = [0.29738478 0.28099891 0.29644993 0.29952241 0.27828572 0.27500621
 0.30000305 0.30041392 0.27237112 0.27792323]
ti_coms = [0.06608063 0.08246649 0.06701547 0.06394299 0.08517969 0.0884592
 0.06346236 0.06305149 0.09109429 0.08554218]
t_total = [29.59996643 29.59996643 29.59996643 29.59996643 29.59996643 29.59996643
 29.59996643 29.59996643 29.59996643 29.59996643]
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.08671664e-05 2.17429093e-04 2.00148185e-05 8.17590855e-07
 3.41327869e-04 3.79632066e-05 1.61405844e-06 5.86284344e-05
 2.73210417e-05 1.96239144e-05]
ene_total = [0.5398481  0.68929927 0.54739189 0.52080677 0.72148512 0.72348741
 0.51695743 0.51825451 0.74408039 0.69823819]
optimize_network iter = 1 obj = 6.219849079279121
eta = 0.6875884070281042
freqs = [5.19666731e+07 1.15668296e+08 5.13030792e+07 1.76080893e+07
 1.34866351e+08 6.51153626e+07 2.20770578e+07 7.30812690e+07
 5.85403612e+07 5.20751181e+07]
eta_min = 0.6875884070281044	eta_max = 0.6875884070280862
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 0.055709725023689025	eta = 0.909090909090909
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 21.21911053031574	eta = 0.0023867732106222467
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 2.23530448131582	eta = 0.022656960154787298
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 2.1650443530624273	eta = 0.023392224965439173
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 2.1650181860209075	eta = 0.023392507690695646
eta = 0.023392507690695646
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.09211545e-04 2.17991631e-03 2.00666013e-04 8.19706147e-06
 3.42210961e-03 3.80614259e-04 1.61823437e-05 5.87801193e-04
 2.73917274e-04 1.96746859e-04]
ene_total = [0.17760125 0.27162923 0.17981404 0.16679559 0.31105873 0.24036644
 0.16575149 0.17957271 0.24445164 0.22797706]
ti_comp = [0.29738478 0.28099891 0.29644993 0.29952241 0.27828572 0.27500621
 0.30000305 0.30041392 0.27237112 0.27792323]
ti_coms = [0.06608063 0.08246649 0.06701547 0.06394299 0.08517969 0.0884592
 0.06346236 0.06305149 0.09109429 0.08554218]
t_total = [29.59996643 29.59996643 29.59996643 29.59996643 29.59996643 29.59996643
 29.59996643 29.59996643 29.59996643 29.59996643]
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.08671664e-05 2.17429093e-04 2.00148185e-05 8.17590855e-07
 3.41327869e-04 3.79632066e-05 1.61405844e-06 5.86284344e-05
 2.73210417e-05 1.96239144e-05]
ene_total = [0.5398481  0.68929927 0.54739189 0.52080677 0.72148512 0.72348741
 0.51695743 0.51825451 0.74408039 0.69823819]
optimize_network iter = 2 obj = 6.219849079279126
eta = 0.6875884070281044
freqs = [5.19666731e+07 1.15668296e+08 5.13030792e+07 1.76080893e+07
 1.34866351e+08 6.51153626e+07 2.20770578e+07 7.30812690e+07
 5.85403612e+07 5.20751181e+07]
Done!
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.04160455e-05 2.12728559e-04 1.95821241e-05 7.99915605e-07
 3.33948805e-04 3.71424915e-05 1.57916459e-06 5.73609639e-05
 2.67303963e-05 1.91996709e-05]
ene_total = [0.00662848 0.00845938 0.00672113 0.0063951  0.00885192 0.00888306
 0.00634781 0.00636251 0.00913616 0.00857342]
At round 8 energy consumption: 0.07635896655888749
At round 8 eta: 0.6875884070281044
At round 8 a_n: 25.09969023353052
At round 8 local rounds: 12.265156719696218
At round 8 global rounds: 81.4381945256065
gradient difference: 0.42647847533226013
train() client id: f_00000-0-0 loss: 1.894006  [   32/  126]
train() client id: f_00000-0-1 loss: 1.765337  [   64/  126]
train() client id: f_00000-0-2 loss: 1.400580  [   96/  126]
train() client id: f_00000-1-0 loss: 1.531596  [   32/  126]
train() client id: f_00000-1-1 loss: 1.506999  [   64/  126]
train() client id: f_00000-1-2 loss: 1.507398  [   96/  126]
train() client id: f_00000-2-0 loss: 1.517349  [   32/  126]
train() client id: f_00000-2-1 loss: 1.323216  [   64/  126]
train() client id: f_00000-2-2 loss: 1.326026  [   96/  126]
train() client id: f_00000-3-0 loss: 1.185433  [   32/  126]
train() client id: f_00000-3-1 loss: 1.322136  [   64/  126]
train() client id: f_00000-3-2 loss: 1.232317  [   96/  126]
train() client id: f_00000-4-0 loss: 1.231569  [   32/  126]
train() client id: f_00000-4-1 loss: 1.165103  [   64/  126]
train() client id: f_00000-4-2 loss: 1.155693  [   96/  126]
train() client id: f_00000-5-0 loss: 1.124028  [   32/  126]
train() client id: f_00000-5-1 loss: 1.092634  [   64/  126]
train() client id: f_00000-5-2 loss: 1.122124  [   96/  126]
train() client id: f_00000-6-0 loss: 1.075023  [   32/  126]
train() client id: f_00000-6-1 loss: 1.000870  [   64/  126]
train() client id: f_00000-6-2 loss: 1.015047  [   96/  126]
train() client id: f_00000-7-0 loss: 1.014519  [   32/  126]
train() client id: f_00000-7-1 loss: 1.002565  [   64/  126]
train() client id: f_00000-7-2 loss: 0.967199  [   96/  126]
train() client id: f_00000-8-0 loss: 0.981323  [   32/  126]
train() client id: f_00000-8-1 loss: 0.985259  [   64/  126]
train() client id: f_00000-8-2 loss: 0.939750  [   96/  126]
train() client id: f_00000-9-0 loss: 0.934396  [   32/  126]
train() client id: f_00000-9-1 loss: 0.959377  [   64/  126]
train() client id: f_00000-9-2 loss: 0.912500  [   96/  126]
train() client id: f_00000-10-0 loss: 0.951748  [   32/  126]
train() client id: f_00000-10-1 loss: 0.932857  [   64/  126]
train() client id: f_00000-10-2 loss: 0.908922  [   96/  126]
train() client id: f_00000-11-0 loss: 0.942127  [   32/  126]
train() client id: f_00000-11-1 loss: 0.905764  [   64/  126]
train() client id: f_00000-11-2 loss: 0.950594  [   96/  126]
train() client id: f_00001-0-0 loss: 0.618476  [   32/  265]
train() client id: f_00001-0-1 loss: 0.510539  [   64/  265]
train() client id: f_00001-0-2 loss: 0.526725  [   96/  265]
train() client id: f_00001-0-3 loss: 0.459756  [  128/  265]
train() client id: f_00001-0-4 loss: 0.491992  [  160/  265]
train() client id: f_00001-0-5 loss: 0.660564  [  192/  265]
train() client id: f_00001-0-6 loss: 0.465622  [  224/  265]
train() client id: f_00001-0-7 loss: 0.506139  [  256/  265]
train() client id: f_00001-1-0 loss: 0.463719  [   32/  265]
train() client id: f_00001-1-1 loss: 0.585903  [   64/  265]
train() client id: f_00001-1-2 loss: 0.441916  [   96/  265]
train() client id: f_00001-1-3 loss: 0.560859  [  128/  265]
train() client id: f_00001-1-4 loss: 0.460434  [  160/  265]
train() client id: f_00001-1-5 loss: 0.483896  [  192/  265]
train() client id: f_00001-1-6 loss: 0.578103  [  224/  265]
train() client id: f_00001-1-7 loss: 0.469836  [  256/  265]
train() client id: f_00001-2-0 loss: 0.422670  [   32/  265]
train() client id: f_00001-2-1 loss: 0.536744  [   64/  265]
train() client id: f_00001-2-2 loss: 0.589516  [   96/  265]
train() client id: f_00001-2-3 loss: 0.516949  [  128/  265]
train() client id: f_00001-2-4 loss: 0.431141  [  160/  265]
train() client id: f_00001-2-5 loss: 0.525318  [  192/  265]
train() client id: f_00001-2-6 loss: 0.468081  [  224/  265]
train() client id: f_00001-2-7 loss: 0.493072  [  256/  265]
train() client id: f_00001-3-0 loss: 0.463021  [   32/  265]
train() client id: f_00001-3-1 loss: 0.598858  [   64/  265]
train() client id: f_00001-3-2 loss: 0.417581  [   96/  265]
train() client id: f_00001-3-3 loss: 0.485146  [  128/  265]
train() client id: f_00001-3-4 loss: 0.460872  [  160/  265]
train() client id: f_00001-3-5 loss: 0.594276  [  192/  265]
train() client id: f_00001-3-6 loss: 0.403348  [  224/  265]
train() client id: f_00001-3-7 loss: 0.476702  [  256/  265]
train() client id: f_00001-4-0 loss: 0.411438  [   32/  265]
train() client id: f_00001-4-1 loss: 0.406155  [   64/  265]
train() client id: f_00001-4-2 loss: 0.501451  [   96/  265]
train() client id: f_00001-4-3 loss: 0.459243  [  128/  265]
train() client id: f_00001-4-4 loss: 0.490049  [  160/  265]
train() client id: f_00001-4-5 loss: 0.572358  [  192/  265]
train() client id: f_00001-4-6 loss: 0.508574  [  224/  265]
train() client id: f_00001-4-7 loss: 0.492365  [  256/  265]
train() client id: f_00001-5-0 loss: 0.396856  [   32/  265]
train() client id: f_00001-5-1 loss: 0.452592  [   64/  265]
train() client id: f_00001-5-2 loss: 0.462745  [   96/  265]
train() client id: f_00001-5-3 loss: 0.509362  [  128/  265]
train() client id: f_00001-5-4 loss: 0.452516  [  160/  265]
train() client id: f_00001-5-5 loss: 0.603086  [  192/  265]
train() client id: f_00001-5-6 loss: 0.498121  [  224/  265]
train() client id: f_00001-5-7 loss: 0.432608  [  256/  265]
train() client id: f_00001-6-0 loss: 0.433631  [   32/  265]
train() client id: f_00001-6-1 loss: 0.488099  [   64/  265]
train() client id: f_00001-6-2 loss: 0.493809  [   96/  265]
train() client id: f_00001-6-3 loss: 0.392700  [  128/  265]
train() client id: f_00001-6-4 loss: 0.390703  [  160/  265]
train() client id: f_00001-6-5 loss: 0.617352  [  192/  265]
train() client id: f_00001-6-6 loss: 0.513523  [  224/  265]
train() client id: f_00001-6-7 loss: 0.392488  [  256/  265]
train() client id: f_00001-7-0 loss: 0.502176  [   32/  265]
train() client id: f_00001-7-1 loss: 0.379533  [   64/  265]
train() client id: f_00001-7-2 loss: 0.378420  [   96/  265]
train() client id: f_00001-7-3 loss: 0.444098  [  128/  265]
train() client id: f_00001-7-4 loss: 0.442276  [  160/  265]
train() client id: f_00001-7-5 loss: 0.515447  [  192/  265]
train() client id: f_00001-7-6 loss: 0.486962  [  224/  265]
train() client id: f_00001-7-7 loss: 0.546232  [  256/  265]
train() client id: f_00001-8-0 loss: 0.600863  [   32/  265]
train() client id: f_00001-8-1 loss: 0.438590  [   64/  265]
train() client id: f_00001-8-2 loss: 0.541576  [   96/  265]
train() client id: f_00001-8-3 loss: 0.446554  [  128/  265]
train() client id: f_00001-8-4 loss: 0.392131  [  160/  265]
train() client id: f_00001-8-5 loss: 0.485655  [  192/  265]
train() client id: f_00001-8-6 loss: 0.380169  [  224/  265]
train() client id: f_00001-8-7 loss: 0.371614  [  256/  265]
train() client id: f_00001-9-0 loss: 0.372260  [   32/  265]
train() client id: f_00001-9-1 loss: 0.432591  [   64/  265]
train() client id: f_00001-9-2 loss: 0.563536  [   96/  265]
train() client id: f_00001-9-3 loss: 0.370872  [  128/  265]
train() client id: f_00001-9-4 loss: 0.475626  [  160/  265]
train() client id: f_00001-9-5 loss: 0.445117  [  192/  265]
train() client id: f_00001-9-6 loss: 0.558811  [  224/  265]
train() client id: f_00001-9-7 loss: 0.438814  [  256/  265]
train() client id: f_00001-10-0 loss: 0.443005  [   32/  265]
train() client id: f_00001-10-1 loss: 0.366975  [   64/  265]
train() client id: f_00001-10-2 loss: 0.540551  [   96/  265]
train() client id: f_00001-10-3 loss: 0.491286  [  128/  265]
train() client id: f_00001-10-4 loss: 0.456511  [  160/  265]
train() client id: f_00001-10-5 loss: 0.494088  [  192/  265]
train() client id: f_00001-10-6 loss: 0.500737  [  224/  265]
train() client id: f_00001-10-7 loss: 0.430098  [  256/  265]
train() client id: f_00001-11-0 loss: 0.416779  [   32/  265]
train() client id: f_00001-11-1 loss: 0.374343  [   64/  265]
train() client id: f_00001-11-2 loss: 0.450647  [   96/  265]
train() client id: f_00001-11-3 loss: 0.434392  [  128/  265]
train() client id: f_00001-11-4 loss: 0.465879  [  160/  265]
train() client id: f_00001-11-5 loss: 0.423837  [  192/  265]
train() client id: f_00001-11-6 loss: 0.484636  [  224/  265]
train() client id: f_00001-11-7 loss: 0.562597  [  256/  265]
train() client id: f_00002-0-0 loss: 1.266602  [   32/  124]
train() client id: f_00002-0-1 loss: 1.255417  [   64/  124]
train() client id: f_00002-0-2 loss: 1.275034  [   96/  124]
train() client id: f_00002-1-0 loss: 1.239543  [   32/  124]
train() client id: f_00002-1-1 loss: 1.178849  [   64/  124]
train() client id: f_00002-1-2 loss: 1.240639  [   96/  124]
train() client id: f_00002-2-0 loss: 1.189774  [   32/  124]
train() client id: f_00002-2-1 loss: 1.199788  [   64/  124]
train() client id: f_00002-2-2 loss: 1.202438  [   96/  124]
train() client id: f_00002-3-0 loss: 1.143810  [   32/  124]
train() client id: f_00002-3-1 loss: 1.157651  [   64/  124]
train() client id: f_00002-3-2 loss: 1.174470  [   96/  124]
train() client id: f_00002-4-0 loss: 1.081822  [   32/  124]
train() client id: f_00002-4-1 loss: 1.221671  [   64/  124]
train() client id: f_00002-4-2 loss: 1.177814  [   96/  124]
train() client id: f_00002-5-0 loss: 1.144751  [   32/  124]
train() client id: f_00002-5-1 loss: 1.148629  [   64/  124]
train() client id: f_00002-5-2 loss: 1.106305  [   96/  124]
train() client id: f_00002-6-0 loss: 1.143746  [   32/  124]
train() client id: f_00002-6-1 loss: 1.069834  [   64/  124]
train() client id: f_00002-6-2 loss: 1.131537  [   96/  124]
train() client id: f_00002-7-0 loss: 1.084360  [   32/  124]
train() client id: f_00002-7-1 loss: 1.251384  [   64/  124]
train() client id: f_00002-7-2 loss: 0.967952  [   96/  124]
train() client id: f_00002-8-0 loss: 1.026523  [   32/  124]
train() client id: f_00002-8-1 loss: 1.229711  [   64/  124]
train() client id: f_00002-8-2 loss: 1.087475  [   96/  124]
train() client id: f_00002-9-0 loss: 1.074828  [   32/  124]
train() client id: f_00002-9-1 loss: 1.040721  [   64/  124]
train() client id: f_00002-9-2 loss: 1.143034  [   96/  124]
train() client id: f_00002-10-0 loss: 1.062030  [   32/  124]
train() client id: f_00002-10-1 loss: 1.121808  [   64/  124]
train() client id: f_00002-10-2 loss: 1.079654  [   96/  124]
train() client id: f_00002-11-0 loss: 1.124086  [   32/  124]
train() client id: f_00002-11-1 loss: 1.110154  [   64/  124]
train() client id: f_00002-11-2 loss: 1.083053  [   96/  124]
train() client id: f_00003-0-0 loss: 0.804178  [   32/   43]
train() client id: f_00003-1-0 loss: 0.807680  [   32/   43]
train() client id: f_00003-2-0 loss: 1.001639  [   32/   43]
train() client id: f_00003-3-0 loss: 0.730248  [   32/   43]
train() client id: f_00003-4-0 loss: 0.782921  [   32/   43]
train() client id: f_00003-5-0 loss: 0.764421  [   32/   43]
train() client id: f_00003-6-0 loss: 0.954445  [   32/   43]
train() client id: f_00003-7-0 loss: 0.853845  [   32/   43]
train() client id: f_00003-8-0 loss: 0.731993  [   32/   43]
train() client id: f_00003-9-0 loss: 0.879514  [   32/   43]
train() client id: f_00003-10-0 loss: 0.811282  [   32/   43]
train() client id: f_00003-11-0 loss: 0.806022  [   32/   43]
train() client id: f_00004-0-0 loss: 0.843677  [   32/  306]
train() client id: f_00004-0-1 loss: 0.998687  [   64/  306]
train() client id: f_00004-0-2 loss: 1.095815  [   96/  306]
train() client id: f_00004-0-3 loss: 0.707705  [  128/  306]
train() client id: f_00004-0-4 loss: 0.955102  [  160/  306]
train() client id: f_00004-0-5 loss: 0.806156  [  192/  306]
train() client id: f_00004-0-6 loss: 0.977453  [  224/  306]
train() client id: f_00004-0-7 loss: 1.061705  [  256/  306]
train() client id: f_00004-0-8 loss: 0.749710  [  288/  306]
train() client id: f_00004-1-0 loss: 0.913355  [   32/  306]
train() client id: f_00004-1-1 loss: 0.873276  [   64/  306]
train() client id: f_00004-1-2 loss: 0.903112  [   96/  306]
train() client id: f_00004-1-3 loss: 0.932421  [  128/  306]
train() client id: f_00004-1-4 loss: 0.902282  [  160/  306]
train() client id: f_00004-1-5 loss: 0.902331  [  192/  306]
train() client id: f_00004-1-6 loss: 0.835345  [  224/  306]
train() client id: f_00004-1-7 loss: 1.013286  [  256/  306]
train() client id: f_00004-1-8 loss: 0.938299  [  288/  306]
train() client id: f_00004-2-0 loss: 1.017648  [   32/  306]
train() client id: f_00004-2-1 loss: 0.977042  [   64/  306]
train() client id: f_00004-2-2 loss: 0.868122  [   96/  306]
train() client id: f_00004-2-3 loss: 0.855090  [  128/  306]
train() client id: f_00004-2-4 loss: 0.819068  [  160/  306]
train() client id: f_00004-2-5 loss: 0.846601  [  192/  306]
train() client id: f_00004-2-6 loss: 0.922368  [  224/  306]
train() client id: f_00004-2-7 loss: 0.969781  [  256/  306]
train() client id: f_00004-2-8 loss: 0.855649  [  288/  306]
train() client id: f_00004-3-0 loss: 0.898907  [   32/  306]
train() client id: f_00004-3-1 loss: 0.728984  [   64/  306]
train() client id: f_00004-3-2 loss: 0.847567  [   96/  306]
train() client id: f_00004-3-3 loss: 1.056400  [  128/  306]
train() client id: f_00004-3-4 loss: 1.112868  [  160/  306]
train() client id: f_00004-3-5 loss: 0.745306  [  192/  306]
train() client id: f_00004-3-6 loss: 0.921186  [  224/  306]
train() client id: f_00004-3-7 loss: 0.907261  [  256/  306]
train() client id: f_00004-3-8 loss: 0.851513  [  288/  306]
train() client id: f_00004-4-0 loss: 0.802658  [   32/  306]
train() client id: f_00004-4-1 loss: 0.980384  [   64/  306]
train() client id: f_00004-4-2 loss: 0.936762  [   96/  306]
train() client id: f_00004-4-3 loss: 0.908207  [  128/  306]
train() client id: f_00004-4-4 loss: 0.691066  [  160/  306]
train() client id: f_00004-4-5 loss: 0.826178  [  192/  306]
train() client id: f_00004-4-6 loss: 1.041168  [  224/  306]
train() client id: f_00004-4-7 loss: 0.976165  [  256/  306]
train() client id: f_00004-4-8 loss: 0.982561  [  288/  306]
train() client id: f_00004-5-0 loss: 1.074960  [   32/  306]
train() client id: f_00004-5-1 loss: 0.855931  [   64/  306]
train() client id: f_00004-5-2 loss: 0.933209  [   96/  306]
train() client id: f_00004-5-3 loss: 0.891569  [  128/  306]
train() client id: f_00004-5-4 loss: 0.880281  [  160/  306]
train() client id: f_00004-5-5 loss: 0.957763  [  192/  306]
train() client id: f_00004-5-6 loss: 0.933307  [  224/  306]
train() client id: f_00004-5-7 loss: 0.847047  [  256/  306]
train() client id: f_00004-5-8 loss: 0.853528  [  288/  306]
train() client id: f_00004-6-0 loss: 0.917987  [   32/  306]
train() client id: f_00004-6-1 loss: 0.861743  [   64/  306]
train() client id: f_00004-6-2 loss: 0.906341  [   96/  306]
train() client id: f_00004-6-3 loss: 0.900649  [  128/  306]
train() client id: f_00004-6-4 loss: 1.001318  [  160/  306]
train() client id: f_00004-6-5 loss: 0.937127  [  192/  306]
train() client id: f_00004-6-6 loss: 0.843589  [  224/  306]
train() client id: f_00004-6-7 loss: 0.918380  [  256/  306]
train() client id: f_00004-6-8 loss: 0.827115  [  288/  306]
train() client id: f_00004-7-0 loss: 1.024264  [   32/  306]
train() client id: f_00004-7-1 loss: 0.768369  [   64/  306]
train() client id: f_00004-7-2 loss: 0.806947  [   96/  306]
train() client id: f_00004-7-3 loss: 0.776307  [  128/  306]
train() client id: f_00004-7-4 loss: 0.929687  [  160/  306]
train() client id: f_00004-7-5 loss: 0.977474  [  192/  306]
train() client id: f_00004-7-6 loss: 1.066386  [  224/  306]
train() client id: f_00004-7-7 loss: 0.957574  [  256/  306]
train() client id: f_00004-7-8 loss: 0.888254  [  288/  306]
train() client id: f_00004-8-0 loss: 0.925573  [   32/  306]
train() client id: f_00004-8-1 loss: 0.995513  [   64/  306]
train() client id: f_00004-8-2 loss: 0.894626  [   96/  306]
train() client id: f_00004-8-3 loss: 0.965436  [  128/  306]
train() client id: f_00004-8-4 loss: 0.862147  [  160/  306]
train() client id: f_00004-8-5 loss: 0.855969  [  192/  306]
train() client id: f_00004-8-6 loss: 0.870328  [  224/  306]
train() client id: f_00004-8-7 loss: 1.019240  [  256/  306]
train() client id: f_00004-8-8 loss: 0.795703  [  288/  306]
train() client id: f_00004-9-0 loss: 0.892293  [   32/  306]
train() client id: f_00004-9-1 loss: 0.844321  [   64/  306]
train() client id: f_00004-9-2 loss: 0.904987  [   96/  306]
train() client id: f_00004-9-3 loss: 0.939927  [  128/  306]
train() client id: f_00004-9-4 loss: 0.928325  [  160/  306]
train() client id: f_00004-9-5 loss: 0.865138  [  192/  306]
train() client id: f_00004-9-6 loss: 0.808440  [  224/  306]
train() client id: f_00004-9-7 loss: 1.014935  [  256/  306]
train() client id: f_00004-9-8 loss: 0.972652  [  288/  306]
train() client id: f_00004-10-0 loss: 0.861496  [   32/  306]
train() client id: f_00004-10-1 loss: 0.979353  [   64/  306]
train() client id: f_00004-10-2 loss: 1.077623  [   96/  306]
train() client id: f_00004-10-3 loss: 0.831390  [  128/  306]
train() client id: f_00004-10-4 loss: 0.873580  [  160/  306]
train() client id: f_00004-10-5 loss: 0.903745  [  192/  306]
train() client id: f_00004-10-6 loss: 0.863493  [  224/  306]
train() client id: f_00004-10-7 loss: 0.881981  [  256/  306]
train() client id: f_00004-10-8 loss: 0.909836  [  288/  306]
train() client id: f_00004-11-0 loss: 0.999720  [   32/  306]
train() client id: f_00004-11-1 loss: 0.790882  [   64/  306]
train() client id: f_00004-11-2 loss: 0.854766  [   96/  306]
train() client id: f_00004-11-3 loss: 0.921226  [  128/  306]
train() client id: f_00004-11-4 loss: 0.816734  [  160/  306]
train() client id: f_00004-11-5 loss: 1.029949  [  192/  306]
train() client id: f_00004-11-6 loss: 0.943272  [  224/  306]
train() client id: f_00004-11-7 loss: 0.976821  [  256/  306]
train() client id: f_00004-11-8 loss: 0.912658  [  288/  306]
train() client id: f_00005-0-0 loss: 0.854628  [   32/  146]
train() client id: f_00005-0-1 loss: 0.735694  [   64/  146]
train() client id: f_00005-0-2 loss: 0.754999  [   96/  146]
train() client id: f_00005-0-3 loss: 0.788215  [  128/  146]
train() client id: f_00005-1-0 loss: 0.820564  [   32/  146]
train() client id: f_00005-1-1 loss: 0.675636  [   64/  146]
train() client id: f_00005-1-2 loss: 0.839890  [   96/  146]
train() client id: f_00005-1-3 loss: 0.811658  [  128/  146]
train() client id: f_00005-2-0 loss: 0.631299  [   32/  146]
train() client id: f_00005-2-1 loss: 0.854737  [   64/  146]
train() client id: f_00005-2-2 loss: 0.890424  [   96/  146]
train() client id: f_00005-2-3 loss: 0.861686  [  128/  146]
train() client id: f_00005-3-0 loss: 0.763363  [   32/  146]
train() client id: f_00005-3-1 loss: 0.703240  [   64/  146]
train() client id: f_00005-3-2 loss: 0.702044  [   96/  146]
train() client id: f_00005-3-3 loss: 0.928331  [  128/  146]
train() client id: f_00005-4-0 loss: 0.875761  [   32/  146]
train() client id: f_00005-4-1 loss: 0.694555  [   64/  146]
train() client id: f_00005-4-2 loss: 0.827050  [   96/  146]
train() client id: f_00005-4-3 loss: 0.738589  [  128/  146]
train() client id: f_00005-5-0 loss: 0.885534  [   32/  146]
train() client id: f_00005-5-1 loss: 0.661021  [   64/  146]
train() client id: f_00005-5-2 loss: 0.748717  [   96/  146]
train() client id: f_00005-5-3 loss: 0.884123  [  128/  146]
train() client id: f_00005-6-0 loss: 0.746356  [   32/  146]
train() client id: f_00005-6-1 loss: 0.776212  [   64/  146]
train() client id: f_00005-6-2 loss: 0.795914  [   96/  146]
train() client id: f_00005-6-3 loss: 0.739244  [  128/  146]
train() client id: f_00005-7-0 loss: 0.742776  [   32/  146]
train() client id: f_00005-7-1 loss: 0.716998  [   64/  146]
train() client id: f_00005-7-2 loss: 0.676371  [   96/  146]
train() client id: f_00005-7-3 loss: 0.908280  [  128/  146]
train() client id: f_00005-8-0 loss: 0.918309  [   32/  146]
train() client id: f_00005-8-1 loss: 1.004329  [   64/  146]
train() client id: f_00005-8-2 loss: 0.622790  [   96/  146]
train() client id: f_00005-8-3 loss: 0.690348  [  128/  146]
train() client id: f_00005-9-0 loss: 0.698047  [   32/  146]
train() client id: f_00005-9-1 loss: 0.701329  [   64/  146]
train() client id: f_00005-9-2 loss: 0.712746  [   96/  146]
train() client id: f_00005-9-3 loss: 0.968632  [  128/  146]
train() client id: f_00005-10-0 loss: 0.736198  [   32/  146]
train() client id: f_00005-10-1 loss: 0.894178  [   64/  146]
train() client id: f_00005-10-2 loss: 0.800643  [   96/  146]
train() client id: f_00005-10-3 loss: 0.676331  [  128/  146]
train() client id: f_00005-11-0 loss: 0.966564  [   32/  146]
train() client id: f_00005-11-1 loss: 0.873401  [   64/  146]
train() client id: f_00005-11-2 loss: 0.735527  [   96/  146]
train() client id: f_00005-11-3 loss: 0.565534  [  128/  146]
train() client id: f_00006-0-0 loss: 0.789827  [   32/   54]
train() client id: f_00006-1-0 loss: 0.793354  [   32/   54]
train() client id: f_00006-2-0 loss: 0.751452  [   32/   54]
train() client id: f_00006-3-0 loss: 0.837824  [   32/   54]
train() client id: f_00006-4-0 loss: 0.749333  [   32/   54]
train() client id: f_00006-5-0 loss: 0.846470  [   32/   54]
train() client id: f_00006-6-0 loss: 0.785976  [   32/   54]
train() client id: f_00006-7-0 loss: 0.851968  [   32/   54]
train() client id: f_00006-8-0 loss: 0.805737  [   32/   54]
train() client id: f_00006-9-0 loss: 0.840168  [   32/   54]
train() client id: f_00006-10-0 loss: 0.791717  [   32/   54]
train() client id: f_00006-11-0 loss: 0.808298  [   32/   54]
train() client id: f_00007-0-0 loss: 0.789983  [   32/  179]
train() client id: f_00007-0-1 loss: 0.782640  [   64/  179]
train() client id: f_00007-0-2 loss: 0.738542  [   96/  179]
train() client id: f_00007-0-3 loss: 0.741353  [  128/  179]
train() client id: f_00007-0-4 loss: 0.765536  [  160/  179]
train() client id: f_00007-1-0 loss: 0.764406  [   32/  179]
train() client id: f_00007-1-1 loss: 0.673750  [   64/  179]
train() client id: f_00007-1-2 loss: 0.728628  [   96/  179]
train() client id: f_00007-1-3 loss: 0.761508  [  128/  179]
train() client id: f_00007-1-4 loss: 0.778607  [  160/  179]
train() client id: f_00007-2-0 loss: 0.740439  [   32/  179]
train() client id: f_00007-2-1 loss: 0.705071  [   64/  179]
train() client id: f_00007-2-2 loss: 0.759050  [   96/  179]
train() client id: f_00007-2-3 loss: 0.838209  [  128/  179]
train() client id: f_00007-2-4 loss: 0.701896  [  160/  179]
train() client id: f_00007-3-0 loss: 0.720260  [   32/  179]
train() client id: f_00007-3-1 loss: 0.714338  [   64/  179]
train() client id: f_00007-3-2 loss: 0.753547  [   96/  179]
train() client id: f_00007-3-3 loss: 0.626329  [  128/  179]
train() client id: f_00007-3-4 loss: 0.770351  [  160/  179]
train() client id: f_00007-4-0 loss: 0.906037  [   32/  179]
train() client id: f_00007-4-1 loss: 0.678432  [   64/  179]
train() client id: f_00007-4-2 loss: 0.713626  [   96/  179]
train() client id: f_00007-4-3 loss: 0.674481  [  128/  179]
train() client id: f_00007-4-4 loss: 0.685446  [  160/  179]
train() client id: f_00007-5-0 loss: 0.830573  [   32/  179]
train() client id: f_00007-5-1 loss: 0.786514  [   64/  179]
train() client id: f_00007-5-2 loss: 0.686099  [   96/  179]
train() client id: f_00007-5-3 loss: 0.752690  [  128/  179]
train() client id: f_00007-5-4 loss: 0.628011  [  160/  179]
train() client id: f_00007-6-0 loss: 0.665039  [   32/  179]
train() client id: f_00007-6-1 loss: 0.683731  [   64/  179]
train() client id: f_00007-6-2 loss: 0.895111  [   96/  179]
train() client id: f_00007-6-3 loss: 0.715870  [  128/  179]
train() client id: f_00007-6-4 loss: 0.608734  [  160/  179]
train() client id: f_00007-7-0 loss: 0.700779  [   32/  179]
train() client id: f_00007-7-1 loss: 0.807740  [   64/  179]
train() client id: f_00007-7-2 loss: 0.767423  [   96/  179]
train() client id: f_00007-7-3 loss: 0.623759  [  128/  179]
train() client id: f_00007-7-4 loss: 0.759098  [  160/  179]
train() client id: f_00007-8-0 loss: 0.608350  [   32/  179]
train() client id: f_00007-8-1 loss: 0.696832  [   64/  179]
train() client id: f_00007-8-2 loss: 0.748521  [   96/  179]
train() client id: f_00007-8-3 loss: 0.768712  [  128/  179]
train() client id: f_00007-8-4 loss: 0.681327  [  160/  179]
train() client id: f_00007-9-0 loss: 0.825373  [   32/  179]
train() client id: f_00007-9-1 loss: 0.615551  [   64/  179]
train() client id: f_00007-9-2 loss: 0.761153  [   96/  179]
train() client id: f_00007-9-3 loss: 0.694239  [  128/  179]
train() client id: f_00007-9-4 loss: 0.755912  [  160/  179]
train() client id: f_00007-10-0 loss: 0.743176  [   32/  179]
train() client id: f_00007-10-1 loss: 0.764181  [   64/  179]
train() client id: f_00007-10-2 loss: 0.767264  [   96/  179]
train() client id: f_00007-10-3 loss: 0.675517  [  128/  179]
train() client id: f_00007-10-4 loss: 0.632574  [  160/  179]
train() client id: f_00007-11-0 loss: 0.744988  [   32/  179]
train() client id: f_00007-11-1 loss: 0.753092  [   64/  179]
train() client id: f_00007-11-2 loss: 0.848960  [   96/  179]
train() client id: f_00007-11-3 loss: 0.624461  [  128/  179]
train() client id: f_00007-11-4 loss: 0.688656  [  160/  179]
train() client id: f_00008-0-0 loss: 0.741036  [   32/  130]
train() client id: f_00008-0-1 loss: 0.738359  [   64/  130]
train() client id: f_00008-0-2 loss: 0.906104  [   96/  130]
train() client id: f_00008-0-3 loss: 0.807912  [  128/  130]
train() client id: f_00008-1-0 loss: 0.858345  [   32/  130]
train() client id: f_00008-1-1 loss: 0.825566  [   64/  130]
train() client id: f_00008-1-2 loss: 0.784667  [   96/  130]
train() client id: f_00008-1-3 loss: 0.682501  [  128/  130]
train() client id: f_00008-2-0 loss: 0.749603  [   32/  130]
train() client id: f_00008-2-1 loss: 0.793751  [   64/  130]
train() client id: f_00008-2-2 loss: 0.776126  [   96/  130]
train() client id: f_00008-2-3 loss: 0.856920  [  128/  130]
train() client id: f_00008-3-0 loss: 0.818383  [   32/  130]
train() client id: f_00008-3-1 loss: 0.731312  [   64/  130]
train() client id: f_00008-3-2 loss: 0.741767  [   96/  130]
train() client id: f_00008-3-3 loss: 0.887447  [  128/  130]
train() client id: f_00008-4-0 loss: 0.775856  [   32/  130]
train() client id: f_00008-4-1 loss: 0.828771  [   64/  130]
train() client id: f_00008-4-2 loss: 0.742519  [   96/  130]
train() client id: f_00008-4-3 loss: 0.820180  [  128/  130]
train() client id: f_00008-5-0 loss: 0.748339  [   32/  130]
train() client id: f_00008-5-1 loss: 0.820579  [   64/  130]
train() client id: f_00008-5-2 loss: 0.847272  [   96/  130]
train() client id: f_00008-5-3 loss: 0.754960  [  128/  130]
train() client id: f_00008-6-0 loss: 0.857515  [   32/  130]
train() client id: f_00008-6-1 loss: 0.760668  [   64/  130]
train() client id: f_00008-6-2 loss: 0.689171  [   96/  130]
train() client id: f_00008-6-3 loss: 0.859588  [  128/  130]
train() client id: f_00008-7-0 loss: 0.855214  [   32/  130]
train() client id: f_00008-7-1 loss: 0.813882  [   64/  130]
train() client id: f_00008-7-2 loss: 0.762706  [   96/  130]
train() client id: f_00008-7-3 loss: 0.730292  [  128/  130]
train() client id: f_00008-8-0 loss: 0.807185  [   32/  130]
train() client id: f_00008-8-1 loss: 0.716211  [   64/  130]
train() client id: f_00008-8-2 loss: 0.853078  [   96/  130]
train() client id: f_00008-8-3 loss: 0.785806  [  128/  130]
train() client id: f_00008-9-0 loss: 0.804037  [   32/  130]
train() client id: f_00008-9-1 loss: 0.748019  [   64/  130]
train() client id: f_00008-9-2 loss: 0.786842  [   96/  130]
train() client id: f_00008-9-3 loss: 0.792630  [  128/  130]
train() client id: f_00008-10-0 loss: 0.746098  [   32/  130]
train() client id: f_00008-10-1 loss: 0.878271  [   64/  130]
train() client id: f_00008-10-2 loss: 0.826567  [   96/  130]
train() client id: f_00008-10-3 loss: 0.711820  [  128/  130]
train() client id: f_00008-11-0 loss: 0.684208  [   32/  130]
train() client id: f_00008-11-1 loss: 0.768466  [   64/  130]
train() client id: f_00008-11-2 loss: 0.851586  [   96/  130]
train() client id: f_00008-11-3 loss: 0.836794  [  128/  130]
train() client id: f_00009-0-0 loss: 1.137783  [   32/  118]
train() client id: f_00009-0-1 loss: 1.091545  [   64/  118]
train() client id: f_00009-0-2 loss: 1.218652  [   96/  118]
train() client id: f_00009-1-0 loss: 1.099305  [   32/  118]
train() client id: f_00009-1-1 loss: 1.158967  [   64/  118]
train() client id: f_00009-1-2 loss: 1.074998  [   96/  118]
train() client id: f_00009-2-0 loss: 1.074672  [   32/  118]
train() client id: f_00009-2-1 loss: 1.017295  [   64/  118]
train() client id: f_00009-2-2 loss: 1.085027  [   96/  118]
train() client id: f_00009-3-0 loss: 0.935787  [   32/  118]
train() client id: f_00009-3-1 loss: 0.973786  [   64/  118]
train() client id: f_00009-3-2 loss: 1.086660  [   96/  118]
train() client id: f_00009-4-0 loss: 1.017216  [   32/  118]
train() client id: f_00009-4-1 loss: 0.952726  [   64/  118]
train() client id: f_00009-4-2 loss: 0.999854  [   96/  118]
train() client id: f_00009-5-0 loss: 0.914153  [   32/  118]
train() client id: f_00009-5-1 loss: 0.987726  [   64/  118]
train() client id: f_00009-5-2 loss: 1.033150  [   96/  118]
train() client id: f_00009-6-0 loss: 0.957124  [   32/  118]
train() client id: f_00009-6-1 loss: 1.017473  [   64/  118]
train() client id: f_00009-6-2 loss: 0.922377  [   96/  118]
train() client id: f_00009-7-0 loss: 0.972398  [   32/  118]
train() client id: f_00009-7-1 loss: 1.000240  [   64/  118]
train() client id: f_00009-7-2 loss: 0.944161  [   96/  118]
train() client id: f_00009-8-0 loss: 1.014040  [   32/  118]
train() client id: f_00009-8-1 loss: 0.993216  [   64/  118]
train() client id: f_00009-8-2 loss: 0.973948  [   96/  118]
train() client id: f_00009-9-0 loss: 0.929059  [   32/  118]
train() client id: f_00009-9-1 loss: 0.908410  [   64/  118]
train() client id: f_00009-9-2 loss: 1.065473  [   96/  118]
train() client id: f_00009-10-0 loss: 1.012658  [   32/  118]
train() client id: f_00009-10-1 loss: 0.971316  [   64/  118]
train() client id: f_00009-10-2 loss: 0.855638  [   96/  118]
train() client id: f_00009-11-0 loss: 0.910706  [   32/  118]
train() client id: f_00009-11-1 loss: 0.946738  [   64/  118]
train() client id: f_00009-11-2 loss: 1.003756  [   96/  118]
At round 8 accuracy: 0.6312997347480106
At round 8 training accuracy: 0.5774647887323944
At round 8 training loss: 0.8669089423698678
update_location
xs = [ -3.9056584    4.20031788  60.00902392  18.81129433   0.97929623
   3.95640986 -22.44319194  -1.32485185  44.66397685   2.93912145]
ys = [ 52.5879595   35.55583871   1.32061395 -22.45517586  14.35018685
  -2.18584926  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [113.05196881 106.21610206 116.63115781 104.20220591 101.02913878
 100.10210345 102.5211559  100.0121567  110.92132708 100.1231756 ]
dists_bs = [210.37580989 227.0626082  292.22212521 276.67917212 238.27940567
 251.83371857 234.14571262 245.9693305  270.25317868 246.78497559]
uav_gains = [7.35860472e-11 8.60043309e-11 6.80689758e-11 9.02204198e-11
 9.74723702e-11 9.97448252e-11 9.39645390e-11 9.99692471e-11
 7.71712154e-11 9.96923511e-11]
bs_gains = [3.45864385e-11 2.79309909e-11 1.37815295e-11 1.60605338e-11
 2.44034856e-11 2.09013928e-11 2.56290612e-11 2.23268472e-11
 1.71528313e-11 2.21208434e-11]
Round 9
-------------------------------
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.80354861 20.47010187  9.65317002  3.45074232 23.61178753 11.38579797
  4.29063581 13.84678522 10.17374854  9.24174878]
obj_prev = 115.9280666591941
eta_min = 3.3564500217132474e-10	eta_max = 0.9202772988105504
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 26.95968104561057	eta = 0.909090909090909
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 46.612772647926946	eta = 0.5257958185768866
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 37.23476446800202	eta = 0.6582236063723965
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 35.55373220830825	eta = 0.6893453774967627
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 35.470824829161245	eta = 0.690956612049402
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 35.47060887359621	eta = 0.690960818797758
eta = 0.690960818797758
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [0.03050446 0.06415621 0.03002026 0.01041025 0.07408226 0.03534644
 0.01307334 0.0433357  0.03147286 0.02856767]
ene_total = [3.01933378 5.87455757 2.98817258 1.36705087 6.70212565 3.5823017
 1.57947799 4.03156066 3.30411446 3.0219136 ]
ti_comp = [0.29712306 0.28209195 0.2961178  0.29961782 0.27945843 0.27622759
 0.30009367 0.30080535 0.27174348 0.27743751]
ti_coms = [0.06670996 0.08174107 0.06771522 0.0642152  0.08437459 0.08760544
 0.06373935 0.06302768 0.09208954 0.08639551]
t_total = [29.54996223 29.54996223 29.54996223 29.54996223 29.54996223 29.54996223
 29.54996223 29.54996223 29.54996223 29.54996223]
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [2.00954316e-05 2.07402819e-04 1.92838817e-05 7.85468046e-07
 3.25378074e-04 3.61728644e-05 1.55069375e-06 5.62142497e-05
 2.63857967e-05 1.89310232e-05]
ene_total = [0.54344022 0.68073341 0.5515389  0.52160987 0.71170427 0.71445587
 0.51780721 0.51646678 0.75008018 0.70322868]
optimize_network iter = 0 obj = 6.211065406906146
eta = 0.690960818797758
freqs = [5.13330407e+07 1.13715060e+08 5.06897303e+07 1.73725523e+07
 1.32546121e+08 6.39806453e+07 2.17820992e+07 7.20327985e+07
 5.79091277e+07 5.14848716e+07]
eta_min = 0.6909608187976134	eta_max = 0.6909608187977386
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 0.05319201668668125	eta = 0.9090909090909091
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 20.968149575428694	eta = 0.002306182461743776
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 2.200288580777995	eta = 0.021977289355824242
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 2.1330781928113622	eta = 0.022669763803801746
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 2.1330546285065273	eta = 0.022670014241468774
eta = 0.022670014241468774
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [2.05846218e-04 2.12451699e-03 1.97533160e-04 8.04588971e-06
 3.33298867e-03 3.70534332e-04 1.58844283e-05 5.75826930e-04
 2.70281154e-04 1.93918678e-04]
ene_total = [0.17661037 0.26448822 0.17897858 0.16512347 0.30228746 0.23450342
 0.1641027  0.17665537 0.24344476 0.22686028]
ti_comp = [0.29712306 0.28209195 0.2961178  0.29961782 0.27945843 0.27622759
 0.30009367 0.30080535 0.27174348 0.27743751]
ti_coms = [0.06670996 0.08174107 0.06771522 0.0642152  0.08437459 0.08760544
 0.06373935 0.06302768 0.09208954 0.08639551]
t_total = [29.54996223 29.54996223 29.54996223 29.54996223 29.54996223 29.54996223
 29.54996223 29.54996223 29.54996223 29.54996223]
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [2.00954316e-05 2.07402819e-04 1.92838817e-05 7.85468046e-07
 3.25378074e-04 3.61728644e-05 1.55069375e-06 5.62142497e-05
 2.63857967e-05 1.89310232e-05]
ene_total = [0.54344022 0.68073341 0.5515389  0.52160987 0.71170427 0.71445587
 0.51780721 0.51646678 0.75008018 0.70322868]
optimize_network iter = 1 obj = 6.211065406903268
eta = 0.6909608187976134
freqs = [5.13330407e+07 1.13715060e+08 5.06897303e+07 1.73725523e+07
 1.32546121e+08 6.39806453e+07 2.17820992e+07 7.20327985e+07
 5.79091277e+07 5.14848716e+07]
Done!
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [1.99212129e-05 2.05604726e-04 1.91166988e-05 7.78658377e-07
 3.22557186e-04 3.58592613e-05 1.53724990e-06 5.57268965e-05
 2.61570432e-05 1.87668994e-05]
ene_total = [0.00669092 0.00837971 0.00679064 0.0064223  0.00876002 0.0087964
 0.00637547 0.00635849 0.00923511 0.00865832]
At round 9 energy consumption: 0.07646738157410429
At round 9 eta: 0.6909608187976134
At round 9 a_n: 24.7571443865612
At round 9 local rounds: 12.104944645341616
At round 9 global rounds: 81.21847247936174
gradient difference: 0.44947710633277893
train() client id: f_00000-0-0 loss: 1.267919  [   32/  126]
train() client id: f_00000-0-1 loss: 1.538373  [   64/  126]
train() client id: f_00000-0-2 loss: 1.386360  [   96/  126]
train() client id: f_00000-1-0 loss: 1.322367  [   32/  126]
train() client id: f_00000-1-1 loss: 1.301502  [   64/  126]
train() client id: f_00000-1-2 loss: 1.340301  [   96/  126]
train() client id: f_00000-2-0 loss: 1.173406  [   32/  126]
train() client id: f_00000-2-1 loss: 1.137838  [   64/  126]
train() client id: f_00000-2-2 loss: 1.118526  [   96/  126]
train() client id: f_00000-3-0 loss: 1.064967  [   32/  126]
train() client id: f_00000-3-1 loss: 1.149504  [   64/  126]
train() client id: f_00000-3-2 loss: 1.085337  [   96/  126]
train() client id: f_00000-4-0 loss: 1.087969  [   32/  126]
train() client id: f_00000-4-1 loss: 0.969456  [   64/  126]
train() client id: f_00000-4-2 loss: 0.981089  [   96/  126]
train() client id: f_00000-5-0 loss: 1.009575  [   32/  126]
train() client id: f_00000-5-1 loss: 0.961046  [   64/  126]
train() client id: f_00000-5-2 loss: 0.983354  [   96/  126]
train() client id: f_00000-6-0 loss: 0.932683  [   32/  126]
train() client id: f_00000-6-1 loss: 0.940426  [   64/  126]
train() client id: f_00000-6-2 loss: 0.942982  [   96/  126]
train() client id: f_00000-7-0 loss: 0.981202  [   32/  126]
train() client id: f_00000-7-1 loss: 0.916159  [   64/  126]
train() client id: f_00000-7-2 loss: 0.885060  [   96/  126]
train() client id: f_00000-8-0 loss: 0.862188  [   32/  126]
train() client id: f_00000-8-1 loss: 0.931846  [   64/  126]
train() client id: f_00000-8-2 loss: 0.948084  [   96/  126]
train() client id: f_00000-9-0 loss: 0.897050  [   32/  126]
train() client id: f_00000-9-1 loss: 0.945594  [   64/  126]
train() client id: f_00000-9-2 loss: 0.932483  [   96/  126]
train() client id: f_00000-10-0 loss: 0.842076  [   32/  126]
train() client id: f_00000-10-1 loss: 0.872292  [   64/  126]
train() client id: f_00000-10-2 loss: 0.985870  [   96/  126]
train() client id: f_00000-11-0 loss: 0.922095  [   32/  126]
train() client id: f_00000-11-1 loss: 0.893423  [   64/  126]
train() client id: f_00000-11-2 loss: 0.946555  [   96/  126]
train() client id: f_00001-0-0 loss: 0.477302  [   32/  265]
train() client id: f_00001-0-1 loss: 0.468199  [   64/  265]
train() client id: f_00001-0-2 loss: 0.443538  [   96/  265]
train() client id: f_00001-0-3 loss: 0.506421  [  128/  265]
train() client id: f_00001-0-4 loss: 0.480210  [  160/  265]
train() client id: f_00001-0-5 loss: 0.426972  [  192/  265]
train() client id: f_00001-0-6 loss: 0.371493  [  224/  265]
train() client id: f_00001-0-7 loss: 0.422248  [  256/  265]
train() client id: f_00001-1-0 loss: 0.374491  [   32/  265]
train() client id: f_00001-1-1 loss: 0.411020  [   64/  265]
train() client id: f_00001-1-2 loss: 0.496836  [   96/  265]
train() client id: f_00001-1-3 loss: 0.458519  [  128/  265]
train() client id: f_00001-1-4 loss: 0.451743  [  160/  265]
train() client id: f_00001-1-5 loss: 0.394935  [  192/  265]
train() client id: f_00001-1-6 loss: 0.322740  [  224/  265]
train() client id: f_00001-1-7 loss: 0.517096  [  256/  265]
train() client id: f_00001-2-0 loss: 0.339582  [   32/  265]
train() client id: f_00001-2-1 loss: 0.456329  [   64/  265]
train() client id: f_00001-2-2 loss: 0.394366  [   96/  265]
train() client id: f_00001-2-3 loss: 0.342372  [  128/  265]
train() client id: f_00001-2-4 loss: 0.364934  [  160/  265]
train() client id: f_00001-2-5 loss: 0.519069  [  192/  265]
train() client id: f_00001-2-6 loss: 0.344827  [  224/  265]
train() client id: f_00001-2-7 loss: 0.475945  [  256/  265]
train() client id: f_00001-3-0 loss: 0.503535  [   32/  265]
train() client id: f_00001-3-1 loss: 0.360425  [   64/  265]
train() client id: f_00001-3-2 loss: 0.373604  [   96/  265]
train() client id: f_00001-3-3 loss: 0.454478  [  128/  265]
train() client id: f_00001-3-4 loss: 0.320179  [  160/  265]
train() client id: f_00001-3-5 loss: 0.326738  [  192/  265]
train() client id: f_00001-3-6 loss: 0.428207  [  224/  265]
train() client id: f_00001-3-7 loss: 0.429924  [  256/  265]
train() client id: f_00001-4-0 loss: 0.372169  [   32/  265]
train() client id: f_00001-4-1 loss: 0.478177  [   64/  265]
train() client id: f_00001-4-2 loss: 0.369572  [   96/  265]
train() client id: f_00001-4-3 loss: 0.421859  [  128/  265]
train() client id: f_00001-4-4 loss: 0.413431  [  160/  265]
train() client id: f_00001-4-5 loss: 0.363165  [  192/  265]
train() client id: f_00001-4-6 loss: 0.300354  [  224/  265]
train() client id: f_00001-4-7 loss: 0.400175  [  256/  265]
train() client id: f_00001-5-0 loss: 0.428132  [   32/  265]
train() client id: f_00001-5-1 loss: 0.421213  [   64/  265]
train() client id: f_00001-5-2 loss: 0.361389  [   96/  265]
train() client id: f_00001-5-3 loss: 0.507352  [  128/  265]
train() client id: f_00001-5-4 loss: 0.304147  [  160/  265]
train() client id: f_00001-5-5 loss: 0.369680  [  192/  265]
train() client id: f_00001-5-6 loss: 0.387026  [  224/  265]
train() client id: f_00001-5-7 loss: 0.291709  [  256/  265]
train() client id: f_00001-6-0 loss: 0.301753  [   32/  265]
train() client id: f_00001-6-1 loss: 0.263403  [   64/  265]
train() client id: f_00001-6-2 loss: 0.419210  [   96/  265]
train() client id: f_00001-6-3 loss: 0.287218  [  128/  265]
train() client id: f_00001-6-4 loss: 0.510447  [  160/  265]
train() client id: f_00001-6-5 loss: 0.410524  [  192/  265]
train() client id: f_00001-6-6 loss: 0.530651  [  224/  265]
train() client id: f_00001-6-7 loss: 0.288209  [  256/  265]
train() client id: f_00001-7-0 loss: 0.349987  [   32/  265]
train() client id: f_00001-7-1 loss: 0.406378  [   64/  265]
train() client id: f_00001-7-2 loss: 0.315677  [   96/  265]
train() client id: f_00001-7-3 loss: 0.355339  [  128/  265]
train() client id: f_00001-7-4 loss: 0.416567  [  160/  265]
train() client id: f_00001-7-5 loss: 0.398556  [  192/  265]
train() client id: f_00001-7-6 loss: 0.312989  [  224/  265]
train() client id: f_00001-7-7 loss: 0.409425  [  256/  265]
train() client id: f_00001-8-0 loss: 0.473662  [   32/  265]
train() client id: f_00001-8-1 loss: 0.383495  [   64/  265]
train() client id: f_00001-8-2 loss: 0.333083  [   96/  265]
train() client id: f_00001-8-3 loss: 0.283783  [  128/  265]
train() client id: f_00001-8-4 loss: 0.311397  [  160/  265]
train() client id: f_00001-8-5 loss: 0.268498  [  192/  265]
train() client id: f_00001-8-6 loss: 0.420206  [  224/  265]
train() client id: f_00001-8-7 loss: 0.392705  [  256/  265]
train() client id: f_00001-9-0 loss: 0.353643  [   32/  265]
train() client id: f_00001-9-1 loss: 0.265579  [   64/  265]
train() client id: f_00001-9-2 loss: 0.478870  [   96/  265]
train() client id: f_00001-9-3 loss: 0.315460  [  128/  265]
train() client id: f_00001-9-4 loss: 0.498979  [  160/  265]
train() client id: f_00001-9-5 loss: 0.405655  [  192/  265]
train() client id: f_00001-9-6 loss: 0.346857  [  224/  265]
train() client id: f_00001-9-7 loss: 0.264677  [  256/  265]
train() client id: f_00001-10-0 loss: 0.468055  [   32/  265]
train() client id: f_00001-10-1 loss: 0.318154  [   64/  265]
train() client id: f_00001-10-2 loss: 0.476376  [   96/  265]
train() client id: f_00001-10-3 loss: 0.275135  [  128/  265]
train() client id: f_00001-10-4 loss: 0.318444  [  160/  265]
train() client id: f_00001-10-5 loss: 0.328260  [  192/  265]
train() client id: f_00001-10-6 loss: 0.389131  [  224/  265]
train() client id: f_00001-10-7 loss: 0.280322  [  256/  265]
train() client id: f_00001-11-0 loss: 0.501553  [   32/  265]
train() client id: f_00001-11-1 loss: 0.410572  [   64/  265]
train() client id: f_00001-11-2 loss: 0.256050  [   96/  265]
train() client id: f_00001-11-3 loss: 0.356470  [  128/  265]
train() client id: f_00001-11-4 loss: 0.310678  [  160/  265]
train() client id: f_00001-11-5 loss: 0.382520  [  192/  265]
train() client id: f_00001-11-6 loss: 0.281293  [  224/  265]
train() client id: f_00001-11-7 loss: 0.376942  [  256/  265]
train() client id: f_00002-0-0 loss: 1.088356  [   32/  124]
train() client id: f_00002-0-1 loss: 1.105335  [   64/  124]
train() client id: f_00002-0-2 loss: 1.115731  [   96/  124]
train() client id: f_00002-1-0 loss: 1.074643  [   32/  124]
train() client id: f_00002-1-1 loss: 1.071833  [   64/  124]
train() client id: f_00002-1-2 loss: 1.062641  [   96/  124]
train() client id: f_00002-2-0 loss: 1.110387  [   32/  124]
train() client id: f_00002-2-1 loss: 1.005176  [   64/  124]
train() client id: f_00002-2-2 loss: 1.040456  [   96/  124]
train() client id: f_00002-3-0 loss: 1.122395  [   32/  124]
train() client id: f_00002-3-1 loss: 0.996205  [   64/  124]
train() client id: f_00002-3-2 loss: 1.013888  [   96/  124]
train() client id: f_00002-4-0 loss: 0.998137  [   32/  124]
train() client id: f_00002-4-1 loss: 1.113928  [   64/  124]
train() client id: f_00002-4-2 loss: 0.964933  [   96/  124]
train() client id: f_00002-5-0 loss: 0.927364  [   32/  124]
train() client id: f_00002-5-1 loss: 1.194994  [   64/  124]
train() client id: f_00002-5-2 loss: 0.897817  [   96/  124]
train() client id: f_00002-6-0 loss: 0.916323  [   32/  124]
train() client id: f_00002-6-1 loss: 1.052274  [   64/  124]
train() client id: f_00002-6-2 loss: 0.950225  [   96/  124]
train() client id: f_00002-7-0 loss: 1.015490  [   32/  124]
train() client id: f_00002-7-1 loss: 0.970424  [   64/  124]
train() client id: f_00002-7-2 loss: 0.876463  [   96/  124]
train() client id: f_00002-8-0 loss: 0.907015  [   32/  124]
train() client id: f_00002-8-1 loss: 0.972755  [   64/  124]
train() client id: f_00002-8-2 loss: 1.042621  [   96/  124]
train() client id: f_00002-9-0 loss: 1.070545  [   32/  124]
train() client id: f_00002-9-1 loss: 0.937463  [   64/  124]
train() client id: f_00002-9-2 loss: 0.926623  [   96/  124]
train() client id: f_00002-10-0 loss: 0.874829  [   32/  124]
train() client id: f_00002-10-1 loss: 0.843508  [   64/  124]
train() client id: f_00002-10-2 loss: 1.030933  [   96/  124]
train() client id: f_00002-11-0 loss: 0.951774  [   32/  124]
train() client id: f_00002-11-1 loss: 1.013321  [   64/  124]
train() client id: f_00002-11-2 loss: 0.937762  [   96/  124]
train() client id: f_00003-0-0 loss: 0.982947  [   32/   43]
train() client id: f_00003-1-0 loss: 0.949743  [   32/   43]
train() client id: f_00003-2-0 loss: 0.789204  [   32/   43]
train() client id: f_00003-3-0 loss: 0.950470  [   32/   43]
train() client id: f_00003-4-0 loss: 0.803344  [   32/   43]
train() client id: f_00003-5-0 loss: 0.867710  [   32/   43]
train() client id: f_00003-6-0 loss: 0.886493  [   32/   43]
train() client id: f_00003-7-0 loss: 0.915887  [   32/   43]
train() client id: f_00003-8-0 loss: 0.886555  [   32/   43]
train() client id: f_00003-9-0 loss: 0.752768  [   32/   43]
train() client id: f_00003-10-0 loss: 0.973807  [   32/   43]
train() client id: f_00003-11-0 loss: 0.840037  [   32/   43]
train() client id: f_00004-0-0 loss: 0.890801  [   32/  306]
train() client id: f_00004-0-1 loss: 0.901783  [   64/  306]
train() client id: f_00004-0-2 loss: 1.092350  [   96/  306]
train() client id: f_00004-0-3 loss: 1.026208  [  128/  306]
train() client id: f_00004-0-4 loss: 0.904609  [  160/  306]
train() client id: f_00004-0-5 loss: 0.762124  [  192/  306]
train() client id: f_00004-0-6 loss: 0.851021  [  224/  306]
train() client id: f_00004-0-7 loss: 0.748513  [  256/  306]
train() client id: f_00004-0-8 loss: 0.904062  [  288/  306]
train() client id: f_00004-1-0 loss: 1.062580  [   32/  306]
train() client id: f_00004-1-1 loss: 0.832749  [   64/  306]
train() client id: f_00004-1-2 loss: 0.683477  [   96/  306]
train() client id: f_00004-1-3 loss: 1.083735  [  128/  306]
train() client id: f_00004-1-4 loss: 0.959802  [  160/  306]
train() client id: f_00004-1-5 loss: 0.763224  [  192/  306]
train() client id: f_00004-1-6 loss: 0.730916  [  224/  306]
train() client id: f_00004-1-7 loss: 0.970415  [  256/  306]
train() client id: f_00004-1-8 loss: 0.836743  [  288/  306]
train() client id: f_00004-2-0 loss: 0.921881  [   32/  306]
train() client id: f_00004-2-1 loss: 0.815488  [   64/  306]
train() client id: f_00004-2-2 loss: 0.967340  [   96/  306]
train() client id: f_00004-2-3 loss: 0.815781  [  128/  306]
train() client id: f_00004-2-4 loss: 0.960083  [  160/  306]
train() client id: f_00004-2-5 loss: 0.834806  [  192/  306]
train() client id: f_00004-2-6 loss: 0.802420  [  224/  306]
train() client id: f_00004-2-7 loss: 0.855523  [  256/  306]
train() client id: f_00004-2-8 loss: 0.947243  [  288/  306]
train() client id: f_00004-3-0 loss: 0.884931  [   32/  306]
train() client id: f_00004-3-1 loss: 0.898351  [   64/  306]
train() client id: f_00004-3-2 loss: 0.920843  [   96/  306]
train() client id: f_00004-3-3 loss: 0.796081  [  128/  306]
train() client id: f_00004-3-4 loss: 0.889676  [  160/  306]
train() client id: f_00004-3-5 loss: 0.834421  [  192/  306]
train() client id: f_00004-3-6 loss: 0.857999  [  224/  306]
train() client id: f_00004-3-7 loss: 0.922799  [  256/  306]
train() client id: f_00004-3-8 loss: 0.894046  [  288/  306]
train() client id: f_00004-4-0 loss: 0.946137  [   32/  306]
train() client id: f_00004-4-1 loss: 0.810214  [   64/  306]
train() client id: f_00004-4-2 loss: 0.747856  [   96/  306]
train() client id: f_00004-4-3 loss: 0.847412  [  128/  306]
train() client id: f_00004-4-4 loss: 0.993527  [  160/  306]
train() client id: f_00004-4-5 loss: 0.869237  [  192/  306]
train() client id: f_00004-4-6 loss: 0.838799  [  224/  306]
train() client id: f_00004-4-7 loss: 0.964917  [  256/  306]
train() client id: f_00004-4-8 loss: 0.980820  [  288/  306]
train() client id: f_00004-5-0 loss: 0.864416  [   32/  306]
train() client id: f_00004-5-1 loss: 0.894055  [   64/  306]
train() client id: f_00004-5-2 loss: 0.908238  [   96/  306]
train() client id: f_00004-5-3 loss: 1.000399  [  128/  306]
train() client id: f_00004-5-4 loss: 0.878753  [  160/  306]
train() client id: f_00004-5-5 loss: 0.788304  [  192/  306]
train() client id: f_00004-5-6 loss: 0.848813  [  224/  306]
train() client id: f_00004-5-7 loss: 0.920373  [  256/  306]
train() client id: f_00004-5-8 loss: 0.855561  [  288/  306]
train() client id: f_00004-6-0 loss: 0.969551  [   32/  306]
train() client id: f_00004-6-1 loss: 0.730261  [   64/  306]
train() client id: f_00004-6-2 loss: 0.859596  [   96/  306]
train() client id: f_00004-6-3 loss: 0.880505  [  128/  306]
train() client id: f_00004-6-4 loss: 0.785347  [  160/  306]
train() client id: f_00004-6-5 loss: 0.938969  [  192/  306]
train() client id: f_00004-6-6 loss: 0.828386  [  224/  306]
train() client id: f_00004-6-7 loss: 0.948803  [  256/  306]
train() client id: f_00004-6-8 loss: 0.981734  [  288/  306]
train() client id: f_00004-7-0 loss: 0.893862  [   32/  306]
train() client id: f_00004-7-1 loss: 0.824943  [   64/  306]
train() client id: f_00004-7-2 loss: 0.868279  [   96/  306]
train() client id: f_00004-7-3 loss: 0.875454  [  128/  306]
train() client id: f_00004-7-4 loss: 0.948911  [  160/  306]
train() client id: f_00004-7-5 loss: 0.813733  [  192/  306]
train() client id: f_00004-7-6 loss: 0.847649  [  224/  306]
train() client id: f_00004-7-7 loss: 0.893390  [  256/  306]
train() client id: f_00004-7-8 loss: 0.891255  [  288/  306]
train() client id: f_00004-8-0 loss: 0.976319  [   32/  306]
train() client id: f_00004-8-1 loss: 0.916620  [   64/  306]
train() client id: f_00004-8-2 loss: 0.889977  [   96/  306]
train() client id: f_00004-8-3 loss: 0.893674  [  128/  306]
train() client id: f_00004-8-4 loss: 0.866037  [  160/  306]
train() client id: f_00004-8-5 loss: 0.788954  [  192/  306]
train() client id: f_00004-8-6 loss: 0.904023  [  224/  306]
train() client id: f_00004-8-7 loss: 0.863221  [  256/  306]
train() client id: f_00004-8-8 loss: 0.830423  [  288/  306]
train() client id: f_00004-9-0 loss: 0.955510  [   32/  306]
train() client id: f_00004-9-1 loss: 0.792621  [   64/  306]
train() client id: f_00004-9-2 loss: 0.954558  [   96/  306]
train() client id: f_00004-9-3 loss: 0.951111  [  128/  306]
train() client id: f_00004-9-4 loss: 0.961728  [  160/  306]
train() client id: f_00004-9-5 loss: 0.816628  [  192/  306]
train() client id: f_00004-9-6 loss: 0.746896  [  224/  306]
train() client id: f_00004-9-7 loss: 0.885541  [  256/  306]
train() client id: f_00004-9-8 loss: 0.907905  [  288/  306]
train() client id: f_00004-10-0 loss: 0.926611  [   32/  306]
train() client id: f_00004-10-1 loss: 0.821855  [   64/  306]
train() client id: f_00004-10-2 loss: 0.956302  [   96/  306]
train() client id: f_00004-10-3 loss: 0.837289  [  128/  306]
train() client id: f_00004-10-4 loss: 0.831545  [  160/  306]
train() client id: f_00004-10-5 loss: 0.922678  [  192/  306]
train() client id: f_00004-10-6 loss: 0.811393  [  224/  306]
train() client id: f_00004-10-7 loss: 0.953946  [  256/  306]
train() client id: f_00004-10-8 loss: 0.964879  [  288/  306]
train() client id: f_00004-11-0 loss: 0.968358  [   32/  306]
train() client id: f_00004-11-1 loss: 0.900724  [   64/  306]
train() client id: f_00004-11-2 loss: 0.866134  [   96/  306]
train() client id: f_00004-11-3 loss: 0.829368  [  128/  306]
train() client id: f_00004-11-4 loss: 0.856929  [  160/  306]
train() client id: f_00004-11-5 loss: 0.876386  [  192/  306]
train() client id: f_00004-11-6 loss: 0.905011  [  224/  306]
train() client id: f_00004-11-7 loss: 0.820551  [  256/  306]
train() client id: f_00004-11-8 loss: 0.866003  [  288/  306]
train() client id: f_00005-0-0 loss: 1.036393  [   32/  146]
train() client id: f_00005-0-1 loss: 0.881746  [   64/  146]
train() client id: f_00005-0-2 loss: 0.827835  [   96/  146]
train() client id: f_00005-0-3 loss: 1.036826  [  128/  146]
train() client id: f_00005-1-0 loss: 0.940367  [   32/  146]
train() client id: f_00005-1-1 loss: 1.057062  [   64/  146]
train() client id: f_00005-1-2 loss: 0.817532  [   96/  146]
train() client id: f_00005-1-3 loss: 0.918252  [  128/  146]
train() client id: f_00005-2-0 loss: 0.951089  [   32/  146]
train() client id: f_00005-2-1 loss: 0.786463  [   64/  146]
train() client id: f_00005-2-2 loss: 0.971733  [   96/  146]
train() client id: f_00005-2-3 loss: 0.914398  [  128/  146]
train() client id: f_00005-3-0 loss: 0.901046  [   32/  146]
train() client id: f_00005-3-1 loss: 0.982208  [   64/  146]
train() client id: f_00005-3-2 loss: 0.899875  [   96/  146]
train() client id: f_00005-3-3 loss: 0.891344  [  128/  146]
train() client id: f_00005-4-0 loss: 0.867530  [   32/  146]
train() client id: f_00005-4-1 loss: 1.049985  [   64/  146]
train() client id: f_00005-4-2 loss: 1.004624  [   96/  146]
train() client id: f_00005-4-3 loss: 0.815668  [  128/  146]
train() client id: f_00005-5-0 loss: 0.843736  [   32/  146]
train() client id: f_00005-5-1 loss: 0.788024  [   64/  146]
train() client id: f_00005-5-2 loss: 0.963062  [   96/  146]
train() client id: f_00005-5-3 loss: 1.017482  [  128/  146]
train() client id: f_00005-6-0 loss: 1.049406  [   32/  146]
train() client id: f_00005-6-1 loss: 1.026259  [   64/  146]
train() client id: f_00005-6-2 loss: 0.727377  [   96/  146]
train() client id: f_00005-6-3 loss: 0.953375  [  128/  146]
train() client id: f_00005-7-0 loss: 0.788072  [   32/  146]
train() client id: f_00005-7-1 loss: 0.824721  [   64/  146]
train() client id: f_00005-7-2 loss: 1.042640  [   96/  146]
train() client id: f_00005-7-3 loss: 1.009894  [  128/  146]
train() client id: f_00005-8-0 loss: 1.016236  [   32/  146]
train() client id: f_00005-8-1 loss: 0.885143  [   64/  146]
train() client id: f_00005-8-2 loss: 0.841355  [   96/  146]
train() client id: f_00005-8-3 loss: 0.986236  [  128/  146]
train() client id: f_00005-9-0 loss: 0.878967  [   32/  146]
train() client id: f_00005-9-1 loss: 0.996593  [   64/  146]
train() client id: f_00005-9-2 loss: 0.854804  [   96/  146]
train() client id: f_00005-9-3 loss: 1.135993  [  128/  146]
train() client id: f_00005-10-0 loss: 0.960754  [   32/  146]
train() client id: f_00005-10-1 loss: 1.178918  [   64/  146]
train() client id: f_00005-10-2 loss: 0.810465  [   96/  146]
train() client id: f_00005-10-3 loss: 0.787494  [  128/  146]
train() client id: f_00005-11-0 loss: 0.887889  [   32/  146]
train() client id: f_00005-11-1 loss: 0.654163  [   64/  146]
train() client id: f_00005-11-2 loss: 0.951980  [   96/  146]
train() client id: f_00005-11-3 loss: 1.189963  [  128/  146]
train() client id: f_00006-0-0 loss: 0.710392  [   32/   54]
train() client id: f_00006-1-0 loss: 0.775223  [   32/   54]
train() client id: f_00006-2-0 loss: 0.724385  [   32/   54]
train() client id: f_00006-3-0 loss: 0.730738  [   32/   54]
train() client id: f_00006-4-0 loss: 0.772277  [   32/   54]
train() client id: f_00006-5-0 loss: 0.717124  [   32/   54]
train() client id: f_00006-6-0 loss: 0.759172  [   32/   54]
train() client id: f_00006-7-0 loss: 0.704554  [   32/   54]
train() client id: f_00006-8-0 loss: 0.711598  [   32/   54]
train() client id: f_00006-9-0 loss: 0.727228  [   32/   54]
train() client id: f_00006-10-0 loss: 0.714956  [   32/   54]
train() client id: f_00006-11-0 loss: 0.726920  [   32/   54]
train() client id: f_00007-0-0 loss: 0.730560  [   32/  179]
train() client id: f_00007-0-1 loss: 0.812333  [   64/  179]
train() client id: f_00007-0-2 loss: 0.610135  [   96/  179]
train() client id: f_00007-0-3 loss: 0.696222  [  128/  179]
train() client id: f_00007-0-4 loss: 0.600694  [  160/  179]
train() client id: f_00007-1-0 loss: 0.660327  [   32/  179]
train() client id: f_00007-1-1 loss: 0.730314  [   64/  179]
train() client id: f_00007-1-2 loss: 0.652521  [   96/  179]
train() client id: f_00007-1-3 loss: 0.599435  [  128/  179]
train() client id: f_00007-1-4 loss: 0.745304  [  160/  179]
train() client id: f_00007-2-0 loss: 0.646272  [   32/  179]
train() client id: f_00007-2-1 loss: 0.715581  [   64/  179]
train() client id: f_00007-2-2 loss: 0.812354  [   96/  179]
train() client id: f_00007-2-3 loss: 0.620831  [  128/  179]
train() client id: f_00007-2-4 loss: 0.586014  [  160/  179]
train() client id: f_00007-3-0 loss: 0.619894  [   32/  179]
train() client id: f_00007-3-1 loss: 0.576724  [   64/  179]
train() client id: f_00007-3-2 loss: 0.641836  [   96/  179]
train() client id: f_00007-3-3 loss: 0.713296  [  128/  179]
train() client id: f_00007-3-4 loss: 0.756180  [  160/  179]
train() client id: f_00007-4-0 loss: 0.697374  [   32/  179]
train() client id: f_00007-4-1 loss: 0.638123  [   64/  179]
train() client id: f_00007-4-2 loss: 0.621697  [   96/  179]
train() client id: f_00007-4-3 loss: 0.686908  [  128/  179]
train() client id: f_00007-4-4 loss: 0.692594  [  160/  179]
train() client id: f_00007-5-0 loss: 0.548728  [   32/  179]
train() client id: f_00007-5-1 loss: 0.726035  [   64/  179]
train() client id: f_00007-5-2 loss: 0.635241  [   96/  179]
train() client id: f_00007-5-3 loss: 0.636540  [  128/  179]
train() client id: f_00007-5-4 loss: 0.640332  [  160/  179]
train() client id: f_00007-6-0 loss: 0.647786  [   32/  179]
train() client id: f_00007-6-1 loss: 0.667945  [   64/  179]
train() client id: f_00007-6-2 loss: 0.644538  [   96/  179]
train() client id: f_00007-6-3 loss: 0.824236  [  128/  179]
train() client id: f_00007-6-4 loss: 0.529301  [  160/  179]
train() client id: f_00007-7-0 loss: 0.540710  [   32/  179]
train() client id: f_00007-7-1 loss: 0.682809  [   64/  179]
train() client id: f_00007-7-2 loss: 0.703085  [   96/  179]
train() client id: f_00007-7-3 loss: 0.690893  [  128/  179]
train() client id: f_00007-7-4 loss: 0.597746  [  160/  179]
train() client id: f_00007-8-0 loss: 0.635850  [   32/  179]
train() client id: f_00007-8-1 loss: 0.691831  [   64/  179]
train() client id: f_00007-8-2 loss: 0.610653  [   96/  179]
train() client id: f_00007-8-3 loss: 0.544288  [  128/  179]
train() client id: f_00007-8-4 loss: 0.679856  [  160/  179]
train() client id: f_00007-9-0 loss: 0.628384  [   32/  179]
train() client id: f_00007-9-1 loss: 0.719440  [   64/  179]
train() client id: f_00007-9-2 loss: 0.593349  [   96/  179]
train() client id: f_00007-9-3 loss: 0.637659  [  128/  179]
train() client id: f_00007-9-4 loss: 0.702690  [  160/  179]
train() client id: f_00007-10-0 loss: 0.672361  [   32/  179]
train() client id: f_00007-10-1 loss: 0.800175  [   64/  179]
train() client id: f_00007-10-2 loss: 0.616336  [   96/  179]
train() client id: f_00007-10-3 loss: 0.546761  [  128/  179]
train() client id: f_00007-10-4 loss: 0.591802  [  160/  179]
train() client id: f_00007-11-0 loss: 0.607823  [   32/  179]
train() client id: f_00007-11-1 loss: 0.757942  [   64/  179]
train() client id: f_00007-11-2 loss: 0.555354  [   96/  179]
train() client id: f_00007-11-3 loss: 0.744853  [  128/  179]
train() client id: f_00007-11-4 loss: 0.617847  [  160/  179]
train() client id: f_00008-0-0 loss: 0.781051  [   32/  130]
train() client id: f_00008-0-1 loss: 0.799420  [   64/  130]
train() client id: f_00008-0-2 loss: 0.681220  [   96/  130]
train() client id: f_00008-0-3 loss: 0.745647  [  128/  130]
train() client id: f_00008-1-0 loss: 0.786506  [   32/  130]
train() client id: f_00008-1-1 loss: 0.686315  [   64/  130]
train() client id: f_00008-1-2 loss: 0.804686  [   96/  130]
train() client id: f_00008-1-3 loss: 0.723724  [  128/  130]
train() client id: f_00008-2-0 loss: 0.761116  [   32/  130]
train() client id: f_00008-2-1 loss: 0.731889  [   64/  130]
train() client id: f_00008-2-2 loss: 0.777542  [   96/  130]
train() client id: f_00008-2-3 loss: 0.732317  [  128/  130]
train() client id: f_00008-3-0 loss: 0.693684  [   32/  130]
train() client id: f_00008-3-1 loss: 0.811310  [   64/  130]
train() client id: f_00008-3-2 loss: 0.698292  [   96/  130]
train() client id: f_00008-3-3 loss: 0.788266  [  128/  130]
train() client id: f_00008-4-0 loss: 0.737035  [   32/  130]
train() client id: f_00008-4-1 loss: 0.722672  [   64/  130]
train() client id: f_00008-4-2 loss: 0.758439  [   96/  130]
train() client id: f_00008-4-3 loss: 0.749635  [  128/  130]
train() client id: f_00008-5-0 loss: 0.829534  [   32/  130]
train() client id: f_00008-5-1 loss: 0.750094  [   64/  130]
train() client id: f_00008-5-2 loss: 0.665067  [   96/  130]
train() client id: f_00008-5-3 loss: 0.739281  [  128/  130]
train() client id: f_00008-6-0 loss: 0.764371  [   32/  130]
train() client id: f_00008-6-1 loss: 0.706731  [   64/  130]
train() client id: f_00008-6-2 loss: 0.745239  [   96/  130]
train() client id: f_00008-6-3 loss: 0.763756  [  128/  130]
train() client id: f_00008-7-0 loss: 0.760867  [   32/  130]
train() client id: f_00008-7-1 loss: 0.675909  [   64/  130]
train() client id: f_00008-7-2 loss: 0.832065  [   96/  130]
train() client id: f_00008-7-3 loss: 0.714616  [  128/  130]
train() client id: f_00008-8-0 loss: 0.843271  [   32/  130]
train() client id: f_00008-8-1 loss: 0.719794  [   64/  130]
train() client id: f_00008-8-2 loss: 0.707281  [   96/  130]
train() client id: f_00008-8-3 loss: 0.716283  [  128/  130]
train() client id: f_00008-9-0 loss: 0.865750  [   32/  130]
train() client id: f_00008-9-1 loss: 0.706107  [   64/  130]
train() client id: f_00008-9-2 loss: 0.759596  [   96/  130]
train() client id: f_00008-9-3 loss: 0.625252  [  128/  130]
train() client id: f_00008-10-0 loss: 0.724768  [   32/  130]
train() client id: f_00008-10-1 loss: 0.777654  [   64/  130]
train() client id: f_00008-10-2 loss: 0.704456  [   96/  130]
train() client id: f_00008-10-3 loss: 0.749429  [  128/  130]
train() client id: f_00008-11-0 loss: 0.754179  [   32/  130]
train() client id: f_00008-11-1 loss: 0.842650  [   64/  130]
train() client id: f_00008-11-2 loss: 0.648521  [   96/  130]
train() client id: f_00008-11-3 loss: 0.731734  [  128/  130]
train() client id: f_00009-0-0 loss: 1.095412  [   32/  118]
train() client id: f_00009-0-1 loss: 1.022675  [   64/  118]
train() client id: f_00009-0-2 loss: 1.159640  [   96/  118]
train() client id: f_00009-1-0 loss: 1.049033  [   32/  118]
train() client id: f_00009-1-1 loss: 1.101639  [   64/  118]
train() client id: f_00009-1-2 loss: 0.993010  [   96/  118]
train() client id: f_00009-2-0 loss: 0.973097  [   32/  118]
train() client id: f_00009-2-1 loss: 0.980407  [   64/  118]
train() client id: f_00009-2-2 loss: 0.992497  [   96/  118]
train() client id: f_00009-3-0 loss: 1.034834  [   32/  118]
train() client id: f_00009-3-1 loss: 0.945706  [   64/  118]
train() client id: f_00009-3-2 loss: 0.968866  [   96/  118]
train() client id: f_00009-4-0 loss: 1.042654  [   32/  118]
train() client id: f_00009-4-1 loss: 0.939738  [   64/  118]
train() client id: f_00009-4-2 loss: 0.916691  [   96/  118]
train() client id: f_00009-5-0 loss: 1.068631  [   32/  118]
train() client id: f_00009-5-1 loss: 0.899146  [   64/  118]
train() client id: f_00009-5-2 loss: 0.922477  [   96/  118]
train() client id: f_00009-6-0 loss: 0.827071  [   32/  118]
train() client id: f_00009-6-1 loss: 0.971698  [   64/  118]
train() client id: f_00009-6-2 loss: 0.900062  [   96/  118]
train() client id: f_00009-7-0 loss: 0.998656  [   32/  118]
train() client id: f_00009-7-1 loss: 0.907125  [   64/  118]
train() client id: f_00009-7-2 loss: 0.830038  [   96/  118]
train() client id: f_00009-8-0 loss: 0.858591  [   32/  118]
train() client id: f_00009-8-1 loss: 0.992295  [   64/  118]
train() client id: f_00009-8-2 loss: 0.917882  [   96/  118]
train() client id: f_00009-9-0 loss: 0.798566  [   32/  118]
train() client id: f_00009-9-1 loss: 0.866886  [   64/  118]
train() client id: f_00009-9-2 loss: 0.971870  [   96/  118]
train() client id: f_00009-10-0 loss: 0.903490  [   32/  118]
train() client id: f_00009-10-1 loss: 0.843131  [   64/  118]
train() client id: f_00009-10-2 loss: 0.945617  [   96/  118]
train() client id: f_00009-11-0 loss: 0.919824  [   32/  118]
train() client id: f_00009-11-1 loss: 0.889566  [   64/  118]
train() client id: f_00009-11-2 loss: 0.852011  [   96/  118]
At round 9 accuracy: 0.6312997347480106
At round 9 training accuracy: 0.5754527162977867
At round 9 training loss: 0.8658488338158992
update_location
xs = [ -3.9056584    4.20031788  65.00902392  18.81129433   0.97929623
   3.95640986 -27.44319194  -6.32485185  49.66397685   2.93912145]
ys = [ 57.5879595   40.55583871   1.32061395 -27.45517586  19.35018685
   2.81415074  -2.62498432   0.82234798  17.56900603  -0.99851822]
dists_uav = [115.46266603 107.99267903 119.28083338 105.39284357 101.85965223
 100.11779374 103.730513   100.2031936  113.02734434 100.04816577]
dists_bs = [207.50629142 224.02675383 296.25809812 280.26936337 234.93738961
 248.34122355 230.91913446 242.46476046 274.3326819  250.27546695]
uav_gains = [6.98045343e-11 8.25105385e-11 6.43507590e-11 8.76937845e-11
 9.54975988e-11 9.97057494e-11 9.12496048e-11 9.94934409e-11
 7.36261378e-11 9.98793178e-11]
bs_gains = [3.59423553e-11 2.90037668e-11 1.32622570e-11 1.54911026e-11
 2.53879770e-11 2.17348876e-11 2.66444222e-11 2.32422371e-11
 1.64481458e-11 2.12678163e-11]
Round 10
-------------------------------
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.67144915 20.18897006  9.52328793  3.40449035 23.28757639 11.22831614
  4.23293599 13.65794809 10.03742584  9.11771254]
obj_prev = 114.35011248363482
eta_min = 2.485131023397881e-10	eta_max = 0.9203875705067922
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 26.5917511352464	eta = 0.909090909090909
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 46.02197555438805	eta = 0.5252777379209097
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 36.745409717816976	eta = 0.6578867782263103
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 35.08220734696584	eta = 0.6890763450194114
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 35.00007514490989	eta = 0.6906933517648766
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 34.99986062549486	eta = 0.690697585128414
eta = 0.690697585128414
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [0.0305359  0.06422234 0.03005121 0.01042098 0.07415862 0.03538287
 0.01308682 0.04338037 0.0315053  0.02859712]
ene_total = [2.98353072 5.7890883  2.95330941 1.35079969 6.60483527 3.5266156
 1.5604156  3.97716561 3.26691924 2.98718119]
ti_comp = [0.30116936 0.28752245 0.30009808 0.30400477 0.28497028 0.28178895
 0.30447482 0.30547464 0.27545826 0.28132541]
ti_coms = [0.06738721 0.08103412 0.06845849 0.0645518  0.08358629 0.08676762
 0.06408176 0.06308193 0.09309831 0.08723116]
t_total = [29.49995804 29.49995804 29.49995804 29.49995804 29.49995804 29.49995804
 29.49995804 29.49995804 29.49995804 29.49995804]
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [1.96196272e-05 2.00260269e-04 1.88338590e-05 7.65323874e-07
 3.13881583e-04 3.48667396e-05 1.51105132e-06 5.46776000e-05
 2.57584149e-05 1.84683789e-05]
ene_total = [0.54095026 0.66464151 0.5494621  0.51674598 0.69416402 0.6972951
 0.51304332 0.50929606 0.74723805 0.69969276]
optimize_network iter = 0 obj = 6.132529161591935
eta = 0.690697585128414
freqs = [5.06955673e+07 1.11682299e+08 5.00689748e+07 1.71395055e+07
 1.30116415e+08 6.27825752e+07 2.14908014e+07 7.10048625e+07
 5.71870607e+07 5.08256882e+07]
eta_min = 0.6906975851284028	eta_max = 0.6906975851284034
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 0.050678425323249086	eta = 0.9090909090909091
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 20.72563143256374	eta = 0.002222913974819694
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 2.1660187348301907	eta = 0.02127003566846808
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 2.101859368191834	eta = 0.021919304614581324
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 2.101838286505997	eta = 0.02191952446779107
eta = 0.02191952446779107
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [2.02550289e-04 2.06745903e-03 1.94438128e-04 7.90109671e-06
 3.24046959e-03 3.59959346e-04 1.55998826e-05 5.64483901e-04
 2.65926276e-04 1.90664962e-04]
ene_total = [0.17569726 0.2574448  0.17820355 0.16359333 0.29359605 0.22873723
 0.16259842 0.17396099 0.24238129 0.22562536]
ti_comp = [0.30116936 0.28752245 0.30009808 0.30400477 0.28497028 0.28178895
 0.30447482 0.30547464 0.27545826 0.28132541]
ti_coms = [0.06738721 0.08103412 0.06845849 0.0645518  0.08358629 0.08676762
 0.06408176 0.06308193 0.09309831 0.08723116]
t_total = [29.49995804 29.49995804 29.49995804 29.49995804 29.49995804 29.49995804
 29.49995804 29.49995804 29.49995804 29.49995804]
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [1.96196272e-05 2.00260269e-04 1.88338590e-05 7.65323874e-07
 3.13881583e-04 3.48667396e-05 1.51105132e-06 5.46776000e-05
 2.57584149e-05 1.84683789e-05]
ene_total = [0.54095026 0.66464151 0.5494621  0.51674598 0.69416402 0.6972951
 0.51304332 0.50929606 0.74723805 0.69969276]
optimize_network iter = 1 obj = 6.132529161591719
eta = 0.6906975851284028
freqs = [5.06955673e+07 1.11682299e+08 5.00689748e+07 1.71395055e+07
 1.30116415e+08 6.27825752e+07 2.14908014e+07 7.10048625e+07
 5.71870607e+07 5.08256882e+07]
Done!
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [1.94295065e-05 1.98319681e-04 1.86513526e-05 7.57907632e-07
 3.10839966e-04 3.45288693e-05 1.49640873e-06 5.41477560e-05
 2.55088073e-05 1.82894141e-05]
ene_total = [0.00675815 0.00830173 0.0068645  0.00645594 0.00866947 0.00871129
 0.00640967 0.00636234 0.00933534 0.00874141]
At round 10 energy consumption: 0.07660983999268792
At round 10 eta: 0.6906975851284028
At round 10 a_n: 24.414598539591886
At round 10 local rounds: 12.11742183622639
At round 10 global rounds: 80.0418722784263
gradient difference: 0.4342661499977112
train() client id: f_00000-0-0 loss: 1.419197  [   32/  126]
train() client id: f_00000-0-1 loss: 1.495283  [   64/  126]
train() client id: f_00000-0-2 loss: 1.439249  [   96/  126]
train() client id: f_00000-1-0 loss: 1.410765  [   32/  126]
train() client id: f_00000-1-1 loss: 1.187538  [   64/  126]
train() client id: f_00000-1-2 loss: 1.250639  [   96/  126]
train() client id: f_00000-2-0 loss: 1.222377  [   32/  126]
train() client id: f_00000-2-1 loss: 1.244976  [   64/  126]
train() client id: f_00000-2-2 loss: 1.128931  [   96/  126]
train() client id: f_00000-3-0 loss: 1.214849  [   32/  126]
train() client id: f_00000-3-1 loss: 1.099823  [   64/  126]
train() client id: f_00000-3-2 loss: 0.980823  [   96/  126]
train() client id: f_00000-4-0 loss: 1.046241  [   32/  126]
train() client id: f_00000-4-1 loss: 0.999203  [   64/  126]
train() client id: f_00000-4-2 loss: 0.998124  [   96/  126]
train() client id: f_00000-5-0 loss: 1.033973  [   32/  126]
train() client id: f_00000-5-1 loss: 0.971746  [   64/  126]
train() client id: f_00000-5-2 loss: 0.991264  [   96/  126]
train() client id: f_00000-6-0 loss: 0.931730  [   32/  126]
train() client id: f_00000-6-1 loss: 0.973418  [   64/  126]
train() client id: f_00000-6-2 loss: 0.951360  [   96/  126]
train() client id: f_00000-7-0 loss: 0.930029  [   32/  126]
train() client id: f_00000-7-1 loss: 0.892975  [   64/  126]
train() client id: f_00000-7-2 loss: 0.935848  [   96/  126]
train() client id: f_00000-8-0 loss: 0.908716  [   32/  126]
train() client id: f_00000-8-1 loss: 0.950169  [   64/  126]
train() client id: f_00000-8-2 loss: 0.871297  [   96/  126]
train() client id: f_00000-9-0 loss: 0.914962  [   32/  126]
train() client id: f_00000-9-1 loss: 0.893243  [   64/  126]
train() client id: f_00000-9-2 loss: 0.867204  [   96/  126]
train() client id: f_00000-10-0 loss: 0.889795  [   32/  126]
train() client id: f_00000-10-1 loss: 0.844851  [   64/  126]
train() client id: f_00000-10-2 loss: 0.840501  [   96/  126]
train() client id: f_00000-11-0 loss: 0.881345  [   32/  126]
train() client id: f_00000-11-1 loss: 0.911585  [   64/  126]
train() client id: f_00000-11-2 loss: 0.861559  [   96/  126]
train() client id: f_00001-0-0 loss: 0.557874  [   32/  265]
train() client id: f_00001-0-1 loss: 0.442043  [   64/  265]
train() client id: f_00001-0-2 loss: 0.438194  [   96/  265]
train() client id: f_00001-0-3 loss: 0.507277  [  128/  265]
train() client id: f_00001-0-4 loss: 0.461868  [  160/  265]
train() client id: f_00001-0-5 loss: 0.560069  [  192/  265]
train() client id: f_00001-0-6 loss: 0.576939  [  224/  265]
train() client id: f_00001-0-7 loss: 0.561676  [  256/  265]
train() client id: f_00001-1-0 loss: 0.590582  [   32/  265]
train() client id: f_00001-1-1 loss: 0.486082  [   64/  265]
train() client id: f_00001-1-2 loss: 0.439704  [   96/  265]
train() client id: f_00001-1-3 loss: 0.534329  [  128/  265]
train() client id: f_00001-1-4 loss: 0.461027  [  160/  265]
train() client id: f_00001-1-5 loss: 0.483960  [  192/  265]
train() client id: f_00001-1-6 loss: 0.472602  [  224/  265]
train() client id: f_00001-1-7 loss: 0.458528  [  256/  265]
train() client id: f_00001-2-0 loss: 0.522744  [   32/  265]
train() client id: f_00001-2-1 loss: 0.402068  [   64/  265]
train() client id: f_00001-2-2 loss: 0.458937  [   96/  265]
train() client id: f_00001-2-3 loss: 0.461955  [  128/  265]
train() client id: f_00001-2-4 loss: 0.416883  [  160/  265]
train() client id: f_00001-2-5 loss: 0.522277  [  192/  265]
train() client id: f_00001-2-6 loss: 0.550894  [  224/  265]
train() client id: f_00001-2-7 loss: 0.454711  [  256/  265]
train() client id: f_00001-3-0 loss: 0.564934  [   32/  265]
train() client id: f_00001-3-1 loss: 0.474058  [   64/  265]
train() client id: f_00001-3-2 loss: 0.397087  [   96/  265]
train() client id: f_00001-3-3 loss: 0.385513  [  128/  265]
train() client id: f_00001-3-4 loss: 0.492878  [  160/  265]
train() client id: f_00001-3-5 loss: 0.530667  [  192/  265]
train() client id: f_00001-3-6 loss: 0.522269  [  224/  265]
train() client id: f_00001-3-7 loss: 0.431763  [  256/  265]
train() client id: f_00001-4-0 loss: 0.399251  [   32/  265]
train() client id: f_00001-4-1 loss: 0.391802  [   64/  265]
train() client id: f_00001-4-2 loss: 0.595654  [   96/  265]
train() client id: f_00001-4-3 loss: 0.491971  [  128/  265]
train() client id: f_00001-4-4 loss: 0.395686  [  160/  265]
train() client id: f_00001-4-5 loss: 0.595195  [  192/  265]
train() client id: f_00001-4-6 loss: 0.413135  [  224/  265]
train() client id: f_00001-4-7 loss: 0.474935  [  256/  265]
train() client id: f_00001-5-0 loss: 0.458627  [   32/  265]
train() client id: f_00001-5-1 loss: 0.535415  [   64/  265]
train() client id: f_00001-5-2 loss: 0.446507  [   96/  265]
train() client id: f_00001-5-3 loss: 0.393933  [  128/  265]
train() client id: f_00001-5-4 loss: 0.436877  [  160/  265]
train() client id: f_00001-5-5 loss: 0.453771  [  192/  265]
train() client id: f_00001-5-6 loss: 0.541771  [  224/  265]
train() client id: f_00001-5-7 loss: 0.495080  [  256/  265]
train() client id: f_00001-6-0 loss: 0.456918  [   32/  265]
train() client id: f_00001-6-1 loss: 0.492915  [   64/  265]
train() client id: f_00001-6-2 loss: 0.505969  [   96/  265]
train() client id: f_00001-6-3 loss: 0.452596  [  128/  265]
train() client id: f_00001-6-4 loss: 0.487986  [  160/  265]
train() client id: f_00001-6-5 loss: 0.389087  [  192/  265]
train() client id: f_00001-6-6 loss: 0.491255  [  224/  265]
train() client id: f_00001-6-7 loss: 0.464770  [  256/  265]
train() client id: f_00001-7-0 loss: 0.481983  [   32/  265]
train() client id: f_00001-7-1 loss: 0.508475  [   64/  265]
train() client id: f_00001-7-2 loss: 0.689383  [   96/  265]
train() client id: f_00001-7-3 loss: 0.378558  [  128/  265]
train() client id: f_00001-7-4 loss: 0.380396  [  160/  265]
train() client id: f_00001-7-5 loss: 0.434429  [  192/  265]
train() client id: f_00001-7-6 loss: 0.471559  [  224/  265]
train() client id: f_00001-7-7 loss: 0.383146  [  256/  265]
train() client id: f_00001-8-0 loss: 0.486495  [   32/  265]
train() client id: f_00001-8-1 loss: 0.744047  [   64/  265]
train() client id: f_00001-8-2 loss: 0.423826  [   96/  265]
train() client id: f_00001-8-3 loss: 0.374525  [  128/  265]
train() client id: f_00001-8-4 loss: 0.501834  [  160/  265]
train() client id: f_00001-8-5 loss: 0.377149  [  192/  265]
train() client id: f_00001-8-6 loss: 0.429087  [  224/  265]
train() client id: f_00001-8-7 loss: 0.372899  [  256/  265]
train() client id: f_00001-9-0 loss: 0.419741  [   32/  265]
train() client id: f_00001-9-1 loss: 0.490432  [   64/  265]
train() client id: f_00001-9-2 loss: 0.487771  [   96/  265]
train() client id: f_00001-9-3 loss: 0.483606  [  128/  265]
train() client id: f_00001-9-4 loss: 0.424922  [  160/  265]
train() client id: f_00001-9-5 loss: 0.589077  [  192/  265]
train() client id: f_00001-9-6 loss: 0.374693  [  224/  265]
train() client id: f_00001-9-7 loss: 0.442843  [  256/  265]
train() client id: f_00001-10-0 loss: 0.492071  [   32/  265]
train() client id: f_00001-10-1 loss: 0.488427  [   64/  265]
train() client id: f_00001-10-2 loss: 0.431769  [   96/  265]
train() client id: f_00001-10-3 loss: 0.442021  [  128/  265]
train() client id: f_00001-10-4 loss: 0.477027  [  160/  265]
train() client id: f_00001-10-5 loss: 0.422219  [  192/  265]
train() client id: f_00001-10-6 loss: 0.441544  [  224/  265]
train() client id: f_00001-10-7 loss: 0.438037  [  256/  265]
train() client id: f_00001-11-0 loss: 0.632361  [   32/  265]
train() client id: f_00001-11-1 loss: 0.493284  [   64/  265]
train() client id: f_00001-11-2 loss: 0.387436  [   96/  265]
train() client id: f_00001-11-3 loss: 0.438098  [  128/  265]
train() client id: f_00001-11-4 loss: 0.375107  [  160/  265]
train() client id: f_00001-11-5 loss: 0.362227  [  192/  265]
train() client id: f_00001-11-6 loss: 0.483170  [  224/  265]
train() client id: f_00001-11-7 loss: 0.478751  [  256/  265]
train() client id: f_00002-0-0 loss: 1.234096  [   32/  124]
train() client id: f_00002-0-1 loss: 1.193259  [   64/  124]
train() client id: f_00002-0-2 loss: 1.140252  [   96/  124]
train() client id: f_00002-1-0 loss: 1.254391  [   32/  124]
train() client id: f_00002-1-1 loss: 1.191239  [   64/  124]
train() client id: f_00002-1-2 loss: 1.096344  [   96/  124]
train() client id: f_00002-2-0 loss: 1.211198  [   32/  124]
train() client id: f_00002-2-1 loss: 1.046948  [   64/  124]
train() client id: f_00002-2-2 loss: 1.199613  [   96/  124]
train() client id: f_00002-3-0 loss: 1.089469  [   32/  124]
train() client id: f_00002-3-1 loss: 1.162297  [   64/  124]
train() client id: f_00002-3-2 loss: 1.078608  [   96/  124]
train() client id: f_00002-4-0 loss: 0.967854  [   32/  124]
train() client id: f_00002-4-1 loss: 1.172838  [   64/  124]
train() client id: f_00002-4-2 loss: 1.134689  [   96/  124]
train() client id: f_00002-5-0 loss: 1.077845  [   32/  124]
train() client id: f_00002-5-1 loss: 1.072540  [   64/  124]
train() client id: f_00002-5-2 loss: 0.994467  [   96/  124]
train() client id: f_00002-6-0 loss: 1.106106  [   32/  124]
train() client id: f_00002-6-1 loss: 1.092854  [   64/  124]
train() client id: f_00002-6-2 loss: 1.024039  [   96/  124]
train() client id: f_00002-7-0 loss: 1.025072  [   32/  124]
train() client id: f_00002-7-1 loss: 0.975399  [   64/  124]
train() client id: f_00002-7-2 loss: 1.073514  [   96/  124]
train() client id: f_00002-8-0 loss: 1.030838  [   32/  124]
train() client id: f_00002-8-1 loss: 1.041618  [   64/  124]
train() client id: f_00002-8-2 loss: 1.034777  [   96/  124]
train() client id: f_00002-9-0 loss: 1.114608  [   32/  124]
train() client id: f_00002-9-1 loss: 0.968696  [   64/  124]
train() client id: f_00002-9-2 loss: 1.044578  [   96/  124]
train() client id: f_00002-10-0 loss: 1.053119  [   32/  124]
train() client id: f_00002-10-1 loss: 1.134232  [   64/  124]
train() client id: f_00002-10-2 loss: 0.997429  [   96/  124]
train() client id: f_00002-11-0 loss: 0.848413  [   32/  124]
train() client id: f_00002-11-1 loss: 1.144597  [   64/  124]
train() client id: f_00002-11-2 loss: 1.016330  [   96/  124]
train() client id: f_00003-0-0 loss: 0.896793  [   32/   43]
train() client id: f_00003-1-0 loss: 0.853903  [   32/   43]
train() client id: f_00003-2-0 loss: 0.772370  [   32/   43]
train() client id: f_00003-3-0 loss: 0.903676  [   32/   43]
train() client id: f_00003-4-0 loss: 0.897158  [   32/   43]
train() client id: f_00003-5-0 loss: 0.811103  [   32/   43]
train() client id: f_00003-6-0 loss: 0.897290  [   32/   43]
train() client id: f_00003-7-0 loss: 0.793250  [   32/   43]
train() client id: f_00003-8-0 loss: 0.813434  [   32/   43]
train() client id: f_00003-9-0 loss: 0.978448  [   32/   43]
train() client id: f_00003-10-0 loss: 0.876503  [   32/   43]
train() client id: f_00003-11-0 loss: 0.917749  [   32/   43]
train() client id: f_00004-0-0 loss: 0.892872  [   32/  306]
train() client id: f_00004-0-1 loss: 0.963349  [   64/  306]
train() client id: f_00004-0-2 loss: 0.979462  [   96/  306]
train() client id: f_00004-0-3 loss: 0.907285  [  128/  306]
train() client id: f_00004-0-4 loss: 1.152514  [  160/  306]
train() client id: f_00004-0-5 loss: 1.027827  [  192/  306]
train() client id: f_00004-0-6 loss: 1.046327  [  224/  306]
train() client id: f_00004-0-7 loss: 0.883482  [  256/  306]
train() client id: f_00004-0-8 loss: 1.067939  [  288/  306]
train() client id: f_00004-1-0 loss: 1.067579  [   32/  306]
train() client id: f_00004-1-1 loss: 0.966012  [   64/  306]
train() client id: f_00004-1-2 loss: 0.932384  [   96/  306]
train() client id: f_00004-1-3 loss: 0.869801  [  128/  306]
train() client id: f_00004-1-4 loss: 0.886763  [  160/  306]
train() client id: f_00004-1-5 loss: 0.980184  [  192/  306]
train() client id: f_00004-1-6 loss: 1.023292  [  224/  306]
train() client id: f_00004-1-7 loss: 1.036245  [  256/  306]
train() client id: f_00004-1-8 loss: 1.026132  [  288/  306]
train() client id: f_00004-2-0 loss: 1.082844  [   32/  306]
train() client id: f_00004-2-1 loss: 0.946402  [   64/  306]
train() client id: f_00004-2-2 loss: 1.104720  [   96/  306]
train() client id: f_00004-2-3 loss: 0.938336  [  128/  306]
train() client id: f_00004-2-4 loss: 0.952067  [  160/  306]
train() client id: f_00004-2-5 loss: 0.857548  [  192/  306]
train() client id: f_00004-2-6 loss: 0.990548  [  224/  306]
train() client id: f_00004-2-7 loss: 0.986079  [  256/  306]
train() client id: f_00004-2-8 loss: 0.983827  [  288/  306]
train() client id: f_00004-3-0 loss: 1.017571  [   32/  306]
train() client id: f_00004-3-1 loss: 0.982869  [   64/  306]
train() client id: f_00004-3-2 loss: 1.177892  [   96/  306]
train() client id: f_00004-3-3 loss: 0.943324  [  128/  306]
train() client id: f_00004-3-4 loss: 0.887790  [  160/  306]
train() client id: f_00004-3-5 loss: 0.899945  [  192/  306]
train() client id: f_00004-3-6 loss: 1.020437  [  224/  306]
train() client id: f_00004-3-7 loss: 0.793930  [  256/  306]
train() client id: f_00004-3-8 loss: 0.982263  [  288/  306]
train() client id: f_00004-4-0 loss: 1.008428  [   32/  306]
train() client id: f_00004-4-1 loss: 0.960669  [   64/  306]
train() client id: f_00004-4-2 loss: 0.931117  [   96/  306]
train() client id: f_00004-4-3 loss: 1.015651  [  128/  306]
train() client id: f_00004-4-4 loss: 0.869855  [  160/  306]
train() client id: f_00004-4-5 loss: 0.904904  [  192/  306]
train() client id: f_00004-4-6 loss: 0.886865  [  224/  306]
train() client id: f_00004-4-7 loss: 1.016277  [  256/  306]
train() client id: f_00004-4-8 loss: 1.007655  [  288/  306]
train() client id: f_00004-5-0 loss: 0.985304  [   32/  306]
train() client id: f_00004-5-1 loss: 1.012443  [   64/  306]
train() client id: f_00004-5-2 loss: 1.027701  [   96/  306]
train() client id: f_00004-5-3 loss: 0.916232  [  128/  306]
train() client id: f_00004-5-4 loss: 0.956790  [  160/  306]
train() client id: f_00004-5-5 loss: 0.914683  [  192/  306]
train() client id: f_00004-5-6 loss: 0.892225  [  224/  306]
train() client id: f_00004-5-7 loss: 0.942914  [  256/  306]
train() client id: f_00004-5-8 loss: 0.981227  [  288/  306]
train() client id: f_00004-6-0 loss: 0.942461  [   32/  306]
train() client id: f_00004-6-1 loss: 0.908102  [   64/  306]
train() client id: f_00004-6-2 loss: 0.933055  [   96/  306]
train() client id: f_00004-6-3 loss: 0.943076  [  128/  306]
train() client id: f_00004-6-4 loss: 0.893418  [  160/  306]
train() client id: f_00004-6-5 loss: 0.950107  [  192/  306]
train() client id: f_00004-6-6 loss: 0.923880  [  224/  306]
train() client id: f_00004-6-7 loss: 0.903537  [  256/  306]
train() client id: f_00004-6-8 loss: 1.121805  [  288/  306]
train() client id: f_00004-7-0 loss: 0.969880  [   32/  306]
train() client id: f_00004-7-1 loss: 1.067891  [   64/  306]
train() client id: f_00004-7-2 loss: 0.880118  [   96/  306]
train() client id: f_00004-7-3 loss: 1.014377  [  128/  306]
train() client id: f_00004-7-4 loss: 0.922385  [  160/  306]
train() client id: f_00004-7-5 loss: 0.873721  [  192/  306]
train() client id: f_00004-7-6 loss: 0.917267  [  224/  306]
train() client id: f_00004-7-7 loss: 1.022986  [  256/  306]
train() client id: f_00004-7-8 loss: 0.873520  [  288/  306]
train() client id: f_00004-8-0 loss: 0.884340  [   32/  306]
train() client id: f_00004-8-1 loss: 0.972907  [   64/  306]
train() client id: f_00004-8-2 loss: 0.963627  [   96/  306]
train() client id: f_00004-8-3 loss: 0.917188  [  128/  306]
train() client id: f_00004-8-4 loss: 0.860996  [  160/  306]
train() client id: f_00004-8-5 loss: 0.823207  [  192/  306]
train() client id: f_00004-8-6 loss: 1.025711  [  224/  306]
train() client id: f_00004-8-7 loss: 1.018008  [  256/  306]
train() client id: f_00004-8-8 loss: 0.962337  [  288/  306]
train() client id: f_00004-9-0 loss: 0.851635  [   32/  306]
train() client id: f_00004-9-1 loss: 1.019430  [   64/  306]
train() client id: f_00004-9-2 loss: 1.019677  [   96/  306]
train() client id: f_00004-9-3 loss: 0.930552  [  128/  306]
train() client id: f_00004-9-4 loss: 0.986269  [  160/  306]
train() client id: f_00004-9-5 loss: 0.796572  [  192/  306]
train() client id: f_00004-9-6 loss: 1.082865  [  224/  306]
train() client id: f_00004-9-7 loss: 0.978955  [  256/  306]
train() client id: f_00004-9-8 loss: 0.850474  [  288/  306]
train() client id: f_00004-10-0 loss: 0.849392  [   32/  306]
train() client id: f_00004-10-1 loss: 0.959466  [   64/  306]
train() client id: f_00004-10-2 loss: 0.906465  [   96/  306]
train() client id: f_00004-10-3 loss: 0.870742  [  128/  306]
train() client id: f_00004-10-4 loss: 1.001593  [  160/  306]
train() client id: f_00004-10-5 loss: 0.899818  [  192/  306]
train() client id: f_00004-10-6 loss: 1.012990  [  224/  306]
train() client id: f_00004-10-7 loss: 0.916400  [  256/  306]
train() client id: f_00004-10-8 loss: 1.041582  [  288/  306]
train() client id: f_00004-11-0 loss: 1.039423  [   32/  306]
train() client id: f_00004-11-1 loss: 0.973835  [   64/  306]
train() client id: f_00004-11-2 loss: 0.885132  [   96/  306]
train() client id: f_00004-11-3 loss: 1.016455  [  128/  306]
train() client id: f_00004-11-4 loss: 0.822797  [  160/  306]
train() client id: f_00004-11-5 loss: 0.779277  [  192/  306]
train() client id: f_00004-11-6 loss: 0.998940  [  224/  306]
train() client id: f_00004-11-7 loss: 0.930636  [  256/  306]
train() client id: f_00004-11-8 loss: 0.941344  [  288/  306]
train() client id: f_00005-0-0 loss: 0.709987  [   32/  146]
train() client id: f_00005-0-1 loss: 0.636265  [   64/  146]
train() client id: f_00005-0-2 loss: 0.452623  [   96/  146]
train() client id: f_00005-0-3 loss: 0.356547  [  128/  146]
train() client id: f_00005-1-0 loss: 0.584509  [   32/  146]
train() client id: f_00005-1-1 loss: 0.408859  [   64/  146]
train() client id: f_00005-1-2 loss: 0.534386  [   96/  146]
train() client id: f_00005-1-3 loss: 0.631378  [  128/  146]
train() client id: f_00005-2-0 loss: 0.563700  [   32/  146]
train() client id: f_00005-2-1 loss: 0.488630  [   64/  146]
train() client id: f_00005-2-2 loss: 0.649392  [   96/  146]
train() client id: f_00005-2-3 loss: 0.404934  [  128/  146]
train() client id: f_00005-3-0 loss: 0.495471  [   32/  146]
train() client id: f_00005-3-1 loss: 0.616507  [   64/  146]
train() client id: f_00005-3-2 loss: 0.451389  [   96/  146]
train() client id: f_00005-3-3 loss: 0.485720  [  128/  146]
train() client id: f_00005-4-0 loss: 0.474734  [   32/  146]
train() client id: f_00005-4-1 loss: 0.538284  [   64/  146]
train() client id: f_00005-4-2 loss: 0.607562  [   96/  146]
train() client id: f_00005-4-3 loss: 0.464018  [  128/  146]
train() client id: f_00005-5-0 loss: 0.662834  [   32/  146]
train() client id: f_00005-5-1 loss: 0.530456  [   64/  146]
train() client id: f_00005-5-2 loss: 0.511243  [   96/  146]
train() client id: f_00005-5-3 loss: 0.351362  [  128/  146]
train() client id: f_00005-6-0 loss: 0.531340  [   32/  146]
train() client id: f_00005-6-1 loss: 0.305595  [   64/  146]
train() client id: f_00005-6-2 loss: 0.573735  [   96/  146]
train() client id: f_00005-6-3 loss: 0.677557  [  128/  146]
train() client id: f_00005-7-0 loss: 0.577711  [   32/  146]
train() client id: f_00005-7-1 loss: 0.564600  [   64/  146]
train() client id: f_00005-7-2 loss: 0.470491  [   96/  146]
train() client id: f_00005-7-3 loss: 0.493780  [  128/  146]
train() client id: f_00005-8-0 loss: 0.443864  [   32/  146]
train() client id: f_00005-8-1 loss: 0.411365  [   64/  146]
train() client id: f_00005-8-2 loss: 0.731533  [   96/  146]
train() client id: f_00005-8-3 loss: 0.415319  [  128/  146]
train() client id: f_00005-9-0 loss: 0.667195  [   32/  146]
train() client id: f_00005-9-1 loss: 0.376823  [   64/  146]
train() client id: f_00005-9-2 loss: 0.481750  [   96/  146]
train() client id: f_00005-9-3 loss: 0.467809  [  128/  146]
train() client id: f_00005-10-0 loss: 0.531189  [   32/  146]
train() client id: f_00005-10-1 loss: 0.206741  [   64/  146]
train() client id: f_00005-10-2 loss: 0.495106  [   96/  146]
train() client id: f_00005-10-3 loss: 0.823074  [  128/  146]
train() client id: f_00005-11-0 loss: 0.707441  [   32/  146]
train() client id: f_00005-11-1 loss: 0.356530  [   64/  146]
train() client id: f_00005-11-2 loss: 0.453407  [   96/  146]
train() client id: f_00005-11-3 loss: 0.510825  [  128/  146]
train() client id: f_00006-0-0 loss: 0.677137  [   32/   54]
train() client id: f_00006-1-0 loss: 0.740763  [   32/   54]
train() client id: f_00006-2-0 loss: 0.742377  [   32/   54]
train() client id: f_00006-3-0 loss: 0.687925  [   32/   54]
train() client id: f_00006-4-0 loss: 0.687374  [   32/   54]
train() client id: f_00006-5-0 loss: 0.701789  [   32/   54]
train() client id: f_00006-6-0 loss: 0.701628  [   32/   54]
train() client id: f_00006-7-0 loss: 0.735930  [   32/   54]
train() client id: f_00006-8-0 loss: 0.700997  [   32/   54]
train() client id: f_00006-9-0 loss: 0.647459  [   32/   54]
train() client id: f_00006-10-0 loss: 0.727791  [   32/   54]
train() client id: f_00006-11-0 loss: 0.738361  [   32/   54]
train() client id: f_00007-0-0 loss: 0.749681  [   32/  179]
train() client id: f_00007-0-1 loss: 0.745546  [   64/  179]
train() client id: f_00007-0-2 loss: 0.622940  [   96/  179]
train() client id: f_00007-0-3 loss: 0.749661  [  128/  179]
train() client id: f_00007-0-4 loss: 0.661355  [  160/  179]
train() client id: f_00007-1-0 loss: 0.886717  [   32/  179]
train() client id: f_00007-1-1 loss: 0.626568  [   64/  179]
train() client id: f_00007-1-2 loss: 0.636180  [   96/  179]
train() client id: f_00007-1-3 loss: 0.660741  [  128/  179]
train() client id: f_00007-1-4 loss: 0.692754  [  160/  179]
train() client id: f_00007-2-0 loss: 0.619510  [   32/  179]
train() client id: f_00007-2-1 loss: 0.782483  [   64/  179]
train() client id: f_00007-2-2 loss: 0.699178  [   96/  179]
train() client id: f_00007-2-3 loss: 0.618705  [  128/  179]
train() client id: f_00007-2-4 loss: 0.661331  [  160/  179]
train() client id: f_00007-3-0 loss: 0.619601  [   32/  179]
train() client id: f_00007-3-1 loss: 0.560317  [   64/  179]
train() client id: f_00007-3-2 loss: 0.751614  [   96/  179]
train() client id: f_00007-3-3 loss: 0.655116  [  128/  179]
train() client id: f_00007-3-4 loss: 0.811592  [  160/  179]
train() client id: f_00007-4-0 loss: 0.708028  [   32/  179]
train() client id: f_00007-4-1 loss: 0.639238  [   64/  179]
train() client id: f_00007-4-2 loss: 0.681657  [   96/  179]
train() client id: f_00007-4-3 loss: 0.595767  [  128/  179]
train() client id: f_00007-4-4 loss: 0.618816  [  160/  179]
train() client id: f_00007-5-0 loss: 0.702652  [   32/  179]
train() client id: f_00007-5-1 loss: 0.666887  [   64/  179]
train() client id: f_00007-5-2 loss: 0.563501  [   96/  179]
train() client id: f_00007-5-3 loss: 0.607044  [  128/  179]
train() client id: f_00007-5-4 loss: 0.751514  [  160/  179]
train() client id: f_00007-6-0 loss: 0.636621  [   32/  179]
train() client id: f_00007-6-1 loss: 0.754978  [   64/  179]
train() client id: f_00007-6-2 loss: 0.639722  [   96/  179]
train() client id: f_00007-6-3 loss: 0.684231  [  128/  179]
train() client id: f_00007-6-4 loss: 0.553238  [  160/  179]
train() client id: f_00007-7-0 loss: 0.553061  [   32/  179]
train() client id: f_00007-7-1 loss: 0.695494  [   64/  179]
train() client id: f_00007-7-2 loss: 0.632055  [   96/  179]
train() client id: f_00007-7-3 loss: 0.756851  [  128/  179]
train() client id: f_00007-7-4 loss: 0.700826  [  160/  179]
train() client id: f_00007-8-0 loss: 0.698745  [   32/  179]
train() client id: f_00007-8-1 loss: 0.638028  [   64/  179]
train() client id: f_00007-8-2 loss: 0.628665  [   96/  179]
train() client id: f_00007-8-3 loss: 0.758634  [  128/  179]
train() client id: f_00007-8-4 loss: 0.614873  [  160/  179]
train() client id: f_00007-9-0 loss: 0.681986  [   32/  179]
train() client id: f_00007-9-1 loss: 0.569865  [   64/  179]
train() client id: f_00007-9-2 loss: 0.692512  [   96/  179]
train() client id: f_00007-9-3 loss: 0.626083  [  128/  179]
train() client id: f_00007-9-4 loss: 0.627900  [  160/  179]
train() client id: f_00007-10-0 loss: 0.840406  [   32/  179]
train() client id: f_00007-10-1 loss: 0.551352  [   64/  179]
train() client id: f_00007-10-2 loss: 0.544183  [   96/  179]
train() client id: f_00007-10-3 loss: 0.619012  [  128/  179]
train() client id: f_00007-10-4 loss: 0.721735  [  160/  179]
train() client id: f_00007-11-0 loss: 0.620933  [   32/  179]
train() client id: f_00007-11-1 loss: 0.745520  [   64/  179]
train() client id: f_00007-11-2 loss: 0.804631  [   96/  179]
train() client id: f_00007-11-3 loss: 0.624233  [  128/  179]
train() client id: f_00007-11-4 loss: 0.548045  [  160/  179]
train() client id: f_00008-0-0 loss: 0.843147  [   32/  130]
train() client id: f_00008-0-1 loss: 0.804911  [   64/  130]
train() client id: f_00008-0-2 loss: 0.831260  [   96/  130]
train() client id: f_00008-0-3 loss: 0.754279  [  128/  130]
train() client id: f_00008-1-0 loss: 0.795183  [   32/  130]
train() client id: f_00008-1-1 loss: 0.790760  [   64/  130]
train() client id: f_00008-1-2 loss: 0.870554  [   96/  130]
train() client id: f_00008-1-3 loss: 0.769285  [  128/  130]
train() client id: f_00008-2-0 loss: 0.796534  [   32/  130]
train() client id: f_00008-2-1 loss: 0.973023  [   64/  130]
train() client id: f_00008-2-2 loss: 0.734684  [   96/  130]
train() client id: f_00008-2-3 loss: 0.726297  [  128/  130]
train() client id: f_00008-3-0 loss: 0.860230  [   32/  130]
train() client id: f_00008-3-1 loss: 0.700762  [   64/  130]
train() client id: f_00008-3-2 loss: 0.781730  [   96/  130]
train() client id: f_00008-3-3 loss: 0.886137  [  128/  130]
train() client id: f_00008-4-0 loss: 0.723725  [   32/  130]
train() client id: f_00008-4-1 loss: 0.855314  [   64/  130]
train() client id: f_00008-4-2 loss: 0.793121  [   96/  130]
train() client id: f_00008-4-3 loss: 0.833685  [  128/  130]
train() client id: f_00008-5-0 loss: 0.829780  [   32/  130]
train() client id: f_00008-5-1 loss: 0.729550  [   64/  130]
train() client id: f_00008-5-2 loss: 0.845245  [   96/  130]
train() client id: f_00008-5-3 loss: 0.792971  [  128/  130]
train() client id: f_00008-6-0 loss: 0.820658  [   32/  130]
train() client id: f_00008-6-1 loss: 0.848612  [   64/  130]
train() client id: f_00008-6-2 loss: 0.756884  [   96/  130]
train() client id: f_00008-6-3 loss: 0.806783  [  128/  130]
train() client id: f_00008-7-0 loss: 0.872472  [   32/  130]
train() client id: f_00008-7-1 loss: 0.691184  [   64/  130]
train() client id: f_00008-7-2 loss: 0.875181  [   96/  130]
train() client id: f_00008-7-3 loss: 0.795000  [  128/  130]
train() client id: f_00008-8-0 loss: 0.799433  [   32/  130]
train() client id: f_00008-8-1 loss: 0.791169  [   64/  130]
train() client id: f_00008-8-2 loss: 0.828317  [   96/  130]
train() client id: f_00008-8-3 loss: 0.807734  [  128/  130]
train() client id: f_00008-9-0 loss: 0.838192  [   32/  130]
train() client id: f_00008-9-1 loss: 0.795208  [   64/  130]
train() client id: f_00008-9-2 loss: 0.891549  [   96/  130]
train() client id: f_00008-9-3 loss: 0.707957  [  128/  130]
train() client id: f_00008-10-0 loss: 0.869217  [   32/  130]
train() client id: f_00008-10-1 loss: 0.855089  [   64/  130]
train() client id: f_00008-10-2 loss: 0.723030  [   96/  130]
train() client id: f_00008-10-3 loss: 0.752592  [  128/  130]
train() client id: f_00008-11-0 loss: 0.761619  [   32/  130]
train() client id: f_00008-11-1 loss: 0.755466  [   64/  130]
train() client id: f_00008-11-2 loss: 0.830712  [   96/  130]
train() client id: f_00008-11-3 loss: 0.853431  [  128/  130]
train() client id: f_00009-0-0 loss: 1.187659  [   32/  118]
train() client id: f_00009-0-1 loss: 1.206852  [   64/  118]
train() client id: f_00009-0-2 loss: 1.132154  [   96/  118]
train() client id: f_00009-1-0 loss: 1.079612  [   32/  118]
train() client id: f_00009-1-1 loss: 1.157683  [   64/  118]
train() client id: f_00009-1-2 loss: 1.080973  [   96/  118]
train() client id: f_00009-2-0 loss: 1.038471  [   32/  118]
train() client id: f_00009-2-1 loss: 1.039389  [   64/  118]
train() client id: f_00009-2-2 loss: 1.134927  [   96/  118]
train() client id: f_00009-3-0 loss: 0.998934  [   32/  118]
train() client id: f_00009-3-1 loss: 1.051248  [   64/  118]
train() client id: f_00009-3-2 loss: 1.087097  [   96/  118]
train() client id: f_00009-4-0 loss: 1.116867  [   32/  118]
train() client id: f_00009-4-1 loss: 0.855774  [   64/  118]
train() client id: f_00009-4-2 loss: 1.016814  [   96/  118]
train() client id: f_00009-5-0 loss: 1.039310  [   32/  118]
train() client id: f_00009-5-1 loss: 0.993270  [   64/  118]
train() client id: f_00009-5-2 loss: 0.954100  [   96/  118]
train() client id: f_00009-6-0 loss: 1.051947  [   32/  118]
train() client id: f_00009-6-1 loss: 0.955737  [   64/  118]
train() client id: f_00009-6-2 loss: 0.911012  [   96/  118]
train() client id: f_00009-7-0 loss: 0.842489  [   32/  118]
train() client id: f_00009-7-1 loss: 1.164773  [   64/  118]
train() client id: f_00009-7-2 loss: 0.856173  [   96/  118]
train() client id: f_00009-8-0 loss: 0.808235  [   32/  118]
train() client id: f_00009-8-1 loss: 0.953993  [   64/  118]
train() client id: f_00009-8-2 loss: 1.062641  [   96/  118]
train() client id: f_00009-9-0 loss: 0.968407  [   32/  118]
train() client id: f_00009-9-1 loss: 0.950375  [   64/  118]
train() client id: f_00009-9-2 loss: 0.869946  [   96/  118]
train() client id: f_00009-10-0 loss: 0.869451  [   32/  118]
train() client id: f_00009-10-1 loss: 0.975925  [   64/  118]
train() client id: f_00009-10-2 loss: 0.947139  [   96/  118]
train() client id: f_00009-11-0 loss: 0.997265  [   32/  118]
train() client id: f_00009-11-1 loss: 0.949907  [   64/  118]
train() client id: f_00009-11-2 loss: 0.860357  [   96/  118]
At round 10 accuracy: 0.6312997347480106
At round 10 training accuracy: 0.5774647887323944
At round 10 training loss: 0.860879052620112
update_location
xs = [ -3.9056584    4.20031788  70.00902392  18.81129433   0.97929623
   3.95640986 -32.44319194 -11.32485185  54.66397685  -2.06087855]
ys = [ 62.5879595   45.55583871   1.32061395 -32.45517586  24.35018685
   7.81415074  -2.62498432   0.82234798  17.56900603  -0.99851822]
dists_uav = [118.03604044 109.96807314 122.07787453 106.80450943 102.92662736
 100.38283783 105.16392559 100.64257809 115.31183954 100.02621786]
dists_bs = [204.71868644 221.06230981 300.3230776  283.90221521 231.65508607
 244.90101025 227.75662137 239.01340671 278.44220249 246.74565476]
uav_gains = [6.60612004e-11 7.88546405e-11 6.07267938e-11 8.48246225e-11
 9.30418280e-11 9.90488970e-11 8.81718085e-11 9.84110509e-11
 7.00330537e-11 9.99341172e-11]
bs_gains = [3.73295817e-11 3.01059902e-11 1.27657316e-11 1.49424392e-11
 2.64080865e-11 2.26006261e-11 2.76933346e-11 2.41942270e-11
 1.57774174e-11 2.21307151e-11]
Round 11
-------------------------------
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.53941363 19.90793381  9.39345483  3.35836813 22.96346205 11.07093228
  4.17536772 13.46928103  9.90106646  8.98947369]
obj_prev = 112.76875362310263
eta_min = 1.8259794231621306e-10	eta_max = 0.9205107351620903
af = 23.839837477165666	bf = 1.854557298226717	zeta = 26.223821224882236	eta = 0.909090909090909
af = 23.839837477165666	bf = 1.854557298226717	zeta = 45.394101441493795	eta = 0.5251747852723034
af = 23.839837477165666	bf = 1.854557298226717	zeta = 36.24067924685511	eta = 0.6578198304391449
af = 23.839837477165666	bf = 1.854557298226717	zeta = 34.59948643670541	eta = 0.6890228709254712
af = 23.839837477165666	bf = 1.854557298226717	zeta = 34.51842070411875	eta = 0.6906410255994443
af = 23.839837477165666	bf = 1.854557298226717	zeta = 34.51820885585075	eta = 0.6906452642639038
eta = 0.6906452642639038
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [0.03054215 0.06423548 0.03005736 0.01042312 0.0741738  0.03539011
 0.01308949 0.04338925 0.03151175 0.02860297]
ene_total = [2.94794512 5.70363107 2.91862696 1.33519898 6.50745954 3.47131979
 1.5419756  3.92315854 3.2297488  2.93914446]
ti_comp = [0.30504778 0.29281109 0.30391469 0.30820672 0.29034205 0.28721115
 0.30867006 0.30995051 0.27903667 0.28677106]
ti_coms = [0.0681094  0.08034609 0.06924248 0.06495046 0.08281513 0.08594602
 0.06448711 0.06320667 0.09412051 0.08638612]
t_total = [29.44995384 29.44995384 29.44995384 29.44995384 29.44995384 29.44995384
 29.44995384 29.44995384 29.44995384 29.44995384]
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [1.91356519e-05 1.93210162e-04 1.83750713e-05 7.45055334e-07
 3.02560183e-04 3.35832973e-05 1.47115905e-06 5.31424719e-05
 2.51174115e-05 1.77845446e-05]
ene_total = [0.53903666 0.6493481  0.54791909 0.51265458 0.677464   0.68094541
 0.50905514 0.50302767 0.7447912  0.68317183]
optimize_network iter = 0 obj = 6.04741367166706
eta = 0.6906452642639038
freqs = [5.00612622e+07 1.09687587e+08 4.94503204e+07 1.69092939e+07
 1.27735205e+08 6.16099244e+07 2.12030516e+07 6.99938362e+07
 5.64652429e+07 4.98707394e+07]
eta_min = 0.6906452642638832	eta_max = 0.6906452642639023
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 0.04826057494319857	eta = 0.909090909090909
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 20.44612752124682	eta = 0.0021457975307437047
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 2.1286929945616637	eta = 0.020610416842846153
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 2.067483129885282	eta = 0.02122060843649869
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 2.0674642356817103	eta = 0.021220802368025433
eta = 0.021220802368025433
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [1.99188226e-04 2.01117734e-03 1.91271134e-04 7.75548441e-06
 3.14943156e-03 3.49577711e-04 1.53136962e-05 5.53174501e-04
 2.61453995e-04 1.85124181e-04]
ene_total = [0.17486013 0.25058139 0.17748901 0.16220541 0.28513266 0.22310299
 0.16123818 0.17146061 0.24129522 0.22009864]
ti_comp = [0.30504778 0.29281109 0.30391469 0.30820672 0.29034205 0.28721115
 0.30867006 0.30995051 0.27903667 0.28677106]
ti_coms = [0.0681094  0.08034609 0.06924248 0.06495046 0.08281513 0.08594602
 0.06448711 0.06320667 0.09412051 0.08638612]
t_total = [29.44995384 29.44995384 29.44995384 29.44995384 29.44995384 29.44995384
 29.44995384 29.44995384 29.44995384 29.44995384]
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [1.91356519e-05 1.93210162e-04 1.83750713e-05 7.45055334e-07
 3.02560183e-04 3.35832973e-05 1.47115905e-06 5.31424719e-05
 2.51174115e-05 1.77845446e-05]
ene_total = [0.53903666 0.6493481  0.54791909 0.51265458 0.677464   0.68094541
 0.50905514 0.50302767 0.7447912  0.68317183]
optimize_network iter = 1 obj = 6.047413671666661
eta = 0.6906452642638832
freqs = [5.00612622e+07 1.09687587e+08 4.94503204e+07 1.69092939e+07
 1.27735205e+08 6.16099244e+07 2.12030516e+07 6.99938362e+07
 5.64652429e+07 4.98707394e+07]
Done!
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [1.89463426e-05 1.91298731e-04 1.81932864e-05 7.37684489e-07
 2.99566950e-04 3.32510571e-05 1.45660485e-06 5.26167325e-05
 2.48689246e-05 1.76086018e-05]
ene_total = [0.00682989 0.00822591 0.00694244 0.00649578 0.00858108 0.00862785
 0.00645017 0.00637328 0.00943692 0.00865622]
At round 11 energy consumption: 0.0766195440375612
At round 11 eta: 0.6906452642638832
At round 11 a_n: 24.072052692622567
At round 11 local rounds: 12.119902394804324
At round 11 global rounds: 78.921043447086
gradient difference: 0.4298335015773773
train() client id: f_00000-0-0 loss: 1.434337  [   32/  126]
train() client id: f_00000-0-1 loss: 1.418806  [   64/  126]
train() client id: f_00000-0-2 loss: 1.386238  [   96/  126]
train() client id: f_00000-1-0 loss: 1.437291  [   32/  126]
train() client id: f_00000-1-1 loss: 1.121644  [   64/  126]
train() client id: f_00000-1-2 loss: 1.196577  [   96/  126]
train() client id: f_00000-2-0 loss: 1.034181  [   32/  126]
train() client id: f_00000-2-1 loss: 1.192015  [   64/  126]
train() client id: f_00000-2-2 loss: 1.202297  [   96/  126]
train() client id: f_00000-3-0 loss: 0.953404  [   32/  126]
train() client id: f_00000-3-1 loss: 1.109443  [   64/  126]
train() client id: f_00000-3-2 loss: 1.071506  [   96/  126]
train() client id: f_00000-4-0 loss: 1.071267  [   32/  126]
train() client id: f_00000-4-1 loss: 1.030357  [   64/  126]
train() client id: f_00000-4-2 loss: 0.968187  [   96/  126]
train() client id: f_00000-5-0 loss: 1.038253  [   32/  126]
train() client id: f_00000-5-1 loss: 1.023582  [   64/  126]
train() client id: f_00000-5-2 loss: 0.985091  [   96/  126]
train() client id: f_00000-6-0 loss: 0.972202  [   32/  126]
train() client id: f_00000-6-1 loss: 0.932874  [   64/  126]
train() client id: f_00000-6-2 loss: 0.939137  [   96/  126]
train() client id: f_00000-7-0 loss: 0.951051  [   32/  126]
train() client id: f_00000-7-1 loss: 0.995838  [   64/  126]
train() client id: f_00000-7-2 loss: 0.937629  [   96/  126]
train() client id: f_00000-8-0 loss: 0.910412  [   32/  126]
train() client id: f_00000-8-1 loss: 0.996960  [   64/  126]
train() client id: f_00000-8-2 loss: 0.968571  [   96/  126]
train() client id: f_00000-9-0 loss: 0.914322  [   32/  126]
train() client id: f_00000-9-1 loss: 0.983603  [   64/  126]
train() client id: f_00000-9-2 loss: 0.945021  [   96/  126]
train() client id: f_00000-10-0 loss: 0.895408  [   32/  126]
train() client id: f_00000-10-1 loss: 0.982299  [   64/  126]
train() client id: f_00000-10-2 loss: 0.922041  [   96/  126]
train() client id: f_00000-11-0 loss: 0.914383  [   32/  126]
train() client id: f_00000-11-1 loss: 0.928211  [   64/  126]
train() client id: f_00000-11-2 loss: 0.875649  [   96/  126]
train() client id: f_00001-0-0 loss: 0.459554  [   32/  265]
train() client id: f_00001-0-1 loss: 0.545631  [   64/  265]
train() client id: f_00001-0-2 loss: 0.564253  [   96/  265]
train() client id: f_00001-0-3 loss: 0.444519  [  128/  265]
train() client id: f_00001-0-4 loss: 0.404602  [  160/  265]
train() client id: f_00001-0-5 loss: 0.557529  [  192/  265]
train() client id: f_00001-0-6 loss: 0.472139  [  224/  265]
train() client id: f_00001-0-7 loss: 0.544195  [  256/  265]
train() client id: f_00001-1-0 loss: 0.522311  [   32/  265]
train() client id: f_00001-1-1 loss: 0.637372  [   64/  265]
train() client id: f_00001-1-2 loss: 0.449794  [   96/  265]
train() client id: f_00001-1-3 loss: 0.391462  [  128/  265]
train() client id: f_00001-1-4 loss: 0.431184  [  160/  265]
train() client id: f_00001-1-5 loss: 0.413773  [  192/  265]
train() client id: f_00001-1-6 loss: 0.620318  [  224/  265]
train() client id: f_00001-1-7 loss: 0.416470  [  256/  265]
train() client id: f_00001-2-0 loss: 0.618582  [   32/  265]
train() client id: f_00001-2-1 loss: 0.435392  [   64/  265]
train() client id: f_00001-2-2 loss: 0.467572  [   96/  265]
train() client id: f_00001-2-3 loss: 0.447969  [  128/  265]
train() client id: f_00001-2-4 loss: 0.433681  [  160/  265]
train() client id: f_00001-2-5 loss: 0.421157  [  192/  265]
train() client id: f_00001-2-6 loss: 0.455223  [  224/  265]
train() client id: f_00001-2-7 loss: 0.565539  [  256/  265]
train() client id: f_00001-3-0 loss: 0.430321  [   32/  265]
train() client id: f_00001-3-1 loss: 0.573070  [   64/  265]
train() client id: f_00001-3-2 loss: 0.408829  [   96/  265]
train() client id: f_00001-3-3 loss: 0.397332  [  128/  265]
train() client id: f_00001-3-4 loss: 0.445056  [  160/  265]
train() client id: f_00001-3-5 loss: 0.393709  [  192/  265]
train() client id: f_00001-3-6 loss: 0.556097  [  224/  265]
train() client id: f_00001-3-7 loss: 0.507772  [  256/  265]
train() client id: f_00001-4-0 loss: 0.600193  [   32/  265]
train() client id: f_00001-4-1 loss: 0.469802  [   64/  265]
train() client id: f_00001-4-2 loss: 0.454934  [   96/  265]
train() client id: f_00001-4-3 loss: 0.380425  [  128/  265]
train() client id: f_00001-4-4 loss: 0.372519  [  160/  265]
train() client id: f_00001-4-5 loss: 0.497879  [  192/  265]
train() client id: f_00001-4-6 loss: 0.426363  [  224/  265]
train() client id: f_00001-4-7 loss: 0.514129  [  256/  265]
train() client id: f_00001-5-0 loss: 0.437528  [   32/  265]
train() client id: f_00001-5-1 loss: 0.369636  [   64/  265]
train() client id: f_00001-5-2 loss: 0.466700  [   96/  265]
train() client id: f_00001-5-3 loss: 0.422238  [  128/  265]
train() client id: f_00001-5-4 loss: 0.525747  [  160/  265]
train() client id: f_00001-5-5 loss: 0.559725  [  192/  265]
train() client id: f_00001-5-6 loss: 0.434962  [  224/  265]
train() client id: f_00001-5-7 loss: 0.474117  [  256/  265]
train() client id: f_00001-6-0 loss: 0.430336  [   32/  265]
train() client id: f_00001-6-1 loss: 0.452294  [   64/  265]
train() client id: f_00001-6-2 loss: 0.385069  [   96/  265]
train() client id: f_00001-6-3 loss: 0.346945  [  128/  265]
train() client id: f_00001-6-4 loss: 0.446128  [  160/  265]
train() client id: f_00001-6-5 loss: 0.580280  [  192/  265]
train() client id: f_00001-6-6 loss: 0.485583  [  224/  265]
train() client id: f_00001-6-7 loss: 0.475790  [  256/  265]
train() client id: f_00001-7-0 loss: 0.473239  [   32/  265]
train() client id: f_00001-7-1 loss: 0.483188  [   64/  265]
train() client id: f_00001-7-2 loss: 0.373849  [   96/  265]
train() client id: f_00001-7-3 loss: 0.497968  [  128/  265]
train() client id: f_00001-7-4 loss: 0.370957  [  160/  265]
train() client id: f_00001-7-5 loss: 0.451426  [  192/  265]
train() client id: f_00001-7-6 loss: 0.368301  [  224/  265]
train() client id: f_00001-7-7 loss: 0.512446  [  256/  265]
train() client id: f_00001-8-0 loss: 0.423089  [   32/  265]
train() client id: f_00001-8-1 loss: 0.422133  [   64/  265]
train() client id: f_00001-8-2 loss: 0.363038  [   96/  265]
train() client id: f_00001-8-3 loss: 0.404090  [  128/  265]
train() client id: f_00001-8-4 loss: 0.436870  [  160/  265]
train() client id: f_00001-8-5 loss: 0.552379  [  192/  265]
train() client id: f_00001-8-6 loss: 0.514094  [  224/  265]
train() client id: f_00001-8-7 loss: 0.454347  [  256/  265]
train() client id: f_00001-9-0 loss: 0.361546  [   32/  265]
train() client id: f_00001-9-1 loss: 0.382144  [   64/  265]
train() client id: f_00001-9-2 loss: 0.361922  [   96/  265]
train() client id: f_00001-9-3 loss: 0.422598  [  128/  265]
train() client id: f_00001-9-4 loss: 0.434436  [  160/  265]
train() client id: f_00001-9-5 loss: 0.475367  [  192/  265]
train() client id: f_00001-9-6 loss: 0.375444  [  224/  265]
train() client id: f_00001-9-7 loss: 0.738631  [  256/  265]
train() client id: f_00001-10-0 loss: 0.346632  [   32/  265]
train() client id: f_00001-10-1 loss: 0.425087  [   64/  265]
train() client id: f_00001-10-2 loss: 0.368644  [   96/  265]
train() client id: f_00001-10-3 loss: 0.419795  [  128/  265]
train() client id: f_00001-10-4 loss: 0.416649  [  160/  265]
train() client id: f_00001-10-5 loss: 0.551891  [  192/  265]
train() client id: f_00001-10-6 loss: 0.633531  [  224/  265]
train() client id: f_00001-10-7 loss: 0.440177  [  256/  265]
train() client id: f_00001-11-0 loss: 0.403069  [   32/  265]
train() client id: f_00001-11-1 loss: 0.415155  [   64/  265]
train() client id: f_00001-11-2 loss: 0.560002  [   96/  265]
train() client id: f_00001-11-3 loss: 0.367041  [  128/  265]
train() client id: f_00001-11-4 loss: 0.347700  [  160/  265]
train() client id: f_00001-11-5 loss: 0.752184  [  192/  265]
train() client id: f_00001-11-6 loss: 0.359426  [  224/  265]
train() client id: f_00001-11-7 loss: 0.407165  [  256/  265]
train() client id: f_00002-0-0 loss: 1.270573  [   32/  124]
train() client id: f_00002-0-1 loss: 1.232041  [   64/  124]
train() client id: f_00002-0-2 loss: 1.192752  [   96/  124]
train() client id: f_00002-1-0 loss: 1.154025  [   32/  124]
train() client id: f_00002-1-1 loss: 1.177317  [   64/  124]
train() client id: f_00002-1-2 loss: 1.217468  [   96/  124]
train() client id: f_00002-2-0 loss: 1.164598  [   32/  124]
train() client id: f_00002-2-1 loss: 1.129468  [   64/  124]
train() client id: f_00002-2-2 loss: 1.078820  [   96/  124]
train() client id: f_00002-3-0 loss: 1.058123  [   32/  124]
train() client id: f_00002-3-1 loss: 1.110983  [   64/  124]
train() client id: f_00002-3-2 loss: 1.135693  [   96/  124]
train() client id: f_00002-4-0 loss: 1.156273  [   32/  124]
train() client id: f_00002-4-1 loss: 1.109624  [   64/  124]
train() client id: f_00002-4-2 loss: 1.103620  [   96/  124]
train() client id: f_00002-5-0 loss: 1.117841  [   32/  124]
train() client id: f_00002-5-1 loss: 1.072781  [   64/  124]
train() client id: f_00002-5-2 loss: 1.033002  [   96/  124]
train() client id: f_00002-6-0 loss: 1.022659  [   32/  124]
train() client id: f_00002-6-1 loss: 1.029359  [   64/  124]
train() client id: f_00002-6-2 loss: 1.020901  [   96/  124]
train() client id: f_00002-7-0 loss: 1.012016  [   32/  124]
train() client id: f_00002-7-1 loss: 1.175046  [   64/  124]
train() client id: f_00002-7-2 loss: 0.911500  [   96/  124]
train() client id: f_00002-8-0 loss: 1.017561  [   32/  124]
train() client id: f_00002-8-1 loss: 1.029678  [   64/  124]
train() client id: f_00002-8-2 loss: 1.005744  [   96/  124]
train() client id: f_00002-9-0 loss: 1.028533  [   32/  124]
train() client id: f_00002-9-1 loss: 0.987303  [   64/  124]
train() client id: f_00002-9-2 loss: 0.943628  [   96/  124]
train() client id: f_00002-10-0 loss: 1.089412  [   32/  124]
train() client id: f_00002-10-1 loss: 1.035218  [   64/  124]
train() client id: f_00002-10-2 loss: 0.894690  [   96/  124]
train() client id: f_00002-11-0 loss: 0.930187  [   32/  124]
train() client id: f_00002-11-1 loss: 1.048204  [   64/  124]
train() client id: f_00002-11-2 loss: 0.906132  [   96/  124]
train() client id: f_00003-0-0 loss: 0.823343  [   32/   43]
train() client id: f_00003-1-0 loss: 0.782302  [   32/   43]
train() client id: f_00003-2-0 loss: 0.804861  [   32/   43]
train() client id: f_00003-3-0 loss: 0.748897  [   32/   43]
train() client id: f_00003-4-0 loss: 0.655355  [   32/   43]
train() client id: f_00003-5-0 loss: 0.803657  [   32/   43]
train() client id: f_00003-6-0 loss: 0.824114  [   32/   43]
train() client id: f_00003-7-0 loss: 0.754678  [   32/   43]
train() client id: f_00003-8-0 loss: 0.873445  [   32/   43]
train() client id: f_00003-9-0 loss: 0.857104  [   32/   43]
train() client id: f_00003-10-0 loss: 0.647532  [   32/   43]
train() client id: f_00003-11-0 loss: 0.685972  [   32/   43]
train() client id: f_00004-0-0 loss: 0.942247  [   32/  306]
train() client id: f_00004-0-1 loss: 0.976539  [   64/  306]
train() client id: f_00004-0-2 loss: 0.921502  [   96/  306]
train() client id: f_00004-0-3 loss: 1.009239  [  128/  306]
train() client id: f_00004-0-4 loss: 0.638406  [  160/  306]
train() client id: f_00004-0-5 loss: 0.894637  [  192/  306]
train() client id: f_00004-0-6 loss: 0.730165  [  224/  306]
train() client id: f_00004-0-7 loss: 0.976225  [  256/  306]
train() client id: f_00004-0-8 loss: 1.045937  [  288/  306]
train() client id: f_00004-1-0 loss: 0.767690  [   32/  306]
train() client id: f_00004-1-1 loss: 0.936346  [   64/  306]
train() client id: f_00004-1-2 loss: 0.888427  [   96/  306]
train() client id: f_00004-1-3 loss: 0.995312  [  128/  306]
train() client id: f_00004-1-4 loss: 0.861847  [  160/  306]
train() client id: f_00004-1-5 loss: 0.901952  [  192/  306]
train() client id: f_00004-1-6 loss: 0.798996  [  224/  306]
train() client id: f_00004-1-7 loss: 1.009493  [  256/  306]
train() client id: f_00004-1-8 loss: 1.014224  [  288/  306]
train() client id: f_00004-2-0 loss: 0.850003  [   32/  306]
train() client id: f_00004-2-1 loss: 1.049618  [   64/  306]
train() client id: f_00004-2-2 loss: 0.810884  [   96/  306]
train() client id: f_00004-2-3 loss: 0.921265  [  128/  306]
train() client id: f_00004-2-4 loss: 0.851673  [  160/  306]
train() client id: f_00004-2-5 loss: 0.968853  [  192/  306]
train() client id: f_00004-2-6 loss: 0.881954  [  224/  306]
train() client id: f_00004-2-7 loss: 0.900316  [  256/  306]
train() client id: f_00004-2-8 loss: 0.911597  [  288/  306]
train() client id: f_00004-3-0 loss: 0.872924  [   32/  306]
train() client id: f_00004-3-1 loss: 0.863467  [   64/  306]
train() client id: f_00004-3-2 loss: 0.852786  [   96/  306]
train() client id: f_00004-3-3 loss: 0.933712  [  128/  306]
train() client id: f_00004-3-4 loss: 0.934525  [  160/  306]
train() client id: f_00004-3-5 loss: 0.986365  [  192/  306]
train() client id: f_00004-3-6 loss: 0.777173  [  224/  306]
train() client id: f_00004-3-7 loss: 0.924244  [  256/  306]
train() client id: f_00004-3-8 loss: 0.924552  [  288/  306]
train() client id: f_00004-4-0 loss: 0.839694  [   32/  306]
train() client id: f_00004-4-1 loss: 0.813900  [   64/  306]
train() client id: f_00004-4-2 loss: 0.879757  [   96/  306]
train() client id: f_00004-4-3 loss: 0.852701  [  128/  306]
train() client id: f_00004-4-4 loss: 0.967570  [  160/  306]
train() client id: f_00004-4-5 loss: 0.776734  [  192/  306]
train() client id: f_00004-4-6 loss: 0.973493  [  224/  306]
train() client id: f_00004-4-7 loss: 1.046548  [  256/  306]
train() client id: f_00004-4-8 loss: 0.981818  [  288/  306]
train() client id: f_00004-5-0 loss: 0.964386  [   32/  306]
train() client id: f_00004-5-1 loss: 0.886000  [   64/  306]
train() client id: f_00004-5-2 loss: 1.010225  [   96/  306]
train() client id: f_00004-5-3 loss: 0.787587  [  128/  306]
train() client id: f_00004-5-4 loss: 0.738559  [  160/  306]
train() client id: f_00004-5-5 loss: 0.958749  [  192/  306]
train() client id: f_00004-5-6 loss: 0.920918  [  224/  306]
train() client id: f_00004-5-7 loss: 0.865629  [  256/  306]
train() client id: f_00004-5-8 loss: 0.933797  [  288/  306]
train() client id: f_00004-6-0 loss: 0.911311  [   32/  306]
train() client id: f_00004-6-1 loss: 0.767335  [   64/  306]
train() client id: f_00004-6-2 loss: 0.841285  [   96/  306]
train() client id: f_00004-6-3 loss: 0.909787  [  128/  306]
train() client id: f_00004-6-4 loss: 0.905610  [  160/  306]
train() client id: f_00004-6-5 loss: 0.836314  [  192/  306]
train() client id: f_00004-6-6 loss: 0.994240  [  224/  306]
train() client id: f_00004-6-7 loss: 1.065415  [  256/  306]
train() client id: f_00004-6-8 loss: 0.832744  [  288/  306]
train() client id: f_00004-7-0 loss: 0.762098  [   32/  306]
train() client id: f_00004-7-1 loss: 0.842075  [   64/  306]
train() client id: f_00004-7-2 loss: 0.960789  [   96/  306]
train() client id: f_00004-7-3 loss: 0.787266  [  128/  306]
train() client id: f_00004-7-4 loss: 0.939973  [  160/  306]
train() client id: f_00004-7-5 loss: 0.783598  [  192/  306]
train() client id: f_00004-7-6 loss: 1.023192  [  224/  306]
train() client id: f_00004-7-7 loss: 1.000144  [  256/  306]
train() client id: f_00004-7-8 loss: 0.925061  [  288/  306]
train() client id: f_00004-8-0 loss: 0.779776  [   32/  306]
train() client id: f_00004-8-1 loss: 0.938602  [   64/  306]
train() client id: f_00004-8-2 loss: 0.944984  [   96/  306]
train() client id: f_00004-8-3 loss: 0.826600  [  128/  306]
train() client id: f_00004-8-4 loss: 0.879036  [  160/  306]
train() client id: f_00004-8-5 loss: 0.788262  [  192/  306]
train() client id: f_00004-8-6 loss: 1.028049  [  224/  306]
train() client id: f_00004-8-7 loss: 0.944754  [  256/  306]
train() client id: f_00004-8-8 loss: 0.888280  [  288/  306]
train() client id: f_00004-9-0 loss: 0.920607  [   32/  306]
train() client id: f_00004-9-1 loss: 0.869956  [   64/  306]
train() client id: f_00004-9-2 loss: 0.896657  [   96/  306]
train() client id: f_00004-9-3 loss: 0.931994  [  128/  306]
train() client id: f_00004-9-4 loss: 0.840073  [  160/  306]
train() client id: f_00004-9-5 loss: 0.794180  [  192/  306]
train() client id: f_00004-9-6 loss: 0.977098  [  224/  306]
train() client id: f_00004-9-7 loss: 1.008293  [  256/  306]
train() client id: f_00004-9-8 loss: 0.790187  [  288/  306]
train() client id: f_00004-10-0 loss: 0.977379  [   32/  306]
train() client id: f_00004-10-1 loss: 0.807893  [   64/  306]
train() client id: f_00004-10-2 loss: 0.933541  [   96/  306]
train() client id: f_00004-10-3 loss: 0.885310  [  128/  306]
train() client id: f_00004-10-4 loss: 0.801719  [  160/  306]
train() client id: f_00004-10-5 loss: 0.982823  [  192/  306]
train() client id: f_00004-10-6 loss: 0.909712  [  224/  306]
train() client id: f_00004-10-7 loss: 0.926781  [  256/  306]
train() client id: f_00004-10-8 loss: 0.873551  [  288/  306]
train() client id: f_00004-11-0 loss: 0.856927  [   32/  306]
train() client id: f_00004-11-1 loss: 0.937186  [   64/  306]
train() client id: f_00004-11-2 loss: 0.833588  [   96/  306]
train() client id: f_00004-11-3 loss: 0.907693  [  128/  306]
train() client id: f_00004-11-4 loss: 0.889482  [  160/  306]
train() client id: f_00004-11-5 loss: 0.804835  [  192/  306]
train() client id: f_00004-11-6 loss: 0.847081  [  224/  306]
train() client id: f_00004-11-7 loss: 0.941663  [  256/  306]
train() client id: f_00004-11-8 loss: 1.016157  [  288/  306]
train() client id: f_00005-0-0 loss: 0.783675  [   32/  146]
train() client id: f_00005-0-1 loss: 0.771930  [   64/  146]
train() client id: f_00005-0-2 loss: 0.704674  [   96/  146]
train() client id: f_00005-0-3 loss: 0.601004  [  128/  146]
train() client id: f_00005-1-0 loss: 0.780275  [   32/  146]
train() client id: f_00005-1-1 loss: 0.648941  [   64/  146]
train() client id: f_00005-1-2 loss: 0.615595  [   96/  146]
train() client id: f_00005-1-3 loss: 0.732134  [  128/  146]
train() client id: f_00005-2-0 loss: 0.592396  [   32/  146]
train() client id: f_00005-2-1 loss: 0.891006  [   64/  146]
train() client id: f_00005-2-2 loss: 0.741664  [   96/  146]
train() client id: f_00005-2-3 loss: 0.636036  [  128/  146]
train() client id: f_00005-3-0 loss: 0.768656  [   32/  146]
train() client id: f_00005-3-1 loss: 0.620090  [   64/  146]
train() client id: f_00005-3-2 loss: 0.706607  [   96/  146]
train() client id: f_00005-3-3 loss: 0.738158  [  128/  146]
train() client id: f_00005-4-0 loss: 0.672127  [   32/  146]
train() client id: f_00005-4-1 loss: 0.728253  [   64/  146]
train() client id: f_00005-4-2 loss: 0.726135  [   96/  146]
train() client id: f_00005-4-3 loss: 0.703445  [  128/  146]
train() client id: f_00005-5-0 loss: 0.974937  [   32/  146]
train() client id: f_00005-5-1 loss: 0.723128  [   64/  146]
train() client id: f_00005-5-2 loss: 0.609419  [   96/  146]
train() client id: f_00005-5-3 loss: 0.492074  [  128/  146]
train() client id: f_00005-6-0 loss: 0.769398  [   32/  146]
train() client id: f_00005-6-1 loss: 0.695016  [   64/  146]
train() client id: f_00005-6-2 loss: 0.630593  [   96/  146]
train() client id: f_00005-6-3 loss: 0.556930  [  128/  146]
train() client id: f_00005-7-0 loss: 0.592992  [   32/  146]
train() client id: f_00005-7-1 loss: 0.641710  [   64/  146]
train() client id: f_00005-7-2 loss: 0.714100  [   96/  146]
train() client id: f_00005-7-3 loss: 0.974203  [  128/  146]
train() client id: f_00005-8-0 loss: 0.651607  [   32/  146]
train() client id: f_00005-8-1 loss: 0.779576  [   64/  146]
train() client id: f_00005-8-2 loss: 0.468089  [   96/  146]
train() client id: f_00005-8-3 loss: 0.684851  [  128/  146]
train() client id: f_00005-9-0 loss: 0.760991  [   32/  146]
train() client id: f_00005-9-1 loss: 0.945704  [   64/  146]
train() client id: f_00005-9-2 loss: 0.608790  [   96/  146]
train() client id: f_00005-9-3 loss: 0.455339  [  128/  146]
train() client id: f_00005-10-0 loss: 0.850914  [   32/  146]
train() client id: f_00005-10-1 loss: 0.751729  [   64/  146]
train() client id: f_00005-10-2 loss: 0.553577  [   96/  146]
train() client id: f_00005-10-3 loss: 0.532733  [  128/  146]
train() client id: f_00005-11-0 loss: 0.935126  [   32/  146]
train() client id: f_00005-11-1 loss: 0.591314  [   64/  146]
train() client id: f_00005-11-2 loss: 0.692867  [   96/  146]
train() client id: f_00005-11-3 loss: 0.571794  [  128/  146]
train() client id: f_00006-0-0 loss: 0.591256  [   32/   54]
train() client id: f_00006-1-0 loss: 0.624700  [   32/   54]
train() client id: f_00006-2-0 loss: 0.581949  [   32/   54]
train() client id: f_00006-3-0 loss: 0.626474  [   32/   54]
train() client id: f_00006-4-0 loss: 0.679825  [   32/   54]
train() client id: f_00006-5-0 loss: 0.661785  [   32/   54]
train() client id: f_00006-6-0 loss: 0.680453  [   32/   54]
train() client id: f_00006-7-0 loss: 0.577772  [   32/   54]
train() client id: f_00006-8-0 loss: 0.616731  [   32/   54]
train() client id: f_00006-9-0 loss: 0.660342  [   32/   54]
train() client id: f_00006-10-0 loss: 0.633120  [   32/   54]
train() client id: f_00006-11-0 loss: 0.572861  [   32/   54]
train() client id: f_00007-0-0 loss: 0.791816  [   32/  179]
train() client id: f_00007-0-1 loss: 0.757736  [   64/  179]
train() client id: f_00007-0-2 loss: 0.757139  [   96/  179]
train() client id: f_00007-0-3 loss: 0.667542  [  128/  179]
train() client id: f_00007-0-4 loss: 0.749091  [  160/  179]
train() client id: f_00007-1-0 loss: 0.793942  [   32/  179]
train() client id: f_00007-1-1 loss: 0.658585  [   64/  179]
train() client id: f_00007-1-2 loss: 0.798709  [   96/  179]
train() client id: f_00007-1-3 loss: 0.793219  [  128/  179]
train() client id: f_00007-1-4 loss: 0.667464  [  160/  179]
train() client id: f_00007-2-0 loss: 0.724175  [   32/  179]
train() client id: f_00007-2-1 loss: 0.823470  [   64/  179]
train() client id: f_00007-2-2 loss: 0.700725  [   96/  179]
train() client id: f_00007-2-3 loss: 0.778835  [  128/  179]
train() client id: f_00007-2-4 loss: 0.692465  [  160/  179]
train() client id: f_00007-3-0 loss: 0.721442  [   32/  179]
train() client id: f_00007-3-1 loss: 0.769604  [   64/  179]
train() client id: f_00007-3-2 loss: 0.768956  [   96/  179]
train() client id: f_00007-3-3 loss: 0.677313  [  128/  179]
train() client id: f_00007-3-4 loss: 0.753441  [  160/  179]
train() client id: f_00007-4-0 loss: 0.680853  [   32/  179]
train() client id: f_00007-4-1 loss: 0.733985  [   64/  179]
train() client id: f_00007-4-2 loss: 0.635600  [   96/  179]
train() client id: f_00007-4-3 loss: 0.709723  [  128/  179]
train() client id: f_00007-4-4 loss: 0.742639  [  160/  179]
train() client id: f_00007-5-0 loss: 0.762272  [   32/  179]
train() client id: f_00007-5-1 loss: 0.720912  [   64/  179]
train() client id: f_00007-5-2 loss: 0.748111  [   96/  179]
train() client id: f_00007-5-3 loss: 0.590887  [  128/  179]
train() client id: f_00007-5-4 loss: 0.735736  [  160/  179]
train() client id: f_00007-6-0 loss: 0.693736  [   32/  179]
train() client id: f_00007-6-1 loss: 0.743887  [   64/  179]
train() client id: f_00007-6-2 loss: 0.837366  [   96/  179]
train() client id: f_00007-6-3 loss: 0.687061  [  128/  179]
train() client id: f_00007-6-4 loss: 0.674483  [  160/  179]
train() client id: f_00007-7-0 loss: 0.697599  [   32/  179]
train() client id: f_00007-7-1 loss: 0.890988  [   64/  179]
train() client id: f_00007-7-2 loss: 0.684935  [   96/  179]
train() client id: f_00007-7-3 loss: 0.749711  [  128/  179]
train() client id: f_00007-7-4 loss: 0.600952  [  160/  179]
train() client id: f_00007-8-0 loss: 0.857036  [   32/  179]
train() client id: f_00007-8-1 loss: 0.738096  [   64/  179]
train() client id: f_00007-8-2 loss: 0.720088  [   96/  179]
train() client id: f_00007-8-3 loss: 0.680282  [  128/  179]
train() client id: f_00007-8-4 loss: 0.595521  [  160/  179]
train() client id: f_00007-9-0 loss: 0.736438  [   32/  179]
train() client id: f_00007-9-1 loss: 0.723172  [   64/  179]
train() client id: f_00007-9-2 loss: 0.671165  [   96/  179]
train() client id: f_00007-9-3 loss: 0.777535  [  128/  179]
train() client id: f_00007-9-4 loss: 0.689269  [  160/  179]
train() client id: f_00007-10-0 loss: 0.684429  [   32/  179]
train() client id: f_00007-10-1 loss: 0.675376  [   64/  179]
train() client id: f_00007-10-2 loss: 0.657429  [   96/  179]
train() client id: f_00007-10-3 loss: 0.581062  [  128/  179]
train() client id: f_00007-10-4 loss: 0.840732  [  160/  179]
train() client id: f_00007-11-0 loss: 0.670693  [   32/  179]
train() client id: f_00007-11-1 loss: 0.597425  [   64/  179]
train() client id: f_00007-11-2 loss: 0.782673  [   96/  179]
train() client id: f_00007-11-3 loss: 0.768375  [  128/  179]
train() client id: f_00007-11-4 loss: 0.754343  [  160/  179]
train() client id: f_00008-0-0 loss: 0.857865  [   32/  130]
train() client id: f_00008-0-1 loss: 0.709448  [   64/  130]
train() client id: f_00008-0-2 loss: 0.737592  [   96/  130]
train() client id: f_00008-0-3 loss: 0.725551  [  128/  130]
train() client id: f_00008-1-0 loss: 0.772642  [   32/  130]
train() client id: f_00008-1-1 loss: 0.803224  [   64/  130]
train() client id: f_00008-1-2 loss: 0.715189  [   96/  130]
train() client id: f_00008-1-3 loss: 0.714731  [  128/  130]
train() client id: f_00008-2-0 loss: 0.707032  [   32/  130]
train() client id: f_00008-2-1 loss: 0.833996  [   64/  130]
train() client id: f_00008-2-2 loss: 0.710848  [   96/  130]
train() client id: f_00008-2-3 loss: 0.776354  [  128/  130]
train() client id: f_00008-3-0 loss: 0.829415  [   32/  130]
train() client id: f_00008-3-1 loss: 0.716959  [   64/  130]
train() client id: f_00008-3-2 loss: 0.783462  [   96/  130]
train() client id: f_00008-3-3 loss: 0.678535  [  128/  130]
train() client id: f_00008-4-0 loss: 0.768429  [   32/  130]
train() client id: f_00008-4-1 loss: 0.663029  [   64/  130]
train() client id: f_00008-4-2 loss: 0.769150  [   96/  130]
train() client id: f_00008-4-3 loss: 0.785444  [  128/  130]
train() client id: f_00008-5-0 loss: 0.667167  [   32/  130]
train() client id: f_00008-5-1 loss: 0.797265  [   64/  130]
train() client id: f_00008-5-2 loss: 0.757479  [   96/  130]
train() client id: f_00008-5-3 loss: 0.777926  [  128/  130]
train() client id: f_00008-6-0 loss: 0.711520  [   32/  130]
train() client id: f_00008-6-1 loss: 0.872430  [   64/  130]
train() client id: f_00008-6-2 loss: 0.747125  [   96/  130]
train() client id: f_00008-6-3 loss: 0.682526  [  128/  130]
train() client id: f_00008-7-0 loss: 0.682815  [   32/  130]
train() client id: f_00008-7-1 loss: 0.720143  [   64/  130]
train() client id: f_00008-7-2 loss: 0.864930  [   96/  130]
train() client id: f_00008-7-3 loss: 0.751097  [  128/  130]
train() client id: f_00008-8-0 loss: 0.816766  [   32/  130]
train() client id: f_00008-8-1 loss: 0.717420  [   64/  130]
train() client id: f_00008-8-2 loss: 0.684057  [   96/  130]
train() client id: f_00008-8-3 loss: 0.800609  [  128/  130]
train() client id: f_00008-9-0 loss: 0.739385  [   32/  130]
train() client id: f_00008-9-1 loss: 0.806367  [   64/  130]
train() client id: f_00008-9-2 loss: 0.856873  [   96/  130]
train() client id: f_00008-9-3 loss: 0.591837  [  128/  130]
train() client id: f_00008-10-0 loss: 0.724196  [   32/  130]
train() client id: f_00008-10-1 loss: 0.692084  [   64/  130]
train() client id: f_00008-10-2 loss: 0.707681  [   96/  130]
train() client id: f_00008-10-3 loss: 0.883141  [  128/  130]
train() client id: f_00008-11-0 loss: 0.727570  [   32/  130]
train() client id: f_00008-11-1 loss: 0.828201  [   64/  130]
train() client id: f_00008-11-2 loss: 0.749008  [   96/  130]
train() client id: f_00008-11-3 loss: 0.710688  [  128/  130]
train() client id: f_00009-0-0 loss: 1.262836  [   32/  118]
train() client id: f_00009-0-1 loss: 1.052762  [   64/  118]
train() client id: f_00009-0-2 loss: 1.238118  [   96/  118]
train() client id: f_00009-1-0 loss: 1.223523  [   32/  118]
train() client id: f_00009-1-1 loss: 1.196599  [   64/  118]
train() client id: f_00009-1-2 loss: 1.080106  [   96/  118]
train() client id: f_00009-2-0 loss: 1.139606  [   32/  118]
train() client id: f_00009-2-1 loss: 1.156964  [   64/  118]
train() client id: f_00009-2-2 loss: 1.031751  [   96/  118]
train() client id: f_00009-3-0 loss: 1.088378  [   32/  118]
train() client id: f_00009-3-1 loss: 1.056888  [   64/  118]
train() client id: f_00009-3-2 loss: 1.052978  [   96/  118]
train() client id: f_00009-4-0 loss: 1.016998  [   32/  118]
train() client id: f_00009-4-1 loss: 1.014766  [   64/  118]
train() client id: f_00009-4-2 loss: 1.057904  [   96/  118]
train() client id: f_00009-5-0 loss: 1.024006  [   32/  118]
train() client id: f_00009-5-1 loss: 0.990178  [   64/  118]
train() client id: f_00009-5-2 loss: 1.099245  [   96/  118]
train() client id: f_00009-6-0 loss: 1.009805  [   32/  118]
train() client id: f_00009-6-1 loss: 0.979959  [   64/  118]
train() client id: f_00009-6-2 loss: 1.007726  [   96/  118]
train() client id: f_00009-7-0 loss: 0.937476  [   32/  118]
train() client id: f_00009-7-1 loss: 0.922134  [   64/  118]
train() client id: f_00009-7-2 loss: 1.079628  [   96/  118]
train() client id: f_00009-8-0 loss: 0.972945  [   32/  118]
train() client id: f_00009-8-1 loss: 0.986079  [   64/  118]
train() client id: f_00009-8-2 loss: 1.042074  [   96/  118]
train() client id: f_00009-9-0 loss: 1.053739  [   32/  118]
train() client id: f_00009-9-1 loss: 1.080401  [   64/  118]
train() client id: f_00009-9-2 loss: 0.849972  [   96/  118]
train() client id: f_00009-10-0 loss: 1.158706  [   32/  118]
train() client id: f_00009-10-1 loss: 0.963240  [   64/  118]
train() client id: f_00009-10-2 loss: 0.902320  [   96/  118]
train() client id: f_00009-11-0 loss: 0.912494  [   32/  118]
train() client id: f_00009-11-1 loss: 0.936464  [   64/  118]
train() client id: f_00009-11-2 loss: 1.036552  [   96/  118]
At round 11 accuracy: 0.6312997347480106
At round 11 training accuracy: 0.5761234071093226
At round 11 training loss: 0.8558555705416612
update_location
xs = [ -3.9056584    4.20031788  75.00902392  18.81129433   0.97929623
   3.95640986 -37.44319194 -16.32485185  59.66397685  -2.06087855]
ys = [ 67.5879595   50.55583871   1.32061395 -37.45517586  29.35018685
  12.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [120.76169275 112.13177738 125.01239015 108.42857093 104.22280215
 100.89527064 106.81237365 101.32707952 117.76442632 100.10124413]
dists_bs = [202.0163859  218.17218706 304.41590165 287.5761109  228.43506905
 241.51531283 224.66087888 235.6176078  282.58043084 243.20450851]
uav_gains = [6.23955507e-11 7.51052065e-11 5.72241493e-11 8.16837356e-11
 9.01758519e-11 9.77960164e-11 8.48090092e-11 9.67574158e-11
 6.64428547e-11 9.97469659e-11]
bs_gains = [3.87446385e-11 3.12360282e-11 1.22909527e-11 1.44140568e-11
 2.74636534e-11 2.34989796e-11 2.87751248e-11 2.51832865e-11
 1.51389674e-11 2.30448319e-11]
Round 12
-------------------------------
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.40743039 19.62699225  9.26366007  3.31236285 22.63944362 10.91364539
  4.11791779 13.28077427  9.76466884  8.86129599]
obj_prev = 111.18819146740302
eta_min = 1.3309903798194387e-10	eta_max = 0.9206473861818304
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 25.855891314518058	eta = 0.9090909090909091
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 44.77490344422677	eta = 0.5249672010962586
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 35.73954375539702	eta = 0.6576848294802706
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 34.11938254858583	eta = 0.6889150384535667
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 34.039315064938776	eta = 0.6905355085914164
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 34.039105597683566	eta = 0.6905397579562299
eta = 0.6905397579562299
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [0.03055476 0.064262   0.03006976 0.01042742 0.07420442 0.03540472
 0.0130949  0.04340716 0.03152475 0.02861478]
ene_total = [2.91251884 5.6185508  2.88405621 1.31991488 6.41048895 3.41632264
 1.52386453 3.86970026 3.19241569 2.89127279]
ti_comp = [0.30907982 0.29827603 0.3078889  0.3125449  0.295892   0.29281253
 0.31300078 0.31455258 0.28279738 0.29241128]
ti_coms = [0.06887363 0.07967742 0.07006456 0.06540855 0.08206146 0.08514092
 0.06495268 0.06340088 0.09515608 0.08554218]
t_total = [29.39994965 29.39994965 29.39994965 29.39994965 29.39994965 29.39994965
 29.39994965 29.39994965 29.39994965 29.39994965]
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [1.86627385e-05 1.86425773e-04 1.79259442e-05 7.25413394e-07
 2.91677471e-04 3.23507450e-05 1.43250282e-06 5.16627645e-05
 2.44841111e-05 1.71262716e-05]
ene_total = [0.53720059 0.6342901  0.54640715 0.5088514  0.66102208 0.66480406
 0.50536026 0.49719653 0.74209725 0.66674105]
optimize_network iter = 0 obj = 5.96397047673148
eta = 0.6905397579562299
freqs = [4.94285918e+07 1.07722361e+08 4.88321676e+07 1.66814724e+07
 1.25391054e+08 6.04562965e+07 2.09183144e+07 6.89982586e+07
 5.57373503e+07 4.89289881e+07]
eta_min = 0.6905397579563017	eta_max = 0.6905397579562269
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 0.04594401078389732	eta = 0.909090909090909
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 20.175396261636216	eta = 0.002070208782478131
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 2.0926044394311156	eta = 0.019959473345172857
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 2.034227485351597	eta = 0.020532257494100595
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 2.0342105771628316	eta = 0.02053242815651352
eta = 0.02053242815651352
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [1.95847449e-04 1.95635877e-03 1.88115503e-04 7.61251424e-06
 3.06087387e-03 3.39489881e-04 1.50327361e-05 5.42150909e-04
 2.56937143e-04 1.79723710e-04]
ene_total = [0.17408175 0.24390138 0.17681862 0.16093954 0.27690584 0.21759158
 0.16000151 0.16914252 0.24017663 0.21465121]
ti_comp = [0.30907982 0.29827603 0.3078889  0.3125449  0.295892   0.29281253
 0.31300078 0.31455258 0.28279738 0.29241128]
ti_coms = [0.06887363 0.07967742 0.07006456 0.06540855 0.08206146 0.08514092
 0.06495268 0.06340088 0.09515608 0.08554218]
t_total = [29.39994965 29.39994965 29.39994965 29.39994965 29.39994965 29.39994965
 29.39994965 29.39994965 29.39994965 29.39994965]
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [1.86627385e-05 1.86425773e-04 1.79259442e-05 7.25413394e-07
 2.91677471e-04 3.23507450e-05 1.43250282e-06 5.16627645e-05
 2.44841111e-05 1.71262716e-05]
ene_total = [0.53720059 0.6342901  0.54640715 0.5088514  0.66102208 0.66480406
 0.50536026 0.49719653 0.74209725 0.66674105]
optimize_network iter = 1 obj = 5.963970476732851
eta = 0.6905397579563017
freqs = [4.94285918e+07 1.07722361e+08 4.88321676e+07 1.66814724e+07
 1.25391054e+08 6.04562965e+07 2.09183144e+07 6.89982586e+07
 5.57373503e+07 4.89289881e+07]
Done!
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [1.84704838e-05 1.84505302e-04 1.77412796e-05 7.17940527e-07
 2.88672747e-04 3.20174828e-05 1.41774584e-06 5.11305590e-05
 2.42318873e-05 1.69498448e-05]
ene_total = [0.00690583 0.00815225 0.0070242  0.00654157 0.00849482 0.00854611
 0.00649669 0.00639122 0.00953984 0.00857117]
At round 12 energy consumption: 0.07666369174488584
At round 12 eta: 0.6905397579563017
At round 12 a_n: 23.72950684565325
At round 12 local rounds: 12.124905073660063
At round 12 global rounds: 77.78722246725123
gradient difference: 0.4733177125453949
train() client id: f_00000-0-0 loss: 1.522657  [   32/  126]
train() client id: f_00000-0-1 loss: 1.457167  [   64/  126]
train() client id: f_00000-0-2 loss: 1.524194  [   96/  126]
train() client id: f_00000-1-0 loss: 1.207565  [   32/  126]
train() client id: f_00000-1-1 loss: 1.444310  [   64/  126]
train() client id: f_00000-1-2 loss: 1.266109  [   96/  126]
train() client id: f_00000-2-0 loss: 1.273764  [   32/  126]
train() client id: f_00000-2-1 loss: 1.109845  [   64/  126]
train() client id: f_00000-2-2 loss: 1.166149  [   96/  126]
train() client id: f_00000-3-0 loss: 1.118329  [   32/  126]
train() client id: f_00000-3-1 loss: 1.140754  [   64/  126]
train() client id: f_00000-3-2 loss: 1.152964  [   96/  126]
train() client id: f_00000-4-0 loss: 1.112624  [   32/  126]
train() client id: f_00000-4-1 loss: 1.150786  [   64/  126]
train() client id: f_00000-4-2 loss: 1.038859  [   96/  126]
train() client id: f_00000-5-0 loss: 1.024028  [   32/  126]
train() client id: f_00000-5-1 loss: 1.074238  [   64/  126]
train() client id: f_00000-5-2 loss: 1.017931  [   96/  126]
train() client id: f_00000-6-0 loss: 1.030683  [   32/  126]
train() client id: f_00000-6-1 loss: 0.982234  [   64/  126]
train() client id: f_00000-6-2 loss: 0.976648  [   96/  126]
train() client id: f_00000-7-0 loss: 1.025101  [   32/  126]
train() client id: f_00000-7-1 loss: 1.017212  [   64/  126]
train() client id: f_00000-7-2 loss: 0.967253  [   96/  126]
train() client id: f_00000-8-0 loss: 0.944015  [   32/  126]
train() client id: f_00000-8-1 loss: 0.987439  [   64/  126]
train() client id: f_00000-8-2 loss: 1.056549  [   96/  126]
train() client id: f_00000-9-0 loss: 0.957609  [   32/  126]
train() client id: f_00000-9-1 loss: 1.020043  [   64/  126]
train() client id: f_00000-9-2 loss: 0.961116  [   96/  126]
train() client id: f_00000-10-0 loss: 0.933823  [   32/  126]
train() client id: f_00000-10-1 loss: 1.042819  [   64/  126]
train() client id: f_00000-10-2 loss: 0.993909  [   96/  126]
train() client id: f_00000-11-0 loss: 0.919765  [   32/  126]
train() client id: f_00000-11-1 loss: 1.017823  [   64/  126]
train() client id: f_00000-11-2 loss: 1.048049  [   96/  126]
train() client id: f_00001-0-0 loss: 0.475086  [   32/  265]
train() client id: f_00001-0-1 loss: 0.696962  [   64/  265]
train() client id: f_00001-0-2 loss: 0.468097  [   96/  265]
train() client id: f_00001-0-3 loss: 0.499463  [  128/  265]
train() client id: f_00001-0-4 loss: 0.483879  [  160/  265]
train() client id: f_00001-0-5 loss: 0.571897  [  192/  265]
train() client id: f_00001-0-6 loss: 0.548267  [  224/  265]
train() client id: f_00001-0-7 loss: 0.562161  [  256/  265]
train() client id: f_00001-1-0 loss: 0.529075  [   32/  265]
train() client id: f_00001-1-1 loss: 0.487168  [   64/  265]
train() client id: f_00001-1-2 loss: 0.515967  [   96/  265]
train() client id: f_00001-1-3 loss: 0.573942  [  128/  265]
train() client id: f_00001-1-4 loss: 0.510194  [  160/  265]
train() client id: f_00001-1-5 loss: 0.485658  [  192/  265]
train() client id: f_00001-1-6 loss: 0.587449  [  224/  265]
train() client id: f_00001-1-7 loss: 0.532578  [  256/  265]
train() client id: f_00001-2-0 loss: 0.548035  [   32/  265]
train() client id: f_00001-2-1 loss: 0.501606  [   64/  265]
train() client id: f_00001-2-2 loss: 0.443712  [   96/  265]
train() client id: f_00001-2-3 loss: 0.472043  [  128/  265]
train() client id: f_00001-2-4 loss: 0.651820  [  160/  265]
train() client id: f_00001-2-5 loss: 0.492044  [  192/  265]
train() client id: f_00001-2-6 loss: 0.495317  [  224/  265]
train() client id: f_00001-2-7 loss: 0.580107  [  256/  265]
train() client id: f_00001-3-0 loss: 0.607443  [   32/  265]
train() client id: f_00001-3-1 loss: 0.564503  [   64/  265]
train() client id: f_00001-3-2 loss: 0.415156  [   96/  265]
train() client id: f_00001-3-3 loss: 0.441372  [  128/  265]
train() client id: f_00001-3-4 loss: 0.480940  [  160/  265]
train() client id: f_00001-3-5 loss: 0.619685  [  192/  265]
train() client id: f_00001-3-6 loss: 0.538320  [  224/  265]
train() client id: f_00001-3-7 loss: 0.473087  [  256/  265]
train() client id: f_00001-4-0 loss: 0.428000  [   32/  265]
train() client id: f_00001-4-1 loss: 0.559597  [   64/  265]
train() client id: f_00001-4-2 loss: 0.445796  [   96/  265]
train() client id: f_00001-4-3 loss: 0.682918  [  128/  265]
train() client id: f_00001-4-4 loss: 0.486835  [  160/  265]
train() client id: f_00001-4-5 loss: 0.435093  [  192/  265]
train() client id: f_00001-4-6 loss: 0.594611  [  224/  265]
train() client id: f_00001-4-7 loss: 0.475021  [  256/  265]
train() client id: f_00001-5-0 loss: 0.481360  [   32/  265]
train() client id: f_00001-5-1 loss: 0.587786  [   64/  265]
train() client id: f_00001-5-2 loss: 0.585402  [   96/  265]
train() client id: f_00001-5-3 loss: 0.429819  [  128/  265]
train() client id: f_00001-5-4 loss: 0.434233  [  160/  265]
train() client id: f_00001-5-5 loss: 0.569907  [  192/  265]
train() client id: f_00001-5-6 loss: 0.424487  [  224/  265]
train() client id: f_00001-5-7 loss: 0.484970  [  256/  265]
train() client id: f_00001-6-0 loss: 0.439286  [   32/  265]
train() client id: f_00001-6-1 loss: 0.553735  [   64/  265]
train() client id: f_00001-6-2 loss: 0.483195  [   96/  265]
train() client id: f_00001-6-3 loss: 0.589251  [  128/  265]
train() client id: f_00001-6-4 loss: 0.421312  [  160/  265]
train() client id: f_00001-6-5 loss: 0.522079  [  192/  265]
train() client id: f_00001-6-6 loss: 0.612388  [  224/  265]
train() client id: f_00001-6-7 loss: 0.439276  [  256/  265]
train() client id: f_00001-7-0 loss: 0.485693  [   32/  265]
train() client id: f_00001-7-1 loss: 0.584249  [   64/  265]
train() client id: f_00001-7-2 loss: 0.403672  [   96/  265]
train() client id: f_00001-7-3 loss: 0.456733  [  128/  265]
train() client id: f_00001-7-4 loss: 0.552859  [  160/  265]
train() client id: f_00001-7-5 loss: 0.641849  [  192/  265]
train() client id: f_00001-7-6 loss: 0.420879  [  224/  265]
train() client id: f_00001-7-7 loss: 0.422572  [  256/  265]
train() client id: f_00001-8-0 loss: 0.573068  [   32/  265]
train() client id: f_00001-8-1 loss: 0.533282  [   64/  265]
train() client id: f_00001-8-2 loss: 0.409892  [   96/  265]
train() client id: f_00001-8-3 loss: 0.524784  [  128/  265]
train() client id: f_00001-8-4 loss: 0.462287  [  160/  265]
train() client id: f_00001-8-5 loss: 0.624571  [  192/  265]
train() client id: f_00001-8-6 loss: 0.441188  [  224/  265]
train() client id: f_00001-8-7 loss: 0.469405  [  256/  265]
train() client id: f_00001-9-0 loss: 0.580128  [   32/  265]
train() client id: f_00001-9-1 loss: 0.409076  [   64/  265]
train() client id: f_00001-9-2 loss: 0.390928  [   96/  265]
train() client id: f_00001-9-3 loss: 0.421897  [  128/  265]
train() client id: f_00001-9-4 loss: 0.667700  [  160/  265]
train() client id: f_00001-9-5 loss: 0.475531  [  192/  265]
train() client id: f_00001-9-6 loss: 0.421560  [  224/  265]
train() client id: f_00001-9-7 loss: 0.588720  [  256/  265]
train() client id: f_00001-10-0 loss: 0.537983  [   32/  265]
train() client id: f_00001-10-1 loss: 0.558350  [   64/  265]
train() client id: f_00001-10-2 loss: 0.512809  [   96/  265]
train() client id: f_00001-10-3 loss: 0.533663  [  128/  265]
train() client id: f_00001-10-4 loss: 0.535286  [  160/  265]
train() client id: f_00001-10-5 loss: 0.489758  [  192/  265]
train() client id: f_00001-10-6 loss: 0.443640  [  224/  265]
train() client id: f_00001-10-7 loss: 0.418960  [  256/  265]
train() client id: f_00001-11-0 loss: 0.558656  [   32/  265]
train() client id: f_00001-11-1 loss: 0.524916  [   64/  265]
train() client id: f_00001-11-2 loss: 0.507972  [   96/  265]
train() client id: f_00001-11-3 loss: 0.514363  [  128/  265]
train() client id: f_00001-11-4 loss: 0.452069  [  160/  265]
train() client id: f_00001-11-5 loss: 0.409652  [  192/  265]
train() client id: f_00001-11-6 loss: 0.454529  [  224/  265]
train() client id: f_00001-11-7 loss: 0.618606  [  256/  265]
train() client id: f_00002-0-0 loss: 1.307359  [   32/  124]
train() client id: f_00002-0-1 loss: 1.263077  [   64/  124]
train() client id: f_00002-0-2 loss: 1.197928  [   96/  124]
train() client id: f_00002-1-0 loss: 1.227942  [   32/  124]
train() client id: f_00002-1-1 loss: 1.264177  [   64/  124]
train() client id: f_00002-1-2 loss: 1.170316  [   96/  124]
train() client id: f_00002-2-0 loss: 1.273252  [   32/  124]
train() client id: f_00002-2-1 loss: 1.173285  [   64/  124]
train() client id: f_00002-2-2 loss: 1.103422  [   96/  124]
train() client id: f_00002-3-0 loss: 1.194259  [   32/  124]
train() client id: f_00002-3-1 loss: 1.093852  [   64/  124]
train() client id: f_00002-3-2 loss: 1.168183  [   96/  124]
train() client id: f_00002-4-0 loss: 1.130776  [   32/  124]
train() client id: f_00002-4-1 loss: 1.081317  [   64/  124]
train() client id: f_00002-4-2 loss: 1.150596  [   96/  124]
train() client id: f_00002-5-0 loss: 1.005296  [   32/  124]
train() client id: f_00002-5-1 loss: 1.119379  [   64/  124]
train() client id: f_00002-5-2 loss: 1.195338  [   96/  124]
train() client id: f_00002-6-0 loss: 1.146106  [   32/  124]
train() client id: f_00002-6-1 loss: 1.120604  [   64/  124]
train() client id: f_00002-6-2 loss: 1.088431  [   96/  124]
train() client id: f_00002-7-0 loss: 1.125620  [   32/  124]
train() client id: f_00002-7-1 loss: 1.041285  [   64/  124]
train() client id: f_00002-7-2 loss: 1.037483  [   96/  124]
train() client id: f_00002-8-0 loss: 1.143010  [   32/  124]
train() client id: f_00002-8-1 loss: 1.087196  [   64/  124]
train() client id: f_00002-8-2 loss: 1.005308  [   96/  124]
train() client id: f_00002-9-0 loss: 1.092681  [   32/  124]
train() client id: f_00002-9-1 loss: 1.116044  [   64/  124]
train() client id: f_00002-9-2 loss: 0.963562  [   96/  124]
train() client id: f_00002-10-0 loss: 1.057778  [   32/  124]
train() client id: f_00002-10-1 loss: 1.114500  [   64/  124]
train() client id: f_00002-10-2 loss: 1.074376  [   96/  124]
train() client id: f_00002-11-0 loss: 1.081960  [   32/  124]
train() client id: f_00002-11-1 loss: 1.084849  [   64/  124]
train() client id: f_00002-11-2 loss: 1.078197  [   96/  124]
train() client id: f_00003-0-0 loss: 0.962996  [   32/   43]
train() client id: f_00003-1-0 loss: 0.922391  [   32/   43]
train() client id: f_00003-2-0 loss: 0.857461  [   32/   43]
train() client id: f_00003-3-0 loss: 0.792094  [   32/   43]
train() client id: f_00003-4-0 loss: 0.840776  [   32/   43]
train() client id: f_00003-5-0 loss: 0.885785  [   32/   43]
train() client id: f_00003-6-0 loss: 0.971050  [   32/   43]
train() client id: f_00003-7-0 loss: 0.939390  [   32/   43]
train() client id: f_00003-8-0 loss: 0.760176  [   32/   43]
train() client id: f_00003-9-0 loss: 0.781477  [   32/   43]
train() client id: f_00003-10-0 loss: 0.842027  [   32/   43]
train() client id: f_00003-11-0 loss: 0.771483  [   32/   43]
train() client id: f_00004-0-0 loss: 0.715426  [   32/  306]
train() client id: f_00004-0-1 loss: 0.725304  [   64/  306]
train() client id: f_00004-0-2 loss: 0.627501  [   96/  306]
train() client id: f_00004-0-3 loss: 0.844520  [  128/  306]
train() client id: f_00004-0-4 loss: 0.759723  [  160/  306]
train() client id: f_00004-0-5 loss: 0.640947  [  192/  306]
train() client id: f_00004-0-6 loss: 0.854308  [  224/  306]
train() client id: f_00004-0-7 loss: 0.793642  [  256/  306]
train() client id: f_00004-0-8 loss: 0.679883  [  288/  306]
train() client id: f_00004-1-0 loss: 0.644825  [   32/  306]
train() client id: f_00004-1-1 loss: 0.764096  [   64/  306]
train() client id: f_00004-1-2 loss: 0.854121  [   96/  306]
train() client id: f_00004-1-3 loss: 0.814122  [  128/  306]
train() client id: f_00004-1-4 loss: 0.688071  [  160/  306]
train() client id: f_00004-1-5 loss: 0.630122  [  192/  306]
train() client id: f_00004-1-6 loss: 0.918773  [  224/  306]
train() client id: f_00004-1-7 loss: 0.647336  [  256/  306]
train() client id: f_00004-1-8 loss: 0.654658  [  288/  306]
train() client id: f_00004-2-0 loss: 0.660556  [   32/  306]
train() client id: f_00004-2-1 loss: 0.849385  [   64/  306]
train() client id: f_00004-2-2 loss: 0.628244  [   96/  306]
train() client id: f_00004-2-3 loss: 0.555119  [  128/  306]
train() client id: f_00004-2-4 loss: 0.774254  [  160/  306]
train() client id: f_00004-2-5 loss: 0.860582  [  192/  306]
train() client id: f_00004-2-6 loss: 0.732499  [  224/  306]
train() client id: f_00004-2-7 loss: 0.847931  [  256/  306]
train() client id: f_00004-2-8 loss: 0.779073  [  288/  306]
train() client id: f_00004-3-0 loss: 0.657105  [   32/  306]
train() client id: f_00004-3-1 loss: 0.811846  [   64/  306]
train() client id: f_00004-3-2 loss: 0.871031  [   96/  306]
train() client id: f_00004-3-3 loss: 0.691056  [  128/  306]
train() client id: f_00004-3-4 loss: 0.869647  [  160/  306]
train() client id: f_00004-3-5 loss: 0.585211  [  192/  306]
train() client id: f_00004-3-6 loss: 0.773978  [  224/  306]
train() client id: f_00004-3-7 loss: 0.612626  [  256/  306]
train() client id: f_00004-3-8 loss: 0.773121  [  288/  306]
train() client id: f_00004-4-0 loss: 0.674844  [   32/  306]
train() client id: f_00004-4-1 loss: 0.833920  [   64/  306]
train() client id: f_00004-4-2 loss: 0.729540  [   96/  306]
train() client id: f_00004-4-3 loss: 0.738669  [  128/  306]
train() client id: f_00004-4-4 loss: 0.861599  [  160/  306]
train() client id: f_00004-4-5 loss: 0.638101  [  192/  306]
train() client id: f_00004-4-6 loss: 0.683194  [  224/  306]
train() client id: f_00004-4-7 loss: 0.836656  [  256/  306]
train() client id: f_00004-4-8 loss: 0.741906  [  288/  306]
train() client id: f_00004-5-0 loss: 0.754759  [   32/  306]
train() client id: f_00004-5-1 loss: 0.750716  [   64/  306]
train() client id: f_00004-5-2 loss: 0.717794  [   96/  306]
train() client id: f_00004-5-3 loss: 0.686472  [  128/  306]
train() client id: f_00004-5-4 loss: 0.896968  [  160/  306]
train() client id: f_00004-5-5 loss: 0.827800  [  192/  306]
train() client id: f_00004-5-6 loss: 0.779818  [  224/  306]
train() client id: f_00004-5-7 loss: 0.782616  [  256/  306]
train() client id: f_00004-5-8 loss: 0.649624  [  288/  306]
train() client id: f_00004-6-0 loss: 0.814812  [   32/  306]
train() client id: f_00004-6-1 loss: 0.683949  [   64/  306]
train() client id: f_00004-6-2 loss: 0.697452  [   96/  306]
train() client id: f_00004-6-3 loss: 0.814262  [  128/  306]
train() client id: f_00004-6-4 loss: 0.694889  [  160/  306]
train() client id: f_00004-6-5 loss: 0.879789  [  192/  306]
train() client id: f_00004-6-6 loss: 0.828630  [  224/  306]
train() client id: f_00004-6-7 loss: 0.652005  [  256/  306]
train() client id: f_00004-6-8 loss: 0.658540  [  288/  306]
train() client id: f_00004-7-0 loss: 0.757573  [   32/  306]
train() client id: f_00004-7-1 loss: 0.668945  [   64/  306]
train() client id: f_00004-7-2 loss: 0.714756  [   96/  306]
train() client id: f_00004-7-3 loss: 0.951199  [  128/  306]
train() client id: f_00004-7-4 loss: 0.785438  [  160/  306]
train() client id: f_00004-7-5 loss: 0.674613  [  192/  306]
train() client id: f_00004-7-6 loss: 0.628842  [  224/  306]
train() client id: f_00004-7-7 loss: 0.698836  [  256/  306]
train() client id: f_00004-7-8 loss: 0.833135  [  288/  306]
train() client id: f_00004-8-0 loss: 0.746208  [   32/  306]
train() client id: f_00004-8-1 loss: 0.726504  [   64/  306]
train() client id: f_00004-8-2 loss: 0.673981  [   96/  306]
train() client id: f_00004-8-3 loss: 0.673228  [  128/  306]
train() client id: f_00004-8-4 loss: 0.794740  [  160/  306]
train() client id: f_00004-8-5 loss: 0.785930  [  192/  306]
train() client id: f_00004-8-6 loss: 0.770334  [  224/  306]
train() client id: f_00004-8-7 loss: 0.760323  [  256/  306]
train() client id: f_00004-8-8 loss: 0.830740  [  288/  306]
train() client id: f_00004-9-0 loss: 0.745848  [   32/  306]
train() client id: f_00004-9-1 loss: 0.706641  [   64/  306]
train() client id: f_00004-9-2 loss: 0.745213  [   96/  306]
train() client id: f_00004-9-3 loss: 0.810497  [  128/  306]
train() client id: f_00004-9-4 loss: 0.763592  [  160/  306]
train() client id: f_00004-9-5 loss: 0.730017  [  192/  306]
train() client id: f_00004-9-6 loss: 0.649709  [  224/  306]
train() client id: f_00004-9-7 loss: 0.939904  [  256/  306]
train() client id: f_00004-9-8 loss: 0.828014  [  288/  306]
train() client id: f_00004-10-0 loss: 0.838690  [   32/  306]
train() client id: f_00004-10-1 loss: 0.746743  [   64/  306]
train() client id: f_00004-10-2 loss: 0.660475  [   96/  306]
train() client id: f_00004-10-3 loss: 0.739715  [  128/  306]
train() client id: f_00004-10-4 loss: 0.840327  [  160/  306]
train() client id: f_00004-10-5 loss: 0.680462  [  192/  306]
train() client id: f_00004-10-6 loss: 0.727153  [  224/  306]
train() client id: f_00004-10-7 loss: 0.781692  [  256/  306]
train() client id: f_00004-10-8 loss: 0.859951  [  288/  306]
train() client id: f_00004-11-0 loss: 0.732298  [   32/  306]
train() client id: f_00004-11-1 loss: 0.825797  [   64/  306]
train() client id: f_00004-11-2 loss: 0.765047  [   96/  306]
train() client id: f_00004-11-3 loss: 0.791299  [  128/  306]
train() client id: f_00004-11-4 loss: 0.706032  [  160/  306]
train() client id: f_00004-11-5 loss: 0.820317  [  192/  306]
train() client id: f_00004-11-6 loss: 0.751696  [  224/  306]
train() client id: f_00004-11-7 loss: 0.822474  [  256/  306]
train() client id: f_00004-11-8 loss: 0.705152  [  288/  306]
train() client id: f_00005-0-0 loss: 0.854562  [   32/  146]
train() client id: f_00005-0-1 loss: 0.900049  [   64/  146]
train() client id: f_00005-0-2 loss: 0.994019  [   96/  146]
train() client id: f_00005-0-3 loss: 0.686986  [  128/  146]
train() client id: f_00005-1-0 loss: 0.739991  [   32/  146]
train() client id: f_00005-1-1 loss: 1.021880  [   64/  146]
train() client id: f_00005-1-2 loss: 0.852158  [   96/  146]
train() client id: f_00005-1-3 loss: 0.704822  [  128/  146]
train() client id: f_00005-2-0 loss: 0.891941  [   32/  146]
train() client id: f_00005-2-1 loss: 0.777735  [   64/  146]
train() client id: f_00005-2-2 loss: 0.765558  [   96/  146]
train() client id: f_00005-2-3 loss: 0.861754  [  128/  146]
train() client id: f_00005-3-0 loss: 0.945095  [   32/  146]
train() client id: f_00005-3-1 loss: 0.903318  [   64/  146]
train() client id: f_00005-3-2 loss: 0.654664  [   96/  146]
train() client id: f_00005-3-3 loss: 0.755313  [  128/  146]
train() client id: f_00005-4-0 loss: 0.730679  [   32/  146]
train() client id: f_00005-4-1 loss: 0.812838  [   64/  146]
train() client id: f_00005-4-2 loss: 0.819593  [   96/  146]
train() client id: f_00005-4-3 loss: 0.876951  [  128/  146]
train() client id: f_00005-5-0 loss: 0.748957  [   32/  146]
train() client id: f_00005-5-1 loss: 0.884902  [   64/  146]
train() client id: f_00005-5-2 loss: 0.871753  [   96/  146]
train() client id: f_00005-5-3 loss: 0.692481  [  128/  146]
train() client id: f_00005-6-0 loss: 0.782612  [   32/  146]
train() client id: f_00005-6-1 loss: 0.691377  [   64/  146]
train() client id: f_00005-6-2 loss: 0.905779  [   96/  146]
train() client id: f_00005-6-3 loss: 0.828307  [  128/  146]
train() client id: f_00005-7-0 loss: 0.822567  [   32/  146]
train() client id: f_00005-7-1 loss: 0.839560  [   64/  146]
train() client id: f_00005-7-2 loss: 0.728304  [   96/  146]
train() client id: f_00005-7-3 loss: 0.757792  [  128/  146]
train() client id: f_00005-8-0 loss: 1.176723  [   32/  146]
train() client id: f_00005-8-1 loss: 0.678714  [   64/  146]
train() client id: f_00005-8-2 loss: 0.597822  [   96/  146]
train() client id: f_00005-8-3 loss: 0.806955  [  128/  146]
train() client id: f_00005-9-0 loss: 0.809132  [   32/  146]
train() client id: f_00005-9-1 loss: 1.170213  [   64/  146]
train() client id: f_00005-9-2 loss: 0.767446  [   96/  146]
train() client id: f_00005-9-3 loss: 0.683414  [  128/  146]
train() client id: f_00005-10-0 loss: 0.632469  [   32/  146]
train() client id: f_00005-10-1 loss: 0.937056  [   64/  146]
train() client id: f_00005-10-2 loss: 0.924908  [   96/  146]
train() client id: f_00005-10-3 loss: 0.874440  [  128/  146]
train() client id: f_00005-11-0 loss: 0.787138  [   32/  146]
train() client id: f_00005-11-1 loss: 0.743244  [   64/  146]
train() client id: f_00005-11-2 loss: 0.672644  [   96/  146]
train() client id: f_00005-11-3 loss: 1.034261  [  128/  146]
train() client id: f_00006-0-0 loss: 0.763406  [   32/   54]
train() client id: f_00006-1-0 loss: 0.715694  [   32/   54]
train() client id: f_00006-2-0 loss: 0.705474  [   32/   54]
train() client id: f_00006-3-0 loss: 0.706106  [   32/   54]
train() client id: f_00006-4-0 loss: 0.663817  [   32/   54]
train() client id: f_00006-5-0 loss: 0.745637  [   32/   54]
train() client id: f_00006-6-0 loss: 0.711538  [   32/   54]
train() client id: f_00006-7-0 loss: 0.723860  [   32/   54]
train() client id: f_00006-8-0 loss: 0.653882  [   32/   54]
train() client id: f_00006-9-0 loss: 0.660244  [   32/   54]
train() client id: f_00006-10-0 loss: 0.651833  [   32/   54]
train() client id: f_00006-11-0 loss: 0.653530  [   32/   54]
train() client id: f_00007-0-0 loss: 0.584504  [   32/  179]
train() client id: f_00007-0-1 loss: 0.667871  [   64/  179]
train() client id: f_00007-0-2 loss: 0.721142  [   96/  179]
train() client id: f_00007-0-3 loss: 0.696730  [  128/  179]
train() client id: f_00007-0-4 loss: 0.519592  [  160/  179]
train() client id: f_00007-1-0 loss: 0.601542  [   32/  179]
train() client id: f_00007-1-1 loss: 0.530105  [   64/  179]
train() client id: f_00007-1-2 loss: 0.499441  [   96/  179]
train() client id: f_00007-1-3 loss: 0.765371  [  128/  179]
train() client id: f_00007-1-4 loss: 0.735108  [  160/  179]
train() client id: f_00007-2-0 loss: 0.506229  [   32/  179]
train() client id: f_00007-2-1 loss: 0.681080  [   64/  179]
train() client id: f_00007-2-2 loss: 0.662098  [   96/  179]
train() client id: f_00007-2-3 loss: 0.695603  [  128/  179]
train() client id: f_00007-2-4 loss: 0.538184  [  160/  179]
train() client id: f_00007-3-0 loss: 0.594712  [   32/  179]
train() client id: f_00007-3-1 loss: 0.600893  [   64/  179]
train() client id: f_00007-3-2 loss: 0.768707  [   96/  179]
train() client id: f_00007-3-3 loss: 0.506884  [  128/  179]
train() client id: f_00007-3-4 loss: 0.546704  [  160/  179]
train() client id: f_00007-4-0 loss: 0.501850  [   32/  179]
train() client id: f_00007-4-1 loss: 0.537282  [   64/  179]
train() client id: f_00007-4-2 loss: 0.934783  [   96/  179]
train() client id: f_00007-4-3 loss: 0.477440  [  128/  179]
train() client id: f_00007-4-4 loss: 0.529860  [  160/  179]
train() client id: f_00007-5-0 loss: 0.536006  [   32/  179]
train() client id: f_00007-5-1 loss: 0.614825  [   64/  179]
train() client id: f_00007-5-2 loss: 0.561596  [   96/  179]
train() client id: f_00007-5-3 loss: 0.646985  [  128/  179]
train() client id: f_00007-5-4 loss: 0.530427  [  160/  179]
train() client id: f_00007-6-0 loss: 0.529186  [   32/  179]
train() client id: f_00007-6-1 loss: 0.589927  [   64/  179]
train() client id: f_00007-6-2 loss: 0.547771  [   96/  179]
train() client id: f_00007-6-3 loss: 0.586060  [  128/  179]
train() client id: f_00007-6-4 loss: 0.620852  [  160/  179]
train() client id: f_00007-7-0 loss: 0.534744  [   32/  179]
train() client id: f_00007-7-1 loss: 0.444008  [   64/  179]
train() client id: f_00007-7-2 loss: 0.698102  [   96/  179]
train() client id: f_00007-7-3 loss: 0.539944  [  128/  179]
train() client id: f_00007-7-4 loss: 0.524330  [  160/  179]
train() client id: f_00007-8-0 loss: 0.630574  [   32/  179]
train() client id: f_00007-8-1 loss: 0.575863  [   64/  179]
train() client id: f_00007-8-2 loss: 0.599413  [   96/  179]
train() client id: f_00007-8-3 loss: 0.472777  [  128/  179]
train() client id: f_00007-8-4 loss: 0.457243  [  160/  179]
train() client id: f_00007-9-0 loss: 0.648860  [   32/  179]
train() client id: f_00007-9-1 loss: 0.623714  [   64/  179]
train() client id: f_00007-9-2 loss: 0.439847  [   96/  179]
train() client id: f_00007-9-3 loss: 0.474104  [  128/  179]
train() client id: f_00007-9-4 loss: 0.550117  [  160/  179]
train() client id: f_00007-10-0 loss: 0.468286  [   32/  179]
train() client id: f_00007-10-1 loss: 0.607712  [   64/  179]
train() client id: f_00007-10-2 loss: 0.591202  [   96/  179]
train() client id: f_00007-10-3 loss: 0.619655  [  128/  179]
train() client id: f_00007-10-4 loss: 0.586497  [  160/  179]
train() client id: f_00007-11-0 loss: 0.520192  [   32/  179]
train() client id: f_00007-11-1 loss: 0.507656  [   64/  179]
train() client id: f_00007-11-2 loss: 0.701080  [   96/  179]
train() client id: f_00007-11-3 loss: 0.606387  [  128/  179]
train() client id: f_00007-11-4 loss: 0.513265  [  160/  179]
train() client id: f_00008-0-0 loss: 0.877338  [   32/  130]
train() client id: f_00008-0-1 loss: 0.634867  [   64/  130]
train() client id: f_00008-0-2 loss: 0.824707  [   96/  130]
train() client id: f_00008-0-3 loss: 0.824698  [  128/  130]
train() client id: f_00008-1-0 loss: 0.886192  [   32/  130]
train() client id: f_00008-1-1 loss: 0.789893  [   64/  130]
train() client id: f_00008-1-2 loss: 0.799618  [   96/  130]
train() client id: f_00008-1-3 loss: 0.729121  [  128/  130]
train() client id: f_00008-2-0 loss: 0.791631  [   32/  130]
train() client id: f_00008-2-1 loss: 0.828300  [   64/  130]
train() client id: f_00008-2-2 loss: 0.757589  [   96/  130]
train() client id: f_00008-2-3 loss: 0.816080  [  128/  130]
train() client id: f_00008-3-0 loss: 0.810106  [   32/  130]
train() client id: f_00008-3-1 loss: 0.703113  [   64/  130]
train() client id: f_00008-3-2 loss: 0.907163  [   96/  130]
train() client id: f_00008-3-3 loss: 0.780680  [  128/  130]
train() client id: f_00008-4-0 loss: 0.857606  [   32/  130]
train() client id: f_00008-4-1 loss: 0.840427  [   64/  130]
train() client id: f_00008-4-2 loss: 0.745124  [   96/  130]
train() client id: f_00008-4-3 loss: 0.757827  [  128/  130]
train() client id: f_00008-5-0 loss: 0.796991  [   32/  130]
train() client id: f_00008-5-1 loss: 0.830518  [   64/  130]
train() client id: f_00008-5-2 loss: 0.814808  [   96/  130]
train() client id: f_00008-5-3 loss: 0.757985  [  128/  130]
train() client id: f_00008-6-0 loss: 0.759606  [   32/  130]
train() client id: f_00008-6-1 loss: 0.911567  [   64/  130]
train() client id: f_00008-6-2 loss: 0.765550  [   96/  130]
train() client id: f_00008-6-3 loss: 0.759170  [  128/  130]
train() client id: f_00008-7-0 loss: 0.785300  [   32/  130]
train() client id: f_00008-7-1 loss: 0.750524  [   64/  130]
train() client id: f_00008-7-2 loss: 0.860733  [   96/  130]
train() client id: f_00008-7-3 loss: 0.748128  [  128/  130]
train() client id: f_00008-8-0 loss: 0.789348  [   32/  130]
train() client id: f_00008-8-1 loss: 0.825474  [   64/  130]
train() client id: f_00008-8-2 loss: 0.728240  [   96/  130]
train() client id: f_00008-8-3 loss: 0.859661  [  128/  130]
train() client id: f_00008-9-0 loss: 0.765472  [   32/  130]
train() client id: f_00008-9-1 loss: 0.864699  [   64/  130]
train() client id: f_00008-9-2 loss: 0.746994  [   96/  130]
train() client id: f_00008-9-3 loss: 0.817968  [  128/  130]
train() client id: f_00008-10-0 loss: 0.727001  [   32/  130]
train() client id: f_00008-10-1 loss: 0.931254  [   64/  130]
train() client id: f_00008-10-2 loss: 0.759988  [   96/  130]
train() client id: f_00008-10-3 loss: 0.745802  [  128/  130]
train() client id: f_00008-11-0 loss: 0.822867  [   32/  130]
train() client id: f_00008-11-1 loss: 0.763023  [   64/  130]
train() client id: f_00008-11-2 loss: 0.778110  [   96/  130]
train() client id: f_00008-11-3 loss: 0.833055  [  128/  130]
train() client id: f_00009-0-0 loss: 1.129158  [   32/  118]
train() client id: f_00009-0-1 loss: 1.031044  [   64/  118]
train() client id: f_00009-0-2 loss: 1.103321  [   96/  118]
train() client id: f_00009-1-0 loss: 0.967771  [   32/  118]
train() client id: f_00009-1-1 loss: 1.175851  [   64/  118]
train() client id: f_00009-1-2 loss: 0.940167  [   96/  118]
train() client id: f_00009-2-0 loss: 0.950500  [   32/  118]
train() client id: f_00009-2-1 loss: 1.067471  [   64/  118]
train() client id: f_00009-2-2 loss: 0.938429  [   96/  118]
train() client id: f_00009-3-0 loss: 0.926823  [   32/  118]
train() client id: f_00009-3-1 loss: 0.963018  [   64/  118]
train() client id: f_00009-3-2 loss: 1.032282  [   96/  118]
train() client id: f_00009-4-0 loss: 0.852094  [   32/  118]
train() client id: f_00009-4-1 loss: 0.979568  [   64/  118]
train() client id: f_00009-4-2 loss: 1.018632  [   96/  118]
train() client id: f_00009-5-0 loss: 0.935345  [   32/  118]
train() client id: f_00009-5-1 loss: 0.867649  [   64/  118]
train() client id: f_00009-5-2 loss: 0.921606  [   96/  118]
train() client id: f_00009-6-0 loss: 0.814969  [   32/  118]
train() client id: f_00009-6-1 loss: 0.877407  [   64/  118]
train() client id: f_00009-6-2 loss: 0.912234  [   96/  118]
train() client id: f_00009-7-0 loss: 0.838854  [   32/  118]
train() client id: f_00009-7-1 loss: 0.974888  [   64/  118]
train() client id: f_00009-7-2 loss: 0.801981  [   96/  118]
train() client id: f_00009-8-0 loss: 0.834016  [   32/  118]
train() client id: f_00009-8-1 loss: 0.792447  [   64/  118]
train() client id: f_00009-8-2 loss: 0.912510  [   96/  118]
train() client id: f_00009-9-0 loss: 0.923565  [   32/  118]
train() client id: f_00009-9-1 loss: 0.841276  [   64/  118]
train() client id: f_00009-9-2 loss: 0.761451  [   96/  118]
train() client id: f_00009-10-0 loss: 0.682419  [   32/  118]
train() client id: f_00009-10-1 loss: 0.906785  [   64/  118]
train() client id: f_00009-10-2 loss: 0.971842  [   96/  118]
train() client id: f_00009-11-0 loss: 0.769112  [   32/  118]
train() client id: f_00009-11-1 loss: 0.954389  [   64/  118]
train() client id: f_00009-11-2 loss: 0.872053  [   96/  118]
At round 12 accuracy: 0.6312997347480106
At round 12 training accuracy: 0.5828303152246814
At round 12 training loss: 0.842926991174425
update_location
xs = [ -3.9056584    4.20031788  80.00902392  18.81129433   0.97929623
   3.95640986 -42.44319194 -21.32485185  64.66397685  -7.06087855]
ys = [ 72.5879595   55.55583871   1.32061395 -42.45517586  34.35018685
  17.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [123.62955161 114.47311424 128.07493092 110.25564272 105.73974824
 101.65135093 108.66607145 102.25177535 120.37483074 100.32879877]
dists_bs = [199.40285797 215.35937777 308.53546217 291.28949744 225.28000941
 238.18645603 221.6347049  232.27980029 286.74612406 239.67486674]
uav_gains = [5.88385192e-11 7.13231094e-11 5.38624893e-11 7.83414303e-11
 8.69762693e-11 9.59875921e-11 8.12381170e-11 9.45846506e-11
 6.28982282e-11 9.91823280e-11]
bs_gains = [4.01833579e-11 3.23918296e-11 1.18369507e-11 1.39054348e-11
 2.85542429e-11 2.44301599e-11 2.98887905e-11 2.62096995e-11
 1.45311815e-11 2.40077301e-11]
Round 13
-------------------------------
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.27548856 19.34614452  9.13389385  3.26646163 22.31552022 10.75645448
  4.0605729  13.0924169   9.62823145  8.73319217]
obj_prev = 109.60837667978073
eta_min = 9.621283683779976e-11	eta_max = 0.9207981058157783
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 25.487961404153893	eta = 0.9090909090909091
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 44.16384428532155	eta = 0.5246570894979226
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 35.24177796016038	eta = 0.6574831164866354
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 33.64173104871985	eta = 0.6887539160877387
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 33.56259776728117	eta = 0.6903778475206297
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 33.56239040785321	eta = 0.6903821129008305
eta = 0.6903821129008305
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [0.0305736  0.06430162 0.03008831 0.01043385 0.07425017 0.03542655
 0.01310297 0.04343393 0.03154419 0.02863242]
ene_total = [2.87721447 5.53384282 2.84956312 1.30490774 6.3139176  3.36162512
 1.50604088 3.81675253 3.15491369 2.84361242]
ti_comp = [0.31327504 0.30392366 0.31202998 0.31702896 0.30162658 0.29859963
 0.31747675 0.31928923 0.2867473  0.29824756]
ti_coms = [0.06967721 0.0790286  0.07092227 0.06592329 0.08132567 0.08435263
 0.0654755  0.06366303 0.09620495 0.08470469]
t_total = [29.34994545 29.34994545 29.34994545 29.34994545 29.34994545 29.34994545
 29.34994545 29.34994545 29.34994545 29.34994545]
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [1.81998670e-05 1.79894040e-04 1.74856014e-05 7.06343005e-07
 2.81211599e-04 3.11665129e-05 1.39497231e-06 5.02341124e-05
 2.38583025e-05 1.64930279e-05]
ene_total = [0.53540984 0.61947247 0.54489739 0.50529856 0.64484267 0.64887784
 0.50191943 0.49177151 0.73915545 0.65045154]
optimize_network iter = 0 obj = 5.882096701797337
eta = 0.6903821129008305
freqs = [4.87967396e+07 1.05785811e+08 4.82138059e+07 1.64556699e+07
 1.23082943e+08 5.93211620e+07 2.06361124e+07 6.80165848e+07
 5.50034670e+07 4.80010972e+07]
eta_min = 0.6903821129008529	eta_max = 0.6903821129008285
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 0.04372443773870311	eta = 0.909090909090909
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 19.912896282792712	eta = 0.0019961681258650003
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 2.05768598057864	eta = 0.019317568000433452
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 2.002030029818999	eta = 0.019854591720065342
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 2.0020149221690455	eta = 0.019854741547231138
eta = 0.019854741547231138
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [1.92521000e-04 1.90294690e-03 1.84965388e-04 7.47180525e-06
 2.97469966e-03 3.29684180e-04 1.47562323e-05 5.31384189e-04
 2.52376804e-04 1.74465793e-04]
ene_total = [0.17335085 0.23740048 0.17618224 0.15978245 0.26890909 0.21220106
 0.15887471 0.16699434 0.23902414 0.20929556]
ti_comp = [0.31327504 0.30392366 0.31202998 0.31702896 0.30162658 0.29859963
 0.31747675 0.31928923 0.2867473  0.29824756]
ti_coms = [0.06967721 0.0790286  0.07092227 0.06592329 0.08132567 0.08435263
 0.0654755  0.06366303 0.09620495 0.08470469]
t_total = [29.34994545 29.34994545 29.34994545 29.34994545 29.34994545 29.34994545
 29.34994545 29.34994545 29.34994545 29.34994545]
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [1.81998670e-05 1.79894040e-04 1.74856014e-05 7.06343005e-07
 2.81211599e-04 3.11665129e-05 1.39497231e-06 5.02341124e-05
 2.38583025e-05 1.64930279e-05]
ene_total = [0.53540984 0.61947247 0.54489739 0.50529856 0.64484267 0.64887784
 0.50191943 0.49177151 0.73915545 0.65045154]
optimize_network iter = 1 obj = 5.882096701797758
eta = 0.6903821129008529
freqs = [4.87967396e+07 1.05785811e+08 4.82138059e+07 1.64556699e+07
 1.23082943e+08 5.93211620e+07 2.06361124e+07 6.80165848e+07
 5.50034670e+07 4.80010972e+07]
Done!
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [1.80012808e-05 1.77931142e-04 1.72948088e-05 6.98635807e-07
 2.78143184e-04 3.08264423e-05 1.37975120e-06 4.96859874e-05
 2.35979748e-05 1.63130657e-05]
ene_total = [0.00698572 0.00808079 0.00710952 0.00659303 0.00841071 0.00846609
 0.00654893 0.00641599 0.00964409 0.00848678]
At round 13 energy consumption: 0.07674165610159829
At round 13 eta: 0.6903821129008529
At round 13 a_n: 23.386960998683932
At round 13 local rounds: 12.132381383768099
At round 13 global rounds: 76.641266006878
gradient difference: 0.3690323233604431
train() client id: f_00000-0-0 loss: 1.545394  [   32/  126]
train() client id: f_00000-0-1 loss: 1.707335  [   64/  126]
train() client id: f_00000-0-2 loss: 1.590345  [   96/  126]
train() client id: f_00000-1-0 loss: 1.447196  [   32/  126]
train() client id: f_00000-1-1 loss: 1.501302  [   64/  126]
train() client id: f_00000-1-2 loss: 1.355768  [   96/  126]
train() client id: f_00000-2-0 loss: 1.546339  [   32/  126]
train() client id: f_00000-2-1 loss: 1.188541  [   64/  126]
train() client id: f_00000-2-2 loss: 1.276175  [   96/  126]
train() client id: f_00000-3-0 loss: 1.365119  [   32/  126]
train() client id: f_00000-3-1 loss: 1.075116  [   64/  126]
train() client id: f_00000-3-2 loss: 1.168057  [   96/  126]
train() client id: f_00000-4-0 loss: 1.155502  [   32/  126]
train() client id: f_00000-4-1 loss: 1.128269  [   64/  126]
train() client id: f_00000-4-2 loss: 1.073691  [   96/  126]
train() client id: f_00000-5-0 loss: 1.064834  [   32/  126]
train() client id: f_00000-5-1 loss: 1.016128  [   64/  126]
train() client id: f_00000-5-2 loss: 0.992324  [   96/  126]
train() client id: f_00000-6-0 loss: 1.018767  [   32/  126]
train() client id: f_00000-6-1 loss: 0.973683  [   64/  126]
train() client id: f_00000-6-2 loss: 1.020232  [   96/  126]
train() client id: f_00000-7-0 loss: 0.959739  [   32/  126]
train() client id: f_00000-7-1 loss: 0.935996  [   64/  126]
train() client id: f_00000-7-2 loss: 0.957176  [   96/  126]
train() client id: f_00000-8-0 loss: 0.940638  [   32/  126]
train() client id: f_00000-8-1 loss: 0.890578  [   64/  126]
train() client id: f_00000-8-2 loss: 0.913374  [   96/  126]
train() client id: f_00000-9-0 loss: 0.875642  [   32/  126]
train() client id: f_00000-9-1 loss: 0.962076  [   64/  126]
train() client id: f_00000-9-2 loss: 0.903341  [   96/  126]
train() client id: f_00000-10-0 loss: 0.883653  [   32/  126]
train() client id: f_00000-10-1 loss: 0.933859  [   64/  126]
train() client id: f_00000-10-2 loss: 0.902847  [   96/  126]
train() client id: f_00000-11-0 loss: 0.833862  [   32/  126]
train() client id: f_00000-11-1 loss: 0.838004  [   64/  126]
train() client id: f_00000-11-2 loss: 0.943121  [   96/  126]
train() client id: f_00001-0-0 loss: 0.376279  [   32/  265]
train() client id: f_00001-0-1 loss: 0.374697  [   64/  265]
train() client id: f_00001-0-2 loss: 0.502926  [   96/  265]
train() client id: f_00001-0-3 loss: 0.343425  [  128/  265]
train() client id: f_00001-0-4 loss: 0.622452  [  160/  265]
train() client id: f_00001-0-5 loss: 0.314879  [  192/  265]
train() client id: f_00001-0-6 loss: 0.503815  [  224/  265]
train() client id: f_00001-0-7 loss: 0.374224  [  256/  265]
train() client id: f_00001-1-0 loss: 0.462893  [   32/  265]
train() client id: f_00001-1-1 loss: 0.485803  [   64/  265]
train() client id: f_00001-1-2 loss: 0.385774  [   96/  265]
train() client id: f_00001-1-3 loss: 0.437679  [  128/  265]
train() client id: f_00001-1-4 loss: 0.318151  [  160/  265]
train() client id: f_00001-1-5 loss: 0.319863  [  192/  265]
train() client id: f_00001-1-6 loss: 0.532332  [  224/  265]
train() client id: f_00001-1-7 loss: 0.330682  [  256/  265]
train() client id: f_00001-2-0 loss: 0.425294  [   32/  265]
train() client id: f_00001-2-1 loss: 0.429976  [   64/  265]
train() client id: f_00001-2-2 loss: 0.427970  [   96/  265]
train() client id: f_00001-2-3 loss: 0.376331  [  128/  265]
train() client id: f_00001-2-4 loss: 0.362470  [  160/  265]
train() client id: f_00001-2-5 loss: 0.293657  [  192/  265]
train() client id: f_00001-2-6 loss: 0.484588  [  224/  265]
train() client id: f_00001-2-7 loss: 0.363652  [  256/  265]
train() client id: f_00001-3-0 loss: 0.425936  [   32/  265]
train() client id: f_00001-3-1 loss: 0.394128  [   64/  265]
train() client id: f_00001-3-2 loss: 0.600944  [   96/  265]
train() client id: f_00001-3-3 loss: 0.316528  [  128/  265]
train() client id: f_00001-3-4 loss: 0.298850  [  160/  265]
train() client id: f_00001-3-5 loss: 0.345704  [  192/  265]
train() client id: f_00001-3-6 loss: 0.346678  [  224/  265]
train() client id: f_00001-3-7 loss: 0.297597  [  256/  265]
train() client id: f_00001-4-0 loss: 0.469050  [   32/  265]
train() client id: f_00001-4-1 loss: 0.371092  [   64/  265]
train() client id: f_00001-4-2 loss: 0.493644  [   96/  265]
train() client id: f_00001-4-3 loss: 0.292167  [  128/  265]
train() client id: f_00001-4-4 loss: 0.410906  [  160/  265]
train() client id: f_00001-4-5 loss: 0.273073  [  192/  265]
train() client id: f_00001-4-6 loss: 0.279909  [  224/  265]
train() client id: f_00001-4-7 loss: 0.405657  [  256/  265]
train() client id: f_00001-5-0 loss: 0.465055  [   32/  265]
train() client id: f_00001-5-1 loss: 0.387672  [   64/  265]
train() client id: f_00001-5-2 loss: 0.392288  [   96/  265]
train() client id: f_00001-5-3 loss: 0.288105  [  128/  265]
train() client id: f_00001-5-4 loss: 0.332234  [  160/  265]
train() client id: f_00001-5-5 loss: 0.462303  [  192/  265]
train() client id: f_00001-5-6 loss: 0.279273  [  224/  265]
train() client id: f_00001-5-7 loss: 0.340647  [  256/  265]
train() client id: f_00001-6-0 loss: 0.355130  [   32/  265]
train() client id: f_00001-6-1 loss: 0.395872  [   64/  265]
train() client id: f_00001-6-2 loss: 0.337637  [   96/  265]
train() client id: f_00001-6-3 loss: 0.336808  [  128/  265]
train() client id: f_00001-6-4 loss: 0.331802  [  160/  265]
train() client id: f_00001-6-5 loss: 0.326579  [  192/  265]
train() client id: f_00001-6-6 loss: 0.333237  [  224/  265]
train() client id: f_00001-6-7 loss: 0.447252  [  256/  265]
train() client id: f_00001-7-0 loss: 0.311832  [   32/  265]
train() client id: f_00001-7-1 loss: 0.394574  [   64/  265]
train() client id: f_00001-7-2 loss: 0.384105  [   96/  265]
train() client id: f_00001-7-3 loss: 0.348031  [  128/  265]
train() client id: f_00001-7-4 loss: 0.388183  [  160/  265]
train() client id: f_00001-7-5 loss: 0.316776  [  192/  265]
train() client id: f_00001-7-6 loss: 0.316729  [  224/  265]
train() client id: f_00001-7-7 loss: 0.319483  [  256/  265]
train() client id: f_00001-8-0 loss: 0.403373  [   32/  265]
train() client id: f_00001-8-1 loss: 0.477730  [   64/  265]
train() client id: f_00001-8-2 loss: 0.363363  [   96/  265]
train() client id: f_00001-8-3 loss: 0.328081  [  128/  265]
train() client id: f_00001-8-4 loss: 0.314193  [  160/  265]
train() client id: f_00001-8-5 loss: 0.382649  [  192/  265]
train() client id: f_00001-8-6 loss: 0.259760  [  224/  265]
train() client id: f_00001-8-7 loss: 0.269339  [  256/  265]
train() client id: f_00001-9-0 loss: 0.324511  [   32/  265]
train() client id: f_00001-9-1 loss: 0.409800  [   64/  265]
train() client id: f_00001-9-2 loss: 0.324748  [   96/  265]
train() client id: f_00001-9-3 loss: 0.310980  [  128/  265]
train() client id: f_00001-9-4 loss: 0.330169  [  160/  265]
train() client id: f_00001-9-5 loss: 0.412475  [  192/  265]
train() client id: f_00001-9-6 loss: 0.335580  [  224/  265]
train() client id: f_00001-9-7 loss: 0.349706  [  256/  265]
train() client id: f_00001-10-0 loss: 0.517606  [   32/  265]
train() client id: f_00001-10-1 loss: 0.316480  [   64/  265]
train() client id: f_00001-10-2 loss: 0.252413  [   96/  265]
train() client id: f_00001-10-3 loss: 0.362132  [  128/  265]
train() client id: f_00001-10-4 loss: 0.244909  [  160/  265]
train() client id: f_00001-10-5 loss: 0.300966  [  192/  265]
train() client id: f_00001-10-6 loss: 0.472255  [  224/  265]
train() client id: f_00001-10-7 loss: 0.312498  [  256/  265]
train() client id: f_00001-11-0 loss: 0.397256  [   32/  265]
train() client id: f_00001-11-1 loss: 0.432314  [   64/  265]
train() client id: f_00001-11-2 loss: 0.319252  [   96/  265]
train() client id: f_00001-11-3 loss: 0.250392  [  128/  265]
train() client id: f_00001-11-4 loss: 0.466446  [  160/  265]
train() client id: f_00001-11-5 loss: 0.318988  [  192/  265]
train() client id: f_00001-11-6 loss: 0.287859  [  224/  265]
train() client id: f_00001-11-7 loss: 0.293513  [  256/  265]
train() client id: f_00002-0-0 loss: 1.150215  [   32/  124]
train() client id: f_00002-0-1 loss: 1.164970  [   64/  124]
train() client id: f_00002-0-2 loss: 1.184652  [   96/  124]
train() client id: f_00002-1-0 loss: 1.188036  [   32/  124]
train() client id: f_00002-1-1 loss: 1.086085  [   64/  124]
train() client id: f_00002-1-2 loss: 1.163353  [   96/  124]
train() client id: f_00002-2-0 loss: 1.026009  [   32/  124]
train() client id: f_00002-2-1 loss: 1.269658  [   64/  124]
train() client id: f_00002-2-2 loss: 1.011188  [   96/  124]
train() client id: f_00002-3-0 loss: 1.059274  [   32/  124]
train() client id: f_00002-3-1 loss: 1.072555  [   64/  124]
train() client id: f_00002-3-2 loss: 1.063165  [   96/  124]
train() client id: f_00002-4-0 loss: 0.956759  [   32/  124]
train() client id: f_00002-4-1 loss: 1.080825  [   64/  124]
train() client id: f_00002-4-2 loss: 1.149286  [   96/  124]
train() client id: f_00002-5-0 loss: 1.095023  [   32/  124]
train() client id: f_00002-5-1 loss: 1.022897  [   64/  124]
train() client id: f_00002-5-2 loss: 1.024209  [   96/  124]
train() client id: f_00002-6-0 loss: 1.012869  [   32/  124]
train() client id: f_00002-6-1 loss: 0.953870  [   64/  124]
train() client id: f_00002-6-2 loss: 1.043274  [   96/  124]
train() client id: f_00002-7-0 loss: 0.928459  [   32/  124]
train() client id: f_00002-7-1 loss: 1.062982  [   64/  124]
train() client id: f_00002-7-2 loss: 0.951230  [   96/  124]
train() client id: f_00002-8-0 loss: 1.181117  [   32/  124]
train() client id: f_00002-8-1 loss: 1.021812  [   64/  124]
train() client id: f_00002-8-2 loss: 0.970338  [   96/  124]
train() client id: f_00002-9-0 loss: 0.964416  [   32/  124]
train() client id: f_00002-9-1 loss: 0.996152  [   64/  124]
train() client id: f_00002-9-2 loss: 1.127668  [   96/  124]
train() client id: f_00002-10-0 loss: 0.929500  [   32/  124]
train() client id: f_00002-10-1 loss: 0.965805  [   64/  124]
train() client id: f_00002-10-2 loss: 1.017667  [   96/  124]
train() client id: f_00002-11-0 loss: 0.986248  [   32/  124]
train() client id: f_00002-11-1 loss: 0.933072  [   64/  124]
train() client id: f_00002-11-2 loss: 1.056575  [   96/  124]
train() client id: f_00003-0-0 loss: 0.894134  [   32/   43]
train() client id: f_00003-1-0 loss: 0.852767  [   32/   43]
train() client id: f_00003-2-0 loss: 0.906463  [   32/   43]
train() client id: f_00003-3-0 loss: 0.756127  [   32/   43]
train() client id: f_00003-4-0 loss: 0.936928  [   32/   43]
train() client id: f_00003-5-0 loss: 0.895523  [   32/   43]
train() client id: f_00003-6-0 loss: 0.876860  [   32/   43]
train() client id: f_00003-7-0 loss: 0.958450  [   32/   43]
train() client id: f_00003-8-0 loss: 0.799657  [   32/   43]
train() client id: f_00003-9-0 loss: 0.879628  [   32/   43]
train() client id: f_00003-10-0 loss: 0.923868  [   32/   43]
train() client id: f_00003-11-0 loss: 0.854914  [   32/   43]
train() client id: f_00004-0-0 loss: 1.000957  [   32/  306]
train() client id: f_00004-0-1 loss: 1.191890  [   64/  306]
train() client id: f_00004-0-2 loss: 1.131455  [   96/  306]
train() client id: f_00004-0-3 loss: 0.808685  [  128/  306]
train() client id: f_00004-0-4 loss: 0.934974  [  160/  306]
train() client id: f_00004-0-5 loss: 0.848612  [  192/  306]
train() client id: f_00004-0-6 loss: 0.856579  [  224/  306]
train() client id: f_00004-0-7 loss: 1.101244  [  256/  306]
train() client id: f_00004-0-8 loss: 0.856727  [  288/  306]
train() client id: f_00004-1-0 loss: 0.935141  [   32/  306]
train() client id: f_00004-1-1 loss: 0.973499  [   64/  306]
train() client id: f_00004-1-2 loss: 0.974002  [   96/  306]
train() client id: f_00004-1-3 loss: 0.895936  [  128/  306]
train() client id: f_00004-1-4 loss: 0.959560  [  160/  306]
train() client id: f_00004-1-5 loss: 1.017875  [  192/  306]
train() client id: f_00004-1-6 loss: 0.943861  [  224/  306]
train() client id: f_00004-1-7 loss: 0.986198  [  256/  306]
train() client id: f_00004-1-8 loss: 1.002730  [  288/  306]
train() client id: f_00004-2-0 loss: 0.910425  [   32/  306]
train() client id: f_00004-2-1 loss: 0.920407  [   64/  306]
train() client id: f_00004-2-2 loss: 0.919932  [   96/  306]
train() client id: f_00004-2-3 loss: 1.179144  [  128/  306]
train() client id: f_00004-2-4 loss: 0.909816  [  160/  306]
train() client id: f_00004-2-5 loss: 0.765463  [  192/  306]
train() client id: f_00004-2-6 loss: 1.055139  [  224/  306]
train() client id: f_00004-2-7 loss: 1.031383  [  256/  306]
train() client id: f_00004-2-8 loss: 0.981404  [  288/  306]
train() client id: f_00004-3-0 loss: 0.898178  [   32/  306]
train() client id: f_00004-3-1 loss: 0.958545  [   64/  306]
train() client id: f_00004-3-2 loss: 0.939256  [   96/  306]
train() client id: f_00004-3-3 loss: 0.899782  [  128/  306]
train() client id: f_00004-3-4 loss: 0.916172  [  160/  306]
train() client id: f_00004-3-5 loss: 0.992581  [  192/  306]
train() client id: f_00004-3-6 loss: 0.931005  [  224/  306]
train() client id: f_00004-3-7 loss: 1.030482  [  256/  306]
train() client id: f_00004-3-8 loss: 0.992178  [  288/  306]
train() client id: f_00004-4-0 loss: 0.917622  [   32/  306]
train() client id: f_00004-4-1 loss: 0.902478  [   64/  306]
train() client id: f_00004-4-2 loss: 0.990703  [   96/  306]
train() client id: f_00004-4-3 loss: 0.957147  [  128/  306]
train() client id: f_00004-4-4 loss: 0.960644  [  160/  306]
train() client id: f_00004-4-5 loss: 0.852865  [  192/  306]
train() client id: f_00004-4-6 loss: 0.976083  [  224/  306]
train() client id: f_00004-4-7 loss: 1.007416  [  256/  306]
train() client id: f_00004-4-8 loss: 1.035879  [  288/  306]
train() client id: f_00004-5-0 loss: 0.827818  [   32/  306]
train() client id: f_00004-5-1 loss: 0.890013  [   64/  306]
train() client id: f_00004-5-2 loss: 0.992506  [   96/  306]
train() client id: f_00004-5-3 loss: 0.922306  [  128/  306]
train() client id: f_00004-5-4 loss: 1.009962  [  160/  306]
train() client id: f_00004-5-5 loss: 0.958309  [  192/  306]
train() client id: f_00004-5-6 loss: 1.003225  [  224/  306]
train() client id: f_00004-5-7 loss: 0.929377  [  256/  306]
train() client id: f_00004-5-8 loss: 0.973251  [  288/  306]
train() client id: f_00004-6-0 loss: 0.927770  [   32/  306]
train() client id: f_00004-6-1 loss: 0.874056  [   64/  306]
train() client id: f_00004-6-2 loss: 0.966154  [   96/  306]
train() client id: f_00004-6-3 loss: 1.007636  [  128/  306]
train() client id: f_00004-6-4 loss: 1.047893  [  160/  306]
train() client id: f_00004-6-5 loss: 0.907063  [  192/  306]
train() client id: f_00004-6-6 loss: 0.979107  [  224/  306]
train() client id: f_00004-6-7 loss: 0.923831  [  256/  306]
train() client id: f_00004-6-8 loss: 0.952382  [  288/  306]
train() client id: f_00004-7-0 loss: 0.935025  [   32/  306]
train() client id: f_00004-7-1 loss: 0.929327  [   64/  306]
train() client id: f_00004-7-2 loss: 0.866676  [   96/  306]
train() client id: f_00004-7-3 loss: 1.030865  [  128/  306]
train() client id: f_00004-7-4 loss: 1.016498  [  160/  306]
train() client id: f_00004-7-5 loss: 1.021958  [  192/  306]
train() client id: f_00004-7-6 loss: 0.957765  [  224/  306]
train() client id: f_00004-7-7 loss: 0.853309  [  256/  306]
train() client id: f_00004-7-8 loss: 0.891207  [  288/  306]
train() client id: f_00004-8-0 loss: 0.851613  [   32/  306]
train() client id: f_00004-8-1 loss: 0.927540  [   64/  306]
train() client id: f_00004-8-2 loss: 1.180033  [   96/  306]
train() client id: f_00004-8-3 loss: 0.927695  [  128/  306]
train() client id: f_00004-8-4 loss: 0.970440  [  160/  306]
train() client id: f_00004-8-5 loss: 0.847764  [  192/  306]
train() client id: f_00004-8-6 loss: 0.926738  [  224/  306]
train() client id: f_00004-8-7 loss: 0.858746  [  256/  306]
train() client id: f_00004-8-8 loss: 0.912351  [  288/  306]
train() client id: f_00004-9-0 loss: 0.846553  [   32/  306]
train() client id: f_00004-9-1 loss: 0.994032  [   64/  306]
train() client id: f_00004-9-2 loss: 1.006260  [   96/  306]
train() client id: f_00004-9-3 loss: 0.890345  [  128/  306]
train() client id: f_00004-9-4 loss: 0.972151  [  160/  306]
train() client id: f_00004-9-5 loss: 0.937033  [  192/  306]
train() client id: f_00004-9-6 loss: 1.017350  [  224/  306]
train() client id: f_00004-9-7 loss: 0.826739  [  256/  306]
train() client id: f_00004-9-8 loss: 0.912173  [  288/  306]
train() client id: f_00004-10-0 loss: 0.947327  [   32/  306]
train() client id: f_00004-10-1 loss: 0.988525  [   64/  306]
train() client id: f_00004-10-2 loss: 0.776761  [   96/  306]
train() client id: f_00004-10-3 loss: 0.898382  [  128/  306]
train() client id: f_00004-10-4 loss: 1.021721  [  160/  306]
train() client id: f_00004-10-5 loss: 0.920811  [  192/  306]
train() client id: f_00004-10-6 loss: 0.920402  [  224/  306]
train() client id: f_00004-10-7 loss: 0.975442  [  256/  306]
train() client id: f_00004-10-8 loss: 0.896096  [  288/  306]
train() client id: f_00004-11-0 loss: 0.894949  [   32/  306]
train() client id: f_00004-11-1 loss: 0.954632  [   64/  306]
train() client id: f_00004-11-2 loss: 1.042289  [   96/  306]
train() client id: f_00004-11-3 loss: 1.007505  [  128/  306]
train() client id: f_00004-11-4 loss: 0.853352  [  160/  306]
train() client id: f_00004-11-5 loss: 0.884270  [  192/  306]
train() client id: f_00004-11-6 loss: 0.804567  [  224/  306]
train() client id: f_00004-11-7 loss: 0.924217  [  256/  306]
train() client id: f_00004-11-8 loss: 1.049346  [  288/  306]
train() client id: f_00005-0-0 loss: 0.595669  [   32/  146]
train() client id: f_00005-0-1 loss: 0.613493  [   64/  146]
train() client id: f_00005-0-2 loss: 0.652332  [   96/  146]
train() client id: f_00005-0-3 loss: 0.346144  [  128/  146]
train() client id: f_00005-1-0 loss: 0.475632  [   32/  146]
train() client id: f_00005-1-1 loss: 0.521286  [   64/  146]
train() client id: f_00005-1-2 loss: 0.473450  [   96/  146]
train() client id: f_00005-1-3 loss: 0.583398  [  128/  146]
train() client id: f_00005-2-0 loss: 0.655019  [   32/  146]
train() client id: f_00005-2-1 loss: 0.377014  [   64/  146]
train() client id: f_00005-2-2 loss: 0.592978  [   96/  146]
train() client id: f_00005-2-3 loss: 0.474511  [  128/  146]
train() client id: f_00005-3-0 loss: 0.559450  [   32/  146]
train() client id: f_00005-3-1 loss: 0.704380  [   64/  146]
train() client id: f_00005-3-2 loss: 0.395349  [   96/  146]
train() client id: f_00005-3-3 loss: 0.381924  [  128/  146]
train() client id: f_00005-4-0 loss: 0.477379  [   32/  146]
train() client id: f_00005-4-1 loss: 0.550907  [   64/  146]
train() client id: f_00005-4-2 loss: 0.557210  [   96/  146]
train() client id: f_00005-4-3 loss: 0.518712  [  128/  146]
train() client id: f_00005-5-0 loss: 0.560825  [   32/  146]
train() client id: f_00005-5-1 loss: 0.415398  [   64/  146]
train() client id: f_00005-5-2 loss: 0.474845  [   96/  146]
train() client id: f_00005-5-3 loss: 0.593572  [  128/  146]
train() client id: f_00005-6-0 loss: 0.337068  [   32/  146]
train() client id: f_00005-6-1 loss: 0.529881  [   64/  146]
train() client id: f_00005-6-2 loss: 0.488118  [   96/  146]
train() client id: f_00005-6-3 loss: 0.591203  [  128/  146]
train() client id: f_00005-7-0 loss: 0.708121  [   32/  146]
train() client id: f_00005-7-1 loss: 0.424004  [   64/  146]
train() client id: f_00005-7-2 loss: 0.317622  [   96/  146]
train() client id: f_00005-7-3 loss: 0.592784  [  128/  146]
train() client id: f_00005-8-0 loss: 0.452026  [   32/  146]
train() client id: f_00005-8-1 loss: 0.342976  [   64/  146]
train() client id: f_00005-8-2 loss: 0.678569  [   96/  146]
train() client id: f_00005-8-3 loss: 0.426485  [  128/  146]
train() client id: f_00005-9-0 loss: 0.354654  [   32/  146]
train() client id: f_00005-9-1 loss: 0.922268  [   64/  146]
train() client id: f_00005-9-2 loss: 0.236705  [   96/  146]
train() client id: f_00005-9-3 loss: 0.485507  [  128/  146]
train() client id: f_00005-10-0 loss: 0.592955  [   32/  146]
train() client id: f_00005-10-1 loss: 0.357531  [   64/  146]
train() client id: f_00005-10-2 loss: 0.428716  [   96/  146]
train() client id: f_00005-10-3 loss: 0.556349  [  128/  146]
train() client id: f_00005-11-0 loss: 0.298563  [   32/  146]
train() client id: f_00005-11-1 loss: 0.286528  [   64/  146]
train() client id: f_00005-11-2 loss: 0.893143  [   96/  146]
train() client id: f_00005-11-3 loss: 0.523644  [  128/  146]
train() client id: f_00006-0-0 loss: 0.670416  [   32/   54]
train() client id: f_00006-1-0 loss: 0.695056  [   32/   54]
train() client id: f_00006-2-0 loss: 0.640455  [   32/   54]
train() client id: f_00006-3-0 loss: 0.732042  [   32/   54]
train() client id: f_00006-4-0 loss: 0.714982  [   32/   54]
train() client id: f_00006-5-0 loss: 0.732625  [   32/   54]
train() client id: f_00006-6-0 loss: 0.663357  [   32/   54]
train() client id: f_00006-7-0 loss: 0.688655  [   32/   54]
train() client id: f_00006-8-0 loss: 0.734864  [   32/   54]
train() client id: f_00006-9-0 loss: 0.679115  [   32/   54]
train() client id: f_00006-10-0 loss: 0.692785  [   32/   54]
train() client id: f_00006-11-0 loss: 0.690515  [   32/   54]
train() client id: f_00007-0-0 loss: 0.848014  [   32/  179]
train() client id: f_00007-0-1 loss: 0.590403  [   64/  179]
train() client id: f_00007-0-2 loss: 0.567129  [   96/  179]
train() client id: f_00007-0-3 loss: 0.659793  [  128/  179]
train() client id: f_00007-0-4 loss: 0.700976  [  160/  179]
train() client id: f_00007-1-0 loss: 0.659486  [   32/  179]
train() client id: f_00007-1-1 loss: 0.692622  [   64/  179]
train() client id: f_00007-1-2 loss: 0.602824  [   96/  179]
train() client id: f_00007-1-3 loss: 0.634844  [  128/  179]
train() client id: f_00007-1-4 loss: 0.637734  [  160/  179]
train() client id: f_00007-2-0 loss: 0.724945  [   32/  179]
train() client id: f_00007-2-1 loss: 0.569561  [   64/  179]
train() client id: f_00007-2-2 loss: 0.673424  [   96/  179]
train() client id: f_00007-2-3 loss: 0.548433  [  128/  179]
train() client id: f_00007-2-4 loss: 0.687696  [  160/  179]
train() client id: f_00007-3-0 loss: 0.774013  [   32/  179]
train() client id: f_00007-3-1 loss: 0.633819  [   64/  179]
train() client id: f_00007-3-2 loss: 0.656204  [   96/  179]
train() client id: f_00007-3-3 loss: 0.618323  [  128/  179]
train() client id: f_00007-3-4 loss: 0.534478  [  160/  179]
train() client id: f_00007-4-0 loss: 0.574632  [   32/  179]
train() client id: f_00007-4-1 loss: 0.526765  [   64/  179]
train() client id: f_00007-4-2 loss: 0.606337  [   96/  179]
train() client id: f_00007-4-3 loss: 0.676962  [  128/  179]
train() client id: f_00007-4-4 loss: 0.625953  [  160/  179]
train() client id: f_00007-5-0 loss: 0.724463  [   32/  179]
train() client id: f_00007-5-1 loss: 0.525048  [   64/  179]
train() client id: f_00007-5-2 loss: 0.625198  [   96/  179]
train() client id: f_00007-5-3 loss: 0.672817  [  128/  179]
train() client id: f_00007-5-4 loss: 0.553672  [  160/  179]
train() client id: f_00007-6-0 loss: 0.575485  [   32/  179]
train() client id: f_00007-6-1 loss: 0.701008  [   64/  179]
train() client id: f_00007-6-2 loss: 0.679500  [   96/  179]
train() client id: f_00007-6-3 loss: 0.532209  [  128/  179]
train() client id: f_00007-6-4 loss: 0.671616  [  160/  179]
train() client id: f_00007-7-0 loss: 0.576291  [   32/  179]
train() client id: f_00007-7-1 loss: 0.611049  [   64/  179]
train() client id: f_00007-7-2 loss: 0.628373  [   96/  179]
train() client id: f_00007-7-3 loss: 0.749125  [  128/  179]
train() client id: f_00007-7-4 loss: 0.586170  [  160/  179]
train() client id: f_00007-8-0 loss: 0.785505  [   32/  179]
train() client id: f_00007-8-1 loss: 0.623003  [   64/  179]
train() client id: f_00007-8-2 loss: 0.511244  [   96/  179]
train() client id: f_00007-8-3 loss: 0.534637  [  128/  179]
train() client id: f_00007-8-4 loss: 0.639071  [  160/  179]
train() client id: f_00007-9-0 loss: 0.724951  [   32/  179]
train() client id: f_00007-9-1 loss: 0.603994  [   64/  179]
train() client id: f_00007-9-2 loss: 0.690564  [   96/  179]
train() client id: f_00007-9-3 loss: 0.593819  [  128/  179]
train() client id: f_00007-9-4 loss: 0.582128  [  160/  179]
train() client id: f_00007-10-0 loss: 0.504213  [   32/  179]
train() client id: f_00007-10-1 loss: 0.732773  [   64/  179]
train() client id: f_00007-10-2 loss: 0.569691  [   96/  179]
train() client id: f_00007-10-3 loss: 0.733201  [  128/  179]
train() client id: f_00007-10-4 loss: 0.608749  [  160/  179]
train() client id: f_00007-11-0 loss: 0.666002  [   32/  179]
train() client id: f_00007-11-1 loss: 0.510131  [   64/  179]
train() client id: f_00007-11-2 loss: 0.675428  [   96/  179]
train() client id: f_00007-11-3 loss: 0.663141  [  128/  179]
train() client id: f_00007-11-4 loss: 0.530610  [  160/  179]
train() client id: f_00008-0-0 loss: 0.784812  [   32/  130]
train() client id: f_00008-0-1 loss: 0.822998  [   64/  130]
train() client id: f_00008-0-2 loss: 0.737168  [   96/  130]
train() client id: f_00008-0-3 loss: 0.658619  [  128/  130]
train() client id: f_00008-1-0 loss: 0.715695  [   32/  130]
train() client id: f_00008-1-1 loss: 0.673895  [   64/  130]
train() client id: f_00008-1-2 loss: 0.813459  [   96/  130]
train() client id: f_00008-1-3 loss: 0.804273  [  128/  130]
train() client id: f_00008-2-0 loss: 0.779747  [   32/  130]
train() client id: f_00008-2-1 loss: 0.811522  [   64/  130]
train() client id: f_00008-2-2 loss: 0.625040  [   96/  130]
train() client id: f_00008-2-3 loss: 0.746359  [  128/  130]
train() client id: f_00008-3-0 loss: 0.866741  [   32/  130]
train() client id: f_00008-3-1 loss: 0.613708  [   64/  130]
train() client id: f_00008-3-2 loss: 0.672289  [   96/  130]
train() client id: f_00008-3-3 loss: 0.800378  [  128/  130]
train() client id: f_00008-4-0 loss: 0.767545  [   32/  130]
train() client id: f_00008-4-1 loss: 0.786636  [   64/  130]
train() client id: f_00008-4-2 loss: 0.707309  [   96/  130]
train() client id: f_00008-4-3 loss: 0.739819  [  128/  130]
train() client id: f_00008-5-0 loss: 0.621000  [   32/  130]
train() client id: f_00008-5-1 loss: 0.863999  [   64/  130]
train() client id: f_00008-5-2 loss: 0.767341  [   96/  130]
train() client id: f_00008-5-3 loss: 0.738579  [  128/  130]
train() client id: f_00008-6-0 loss: 0.734467  [   32/  130]
train() client id: f_00008-6-1 loss: 0.706091  [   64/  130]
train() client id: f_00008-6-2 loss: 0.876228  [   96/  130]
train() client id: f_00008-6-3 loss: 0.671093  [  128/  130]
train() client id: f_00008-7-0 loss: 0.758365  [   32/  130]
train() client id: f_00008-7-1 loss: 0.740420  [   64/  130]
train() client id: f_00008-7-2 loss: 0.681693  [   96/  130]
train() client id: f_00008-7-3 loss: 0.745484  [  128/  130]
train() client id: f_00008-8-0 loss: 0.763321  [   32/  130]
train() client id: f_00008-8-1 loss: 0.661871  [   64/  130]
train() client id: f_00008-8-2 loss: 0.843572  [   96/  130]
train() client id: f_00008-8-3 loss: 0.718245  [  128/  130]
train() client id: f_00008-9-0 loss: 0.672858  [   32/  130]
train() client id: f_00008-9-1 loss: 0.709612  [   64/  130]
train() client id: f_00008-9-2 loss: 0.779706  [   96/  130]
train() client id: f_00008-9-3 loss: 0.815953  [  128/  130]
train() client id: f_00008-10-0 loss: 0.640842  [   32/  130]
train() client id: f_00008-10-1 loss: 0.828163  [   64/  130]
train() client id: f_00008-10-2 loss: 0.744032  [   96/  130]
train() client id: f_00008-10-3 loss: 0.765719  [  128/  130]
train() client id: f_00008-11-0 loss: 0.710028  [   32/  130]
train() client id: f_00008-11-1 loss: 0.822636  [   64/  130]
train() client id: f_00008-11-2 loss: 0.668157  [   96/  130]
train() client id: f_00008-11-3 loss: 0.794541  [  128/  130]
train() client id: f_00009-0-0 loss: 1.225905  [   32/  118]
train() client id: f_00009-0-1 loss: 1.277461  [   64/  118]
train() client id: f_00009-0-2 loss: 1.279307  [   96/  118]
train() client id: f_00009-1-0 loss: 1.060449  [   32/  118]
train() client id: f_00009-1-1 loss: 1.209561  [   64/  118]
train() client id: f_00009-1-2 loss: 1.306627  [   96/  118]
train() client id: f_00009-2-0 loss: 1.138770  [   32/  118]
train() client id: f_00009-2-1 loss: 1.114407  [   64/  118]
train() client id: f_00009-2-2 loss: 1.148253  [   96/  118]
train() client id: f_00009-3-0 loss: 1.087391  [   32/  118]
train() client id: f_00009-3-1 loss: 1.127082  [   64/  118]
train() client id: f_00009-3-2 loss: 1.050668  [   96/  118]
train() client id: f_00009-4-0 loss: 1.068331  [   32/  118]
train() client id: f_00009-4-1 loss: 1.001996  [   64/  118]
train() client id: f_00009-4-2 loss: 1.005915  [   96/  118]
train() client id: f_00009-5-0 loss: 0.949873  [   32/  118]
train() client id: f_00009-5-1 loss: 1.158290  [   64/  118]
train() client id: f_00009-5-2 loss: 0.977388  [   96/  118]
train() client id: f_00009-6-0 loss: 1.101353  [   32/  118]
train() client id: f_00009-6-1 loss: 0.950850  [   64/  118]
train() client id: f_00009-6-2 loss: 0.892318  [   96/  118]
train() client id: f_00009-7-0 loss: 1.021663  [   32/  118]
train() client id: f_00009-7-1 loss: 1.007523  [   64/  118]
train() client id: f_00009-7-2 loss: 0.932464  [   96/  118]
train() client id: f_00009-8-0 loss: 0.925434  [   32/  118]
train() client id: f_00009-8-1 loss: 1.036380  [   64/  118]
train() client id: f_00009-8-2 loss: 0.807743  [   96/  118]
train() client id: f_00009-9-0 loss: 0.910288  [   32/  118]
train() client id: f_00009-9-1 loss: 1.003005  [   64/  118]
train() client id: f_00009-9-2 loss: 0.931265  [   96/  118]
train() client id: f_00009-10-0 loss: 0.953203  [   32/  118]
train() client id: f_00009-10-1 loss: 0.883253  [   64/  118]
train() client id: f_00009-10-2 loss: 0.888437  [   96/  118]
train() client id: f_00009-11-0 loss: 0.838511  [   32/  118]
train() client id: f_00009-11-1 loss: 0.886739  [   64/  118]
train() client id: f_00009-11-2 loss: 0.913479  [   96/  118]
At round 13 accuracy: 0.6312997347480106
At round 13 training accuracy: 0.5714285714285714
At round 13 training loss: 0.8562019126247817
update_location
xs = [ -3.9056584    4.20031788  85.00902392  18.81129433   0.97929623
   3.95640986 -47.44319194 -26.32485185  69.66397685 -12.06087855]
ys = [ 77.5879595   60.55583871   1.32061395 -47.45517586  39.35018685
  22.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [126.62995549 116.98141849 131.25653572 112.27581445 107.46811725
 102.64569476 110.71470997 103.41022232 123.13301606 100.80414995]
dists_bs = [196.88163795 212.62695027 312.6807024  295.04088374 222.19267429
 234.91685624 218.6809876  229.00251994 290.93810241 236.19832881]
uav_gains = [5.54132201e-11 6.75605000e-11 5.06550612e-11 7.48645336e-11
 8.35211431e-11 9.36797744e-11 7.75317969e-11 9.19578362e-11
 5.94337263e-11 9.80171753e-11]
bs_gains = [4.16408405e-11 3.35708871e-11 1.14027884e-11 1.34160270e-11
 2.96791066e-11 2.53941909e-11 3.10329607e-11 2.72735301e-11
 1.39525118e-11 2.50103050e-11]
Round 14
-------------------------------
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.14357806 19.06538973  9.00414718  3.22065168 21.99169094 10.5993586
  4.00331986 12.90419715  9.4917528   8.60518442]
obj_prev = 108.02927042739734
eta_min = 6.894580279323513e-11	eta_max = 0.920963466545226
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 25.120031493789718	eta = 0.9090909090909091
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 43.56050107516789	eta = 0.5242453990066629
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 34.74720289395531	eta = 0.6572152681404528
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 33.166400765192755	eta = 0.6885399603277947
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 33.08814089362885	eta = 0.6901684908951391
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 33.08793538155577	eta = 0.6901727775922599
eta = 0.6901727775922599
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [0.03059863 0.06435425 0.03011293 0.01044239 0.07431095 0.03545555
 0.0131137  0.04346948 0.03157001 0.02865586]
ene_total = [2.84199716 5.44950306 2.81511635 1.29013739 6.21774054 3.30722781
 1.48846279 3.76427446 3.1172366  2.79623923]
ti_comp = [0.31764397 0.30976147 0.31634818 0.32166983 0.30755338 0.30458011
 0.32210904 0.32417046 0.29089448 0.30427822]
ti_coms = [0.07051759 0.0784001  0.07181339 0.06649173 0.08060818 0.08358146
 0.06605252 0.06399111 0.09726708 0.08388335]
t_total = [29.29994125 29.29994125 29.29994125 29.29994125 29.29994125 29.29994125
 29.29994125 29.29994125 29.29994125 29.29994125]
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [1.77461660e-05 1.73602902e-04 1.70533033e-05 6.87794728e-07
 2.71142377e-04 3.00282236e-05 1.35846854e-06 4.88524522e-05
 2.32398452e-05 1.58846785e-05]
ene_total = [0.53363372 0.6048986  0.54336256 0.50195733 0.62892872 0.63317194
 0.49869262 0.4867173  0.73596393 0.63438309]
optimize_network iter = 0 obj = 5.8017097996113725
eta = 0.6901727775922599
freqs = [4.81649713e+07 1.03877111e+08 4.75946046e+07 1.62315316e+07
 1.20809836e+08 5.82039824e+07 2.03559896e+07 6.70472512e+07
 5.42636806e+07 4.70882466e+07]
eta_min = 0.6901727775922659	eta_max = 0.6901727775922564
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 0.041597769241550876	eta = 0.9090909090909091
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 19.658200800192578	eta = 0.0019236833645317572
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 2.023882036535733	eta = 0.01868495948542783
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 1.9708396260580614	eta = 0.01918783921124654
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 1.9708261485414031	eta = 0.01918797042749958
eta = 0.01918797042749958
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [1.89202933e-04 1.85088870e-03 1.81815891e-04 7.33300812e-06
 2.89081782e-03 3.20149601e-04 1.44834795e-05 5.20846430e-04
 2.47774470e-04 1.69356454e-04]
ene_total = [0.17265704 0.23107452 0.17557065 0.15872099 0.26113616 0.20692952
 0.15784421 0.16500284 0.23783644 0.20405377]
ti_comp = [0.31764397 0.30976147 0.31634818 0.32166983 0.30755338 0.30458011
 0.32210904 0.32417046 0.29089448 0.30427822]
ti_coms = [0.07051759 0.0784001  0.07181339 0.06649173 0.08060818 0.08358146
 0.06605252 0.06399111 0.09726708 0.08388335]
t_total = [29.29994125 29.29994125 29.29994125 29.29994125 29.29994125 29.29994125
 29.29994125 29.29994125 29.29994125 29.29994125]
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [1.77461660e-05 1.73602902e-04 1.70533033e-05 6.87794728e-07
 2.71142377e-04 3.00282236e-05 1.35846854e-06 4.88524522e-05
 2.32398452e-05 1.58846785e-05]
ene_total = [0.53363372 0.6048986  0.54336256 0.50195733 0.62892872 0.63317194
 0.49869262 0.4867173  0.73596393 0.63438309]
optimize_network iter = 1 obj = 5.801709799611484
eta = 0.6901727775922659
freqs = [4.81649713e+07 1.03877111e+08 4.75946046e+07 1.62315316e+07
 1.20809836e+08 5.82039824e+07 2.03559896e+07 6.70472512e+07
 5.42636806e+07 4.70882466e+07]
Done!
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [1.75381754e-05 1.71568221e-04 1.68534331e-05 6.79733555e-07
 2.67964503e-04 2.96762833e-05 1.34254685e-06 4.82798860e-05
 2.29674669e-05 1.56985050e-05]
ene_total = [0.0070693  0.00801158 0.00719819 0.00664985 0.00832878 0.00838782
 0.00660659 0.00644739 0.00974968 0.00840403]
At round 14 energy consumption: 0.0768532209744656
At round 14 eta: 0.6901727775922659
At round 14 a_n: 23.044415151714617
At round 14 local rounds: 12.142311740979952
At round 14 global rounds: 75.48388039288098
gradient difference: 0.47397980093955994
train() client id: f_00000-0-0 loss: 1.216918  [   32/  126]
train() client id: f_00000-0-1 loss: 1.120495  [   64/  126]
train() client id: f_00000-0-2 loss: 1.254956  [   96/  126]
train() client id: f_00000-1-0 loss: 1.035410  [   32/  126]
train() client id: f_00000-1-1 loss: 1.115328  [   64/  126]
train() client id: f_00000-1-2 loss: 1.310854  [   96/  126]
train() client id: f_00000-2-0 loss: 1.232879  [   32/  126]
train() client id: f_00000-2-1 loss: 1.101193  [   64/  126]
train() client id: f_00000-2-2 loss: 0.978348  [   96/  126]
train() client id: f_00000-3-0 loss: 1.029428  [   32/  126]
train() client id: f_00000-3-1 loss: 1.103038  [   64/  126]
train() client id: f_00000-3-2 loss: 0.846995  [   96/  126]
train() client id: f_00000-4-0 loss: 0.985288  [   32/  126]
train() client id: f_00000-4-1 loss: 0.840120  [   64/  126]
train() client id: f_00000-4-2 loss: 0.935788  [   96/  126]
train() client id: f_00000-5-0 loss: 0.897157  [   32/  126]
train() client id: f_00000-5-1 loss: 0.879907  [   64/  126]
train() client id: f_00000-5-2 loss: 0.933433  [   96/  126]
train() client id: f_00000-6-0 loss: 0.886271  [   32/  126]
train() client id: f_00000-6-1 loss: 0.885348  [   64/  126]
train() client id: f_00000-6-2 loss: 0.882514  [   96/  126]
train() client id: f_00000-7-0 loss: 0.860840  [   32/  126]
train() client id: f_00000-7-1 loss: 0.803876  [   64/  126]
train() client id: f_00000-7-2 loss: 0.904727  [   96/  126]
train() client id: f_00000-8-0 loss: 0.812987  [   32/  126]
train() client id: f_00000-8-1 loss: 0.841855  [   64/  126]
train() client id: f_00000-8-2 loss: 0.850694  [   96/  126]
train() client id: f_00000-9-0 loss: 0.814851  [   32/  126]
train() client id: f_00000-9-1 loss: 0.824301  [   64/  126]
train() client id: f_00000-9-2 loss: 0.941701  [   96/  126]
train() client id: f_00000-10-0 loss: 0.845481  [   32/  126]
train() client id: f_00000-10-1 loss: 0.850567  [   64/  126]
train() client id: f_00000-10-2 loss: 0.914896  [   96/  126]
train() client id: f_00000-11-0 loss: 0.796979  [   32/  126]
train() client id: f_00000-11-1 loss: 0.869242  [   64/  126]
train() client id: f_00000-11-2 loss: 0.864232  [   96/  126]
train() client id: f_00001-0-0 loss: 0.501202  [   32/  265]
train() client id: f_00001-0-1 loss: 0.511626  [   64/  265]
train() client id: f_00001-0-2 loss: 0.458264  [   96/  265]
train() client id: f_00001-0-3 loss: 0.644946  [  128/  265]
train() client id: f_00001-0-4 loss: 0.517948  [  160/  265]
train() client id: f_00001-0-5 loss: 0.476936  [  192/  265]
train() client id: f_00001-0-6 loss: 0.554635  [  224/  265]
train() client id: f_00001-0-7 loss: 0.630763  [  256/  265]
train() client id: f_00001-1-0 loss: 0.445140  [   32/  265]
train() client id: f_00001-1-1 loss: 0.506477  [   64/  265]
train() client id: f_00001-1-2 loss: 0.591517  [   96/  265]
train() client id: f_00001-1-3 loss: 0.530476  [  128/  265]
train() client id: f_00001-1-4 loss: 0.626284  [  160/  265]
train() client id: f_00001-1-5 loss: 0.519231  [  192/  265]
train() client id: f_00001-1-6 loss: 0.476916  [  224/  265]
train() client id: f_00001-1-7 loss: 0.556804  [  256/  265]
train() client id: f_00001-2-0 loss: 0.516418  [   32/  265]
train() client id: f_00001-2-1 loss: 0.612592  [   64/  265]
train() client id: f_00001-2-2 loss: 0.503592  [   96/  265]
train() client id: f_00001-2-3 loss: 0.493534  [  128/  265]
train() client id: f_00001-2-4 loss: 0.635757  [  160/  265]
train() client id: f_00001-2-5 loss: 0.443043  [  192/  265]
train() client id: f_00001-2-6 loss: 0.484274  [  224/  265]
train() client id: f_00001-2-7 loss: 0.517913  [  256/  265]
train() client id: f_00001-3-0 loss: 0.429179  [   32/  265]
train() client id: f_00001-3-1 loss: 0.536041  [   64/  265]
train() client id: f_00001-3-2 loss: 0.503924  [   96/  265]
train() client id: f_00001-3-3 loss: 0.570716  [  128/  265]
train() client id: f_00001-3-4 loss: 0.430702  [  160/  265]
train() client id: f_00001-3-5 loss: 0.429315  [  192/  265]
train() client id: f_00001-3-6 loss: 0.690220  [  224/  265]
train() client id: f_00001-3-7 loss: 0.490037  [  256/  265]
train() client id: f_00001-4-0 loss: 0.466896  [   32/  265]
train() client id: f_00001-4-1 loss: 0.552995  [   64/  265]
train() client id: f_00001-4-2 loss: 0.518900  [   96/  265]
train() client id: f_00001-4-3 loss: 0.478744  [  128/  265]
train() client id: f_00001-4-4 loss: 0.469440  [  160/  265]
train() client id: f_00001-4-5 loss: 0.505659  [  192/  265]
train() client id: f_00001-4-6 loss: 0.562125  [  224/  265]
train() client id: f_00001-4-7 loss: 0.549415  [  256/  265]
train() client id: f_00001-5-0 loss: 0.497729  [   32/  265]
train() client id: f_00001-5-1 loss: 0.479796  [   64/  265]
train() client id: f_00001-5-2 loss: 0.475978  [   96/  265]
train() client id: f_00001-5-3 loss: 0.416759  [  128/  265]
train() client id: f_00001-5-4 loss: 0.615089  [  160/  265]
train() client id: f_00001-5-5 loss: 0.512281  [  192/  265]
train() client id: f_00001-5-6 loss: 0.559061  [  224/  265]
train() client id: f_00001-5-7 loss: 0.533600  [  256/  265]
train() client id: f_00001-6-0 loss: 0.486275  [   32/  265]
train() client id: f_00001-6-1 loss: 0.546710  [   64/  265]
train() client id: f_00001-6-2 loss: 0.465856  [   96/  265]
train() client id: f_00001-6-3 loss: 0.504165  [  128/  265]
train() client id: f_00001-6-4 loss: 0.480989  [  160/  265]
train() client id: f_00001-6-5 loss: 0.463939  [  192/  265]
train() client id: f_00001-6-6 loss: 0.524262  [  224/  265]
train() client id: f_00001-6-7 loss: 0.505727  [  256/  265]
train() client id: f_00001-7-0 loss: 0.584090  [   32/  265]
train() client id: f_00001-7-1 loss: 0.543539  [   64/  265]
train() client id: f_00001-7-2 loss: 0.500444  [   96/  265]
train() client id: f_00001-7-3 loss: 0.478923  [  128/  265]
train() client id: f_00001-7-4 loss: 0.467226  [  160/  265]
train() client id: f_00001-7-5 loss: 0.491553  [  192/  265]
train() client id: f_00001-7-6 loss: 0.479426  [  224/  265]
train() client id: f_00001-7-7 loss: 0.513489  [  256/  265]
train() client id: f_00001-8-0 loss: 0.566997  [   32/  265]
train() client id: f_00001-8-1 loss: 0.498597  [   64/  265]
train() client id: f_00001-8-2 loss: 0.503889  [   96/  265]
train() client id: f_00001-8-3 loss: 0.528969  [  128/  265]
train() client id: f_00001-8-4 loss: 0.542945  [  160/  265]
train() client id: f_00001-8-5 loss: 0.395069  [  192/  265]
train() client id: f_00001-8-6 loss: 0.484423  [  224/  265]
train() client id: f_00001-8-7 loss: 0.524923  [  256/  265]
train() client id: f_00001-9-0 loss: 0.413735  [   32/  265]
train() client id: f_00001-9-1 loss: 0.551874  [   64/  265]
train() client id: f_00001-9-2 loss: 0.477420  [   96/  265]
train() client id: f_00001-9-3 loss: 0.580415  [  128/  265]
train() client id: f_00001-9-4 loss: 0.513564  [  160/  265]
train() client id: f_00001-9-5 loss: 0.452910  [  192/  265]
train() client id: f_00001-9-6 loss: 0.525781  [  224/  265]
train() client id: f_00001-9-7 loss: 0.519580  [  256/  265]
train() client id: f_00001-10-0 loss: 0.476428  [   32/  265]
train() client id: f_00001-10-1 loss: 0.611772  [   64/  265]
train() client id: f_00001-10-2 loss: 0.474867  [   96/  265]
train() client id: f_00001-10-3 loss: 0.468425  [  128/  265]
train() client id: f_00001-10-4 loss: 0.469504  [  160/  265]
train() client id: f_00001-10-5 loss: 0.521957  [  192/  265]
train() client id: f_00001-10-6 loss: 0.456453  [  224/  265]
train() client id: f_00001-10-7 loss: 0.407859  [  256/  265]
train() client id: f_00001-11-0 loss: 0.528359  [   32/  265]
train() client id: f_00001-11-1 loss: 0.563464  [   64/  265]
train() client id: f_00001-11-2 loss: 0.534918  [   96/  265]
train() client id: f_00001-11-3 loss: 0.569632  [  128/  265]
train() client id: f_00001-11-4 loss: 0.504614  [  160/  265]
train() client id: f_00001-11-5 loss: 0.480560  [  192/  265]
train() client id: f_00001-11-6 loss: 0.408240  [  224/  265]
train() client id: f_00001-11-7 loss: 0.407158  [  256/  265]
train() client id: f_00002-0-0 loss: 1.181833  [   32/  124]
train() client id: f_00002-0-1 loss: 1.055884  [   64/  124]
train() client id: f_00002-0-2 loss: 1.134239  [   96/  124]
train() client id: f_00002-1-0 loss: 0.994972  [   32/  124]
train() client id: f_00002-1-1 loss: 1.078709  [   64/  124]
train() client id: f_00002-1-2 loss: 1.198172  [   96/  124]
train() client id: f_00002-2-0 loss: 1.154670  [   32/  124]
train() client id: f_00002-2-1 loss: 1.005401  [   64/  124]
train() client id: f_00002-2-2 loss: 1.028856  [   96/  124]
train() client id: f_00002-3-0 loss: 1.083083  [   32/  124]
train() client id: f_00002-3-1 loss: 1.068985  [   64/  124]
train() client id: f_00002-3-2 loss: 0.965950  [   96/  124]
train() client id: f_00002-4-0 loss: 1.077377  [   32/  124]
train() client id: f_00002-4-1 loss: 1.002361  [   64/  124]
train() client id: f_00002-4-2 loss: 0.947812  [   96/  124]
train() client id: f_00002-5-0 loss: 1.052465  [   32/  124]
train() client id: f_00002-5-1 loss: 1.003195  [   64/  124]
train() client id: f_00002-5-2 loss: 0.948335  [   96/  124]
train() client id: f_00002-6-0 loss: 1.000365  [   32/  124]
train() client id: f_00002-6-1 loss: 0.874857  [   64/  124]
train() client id: f_00002-6-2 loss: 0.908186  [   96/  124]
train() client id: f_00002-7-0 loss: 0.964635  [   32/  124]
train() client id: f_00002-7-1 loss: 0.953701  [   64/  124]
train() client id: f_00002-7-2 loss: 0.874181  [   96/  124]
train() client id: f_00002-8-0 loss: 0.943200  [   32/  124]
train() client id: f_00002-8-1 loss: 0.895871  [   64/  124]
train() client id: f_00002-8-2 loss: 0.879058  [   96/  124]
train() client id: f_00002-9-0 loss: 1.069981  [   32/  124]
train() client id: f_00002-9-1 loss: 0.908222  [   64/  124]
train() client id: f_00002-9-2 loss: 0.900499  [   96/  124]
train() client id: f_00002-10-0 loss: 1.015534  [   32/  124]
train() client id: f_00002-10-1 loss: 1.007877  [   64/  124]
train() client id: f_00002-10-2 loss: 0.911587  [   96/  124]
train() client id: f_00002-11-0 loss: 1.093354  [   32/  124]
train() client id: f_00002-11-1 loss: 0.887758  [   64/  124]
train() client id: f_00002-11-2 loss: 0.818217  [   96/  124]
train() client id: f_00003-0-0 loss: 0.802292  [   32/   43]
train() client id: f_00003-1-0 loss: 0.817989  [   32/   43]
train() client id: f_00003-2-0 loss: 0.934306  [   32/   43]
train() client id: f_00003-3-0 loss: 0.831858  [   32/   43]
train() client id: f_00003-4-0 loss: 0.796536  [   32/   43]
train() client id: f_00003-5-0 loss: 0.826413  [   32/   43]
train() client id: f_00003-6-0 loss: 0.735885  [   32/   43]
train() client id: f_00003-7-0 loss: 0.876658  [   32/   43]
train() client id: f_00003-8-0 loss: 1.036929  [   32/   43]
train() client id: f_00003-9-0 loss: 0.814711  [   32/   43]
train() client id: f_00003-10-0 loss: 0.852157  [   32/   43]
train() client id: f_00003-11-0 loss: 0.820548  [   32/   43]
train() client id: f_00004-0-0 loss: 0.768525  [   32/  306]
train() client id: f_00004-0-1 loss: 0.764180  [   64/  306]
train() client id: f_00004-0-2 loss: 0.769969  [   96/  306]
train() client id: f_00004-0-3 loss: 0.688213  [  128/  306]
train() client id: f_00004-0-4 loss: 0.898678  [  160/  306]
train() client id: f_00004-0-5 loss: 0.776262  [  192/  306]
train() client id: f_00004-0-6 loss: 0.730815  [  224/  306]
train() client id: f_00004-0-7 loss: 0.728865  [  256/  306]
train() client id: f_00004-0-8 loss: 0.813631  [  288/  306]
train() client id: f_00004-1-0 loss: 0.720006  [   32/  306]
train() client id: f_00004-1-1 loss: 0.855198  [   64/  306]
train() client id: f_00004-1-2 loss: 0.884149  [   96/  306]
train() client id: f_00004-1-3 loss: 0.779836  [  128/  306]
train() client id: f_00004-1-4 loss: 0.912579  [  160/  306]
train() client id: f_00004-1-5 loss: 0.796492  [  192/  306]
train() client id: f_00004-1-6 loss: 0.607474  [  224/  306]
train() client id: f_00004-1-7 loss: 0.694037  [  256/  306]
train() client id: f_00004-1-8 loss: 0.746464  [  288/  306]
train() client id: f_00004-2-0 loss: 0.713930  [   32/  306]
train() client id: f_00004-2-1 loss: 0.775513  [   64/  306]
train() client id: f_00004-2-2 loss: 0.878463  [   96/  306]
train() client id: f_00004-2-3 loss: 0.717145  [  128/  306]
train() client id: f_00004-2-4 loss: 0.850708  [  160/  306]
train() client id: f_00004-2-5 loss: 0.654548  [  192/  306]
train() client id: f_00004-2-6 loss: 0.843851  [  224/  306]
train() client id: f_00004-2-7 loss: 0.838670  [  256/  306]
train() client id: f_00004-2-8 loss: 0.832695  [  288/  306]
train() client id: f_00004-3-0 loss: 0.692742  [   32/  306]
train() client id: f_00004-3-1 loss: 0.886534  [   64/  306]
train() client id: f_00004-3-2 loss: 0.932675  [   96/  306]
train() client id: f_00004-3-3 loss: 0.753338  [  128/  306]
train() client id: f_00004-3-4 loss: 0.766020  [  160/  306]
train() client id: f_00004-3-5 loss: 0.666351  [  192/  306]
train() client id: f_00004-3-6 loss: 0.953665  [  224/  306]
train() client id: f_00004-3-7 loss: 0.848720  [  256/  306]
train() client id: f_00004-3-8 loss: 0.704789  [  288/  306]
train() client id: f_00004-4-0 loss: 0.731680  [   32/  306]
train() client id: f_00004-4-1 loss: 0.809673  [   64/  306]
train() client id: f_00004-4-2 loss: 0.908335  [   96/  306]
train() client id: f_00004-4-3 loss: 0.820939  [  128/  306]
train() client id: f_00004-4-4 loss: 0.819769  [  160/  306]
train() client id: f_00004-4-5 loss: 0.667455  [  192/  306]
train() client id: f_00004-4-6 loss: 0.745937  [  224/  306]
train() client id: f_00004-4-7 loss: 0.795999  [  256/  306]
train() client id: f_00004-4-8 loss: 0.812835  [  288/  306]
train() client id: f_00004-5-0 loss: 0.858200  [   32/  306]
train() client id: f_00004-5-1 loss: 0.663387  [   64/  306]
train() client id: f_00004-5-2 loss: 0.594151  [   96/  306]
train() client id: f_00004-5-3 loss: 0.787507  [  128/  306]
train() client id: f_00004-5-4 loss: 0.747696  [  160/  306]
train() client id: f_00004-5-5 loss: 0.764510  [  192/  306]
train() client id: f_00004-5-6 loss: 0.865156  [  224/  306]
train() client id: f_00004-5-7 loss: 0.793356  [  256/  306]
train() client id: f_00004-5-8 loss: 0.838037  [  288/  306]
train() client id: f_00004-6-0 loss: 0.906041  [   32/  306]
train() client id: f_00004-6-1 loss: 0.786527  [   64/  306]
train() client id: f_00004-6-2 loss: 0.836493  [   96/  306]
train() client id: f_00004-6-3 loss: 0.771207  [  128/  306]
train() client id: f_00004-6-4 loss: 0.751373  [  160/  306]
train() client id: f_00004-6-5 loss: 0.831328  [  192/  306]
train() client id: f_00004-6-6 loss: 0.772519  [  224/  306]
train() client id: f_00004-6-7 loss: 0.774602  [  256/  306]
train() client id: f_00004-6-8 loss: 0.679482  [  288/  306]
train() client id: f_00004-7-0 loss: 0.822401  [   32/  306]
train() client id: f_00004-7-1 loss: 0.897507  [   64/  306]
train() client id: f_00004-7-2 loss: 0.797857  [   96/  306]
train() client id: f_00004-7-3 loss: 0.674499  [  128/  306]
train() client id: f_00004-7-4 loss: 0.758901  [  160/  306]
train() client id: f_00004-7-5 loss: 0.751276  [  192/  306]
train() client id: f_00004-7-6 loss: 0.736894  [  224/  306]
train() client id: f_00004-7-7 loss: 0.767032  [  256/  306]
train() client id: f_00004-7-8 loss: 0.844921  [  288/  306]
train() client id: f_00004-8-0 loss: 0.741191  [   32/  306]
train() client id: f_00004-8-1 loss: 0.747618  [   64/  306]
train() client id: f_00004-8-2 loss: 0.849254  [   96/  306]
train() client id: f_00004-8-3 loss: 0.842636  [  128/  306]
train() client id: f_00004-8-4 loss: 0.834609  [  160/  306]
train() client id: f_00004-8-5 loss: 0.740561  [  192/  306]
train() client id: f_00004-8-6 loss: 0.921584  [  224/  306]
train() client id: f_00004-8-7 loss: 0.762590  [  256/  306]
train() client id: f_00004-8-8 loss: 0.711774  [  288/  306]
train() client id: f_00004-9-0 loss: 0.807748  [   32/  306]
train() client id: f_00004-9-1 loss: 0.687678  [   64/  306]
train() client id: f_00004-9-2 loss: 0.809844  [   96/  306]
train() client id: f_00004-9-3 loss: 0.664146  [  128/  306]
train() client id: f_00004-9-4 loss: 0.829955  [  160/  306]
train() client id: f_00004-9-5 loss: 0.807873  [  192/  306]
train() client id: f_00004-9-6 loss: 0.888427  [  224/  306]
train() client id: f_00004-9-7 loss: 0.823625  [  256/  306]
train() client id: f_00004-9-8 loss: 0.785561  [  288/  306]
train() client id: f_00004-10-0 loss: 0.921161  [   32/  306]
train() client id: f_00004-10-1 loss: 0.883735  [   64/  306]
train() client id: f_00004-10-2 loss: 0.724543  [   96/  306]
train() client id: f_00004-10-3 loss: 0.690333  [  128/  306]
train() client id: f_00004-10-4 loss: 0.819240  [  160/  306]
train() client id: f_00004-10-5 loss: 0.789258  [  192/  306]
train() client id: f_00004-10-6 loss: 0.777034  [  224/  306]
train() client id: f_00004-10-7 loss: 0.768896  [  256/  306]
train() client id: f_00004-10-8 loss: 0.769268  [  288/  306]
train() client id: f_00004-11-0 loss: 0.859684  [   32/  306]
train() client id: f_00004-11-1 loss: 0.797006  [   64/  306]
train() client id: f_00004-11-2 loss: 0.883881  [   96/  306]
train() client id: f_00004-11-3 loss: 0.802512  [  128/  306]
train() client id: f_00004-11-4 loss: 0.749270  [  160/  306]
train() client id: f_00004-11-5 loss: 0.733297  [  192/  306]
train() client id: f_00004-11-6 loss: 0.766334  [  224/  306]
train() client id: f_00004-11-7 loss: 0.703858  [  256/  306]
train() client id: f_00004-11-8 loss: 0.908256  [  288/  306]
train() client id: f_00005-0-0 loss: 0.815237  [   32/  146]
train() client id: f_00005-0-1 loss: 0.686785  [   64/  146]
train() client id: f_00005-0-2 loss: 0.635975  [   96/  146]
train() client id: f_00005-0-3 loss: 0.554854  [  128/  146]
train() client id: f_00005-1-0 loss: 0.658905  [   32/  146]
train() client id: f_00005-1-1 loss: 0.780211  [   64/  146]
train() client id: f_00005-1-2 loss: 0.527915  [   96/  146]
train() client id: f_00005-1-3 loss: 0.747239  [  128/  146]
train() client id: f_00005-2-0 loss: 0.748871  [   32/  146]
train() client id: f_00005-2-1 loss: 0.648231  [   64/  146]
train() client id: f_00005-2-2 loss: 0.586050  [   96/  146]
train() client id: f_00005-2-3 loss: 0.690080  [  128/  146]
train() client id: f_00005-3-0 loss: 0.714525  [   32/  146]
train() client id: f_00005-3-1 loss: 0.701077  [   64/  146]
train() client id: f_00005-3-2 loss: 0.628517  [   96/  146]
train() client id: f_00005-3-3 loss: 0.596691  [  128/  146]
train() client id: f_00005-4-0 loss: 0.512037  [   32/  146]
train() client id: f_00005-4-1 loss: 0.689379  [   64/  146]
train() client id: f_00005-4-2 loss: 0.632452  [   96/  146]
train() client id: f_00005-4-3 loss: 0.815231  [  128/  146]
train() client id: f_00005-5-0 loss: 0.747581  [   32/  146]
train() client id: f_00005-5-1 loss: 0.897121  [   64/  146]
train() client id: f_00005-5-2 loss: 0.558973  [   96/  146]
train() client id: f_00005-5-3 loss: 0.535713  [  128/  146]
train() client id: f_00005-6-0 loss: 0.456061  [   32/  146]
train() client id: f_00005-6-1 loss: 0.847513  [   64/  146]
train() client id: f_00005-6-2 loss: 0.623032  [   96/  146]
train() client id: f_00005-6-3 loss: 0.696354  [  128/  146]
train() client id: f_00005-7-0 loss: 0.603140  [   32/  146]
train() client id: f_00005-7-1 loss: 0.596127  [   64/  146]
train() client id: f_00005-7-2 loss: 0.647389  [   96/  146]
train() client id: f_00005-7-3 loss: 0.811268  [  128/  146]
train() client id: f_00005-8-0 loss: 0.662035  [   32/  146]
train() client id: f_00005-8-1 loss: 0.522806  [   64/  146]
train() client id: f_00005-8-2 loss: 0.607457  [   96/  146]
train() client id: f_00005-8-3 loss: 0.736055  [  128/  146]
train() client id: f_00005-9-0 loss: 0.611017  [   32/  146]
train() client id: f_00005-9-1 loss: 0.755023  [   64/  146]
train() client id: f_00005-9-2 loss: 0.886118  [   96/  146]
train() client id: f_00005-9-3 loss: 0.551245  [  128/  146]
train() client id: f_00005-10-0 loss: 0.691203  [   32/  146]
train() client id: f_00005-10-1 loss: 0.583370  [   64/  146]
train() client id: f_00005-10-2 loss: 0.793529  [   96/  146]
train() client id: f_00005-10-3 loss: 0.549439  [  128/  146]
train() client id: f_00005-11-0 loss: 0.790214  [   32/  146]
train() client id: f_00005-11-1 loss: 0.614722  [   64/  146]
train() client id: f_00005-11-2 loss: 0.607685  [   96/  146]
train() client id: f_00005-11-3 loss: 0.608466  [  128/  146]
train() client id: f_00006-0-0 loss: 0.650826  [   32/   54]
train() client id: f_00006-1-0 loss: 0.653488  [   32/   54]
train() client id: f_00006-2-0 loss: 0.689429  [   32/   54]
train() client id: f_00006-3-0 loss: 0.648364  [   32/   54]
train() client id: f_00006-4-0 loss: 0.664936  [   32/   54]
train() client id: f_00006-5-0 loss: 0.661928  [   32/   54]
train() client id: f_00006-6-0 loss: 0.660462  [   32/   54]
train() client id: f_00006-7-0 loss: 0.637629  [   32/   54]
train() client id: f_00006-8-0 loss: 0.697654  [   32/   54]
train() client id: f_00006-9-0 loss: 0.709009  [   32/   54]
train() client id: f_00006-10-0 loss: 0.585424  [   32/   54]
train() client id: f_00006-11-0 loss: 0.648638  [   32/   54]
train() client id: f_00007-0-0 loss: 0.821359  [   32/  179]
train() client id: f_00007-0-1 loss: 0.704857  [   64/  179]
train() client id: f_00007-0-2 loss: 0.728808  [   96/  179]
train() client id: f_00007-0-3 loss: 0.709925  [  128/  179]
train() client id: f_00007-0-4 loss: 0.718143  [  160/  179]
train() client id: f_00007-1-0 loss: 0.782184  [   32/  179]
train() client id: f_00007-1-1 loss: 0.792394  [   64/  179]
train() client id: f_00007-1-2 loss: 0.686888  [   96/  179]
train() client id: f_00007-1-3 loss: 0.742377  [  128/  179]
train() client id: f_00007-1-4 loss: 0.737328  [  160/  179]
train() client id: f_00007-2-0 loss: 0.718720  [   32/  179]
train() client id: f_00007-2-1 loss: 0.806947  [   64/  179]
train() client id: f_00007-2-2 loss: 0.699575  [   96/  179]
train() client id: f_00007-2-3 loss: 0.628773  [  128/  179]
train() client id: f_00007-2-4 loss: 0.692962  [  160/  179]
train() client id: f_00007-3-0 loss: 0.694559  [   32/  179]
train() client id: f_00007-3-1 loss: 0.703744  [   64/  179]
train() client id: f_00007-3-2 loss: 0.755895  [   96/  179]
train() client id: f_00007-3-3 loss: 0.722260  [  128/  179]
train() client id: f_00007-3-4 loss: 0.785493  [  160/  179]
train() client id: f_00007-4-0 loss: 0.680998  [   32/  179]
train() client id: f_00007-4-1 loss: 0.903282  [   64/  179]
train() client id: f_00007-4-2 loss: 0.770568  [   96/  179]
train() client id: f_00007-4-3 loss: 0.618528  [  128/  179]
train() client id: f_00007-4-4 loss: 0.588630  [  160/  179]
train() client id: f_00007-5-0 loss: 0.665459  [   32/  179]
train() client id: f_00007-5-1 loss: 0.698044  [   64/  179]
train() client id: f_00007-5-2 loss: 0.695925  [   96/  179]
train() client id: f_00007-5-3 loss: 0.655122  [  128/  179]
train() client id: f_00007-5-4 loss: 0.853825  [  160/  179]
train() client id: f_00007-6-0 loss: 0.603041  [   32/  179]
train() client id: f_00007-6-1 loss: 0.696271  [   64/  179]
train() client id: f_00007-6-2 loss: 0.755940  [   96/  179]
train() client id: f_00007-6-3 loss: 0.669387  [  128/  179]
train() client id: f_00007-6-4 loss: 0.913889  [  160/  179]
train() client id: f_00007-7-0 loss: 0.606988  [   32/  179]
train() client id: f_00007-7-1 loss: 0.658742  [   64/  179]
train() client id: f_00007-7-2 loss: 0.827422  [   96/  179]
train() client id: f_00007-7-3 loss: 0.715696  [  128/  179]
train() client id: f_00007-7-4 loss: 0.742182  [  160/  179]
train() client id: f_00007-8-0 loss: 0.584595  [   32/  179]
train() client id: f_00007-8-1 loss: 0.688052  [   64/  179]
train() client id: f_00007-8-2 loss: 0.894636  [   96/  179]
train() client id: f_00007-8-3 loss: 0.789752  [  128/  179]
train() client id: f_00007-8-4 loss: 0.655693  [  160/  179]
train() client id: f_00007-9-0 loss: 0.797794  [   32/  179]
train() client id: f_00007-9-1 loss: 0.682414  [   64/  179]
train() client id: f_00007-9-2 loss: 0.576731  [   96/  179]
train() client id: f_00007-9-3 loss: 0.764979  [  128/  179]
train() client id: f_00007-9-4 loss: 0.674835  [  160/  179]
train() client id: f_00007-10-0 loss: 0.657107  [   32/  179]
train() client id: f_00007-10-1 loss: 0.781100  [   64/  179]
train() client id: f_00007-10-2 loss: 0.744730  [   96/  179]
train() client id: f_00007-10-3 loss: 0.614002  [  128/  179]
train() client id: f_00007-10-4 loss: 0.759375  [  160/  179]
train() client id: f_00007-11-0 loss: 0.710487  [   32/  179]
train() client id: f_00007-11-1 loss: 0.748687  [   64/  179]
train() client id: f_00007-11-2 loss: 0.673371  [   96/  179]
train() client id: f_00007-11-3 loss: 0.687507  [  128/  179]
train() client id: f_00007-11-4 loss: 0.751869  [  160/  179]
train() client id: f_00008-0-0 loss: 0.757984  [   32/  130]
train() client id: f_00008-0-1 loss: 0.739266  [   64/  130]
train() client id: f_00008-0-2 loss: 0.680404  [   96/  130]
train() client id: f_00008-0-3 loss: 0.754204  [  128/  130]
train() client id: f_00008-1-0 loss: 0.690419  [   32/  130]
train() client id: f_00008-1-1 loss: 0.620722  [   64/  130]
train() client id: f_00008-1-2 loss: 0.795762  [   96/  130]
train() client id: f_00008-1-3 loss: 0.823471  [  128/  130]
train() client id: f_00008-2-0 loss: 0.801946  [   32/  130]
train() client id: f_00008-2-1 loss: 0.658360  [   64/  130]
train() client id: f_00008-2-2 loss: 0.816417  [   96/  130]
train() client id: f_00008-2-3 loss: 0.638806  [  128/  130]
train() client id: f_00008-3-0 loss: 0.663028  [   32/  130]
train() client id: f_00008-3-1 loss: 0.816464  [   64/  130]
train() client id: f_00008-3-2 loss: 0.823310  [   96/  130]
train() client id: f_00008-3-3 loss: 0.605581  [  128/  130]
train() client id: f_00008-4-0 loss: 0.636536  [   32/  130]
train() client id: f_00008-4-1 loss: 0.785814  [   64/  130]
train() client id: f_00008-4-2 loss: 0.786176  [   96/  130]
train() client id: f_00008-4-3 loss: 0.728329  [  128/  130]
train() client id: f_00008-5-0 loss: 0.722623  [   32/  130]
train() client id: f_00008-5-1 loss: 0.847745  [   64/  130]
train() client id: f_00008-5-2 loss: 0.679503  [   96/  130]
train() client id: f_00008-5-3 loss: 0.666193  [  128/  130]
train() client id: f_00008-6-0 loss: 0.634956  [   32/  130]
train() client id: f_00008-6-1 loss: 0.818978  [   64/  130]
train() client id: f_00008-6-2 loss: 0.665261  [   96/  130]
train() client id: f_00008-6-3 loss: 0.808969  [  128/  130]
train() client id: f_00008-7-0 loss: 0.719054  [   32/  130]
train() client id: f_00008-7-1 loss: 0.733922  [   64/  130]
train() client id: f_00008-7-2 loss: 0.753950  [   96/  130]
train() client id: f_00008-7-3 loss: 0.688722  [  128/  130]
train() client id: f_00008-8-0 loss: 0.697409  [   32/  130]
train() client id: f_00008-8-1 loss: 0.728224  [   64/  130]
train() client id: f_00008-8-2 loss: 0.775871  [   96/  130]
train() client id: f_00008-8-3 loss: 0.731825  [  128/  130]
train() client id: f_00008-9-0 loss: 0.787003  [   32/  130]
train() client id: f_00008-9-1 loss: 0.777049  [   64/  130]
train() client id: f_00008-9-2 loss: 0.762294  [   96/  130]
train() client id: f_00008-9-3 loss: 0.611370  [  128/  130]
train() client id: f_00008-10-0 loss: 0.688453  [   32/  130]
train() client id: f_00008-10-1 loss: 0.774654  [   64/  130]
train() client id: f_00008-10-2 loss: 0.683887  [   96/  130]
train() client id: f_00008-10-3 loss: 0.725973  [  128/  130]
train() client id: f_00008-11-0 loss: 0.664617  [   32/  130]
train() client id: f_00008-11-1 loss: 0.820989  [   64/  130]
train() client id: f_00008-11-2 loss: 0.779244  [   96/  130]
train() client id: f_00008-11-3 loss: 0.666773  [  128/  130]
train() client id: f_00009-0-0 loss: 1.132262  [   32/  118]
train() client id: f_00009-0-1 loss: 1.122000  [   64/  118]
train() client id: f_00009-0-2 loss: 1.035881  [   96/  118]
train() client id: f_00009-1-0 loss: 1.066975  [   32/  118]
train() client id: f_00009-1-1 loss: 1.037089  [   64/  118]
train() client id: f_00009-1-2 loss: 1.079009  [   96/  118]
train() client id: f_00009-2-0 loss: 1.019299  [   32/  118]
train() client id: f_00009-2-1 loss: 1.011543  [   64/  118]
train() client id: f_00009-2-2 loss: 0.988709  [   96/  118]
train() client id: f_00009-3-0 loss: 0.951808  [   32/  118]
train() client id: f_00009-3-1 loss: 1.057371  [   64/  118]
train() client id: f_00009-3-2 loss: 0.876159  [   96/  118]
train() client id: f_00009-4-0 loss: 1.002117  [   32/  118]
train() client id: f_00009-4-1 loss: 0.907681  [   64/  118]
train() client id: f_00009-4-2 loss: 0.999546  [   96/  118]
train() client id: f_00009-5-0 loss: 0.858846  [   32/  118]
train() client id: f_00009-5-1 loss: 1.028763  [   64/  118]
train() client id: f_00009-5-2 loss: 0.955919  [   96/  118]
train() client id: f_00009-6-0 loss: 0.917270  [   32/  118]
train() client id: f_00009-6-1 loss: 0.893090  [   64/  118]
train() client id: f_00009-6-2 loss: 0.882702  [   96/  118]
train() client id: f_00009-7-0 loss: 0.952137  [   32/  118]
train() client id: f_00009-7-1 loss: 0.944244  [   64/  118]
train() client id: f_00009-7-2 loss: 0.847930  [   96/  118]
train() client id: f_00009-8-0 loss: 0.968306  [   32/  118]
train() client id: f_00009-8-1 loss: 0.961660  [   64/  118]
train() client id: f_00009-8-2 loss: 0.965536  [   96/  118]
train() client id: f_00009-9-0 loss: 0.785086  [   32/  118]
train() client id: f_00009-9-1 loss: 0.907795  [   64/  118]
train() client id: f_00009-9-2 loss: 0.863448  [   96/  118]
train() client id: f_00009-10-0 loss: 0.890821  [   32/  118]
train() client id: f_00009-10-1 loss: 0.938411  [   64/  118]
train() client id: f_00009-10-2 loss: 0.888130  [   96/  118]
train() client id: f_00009-11-0 loss: 0.853672  [   32/  118]
train() client id: f_00009-11-1 loss: 0.928686  [   64/  118]
train() client id: f_00009-11-2 loss: 0.912593  [   96/  118]
At round 14 accuracy: 0.6312997347480106
At round 14 training accuracy: 0.5700871898054997
At round 14 training loss: 0.8580951085541937
update_location
xs = [ -3.9056584    4.20031788  90.00902392  18.81129433   0.97929623
   3.95640986 -52.44319194 -31.32485185  74.66397685 -17.06087855]
ys = [ 82.5879595   65.55583871   1.32061395 -52.45517586  44.35018685
  27.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [129.75370986 119.64618949 134.54875848 114.47886385 109.39788889
 103.8714598  112.94768224 104.79466875 126.02927998 101.52381707]
dists_bs = [194.45631632 209.97804258 316.85061448 298.82883869 219.17592563
 231.70902195 215.80270215 225.7884024  295.15524593 232.77727406]
uav_gains = [5.21358841e-11 6.38604963e-11 4.76097128e-11 7.13141531e-11
 7.98862224e-11 9.09403593e-11 7.37560435e-11 8.89506057e-11
 5.60762293e-11 9.62893323e-11]
bs_gains = [4.31114205e-11 3.47702020e-11 1.09875619e-11 1.29452686e-11
 3.08371410e-11 2.63908782e-11 3.22058545e-11 2.83745851e-11
 1.34014770e-11 2.60531636e-11]
Round 15
-------------------------------
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.01168963 18.78472696  8.87441189  3.17492049 21.66795492 10.44235678
  3.94614583 12.71610262  9.35523145  8.4772718 ]
obj_prev = 106.45081237176021
eta_min = 4.89584141592709e-11	eta_max = 0.9211440323876784
af = 22.501910530386866	bf = 1.761176745733534	zeta = 24.752101583425553	eta = 0.9090909090909091
af = 22.501910530386866	bf = 1.761176745733534	zeta = 42.96421672758091	eta = 0.5237360818902523
af = 22.501910530386866	bf = 1.761176745733534	zeta = 34.25554170777676	eta = 0.6568838035709252
af = 22.501910530386866	bf = 1.761176745733534	zeta = 32.69318907763576	eta = 0.6882751779568552
af = 22.501910530386866	bf = 1.761176745733534	zeta = 32.6157469047972	eta = 0.6899094046832094
af = 22.501910530386866	bf = 1.761176745733534	zeta = 32.61554299983585	eta = 0.6899137178399916
eta = 0.6899137178399916
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [0.0306296  0.06441941 0.03014342 0.01045296 0.07438618 0.03549145
 0.01312697 0.04351349 0.03160197 0.02868487]
ene_total = [2.80683503 5.36552507 2.7806876  1.27556609 6.12194985 3.25313134
 1.47109077 3.71222193 3.07937974 2.74915557]
ti_comp = [0.32219551 0.3157955  0.32085207 0.32647704 0.31367854 0.31076018
 0.32690731 0.32920522 0.2952455  0.31050951]
ti_coms = [0.07139244 0.07779246 0.07273588 0.06711092 0.07990942 0.08282778
 0.06668064 0.06438274 0.09834245 0.08307845]
t_total = [29.24993706 29.24993706 29.24993706 29.24993706 29.24993706 29.24993706
 29.24993706 29.24993706 29.24993706 29.24993706]
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [1.73007617e-05 1.67539923e-04 1.66283024e-05 6.69719050e-07
 2.61449152e-04 2.89334621e-05 1.32289255e-06 4.75136331e-05
 2.26285138e-05 1.52999065e-05]
ene_total = [0.5318468  0.59057441 0.54178073 0.49879221 0.61328584 0.61769428
 0.4956431  0.48199869 0.73252479 0.61854397]
optimize_network iter = 0 obj = 5.7226848172315465
eta = 0.6899137178399916
freqs = [4.75326373e+07 1.01995448e+08 4.69740154e+07 1.60087223e+07
 1.18570726e+08 5.71042387e+07 2.00775158e+07 6.60886950e+07
 5.35181277e+07 4.61899994e+07]
eta_min = 0.6899137178400166	eta_max = 0.6899137178399891
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 0.03956004889123434	eta = 0.9090909090909091
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 19.410648956797782	eta = 0.0018527809600934662
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 1.9911157923761436	eta = 0.018062074012930685
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 1.9405838032428373	eta = 0.018532402852232132
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 1.9405717992779243	eta = 0.01853251748973932
eta = 0.01853251748973932
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [1.85887615e-04 1.80012864e-03 1.78662392e-04 7.19578012e-06
 2.80913409e-03 3.10874884e-04 1.42137870e-05 5.10509081e-04
 2.43131519e-04 1.64389475e-04]
ene_total = [0.1719908  0.2249194  0.17497549 0.15774238 0.25358093 0.2017751
 0.15689688 0.1631543  0.23661227 0.19892425]
ti_comp = [0.32219551 0.3157955  0.32085207 0.32647704 0.31367854 0.31076018
 0.32690731 0.32920522 0.2952455  0.31050951]
ti_coms = [0.07139244 0.07779246 0.07273588 0.06711092 0.07990942 0.08282778
 0.06668064 0.06438274 0.09834245 0.08307845]
t_total = [29.24993706 29.24993706 29.24993706 29.24993706 29.24993706 29.24993706
 29.24993706 29.24993706 29.24993706 29.24993706]
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [1.73007617e-05 1.67539923e-04 1.66283024e-05 6.69719050e-07
 2.61449152e-04 2.89334621e-05 1.32289255e-06 4.75136331e-05
 2.26285138e-05 1.52999065e-05]
ene_total = [0.5318468  0.59057441 0.54178073 0.49879221 0.61328584 0.61769428
 0.4956431  0.48199869 0.73252479 0.61854397]
optimize_network iter = 1 obj = 5.722684817232005
eta = 0.6899137178400166
freqs = [4.75326373e+07 1.01995448e+08 4.69740154e+07 1.60087223e+07
 1.18570726e+08 5.71042387e+07 2.00775158e+07 6.60886950e+07
 5.35181277e+07 4.61899994e+07]
Done!
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [1.70806981e-05 1.65408836e-04 1.64167924e-05 6.61200306e-07
 2.58123551e-04 2.85654321e-05 1.30606552e-06 4.69092656e-05
 2.23406819e-05 1.51052936e-05]
ene_total = [0.00715633 0.00794465 0.00729    0.00671175 0.00824907 0.00831134
 0.00666937 0.00648518 0.00985659 0.00832295]
At round 15 energy consumption: 0.07699723536707746
At round 15 eta: 0.6899137178400166
At round 15 a_n: 22.701869304745298
At round 15 local rounds: 12.154605075633594
At round 15 global rounds: 74.31613869273092
gradient difference: 0.45648184418678284
train() client id: f_00000-0-0 loss: 1.678392  [   32/  126]
train() client id: f_00000-0-1 loss: 1.279585  [   64/  126]
train() client id: f_00000-0-2 loss: 1.526833  [   96/  126]
train() client id: f_00000-1-0 loss: 1.330758  [   32/  126]
train() client id: f_00000-1-1 loss: 1.294643  [   64/  126]
train() client id: f_00000-1-2 loss: 1.357490  [   96/  126]
train() client id: f_00000-2-0 loss: 1.132760  [   32/  126]
train() client id: f_00000-2-1 loss: 1.253458  [   64/  126]
train() client id: f_00000-2-2 loss: 1.225019  [   96/  126]
train() client id: f_00000-3-0 loss: 1.166812  [   32/  126]
train() client id: f_00000-3-1 loss: 1.147776  [   64/  126]
train() client id: f_00000-3-2 loss: 1.097119  [   96/  126]
train() client id: f_00000-4-0 loss: 1.109586  [   32/  126]
train() client id: f_00000-4-1 loss: 1.043000  [   64/  126]
train() client id: f_00000-4-2 loss: 1.120229  [   96/  126]
train() client id: f_00000-5-0 loss: 1.020807  [   32/  126]
train() client id: f_00000-5-1 loss: 0.988307  [   64/  126]
train() client id: f_00000-5-2 loss: 0.943452  [   96/  126]
train() client id: f_00000-6-0 loss: 0.980524  [   32/  126]
train() client id: f_00000-6-1 loss: 0.987282  [   64/  126]
train() client id: f_00000-6-2 loss: 0.964566  [   96/  126]
train() client id: f_00000-7-0 loss: 0.995462  [   32/  126]
train() client id: f_00000-7-1 loss: 0.987222  [   64/  126]
train() client id: f_00000-7-2 loss: 0.870881  [   96/  126]
train() client id: f_00000-8-0 loss: 0.942028  [   32/  126]
train() client id: f_00000-8-1 loss: 0.963882  [   64/  126]
train() client id: f_00000-8-2 loss: 0.897858  [   96/  126]
train() client id: f_00000-9-0 loss: 0.923922  [   32/  126]
train() client id: f_00000-9-1 loss: 0.934430  [   64/  126]
train() client id: f_00000-9-2 loss: 0.901542  [   96/  126]
train() client id: f_00000-10-0 loss: 0.939034  [   32/  126]
train() client id: f_00000-10-1 loss: 0.925517  [   64/  126]
train() client id: f_00000-10-2 loss: 0.869208  [   96/  126]
train() client id: f_00000-11-0 loss: 0.991152  [   32/  126]
train() client id: f_00000-11-1 loss: 0.887519  [   64/  126]
train() client id: f_00000-11-2 loss: 0.833951  [   96/  126]
train() client id: f_00001-0-0 loss: 0.529568  [   32/  265]
train() client id: f_00001-0-1 loss: 0.548623  [   64/  265]
train() client id: f_00001-0-2 loss: 0.573322  [   96/  265]
train() client id: f_00001-0-3 loss: 0.490139  [  128/  265]
train() client id: f_00001-0-4 loss: 0.549472  [  160/  265]
train() client id: f_00001-0-5 loss: 0.540441  [  192/  265]
train() client id: f_00001-0-6 loss: 0.586621  [  224/  265]
train() client id: f_00001-0-7 loss: 0.514720  [  256/  265]
train() client id: f_00001-1-0 loss: 0.601355  [   32/  265]
train() client id: f_00001-1-1 loss: 0.571642  [   64/  265]
train() client id: f_00001-1-2 loss: 0.536104  [   96/  265]
train() client id: f_00001-1-3 loss: 0.628951  [  128/  265]
train() client id: f_00001-1-4 loss: 0.548861  [  160/  265]
train() client id: f_00001-1-5 loss: 0.508560  [  192/  265]
train() client id: f_00001-1-6 loss: 0.458630  [  224/  265]
train() client id: f_00001-1-7 loss: 0.469844  [  256/  265]
train() client id: f_00001-2-0 loss: 0.472936  [   32/  265]
train() client id: f_00001-2-1 loss: 0.576379  [   64/  265]
train() client id: f_00001-2-2 loss: 0.602938  [   96/  265]
train() client id: f_00001-2-3 loss: 0.471988  [  128/  265]
train() client id: f_00001-2-4 loss: 0.564197  [  160/  265]
train() client id: f_00001-2-5 loss: 0.441726  [  192/  265]
train() client id: f_00001-2-6 loss: 0.488941  [  224/  265]
train() client id: f_00001-2-7 loss: 0.489465  [  256/  265]
train() client id: f_00001-3-0 loss: 0.632484  [   32/  265]
train() client id: f_00001-3-1 loss: 0.569486  [   64/  265]
train() client id: f_00001-3-2 loss: 0.462592  [   96/  265]
train() client id: f_00001-3-3 loss: 0.502284  [  128/  265]
train() client id: f_00001-3-4 loss: 0.557364  [  160/  265]
train() client id: f_00001-3-5 loss: 0.525393  [  192/  265]
train() client id: f_00001-3-6 loss: 0.456295  [  224/  265]
train() client id: f_00001-3-7 loss: 0.544401  [  256/  265]
train() client id: f_00001-4-0 loss: 0.536042  [   32/  265]
train() client id: f_00001-4-1 loss: 0.501395  [   64/  265]
train() client id: f_00001-4-2 loss: 0.590213  [   96/  265]
train() client id: f_00001-4-3 loss: 0.433046  [  128/  265]
train() client id: f_00001-4-4 loss: 0.511384  [  160/  265]
train() client id: f_00001-4-5 loss: 0.508148  [  192/  265]
train() client id: f_00001-4-6 loss: 0.583112  [  224/  265]
train() client id: f_00001-4-7 loss: 0.558464  [  256/  265]
train() client id: f_00001-5-0 loss: 0.504153  [   32/  265]
train() client id: f_00001-5-1 loss: 0.659030  [   64/  265]
train() client id: f_00001-5-2 loss: 0.514210  [   96/  265]
train() client id: f_00001-5-3 loss: 0.451396  [  128/  265]
train() client id: f_00001-5-4 loss: 0.556208  [  160/  265]
train() client id: f_00001-5-5 loss: 0.501315  [  192/  265]
train() client id: f_00001-5-6 loss: 0.490444  [  224/  265]
train() client id: f_00001-5-7 loss: 0.459334  [  256/  265]
train() client id: f_00001-6-0 loss: 0.700817  [   32/  265]
train() client id: f_00001-6-1 loss: 0.559800  [   64/  265]
train() client id: f_00001-6-2 loss: 0.544428  [   96/  265]
train() client id: f_00001-6-3 loss: 0.444876  [  128/  265]
train() client id: f_00001-6-4 loss: 0.536976  [  160/  265]
train() client id: f_00001-6-5 loss: 0.484534  [  192/  265]
train() client id: f_00001-6-6 loss: 0.444728  [  224/  265]
train() client id: f_00001-6-7 loss: 0.419601  [  256/  265]
train() client id: f_00001-7-0 loss: 0.598418  [   32/  265]
train() client id: f_00001-7-1 loss: 0.534469  [   64/  265]
train() client id: f_00001-7-2 loss: 0.542878  [   96/  265]
train() client id: f_00001-7-3 loss: 0.498773  [  128/  265]
train() client id: f_00001-7-4 loss: 0.572561  [  160/  265]
train() client id: f_00001-7-5 loss: 0.490294  [  192/  265]
train() client id: f_00001-7-6 loss: 0.492655  [  224/  265]
train() client id: f_00001-7-7 loss: 0.449306  [  256/  265]
train() client id: f_00001-8-0 loss: 0.479397  [   32/  265]
train() client id: f_00001-8-1 loss: 0.436808  [   64/  265]
train() client id: f_00001-8-2 loss: 0.549467  [   96/  265]
train() client id: f_00001-8-3 loss: 0.591247  [  128/  265]
train() client id: f_00001-8-4 loss: 0.641047  [  160/  265]
train() client id: f_00001-8-5 loss: 0.577941  [  192/  265]
train() client id: f_00001-8-6 loss: 0.434271  [  224/  265]
train() client id: f_00001-8-7 loss: 0.474754  [  256/  265]
train() client id: f_00001-9-0 loss: 0.612195  [   32/  265]
train() client id: f_00001-9-1 loss: 0.435248  [   64/  265]
train() client id: f_00001-9-2 loss: 0.618046  [   96/  265]
train() client id: f_00001-9-3 loss: 0.531642  [  128/  265]
train() client id: f_00001-9-4 loss: 0.602235  [  160/  265]
train() client id: f_00001-9-5 loss: 0.425673  [  192/  265]
train() client id: f_00001-9-6 loss: 0.438246  [  224/  265]
train() client id: f_00001-9-7 loss: 0.441924  [  256/  265]
train() client id: f_00001-10-0 loss: 0.578376  [   32/  265]
train() client id: f_00001-10-1 loss: 0.498998  [   64/  265]
train() client id: f_00001-10-2 loss: 0.493001  [   96/  265]
train() client id: f_00001-10-3 loss: 0.494414  [  128/  265]
train() client id: f_00001-10-4 loss: 0.534192  [  160/  265]
train() client id: f_00001-10-5 loss: 0.595331  [  192/  265]
train() client id: f_00001-10-6 loss: 0.495471  [  224/  265]
train() client id: f_00001-10-7 loss: 0.486173  [  256/  265]
train() client id: f_00001-11-0 loss: 0.503950  [   32/  265]
train() client id: f_00001-11-1 loss: 0.590526  [   64/  265]
train() client id: f_00001-11-2 loss: 0.506796  [   96/  265]
train() client id: f_00001-11-3 loss: 0.536491  [  128/  265]
train() client id: f_00001-11-4 loss: 0.431851  [  160/  265]
train() client id: f_00001-11-5 loss: 0.601115  [  192/  265]
train() client id: f_00001-11-6 loss: 0.494173  [  224/  265]
train() client id: f_00001-11-7 loss: 0.480114  [  256/  265]
train() client id: f_00002-0-0 loss: 1.105791  [   32/  124]
train() client id: f_00002-0-1 loss: 1.249751  [   64/  124]
train() client id: f_00002-0-2 loss: 1.311916  [   96/  124]
train() client id: f_00002-1-0 loss: 1.253609  [   32/  124]
train() client id: f_00002-1-1 loss: 1.150157  [   64/  124]
train() client id: f_00002-1-2 loss: 1.170868  [   96/  124]
train() client id: f_00002-2-0 loss: 1.343135  [   32/  124]
train() client id: f_00002-2-1 loss: 1.054063  [   64/  124]
train() client id: f_00002-2-2 loss: 1.128189  [   96/  124]
train() client id: f_00002-3-0 loss: 1.200176  [   32/  124]
train() client id: f_00002-3-1 loss: 1.041056  [   64/  124]
train() client id: f_00002-3-2 loss: 1.253051  [   96/  124]
train() client id: f_00002-4-0 loss: 1.110497  [   32/  124]
train() client id: f_00002-4-1 loss: 1.143325  [   64/  124]
train() client id: f_00002-4-2 loss: 1.068895  [   96/  124]
train() client id: f_00002-5-0 loss: 1.038164  [   32/  124]
train() client id: f_00002-5-1 loss: 1.257006  [   64/  124]
train() client id: f_00002-5-2 loss: 1.120225  [   96/  124]
train() client id: f_00002-6-0 loss: 1.287431  [   32/  124]
train() client id: f_00002-6-1 loss: 1.055549  [   64/  124]
train() client id: f_00002-6-2 loss: 1.049762  [   96/  124]
train() client id: f_00002-7-0 loss: 1.168447  [   32/  124]
train() client id: f_00002-7-1 loss: 0.957843  [   64/  124]
train() client id: f_00002-7-2 loss: 1.178493  [   96/  124]
train() client id: f_00002-8-0 loss: 1.096489  [   32/  124]
train() client id: f_00002-8-1 loss: 1.095933  [   64/  124]
train() client id: f_00002-8-2 loss: 1.175455  [   96/  124]
train() client id: f_00002-9-0 loss: 1.159568  [   32/  124]
train() client id: f_00002-9-1 loss: 1.093978  [   64/  124]
train() client id: f_00002-9-2 loss: 1.087570  [   96/  124]
train() client id: f_00002-10-0 loss: 1.107157  [   32/  124]
train() client id: f_00002-10-1 loss: 1.091738  [   64/  124]
train() client id: f_00002-10-2 loss: 1.110451  [   96/  124]
train() client id: f_00002-11-0 loss: 1.026069  [   32/  124]
train() client id: f_00002-11-1 loss: 1.179116  [   64/  124]
train() client id: f_00002-11-2 loss: 1.089854  [   96/  124]
train() client id: f_00003-0-0 loss: 0.865174  [   32/   43]
train() client id: f_00003-1-0 loss: 0.692404  [   32/   43]
train() client id: f_00003-2-0 loss: 0.821222  [   32/   43]
train() client id: f_00003-3-0 loss: 0.755169  [   32/   43]
train() client id: f_00003-4-0 loss: 0.835751  [   32/   43]
train() client id: f_00003-5-0 loss: 0.789678  [   32/   43]
train() client id: f_00003-6-0 loss: 0.826563  [   32/   43]
train() client id: f_00003-7-0 loss: 0.900001  [   32/   43]
train() client id: f_00003-8-0 loss: 0.881552  [   32/   43]
train() client id: f_00003-9-0 loss: 0.807354  [   32/   43]
train() client id: f_00003-10-0 loss: 0.700999  [   32/   43]
train() client id: f_00003-11-0 loss: 0.834646  [   32/   43]
train() client id: f_00004-0-0 loss: 0.732837  [   32/  306]
train() client id: f_00004-0-1 loss: 0.758830  [   64/  306]
train() client id: f_00004-0-2 loss: 0.846030  [   96/  306]
train() client id: f_00004-0-3 loss: 0.736449  [  128/  306]
train() client id: f_00004-0-4 loss: 0.774987  [  160/  306]
train() client id: f_00004-0-5 loss: 0.797017  [  192/  306]
train() client id: f_00004-0-6 loss: 0.801416  [  224/  306]
train() client id: f_00004-0-7 loss: 0.748168  [  256/  306]
train() client id: f_00004-0-8 loss: 0.707232  [  288/  306]
train() client id: f_00004-1-0 loss: 0.855809  [   32/  306]
train() client id: f_00004-1-1 loss: 0.771166  [   64/  306]
train() client id: f_00004-1-2 loss: 0.845773  [   96/  306]
train() client id: f_00004-1-3 loss: 0.831948  [  128/  306]
train() client id: f_00004-1-4 loss: 0.879398  [  160/  306]
train() client id: f_00004-1-5 loss: 0.571161  [  192/  306]
train() client id: f_00004-1-6 loss: 0.598784  [  224/  306]
train() client id: f_00004-1-7 loss: 0.807525  [  256/  306]
train() client id: f_00004-1-8 loss: 0.806295  [  288/  306]
train() client id: f_00004-2-0 loss: 0.816782  [   32/  306]
train() client id: f_00004-2-1 loss: 0.801045  [   64/  306]
train() client id: f_00004-2-2 loss: 0.859413  [   96/  306]
train() client id: f_00004-2-3 loss: 0.732028  [  128/  306]
train() client id: f_00004-2-4 loss: 0.750797  [  160/  306]
train() client id: f_00004-2-5 loss: 0.646209  [  192/  306]
train() client id: f_00004-2-6 loss: 0.778914  [  224/  306]
train() client id: f_00004-2-7 loss: 0.870565  [  256/  306]
train() client id: f_00004-2-8 loss: 0.661374  [  288/  306]
train() client id: f_00004-3-0 loss: 0.830333  [   32/  306]
train() client id: f_00004-3-1 loss: 0.738276  [   64/  306]
train() client id: f_00004-3-2 loss: 0.818841  [   96/  306]
train() client id: f_00004-3-3 loss: 0.794016  [  128/  306]
train() client id: f_00004-3-4 loss: 0.682474  [  160/  306]
train() client id: f_00004-3-5 loss: 0.743593  [  192/  306]
train() client id: f_00004-3-6 loss: 0.798724  [  224/  306]
train() client id: f_00004-3-7 loss: 0.644011  [  256/  306]
train() client id: f_00004-3-8 loss: 0.825473  [  288/  306]
train() client id: f_00004-4-0 loss: 0.639063  [   32/  306]
train() client id: f_00004-4-1 loss: 0.836829  [   64/  306]
train() client id: f_00004-4-2 loss: 0.709003  [   96/  306]
train() client id: f_00004-4-3 loss: 0.814624  [  128/  306]
train() client id: f_00004-4-4 loss: 0.707633  [  160/  306]
train() client id: f_00004-4-5 loss: 0.846326  [  192/  306]
train() client id: f_00004-4-6 loss: 0.742636  [  224/  306]
train() client id: f_00004-4-7 loss: 0.767819  [  256/  306]
train() client id: f_00004-4-8 loss: 0.916108  [  288/  306]
train() client id: f_00004-5-0 loss: 0.724738  [   32/  306]
train() client id: f_00004-5-1 loss: 0.802134  [   64/  306]
train() client id: f_00004-5-2 loss: 0.957959  [   96/  306]
train() client id: f_00004-5-3 loss: 0.720752  [  128/  306]
train() client id: f_00004-5-4 loss: 0.747362  [  160/  306]
train() client id: f_00004-5-5 loss: 0.780734  [  192/  306]
train() client id: f_00004-5-6 loss: 0.691034  [  224/  306]
train() client id: f_00004-5-7 loss: 0.734899  [  256/  306]
train() client id: f_00004-5-8 loss: 0.795952  [  288/  306]
train() client id: f_00004-6-0 loss: 0.819105  [   32/  306]
train() client id: f_00004-6-1 loss: 0.683655  [   64/  306]
train() client id: f_00004-6-2 loss: 0.775182  [   96/  306]
train() client id: f_00004-6-3 loss: 0.824096  [  128/  306]
train() client id: f_00004-6-4 loss: 0.792984  [  160/  306]
train() client id: f_00004-6-5 loss: 0.775778  [  192/  306]
train() client id: f_00004-6-6 loss: 0.744801  [  224/  306]
train() client id: f_00004-6-7 loss: 0.763454  [  256/  306]
train() client id: f_00004-6-8 loss: 0.706933  [  288/  306]
train() client id: f_00004-7-0 loss: 0.739996  [   32/  306]
train() client id: f_00004-7-1 loss: 0.862053  [   64/  306]
train() client id: f_00004-7-2 loss: 0.765257  [   96/  306]
train() client id: f_00004-7-3 loss: 0.722211  [  128/  306]
train() client id: f_00004-7-4 loss: 0.890590  [  160/  306]
train() client id: f_00004-7-5 loss: 0.830984  [  192/  306]
train() client id: f_00004-7-6 loss: 0.733204  [  224/  306]
train() client id: f_00004-7-7 loss: 0.772109  [  256/  306]
train() client id: f_00004-7-8 loss: 0.724774  [  288/  306]
train() client id: f_00004-8-0 loss: 0.732250  [   32/  306]
train() client id: f_00004-8-1 loss: 0.765773  [   64/  306]
train() client id: f_00004-8-2 loss: 0.858917  [   96/  306]
train() client id: f_00004-8-3 loss: 0.793966  [  128/  306]
train() client id: f_00004-8-4 loss: 0.701402  [  160/  306]
train() client id: f_00004-8-5 loss: 0.797894  [  192/  306]
train() client id: f_00004-8-6 loss: 0.772340  [  224/  306]
train() client id: f_00004-8-7 loss: 0.771275  [  256/  306]
train() client id: f_00004-8-8 loss: 0.825357  [  288/  306]
train() client id: f_00004-9-0 loss: 0.755168  [   32/  306]
train() client id: f_00004-9-1 loss: 0.807733  [   64/  306]
train() client id: f_00004-9-2 loss: 0.715025  [   96/  306]
train() client id: f_00004-9-3 loss: 0.818930  [  128/  306]
train() client id: f_00004-9-4 loss: 0.855525  [  160/  306]
train() client id: f_00004-9-5 loss: 0.740909  [  192/  306]
train() client id: f_00004-9-6 loss: 0.864946  [  224/  306]
train() client id: f_00004-9-7 loss: 0.778092  [  256/  306]
train() client id: f_00004-9-8 loss: 0.728664  [  288/  306]
train() client id: f_00004-10-0 loss: 0.881031  [   32/  306]
train() client id: f_00004-10-1 loss: 0.658065  [   64/  306]
train() client id: f_00004-10-2 loss: 0.793287  [   96/  306]
train() client id: f_00004-10-3 loss: 0.918420  [  128/  306]
train() client id: f_00004-10-4 loss: 0.788208  [  160/  306]
train() client id: f_00004-10-5 loss: 0.738301  [  192/  306]
train() client id: f_00004-10-6 loss: 0.686284  [  224/  306]
train() client id: f_00004-10-7 loss: 0.811203  [  256/  306]
train() client id: f_00004-10-8 loss: 0.784005  [  288/  306]
train() client id: f_00004-11-0 loss: 0.850996  [   32/  306]
train() client id: f_00004-11-1 loss: 0.759232  [   64/  306]
train() client id: f_00004-11-2 loss: 0.724924  [   96/  306]
train() client id: f_00004-11-3 loss: 0.825153  [  128/  306]
train() client id: f_00004-11-4 loss: 0.770878  [  160/  306]
train() client id: f_00004-11-5 loss: 0.738796  [  192/  306]
train() client id: f_00004-11-6 loss: 0.741102  [  224/  306]
train() client id: f_00004-11-7 loss: 0.809883  [  256/  306]
train() client id: f_00004-11-8 loss: 0.846453  [  288/  306]
train() client id: f_00005-0-0 loss: 0.705780  [   32/  146]
train() client id: f_00005-0-1 loss: 0.659270  [   64/  146]
train() client id: f_00005-0-2 loss: 0.944813  [   96/  146]
train() client id: f_00005-0-3 loss: 0.588578  [  128/  146]
train() client id: f_00005-1-0 loss: 0.736883  [   32/  146]
train() client id: f_00005-1-1 loss: 0.702788  [   64/  146]
train() client id: f_00005-1-2 loss: 0.819475  [   96/  146]
train() client id: f_00005-1-3 loss: 0.759645  [  128/  146]
train() client id: f_00005-2-0 loss: 0.678036  [   32/  146]
train() client id: f_00005-2-1 loss: 0.704151  [   64/  146]
train() client id: f_00005-2-2 loss: 0.701007  [   96/  146]
train() client id: f_00005-2-3 loss: 0.712945  [  128/  146]
train() client id: f_00005-3-0 loss: 0.713840  [   32/  146]
train() client id: f_00005-3-1 loss: 0.594184  [   64/  146]
train() client id: f_00005-3-2 loss: 0.802680  [   96/  146]
train() client id: f_00005-3-3 loss: 0.884469  [  128/  146]
train() client id: f_00005-4-0 loss: 0.740276  [   32/  146]
train() client id: f_00005-4-1 loss: 0.642267  [   64/  146]
train() client id: f_00005-4-2 loss: 0.662980  [   96/  146]
train() client id: f_00005-4-3 loss: 0.868863  [  128/  146]
train() client id: f_00005-5-0 loss: 0.667676  [   32/  146]
train() client id: f_00005-5-1 loss: 0.635235  [   64/  146]
train() client id: f_00005-5-2 loss: 0.830898  [   96/  146]
train() client id: f_00005-5-3 loss: 0.653756  [  128/  146]
train() client id: f_00005-6-0 loss: 0.772574  [   32/  146]
train() client id: f_00005-6-1 loss: 0.690577  [   64/  146]
train() client id: f_00005-6-2 loss: 0.602084  [   96/  146]
train() client id: f_00005-6-3 loss: 0.796115  [  128/  146]
train() client id: f_00005-7-0 loss: 0.785614  [   32/  146]
train() client id: f_00005-7-1 loss: 0.759192  [   64/  146]
train() client id: f_00005-7-2 loss: 0.579867  [   96/  146]
train() client id: f_00005-7-3 loss: 0.847139  [  128/  146]
train() client id: f_00005-8-0 loss: 0.690053  [   32/  146]
train() client id: f_00005-8-1 loss: 0.818681  [   64/  146]
train() client id: f_00005-8-2 loss: 0.600089  [   96/  146]
train() client id: f_00005-8-3 loss: 0.531590  [  128/  146]
train() client id: f_00005-9-0 loss: 0.782578  [   32/  146]
train() client id: f_00005-9-1 loss: 0.701385  [   64/  146]
train() client id: f_00005-9-2 loss: 0.820965  [   96/  146]
train() client id: f_00005-9-3 loss: 0.578173  [  128/  146]
train() client id: f_00005-10-0 loss: 0.796706  [   32/  146]
train() client id: f_00005-10-1 loss: 0.645315  [   64/  146]
train() client id: f_00005-10-2 loss: 0.652452  [   96/  146]
train() client id: f_00005-10-3 loss: 0.675870  [  128/  146]
train() client id: f_00005-11-0 loss: 0.931854  [   32/  146]
train() client id: f_00005-11-1 loss: 0.861946  [   64/  146]
train() client id: f_00005-11-2 loss: 0.631134  [   96/  146]
train() client id: f_00005-11-3 loss: 0.578040  [  128/  146]
train() client id: f_00006-0-0 loss: 0.588878  [   32/   54]
train() client id: f_00006-1-0 loss: 0.643612  [   32/   54]
train() client id: f_00006-2-0 loss: 0.552636  [   32/   54]
train() client id: f_00006-3-0 loss: 0.603492  [   32/   54]
train() client id: f_00006-4-0 loss: 0.646481  [   32/   54]
train() client id: f_00006-5-0 loss: 0.543524  [   32/   54]
train() client id: f_00006-6-0 loss: 0.601736  [   32/   54]
train() client id: f_00006-7-0 loss: 0.595251  [   32/   54]
train() client id: f_00006-8-0 loss: 0.558846  [   32/   54]
train() client id: f_00006-9-0 loss: 0.590492  [   32/   54]
train() client id: f_00006-10-0 loss: 0.658888  [   32/   54]
train() client id: f_00006-11-0 loss: 0.552168  [   32/   54]
train() client id: f_00007-0-0 loss: 0.569912  [   32/  179]
train() client id: f_00007-0-1 loss: 0.500275  [   64/  179]
train() client id: f_00007-0-2 loss: 0.652717  [   96/  179]
train() client id: f_00007-0-3 loss: 0.716098  [  128/  179]
train() client id: f_00007-0-4 loss: 0.588045  [  160/  179]
train() client id: f_00007-1-0 loss: 0.493248  [   32/  179]
train() client id: f_00007-1-1 loss: 0.512757  [   64/  179]
train() client id: f_00007-1-2 loss: 0.549707  [   96/  179]
train() client id: f_00007-1-3 loss: 0.605688  [  128/  179]
train() client id: f_00007-1-4 loss: 0.696194  [  160/  179]
train() client id: f_00007-2-0 loss: 0.535049  [   32/  179]
train() client id: f_00007-2-1 loss: 0.699695  [   64/  179]
train() client id: f_00007-2-2 loss: 0.489904  [   96/  179]
train() client id: f_00007-2-3 loss: 0.538719  [  128/  179]
train() client id: f_00007-2-4 loss: 0.577247  [  160/  179]
train() client id: f_00007-3-0 loss: 0.615353  [   32/  179]
train() client id: f_00007-3-1 loss: 0.602406  [   64/  179]
train() client id: f_00007-3-2 loss: 0.576290  [   96/  179]
train() client id: f_00007-3-3 loss: 0.466100  [  128/  179]
train() client id: f_00007-3-4 loss: 0.500017  [  160/  179]
train() client id: f_00007-4-0 loss: 0.415044  [   32/  179]
train() client id: f_00007-4-1 loss: 0.628075  [   64/  179]
train() client id: f_00007-4-2 loss: 0.535049  [   96/  179]
train() client id: f_00007-4-3 loss: 0.565537  [  128/  179]
train() client id: f_00007-4-4 loss: 0.675094  [  160/  179]
train() client id: f_00007-5-0 loss: 0.680624  [   32/  179]
train() client id: f_00007-5-1 loss: 0.486271  [   64/  179]
train() client id: f_00007-5-2 loss: 0.495856  [   96/  179]
train() client id: f_00007-5-3 loss: 0.557652  [  128/  179]
train() client id: f_00007-5-4 loss: 0.496070  [  160/  179]
train() client id: f_00007-6-0 loss: 0.536762  [   32/  179]
train() client id: f_00007-6-1 loss: 0.471771  [   64/  179]
train() client id: f_00007-6-2 loss: 0.494405  [   96/  179]
train() client id: f_00007-6-3 loss: 0.698040  [  128/  179]
train() client id: f_00007-6-4 loss: 0.569581  [  160/  179]
train() client id: f_00007-7-0 loss: 0.530739  [   32/  179]
train() client id: f_00007-7-1 loss: 0.590839  [   64/  179]
train() client id: f_00007-7-2 loss: 0.493057  [   96/  179]
train() client id: f_00007-7-3 loss: 0.477175  [  128/  179]
train() client id: f_00007-7-4 loss: 0.500630  [  160/  179]
train() client id: f_00007-8-0 loss: 0.571596  [   32/  179]
train() client id: f_00007-8-1 loss: 0.600926  [   64/  179]
train() client id: f_00007-8-2 loss: 0.424465  [   96/  179]
train() client id: f_00007-8-3 loss: 0.495122  [  128/  179]
train() client id: f_00007-8-4 loss: 0.489973  [  160/  179]
train() client id: f_00007-9-0 loss: 0.644649  [   32/  179]
train() client id: f_00007-9-1 loss: 0.533992  [   64/  179]
train() client id: f_00007-9-2 loss: 0.460790  [   96/  179]
train() client id: f_00007-9-3 loss: 0.590206  [  128/  179]
train() client id: f_00007-9-4 loss: 0.493981  [  160/  179]
train() client id: f_00007-10-0 loss: 0.491420  [   32/  179]
train() client id: f_00007-10-1 loss: 0.508310  [   64/  179]
train() client id: f_00007-10-2 loss: 0.565079  [   96/  179]
train() client id: f_00007-10-3 loss: 0.488705  [  128/  179]
train() client id: f_00007-10-4 loss: 0.572555  [  160/  179]
train() client id: f_00007-11-0 loss: 0.583620  [   32/  179]
train() client id: f_00007-11-1 loss: 0.402840  [   64/  179]
train() client id: f_00007-11-2 loss: 0.475990  [   96/  179]
train() client id: f_00007-11-3 loss: 0.674947  [  128/  179]
train() client id: f_00007-11-4 loss: 0.473898  [  160/  179]
train() client id: f_00008-0-0 loss: 0.809909  [   32/  130]
train() client id: f_00008-0-1 loss: 0.834098  [   64/  130]
train() client id: f_00008-0-2 loss: 0.873877  [   96/  130]
train() client id: f_00008-0-3 loss: 0.838556  [  128/  130]
train() client id: f_00008-1-0 loss: 0.838036  [   32/  130]
train() client id: f_00008-1-1 loss: 0.720517  [   64/  130]
train() client id: f_00008-1-2 loss: 0.910137  [   96/  130]
train() client id: f_00008-1-3 loss: 0.841776  [  128/  130]
train() client id: f_00008-2-0 loss: 0.875094  [   32/  130]
train() client id: f_00008-2-1 loss: 0.808187  [   64/  130]
train() client id: f_00008-2-2 loss: 0.873338  [   96/  130]
train() client id: f_00008-2-3 loss: 0.788665  [  128/  130]
train() client id: f_00008-3-0 loss: 0.817747  [   32/  130]
train() client id: f_00008-3-1 loss: 0.906798  [   64/  130]
train() client id: f_00008-3-2 loss: 0.848632  [   96/  130]
train() client id: f_00008-3-3 loss: 0.758397  [  128/  130]
train() client id: f_00008-4-0 loss: 0.835100  [   32/  130]
train() client id: f_00008-4-1 loss: 0.803056  [   64/  130]
train() client id: f_00008-4-2 loss: 0.826362  [   96/  130]
train() client id: f_00008-4-3 loss: 0.883309  [  128/  130]
train() client id: f_00008-5-0 loss: 0.847306  [   32/  130]
train() client id: f_00008-5-1 loss: 0.778771  [   64/  130]
train() client id: f_00008-5-2 loss: 0.799721  [   96/  130]
train() client id: f_00008-5-3 loss: 0.902326  [  128/  130]
train() client id: f_00008-6-0 loss: 0.741286  [   32/  130]
train() client id: f_00008-6-1 loss: 0.806835  [   64/  130]
train() client id: f_00008-6-2 loss: 0.915930  [   96/  130]
train() client id: f_00008-6-3 loss: 0.887354  [  128/  130]
train() client id: f_00008-7-0 loss: 0.728474  [   32/  130]
train() client id: f_00008-7-1 loss: 0.834990  [   64/  130]
train() client id: f_00008-7-2 loss: 0.947820  [   96/  130]
train() client id: f_00008-7-3 loss: 0.835421  [  128/  130]
train() client id: f_00008-8-0 loss: 0.898161  [   32/  130]
train() client id: f_00008-8-1 loss: 0.760756  [   64/  130]
train() client id: f_00008-8-2 loss: 0.850954  [   96/  130]
train() client id: f_00008-8-3 loss: 0.825457  [  128/  130]
train() client id: f_00008-9-0 loss: 0.841305  [   32/  130]
train() client id: f_00008-9-1 loss: 0.762869  [   64/  130]
train() client id: f_00008-9-2 loss: 0.782020  [   96/  130]
train() client id: f_00008-9-3 loss: 0.924662  [  128/  130]
train() client id: f_00008-10-0 loss: 0.847576  [   32/  130]
train() client id: f_00008-10-1 loss: 0.894019  [   64/  130]
train() client id: f_00008-10-2 loss: 0.877825  [   96/  130]
train() client id: f_00008-10-3 loss: 0.716949  [  128/  130]
train() client id: f_00008-11-0 loss: 0.884043  [   32/  130]
train() client id: f_00008-11-1 loss: 0.803430  [   64/  130]
train() client id: f_00008-11-2 loss: 0.854507  [   96/  130]
train() client id: f_00008-11-3 loss: 0.803589  [  128/  130]
train() client id: f_00009-0-0 loss: 1.155433  [   32/  118]
train() client id: f_00009-0-1 loss: 0.967334  [   64/  118]
train() client id: f_00009-0-2 loss: 1.100545  [   96/  118]
train() client id: f_00009-1-0 loss: 0.873056  [   32/  118]
train() client id: f_00009-1-1 loss: 1.150936  [   64/  118]
train() client id: f_00009-1-2 loss: 1.055152  [   96/  118]
train() client id: f_00009-2-0 loss: 1.057508  [   32/  118]
train() client id: f_00009-2-1 loss: 0.997119  [   64/  118]
train() client id: f_00009-2-2 loss: 0.985502  [   96/  118]
train() client id: f_00009-3-0 loss: 1.075674  [   32/  118]
train() client id: f_00009-3-1 loss: 1.004945  [   64/  118]
train() client id: f_00009-3-2 loss: 0.889829  [   96/  118]
train() client id: f_00009-4-0 loss: 1.001462  [   32/  118]
train() client id: f_00009-4-1 loss: 0.938346  [   64/  118]
train() client id: f_00009-4-2 loss: 0.995335  [   96/  118]
train() client id: f_00009-5-0 loss: 0.822350  [   32/  118]
train() client id: f_00009-5-1 loss: 1.053503  [   64/  118]
train() client id: f_00009-5-2 loss: 0.834847  [   96/  118]
train() client id: f_00009-6-0 loss: 0.829881  [   32/  118]
train() client id: f_00009-6-1 loss: 0.905143  [   64/  118]
train() client id: f_00009-6-2 loss: 0.921080  [   96/  118]
train() client id: f_00009-7-0 loss: 0.981951  [   32/  118]
train() client id: f_00009-7-1 loss: 0.813253  [   64/  118]
train() client id: f_00009-7-2 loss: 0.997641  [   96/  118]
train() client id: f_00009-8-0 loss: 0.919129  [   32/  118]
train() client id: f_00009-8-1 loss: 0.951021  [   64/  118]
train() client id: f_00009-8-2 loss: 0.736828  [   96/  118]
train() client id: f_00009-9-0 loss: 0.777887  [   32/  118]
train() client id: f_00009-9-1 loss: 0.894460  [   64/  118]
train() client id: f_00009-9-2 loss: 0.862942  [   96/  118]
train() client id: f_00009-10-0 loss: 0.871332  [   32/  118]
train() client id: f_00009-10-1 loss: 0.829086  [   64/  118]
train() client id: f_00009-10-2 loss: 0.902360  [   96/  118]
train() client id: f_00009-11-0 loss: 0.897863  [   32/  118]
train() client id: f_00009-11-1 loss: 0.882885  [   64/  118]
train() client id: f_00009-11-2 loss: 0.832967  [   96/  118]
At round 15 accuracy: 0.6312997347480106
At round 15 training accuracy: 0.5767940979208585
At round 15 training loss: 0.8431118213373184
update_location
xs = [ -3.9056584    4.20031788  95.00902392  18.81129433   0.97929623
   3.95640986 -57.44319194 -36.32485185  79.66397685 -22.06087855]
ys = [ 87.5879595   70.55583871   1.32061395 -57.45517586  49.35018685
  32.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [132.99212314 122.45721312 137.94367926 116.85444804 111.51860815
 105.32056622 115.35428402 106.39629278 129.05432647 102.48265326]
dists_bs = [192.13052478 207.41585464 321.04423704 302.65198924 216.2327178
 228.56555375 213.0029065  222.6401832  299.39649124 229.41418462]
uav_gains = [4.90168661e-11 6.02574305e-11 4.47298682e-11 6.77442080e-11
 7.61419638e-11 8.78443206e-11 6.99686398e-11 8.56406368e-11
 5.28456671e-11 9.40528225e-11]
bs_gains = [4.45886427e-11 3.59862526e-11 1.05904015e-11 1.24925816e-11
 3.20268441e-11 2.74197759e-11 3.34052405e-11 2.95123738e-11
 1.28766623e-11 2.71367191e-11]
Round 16
-------------------------------
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.87981483 18.50415528  8.7446806   3.12925602 21.34431125 10.28544805
  3.88903844 12.52812053  9.21866598  8.34945337]
obj_prev = 104.87294435893298
eta_min = 3.443571724151839e-11	eta_max = 0.9213403601232922
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 24.38417167306138	eta = 0.9090909090909091
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 42.374357946194124	eta = 0.5231330896350052
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 33.76652681787571	eta = 0.6564912320788918
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 32.22189998010197	eta = 0.6879615667412922
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 32.14522467977385	eta = 0.6896025463975112
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 32.14502216089842	eta = 0.6896068910058766
eta = 0.6896068910058766
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [0.03066631 0.06449661 0.03017954 0.01046549 0.07447533 0.03553398
 0.0131427  0.04356563 0.03163984 0.02871924]
ene_total = [2.771699   5.28190218 2.74625142 1.26115748 6.02653747 3.19933593
 1.45388682 3.66054961 3.04133893 2.70236334]
ti_comp = [0.32693877 0.3220322  0.32555044 0.33146051 0.32000855 0.31714645
 0.33188162 0.33440318 0.29980737 0.31694807]
ti_coms = [0.07229963 0.0772062  0.07368796 0.06777789 0.07922985 0.08209195
 0.06735678 0.06483522 0.09943103 0.08229033]
t_total = [29.19993286 29.19993286 29.19993286 29.19993286 29.19993286 29.19993286
 29.19993286 29.19993286 29.19993286 29.19993286]
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [1.68628817e-05 1.61693297e-04 1.62099429e-05 6.52070823e-07
 2.52112341e-04 2.78799409e-05 1.28815427e-06 4.62137600e-05
 2.20241139e-05 1.47374657e-05]
ene_total = [0.53002626 0.57650519 0.54013261 0.49576897 0.59791914 0.60245213
 0.49273557 0.47757895 0.72884031 0.60294189]
optimize_network iter = 0 obj = 5.644901027509804
eta = 0.6896068910058766
freqs = [4.68991664e+07 1.00139996e+08 4.63515646e+07 1.57869290e+07
 1.16364588e+08 5.60214049e+07 1.98002903e+07 6.51393823e+07
 5.27669565e+07 4.53059144e+07]
eta_min = 0.6896068910059152	eta_max = 0.6896068910058611
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 0.03760750116688597	eta = 0.909090909090909
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 19.169603857786115	eta = 0.0017834816868453646
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 1.9593134418751286	eta = 0.01744929458133157
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 1.9111928987478155	eta = 0.017888637743914634
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 1.9111822247051171	eta = 0.01788873765279858
eta = 0.01788873765279858
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [1.82570215e-04 1.75061300e-03 1.75501009e-04 7.05980817e-06
 2.72955744e-03 3.01849167e-04 1.39465251e-05 5.00344854e-04
 2.38449589e-04 1.59558866e-04]
ene_total = [0.17134348 0.21893109 0.17438925 0.15683431 0.24623749 0.19673596
 0.1560201  0.1614347  0.23535039 0.19390545]
ti_comp = [0.32693877 0.3220322  0.32555044 0.33146051 0.32000855 0.31714645
 0.33188162 0.33440318 0.29980737 0.31694807]
ti_coms = [0.07229963 0.0772062  0.07368796 0.06777789 0.07922985 0.08209195
 0.06735678 0.06483522 0.09943103 0.08229033]
t_total = [29.19993286 29.19993286 29.19993286 29.19993286 29.19993286 29.19993286
 29.19993286 29.19993286 29.19993286 29.19993286]
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [1.68628817e-05 1.61693297e-04 1.62099429e-05 6.52070823e-07
 2.52112341e-04 2.78799409e-05 1.28815427e-06 4.62137600e-05
 2.20241139e-05 1.47374657e-05]
ene_total = [0.53002626 0.57650519 0.54013261 0.49576897 0.59791914 0.60245213
 0.49273557 0.47757895 0.72884031 0.60294189]
optimize_network iter = 1 obj = 5.644901027510502
eta = 0.6896068910059152
freqs = [4.68991664e+07 1.00139996e+08 4.63515646e+07 1.57869290e+07
 1.16364588e+08 5.60214049e+07 1.98002903e+07 6.51393823e+07
 5.27669565e+07 4.53059144e+07]
Done!
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [1.66284604e-05 1.59445500e-04 1.59845985e-05 6.43005986e-07
 2.48607572e-04 2.74923648e-05 1.27024685e-06 4.55713142e-05
 2.17179432e-05 1.45325912e-05]
ene_total = [0.00724659 0.00788007 0.00738478 0.00677843 0.00817159 0.00823669
 0.00673695 0.00652909 0.00996482 0.00824357]
At round 16 energy consumption: 0.0771725767342164
At round 16 eta: 0.6896068910059152
At round 16 a_n: 22.359323457775982
At round 16 local rounds: 12.169171106060379
At round 16 global rounds: 73.13908926108901
gradient difference: 0.46780434250831604
train() client id: f_00000-0-0 loss: 1.663135  [   32/  126]
train() client id: f_00000-0-1 loss: 1.611418  [   64/  126]
train() client id: f_00000-0-2 loss: 1.284001  [   96/  126]
train() client id: f_00000-1-0 loss: 1.433282  [   32/  126]
train() client id: f_00000-1-1 loss: 1.319112  [   64/  126]
train() client id: f_00000-1-2 loss: 1.361409  [   96/  126]
train() client id: f_00000-2-0 loss: 1.227705  [   32/  126]
train() client id: f_00000-2-1 loss: 1.215197  [   64/  126]
train() client id: f_00000-2-2 loss: 1.381587  [   96/  126]
train() client id: f_00000-3-0 loss: 1.156731  [   32/  126]
train() client id: f_00000-3-1 loss: 1.144112  [   64/  126]
train() client id: f_00000-3-2 loss: 1.170727  [   96/  126]
train() client id: f_00000-4-0 loss: 1.188500  [   32/  126]
train() client id: f_00000-4-1 loss: 1.071235  [   64/  126]
train() client id: f_00000-4-2 loss: 1.135747  [   96/  126]
train() client id: f_00000-5-0 loss: 1.071708  [   32/  126]
train() client id: f_00000-5-1 loss: 1.086090  [   64/  126]
train() client id: f_00000-5-2 loss: 1.066633  [   96/  126]
train() client id: f_00000-6-0 loss: 1.032261  [   32/  126]
train() client id: f_00000-6-1 loss: 1.082522  [   64/  126]
train() client id: f_00000-6-2 loss: 0.999941  [   96/  126]
train() client id: f_00000-7-0 loss: 1.009511  [   32/  126]
train() client id: f_00000-7-1 loss: 0.983676  [   64/  126]
train() client id: f_00000-7-2 loss: 1.092430  [   96/  126]
train() client id: f_00000-8-0 loss: 0.957823  [   32/  126]
train() client id: f_00000-8-1 loss: 0.975111  [   64/  126]
train() client id: f_00000-8-2 loss: 1.018129  [   96/  126]
train() client id: f_00000-9-0 loss: 0.986998  [   32/  126]
train() client id: f_00000-9-1 loss: 0.925162  [   64/  126]
train() client id: f_00000-9-2 loss: 0.985076  [   96/  126]
train() client id: f_00000-10-0 loss: 0.911853  [   32/  126]
train() client id: f_00000-10-1 loss: 1.052639  [   64/  126]
train() client id: f_00000-10-2 loss: 0.976282  [   96/  126]
train() client id: f_00000-11-0 loss: 0.967587  [   32/  126]
train() client id: f_00000-11-1 loss: 0.970891  [   64/  126]
train() client id: f_00000-11-2 loss: 0.971118  [   96/  126]
train() client id: f_00001-0-0 loss: 0.515269  [   32/  265]
train() client id: f_00001-0-1 loss: 0.564155  [   64/  265]
train() client id: f_00001-0-2 loss: 0.649686  [   96/  265]
train() client id: f_00001-0-3 loss: 0.501525  [  128/  265]
train() client id: f_00001-0-4 loss: 0.466462  [  160/  265]
train() client id: f_00001-0-5 loss: 0.543175  [  192/  265]
train() client id: f_00001-0-6 loss: 0.474353  [  224/  265]
train() client id: f_00001-0-7 loss: 0.485783  [  256/  265]
train() client id: f_00001-1-0 loss: 0.483430  [   32/  265]
train() client id: f_00001-1-1 loss: 0.481982  [   64/  265]
train() client id: f_00001-1-2 loss: 0.420657  [   96/  265]
train() client id: f_00001-1-3 loss: 0.663795  [  128/  265]
train() client id: f_00001-1-4 loss: 0.614591  [  160/  265]
train() client id: f_00001-1-5 loss: 0.516155  [  192/  265]
train() client id: f_00001-1-6 loss: 0.416326  [  224/  265]
train() client id: f_00001-1-7 loss: 0.482603  [  256/  265]
train() client id: f_00001-2-0 loss: 0.539230  [   32/  265]
train() client id: f_00001-2-1 loss: 0.595642  [   64/  265]
train() client id: f_00001-2-2 loss: 0.687883  [   96/  265]
train() client id: f_00001-2-3 loss: 0.466507  [  128/  265]
train() client id: f_00001-2-4 loss: 0.421864  [  160/  265]
train() client id: f_00001-2-5 loss: 0.465358  [  192/  265]
train() client id: f_00001-2-6 loss: 0.423016  [  224/  265]
train() client id: f_00001-2-7 loss: 0.476723  [  256/  265]
train() client id: f_00001-3-0 loss: 0.522195  [   32/  265]
train() client id: f_00001-3-1 loss: 0.607516  [   64/  265]
train() client id: f_00001-3-2 loss: 0.565486  [   96/  265]
train() client id: f_00001-3-3 loss: 0.522773  [  128/  265]
train() client id: f_00001-3-4 loss: 0.492914  [  160/  265]
train() client id: f_00001-3-5 loss: 0.419103  [  192/  265]
train() client id: f_00001-3-6 loss: 0.430555  [  224/  265]
train() client id: f_00001-3-7 loss: 0.482446  [  256/  265]
train() client id: f_00001-4-0 loss: 0.500961  [   32/  265]
train() client id: f_00001-4-1 loss: 0.614179  [   64/  265]
train() client id: f_00001-4-2 loss: 0.460913  [   96/  265]
train() client id: f_00001-4-3 loss: 0.577400  [  128/  265]
train() client id: f_00001-4-4 loss: 0.572273  [  160/  265]
train() client id: f_00001-4-5 loss: 0.414259  [  192/  265]
train() client id: f_00001-4-6 loss: 0.424871  [  224/  265]
train() client id: f_00001-4-7 loss: 0.455811  [  256/  265]
train() client id: f_00001-5-0 loss: 0.470655  [   32/  265]
train() client id: f_00001-5-1 loss: 0.567172  [   64/  265]
train() client id: f_00001-5-2 loss: 0.464837  [   96/  265]
train() client id: f_00001-5-3 loss: 0.539459  [  128/  265]
train() client id: f_00001-5-4 loss: 0.463201  [  160/  265]
train() client id: f_00001-5-5 loss: 0.509202  [  192/  265]
train() client id: f_00001-5-6 loss: 0.412565  [  224/  265]
train() client id: f_00001-5-7 loss: 0.578780  [  256/  265]
train() client id: f_00001-6-0 loss: 0.549339  [   32/  265]
train() client id: f_00001-6-1 loss: 0.519455  [   64/  265]
train() client id: f_00001-6-2 loss: 0.465886  [   96/  265]
train() client id: f_00001-6-3 loss: 0.401632  [  128/  265]
train() client id: f_00001-6-4 loss: 0.556233  [  160/  265]
train() client id: f_00001-6-5 loss: 0.563889  [  192/  265]
train() client id: f_00001-6-6 loss: 0.410070  [  224/  265]
train() client id: f_00001-6-7 loss: 0.448803  [  256/  265]
train() client id: f_00001-7-0 loss: 0.557736  [   32/  265]
train() client id: f_00001-7-1 loss: 0.383202  [   64/  265]
train() client id: f_00001-7-2 loss: 0.548419  [   96/  265]
train() client id: f_00001-7-3 loss: 0.474705  [  128/  265]
train() client id: f_00001-7-4 loss: 0.381600  [  160/  265]
train() client id: f_00001-7-5 loss: 0.473856  [  192/  265]
train() client id: f_00001-7-6 loss: 0.599583  [  224/  265]
train() client id: f_00001-7-7 loss: 0.505416  [  256/  265]
train() client id: f_00001-8-0 loss: 0.408788  [   32/  265]
train() client id: f_00001-8-1 loss: 0.601573  [   64/  265]
train() client id: f_00001-8-2 loss: 0.435686  [   96/  265]
train() client id: f_00001-8-3 loss: 0.594711  [  128/  265]
train() client id: f_00001-8-4 loss: 0.510386  [  160/  265]
train() client id: f_00001-8-5 loss: 0.525603  [  192/  265]
train() client id: f_00001-8-6 loss: 0.436501  [  224/  265]
train() client id: f_00001-8-7 loss: 0.480543  [  256/  265]
train() client id: f_00001-9-0 loss: 0.464991  [   32/  265]
train() client id: f_00001-9-1 loss: 0.460312  [   64/  265]
train() client id: f_00001-9-2 loss: 0.441896  [   96/  265]
train() client id: f_00001-9-3 loss: 0.638425  [  128/  265]
train() client id: f_00001-9-4 loss: 0.596880  [  160/  265]
train() client id: f_00001-9-5 loss: 0.419491  [  192/  265]
train() client id: f_00001-9-6 loss: 0.461502  [  224/  265]
train() client id: f_00001-9-7 loss: 0.453831  [  256/  265]
train() client id: f_00001-10-0 loss: 0.530148  [   32/  265]
train() client id: f_00001-10-1 loss: 0.436650  [   64/  265]
train() client id: f_00001-10-2 loss: 0.415386  [   96/  265]
train() client id: f_00001-10-3 loss: 0.501754  [  128/  265]
train() client id: f_00001-10-4 loss: 0.673384  [  160/  265]
train() client id: f_00001-10-5 loss: 0.461187  [  192/  265]
train() client id: f_00001-10-6 loss: 0.563118  [  224/  265]
train() client id: f_00001-10-7 loss: 0.412242  [  256/  265]
train() client id: f_00001-11-0 loss: 0.500973  [   32/  265]
train() client id: f_00001-11-1 loss: 0.459034  [   64/  265]
train() client id: f_00001-11-2 loss: 0.534355  [   96/  265]
train() client id: f_00001-11-3 loss: 0.457069  [  128/  265]
train() client id: f_00001-11-4 loss: 0.405286  [  160/  265]
train() client id: f_00001-11-5 loss: 0.413940  [  192/  265]
train() client id: f_00001-11-6 loss: 0.623867  [  224/  265]
train() client id: f_00001-11-7 loss: 0.608410  [  256/  265]
train() client id: f_00002-0-0 loss: 1.295280  [   32/  124]
train() client id: f_00002-0-1 loss: 1.053888  [   64/  124]
train() client id: f_00002-0-2 loss: 1.158827  [   96/  124]
train() client id: f_00002-1-0 loss: 1.129451  [   32/  124]
train() client id: f_00002-1-1 loss: 1.074329  [   64/  124]
train() client id: f_00002-1-2 loss: 1.138739  [   96/  124]
train() client id: f_00002-2-0 loss: 1.033010  [   32/  124]
train() client id: f_00002-2-1 loss: 1.039046  [   64/  124]
train() client id: f_00002-2-2 loss: 1.071960  [   96/  124]
train() client id: f_00002-3-0 loss: 1.106609  [   32/  124]
train() client id: f_00002-3-1 loss: 1.103617  [   64/  124]
train() client id: f_00002-3-2 loss: 1.017417  [   96/  124]
train() client id: f_00002-4-0 loss: 1.070634  [   32/  124]
train() client id: f_00002-4-1 loss: 1.046683  [   64/  124]
train() client id: f_00002-4-2 loss: 1.016117  [   96/  124]
train() client id: f_00002-5-0 loss: 0.937433  [   32/  124]
train() client id: f_00002-5-1 loss: 1.189013  [   64/  124]
train() client id: f_00002-5-2 loss: 0.992221  [   96/  124]
train() client id: f_00002-6-0 loss: 0.907831  [   32/  124]
train() client id: f_00002-6-1 loss: 1.061196  [   64/  124]
train() client id: f_00002-6-2 loss: 1.123136  [   96/  124]
train() client id: f_00002-7-0 loss: 0.867596  [   32/  124]
train() client id: f_00002-7-1 loss: 1.055537  [   64/  124]
train() client id: f_00002-7-2 loss: 1.020828  [   96/  124]
train() client id: f_00002-8-0 loss: 0.886630  [   32/  124]
train() client id: f_00002-8-1 loss: 1.171342  [   64/  124]
train() client id: f_00002-8-2 loss: 0.870411  [   96/  124]
train() client id: f_00002-9-0 loss: 0.946094  [   32/  124]
train() client id: f_00002-9-1 loss: 1.023567  [   64/  124]
train() client id: f_00002-9-2 loss: 0.941514  [   96/  124]
train() client id: f_00002-10-0 loss: 1.021370  [   32/  124]
train() client id: f_00002-10-1 loss: 1.066914  [   64/  124]
train() client id: f_00002-10-2 loss: 0.862712  [   96/  124]
train() client id: f_00002-11-0 loss: 1.058470  [   32/  124]
train() client id: f_00002-11-1 loss: 0.957937  [   64/  124]
train() client id: f_00002-11-2 loss: 0.916253  [   96/  124]
train() client id: f_00003-0-0 loss: 0.646091  [   32/   43]
train() client id: f_00003-1-0 loss: 0.911736  [   32/   43]
train() client id: f_00003-2-0 loss: 0.754243  [   32/   43]
train() client id: f_00003-3-0 loss: 0.746494  [   32/   43]
train() client id: f_00003-4-0 loss: 0.797608  [   32/   43]
train() client id: f_00003-5-0 loss: 0.772011  [   32/   43]
train() client id: f_00003-6-0 loss: 0.651104  [   32/   43]
train() client id: f_00003-7-0 loss: 0.613380  [   32/   43]
train() client id: f_00003-8-0 loss: 0.896268  [   32/   43]
train() client id: f_00003-9-0 loss: 0.651015  [   32/   43]
train() client id: f_00003-10-0 loss: 0.573699  [   32/   43]
train() client id: f_00003-11-0 loss: 0.790380  [   32/   43]
train() client id: f_00004-0-0 loss: 0.797670  [   32/  306]
train() client id: f_00004-0-1 loss: 0.958236  [   64/  306]
train() client id: f_00004-0-2 loss: 0.940998  [   96/  306]
train() client id: f_00004-0-3 loss: 0.742306  [  128/  306]
train() client id: f_00004-0-4 loss: 0.651704  [  160/  306]
train() client id: f_00004-0-5 loss: 0.785507  [  192/  306]
train() client id: f_00004-0-6 loss: 0.873870  [  224/  306]
train() client id: f_00004-0-7 loss: 0.724673  [  256/  306]
train() client id: f_00004-0-8 loss: 0.885573  [  288/  306]
train() client id: f_00004-1-0 loss: 0.803354  [   32/  306]
train() client id: f_00004-1-1 loss: 0.828772  [   64/  306]
train() client id: f_00004-1-2 loss: 0.770858  [   96/  306]
train() client id: f_00004-1-3 loss: 0.712992  [  128/  306]
train() client id: f_00004-1-4 loss: 0.819653  [  160/  306]
train() client id: f_00004-1-5 loss: 0.809537  [  192/  306]
train() client id: f_00004-1-6 loss: 0.843671  [  224/  306]
train() client id: f_00004-1-7 loss: 0.949370  [  256/  306]
train() client id: f_00004-1-8 loss: 0.792101  [  288/  306]
train() client id: f_00004-2-0 loss: 0.951740  [   32/  306]
train() client id: f_00004-2-1 loss: 0.821578  [   64/  306]
train() client id: f_00004-2-2 loss: 0.765764  [   96/  306]
train() client id: f_00004-2-3 loss: 0.746437  [  128/  306]
train() client id: f_00004-2-4 loss: 0.812642  [  160/  306]
train() client id: f_00004-2-5 loss: 0.800101  [  192/  306]
train() client id: f_00004-2-6 loss: 0.842022  [  224/  306]
train() client id: f_00004-2-7 loss: 0.778789  [  256/  306]
train() client id: f_00004-2-8 loss: 0.768437  [  288/  306]
train() client id: f_00004-3-0 loss: 0.816274  [   32/  306]
train() client id: f_00004-3-1 loss: 0.834604  [   64/  306]
train() client id: f_00004-3-2 loss: 0.972368  [   96/  306]
train() client id: f_00004-3-3 loss: 0.671564  [  128/  306]
train() client id: f_00004-3-4 loss: 0.808101  [  160/  306]
train() client id: f_00004-3-5 loss: 0.882130  [  192/  306]
train() client id: f_00004-3-6 loss: 0.759734  [  224/  306]
train() client id: f_00004-3-7 loss: 0.815770  [  256/  306]
train() client id: f_00004-3-8 loss: 0.741824  [  288/  306]
train() client id: f_00004-4-0 loss: 0.846389  [   32/  306]
train() client id: f_00004-4-1 loss: 0.783450  [   64/  306]
train() client id: f_00004-4-2 loss: 0.774284  [   96/  306]
train() client id: f_00004-4-3 loss: 0.763356  [  128/  306]
train() client id: f_00004-4-4 loss: 0.733290  [  160/  306]
train() client id: f_00004-4-5 loss: 0.978396  [  192/  306]
train() client id: f_00004-4-6 loss: 0.796370  [  224/  306]
train() client id: f_00004-4-7 loss: 0.881147  [  256/  306]
train() client id: f_00004-4-8 loss: 0.798207  [  288/  306]
train() client id: f_00004-5-0 loss: 0.772475  [   32/  306]
train() client id: f_00004-5-1 loss: 0.723121  [   64/  306]
train() client id: f_00004-5-2 loss: 0.768068  [   96/  306]
train() client id: f_00004-5-3 loss: 0.934655  [  128/  306]
train() client id: f_00004-5-4 loss: 0.875928  [  160/  306]
train() client id: f_00004-5-5 loss: 0.824584  [  192/  306]
train() client id: f_00004-5-6 loss: 0.777244  [  224/  306]
train() client id: f_00004-5-7 loss: 0.780620  [  256/  306]
train() client id: f_00004-5-8 loss: 0.891751  [  288/  306]
train() client id: f_00004-6-0 loss: 0.772049  [   32/  306]
train() client id: f_00004-6-1 loss: 0.972180  [   64/  306]
train() client id: f_00004-6-2 loss: 0.835221  [   96/  306]
train() client id: f_00004-6-3 loss: 0.799893  [  128/  306]
train() client id: f_00004-6-4 loss: 0.751914  [  160/  306]
train() client id: f_00004-6-5 loss: 0.730092  [  192/  306]
train() client id: f_00004-6-6 loss: 0.848181  [  224/  306]
train() client id: f_00004-6-7 loss: 0.800858  [  256/  306]
train() client id: f_00004-6-8 loss: 0.818489  [  288/  306]
train() client id: f_00004-7-0 loss: 0.827172  [   32/  306]
train() client id: f_00004-7-1 loss: 0.798264  [   64/  306]
train() client id: f_00004-7-2 loss: 0.743669  [   96/  306]
train() client id: f_00004-7-3 loss: 0.830449  [  128/  306]
train() client id: f_00004-7-4 loss: 0.878480  [  160/  306]
train() client id: f_00004-7-5 loss: 0.818694  [  192/  306]
train() client id: f_00004-7-6 loss: 0.862079  [  224/  306]
train() client id: f_00004-7-7 loss: 0.864467  [  256/  306]
train() client id: f_00004-7-8 loss: 0.762339  [  288/  306]
train() client id: f_00004-8-0 loss: 0.667683  [   32/  306]
train() client id: f_00004-8-1 loss: 0.909469  [   64/  306]
train() client id: f_00004-8-2 loss: 0.824124  [   96/  306]
train() client id: f_00004-8-3 loss: 0.854566  [  128/  306]
train() client id: f_00004-8-4 loss: 0.750373  [  160/  306]
train() client id: f_00004-8-5 loss: 0.872696  [  192/  306]
train() client id: f_00004-8-6 loss: 0.902292  [  224/  306]
train() client id: f_00004-8-7 loss: 0.847073  [  256/  306]
train() client id: f_00004-8-8 loss: 0.826342  [  288/  306]
train() client id: f_00004-9-0 loss: 0.803680  [   32/  306]
train() client id: f_00004-9-1 loss: 0.931564  [   64/  306]
train() client id: f_00004-9-2 loss: 0.746512  [   96/  306]
train() client id: f_00004-9-3 loss: 0.897868  [  128/  306]
train() client id: f_00004-9-4 loss: 0.677843  [  160/  306]
train() client id: f_00004-9-5 loss: 0.827454  [  192/  306]
train() client id: f_00004-9-6 loss: 0.899426  [  224/  306]
train() client id: f_00004-9-7 loss: 0.772096  [  256/  306]
train() client id: f_00004-9-8 loss: 0.880192  [  288/  306]
train() client id: f_00004-10-0 loss: 0.828476  [   32/  306]
train() client id: f_00004-10-1 loss: 0.903263  [   64/  306]
train() client id: f_00004-10-2 loss: 0.739325  [   96/  306]
train() client id: f_00004-10-3 loss: 0.890948  [  128/  306]
train() client id: f_00004-10-4 loss: 0.908117  [  160/  306]
train() client id: f_00004-10-5 loss: 0.845706  [  192/  306]
train() client id: f_00004-10-6 loss: 0.766683  [  224/  306]
train() client id: f_00004-10-7 loss: 0.837125  [  256/  306]
train() client id: f_00004-10-8 loss: 0.701447  [  288/  306]
train() client id: f_00004-11-0 loss: 0.809919  [   32/  306]
train() client id: f_00004-11-1 loss: 0.954742  [   64/  306]
train() client id: f_00004-11-2 loss: 0.798874  [   96/  306]
train() client id: f_00004-11-3 loss: 0.832117  [  128/  306]
train() client id: f_00004-11-4 loss: 0.802808  [  160/  306]
train() client id: f_00004-11-5 loss: 0.799725  [  192/  306]
train() client id: f_00004-11-6 loss: 0.699480  [  224/  306]
train() client id: f_00004-11-7 loss: 0.881788  [  256/  306]
train() client id: f_00004-11-8 loss: 0.891318  [  288/  306]
train() client id: f_00005-0-0 loss: 0.784819  [   32/  146]
train() client id: f_00005-0-1 loss: 0.801338  [   64/  146]
train() client id: f_00005-0-2 loss: 0.754374  [   96/  146]
train() client id: f_00005-0-3 loss: 0.899559  [  128/  146]
train() client id: f_00005-1-0 loss: 0.896133  [   32/  146]
train() client id: f_00005-1-1 loss: 0.751783  [   64/  146]
train() client id: f_00005-1-2 loss: 0.792582  [   96/  146]
train() client id: f_00005-1-3 loss: 0.703258  [  128/  146]
train() client id: f_00005-2-0 loss: 0.955677  [   32/  146]
train() client id: f_00005-2-1 loss: 0.893947  [   64/  146]
train() client id: f_00005-2-2 loss: 0.750865  [   96/  146]
train() client id: f_00005-2-3 loss: 0.669351  [  128/  146]
train() client id: f_00005-3-0 loss: 0.674511  [   32/  146]
train() client id: f_00005-3-1 loss: 0.699564  [   64/  146]
train() client id: f_00005-3-2 loss: 0.803665  [   96/  146]
train() client id: f_00005-3-3 loss: 0.889733  [  128/  146]
train() client id: f_00005-4-0 loss: 0.879811  [   32/  146]
train() client id: f_00005-4-1 loss: 0.601683  [   64/  146]
train() client id: f_00005-4-2 loss: 0.840598  [   96/  146]
train() client id: f_00005-4-3 loss: 0.827886  [  128/  146]
train() client id: f_00005-5-0 loss: 0.991521  [   32/  146]
train() client id: f_00005-5-1 loss: 0.812547  [   64/  146]
train() client id: f_00005-5-2 loss: 0.717819  [   96/  146]
train() client id: f_00005-5-3 loss: 0.621374  [  128/  146]
train() client id: f_00005-6-0 loss: 0.847634  [   32/  146]
train() client id: f_00005-6-1 loss: 0.886282  [   64/  146]
train() client id: f_00005-6-2 loss: 0.652532  [   96/  146]
train() client id: f_00005-6-3 loss: 0.837792  [  128/  146]
train() client id: f_00005-7-0 loss: 0.812238  [   32/  146]
train() client id: f_00005-7-1 loss: 0.839536  [   64/  146]
train() client id: f_00005-7-2 loss: 0.618286  [   96/  146]
train() client id: f_00005-7-3 loss: 0.840486  [  128/  146]
train() client id: f_00005-8-0 loss: 0.738048  [   32/  146]
train() client id: f_00005-8-1 loss: 0.889117  [   64/  146]
train() client id: f_00005-8-2 loss: 0.804579  [   96/  146]
train() client id: f_00005-8-3 loss: 0.733438  [  128/  146]
train() client id: f_00005-9-0 loss: 0.779872  [   32/  146]
train() client id: f_00005-9-1 loss: 0.767130  [   64/  146]
train() client id: f_00005-9-2 loss: 0.894883  [   96/  146]
train() client id: f_00005-9-3 loss: 0.757365  [  128/  146]
train() client id: f_00005-10-0 loss: 0.752376  [   32/  146]
train() client id: f_00005-10-1 loss: 0.987761  [   64/  146]
train() client id: f_00005-10-2 loss: 0.622514  [   96/  146]
train() client id: f_00005-10-3 loss: 0.796481  [  128/  146]
train() client id: f_00005-11-0 loss: 0.843198  [   32/  146]
train() client id: f_00005-11-1 loss: 0.762501  [   64/  146]
train() client id: f_00005-11-2 loss: 0.773680  [   96/  146]
train() client id: f_00005-11-3 loss: 0.805754  [  128/  146]
train() client id: f_00006-0-0 loss: 0.612331  [   32/   54]
train() client id: f_00006-1-0 loss: 0.647438  [   32/   54]
train() client id: f_00006-2-0 loss: 0.710723  [   32/   54]
train() client id: f_00006-3-0 loss: 0.713457  [   32/   54]
train() client id: f_00006-4-0 loss: 0.718942  [   32/   54]
train() client id: f_00006-5-0 loss: 0.662464  [   32/   54]
train() client id: f_00006-6-0 loss: 0.656918  [   32/   54]
train() client id: f_00006-7-0 loss: 0.692792  [   32/   54]
train() client id: f_00006-8-0 loss: 0.704765  [   32/   54]
train() client id: f_00006-9-0 loss: 0.643456  [   32/   54]
train() client id: f_00006-10-0 loss: 0.661327  [   32/   54]
train() client id: f_00006-11-0 loss: 0.647620  [   32/   54]
train() client id: f_00007-0-0 loss: 0.665030  [   32/  179]
train() client id: f_00007-0-1 loss: 0.585580  [   64/  179]
train() client id: f_00007-0-2 loss: 0.726761  [   96/  179]
train() client id: f_00007-0-3 loss: 0.777210  [  128/  179]
train() client id: f_00007-0-4 loss: 0.616151  [  160/  179]
train() client id: f_00007-1-0 loss: 0.751569  [   32/  179]
train() client id: f_00007-1-1 loss: 0.754048  [   64/  179]
train() client id: f_00007-1-2 loss: 0.626492  [   96/  179]
train() client id: f_00007-1-3 loss: 0.619699  [  128/  179]
train() client id: f_00007-1-4 loss: 0.574512  [  160/  179]
train() client id: f_00007-2-0 loss: 0.512560  [   32/  179]
train() client id: f_00007-2-1 loss: 0.653683  [   64/  179]
train() client id: f_00007-2-2 loss: 0.597045  [   96/  179]
train() client id: f_00007-2-3 loss: 0.900711  [  128/  179]
train() client id: f_00007-2-4 loss: 0.542040  [  160/  179]
train() client id: f_00007-3-0 loss: 0.509576  [   32/  179]
train() client id: f_00007-3-1 loss: 0.792063  [   64/  179]
train() client id: f_00007-3-2 loss: 0.668613  [   96/  179]
train() client id: f_00007-3-3 loss: 0.542943  [  128/  179]
train() client id: f_00007-3-4 loss: 0.615122  [  160/  179]
train() client id: f_00007-4-0 loss: 0.690856  [   32/  179]
train() client id: f_00007-4-1 loss: 0.537365  [   64/  179]
train() client id: f_00007-4-2 loss: 0.652019  [   96/  179]
train() client id: f_00007-4-3 loss: 0.606613  [  128/  179]
train() client id: f_00007-4-4 loss: 0.669608  [  160/  179]
train() client id: f_00007-5-0 loss: 0.653421  [   32/  179]
train() client id: f_00007-5-1 loss: 0.693876  [   64/  179]
train() client id: f_00007-5-2 loss: 0.689864  [   96/  179]
train() client id: f_00007-5-3 loss: 0.548000  [  128/  179]
train() client id: f_00007-5-4 loss: 0.610228  [  160/  179]
train() client id: f_00007-6-0 loss: 0.595457  [   32/  179]
train() client id: f_00007-6-1 loss: 0.591409  [   64/  179]
train() client id: f_00007-6-2 loss: 0.669936  [   96/  179]
train() client id: f_00007-6-3 loss: 0.644701  [  128/  179]
train() client id: f_00007-6-4 loss: 0.602257  [  160/  179]
train() client id: f_00007-7-0 loss: 0.669829  [   32/  179]
train() client id: f_00007-7-1 loss: 0.556138  [   64/  179]
train() client id: f_00007-7-2 loss: 0.547243  [   96/  179]
train() client id: f_00007-7-3 loss: 0.526168  [  128/  179]
train() client id: f_00007-7-4 loss: 0.657549  [  160/  179]
train() client id: f_00007-8-0 loss: 0.524092  [   32/  179]
train() client id: f_00007-8-1 loss: 0.580161  [   64/  179]
train() client id: f_00007-8-2 loss: 0.646242  [   96/  179]
train() client id: f_00007-8-3 loss: 0.638329  [  128/  179]
train() client id: f_00007-8-4 loss: 0.768693  [  160/  179]
train() client id: f_00007-9-0 loss: 0.589296  [   32/  179]
train() client id: f_00007-9-1 loss: 0.556629  [   64/  179]
train() client id: f_00007-9-2 loss: 0.630465  [   96/  179]
train() client id: f_00007-9-3 loss: 0.574035  [  128/  179]
train() client id: f_00007-9-4 loss: 0.710562  [  160/  179]
train() client id: f_00007-10-0 loss: 0.664944  [   32/  179]
train() client id: f_00007-10-1 loss: 0.574244  [   64/  179]
train() client id: f_00007-10-2 loss: 0.617836  [   96/  179]
train() client id: f_00007-10-3 loss: 0.588841  [  128/  179]
train() client id: f_00007-10-4 loss: 0.629388  [  160/  179]
train() client id: f_00007-11-0 loss: 0.468904  [   32/  179]
train() client id: f_00007-11-1 loss: 0.754835  [   64/  179]
train() client id: f_00007-11-2 loss: 0.707109  [   96/  179]
train() client id: f_00007-11-3 loss: 0.579673  [  128/  179]
train() client id: f_00007-11-4 loss: 0.583060  [  160/  179]
train() client id: f_00008-0-0 loss: 0.717105  [   32/  130]
train() client id: f_00008-0-1 loss: 0.864200  [   64/  130]
train() client id: f_00008-0-2 loss: 0.828196  [   96/  130]
train() client id: f_00008-0-3 loss: 0.725557  [  128/  130]
train() client id: f_00008-1-0 loss: 0.704389  [   32/  130]
train() client id: f_00008-1-1 loss: 0.804700  [   64/  130]
train() client id: f_00008-1-2 loss: 0.797140  [   96/  130]
train() client id: f_00008-1-3 loss: 0.785556  [  128/  130]
train() client id: f_00008-2-0 loss: 0.822540  [   32/  130]
train() client id: f_00008-2-1 loss: 0.804587  [   64/  130]
train() client id: f_00008-2-2 loss: 0.700619  [   96/  130]
train() client id: f_00008-2-3 loss: 0.797847  [  128/  130]
train() client id: f_00008-3-0 loss: 0.758219  [   32/  130]
train() client id: f_00008-3-1 loss: 0.780292  [   64/  130]
train() client id: f_00008-3-2 loss: 0.693536  [   96/  130]
train() client id: f_00008-3-3 loss: 0.896093  [  128/  130]
train() client id: f_00008-4-0 loss: 0.794517  [   32/  130]
train() client id: f_00008-4-1 loss: 0.750697  [   64/  130]
train() client id: f_00008-4-2 loss: 0.863074  [   96/  130]
train() client id: f_00008-4-3 loss: 0.706870  [  128/  130]
train() client id: f_00008-5-0 loss: 0.804045  [   32/  130]
train() client id: f_00008-5-1 loss: 0.871497  [   64/  130]
train() client id: f_00008-5-2 loss: 0.668719  [   96/  130]
train() client id: f_00008-5-3 loss: 0.760337  [  128/  130]
train() client id: f_00008-6-0 loss: 0.739136  [   32/  130]
train() client id: f_00008-6-1 loss: 0.932690  [   64/  130]
train() client id: f_00008-6-2 loss: 0.774643  [   96/  130]
train() client id: f_00008-6-3 loss: 0.675346  [  128/  130]
train() client id: f_00008-7-0 loss: 0.741674  [   32/  130]
train() client id: f_00008-7-1 loss: 0.828156  [   64/  130]
train() client id: f_00008-7-2 loss: 0.812396  [   96/  130]
train() client id: f_00008-7-3 loss: 0.737971  [  128/  130]
train() client id: f_00008-8-0 loss: 0.800590  [   32/  130]
train() client id: f_00008-8-1 loss: 0.714644  [   64/  130]
train() client id: f_00008-8-2 loss: 0.805484  [   96/  130]
train() client id: f_00008-8-3 loss: 0.806547  [  128/  130]
train() client id: f_00008-9-0 loss: 0.718403  [   32/  130]
train() client id: f_00008-9-1 loss: 0.788734  [   64/  130]
train() client id: f_00008-9-2 loss: 0.791057  [   96/  130]
train() client id: f_00008-9-3 loss: 0.805628  [  128/  130]
train() client id: f_00008-10-0 loss: 0.677667  [   32/  130]
train() client id: f_00008-10-1 loss: 0.820963  [   64/  130]
train() client id: f_00008-10-2 loss: 0.738909  [   96/  130]
train() client id: f_00008-10-3 loss: 0.856366  [  128/  130]
train() client id: f_00008-11-0 loss: 0.812850  [   32/  130]
train() client id: f_00008-11-1 loss: 0.720772  [   64/  130]
train() client id: f_00008-11-2 loss: 0.823208  [   96/  130]
train() client id: f_00008-11-3 loss: 0.701564  [  128/  130]
train() client id: f_00009-0-0 loss: 1.172127  [   32/  118]
train() client id: f_00009-0-1 loss: 1.163219  [   64/  118]
train() client id: f_00009-0-2 loss: 1.144172  [   96/  118]
train() client id: f_00009-1-0 loss: 1.172521  [   32/  118]
train() client id: f_00009-1-1 loss: 1.167978  [   64/  118]
train() client id: f_00009-1-2 loss: 1.026394  [   96/  118]
train() client id: f_00009-2-0 loss: 0.979819  [   32/  118]
train() client id: f_00009-2-1 loss: 1.190257  [   64/  118]
train() client id: f_00009-2-2 loss: 1.008913  [   96/  118]
train() client id: f_00009-3-0 loss: 1.025342  [   32/  118]
train() client id: f_00009-3-1 loss: 0.968665  [   64/  118]
train() client id: f_00009-3-2 loss: 1.125265  [   96/  118]
train() client id: f_00009-4-0 loss: 0.992416  [   32/  118]
train() client id: f_00009-4-1 loss: 1.126206  [   64/  118]
train() client id: f_00009-4-2 loss: 1.005873  [   96/  118]
train() client id: f_00009-5-0 loss: 0.952267  [   32/  118]
train() client id: f_00009-5-1 loss: 1.023131  [   64/  118]
train() client id: f_00009-5-2 loss: 0.955093  [   96/  118]
train() client id: f_00009-6-0 loss: 0.971451  [   32/  118]
train() client id: f_00009-6-1 loss: 1.052441  [   64/  118]
train() client id: f_00009-6-2 loss: 0.900536  [   96/  118]
train() client id: f_00009-7-0 loss: 0.794314  [   32/  118]
train() client id: f_00009-7-1 loss: 1.044033  [   64/  118]
train() client id: f_00009-7-2 loss: 0.899321  [   96/  118]
train() client id: f_00009-8-0 loss: 0.927141  [   32/  118]
train() client id: f_00009-8-1 loss: 0.871349  [   64/  118]
train() client id: f_00009-8-2 loss: 1.029801  [   96/  118]
train() client id: f_00009-9-0 loss: 0.895487  [   32/  118]
train() client id: f_00009-9-1 loss: 0.954383  [   64/  118]
train() client id: f_00009-9-2 loss: 0.941057  [   96/  118]
train() client id: f_00009-10-0 loss: 0.919286  [   32/  118]
train() client id: f_00009-10-1 loss: 1.029726  [   64/  118]
train() client id: f_00009-10-2 loss: 0.768748  [   96/  118]
train() client id: f_00009-11-0 loss: 0.922908  [   32/  118]
train() client id: f_00009-11-1 loss: 0.848611  [   64/  118]
train() client id: f_00009-11-2 loss: 0.886707  [   96/  118]
At round 16 accuracy: 0.6312997347480106
At round 16 training accuracy: 0.5814889336016097
At round 16 training loss: 0.8345347554630916
update_location
xs = [ -3.9056584    4.20031788 100.00902392  18.81129433   0.97929623
   3.95640986 -62.44319194 -41.32485185  84.66397685 -27.06087855]
ys = [ 92.5879595   75.55583871   1.32061395 -62.45517586  54.35018685
  37.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [136.33702509 125.40465475 141.43390289 119.39226854 113.81960214
 106.98393887 117.92388546 108.20545105 132.19931524 103.6740228 ]
dists_bs = [189.90792018 204.94363894 325.26065297 306.50901839 213.36609411
 225.48914357 210.28473576 219.56069707 303.66082845 226.11164695]
uav_gains = [4.60616434e-11 5.67774848e-11 4.20154197e-11 6.42006706e-11
 7.23514457e-11 8.44693888e-11 6.62184205e-11 8.21054949e-11
 4.97558800e-11 9.13739608e-11]
bs_gains = [4.60652538e-11 3.72149675e-11 1.02104716e-11 1.20573805e-11
 3.32462714e-11 2.84801519e-11 3.46283970e-11 3.06860651e-11
 1.23767188e-11 2.82611527e-11]
Round 17
-------------------------------
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.74794599 18.22367373  8.61494673  3.08364678 21.02075903 10.12863146
  3.83198593 12.340238    9.082055    8.2217282 ]
obj_prev = 103.29561085136496
eta_min = 2.3980574083554427e-11	eta_max = 0.9215530004474818
af = 21.83294705699746	bf = 1.718217883710131	zeta = 24.01624176269721	eta = 0.9090909090909091
af = 21.83294705699746	bf = 1.718217883710131	zeta = 41.790319927079516	eta = 0.5224402946685754
af = 21.83294705699746	bf = 1.718217883710131	zeta = 33.27990204277066	eta = 0.6560400036315671
af = 21.83294705699746	bf = 1.718217883710131	zeta = 31.752345685579005	eta = 0.6876010759391976
af = 21.83294705699746	bf = 1.718217883710131	zeta = 31.67639108760098	eta = 0.6892498263649575
af = 21.83294705699746	bf = 1.718217883710131	zeta = 31.676189751909728	eta = 0.6892542072766557
eta = 0.6892542072766557
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [0.03070852 0.06458539 0.03022109 0.01047989 0.07457784 0.03558289
 0.0131608  0.0436256  0.0316834  0.02875878]
ene_total = [2.73656278 5.19862747 2.71178509 1.24687687 5.93149526 3.1458414
 1.43681478 3.6092118  3.0031103  2.655864  ]
ti_comp = [0.33188313 0.32847851 0.33045231 0.33663064 0.32655044 0.32374602
 0.33704245 0.33977473 0.30458757 0.32360102]
ti_coms = [0.07323726 0.07664188 0.07466807 0.06848974 0.07856994 0.08137436
 0.06807793 0.06534565 0.10053281 0.08151936]
t_total = [29.14992867 29.14992867 29.14992867 29.14992867 29.14992867 29.14992867
 29.14992867 29.14992867 29.14992867 29.14992867]
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [1.64318511e-05 1.56051844e-04 1.57976562e-05 6.34809284e-07
 2.43113417e-04 2.68654979e-05 1.25417250e-06 4.49492172e-05
 2.14264829e-05 1.41961788e-05]
ene_total = [0.52815183 0.56269552 0.53840142 0.492855   0.58283307 0.5874521
 0.48993648 0.47342069 0.72491292 0.58758383]
optimize_network iter = 0 obj = 5.56824286220838
eta = 0.6892542072766557
freqs = [4.62640631e+07 9.83099137e+07 4.57268501e+07 1.55658631e+07
 1.14190388e+08 5.49549501e+07 1.95239441e+07 6.41978302e+07
 5.20103250e+07 4.44355465e+07]
eta_min = 0.6892542072766561	eta_max = 0.6892542072766527
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 0.03573652736322809	eta = 0.9090909090909091
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 18.93445726929715	eta = 0.0017158005474531947
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 1.9284046028512083	eta = 0.016846958413371853
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 1.882600477836571	eta = 0.017256848986739414
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 1.8825910021965795	eta = 0.01725693584558884
eta = 0.01725693584558884
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [1.79246673e-04 1.70228989e-03 1.72328565e-04 6.92481033e-06
 2.65200012e-03 2.93061998e-04 1.36811274e-05 4.90328058e-04
 2.33730561e-04 1.54858866e-04]
ene_total = [0.17070727 0.21310565 0.17380525 0.15598508 0.23910014 0.1918103
 0.15520187 0.15983003 0.23404959 0.18899581]
ti_comp = [0.33188313 0.32847851 0.33045231 0.33663064 0.32655044 0.32374602
 0.33704245 0.33977473 0.30458757 0.32360102]
ti_coms = [0.07323726 0.07664188 0.07466807 0.06848974 0.07856994 0.08137436
 0.06807793 0.06534565 0.10053281 0.08151936]
t_total = [29.14992867 29.14992867 29.14992867 29.14992867 29.14992867 29.14992867
 29.14992867 29.14992867 29.14992867 29.14992867]
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [1.64318511e-05 1.56051844e-04 1.57976562e-05 6.34809284e-07
 2.43113417e-04 2.68654979e-05 1.25417250e-06 4.49492172e-05
 2.14264829e-05 1.41961788e-05]
ene_total = [0.52815183 0.56269552 0.53840142 0.492855   0.58283307 0.5874521
 0.48993648 0.47342069 0.72491292 0.58758383]
optimize_network iter = 1 obj = 5.56824286220839
eta = 0.6892542072766561
freqs = [4.62640631e+07 9.83099137e+07 4.57268501e+07 1.55658631e+07
 1.14190388e+08 5.49549501e+07 1.95239441e+07 6.41978302e+07
 5.20103250e+07 4.44355465e+07]
Done!
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [1.61811483e-05 1.53670942e-04 1.55566295e-05 6.25123921e-07
 2.39404205e-04 2.64556077e-05 1.23503744e-06 4.42634215e-05
 2.10995764e-05 1.39795860e-05]
ene_total = [0.00733991 0.00781786 0.00748236 0.0068496  0.0080964  0.00816389
 0.00680903 0.00657883 0.01007438 0.00816592]
At round 17 energy consumption: 0.07737817222069016
At round 17 eta: 0.6892542072766561
At round 17 a_n: 22.016777610806663
At round 17 local rounds: 12.185922115151007
At round 17 global rounds: 71.9537447693859
gradient difference: 0.45332565903663635
train() client id: f_00000-0-0 loss: 1.551245  [   32/  126]
train() client id: f_00000-0-1 loss: 1.350969  [   64/  126]
train() client id: f_00000-0-2 loss: 1.396173  [   96/  126]
train() client id: f_00000-1-0 loss: 1.384248  [   32/  126]
train() client id: f_00000-1-1 loss: 1.528479  [   64/  126]
train() client id: f_00000-1-2 loss: 1.161472  [   96/  126]
train() client id: f_00000-2-0 loss: 1.285588  [   32/  126]
train() client id: f_00000-2-1 loss: 1.119616  [   64/  126]
train() client id: f_00000-2-2 loss: 1.238067  [   96/  126]
train() client id: f_00000-3-0 loss: 1.082951  [   32/  126]
train() client id: f_00000-3-1 loss: 1.069703  [   64/  126]
train() client id: f_00000-3-2 loss: 1.169394  [   96/  126]
train() client id: f_00000-4-0 loss: 1.078761  [   32/  126]
train() client id: f_00000-4-1 loss: 1.010963  [   64/  126]
train() client id: f_00000-4-2 loss: 1.052963  [   96/  126]
train() client id: f_00000-5-0 loss: 0.960845  [   32/  126]
train() client id: f_00000-5-1 loss: 0.972843  [   64/  126]
train() client id: f_00000-5-2 loss: 1.010462  [   96/  126]
train() client id: f_00000-6-0 loss: 1.021600  [   32/  126]
train() client id: f_00000-6-1 loss: 0.920205  [   64/  126]
train() client id: f_00000-6-2 loss: 1.000353  [   96/  126]
train() client id: f_00000-7-0 loss: 0.916212  [   32/  126]
train() client id: f_00000-7-1 loss: 0.916298  [   64/  126]
train() client id: f_00000-7-2 loss: 0.946078  [   96/  126]
train() client id: f_00000-8-0 loss: 0.876943  [   32/  126]
train() client id: f_00000-8-1 loss: 0.908137  [   64/  126]
train() client id: f_00000-8-2 loss: 0.940288  [   96/  126]
train() client id: f_00000-9-0 loss: 0.840863  [   32/  126]
train() client id: f_00000-9-1 loss: 0.952122  [   64/  126]
train() client id: f_00000-9-2 loss: 0.867181  [   96/  126]
train() client id: f_00000-10-0 loss: 0.869748  [   32/  126]
train() client id: f_00000-10-1 loss: 0.822759  [   64/  126]
train() client id: f_00000-10-2 loss: 0.874160  [   96/  126]
train() client id: f_00000-11-0 loss: 0.854397  [   32/  126]
train() client id: f_00000-11-1 loss: 0.883897  [   64/  126]
train() client id: f_00000-11-2 loss: 0.882805  [   96/  126]
train() client id: f_00001-0-0 loss: 0.460583  [   32/  265]
train() client id: f_00001-0-1 loss: 0.430914  [   64/  265]
train() client id: f_00001-0-2 loss: 0.456976  [   96/  265]
train() client id: f_00001-0-3 loss: 0.450026  [  128/  265]
train() client id: f_00001-0-4 loss: 0.513637  [  160/  265]
train() client id: f_00001-0-5 loss: 0.425507  [  192/  265]
train() client id: f_00001-0-6 loss: 0.434761  [  224/  265]
train() client id: f_00001-0-7 loss: 0.422829  [  256/  265]
train() client id: f_00001-1-0 loss: 0.368263  [   32/  265]
train() client id: f_00001-1-1 loss: 0.348112  [   64/  265]
train() client id: f_00001-1-2 loss: 0.598512  [   96/  265]
train() client id: f_00001-1-3 loss: 0.497344  [  128/  265]
train() client id: f_00001-1-4 loss: 0.432250  [  160/  265]
train() client id: f_00001-1-5 loss: 0.375580  [  192/  265]
train() client id: f_00001-1-6 loss: 0.353758  [  224/  265]
train() client id: f_00001-1-7 loss: 0.566462  [  256/  265]
train() client id: f_00001-2-0 loss: 0.426519  [   32/  265]
train() client id: f_00001-2-1 loss: 0.423828  [   64/  265]
train() client id: f_00001-2-2 loss: 0.397652  [   96/  265]
train() client id: f_00001-2-3 loss: 0.387141  [  128/  265]
train() client id: f_00001-2-4 loss: 0.412360  [  160/  265]
train() client id: f_00001-2-5 loss: 0.463050  [  192/  265]
train() client id: f_00001-2-6 loss: 0.474591  [  224/  265]
train() client id: f_00001-2-7 loss: 0.506970  [  256/  265]
train() client id: f_00001-3-0 loss: 0.413522  [   32/  265]
train() client id: f_00001-3-1 loss: 0.347386  [   64/  265]
train() client id: f_00001-3-2 loss: 0.346735  [   96/  265]
train() client id: f_00001-3-3 loss: 0.422521  [  128/  265]
train() client id: f_00001-3-4 loss: 0.567025  [  160/  265]
train() client id: f_00001-3-5 loss: 0.544927  [  192/  265]
train() client id: f_00001-3-6 loss: 0.403807  [  224/  265]
train() client id: f_00001-3-7 loss: 0.391041  [  256/  265]
train() client id: f_00001-4-0 loss: 0.564168  [   32/  265]
train() client id: f_00001-4-1 loss: 0.320948  [   64/  265]
train() client id: f_00001-4-2 loss: 0.488911  [   96/  265]
train() client id: f_00001-4-3 loss: 0.330947  [  128/  265]
train() client id: f_00001-4-4 loss: 0.459015  [  160/  265]
train() client id: f_00001-4-5 loss: 0.478916  [  192/  265]
train() client id: f_00001-4-6 loss: 0.410955  [  224/  265]
train() client id: f_00001-4-7 loss: 0.343221  [  256/  265]
train() client id: f_00001-5-0 loss: 0.477732  [   32/  265]
train() client id: f_00001-5-1 loss: 0.390746  [   64/  265]
train() client id: f_00001-5-2 loss: 0.337794  [   96/  265]
train() client id: f_00001-5-3 loss: 0.331307  [  128/  265]
train() client id: f_00001-5-4 loss: 0.381639  [  160/  265]
train() client id: f_00001-5-5 loss: 0.593762  [  192/  265]
train() client id: f_00001-5-6 loss: 0.518906  [  224/  265]
train() client id: f_00001-5-7 loss: 0.341036  [  256/  265]
train() client id: f_00001-6-0 loss: 0.324125  [   32/  265]
train() client id: f_00001-6-1 loss: 0.331107  [   64/  265]
train() client id: f_00001-6-2 loss: 0.592516  [   96/  265]
train() client id: f_00001-6-3 loss: 0.337409  [  128/  265]
train() client id: f_00001-6-4 loss: 0.400714  [  160/  265]
train() client id: f_00001-6-5 loss: 0.432090  [  192/  265]
train() client id: f_00001-6-6 loss: 0.328755  [  224/  265]
train() client id: f_00001-6-7 loss: 0.565243  [  256/  265]
train() client id: f_00001-7-0 loss: 0.518920  [   32/  265]
train() client id: f_00001-7-1 loss: 0.392474  [   64/  265]
train() client id: f_00001-7-2 loss: 0.443863  [   96/  265]
train() client id: f_00001-7-3 loss: 0.369206  [  128/  265]
train() client id: f_00001-7-4 loss: 0.444496  [  160/  265]
train() client id: f_00001-7-5 loss: 0.391398  [  192/  265]
train() client id: f_00001-7-6 loss: 0.362294  [  224/  265]
train() client id: f_00001-7-7 loss: 0.324540  [  256/  265]
train() client id: f_00001-8-0 loss: 0.364398  [   32/  265]
train() client id: f_00001-8-1 loss: 0.605971  [   64/  265]
train() client id: f_00001-8-2 loss: 0.354245  [   96/  265]
train() client id: f_00001-8-3 loss: 0.378455  [  128/  265]
train() client id: f_00001-8-4 loss: 0.323333  [  160/  265]
train() client id: f_00001-8-5 loss: 0.509354  [  192/  265]
train() client id: f_00001-8-6 loss: 0.337439  [  224/  265]
train() client id: f_00001-8-7 loss: 0.471875  [  256/  265]
train() client id: f_00001-9-0 loss: 0.339145  [   32/  265]
train() client id: f_00001-9-1 loss: 0.445758  [   64/  265]
train() client id: f_00001-9-2 loss: 0.439053  [   96/  265]
train() client id: f_00001-9-3 loss: 0.409712  [  128/  265]
train() client id: f_00001-9-4 loss: 0.486014  [  160/  265]
train() client id: f_00001-9-5 loss: 0.304325  [  192/  265]
train() client id: f_00001-9-6 loss: 0.459579  [  224/  265]
train() client id: f_00001-9-7 loss: 0.382257  [  256/  265]
train() client id: f_00001-10-0 loss: 0.382993  [   32/  265]
train() client id: f_00001-10-1 loss: 0.361221  [   64/  265]
train() client id: f_00001-10-2 loss: 0.362946  [   96/  265]
train() client id: f_00001-10-3 loss: 0.452524  [  128/  265]
train() client id: f_00001-10-4 loss: 0.432435  [  160/  265]
train() client id: f_00001-10-5 loss: 0.424222  [  192/  265]
train() client id: f_00001-10-6 loss: 0.418688  [  224/  265]
train() client id: f_00001-10-7 loss: 0.499760  [  256/  265]
train() client id: f_00001-11-0 loss: 0.314077  [   32/  265]
train() client id: f_00001-11-1 loss: 0.406343  [   64/  265]
train() client id: f_00001-11-2 loss: 0.365377  [   96/  265]
train() client id: f_00001-11-3 loss: 0.410665  [  128/  265]
train() client id: f_00001-11-4 loss: 0.381558  [  160/  265]
train() client id: f_00001-11-5 loss: 0.438869  [  192/  265]
train() client id: f_00001-11-6 loss: 0.600408  [  224/  265]
train() client id: f_00001-11-7 loss: 0.352023  [  256/  265]
train() client id: f_00002-0-0 loss: 1.232778  [   32/  124]
train() client id: f_00002-0-1 loss: 1.295933  [   64/  124]
train() client id: f_00002-0-2 loss: 1.162378  [   96/  124]
train() client id: f_00002-1-0 loss: 1.175985  [   32/  124]
train() client id: f_00002-1-1 loss: 1.300298  [   64/  124]
train() client id: f_00002-1-2 loss: 1.073451  [   96/  124]
train() client id: f_00002-2-0 loss: 1.105243  [   32/  124]
train() client id: f_00002-2-1 loss: 1.108238  [   64/  124]
train() client id: f_00002-2-2 loss: 1.158543  [   96/  124]
train() client id: f_00002-3-0 loss: 1.152002  [   32/  124]
train() client id: f_00002-3-1 loss: 1.026888  [   64/  124]
train() client id: f_00002-3-2 loss: 1.163598  [   96/  124]
train() client id: f_00002-4-0 loss: 1.180829  [   32/  124]
train() client id: f_00002-4-1 loss: 0.968276  [   64/  124]
train() client id: f_00002-4-2 loss: 1.232784  [   96/  124]
train() client id: f_00002-5-0 loss: 1.000708  [   32/  124]
train() client id: f_00002-5-1 loss: 1.092131  [   64/  124]
train() client id: f_00002-5-2 loss: 1.197338  [   96/  124]
train() client id: f_00002-6-0 loss: 1.127443  [   32/  124]
train() client id: f_00002-6-1 loss: 1.148412  [   64/  124]
train() client id: f_00002-6-2 loss: 1.061346  [   96/  124]
train() client id: f_00002-7-0 loss: 1.156622  [   32/  124]
train() client id: f_00002-7-1 loss: 1.047801  [   64/  124]
train() client id: f_00002-7-2 loss: 1.035651  [   96/  124]
train() client id: f_00002-8-0 loss: 1.096133  [   32/  124]
train() client id: f_00002-8-1 loss: 1.084519  [   64/  124]
train() client id: f_00002-8-2 loss: 1.093076  [   96/  124]
train() client id: f_00002-9-0 loss: 1.115925  [   32/  124]
train() client id: f_00002-9-1 loss: 1.045005  [   64/  124]
train() client id: f_00002-9-2 loss: 1.064272  [   96/  124]
train() client id: f_00002-10-0 loss: 1.120684  [   32/  124]
train() client id: f_00002-10-1 loss: 1.108567  [   64/  124]
train() client id: f_00002-10-2 loss: 0.993296  [   96/  124]
train() client id: f_00002-11-0 loss: 1.213111  [   32/  124]
train() client id: f_00002-11-1 loss: 0.964087  [   64/  124]
train() client id: f_00002-11-2 loss: 1.007962  [   96/  124]
train() client id: f_00003-0-0 loss: 0.756653  [   32/   43]
train() client id: f_00003-1-0 loss: 0.677431  [   32/   43]
train() client id: f_00003-2-0 loss: 0.562514  [   32/   43]
train() client id: f_00003-3-0 loss: 0.825910  [   32/   43]
train() client id: f_00003-4-0 loss: 0.631929  [   32/   43]
train() client id: f_00003-5-0 loss: 0.732391  [   32/   43]
train() client id: f_00003-6-0 loss: 0.798313  [   32/   43]
train() client id: f_00003-7-0 loss: 0.766353  [   32/   43]
train() client id: f_00003-8-0 loss: 0.614011  [   32/   43]
train() client id: f_00003-9-0 loss: 0.598887  [   32/   43]
train() client id: f_00003-10-0 loss: 0.577104  [   32/   43]
train() client id: f_00003-11-0 loss: 0.816647  [   32/   43]
train() client id: f_00004-0-0 loss: 0.781595  [   32/  306]
train() client id: f_00004-0-1 loss: 0.810026  [   64/  306]
train() client id: f_00004-0-2 loss: 0.954509  [   96/  306]
train() client id: f_00004-0-3 loss: 0.881250  [  128/  306]
train() client id: f_00004-0-4 loss: 0.849345  [  160/  306]
train() client id: f_00004-0-5 loss: 1.002633  [  192/  306]
train() client id: f_00004-0-6 loss: 0.734769  [  224/  306]
train() client id: f_00004-0-7 loss: 0.811363  [  256/  306]
train() client id: f_00004-0-8 loss: 0.746532  [  288/  306]
train() client id: f_00004-1-0 loss: 0.913015  [   32/  306]
train() client id: f_00004-1-1 loss: 0.747955  [   64/  306]
train() client id: f_00004-1-2 loss: 0.883346  [   96/  306]
train() client id: f_00004-1-3 loss: 0.997955  [  128/  306]
train() client id: f_00004-1-4 loss: 0.730830  [  160/  306]
train() client id: f_00004-1-5 loss: 0.754724  [  192/  306]
train() client id: f_00004-1-6 loss: 0.907096  [  224/  306]
train() client id: f_00004-1-7 loss: 0.908371  [  256/  306]
train() client id: f_00004-1-8 loss: 0.748868  [  288/  306]
train() client id: f_00004-2-0 loss: 0.791038  [   32/  306]
train() client id: f_00004-2-1 loss: 0.978492  [   64/  306]
train() client id: f_00004-2-2 loss: 0.982899  [   96/  306]
train() client id: f_00004-2-3 loss: 0.708995  [  128/  306]
train() client id: f_00004-2-4 loss: 0.870913  [  160/  306]
train() client id: f_00004-2-5 loss: 0.823339  [  192/  306]
train() client id: f_00004-2-6 loss: 0.981726  [  224/  306]
train() client id: f_00004-2-7 loss: 0.881554  [  256/  306]
train() client id: f_00004-2-8 loss: 0.698356  [  288/  306]
train() client id: f_00004-3-0 loss: 0.783561  [   32/  306]
train() client id: f_00004-3-1 loss: 0.841362  [   64/  306]
train() client id: f_00004-3-2 loss: 0.783665  [   96/  306]
train() client id: f_00004-3-3 loss: 0.781804  [  128/  306]
train() client id: f_00004-3-4 loss: 0.839209  [  160/  306]
train() client id: f_00004-3-5 loss: 0.803445  [  192/  306]
train() client id: f_00004-3-6 loss: 1.043812  [  224/  306]
train() client id: f_00004-3-7 loss: 0.909296  [  256/  306]
train() client id: f_00004-3-8 loss: 0.820092  [  288/  306]
train() client id: f_00004-4-0 loss: 0.786087  [   32/  306]
train() client id: f_00004-4-1 loss: 0.819541  [   64/  306]
train() client id: f_00004-4-2 loss: 0.934881  [   96/  306]
train() client id: f_00004-4-3 loss: 0.837692  [  128/  306]
train() client id: f_00004-4-4 loss: 0.912276  [  160/  306]
train() client id: f_00004-4-5 loss: 0.877118  [  192/  306]
train() client id: f_00004-4-6 loss: 0.709326  [  224/  306]
train() client id: f_00004-4-7 loss: 0.838838  [  256/  306]
train() client id: f_00004-4-8 loss: 0.865150  [  288/  306]
train() client id: f_00004-5-0 loss: 0.806062  [   32/  306]
train() client id: f_00004-5-1 loss: 0.789478  [   64/  306]
train() client id: f_00004-5-2 loss: 0.870761  [   96/  306]
train() client id: f_00004-5-3 loss: 0.785945  [  128/  306]
train() client id: f_00004-5-4 loss: 0.749435  [  160/  306]
train() client id: f_00004-5-5 loss: 0.840561  [  192/  306]
train() client id: f_00004-5-6 loss: 0.973211  [  224/  306]
train() client id: f_00004-5-7 loss: 0.944020  [  256/  306]
train() client id: f_00004-5-8 loss: 0.794316  [  288/  306]
train() client id: f_00004-6-0 loss: 0.977951  [   32/  306]
train() client id: f_00004-6-1 loss: 0.949602  [   64/  306]
train() client id: f_00004-6-2 loss: 1.022574  [   96/  306]
train() client id: f_00004-6-3 loss: 0.794772  [  128/  306]
train() client id: f_00004-6-4 loss: 0.877669  [  160/  306]
train() client id: f_00004-6-5 loss: 0.815627  [  192/  306]
train() client id: f_00004-6-6 loss: 0.714453  [  224/  306]
train() client id: f_00004-6-7 loss: 0.807149  [  256/  306]
train() client id: f_00004-6-8 loss: 0.785884  [  288/  306]
train() client id: f_00004-7-0 loss: 0.821668  [   32/  306]
train() client id: f_00004-7-1 loss: 0.875446  [   64/  306]
train() client id: f_00004-7-2 loss: 0.772850  [   96/  306]
train() client id: f_00004-7-3 loss: 0.806349  [  128/  306]
train() client id: f_00004-7-4 loss: 0.929560  [  160/  306]
train() client id: f_00004-7-5 loss: 0.741831  [  192/  306]
train() client id: f_00004-7-6 loss: 0.952428  [  224/  306]
train() client id: f_00004-7-7 loss: 0.801448  [  256/  306]
train() client id: f_00004-7-8 loss: 0.827066  [  288/  306]
train() client id: f_00004-8-0 loss: 0.901776  [   32/  306]
train() client id: f_00004-8-1 loss: 0.812372  [   64/  306]
train() client id: f_00004-8-2 loss: 0.953972  [   96/  306]
train() client id: f_00004-8-3 loss: 0.731566  [  128/  306]
train() client id: f_00004-8-4 loss: 0.841009  [  160/  306]
train() client id: f_00004-8-5 loss: 0.836798  [  192/  306]
train() client id: f_00004-8-6 loss: 0.817766  [  224/  306]
train() client id: f_00004-8-7 loss: 0.789622  [  256/  306]
train() client id: f_00004-8-8 loss: 0.880025  [  288/  306]
train() client id: f_00004-9-0 loss: 0.812660  [   32/  306]
train() client id: f_00004-9-1 loss: 0.829108  [   64/  306]
train() client id: f_00004-9-2 loss: 0.820655  [   96/  306]
train() client id: f_00004-9-3 loss: 0.844960  [  128/  306]
train() client id: f_00004-9-4 loss: 0.727142  [  160/  306]
train() client id: f_00004-9-5 loss: 0.817888  [  192/  306]
train() client id: f_00004-9-6 loss: 0.874451  [  224/  306]
train() client id: f_00004-9-7 loss: 0.911804  [  256/  306]
train() client id: f_00004-9-8 loss: 0.945635  [  288/  306]
train() client id: f_00004-10-0 loss: 0.809453  [   32/  306]
train() client id: f_00004-10-1 loss: 0.833163  [   64/  306]
train() client id: f_00004-10-2 loss: 1.030809  [   96/  306]
train() client id: f_00004-10-3 loss: 0.824180  [  128/  306]
train() client id: f_00004-10-4 loss: 0.730995  [  160/  306]
train() client id: f_00004-10-5 loss: 0.820043  [  192/  306]
train() client id: f_00004-10-6 loss: 0.813958  [  224/  306]
train() client id: f_00004-10-7 loss: 0.741880  [  256/  306]
train() client id: f_00004-10-8 loss: 0.911050  [  288/  306]
train() client id: f_00004-11-0 loss: 0.852332  [   32/  306]
train() client id: f_00004-11-1 loss: 0.828425  [   64/  306]
train() client id: f_00004-11-2 loss: 0.796851  [   96/  306]
train() client id: f_00004-11-3 loss: 0.867875  [  128/  306]
train() client id: f_00004-11-4 loss: 0.950222  [  160/  306]
train() client id: f_00004-11-5 loss: 0.875802  [  192/  306]
train() client id: f_00004-11-6 loss: 0.756914  [  224/  306]
train() client id: f_00004-11-7 loss: 0.860634  [  256/  306]
train() client id: f_00004-11-8 loss: 0.828222  [  288/  306]
train() client id: f_00005-0-0 loss: 0.612351  [   32/  146]
train() client id: f_00005-0-1 loss: 0.655420  [   64/  146]
train() client id: f_00005-0-2 loss: 0.748290  [   96/  146]
train() client id: f_00005-0-3 loss: 0.546532  [  128/  146]
train() client id: f_00005-1-0 loss: 0.653378  [   32/  146]
train() client id: f_00005-1-1 loss: 0.606505  [   64/  146]
train() client id: f_00005-1-2 loss: 0.677712  [   96/  146]
train() client id: f_00005-1-3 loss: 0.665615  [  128/  146]
train() client id: f_00005-2-0 loss: 0.770022  [   32/  146]
train() client id: f_00005-2-1 loss: 0.699835  [   64/  146]
train() client id: f_00005-2-2 loss: 0.568583  [   96/  146]
train() client id: f_00005-2-3 loss: 0.634292  [  128/  146]
train() client id: f_00005-3-0 loss: 0.559564  [   32/  146]
train() client id: f_00005-3-1 loss: 0.501726  [   64/  146]
train() client id: f_00005-3-2 loss: 0.604551  [   96/  146]
train() client id: f_00005-3-3 loss: 0.763972  [  128/  146]
train() client id: f_00005-4-0 loss: 0.702233  [   32/  146]
train() client id: f_00005-4-1 loss: 0.716915  [   64/  146]
train() client id: f_00005-4-2 loss: 0.704886  [   96/  146]
train() client id: f_00005-4-3 loss: 0.555583  [  128/  146]
train() client id: f_00005-5-0 loss: 0.729180  [   32/  146]
train() client id: f_00005-5-1 loss: 0.702678  [   64/  146]
train() client id: f_00005-5-2 loss: 0.605910  [   96/  146]
train() client id: f_00005-5-3 loss: 0.533796  [  128/  146]
train() client id: f_00005-6-0 loss: 0.550016  [   32/  146]
train() client id: f_00005-6-1 loss: 0.679712  [   64/  146]
train() client id: f_00005-6-2 loss: 0.551180  [   96/  146]
train() client id: f_00005-6-3 loss: 0.767003  [  128/  146]
train() client id: f_00005-7-0 loss: 0.611544  [   32/  146]
train() client id: f_00005-7-1 loss: 0.837716  [   64/  146]
train() client id: f_00005-7-2 loss: 0.714652  [   96/  146]
train() client id: f_00005-7-3 loss: 0.381390  [  128/  146]
train() client id: f_00005-8-0 loss: 0.794260  [   32/  146]
train() client id: f_00005-8-1 loss: 0.662205  [   64/  146]
train() client id: f_00005-8-2 loss: 0.648248  [   96/  146]
train() client id: f_00005-8-3 loss: 0.524522  [  128/  146]
train() client id: f_00005-9-0 loss: 0.855060  [   32/  146]
train() client id: f_00005-9-1 loss: 0.461354  [   64/  146]
train() client id: f_00005-9-2 loss: 0.461978  [   96/  146]
train() client id: f_00005-9-3 loss: 0.638539  [  128/  146]
train() client id: f_00005-10-0 loss: 0.770590  [   32/  146]
train() client id: f_00005-10-1 loss: 0.471737  [   64/  146]
train() client id: f_00005-10-2 loss: 0.737666  [   96/  146]
train() client id: f_00005-10-3 loss: 0.475754  [  128/  146]
train() client id: f_00005-11-0 loss: 0.609011  [   32/  146]
train() client id: f_00005-11-1 loss: 0.754958  [   64/  146]
train() client id: f_00005-11-2 loss: 0.564975  [   96/  146]
train() client id: f_00005-11-3 loss: 0.597914  [  128/  146]
train() client id: f_00006-0-0 loss: 0.611631  [   32/   54]
train() client id: f_00006-1-0 loss: 0.645171  [   32/   54]
train() client id: f_00006-2-0 loss: 0.603142  [   32/   54]
train() client id: f_00006-3-0 loss: 0.608704  [   32/   54]
train() client id: f_00006-4-0 loss: 0.665974  [   32/   54]
train() client id: f_00006-5-0 loss: 0.644504  [   32/   54]
train() client id: f_00006-6-0 loss: 0.635115  [   32/   54]
train() client id: f_00006-7-0 loss: 0.594303  [   32/   54]
train() client id: f_00006-8-0 loss: 0.661038  [   32/   54]
train() client id: f_00006-9-0 loss: 0.598307  [   32/   54]
train() client id: f_00006-10-0 loss: 0.647064  [   32/   54]
train() client id: f_00006-11-0 loss: 0.602418  [   32/   54]
train() client id: f_00007-0-0 loss: 0.709446  [   32/  179]
train() client id: f_00007-0-1 loss: 0.569920  [   64/  179]
train() client id: f_00007-0-2 loss: 0.717699  [   96/  179]
train() client id: f_00007-0-3 loss: 0.668840  [  128/  179]
train() client id: f_00007-0-4 loss: 0.752548  [  160/  179]
train() client id: f_00007-1-0 loss: 0.650561  [   32/  179]
train() client id: f_00007-1-1 loss: 0.638249  [   64/  179]
train() client id: f_00007-1-2 loss: 0.767325  [   96/  179]
train() client id: f_00007-1-3 loss: 0.730417  [  128/  179]
train() client id: f_00007-1-4 loss: 0.547750  [  160/  179]
train() client id: f_00007-2-0 loss: 0.680095  [   32/  179]
train() client id: f_00007-2-1 loss: 0.590445  [   64/  179]
train() client id: f_00007-2-2 loss: 0.608067  [   96/  179]
train() client id: f_00007-2-3 loss: 0.723011  [  128/  179]
train() client id: f_00007-2-4 loss: 0.650923  [  160/  179]
train() client id: f_00007-3-0 loss: 0.666381  [   32/  179]
train() client id: f_00007-3-1 loss: 0.731302  [   64/  179]
train() client id: f_00007-3-2 loss: 0.637342  [   96/  179]
train() client id: f_00007-3-3 loss: 0.680774  [  128/  179]
train() client id: f_00007-3-4 loss: 0.600181  [  160/  179]
train() client id: f_00007-4-0 loss: 0.691078  [   32/  179]
train() client id: f_00007-4-1 loss: 0.621829  [   64/  179]
train() client id: f_00007-4-2 loss: 0.658422  [   96/  179]
train() client id: f_00007-4-3 loss: 0.557277  [  128/  179]
train() client id: f_00007-4-4 loss: 0.714818  [  160/  179]
train() client id: f_00007-5-0 loss: 0.671251  [   32/  179]
train() client id: f_00007-5-1 loss: 0.705205  [   64/  179]
train() client id: f_00007-5-2 loss: 0.620031  [   96/  179]
train() client id: f_00007-5-3 loss: 0.633538  [  128/  179]
train() client id: f_00007-5-4 loss: 0.612883  [  160/  179]
train() client id: f_00007-6-0 loss: 0.545737  [   32/  179]
train() client id: f_00007-6-1 loss: 0.614264  [   64/  179]
train() client id: f_00007-6-2 loss: 0.879955  [   96/  179]
train() client id: f_00007-6-3 loss: 0.652245  [  128/  179]
train() client id: f_00007-6-4 loss: 0.600568  [  160/  179]
train() client id: f_00007-7-0 loss: 0.668219  [   32/  179]
train() client id: f_00007-7-1 loss: 0.657696  [   64/  179]
train() client id: f_00007-7-2 loss: 0.605778  [   96/  179]
train() client id: f_00007-7-3 loss: 0.530727  [  128/  179]
train() client id: f_00007-7-4 loss: 0.832000  [  160/  179]
train() client id: f_00007-8-0 loss: 0.718738  [   32/  179]
train() client id: f_00007-8-1 loss: 0.627100  [   64/  179]
train() client id: f_00007-8-2 loss: 0.540894  [   96/  179]
train() client id: f_00007-8-3 loss: 0.809175  [  128/  179]
train() client id: f_00007-8-4 loss: 0.611350  [  160/  179]
train() client id: f_00007-9-0 loss: 0.597020  [   32/  179]
train() client id: f_00007-9-1 loss: 0.572133  [   64/  179]
train() client id: f_00007-9-2 loss: 0.899936  [   96/  179]
train() client id: f_00007-9-3 loss: 0.601410  [  128/  179]
train() client id: f_00007-9-4 loss: 0.600884  [  160/  179]
train() client id: f_00007-10-0 loss: 0.616383  [   32/  179]
train() client id: f_00007-10-1 loss: 0.529933  [   64/  179]
train() client id: f_00007-10-2 loss: 0.525627  [   96/  179]
train() client id: f_00007-10-3 loss: 0.611395  [  128/  179]
train() client id: f_00007-10-4 loss: 0.667858  [  160/  179]
train() client id: f_00007-11-0 loss: 0.670922  [   32/  179]
train() client id: f_00007-11-1 loss: 0.522038  [   64/  179]
train() client id: f_00007-11-2 loss: 0.697837  [   96/  179]
train() client id: f_00007-11-3 loss: 0.637678  [  128/  179]
train() client id: f_00007-11-4 loss: 0.680421  [  160/  179]
train() client id: f_00008-0-0 loss: 0.948253  [   32/  130]
train() client id: f_00008-0-1 loss: 0.827591  [   64/  130]
train() client id: f_00008-0-2 loss: 0.732897  [   96/  130]
train() client id: f_00008-0-3 loss: 0.821813  [  128/  130]
train() client id: f_00008-1-0 loss: 0.825794  [   32/  130]
train() client id: f_00008-1-1 loss: 0.764761  [   64/  130]
train() client id: f_00008-1-2 loss: 0.794737  [   96/  130]
train() client id: f_00008-1-3 loss: 0.916877  [  128/  130]
train() client id: f_00008-2-0 loss: 0.831498  [   32/  130]
train() client id: f_00008-2-1 loss: 0.798393  [   64/  130]
train() client id: f_00008-2-2 loss: 0.826397  [   96/  130]
train() client id: f_00008-2-3 loss: 0.882911  [  128/  130]
train() client id: f_00008-3-0 loss: 0.921410  [   32/  130]
train() client id: f_00008-3-1 loss: 0.800962  [   64/  130]
train() client id: f_00008-3-2 loss: 0.735658  [   96/  130]
train() client id: f_00008-3-3 loss: 0.873189  [  128/  130]
train() client id: f_00008-4-0 loss: 0.823276  [   32/  130]
train() client id: f_00008-4-1 loss: 0.838920  [   64/  130]
train() client id: f_00008-4-2 loss: 0.868241  [   96/  130]
train() client id: f_00008-4-3 loss: 0.797374  [  128/  130]
train() client id: f_00008-5-0 loss: 0.794792  [   32/  130]
train() client id: f_00008-5-1 loss: 0.931274  [   64/  130]
train() client id: f_00008-5-2 loss: 0.816944  [   96/  130]
train() client id: f_00008-5-3 loss: 0.790691  [  128/  130]
train() client id: f_00008-6-0 loss: 0.938504  [   32/  130]
train() client id: f_00008-6-1 loss: 0.739392  [   64/  130]
train() client id: f_00008-6-2 loss: 0.875572  [   96/  130]
train() client id: f_00008-6-3 loss: 0.784448  [  128/  130]
train() client id: f_00008-7-0 loss: 0.845132  [   32/  130]
train() client id: f_00008-7-1 loss: 0.769335  [   64/  130]
train() client id: f_00008-7-2 loss: 0.883367  [   96/  130]
train() client id: f_00008-7-3 loss: 0.835734  [  128/  130]
train() client id: f_00008-8-0 loss: 0.725680  [   32/  130]
train() client id: f_00008-8-1 loss: 0.814665  [   64/  130]
train() client id: f_00008-8-2 loss: 0.914716  [   96/  130]
train() client id: f_00008-8-3 loss: 0.881039  [  128/  130]
train() client id: f_00008-9-0 loss: 0.902178  [   32/  130]
train() client id: f_00008-9-1 loss: 0.735882  [   64/  130]
train() client id: f_00008-9-2 loss: 0.789462  [   96/  130]
train() client id: f_00008-9-3 loss: 0.877730  [  128/  130]
train() client id: f_00008-10-0 loss: 0.782342  [   32/  130]
train() client id: f_00008-10-1 loss: 0.858630  [   64/  130]
train() client id: f_00008-10-2 loss: 0.913081  [   96/  130]
train() client id: f_00008-10-3 loss: 0.743883  [  128/  130]
train() client id: f_00008-11-0 loss: 0.887743  [   32/  130]
train() client id: f_00008-11-1 loss: 0.839005  [   64/  130]
train() client id: f_00008-11-2 loss: 0.785126  [   96/  130]
train() client id: f_00008-11-3 loss: 0.786051  [  128/  130]
train() client id: f_00009-0-0 loss: 1.157791  [   32/  118]
train() client id: f_00009-0-1 loss: 1.311050  [   64/  118]
train() client id: f_00009-0-2 loss: 1.063998  [   96/  118]
train() client id: f_00009-1-0 loss: 1.168629  [   32/  118]
train() client id: f_00009-1-1 loss: 1.119883  [   64/  118]
train() client id: f_00009-1-2 loss: 1.125788  [   96/  118]
train() client id: f_00009-2-0 loss: 1.038759  [   32/  118]
train() client id: f_00009-2-1 loss: 1.099363  [   64/  118]
train() client id: f_00009-2-2 loss: 1.083674  [   96/  118]
train() client id: f_00009-3-0 loss: 1.022551  [   32/  118]
train() client id: f_00009-3-1 loss: 0.998026  [   64/  118]
train() client id: f_00009-3-2 loss: 1.074405  [   96/  118]
train() client id: f_00009-4-0 loss: 1.025182  [   32/  118]
train() client id: f_00009-4-1 loss: 1.013136  [   64/  118]
train() client id: f_00009-4-2 loss: 0.983526  [   96/  118]
train() client id: f_00009-5-0 loss: 0.955260  [   32/  118]
train() client id: f_00009-5-1 loss: 0.947677  [   64/  118]
train() client id: f_00009-5-2 loss: 0.962032  [   96/  118]
train() client id: f_00009-6-0 loss: 0.944409  [   32/  118]
train() client id: f_00009-6-1 loss: 1.007332  [   64/  118]
train() client id: f_00009-6-2 loss: 0.960104  [   96/  118]
train() client id: f_00009-7-0 loss: 0.911187  [   32/  118]
train() client id: f_00009-7-1 loss: 0.930428  [   64/  118]
train() client id: f_00009-7-2 loss: 0.872945  [   96/  118]
train() client id: f_00009-8-0 loss: 0.962514  [   32/  118]
train() client id: f_00009-8-1 loss: 0.806330  [   64/  118]
train() client id: f_00009-8-2 loss: 0.904147  [   96/  118]
train() client id: f_00009-9-0 loss: 0.985267  [   32/  118]
train() client id: f_00009-9-1 loss: 0.872402  [   64/  118]
train() client id: f_00009-9-2 loss: 0.851769  [   96/  118]
train() client id: f_00009-10-0 loss: 0.869770  [   32/  118]
train() client id: f_00009-10-1 loss: 0.806843  [   64/  118]
train() client id: f_00009-10-2 loss: 0.869230  [   96/  118]
train() client id: f_00009-11-0 loss: 0.769206  [   32/  118]
train() client id: f_00009-11-1 loss: 0.909612  [   64/  118]
train() client id: f_00009-11-2 loss: 0.948524  [   96/  118]
At round 17 accuracy: 0.6339522546419099
At round 17 training accuracy: 0.579476861167002
At round 17 training loss: 0.8365417925323212
update_location
xs = [ -3.9056584    4.20031788 105.00902392  18.81129433   0.97929623
   3.95640986 -67.44319194 -46.32485185  89.66397685 -32.06087855]
ys = [ 97.5879595   80.55583871   1.32061395 -67.45517586  59.35018685
  42.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [139.78077123 128.47912601 145.01254816 122.08220814 116.29017026
 108.85175553 120.64607197 110.21192383 135.45589215 105.09001756]
dists_bs = [187.79216635 202.56468974 329.49898727 310.39866319 210.57918222
 222.48257319 207.65139541 216.55287625 307.94729826 222.87235287]
uav_gains = [4.32717486e-11 5.34395670e-11 3.94635132e-11 6.07214029e-11
 6.85691473e-11 8.08920872e-11 6.25451947e-11 7.84191512e-11
 4.68155255e-11 8.83269213e-11]
bs_gains = [4.75332122e-11 3.84517052e-11 9.84697079e-12 1.16390766e-11
 3.44929920e-11 2.95709508e-11 3.58720746e-11 3.18944419e-11
 1.19003619e-11 2.94263727e-11]
Round 18
-------------------------------
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.61607623 17.94328127  8.48520447  3.03808191 20.69729735  9.97190604
  3.77497723 12.15244223  8.94539714  8.09409538]
obj_prev = 101.71875926654823
eta_min = 1.6526197130074664e-11	eta_max = 0.921782499053972
af = 21.498465320302763	bf = 1.69748013632556	zeta = 23.64831185233304	eta = 0.9090909090909091
af = 21.498465320302763	bf = 1.69748013632556	zeta = 41.211530046727255	eta = 0.5216614208675814
af = 21.498465320302763	bf = 1.69748013632556	zeta = 32.795424289084025	eta = 0.6555324648584754
af = 21.498465320302763	bf = 1.69748013632556	zeta = 31.284347894451706	eta = 0.6871955711793989
af = 21.498465320302763	bf = 1.69748013632556	zeta = 31.2090722305542	eta = 0.6888530732821788
af = 21.498465320302763	bf = 1.69748013632556	zeta = 31.208871892116978	eta = 0.6888574952218328
eta = 0.6888574952218328
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [0.03075603 0.06468531 0.03026784 0.01049611 0.07469322 0.03563794
 0.01318116 0.04369309 0.03173241 0.02880327]
ene_total = [2.70140281 5.11569384 2.67726863 1.23269158 5.83681499 3.09264715
 1.41984069 3.55816331 2.96469034 2.60965854]
ti_comp = [0.33703834 0.33514192 0.33556709 0.34199829 0.33331177 0.33056654
 0.34240076 0.34533101 0.3095942  0.33047606]
ti_coms = [0.07420365 0.07610006 0.0756749  0.0692437  0.07793021 0.08067545
 0.06884123 0.06591098 0.10164779 0.08076593]
t_total = [29.09992447 29.09992447 29.09992447 29.09992447 29.09992447 29.09992447
 29.09992447 29.09992447 29.09992447 29.09992447]
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [1.60070873e-05 1.50605001e-04 1.53909571e-05 6.17897996e-07
 2.34434896e-04 2.58880941e-05 1.22087479e-06 4.37166851e-05
 2.08354902e-05 1.36749361e-05]
ene_total = [0.52620567 0.54914919 0.53657281 0.49001957 0.56803143 0.57270002
 0.4872143  0.4694866  0.72074503 0.57247604]
optimize_network iter = 0 obj = 5.492600646974247
eta = 0.6888574952218328
freqs = [4.56269040e+07 9.65043471e+07 4.50995376e+07 1.53452611e+07
 1.12047077e+08 5.39043395e+07 1.92481414e+07 6.32626258e+07
 5.12483990e+07 4.35784487e+07]
eta_min = 0.6888574952218411	eta_max = 0.6888574952218237
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 0.033943701105718926	eta = 0.909090909090909
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 18.704633302137236	eta = 0.001649746862055949
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 1.8983226351773008	eta = 0.016255355925430464
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 1.8547436567121627	eta = 0.016637291080325793
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 1.8547352592532886	eta = 0.016637366406961725
eta = 0.016637366406961725
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [1.75913676e-04 1.65510932e-03 1.69142568e-04 6.79053635e-06
 2.57637780e-03 2.84503340e-04 1.34170925e-05 4.80434863e-04
 2.28976553e-04 1.50283948e-04]
ene_total = [0.17007518 0.20743924 0.17321761 0.15518366 0.23216334 0.18699632
 0.15443092 0.1583265  0.23270867 0.18419382]
ti_comp = [0.33703834 0.33514192 0.33556709 0.34199829 0.33331177 0.33056654
 0.34240076 0.34533101 0.3095942  0.33047606]
ti_coms = [0.07420365 0.07610006 0.0756749  0.0692437  0.07793021 0.08067545
 0.06884123 0.06591098 0.10164779 0.08076593]
t_total = [29.09992447 29.09992447 29.09992447 29.09992447 29.09992447 29.09992447
 29.09992447 29.09992447 29.09992447 29.09992447]
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [1.60070873e-05 1.50605001e-04 1.53909571e-05 6.17897996e-07
 2.34434896e-04 2.58880941e-05 1.22087479e-06 4.37166851e-05
 2.08354902e-05 1.36749361e-05]
ene_total = [0.52620567 0.54914919 0.53657281 0.49001957 0.56803143 0.57270002
 0.4872143  0.4694866  0.72074503 0.57247604]
optimize_network iter = 1 obj = 5.492600646974393
eta = 0.6888574952218411
freqs = [4.56269040e+07 9.65043471e+07 4.50995376e+07 1.53452611e+07
 1.12047077e+08 5.39043395e+07 1.92481414e+07 6.32626258e+07
 5.12483990e+07 4.35784487e+07]
Done!
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [1.57385166e-05 1.48078115e-04 1.51327241e-05 6.07530761e-07
 2.30501493e-04 2.54537376e-05 1.20039067e-06 4.29831965e-05
 2.04859075e-05 1.34454948e-05]
ene_total = [0.0074361  0.00775808 0.00758262 0.00692498 0.00802352 0.008093
 0.00688532 0.00663408 0.01018527 0.00809004]
At round 18 energy consumption: 0.07761301768692713
At round 18 eta: 0.6888574952218411
At round 18 a_n: 21.674231763837348
At round 18 local rounds: 12.204774532627562
At round 18 global rounds: 70.7610733753795
gradient difference: 0.4990878999233246
train() client id: f_00000-0-0 loss: 1.402886  [   32/  126]
train() client id: f_00000-0-1 loss: 1.399624  [   64/  126]
train() client id: f_00000-0-2 loss: 1.245245  [   96/  126]
train() client id: f_00000-1-0 loss: 1.184693  [   32/  126]
train() client id: f_00000-1-1 loss: 1.087164  [   64/  126]
train() client id: f_00000-1-2 loss: 1.292026  [   96/  126]
train() client id: f_00000-2-0 loss: 1.159399  [   32/  126]
train() client id: f_00000-2-1 loss: 1.062047  [   64/  126]
train() client id: f_00000-2-2 loss: 1.120782  [   96/  126]
train() client id: f_00000-3-0 loss: 1.053062  [   32/  126]
train() client id: f_00000-3-1 loss: 1.086142  [   64/  126]
train() client id: f_00000-3-2 loss: 1.057626  [   96/  126]
train() client id: f_00000-4-0 loss: 0.934893  [   32/  126]
train() client id: f_00000-4-1 loss: 0.996944  [   64/  126]
train() client id: f_00000-4-2 loss: 0.995029  [   96/  126]
train() client id: f_00000-5-0 loss: 0.967943  [   32/  126]
train() client id: f_00000-5-1 loss: 0.992137  [   64/  126]
train() client id: f_00000-5-2 loss: 0.937107  [   96/  126]
train() client id: f_00000-6-0 loss: 0.905906  [   32/  126]
train() client id: f_00000-6-1 loss: 0.921815  [   64/  126]
train() client id: f_00000-6-2 loss: 1.024267  [   96/  126]
train() client id: f_00000-7-0 loss: 0.903211  [   32/  126]
train() client id: f_00000-7-1 loss: 0.899804  [   64/  126]
train() client id: f_00000-7-2 loss: 0.933935  [   96/  126]
train() client id: f_00000-8-0 loss: 0.821732  [   32/  126]
train() client id: f_00000-8-1 loss: 0.960142  [   64/  126]
train() client id: f_00000-8-2 loss: 0.870010  [   96/  126]
train() client id: f_00000-9-0 loss: 0.943052  [   32/  126]
train() client id: f_00000-9-1 loss: 0.858555  [   64/  126]
train() client id: f_00000-9-2 loss: 0.908191  [   96/  126]
train() client id: f_00000-10-0 loss: 0.905031  [   32/  126]
train() client id: f_00000-10-1 loss: 0.941576  [   64/  126]
train() client id: f_00000-10-2 loss: 0.859451  [   96/  126]
train() client id: f_00000-11-0 loss: 0.843537  [   32/  126]
train() client id: f_00000-11-1 loss: 0.817494  [   64/  126]
train() client id: f_00000-11-2 loss: 0.906096  [   96/  126]
train() client id: f_00001-0-0 loss: 0.460443  [   32/  265]
train() client id: f_00001-0-1 loss: 0.498118  [   64/  265]
train() client id: f_00001-0-2 loss: 0.464023  [   96/  265]
train() client id: f_00001-0-3 loss: 0.546725  [  128/  265]
train() client id: f_00001-0-4 loss: 0.554144  [  160/  265]
train() client id: f_00001-0-5 loss: 0.569687  [  192/  265]
train() client id: f_00001-0-6 loss: 0.431446  [  224/  265]
train() client id: f_00001-0-7 loss: 0.518965  [  256/  265]
train() client id: f_00001-1-0 loss: 0.492745  [   32/  265]
train() client id: f_00001-1-1 loss: 0.458918  [   64/  265]
train() client id: f_00001-1-2 loss: 0.472875  [   96/  265]
train() client id: f_00001-1-3 loss: 0.474537  [  128/  265]
train() client id: f_00001-1-4 loss: 0.711509  [  160/  265]
train() client id: f_00001-1-5 loss: 0.449193  [  192/  265]
train() client id: f_00001-1-6 loss: 0.472518  [  224/  265]
train() client id: f_00001-1-7 loss: 0.436267  [  256/  265]
train() client id: f_00001-2-0 loss: 0.601520  [   32/  265]
train() client id: f_00001-2-1 loss: 0.429745  [   64/  265]
train() client id: f_00001-2-2 loss: 0.464578  [   96/  265]
train() client id: f_00001-2-3 loss: 0.572147  [  128/  265]
train() client id: f_00001-2-4 loss: 0.422691  [  160/  265]
train() client id: f_00001-2-5 loss: 0.538098  [  192/  265]
train() client id: f_00001-2-6 loss: 0.453511  [  224/  265]
train() client id: f_00001-2-7 loss: 0.495723  [  256/  265]
train() client id: f_00001-3-0 loss: 0.456482  [   32/  265]
train() client id: f_00001-3-1 loss: 0.495771  [   64/  265]
train() client id: f_00001-3-2 loss: 0.410609  [   96/  265]
train() client id: f_00001-3-3 loss: 0.620544  [  128/  265]
train() client id: f_00001-3-4 loss: 0.413357  [  160/  265]
train() client id: f_00001-3-5 loss: 0.467137  [  192/  265]
train() client id: f_00001-3-6 loss: 0.506735  [  224/  265]
train() client id: f_00001-3-7 loss: 0.492436  [  256/  265]
train() client id: f_00001-4-0 loss: 0.450690  [   32/  265]
train() client id: f_00001-4-1 loss: 0.407861  [   64/  265]
train() client id: f_00001-4-2 loss: 0.483805  [   96/  265]
train() client id: f_00001-4-3 loss: 0.489083  [  128/  265]
train() client id: f_00001-4-4 loss: 0.563156  [  160/  265]
train() client id: f_00001-4-5 loss: 0.660162  [  192/  265]
train() client id: f_00001-4-6 loss: 0.386749  [  224/  265]
train() client id: f_00001-4-7 loss: 0.449800  [  256/  265]
train() client id: f_00001-5-0 loss: 0.514971  [   32/  265]
train() client id: f_00001-5-1 loss: 0.526703  [   64/  265]
train() client id: f_00001-5-2 loss: 0.412807  [   96/  265]
train() client id: f_00001-5-3 loss: 0.526960  [  128/  265]
train() client id: f_00001-5-4 loss: 0.487332  [  160/  265]
train() client id: f_00001-5-5 loss: 0.599721  [  192/  265]
train() client id: f_00001-5-6 loss: 0.377065  [  224/  265]
train() client id: f_00001-5-7 loss: 0.376152  [  256/  265]
train() client id: f_00001-6-0 loss: 0.487669  [   32/  265]
train() client id: f_00001-6-1 loss: 0.445007  [   64/  265]
train() client id: f_00001-6-2 loss: 0.392056  [   96/  265]
train() client id: f_00001-6-3 loss: 0.535948  [  128/  265]
train() client id: f_00001-6-4 loss: 0.385788  [  160/  265]
train() client id: f_00001-6-5 loss: 0.641932  [  192/  265]
train() client id: f_00001-6-6 loss: 0.462158  [  224/  265]
train() client id: f_00001-6-7 loss: 0.458390  [  256/  265]
train() client id: f_00001-7-0 loss: 0.459919  [   32/  265]
train() client id: f_00001-7-1 loss: 0.571272  [   64/  265]
train() client id: f_00001-7-2 loss: 0.399401  [   96/  265]
train() client id: f_00001-7-3 loss: 0.462158  [  128/  265]
train() client id: f_00001-7-4 loss: 0.486165  [  160/  265]
train() client id: f_00001-7-5 loss: 0.435641  [  192/  265]
train() client id: f_00001-7-6 loss: 0.492753  [  224/  265]
train() client id: f_00001-7-7 loss: 0.386850  [  256/  265]
train() client id: f_00001-8-0 loss: 0.383011  [   32/  265]
train() client id: f_00001-8-1 loss: 0.514829  [   64/  265]
train() client id: f_00001-8-2 loss: 0.395786  [   96/  265]
train() client id: f_00001-8-3 loss: 0.491502  [  128/  265]
train() client id: f_00001-8-4 loss: 0.480514  [  160/  265]
train() client id: f_00001-8-5 loss: 0.513372  [  192/  265]
train() client id: f_00001-8-6 loss: 0.507508  [  224/  265]
train() client id: f_00001-8-7 loss: 0.427925  [  256/  265]
train() client id: f_00001-9-0 loss: 0.688492  [   32/  265]
train() client id: f_00001-9-1 loss: 0.391669  [   64/  265]
train() client id: f_00001-9-2 loss: 0.469898  [   96/  265]
train() client id: f_00001-9-3 loss: 0.572038  [  128/  265]
train() client id: f_00001-9-4 loss: 0.387539  [  160/  265]
train() client id: f_00001-9-5 loss: 0.382053  [  192/  265]
train() client id: f_00001-9-6 loss: 0.572950  [  224/  265]
train() client id: f_00001-9-7 loss: 0.374973  [  256/  265]
train() client id: f_00001-10-0 loss: 0.425098  [   32/  265]
train() client id: f_00001-10-1 loss: 0.389572  [   64/  265]
train() client id: f_00001-10-2 loss: 0.666681  [   96/  265]
train() client id: f_00001-10-3 loss: 0.449592  [  128/  265]
train() client id: f_00001-10-4 loss: 0.487021  [  160/  265]
train() client id: f_00001-10-5 loss: 0.396868  [  192/  265]
train() client id: f_00001-10-6 loss: 0.472415  [  224/  265]
train() client id: f_00001-10-7 loss: 0.562978  [  256/  265]
train() client id: f_00001-11-0 loss: 0.507224  [   32/  265]
train() client id: f_00001-11-1 loss: 0.495591  [   64/  265]
train() client id: f_00001-11-2 loss: 0.498352  [   96/  265]
train() client id: f_00001-11-3 loss: 0.504623  [  128/  265]
train() client id: f_00001-11-4 loss: 0.486699  [  160/  265]
train() client id: f_00001-11-5 loss: 0.471320  [  192/  265]
train() client id: f_00001-11-6 loss: 0.407839  [  224/  265]
train() client id: f_00001-11-7 loss: 0.472973  [  256/  265]
train() client id: f_00002-0-0 loss: 1.227086  [   32/  124]
train() client id: f_00002-0-1 loss: 1.224607  [   64/  124]
train() client id: f_00002-0-2 loss: 1.118454  [   96/  124]
train() client id: f_00002-1-0 loss: 1.305089  [   32/  124]
train() client id: f_00002-1-1 loss: 1.131762  [   64/  124]
train() client id: f_00002-1-2 loss: 1.094794  [   96/  124]
train() client id: f_00002-2-0 loss: 1.173325  [   32/  124]
train() client id: f_00002-2-1 loss: 1.116297  [   64/  124]
train() client id: f_00002-2-2 loss: 1.132931  [   96/  124]
train() client id: f_00002-3-0 loss: 1.106822  [   32/  124]
train() client id: f_00002-3-1 loss: 1.142424  [   64/  124]
train() client id: f_00002-3-2 loss: 1.032492  [   96/  124]
train() client id: f_00002-4-0 loss: 1.168536  [   32/  124]
train() client id: f_00002-4-1 loss: 1.115016  [   64/  124]
train() client id: f_00002-4-2 loss: 1.050105  [   96/  124]
train() client id: f_00002-5-0 loss: 1.062960  [   32/  124]
train() client id: f_00002-5-1 loss: 0.995008  [   64/  124]
train() client id: f_00002-5-2 loss: 1.109990  [   96/  124]
train() client id: f_00002-6-0 loss: 1.158976  [   32/  124]
train() client id: f_00002-6-1 loss: 1.004812  [   64/  124]
train() client id: f_00002-6-2 loss: 1.034256  [   96/  124]
train() client id: f_00002-7-0 loss: 0.977192  [   32/  124]
train() client id: f_00002-7-1 loss: 1.025621  [   64/  124]
train() client id: f_00002-7-2 loss: 1.129379  [   96/  124]
train() client id: f_00002-8-0 loss: 1.076225  [   32/  124]
train() client id: f_00002-8-1 loss: 0.958742  [   64/  124]
train() client id: f_00002-8-2 loss: 1.133111  [   96/  124]
train() client id: f_00002-9-0 loss: 0.969416  [   32/  124]
train() client id: f_00002-9-1 loss: 1.106083  [   64/  124]
train() client id: f_00002-9-2 loss: 1.056560  [   96/  124]
train() client id: f_00002-10-0 loss: 1.159771  [   32/  124]
train() client id: f_00002-10-1 loss: 1.066602  [   64/  124]
train() client id: f_00002-10-2 loss: 1.048868  [   96/  124]
train() client id: f_00002-11-0 loss: 1.037911  [   32/  124]
train() client id: f_00002-11-1 loss: 1.086233  [   64/  124]
train() client id: f_00002-11-2 loss: 1.130106  [   96/  124]
train() client id: f_00003-0-0 loss: 0.587140  [   32/   43]
train() client id: f_00003-1-0 loss: 0.571623  [   32/   43]
train() client id: f_00003-2-0 loss: 0.568039  [   32/   43]
train() client id: f_00003-3-0 loss: 0.563482  [   32/   43]
train() client id: f_00003-4-0 loss: 0.742614  [   32/   43]
train() client id: f_00003-5-0 loss: 0.706360  [   32/   43]
train() client id: f_00003-6-0 loss: 0.610725  [   32/   43]
train() client id: f_00003-7-0 loss: 0.614492  [   32/   43]
train() client id: f_00003-8-0 loss: 0.485243  [   32/   43]
train() client id: f_00003-9-0 loss: 0.711168  [   32/   43]
train() client id: f_00003-10-0 loss: 0.514478  [   32/   43]
train() client id: f_00003-11-0 loss: 0.609230  [   32/   43]
train() client id: f_00004-0-0 loss: 1.008671  [   32/  306]
train() client id: f_00004-0-1 loss: 0.856959  [   64/  306]
train() client id: f_00004-0-2 loss: 1.045199  [   96/  306]
train() client id: f_00004-0-3 loss: 0.824162  [  128/  306]
train() client id: f_00004-0-4 loss: 0.799716  [  160/  306]
train() client id: f_00004-0-5 loss: 0.906242  [  192/  306]
train() client id: f_00004-0-6 loss: 0.941660  [  224/  306]
train() client id: f_00004-0-7 loss: 0.961826  [  256/  306]
train() client id: f_00004-0-8 loss: 0.888292  [  288/  306]
train() client id: f_00004-1-0 loss: 0.754594  [   32/  306]
train() client id: f_00004-1-1 loss: 0.798119  [   64/  306]
train() client id: f_00004-1-2 loss: 0.907235  [   96/  306]
train() client id: f_00004-1-3 loss: 0.917327  [  128/  306]
train() client id: f_00004-1-4 loss: 0.846144  [  160/  306]
train() client id: f_00004-1-5 loss: 0.884870  [  192/  306]
train() client id: f_00004-1-6 loss: 0.849486  [  224/  306]
train() client id: f_00004-1-7 loss: 1.172591  [  256/  306]
train() client id: f_00004-1-8 loss: 1.007225  [  288/  306]
train() client id: f_00004-2-0 loss: 0.923556  [   32/  306]
train() client id: f_00004-2-1 loss: 0.864372  [   64/  306]
train() client id: f_00004-2-2 loss: 0.802244  [   96/  306]
train() client id: f_00004-2-3 loss: 0.973308  [  128/  306]
train() client id: f_00004-2-4 loss: 1.024496  [  160/  306]
train() client id: f_00004-2-5 loss: 0.879915  [  192/  306]
train() client id: f_00004-2-6 loss: 0.918861  [  224/  306]
train() client id: f_00004-2-7 loss: 0.925770  [  256/  306]
train() client id: f_00004-2-8 loss: 0.840259  [  288/  306]
train() client id: f_00004-3-0 loss: 0.844367  [   32/  306]
train() client id: f_00004-3-1 loss: 0.881866  [   64/  306]
train() client id: f_00004-3-2 loss: 0.963001  [   96/  306]
train() client id: f_00004-3-3 loss: 0.754998  [  128/  306]
train() client id: f_00004-3-4 loss: 0.977519  [  160/  306]
train() client id: f_00004-3-5 loss: 0.939439  [  192/  306]
train() client id: f_00004-3-6 loss: 0.845010  [  224/  306]
train() client id: f_00004-3-7 loss: 0.856766  [  256/  306]
train() client id: f_00004-3-8 loss: 0.997908  [  288/  306]
train() client id: f_00004-4-0 loss: 0.878932  [   32/  306]
train() client id: f_00004-4-1 loss: 0.830136  [   64/  306]
train() client id: f_00004-4-2 loss: 1.067718  [   96/  306]
train() client id: f_00004-4-3 loss: 0.950213  [  128/  306]
train() client id: f_00004-4-4 loss: 0.787571  [  160/  306]
train() client id: f_00004-4-5 loss: 0.834188  [  192/  306]
train() client id: f_00004-4-6 loss: 0.917631  [  224/  306]
train() client id: f_00004-4-7 loss: 0.871101  [  256/  306]
train() client id: f_00004-4-8 loss: 0.986879  [  288/  306]
train() client id: f_00004-5-0 loss: 0.991273  [   32/  306]
train() client id: f_00004-5-1 loss: 0.924538  [   64/  306]
train() client id: f_00004-5-2 loss: 0.848108  [   96/  306]
train() client id: f_00004-5-3 loss: 0.893372  [  128/  306]
train() client id: f_00004-5-4 loss: 0.956849  [  160/  306]
train() client id: f_00004-5-5 loss: 0.782024  [  192/  306]
train() client id: f_00004-5-6 loss: 0.781837  [  224/  306]
train() client id: f_00004-5-7 loss: 0.846688  [  256/  306]
train() client id: f_00004-5-8 loss: 0.960053  [  288/  306]
train() client id: f_00004-6-0 loss: 0.964647  [   32/  306]
train() client id: f_00004-6-1 loss: 1.036815  [   64/  306]
train() client id: f_00004-6-2 loss: 0.863046  [   96/  306]
train() client id: f_00004-6-3 loss: 0.846957  [  128/  306]
train() client id: f_00004-6-4 loss: 1.009266  [  160/  306]
train() client id: f_00004-6-5 loss: 0.914596  [  192/  306]
train() client id: f_00004-6-6 loss: 0.820232  [  224/  306]
train() client id: f_00004-6-7 loss: 0.823501  [  256/  306]
train() client id: f_00004-6-8 loss: 0.735556  [  288/  306]
train() client id: f_00004-7-0 loss: 0.757311  [   32/  306]
train() client id: f_00004-7-1 loss: 0.945536  [   64/  306]
train() client id: f_00004-7-2 loss: 0.872471  [   96/  306]
train() client id: f_00004-7-3 loss: 0.955681  [  128/  306]
train() client id: f_00004-7-4 loss: 0.900687  [  160/  306]
train() client id: f_00004-7-5 loss: 0.919455  [  192/  306]
train() client id: f_00004-7-6 loss: 0.820476  [  224/  306]
train() client id: f_00004-7-7 loss: 0.850899  [  256/  306]
train() client id: f_00004-7-8 loss: 0.967490  [  288/  306]
train() client id: f_00004-8-0 loss: 0.972661  [   32/  306]
train() client id: f_00004-8-1 loss: 0.794896  [   64/  306]
train() client id: f_00004-8-2 loss: 0.912806  [   96/  306]
train() client id: f_00004-8-3 loss: 0.973306  [  128/  306]
train() client id: f_00004-8-4 loss: 0.856735  [  160/  306]
train() client id: f_00004-8-5 loss: 0.823517  [  192/  306]
train() client id: f_00004-8-6 loss: 0.942317  [  224/  306]
train() client id: f_00004-8-7 loss: 0.960104  [  256/  306]
train() client id: f_00004-8-8 loss: 0.856625  [  288/  306]
train() client id: f_00004-9-0 loss: 0.893638  [   32/  306]
train() client id: f_00004-9-1 loss: 0.886890  [   64/  306]
train() client id: f_00004-9-2 loss: 0.984119  [   96/  306]
train() client id: f_00004-9-3 loss: 0.893567  [  128/  306]
train() client id: f_00004-9-4 loss: 0.835557  [  160/  306]
train() client id: f_00004-9-5 loss: 0.748970  [  192/  306]
train() client id: f_00004-9-6 loss: 0.809206  [  224/  306]
train() client id: f_00004-9-7 loss: 0.950126  [  256/  306]
train() client id: f_00004-9-8 loss: 0.956165  [  288/  306]
train() client id: f_00004-10-0 loss: 0.857805  [   32/  306]
train() client id: f_00004-10-1 loss: 0.853465  [   64/  306]
train() client id: f_00004-10-2 loss: 0.943050  [   96/  306]
train() client id: f_00004-10-3 loss: 0.799540  [  128/  306]
train() client id: f_00004-10-4 loss: 0.988034  [  160/  306]
train() client id: f_00004-10-5 loss: 0.866907  [  192/  306]
train() client id: f_00004-10-6 loss: 0.898226  [  224/  306]
train() client id: f_00004-10-7 loss: 0.922856  [  256/  306]
train() client id: f_00004-10-8 loss: 0.852745  [  288/  306]
train() client id: f_00004-11-0 loss: 0.918018  [   32/  306]
train() client id: f_00004-11-1 loss: 0.828494  [   64/  306]
train() client id: f_00004-11-2 loss: 0.849288  [   96/  306]
train() client id: f_00004-11-3 loss: 0.904668  [  128/  306]
train() client id: f_00004-11-4 loss: 0.790242  [  160/  306]
train() client id: f_00004-11-5 loss: 1.005170  [  192/  306]
train() client id: f_00004-11-6 loss: 0.868345  [  224/  306]
train() client id: f_00004-11-7 loss: 0.832546  [  256/  306]
train() client id: f_00004-11-8 loss: 0.963944  [  288/  306]
train() client id: f_00005-0-0 loss: 0.728428  [   32/  146]
train() client id: f_00005-0-1 loss: 0.651353  [   64/  146]
train() client id: f_00005-0-2 loss: 0.804925  [   96/  146]
train() client id: f_00005-0-3 loss: 0.767467  [  128/  146]
train() client id: f_00005-1-0 loss: 0.589744  [   32/  146]
train() client id: f_00005-1-1 loss: 0.754231  [   64/  146]
train() client id: f_00005-1-2 loss: 0.756970  [   96/  146]
train() client id: f_00005-1-3 loss: 0.638692  [  128/  146]
train() client id: f_00005-2-0 loss: 0.683715  [   32/  146]
train() client id: f_00005-2-1 loss: 0.727427  [   64/  146]
train() client id: f_00005-2-2 loss: 0.841241  [   96/  146]
train() client id: f_00005-2-3 loss: 0.613641  [  128/  146]
train() client id: f_00005-3-0 loss: 0.583180  [   32/  146]
train() client id: f_00005-3-1 loss: 0.737932  [   64/  146]
train() client id: f_00005-3-2 loss: 0.822067  [   96/  146]
train() client id: f_00005-3-3 loss: 0.695785  [  128/  146]
train() client id: f_00005-4-0 loss: 0.583922  [   32/  146]
train() client id: f_00005-4-1 loss: 0.612543  [   64/  146]
train() client id: f_00005-4-2 loss: 0.777195  [   96/  146]
train() client id: f_00005-4-3 loss: 0.849132  [  128/  146]
train() client id: f_00005-5-0 loss: 0.743632  [   32/  146]
train() client id: f_00005-5-1 loss: 0.568557  [   64/  146]
train() client id: f_00005-5-2 loss: 0.766931  [   96/  146]
train() client id: f_00005-5-3 loss: 0.796602  [  128/  146]
train() client id: f_00005-6-0 loss: 0.715968  [   32/  146]
train() client id: f_00005-6-1 loss: 0.831756  [   64/  146]
train() client id: f_00005-6-2 loss: 0.659901  [   96/  146]
train() client id: f_00005-6-3 loss: 0.715128  [  128/  146]
train() client id: f_00005-7-0 loss: 0.852439  [   32/  146]
train() client id: f_00005-7-1 loss: 0.755585  [   64/  146]
train() client id: f_00005-7-2 loss: 0.710809  [   96/  146]
train() client id: f_00005-7-3 loss: 0.508758  [  128/  146]
train() client id: f_00005-8-0 loss: 0.795405  [   32/  146]
train() client id: f_00005-8-1 loss: 0.862162  [   64/  146]
train() client id: f_00005-8-2 loss: 0.594007  [   96/  146]
train() client id: f_00005-8-3 loss: 0.689920  [  128/  146]
train() client id: f_00005-9-0 loss: 0.861561  [   32/  146]
train() client id: f_00005-9-1 loss: 0.709524  [   64/  146]
train() client id: f_00005-9-2 loss: 0.791325  [   96/  146]
train() client id: f_00005-9-3 loss: 0.630761  [  128/  146]
train() client id: f_00005-10-0 loss: 0.638266  [   32/  146]
train() client id: f_00005-10-1 loss: 0.867286  [   64/  146]
train() client id: f_00005-10-2 loss: 0.627080  [   96/  146]
train() client id: f_00005-10-3 loss: 0.684023  [  128/  146]
train() client id: f_00005-11-0 loss: 0.654186  [   32/  146]
train() client id: f_00005-11-1 loss: 0.651554  [   64/  146]
train() client id: f_00005-11-2 loss: 0.772715  [   96/  146]
train() client id: f_00005-11-3 loss: 0.715384  [  128/  146]
train() client id: f_00006-0-0 loss: 0.576394  [   32/   54]
train() client id: f_00006-1-0 loss: 0.600586  [   32/   54]
train() client id: f_00006-2-0 loss: 0.609171  [   32/   54]
train() client id: f_00006-3-0 loss: 0.553063  [   32/   54]
train() client id: f_00006-4-0 loss: 0.564133  [   32/   54]
train() client id: f_00006-5-0 loss: 0.559238  [   32/   54]
train() client id: f_00006-6-0 loss: 0.622567  [   32/   54]
train() client id: f_00006-7-0 loss: 0.583197  [   32/   54]
train() client id: f_00006-8-0 loss: 0.631934  [   32/   54]
train() client id: f_00006-9-0 loss: 0.586501  [   32/   54]
train() client id: f_00006-10-0 loss: 0.577571  [   32/   54]
train() client id: f_00006-11-0 loss: 0.616288  [   32/   54]
train() client id: f_00007-0-0 loss: 0.596058  [   32/  179]
train() client id: f_00007-0-1 loss: 0.577876  [   64/  179]
train() client id: f_00007-0-2 loss: 0.499398  [   96/  179]
train() client id: f_00007-0-3 loss: 0.572135  [  128/  179]
train() client id: f_00007-0-4 loss: 0.774901  [  160/  179]
train() client id: f_00007-1-0 loss: 0.497803  [   32/  179]
train() client id: f_00007-1-1 loss: 0.775302  [   64/  179]
train() client id: f_00007-1-2 loss: 0.532099  [   96/  179]
train() client id: f_00007-1-3 loss: 0.590968  [  128/  179]
train() client id: f_00007-1-4 loss: 0.578263  [  160/  179]
train() client id: f_00007-2-0 loss: 0.494449  [   32/  179]
train() client id: f_00007-2-1 loss: 0.668533  [   64/  179]
train() client id: f_00007-2-2 loss: 0.547036  [   96/  179]
train() client id: f_00007-2-3 loss: 0.538346  [  128/  179]
train() client id: f_00007-2-4 loss: 0.532436  [  160/  179]
train() client id: f_00007-3-0 loss: 0.705933  [   32/  179]
train() client id: f_00007-3-1 loss: 0.533182  [   64/  179]
train() client id: f_00007-3-2 loss: 0.725044  [   96/  179]
train() client id: f_00007-3-3 loss: 0.471908  [  128/  179]
train() client id: f_00007-3-4 loss: 0.432441  [  160/  179]
train() client id: f_00007-4-0 loss: 0.529940  [   32/  179]
train() client id: f_00007-4-1 loss: 0.756339  [   64/  179]
train() client id: f_00007-4-2 loss: 0.551533  [   96/  179]
train() client id: f_00007-4-3 loss: 0.477351  [  128/  179]
train() client id: f_00007-4-4 loss: 0.529251  [  160/  179]
train() client id: f_00007-5-0 loss: 0.455521  [   32/  179]
train() client id: f_00007-5-1 loss: 0.516341  [   64/  179]
train() client id: f_00007-5-2 loss: 0.620076  [   96/  179]
train() client id: f_00007-5-3 loss: 0.675807  [  128/  179]
train() client id: f_00007-5-4 loss: 0.565963  [  160/  179]
train() client id: f_00007-6-0 loss: 0.570352  [   32/  179]
train() client id: f_00007-6-1 loss: 0.659083  [   64/  179]
train() client id: f_00007-6-2 loss: 0.521922  [   96/  179]
train() client id: f_00007-6-3 loss: 0.494460  [  128/  179]
train() client id: f_00007-6-4 loss: 0.642131  [  160/  179]
train() client id: f_00007-7-0 loss: 0.525407  [   32/  179]
train() client id: f_00007-7-1 loss: 0.526874  [   64/  179]
train() client id: f_00007-7-2 loss: 0.512616  [   96/  179]
train() client id: f_00007-7-3 loss: 0.775932  [  128/  179]
train() client id: f_00007-7-4 loss: 0.433061  [  160/  179]
train() client id: f_00007-8-0 loss: 0.608610  [   32/  179]
train() client id: f_00007-8-1 loss: 0.723688  [   64/  179]
train() client id: f_00007-8-2 loss: 0.604990  [   96/  179]
train() client id: f_00007-8-3 loss: 0.446089  [  128/  179]
train() client id: f_00007-8-4 loss: 0.461250  [  160/  179]
train() client id: f_00007-9-0 loss: 0.498007  [   32/  179]
train() client id: f_00007-9-1 loss: 0.518232  [   64/  179]
train() client id: f_00007-9-2 loss: 0.436203  [   96/  179]
train() client id: f_00007-9-3 loss: 0.731169  [  128/  179]
train() client id: f_00007-9-4 loss: 0.559706  [  160/  179]
train() client id: f_00007-10-0 loss: 0.399654  [   32/  179]
train() client id: f_00007-10-1 loss: 0.584868  [   64/  179]
train() client id: f_00007-10-2 loss: 0.631497  [   96/  179]
train() client id: f_00007-10-3 loss: 0.537983  [  128/  179]
train() client id: f_00007-10-4 loss: 0.706313  [  160/  179]
train() client id: f_00007-11-0 loss: 0.524437  [   32/  179]
train() client id: f_00007-11-1 loss: 0.487071  [   64/  179]
train() client id: f_00007-11-2 loss: 0.544612  [   96/  179]
train() client id: f_00007-11-3 loss: 0.522161  [  128/  179]
train() client id: f_00007-11-4 loss: 0.691268  [  160/  179]
train() client id: f_00008-0-0 loss: 0.809619  [   32/  130]
train() client id: f_00008-0-1 loss: 0.821651  [   64/  130]
train() client id: f_00008-0-2 loss: 0.811387  [   96/  130]
train() client id: f_00008-0-3 loss: 0.786211  [  128/  130]
train() client id: f_00008-1-0 loss: 0.894329  [   32/  130]
train() client id: f_00008-1-1 loss: 0.737253  [   64/  130]
train() client id: f_00008-1-2 loss: 0.842237  [   96/  130]
train() client id: f_00008-1-3 loss: 0.748770  [  128/  130]
train() client id: f_00008-2-0 loss: 0.984419  [   32/  130]
train() client id: f_00008-2-1 loss: 0.714809  [   64/  130]
train() client id: f_00008-2-2 loss: 0.763003  [   96/  130]
train() client id: f_00008-2-3 loss: 0.765498  [  128/  130]
train() client id: f_00008-3-0 loss: 0.663822  [   32/  130]
train() client id: f_00008-3-1 loss: 0.916115  [   64/  130]
train() client id: f_00008-3-2 loss: 0.748881  [   96/  130]
train() client id: f_00008-3-3 loss: 0.891043  [  128/  130]
train() client id: f_00008-4-0 loss: 0.883502  [   32/  130]
train() client id: f_00008-4-1 loss: 0.876850  [   64/  130]
train() client id: f_00008-4-2 loss: 0.709361  [   96/  130]
train() client id: f_00008-4-3 loss: 0.759201  [  128/  130]
train() client id: f_00008-5-0 loss: 0.758099  [   32/  130]
train() client id: f_00008-5-1 loss: 0.833898  [   64/  130]
train() client id: f_00008-5-2 loss: 0.864958  [   96/  130]
train() client id: f_00008-5-3 loss: 0.732570  [  128/  130]
train() client id: f_00008-6-0 loss: 0.752756  [   32/  130]
train() client id: f_00008-6-1 loss: 0.686800  [   64/  130]
train() client id: f_00008-6-2 loss: 1.020978  [   96/  130]
train() client id: f_00008-6-3 loss: 0.740303  [  128/  130]
train() client id: f_00008-7-0 loss: 0.868947  [   32/  130]
train() client id: f_00008-7-1 loss: 0.786462  [   64/  130]
train() client id: f_00008-7-2 loss: 0.770346  [   96/  130]
train() client id: f_00008-7-3 loss: 0.765893  [  128/  130]
train() client id: f_00008-8-0 loss: 0.706686  [   32/  130]
train() client id: f_00008-8-1 loss: 0.829504  [   64/  130]
train() client id: f_00008-8-2 loss: 0.867672  [   96/  130]
train() client id: f_00008-8-3 loss: 0.793618  [  128/  130]
train() client id: f_00008-9-0 loss: 0.718049  [   32/  130]
train() client id: f_00008-9-1 loss: 0.831944  [   64/  130]
train() client id: f_00008-9-2 loss: 0.807583  [   96/  130]
train() client id: f_00008-9-3 loss: 0.840356  [  128/  130]
train() client id: f_00008-10-0 loss: 0.798414  [   32/  130]
train() client id: f_00008-10-1 loss: 0.844141  [   64/  130]
train() client id: f_00008-10-2 loss: 0.747707  [   96/  130]
train() client id: f_00008-10-3 loss: 0.790738  [  128/  130]
train() client id: f_00008-11-0 loss: 0.750524  [   32/  130]
train() client id: f_00008-11-1 loss: 0.911041  [   64/  130]
train() client id: f_00008-11-2 loss: 0.828531  [   96/  130]
train() client id: f_00008-11-3 loss: 0.714961  [  128/  130]
train() client id: f_00009-0-0 loss: 0.927260  [   32/  118]
train() client id: f_00009-0-1 loss: 0.932499  [   64/  118]
train() client id: f_00009-0-2 loss: 1.056341  [   96/  118]
train() client id: f_00009-1-0 loss: 1.074059  [   32/  118]
train() client id: f_00009-1-1 loss: 0.806514  [   64/  118]
train() client id: f_00009-1-2 loss: 0.874312  [   96/  118]
train() client id: f_00009-2-0 loss: 0.921883  [   32/  118]
train() client id: f_00009-2-1 loss: 0.831501  [   64/  118]
train() client id: f_00009-2-2 loss: 0.958751  [   96/  118]
train() client id: f_00009-3-0 loss: 0.927925  [   32/  118]
train() client id: f_00009-3-1 loss: 0.906035  [   64/  118]
train() client id: f_00009-3-2 loss: 0.778001  [   96/  118]
train() client id: f_00009-4-0 loss: 0.960696  [   32/  118]
train() client id: f_00009-4-1 loss: 0.878472  [   64/  118]
train() client id: f_00009-4-2 loss: 0.750935  [   96/  118]
train() client id: f_00009-5-0 loss: 0.917764  [   32/  118]
train() client id: f_00009-5-1 loss: 0.754141  [   64/  118]
train() client id: f_00009-5-2 loss: 0.721560  [   96/  118]
train() client id: f_00009-6-0 loss: 0.901070  [   32/  118]
train() client id: f_00009-6-1 loss: 0.746837  [   64/  118]
train() client id: f_00009-6-2 loss: 0.769296  [   96/  118]
train() client id: f_00009-7-0 loss: 0.766604  [   32/  118]
train() client id: f_00009-7-1 loss: 0.784547  [   64/  118]
train() client id: f_00009-7-2 loss: 0.918353  [   96/  118]
train() client id: f_00009-8-0 loss: 0.973850  [   32/  118]
train() client id: f_00009-8-1 loss: 0.680193  [   64/  118]
train() client id: f_00009-8-2 loss: 0.837983  [   96/  118]
train() client id: f_00009-9-0 loss: 0.814725  [   32/  118]
train() client id: f_00009-9-1 loss: 0.703065  [   64/  118]
train() client id: f_00009-9-2 loss: 0.875503  [   96/  118]
train() client id: f_00009-10-0 loss: 0.682242  [   32/  118]
train() client id: f_00009-10-1 loss: 0.873060  [   64/  118]
train() client id: f_00009-10-2 loss: 0.751307  [   96/  118]
train() client id: f_00009-11-0 loss: 0.764710  [   32/  118]
train() client id: f_00009-11-1 loss: 0.647527  [   64/  118]
train() client id: f_00009-11-2 loss: 0.924756  [   96/  118]
At round 18 accuracy: 0.6339522546419099
At round 18 training accuracy: 0.5774647887323944
At round 18 training loss: 0.8346664453874346
update_location
xs = [ -3.9056584    4.20031788 110.00902392  18.81129433   0.97929623
   3.95640986 -72.44319194 -51.32485185  94.66397685 -37.06087855]
ys = [102.5879595   85.55583871   1.32061395 -72.45517586  64.35018685
  47.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [143.31623635 131.67172896 148.67323015 124.91443993 118.91974423
 110.91368802 123.510755   112.40514523 138.81620397 106.72169684]
dists_bs = [185.78691379 200.28233051 333.75840492 314.31971282 207.87518816
 219.54871187 205.10615284 213.61974799 312.25498919 219.69909981]
uav_gains = [4.06456051e-11 5.02562998e-11 3.70692179e-11 5.73364491e-11
 6.48404832e-11 7.71845050e-11 5.89801623e-11 7.46493522e-11
 4.40289659e-11 8.49892801e-11]
bs_gains = [4.89837182e-11 3.96912434e-11 9.49913137e-12 1.12370821e-11
 3.57640465e-11 3.06907558e-11 3.71324630e-11 3.31358541e-11
 1.14463706e-11 3.06319686e-11]
Round 19
-------------------------------
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.48419946 17.66297686  8.35544877  2.99255128 20.37392527  9.81527082
  3.718002   11.96472075  8.80869105  7.96655398]
obj_prev = 100.14234023128081
eta_min = 1.126497835179761e-11	eta_max = 0.9220293976525566
af = 21.163983583608065	bf = 1.67717061299475	zeta = 23.280381941968873	eta = 0.9090909090909091
af = 21.163983583608065	bf = 1.67717061299475	zeta = 40.637450630966356	eta = 0.5207999826514902
af = 21.163983583608065	bf = 1.67717061299475	zeta = 32.312864825181585	eta = 0.6549708203871438
af = 21.163983583608065	bf = 1.67717061299475	zeta = 30.81773875501804	eta = 0.6867468035811661
af = 21.163983583608065	bf = 1.67717061299475	zeta = 30.743104385934746	eta = 0.6884140039315868
af = 21.163983583608065	bf = 1.67717061299475	zeta = 30.74290487459993	eta = 0.6884184715119078
eta = 0.6884184715119078
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [0.03080864 0.06479595 0.03031961 0.01051406 0.07482098 0.0356989
 0.0132037  0.04376783 0.03178669 0.02885253]
ene_total = [2.66619823 5.03309398 2.64268475 1.21857109 5.74248846 3.03975216
 1.40293291 3.50736005 2.92607578 2.56374747]
ti_comp = [0.34241459 0.34203063 0.34090455 0.34757484 0.34030078 0.33761632
 0.34796803 0.35108386 0.31483598 0.33758153]
ti_coms = [0.07519738 0.07558134 0.07670742 0.07003712 0.07731119 0.07999565
 0.06964393 0.0665281  0.10277599 0.08003044]
t_total = [29.04992027 29.04992027 29.04992027 29.04992027 29.04992027 29.04992027
 29.04992027 29.04992027 29.04992027 29.04992027]
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [1.55880946e-05 1.45342807e-04 1.49894390e-05 6.01304716e-07
 2.26060309e-04 2.49458102e-05 1.18819712e-06 4.25131485e-05
 2.02510366e-05 1.31726936e-05]
ene_total = [0.52417228 0.53586919 0.53463478 0.48723401 0.55351726 0.55820092
 0.48453973 0.46574004 0.71633895 0.557624  ]
optimize_network iter = 0 obj = 5.417871154985847
eta = 0.6884184715119078
freqs = [4.49873339e+07 9.47224336e+07 4.44693580e+07 1.51248853e+07
 1.09933602e+08 5.28690360e+07 1.89725795e+07 6.23324407e+07
 5.04813506e+07 4.27341730e+07]
eta_min = 0.6884184715119183	eta_max = 0.6884184715118975
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 0.03222576338096967	eta = 0.909090909090909
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 18.47959117596394	eta = 0.0015853244938805354
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 1.8690048705094464	eta = 0.015674730970695012
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 1.8275633379854408	eta = 0.016030168650925096
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 1.827555909002531	eta = 0.016030233813281204
eta = 0.016030233813281204
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [1.72568617e-04 1.60902329e-03 1.65941177e-04 6.65676762e-06
 2.50260959e-03 2.76163578e-04 1.31539832e-05 4.70643491e-04
 2.24189901e-04 1.45828825e-04]
ene_total = [0.16944101 0.20192804 0.17262124 0.15441975 0.22542177 0.18229224
 0.15369677 0.1569107  0.23132642 0.17949795]
ti_comp = [0.34241459 0.34203063 0.34090455 0.34757484 0.34030078 0.33761632
 0.34796803 0.35108386 0.31483598 0.33758153]
ti_coms = [0.07519738 0.07558134 0.07670742 0.07003712 0.07731119 0.07999565
 0.06964393 0.0665281  0.10277599 0.08003044]
t_total = [29.04992027 29.04992027 29.04992027 29.04992027 29.04992027 29.04992027
 29.04992027 29.04992027 29.04992027 29.04992027]
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [1.55880946e-05 1.45342807e-04 1.49894390e-05 6.01304716e-07
 2.26060309e-04 2.49458102e-05 1.18819712e-06 4.25131485e-05
 2.02510366e-05 1.31726936e-05]
ene_total = [0.52417228 0.53586919 0.53463478 0.48723401 0.55351726 0.55820092
 0.48453973 0.46574004 0.71633895 0.557624  ]
optimize_network iter = 1 obj = 5.4178711549860274
eta = 0.6884184715119183
freqs = [4.49873339e+07 9.47224336e+07 4.44693580e+07 1.51248853e+07
 1.09933602e+08 5.28690360e+07 1.89725795e+07 6.23324407e+07
 5.04813506e+07 4.27341730e+07]
Done!
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [1.53003832e-05 1.42660197e-04 1.47127771e-05 5.90206361e-07
 2.21887886e-04 2.44853823e-05 1.16626642e-06 4.17284781e-05
 1.98772608e-05 1.29295635e-05]
ene_total = [0.00753504 0.00770079 0.00768545 0.0070043  0.00795301 0.00802405
 0.00696556 0.00669454 0.01029748 0.00801597]
At round 19 energy consumption: 0.07787619467639152
At round 19 eta: 0.6884184715119183
At round 19 a_n: 21.33168591686803
At round 19 local rounds: 12.225650330368824
At round 19 global rounds: 69.56199190949924
gradient difference: 0.41069287061691284
train() client id: f_00000-0-0 loss: 1.565428  [   32/  126]
train() client id: f_00000-0-1 loss: 1.484560  [   64/  126]
train() client id: f_00000-0-2 loss: 1.150960  [   96/  126]
train() client id: f_00000-1-0 loss: 1.470743  [   32/  126]
train() client id: f_00000-1-1 loss: 1.260121  [   64/  126]
train() client id: f_00000-1-2 loss: 1.195260  [   96/  126]
train() client id: f_00000-2-0 loss: 1.263455  [   32/  126]
train() client id: f_00000-2-1 loss: 1.162769  [   64/  126]
train() client id: f_00000-2-2 loss: 1.046710  [   96/  126]
train() client id: f_00000-3-0 loss: 1.153957  [   32/  126]
train() client id: f_00000-3-1 loss: 1.000489  [   64/  126]
train() client id: f_00000-3-2 loss: 1.003877  [   96/  126]
train() client id: f_00000-4-0 loss: 1.006396  [   32/  126]
train() client id: f_00000-4-1 loss: 1.023599  [   64/  126]
train() client id: f_00000-4-2 loss: 1.055201  [   96/  126]
train() client id: f_00000-5-0 loss: 1.000560  [   32/  126]
train() client id: f_00000-5-1 loss: 0.938241  [   64/  126]
train() client id: f_00000-5-2 loss: 0.995741  [   96/  126]
train() client id: f_00000-6-0 loss: 0.946134  [   32/  126]
train() client id: f_00000-6-1 loss: 0.937227  [   64/  126]
train() client id: f_00000-6-2 loss: 0.960225  [   96/  126]
train() client id: f_00000-7-0 loss: 0.916632  [   32/  126]
train() client id: f_00000-7-1 loss: 0.890761  [   64/  126]
train() client id: f_00000-7-2 loss: 0.965421  [   96/  126]
train() client id: f_00000-8-0 loss: 0.928084  [   32/  126]
train() client id: f_00000-8-1 loss: 0.878266  [   64/  126]
train() client id: f_00000-8-2 loss: 0.974819  [   96/  126]
train() client id: f_00000-9-0 loss: 0.853228  [   32/  126]
train() client id: f_00000-9-1 loss: 0.920553  [   64/  126]
train() client id: f_00000-9-2 loss: 0.874933  [   96/  126]
train() client id: f_00000-10-0 loss: 0.785887  [   32/  126]
train() client id: f_00000-10-1 loss: 0.957808  [   64/  126]
train() client id: f_00000-10-2 loss: 0.848192  [   96/  126]
train() client id: f_00000-11-0 loss: 0.833936  [   32/  126]
train() client id: f_00000-11-1 loss: 0.846772  [   64/  126]
train() client id: f_00000-11-2 loss: 1.009500  [   96/  126]
train() client id: f_00001-0-0 loss: 0.409595  [   32/  265]
train() client id: f_00001-0-1 loss: 0.518345  [   64/  265]
train() client id: f_00001-0-2 loss: 0.572325  [   96/  265]
train() client id: f_00001-0-3 loss: 0.515693  [  128/  265]
train() client id: f_00001-0-4 loss: 0.408518  [  160/  265]
train() client id: f_00001-0-5 loss: 0.448291  [  192/  265]
train() client id: f_00001-0-6 loss: 0.599987  [  224/  265]
train() client id: f_00001-0-7 loss: 0.454282  [  256/  265]
train() client id: f_00001-1-0 loss: 0.556098  [   32/  265]
train() client id: f_00001-1-1 loss: 0.518753  [   64/  265]
train() client id: f_00001-1-2 loss: 0.463437  [   96/  265]
train() client id: f_00001-1-3 loss: 0.451707  [  128/  265]
train() client id: f_00001-1-4 loss: 0.382964  [  160/  265]
train() client id: f_00001-1-5 loss: 0.455638  [  192/  265]
train() client id: f_00001-1-6 loss: 0.588406  [  224/  265]
train() client id: f_00001-1-7 loss: 0.411374  [  256/  265]
train() client id: f_00001-2-0 loss: 0.449875  [   32/  265]
train() client id: f_00001-2-1 loss: 0.418913  [   64/  265]
train() client id: f_00001-2-2 loss: 0.614771  [   96/  265]
train() client id: f_00001-2-3 loss: 0.448779  [  128/  265]
train() client id: f_00001-2-4 loss: 0.487698  [  160/  265]
train() client id: f_00001-2-5 loss: 0.486602  [  192/  265]
train() client id: f_00001-2-6 loss: 0.492739  [  224/  265]
train() client id: f_00001-2-7 loss: 0.374884  [  256/  265]
train() client id: f_00001-3-0 loss: 0.554044  [   32/  265]
train() client id: f_00001-3-1 loss: 0.459725  [   64/  265]
train() client id: f_00001-3-2 loss: 0.387850  [   96/  265]
train() client id: f_00001-3-3 loss: 0.510976  [  128/  265]
train() client id: f_00001-3-4 loss: 0.473312  [  160/  265]
train() client id: f_00001-3-5 loss: 0.432916  [  192/  265]
train() client id: f_00001-3-6 loss: 0.443852  [  224/  265]
train() client id: f_00001-3-7 loss: 0.457802  [  256/  265]
train() client id: f_00001-4-0 loss: 0.432913  [   32/  265]
train() client id: f_00001-4-1 loss: 0.540322  [   64/  265]
train() client id: f_00001-4-2 loss: 0.436655  [   96/  265]
train() client id: f_00001-4-3 loss: 0.419255  [  128/  265]
train() client id: f_00001-4-4 loss: 0.396809  [  160/  265]
train() client id: f_00001-4-5 loss: 0.466752  [  192/  265]
train() client id: f_00001-4-6 loss: 0.520853  [  224/  265]
train() client id: f_00001-4-7 loss: 0.478490  [  256/  265]
train() client id: f_00001-5-0 loss: 0.463833  [   32/  265]
train() client id: f_00001-5-1 loss: 0.454475  [   64/  265]
train() client id: f_00001-5-2 loss: 0.533101  [   96/  265]
train() client id: f_00001-5-3 loss: 0.541004  [  128/  265]
train() client id: f_00001-5-4 loss: 0.506606  [  160/  265]
train() client id: f_00001-5-5 loss: 0.353003  [  192/  265]
train() client id: f_00001-5-6 loss: 0.435358  [  224/  265]
train() client id: f_00001-5-7 loss: 0.376627  [  256/  265]
train() client id: f_00001-6-0 loss: 0.363355  [   32/  265]
train() client id: f_00001-6-1 loss: 0.425824  [   64/  265]
train() client id: f_00001-6-2 loss: 0.357327  [   96/  265]
train() client id: f_00001-6-3 loss: 0.524412  [  128/  265]
train() client id: f_00001-6-4 loss: 0.500612  [  160/  265]
train() client id: f_00001-6-5 loss: 0.446450  [  192/  265]
train() client id: f_00001-6-6 loss: 0.488288  [  224/  265]
train() client id: f_00001-6-7 loss: 0.491916  [  256/  265]
train() client id: f_00001-7-0 loss: 0.368383  [   32/  265]
train() client id: f_00001-7-1 loss: 0.564456  [   64/  265]
train() client id: f_00001-7-2 loss: 0.404788  [   96/  265]
train() client id: f_00001-7-3 loss: 0.444068  [  128/  265]
train() client id: f_00001-7-4 loss: 0.343534  [  160/  265]
train() client id: f_00001-7-5 loss: 0.610279  [  192/  265]
train() client id: f_00001-7-6 loss: 0.354921  [  224/  265]
train() client id: f_00001-7-7 loss: 0.494754  [  256/  265]
train() client id: f_00001-8-0 loss: 0.436263  [   32/  265]
train() client id: f_00001-8-1 loss: 0.395027  [   64/  265]
train() client id: f_00001-8-2 loss: 0.510811  [   96/  265]
train() client id: f_00001-8-3 loss: 0.402793  [  128/  265]
train() client id: f_00001-8-4 loss: 0.427290  [  160/  265]
train() client id: f_00001-8-5 loss: 0.373359  [  192/  265]
train() client id: f_00001-8-6 loss: 0.514906  [  224/  265]
train() client id: f_00001-8-7 loss: 0.547610  [  256/  265]
train() client id: f_00001-9-0 loss: 0.415009  [   32/  265]
train() client id: f_00001-9-1 loss: 0.408836  [   64/  265]
train() client id: f_00001-9-2 loss: 0.349639  [   96/  265]
train() client id: f_00001-9-3 loss: 0.398942  [  128/  265]
train() client id: f_00001-9-4 loss: 0.443835  [  160/  265]
train() client id: f_00001-9-5 loss: 0.650938  [  192/  265]
train() client id: f_00001-9-6 loss: 0.461885  [  224/  265]
train() client id: f_00001-9-7 loss: 0.468666  [  256/  265]
train() client id: f_00001-10-0 loss: 0.339227  [   32/  265]
train() client id: f_00001-10-1 loss: 0.432438  [   64/  265]
train() client id: f_00001-10-2 loss: 0.377542  [   96/  265]
train() client id: f_00001-10-3 loss: 0.537254  [  128/  265]
train() client id: f_00001-10-4 loss: 0.484944  [  160/  265]
train() client id: f_00001-10-5 loss: 0.478149  [  192/  265]
train() client id: f_00001-10-6 loss: 0.391842  [  224/  265]
train() client id: f_00001-10-7 loss: 0.444721  [  256/  265]
train() client id: f_00001-11-0 loss: 0.417570  [   32/  265]
train() client id: f_00001-11-1 loss: 0.543999  [   64/  265]
train() client id: f_00001-11-2 loss: 0.420271  [   96/  265]
train() client id: f_00001-11-3 loss: 0.516391  [  128/  265]
train() client id: f_00001-11-4 loss: 0.415955  [  160/  265]
train() client id: f_00001-11-5 loss: 0.516217  [  192/  265]
train() client id: f_00001-11-6 loss: 0.346849  [  224/  265]
train() client id: f_00001-11-7 loss: 0.419936  [  256/  265]
train() client id: f_00002-0-0 loss: 1.167285  [   32/  124]
train() client id: f_00002-0-1 loss: 0.985145  [   64/  124]
train() client id: f_00002-0-2 loss: 1.018376  [   96/  124]
train() client id: f_00002-1-0 loss: 1.104152  [   32/  124]
train() client id: f_00002-1-1 loss: 0.983407  [   64/  124]
train() client id: f_00002-1-2 loss: 0.944053  [   96/  124]
train() client id: f_00002-2-0 loss: 0.962618  [   32/  124]
train() client id: f_00002-2-1 loss: 1.117033  [   64/  124]
train() client id: f_00002-2-2 loss: 1.026563  [   96/  124]
train() client id: f_00002-3-0 loss: 1.038605  [   32/  124]
train() client id: f_00002-3-1 loss: 0.958966  [   64/  124]
train() client id: f_00002-3-2 loss: 0.992140  [   96/  124]
train() client id: f_00002-4-0 loss: 0.899341  [   32/  124]
train() client id: f_00002-4-1 loss: 1.002007  [   64/  124]
train() client id: f_00002-4-2 loss: 0.878392  [   96/  124]
train() client id: f_00002-5-0 loss: 0.900827  [   32/  124]
train() client id: f_00002-5-1 loss: 0.924259  [   64/  124]
train() client id: f_00002-5-2 loss: 1.042292  [   96/  124]
train() client id: f_00002-6-0 loss: 0.934775  [   32/  124]
train() client id: f_00002-6-1 loss: 0.981513  [   64/  124]
train() client id: f_00002-6-2 loss: 0.820133  [   96/  124]
train() client id: f_00002-7-0 loss: 0.867924  [   32/  124]
train() client id: f_00002-7-1 loss: 0.754710  [   64/  124]
train() client id: f_00002-7-2 loss: 1.206318  [   96/  124]
train() client id: f_00002-8-0 loss: 0.908191  [   32/  124]
train() client id: f_00002-8-1 loss: 0.953280  [   64/  124]
train() client id: f_00002-8-2 loss: 0.935544  [   96/  124]
train() client id: f_00002-9-0 loss: 0.935135  [   32/  124]
train() client id: f_00002-9-1 loss: 0.980881  [   64/  124]
train() client id: f_00002-9-2 loss: 0.842564  [   96/  124]
train() client id: f_00002-10-0 loss: 0.879879  [   32/  124]
train() client id: f_00002-10-1 loss: 0.933683  [   64/  124]
train() client id: f_00002-10-2 loss: 0.898113  [   96/  124]
train() client id: f_00002-11-0 loss: 1.036371  [   32/  124]
train() client id: f_00002-11-1 loss: 0.931610  [   64/  124]
train() client id: f_00002-11-2 loss: 0.922383  [   96/  124]
train() client id: f_00003-0-0 loss: 0.827083  [   32/   43]
train() client id: f_00003-1-0 loss: 0.900482  [   32/   43]
train() client id: f_00003-2-0 loss: 0.870071  [   32/   43]
train() client id: f_00003-3-0 loss: 0.764893  [   32/   43]
train() client id: f_00003-4-0 loss: 0.806807  [   32/   43]
train() client id: f_00003-5-0 loss: 0.810541  [   32/   43]
train() client id: f_00003-6-0 loss: 0.720544  [   32/   43]
train() client id: f_00003-7-0 loss: 0.678435  [   32/   43]
train() client id: f_00003-8-0 loss: 0.831232  [   32/   43]
train() client id: f_00003-9-0 loss: 0.717274  [   32/   43]
train() client id: f_00003-10-0 loss: 0.873599  [   32/   43]
train() client id: f_00003-11-0 loss: 0.712545  [   32/   43]
train() client id: f_00004-0-0 loss: 0.881094  [   32/  306]
train() client id: f_00004-0-1 loss: 0.741435  [   64/  306]
train() client id: f_00004-0-2 loss: 0.840391  [   96/  306]
train() client id: f_00004-0-3 loss: 0.688353  [  128/  306]
train() client id: f_00004-0-4 loss: 0.753438  [  160/  306]
train() client id: f_00004-0-5 loss: 0.820082  [  192/  306]
train() client id: f_00004-0-6 loss: 0.841783  [  224/  306]
train() client id: f_00004-0-7 loss: 0.790584  [  256/  306]
train() client id: f_00004-0-8 loss: 0.851069  [  288/  306]
train() client id: f_00004-1-0 loss: 0.862431  [   32/  306]
train() client id: f_00004-1-1 loss: 0.802631  [   64/  306]
train() client id: f_00004-1-2 loss: 0.896829  [   96/  306]
train() client id: f_00004-1-3 loss: 0.715082  [  128/  306]
train() client id: f_00004-1-4 loss: 0.664258  [  160/  306]
train() client id: f_00004-1-5 loss: 0.852351  [  192/  306]
train() client id: f_00004-1-6 loss: 0.817128  [  224/  306]
train() client id: f_00004-1-7 loss: 0.787708  [  256/  306]
train() client id: f_00004-1-8 loss: 0.819222  [  288/  306]
train() client id: f_00004-2-0 loss: 0.706815  [   32/  306]
train() client id: f_00004-2-1 loss: 0.852232  [   64/  306]
train() client id: f_00004-2-2 loss: 0.765451  [   96/  306]
train() client id: f_00004-2-3 loss: 0.812937  [  128/  306]
train() client id: f_00004-2-4 loss: 0.832140  [  160/  306]
train() client id: f_00004-2-5 loss: 0.846793  [  192/  306]
train() client id: f_00004-2-6 loss: 0.816432  [  224/  306]
train() client id: f_00004-2-7 loss: 0.873082  [  256/  306]
train() client id: f_00004-2-8 loss: 0.780741  [  288/  306]
train() client id: f_00004-3-0 loss: 0.736093  [   32/  306]
train() client id: f_00004-3-1 loss: 0.819830  [   64/  306]
train() client id: f_00004-3-2 loss: 0.657265  [   96/  306]
train() client id: f_00004-3-3 loss: 0.851148  [  128/  306]
train() client id: f_00004-3-4 loss: 0.782021  [  160/  306]
train() client id: f_00004-3-5 loss: 0.909463  [  192/  306]
train() client id: f_00004-3-6 loss: 0.793480  [  224/  306]
train() client id: f_00004-3-7 loss: 0.738221  [  256/  306]
train() client id: f_00004-3-8 loss: 0.806918  [  288/  306]
train() client id: f_00004-4-0 loss: 0.767233  [   32/  306]
train() client id: f_00004-4-1 loss: 0.804924  [   64/  306]
train() client id: f_00004-4-2 loss: 0.771896  [   96/  306]
train() client id: f_00004-4-3 loss: 0.901907  [  128/  306]
train() client id: f_00004-4-4 loss: 0.782890  [  160/  306]
train() client id: f_00004-4-5 loss: 0.782843  [  192/  306]
train() client id: f_00004-4-6 loss: 0.793322  [  224/  306]
train() client id: f_00004-4-7 loss: 0.784093  [  256/  306]
train() client id: f_00004-4-8 loss: 0.792063  [  288/  306]
train() client id: f_00004-5-0 loss: 0.729257  [   32/  306]
train() client id: f_00004-5-1 loss: 0.811584  [   64/  306]
train() client id: f_00004-5-2 loss: 0.878327  [   96/  306]
train() client id: f_00004-5-3 loss: 0.823469  [  128/  306]
train() client id: f_00004-5-4 loss: 0.856801  [  160/  306]
train() client id: f_00004-5-5 loss: 0.843435  [  192/  306]
train() client id: f_00004-5-6 loss: 0.680191  [  224/  306]
train() client id: f_00004-5-7 loss: 0.844258  [  256/  306]
train() client id: f_00004-5-8 loss: 0.869771  [  288/  306]
train() client id: f_00004-6-0 loss: 0.760043  [   32/  306]
train() client id: f_00004-6-1 loss: 0.739101  [   64/  306]
train() client id: f_00004-6-2 loss: 0.813448  [   96/  306]
train() client id: f_00004-6-3 loss: 0.716324  [  128/  306]
train() client id: f_00004-6-4 loss: 0.805358  [  160/  306]
train() client id: f_00004-6-5 loss: 0.797428  [  192/  306]
train() client id: f_00004-6-6 loss: 0.877870  [  224/  306]
train() client id: f_00004-6-7 loss: 0.853893  [  256/  306]
train() client id: f_00004-6-8 loss: 0.906002  [  288/  306]
train() client id: f_00004-7-0 loss: 0.851118  [   32/  306]
train() client id: f_00004-7-1 loss: 0.830574  [   64/  306]
train() client id: f_00004-7-2 loss: 0.767394  [   96/  306]
train() client id: f_00004-7-3 loss: 0.800048  [  128/  306]
train() client id: f_00004-7-4 loss: 0.681104  [  160/  306]
train() client id: f_00004-7-5 loss: 0.799989  [  192/  306]
train() client id: f_00004-7-6 loss: 0.817249  [  224/  306]
train() client id: f_00004-7-7 loss: 0.803422  [  256/  306]
train() client id: f_00004-7-8 loss: 0.831317  [  288/  306]
train() client id: f_00004-8-0 loss: 0.835513  [   32/  306]
train() client id: f_00004-8-1 loss: 0.751074  [   64/  306]
train() client id: f_00004-8-2 loss: 0.776517  [   96/  306]
train() client id: f_00004-8-3 loss: 0.860346  [  128/  306]
train() client id: f_00004-8-4 loss: 0.738546  [  160/  306]
train() client id: f_00004-8-5 loss: 0.840577  [  192/  306]
train() client id: f_00004-8-6 loss: 0.740790  [  224/  306]
train() client id: f_00004-8-7 loss: 0.959156  [  256/  306]
train() client id: f_00004-8-8 loss: 0.770984  [  288/  306]
train() client id: f_00004-9-0 loss: 0.905118  [   32/  306]
train() client id: f_00004-9-1 loss: 0.837148  [   64/  306]
train() client id: f_00004-9-2 loss: 0.762080  [   96/  306]
train() client id: f_00004-9-3 loss: 0.749110  [  128/  306]
train() client id: f_00004-9-4 loss: 0.895042  [  160/  306]
train() client id: f_00004-9-5 loss: 0.784176  [  192/  306]
train() client id: f_00004-9-6 loss: 0.832137  [  224/  306]
train() client id: f_00004-9-7 loss: 0.644104  [  256/  306]
train() client id: f_00004-9-8 loss: 0.808003  [  288/  306]
train() client id: f_00004-10-0 loss: 0.777701  [   32/  306]
train() client id: f_00004-10-1 loss: 0.777986  [   64/  306]
train() client id: f_00004-10-2 loss: 0.823004  [   96/  306]
train() client id: f_00004-10-3 loss: 0.825659  [  128/  306]
train() client id: f_00004-10-4 loss: 0.857013  [  160/  306]
train() client id: f_00004-10-5 loss: 0.817645  [  192/  306]
train() client id: f_00004-10-6 loss: 0.769911  [  224/  306]
train() client id: f_00004-10-7 loss: 0.773363  [  256/  306]
train() client id: f_00004-10-8 loss: 0.877170  [  288/  306]
train() client id: f_00004-11-0 loss: 0.774794  [   32/  306]
train() client id: f_00004-11-1 loss: 0.732209  [   64/  306]
train() client id: f_00004-11-2 loss: 0.820304  [   96/  306]
train() client id: f_00004-11-3 loss: 0.726240  [  128/  306]
train() client id: f_00004-11-4 loss: 0.773977  [  160/  306]
train() client id: f_00004-11-5 loss: 0.839262  [  192/  306]
train() client id: f_00004-11-6 loss: 0.856832  [  224/  306]
train() client id: f_00004-11-7 loss: 0.861822  [  256/  306]
train() client id: f_00004-11-8 loss: 0.840226  [  288/  306]
train() client id: f_00005-0-0 loss: 0.576549  [   32/  146]
train() client id: f_00005-0-1 loss: 0.572179  [   64/  146]
train() client id: f_00005-0-2 loss: 0.688174  [   96/  146]
train() client id: f_00005-0-3 loss: 0.803493  [  128/  146]
train() client id: f_00005-1-0 loss: 0.946119  [   32/  146]
train() client id: f_00005-1-1 loss: 0.608416  [   64/  146]
train() client id: f_00005-1-2 loss: 0.577329  [   96/  146]
train() client id: f_00005-1-3 loss: 0.582805  [  128/  146]
train() client id: f_00005-2-0 loss: 0.657731  [   32/  146]
train() client id: f_00005-2-1 loss: 0.601620  [   64/  146]
train() client id: f_00005-2-2 loss: 0.691027  [   96/  146]
train() client id: f_00005-2-3 loss: 0.765529  [  128/  146]
train() client id: f_00005-3-0 loss: 0.672212  [   32/  146]
train() client id: f_00005-3-1 loss: 0.655944  [   64/  146]
train() client id: f_00005-3-2 loss: 0.638844  [   96/  146]
train() client id: f_00005-3-3 loss: 0.745372  [  128/  146]
train() client id: f_00005-4-0 loss: 0.686679  [   32/  146]
train() client id: f_00005-4-1 loss: 0.691774  [   64/  146]
train() client id: f_00005-4-2 loss: 0.549943  [   96/  146]
train() client id: f_00005-4-3 loss: 0.767855  [  128/  146]
train() client id: f_00005-5-0 loss: 0.727668  [   32/  146]
train() client id: f_00005-5-1 loss: 0.509991  [   64/  146]
train() client id: f_00005-5-2 loss: 0.646162  [   96/  146]
train() client id: f_00005-5-3 loss: 0.735669  [  128/  146]
train() client id: f_00005-6-0 loss: 0.738501  [   32/  146]
train() client id: f_00005-6-1 loss: 0.802336  [   64/  146]
train() client id: f_00005-6-2 loss: 0.614874  [   96/  146]
train() client id: f_00005-6-3 loss: 0.610887  [  128/  146]
train() client id: f_00005-7-0 loss: 0.568018  [   32/  146]
train() client id: f_00005-7-1 loss: 0.853669  [   64/  146]
train() client id: f_00005-7-2 loss: 0.517677  [   96/  146]
train() client id: f_00005-7-3 loss: 0.787443  [  128/  146]
train() client id: f_00005-8-0 loss: 0.911366  [   32/  146]
train() client id: f_00005-8-1 loss: 0.559245  [   64/  146]
train() client id: f_00005-8-2 loss: 0.729674  [   96/  146]
train() client id: f_00005-8-3 loss: 0.569436  [  128/  146]
train() client id: f_00005-9-0 loss: 0.635248  [   32/  146]
train() client id: f_00005-9-1 loss: 0.614458  [   64/  146]
train() client id: f_00005-9-2 loss: 0.728339  [   96/  146]
train() client id: f_00005-9-3 loss: 0.826175  [  128/  146]
train() client id: f_00005-10-0 loss: 0.558184  [   32/  146]
train() client id: f_00005-10-1 loss: 0.900884  [   64/  146]
train() client id: f_00005-10-2 loss: 0.562676  [   96/  146]
train() client id: f_00005-10-3 loss: 0.650073  [  128/  146]
train() client id: f_00005-11-0 loss: 0.682765  [   32/  146]
train() client id: f_00005-11-1 loss: 0.572621  [   64/  146]
train() client id: f_00005-11-2 loss: 0.767713  [   96/  146]
train() client id: f_00005-11-3 loss: 0.681321  [  128/  146]
train() client id: f_00006-0-0 loss: 0.527786  [   32/   54]
train() client id: f_00006-1-0 loss: 0.552258  [   32/   54]
train() client id: f_00006-2-0 loss: 0.568710  [   32/   54]
train() client id: f_00006-3-0 loss: 0.509251  [   32/   54]
train() client id: f_00006-4-0 loss: 0.584732  [   32/   54]
train() client id: f_00006-5-0 loss: 0.532055  [   32/   54]
train() client id: f_00006-6-0 loss: 0.593397  [   32/   54]
train() client id: f_00006-7-0 loss: 0.484047  [   32/   54]
train() client id: f_00006-8-0 loss: 0.577604  [   32/   54]
train() client id: f_00006-9-0 loss: 0.581944  [   32/   54]
train() client id: f_00006-10-0 loss: 0.527190  [   32/   54]
train() client id: f_00006-11-0 loss: 0.545121  [   32/   54]
train() client id: f_00007-0-0 loss: 0.476949  [   32/  179]
train() client id: f_00007-0-1 loss: 0.361803  [   64/  179]
train() client id: f_00007-0-2 loss: 0.744847  [   96/  179]
train() client id: f_00007-0-3 loss: 0.384379  [  128/  179]
train() client id: f_00007-0-4 loss: 0.424947  [  160/  179]
train() client id: f_00007-1-0 loss: 0.538696  [   32/  179]
train() client id: f_00007-1-1 loss: 0.401812  [   64/  179]
train() client id: f_00007-1-2 loss: 0.576820  [   96/  179]
train() client id: f_00007-1-3 loss: 0.580651  [  128/  179]
train() client id: f_00007-1-4 loss: 0.340673  [  160/  179]
train() client id: f_00007-2-0 loss: 0.508614  [   32/  179]
train() client id: f_00007-2-1 loss: 0.752900  [   64/  179]
train() client id: f_00007-2-2 loss: 0.389976  [   96/  179]
train() client id: f_00007-2-3 loss: 0.348829  [  128/  179]
train() client id: f_00007-2-4 loss: 0.303294  [  160/  179]
train() client id: f_00007-3-0 loss: 0.448331  [   32/  179]
train() client id: f_00007-3-1 loss: 0.447115  [   64/  179]
train() client id: f_00007-3-2 loss: 0.332873  [   96/  179]
train() client id: f_00007-3-3 loss: 0.577297  [  128/  179]
train() client id: f_00007-3-4 loss: 0.377411  [  160/  179]
train() client id: f_00007-4-0 loss: 0.379077  [   32/  179]
train() client id: f_00007-4-1 loss: 0.390736  [   64/  179]
train() client id: f_00007-4-2 loss: 0.393009  [   96/  179]
train() client id: f_00007-4-3 loss: 0.609478  [  128/  179]
train() client id: f_00007-4-4 loss: 0.395235  [  160/  179]
train() client id: f_00007-5-0 loss: 0.555421  [   32/  179]
train() client id: f_00007-5-1 loss: 0.687302  [   64/  179]
train() client id: f_00007-5-2 loss: 0.303697  [   96/  179]
train() client id: f_00007-5-3 loss: 0.382984  [  128/  179]
train() client id: f_00007-5-4 loss: 0.306374  [  160/  179]
train() client id: f_00007-6-0 loss: 0.387246  [   32/  179]
train() client id: f_00007-6-1 loss: 0.458597  [   64/  179]
train() client id: f_00007-6-2 loss: 0.588599  [   96/  179]
train() client id: f_00007-6-3 loss: 0.386721  [  128/  179]
train() client id: f_00007-6-4 loss: 0.372407  [  160/  179]
train() client id: f_00007-7-0 loss: 0.461541  [   32/  179]
train() client id: f_00007-7-1 loss: 0.376874  [   64/  179]
train() client id: f_00007-7-2 loss: 0.289642  [   96/  179]
train() client id: f_00007-7-3 loss: 0.401403  [  128/  179]
train() client id: f_00007-7-4 loss: 0.388154  [  160/  179]
train() client id: f_00007-8-0 loss: 0.280348  [   32/  179]
train() client id: f_00007-8-1 loss: 0.477774  [   64/  179]
train() client id: f_00007-8-2 loss: 0.567055  [   96/  179]
train() client id: f_00007-8-3 loss: 0.443861  [  128/  179]
train() client id: f_00007-8-4 loss: 0.384318  [  160/  179]
train() client id: f_00007-9-0 loss: 0.400335  [   32/  179]
train() client id: f_00007-9-1 loss: 0.424341  [   64/  179]
train() client id: f_00007-9-2 loss: 0.475203  [   96/  179]
train() client id: f_00007-9-3 loss: 0.359111  [  128/  179]
train() client id: f_00007-9-4 loss: 0.442050  [  160/  179]
train() client id: f_00007-10-0 loss: 0.592827  [   32/  179]
train() client id: f_00007-10-1 loss: 0.294797  [   64/  179]
train() client id: f_00007-10-2 loss: 0.416214  [   96/  179]
train() client id: f_00007-10-3 loss: 0.289524  [  128/  179]
train() client id: f_00007-10-4 loss: 0.352121  [  160/  179]
train() client id: f_00007-11-0 loss: 0.292857  [   32/  179]
train() client id: f_00007-11-1 loss: 0.582984  [   64/  179]
train() client id: f_00007-11-2 loss: 0.440354  [   96/  179]
train() client id: f_00007-11-3 loss: 0.349974  [  128/  179]
train() client id: f_00007-11-4 loss: 0.437122  [  160/  179]
train() client id: f_00008-0-0 loss: 0.856751  [   32/  130]
train() client id: f_00008-0-1 loss: 0.712173  [   64/  130]
train() client id: f_00008-0-2 loss: 0.774619  [   96/  130]
train() client id: f_00008-0-3 loss: 0.849080  [  128/  130]
train() client id: f_00008-1-0 loss: 0.817243  [   32/  130]
train() client id: f_00008-1-1 loss: 0.847117  [   64/  130]
train() client id: f_00008-1-2 loss: 0.774824  [   96/  130]
train() client id: f_00008-1-3 loss: 0.789061  [  128/  130]
train() client id: f_00008-2-0 loss: 0.772883  [   32/  130]
train() client id: f_00008-2-1 loss: 0.857673  [   64/  130]
train() client id: f_00008-2-2 loss: 0.820582  [   96/  130]
train() client id: f_00008-2-3 loss: 0.771251  [  128/  130]
train() client id: f_00008-3-0 loss: 0.749757  [   32/  130]
train() client id: f_00008-3-1 loss: 0.774317  [   64/  130]
train() client id: f_00008-3-2 loss: 0.797289  [   96/  130]
train() client id: f_00008-3-3 loss: 0.893717  [  128/  130]
train() client id: f_00008-4-0 loss: 0.838823  [   32/  130]
train() client id: f_00008-4-1 loss: 0.798208  [   64/  130]
train() client id: f_00008-4-2 loss: 0.756780  [   96/  130]
train() client id: f_00008-4-3 loss: 0.829393  [  128/  130]
train() client id: f_00008-5-0 loss: 0.867631  [   32/  130]
train() client id: f_00008-5-1 loss: 0.788915  [   64/  130]
train() client id: f_00008-5-2 loss: 0.835254  [   96/  130]
train() client id: f_00008-5-3 loss: 0.721659  [  128/  130]
train() client id: f_00008-6-0 loss: 0.804448  [   32/  130]
train() client id: f_00008-6-1 loss: 0.801771  [   64/  130]
train() client id: f_00008-6-2 loss: 0.701645  [   96/  130]
train() client id: f_00008-6-3 loss: 0.910084  [  128/  130]
train() client id: f_00008-7-0 loss: 0.805877  [   32/  130]
train() client id: f_00008-7-1 loss: 0.851542  [   64/  130]
train() client id: f_00008-7-2 loss: 0.792799  [   96/  130]
train() client id: f_00008-7-3 loss: 0.773820  [  128/  130]
train() client id: f_00008-8-0 loss: 0.775090  [   32/  130]
train() client id: f_00008-8-1 loss: 0.830462  [   64/  130]
train() client id: f_00008-8-2 loss: 0.871962  [   96/  130]
train() client id: f_00008-8-3 loss: 0.667019  [  128/  130]
train() client id: f_00008-9-0 loss: 0.738382  [   32/  130]
train() client id: f_00008-9-1 loss: 0.839542  [   64/  130]
train() client id: f_00008-9-2 loss: 0.844151  [   96/  130]
train() client id: f_00008-9-3 loss: 0.803315  [  128/  130]
train() client id: f_00008-10-0 loss: 0.809940  [   32/  130]
train() client id: f_00008-10-1 loss: 0.829158  [   64/  130]
train() client id: f_00008-10-2 loss: 0.624744  [   96/  130]
train() client id: f_00008-10-3 loss: 0.932751  [  128/  130]
train() client id: f_00008-11-0 loss: 0.771351  [   32/  130]
train() client id: f_00008-11-1 loss: 0.793685  [   64/  130]
train() client id: f_00008-11-2 loss: 0.906683  [   96/  130]
train() client id: f_00008-11-3 loss: 0.746053  [  128/  130]
train() client id: f_00009-0-0 loss: 1.027283  [   32/  118]
train() client id: f_00009-0-1 loss: 1.069543  [   64/  118]
train() client id: f_00009-0-2 loss: 1.075845  [   96/  118]
train() client id: f_00009-1-0 loss: 0.907256  [   32/  118]
train() client id: f_00009-1-1 loss: 0.955033  [   64/  118]
train() client id: f_00009-1-2 loss: 1.012113  [   96/  118]
train() client id: f_00009-2-0 loss: 1.048484  [   32/  118]
train() client id: f_00009-2-1 loss: 1.011656  [   64/  118]
train() client id: f_00009-2-2 loss: 0.851492  [   96/  118]
train() client id: f_00009-3-0 loss: 0.909604  [   32/  118]
train() client id: f_00009-3-1 loss: 0.906500  [   64/  118]
train() client id: f_00009-3-2 loss: 0.892002  [   96/  118]
train() client id: f_00009-4-0 loss: 0.908466  [   32/  118]
train() client id: f_00009-4-1 loss: 0.871391  [   64/  118]
train() client id: f_00009-4-2 loss: 0.865735  [   96/  118]
train() client id: f_00009-5-0 loss: 0.789305  [   32/  118]
train() client id: f_00009-5-1 loss: 0.930903  [   64/  118]
train() client id: f_00009-5-2 loss: 0.774232  [   96/  118]
train() client id: f_00009-6-0 loss: 0.821244  [   32/  118]
train() client id: f_00009-6-1 loss: 0.913850  [   64/  118]
train() client id: f_00009-6-2 loss: 0.765840  [   96/  118]
train() client id: f_00009-7-0 loss: 0.732867  [   32/  118]
train() client id: f_00009-7-1 loss: 0.778477  [   64/  118]
train() client id: f_00009-7-2 loss: 0.886289  [   96/  118]
train() client id: f_00009-8-0 loss: 0.756842  [   32/  118]
train() client id: f_00009-8-1 loss: 0.870496  [   64/  118]
train() client id: f_00009-8-2 loss: 0.832677  [   96/  118]
train() client id: f_00009-9-0 loss: 0.819858  [   32/  118]
train() client id: f_00009-9-1 loss: 0.800089  [   64/  118]
train() client id: f_00009-9-2 loss: 0.672430  [   96/  118]
train() client id: f_00009-10-0 loss: 0.786982  [   32/  118]
train() client id: f_00009-10-1 loss: 0.548421  [   64/  118]
train() client id: f_00009-10-2 loss: 0.778403  [   96/  118]
train() client id: f_00009-11-0 loss: 0.645809  [   32/  118]
train() client id: f_00009-11-1 loss: 0.823719  [   64/  118]
train() client id: f_00009-11-2 loss: 0.727724  [   96/  118]
At round 19 accuracy: 0.6339522546419099
At round 19 training accuracy: 0.5767940979208585
At round 19 training loss: 0.8439634070037736
update_location
xs = [ -3.9056584    4.20031788 115.00902392  18.81129433   0.97929623
   3.95640986 -77.44319194 -56.32485185  99.66397685 -42.06087855]
ys = [107.5879595   90.55583871   1.32061395 -77.45517586  69.35018685
  52.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [146.93680001 134.9740812  152.41003774 127.87950994 121.69801739
 113.15912556 126.50825475 114.77441    142.27290063 108.55933567]
dists_bs = [183.89577736 198.09989981 338.03810893 318.27100658 205.25738895
 216.69051292 202.65232753 210.76443071 316.58303499 216.59479044]
uav_gains = [3.81792511e-11 4.72350303e-11 3.48260844e-11 5.40686414e-11
 6.12019359e-11 7.34119429e-11 5.55466623e-11 7.08558845e-11
 4.13970915e-11 8.14379617e-11]
bs_gains = [5.04072687e-11 4.09277782e-11 9.16621896e-12 1.08508127e-11
 3.70559068e-11 3.18377503e-11 3.84051633e-11 3.44081712e-11
 1.10135848e-11 3.18771632e-11]
Round 20
-------------------------------
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.3523103  17.38275937  8.22567535  2.94704545 20.05064182  9.65872482
  3.66105067 11.77706151  8.67193539  7.83910309]
obj_prev = 98.5663077651687
eta_min = 7.590997972408306e-12	eta_max = 0.9222942349256907
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 22.912452031604698	eta = 0.9090909090909091
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 40.06758095330687	eta = 0.519859231611392
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 31.832010204673995	eta = 0.654357098812908
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 30.352361561329435	eta = 0.6862563825495306
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 30.27833469058294	eta = 0.6879341965062457
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 30.27813585075753	eta = 0.6879387142452571
eta = 0.6879387142452571
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [0.03086617 0.06491693 0.03037623 0.01053369 0.07496069 0.03576556
 0.01322836 0.04384955 0.03184604 0.02890641]
ene_total = [2.63093078 4.95082043 2.60801881 1.20448716 5.64850744 2.98715498
 1.38606222 3.45675966 2.88726353 2.51813083]
ti_comp = [0.34802257 0.34915355 0.34647497 0.3533723  0.34752642 0.34490443
 0.35375633 0.35704591 0.32032243 0.34492651]
ti_coms = [0.07621728 0.0750863  0.07776488 0.07086754 0.07671342 0.07933541
 0.07048351 0.06719393 0.10391742 0.07931333]
t_total = [28.99991608 28.99991608 28.99991608 28.99991608 28.99991608 28.99991608
 28.99991608 28.99991608 28.99991608 28.99991608]
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [1.51744596e-05 1.40255892e-04 1.45927700e-05 5.85001217e-07
 2.17974172e-04 2.40368430e-05 1.15608361e-06 4.13358984e-05
 1.96730538e-05 1.26884708e-05]
ene_total = [0.52203848 0.52285762 0.53257769 0.48447181 0.53929285 0.54395903
 0.48188571 0.46214562 0.71169684 0.54303237]
optimize_network iter = 0 obj = 5.343958006285102
eta = 0.6879387142452571
freqs = [4.43450629e+07 9.29633038e+07 4.38361048e+07 1.49045232e+07
 1.07848904e+08 5.18485023e+07 1.86969890e+07 6.14060420e+07
 4.97093568e+07 4.19022717e+07]
eta_min = 0.6879387142452722	eta_max = 0.6879387142452453
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 0.030579617084646203	eta = 0.9090909090909091
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 18.258827212431594	eta = 0.001522532174257359
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 1.8403927678865823	eta = 0.015105282079029619
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 1.8010043726548246	eta = 0.015435638201229906
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 1.800997812209442	eta = 0.015435694428206238
eta = 0.015435694428206238
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [1.69209568e-04 1.56398576e-03 1.62723179e-04 6.52331649e-06
 2.43061804e-03 2.68033519e-04 1.28914250e-05 4.60934337e-04
 2.19373145e-04 1.41488442e-04]
ene_total = [0.16879936 0.19656835 0.17201187 0.15368379 0.21887026 0.17769628
 0.15298971 0.15556983 0.22990166 0.17490671]
ti_comp = [0.34802257 0.34915355 0.34647497 0.3533723  0.34752642 0.34490443
 0.35375633 0.35704591 0.32032243 0.34492651]
ti_coms = [0.07621728 0.0750863  0.07776488 0.07086754 0.07671342 0.07933541
 0.07048351 0.06719393 0.10391742 0.07931333]
t_total = [28.99991608 28.99991608 28.99991608 28.99991608 28.99991608 28.99991608
 28.99991608 28.99991608 28.99991608 28.99991608]
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [1.51744596e-05 1.40255892e-04 1.45927700e-05 5.85001217e-07
 2.17974172e-04 2.40368430e-05 1.15608361e-06 4.13358984e-05
 1.96730538e-05 1.26884708e-05]
ene_total = [0.52203848 0.52285762 0.53257769 0.48447181 0.53929285 0.54395903
 0.48188571 0.46214562 0.71169684 0.54303237]
optimize_network iter = 1 obj = 5.343958006285359
eta = 0.6879387142452722
freqs = [4.43450629e+07 9.29633038e+07 4.38361048e+07 1.49045232e+07
 1.07848904e+08 5.18485023e+07 1.86969890e+07 6.14060420e+07
 4.97093568e+07 4.19022717e+07]
Done!
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [1.48666236e-05 1.37410596e-04 1.42967344e-05 5.73133613e-07
 2.13552247e-04 2.35492206e-05 1.13263077e-06 4.04973394e-05
 1.92739572e-05 1.24310667e-05]
ene_total = [0.00763659 0.00764604 0.00779078 0.00708733 0.00788489 0.00795709
 0.00704948 0.00675989 0.01041102 0.00794376]
At round 20 energy consumption: 0.07816688572627806
At round 20 eta: 0.6879387142452722
At round 20 a_n: 20.989140069898713
At round 20 local rounds: 12.248478257840025
At round 20 global rounds: 68.35736084749131
gradient difference: 0.39586159586906433
train() client id: f_00000-0-0 loss: 1.476672  [   32/  126]
train() client id: f_00000-0-1 loss: 1.346325  [   64/  126]
train() client id: f_00000-0-2 loss: 1.517069  [   96/  126]
train() client id: f_00000-1-0 loss: 1.384053  [   32/  126]
train() client id: f_00000-1-1 loss: 1.405492  [   64/  126]
train() client id: f_00000-1-2 loss: 1.141306  [   96/  126]
train() client id: f_00000-2-0 loss: 1.241161  [   32/  126]
train() client id: f_00000-2-1 loss: 1.135382  [   64/  126]
train() client id: f_00000-2-2 loss: 1.049428  [   96/  126]
train() client id: f_00000-3-0 loss: 1.063090  [   32/  126]
train() client id: f_00000-3-1 loss: 1.011319  [   64/  126]
train() client id: f_00000-3-2 loss: 1.178578  [   96/  126]
train() client id: f_00000-4-0 loss: 1.029292  [   32/  126]
train() client id: f_00000-4-1 loss: 1.064661  [   64/  126]
train() client id: f_00000-4-2 loss: 0.911719  [   96/  126]
train() client id: f_00000-5-0 loss: 0.879802  [   32/  126]
train() client id: f_00000-5-1 loss: 1.063296  [   64/  126]
train() client id: f_00000-5-2 loss: 0.865835  [   96/  126]
train() client id: f_00000-6-0 loss: 0.958118  [   32/  126]
train() client id: f_00000-6-1 loss: 0.741213  [   64/  126]
train() client id: f_00000-6-2 loss: 0.897799  [   96/  126]
train() client id: f_00000-7-0 loss: 0.883658  [   32/  126]
train() client id: f_00000-7-1 loss: 0.801480  [   64/  126]
train() client id: f_00000-7-2 loss: 0.744683  [   96/  126]
train() client id: f_00000-8-0 loss: 0.864452  [   32/  126]
train() client id: f_00000-8-1 loss: 0.819193  [   64/  126]
train() client id: f_00000-8-2 loss: 0.776385  [   96/  126]
train() client id: f_00000-9-0 loss: 0.752831  [   32/  126]
train() client id: f_00000-9-1 loss: 0.712665  [   64/  126]
train() client id: f_00000-9-2 loss: 0.853795  [   96/  126]
train() client id: f_00000-10-0 loss: 0.794146  [   32/  126]
train() client id: f_00000-10-1 loss: 0.797117  [   64/  126]
train() client id: f_00000-10-2 loss: 0.744061  [   96/  126]
train() client id: f_00000-11-0 loss: 0.802908  [   32/  126]
train() client id: f_00000-11-1 loss: 0.728226  [   64/  126]
train() client id: f_00000-11-2 loss: 0.750329  [   96/  126]
train() client id: f_00001-0-0 loss: 0.487570  [   32/  265]
train() client id: f_00001-0-1 loss: 0.584064  [   64/  265]
train() client id: f_00001-0-2 loss: 0.550161  [   96/  265]
train() client id: f_00001-0-3 loss: 0.406825  [  128/  265]
train() client id: f_00001-0-4 loss: 0.639027  [  160/  265]
train() client id: f_00001-0-5 loss: 0.396940  [  192/  265]
train() client id: f_00001-0-6 loss: 0.463532  [  224/  265]
train() client id: f_00001-0-7 loss: 0.473212  [  256/  265]
train() client id: f_00001-1-0 loss: 0.490302  [   32/  265]
train() client id: f_00001-1-1 loss: 0.639346  [   64/  265]
train() client id: f_00001-1-2 loss: 0.442667  [   96/  265]
train() client id: f_00001-1-3 loss: 0.469381  [  128/  265]
train() client id: f_00001-1-4 loss: 0.413039  [  160/  265]
train() client id: f_00001-1-5 loss: 0.491437  [  192/  265]
train() client id: f_00001-1-6 loss: 0.510099  [  224/  265]
train() client id: f_00001-1-7 loss: 0.477781  [  256/  265]
train() client id: f_00001-2-0 loss: 0.397757  [   32/  265]
train() client id: f_00001-2-1 loss: 0.500693  [   64/  265]
train() client id: f_00001-2-2 loss: 0.479337  [   96/  265]
train() client id: f_00001-2-3 loss: 0.646372  [  128/  265]
train() client id: f_00001-2-4 loss: 0.390031  [  160/  265]
train() client id: f_00001-2-5 loss: 0.392991  [  192/  265]
train() client id: f_00001-2-6 loss: 0.579401  [  224/  265]
train() client id: f_00001-2-7 loss: 0.487745  [  256/  265]
train() client id: f_00001-3-0 loss: 0.557331  [   32/  265]
train() client id: f_00001-3-1 loss: 0.381882  [   64/  265]
train() client id: f_00001-3-2 loss: 0.555778  [   96/  265]
train() client id: f_00001-3-3 loss: 0.514742  [  128/  265]
train() client id: f_00001-3-4 loss: 0.509695  [  160/  265]
train() client id: f_00001-3-5 loss: 0.413814  [  192/  265]
train() client id: f_00001-3-6 loss: 0.452192  [  224/  265]
train() client id: f_00001-3-7 loss: 0.450007  [  256/  265]
train() client id: f_00001-4-0 loss: 0.492876  [   32/  265]
train() client id: f_00001-4-1 loss: 0.405112  [   64/  265]
train() client id: f_00001-4-2 loss: 0.404071  [   96/  265]
train() client id: f_00001-4-3 loss: 0.380146  [  128/  265]
train() client id: f_00001-4-4 loss: 0.433854  [  160/  265]
train() client id: f_00001-4-5 loss: 0.563093  [  192/  265]
train() client id: f_00001-4-6 loss: 0.497124  [  224/  265]
train() client id: f_00001-4-7 loss: 0.542652  [  256/  265]
train() client id: f_00001-5-0 loss: 0.424146  [   32/  265]
train() client id: f_00001-5-1 loss: 0.376979  [   64/  265]
train() client id: f_00001-5-2 loss: 0.491051  [   96/  265]
train() client id: f_00001-5-3 loss: 0.551656  [  128/  265]
train() client id: f_00001-5-4 loss: 0.430976  [  160/  265]
train() client id: f_00001-5-5 loss: 0.624853  [  192/  265]
train() client id: f_00001-5-6 loss: 0.454113  [  224/  265]
train() client id: f_00001-5-7 loss: 0.446307  [  256/  265]
train() client id: f_00001-6-0 loss: 0.511265  [   32/  265]
train() client id: f_00001-6-1 loss: 0.429246  [   64/  265]
train() client id: f_00001-6-2 loss: 0.640406  [   96/  265]
train() client id: f_00001-6-3 loss: 0.406305  [  128/  265]
train() client id: f_00001-6-4 loss: 0.472873  [  160/  265]
train() client id: f_00001-6-5 loss: 0.466169  [  192/  265]
train() client id: f_00001-6-6 loss: 0.387973  [  224/  265]
train() client id: f_00001-6-7 loss: 0.453796  [  256/  265]
train() client id: f_00001-7-0 loss: 0.577041  [   32/  265]
train() client id: f_00001-7-1 loss: 0.507899  [   64/  265]
train() client id: f_00001-7-2 loss: 0.565326  [   96/  265]
train() client id: f_00001-7-3 loss: 0.390082  [  128/  265]
train() client id: f_00001-7-4 loss: 0.477802  [  160/  265]
train() client id: f_00001-7-5 loss: 0.421300  [  192/  265]
train() client id: f_00001-7-6 loss: 0.371636  [  224/  265]
train() client id: f_00001-7-7 loss: 0.463523  [  256/  265]
train() client id: f_00001-8-0 loss: 0.422567  [   32/  265]
train() client id: f_00001-8-1 loss: 0.381143  [   64/  265]
train() client id: f_00001-8-2 loss: 0.425877  [   96/  265]
train() client id: f_00001-8-3 loss: 0.520171  [  128/  265]
train() client id: f_00001-8-4 loss: 0.525986  [  160/  265]
train() client id: f_00001-8-5 loss: 0.392551  [  192/  265]
train() client id: f_00001-8-6 loss: 0.612512  [  224/  265]
train() client id: f_00001-8-7 loss: 0.499034  [  256/  265]
train() client id: f_00001-9-0 loss: 0.373660  [   32/  265]
train() client id: f_00001-9-1 loss: 0.539182  [   64/  265]
train() client id: f_00001-9-2 loss: 0.363833  [   96/  265]
train() client id: f_00001-9-3 loss: 0.516985  [  128/  265]
train() client id: f_00001-9-4 loss: 0.473000  [  160/  265]
train() client id: f_00001-9-5 loss: 0.457386  [  192/  265]
train() client id: f_00001-9-6 loss: 0.437642  [  224/  265]
train() client id: f_00001-9-7 loss: 0.535288  [  256/  265]
train() client id: f_00001-10-0 loss: 0.492595  [   32/  265]
train() client id: f_00001-10-1 loss: 0.586366  [   64/  265]
train() client id: f_00001-10-2 loss: 0.448271  [   96/  265]
train() client id: f_00001-10-3 loss: 0.368326  [  128/  265]
train() client id: f_00001-10-4 loss: 0.442251  [  160/  265]
train() client id: f_00001-10-5 loss: 0.441526  [  192/  265]
train() client id: f_00001-10-6 loss: 0.523646  [  224/  265]
train() client id: f_00001-10-7 loss: 0.472787  [  256/  265]
train() client id: f_00001-11-0 loss: 0.553371  [   32/  265]
train() client id: f_00001-11-1 loss: 0.366361  [   64/  265]
train() client id: f_00001-11-2 loss: 0.443211  [   96/  265]
train() client id: f_00001-11-3 loss: 0.477700  [  128/  265]
train() client id: f_00001-11-4 loss: 0.381272  [  160/  265]
train() client id: f_00001-11-5 loss: 0.533260  [  192/  265]
train() client id: f_00001-11-6 loss: 0.464263  [  224/  265]
train() client id: f_00001-11-7 loss: 0.543549  [  256/  265]
train() client id: f_00002-0-0 loss: 1.339113  [   32/  124]
train() client id: f_00002-0-1 loss: 1.149775  [   64/  124]
train() client id: f_00002-0-2 loss: 1.263888  [   96/  124]
train() client id: f_00002-1-0 loss: 1.117389  [   32/  124]
train() client id: f_00002-1-1 loss: 1.298935  [   64/  124]
train() client id: f_00002-1-2 loss: 1.246775  [   96/  124]
train() client id: f_00002-2-0 loss: 1.090222  [   32/  124]
train() client id: f_00002-2-1 loss: 1.222523  [   64/  124]
train() client id: f_00002-2-2 loss: 1.058671  [   96/  124]
train() client id: f_00002-3-0 loss: 1.142612  [   32/  124]
train() client id: f_00002-3-1 loss: 1.069530  [   64/  124]
train() client id: f_00002-3-2 loss: 1.071211  [   96/  124]
train() client id: f_00002-4-0 loss: 1.123127  [   32/  124]
train() client id: f_00002-4-1 loss: 0.959210  [   64/  124]
train() client id: f_00002-4-2 loss: 1.070568  [   96/  124]
train() client id: f_00002-5-0 loss: 1.017744  [   32/  124]
train() client id: f_00002-5-1 loss: 1.074727  [   64/  124]
train() client id: f_00002-5-2 loss: 0.966513  [   96/  124]
train() client id: f_00002-6-0 loss: 1.079327  [   32/  124]
train() client id: f_00002-6-1 loss: 0.896951  [   64/  124]
train() client id: f_00002-6-2 loss: 1.119440  [   96/  124]
train() client id: f_00002-7-0 loss: 0.981089  [   32/  124]
train() client id: f_00002-7-1 loss: 0.939306  [   64/  124]
train() client id: f_00002-7-2 loss: 1.071873  [   96/  124]
train() client id: f_00002-8-0 loss: 1.073922  [   32/  124]
train() client id: f_00002-8-1 loss: 0.984525  [   64/  124]
train() client id: f_00002-8-2 loss: 0.982174  [   96/  124]
train() client id: f_00002-9-0 loss: 1.054725  [   32/  124]
train() client id: f_00002-9-1 loss: 1.026689  [   64/  124]
train() client id: f_00002-9-2 loss: 0.968737  [   96/  124]
train() client id: f_00002-10-0 loss: 1.033964  [   32/  124]
train() client id: f_00002-10-1 loss: 0.896095  [   64/  124]
train() client id: f_00002-10-2 loss: 0.898241  [   96/  124]
train() client id: f_00002-11-0 loss: 1.042657  [   32/  124]
train() client id: f_00002-11-1 loss: 0.968422  [   64/  124]
train() client id: f_00002-11-2 loss: 0.900301  [   96/  124]
train() client id: f_00003-0-0 loss: 0.855881  [   32/   43]
train() client id: f_00003-1-0 loss: 0.740870  [   32/   43]
train() client id: f_00003-2-0 loss: 0.828934  [   32/   43]
train() client id: f_00003-3-0 loss: 0.791970  [   32/   43]
train() client id: f_00003-4-0 loss: 0.800970  [   32/   43]
train() client id: f_00003-5-0 loss: 0.777945  [   32/   43]
train() client id: f_00003-6-0 loss: 0.838965  [   32/   43]
train() client id: f_00003-7-0 loss: 0.753879  [   32/   43]
train() client id: f_00003-8-0 loss: 0.908139  [   32/   43]
train() client id: f_00003-9-0 loss: 0.915991  [   32/   43]
train() client id: f_00003-10-0 loss: 0.707560  [   32/   43]
train() client id: f_00003-11-0 loss: 0.742414  [   32/   43]
train() client id: f_00004-0-0 loss: 0.759661  [   32/  306]
train() client id: f_00004-0-1 loss: 0.824065  [   64/  306]
train() client id: f_00004-0-2 loss: 0.810665  [   96/  306]
train() client id: f_00004-0-3 loss: 0.938576  [  128/  306]
train() client id: f_00004-0-4 loss: 0.798839  [  160/  306]
train() client id: f_00004-0-5 loss: 0.910960  [  192/  306]
train() client id: f_00004-0-6 loss: 0.808866  [  224/  306]
train() client id: f_00004-0-7 loss: 0.792481  [  256/  306]
train() client id: f_00004-0-8 loss: 0.621316  [  288/  306]
train() client id: f_00004-1-0 loss: 0.876772  [   32/  306]
train() client id: f_00004-1-1 loss: 0.835587  [   64/  306]
train() client id: f_00004-1-2 loss: 0.833132  [   96/  306]
train() client id: f_00004-1-3 loss: 0.802701  [  128/  306]
train() client id: f_00004-1-4 loss: 0.798406  [  160/  306]
train() client id: f_00004-1-5 loss: 0.814267  [  192/  306]
train() client id: f_00004-1-6 loss: 0.850633  [  224/  306]
train() client id: f_00004-1-7 loss: 0.754614  [  256/  306]
train() client id: f_00004-1-8 loss: 0.857473  [  288/  306]
train() client id: f_00004-2-0 loss: 0.813928  [   32/  306]
train() client id: f_00004-2-1 loss: 0.826965  [   64/  306]
train() client id: f_00004-2-2 loss: 0.760573  [   96/  306]
train() client id: f_00004-2-3 loss: 0.788611  [  128/  306]
train() client id: f_00004-2-4 loss: 1.041613  [  160/  306]
train() client id: f_00004-2-5 loss: 0.717898  [  192/  306]
train() client id: f_00004-2-6 loss: 0.678255  [  224/  306]
train() client id: f_00004-2-7 loss: 0.806395  [  256/  306]
train() client id: f_00004-2-8 loss: 0.876110  [  288/  306]
train() client id: f_00004-3-0 loss: 0.778508  [   32/  306]
train() client id: f_00004-3-1 loss: 0.842561  [   64/  306]
train() client id: f_00004-3-2 loss: 0.724154  [   96/  306]
train() client id: f_00004-3-3 loss: 0.816300  [  128/  306]
train() client id: f_00004-3-4 loss: 0.872518  [  160/  306]
train() client id: f_00004-3-5 loss: 0.806614  [  192/  306]
train() client id: f_00004-3-6 loss: 0.725457  [  224/  306]
train() client id: f_00004-3-7 loss: 0.842281  [  256/  306]
train() client id: f_00004-3-8 loss: 0.947306  [  288/  306]
train() client id: f_00004-4-0 loss: 0.939384  [   32/  306]
train() client id: f_00004-4-1 loss: 0.782535  [   64/  306]
train() client id: f_00004-4-2 loss: 0.750251  [   96/  306]
train() client id: f_00004-4-3 loss: 0.627769  [  128/  306]
train() client id: f_00004-4-4 loss: 0.766502  [  160/  306]
train() client id: f_00004-4-5 loss: 0.792934  [  192/  306]
train() client id: f_00004-4-6 loss: 0.943109  [  224/  306]
train() client id: f_00004-4-7 loss: 0.871948  [  256/  306]
train() client id: f_00004-4-8 loss: 0.829753  [  288/  306]
train() client id: f_00004-5-0 loss: 0.858267  [   32/  306]
train() client id: f_00004-5-1 loss: 0.935046  [   64/  306]
train() client id: f_00004-5-2 loss: 0.815387  [   96/  306]
train() client id: f_00004-5-3 loss: 0.742961  [  128/  306]
train() client id: f_00004-5-4 loss: 0.795818  [  160/  306]
train() client id: f_00004-5-5 loss: 0.741325  [  192/  306]
train() client id: f_00004-5-6 loss: 0.695468  [  224/  306]
train() client id: f_00004-5-7 loss: 0.824684  [  256/  306]
train() client id: f_00004-5-8 loss: 0.832415  [  288/  306]
train() client id: f_00004-6-0 loss: 0.758383  [   32/  306]
train() client id: f_00004-6-1 loss: 0.995413  [   64/  306]
train() client id: f_00004-6-2 loss: 0.757021  [   96/  306]
train() client id: f_00004-6-3 loss: 0.805460  [  128/  306]
train() client id: f_00004-6-4 loss: 0.835819  [  160/  306]
train() client id: f_00004-6-5 loss: 0.776868  [  192/  306]
train() client id: f_00004-6-6 loss: 0.861150  [  224/  306]
train() client id: f_00004-6-7 loss: 0.809269  [  256/  306]
train() client id: f_00004-6-8 loss: 0.754013  [  288/  306]
train() client id: f_00004-7-0 loss: 0.862889  [   32/  306]
train() client id: f_00004-7-1 loss: 1.015360  [   64/  306]
train() client id: f_00004-7-2 loss: 0.706472  [   96/  306]
train() client id: f_00004-7-3 loss: 0.882492  [  128/  306]
train() client id: f_00004-7-4 loss: 0.756179  [  160/  306]
train() client id: f_00004-7-5 loss: 0.817852  [  192/  306]
train() client id: f_00004-7-6 loss: 0.711076  [  224/  306]
train() client id: f_00004-7-7 loss: 0.791887  [  256/  306]
train() client id: f_00004-7-8 loss: 0.845303  [  288/  306]
train() client id: f_00004-8-0 loss: 0.707708  [   32/  306]
train() client id: f_00004-8-1 loss: 0.883996  [   64/  306]
train() client id: f_00004-8-2 loss: 0.843933  [   96/  306]
train() client id: f_00004-8-3 loss: 0.782598  [  128/  306]
train() client id: f_00004-8-4 loss: 0.779319  [  160/  306]
train() client id: f_00004-8-5 loss: 0.778567  [  192/  306]
train() client id: f_00004-8-6 loss: 0.877160  [  224/  306]
train() client id: f_00004-8-7 loss: 0.862791  [  256/  306]
train() client id: f_00004-8-8 loss: 0.803430  [  288/  306]
train() client id: f_00004-9-0 loss: 0.778117  [   32/  306]
train() client id: f_00004-9-1 loss: 0.852137  [   64/  306]
train() client id: f_00004-9-2 loss: 0.904290  [   96/  306]
train() client id: f_00004-9-3 loss: 0.739849  [  128/  306]
train() client id: f_00004-9-4 loss: 0.695052  [  160/  306]
train() client id: f_00004-9-5 loss: 0.865457  [  192/  306]
train() client id: f_00004-9-6 loss: 0.842440  [  224/  306]
train() client id: f_00004-9-7 loss: 0.791607  [  256/  306]
train() client id: f_00004-9-8 loss: 0.754499  [  288/  306]
train() client id: f_00004-10-0 loss: 0.825497  [   32/  306]
train() client id: f_00004-10-1 loss: 0.805031  [   64/  306]
train() client id: f_00004-10-2 loss: 0.814391  [   96/  306]
train() client id: f_00004-10-3 loss: 0.840970  [  128/  306]
train() client id: f_00004-10-4 loss: 0.776438  [  160/  306]
train() client id: f_00004-10-5 loss: 0.729979  [  192/  306]
train() client id: f_00004-10-6 loss: 0.888433  [  224/  306]
train() client id: f_00004-10-7 loss: 0.774254  [  256/  306]
train() client id: f_00004-10-8 loss: 0.870009  [  288/  306]
train() client id: f_00004-11-0 loss: 0.808652  [   32/  306]
train() client id: f_00004-11-1 loss: 0.756458  [   64/  306]
train() client id: f_00004-11-2 loss: 0.769997  [   96/  306]
train() client id: f_00004-11-3 loss: 0.881030  [  128/  306]
train() client id: f_00004-11-4 loss: 0.860989  [  160/  306]
train() client id: f_00004-11-5 loss: 0.793818  [  192/  306]
train() client id: f_00004-11-6 loss: 0.876144  [  224/  306]
train() client id: f_00004-11-7 loss: 0.857939  [  256/  306]
train() client id: f_00004-11-8 loss: 0.805176  [  288/  306]
train() client id: f_00005-0-0 loss: 0.690844  [   32/  146]
train() client id: f_00005-0-1 loss: 0.449018  [   64/  146]
train() client id: f_00005-0-2 loss: 0.721795  [   96/  146]
train() client id: f_00005-0-3 loss: 0.673976  [  128/  146]
train() client id: f_00005-1-0 loss: 0.948654  [   32/  146]
train() client id: f_00005-1-1 loss: 0.603947  [   64/  146]
train() client id: f_00005-1-2 loss: 0.600429  [   96/  146]
train() client id: f_00005-1-3 loss: 0.464658  [  128/  146]
train() client id: f_00005-2-0 loss: 0.628033  [   32/  146]
train() client id: f_00005-2-1 loss: 0.583380  [   64/  146]
train() client id: f_00005-2-2 loss: 0.757011  [   96/  146]
train() client id: f_00005-2-3 loss: 0.705738  [  128/  146]
train() client id: f_00005-3-0 loss: 0.600389  [   32/  146]
train() client id: f_00005-3-1 loss: 0.788916  [   64/  146]
train() client id: f_00005-3-2 loss: 0.710379  [   96/  146]
train() client id: f_00005-3-3 loss: 0.615683  [  128/  146]
train() client id: f_00005-4-0 loss: 0.431642  [   32/  146]
train() client id: f_00005-4-1 loss: 0.844355  [   64/  146]
train() client id: f_00005-4-2 loss: 0.625410  [   96/  146]
train() client id: f_00005-4-3 loss: 0.836988  [  128/  146]
train() client id: f_00005-5-0 loss: 0.503036  [   32/  146]
train() client id: f_00005-5-1 loss: 0.679273  [   64/  146]
train() client id: f_00005-5-2 loss: 0.862529  [   96/  146]
train() client id: f_00005-5-3 loss: 0.633010  [  128/  146]
train() client id: f_00005-6-0 loss: 0.747263  [   32/  146]
train() client id: f_00005-6-1 loss: 0.670338  [   64/  146]
train() client id: f_00005-6-2 loss: 0.585290  [   96/  146]
train() client id: f_00005-6-3 loss: 0.639698  [  128/  146]
train() client id: f_00005-7-0 loss: 0.624775  [   32/  146]
train() client id: f_00005-7-1 loss: 0.707977  [   64/  146]
train() client id: f_00005-7-2 loss: 0.654850  [   96/  146]
train() client id: f_00005-7-3 loss: 0.684809  [  128/  146]
train() client id: f_00005-8-0 loss: 0.772328  [   32/  146]
train() client id: f_00005-8-1 loss: 0.673062  [   64/  146]
train() client id: f_00005-8-2 loss: 0.663158  [   96/  146]
train() client id: f_00005-8-3 loss: 0.668885  [  128/  146]
train() client id: f_00005-9-0 loss: 0.599057  [   32/  146]
train() client id: f_00005-9-1 loss: 0.892116  [   64/  146]
train() client id: f_00005-9-2 loss: 0.596200  [   96/  146]
train() client id: f_00005-9-3 loss: 0.639253  [  128/  146]
train() client id: f_00005-10-0 loss: 0.626482  [   32/  146]
train() client id: f_00005-10-1 loss: 0.635587  [   64/  146]
train() client id: f_00005-10-2 loss: 0.493666  [   96/  146]
train() client id: f_00005-10-3 loss: 0.681042  [  128/  146]
train() client id: f_00005-11-0 loss: 0.567144  [   32/  146]
train() client id: f_00005-11-1 loss: 0.640871  [   64/  146]
train() client id: f_00005-11-2 loss: 0.728529  [   96/  146]
train() client id: f_00005-11-3 loss: 0.640341  [  128/  146]
train() client id: f_00006-0-0 loss: 0.545984  [   32/   54]
train() client id: f_00006-1-0 loss: 0.501473  [   32/   54]
train() client id: f_00006-2-0 loss: 0.516277  [   32/   54]
train() client id: f_00006-3-0 loss: 0.496609  [   32/   54]
train() client id: f_00006-4-0 loss: 0.556327  [   32/   54]
train() client id: f_00006-5-0 loss: 0.609129  [   32/   54]
train() client id: f_00006-6-0 loss: 0.545486  [   32/   54]
train() client id: f_00006-7-0 loss: 0.611661  [   32/   54]
train() client id: f_00006-8-0 loss: 0.510658  [   32/   54]
train() client id: f_00006-9-0 loss: 0.601290  [   32/   54]
train() client id: f_00006-10-0 loss: 0.591849  [   32/   54]
train() client id: f_00006-11-0 loss: 0.558271  [   32/   54]
train() client id: f_00007-0-0 loss: 0.571744  [   32/  179]
train() client id: f_00007-0-1 loss: 0.734208  [   64/  179]
train() client id: f_00007-0-2 loss: 0.554570  [   96/  179]
train() client id: f_00007-0-3 loss: 0.621545  [  128/  179]
train() client id: f_00007-0-4 loss: 0.436420  [  160/  179]
train() client id: f_00007-1-0 loss: 0.517799  [   32/  179]
train() client id: f_00007-1-1 loss: 0.533582  [   64/  179]
train() client id: f_00007-1-2 loss: 0.623993  [   96/  179]
train() client id: f_00007-1-3 loss: 0.614113  [  128/  179]
train() client id: f_00007-1-4 loss: 0.662224  [  160/  179]
train() client id: f_00007-2-0 loss: 0.718841  [   32/  179]
train() client id: f_00007-2-1 loss: 0.633960  [   64/  179]
train() client id: f_00007-2-2 loss: 0.473292  [   96/  179]
train() client id: f_00007-2-3 loss: 0.606546  [  128/  179]
train() client id: f_00007-2-4 loss: 0.453787  [  160/  179]
train() client id: f_00007-3-0 loss: 0.518561  [   32/  179]
train() client id: f_00007-3-1 loss: 0.693417  [   64/  179]
train() client id: f_00007-3-2 loss: 0.452493  [   96/  179]
train() client id: f_00007-3-3 loss: 0.621619  [  128/  179]
train() client id: f_00007-3-4 loss: 0.485747  [  160/  179]
train() client id: f_00007-4-0 loss: 0.461353  [   32/  179]
train() client id: f_00007-4-1 loss: 0.516925  [   64/  179]
train() client id: f_00007-4-2 loss: 0.579965  [   96/  179]
train() client id: f_00007-4-3 loss: 0.488688  [  128/  179]
train() client id: f_00007-4-4 loss: 0.706691  [  160/  179]
train() client id: f_00007-5-0 loss: 0.510616  [   32/  179]
train() client id: f_00007-5-1 loss: 0.485480  [   64/  179]
train() client id: f_00007-5-2 loss: 0.500293  [   96/  179]
train() client id: f_00007-5-3 loss: 0.527704  [  128/  179]
train() client id: f_00007-5-4 loss: 0.776231  [  160/  179]
train() client id: f_00007-6-0 loss: 0.651342  [   32/  179]
train() client id: f_00007-6-1 loss: 0.573941  [   64/  179]
train() client id: f_00007-6-2 loss: 0.715645  [   96/  179]
train() client id: f_00007-6-3 loss: 0.413434  [  128/  179]
train() client id: f_00007-6-4 loss: 0.423280  [  160/  179]
train() client id: f_00007-7-0 loss: 0.563016  [   32/  179]
train() client id: f_00007-7-1 loss: 0.505985  [   64/  179]
train() client id: f_00007-7-2 loss: 0.654215  [   96/  179]
train() client id: f_00007-7-3 loss: 0.418490  [  128/  179]
train() client id: f_00007-7-4 loss: 0.504705  [  160/  179]
train() client id: f_00007-8-0 loss: 0.612723  [   32/  179]
train() client id: f_00007-8-1 loss: 0.488075  [   64/  179]
train() client id: f_00007-8-2 loss: 0.389141  [   96/  179]
train() client id: f_00007-8-3 loss: 0.473761  [  128/  179]
train() client id: f_00007-8-4 loss: 0.687953  [  160/  179]
train() client id: f_00007-9-0 loss: 0.397023  [   32/  179]
train() client id: f_00007-9-1 loss: 0.406747  [   64/  179]
train() client id: f_00007-9-2 loss: 0.740749  [   96/  179]
train() client id: f_00007-9-3 loss: 0.545013  [  128/  179]
train() client id: f_00007-9-4 loss: 0.628498  [  160/  179]
train() client id: f_00007-10-0 loss: 0.607373  [   32/  179]
train() client id: f_00007-10-1 loss: 0.406827  [   64/  179]
train() client id: f_00007-10-2 loss: 0.482953  [   96/  179]
train() client id: f_00007-10-3 loss: 0.523050  [  128/  179]
train() client id: f_00007-10-4 loss: 0.558770  [  160/  179]
train() client id: f_00007-11-0 loss: 0.476184  [   32/  179]
train() client id: f_00007-11-1 loss: 0.406954  [   64/  179]
train() client id: f_00007-11-2 loss: 0.583310  [   96/  179]
train() client id: f_00007-11-3 loss: 0.618988  [  128/  179]
train() client id: f_00007-11-4 loss: 0.509726  [  160/  179]
train() client id: f_00008-0-0 loss: 0.817064  [   32/  130]
train() client id: f_00008-0-1 loss: 0.686382  [   64/  130]
train() client id: f_00008-0-2 loss: 0.926572  [   96/  130]
train() client id: f_00008-0-3 loss: 0.725041  [  128/  130]
train() client id: f_00008-1-0 loss: 0.791383  [   32/  130]
train() client id: f_00008-1-1 loss: 0.823265  [   64/  130]
train() client id: f_00008-1-2 loss: 0.788388  [   96/  130]
train() client id: f_00008-1-3 loss: 0.772892  [  128/  130]
train() client id: f_00008-2-0 loss: 0.785728  [   32/  130]
train() client id: f_00008-2-1 loss: 0.801150  [   64/  130]
train() client id: f_00008-2-2 loss: 0.801314  [   96/  130]
train() client id: f_00008-2-3 loss: 0.783662  [  128/  130]
train() client id: f_00008-3-0 loss: 0.759955  [   32/  130]
train() client id: f_00008-3-1 loss: 0.772720  [   64/  130]
train() client id: f_00008-3-2 loss: 0.786411  [   96/  130]
train() client id: f_00008-3-3 loss: 0.810611  [  128/  130]
train() client id: f_00008-4-0 loss: 0.759336  [   32/  130]
train() client id: f_00008-4-1 loss: 0.757379  [   64/  130]
train() client id: f_00008-4-2 loss: 0.844298  [   96/  130]
train() client id: f_00008-4-3 loss: 0.805262  [  128/  130]
train() client id: f_00008-5-0 loss: 0.839559  [   32/  130]
train() client id: f_00008-5-1 loss: 0.789431  [   64/  130]
train() client id: f_00008-5-2 loss: 0.781297  [   96/  130]
train() client id: f_00008-5-3 loss: 0.755324  [  128/  130]
train() client id: f_00008-6-0 loss: 0.835360  [   32/  130]
train() client id: f_00008-6-1 loss: 0.716834  [   64/  130]
train() client id: f_00008-6-2 loss: 0.795946  [   96/  130]
train() client id: f_00008-6-3 loss: 0.805409  [  128/  130]
train() client id: f_00008-7-0 loss: 0.880364  [   32/  130]
train() client id: f_00008-7-1 loss: 0.657000  [   64/  130]
train() client id: f_00008-7-2 loss: 0.838608  [   96/  130]
train() client id: f_00008-7-3 loss: 0.789036  [  128/  130]
train() client id: f_00008-8-0 loss: 0.787828  [   32/  130]
train() client id: f_00008-8-1 loss: 0.686202  [   64/  130]
train() client id: f_00008-8-2 loss: 0.799710  [   96/  130]
train() client id: f_00008-8-3 loss: 0.848381  [  128/  130]
train() client id: f_00008-9-0 loss: 0.720188  [   32/  130]
train() client id: f_00008-9-1 loss: 0.727564  [   64/  130]
train() client id: f_00008-9-2 loss: 0.839560  [   96/  130]
train() client id: f_00008-9-3 loss: 0.815276  [  128/  130]
train() client id: f_00008-10-0 loss: 0.723761  [   32/  130]
train() client id: f_00008-10-1 loss: 0.951986  [   64/  130]
train() client id: f_00008-10-2 loss: 0.737268  [   96/  130]
train() client id: f_00008-10-3 loss: 0.736525  [  128/  130]
train() client id: f_00008-11-0 loss: 0.663031  [   32/  130]
train() client id: f_00008-11-1 loss: 0.772945  [   64/  130]
train() client id: f_00008-11-2 loss: 0.835634  [   96/  130]
train() client id: f_00008-11-3 loss: 0.847446  [  128/  130]
train() client id: f_00009-0-0 loss: 1.169300  [   32/  118]
train() client id: f_00009-0-1 loss: 1.116444  [   64/  118]
train() client id: f_00009-0-2 loss: 1.183383  [   96/  118]
train() client id: f_00009-1-0 loss: 0.996789  [   32/  118]
train() client id: f_00009-1-1 loss: 1.194593  [   64/  118]
train() client id: f_00009-1-2 loss: 1.168352  [   96/  118]
train() client id: f_00009-2-0 loss: 0.988839  [   32/  118]
train() client id: f_00009-2-1 loss: 1.003620  [   64/  118]
train() client id: f_00009-2-2 loss: 1.020156  [   96/  118]
train() client id: f_00009-3-0 loss: 1.037178  [   32/  118]
train() client id: f_00009-3-1 loss: 0.944504  [   64/  118]
train() client id: f_00009-3-2 loss: 0.894181  [   96/  118]
train() client id: f_00009-4-0 loss: 1.000237  [   32/  118]
train() client id: f_00009-4-1 loss: 0.907805  [   64/  118]
train() client id: f_00009-4-2 loss: 1.014726  [   96/  118]
train() client id: f_00009-5-0 loss: 0.948476  [   32/  118]
train() client id: f_00009-5-1 loss: 0.963297  [   64/  118]
train() client id: f_00009-5-2 loss: 0.843349  [   96/  118]
train() client id: f_00009-6-0 loss: 0.966280  [   32/  118]
train() client id: f_00009-6-1 loss: 0.891877  [   64/  118]
train() client id: f_00009-6-2 loss: 1.043921  [   96/  118]
train() client id: f_00009-7-0 loss: 0.866695  [   32/  118]
train() client id: f_00009-7-1 loss: 0.908191  [   64/  118]
train() client id: f_00009-7-2 loss: 1.002455  [   96/  118]
train() client id: f_00009-8-0 loss: 0.889608  [   32/  118]
train() client id: f_00009-8-1 loss: 0.909227  [   64/  118]
train() client id: f_00009-8-2 loss: 0.983311  [   96/  118]
train() client id: f_00009-9-0 loss: 0.778322  [   32/  118]
train() client id: f_00009-9-1 loss: 0.954845  [   64/  118]
train() client id: f_00009-9-2 loss: 0.905294  [   96/  118]
train() client id: f_00009-10-0 loss: 1.009972  [   32/  118]
train() client id: f_00009-10-1 loss: 0.984751  [   64/  118]
train() client id: f_00009-10-2 loss: 0.740463  [   96/  118]
train() client id: f_00009-11-0 loss: 0.892838  [   32/  118]
train() client id: f_00009-11-1 loss: 0.827078  [   64/  118]
train() client id: f_00009-11-2 loss: 0.859562  [   96/  118]
At round 20 accuracy: 0.6339522546419099
At round 20 training accuracy: 0.579476861167002
At round 20 training loss: 0.8404099186338115
update_location
xs = [ -3.9056584    4.20031788 120.00902392  18.81129433   0.97929623
   3.95640986 -82.44319194 -61.32485185 104.66397685 -47.06087855]
ys = [112.5879595   95.55583871   1.32061395 -82.45517586  74.35018685
  57.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [150.63632627 138.37832555 156.21750812 130.96839627 124.61504446
 115.57737324 129.62935794 117.30905212 145.81912777 110.59266769]
dists_bs = [182.12231199 196.02073536 342.3373385  322.25143194 202.72912368
 213.91100929 200.2932794  207.99012902 320.93061214 213.56243122]
uav_gains = [3.58669488e-11 4.43787932e-11 3.27265969e-11 5.09343950e-11
 5.76816207e-11 6.96314375e-11 5.22611087e-11 6.70896685e-11
 3.89180544e-11 7.77458897e-11]
bs_gains = [5.17937375e-11 4.21549371e-11 8.84753174e-12 1.04796913e-11
 3.83644419e-11 3.30096791e-11 3.96851685e-11 3.57087343e-11
 1.06009035e-11 3.31607609e-11]
Round 21
-------------------------------
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.22040415 17.10262761  8.09588073  2.90155574 19.72744601  9.50226703
  3.60411444 11.58945304  8.53512887  7.71174179]
obj_prev = 96.99061941020106
eta_min = 5.053940464590129e-12	eta_max = 0.9225775474279215
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 22.544522121240533	eta = 0.9090909090909091
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 39.50145864601099	eta = 0.5188421089429398
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 31.352662916909924	eta = 0.6536931221610769
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 29.88807124482688	eta = 0.6857257513318464
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 29.81462162317344	eta = 0.6874150666493414
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 29.81442331258453	eta = 0.6874196389895562
eta = 0.6874196389895562
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [0.03092845 0.06504793 0.03043752 0.01055495 0.07511195 0.03583773
 0.01325505 0.04393804 0.03191031 0.02896474]
ene_total = [2.59558483 4.86886559 2.57325892 1.19041384 5.55486377 2.93485376
 1.36920185 3.40632189 2.84825067 2.47280818]
ti_comp = [0.35387355 0.35652047 0.35228915 0.35940333 0.35499852 0.35244077
 0.35977839 0.36323055 0.32606389 0.35252092]
ti_coms = [0.07726245 0.07461553 0.07884685 0.07173267 0.07613748 0.07869523
 0.07135761 0.06790545 0.10507211 0.07861508]
t_total = [28.94991188 28.94991188 28.94991188 28.94991188 28.94991188 28.94991188
 28.94991188 28.94991188 28.94991188 28.94991188]
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [1.47658462e-05 1.35335454e-04 1.42006892e-05 5.68963075e-07
 2.10161950e-04 2.31595009e-05 1.12448598e-06 4.01825275e-05
 1.91015032e-05 1.22213485e-05]
ene_total = [0.51979327 0.51011572 0.53039422 0.48170862 0.52535973 0.52997772
 0.47922748 0.45866954 0.70682062 0.52870501]
optimize_network iter = 0 obj = 5.270771946473119
eta = 0.6874196389895562
freqs = [4.36998634e+07 9.12260848e+07 4.31996322e+07 1.46839871e+07
 1.05791923e+08 5.08422020e+07 1.84211324e+07 6.04822990e+07
 4.89325984e+07 4.10822992e+07]
eta_min = 0.6874196389895763	eta_max = 0.6874196389895456
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 0.029002321139164264	eta = 0.9090909090909091
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 18.041876240655995	eta = 0.0014613638924501724
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 1.8124320127004978	eta = 0.014547164420730322
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 1.775015665989668	eta = 0.01485381058620065
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 1.7750098831933823	eta = 0.01485385897835976
eta = 0.01485385897835976
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [1.65835243e-04 1.51995270e-03 1.59487963e-04 6.39002520e-06
 2.36032919e-03 2.60104392e-04 1.26291038e-05 4.51290031e-04
 2.14529013e-04 1.37257984e-04]
ene_total = [0.1681456  0.19135646 0.17138602 0.15296698 0.2125038  0.17320668
 0.15230083 0.15429171 0.22843322 0.17041859]
ti_comp = [0.35387355 0.35652047 0.35228915 0.35940333 0.35499852 0.35244077
 0.35977839 0.36323055 0.32606389 0.35252092]
ti_coms = [0.07726245 0.07461553 0.07884685 0.07173267 0.07613748 0.07869523
 0.07135761 0.06790545 0.10507211 0.07861508]
t_total = [28.94991188 28.94991188 28.94991188 28.94991188 28.94991188 28.94991188
 28.94991188 28.94991188 28.94991188 28.94991188]
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [1.47658462e-05 1.35335454e-04 1.42006892e-05 5.68963075e-07
 2.10161950e-04 2.31595009e-05 1.12448598e-06 4.01825275e-05
 1.91015032e-05 1.22213485e-05]
ene_total = [0.51979327 0.51011572 0.53039422 0.48170862 0.52535973 0.52997772
 0.47922748 0.45866954 0.70682062 0.52870501]
optimize_network iter = 1 obj = 5.270771946473455
eta = 0.6874196389895763
freqs = [4.36998634e+07 9.12260848e+07 4.31996322e+07 1.46839871e+07
 1.05791923e+08 5.08422020e+07 1.84211324e+07 6.04822990e+07
 4.89325984e+07 4.10822992e+07]
Done!
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [1.44371661e-05 1.32322957e-04 1.38845892e-05 5.56298252e-07
 2.05483854e-04 2.26439824e-05 1.09945550e-06 3.92880853e-05
 1.86763136e-05 1.19493076e-05]
ene_total = [0.00774068 0.00759388 0.00789857 0.00717382 0.00781923 0.00789217
 0.00713686 0.00682983 0.01052589 0.00787346]
At round 21 energy consumption: 0.07848438862700059
At round 21 eta: 0.6874196389895763
At round 21 a_n: 20.646594222929394
At round 21 local rounds: 12.273194960737252
At round 21 global rounds: 67.14798076901184
gradient difference: 0.46172624826431274
train() client id: f_00000-0-0 loss: 1.287224  [   32/  126]
train() client id: f_00000-0-1 loss: 1.194085  [   64/  126]
train() client id: f_00000-0-2 loss: 1.118989  [   96/  126]
train() client id: f_00000-1-0 loss: 1.142511  [   32/  126]
train() client id: f_00000-1-1 loss: 1.091175  [   64/  126]
train() client id: f_00000-1-2 loss: 1.138955  [   96/  126]
train() client id: f_00000-2-0 loss: 1.038243  [   32/  126]
train() client id: f_00000-2-1 loss: 0.888403  [   64/  126]
train() client id: f_00000-2-2 loss: 1.077445  [   96/  126]
train() client id: f_00000-3-0 loss: 0.968244  [   32/  126]
train() client id: f_00000-3-1 loss: 1.031134  [   64/  126]
train() client id: f_00000-3-2 loss: 1.005783  [   96/  126]
train() client id: f_00000-4-0 loss: 0.993014  [   32/  126]
train() client id: f_00000-4-1 loss: 1.009449  [   64/  126]
train() client id: f_00000-4-2 loss: 0.901087  [   96/  126]
train() client id: f_00000-5-0 loss: 0.909712  [   32/  126]
train() client id: f_00000-5-1 loss: 0.933395  [   64/  126]
train() client id: f_00000-5-2 loss: 0.874064  [   96/  126]
train() client id: f_00000-6-0 loss: 0.915362  [   32/  126]
train() client id: f_00000-6-1 loss: 0.913693  [   64/  126]
train() client id: f_00000-6-2 loss: 0.961011  [   96/  126]
train() client id: f_00000-7-0 loss: 0.835772  [   32/  126]
train() client id: f_00000-7-1 loss: 0.934314  [   64/  126]
train() client id: f_00000-7-2 loss: 0.844287  [   96/  126]
train() client id: f_00000-8-0 loss: 0.953061  [   32/  126]
train() client id: f_00000-8-1 loss: 0.902621  [   64/  126]
train() client id: f_00000-8-2 loss: 0.845033  [   96/  126]
train() client id: f_00000-9-0 loss: 0.881284  [   32/  126]
train() client id: f_00000-9-1 loss: 0.898428  [   64/  126]
train() client id: f_00000-9-2 loss: 0.955585  [   96/  126]
train() client id: f_00000-10-0 loss: 0.853352  [   32/  126]
train() client id: f_00000-10-1 loss: 0.931947  [   64/  126]
train() client id: f_00000-10-2 loss: 0.835451  [   96/  126]
train() client id: f_00000-11-0 loss: 0.884283  [   32/  126]
train() client id: f_00000-11-1 loss: 0.893591  [   64/  126]
train() client id: f_00000-11-2 loss: 0.963152  [   96/  126]
train() client id: f_00001-0-0 loss: 0.334545  [   32/  265]
train() client id: f_00001-0-1 loss: 0.381775  [   64/  265]
train() client id: f_00001-0-2 loss: 0.334556  [   96/  265]
train() client id: f_00001-0-3 loss: 0.365650  [  128/  265]
train() client id: f_00001-0-4 loss: 0.348212  [  160/  265]
train() client id: f_00001-0-5 loss: 0.542625  [  192/  265]
train() client id: f_00001-0-6 loss: 0.477480  [  224/  265]
train() client id: f_00001-0-7 loss: 0.356150  [  256/  265]
train() client id: f_00001-1-0 loss: 0.438477  [   32/  265]
train() client id: f_00001-1-1 loss: 0.366151  [   64/  265]
train() client id: f_00001-1-2 loss: 0.419009  [   96/  265]
train() client id: f_00001-1-3 loss: 0.441588  [  128/  265]
train() client id: f_00001-1-4 loss: 0.336981  [  160/  265]
train() client id: f_00001-1-5 loss: 0.343531  [  192/  265]
train() client id: f_00001-1-6 loss: 0.289292  [  224/  265]
train() client id: f_00001-1-7 loss: 0.381256  [  256/  265]
train() client id: f_00001-2-0 loss: 0.398043  [   32/  265]
train() client id: f_00001-2-1 loss: 0.330220  [   64/  265]
train() client id: f_00001-2-2 loss: 0.482907  [   96/  265]
train() client id: f_00001-2-3 loss: 0.267021  [  128/  265]
train() client id: f_00001-2-4 loss: 0.414103  [  160/  265]
train() client id: f_00001-2-5 loss: 0.334057  [  192/  265]
train() client id: f_00001-2-6 loss: 0.270577  [  224/  265]
train() client id: f_00001-2-7 loss: 0.399136  [  256/  265]
train() client id: f_00001-3-0 loss: 0.503776  [   32/  265]
train() client id: f_00001-3-1 loss: 0.331013  [   64/  265]
train() client id: f_00001-3-2 loss: 0.278101  [   96/  265]
train() client id: f_00001-3-3 loss: 0.364006  [  128/  265]
train() client id: f_00001-3-4 loss: 0.277241  [  160/  265]
train() client id: f_00001-3-5 loss: 0.440557  [  192/  265]
train() client id: f_00001-3-6 loss: 0.312789  [  224/  265]
train() client id: f_00001-3-7 loss: 0.373946  [  256/  265]
train() client id: f_00001-4-0 loss: 0.372443  [   32/  265]
train() client id: f_00001-4-1 loss: 0.595824  [   64/  265]
train() client id: f_00001-4-2 loss: 0.257189  [   96/  265]
train() client id: f_00001-4-3 loss: 0.332387  [  128/  265]
train() client id: f_00001-4-4 loss: 0.407979  [  160/  265]
train() client id: f_00001-4-5 loss: 0.350739  [  192/  265]
train() client id: f_00001-4-6 loss: 0.251369  [  224/  265]
train() client id: f_00001-4-7 loss: 0.280476  [  256/  265]
train() client id: f_00001-5-0 loss: 0.336248  [   32/  265]
train() client id: f_00001-5-1 loss: 0.276763  [   64/  265]
train() client id: f_00001-5-2 loss: 0.293786  [   96/  265]
train() client id: f_00001-5-3 loss: 0.495350  [  128/  265]
train() client id: f_00001-5-4 loss: 0.373222  [  160/  265]
train() client id: f_00001-5-5 loss: 0.269538  [  192/  265]
train() client id: f_00001-5-6 loss: 0.247786  [  224/  265]
train() client id: f_00001-5-7 loss: 0.450416  [  256/  265]
train() client id: f_00001-6-0 loss: 0.273624  [   32/  265]
train() client id: f_00001-6-1 loss: 0.301915  [   64/  265]
train() client id: f_00001-6-2 loss: 0.529879  [   96/  265]
train() client id: f_00001-6-3 loss: 0.349139  [  128/  265]
train() client id: f_00001-6-4 loss: 0.362553  [  160/  265]
train() client id: f_00001-6-5 loss: 0.335445  [  192/  265]
train() client id: f_00001-6-6 loss: 0.347186  [  224/  265]
train() client id: f_00001-6-7 loss: 0.261487  [  256/  265]
train() client id: f_00001-7-0 loss: 0.398392  [   32/  265]
train() client id: f_00001-7-1 loss: 0.252868  [   64/  265]
train() client id: f_00001-7-2 loss: 0.381584  [   96/  265]
train() client id: f_00001-7-3 loss: 0.377767  [  128/  265]
train() client id: f_00001-7-4 loss: 0.347203  [  160/  265]
train() client id: f_00001-7-5 loss: 0.344678  [  192/  265]
train() client id: f_00001-7-6 loss: 0.364979  [  224/  265]
train() client id: f_00001-7-7 loss: 0.263627  [  256/  265]
train() client id: f_00001-8-0 loss: 0.275311  [   32/  265]
train() client id: f_00001-8-1 loss: 0.295801  [   64/  265]
train() client id: f_00001-8-2 loss: 0.405104  [   96/  265]
train() client id: f_00001-8-3 loss: 0.329891  [  128/  265]
train() client id: f_00001-8-4 loss: 0.271233  [  160/  265]
train() client id: f_00001-8-5 loss: 0.333989  [  192/  265]
train() client id: f_00001-8-6 loss: 0.339587  [  224/  265]
train() client id: f_00001-8-7 loss: 0.380174  [  256/  265]
train() client id: f_00001-9-0 loss: 0.291570  [   32/  265]
train() client id: f_00001-9-1 loss: 0.321492  [   64/  265]
train() client id: f_00001-9-2 loss: 0.221526  [   96/  265]
train() client id: f_00001-9-3 loss: 0.473383  [  128/  265]
train() client id: f_00001-9-4 loss: 0.408779  [  160/  265]
train() client id: f_00001-9-5 loss: 0.246638  [  192/  265]
train() client id: f_00001-9-6 loss: 0.316150  [  224/  265]
train() client id: f_00001-9-7 loss: 0.407184  [  256/  265]
train() client id: f_00001-10-0 loss: 0.304342  [   32/  265]
train() client id: f_00001-10-1 loss: 0.341868  [   64/  265]
train() client id: f_00001-10-2 loss: 0.225703  [   96/  265]
train() client id: f_00001-10-3 loss: 0.301129  [  128/  265]
train() client id: f_00001-10-4 loss: 0.394363  [  160/  265]
train() client id: f_00001-10-5 loss: 0.474376  [  192/  265]
train() client id: f_00001-10-6 loss: 0.287934  [  224/  265]
train() client id: f_00001-10-7 loss: 0.343939  [  256/  265]
train() client id: f_00001-11-0 loss: 0.326912  [   32/  265]
train() client id: f_00001-11-1 loss: 0.304325  [   64/  265]
train() client id: f_00001-11-2 loss: 0.330068  [   96/  265]
train() client id: f_00001-11-3 loss: 0.293697  [  128/  265]
train() client id: f_00001-11-4 loss: 0.430408  [  160/  265]
train() client id: f_00001-11-5 loss: 0.295407  [  192/  265]
train() client id: f_00001-11-6 loss: 0.359717  [  224/  265]
train() client id: f_00001-11-7 loss: 0.325579  [  256/  265]
train() client id: f_00002-0-0 loss: 1.308251  [   32/  124]
train() client id: f_00002-0-1 loss: 1.239287  [   64/  124]
train() client id: f_00002-0-2 loss: 1.173724  [   96/  124]
train() client id: f_00002-1-0 loss: 1.025185  [   32/  124]
train() client id: f_00002-1-1 loss: 1.290071  [   64/  124]
train() client id: f_00002-1-2 loss: 1.162532  [   96/  124]
train() client id: f_00002-2-0 loss: 1.172962  [   32/  124]
train() client id: f_00002-2-1 loss: 1.022366  [   64/  124]
train() client id: f_00002-2-2 loss: 1.158425  [   96/  124]
train() client id: f_00002-3-0 loss: 1.052832  [   32/  124]
train() client id: f_00002-3-1 loss: 1.163728  [   64/  124]
train() client id: f_00002-3-2 loss: 1.175552  [   96/  124]
train() client id: f_00002-4-0 loss: 1.064636  [   32/  124]
train() client id: f_00002-4-1 loss: 1.060243  [   64/  124]
train() client id: f_00002-4-2 loss: 1.119502  [   96/  124]
train() client id: f_00002-5-0 loss: 1.059422  [   32/  124]
train() client id: f_00002-5-1 loss: 1.100421  [   64/  124]
train() client id: f_00002-5-2 loss: 1.033133  [   96/  124]
train() client id: f_00002-6-0 loss: 1.071859  [   32/  124]
train() client id: f_00002-6-1 loss: 0.989446  [   64/  124]
train() client id: f_00002-6-2 loss: 1.263103  [   96/  124]
train() client id: f_00002-7-0 loss: 0.966486  [   32/  124]
train() client id: f_00002-7-1 loss: 1.098706  [   64/  124]
train() client id: f_00002-7-2 loss: 0.925757  [   96/  124]
train() client id: f_00002-8-0 loss: 1.016572  [   32/  124]
train() client id: f_00002-8-1 loss: 1.089716  [   64/  124]
train() client id: f_00002-8-2 loss: 1.087618  [   96/  124]
train() client id: f_00002-9-0 loss: 1.013856  [   32/  124]
train() client id: f_00002-9-1 loss: 1.034896  [   64/  124]
train() client id: f_00002-9-2 loss: 1.138153  [   96/  124]
train() client id: f_00002-10-0 loss: 1.065576  [   32/  124]
train() client id: f_00002-10-1 loss: 1.047507  [   64/  124]
train() client id: f_00002-10-2 loss: 1.054569  [   96/  124]
train() client id: f_00002-11-0 loss: 1.171551  [   32/  124]
train() client id: f_00002-11-1 loss: 0.931202  [   64/  124]
train() client id: f_00002-11-2 loss: 0.997706  [   96/  124]
train() client id: f_00003-0-0 loss: 0.792612  [   32/   43]
train() client id: f_00003-1-0 loss: 0.788700  [   32/   43]
train() client id: f_00003-2-0 loss: 0.835594  [   32/   43]
train() client id: f_00003-3-0 loss: 0.837774  [   32/   43]
train() client id: f_00003-4-0 loss: 0.743665  [   32/   43]
train() client id: f_00003-5-0 loss: 0.897352  [   32/   43]
train() client id: f_00003-6-0 loss: 0.741951  [   32/   43]
train() client id: f_00003-7-0 loss: 0.834859  [   32/   43]
train() client id: f_00003-8-0 loss: 0.712290  [   32/   43]
train() client id: f_00003-9-0 loss: 1.004720  [   32/   43]
train() client id: f_00003-10-0 loss: 0.905963  [   32/   43]
train() client id: f_00003-11-0 loss: 0.819684  [   32/   43]
train() client id: f_00004-0-0 loss: 0.956359  [   32/  306]
train() client id: f_00004-0-1 loss: 1.052967  [   64/  306]
train() client id: f_00004-0-2 loss: 0.918922  [   96/  306]
train() client id: f_00004-0-3 loss: 1.079913  [  128/  306]
train() client id: f_00004-0-4 loss: 0.849029  [  160/  306]
train() client id: f_00004-0-5 loss: 0.855870  [  192/  306]
train() client id: f_00004-0-6 loss: 1.029414  [  224/  306]
train() client id: f_00004-0-7 loss: 0.807128  [  256/  306]
train() client id: f_00004-0-8 loss: 0.967948  [  288/  306]
train() client id: f_00004-1-0 loss: 0.854042  [   32/  306]
train() client id: f_00004-1-1 loss: 0.896675  [   64/  306]
train() client id: f_00004-1-2 loss: 0.974956  [   96/  306]
train() client id: f_00004-1-3 loss: 1.111337  [  128/  306]
train() client id: f_00004-1-4 loss: 0.896730  [  160/  306]
train() client id: f_00004-1-5 loss: 0.955389  [  192/  306]
train() client id: f_00004-1-6 loss: 0.842773  [  224/  306]
train() client id: f_00004-1-7 loss: 0.899700  [  256/  306]
train() client id: f_00004-1-8 loss: 1.021347  [  288/  306]
train() client id: f_00004-2-0 loss: 0.748632  [   32/  306]
train() client id: f_00004-2-1 loss: 1.105417  [   64/  306]
train() client id: f_00004-2-2 loss: 0.871319  [   96/  306]
train() client id: f_00004-2-3 loss: 1.017479  [  128/  306]
train() client id: f_00004-2-4 loss: 0.844515  [  160/  306]
train() client id: f_00004-2-5 loss: 0.855313  [  192/  306]
train() client id: f_00004-2-6 loss: 0.952442  [  224/  306]
train() client id: f_00004-2-7 loss: 1.043888  [  256/  306]
train() client id: f_00004-2-8 loss: 0.929434  [  288/  306]
train() client id: f_00004-3-0 loss: 0.915184  [   32/  306]
train() client id: f_00004-3-1 loss: 0.934801  [   64/  306]
train() client id: f_00004-3-2 loss: 1.025874  [   96/  306]
train() client id: f_00004-3-3 loss: 0.748355  [  128/  306]
train() client id: f_00004-3-4 loss: 0.995207  [  160/  306]
train() client id: f_00004-3-5 loss: 0.893404  [  192/  306]
train() client id: f_00004-3-6 loss: 0.995797  [  224/  306]
train() client id: f_00004-3-7 loss: 1.039132  [  256/  306]
train() client id: f_00004-3-8 loss: 0.835209  [  288/  306]
train() client id: f_00004-4-0 loss: 0.993204  [   32/  306]
train() client id: f_00004-4-1 loss: 0.871292  [   64/  306]
train() client id: f_00004-4-2 loss: 1.027463  [   96/  306]
train() client id: f_00004-4-3 loss: 0.799428  [  128/  306]
train() client id: f_00004-4-4 loss: 0.913082  [  160/  306]
train() client id: f_00004-4-5 loss: 0.848681  [  192/  306]
train() client id: f_00004-4-6 loss: 0.868376  [  224/  306]
train() client id: f_00004-4-7 loss: 0.934105  [  256/  306]
train() client id: f_00004-4-8 loss: 1.103518  [  288/  306]
train() client id: f_00004-5-0 loss: 0.829497  [   32/  306]
train() client id: f_00004-5-1 loss: 1.104537  [   64/  306]
train() client id: f_00004-5-2 loss: 0.978863  [   96/  306]
train() client id: f_00004-5-3 loss: 0.809057  [  128/  306]
train() client id: f_00004-5-4 loss: 0.900701  [  160/  306]
train() client id: f_00004-5-5 loss: 0.800403  [  192/  306]
train() client id: f_00004-5-6 loss: 0.815219  [  224/  306]
train() client id: f_00004-5-7 loss: 0.979980  [  256/  306]
train() client id: f_00004-5-8 loss: 0.939348  [  288/  306]
train() client id: f_00004-6-0 loss: 0.930593  [   32/  306]
train() client id: f_00004-6-1 loss: 0.902249  [   64/  306]
train() client id: f_00004-6-2 loss: 0.765696  [   96/  306]
train() client id: f_00004-6-3 loss: 0.913498  [  128/  306]
train() client id: f_00004-6-4 loss: 0.973293  [  160/  306]
train() client id: f_00004-6-5 loss: 0.978543  [  192/  306]
train() client id: f_00004-6-6 loss: 0.962028  [  224/  306]
train() client id: f_00004-6-7 loss: 0.951677  [  256/  306]
train() client id: f_00004-6-8 loss: 0.847332  [  288/  306]
train() client id: f_00004-7-0 loss: 0.862810  [   32/  306]
train() client id: f_00004-7-1 loss: 0.926390  [   64/  306]
train() client id: f_00004-7-2 loss: 0.908065  [   96/  306]
train() client id: f_00004-7-3 loss: 0.812754  [  128/  306]
train() client id: f_00004-7-4 loss: 0.993054  [  160/  306]
train() client id: f_00004-7-5 loss: 0.778698  [  192/  306]
train() client id: f_00004-7-6 loss: 0.906690  [  224/  306]
train() client id: f_00004-7-7 loss: 0.944468  [  256/  306]
train() client id: f_00004-7-8 loss: 0.992427  [  288/  306]
train() client id: f_00004-8-0 loss: 0.848353  [   32/  306]
train() client id: f_00004-8-1 loss: 0.901791  [   64/  306]
train() client id: f_00004-8-2 loss: 0.844968  [   96/  306]
train() client id: f_00004-8-3 loss: 0.987040  [  128/  306]
train() client id: f_00004-8-4 loss: 1.054538  [  160/  306]
train() client id: f_00004-8-5 loss: 0.878738  [  192/  306]
train() client id: f_00004-8-6 loss: 0.949215  [  224/  306]
train() client id: f_00004-8-7 loss: 0.929449  [  256/  306]
train() client id: f_00004-8-8 loss: 0.827259  [  288/  306]
train() client id: f_00004-9-0 loss: 0.838487  [   32/  306]
train() client id: f_00004-9-1 loss: 0.926485  [   64/  306]
train() client id: f_00004-9-2 loss: 0.854255  [   96/  306]
train() client id: f_00004-9-3 loss: 0.895221  [  128/  306]
train() client id: f_00004-9-4 loss: 0.930032  [  160/  306]
train() client id: f_00004-9-5 loss: 0.956102  [  192/  306]
train() client id: f_00004-9-6 loss: 0.934734  [  224/  306]
train() client id: f_00004-9-7 loss: 0.918944  [  256/  306]
train() client id: f_00004-9-8 loss: 0.904701  [  288/  306]
train() client id: f_00004-10-0 loss: 0.820762  [   32/  306]
train() client id: f_00004-10-1 loss: 1.017152  [   64/  306]
train() client id: f_00004-10-2 loss: 0.935734  [   96/  306]
train() client id: f_00004-10-3 loss: 0.919395  [  128/  306]
train() client id: f_00004-10-4 loss: 0.820029  [  160/  306]
train() client id: f_00004-10-5 loss: 0.835129  [  192/  306]
train() client id: f_00004-10-6 loss: 0.929500  [  224/  306]
train() client id: f_00004-10-7 loss: 0.975501  [  256/  306]
train() client id: f_00004-10-8 loss: 0.879736  [  288/  306]
train() client id: f_00004-11-0 loss: 0.907277  [   32/  306]
train() client id: f_00004-11-1 loss: 0.768717  [   64/  306]
train() client id: f_00004-11-2 loss: 0.933272  [   96/  306]
train() client id: f_00004-11-3 loss: 0.943836  [  128/  306]
train() client id: f_00004-11-4 loss: 0.835826  [  160/  306]
train() client id: f_00004-11-5 loss: 0.895140  [  192/  306]
train() client id: f_00004-11-6 loss: 0.967487  [  224/  306]
train() client id: f_00004-11-7 loss: 0.989325  [  256/  306]
train() client id: f_00004-11-8 loss: 0.871753  [  288/  306]
train() client id: f_00005-0-0 loss: 0.618438  [   32/  146]
train() client id: f_00005-0-1 loss: 0.571212  [   64/  146]
train() client id: f_00005-0-2 loss: 0.875601  [   96/  146]
train() client id: f_00005-0-3 loss: 0.819056  [  128/  146]
train() client id: f_00005-1-0 loss: 0.438712  [   32/  146]
train() client id: f_00005-1-1 loss: 0.572500  [   64/  146]
train() client id: f_00005-1-2 loss: 0.796927  [   96/  146]
train() client id: f_00005-1-3 loss: 0.852074  [  128/  146]
train() client id: f_00005-2-0 loss: 0.721713  [   32/  146]
train() client id: f_00005-2-1 loss: 0.965159  [   64/  146]
train() client id: f_00005-2-2 loss: 0.623607  [   96/  146]
train() client id: f_00005-2-3 loss: 0.554631  [  128/  146]
train() client id: f_00005-3-0 loss: 0.709332  [   32/  146]
train() client id: f_00005-3-1 loss: 0.714258  [   64/  146]
train() client id: f_00005-3-2 loss: 0.602982  [   96/  146]
train() client id: f_00005-3-3 loss: 0.705773  [  128/  146]
train() client id: f_00005-4-0 loss: 0.688356  [   32/  146]
train() client id: f_00005-4-1 loss: 0.572325  [   64/  146]
train() client id: f_00005-4-2 loss: 0.715805  [   96/  146]
train() client id: f_00005-4-3 loss: 0.782900  [  128/  146]
train() client id: f_00005-5-0 loss: 0.467413  [   32/  146]
train() client id: f_00005-5-1 loss: 0.710000  [   64/  146]
train() client id: f_00005-5-2 loss: 0.862376  [   96/  146]
train() client id: f_00005-5-3 loss: 0.783622  [  128/  146]
train() client id: f_00005-6-0 loss: 0.798505  [   32/  146]
train() client id: f_00005-6-1 loss: 0.686931  [   64/  146]
train() client id: f_00005-6-2 loss: 0.525101  [   96/  146]
train() client id: f_00005-6-3 loss: 0.663119  [  128/  146]
train() client id: f_00005-7-0 loss: 0.536187  [   32/  146]
train() client id: f_00005-7-1 loss: 0.937722  [   64/  146]
train() client id: f_00005-7-2 loss: 0.667465  [   96/  146]
train() client id: f_00005-7-3 loss: 0.583702  [  128/  146]
train() client id: f_00005-8-0 loss: 0.482615  [   32/  146]
train() client id: f_00005-8-1 loss: 0.545013  [   64/  146]
train() client id: f_00005-8-2 loss: 0.709814  [   96/  146]
train() client id: f_00005-8-3 loss: 1.102553  [  128/  146]
train() client id: f_00005-9-0 loss: 0.619391  [   32/  146]
train() client id: f_00005-9-1 loss: 0.898305  [   64/  146]
train() client id: f_00005-9-2 loss: 0.634809  [   96/  146]
train() client id: f_00005-9-3 loss: 0.637482  [  128/  146]
train() client id: f_00005-10-0 loss: 0.547888  [   32/  146]
train() client id: f_00005-10-1 loss: 0.667596  [   64/  146]
train() client id: f_00005-10-2 loss: 0.681803  [   96/  146]
train() client id: f_00005-10-3 loss: 0.639828  [  128/  146]
train() client id: f_00005-11-0 loss: 0.705714  [   32/  146]
train() client id: f_00005-11-1 loss: 0.730123  [   64/  146]
train() client id: f_00005-11-2 loss: 0.510469  [   96/  146]
train() client id: f_00005-11-3 loss: 0.920692  [  128/  146]
train() client id: f_00006-0-0 loss: 0.602763  [   32/   54]
train() client id: f_00006-1-0 loss: 0.503935  [   32/   54]
train() client id: f_00006-2-0 loss: 0.606809  [   32/   54]
train() client id: f_00006-3-0 loss: 0.600860  [   32/   54]
train() client id: f_00006-4-0 loss: 0.556932  [   32/   54]
train() client id: f_00006-5-0 loss: 0.505861  [   32/   54]
train() client id: f_00006-6-0 loss: 0.601466  [   32/   54]
train() client id: f_00006-7-0 loss: 0.554880  [   32/   54]
train() client id: f_00006-8-0 loss: 0.562487  [   32/   54]
train() client id: f_00006-9-0 loss: 0.546311  [   32/   54]
train() client id: f_00006-10-0 loss: 0.595036  [   32/   54]
train() client id: f_00006-11-0 loss: 0.541714  [   32/   54]
train() client id: f_00007-0-0 loss: 0.670514  [   32/  179]
train() client id: f_00007-0-1 loss: 0.699082  [   64/  179]
train() client id: f_00007-0-2 loss: 0.517712  [   96/  179]
train() client id: f_00007-0-3 loss: 0.694498  [  128/  179]
train() client id: f_00007-0-4 loss: 0.535402  [  160/  179]
train() client id: f_00007-1-0 loss: 0.671551  [   32/  179]
train() client id: f_00007-1-1 loss: 0.508351  [   64/  179]
train() client id: f_00007-1-2 loss: 0.538905  [   96/  179]
train() client id: f_00007-1-3 loss: 0.615479  [  128/  179]
train() client id: f_00007-1-4 loss: 0.633037  [  160/  179]
train() client id: f_00007-2-0 loss: 0.682666  [   32/  179]
train() client id: f_00007-2-1 loss: 0.453618  [   64/  179]
train() client id: f_00007-2-2 loss: 0.515750  [   96/  179]
train() client id: f_00007-2-3 loss: 0.795235  [  128/  179]
train() client id: f_00007-2-4 loss: 0.557025  [  160/  179]
train() client id: f_00007-3-0 loss: 0.494556  [   32/  179]
train() client id: f_00007-3-1 loss: 0.600792  [   64/  179]
train() client id: f_00007-3-2 loss: 0.689942  [   96/  179]
train() client id: f_00007-3-3 loss: 0.628888  [  128/  179]
train() client id: f_00007-3-4 loss: 0.527791  [  160/  179]
train() client id: f_00007-4-0 loss: 0.547437  [   32/  179]
train() client id: f_00007-4-1 loss: 0.552840  [   64/  179]
train() client id: f_00007-4-2 loss: 0.659499  [   96/  179]
train() client id: f_00007-4-3 loss: 0.463903  [  128/  179]
train() client id: f_00007-4-4 loss: 0.662542  [  160/  179]
train() client id: f_00007-5-0 loss: 0.616453  [   32/  179]
train() client id: f_00007-5-1 loss: 0.686627  [   64/  179]
train() client id: f_00007-5-2 loss: 0.415974  [   96/  179]
train() client id: f_00007-5-3 loss: 0.610767  [  128/  179]
train() client id: f_00007-5-4 loss: 0.468050  [  160/  179]
train() client id: f_00007-6-0 loss: 0.646309  [   32/  179]
train() client id: f_00007-6-1 loss: 0.409426  [   64/  179]
train() client id: f_00007-6-2 loss: 0.611214  [   96/  179]
train() client id: f_00007-6-3 loss: 0.618890  [  128/  179]
train() client id: f_00007-6-4 loss: 0.609049  [  160/  179]
train() client id: f_00007-7-0 loss: 0.620363  [   32/  179]
train() client id: f_00007-7-1 loss: 0.419976  [   64/  179]
train() client id: f_00007-7-2 loss: 0.559554  [   96/  179]
train() client id: f_00007-7-3 loss: 0.725793  [  128/  179]
train() client id: f_00007-7-4 loss: 0.442074  [  160/  179]
train() client id: f_00007-8-0 loss: 0.641213  [   32/  179]
train() client id: f_00007-8-1 loss: 0.440024  [   64/  179]
train() client id: f_00007-8-2 loss: 0.507594  [   96/  179]
train() client id: f_00007-8-3 loss: 0.662998  [  128/  179]
train() client id: f_00007-8-4 loss: 0.519941  [  160/  179]
train() client id: f_00007-9-0 loss: 0.501170  [   32/  179]
train() client id: f_00007-9-1 loss: 0.644336  [   64/  179]
train() client id: f_00007-9-2 loss: 0.655255  [   96/  179]
train() client id: f_00007-9-3 loss: 0.444203  [  128/  179]
train() client id: f_00007-9-4 loss: 0.523372  [  160/  179]
train() client id: f_00007-10-0 loss: 0.587022  [   32/  179]
train() client id: f_00007-10-1 loss: 0.528285  [   64/  179]
train() client id: f_00007-10-2 loss: 0.506672  [   96/  179]
train() client id: f_00007-10-3 loss: 0.589717  [  128/  179]
train() client id: f_00007-10-4 loss: 0.491225  [  160/  179]
train() client id: f_00007-11-0 loss: 0.573074  [   32/  179]
train() client id: f_00007-11-1 loss: 0.503375  [   64/  179]
train() client id: f_00007-11-2 loss: 0.515065  [   96/  179]
train() client id: f_00007-11-3 loss: 0.464409  [  128/  179]
train() client id: f_00007-11-4 loss: 0.683337  [  160/  179]
train() client id: f_00008-0-0 loss: 0.786425  [   32/  130]
train() client id: f_00008-0-1 loss: 0.612974  [   64/  130]
train() client id: f_00008-0-2 loss: 0.710571  [   96/  130]
train() client id: f_00008-0-3 loss: 0.732476  [  128/  130]
train() client id: f_00008-1-0 loss: 0.663161  [   32/  130]
train() client id: f_00008-1-1 loss: 0.786482  [   64/  130]
train() client id: f_00008-1-2 loss: 0.796255  [   96/  130]
train() client id: f_00008-1-3 loss: 0.628868  [  128/  130]
train() client id: f_00008-2-0 loss: 0.646595  [   32/  130]
train() client id: f_00008-2-1 loss: 0.700771  [   64/  130]
train() client id: f_00008-2-2 loss: 0.744372  [   96/  130]
train() client id: f_00008-2-3 loss: 0.789737  [  128/  130]
train() client id: f_00008-3-0 loss: 0.773151  [   32/  130]
train() client id: f_00008-3-1 loss: 0.732044  [   64/  130]
train() client id: f_00008-3-2 loss: 0.665066  [   96/  130]
train() client id: f_00008-3-3 loss: 0.655471  [  128/  130]
train() client id: f_00008-4-0 loss: 0.757954  [   32/  130]
train() client id: f_00008-4-1 loss: 0.663723  [   64/  130]
train() client id: f_00008-4-2 loss: 0.728962  [   96/  130]
train() client id: f_00008-4-3 loss: 0.714424  [  128/  130]
train() client id: f_00008-5-0 loss: 0.821612  [   32/  130]
train() client id: f_00008-5-1 loss: 0.695766  [   64/  130]
train() client id: f_00008-5-2 loss: 0.712605  [   96/  130]
train() client id: f_00008-5-3 loss: 0.642274  [  128/  130]
train() client id: f_00008-6-0 loss: 0.770487  [   32/  130]
train() client id: f_00008-6-1 loss: 0.644974  [   64/  130]
train() client id: f_00008-6-2 loss: 0.714401  [   96/  130]
train() client id: f_00008-6-3 loss: 0.734330  [  128/  130]
train() client id: f_00008-7-0 loss: 0.787261  [   32/  130]
train() client id: f_00008-7-1 loss: 0.676754  [   64/  130]
train() client id: f_00008-7-2 loss: 0.694103  [   96/  130]
train() client id: f_00008-7-3 loss: 0.686298  [  128/  130]
train() client id: f_00008-8-0 loss: 0.684389  [   32/  130]
train() client id: f_00008-8-1 loss: 0.813247  [   64/  130]
train() client id: f_00008-8-2 loss: 0.623673  [   96/  130]
train() client id: f_00008-8-3 loss: 0.706201  [  128/  130]
train() client id: f_00008-9-0 loss: 0.736535  [   32/  130]
train() client id: f_00008-9-1 loss: 0.743941  [   64/  130]
train() client id: f_00008-9-2 loss: 0.764549  [   96/  130]
train() client id: f_00008-9-3 loss: 0.609442  [  128/  130]
train() client id: f_00008-10-0 loss: 0.775964  [   32/  130]
train() client id: f_00008-10-1 loss: 0.626324  [   64/  130]
train() client id: f_00008-10-2 loss: 0.779807  [   96/  130]
train() client id: f_00008-10-3 loss: 0.670305  [  128/  130]
train() client id: f_00008-11-0 loss: 0.622642  [   32/  130]
train() client id: f_00008-11-1 loss: 0.729392  [   64/  130]
train() client id: f_00008-11-2 loss: 0.648475  [   96/  130]
train() client id: f_00008-11-3 loss: 0.862133  [  128/  130]
train() client id: f_00009-0-0 loss: 0.982410  [   32/  118]
train() client id: f_00009-0-1 loss: 1.231739  [   64/  118]
train() client id: f_00009-0-2 loss: 1.129711  [   96/  118]
train() client id: f_00009-1-0 loss: 1.181013  [   32/  118]
train() client id: f_00009-1-1 loss: 1.069272  [   64/  118]
train() client id: f_00009-1-2 loss: 1.075720  [   96/  118]
train() client id: f_00009-2-0 loss: 1.056290  [   32/  118]
train() client id: f_00009-2-1 loss: 1.097274  [   64/  118]
train() client id: f_00009-2-2 loss: 1.064039  [   96/  118]
train() client id: f_00009-3-0 loss: 0.897453  [   32/  118]
train() client id: f_00009-3-1 loss: 1.043454  [   64/  118]
train() client id: f_00009-3-2 loss: 0.991964  [   96/  118]
train() client id: f_00009-4-0 loss: 1.055949  [   32/  118]
train() client id: f_00009-4-1 loss: 0.993281  [   64/  118]
train() client id: f_00009-4-2 loss: 0.918328  [   96/  118]
train() client id: f_00009-5-0 loss: 0.953239  [   32/  118]
train() client id: f_00009-5-1 loss: 0.903189  [   64/  118]
train() client id: f_00009-5-2 loss: 0.971358  [   96/  118]
train() client id: f_00009-6-0 loss: 0.915026  [   32/  118]
train() client id: f_00009-6-1 loss: 0.899577  [   64/  118]
train() client id: f_00009-6-2 loss: 0.981278  [   96/  118]
train() client id: f_00009-7-0 loss: 0.859819  [   32/  118]
train() client id: f_00009-7-1 loss: 0.964208  [   64/  118]
train() client id: f_00009-7-2 loss: 0.981447  [   96/  118]
train() client id: f_00009-8-0 loss: 0.923643  [   32/  118]
train() client id: f_00009-8-1 loss: 0.912300  [   64/  118]
train() client id: f_00009-8-2 loss: 0.868578  [   96/  118]
train() client id: f_00009-9-0 loss: 0.875525  [   32/  118]
train() client id: f_00009-9-1 loss: 0.811761  [   64/  118]
train() client id: f_00009-9-2 loss: 0.933809  [   96/  118]
train() client id: f_00009-10-0 loss: 0.840560  [   32/  118]
train() client id: f_00009-10-1 loss: 0.896857  [   64/  118]
train() client id: f_00009-10-2 loss: 0.957067  [   96/  118]
train() client id: f_00009-11-0 loss: 0.863785  [   32/  118]
train() client id: f_00009-11-1 loss: 0.829952  [   64/  118]
train() client id: f_00009-11-2 loss: 1.062145  [   96/  118]
At round 21 accuracy: 0.636604774535809
At round 21 training accuracy: 0.5788061703554661
At round 21 training loss: 0.8381867752396104
update_location
xs = [ -3.9056584    4.20031788 125.00902392  18.81129433   0.97929623
   3.95640986 -87.44319194 -66.32485185 109.66397685 -52.06087855]
ys = [117.5879595  100.55583871   1.32061395 -87.45517586  79.35018685
  62.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [154.40913958 141.87712771 160.0905996  134.17254778 127.66131432
 118.15782121 132.86535425 119.99859261 149.44851217 112.81111174]
dists_bs = [180.46998676 194.04815659 346.65536714 326.25992268 200.29378287
 211.21330783 198.03239556 205.30012735 325.29693755 210.60513008]
uav_gains = [3.37016863e-11 4.16871884e-11 3.07625343e-11 4.79445886e-11
 5.43001226e-11 6.58910772e-11 4.91340014e-11 6.33925495e-11
 3.65879032e-11 7.39794975e-11]
bs_gains = [5.31324844e-11 4.33658063e-11 8.54239964e-12 1.01231494e-11
 3.96848892e-11 3.42038117e-11 4.09668536e-11 3.70343113e-11
 1.02072830e-11 3.44810940e-11]
Round 22
-------------------------------
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.08847719 16.82258033  7.96606227  2.85607422 19.40433678  9.34589642
  3.54718528 11.40188453  8.39827017  7.58446914]
obj_prev = 95.4152363247567
eta_min = 3.3224549121166505e-12	eta_max = 0.9228798704318716
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 22.17659221087636	eta = 0.909090909090909
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 38.93866072382321	eta = 0.5177512014733847
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 30.874641848632802	eta = 0.6529804773886541
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 29.424734721470024	eta = 0.6851561641714188
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 29.351835344463147	eta = 0.6868578450692009
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 29.351637432902866	eta = 0.6868624763988199
eta = 0.6868624763988199
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [0.03099536 0.06518865 0.03050337 0.01057778 0.07527445 0.03591526
 0.01328373 0.04403309 0.03197934 0.0290274 ]
ene_total = [2.56014735 4.7872217  2.53839595 1.17632747 5.46154932 2.88284624
 1.35232746 3.3560089  2.80903437 2.42777865]
ti_comp = [0.35997945 0.36414214 0.35835849 0.36568135 0.36272784 0.36023616
 0.36604767 0.36965205 0.33207169 0.36037562]
ti_coms = [0.07833233 0.07416965 0.07995329 0.07263043 0.07558394 0.07807562
 0.07226411 0.06865973 0.1062401  0.07793616]
t_total = [28.89990768 28.89990768 28.89990768 28.89990768 28.89990768 28.89990768
 28.89990768 28.89990768 28.89990768 28.89990768]
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [1.43619908e-05 1.30573243e-04 1.38130026e-05 5.53169442e-07
 2.02610025e-04 2.23121998e-05 1.09336312e-06 3.90509212e-05
 1.85363747e-05 1.17704659e-05]
ene_total = [0.51742795 0.49764382 0.5280795  0.47892225 0.51171862 0.51625955
 0.47654255 0.45527992 0.70171191 0.51464497]
optimize_network iter = 0 obj = 5.19823104157566
eta = 0.6868624763988199
freqs = [4.30515682e+07 8.95099024e+07 4.25598540e+07 1.44631133e+07
 1.03761604e+08 4.98496017e+07 1.81448029e+07 5.95601869e+07
 4.81512593e+07 4.02738129e+07]
eta_min = 0.6868624763988167	eta_max = 0.6868624763987461
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 0.02749108426556509	eta = 0.9090909090909091
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 17.828312615008755	eta = 0.0014018093201842298
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 1.7850725781134285	eta = 0.014000492245133406
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 1.749550246261795	eta = 0.014284753947636938
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 1.7495451585936168	eta = 0.014284795487626767
eta = 0.014284795487626767
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [1.62444973e-04 1.47688208e-03 1.56235501e-04 6.25676453e-06
 2.29167254e-03 2.52367846e-04 1.23667633e-05 4.41695438e-04
 2.09660410e-04 1.33132867e-04]
ene_total = [0.16747587 0.18628874 0.17074105 0.15226131 0.20631753 0.16882169
 0.151622   0.15306496 0.22691989 0.16603211]
ti_comp = [0.35997945 0.36414214 0.35835849 0.36568135 0.36272784 0.36023616
 0.36604767 0.36965205 0.33207169 0.36037562]
ti_coms = [0.07833233 0.07416965 0.07995329 0.07263043 0.07558394 0.07807562
 0.07226411 0.06865973 0.1062401  0.07793616]
t_total = [28.89990768 28.89990768 28.89990768 28.89990768 28.89990768 28.89990768
 28.89990768 28.89990768 28.89990768 28.89990768]
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [1.43619908e-05 1.30573243e-04 1.38130026e-05 5.53169442e-07
 2.02610025e-04 2.23121998e-05 1.09336312e-06 3.90509212e-05
 1.85363747e-05 1.17704659e-05]
ene_total = [0.51742795 0.49764382 0.5280795  0.47892225 0.51171862 0.51625955
 0.47654255 0.45527992 0.70171191 0.51464497]
optimize_network iter = 1 obj = 5.198231041575606
eta = 0.6868624763988167
freqs = [4.30515682e+07 8.95099024e+07 4.25598540e+07 1.44631133e+07
 1.03761604e+08 4.98496017e+07 1.81448029e+07 5.95601869e+07
 4.81512593e+07 4.02738129e+07]
Done!
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [1.40119877e-05 1.27391160e-04 1.34763784e-05 5.39688647e-07
 1.97672399e-04 2.17684492e-05 1.06671775e-06 3.80992464e-05
 1.80846414e-05 1.14836185e-05]
ene_total = [0.00784724 0.00754436 0.00800881 0.00726358 0.00775607 0.00782933
 0.00722748 0.00690407 0.01064209 0.0078051 ]
At round 22 energy consumption: 0.0788281303900666
At round 22 eta: 0.6868624763988167
At round 22 a_n: 20.30404837596008
At round 22 local rounds: 12.299746037466248
At round 22 global rounds: 65.93458997020494
gradient difference: 0.4944516718387604
train() client id: f_00000-0-0 loss: 1.433133  [   32/  126]
train() client id: f_00000-0-1 loss: 1.153821  [   64/  126]
train() client id: f_00000-0-2 loss: 1.230956  [   96/  126]
train() client id: f_00000-1-0 loss: 1.450359  [   32/  126]
train() client id: f_00000-1-1 loss: 1.266672  [   64/  126]
train() client id: f_00000-1-2 loss: 0.991367  [   96/  126]
train() client id: f_00000-2-0 loss: 1.099335  [   32/  126]
train() client id: f_00000-2-1 loss: 1.067850  [   64/  126]
train() client id: f_00000-2-2 loss: 1.154376  [   96/  126]
train() client id: f_00000-3-0 loss: 1.110496  [   32/  126]
train() client id: f_00000-3-1 loss: 1.036685  [   64/  126]
train() client id: f_00000-3-2 loss: 0.883238  [   96/  126]
train() client id: f_00000-4-0 loss: 1.043268  [   32/  126]
train() client id: f_00000-4-1 loss: 0.997082  [   64/  126]
train() client id: f_00000-4-2 loss: 0.963671  [   96/  126]
train() client id: f_00000-5-0 loss: 0.844995  [   32/  126]
train() client id: f_00000-5-1 loss: 0.971936  [   64/  126]
train() client id: f_00000-5-2 loss: 0.863355  [   96/  126]
train() client id: f_00000-6-0 loss: 0.974877  [   32/  126]
train() client id: f_00000-6-1 loss: 0.888492  [   64/  126]
train() client id: f_00000-6-2 loss: 0.879169  [   96/  126]
train() client id: f_00000-7-0 loss: 1.014906  [   32/  126]
train() client id: f_00000-7-1 loss: 0.827842  [   64/  126]
train() client id: f_00000-7-2 loss: 0.841141  [   96/  126]
train() client id: f_00000-8-0 loss: 0.811966  [   32/  126]
train() client id: f_00000-8-1 loss: 0.886328  [   64/  126]
train() client id: f_00000-8-2 loss: 0.799364  [   96/  126]
train() client id: f_00000-9-0 loss: 0.852601  [   32/  126]
train() client id: f_00000-9-1 loss: 0.828945  [   64/  126]
train() client id: f_00000-9-2 loss: 0.806878  [   96/  126]
train() client id: f_00000-10-0 loss: 0.832052  [   32/  126]
train() client id: f_00000-10-1 loss: 0.785114  [   64/  126]
train() client id: f_00000-10-2 loss: 0.915501  [   96/  126]
train() client id: f_00000-11-0 loss: 0.841154  [   32/  126]
train() client id: f_00000-11-1 loss: 0.873037  [   64/  126]
train() client id: f_00000-11-2 loss: 0.846571  [   96/  126]
train() client id: f_00001-0-0 loss: 0.421233  [   32/  265]
train() client id: f_00001-0-1 loss: 0.469690  [   64/  265]
train() client id: f_00001-0-2 loss: 0.516534  [   96/  265]
train() client id: f_00001-0-3 loss: 0.463276  [  128/  265]
train() client id: f_00001-0-4 loss: 0.583162  [  160/  265]
train() client id: f_00001-0-5 loss: 0.452086  [  192/  265]
train() client id: f_00001-0-6 loss: 0.422808  [  224/  265]
train() client id: f_00001-0-7 loss: 0.454713  [  256/  265]
train() client id: f_00001-1-0 loss: 0.424635  [   32/  265]
train() client id: f_00001-1-1 loss: 0.491731  [   64/  265]
train() client id: f_00001-1-2 loss: 0.499444  [   96/  265]
train() client id: f_00001-1-3 loss: 0.644475  [  128/  265]
train() client id: f_00001-1-4 loss: 0.393014  [  160/  265]
train() client id: f_00001-1-5 loss: 0.448555  [  192/  265]
train() client id: f_00001-1-6 loss: 0.445149  [  224/  265]
train() client id: f_00001-1-7 loss: 0.450906  [  256/  265]
train() client id: f_00001-2-0 loss: 0.441011  [   32/  265]
train() client id: f_00001-2-1 loss: 0.439217  [   64/  265]
train() client id: f_00001-2-2 loss: 0.532944  [   96/  265]
train() client id: f_00001-2-3 loss: 0.454279  [  128/  265]
train() client id: f_00001-2-4 loss: 0.525502  [  160/  265]
train() client id: f_00001-2-5 loss: 0.473756  [  192/  265]
train() client id: f_00001-2-6 loss: 0.397870  [  224/  265]
train() client id: f_00001-2-7 loss: 0.477265  [  256/  265]
train() client id: f_00001-3-0 loss: 0.486045  [   32/  265]
train() client id: f_00001-3-1 loss: 0.483924  [   64/  265]
train() client id: f_00001-3-2 loss: 0.471030  [   96/  265]
train() client id: f_00001-3-3 loss: 0.445383  [  128/  265]
train() client id: f_00001-3-4 loss: 0.382598  [  160/  265]
train() client id: f_00001-3-5 loss: 0.425892  [  192/  265]
train() client id: f_00001-3-6 loss: 0.519320  [  224/  265]
train() client id: f_00001-3-7 loss: 0.508944  [  256/  265]
train() client id: f_00001-4-0 loss: 0.476344  [   32/  265]
train() client id: f_00001-4-1 loss: 0.391075  [   64/  265]
train() client id: f_00001-4-2 loss: 0.538386  [   96/  265]
train() client id: f_00001-4-3 loss: 0.598084  [  128/  265]
train() client id: f_00001-4-4 loss: 0.431738  [  160/  265]
train() client id: f_00001-4-5 loss: 0.398861  [  192/  265]
train() client id: f_00001-4-6 loss: 0.425264  [  224/  265]
train() client id: f_00001-4-7 loss: 0.371677  [  256/  265]
train() client id: f_00001-5-0 loss: 0.511572  [   32/  265]
train() client id: f_00001-5-1 loss: 0.442198  [   64/  265]
train() client id: f_00001-5-2 loss: 0.431692  [   96/  265]
train() client id: f_00001-5-3 loss: 0.366880  [  128/  265]
train() client id: f_00001-5-4 loss: 0.529527  [  160/  265]
train() client id: f_00001-5-5 loss: 0.430643  [  192/  265]
train() client id: f_00001-5-6 loss: 0.444941  [  224/  265]
train() client id: f_00001-5-7 loss: 0.438708  [  256/  265]
train() client id: f_00001-6-0 loss: 0.434511  [   32/  265]
train() client id: f_00001-6-1 loss: 0.515256  [   64/  265]
train() client id: f_00001-6-2 loss: 0.407067  [   96/  265]
train() client id: f_00001-6-3 loss: 0.470470  [  128/  265]
train() client id: f_00001-6-4 loss: 0.462146  [  160/  265]
train() client id: f_00001-6-5 loss: 0.360610  [  192/  265]
train() client id: f_00001-6-6 loss: 0.421239  [  224/  265]
train() client id: f_00001-6-7 loss: 0.587345  [  256/  265]
train() client id: f_00001-7-0 loss: 0.427320  [   32/  265]
train() client id: f_00001-7-1 loss: 0.427565  [   64/  265]
train() client id: f_00001-7-2 loss: 0.548916  [   96/  265]
train() client id: f_00001-7-3 loss: 0.382756  [  128/  265]
train() client id: f_00001-7-4 loss: 0.460683  [  160/  265]
train() client id: f_00001-7-5 loss: 0.344608  [  192/  265]
train() client id: f_00001-7-6 loss: 0.521497  [  224/  265]
train() client id: f_00001-7-7 loss: 0.456856  [  256/  265]
train() client id: f_00001-8-0 loss: 0.460301  [   32/  265]
train() client id: f_00001-8-1 loss: 0.496569  [   64/  265]
train() client id: f_00001-8-2 loss: 0.500942  [   96/  265]
train() client id: f_00001-8-3 loss: 0.517885  [  128/  265]
train() client id: f_00001-8-4 loss: 0.431740  [  160/  265]
train() client id: f_00001-8-5 loss: 0.354825  [  192/  265]
train() client id: f_00001-8-6 loss: 0.374992  [  224/  265]
train() client id: f_00001-8-7 loss: 0.407893  [  256/  265]
train() client id: f_00001-9-0 loss: 0.546952  [   32/  265]
train() client id: f_00001-9-1 loss: 0.505878  [   64/  265]
train() client id: f_00001-9-2 loss: 0.510473  [   96/  265]
train() client id: f_00001-9-3 loss: 0.467551  [  128/  265]
train() client id: f_00001-9-4 loss: 0.368041  [  160/  265]
train() client id: f_00001-9-5 loss: 0.365396  [  192/  265]
train() client id: f_00001-9-6 loss: 0.435325  [  224/  265]
train() client id: f_00001-9-7 loss: 0.457093  [  256/  265]
train() client id: f_00001-10-0 loss: 0.594719  [   32/  265]
train() client id: f_00001-10-1 loss: 0.432274  [   64/  265]
train() client id: f_00001-10-2 loss: 0.401855  [   96/  265]
train() client id: f_00001-10-3 loss: 0.440124  [  128/  265]
train() client id: f_00001-10-4 loss: 0.429465  [  160/  265]
train() client id: f_00001-10-5 loss: 0.400447  [  192/  265]
train() client id: f_00001-10-6 loss: 0.448640  [  224/  265]
train() client id: f_00001-10-7 loss: 0.431580  [  256/  265]
train() client id: f_00001-11-0 loss: 0.375009  [   32/  265]
train() client id: f_00001-11-1 loss: 0.444668  [   64/  265]
train() client id: f_00001-11-2 loss: 0.357341  [   96/  265]
train() client id: f_00001-11-3 loss: 0.427493  [  128/  265]
train() client id: f_00001-11-4 loss: 0.499870  [  160/  265]
train() client id: f_00001-11-5 loss: 0.407185  [  192/  265]
train() client id: f_00001-11-6 loss: 0.701176  [  224/  265]
train() client id: f_00001-11-7 loss: 0.454200  [  256/  265]
train() client id: f_00002-0-0 loss: 1.137063  [   32/  124]
train() client id: f_00002-0-1 loss: 1.235334  [   64/  124]
train() client id: f_00002-0-2 loss: 1.035921  [   96/  124]
train() client id: f_00002-1-0 loss: 1.063648  [   32/  124]
train() client id: f_00002-1-1 loss: 1.034954  [   64/  124]
train() client id: f_00002-1-2 loss: 1.077303  [   96/  124]
train() client id: f_00002-2-0 loss: 1.018910  [   32/  124]
train() client id: f_00002-2-1 loss: 1.126666  [   64/  124]
train() client id: f_00002-2-2 loss: 1.047077  [   96/  124]
train() client id: f_00002-3-0 loss: 0.956090  [   32/  124]
train() client id: f_00002-3-1 loss: 1.189999  [   64/  124]
train() client id: f_00002-3-2 loss: 1.005013  [   96/  124]
train() client id: f_00002-4-0 loss: 1.141874  [   32/  124]
train() client id: f_00002-4-1 loss: 1.075571  [   64/  124]
train() client id: f_00002-4-2 loss: 0.926674  [   96/  124]
train() client id: f_00002-5-0 loss: 0.905371  [   32/  124]
train() client id: f_00002-5-1 loss: 0.943969  [   64/  124]
train() client id: f_00002-5-2 loss: 1.187364  [   96/  124]
train() client id: f_00002-6-0 loss: 1.057297  [   32/  124]
train() client id: f_00002-6-1 loss: 1.094943  [   64/  124]
train() client id: f_00002-6-2 loss: 0.982889  [   96/  124]
train() client id: f_00002-7-0 loss: 1.053729  [   32/  124]
train() client id: f_00002-7-1 loss: 1.064000  [   64/  124]
train() client id: f_00002-7-2 loss: 1.125295  [   96/  124]
train() client id: f_00002-8-0 loss: 0.993186  [   32/  124]
train() client id: f_00002-8-1 loss: 0.961640  [   64/  124]
train() client id: f_00002-8-2 loss: 1.026283  [   96/  124]
train() client id: f_00002-9-0 loss: 1.139833  [   32/  124]
train() client id: f_00002-9-1 loss: 1.064362  [   64/  124]
train() client id: f_00002-9-2 loss: 1.002055  [   96/  124]
train() client id: f_00002-10-0 loss: 0.943857  [   32/  124]
train() client id: f_00002-10-1 loss: 1.107098  [   64/  124]
train() client id: f_00002-10-2 loss: 0.881902  [   96/  124]
train() client id: f_00002-11-0 loss: 1.086740  [   32/  124]
train() client id: f_00002-11-1 loss: 1.076292  [   64/  124]
train() client id: f_00002-11-2 loss: 1.087063  [   96/  124]
train() client id: f_00003-0-0 loss: 0.746555  [   32/   43]
train() client id: f_00003-1-0 loss: 0.748680  [   32/   43]
train() client id: f_00003-2-0 loss: 0.753086  [   32/   43]
train() client id: f_00003-3-0 loss: 0.700965  [   32/   43]
train() client id: f_00003-4-0 loss: 0.589947  [   32/   43]
train() client id: f_00003-5-0 loss: 0.851098  [   32/   43]
train() client id: f_00003-6-0 loss: 0.710225  [   32/   43]
train() client id: f_00003-7-0 loss: 0.846756  [   32/   43]
train() client id: f_00003-8-0 loss: 0.668924  [   32/   43]
train() client id: f_00003-9-0 loss: 0.844832  [   32/   43]
train() client id: f_00003-10-0 loss: 0.739495  [   32/   43]
train() client id: f_00003-11-0 loss: 0.657479  [   32/   43]
train() client id: f_00004-0-0 loss: 0.971886  [   32/  306]
train() client id: f_00004-0-1 loss: 0.852068  [   64/  306]
train() client id: f_00004-0-2 loss: 0.943816  [   96/  306]
train() client id: f_00004-0-3 loss: 0.949503  [  128/  306]
train() client id: f_00004-0-4 loss: 0.997948  [  160/  306]
train() client id: f_00004-0-5 loss: 0.927556  [  192/  306]
train() client id: f_00004-0-6 loss: 1.026954  [  224/  306]
train() client id: f_00004-0-7 loss: 0.923981  [  256/  306]
train() client id: f_00004-0-8 loss: 0.939643  [  288/  306]
train() client id: f_00004-1-0 loss: 0.978794  [   32/  306]
train() client id: f_00004-1-1 loss: 0.864671  [   64/  306]
train() client id: f_00004-1-2 loss: 0.939495  [   96/  306]
train() client id: f_00004-1-3 loss: 0.914270  [  128/  306]
train() client id: f_00004-1-4 loss: 0.990384  [  160/  306]
train() client id: f_00004-1-5 loss: 1.012464  [  192/  306]
train() client id: f_00004-1-6 loss: 1.019406  [  224/  306]
train() client id: f_00004-1-7 loss: 0.882805  [  256/  306]
train() client id: f_00004-1-8 loss: 0.805744  [  288/  306]
train() client id: f_00004-2-0 loss: 1.258675  [   32/  306]
train() client id: f_00004-2-1 loss: 0.872214  [   64/  306]
train() client id: f_00004-2-2 loss: 0.958409  [   96/  306]
train() client id: f_00004-2-3 loss: 0.838453  [  128/  306]
train() client id: f_00004-2-4 loss: 0.845145  [  160/  306]
train() client id: f_00004-2-5 loss: 0.913299  [  192/  306]
train() client id: f_00004-2-6 loss: 0.897361  [  224/  306]
train() client id: f_00004-2-7 loss: 0.964037  [  256/  306]
train() client id: f_00004-2-8 loss: 0.909809  [  288/  306]
train() client id: f_00004-3-0 loss: 0.879023  [   32/  306]
train() client id: f_00004-3-1 loss: 0.826600  [   64/  306]
train() client id: f_00004-3-2 loss: 0.979580  [   96/  306]
train() client id: f_00004-3-3 loss: 0.921298  [  128/  306]
train() client id: f_00004-3-4 loss: 1.055203  [  160/  306]
train() client id: f_00004-3-5 loss: 0.988498  [  192/  306]
train() client id: f_00004-3-6 loss: 0.943536  [  224/  306]
train() client id: f_00004-3-7 loss: 0.922649  [  256/  306]
train() client id: f_00004-3-8 loss: 0.885990  [  288/  306]
train() client id: f_00004-4-0 loss: 0.950238  [   32/  306]
train() client id: f_00004-4-1 loss: 0.911254  [   64/  306]
train() client id: f_00004-4-2 loss: 0.895847  [   96/  306]
train() client id: f_00004-4-3 loss: 1.017393  [  128/  306]
train() client id: f_00004-4-4 loss: 1.039305  [  160/  306]
train() client id: f_00004-4-5 loss: 0.882126  [  192/  306]
train() client id: f_00004-4-6 loss: 0.858407  [  224/  306]
train() client id: f_00004-4-7 loss: 0.959088  [  256/  306]
train() client id: f_00004-4-8 loss: 0.839762  [  288/  306]
train() client id: f_00004-5-0 loss: 0.876166  [   32/  306]
train() client id: f_00004-5-1 loss: 0.921008  [   64/  306]
train() client id: f_00004-5-2 loss: 1.040957  [   96/  306]
train() client id: f_00004-5-3 loss: 0.949163  [  128/  306]
train() client id: f_00004-5-4 loss: 1.048203  [  160/  306]
train() client id: f_00004-5-5 loss: 0.820299  [  192/  306]
train() client id: f_00004-5-6 loss: 0.944535  [  224/  306]
train() client id: f_00004-5-7 loss: 0.822343  [  256/  306]
train() client id: f_00004-5-8 loss: 0.861561  [  288/  306]
train() client id: f_00004-6-0 loss: 0.981000  [   32/  306]
train() client id: f_00004-6-1 loss: 0.903355  [   64/  306]
train() client id: f_00004-6-2 loss: 0.994958  [   96/  306]
train() client id: f_00004-6-3 loss: 0.947609  [  128/  306]
train() client id: f_00004-6-4 loss: 0.896264  [  160/  306]
train() client id: f_00004-6-5 loss: 0.968080  [  192/  306]
train() client id: f_00004-6-6 loss: 0.890002  [  224/  306]
train() client id: f_00004-6-7 loss: 0.915363  [  256/  306]
train() client id: f_00004-6-8 loss: 0.872659  [  288/  306]
train() client id: f_00004-7-0 loss: 0.970626  [   32/  306]
train() client id: f_00004-7-1 loss: 0.801162  [   64/  306]
train() client id: f_00004-7-2 loss: 0.934031  [   96/  306]
train() client id: f_00004-7-3 loss: 0.932230  [  128/  306]
train() client id: f_00004-7-4 loss: 0.900432  [  160/  306]
train() client id: f_00004-7-5 loss: 0.927991  [  192/  306]
train() client id: f_00004-7-6 loss: 1.011263  [  224/  306]
train() client id: f_00004-7-7 loss: 0.873741  [  256/  306]
train() client id: f_00004-7-8 loss: 1.011787  [  288/  306]
train() client id: f_00004-8-0 loss: 0.879445  [   32/  306]
train() client id: f_00004-8-1 loss: 0.853380  [   64/  306]
train() client id: f_00004-8-2 loss: 0.961035  [   96/  306]
train() client id: f_00004-8-3 loss: 0.965704  [  128/  306]
train() client id: f_00004-8-4 loss: 0.917552  [  160/  306]
train() client id: f_00004-8-5 loss: 0.857481  [  192/  306]
train() client id: f_00004-8-6 loss: 0.929156  [  224/  306]
train() client id: f_00004-8-7 loss: 0.901743  [  256/  306]
train() client id: f_00004-8-8 loss: 1.034931  [  288/  306]
train() client id: f_00004-9-0 loss: 0.950273  [   32/  306]
train() client id: f_00004-9-1 loss: 0.833563  [   64/  306]
train() client id: f_00004-9-2 loss: 0.958571  [   96/  306]
train() client id: f_00004-9-3 loss: 0.944409  [  128/  306]
train() client id: f_00004-9-4 loss: 0.873971  [  160/  306]
train() client id: f_00004-9-5 loss: 0.996227  [  192/  306]
train() client id: f_00004-9-6 loss: 1.043601  [  224/  306]
train() client id: f_00004-9-7 loss: 0.905909  [  256/  306]
train() client id: f_00004-9-8 loss: 0.862105  [  288/  306]
train() client id: f_00004-10-0 loss: 0.991062  [   32/  306]
train() client id: f_00004-10-1 loss: 0.943583  [   64/  306]
train() client id: f_00004-10-2 loss: 0.787194  [   96/  306]
train() client id: f_00004-10-3 loss: 0.845940  [  128/  306]
train() client id: f_00004-10-4 loss: 0.938740  [  160/  306]
train() client id: f_00004-10-5 loss: 0.939162  [  192/  306]
train() client id: f_00004-10-6 loss: 0.867679  [  224/  306]
train() client id: f_00004-10-7 loss: 0.941011  [  256/  306]
train() client id: f_00004-10-8 loss: 1.015336  [  288/  306]
train() client id: f_00004-11-0 loss: 0.880268  [   32/  306]
train() client id: f_00004-11-1 loss: 0.929577  [   64/  306]
train() client id: f_00004-11-2 loss: 0.871544  [   96/  306]
train() client id: f_00004-11-3 loss: 0.920797  [  128/  306]
train() client id: f_00004-11-4 loss: 1.009239  [  160/  306]
train() client id: f_00004-11-5 loss: 0.917073  [  192/  306]
train() client id: f_00004-11-6 loss: 0.943205  [  224/  306]
train() client id: f_00004-11-7 loss: 0.900885  [  256/  306]
train() client id: f_00004-11-8 loss: 0.866580  [  288/  306]
train() client id: f_00005-0-0 loss: 0.428760  [   32/  146]
train() client id: f_00005-0-1 loss: 0.626693  [   64/  146]
train() client id: f_00005-0-2 loss: 0.440662  [   96/  146]
train() client id: f_00005-0-3 loss: 0.377549  [  128/  146]
train() client id: f_00005-1-0 loss: 0.640704  [   32/  146]
train() client id: f_00005-1-1 loss: 0.367288  [   64/  146]
train() client id: f_00005-1-2 loss: 0.316933  [   96/  146]
train() client id: f_00005-1-3 loss: 0.440887  [  128/  146]
train() client id: f_00005-2-0 loss: 0.560027  [   32/  146]
train() client id: f_00005-2-1 loss: 0.354086  [   64/  146]
train() client id: f_00005-2-2 loss: 0.460658  [   96/  146]
train() client id: f_00005-2-3 loss: 0.273238  [  128/  146]
train() client id: f_00005-3-0 loss: 0.546265  [   32/  146]
train() client id: f_00005-3-1 loss: 0.392002  [   64/  146]
train() client id: f_00005-3-2 loss: 0.240803  [   96/  146]
train() client id: f_00005-3-3 loss: 0.628971  [  128/  146]
train() client id: f_00005-4-0 loss: 0.339341  [   32/  146]
train() client id: f_00005-4-1 loss: 0.413961  [   64/  146]
train() client id: f_00005-4-2 loss: 0.330884  [   96/  146]
train() client id: f_00005-4-3 loss: 0.499111  [  128/  146]
train() client id: f_00005-5-0 loss: 0.492595  [   32/  146]
train() client id: f_00005-5-1 loss: 0.390380  [   64/  146]
train() client id: f_00005-5-2 loss: 0.483010  [   96/  146]
train() client id: f_00005-5-3 loss: 0.259076  [  128/  146]
train() client id: f_00005-6-0 loss: 0.632149  [   32/  146]
train() client id: f_00005-6-1 loss: 0.283119  [   64/  146]
train() client id: f_00005-6-2 loss: 0.461826  [   96/  146]
train() client id: f_00005-6-3 loss: 0.444977  [  128/  146]
train() client id: f_00005-7-0 loss: 0.249822  [   32/  146]
train() client id: f_00005-7-1 loss: 0.576801  [   64/  146]
train() client id: f_00005-7-2 loss: 0.359319  [   96/  146]
train() client id: f_00005-7-3 loss: 0.577523  [  128/  146]
train() client id: f_00005-8-0 loss: 0.312552  [   32/  146]
train() client id: f_00005-8-1 loss: 0.354515  [   64/  146]
train() client id: f_00005-8-2 loss: 0.584624  [   96/  146]
train() client id: f_00005-8-3 loss: 0.415347  [  128/  146]
train() client id: f_00005-9-0 loss: 0.546880  [   32/  146]
train() client id: f_00005-9-1 loss: 0.528829  [   64/  146]
train() client id: f_00005-9-2 loss: 0.352048  [   96/  146]
train() client id: f_00005-9-3 loss: 0.331753  [  128/  146]
train() client id: f_00005-10-0 loss: 0.329547  [   32/  146]
train() client id: f_00005-10-1 loss: 0.387449  [   64/  146]
train() client id: f_00005-10-2 loss: 0.589478  [   96/  146]
train() client id: f_00005-10-3 loss: 0.347028  [  128/  146]
train() client id: f_00005-11-0 loss: 0.585868  [   32/  146]
train() client id: f_00005-11-1 loss: 0.509332  [   64/  146]
train() client id: f_00005-11-2 loss: 0.315382  [   96/  146]
train() client id: f_00005-11-3 loss: 0.299163  [  128/  146]
train() client id: f_00006-0-0 loss: 0.573961  [   32/   54]
train() client id: f_00006-1-0 loss: 0.566786  [   32/   54]
train() client id: f_00006-2-0 loss: 0.590746  [   32/   54]
train() client id: f_00006-3-0 loss: 0.568562  [   32/   54]
train() client id: f_00006-4-0 loss: 0.567963  [   32/   54]
train() client id: f_00006-5-0 loss: 0.564012  [   32/   54]
train() client id: f_00006-6-0 loss: 0.568277  [   32/   54]
train() client id: f_00006-7-0 loss: 0.622734  [   32/   54]
train() client id: f_00006-8-0 loss: 0.572169  [   32/   54]
train() client id: f_00006-9-0 loss: 0.577661  [   32/   54]
train() client id: f_00006-10-0 loss: 0.622874  [   32/   54]
train() client id: f_00006-11-0 loss: 0.586459  [   32/   54]
train() client id: f_00007-0-0 loss: 0.573244  [   32/  179]
train() client id: f_00007-0-1 loss: 0.489163  [   64/  179]
train() client id: f_00007-0-2 loss: 0.326220  [   96/  179]
train() client id: f_00007-0-3 loss: 0.519095  [  128/  179]
train() client id: f_00007-0-4 loss: 0.516032  [  160/  179]
train() client id: f_00007-1-0 loss: 0.430165  [   32/  179]
train() client id: f_00007-1-1 loss: 0.467580  [   64/  179]
train() client id: f_00007-1-2 loss: 0.393167  [   96/  179]
train() client id: f_00007-1-3 loss: 0.557216  [  128/  179]
train() client id: f_00007-1-4 loss: 0.465762  [  160/  179]
train() client id: f_00007-2-0 loss: 0.342829  [   32/  179]
train() client id: f_00007-2-1 loss: 0.416260  [   64/  179]
train() client id: f_00007-2-2 loss: 0.332880  [   96/  179]
train() client id: f_00007-2-3 loss: 0.701448  [  128/  179]
train() client id: f_00007-2-4 loss: 0.356594  [  160/  179]
train() client id: f_00007-3-0 loss: 0.324696  [   32/  179]
train() client id: f_00007-3-1 loss: 0.751119  [   64/  179]
train() client id: f_00007-3-2 loss: 0.462322  [   96/  179]
train() client id: f_00007-3-3 loss: 0.381001  [  128/  179]
train() client id: f_00007-3-4 loss: 0.309759  [  160/  179]
train() client id: f_00007-4-0 loss: 0.385368  [   32/  179]
train() client id: f_00007-4-1 loss: 0.296531  [   64/  179]
train() client id: f_00007-4-2 loss: 0.539331  [   96/  179]
train() client id: f_00007-4-3 loss: 0.443750  [  128/  179]
train() client id: f_00007-4-4 loss: 0.512196  [  160/  179]
train() client id: f_00007-5-0 loss: 0.476499  [   32/  179]
train() client id: f_00007-5-1 loss: 0.437856  [   64/  179]
train() client id: f_00007-5-2 loss: 0.520665  [   96/  179]
train() client id: f_00007-5-3 loss: 0.285477  [  128/  179]
train() client id: f_00007-5-4 loss: 0.270247  [  160/  179]
train() client id: f_00007-6-0 loss: 0.470327  [   32/  179]
train() client id: f_00007-6-1 loss: 0.545044  [   64/  179]
train() client id: f_00007-6-2 loss: 0.425113  [   96/  179]
train() client id: f_00007-6-3 loss: 0.313349  [  128/  179]
train() client id: f_00007-6-4 loss: 0.372455  [  160/  179]
train() client id: f_00007-7-0 loss: 0.361968  [   32/  179]
train() client id: f_00007-7-1 loss: 0.737593  [   64/  179]
train() client id: f_00007-7-2 loss: 0.348579  [   96/  179]
train() client id: f_00007-7-3 loss: 0.290439  [  128/  179]
train() client id: f_00007-7-4 loss: 0.348415  [  160/  179]
train() client id: f_00007-8-0 loss: 0.459049  [   32/  179]
train() client id: f_00007-8-1 loss: 0.482960  [   64/  179]
train() client id: f_00007-8-2 loss: 0.323001  [   96/  179]
train() client id: f_00007-8-3 loss: 0.433384  [  128/  179]
train() client id: f_00007-8-4 loss: 0.298483  [  160/  179]
train() client id: f_00007-9-0 loss: 0.446481  [   32/  179]
train() client id: f_00007-9-1 loss: 0.358315  [   64/  179]
train() client id: f_00007-9-2 loss: 0.418808  [   96/  179]
train() client id: f_00007-9-3 loss: 0.259724  [  128/  179]
train() client id: f_00007-9-4 loss: 0.452749  [  160/  179]
train() client id: f_00007-10-0 loss: 0.359936  [   32/  179]
train() client id: f_00007-10-1 loss: 0.469407  [   64/  179]
train() client id: f_00007-10-2 loss: 0.510517  [   96/  179]
train() client id: f_00007-10-3 loss: 0.354428  [  128/  179]
train() client id: f_00007-10-4 loss: 0.273337  [  160/  179]
train() client id: f_00007-11-0 loss: 0.410532  [   32/  179]
train() client id: f_00007-11-1 loss: 0.347908  [   64/  179]
train() client id: f_00007-11-2 loss: 0.547361  [   96/  179]
train() client id: f_00007-11-3 loss: 0.437853  [  128/  179]
train() client id: f_00007-11-4 loss: 0.289575  [  160/  179]
train() client id: f_00008-0-0 loss: 0.483849  [   32/  130]
train() client id: f_00008-0-1 loss: 0.637704  [   64/  130]
train() client id: f_00008-0-2 loss: 0.624750  [   96/  130]
train() client id: f_00008-0-3 loss: 0.669078  [  128/  130]
train() client id: f_00008-1-0 loss: 0.616709  [   32/  130]
train() client id: f_00008-1-1 loss: 0.592163  [   64/  130]
train() client id: f_00008-1-2 loss: 0.522941  [   96/  130]
train() client id: f_00008-1-3 loss: 0.672139  [  128/  130]
train() client id: f_00008-2-0 loss: 0.669435  [   32/  130]
train() client id: f_00008-2-1 loss: 0.564623  [   64/  130]
train() client id: f_00008-2-2 loss: 0.487816  [   96/  130]
train() client id: f_00008-2-3 loss: 0.637688  [  128/  130]
train() client id: f_00008-3-0 loss: 0.529606  [   32/  130]
train() client id: f_00008-3-1 loss: 0.727777  [   64/  130]
train() client id: f_00008-3-2 loss: 0.541796  [   96/  130]
train() client id: f_00008-3-3 loss: 0.605748  [  128/  130]
train() client id: f_00008-4-0 loss: 0.516668  [   32/  130]
train() client id: f_00008-4-1 loss: 0.643188  [   64/  130]
train() client id: f_00008-4-2 loss: 0.635613  [   96/  130]
train() client id: f_00008-4-3 loss: 0.570307  [  128/  130]
train() client id: f_00008-5-0 loss: 0.628108  [   32/  130]
train() client id: f_00008-5-1 loss: 0.647760  [   64/  130]
train() client id: f_00008-5-2 loss: 0.481715  [   96/  130]
train() client id: f_00008-5-3 loss: 0.647632  [  128/  130]
train() client id: f_00008-6-0 loss: 0.638152  [   32/  130]
train() client id: f_00008-6-1 loss: 0.616981  [   64/  130]
train() client id: f_00008-6-2 loss: 0.559287  [   96/  130]
train() client id: f_00008-6-3 loss: 0.589595  [  128/  130]
train() client id: f_00008-7-0 loss: 0.659763  [   32/  130]
train() client id: f_00008-7-1 loss: 0.577111  [   64/  130]
train() client id: f_00008-7-2 loss: 0.520564  [   96/  130]
train() client id: f_00008-7-3 loss: 0.620211  [  128/  130]
train() client id: f_00008-8-0 loss: 0.540344  [   32/  130]
train() client id: f_00008-8-1 loss: 0.678028  [   64/  130]
train() client id: f_00008-8-2 loss: 0.564877  [   96/  130]
train() client id: f_00008-8-3 loss: 0.608811  [  128/  130]
train() client id: f_00008-9-0 loss: 0.591004  [   32/  130]
train() client id: f_00008-9-1 loss: 0.508204  [   64/  130]
train() client id: f_00008-9-2 loss: 0.636778  [   96/  130]
train() client id: f_00008-9-3 loss: 0.607545  [  128/  130]
train() client id: f_00008-10-0 loss: 0.526638  [   32/  130]
train() client id: f_00008-10-1 loss: 0.572060  [   64/  130]
train() client id: f_00008-10-2 loss: 0.692613  [   96/  130]
train() client id: f_00008-10-3 loss: 0.611023  [  128/  130]
train() client id: f_00008-11-0 loss: 0.471264  [   32/  130]
train() client id: f_00008-11-1 loss: 0.572004  [   64/  130]
train() client id: f_00008-11-2 loss: 0.645463  [   96/  130]
train() client id: f_00008-11-3 loss: 0.673370  [  128/  130]
train() client id: f_00009-0-0 loss: 1.034484  [   32/  118]
train() client id: f_00009-0-1 loss: 0.920915  [   64/  118]
train() client id: f_00009-0-2 loss: 0.956949  [   96/  118]
train() client id: f_00009-1-0 loss: 0.935753  [   32/  118]
train() client id: f_00009-1-1 loss: 0.871411  [   64/  118]
train() client id: f_00009-1-2 loss: 0.904750  [   96/  118]
train() client id: f_00009-2-0 loss: 0.834300  [   32/  118]
train() client id: f_00009-2-1 loss: 0.962510  [   64/  118]
train() client id: f_00009-2-2 loss: 0.878843  [   96/  118]
train() client id: f_00009-3-0 loss: 0.884447  [   32/  118]
train() client id: f_00009-3-1 loss: 0.825335  [   64/  118]
train() client id: f_00009-3-2 loss: 0.829087  [   96/  118]
train() client id: f_00009-4-0 loss: 0.904166  [   32/  118]
train() client id: f_00009-4-1 loss: 0.777952  [   64/  118]
train() client id: f_00009-4-2 loss: 0.751547  [   96/  118]
train() client id: f_00009-5-0 loss: 0.747911  [   32/  118]
train() client id: f_00009-5-1 loss: 0.820126  [   64/  118]
train() client id: f_00009-5-2 loss: 0.834425  [   96/  118]
train() client id: f_00009-6-0 loss: 0.764108  [   32/  118]
train() client id: f_00009-6-1 loss: 0.756136  [   64/  118]
train() client id: f_00009-6-2 loss: 0.706601  [   96/  118]
train() client id: f_00009-7-0 loss: 0.694855  [   32/  118]
train() client id: f_00009-7-1 loss: 0.711603  [   64/  118]
train() client id: f_00009-7-2 loss: 0.831458  [   96/  118]
train() client id: f_00009-8-0 loss: 0.726114  [   32/  118]
train() client id: f_00009-8-1 loss: 0.791368  [   64/  118]
train() client id: f_00009-8-2 loss: 0.769825  [   96/  118]
train() client id: f_00009-9-0 loss: 0.672286  [   32/  118]
train() client id: f_00009-9-1 loss: 0.717567  [   64/  118]
train() client id: f_00009-9-2 loss: 0.747443  [   96/  118]
train() client id: f_00009-10-0 loss: 0.861988  [   32/  118]
train() client id: f_00009-10-1 loss: 0.532092  [   64/  118]
train() client id: f_00009-10-2 loss: 0.723720  [   96/  118]
train() client id: f_00009-11-0 loss: 0.590010  [   32/  118]
train() client id: f_00009-11-1 loss: 0.631923  [   64/  118]
train() client id: f_00009-11-2 loss: 0.737757  [   96/  118]
At round 22 accuracy: 0.6392572944297082
At round 22 training accuracy: 0.5734406438631791
At round 22 training loss: 0.8396802913637408
update_location
xs = [ -3.9056584    4.20031788 130.00902392  18.81129433   0.97929623
   3.95640986 -92.44319194 -71.32485185 114.66397685 -57.06087855]
ys = [122.5879595  105.55583871   1.32061395 -92.45517586  84.35018685
  67.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [158.24999836 145.46366473 164.02466376 137.48390574 130.82779919
 120.89008322 136.20805511 122.83285695 153.15514213 115.2039744 ]
dists_bs = [178.94215746 192.18544551 350.99150105 330.29545698 197.95479617
 208.60058224 195.87307526 202.69778195 329.6812663  207.72609273]
uav_gains = [3.16755799e-11 3.91571505e-11 2.89252519e-11 4.51054531e-11
 5.10714755e-11 6.22299652e-11 4.61709278e-11 5.97976208e-11
 3.44011166e-11 7.01971242e-11]
bs_gains = [5.44124914e-11 4.45529740e-11 8.25018342e-12 9.78062943e-12
 4.10118354e-11 3.54169077e-11 4.22439780e-11 3.83810539e-11
 9.83173423e-12 3.58359679e-11]
Round 23
-------------------------------
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.95652636 16.54261616  7.83621819  2.81059366 19.08131306  9.18961195
  3.4902559  11.21434588  8.261358    7.45728421]
obj_prev = 93.84012336038582
eta_min = 2.1552750513700054e-12	eta_max = 0.9232017387246043
af = 19.826056636829264	bf = 1.599357197144509	zeta = 21.808662300512193	eta = 0.909090909090909
af = 19.826056636829264	bf = 1.599357197144509	zeta = 38.37880442012573	eta = 0.5165886987983539
af = 19.826056636829264	bf = 1.599357197144509	zeta = 30.397782640654498	eta = 0.6522204882902731
af = 19.826056636829264	bf = 1.599357197144509	zeta = 28.962231155820284	eta = 0.6845486637463356
af = 19.826056636829264	bf = 1.599357197144509	zeta = 28.889857955433257	eta = 0.686263555446129
af = 19.826056636829264	bf = 1.599357197144509	zeta = 28.889660323483184	eta = 0.6862682501224668
eta = 0.6862682501224668
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [0.03106678 0.06533886 0.03057366 0.01060215 0.07544789 0.03599801
 0.01331433 0.04413455 0.03205303 0.02909429]
ene_total = [2.52460799 4.70588087 2.50342372 1.16220666 5.36855605 2.83112977
 1.33541705 3.30578544 2.76961187 2.3830409 ]
ti_comp = [0.36635294 0.37203036 0.36469503 0.37222065 0.37072619 0.36830251
 0.37257852 0.37632559 0.33835818 0.3685025 ]
ti_coms = [0.07942667 0.07374925 0.08108457 0.07355896 0.07505341 0.0774771
 0.07320109 0.06945402 0.10742142 0.07727711]
t_total = [28.84990349 28.84990349 28.84990349 28.84990349 28.84990349 28.84990349
 28.84990349 28.84990349 28.84990349 28.84990349]
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [1.39626984e-05 1.25961536e-04 1.34295805e-05 5.37602806e-07
 1.95305657e-04 2.14934581e-05 1.06268058e-06 3.79392448e-05
 1.79776854e-05 1.13350184e-05]
ene_total = [0.51493605 0.48544134 0.52563117 0.47609264 0.49836942 0.50280622
 0.47381058 0.45194692 0.69637198 0.5008545 ]
optimize_network iter = 0 obj = 5.126260825771548
eta = 0.6862682501224668
freqs = [4.24000684e+07 8.78138843e+07 4.19167432e+07 1.42417608e+07
 1.01756896e+08 4.88701724e+07 1.78678231e+07 5.86387868e+07
 4.73655258e+07 3.94763747e+07]
eta_min = 0.686268250122481	eta_max = 0.6862682501224266
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 0.026043258509324534	eta = 0.9090909090909091
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 17.617751045101738	eta = 0.0013438542463973541
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 1.7582687679572628	eta = 0.01346534158224146
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 1.7245653152193903	eta = 0.013728496882659091
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 1.7245608478782077	eta = 0.013728532445265985
eta = 0.013728532445265985
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [1.59038678e-04 1.43473385e-03 1.52966329e-04 6.12343236e-06
 2.22458099e-03 2.44815942e-04 1.21042015e-05 4.32137624e-04
 2.04770398e-04 1.29108736e-04]
ene_total = [0.16678716 0.18136157 0.17007523 0.15155948 0.20030673 0.16453951
 0.15094588 0.151879   0.22536052 0.16174577]
ti_comp = [0.36635294 0.37203036 0.36469503 0.37222065 0.37072619 0.36830251
 0.37257852 0.37632559 0.33835818 0.3685025 ]
ti_coms = [0.07942667 0.07374925 0.08108457 0.07355896 0.07505341 0.0774771
 0.07320109 0.06945402 0.10742142 0.07727711]
t_total = [28.84990349 28.84990349 28.84990349 28.84990349 28.84990349 28.84990349
 28.84990349 28.84990349 28.84990349 28.84990349]
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [1.39626984e-05 1.25961536e-04 1.34295805e-05 5.37602806e-07
 1.95305657e-04 2.14934581e-05 1.06268058e-06 3.79392448e-05
 1.79776854e-05 1.13350184e-05]
ene_total = [0.51493605 0.48544134 0.52563117 0.47609264 0.49836942 0.50280622
 0.47381058 0.45194692 0.69637198 0.5008545 ]
optimize_network iter = 1 obj = 5.126260825771778
eta = 0.686268250122481
freqs = [4.24000684e+07 8.78138843e+07 4.19167432e+07 1.42417608e+07
 1.01756896e+08 4.88701724e+07 1.78678231e+07 5.86387868e+07
 4.73655258e+07 3.94763747e+07]
Done!
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [1.35911095e-05 1.22609325e-04 1.30721794e-05 5.23295595e-07
 1.90107992e-04 2.09214532e-05 1.03439949e-06 3.69295686e-05
 1.74992457e-05 1.10333598e-05]
ene_total = [0.00795626 0.00749753 0.00812153 0.00735642 0.00769545 0.00776863
 0.00732114 0.00698233 0.01075964 0.00773874]
At round 23 energy consumption: 0.07919768178596374
At round 23 eta: 0.686268250122481
At round 23 a_n: 19.96150252899076
At round 23 local rounds: 12.328087095105714
At round 23 global rounds: 64.71786290003097
gradient difference: 0.4879802167415619
train() client id: f_00000-0-0 loss: 1.423675  [   32/  126]
train() client id: f_00000-0-1 loss: 1.164665  [   64/  126]
train() client id: f_00000-0-2 loss: 1.069602  [   96/  126]
train() client id: f_00000-1-0 loss: 0.924062  [   32/  126]
train() client id: f_00000-1-1 loss: 1.134484  [   64/  126]
train() client id: f_00000-1-2 loss: 1.083892  [   96/  126]
train() client id: f_00000-2-0 loss: 0.982856  [   32/  126]
train() client id: f_00000-2-1 loss: 0.846446  [   64/  126]
train() client id: f_00000-2-2 loss: 1.069681  [   96/  126]
train() client id: f_00000-3-0 loss: 0.982584  [   32/  126]
train() client id: f_00000-3-1 loss: 0.988969  [   64/  126]
train() client id: f_00000-3-2 loss: 0.958405  [   96/  126]
train() client id: f_00000-4-0 loss: 0.990661  [   32/  126]
train() client id: f_00000-4-1 loss: 0.858671  [   64/  126]
train() client id: f_00000-4-2 loss: 0.951023  [   96/  126]
train() client id: f_00000-5-0 loss: 0.921248  [   32/  126]
train() client id: f_00000-5-1 loss: 0.805288  [   64/  126]
train() client id: f_00000-5-2 loss: 0.937176  [   96/  126]
train() client id: f_00000-6-0 loss: 0.867606  [   32/  126]
train() client id: f_00000-6-1 loss: 0.920089  [   64/  126]
train() client id: f_00000-6-2 loss: 0.923744  [   96/  126]
train() client id: f_00000-7-0 loss: 0.869018  [   32/  126]
train() client id: f_00000-7-1 loss: 0.835335  [   64/  126]
train() client id: f_00000-7-2 loss: 0.820687  [   96/  126]
train() client id: f_00000-8-0 loss: 0.909651  [   32/  126]
train() client id: f_00000-8-1 loss: 0.816502  [   64/  126]
train() client id: f_00000-8-2 loss: 1.003189  [   96/  126]
train() client id: f_00000-9-0 loss: 0.883150  [   32/  126]
train() client id: f_00000-9-1 loss: 0.903890  [   64/  126]
train() client id: f_00000-9-2 loss: 0.878786  [   96/  126]
train() client id: f_00000-10-0 loss: 0.827453  [   32/  126]
train() client id: f_00000-10-1 loss: 0.945341  [   64/  126]
train() client id: f_00000-10-2 loss: 0.863700  [   96/  126]
train() client id: f_00000-11-0 loss: 0.864658  [   32/  126]
train() client id: f_00000-11-1 loss: 0.865693  [   64/  126]
train() client id: f_00000-11-2 loss: 0.811157  [   96/  126]
train() client id: f_00001-0-0 loss: 0.459454  [   32/  265]
train() client id: f_00001-0-1 loss: 0.456449  [   64/  265]
train() client id: f_00001-0-2 loss: 0.510964  [   96/  265]
train() client id: f_00001-0-3 loss: 0.445560  [  128/  265]
train() client id: f_00001-0-4 loss: 0.365838  [  160/  265]
train() client id: f_00001-0-5 loss: 0.494890  [  192/  265]
train() client id: f_00001-0-6 loss: 0.399840  [  224/  265]
train() client id: f_00001-0-7 loss: 0.518168  [  256/  265]
train() client id: f_00001-1-0 loss: 0.509928  [   32/  265]
train() client id: f_00001-1-1 loss: 0.377519  [   64/  265]
train() client id: f_00001-1-2 loss: 0.465247  [   96/  265]
train() client id: f_00001-1-3 loss: 0.449268  [  128/  265]
train() client id: f_00001-1-4 loss: 0.450175  [  160/  265]
train() client id: f_00001-1-5 loss: 0.439950  [  192/  265]
train() client id: f_00001-1-6 loss: 0.395931  [  224/  265]
train() client id: f_00001-1-7 loss: 0.501944  [  256/  265]
train() client id: f_00001-2-0 loss: 0.433010  [   32/  265]
train() client id: f_00001-2-1 loss: 0.380259  [   64/  265]
train() client id: f_00001-2-2 loss: 0.354789  [   96/  265]
train() client id: f_00001-2-3 loss: 0.412051  [  128/  265]
train() client id: f_00001-2-4 loss: 0.631984  [  160/  265]
train() client id: f_00001-2-5 loss: 0.485727  [  192/  265]
train() client id: f_00001-2-6 loss: 0.367524  [  224/  265]
train() client id: f_00001-2-7 loss: 0.408891  [  256/  265]
train() client id: f_00001-3-0 loss: 0.431973  [   32/  265]
train() client id: f_00001-3-1 loss: 0.378055  [   64/  265]
train() client id: f_00001-3-2 loss: 0.422778  [   96/  265]
train() client id: f_00001-3-3 loss: 0.473650  [  128/  265]
train() client id: f_00001-3-4 loss: 0.424768  [  160/  265]
train() client id: f_00001-3-5 loss: 0.597853  [  192/  265]
train() client id: f_00001-3-6 loss: 0.365445  [  224/  265]
train() client id: f_00001-3-7 loss: 0.403524  [  256/  265]
train() client id: f_00001-4-0 loss: 0.491983  [   32/  265]
train() client id: f_00001-4-1 loss: 0.452099  [   64/  265]
train() client id: f_00001-4-2 loss: 0.483592  [   96/  265]
train() client id: f_00001-4-3 loss: 0.392023  [  128/  265]
train() client id: f_00001-4-4 loss: 0.414692  [  160/  265]
train() client id: f_00001-4-5 loss: 0.371328  [  192/  265]
train() client id: f_00001-4-6 loss: 0.444214  [  224/  265]
train() client id: f_00001-4-7 loss: 0.471954  [  256/  265]
train() client id: f_00001-5-0 loss: 0.434850  [   32/  265]
train() client id: f_00001-5-1 loss: 0.399262  [   64/  265]
train() client id: f_00001-5-2 loss: 0.432098  [   96/  265]
train() client id: f_00001-5-3 loss: 0.610175  [  128/  265]
train() client id: f_00001-5-4 loss: 0.521736  [  160/  265]
train() client id: f_00001-5-5 loss: 0.417606  [  192/  265]
train() client id: f_00001-5-6 loss: 0.335827  [  224/  265]
train() client id: f_00001-5-7 loss: 0.349011  [  256/  265]
train() client id: f_00001-6-0 loss: 0.485512  [   32/  265]
train() client id: f_00001-6-1 loss: 0.425396  [   64/  265]
train() client id: f_00001-6-2 loss: 0.537233  [   96/  265]
train() client id: f_00001-6-3 loss: 0.473388  [  128/  265]
train() client id: f_00001-6-4 loss: 0.392233  [  160/  265]
train() client id: f_00001-6-5 loss: 0.383518  [  192/  265]
train() client id: f_00001-6-6 loss: 0.348183  [  224/  265]
train() client id: f_00001-6-7 loss: 0.441737  [  256/  265]
train() client id: f_00001-7-0 loss: 0.327341  [   32/  265]
train() client id: f_00001-7-1 loss: 0.391506  [   64/  265]
train() client id: f_00001-7-2 loss: 0.339689  [   96/  265]
train() client id: f_00001-7-3 loss: 0.606682  [  128/  265]
train() client id: f_00001-7-4 loss: 0.426303  [  160/  265]
train() client id: f_00001-7-5 loss: 0.504351  [  192/  265]
train() client id: f_00001-7-6 loss: 0.414939  [  224/  265]
train() client id: f_00001-7-7 loss: 0.467622  [  256/  265]
train() client id: f_00001-8-0 loss: 0.371160  [   32/  265]
train() client id: f_00001-8-1 loss: 0.396591  [   64/  265]
train() client id: f_00001-8-2 loss: 0.361586  [   96/  265]
train() client id: f_00001-8-3 loss: 0.480779  [  128/  265]
train() client id: f_00001-8-4 loss: 0.434711  [  160/  265]
train() client id: f_00001-8-5 loss: 0.478129  [  192/  265]
train() client id: f_00001-8-6 loss: 0.343405  [  224/  265]
train() client id: f_00001-8-7 loss: 0.608973  [  256/  265]
train() client id: f_00001-9-0 loss: 0.414139  [   32/  265]
train() client id: f_00001-9-1 loss: 0.320426  [   64/  265]
train() client id: f_00001-9-2 loss: 0.456850  [   96/  265]
train() client id: f_00001-9-3 loss: 0.454489  [  128/  265]
train() client id: f_00001-9-4 loss: 0.455247  [  160/  265]
train() client id: f_00001-9-5 loss: 0.350314  [  192/  265]
train() client id: f_00001-9-6 loss: 0.414000  [  224/  265]
train() client id: f_00001-9-7 loss: 0.479764  [  256/  265]
train() client id: f_00001-10-0 loss: 0.400211  [   32/  265]
train() client id: f_00001-10-1 loss: 0.492653  [   64/  265]
train() client id: f_00001-10-2 loss: 0.644020  [   96/  265]
train() client id: f_00001-10-3 loss: 0.339194  [  128/  265]
train() client id: f_00001-10-4 loss: 0.348494  [  160/  265]
train() client id: f_00001-10-5 loss: 0.344697  [  192/  265]
train() client id: f_00001-10-6 loss: 0.420030  [  224/  265]
train() client id: f_00001-10-7 loss: 0.445131  [  256/  265]
train() client id: f_00001-11-0 loss: 0.512440  [   32/  265]
train() client id: f_00001-11-1 loss: 0.470165  [   64/  265]
train() client id: f_00001-11-2 loss: 0.335800  [   96/  265]
train() client id: f_00001-11-3 loss: 0.440770  [  128/  265]
train() client id: f_00001-11-4 loss: 0.407444  [  160/  265]
train() client id: f_00001-11-5 loss: 0.427305  [  192/  265]
train() client id: f_00001-11-6 loss: 0.422039  [  224/  265]
train() client id: f_00001-11-7 loss: 0.391758  [  256/  265]
train() client id: f_00002-0-0 loss: 1.168883  [   32/  124]
train() client id: f_00002-0-1 loss: 1.119518  [   64/  124]
train() client id: f_00002-0-2 loss: 0.996068  [   96/  124]
train() client id: f_00002-1-0 loss: 1.138296  [   32/  124]
train() client id: f_00002-1-1 loss: 1.029791  [   64/  124]
train() client id: f_00002-1-2 loss: 1.079769  [   96/  124]
train() client id: f_00002-2-0 loss: 1.043027  [   32/  124]
train() client id: f_00002-2-1 loss: 0.851643  [   64/  124]
train() client id: f_00002-2-2 loss: 1.163583  [   96/  124]
train() client id: f_00002-3-0 loss: 0.973685  [   32/  124]
train() client id: f_00002-3-1 loss: 0.968326  [   64/  124]
train() client id: f_00002-3-2 loss: 1.191280  [   96/  124]
train() client id: f_00002-4-0 loss: 0.986236  [   32/  124]
train() client id: f_00002-4-1 loss: 0.985646  [   64/  124]
train() client id: f_00002-4-2 loss: 0.963709  [   96/  124]
train() client id: f_00002-5-0 loss: 0.900863  [   32/  124]
train() client id: f_00002-5-1 loss: 1.007806  [   64/  124]
train() client id: f_00002-5-2 loss: 1.079752  [   96/  124]
train() client id: f_00002-6-0 loss: 0.948092  [   32/  124]
train() client id: f_00002-6-1 loss: 0.965158  [   64/  124]
train() client id: f_00002-6-2 loss: 0.993187  [   96/  124]
train() client id: f_00002-7-0 loss: 1.010747  [   32/  124]
train() client id: f_00002-7-1 loss: 0.888386  [   64/  124]
train() client id: f_00002-7-2 loss: 1.047658  [   96/  124]
train() client id: f_00002-8-0 loss: 0.879201  [   32/  124]
train() client id: f_00002-8-1 loss: 0.934285  [   64/  124]
train() client id: f_00002-8-2 loss: 1.082797  [   96/  124]
train() client id: f_00002-9-0 loss: 0.963195  [   32/  124]
train() client id: f_00002-9-1 loss: 0.961223  [   64/  124]
train() client id: f_00002-9-2 loss: 0.903562  [   96/  124]
train() client id: f_00002-10-0 loss: 0.916920  [   32/  124]
train() client id: f_00002-10-1 loss: 0.868263  [   64/  124]
train() client id: f_00002-10-2 loss: 1.034592  [   96/  124]
train() client id: f_00002-11-0 loss: 0.808998  [   32/  124]
train() client id: f_00002-11-1 loss: 1.028456  [   64/  124]
train() client id: f_00002-11-2 loss: 0.912517  [   96/  124]
train() client id: f_00003-0-0 loss: 1.001490  [   32/   43]
train() client id: f_00003-1-0 loss: 0.973274  [   32/   43]
train() client id: f_00003-2-0 loss: 0.866558  [   32/   43]
train() client id: f_00003-3-0 loss: 0.714945  [   32/   43]
train() client id: f_00003-4-0 loss: 0.855459  [   32/   43]
train() client id: f_00003-5-0 loss: 0.799232  [   32/   43]
train() client id: f_00003-6-0 loss: 0.747485  [   32/   43]
train() client id: f_00003-7-0 loss: 0.970376  [   32/   43]
train() client id: f_00003-8-0 loss: 0.751806  [   32/   43]
train() client id: f_00003-9-0 loss: 0.842915  [   32/   43]
train() client id: f_00003-10-0 loss: 0.908984  [   32/   43]
train() client id: f_00003-11-0 loss: 0.766107  [   32/   43]
train() client id: f_00004-0-0 loss: 1.060856  [   32/  306]
train() client id: f_00004-0-1 loss: 0.934212  [   64/  306]
train() client id: f_00004-0-2 loss: 0.961214  [   96/  306]
train() client id: f_00004-0-3 loss: 1.021595  [  128/  306]
train() client id: f_00004-0-4 loss: 0.964168  [  160/  306]
train() client id: f_00004-0-5 loss: 0.881320  [  192/  306]
train() client id: f_00004-0-6 loss: 0.909687  [  224/  306]
train() client id: f_00004-0-7 loss: 0.887586  [  256/  306]
train() client id: f_00004-0-8 loss: 0.974848  [  288/  306]
train() client id: f_00004-1-0 loss: 0.957797  [   32/  306]
train() client id: f_00004-1-1 loss: 1.056078  [   64/  306]
train() client id: f_00004-1-2 loss: 1.054044  [   96/  306]
train() client id: f_00004-1-3 loss: 0.953634  [  128/  306]
train() client id: f_00004-1-4 loss: 0.815481  [  160/  306]
train() client id: f_00004-1-5 loss: 1.015802  [  192/  306]
train() client id: f_00004-1-6 loss: 0.855806  [  224/  306]
train() client id: f_00004-1-7 loss: 0.951600  [  256/  306]
train() client id: f_00004-1-8 loss: 0.894894  [  288/  306]
train() client id: f_00004-2-0 loss: 1.049605  [   32/  306]
train() client id: f_00004-2-1 loss: 0.970371  [   64/  306]
train() client id: f_00004-2-2 loss: 0.872942  [   96/  306]
train() client id: f_00004-2-3 loss: 0.966348  [  128/  306]
train() client id: f_00004-2-4 loss: 0.968445  [  160/  306]
train() client id: f_00004-2-5 loss: 0.961371  [  192/  306]
train() client id: f_00004-2-6 loss: 0.905465  [  224/  306]
train() client id: f_00004-2-7 loss: 0.934180  [  256/  306]
train() client id: f_00004-2-8 loss: 0.994231  [  288/  306]
train() client id: f_00004-3-0 loss: 0.830600  [   32/  306]
train() client id: f_00004-3-1 loss: 0.982501  [   64/  306]
train() client id: f_00004-3-2 loss: 0.978581  [   96/  306]
train() client id: f_00004-3-3 loss: 0.949899  [  128/  306]
train() client id: f_00004-3-4 loss: 1.011687  [  160/  306]
train() client id: f_00004-3-5 loss: 1.004382  [  192/  306]
train() client id: f_00004-3-6 loss: 0.979880  [  224/  306]
train() client id: f_00004-3-7 loss: 1.033429  [  256/  306]
train() client id: f_00004-3-8 loss: 0.774638  [  288/  306]
train() client id: f_00004-4-0 loss: 1.055298  [   32/  306]
train() client id: f_00004-4-1 loss: 0.965842  [   64/  306]
train() client id: f_00004-4-2 loss: 0.945137  [   96/  306]
train() client id: f_00004-4-3 loss: 0.864151  [  128/  306]
train() client id: f_00004-4-4 loss: 0.823214  [  160/  306]
train() client id: f_00004-4-5 loss: 0.945171  [  192/  306]
train() client id: f_00004-4-6 loss: 0.968680  [  224/  306]
train() client id: f_00004-4-7 loss: 0.969974  [  256/  306]
train() client id: f_00004-4-8 loss: 0.854747  [  288/  306]
train() client id: f_00004-5-0 loss: 0.868650  [   32/  306]
train() client id: f_00004-5-1 loss: 1.079710  [   64/  306]
train() client id: f_00004-5-2 loss: 0.872956  [   96/  306]
train() client id: f_00004-5-3 loss: 0.864016  [  128/  306]
train() client id: f_00004-5-4 loss: 0.945556  [  160/  306]
train() client id: f_00004-5-5 loss: 0.913210  [  192/  306]
train() client id: f_00004-5-6 loss: 1.080767  [  224/  306]
train() client id: f_00004-5-7 loss: 0.985147  [  256/  306]
train() client id: f_00004-5-8 loss: 0.980615  [  288/  306]
train() client id: f_00004-6-0 loss: 0.953114  [   32/  306]
train() client id: f_00004-6-1 loss: 0.977079  [   64/  306]
train() client id: f_00004-6-2 loss: 0.851853  [   96/  306]
train() client id: f_00004-6-3 loss: 1.007539  [  128/  306]
train() client id: f_00004-6-4 loss: 1.050491  [  160/  306]
train() client id: f_00004-6-5 loss: 0.926997  [  192/  306]
train() client id: f_00004-6-6 loss: 0.951865  [  224/  306]
train() client id: f_00004-6-7 loss: 0.860747  [  256/  306]
train() client id: f_00004-6-8 loss: 0.937321  [  288/  306]
train() client id: f_00004-7-0 loss: 0.973078  [   32/  306]
train() client id: f_00004-7-1 loss: 0.943916  [   64/  306]
train() client id: f_00004-7-2 loss: 0.976530  [   96/  306]
train() client id: f_00004-7-3 loss: 0.993091  [  128/  306]
train() client id: f_00004-7-4 loss: 0.792246  [  160/  306]
train() client id: f_00004-7-5 loss: 1.021177  [  192/  306]
train() client id: f_00004-7-6 loss: 0.936311  [  224/  306]
train() client id: f_00004-7-7 loss: 0.958897  [  256/  306]
train() client id: f_00004-7-8 loss: 0.912200  [  288/  306]
train() client id: f_00004-8-0 loss: 0.982119  [   32/  306]
train() client id: f_00004-8-1 loss: 0.919388  [   64/  306]
train() client id: f_00004-8-2 loss: 0.990070  [   96/  306]
train() client id: f_00004-8-3 loss: 1.019340  [  128/  306]
train() client id: f_00004-8-4 loss: 0.849183  [  160/  306]
train() client id: f_00004-8-5 loss: 0.988448  [  192/  306]
train() client id: f_00004-8-6 loss: 0.824568  [  224/  306]
train() client id: f_00004-8-7 loss: 1.018771  [  256/  306]
train() client id: f_00004-8-8 loss: 0.927731  [  288/  306]
train() client id: f_00004-9-0 loss: 0.969820  [   32/  306]
train() client id: f_00004-9-1 loss: 1.006361  [   64/  306]
train() client id: f_00004-9-2 loss: 0.978625  [   96/  306]
train() client id: f_00004-9-3 loss: 0.918238  [  128/  306]
train() client id: f_00004-9-4 loss: 0.927246  [  160/  306]
train() client id: f_00004-9-5 loss: 0.779108  [  192/  306]
train() client id: f_00004-9-6 loss: 0.955747  [  224/  306]
train() client id: f_00004-9-7 loss: 0.995290  [  256/  306]
train() client id: f_00004-9-8 loss: 0.882033  [  288/  306]
train() client id: f_00004-10-0 loss: 1.024790  [   32/  306]
train() client id: f_00004-10-1 loss: 0.917882  [   64/  306]
train() client id: f_00004-10-2 loss: 0.918513  [   96/  306]
train() client id: f_00004-10-3 loss: 0.900383  [  128/  306]
train() client id: f_00004-10-4 loss: 0.910363  [  160/  306]
train() client id: f_00004-10-5 loss: 0.932750  [  192/  306]
train() client id: f_00004-10-6 loss: 0.890075  [  224/  306]
train() client id: f_00004-10-7 loss: 1.000408  [  256/  306]
train() client id: f_00004-10-8 loss: 0.974988  [  288/  306]
train() client id: f_00004-11-0 loss: 1.011901  [   32/  306]
train() client id: f_00004-11-1 loss: 0.897561  [   64/  306]
train() client id: f_00004-11-2 loss: 0.860747  [   96/  306]
train() client id: f_00004-11-3 loss: 0.838899  [  128/  306]
train() client id: f_00004-11-4 loss: 1.029859  [  160/  306]
train() client id: f_00004-11-5 loss: 0.963503  [  192/  306]
train() client id: f_00004-11-6 loss: 0.979806  [  224/  306]
train() client id: f_00004-11-7 loss: 0.907816  [  256/  306]
train() client id: f_00004-11-8 loss: 0.874585  [  288/  306]
train() client id: f_00005-0-0 loss: 0.677048  [   32/  146]
train() client id: f_00005-0-1 loss: 0.591528  [   64/  146]
train() client id: f_00005-0-2 loss: 0.749643  [   96/  146]
train() client id: f_00005-0-3 loss: 1.097852  [  128/  146]
train() client id: f_00005-1-0 loss: 0.701492  [   32/  146]
train() client id: f_00005-1-1 loss: 0.935750  [   64/  146]
train() client id: f_00005-1-2 loss: 0.691726  [   96/  146]
train() client id: f_00005-1-3 loss: 0.841268  [  128/  146]
train() client id: f_00005-2-0 loss: 0.528414  [   32/  146]
train() client id: f_00005-2-1 loss: 0.931178  [   64/  146]
train() client id: f_00005-2-2 loss: 0.837537  [   96/  146]
train() client id: f_00005-2-3 loss: 0.857220  [  128/  146]
train() client id: f_00005-3-0 loss: 0.732141  [   32/  146]
train() client id: f_00005-3-1 loss: 0.621177  [   64/  146]
train() client id: f_00005-3-2 loss: 0.639168  [   96/  146]
train() client id: f_00005-3-3 loss: 0.921874  [  128/  146]
train() client id: f_00005-4-0 loss: 1.027948  [   32/  146]
train() client id: f_00005-4-1 loss: 0.682693  [   64/  146]
train() client id: f_00005-4-2 loss: 0.774208  [   96/  146]
train() client id: f_00005-4-3 loss: 0.572724  [  128/  146]
train() client id: f_00005-5-0 loss: 1.073280  [   32/  146]
train() client id: f_00005-5-1 loss: 0.528887  [   64/  146]
train() client id: f_00005-5-2 loss: 0.744412  [   96/  146]
train() client id: f_00005-5-3 loss: 0.768253  [  128/  146]
train() client id: f_00005-6-0 loss: 0.658781  [   32/  146]
train() client id: f_00005-6-1 loss: 0.722973  [   64/  146]
train() client id: f_00005-6-2 loss: 0.895971  [   96/  146]
train() client id: f_00005-6-3 loss: 0.668485  [  128/  146]
train() client id: f_00005-7-0 loss: 0.912509  [   32/  146]
train() client id: f_00005-7-1 loss: 0.754620  [   64/  146]
train() client id: f_00005-7-2 loss: 0.690990  [   96/  146]
train() client id: f_00005-7-3 loss: 0.610262  [  128/  146]
train() client id: f_00005-8-0 loss: 0.474563  [   32/  146]
train() client id: f_00005-8-1 loss: 0.758586  [   64/  146]
train() client id: f_00005-8-2 loss: 0.675266  [   96/  146]
train() client id: f_00005-8-3 loss: 1.125235  [  128/  146]
train() client id: f_00005-9-0 loss: 0.494176  [   32/  146]
train() client id: f_00005-9-1 loss: 0.787121  [   64/  146]
train() client id: f_00005-9-2 loss: 0.855800  [   96/  146]
train() client id: f_00005-9-3 loss: 0.749226  [  128/  146]
train() client id: f_00005-10-0 loss: 0.911969  [   32/  146]
train() client id: f_00005-10-1 loss: 0.641924  [   64/  146]
train() client id: f_00005-10-2 loss: 0.648487  [   96/  146]
train() client id: f_00005-10-3 loss: 0.726357  [  128/  146]
train() client id: f_00005-11-0 loss: 0.871152  [   32/  146]
train() client id: f_00005-11-1 loss: 0.570271  [   64/  146]
train() client id: f_00005-11-2 loss: 0.693865  [   96/  146]
train() client id: f_00005-11-3 loss: 0.691595  [  128/  146]
train() client id: f_00006-0-0 loss: 0.596767  [   32/   54]
train() client id: f_00006-1-0 loss: 0.586890  [   32/   54]
train() client id: f_00006-2-0 loss: 0.557050  [   32/   54]
train() client id: f_00006-3-0 loss: 0.570167  [   32/   54]
train() client id: f_00006-4-0 loss: 0.542466  [   32/   54]
train() client id: f_00006-5-0 loss: 0.491519  [   32/   54]
train() client id: f_00006-6-0 loss: 0.579796  [   32/   54]
train() client id: f_00006-7-0 loss: 0.486043  [   32/   54]
train() client id: f_00006-8-0 loss: 0.589672  [   32/   54]
train() client id: f_00006-9-0 loss: 0.596042  [   32/   54]
train() client id: f_00006-10-0 loss: 0.591204  [   32/   54]
train() client id: f_00006-11-0 loss: 0.523348  [   32/   54]
train() client id: f_00007-0-0 loss: 0.743113  [   32/  179]
train() client id: f_00007-0-1 loss: 0.644965  [   64/  179]
train() client id: f_00007-0-2 loss: 0.707323  [   96/  179]
train() client id: f_00007-0-3 loss: 0.857429  [  128/  179]
train() client id: f_00007-0-4 loss: 0.672545  [  160/  179]
train() client id: f_00007-1-0 loss: 0.679657  [   32/  179]
train() client id: f_00007-1-1 loss: 0.759649  [   64/  179]
train() client id: f_00007-1-2 loss: 0.818665  [   96/  179]
train() client id: f_00007-1-3 loss: 0.723823  [  128/  179]
train() client id: f_00007-1-4 loss: 0.642590  [  160/  179]
train() client id: f_00007-2-0 loss: 0.706063  [   32/  179]
train() client id: f_00007-2-1 loss: 0.784682  [   64/  179]
train() client id: f_00007-2-2 loss: 0.602482  [   96/  179]
train() client id: f_00007-2-3 loss: 0.693604  [  128/  179]
train() client id: f_00007-2-4 loss: 0.693366  [  160/  179]
train() client id: f_00007-3-0 loss: 0.667081  [   32/  179]
train() client id: f_00007-3-1 loss: 0.867378  [   64/  179]
train() client id: f_00007-3-2 loss: 0.728752  [   96/  179]
train() client id: f_00007-3-3 loss: 0.638048  [  128/  179]
train() client id: f_00007-3-4 loss: 0.677169  [  160/  179]
train() client id: f_00007-4-0 loss: 0.845147  [   32/  179]
train() client id: f_00007-4-1 loss: 0.653901  [   64/  179]
train() client id: f_00007-4-2 loss: 0.661132  [   96/  179]
train() client id: f_00007-4-3 loss: 0.696830  [  128/  179]
train() client id: f_00007-4-4 loss: 0.614396  [  160/  179]
train() client id: f_00007-5-0 loss: 0.597168  [   32/  179]
train() client id: f_00007-5-1 loss: 0.722998  [   64/  179]
train() client id: f_00007-5-2 loss: 0.635234  [   96/  179]
train() client id: f_00007-5-3 loss: 0.840443  [  128/  179]
train() client id: f_00007-5-4 loss: 0.648816  [  160/  179]
train() client id: f_00007-6-0 loss: 0.621364  [   32/  179]
train() client id: f_00007-6-1 loss: 0.677620  [   64/  179]
train() client id: f_00007-6-2 loss: 0.673639  [   96/  179]
train() client id: f_00007-6-3 loss: 0.939872  [  128/  179]
train() client id: f_00007-6-4 loss: 0.641505  [  160/  179]
train() client id: f_00007-7-0 loss: 0.542040  [   32/  179]
train() client id: f_00007-7-1 loss: 0.753191  [   64/  179]
train() client id: f_00007-7-2 loss: 0.866757  [   96/  179]
train() client id: f_00007-7-3 loss: 0.743299  [  128/  179]
train() client id: f_00007-7-4 loss: 0.668307  [  160/  179]
train() client id: f_00007-8-0 loss: 0.744268  [   32/  179]
train() client id: f_00007-8-1 loss: 0.672846  [   64/  179]
train() client id: f_00007-8-2 loss: 0.816616  [   96/  179]
train() client id: f_00007-8-3 loss: 0.592932  [  128/  179]
train() client id: f_00007-8-4 loss: 0.646789  [  160/  179]
train() client id: f_00007-9-0 loss: 0.644690  [   32/  179]
train() client id: f_00007-9-1 loss: 0.890021  [   64/  179]
train() client id: f_00007-9-2 loss: 0.642714  [   96/  179]
train() client id: f_00007-9-3 loss: 0.708419  [  128/  179]
train() client id: f_00007-9-4 loss: 0.694100  [  160/  179]
train() client id: f_00007-10-0 loss: 0.753749  [   32/  179]
train() client id: f_00007-10-1 loss: 0.754328  [   64/  179]
train() client id: f_00007-10-2 loss: 0.556543  [   96/  179]
train() client id: f_00007-10-3 loss: 0.541688  [  128/  179]
train() client id: f_00007-10-4 loss: 0.868357  [  160/  179]
train() client id: f_00007-11-0 loss: 0.750056  [   32/  179]
train() client id: f_00007-11-1 loss: 0.625708  [   64/  179]
train() client id: f_00007-11-2 loss: 0.638505  [   96/  179]
train() client id: f_00007-11-3 loss: 0.852764  [  128/  179]
train() client id: f_00007-11-4 loss: 0.605807  [  160/  179]
train() client id: f_00008-0-0 loss: 0.724550  [   32/  130]
train() client id: f_00008-0-1 loss: 0.707168  [   64/  130]
train() client id: f_00008-0-2 loss: 0.656533  [   96/  130]
train() client id: f_00008-0-3 loss: 0.620393  [  128/  130]
train() client id: f_00008-1-0 loss: 0.669086  [   32/  130]
train() client id: f_00008-1-1 loss: 0.736299  [   64/  130]
train() client id: f_00008-1-2 loss: 0.657456  [   96/  130]
train() client id: f_00008-1-3 loss: 0.642225  [  128/  130]
train() client id: f_00008-2-0 loss: 0.695052  [   32/  130]
train() client id: f_00008-2-1 loss: 0.688883  [   64/  130]
train() client id: f_00008-2-2 loss: 0.585258  [   96/  130]
train() client id: f_00008-2-3 loss: 0.742222  [  128/  130]
train() client id: f_00008-3-0 loss: 0.727184  [   32/  130]
train() client id: f_00008-3-1 loss: 0.649321  [   64/  130]
train() client id: f_00008-3-2 loss: 0.601171  [   96/  130]
train() client id: f_00008-3-3 loss: 0.718045  [  128/  130]
train() client id: f_00008-4-0 loss: 0.654912  [   32/  130]
train() client id: f_00008-4-1 loss: 0.678546  [   64/  130]
train() client id: f_00008-4-2 loss: 0.656190  [   96/  130]
train() client id: f_00008-4-3 loss: 0.710753  [  128/  130]
train() client id: f_00008-5-0 loss: 0.646338  [   32/  130]
train() client id: f_00008-5-1 loss: 0.571372  [   64/  130]
train() client id: f_00008-5-2 loss: 0.577990  [   96/  130]
train() client id: f_00008-5-3 loss: 0.844369  [  128/  130]
train() client id: f_00008-6-0 loss: 0.701229  [   32/  130]
train() client id: f_00008-6-1 loss: 0.658301  [   64/  130]
train() client id: f_00008-6-2 loss: 0.741673  [   96/  130]
train() client id: f_00008-6-3 loss: 0.605837  [  128/  130]
train() client id: f_00008-7-0 loss: 0.575556  [   32/  130]
train() client id: f_00008-7-1 loss: 0.582794  [   64/  130]
train() client id: f_00008-7-2 loss: 0.788140  [   96/  130]
train() client id: f_00008-7-3 loss: 0.764477  [  128/  130]
train() client id: f_00008-8-0 loss: 0.715187  [   32/  130]
train() client id: f_00008-8-1 loss: 0.577278  [   64/  130]
train() client id: f_00008-8-2 loss: 0.724674  [   96/  130]
train() client id: f_00008-8-3 loss: 0.682884  [  128/  130]
train() client id: f_00008-9-0 loss: 0.735573  [   32/  130]
train() client id: f_00008-9-1 loss: 0.647360  [   64/  130]
train() client id: f_00008-9-2 loss: 0.651770  [   96/  130]
train() client id: f_00008-9-3 loss: 0.677283  [  128/  130]
train() client id: f_00008-10-0 loss: 0.648241  [   32/  130]
train() client id: f_00008-10-1 loss: 0.608027  [   64/  130]
train() client id: f_00008-10-2 loss: 0.728819  [   96/  130]
train() client id: f_00008-10-3 loss: 0.731717  [  128/  130]
train() client id: f_00008-11-0 loss: 0.649310  [   32/  130]
train() client id: f_00008-11-1 loss: 0.687237  [   64/  130]
train() client id: f_00008-11-2 loss: 0.678248  [   96/  130]
train() client id: f_00008-11-3 loss: 0.653724  [  128/  130]
train() client id: f_00009-0-0 loss: 1.084445  [   32/  118]
train() client id: f_00009-0-1 loss: 1.132754  [   64/  118]
train() client id: f_00009-0-2 loss: 0.927056  [   96/  118]
train() client id: f_00009-1-0 loss: 0.933579  [   32/  118]
train() client id: f_00009-1-1 loss: 0.957998  [   64/  118]
train() client id: f_00009-1-2 loss: 0.973457  [   96/  118]
train() client id: f_00009-2-0 loss: 0.895975  [   32/  118]
train() client id: f_00009-2-1 loss: 0.923567  [   64/  118]
train() client id: f_00009-2-2 loss: 1.075382  [   96/  118]
train() client id: f_00009-3-0 loss: 0.962341  [   32/  118]
train() client id: f_00009-3-1 loss: 0.919700  [   64/  118]
train() client id: f_00009-3-2 loss: 0.900841  [   96/  118]
train() client id: f_00009-4-0 loss: 0.859838  [   32/  118]
train() client id: f_00009-4-1 loss: 0.819057  [   64/  118]
train() client id: f_00009-4-2 loss: 0.941665  [   96/  118]
train() client id: f_00009-5-0 loss: 0.803481  [   32/  118]
train() client id: f_00009-5-1 loss: 1.034208  [   64/  118]
train() client id: f_00009-5-2 loss: 0.808039  [   96/  118]
train() client id: f_00009-6-0 loss: 0.708306  [   32/  118]
train() client id: f_00009-6-1 loss: 0.904296  [   64/  118]
train() client id: f_00009-6-2 loss: 0.952913  [   96/  118]
train() client id: f_00009-7-0 loss: 0.758546  [   32/  118]
train() client id: f_00009-7-1 loss: 0.836812  [   64/  118]
train() client id: f_00009-7-2 loss: 0.889354  [   96/  118]
train() client id: f_00009-8-0 loss: 0.874314  [   32/  118]
train() client id: f_00009-8-1 loss: 0.800534  [   64/  118]
train() client id: f_00009-8-2 loss: 0.796165  [   96/  118]
train() client id: f_00009-9-0 loss: 0.839017  [   32/  118]
train() client id: f_00009-9-1 loss: 0.757341  [   64/  118]
train() client id: f_00009-9-2 loss: 0.790121  [   96/  118]
train() client id: f_00009-10-0 loss: 0.764512  [   32/  118]
train() client id: f_00009-10-1 loss: 0.752156  [   64/  118]
train() client id: f_00009-10-2 loss: 0.865060  [   96/  118]
train() client id: f_00009-11-0 loss: 0.722836  [   32/  118]
train() client id: f_00009-11-1 loss: 0.763842  [   64/  118]
train() client id: f_00009-11-2 loss: 1.035936  [   96/  118]
At round 23 accuracy: 0.6419098143236074
At round 23 training accuracy: 0.5754527162977867
At round 23 training loss: 0.8428618644553859
update_location
xs = [ -3.9056584    4.20031788 135.00902392  18.81129433   0.97929623
   3.95640986 -97.44319194 -76.32485185 119.66397685 -62.06087855]
ys = [127.5879595  110.55583871   1.32061395 -97.45517586  89.35018685
  72.81415074  -2.62498432   0.82234798  17.56900603   4.00148178]
dists_uav = [162.15406741 149.13160679 168.01541763 140.89491153 134.10598387
 123.76410517 139.64979842 125.80206384 156.93354431 117.76062374]
dists_bs = [177.54203815 190.43582607 355.34507742 334.35705565 195.71561817
 206.07606464 193.81871306 200.18651135 334.08288959 204.92861778]
uav_gains = [2.97801922e-11 3.67836040e-11 2.72058983e-11 4.24194154e-11
 4.80041803e-11 5.86786623e-11 4.33735009e-11 5.63299134e-11
 3.23510419e-11 6.64482197e-11]
bs_gains = [5.56225278e-11 4.57085931e-11 7.97027370e-12 9.45158637e-12
 4.23392081e-11 3.66451867e-11 4.35097021e-11 3.97444612e-11
 9.47332053e-12 3.72226060e-11]
Round 24
-------------------------------
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.82454946 16.26273367  7.70634767  2.76510758 18.75837368  9.03341251
  3.43331975 11.02682771  8.12439108  7.33018602]
obj_prev = 92.26524913835074
eta_min = 1.3786631317009973e-12	eta_max = 0.9235436873576162
af = 19.491574900134562	bf = 1.580577038913744	zeta = 21.44073239014802	eta = 0.909090909090909
af = 19.491574900134562	bf = 1.580577038913744	zeta = 37.821548020465336	eta = 0.5153563489677266
af = 19.491574900134562	bf = 1.580577038913744	zeta = 29.921938016706985	eta = 0.65141418611493
af = 19.491574900134562	bf = 1.580577038913744	zeta = 28.500452198585428	eta = 0.6839040575328834
af = 19.491574900134562	bf = 1.580577038913744	zeta = 28.428583728430333	eta = 0.6856329912995908
af = 19.491574900134562	bf = 1.580577038913744	zeta = 28.428386266155645	eta = 0.685637753675084
eta = 0.685637753675084
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [0.03114263 0.06549838 0.0306483  0.01062804 0.07563209 0.0360859
 0.01334684 0.0442423  0.03213128 0.02916532]
ene_total = [2.48895914 4.62483507 2.46833909 1.14803217 5.27587599 2.77970132
 1.31845091 3.25561899 2.72998043 2.33859317]
ti_comp = [0.37300745 0.38019815 0.37131151 0.37903648 0.37900657 0.37665288
 0.37938621 0.38326735 0.34493694 0.37691463]
ti_coms = [0.08054563 0.07335493 0.08224157 0.07451661 0.07454651 0.07690021
 0.07416687 0.07028573 0.10861614 0.07663845]
t_total = [28.79989929 28.79989929 28.79989929 28.79989929 28.79989929 28.79989929
 28.79989929 28.79989929 28.79989929 28.79989929]
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [1.35678390e-05 1.21493121e-04 1.30503540e-05 5.22248762e-07
 1.88236947e-04 2.07018929e-05 1.03241013e-06 3.68459284e-05
 1.74254783e-05 1.09142550e-05]
ene_total = [0.51231347 0.47350677 0.52304957 0.47320176 0.48531125 0.48961862
 0.47101339 0.44864286 0.69080172 0.48733502]
optimize_network iter = 0 obj = 5.054794435766291
eta = 0.685637753675084
freqs = [41745313.04826085 86137162.39129016 41270331.48315956 14019810.25468265
 99776755.16786237 47903390.74277508 17590043.05099849 57717284.54230393
 46575586.46414848 38689551.73454573]
eta_min = 0.6856377536750926	eta_max = 0.6856377536750637
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 0.024656332621380766	eta = 0.909090909090909
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 17.409847423005747	eta = 0.0012874810038828607
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 1.7319792587002336	eta = 0.012941753040645609
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 1.7000222977574324	eta = 0.013185031671165255
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 1.700018383049954	eta = 0.013185062032920516
eta = 0.013185062032920516
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [1.55616847e-04 1.39346998e-03 1.49681533e-04 5.98995210e-06
 2.15899085e-03 2.37441151e-04 1.18412674e-05 4.22605781e-04
 1.99862188e-04 1.25181464e-04]
ene_total = [0.16607727 0.17657133 0.16938779 0.15085494 0.1944668  0.16035839
 0.15026585 0.15072403 0.2237539  0.15755809]
ti_comp = [0.37300745 0.38019815 0.37131151 0.37903648 0.37900657 0.37665288
 0.37938621 0.38326735 0.34493694 0.37691463]
ti_coms = [0.08054563 0.07335493 0.08224157 0.07451661 0.07454651 0.07690021
 0.07416687 0.07028573 0.10861614 0.07663845]
t_total = [28.79989929 28.79989929 28.79989929 28.79989929 28.79989929 28.79989929
 28.79989929 28.79989929 28.79989929 28.79989929]
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [1.35678390e-05 1.21493121e-04 1.30503540e-05 5.22248762e-07
 1.88236947e-04 2.07018929e-05 1.03241013e-06 3.68459284e-05
 1.74254783e-05 1.09142550e-05]
ene_total = [0.51231347 0.47350677 0.52304957 0.47320176 0.48531125 0.48961862
 0.47101339 0.44864286 0.69080172 0.48733502]
optimize_network iter = 1 obj = 5.0547944357664285
eta = 0.6856377536750926
freqs = [41745313.04826085 86137162.3912901  41270331.48315956 14019810.25468264
 99776755.16786231 47903390.74277506 17590043.05099848 57717284.54230388
 46575586.46414861 38689551.73454572]
Done!
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [1.31745940e-05 1.17971811e-04 1.26721075e-05 5.07112105e-07
 1.82781160e-04 2.01018772e-05 1.00248715e-06 3.57780002e-05
 1.69204250e-05 1.05979204e-05]
ene_total = [0.00806774 0.00745347 0.00823683 0.00745217 0.00763743 0.00771012
 0.00741769 0.00706435 0.01087853 0.00767444]
At round 24 energy consumption: 0.07959277336451151
At round 24 eta: 0.6856377536750926
At round 24 a_n: 19.61895668202144
At round 24 local rounds: 12.358184869136853
At round 24 global rounds: 63.498409119903215
gradient difference: 0.45951047539711
train() client id: f_00000-0-0 loss: 1.396531  [   32/  126]
train() client id: f_00000-0-1 loss: 1.142050  [   64/  126]
train() client id: f_00000-0-2 loss: 1.257464  [   96/  126]
train() client id: f_00000-1-0 loss: 1.127039  [   32/  126]
train() client id: f_00000-1-1 loss: 1.221869  [   64/  126]
train() client id: f_00000-1-2 loss: 1.193540  [   96/  126]
train() client id: f_00000-2-0 loss: 1.086958  [   32/  126]
train() client id: f_00000-2-1 loss: 1.110188  [   64/  126]
train() client id: f_00000-2-2 loss: 1.032603  [   96/  126]
train() client id: f_00000-3-0 loss: 1.029466  [   32/  126]
train() client id: f_00000-3-1 loss: 0.936150  [   64/  126]
train() client id: f_00000-3-2 loss: 1.060193  [   96/  126]
train() client id: f_00000-4-0 loss: 0.980904  [   32/  126]
train() client id: f_00000-4-1 loss: 1.133767  [   64/  126]
train() client id: f_00000-4-2 loss: 0.902673  [   96/  126]
train() client id: f_00000-5-0 loss: 0.973721  [   32/  126]
train() client id: f_00000-5-1 loss: 0.928419  [   64/  126]
train() client id: f_00000-5-2 loss: 1.001386  [   96/  126]
train() client id: f_00000-6-0 loss: 0.957450  [   32/  126]
train() client id: f_00000-6-1 loss: 0.947016  [   64/  126]
train() client id: f_00000-6-2 loss: 0.855486  [   96/  126]
train() client id: f_00000-7-0 loss: 0.978646  [   32/  126]
train() client id: f_00000-7-1 loss: 0.940782  [   64/  126]
train() client id: f_00000-7-2 loss: 0.917873  [   96/  126]
train() client id: f_00000-8-0 loss: 0.913389  [   32/  126]
train() client id: f_00000-8-1 loss: 0.927775  [   64/  126]
train() client id: f_00000-8-2 loss: 0.940331  [   96/  126]
train() client id: f_00000-9-0 loss: 0.961803  [   32/  126]
train() client id: f_00000-9-1 loss: 0.943227  [   64/  126]
train() client id: f_00000-9-2 loss: 0.974186  [   96/  126]
train() client id: f_00000-10-0 loss: 1.033402  [   32/  126]
train() client id: f_00000-10-1 loss: 0.851166  [   64/  126]
train() client id: f_00000-10-2 loss: 0.981451  [   96/  126]
train() client id: f_00000-11-0 loss: 0.891578  [   32/  126]
train() client id: f_00000-11-1 loss: 0.900292  [   64/  126]
train() client id: f_00000-11-2 loss: 0.926066  [   96/  126]
train() client id: f_00001-0-0 loss: 0.533397  [   32/  265]
train() client id: f_00001-0-1 loss: 0.435388  [   64/  265]
train() client id: f_00001-0-2 loss: 0.515760  [   96/  265]
train() client id: f_00001-0-3 loss: 0.550368  [  128/  265]
train() client id: f_00001-0-4 loss: 0.502759  [  160/  265]
train() client id: f_00001-0-5 loss: 0.546959  [  192/  265]
train() client id: f_00001-0-6 loss: 0.497383  [  224/  265]
train() client id: f_00001-0-7 loss: 0.478236  [  256/  265]
train() client id: f_00001-1-0 loss: 0.416620  [   32/  265]
train() client id: f_00001-1-1 loss: 0.407816  [   64/  265]
train() client id: f_00001-1-2 loss: 0.449970  [   96/  265]
train() client id: f_00001-1-3 loss: 0.613594  [  128/  265]
train() client id: f_00001-1-4 loss: 0.700777  [  160/  265]
train() client id: f_00001-1-5 loss: 0.403557  [  192/  265]
train() client id: f_00001-1-6 loss: 0.514314  [  224/  265]
train() client id: f_00001-1-7 loss: 0.435578  [  256/  265]
train() client id: f_00001-2-0 loss: 0.472830  [   32/  265]
train() client id: f_00001-2-1 loss: 0.521724  [   64/  265]
train() client id: f_00001-2-2 loss: 0.564965  [   96/  265]
train() client id: f_00001-2-3 loss: 0.616809  [  128/  265]
train() client id: f_00001-2-4 loss: 0.422224  [  160/  265]
train() client id: f_00001-2-5 loss: 0.427308  [  192/  265]
train() client id: f_00001-2-6 loss: 0.547002  [  224/  265]
train() client id: f_00001-2-7 loss: 0.388032  [  256/  265]
train() client id: f_00001-3-0 loss: 0.397585  [   32/  265]
train() client id: f_00001-3-1 loss: 0.583814  [   64/  265]
train() client id: f_00001-3-2 loss: 0.419685  [   96/  265]
train() client id: f_00001-3-3 loss: 0.455091  [  128/  265]
train() client id: f_00001-3-4 loss: 0.434778  [  160/  265]
train() client id: f_00001-3-5 loss: 0.469288  [  192/  265]
train() client id: f_00001-3-6 loss: 0.535867  [  224/  265]
train() client id: f_00001-3-7 loss: 0.628164  [  256/  265]
train() client id: f_00001-4-0 loss: 0.497758  [   32/  265]
train() client id: f_00001-4-1 loss: 0.411825  [   64/  265]
train() client id: f_00001-4-2 loss: 0.550301  [   96/  265]
train() client id: f_00001-4-3 loss: 0.548252  [  128/  265]
train() client id: f_00001-4-4 loss: 0.446251  [  160/  265]
train() client id: f_00001-4-5 loss: 0.449203  [  192/  265]
train() client id: f_00001-4-6 loss: 0.519965  [  224/  265]
train() client id: f_00001-4-7 loss: 0.503617  [  256/  265]
train() client id: f_00001-5-0 loss: 0.582518  [   32/  265]
train() client id: f_00001-5-1 loss: 0.465824  [   64/  265]
train() client id: f_00001-5-2 loss: 0.483921  [   96/  265]
train() client id: f_00001-5-3 loss: 0.527401  [  128/  265]
train() client id: f_00001-5-4 loss: 0.392791  [  160/  265]
train() client id: f_00001-5-5 loss: 0.497229  [  192/  265]
train() client id: f_00001-5-6 loss: 0.539568  [  224/  265]
train() client id: f_00001-5-7 loss: 0.404884  [  256/  265]
train() client id: f_00001-6-0 loss: 0.393904  [   32/  265]
train() client id: f_00001-6-1 loss: 0.415721  [   64/  265]
train() client id: f_00001-6-2 loss: 0.395459  [   96/  265]
train() client id: f_00001-6-3 loss: 0.483410  [  128/  265]
train() client id: f_00001-6-4 loss: 0.564959  [  160/  265]
train() client id: f_00001-6-5 loss: 0.463415  [  192/  265]
train() client id: f_00001-6-6 loss: 0.544135  [  224/  265]
train() client id: f_00001-6-7 loss: 0.636225  [  256/  265]
train() client id: f_00001-7-0 loss: 0.566247  [   32/  265]
train() client id: f_00001-7-1 loss: 0.491149  [   64/  265]
train() client id: f_00001-7-2 loss: 0.445750  [   96/  265]
train() client id: f_00001-7-3 loss: 0.468502  [  128/  265]
train() client id: f_00001-7-4 loss: 0.520436  [  160/  265]
train() client id: f_00001-7-5 loss: 0.381095  [  192/  265]
train() client id: f_00001-7-6 loss: 0.395046  [  224/  265]
train() client id: f_00001-7-7 loss: 0.540298  [  256/  265]
train() client id: f_00001-8-0 loss: 0.505053  [   32/  265]
train() client id: f_00001-8-1 loss: 0.520143  [   64/  265]
train() client id: f_00001-8-2 loss: 0.404426  [   96/  265]
train() client id: f_00001-8-3 loss: 0.377292  [  128/  265]
train() client id: f_00001-8-4 loss: 0.491497  [  160/  265]
train() client id: f_00001-8-5 loss: 0.543698  [  192/  265]
train() client id: f_00001-8-6 loss: 0.502976  [  224/  265]
train() client id: f_00001-8-7 loss: 0.544619  [  256/  265]
train() client id: f_00001-9-0 loss: 0.534678  [   32/  265]
train() client id: f_00001-9-1 loss: 0.446251  [   64/  265]
train() client id: f_00001-9-2 loss: 0.436905  [   96/  265]
train() client id: f_00001-9-3 loss: 0.512108  [  128/  265]
train() client id: f_00001-9-4 loss: 0.569798  [  160/  265]
train() client id: f_00001-9-5 loss: 0.435291  [  192/  265]
train() client id: f_00001-9-6 loss: 0.388575  [  224/  265]
train() client id: f_00001-9-7 loss: 0.482422  [  256/  265]
train() client id: f_00001-10-0 loss: 0.487849  [   32/  265]
train() client id: f_00001-10-1 loss: 0.464761  [   64/  265]
train() client id: f_00001-10-2 loss: 0.396952  [   96/  265]
train() client id: f_00001-10-3 loss: 0.569090  [  128/  265]
train() client id: f_00001-10-4 loss: 0.488015  [  160/  265]
train() client id: f_00001-10-5 loss: 0.654587  [  192/  265]
train() client id: f_00001-10-6 loss: 0.425672  [  224/  265]
train() client id: f_00001-10-7 loss: 0.388849  [  256/  265]
train() client id: f_00001-11-0 loss: 0.508291  [   32/  265]
train() client id: f_00001-11-1 loss: 0.576036  [   64/  265]
train() client id: f_00001-11-2 loss: 0.404859  [   96/  265]
train() client id: f_00001-11-3 loss: 0.396105  [  128/  265]
train() client id: f_00001-11-4 loss: 0.599644  [  160/  265]
train() client id: f_00001-11-5 loss: 0.463174  [  192/  265]
train() client id: f_00001-11-6 loss: 0.461498  [  224/  265]
train() client id: f_00001-11-7 loss: 0.429246  [  256/  265]
train() client id: f_00002-0-0 loss: 1.200094  [   32/  124]
train() client id: f_00002-0-1 loss: 1.227819  [   64/  124]
train() client id: f_00002-0-2 loss: 1.097169  [   96/  124]
train() client id: f_00002-1-0 loss: 1.186554  [   32/  124]
train() client id: f_00002-1-1 loss: 1.146460  [   64/  124]
train() client id: f_00002-1-2 loss: 1.065970  [   96/  124]
train() client id: f_00002-2-0 loss: 1.058567  [   32/  124]
train() client id: f_00002-2-1 loss: 1.206596  [   64/  124]
train() client id: f_00002-2-2 loss: 1.095975  [   96/  124]
train() client id: f_00002-3-0 loss: 1.007312  [   32/  124]
train() client id: f_00002-3-1 loss: 1.155405  [   64/  124]
train() client id: f_00002-3-2 loss: 1.154064  [   96/  124]
train() client id: f_00002-4-0 loss: 1.066108  [   32/  124]
train() client id: f_00002-4-1 loss: 0.905806  [   64/  124]
train() client id: f_00002-4-2 loss: 1.183440  [   96/  124]
train() client id: f_00002-5-0 loss: 1.069081  [   32/  124]
train() client id: f_00002-5-1 loss: 0.966700  [   64/  124]
train() client id: f_00002-5-2 loss: 1.120258  [   96/  124]
train() client id: f_00002-6-0 loss: 1.079710  [   32/  124]
train() client id: f_00002-6-1 loss: 1.042422  [   64/  124]
train() client id: f_00002-6-2 loss: 0.997353  [   96/  124]
train() client id: f_00002-7-0 loss: 1.105798  [   32/  124]
train() client id: f_00002-7-1 loss: 0.941759  [   64/  124]
train() client id: f_00002-7-2 loss: 1.028500  [   96/  124]
train() client id: f_00002-8-0 loss: 0.962484  [   32/  124]
train() client id: f_00002-8-1 loss: 0.940386  [   64/  124]
train() client id: f_00002-8-2 loss: 1.141782  [   96/  124]
train() client id: f_00002-9-0 loss: 0.927133  [   32/  124]
train() client id: f_00002-9-1 loss: 1.031956  [   64/  124]
train() client id: f_00002-9-2 loss: 1.107140  [   96/  124]
train() client id: f_00002-10-0 loss: 0.884066  [   32/  124]
train() client id: f_00002-10-1 loss: 1.079285  [   64/  124]
train() client id: f_00002-10-2 loss: 1.153149  [   96/  124]
train() client id: f_00002-11-0 loss: 1.064585  [   32/  124]
train() client id: f_00002-11-1 loss: 0.991198  [   64/  124]
train() client id: f_00002-11-2 loss: 1.082456  [   96/  124]
train() client id: f_00003-0-0 loss: 0.895660  [   32/   43]
train() client id: f_00003-1-0 loss: 0.810588  [   32/   43]
train() client id: f_00003-2-0 loss: 0.767811  [   32/   43]
train() client id: f_00003-3-0 loss: 0.836750  [   32/   43]
train() client id: f_00003-4-0 loss: 0.761999  [   32/   43]
train() client id: f_00003-5-0 loss: 0.819351  [   32/   43]
train() client id: f_00003-6-0 loss: 0.950707  [   32/   43]
train() client id: f_00003-7-0 loss: 0.724309  [   32/   43]
train() client id: f_00003-8-0 loss: 0.803416  [   32/   43]
train() client id: f_00003-9-0 loss: 0.834388  [   32/   43]
train() client id: f_00003-10-0 loss: 0.662206  [   32/   43]
train() client id: f_00003-11-0 loss: 0.943061  [   32/   43]
train() client id: f_00004-0-0 loss: 0.832954  [   32/  306]
train() client id: f_00004-0-1 loss: 0.723753  [   64/  306]
train() client id: f_00004-0-2 loss: 0.925218  [   96/  306]
train() client id: f_00004-0-3 loss: 0.949771  [  128/  306]
train() client id: f_00004-0-4 loss: 1.019153  [  160/  306]
train() client id: f_00004-0-5 loss: 0.932487  [  192/  306]
train() client id: f_00004-0-6 loss: 0.731684  [  224/  306]
train() client id: f_00004-0-7 loss: 0.850243  [  256/  306]
train() client id: f_00004-0-8 loss: 0.789003  [  288/  306]
train() client id: f_00004-1-0 loss: 0.690826  [   32/  306]
train() client id: f_00004-1-1 loss: 0.915550  [   64/  306]
train() client id: f_00004-1-2 loss: 0.997848  [   96/  306]
train() client id: f_00004-1-3 loss: 0.774718  [  128/  306]
train() client id: f_00004-1-4 loss: 0.966903  [  160/  306]
train() client id: f_00004-1-5 loss: 0.821692  [  192/  306]
train() client id: f_00004-1-6 loss: 1.017067  [  224/  306]
train() client id: f_00004-1-7 loss: 0.837095  [  256/  306]
train() client id: f_00004-1-8 loss: 0.914961  [  288/  306]
train() client id: f_00004-2-0 loss: 0.764336  [   32/  306]
train() client id: f_00004-2-1 loss: 0.834217  [   64/  306]
train() client id: f_00004-2-2 loss: 0.817761  [   96/  306]
train() client id: f_00004-2-3 loss: 0.990993  [  128/  306]
train() client id: f_00004-2-4 loss: 0.819387  [  160/  306]
train() client id: f_00004-2-5 loss: 0.770118  [  192/  306]
train() client id: f_00004-2-6 loss: 0.970673  [  224/  306]
train() client id: f_00004-2-7 loss: 1.030326  [  256/  306]
train() client id: f_00004-2-8 loss: 0.845119  [  288/  306]
train() client id: f_00004-3-0 loss: 0.826445  [   32/  306]
train() client id: f_00004-3-1 loss: 0.870704  [   64/  306]
train() client id: f_00004-3-2 loss: 0.895143  [   96/  306]
train() client id: f_00004-3-3 loss: 0.975217  [  128/  306]
train() client id: f_00004-3-4 loss: 0.907264  [  160/  306]
train() client id: f_00004-3-5 loss: 0.864330  [  192/  306]
train() client id: f_00004-3-6 loss: 0.866618  [  224/  306]
train() client id: f_00004-3-7 loss: 0.835478  [  256/  306]
train() client id: f_00004-3-8 loss: 0.879129  [  288/  306]
train() client id: f_00004-4-0 loss: 0.847954  [   32/  306]
train() client id: f_00004-4-1 loss: 0.946635  [   64/  306]
train() client id: f_00004-4-2 loss: 0.908312  [   96/  306]
train() client id: f_00004-4-3 loss: 0.783053  [  128/  306]
train() client id: f_00004-4-4 loss: 0.728641  [  160/  306]
train() client id: f_00004-4-5 loss: 0.900640  [  192/  306]
train() client id: f_00004-4-6 loss: 0.830518  [  224/  306]
train() client id: f_00004-4-7 loss: 1.044584  [  256/  306]
train() client id: f_00004-4-8 loss: 0.854406  [  288/  306]
train() client id: f_00004-5-0 loss: 1.003001  [   32/  306]
train() client id: f_00004-5-1 loss: 0.850042  [   64/  306]
train() client id: f_00004-5-2 loss: 0.876879  [   96/  306]
train() client id: f_00004-5-3 loss: 0.902298  [  128/  306]
train() client id: f_00004-5-4 loss: 0.836519  [  160/  306]
train() client id: f_00004-5-5 loss: 0.860315  [  192/  306]
train() client id: f_00004-5-6 loss: 0.739753  [  224/  306]
train() client id: f_00004-5-7 loss: 0.862334  [  256/  306]
train() client id: f_00004-5-8 loss: 1.009642  [  288/  306]
train() client id: f_00004-6-0 loss: 0.817978  [   32/  306]
train() client id: f_00004-6-1 loss: 0.920143  [   64/  306]
train() client id: f_00004-6-2 loss: 0.851743  [   96/  306]
train() client id: f_00004-6-3 loss: 0.725917  [  128/  306]
train() client id: f_00004-6-4 loss: 0.767349  [  160/  306]
train() client id: f_00004-6-5 loss: 0.939372  [  192/  306]
train() client id: f_00004-6-6 loss: 0.853198  [  224/  306]
train() client id: f_00004-6-7 loss: 0.910913  [  256/  306]
train() client id: f_00004-6-8 loss: 0.953684  [  288/  306]
train() client id: f_00004-7-0 loss: 0.833026  [   32/  306]
train() client id: f_00004-7-1 loss: 0.893648  [   64/  306]
train() client id: f_00004-7-2 loss: 0.908936  [   96/  306]
train() client id: f_00004-7-3 loss: 0.873599  [  128/  306]
train() client id: f_00004-7-4 loss: 0.930358  [  160/  306]
train() client id: f_00004-7-5 loss: 0.796286  [  192/  306]
train() client id: f_00004-7-6 loss: 0.873297  [  224/  306]
train() client id: f_00004-7-7 loss: 0.714597  [  256/  306]
train() client id: f_00004-7-8 loss: 0.904148  [  288/  306]
train() client id: f_00004-8-0 loss: 0.781443  [   32/  306]
train() client id: f_00004-8-1 loss: 0.968549  [   64/  306]
train() client id: f_00004-8-2 loss: 0.805950  [   96/  306]
train() client id: f_00004-8-3 loss: 0.912103  [  128/  306]
train() client id: f_00004-8-4 loss: 0.853875  [  160/  306]
train() client id: f_00004-8-5 loss: 0.933954  [  192/  306]
train() client id: f_00004-8-6 loss: 0.794901  [  224/  306]
train() client id: f_00004-8-7 loss: 0.876395  [  256/  306]
train() client id: f_00004-8-8 loss: 0.979043  [  288/  306]
train() client id: f_00004-9-0 loss: 0.912785  [   32/  306]
train() client id: f_00004-9-1 loss: 0.848862  [   64/  306]
train() client id: f_00004-9-2 loss: 0.939669  [   96/  306]
train() client id: f_00004-9-3 loss: 0.835147  [  128/  306]
train() client id: f_00004-9-4 loss: 0.757066  [  160/  306]
train() client id: f_00004-9-5 loss: 0.986022  [  192/  306]
train() client id: f_00004-9-6 loss: 0.811212  [  224/  306]
train() client id: f_00004-9-7 loss: 0.793397  [  256/  306]
train() client id: f_00004-9-8 loss: 0.950624  [  288/  306]
train() client id: f_00004-10-0 loss: 0.844025  [   32/  306]
train() client id: f_00004-10-1 loss: 0.870244  [   64/  306]
train() client id: f_00004-10-2 loss: 0.976600  [   96/  306]
train() client id: f_00004-10-3 loss: 0.813853  [  128/  306]
train() client id: f_00004-10-4 loss: 0.973519  [  160/  306]
train() client id: f_00004-10-5 loss: 0.942386  [  192/  306]
train() client id: f_00004-10-6 loss: 0.872432  [  224/  306]
train() client id: f_00004-10-7 loss: 0.873063  [  256/  306]
train() client id: f_00004-10-8 loss: 0.774011  [  288/  306]
train() client id: f_00004-11-0 loss: 0.790478  [   32/  306]
train() client id: f_00004-11-1 loss: 0.782360  [   64/  306]
train() client id: f_00004-11-2 loss: 0.847509  [   96/  306]
train() client id: f_00004-11-3 loss: 0.903401  [  128/  306]
train() client id: f_00004-11-4 loss: 0.993524  [  160/  306]
train() client id: f_00004-11-5 loss: 0.960711  [  192/  306]
train() client id: f_00004-11-6 loss: 0.895647  [  224/  306]
train() client id: f_00004-11-7 loss: 0.779510  [  256/  306]
train() client id: f_00004-11-8 loss: 0.934311  [  288/  306]
train() client id: f_00005-0-0 loss: 0.852690  [   32/  146]
train() client id: f_00005-0-1 loss: 0.679068  [   64/  146]
train() client id: f_00005-0-2 loss: 0.741638  [   96/  146]
train() client id: f_00005-0-3 loss: 0.688761  [  128/  146]
train() client id: f_00005-1-0 loss: 0.779842  [   32/  146]
train() client id: f_00005-1-1 loss: 0.694837  [   64/  146]
train() client id: f_00005-1-2 loss: 0.685540  [   96/  146]
train() client id: f_00005-1-3 loss: 0.808528  [  128/  146]
train() client id: f_00005-2-0 loss: 0.725257  [   32/  146]
train() client id: f_00005-2-1 loss: 0.592279  [   64/  146]
train() client id: f_00005-2-2 loss: 0.615577  [   96/  146]
train() client id: f_00005-2-3 loss: 0.877757  [  128/  146]
train() client id: f_00005-3-0 loss: 0.576858  [   32/  146]
train() client id: f_00005-3-1 loss: 0.823459  [   64/  146]
train() client id: f_00005-3-2 loss: 0.653972  [   96/  146]
train() client id: f_00005-3-3 loss: 0.784266  [  128/  146]
train() client id: f_00005-4-0 loss: 0.819059  [   32/  146]
train() client id: f_00005-4-1 loss: 0.589977  [   64/  146]
train() client id: f_00005-4-2 loss: 0.577838  [   96/  146]
train() client id: f_00005-4-3 loss: 0.904699  [  128/  146]
train() client id: f_00005-5-0 loss: 0.930918  [   32/  146]
train() client id: f_00005-5-1 loss: 0.808727  [   64/  146]
train() client id: f_00005-5-2 loss: 0.527187  [   96/  146]
train() client id: f_00005-5-3 loss: 0.740792  [  128/  146]
train() client id: f_00005-6-0 loss: 0.691986  [   32/  146]
train() client id: f_00005-6-1 loss: 0.691021  [   64/  146]
train() client id: f_00005-6-2 loss: 0.806539  [   96/  146]
train() client id: f_00005-6-3 loss: 0.843611  [  128/  146]
train() client id: f_00005-7-0 loss: 0.544699  [   32/  146]
train() client id: f_00005-7-1 loss: 0.482972  [   64/  146]
train() client id: f_00005-7-2 loss: 0.721688  [   96/  146]
train() client id: f_00005-7-3 loss: 1.140390  [  128/  146]
train() client id: f_00005-8-0 loss: 0.711823  [   32/  146]
train() client id: f_00005-8-1 loss: 0.665799  [   64/  146]
train() client id: f_00005-8-2 loss: 0.957561  [   96/  146]
train() client id: f_00005-8-3 loss: 0.627761  [  128/  146]
train() client id: f_00005-9-0 loss: 0.687603  [   32/  146]
train() client id: f_00005-9-1 loss: 0.812897  [   64/  146]
train() client id: f_00005-9-2 loss: 0.635710  [   96/  146]
train() client id: f_00005-9-3 loss: 0.636218  [  128/  146]
train() client id: f_00005-10-0 loss: 0.814816  [   32/  146]
train() client id: f_00005-10-1 loss: 0.754038  [   64/  146]
train() client id: f_00005-10-2 loss: 0.416017  [   96/  146]
train() client id: f_00005-10-3 loss: 0.764506  [  128/  146]
train() client id: f_00005-11-0 loss: 0.576951  [   32/  146]
train() client id: f_00005-11-1 loss: 0.739800  [   64/  146]
train() client id: f_00005-11-2 loss: 0.642673  [   96/  146]
train() client id: f_00005-11-3 loss: 0.727856  [  128/  146]
train() client id: f_00006-0-0 loss: 0.599401  [   32/   54]
train() client id: f_00006-1-0 loss: 0.605387  [   32/   54]
train() client id: f_00006-2-0 loss: 0.579073  [   32/   54]
train() client id: f_00006-3-0 loss: 0.499231  [   32/   54]
train() client id: f_00006-4-0 loss: 0.628097  [   32/   54]
train() client id: f_00006-5-0 loss: 0.611082  [   32/   54]
train() client id: f_00006-6-0 loss: 0.611961  [   32/   54]
train() client id: f_00006-7-0 loss: 0.506914  [   32/   54]
train() client id: f_00006-8-0 loss: 0.546718  [   32/   54]
train() client id: f_00006-9-0 loss: 0.597536  [   32/   54]
train() client id: f_00006-10-0 loss: 0.605119  [   32/   54]
train() client id: f_00006-11-0 loss: 0.619431  [   32/   54]
train() client id: f_00007-0-0 loss: 0.582890  [   32/  179]
train() client id: f_00007-0-1 loss: 0.744665  [   64/  179]
train() client id: f_00007-0-2 loss: 0.725017  [   96/  179]
train() client id: f_00007-0-3 loss: 0.762010  [  128/  179]
train() client id: f_00007-0-4 loss: 0.729429  [  160/  179]
train() client id: f_00007-1-0 loss: 0.754303  [   32/  179]
train() client id: f_00007-1-1 loss: 0.905268  [   64/  179]
train() client id: f_00007-1-2 loss: 0.722954  [   96/  179]
train() client id: f_00007-1-3 loss: 0.572146  [  128/  179]
train() client id: f_00007-1-4 loss: 0.663291  [  160/  179]
train() client id: f_00007-2-0 loss: 0.633508  [   32/  179]
train() client id: f_00007-2-1 loss: 0.730147  [   64/  179]
train() client id: f_00007-2-2 loss: 0.786888  [   96/  179]
train() client id: f_00007-2-3 loss: 0.698470  [  128/  179]
train() client id: f_00007-2-4 loss: 0.685618  [  160/  179]
train() client id: f_00007-3-0 loss: 0.998010  [   32/  179]
train() client id: f_00007-3-1 loss: 0.565046  [   64/  179]
train() client id: f_00007-3-2 loss: 0.559419  [   96/  179]
train() client id: f_00007-3-3 loss: 0.667494  [  128/  179]
train() client id: f_00007-3-4 loss: 0.767482  [  160/  179]
train() client id: f_00007-4-0 loss: 0.630515  [   32/  179]
train() client id: f_00007-4-1 loss: 0.684297  [   64/  179]
train() client id: f_00007-4-2 loss: 0.717121  [   96/  179]
train() client id: f_00007-4-3 loss: 0.626758  [  128/  179]
train() client id: f_00007-4-4 loss: 0.669724  [  160/  179]
train() client id: f_00007-5-0 loss: 0.560196  [   32/  179]
train() client id: f_00007-5-1 loss: 0.586922  [   64/  179]
train() client id: f_00007-5-2 loss: 0.758394  [   96/  179]
train() client id: f_00007-5-3 loss: 0.734401  [  128/  179]
train() client id: f_00007-5-4 loss: 0.778734  [  160/  179]
train() client id: f_00007-6-0 loss: 0.731654  [   32/  179]
train() client id: f_00007-6-1 loss: 0.867576  [   64/  179]
train() client id: f_00007-6-2 loss: 0.569348  [   96/  179]
train() client id: f_00007-6-3 loss: 0.681225  [  128/  179]
train() client id: f_00007-6-4 loss: 0.683996  [  160/  179]
train() client id: f_00007-7-0 loss: 0.643467  [   32/  179]
train() client id: f_00007-7-1 loss: 0.554912  [   64/  179]
train() client id: f_00007-7-2 loss: 0.672844  [   96/  179]
train() client id: f_00007-7-3 loss: 0.697863  [  128/  179]
train() client id: f_00007-7-4 loss: 0.649712  [  160/  179]
train() client id: f_00007-8-0 loss: 0.618891  [   32/  179]
train() client id: f_00007-8-1 loss: 0.615960  [   64/  179]
train() client id: f_00007-8-2 loss: 0.870978  [   96/  179]
train() client id: f_00007-8-3 loss: 0.764283  [  128/  179]
train() client id: f_00007-8-4 loss: 0.646317  [  160/  179]
train() client id: f_00007-9-0 loss: 0.763924  [   32/  179]
train() client id: f_00007-9-1 loss: 0.585835  [   64/  179]
train() client id: f_00007-9-2 loss: 0.741712  [   96/  179]
train() client id: f_00007-9-3 loss: 0.662670  [  128/  179]
train() client id: f_00007-9-4 loss: 0.547469  [  160/  179]
train() client id: f_00007-10-0 loss: 0.887941  [   32/  179]
train() client id: f_00007-10-1 loss: 0.636769  [   64/  179]
train() client id: f_00007-10-2 loss: 0.627737  [   96/  179]
train() client id: f_00007-10-3 loss: 0.713275  [  128/  179]
train() client id: f_00007-10-4 loss: 0.586155  [  160/  179]
train() client id: f_00007-11-0 loss: 0.661970  [   32/  179]
train() client id: f_00007-11-1 loss: 0.776718  [   64/  179]
train() client id: f_00007-11-2 loss: 0.818969  [   96/  179]
train() client id: f_00007-11-3 loss: 0.533857  [  128/  179]
train() client id: f_00007-11-4 loss: 0.660164  [  160/  179]
train() client id: f_00008-0-0 loss: 0.836852  [   32/  130]
train() client id: f_00008-0-1 loss: 0.742426  [   64/  130]
train() client id: f_00008-0-2 loss: 0.655162  [   96/  130]
train() client id: f_00008-0-3 loss: 0.826063  [  128/  130]
train() client id: f_00008-1-0 loss: 0.815655  [   32/  130]
train() client id: f_00008-1-1 loss: 0.698288  [   64/  130]
train() client id: f_00008-1-2 loss: 0.727900  [   96/  130]
train() client id: f_00008-1-3 loss: 0.793574  [  128/  130]
train() client id: f_00008-2-0 loss: 0.749897  [   32/  130]
train() client id: f_00008-2-1 loss: 0.723169  [   64/  130]
train() client id: f_00008-2-2 loss: 0.737558  [   96/  130]
train() client id: f_00008-2-3 loss: 0.859216  [  128/  130]
train() client id: f_00008-3-0 loss: 0.817230  [   32/  130]
train() client id: f_00008-3-1 loss: 0.741175  [   64/  130]
train() client id: f_00008-3-2 loss: 0.596737  [   96/  130]
train() client id: f_00008-3-3 loss: 0.868607  [  128/  130]
train() client id: f_00008-4-0 loss: 0.653150  [   32/  130]
train() client id: f_00008-4-1 loss: 0.692370  [   64/  130]
train() client id: f_00008-4-2 loss: 0.873366  [   96/  130]
train() client id: f_00008-4-3 loss: 0.825156  [  128/  130]
train() client id: f_00008-5-0 loss: 0.824605  [   32/  130]
train() client id: f_00008-5-1 loss: 0.658307  [   64/  130]
train() client id: f_00008-5-2 loss: 0.668408  [   96/  130]
train() client id: f_00008-5-3 loss: 0.905197  [  128/  130]
train() client id: f_00008-6-0 loss: 0.722332  [   32/  130]
train() client id: f_00008-6-1 loss: 0.825452  [   64/  130]
train() client id: f_00008-6-2 loss: 0.829977  [   96/  130]
train() client id: f_00008-6-3 loss: 0.684280  [  128/  130]
train() client id: f_00008-7-0 loss: 0.899685  [   32/  130]
train() client id: f_00008-7-1 loss: 0.806623  [   64/  130]
train() client id: f_00008-7-2 loss: 0.670335  [   96/  130]
train() client id: f_00008-7-3 loss: 0.643198  [  128/  130]
train() client id: f_00008-8-0 loss: 0.806114  [   32/  130]
train() client id: f_00008-8-1 loss: 0.754128  [   64/  130]
train() client id: f_00008-8-2 loss: 0.748638  [   96/  130]
train() client id: f_00008-8-3 loss: 0.748784  [  128/  130]
train() client id: f_00008-9-0 loss: 0.738037  [   32/  130]
train() client id: f_00008-9-1 loss: 0.680348  [   64/  130]
train() client id: f_00008-9-2 loss: 0.703551  [   96/  130]
train() client id: f_00008-9-3 loss: 0.937070  [  128/  130]
train() client id: f_00008-10-0 loss: 0.885131  [   32/  130]
train() client id: f_00008-10-1 loss: 0.795463  [   64/  130]
train() client id: f_00008-10-2 loss: 0.678387  [   96/  130]
train() client id: f_00008-10-3 loss: 0.707352  [  128/  130]
train() client id: f_00008-11-0 loss: 0.787859  [   32/  130]
train() client id: f_00008-11-1 loss: 0.747337  [   64/  130]
train() client id: f_00008-11-2 loss: 0.721265  [   96/  130]
train() client id: f_00008-11-3 loss: 0.801798  [  128/  130]
train() client id: f_00009-0-0 loss: 1.060625  [   32/  118]
train() client id: f_00009-0-1 loss: 0.925067  [   64/  118]
train() client id: f_00009-0-2 loss: 0.758240  [   96/  118]
train() client id: f_00009-1-0 loss: 0.861214  [   32/  118]
train() client id: f_00009-1-1 loss: 0.980104  [   64/  118]
train() client id: f_00009-1-2 loss: 0.898965  [   96/  118]
train() client id: f_00009-2-0 loss: 0.938998  [   32/  118]
train() client id: f_00009-2-1 loss: 0.766765  [   64/  118]
train() client id: f_00009-2-2 loss: 0.810462  [   96/  118]
train() client id: f_00009-3-0 loss: 0.875499  [   32/  118]
train() client id: f_00009-3-1 loss: 0.768706  [   64/  118]
train() client id: f_00009-3-2 loss: 0.773745  [   96/  118]
train() client id: f_00009-4-0 loss: 0.758111  [   32/  118]
train() client id: f_00009-4-1 loss: 0.701025  [   64/  118]
train() client id: f_00009-4-2 loss: 0.768354  [   96/  118]
train() client id: f_00009-5-0 loss: 0.711625  [   32/  118]
train() client id: f_00009-5-1 loss: 0.770600  [   64/  118]
train() client id: f_00009-5-2 loss: 0.708949  [   96/  118]
train() client id: f_00009-6-0 loss: 0.717884  [   32/  118]
train() client id: f_00009-6-1 loss: 0.653038  [   64/  118]
train() client id: f_00009-6-2 loss: 0.712968  [   96/  118]
train() client id: f_00009-7-0 loss: 0.666302  [   32/  118]
train() client id: f_00009-7-1 loss: 0.673664  [   64/  118]
train() client id: f_00009-7-2 loss: 0.767295  [   96/  118]
train() client id: f_00009-8-0 loss: 0.648389  [   32/  118]
train() client id: f_00009-8-1 loss: 0.673834  [   64/  118]
train() client id: f_00009-8-2 loss: 0.648852  [   96/  118]
train() client id: f_00009-9-0 loss: 0.691577  [   32/  118]
train() client id: f_00009-9-1 loss: 0.498509  [   64/  118]
train() client id: f_00009-9-2 loss: 0.854637  [   96/  118]
train() client id: f_00009-10-0 loss: 0.612105  [   32/  118]
train() client id: f_00009-10-1 loss: 0.733611  [   64/  118]
train() client id: f_00009-10-2 loss: 0.675155  [   96/  118]
train() client id: f_00009-11-0 loss: 0.675444  [   32/  118]
train() client id: f_00009-11-1 loss: 0.617825  [   64/  118]
train() client id: f_00009-11-2 loss: 0.634706  [   96/  118]
At round 24 accuracy: 0.6419098143236074
At round 24 training accuracy: 0.5767940979208585
At round 24 training loss: 0.8409060456662397
update_location
xs = [  -3.9056584     4.20031788  140.00902392   18.81129433    0.97929623
    3.95640986 -102.44319194  -81.32485185  124.66397685  -67.06087855]
ys = [ 132.5879595   115.55583871    1.32061395 -102.45517586   94.35018685
   77.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [166.11689008 152.87509454 172.05891665 144.39850365 137.48787866
 126.77024586 143.18344219 128.89688819 160.77865871 120.47063247]
dists_bs = [176.27267203 188.80244235 359.71546295 338.4437803  193.57971242
 203.64303555 191.87268031 197.76978496 338.50113277 202.21609029]
uav_gains = [2.80067787e-11 3.45600080e-11 2.55955797e-11 3.98858645e-11
 4.51021890e-11 5.52599497e-11 4.07401996e-11 5.30073043e-11
 3.04302492e-11 6.27732201e-11]
bs_gains = [5.67513399e-11 4.68244606e-11 7.70208994e-12 9.13548862e-12
 4.36602816e-11 3.78843039e-11 4.47566205e-11 4.11193508e-11
 9.13115535e-12 3.86375962e-11]
Round 25
-------------------------------
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.69254516 15.98293133  7.57645092  2.71961017 18.43551744  8.87729699
  3.37637101 10.83932141  7.98736814  7.20317358]
obj_prev = 90.69058614039777
eta_min = 8.689629583387306e-13	eta_max = 0.9239062523538892
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 21.07280247978385	eta = 0.909090909090909
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 37.266591851058905	eta = 0.5140554102721235
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 29.4469781495679	eta = 0.65056227726107
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 28.03930224522773	eta = 0.683222891778642
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 27.967919357716607	eta = 0.6849666905290988
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 27.967721963314986	eta = 0.684971524980406
eta = 0.684971524980406
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [0.03122285 0.0656671  0.03072725 0.01065542 0.07582691 0.03617885
 0.01338122 0.04435627 0.03221405 0.02924044]
ene_total = [2.45319609 4.54407619 2.43314225 1.13378689 5.18350126 2.72855748
 1.30141153 3.20547972 2.69013729 2.29443326]
ti_comp = [0.37995732 0.38865987 0.37822142 0.38614518 0.38758329 0.38530164
 0.38648714 0.39049469 0.35182285 0.38562639]
ti_coms = [0.08168984 0.07298729 0.08342574 0.07550198 0.07406386 0.07634552
 0.07516002 0.07115247 0.10982431 0.07602077]
t_total = [28.7498951 28.7498951 28.7498951 28.7498951 28.7498951 28.7498951
 28.7498951 28.7498951 28.7498951 28.7498951]
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [1.31773438e-05 1.17161273e-04 1.26753124e-05 5.07095790e-07
 1.81392801e-04 1.99362151e-05 1.00252924e-06 3.57696506e-05
 1.68798211e-05 1.05074756e-05]
ene_total = [0.50955862 0.46183769 0.52033793 0.47023356 0.47254241 0.47669679
 0.46813477 0.44534224 0.68500157 0.47408718]
optimize_network iter = 0 obj = 4.983772760330714
eta = 0.684971524980406
freqs = [41087307.98029104 84478875.62799275 40620710.39935426 13797163.01214605
 97820151.49584162 46948740.71955757 17311339.16633044 56794967.35235866
 45781631.37350148 37912918.15352243]
eta_min = 0.6849715249804152	eta_max = 0.68497152498039
af = 0.02120720489719426	bf = 1.5620059925242489	zeta = 0.023327925386913688	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [3.31926004e-06 2.95119211e-05 3.19280262e-06 1.27733086e-07
 4.56912927e-05 5.02176180e-06 2.52528528e-07 9.01006858e-06
 4.25188234e-06 2.64674311e-06]
ene_total = [1.76365275 1.58149679 1.80108751 1.6294247  1.6082219  1.64868503
 1.62207175 1.53747541 2.37101979 1.64116416]
ti_comp = [0.37995732 0.38865987 0.37822142 0.38614518 0.38758329 0.38530164
 0.38648714 0.39049469 0.35182285 0.38562639]
ti_coms = [0.08168984 0.07298729 0.08342574 0.07550198 0.07406386 0.07634552
 0.07516002 0.07115247 0.10982431 0.07602077]
t_total = [28.7498951 28.7498951 28.7498951 28.7498951 28.7498951 28.7498951
 28.7498951 28.7498951 28.7498951 28.7498951]
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [1.31773438e-05 1.17161273e-04 1.26753124e-05 5.07095790e-07
 1.81392801e-04 1.99362151e-05 1.00252924e-06 3.57696506e-05
 1.68798211e-05 1.05074756e-05]
ene_total = [0.50955862 0.46183769 0.52033793 0.47023356 0.47254241 0.47669679
 0.46813477 0.44534224 0.68500157 0.47408718]
optimize_network iter = 1 obj = 4.983772760330463
eta = 0.68497152498039
freqs = [41087307.98029105 84478875.62799287 40620710.39935425 13797163.01214607
 97820151.49584176 46948740.71955761 17311339.16633046 56794967.35235876
 45781631.37350126 37912918.15352247]
Done!
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [1.27625416e-05 1.13473219e-04 1.22763133e-05 4.91133205e-07
 1.75682838e-04 1.93086541e-05 9.70971182e-07 3.46436777e-05
 1.63484706e-05 1.01767167e-05]
ene_total = [0.00818175 0.0074122  0.00835485 0.00755069 0.00758207 0.00765386
 0.00751697 0.00714989 0.01099878 0.00761225]
At round 25 energy consumption: 0.08001331387073954
At round 25 eta: 0.68497152498039
At round 25 a_n: 19.276410835052125
At round 25 local rounds: 12.390018469521971
At round 25 global rounds: 62.276772538737
gradient difference: 0.4617874026298523
train() client id: f_00000-0-0 loss: 1.314346  [   32/  126]
train() client id: f_00000-0-1 loss: 1.514135  [   64/  126]
train() client id: f_00000-0-2 loss: 1.008708  [   96/  126]
train() client id: f_00000-1-0 loss: 1.023266  [   32/  126]
train() client id: f_00000-1-1 loss: 1.218321  [   64/  126]
train() client id: f_00000-1-2 loss: 1.084854  [   96/  126]
train() client id: f_00000-2-0 loss: 1.201065  [   32/  126]
train() client id: f_00000-2-1 loss: 1.027030  [   64/  126]
train() client id: f_00000-2-2 loss: 1.014033  [   96/  126]
train() client id: f_00000-3-0 loss: 1.052606  [   32/  126]
train() client id: f_00000-3-1 loss: 1.140681  [   64/  126]
train() client id: f_00000-3-2 loss: 0.913293  [   96/  126]
train() client id: f_00000-4-0 loss: 1.021187  [   32/  126]
train() client id: f_00000-4-1 loss: 0.942309  [   64/  126]
train() client id: f_00000-4-2 loss: 0.930574  [   96/  126]
train() client id: f_00000-5-0 loss: 0.993959  [   32/  126]
train() client id: f_00000-5-1 loss: 0.909392  [   64/  126]
train() client id: f_00000-5-2 loss: 0.943655  [   96/  126]
train() client id: f_00000-6-0 loss: 0.879183  [   32/  126]
train() client id: f_00000-6-1 loss: 0.853777  [   64/  126]
train() client id: f_00000-6-2 loss: 0.962496  [   96/  126]
train() client id: f_00000-7-0 loss: 0.929502  [   32/  126]
train() client id: f_00000-7-1 loss: 0.816064  [   64/  126]
train() client id: f_00000-7-2 loss: 0.830557  [   96/  126]
train() client id: f_00000-8-0 loss: 0.915745  [   32/  126]
train() client id: f_00000-8-1 loss: 0.896949  [   64/  126]
train() client id: f_00000-8-2 loss: 0.847902  [   96/  126]
train() client id: f_00000-9-0 loss: 0.881449  [   32/  126]
train() client id: f_00000-9-1 loss: 0.784976  [   64/  126]
train() client id: f_00000-9-2 loss: 0.933062  [   96/  126]
train() client id: f_00000-10-0 loss: 0.915340  [   32/  126]
train() client id: f_00000-10-1 loss: 0.872959  [   64/  126]
train() client id: f_00000-10-2 loss: 0.903856  [   96/  126]
train() client id: f_00000-11-0 loss: 0.880184  [   32/  126]
train() client id: f_00000-11-1 loss: 0.828361  [   64/  126]
train() client id: f_00000-11-2 loss: 0.912842  [   96/  126]
train() client id: f_00001-0-0 loss: 0.433107  [   32/  265]
train() client id: f_00001-0-1 loss: 0.465878  [   64/  265]
train() client id: f_00001-0-2 loss: 0.504469  [   96/  265]
train() client id: f_00001-0-3 loss: 0.418226  [  128/  265]
train() client id: f_00001-0-4 loss: 0.488056  [  160/  265]
train() client id: f_00001-0-5 loss: 0.474758  [  192/  265]
train() client id: f_00001-0-6 loss: 0.588131  [  224/  265]
train() client id: f_00001-0-7 loss: 0.621617  [  256/  265]
train() client id: f_00001-1-0 loss: 0.427557  [   32/  265]
train() client id: f_00001-1-1 loss: 0.456032  [   64/  265]
train() client id: f_00001-1-2 loss: 0.589488  [   96/  265]
train() client id: f_00001-1-3 loss: 0.457196  [  128/  265]
train() client id: f_00001-1-4 loss: 0.527272  [  160/  265]
train() client id: f_00001-1-5 loss: 0.595446  [  192/  265]
train() client id: f_00001-1-6 loss: 0.502622  [  224/  265]
train() client id: f_00001-1-7 loss: 0.444324  [  256/  265]
train() client id: f_00001-2-0 loss: 0.513475  [   32/  265]
train() client id: f_00001-2-1 loss: 0.584676  [   64/  265]
train() client id: f_00001-2-2 loss: 0.420482  [   96/  265]
train() client id: f_00001-2-3 loss: 0.478487  [  128/  265]
train() client id: f_00001-2-4 loss: 0.551558  [  160/  265]
train() client id: f_00001-2-5 loss: 0.446366  [  192/  265]
train() client id: f_00001-2-6 loss: 0.409278  [  224/  265]
train() client id: f_00001-2-7 loss: 0.521030  [  256/  265]
train() client id: f_00001-3-0 loss: 0.481334  [   32/  265]
train() client id: f_00001-3-1 loss: 0.389591  [   64/  265]
train() client id: f_00001-3-2 loss: 0.525939  [   96/  265]
train() client id: f_00001-3-3 loss: 0.461300  [  128/  265]
train() client id: f_00001-3-4 loss: 0.562219  [  160/  265]
train() client id: f_00001-3-5 loss: 0.463917  [  192/  265]
train() client id: f_00001-3-6 loss: 0.459755  [  224/  265]
train() client id: f_00001-3-7 loss: 0.482465  [  256/  265]
train() client id: f_00001-4-0 loss: 0.399010  [   32/  265]
train() client id: f_00001-4-1 loss: 0.505371  [   64/  265]
train() client id: f_00001-4-2 loss: 0.660838  [   96/  265]
train() client id: f_00001-4-3 loss: 0.406176  [  128/  265]
train() client id: f_00001-4-4 loss: 0.475957  [  160/  265]
train() client id: f_00001-4-5 loss: 0.494839  [  192/  265]
train() client id: f_00001-4-6 loss: 0.405080  [  224/  265]
train() client id: f_00001-4-7 loss: 0.522243  [  256/  265]
train() client id: f_00001-5-0 loss: 0.538024  [   32/  265]
train() client id: f_00001-5-1 loss: 0.473830  [   64/  265]
train() client id: f_00001-5-2 loss: 0.462144  [   96/  265]
train() client id: f_00001-5-3 loss: 0.458270  [  128/  265]
train() client id: f_00001-5-4 loss: 0.425920  [  160/  265]
train() client id: f_00001-5-5 loss: 0.447815  [  192/  265]
train() client id: f_00001-5-6 loss: 0.514938  [  224/  265]
train() client id: f_00001-5-7 loss: 0.524791  [  256/  265]
train() client id: f_00001-6-0 loss: 0.434282  [   32/  265]
train() client id: f_00001-6-1 loss: 0.363709  [   64/  265]
train() client id: f_00001-6-2 loss: 0.537175  [   96/  265]
train() client id: f_00001-6-3 loss: 0.492924  [  128/  265]
train() client id: f_00001-6-4 loss: 0.472270  [  160/  265]
train() client id: f_00001-6-5 loss: 0.466152  [  192/  265]
train() client id: f_00001-6-6 loss: 0.497136  [  224/  265]
train() client id: f_00001-6-7 loss: 0.519678  [  256/  265]
train() client id: f_00001-7-0 loss: 0.479030  [   32/  265]
train() client id: f_00001-7-1 loss: 0.520183  [   64/  265]
train() client id: f_00001-7-2 loss: 0.522490  [   96/  265]
train() client id: f_00001-7-3 loss: 0.499319  [  128/  265]
train() client id: f_00001-7-4 loss: 0.386051  [  160/  265]
train() client id: f_00001-7-5 loss: 0.561376  [  192/  265]
train() client id: f_00001-7-6 loss: 0.377348  [  224/  265]
train() client id: f_00001-7-7 loss: 0.487820  [  256/  265]
train() client id: f_00001-8-0 loss: 0.461760  [   32/  265]
train() client id: f_00001-8-1 loss: 0.496009  [   64/  265]
train() client id: f_00001-8-2 loss: 0.545607  [   96/  265]
train() client id: f_00001-8-3 loss: 0.432655  [  128/  265]
train() client id: f_00001-8-4 loss: 0.568939  [  160/  265]
train() client id: f_00001-8-5 loss: 0.384199  [  192/  265]
train() client id: f_00001-8-6 loss: 0.522805  [  224/  265]
train() client id: f_00001-8-7 loss: 0.422409  [  256/  265]
train() client id: f_00001-9-0 loss: 0.514196  [   32/  265]
train() client id: f_00001-9-1 loss: 0.577117  [   64/  265]
train() client id: f_00001-9-2 loss: 0.421796  [   96/  265]
train() client id: f_00001-9-3 loss: 0.443288  [  128/  265]
train() client id: f_00001-9-4 loss: 0.436643  [  160/  265]
train() client id: f_00001-9-5 loss: 0.436219  [  192/  265]
train() client id: f_00001-9-6 loss: 0.454718  [  224/  265]
train() client id: f_00001-9-7 loss: 0.488020  [  256/  265]
train() client id: f_00001-10-0 loss: 0.360340  [   32/  265]
train() client id: f_00001-10-1 loss: 0.449054  [   64/  265]
train() client id: f_00001-10-2 loss: 0.528164  [   96/  265]
train() client id: f_00001-10-3 loss: 0.687567  [  128/  265]
train() client id: f_00001-10-4 loss: 0.474576  [  160/  265]
train() client id: f_00001-10-5 loss: 0.511897  [  192/  265]
train() client id: f_00001-10-6 loss: 0.364712  [  224/  265]
train() client id: f_00001-10-7 loss: 0.454993  [  256/  265]
train() client id: f_00001-11-0 loss: 0.371407  [   32/  265]
train() client id: f_00001-11-1 loss: 0.517247  [   64/  265]
train() client id: f_00001-11-2 loss: 0.444816  [   96/  265]
train() client id: f_00001-11-3 loss: 0.543737  [  128/  265]
train() client id: f_00001-11-4 loss: 0.440185  [  160/  265]
train() client id: f_00001-11-5 loss: 0.431250  [  192/  265]
train() client id: f_00001-11-6 loss: 0.533192  [  224/  265]
train() client id: f_00001-11-7 loss: 0.511970  [  256/  265]
train() client id: f_00002-0-0 loss: 1.001933  [   32/  124]
train() client id: f_00002-0-1 loss: 1.069469  [   64/  124]
train() client id: f_00002-0-2 loss: 1.001374  [   96/  124]
train() client id: f_00002-1-0 loss: 1.104031  [   32/  124]
train() client id: f_00002-1-1 loss: 1.036613  [   64/  124]
train() client id: f_00002-1-2 loss: 0.971602  [   96/  124]
train() client id: f_00002-2-0 loss: 0.981611  [   32/  124]
train() client id: f_00002-2-1 loss: 0.906993  [   64/  124]
train() client id: f_00002-2-2 loss: 1.175228  [   96/  124]
train() client id: f_00002-3-0 loss: 1.101990  [   32/  124]
train() client id: f_00002-3-1 loss: 0.883819  [   64/  124]
train() client id: f_00002-3-2 loss: 1.033827  [   96/  124]
train() client id: f_00002-4-0 loss: 0.820788  [   32/  124]
train() client id: f_00002-4-1 loss: 0.985819  [   64/  124]
train() client id: f_00002-4-2 loss: 1.022718  [   96/  124]
train() client id: f_00002-5-0 loss: 0.889953  [   32/  124]
train() client id: f_00002-5-1 loss: 1.099156  [   64/  124]
train() client id: f_00002-5-2 loss: 0.859995  [   96/  124]
train() client id: f_00002-6-0 loss: 1.018849  [   32/  124]
train() client id: f_00002-6-1 loss: 1.021428  [   64/  124]
train() client id: f_00002-6-2 loss: 0.820116  [   96/  124]
train() client id: f_00002-7-0 loss: 1.120134  [   32/  124]
train() client id: f_00002-7-1 loss: 0.854547  [   64/  124]
train() client id: f_00002-7-2 loss: 0.935032  [   96/  124]
train() client id: f_00002-8-0 loss: 0.994299  [   32/  124]
train() client id: f_00002-8-1 loss: 0.833112  [   64/  124]
train() client id: f_00002-8-2 loss: 0.790813  [   96/  124]
train() client id: f_00002-9-0 loss: 0.827538  [   32/  124]
train() client id: f_00002-9-1 loss: 0.856647  [   64/  124]
train() client id: f_00002-9-2 loss: 1.015887  [   96/  124]
train() client id: f_00002-10-0 loss: 0.970576  [   32/  124]
train() client id: f_00002-10-1 loss: 0.811971  [   64/  124]
train() client id: f_00002-10-2 loss: 0.879184  [   96/  124]
train() client id: f_00002-11-0 loss: 1.048272  [   32/  124]
train() client id: f_00002-11-1 loss: 0.841625  [   64/  124]
train() client id: f_00002-11-2 loss: 0.796567  [   96/  124]
train() client id: f_00003-0-0 loss: 0.538268  [   32/   43]
train() client id: f_00003-1-0 loss: 0.604541  [   32/   43]
train() client id: f_00003-2-0 loss: 0.583005  [   32/   43]
train() client id: f_00003-3-0 loss: 0.553915  [   32/   43]
train() client id: f_00003-4-0 loss: 0.482903  [   32/   43]
train() client id: f_00003-5-0 loss: 0.549199  [   32/   43]
train() client id: f_00003-6-0 loss: 0.315946  [   32/   43]
train() client id: f_00003-7-0 loss: 0.516595  [   32/   43]
train() client id: f_00003-8-0 loss: 0.577189  [   32/   43]
train() client id: f_00003-9-0 loss: 0.608129  [   32/   43]
train() client id: f_00003-10-0 loss: 0.659530  [   32/   43]
train() client id: f_00003-11-0 loss: 0.560962  [   32/   43]
train() client id: f_00004-0-0 loss: 0.727282  [   32/  306]
train() client id: f_00004-0-1 loss: 0.814142  [   64/  306]
train() client id: f_00004-0-2 loss: 0.794783  [   96/  306]
train() client id: f_00004-0-3 loss: 0.804083  [  128/  306]
train() client id: f_00004-0-4 loss: 0.810570  [  160/  306]
train() client id: f_00004-0-5 loss: 0.753035  [  192/  306]
train() client id: f_00004-0-6 loss: 0.809413  [  224/  306]
train() client id: f_00004-0-7 loss: 0.856247  [  256/  306]
train() client id: f_00004-0-8 loss: 0.793469  [  288/  306]
train() client id: f_00004-1-0 loss: 0.771979  [   32/  306]
train() client id: f_00004-1-1 loss: 0.752232  [   64/  306]
train() client id: f_00004-1-2 loss: 0.889128  [   96/  306]
train() client id: f_00004-1-3 loss: 0.781903  [  128/  306]
train() client id: f_00004-1-4 loss: 0.704599  [  160/  306]
train() client id: f_00004-1-5 loss: 0.911369  [  192/  306]
train() client id: f_00004-1-6 loss: 0.767426  [  224/  306]
train() client id: f_00004-1-7 loss: 0.849584  [  256/  306]
train() client id: f_00004-1-8 loss: 0.726295  [  288/  306]
train() client id: f_00004-2-0 loss: 0.899481  [   32/  306]
train() client id: f_00004-2-1 loss: 0.848647  [   64/  306]
train() client id: f_00004-2-2 loss: 0.850935  [   96/  306]
train() client id: f_00004-2-3 loss: 0.710665  [  128/  306]
train() client id: f_00004-2-4 loss: 0.749751  [  160/  306]
train() client id: f_00004-2-5 loss: 0.856707  [  192/  306]
train() client id: f_00004-2-6 loss: 0.732306  [  224/  306]
train() client id: f_00004-2-7 loss: 0.773752  [  256/  306]
train() client id: f_00004-2-8 loss: 0.691750  [  288/  306]
train() client id: f_00004-3-0 loss: 0.885565  [   32/  306]
train() client id: f_00004-3-1 loss: 0.769404  [   64/  306]
train() client id: f_00004-3-2 loss: 0.763405  [   96/  306]
train() client id: f_00004-3-3 loss: 0.906645  [  128/  306]
train() client id: f_00004-3-4 loss: 0.761473  [  160/  306]
train() client id: f_00004-3-5 loss: 0.734643  [  192/  306]
train() client id: f_00004-3-6 loss: 0.781402  [  224/  306]
train() client id: f_00004-3-7 loss: 0.843752  [  256/  306]
train() client id: f_00004-3-8 loss: 0.758334  [  288/  306]
train() client id: f_00004-4-0 loss: 0.820095  [   32/  306]
train() client id: f_00004-4-1 loss: 0.812780  [   64/  306]
train() client id: f_00004-4-2 loss: 0.785526  [   96/  306]
train() client id: f_00004-4-3 loss: 0.842108  [  128/  306]
train() client id: f_00004-4-4 loss: 0.788468  [  160/  306]
train() client id: f_00004-4-5 loss: 0.741677  [  192/  306]
train() client id: f_00004-4-6 loss: 0.729970  [  224/  306]
train() client id: f_00004-4-7 loss: 0.816268  [  256/  306]
train() client id: f_00004-4-8 loss: 0.834245  [  288/  306]
train() client id: f_00004-5-0 loss: 0.711103  [   32/  306]
train() client id: f_00004-5-1 loss: 0.923388  [   64/  306]
train() client id: f_00004-5-2 loss: 0.721420  [   96/  306]
train() client id: f_00004-5-3 loss: 0.701333  [  128/  306]
train() client id: f_00004-5-4 loss: 0.704632  [  160/  306]
train() client id: f_00004-5-5 loss: 0.896046  [  192/  306]
train() client id: f_00004-5-6 loss: 0.944502  [  224/  306]
train() client id: f_00004-5-7 loss: 0.719324  [  256/  306]
train() client id: f_00004-5-8 loss: 0.736443  [  288/  306]
train() client id: f_00004-6-0 loss: 0.827803  [   32/  306]
train() client id: f_00004-6-1 loss: 0.774404  [   64/  306]
train() client id: f_00004-6-2 loss: 0.837868  [   96/  306]
train() client id: f_00004-6-3 loss: 0.724962  [  128/  306]
train() client id: f_00004-6-4 loss: 0.758591  [  160/  306]
train() client id: f_00004-6-5 loss: 0.841178  [  192/  306]
train() client id: f_00004-6-6 loss: 0.836719  [  224/  306]
train() client id: f_00004-6-7 loss: 0.804272  [  256/  306]
train() client id: f_00004-6-8 loss: 0.818171  [  288/  306]
train() client id: f_00004-7-0 loss: 0.844012  [   32/  306]
train() client id: f_00004-7-1 loss: 0.911079  [   64/  306]
train() client id: f_00004-7-2 loss: 0.818927  [   96/  306]
train() client id: f_00004-7-3 loss: 0.785351  [  128/  306]
train() client id: f_00004-7-4 loss: 0.836353  [  160/  306]
train() client id: f_00004-7-5 loss: 0.718481  [  192/  306]
train() client id: f_00004-7-6 loss: 0.704403  [  224/  306]
train() client id: f_00004-7-7 loss: 0.758339  [  256/  306]
train() client id: f_00004-7-8 loss: 0.834986  [  288/  306]
train() client id: f_00004-8-0 loss: 0.719269  [   32/  306]
train() client id: f_00004-8-1 loss: 0.838377  [   64/  306]
train() client id: f_00004-8-2 loss: 0.817022  [   96/  306]
train() client id: f_00004-8-3 loss: 0.804428  [  128/  306]
train() client id: f_00004-8-4 loss: 0.823231  [  160/  306]
train() client id: f_00004-8-5 loss: 0.756736  [  192/  306]
train() client id: f_00004-8-6 loss: 0.895005  [  224/  306]
train() client id: f_00004-8-7 loss: 0.867392  [  256/  306]
train() client id: f_00004-8-8 loss: 0.779894  [  288/  306]
train() client id: f_00004-9-0 loss: 0.860070  [   32/  306]
train() client id: f_00004-9-1 loss: 0.761304  [   64/  306]
train() client id: f_00004-9-2 loss: 0.783202  [   96/  306]
train() client id: f_00004-9-3 loss: 0.623330  [  128/  306]
train() client id: f_00004-9-4 loss: 0.869136  [  160/  306]
train() client id: f_00004-9-5 loss: 0.817297  [  192/  306]
train() client id: f_00004-9-6 loss: 0.818198  [  224/  306]
train() client id: f_00004-9-7 loss: 0.829359  [  256/  306]
train() client id: f_00004-9-8 loss: 0.848172  [  288/  306]
train() client id: f_00004-10-0 loss: 0.681496  [   32/  306]
train() client id: f_00004-10-1 loss: 0.755892  [   64/  306]
train() client id: f_00004-10-2 loss: 0.853151  [   96/  306]
train() client id: f_00004-10-3 loss: 0.786316  [  128/  306]
train() client id: f_00004-10-4 loss: 0.900645  [  160/  306]
train() client id: f_00004-10-5 loss: 0.765059  [  192/  306]
train() client id: f_00004-10-6 loss: 0.734676  [  224/  306]
train() client id: f_00004-10-7 loss: 0.731618  [  256/  306]
train() client id: f_00004-10-8 loss: 0.976709  [  288/  306]
train() client id: f_00004-11-0 loss: 0.815235  [   32/  306]
train() client id: f_00004-11-1 loss: 0.864531  [   64/  306]
train() client id: f_00004-11-2 loss: 0.842894  [   96/  306]
train() client id: f_00004-11-3 loss: 0.921839  [  128/  306]
train() client id: f_00004-11-4 loss: 0.810785  [  160/  306]
train() client id: f_00004-11-5 loss: 0.788814  [  192/  306]
train() client id: f_00004-11-6 loss: 0.755297  [  224/  306]
train() client id: f_00004-11-7 loss: 0.745795  [  256/  306]
train() client id: f_00004-11-8 loss: 0.764765  [  288/  306]
train() client id: f_00005-0-0 loss: 0.650756  [   32/  146]
train() client id: f_00005-0-1 loss: 0.738427  [   64/  146]
train() client id: f_00005-0-2 loss: 0.623758  [   96/  146]
train() client id: f_00005-0-3 loss: 0.504452  [  128/  146]
train() client id: f_00005-1-0 loss: 0.715819  [   32/  146]
train() client id: f_00005-1-1 loss: 0.722920  [   64/  146]
train() client id: f_00005-1-2 loss: 0.688890  [   96/  146]
train() client id: f_00005-1-3 loss: 0.610635  [  128/  146]
train() client id: f_00005-2-0 loss: 0.647165  [   32/  146]
train() client id: f_00005-2-1 loss: 0.762840  [   64/  146]
train() client id: f_00005-2-2 loss: 0.605449  [   96/  146]
train() client id: f_00005-2-3 loss: 0.530576  [  128/  146]
train() client id: f_00005-3-0 loss: 0.759703  [   32/  146]
train() client id: f_00005-3-1 loss: 0.523064  [   64/  146]
train() client id: f_00005-3-2 loss: 0.579553  [   96/  146]
train() client id: f_00005-3-3 loss: 0.553077  [  128/  146]
train() client id: f_00005-4-0 loss: 0.334620  [   32/  146]
train() client id: f_00005-4-1 loss: 0.727322  [   64/  146]
train() client id: f_00005-4-2 loss: 1.055189  [   96/  146]
train() client id: f_00005-4-3 loss: 0.579272  [  128/  146]
train() client id: f_00005-5-0 loss: 0.584838  [   32/  146]
train() client id: f_00005-5-1 loss: 0.327792  [   64/  146]
train() client id: f_00005-5-2 loss: 0.679810  [   96/  146]
train() client id: f_00005-5-3 loss: 0.940842  [  128/  146]
train() client id: f_00005-6-0 loss: 0.756683  [   32/  146]
train() client id: f_00005-6-1 loss: 0.631995  [   64/  146]
train() client id: f_00005-6-2 loss: 0.738730  [   96/  146]
train() client id: f_00005-6-3 loss: 0.603504  [  128/  146]
train() client id: f_00005-7-0 loss: 0.715433  [   32/  146]
train() client id: f_00005-7-1 loss: 0.390559  [   64/  146]
train() client id: f_00005-7-2 loss: 0.854201  [   96/  146]
train() client id: f_00005-7-3 loss: 0.828737  [  128/  146]
train() client id: f_00005-8-0 loss: 0.518609  [   32/  146]
train() client id: f_00005-8-1 loss: 0.535737  [   64/  146]
train() client id: f_00005-8-2 loss: 0.674748  [   96/  146]
train() client id: f_00005-8-3 loss: 0.726524  [  128/  146]
train() client id: f_00005-9-0 loss: 0.786520  [   32/  146]
train() client id: f_00005-9-1 loss: 0.659189  [   64/  146]
train() client id: f_00005-9-2 loss: 0.650934  [   96/  146]
train() client id: f_00005-9-3 loss: 0.547076  [  128/  146]
train() client id: f_00005-10-0 loss: 0.756065  [   32/  146]
train() client id: f_00005-10-1 loss: 0.729691  [   64/  146]
train() client id: f_00005-10-2 loss: 0.601323  [   96/  146]
train() client id: f_00005-10-3 loss: 0.654659  [  128/  146]
train() client id: f_00005-11-0 loss: 0.596285  [   32/  146]
train() client id: f_00005-11-1 loss: 0.529192  [   64/  146]
train() client id: f_00005-11-2 loss: 0.544154  [   96/  146]
train() client id: f_00005-11-3 loss: 0.832290  [  128/  146]
train() client id: f_00006-0-0 loss: 0.520639  [   32/   54]
train() client id: f_00006-1-0 loss: 0.512348  [   32/   54]
train() client id: f_00006-2-0 loss: 0.502641  [   32/   54]
train() client id: f_00006-3-0 loss: 0.457004  [   32/   54]
train() client id: f_00006-4-0 loss: 0.514383  [   32/   54]
train() client id: f_00006-5-0 loss: 0.472132  [   32/   54]
train() client id: f_00006-6-0 loss: 0.496828  [   32/   54]
train() client id: f_00006-7-0 loss: 0.549804  [   32/   54]
train() client id: f_00006-8-0 loss: 0.549157  [   32/   54]
train() client id: f_00006-9-0 loss: 0.538958  [   32/   54]
train() client id: f_00006-10-0 loss: 0.545002  [   32/   54]
train() client id: f_00006-11-0 loss: 0.483175  [   32/   54]
train() client id: f_00007-0-0 loss: 0.504540  [   32/  179]
train() client id: f_00007-0-1 loss: 0.634846  [   64/  179]
train() client id: f_00007-0-2 loss: 0.568282  [   96/  179]
train() client id: f_00007-0-3 loss: 0.696060  [  128/  179]
train() client id: f_00007-0-4 loss: 0.861543  [  160/  179]
train() client id: f_00007-1-0 loss: 0.688323  [   32/  179]
train() client id: f_00007-1-1 loss: 0.656812  [   64/  179]
train() client id: f_00007-1-2 loss: 0.665738  [   96/  179]
train() client id: f_00007-1-3 loss: 0.582733  [  128/  179]
train() client id: f_00007-1-4 loss: 0.641372  [  160/  179]
train() client id: f_00007-2-0 loss: 0.548627  [   32/  179]
train() client id: f_00007-2-1 loss: 0.545571  [   64/  179]
train() client id: f_00007-2-2 loss: 0.866356  [   96/  179]
train() client id: f_00007-2-3 loss: 0.510373  [  128/  179]
train() client id: f_00007-2-4 loss: 0.564481  [  160/  179]
train() client id: f_00007-3-0 loss: 0.544913  [   32/  179]
train() client id: f_00007-3-1 loss: 0.631169  [   64/  179]
train() client id: f_00007-3-2 loss: 0.453709  [   96/  179]
train() client id: f_00007-3-3 loss: 0.607276  [  128/  179]
train() client id: f_00007-3-4 loss: 0.668916  [  160/  179]
train() client id: f_00007-4-0 loss: 0.571276  [   32/  179]
train() client id: f_00007-4-1 loss: 0.593167  [   64/  179]
train() client id: f_00007-4-2 loss: 0.717148  [   96/  179]
train() client id: f_00007-4-3 loss: 0.522640  [  128/  179]
train() client id: f_00007-4-4 loss: 0.721002  [  160/  179]
train() client id: f_00007-5-0 loss: 0.629232  [   32/  179]
train() client id: f_00007-5-1 loss: 0.550449  [   64/  179]
train() client id: f_00007-5-2 loss: 0.612811  [   96/  179]
train() client id: f_00007-5-3 loss: 0.478081  [  128/  179]
train() client id: f_00007-5-4 loss: 0.594806  [  160/  179]
train() client id: f_00007-6-0 loss: 0.477641  [   32/  179]
train() client id: f_00007-6-1 loss: 0.686425  [   64/  179]
train() client id: f_00007-6-2 loss: 0.540078  [   96/  179]
train() client id: f_00007-6-3 loss: 0.545986  [  128/  179]
train() client id: f_00007-6-4 loss: 0.606178  [  160/  179]
train() client id: f_00007-7-0 loss: 0.546938  [   32/  179]
train() client id: f_00007-7-1 loss: 0.688855  [   64/  179]
train() client id: f_00007-7-2 loss: 0.466948  [   96/  179]
train() client id: f_00007-7-3 loss: 0.544182  [  128/  179]
train() client id: f_00007-7-4 loss: 0.756046  [  160/  179]
train() client id: f_00007-8-0 loss: 0.502668  [   32/  179]
train() client id: f_00007-8-1 loss: 0.659925  [   64/  179]
train() client id: f_00007-8-2 loss: 0.697179  [   96/  179]
train() client id: f_00007-8-3 loss: 0.755354  [  128/  179]
train() client id: f_00007-8-4 loss: 0.445225  [  160/  179]
train() client id: f_00007-9-0 loss: 0.578672  [   32/  179]
train() client id: f_00007-9-1 loss: 0.458293  [   64/  179]
train() client id: f_00007-9-2 loss: 0.531479  [   96/  179]
train() client id: f_00007-9-3 loss: 0.588532  [  128/  179]
train() client id: f_00007-9-4 loss: 0.808554  [  160/  179]
train() client id: f_00007-10-0 loss: 0.633085  [   32/  179]
train() client id: f_00007-10-1 loss: 0.704477  [   64/  179]
train() client id: f_00007-10-2 loss: 0.506484  [   96/  179]
train() client id: f_00007-10-3 loss: 0.577581  [  128/  179]
train() client id: f_00007-10-4 loss: 0.629830  [  160/  179]
train() client id: f_00007-11-0 loss: 0.554439  [   32/  179]
train() client id: f_00007-11-1 loss: 0.745436  [   64/  179]
train() client id: f_00007-11-2 loss: 0.638875  [   96/  179]
train() client id: f_00007-11-3 loss: 0.573742  [  128/  179]
train() client id: f_00007-11-4 loss: 0.527454  [  160/  179]
train() client id: f_00008-0-0 loss: 0.735278  [   32/  130]
train() client id: f_00008-0-1 loss: 0.775813  [   64/  130]
train() client id: f_00008-0-2 loss: 0.591404  [   96/  130]
train() client id: f_00008-0-3 loss: 0.678636  [  128/  130]
train() client id: f_00008-1-0 loss: 0.577508  [   32/  130]
train() client id: f_00008-1-1 loss: 0.788565  [   64/  130]
train() client id: f_00008-1-2 loss: 0.859812  [   96/  130]
train() client id: f_00008-1-3 loss: 0.592782  [  128/  130]
train() client id: f_00008-2-0 loss: 0.601870  [   32/  130]
train() client id: f_00008-2-1 loss: 0.728470  [   64/  130]
train() client id: f_00008-2-2 loss: 0.833236  [   96/  130]
train() client id: f_00008-2-3 loss: 0.652980  [  128/  130]
train() client id: f_00008-3-0 loss: 0.712631  [   32/  130]
train() client id: f_00008-3-1 loss: 0.675666  [   64/  130]
train() client id: f_00008-3-2 loss: 0.770958  [   96/  130]
train() client id: f_00008-3-3 loss: 0.648812  [  128/  130]
train() client id: f_00008-4-0 loss: 0.649410  [   32/  130]
train() client id: f_00008-4-1 loss: 0.659772  [   64/  130]
train() client id: f_00008-4-2 loss: 0.767243  [   96/  130]
train() client id: f_00008-4-3 loss: 0.724548  [  128/  130]
train() client id: f_00008-5-0 loss: 0.736009  [   32/  130]
train() client id: f_00008-5-1 loss: 0.701601  [   64/  130]
train() client id: f_00008-5-2 loss: 0.672259  [   96/  130]
train() client id: f_00008-5-3 loss: 0.691718  [  128/  130]
train() client id: f_00008-6-0 loss: 0.774328  [   32/  130]
train() client id: f_00008-6-1 loss: 0.724621  [   64/  130]
train() client id: f_00008-6-2 loss: 0.649732  [   96/  130]
train() client id: f_00008-6-3 loss: 0.622583  [  128/  130]
train() client id: f_00008-7-0 loss: 0.733637  [   32/  130]
train() client id: f_00008-7-1 loss: 0.612331  [   64/  130]
train() client id: f_00008-7-2 loss: 0.673652  [   96/  130]
train() client id: f_00008-7-3 loss: 0.778942  [  128/  130]
train() client id: f_00008-8-0 loss: 0.687762  [   32/  130]
train() client id: f_00008-8-1 loss: 0.673652  [   64/  130]
train() client id: f_00008-8-2 loss: 0.784873  [   96/  130]
train() client id: f_00008-8-3 loss: 0.620786  [  128/  130]
train() client id: f_00008-9-0 loss: 0.652159  [   32/  130]
train() client id: f_00008-9-1 loss: 0.678282  [   64/  130]
train() client id: f_00008-9-2 loss: 0.804160  [   96/  130]
train() client id: f_00008-9-3 loss: 0.632265  [  128/  130]
train() client id: f_00008-10-0 loss: 0.652526  [   32/  130]
train() client id: f_00008-10-1 loss: 0.697747  [   64/  130]
train() client id: f_00008-10-2 loss: 0.710884  [   96/  130]
train() client id: f_00008-10-3 loss: 0.730382  [  128/  130]
train() client id: f_00008-11-0 loss: 0.872016  [   32/  130]
train() client id: f_00008-11-1 loss: 0.696477  [   64/  130]
train() client id: f_00008-11-2 loss: 0.610141  [   96/  130]
train() client id: f_00008-11-3 loss: 0.613030  [  128/  130]
train() client id: f_00009-0-0 loss: 1.056712  [   32/  118]
train() client id: f_00009-0-1 loss: 1.121920  [   64/  118]
train() client id: f_00009-0-2 loss: 1.262232  [   96/  118]
train() client id: f_00009-1-0 loss: 1.035026  [   32/  118]
train() client id: f_00009-1-1 loss: 1.243323  [   64/  118]
train() client id: f_00009-1-2 loss: 0.972207  [   96/  118]
train() client id: f_00009-2-0 loss: 1.077796  [   32/  118]
train() client id: f_00009-2-1 loss: 0.925701  [   64/  118]
train() client id: f_00009-2-2 loss: 1.137711  [   96/  118]
train() client id: f_00009-3-0 loss: 1.063378  [   32/  118]
train() client id: f_00009-3-1 loss: 1.059847  [   64/  118]
train() client id: f_00009-3-2 loss: 0.939309  [   96/  118]
train() client id: f_00009-4-0 loss: 0.934816  [   32/  118]
train() client id: f_00009-4-1 loss: 1.002720  [   64/  118]
train() client id: f_00009-4-2 loss: 1.064258  [   96/  118]
train() client id: f_00009-5-0 loss: 0.829681  [   32/  118]
train() client id: f_00009-5-1 loss: 1.103448  [   64/  118]
train() client id: f_00009-5-2 loss: 0.962140  [   96/  118]
train() client id: f_00009-6-0 loss: 0.960946  [   32/  118]
train() client id: f_00009-6-1 loss: 0.960328  [   64/  118]
train() client id: f_00009-6-2 loss: 0.953717  [   96/  118]
train() client id: f_00009-7-0 loss: 0.940916  [   32/  118]
train() client id: f_00009-7-1 loss: 0.983939  [   64/  118]
train() client id: f_00009-7-2 loss: 0.832156  [   96/  118]
train() client id: f_00009-8-0 loss: 0.902931  [   32/  118]
train() client id: f_00009-8-1 loss: 0.834010  [   64/  118]
train() client id: f_00009-8-2 loss: 0.887853  [   96/  118]
train() client id: f_00009-9-0 loss: 0.885448  [   32/  118]
train() client id: f_00009-9-1 loss: 0.753569  [   64/  118]
train() client id: f_00009-9-2 loss: 0.975915  [   96/  118]
train() client id: f_00009-10-0 loss: 0.833691  [   32/  118]
train() client id: f_00009-10-1 loss: 0.859963  [   64/  118]
train() client id: f_00009-10-2 loss: 1.021151  [   96/  118]
train() client id: f_00009-11-0 loss: 0.801251  [   32/  118]
train() client id: f_00009-11-1 loss: 0.981617  [   64/  118]
train() client id: f_00009-11-2 loss: 0.868635  [   96/  118]
At round 25 accuracy: 0.6419098143236074
At round 25 training accuracy: 0.5774647887323944
At round 25 training loss: 0.8387249677009146
update_location
xs = [  -3.9056584     4.20031788  145.00902392   18.81129433    0.97929623
    3.95640986 -107.44319194  -86.32485185  129.66397685  -72.06087855]
ys = [ 137.5879595   120.55583871    1.32061395 -107.45517586   99.35018685
   82.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [170.13436092 156.68871343 176.15152863 147.98810632 140.96601948
 129.89933311 146.80235024 132.10850201 164.68581258 123.32389093]
dists_bs = [175.13690217 187.28833553 364.10205235 342.55473166 191.55053362
 201.30481225 190.03830501 195.4511099  342.93535346 199.59197368]
uav_gains = [2.63464755e-11 3.24787964e-11 2.40854862e-11 3.75018239e-11
 4.23658097e-11 5.19897705e-11 3.82670974e-11 4.98415206e-11
 2.86308105e-11 5.92039294e-11]
bs_gains = [5.77878620e-11 4.78921177e-11 7.44507933e-12 8.83181915e-12
 4.49676985e-11 3.91293331e-11 4.59768137e-11 4.24998396e-11
 8.80439989e-12 4.00768410e-11]
Round 26
-------------------------------
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.5605131  15.70320748  7.4465293   2.67409633 18.11274305  8.72126419
  3.31940453 10.65181908  7.8502879   7.07624586]
obj_prev = 89.11611082472227
eta_min = 5.392428774203226e-13	eta_max = 0.9242899713750001
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 20.70487256941968	eta = 0.909090909090909
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 36.71367954165409	eta = 0.5126865969778287
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 28.972791112910695	eta = 0.6496651065957375
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 27.57869875098564	eta = 0.6825054219090905
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 27.5077842638857	eta = 0.6842649064780151
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 27.50758684227239	eta = 0.6842698174388544
eta = 0.6842698174388544
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [0.03130742 0.06584498 0.03081048 0.01068428 0.07603231 0.03627686
 0.01341747 0.04447642 0.03230131 0.02931965]
ene_total = [2.4173172  4.46359599 2.39783696 1.11945579 5.09142412 2.67769452
 1.2842835  3.15534052 2.65007964 2.25055859]
ti_comp = [0.3872178  0.39743138 0.38543905 0.39356432 0.39647217 0.39426467
 0.39389891 0.39802621 0.35903228 0.39465362]
ti_coms = [0.08286047 0.07264689 0.08463921 0.07651394 0.0736061  0.07581359
 0.07617935 0.07205206 0.11104599 0.07542464]
t_total = [28.6998909 28.6998909 28.6998909 28.6998909 28.6998909 28.6998909
 28.6998909 28.6998909 28.6998909 28.6998909]
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [1.27912028e-05 1.12959740e-04 1.23045006e-05 4.92135047e-07
 1.74762896e-04 1.91952252e-05 9.73020721e-07 3.47093215e-05
 1.63408045e-05 1.01140293e-05]
ene_total = [0.50667253 0.45043075 0.51750266 0.46717392 0.46006034 0.46403994
 0.4651605  0.44202165 0.67897151 0.46111082]
optimize_network iter = 0 obj = 4.913144626268169
eta = 0.6842698174388544
freqs = [40426116.40412594 82838172.3252855  39968031.32913876 13573739.87970061
 95886067.29898961 46005714.64645749 17031612.51169543 55871219.40930291
 44983852.28187121 37146055.45423073]
eta_min = 0.6842698174388597	eta_max = 0.6842698174388548
af = 0.020050708162629324	bf = 1.5436207515894453	zeta = 0.022055778978892257	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [3.21329018e-06 2.83767235e-05 3.09102528e-06 1.23629712e-07
 4.39023530e-05 4.82205074e-06 2.44433458e-07 8.71936159e-06
 4.10498900e-06 2.54075489e-06]
ene_total = [1.757659   1.54642537 1.79534967 1.62243185 1.57005655 1.60857787
 1.61536282 1.52964443 2.35549529 1.59984681]
ti_comp = [0.3872178  0.39743138 0.38543905 0.39356432 0.39647217 0.39426467
 0.39389891 0.39802621 0.35903228 0.39465362]
ti_coms = [0.08286047 0.07264689 0.08463921 0.07651394 0.0736061  0.07581359
 0.07617935 0.07205206 0.11104599 0.07542464]
t_total = [28.6998909 28.6998909 28.6998909 28.6998909 28.6998909 28.6998909
 28.6998909 28.6998909 28.6998909 28.6998909]
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [1.27912028e-05 1.12959740e-04 1.23045006e-05 4.92135047e-07
 1.74762896e-04 1.91952252e-05 9.73020721e-07 3.47093215e-05
 1.63408045e-05 1.01140293e-05]
ene_total = [0.50667253 0.45043075 0.51750266 0.46717392 0.46006034 0.46403994
 0.4651605  0.44202165 0.67897151 0.46111082]
optimize_network iter = 1 obj = 4.913144626268177
eta = 0.6842698174388548
freqs = [40426116.40412594 82838172.32528551 39968031.32913877 13573739.87970061
 95886067.29898961 46005714.64645751 17031612.51169543 55871219.40930291
 44983852.28187121 37146055.45423073]
Done!
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [1.23550879e-05 1.09108388e-04 1.18849799e-05 4.75355749e-07
 1.68804372e-04 1.85407658e-05 9.39845672e-07 3.35259105e-05
 1.57836663e-05 9.76919241e-06]
ene_total = [0.0082984  0.0073738  0.00847581 0.00765187 0.00752941 0.0075999
 0.00761888 0.00723873 0.01112038 0.00755223]
At round 26 energy consumption: 0.08045941191860002
At round 26 eta: 0.6842698174388548
At round 26 a_n: 18.93386498808281
At round 26 local rounds: 12.423580809938418
At round 26 global rounds: 61.05343074483861
gradient difference: 0.41426676511764526
train() client id: f_00000-0-0 loss: 1.427159  [   32/  126]
train() client id: f_00000-0-1 loss: 1.534276  [   64/  126]
train() client id: f_00000-0-2 loss: 1.259192  [   96/  126]
train() client id: f_00000-1-0 loss: 1.350614  [   32/  126]
train() client id: f_00000-1-1 loss: 1.349423  [   64/  126]
train() client id: f_00000-1-2 loss: 1.198871  [   96/  126]
train() client id: f_00000-2-0 loss: 1.175775  [   32/  126]
train() client id: f_00000-2-1 loss: 1.145728  [   64/  126]
train() client id: f_00000-2-2 loss: 1.113976  [   96/  126]
train() client id: f_00000-3-0 loss: 1.174520  [   32/  126]
train() client id: f_00000-3-1 loss: 1.050144  [   64/  126]
train() client id: f_00000-3-2 loss: 1.054387  [   96/  126]
train() client id: f_00000-4-0 loss: 0.975163  [   32/  126]
train() client id: f_00000-4-1 loss: 1.059083  [   64/  126]
train() client id: f_00000-4-2 loss: 1.046638  [   96/  126]
train() client id: f_00000-5-0 loss: 0.929425  [   32/  126]
train() client id: f_00000-5-1 loss: 0.921072  [   64/  126]
train() client id: f_00000-5-2 loss: 0.967852  [   96/  126]
train() client id: f_00000-6-0 loss: 0.996437  [   32/  126]
train() client id: f_00000-6-1 loss: 0.886664  [   64/  126]
train() client id: f_00000-6-2 loss: 0.872360  [   96/  126]
train() client id: f_00000-7-0 loss: 0.847599  [   32/  126]
train() client id: f_00000-7-1 loss: 0.877613  [   64/  126]
train() client id: f_00000-7-2 loss: 0.865933  [   96/  126]
train() client id: f_00000-8-0 loss: 0.698899  [   32/  126]
train() client id: f_00000-8-1 loss: 0.935307  [   64/  126]
train() client id: f_00000-8-2 loss: 0.858850  [   96/  126]
train() client id: f_00000-9-0 loss: 0.784721  [   32/  126]
train() client id: f_00000-9-1 loss: 0.770314  [   64/  126]
train() client id: f_00000-9-2 loss: 0.853242  [   96/  126]
train() client id: f_00000-10-0 loss: 0.778727  [   32/  126]
train() client id: f_00000-10-1 loss: 0.779251  [   64/  126]
train() client id: f_00000-10-2 loss: 0.885277  [   96/  126]
train() client id: f_00000-11-0 loss: 0.763156  [   32/  126]
train() client id: f_00000-11-1 loss: 0.799234  [   64/  126]
train() client id: f_00000-11-2 loss: 0.804384  [   96/  126]
train() client id: f_00001-0-0 loss: 0.518140  [   32/  265]
train() client id: f_00001-0-1 loss: 0.414844  [   64/  265]
train() client id: f_00001-0-2 loss: 0.437242  [   96/  265]
train() client id: f_00001-0-3 loss: 0.467315  [  128/  265]
train() client id: f_00001-0-4 loss: 0.517743  [  160/  265]
train() client id: f_00001-0-5 loss: 0.391259  [  192/  265]
train() client id: f_00001-0-6 loss: 0.448436  [  224/  265]
train() client id: f_00001-0-7 loss: 0.386655  [  256/  265]
train() client id: f_00001-1-0 loss: 0.375033  [   32/  265]
train() client id: f_00001-1-1 loss: 0.482135  [   64/  265]
train() client id: f_00001-1-2 loss: 0.449545  [   96/  265]
train() client id: f_00001-1-3 loss: 0.445790  [  128/  265]
train() client id: f_00001-1-4 loss: 0.506557  [  160/  265]
train() client id: f_00001-1-5 loss: 0.455485  [  192/  265]
train() client id: f_00001-1-6 loss: 0.332954  [  224/  265]
train() client id: f_00001-1-7 loss: 0.448977  [  256/  265]
train() client id: f_00001-2-0 loss: 0.414508  [   32/  265]
train() client id: f_00001-2-1 loss: 0.335751  [   64/  265]
train() client id: f_00001-2-2 loss: 0.531964  [   96/  265]
train() client id: f_00001-2-3 loss: 0.380158  [  128/  265]
train() client id: f_00001-2-4 loss: 0.437060  [  160/  265]
train() client id: f_00001-2-5 loss: 0.549407  [  192/  265]
train() client id: f_00001-2-6 loss: 0.372320  [  224/  265]
train() client id: f_00001-2-7 loss: 0.388126  [  256/  265]
train() client id: f_00001-3-0 loss: 0.486684  [   32/  265]
train() client id: f_00001-3-1 loss: 0.403958  [   64/  265]
train() client id: f_00001-3-2 loss: 0.470869  [   96/  265]
train() client id: f_00001-3-3 loss: 0.425996  [  128/  265]
train() client id: f_00001-3-4 loss: 0.400446  [  160/  265]
train() client id: f_00001-3-5 loss: 0.331221  [  192/  265]
train() client id: f_00001-3-6 loss: 0.372883  [  224/  265]
train() client id: f_00001-3-7 loss: 0.412233  [  256/  265]
train() client id: f_00001-4-0 loss: 0.545835  [   32/  265]
train() client id: f_00001-4-1 loss: 0.466597  [   64/  265]
train() client id: f_00001-4-2 loss: 0.333996  [   96/  265]
train() client id: f_00001-4-3 loss: 0.371104  [  128/  265]
train() client id: f_00001-4-4 loss: 0.312455  [  160/  265]
train() client id: f_00001-4-5 loss: 0.326242  [  192/  265]
train() client id: f_00001-4-6 loss: 0.380904  [  224/  265]
train() client id: f_00001-4-7 loss: 0.506028  [  256/  265]
train() client id: f_00001-5-0 loss: 0.455980  [   32/  265]
train() client id: f_00001-5-1 loss: 0.419521  [   64/  265]
train() client id: f_00001-5-2 loss: 0.381005  [   96/  265]
train() client id: f_00001-5-3 loss: 0.370088  [  128/  265]
train() client id: f_00001-5-4 loss: 0.355738  [  160/  265]
train() client id: f_00001-5-5 loss: 0.421866  [  192/  265]
train() client id: f_00001-5-6 loss: 0.412376  [  224/  265]
train() client id: f_00001-5-7 loss: 0.449958  [  256/  265]
train() client id: f_00001-6-0 loss: 0.373097  [   32/  265]
train() client id: f_00001-6-1 loss: 0.403705  [   64/  265]
train() client id: f_00001-6-2 loss: 0.468521  [   96/  265]
train() client id: f_00001-6-3 loss: 0.432279  [  128/  265]
train() client id: f_00001-6-4 loss: 0.457185  [  160/  265]
train() client id: f_00001-6-5 loss: 0.380478  [  192/  265]
train() client id: f_00001-6-6 loss: 0.314199  [  224/  265]
train() client id: f_00001-6-7 loss: 0.352168  [  256/  265]
train() client id: f_00001-7-0 loss: 0.443705  [   32/  265]
train() client id: f_00001-7-1 loss: 0.413985  [   64/  265]
train() client id: f_00001-7-2 loss: 0.307986  [   96/  265]
train() client id: f_00001-7-3 loss: 0.359821  [  128/  265]
train() client id: f_00001-7-4 loss: 0.569256  [  160/  265]
train() client id: f_00001-7-5 loss: 0.370815  [  192/  265]
train() client id: f_00001-7-6 loss: 0.337830  [  224/  265]
train() client id: f_00001-7-7 loss: 0.404568  [  256/  265]
train() client id: f_00001-8-0 loss: 0.465156  [   32/  265]
train() client id: f_00001-8-1 loss: 0.393848  [   64/  265]
train() client id: f_00001-8-2 loss: 0.314809  [   96/  265]
train() client id: f_00001-8-3 loss: 0.299569  [  128/  265]
train() client id: f_00001-8-4 loss: 0.447668  [  160/  265]
train() client id: f_00001-8-5 loss: 0.498155  [  192/  265]
train() client id: f_00001-8-6 loss: 0.353170  [  224/  265]
train() client id: f_00001-8-7 loss: 0.359510  [  256/  265]
train() client id: f_00001-9-0 loss: 0.522539  [   32/  265]
train() client id: f_00001-9-1 loss: 0.385159  [   64/  265]
train() client id: f_00001-9-2 loss: 0.308933  [   96/  265]
train() client id: f_00001-9-3 loss: 0.379915  [  128/  265]
train() client id: f_00001-9-4 loss: 0.317452  [  160/  265]
train() client id: f_00001-9-5 loss: 0.422500  [  192/  265]
train() client id: f_00001-9-6 loss: 0.426798  [  224/  265]
train() client id: f_00001-9-7 loss: 0.414061  [  256/  265]
train() client id: f_00001-10-0 loss: 0.414788  [   32/  265]
train() client id: f_00001-10-1 loss: 0.370143  [   64/  265]
train() client id: f_00001-10-2 loss: 0.386135  [   96/  265]
train() client id: f_00001-10-3 loss: 0.385521  [  128/  265]
train() client id: f_00001-10-4 loss: 0.419141  [  160/  265]
train() client id: f_00001-10-5 loss: 0.452781  [  192/  265]
train() client id: f_00001-10-6 loss: 0.432406  [  224/  265]
train() client id: f_00001-10-7 loss: 0.307244  [  256/  265]
train() client id: f_00001-11-0 loss: 0.362350  [   32/  265]
train() client id: f_00001-11-1 loss: 0.310936  [   64/  265]
train() client id: f_00001-11-2 loss: 0.572320  [   96/  265]
train() client id: f_00001-11-3 loss: 0.438059  [  128/  265]
train() client id: f_00001-11-4 loss: 0.351787  [  160/  265]
train() client id: f_00001-11-5 loss: 0.305070  [  192/  265]
train() client id: f_00001-11-6 loss: 0.513224  [  224/  265]
train() client id: f_00001-11-7 loss: 0.294297  [  256/  265]
train() client id: f_00002-0-0 loss: 1.241698  [   32/  124]
train() client id: f_00002-0-1 loss: 1.278995  [   64/  124]
train() client id: f_00002-0-2 loss: 1.307853  [   96/  124]
train() client id: f_00002-1-0 loss: 1.154170  [   32/  124]
train() client id: f_00002-1-1 loss: 1.350159  [   64/  124]
train() client id: f_00002-1-2 loss: 1.206393  [   96/  124]
train() client id: f_00002-2-0 loss: 1.204840  [   32/  124]
train() client id: f_00002-2-1 loss: 1.275253  [   64/  124]
train() client id: f_00002-2-2 loss: 1.167516  [   96/  124]
train() client id: f_00002-3-0 loss: 1.280343  [   32/  124]
train() client id: f_00002-3-1 loss: 1.158801  [   64/  124]
train() client id: f_00002-3-2 loss: 1.236508  [   96/  124]
train() client id: f_00002-4-0 loss: 1.096317  [   32/  124]
train() client id: f_00002-4-1 loss: 1.264761  [   64/  124]
train() client id: f_00002-4-2 loss: 1.072404  [   96/  124]
train() client id: f_00002-5-0 loss: 1.133214  [   32/  124]
train() client id: f_00002-5-1 loss: 1.135965  [   64/  124]
train() client id: f_00002-5-2 loss: 1.147154  [   96/  124]
train() client id: f_00002-6-0 loss: 1.078630  [   32/  124]
train() client id: f_00002-6-1 loss: 1.174357  [   64/  124]
train() client id: f_00002-6-2 loss: 1.037090  [   96/  124]
train() client id: f_00002-7-0 loss: 1.150451  [   32/  124]
train() client id: f_00002-7-1 loss: 1.024998  [   64/  124]
train() client id: f_00002-7-2 loss: 1.265537  [   96/  124]
train() client id: f_00002-8-0 loss: 1.174727  [   32/  124]
train() client id: f_00002-8-1 loss: 1.080969  [   64/  124]
train() client id: f_00002-8-2 loss: 1.083777  [   96/  124]
train() client id: f_00002-9-0 loss: 1.083516  [   32/  124]
train() client id: f_00002-9-1 loss: 1.047734  [   64/  124]
train() client id: f_00002-9-2 loss: 1.076045  [   96/  124]
train() client id: f_00002-10-0 loss: 1.186274  [   32/  124]
train() client id: f_00002-10-1 loss: 1.085990  [   64/  124]
train() client id: f_00002-10-2 loss: 1.093675  [   96/  124]
train() client id: f_00002-11-0 loss: 1.049219  [   32/  124]
train() client id: f_00002-11-1 loss: 1.016484  [   64/  124]
train() client id: f_00002-11-2 loss: 1.159561  [   96/  124]
train() client id: f_00003-0-0 loss: 0.847610  [   32/   43]
train() client id: f_00003-1-0 loss: 0.831515  [   32/   43]
train() client id: f_00003-2-0 loss: 0.651447  [   32/   43]
train() client id: f_00003-3-0 loss: 0.728957  [   32/   43]
train() client id: f_00003-4-0 loss: 0.669066  [   32/   43]
train() client id: f_00003-5-0 loss: 0.649153  [   32/   43]
train() client id: f_00003-6-0 loss: 0.820653  [   32/   43]
train() client id: f_00003-7-0 loss: 0.910399  [   32/   43]
train() client id: f_00003-8-0 loss: 0.661716  [   32/   43]
train() client id: f_00003-9-0 loss: 0.718786  [   32/   43]
train() client id: f_00003-10-0 loss: 0.585761  [   32/   43]
train() client id: f_00003-11-0 loss: 0.591387  [   32/   43]
train() client id: f_00004-0-0 loss: 0.683886  [   32/  306]
train() client id: f_00004-0-1 loss: 1.011050  [   64/  306]
train() client id: f_00004-0-2 loss: 1.080926  [   96/  306]
train() client id: f_00004-0-3 loss: 0.961149  [  128/  306]
train() client id: f_00004-0-4 loss: 0.930199  [  160/  306]
train() client id: f_00004-0-5 loss: 0.855047  [  192/  306]
train() client id: f_00004-0-6 loss: 0.962530  [  224/  306]
train() client id: f_00004-0-7 loss: 0.959559  [  256/  306]
train() client id: f_00004-0-8 loss: 0.840464  [  288/  306]
train() client id: f_00004-1-0 loss: 0.976149  [   32/  306]
train() client id: f_00004-1-1 loss: 0.851619  [   64/  306]
train() client id: f_00004-1-2 loss: 0.786142  [   96/  306]
train() client id: f_00004-1-3 loss: 0.841991  [  128/  306]
train() client id: f_00004-1-4 loss: 0.923294  [  160/  306]
train() client id: f_00004-1-5 loss: 0.983923  [  192/  306]
train() client id: f_00004-1-6 loss: 0.895537  [  224/  306]
train() client id: f_00004-1-7 loss: 0.745191  [  256/  306]
train() client id: f_00004-1-8 loss: 1.055711  [  288/  306]
train() client id: f_00004-2-0 loss: 0.881095  [   32/  306]
train() client id: f_00004-2-1 loss: 0.907934  [   64/  306]
train() client id: f_00004-2-2 loss: 0.994206  [   96/  306]
train() client id: f_00004-2-3 loss: 1.017346  [  128/  306]
train() client id: f_00004-2-4 loss: 0.933065  [  160/  306]
train() client id: f_00004-2-5 loss: 0.871879  [  192/  306]
train() client id: f_00004-2-6 loss: 1.043734  [  224/  306]
train() client id: f_00004-2-7 loss: 0.793477  [  256/  306]
train() client id: f_00004-2-8 loss: 0.846666  [  288/  306]
train() client id: f_00004-3-0 loss: 0.844115  [   32/  306]
train() client id: f_00004-3-1 loss: 0.956775  [   64/  306]
train() client id: f_00004-3-2 loss: 0.897497  [   96/  306]
train() client id: f_00004-3-3 loss: 0.890923  [  128/  306]
train() client id: f_00004-3-4 loss: 0.874229  [  160/  306]
train() client id: f_00004-3-5 loss: 0.970136  [  192/  306]
train() client id: f_00004-3-6 loss: 0.919253  [  224/  306]
train() client id: f_00004-3-7 loss: 0.900669  [  256/  306]
train() client id: f_00004-3-8 loss: 0.881571  [  288/  306]
train() client id: f_00004-4-0 loss: 0.932014  [   32/  306]
train() client id: f_00004-4-1 loss: 0.834436  [   64/  306]
train() client id: f_00004-4-2 loss: 0.897415  [   96/  306]
train() client id: f_00004-4-3 loss: 0.852703  [  128/  306]
train() client id: f_00004-4-4 loss: 0.802564  [  160/  306]
train() client id: f_00004-4-5 loss: 0.947028  [  192/  306]
train() client id: f_00004-4-6 loss: 1.021302  [  224/  306]
train() client id: f_00004-4-7 loss: 0.919751  [  256/  306]
train() client id: f_00004-4-8 loss: 0.976400  [  288/  306]
train() client id: f_00004-5-0 loss: 0.851847  [   32/  306]
train() client id: f_00004-5-1 loss: 0.818287  [   64/  306]
train() client id: f_00004-5-2 loss: 0.883136  [   96/  306]
train() client id: f_00004-5-3 loss: 0.904364  [  128/  306]
train() client id: f_00004-5-4 loss: 1.033298  [  160/  306]
train() client id: f_00004-5-5 loss: 0.875918  [  192/  306]
train() client id: f_00004-5-6 loss: 0.894828  [  224/  306]
train() client id: f_00004-5-7 loss: 0.824083  [  256/  306]
train() client id: f_00004-5-8 loss: 1.090229  [  288/  306]
train() client id: f_00004-6-0 loss: 0.933348  [   32/  306]
train() client id: f_00004-6-1 loss: 0.930432  [   64/  306]
train() client id: f_00004-6-2 loss: 0.835795  [   96/  306]
train() client id: f_00004-6-3 loss: 0.826801  [  128/  306]
train() client id: f_00004-6-4 loss: 0.929367  [  160/  306]
train() client id: f_00004-6-5 loss: 0.925032  [  192/  306]
train() client id: f_00004-6-6 loss: 1.022264  [  224/  306]
train() client id: f_00004-6-7 loss: 0.875370  [  256/  306]
train() client id: f_00004-6-8 loss: 0.868288  [  288/  306]
train() client id: f_00004-7-0 loss: 0.919670  [   32/  306]
train() client id: f_00004-7-1 loss: 0.766794  [   64/  306]
train() client id: f_00004-7-2 loss: 1.044865  [   96/  306]
train() client id: f_00004-7-3 loss: 0.928597  [  128/  306]
train() client id: f_00004-7-4 loss: 0.914160  [  160/  306]
train() client id: f_00004-7-5 loss: 0.921362  [  192/  306]
train() client id: f_00004-7-6 loss: 0.806574  [  224/  306]
train() client id: f_00004-7-7 loss: 1.016585  [  256/  306]
train() client id: f_00004-7-8 loss: 0.785814  [  288/  306]
train() client id: f_00004-8-0 loss: 0.954744  [   32/  306]
train() client id: f_00004-8-1 loss: 0.910622  [   64/  306]
train() client id: f_00004-8-2 loss: 1.017341  [   96/  306]
train() client id: f_00004-8-3 loss: 0.807870  [  128/  306]
train() client id: f_00004-8-4 loss: 0.851946  [  160/  306]
train() client id: f_00004-8-5 loss: 0.887108  [  192/  306]
train() client id: f_00004-8-6 loss: 0.836592  [  224/  306]
train() client id: f_00004-8-7 loss: 1.005008  [  256/  306]
train() client id: f_00004-8-8 loss: 0.893767  [  288/  306]
train() client id: f_00004-9-0 loss: 0.883214  [   32/  306]
train() client id: f_00004-9-1 loss: 0.994314  [   64/  306]
train() client id: f_00004-9-2 loss: 0.895006  [   96/  306]
train() client id: f_00004-9-3 loss: 0.916329  [  128/  306]
train() client id: f_00004-9-4 loss: 0.940014  [  160/  306]
train() client id: f_00004-9-5 loss: 0.957225  [  192/  306]
train() client id: f_00004-9-6 loss: 0.851700  [  224/  306]
train() client id: f_00004-9-7 loss: 0.845213  [  256/  306]
train() client id: f_00004-9-8 loss: 0.940670  [  288/  306]
train() client id: f_00004-10-0 loss: 0.850171  [   32/  306]
train() client id: f_00004-10-1 loss: 0.936637  [   64/  306]
train() client id: f_00004-10-2 loss: 0.923782  [   96/  306]
train() client id: f_00004-10-3 loss: 1.004761  [  128/  306]
train() client id: f_00004-10-4 loss: 0.872946  [  160/  306]
train() client id: f_00004-10-5 loss: 0.928480  [  192/  306]
train() client id: f_00004-10-6 loss: 0.950342  [  224/  306]
train() client id: f_00004-10-7 loss: 0.802991  [  256/  306]
train() client id: f_00004-10-8 loss: 0.927915  [  288/  306]
train() client id: f_00004-11-0 loss: 1.014171  [   32/  306]
train() client id: f_00004-11-1 loss: 0.836909  [   64/  306]
train() client id: f_00004-11-2 loss: 0.763028  [   96/  306]
train() client id: f_00004-11-3 loss: 0.883945  [  128/  306]
train() client id: f_00004-11-4 loss: 1.028232  [  160/  306]
train() client id: f_00004-11-5 loss: 0.823515  [  192/  306]
train() client id: f_00004-11-6 loss: 1.048306  [  224/  306]
train() client id: f_00004-11-7 loss: 0.943028  [  256/  306]
train() client id: f_00004-11-8 loss: 0.817037  [  288/  306]
train() client id: f_00005-0-0 loss: 0.292686  [   32/  146]
train() client id: f_00005-0-1 loss: 0.893693  [   64/  146]
train() client id: f_00005-0-2 loss: 0.432865  [   96/  146]
train() client id: f_00005-0-3 loss: 0.273538  [  128/  146]
train() client id: f_00005-1-0 loss: 0.427846  [   32/  146]
train() client id: f_00005-1-1 loss: 0.446247  [   64/  146]
train() client id: f_00005-1-2 loss: 0.654347  [   96/  146]
train() client id: f_00005-1-3 loss: 0.461610  [  128/  146]
train() client id: f_00005-2-0 loss: 0.287571  [   32/  146]
train() client id: f_00005-2-1 loss: 0.406799  [   64/  146]
train() client id: f_00005-2-2 loss: 0.683135  [   96/  146]
train() client id: f_00005-2-3 loss: 0.545456  [  128/  146]
train() client id: f_00005-3-0 loss: 0.381319  [   32/  146]
train() client id: f_00005-3-1 loss: 0.633743  [   64/  146]
train() client id: f_00005-3-2 loss: 0.225091  [   96/  146]
train() client id: f_00005-3-3 loss: 0.836902  [  128/  146]
train() client id: f_00005-4-0 loss: 0.321949  [   32/  146]
train() client id: f_00005-4-1 loss: 0.543813  [   64/  146]
train() client id: f_00005-4-2 loss: 0.590742  [   96/  146]
train() client id: f_00005-4-3 loss: 0.583096  [  128/  146]
train() client id: f_00005-5-0 loss: 0.576296  [   32/  146]
train() client id: f_00005-5-1 loss: 0.326405  [   64/  146]
train() client id: f_00005-5-2 loss: 0.482100  [   96/  146]
train() client id: f_00005-5-3 loss: 0.431203  [  128/  146]
train() client id: f_00005-6-0 loss: 0.627277  [   32/  146]
train() client id: f_00005-6-1 loss: 0.519654  [   64/  146]
train() client id: f_00005-6-2 loss: 0.569654  [   96/  146]
train() client id: f_00005-6-3 loss: 0.310218  [  128/  146]
train() client id: f_00005-7-0 loss: 0.347897  [   32/  146]
train() client id: f_00005-7-1 loss: 0.458720  [   64/  146]
train() client id: f_00005-7-2 loss: 0.553469  [   96/  146]
train() client id: f_00005-7-3 loss: 0.600623  [  128/  146]
train() client id: f_00005-8-0 loss: 0.688162  [   32/  146]
train() client id: f_00005-8-1 loss: 0.445957  [   64/  146]
train() client id: f_00005-8-2 loss: 0.519203  [   96/  146]
train() client id: f_00005-8-3 loss: 0.391717  [  128/  146]
train() client id: f_00005-9-0 loss: 0.218197  [   32/  146]
train() client id: f_00005-9-1 loss: 0.568250  [   64/  146]
train() client id: f_00005-9-2 loss: 0.654510  [   96/  146]
train() client id: f_00005-9-3 loss: 0.419678  [  128/  146]
train() client id: f_00005-10-0 loss: 0.483373  [   32/  146]
train() client id: f_00005-10-1 loss: 0.220863  [   64/  146]
train() client id: f_00005-10-2 loss: 0.755653  [   96/  146]
train() client id: f_00005-10-3 loss: 0.480563  [  128/  146]
train() client id: f_00005-11-0 loss: 0.338844  [   32/  146]
train() client id: f_00005-11-1 loss: 0.624290  [   64/  146]
train() client id: f_00005-11-2 loss: 0.520452  [   96/  146]
train() client id: f_00005-11-3 loss: 0.604290  [  128/  146]
train() client id: f_00006-0-0 loss: 0.540561  [   32/   54]
train() client id: f_00006-1-0 loss: 0.596870  [   32/   54]
train() client id: f_00006-2-0 loss: 0.494897  [   32/   54]
train() client id: f_00006-3-0 loss: 0.501171  [   32/   54]
train() client id: f_00006-4-0 loss: 0.563194  [   32/   54]
train() client id: f_00006-5-0 loss: 0.599938  [   32/   54]
train() client id: f_00006-6-0 loss: 0.560881  [   32/   54]
train() client id: f_00006-7-0 loss: 0.547056  [   32/   54]
train() client id: f_00006-8-0 loss: 0.599829  [   32/   54]
train() client id: f_00006-9-0 loss: 0.560285  [   32/   54]
train() client id: f_00006-10-0 loss: 0.511470  [   32/   54]
train() client id: f_00006-11-0 loss: 0.493754  [   32/   54]
train() client id: f_00007-0-0 loss: 0.577648  [   32/  179]
train() client id: f_00007-0-1 loss: 0.688140  [   64/  179]
train() client id: f_00007-0-2 loss: 0.574657  [   96/  179]
train() client id: f_00007-0-3 loss: 0.607271  [  128/  179]
train() client id: f_00007-0-4 loss: 0.805868  [  160/  179]
train() client id: f_00007-1-0 loss: 0.522617  [   32/  179]
train() client id: f_00007-1-1 loss: 0.699704  [   64/  179]
train() client id: f_00007-1-2 loss: 0.684607  [   96/  179]
train() client id: f_00007-1-3 loss: 0.675935  [  128/  179]
train() client id: f_00007-1-4 loss: 0.599962  [  160/  179]
train() client id: f_00007-2-0 loss: 0.704066  [   32/  179]
train() client id: f_00007-2-1 loss: 0.711442  [   64/  179]
train() client id: f_00007-2-2 loss: 0.595342  [   96/  179]
train() client id: f_00007-2-3 loss: 0.599322  [  128/  179]
train() client id: f_00007-2-4 loss: 0.579524  [  160/  179]
train() client id: f_00007-3-0 loss: 0.743268  [   32/  179]
train() client id: f_00007-3-1 loss: 0.662244  [   64/  179]
train() client id: f_00007-3-2 loss: 0.607367  [   96/  179]
train() client id: f_00007-3-3 loss: 0.659090  [  128/  179]
train() client id: f_00007-3-4 loss: 0.526187  [  160/  179]
train() client id: f_00007-4-0 loss: 0.740556  [   32/  179]
train() client id: f_00007-4-1 loss: 0.553163  [   64/  179]
train() client id: f_00007-4-2 loss: 0.573147  [   96/  179]
train() client id: f_00007-4-3 loss: 0.455994  [  128/  179]
train() client id: f_00007-4-4 loss: 0.748215  [  160/  179]
train() client id: f_00007-5-0 loss: 0.566608  [   32/  179]
train() client id: f_00007-5-1 loss: 0.622832  [   64/  179]
train() client id: f_00007-5-2 loss: 0.591900  [   96/  179]
train() client id: f_00007-5-3 loss: 0.594251  [  128/  179]
train() client id: f_00007-5-4 loss: 0.596764  [  160/  179]
train() client id: f_00007-6-0 loss: 0.635669  [   32/  179]
train() client id: f_00007-6-1 loss: 0.565412  [   64/  179]
train() client id: f_00007-6-2 loss: 0.587869  [   96/  179]
train() client id: f_00007-6-3 loss: 0.561857  [  128/  179]
train() client id: f_00007-6-4 loss: 0.672906  [  160/  179]
train() client id: f_00007-7-0 loss: 0.768344  [   32/  179]
train() client id: f_00007-7-1 loss: 0.577452  [   64/  179]
train() client id: f_00007-7-2 loss: 0.475311  [   96/  179]
train() client id: f_00007-7-3 loss: 0.501043  [  128/  179]
train() client id: f_00007-7-4 loss: 0.742827  [  160/  179]
train() client id: f_00007-8-0 loss: 0.774134  [   32/  179]
train() client id: f_00007-8-1 loss: 0.636233  [   64/  179]
train() client id: f_00007-8-2 loss: 0.572021  [   96/  179]
train() client id: f_00007-8-3 loss: 0.479015  [  128/  179]
train() client id: f_00007-8-4 loss: 0.585113  [  160/  179]
train() client id: f_00007-9-0 loss: 0.470008  [   32/  179]
train() client id: f_00007-9-1 loss: 0.595821  [   64/  179]
train() client id: f_00007-9-2 loss: 0.761720  [   96/  179]
train() client id: f_00007-9-3 loss: 0.653440  [  128/  179]
train() client id: f_00007-9-4 loss: 0.569623  [  160/  179]
train() client id: f_00007-10-0 loss: 0.836906  [   32/  179]
train() client id: f_00007-10-1 loss: 0.528866  [   64/  179]
train() client id: f_00007-10-2 loss: 0.573689  [   96/  179]
train() client id: f_00007-10-3 loss: 0.576601  [  128/  179]
train() client id: f_00007-10-4 loss: 0.596313  [  160/  179]
train() client id: f_00007-11-0 loss: 0.597006  [   32/  179]
train() client id: f_00007-11-1 loss: 0.475228  [   64/  179]
train() client id: f_00007-11-2 loss: 0.625232  [   96/  179]
train() client id: f_00007-11-3 loss: 0.667113  [  128/  179]
train() client id: f_00007-11-4 loss: 0.587665  [  160/  179]
train() client id: f_00008-0-0 loss: 0.850154  [   32/  130]
train() client id: f_00008-0-1 loss: 0.850943  [   64/  130]
train() client id: f_00008-0-2 loss: 0.646074  [   96/  130]
train() client id: f_00008-0-3 loss: 0.892011  [  128/  130]
train() client id: f_00008-1-0 loss: 0.846726  [   32/  130]
train() client id: f_00008-1-1 loss: 0.727672  [   64/  130]
train() client id: f_00008-1-2 loss: 0.863002  [   96/  130]
train() client id: f_00008-1-3 loss: 0.768352  [  128/  130]
train() client id: f_00008-2-0 loss: 0.802305  [   32/  130]
train() client id: f_00008-2-1 loss: 0.826021  [   64/  130]
train() client id: f_00008-2-2 loss: 0.944228  [   96/  130]
train() client id: f_00008-2-3 loss: 0.665081  [  128/  130]
train() client id: f_00008-3-0 loss: 0.753917  [   32/  130]
train() client id: f_00008-3-1 loss: 0.923859  [   64/  130]
train() client id: f_00008-3-2 loss: 0.752022  [   96/  130]
train() client id: f_00008-3-3 loss: 0.800418  [  128/  130]
train() client id: f_00008-4-0 loss: 0.910552  [   32/  130]
train() client id: f_00008-4-1 loss: 0.691020  [   64/  130]
train() client id: f_00008-4-2 loss: 0.871247  [   96/  130]
train() client id: f_00008-4-3 loss: 0.752648  [  128/  130]
train() client id: f_00008-5-0 loss: 0.848656  [   32/  130]
train() client id: f_00008-5-1 loss: 0.753910  [   64/  130]
train() client id: f_00008-5-2 loss: 0.785101  [   96/  130]
train() client id: f_00008-5-3 loss: 0.822184  [  128/  130]
train() client id: f_00008-6-0 loss: 0.823643  [   32/  130]
train() client id: f_00008-6-1 loss: 0.757852  [   64/  130]
train() client id: f_00008-6-2 loss: 0.738826  [   96/  130]
train() client id: f_00008-6-3 loss: 0.874789  [  128/  130]
train() client id: f_00008-7-0 loss: 0.831869  [   32/  130]
train() client id: f_00008-7-1 loss: 0.875048  [   64/  130]
train() client id: f_00008-7-2 loss: 0.770729  [   96/  130]
train() client id: f_00008-7-3 loss: 0.748080  [  128/  130]
train() client id: f_00008-8-0 loss: 0.845203  [   32/  130]
train() client id: f_00008-8-1 loss: 0.767652  [   64/  130]
train() client id: f_00008-8-2 loss: 0.839945  [   96/  130]
train() client id: f_00008-8-3 loss: 0.781296  [  128/  130]
train() client id: f_00008-9-0 loss: 0.747785  [   32/  130]
train() client id: f_00008-9-1 loss: 0.864656  [   64/  130]
train() client id: f_00008-9-2 loss: 0.881858  [   96/  130]
train() client id: f_00008-9-3 loss: 0.695310  [  128/  130]
train() client id: f_00008-10-0 loss: 0.792337  [   32/  130]
train() client id: f_00008-10-1 loss: 0.816152  [   64/  130]
train() client id: f_00008-10-2 loss: 0.825407  [   96/  130]
train() client id: f_00008-10-3 loss: 0.789781  [  128/  130]
train() client id: f_00008-11-0 loss: 0.744556  [   32/  130]
train() client id: f_00008-11-1 loss: 0.897667  [   64/  130]
train() client id: f_00008-11-2 loss: 0.877664  [   96/  130]
train() client id: f_00008-11-3 loss: 0.713636  [  128/  130]
train() client id: f_00009-0-0 loss: 1.039743  [   32/  118]
train() client id: f_00009-0-1 loss: 1.195806  [   64/  118]
train() client id: f_00009-0-2 loss: 0.928174  [   96/  118]
train() client id: f_00009-1-0 loss: 0.900327  [   32/  118]
train() client id: f_00009-1-1 loss: 1.160201  [   64/  118]
train() client id: f_00009-1-2 loss: 1.058883  [   96/  118]
train() client id: f_00009-2-0 loss: 0.860930  [   32/  118]
train() client id: f_00009-2-1 loss: 1.141956  [   64/  118]
train() client id: f_00009-2-2 loss: 0.994208  [   96/  118]
train() client id: f_00009-3-0 loss: 1.052702  [   32/  118]
train() client id: f_00009-3-1 loss: 0.905907  [   64/  118]
train() client id: f_00009-3-2 loss: 0.920847  [   96/  118]
train() client id: f_00009-4-0 loss: 1.065858  [   32/  118]
train() client id: f_00009-4-1 loss: 0.892272  [   64/  118]
train() client id: f_00009-4-2 loss: 0.857672  [   96/  118]
train() client id: f_00009-5-0 loss: 0.881271  [   32/  118]
train() client id: f_00009-5-1 loss: 0.913826  [   64/  118]
train() client id: f_00009-5-2 loss: 0.865964  [   96/  118]
train() client id: f_00009-6-0 loss: 0.950180  [   32/  118]
train() client id: f_00009-6-1 loss: 0.868588  [   64/  118]
train() client id: f_00009-6-2 loss: 0.915497  [   96/  118]
train() client id: f_00009-7-0 loss: 0.962151  [   32/  118]
train() client id: f_00009-7-1 loss: 0.833397  [   64/  118]
train() client id: f_00009-7-2 loss: 0.820970  [   96/  118]
train() client id: f_00009-8-0 loss: 0.822424  [   32/  118]
train() client id: f_00009-8-1 loss: 0.863793  [   64/  118]
train() client id: f_00009-8-2 loss: 0.940612  [   96/  118]
train() client id: f_00009-9-0 loss: 0.935649  [   32/  118]
train() client id: f_00009-9-1 loss: 0.882571  [   64/  118]
train() client id: f_00009-9-2 loss: 0.760776  [   96/  118]
train() client id: f_00009-10-0 loss: 0.754364  [   32/  118]
train() client id: f_00009-10-1 loss: 0.857247  [   64/  118]
train() client id: f_00009-10-2 loss: 0.924613  [   96/  118]
train() client id: f_00009-11-0 loss: 0.686940  [   32/  118]
train() client id: f_00009-11-1 loss: 0.839103  [   64/  118]
train() client id: f_00009-11-2 loss: 1.014160  [   96/  118]
At round 26 accuracy: 0.6472148541114059
At round 26 training accuracy: 0.5814889336016097
At round 26 training loss: 0.83527848215675
update_location
xs = [  -3.9056584     4.20031788  150.00902392   18.81129433    0.97929623
    3.95640986 -112.44319194  -91.32485185  134.66397685  -77.06087855]
ys = [ 142.5879595   125.55583871    1.32061395 -112.45517586  104.35018685
   87.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [174.20269906 160.56746652 180.28990897 151.65761231 144.53345812
 133.14269882 150.50037194 135.42859676 168.65069414 126.31069179]
dists_bs = [174.13734262 185.89642012 368.50426696 346.68904791 189.63150793
 199.06473556 188.31885006 193.23401585 347.38493983 197.05979992]
uav_gains = [2.47904434e-11 3.05317297e-11 2.26669887e-11 3.52625235e-11
 3.97925071e-11 4.88782381e-11 3.59484763e-11 4.68391522e-11
 2.69445184e-11 5.57642431e-11]
bs_gains = [5.87214444e-11 4.89029672e-11 7.19871577e-12 8.54007614e-12
 4.62535094e-11 4.03747604e-11 4.71619185e-11 4.38793376e-11
 8.49226080e-12 4.15355131e-11]
Round 27
-------------------------------
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.428454   15.42356035  7.31658542  2.62856167 17.79004913  8.56531288
  3.26241587 10.46431357  7.71314909  6.94940181]
obj_prev = 87.54180377366474
eta_min = 3.2918080146347544e-13	eta_max = 0.9246953843511696
af = 18.488129690050464	bf = 1.525402092626451	zeta = 20.336942659055513	eta = 0.909090909090909
af = 18.488129690050464	bf = 1.525402092626451	zeta = 36.16259963393917	eta = 0.5112500173438599
af = 18.488129690050464	bf = 1.525402092626451	zeta = 28.49928344660974	eta = 0.6487226152435002
af = 18.488129690050464	bf = 1.525402092626451	zeta = 27.11857262246232	eta = 0.6817515784269833
af = 18.488129690050464	bf = 1.525402092626451	zeta = 27.048110971781785	eta = 0.6835275745998821
af = 18.488129690050464	bf = 1.525402092626451	zeta = 27.047913433089164	eta = 0.6835325665983885
eta = 0.6835325665983885
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [0.03139638 0.06603207 0.03089802 0.01071464 0.07624835 0.03637993
 0.01345559 0.04460279 0.03239309 0.02940296]
ene_total = [2.38132415 4.38338617 2.36243079 1.10502586 4.99963694 2.62710832
 1.26705354 3.1051769  2.60980459 2.20696617]
ti_comp = [0.39480521 0.40653023 0.39297956 0.40131289 0.40569068 0.40355948
 0.40164054 0.40588199 0.36658327 0.40401382]
ti_coms = [0.08405931 0.07233429 0.08588495 0.07755163 0.07317384 0.07530504
 0.07722398 0.07298253 0.11228124 0.07485069]
t_total = [28.6498867 28.6498867 28.6498867 28.6498867 28.6498867 28.6498867
 28.6498867 28.6498867 28.6498867 28.6498867]
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [1.24094611e-05 1.08882722e-04 1.19380162e-05 4.77360184e-07
 1.68337651e-04 1.84778100e-05 9.43872309e-07 3.36640665e-05
 1.58085418e-05 9.73331126e-06]
ene_total = [0.50365913 0.43928163 0.51455356 0.4640106  0.44786169 0.45164645
 0.46207824 0.43865975 0.67271096 0.448405  ]
optimize_network iter = 0 obj = 4.842867033053818
eta = 0.6835325665983885
freqs = [39761858.95611374 81214212.7965591  39312506.23491908 13349480.34302717
 93973501.61782414 45073814.47997239 16750787.69960297 54945517.04012107
 44182442.51361968 36388553.66776452]
eta_min = 0.6835325665983994	eta_max = 0.6835325665983844
af = 0.018943411267469068	bf = 1.525402092626451	zeta = 0.020837752394215977	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [3.10856006e-06 2.72750345e-05 2.99046350e-06 1.19578344e-07
 4.21684466e-05 4.62867660e-06 2.36439257e-07 8.43282168e-06
 3.96002704e-06 2.43818667e-06]
ene_total = [1.75137172 1.512205   1.78937036 1.61521218 1.53279247 1.56936094
 1.60841252 1.52178181 2.33933449 1.55944202]
ti_comp = [0.39480521 0.40653023 0.39297956 0.40131289 0.40569068 0.40355948
 0.40164054 0.40588199 0.36658327 0.40401382]
ti_coms = [0.08405931 0.07233429 0.08588495 0.07755163 0.07317384 0.07530504
 0.07722398 0.07298253 0.11228124 0.07485069]
t_total = [28.6498867 28.6498867 28.6498867 28.6498867 28.6498867 28.6498867
 28.6498867 28.6498867 28.6498867 28.6498867]
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [1.24094611e-05 1.08882722e-04 1.19380162e-05 4.77360184e-07
 1.68337651e-04 1.84778100e-05 9.43872309e-07 3.36640665e-05
 1.58085418e-05 9.73331126e-06]
ene_total = [0.50365913 0.43928163 0.51455356 0.4640106  0.44786169 0.45164645
 0.46207824 0.43865975 0.67271096 0.448405  ]
optimize_network iter = 1 obj = 4.842867033053756
eta = 0.6835325665983844
freqs = [39761858.95611373 81214212.79655911 39312506.23491906 13349480.34302717
 93973501.61782415 45073814.4799724  16750787.69960297 54945517.04012109
 44182442.51361962 36388553.66776453]
Done!
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [1.19524010e-05 1.04872399e-04 1.14983202e-05 4.59778254e-07
 1.62137509e-04 1.77972431e-05 9.09107999e-07 3.24241657e-05
 1.52262882e-05 9.37481801e-06]
ene_total = [0.00841788 0.0073383  0.00859999 0.00775562 0.00747952 0.0075483
 0.00772331 0.00733068 0.01124335 0.00749444]
At round 27 energy consumption: 0.08093140167794016
At round 27 eta: 0.6835325665983844
At round 27 a_n: 18.59131914111349
At round 27 local rounds: 12.458880266672946
At round 27 global rounds: 59.82879433933611
gradient difference: 0.43182167410850525
train() client id: f_00000-0-0 loss: 1.282500  [   32/  126]
train() client id: f_00000-0-1 loss: 1.326460  [   64/  126]
train() client id: f_00000-0-2 loss: 1.198815  [   96/  126]
train() client id: f_00000-1-0 loss: 1.139557  [   32/  126]
train() client id: f_00000-1-1 loss: 1.153749  [   64/  126]
train() client id: f_00000-1-2 loss: 1.160356  [   96/  126]
train() client id: f_00000-2-0 loss: 1.099063  [   32/  126]
train() client id: f_00000-2-1 loss: 1.059733  [   64/  126]
train() client id: f_00000-2-2 loss: 1.077552  [   96/  126]
train() client id: f_00000-3-0 loss: 1.012430  [   32/  126]
train() client id: f_00000-3-1 loss: 0.908766  [   64/  126]
train() client id: f_00000-3-2 loss: 1.042172  [   96/  126]
train() client id: f_00000-4-0 loss: 0.936311  [   32/  126]
train() client id: f_00000-4-1 loss: 0.991393  [   64/  126]
train() client id: f_00000-4-2 loss: 0.913898  [   96/  126]
train() client id: f_00000-5-0 loss: 0.908343  [   32/  126]
train() client id: f_00000-5-1 loss: 0.956742  [   64/  126]
train() client id: f_00000-5-2 loss: 0.838742  [   96/  126]
train() client id: f_00000-6-0 loss: 0.971975  [   32/  126]
train() client id: f_00000-6-1 loss: 0.893432  [   64/  126]
train() client id: f_00000-6-2 loss: 0.861934  [   96/  126]
train() client id: f_00000-7-0 loss: 0.891536  [   32/  126]
train() client id: f_00000-7-1 loss: 0.796985  [   64/  126]
train() client id: f_00000-7-2 loss: 0.913212  [   96/  126]
train() client id: f_00000-8-0 loss: 0.813919  [   32/  126]
train() client id: f_00000-8-1 loss: 0.905460  [   64/  126]
train() client id: f_00000-8-2 loss: 0.883059  [   96/  126]
train() client id: f_00000-9-0 loss: 0.856403  [   32/  126]
train() client id: f_00000-9-1 loss: 0.815011  [   64/  126]
train() client id: f_00000-9-2 loss: 0.903876  [   96/  126]
train() client id: f_00000-10-0 loss: 0.884220  [   32/  126]
train() client id: f_00000-10-1 loss: 0.811072  [   64/  126]
train() client id: f_00000-10-2 loss: 0.787796  [   96/  126]
train() client id: f_00000-11-0 loss: 0.737964  [   32/  126]
train() client id: f_00000-11-1 loss: 0.878944  [   64/  126]
train() client id: f_00000-11-2 loss: 0.887540  [   96/  126]
train() client id: f_00001-0-0 loss: 0.422219  [   32/  265]
train() client id: f_00001-0-1 loss: 0.481251  [   64/  265]
train() client id: f_00001-0-2 loss: 0.431990  [   96/  265]
train() client id: f_00001-0-3 loss: 0.659881  [  128/  265]
train() client id: f_00001-0-4 loss: 0.511425  [  160/  265]
train() client id: f_00001-0-5 loss: 0.512413  [  192/  265]
train() client id: f_00001-0-6 loss: 0.633596  [  224/  265]
train() client id: f_00001-0-7 loss: 0.561231  [  256/  265]
train() client id: f_00001-1-0 loss: 0.563259  [   32/  265]
train() client id: f_00001-1-1 loss: 0.471034  [   64/  265]
train() client id: f_00001-1-2 loss: 0.517810  [   96/  265]
train() client id: f_00001-1-3 loss: 0.455997  [  128/  265]
train() client id: f_00001-1-4 loss: 0.445658  [  160/  265]
train() client id: f_00001-1-5 loss: 0.518357  [  192/  265]
train() client id: f_00001-1-6 loss: 0.556630  [  224/  265]
train() client id: f_00001-1-7 loss: 0.581758  [  256/  265]
train() client id: f_00001-2-0 loss: 0.595871  [   32/  265]
train() client id: f_00001-2-1 loss: 0.495208  [   64/  265]
train() client id: f_00001-2-2 loss: 0.467155  [   96/  265]
train() client id: f_00001-2-3 loss: 0.424505  [  128/  265]
train() client id: f_00001-2-4 loss: 0.540495  [  160/  265]
train() client id: f_00001-2-5 loss: 0.709867  [  192/  265]
train() client id: f_00001-2-6 loss: 0.452273  [  224/  265]
train() client id: f_00001-2-7 loss: 0.428414  [  256/  265]
train() client id: f_00001-3-0 loss: 0.581694  [   32/  265]
train() client id: f_00001-3-1 loss: 0.428099  [   64/  265]
train() client id: f_00001-3-2 loss: 0.422708  [   96/  265]
train() client id: f_00001-3-3 loss: 0.485472  [  128/  265]
train() client id: f_00001-3-4 loss: 0.482084  [  160/  265]
train() client id: f_00001-3-5 loss: 0.532397  [  192/  265]
train() client id: f_00001-3-6 loss: 0.538588  [  224/  265]
train() client id: f_00001-3-7 loss: 0.607579  [  256/  265]
train() client id: f_00001-4-0 loss: 0.510606  [   32/  265]
train() client id: f_00001-4-1 loss: 0.417395  [   64/  265]
train() client id: f_00001-4-2 loss: 0.423729  [   96/  265]
train() client id: f_00001-4-3 loss: 0.439311  [  128/  265]
train() client id: f_00001-4-4 loss: 0.699535  [  160/  265]
train() client id: f_00001-4-5 loss: 0.437555  [  192/  265]
train() client id: f_00001-4-6 loss: 0.581732  [  224/  265]
train() client id: f_00001-4-7 loss: 0.472073  [  256/  265]
train() client id: f_00001-5-0 loss: 0.417955  [   32/  265]
train() client id: f_00001-5-1 loss: 0.408032  [   64/  265]
train() client id: f_00001-5-2 loss: 0.422850  [   96/  265]
train() client id: f_00001-5-3 loss: 0.586279  [  128/  265]
train() client id: f_00001-5-4 loss: 0.568948  [  160/  265]
train() client id: f_00001-5-5 loss: 0.482605  [  192/  265]
train() client id: f_00001-5-6 loss: 0.632291  [  224/  265]
train() client id: f_00001-5-7 loss: 0.481948  [  256/  265]
train() client id: f_00001-6-0 loss: 0.447687  [   32/  265]
train() client id: f_00001-6-1 loss: 0.414554  [   64/  265]
train() client id: f_00001-6-2 loss: 0.441121  [   96/  265]
train() client id: f_00001-6-3 loss: 0.530941  [  128/  265]
train() client id: f_00001-6-4 loss: 0.694758  [  160/  265]
train() client id: f_00001-6-5 loss: 0.449763  [  192/  265]
train() client id: f_00001-6-6 loss: 0.417839  [  224/  265]
train() client id: f_00001-6-7 loss: 0.599934  [  256/  265]
train() client id: f_00001-7-0 loss: 0.521810  [   32/  265]
train() client id: f_00001-7-1 loss: 0.648758  [   64/  265]
train() client id: f_00001-7-2 loss: 0.462192  [   96/  265]
train() client id: f_00001-7-3 loss: 0.495818  [  128/  265]
train() client id: f_00001-7-4 loss: 0.489461  [  160/  265]
train() client id: f_00001-7-5 loss: 0.448556  [  192/  265]
train() client id: f_00001-7-6 loss: 0.489764  [  224/  265]
train() client id: f_00001-7-7 loss: 0.471910  [  256/  265]
train() client id: f_00001-8-0 loss: 0.512270  [   32/  265]
train() client id: f_00001-8-1 loss: 0.561595  [   64/  265]
train() client id: f_00001-8-2 loss: 0.419701  [   96/  265]
train() client id: f_00001-8-3 loss: 0.481605  [  128/  265]
train() client id: f_00001-8-4 loss: 0.512020  [  160/  265]
train() client id: f_00001-8-5 loss: 0.411648  [  192/  265]
train() client id: f_00001-8-6 loss: 0.578460  [  224/  265]
train() client id: f_00001-8-7 loss: 0.546074  [  256/  265]
train() client id: f_00001-9-0 loss: 0.532412  [   32/  265]
train() client id: f_00001-9-1 loss: 0.480385  [   64/  265]
train() client id: f_00001-9-2 loss: 0.428802  [   96/  265]
train() client id: f_00001-9-3 loss: 0.562132  [  128/  265]
train() client id: f_00001-9-4 loss: 0.535348  [  160/  265]
train() client id: f_00001-9-5 loss: 0.395263  [  192/  265]
train() client id: f_00001-9-6 loss: 0.559647  [  224/  265]
train() client id: f_00001-9-7 loss: 0.531322  [  256/  265]
train() client id: f_00001-10-0 loss: 0.604098  [   32/  265]
train() client id: f_00001-10-1 loss: 0.463367  [   64/  265]
train() client id: f_00001-10-2 loss: 0.544896  [   96/  265]
train() client id: f_00001-10-3 loss: 0.567093  [  128/  265]
train() client id: f_00001-10-4 loss: 0.406624  [  160/  265]
train() client id: f_00001-10-5 loss: 0.390222  [  192/  265]
train() client id: f_00001-10-6 loss: 0.506804  [  224/  265]
train() client id: f_00001-10-7 loss: 0.478234  [  256/  265]
train() client id: f_00001-11-0 loss: 0.608132  [   32/  265]
train() client id: f_00001-11-1 loss: 0.486298  [   64/  265]
train() client id: f_00001-11-2 loss: 0.407043  [   96/  265]
train() client id: f_00001-11-3 loss: 0.510214  [  128/  265]
train() client id: f_00001-11-4 loss: 0.460548  [  160/  265]
train() client id: f_00001-11-5 loss: 0.419104  [  192/  265]
train() client id: f_00001-11-6 loss: 0.597099  [  224/  265]
train() client id: f_00001-11-7 loss: 0.467738  [  256/  265]
train() client id: f_00002-0-0 loss: 1.263948  [   32/  124]
train() client id: f_00002-0-1 loss: 1.037870  [   64/  124]
train() client id: f_00002-0-2 loss: 0.965786  [   96/  124]
train() client id: f_00002-1-0 loss: 1.078493  [   32/  124]
train() client id: f_00002-1-1 loss: 1.030912  [   64/  124]
train() client id: f_00002-1-2 loss: 1.065398  [   96/  124]
train() client id: f_00002-2-0 loss: 1.062247  [   32/  124]
train() client id: f_00002-2-1 loss: 0.994916  [   64/  124]
train() client id: f_00002-2-2 loss: 1.163129  [   96/  124]
train() client id: f_00002-3-0 loss: 1.202776  [   32/  124]
train() client id: f_00002-3-1 loss: 1.077318  [   64/  124]
train() client id: f_00002-3-2 loss: 0.760587  [   96/  124]
train() client id: f_00002-4-0 loss: 0.952494  [   32/  124]
train() client id: f_00002-4-1 loss: 0.934362  [   64/  124]
train() client id: f_00002-4-2 loss: 1.023268  [   96/  124]
train() client id: f_00002-5-0 loss: 0.894649  [   32/  124]
train() client id: f_00002-5-1 loss: 1.107954  [   64/  124]
train() client id: f_00002-5-2 loss: 0.843048  [   96/  124]
train() client id: f_00002-6-0 loss: 0.865024  [   32/  124]
train() client id: f_00002-6-1 loss: 1.049247  [   64/  124]
train() client id: f_00002-6-2 loss: 1.088968  [   96/  124]
train() client id: f_00002-7-0 loss: 0.742697  [   32/  124]
train() client id: f_00002-7-1 loss: 0.992148  [   64/  124]
train() client id: f_00002-7-2 loss: 1.134616  [   96/  124]
train() client id: f_00002-8-0 loss: 0.961944  [   32/  124]
train() client id: f_00002-8-1 loss: 0.892985  [   64/  124]
train() client id: f_00002-8-2 loss: 0.951542  [   96/  124]
train() client id: f_00002-9-0 loss: 0.930884  [   32/  124]
train() client id: f_00002-9-1 loss: 0.942855  [   64/  124]
train() client id: f_00002-9-2 loss: 1.052785  [   96/  124]
train() client id: f_00002-10-0 loss: 0.919011  [   32/  124]
train() client id: f_00002-10-1 loss: 0.962163  [   64/  124]
train() client id: f_00002-10-2 loss: 0.953095  [   96/  124]
train() client id: f_00002-11-0 loss: 1.070512  [   32/  124]
train() client id: f_00002-11-1 loss: 0.754069  [   64/  124]
train() client id: f_00002-11-2 loss: 0.895947  [   96/  124]
train() client id: f_00003-0-0 loss: 0.939547  [   32/   43]
train() client id: f_00003-1-0 loss: 1.003812  [   32/   43]
train() client id: f_00003-2-0 loss: 0.995016  [   32/   43]
train() client id: f_00003-3-0 loss: 0.857032  [   32/   43]
train() client id: f_00003-4-0 loss: 0.807086  [   32/   43]
train() client id: f_00003-5-0 loss: 0.863883  [   32/   43]
train() client id: f_00003-6-0 loss: 0.931642  [   32/   43]
train() client id: f_00003-7-0 loss: 0.842156  [   32/   43]
train() client id: f_00003-8-0 loss: 0.869572  [   32/   43]
train() client id: f_00003-9-0 loss: 0.828330  [   32/   43]
train() client id: f_00003-10-0 loss: 0.980862  [   32/   43]
train() client id: f_00003-11-0 loss: 0.803080  [   32/   43]
train() client id: f_00004-0-0 loss: 0.961777  [   32/  306]
train() client id: f_00004-0-1 loss: 0.940587  [   64/  306]
train() client id: f_00004-0-2 loss: 1.000776  [   96/  306]
train() client id: f_00004-0-3 loss: 0.903476  [  128/  306]
train() client id: f_00004-0-4 loss: 1.061737  [  160/  306]
train() client id: f_00004-0-5 loss: 0.865227  [  192/  306]
train() client id: f_00004-0-6 loss: 0.884591  [  224/  306]
train() client id: f_00004-0-7 loss: 0.869929  [  256/  306]
train() client id: f_00004-0-8 loss: 0.979897  [  288/  306]
train() client id: f_00004-1-0 loss: 0.909489  [   32/  306]
train() client id: f_00004-1-1 loss: 1.022166  [   64/  306]
train() client id: f_00004-1-2 loss: 0.935916  [   96/  306]
train() client id: f_00004-1-3 loss: 0.979969  [  128/  306]
train() client id: f_00004-1-4 loss: 0.911694  [  160/  306]
train() client id: f_00004-1-5 loss: 0.917195  [  192/  306]
train() client id: f_00004-1-6 loss: 0.961929  [  224/  306]
train() client id: f_00004-1-7 loss: 0.957542  [  256/  306]
train() client id: f_00004-1-8 loss: 0.863662  [  288/  306]
train() client id: f_00004-2-0 loss: 1.044855  [   32/  306]
train() client id: f_00004-2-1 loss: 0.844114  [   64/  306]
train() client id: f_00004-2-2 loss: 1.023787  [   96/  306]
train() client id: f_00004-2-3 loss: 0.906714  [  128/  306]
train() client id: f_00004-2-4 loss: 0.834427  [  160/  306]
train() client id: f_00004-2-5 loss: 0.994823  [  192/  306]
train() client id: f_00004-2-6 loss: 0.862307  [  224/  306]
train() client id: f_00004-2-7 loss: 0.964264  [  256/  306]
train() client id: f_00004-2-8 loss: 0.874614  [  288/  306]
train() client id: f_00004-3-0 loss: 0.913605  [   32/  306]
train() client id: f_00004-3-1 loss: 1.068657  [   64/  306]
train() client id: f_00004-3-2 loss: 0.908242  [   96/  306]
train() client id: f_00004-3-3 loss: 0.806352  [  128/  306]
train() client id: f_00004-3-4 loss: 0.850239  [  160/  306]
train() client id: f_00004-3-5 loss: 0.815756  [  192/  306]
train() client id: f_00004-3-6 loss: 1.000087  [  224/  306]
train() client id: f_00004-3-7 loss: 0.931437  [  256/  306]
train() client id: f_00004-3-8 loss: 1.060447  [  288/  306]
train() client id: f_00004-4-0 loss: 0.924945  [   32/  306]
train() client id: f_00004-4-1 loss: 0.928727  [   64/  306]
train() client id: f_00004-4-2 loss: 0.968313  [   96/  306]
train() client id: f_00004-4-3 loss: 0.910777  [  128/  306]
train() client id: f_00004-4-4 loss: 1.052248  [  160/  306]
train() client id: f_00004-4-5 loss: 0.842451  [  192/  306]
train() client id: f_00004-4-6 loss: 0.899805  [  224/  306]
train() client id: f_00004-4-7 loss: 0.958276  [  256/  306]
train() client id: f_00004-4-8 loss: 0.954324  [  288/  306]
train() client id: f_00004-5-0 loss: 0.832045  [   32/  306]
train() client id: f_00004-5-1 loss: 0.880025  [   64/  306]
train() client id: f_00004-5-2 loss: 0.899508  [   96/  306]
train() client id: f_00004-5-3 loss: 0.995854  [  128/  306]
train() client id: f_00004-5-4 loss: 1.018883  [  160/  306]
train() client id: f_00004-5-5 loss: 0.955797  [  192/  306]
train() client id: f_00004-5-6 loss: 1.060238  [  224/  306]
train() client id: f_00004-5-7 loss: 0.909358  [  256/  306]
train() client id: f_00004-5-8 loss: 0.905117  [  288/  306]
train() client id: f_00004-6-0 loss: 0.870544  [   32/  306]
train() client id: f_00004-6-1 loss: 0.977400  [   64/  306]
train() client id: f_00004-6-2 loss: 0.906501  [   96/  306]
train() client id: f_00004-6-3 loss: 0.909176  [  128/  306]
train() client id: f_00004-6-4 loss: 0.929715  [  160/  306]
train() client id: f_00004-6-5 loss: 0.831937  [  192/  306]
train() client id: f_00004-6-6 loss: 0.894446  [  224/  306]
train() client id: f_00004-6-7 loss: 1.043272  [  256/  306]
train() client id: f_00004-6-8 loss: 0.957627  [  288/  306]
train() client id: f_00004-7-0 loss: 0.925023  [   32/  306]
train() client id: f_00004-7-1 loss: 0.865835  [   64/  306]
train() client id: f_00004-7-2 loss: 0.872233  [   96/  306]
train() client id: f_00004-7-3 loss: 0.944713  [  128/  306]
train() client id: f_00004-7-4 loss: 1.142386  [  160/  306]
train() client id: f_00004-7-5 loss: 0.876428  [  192/  306]
train() client id: f_00004-7-6 loss: 0.991694  [  224/  306]
train() client id: f_00004-7-7 loss: 0.996499  [  256/  306]
train() client id: f_00004-7-8 loss: 0.795258  [  288/  306]
train() client id: f_00004-8-0 loss: 0.778785  [   32/  306]
train() client id: f_00004-8-1 loss: 0.918970  [   64/  306]
train() client id: f_00004-8-2 loss: 0.904928  [   96/  306]
train() client id: f_00004-8-3 loss: 0.905221  [  128/  306]
train() client id: f_00004-8-4 loss: 1.002449  [  160/  306]
train() client id: f_00004-8-5 loss: 0.936874  [  192/  306]
train() client id: f_00004-8-6 loss: 0.757620  [  224/  306]
train() client id: f_00004-8-7 loss: 1.111223  [  256/  306]
train() client id: f_00004-8-8 loss: 1.021913  [  288/  306]
train() client id: f_00004-9-0 loss: 0.860542  [   32/  306]
train() client id: f_00004-9-1 loss: 0.932336  [   64/  306]
train() client id: f_00004-9-2 loss: 0.920552  [   96/  306]
train() client id: f_00004-9-3 loss: 0.906266  [  128/  306]
train() client id: f_00004-9-4 loss: 0.871161  [  160/  306]
train() client id: f_00004-9-5 loss: 0.991598  [  192/  306]
train() client id: f_00004-9-6 loss: 0.910100  [  224/  306]
train() client id: f_00004-9-7 loss: 0.943910  [  256/  306]
train() client id: f_00004-9-8 loss: 1.005053  [  288/  306]
train() client id: f_00004-10-0 loss: 0.910753  [   32/  306]
train() client id: f_00004-10-1 loss: 0.967793  [   64/  306]
train() client id: f_00004-10-2 loss: 0.943139  [   96/  306]
train() client id: f_00004-10-3 loss: 0.880482  [  128/  306]
train() client id: f_00004-10-4 loss: 0.909606  [  160/  306]
train() client id: f_00004-10-5 loss: 1.018500  [  192/  306]
train() client id: f_00004-10-6 loss: 0.892220  [  224/  306]
train() client id: f_00004-10-7 loss: 0.872579  [  256/  306]
train() client id: f_00004-10-8 loss: 1.024309  [  288/  306]
train() client id: f_00004-11-0 loss: 0.873071  [   32/  306]
train() client id: f_00004-11-1 loss: 0.886966  [   64/  306]
train() client id: f_00004-11-2 loss: 0.908012  [   96/  306]
train() client id: f_00004-11-3 loss: 0.953461  [  128/  306]
train() client id: f_00004-11-4 loss: 0.940424  [  160/  306]
train() client id: f_00004-11-5 loss: 0.838161  [  192/  306]
train() client id: f_00004-11-6 loss: 1.036099  [  224/  306]
train() client id: f_00004-11-7 loss: 0.926210  [  256/  306]
train() client id: f_00004-11-8 loss: 0.971536  [  288/  306]
train() client id: f_00005-0-0 loss: 0.650398  [   32/  146]
train() client id: f_00005-0-1 loss: 0.546275  [   64/  146]
train() client id: f_00005-0-2 loss: 0.470291  [   96/  146]
train() client id: f_00005-0-3 loss: 0.654286  [  128/  146]
train() client id: f_00005-1-0 loss: 0.629435  [   32/  146]
train() client id: f_00005-1-1 loss: 0.363303  [   64/  146]
train() client id: f_00005-1-2 loss: 0.479266  [   96/  146]
train() client id: f_00005-1-3 loss: 0.576447  [  128/  146]
train() client id: f_00005-2-0 loss: 0.543583  [   32/  146]
train() client id: f_00005-2-1 loss: 0.564785  [   64/  146]
train() client id: f_00005-2-2 loss: 0.383897  [   96/  146]
train() client id: f_00005-2-3 loss: 0.480657  [  128/  146]
train() client id: f_00005-3-0 loss: 0.584284  [   32/  146]
train() client id: f_00005-3-1 loss: 0.773903  [   64/  146]
train() client id: f_00005-3-2 loss: 0.451778  [   96/  146]
train() client id: f_00005-3-3 loss: 0.405452  [  128/  146]
train() client id: f_00005-4-0 loss: 0.519736  [   32/  146]
train() client id: f_00005-4-1 loss: 0.479875  [   64/  146]
train() client id: f_00005-4-2 loss: 0.650993  [   96/  146]
train() client id: f_00005-4-3 loss: 0.503263  [  128/  146]
train() client id: f_00005-5-0 loss: 0.815337  [   32/  146]
train() client id: f_00005-5-1 loss: 0.446218  [   64/  146]
train() client id: f_00005-5-2 loss: 0.392351  [   96/  146]
train() client id: f_00005-5-3 loss: 0.384142  [  128/  146]
train() client id: f_00005-6-0 loss: 0.564822  [   32/  146]
train() client id: f_00005-6-1 loss: 0.493066  [   64/  146]
train() client id: f_00005-6-2 loss: 0.326277  [   96/  146]
train() client id: f_00005-6-3 loss: 0.638310  [  128/  146]
train() client id: f_00005-7-0 loss: 0.685674  [   32/  146]
train() client id: f_00005-7-1 loss: 0.387177  [   64/  146]
train() client id: f_00005-7-2 loss: 0.494930  [   96/  146]
train() client id: f_00005-7-3 loss: 0.480490  [  128/  146]
train() client id: f_00005-8-0 loss: 0.338908  [   32/  146]
train() client id: f_00005-8-1 loss: 0.670574  [   64/  146]
train() client id: f_00005-8-2 loss: 0.462072  [   96/  146]
train() client id: f_00005-8-3 loss: 0.578775  [  128/  146]
train() client id: f_00005-9-0 loss: 0.842466  [   32/  146]
train() client id: f_00005-9-1 loss: 0.405134  [   64/  146]
train() client id: f_00005-9-2 loss: 0.433332  [   96/  146]
train() client id: f_00005-9-3 loss: 0.347162  [  128/  146]
train() client id: f_00005-10-0 loss: 0.496591  [   32/  146]
train() client id: f_00005-10-1 loss: 0.367245  [   64/  146]
train() client id: f_00005-10-2 loss: 0.472016  [   96/  146]
train() client id: f_00005-10-3 loss: 0.518932  [  128/  146]
train() client id: f_00005-11-0 loss: 0.364195  [   32/  146]
train() client id: f_00005-11-1 loss: 0.448254  [   64/  146]
train() client id: f_00005-11-2 loss: 0.573169  [   96/  146]
train() client id: f_00005-11-3 loss: 0.675790  [  128/  146]
train() client id: f_00006-0-0 loss: 0.624000  [   32/   54]
train() client id: f_00006-1-0 loss: 0.640168  [   32/   54]
train() client id: f_00006-2-0 loss: 0.632224  [   32/   54]
train() client id: f_00006-3-0 loss: 0.579987  [   32/   54]
train() client id: f_00006-4-0 loss: 0.514269  [   32/   54]
train() client id: f_00006-5-0 loss: 0.631289  [   32/   54]
train() client id: f_00006-6-0 loss: 0.629988  [   32/   54]
train() client id: f_00006-7-0 loss: 0.565545  [   32/   54]
train() client id: f_00006-8-0 loss: 0.583835  [   32/   54]
train() client id: f_00006-9-0 loss: 0.567498  [   32/   54]
train() client id: f_00006-10-0 loss: 0.637142  [   32/   54]
train() client id: f_00006-11-0 loss: 0.518949  [   32/   54]
train() client id: f_00007-0-0 loss: 0.709676  [   32/  179]
train() client id: f_00007-0-1 loss: 0.515916  [   64/  179]
train() client id: f_00007-0-2 loss: 0.539674  [   96/  179]
train() client id: f_00007-0-3 loss: 0.578706  [  128/  179]
train() client id: f_00007-0-4 loss: 0.792784  [  160/  179]
train() client id: f_00007-1-0 loss: 0.674587  [   32/  179]
train() client id: f_00007-1-1 loss: 0.661839  [   64/  179]
train() client id: f_00007-1-2 loss: 0.487209  [   96/  179]
train() client id: f_00007-1-3 loss: 0.601718  [  128/  179]
train() client id: f_00007-1-4 loss: 0.728744  [  160/  179]
train() client id: f_00007-2-0 loss: 0.562927  [   32/  179]
train() client id: f_00007-2-1 loss: 0.623654  [   64/  179]
train() client id: f_00007-2-2 loss: 0.541787  [   96/  179]
train() client id: f_00007-2-3 loss: 0.592261  [  128/  179]
train() client id: f_00007-2-4 loss: 0.726272  [  160/  179]
train() client id: f_00007-3-0 loss: 0.452336  [   32/  179]
train() client id: f_00007-3-1 loss: 0.569935  [   64/  179]
train() client id: f_00007-3-2 loss: 0.638314  [   96/  179]
train() client id: f_00007-3-3 loss: 0.726786  [  128/  179]
train() client id: f_00007-3-4 loss: 0.554147  [  160/  179]
train() client id: f_00007-4-0 loss: 0.552154  [   32/  179]
train() client id: f_00007-4-1 loss: 0.746669  [   64/  179]
train() client id: f_00007-4-2 loss: 0.668002  [   96/  179]
train() client id: f_00007-4-3 loss: 0.571787  [  128/  179]
train() client id: f_00007-4-4 loss: 0.557376  [  160/  179]
train() client id: f_00007-5-0 loss: 0.573540  [   32/  179]
train() client id: f_00007-5-1 loss: 0.569889  [   64/  179]
train() client id: f_00007-5-2 loss: 0.514148  [   96/  179]
train() client id: f_00007-5-3 loss: 0.679386  [  128/  179]
train() client id: f_00007-5-4 loss: 0.642492  [  160/  179]
train() client id: f_00007-6-0 loss: 0.636397  [   32/  179]
train() client id: f_00007-6-1 loss: 0.640141  [   64/  179]
train() client id: f_00007-6-2 loss: 0.721914  [   96/  179]
train() client id: f_00007-6-3 loss: 0.433978  [  128/  179]
train() client id: f_00007-6-4 loss: 0.653359  [  160/  179]
train() client id: f_00007-7-0 loss: 0.545735  [   32/  179]
train() client id: f_00007-7-1 loss: 0.582879  [   64/  179]
train() client id: f_00007-7-2 loss: 0.499246  [   96/  179]
train() client id: f_00007-7-3 loss: 0.620657  [  128/  179]
train() client id: f_00007-7-4 loss: 0.648296  [  160/  179]
train() client id: f_00007-8-0 loss: 0.650168  [   32/  179]
train() client id: f_00007-8-1 loss: 0.461651  [   64/  179]
train() client id: f_00007-8-2 loss: 0.805003  [   96/  179]
train() client id: f_00007-8-3 loss: 0.547405  [  128/  179]
train() client id: f_00007-8-4 loss: 0.553599  [  160/  179]
train() client id: f_00007-9-0 loss: 0.511606  [   32/  179]
train() client id: f_00007-9-1 loss: 0.648005  [   64/  179]
train() client id: f_00007-9-2 loss: 0.503707  [   96/  179]
train() client id: f_00007-9-3 loss: 0.531073  [  128/  179]
train() client id: f_00007-9-4 loss: 0.789744  [  160/  179]
train() client id: f_00007-10-0 loss: 0.523601  [   32/  179]
train() client id: f_00007-10-1 loss: 0.533320  [   64/  179]
train() client id: f_00007-10-2 loss: 0.836980  [   96/  179]
train() client id: f_00007-10-3 loss: 0.463474  [  128/  179]
train() client id: f_00007-10-4 loss: 0.465913  [  160/  179]
train() client id: f_00007-11-0 loss: 0.434454  [   32/  179]
train() client id: f_00007-11-1 loss: 0.556952  [   64/  179]
train() client id: f_00007-11-2 loss: 0.637815  [   96/  179]
train() client id: f_00007-11-3 loss: 0.702424  [  128/  179]
train() client id: f_00007-11-4 loss: 0.484283  [  160/  179]
train() client id: f_00008-0-0 loss: 0.926503  [   32/  130]
train() client id: f_00008-0-1 loss: 0.839781  [   64/  130]
train() client id: f_00008-0-2 loss: 0.782374  [   96/  130]
train() client id: f_00008-0-3 loss: 0.769924  [  128/  130]
train() client id: f_00008-1-0 loss: 0.826225  [   32/  130]
train() client id: f_00008-1-1 loss: 0.943759  [   64/  130]
train() client id: f_00008-1-2 loss: 0.797324  [   96/  130]
train() client id: f_00008-1-3 loss: 0.720673  [  128/  130]
train() client id: f_00008-2-0 loss: 0.849482  [   32/  130]
train() client id: f_00008-2-1 loss: 0.821205  [   64/  130]
train() client id: f_00008-2-2 loss: 0.774777  [   96/  130]
train() client id: f_00008-2-3 loss: 0.844931  [  128/  130]
train() client id: f_00008-3-0 loss: 0.871935  [   32/  130]
train() client id: f_00008-3-1 loss: 0.784423  [   64/  130]
train() client id: f_00008-3-2 loss: 0.738047  [   96/  130]
train() client id: f_00008-3-3 loss: 0.906496  [  128/  130]
train() client id: f_00008-4-0 loss: 0.824367  [   32/  130]
train() client id: f_00008-4-1 loss: 0.870540  [   64/  130]
train() client id: f_00008-4-2 loss: 0.817665  [   96/  130]
train() client id: f_00008-4-3 loss: 0.793863  [  128/  130]
train() client id: f_00008-5-0 loss: 0.783015  [   32/  130]
train() client id: f_00008-5-1 loss: 0.857756  [   64/  130]
train() client id: f_00008-5-2 loss: 0.895424  [   96/  130]
train() client id: f_00008-5-3 loss: 0.751758  [  128/  130]
train() client id: f_00008-6-0 loss: 0.860200  [   32/  130]
train() client id: f_00008-6-1 loss: 0.829178  [   64/  130]
train() client id: f_00008-6-2 loss: 0.844066  [   96/  130]
train() client id: f_00008-6-3 loss: 0.777102  [  128/  130]
train() client id: f_00008-7-0 loss: 0.775012  [   32/  130]
train() client id: f_00008-7-1 loss: 0.828368  [   64/  130]
train() client id: f_00008-7-2 loss: 0.921088  [   96/  130]
train() client id: f_00008-7-3 loss: 0.797659  [  128/  130]
train() client id: f_00008-8-0 loss: 0.718466  [   32/  130]
train() client id: f_00008-8-1 loss: 0.865285  [   64/  130]
train() client id: f_00008-8-2 loss: 0.888268  [   96/  130]
train() client id: f_00008-8-3 loss: 0.816702  [  128/  130]
train() client id: f_00008-9-0 loss: 0.923987  [   32/  130]
train() client id: f_00008-9-1 loss: 0.829271  [   64/  130]
train() client id: f_00008-9-2 loss: 0.770744  [   96/  130]
train() client id: f_00008-9-3 loss: 0.792644  [  128/  130]
train() client id: f_00008-10-0 loss: 0.783764  [   32/  130]
train() client id: f_00008-10-1 loss: 0.879661  [   64/  130]
train() client id: f_00008-10-2 loss: 0.853514  [   96/  130]
train() client id: f_00008-10-3 loss: 0.795848  [  128/  130]
train() client id: f_00008-11-0 loss: 0.845584  [   32/  130]
train() client id: f_00008-11-1 loss: 0.873103  [   64/  130]
train() client id: f_00008-11-2 loss: 0.782739  [   96/  130]
train() client id: f_00008-11-3 loss: 0.820592  [  128/  130]
train() client id: f_00009-0-0 loss: 0.827052  [   32/  118]
train() client id: f_00009-0-1 loss: 1.005382  [   64/  118]
train() client id: f_00009-0-2 loss: 1.068800  [   96/  118]
train() client id: f_00009-1-0 loss: 0.766462  [   32/  118]
train() client id: f_00009-1-1 loss: 0.960588  [   64/  118]
train() client id: f_00009-1-2 loss: 1.009168  [   96/  118]
train() client id: f_00009-2-0 loss: 0.764058  [   32/  118]
train() client id: f_00009-2-1 loss: 0.875167  [   64/  118]
train() client id: f_00009-2-2 loss: 0.842352  [   96/  118]
train() client id: f_00009-3-0 loss: 0.875839  [   32/  118]
train() client id: f_00009-3-1 loss: 0.689189  [   64/  118]
train() client id: f_00009-3-2 loss: 0.847902  [   96/  118]
train() client id: f_00009-4-0 loss: 0.800390  [   32/  118]
train() client id: f_00009-4-1 loss: 0.790804  [   64/  118]
train() client id: f_00009-4-2 loss: 0.786686  [   96/  118]
train() client id: f_00009-5-0 loss: 0.882396  [   32/  118]
train() client id: f_00009-5-1 loss: 0.842079  [   64/  118]
train() client id: f_00009-5-2 loss: 0.627055  [   96/  118]
train() client id: f_00009-6-0 loss: 0.693530  [   32/  118]
train() client id: f_00009-6-1 loss: 0.790649  [   64/  118]
train() client id: f_00009-6-2 loss: 0.731104  [   96/  118]
train() client id: f_00009-7-0 loss: 0.631545  [   32/  118]
train() client id: f_00009-7-1 loss: 0.783507  [   64/  118]
train() client id: f_00009-7-2 loss: 0.795237  [   96/  118]
train() client id: f_00009-8-0 loss: 0.671327  [   32/  118]
train() client id: f_00009-8-1 loss: 0.922182  [   64/  118]
train() client id: f_00009-8-2 loss: 0.595114  [   96/  118]
train() client id: f_00009-9-0 loss: 0.612636  [   32/  118]
train() client id: f_00009-9-1 loss: 0.828209  [   64/  118]
train() client id: f_00009-9-2 loss: 0.594446  [   96/  118]
train() client id: f_00009-10-0 loss: 0.619168  [   32/  118]
train() client id: f_00009-10-1 loss: 0.689192  [   64/  118]
train() client id: f_00009-10-2 loss: 0.775134  [   96/  118]
train() client id: f_00009-11-0 loss: 0.692605  [   32/  118]
train() client id: f_00009-11-1 loss: 0.653180  [   64/  118]
train() client id: f_00009-11-2 loss: 0.682838  [   96/  118]
At round 27 accuracy: 0.6525198938992043
At round 27 training accuracy: 0.579476861167002
At round 27 training loss: 0.8351248842686342
update_location
xs = [  -3.9056584     4.20031788  155.00902392   18.81129433    0.97929623
    3.95640986 -117.44319194  -96.32485185  139.66397685  -82.06087855]
ys = [ 147.5879595   130.55583871    1.32061395 -117.45517586  109.35018685
   92.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [178.31842293 164.50674664 184.47097744 155.40136142 148.18374534
 136.49219669 154.27181815 138.84939085 172.66932676 129.42178968]
dists_bs = [173.27635064 184.62945973 372.92155342 350.84590306 187.82601169
 196.92615481 186.71749037 191.12203797 351.84930892 194.62315774]
uav_gains = [2.33299769e-11 2.87101678e-11 2.13317186e-11 3.31618763e-11
 3.73775877e-11 4.59306330e-11 3.37773315e-11 4.40026087e-11
 2.53630547e-11 5.24710704e-11]
bs_gains = [5.95420887e-11 4.98484075e-11 6.96249872e-12 8.25977351e-12
 4.75092334e-11 4.16144887e-11 4.83032198e-11 4.52505576e-11
 8.19398791e-12 4.30080195e-11]
Round 28
-------------------------------
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.2963697  15.14398805  7.18662324  2.58300247 17.46743421  8.40944175
  3.20540132 10.27679841  7.57595044  6.8226403 ]
obj_prev = 85.96764987436393
eta_min = 1.97492431522131e-13	eta_max = 0.9251230340770827
af = 18.15364795335576	bf = 1.507335053902618	zeta = 19.96901274869134	eta = 0.909090909090909
af = 18.15364795335576	bf = 1.507335053902618	zeta = 35.613187548855024	eta = 0.509745102946827
af = 18.15364795335576	bf = 1.507335053902618	zeta = 28.02638083867362	eta = 0.6477342921247089
af = 18.15364795335576	bf = 1.507335053902618	zeta = 26.65886868799346	eta = 0.6809609277055236
af = 18.15364795335576	bf = 1.507335053902618	zeta = 26.58884556407932	eta = 0.682754274141212
af = 18.15364795335576	bf = 1.507335053902618	zeta = 26.588647822049044	eta = 0.6827593518426902
eta = 0.6827593518426902
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [0.03148978 0.0662285  0.03098994 0.01074651 0.07647517 0.03648815
 0.01349562 0.04473547 0.03248945 0.02949042]
ene_total = [2.34522213 4.30343838 2.32693545 1.09048608 4.90813228 2.57679446
 1.24971036 3.05496696 2.56930912 2.1636526 ]
ti_comp = [0.402737   0.41597589 0.40085904 0.40941141 0.41525818 0.41320547
 0.40973259 0.41408375 0.37449574 0.41372635]
ti_coms = [0.08528891 0.07205001 0.08716686 0.07861449 0.07276772 0.07482043
 0.07829331 0.07394215 0.11353016 0.07429955]
t_total = [28.59988251 28.59988251 28.59988251 28.59988251 28.59988251 28.59988251
 28.59988251 28.59988251 28.59988251 28.59988251]
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [1.20322169e-05 1.04924856e-04 1.15760063e-05 4.62767175e-07
 1.62108192e-04 1.77829383e-05 9.15076330e-07 3.26332096e-05
 1.52831670e-05 9.36476148e-06]
ene_total = [0.50052546 0.42838512 0.51150413 0.46073322 0.43594225 0.43951388
 0.4588775  0.43523713 0.66621878 0.43596797]
optimize_network iter = 0 obj = 4.772905437718752
eta = 0.6827593518426902
freqs = [39094714.05548728 79606171.90046072 38654407.63725649 13124341.80140644
 92081472.39087442 44152552.9427729  16468811.85493763 54017423.64815775
 43377597.092848   35640012.17367139]
eta_min = 0.6827593518432681	eta_max = 0.6827593518426932
af = 0.017883468190837512	bf = 1.507335053902618	zeta = 0.019671815009921264	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [3.00512114e-06 2.62056364e-05 2.89117971e-06 1.15578985e-07
 4.04875310e-05 4.44139964e-06 2.28546015e-07 8.15034742e-06
 3.81706619e-06 2.33890753e-06]
ene_total = [1.74481114 1.47881446 1.78319283 1.60772537 1.49641265 1.53101993
 1.60118017 1.51381715 2.32252357 1.51993757]
ti_comp = [0.402737   0.41597589 0.40085904 0.40941141 0.41525818 0.41320547
 0.40973259 0.41408375 0.37449574 0.41372635]
ti_coms = [0.08528891 0.07205001 0.08716686 0.07861449 0.07276772 0.07482043
 0.07829331 0.07394215 0.11353016 0.07429955]
t_total = [28.59988251 28.59988251 28.59988251 28.59988251 28.59988251 28.59988251
 28.59988251 28.59988251 28.59988251 28.59988251]
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [1.20322169e-05 1.04924856e-04 1.15760063e-05 4.62767175e-07
 1.62108192e-04 1.77829383e-05 9.15076330e-07 3.26332096e-05
 1.52831670e-05 9.36476148e-06]
ene_total = [0.50052546 0.42838512 0.51150413 0.46073322 0.43594225 0.43951388
 0.4588775  0.43523713 0.66621878 0.43596797]
optimize_network iter = 1 obj = 4.772905437718796
eta = 0.6827593518426932
freqs = [39094714.05548728 79606171.9004607  38654407.6372565  13124341.80140644
 92081472.39087439 44152552.94277289 16468811.85493763 54017423.64815772
 43377597.09284804 35640012.17367138]
Done!
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [1.15546788e-05 1.00760567e-04 1.11165744e-05 4.44400737e-07
 1.55674395e-04 1.70771639e-05 8.78758515e-07 3.13380533e-05
 1.46766043e-05 8.99309011e-06]
ene_total = [0.00854045 0.00730576 0.0087278  0.00786189 0.00743245 0.00749912
 0.00783021 0.00742555 0.01136769 0.00743895]
At round 28 energy consumption: 0.0814298731583106
At round 28 eta: 0.6827593518426932
At round 28 a_n: 18.248773294144172
At round 28 local rounds: 12.495942598469853
At round 28 global rounds: 58.60320626975522
gradient difference: 0.4640706479549408
train() client id: f_00000-0-0 loss: 1.308413  [   32/  126]
train() client id: f_00000-0-1 loss: 1.179465  [   64/  126]
train() client id: f_00000-0-2 loss: 1.114902  [   96/  126]
train() client id: f_00000-1-0 loss: 1.025127  [   32/  126]
train() client id: f_00000-1-1 loss: 0.946497  [   64/  126]
train() client id: f_00000-1-2 loss: 1.102568  [   96/  126]
train() client id: f_00000-2-0 loss: 1.077992  [   32/  126]
train() client id: f_00000-2-1 loss: 0.928485  [   64/  126]
train() client id: f_00000-2-2 loss: 1.003951  [   96/  126]
train() client id: f_00000-3-0 loss: 0.981167  [   32/  126]
train() client id: f_00000-3-1 loss: 0.989389  [   64/  126]
train() client id: f_00000-3-2 loss: 0.994965  [   96/  126]
train() client id: f_00000-4-0 loss: 1.029546  [   32/  126]
train() client id: f_00000-4-1 loss: 0.967917  [   64/  126]
train() client id: f_00000-4-2 loss: 0.849832  [   96/  126]
train() client id: f_00000-5-0 loss: 0.927285  [   32/  126]
train() client id: f_00000-5-1 loss: 0.964316  [   64/  126]
train() client id: f_00000-5-2 loss: 0.778572  [   96/  126]
train() client id: f_00000-6-0 loss: 0.887050  [   32/  126]
train() client id: f_00000-6-1 loss: 0.874502  [   64/  126]
train() client id: f_00000-6-2 loss: 0.902563  [   96/  126]
train() client id: f_00000-7-0 loss: 0.889779  [   32/  126]
train() client id: f_00000-7-1 loss: 0.905044  [   64/  126]
train() client id: f_00000-7-2 loss: 0.852274  [   96/  126]
train() client id: f_00000-8-0 loss: 0.887750  [   32/  126]
train() client id: f_00000-8-1 loss: 0.847292  [   64/  126]
train() client id: f_00000-8-2 loss: 0.822538  [   96/  126]
train() client id: f_00000-9-0 loss: 0.925060  [   32/  126]
train() client id: f_00000-9-1 loss: 0.777208  [   64/  126]
train() client id: f_00000-9-2 loss: 0.800172  [   96/  126]
train() client id: f_00000-10-0 loss: 0.795836  [   32/  126]
train() client id: f_00000-10-1 loss: 0.820716  [   64/  126]
train() client id: f_00000-10-2 loss: 0.931475  [   96/  126]
train() client id: f_00000-11-0 loss: 0.782974  [   32/  126]
train() client id: f_00000-11-1 loss: 0.861856  [   64/  126]
train() client id: f_00000-11-2 loss: 0.798736  [   96/  126]
train() client id: f_00001-0-0 loss: 0.565406  [   32/  265]
train() client id: f_00001-0-1 loss: 0.409443  [   64/  265]
train() client id: f_00001-0-2 loss: 0.569764  [   96/  265]
train() client id: f_00001-0-3 loss: 0.458766  [  128/  265]
train() client id: f_00001-0-4 loss: 0.452473  [  160/  265]
train() client id: f_00001-0-5 loss: 0.431345  [  192/  265]
train() client id: f_00001-0-6 loss: 0.550258  [  224/  265]
train() client id: f_00001-0-7 loss: 0.370603  [  256/  265]
train() client id: f_00001-1-0 loss: 0.496702  [   32/  265]
train() client id: f_00001-1-1 loss: 0.486153  [   64/  265]
train() client id: f_00001-1-2 loss: 0.372814  [   96/  265]
train() client id: f_00001-1-3 loss: 0.536981  [  128/  265]
train() client id: f_00001-1-4 loss: 0.379592  [  160/  265]
train() client id: f_00001-1-5 loss: 0.436742  [  192/  265]
train() client id: f_00001-1-6 loss: 0.559991  [  224/  265]
train() client id: f_00001-1-7 loss: 0.453578  [  256/  265]
train() client id: f_00001-2-0 loss: 0.553444  [   32/  265]
train() client id: f_00001-2-1 loss: 0.423122  [   64/  265]
train() client id: f_00001-2-2 loss: 0.651297  [   96/  265]
train() client id: f_00001-2-3 loss: 0.409867  [  128/  265]
train() client id: f_00001-2-4 loss: 0.427888  [  160/  265]
train() client id: f_00001-2-5 loss: 0.375456  [  192/  265]
train() client id: f_00001-2-6 loss: 0.436258  [  224/  265]
train() client id: f_00001-2-7 loss: 0.450488  [  256/  265]
train() client id: f_00001-3-0 loss: 0.447563  [   32/  265]
train() client id: f_00001-3-1 loss: 0.427654  [   64/  265]
train() client id: f_00001-3-2 loss: 0.383955  [   96/  265]
train() client id: f_00001-3-3 loss: 0.537283  [  128/  265]
train() client id: f_00001-3-4 loss: 0.441945  [  160/  265]
train() client id: f_00001-3-5 loss: 0.451385  [  192/  265]
train() client id: f_00001-3-6 loss: 0.493688  [  224/  265]
train() client id: f_00001-3-7 loss: 0.518644  [  256/  265]
train() client id: f_00001-4-0 loss: 0.510529  [   32/  265]
train() client id: f_00001-4-1 loss: 0.588624  [   64/  265]
train() client id: f_00001-4-2 loss: 0.389931  [   96/  265]
train() client id: f_00001-4-3 loss: 0.488210  [  128/  265]
train() client id: f_00001-4-4 loss: 0.403229  [  160/  265]
train() client id: f_00001-4-5 loss: 0.463642  [  192/  265]
train() client id: f_00001-4-6 loss: 0.412944  [  224/  265]
train() client id: f_00001-4-7 loss: 0.419188  [  256/  265]
train() client id: f_00001-5-0 loss: 0.393344  [   32/  265]
train() client id: f_00001-5-1 loss: 0.352005  [   64/  265]
train() client id: f_00001-5-2 loss: 0.543633  [   96/  265]
train() client id: f_00001-5-3 loss: 0.424143  [  128/  265]
train() client id: f_00001-5-4 loss: 0.474114  [  160/  265]
train() client id: f_00001-5-5 loss: 0.535812  [  192/  265]
train() client id: f_00001-5-6 loss: 0.480787  [  224/  265]
train() client id: f_00001-5-7 loss: 0.365496  [  256/  265]
train() client id: f_00001-6-0 loss: 0.507463  [   32/  265]
train() client id: f_00001-6-1 loss: 0.433170  [   64/  265]
train() client id: f_00001-6-2 loss: 0.382472  [   96/  265]
train() client id: f_00001-6-3 loss: 0.459266  [  128/  265]
train() client id: f_00001-6-4 loss: 0.435461  [  160/  265]
train() client id: f_00001-6-5 loss: 0.481650  [  192/  265]
train() client id: f_00001-6-6 loss: 0.459186  [  224/  265]
train() client id: f_00001-6-7 loss: 0.384853  [  256/  265]
train() client id: f_00001-7-0 loss: 0.511544  [   32/  265]
train() client id: f_00001-7-1 loss: 0.432673  [   64/  265]
train() client id: f_00001-7-2 loss: 0.354959  [   96/  265]
train() client id: f_00001-7-3 loss: 0.423827  [  128/  265]
train() client id: f_00001-7-4 loss: 0.407620  [  160/  265]
train() client id: f_00001-7-5 loss: 0.433946  [  192/  265]
train() client id: f_00001-7-6 loss: 0.488255  [  224/  265]
train() client id: f_00001-7-7 loss: 0.572118  [  256/  265]
train() client id: f_00001-8-0 loss: 0.343753  [   32/  265]
train() client id: f_00001-8-1 loss: 0.446181  [   64/  265]
train() client id: f_00001-8-2 loss: 0.557764  [   96/  265]
train() client id: f_00001-8-3 loss: 0.498739  [  128/  265]
train() client id: f_00001-8-4 loss: 0.415831  [  160/  265]
train() client id: f_00001-8-5 loss: 0.532185  [  192/  265]
train() client id: f_00001-8-6 loss: 0.417838  [  224/  265]
train() client id: f_00001-8-7 loss: 0.404975  [  256/  265]
train() client id: f_00001-9-0 loss: 0.477657  [   32/  265]
train() client id: f_00001-9-1 loss: 0.351941  [   64/  265]
train() client id: f_00001-9-2 loss: 0.497382  [   96/  265]
train() client id: f_00001-9-3 loss: 0.378054  [  128/  265]
train() client id: f_00001-9-4 loss: 0.444729  [  160/  265]
train() client id: f_00001-9-5 loss: 0.355019  [  192/  265]
train() client id: f_00001-9-6 loss: 0.441371  [  224/  265]
train() client id: f_00001-9-7 loss: 0.662495  [  256/  265]
train() client id: f_00001-10-0 loss: 0.420444  [   32/  265]
train() client id: f_00001-10-1 loss: 0.403709  [   64/  265]
train() client id: f_00001-10-2 loss: 0.397890  [   96/  265]
train() client id: f_00001-10-3 loss: 0.401990  [  128/  265]
train() client id: f_00001-10-4 loss: 0.436896  [  160/  265]
train() client id: f_00001-10-5 loss: 0.666631  [  192/  265]
train() client id: f_00001-10-6 loss: 0.466720  [  224/  265]
train() client id: f_00001-10-7 loss: 0.335631  [  256/  265]
train() client id: f_00001-11-0 loss: 0.355211  [   32/  265]
train() client id: f_00001-11-1 loss: 0.664539  [   64/  265]
train() client id: f_00001-11-2 loss: 0.363894  [   96/  265]
train() client id: f_00001-11-3 loss: 0.464458  [  128/  265]
train() client id: f_00001-11-4 loss: 0.354716  [  160/  265]
train() client id: f_00001-11-5 loss: 0.474399  [  192/  265]
train() client id: f_00001-11-6 loss: 0.522900  [  224/  265]
train() client id: f_00001-11-7 loss: 0.415881  [  256/  265]
train() client id: f_00002-0-0 loss: 1.088255  [   32/  124]
train() client id: f_00002-0-1 loss: 1.164452  [   64/  124]
train() client id: f_00002-0-2 loss: 1.205889  [   96/  124]
train() client id: f_00002-1-0 loss: 1.142731  [   32/  124]
train() client id: f_00002-1-1 loss: 1.215857  [   64/  124]
train() client id: f_00002-1-2 loss: 1.162510  [   96/  124]
train() client id: f_00002-2-0 loss: 0.989864  [   32/  124]
train() client id: f_00002-2-1 loss: 1.199095  [   64/  124]
train() client id: f_00002-2-2 loss: 1.112959  [   96/  124]
train() client id: f_00002-3-0 loss: 1.067828  [   32/  124]
train() client id: f_00002-3-1 loss: 1.213762  [   64/  124]
train() client id: f_00002-3-2 loss: 1.053006  [   96/  124]
train() client id: f_00002-4-0 loss: 0.828619  [   32/  124]
train() client id: f_00002-4-1 loss: 1.219130  [   64/  124]
train() client id: f_00002-4-2 loss: 1.113703  [   96/  124]
train() client id: f_00002-5-0 loss: 0.979666  [   32/  124]
train() client id: f_00002-5-1 loss: 1.134935  [   64/  124]
train() client id: f_00002-5-2 loss: 1.037276  [   96/  124]
train() client id: f_00002-6-0 loss: 0.936719  [   32/  124]
train() client id: f_00002-6-1 loss: 1.249093  [   64/  124]
train() client id: f_00002-6-2 loss: 0.976669  [   96/  124]
train() client id: f_00002-7-0 loss: 0.980305  [   32/  124]
train() client id: f_00002-7-1 loss: 0.945955  [   64/  124]
train() client id: f_00002-7-2 loss: 1.030869  [   96/  124]
train() client id: f_00002-8-0 loss: 1.092431  [   32/  124]
train() client id: f_00002-8-1 loss: 1.050800  [   64/  124]
train() client id: f_00002-8-2 loss: 1.007586  [   96/  124]
train() client id: f_00002-9-0 loss: 1.157702  [   32/  124]
train() client id: f_00002-9-1 loss: 0.976384  [   64/  124]
train() client id: f_00002-9-2 loss: 1.007408  [   96/  124]
train() client id: f_00002-10-0 loss: 1.071107  [   32/  124]
train() client id: f_00002-10-1 loss: 0.877148  [   64/  124]
train() client id: f_00002-10-2 loss: 1.046093  [   96/  124]
train() client id: f_00002-11-0 loss: 0.967331  [   32/  124]
train() client id: f_00002-11-1 loss: 1.142529  [   64/  124]
train() client id: f_00002-11-2 loss: 0.918740  [   96/  124]
train() client id: f_00003-0-0 loss: 0.876583  [   32/   43]
train() client id: f_00003-1-0 loss: 0.767440  [   32/   43]
train() client id: f_00003-2-0 loss: 0.649824  [   32/   43]
train() client id: f_00003-3-0 loss: 0.650393  [   32/   43]
train() client id: f_00003-4-0 loss: 0.852455  [   32/   43]
train() client id: f_00003-5-0 loss: 0.481663  [   32/   43]
train() client id: f_00003-6-0 loss: 0.697656  [   32/   43]
train() client id: f_00003-7-0 loss: 0.863611  [   32/   43]
train() client id: f_00003-8-0 loss: 0.688126  [   32/   43]
train() client id: f_00003-9-0 loss: 0.775568  [   32/   43]
train() client id: f_00003-10-0 loss: 0.848842  [   32/   43]
train() client id: f_00003-11-0 loss: 0.762843  [   32/   43]
train() client id: f_00004-0-0 loss: 0.742248  [   32/  306]
train() client id: f_00004-0-1 loss: 0.796554  [   64/  306]
train() client id: f_00004-0-2 loss: 0.773728  [   96/  306]
train() client id: f_00004-0-3 loss: 0.655852  [  128/  306]
train() client id: f_00004-0-4 loss: 0.794683  [  160/  306]
train() client id: f_00004-0-5 loss: 0.886411  [  192/  306]
train() client id: f_00004-0-6 loss: 0.700419  [  224/  306]
train() client id: f_00004-0-7 loss: 0.830661  [  256/  306]
train() client id: f_00004-0-8 loss: 0.946650  [  288/  306]
train() client id: f_00004-1-0 loss: 0.832406  [   32/  306]
train() client id: f_00004-1-1 loss: 0.727646  [   64/  306]
train() client id: f_00004-1-2 loss: 0.777291  [   96/  306]
train() client id: f_00004-1-3 loss: 0.685175  [  128/  306]
train() client id: f_00004-1-4 loss: 0.997055  [  160/  306]
train() client id: f_00004-1-5 loss: 0.916634  [  192/  306]
train() client id: f_00004-1-6 loss: 0.802019  [  224/  306]
train() client id: f_00004-1-7 loss: 0.785346  [  256/  306]
train() client id: f_00004-1-8 loss: 0.718855  [  288/  306]
train() client id: f_00004-2-0 loss: 0.920605  [   32/  306]
train() client id: f_00004-2-1 loss: 0.729629  [   64/  306]
train() client id: f_00004-2-2 loss: 0.844995  [   96/  306]
train() client id: f_00004-2-3 loss: 0.799899  [  128/  306]
train() client id: f_00004-2-4 loss: 0.983873  [  160/  306]
train() client id: f_00004-2-5 loss: 0.800860  [  192/  306]
train() client id: f_00004-2-6 loss: 0.719296  [  224/  306]
train() client id: f_00004-2-7 loss: 0.843226  [  256/  306]
train() client id: f_00004-2-8 loss: 0.650399  [  288/  306]
train() client id: f_00004-3-0 loss: 0.802673  [   32/  306]
train() client id: f_00004-3-1 loss: 0.632363  [   64/  306]
train() client id: f_00004-3-2 loss: 0.765701  [   96/  306]
train() client id: f_00004-3-3 loss: 0.688356  [  128/  306]
train() client id: f_00004-3-4 loss: 0.853789  [  160/  306]
train() client id: f_00004-3-5 loss: 0.920313  [  192/  306]
train() client id: f_00004-3-6 loss: 0.795934  [  224/  306]
train() client id: f_00004-3-7 loss: 0.849941  [  256/  306]
train() client id: f_00004-3-8 loss: 0.912548  [  288/  306]
train() client id: f_00004-4-0 loss: 0.887476  [   32/  306]
train() client id: f_00004-4-1 loss: 0.853772  [   64/  306]
train() client id: f_00004-4-2 loss: 0.728480  [   96/  306]
train() client id: f_00004-4-3 loss: 0.812510  [  128/  306]
train() client id: f_00004-4-4 loss: 0.707699  [  160/  306]
train() client id: f_00004-4-5 loss: 0.765410  [  192/  306]
train() client id: f_00004-4-6 loss: 0.771164  [  224/  306]
train() client id: f_00004-4-7 loss: 0.800164  [  256/  306]
train() client id: f_00004-4-8 loss: 0.873787  [  288/  306]
train() client id: f_00004-5-0 loss: 0.664227  [   32/  306]
train() client id: f_00004-5-1 loss: 0.904469  [   64/  306]
train() client id: f_00004-5-2 loss: 0.795229  [   96/  306]
train() client id: f_00004-5-3 loss: 0.813175  [  128/  306]
train() client id: f_00004-5-4 loss: 0.861214  [  160/  306]
train() client id: f_00004-5-5 loss: 0.778085  [  192/  306]
train() client id: f_00004-5-6 loss: 0.807019  [  224/  306]
train() client id: f_00004-5-7 loss: 0.760246  [  256/  306]
train() client id: f_00004-5-8 loss: 0.706219  [  288/  306]
train() client id: f_00004-6-0 loss: 0.738511  [   32/  306]
train() client id: f_00004-6-1 loss: 0.972694  [   64/  306]
train() client id: f_00004-6-2 loss: 0.723530  [   96/  306]
train() client id: f_00004-6-3 loss: 0.839358  [  128/  306]
train() client id: f_00004-6-4 loss: 0.876650  [  160/  306]
train() client id: f_00004-6-5 loss: 0.798366  [  192/  306]
train() client id: f_00004-6-6 loss: 0.774451  [  224/  306]
train() client id: f_00004-6-7 loss: 0.733312  [  256/  306]
train() client id: f_00004-6-8 loss: 0.752984  [  288/  306]
train() client id: f_00004-7-0 loss: 0.877096  [   32/  306]
train() client id: f_00004-7-1 loss: 0.838353  [   64/  306]
train() client id: f_00004-7-2 loss: 0.893036  [   96/  306]
train() client id: f_00004-7-3 loss: 0.771125  [  128/  306]
train() client id: f_00004-7-4 loss: 0.868586  [  160/  306]
train() client id: f_00004-7-5 loss: 0.759328  [  192/  306]
train() client id: f_00004-7-6 loss: 0.791673  [  224/  306]
train() client id: f_00004-7-7 loss: 0.747761  [  256/  306]
train() client id: f_00004-7-8 loss: 0.714207  [  288/  306]
train() client id: f_00004-8-0 loss: 0.921682  [   32/  306]
train() client id: f_00004-8-1 loss: 0.772786  [   64/  306]
train() client id: f_00004-8-2 loss: 0.748622  [   96/  306]
train() client id: f_00004-8-3 loss: 0.732509  [  128/  306]
train() client id: f_00004-8-4 loss: 0.743768  [  160/  306]
train() client id: f_00004-8-5 loss: 0.820238  [  192/  306]
train() client id: f_00004-8-6 loss: 0.677049  [  224/  306]
train() client id: f_00004-8-7 loss: 0.847798  [  256/  306]
train() client id: f_00004-8-8 loss: 0.881896  [  288/  306]
train() client id: f_00004-9-0 loss: 0.913789  [   32/  306]
train() client id: f_00004-9-1 loss: 0.810703  [   64/  306]
train() client id: f_00004-9-2 loss: 0.783255  [   96/  306]
train() client id: f_00004-9-3 loss: 0.948069  [  128/  306]
train() client id: f_00004-9-4 loss: 0.846229  [  160/  306]
train() client id: f_00004-9-5 loss: 0.664992  [  192/  306]
train() client id: f_00004-9-6 loss: 0.758530  [  224/  306]
train() client id: f_00004-9-7 loss: 0.821032  [  256/  306]
train() client id: f_00004-9-8 loss: 0.692248  [  288/  306]
train() client id: f_00004-10-0 loss: 0.746688  [   32/  306]
train() client id: f_00004-10-1 loss: 0.769638  [   64/  306]
train() client id: f_00004-10-2 loss: 0.825262  [   96/  306]
train() client id: f_00004-10-3 loss: 0.849427  [  128/  306]
train() client id: f_00004-10-4 loss: 0.725973  [  160/  306]
train() client id: f_00004-10-5 loss: 0.880173  [  192/  306]
train() client id: f_00004-10-6 loss: 0.859758  [  224/  306]
train() client id: f_00004-10-7 loss: 0.732640  [  256/  306]
train() client id: f_00004-10-8 loss: 0.820584  [  288/  306]
train() client id: f_00004-11-0 loss: 0.796684  [   32/  306]
train() client id: f_00004-11-1 loss: 0.707830  [   64/  306]
train() client id: f_00004-11-2 loss: 0.769528  [   96/  306]
train() client id: f_00004-11-3 loss: 0.886803  [  128/  306]
train() client id: f_00004-11-4 loss: 0.803834  [  160/  306]
train() client id: f_00004-11-5 loss: 0.769865  [  192/  306]
train() client id: f_00004-11-6 loss: 0.862996  [  224/  306]
train() client id: f_00004-11-7 loss: 0.834792  [  256/  306]
train() client id: f_00004-11-8 loss: 0.829335  [  288/  306]
train() client id: f_00005-0-0 loss: 0.618165  [   32/  146]
train() client id: f_00005-0-1 loss: 0.354250  [   64/  146]
train() client id: f_00005-0-2 loss: 0.533968  [   96/  146]
train() client id: f_00005-0-3 loss: 0.453667  [  128/  146]
train() client id: f_00005-1-0 loss: 0.785714  [   32/  146]
train() client id: f_00005-1-1 loss: 0.518782  [   64/  146]
train() client id: f_00005-1-2 loss: 0.468055  [   96/  146]
train() client id: f_00005-1-3 loss: 0.223513  [  128/  146]
train() client id: f_00005-2-0 loss: 0.520123  [   32/  146]
train() client id: f_00005-2-1 loss: 0.765916  [   64/  146]
train() client id: f_00005-2-2 loss: 0.375549  [   96/  146]
train() client id: f_00005-2-3 loss: 0.497483  [  128/  146]
train() client id: f_00005-3-0 loss: 0.487262  [   32/  146]
train() client id: f_00005-3-1 loss: 0.436125  [   64/  146]
train() client id: f_00005-3-2 loss: 0.541810  [   96/  146]
train() client id: f_00005-3-3 loss: 0.591281  [  128/  146]
train() client id: f_00005-4-0 loss: 0.614620  [   32/  146]
train() client id: f_00005-4-1 loss: 0.476355  [   64/  146]
train() client id: f_00005-4-2 loss: 0.192780  [   96/  146]
train() client id: f_00005-4-3 loss: 0.693912  [  128/  146]
train() client id: f_00005-5-0 loss: 0.436273  [   32/  146]
train() client id: f_00005-5-1 loss: 0.264444  [   64/  146]
train() client id: f_00005-5-2 loss: 0.663835  [   96/  146]
train() client id: f_00005-5-3 loss: 0.575507  [  128/  146]
train() client id: f_00005-6-0 loss: 0.583480  [   32/  146]
train() client id: f_00005-6-1 loss: 0.572134  [   64/  146]
train() client id: f_00005-6-2 loss: 0.487743  [   96/  146]
train() client id: f_00005-6-3 loss: 0.458123  [  128/  146]
train() client id: f_00005-7-0 loss: 0.566427  [   32/  146]
train() client id: f_00005-7-1 loss: 0.607376  [   64/  146]
train() client id: f_00005-7-2 loss: 0.572247  [   96/  146]
train() client id: f_00005-7-3 loss: 0.373267  [  128/  146]
train() client id: f_00005-8-0 loss: 0.514914  [   32/  146]
train() client id: f_00005-8-1 loss: 0.528835  [   64/  146]
train() client id: f_00005-8-2 loss: 0.339038  [   96/  146]
train() client id: f_00005-8-3 loss: 0.678095  [  128/  146]
train() client id: f_00005-9-0 loss: 0.346758  [   32/  146]
train() client id: f_00005-9-1 loss: 0.346992  [   64/  146]
train() client id: f_00005-9-2 loss: 0.662029  [   96/  146]
train() client id: f_00005-9-3 loss: 0.605145  [  128/  146]
train() client id: f_00005-10-0 loss: 0.484498  [   32/  146]
train() client id: f_00005-10-1 loss: 0.481179  [   64/  146]
train() client id: f_00005-10-2 loss: 0.654185  [   96/  146]
train() client id: f_00005-10-3 loss: 0.384028  [  128/  146]
train() client id: f_00005-11-0 loss: 0.476062  [   32/  146]
train() client id: f_00005-11-1 loss: 0.365546  [   64/  146]
train() client id: f_00005-11-2 loss: 0.574025  [   96/  146]
train() client id: f_00005-11-3 loss: 0.482148  [  128/  146]
train() client id: f_00006-0-0 loss: 0.625919  [   32/   54]
train() client id: f_00006-1-0 loss: 0.627956  [   32/   54]
train() client id: f_00006-2-0 loss: 0.639712  [   32/   54]
train() client id: f_00006-3-0 loss: 0.530930  [   32/   54]
train() client id: f_00006-4-0 loss: 0.640464  [   32/   54]
train() client id: f_00006-5-0 loss: 0.627244  [   32/   54]
train() client id: f_00006-6-0 loss: 0.579823  [   32/   54]
train() client id: f_00006-7-0 loss: 0.524482  [   32/   54]
train() client id: f_00006-8-0 loss: 0.581526  [   32/   54]
train() client id: f_00006-9-0 loss: 0.632144  [   32/   54]
train() client id: f_00006-10-0 loss: 0.584883  [   32/   54]
train() client id: f_00006-11-0 loss: 0.578111  [   32/   54]
train() client id: f_00007-0-0 loss: 0.718441  [   32/  179]
train() client id: f_00007-0-1 loss: 0.855501  [   64/  179]
train() client id: f_00007-0-2 loss: 0.566313  [   96/  179]
train() client id: f_00007-0-3 loss: 0.626872  [  128/  179]
train() client id: f_00007-0-4 loss: 0.718631  [  160/  179]
train() client id: f_00007-1-0 loss: 0.562973  [   32/  179]
train() client id: f_00007-1-1 loss: 0.726602  [   64/  179]
train() client id: f_00007-1-2 loss: 0.770878  [   96/  179]
train() client id: f_00007-1-3 loss: 0.726269  [  128/  179]
train() client id: f_00007-1-4 loss: 0.810084  [  160/  179]
train() client id: f_00007-2-0 loss: 0.797326  [   32/  179]
train() client id: f_00007-2-1 loss: 0.610770  [   64/  179]
train() client id: f_00007-2-2 loss: 0.643830  [   96/  179]
train() client id: f_00007-2-3 loss: 0.788262  [  128/  179]
train() client id: f_00007-2-4 loss: 0.747639  [  160/  179]
train() client id: f_00007-3-0 loss: 0.685801  [   32/  179]
train() client id: f_00007-3-1 loss: 0.639921  [   64/  179]
train() client id: f_00007-3-2 loss: 0.768687  [   96/  179]
train() client id: f_00007-3-3 loss: 0.603487  [  128/  179]
train() client id: f_00007-3-4 loss: 0.750947  [  160/  179]
train() client id: f_00007-4-0 loss: 0.586375  [   32/  179]
train() client id: f_00007-4-1 loss: 0.651739  [   64/  179]
train() client id: f_00007-4-2 loss: 0.565106  [   96/  179]
train() client id: f_00007-4-3 loss: 0.741066  [  128/  179]
train() client id: f_00007-4-4 loss: 0.925433  [  160/  179]
train() client id: f_00007-5-0 loss: 0.523190  [   32/  179]
train() client id: f_00007-5-1 loss: 0.651580  [   64/  179]
train() client id: f_00007-5-2 loss: 0.770245  [   96/  179]
train() client id: f_00007-5-3 loss: 0.533065  [  128/  179]
train() client id: f_00007-5-4 loss: 0.714227  [  160/  179]
train() client id: f_00007-6-0 loss: 0.711107  [   32/  179]
train() client id: f_00007-6-1 loss: 0.632246  [   64/  179]
train() client id: f_00007-6-2 loss: 0.713232  [   96/  179]
train() client id: f_00007-6-3 loss: 0.791708  [  128/  179]
train() client id: f_00007-6-4 loss: 0.665430  [  160/  179]
train() client id: f_00007-7-0 loss: 0.622110  [   32/  179]
train() client id: f_00007-7-1 loss: 0.669956  [   64/  179]
train() client id: f_00007-7-2 loss: 0.730617  [   96/  179]
train() client id: f_00007-7-3 loss: 0.616593  [  128/  179]
train() client id: f_00007-7-4 loss: 0.868859  [  160/  179]
train() client id: f_00007-8-0 loss: 0.889042  [   32/  179]
train() client id: f_00007-8-1 loss: 0.548089  [   64/  179]
train() client id: f_00007-8-2 loss: 0.757006  [   96/  179]
train() client id: f_00007-8-3 loss: 0.729749  [  128/  179]
train() client id: f_00007-8-4 loss: 0.563783  [  160/  179]
train() client id: f_00007-9-0 loss: 0.813511  [   32/  179]
train() client id: f_00007-9-1 loss: 0.540020  [   64/  179]
train() client id: f_00007-9-2 loss: 0.778858  [   96/  179]
train() client id: f_00007-9-3 loss: 0.673033  [  128/  179]
train() client id: f_00007-9-4 loss: 0.569426  [  160/  179]
train() client id: f_00007-10-0 loss: 0.602956  [   32/  179]
train() client id: f_00007-10-1 loss: 0.821318  [   64/  179]
train() client id: f_00007-10-2 loss: 0.698013  [   96/  179]
train() client id: f_00007-10-3 loss: 0.707517  [  128/  179]
train() client id: f_00007-10-4 loss: 0.698296  [  160/  179]
train() client id: f_00007-11-0 loss: 0.641634  [   32/  179]
train() client id: f_00007-11-1 loss: 0.791564  [   64/  179]
train() client id: f_00007-11-2 loss: 0.527470  [   96/  179]
train() client id: f_00007-11-3 loss: 0.647891  [  128/  179]
train() client id: f_00007-11-4 loss: 0.737593  [  160/  179]
train() client id: f_00008-0-0 loss: 0.673943  [   32/  130]
train() client id: f_00008-0-1 loss: 0.834809  [   64/  130]
train() client id: f_00008-0-2 loss: 0.768203  [   96/  130]
train() client id: f_00008-0-3 loss: 0.740148  [  128/  130]
train() client id: f_00008-1-0 loss: 0.792995  [   32/  130]
train() client id: f_00008-1-1 loss: 0.614014  [   64/  130]
train() client id: f_00008-1-2 loss: 0.780722  [   96/  130]
train() client id: f_00008-1-3 loss: 0.858195  [  128/  130]
train() client id: f_00008-2-0 loss: 0.778984  [   32/  130]
train() client id: f_00008-2-1 loss: 0.794090  [   64/  130]
train() client id: f_00008-2-2 loss: 0.669251  [   96/  130]
train() client id: f_00008-2-3 loss: 0.780576  [  128/  130]
train() client id: f_00008-3-0 loss: 0.753660  [   32/  130]
train() client id: f_00008-3-1 loss: 0.866896  [   64/  130]
train() client id: f_00008-3-2 loss: 0.735803  [   96/  130]
train() client id: f_00008-3-3 loss: 0.660084  [  128/  130]
train() client id: f_00008-4-0 loss: 0.867273  [   32/  130]
train() client id: f_00008-4-1 loss: 0.729876  [   64/  130]
train() client id: f_00008-4-2 loss: 0.637061  [   96/  130]
train() client id: f_00008-4-3 loss: 0.758007  [  128/  130]
train() client id: f_00008-5-0 loss: 0.846298  [   32/  130]
train() client id: f_00008-5-1 loss: 0.722687  [   64/  130]
train() client id: f_00008-5-2 loss: 0.697535  [   96/  130]
train() client id: f_00008-5-3 loss: 0.785017  [  128/  130]
train() client id: f_00008-6-0 loss: 0.775907  [   32/  130]
train() client id: f_00008-6-1 loss: 0.749970  [   64/  130]
train() client id: f_00008-6-2 loss: 0.781374  [   96/  130]
train() client id: f_00008-6-3 loss: 0.740442  [  128/  130]
train() client id: f_00008-7-0 loss: 0.683475  [   32/  130]
train() client id: f_00008-7-1 loss: 0.702760  [   64/  130]
train() client id: f_00008-7-2 loss: 0.903069  [   96/  130]
train() client id: f_00008-7-3 loss: 0.729412  [  128/  130]
train() client id: f_00008-8-0 loss: 0.769810  [   32/  130]
train() client id: f_00008-8-1 loss: 0.739904  [   64/  130]
train() client id: f_00008-8-2 loss: 0.749096  [   96/  130]
train() client id: f_00008-8-3 loss: 0.787112  [  128/  130]
train() client id: f_00008-9-0 loss: 0.962758  [   32/  130]
train() client id: f_00008-9-1 loss: 0.725982  [   64/  130]
train() client id: f_00008-9-2 loss: 0.631922  [   96/  130]
train() client id: f_00008-9-3 loss: 0.721441  [  128/  130]
train() client id: f_00008-10-0 loss: 0.748352  [   32/  130]
train() client id: f_00008-10-1 loss: 0.700519  [   64/  130]
train() client id: f_00008-10-2 loss: 0.808976  [   96/  130]
train() client id: f_00008-10-3 loss: 0.778589  [  128/  130]
train() client id: f_00008-11-0 loss: 0.690990  [   32/  130]
train() client id: f_00008-11-1 loss: 0.901601  [   64/  130]
train() client id: f_00008-11-2 loss: 0.750304  [   96/  130]
train() client id: f_00008-11-3 loss: 0.694635  [  128/  130]
train() client id: f_00009-0-0 loss: 1.162785  [   32/  118]
train() client id: f_00009-0-1 loss: 1.049127  [   64/  118]
train() client id: f_00009-0-2 loss: 1.053409  [   96/  118]
train() client id: f_00009-1-0 loss: 1.053006  [   32/  118]
train() client id: f_00009-1-1 loss: 1.051525  [   64/  118]
train() client id: f_00009-1-2 loss: 1.015221  [   96/  118]
train() client id: f_00009-2-0 loss: 1.061820  [   32/  118]
train() client id: f_00009-2-1 loss: 0.981244  [   64/  118]
train() client id: f_00009-2-2 loss: 1.006476  [   96/  118]
train() client id: f_00009-3-0 loss: 0.839063  [   32/  118]
train() client id: f_00009-3-1 loss: 0.880680  [   64/  118]
train() client id: f_00009-3-2 loss: 1.049021  [   96/  118]
train() client id: f_00009-4-0 loss: 0.931583  [   32/  118]
train() client id: f_00009-4-1 loss: 1.045957  [   64/  118]
train() client id: f_00009-4-2 loss: 0.816452  [   96/  118]
train() client id: f_00009-5-0 loss: 1.010169  [   32/  118]
train() client id: f_00009-5-1 loss: 0.909616  [   64/  118]
train() client id: f_00009-5-2 loss: 0.919775  [   96/  118]
train() client id: f_00009-6-0 loss: 0.756090  [   32/  118]
train() client id: f_00009-6-1 loss: 0.976998  [   64/  118]
train() client id: f_00009-6-2 loss: 0.845008  [   96/  118]
train() client id: f_00009-7-0 loss: 0.853609  [   32/  118]
train() client id: f_00009-7-1 loss: 0.894831  [   64/  118]
train() client id: f_00009-7-2 loss: 0.804895  [   96/  118]
train() client id: f_00009-8-0 loss: 0.837480  [   32/  118]
train() client id: f_00009-8-1 loss: 0.840152  [   64/  118]
train() client id: f_00009-8-2 loss: 0.879781  [   96/  118]
train() client id: f_00009-9-0 loss: 0.925440  [   32/  118]
train() client id: f_00009-9-1 loss: 0.828462  [   64/  118]
train() client id: f_00009-9-2 loss: 0.784713  [   96/  118]
train() client id: f_00009-10-0 loss: 0.759115  [   32/  118]
train() client id: f_00009-10-1 loss: 0.864454  [   64/  118]
train() client id: f_00009-10-2 loss: 0.855318  [   96/  118]
train() client id: f_00009-11-0 loss: 0.916293  [   32/  118]
train() client id: f_00009-11-1 loss: 0.803999  [   64/  118]
train() client id: f_00009-11-2 loss: 0.776503  [   96/  118]
At round 28 accuracy: 0.6551724137931034
At round 28 training accuracy: 0.5841716968477532
At round 28 training loss: 0.8370955554830489
update_location
xs = [  -3.9056584     4.20031788  160.00902392   18.81129433    0.97929623
    3.95640986 -122.44319194 -101.32485185  144.66397685  -87.06087855]
ys = [ 152.5879595   135.55583871    1.32061395 -122.45517586  114.35018685
   97.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [182.47832625 168.50230882 188.69189637 159.21411649 151.91090893
 139.94020603 158.11143474 142.36362547 176.73804393 132.64843923]
dists_bs = [172.55600043 183.49004274 377.35338245 355.02450543 186.13734858
 194.89241123 185.23728871 189.11869796 356.32790511 192.28567891]
uav_gains = [2.19565925e-11 2.70052809e-11 2.00716392e-11 3.11928652e-11
 3.51147715e-11 4.31483308e-11 3.17457777e-11 4.13309852e-11
 2.38781215e-11 4.93353388e-11]
bs_gains = [6.02406849e-11 5.07199808e-11 6.73595215e-12 7.99044117e-12
 4.87259393e-11 4.28418567e-11 4.93917622e-11 4.66055429e-11
 7.90887203e-12 4.44879766e-11]
Round 29
-------------------------------
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.16426333 14.86448857  7.05664821  2.53741575 17.14489673  8.25364943
  3.14835783 10.08926783  7.43869069  6.69596016]
obj_prev = 84.39363852739328
eta_min = 1.1633239161667213e-13	eta_max = 0.9255734667757686
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 19.601082838327166	eta = 0.9090909090909091
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 35.06532785864593	eta = 0.5081705292616409
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 27.55402889869821	eta = 0.6466991191078749
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 26.19954622843339	eta = 0.6801326275385099
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 26.129948192651668	eta = 0.6819441847065052
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 26.129750162898475	eta = 0.6819493529625255
eta = 0.6819493529625255
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [0.03158773 0.06643451 0.03108634 0.01077994 0.07671306 0.03660165
 0.0135376  0.04487463 0.03259051 0.02958216]
ene_total = [2.30902016 4.22374425 2.291367   1.07582749 4.81690287 2.52674818
 1.23224478 3.00469125 2.52859005 2.12061414]
ti_comp = [0.41103184 0.42578996 0.40909461 0.41788221 0.42519616 0.42322413
 0.41819745 0.42265509 0.38279172 0.42381267]
ti_coms = [0.08655269 0.07179457 0.08848992 0.07970232 0.07238837 0.0743604
 0.07938708 0.07492943 0.11479281 0.07377186]
t_total = [28.54987831 28.54987831 28.54987831 28.54987831 28.54987831 28.54987831
 28.54987831 28.54987831 28.54987831 28.54987831]
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [1.16596177e-05 1.01081200e-04 1.12186639e-05 4.48354160e-07
 1.56066329e-04 1.71096574e-05 8.86629391e-07 3.16162585e-05
 1.47648345e-05 9.00786229e-06]
ene_total = [0.49728187 0.41773503 0.50837179 0.45733327 0.42429695 0.4276389
 0.45554967 0.43173622 0.65949319 0.42379718]
optimize_network iter = 0 obj = 4.703234079287297
eta = 0.6819493529625255
freqs = [38424917.91998439 78013241.65854445 37994067.71862566 12898298.99218661
 90209018.95065838 43241454.64083756 16185653.82662158 53086584.80672698
 42569513.0112679  34900040.52352524]
eta_min = 0.6819493529625276	eta_max = 0.6819493529625246
af = 0.016869127524841664	bf = 1.4894091419856037	zeta = 0.018556040277325832	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [2.90303184e-06 2.51673727e-05 2.79324241e-06 1.11631997e-07
 3.88576653e-05 4.25999219e-06 2.20754524e-07 7.87187088e-06
 3.67617412e-06 2.24279318e-06]
ene_total = [1.73801127 1.44623117 1.77687644 1.59993898 1.46089904 1.49353974
 1.59363288 1.50568747 2.3050487  1.48132066]
ti_comp = [0.41103184 0.42578996 0.40909461 0.41788221 0.42519616 0.42322413
 0.41819745 0.42265509 0.38279172 0.42381267]
ti_coms = [0.08655269 0.07179457 0.08848992 0.07970232 0.07238837 0.0743604
 0.07938708 0.07492943 0.11479281 0.07377186]
t_total = [28.54987831 28.54987831 28.54987831 28.54987831 28.54987831 28.54987831
 28.54987831 28.54987831 28.54987831 28.54987831]
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [1.16596177e-05 1.01081200e-04 1.12186639e-05 4.48354160e-07
 1.56066329e-04 1.71096574e-05 8.86629391e-07 3.16162585e-05
 1.47648345e-05 9.00786229e-06]
ene_total = [0.49728187 0.41773503 0.50837179 0.45733327 0.42429695 0.4276389
 0.45554967 0.43173622 0.65949319 0.42379718]
optimize_network iter = 1 obj = 4.703234079287284
eta = 0.6819493529625246
freqs = [38424917.91998437 78013241.65854444 37994067.71862565 12898298.99218661
 90209018.95065837 43241454.64083755 16185653.82662157 53086584.80672697
 42569513.01126787 34900040.52352523]
Done!
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [1.11621458e-05 9.67684474e-05 1.07400059e-05 4.29224582e-07
 1.49407568e-04 1.63796530e-05 8.48800263e-07 3.02673121e-05
 1.41348748e-05 8.62353083e-06]
ene_total = [0.00866643 0.00727623 0.00885973 0.00797066 0.00738824 0.00745242
 0.00793956 0.00752321 0.01149342 0.00738581]
At round 29 energy consumption: 0.08195570742412175
At round 29 eta: 0.6819493529625246
At round 29 a_n: 17.906227447174857
At round 29 local rounds: 12.534813137868813
At round 29 global rounds: 57.376941264307334
gradient difference: 0.5033179521560669
train() client id: f_00000-0-0 loss: 1.274499  [   32/  126]
train() client id: f_00000-0-1 loss: 1.270721  [   64/  126]
train() client id: f_00000-0-2 loss: 0.963783  [   96/  126]
train() client id: f_00000-1-0 loss: 1.060277  [   32/  126]
train() client id: f_00000-1-1 loss: 1.034887  [   64/  126]
train() client id: f_00000-1-2 loss: 1.034497  [   96/  126]
train() client id: f_00000-2-0 loss: 1.073472  [   32/  126]
train() client id: f_00000-2-1 loss: 1.031686  [   64/  126]
train() client id: f_00000-2-2 loss: 1.075391  [   96/  126]
train() client id: f_00000-3-0 loss: 0.961993  [   32/  126]
train() client id: f_00000-3-1 loss: 1.004516  [   64/  126]
train() client id: f_00000-3-2 loss: 1.000574  [   96/  126]
train() client id: f_00000-4-0 loss: 1.054230  [   32/  126]
train() client id: f_00000-4-1 loss: 0.829793  [   64/  126]
train() client id: f_00000-4-2 loss: 1.019351  [   96/  126]
train() client id: f_00000-5-0 loss: 0.920456  [   32/  126]
train() client id: f_00000-5-1 loss: 1.033637  [   64/  126]
train() client id: f_00000-5-2 loss: 0.871817  [   96/  126]
train() client id: f_00000-6-0 loss: 0.886675  [   32/  126]
train() client id: f_00000-6-1 loss: 0.849902  [   64/  126]
train() client id: f_00000-6-2 loss: 1.093258  [   96/  126]
train() client id: f_00000-7-0 loss: 0.927906  [   32/  126]
train() client id: f_00000-7-1 loss: 0.951256  [   64/  126]
train() client id: f_00000-7-2 loss: 0.902337  [   96/  126]
train() client id: f_00000-8-0 loss: 0.911947  [   32/  126]
train() client id: f_00000-8-1 loss: 0.838737  [   64/  126]
train() client id: f_00000-8-2 loss: 0.945637  [   96/  126]
train() client id: f_00000-9-0 loss: 0.908481  [   32/  126]
train() client id: f_00000-9-1 loss: 0.947898  [   64/  126]
train() client id: f_00000-9-2 loss: 0.922063  [   96/  126]
train() client id: f_00000-10-0 loss: 0.880795  [   32/  126]
train() client id: f_00000-10-1 loss: 0.991234  [   64/  126]
train() client id: f_00000-10-2 loss: 0.887717  [   96/  126]
train() client id: f_00000-11-0 loss: 0.938130  [   32/  126]
train() client id: f_00000-11-1 loss: 0.885979  [   64/  126]
train() client id: f_00000-11-2 loss: 0.990192  [   96/  126]
train() client id: f_00001-0-0 loss: 0.578829  [   32/  265]
train() client id: f_00001-0-1 loss: 0.454393  [   64/  265]
train() client id: f_00001-0-2 loss: 0.441027  [   96/  265]
train() client id: f_00001-0-3 loss: 0.587644  [  128/  265]
train() client id: f_00001-0-4 loss: 0.432841  [  160/  265]
train() client id: f_00001-0-5 loss: 0.391578  [  192/  265]
train() client id: f_00001-0-6 loss: 0.448409  [  224/  265]
train() client id: f_00001-0-7 loss: 0.470740  [  256/  265]
train() client id: f_00001-1-0 loss: 0.504382  [   32/  265]
train() client id: f_00001-1-1 loss: 0.472024  [   64/  265]
train() client id: f_00001-1-2 loss: 0.420555  [   96/  265]
train() client id: f_00001-1-3 loss: 0.355201  [  128/  265]
train() client id: f_00001-1-4 loss: 0.416086  [  160/  265]
train() client id: f_00001-1-5 loss: 0.422649  [  192/  265]
train() client id: f_00001-1-6 loss: 0.587359  [  224/  265]
train() client id: f_00001-1-7 loss: 0.456354  [  256/  265]
train() client id: f_00001-2-0 loss: 0.562992  [   32/  265]
train() client id: f_00001-2-1 loss: 0.382416  [   64/  265]
train() client id: f_00001-2-2 loss: 0.415050  [   96/  265]
train() client id: f_00001-2-3 loss: 0.513150  [  128/  265]
train() client id: f_00001-2-4 loss: 0.379100  [  160/  265]
train() client id: f_00001-2-5 loss: 0.569925  [  192/  265]
train() client id: f_00001-2-6 loss: 0.353803  [  224/  265]
train() client id: f_00001-2-7 loss: 0.511791  [  256/  265]
train() client id: f_00001-3-0 loss: 0.385312  [   32/  265]
train() client id: f_00001-3-1 loss: 0.431019  [   64/  265]
train() client id: f_00001-3-2 loss: 0.480791  [   96/  265]
train() client id: f_00001-3-3 loss: 0.463024  [  128/  265]
train() client id: f_00001-3-4 loss: 0.564088  [  160/  265]
train() client id: f_00001-3-5 loss: 0.518321  [  192/  265]
train() client id: f_00001-3-6 loss: 0.377179  [  224/  265]
train() client id: f_00001-3-7 loss: 0.373312  [  256/  265]
train() client id: f_00001-4-0 loss: 0.421769  [   32/  265]
train() client id: f_00001-4-1 loss: 0.515236  [   64/  265]
train() client id: f_00001-4-2 loss: 0.408882  [   96/  265]
train() client id: f_00001-4-3 loss: 0.372720  [  128/  265]
train() client id: f_00001-4-4 loss: 0.440982  [  160/  265]
train() client id: f_00001-4-5 loss: 0.409068  [  192/  265]
train() client id: f_00001-4-6 loss: 0.609207  [  224/  265]
train() client id: f_00001-4-7 loss: 0.454863  [  256/  265]
train() client id: f_00001-5-0 loss: 0.392021  [   32/  265]
train() client id: f_00001-5-1 loss: 0.364267  [   64/  265]
train() client id: f_00001-5-2 loss: 0.573045  [   96/  265]
train() client id: f_00001-5-3 loss: 0.443833  [  128/  265]
train() client id: f_00001-5-4 loss: 0.436403  [  160/  265]
train() client id: f_00001-5-5 loss: 0.486526  [  192/  265]
train() client id: f_00001-5-6 loss: 0.422368  [  224/  265]
train() client id: f_00001-5-7 loss: 0.465778  [  256/  265]
train() client id: f_00001-6-0 loss: 0.404323  [   32/  265]
train() client id: f_00001-6-1 loss: 0.546419  [   64/  265]
train() client id: f_00001-6-2 loss: 0.342843  [   96/  265]
train() client id: f_00001-6-3 loss: 0.413342  [  128/  265]
train() client id: f_00001-6-4 loss: 0.408298  [  160/  265]
train() client id: f_00001-6-5 loss: 0.473084  [  192/  265]
train() client id: f_00001-6-6 loss: 0.461162  [  224/  265]
train() client id: f_00001-6-7 loss: 0.553784  [  256/  265]
train() client id: f_00001-7-0 loss: 0.403283  [   32/  265]
train() client id: f_00001-7-1 loss: 0.342102  [   64/  265]
train() client id: f_00001-7-2 loss: 0.415091  [   96/  265]
train() client id: f_00001-7-3 loss: 0.453110  [  128/  265]
train() client id: f_00001-7-4 loss: 0.515011  [  160/  265]
train() client id: f_00001-7-5 loss: 0.552993  [  192/  265]
train() client id: f_00001-7-6 loss: 0.470838  [  224/  265]
train() client id: f_00001-7-7 loss: 0.428330  [  256/  265]
train() client id: f_00001-8-0 loss: 0.346830  [   32/  265]
train() client id: f_00001-8-1 loss: 0.450846  [   64/  265]
train() client id: f_00001-8-2 loss: 0.522654  [   96/  265]
train() client id: f_00001-8-3 loss: 0.401026  [  128/  265]
train() client id: f_00001-8-4 loss: 0.432071  [  160/  265]
train() client id: f_00001-8-5 loss: 0.453235  [  192/  265]
train() client id: f_00001-8-6 loss: 0.479348  [  224/  265]
train() client id: f_00001-8-7 loss: 0.482403  [  256/  265]
train() client id: f_00001-9-0 loss: 0.574919  [   32/  265]
train() client id: f_00001-9-1 loss: 0.404727  [   64/  265]
train() client id: f_00001-9-2 loss: 0.353942  [   96/  265]
train() client id: f_00001-9-3 loss: 0.385580  [  128/  265]
train() client id: f_00001-9-4 loss: 0.420430  [  160/  265]
train() client id: f_00001-9-5 loss: 0.472272  [  192/  265]
train() client id: f_00001-9-6 loss: 0.416373  [  224/  265]
train() client id: f_00001-9-7 loss: 0.487224  [  256/  265]
train() client id: f_00001-10-0 loss: 0.600611  [   32/  265]
train() client id: f_00001-10-1 loss: 0.420594  [   64/  265]
train() client id: f_00001-10-2 loss: 0.346400  [   96/  265]
train() client id: f_00001-10-3 loss: 0.547114  [  128/  265]
train() client id: f_00001-10-4 loss: 0.444876  [  160/  265]
train() client id: f_00001-10-5 loss: 0.407289  [  192/  265]
train() client id: f_00001-10-6 loss: 0.381552  [  224/  265]
train() client id: f_00001-10-7 loss: 0.413993  [  256/  265]
train() client id: f_00001-11-0 loss: 0.452080  [   32/  265]
train() client id: f_00001-11-1 loss: 0.509635  [   64/  265]
train() client id: f_00001-11-2 loss: 0.350995  [   96/  265]
train() client id: f_00001-11-3 loss: 0.354629  [  128/  265]
train() client id: f_00001-11-4 loss: 0.551649  [  160/  265]
train() client id: f_00001-11-5 loss: 0.503728  [  192/  265]
train() client id: f_00001-11-6 loss: 0.375659  [  224/  265]
train() client id: f_00001-11-7 loss: 0.470574  [  256/  265]
train() client id: f_00002-0-0 loss: 1.361607  [   32/  124]
train() client id: f_00002-0-1 loss: 1.215080  [   64/  124]
train() client id: f_00002-0-2 loss: 1.256774  [   96/  124]
train() client id: f_00002-1-0 loss: 1.244951  [   32/  124]
train() client id: f_00002-1-1 loss: 1.135909  [   64/  124]
train() client id: f_00002-1-2 loss: 1.391841  [   96/  124]
train() client id: f_00002-2-0 loss: 1.331508  [   32/  124]
train() client id: f_00002-2-1 loss: 1.179953  [   64/  124]
train() client id: f_00002-2-2 loss: 1.077749  [   96/  124]
train() client id: f_00002-3-0 loss: 1.088464  [   32/  124]
train() client id: f_00002-3-1 loss: 1.169829  [   64/  124]
train() client id: f_00002-3-2 loss: 1.277919  [   96/  124]
train() client id: f_00002-4-0 loss: 1.232713  [   32/  124]
train() client id: f_00002-4-1 loss: 1.051499  [   64/  124]
train() client id: f_00002-4-2 loss: 1.087869  [   96/  124]
train() client id: f_00002-5-0 loss: 1.147325  [   32/  124]
train() client id: f_00002-5-1 loss: 1.096557  [   64/  124]
train() client id: f_00002-5-2 loss: 1.308263  [   96/  124]
train() client id: f_00002-6-0 loss: 1.045717  [   32/  124]
train() client id: f_00002-6-1 loss: 1.218050  [   64/  124]
train() client id: f_00002-6-2 loss: 1.040600  [   96/  124]
train() client id: f_00002-7-0 loss: 1.182152  [   32/  124]
train() client id: f_00002-7-1 loss: 1.068960  [   64/  124]
train() client id: f_00002-7-2 loss: 1.093836  [   96/  124]
train() client id: f_00002-8-0 loss: 1.152237  [   32/  124]
train() client id: f_00002-8-1 loss: 1.094527  [   64/  124]
train() client id: f_00002-8-2 loss: 1.068731  [   96/  124]
train() client id: f_00002-9-0 loss: 0.991063  [   32/  124]
train() client id: f_00002-9-1 loss: 1.093633  [   64/  124]
train() client id: f_00002-9-2 loss: 1.079538  [   96/  124]
train() client id: f_00002-10-0 loss: 1.216805  [   32/  124]
train() client id: f_00002-10-1 loss: 1.004173  [   64/  124]
train() client id: f_00002-10-2 loss: 1.073920  [   96/  124]
train() client id: f_00002-11-0 loss: 1.123819  [   32/  124]
train() client id: f_00002-11-1 loss: 1.199431  [   64/  124]
train() client id: f_00002-11-2 loss: 1.022982  [   96/  124]
train() client id: f_00003-0-0 loss: 0.724969  [   32/   43]
train() client id: f_00003-1-0 loss: 0.689030  [   32/   43]
train() client id: f_00003-2-0 loss: 0.675580  [   32/   43]
train() client id: f_00003-3-0 loss: 0.606954  [   32/   43]
train() client id: f_00003-4-0 loss: 0.632737  [   32/   43]
train() client id: f_00003-5-0 loss: 0.465571  [   32/   43]
train() client id: f_00003-6-0 loss: 0.679274  [   32/   43]
train() client id: f_00003-7-0 loss: 0.626994  [   32/   43]
train() client id: f_00003-8-0 loss: 0.702283  [   32/   43]
train() client id: f_00003-9-0 loss: 0.686713  [   32/   43]
train() client id: f_00003-10-0 loss: 0.589453  [   32/   43]
train() client id: f_00003-11-0 loss: 0.657475  [   32/   43]
train() client id: f_00004-0-0 loss: 0.902851  [   32/  306]
train() client id: f_00004-0-1 loss: 0.767314  [   64/  306]
train() client id: f_00004-0-2 loss: 0.719754  [   96/  306]
train() client id: f_00004-0-3 loss: 0.986130  [  128/  306]
train() client id: f_00004-0-4 loss: 0.705022  [  160/  306]
train() client id: f_00004-0-5 loss: 1.024851  [  192/  306]
train() client id: f_00004-0-6 loss: 0.838757  [  224/  306]
train() client id: f_00004-0-7 loss: 0.711021  [  256/  306]
train() client id: f_00004-0-8 loss: 0.947958  [  288/  306]
train() client id: f_00004-1-0 loss: 0.963695  [   32/  306]
train() client id: f_00004-1-1 loss: 0.913960  [   64/  306]
train() client id: f_00004-1-2 loss: 0.724583  [   96/  306]
train() client id: f_00004-1-3 loss: 0.884037  [  128/  306]
train() client id: f_00004-1-4 loss: 0.910293  [  160/  306]
train() client id: f_00004-1-5 loss: 0.849670  [  192/  306]
train() client id: f_00004-1-6 loss: 0.777439  [  224/  306]
train() client id: f_00004-1-7 loss: 0.807082  [  256/  306]
train() client id: f_00004-1-8 loss: 0.919015  [  288/  306]
train() client id: f_00004-2-0 loss: 0.864037  [   32/  306]
train() client id: f_00004-2-1 loss: 0.842749  [   64/  306]
train() client id: f_00004-2-2 loss: 0.962232  [   96/  306]
train() client id: f_00004-2-3 loss: 0.773849  [  128/  306]
train() client id: f_00004-2-4 loss: 0.770939  [  160/  306]
train() client id: f_00004-2-5 loss: 0.787089  [  192/  306]
train() client id: f_00004-2-6 loss: 0.964899  [  224/  306]
train() client id: f_00004-2-7 loss: 0.768357  [  256/  306]
train() client id: f_00004-2-8 loss: 0.930634  [  288/  306]
train() client id: f_00004-3-0 loss: 0.774044  [   32/  306]
train() client id: f_00004-3-1 loss: 0.844625  [   64/  306]
train() client id: f_00004-3-2 loss: 0.734920  [   96/  306]
train() client id: f_00004-3-3 loss: 0.907891  [  128/  306]
train() client id: f_00004-3-4 loss: 0.796142  [  160/  306]
train() client id: f_00004-3-5 loss: 0.974085  [  192/  306]
train() client id: f_00004-3-6 loss: 1.032335  [  224/  306]
train() client id: f_00004-3-7 loss: 0.818219  [  256/  306]
train() client id: f_00004-3-8 loss: 0.760387  [  288/  306]
train() client id: f_00004-4-0 loss: 0.970934  [   32/  306]
train() client id: f_00004-4-1 loss: 0.805025  [   64/  306]
train() client id: f_00004-4-2 loss: 0.925655  [   96/  306]
train() client id: f_00004-4-3 loss: 1.028817  [  128/  306]
train() client id: f_00004-4-4 loss: 0.792225  [  160/  306]
train() client id: f_00004-4-5 loss: 0.903450  [  192/  306]
train() client id: f_00004-4-6 loss: 0.744365  [  224/  306]
train() client id: f_00004-4-7 loss: 0.771823  [  256/  306]
train() client id: f_00004-4-8 loss: 0.850590  [  288/  306]
train() client id: f_00004-5-0 loss: 0.950424  [   32/  306]
train() client id: f_00004-5-1 loss: 0.951307  [   64/  306]
train() client id: f_00004-5-2 loss: 0.847371  [   96/  306]
train() client id: f_00004-5-3 loss: 0.860559  [  128/  306]
train() client id: f_00004-5-4 loss: 0.953000  [  160/  306]
train() client id: f_00004-5-5 loss: 0.805007  [  192/  306]
train() client id: f_00004-5-6 loss: 0.828063  [  224/  306]
train() client id: f_00004-5-7 loss: 0.726430  [  256/  306]
train() client id: f_00004-5-8 loss: 0.721687  [  288/  306]
train() client id: f_00004-6-0 loss: 0.949117  [   32/  306]
train() client id: f_00004-6-1 loss: 0.792157  [   64/  306]
train() client id: f_00004-6-2 loss: 0.824645  [   96/  306]
train() client id: f_00004-6-3 loss: 0.841999  [  128/  306]
train() client id: f_00004-6-4 loss: 0.882569  [  160/  306]
train() client id: f_00004-6-5 loss: 0.904254  [  192/  306]
train() client id: f_00004-6-6 loss: 0.816470  [  224/  306]
train() client id: f_00004-6-7 loss: 0.792969  [  256/  306]
train() client id: f_00004-6-8 loss: 0.822474  [  288/  306]
train() client id: f_00004-7-0 loss: 0.944267  [   32/  306]
train() client id: f_00004-7-1 loss: 0.785056  [   64/  306]
train() client id: f_00004-7-2 loss: 0.738984  [   96/  306]
train() client id: f_00004-7-3 loss: 0.816927  [  128/  306]
train() client id: f_00004-7-4 loss: 0.885256  [  160/  306]
train() client id: f_00004-7-5 loss: 0.849576  [  192/  306]
train() client id: f_00004-7-6 loss: 0.886020  [  224/  306]
train() client id: f_00004-7-7 loss: 0.852472  [  256/  306]
train() client id: f_00004-7-8 loss: 0.860273  [  288/  306]
train() client id: f_00004-8-0 loss: 0.791819  [   32/  306]
train() client id: f_00004-8-1 loss: 0.981014  [   64/  306]
train() client id: f_00004-8-2 loss: 0.820490  [   96/  306]
train() client id: f_00004-8-3 loss: 0.766286  [  128/  306]
train() client id: f_00004-8-4 loss: 0.872502  [  160/  306]
train() client id: f_00004-8-5 loss: 0.861956  [  192/  306]
train() client id: f_00004-8-6 loss: 0.773948  [  224/  306]
train() client id: f_00004-8-7 loss: 0.955594  [  256/  306]
train() client id: f_00004-8-8 loss: 0.909754  [  288/  306]
train() client id: f_00004-9-0 loss: 0.860415  [   32/  306]
train() client id: f_00004-9-1 loss: 0.867369  [   64/  306]
train() client id: f_00004-9-2 loss: 0.805370  [   96/  306]
train() client id: f_00004-9-3 loss: 0.903730  [  128/  306]
train() client id: f_00004-9-4 loss: 0.805722  [  160/  306]
train() client id: f_00004-9-5 loss: 0.872206  [  192/  306]
train() client id: f_00004-9-6 loss: 0.981848  [  224/  306]
train() client id: f_00004-9-7 loss: 0.743815  [  256/  306]
train() client id: f_00004-9-8 loss: 0.816104  [  288/  306]
train() client id: f_00004-10-0 loss: 0.890926  [   32/  306]
train() client id: f_00004-10-1 loss: 0.729905  [   64/  306]
train() client id: f_00004-10-2 loss: 0.778643  [   96/  306]
train() client id: f_00004-10-3 loss: 1.004622  [  128/  306]
train() client id: f_00004-10-4 loss: 0.840125  [  160/  306]
train() client id: f_00004-10-5 loss: 0.825424  [  192/  306]
train() client id: f_00004-10-6 loss: 0.774520  [  224/  306]
train() client id: f_00004-10-7 loss: 0.876074  [  256/  306]
train() client id: f_00004-10-8 loss: 0.881648  [  288/  306]
train() client id: f_00004-11-0 loss: 0.984482  [   32/  306]
train() client id: f_00004-11-1 loss: 0.907707  [   64/  306]
train() client id: f_00004-11-2 loss: 0.962568  [   96/  306]
train() client id: f_00004-11-3 loss: 0.798557  [  128/  306]
train() client id: f_00004-11-4 loss: 0.783214  [  160/  306]
train() client id: f_00004-11-5 loss: 0.880308  [  192/  306]
train() client id: f_00004-11-6 loss: 0.774592  [  224/  306]
train() client id: f_00004-11-7 loss: 0.837722  [  256/  306]
train() client id: f_00004-11-8 loss: 0.778404  [  288/  306]
train() client id: f_00005-0-0 loss: 0.711815  [   32/  146]
train() client id: f_00005-0-1 loss: 0.681348  [   64/  146]
train() client id: f_00005-0-2 loss: 0.563611  [   96/  146]
train() client id: f_00005-0-3 loss: 0.830095  [  128/  146]
train() client id: f_00005-1-0 loss: 0.649395  [   32/  146]
train() client id: f_00005-1-1 loss: 0.629779  [   64/  146]
train() client id: f_00005-1-2 loss: 0.632873  [   96/  146]
train() client id: f_00005-1-3 loss: 0.759076  [  128/  146]
train() client id: f_00005-2-0 loss: 0.690808  [   32/  146]
train() client id: f_00005-2-1 loss: 0.513141  [   64/  146]
train() client id: f_00005-2-2 loss: 0.677736  [   96/  146]
train() client id: f_00005-2-3 loss: 0.791616  [  128/  146]
train() client id: f_00005-3-0 loss: 0.512318  [   32/  146]
train() client id: f_00005-3-1 loss: 0.819674  [   64/  146]
train() client id: f_00005-3-2 loss: 0.679760  [   96/  146]
train() client id: f_00005-3-3 loss: 0.718392  [  128/  146]
train() client id: f_00005-4-0 loss: 0.869936  [   32/  146]
train() client id: f_00005-4-1 loss: 0.849395  [   64/  146]
train() client id: f_00005-4-2 loss: 0.512399  [   96/  146]
train() client id: f_00005-4-3 loss: 0.591205  [  128/  146]
train() client id: f_00005-5-0 loss: 0.753578  [   32/  146]
train() client id: f_00005-5-1 loss: 0.624930  [   64/  146]
train() client id: f_00005-5-2 loss: 0.809754  [   96/  146]
train() client id: f_00005-5-3 loss: 0.416904  [  128/  146]
train() client id: f_00005-6-0 loss: 0.819204  [   32/  146]
train() client id: f_00005-6-1 loss: 0.655120  [   64/  146]
train() client id: f_00005-6-2 loss: 0.684135  [   96/  146]
train() client id: f_00005-6-3 loss: 0.572247  [  128/  146]
train() client id: f_00005-7-0 loss: 0.789721  [   32/  146]
train() client id: f_00005-7-1 loss: 0.858942  [   64/  146]
train() client id: f_00005-7-2 loss: 0.643623  [   96/  146]
train() client id: f_00005-7-3 loss: 0.466836  [  128/  146]
train() client id: f_00005-8-0 loss: 0.837500  [   32/  146]
train() client id: f_00005-8-1 loss: 0.756587  [   64/  146]
train() client id: f_00005-8-2 loss: 0.384173  [   96/  146]
train() client id: f_00005-8-3 loss: 0.858073  [  128/  146]
train() client id: f_00005-9-0 loss: 0.735417  [   32/  146]
train() client id: f_00005-9-1 loss: 0.999178  [   64/  146]
train() client id: f_00005-9-2 loss: 0.480523  [   96/  146]
train() client id: f_00005-9-3 loss: 0.569399  [  128/  146]
train() client id: f_00005-10-0 loss: 0.706644  [   32/  146]
train() client id: f_00005-10-1 loss: 0.852530  [   64/  146]
train() client id: f_00005-10-2 loss: 0.509848  [   96/  146]
train() client id: f_00005-10-3 loss: 0.743203  [  128/  146]
train() client id: f_00005-11-0 loss: 0.742880  [   32/  146]
train() client id: f_00005-11-1 loss: 0.687852  [   64/  146]
train() client id: f_00005-11-2 loss: 0.692546  [   96/  146]
train() client id: f_00005-11-3 loss: 0.678953  [  128/  146]
train() client id: f_00006-0-0 loss: 0.584676  [   32/   54]
train() client id: f_00006-1-0 loss: 0.530921  [   32/   54]
train() client id: f_00006-2-0 loss: 0.547915  [   32/   54]
train() client id: f_00006-3-0 loss: 0.545488  [   32/   54]
train() client id: f_00006-4-0 loss: 0.491405  [   32/   54]
train() client id: f_00006-5-0 loss: 0.484908  [   32/   54]
train() client id: f_00006-6-0 loss: 0.585045  [   32/   54]
train() client id: f_00006-7-0 loss: 0.534234  [   32/   54]
train() client id: f_00006-8-0 loss: 0.556590  [   32/   54]
train() client id: f_00006-9-0 loss: 0.586605  [   32/   54]
train() client id: f_00006-10-0 loss: 0.610265  [   32/   54]
train() client id: f_00006-11-0 loss: 0.527925  [   32/   54]
train() client id: f_00007-0-0 loss: 0.803782  [   32/  179]
train() client id: f_00007-0-1 loss: 0.428787  [   64/  179]
train() client id: f_00007-0-2 loss: 0.437468  [   96/  179]
train() client id: f_00007-0-3 loss: 0.649704  [  128/  179]
train() client id: f_00007-0-4 loss: 0.636444  [  160/  179]
train() client id: f_00007-1-0 loss: 0.658014  [   32/  179]
train() client id: f_00007-1-1 loss: 0.436904  [   64/  179]
train() client id: f_00007-1-2 loss: 0.655657  [   96/  179]
train() client id: f_00007-1-3 loss: 0.447190  [  128/  179]
train() client id: f_00007-1-4 loss: 0.504571  [  160/  179]
train() client id: f_00007-2-0 loss: 0.433172  [   32/  179]
train() client id: f_00007-2-1 loss: 0.617722  [   64/  179]
train() client id: f_00007-2-2 loss: 0.482090  [   96/  179]
train() client id: f_00007-2-3 loss: 0.647742  [  128/  179]
train() client id: f_00007-2-4 loss: 0.526177  [  160/  179]
train() client id: f_00007-3-0 loss: 0.582333  [   32/  179]
train() client id: f_00007-3-1 loss: 0.609270  [   64/  179]
train() client id: f_00007-3-2 loss: 0.433797  [   96/  179]
train() client id: f_00007-3-3 loss: 0.570161  [  128/  179]
train() client id: f_00007-3-4 loss: 0.427712  [  160/  179]
train() client id: f_00007-4-0 loss: 0.655970  [   32/  179]
train() client id: f_00007-4-1 loss: 0.467093  [   64/  179]
train() client id: f_00007-4-2 loss: 0.478661  [   96/  179]
train() client id: f_00007-4-3 loss: 0.649378  [  128/  179]
train() client id: f_00007-4-4 loss: 0.460565  [  160/  179]
train() client id: f_00007-5-0 loss: 0.584902  [   32/  179]
train() client id: f_00007-5-1 loss: 0.552451  [   64/  179]
train() client id: f_00007-5-2 loss: 0.372901  [   96/  179]
train() client id: f_00007-5-3 loss: 0.393239  [  128/  179]
train() client id: f_00007-5-4 loss: 0.705153  [  160/  179]
train() client id: f_00007-6-0 loss: 0.514116  [   32/  179]
train() client id: f_00007-6-1 loss: 0.447231  [   64/  179]
train() client id: f_00007-6-2 loss: 0.489206  [   96/  179]
train() client id: f_00007-6-3 loss: 0.647574  [  128/  179]
train() client id: f_00007-6-4 loss: 0.439971  [  160/  179]
train() client id: f_00007-7-0 loss: 0.472448  [   32/  179]
train() client id: f_00007-7-1 loss: 0.373113  [   64/  179]
train() client id: f_00007-7-2 loss: 0.618855  [   96/  179]
train() client id: f_00007-7-3 loss: 0.620607  [  128/  179]
train() client id: f_00007-7-4 loss: 0.486630  [  160/  179]
train() client id: f_00007-8-0 loss: 0.656080  [   32/  179]
train() client id: f_00007-8-1 loss: 0.450963  [   64/  179]
train() client id: f_00007-8-2 loss: 0.476689  [   96/  179]
train() client id: f_00007-8-3 loss: 0.469943  [  128/  179]
train() client id: f_00007-8-4 loss: 0.481085  [  160/  179]
train() client id: f_00007-9-0 loss: 0.547716  [   32/  179]
train() client id: f_00007-9-1 loss: 0.415628  [   64/  179]
train() client id: f_00007-9-2 loss: 0.465629  [   96/  179]
train() client id: f_00007-9-3 loss: 0.629978  [  128/  179]
train() client id: f_00007-9-4 loss: 0.493422  [  160/  179]
train() client id: f_00007-10-0 loss: 0.413509  [   32/  179]
train() client id: f_00007-10-1 loss: 0.550552  [   64/  179]
train() client id: f_00007-10-2 loss: 0.438183  [   96/  179]
train() client id: f_00007-10-3 loss: 0.792733  [  128/  179]
train() client id: f_00007-10-4 loss: 0.451058  [  160/  179]
train() client id: f_00007-11-0 loss: 0.593578  [   32/  179]
train() client id: f_00007-11-1 loss: 0.526148  [   64/  179]
train() client id: f_00007-11-2 loss: 0.496026  [   96/  179]
train() client id: f_00007-11-3 loss: 0.439436  [  128/  179]
train() client id: f_00007-11-4 loss: 0.459422  [  160/  179]
train() client id: f_00008-0-0 loss: 0.883441  [   32/  130]
train() client id: f_00008-0-1 loss: 0.790124  [   64/  130]
train() client id: f_00008-0-2 loss: 0.798941  [   96/  130]
train() client id: f_00008-0-3 loss: 0.709317  [  128/  130]
train() client id: f_00008-1-0 loss: 0.832751  [   32/  130]
train() client id: f_00008-1-1 loss: 0.807403  [   64/  130]
train() client id: f_00008-1-2 loss: 0.731045  [   96/  130]
train() client id: f_00008-1-3 loss: 0.810253  [  128/  130]
train() client id: f_00008-2-0 loss: 0.814208  [   32/  130]
train() client id: f_00008-2-1 loss: 0.744374  [   64/  130]
train() client id: f_00008-2-2 loss: 0.697309  [   96/  130]
train() client id: f_00008-2-3 loss: 0.884989  [  128/  130]
train() client id: f_00008-3-0 loss: 0.860177  [   32/  130]
train() client id: f_00008-3-1 loss: 0.705125  [   64/  130]
train() client id: f_00008-3-2 loss: 0.764670  [   96/  130]
train() client id: f_00008-3-3 loss: 0.810667  [  128/  130]
train() client id: f_00008-4-0 loss: 0.759424  [   32/  130]
train() client id: f_00008-4-1 loss: 0.883274  [   64/  130]
train() client id: f_00008-4-2 loss: 0.852446  [   96/  130]
train() client id: f_00008-4-3 loss: 0.671608  [  128/  130]
train() client id: f_00008-5-0 loss: 0.764675  [   32/  130]
train() client id: f_00008-5-1 loss: 0.924027  [   64/  130]
train() client id: f_00008-5-2 loss: 0.725345  [   96/  130]
train() client id: f_00008-5-3 loss: 0.760477  [  128/  130]
train() client id: f_00008-6-0 loss: 0.839254  [   32/  130]
train() client id: f_00008-6-1 loss: 0.836074  [   64/  130]
train() client id: f_00008-6-2 loss: 0.713348  [   96/  130]
train() client id: f_00008-6-3 loss: 0.778546  [  128/  130]
train() client id: f_00008-7-0 loss: 0.759494  [   32/  130]
train() client id: f_00008-7-1 loss: 0.727405  [   64/  130]
train() client id: f_00008-7-2 loss: 0.846097  [   96/  130]
train() client id: f_00008-7-3 loss: 0.844274  [  128/  130]
train() client id: f_00008-8-0 loss: 0.742649  [   32/  130]
train() client id: f_00008-8-1 loss: 0.768480  [   64/  130]
train() client id: f_00008-8-2 loss: 0.884061  [   96/  130]
train() client id: f_00008-8-3 loss: 0.773975  [  128/  130]
train() client id: f_00008-9-0 loss: 0.860856  [   32/  130]
train() client id: f_00008-9-1 loss: 0.748302  [   64/  130]
train() client id: f_00008-9-2 loss: 0.839528  [   96/  130]
train() client id: f_00008-9-3 loss: 0.725484  [  128/  130]
train() client id: f_00008-10-0 loss: 0.741585  [   32/  130]
train() client id: f_00008-10-1 loss: 0.867757  [   64/  130]
train() client id: f_00008-10-2 loss: 0.803932  [   96/  130]
train() client id: f_00008-10-3 loss: 0.745498  [  128/  130]
train() client id: f_00008-11-0 loss: 0.905594  [   32/  130]
train() client id: f_00008-11-1 loss: 0.711077  [   64/  130]
train() client id: f_00008-11-2 loss: 0.775891  [   96/  130]
train() client id: f_00008-11-3 loss: 0.784104  [  128/  130]
train() client id: f_00009-0-0 loss: 1.168797  [   32/  118]
train() client id: f_00009-0-1 loss: 0.968163  [   64/  118]
train() client id: f_00009-0-2 loss: 1.122059  [   96/  118]
train() client id: f_00009-1-0 loss: 0.929631  [   32/  118]
train() client id: f_00009-1-1 loss: 1.159362  [   64/  118]
train() client id: f_00009-1-2 loss: 0.965528  [   96/  118]
train() client id: f_00009-2-0 loss: 1.074951  [   32/  118]
train() client id: f_00009-2-1 loss: 0.924314  [   64/  118]
train() client id: f_00009-2-2 loss: 1.036113  [   96/  118]
train() client id: f_00009-3-0 loss: 1.015342  [   32/  118]
train() client id: f_00009-3-1 loss: 0.933937  [   64/  118]
train() client id: f_00009-3-2 loss: 0.930135  [   96/  118]
train() client id: f_00009-4-0 loss: 0.917494  [   32/  118]
train() client id: f_00009-4-1 loss: 1.045811  [   64/  118]
train() client id: f_00009-4-2 loss: 0.851566  [   96/  118]
train() client id: f_00009-5-0 loss: 0.978158  [   32/  118]
train() client id: f_00009-5-1 loss: 0.903444  [   64/  118]
train() client id: f_00009-5-2 loss: 0.869817  [   96/  118]
train() client id: f_00009-6-0 loss: 0.850557  [   32/  118]
train() client id: f_00009-6-1 loss: 0.921217  [   64/  118]
train() client id: f_00009-6-2 loss: 0.859341  [   96/  118]
train() client id: f_00009-7-0 loss: 0.925252  [   32/  118]
train() client id: f_00009-7-1 loss: 0.898479  [   64/  118]
train() client id: f_00009-7-2 loss: 0.847272  [   96/  118]
train() client id: f_00009-8-0 loss: 0.744417  [   32/  118]
train() client id: f_00009-8-1 loss: 0.973720  [   64/  118]
train() client id: f_00009-8-2 loss: 0.915124  [   96/  118]
train() client id: f_00009-9-0 loss: 0.804295  [   32/  118]
train() client id: f_00009-9-1 loss: 1.038207  [   64/  118]
train() client id: f_00009-9-2 loss: 0.917053  [   96/  118]
train() client id: f_00009-10-0 loss: 0.966277  [   32/  118]
train() client id: f_00009-10-1 loss: 0.876839  [   64/  118]
train() client id: f_00009-10-2 loss: 0.750879  [   96/  118]
train() client id: f_00009-11-0 loss: 0.856876  [   32/  118]
train() client id: f_00009-11-1 loss: 0.916238  [   64/  118]
train() client id: f_00009-11-2 loss: 0.882026  [   96/  118]
At round 29 accuracy: 0.6551724137931034
At round 29 training accuracy: 0.5848423876592891
At round 29 training loss: 0.824702471427881
update_location
xs = [  -3.9056584     4.20031788  165.00902392   18.81129433    0.97929623
    3.95640986 -127.44319194 -106.32485185  149.66397685  -92.06087855]
ys = [ 157.5879595   140.55583871    1.32061395 -127.45517586  119.35018685
  102.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [186.67945561 172.55024331 192.95005052 163.09103791 155.70942849
 143.47962494 162.01437502 145.96455178 180.85346538 135.9824151 ]
dists_bs = [171.9780593  182.48055834 381.79924762 359.22409609 184.56872542
 192.96681959 183.881171   187.22748312 360.82019861 190.05102236]
uav_gains = [2.06621021e-11 2.54082094e-11 1.88791154e-11 2.93478532e-11
 3.29966588e-11 4.05296333e-11 2.98453693e-11 3.88208176e-11
 2.24815448e-11 4.63629961e-11]
bs_gains = [6.08092382e-11 5.15095318e-11 6.51862344e-12 7.73162525e-12
 4.98943499e-11 4.40496737e-11 5.04184808e-11 4.79357148e-11
 7.63624282e-12 4.59681998e-11]
Round 30
-------------------------------
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.03213939 14.58505974  6.92666737  2.4917993  16.82243501  8.09793447
  3.09128315  9.90171672  7.30136856  6.56936016]
obj_prev = 82.8197638713389
eta_min = 6.720783557496832e-14	eta_max = 0.9260472326332857
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 19.233152927962998	eta = 0.9090909090909091
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 34.518956732515555	eta = 0.5065241286245609
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 27.08219396594083	eta = 0.6456155103960739
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 25.74057952697607	eta = 0.6792653779081567
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 25.671393607462342	eta = 0.6810960381552402
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 25.671195205593733	eta = 0.6811013020600015
eta = 0.6811013020600015
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [0.03169041 0.06665046 0.03118739 0.01081498 0.07696242 0.03672063
 0.0135816  0.0450205  0.03269645 0.02967832]
ene_total = [2.2727313  4.14429542 2.25574611 1.06104318 4.72594168 2.47696442
 1.21464967 2.95433273 2.48764402 2.07784665]
ti_comp = [0.41970979 0.43599649 0.41770452 0.42674957 0.43552852 0.43363939
 0.42705945 0.43162177 0.39149563 0.43429663]
ti_coms = [0.08785512 0.07156842 0.08986039 0.08081534 0.07203639 0.07392552
 0.08050546 0.07594314 0.11606928 0.07326828]
t_total = [28.49987411 28.49987411 28.49987411 28.49987411 28.49987411 28.49987411
 28.49987411 28.49987411 28.49987411 28.49987411]
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [1.12918568e-05 9.73472227e-05 1.08662233e-05 4.34121306e-07
 1.50204523e-04 1.64570901e-05 8.58532087e-07 3.06128901e-05
 1.42537176e-05 8.66213655e-06]
ene_total = [0.49394235 0.40732423 0.50517807 0.45380416 0.41291985 0.4160174
 0.45208802 0.42814121 0.65253176 0.41188928]
optimize_network iter = 0 obj = 4.633836318316938
eta = 0.6811013020600015
freqs = [37752763.88864101 76434633.93405241 37331875.99521733 12671343.57363147
 88355204.53366868 42340057.1595814  15901303.6163569  52152723.73147339
 41758389.65718008 34168259.23522189]
eta_min = 0.6811013020600248	eta_max = 0.6811013020600005
af = 0.015898726867838657	bf = 1.4716185540757498	zeta = 0.017488599554622525	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [2.80235665e-06 2.41591478e-05 2.69672506e-06 1.07738059e-07
 3.72770088e-05 4.08423848e-06 2.13066208e-07 7.59735426e-06
 3.53741648e-06 2.14972581e-06]
ene_total = [1.73102112 1.4144311  1.77049778 1.59182881 1.42623245 1.45690442
 1.58574596 1.49733708 2.28689603 1.44357775]
ti_comp = [0.41970979 0.43599649 0.41770452 0.42674957 0.43552852 0.43363939
 0.42705945 0.43162177 0.39149563 0.43429663]
ti_coms = [0.08785512 0.07156842 0.08986039 0.08081534 0.07203639 0.07392552
 0.08050546 0.07594314 0.11606928 0.07326828]
t_total = [28.49987411 28.49987411 28.49987411 28.49987411 28.49987411 28.49987411
 28.49987411 28.49987411 28.49987411 28.49987411]
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [1.12918568e-05 9.73472227e-05 1.08662233e-05 4.34121306e-07
 1.50204523e-04 1.64570901e-05 8.58532087e-07 3.06128901e-05
 1.42537176e-05 8.66213655e-06]
ene_total = [0.49394235 0.40732423 0.50517807 0.45380416 0.41291985 0.4160174
 0.45208802 0.42814121 0.65253176 0.41188928]
optimize_network iter = 1 obj = 4.633836318316922
eta = 0.6811013020600005
freqs = [37752763.88864101 76434633.93405241 37331875.99521733 12671343.57363147
 88355204.53366868 42340057.1595814  15901303.6163569  52152723.73147339
 41758389.65718006 34168259.23522189]
Done!
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [1.07750501e-05 9.28918269e-05 1.03688971e-05 4.14252406e-07
 1.43329950e-04 1.57038807e-05 8.19238720e-07 2.92117968e-05
 1.36013522e-05 8.26568717e-06]
ene_total = [0.00879629 0.00724973 0.00899641 0.00808195 0.00734697 0.00740826
 0.00805137 0.00762353 0.01162053 0.00733509]
At round 30 energy consumption: 0.08251011673575459
At round 30 eta: 0.6811013020600005
At round 30 a_n: 17.56368160020554
At round 30 local rounds: 12.575559237281574
At round 30 global rounds: 56.15020557576531
gradient difference: 0.4008927345275879
train() client id: f_00000-0-0 loss: 1.256725  [   32/  126]
train() client id: f_00000-0-1 loss: 1.082366  [   64/  126]
train() client id: f_00000-0-2 loss: 1.233729  [   96/  126]
train() client id: f_00000-1-0 loss: 1.074756  [   32/  126]
train() client id: f_00000-1-1 loss: 1.281330  [   64/  126]
train() client id: f_00000-1-2 loss: 1.012499  [   96/  126]
train() client id: f_00000-2-0 loss: 0.953283  [   32/  126]
train() client id: f_00000-2-1 loss: 0.940407  [   64/  126]
train() client id: f_00000-2-2 loss: 1.006892  [   96/  126]
train() client id: f_00000-3-0 loss: 1.062778  [   32/  126]
train() client id: f_00000-3-1 loss: 0.939361  [   64/  126]
train() client id: f_00000-3-2 loss: 0.883128  [   96/  126]
train() client id: f_00000-4-0 loss: 0.839084  [   32/  126]
train() client id: f_00000-4-1 loss: 0.760863  [   64/  126]
train() client id: f_00000-4-2 loss: 1.027765  [   96/  126]
train() client id: f_00000-5-0 loss: 0.884786  [   32/  126]
train() client id: f_00000-5-1 loss: 0.946112  [   64/  126]
train() client id: f_00000-5-2 loss: 0.733459  [   96/  126]
train() client id: f_00000-6-0 loss: 0.799916  [   32/  126]
train() client id: f_00000-6-1 loss: 0.820583  [   64/  126]
train() client id: f_00000-6-2 loss: 0.741065  [   96/  126]
train() client id: f_00000-7-0 loss: 0.644598  [   32/  126]
train() client id: f_00000-7-1 loss: 0.855067  [   64/  126]
train() client id: f_00000-7-2 loss: 0.828279  [   96/  126]
train() client id: f_00000-8-0 loss: 0.875678  [   32/  126]
train() client id: f_00000-8-1 loss: 0.744677  [   64/  126]
train() client id: f_00000-8-2 loss: 0.764006  [   96/  126]
train() client id: f_00000-9-0 loss: 0.778265  [   32/  126]
train() client id: f_00000-9-1 loss: 0.777447  [   64/  126]
train() client id: f_00000-9-2 loss: 0.758207  [   96/  126]
train() client id: f_00000-10-0 loss: 0.777629  [   32/  126]
train() client id: f_00000-10-1 loss: 0.706961  [   64/  126]
train() client id: f_00000-10-2 loss: 0.799130  [   96/  126]
train() client id: f_00000-11-0 loss: 0.744385  [   32/  126]
train() client id: f_00000-11-1 loss: 0.768595  [   64/  126]
train() client id: f_00000-11-2 loss: 0.784770  [   96/  126]
train() client id: f_00001-0-0 loss: 0.469435  [   32/  265]
train() client id: f_00001-0-1 loss: 0.398748  [   64/  265]
train() client id: f_00001-0-2 loss: 0.454854  [   96/  265]
train() client id: f_00001-0-3 loss: 0.420169  [  128/  265]
train() client id: f_00001-0-4 loss: 0.431841  [  160/  265]
train() client id: f_00001-0-5 loss: 0.554155  [  192/  265]
train() client id: f_00001-0-6 loss: 0.384612  [  224/  265]
train() client id: f_00001-0-7 loss: 0.322660  [  256/  265]
train() client id: f_00001-1-0 loss: 0.312853  [   32/  265]
train() client id: f_00001-1-1 loss: 0.453634  [   64/  265]
train() client id: f_00001-1-2 loss: 0.477846  [   96/  265]
train() client id: f_00001-1-3 loss: 0.381371  [  128/  265]
train() client id: f_00001-1-4 loss: 0.340589  [  160/  265]
train() client id: f_00001-1-5 loss: 0.402356  [  192/  265]
train() client id: f_00001-1-6 loss: 0.567495  [  224/  265]
train() client id: f_00001-1-7 loss: 0.367007  [  256/  265]
train() client id: f_00001-2-0 loss: 0.395462  [   32/  265]
train() client id: f_00001-2-1 loss: 0.336457  [   64/  265]
train() client id: f_00001-2-2 loss: 0.346734  [   96/  265]
train() client id: f_00001-2-3 loss: 0.450664  [  128/  265]
train() client id: f_00001-2-4 loss: 0.502156  [  160/  265]
train() client id: f_00001-2-5 loss: 0.317289  [  192/  265]
train() client id: f_00001-2-6 loss: 0.518455  [  224/  265]
train() client id: f_00001-2-7 loss: 0.407895  [  256/  265]
train() client id: f_00001-3-0 loss: 0.354647  [   32/  265]
train() client id: f_00001-3-1 loss: 0.322048  [   64/  265]
train() client id: f_00001-3-2 loss: 0.344460  [   96/  265]
train() client id: f_00001-3-3 loss: 0.325163  [  128/  265]
train() client id: f_00001-3-4 loss: 0.343157  [  160/  265]
train() client id: f_00001-3-5 loss: 0.447099  [  192/  265]
train() client id: f_00001-3-6 loss: 0.431257  [  224/  265]
train() client id: f_00001-3-7 loss: 0.530433  [  256/  265]
train() client id: f_00001-4-0 loss: 0.365578  [   32/  265]
train() client id: f_00001-4-1 loss: 0.428722  [   64/  265]
train() client id: f_00001-4-2 loss: 0.417142  [   96/  265]
train() client id: f_00001-4-3 loss: 0.354808  [  128/  265]
train() client id: f_00001-4-4 loss: 0.499189  [  160/  265]
train() client id: f_00001-4-5 loss: 0.311149  [  192/  265]
train() client id: f_00001-4-6 loss: 0.368179  [  224/  265]
train() client id: f_00001-4-7 loss: 0.385467  [  256/  265]
train() client id: f_00001-5-0 loss: 0.346927  [   32/  265]
train() client id: f_00001-5-1 loss: 0.304087  [   64/  265]
train() client id: f_00001-5-2 loss: 0.358496  [   96/  265]
train() client id: f_00001-5-3 loss: 0.373057  [  128/  265]
train() client id: f_00001-5-4 loss: 0.359542  [  160/  265]
train() client id: f_00001-5-5 loss: 0.406004  [  192/  265]
train() client id: f_00001-5-6 loss: 0.460019  [  224/  265]
train() client id: f_00001-5-7 loss: 0.535220  [  256/  265]
train() client id: f_00001-6-0 loss: 0.441320  [   32/  265]
train() client id: f_00001-6-1 loss: 0.360952  [   64/  265]
train() client id: f_00001-6-2 loss: 0.358938  [   96/  265]
train() client id: f_00001-6-3 loss: 0.333027  [  128/  265]
train() client id: f_00001-6-4 loss: 0.323712  [  160/  265]
train() client id: f_00001-6-5 loss: 0.373705  [  192/  265]
train() client id: f_00001-6-6 loss: 0.294093  [  224/  265]
train() client id: f_00001-6-7 loss: 0.511787  [  256/  265]
train() client id: f_00001-7-0 loss: 0.373997  [   32/  265]
train() client id: f_00001-7-1 loss: 0.407323  [   64/  265]
train() client id: f_00001-7-2 loss: 0.398511  [   96/  265]
train() client id: f_00001-7-3 loss: 0.324618  [  128/  265]
train() client id: f_00001-7-4 loss: 0.510464  [  160/  265]
train() client id: f_00001-7-5 loss: 0.279918  [  192/  265]
train() client id: f_00001-7-6 loss: 0.308188  [  224/  265]
train() client id: f_00001-7-7 loss: 0.485846  [  256/  265]
train() client id: f_00001-8-0 loss: 0.368420  [   32/  265]
train() client id: f_00001-8-1 loss: 0.368875  [   64/  265]
train() client id: f_00001-8-2 loss: 0.447543  [   96/  265]
train() client id: f_00001-8-3 loss: 0.479787  [  128/  265]
train() client id: f_00001-8-4 loss: 0.281451  [  160/  265]
train() client id: f_00001-8-5 loss: 0.279915  [  192/  265]
train() client id: f_00001-8-6 loss: 0.445885  [  224/  265]
train() client id: f_00001-8-7 loss: 0.393638  [  256/  265]
train() client id: f_00001-9-0 loss: 0.281646  [   32/  265]
train() client id: f_00001-9-1 loss: 0.475893  [   64/  265]
train() client id: f_00001-9-2 loss: 0.375397  [   96/  265]
train() client id: f_00001-9-3 loss: 0.411958  [  128/  265]
train() client id: f_00001-9-4 loss: 0.336128  [  160/  265]
train() client id: f_00001-9-5 loss: 0.403903  [  192/  265]
train() client id: f_00001-9-6 loss: 0.411531  [  224/  265]
train() client id: f_00001-9-7 loss: 0.365967  [  256/  265]
train() client id: f_00001-10-0 loss: 0.519741  [   32/  265]
train() client id: f_00001-10-1 loss: 0.390038  [   64/  265]
train() client id: f_00001-10-2 loss: 0.367531  [   96/  265]
train() client id: f_00001-10-3 loss: 0.425951  [  128/  265]
train() client id: f_00001-10-4 loss: 0.296692  [  160/  265]
train() client id: f_00001-10-5 loss: 0.270781  [  192/  265]
train() client id: f_00001-10-6 loss: 0.370576  [  224/  265]
train() client id: f_00001-10-7 loss: 0.395111  [  256/  265]
train() client id: f_00001-11-0 loss: 0.345900  [   32/  265]
train() client id: f_00001-11-1 loss: 0.417279  [   64/  265]
train() client id: f_00001-11-2 loss: 0.287359  [   96/  265]
train() client id: f_00001-11-3 loss: 0.405010  [  128/  265]
train() client id: f_00001-11-4 loss: 0.563246  [  160/  265]
train() client id: f_00001-11-5 loss: 0.387358  [  192/  265]
train() client id: f_00001-11-6 loss: 0.275688  [  224/  265]
train() client id: f_00001-11-7 loss: 0.364047  [  256/  265]
train() client id: f_00002-0-0 loss: 1.179560  [   32/  124]
train() client id: f_00002-0-1 loss: 1.132706  [   64/  124]
train() client id: f_00002-0-2 loss: 1.277005  [   96/  124]
train() client id: f_00002-1-0 loss: 1.094532  [   32/  124]
train() client id: f_00002-1-1 loss: 0.965062  [   64/  124]
train() client id: f_00002-1-2 loss: 1.177885  [   96/  124]
train() client id: f_00002-2-0 loss: 1.134348  [   32/  124]
train() client id: f_00002-2-1 loss: 1.047658  [   64/  124]
train() client id: f_00002-2-2 loss: 1.111102  [   96/  124]
train() client id: f_00002-3-0 loss: 0.958712  [   32/  124]
train() client id: f_00002-3-1 loss: 0.925998  [   64/  124]
train() client id: f_00002-3-2 loss: 1.162199  [   96/  124]
train() client id: f_00002-4-0 loss: 1.215997  [   32/  124]
train() client id: f_00002-4-1 loss: 1.035596  [   64/  124]
train() client id: f_00002-4-2 loss: 1.022307  [   96/  124]
train() client id: f_00002-5-0 loss: 1.034555  [   32/  124]
train() client id: f_00002-5-1 loss: 1.038584  [   64/  124]
train() client id: f_00002-5-2 loss: 1.055643  [   96/  124]
train() client id: f_00002-6-0 loss: 0.987095  [   32/  124]
train() client id: f_00002-6-1 loss: 0.935350  [   64/  124]
train() client id: f_00002-6-2 loss: 1.114196  [   96/  124]
train() client id: f_00002-7-0 loss: 1.174130  [   32/  124]
train() client id: f_00002-7-1 loss: 0.900752  [   64/  124]
train() client id: f_00002-7-2 loss: 0.952097  [   96/  124]
train() client id: f_00002-8-0 loss: 0.958719  [   32/  124]
train() client id: f_00002-8-1 loss: 1.070035  [   64/  124]
train() client id: f_00002-8-2 loss: 1.023525  [   96/  124]
train() client id: f_00002-9-0 loss: 1.207863  [   32/  124]
train() client id: f_00002-9-1 loss: 0.840246  [   64/  124]
train() client id: f_00002-9-2 loss: 1.000084  [   96/  124]
train() client id: f_00002-10-0 loss: 0.984890  [   32/  124]
train() client id: f_00002-10-1 loss: 1.089720  [   64/  124]
train() client id: f_00002-10-2 loss: 1.023879  [   96/  124]
train() client id: f_00002-11-0 loss: 0.957140  [   32/  124]
train() client id: f_00002-11-1 loss: 0.928964  [   64/  124]
train() client id: f_00002-11-2 loss: 1.002527  [   96/  124]
train() client id: f_00003-0-0 loss: 0.794672  [   32/   43]
train() client id: f_00003-1-0 loss: 0.655601  [   32/   43]
train() client id: f_00003-2-0 loss: 0.677785  [   32/   43]
train() client id: f_00003-3-0 loss: 0.583764  [   32/   43]
train() client id: f_00003-4-0 loss: 0.870996  [   32/   43]
train() client id: f_00003-5-0 loss: 0.560953  [   32/   43]
train() client id: f_00003-6-0 loss: 0.642112  [   32/   43]
train() client id: f_00003-7-0 loss: 0.648288  [   32/   43]
train() client id: f_00003-8-0 loss: 0.682224  [   32/   43]
train() client id: f_00003-9-0 loss: 0.713925  [   32/   43]
train() client id: f_00003-10-0 loss: 0.686332  [   32/   43]
train() client id: f_00003-11-0 loss: 0.665399  [   32/   43]
train() client id: f_00004-0-0 loss: 0.795706  [   32/  306]
train() client id: f_00004-0-1 loss: 0.850834  [   64/  306]
train() client id: f_00004-0-2 loss: 1.020214  [   96/  306]
train() client id: f_00004-0-3 loss: 0.833600  [  128/  306]
train() client id: f_00004-0-4 loss: 0.869488  [  160/  306]
train() client id: f_00004-0-5 loss: 1.011600  [  192/  306]
train() client id: f_00004-0-6 loss: 0.867300  [  224/  306]
train() client id: f_00004-0-7 loss: 0.944934  [  256/  306]
train() client id: f_00004-0-8 loss: 1.121781  [  288/  306]
train() client id: f_00004-1-0 loss: 0.850033  [   32/  306]
train() client id: f_00004-1-1 loss: 0.828936  [   64/  306]
train() client id: f_00004-1-2 loss: 0.899423  [   96/  306]
train() client id: f_00004-1-3 loss: 0.953054  [  128/  306]
train() client id: f_00004-1-4 loss: 1.043584  [  160/  306]
train() client id: f_00004-1-5 loss: 1.016393  [  192/  306]
train() client id: f_00004-1-6 loss: 0.883015  [  224/  306]
train() client id: f_00004-1-7 loss: 0.914203  [  256/  306]
train() client id: f_00004-1-8 loss: 0.901235  [  288/  306]
train() client id: f_00004-2-0 loss: 0.909497  [   32/  306]
train() client id: f_00004-2-1 loss: 0.839585  [   64/  306]
train() client id: f_00004-2-2 loss: 0.852078  [   96/  306]
train() client id: f_00004-2-3 loss: 0.997744  [  128/  306]
train() client id: f_00004-2-4 loss: 1.009445  [  160/  306]
train() client id: f_00004-2-5 loss: 0.936413  [  192/  306]
train() client id: f_00004-2-6 loss: 0.957453  [  224/  306]
train() client id: f_00004-2-7 loss: 0.927697  [  256/  306]
train() client id: f_00004-2-8 loss: 0.936427  [  288/  306]
train() client id: f_00004-3-0 loss: 0.909386  [   32/  306]
train() client id: f_00004-3-1 loss: 0.743513  [   64/  306]
train() client id: f_00004-3-2 loss: 0.991603  [   96/  306]
train() client id: f_00004-3-3 loss: 0.960503  [  128/  306]
train() client id: f_00004-3-4 loss: 0.933835  [  160/  306]
train() client id: f_00004-3-5 loss: 0.904928  [  192/  306]
train() client id: f_00004-3-6 loss: 1.017121  [  224/  306]
train() client id: f_00004-3-7 loss: 0.922306  [  256/  306]
train() client id: f_00004-3-8 loss: 1.006766  [  288/  306]
train() client id: f_00004-4-0 loss: 0.732744  [   32/  306]
train() client id: f_00004-4-1 loss: 0.846980  [   64/  306]
train() client id: f_00004-4-2 loss: 0.949306  [   96/  306]
train() client id: f_00004-4-3 loss: 0.874372  [  128/  306]
train() client id: f_00004-4-4 loss: 0.979794  [  160/  306]
train() client id: f_00004-4-5 loss: 0.920991  [  192/  306]
train() client id: f_00004-4-6 loss: 0.969811  [  224/  306]
train() client id: f_00004-4-7 loss: 0.956402  [  256/  306]
train() client id: f_00004-4-8 loss: 1.009115  [  288/  306]
train() client id: f_00004-5-0 loss: 0.803796  [   32/  306]
train() client id: f_00004-5-1 loss: 0.865242  [   64/  306]
train() client id: f_00004-5-2 loss: 0.984587  [   96/  306]
train() client id: f_00004-5-3 loss: 0.765822  [  128/  306]
train() client id: f_00004-5-4 loss: 1.056709  [  160/  306]
train() client id: f_00004-5-5 loss: 0.889456  [  192/  306]
train() client id: f_00004-5-6 loss: 0.924138  [  224/  306]
train() client id: f_00004-5-7 loss: 0.945917  [  256/  306]
train() client id: f_00004-5-8 loss: 1.018557  [  288/  306]
train() client id: f_00004-6-0 loss: 0.824253  [   32/  306]
train() client id: f_00004-6-1 loss: 0.858376  [   64/  306]
train() client id: f_00004-6-2 loss: 0.899285  [   96/  306]
train() client id: f_00004-6-3 loss: 0.754508  [  128/  306]
train() client id: f_00004-6-4 loss: 0.894702  [  160/  306]
train() client id: f_00004-6-5 loss: 1.071847  [  192/  306]
train() client id: f_00004-6-6 loss: 0.950445  [  224/  306]
train() client id: f_00004-6-7 loss: 0.893925  [  256/  306]
train() client id: f_00004-6-8 loss: 1.070438  [  288/  306]
train() client id: f_00004-7-0 loss: 0.921081  [   32/  306]
train() client id: f_00004-7-1 loss: 1.012631  [   64/  306]
train() client id: f_00004-7-2 loss: 0.933754  [   96/  306]
train() client id: f_00004-7-3 loss: 0.792185  [  128/  306]
train() client id: f_00004-7-4 loss: 0.973719  [  160/  306]
train() client id: f_00004-7-5 loss: 0.985830  [  192/  306]
train() client id: f_00004-7-6 loss: 1.018624  [  224/  306]
train() client id: f_00004-7-7 loss: 0.871281  [  256/  306]
train() client id: f_00004-7-8 loss: 0.799790  [  288/  306]
train() client id: f_00004-8-0 loss: 0.884158  [   32/  306]
train() client id: f_00004-8-1 loss: 0.906458  [   64/  306]
train() client id: f_00004-8-2 loss: 0.788454  [   96/  306]
train() client id: f_00004-8-3 loss: 0.942873  [  128/  306]
train() client id: f_00004-8-4 loss: 1.008198  [  160/  306]
train() client id: f_00004-8-5 loss: 0.992598  [  192/  306]
train() client id: f_00004-8-6 loss: 0.890231  [  224/  306]
train() client id: f_00004-8-7 loss: 0.925418  [  256/  306]
train() client id: f_00004-8-8 loss: 0.855616  [  288/  306]
train() client id: f_00004-9-0 loss: 0.846577  [   32/  306]
train() client id: f_00004-9-1 loss: 0.963153  [   64/  306]
train() client id: f_00004-9-2 loss: 0.875977  [   96/  306]
train() client id: f_00004-9-3 loss: 1.006275  [  128/  306]
train() client id: f_00004-9-4 loss: 0.928933  [  160/  306]
train() client id: f_00004-9-5 loss: 0.942158  [  192/  306]
train() client id: f_00004-9-6 loss: 0.897671  [  224/  306]
train() client id: f_00004-9-7 loss: 0.838750  [  256/  306]
train() client id: f_00004-9-8 loss: 0.940495  [  288/  306]
train() client id: f_00004-10-0 loss: 1.042572  [   32/  306]
train() client id: f_00004-10-1 loss: 0.927864  [   64/  306]
train() client id: f_00004-10-2 loss: 0.796914  [   96/  306]
train() client id: f_00004-10-3 loss: 1.016110  [  128/  306]
train() client id: f_00004-10-4 loss: 0.859509  [  160/  306]
train() client id: f_00004-10-5 loss: 0.841574  [  192/  306]
train() client id: f_00004-10-6 loss: 0.930923  [  224/  306]
train() client id: f_00004-10-7 loss: 0.903300  [  256/  306]
train() client id: f_00004-10-8 loss: 0.855315  [  288/  306]
train() client id: f_00004-11-0 loss: 1.079986  [   32/  306]
train() client id: f_00004-11-1 loss: 0.946720  [   64/  306]
train() client id: f_00004-11-2 loss: 0.805378  [   96/  306]
train() client id: f_00004-11-3 loss: 0.893949  [  128/  306]
train() client id: f_00004-11-4 loss: 0.912908  [  160/  306]
train() client id: f_00004-11-5 loss: 0.926815  [  192/  306]
train() client id: f_00004-11-6 loss: 0.837161  [  224/  306]
train() client id: f_00004-11-7 loss: 0.962419  [  256/  306]
train() client id: f_00004-11-8 loss: 0.800705  [  288/  306]
train() client id: f_00005-0-0 loss: 0.619402  [   32/  146]
train() client id: f_00005-0-1 loss: 0.660826  [   64/  146]
train() client id: f_00005-0-2 loss: 0.826915  [   96/  146]
train() client id: f_00005-0-3 loss: 0.577085  [  128/  146]
train() client id: f_00005-1-0 loss: 0.923015  [   32/  146]
train() client id: f_00005-1-1 loss: 0.658402  [   64/  146]
train() client id: f_00005-1-2 loss: 0.501384  [   96/  146]
train() client id: f_00005-1-3 loss: 0.483881  [  128/  146]
train() client id: f_00005-2-0 loss: 0.742197  [   32/  146]
train() client id: f_00005-2-1 loss: 0.693096  [   64/  146]
train() client id: f_00005-2-2 loss: 0.623542  [   96/  146]
train() client id: f_00005-2-3 loss: 0.645041  [  128/  146]
train() client id: f_00005-3-0 loss: 0.481055  [   32/  146]
train() client id: f_00005-3-1 loss: 0.712734  [   64/  146]
train() client id: f_00005-3-2 loss: 0.723020  [   96/  146]
train() client id: f_00005-3-3 loss: 0.582371  [  128/  146]
train() client id: f_00005-4-0 loss: 0.667213  [   32/  146]
train() client id: f_00005-4-1 loss: 0.635436  [   64/  146]
train() client id: f_00005-4-2 loss: 0.562996  [   96/  146]
train() client id: f_00005-4-3 loss: 0.742797  [  128/  146]
train() client id: f_00005-5-0 loss: 0.536657  [   32/  146]
train() client id: f_00005-5-1 loss: 0.942012  [   64/  146]
train() client id: f_00005-5-2 loss: 0.496185  [   96/  146]
train() client id: f_00005-5-3 loss: 0.470410  [  128/  146]
train() client id: f_00005-6-0 loss: 0.718897  [   32/  146]
train() client id: f_00005-6-1 loss: 0.509319  [   64/  146]
train() client id: f_00005-6-2 loss: 0.572733  [   96/  146]
train() client id: f_00005-6-3 loss: 0.680493  [  128/  146]
train() client id: f_00005-7-0 loss: 0.612434  [   32/  146]
train() client id: f_00005-7-1 loss: 0.632946  [   64/  146]
train() client id: f_00005-7-2 loss: 0.815521  [   96/  146]
train() client id: f_00005-7-3 loss: 0.543998  [  128/  146]
train() client id: f_00005-8-0 loss: 0.601411  [   32/  146]
train() client id: f_00005-8-1 loss: 0.581120  [   64/  146]
train() client id: f_00005-8-2 loss: 0.417259  [   96/  146]
train() client id: f_00005-8-3 loss: 0.909005  [  128/  146]
train() client id: f_00005-9-0 loss: 0.783473  [   32/  146]
train() client id: f_00005-9-1 loss: 0.731281  [   64/  146]
train() client id: f_00005-9-2 loss: 0.788659  [   96/  146]
train() client id: f_00005-9-3 loss: 0.404156  [  128/  146]
train() client id: f_00005-10-0 loss: 0.577927  [   32/  146]
train() client id: f_00005-10-1 loss: 0.792596  [   64/  146]
train() client id: f_00005-10-2 loss: 0.730696  [   96/  146]
train() client id: f_00005-10-3 loss: 0.285562  [  128/  146]
train() client id: f_00005-11-0 loss: 0.490234  [   32/  146]
train() client id: f_00005-11-1 loss: 0.717129  [   64/  146]
train() client id: f_00005-11-2 loss: 0.808914  [   96/  146]
train() client id: f_00005-11-3 loss: 0.364666  [  128/  146]
train() client id: f_00006-0-0 loss: 0.593032  [   32/   54]
train() client id: f_00006-1-0 loss: 0.535076  [   32/   54]
train() client id: f_00006-2-0 loss: 0.508409  [   32/   54]
train() client id: f_00006-3-0 loss: 0.584812  [   32/   54]
train() client id: f_00006-4-0 loss: 0.489907  [   32/   54]
train() client id: f_00006-5-0 loss: 0.580612  [   32/   54]
train() client id: f_00006-6-0 loss: 0.538774  [   32/   54]
train() client id: f_00006-7-0 loss: 0.534796  [   32/   54]
train() client id: f_00006-8-0 loss: 0.536259  [   32/   54]
train() client id: f_00006-9-0 loss: 0.598953  [   32/   54]
train() client id: f_00006-10-0 loss: 0.554295  [   32/   54]
train() client id: f_00006-11-0 loss: 0.542201  [   32/   54]
train() client id: f_00007-0-0 loss: 0.611859  [   32/  179]
train() client id: f_00007-0-1 loss: 0.568062  [   64/  179]
train() client id: f_00007-0-2 loss: 0.791674  [   96/  179]
train() client id: f_00007-0-3 loss: 0.613809  [  128/  179]
train() client id: f_00007-0-4 loss: 0.740757  [  160/  179]
train() client id: f_00007-1-0 loss: 0.573822  [   32/  179]
train() client id: f_00007-1-1 loss: 0.606177  [   64/  179]
train() client id: f_00007-1-2 loss: 0.831088  [   96/  179]
train() client id: f_00007-1-3 loss: 0.609023  [  128/  179]
train() client id: f_00007-1-4 loss: 0.651860  [  160/  179]
train() client id: f_00007-2-0 loss: 0.544106  [   32/  179]
train() client id: f_00007-2-1 loss: 0.752469  [   64/  179]
train() client id: f_00007-2-2 loss: 0.715312  [   96/  179]
train() client id: f_00007-2-3 loss: 0.678999  [  128/  179]
train() client id: f_00007-2-4 loss: 0.486031  [  160/  179]
train() client id: f_00007-3-0 loss: 0.502965  [   32/  179]
train() client id: f_00007-3-1 loss: 0.583506  [   64/  179]
train() client id: f_00007-3-2 loss: 0.486918  [   96/  179]
train() client id: f_00007-3-3 loss: 0.809210  [  128/  179]
train() client id: f_00007-3-4 loss: 0.570607  [  160/  179]
train() client id: f_00007-4-0 loss: 0.647846  [   32/  179]
train() client id: f_00007-4-1 loss: 0.721980  [   64/  179]
train() client id: f_00007-4-2 loss: 0.500055  [   96/  179]
train() client id: f_00007-4-3 loss: 0.538295  [  128/  179]
train() client id: f_00007-4-4 loss: 0.660574  [  160/  179]
train() client id: f_00007-5-0 loss: 0.557366  [   32/  179]
train() client id: f_00007-5-1 loss: 0.585243  [   64/  179]
train() client id: f_00007-5-2 loss: 0.627334  [   96/  179]
train() client id: f_00007-5-3 loss: 0.633689  [  128/  179]
train() client id: f_00007-5-4 loss: 0.476905  [  160/  179]
train() client id: f_00007-6-0 loss: 0.590148  [   32/  179]
train() client id: f_00007-6-1 loss: 0.634828  [   64/  179]
train() client id: f_00007-6-2 loss: 0.578898  [   96/  179]
train() client id: f_00007-6-3 loss: 0.744322  [  128/  179]
train() client id: f_00007-6-4 loss: 0.524437  [  160/  179]
train() client id: f_00007-7-0 loss: 0.552123  [   32/  179]
train() client id: f_00007-7-1 loss: 0.649965  [   64/  179]
train() client id: f_00007-7-2 loss: 0.618519  [   96/  179]
train() client id: f_00007-7-3 loss: 0.457955  [  128/  179]
train() client id: f_00007-7-4 loss: 0.784019  [  160/  179]
train() client id: f_00007-8-0 loss: 0.529866  [   32/  179]
train() client id: f_00007-8-1 loss: 0.695066  [   64/  179]
train() client id: f_00007-8-2 loss: 0.481363  [   96/  179]
train() client id: f_00007-8-3 loss: 0.638243  [  128/  179]
train() client id: f_00007-8-4 loss: 0.736801  [  160/  179]
train() client id: f_00007-9-0 loss: 0.621157  [   32/  179]
train() client id: f_00007-9-1 loss: 0.638434  [   64/  179]
train() client id: f_00007-9-2 loss: 0.773309  [   96/  179]
train() client id: f_00007-9-3 loss: 0.418078  [  128/  179]
train() client id: f_00007-9-4 loss: 0.642260  [  160/  179]
train() client id: f_00007-10-0 loss: 0.684887  [   32/  179]
train() client id: f_00007-10-1 loss: 0.675408  [   64/  179]
train() client id: f_00007-10-2 loss: 0.639680  [   96/  179]
train() client id: f_00007-10-3 loss: 0.496721  [  128/  179]
train() client id: f_00007-10-4 loss: 0.580256  [  160/  179]
train() client id: f_00007-11-0 loss: 0.467140  [   32/  179]
train() client id: f_00007-11-1 loss: 0.833300  [   64/  179]
train() client id: f_00007-11-2 loss: 0.745511  [   96/  179]
train() client id: f_00007-11-3 loss: 0.552553  [  128/  179]
train() client id: f_00007-11-4 loss: 0.497009  [  160/  179]
train() client id: f_00008-0-0 loss: 0.735803  [   32/  130]
train() client id: f_00008-0-1 loss: 0.869989  [   64/  130]
train() client id: f_00008-0-2 loss: 0.639977  [   96/  130]
train() client id: f_00008-0-3 loss: 0.627611  [  128/  130]
train() client id: f_00008-1-0 loss: 0.653179  [   32/  130]
train() client id: f_00008-1-1 loss: 0.763364  [   64/  130]
train() client id: f_00008-1-2 loss: 0.717694  [   96/  130]
train() client id: f_00008-1-3 loss: 0.722520  [  128/  130]
train() client id: f_00008-2-0 loss: 0.668950  [   32/  130]
train() client id: f_00008-2-1 loss: 0.735663  [   64/  130]
train() client id: f_00008-2-2 loss: 0.672219  [   96/  130]
train() client id: f_00008-2-3 loss: 0.766963  [  128/  130]
train() client id: f_00008-3-0 loss: 0.721072  [   32/  130]
train() client id: f_00008-3-1 loss: 0.776425  [   64/  130]
train() client id: f_00008-3-2 loss: 0.666935  [   96/  130]
train() client id: f_00008-3-3 loss: 0.678027  [  128/  130]
train() client id: f_00008-4-0 loss: 0.626733  [   32/  130]
train() client id: f_00008-4-1 loss: 0.730628  [   64/  130]
train() client id: f_00008-4-2 loss: 0.719757  [   96/  130]
train() client id: f_00008-4-3 loss: 0.780764  [  128/  130]
train() client id: f_00008-5-0 loss: 0.746865  [   32/  130]
train() client id: f_00008-5-1 loss: 0.642725  [   64/  130]
train() client id: f_00008-5-2 loss: 0.722064  [   96/  130]
train() client id: f_00008-5-3 loss: 0.756366  [  128/  130]
train() client id: f_00008-6-0 loss: 0.704856  [   32/  130]
train() client id: f_00008-6-1 loss: 0.700615  [   64/  130]
train() client id: f_00008-6-2 loss: 0.771113  [   96/  130]
train() client id: f_00008-6-3 loss: 0.688399  [  128/  130]
train() client id: f_00008-7-0 loss: 0.632873  [   32/  130]
train() client id: f_00008-7-1 loss: 0.768072  [   64/  130]
train() client id: f_00008-7-2 loss: 0.692535  [   96/  130]
train() client id: f_00008-7-3 loss: 0.775611  [  128/  130]
train() client id: f_00008-8-0 loss: 0.843779  [   32/  130]
train() client id: f_00008-8-1 loss: 0.663064  [   64/  130]
train() client id: f_00008-8-2 loss: 0.615663  [   96/  130]
train() client id: f_00008-8-3 loss: 0.747836  [  128/  130]
train() client id: f_00008-9-0 loss: 0.727056  [   32/  130]
train() client id: f_00008-9-1 loss: 0.687360  [   64/  130]
train() client id: f_00008-9-2 loss: 0.711428  [   96/  130]
train() client id: f_00008-9-3 loss: 0.748692  [  128/  130]
train() client id: f_00008-10-0 loss: 0.742429  [   32/  130]
train() client id: f_00008-10-1 loss: 0.701063  [   64/  130]
train() client id: f_00008-10-2 loss: 0.737942  [   96/  130]
train() client id: f_00008-10-3 loss: 0.681395  [  128/  130]
train() client id: f_00008-11-0 loss: 0.722855  [   32/  130]
train() client id: f_00008-11-1 loss: 0.683359  [   64/  130]
train() client id: f_00008-11-2 loss: 0.731470  [   96/  130]
train() client id: f_00008-11-3 loss: 0.642148  [  128/  130]
train() client id: f_00009-0-0 loss: 1.252973  [   32/  118]
train() client id: f_00009-0-1 loss: 1.015307  [   64/  118]
train() client id: f_00009-0-2 loss: 1.126102  [   96/  118]
train() client id: f_00009-1-0 loss: 1.033023  [   32/  118]
train() client id: f_00009-1-1 loss: 1.047347  [   64/  118]
train() client id: f_00009-1-2 loss: 1.154646  [   96/  118]
train() client id: f_00009-2-0 loss: 1.096991  [   32/  118]
train() client id: f_00009-2-1 loss: 1.203449  [   64/  118]
train() client id: f_00009-2-2 loss: 0.937330  [   96/  118]
train() client id: f_00009-3-0 loss: 1.003240  [   32/  118]
train() client id: f_00009-3-1 loss: 1.180479  [   64/  118]
train() client id: f_00009-3-2 loss: 0.929945  [   96/  118]
train() client id: f_00009-4-0 loss: 1.011294  [   32/  118]
train() client id: f_00009-4-1 loss: 0.900200  [   64/  118]
train() client id: f_00009-4-2 loss: 0.970110  [   96/  118]
train() client id: f_00009-5-0 loss: 0.885501  [   32/  118]
train() client id: f_00009-5-1 loss: 1.115944  [   64/  118]
train() client id: f_00009-5-2 loss: 0.899449  [   96/  118]
train() client id: f_00009-6-0 loss: 0.934407  [   32/  118]
train() client id: f_00009-6-1 loss: 0.796789  [   64/  118]
train() client id: f_00009-6-2 loss: 0.997477  [   96/  118]
train() client id: f_00009-7-0 loss: 0.913059  [   32/  118]
train() client id: f_00009-7-1 loss: 0.928082  [   64/  118]
train() client id: f_00009-7-2 loss: 0.795356  [   96/  118]
train() client id: f_00009-8-0 loss: 0.853176  [   32/  118]
train() client id: f_00009-8-1 loss: 0.912319  [   64/  118]
train() client id: f_00009-8-2 loss: 0.873964  [   96/  118]
train() client id: f_00009-9-0 loss: 0.720615  [   32/  118]
train() client id: f_00009-9-1 loss: 0.810705  [   64/  118]
train() client id: f_00009-9-2 loss: 0.931886  [   96/  118]
train() client id: f_00009-10-0 loss: 0.834938  [   32/  118]
train() client id: f_00009-10-1 loss: 0.761517  [   64/  118]
train() client id: f_00009-10-2 loss: 0.871306  [   96/  118]
train() client id: f_00009-11-0 loss: 0.739734  [   32/  118]
train() client id: f_00009-11-1 loss: 0.957833  [   64/  118]
train() client id: f_00009-11-2 loss: 0.873944  [   96/  118]
At round 30 accuracy: 0.6525198938992043
At round 30 training accuracy: 0.5841716968477532
At round 30 training loss: 0.8291868868912929
update_location
xs = [  -3.9056584     4.20031788  170.00902392   18.81129433    0.97929623
    3.95640986 -132.44319194 -111.32485185  154.66397685  -97.06087855]
ys = [ 162.5879595   145.55583871    1.32061395 -132.45517586  124.35018685
  107.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [190.91908951 176.64694974 197.24302835 167.02765761 159.57420841
 147.10385542 165.97617188 149.64591172 185.01247447 139.41601774]
dists_bs = [171.5439666  181.60317332 386.25866427 363.4439475  183.12322702
 191.15264835 182.65190108 185.45182381 365.32568415 187.92285621]
uav_gains = [1.94386836e-11 2.39101861e-11 1.77469892e-11 2.76188258e-11
 3.10151017e-11 3.80704871e-11 2.80673490e-11 3.64667227e-11
 2.11653608e-11 4.35559512e-11]
bs_gains = [6.12410786e-11 5.22093724e-11 6.31008238e-12 7.48288810e-12
 5.10049673e-11 4.52302711e-11 5.13743493e-11 4.92319435e-11
 7.37546672e-12 4.74407113e-11]
Round 31
-------------------------------
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.90000386 14.30569929  6.7966894   2.4461517  16.50004725  7.94229532
  3.0341758   9.71414062  7.16398278  6.44283898]
obj_prev = 81.24602500345188
eta_min = 3.803699202355281e-14	eta_max = 0.9265448863059464
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 18.86522301759883	eta = 0.9090909090909091
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 33.97406433952712	eta = 0.5048027981544223
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 26.610863860443008	eta = 0.6444812477044536
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 25.281958371636836	eta = 0.6783573681741374
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 25.213171638427752	eta = 0.6802080670062467
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 25.212972778010624	eta = 0.680213431961071
eta = 0.680213431961071
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [0.03179805 0.06687685 0.03129332 0.01085171 0.07722383 0.03684536
 0.01362773 0.04517342 0.03280751 0.02977912]
ene_total = [2.23637289 4.06508358 2.2200981  1.04612844 4.63524192 2.42743785
 1.19692013 2.90387674 2.44646747 2.03534566]
ti_comp = [0.42879245 0.44662227 0.42670833 0.43604005 0.44628188 0.44447784
 0.43634521 0.44101196 0.40063459 0.44520477]
ti_coms = [0.08920181 0.07137199 0.09128593 0.08195421 0.07171238 0.07351642
 0.08164905 0.0769823  0.11735967 0.07278949]
t_total = [28.44986992 28.44986992 28.44986992 28.44986992 28.44986992 28.44986992
 28.44986992 28.44986992 28.44986992 28.44986992]
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [1.09291688e-05 9.37187797e-05 1.05189542e-05 4.20070663e-07
 1.44515864e-04 1.58244307e-05 8.30788735e-07 2.96229369e-05
 1.37500070e-05 8.32714570e-06]
ene_total = [0.49052461 0.39714469 0.5019487  0.45014129 0.40180415 0.40464437
 0.44848781 0.42443794 0.64533131 0.40024009]
optimize_network iter = 0 obj = 4.564704952114099
eta = 0.680213431961071
freqs = [37078600.58661079 74869583.15751041 36668274.98538263 12443483.84142069
 86519118.7997421  41447912.13753933 15615771.9937943  51215637.28090309
 40944429.42168844 33444300.55685099]
eta_min = 0.6802134319610765	eta_max = 0.6802134319610728
af = 0.014970687331339992	bf = 1.4539623964515263	zeta = 0.016467756064473992	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [2.70316510e-06 2.31799269e-05 2.60170470e-06 1.03898144e-07
 3.57438195e-05 3.91393429e-06 2.05483065e-07 7.32678673e-06
 3.40085688e-06 2.05959391e-06]
ene_total = [1.72390573 1.38338877 1.76415135 1.58337944 1.39239245 1.42109708
 1.57750334 1.48871744 2.26805167 1.40669455]
ti_comp = [0.42879245 0.44662227 0.42670833 0.43604005 0.44628188 0.44447784
 0.43634521 0.44101196 0.40063459 0.44520477]
ti_coms = [0.08920181 0.07137199 0.09128593 0.08195421 0.07171238 0.07351642
 0.08164905 0.0769823  0.11735967 0.07278949]
t_total = [28.44986992 28.44986992 28.44986992 28.44986992 28.44986992 28.44986992
 28.44986992 28.44986992 28.44986992 28.44986992]
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [1.09291688e-05 9.37187797e-05 1.05189542e-05 4.20070663e-07
 1.44515864e-04 1.58244307e-05 8.30788735e-07 2.96229369e-05
 1.37500070e-05 8.32714570e-06]
ene_total = [0.49052461 0.39714469 0.5019487  0.45014129 0.40180415 0.40464437
 0.44848781 0.42443794 0.64533131 0.40024009]
optimize_network iter = 1 obj = 4.564704952114124
eta = 0.6802134319610728
freqs = [37078600.58661079 74869583.15751038 36668274.98538262 12443483.84142068
 86519118.79974204 41447912.1375393  15615771.9937943  51215637.28090306
 40944429.42168847 33444300.55685098]
Done!
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [1.03936590e-05 8.91267263e-05 1.00035442e-05 3.99487948e-07
 1.37434843e-04 1.50490617e-05 7.90081565e-07 2.81714657e-05
 1.30762811e-05 7.91913038e-06]
ene_total = [0.00893058 0.00722633 0.0091386  0.00819582 0.00730867 0.00736669
 0.00816569 0.0077264  0.01174904 0.00728687]
At round 31 energy consumption: 0.08309468916898873
At round 31 eta: 0.6802134319610728
At round 31 a_n: 17.221135753236222
At round 31 local rounds: 12.618272918924527
At round 31 global rounds: 54.923137353497395
gradient difference: 0.43698960542678833
train() client id: f_00000-0-0 loss: 0.865209  [   32/  126]
train() client id: f_00000-0-1 loss: 1.345089  [   64/  126]
train() client id: f_00000-0-2 loss: 1.327331  [   96/  126]
train() client id: f_00000-1-0 loss: 1.255973  [   32/  126]
train() client id: f_00000-1-1 loss: 1.023963  [   64/  126]
train() client id: f_00000-1-2 loss: 1.117057  [   96/  126]
train() client id: f_00000-2-0 loss: 1.053773  [   32/  126]
train() client id: f_00000-2-1 loss: 0.914666  [   64/  126]
train() client id: f_00000-2-2 loss: 1.192134  [   96/  126]
train() client id: f_00000-3-0 loss: 0.890844  [   32/  126]
train() client id: f_00000-3-1 loss: 1.063592  [   64/  126]
train() client id: f_00000-3-2 loss: 0.922374  [   96/  126]
train() client id: f_00000-4-0 loss: 1.001128  [   32/  126]
train() client id: f_00000-4-1 loss: 0.904797  [   64/  126]
train() client id: f_00000-4-2 loss: 0.879798  [   96/  126]
train() client id: f_00000-5-0 loss: 0.961606  [   32/  126]
train() client id: f_00000-5-1 loss: 0.859860  [   64/  126]
train() client id: f_00000-5-2 loss: 0.950865  [   96/  126]
train() client id: f_00000-6-0 loss: 0.859961  [   32/  126]
train() client id: f_00000-6-1 loss: 0.917180  [   64/  126]
train() client id: f_00000-6-2 loss: 0.909084  [   96/  126]
train() client id: f_00000-7-0 loss: 0.780679  [   32/  126]
train() client id: f_00000-7-1 loss: 0.903502  [   64/  126]
train() client id: f_00000-7-2 loss: 0.756667  [   96/  126]
train() client id: f_00000-8-0 loss: 0.839595  [   32/  126]
train() client id: f_00000-8-1 loss: 0.890321  [   64/  126]
train() client id: f_00000-8-2 loss: 0.839591  [   96/  126]
train() client id: f_00000-9-0 loss: 0.749486  [   32/  126]
train() client id: f_00000-9-1 loss: 0.773332  [   64/  126]
train() client id: f_00000-9-2 loss: 0.851448  [   96/  126]
train() client id: f_00000-10-0 loss: 0.844698  [   32/  126]
train() client id: f_00000-10-1 loss: 0.704652  [   64/  126]
train() client id: f_00000-10-2 loss: 1.007694  [   96/  126]
train() client id: f_00000-11-0 loss: 0.743310  [   32/  126]
train() client id: f_00000-11-1 loss: 0.847317  [   64/  126]
train() client id: f_00000-11-2 loss: 0.863096  [   96/  126]
train() client id: f_00001-0-0 loss: 0.478463  [   32/  265]
train() client id: f_00001-0-1 loss: 0.390646  [   64/  265]
train() client id: f_00001-0-2 loss: 0.556661  [   96/  265]
train() client id: f_00001-0-3 loss: 0.425043  [  128/  265]
train() client id: f_00001-0-4 loss: 0.454119  [  160/  265]
train() client id: f_00001-0-5 loss: 0.533377  [  192/  265]
train() client id: f_00001-0-6 loss: 0.393988  [  224/  265]
train() client id: f_00001-0-7 loss: 0.333751  [  256/  265]
train() client id: f_00001-1-0 loss: 0.424954  [   32/  265]
train() client id: f_00001-1-1 loss: 0.422603  [   64/  265]
train() client id: f_00001-1-2 loss: 0.479475  [   96/  265]
train() client id: f_00001-1-3 loss: 0.417120  [  128/  265]
train() client id: f_00001-1-4 loss: 0.376597  [  160/  265]
train() client id: f_00001-1-5 loss: 0.501928  [  192/  265]
train() client id: f_00001-1-6 loss: 0.510470  [  224/  265]
train() client id: f_00001-1-7 loss: 0.353482  [  256/  265]
train() client id: f_00001-2-0 loss: 0.477604  [   32/  265]
train() client id: f_00001-2-1 loss: 0.442445  [   64/  265]
train() client id: f_00001-2-2 loss: 0.449623  [   96/  265]
train() client id: f_00001-2-3 loss: 0.391529  [  128/  265]
train() client id: f_00001-2-4 loss: 0.390209  [  160/  265]
train() client id: f_00001-2-5 loss: 0.379677  [  192/  265]
train() client id: f_00001-2-6 loss: 0.394845  [  224/  265]
train() client id: f_00001-2-7 loss: 0.522326  [  256/  265]
train() client id: f_00001-3-0 loss: 0.464839  [   32/  265]
train() client id: f_00001-3-1 loss: 0.368698  [   64/  265]
train() client id: f_00001-3-2 loss: 0.455932  [   96/  265]
train() client id: f_00001-3-3 loss: 0.468354  [  128/  265]
train() client id: f_00001-3-4 loss: 0.473452  [  160/  265]
train() client id: f_00001-3-5 loss: 0.469087  [  192/  265]
train() client id: f_00001-3-6 loss: 0.329208  [  224/  265]
train() client id: f_00001-3-7 loss: 0.355914  [  256/  265]
train() client id: f_00001-4-0 loss: 0.419770  [   32/  265]
train() client id: f_00001-4-1 loss: 0.429793  [   64/  265]
train() client id: f_00001-4-2 loss: 0.353054  [   96/  265]
train() client id: f_00001-4-3 loss: 0.504261  [  128/  265]
train() client id: f_00001-4-4 loss: 0.422518  [  160/  265]
train() client id: f_00001-4-5 loss: 0.464767  [  192/  265]
train() client id: f_00001-4-6 loss: 0.373023  [  224/  265]
train() client id: f_00001-4-7 loss: 0.347015  [  256/  265]
train() client id: f_00001-5-0 loss: 0.431741  [   32/  265]
train() client id: f_00001-5-1 loss: 0.324416  [   64/  265]
train() client id: f_00001-5-2 loss: 0.399003  [   96/  265]
train() client id: f_00001-5-3 loss: 0.393149  [  128/  265]
train() client id: f_00001-5-4 loss: 0.463915  [  160/  265]
train() client id: f_00001-5-5 loss: 0.481295  [  192/  265]
train() client id: f_00001-5-6 loss: 0.374107  [  224/  265]
train() client id: f_00001-5-7 loss: 0.477727  [  256/  265]
train() client id: f_00001-6-0 loss: 0.427850  [   32/  265]
train() client id: f_00001-6-1 loss: 0.409028  [   64/  265]
train() client id: f_00001-6-2 loss: 0.470711  [   96/  265]
train() client id: f_00001-6-3 loss: 0.448863  [  128/  265]
train() client id: f_00001-6-4 loss: 0.397688  [  160/  265]
train() client id: f_00001-6-5 loss: 0.457755  [  192/  265]
train() client id: f_00001-6-6 loss: 0.329078  [  224/  265]
train() client id: f_00001-6-7 loss: 0.388310  [  256/  265]
train() client id: f_00001-7-0 loss: 0.400543  [   32/  265]
train() client id: f_00001-7-1 loss: 0.325950  [   64/  265]
train() client id: f_00001-7-2 loss: 0.429207  [   96/  265]
train() client id: f_00001-7-3 loss: 0.454998  [  128/  265]
train() client id: f_00001-7-4 loss: 0.541742  [  160/  265]
train() client id: f_00001-7-5 loss: 0.456223  [  192/  265]
train() client id: f_00001-7-6 loss: 0.319878  [  224/  265]
train() client id: f_00001-7-7 loss: 0.297762  [  256/  265]
train() client id: f_00001-8-0 loss: 0.392371  [   32/  265]
train() client id: f_00001-8-1 loss: 0.410795  [   64/  265]
train() client id: f_00001-8-2 loss: 0.359228  [   96/  265]
train() client id: f_00001-8-3 loss: 0.411282  [  128/  265]
train() client id: f_00001-8-4 loss: 0.550134  [  160/  265]
train() client id: f_00001-8-5 loss: 0.381211  [  192/  265]
train() client id: f_00001-8-6 loss: 0.370382  [  224/  265]
train() client id: f_00001-8-7 loss: 0.348036  [  256/  265]
train() client id: f_00001-9-0 loss: 0.328708  [   32/  265]
train() client id: f_00001-9-1 loss: 0.486420  [   64/  265]
train() client id: f_00001-9-2 loss: 0.408053  [   96/  265]
train() client id: f_00001-9-3 loss: 0.462096  [  128/  265]
train() client id: f_00001-9-4 loss: 0.359009  [  160/  265]
train() client id: f_00001-9-5 loss: 0.407615  [  192/  265]
train() client id: f_00001-9-6 loss: 0.428141  [  224/  265]
train() client id: f_00001-9-7 loss: 0.407446  [  256/  265]
train() client id: f_00001-10-0 loss: 0.380524  [   32/  265]
train() client id: f_00001-10-1 loss: 0.362637  [   64/  265]
train() client id: f_00001-10-2 loss: 0.382543  [   96/  265]
train() client id: f_00001-10-3 loss: 0.359339  [  128/  265]
train() client id: f_00001-10-4 loss: 0.298676  [  160/  265]
train() client id: f_00001-10-5 loss: 0.396675  [  192/  265]
train() client id: f_00001-10-6 loss: 0.444905  [  224/  265]
train() client id: f_00001-10-7 loss: 0.502153  [  256/  265]
train() client id: f_00001-11-0 loss: 0.381936  [   32/  265]
train() client id: f_00001-11-1 loss: 0.425012  [   64/  265]
train() client id: f_00001-11-2 loss: 0.403140  [   96/  265]
train() client id: f_00001-11-3 loss: 0.440431  [  128/  265]
train() client id: f_00001-11-4 loss: 0.322410  [  160/  265]
train() client id: f_00001-11-5 loss: 0.496432  [  192/  265]
train() client id: f_00001-11-6 loss: 0.418161  [  224/  265]
train() client id: f_00001-11-7 loss: 0.323206  [  256/  265]
train() client id: f_00002-0-0 loss: 1.170776  [   32/  124]
train() client id: f_00002-0-1 loss: 1.142908  [   64/  124]
train() client id: f_00002-0-2 loss: 1.375597  [   96/  124]
train() client id: f_00002-1-0 loss: 1.184675  [   32/  124]
train() client id: f_00002-1-1 loss: 1.327794  [   64/  124]
train() client id: f_00002-1-2 loss: 1.038245  [   96/  124]
train() client id: f_00002-2-0 loss: 1.181691  [   32/  124]
train() client id: f_00002-2-1 loss: 0.896939  [   64/  124]
train() client id: f_00002-2-2 loss: 1.265400  [   96/  124]
train() client id: f_00002-3-0 loss: 1.082450  [   32/  124]
train() client id: f_00002-3-1 loss: 1.099812  [   64/  124]
train() client id: f_00002-3-2 loss: 1.209980  [   96/  124]
train() client id: f_00002-4-0 loss: 0.837544  [   32/  124]
train() client id: f_00002-4-1 loss: 1.102147  [   64/  124]
train() client id: f_00002-4-2 loss: 1.160784  [   96/  124]
train() client id: f_00002-5-0 loss: 1.162471  [   32/  124]
train() client id: f_00002-5-1 loss: 0.831813  [   64/  124]
train() client id: f_00002-5-2 loss: 1.134904  [   96/  124]
train() client id: f_00002-6-0 loss: 0.982794  [   32/  124]
train() client id: f_00002-6-1 loss: 1.008543  [   64/  124]
train() client id: f_00002-6-2 loss: 0.958773  [   96/  124]
train() client id: f_00002-7-0 loss: 1.068641  [   32/  124]
train() client id: f_00002-7-1 loss: 0.947388  [   64/  124]
train() client id: f_00002-7-2 loss: 0.981483  [   96/  124]
train() client id: f_00002-8-0 loss: 1.041232  [   32/  124]
train() client id: f_00002-8-1 loss: 1.075240  [   64/  124]
train() client id: f_00002-8-2 loss: 0.944161  [   96/  124]
train() client id: f_00002-9-0 loss: 0.943462  [   32/  124]
train() client id: f_00002-9-1 loss: 0.918742  [   64/  124]
train() client id: f_00002-9-2 loss: 0.918107  [   96/  124]
train() client id: f_00002-10-0 loss: 1.090276  [   32/  124]
train() client id: f_00002-10-1 loss: 0.891631  [   64/  124]
train() client id: f_00002-10-2 loss: 0.937616  [   96/  124]
train() client id: f_00002-11-0 loss: 0.942180  [   32/  124]
train() client id: f_00002-11-1 loss: 0.977520  [   64/  124]
train() client id: f_00002-11-2 loss: 0.978937  [   96/  124]
train() client id: f_00003-0-0 loss: 0.636191  [   32/   43]
train() client id: f_00003-1-0 loss: 0.526450  [   32/   43]
train() client id: f_00003-2-0 loss: 0.878553  [   32/   43]
train() client id: f_00003-3-0 loss: 0.697735  [   32/   43]
train() client id: f_00003-4-0 loss: 0.616600  [   32/   43]
train() client id: f_00003-5-0 loss: 0.747535  [   32/   43]
train() client id: f_00003-6-0 loss: 0.505866  [   32/   43]
train() client id: f_00003-7-0 loss: 0.745534  [   32/   43]
train() client id: f_00003-8-0 loss: 0.568085  [   32/   43]
train() client id: f_00003-9-0 loss: 0.545828  [   32/   43]
train() client id: f_00003-10-0 loss: 0.695402  [   32/   43]
train() client id: f_00003-11-0 loss: 0.725243  [   32/   43]
train() client id: f_00004-0-0 loss: 1.121355  [   32/  306]
train() client id: f_00004-0-1 loss: 1.050421  [   64/  306]
train() client id: f_00004-0-2 loss: 1.004684  [   96/  306]
train() client id: f_00004-0-3 loss: 1.065162  [  128/  306]
train() client id: f_00004-0-4 loss: 0.798743  [  160/  306]
train() client id: f_00004-0-5 loss: 0.926170  [  192/  306]
train() client id: f_00004-0-6 loss: 0.910464  [  224/  306]
train() client id: f_00004-0-7 loss: 1.044197  [  256/  306]
train() client id: f_00004-0-8 loss: 0.937572  [  288/  306]
train() client id: f_00004-1-0 loss: 0.892288  [   32/  306]
train() client id: f_00004-1-1 loss: 1.047273  [   64/  306]
train() client id: f_00004-1-2 loss: 0.979679  [   96/  306]
train() client id: f_00004-1-3 loss: 1.147411  [  128/  306]
train() client id: f_00004-1-4 loss: 0.966779  [  160/  306]
train() client id: f_00004-1-5 loss: 0.879448  [  192/  306]
train() client id: f_00004-1-6 loss: 0.978496  [  224/  306]
train() client id: f_00004-1-7 loss: 0.978809  [  256/  306]
train() client id: f_00004-1-8 loss: 0.888064  [  288/  306]
train() client id: f_00004-2-0 loss: 0.973539  [   32/  306]
train() client id: f_00004-2-1 loss: 1.020006  [   64/  306]
train() client id: f_00004-2-2 loss: 0.961853  [   96/  306]
train() client id: f_00004-2-3 loss: 0.880951  [  128/  306]
train() client id: f_00004-2-4 loss: 0.852903  [  160/  306]
train() client id: f_00004-2-5 loss: 0.964235  [  192/  306]
train() client id: f_00004-2-6 loss: 1.036042  [  224/  306]
train() client id: f_00004-2-7 loss: 0.997100  [  256/  306]
train() client id: f_00004-2-8 loss: 0.991223  [  288/  306]
train() client id: f_00004-3-0 loss: 0.935500  [   32/  306]
train() client id: f_00004-3-1 loss: 1.009629  [   64/  306]
train() client id: f_00004-3-2 loss: 1.064388  [   96/  306]
train() client id: f_00004-3-3 loss: 1.065261  [  128/  306]
train() client id: f_00004-3-4 loss: 0.891388  [  160/  306]
train() client id: f_00004-3-5 loss: 0.915290  [  192/  306]
train() client id: f_00004-3-6 loss: 0.925634  [  224/  306]
train() client id: f_00004-3-7 loss: 0.918302  [  256/  306]
train() client id: f_00004-3-8 loss: 0.903487  [  288/  306]
train() client id: f_00004-4-0 loss: 1.041344  [   32/  306]
train() client id: f_00004-4-1 loss: 0.988289  [   64/  306]
train() client id: f_00004-4-2 loss: 0.957051  [   96/  306]
train() client id: f_00004-4-3 loss: 0.939842  [  128/  306]
train() client id: f_00004-4-4 loss: 1.016986  [  160/  306]
train() client id: f_00004-4-5 loss: 0.950927  [  192/  306]
train() client id: f_00004-4-6 loss: 0.970214  [  224/  306]
train() client id: f_00004-4-7 loss: 0.987336  [  256/  306]
train() client id: f_00004-4-8 loss: 0.843615  [  288/  306]
train() client id: f_00004-5-0 loss: 0.880920  [   32/  306]
train() client id: f_00004-5-1 loss: 0.926423  [   64/  306]
train() client id: f_00004-5-2 loss: 1.095377  [   96/  306]
train() client id: f_00004-5-3 loss: 1.005906  [  128/  306]
train() client id: f_00004-5-4 loss: 0.947338  [  160/  306]
train() client id: f_00004-5-5 loss: 0.893620  [  192/  306]
train() client id: f_00004-5-6 loss: 1.052939  [  224/  306]
train() client id: f_00004-5-7 loss: 0.882206  [  256/  306]
train() client id: f_00004-5-8 loss: 0.884422  [  288/  306]
train() client id: f_00004-6-0 loss: 0.904975  [   32/  306]
train() client id: f_00004-6-1 loss: 1.012426  [   64/  306]
train() client id: f_00004-6-2 loss: 1.018082  [   96/  306]
train() client id: f_00004-6-3 loss: 1.072664  [  128/  306]
train() client id: f_00004-6-4 loss: 1.041109  [  160/  306]
train() client id: f_00004-6-5 loss: 0.818407  [  192/  306]
train() client id: f_00004-6-6 loss: 0.837823  [  224/  306]
train() client id: f_00004-6-7 loss: 0.950087  [  256/  306]
train() client id: f_00004-6-8 loss: 0.910422  [  288/  306]
train() client id: f_00004-7-0 loss: 1.121559  [   32/  306]
train() client id: f_00004-7-1 loss: 0.901575  [   64/  306]
train() client id: f_00004-7-2 loss: 0.897260  [   96/  306]
train() client id: f_00004-7-3 loss: 0.892576  [  128/  306]
train() client id: f_00004-7-4 loss: 1.061316  [  160/  306]
train() client id: f_00004-7-5 loss: 0.957590  [  192/  306]
train() client id: f_00004-7-6 loss: 0.890564  [  224/  306]
train() client id: f_00004-7-7 loss: 0.727069  [  256/  306]
train() client id: f_00004-7-8 loss: 0.998341  [  288/  306]
train() client id: f_00004-8-0 loss: 0.835143  [   32/  306]
train() client id: f_00004-8-1 loss: 0.898333  [   64/  306]
train() client id: f_00004-8-2 loss: 1.030823  [   96/  306]
train() client id: f_00004-8-3 loss: 0.943709  [  128/  306]
train() client id: f_00004-8-4 loss: 0.955656  [  160/  306]
train() client id: f_00004-8-5 loss: 0.905882  [  192/  306]
train() client id: f_00004-8-6 loss: 0.881580  [  224/  306]
train() client id: f_00004-8-7 loss: 1.036999  [  256/  306]
train() client id: f_00004-8-8 loss: 0.978526  [  288/  306]
train() client id: f_00004-9-0 loss: 0.902553  [   32/  306]
train() client id: f_00004-9-1 loss: 0.951338  [   64/  306]
train() client id: f_00004-9-2 loss: 0.928458  [   96/  306]
train() client id: f_00004-9-3 loss: 0.935157  [  128/  306]
train() client id: f_00004-9-4 loss: 0.974421  [  160/  306]
train() client id: f_00004-9-5 loss: 0.863644  [  192/  306]
train() client id: f_00004-9-6 loss: 0.992300  [  224/  306]
train() client id: f_00004-9-7 loss: 1.023752  [  256/  306]
train() client id: f_00004-9-8 loss: 0.887465  [  288/  306]
train() client id: f_00004-10-0 loss: 0.883070  [   32/  306]
train() client id: f_00004-10-1 loss: 0.906947  [   64/  306]
train() client id: f_00004-10-2 loss: 0.914993  [   96/  306]
train() client id: f_00004-10-3 loss: 0.947034  [  128/  306]
train() client id: f_00004-10-4 loss: 0.897239  [  160/  306]
train() client id: f_00004-10-5 loss: 0.991130  [  192/  306]
train() client id: f_00004-10-6 loss: 0.961381  [  224/  306]
train() client id: f_00004-10-7 loss: 0.950965  [  256/  306]
train() client id: f_00004-10-8 loss: 1.002338  [  288/  306]
train() client id: f_00004-11-0 loss: 0.947561  [   32/  306]
train() client id: f_00004-11-1 loss: 1.050807  [   64/  306]
train() client id: f_00004-11-2 loss: 0.923799  [   96/  306]
train() client id: f_00004-11-3 loss: 0.848843  [  128/  306]
train() client id: f_00004-11-4 loss: 0.861235  [  160/  306]
train() client id: f_00004-11-5 loss: 0.977364  [  192/  306]
train() client id: f_00004-11-6 loss: 0.801123  [  224/  306]
train() client id: f_00004-11-7 loss: 0.874153  [  256/  306]
train() client id: f_00004-11-8 loss: 1.091646  [  288/  306]
train() client id: f_00005-0-0 loss: 0.792472  [   32/  146]
train() client id: f_00005-0-1 loss: 0.847324  [   64/  146]
train() client id: f_00005-0-2 loss: 1.012121  [   96/  146]
train() client id: f_00005-0-3 loss: 0.876348  [  128/  146]
train() client id: f_00005-1-0 loss: 0.829386  [   32/  146]
train() client id: f_00005-1-1 loss: 0.953524  [   64/  146]
train() client id: f_00005-1-2 loss: 0.794985  [   96/  146]
train() client id: f_00005-1-3 loss: 0.825913  [  128/  146]
train() client id: f_00005-2-0 loss: 1.033515  [   32/  146]
train() client id: f_00005-2-1 loss: 0.895254  [   64/  146]
train() client id: f_00005-2-2 loss: 0.723109  [   96/  146]
train() client id: f_00005-2-3 loss: 0.938887  [  128/  146]
train() client id: f_00005-3-0 loss: 0.967455  [   32/  146]
train() client id: f_00005-3-1 loss: 0.709822  [   64/  146]
train() client id: f_00005-3-2 loss: 0.766490  [   96/  146]
train() client id: f_00005-3-3 loss: 1.080521  [  128/  146]
train() client id: f_00005-4-0 loss: 0.917432  [   32/  146]
train() client id: f_00005-4-1 loss: 1.085048  [   64/  146]
train() client id: f_00005-4-2 loss: 0.863114  [   96/  146]
train() client id: f_00005-4-3 loss: 0.815895  [  128/  146]
train() client id: f_00005-5-0 loss: 0.998429  [   32/  146]
train() client id: f_00005-5-1 loss: 0.844113  [   64/  146]
train() client id: f_00005-5-2 loss: 0.815125  [   96/  146]
train() client id: f_00005-5-3 loss: 0.897756  [  128/  146]
train() client id: f_00005-6-0 loss: 0.820950  [   32/  146]
train() client id: f_00005-6-1 loss: 0.785442  [   64/  146]
train() client id: f_00005-6-2 loss: 0.881254  [   96/  146]
train() client id: f_00005-6-3 loss: 0.935972  [  128/  146]
train() client id: f_00005-7-0 loss: 0.809303  [   32/  146]
train() client id: f_00005-7-1 loss: 0.775767  [   64/  146]
train() client id: f_00005-7-2 loss: 1.065029  [   96/  146]
train() client id: f_00005-7-3 loss: 0.965704  [  128/  146]
train() client id: f_00005-8-0 loss: 0.895124  [   32/  146]
train() client id: f_00005-8-1 loss: 0.834594  [   64/  146]
train() client id: f_00005-8-2 loss: 0.937318  [   96/  146]
train() client id: f_00005-8-3 loss: 0.965598  [  128/  146]
train() client id: f_00005-9-0 loss: 0.785459  [   32/  146]
train() client id: f_00005-9-1 loss: 0.826904  [   64/  146]
train() client id: f_00005-9-2 loss: 0.829378  [   96/  146]
train() client id: f_00005-9-3 loss: 0.897763  [  128/  146]
train() client id: f_00005-10-0 loss: 0.930686  [   32/  146]
train() client id: f_00005-10-1 loss: 0.658921  [   64/  146]
train() client id: f_00005-10-2 loss: 1.135509  [   96/  146]
train() client id: f_00005-10-3 loss: 0.729349  [  128/  146]
train() client id: f_00005-11-0 loss: 0.981389  [   32/  146]
train() client id: f_00005-11-1 loss: 0.694588  [   64/  146]
train() client id: f_00005-11-2 loss: 0.753923  [   96/  146]
train() client id: f_00005-11-3 loss: 1.047508  [  128/  146]
train() client id: f_00006-0-0 loss: 0.513476  [   32/   54]
train() client id: f_00006-1-0 loss: 0.469847  [   32/   54]
train() client id: f_00006-2-0 loss: 0.592885  [   32/   54]
train() client id: f_00006-3-0 loss: 0.533659  [   32/   54]
train() client id: f_00006-4-0 loss: 0.588381  [   32/   54]
train() client id: f_00006-5-0 loss: 0.533910  [   32/   54]
train() client id: f_00006-6-0 loss: 0.581503  [   32/   54]
train() client id: f_00006-7-0 loss: 0.539326  [   32/   54]
train() client id: f_00006-8-0 loss: 0.511711  [   32/   54]
train() client id: f_00006-9-0 loss: 0.508664  [   32/   54]
train() client id: f_00006-10-0 loss: 0.561357  [   32/   54]
train() client id: f_00006-11-0 loss: 0.521293  [   32/   54]
train() client id: f_00007-0-0 loss: 0.648767  [   32/  179]
train() client id: f_00007-0-1 loss: 0.597165  [   64/  179]
train() client id: f_00007-0-2 loss: 0.955821  [   96/  179]
train() client id: f_00007-0-3 loss: 0.590884  [  128/  179]
train() client id: f_00007-0-4 loss: 0.723354  [  160/  179]
train() client id: f_00007-1-0 loss: 0.857535  [   32/  179]
train() client id: f_00007-1-1 loss: 0.573668  [   64/  179]
train() client id: f_00007-1-2 loss: 0.822044  [   96/  179]
train() client id: f_00007-1-3 loss: 0.635854  [  128/  179]
train() client id: f_00007-1-4 loss: 0.720915  [  160/  179]
train() client id: f_00007-2-0 loss: 0.603532  [   32/  179]
train() client id: f_00007-2-1 loss: 0.647999  [   64/  179]
train() client id: f_00007-2-2 loss: 0.637135  [   96/  179]
train() client id: f_00007-2-3 loss: 0.772706  [  128/  179]
train() client id: f_00007-2-4 loss: 0.711167  [  160/  179]
train() client id: f_00007-3-0 loss: 0.534775  [   32/  179]
train() client id: f_00007-3-1 loss: 0.574859  [   64/  179]
train() client id: f_00007-3-2 loss: 0.905097  [   96/  179]
train() client id: f_00007-3-3 loss: 0.633438  [  128/  179]
train() client id: f_00007-3-4 loss: 0.917589  [  160/  179]
train() client id: f_00007-4-0 loss: 0.776905  [   32/  179]
train() client id: f_00007-4-1 loss: 0.569841  [   64/  179]
train() client id: f_00007-4-2 loss: 0.725641  [   96/  179]
train() client id: f_00007-4-3 loss: 0.747772  [  128/  179]
train() client id: f_00007-4-4 loss: 0.710721  [  160/  179]
train() client id: f_00007-5-0 loss: 0.702073  [   32/  179]
train() client id: f_00007-5-1 loss: 0.819487  [   64/  179]
train() client id: f_00007-5-2 loss: 0.533504  [   96/  179]
train() client id: f_00007-5-3 loss: 0.626608  [  128/  179]
train() client id: f_00007-5-4 loss: 0.677602  [  160/  179]
train() client id: f_00007-6-0 loss: 0.627813  [   32/  179]
train() client id: f_00007-6-1 loss: 0.703885  [   64/  179]
train() client id: f_00007-6-2 loss: 0.717236  [   96/  179]
train() client id: f_00007-6-3 loss: 0.773328  [  128/  179]
train() client id: f_00007-6-4 loss: 0.747446  [  160/  179]
train() client id: f_00007-7-0 loss: 0.569808  [   32/  179]
train() client id: f_00007-7-1 loss: 0.764837  [   64/  179]
train() client id: f_00007-7-2 loss: 0.742988  [   96/  179]
train() client id: f_00007-7-3 loss: 0.718888  [  128/  179]
train() client id: f_00007-7-4 loss: 0.768530  [  160/  179]
train() client id: f_00007-8-0 loss: 0.775455  [   32/  179]
train() client id: f_00007-8-1 loss: 0.660453  [   64/  179]
train() client id: f_00007-8-2 loss: 0.721801  [   96/  179]
train() client id: f_00007-8-3 loss: 0.643821  [  128/  179]
train() client id: f_00007-8-4 loss: 0.668387  [  160/  179]
train() client id: f_00007-9-0 loss: 0.719584  [   32/  179]
train() client id: f_00007-9-1 loss: 0.576671  [   64/  179]
train() client id: f_00007-9-2 loss: 0.633827  [   96/  179]
train() client id: f_00007-9-3 loss: 0.934607  [  128/  179]
train() client id: f_00007-9-4 loss: 0.718752  [  160/  179]
train() client id: f_00007-10-0 loss: 0.673125  [   32/  179]
train() client id: f_00007-10-1 loss: 0.662221  [   64/  179]
train() client id: f_00007-10-2 loss: 0.620955  [   96/  179]
train() client id: f_00007-10-3 loss: 0.726755  [  128/  179]
train() client id: f_00007-10-4 loss: 0.904592  [  160/  179]
train() client id: f_00007-11-0 loss: 0.778621  [   32/  179]
train() client id: f_00007-11-1 loss: 0.726436  [   64/  179]
train() client id: f_00007-11-2 loss: 0.656146  [   96/  179]
train() client id: f_00007-11-3 loss: 0.724454  [  128/  179]
train() client id: f_00007-11-4 loss: 0.550412  [  160/  179]
train() client id: f_00008-0-0 loss: 0.902213  [   32/  130]
train() client id: f_00008-0-1 loss: 0.841446  [   64/  130]
train() client id: f_00008-0-2 loss: 0.630832  [   96/  130]
train() client id: f_00008-0-3 loss: 0.696115  [  128/  130]
train() client id: f_00008-1-0 loss: 0.711471  [   32/  130]
train() client id: f_00008-1-1 loss: 0.725735  [   64/  130]
train() client id: f_00008-1-2 loss: 0.843324  [   96/  130]
train() client id: f_00008-1-3 loss: 0.828960  [  128/  130]
train() client id: f_00008-2-0 loss: 0.770561  [   32/  130]
train() client id: f_00008-2-1 loss: 0.710525  [   64/  130]
train() client id: f_00008-2-2 loss: 0.801808  [   96/  130]
train() client id: f_00008-2-3 loss: 0.770294  [  128/  130]
train() client id: f_00008-3-0 loss: 0.652335  [   32/  130]
train() client id: f_00008-3-1 loss: 0.898195  [   64/  130]
train() client id: f_00008-3-2 loss: 0.795037  [   96/  130]
train() client id: f_00008-3-3 loss: 0.764695  [  128/  130]
train() client id: f_00008-4-0 loss: 0.829730  [   32/  130]
train() client id: f_00008-4-1 loss: 0.705538  [   64/  130]
train() client id: f_00008-4-2 loss: 0.795388  [   96/  130]
train() client id: f_00008-4-3 loss: 0.748309  [  128/  130]
train() client id: f_00008-5-0 loss: 0.736201  [   32/  130]
train() client id: f_00008-5-1 loss: 0.757798  [   64/  130]
train() client id: f_00008-5-2 loss: 0.801997  [   96/  130]
train() client id: f_00008-5-3 loss: 0.757271  [  128/  130]
train() client id: f_00008-6-0 loss: 0.645373  [   32/  130]
train() client id: f_00008-6-1 loss: 0.798703  [   64/  130]
train() client id: f_00008-6-2 loss: 0.882484  [   96/  130]
train() client id: f_00008-6-3 loss: 0.771549  [  128/  130]
train() client id: f_00008-7-0 loss: 0.815248  [   32/  130]
train() client id: f_00008-7-1 loss: 0.766440  [   64/  130]
train() client id: f_00008-7-2 loss: 0.670746  [   96/  130]
train() client id: f_00008-7-3 loss: 0.833013  [  128/  130]
train() client id: f_00008-8-0 loss: 0.794740  [   32/  130]
train() client id: f_00008-8-1 loss: 0.763173  [   64/  130]
train() client id: f_00008-8-2 loss: 0.782937  [   96/  130]
train() client id: f_00008-8-3 loss: 0.711694  [  128/  130]
train() client id: f_00008-9-0 loss: 0.617249  [   32/  130]
train() client id: f_00008-9-1 loss: 0.810502  [   64/  130]
train() client id: f_00008-9-2 loss: 0.718699  [   96/  130]
train() client id: f_00008-9-3 loss: 0.929900  [  128/  130]
train() client id: f_00008-10-0 loss: 0.733080  [   32/  130]
train() client id: f_00008-10-1 loss: 0.735400  [   64/  130]
train() client id: f_00008-10-2 loss: 0.775074  [   96/  130]
train() client id: f_00008-10-3 loss: 0.821792  [  128/  130]
train() client id: f_00008-11-0 loss: 0.740250  [   32/  130]
train() client id: f_00008-11-1 loss: 0.719847  [   64/  130]
train() client id: f_00008-11-2 loss: 0.696382  [   96/  130]
train() client id: f_00008-11-3 loss: 0.934686  [  128/  130]
train() client id: f_00009-0-0 loss: 0.878703  [   32/  118]
train() client id: f_00009-0-1 loss: 1.041435  [   64/  118]
train() client id: f_00009-0-2 loss: 1.026182  [   96/  118]
train() client id: f_00009-1-0 loss: 0.884862  [   32/  118]
train() client id: f_00009-1-1 loss: 1.012059  [   64/  118]
train() client id: f_00009-1-2 loss: 0.927218  [   96/  118]
train() client id: f_00009-2-0 loss: 0.833572  [   32/  118]
train() client id: f_00009-2-1 loss: 0.906761  [   64/  118]
train() client id: f_00009-2-2 loss: 0.924475  [   96/  118]
train() client id: f_00009-3-0 loss: 0.875982  [   32/  118]
train() client id: f_00009-3-1 loss: 0.893248  [   64/  118]
train() client id: f_00009-3-2 loss: 0.951034  [   96/  118]
train() client id: f_00009-4-0 loss: 0.939491  [   32/  118]
train() client id: f_00009-4-1 loss: 0.826199  [   64/  118]
train() client id: f_00009-4-2 loss: 0.760302  [   96/  118]
train() client id: f_00009-5-0 loss: 0.710314  [   32/  118]
train() client id: f_00009-5-1 loss: 0.872112  [   64/  118]
train() client id: f_00009-5-2 loss: 0.974699  [   96/  118]
train() client id: f_00009-6-0 loss: 0.810390  [   32/  118]
train() client id: f_00009-6-1 loss: 0.835336  [   64/  118]
train() client id: f_00009-6-2 loss: 0.809845  [   96/  118]
train() client id: f_00009-7-0 loss: 0.770549  [   32/  118]
train() client id: f_00009-7-1 loss: 0.756714  [   64/  118]
train() client id: f_00009-7-2 loss: 0.957575  [   96/  118]
train() client id: f_00009-8-0 loss: 0.831042  [   32/  118]
train() client id: f_00009-8-1 loss: 0.799270  [   64/  118]
train() client id: f_00009-8-2 loss: 0.780605  [   96/  118]
train() client id: f_00009-9-0 loss: 0.904881  [   32/  118]
train() client id: f_00009-9-1 loss: 0.628876  [   64/  118]
train() client id: f_00009-9-2 loss: 0.891168  [   96/  118]
train() client id: f_00009-10-0 loss: 0.793752  [   32/  118]
train() client id: f_00009-10-1 loss: 0.703497  [   64/  118]
train() client id: f_00009-10-2 loss: 0.874429  [   96/  118]
train() client id: f_00009-11-0 loss: 0.681900  [   32/  118]
train() client id: f_00009-11-1 loss: 0.772120  [   64/  118]
train() client id: f_00009-11-2 loss: 0.852978  [   96/  118]
At round 31 accuracy: 0.6525198938992043
At round 31 training accuracy: 0.579476861167002
At round 31 training loss: 0.8385042555222116
update_location
xs = [  -3.9056584     4.20031788  175.00902392   18.81129433    0.97929623
    3.95640986 -137.44319194 -116.32485185  159.66397685 -102.06087855]
ys = [ 167.5879595   150.55583871    1.32061395 -137.45517586  129.35018685
  112.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [195.19471903 180.78911261 201.56860488 171.01985313 163.50055002
 150.80678296 169.99271029 153.40191463 189.21219695 142.94206794]
dists_bs = [171.2548162  180.85981021 390.7311684  367.68336205 181.80379023
 189.45309836 181.55205558 183.79506923 369.84387958 185.90483768]
uav_gains = [1.82789532e-11 2.25026314e-11 1.66686654e-11 2.59975805e-11
 2.91614959e-11 3.57650883e-11 2.64028374e-11 3.42619275e-11
 1.99218930e-11 4.09129213e-11]
bs_gains = [6.15310409e-11 5.28124466e-11 6.10992006e-12 7.24380826e-12
 5.20482189e-11 4.63755733e-11 5.22505430e-11 5.04846428e-11
 7.12594497e-12 4.88967675e-11]
Round 32
-------------------------------
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.76786425 14.02640477  6.66672465  2.4004724  16.17773155  7.78673034
  2.97703515  9.52653575  7.02653207  6.31639524]
obj_prev = 79.67242616820144
eta_min = 2.1062834512478834e-14	eta_max = 0.9270669874026624
af = 16.815721006576958	bf = 1.436444871137545	zeta = 18.497293107234654	eta = 0.9090909090909091
af = 16.815721006576958	bf = 1.436444871137545	zeta = 33.430696901951336	eta = 0.5030024069164837
af = 16.815721006576958	bf = 1.436444871137545	zeta = 26.140048449463926	eta = 0.643293413900379
af = 16.815721006576958	bf = 1.436444871137545	zeta = 24.823688418099213	eta = 0.677406222772134
af = 16.815721006576958	bf = 1.436444871137545	zeta = 24.755287540536187	eta = 0.6792779513888344
af = 16.815721006576958	bf = 1.436444871137545	zeta = 24.755088130913464	eta = 0.6792834231754543
eta = 0.6792834231754543
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [0.03191095 0.06711429 0.03140442 0.01089024 0.07749801 0.03697618
 0.01367612 0.0453338  0.03292399 0.02988485]
ene_total = [2.19996663 3.98610054 2.18445289 1.03108082 4.54479709 2.37816282
 1.17905352 2.85331091 2.40505659 1.99310631]
ti_comp = [0.43830311 0.4576972  0.43612707 0.44578269 0.45748595 0.45576916
 0.44608386 0.45085659 0.41023879 0.45656667]
ti_coms = [0.09059974 0.07120565 0.09277578 0.08312016 0.07141689 0.07313368
 0.08281899 0.07804626 0.11866406 0.07233618]
t_total = [28.39986572 28.39986572 28.39986572 28.39986572 28.39986572 28.39986572
 28.39986572 28.39986572 28.39986572 28.39986572]
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [1.05718244e-05 9.01921040e-05 1.01771547e-05 4.06206033e-07
 1.38994033e-04 1.52109415e-05 8.03407087e-07 2.86463737e-05
 1.32539102e-05 8.00248765e-06]
ene_total = [0.48705029 0.38718743 0.49871352 0.44634221 0.39094219 0.39351399
 0.44474637 0.42061388 0.63788794 0.38884464]
optimize_network iter = 0 obj = 4.495842450611667
eta = 0.6792834231754543
freqs = [36402828.40748152 73317349.05671008 36003753.27798384 12214744.53533785
 84699880.32505599 40564586.30164858 15329090.24416601 50275192.55652302
 40127838.49551824 32727809.18735842]
eta_min = 0.6792834231754692	eta_max = 0.6792834231754555
af = 0.014083508139639458	bf = 1.436444871137545	zeta = 0.015491858953603405	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [2.60553049e-06 2.22287345e-05 2.50826025e-06 1.00113487e-07
 3.42564517e-05 3.74888670e-06 1.98007609e-07 7.06018162e-06
 3.26655702e-06 1.97229209e-06]
ene_total = [1.71674703 1.35307722 1.75794987 1.57458483 1.35935733 1.38609978
 1.56889819 1.47978715 2.24850168 1.37065582]
ti_comp = [0.43830311 0.4576972  0.43612707 0.44578269 0.45748595 0.45576916
 0.44608386 0.45085659 0.41023879 0.45656667]
ti_coms = [0.09059974 0.07120565 0.09277578 0.08312016 0.07141689 0.07313368
 0.08281899 0.07804626 0.11866406 0.07233618]
t_total = [28.39986572 28.39986572 28.39986572 28.39986572 28.39986572 28.39986572
 28.39986572 28.39986572 28.39986572 28.39986572]
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [1.05718244e-05 9.01921040e-05 1.01771547e-05 4.06206033e-07
 1.38994033e-04 1.52109415e-05 8.03407087e-07 2.86463737e-05
 1.32539102e-05 8.00248765e-06]
ene_total = [0.48705029 0.38718743 0.49871352 0.44634221 0.39094219 0.39351399
 0.44474637 0.42061388 0.63788794 0.38884464]
optimize_network iter = 1 obj = 4.495842450611684
eta = 0.6792834231754555
freqs = [36402828.40748152 73317349.05671008 36003753.27798384 12214744.53533785
 84699880.32505599 40564586.30164856 15329090.24416601 50275192.55652302
 40127838.49551825 32727809.18735842]
Done!
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [1.00182543e-05 8.54693954e-05 9.64425066e-06 3.84935959e-07
 1.31715920e-04 1.44144544e-05 7.61338465e-07 2.71463702e-05
 1.25598987e-05 7.58345522e-06]
ene_total = [0.00906999 0.00720603 0.00928722 0.0083124  0.00727341 0.00732778
 0.00828266 0.00783177 0.01187897 0.0072412 ]
At round 32 energy consumption: 0.08371143671629988
At round 32 eta: 0.6792834231754555
At round 32 a_n: 16.878589906266903
At round 32 local rounds: 12.663073636613152
At round 32 global rounds: 53.69580806749958
gradient difference: 0.4266306757926941
train() client id: f_00000-0-0 loss: 1.296455  [   32/  126]
train() client id: f_00000-0-1 loss: 1.225846  [   64/  126]
train() client id: f_00000-0-2 loss: 1.182618  [   96/  126]
train() client id: f_00000-1-0 loss: 1.104594  [   32/  126]
train() client id: f_00000-1-1 loss: 1.117051  [   64/  126]
train() client id: f_00000-1-2 loss: 1.244762  [   96/  126]
train() client id: f_00000-2-0 loss: 1.112885  [   32/  126]
train() client id: f_00000-2-1 loss: 1.054131  [   64/  126]
train() client id: f_00000-2-2 loss: 1.001883  [   96/  126]
train() client id: f_00000-3-0 loss: 1.059671  [   32/  126]
train() client id: f_00000-3-1 loss: 1.109755  [   64/  126]
train() client id: f_00000-3-2 loss: 0.887554  [   96/  126]
train() client id: f_00000-4-0 loss: 0.982177  [   32/  126]
train() client id: f_00000-4-1 loss: 0.899275  [   64/  126]
train() client id: f_00000-4-2 loss: 0.977062  [   96/  126]
train() client id: f_00000-5-0 loss: 0.899684  [   32/  126]
train() client id: f_00000-5-1 loss: 0.934240  [   64/  126]
train() client id: f_00000-5-2 loss: 0.925550  [   96/  126]
train() client id: f_00000-6-0 loss: 0.804975  [   32/  126]
train() client id: f_00000-6-1 loss: 0.885939  [   64/  126]
train() client id: f_00000-6-2 loss: 0.929102  [   96/  126]
train() client id: f_00000-7-0 loss: 0.933971  [   32/  126]
train() client id: f_00000-7-1 loss: 0.835486  [   64/  126]
train() client id: f_00000-7-2 loss: 0.807607  [   96/  126]
train() client id: f_00000-8-0 loss: 0.848309  [   32/  126]
train() client id: f_00000-8-1 loss: 0.852787  [   64/  126]
train() client id: f_00000-8-2 loss: 0.900099  [   96/  126]
train() client id: f_00000-9-0 loss: 0.823064  [   32/  126]
train() client id: f_00000-9-1 loss: 0.818953  [   64/  126]
train() client id: f_00000-9-2 loss: 0.899020  [   96/  126]
train() client id: f_00000-10-0 loss: 0.958888  [   32/  126]
train() client id: f_00000-10-1 loss: 0.810945  [   64/  126]
train() client id: f_00000-10-2 loss: 0.802243  [   96/  126]
train() client id: f_00000-11-0 loss: 0.843626  [   32/  126]
train() client id: f_00000-11-1 loss: 0.884924  [   64/  126]
train() client id: f_00000-11-2 loss: 0.858953  [   96/  126]
train() client id: f_00001-0-0 loss: 0.524057  [   32/  265]
train() client id: f_00001-0-1 loss: 0.532465  [   64/  265]
train() client id: f_00001-0-2 loss: 0.433261  [   96/  265]
train() client id: f_00001-0-3 loss: 0.538718  [  128/  265]
train() client id: f_00001-0-4 loss: 0.464118  [  160/  265]
train() client id: f_00001-0-5 loss: 0.445550  [  192/  265]
train() client id: f_00001-0-6 loss: 0.485258  [  224/  265]
train() client id: f_00001-0-7 loss: 0.755569  [  256/  265]
train() client id: f_00001-1-0 loss: 0.437820  [   32/  265]
train() client id: f_00001-1-1 loss: 0.594537  [   64/  265]
train() client id: f_00001-1-2 loss: 0.446338  [   96/  265]
train() client id: f_00001-1-3 loss: 0.482966  [  128/  265]
train() client id: f_00001-1-4 loss: 0.440621  [  160/  265]
train() client id: f_00001-1-5 loss: 0.553585  [  192/  265]
train() client id: f_00001-1-6 loss: 0.652236  [  224/  265]
train() client id: f_00001-1-7 loss: 0.593282  [  256/  265]
train() client id: f_00001-2-0 loss: 0.431772  [   32/  265]
train() client id: f_00001-2-1 loss: 0.677012  [   64/  265]
train() client id: f_00001-2-2 loss: 0.533164  [   96/  265]
train() client id: f_00001-2-3 loss: 0.436605  [  128/  265]
train() client id: f_00001-2-4 loss: 0.678188  [  160/  265]
train() client id: f_00001-2-5 loss: 0.431622  [  192/  265]
train() client id: f_00001-2-6 loss: 0.435542  [  224/  265]
train() client id: f_00001-2-7 loss: 0.533276  [  256/  265]
train() client id: f_00001-3-0 loss: 0.436971  [   32/  265]
train() client id: f_00001-3-1 loss: 0.475266  [   64/  265]
train() client id: f_00001-3-2 loss: 0.486904  [   96/  265]
train() client id: f_00001-3-3 loss: 0.502077  [  128/  265]
train() client id: f_00001-3-4 loss: 0.554877  [  160/  265]
train() client id: f_00001-3-5 loss: 0.536826  [  192/  265]
train() client id: f_00001-3-6 loss: 0.468329  [  224/  265]
train() client id: f_00001-3-7 loss: 0.675581  [  256/  265]
train() client id: f_00001-4-0 loss: 0.550059  [   32/  265]
train() client id: f_00001-4-1 loss: 0.659986  [   64/  265]
train() client id: f_00001-4-2 loss: 0.429027  [   96/  265]
train() client id: f_00001-4-3 loss: 0.431707  [  128/  265]
train() client id: f_00001-4-4 loss: 0.444546  [  160/  265]
train() client id: f_00001-4-5 loss: 0.577888  [  192/  265]
train() client id: f_00001-4-6 loss: 0.549576  [  224/  265]
train() client id: f_00001-4-7 loss: 0.475496  [  256/  265]
train() client id: f_00001-5-0 loss: 0.579073  [   32/  265]
train() client id: f_00001-5-1 loss: 0.612128  [   64/  265]
train() client id: f_00001-5-2 loss: 0.495454  [   96/  265]
train() client id: f_00001-5-3 loss: 0.436782  [  128/  265]
train() client id: f_00001-5-4 loss: 0.438718  [  160/  265]
train() client id: f_00001-5-5 loss: 0.511963  [  192/  265]
train() client id: f_00001-5-6 loss: 0.574102  [  224/  265]
train() client id: f_00001-5-7 loss: 0.472493  [  256/  265]
train() client id: f_00001-6-0 loss: 0.589487  [   32/  265]
train() client id: f_00001-6-1 loss: 0.400871  [   64/  265]
train() client id: f_00001-6-2 loss: 0.537359  [   96/  265]
train() client id: f_00001-6-3 loss: 0.470378  [  128/  265]
train() client id: f_00001-6-4 loss: 0.557050  [  160/  265]
train() client id: f_00001-6-5 loss: 0.508307  [  192/  265]
train() client id: f_00001-6-6 loss: 0.524202  [  224/  265]
train() client id: f_00001-6-7 loss: 0.443531  [  256/  265]
train() client id: f_00001-7-0 loss: 0.551838  [   32/  265]
train() client id: f_00001-7-1 loss: 0.582803  [   64/  265]
train() client id: f_00001-7-2 loss: 0.458747  [   96/  265]
train() client id: f_00001-7-3 loss: 0.493931  [  128/  265]
train() client id: f_00001-7-4 loss: 0.469264  [  160/  265]
train() client id: f_00001-7-5 loss: 0.515867  [  192/  265]
train() client id: f_00001-7-6 loss: 0.529437  [  224/  265]
train() client id: f_00001-7-7 loss: 0.432440  [  256/  265]
train() client id: f_00001-8-0 loss: 0.567335  [   32/  265]
train() client id: f_00001-8-1 loss: 0.424014  [   64/  265]
train() client id: f_00001-8-2 loss: 0.420119  [   96/  265]
train() client id: f_00001-8-3 loss: 0.436010  [  128/  265]
train() client id: f_00001-8-4 loss: 0.687539  [  160/  265]
train() client id: f_00001-8-5 loss: 0.500370  [  192/  265]
train() client id: f_00001-8-6 loss: 0.569251  [  224/  265]
train() client id: f_00001-8-7 loss: 0.463437  [  256/  265]
train() client id: f_00001-9-0 loss: 0.559554  [   32/  265]
train() client id: f_00001-9-1 loss: 0.468293  [   64/  265]
train() client id: f_00001-9-2 loss: 0.430634  [   96/  265]
train() client id: f_00001-9-3 loss: 0.482613  [  128/  265]
train() client id: f_00001-9-4 loss: 0.474921  [  160/  265]
train() client id: f_00001-9-5 loss: 0.429505  [  192/  265]
train() client id: f_00001-9-6 loss: 0.530398  [  224/  265]
train() client id: f_00001-9-7 loss: 0.752961  [  256/  265]
train() client id: f_00001-10-0 loss: 0.547712  [   32/  265]
train() client id: f_00001-10-1 loss: 0.489136  [   64/  265]
train() client id: f_00001-10-2 loss: 0.603981  [   96/  265]
train() client id: f_00001-10-3 loss: 0.583705  [  128/  265]
train() client id: f_00001-10-4 loss: 0.475216  [  160/  265]
train() client id: f_00001-10-5 loss: 0.504354  [  192/  265]
train() client id: f_00001-10-6 loss: 0.418585  [  224/  265]
train() client id: f_00001-10-7 loss: 0.492949  [  256/  265]
train() client id: f_00001-11-0 loss: 0.556230  [   32/  265]
train() client id: f_00001-11-1 loss: 0.422071  [   64/  265]
train() client id: f_00001-11-2 loss: 0.513026  [   96/  265]
train() client id: f_00001-11-3 loss: 0.439774  [  128/  265]
train() client id: f_00001-11-4 loss: 0.499369  [  160/  265]
train() client id: f_00001-11-5 loss: 0.601521  [  192/  265]
train() client id: f_00001-11-6 loss: 0.609709  [  224/  265]
train() client id: f_00001-11-7 loss: 0.500290  [  256/  265]
train() client id: f_00002-0-0 loss: 1.242106  [   32/  124]
train() client id: f_00002-0-1 loss: 1.294509  [   64/  124]
train() client id: f_00002-0-2 loss: 1.157727  [   96/  124]
train() client id: f_00002-1-0 loss: 1.164242  [   32/  124]
train() client id: f_00002-1-1 loss: 0.981341  [   64/  124]
train() client id: f_00002-1-2 loss: 1.141782  [   96/  124]
train() client id: f_00002-2-0 loss: 1.108909  [   32/  124]
train() client id: f_00002-2-1 loss: 1.117651  [   64/  124]
train() client id: f_00002-2-2 loss: 1.198523  [   96/  124]
train() client id: f_00002-3-0 loss: 0.924357  [   32/  124]
train() client id: f_00002-3-1 loss: 1.115956  [   64/  124]
train() client id: f_00002-3-2 loss: 1.085135  [   96/  124]
train() client id: f_00002-4-0 loss: 0.931537  [   32/  124]
train() client id: f_00002-4-1 loss: 1.105189  [   64/  124]
train() client id: f_00002-4-2 loss: 1.028765  [   96/  124]
train() client id: f_00002-5-0 loss: 1.102273  [   32/  124]
train() client id: f_00002-5-1 loss: 0.956149  [   64/  124]
train() client id: f_00002-5-2 loss: 0.957132  [   96/  124]
train() client id: f_00002-6-0 loss: 1.050562  [   32/  124]
train() client id: f_00002-6-1 loss: 0.921256  [   64/  124]
train() client id: f_00002-6-2 loss: 1.036920  [   96/  124]
train() client id: f_00002-7-0 loss: 0.911079  [   32/  124]
train() client id: f_00002-7-1 loss: 1.017325  [   64/  124]
train() client id: f_00002-7-2 loss: 0.956022  [   96/  124]
train() client id: f_00002-8-0 loss: 1.086482  [   32/  124]
train() client id: f_00002-8-1 loss: 0.910614  [   64/  124]
train() client id: f_00002-8-2 loss: 0.820871  [   96/  124]
train() client id: f_00002-9-0 loss: 0.960697  [   32/  124]
train() client id: f_00002-9-1 loss: 0.958027  [   64/  124]
train() client id: f_00002-9-2 loss: 0.876844  [   96/  124]
train() client id: f_00002-10-0 loss: 0.815390  [   32/  124]
train() client id: f_00002-10-1 loss: 0.982416  [   64/  124]
train() client id: f_00002-10-2 loss: 1.128956  [   96/  124]
train() client id: f_00002-11-0 loss: 1.006940  [   32/  124]
train() client id: f_00002-11-1 loss: 0.885957  [   64/  124]
train() client id: f_00002-11-2 loss: 0.855121  [   96/  124]
train() client id: f_00003-0-0 loss: 0.722374  [   32/   43]
train() client id: f_00003-1-0 loss: 0.887851  [   32/   43]
train() client id: f_00003-2-0 loss: 0.664124  [   32/   43]
train() client id: f_00003-3-0 loss: 0.814446  [   32/   43]
train() client id: f_00003-4-0 loss: 0.831907  [   32/   43]
train() client id: f_00003-5-0 loss: 0.655478  [   32/   43]
train() client id: f_00003-6-0 loss: 0.739316  [   32/   43]
train() client id: f_00003-7-0 loss: 0.726308  [   32/   43]
train() client id: f_00003-8-0 loss: 0.614927  [   32/   43]
train() client id: f_00003-9-0 loss: 0.747452  [   32/   43]
train() client id: f_00003-10-0 loss: 0.693303  [   32/   43]
train() client id: f_00003-11-0 loss: 0.586548  [   32/   43]
train() client id: f_00004-0-0 loss: 0.781598  [   32/  306]
train() client id: f_00004-0-1 loss: 0.589896  [   64/  306]
train() client id: f_00004-0-2 loss: 0.635124  [   96/  306]
train() client id: f_00004-0-3 loss: 0.709656  [  128/  306]
train() client id: f_00004-0-4 loss: 0.630470  [  160/  306]
train() client id: f_00004-0-5 loss: 0.587789  [  192/  306]
train() client id: f_00004-0-6 loss: 0.871096  [  224/  306]
train() client id: f_00004-0-7 loss: 0.627428  [  256/  306]
train() client id: f_00004-0-8 loss: 0.726481  [  288/  306]
train() client id: f_00004-1-0 loss: 0.727507  [   32/  306]
train() client id: f_00004-1-1 loss: 0.718165  [   64/  306]
train() client id: f_00004-1-2 loss: 0.755416  [   96/  306]
train() client id: f_00004-1-3 loss: 0.805343  [  128/  306]
train() client id: f_00004-1-4 loss: 0.723501  [  160/  306]
train() client id: f_00004-1-5 loss: 0.658675  [  192/  306]
train() client id: f_00004-1-6 loss: 0.592592  [  224/  306]
train() client id: f_00004-1-7 loss: 0.644563  [  256/  306]
train() client id: f_00004-1-8 loss: 0.651365  [  288/  306]
train() client id: f_00004-2-0 loss: 0.683355  [   32/  306]
train() client id: f_00004-2-1 loss: 0.643360  [   64/  306]
train() client id: f_00004-2-2 loss: 0.883749  [   96/  306]
train() client id: f_00004-2-3 loss: 0.761964  [  128/  306]
train() client id: f_00004-2-4 loss: 0.701718  [  160/  306]
train() client id: f_00004-2-5 loss: 0.643255  [  192/  306]
train() client id: f_00004-2-6 loss: 0.680934  [  224/  306]
train() client id: f_00004-2-7 loss: 0.641597  [  256/  306]
train() client id: f_00004-2-8 loss: 0.656872  [  288/  306]
train() client id: f_00004-3-0 loss: 0.719140  [   32/  306]
train() client id: f_00004-3-1 loss: 0.670407  [   64/  306]
train() client id: f_00004-3-2 loss: 0.621972  [   96/  306]
train() client id: f_00004-3-3 loss: 0.700125  [  128/  306]
train() client id: f_00004-3-4 loss: 0.632373  [  160/  306]
train() client id: f_00004-3-5 loss: 0.724401  [  192/  306]
train() client id: f_00004-3-6 loss: 0.759872  [  224/  306]
train() client id: f_00004-3-7 loss: 0.695653  [  256/  306]
train() client id: f_00004-3-8 loss: 0.612367  [  288/  306]
train() client id: f_00004-4-0 loss: 0.631507  [   32/  306]
train() client id: f_00004-4-1 loss: 0.755728  [   64/  306]
train() client id: f_00004-4-2 loss: 0.650637  [   96/  306]
train() client id: f_00004-4-3 loss: 0.537527  [  128/  306]
train() client id: f_00004-4-4 loss: 0.714068  [  160/  306]
train() client id: f_00004-4-5 loss: 0.611733  [  192/  306]
train() client id: f_00004-4-6 loss: 0.819247  [  224/  306]
train() client id: f_00004-4-7 loss: 0.812366  [  256/  306]
train() client id: f_00004-4-8 loss: 0.708529  [  288/  306]
train() client id: f_00004-5-0 loss: 0.747558  [   32/  306]
train() client id: f_00004-5-1 loss: 0.806740  [   64/  306]
train() client id: f_00004-5-2 loss: 0.669811  [   96/  306]
train() client id: f_00004-5-3 loss: 0.744579  [  128/  306]
train() client id: f_00004-5-4 loss: 0.606483  [  160/  306]
train() client id: f_00004-5-5 loss: 0.606437  [  192/  306]
train() client id: f_00004-5-6 loss: 0.688074  [  224/  306]
train() client id: f_00004-5-7 loss: 0.651316  [  256/  306]
train() client id: f_00004-5-8 loss: 0.690548  [  288/  306]
train() client id: f_00004-6-0 loss: 0.617789  [   32/  306]
train() client id: f_00004-6-1 loss: 0.681341  [   64/  306]
train() client id: f_00004-6-2 loss: 0.785302  [   96/  306]
train() client id: f_00004-6-3 loss: 0.614481  [  128/  306]
train() client id: f_00004-6-4 loss: 0.587102  [  160/  306]
train() client id: f_00004-6-5 loss: 0.775493  [  192/  306]
train() client id: f_00004-6-6 loss: 0.644926  [  224/  306]
train() client id: f_00004-6-7 loss: 0.718206  [  256/  306]
train() client id: f_00004-6-8 loss: 0.798260  [  288/  306]
train() client id: f_00004-7-0 loss: 0.735879  [   32/  306]
train() client id: f_00004-7-1 loss: 0.831882  [   64/  306]
train() client id: f_00004-7-2 loss: 0.687489  [   96/  306]
train() client id: f_00004-7-3 loss: 0.668091  [  128/  306]
train() client id: f_00004-7-4 loss: 0.631422  [  160/  306]
train() client id: f_00004-7-5 loss: 0.691943  [  192/  306]
train() client id: f_00004-7-6 loss: 0.665899  [  224/  306]
train() client id: f_00004-7-7 loss: 0.777119  [  256/  306]
train() client id: f_00004-7-8 loss: 0.558548  [  288/  306]
train() client id: f_00004-8-0 loss: 0.733601  [   32/  306]
train() client id: f_00004-8-1 loss: 0.641716  [   64/  306]
train() client id: f_00004-8-2 loss: 0.674299  [   96/  306]
train() client id: f_00004-8-3 loss: 0.732672  [  128/  306]
train() client id: f_00004-8-4 loss: 0.759989  [  160/  306]
train() client id: f_00004-8-5 loss: 0.713792  [  192/  306]
train() client id: f_00004-8-6 loss: 0.582100  [  224/  306]
train() client id: f_00004-8-7 loss: 0.763169  [  256/  306]
train() client id: f_00004-8-8 loss: 0.674907  [  288/  306]
train() client id: f_00004-9-0 loss: 0.783746  [   32/  306]
train() client id: f_00004-9-1 loss: 0.628606  [   64/  306]
train() client id: f_00004-9-2 loss: 0.776768  [   96/  306]
train() client id: f_00004-9-3 loss: 0.545329  [  128/  306]
train() client id: f_00004-9-4 loss: 0.578379  [  160/  306]
train() client id: f_00004-9-5 loss: 0.735005  [  192/  306]
train() client id: f_00004-9-6 loss: 0.613561  [  224/  306]
train() client id: f_00004-9-7 loss: 0.716408  [  256/  306]
train() client id: f_00004-9-8 loss: 0.834975  [  288/  306]
train() client id: f_00004-10-0 loss: 0.606413  [   32/  306]
train() client id: f_00004-10-1 loss: 0.760692  [   64/  306]
train() client id: f_00004-10-2 loss: 0.578828  [   96/  306]
train() client id: f_00004-10-3 loss: 0.726518  [  128/  306]
train() client id: f_00004-10-4 loss: 0.756957  [  160/  306]
train() client id: f_00004-10-5 loss: 0.639664  [  192/  306]
train() client id: f_00004-10-6 loss: 0.842217  [  224/  306]
train() client id: f_00004-10-7 loss: 0.705515  [  256/  306]
train() client id: f_00004-10-8 loss: 0.734553  [  288/  306]
train() client id: f_00004-11-0 loss: 0.755807  [   32/  306]
train() client id: f_00004-11-1 loss: 0.632707  [   64/  306]
train() client id: f_00004-11-2 loss: 0.593678  [   96/  306]
train() client id: f_00004-11-3 loss: 0.674607  [  128/  306]
train() client id: f_00004-11-4 loss: 0.846928  [  160/  306]
train() client id: f_00004-11-5 loss: 0.642594  [  192/  306]
train() client id: f_00004-11-6 loss: 0.644346  [  224/  306]
train() client id: f_00004-11-7 loss: 0.670775  [  256/  306]
train() client id: f_00004-11-8 loss: 0.757203  [  288/  306]
train() client id: f_00005-0-0 loss: 0.515655  [   32/  146]
train() client id: f_00005-0-1 loss: 0.347372  [   64/  146]
train() client id: f_00005-0-2 loss: 0.413255  [   96/  146]
train() client id: f_00005-0-3 loss: 0.537003  [  128/  146]
train() client id: f_00005-1-0 loss: 0.291838  [   32/  146]
train() client id: f_00005-1-1 loss: 0.280174  [   64/  146]
train() client id: f_00005-1-2 loss: 0.626821  [   96/  146]
train() client id: f_00005-1-3 loss: 0.562270  [  128/  146]
train() client id: f_00005-2-0 loss: 0.380611  [   32/  146]
train() client id: f_00005-2-1 loss: 0.341358  [   64/  146]
train() client id: f_00005-2-2 loss: 0.382091  [   96/  146]
train() client id: f_00005-2-3 loss: 0.561002  [  128/  146]
train() client id: f_00005-3-0 loss: 0.539637  [   32/  146]
train() client id: f_00005-3-1 loss: 0.607686  [   64/  146]
train() client id: f_00005-3-2 loss: 0.519188  [   96/  146]
train() client id: f_00005-3-3 loss: 0.374418  [  128/  146]
train() client id: f_00005-4-0 loss: 0.463194  [   32/  146]
train() client id: f_00005-4-1 loss: 0.438518  [   64/  146]
train() client id: f_00005-4-2 loss: 0.415209  [   96/  146]
train() client id: f_00005-4-3 loss: 0.541016  [  128/  146]
train() client id: f_00005-5-0 loss: 0.639975  [   32/  146]
train() client id: f_00005-5-1 loss: 0.455911  [   64/  146]
train() client id: f_00005-5-2 loss: 0.643215  [   96/  146]
train() client id: f_00005-5-3 loss: 0.266650  [  128/  146]
train() client id: f_00005-6-0 loss: 0.643929  [   32/  146]
train() client id: f_00005-6-1 loss: 0.151207  [   64/  146]
train() client id: f_00005-6-2 loss: 0.614309  [   96/  146]
train() client id: f_00005-6-3 loss: 0.497423  [  128/  146]
train() client id: f_00005-7-0 loss: 0.543668  [   32/  146]
train() client id: f_00005-7-1 loss: 0.352488  [   64/  146]
train() client id: f_00005-7-2 loss: 0.696374  [   96/  146]
train() client id: f_00005-7-3 loss: 0.323981  [  128/  146]
train() client id: f_00005-8-0 loss: 0.358692  [   32/  146]
train() client id: f_00005-8-1 loss: 0.380846  [   64/  146]
train() client id: f_00005-8-2 loss: 0.500467  [   96/  146]
train() client id: f_00005-8-3 loss: 0.468835  [  128/  146]
train() client id: f_00005-9-0 loss: 0.373220  [   32/  146]
train() client id: f_00005-9-1 loss: 0.377806  [   64/  146]
train() client id: f_00005-9-2 loss: 0.515075  [   96/  146]
train() client id: f_00005-9-3 loss: 0.693029  [  128/  146]
train() client id: f_00005-10-0 loss: 0.553571  [   32/  146]
train() client id: f_00005-10-1 loss: 0.532789  [   64/  146]
train() client id: f_00005-10-2 loss: 0.491143  [   96/  146]
train() client id: f_00005-10-3 loss: 0.332883  [  128/  146]
train() client id: f_00005-11-0 loss: 0.493768  [   32/  146]
train() client id: f_00005-11-1 loss: 0.425739  [   64/  146]
train() client id: f_00005-11-2 loss: 0.394759  [   96/  146]
train() client id: f_00005-11-3 loss: 0.469221  [  128/  146]
train() client id: f_00006-0-0 loss: 0.598637  [   32/   54]
train() client id: f_00006-1-0 loss: 0.494055  [   32/   54]
train() client id: f_00006-2-0 loss: 0.542201  [   32/   54]
train() client id: f_00006-3-0 loss: 0.603859  [   32/   54]
train() client id: f_00006-4-0 loss: 0.605971  [   32/   54]
train() client id: f_00006-5-0 loss: 0.565339  [   32/   54]
train() client id: f_00006-6-0 loss: 0.525928  [   32/   54]
train() client id: f_00006-7-0 loss: 0.599041  [   32/   54]
train() client id: f_00006-8-0 loss: 0.528535  [   32/   54]
train() client id: f_00006-9-0 loss: 0.557222  [   32/   54]
train() client id: f_00006-10-0 loss: 0.595028  [   32/   54]
train() client id: f_00006-11-0 loss: 0.548330  [   32/   54]
train() client id: f_00007-0-0 loss: 0.650474  [   32/  179]
train() client id: f_00007-0-1 loss: 0.783357  [   64/  179]
train() client id: f_00007-0-2 loss: 0.835335  [   96/  179]
train() client id: f_00007-0-3 loss: 0.754380  [  128/  179]
train() client id: f_00007-0-4 loss: 0.604425  [  160/  179]
train() client id: f_00007-1-0 loss: 0.762433  [   32/  179]
train() client id: f_00007-1-1 loss: 0.730095  [   64/  179]
train() client id: f_00007-1-2 loss: 0.732437  [   96/  179]
train() client id: f_00007-1-3 loss: 0.601857  [  128/  179]
train() client id: f_00007-1-4 loss: 0.817854  [  160/  179]
train() client id: f_00007-2-0 loss: 0.737233  [   32/  179]
train() client id: f_00007-2-1 loss: 0.683297  [   64/  179]
train() client id: f_00007-2-2 loss: 0.748977  [   96/  179]
train() client id: f_00007-2-3 loss: 0.646864  [  128/  179]
train() client id: f_00007-2-4 loss: 0.722861  [  160/  179]
train() client id: f_00007-3-0 loss: 0.691955  [   32/  179]
train() client id: f_00007-3-1 loss: 0.933139  [   64/  179]
train() client id: f_00007-3-2 loss: 0.729370  [   96/  179]
train() client id: f_00007-3-3 loss: 0.596285  [  128/  179]
train() client id: f_00007-3-4 loss: 0.641786  [  160/  179]
train() client id: f_00007-4-0 loss: 0.715230  [   32/  179]
train() client id: f_00007-4-1 loss: 0.741923  [   64/  179]
train() client id: f_00007-4-2 loss: 0.748877  [   96/  179]
train() client id: f_00007-4-3 loss: 0.728282  [  128/  179]
train() client id: f_00007-4-4 loss: 0.556322  [  160/  179]
train() client id: f_00007-5-0 loss: 0.551856  [   32/  179]
train() client id: f_00007-5-1 loss: 0.535267  [   64/  179]
train() client id: f_00007-5-2 loss: 0.751137  [   96/  179]
train() client id: f_00007-5-3 loss: 0.971573  [  128/  179]
train() client id: f_00007-5-4 loss: 0.743715  [  160/  179]
train() client id: f_00007-6-0 loss: 0.854792  [   32/  179]
train() client id: f_00007-6-1 loss: 0.647107  [   64/  179]
train() client id: f_00007-6-2 loss: 0.755435  [   96/  179]
train() client id: f_00007-6-3 loss: 0.667531  [  128/  179]
train() client id: f_00007-6-4 loss: 0.624630  [  160/  179]
train() client id: f_00007-7-0 loss: 0.560842  [   32/  179]
train() client id: f_00007-7-1 loss: 0.656513  [   64/  179]
train() client id: f_00007-7-2 loss: 0.677929  [   96/  179]
train() client id: f_00007-7-3 loss: 0.802191  [  128/  179]
train() client id: f_00007-7-4 loss: 0.734863  [  160/  179]
train() client id: f_00007-8-0 loss: 0.530308  [   32/  179]
train() client id: f_00007-8-1 loss: 0.577686  [   64/  179]
train() client id: f_00007-8-2 loss: 0.803732  [   96/  179]
train() client id: f_00007-8-3 loss: 0.656102  [  128/  179]
train() client id: f_00007-8-4 loss: 0.703455  [  160/  179]
train() client id: f_00007-9-0 loss: 0.584017  [   32/  179]
train() client id: f_00007-9-1 loss: 0.615592  [   64/  179]
train() client id: f_00007-9-2 loss: 0.633189  [   96/  179]
train() client id: f_00007-9-3 loss: 0.516858  [  128/  179]
train() client id: f_00007-9-4 loss: 0.848238  [  160/  179]
train() client id: f_00007-10-0 loss: 0.727859  [   32/  179]
train() client id: f_00007-10-1 loss: 0.690011  [   64/  179]
train() client id: f_00007-10-2 loss: 0.715703  [   96/  179]
train() client id: f_00007-10-3 loss: 0.646715  [  128/  179]
train() client id: f_00007-10-4 loss: 0.647445  [  160/  179]
train() client id: f_00007-11-0 loss: 0.893233  [   32/  179]
train() client id: f_00007-11-1 loss: 0.558399  [   64/  179]
train() client id: f_00007-11-2 loss: 0.556830  [   96/  179]
train() client id: f_00007-11-3 loss: 0.621872  [  128/  179]
train() client id: f_00007-11-4 loss: 0.650482  [  160/  179]
train() client id: f_00008-0-0 loss: 0.558203  [   32/  130]
train() client id: f_00008-0-1 loss: 0.537168  [   64/  130]
train() client id: f_00008-0-2 loss: 0.604024  [   96/  130]
train() client id: f_00008-0-3 loss: 0.642996  [  128/  130]
train() client id: f_00008-1-0 loss: 0.649673  [   32/  130]
train() client id: f_00008-1-1 loss: 0.459100  [   64/  130]
train() client id: f_00008-1-2 loss: 0.678960  [   96/  130]
train() client id: f_00008-1-3 loss: 0.569105  [  128/  130]
train() client id: f_00008-2-0 loss: 0.527265  [   32/  130]
train() client id: f_00008-2-1 loss: 0.667724  [   64/  130]
train() client id: f_00008-2-2 loss: 0.574338  [   96/  130]
train() client id: f_00008-2-3 loss: 0.548571  [  128/  130]
train() client id: f_00008-3-0 loss: 0.637674  [   32/  130]
train() client id: f_00008-3-1 loss: 0.589345  [   64/  130]
train() client id: f_00008-3-2 loss: 0.589607  [   96/  130]
train() client id: f_00008-3-3 loss: 0.533866  [  128/  130]
train() client id: f_00008-4-0 loss: 0.605603  [   32/  130]
train() client id: f_00008-4-1 loss: 0.540449  [   64/  130]
train() client id: f_00008-4-2 loss: 0.657792  [   96/  130]
train() client id: f_00008-4-3 loss: 0.549699  [  128/  130]
train() client id: f_00008-5-0 loss: 0.613865  [   32/  130]
train() client id: f_00008-5-1 loss: 0.617978  [   64/  130]
train() client id: f_00008-5-2 loss: 0.587448  [   96/  130]
train() client id: f_00008-5-3 loss: 0.539321  [  128/  130]
train() client id: f_00008-6-0 loss: 0.455557  [   32/  130]
train() client id: f_00008-6-1 loss: 0.575340  [   64/  130]
train() client id: f_00008-6-2 loss: 0.578484  [   96/  130]
train() client id: f_00008-6-3 loss: 0.723188  [  128/  130]
train() client id: f_00008-7-0 loss: 0.594345  [   32/  130]
train() client id: f_00008-7-1 loss: 0.561136  [   64/  130]
train() client id: f_00008-7-2 loss: 0.515760  [   96/  130]
train() client id: f_00008-7-3 loss: 0.647493  [  128/  130]
train() client id: f_00008-8-0 loss: 0.532619  [   32/  130]
train() client id: f_00008-8-1 loss: 0.571876  [   64/  130]
train() client id: f_00008-8-2 loss: 0.635744  [   96/  130]
train() client id: f_00008-8-3 loss: 0.557933  [  128/  130]
train() client id: f_00008-9-0 loss: 0.572201  [   32/  130]
train() client id: f_00008-9-1 loss: 0.633848  [   64/  130]
train() client id: f_00008-9-2 loss: 0.608701  [   96/  130]
train() client id: f_00008-9-3 loss: 0.540074  [  128/  130]
train() client id: f_00008-10-0 loss: 0.628723  [   32/  130]
train() client id: f_00008-10-1 loss: 0.530790  [   64/  130]
train() client id: f_00008-10-2 loss: 0.596947  [   96/  130]
train() client id: f_00008-10-3 loss: 0.598726  [  128/  130]
train() client id: f_00008-11-0 loss: 0.652874  [   32/  130]
train() client id: f_00008-11-1 loss: 0.593110  [   64/  130]
train() client id: f_00008-11-2 loss: 0.552359  [   96/  130]
train() client id: f_00008-11-3 loss: 0.538303  [  128/  130]
train() client id: f_00009-0-0 loss: 1.083382  [   32/  118]
train() client id: f_00009-0-1 loss: 0.965668  [   64/  118]
train() client id: f_00009-0-2 loss: 1.106832  [   96/  118]
train() client id: f_00009-1-0 loss: 0.956325  [   32/  118]
train() client id: f_00009-1-1 loss: 1.090918  [   64/  118]
train() client id: f_00009-1-2 loss: 1.047721  [   96/  118]
train() client id: f_00009-2-0 loss: 0.987527  [   32/  118]
train() client id: f_00009-2-1 loss: 0.899858  [   64/  118]
train() client id: f_00009-2-2 loss: 0.907455  [   96/  118]
train() client id: f_00009-3-0 loss: 0.961279  [   32/  118]
train() client id: f_00009-3-1 loss: 0.948461  [   64/  118]
train() client id: f_00009-3-2 loss: 0.776405  [   96/  118]
train() client id: f_00009-4-0 loss: 0.967897  [   32/  118]
train() client id: f_00009-4-1 loss: 0.827040  [   64/  118]
train() client id: f_00009-4-2 loss: 0.826415  [   96/  118]
train() client id: f_00009-5-0 loss: 0.906081  [   32/  118]
train() client id: f_00009-5-1 loss: 0.865667  [   64/  118]
train() client id: f_00009-5-2 loss: 0.808587  [   96/  118]
train() client id: f_00009-6-0 loss: 0.952468  [   32/  118]
train() client id: f_00009-6-1 loss: 0.803446  [   64/  118]
train() client id: f_00009-6-2 loss: 0.771905  [   96/  118]
train() client id: f_00009-7-0 loss: 0.767638  [   32/  118]
train() client id: f_00009-7-1 loss: 0.864715  [   64/  118]
train() client id: f_00009-7-2 loss: 0.898825  [   96/  118]
train() client id: f_00009-8-0 loss: 0.849337  [   32/  118]
train() client id: f_00009-8-1 loss: 0.809463  [   64/  118]
train() client id: f_00009-8-2 loss: 0.654946  [   96/  118]
train() client id: f_00009-9-0 loss: 0.781629  [   32/  118]
train() client id: f_00009-9-1 loss: 0.904984  [   64/  118]
train() client id: f_00009-9-2 loss: 0.711904  [   96/  118]
train() client id: f_00009-10-0 loss: 0.791026  [   32/  118]
train() client id: f_00009-10-1 loss: 0.789507  [   64/  118]
train() client id: f_00009-10-2 loss: 0.773674  [   96/  118]
train() client id: f_00009-11-0 loss: 0.794635  [   32/  118]
train() client id: f_00009-11-1 loss: 0.861334  [   64/  118]
train() client id: f_00009-11-2 loss: 0.772002  [   96/  118]
At round 32 accuracy: 0.6525198938992043
At round 32 training accuracy: 0.5835010060362174
At round 32 training loss: 0.8211332843170693
update_location
xs = [  -3.9056584     4.20031788  180.00902392   18.81129433    0.97929623
    3.95640986 -142.44319194 -121.32485185  164.66397685 -107.06087855]
ys = [ 172.5879595   155.55583871    1.32061395 -142.45517586  134.35018685
  117.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [199.50402986 184.9736782  205.92472584 175.06382243 167.4841238
 154.58275225 174.06020071 157.22721117 193.44998125 146.55389307]
dists_bs = [171.11134289 180.25212713 395.21631571 371.94167082 180.61317784
 187.87128037 180.58399931 182.26046195 374.37432475 184.00059091]
uav_gains = [1.71760459e-11 2.11772319e-11 1.56382078e-11 2.44758737e-11
 2.74270048e-11 3.36063783e-11 2.48429772e-11 3.21986963e-11
 1.87438290e-11 3.84301675e-11]
bs_gains = [6.16756090e-11 5.33124903e-11 5.91774794e-12 7.01398027e-12
 5.30146221e-11 4.74771870e-11 5.30386105e-11 5.16838882e-11
 6.88711171e-12 5.03269111e-11]
Round 33
-------------------------------
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.63572969 13.74717363  6.53678508  2.35476176 15.85548586  7.6312378
  2.9198615   9.33889895  6.88901514  6.19002745]
obj_prev = 78.09897687730752
eta_min = 1.1396317579661002e-14	eta_max = 0.9276141009437577
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 18.129363196870486	eta = 0.9090909090909091
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 32.88895800241827	eta = 0.5011177085230375
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 25.66977986634642	eta = 0.642048329034932
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 24.365791295624447	eta = 0.6764089485097873
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 24.297762088268055	eta = 0.678302767555703
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 24.29756203224391	eta = 0.678308352418689
eta = 0.678308352418689
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [0.03202948 0.06736359 0.03152108 0.0109307  0.07778588 0.03711352
 0.01372692 0.0455022  0.03304629 0.02999586]
ene_total = [2.16353862 3.9073382  2.14884473 1.01590035 4.454601   2.32913348
 1.16104971 2.80262517 2.36340733 1.95112344]
ti_comp = [0.44826702 0.46925465 0.4459835  0.45600932 0.4691739  0.46754648
 0.45630733 0.4611897  0.42034182 0.46841538]
ti_coms = [0.09205736 0.07106973 0.09434088 0.08431506 0.07115048 0.0727779
 0.08401706 0.07913468 0.11998256 0.07190901]
t_total = [28.34986153 28.34986153 28.34986153 28.34986153 28.34986153 28.34986153
 28.34986153 28.34986153 28.34986153 28.34986153]
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [1.02201224e-05 8.67637833e-05 9.84114204e-06 3.92532807e-07
 1.33633277e-04 1.46159488e-05 7.76398040e-07 2.76833041e-05
 1.27656487e-05 7.68779460e-06]
ene_total = [0.48354488 0.3774426  0.49550619 0.44240675 0.38032544 0.3826196
 0.44086332 0.41665811 0.63019699 0.37769716]
optimize_network iter = 0 obj = 4.427261042788227
eta = 0.678308352418689
freqs = [35725893.75226643 71777219.31597894 35338835.42638909 11985166.67285503
 82896638.99614602 39689662.43089405 15041309.96971984 49331324.0982344
 39308827.86172418 32018442.92741502]
eta_min = 0.6783083524187157	eta_max = 0.6783083524186895
af = 0.013235761297397841	bf = 1.4190753947365375	zeta = 0.014559337427137626	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [2.50952835e-06 2.13046542e-05 2.41647056e-06 9.63855586e-08
 3.28133544e-05 3.58891376e-06 1.90642814e-07 6.79757383e-06
 3.13457667e-06 1.88772088e-06]
ene_total = [1.70964427 1.323468   1.75202375 1.56544914 1.327104   1.35189348
 1.55993376 1.47051205 2.22823206 1.33544537]
ti_comp = [0.44826702 0.46925465 0.4459835  0.45600932 0.4691739  0.46754648
 0.45630733 0.4611897  0.42034182 0.46841538]
ti_coms = [0.09205736 0.07106973 0.09434088 0.08431506 0.07115048 0.0727779
 0.08401706 0.07913468 0.11998256 0.07190901]
t_total = [28.34986153 28.34986153 28.34986153 28.34986153 28.34986153 28.34986153
 28.34986153 28.34986153 28.34986153 28.34986153]
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [1.02201224e-05 8.67637833e-05 9.84114204e-06 3.92532807e-07
 1.33633277e-04 1.46159488e-05 7.76398040e-07 2.76833041e-05
 1.27656487e-05 7.68779460e-06]
ene_total = [0.48354488 0.3774426  0.49550619 0.44240675 0.38032544 0.3826196
 0.44086332 0.41665811 0.63019699 0.37769716]
optimize_network iter = 1 obj = 4.427261042788233
eta = 0.6783083524186895
freqs = [35725893.75226644 71777219.31597894 35338835.4263891  11985166.67285503
 82896638.99614602 39689662.43089406 15041309.96971984 49331324.09823441
 39308827.86172419 32018442.92741502]
Done!
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [9.64912650e-06 8.19163105e-05 9.29131967e-06 3.70602088e-07
 1.26167217e-04 1.37993591e-05 7.33020858e-07 2.61366442e-05
 1.20524348e-05 7.25827927e-06]
ene_total = [0.00921539 0.00718889 0.00944338 0.00843188 0.00724122 0.00729159
 0.00840244 0.0079396  0.01201031 0.00719816]
At round 33 energy consumption: 0.08436284522778167
At round 33 eta: 0.6783083524186895
At round 33 a_n: 16.536044059297588
At round 33 local rounds: 12.71011101057834
At round 33 global rounds: 52.468225498458686
gradient difference: 0.36990517377853394
train() client id: f_00000-0-0 loss: 1.212463  [   32/  126]
train() client id: f_00000-0-1 loss: 1.237599  [   64/  126]
train() client id: f_00000-0-2 loss: 1.387924  [   96/  126]
train() client id: f_00000-1-0 loss: 1.100420  [   32/  126]
train() client id: f_00000-1-1 loss: 1.120708  [   64/  126]
train() client id: f_00000-1-2 loss: 1.182589  [   96/  126]
train() client id: f_00000-2-0 loss: 1.212521  [   32/  126]
train() client id: f_00000-2-1 loss: 0.936052  [   64/  126]
train() client id: f_00000-2-2 loss: 0.949785  [   96/  126]
train() client id: f_00000-3-0 loss: 0.907406  [   32/  126]
train() client id: f_00000-3-1 loss: 1.111283  [   64/  126]
train() client id: f_00000-3-2 loss: 0.914132  [   96/  126]
train() client id: f_00000-4-0 loss: 1.079439  [   32/  126]
train() client id: f_00000-4-1 loss: 0.954508  [   64/  126]
train() client id: f_00000-4-2 loss: 0.852966  [   96/  126]
train() client id: f_00000-5-0 loss: 0.977690  [   32/  126]
train() client id: f_00000-5-1 loss: 0.941726  [   64/  126]
train() client id: f_00000-5-2 loss: 0.847567  [   96/  126]
train() client id: f_00000-6-0 loss: 0.896109  [   32/  126]
train() client id: f_00000-6-1 loss: 0.822621  [   64/  126]
train() client id: f_00000-6-2 loss: 0.914641  [   96/  126]
train() client id: f_00000-7-0 loss: 0.739350  [   32/  126]
train() client id: f_00000-7-1 loss: 0.921439  [   64/  126]
train() client id: f_00000-7-2 loss: 0.968462  [   96/  126]
train() client id: f_00000-8-0 loss: 0.855937  [   32/  126]
train() client id: f_00000-8-1 loss: 0.873474  [   64/  126]
train() client id: f_00000-8-2 loss: 0.947567  [   96/  126]
train() client id: f_00000-9-0 loss: 0.794255  [   32/  126]
train() client id: f_00000-9-1 loss: 0.920788  [   64/  126]
train() client id: f_00000-9-2 loss: 0.818587  [   96/  126]
train() client id: f_00000-10-0 loss: 0.738436  [   32/  126]
train() client id: f_00000-10-1 loss: 0.842189  [   64/  126]
train() client id: f_00000-10-2 loss: 0.896745  [   96/  126]
train() client id: f_00000-11-0 loss: 0.886131  [   32/  126]
train() client id: f_00000-11-1 loss: 0.932539  [   64/  126]
train() client id: f_00000-11-2 loss: 0.830089  [   96/  126]
train() client id: f_00001-0-0 loss: 0.350091  [   32/  265]
train() client id: f_00001-0-1 loss: 0.482945  [   64/  265]
train() client id: f_00001-0-2 loss: 0.361046  [   96/  265]
train() client id: f_00001-0-3 loss: 0.501134  [  128/  265]
train() client id: f_00001-0-4 loss: 0.362955  [  160/  265]
train() client id: f_00001-0-5 loss: 0.514175  [  192/  265]
train() client id: f_00001-0-6 loss: 0.412012  [  224/  265]
train() client id: f_00001-0-7 loss: 0.455357  [  256/  265]
train() client id: f_00001-1-0 loss: 0.449390  [   32/  265]
train() client id: f_00001-1-1 loss: 0.430459  [   64/  265]
train() client id: f_00001-1-2 loss: 0.426366  [   96/  265]
train() client id: f_00001-1-3 loss: 0.419269  [  128/  265]
train() client id: f_00001-1-4 loss: 0.474759  [  160/  265]
train() client id: f_00001-1-5 loss: 0.402479  [  192/  265]
train() client id: f_00001-1-6 loss: 0.403014  [  224/  265]
train() client id: f_00001-1-7 loss: 0.344749  [  256/  265]
train() client id: f_00001-2-0 loss: 0.377668  [   32/  265]
train() client id: f_00001-2-1 loss: 0.472741  [   64/  265]
train() client id: f_00001-2-2 loss: 0.457040  [   96/  265]
train() client id: f_00001-2-3 loss: 0.320319  [  128/  265]
train() client id: f_00001-2-4 loss: 0.443590  [  160/  265]
train() client id: f_00001-2-5 loss: 0.352953  [  192/  265]
train() client id: f_00001-2-6 loss: 0.397198  [  224/  265]
train() client id: f_00001-2-7 loss: 0.428081  [  256/  265]
train() client id: f_00001-3-0 loss: 0.452177  [   32/  265]
train() client id: f_00001-3-1 loss: 0.322636  [   64/  265]
train() client id: f_00001-3-2 loss: 0.325799  [   96/  265]
train() client id: f_00001-3-3 loss: 0.450218  [  128/  265]
train() client id: f_00001-3-4 loss: 0.416789  [  160/  265]
train() client id: f_00001-3-5 loss: 0.488270  [  192/  265]
train() client id: f_00001-3-6 loss: 0.375542  [  224/  265]
train() client id: f_00001-3-7 loss: 0.330798  [  256/  265]
train() client id: f_00001-4-0 loss: 0.313523  [   32/  265]
train() client id: f_00001-4-1 loss: 0.408136  [   64/  265]
train() client id: f_00001-4-2 loss: 0.360974  [   96/  265]
train() client id: f_00001-4-3 loss: 0.368180  [  128/  265]
train() client id: f_00001-4-4 loss: 0.295392  [  160/  265]
train() client id: f_00001-4-5 loss: 0.362433  [  192/  265]
train() client id: f_00001-4-6 loss: 0.594369  [  224/  265]
train() client id: f_00001-4-7 loss: 0.493424  [  256/  265]
train() client id: f_00001-5-0 loss: 0.447539  [   32/  265]
train() client id: f_00001-5-1 loss: 0.325803  [   64/  265]
train() client id: f_00001-5-2 loss: 0.469171  [   96/  265]
train() client id: f_00001-5-3 loss: 0.483458  [  128/  265]
train() client id: f_00001-5-4 loss: 0.386382  [  160/  265]
train() client id: f_00001-5-5 loss: 0.332027  [  192/  265]
train() client id: f_00001-5-6 loss: 0.343047  [  224/  265]
train() client id: f_00001-5-7 loss: 0.378647  [  256/  265]
train() client id: f_00001-6-0 loss: 0.462527  [   32/  265]
train() client id: f_00001-6-1 loss: 0.380762  [   64/  265]
train() client id: f_00001-6-2 loss: 0.404946  [   96/  265]
train() client id: f_00001-6-3 loss: 0.306867  [  128/  265]
train() client id: f_00001-6-4 loss: 0.381409  [  160/  265]
train() client id: f_00001-6-5 loss: 0.443048  [  192/  265]
train() client id: f_00001-6-6 loss: 0.296676  [  224/  265]
train() client id: f_00001-6-7 loss: 0.461864  [  256/  265]
train() client id: f_00001-7-0 loss: 0.535210  [   32/  265]
train() client id: f_00001-7-1 loss: 0.357845  [   64/  265]
train() client id: f_00001-7-2 loss: 0.425567  [   96/  265]
train() client id: f_00001-7-3 loss: 0.312678  [  128/  265]
train() client id: f_00001-7-4 loss: 0.372667  [  160/  265]
train() client id: f_00001-7-5 loss: 0.314880  [  192/  265]
train() client id: f_00001-7-6 loss: 0.416646  [  224/  265]
train() client id: f_00001-7-7 loss: 0.384204  [  256/  265]
train() client id: f_00001-8-0 loss: 0.398722  [   32/  265]
train() client id: f_00001-8-1 loss: 0.415719  [   64/  265]
train() client id: f_00001-8-2 loss: 0.365370  [   96/  265]
train() client id: f_00001-8-3 loss: 0.352860  [  128/  265]
train() client id: f_00001-8-4 loss: 0.338956  [  160/  265]
train() client id: f_00001-8-5 loss: 0.435566  [  192/  265]
train() client id: f_00001-8-6 loss: 0.379114  [  224/  265]
train() client id: f_00001-8-7 loss: 0.408794  [  256/  265]
train() client id: f_00001-9-0 loss: 0.360575  [   32/  265]
train() client id: f_00001-9-1 loss: 0.458944  [   64/  265]
train() client id: f_00001-9-2 loss: 0.437989  [   96/  265]
train() client id: f_00001-9-3 loss: 0.401161  [  128/  265]
train() client id: f_00001-9-4 loss: 0.295809  [  160/  265]
train() client id: f_00001-9-5 loss: 0.387695  [  192/  265]
train() client id: f_00001-9-6 loss: 0.399292  [  224/  265]
train() client id: f_00001-9-7 loss: 0.341243  [  256/  265]
train() client id: f_00001-10-0 loss: 0.425589  [   32/  265]
train() client id: f_00001-10-1 loss: 0.430014  [   64/  265]
train() client id: f_00001-10-2 loss: 0.342159  [   96/  265]
train() client id: f_00001-10-3 loss: 0.449457  [  128/  265]
train() client id: f_00001-10-4 loss: 0.361632  [  160/  265]
train() client id: f_00001-10-5 loss: 0.358250  [  192/  265]
train() client id: f_00001-10-6 loss: 0.304398  [  224/  265]
train() client id: f_00001-10-7 loss: 0.410235  [  256/  265]
train() client id: f_00001-11-0 loss: 0.405944  [   32/  265]
train() client id: f_00001-11-1 loss: 0.274169  [   64/  265]
train() client id: f_00001-11-2 loss: 0.416057  [   96/  265]
train() client id: f_00001-11-3 loss: 0.294438  [  128/  265]
train() client id: f_00001-11-4 loss: 0.385752  [  160/  265]
train() client id: f_00001-11-5 loss: 0.357307  [  192/  265]
train() client id: f_00001-11-6 loss: 0.465709  [  224/  265]
train() client id: f_00001-11-7 loss: 0.466416  [  256/  265]
train() client id: f_00002-0-0 loss: 1.238410  [   32/  124]
train() client id: f_00002-0-1 loss: 1.186271  [   64/  124]
train() client id: f_00002-0-2 loss: 1.516358  [   96/  124]
train() client id: f_00002-1-0 loss: 1.161854  [   32/  124]
train() client id: f_00002-1-1 loss: 1.404953  [   64/  124]
train() client id: f_00002-1-2 loss: 1.235469  [   96/  124]
train() client id: f_00002-2-0 loss: 1.462306  [   32/  124]
train() client id: f_00002-2-1 loss: 1.170315  [   64/  124]
train() client id: f_00002-2-2 loss: 1.138136  [   96/  124]
train() client id: f_00002-3-0 loss: 1.259621  [   32/  124]
train() client id: f_00002-3-1 loss: 1.169201  [   64/  124]
train() client id: f_00002-3-2 loss: 1.225656  [   96/  124]
train() client id: f_00002-4-0 loss: 1.274621  [   32/  124]
train() client id: f_00002-4-1 loss: 1.057459  [   64/  124]
train() client id: f_00002-4-2 loss: 1.225852  [   96/  124]
train() client id: f_00002-5-0 loss: 1.234626  [   32/  124]
train() client id: f_00002-5-1 loss: 0.963287  [   64/  124]
train() client id: f_00002-5-2 loss: 1.124954  [   96/  124]
train() client id: f_00002-6-0 loss: 1.115814  [   32/  124]
train() client id: f_00002-6-1 loss: 1.188134  [   64/  124]
train() client id: f_00002-6-2 loss: 1.045401  [   96/  124]
train() client id: f_00002-7-0 loss: 1.088053  [   32/  124]
train() client id: f_00002-7-1 loss: 1.155880  [   64/  124]
train() client id: f_00002-7-2 loss: 1.051017  [   96/  124]
train() client id: f_00002-8-0 loss: 1.360273  [   32/  124]
train() client id: f_00002-8-1 loss: 0.915000  [   64/  124]
train() client id: f_00002-8-2 loss: 1.076825  [   96/  124]
train() client id: f_00002-9-0 loss: 1.092733  [   32/  124]
train() client id: f_00002-9-1 loss: 0.996641  [   64/  124]
train() client id: f_00002-9-2 loss: 1.159220  [   96/  124]
train() client id: f_00002-10-0 loss: 1.013032  [   32/  124]
train() client id: f_00002-10-1 loss: 1.132145  [   64/  124]
train() client id: f_00002-10-2 loss: 1.140882  [   96/  124]
train() client id: f_00002-11-0 loss: 0.975648  [   32/  124]
train() client id: f_00002-11-1 loss: 1.193319  [   64/  124]
train() client id: f_00002-11-2 loss: 0.995322  [   96/  124]
train() client id: f_00003-0-0 loss: 0.770209  [   32/   43]
train() client id: f_00003-1-0 loss: 0.856865  [   32/   43]
train() client id: f_00003-2-0 loss: 0.927054  [   32/   43]
train() client id: f_00003-3-0 loss: 0.899520  [   32/   43]
train() client id: f_00003-4-0 loss: 0.823227  [   32/   43]
train() client id: f_00003-5-0 loss: 0.898048  [   32/   43]
train() client id: f_00003-6-0 loss: 0.709211  [   32/   43]
train() client id: f_00003-7-0 loss: 0.691131  [   32/   43]
train() client id: f_00003-8-0 loss: 0.848198  [   32/   43]
train() client id: f_00003-9-0 loss: 0.795361  [   32/   43]
train() client id: f_00003-10-0 loss: 0.818718  [   32/   43]
train() client id: f_00003-11-0 loss: 0.744997  [   32/   43]
train() client id: f_00004-0-0 loss: 0.730697  [   32/  306]
train() client id: f_00004-0-1 loss: 0.813697  [   64/  306]
train() client id: f_00004-0-2 loss: 0.891736  [   96/  306]
train() client id: f_00004-0-3 loss: 0.839951  [  128/  306]
train() client id: f_00004-0-4 loss: 0.766629  [  160/  306]
train() client id: f_00004-0-5 loss: 0.835190  [  192/  306]
train() client id: f_00004-0-6 loss: 0.792562  [  224/  306]
train() client id: f_00004-0-7 loss: 0.830121  [  256/  306]
train() client id: f_00004-0-8 loss: 0.739369  [  288/  306]
train() client id: f_00004-1-0 loss: 0.846736  [   32/  306]
train() client id: f_00004-1-1 loss: 0.813357  [   64/  306]
train() client id: f_00004-1-2 loss: 0.862899  [   96/  306]
train() client id: f_00004-1-3 loss: 0.777127  [  128/  306]
train() client id: f_00004-1-4 loss: 0.646791  [  160/  306]
train() client id: f_00004-1-5 loss: 0.682004  [  192/  306]
train() client id: f_00004-1-6 loss: 0.966492  [  224/  306]
train() client id: f_00004-1-7 loss: 0.789465  [  256/  306]
train() client id: f_00004-1-8 loss: 0.860929  [  288/  306]
train() client id: f_00004-2-0 loss: 0.783575  [   32/  306]
train() client id: f_00004-2-1 loss: 0.825994  [   64/  306]
train() client id: f_00004-2-2 loss: 0.874348  [   96/  306]
train() client id: f_00004-2-3 loss: 0.753959  [  128/  306]
train() client id: f_00004-2-4 loss: 0.701432  [  160/  306]
train() client id: f_00004-2-5 loss: 0.789884  [  192/  306]
train() client id: f_00004-2-6 loss: 0.824470  [  224/  306]
train() client id: f_00004-2-7 loss: 0.853664  [  256/  306]
train() client id: f_00004-2-8 loss: 0.814717  [  288/  306]
train() client id: f_00004-3-0 loss: 0.898791  [   32/  306]
train() client id: f_00004-3-1 loss: 0.803349  [   64/  306]
train() client id: f_00004-3-2 loss: 0.777696  [   96/  306]
train() client id: f_00004-3-3 loss: 0.853171  [  128/  306]
train() client id: f_00004-3-4 loss: 0.793132  [  160/  306]
train() client id: f_00004-3-5 loss: 0.822858  [  192/  306]
train() client id: f_00004-3-6 loss: 0.840809  [  224/  306]
train() client id: f_00004-3-7 loss: 0.743793  [  256/  306]
train() client id: f_00004-3-8 loss: 0.710125  [  288/  306]
train() client id: f_00004-4-0 loss: 0.752717  [   32/  306]
train() client id: f_00004-4-1 loss: 0.743486  [   64/  306]
train() client id: f_00004-4-2 loss: 0.838094  [   96/  306]
train() client id: f_00004-4-3 loss: 0.795627  [  128/  306]
train() client id: f_00004-4-4 loss: 0.849296  [  160/  306]
train() client id: f_00004-4-5 loss: 0.856563  [  192/  306]
train() client id: f_00004-4-6 loss: 0.794655  [  224/  306]
train() client id: f_00004-4-7 loss: 0.861289  [  256/  306]
train() client id: f_00004-4-8 loss: 0.808776  [  288/  306]
train() client id: f_00004-5-0 loss: 0.750830  [   32/  306]
train() client id: f_00004-5-1 loss: 0.687260  [   64/  306]
train() client id: f_00004-5-2 loss: 0.888007  [   96/  306]
train() client id: f_00004-5-3 loss: 0.838196  [  128/  306]
train() client id: f_00004-5-4 loss: 0.752970  [  160/  306]
train() client id: f_00004-5-5 loss: 0.849212  [  192/  306]
train() client id: f_00004-5-6 loss: 0.803407  [  224/  306]
train() client id: f_00004-5-7 loss: 0.901233  [  256/  306]
train() client id: f_00004-5-8 loss: 0.792298  [  288/  306]
train() client id: f_00004-6-0 loss: 0.792975  [   32/  306]
train() client id: f_00004-6-1 loss: 0.887766  [   64/  306]
train() client id: f_00004-6-2 loss: 0.781912  [   96/  306]
train() client id: f_00004-6-3 loss: 0.804040  [  128/  306]
train() client id: f_00004-6-4 loss: 0.671794  [  160/  306]
train() client id: f_00004-6-5 loss: 0.781376  [  192/  306]
train() client id: f_00004-6-6 loss: 0.867999  [  224/  306]
train() client id: f_00004-6-7 loss: 0.750913  [  256/  306]
train() client id: f_00004-6-8 loss: 0.842074  [  288/  306]
train() client id: f_00004-7-0 loss: 0.940745  [   32/  306]
train() client id: f_00004-7-1 loss: 0.719816  [   64/  306]
train() client id: f_00004-7-2 loss: 0.791299  [   96/  306]
train() client id: f_00004-7-3 loss: 0.811340  [  128/  306]
train() client id: f_00004-7-4 loss: 0.892478  [  160/  306]
train() client id: f_00004-7-5 loss: 0.851167  [  192/  306]
train() client id: f_00004-7-6 loss: 0.747913  [  224/  306]
train() client id: f_00004-7-7 loss: 0.707657  [  256/  306]
train() client id: f_00004-7-8 loss: 0.777558  [  288/  306]
train() client id: f_00004-8-0 loss: 0.814256  [   32/  306]
train() client id: f_00004-8-1 loss: 0.792022  [   64/  306]
train() client id: f_00004-8-2 loss: 0.822648  [   96/  306]
train() client id: f_00004-8-3 loss: 0.848215  [  128/  306]
train() client id: f_00004-8-4 loss: 0.710665  [  160/  306]
train() client id: f_00004-8-5 loss: 0.858691  [  192/  306]
train() client id: f_00004-8-6 loss: 0.755476  [  224/  306]
train() client id: f_00004-8-7 loss: 0.813826  [  256/  306]
train() client id: f_00004-8-8 loss: 0.899664  [  288/  306]
train() client id: f_00004-9-0 loss: 0.751257  [   32/  306]
train() client id: f_00004-9-1 loss: 0.750252  [   64/  306]
train() client id: f_00004-9-2 loss: 0.818330  [   96/  306]
train() client id: f_00004-9-3 loss: 0.969697  [  128/  306]
train() client id: f_00004-9-4 loss: 0.883191  [  160/  306]
train() client id: f_00004-9-5 loss: 0.813122  [  192/  306]
train() client id: f_00004-9-6 loss: 0.718792  [  224/  306]
train() client id: f_00004-9-7 loss: 0.785905  [  256/  306]
train() client id: f_00004-9-8 loss: 0.787570  [  288/  306]
train() client id: f_00004-10-0 loss: 0.808099  [   32/  306]
train() client id: f_00004-10-1 loss: 0.785939  [   64/  306]
train() client id: f_00004-10-2 loss: 0.871755  [   96/  306]
train() client id: f_00004-10-3 loss: 0.891020  [  128/  306]
train() client id: f_00004-10-4 loss: 0.813788  [  160/  306]
train() client id: f_00004-10-5 loss: 0.649847  [  192/  306]
train() client id: f_00004-10-6 loss: 0.853444  [  224/  306]
train() client id: f_00004-10-7 loss: 0.799855  [  256/  306]
train() client id: f_00004-10-8 loss: 0.756211  [  288/  306]
train() client id: f_00004-11-0 loss: 0.857698  [   32/  306]
train() client id: f_00004-11-1 loss: 0.885756  [   64/  306]
train() client id: f_00004-11-2 loss: 0.869586  [   96/  306]
train() client id: f_00004-11-3 loss: 0.758708  [  128/  306]
train() client id: f_00004-11-4 loss: 0.848715  [  160/  306]
train() client id: f_00004-11-5 loss: 0.732745  [  192/  306]
train() client id: f_00004-11-6 loss: 0.836475  [  224/  306]
train() client id: f_00004-11-7 loss: 0.860250  [  256/  306]
train() client id: f_00004-11-8 loss: 0.738923  [  288/  306]
train() client id: f_00005-0-0 loss: 0.585554  [   32/  146]
train() client id: f_00005-0-1 loss: 0.699289  [   64/  146]
train() client id: f_00005-0-2 loss: 0.896276  [   96/  146]
train() client id: f_00005-0-3 loss: 0.505523  [  128/  146]
train() client id: f_00005-1-0 loss: 0.680007  [   32/  146]
train() client id: f_00005-1-1 loss: 0.576102  [   64/  146]
train() client id: f_00005-1-2 loss: 0.549830  [   96/  146]
train() client id: f_00005-1-3 loss: 0.689009  [  128/  146]
train() client id: f_00005-2-0 loss: 0.667524  [   32/  146]
train() client id: f_00005-2-1 loss: 0.489618  [   64/  146]
train() client id: f_00005-2-2 loss: 0.502777  [   96/  146]
train() client id: f_00005-2-3 loss: 0.919891  [  128/  146]
train() client id: f_00005-3-0 loss: 0.511550  [   32/  146]
train() client id: f_00005-3-1 loss: 0.497154  [   64/  146]
train() client id: f_00005-3-2 loss: 0.422697  [   96/  146]
train() client id: f_00005-3-3 loss: 0.827169  [  128/  146]
train() client id: f_00005-4-0 loss: 0.749631  [   32/  146]
train() client id: f_00005-4-1 loss: 0.558135  [   64/  146]
train() client id: f_00005-4-2 loss: 0.664857  [   96/  146]
train() client id: f_00005-4-3 loss: 0.592680  [  128/  146]
train() client id: f_00005-5-0 loss: 0.992980  [   32/  146]
train() client id: f_00005-5-1 loss: 0.600459  [   64/  146]
train() client id: f_00005-5-2 loss: 0.559493  [   96/  146]
train() client id: f_00005-5-3 loss: 0.397671  [  128/  146]
train() client id: f_00005-6-0 loss: 0.585463  [   32/  146]
train() client id: f_00005-6-1 loss: 0.746653  [   64/  146]
train() client id: f_00005-6-2 loss: 0.479264  [   96/  146]
train() client id: f_00005-6-3 loss: 0.551204  [  128/  146]
train() client id: f_00005-7-0 loss: 0.747964  [   32/  146]
train() client id: f_00005-7-1 loss: 0.624075  [   64/  146]
train() client id: f_00005-7-2 loss: 0.459351  [   96/  146]
train() client id: f_00005-7-3 loss: 0.865542  [  128/  146]
train() client id: f_00005-8-0 loss: 0.601256  [   32/  146]
train() client id: f_00005-8-1 loss: 0.620574  [   64/  146]
train() client id: f_00005-8-2 loss: 0.658673  [   96/  146]
train() client id: f_00005-8-3 loss: 0.604725  [  128/  146]
train() client id: f_00005-9-0 loss: 0.708901  [   32/  146]
train() client id: f_00005-9-1 loss: 0.475794  [   64/  146]
train() client id: f_00005-9-2 loss: 0.716110  [   96/  146]
train() client id: f_00005-9-3 loss: 0.421850  [  128/  146]
train() client id: f_00005-10-0 loss: 0.542466  [   32/  146]
train() client id: f_00005-10-1 loss: 0.371706  [   64/  146]
train() client id: f_00005-10-2 loss: 0.423656  [   96/  146]
train() client id: f_00005-10-3 loss: 1.059032  [  128/  146]
train() client id: f_00005-11-0 loss: 0.746649  [   32/  146]
train() client id: f_00005-11-1 loss: 0.386427  [   64/  146]
train() client id: f_00005-11-2 loss: 0.564838  [   96/  146]
train() client id: f_00005-11-3 loss: 0.874384  [  128/  146]
train() client id: f_00006-0-0 loss: 0.540695  [   32/   54]
train() client id: f_00006-1-0 loss: 0.576705  [   32/   54]
train() client id: f_00006-2-0 loss: 0.483521  [   32/   54]
train() client id: f_00006-3-0 loss: 0.554292  [   32/   54]
train() client id: f_00006-4-0 loss: 0.611317  [   32/   54]
train() client id: f_00006-5-0 loss: 0.541572  [   32/   54]
train() client id: f_00006-6-0 loss: 0.590743  [   32/   54]
train() client id: f_00006-7-0 loss: 0.590449  [   32/   54]
train() client id: f_00006-8-0 loss: 0.477914  [   32/   54]
train() client id: f_00006-9-0 loss: 0.553084  [   32/   54]
train() client id: f_00006-10-0 loss: 0.501708  [   32/   54]
train() client id: f_00006-11-0 loss: 0.564787  [   32/   54]
train() client id: f_00007-0-0 loss: 0.578193  [   32/  179]
train() client id: f_00007-0-1 loss: 0.604968  [   64/  179]
train() client id: f_00007-0-2 loss: 0.648730  [   96/  179]
train() client id: f_00007-0-3 loss: 0.555325  [  128/  179]
train() client id: f_00007-0-4 loss: 0.672183  [  160/  179]
train() client id: f_00007-1-0 loss: 0.628512  [   32/  179]
train() client id: f_00007-1-1 loss: 0.547611  [   64/  179]
train() client id: f_00007-1-2 loss: 0.668426  [   96/  179]
train() client id: f_00007-1-3 loss: 0.682842  [  128/  179]
train() client id: f_00007-1-4 loss: 0.511676  [  160/  179]
train() client id: f_00007-2-0 loss: 0.629111  [   32/  179]
train() client id: f_00007-2-1 loss: 0.664490  [   64/  179]
train() client id: f_00007-2-2 loss: 0.556747  [   96/  179]
train() client id: f_00007-2-3 loss: 0.568532  [  128/  179]
train() client id: f_00007-2-4 loss: 0.594565  [  160/  179]
train() client id: f_00007-3-0 loss: 0.548162  [   32/  179]
train() client id: f_00007-3-1 loss: 0.610708  [   64/  179]
train() client id: f_00007-3-2 loss: 0.495146  [   96/  179]
train() client id: f_00007-3-3 loss: 0.555520  [  128/  179]
train() client id: f_00007-3-4 loss: 0.744739  [  160/  179]
train() client id: f_00007-4-0 loss: 0.531428  [   32/  179]
train() client id: f_00007-4-1 loss: 0.559729  [   64/  179]
train() client id: f_00007-4-2 loss: 0.644401  [   96/  179]
train() client id: f_00007-4-3 loss: 0.664438  [  128/  179]
train() client id: f_00007-4-4 loss: 0.609848  [  160/  179]
train() client id: f_00007-5-0 loss: 0.625992  [   32/  179]
train() client id: f_00007-5-1 loss: 0.642741  [   64/  179]
train() client id: f_00007-5-2 loss: 0.601979  [   96/  179]
train() client id: f_00007-5-3 loss: 0.414450  [  128/  179]
train() client id: f_00007-5-4 loss: 0.597741  [  160/  179]
train() client id: f_00007-6-0 loss: 0.732665  [   32/  179]
train() client id: f_00007-6-1 loss: 0.526120  [   64/  179]
train() client id: f_00007-6-2 loss: 0.548148  [   96/  179]
train() client id: f_00007-6-3 loss: 0.434872  [  128/  179]
train() client id: f_00007-6-4 loss: 0.634306  [  160/  179]
train() client id: f_00007-7-0 loss: 0.549130  [   32/  179]
train() client id: f_00007-7-1 loss: 0.693422  [   64/  179]
train() client id: f_00007-7-2 loss: 0.531823  [   96/  179]
train() client id: f_00007-7-3 loss: 0.577137  [  128/  179]
train() client id: f_00007-7-4 loss: 0.528847  [  160/  179]
train() client id: f_00007-8-0 loss: 0.769565  [   32/  179]
train() client id: f_00007-8-1 loss: 0.671105  [   64/  179]
train() client id: f_00007-8-2 loss: 0.514945  [   96/  179]
train() client id: f_00007-8-3 loss: 0.439204  [  128/  179]
train() client id: f_00007-8-4 loss: 0.545725  [  160/  179]
train() client id: f_00007-9-0 loss: 0.746294  [   32/  179]
train() client id: f_00007-9-1 loss: 0.572599  [   64/  179]
train() client id: f_00007-9-2 loss: 0.491837  [   96/  179]
train() client id: f_00007-9-3 loss: 0.459182  [  128/  179]
train() client id: f_00007-9-4 loss: 0.555807  [  160/  179]
train() client id: f_00007-10-0 loss: 0.616544  [   32/  179]
train() client id: f_00007-10-1 loss: 0.562794  [   64/  179]
train() client id: f_00007-10-2 loss: 0.519481  [   96/  179]
train() client id: f_00007-10-3 loss: 0.422192  [  128/  179]
train() client id: f_00007-10-4 loss: 0.677028  [  160/  179]
train() client id: f_00007-11-0 loss: 0.535084  [   32/  179]
train() client id: f_00007-11-1 loss: 0.491345  [   64/  179]
train() client id: f_00007-11-2 loss: 0.627376  [   96/  179]
train() client id: f_00007-11-3 loss: 0.448577  [  128/  179]
train() client id: f_00007-11-4 loss: 0.553414  [  160/  179]
train() client id: f_00008-0-0 loss: 0.715307  [   32/  130]
train() client id: f_00008-0-1 loss: 0.858021  [   64/  130]
train() client id: f_00008-0-2 loss: 0.825934  [   96/  130]
train() client id: f_00008-0-3 loss: 0.740394  [  128/  130]
train() client id: f_00008-1-0 loss: 0.785780  [   32/  130]
train() client id: f_00008-1-1 loss: 0.729627  [   64/  130]
train() client id: f_00008-1-2 loss: 0.908958  [   96/  130]
train() client id: f_00008-1-3 loss: 0.711429  [  128/  130]
train() client id: f_00008-2-0 loss: 0.850358  [   32/  130]
train() client id: f_00008-2-1 loss: 0.757187  [   64/  130]
train() client id: f_00008-2-2 loss: 0.778657  [   96/  130]
train() client id: f_00008-2-3 loss: 0.745282  [  128/  130]
train() client id: f_00008-3-0 loss: 0.747005  [   32/  130]
train() client id: f_00008-3-1 loss: 0.882486  [   64/  130]
train() client id: f_00008-3-2 loss: 0.835495  [   96/  130]
train() client id: f_00008-3-3 loss: 0.666748  [  128/  130]
train() client id: f_00008-4-0 loss: 0.748319  [   32/  130]
train() client id: f_00008-4-1 loss: 0.760394  [   64/  130]
train() client id: f_00008-4-2 loss: 0.845359  [   96/  130]
train() client id: f_00008-4-3 loss: 0.765015  [  128/  130]
train() client id: f_00008-5-0 loss: 0.674099  [   32/  130]
train() client id: f_00008-5-1 loss: 0.850245  [   64/  130]
train() client id: f_00008-5-2 loss: 0.692051  [   96/  130]
train() client id: f_00008-5-3 loss: 0.914717  [  128/  130]
train() client id: f_00008-6-0 loss: 0.798735  [   32/  130]
train() client id: f_00008-6-1 loss: 0.830953  [   64/  130]
train() client id: f_00008-6-2 loss: 0.637733  [   96/  130]
train() client id: f_00008-6-3 loss: 0.846225  [  128/  130]
train() client id: f_00008-7-0 loss: 0.678176  [   32/  130]
train() client id: f_00008-7-1 loss: 0.721442  [   64/  130]
train() client id: f_00008-7-2 loss: 0.805377  [   96/  130]
train() client id: f_00008-7-3 loss: 0.916910  [  128/  130]
train() client id: f_00008-8-0 loss: 0.739517  [   32/  130]
train() client id: f_00008-8-1 loss: 0.809651  [   64/  130]
train() client id: f_00008-8-2 loss: 0.812570  [   96/  130]
train() client id: f_00008-8-3 loss: 0.732802  [  128/  130]
train() client id: f_00008-9-0 loss: 0.815077  [   32/  130]
train() client id: f_00008-9-1 loss: 0.795995  [   64/  130]
train() client id: f_00008-9-2 loss: 0.819227  [   96/  130]
train() client id: f_00008-9-3 loss: 0.648828  [  128/  130]
train() client id: f_00008-10-0 loss: 0.677836  [   32/  130]
train() client id: f_00008-10-1 loss: 0.789930  [   64/  130]
train() client id: f_00008-10-2 loss: 0.911522  [   96/  130]
train() client id: f_00008-10-3 loss: 0.747176  [  128/  130]
train() client id: f_00008-11-0 loss: 0.804995  [   32/  130]
train() client id: f_00008-11-1 loss: 0.791626  [   64/  130]
train() client id: f_00008-11-2 loss: 0.831463  [   96/  130]
train() client id: f_00008-11-3 loss: 0.681418  [  128/  130]
train() client id: f_00009-0-0 loss: 1.158732  [   32/  118]
train() client id: f_00009-0-1 loss: 1.098441  [   64/  118]
train() client id: f_00009-0-2 loss: 1.278485  [   96/  118]
train() client id: f_00009-1-0 loss: 1.120435  [   32/  118]
train() client id: f_00009-1-1 loss: 1.120506  [   64/  118]
train() client id: f_00009-1-2 loss: 0.961406  [   96/  118]
train() client id: f_00009-2-0 loss: 0.962771  [   32/  118]
train() client id: f_00009-2-1 loss: 1.017011  [   64/  118]
train() client id: f_00009-2-2 loss: 0.996644  [   96/  118]
train() client id: f_00009-3-0 loss: 1.086308  [   32/  118]
train() client id: f_00009-3-1 loss: 0.866438  [   64/  118]
train() client id: f_00009-3-2 loss: 1.026170  [   96/  118]
train() client id: f_00009-4-0 loss: 0.807922  [   32/  118]
train() client id: f_00009-4-1 loss: 0.923614  [   64/  118]
train() client id: f_00009-4-2 loss: 1.024478  [   96/  118]
train() client id: f_00009-5-0 loss: 0.918546  [   32/  118]
train() client id: f_00009-5-1 loss: 0.904175  [   64/  118]
train() client id: f_00009-5-2 loss: 0.891403  [   96/  118]
train() client id: f_00009-6-0 loss: 0.968817  [   32/  118]
train() client id: f_00009-6-1 loss: 0.972379  [   64/  118]
train() client id: f_00009-6-2 loss: 0.841817  [   96/  118]
train() client id: f_00009-7-0 loss: 0.802571  [   32/  118]
train() client id: f_00009-7-1 loss: 1.024033  [   64/  118]
train() client id: f_00009-7-2 loss: 0.814901  [   96/  118]
train() client id: f_00009-8-0 loss: 0.962832  [   32/  118]
train() client id: f_00009-8-1 loss: 0.859799  [   64/  118]
train() client id: f_00009-8-2 loss: 0.828053  [   96/  118]
train() client id: f_00009-9-0 loss: 0.800298  [   32/  118]
train() client id: f_00009-9-1 loss: 0.759219  [   64/  118]
train() client id: f_00009-9-2 loss: 1.046592  [   96/  118]
train() client id: f_00009-10-0 loss: 0.856710  [   32/  118]
train() client id: f_00009-10-1 loss: 0.791942  [   64/  118]
train() client id: f_00009-10-2 loss: 0.848289  [   96/  118]
train() client id: f_00009-11-0 loss: 0.764567  [   32/  118]
train() client id: f_00009-11-1 loss: 1.064154  [   64/  118]
train() client id: f_00009-11-2 loss: 0.695899  [   96/  118]
At round 33 accuracy: 0.6525198938992043
At round 33 training accuracy: 0.5821596244131455
At round 33 training loss: 0.8392742312067099
update_location
xs = [  -3.9056584     4.20031788  185.00902392   18.81129433    0.97929623
    3.95640986 -147.44319194 -126.32485185  169.66397685 -112.06087855]
ys = [ 177.5879595   160.55583871    1.32061395 -147.45517586  139.35018685
  122.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [203.84488595 189.197833   210.30949325 179.15605957 171.52094215
 158.42654071 178.17515369 161.11686582 197.72338004 150.24530728]
dists_bs = [171.11391311 179.78149994 399.71368058 376.21823221 179.55395256
 186.4101915  179.74986154 180.85111144 378.91658027 182.21368292]
uav_gains = [1.61237068e-11 1.99260114e-11 1.46504448e-11 2.30455348e-11
 2.58027311e-11 3.15864432e-11 2.33790442e-11 3.02686694e-11
 1.76243014e-11 3.61021161e-11]
bs_gains = [6.16730151e-11 5.37041796e-11 5.73319680e-12 6.79301444e-12
 5.38949620e-11 4.85265100e-11 5.37306513e-11 5.28195595e-11
 6.65843217e-12 5.17210489e-11]
Round 34
-------------------------------
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.50361085 13.46800316  6.40688418  2.3090212  15.53330799  7.47581585
  2.86265615  9.15122779  6.75143071  6.06373403]
obj_prev = 76.52569191747648
eta_min = 6.016067092016877e-15	eta_max = 0.9281867977986356
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 17.761433286506318	eta = 0.9090909090909091
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 32.349008668159414	eta = 0.4991422673511644
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 25.200112188715238	eta = 0.6407414940167677
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 23.908304317469916	eta = 0.6753618876010812
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 23.840631284811383	eta = 0.6772789419999331
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 23.84043047624284	eta = 0.6772846467381501
eta = 0.6772846467381501
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [0.03215411 0.06762571 0.03164373 0.01097323 0.07808855 0.03725794
 0.01378033 0.04567925 0.03317488 0.03011258]
ene_total = [2.12711913 3.82878864 2.11331163 1.00058971 4.36464773 2.28034368
 1.14291118 2.75181183 2.3215154  1.90939155]
ti_comp = [0.45871166 0.48133195 0.45630249 0.4667549  0.48138282 0.47984682
 0.46705067 0.47204883 0.43098118 0.48078779]
ti_coms = [0.09358479 0.0709645  0.09599397 0.08554155 0.07091363 0.07244963
 0.08524578 0.08024762 0.12131527 0.07150866]
t_total = [28.29985733 28.29985733 28.29985733 28.29985733 28.29985733 28.29985733
 28.29985733 28.29985733 28.29985733 28.29985733]
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [9.87438211e-06 8.34307382e-05 9.51124209e-06 3.79057801e-07
 1.28428366e-04 1.40388386e-05 7.49775295e-07 2.67339474e-05
 1.22854563e-05 7.38273045e-06]
ene_total = [0.48003762 0.36789952 0.49236369 0.43833721 0.36994456 0.37195374
 0.43684068 0.41256138 0.62225307 0.3667911 ]
optimize_network iter = 0 obj = 4.3589825705706815
eta = 0.6772846467381501
freqs = [35048280.46691417 70248512.05463123 34674068.18486581 11754807.32564252
 81108578.19233961 38822740.19627412 14752502.84041418 48384031.59575192
 38487614.47521663 31315873.21898438]
eta_min = 0.677284646738166	eta_max = 0.6772846467381503
af = 0.012426086302136423	bf = 1.4018686060877306	zeta = 0.013668694932350066	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [2.41523466e-06 2.04068274e-05 2.32641206e-06 9.27160332e-08
 3.14130684e-05 3.43384418e-06 1.83392061e-07 6.53901742e-06
 3.00497385e-06 1.80578656e-06]
ene_total = [1.70271381 1.29453122 1.74651965 1.55598755 1.29560796 1.31845798
 1.55062412 1.46086543 2.20722871 1.30104589]
ti_comp = [0.45871166 0.48133195 0.45630249 0.4667549  0.48138282 0.47984682
 0.46705067 0.47204883 0.43098118 0.48078779]
ti_coms = [0.09358479 0.0709645  0.09599397 0.08554155 0.07091363 0.07244963
 0.08524578 0.08024762 0.12131527 0.07150866]
t_total = [28.29985733 28.29985733 28.29985733 28.29985733 28.29985733 28.29985733
 28.29985733 28.29985733 28.29985733 28.29985733]
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [9.87438211e-06 8.34307382e-05 9.51124209e-06 3.79057801e-07
 1.28428366e-04 1.40388386e-05 7.49775295e-07 2.67339474e-05
 1.22854563e-05 7.38273045e-06]
ene_total = [0.48003762 0.36789952 0.49236369 0.43833721 0.36994456 0.37195374
 0.43684068 0.41256138 0.62225307 0.3667911 ]
optimize_network iter = 1 obj = 4.358982570570683
eta = 0.6772846467381503
freqs = [35048280.46691417 70248512.05463123 34674068.18486582 11754807.32564252
 81108578.19233961 38822740.19627412 14752502.84041418 48384031.59575192
 38487614.47521663 31315873.21898438]
Done!
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [9.28656765e-06 7.84641698e-05 8.94504507e-06 3.56492778e-07
 1.20783123e-04 1.32031172e-05 7.05141742e-07 2.51424959e-05
 1.15541125e-05 6.94324212e-06]
ene_total = [0.00936777 0.00717491 0.00960834 0.00855451 0.00721215 0.00725817
 0.00852528 0.0080499  0.01214308 0.00715781]
At round 34 energy consumption: 0.0850519238371433
At round 34 eta: 0.6772846467381503
At round 34 a_n: 16.193498212328272
At round 34 local rounds: 12.759567346547202
At round 34 global rounds: 51.24033886878731
gradient difference: 0.4620048999786377
train() client id: f_00000-0-0 loss: 1.370587  [   32/  126]
train() client id: f_00000-0-1 loss: 1.203779  [   64/  126]
train() client id: f_00000-0-2 loss: 1.001823  [   96/  126]
train() client id: f_00000-1-0 loss: 1.187329  [   32/  126]
train() client id: f_00000-1-1 loss: 1.056426  [   64/  126]
train() client id: f_00000-1-2 loss: 1.098763  [   96/  126]
train() client id: f_00000-2-0 loss: 1.008609  [   32/  126]
train() client id: f_00000-2-1 loss: 1.038702  [   64/  126]
train() client id: f_00000-2-2 loss: 0.946483  [   96/  126]
train() client id: f_00000-3-0 loss: 0.919306  [   32/  126]
train() client id: f_00000-3-1 loss: 0.946179  [   64/  126]
train() client id: f_00000-3-2 loss: 1.022303  [   96/  126]
train() client id: f_00000-4-0 loss: 1.025955  [   32/  126]
train() client id: f_00000-4-1 loss: 0.843510  [   64/  126]
train() client id: f_00000-4-2 loss: 0.938809  [   96/  126]
train() client id: f_00000-5-0 loss: 0.899333  [   32/  126]
train() client id: f_00000-5-1 loss: 0.890777  [   64/  126]
train() client id: f_00000-5-2 loss: 0.976853  [   96/  126]
train() client id: f_00000-6-0 loss: 0.976204  [   32/  126]
train() client id: f_00000-6-1 loss: 0.927542  [   64/  126]
train() client id: f_00000-6-2 loss: 0.888382  [   96/  126]
train() client id: f_00000-7-0 loss: 0.856462  [   32/  126]
train() client id: f_00000-7-1 loss: 0.859236  [   64/  126]
train() client id: f_00000-7-2 loss: 0.910548  [   96/  126]
train() client id: f_00000-8-0 loss: 0.867822  [   32/  126]
train() client id: f_00000-8-1 loss: 0.818233  [   64/  126]
train() client id: f_00000-8-2 loss: 0.828867  [   96/  126]
train() client id: f_00000-9-0 loss: 0.980779  [   32/  126]
train() client id: f_00000-9-1 loss: 0.756990  [   64/  126]
train() client id: f_00000-9-2 loss: 0.895156  [   96/  126]
train() client id: f_00000-10-0 loss: 0.905650  [   32/  126]
train() client id: f_00000-10-1 loss: 0.817855  [   64/  126]
train() client id: f_00000-10-2 loss: 0.835023  [   96/  126]
train() client id: f_00000-11-0 loss: 0.869841  [   32/  126]
train() client id: f_00000-11-1 loss: 0.873277  [   64/  126]
train() client id: f_00000-11-2 loss: 0.921731  [   96/  126]
train() client id: f_00001-0-0 loss: 0.392909  [   32/  265]
train() client id: f_00001-0-1 loss: 0.337313  [   64/  265]
train() client id: f_00001-0-2 loss: 0.297294  [   96/  265]
train() client id: f_00001-0-3 loss: 0.302431  [  128/  265]
train() client id: f_00001-0-4 loss: 0.532264  [  160/  265]
train() client id: f_00001-0-5 loss: 0.463782  [  192/  265]
train() client id: f_00001-0-6 loss: 0.297218  [  224/  265]
train() client id: f_00001-0-7 loss: 0.468952  [  256/  265]
train() client id: f_00001-1-0 loss: 0.388754  [   32/  265]
train() client id: f_00001-1-1 loss: 0.412828  [   64/  265]
train() client id: f_00001-1-2 loss: 0.444587  [   96/  265]
train() client id: f_00001-1-3 loss: 0.316213  [  128/  265]
train() client id: f_00001-1-4 loss: 0.314733  [  160/  265]
train() client id: f_00001-1-5 loss: 0.496290  [  192/  265]
train() client id: f_00001-1-6 loss: 0.350499  [  224/  265]
train() client id: f_00001-1-7 loss: 0.294689  [  256/  265]
train() client id: f_00001-2-0 loss: 0.347333  [   32/  265]
train() client id: f_00001-2-1 loss: 0.304292  [   64/  265]
train() client id: f_00001-2-2 loss: 0.427195  [   96/  265]
train() client id: f_00001-2-3 loss: 0.397923  [  128/  265]
train() client id: f_00001-2-4 loss: 0.279125  [  160/  265]
train() client id: f_00001-2-5 loss: 0.459214  [  192/  265]
train() client id: f_00001-2-6 loss: 0.397905  [  224/  265]
train() client id: f_00001-2-7 loss: 0.357544  [  256/  265]
train() client id: f_00001-3-0 loss: 0.274980  [   32/  265]
train() client id: f_00001-3-1 loss: 0.396726  [   64/  265]
train() client id: f_00001-3-2 loss: 0.277623  [   96/  265]
train() client id: f_00001-3-3 loss: 0.333457  [  128/  265]
train() client id: f_00001-3-4 loss: 0.340716  [  160/  265]
train() client id: f_00001-3-5 loss: 0.588231  [  192/  265]
train() client id: f_00001-3-6 loss: 0.342539  [  224/  265]
train() client id: f_00001-3-7 loss: 0.382232  [  256/  265]
train() client id: f_00001-4-0 loss: 0.426003  [   32/  265]
train() client id: f_00001-4-1 loss: 0.374790  [   64/  265]
train() client id: f_00001-4-2 loss: 0.508329  [   96/  265]
train() client id: f_00001-4-3 loss: 0.282240  [  128/  265]
train() client id: f_00001-4-4 loss: 0.277293  [  160/  265]
train() client id: f_00001-4-5 loss: 0.293523  [  192/  265]
train() client id: f_00001-4-6 loss: 0.319594  [  224/  265]
train() client id: f_00001-4-7 loss: 0.425982  [  256/  265]
train() client id: f_00001-5-0 loss: 0.318344  [   32/  265]
train() client id: f_00001-5-1 loss: 0.237564  [   64/  265]
train() client id: f_00001-5-2 loss: 0.409073  [   96/  265]
train() client id: f_00001-5-3 loss: 0.359022  [  128/  265]
train() client id: f_00001-5-4 loss: 0.257198  [  160/  265]
train() client id: f_00001-5-5 loss: 0.395407  [  192/  265]
train() client id: f_00001-5-6 loss: 0.359719  [  224/  265]
train() client id: f_00001-5-7 loss: 0.419188  [  256/  265]
train() client id: f_00001-6-0 loss: 0.331458  [   32/  265]
train() client id: f_00001-6-1 loss: 0.409525  [   64/  265]
train() client id: f_00001-6-2 loss: 0.373501  [   96/  265]
train() client id: f_00001-6-3 loss: 0.274368  [  128/  265]
train() client id: f_00001-6-4 loss: 0.409052  [  160/  265]
train() client id: f_00001-6-5 loss: 0.343528  [  192/  265]
train() client id: f_00001-6-6 loss: 0.351531  [  224/  265]
train() client id: f_00001-6-7 loss: 0.368574  [  256/  265]
train() client id: f_00001-7-0 loss: 0.275799  [   32/  265]
train() client id: f_00001-7-1 loss: 0.329145  [   64/  265]
train() client id: f_00001-7-2 loss: 0.257935  [   96/  265]
train() client id: f_00001-7-3 loss: 0.408779  [  128/  265]
train() client id: f_00001-7-4 loss: 0.427327  [  160/  265]
train() client id: f_00001-7-5 loss: 0.379950  [  192/  265]
train() client id: f_00001-7-6 loss: 0.323808  [  224/  265]
train() client id: f_00001-7-7 loss: 0.401147  [  256/  265]
train() client id: f_00001-8-0 loss: 0.401645  [   32/  265]
train() client id: f_00001-8-1 loss: 0.320334  [   64/  265]
train() client id: f_00001-8-2 loss: 0.307840  [   96/  265]
train() client id: f_00001-8-3 loss: 0.434866  [  128/  265]
train() client id: f_00001-8-4 loss: 0.320092  [  160/  265]
train() client id: f_00001-8-5 loss: 0.260640  [  192/  265]
train() client id: f_00001-8-6 loss: 0.428136  [  224/  265]
train() client id: f_00001-8-7 loss: 0.369270  [  256/  265]
train() client id: f_00001-9-0 loss: 0.265297  [   32/  265]
train() client id: f_00001-9-1 loss: 0.370725  [   64/  265]
train() client id: f_00001-9-2 loss: 0.395289  [   96/  265]
train() client id: f_00001-9-3 loss: 0.258508  [  128/  265]
train() client id: f_00001-9-4 loss: 0.363714  [  160/  265]
train() client id: f_00001-9-5 loss: 0.388907  [  192/  265]
train() client id: f_00001-9-6 loss: 0.385106  [  224/  265]
train() client id: f_00001-9-7 loss: 0.306273  [  256/  265]
train() client id: f_00001-10-0 loss: 0.270588  [   32/  265]
train() client id: f_00001-10-1 loss: 0.408884  [   64/  265]
train() client id: f_00001-10-2 loss: 0.249280  [   96/  265]
train() client id: f_00001-10-3 loss: 0.397748  [  128/  265]
train() client id: f_00001-10-4 loss: 0.349674  [  160/  265]
train() client id: f_00001-10-5 loss: 0.385263  [  192/  265]
train() client id: f_00001-10-6 loss: 0.367499  [  224/  265]
train() client id: f_00001-10-7 loss: 0.392306  [  256/  265]
train() client id: f_00001-11-0 loss: 0.441697  [   32/  265]
train() client id: f_00001-11-1 loss: 0.241117  [   64/  265]
train() client id: f_00001-11-2 loss: 0.325028  [   96/  265]
train() client id: f_00001-11-3 loss: 0.343303  [  128/  265]
train() client id: f_00001-11-4 loss: 0.359861  [  160/  265]
train() client id: f_00001-11-5 loss: 0.402653  [  192/  265]
train() client id: f_00001-11-6 loss: 0.392514  [  224/  265]
train() client id: f_00001-11-7 loss: 0.318312  [  256/  265]
train() client id: f_00002-0-0 loss: 1.298195  [   32/  124]
train() client id: f_00002-0-1 loss: 1.195744  [   64/  124]
train() client id: f_00002-0-2 loss: 1.274338  [   96/  124]
train() client id: f_00002-1-0 loss: 1.231760  [   32/  124]
train() client id: f_00002-1-1 loss: 1.089335  [   64/  124]
train() client id: f_00002-1-2 loss: 1.209497  [   96/  124]
train() client id: f_00002-2-0 loss: 1.165424  [   32/  124]
train() client id: f_00002-2-1 loss: 1.253582  [   64/  124]
train() client id: f_00002-2-2 loss: 1.156748  [   96/  124]
train() client id: f_00002-3-0 loss: 1.162375  [   32/  124]
train() client id: f_00002-3-1 loss: 1.255589  [   64/  124]
train() client id: f_00002-3-2 loss: 0.992487  [   96/  124]
train() client id: f_00002-4-0 loss: 1.068200  [   32/  124]
train() client id: f_00002-4-1 loss: 1.247744  [   64/  124]
train() client id: f_00002-4-2 loss: 0.963296  [   96/  124]
train() client id: f_00002-5-0 loss: 1.050350  [   32/  124]
train() client id: f_00002-5-1 loss: 1.121911  [   64/  124]
train() client id: f_00002-5-2 loss: 1.143487  [   96/  124]
train() client id: f_00002-6-0 loss: 1.152462  [   32/  124]
train() client id: f_00002-6-1 loss: 1.087577  [   64/  124]
train() client id: f_00002-6-2 loss: 1.134642  [   96/  124]
train() client id: f_00002-7-0 loss: 1.133951  [   32/  124]
train() client id: f_00002-7-1 loss: 1.095078  [   64/  124]
train() client id: f_00002-7-2 loss: 0.928635  [   96/  124]
train() client id: f_00002-8-0 loss: 1.118734  [   32/  124]
train() client id: f_00002-8-1 loss: 0.934515  [   64/  124]
train() client id: f_00002-8-2 loss: 1.177297  [   96/  124]
train() client id: f_00002-9-0 loss: 1.029985  [   32/  124]
train() client id: f_00002-9-1 loss: 0.887300  [   64/  124]
train() client id: f_00002-9-2 loss: 1.234173  [   96/  124]
train() client id: f_00002-10-0 loss: 1.079778  [   32/  124]
train() client id: f_00002-10-1 loss: 1.056934  [   64/  124]
train() client id: f_00002-10-2 loss: 1.080085  [   96/  124]
train() client id: f_00002-11-0 loss: 1.056267  [   32/  124]
train() client id: f_00002-11-1 loss: 1.173938  [   64/  124]
train() client id: f_00002-11-2 loss: 1.047834  [   96/  124]
train() client id: f_00003-0-0 loss: 0.676755  [   32/   43]
train() client id: f_00003-1-0 loss: 0.794042  [   32/   43]
train() client id: f_00003-2-0 loss: 0.757992  [   32/   43]
train() client id: f_00003-3-0 loss: 0.538543  [   32/   43]
train() client id: f_00003-4-0 loss: 0.843167  [   32/   43]
train() client id: f_00003-5-0 loss: 0.690465  [   32/   43]
train() client id: f_00003-6-0 loss: 0.888224  [   32/   43]
train() client id: f_00003-7-0 loss: 0.674368  [   32/   43]
train() client id: f_00003-8-0 loss: 0.838513  [   32/   43]
train() client id: f_00003-9-0 loss: 0.796003  [   32/   43]
train() client id: f_00003-10-0 loss: 0.833311  [   32/   43]
train() client id: f_00003-11-0 loss: 0.680658  [   32/   43]
train() client id: f_00004-0-0 loss: 0.991892  [   32/  306]
train() client id: f_00004-0-1 loss: 0.854302  [   64/  306]
train() client id: f_00004-0-2 loss: 0.763822  [   96/  306]
train() client id: f_00004-0-3 loss: 0.932874  [  128/  306]
train() client id: f_00004-0-4 loss: 0.932176  [  160/  306]
train() client id: f_00004-0-5 loss: 1.154073  [  192/  306]
train() client id: f_00004-0-6 loss: 0.876895  [  224/  306]
train() client id: f_00004-0-7 loss: 0.816461  [  256/  306]
train() client id: f_00004-0-8 loss: 1.062983  [  288/  306]
train() client id: f_00004-1-0 loss: 1.025564  [   32/  306]
train() client id: f_00004-1-1 loss: 0.825990  [   64/  306]
train() client id: f_00004-1-2 loss: 0.787586  [   96/  306]
train() client id: f_00004-1-3 loss: 0.885073  [  128/  306]
train() client id: f_00004-1-4 loss: 0.882626  [  160/  306]
train() client id: f_00004-1-5 loss: 0.980619  [  192/  306]
train() client id: f_00004-1-6 loss: 0.958252  [  224/  306]
train() client id: f_00004-1-7 loss: 0.971944  [  256/  306]
train() client id: f_00004-1-8 loss: 0.984786  [  288/  306]
train() client id: f_00004-2-0 loss: 0.978858  [   32/  306]
train() client id: f_00004-2-1 loss: 0.915408  [   64/  306]
train() client id: f_00004-2-2 loss: 0.879045  [   96/  306]
train() client id: f_00004-2-3 loss: 0.834406  [  128/  306]
train() client id: f_00004-2-4 loss: 0.947181  [  160/  306]
train() client id: f_00004-2-5 loss: 1.018035  [  192/  306]
train() client id: f_00004-2-6 loss: 0.905346  [  224/  306]
train() client id: f_00004-2-7 loss: 0.943294  [  256/  306]
train() client id: f_00004-2-8 loss: 0.821280  [  288/  306]
train() client id: f_00004-3-0 loss: 0.935632  [   32/  306]
train() client id: f_00004-3-1 loss: 0.850428  [   64/  306]
train() client id: f_00004-3-2 loss: 0.999621  [   96/  306]
train() client id: f_00004-3-3 loss: 0.904554  [  128/  306]
train() client id: f_00004-3-4 loss: 0.721570  [  160/  306]
train() client id: f_00004-3-5 loss: 0.970371  [  192/  306]
train() client id: f_00004-3-6 loss: 0.896798  [  224/  306]
train() client id: f_00004-3-7 loss: 0.908701  [  256/  306]
train() client id: f_00004-3-8 loss: 1.036330  [  288/  306]
train() client id: f_00004-4-0 loss: 0.929502  [   32/  306]
train() client id: f_00004-4-1 loss: 0.818325  [   64/  306]
train() client id: f_00004-4-2 loss: 0.884253  [   96/  306]
train() client id: f_00004-4-3 loss: 0.880341  [  128/  306]
train() client id: f_00004-4-4 loss: 0.859419  [  160/  306]
train() client id: f_00004-4-5 loss: 0.893854  [  192/  306]
train() client id: f_00004-4-6 loss: 1.100934  [  224/  306]
train() client id: f_00004-4-7 loss: 0.954614  [  256/  306]
train() client id: f_00004-4-8 loss: 0.889894  [  288/  306]
train() client id: f_00004-5-0 loss: 1.038186  [   32/  306]
train() client id: f_00004-5-1 loss: 0.767921  [   64/  306]
train() client id: f_00004-5-2 loss: 0.860913  [   96/  306]
train() client id: f_00004-5-3 loss: 0.904952  [  128/  306]
train() client id: f_00004-5-4 loss: 0.898455  [  160/  306]
train() client id: f_00004-5-5 loss: 1.026273  [  192/  306]
train() client id: f_00004-5-6 loss: 0.725164  [  224/  306]
train() client id: f_00004-5-7 loss: 1.009296  [  256/  306]
train() client id: f_00004-5-8 loss: 0.880239  [  288/  306]
train() client id: f_00004-6-0 loss: 0.900894  [   32/  306]
train() client id: f_00004-6-1 loss: 0.922049  [   64/  306]
train() client id: f_00004-6-2 loss: 0.854482  [   96/  306]
train() client id: f_00004-6-3 loss: 0.916086  [  128/  306]
train() client id: f_00004-6-4 loss: 0.936370  [  160/  306]
train() client id: f_00004-6-5 loss: 0.885251  [  192/  306]
train() client id: f_00004-6-6 loss: 0.869466  [  224/  306]
train() client id: f_00004-6-7 loss: 0.865182  [  256/  306]
train() client id: f_00004-6-8 loss: 0.991822  [  288/  306]
train() client id: f_00004-7-0 loss: 0.963209  [   32/  306]
train() client id: f_00004-7-1 loss: 0.903466  [   64/  306]
train() client id: f_00004-7-2 loss: 0.985566  [   96/  306]
train() client id: f_00004-7-3 loss: 0.864722  [  128/  306]
train() client id: f_00004-7-4 loss: 0.821080  [  160/  306]
train() client id: f_00004-7-5 loss: 0.936586  [  192/  306]
train() client id: f_00004-7-6 loss: 0.796057  [  224/  306]
train() client id: f_00004-7-7 loss: 0.841682  [  256/  306]
train() client id: f_00004-7-8 loss: 0.995256  [  288/  306]
train() client id: f_00004-8-0 loss: 0.938854  [   32/  306]
train() client id: f_00004-8-1 loss: 0.768580  [   64/  306]
train() client id: f_00004-8-2 loss: 0.948546  [   96/  306]
train() client id: f_00004-8-3 loss: 0.901008  [  128/  306]
train() client id: f_00004-8-4 loss: 0.905071  [  160/  306]
train() client id: f_00004-8-5 loss: 0.804256  [  192/  306]
train() client id: f_00004-8-6 loss: 0.835468  [  224/  306]
train() client id: f_00004-8-7 loss: 1.075730  [  256/  306]
train() client id: f_00004-8-8 loss: 0.954976  [  288/  306]
train() client id: f_00004-9-0 loss: 0.848767  [   32/  306]
train() client id: f_00004-9-1 loss: 0.933514  [   64/  306]
train() client id: f_00004-9-2 loss: 0.916711  [   96/  306]
train() client id: f_00004-9-3 loss: 0.842951  [  128/  306]
train() client id: f_00004-9-4 loss: 0.878425  [  160/  306]
train() client id: f_00004-9-5 loss: 0.907527  [  192/  306]
train() client id: f_00004-9-6 loss: 0.885494  [  224/  306]
train() client id: f_00004-9-7 loss: 0.930961  [  256/  306]
train() client id: f_00004-9-8 loss: 0.952248  [  288/  306]
train() client id: f_00004-10-0 loss: 0.889102  [   32/  306]
train() client id: f_00004-10-1 loss: 0.840718  [   64/  306]
train() client id: f_00004-10-2 loss: 0.832401  [   96/  306]
train() client id: f_00004-10-3 loss: 1.055573  [  128/  306]
train() client id: f_00004-10-4 loss: 0.893685  [  160/  306]
train() client id: f_00004-10-5 loss: 0.859503  [  192/  306]
train() client id: f_00004-10-6 loss: 0.793193  [  224/  306]
train() client id: f_00004-10-7 loss: 0.868784  [  256/  306]
train() client id: f_00004-10-8 loss: 1.041009  [  288/  306]
train() client id: f_00004-11-0 loss: 1.008266  [   32/  306]
train() client id: f_00004-11-1 loss: 0.754486  [   64/  306]
train() client id: f_00004-11-2 loss: 0.941519  [   96/  306]
train() client id: f_00004-11-3 loss: 0.767743  [  128/  306]
train() client id: f_00004-11-4 loss: 0.948010  [  160/  306]
train() client id: f_00004-11-5 loss: 0.834788  [  192/  306]
train() client id: f_00004-11-6 loss: 0.956899  [  224/  306]
train() client id: f_00004-11-7 loss: 0.884926  [  256/  306]
train() client id: f_00004-11-8 loss: 0.849992  [  288/  306]
train() client id: f_00005-0-0 loss: 0.317945  [   32/  146]
train() client id: f_00005-0-1 loss: 0.417060  [   64/  146]
train() client id: f_00005-0-2 loss: 0.588964  [   96/  146]
train() client id: f_00005-0-3 loss: 0.449757  [  128/  146]
train() client id: f_00005-1-0 loss: 0.390056  [   32/  146]
train() client id: f_00005-1-1 loss: 0.557850  [   64/  146]
train() client id: f_00005-1-2 loss: 0.659060  [   96/  146]
train() client id: f_00005-1-3 loss: 0.255112  [  128/  146]
train() client id: f_00005-2-0 loss: 0.631092  [   32/  146]
train() client id: f_00005-2-1 loss: 0.380509  [   64/  146]
train() client id: f_00005-2-2 loss: 0.462712  [   96/  146]
train() client id: f_00005-2-3 loss: 0.313212  [  128/  146]
train() client id: f_00005-3-0 loss: 0.587309  [   32/  146]
train() client id: f_00005-3-1 loss: 0.286822  [   64/  146]
train() client id: f_00005-3-2 loss: 0.508773  [   96/  146]
train() client id: f_00005-3-3 loss: 0.312495  [  128/  146]
train() client id: f_00005-4-0 loss: 0.435384  [   32/  146]
train() client id: f_00005-4-1 loss: 0.290385  [   64/  146]
train() client id: f_00005-4-2 loss: 0.558243  [   96/  146]
train() client id: f_00005-4-3 loss: 0.433218  [  128/  146]
train() client id: f_00005-5-0 loss: 0.476278  [   32/  146]
train() client id: f_00005-5-1 loss: 0.504027  [   64/  146]
train() client id: f_00005-5-2 loss: 0.421895  [   96/  146]
train() client id: f_00005-5-3 loss: 0.148266  [  128/  146]
train() client id: f_00005-6-0 loss: 0.641720  [   32/  146]
train() client id: f_00005-6-1 loss: 0.519965  [   64/  146]
train() client id: f_00005-6-2 loss: 0.220384  [   96/  146]
train() client id: f_00005-6-3 loss: 0.338180  [  128/  146]
train() client id: f_00005-7-0 loss: 0.342667  [   32/  146]
train() client id: f_00005-7-1 loss: 0.493292  [   64/  146]
train() client id: f_00005-7-2 loss: 0.355371  [   96/  146]
train() client id: f_00005-7-3 loss: 0.510151  [  128/  146]
train() client id: f_00005-8-0 loss: 0.558511  [   32/  146]
train() client id: f_00005-8-1 loss: 0.494047  [   64/  146]
train() client id: f_00005-8-2 loss: 0.312875  [   96/  146]
train() client id: f_00005-8-3 loss: 0.415153  [  128/  146]
train() client id: f_00005-9-0 loss: 0.460393  [   32/  146]
train() client id: f_00005-9-1 loss: 0.265575  [   64/  146]
train() client id: f_00005-9-2 loss: 0.708716  [   96/  146]
train() client id: f_00005-9-3 loss: 0.223664  [  128/  146]
train() client id: f_00005-10-0 loss: 0.561819  [   32/  146]
train() client id: f_00005-10-1 loss: 0.518880  [   64/  146]
train() client id: f_00005-10-2 loss: 0.139382  [   96/  146]
train() client id: f_00005-10-3 loss: 0.646479  [  128/  146]
train() client id: f_00005-11-0 loss: 0.270178  [   32/  146]
train() client id: f_00005-11-1 loss: 0.550352  [   64/  146]
train() client id: f_00005-11-2 loss: 0.375599  [   96/  146]
train() client id: f_00005-11-3 loss: 0.428629  [  128/  146]
train() client id: f_00006-0-0 loss: 0.566601  [   32/   54]
train() client id: f_00006-1-0 loss: 0.506935  [   32/   54]
train() client id: f_00006-2-0 loss: 0.525520  [   32/   54]
train() client id: f_00006-3-0 loss: 0.497244  [   32/   54]
train() client id: f_00006-4-0 loss: 0.491474  [   32/   54]
train() client id: f_00006-5-0 loss: 0.521355  [   32/   54]
train() client id: f_00006-6-0 loss: 0.514663  [   32/   54]
train() client id: f_00006-7-0 loss: 0.565959  [   32/   54]
train() client id: f_00006-8-0 loss: 0.516156  [   32/   54]
train() client id: f_00006-9-0 loss: 0.552696  [   32/   54]
train() client id: f_00006-10-0 loss: 0.515965  [   32/   54]
train() client id: f_00006-11-0 loss: 0.530293  [   32/   54]
train() client id: f_00007-0-0 loss: 0.735649  [   32/  179]
train() client id: f_00007-0-1 loss: 0.747259  [   64/  179]
train() client id: f_00007-0-2 loss: 0.518210  [   96/  179]
train() client id: f_00007-0-3 loss: 0.631285  [  128/  179]
train() client id: f_00007-0-4 loss: 0.583903  [  160/  179]
train() client id: f_00007-1-0 loss: 0.605025  [   32/  179]
train() client id: f_00007-1-1 loss: 0.583778  [   64/  179]
train() client id: f_00007-1-2 loss: 0.702952  [   96/  179]
train() client id: f_00007-1-3 loss: 0.533103  [  128/  179]
train() client id: f_00007-1-4 loss: 0.703348  [  160/  179]
train() client id: f_00007-2-0 loss: 0.618593  [   32/  179]
train() client id: f_00007-2-1 loss: 0.637105  [   64/  179]
train() client id: f_00007-2-2 loss: 0.499301  [   96/  179]
train() client id: f_00007-2-3 loss: 0.671162  [  128/  179]
train() client id: f_00007-2-4 loss: 0.512344  [  160/  179]
train() client id: f_00007-3-0 loss: 0.584697  [   32/  179]
train() client id: f_00007-3-1 loss: 0.469238  [   64/  179]
train() client id: f_00007-3-2 loss: 0.486026  [   96/  179]
train() client id: f_00007-3-3 loss: 0.537842  [  128/  179]
train() client id: f_00007-3-4 loss: 0.719453  [  160/  179]
train() client id: f_00007-4-0 loss: 0.476691  [   32/  179]
train() client id: f_00007-4-1 loss: 0.780789  [   64/  179]
train() client id: f_00007-4-2 loss: 0.652971  [   96/  179]
train() client id: f_00007-4-3 loss: 0.615739  [  128/  179]
train() client id: f_00007-4-4 loss: 0.547203  [  160/  179]
train() client id: f_00007-5-0 loss: 0.450979  [   32/  179]
train() client id: f_00007-5-1 loss: 0.703232  [   64/  179]
train() client id: f_00007-5-2 loss: 0.644068  [   96/  179]
train() client id: f_00007-5-3 loss: 0.526999  [  128/  179]
train() client id: f_00007-5-4 loss: 0.541411  [  160/  179]
train() client id: f_00007-6-0 loss: 0.527164  [   32/  179]
train() client id: f_00007-6-1 loss: 0.457833  [   64/  179]
train() client id: f_00007-6-2 loss: 0.599532  [   96/  179]
train() client id: f_00007-6-3 loss: 0.639485  [  128/  179]
train() client id: f_00007-6-4 loss: 0.632691  [  160/  179]
train() client id: f_00007-7-0 loss: 0.461661  [   32/  179]
train() client id: f_00007-7-1 loss: 0.539667  [   64/  179]
train() client id: f_00007-7-2 loss: 0.710137  [   96/  179]
train() client id: f_00007-7-3 loss: 0.689674  [  128/  179]
train() client id: f_00007-7-4 loss: 0.650793  [  160/  179]
train() client id: f_00007-8-0 loss: 0.431316  [   32/  179]
train() client id: f_00007-8-1 loss: 0.541533  [   64/  179]
train() client id: f_00007-8-2 loss: 0.661586  [   96/  179]
train() client id: f_00007-8-3 loss: 0.419921  [  128/  179]
train() client id: f_00007-8-4 loss: 0.871385  [  160/  179]
train() client id: f_00007-9-0 loss: 0.445370  [   32/  179]
train() client id: f_00007-9-1 loss: 0.519113  [   64/  179]
train() client id: f_00007-9-2 loss: 0.524676  [   96/  179]
train() client id: f_00007-9-3 loss: 0.623006  [  128/  179]
train() client id: f_00007-9-4 loss: 0.743681  [  160/  179]
train() client id: f_00007-10-0 loss: 0.448247  [   32/  179]
train() client id: f_00007-10-1 loss: 0.700046  [   64/  179]
train() client id: f_00007-10-2 loss: 0.689061  [   96/  179]
train() client id: f_00007-10-3 loss: 0.511509  [  128/  179]
train() client id: f_00007-10-4 loss: 0.651751  [  160/  179]
train() client id: f_00007-11-0 loss: 0.584039  [   32/  179]
train() client id: f_00007-11-1 loss: 0.650288  [   64/  179]
train() client id: f_00007-11-2 loss: 0.526434  [   96/  179]
train() client id: f_00007-11-3 loss: 0.521507  [  128/  179]
train() client id: f_00007-11-4 loss: 0.718307  [  160/  179]
train() client id: f_00008-0-0 loss: 0.843933  [   32/  130]
train() client id: f_00008-0-1 loss: 0.867908  [   64/  130]
train() client id: f_00008-0-2 loss: 0.846457  [   96/  130]
train() client id: f_00008-0-3 loss: 0.762885  [  128/  130]
train() client id: f_00008-1-0 loss: 0.779306  [   32/  130]
train() client id: f_00008-1-1 loss: 0.853073  [   64/  130]
train() client id: f_00008-1-2 loss: 0.856748  [   96/  130]
train() client id: f_00008-1-3 loss: 0.836606  [  128/  130]
train() client id: f_00008-2-0 loss: 0.921969  [   32/  130]
train() client id: f_00008-2-1 loss: 0.768119  [   64/  130]
train() client id: f_00008-2-2 loss: 0.737995  [   96/  130]
train() client id: f_00008-2-3 loss: 0.887734  [  128/  130]
train() client id: f_00008-3-0 loss: 0.842240  [   32/  130]
train() client id: f_00008-3-1 loss: 0.776653  [   64/  130]
train() client id: f_00008-3-2 loss: 0.875172  [   96/  130]
train() client id: f_00008-3-3 loss: 0.820976  [  128/  130]
train() client id: f_00008-4-0 loss: 0.843985  [   32/  130]
train() client id: f_00008-4-1 loss: 0.839075  [   64/  130]
train() client id: f_00008-4-2 loss: 0.788151  [   96/  130]
train() client id: f_00008-4-3 loss: 0.837063  [  128/  130]
train() client id: f_00008-5-0 loss: 0.806462  [   32/  130]
train() client id: f_00008-5-1 loss: 0.819013  [   64/  130]
train() client id: f_00008-5-2 loss: 0.824071  [   96/  130]
train() client id: f_00008-5-3 loss: 0.861178  [  128/  130]
train() client id: f_00008-6-0 loss: 0.917239  [   32/  130]
train() client id: f_00008-6-1 loss: 0.840940  [   64/  130]
train() client id: f_00008-6-2 loss: 0.717489  [   96/  130]
train() client id: f_00008-6-3 loss: 0.825319  [  128/  130]
train() client id: f_00008-7-0 loss: 0.810814  [   32/  130]
train() client id: f_00008-7-1 loss: 0.991109  [   64/  130]
train() client id: f_00008-7-2 loss: 0.737191  [   96/  130]
train() client id: f_00008-7-3 loss: 0.759311  [  128/  130]
train() client id: f_00008-8-0 loss: 0.899674  [   32/  130]
train() client id: f_00008-8-1 loss: 0.805764  [   64/  130]
train() client id: f_00008-8-2 loss: 0.758598  [   96/  130]
train() client id: f_00008-8-3 loss: 0.838894  [  128/  130]
train() client id: f_00008-9-0 loss: 0.844225  [   32/  130]
train() client id: f_00008-9-1 loss: 0.771482  [   64/  130]
train() client id: f_00008-9-2 loss: 0.891298  [   96/  130]
train() client id: f_00008-9-3 loss: 0.728531  [  128/  130]
train() client id: f_00008-10-0 loss: 0.889022  [   32/  130]
train() client id: f_00008-10-1 loss: 0.753968  [   64/  130]
train() client id: f_00008-10-2 loss: 0.763088  [   96/  130]
train() client id: f_00008-10-3 loss: 0.899589  [  128/  130]
train() client id: f_00008-11-0 loss: 0.878225  [   32/  130]
train() client id: f_00008-11-1 loss: 0.864663  [   64/  130]
train() client id: f_00008-11-2 loss: 0.815650  [   96/  130]
train() client id: f_00008-11-3 loss: 0.736841  [  128/  130]
train() client id: f_00009-0-0 loss: 1.200227  [   32/  118]
train() client id: f_00009-0-1 loss: 1.069778  [   64/  118]
train() client id: f_00009-0-2 loss: 1.271488  [   96/  118]
train() client id: f_00009-1-0 loss: 1.202166  [   32/  118]
train() client id: f_00009-1-1 loss: 1.068216  [   64/  118]
train() client id: f_00009-1-2 loss: 1.195685  [   96/  118]
train() client id: f_00009-2-0 loss: 1.080504  [   32/  118]
train() client id: f_00009-2-1 loss: 1.026884  [   64/  118]
train() client id: f_00009-2-2 loss: 1.175818  [   96/  118]
train() client id: f_00009-3-0 loss: 0.948107  [   32/  118]
train() client id: f_00009-3-1 loss: 1.111722  [   64/  118]
train() client id: f_00009-3-2 loss: 1.054478  [   96/  118]
train() client id: f_00009-4-0 loss: 1.197545  [   32/  118]
train() client id: f_00009-4-1 loss: 1.065793  [   64/  118]
train() client id: f_00009-4-2 loss: 0.908027  [   96/  118]
train() client id: f_00009-5-0 loss: 1.107900  [   32/  118]
train() client id: f_00009-5-1 loss: 0.954196  [   64/  118]
train() client id: f_00009-5-2 loss: 0.977113  [   96/  118]
train() client id: f_00009-6-0 loss: 0.974837  [   32/  118]
train() client id: f_00009-6-1 loss: 0.986135  [   64/  118]
train() client id: f_00009-6-2 loss: 1.023381  [   96/  118]
train() client id: f_00009-7-0 loss: 1.003228  [   32/  118]
train() client id: f_00009-7-1 loss: 1.032126  [   64/  118]
train() client id: f_00009-7-2 loss: 1.071436  [   96/  118]
train() client id: f_00009-8-0 loss: 1.106639  [   32/  118]
train() client id: f_00009-8-1 loss: 1.044245  [   64/  118]
train() client id: f_00009-8-2 loss: 0.827808  [   96/  118]
train() client id: f_00009-9-0 loss: 0.927795  [   32/  118]
train() client id: f_00009-9-1 loss: 0.954478  [   64/  118]
train() client id: f_00009-9-2 loss: 0.941473  [   96/  118]
train() client id: f_00009-10-0 loss: 1.098500  [   32/  118]
train() client id: f_00009-10-1 loss: 0.946039  [   64/  118]
train() client id: f_00009-10-2 loss: 0.888646  [   96/  118]
train() client id: f_00009-11-0 loss: 1.012395  [   32/  118]
train() client id: f_00009-11-1 loss: 1.029696  [   64/  118]
train() client id: f_00009-11-2 loss: 0.913760  [   96/  118]
At round 34 accuracy: 0.6525198938992043
At round 34 training accuracy: 0.5868544600938967
At round 34 training loss: 0.8260260718284358
update_location
xs = [  -3.9056584     4.20031788  190.00902392   18.81129433    0.97929623
    3.95640986 -152.44319194 -131.32485185  174.66397685 -117.06087855]
ys = [ 182.5879595   165.55583871    1.32061395 -152.45517586  144.35018685
  127.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [208.21531433 193.45898377 214.72115217 183.29333169 175.60733317
 162.33333086 182.33435582 165.066329   202.03013335 154.01058776]
dists_bs = [171.26252029 179.44900699 404.22285522 380.51243082 178.62845167
 185.07269113 179.05151394 179.56996694 383.47022645 180.54759768]
uav_gains = [1.51163930e-11 1.87414015e-11 1.37010772e-11 2.16985605e-11
 2.42798461e-11 2.96968280e-11 2.20025346e-11 2.84631267e-11
 1.65569782e-11 3.39218710e-11]
bs_gains = [6.15232911e-11 5.39832614e-11 5.55591589e-12 6.58053658e-12
 5.46804790e-11 4.95148575e-11 5.43194911e-11 5.38815050e-11
 6.43940092e-12 5.30685578e-11]
Round 35
-------------------------------
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.3715199  13.18889052  6.27703667  2.2632532  15.21119563  7.32046252
  2.80542146  8.96352054  6.61377748  5.93751329]
obj_prev = 74.95259119677627
eta_min = 3.0936410554122904e-15	eta_max = 0.9287856551033156
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 17.39350337614215	eta = 0.9090909090909091
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 31.811065698035446	eta = 0.4970684084145405
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 24.73112036289277	eta = 0.6393675484357765
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 23.451279643601314	eta = 0.6742606815832007
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 23.38394553843602	eta = 0.6762022161958229
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 23.38374385979249	eta = 0.6762080482621735
eta = 0.6762080482621735
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [0.03228538 0.0679018  0.03177292 0.01101803 0.07840736 0.03741005
 0.01383659 0.04586574 0.03331032 0.03023552]
ene_total = [2.0907422  3.75044408 2.07789445 0.9851544  4.27493166 2.23178708
 1.1246433  2.70086556 2.2793763  1.86790483]
ti_comp = [0.46966707 0.49397079 0.46711134 0.47805782 0.49415416 0.49271153
 0.47835241 0.48347539 0.44219867 0.49372516]
ti_coms = [0.09519389 0.07089017 0.09774963 0.08680315 0.07070681 0.07214943
 0.08650855 0.08138558 0.1226623  0.07113581]
t_total = [28.24985313 28.24985313 28.24985313 28.24985313 28.24985313 28.24985313
 28.24985313 28.24985313 28.24985313 28.24985313]
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [9.53493184e-06 8.01901942e-05 9.18777644e-06 3.65789049e-07
 1.23374545e-04 1.34790504e-05 7.23554964e-07 2.57986237e-05
 1.18135760e-05 7.08698796e-06]
ene_total = [0.47656101 0.35854674 0.48932539 0.4341386  0.35978946 0.36150821
 0.43268317 0.40831618 0.61405008 0.35611918]
optimize_network iter = 0 obj = 4.291038018675944
eta = 0.6762080482621735
freqs = [34370497.98105174 68730577.98040815 34010002.78410535 11523739.23427672
 79334916.60468721 37963436.80656631 14462760.16316765 47433377.96218494
 37664422.59882781 30619785.51757285]
eta_min = 0.6762080482621772	eta_max = 0.6762080482621733
af = 0.011653184886415587	bf = 1.3848442141784587	zeta = 0.012818503375057147	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [2.32272370e-06 1.95344516e-05 2.23815612e-06 8.91067609e-08
 3.00542243e-05 3.28351691e-06 1.76259075e-07 6.28458343e-06
 2.87780483e-06 1.72640090e-06]
ene_total = [1.69608812 1.26623554 1.74159793 1.54622714 1.26484323 1.28577181
 1.54099513 1.45082842 2.18547748 1.2674389 ]
ti_comp = [0.46966707 0.49397079 0.46711134 0.47805782 0.49415416 0.49271153
 0.47835241 0.48347539 0.44219867 0.49372516]
ti_coms = [0.09519389 0.07089017 0.09774963 0.08680315 0.07070681 0.07214943
 0.08650855 0.08138558 0.1226623  0.07113581]
t_total = [28.24985313 28.24985313 28.24985313 28.24985313 28.24985313 28.24985313
 28.24985313 28.24985313 28.24985313 28.24985313]
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [9.53493184e-06 8.01901942e-05 9.18777644e-06 3.65789049e-07
 1.23374545e-04 1.34790504e-05 7.23554964e-07 2.57986237e-05
 1.18135760e-05 7.08698796e-06]
ene_total = [0.47656101 0.35854674 0.48932539 0.4341386  0.35978946 0.36150821
 0.43268317 0.40831618 0.61405008 0.35611918]
optimize_network iter = 1 obj = 4.291038018675939
eta = 0.6762080482621733
freqs = [34370497.98105174 68730577.98040818 34010002.78410535 11523739.23427672
 79334916.60468724 37963436.80656631 14462760.16316766 47433377.96218494
 37664422.59882782 30619785.51757286]
Done!
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [8.93086335e-06 7.51098884e-05 8.60570135e-06 3.42615140e-07
 1.15558372e-04 1.26251094e-05 6.77715438e-07 2.41641982e-05
 1.10651481e-05 6.63800456e-06]
ene_total = [0.00952832 0.00716413 0.00978357 0.00868066 0.00718624 0.00722757
 0.00865153 0.00816272 0.01227729 0.00712022]
At round 35 energy consumption: 0.0857822507966446
At round 35 eta: 0.6762080482621733
At round 35 a_n: 15.850952365358953
At round 35 local rounds: 12.811659702043181
At round 35 global rounds: 50.012046702878195
gradient difference: 0.4489191472530365
train() client id: f_00000-0-0 loss: 1.218801  [   32/  126]
train() client id: f_00000-0-1 loss: 1.037925  [   64/  126]
train() client id: f_00000-0-2 loss: 1.129862  [   96/  126]
train() client id: f_00000-1-0 loss: 1.232005  [   32/  126]
train() client id: f_00000-1-1 loss: 0.997182  [   64/  126]
train() client id: f_00000-1-2 loss: 1.114556  [   96/  126]
train() client id: f_00000-2-0 loss: 1.136696  [   32/  126]
train() client id: f_00000-2-1 loss: 1.039461  [   64/  126]
train() client id: f_00000-2-2 loss: 0.988546  [   96/  126]
train() client id: f_00000-3-0 loss: 0.978972  [   32/  126]
train() client id: f_00000-3-1 loss: 1.023896  [   64/  126]
train() client id: f_00000-3-2 loss: 1.030083  [   96/  126]
train() client id: f_00000-4-0 loss: 0.950541  [   32/  126]
train() client id: f_00000-4-1 loss: 0.907523  [   64/  126]
train() client id: f_00000-4-2 loss: 0.924687  [   96/  126]
train() client id: f_00000-5-0 loss: 0.957227  [   32/  126]
train() client id: f_00000-5-1 loss: 0.859775  [   64/  126]
train() client id: f_00000-5-2 loss: 0.915944  [   96/  126]
train() client id: f_00000-6-0 loss: 0.849623  [   32/  126]
train() client id: f_00000-6-1 loss: 0.918830  [   64/  126]
train() client id: f_00000-6-2 loss: 0.866761  [   96/  126]
train() client id: f_00000-7-0 loss: 0.753743  [   32/  126]
train() client id: f_00000-7-1 loss: 0.940801  [   64/  126]
train() client id: f_00000-7-2 loss: 0.857267  [   96/  126]
train() client id: f_00000-8-0 loss: 0.969409  [   32/  126]
train() client id: f_00000-8-1 loss: 0.814208  [   64/  126]
train() client id: f_00000-8-2 loss: 0.891258  [   96/  126]
train() client id: f_00000-9-0 loss: 0.817052  [   32/  126]
train() client id: f_00000-9-1 loss: 0.961508  [   64/  126]
train() client id: f_00000-9-2 loss: 0.832346  [   96/  126]
train() client id: f_00000-10-0 loss: 0.820495  [   32/  126]
train() client id: f_00000-10-1 loss: 0.922738  [   64/  126]
train() client id: f_00000-10-2 loss: 0.771317  [   96/  126]
train() client id: f_00000-11-0 loss: 0.847766  [   32/  126]
train() client id: f_00000-11-1 loss: 0.789100  [   64/  126]
train() client id: f_00000-11-2 loss: 0.996219  [   96/  126]
train() client id: f_00001-0-0 loss: 0.505117  [   32/  265]
train() client id: f_00001-0-1 loss: 0.457755  [   64/  265]
train() client id: f_00001-0-2 loss: 0.550356  [   96/  265]
train() client id: f_00001-0-3 loss: 0.582467  [  128/  265]
train() client id: f_00001-0-4 loss: 0.526680  [  160/  265]
train() client id: f_00001-0-5 loss: 0.421237  [  192/  265]
train() client id: f_00001-0-6 loss: 0.476916  [  224/  265]
train() client id: f_00001-0-7 loss: 0.553736  [  256/  265]
train() client id: f_00001-1-0 loss: 0.488941  [   32/  265]
train() client id: f_00001-1-1 loss: 0.619251  [   64/  265]
train() client id: f_00001-1-2 loss: 0.581730  [   96/  265]
train() client id: f_00001-1-3 loss: 0.482754  [  128/  265]
train() client id: f_00001-1-4 loss: 0.509589  [  160/  265]
train() client id: f_00001-1-5 loss: 0.423024  [  192/  265]
train() client id: f_00001-1-6 loss: 0.505135  [  224/  265]
train() client id: f_00001-1-7 loss: 0.482549  [  256/  265]
train() client id: f_00001-2-0 loss: 0.497367  [   32/  265]
train() client id: f_00001-2-1 loss: 0.509034  [   64/  265]
train() client id: f_00001-2-2 loss: 0.605089  [   96/  265]
train() client id: f_00001-2-3 loss: 0.585872  [  128/  265]
train() client id: f_00001-2-4 loss: 0.421608  [  160/  265]
train() client id: f_00001-2-5 loss: 0.427531  [  192/  265]
train() client id: f_00001-2-6 loss: 0.399151  [  224/  265]
train() client id: f_00001-2-7 loss: 0.550919  [  256/  265]
train() client id: f_00001-3-0 loss: 0.511664  [   32/  265]
train() client id: f_00001-3-1 loss: 0.517843  [   64/  265]
train() client id: f_00001-3-2 loss: 0.468070  [   96/  265]
train() client id: f_00001-3-3 loss: 0.396096  [  128/  265]
train() client id: f_00001-3-4 loss: 0.460259  [  160/  265]
train() client id: f_00001-3-5 loss: 0.635775  [  192/  265]
train() client id: f_00001-3-6 loss: 0.497764  [  224/  265]
train() client id: f_00001-3-7 loss: 0.545455  [  256/  265]
train() client id: f_00001-4-0 loss: 0.513907  [   32/  265]
train() client id: f_00001-4-1 loss: 0.433220  [   64/  265]
train() client id: f_00001-4-2 loss: 0.568792  [   96/  265]
train() client id: f_00001-4-3 loss: 0.702568  [  128/  265]
train() client id: f_00001-4-4 loss: 0.534367  [  160/  265]
train() client id: f_00001-4-5 loss: 0.404720  [  192/  265]
train() client id: f_00001-4-6 loss: 0.441961  [  224/  265]
train() client id: f_00001-4-7 loss: 0.382960  [  256/  265]
train() client id: f_00001-5-0 loss: 0.732687  [   32/  265]
train() client id: f_00001-5-1 loss: 0.513058  [   64/  265]
train() client id: f_00001-5-2 loss: 0.511932  [   96/  265]
train() client id: f_00001-5-3 loss: 0.408570  [  128/  265]
train() client id: f_00001-5-4 loss: 0.418376  [  160/  265]
train() client id: f_00001-5-5 loss: 0.393867  [  192/  265]
train() client id: f_00001-5-6 loss: 0.518280  [  224/  265]
train() client id: f_00001-5-7 loss: 0.503209  [  256/  265]
train() client id: f_00001-6-0 loss: 0.436178  [   32/  265]
train() client id: f_00001-6-1 loss: 0.550526  [   64/  265]
train() client id: f_00001-6-2 loss: 0.509836  [   96/  265]
train() client id: f_00001-6-3 loss: 0.469255  [  128/  265]
train() client id: f_00001-6-4 loss: 0.464481  [  160/  265]
train() client id: f_00001-6-5 loss: 0.426582  [  192/  265]
train() client id: f_00001-6-6 loss: 0.395315  [  224/  265]
train() client id: f_00001-6-7 loss: 0.712100  [  256/  265]
train() client id: f_00001-7-0 loss: 0.404306  [   32/  265]
train() client id: f_00001-7-1 loss: 0.469062  [   64/  265]
train() client id: f_00001-7-2 loss: 0.483272  [   96/  265]
train() client id: f_00001-7-3 loss: 0.515437  [  128/  265]
train() client id: f_00001-7-4 loss: 0.627772  [  160/  265]
train() client id: f_00001-7-5 loss: 0.591897  [  192/  265]
train() client id: f_00001-7-6 loss: 0.413332  [  224/  265]
train() client id: f_00001-7-7 loss: 0.508409  [  256/  265]
train() client id: f_00001-8-0 loss: 0.634623  [   32/  265]
train() client id: f_00001-8-1 loss: 0.495995  [   64/  265]
train() client id: f_00001-8-2 loss: 0.404312  [   96/  265]
train() client id: f_00001-8-3 loss: 0.500698  [  128/  265]
train() client id: f_00001-8-4 loss: 0.531242  [  160/  265]
train() client id: f_00001-8-5 loss: 0.479395  [  192/  265]
train() client id: f_00001-8-6 loss: 0.498610  [  224/  265]
train() client id: f_00001-8-7 loss: 0.407500  [  256/  265]
train() client id: f_00001-9-0 loss: 0.390256  [   32/  265]
train() client id: f_00001-9-1 loss: 0.501513  [   64/  265]
train() client id: f_00001-9-2 loss: 0.432301  [   96/  265]
train() client id: f_00001-9-3 loss: 0.453030  [  128/  265]
train() client id: f_00001-9-4 loss: 0.634879  [  160/  265]
train() client id: f_00001-9-5 loss: 0.578421  [  192/  265]
train() client id: f_00001-9-6 loss: 0.402245  [  224/  265]
train() client id: f_00001-9-7 loss: 0.624100  [  256/  265]
train() client id: f_00001-10-0 loss: 0.487589  [   32/  265]
train() client id: f_00001-10-1 loss: 0.403333  [   64/  265]
train() client id: f_00001-10-2 loss: 0.506482  [   96/  265]
train() client id: f_00001-10-3 loss: 0.519565  [  128/  265]
train() client id: f_00001-10-4 loss: 0.497500  [  160/  265]
train() client id: f_00001-10-5 loss: 0.484849  [  192/  265]
train() client id: f_00001-10-6 loss: 0.667715  [  224/  265]
train() client id: f_00001-10-7 loss: 0.466892  [  256/  265]
train() client id: f_00001-11-0 loss: 0.664765  [   32/  265]
train() client id: f_00001-11-1 loss: 0.495973  [   64/  265]
train() client id: f_00001-11-2 loss: 0.425441  [   96/  265]
train() client id: f_00001-11-3 loss: 0.410200  [  128/  265]
train() client id: f_00001-11-4 loss: 0.410531  [  160/  265]
train() client id: f_00001-11-5 loss: 0.669893  [  192/  265]
train() client id: f_00001-11-6 loss: 0.479398  [  224/  265]
train() client id: f_00001-11-7 loss: 0.478199  [  256/  265]
train() client id: f_00002-0-0 loss: 1.088635  [   32/  124]
train() client id: f_00002-0-1 loss: 1.132617  [   64/  124]
train() client id: f_00002-0-2 loss: 1.148592  [   96/  124]
train() client id: f_00002-1-0 loss: 1.155427  [   32/  124]
train() client id: f_00002-1-1 loss: 1.109864  [   64/  124]
train() client id: f_00002-1-2 loss: 1.014246  [   96/  124]
train() client id: f_00002-2-0 loss: 1.049416  [   32/  124]
train() client id: f_00002-2-1 loss: 1.201443  [   64/  124]
train() client id: f_00002-2-2 loss: 0.916115  [   96/  124]
train() client id: f_00002-3-0 loss: 1.047793  [   32/  124]
train() client id: f_00002-3-1 loss: 0.955079  [   64/  124]
train() client id: f_00002-3-2 loss: 1.019758  [   96/  124]
train() client id: f_00002-4-0 loss: 0.884864  [   32/  124]
train() client id: f_00002-4-1 loss: 0.863173  [   64/  124]
train() client id: f_00002-4-2 loss: 1.019393  [   96/  124]
train() client id: f_00002-5-0 loss: 1.128492  [   32/  124]
train() client id: f_00002-5-1 loss: 0.924869  [   64/  124]
train() client id: f_00002-5-2 loss: 1.049719  [   96/  124]
train() client id: f_00002-6-0 loss: 0.915901  [   32/  124]
train() client id: f_00002-6-1 loss: 0.904980  [   64/  124]
train() client id: f_00002-6-2 loss: 1.012021  [   96/  124]
train() client id: f_00002-7-0 loss: 0.866310  [   32/  124]
train() client id: f_00002-7-1 loss: 0.874298  [   64/  124]
train() client id: f_00002-7-2 loss: 1.172075  [   96/  124]
train() client id: f_00002-8-0 loss: 1.010413  [   32/  124]
train() client id: f_00002-8-1 loss: 1.071001  [   64/  124]
train() client id: f_00002-8-2 loss: 0.931841  [   96/  124]
train() client id: f_00002-9-0 loss: 0.986052  [   32/  124]
train() client id: f_00002-9-1 loss: 1.179808  [   64/  124]
train() client id: f_00002-9-2 loss: 0.823043  [   96/  124]
train() client id: f_00002-10-0 loss: 0.921787  [   32/  124]
train() client id: f_00002-10-1 loss: 0.990583  [   64/  124]
train() client id: f_00002-10-2 loss: 0.954653  [   96/  124]
train() client id: f_00002-11-0 loss: 0.955028  [   32/  124]
train() client id: f_00002-11-1 loss: 0.988942  [   64/  124]
train() client id: f_00002-11-2 loss: 1.009699  [   96/  124]
train() client id: f_00003-0-0 loss: 0.697734  [   32/   43]
train() client id: f_00003-1-0 loss: 0.849343  [   32/   43]
train() client id: f_00003-2-0 loss: 0.641399  [   32/   43]
train() client id: f_00003-3-0 loss: 0.724748  [   32/   43]
train() client id: f_00003-4-0 loss: 0.773981  [   32/   43]
train() client id: f_00003-5-0 loss: 0.764666  [   32/   43]
train() client id: f_00003-6-0 loss: 0.698124  [   32/   43]
train() client id: f_00003-7-0 loss: 0.903708  [   32/   43]
train() client id: f_00003-8-0 loss: 0.798434  [   32/   43]
train() client id: f_00003-9-0 loss: 0.694533  [   32/   43]
train() client id: f_00003-10-0 loss: 0.804006  [   32/   43]
train() client id: f_00003-11-0 loss: 0.929639  [   32/   43]
train() client id: f_00004-0-0 loss: 0.839256  [   32/  306]
train() client id: f_00004-0-1 loss: 0.853767  [   64/  306]
train() client id: f_00004-0-2 loss: 0.801791  [   96/  306]
train() client id: f_00004-0-3 loss: 1.030318  [  128/  306]
train() client id: f_00004-0-4 loss: 0.950313  [  160/  306]
train() client id: f_00004-0-5 loss: 0.869722  [  192/  306]
train() client id: f_00004-0-6 loss: 0.969355  [  224/  306]
train() client id: f_00004-0-7 loss: 0.890958  [  256/  306]
train() client id: f_00004-0-8 loss: 0.952798  [  288/  306]
train() client id: f_00004-1-0 loss: 0.873329  [   32/  306]
train() client id: f_00004-1-1 loss: 1.054767  [   64/  306]
train() client id: f_00004-1-2 loss: 0.885363  [   96/  306]
train() client id: f_00004-1-3 loss: 0.908410  [  128/  306]
train() client id: f_00004-1-4 loss: 0.945684  [  160/  306]
train() client id: f_00004-1-5 loss: 0.939063  [  192/  306]
train() client id: f_00004-1-6 loss: 0.935739  [  224/  306]
train() client id: f_00004-1-7 loss: 0.927618  [  256/  306]
train() client id: f_00004-1-8 loss: 0.777418  [  288/  306]
train() client id: f_00004-2-0 loss: 0.880048  [   32/  306]
train() client id: f_00004-2-1 loss: 0.885894  [   64/  306]
train() client id: f_00004-2-2 loss: 0.954151  [   96/  306]
train() client id: f_00004-2-3 loss: 0.976610  [  128/  306]
train() client id: f_00004-2-4 loss: 0.825648  [  160/  306]
train() client id: f_00004-2-5 loss: 0.899196  [  192/  306]
train() client id: f_00004-2-6 loss: 0.970961  [  224/  306]
train() client id: f_00004-2-7 loss: 0.864327  [  256/  306]
train() client id: f_00004-2-8 loss: 1.030639  [  288/  306]
train() client id: f_00004-3-0 loss: 1.005349  [   32/  306]
train() client id: f_00004-3-1 loss: 0.910473  [   64/  306]
train() client id: f_00004-3-2 loss: 1.001488  [   96/  306]
train() client id: f_00004-3-3 loss: 0.949202  [  128/  306]
train() client id: f_00004-3-4 loss: 0.958426  [  160/  306]
train() client id: f_00004-3-5 loss: 0.859549  [  192/  306]
train() client id: f_00004-3-6 loss: 0.839318  [  224/  306]
train() client id: f_00004-3-7 loss: 0.879173  [  256/  306]
train() client id: f_00004-3-8 loss: 0.778080  [  288/  306]
train() client id: f_00004-4-0 loss: 1.003749  [   32/  306]
train() client id: f_00004-4-1 loss: 0.877205  [   64/  306]
train() client id: f_00004-4-2 loss: 0.814152  [   96/  306]
train() client id: f_00004-4-3 loss: 0.948641  [  128/  306]
train() client id: f_00004-4-4 loss: 0.796129  [  160/  306]
train() client id: f_00004-4-5 loss: 0.933626  [  192/  306]
train() client id: f_00004-4-6 loss: 0.927449  [  224/  306]
train() client id: f_00004-4-7 loss: 0.811501  [  256/  306]
train() client id: f_00004-4-8 loss: 0.961575  [  288/  306]
train() client id: f_00004-5-0 loss: 0.908913  [   32/  306]
train() client id: f_00004-5-1 loss: 0.760936  [   64/  306]
train() client id: f_00004-5-2 loss: 0.990176  [   96/  306]
train() client id: f_00004-5-3 loss: 0.886839  [  128/  306]
train() client id: f_00004-5-4 loss: 0.839197  [  160/  306]
train() client id: f_00004-5-5 loss: 1.095106  [  192/  306]
train() client id: f_00004-5-6 loss: 1.052042  [  224/  306]
train() client id: f_00004-5-7 loss: 0.783619  [  256/  306]
train() client id: f_00004-5-8 loss: 0.893960  [  288/  306]
train() client id: f_00004-6-0 loss: 0.894106  [   32/  306]
train() client id: f_00004-6-1 loss: 0.724171  [   64/  306]
train() client id: f_00004-6-2 loss: 0.850882  [   96/  306]
train() client id: f_00004-6-3 loss: 0.864663  [  128/  306]
train() client id: f_00004-6-4 loss: 0.996348  [  160/  306]
train() client id: f_00004-6-5 loss: 1.025892  [  192/  306]
train() client id: f_00004-6-6 loss: 0.977348  [  224/  306]
train() client id: f_00004-6-7 loss: 0.955903  [  256/  306]
train() client id: f_00004-6-8 loss: 0.910274  [  288/  306]
train() client id: f_00004-7-0 loss: 0.942985  [   32/  306]
train() client id: f_00004-7-1 loss: 0.877535  [   64/  306]
train() client id: f_00004-7-2 loss: 1.027374  [   96/  306]
train() client id: f_00004-7-3 loss: 0.854478  [  128/  306]
train() client id: f_00004-7-4 loss: 0.849645  [  160/  306]
train() client id: f_00004-7-5 loss: 0.989289  [  192/  306]
train() client id: f_00004-7-6 loss: 0.868474  [  224/  306]
train() client id: f_00004-7-7 loss: 0.830272  [  256/  306]
train() client id: f_00004-7-8 loss: 0.941887  [  288/  306]
train() client id: f_00004-8-0 loss: 0.958908  [   32/  306]
train() client id: f_00004-8-1 loss: 0.869861  [   64/  306]
train() client id: f_00004-8-2 loss: 0.841701  [   96/  306]
train() client id: f_00004-8-3 loss: 0.925062  [  128/  306]
train() client id: f_00004-8-4 loss: 1.000118  [  160/  306]
train() client id: f_00004-8-5 loss: 0.830178  [  192/  306]
train() client id: f_00004-8-6 loss: 0.835697  [  224/  306]
train() client id: f_00004-8-7 loss: 0.991807  [  256/  306]
train() client id: f_00004-8-8 loss: 0.877617  [  288/  306]
train() client id: f_00004-9-0 loss: 0.861372  [   32/  306]
train() client id: f_00004-9-1 loss: 0.847463  [   64/  306]
train() client id: f_00004-9-2 loss: 0.965371  [   96/  306]
train() client id: f_00004-9-3 loss: 0.915485  [  128/  306]
train() client id: f_00004-9-4 loss: 0.907988  [  160/  306]
train() client id: f_00004-9-5 loss: 0.912184  [  192/  306]
train() client id: f_00004-9-6 loss: 0.864375  [  224/  306]
train() client id: f_00004-9-7 loss: 0.898823  [  256/  306]
train() client id: f_00004-9-8 loss: 0.983358  [  288/  306]
train() client id: f_00004-10-0 loss: 0.845319  [   32/  306]
train() client id: f_00004-10-1 loss: 0.961896  [   64/  306]
train() client id: f_00004-10-2 loss: 0.863294  [   96/  306]
train() client id: f_00004-10-3 loss: 0.948222  [  128/  306]
train() client id: f_00004-10-4 loss: 0.932998  [  160/  306]
train() client id: f_00004-10-5 loss: 0.892023  [  192/  306]
train() client id: f_00004-10-6 loss: 0.912994  [  224/  306]
train() client id: f_00004-10-7 loss: 0.918824  [  256/  306]
train() client id: f_00004-10-8 loss: 0.946788  [  288/  306]
train() client id: f_00004-11-0 loss: 0.892095  [   32/  306]
train() client id: f_00004-11-1 loss: 0.822009  [   64/  306]
train() client id: f_00004-11-2 loss: 0.818258  [   96/  306]
train() client id: f_00004-11-3 loss: 0.830096  [  128/  306]
train() client id: f_00004-11-4 loss: 1.060006  [  160/  306]
train() client id: f_00004-11-5 loss: 0.838567  [  192/  306]
train() client id: f_00004-11-6 loss: 0.957485  [  224/  306]
train() client id: f_00004-11-7 loss: 0.932622  [  256/  306]
train() client id: f_00004-11-8 loss: 0.943492  [  288/  306]
train() client id: f_00005-0-0 loss: 0.541607  [   32/  146]
train() client id: f_00005-0-1 loss: 0.488842  [   64/  146]
train() client id: f_00005-0-2 loss: 0.518157  [   96/  146]
train() client id: f_00005-0-3 loss: 0.758570  [  128/  146]
train() client id: f_00005-1-0 loss: 0.362748  [   32/  146]
train() client id: f_00005-1-1 loss: 0.786231  [   64/  146]
train() client id: f_00005-1-2 loss: 0.457777  [   96/  146]
train() client id: f_00005-1-3 loss: 0.637054  [  128/  146]
train() client id: f_00005-2-0 loss: 0.563358  [   32/  146]
train() client id: f_00005-2-1 loss: 0.597621  [   64/  146]
train() client id: f_00005-2-2 loss: 0.700064  [   96/  146]
train() client id: f_00005-2-3 loss: 0.638510  [  128/  146]
train() client id: f_00005-3-0 loss: 0.704231  [   32/  146]
train() client id: f_00005-3-1 loss: 0.525834  [   64/  146]
train() client id: f_00005-3-2 loss: 0.639358  [   96/  146]
train() client id: f_00005-3-3 loss: 0.469295  [  128/  146]
train() client id: f_00005-4-0 loss: 0.500435  [   32/  146]
train() client id: f_00005-4-1 loss: 0.791470  [   64/  146]
train() client id: f_00005-4-2 loss: 0.542828  [   96/  146]
train() client id: f_00005-4-3 loss: 0.593310  [  128/  146]
train() client id: f_00005-5-0 loss: 0.730229  [   32/  146]
train() client id: f_00005-5-1 loss: 0.512563  [   64/  146]
train() client id: f_00005-5-2 loss: 0.614936  [   96/  146]
train() client id: f_00005-5-3 loss: 0.670865  [  128/  146]
train() client id: f_00005-6-0 loss: 0.439594  [   32/  146]
train() client id: f_00005-6-1 loss: 0.527288  [   64/  146]
train() client id: f_00005-6-2 loss: 0.424908  [   96/  146]
train() client id: f_00005-6-3 loss: 0.883730  [  128/  146]
train() client id: f_00005-7-0 loss: 0.484798  [   32/  146]
train() client id: f_00005-7-1 loss: 0.640603  [   64/  146]
train() client id: f_00005-7-2 loss: 0.491519  [   96/  146]
train() client id: f_00005-7-3 loss: 0.801937  [  128/  146]
train() client id: f_00005-8-0 loss: 0.414030  [   32/  146]
train() client id: f_00005-8-1 loss: 0.473171  [   64/  146]
train() client id: f_00005-8-2 loss: 0.698575  [   96/  146]
train() client id: f_00005-8-3 loss: 0.815364  [  128/  146]
train() client id: f_00005-9-0 loss: 0.764478  [   32/  146]
train() client id: f_00005-9-1 loss: 0.796240  [   64/  146]
train() client id: f_00005-9-2 loss: 0.577452  [   96/  146]
train() client id: f_00005-9-3 loss: 0.303761  [  128/  146]
train() client id: f_00005-10-0 loss: 0.701378  [   32/  146]
train() client id: f_00005-10-1 loss: 0.588687  [   64/  146]
train() client id: f_00005-10-2 loss: 0.402158  [   96/  146]
train() client id: f_00005-10-3 loss: 0.646011  [  128/  146]
train() client id: f_00005-11-0 loss: 0.655425  [   32/  146]
train() client id: f_00005-11-1 loss: 0.611295  [   64/  146]
train() client id: f_00005-11-2 loss: 0.489456  [   96/  146]
train() client id: f_00005-11-3 loss: 0.684329  [  128/  146]
train() client id: f_00006-0-0 loss: 0.498765  [   32/   54]
train() client id: f_00006-1-0 loss: 0.508374  [   32/   54]
train() client id: f_00006-2-0 loss: 0.555115  [   32/   54]
train() client id: f_00006-3-0 loss: 0.557284  [   32/   54]
train() client id: f_00006-4-0 loss: 0.508867  [   32/   54]
train() client id: f_00006-5-0 loss: 0.518928  [   32/   54]
train() client id: f_00006-6-0 loss: 0.562176  [   32/   54]
train() client id: f_00006-7-0 loss: 0.563246  [   32/   54]
train() client id: f_00006-8-0 loss: 0.518364  [   32/   54]
train() client id: f_00006-9-0 loss: 0.498281  [   32/   54]
train() client id: f_00006-10-0 loss: 0.493417  [   32/   54]
train() client id: f_00006-11-0 loss: 0.462773  [   32/   54]
train() client id: f_00007-0-0 loss: 0.382922  [   32/  179]
train() client id: f_00007-0-1 loss: 0.521843  [   64/  179]
train() client id: f_00007-0-2 loss: 0.519191  [   96/  179]
train() client id: f_00007-0-3 loss: 0.507751  [  128/  179]
train() client id: f_00007-0-4 loss: 0.486654  [  160/  179]
train() client id: f_00007-1-0 loss: 0.467867  [   32/  179]
train() client id: f_00007-1-1 loss: 0.410188  [   64/  179]
train() client id: f_00007-1-2 loss: 0.502339  [   96/  179]
train() client id: f_00007-1-3 loss: 0.410726  [  128/  179]
train() client id: f_00007-1-4 loss: 0.492971  [  160/  179]
train() client id: f_00007-2-0 loss: 0.508713  [   32/  179]
train() client id: f_00007-2-1 loss: 0.404080  [   64/  179]
train() client id: f_00007-2-2 loss: 0.404446  [   96/  179]
train() client id: f_00007-2-3 loss: 0.445828  [  128/  179]
train() client id: f_00007-2-4 loss: 0.473623  [  160/  179]
train() client id: f_00007-3-0 loss: 0.312278  [   32/  179]
train() client id: f_00007-3-1 loss: 0.452805  [   64/  179]
train() client id: f_00007-3-2 loss: 0.494790  [   96/  179]
train() client id: f_00007-3-3 loss: 0.419376  [  128/  179]
train() client id: f_00007-3-4 loss: 0.366883  [  160/  179]
train() client id: f_00007-4-0 loss: 0.519911  [   32/  179]
train() client id: f_00007-4-1 loss: 0.374503  [   64/  179]
train() client id: f_00007-4-2 loss: 0.424489  [   96/  179]
train() client id: f_00007-4-3 loss: 0.287479  [  128/  179]
train() client id: f_00007-4-4 loss: 0.533155  [  160/  179]
train() client id: f_00007-5-0 loss: 0.317479  [   32/  179]
train() client id: f_00007-5-1 loss: 0.540289  [   64/  179]
train() client id: f_00007-5-2 loss: 0.408729  [   96/  179]
train() client id: f_00007-5-3 loss: 0.340784  [  128/  179]
train() client id: f_00007-5-4 loss: 0.436619  [  160/  179]
train() client id: f_00007-6-0 loss: 0.417208  [   32/  179]
train() client id: f_00007-6-1 loss: 0.309463  [   64/  179]
train() client id: f_00007-6-2 loss: 0.484900  [   96/  179]
train() client id: f_00007-6-3 loss: 0.423006  [  128/  179]
train() client id: f_00007-6-4 loss: 0.340132  [  160/  179]
train() client id: f_00007-7-0 loss: 0.249230  [   32/  179]
train() client id: f_00007-7-1 loss: 0.373394  [   64/  179]
train() client id: f_00007-7-2 loss: 0.378753  [   96/  179]
train() client id: f_00007-7-3 loss: 0.429202  [  128/  179]
train() client id: f_00007-7-4 loss: 0.382671  [  160/  179]
train() client id: f_00007-8-0 loss: 0.282426  [   32/  179]
train() client id: f_00007-8-1 loss: 0.245830  [   64/  179]
train() client id: f_00007-8-2 loss: 0.342469  [   96/  179]
train() client id: f_00007-8-3 loss: 0.391282  [  128/  179]
train() client id: f_00007-8-4 loss: 0.482689  [  160/  179]
train() client id: f_00007-9-0 loss: 0.337851  [   32/  179]
train() client id: f_00007-9-1 loss: 0.574958  [   64/  179]
train() client id: f_00007-9-2 loss: 0.337601  [   96/  179]
train() client id: f_00007-9-3 loss: 0.256443  [  128/  179]
train() client id: f_00007-9-4 loss: 0.324427  [  160/  179]
train() client id: f_00007-10-0 loss: 0.387779  [   32/  179]
train() client id: f_00007-10-1 loss: 0.275677  [   64/  179]
train() client id: f_00007-10-2 loss: 0.404329  [   96/  179]
train() client id: f_00007-10-3 loss: 0.574330  [  128/  179]
train() client id: f_00007-10-4 loss: 0.247430  [  160/  179]
train() client id: f_00007-11-0 loss: 0.413833  [   32/  179]
train() client id: f_00007-11-1 loss: 0.583540  [   64/  179]
train() client id: f_00007-11-2 loss: 0.317285  [   96/  179]
train() client id: f_00007-11-3 loss: 0.256252  [  128/  179]
train() client id: f_00007-11-4 loss: 0.303074  [  160/  179]
train() client id: f_00008-0-0 loss: 0.877215  [   32/  130]
train() client id: f_00008-0-1 loss: 0.733853  [   64/  130]
train() client id: f_00008-0-2 loss: 0.750669  [   96/  130]
train() client id: f_00008-0-3 loss: 0.842360  [  128/  130]
train() client id: f_00008-1-0 loss: 0.896621  [   32/  130]
train() client id: f_00008-1-1 loss: 0.878283  [   64/  130]
train() client id: f_00008-1-2 loss: 0.758273  [   96/  130]
train() client id: f_00008-1-3 loss: 0.663828  [  128/  130]
train() client id: f_00008-2-0 loss: 0.921517  [   32/  130]
train() client id: f_00008-2-1 loss: 0.744531  [   64/  130]
train() client id: f_00008-2-2 loss: 0.804898  [   96/  130]
train() client id: f_00008-2-3 loss: 0.755565  [  128/  130]
train() client id: f_00008-3-0 loss: 0.787194  [   32/  130]
train() client id: f_00008-3-1 loss: 0.801487  [   64/  130]
train() client id: f_00008-3-2 loss: 0.888273  [   96/  130]
train() client id: f_00008-3-3 loss: 0.752489  [  128/  130]
train() client id: f_00008-4-0 loss: 0.742321  [   32/  130]
train() client id: f_00008-4-1 loss: 0.749223  [   64/  130]
train() client id: f_00008-4-2 loss: 0.782124  [   96/  130]
train() client id: f_00008-4-3 loss: 0.913495  [  128/  130]
train() client id: f_00008-5-0 loss: 0.658214  [   32/  130]
train() client id: f_00008-5-1 loss: 0.858590  [   64/  130]
train() client id: f_00008-5-2 loss: 0.812516  [   96/  130]
train() client id: f_00008-5-3 loss: 0.899548  [  128/  130]
train() client id: f_00008-6-0 loss: 0.816131  [   32/  130]
train() client id: f_00008-6-1 loss: 0.752813  [   64/  130]
train() client id: f_00008-6-2 loss: 0.826904  [   96/  130]
train() client id: f_00008-6-3 loss: 0.825858  [  128/  130]
train() client id: f_00008-7-0 loss: 0.848767  [   32/  130]
train() client id: f_00008-7-1 loss: 0.748155  [   64/  130]
train() client id: f_00008-7-2 loss: 0.833209  [   96/  130]
train() client id: f_00008-7-3 loss: 0.792827  [  128/  130]
train() client id: f_00008-8-0 loss: 0.758369  [   32/  130]
train() client id: f_00008-8-1 loss: 0.696896  [   64/  130]
train() client id: f_00008-8-2 loss: 0.935574  [   96/  130]
train() client id: f_00008-8-3 loss: 0.828994  [  128/  130]
train() client id: f_00008-9-0 loss: 0.774200  [   32/  130]
train() client id: f_00008-9-1 loss: 0.763193  [   64/  130]
train() client id: f_00008-9-2 loss: 0.909356  [   96/  130]
train() client id: f_00008-9-3 loss: 0.765679  [  128/  130]
train() client id: f_00008-10-0 loss: 0.816210  [   32/  130]
train() client id: f_00008-10-1 loss: 0.853629  [   64/  130]
train() client id: f_00008-10-2 loss: 0.797733  [   96/  130]
train() client id: f_00008-10-3 loss: 0.742069  [  128/  130]
train() client id: f_00008-11-0 loss: 0.751343  [   32/  130]
train() client id: f_00008-11-1 loss: 0.789473  [   64/  130]
train() client id: f_00008-11-2 loss: 0.840115  [   96/  130]
train() client id: f_00008-11-3 loss: 0.760750  [  128/  130]
train() client id: f_00009-0-0 loss: 1.186639  [   32/  118]
train() client id: f_00009-0-1 loss: 1.170980  [   64/  118]
train() client id: f_00009-0-2 loss: 1.179062  [   96/  118]
train() client id: f_00009-1-0 loss: 1.144421  [   32/  118]
train() client id: f_00009-1-1 loss: 1.067360  [   64/  118]
train() client id: f_00009-1-2 loss: 1.104827  [   96/  118]
train() client id: f_00009-2-0 loss: 0.997070  [   32/  118]
train() client id: f_00009-2-1 loss: 1.061596  [   64/  118]
train() client id: f_00009-2-2 loss: 1.110205  [   96/  118]
train() client id: f_00009-3-0 loss: 0.905331  [   32/  118]
train() client id: f_00009-3-1 loss: 0.960702  [   64/  118]
train() client id: f_00009-3-2 loss: 1.058817  [   96/  118]
train() client id: f_00009-4-0 loss: 0.908514  [   32/  118]
train() client id: f_00009-4-1 loss: 1.128000  [   64/  118]
train() client id: f_00009-4-2 loss: 0.941299  [   96/  118]
train() client id: f_00009-5-0 loss: 0.862347  [   32/  118]
train() client id: f_00009-5-1 loss: 1.005293  [   64/  118]
train() client id: f_00009-5-2 loss: 1.110990  [   96/  118]
train() client id: f_00009-6-0 loss: 1.067454  [   32/  118]
train() client id: f_00009-6-1 loss: 0.877029  [   64/  118]
train() client id: f_00009-6-2 loss: 0.849212  [   96/  118]
train() client id: f_00009-7-0 loss: 0.868056  [   32/  118]
train() client id: f_00009-7-1 loss: 1.063312  [   64/  118]
train() client id: f_00009-7-2 loss: 0.842728  [   96/  118]
train() client id: f_00009-8-0 loss: 1.003322  [   32/  118]
train() client id: f_00009-8-1 loss: 0.807037  [   64/  118]
train() client id: f_00009-8-2 loss: 0.817112  [   96/  118]
train() client id: f_00009-9-0 loss: 0.909976  [   32/  118]
train() client id: f_00009-9-1 loss: 0.850228  [   64/  118]
train() client id: f_00009-9-2 loss: 0.923222  [   96/  118]
train() client id: f_00009-10-0 loss: 1.101533  [   32/  118]
train() client id: f_00009-10-1 loss: 0.782731  [   64/  118]
train() client id: f_00009-10-2 loss: 0.816874  [   96/  118]
train() client id: f_00009-11-0 loss: 0.939590  [   32/  118]
train() client id: f_00009-11-1 loss: 0.881902  [   64/  118]
train() client id: f_00009-11-2 loss: 0.883820  [   96/  118]
At round 35 accuracy: 0.6525198938992043
At round 35 training accuracy: 0.5875251509054326
At round 35 training loss: 0.8145077933764501
update_location
xs = [  -3.9056584     4.20031788  195.00902392   18.81129433    0.97929623
    3.95640986 -157.44319194 -136.32485185  179.66397685 -122.06087855]
ys = [ 187.5879595   170.55583871    1.32061395 -157.45517586  149.35018685
  132.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [212.61349138 197.75473898 219.15807863 187.47265721 179.7399158
 166.29868255 186.53484723 169.07140943 206.36815295 157.84444852]
dists_bs = [171.55678491 179.25541692 408.74344878 384.8236762  177.83876297
 183.86147641 178.49055035 178.41979023 388.03486227 179.00570888]
uav_gains = [1.41493817e-11 1.76163185e-11 1.27867778e-11 2.04271955e-11
 2.28496909e-11 2.79287797e-11 2.07052398e-11 2.67731890e-11
 1.55361629e-11 3.18816260e-11]
bs_gains = [6.12282672e-11 5.41466606e-11 5.38557194e-12 6.37618765e-12
 5.53630598e-11 5.04336040e-11 5.47988501e-11 5.48597238e-11
 6.22954026e-12 5.43584189e-11]
Round 36
-------------------------------
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.2394703  12.90983275  6.14725815  2.21746141 14.88914632  7.16517573
  2.74816098  8.77577622  6.47605413  5.8113634 ]
obj_prev = 73.37969937876653
eta_min = 1.5469793420595954e-15	eta_max = 0.9294112566599255
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 17.025573465777978	eta = 0.9090909090909091
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 31.275397678211032	eta = 0.4948872023642162
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 24.26289815788516	eta = 0.6379202500492736
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 22.99478274194065	eta = 0.6731002520657826
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 22.927768157082628	eta = 0.6750676277672019
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 22.927565477060664	eta = 0.6750735953751348
eta = 0.6750735953751348
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [0.03242394 0.0681932  0.03190927 0.01106531 0.07874384 0.03757059
 0.01389597 0.04606258 0.03345327 0.03036527]
ene_total = [2.05444491 3.67229692 2.04263567 0.969603   4.18544742 2.18345709
 1.10625446 2.64978349 2.23698535 1.82665715]
ti_comp = [0.4811663  0.50721776 0.47844041 0.48996025 0.50753423 0.50618685
 0.49025487 0.4955151  0.45404092 0.50727357]
ti_coms = [0.09689836 0.07084691 0.09962426 0.08810442 0.07053044 0.07187782
 0.08780979 0.08254957 0.12402375 0.0707911 ]
t_total = [28.19984894 28.19984894 28.19984894 28.19984894 28.19984894 28.19984894
 28.19984894 28.19984894 28.19984894 28.19984894]
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [9.20209687e-06 7.70396484e-05 8.87104805e-06 3.52735562e-07
 1.18467487e-04 1.29360722e-05 6.97755096e-07 2.48777379e-05
 1.13502559e-05 6.80028536e-06]
ene_total = [0.47315021 0.34937218 0.48643187 0.42981882 0.34984932 0.35127413
 0.42839839 0.40391691 0.60558136 0.34567345]
optimize_network iter = 0 obj = 4.223466627519707
eta = 0.6750735953751348
freqs = [33693065.79010019 67222802.04491435 33347173.22625211 11292050.13707799
 77574909.50601687 37111387.37372465 14172192.11427163 46479487.54198894
 36839485.23777357 29929879.4291095 ]
eta_min = 0.6750735953751431	eta_max = 0.675073595375134
af = 0.01091581578850546	bf = 1.3680266359327833	zeta = 0.012007397367356006	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [2.23206564e-06 1.86867792e-05 2.15176626e-06 8.55597305e-08
 2.87355384e-05 3.13778074e-06 1.69247857e-07 6.03435770e-06
 2.75312427e-06 1.64948093e-06]
ene_total = [1.68991368 1.23854826 1.73742856 1.53620773 1.23478239 1.25381221
 1.53108527 1.44039041 2.16296411 1.23460465]
ti_comp = [0.4811663  0.50721776 0.47844041 0.48996025 0.50753423 0.50618685
 0.49025487 0.4955151  0.45404092 0.50727357]
ti_coms = [0.09689836 0.07084691 0.09962426 0.08810442 0.07053044 0.07187782
 0.08780979 0.08254957 0.12402375 0.0707911 ]
t_total = [28.19984894 28.19984894 28.19984894 28.19984894 28.19984894 28.19984894
 28.19984894 28.19984894 28.19984894 28.19984894]
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [9.20209687e-06 7.70396484e-05 8.87104805e-06 3.52735562e-07
 1.18467487e-04 1.29360722e-05 6.97755096e-07 2.48777379e-05
 1.13502559e-05 6.80028536e-06]
ene_total = [0.47315021 0.34937218 0.48643187 0.42981882 0.34984932 0.35127413
 0.42839839 0.40391691 0.60558136 0.34567345]
optimize_network iter = 1 obj = 4.223466627519696
eta = 0.675073595375134
freqs = [33693065.7901002  67222802.04491436 33347173.22625211 11292050.13707799
 77574909.50601688 37111387.37372466 14172192.11427163 46479487.54198895
 36839485.23777357 29929879.42910951]
Done!
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [8.58228348e-06 7.18505913e-05 8.27353268e-06 3.28976822e-07
 1.10488030e-04 1.20647544e-05 6.50757335e-07 2.32020813e-05
 1.05857518e-05 6.34224759e-06]
ene_total = [0.00969842 0.00715654 0.0099707  0.00881077 0.00716353 0.00719985
 0.00878163 0.00827816 0.01241296 0.00708545]
At round 36 energy consumption: 0.08655801199959033
At round 36 eta: 0.675073595375134
At round 36 a_n: 15.508406518389636
At round 36 local rounds: 12.866641222936334
At round 36 global rounds: 48.78320795030245
gradient difference: 0.48461294174194336
train() client id: f_00000-0-0 loss: 0.985313  [   32/  126]
train() client id: f_00000-0-1 loss: 1.260783  [   64/  126]
train() client id: f_00000-0-2 loss: 1.386651  [   96/  126]
train() client id: f_00000-1-0 loss: 1.123960  [   32/  126]
train() client id: f_00000-1-1 loss: 1.223496  [   64/  126]
train() client id: f_00000-1-2 loss: 0.965750  [   96/  126]
train() client id: f_00000-2-0 loss: 0.941674  [   32/  126]
train() client id: f_00000-2-1 loss: 1.120240  [   64/  126]
train() client id: f_00000-2-2 loss: 1.023802  [   96/  126]
train() client id: f_00000-3-0 loss: 0.918604  [   32/  126]
train() client id: f_00000-3-1 loss: 1.035740  [   64/  126]
train() client id: f_00000-3-2 loss: 0.938549  [   96/  126]
train() client id: f_00000-4-0 loss: 0.830512  [   32/  126]
train() client id: f_00000-4-1 loss: 0.892671  [   64/  126]
train() client id: f_00000-4-2 loss: 0.962276  [   96/  126]
train() client id: f_00000-5-0 loss: 0.819770  [   32/  126]
train() client id: f_00000-5-1 loss: 0.809850  [   64/  126]
train() client id: f_00000-5-2 loss: 0.822796  [   96/  126]
train() client id: f_00000-6-0 loss: 0.782848  [   32/  126]
train() client id: f_00000-6-1 loss: 0.811724  [   64/  126]
train() client id: f_00000-6-2 loss: 0.793020  [   96/  126]
train() client id: f_00000-7-0 loss: 0.795607  [   32/  126]
train() client id: f_00000-7-1 loss: 0.626035  [   64/  126]
train() client id: f_00000-7-2 loss: 0.738848  [   96/  126]
train() client id: f_00000-8-0 loss: 0.788383  [   32/  126]
train() client id: f_00000-8-1 loss: 0.757959  [   64/  126]
train() client id: f_00000-8-2 loss: 0.636871  [   96/  126]
train() client id: f_00000-9-0 loss: 0.816118  [   32/  126]
train() client id: f_00000-9-1 loss: 0.661018  [   64/  126]
train() client id: f_00000-9-2 loss: 0.679187  [   96/  126]
train() client id: f_00000-10-0 loss: 0.756706  [   32/  126]
train() client id: f_00000-10-1 loss: 0.647245  [   64/  126]
train() client id: f_00000-10-2 loss: 0.631815  [   96/  126]
train() client id: f_00000-11-0 loss: 0.643341  [   32/  126]
train() client id: f_00000-11-1 loss: 0.676148  [   64/  126]
train() client id: f_00000-11-2 loss: 0.572938  [   96/  126]
train() client id: f_00001-0-0 loss: 0.474316  [   32/  265]
train() client id: f_00001-0-1 loss: 0.631036  [   64/  265]
train() client id: f_00001-0-2 loss: 0.393310  [   96/  265]
train() client id: f_00001-0-3 loss: 0.506306  [  128/  265]
train() client id: f_00001-0-4 loss: 0.357220  [  160/  265]
train() client id: f_00001-0-5 loss: 0.418194  [  192/  265]
train() client id: f_00001-0-6 loss: 0.397556  [  224/  265]
train() client id: f_00001-0-7 loss: 0.512702  [  256/  265]
train() client id: f_00001-1-0 loss: 0.429773  [   32/  265]
train() client id: f_00001-1-1 loss: 0.442301  [   64/  265]
train() client id: f_00001-1-2 loss: 0.547224  [   96/  265]
train() client id: f_00001-1-3 loss: 0.378427  [  128/  265]
train() client id: f_00001-1-4 loss: 0.667302  [  160/  265]
train() client id: f_00001-1-5 loss: 0.408972  [  192/  265]
train() client id: f_00001-1-6 loss: 0.372465  [  224/  265]
train() client id: f_00001-1-7 loss: 0.367624  [  256/  265]
train() client id: f_00001-2-0 loss: 0.406632  [   32/  265]
train() client id: f_00001-2-1 loss: 0.470369  [   64/  265]
train() client id: f_00001-2-2 loss: 0.367633  [   96/  265]
train() client id: f_00001-2-3 loss: 0.436182  [  128/  265]
train() client id: f_00001-2-4 loss: 0.550624  [  160/  265]
train() client id: f_00001-2-5 loss: 0.395557  [  192/  265]
train() client id: f_00001-2-6 loss: 0.437061  [  224/  265]
train() client id: f_00001-2-7 loss: 0.483341  [  256/  265]
train() client id: f_00001-3-0 loss: 0.473629  [   32/  265]
train() client id: f_00001-3-1 loss: 0.393851  [   64/  265]
train() client id: f_00001-3-2 loss: 0.351387  [   96/  265]
train() client id: f_00001-3-3 loss: 0.555357  [  128/  265]
train() client id: f_00001-3-4 loss: 0.454345  [  160/  265]
train() client id: f_00001-3-5 loss: 0.422564  [  192/  265]
train() client id: f_00001-3-6 loss: 0.429093  [  224/  265]
train() client id: f_00001-3-7 loss: 0.426708  [  256/  265]
train() client id: f_00001-4-0 loss: 0.375380  [   32/  265]
train() client id: f_00001-4-1 loss: 0.623404  [   64/  265]
train() client id: f_00001-4-2 loss: 0.343651  [   96/  265]
train() client id: f_00001-4-3 loss: 0.406133  [  128/  265]
train() client id: f_00001-4-4 loss: 0.343137  [  160/  265]
train() client id: f_00001-4-5 loss: 0.380200  [  192/  265]
train() client id: f_00001-4-6 loss: 0.472097  [  224/  265]
train() client id: f_00001-4-7 loss: 0.460890  [  256/  265]
train() client id: f_00001-5-0 loss: 0.366687  [   32/  265]
train() client id: f_00001-5-1 loss: 0.555415  [   64/  265]
train() client id: f_00001-5-2 loss: 0.456699  [   96/  265]
train() client id: f_00001-5-3 loss: 0.372325  [  128/  265]
train() client id: f_00001-5-4 loss: 0.394156  [  160/  265]
train() client id: f_00001-5-5 loss: 0.354621  [  192/  265]
train() client id: f_00001-5-6 loss: 0.484397  [  224/  265]
train() client id: f_00001-5-7 loss: 0.462924  [  256/  265]
train() client id: f_00001-6-0 loss: 0.530897  [   32/  265]
train() client id: f_00001-6-1 loss: 0.429879  [   64/  265]
train() client id: f_00001-6-2 loss: 0.410367  [   96/  265]
train() client id: f_00001-6-3 loss: 0.336783  [  128/  265]
train() client id: f_00001-6-4 loss: 0.437848  [  160/  265]
train() client id: f_00001-6-5 loss: 0.350721  [  192/  265]
train() client id: f_00001-6-6 loss: 0.406940  [  224/  265]
train() client id: f_00001-6-7 loss: 0.525954  [  256/  265]
train() client id: f_00001-7-0 loss: 0.322569  [   32/  265]
train() client id: f_00001-7-1 loss: 0.495930  [   64/  265]
train() client id: f_00001-7-2 loss: 0.543153  [   96/  265]
train() client id: f_00001-7-3 loss: 0.473466  [  128/  265]
train() client id: f_00001-7-4 loss: 0.358238  [  160/  265]
train() client id: f_00001-7-5 loss: 0.508535  [  192/  265]
train() client id: f_00001-7-6 loss: 0.308832  [  224/  265]
train() client id: f_00001-7-7 loss: 0.340190  [  256/  265]
train() client id: f_00001-8-0 loss: 0.461901  [   32/  265]
train() client id: f_00001-8-1 loss: 0.415833  [   64/  265]
train() client id: f_00001-8-2 loss: 0.326363  [   96/  265]
train() client id: f_00001-8-3 loss: 0.500739  [  128/  265]
train() client id: f_00001-8-4 loss: 0.561454  [  160/  265]
train() client id: f_00001-8-5 loss: 0.339455  [  192/  265]
train() client id: f_00001-8-6 loss: 0.386252  [  224/  265]
train() client id: f_00001-8-7 loss: 0.387024  [  256/  265]
train() client id: f_00001-9-0 loss: 0.324302  [   32/  265]
train() client id: f_00001-9-1 loss: 0.356521  [   64/  265]
train() client id: f_00001-9-2 loss: 0.390217  [   96/  265]
train() client id: f_00001-9-3 loss: 0.472699  [  128/  265]
train() client id: f_00001-9-4 loss: 0.445131  [  160/  265]
train() client id: f_00001-9-5 loss: 0.488067  [  192/  265]
train() client id: f_00001-9-6 loss: 0.476460  [  224/  265]
train() client id: f_00001-9-7 loss: 0.403888  [  256/  265]
train() client id: f_00001-10-0 loss: 0.402568  [   32/  265]
train() client id: f_00001-10-1 loss: 0.357281  [   64/  265]
train() client id: f_00001-10-2 loss: 0.540257  [   96/  265]
train() client id: f_00001-10-3 loss: 0.372293  [  128/  265]
train() client id: f_00001-10-4 loss: 0.327295  [  160/  265]
train() client id: f_00001-10-5 loss: 0.321481  [  192/  265]
train() client id: f_00001-10-6 loss: 0.326923  [  224/  265]
train() client id: f_00001-10-7 loss: 0.603006  [  256/  265]
train() client id: f_00001-11-0 loss: 0.336939  [   32/  265]
train() client id: f_00001-11-1 loss: 0.399893  [   64/  265]
train() client id: f_00001-11-2 loss: 0.424481  [   96/  265]
train() client id: f_00001-11-3 loss: 0.462577  [  128/  265]
train() client id: f_00001-11-4 loss: 0.380191  [  160/  265]
train() client id: f_00001-11-5 loss: 0.425464  [  192/  265]
train() client id: f_00001-11-6 loss: 0.463482  [  224/  265]
train() client id: f_00001-11-7 loss: 0.413835  [  256/  265]
train() client id: f_00002-0-0 loss: 0.981521  [   32/  124]
train() client id: f_00002-0-1 loss: 1.274504  [   64/  124]
train() client id: f_00002-0-2 loss: 1.020781  [   96/  124]
train() client id: f_00002-1-0 loss: 1.005757  [   32/  124]
train() client id: f_00002-1-1 loss: 1.233831  [   64/  124]
train() client id: f_00002-1-2 loss: 1.052524  [   96/  124]
train() client id: f_00002-2-0 loss: 1.024348  [   32/  124]
train() client id: f_00002-2-1 loss: 1.004359  [   64/  124]
train() client id: f_00002-2-2 loss: 0.985005  [   96/  124]
train() client id: f_00002-3-0 loss: 1.118384  [   32/  124]
train() client id: f_00002-3-1 loss: 1.001352  [   64/  124]
train() client id: f_00002-3-2 loss: 1.026921  [   96/  124]
train() client id: f_00002-4-0 loss: 1.134851  [   32/  124]
train() client id: f_00002-4-1 loss: 0.929294  [   64/  124]
train() client id: f_00002-4-2 loss: 0.901631  [   96/  124]
train() client id: f_00002-5-0 loss: 1.182553  [   32/  124]
train() client id: f_00002-5-1 loss: 0.851810  [   64/  124]
train() client id: f_00002-5-2 loss: 0.865099  [   96/  124]
train() client id: f_00002-6-0 loss: 1.145342  [   32/  124]
train() client id: f_00002-6-1 loss: 0.899410  [   64/  124]
train() client id: f_00002-6-2 loss: 0.902390  [   96/  124]
train() client id: f_00002-7-0 loss: 0.842964  [   32/  124]
train() client id: f_00002-7-1 loss: 0.930166  [   64/  124]
train() client id: f_00002-7-2 loss: 0.991463  [   96/  124]
train() client id: f_00002-8-0 loss: 0.889217  [   32/  124]
train() client id: f_00002-8-1 loss: 1.051625  [   64/  124]
train() client id: f_00002-8-2 loss: 0.970630  [   96/  124]
train() client id: f_00002-9-0 loss: 0.848293  [   32/  124]
train() client id: f_00002-9-1 loss: 0.828426  [   64/  124]
train() client id: f_00002-9-2 loss: 1.120312  [   96/  124]
train() client id: f_00002-10-0 loss: 0.912493  [   32/  124]
train() client id: f_00002-10-1 loss: 0.742257  [   64/  124]
train() client id: f_00002-10-2 loss: 0.973386  [   96/  124]
train() client id: f_00002-11-0 loss: 0.870269  [   32/  124]
train() client id: f_00002-11-1 loss: 0.998748  [   64/  124]
train() client id: f_00002-11-2 loss: 0.914874  [   96/  124]
train() client id: f_00003-0-0 loss: 0.310842  [   32/   43]
train() client id: f_00003-1-0 loss: 0.419083  [   32/   43]
train() client id: f_00003-2-0 loss: 0.483118  [   32/   43]
train() client id: f_00003-3-0 loss: 0.524766  [   32/   43]
train() client id: f_00003-4-0 loss: 0.360944  [   32/   43]
train() client id: f_00003-5-0 loss: 0.531156  [   32/   43]
train() client id: f_00003-6-0 loss: 0.557883  [   32/   43]
train() client id: f_00003-7-0 loss: 0.398562  [   32/   43]
train() client id: f_00003-8-0 loss: 0.359841  [   32/   43]
train() client id: f_00003-9-0 loss: 0.476634  [   32/   43]
train() client id: f_00003-10-0 loss: 0.610321  [   32/   43]
train() client id: f_00003-11-0 loss: 0.445842  [   32/   43]
train() client id: f_00004-0-0 loss: 0.817190  [   32/  306]
train() client id: f_00004-0-1 loss: 0.897433  [   64/  306]
train() client id: f_00004-0-2 loss: 0.874021  [   96/  306]
train() client id: f_00004-0-3 loss: 0.757129  [  128/  306]
train() client id: f_00004-0-4 loss: 0.918583  [  160/  306]
train() client id: f_00004-0-5 loss: 0.825599  [  192/  306]
train() client id: f_00004-0-6 loss: 0.770562  [  224/  306]
train() client id: f_00004-0-7 loss: 0.750889  [  256/  306]
train() client id: f_00004-0-8 loss: 0.970945  [  288/  306]
train() client id: f_00004-1-0 loss: 0.895649  [   32/  306]
train() client id: f_00004-1-1 loss: 0.825744  [   64/  306]
train() client id: f_00004-1-2 loss: 0.734952  [   96/  306]
train() client id: f_00004-1-3 loss: 0.835671  [  128/  306]
train() client id: f_00004-1-4 loss: 0.834728  [  160/  306]
train() client id: f_00004-1-5 loss: 0.883793  [  192/  306]
train() client id: f_00004-1-6 loss: 0.785297  [  224/  306]
train() client id: f_00004-1-7 loss: 0.949294  [  256/  306]
train() client id: f_00004-1-8 loss: 0.762867  [  288/  306]
train() client id: f_00004-2-0 loss: 0.852437  [   32/  306]
train() client id: f_00004-2-1 loss: 0.807774  [   64/  306]
train() client id: f_00004-2-2 loss: 0.895826  [   96/  306]
train() client id: f_00004-2-3 loss: 1.053162  [  128/  306]
train() client id: f_00004-2-4 loss: 0.834913  [  160/  306]
train() client id: f_00004-2-5 loss: 0.703470  [  192/  306]
train() client id: f_00004-2-6 loss: 0.956189  [  224/  306]
train() client id: f_00004-2-7 loss: 0.808140  [  256/  306]
train() client id: f_00004-2-8 loss: 0.745308  [  288/  306]
train() client id: f_00004-3-0 loss: 0.966925  [   32/  306]
train() client id: f_00004-3-1 loss: 1.025274  [   64/  306]
train() client id: f_00004-3-2 loss: 0.809969  [   96/  306]
train() client id: f_00004-3-3 loss: 0.840450  [  128/  306]
train() client id: f_00004-3-4 loss: 0.911705  [  160/  306]
train() client id: f_00004-3-5 loss: 0.769913  [  192/  306]
train() client id: f_00004-3-6 loss: 0.785612  [  224/  306]
train() client id: f_00004-3-7 loss: 0.782217  [  256/  306]
train() client id: f_00004-3-8 loss: 0.744865  [  288/  306]
train() client id: f_00004-4-0 loss: 1.009717  [   32/  306]
train() client id: f_00004-4-1 loss: 0.675379  [   64/  306]
train() client id: f_00004-4-2 loss: 0.798255  [   96/  306]
train() client id: f_00004-4-3 loss: 0.811346  [  128/  306]
train() client id: f_00004-4-4 loss: 0.799350  [  160/  306]
train() client id: f_00004-4-5 loss: 0.959787  [  192/  306]
train() client id: f_00004-4-6 loss: 0.956180  [  224/  306]
train() client id: f_00004-4-7 loss: 0.698365  [  256/  306]
train() client id: f_00004-4-8 loss: 0.856154  [  288/  306]
train() client id: f_00004-5-0 loss: 0.794241  [   32/  306]
train() client id: f_00004-5-1 loss: 0.757519  [   64/  306]
train() client id: f_00004-5-2 loss: 0.865871  [   96/  306]
train() client id: f_00004-5-3 loss: 0.860841  [  128/  306]
train() client id: f_00004-5-4 loss: 0.905245  [  160/  306]
train() client id: f_00004-5-5 loss: 0.886281  [  192/  306]
train() client id: f_00004-5-6 loss: 0.815938  [  224/  306]
train() client id: f_00004-5-7 loss: 0.930913  [  256/  306]
train() client id: f_00004-5-8 loss: 0.765342  [  288/  306]
train() client id: f_00004-6-0 loss: 0.734714  [   32/  306]
train() client id: f_00004-6-1 loss: 0.674923  [   64/  306]
train() client id: f_00004-6-2 loss: 1.016391  [   96/  306]
train() client id: f_00004-6-3 loss: 0.801207  [  128/  306]
train() client id: f_00004-6-4 loss: 0.873775  [  160/  306]
train() client id: f_00004-6-5 loss: 0.768334  [  192/  306]
train() client id: f_00004-6-6 loss: 0.896168  [  224/  306]
train() client id: f_00004-6-7 loss: 0.846986  [  256/  306]
train() client id: f_00004-6-8 loss: 0.753870  [  288/  306]
train() client id: f_00004-7-0 loss: 0.749474  [   32/  306]
train() client id: f_00004-7-1 loss: 0.895802  [   64/  306]
train() client id: f_00004-7-2 loss: 0.738342  [   96/  306]
train() client id: f_00004-7-3 loss: 0.968430  [  128/  306]
train() client id: f_00004-7-4 loss: 0.842477  [  160/  306]
train() client id: f_00004-7-5 loss: 0.839655  [  192/  306]
train() client id: f_00004-7-6 loss: 0.741165  [  224/  306]
train() client id: f_00004-7-7 loss: 0.853106  [  256/  306]
train() client id: f_00004-7-8 loss: 0.818094  [  288/  306]
train() client id: f_00004-8-0 loss: 0.879146  [   32/  306]
train() client id: f_00004-8-1 loss: 0.796685  [   64/  306]
train() client id: f_00004-8-2 loss: 0.814815  [   96/  306]
train() client id: f_00004-8-3 loss: 0.863017  [  128/  306]
train() client id: f_00004-8-4 loss: 0.807668  [  160/  306]
train() client id: f_00004-8-5 loss: 0.873040  [  192/  306]
train() client id: f_00004-8-6 loss: 0.809306  [  224/  306]
train() client id: f_00004-8-7 loss: 0.779582  [  256/  306]
train() client id: f_00004-8-8 loss: 0.894638  [  288/  306]
train() client id: f_00004-9-0 loss: 0.838545  [   32/  306]
train() client id: f_00004-9-1 loss: 0.853384  [   64/  306]
train() client id: f_00004-9-2 loss: 0.771837  [   96/  306]
train() client id: f_00004-9-3 loss: 0.818820  [  128/  306]
train() client id: f_00004-9-4 loss: 0.831509  [  160/  306]
train() client id: f_00004-9-5 loss: 0.795420  [  192/  306]
train() client id: f_00004-9-6 loss: 0.933543  [  224/  306]
train() client id: f_00004-9-7 loss: 0.930648  [  256/  306]
train() client id: f_00004-9-8 loss: 0.709550  [  288/  306]
train() client id: f_00004-10-0 loss: 0.767562  [   32/  306]
train() client id: f_00004-10-1 loss: 0.905162  [   64/  306]
train() client id: f_00004-10-2 loss: 0.799212  [   96/  306]
train() client id: f_00004-10-3 loss: 0.770009  [  128/  306]
train() client id: f_00004-10-4 loss: 0.892942  [  160/  306]
train() client id: f_00004-10-5 loss: 0.821509  [  192/  306]
train() client id: f_00004-10-6 loss: 0.780566  [  224/  306]
train() client id: f_00004-10-7 loss: 0.753866  [  256/  306]
train() client id: f_00004-10-8 loss: 0.956062  [  288/  306]
train() client id: f_00004-11-0 loss: 0.779325  [   32/  306]
train() client id: f_00004-11-1 loss: 0.887944  [   64/  306]
train() client id: f_00004-11-2 loss: 0.770498  [   96/  306]
train() client id: f_00004-11-3 loss: 0.910825  [  128/  306]
train() client id: f_00004-11-4 loss: 0.765948  [  160/  306]
train() client id: f_00004-11-5 loss: 0.907001  [  192/  306]
train() client id: f_00004-11-6 loss: 0.940348  [  224/  306]
train() client id: f_00004-11-7 loss: 0.746843  [  256/  306]
train() client id: f_00004-11-8 loss: 0.790790  [  288/  306]
train() client id: f_00005-0-0 loss: 0.495319  [   32/  146]
train() client id: f_00005-0-1 loss: 0.412728  [   64/  146]
train() client id: f_00005-0-2 loss: 0.489086  [   96/  146]
train() client id: f_00005-0-3 loss: 0.650139  [  128/  146]
train() client id: f_00005-1-0 loss: 0.593499  [   32/  146]
train() client id: f_00005-1-1 loss: 0.514419  [   64/  146]
train() client id: f_00005-1-2 loss: 0.367961  [   96/  146]
train() client id: f_00005-1-3 loss: 0.717397  [  128/  146]
train() client id: f_00005-2-0 loss: 0.762128  [   32/  146]
train() client id: f_00005-2-1 loss: 0.472721  [   64/  146]
train() client id: f_00005-2-2 loss: 0.439518  [   96/  146]
train() client id: f_00005-2-3 loss: 0.534113  [  128/  146]
train() client id: f_00005-3-0 loss: 0.521190  [   32/  146]
train() client id: f_00005-3-1 loss: 0.576872  [   64/  146]
train() client id: f_00005-3-2 loss: 0.577329  [   96/  146]
train() client id: f_00005-3-3 loss: 0.670877  [  128/  146]
train() client id: f_00005-4-0 loss: 0.505503  [   32/  146]
train() client id: f_00005-4-1 loss: 0.592741  [   64/  146]
train() client id: f_00005-4-2 loss: 0.596649  [   96/  146]
train() client id: f_00005-4-3 loss: 0.629162  [  128/  146]
train() client id: f_00005-5-0 loss: 0.451754  [   32/  146]
train() client id: f_00005-5-1 loss: 0.760613  [   64/  146]
train() client id: f_00005-5-2 loss: 0.407631  [   96/  146]
train() client id: f_00005-5-3 loss: 0.589220  [  128/  146]
train() client id: f_00005-6-0 loss: 0.706847  [   32/  146]
train() client id: f_00005-6-1 loss: 0.497655  [   64/  146]
train() client id: f_00005-6-2 loss: 0.780940  [   96/  146]
train() client id: f_00005-6-3 loss: 0.240104  [  128/  146]
train() client id: f_00005-7-0 loss: 0.608076  [   32/  146]
train() client id: f_00005-7-1 loss: 0.585983  [   64/  146]
train() client id: f_00005-7-2 loss: 0.471710  [   96/  146]
train() client id: f_00005-7-3 loss: 0.604361  [  128/  146]
train() client id: f_00005-8-0 loss: 0.622632  [   32/  146]
train() client id: f_00005-8-1 loss: 0.674979  [   64/  146]
train() client id: f_00005-8-2 loss: 0.575770  [   96/  146]
train() client id: f_00005-8-3 loss: 0.317077  [  128/  146]
train() client id: f_00005-9-0 loss: 0.318976  [   32/  146]
train() client id: f_00005-9-1 loss: 0.411545  [   64/  146]
train() client id: f_00005-9-2 loss: 0.683356  [   96/  146]
train() client id: f_00005-9-3 loss: 0.673143  [  128/  146]
train() client id: f_00005-10-0 loss: 0.678482  [   32/  146]
train() client id: f_00005-10-1 loss: 0.394169  [   64/  146]
train() client id: f_00005-10-2 loss: 0.712725  [   96/  146]
train() client id: f_00005-10-3 loss: 0.522478  [  128/  146]
train() client id: f_00005-11-0 loss: 0.600053  [   32/  146]
train() client id: f_00005-11-1 loss: 0.663346  [   64/  146]
train() client id: f_00005-11-2 loss: 0.378888  [   96/  146]
train() client id: f_00005-11-3 loss: 0.577106  [  128/  146]
train() client id: f_00006-0-0 loss: 0.585020  [   32/   54]
train() client id: f_00006-1-0 loss: 0.625859  [   32/   54]
train() client id: f_00006-2-0 loss: 0.626345  [   32/   54]
train() client id: f_00006-3-0 loss: 0.633471  [   32/   54]
train() client id: f_00006-4-0 loss: 0.625096  [   32/   54]
train() client id: f_00006-5-0 loss: 0.553448  [   32/   54]
train() client id: f_00006-6-0 loss: 0.589189  [   32/   54]
train() client id: f_00006-7-0 loss: 0.615738  [   32/   54]
train() client id: f_00006-8-0 loss: 0.532277  [   32/   54]
train() client id: f_00006-9-0 loss: 0.571964  [   32/   54]
train() client id: f_00006-10-0 loss: 0.619716  [   32/   54]
train() client id: f_00006-11-0 loss: 0.596140  [   32/   54]
train() client id: f_00007-0-0 loss: 0.750375  [   32/  179]
train() client id: f_00007-0-1 loss: 0.637430  [   64/  179]
train() client id: f_00007-0-2 loss: 0.645845  [   96/  179]
train() client id: f_00007-0-3 loss: 0.670749  [  128/  179]
train() client id: f_00007-0-4 loss: 0.492843  [  160/  179]
train() client id: f_00007-1-0 loss: 0.536904  [   32/  179]
train() client id: f_00007-1-1 loss: 0.921134  [   64/  179]
train() client id: f_00007-1-2 loss: 0.523136  [   96/  179]
train() client id: f_00007-1-3 loss: 0.533187  [  128/  179]
train() client id: f_00007-1-4 loss: 0.713142  [  160/  179]
train() client id: f_00007-2-0 loss: 0.616376  [   32/  179]
train() client id: f_00007-2-1 loss: 0.580560  [   64/  179]
train() client id: f_00007-2-2 loss: 0.661433  [   96/  179]
train() client id: f_00007-2-3 loss: 0.738474  [  128/  179]
train() client id: f_00007-2-4 loss: 0.644322  [  160/  179]
train() client id: f_00007-3-0 loss: 0.574014  [   32/  179]
train() client id: f_00007-3-1 loss: 0.681961  [   64/  179]
train() client id: f_00007-3-2 loss: 0.691231  [   96/  179]
train() client id: f_00007-3-3 loss: 0.538654  [  128/  179]
train() client id: f_00007-3-4 loss: 0.569759  [  160/  179]
train() client id: f_00007-4-0 loss: 0.579145  [   32/  179]
train() client id: f_00007-4-1 loss: 0.827017  [   64/  179]
train() client id: f_00007-4-2 loss: 0.648008  [   96/  179]
train() client id: f_00007-4-3 loss: 0.518916  [  128/  179]
train() client id: f_00007-4-4 loss: 0.636569  [  160/  179]
train() client id: f_00007-5-0 loss: 0.746811  [   32/  179]
train() client id: f_00007-5-1 loss: 0.818161  [   64/  179]
train() client id: f_00007-5-2 loss: 0.476424  [   96/  179]
train() client id: f_00007-5-3 loss: 0.619587  [  128/  179]
train() client id: f_00007-5-4 loss: 0.519182  [  160/  179]
train() client id: f_00007-6-0 loss: 0.758837  [   32/  179]
train() client id: f_00007-6-1 loss: 0.468179  [   64/  179]
train() client id: f_00007-6-2 loss: 0.625530  [   96/  179]
train() client id: f_00007-6-3 loss: 0.579296  [  128/  179]
train() client id: f_00007-6-4 loss: 0.659348  [  160/  179]
train() client id: f_00007-7-0 loss: 0.610219  [   32/  179]
train() client id: f_00007-7-1 loss: 0.551048  [   64/  179]
train() client id: f_00007-7-2 loss: 0.657002  [   96/  179]
train() client id: f_00007-7-3 loss: 0.552624  [  128/  179]
train() client id: f_00007-7-4 loss: 0.647603  [  160/  179]
train() client id: f_00007-8-0 loss: 0.554371  [   32/  179]
train() client id: f_00007-8-1 loss: 0.457685  [   64/  179]
train() client id: f_00007-8-2 loss: 0.489612  [   96/  179]
train() client id: f_00007-8-3 loss: 0.596651  [  128/  179]
train() client id: f_00007-8-4 loss: 0.912445  [  160/  179]
train() client id: f_00007-9-0 loss: 0.660980  [   32/  179]
train() client id: f_00007-9-1 loss: 0.607262  [   64/  179]
train() client id: f_00007-9-2 loss: 0.628906  [   96/  179]
train() client id: f_00007-9-3 loss: 0.706856  [  128/  179]
train() client id: f_00007-9-4 loss: 0.582482  [  160/  179]
train() client id: f_00007-10-0 loss: 0.760692  [   32/  179]
train() client id: f_00007-10-1 loss: 0.641751  [   64/  179]
train() client id: f_00007-10-2 loss: 0.502055  [   96/  179]
train() client id: f_00007-10-3 loss: 0.599778  [  128/  179]
train() client id: f_00007-10-4 loss: 0.680644  [  160/  179]
train() client id: f_00007-11-0 loss: 0.696564  [   32/  179]
train() client id: f_00007-11-1 loss: 0.723580  [   64/  179]
train() client id: f_00007-11-2 loss: 0.562152  [   96/  179]
train() client id: f_00007-11-3 loss: 0.491980  [  128/  179]
train() client id: f_00007-11-4 loss: 0.578467  [  160/  179]
train() client id: f_00008-0-0 loss: 0.663134  [   32/  130]
train() client id: f_00008-0-1 loss: 0.869326  [   64/  130]
train() client id: f_00008-0-2 loss: 0.658177  [   96/  130]
train() client id: f_00008-0-3 loss: 0.715518  [  128/  130]
train() client id: f_00008-1-0 loss: 0.681424  [   32/  130]
train() client id: f_00008-1-1 loss: 0.723646  [   64/  130]
train() client id: f_00008-1-2 loss: 0.772722  [   96/  130]
train() client id: f_00008-1-3 loss: 0.724894  [  128/  130]
train() client id: f_00008-2-0 loss: 0.668283  [   32/  130]
train() client id: f_00008-2-1 loss: 0.639499  [   64/  130]
train() client id: f_00008-2-2 loss: 0.852950  [   96/  130]
train() client id: f_00008-2-3 loss: 0.745627  [  128/  130]
train() client id: f_00008-3-0 loss: 0.713278  [   32/  130]
train() client id: f_00008-3-1 loss: 0.638010  [   64/  130]
train() client id: f_00008-3-2 loss: 0.805538  [   96/  130]
train() client id: f_00008-3-3 loss: 0.737278  [  128/  130]
train() client id: f_00008-4-0 loss: 0.806266  [   32/  130]
train() client id: f_00008-4-1 loss: 0.696543  [   64/  130]
train() client id: f_00008-4-2 loss: 0.691986  [   96/  130]
train() client id: f_00008-4-3 loss: 0.747067  [  128/  130]
train() client id: f_00008-5-0 loss: 0.809552  [   32/  130]
train() client id: f_00008-5-1 loss: 0.735044  [   64/  130]
train() client id: f_00008-5-2 loss: 0.668799  [   96/  130]
train() client id: f_00008-5-3 loss: 0.688409  [  128/  130]
train() client id: f_00008-6-0 loss: 0.768103  [   32/  130]
train() client id: f_00008-6-1 loss: 0.669995  [   64/  130]
train() client id: f_00008-6-2 loss: 0.694415  [   96/  130]
train() client id: f_00008-6-3 loss: 0.789121  [  128/  130]
train() client id: f_00008-7-0 loss: 0.708429  [   32/  130]
train() client id: f_00008-7-1 loss: 0.713088  [   64/  130]
train() client id: f_00008-7-2 loss: 0.746623  [   96/  130]
train() client id: f_00008-7-3 loss: 0.769574  [  128/  130]
train() client id: f_00008-8-0 loss: 0.769399  [   32/  130]
train() client id: f_00008-8-1 loss: 0.685465  [   64/  130]
train() client id: f_00008-8-2 loss: 0.692942  [   96/  130]
train() client id: f_00008-8-3 loss: 0.759060  [  128/  130]
train() client id: f_00008-9-0 loss: 0.684260  [   32/  130]
train() client id: f_00008-9-1 loss: 0.784941  [   64/  130]
train() client id: f_00008-9-2 loss: 0.748247  [   96/  130]
train() client id: f_00008-9-3 loss: 0.684066  [  128/  130]
train() client id: f_00008-10-0 loss: 0.817882  [   32/  130]
train() client id: f_00008-10-1 loss: 0.699805  [   64/  130]
train() client id: f_00008-10-2 loss: 0.720470  [   96/  130]
train() client id: f_00008-10-3 loss: 0.694778  [  128/  130]
train() client id: f_00008-11-0 loss: 0.672082  [   32/  130]
train() client id: f_00008-11-1 loss: 0.709570  [   64/  130]
train() client id: f_00008-11-2 loss: 0.765375  [   96/  130]
train() client id: f_00008-11-3 loss: 0.781180  [  128/  130]
train() client id: f_00009-0-0 loss: 1.078357  [   32/  118]
train() client id: f_00009-0-1 loss: 0.938288  [   64/  118]
train() client id: f_00009-0-2 loss: 1.091089  [   96/  118]
train() client id: f_00009-1-0 loss: 0.940607  [   32/  118]
train() client id: f_00009-1-1 loss: 0.949393  [   64/  118]
train() client id: f_00009-1-2 loss: 1.096347  [   96/  118]
train() client id: f_00009-2-0 loss: 0.881320  [   32/  118]
train() client id: f_00009-2-1 loss: 0.944165  [   64/  118]
train() client id: f_00009-2-2 loss: 1.025366  [   96/  118]
train() client id: f_00009-3-0 loss: 0.997303  [   32/  118]
train() client id: f_00009-3-1 loss: 0.946543  [   64/  118]
train() client id: f_00009-3-2 loss: 0.841298  [   96/  118]
train() client id: f_00009-4-0 loss: 0.743240  [   32/  118]
train() client id: f_00009-4-1 loss: 0.918969  [   64/  118]
train() client id: f_00009-4-2 loss: 0.957347  [   96/  118]
train() client id: f_00009-5-0 loss: 0.827230  [   32/  118]
train() client id: f_00009-5-1 loss: 0.797113  [   64/  118]
train() client id: f_00009-5-2 loss: 0.971016  [   96/  118]
train() client id: f_00009-6-0 loss: 0.819151  [   32/  118]
train() client id: f_00009-6-1 loss: 0.834771  [   64/  118]
train() client id: f_00009-6-2 loss: 0.754099  [   96/  118]
train() client id: f_00009-7-0 loss: 0.923920  [   32/  118]
train() client id: f_00009-7-1 loss: 0.899687  [   64/  118]
train() client id: f_00009-7-2 loss: 0.752819  [   96/  118]
train() client id: f_00009-8-0 loss: 1.189082  [   32/  118]
train() client id: f_00009-8-1 loss: 0.760547  [   64/  118]
train() client id: f_00009-8-2 loss: 0.680047  [   96/  118]
train() client id: f_00009-9-0 loss: 0.811843  [   32/  118]
train() client id: f_00009-9-1 loss: 0.846863  [   64/  118]
train() client id: f_00009-9-2 loss: 0.801127  [   96/  118]
train() client id: f_00009-10-0 loss: 0.850883  [   32/  118]
train() client id: f_00009-10-1 loss: 0.925670  [   64/  118]
train() client id: f_00009-10-2 loss: 0.744408  [   96/  118]
train() client id: f_00009-11-0 loss: 0.897960  [   32/  118]
train() client id: f_00009-11-1 loss: 0.721978  [   64/  118]
train() client id: f_00009-11-2 loss: 0.901685  [   96/  118]
At round 36 accuracy: 0.6525198938992043
At round 36 training accuracy: 0.5902079141515761
At round 36 training loss: 0.8223904404957444
update_location
xs = [  -3.9056584     4.20031788  200.00902392   18.81129433    0.97929623
    3.95640986 -162.44319194 -141.32485185  184.66397685 -127.06087855]
ys = [ 192.5879595   175.55583871    1.32061395 -162.45517586  154.35018685
  137.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [217.03773016 202.08289184 223.6187686  191.69128556 183.91557628
 170.31850552 190.7739006  173.12824728 210.73550797 161.74201283]
dists_bs = [171.99595939 179.20117992 413.27508654 389.1514018  177.18670233
 182.77905793 178.06826916 177.40312868 392.61010444 177.59125147]
uav_gains = [1.32188752e-11 1.65442503e-11 1.19052653e-11 1.92240094e-11
 2.15038573e-11 2.62734338e-11 1.94793158e-11 2.51899720e-11
 1.45569033e-11 2.99729913e-11]
bs_gains = [6.07915198e-11 5.41925596e-11 5.22184833e-12 6.17962338e-12
 5.59354239e-11 5.12743388e-11 5.51634959e-11 5.57445631e-11
 6.02839862e-12 5.55793797e-11]
Round 37
-------------------------------
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.10747648 12.63082676  6.01756463  2.17165072 14.56715747  7.00995328
  2.69087946  8.58799468  6.33825934  5.68528245]
obj_prev = 71.80704525657299
eta_min = 7.508228350489047e-16	eta_max = 0.9300641933191646
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 16.65764355541381	eta = 0.909090909090909
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 30.742318165825036	eta = 0.4925884977645458
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 23.795554951531557	eta = 0.636392483972255
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 22.538890009475697	eta = 0.6718748046925549
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 22.472173027112785	eta = 0.6738695143025546
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 22.47196919842037	eta = 0.6738756265369006
eta = 0.6738756265369006
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [0.0325705  0.06850145 0.03205351 0.01111533 0.07909978 0.03774042
 0.01395879 0.04627079 0.03360448 0.03050253]
ene_total = [2.01826629 3.59433964 2.00757769 0.95394728 4.09618977 2.13534693
 1.08775634 2.59856536 2.19433777 1.78564212]
ti_comp = [0.49324588 0.52112484 0.49032366 0.50250852 0.52157476 0.52032434
 0.50280454 0.5082184  0.46655987 0.52148445]
ti_coms = [0.09871374 0.07083478 0.10163597 0.08945111 0.07038486 0.07163528
 0.08915509 0.08374122 0.12539975 0.07047517]
t_total = [28.14984474 28.14984474 28.14984474 28.14984474 28.14984474 28.14984474
 28.14984474 28.14984474 28.14984474 28.14984474]
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [8.87618492e-06 7.39768305e-05 8.56132548e-06 3.39907040e-07
 1.13703226e-04 1.24094325e-05 6.72395117e-07 2.39717625e-05
 1.08957452e-05 6.52236260e-06]
ene_total = [0.46984203 0.34036317 0.48372332 0.42538889 0.34011276 0.34124206
 0.42399702 0.39936008 0.59683981 0.33544538]
optimize_network iter = 0 obj = 4.156314504851677
eta = 0.6738756265369006
freqs = [33016494.15795401 65724604.40972588 32686070.96869287 11059841.67344831
 75827849.2653193  36266244.90193002 13880926.45922257 45522544.15430705
 36013045.58118701 29245868.53629994]
eta_min = 0.673875626536905	eta_max = 0.6738756265369027
af = 0.01021278956996407	bf = 1.3514443765451492	zeta = 0.011234068526960479	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [2.14332394e-06 1.78631149e-05 2.06729513e-06 8.20770299e-08
 2.74558099e-05 2.99649386e-06 1.62362610e-07 5.78843872e-06
 2.63098523e-06 1.57494870e-06]
ene_total = [1.68434772 1.21143542 1.73418576 1.52598255 1.2053965  1.2225551
 1.52094637 1.42954967 2.1396742  1.20252205]
ti_comp = [0.49324588 0.52112484 0.49032366 0.50250852 0.52157476 0.52032434
 0.50280454 0.5082184  0.46655987 0.52148445]
ti_coms = [0.09871374 0.07083478 0.10163597 0.08945111 0.07038486 0.07163528
 0.08915509 0.08374122 0.12539975 0.07047517]
t_total = [28.14984474 28.14984474 28.14984474 28.14984474 28.14984474 28.14984474
 28.14984474 28.14984474 28.14984474 28.14984474]
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [8.87618492e-06 7.39768305e-05 8.56132548e-06 3.39907040e-07
 1.13703226e-04 1.24094325e-05 6.72395117e-07 2.39717625e-05
 1.08957452e-05 6.52236260e-06]
ene_total = [0.46984203 0.34036317 0.48372332 0.42538889 0.34011276 0.34124206
 0.42399702 0.39936008 0.59683981 0.33544538]
optimize_network iter = 1 obj = 4.156314504851704
eta = 0.6738756265369027
freqs = [33016494.15795401 65724604.40972584 32686070.96869286 11059841.67344831
 75827849.26531927 36266244.90193    13880926.45922257 45522544.15430704
 36013045.58118702 29245868.53629993]
Done!
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [8.24107198e-06 6.86836056e-05 7.94874151e-06 3.15585852e-07
 1.05567480e-04 1.15215069e-05 6.24283587e-07 2.22565238e-05
 1.01161277e-05 6.05567145e-06]
ene_total = [0.00987962 0.00715216 0.01017155 0.00894543 0.00714405 0.00717505
 0.00891613 0.00839638 0.01255009 0.00705357]
At round 37 energy consumption: 0.08738402800819062
At round 37 eta: 0.6738756265369027
At round 37 a_n: 15.165860671420319
At round 37 local rounds: 12.924801449949687
At round 37 global rounds: 47.55365676507614
gradient difference: 0.44276508688926697
train() client id: f_00000-0-0 loss: 1.273352  [   32/  126]
train() client id: f_00000-0-1 loss: 1.348693  [   64/  126]
train() client id: f_00000-0-2 loss: 1.362168  [   96/  126]
train() client id: f_00000-1-0 loss: 1.304703  [   32/  126]
train() client id: f_00000-1-1 loss: 1.310416  [   64/  126]
train() client id: f_00000-1-2 loss: 1.191088  [   96/  126]
train() client id: f_00000-2-0 loss: 1.133610  [   32/  126]
train() client id: f_00000-2-1 loss: 1.335228  [   64/  126]
train() client id: f_00000-2-2 loss: 1.061018  [   96/  126]
train() client id: f_00000-3-0 loss: 1.150074  [   32/  126]
train() client id: f_00000-3-1 loss: 1.170112  [   64/  126]
train() client id: f_00000-3-2 loss: 1.130575  [   96/  126]
train() client id: f_00000-4-0 loss: 1.151971  [   32/  126]
train() client id: f_00000-4-1 loss: 1.097391  [   64/  126]
train() client id: f_00000-4-2 loss: 0.947002  [   96/  126]
train() client id: f_00000-5-0 loss: 0.997245  [   32/  126]
train() client id: f_00000-5-1 loss: 0.991255  [   64/  126]
train() client id: f_00000-5-2 loss: 1.089950  [   96/  126]
train() client id: f_00000-6-0 loss: 1.030086  [   32/  126]
train() client id: f_00000-6-1 loss: 1.017282  [   64/  126]
train() client id: f_00000-6-2 loss: 1.018973  [   96/  126]
train() client id: f_00000-7-0 loss: 0.968254  [   32/  126]
train() client id: f_00000-7-1 loss: 1.022038  [   64/  126]
train() client id: f_00000-7-2 loss: 0.955438  [   96/  126]
train() client id: f_00000-8-0 loss: 0.903841  [   32/  126]
train() client id: f_00000-8-1 loss: 0.948644  [   64/  126]
train() client id: f_00000-8-2 loss: 0.941700  [   96/  126]
train() client id: f_00000-9-0 loss: 1.029536  [   32/  126]
train() client id: f_00000-9-1 loss: 1.059397  [   64/  126]
train() client id: f_00000-9-2 loss: 0.887139  [   96/  126]
train() client id: f_00000-10-0 loss: 0.865101  [   32/  126]
train() client id: f_00000-10-1 loss: 0.950838  [   64/  126]
train() client id: f_00000-10-2 loss: 1.103872  [   96/  126]
train() client id: f_00000-11-0 loss: 0.868241  [   32/  126]
train() client id: f_00000-11-1 loss: 1.097365  [   64/  126]
train() client id: f_00000-11-2 loss: 0.948772  [   96/  126]
train() client id: f_00001-0-0 loss: 0.406668  [   32/  265]
train() client id: f_00001-0-1 loss: 0.397210  [   64/  265]
train() client id: f_00001-0-2 loss: 0.413838  [   96/  265]
train() client id: f_00001-0-3 loss: 0.411593  [  128/  265]
train() client id: f_00001-0-4 loss: 0.396490  [  160/  265]
train() client id: f_00001-0-5 loss: 0.506960  [  192/  265]
train() client id: f_00001-0-6 loss: 0.447352  [  224/  265]
train() client id: f_00001-0-7 loss: 0.496933  [  256/  265]
train() client id: f_00001-1-0 loss: 0.427674  [   32/  265]
train() client id: f_00001-1-1 loss: 0.459052  [   64/  265]
train() client id: f_00001-1-2 loss: 0.367448  [   96/  265]
train() client id: f_00001-1-3 loss: 0.508253  [  128/  265]
train() client id: f_00001-1-4 loss: 0.379559  [  160/  265]
train() client id: f_00001-1-5 loss: 0.375349  [  192/  265]
train() client id: f_00001-1-6 loss: 0.469167  [  224/  265]
train() client id: f_00001-1-7 loss: 0.422931  [  256/  265]
train() client id: f_00001-2-0 loss: 0.343509  [   32/  265]
train() client id: f_00001-2-1 loss: 0.544735  [   64/  265]
train() client id: f_00001-2-2 loss: 0.386642  [   96/  265]
train() client id: f_00001-2-3 loss: 0.404888  [  128/  265]
train() client id: f_00001-2-4 loss: 0.416153  [  160/  265]
train() client id: f_00001-2-5 loss: 0.422667  [  192/  265]
train() client id: f_00001-2-6 loss: 0.332142  [  224/  265]
train() client id: f_00001-2-7 loss: 0.426585  [  256/  265]
train() client id: f_00001-3-0 loss: 0.377836  [   32/  265]
train() client id: f_00001-3-1 loss: 0.361368  [   64/  265]
train() client id: f_00001-3-2 loss: 0.350018  [   96/  265]
train() client id: f_00001-3-3 loss: 0.560479  [  128/  265]
train() client id: f_00001-3-4 loss: 0.316620  [  160/  265]
train() client id: f_00001-3-5 loss: 0.408759  [  192/  265]
train() client id: f_00001-3-6 loss: 0.449652  [  224/  265]
train() client id: f_00001-3-7 loss: 0.477452  [  256/  265]
train() client id: f_00001-4-0 loss: 0.439414  [   32/  265]
train() client id: f_00001-4-1 loss: 0.360149  [   64/  265]
train() client id: f_00001-4-2 loss: 0.457529  [   96/  265]
train() client id: f_00001-4-3 loss: 0.414650  [  128/  265]
train() client id: f_00001-4-4 loss: 0.351262  [  160/  265]
train() client id: f_00001-4-5 loss: 0.400757  [  192/  265]
train() client id: f_00001-4-6 loss: 0.408376  [  224/  265]
train() client id: f_00001-4-7 loss: 0.436880  [  256/  265]
train() client id: f_00001-5-0 loss: 0.408688  [   32/  265]
train() client id: f_00001-5-1 loss: 0.356016  [   64/  265]
train() client id: f_00001-5-2 loss: 0.346198  [   96/  265]
train() client id: f_00001-5-3 loss: 0.489939  [  128/  265]
train() client id: f_00001-5-4 loss: 0.354195  [  160/  265]
train() client id: f_00001-5-5 loss: 0.347965  [  192/  265]
train() client id: f_00001-5-6 loss: 0.342882  [  224/  265]
train() client id: f_00001-5-7 loss: 0.533496  [  256/  265]
train() client id: f_00001-6-0 loss: 0.479889  [   32/  265]
train() client id: f_00001-6-1 loss: 0.377619  [   64/  265]
train() client id: f_00001-6-2 loss: 0.382326  [   96/  265]
train() client id: f_00001-6-3 loss: 0.410525  [  128/  265]
train() client id: f_00001-6-4 loss: 0.400710  [  160/  265]
train() client id: f_00001-6-5 loss: 0.362643  [  192/  265]
train() client id: f_00001-6-6 loss: 0.347151  [  224/  265]
train() client id: f_00001-6-7 loss: 0.437058  [  256/  265]
train() client id: f_00001-7-0 loss: 0.457720  [   32/  265]
train() client id: f_00001-7-1 loss: 0.376232  [   64/  265]
train() client id: f_00001-7-2 loss: 0.418211  [   96/  265]
train() client id: f_00001-7-3 loss: 0.342352  [  128/  265]
train() client id: f_00001-7-4 loss: 0.387109  [  160/  265]
train() client id: f_00001-7-5 loss: 0.355007  [  192/  265]
train() client id: f_00001-7-6 loss: 0.491045  [  224/  265]
train() client id: f_00001-7-7 loss: 0.360696  [  256/  265]
train() client id: f_00001-8-0 loss: 0.393414  [   32/  265]
train() client id: f_00001-8-1 loss: 0.350389  [   64/  265]
train() client id: f_00001-8-2 loss: 0.471129  [   96/  265]
train() client id: f_00001-8-3 loss: 0.338056  [  128/  265]
train() client id: f_00001-8-4 loss: 0.543035  [  160/  265]
train() client id: f_00001-8-5 loss: 0.466558  [  192/  265]
train() client id: f_00001-8-6 loss: 0.308312  [  224/  265]
train() client id: f_00001-8-7 loss: 0.302019  [  256/  265]
train() client id: f_00001-9-0 loss: 0.314433  [   32/  265]
train() client id: f_00001-9-1 loss: 0.368551  [   64/  265]
train() client id: f_00001-9-2 loss: 0.490706  [   96/  265]
train() client id: f_00001-9-3 loss: 0.376824  [  128/  265]
train() client id: f_00001-9-4 loss: 0.365292  [  160/  265]
train() client id: f_00001-9-5 loss: 0.512942  [  192/  265]
train() client id: f_00001-9-6 loss: 0.432763  [  224/  265]
train() client id: f_00001-9-7 loss: 0.291729  [  256/  265]
train() client id: f_00001-10-0 loss: 0.294136  [   32/  265]
train() client id: f_00001-10-1 loss: 0.338365  [   64/  265]
train() client id: f_00001-10-2 loss: 0.488332  [   96/  265]
train() client id: f_00001-10-3 loss: 0.384324  [  128/  265]
train() client id: f_00001-10-4 loss: 0.487093  [  160/  265]
train() client id: f_00001-10-5 loss: 0.413109  [  192/  265]
train() client id: f_00001-10-6 loss: 0.398503  [  224/  265]
train() client id: f_00001-10-7 loss: 0.349505  [  256/  265]
train() client id: f_00001-11-0 loss: 0.359394  [   32/  265]
train() client id: f_00001-11-1 loss: 0.417659  [   64/  265]
train() client id: f_00001-11-2 loss: 0.532406  [   96/  265]
train() client id: f_00001-11-3 loss: 0.381794  [  128/  265]
train() client id: f_00001-11-4 loss: 0.353449  [  160/  265]
train() client id: f_00001-11-5 loss: 0.292629  [  192/  265]
train() client id: f_00001-11-6 loss: 0.365722  [  224/  265]
train() client id: f_00001-11-7 loss: 0.375524  [  256/  265]
train() client id: f_00002-0-0 loss: 1.205203  [   32/  124]
train() client id: f_00002-0-1 loss: 0.966061  [   64/  124]
train() client id: f_00002-0-2 loss: 1.310615  [   96/  124]
train() client id: f_00002-1-0 loss: 1.156073  [   32/  124]
train() client id: f_00002-1-1 loss: 1.063182  [   64/  124]
train() client id: f_00002-1-2 loss: 1.112911  [   96/  124]
train() client id: f_00002-2-0 loss: 0.990772  [   32/  124]
train() client id: f_00002-2-1 loss: 1.162527  [   64/  124]
train() client id: f_00002-2-2 loss: 0.951262  [   96/  124]
train() client id: f_00002-3-0 loss: 1.153582  [   32/  124]
train() client id: f_00002-3-1 loss: 1.008847  [   64/  124]
train() client id: f_00002-3-2 loss: 1.055857  [   96/  124]
train() client id: f_00002-4-0 loss: 1.052034  [   32/  124]
train() client id: f_00002-4-1 loss: 1.144346  [   64/  124]
train() client id: f_00002-4-2 loss: 0.939259  [   96/  124]
train() client id: f_00002-5-0 loss: 0.923744  [   32/  124]
train() client id: f_00002-5-1 loss: 1.063797  [   64/  124]
train() client id: f_00002-5-2 loss: 0.964467  [   96/  124]
train() client id: f_00002-6-0 loss: 1.118734  [   32/  124]
train() client id: f_00002-6-1 loss: 1.041861  [   64/  124]
train() client id: f_00002-6-2 loss: 0.864183  [   96/  124]
train() client id: f_00002-7-0 loss: 0.935367  [   32/  124]
train() client id: f_00002-7-1 loss: 0.984737  [   64/  124]
train() client id: f_00002-7-2 loss: 1.011869  [   96/  124]
train() client id: f_00002-8-0 loss: 0.904093  [   32/  124]
train() client id: f_00002-8-1 loss: 0.996390  [   64/  124]
train() client id: f_00002-8-2 loss: 1.019031  [   96/  124]
train() client id: f_00002-9-0 loss: 1.116103  [   32/  124]
train() client id: f_00002-9-1 loss: 0.998434  [   64/  124]
train() client id: f_00002-9-2 loss: 0.813042  [   96/  124]
train() client id: f_00002-10-0 loss: 1.142281  [   32/  124]
train() client id: f_00002-10-1 loss: 0.976483  [   64/  124]
train() client id: f_00002-10-2 loss: 0.734145  [   96/  124]
train() client id: f_00002-11-0 loss: 0.933193  [   32/  124]
train() client id: f_00002-11-1 loss: 1.020478  [   64/  124]
train() client id: f_00002-11-2 loss: 0.845866  [   96/  124]
train() client id: f_00003-0-0 loss: 0.857284  [   32/   43]
train() client id: f_00003-1-0 loss: 0.848537  [   32/   43]
train() client id: f_00003-2-0 loss: 0.732475  [   32/   43]
train() client id: f_00003-3-0 loss: 0.850564  [   32/   43]
train() client id: f_00003-4-0 loss: 0.752579  [   32/   43]
train() client id: f_00003-5-0 loss: 0.662925  [   32/   43]
train() client id: f_00003-6-0 loss: 0.825233  [   32/   43]
train() client id: f_00003-7-0 loss: 0.726581  [   32/   43]
train() client id: f_00003-8-0 loss: 0.994334  [   32/   43]
train() client id: f_00003-9-0 loss: 0.672589  [   32/   43]
train() client id: f_00003-10-0 loss: 0.471102  [   32/   43]
train() client id: f_00003-11-0 loss: 0.769509  [   32/   43]
train() client id: f_00004-0-0 loss: 0.900115  [   32/  306]
train() client id: f_00004-0-1 loss: 0.917226  [   64/  306]
train() client id: f_00004-0-2 loss: 0.926411  [   96/  306]
train() client id: f_00004-0-3 loss: 0.791408  [  128/  306]
train() client id: f_00004-0-4 loss: 0.967555  [  160/  306]
train() client id: f_00004-0-5 loss: 0.937587  [  192/  306]
train() client id: f_00004-0-6 loss: 0.823078  [  224/  306]
train() client id: f_00004-0-7 loss: 0.767146  [  256/  306]
train() client id: f_00004-0-8 loss: 0.989001  [  288/  306]
train() client id: f_00004-1-0 loss: 0.900277  [   32/  306]
train() client id: f_00004-1-1 loss: 0.955773  [   64/  306]
train() client id: f_00004-1-2 loss: 0.837303  [   96/  306]
train() client id: f_00004-1-3 loss: 0.838093  [  128/  306]
train() client id: f_00004-1-4 loss: 1.077896  [  160/  306]
train() client id: f_00004-1-5 loss: 0.899928  [  192/  306]
train() client id: f_00004-1-6 loss: 0.828141  [  224/  306]
train() client id: f_00004-1-7 loss: 0.970682  [  256/  306]
train() client id: f_00004-1-8 loss: 0.883446  [  288/  306]
train() client id: f_00004-2-0 loss: 0.842808  [   32/  306]
train() client id: f_00004-2-1 loss: 1.025938  [   64/  306]
train() client id: f_00004-2-2 loss: 0.902367  [   96/  306]
train() client id: f_00004-2-3 loss: 0.874809  [  128/  306]
train() client id: f_00004-2-4 loss: 0.751495  [  160/  306]
train() client id: f_00004-2-5 loss: 0.966558  [  192/  306]
train() client id: f_00004-2-6 loss: 0.985204  [  224/  306]
train() client id: f_00004-2-7 loss: 0.817026  [  256/  306]
train() client id: f_00004-2-8 loss: 0.952822  [  288/  306]
train() client id: f_00004-3-0 loss: 0.837218  [   32/  306]
train() client id: f_00004-3-1 loss: 1.083665  [   64/  306]
train() client id: f_00004-3-2 loss: 0.927181  [   96/  306]
train() client id: f_00004-3-3 loss: 0.839465  [  128/  306]
train() client id: f_00004-3-4 loss: 0.875192  [  160/  306]
train() client id: f_00004-3-5 loss: 0.818278  [  192/  306]
train() client id: f_00004-3-6 loss: 0.818079  [  224/  306]
train() client id: f_00004-3-7 loss: 0.883632  [  256/  306]
train() client id: f_00004-3-8 loss: 0.958127  [  288/  306]
train() client id: f_00004-4-0 loss: 0.857606  [   32/  306]
train() client id: f_00004-4-1 loss: 0.872467  [   64/  306]
train() client id: f_00004-4-2 loss: 0.914291  [   96/  306]
train() client id: f_00004-4-3 loss: 0.977357  [  128/  306]
train() client id: f_00004-4-4 loss: 0.962674  [  160/  306]
train() client id: f_00004-4-5 loss: 0.762765  [  192/  306]
train() client id: f_00004-4-6 loss: 0.907233  [  224/  306]
train() client id: f_00004-4-7 loss: 0.901606  [  256/  306]
train() client id: f_00004-4-8 loss: 0.914244  [  288/  306]
train() client id: f_00004-5-0 loss: 0.840209  [   32/  306]
train() client id: f_00004-5-1 loss: 0.904568  [   64/  306]
train() client id: f_00004-5-2 loss: 0.903808  [   96/  306]
train() client id: f_00004-5-3 loss: 1.029721  [  128/  306]
train() client id: f_00004-5-4 loss: 0.861275  [  160/  306]
train() client id: f_00004-5-5 loss: 0.952101  [  192/  306]
train() client id: f_00004-5-6 loss: 0.710868  [  224/  306]
train() client id: f_00004-5-7 loss: 0.918015  [  256/  306]
train() client id: f_00004-5-8 loss: 0.938214  [  288/  306]
train() client id: f_00004-6-0 loss: 0.875091  [   32/  306]
train() client id: f_00004-6-1 loss: 0.836079  [   64/  306]
train() client id: f_00004-6-2 loss: 0.951844  [   96/  306]
train() client id: f_00004-6-3 loss: 1.026596  [  128/  306]
train() client id: f_00004-6-4 loss: 0.934553  [  160/  306]
train() client id: f_00004-6-5 loss: 0.869932  [  192/  306]
train() client id: f_00004-6-6 loss: 0.802094  [  224/  306]
train() client id: f_00004-6-7 loss: 0.940273  [  256/  306]
train() client id: f_00004-6-8 loss: 0.741541  [  288/  306]
train() client id: f_00004-7-0 loss: 0.884427  [   32/  306]
train() client id: f_00004-7-1 loss: 0.843894  [   64/  306]
train() client id: f_00004-7-2 loss: 1.017033  [   96/  306]
train() client id: f_00004-7-3 loss: 0.927086  [  128/  306]
train() client id: f_00004-7-4 loss: 1.019995  [  160/  306]
train() client id: f_00004-7-5 loss: 0.884112  [  192/  306]
train() client id: f_00004-7-6 loss: 0.871805  [  224/  306]
train() client id: f_00004-7-7 loss: 0.762503  [  256/  306]
train() client id: f_00004-7-8 loss: 0.808964  [  288/  306]
train() client id: f_00004-8-0 loss: 0.892619  [   32/  306]
train() client id: f_00004-8-1 loss: 0.900039  [   64/  306]
train() client id: f_00004-8-2 loss: 0.852414  [   96/  306]
train() client id: f_00004-8-3 loss: 0.823858  [  128/  306]
train() client id: f_00004-8-4 loss: 0.947168  [  160/  306]
train() client id: f_00004-8-5 loss: 0.950919  [  192/  306]
train() client id: f_00004-8-6 loss: 1.028216  [  224/  306]
train() client id: f_00004-8-7 loss: 0.861509  [  256/  306]
train() client id: f_00004-8-8 loss: 0.831389  [  288/  306]
train() client id: f_00004-9-0 loss: 0.932440  [   32/  306]
train() client id: f_00004-9-1 loss: 0.951976  [   64/  306]
train() client id: f_00004-9-2 loss: 0.906073  [   96/  306]
train() client id: f_00004-9-3 loss: 0.750364  [  128/  306]
train() client id: f_00004-9-4 loss: 0.941545  [  160/  306]
train() client id: f_00004-9-5 loss: 0.970326  [  192/  306]
train() client id: f_00004-9-6 loss: 0.774657  [  224/  306]
train() client id: f_00004-9-7 loss: 0.787447  [  256/  306]
train() client id: f_00004-9-8 loss: 0.962496  [  288/  306]
train() client id: f_00004-10-0 loss: 0.798985  [   32/  306]
train() client id: f_00004-10-1 loss: 0.878151  [   64/  306]
train() client id: f_00004-10-2 loss: 0.911456  [   96/  306]
train() client id: f_00004-10-3 loss: 1.029091  [  128/  306]
train() client id: f_00004-10-4 loss: 0.825138  [  160/  306]
train() client id: f_00004-10-5 loss: 0.870224  [  192/  306]
train() client id: f_00004-10-6 loss: 0.831416  [  224/  306]
train() client id: f_00004-10-7 loss: 0.863215  [  256/  306]
train() client id: f_00004-10-8 loss: 0.984998  [  288/  306]
train() client id: f_00004-11-0 loss: 0.754333  [   32/  306]
train() client id: f_00004-11-1 loss: 0.908607  [   64/  306]
train() client id: f_00004-11-2 loss: 0.912577  [   96/  306]
train() client id: f_00004-11-3 loss: 0.850159  [  128/  306]
train() client id: f_00004-11-4 loss: 0.823718  [  160/  306]
train() client id: f_00004-11-5 loss: 0.810316  [  192/  306]
train() client id: f_00004-11-6 loss: 0.903082  [  224/  306]
train() client id: f_00004-11-7 loss: 0.947211  [  256/  306]
train() client id: f_00004-11-8 loss: 0.976124  [  288/  306]
train() client id: f_00005-0-0 loss: 0.547160  [   32/  146]
train() client id: f_00005-0-1 loss: 0.630983  [   64/  146]
train() client id: f_00005-0-2 loss: 0.501317  [   96/  146]
train() client id: f_00005-0-3 loss: 0.704156  [  128/  146]
train() client id: f_00005-1-0 loss: 0.599568  [   32/  146]
train() client id: f_00005-1-1 loss: 0.907309  [   64/  146]
train() client id: f_00005-1-2 loss: 0.572244  [   96/  146]
train() client id: f_00005-1-3 loss: 0.585554  [  128/  146]
train() client id: f_00005-2-0 loss: 0.568026  [   32/  146]
train() client id: f_00005-2-1 loss: 0.683809  [   64/  146]
train() client id: f_00005-2-2 loss: 0.676549  [   96/  146]
train() client id: f_00005-2-3 loss: 0.516818  [  128/  146]
train() client id: f_00005-3-0 loss: 0.485399  [   32/  146]
train() client id: f_00005-3-1 loss: 0.763071  [   64/  146]
train() client id: f_00005-3-2 loss: 0.653683  [   96/  146]
train() client id: f_00005-3-3 loss: 0.555035  [  128/  146]
train() client id: f_00005-4-0 loss: 0.714645  [   32/  146]
train() client id: f_00005-4-1 loss: 0.581156  [   64/  146]
train() client id: f_00005-4-2 loss: 0.682416  [   96/  146]
train() client id: f_00005-4-3 loss: 0.617446  [  128/  146]
train() client id: f_00005-5-0 loss: 0.493499  [   32/  146]
train() client id: f_00005-5-1 loss: 0.562751  [   64/  146]
train() client id: f_00005-5-2 loss: 0.525914  [   96/  146]
train() client id: f_00005-5-3 loss: 0.772270  [  128/  146]
train() client id: f_00005-6-0 loss: 0.360216  [   32/  146]
train() client id: f_00005-6-1 loss: 0.543261  [   64/  146]
train() client id: f_00005-6-2 loss: 0.646070  [   96/  146]
train() client id: f_00005-6-3 loss: 0.738392  [  128/  146]
train() client id: f_00005-7-0 loss: 0.715166  [   32/  146]
train() client id: f_00005-7-1 loss: 0.573587  [   64/  146]
train() client id: f_00005-7-2 loss: 0.714485  [   96/  146]
train() client id: f_00005-7-3 loss: 0.555629  [  128/  146]
train() client id: f_00005-8-0 loss: 0.518730  [   32/  146]
train() client id: f_00005-8-1 loss: 0.591699  [   64/  146]
train() client id: f_00005-8-2 loss: 0.697309  [   96/  146]
train() client id: f_00005-8-3 loss: 0.444044  [  128/  146]
train() client id: f_00005-9-0 loss: 0.744435  [   32/  146]
train() client id: f_00005-9-1 loss: 0.615378  [   64/  146]
train() client id: f_00005-9-2 loss: 0.393393  [   96/  146]
train() client id: f_00005-9-3 loss: 0.490958  [  128/  146]
train() client id: f_00005-10-0 loss: 0.393422  [   32/  146]
train() client id: f_00005-10-1 loss: 0.831871  [   64/  146]
train() client id: f_00005-10-2 loss: 0.674476  [   96/  146]
train() client id: f_00005-10-3 loss: 0.609756  [  128/  146]
train() client id: f_00005-11-0 loss: 0.547507  [   32/  146]
train() client id: f_00005-11-1 loss: 0.451523  [   64/  146]
train() client id: f_00005-11-2 loss: 0.658933  [   96/  146]
train() client id: f_00005-11-3 loss: 0.750568  [  128/  146]
train() client id: f_00006-0-0 loss: 0.549187  [   32/   54]
train() client id: f_00006-1-0 loss: 0.514698  [   32/   54]
train() client id: f_00006-2-0 loss: 0.528223  [   32/   54]
train() client id: f_00006-3-0 loss: 0.492359  [   32/   54]
train() client id: f_00006-4-0 loss: 0.450337  [   32/   54]
train() client id: f_00006-5-0 loss: 0.507226  [   32/   54]
train() client id: f_00006-6-0 loss: 0.477815  [   32/   54]
train() client id: f_00006-7-0 loss: 0.546822  [   32/   54]
train() client id: f_00006-8-0 loss: 0.456610  [   32/   54]
train() client id: f_00006-9-0 loss: 0.520921  [   32/   54]
train() client id: f_00006-10-0 loss: 0.549852  [   32/   54]
train() client id: f_00006-11-0 loss: 0.510477  [   32/   54]
train() client id: f_00007-0-0 loss: 0.863954  [   32/  179]
train() client id: f_00007-0-1 loss: 0.810877  [   64/  179]
train() client id: f_00007-0-2 loss: 0.648138  [   96/  179]
train() client id: f_00007-0-3 loss: 0.461502  [  128/  179]
train() client id: f_00007-0-4 loss: 0.488036  [  160/  179]
train() client id: f_00007-1-0 loss: 0.492377  [   32/  179]
train() client id: f_00007-1-1 loss: 0.708907  [   64/  179]
train() client id: f_00007-1-2 loss: 0.506342  [   96/  179]
train() client id: f_00007-1-3 loss: 0.647762  [  128/  179]
train() client id: f_00007-1-4 loss: 0.666836  [  160/  179]
train() client id: f_00007-2-0 loss: 0.692241  [   32/  179]
train() client id: f_00007-2-1 loss: 0.668356  [   64/  179]
train() client id: f_00007-2-2 loss: 0.493543  [   96/  179]
train() client id: f_00007-2-3 loss: 0.616876  [  128/  179]
train() client id: f_00007-2-4 loss: 0.629131  [  160/  179]
train() client id: f_00007-3-0 loss: 0.724798  [   32/  179]
train() client id: f_00007-3-1 loss: 0.773664  [   64/  179]
train() client id: f_00007-3-2 loss: 0.505747  [   96/  179]
train() client id: f_00007-3-3 loss: 0.450104  [  128/  179]
train() client id: f_00007-3-4 loss: 0.673951  [  160/  179]
train() client id: f_00007-4-0 loss: 0.686762  [   32/  179]
train() client id: f_00007-4-1 loss: 0.577082  [   64/  179]
train() client id: f_00007-4-2 loss: 0.517484  [   96/  179]
train() client id: f_00007-4-3 loss: 0.700638  [  128/  179]
train() client id: f_00007-4-4 loss: 0.666473  [  160/  179]
train() client id: f_00007-5-0 loss: 0.496357  [   32/  179]
train() client id: f_00007-5-1 loss: 0.649246  [   64/  179]
train() client id: f_00007-5-2 loss: 0.549994  [   96/  179]
train() client id: f_00007-5-3 loss: 0.836165  [  128/  179]
train() client id: f_00007-5-4 loss: 0.520823  [  160/  179]
train() client id: f_00007-6-0 loss: 0.631494  [   32/  179]
train() client id: f_00007-6-1 loss: 0.605915  [   64/  179]
train() client id: f_00007-6-2 loss: 0.702687  [   96/  179]
train() client id: f_00007-6-3 loss: 0.489490  [  128/  179]
train() client id: f_00007-6-4 loss: 0.672294  [  160/  179]
train() client id: f_00007-7-0 loss: 0.470611  [   32/  179]
train() client id: f_00007-7-1 loss: 0.674629  [   64/  179]
train() client id: f_00007-7-2 loss: 0.483400  [   96/  179]
train() client id: f_00007-7-3 loss: 0.956726  [  128/  179]
train() client id: f_00007-7-4 loss: 0.516749  [  160/  179]
train() client id: f_00007-8-0 loss: 0.579455  [   32/  179]
train() client id: f_00007-8-1 loss: 0.518250  [   64/  179]
train() client id: f_00007-8-2 loss: 0.560982  [   96/  179]
train() client id: f_00007-8-3 loss: 0.801532  [  128/  179]
train() client id: f_00007-8-4 loss: 0.569884  [  160/  179]
train() client id: f_00007-9-0 loss: 0.693276  [   32/  179]
train() client id: f_00007-9-1 loss: 0.588517  [   64/  179]
train() client id: f_00007-9-2 loss: 0.668298  [   96/  179]
train() client id: f_00007-9-3 loss: 0.577050  [  128/  179]
train() client id: f_00007-9-4 loss: 0.563277  [  160/  179]
train() client id: f_00007-10-0 loss: 0.562032  [   32/  179]
train() client id: f_00007-10-1 loss: 0.673224  [   64/  179]
train() client id: f_00007-10-2 loss: 0.637233  [   96/  179]
train() client id: f_00007-10-3 loss: 0.488764  [  128/  179]
train() client id: f_00007-10-4 loss: 0.652135  [  160/  179]
train() client id: f_00007-11-0 loss: 0.732671  [   32/  179]
train() client id: f_00007-11-1 loss: 0.672531  [   64/  179]
train() client id: f_00007-11-2 loss: 0.619119  [   96/  179]
train() client id: f_00007-11-3 loss: 0.461202  [  128/  179]
train() client id: f_00007-11-4 loss: 0.614174  [  160/  179]
train() client id: f_00008-0-0 loss: 0.719784  [   32/  130]
train() client id: f_00008-0-1 loss: 0.657318  [   64/  130]
train() client id: f_00008-0-2 loss: 0.655502  [   96/  130]
train() client id: f_00008-0-3 loss: 0.754433  [  128/  130]
train() client id: f_00008-1-0 loss: 0.691111  [   32/  130]
train() client id: f_00008-1-1 loss: 0.669844  [   64/  130]
train() client id: f_00008-1-2 loss: 0.746681  [   96/  130]
train() client id: f_00008-1-3 loss: 0.642344  [  128/  130]
train() client id: f_00008-2-0 loss: 0.671089  [   32/  130]
train() client id: f_00008-2-1 loss: 0.708768  [   64/  130]
train() client id: f_00008-2-2 loss: 0.700616  [   96/  130]
train() client id: f_00008-2-3 loss: 0.670463  [  128/  130]
train() client id: f_00008-3-0 loss: 0.717564  [   32/  130]
train() client id: f_00008-3-1 loss: 0.724214  [   64/  130]
train() client id: f_00008-3-2 loss: 0.706745  [   96/  130]
train() client id: f_00008-3-3 loss: 0.624833  [  128/  130]
train() client id: f_00008-4-0 loss: 0.707144  [   32/  130]
train() client id: f_00008-4-1 loss: 0.789631  [   64/  130]
train() client id: f_00008-4-2 loss: 0.657072  [   96/  130]
train() client id: f_00008-4-3 loss: 0.581050  [  128/  130]
train() client id: f_00008-5-0 loss: 0.625541  [   32/  130]
train() client id: f_00008-5-1 loss: 0.764420  [   64/  130]
train() client id: f_00008-5-2 loss: 0.791057  [   96/  130]
train() client id: f_00008-5-3 loss: 0.616830  [  128/  130]
train() client id: f_00008-6-0 loss: 0.643158  [   32/  130]
train() client id: f_00008-6-1 loss: 0.603254  [   64/  130]
train() client id: f_00008-6-2 loss: 0.767091  [   96/  130]
train() client id: f_00008-6-3 loss: 0.773885  [  128/  130]
train() client id: f_00008-7-0 loss: 0.626825  [   32/  130]
train() client id: f_00008-7-1 loss: 0.620670  [   64/  130]
train() client id: f_00008-7-2 loss: 0.734265  [   96/  130]
train() client id: f_00008-7-3 loss: 0.776867  [  128/  130]
train() client id: f_00008-8-0 loss: 0.651760  [   32/  130]
train() client id: f_00008-8-1 loss: 0.663655  [   64/  130]
train() client id: f_00008-8-2 loss: 0.729066  [   96/  130]
train() client id: f_00008-8-3 loss: 0.740484  [  128/  130]
train() client id: f_00008-9-0 loss: 0.699472  [   32/  130]
train() client id: f_00008-9-1 loss: 0.714391  [   64/  130]
train() client id: f_00008-9-2 loss: 0.674312  [   96/  130]
train() client id: f_00008-9-3 loss: 0.690220  [  128/  130]
train() client id: f_00008-10-0 loss: 0.815508  [   32/  130]
train() client id: f_00008-10-1 loss: 0.710635  [   64/  130]
train() client id: f_00008-10-2 loss: 0.684616  [   96/  130]
train() client id: f_00008-10-3 loss: 0.582496  [  128/  130]
train() client id: f_00008-11-0 loss: 0.693445  [   32/  130]
train() client id: f_00008-11-1 loss: 0.711426  [   64/  130]
train() client id: f_00008-11-2 loss: 0.711183  [   96/  130]
train() client id: f_00008-11-3 loss: 0.665890  [  128/  130]
train() client id: f_00009-0-0 loss: 1.123583  [   32/  118]
train() client id: f_00009-0-1 loss: 0.856615  [   64/  118]
train() client id: f_00009-0-2 loss: 0.979944  [   96/  118]
train() client id: f_00009-1-0 loss: 0.848393  [   32/  118]
train() client id: f_00009-1-1 loss: 1.041913  [   64/  118]
train() client id: f_00009-1-2 loss: 0.920520  [   96/  118]
train() client id: f_00009-2-0 loss: 0.929465  [   32/  118]
train() client id: f_00009-2-1 loss: 0.952716  [   64/  118]
train() client id: f_00009-2-2 loss: 0.864914  [   96/  118]
train() client id: f_00009-3-0 loss: 0.931378  [   32/  118]
train() client id: f_00009-3-1 loss: 0.871725  [   64/  118]
train() client id: f_00009-3-2 loss: 0.755112  [   96/  118]
train() client id: f_00009-4-0 loss: 0.943092  [   32/  118]
train() client id: f_00009-4-1 loss: 0.808571  [   64/  118]
train() client id: f_00009-4-2 loss: 0.823187  [   96/  118]
train() client id: f_00009-5-0 loss: 0.892056  [   32/  118]
train() client id: f_00009-5-1 loss: 0.906754  [   64/  118]
train() client id: f_00009-5-2 loss: 0.798961  [   96/  118]
train() client id: f_00009-6-0 loss: 1.073115  [   32/  118]
train() client id: f_00009-6-1 loss: 0.747046  [   64/  118]
train() client id: f_00009-6-2 loss: 0.742543  [   96/  118]
train() client id: f_00009-7-0 loss: 0.752976  [   32/  118]
train() client id: f_00009-7-1 loss: 0.921442  [   64/  118]
train() client id: f_00009-7-2 loss: 0.787554  [   96/  118]
train() client id: f_00009-8-0 loss: 0.860429  [   32/  118]
train() client id: f_00009-8-1 loss: 0.816569  [   64/  118]
train() client id: f_00009-8-2 loss: 0.867716  [   96/  118]
train() client id: f_00009-9-0 loss: 0.826646  [   32/  118]
train() client id: f_00009-9-1 loss: 0.839416  [   64/  118]
train() client id: f_00009-9-2 loss: 0.775281  [   96/  118]
train() client id: f_00009-10-0 loss: 0.783679  [   32/  118]
train() client id: f_00009-10-1 loss: 0.872617  [   64/  118]
train() client id: f_00009-10-2 loss: 0.916275  [   96/  118]
train() client id: f_00009-11-0 loss: 0.798505  [   32/  118]
train() client id: f_00009-11-1 loss: 0.814898  [   64/  118]
train() client id: f_00009-11-2 loss: 0.916206  [   96/  118]
At round 37 accuracy: 0.6525198938992043
At round 37 training accuracy: 0.5875251509054326
At round 37 training loss: 0.8285498619118732
update_location
xs = [  -3.9056584     4.20031788  205.00902392   18.81129433    0.97929623
    3.95640986 -167.44319194 -146.32485185  189.66397685 -132.06087855]
ys = [ 197.5879595   180.55583871    1.32061395 -167.45517586  159.35018685
  142.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [221.4864689  206.44140467 228.10182794 195.94667825 188.13144625
 174.389033   195.04900171 177.23328842 215.13041181 165.69878545]
dists_bs = [172.57893742 179.28642244 417.81740916 393.49506386 176.67379362
 181.82773585 177.7856586  176.5222892  397.19558643 176.30729249]
uav_gains = [1.23220895e-11 1.55193540e-11 1.10553362e-11 1.80819760e-11
 2.02342599e-11 2.47219556e-11 1.83173556e-11 2.37047028e-11
 1.36151003e-11 2.81872466e-11]
bs_gains = [6.02182696e-11 5.41204454e-11 5.06444427e-12 5.99051385e-12
 5.63913003e-11 5.20290288e-11 5.54093755e-11 5.65269226e-11
 5.83554908e-12 5.67201424e-11]
Round 38
-------------------------------
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.97555343 12.35186937  5.88797184  2.12582729 14.24522634  6.85479284
  2.63358293  8.40017663  6.2003918   5.55926836]
obj_prev = 70.23466082817666
eta_min = 3.529677745883019e-16	eta_max = 0.9307450633473059
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 16.289713645049638	eta = 0.9090909090909091
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 30.212175618890367	eta = 0.49016101234197246
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 23.329211198156166	eta = 0.6347763094355793
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 22.083685449611664	eta = 0.6705778625672812
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 22.017241376491434	eta = 0.6726015459057763
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 22.017036233942203	eta = 0.6726078128344527
eta = 0.6726078128344527
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [0.03272589 0.06882827 0.03220643 0.01116836 0.07947717 0.03792048
 0.01402538 0.04649155 0.03376481 0.03064806]
ene_total = [1.98224589 3.51656478 1.97276077 0.93820235 4.0071535  2.08744961
 1.06916394 2.54721356 2.15142878 1.74485306]
ti_comp = [0.50594645 0.53574993 0.50279946 0.51575345 0.53633338 0.53518151
 0.51605241 0.52164089 0.47981336 0.53641515]
ti_coms = [0.10065732 0.07085384 0.10380431 0.09085032 0.07027039 0.07142226
 0.09055136 0.08496288 0.12679041 0.07018862]
t_total = [28.09984055 28.09984055 28.09984055 28.09984055 28.09984055 28.09984055
 28.09984055 28.09984055 28.09984055 28.09984055]
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [8.55747057e-06 7.09996571e-05 8.25882640e-06 3.27313532e-07
 1.09078086e-04 1.18986925e-05 6.47495169e-07 2.30812175e-05
 1.04502883e-05 6.25297698e-06]
ene_total = [0.46667354 0.33150672 0.48123759 0.42086309 0.33056794 0.33140208
 0.41949304 0.39464461 0.58781812 0.32542594]
optimize_network iter = 0 obj = 4.0896326721833045
eta = 0.6726078128344527
freqs = [32341261.25530905 64235440.52933019 32027116.84044752 10827227.71361229
 74093064.8966756  35427679.80345908 13589106.57226709 44562788.60807764
 35185358.32378028 28567479.83964483]
eta_min = 0.6726078128344555	eta_max = 0.6726078128344546
af = 0.00954296352207343	bf = 1.3351291140167194	zeta = 0.010497259874280773	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [2.05655253e-06 1.70628135e-05 1.98478162e-06 7.86607991e-08
 2.62139160e-05 2.85952327e-06 1.55607644e-07 5.54693540e-06
 2.51143918e-06 1.50273092e-06]
ene_total = [1.67955348 1.18486183 1.73204102 1.5156187  1.17665519 1.19197501
 1.51064412 1.41831402 2.11559328 1.17116856]
ti_comp = [0.50594645 0.53574993 0.50279946 0.51575345 0.53633338 0.53518151
 0.51605241 0.52164089 0.47981336 0.53641515]
ti_coms = [0.10065732 0.07085384 0.10380431 0.09085032 0.07027039 0.07142226
 0.09055136 0.08496288 0.12679041 0.07018862]
t_total = [28.09984055 28.09984055 28.09984055 28.09984055 28.09984055 28.09984055
 28.09984055 28.09984055 28.09984055 28.09984055]
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [8.55747057e-06 7.09996571e-05 8.25882640e-06 3.27313532e-07
 1.09078086e-04 1.18986925e-05 6.47495169e-07 2.30812175e-05
 1.04502883e-05 6.25297698e-06]
ene_total = [0.46667354 0.33150672 0.48123759 0.42086309 0.33056794 0.33140208
 0.41949304 0.39464461 0.58781812 0.32542594]
optimize_network iter = 1 obj = 4.0896326721833285
eta = 0.6726078128344546
freqs = [32341261.25530905 64235440.52933017 32027116.84044754 10827227.71361229
 74093064.89667559 35427679.80345907 13589106.57226709 44562788.60807764
 35185358.32378031 28567479.83964482]
Done!
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [7.90743628e-06 6.56064499e-05 7.63147743e-06 3.02450459e-07
 1.00792402e-04 1.09948555e-05 5.98310768e-07 2.13279445e-05
 9.65647363e-06 5.77799440e-06]
ene_total = [0.01007364 0.00715099 0.01038806 0.00908533 0.00712783 0.00715322
 0.00905573 0.00851762 0.0126887  0.00702464]
At round 38 energy consumption: 0.08826576524222389
At round 38 eta: 0.6726078128344546
At round 38 a_n: 14.823314824451002
At round 38 local rounds: 12.986465296587173
At round 38 global rounds: 46.323221096756726
gradient difference: 0.48741650581359863
train() client id: f_00000-0-0 loss: 1.209142  [   32/  126]
train() client id: f_00000-0-1 loss: 1.007692  [   64/  126]
train() client id: f_00000-0-2 loss: 0.815024  [   96/  126]
train() client id: f_00000-1-0 loss: 0.999807  [   32/  126]
train() client id: f_00000-1-1 loss: 0.844586  [   64/  126]
train() client id: f_00000-1-2 loss: 0.845457  [   96/  126]
train() client id: f_00000-2-0 loss: 0.994216  [   32/  126]
train() client id: f_00000-2-1 loss: 1.035238  [   64/  126]
train() client id: f_00000-2-2 loss: 0.843276  [   96/  126]
train() client id: f_00000-3-0 loss: 0.842711  [   32/  126]
train() client id: f_00000-3-1 loss: 1.016619  [   64/  126]
train() client id: f_00000-3-2 loss: 0.909497  [   96/  126]
train() client id: f_00000-4-0 loss: 0.945830  [   32/  126]
train() client id: f_00000-4-1 loss: 0.794556  [   64/  126]
train() client id: f_00000-4-2 loss: 0.801066  [   96/  126]
train() client id: f_00000-5-0 loss: 0.875045  [   32/  126]
train() client id: f_00000-5-1 loss: 0.929832  [   64/  126]
train() client id: f_00000-5-2 loss: 0.856665  [   96/  126]
train() client id: f_00000-6-0 loss: 0.879222  [   32/  126]
train() client id: f_00000-6-1 loss: 0.883598  [   64/  126]
train() client id: f_00000-6-2 loss: 0.837370  [   96/  126]
train() client id: f_00000-7-0 loss: 0.747069  [   32/  126]
train() client id: f_00000-7-1 loss: 0.882514  [   64/  126]
train() client id: f_00000-7-2 loss: 0.853958  [   96/  126]
train() client id: f_00000-8-0 loss: 0.851173  [   32/  126]
train() client id: f_00000-8-1 loss: 0.764584  [   64/  126]
train() client id: f_00000-8-2 loss: 0.788885  [   96/  126]
train() client id: f_00000-9-0 loss: 0.802642  [   32/  126]
train() client id: f_00000-9-1 loss: 0.805411  [   64/  126]
train() client id: f_00000-9-2 loss: 0.867135  [   96/  126]
train() client id: f_00000-10-0 loss: 0.688905  [   32/  126]
train() client id: f_00000-10-1 loss: 0.819053  [   64/  126]
train() client id: f_00000-10-2 loss: 0.910694  [   96/  126]
train() client id: f_00000-11-0 loss: 0.883479  [   32/  126]
train() client id: f_00000-11-1 loss: 0.769077  [   64/  126]
train() client id: f_00000-11-2 loss: 0.853046  [   96/  126]
train() client id: f_00001-0-0 loss: 0.432082  [   32/  265]
train() client id: f_00001-0-1 loss: 0.492674  [   64/  265]
train() client id: f_00001-0-2 loss: 0.459874  [   96/  265]
train() client id: f_00001-0-3 loss: 0.579876  [  128/  265]
train() client id: f_00001-0-4 loss: 0.414122  [  160/  265]
train() client id: f_00001-0-5 loss: 0.358741  [  192/  265]
train() client id: f_00001-0-6 loss: 0.498720  [  224/  265]
train() client id: f_00001-0-7 loss: 0.425024  [  256/  265]
train() client id: f_00001-1-0 loss: 0.443162  [   32/  265]
train() client id: f_00001-1-1 loss: 0.406034  [   64/  265]
train() client id: f_00001-1-2 loss: 0.409142  [   96/  265]
train() client id: f_00001-1-3 loss: 0.401958  [  128/  265]
train() client id: f_00001-1-4 loss: 0.536571  [  160/  265]
train() client id: f_00001-1-5 loss: 0.606934  [  192/  265]
train() client id: f_00001-1-6 loss: 0.377032  [  224/  265]
train() client id: f_00001-1-7 loss: 0.452734  [  256/  265]
train() client id: f_00001-2-0 loss: 0.447270  [   32/  265]
train() client id: f_00001-2-1 loss: 0.427561  [   64/  265]
train() client id: f_00001-2-2 loss: 0.614961  [   96/  265]
train() client id: f_00001-2-3 loss: 0.366786  [  128/  265]
train() client id: f_00001-2-4 loss: 0.418933  [  160/  265]
train() client id: f_00001-2-5 loss: 0.494364  [  192/  265]
train() client id: f_00001-2-6 loss: 0.347644  [  224/  265]
train() client id: f_00001-2-7 loss: 0.447188  [  256/  265]
train() client id: f_00001-3-0 loss: 0.348370  [   32/  265]
train() client id: f_00001-3-1 loss: 0.581480  [   64/  265]
train() client id: f_00001-3-2 loss: 0.415808  [   96/  265]
train() client id: f_00001-3-3 loss: 0.437312  [  128/  265]
train() client id: f_00001-3-4 loss: 0.488152  [  160/  265]
train() client id: f_00001-3-5 loss: 0.473918  [  192/  265]
train() client id: f_00001-3-6 loss: 0.403923  [  224/  265]
train() client id: f_00001-3-7 loss: 0.383332  [  256/  265]
train() client id: f_00001-4-0 loss: 0.543242  [   32/  265]
train() client id: f_00001-4-1 loss: 0.355548  [   64/  265]
train() client id: f_00001-4-2 loss: 0.413436  [   96/  265]
train() client id: f_00001-4-3 loss: 0.364139  [  128/  265]
train() client id: f_00001-4-4 loss: 0.415457  [  160/  265]
train() client id: f_00001-4-5 loss: 0.543185  [  192/  265]
train() client id: f_00001-4-6 loss: 0.550024  [  224/  265]
train() client id: f_00001-4-7 loss: 0.402173  [  256/  265]
train() client id: f_00001-5-0 loss: 0.406717  [   32/  265]
train() client id: f_00001-5-1 loss: 0.493868  [   64/  265]
train() client id: f_00001-5-2 loss: 0.400805  [   96/  265]
train() client id: f_00001-5-3 loss: 0.350603  [  128/  265]
train() client id: f_00001-5-4 loss: 0.527953  [  160/  265]
train() client id: f_00001-5-5 loss: 0.396064  [  192/  265]
train() client id: f_00001-5-6 loss: 0.347640  [  224/  265]
train() client id: f_00001-5-7 loss: 0.636890  [  256/  265]
train() client id: f_00001-6-0 loss: 0.422359  [   32/  265]
train() client id: f_00001-6-1 loss: 0.491789  [   64/  265]
train() client id: f_00001-6-2 loss: 0.429101  [   96/  265]
train() client id: f_00001-6-3 loss: 0.514614  [  128/  265]
train() client id: f_00001-6-4 loss: 0.418013  [  160/  265]
train() client id: f_00001-6-5 loss: 0.421833  [  192/  265]
train() client id: f_00001-6-6 loss: 0.365391  [  224/  265]
train() client id: f_00001-6-7 loss: 0.423007  [  256/  265]
train() client id: f_00001-7-0 loss: 0.351905  [   32/  265]
train() client id: f_00001-7-1 loss: 0.523500  [   64/  265]
train() client id: f_00001-7-2 loss: 0.435233  [   96/  265]
train() client id: f_00001-7-3 loss: 0.425516  [  128/  265]
train() client id: f_00001-7-4 loss: 0.386025  [  160/  265]
train() client id: f_00001-7-5 loss: 0.393212  [  192/  265]
train() client id: f_00001-7-6 loss: 0.595272  [  224/  265]
train() client id: f_00001-7-7 loss: 0.415055  [  256/  265]
train() client id: f_00001-8-0 loss: 0.466155  [   32/  265]
train() client id: f_00001-8-1 loss: 0.337979  [   64/  265]
train() client id: f_00001-8-2 loss: 0.590019  [   96/  265]
train() client id: f_00001-8-3 loss: 0.381393  [  128/  265]
train() client id: f_00001-8-4 loss: 0.485860  [  160/  265]
train() client id: f_00001-8-5 loss: 0.434557  [  192/  265]
train() client id: f_00001-8-6 loss: 0.424145  [  224/  265]
train() client id: f_00001-8-7 loss: 0.408230  [  256/  265]
train() client id: f_00001-9-0 loss: 0.353554  [   32/  265]
train() client id: f_00001-9-1 loss: 0.472769  [   64/  265]
train() client id: f_00001-9-2 loss: 0.347804  [   96/  265]
train() client id: f_00001-9-3 loss: 0.482500  [  128/  265]
train() client id: f_00001-9-4 loss: 0.340617  [  160/  265]
train() client id: f_00001-9-5 loss: 0.325993  [  192/  265]
train() client id: f_00001-9-6 loss: 0.547843  [  224/  265]
train() client id: f_00001-9-7 loss: 0.553090  [  256/  265]
train() client id: f_00001-10-0 loss: 0.543723  [   32/  265]
train() client id: f_00001-10-1 loss: 0.325792  [   64/  265]
train() client id: f_00001-10-2 loss: 0.552138  [   96/  265]
train() client id: f_00001-10-3 loss: 0.437168  [  128/  265]
train() client id: f_00001-10-4 loss: 0.449798  [  160/  265]
train() client id: f_00001-10-5 loss: 0.419244  [  192/  265]
train() client id: f_00001-10-6 loss: 0.366992  [  224/  265]
train() client id: f_00001-10-7 loss: 0.417076  [  256/  265]
train() client id: f_00001-11-0 loss: 0.355792  [   32/  265]
train() client id: f_00001-11-1 loss: 0.526476  [   64/  265]
train() client id: f_00001-11-2 loss: 0.431376  [   96/  265]
train() client id: f_00001-11-3 loss: 0.519705  [  128/  265]
train() client id: f_00001-11-4 loss: 0.543229  [  160/  265]
train() client id: f_00001-11-5 loss: 0.406019  [  192/  265]
train() client id: f_00001-11-6 loss: 0.396115  [  224/  265]
train() client id: f_00001-11-7 loss: 0.336575  [  256/  265]
train() client id: f_00002-0-0 loss: 1.321940  [   32/  124]
train() client id: f_00002-0-1 loss: 1.466185  [   64/  124]
train() client id: f_00002-0-2 loss: 1.163855  [   96/  124]
train() client id: f_00002-1-0 loss: 1.305425  [   32/  124]
train() client id: f_00002-1-1 loss: 1.324254  [   64/  124]
train() client id: f_00002-1-2 loss: 1.100943  [   96/  124]
train() client id: f_00002-2-0 loss: 1.209979  [   32/  124]
train() client id: f_00002-2-1 loss: 1.196585  [   64/  124]
train() client id: f_00002-2-2 loss: 1.131210  [   96/  124]
train() client id: f_00002-3-0 loss: 1.118275  [   32/  124]
train() client id: f_00002-3-1 loss: 1.095257  [   64/  124]
train() client id: f_00002-3-2 loss: 1.224385  [   96/  124]
train() client id: f_00002-4-0 loss: 1.231997  [   32/  124]
train() client id: f_00002-4-1 loss: 1.233652  [   64/  124]
train() client id: f_00002-4-2 loss: 1.072025  [   96/  124]
train() client id: f_00002-5-0 loss: 1.101216  [   32/  124]
train() client id: f_00002-5-1 loss: 1.055578  [   64/  124]
train() client id: f_00002-5-2 loss: 1.294019  [   96/  124]
train() client id: f_00002-6-0 loss: 1.149181  [   32/  124]
train() client id: f_00002-6-1 loss: 1.007945  [   64/  124]
train() client id: f_00002-6-2 loss: 1.315154  [   96/  124]
train() client id: f_00002-7-0 loss: 0.955605  [   32/  124]
train() client id: f_00002-7-1 loss: 1.312858  [   64/  124]
train() client id: f_00002-7-2 loss: 1.099057  [   96/  124]
train() client id: f_00002-8-0 loss: 1.113170  [   32/  124]
train() client id: f_00002-8-1 loss: 1.069166  [   64/  124]
train() client id: f_00002-8-2 loss: 1.057456  [   96/  124]
train() client id: f_00002-9-0 loss: 1.135530  [   32/  124]
train() client id: f_00002-9-1 loss: 1.060732  [   64/  124]
train() client id: f_00002-9-2 loss: 1.132012  [   96/  124]
train() client id: f_00002-10-0 loss: 1.047046  [   32/  124]
train() client id: f_00002-10-1 loss: 1.055144  [   64/  124]
train() client id: f_00002-10-2 loss: 1.207021  [   96/  124]
train() client id: f_00002-11-0 loss: 1.010082  [   32/  124]
train() client id: f_00002-11-1 loss: 1.215070  [   64/  124]
train() client id: f_00002-11-2 loss: 0.952390  [   96/  124]
train() client id: f_00003-0-0 loss: 0.600584  [   32/   43]
train() client id: f_00003-1-0 loss: 0.723419  [   32/   43]
train() client id: f_00003-2-0 loss: 0.535752  [   32/   43]
train() client id: f_00003-3-0 loss: 0.725668  [   32/   43]
train() client id: f_00003-4-0 loss: 0.743746  [   32/   43]
train() client id: f_00003-5-0 loss: 0.475366  [   32/   43]
train() client id: f_00003-6-0 loss: 0.613312  [   32/   43]
train() client id: f_00003-7-0 loss: 0.643861  [   32/   43]
train() client id: f_00003-8-0 loss: 0.625286  [   32/   43]
train() client id: f_00003-9-0 loss: 0.523870  [   32/   43]
train() client id: f_00003-10-0 loss: 0.569711  [   32/   43]
train() client id: f_00003-11-0 loss: 0.629514  [   32/   43]
train() client id: f_00004-0-0 loss: 0.766325  [   32/  306]
train() client id: f_00004-0-1 loss: 0.880701  [   64/  306]
train() client id: f_00004-0-2 loss: 0.683361  [   96/  306]
train() client id: f_00004-0-3 loss: 0.720832  [  128/  306]
train() client id: f_00004-0-4 loss: 0.920796  [  160/  306]
train() client id: f_00004-0-5 loss: 0.730603  [  192/  306]
train() client id: f_00004-0-6 loss: 0.827896  [  224/  306]
train() client id: f_00004-0-7 loss: 0.813853  [  256/  306]
train() client id: f_00004-0-8 loss: 0.703604  [  288/  306]
train() client id: f_00004-1-0 loss: 0.745769  [   32/  306]
train() client id: f_00004-1-1 loss: 0.799374  [   64/  306]
train() client id: f_00004-1-2 loss: 0.668602  [   96/  306]
train() client id: f_00004-1-3 loss: 0.754674  [  128/  306]
train() client id: f_00004-1-4 loss: 0.635601  [  160/  306]
train() client id: f_00004-1-5 loss: 0.884890  [  192/  306]
train() client id: f_00004-1-6 loss: 0.965896  [  224/  306]
train() client id: f_00004-1-7 loss: 0.687374  [  256/  306]
train() client id: f_00004-1-8 loss: 0.807869  [  288/  306]
train() client id: f_00004-2-0 loss: 0.818010  [   32/  306]
train() client id: f_00004-2-1 loss: 0.808690  [   64/  306]
train() client id: f_00004-2-2 loss: 0.825280  [   96/  306]
train() client id: f_00004-2-3 loss: 0.788372  [  128/  306]
train() client id: f_00004-2-4 loss: 0.856288  [  160/  306]
train() client id: f_00004-2-5 loss: 0.733165  [  192/  306]
train() client id: f_00004-2-6 loss: 0.842834  [  224/  306]
train() client id: f_00004-2-7 loss: 0.712817  [  256/  306]
train() client id: f_00004-2-8 loss: 0.740235  [  288/  306]
train() client id: f_00004-3-0 loss: 0.726471  [   32/  306]
train() client id: f_00004-3-1 loss: 0.897938  [   64/  306]
train() client id: f_00004-3-2 loss: 0.936206  [   96/  306]
train() client id: f_00004-3-3 loss: 0.785278  [  128/  306]
train() client id: f_00004-3-4 loss: 0.699372  [  160/  306]
train() client id: f_00004-3-5 loss: 0.782616  [  192/  306]
train() client id: f_00004-3-6 loss: 0.572197  [  224/  306]
train() client id: f_00004-3-7 loss: 0.865214  [  256/  306]
train() client id: f_00004-3-8 loss: 0.777434  [  288/  306]
train() client id: f_00004-4-0 loss: 0.717498  [   32/  306]
train() client id: f_00004-4-1 loss: 0.751316  [   64/  306]
train() client id: f_00004-4-2 loss: 0.944146  [   96/  306]
train() client id: f_00004-4-3 loss: 0.889965  [  128/  306]
train() client id: f_00004-4-4 loss: 0.794156  [  160/  306]
train() client id: f_00004-4-5 loss: 0.737999  [  192/  306]
train() client id: f_00004-4-6 loss: 0.933418  [  224/  306]
train() client id: f_00004-4-7 loss: 0.555084  [  256/  306]
train() client id: f_00004-4-8 loss: 0.784220  [  288/  306]
train() client id: f_00004-5-0 loss: 0.777053  [   32/  306]
train() client id: f_00004-5-1 loss: 0.724403  [   64/  306]
train() client id: f_00004-5-2 loss: 0.805224  [   96/  306]
train() client id: f_00004-5-3 loss: 0.737687  [  128/  306]
train() client id: f_00004-5-4 loss: 0.984803  [  160/  306]
train() client id: f_00004-5-5 loss: 0.738476  [  192/  306]
train() client id: f_00004-5-6 loss: 0.789989  [  224/  306]
train() client id: f_00004-5-7 loss: 0.815339  [  256/  306]
train() client id: f_00004-5-8 loss: 0.762867  [  288/  306]
train() client id: f_00004-6-0 loss: 0.853049  [   32/  306]
train() client id: f_00004-6-1 loss: 0.720969  [   64/  306]
train() client id: f_00004-6-2 loss: 0.776864  [   96/  306]
train() client id: f_00004-6-3 loss: 0.802197  [  128/  306]
train() client id: f_00004-6-4 loss: 0.719698  [  160/  306]
train() client id: f_00004-6-5 loss: 0.726965  [  192/  306]
train() client id: f_00004-6-6 loss: 0.946440  [  224/  306]
train() client id: f_00004-6-7 loss: 0.851651  [  256/  306]
train() client id: f_00004-6-8 loss: 0.662662  [  288/  306]
train() client id: f_00004-7-0 loss: 0.818385  [   32/  306]
train() client id: f_00004-7-1 loss: 0.730812  [   64/  306]
train() client id: f_00004-7-2 loss: 0.792080  [   96/  306]
train() client id: f_00004-7-3 loss: 0.764353  [  128/  306]
train() client id: f_00004-7-4 loss: 0.780482  [  160/  306]
train() client id: f_00004-7-5 loss: 0.871697  [  192/  306]
train() client id: f_00004-7-6 loss: 0.726070  [  224/  306]
train() client id: f_00004-7-7 loss: 0.780177  [  256/  306]
train() client id: f_00004-7-8 loss: 0.765901  [  288/  306]
train() client id: f_00004-8-0 loss: 0.740894  [   32/  306]
train() client id: f_00004-8-1 loss: 0.785800  [   64/  306]
train() client id: f_00004-8-2 loss: 0.756702  [   96/  306]
train() client id: f_00004-8-3 loss: 0.887356  [  128/  306]
train() client id: f_00004-8-4 loss: 0.717711  [  160/  306]
train() client id: f_00004-8-5 loss: 0.828363  [  192/  306]
train() client id: f_00004-8-6 loss: 0.758212  [  224/  306]
train() client id: f_00004-8-7 loss: 0.836184  [  256/  306]
train() client id: f_00004-8-8 loss: 0.765348  [  288/  306]
train() client id: f_00004-9-0 loss: 0.724142  [   32/  306]
train() client id: f_00004-9-1 loss: 0.829484  [   64/  306]
train() client id: f_00004-9-2 loss: 0.759206  [   96/  306]
train() client id: f_00004-9-3 loss: 0.899382  [  128/  306]
train() client id: f_00004-9-4 loss: 0.809436  [  160/  306]
train() client id: f_00004-9-5 loss: 0.861629  [  192/  306]
train() client id: f_00004-9-6 loss: 0.663966  [  224/  306]
train() client id: f_00004-9-7 loss: 0.802862  [  256/  306]
train() client id: f_00004-9-8 loss: 0.691630  [  288/  306]
train() client id: f_00004-10-0 loss: 0.785342  [   32/  306]
train() client id: f_00004-10-1 loss: 0.874039  [   64/  306]
train() client id: f_00004-10-2 loss: 0.625269  [   96/  306]
train() client id: f_00004-10-3 loss: 0.980811  [  128/  306]
train() client id: f_00004-10-4 loss: 0.780869  [  160/  306]
train() client id: f_00004-10-5 loss: 0.735581  [  192/  306]
train() client id: f_00004-10-6 loss: 0.803090  [  224/  306]
train() client id: f_00004-10-7 loss: 0.821053  [  256/  306]
train() client id: f_00004-10-8 loss: 0.785541  [  288/  306]
train() client id: f_00004-11-0 loss: 0.933003  [   32/  306]
train() client id: f_00004-11-1 loss: 0.753062  [   64/  306]
train() client id: f_00004-11-2 loss: 0.712101  [   96/  306]
train() client id: f_00004-11-3 loss: 0.692976  [  128/  306]
train() client id: f_00004-11-4 loss: 0.733212  [  160/  306]
train() client id: f_00004-11-5 loss: 0.934409  [  192/  306]
train() client id: f_00004-11-6 loss: 0.770250  [  224/  306]
train() client id: f_00004-11-7 loss: 0.878078  [  256/  306]
train() client id: f_00004-11-8 loss: 0.679326  [  288/  306]
train() client id: f_00005-0-0 loss: 0.530908  [   32/  146]
train() client id: f_00005-0-1 loss: 0.675278  [   64/  146]
train() client id: f_00005-0-2 loss: 0.480647  [   96/  146]
train() client id: f_00005-0-3 loss: 0.493300  [  128/  146]
train() client id: f_00005-1-0 loss: 0.635245  [   32/  146]
train() client id: f_00005-1-1 loss: 0.489717  [   64/  146]
train() client id: f_00005-1-2 loss: 0.449441  [   96/  146]
train() client id: f_00005-1-3 loss: 0.676817  [  128/  146]
train() client id: f_00005-2-0 loss: 0.528228  [   32/  146]
train() client id: f_00005-2-1 loss: 0.330112  [   64/  146]
train() client id: f_00005-2-2 loss: 0.570606  [   96/  146]
train() client id: f_00005-2-3 loss: 0.616754  [  128/  146]
train() client id: f_00005-3-0 loss: 0.511729  [   32/  146]
train() client id: f_00005-3-1 loss: 0.619570  [   64/  146]
train() client id: f_00005-3-2 loss: 0.417354  [   96/  146]
train() client id: f_00005-3-3 loss: 0.604322  [  128/  146]
train() client id: f_00005-4-0 loss: 0.778991  [   32/  146]
train() client id: f_00005-4-1 loss: 0.443484  [   64/  146]
train() client id: f_00005-4-2 loss: 0.522475  [   96/  146]
train() client id: f_00005-4-3 loss: 0.399624  [  128/  146]
train() client id: f_00005-5-0 loss: 0.232865  [   32/  146]
train() client id: f_00005-5-1 loss: 0.402111  [   64/  146]
train() client id: f_00005-5-2 loss: 0.672227  [   96/  146]
train() client id: f_00005-5-3 loss: 0.572500  [  128/  146]
train() client id: f_00005-6-0 loss: 0.335925  [   32/  146]
train() client id: f_00005-6-1 loss: 0.568538  [   64/  146]
train() client id: f_00005-6-2 loss: 0.396738  [   96/  146]
train() client id: f_00005-6-3 loss: 0.748749  [  128/  146]
train() client id: f_00005-7-0 loss: 0.385611  [   32/  146]
train() client id: f_00005-7-1 loss: 0.467221  [   64/  146]
train() client id: f_00005-7-2 loss: 0.647840  [   96/  146]
train() client id: f_00005-7-3 loss: 0.594937  [  128/  146]
train() client id: f_00005-8-0 loss: 0.455220  [   32/  146]
train() client id: f_00005-8-1 loss: 0.323520  [   64/  146]
train() client id: f_00005-8-2 loss: 0.759302  [   96/  146]
train() client id: f_00005-8-3 loss: 0.525405  [  128/  146]
train() client id: f_00005-9-0 loss: 0.343228  [   32/  146]
train() client id: f_00005-9-1 loss: 0.544324  [   64/  146]
train() client id: f_00005-9-2 loss: 0.669958  [   96/  146]
train() client id: f_00005-9-3 loss: 0.361793  [  128/  146]
train() client id: f_00005-10-0 loss: 0.464790  [   32/  146]
train() client id: f_00005-10-1 loss: 0.517754  [   64/  146]
train() client id: f_00005-10-2 loss: 0.517601  [   96/  146]
train() client id: f_00005-10-3 loss: 0.738872  [  128/  146]
train() client id: f_00005-11-0 loss: 0.316767  [   32/  146]
train() client id: f_00005-11-1 loss: 0.593523  [   64/  146]
train() client id: f_00005-11-2 loss: 0.629268  [   96/  146]
train() client id: f_00005-11-3 loss: 0.591208  [  128/  146]
train() client id: f_00006-0-0 loss: 0.549559  [   32/   54]
train() client id: f_00006-1-0 loss: 0.568649  [   32/   54]
train() client id: f_00006-2-0 loss: 0.512239  [   32/   54]
train() client id: f_00006-3-0 loss: 0.497688  [   32/   54]
train() client id: f_00006-4-0 loss: 0.472778  [   32/   54]
train() client id: f_00006-5-0 loss: 0.483502  [   32/   54]
train() client id: f_00006-6-0 loss: 0.467131  [   32/   54]
train() client id: f_00006-7-0 loss: 0.545385  [   32/   54]
train() client id: f_00006-8-0 loss: 0.546432  [   32/   54]
train() client id: f_00006-9-0 loss: 0.513940  [   32/   54]
train() client id: f_00006-10-0 loss: 0.514287  [   32/   54]
train() client id: f_00006-11-0 loss: 0.557826  [   32/   54]
train() client id: f_00007-0-0 loss: 0.506975  [   32/  179]
train() client id: f_00007-0-1 loss: 0.663241  [   64/  179]
train() client id: f_00007-0-2 loss: 0.605808  [   96/  179]
train() client id: f_00007-0-3 loss: 0.632338  [  128/  179]
train() client id: f_00007-0-4 loss: 0.517117  [  160/  179]
train() client id: f_00007-1-0 loss: 0.745433  [   32/  179]
train() client id: f_00007-1-1 loss: 0.776421  [   64/  179]
train() client id: f_00007-1-2 loss: 0.401997  [   96/  179]
train() client id: f_00007-1-3 loss: 0.565407  [  128/  179]
train() client id: f_00007-1-4 loss: 0.499315  [  160/  179]
train() client id: f_00007-2-0 loss: 0.684736  [   32/  179]
train() client id: f_00007-2-1 loss: 0.537773  [   64/  179]
train() client id: f_00007-2-2 loss: 0.469787  [   96/  179]
train() client id: f_00007-2-3 loss: 0.834018  [  128/  179]
train() client id: f_00007-2-4 loss: 0.465624  [  160/  179]
train() client id: f_00007-3-0 loss: 0.527834  [   32/  179]
train() client id: f_00007-3-1 loss: 0.592469  [   64/  179]
train() client id: f_00007-3-2 loss: 0.701362  [   96/  179]
train() client id: f_00007-3-3 loss: 0.527137  [  128/  179]
train() client id: f_00007-3-4 loss: 0.546867  [  160/  179]
train() client id: f_00007-4-0 loss: 0.508467  [   32/  179]
train() client id: f_00007-4-1 loss: 0.617324  [   64/  179]
train() client id: f_00007-4-2 loss: 0.679174  [   96/  179]
train() client id: f_00007-4-3 loss: 0.422824  [  128/  179]
train() client id: f_00007-4-4 loss: 0.630930  [  160/  179]
train() client id: f_00007-5-0 loss: 0.452094  [   32/  179]
train() client id: f_00007-5-1 loss: 0.710775  [   64/  179]
train() client id: f_00007-5-2 loss: 0.496813  [   96/  179]
train() client id: f_00007-5-3 loss: 0.526311  [  128/  179]
train() client id: f_00007-5-4 loss: 0.592277  [  160/  179]
train() client id: f_00007-6-0 loss: 0.501894  [   32/  179]
train() client id: f_00007-6-1 loss: 0.498944  [   64/  179]
train() client id: f_00007-6-2 loss: 0.637757  [   96/  179]
train() client id: f_00007-6-3 loss: 0.536597  [  128/  179]
train() client id: f_00007-6-4 loss: 0.684390  [  160/  179]
train() client id: f_00007-7-0 loss: 0.375606  [   32/  179]
train() client id: f_00007-7-1 loss: 0.463279  [   64/  179]
train() client id: f_00007-7-2 loss: 0.513889  [   96/  179]
train() client id: f_00007-7-3 loss: 0.677199  [  128/  179]
train() client id: f_00007-7-4 loss: 0.675456  [  160/  179]
train() client id: f_00007-8-0 loss: 0.554458  [   32/  179]
train() client id: f_00007-8-1 loss: 0.709995  [   64/  179]
train() client id: f_00007-8-2 loss: 0.645699  [   96/  179]
train() client id: f_00007-8-3 loss: 0.491585  [  128/  179]
train() client id: f_00007-8-4 loss: 0.415434  [  160/  179]
train() client id: f_00007-9-0 loss: 0.586384  [   32/  179]
train() client id: f_00007-9-1 loss: 0.725982  [   64/  179]
train() client id: f_00007-9-2 loss: 0.511462  [   96/  179]
train() client id: f_00007-9-3 loss: 0.455996  [  128/  179]
train() client id: f_00007-9-4 loss: 0.383421  [  160/  179]
train() client id: f_00007-10-0 loss: 0.501781  [   32/  179]
train() client id: f_00007-10-1 loss: 0.514560  [   64/  179]
train() client id: f_00007-10-2 loss: 0.508936  [   96/  179]
train() client id: f_00007-10-3 loss: 0.545419  [  128/  179]
train() client id: f_00007-10-4 loss: 0.543293  [  160/  179]
train() client id: f_00007-11-0 loss: 0.578648  [   32/  179]
train() client id: f_00007-11-1 loss: 0.393875  [   64/  179]
train() client id: f_00007-11-2 loss: 0.415992  [   96/  179]
train() client id: f_00007-11-3 loss: 0.764039  [  128/  179]
train() client id: f_00007-11-4 loss: 0.680868  [  160/  179]
train() client id: f_00008-0-0 loss: 0.786728  [   32/  130]
train() client id: f_00008-0-1 loss: 0.668150  [   64/  130]
train() client id: f_00008-0-2 loss: 0.782320  [   96/  130]
train() client id: f_00008-0-3 loss: 0.838830  [  128/  130]
train() client id: f_00008-1-0 loss: 0.870352  [   32/  130]
train() client id: f_00008-1-1 loss: 0.731855  [   64/  130]
train() client id: f_00008-1-2 loss: 0.752173  [   96/  130]
train() client id: f_00008-1-3 loss: 0.711401  [  128/  130]
train() client id: f_00008-2-0 loss: 0.699544  [   32/  130]
train() client id: f_00008-2-1 loss: 0.818463  [   64/  130]
train() client id: f_00008-2-2 loss: 0.831919  [   96/  130]
train() client id: f_00008-2-3 loss: 0.712258  [  128/  130]
train() client id: f_00008-3-0 loss: 0.764064  [   32/  130]
train() client id: f_00008-3-1 loss: 0.800004  [   64/  130]
train() client id: f_00008-3-2 loss: 0.840538  [   96/  130]
train() client id: f_00008-3-3 loss: 0.691713  [  128/  130]
train() client id: f_00008-4-0 loss: 0.691889  [   32/  130]
train() client id: f_00008-4-1 loss: 0.770802  [   64/  130]
train() client id: f_00008-4-2 loss: 0.806632  [   96/  130]
train() client id: f_00008-4-3 loss: 0.836162  [  128/  130]
train() client id: f_00008-5-0 loss: 0.642758  [   32/  130]
train() client id: f_00008-5-1 loss: 0.795255  [   64/  130]
train() client id: f_00008-5-2 loss: 0.792036  [   96/  130]
train() client id: f_00008-5-3 loss: 0.872872  [  128/  130]
train() client id: f_00008-6-0 loss: 0.756643  [   32/  130]
train() client id: f_00008-6-1 loss: 0.831265  [   64/  130]
train() client id: f_00008-6-2 loss: 0.737427  [   96/  130]
train() client id: f_00008-6-3 loss: 0.780934  [  128/  130]
train() client id: f_00008-7-0 loss: 0.695080  [   32/  130]
train() client id: f_00008-7-1 loss: 0.760755  [   64/  130]
train() client id: f_00008-7-2 loss: 0.804438  [   96/  130]
train() client id: f_00008-7-3 loss: 0.803997  [  128/  130]
train() client id: f_00008-8-0 loss: 0.809313  [   32/  130]
train() client id: f_00008-8-1 loss: 0.807490  [   64/  130]
train() client id: f_00008-8-2 loss: 0.795130  [   96/  130]
train() client id: f_00008-8-3 loss: 0.695085  [  128/  130]
train() client id: f_00008-9-0 loss: 0.860756  [   32/  130]
train() client id: f_00008-9-1 loss: 0.700816  [   64/  130]
train() client id: f_00008-9-2 loss: 0.773799  [   96/  130]
train() client id: f_00008-9-3 loss: 0.754941  [  128/  130]
train() client id: f_00008-10-0 loss: 0.688429  [   32/  130]
train() client id: f_00008-10-1 loss: 0.878818  [   64/  130]
train() client id: f_00008-10-2 loss: 0.822387  [   96/  130]
train() client id: f_00008-10-3 loss: 0.711293  [  128/  130]
train() client id: f_00008-11-0 loss: 0.715585  [   32/  130]
train() client id: f_00008-11-1 loss: 0.784253  [   64/  130]
train() client id: f_00008-11-2 loss: 0.880309  [   96/  130]
train() client id: f_00008-11-3 loss: 0.725739  [  128/  130]
train() client id: f_00009-0-0 loss: 1.012276  [   32/  118]
train() client id: f_00009-0-1 loss: 0.979035  [   64/  118]
train() client id: f_00009-0-2 loss: 0.952717  [   96/  118]
train() client id: f_00009-1-0 loss: 0.774146  [   32/  118]
train() client id: f_00009-1-1 loss: 0.945418  [   64/  118]
train() client id: f_00009-1-2 loss: 1.010413  [   96/  118]
train() client id: f_00009-2-0 loss: 0.953572  [   32/  118]
train() client id: f_00009-2-1 loss: 0.820325  [   64/  118]
train() client id: f_00009-2-2 loss: 0.885743  [   96/  118]
train() client id: f_00009-3-0 loss: 1.010049  [   32/  118]
train() client id: f_00009-3-1 loss: 0.804833  [   64/  118]
train() client id: f_00009-3-2 loss: 0.738747  [   96/  118]
train() client id: f_00009-4-0 loss: 0.756014  [   32/  118]
train() client id: f_00009-4-1 loss: 0.846408  [   64/  118]
train() client id: f_00009-4-2 loss: 0.870707  [   96/  118]
train() client id: f_00009-5-0 loss: 0.787575  [   32/  118]
train() client id: f_00009-5-1 loss: 0.845907  [   64/  118]
train() client id: f_00009-5-2 loss: 0.789742  [   96/  118]
train() client id: f_00009-6-0 loss: 0.855957  [   32/  118]
train() client id: f_00009-6-1 loss: 0.778272  [   64/  118]
train() client id: f_00009-6-2 loss: 0.727564  [   96/  118]
train() client id: f_00009-7-0 loss: 0.843041  [   32/  118]
train() client id: f_00009-7-1 loss: 0.797109  [   64/  118]
train() client id: f_00009-7-2 loss: 0.683128  [   96/  118]
train() client id: f_00009-8-0 loss: 0.734924  [   32/  118]
train() client id: f_00009-8-1 loss: 0.814638  [   64/  118]
train() client id: f_00009-8-2 loss: 0.765516  [   96/  118]
train() client id: f_00009-9-0 loss: 0.754147  [   32/  118]
train() client id: f_00009-9-1 loss: 0.782176  [   64/  118]
train() client id: f_00009-9-2 loss: 0.763403  [   96/  118]
train() client id: f_00009-10-0 loss: 0.820415  [   32/  118]
train() client id: f_00009-10-1 loss: 0.798918  [   64/  118]
train() client id: f_00009-10-2 loss: 0.644683  [   96/  118]
train() client id: f_00009-11-0 loss: 0.787181  [   32/  118]
train() client id: f_00009-11-1 loss: 0.706243  [   64/  118]
train() client id: f_00009-11-2 loss: 0.668207  [   96/  118]
At round 38 accuracy: 0.6525198938992043
At round 38 training accuracy: 0.590878604963112
At round 38 training loss: 0.8302487554414651
update_location
xs = [  -3.9056584     4.20031788  210.00902392   18.81129433    0.97929623
    3.95640986 -172.44319194 -151.32485185  194.66397685 -137.06087855]
ys = [ 202.5879595   185.55583871    1.32061395 -172.45517586  164.35018685
  147.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [225.95826053 210.82839455 232.60596327 200.23649137 192.3848823
 178.50679634 199.35783152 181.38326009 219.5512101  169.71062514]
dists_bs = [173.30426779 179.51094579 422.3700719  397.85414041 176.30125133
 181.00957718 177.64338525 175.77931364 401.79095764 175.15670175]
uav_gains = [1.14573089e-11 1.45365625e-11 1.02368353e-11 1.69945603e-11
 1.90332055e-11 2.32656486e-11 1.72124693e-11 2.23088121e-11
 1.27076078e-11 2.65155347e-11]
bs_gains = [5.95152365e-11 5.39311234e-11 4.91307394e-12 5.80854309e-12
 5.67255845e-11 5.26901863e-11 5.55337207e-11 5.71984612e-11
 5.65058798e-12 5.77695734e-11]
Round 39
-------------------------------
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.84371614 12.07295727  5.75849455  2.07999855 13.92335007  6.69969194
  2.5762787   8.21232374  6.06245015  5.43331894]
obj_prev = 68.66258004963703
eta_min = 1.603615121775097e-16	eta_max = 0.9314544727787696
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 15.921783734685466	eta = 0.909090909090909
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 29.685339818895304	eta = 0.4875924930628838
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 22.863992502600777	eta = 0.6330630509114977
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 21.629256357976853	eta = 0.6692023345673616
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 21.563057577188566	eta = 0.6712567917560257
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 21.562850936268624	eta = 0.6712632245380996
eta = 0.6712632245380996
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [0.03289102 0.06917555 0.03236894 0.01122471 0.07987818 0.03811181
 0.01409615 0.04672613 0.03393518 0.0308027 ]
ene_total = [1.94642197 3.43896482 1.93822068 0.92238666 3.91833323 2.03975789
 1.05049566 2.49573328 2.10825374 1.704283  ]
ti_comp = [0.51931345 0.55115743 0.51591145 0.5297508  0.55187418 0.5508223
 0.53005444 0.53584373 0.4938656  0.55212944]
ti_coms = [0.102748   0.07090402 0.10615    0.09231065 0.07018727 0.07123916
 0.09200701 0.08621772 0.12819586 0.06993201]
t_total = [28.04983635 28.04983635 28.04983635 28.04983635 28.04983635 28.04983635
 28.04983635 28.04983635 28.04983635 28.04983635]
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [8.24617860e-06 6.81061808e-05 7.96370126e-06 3.14965047e-07
 1.04588604e-04 1.14034376e-05 6.23075349e-07 2.22066489e-05
 1.00141191e-05 5.99189852e-06]
ene_total = [0.4636805  0.32278962 0.47900796 0.41625902 0.32120273 0.32174399
 0.41490377 0.38977208 0.57850904 0.31560581]
optimize_network iter = 0 obj = 4.023474514862622
eta = 0.6712632245380996
freqs = [31667787.3789384  62754800.17478311 31370631.5489173  10594331.96949695
 72369920.453618   34595378.85521331 13296888.56956221 43600515.27273497
 34356690.70712458 27894452.74815087]
eta_min = 0.6712632245381114	eta_max = 0.6712632245380962
af = 0.008905236727720691	bf = 1.3191144648464428	zeta = 0.009795760400492761	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [1.97179310e-06 1.62852764e-05 1.90424825e-06 7.53131768e-08
 2.50088069e-05 2.72674418e-06 1.48987275e-07 5.30996467e-06
 2.39453594e-06 1.43275870e-06]
ene_total = [1.67569409 1.15879127 1.73115483 1.50519706 1.14852664 1.1620451
 1.50025809 1.40670146 2.09070673 1.14052021]
ti_comp = [0.51931345 0.55115743 0.51591145 0.5297508  0.55187418 0.5508223
 0.53005444 0.53584373 0.4938656  0.55212944]
ti_coms = [0.102748   0.07090402 0.10615    0.09231065 0.07018727 0.07123916
 0.09200701 0.08621772 0.12819586 0.06993201]
t_total = [28.04983635 28.04983635 28.04983635 28.04983635 28.04983635 28.04983635
 28.04983635 28.04983635 28.04983635 28.04983635]
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [8.24617860e-06 6.81061808e-05 7.96370126e-06 3.14965047e-07
 1.04588604e-04 1.14034376e-05 6.23075349e-07 2.22066489e-05
 1.00141191e-05 5.99189852e-06]
ene_total = [0.4636805  0.32278962 0.47900796 0.41625902 0.32120273 0.32174399
 0.41490377 0.38977208 0.57850904 0.31560581]
optimize_network iter = 1 obj = 4.0234745148625795
eta = 0.6712632245380962
freqs = [31667787.37893841 62754800.17478317 31370631.54891731 10594331.96949696
 72369920.4536181  34595378.85521334 13296888.56956222 43600515.27273501
 34356690.70712457 27894452.7481509 ]
Done!
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [8.21333132e-06 6.78348912e-05 7.93197918e-06 3.13710436e-07
 1.04171993e-04 1.13580139e-05 6.20593432e-07 2.21181924e-05
 9.97422956e-06 5.96803079e-06]
ene_total = [0.01028301 0.00715824 0.01062293 0.00923138 0.0071229  0.00713527
 0.00920132 0.00864389 0.01282956 0.00699917]
At round 39 energy consumption: 0.08922767374894407
At round 39 eta: 0.6712632245380962
At round 39 a_n: 14.480768977481684
At round 39 local rounds: 13.051990433476096
At round 39 global rounds: 45.091744918477566
gradient difference: 0.41282376646995544
train() client id: f_00000-0-0 loss: 1.503155  [   32/  126]
train() client id: f_00000-0-1 loss: 1.197444  [   64/  126]
train() client id: f_00000-0-2 loss: 1.349442  [   96/  126]
train() client id: f_00000-1-0 loss: 1.308866  [   32/  126]
train() client id: f_00000-1-1 loss: 1.086642  [   64/  126]
train() client id: f_00000-1-2 loss: 1.109636  [   96/  126]
train() client id: f_00000-2-0 loss: 1.366608  [   32/  126]
train() client id: f_00000-2-1 loss: 1.073293  [   64/  126]
train() client id: f_00000-2-2 loss: 0.965982  [   96/  126]
train() client id: f_00000-3-0 loss: 1.065491  [   32/  126]
train() client id: f_00000-3-1 loss: 0.906894  [   64/  126]
train() client id: f_00000-3-2 loss: 1.075213  [   96/  126]
train() client id: f_00000-4-0 loss: 1.222025  [   32/  126]
train() client id: f_00000-4-1 loss: 0.844878  [   64/  126]
train() client id: f_00000-4-2 loss: 0.934475  [   96/  126]
train() client id: f_00000-5-0 loss: 0.947210  [   32/  126]
train() client id: f_00000-5-1 loss: 0.933405  [   64/  126]
train() client id: f_00000-5-2 loss: 0.952427  [   96/  126]
train() client id: f_00000-6-0 loss: 0.951881  [   32/  126]
train() client id: f_00000-6-1 loss: 0.837311  [   64/  126]
train() client id: f_00000-6-2 loss: 0.916366  [   96/  126]
train() client id: f_00000-7-0 loss: 0.978661  [   32/  126]
train() client id: f_00000-7-1 loss: 0.791501  [   64/  126]
train() client id: f_00000-7-2 loss: 0.875494  [   96/  126]
train() client id: f_00000-8-0 loss: 0.875598  [   32/  126]
train() client id: f_00000-8-1 loss: 0.948523  [   64/  126]
train() client id: f_00000-8-2 loss: 0.846676  [   96/  126]
train() client id: f_00000-9-0 loss: 0.803595  [   32/  126]
train() client id: f_00000-9-1 loss: 0.836694  [   64/  126]
train() client id: f_00000-9-2 loss: 0.857116  [   96/  126]
train() client id: f_00000-10-0 loss: 0.788517  [   32/  126]
train() client id: f_00000-10-1 loss: 0.931212  [   64/  126]
train() client id: f_00000-10-2 loss: 0.867105  [   96/  126]
train() client id: f_00000-11-0 loss: 0.767420  [   32/  126]
train() client id: f_00000-11-1 loss: 0.893642  [   64/  126]
train() client id: f_00000-11-2 loss: 0.803035  [   96/  126]
train() client id: f_00000-12-0 loss: 0.768216  [   32/  126]
train() client id: f_00000-12-1 loss: 0.787238  [   64/  126]
train() client id: f_00000-12-2 loss: 1.053334  [   96/  126]
train() client id: f_00001-0-0 loss: 0.524595  [   32/  265]
train() client id: f_00001-0-1 loss: 0.501407  [   64/  265]
train() client id: f_00001-0-2 loss: 0.504736  [   96/  265]
train() client id: f_00001-0-3 loss: 0.473725  [  128/  265]
train() client id: f_00001-0-4 loss: 0.494988  [  160/  265]
train() client id: f_00001-0-5 loss: 0.511146  [  192/  265]
train() client id: f_00001-0-6 loss: 0.460943  [  224/  265]
train() client id: f_00001-0-7 loss: 0.488408  [  256/  265]
train() client id: f_00001-1-0 loss: 0.451627  [   32/  265]
train() client id: f_00001-1-1 loss: 0.539305  [   64/  265]
train() client id: f_00001-1-2 loss: 0.410541  [   96/  265]
train() client id: f_00001-1-3 loss: 0.407747  [  128/  265]
train() client id: f_00001-1-4 loss: 0.508879  [  160/  265]
train() client id: f_00001-1-5 loss: 0.529291  [  192/  265]
train() client id: f_00001-1-6 loss: 0.436505  [  224/  265]
train() client id: f_00001-1-7 loss: 0.629469  [  256/  265]
train() client id: f_00001-2-0 loss: 0.521862  [   32/  265]
train() client id: f_00001-2-1 loss: 0.440870  [   64/  265]
train() client id: f_00001-2-2 loss: 0.636030  [   96/  265]
train() client id: f_00001-2-3 loss: 0.455248  [  128/  265]
train() client id: f_00001-2-4 loss: 0.371217  [  160/  265]
train() client id: f_00001-2-5 loss: 0.438865  [  192/  265]
train() client id: f_00001-2-6 loss: 0.629126  [  224/  265]
train() client id: f_00001-2-7 loss: 0.375426  [  256/  265]
train() client id: f_00001-3-0 loss: 0.402831  [   32/  265]
train() client id: f_00001-3-1 loss: 0.479200  [   64/  265]
train() client id: f_00001-3-2 loss: 0.563886  [   96/  265]
train() client id: f_00001-3-3 loss: 0.605616  [  128/  265]
train() client id: f_00001-3-4 loss: 0.383197  [  160/  265]
train() client id: f_00001-3-5 loss: 0.559049  [  192/  265]
train() client id: f_00001-3-6 loss: 0.428085  [  224/  265]
train() client id: f_00001-3-7 loss: 0.418971  [  256/  265]
train() client id: f_00001-4-0 loss: 0.467684  [   32/  265]
train() client id: f_00001-4-1 loss: 0.534712  [   64/  265]
train() client id: f_00001-4-2 loss: 0.469036  [   96/  265]
train() client id: f_00001-4-3 loss: 0.472038  [  128/  265]
train() client id: f_00001-4-4 loss: 0.546220  [  160/  265]
train() client id: f_00001-4-5 loss: 0.452309  [  192/  265]
train() client id: f_00001-4-6 loss: 0.404125  [  224/  265]
train() client id: f_00001-4-7 loss: 0.473138  [  256/  265]
train() client id: f_00001-5-0 loss: 0.527595  [   32/  265]
train() client id: f_00001-5-1 loss: 0.456802  [   64/  265]
train() client id: f_00001-5-2 loss: 0.422165  [   96/  265]
train() client id: f_00001-5-3 loss: 0.581135  [  128/  265]
train() client id: f_00001-5-4 loss: 0.516055  [  160/  265]
train() client id: f_00001-5-5 loss: 0.491326  [  192/  265]
train() client id: f_00001-5-6 loss: 0.386550  [  224/  265]
train() client id: f_00001-5-7 loss: 0.428237  [  256/  265]
train() client id: f_00001-6-0 loss: 0.606497  [   32/  265]
train() client id: f_00001-6-1 loss: 0.439764  [   64/  265]
train() client id: f_00001-6-2 loss: 0.450690  [   96/  265]
train() client id: f_00001-6-3 loss: 0.595954  [  128/  265]
train() client id: f_00001-6-4 loss: 0.444235  [  160/  265]
train() client id: f_00001-6-5 loss: 0.374359  [  192/  265]
train() client id: f_00001-6-6 loss: 0.393464  [  224/  265]
train() client id: f_00001-6-7 loss: 0.496870  [  256/  265]
train() client id: f_00001-7-0 loss: 0.605884  [   32/  265]
train() client id: f_00001-7-1 loss: 0.550288  [   64/  265]
train() client id: f_00001-7-2 loss: 0.368076  [   96/  265]
train() client id: f_00001-7-3 loss: 0.556669  [  128/  265]
train() client id: f_00001-7-4 loss: 0.403084  [  160/  265]
train() client id: f_00001-7-5 loss: 0.537377  [  192/  265]
train() client id: f_00001-7-6 loss: 0.380156  [  224/  265]
train() client id: f_00001-7-7 loss: 0.399667  [  256/  265]
train() client id: f_00001-8-0 loss: 0.451273  [   32/  265]
train() client id: f_00001-8-1 loss: 0.411213  [   64/  265]
train() client id: f_00001-8-2 loss: 0.449966  [   96/  265]
train() client id: f_00001-8-3 loss: 0.470609  [  128/  265]
train() client id: f_00001-8-4 loss: 0.532484  [  160/  265]
train() client id: f_00001-8-5 loss: 0.456289  [  192/  265]
train() client id: f_00001-8-6 loss: 0.529873  [  224/  265]
train() client id: f_00001-8-7 loss: 0.500139  [  256/  265]
train() client id: f_00001-9-0 loss: 0.369191  [   32/  265]
train() client id: f_00001-9-1 loss: 0.518776  [   64/  265]
train() client id: f_00001-9-2 loss: 0.481418  [   96/  265]
train() client id: f_00001-9-3 loss: 0.450728  [  128/  265]
train() client id: f_00001-9-4 loss: 0.523717  [  160/  265]
train() client id: f_00001-9-5 loss: 0.388201  [  192/  265]
train() client id: f_00001-9-6 loss: 0.372593  [  224/  265]
train() client id: f_00001-9-7 loss: 0.566225  [  256/  265]
train() client id: f_00001-10-0 loss: 0.460070  [   32/  265]
train() client id: f_00001-10-1 loss: 0.641222  [   64/  265]
train() client id: f_00001-10-2 loss: 0.456964  [   96/  265]
train() client id: f_00001-10-3 loss: 0.456274  [  128/  265]
train() client id: f_00001-10-4 loss: 0.417915  [  160/  265]
train() client id: f_00001-10-5 loss: 0.457255  [  192/  265]
train() client id: f_00001-10-6 loss: 0.440536  [  224/  265]
train() client id: f_00001-10-7 loss: 0.475493  [  256/  265]
train() client id: f_00001-11-0 loss: 0.455123  [   32/  265]
train() client id: f_00001-11-1 loss: 0.533154  [   64/  265]
train() client id: f_00001-11-2 loss: 0.524542  [   96/  265]
train() client id: f_00001-11-3 loss: 0.450205  [  128/  265]
train() client id: f_00001-11-4 loss: 0.392072  [  160/  265]
train() client id: f_00001-11-5 loss: 0.544471  [  192/  265]
train() client id: f_00001-11-6 loss: 0.473990  [  224/  265]
train() client id: f_00001-11-7 loss: 0.385188  [  256/  265]
train() client id: f_00001-12-0 loss: 0.519741  [   32/  265]
train() client id: f_00001-12-1 loss: 0.505093  [   64/  265]
train() client id: f_00001-12-2 loss: 0.531778  [   96/  265]
train() client id: f_00001-12-3 loss: 0.387830  [  128/  265]
train() client id: f_00001-12-4 loss: 0.656596  [  160/  265]
train() client id: f_00001-12-5 loss: 0.473186  [  192/  265]
train() client id: f_00001-12-6 loss: 0.373168  [  224/  265]
train() client id: f_00001-12-7 loss: 0.384889  [  256/  265]
train() client id: f_00002-0-0 loss: 1.510804  [   32/  124]
train() client id: f_00002-0-1 loss: 1.154584  [   64/  124]
train() client id: f_00002-0-2 loss: 1.205222  [   96/  124]
train() client id: f_00002-1-0 loss: 1.171479  [   32/  124]
train() client id: f_00002-1-1 loss: 1.296429  [   64/  124]
train() client id: f_00002-1-2 loss: 1.204733  [   96/  124]
train() client id: f_00002-2-0 loss: 1.263662  [   32/  124]
train() client id: f_00002-2-1 loss: 1.225606  [   64/  124]
train() client id: f_00002-2-2 loss: 1.170952  [   96/  124]
train() client id: f_00002-3-0 loss: 1.161759  [   32/  124]
train() client id: f_00002-3-1 loss: 1.102085  [   64/  124]
train() client id: f_00002-3-2 loss: 1.253262  [   96/  124]
train() client id: f_00002-4-0 loss: 1.193678  [   32/  124]
train() client id: f_00002-4-1 loss: 1.320154  [   64/  124]
train() client id: f_00002-4-2 loss: 0.928478  [   96/  124]
train() client id: f_00002-5-0 loss: 1.001841  [   32/  124]
train() client id: f_00002-5-1 loss: 1.001710  [   64/  124]
train() client id: f_00002-5-2 loss: 1.223157  [   96/  124]
train() client id: f_00002-6-0 loss: 1.119153  [   32/  124]
train() client id: f_00002-6-1 loss: 0.940515  [   64/  124]
train() client id: f_00002-6-2 loss: 1.205185  [   96/  124]
train() client id: f_00002-7-0 loss: 1.066960  [   32/  124]
train() client id: f_00002-7-1 loss: 1.009908  [   64/  124]
train() client id: f_00002-7-2 loss: 1.226282  [   96/  124]
train() client id: f_00002-8-0 loss: 1.067169  [   32/  124]
train() client id: f_00002-8-1 loss: 0.941527  [   64/  124]
train() client id: f_00002-8-2 loss: 1.113453  [   96/  124]
train() client id: f_00002-9-0 loss: 1.190238  [   32/  124]
train() client id: f_00002-9-1 loss: 1.066148  [   64/  124]
train() client id: f_00002-9-2 loss: 1.022424  [   96/  124]
train() client id: f_00002-10-0 loss: 1.059108  [   32/  124]
train() client id: f_00002-10-1 loss: 1.182164  [   64/  124]
train() client id: f_00002-10-2 loss: 1.061306  [   96/  124]
train() client id: f_00002-11-0 loss: 1.014579  [   32/  124]
train() client id: f_00002-11-1 loss: 1.131182  [   64/  124]
train() client id: f_00002-11-2 loss: 1.107336  [   96/  124]
train() client id: f_00002-12-0 loss: 1.258289  [   32/  124]
train() client id: f_00002-12-1 loss: 0.946034  [   64/  124]
train() client id: f_00002-12-2 loss: 0.970701  [   96/  124]
train() client id: f_00003-0-0 loss: 0.705082  [   32/   43]
train() client id: f_00003-1-0 loss: 0.636641  [   32/   43]
train() client id: f_00003-2-0 loss: 0.842840  [   32/   43]
train() client id: f_00003-3-0 loss: 0.733459  [   32/   43]
train() client id: f_00003-4-0 loss: 0.932710  [   32/   43]
train() client id: f_00003-5-0 loss: 0.715036  [   32/   43]
train() client id: f_00003-6-0 loss: 0.833224  [   32/   43]
train() client id: f_00003-7-0 loss: 0.715562  [   32/   43]
train() client id: f_00003-8-0 loss: 0.977097  [   32/   43]
train() client id: f_00003-9-0 loss: 0.755704  [   32/   43]
train() client id: f_00003-10-0 loss: 0.457787  [   32/   43]
train() client id: f_00003-11-0 loss: 0.660968  [   32/   43]
train() client id: f_00003-12-0 loss: 0.773926  [   32/   43]
train() client id: f_00004-0-0 loss: 0.946483  [   32/  306]
train() client id: f_00004-0-1 loss: 0.837206  [   64/  306]
train() client id: f_00004-0-2 loss: 1.046860  [   96/  306]
train() client id: f_00004-0-3 loss: 0.932722  [  128/  306]
train() client id: f_00004-0-4 loss: 0.925781  [  160/  306]
train() client id: f_00004-0-5 loss: 0.996741  [  192/  306]
train() client id: f_00004-0-6 loss: 1.027503  [  224/  306]
train() client id: f_00004-0-7 loss: 0.908289  [  256/  306]
train() client id: f_00004-0-8 loss: 0.878449  [  288/  306]
train() client id: f_00004-1-0 loss: 0.880845  [   32/  306]
train() client id: f_00004-1-1 loss: 0.828400  [   64/  306]
train() client id: f_00004-1-2 loss: 0.877102  [   96/  306]
train() client id: f_00004-1-3 loss: 1.026363  [  128/  306]
train() client id: f_00004-1-4 loss: 0.906090  [  160/  306]
train() client id: f_00004-1-5 loss: 0.905363  [  192/  306]
train() client id: f_00004-1-6 loss: 1.082299  [  224/  306]
train() client id: f_00004-1-7 loss: 0.948999  [  256/  306]
train() client id: f_00004-1-8 loss: 0.905334  [  288/  306]
train() client id: f_00004-2-0 loss: 0.929968  [   32/  306]
train() client id: f_00004-2-1 loss: 0.927669  [   64/  306]
train() client id: f_00004-2-2 loss: 0.888741  [   96/  306]
train() client id: f_00004-2-3 loss: 0.989079  [  128/  306]
train() client id: f_00004-2-4 loss: 0.855299  [  160/  306]
train() client id: f_00004-2-5 loss: 0.896476  [  192/  306]
train() client id: f_00004-2-6 loss: 0.827801  [  224/  306]
train() client id: f_00004-2-7 loss: 0.981597  [  256/  306]
train() client id: f_00004-2-8 loss: 1.089135  [  288/  306]
train() client id: f_00004-3-0 loss: 0.932722  [   32/  306]
train() client id: f_00004-3-1 loss: 0.880362  [   64/  306]
train() client id: f_00004-3-2 loss: 0.921223  [   96/  306]
train() client id: f_00004-3-3 loss: 0.961154  [  128/  306]
train() client id: f_00004-3-4 loss: 0.987974  [  160/  306]
train() client id: f_00004-3-5 loss: 0.923646  [  192/  306]
train() client id: f_00004-3-6 loss: 0.962386  [  224/  306]
train() client id: f_00004-3-7 loss: 0.893850  [  256/  306]
train() client id: f_00004-3-8 loss: 0.933855  [  288/  306]
train() client id: f_00004-4-0 loss: 0.767209  [   32/  306]
train() client id: f_00004-4-1 loss: 0.920656  [   64/  306]
train() client id: f_00004-4-2 loss: 0.978657  [   96/  306]
train() client id: f_00004-4-3 loss: 0.915682  [  128/  306]
train() client id: f_00004-4-4 loss: 0.879762  [  160/  306]
train() client id: f_00004-4-5 loss: 0.915810  [  192/  306]
train() client id: f_00004-4-6 loss: 0.881616  [  224/  306]
train() client id: f_00004-4-7 loss: 1.042898  [  256/  306]
train() client id: f_00004-4-8 loss: 0.943531  [  288/  306]
train() client id: f_00004-5-0 loss: 1.039509  [   32/  306]
train() client id: f_00004-5-1 loss: 0.828282  [   64/  306]
train() client id: f_00004-5-2 loss: 0.917317  [   96/  306]
train() client id: f_00004-5-3 loss: 0.940002  [  128/  306]
train() client id: f_00004-5-4 loss: 1.010713  [  160/  306]
train() client id: f_00004-5-5 loss: 0.886977  [  192/  306]
train() client id: f_00004-5-6 loss: 0.776971  [  224/  306]
train() client id: f_00004-5-7 loss: 0.937844  [  256/  306]
train() client id: f_00004-5-8 loss: 0.882951  [  288/  306]
train() client id: f_00004-6-0 loss: 0.845605  [   32/  306]
train() client id: f_00004-6-1 loss: 0.868085  [   64/  306]
train() client id: f_00004-6-2 loss: 0.926439  [   96/  306]
train() client id: f_00004-6-3 loss: 0.988544  [  128/  306]
train() client id: f_00004-6-4 loss: 0.902245  [  160/  306]
train() client id: f_00004-6-5 loss: 1.080703  [  192/  306]
train() client id: f_00004-6-6 loss: 0.786743  [  224/  306]
train() client id: f_00004-6-7 loss: 0.972761  [  256/  306]
train() client id: f_00004-6-8 loss: 0.972301  [  288/  306]
train() client id: f_00004-7-0 loss: 0.787500  [   32/  306]
train() client id: f_00004-7-1 loss: 1.059782  [   64/  306]
train() client id: f_00004-7-2 loss: 0.906283  [   96/  306]
train() client id: f_00004-7-3 loss: 0.855583  [  128/  306]
train() client id: f_00004-7-4 loss: 0.921619  [  160/  306]
train() client id: f_00004-7-5 loss: 0.972280  [  192/  306]
train() client id: f_00004-7-6 loss: 0.917780  [  224/  306]
train() client id: f_00004-7-7 loss: 0.865624  [  256/  306]
train() client id: f_00004-7-8 loss: 0.934141  [  288/  306]
train() client id: f_00004-8-0 loss: 0.793517  [   32/  306]
train() client id: f_00004-8-1 loss: 0.945268  [   64/  306]
train() client id: f_00004-8-2 loss: 0.881702  [   96/  306]
train() client id: f_00004-8-3 loss: 0.902119  [  128/  306]
train() client id: f_00004-8-4 loss: 0.900657  [  160/  306]
train() client id: f_00004-8-5 loss: 0.924996  [  192/  306]
train() client id: f_00004-8-6 loss: 1.012774  [  224/  306]
train() client id: f_00004-8-7 loss: 1.030082  [  256/  306]
train() client id: f_00004-8-8 loss: 0.863691  [  288/  306]
train() client id: f_00004-9-0 loss: 0.869038  [   32/  306]
train() client id: f_00004-9-1 loss: 0.927690  [   64/  306]
train() client id: f_00004-9-2 loss: 0.840447  [   96/  306]
train() client id: f_00004-9-3 loss: 0.925499  [  128/  306]
train() client id: f_00004-9-4 loss: 0.917979  [  160/  306]
train() client id: f_00004-9-5 loss: 0.983152  [  192/  306]
train() client id: f_00004-9-6 loss: 0.956214  [  224/  306]
train() client id: f_00004-9-7 loss: 0.999294  [  256/  306]
train() client id: f_00004-9-8 loss: 0.921472  [  288/  306]
train() client id: f_00004-10-0 loss: 0.925866  [   32/  306]
train() client id: f_00004-10-1 loss: 0.993235  [   64/  306]
train() client id: f_00004-10-2 loss: 0.901295  [   96/  306]
train() client id: f_00004-10-3 loss: 0.921153  [  128/  306]
train() client id: f_00004-10-4 loss: 0.864188  [  160/  306]
train() client id: f_00004-10-5 loss: 0.941669  [  192/  306]
train() client id: f_00004-10-6 loss: 0.980352  [  224/  306]
train() client id: f_00004-10-7 loss: 0.777429  [  256/  306]
train() client id: f_00004-10-8 loss: 0.942813  [  288/  306]
train() client id: f_00004-11-0 loss: 0.806013  [   32/  306]
train() client id: f_00004-11-1 loss: 0.921916  [   64/  306]
train() client id: f_00004-11-2 loss: 1.062816  [   96/  306]
train() client id: f_00004-11-3 loss: 0.887549  [  128/  306]
train() client id: f_00004-11-4 loss: 0.868544  [  160/  306]
train() client id: f_00004-11-5 loss: 0.889586  [  192/  306]
train() client id: f_00004-11-6 loss: 0.863959  [  224/  306]
train() client id: f_00004-11-7 loss: 0.972306  [  256/  306]
train() client id: f_00004-11-8 loss: 0.921757  [  288/  306]
train() client id: f_00004-12-0 loss: 0.829153  [   32/  306]
train() client id: f_00004-12-1 loss: 0.870840  [   64/  306]
train() client id: f_00004-12-2 loss: 0.965480  [   96/  306]
train() client id: f_00004-12-3 loss: 0.974899  [  128/  306]
train() client id: f_00004-12-4 loss: 0.832521  [  160/  306]
train() client id: f_00004-12-5 loss: 0.984823  [  192/  306]
train() client id: f_00004-12-6 loss: 0.992340  [  224/  306]
train() client id: f_00004-12-7 loss: 0.961803  [  256/  306]
train() client id: f_00004-12-8 loss: 0.971032  [  288/  306]
train() client id: f_00005-0-0 loss: 0.566701  [   32/  146]
train() client id: f_00005-0-1 loss: 0.646167  [   64/  146]
train() client id: f_00005-0-2 loss: 0.381853  [   96/  146]
train() client id: f_00005-0-3 loss: 0.472063  [  128/  146]
train() client id: f_00005-1-0 loss: 0.419521  [   32/  146]
train() client id: f_00005-1-1 loss: 0.866315  [   64/  146]
train() client id: f_00005-1-2 loss: 0.418624  [   96/  146]
train() client id: f_00005-1-3 loss: 0.619628  [  128/  146]
train() client id: f_00005-2-0 loss: 0.849075  [   32/  146]
train() client id: f_00005-2-1 loss: 0.337797  [   64/  146]
train() client id: f_00005-2-2 loss: 0.638542  [   96/  146]
train() client id: f_00005-2-3 loss: 0.296478  [  128/  146]
train() client id: f_00005-3-0 loss: 0.667499  [   32/  146]
train() client id: f_00005-3-1 loss: 0.502575  [   64/  146]
train() client id: f_00005-3-2 loss: 0.563249  [   96/  146]
train() client id: f_00005-3-3 loss: 0.562677  [  128/  146]
train() client id: f_00005-4-0 loss: 0.303019  [   32/  146]
train() client id: f_00005-4-1 loss: 0.642891  [   64/  146]
train() client id: f_00005-4-2 loss: 0.602302  [   96/  146]
train() client id: f_00005-4-3 loss: 0.685266  [  128/  146]
train() client id: f_00005-5-0 loss: 0.668448  [   32/  146]
train() client id: f_00005-5-1 loss: 0.350943  [   64/  146]
train() client id: f_00005-5-2 loss: 0.559466  [   96/  146]
train() client id: f_00005-5-3 loss: 0.366317  [  128/  146]
train() client id: f_00005-6-0 loss: 0.633378  [   32/  146]
train() client id: f_00005-6-1 loss: 0.415849  [   64/  146]
train() client id: f_00005-6-2 loss: 0.578727  [   96/  146]
train() client id: f_00005-6-3 loss: 0.468599  [  128/  146]
train() client id: f_00005-7-0 loss: 0.520906  [   32/  146]
train() client id: f_00005-7-1 loss: 0.748434  [   64/  146]
train() client id: f_00005-7-2 loss: 0.417287  [   96/  146]
train() client id: f_00005-7-3 loss: 0.513041  [  128/  146]
train() client id: f_00005-8-0 loss: 0.382800  [   32/  146]
train() client id: f_00005-8-1 loss: 0.710691  [   64/  146]
train() client id: f_00005-8-2 loss: 0.679510  [   96/  146]
train() client id: f_00005-8-3 loss: 0.569666  [  128/  146]
train() client id: f_00005-9-0 loss: 0.529298  [   32/  146]
train() client id: f_00005-9-1 loss: 0.496571  [   64/  146]
train() client id: f_00005-9-2 loss: 0.470629  [   96/  146]
train() client id: f_00005-9-3 loss: 0.729708  [  128/  146]
train() client id: f_00005-10-0 loss: 0.284190  [   32/  146]
train() client id: f_00005-10-1 loss: 0.737334  [   64/  146]
train() client id: f_00005-10-2 loss: 0.304954  [   96/  146]
train() client id: f_00005-10-3 loss: 0.677102  [  128/  146]
train() client id: f_00005-11-0 loss: 0.553805  [   32/  146]
train() client id: f_00005-11-1 loss: 0.537459  [   64/  146]
train() client id: f_00005-11-2 loss: 0.775123  [   96/  146]
train() client id: f_00005-11-3 loss: 0.334377  [  128/  146]
train() client id: f_00005-12-0 loss: 0.387796  [   32/  146]
train() client id: f_00005-12-1 loss: 0.401903  [   64/  146]
train() client id: f_00005-12-2 loss: 0.872108  [   96/  146]
train() client id: f_00005-12-3 loss: 0.555236  [  128/  146]
train() client id: f_00006-0-0 loss: 0.396687  [   32/   54]
train() client id: f_00006-1-0 loss: 0.461010  [   32/   54]
train() client id: f_00006-2-0 loss: 0.402412  [   32/   54]
train() client id: f_00006-3-0 loss: 0.455986  [   32/   54]
train() client id: f_00006-4-0 loss: 0.491761  [   32/   54]
train() client id: f_00006-5-0 loss: 0.443316  [   32/   54]
train() client id: f_00006-6-0 loss: 0.516834  [   32/   54]
train() client id: f_00006-7-0 loss: 0.515221  [   32/   54]
train() client id: f_00006-8-0 loss: 0.451927  [   32/   54]
train() client id: f_00006-9-0 loss: 0.416150  [   32/   54]
train() client id: f_00006-10-0 loss: 0.463465  [   32/   54]
train() client id: f_00006-11-0 loss: 0.449798  [   32/   54]
train() client id: f_00006-12-0 loss: 0.504120  [   32/   54]
train() client id: f_00007-0-0 loss: 0.722839  [   32/  179]
train() client id: f_00007-0-1 loss: 0.409308  [   64/  179]
train() client id: f_00007-0-2 loss: 0.316286  [   96/  179]
train() client id: f_00007-0-3 loss: 0.522499  [  128/  179]
train() client id: f_00007-0-4 loss: 0.472851  [  160/  179]
train() client id: f_00007-1-0 loss: 0.361203  [   32/  179]
train() client id: f_00007-1-1 loss: 0.380924  [   64/  179]
train() client id: f_00007-1-2 loss: 0.381523  [   96/  179]
train() client id: f_00007-1-3 loss: 0.374195  [  128/  179]
train() client id: f_00007-1-4 loss: 0.537764  [  160/  179]
train() client id: f_00007-2-0 loss: 0.443316  [   32/  179]
train() client id: f_00007-2-1 loss: 0.441264  [   64/  179]
train() client id: f_00007-2-2 loss: 0.346903  [   96/  179]
train() client id: f_00007-2-3 loss: 0.427212  [  128/  179]
train() client id: f_00007-2-4 loss: 0.451556  [  160/  179]
train() client id: f_00007-3-0 loss: 0.475414  [   32/  179]
train() client id: f_00007-3-1 loss: 0.299476  [   64/  179]
train() client id: f_00007-3-2 loss: 0.410793  [   96/  179]
train() client id: f_00007-3-3 loss: 0.470425  [  128/  179]
train() client id: f_00007-3-4 loss: 0.624959  [  160/  179]
train() client id: f_00007-4-0 loss: 0.501011  [   32/  179]
train() client id: f_00007-4-1 loss: 0.301105  [   64/  179]
train() client id: f_00007-4-2 loss: 0.293675  [   96/  179]
train() client id: f_00007-4-3 loss: 0.490172  [  128/  179]
train() client id: f_00007-4-4 loss: 0.713218  [  160/  179]
train() client id: f_00007-5-0 loss: 0.618462  [   32/  179]
train() client id: f_00007-5-1 loss: 0.405691  [   64/  179]
train() client id: f_00007-5-2 loss: 0.392141  [   96/  179]
train() client id: f_00007-5-3 loss: 0.370271  [  128/  179]
train() client id: f_00007-5-4 loss: 0.448130  [  160/  179]
train() client id: f_00007-6-0 loss: 0.498111  [   32/  179]
train() client id: f_00007-6-1 loss: 0.499328  [   64/  179]
train() client id: f_00007-6-2 loss: 0.475048  [   96/  179]
train() client id: f_00007-6-3 loss: 0.371283  [  128/  179]
train() client id: f_00007-6-4 loss: 0.410501  [  160/  179]
train() client id: f_00007-7-0 loss: 0.376072  [   32/  179]
train() client id: f_00007-7-1 loss: 0.565974  [   64/  179]
train() client id: f_00007-7-2 loss: 0.459411  [   96/  179]
train() client id: f_00007-7-3 loss: 0.463074  [  128/  179]
train() client id: f_00007-7-4 loss: 0.281048  [  160/  179]
train() client id: f_00007-8-0 loss: 0.425863  [   32/  179]
train() client id: f_00007-8-1 loss: 0.420114  [   64/  179]
train() client id: f_00007-8-2 loss: 0.282266  [   96/  179]
train() client id: f_00007-8-3 loss: 0.534162  [  128/  179]
train() client id: f_00007-8-4 loss: 0.289924  [  160/  179]
train() client id: f_00007-9-0 loss: 0.396966  [   32/  179]
train() client id: f_00007-9-1 loss: 0.465859  [   64/  179]
train() client id: f_00007-9-2 loss: 0.440431  [   96/  179]
train() client id: f_00007-9-3 loss: 0.364753  [  128/  179]
train() client id: f_00007-9-4 loss: 0.572144  [  160/  179]
train() client id: f_00007-10-0 loss: 0.281237  [   32/  179]
train() client id: f_00007-10-1 loss: 0.357606  [   64/  179]
train() client id: f_00007-10-2 loss: 0.470383  [   96/  179]
train() client id: f_00007-10-3 loss: 0.468826  [  128/  179]
train() client id: f_00007-10-4 loss: 0.651164  [  160/  179]
train() client id: f_00007-11-0 loss: 0.278816  [   32/  179]
train() client id: f_00007-11-1 loss: 0.489452  [   64/  179]
train() client id: f_00007-11-2 loss: 0.400061  [   96/  179]
train() client id: f_00007-11-3 loss: 0.535188  [  128/  179]
train() client id: f_00007-11-4 loss: 0.332890  [  160/  179]
train() client id: f_00007-12-0 loss: 0.630462  [   32/  179]
train() client id: f_00007-12-1 loss: 0.292454  [   64/  179]
train() client id: f_00007-12-2 loss: 0.386071  [   96/  179]
train() client id: f_00007-12-3 loss: 0.464197  [  128/  179]
train() client id: f_00007-12-4 loss: 0.442419  [  160/  179]
train() client id: f_00008-0-0 loss: 0.694656  [   32/  130]
train() client id: f_00008-0-1 loss: 0.852375  [   64/  130]
train() client id: f_00008-0-2 loss: 0.705936  [   96/  130]
train() client id: f_00008-0-3 loss: 0.767508  [  128/  130]
train() client id: f_00008-1-0 loss: 0.807738  [   32/  130]
train() client id: f_00008-1-1 loss: 0.682334  [   64/  130]
train() client id: f_00008-1-2 loss: 0.747485  [   96/  130]
train() client id: f_00008-1-3 loss: 0.762084  [  128/  130]
train() client id: f_00008-2-0 loss: 0.783156  [   32/  130]
train() client id: f_00008-2-1 loss: 0.690717  [   64/  130]
train() client id: f_00008-2-2 loss: 0.780071  [   96/  130]
train() client id: f_00008-2-3 loss: 0.768052  [  128/  130]
train() client id: f_00008-3-0 loss: 0.678630  [   32/  130]
train() client id: f_00008-3-1 loss: 0.811430  [   64/  130]
train() client id: f_00008-3-2 loss: 0.645799  [   96/  130]
train() client id: f_00008-3-3 loss: 0.877804  [  128/  130]
train() client id: f_00008-4-0 loss: 0.769502  [   32/  130]
train() client id: f_00008-4-1 loss: 0.827581  [   64/  130]
train() client id: f_00008-4-2 loss: 0.731097  [   96/  130]
train() client id: f_00008-4-3 loss: 0.679304  [  128/  130]
train() client id: f_00008-5-0 loss: 0.711379  [   32/  130]
train() client id: f_00008-5-1 loss: 0.838909  [   64/  130]
train() client id: f_00008-5-2 loss: 0.682435  [   96/  130]
train() client id: f_00008-5-3 loss: 0.764436  [  128/  130]
train() client id: f_00008-6-0 loss: 0.758495  [   32/  130]
train() client id: f_00008-6-1 loss: 0.837178  [   64/  130]
train() client id: f_00008-6-2 loss: 0.793513  [   96/  130]
train() client id: f_00008-6-3 loss: 0.614460  [  128/  130]
train() client id: f_00008-7-0 loss: 0.889813  [   32/  130]
train() client id: f_00008-7-1 loss: 0.733431  [   64/  130]
train() client id: f_00008-7-2 loss: 0.697559  [   96/  130]
train() client id: f_00008-7-3 loss: 0.677564  [  128/  130]
train() client id: f_00008-8-0 loss: 0.688927  [   32/  130]
train() client id: f_00008-8-1 loss: 0.787510  [   64/  130]
train() client id: f_00008-8-2 loss: 0.790138  [   96/  130]
train() client id: f_00008-8-3 loss: 0.732983  [  128/  130]
train() client id: f_00008-9-0 loss: 0.793620  [   32/  130]
train() client id: f_00008-9-1 loss: 0.740760  [   64/  130]
train() client id: f_00008-9-2 loss: 0.793553  [   96/  130]
train() client id: f_00008-9-3 loss: 0.661030  [  128/  130]
train() client id: f_00008-10-0 loss: 0.654227  [   32/  130]
train() client id: f_00008-10-1 loss: 0.850420  [   64/  130]
train() client id: f_00008-10-2 loss: 0.777693  [   96/  130]
train() client id: f_00008-10-3 loss: 0.695278  [  128/  130]
train() client id: f_00008-11-0 loss: 0.834689  [   32/  130]
train() client id: f_00008-11-1 loss: 0.673912  [   64/  130]
train() client id: f_00008-11-2 loss: 0.699279  [   96/  130]
train() client id: f_00008-11-3 loss: 0.787288  [  128/  130]
train() client id: f_00008-12-0 loss: 0.843110  [   32/  130]
train() client id: f_00008-12-1 loss: 0.656691  [   64/  130]
train() client id: f_00008-12-2 loss: 0.765169  [   96/  130]
train() client id: f_00008-12-3 loss: 0.723310  [  128/  130]
train() client id: f_00009-0-0 loss: 1.161171  [   32/  118]
train() client id: f_00009-0-1 loss: 1.102610  [   64/  118]
train() client id: f_00009-0-2 loss: 1.108354  [   96/  118]
train() client id: f_00009-1-0 loss: 1.174085  [   32/  118]
train() client id: f_00009-1-1 loss: 1.166361  [   64/  118]
train() client id: f_00009-1-2 loss: 0.990738  [   96/  118]
train() client id: f_00009-2-0 loss: 0.992806  [   32/  118]
train() client id: f_00009-2-1 loss: 1.217970  [   64/  118]
train() client id: f_00009-2-2 loss: 0.996499  [   96/  118]
train() client id: f_00009-3-0 loss: 1.020032  [   32/  118]
train() client id: f_00009-3-1 loss: 1.043139  [   64/  118]
train() client id: f_00009-3-2 loss: 0.927886  [   96/  118]
train() client id: f_00009-4-0 loss: 1.114507  [   32/  118]
train() client id: f_00009-4-1 loss: 0.916130  [   64/  118]
train() client id: f_00009-4-2 loss: 0.820814  [   96/  118]
train() client id: f_00009-5-0 loss: 1.028445  [   32/  118]
train() client id: f_00009-5-1 loss: 0.922128  [   64/  118]
train() client id: f_00009-5-2 loss: 0.946704  [   96/  118]
train() client id: f_00009-6-0 loss: 0.789805  [   32/  118]
train() client id: f_00009-6-1 loss: 1.012672  [   64/  118]
train() client id: f_00009-6-2 loss: 0.994882  [   96/  118]
train() client id: f_00009-7-0 loss: 0.891822  [   32/  118]
train() client id: f_00009-7-1 loss: 0.773068  [   64/  118]
train() client id: f_00009-7-2 loss: 0.994388  [   96/  118]
train() client id: f_00009-8-0 loss: 0.774244  [   32/  118]
train() client id: f_00009-8-1 loss: 0.753032  [   64/  118]
train() client id: f_00009-8-2 loss: 1.132262  [   96/  118]
train() client id: f_00009-9-0 loss: 0.957562  [   32/  118]
train() client id: f_00009-9-1 loss: 0.970100  [   64/  118]
train() client id: f_00009-9-2 loss: 0.806076  [   96/  118]
train() client id: f_00009-10-0 loss: 0.877167  [   32/  118]
train() client id: f_00009-10-1 loss: 0.904332  [   64/  118]
train() client id: f_00009-10-2 loss: 0.963850  [   96/  118]
train() client id: f_00009-11-0 loss: 0.972234  [   32/  118]
train() client id: f_00009-11-1 loss: 0.810312  [   64/  118]
train() client id: f_00009-11-2 loss: 0.919681  [   96/  118]
train() client id: f_00009-12-0 loss: 0.888932  [   32/  118]
train() client id: f_00009-12-1 loss: 0.816675  [   64/  118]
train() client id: f_00009-12-2 loss: 0.911169  [   96/  118]
At round 39 accuracy: 0.6525198938992043
At round 39 training accuracy: 0.5935613682092555
At round 39 training loss: 0.819579244740832
update_location
xs = [  -3.9056584     4.20031788  215.00902392   18.81129433    0.97929623
    3.95640986 -177.44319194 -156.32485185  199.66397685 -142.06087855]
ys = [ 207.5879595   190.55583871    1.32061395 -177.45517586  169.35018685
  152.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [230.45176306 215.24212027 237.12997361 204.55855942 196.67344713
 182.66860115 203.69824964 185.57514802 223.99636967 173.77371801]
dists_bs = [174.17017204 179.87422841 426.93274397 402.22813029 176.06996646
 180.32639446 177.6417863  175.17595617 406.39588262 174.14212286]
uav_gains = [1.06238871e-11 1.35916922e-11 9.45055194e-12 1.59558146e-11
 1.78934679e-11 2.18960415e-11 1.61583748e-11 2.09940106e-11
 1.18323052e-11 2.49490095e-11]
bs_gains = [5.86904580e-11 5.36266962e-11 4.76746578e-12 5.63340860e-12
 5.69344720e-11 5.32510343e-11 5.55351203e-11 5.77517960e-11
 5.47313357e-12 5.87169311e-11]
Round 40
-------------------------------
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.71197888 11.79408709  5.62914561  2.03417315 13.60152567  6.544648
  2.51897531  8.02443866  5.92443305  5.30743185]
obj_prev = 67.09083726402451
eta_min = 7.023642441479729e-17	eta_max = 0.9321930357555286
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 15.5538538243213	eta = 0.909090909090909
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 29.16218476187658	eta = 0.4848699515649548
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 22.40002232600407	eta = 0.6312434383873119
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 21.175688042561685	eta = 0.667740622387296
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 21.10970401340027	eta = 0.6698278243997874
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 21.109495669486368	eta = 0.6698344353843775
eta = 0.6698344353843775
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [0.03306684 0.06954534 0.03254197 0.01128472 0.08030519 0.03831555
 0.0141715  0.04697591 0.03411658 0.03096736]
ene_total = [1.91082934 3.36153203 1.90398611 0.90652186 3.82972324 1.99226433
 1.03177315 2.44413259 2.06480829 1.66392474]
ti_comp = [0.53339798 0.56741874 0.52970956 0.54456162 0.56826829 0.56731763
 0.54487186 0.55089405 0.50878776 0.5686981 ]
ti_coms = [0.10500598 0.07098523 0.10869441 0.09384235 0.07013567 0.07108634
 0.09353211 0.08750992 0.12961621 0.06970587]
t_total = [27.99983215 27.99983215 27.99983215 27.99983215 27.99983215 27.99983215
 27.99983215 27.99983215 27.99983215 27.99983215]
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [7.94246766e-06 6.52945351e-05 7.67601831e-06 3.02871121e-07
 1.00231444e-04 1.09232674e-05 5.99154859e-07 2.13486048e-05
 9.58745418e-06 5.73890489e-06]
ene_total = [0.46089529 0.31419869 0.47706073 0.4115975  0.31200491 0.31225746
 0.41024983 0.3847471  0.56890568 0.30597546]
optimize_network iter = 0 obj = 3.9578926480979075
eta = 0.6698344353843775
freqs = [30996407.38459397 61282205.26212957 30716806.61521623 10361284.76037134
 70657812.12672363 33769043.53321395 13004437.39099001 42636067.25492053
 33527323.09434476 27226537.57500104]
eta_min = 0.6698344353843826	eta_max = 0.6698344353843708
af = 0.008298545367209708	bf = 1.3034344286740114	zeta = 0.00912839990393068	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [1.88907249e-06 1.55299480e-05 1.82569896e-06 7.20362393e-08
 2.38395007e-05 2.59803940e-06 1.42505706e-07 5.07764887e-06
 2.28032353e-06 1.36496714e-06]
ene_total = [1.67292505 1.13318653 1.73166738 1.49481175 1.1209777  1.13273715
 1.48988129 1.3947409  2.06499979 1.11055146]
ti_comp = [0.53339798 0.56741874 0.52970956 0.54456162 0.56826829 0.56731763
 0.54487186 0.55089405 0.50878776 0.5686981 ]
ti_coms = [0.10500598 0.07098523 0.10869441 0.09384235 0.07013567 0.07108634
 0.09353211 0.08750992 0.12961621 0.06970587]
t_total = [27.99983215 27.99983215 27.99983215 27.99983215 27.99983215 27.99983215
 27.99983215 27.99983215 27.99983215 27.99983215]
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [7.94246766e-06 6.52945351e-05 7.67601831e-06 3.02871121e-07
 1.00231444e-04 1.09232674e-05 5.99154859e-07 2.13486048e-05
 9.58745418e-06 5.73890489e-06]
ene_total = [0.46089529 0.31419869 0.47706073 0.4115975  0.31200491 0.31225746
 0.41024983 0.3847471  0.56890568 0.30597546]
optimize_network iter = 1 obj = 3.9578926480978276
eta = 0.6698344353843708
freqs = [30996407.384594   61282205.26212971 30716806.61521625 10361284.76037136
 70657812.12672378 33769043.53321403 13004437.39099002 42636067.2549206
 33527323.09434475 27226537.5750011 ]
Done!
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [7.86876585e-06 6.46886370e-05 7.60478900e-06 3.00060640e-07
 9.93013504e-05 1.08219054e-05 5.93595025e-07 2.11505013e-05
 9.49848778e-06 5.68565095e-06]
ene_total = [0.01050847 0.00716321 0.01087705 0.00938453 0.00711287 0.00711946
 0.0093538  0.00877214 0.01297112 0.00697627]
At round 40 energy consumption: 0.09023892340678759
At round 40 eta: 0.6698344353843708
At round 40 a_n: 14.138223130512367
At round 40 local rounds: 13.121762881988987
At round 40 global rounds: 43.859113515789716
gradient difference: 0.4006223678588867
train() client id: f_00000-0-0 loss: 1.109036  [   32/  126]
train() client id: f_00000-0-1 loss: 1.493413  [   64/  126]
train() client id: f_00000-0-2 loss: 1.350294  [   96/  126]
train() client id: f_00000-1-0 loss: 1.044255  [   32/  126]
train() client id: f_00000-1-1 loss: 1.106146  [   64/  126]
train() client id: f_00000-1-2 loss: 1.094420  [   96/  126]
train() client id: f_00000-2-0 loss: 1.071203  [   32/  126]
train() client id: f_00000-2-1 loss: 1.226017  [   64/  126]
train() client id: f_00000-2-2 loss: 0.943149  [   96/  126]
train() client id: f_00000-3-0 loss: 0.883551  [   32/  126]
train() client id: f_00000-3-1 loss: 0.943022  [   64/  126]
train() client id: f_00000-3-2 loss: 0.876405  [   96/  126]
train() client id: f_00000-4-0 loss: 0.986171  [   32/  126]
train() client id: f_00000-4-1 loss: 0.848126  [   64/  126]
train() client id: f_00000-4-2 loss: 0.848351  [   96/  126]
train() client id: f_00000-5-0 loss: 0.771550  [   32/  126]
train() client id: f_00000-5-1 loss: 0.846271  [   64/  126]
train() client id: f_00000-5-2 loss: 0.773691  [   96/  126]
train() client id: f_00000-6-0 loss: 0.844281  [   32/  126]
train() client id: f_00000-6-1 loss: 0.656816  [   64/  126]
train() client id: f_00000-6-2 loss: 0.702304  [   96/  126]
train() client id: f_00000-7-0 loss: 0.741480  [   32/  126]
train() client id: f_00000-7-1 loss: 0.636637  [   64/  126]
train() client id: f_00000-7-2 loss: 0.681031  [   96/  126]
train() client id: f_00000-8-0 loss: 0.673122  [   32/  126]
train() client id: f_00000-8-1 loss: 0.676981  [   64/  126]
train() client id: f_00000-8-2 loss: 0.683857  [   96/  126]
train() client id: f_00000-9-0 loss: 0.669403  [   32/  126]
train() client id: f_00000-9-1 loss: 0.669257  [   64/  126]
train() client id: f_00000-9-2 loss: 0.590330  [   96/  126]
train() client id: f_00000-10-0 loss: 0.621135  [   32/  126]
train() client id: f_00000-10-1 loss: 0.577863  [   64/  126]
train() client id: f_00000-10-2 loss: 0.697601  [   96/  126]
train() client id: f_00000-11-0 loss: 0.782561  [   32/  126]
train() client id: f_00000-11-1 loss: 0.533695  [   64/  126]
train() client id: f_00000-11-2 loss: 0.658327  [   96/  126]
train() client id: f_00000-12-0 loss: 0.631854  [   32/  126]
train() client id: f_00000-12-1 loss: 0.560410  [   64/  126]
train() client id: f_00000-12-2 loss: 0.652130  [   96/  126]
train() client id: f_00001-0-0 loss: 0.494385  [   32/  265]
train() client id: f_00001-0-1 loss: 0.556401  [   64/  265]
train() client id: f_00001-0-2 loss: 0.461877  [   96/  265]
train() client id: f_00001-0-3 loss: 0.485046  [  128/  265]
train() client id: f_00001-0-4 loss: 0.539681  [  160/  265]
train() client id: f_00001-0-5 loss: 0.627258  [  192/  265]
train() client id: f_00001-0-6 loss: 0.444842  [  224/  265]
train() client id: f_00001-0-7 loss: 0.499140  [  256/  265]
train() client id: f_00001-1-0 loss: 0.573416  [   32/  265]
train() client id: f_00001-1-1 loss: 0.529550  [   64/  265]
train() client id: f_00001-1-2 loss: 0.425968  [   96/  265]
train() client id: f_00001-1-3 loss: 0.524944  [  128/  265]
train() client id: f_00001-1-4 loss: 0.543117  [  160/  265]
train() client id: f_00001-1-5 loss: 0.550070  [  192/  265]
train() client id: f_00001-1-6 loss: 0.436688  [  224/  265]
train() client id: f_00001-1-7 loss: 0.489006  [  256/  265]
train() client id: f_00001-2-0 loss: 0.436509  [   32/  265]
train() client id: f_00001-2-1 loss: 0.533570  [   64/  265]
train() client id: f_00001-2-2 loss: 0.422283  [   96/  265]
train() client id: f_00001-2-3 loss: 0.606001  [  128/  265]
train() client id: f_00001-2-4 loss: 0.535468  [  160/  265]
train() client id: f_00001-2-5 loss: 0.562939  [  192/  265]
train() client id: f_00001-2-6 loss: 0.457362  [  224/  265]
train() client id: f_00001-2-7 loss: 0.476735  [  256/  265]
train() client id: f_00001-3-0 loss: 0.418169  [   32/  265]
train() client id: f_00001-3-1 loss: 0.473906  [   64/  265]
train() client id: f_00001-3-2 loss: 0.457643  [   96/  265]
train() client id: f_00001-3-3 loss: 0.534403  [  128/  265]
train() client id: f_00001-3-4 loss: 0.525082  [  160/  265]
train() client id: f_00001-3-5 loss: 0.442205  [  192/  265]
train() client id: f_00001-3-6 loss: 0.598977  [  224/  265]
train() client id: f_00001-3-7 loss: 0.556056  [  256/  265]
train() client id: f_00001-4-0 loss: 0.542173  [   32/  265]
train() client id: f_00001-4-1 loss: 0.562497  [   64/  265]
train() client id: f_00001-4-2 loss: 0.459080  [   96/  265]
train() client id: f_00001-4-3 loss: 0.480455  [  128/  265]
train() client id: f_00001-4-4 loss: 0.597858  [  160/  265]
train() client id: f_00001-4-5 loss: 0.409006  [  192/  265]
train() client id: f_00001-4-6 loss: 0.461644  [  224/  265]
train() client id: f_00001-4-7 loss: 0.450371  [  256/  265]
train() client id: f_00001-5-0 loss: 0.497599  [   32/  265]
train() client id: f_00001-5-1 loss: 0.542084  [   64/  265]
train() client id: f_00001-5-2 loss: 0.462770  [   96/  265]
train() client id: f_00001-5-3 loss: 0.460205  [  128/  265]
train() client id: f_00001-5-4 loss: 0.488490  [  160/  265]
train() client id: f_00001-5-5 loss: 0.433214  [  192/  265]
train() client id: f_00001-5-6 loss: 0.527222  [  224/  265]
train() client id: f_00001-5-7 loss: 0.503258  [  256/  265]
train() client id: f_00001-6-0 loss: 0.452723  [   32/  265]
train() client id: f_00001-6-1 loss: 0.451386  [   64/  265]
train() client id: f_00001-6-2 loss: 0.493473  [   96/  265]
train() client id: f_00001-6-3 loss: 0.604967  [  128/  265]
train() client id: f_00001-6-4 loss: 0.463101  [  160/  265]
train() client id: f_00001-6-5 loss: 0.565355  [  192/  265]
train() client id: f_00001-6-6 loss: 0.470726  [  224/  265]
train() client id: f_00001-6-7 loss: 0.468720  [  256/  265]
train() client id: f_00001-7-0 loss: 0.629963  [   32/  265]
train() client id: f_00001-7-1 loss: 0.502770  [   64/  265]
train() client id: f_00001-7-2 loss: 0.640804  [   96/  265]
train() client id: f_00001-7-3 loss: 0.396992  [  128/  265]
train() client id: f_00001-7-4 loss: 0.385210  [  160/  265]
train() client id: f_00001-7-5 loss: 0.488209  [  192/  265]
train() client id: f_00001-7-6 loss: 0.444369  [  224/  265]
train() client id: f_00001-7-7 loss: 0.474079  [  256/  265]
train() client id: f_00001-8-0 loss: 0.523521  [   32/  265]
train() client id: f_00001-8-1 loss: 0.490279  [   64/  265]
train() client id: f_00001-8-2 loss: 0.506676  [   96/  265]
train() client id: f_00001-8-3 loss: 0.629947  [  128/  265]
train() client id: f_00001-8-4 loss: 0.444717  [  160/  265]
train() client id: f_00001-8-5 loss: 0.482269  [  192/  265]
train() client id: f_00001-8-6 loss: 0.392907  [  224/  265]
train() client id: f_00001-8-7 loss: 0.395504  [  256/  265]
train() client id: f_00001-9-0 loss: 0.642218  [   32/  265]
train() client id: f_00001-9-1 loss: 0.518198  [   64/  265]
train() client id: f_00001-9-2 loss: 0.399867  [   96/  265]
train() client id: f_00001-9-3 loss: 0.551927  [  128/  265]
train() client id: f_00001-9-4 loss: 0.509412  [  160/  265]
train() client id: f_00001-9-5 loss: 0.470510  [  192/  265]
train() client id: f_00001-9-6 loss: 0.405333  [  224/  265]
train() client id: f_00001-9-7 loss: 0.470209  [  256/  265]
train() client id: f_00001-10-0 loss: 0.437700  [   32/  265]
train() client id: f_00001-10-1 loss: 0.568628  [   64/  265]
train() client id: f_00001-10-2 loss: 0.476321  [   96/  265]
train() client id: f_00001-10-3 loss: 0.453825  [  128/  265]
train() client id: f_00001-10-4 loss: 0.549735  [  160/  265]
train() client id: f_00001-10-5 loss: 0.466258  [  192/  265]
train() client id: f_00001-10-6 loss: 0.536418  [  224/  265]
train() client id: f_00001-10-7 loss: 0.394285  [  256/  265]
train() client id: f_00001-11-0 loss: 0.516031  [   32/  265]
train() client id: f_00001-11-1 loss: 0.476516  [   64/  265]
train() client id: f_00001-11-2 loss: 0.400150  [   96/  265]
train() client id: f_00001-11-3 loss: 0.434803  [  128/  265]
train() client id: f_00001-11-4 loss: 0.577708  [  160/  265]
train() client id: f_00001-11-5 loss: 0.477037  [  192/  265]
train() client id: f_00001-11-6 loss: 0.509710  [  224/  265]
train() client id: f_00001-11-7 loss: 0.458221  [  256/  265]
train() client id: f_00001-12-0 loss: 0.500552  [   32/  265]
train() client id: f_00001-12-1 loss: 0.482877  [   64/  265]
train() client id: f_00001-12-2 loss: 0.539285  [   96/  265]
train() client id: f_00001-12-3 loss: 0.495799  [  128/  265]
train() client id: f_00001-12-4 loss: 0.471119  [  160/  265]
train() client id: f_00001-12-5 loss: 0.575045  [  192/  265]
train() client id: f_00001-12-6 loss: 0.466206  [  224/  265]
train() client id: f_00001-12-7 loss: 0.438741  [  256/  265]
train() client id: f_00002-0-0 loss: 1.350659  [   32/  124]
train() client id: f_00002-0-1 loss: 1.430925  [   64/  124]
train() client id: f_00002-0-2 loss: 1.292860  [   96/  124]
train() client id: f_00002-1-0 loss: 1.261551  [   32/  124]
train() client id: f_00002-1-1 loss: 1.555197  [   64/  124]
train() client id: f_00002-1-2 loss: 1.256451  [   96/  124]
train() client id: f_00002-2-0 loss: 1.222476  [   32/  124]
train() client id: f_00002-2-1 loss: 1.389821  [   64/  124]
train() client id: f_00002-2-2 loss: 1.313988  [   96/  124]
train() client id: f_00002-3-0 loss: 1.202862  [   32/  124]
train() client id: f_00002-3-1 loss: 1.159294  [   64/  124]
train() client id: f_00002-3-2 loss: 1.241480  [   96/  124]
train() client id: f_00002-4-0 loss: 1.139504  [   32/  124]
train() client id: f_00002-4-1 loss: 1.220523  [   64/  124]
train() client id: f_00002-4-2 loss: 1.201112  [   96/  124]
train() client id: f_00002-5-0 loss: 1.150183  [   32/  124]
train() client id: f_00002-5-1 loss: 1.088427  [   64/  124]
train() client id: f_00002-5-2 loss: 1.183881  [   96/  124]
train() client id: f_00002-6-0 loss: 1.047153  [   32/  124]
train() client id: f_00002-6-1 loss: 1.204846  [   64/  124]
train() client id: f_00002-6-2 loss: 1.185487  [   96/  124]
train() client id: f_00002-7-0 loss: 1.178512  [   32/  124]
train() client id: f_00002-7-1 loss: 1.125006  [   64/  124]
train() client id: f_00002-7-2 loss: 0.984145  [   96/  124]
train() client id: f_00002-8-0 loss: 1.031659  [   32/  124]
train() client id: f_00002-8-1 loss: 1.039841  [   64/  124]
train() client id: f_00002-8-2 loss: 1.189408  [   96/  124]
train() client id: f_00002-9-0 loss: 1.009722  [   32/  124]
train() client id: f_00002-9-1 loss: 1.107109  [   64/  124]
train() client id: f_00002-9-2 loss: 1.255332  [   96/  124]
train() client id: f_00002-10-0 loss: 0.971737  [   32/  124]
train() client id: f_00002-10-1 loss: 1.267452  [   64/  124]
train() client id: f_00002-10-2 loss: 1.152300  [   96/  124]
train() client id: f_00002-11-0 loss: 1.012281  [   32/  124]
train() client id: f_00002-11-1 loss: 1.129813  [   64/  124]
train() client id: f_00002-11-2 loss: 1.170712  [   96/  124]
train() client id: f_00002-12-0 loss: 1.249298  [   32/  124]
train() client id: f_00002-12-1 loss: 1.018405  [   64/  124]
train() client id: f_00002-12-2 loss: 1.025863  [   96/  124]
train() client id: f_00003-0-0 loss: 0.528886  [   32/   43]
train() client id: f_00003-1-0 loss: 0.600359  [   32/   43]
train() client id: f_00003-2-0 loss: 0.498461  [   32/   43]
train() client id: f_00003-3-0 loss: 0.551519  [   32/   43]
train() client id: f_00003-4-0 loss: 0.666824  [   32/   43]
train() client id: f_00003-5-0 loss: 0.463875  [   32/   43]
train() client id: f_00003-6-0 loss: 0.664298  [   32/   43]
train() client id: f_00003-7-0 loss: 0.445665  [   32/   43]
train() client id: f_00003-8-0 loss: 0.466522  [   32/   43]
train() client id: f_00003-9-0 loss: 0.517057  [   32/   43]
train() client id: f_00003-10-0 loss: 0.635875  [   32/   43]
train() client id: f_00003-11-0 loss: 0.508012  [   32/   43]
train() client id: f_00003-12-0 loss: 0.596366  [   32/   43]
train() client id: f_00004-0-0 loss: 0.918195  [   32/  306]
train() client id: f_00004-0-1 loss: 0.626206  [   64/  306]
train() client id: f_00004-0-2 loss: 1.081289  [   96/  306]
train() client id: f_00004-0-3 loss: 0.737671  [  128/  306]
train() client id: f_00004-0-4 loss: 0.792184  [  160/  306]
train() client id: f_00004-0-5 loss: 0.824558  [  192/  306]
train() client id: f_00004-0-6 loss: 0.854981  [  224/  306]
train() client id: f_00004-0-7 loss: 0.799544  [  256/  306]
train() client id: f_00004-0-8 loss: 0.893111  [  288/  306]
train() client id: f_00004-1-0 loss: 0.665682  [   32/  306]
train() client id: f_00004-1-1 loss: 1.029069  [   64/  306]
train() client id: f_00004-1-2 loss: 0.735979  [   96/  306]
train() client id: f_00004-1-3 loss: 0.945023  [  128/  306]
train() client id: f_00004-1-4 loss: 0.879183  [  160/  306]
train() client id: f_00004-1-5 loss: 0.844939  [  192/  306]
train() client id: f_00004-1-6 loss: 0.754124  [  224/  306]
train() client id: f_00004-1-7 loss: 0.800756  [  256/  306]
train() client id: f_00004-1-8 loss: 0.745318  [  288/  306]
train() client id: f_00004-2-0 loss: 0.830299  [   32/  306]
train() client id: f_00004-2-1 loss: 0.913592  [   64/  306]
train() client id: f_00004-2-2 loss: 0.826579  [   96/  306]
train() client id: f_00004-2-3 loss: 0.771151  [  128/  306]
train() client id: f_00004-2-4 loss: 0.912350  [  160/  306]
train() client id: f_00004-2-5 loss: 0.692208  [  192/  306]
train() client id: f_00004-2-6 loss: 0.790333  [  224/  306]
train() client id: f_00004-2-7 loss: 0.858798  [  256/  306]
train() client id: f_00004-2-8 loss: 0.796199  [  288/  306]
train() client id: f_00004-3-0 loss: 0.792952  [   32/  306]
train() client id: f_00004-3-1 loss: 0.871204  [   64/  306]
train() client id: f_00004-3-2 loss: 0.801907  [   96/  306]
train() client id: f_00004-3-3 loss: 0.694960  [  128/  306]
train() client id: f_00004-3-4 loss: 0.839844  [  160/  306]
train() client id: f_00004-3-5 loss: 0.878321  [  192/  306]
train() client id: f_00004-3-6 loss: 0.788303  [  224/  306]
train() client id: f_00004-3-7 loss: 0.882382  [  256/  306]
train() client id: f_00004-3-8 loss: 0.818528  [  288/  306]
train() client id: f_00004-4-0 loss: 0.704130  [   32/  306]
train() client id: f_00004-4-1 loss: 0.873318  [   64/  306]
train() client id: f_00004-4-2 loss: 0.839738  [   96/  306]
train() client id: f_00004-4-3 loss: 0.821651  [  128/  306]
train() client id: f_00004-4-4 loss: 0.821142  [  160/  306]
train() client id: f_00004-4-5 loss: 0.866027  [  192/  306]
train() client id: f_00004-4-6 loss: 0.743936  [  224/  306]
train() client id: f_00004-4-7 loss: 0.818878  [  256/  306]
train() client id: f_00004-4-8 loss: 0.825025  [  288/  306]
train() client id: f_00004-5-0 loss: 0.915728  [   32/  306]
train() client id: f_00004-5-1 loss: 0.861470  [   64/  306]
train() client id: f_00004-5-2 loss: 0.673711  [   96/  306]
train() client id: f_00004-5-3 loss: 0.754311  [  128/  306]
train() client id: f_00004-5-4 loss: 0.832506  [  160/  306]
train() client id: f_00004-5-5 loss: 0.898387  [  192/  306]
train() client id: f_00004-5-6 loss: 0.848829  [  224/  306]
train() client id: f_00004-5-7 loss: 0.768581  [  256/  306]
train() client id: f_00004-5-8 loss: 0.762112  [  288/  306]
train() client id: f_00004-6-0 loss: 0.727557  [   32/  306]
train() client id: f_00004-6-1 loss: 0.744152  [   64/  306]
train() client id: f_00004-6-2 loss: 0.737013  [   96/  306]
train() client id: f_00004-6-3 loss: 0.901513  [  128/  306]
train() client id: f_00004-6-4 loss: 0.831281  [  160/  306]
train() client id: f_00004-6-5 loss: 0.898618  [  192/  306]
train() client id: f_00004-6-6 loss: 0.794382  [  224/  306]
train() client id: f_00004-6-7 loss: 0.820966  [  256/  306]
train() client id: f_00004-6-8 loss: 0.854772  [  288/  306]
train() client id: f_00004-7-0 loss: 0.796712  [   32/  306]
train() client id: f_00004-7-1 loss: 0.863079  [   64/  306]
train() client id: f_00004-7-2 loss: 0.822195  [   96/  306]
train() client id: f_00004-7-3 loss: 0.841208  [  128/  306]
train() client id: f_00004-7-4 loss: 0.863054  [  160/  306]
train() client id: f_00004-7-5 loss: 0.788270  [  192/  306]
train() client id: f_00004-7-6 loss: 0.740468  [  224/  306]
train() client id: f_00004-7-7 loss: 0.756571  [  256/  306]
train() client id: f_00004-7-8 loss: 0.792608  [  288/  306]
train() client id: f_00004-8-0 loss: 0.730475  [   32/  306]
train() client id: f_00004-8-1 loss: 0.901877  [   64/  306]
train() client id: f_00004-8-2 loss: 0.738551  [   96/  306]
train() client id: f_00004-8-3 loss: 1.006829  [  128/  306]
train() client id: f_00004-8-4 loss: 0.819130  [  160/  306]
train() client id: f_00004-8-5 loss: 0.817468  [  192/  306]
train() client id: f_00004-8-6 loss: 0.806472  [  224/  306]
train() client id: f_00004-8-7 loss: 0.742649  [  256/  306]
train() client id: f_00004-8-8 loss: 0.812467  [  288/  306]
train() client id: f_00004-9-0 loss: 0.887827  [   32/  306]
train() client id: f_00004-9-1 loss: 0.734923  [   64/  306]
train() client id: f_00004-9-2 loss: 0.718782  [   96/  306]
train() client id: f_00004-9-3 loss: 0.813322  [  128/  306]
train() client id: f_00004-9-4 loss: 0.866855  [  160/  306]
train() client id: f_00004-9-5 loss: 0.775514  [  192/  306]
train() client id: f_00004-9-6 loss: 0.851358  [  224/  306]
train() client id: f_00004-9-7 loss: 0.833487  [  256/  306]
train() client id: f_00004-9-8 loss: 0.802431  [  288/  306]
train() client id: f_00004-10-0 loss: 0.788575  [   32/  306]
train() client id: f_00004-10-1 loss: 0.800991  [   64/  306]
train() client id: f_00004-10-2 loss: 0.801835  [   96/  306]
train() client id: f_00004-10-3 loss: 0.823046  [  128/  306]
train() client id: f_00004-10-4 loss: 0.845947  [  160/  306]
train() client id: f_00004-10-5 loss: 0.915401  [  192/  306]
train() client id: f_00004-10-6 loss: 0.725100  [  224/  306]
train() client id: f_00004-10-7 loss: 0.895180  [  256/  306]
train() client id: f_00004-10-8 loss: 0.761778  [  288/  306]
train() client id: f_00004-11-0 loss: 0.744501  [   32/  306]
train() client id: f_00004-11-1 loss: 0.885107  [   64/  306]
train() client id: f_00004-11-2 loss: 0.708914  [   96/  306]
train() client id: f_00004-11-3 loss: 0.778105  [  128/  306]
train() client id: f_00004-11-4 loss: 0.874115  [  160/  306]
train() client id: f_00004-11-5 loss: 0.762450  [  192/  306]
train() client id: f_00004-11-6 loss: 0.818324  [  224/  306]
train() client id: f_00004-11-7 loss: 0.876522  [  256/  306]
train() client id: f_00004-11-8 loss: 0.794666  [  288/  306]
train() client id: f_00004-12-0 loss: 0.850027  [   32/  306]
train() client id: f_00004-12-1 loss: 0.744396  [   64/  306]
train() client id: f_00004-12-2 loss: 0.735517  [   96/  306]
train() client id: f_00004-12-3 loss: 0.812111  [  128/  306]
train() client id: f_00004-12-4 loss: 0.771390  [  160/  306]
train() client id: f_00004-12-5 loss: 0.825610  [  192/  306]
train() client id: f_00004-12-6 loss: 0.900519  [  224/  306]
train() client id: f_00004-12-7 loss: 0.745019  [  256/  306]
train() client id: f_00004-12-8 loss: 0.839513  [  288/  306]
train() client id: f_00005-0-0 loss: 0.542786  [   32/  146]
train() client id: f_00005-0-1 loss: 0.628290  [   64/  146]
train() client id: f_00005-0-2 loss: 0.707430  [   96/  146]
train() client id: f_00005-0-3 loss: 0.646392  [  128/  146]
train() client id: f_00005-1-0 loss: 0.797009  [   32/  146]
train() client id: f_00005-1-1 loss: 0.662881  [   64/  146]
train() client id: f_00005-1-2 loss: 0.399484  [   96/  146]
train() client id: f_00005-1-3 loss: 0.545515  [  128/  146]
train() client id: f_00005-2-0 loss: 0.474851  [   32/  146]
train() client id: f_00005-2-1 loss: 0.447360  [   64/  146]
train() client id: f_00005-2-2 loss: 0.938426  [   96/  146]
train() client id: f_00005-2-3 loss: 0.673629  [  128/  146]
train() client id: f_00005-3-0 loss: 0.558797  [   32/  146]
train() client id: f_00005-3-1 loss: 0.546921  [   64/  146]
train() client id: f_00005-3-2 loss: 0.864807  [   96/  146]
train() client id: f_00005-3-3 loss: 0.480341  [  128/  146]
train() client id: f_00005-4-0 loss: 0.617765  [   32/  146]
train() client id: f_00005-4-1 loss: 0.535800  [   64/  146]
train() client id: f_00005-4-2 loss: 0.787161  [   96/  146]
train() client id: f_00005-4-3 loss: 0.549485  [  128/  146]
train() client id: f_00005-5-0 loss: 0.715720  [   32/  146]
train() client id: f_00005-5-1 loss: 0.402613  [   64/  146]
train() client id: f_00005-5-2 loss: 0.444443  [   96/  146]
train() client id: f_00005-5-3 loss: 0.800578  [  128/  146]
train() client id: f_00005-6-0 loss: 0.751379  [   32/  146]
train() client id: f_00005-6-1 loss: 0.586437  [   64/  146]
train() client id: f_00005-6-2 loss: 0.652164  [   96/  146]
train() client id: f_00005-6-3 loss: 0.483986  [  128/  146]
train() client id: f_00005-7-0 loss: 0.514704  [   32/  146]
train() client id: f_00005-7-1 loss: 0.644584  [   64/  146]
train() client id: f_00005-7-2 loss: 0.578175  [   96/  146]
train() client id: f_00005-7-3 loss: 0.602163  [  128/  146]
train() client id: f_00005-8-0 loss: 0.667021  [   32/  146]
train() client id: f_00005-8-1 loss: 0.418273  [   64/  146]
train() client id: f_00005-8-2 loss: 0.735347  [   96/  146]
train() client id: f_00005-8-3 loss: 0.825827  [  128/  146]
train() client id: f_00005-9-0 loss: 0.688887  [   32/  146]
train() client id: f_00005-9-1 loss: 0.460192  [   64/  146]
train() client id: f_00005-9-2 loss: 0.868687  [   96/  146]
train() client id: f_00005-9-3 loss: 0.411736  [  128/  146]
train() client id: f_00005-10-0 loss: 0.608418  [   32/  146]
train() client id: f_00005-10-1 loss: 0.614114  [   64/  146]
train() client id: f_00005-10-2 loss: 0.433410  [   96/  146]
train() client id: f_00005-10-3 loss: 0.768951  [  128/  146]
train() client id: f_00005-11-0 loss: 0.774651  [   32/  146]
train() client id: f_00005-11-1 loss: 0.464004  [   64/  146]
train() client id: f_00005-11-2 loss: 0.536198  [   96/  146]
train() client id: f_00005-11-3 loss: 0.523751  [  128/  146]
train() client id: f_00005-12-0 loss: 0.386611  [   32/  146]
train() client id: f_00005-12-1 loss: 0.808671  [   64/  146]
train() client id: f_00005-12-2 loss: 0.472945  [   96/  146]
train() client id: f_00005-12-3 loss: 0.503294  [  128/  146]
train() client id: f_00006-0-0 loss: 0.440595  [   32/   54]
train() client id: f_00006-1-0 loss: 0.411096  [   32/   54]
train() client id: f_00006-2-0 loss: 0.494799  [   32/   54]
train() client id: f_00006-3-0 loss: 0.471102  [   32/   54]
train() client id: f_00006-4-0 loss: 0.472817  [   32/   54]
train() client id: f_00006-5-0 loss: 0.494534  [   32/   54]
train() client id: f_00006-6-0 loss: 0.506968  [   32/   54]
train() client id: f_00006-7-0 loss: 0.450962  [   32/   54]
train() client id: f_00006-8-0 loss: 0.437824  [   32/   54]
train() client id: f_00006-9-0 loss: 0.517293  [   32/   54]
train() client id: f_00006-10-0 loss: 0.497489  [   32/   54]
train() client id: f_00006-11-0 loss: 0.456222  [   32/   54]
train() client id: f_00006-12-0 loss: 0.456251  [   32/   54]
train() client id: f_00007-0-0 loss: 0.619519  [   32/  179]
train() client id: f_00007-0-1 loss: 0.632076  [   64/  179]
train() client id: f_00007-0-2 loss: 0.591014  [   96/  179]
train() client id: f_00007-0-3 loss: 0.486724  [  128/  179]
train() client id: f_00007-0-4 loss: 0.525990  [  160/  179]
train() client id: f_00007-1-0 loss: 0.508857  [   32/  179]
train() client id: f_00007-1-1 loss: 0.666287  [   64/  179]
train() client id: f_00007-1-2 loss: 0.511606  [   96/  179]
train() client id: f_00007-1-3 loss: 0.684613  [  128/  179]
train() client id: f_00007-1-4 loss: 0.402587  [  160/  179]
train() client id: f_00007-2-0 loss: 0.466237  [   32/  179]
train() client id: f_00007-2-1 loss: 0.641473  [   64/  179]
train() client id: f_00007-2-2 loss: 0.405336  [   96/  179]
train() client id: f_00007-2-3 loss: 0.537611  [  128/  179]
train() client id: f_00007-2-4 loss: 0.607323  [  160/  179]
train() client id: f_00007-3-0 loss: 0.714596  [   32/  179]
train() client id: f_00007-3-1 loss: 0.501515  [   64/  179]
train() client id: f_00007-3-2 loss: 0.439832  [   96/  179]
train() client id: f_00007-3-3 loss: 0.439772  [  128/  179]
train() client id: f_00007-3-4 loss: 0.471322  [  160/  179]
train() client id: f_00007-4-0 loss: 0.563895  [   32/  179]
train() client id: f_00007-4-1 loss: 0.389595  [   64/  179]
train() client id: f_00007-4-2 loss: 0.580558  [   96/  179]
train() client id: f_00007-4-3 loss: 0.666563  [  128/  179]
train() client id: f_00007-4-4 loss: 0.440844  [  160/  179]
train() client id: f_00007-5-0 loss: 0.820193  [   32/  179]
train() client id: f_00007-5-1 loss: 0.460156  [   64/  179]
train() client id: f_00007-5-2 loss: 0.518713  [   96/  179]
train() client id: f_00007-5-3 loss: 0.380696  [  128/  179]
train() client id: f_00007-5-4 loss: 0.422333  [  160/  179]
train() client id: f_00007-6-0 loss: 0.471957  [   32/  179]
train() client id: f_00007-6-1 loss: 0.485257  [   64/  179]
train() client id: f_00007-6-2 loss: 0.592740  [   96/  179]
train() client id: f_00007-6-3 loss: 0.406802  [  128/  179]
train() client id: f_00007-6-4 loss: 0.680776  [  160/  179]
train() client id: f_00007-7-0 loss: 0.666059  [   32/  179]
train() client id: f_00007-7-1 loss: 0.512898  [   64/  179]
train() client id: f_00007-7-2 loss: 0.416285  [   96/  179]
train() client id: f_00007-7-3 loss: 0.577098  [  128/  179]
train() client id: f_00007-7-4 loss: 0.404488  [  160/  179]
train() client id: f_00007-8-0 loss: 0.519815  [   32/  179]
train() client id: f_00007-8-1 loss: 0.360394  [   64/  179]
train() client id: f_00007-8-2 loss: 0.435643  [   96/  179]
train() client id: f_00007-8-3 loss: 0.620586  [  128/  179]
train() client id: f_00007-8-4 loss: 0.547821  [  160/  179]
train() client id: f_00007-9-0 loss: 0.326309  [   32/  179]
train() client id: f_00007-9-1 loss: 0.532247  [   64/  179]
train() client id: f_00007-9-2 loss: 0.488160  [   96/  179]
train() client id: f_00007-9-3 loss: 0.767372  [  128/  179]
train() client id: f_00007-9-4 loss: 0.456049  [  160/  179]
train() client id: f_00007-10-0 loss: 0.636059  [   32/  179]
train() client id: f_00007-10-1 loss: 0.434663  [   64/  179]
train() client id: f_00007-10-2 loss: 0.590495  [   96/  179]
train() client id: f_00007-10-3 loss: 0.446082  [  128/  179]
train() client id: f_00007-10-4 loss: 0.352748  [  160/  179]
train() client id: f_00007-11-0 loss: 0.479299  [   32/  179]
train() client id: f_00007-11-1 loss: 0.482538  [   64/  179]
train() client id: f_00007-11-2 loss: 0.418689  [   96/  179]
train() client id: f_00007-11-3 loss: 0.633629  [  128/  179]
train() client id: f_00007-11-4 loss: 0.571768  [  160/  179]
train() client id: f_00007-12-0 loss: 0.524231  [   32/  179]
train() client id: f_00007-12-1 loss: 0.369658  [   64/  179]
train() client id: f_00007-12-2 loss: 0.596536  [   96/  179]
train() client id: f_00007-12-3 loss: 0.684321  [  128/  179]
train() client id: f_00007-12-4 loss: 0.356050  [  160/  179]
train() client id: f_00008-0-0 loss: 0.736956  [   32/  130]
train() client id: f_00008-0-1 loss: 0.703489  [   64/  130]
train() client id: f_00008-0-2 loss: 0.752734  [   96/  130]
train() client id: f_00008-0-3 loss: 0.800253  [  128/  130]
train() client id: f_00008-1-0 loss: 0.872927  [   32/  130]
train() client id: f_00008-1-1 loss: 0.577039  [   64/  130]
train() client id: f_00008-1-2 loss: 0.757402  [   96/  130]
train() client id: f_00008-1-3 loss: 0.761522  [  128/  130]
train() client id: f_00008-2-0 loss: 0.696225  [   32/  130]
train() client id: f_00008-2-1 loss: 0.803905  [   64/  130]
train() client id: f_00008-2-2 loss: 0.787445  [   96/  130]
train() client id: f_00008-2-3 loss: 0.718497  [  128/  130]
train() client id: f_00008-3-0 loss: 0.689916  [   32/  130]
train() client id: f_00008-3-1 loss: 0.711065  [   64/  130]
train() client id: f_00008-3-2 loss: 0.849632  [   96/  130]
train() client id: f_00008-3-3 loss: 0.745999  [  128/  130]
train() client id: f_00008-4-0 loss: 0.708935  [   32/  130]
train() client id: f_00008-4-1 loss: 0.815894  [   64/  130]
train() client id: f_00008-4-2 loss: 0.727162  [   96/  130]
train() client id: f_00008-4-3 loss: 0.741170  [  128/  130]
train() client id: f_00008-5-0 loss: 0.861944  [   32/  130]
train() client id: f_00008-5-1 loss: 0.786539  [   64/  130]
train() client id: f_00008-5-2 loss: 0.608909  [   96/  130]
train() client id: f_00008-5-3 loss: 0.733433  [  128/  130]
train() client id: f_00008-6-0 loss: 0.635183  [   32/  130]
train() client id: f_00008-6-1 loss: 0.795046  [   64/  130]
train() client id: f_00008-6-2 loss: 0.817798  [   96/  130]
train() client id: f_00008-6-3 loss: 0.723849  [  128/  130]
train() client id: f_00008-7-0 loss: 0.725349  [   32/  130]
train() client id: f_00008-7-1 loss: 0.681488  [   64/  130]
train() client id: f_00008-7-2 loss: 0.833016  [   96/  130]
train() client id: f_00008-7-3 loss: 0.748743  [  128/  130]
train() client id: f_00008-8-0 loss: 0.732645  [   32/  130]
train() client id: f_00008-8-1 loss: 0.864686  [   64/  130]
train() client id: f_00008-8-2 loss: 0.668890  [   96/  130]
train() client id: f_00008-8-3 loss: 0.712537  [  128/  130]
train() client id: f_00008-9-0 loss: 0.697908  [   32/  130]
train() client id: f_00008-9-1 loss: 0.679418  [   64/  130]
train() client id: f_00008-9-2 loss: 0.754275  [   96/  130]
train() client id: f_00008-9-3 loss: 0.840080  [  128/  130]
train() client id: f_00008-10-0 loss: 0.690470  [   32/  130]
train() client id: f_00008-10-1 loss: 0.817784  [   64/  130]
train() client id: f_00008-10-2 loss: 0.722317  [   96/  130]
train() client id: f_00008-10-3 loss: 0.688560  [  128/  130]
train() client id: f_00008-11-0 loss: 0.785268  [   32/  130]
train() client id: f_00008-11-1 loss: 0.759147  [   64/  130]
train() client id: f_00008-11-2 loss: 0.719921  [   96/  130]
train() client id: f_00008-11-3 loss: 0.690094  [  128/  130]
train() client id: f_00008-12-0 loss: 0.736207  [   32/  130]
train() client id: f_00008-12-1 loss: 0.700221  [   64/  130]
train() client id: f_00008-12-2 loss: 0.757150  [   96/  130]
train() client id: f_00008-12-3 loss: 0.798045  [  128/  130]
train() client id: f_00009-0-0 loss: 0.931832  [   32/  118]
train() client id: f_00009-0-1 loss: 1.227895  [   64/  118]
train() client id: f_00009-0-2 loss: 0.981952  [   96/  118]
train() client id: f_00009-1-0 loss: 0.889475  [   32/  118]
train() client id: f_00009-1-1 loss: 1.147468  [   64/  118]
train() client id: f_00009-1-2 loss: 0.933570  [   96/  118]
train() client id: f_00009-2-0 loss: 0.872761  [   32/  118]
train() client id: f_00009-2-1 loss: 0.929235  [   64/  118]
train() client id: f_00009-2-2 loss: 1.121534  [   96/  118]
train() client id: f_00009-3-0 loss: 1.023620  [   32/  118]
train() client id: f_00009-3-1 loss: 0.978765  [   64/  118]
train() client id: f_00009-3-2 loss: 0.912805  [   96/  118]
train() client id: f_00009-4-0 loss: 0.808778  [   32/  118]
train() client id: f_00009-4-1 loss: 0.969153  [   64/  118]
train() client id: f_00009-4-2 loss: 0.921779  [   96/  118]
train() client id: f_00009-5-0 loss: 0.885181  [   32/  118]
train() client id: f_00009-5-1 loss: 0.847457  [   64/  118]
train() client id: f_00009-5-2 loss: 0.923801  [   96/  118]
train() client id: f_00009-6-0 loss: 0.875056  [   32/  118]
train() client id: f_00009-6-1 loss: 0.882628  [   64/  118]
train() client id: f_00009-6-2 loss: 0.863174  [   96/  118]
train() client id: f_00009-7-0 loss: 0.900654  [   32/  118]
train() client id: f_00009-7-1 loss: 0.793266  [   64/  118]
train() client id: f_00009-7-2 loss: 0.845995  [   96/  118]
train() client id: f_00009-8-0 loss: 0.803973  [   32/  118]
train() client id: f_00009-8-1 loss: 0.884411  [   64/  118]
train() client id: f_00009-8-2 loss: 0.792335  [   96/  118]
train() client id: f_00009-9-0 loss: 0.801018  [   32/  118]
train() client id: f_00009-9-1 loss: 0.711024  [   64/  118]
train() client id: f_00009-9-2 loss: 0.922379  [   96/  118]
train() client id: f_00009-10-0 loss: 0.879938  [   32/  118]
train() client id: f_00009-10-1 loss: 0.997629  [   64/  118]
train() client id: f_00009-10-2 loss: 0.620657  [   96/  118]
train() client id: f_00009-11-0 loss: 0.837344  [   32/  118]
train() client id: f_00009-11-1 loss: 0.716331  [   64/  118]
train() client id: f_00009-11-2 loss: 0.912030  [   96/  118]
train() client id: f_00009-12-0 loss: 0.853376  [   32/  118]
train() client id: f_00009-12-1 loss: 0.809106  [   64/  118]
train() client id: f_00009-12-2 loss: 0.901849  [   96/  118]
At round 40 accuracy: 0.649867374005305
At round 40 training accuracy: 0.590878604963112
At round 40 training loss: 0.8184263062299216
update_location
xs = [  -3.9056584     4.20031788  220.00902392   18.81129433    0.97929623
    3.95640986 -182.44319194 -161.32485185  204.66397685 -147.06087855]
ys = [ 212.5879595   195.55583871    1.32061395 -182.45517586  174.35018685
  157.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [234.96573089 219.68097033 241.67274283 208.9108805  200.99489216
 186.87150493 208.06827924 189.80617503 228.46446856 177.88455205]
dists_bs = [175.17456558 180.3754319  431.50510786 406.61655224 175.98049596
 179.77972646 177.78086556 174.71366329 411.01004024 173.26594513]
uav_gains = [9.82218151e-12 1.26815403e-11 8.69804006e-12 1.49604850e-11
 1.68083716e-11 2.06049621e-11 1.51494992e-11 1.97523587e-11
 1.09881270e-11 2.34789484e-11]
bs_gains = [5.77530797e-11 5.32105086e-11 4.62736173e-12 5.46482092e-12
 5.70155582e-11 5.37056630e-11 5.54135584e-11 5.81806876e-11
 5.30282480e-12 5.95521015e-11]
Round 41
-------------------------------
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.58035445 11.51525536  5.49993513  1.98836081 13.27975002  6.38965832
  2.46168242  7.8365251   5.78633913  5.1816046 ]
obj_prev = 65.51946532847218
eta_min = 2.9576516171485466e-17	eta_max = 0.9329613748543103
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 15.185923913957124	eta = 0.9090909090909091
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 28.643068262013102	eta = 0.4819799767971639
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 21.937413464592197	eta = 0.6293077986886223
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 20.723057688485657	eta = 0.6661847678972268
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 20.65725512436301	eta = 0.668306863289048
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 20.65704485277882	eta = 0.6683136660986402
eta = 0.6683136660986402
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [0.0332544  0.06993981 0.03272655 0.01134872 0.08076069 0.03853288
 0.01425189 0.04724236 0.0343101  0.03114301]
ene_total = [1.87549698 3.28425833 1.87007596 0.89063245 3.74131718 1.94496118
 1.01302102 2.39242243 2.02108855 1.62377077]
ti_comp = [0.54825773 0.58461281 0.54425104 0.56025271 0.5855944  0.58474602
 0.56057165 0.56686532 0.52465853 0.58619944]
ti_coms = [0.10745239 0.0710973  0.11145908 0.09545741 0.07011572 0.0709641
 0.09513846 0.0888448  0.13105159 0.06951068]
t_total = [27.94982796 27.94982796 27.94982796 27.94982796 27.94982796 27.94982796
 27.94982796 27.94982796 27.94982796 27.94982796]
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [7.64641487e-06 6.25628775e-05 7.39575090e-06 2.91040345e-07
 9.60033089e-05 1.04577867e-05 5.75751087e-07 2.05076110e-05
 9.17048565e-06 5.49377638e-06]
ene_total = [0.45834474 0.30572102 0.4754127  0.40690231 0.30296238 0.30293225
 0.40555492 0.37957759 0.55900197 0.29652539]
optimize_network iter = 0 obj = 3.892935261137077
eta = 0.6683136660986402
freqs = [30327342.94858874 59817206.41115162 30065677.93401769 10128218.84644976
 68956163.97393337 32948387.69791197 12711921.71227483 41669829.72547625
 32697548.88052001 26563493.52335148]
eta_min = 0.6683136660986426	eta_max = 0.6683136660986362
af = 0.007721858371378171	bf = 1.2881215340611454	zeta = 0.008494044208515988	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [1.80840054e-06 1.47963121e-05 1.74911774e-06 6.88319331e-08
 2.27050766e-05 2.47329858e-06 1.36166896e-07 4.85011281e-06
 2.16884794e-06 1.29929494e-06]
ene_total = [1.67138562 1.10800963 1.73368868 1.48456874 1.09397392 1.1040216
 1.47961889 1.38247263 2.03845756 1.08123529]
ti_comp = [0.54825773 0.58461281 0.54425104 0.56025271 0.5855944  0.58474602
 0.56057165 0.56686532 0.52465853 0.58619944]
ti_coms = [0.10745239 0.0710973  0.11145908 0.09545741 0.07011572 0.0709641
 0.09513846 0.0888448  0.13105159 0.06951068]
t_total = [27.94982796 27.94982796 27.94982796 27.94982796 27.94982796 27.94982796
 27.94982796 27.94982796 27.94982796 27.94982796]
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [7.64641487e-06 6.25628775e-05 7.39575090e-06 2.91040345e-07
 9.60033089e-05 1.04577867e-05 5.75751087e-07 2.05076110e-05
 9.17048565e-06 5.49377638e-06]
ene_total = [0.45834474 0.30572102 0.4754127  0.40690231 0.30296238 0.30293225
 0.40555492 0.37957759 0.55900197 0.29652539]
optimize_network iter = 1 obj = 3.8929352611370294
eta = 0.6683136660986362
freqs = [30327342.94858875 59817206.41115168 30065677.9340177  10128218.84644976
 68956163.97393346 32948387.697912   12711921.71227484 41669829.72547627
 32697548.88052    26563493.52335151]
Done!
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [7.53273391e-06 6.16327412e-05 7.28579662e-06 2.86713384e-07
 9.45760062e-05 1.03023084e-05 5.67191267e-07 2.02027197e-05
 9.03414599e-06 5.41209917e-06]
ene_total = [0.01075277 0.00717136 0.01115319 0.00954603 0.00710615 0.00710671
 0.00951441 0.00890468 0.01311419 0.00695648]
At round 41 energy consumption: 0.09132598543578574
At round 41 eta: 0.6683136660986362
At round 41 a_n: 13.79567728354305
At round 41 local rounds: 13.19619071883817
At round 41 global rounds: 42.625280831488105
gradient difference: 0.458886057138443
train() client id: f_00000-0-0 loss: 1.409319  [   32/  126]
train() client id: f_00000-0-1 loss: 1.223443  [   64/  126]
train() client id: f_00000-0-2 loss: 1.213935  [   96/  126]
train() client id: f_00000-1-0 loss: 1.120921  [   32/  126]
train() client id: f_00000-1-1 loss: 1.114922  [   64/  126]
train() client id: f_00000-1-2 loss: 1.348473  [   96/  126]
train() client id: f_00000-2-0 loss: 1.046170  [   32/  126]
train() client id: f_00000-2-1 loss: 1.268027  [   64/  126]
train() client id: f_00000-2-2 loss: 0.983029  [   96/  126]
train() client id: f_00000-3-0 loss: 1.172497  [   32/  126]
train() client id: f_00000-3-1 loss: 0.952849  [   64/  126]
train() client id: f_00000-3-2 loss: 1.027838  [   96/  126]
train() client id: f_00000-4-0 loss: 1.077048  [   32/  126]
train() client id: f_00000-4-1 loss: 0.936784  [   64/  126]
train() client id: f_00000-4-2 loss: 0.949402  [   96/  126]
train() client id: f_00000-5-0 loss: 1.066461  [   32/  126]
train() client id: f_00000-5-1 loss: 0.790500  [   64/  126]
train() client id: f_00000-5-2 loss: 0.919052  [   96/  126]
train() client id: f_00000-6-0 loss: 1.024759  [   32/  126]
train() client id: f_00000-6-1 loss: 0.922513  [   64/  126]
train() client id: f_00000-6-2 loss: 0.740026  [   96/  126]
train() client id: f_00000-7-0 loss: 0.917910  [   32/  126]
train() client id: f_00000-7-1 loss: 0.919829  [   64/  126]
train() client id: f_00000-7-2 loss: 0.721857  [   96/  126]
train() client id: f_00000-8-0 loss: 0.863652  [   32/  126]
train() client id: f_00000-8-1 loss: 0.871343  [   64/  126]
train() client id: f_00000-8-2 loss: 0.889956  [   96/  126]
train() client id: f_00000-9-0 loss: 0.801820  [   32/  126]
train() client id: f_00000-9-1 loss: 0.803170  [   64/  126]
train() client id: f_00000-9-2 loss: 0.852971  [   96/  126]
train() client id: f_00000-10-0 loss: 0.807638  [   32/  126]
train() client id: f_00000-10-1 loss: 0.766608  [   64/  126]
train() client id: f_00000-10-2 loss: 0.782847  [   96/  126]
train() client id: f_00000-11-0 loss: 0.712386  [   32/  126]
train() client id: f_00000-11-1 loss: 0.844100  [   64/  126]
train() client id: f_00000-11-2 loss: 0.756600  [   96/  126]
train() client id: f_00000-12-0 loss: 0.873671  [   32/  126]
train() client id: f_00000-12-1 loss: 0.792289  [   64/  126]
train() client id: f_00000-12-2 loss: 0.723288  [   96/  126]
train() client id: f_00001-0-0 loss: 0.585994  [   32/  265]
train() client id: f_00001-0-1 loss: 0.446975  [   64/  265]
train() client id: f_00001-0-2 loss: 0.565022  [   96/  265]
train() client id: f_00001-0-3 loss: 0.509441  [  128/  265]
train() client id: f_00001-0-4 loss: 0.467446  [  160/  265]
train() client id: f_00001-0-5 loss: 0.395169  [  192/  265]
train() client id: f_00001-0-6 loss: 0.499773  [  224/  265]
train() client id: f_00001-0-7 loss: 0.442484  [  256/  265]
train() client id: f_00001-1-0 loss: 0.472618  [   32/  265]
train() client id: f_00001-1-1 loss: 0.458706  [   64/  265]
train() client id: f_00001-1-2 loss: 0.454394  [   96/  265]
train() client id: f_00001-1-3 loss: 0.571867  [  128/  265]
train() client id: f_00001-1-4 loss: 0.446584  [  160/  265]
train() client id: f_00001-1-5 loss: 0.677789  [  192/  265]
train() client id: f_00001-1-6 loss: 0.388809  [  224/  265]
train() client id: f_00001-1-7 loss: 0.377596  [  256/  265]
train() client id: f_00001-2-0 loss: 0.494423  [   32/  265]
train() client id: f_00001-2-1 loss: 0.480692  [   64/  265]
train() client id: f_00001-2-2 loss: 0.467213  [   96/  265]
train() client id: f_00001-2-3 loss: 0.408625  [  128/  265]
train() client id: f_00001-2-4 loss: 0.451743  [  160/  265]
train() client id: f_00001-2-5 loss: 0.442068  [  192/  265]
train() client id: f_00001-2-6 loss: 0.471129  [  224/  265]
train() client id: f_00001-2-7 loss: 0.552107  [  256/  265]
train() client id: f_00001-3-0 loss: 0.620679  [   32/  265]
train() client id: f_00001-3-1 loss: 0.383919  [   64/  265]
train() client id: f_00001-3-2 loss: 0.421529  [   96/  265]
train() client id: f_00001-3-3 loss: 0.383564  [  128/  265]
train() client id: f_00001-3-4 loss: 0.384537  [  160/  265]
train() client id: f_00001-3-5 loss: 0.439500  [  192/  265]
train() client id: f_00001-3-6 loss: 0.578913  [  224/  265]
train() client id: f_00001-3-7 loss: 0.554516  [  256/  265]
train() client id: f_00001-4-0 loss: 0.579149  [   32/  265]
train() client id: f_00001-4-1 loss: 0.483464  [   64/  265]
train() client id: f_00001-4-2 loss: 0.463997  [   96/  265]
train() client id: f_00001-4-3 loss: 0.392757  [  128/  265]
train() client id: f_00001-4-4 loss: 0.477649  [  160/  265]
train() client id: f_00001-4-5 loss: 0.399505  [  192/  265]
train() client id: f_00001-4-6 loss: 0.565352  [  224/  265]
train() client id: f_00001-4-7 loss: 0.380044  [  256/  265]
train() client id: f_00001-5-0 loss: 0.549038  [   32/  265]
train() client id: f_00001-5-1 loss: 0.447654  [   64/  265]
train() client id: f_00001-5-2 loss: 0.477035  [   96/  265]
train() client id: f_00001-5-3 loss: 0.467181  [  128/  265]
train() client id: f_00001-5-4 loss: 0.444495  [  160/  265]
train() client id: f_00001-5-5 loss: 0.367225  [  192/  265]
train() client id: f_00001-5-6 loss: 0.394940  [  224/  265]
train() client id: f_00001-5-7 loss: 0.568203  [  256/  265]
train() client id: f_00001-6-0 loss: 0.422552  [   32/  265]
train() client id: f_00001-6-1 loss: 0.518445  [   64/  265]
train() client id: f_00001-6-2 loss: 0.362671  [   96/  265]
train() client id: f_00001-6-3 loss: 0.430947  [  128/  265]
train() client id: f_00001-6-4 loss: 0.420916  [  160/  265]
train() client id: f_00001-6-5 loss: 0.485531  [  192/  265]
train() client id: f_00001-6-6 loss: 0.407685  [  224/  265]
train() client id: f_00001-6-7 loss: 0.511596  [  256/  265]
train() client id: f_00001-7-0 loss: 0.402463  [   32/  265]
train() client id: f_00001-7-1 loss: 0.425418  [   64/  265]
train() client id: f_00001-7-2 loss: 0.476964  [   96/  265]
train() client id: f_00001-7-3 loss: 0.530989  [  128/  265]
train() client id: f_00001-7-4 loss: 0.519617  [  160/  265]
train() client id: f_00001-7-5 loss: 0.446889  [  192/  265]
train() client id: f_00001-7-6 loss: 0.457930  [  224/  265]
train() client id: f_00001-7-7 loss: 0.430925  [  256/  265]
train() client id: f_00001-8-0 loss: 0.326440  [   32/  265]
train() client id: f_00001-8-1 loss: 0.495698  [   64/  265]
train() client id: f_00001-8-2 loss: 0.368636  [   96/  265]
train() client id: f_00001-8-3 loss: 0.734865  [  128/  265]
train() client id: f_00001-8-4 loss: 0.513588  [  160/  265]
train() client id: f_00001-8-5 loss: 0.451902  [  192/  265]
train() client id: f_00001-8-6 loss: 0.365345  [  224/  265]
train() client id: f_00001-8-7 loss: 0.413803  [  256/  265]
train() client id: f_00001-9-0 loss: 0.560021  [   32/  265]
train() client id: f_00001-9-1 loss: 0.360576  [   64/  265]
train() client id: f_00001-9-2 loss: 0.540818  [   96/  265]
train() client id: f_00001-9-3 loss: 0.472521  [  128/  265]
train() client id: f_00001-9-4 loss: 0.407303  [  160/  265]
train() client id: f_00001-9-5 loss: 0.512884  [  192/  265]
train() client id: f_00001-9-6 loss: 0.405267  [  224/  265]
train() client id: f_00001-9-7 loss: 0.406069  [  256/  265]
train() client id: f_00001-10-0 loss: 0.461616  [   32/  265]
train() client id: f_00001-10-1 loss: 0.547614  [   64/  265]
train() client id: f_00001-10-2 loss: 0.407194  [   96/  265]
train() client id: f_00001-10-3 loss: 0.452942  [  128/  265]
train() client id: f_00001-10-4 loss: 0.360029  [  160/  265]
train() client id: f_00001-10-5 loss: 0.438363  [  192/  265]
train() client id: f_00001-10-6 loss: 0.429743  [  224/  265]
train() client id: f_00001-10-7 loss: 0.450545  [  256/  265]
train() client id: f_00001-11-0 loss: 0.569117  [   32/  265]
train() client id: f_00001-11-1 loss: 0.340155  [   64/  265]
train() client id: f_00001-11-2 loss: 0.415281  [   96/  265]
train() client id: f_00001-11-3 loss: 0.517984  [  128/  265]
train() client id: f_00001-11-4 loss: 0.415726  [  160/  265]
train() client id: f_00001-11-5 loss: 0.407049  [  192/  265]
train() client id: f_00001-11-6 loss: 0.428904  [  224/  265]
train() client id: f_00001-11-7 loss: 0.549567  [  256/  265]
train() client id: f_00001-12-0 loss: 0.451543  [   32/  265]
train() client id: f_00001-12-1 loss: 0.376596  [   64/  265]
train() client id: f_00001-12-2 loss: 0.511898  [   96/  265]
train() client id: f_00001-12-3 loss: 0.368149  [  128/  265]
train() client id: f_00001-12-4 loss: 0.489796  [  160/  265]
train() client id: f_00001-12-5 loss: 0.439001  [  192/  265]
train() client id: f_00001-12-6 loss: 0.546390  [  224/  265]
train() client id: f_00001-12-7 loss: 0.492364  [  256/  265]
train() client id: f_00002-0-0 loss: 1.146746  [   32/  124]
train() client id: f_00002-0-1 loss: 1.289433  [   64/  124]
train() client id: f_00002-0-2 loss: 1.280252  [   96/  124]
train() client id: f_00002-1-0 loss: 1.163566  [   32/  124]
train() client id: f_00002-1-1 loss: 1.118865  [   64/  124]
train() client id: f_00002-1-2 loss: 1.212068  [   96/  124]
train() client id: f_00002-2-0 loss: 1.129723  [   32/  124]
train() client id: f_00002-2-1 loss: 1.037458  [   64/  124]
train() client id: f_00002-2-2 loss: 1.212659  [   96/  124]
train() client id: f_00002-3-0 loss: 1.055989  [   32/  124]
train() client id: f_00002-3-1 loss: 1.132713  [   64/  124]
train() client id: f_00002-3-2 loss: 1.200734  [   96/  124]
train() client id: f_00002-4-0 loss: 1.022632  [   32/  124]
train() client id: f_00002-4-1 loss: 1.226384  [   64/  124]
train() client id: f_00002-4-2 loss: 1.085359  [   96/  124]
train() client id: f_00002-5-0 loss: 0.995845  [   32/  124]
train() client id: f_00002-5-1 loss: 1.077817  [   64/  124]
train() client id: f_00002-5-2 loss: 1.017387  [   96/  124]
train() client id: f_00002-6-0 loss: 1.196261  [   32/  124]
train() client id: f_00002-6-1 loss: 1.102192  [   64/  124]
train() client id: f_00002-6-2 loss: 1.058841  [   96/  124]
train() client id: f_00002-7-0 loss: 0.962260  [   32/  124]
train() client id: f_00002-7-1 loss: 0.954050  [   64/  124]
train() client id: f_00002-7-2 loss: 1.180736  [   96/  124]
train() client id: f_00002-8-0 loss: 0.989385  [   32/  124]
train() client id: f_00002-8-1 loss: 1.215577  [   64/  124]
train() client id: f_00002-8-2 loss: 0.979791  [   96/  124]
train() client id: f_00002-9-0 loss: 1.070980  [   32/  124]
train() client id: f_00002-9-1 loss: 0.893578  [   64/  124]
train() client id: f_00002-9-2 loss: 1.144551  [   96/  124]
train() client id: f_00002-10-0 loss: 1.247568  [   32/  124]
train() client id: f_00002-10-1 loss: 0.903254  [   64/  124]
train() client id: f_00002-10-2 loss: 0.823771  [   96/  124]
train() client id: f_00002-11-0 loss: 1.171213  [   32/  124]
train() client id: f_00002-11-1 loss: 1.076061  [   64/  124]
train() client id: f_00002-11-2 loss: 0.934361  [   96/  124]
train() client id: f_00002-12-0 loss: 0.981815  [   32/  124]
train() client id: f_00002-12-1 loss: 1.058776  [   64/  124]
train() client id: f_00002-12-2 loss: 1.048173  [   96/  124]
train() client id: f_00003-0-0 loss: 0.590219  [   32/   43]
train() client id: f_00003-1-0 loss: 0.499492  [   32/   43]
train() client id: f_00003-2-0 loss: 0.524841  [   32/   43]
train() client id: f_00003-3-0 loss: 0.694040  [   32/   43]
train() client id: f_00003-4-0 loss: 0.496372  [   32/   43]
train() client id: f_00003-5-0 loss: 0.615626  [   32/   43]
train() client id: f_00003-6-0 loss: 0.489681  [   32/   43]
train() client id: f_00003-7-0 loss: 0.783113  [   32/   43]
train() client id: f_00003-8-0 loss: 0.716198  [   32/   43]
train() client id: f_00003-9-0 loss: 0.504694  [   32/   43]
train() client id: f_00003-10-0 loss: 0.763882  [   32/   43]
train() client id: f_00003-11-0 loss: 0.491892  [   32/   43]
train() client id: f_00003-12-0 loss: 0.689820  [   32/   43]
train() client id: f_00004-0-0 loss: 0.780519  [   32/  306]
train() client id: f_00004-0-1 loss: 0.677776  [   64/  306]
train() client id: f_00004-0-2 loss: 0.810745  [   96/  306]
train() client id: f_00004-0-3 loss: 0.771742  [  128/  306]
train() client id: f_00004-0-4 loss: 0.830283  [  160/  306]
train() client id: f_00004-0-5 loss: 0.780195  [  192/  306]
train() client id: f_00004-0-6 loss: 0.749045  [  224/  306]
train() client id: f_00004-0-7 loss: 0.675439  [  256/  306]
train() client id: f_00004-0-8 loss: 0.653283  [  288/  306]
train() client id: f_00004-1-0 loss: 0.722278  [   32/  306]
train() client id: f_00004-1-1 loss: 0.789277  [   64/  306]
train() client id: f_00004-1-2 loss: 0.849594  [   96/  306]
train() client id: f_00004-1-3 loss: 0.777434  [  128/  306]
train() client id: f_00004-1-4 loss: 0.860883  [  160/  306]
train() client id: f_00004-1-5 loss: 0.695875  [  192/  306]
train() client id: f_00004-1-6 loss: 0.713812  [  224/  306]
train() client id: f_00004-1-7 loss: 0.633828  [  256/  306]
train() client id: f_00004-1-8 loss: 0.692199  [  288/  306]
train() client id: f_00004-2-0 loss: 0.681891  [   32/  306]
train() client id: f_00004-2-1 loss: 0.732185  [   64/  306]
train() client id: f_00004-2-2 loss: 0.750207  [   96/  306]
train() client id: f_00004-2-3 loss: 0.797319  [  128/  306]
train() client id: f_00004-2-4 loss: 0.822946  [  160/  306]
train() client id: f_00004-2-5 loss: 0.840606  [  192/  306]
train() client id: f_00004-2-6 loss: 0.727318  [  224/  306]
train() client id: f_00004-2-7 loss: 0.742993  [  256/  306]
train() client id: f_00004-2-8 loss: 0.746755  [  288/  306]
train() client id: f_00004-3-0 loss: 0.886167  [   32/  306]
train() client id: f_00004-3-1 loss: 0.781883  [   64/  306]
train() client id: f_00004-3-2 loss: 0.532624  [   96/  306]
train() client id: f_00004-3-3 loss: 0.828956  [  128/  306]
train() client id: f_00004-3-4 loss: 0.807049  [  160/  306]
train() client id: f_00004-3-5 loss: 0.707339  [  192/  306]
train() client id: f_00004-3-6 loss: 0.795700  [  224/  306]
train() client id: f_00004-3-7 loss: 0.795519  [  256/  306]
train() client id: f_00004-3-8 loss: 0.645888  [  288/  306]
train() client id: f_00004-4-0 loss: 0.776623  [   32/  306]
train() client id: f_00004-4-1 loss: 0.687367  [   64/  306]
train() client id: f_00004-4-2 loss: 0.929752  [   96/  306]
train() client id: f_00004-4-3 loss: 0.686604  [  128/  306]
train() client id: f_00004-4-4 loss: 0.836940  [  160/  306]
train() client id: f_00004-4-5 loss: 0.865032  [  192/  306]
train() client id: f_00004-4-6 loss: 0.630653  [  224/  306]
train() client id: f_00004-4-7 loss: 0.728952  [  256/  306]
train() client id: f_00004-4-8 loss: 0.788001  [  288/  306]
train() client id: f_00004-5-0 loss: 0.780315  [   32/  306]
train() client id: f_00004-5-1 loss: 0.794940  [   64/  306]
train() client id: f_00004-5-2 loss: 0.711466  [   96/  306]
train() client id: f_00004-5-3 loss: 0.754568  [  128/  306]
train() client id: f_00004-5-4 loss: 0.736881  [  160/  306]
train() client id: f_00004-5-5 loss: 0.799484  [  192/  306]
train() client id: f_00004-5-6 loss: 0.831924  [  224/  306]
train() client id: f_00004-5-7 loss: 0.823140  [  256/  306]
train() client id: f_00004-5-8 loss: 0.744768  [  288/  306]
train() client id: f_00004-6-0 loss: 0.813802  [   32/  306]
train() client id: f_00004-6-1 loss: 0.693995  [   64/  306]
train() client id: f_00004-6-2 loss: 0.729913  [   96/  306]
train() client id: f_00004-6-3 loss: 0.698098  [  128/  306]
train() client id: f_00004-6-4 loss: 0.722524  [  160/  306]
train() client id: f_00004-6-5 loss: 0.719076  [  192/  306]
train() client id: f_00004-6-6 loss: 0.725846  [  224/  306]
train() client id: f_00004-6-7 loss: 0.761884  [  256/  306]
train() client id: f_00004-6-8 loss: 0.871775  [  288/  306]
train() client id: f_00004-7-0 loss: 0.739058  [   32/  306]
train() client id: f_00004-7-1 loss: 0.712944  [   64/  306]
train() client id: f_00004-7-2 loss: 0.865290  [   96/  306]
train() client id: f_00004-7-3 loss: 0.734022  [  128/  306]
train() client id: f_00004-7-4 loss: 0.846624  [  160/  306]
train() client id: f_00004-7-5 loss: 0.711754  [  192/  306]
train() client id: f_00004-7-6 loss: 0.780236  [  224/  306]
train() client id: f_00004-7-7 loss: 0.716330  [  256/  306]
train() client id: f_00004-7-8 loss: 0.836666  [  288/  306]
train() client id: f_00004-8-0 loss: 0.733700  [   32/  306]
train() client id: f_00004-8-1 loss: 0.758328  [   64/  306]
train() client id: f_00004-8-2 loss: 0.696052  [   96/  306]
train() client id: f_00004-8-3 loss: 0.797185  [  128/  306]
train() client id: f_00004-8-4 loss: 0.876820  [  160/  306]
train() client id: f_00004-8-5 loss: 0.773494  [  192/  306]
train() client id: f_00004-8-6 loss: 0.733621  [  224/  306]
train() client id: f_00004-8-7 loss: 0.733772  [  256/  306]
train() client id: f_00004-8-8 loss: 0.838075  [  288/  306]
train() client id: f_00004-9-0 loss: 0.774349  [   32/  306]
train() client id: f_00004-9-1 loss: 0.670606  [   64/  306]
train() client id: f_00004-9-2 loss: 0.879152  [   96/  306]
train() client id: f_00004-9-3 loss: 0.781229  [  128/  306]
train() client id: f_00004-9-4 loss: 0.769031  [  160/  306]
train() client id: f_00004-9-5 loss: 0.815598  [  192/  306]
train() client id: f_00004-9-6 loss: 0.822550  [  224/  306]
train() client id: f_00004-9-7 loss: 0.886621  [  256/  306]
train() client id: f_00004-9-8 loss: 0.674155  [  288/  306]
train() client id: f_00004-10-0 loss: 0.809774  [   32/  306]
train() client id: f_00004-10-1 loss: 0.818481  [   64/  306]
train() client id: f_00004-10-2 loss: 0.817685  [   96/  306]
train() client id: f_00004-10-3 loss: 0.768101  [  128/  306]
train() client id: f_00004-10-4 loss: 0.701529  [  160/  306]
train() client id: f_00004-10-5 loss: 0.685747  [  192/  306]
train() client id: f_00004-10-6 loss: 0.751094  [  224/  306]
train() client id: f_00004-10-7 loss: 0.701860  [  256/  306]
train() client id: f_00004-10-8 loss: 0.872609  [  288/  306]
train() client id: f_00004-11-0 loss: 0.871278  [   32/  306]
train() client id: f_00004-11-1 loss: 0.731442  [   64/  306]
train() client id: f_00004-11-2 loss: 0.754742  [   96/  306]
train() client id: f_00004-11-3 loss: 0.891750  [  128/  306]
train() client id: f_00004-11-4 loss: 0.739414  [  160/  306]
train() client id: f_00004-11-5 loss: 0.695680  [  192/  306]
train() client id: f_00004-11-6 loss: 0.828734  [  224/  306]
train() client id: f_00004-11-7 loss: 0.760009  [  256/  306]
train() client id: f_00004-11-8 loss: 0.837767  [  288/  306]
train() client id: f_00004-12-0 loss: 0.778053  [   32/  306]
train() client id: f_00004-12-1 loss: 0.727125  [   64/  306]
train() client id: f_00004-12-2 loss: 0.863983  [   96/  306]
train() client id: f_00004-12-3 loss: 0.816531  [  128/  306]
train() client id: f_00004-12-4 loss: 0.914594  [  160/  306]
train() client id: f_00004-12-5 loss: 0.669155  [  192/  306]
train() client id: f_00004-12-6 loss: 0.719992  [  224/  306]
train() client id: f_00004-12-7 loss: 0.722395  [  256/  306]
train() client id: f_00004-12-8 loss: 0.812126  [  288/  306]
train() client id: f_00005-0-0 loss: 0.610805  [   32/  146]
train() client id: f_00005-0-1 loss: 0.742405  [   64/  146]
train() client id: f_00005-0-2 loss: 0.780008  [   96/  146]
train() client id: f_00005-0-3 loss: 0.827020  [  128/  146]
train() client id: f_00005-1-0 loss: 0.612067  [   32/  146]
train() client id: f_00005-1-1 loss: 0.564517  [   64/  146]
train() client id: f_00005-1-2 loss: 1.050302  [   96/  146]
train() client id: f_00005-1-3 loss: 0.564225  [  128/  146]
train() client id: f_00005-2-0 loss: 0.656584  [   32/  146]
train() client id: f_00005-2-1 loss: 0.469010  [   64/  146]
train() client id: f_00005-2-2 loss: 0.577811  [   96/  146]
train() client id: f_00005-2-3 loss: 1.057184  [  128/  146]
train() client id: f_00005-3-0 loss: 0.928779  [   32/  146]
train() client id: f_00005-3-1 loss: 0.960281  [   64/  146]
train() client id: f_00005-3-2 loss: 0.543117  [   96/  146]
train() client id: f_00005-3-3 loss: 0.553056  [  128/  146]
train() client id: f_00005-4-0 loss: 0.610559  [   32/  146]
train() client id: f_00005-4-1 loss: 0.584782  [   64/  146]
train() client id: f_00005-4-2 loss: 0.969483  [   96/  146]
train() client id: f_00005-4-3 loss: 0.699191  [  128/  146]
train() client id: f_00005-5-0 loss: 0.668550  [   32/  146]
train() client id: f_00005-5-1 loss: 0.879093  [   64/  146]
train() client id: f_00005-5-2 loss: 0.654462  [   96/  146]
train() client id: f_00005-5-3 loss: 0.577995  [  128/  146]
train() client id: f_00005-6-0 loss: 0.556176  [   32/  146]
train() client id: f_00005-6-1 loss: 0.662173  [   64/  146]
train() client id: f_00005-6-2 loss: 0.645352  [   96/  146]
train() client id: f_00005-6-3 loss: 0.859768  [  128/  146]
train() client id: f_00005-7-0 loss: 0.597270  [   32/  146]
train() client id: f_00005-7-1 loss: 0.592681  [   64/  146]
train() client id: f_00005-7-2 loss: 0.815850  [   96/  146]
train() client id: f_00005-7-3 loss: 0.558222  [  128/  146]
train() client id: f_00005-8-0 loss: 0.581155  [   32/  146]
train() client id: f_00005-8-1 loss: 0.637519  [   64/  146]
train() client id: f_00005-8-2 loss: 0.611959  [   96/  146]
train() client id: f_00005-8-3 loss: 0.917200  [  128/  146]
train() client id: f_00005-9-0 loss: 0.770687  [   32/  146]
train() client id: f_00005-9-1 loss: 0.595309  [   64/  146]
train() client id: f_00005-9-2 loss: 0.444268  [   96/  146]
train() client id: f_00005-9-3 loss: 0.860531  [  128/  146]
train() client id: f_00005-10-0 loss: 0.963497  [   32/  146]
train() client id: f_00005-10-1 loss: 0.813364  [   64/  146]
train() client id: f_00005-10-2 loss: 0.637115  [   96/  146]
train() client id: f_00005-10-3 loss: 0.461579  [  128/  146]
train() client id: f_00005-11-0 loss: 0.699862  [   32/  146]
train() client id: f_00005-11-1 loss: 0.673800  [   64/  146]
train() client id: f_00005-11-2 loss: 0.852308  [   96/  146]
train() client id: f_00005-11-3 loss: 0.614995  [  128/  146]
train() client id: f_00005-12-0 loss: 0.646638  [   32/  146]
train() client id: f_00005-12-1 loss: 0.990508  [   64/  146]
train() client id: f_00005-12-2 loss: 0.782559  [   96/  146]
train() client id: f_00005-12-3 loss: 0.404689  [  128/  146]
train() client id: f_00006-0-0 loss: 0.432441  [   32/   54]
train() client id: f_00006-1-0 loss: 0.506070  [   32/   54]
train() client id: f_00006-2-0 loss: 0.489014  [   32/   54]
train() client id: f_00006-3-0 loss: 0.505080  [   32/   54]
train() client id: f_00006-4-0 loss: 0.537949  [   32/   54]
train() client id: f_00006-5-0 loss: 0.549403  [   32/   54]
train() client id: f_00006-6-0 loss: 0.483198  [   32/   54]
train() client id: f_00006-7-0 loss: 0.539564  [   32/   54]
train() client id: f_00006-8-0 loss: 0.478063  [   32/   54]
train() client id: f_00006-9-0 loss: 0.424842  [   32/   54]
train() client id: f_00006-10-0 loss: 0.466240  [   32/   54]
train() client id: f_00006-11-0 loss: 0.558712  [   32/   54]
train() client id: f_00006-12-0 loss: 0.469199  [   32/   54]
train() client id: f_00007-0-0 loss: 0.688571  [   32/  179]
train() client id: f_00007-0-1 loss: 1.007266  [   64/  179]
train() client id: f_00007-0-2 loss: 0.662400  [   96/  179]
train() client id: f_00007-0-3 loss: 0.703560  [  128/  179]
train() client id: f_00007-0-4 loss: 0.549578  [  160/  179]
train() client id: f_00007-1-0 loss: 0.672486  [   32/  179]
train() client id: f_00007-1-1 loss: 0.638270  [   64/  179]
train() client id: f_00007-1-2 loss: 0.665521  [   96/  179]
train() client id: f_00007-1-3 loss: 0.681594  [  128/  179]
train() client id: f_00007-1-4 loss: 0.723895  [  160/  179]
train() client id: f_00007-2-0 loss: 0.665689  [   32/  179]
train() client id: f_00007-2-1 loss: 0.609390  [   64/  179]
train() client id: f_00007-2-2 loss: 0.660862  [   96/  179]
train() client id: f_00007-2-3 loss: 0.912634  [  128/  179]
train() client id: f_00007-2-4 loss: 0.630698  [  160/  179]
train() client id: f_00007-3-0 loss: 0.781626  [   32/  179]
train() client id: f_00007-3-1 loss: 0.571492  [   64/  179]
train() client id: f_00007-3-2 loss: 0.928100  [   96/  179]
train() client id: f_00007-3-3 loss: 0.639825  [  128/  179]
train() client id: f_00007-3-4 loss: 0.611633  [  160/  179]
train() client id: f_00007-4-0 loss: 0.678602  [   32/  179]
train() client id: f_00007-4-1 loss: 0.802455  [   64/  179]
train() client id: f_00007-4-2 loss: 0.550982  [   96/  179]
train() client id: f_00007-4-3 loss: 0.542245  [  128/  179]
train() client id: f_00007-4-4 loss: 0.786810  [  160/  179]
train() client id: f_00007-5-0 loss: 0.685874  [   32/  179]
train() client id: f_00007-5-1 loss: 0.715523  [   64/  179]
train() client id: f_00007-5-2 loss: 0.625959  [   96/  179]
train() client id: f_00007-5-3 loss: 0.682721  [  128/  179]
train() client id: f_00007-5-4 loss: 0.808085  [  160/  179]
train() client id: f_00007-6-0 loss: 0.819148  [   32/  179]
train() client id: f_00007-6-1 loss: 0.646712  [   64/  179]
train() client id: f_00007-6-2 loss: 0.647616  [   96/  179]
train() client id: f_00007-6-3 loss: 0.634369  [  128/  179]
train() client id: f_00007-6-4 loss: 0.689938  [  160/  179]
train() client id: f_00007-7-0 loss: 0.612735  [   32/  179]
train() client id: f_00007-7-1 loss: 0.779299  [   64/  179]
train() client id: f_00007-7-2 loss: 0.634305  [   96/  179]
train() client id: f_00007-7-3 loss: 0.636394  [  128/  179]
train() client id: f_00007-7-4 loss: 0.704654  [  160/  179]
train() client id: f_00007-8-0 loss: 0.723176  [   32/  179]
train() client id: f_00007-8-1 loss: 0.857954  [   64/  179]
train() client id: f_00007-8-2 loss: 0.714123  [   96/  179]
train() client id: f_00007-8-3 loss: 0.676044  [  128/  179]
train() client id: f_00007-8-4 loss: 0.531128  [  160/  179]
train() client id: f_00007-9-0 loss: 0.740795  [   32/  179]
train() client id: f_00007-9-1 loss: 0.738672  [   64/  179]
train() client id: f_00007-9-2 loss: 0.705025  [   96/  179]
train() client id: f_00007-9-3 loss: 0.558478  [  128/  179]
train() client id: f_00007-9-4 loss: 0.781192  [  160/  179]
train() client id: f_00007-10-0 loss: 0.696001  [   32/  179]
train() client id: f_00007-10-1 loss: 0.695549  [   64/  179]
train() client id: f_00007-10-2 loss: 0.637096  [   96/  179]
train() client id: f_00007-10-3 loss: 0.885729  [  128/  179]
train() client id: f_00007-10-4 loss: 0.564775  [  160/  179]
train() client id: f_00007-11-0 loss: 0.554992  [   32/  179]
train() client id: f_00007-11-1 loss: 0.608499  [   64/  179]
train() client id: f_00007-11-2 loss: 0.746772  [   96/  179]
train() client id: f_00007-11-3 loss: 0.748203  [  128/  179]
train() client id: f_00007-11-4 loss: 0.537360  [  160/  179]
train() client id: f_00007-12-0 loss: 0.571569  [   32/  179]
train() client id: f_00007-12-1 loss: 0.732908  [   64/  179]
train() client id: f_00007-12-2 loss: 0.604699  [   96/  179]
train() client id: f_00007-12-3 loss: 0.747498  [  128/  179]
train() client id: f_00007-12-4 loss: 0.624383  [  160/  179]
train() client id: f_00008-0-0 loss: 0.685373  [   32/  130]
train() client id: f_00008-0-1 loss: 0.740762  [   64/  130]
train() client id: f_00008-0-2 loss: 0.604451  [   96/  130]
train() client id: f_00008-0-3 loss: 0.578941  [  128/  130]
train() client id: f_00008-1-0 loss: 0.610074  [   32/  130]
train() client id: f_00008-1-1 loss: 0.599396  [   64/  130]
train() client id: f_00008-1-2 loss: 0.720657  [   96/  130]
train() client id: f_00008-1-3 loss: 0.714937  [  128/  130]
train() client id: f_00008-2-0 loss: 0.684112  [   32/  130]
train() client id: f_00008-2-1 loss: 0.606519  [   64/  130]
train() client id: f_00008-2-2 loss: 0.666212  [   96/  130]
train() client id: f_00008-2-3 loss: 0.692568  [  128/  130]
train() client id: f_00008-3-0 loss: 0.634120  [   32/  130]
train() client id: f_00008-3-1 loss: 0.789787  [   64/  130]
train() client id: f_00008-3-2 loss: 0.577395  [   96/  130]
train() client id: f_00008-3-3 loss: 0.644420  [  128/  130]
train() client id: f_00008-4-0 loss: 0.779266  [   32/  130]
train() client id: f_00008-4-1 loss: 0.612860  [   64/  130]
train() client id: f_00008-4-2 loss: 0.634033  [   96/  130]
train() client id: f_00008-4-3 loss: 0.619956  [  128/  130]
train() client id: f_00008-5-0 loss: 0.535439  [   32/  130]
train() client id: f_00008-5-1 loss: 0.638814  [   64/  130]
train() client id: f_00008-5-2 loss: 0.778078  [   96/  130]
train() client id: f_00008-5-3 loss: 0.688505  [  128/  130]
train() client id: f_00008-6-0 loss: 0.632931  [   32/  130]
train() client id: f_00008-6-1 loss: 0.606252  [   64/  130]
train() client id: f_00008-6-2 loss: 0.573463  [   96/  130]
train() client id: f_00008-6-3 loss: 0.835043  [  128/  130]
train() client id: f_00008-7-0 loss: 0.723389  [   32/  130]
train() client id: f_00008-7-1 loss: 0.621513  [   64/  130]
train() client id: f_00008-7-2 loss: 0.615543  [   96/  130]
train() client id: f_00008-7-3 loss: 0.686762  [  128/  130]
train() client id: f_00008-8-0 loss: 0.637589  [   32/  130]
train() client id: f_00008-8-1 loss: 0.589605  [   64/  130]
train() client id: f_00008-8-2 loss: 0.694750  [   96/  130]
train() client id: f_00008-8-3 loss: 0.729280  [  128/  130]
train() client id: f_00008-9-0 loss: 0.649211  [   32/  130]
train() client id: f_00008-9-1 loss: 0.750816  [   64/  130]
train() client id: f_00008-9-2 loss: 0.543457  [   96/  130]
train() client id: f_00008-9-3 loss: 0.705785  [  128/  130]
train() client id: f_00008-10-0 loss: 0.641447  [   32/  130]
train() client id: f_00008-10-1 loss: 0.698771  [   64/  130]
train() client id: f_00008-10-2 loss: 0.626163  [   96/  130]
train() client id: f_00008-10-3 loss: 0.682060  [  128/  130]
train() client id: f_00008-11-0 loss: 0.601284  [   32/  130]
train() client id: f_00008-11-1 loss: 0.733351  [   64/  130]
train() client id: f_00008-11-2 loss: 0.696883  [   96/  130]
train() client id: f_00008-11-3 loss: 0.619805  [  128/  130]
train() client id: f_00008-12-0 loss: 0.621108  [   32/  130]
train() client id: f_00008-12-1 loss: 0.667780  [   64/  130]
train() client id: f_00008-12-2 loss: 0.688138  [   96/  130]
train() client id: f_00008-12-3 loss: 0.669342  [  128/  130]
train() client id: f_00009-0-0 loss: 0.855769  [   32/  118]
train() client id: f_00009-0-1 loss: 0.939258  [   64/  118]
train() client id: f_00009-0-2 loss: 1.160042  [   96/  118]
train() client id: f_00009-1-0 loss: 0.902404  [   32/  118]
train() client id: f_00009-1-1 loss: 0.926118  [   64/  118]
train() client id: f_00009-1-2 loss: 0.882202  [   96/  118]
train() client id: f_00009-2-0 loss: 0.923199  [   32/  118]
train() client id: f_00009-2-1 loss: 0.827822  [   64/  118]
train() client id: f_00009-2-2 loss: 0.897822  [   96/  118]
train() client id: f_00009-3-0 loss: 0.886659  [   32/  118]
train() client id: f_00009-3-1 loss: 0.881105  [   64/  118]
train() client id: f_00009-3-2 loss: 0.901578  [   96/  118]
train() client id: f_00009-4-0 loss: 0.814657  [   32/  118]
train() client id: f_00009-4-1 loss: 0.785902  [   64/  118]
train() client id: f_00009-4-2 loss: 0.879632  [   96/  118]
train() client id: f_00009-5-0 loss: 0.765161  [   32/  118]
train() client id: f_00009-5-1 loss: 0.756614  [   64/  118]
train() client id: f_00009-5-2 loss: 0.891462  [   96/  118]
train() client id: f_00009-6-0 loss: 0.944869  [   32/  118]
train() client id: f_00009-6-1 loss: 0.820512  [   64/  118]
train() client id: f_00009-6-2 loss: 0.728034  [   96/  118]
train() client id: f_00009-7-0 loss: 0.875182  [   32/  118]
train() client id: f_00009-7-1 loss: 0.827821  [   64/  118]
train() client id: f_00009-7-2 loss: 0.761669  [   96/  118]
train() client id: f_00009-8-0 loss: 0.849236  [   32/  118]
train() client id: f_00009-8-1 loss: 0.834477  [   64/  118]
train() client id: f_00009-8-2 loss: 0.748483  [   96/  118]
train() client id: f_00009-9-0 loss: 0.847743  [   32/  118]
train() client id: f_00009-9-1 loss: 0.851709  [   64/  118]
train() client id: f_00009-9-2 loss: 0.808555  [   96/  118]
train() client id: f_00009-10-0 loss: 0.971439  [   32/  118]
train() client id: f_00009-10-1 loss: 0.722409  [   64/  118]
train() client id: f_00009-10-2 loss: 0.740692  [   96/  118]
train() client id: f_00009-11-0 loss: 0.807318  [   32/  118]
train() client id: f_00009-11-1 loss: 0.778889  [   64/  118]
train() client id: f_00009-11-2 loss: 0.686808  [   96/  118]
train() client id: f_00009-12-0 loss: 0.711284  [   32/  118]
train() client id: f_00009-12-1 loss: 0.884933  [   64/  118]
train() client id: f_00009-12-2 loss: 0.861272  [   96/  118]
At round 41 accuracy: 0.6472148541114059
At round 41 training accuracy: 0.5915492957746479
At round 41 training loss: 0.8313075417274199
update_location
xs = [  -3.9056584     4.20031788  225.00902392   18.81129433    0.97929623
    3.95640986 -187.44319194 -166.32485185  209.66397685 -152.06087855]
ys = [ 217.5879595   200.55583871    1.32061395 -187.45517586  179.35018685
  162.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [239.49900686 224.14345208 246.23323266 213.29160263 205.34714155
 191.11279617 212.46609317 194.07378133 232.95418683 182.039893  ]
dists_bs = [176.31508166 181.0134106  436.08685872 411.01894399 176.03305606
 179.37082135 178.06029338 174.39355681 415.63312302 172.53027713]
uav_gains = [9.05340846e-12 1.18039549e-11 7.98136893e-12 1.40041215e-11
 1.57718869e-11 1.93846067e-11 1.41810871e-11 1.85763382e-11
 1.01750306e-11 2.20968418e-11]
bs_gains = [5.67131265e-11 5.26870620e-11 4.49251656e-12 5.30250316e-12
 5.69679045e-11 5.40491731e-11 5.51704147e-11 5.84802023e-11
 5.13932005e-12 6.02658366e-11]
Round 42
-------------------------------
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.44885328 11.23645857  5.37086954  1.94257213 12.9580199   6.23472006
  2.4044106   7.64858783  5.64816701  5.05583458]
obj_prev = 63.948493486950056
eta_min = 1.1938832184881957e-17	eta_max = 0.9337601214025776
af = 13.47090363962996	bf = 1.273204732538192	zeta = 14.817994003592956	eta = 0.9090909090909091
af = 13.47090363962996	bf = 1.273204732538192	zeta = 28.128308786138636	eta = 0.47890912112953954
af = 13.47090363962996	bf = 1.273204732538192	zeta = 21.476258560043068	eta = 0.6272462962749386
af = 13.47090363962996	bf = 1.273204732538192	zeta = 20.271427562519737	eta = 0.6645266396796146
af = 13.47090363962996	bf = 1.273204732538192	zeta = 20.2057708129807	eta = 0.666685956418743
af = 13.47090363962996	bf = 1.273204732538192	zeta = 20.20555837007598	eta = 0.6666929660097932
eta = 0.6666929660097932
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [0.03345475 0.07036119 0.03292373 0.0114171  0.08124726 0.03876503
 0.01433775 0.04752699 0.03451681 0.03133064]
ene_total = [1.84044551 3.20713505 1.83649691 0.87474524 3.65310778 1.89784042
 0.99426624 2.34061661 1.97709129 1.58381332]
ti_comp = [0.56395803 0.60282675 0.55960175 0.5768971  0.60393932 0.60319406
 0.57722705 0.58383781 0.54156463 0.6047199 ]
ti_coms = [0.11010873 0.07124001 0.11446502 0.09716967 0.07012744 0.0708727
 0.09683971 0.09022895 0.13250214 0.06934686]
t_total = [27.89982376 27.89982376 27.89982376 27.89982376 27.89982376 27.89982376
 27.89982376 27.89982376 27.89982376 27.89982376]
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [7.35800290e-06 5.99093333e-05 7.12276845e-06 2.79479884e-07
 9.19008568e-05 1.00065955e-05 5.52878649e-07 1.96841446e-05
 8.76337451e-06 5.25629088e-06]
ene_total = [0.45604785 0.2973442  0.47406892 0.4021997  0.29406337 0.29375838
 0.40084532 0.37427503 0.548793   0.28724627]
optimize_network iter = 0 obj = 3.828642052039432
eta = 0.6666929660097932
freqs = [29660676.66717937 58359378.23819214 29417104.30254062  9895264.30741952
 67264422.3037599  32133134.64789411 12419507.6458075  40702220.97084436
 31867673.54934066 25905086.18443851]
eta_min = 0.6666929660097993	eta_max = 0.6666929660097998
af = 0.007174173529547853	bf = 1.273204732538192	zeta = 0.007891590882502639	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [1.72976861e-06 1.40838874e-05 1.67446812e-06 6.57020031e-08
 2.16046690e-05 2.35241749e-06 1.29974416e-07 4.62748057e-06
 2.06015278e-06 1.23568407e-06]
ene_total = [1.67118944 1.08322193 1.73728876 1.47458345 1.06747966 1.07586755
 1.46958603 1.36994863 2.01106501 1.0525431 ]
ti_comp = [0.56395803 0.60282675 0.55960175 0.5768971  0.60393932 0.60319406
 0.57722705 0.58383781 0.54156463 0.6047199 ]
ti_coms = [0.11010873 0.07124001 0.11446502 0.09716967 0.07012744 0.0708727
 0.09683971 0.09022895 0.13250214 0.06934686]
t_total = [27.89982376 27.89982376 27.89982376 27.89982376 27.89982376 27.89982376
 27.89982376 27.89982376 27.89982376 27.89982376]
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [7.35800290e-06 5.99093333e-05 7.12276845e-06 2.79479884e-07
 9.19008568e-05 1.00065955e-05 5.52878649e-07 1.96841446e-05
 8.76337451e-06 5.25629088e-06]
ene_total = [0.45604785 0.2973442  0.47406892 0.4021997  0.29406337 0.29375838
 0.40084532 0.37427503 0.548793   0.28724627]
optimize_network iter = 1 obj = 3.8286420520395077
eta = 0.6666929660097998
freqs = [29660676.66717935 58359378.23819202 29417104.30254061  9895264.30741951
 67264422.30375975 32133134.64789404 12419507.64580748 40702220.97084429
 31867673.54934067 25905086.18443845]
Done!
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [7.20519951e-06 5.86651983e-05 6.97485017e-06 2.73675935e-07
 8.99923549e-05 9.79878887e-06 5.41397037e-07 1.92753647e-05
 8.58138582e-06 5.14713367e-06]
ene_total = [0.01101808 0.00718267 0.01145348 0.00971724 0.00710274 0.00709707
 0.00968451 0.00904217 0.01325879 0.00693983]
At round 42 energy consumption: 0.09249657683909618
At round 42 eta: 0.6666929660097998
At round 42 a_n: 13.453131436573733
At round 42 local rounds: 13.275695913565364
At round 42 global rounds: 41.390297463535276
gradient difference: 0.4090867042541504
train() client id: f_00000-0-0 loss: 1.287523  [   32/  126]
train() client id: f_00000-0-1 loss: 1.279330  [   64/  126]
train() client id: f_00000-0-2 loss: 1.148481  [   96/  126]
train() client id: f_00000-1-0 loss: 1.193007  [   32/  126]
train() client id: f_00000-1-1 loss: 1.325343  [   64/  126]
train() client id: f_00000-1-2 loss: 1.133926  [   96/  126]
train() client id: f_00000-2-0 loss: 1.013285  [   32/  126]
train() client id: f_00000-2-1 loss: 0.901067  [   64/  126]
train() client id: f_00000-2-2 loss: 1.162787  [   96/  126]
train() client id: f_00000-3-0 loss: 1.116560  [   32/  126]
train() client id: f_00000-3-1 loss: 1.045185  [   64/  126]
train() client id: f_00000-3-2 loss: 0.972643  [   96/  126]
train() client id: f_00000-4-0 loss: 0.841263  [   32/  126]
train() client id: f_00000-4-1 loss: 1.078531  [   64/  126]
train() client id: f_00000-4-2 loss: 0.976795  [   96/  126]
train() client id: f_00000-5-0 loss: 0.896180  [   32/  126]
train() client id: f_00000-5-1 loss: 1.022535  [   64/  126]
train() client id: f_00000-5-2 loss: 0.953944  [   96/  126]
train() client id: f_00000-6-0 loss: 0.874158  [   32/  126]
train() client id: f_00000-6-1 loss: 0.904241  [   64/  126]
train() client id: f_00000-6-2 loss: 0.948065  [   96/  126]
train() client id: f_00000-7-0 loss: 0.922334  [   32/  126]
train() client id: f_00000-7-1 loss: 0.794412  [   64/  126]
train() client id: f_00000-7-2 loss: 0.895431  [   96/  126]
train() client id: f_00000-8-0 loss: 0.849381  [   32/  126]
train() client id: f_00000-8-1 loss: 0.875548  [   64/  126]
train() client id: f_00000-8-2 loss: 0.934632  [   96/  126]
train() client id: f_00000-9-0 loss: 0.922500  [   32/  126]
train() client id: f_00000-9-1 loss: 0.810875  [   64/  126]
train() client id: f_00000-9-2 loss: 0.869555  [   96/  126]
train() client id: f_00000-10-0 loss: 0.919060  [   32/  126]
train() client id: f_00000-10-1 loss: 0.897948  [   64/  126]
train() client id: f_00000-10-2 loss: 0.769988  [   96/  126]
train() client id: f_00000-11-0 loss: 0.903378  [   32/  126]
train() client id: f_00000-11-1 loss: 0.897932  [   64/  126]
train() client id: f_00000-11-2 loss: 0.888051  [   96/  126]
train() client id: f_00000-12-0 loss: 0.836388  [   32/  126]
train() client id: f_00000-12-1 loss: 0.840471  [   64/  126]
train() client id: f_00000-12-2 loss: 0.882398  [   96/  126]
train() client id: f_00001-0-0 loss: 0.447196  [   32/  265]
train() client id: f_00001-0-1 loss: 0.412774  [   64/  265]
train() client id: f_00001-0-2 loss: 0.380724  [   96/  265]
train() client id: f_00001-0-3 loss: 0.419138  [  128/  265]
train() client id: f_00001-0-4 loss: 0.384854  [  160/  265]
train() client id: f_00001-0-5 loss: 0.387159  [  192/  265]
train() client id: f_00001-0-6 loss: 0.348401  [  224/  265]
train() client id: f_00001-0-7 loss: 0.379010  [  256/  265]
train() client id: f_00001-1-0 loss: 0.324885  [   32/  265]
train() client id: f_00001-1-1 loss: 0.420983  [   64/  265]
train() client id: f_00001-1-2 loss: 0.426806  [   96/  265]
train() client id: f_00001-1-3 loss: 0.323158  [  128/  265]
train() client id: f_00001-1-4 loss: 0.499822  [  160/  265]
train() client id: f_00001-1-5 loss: 0.370626  [  192/  265]
train() client id: f_00001-1-6 loss: 0.374799  [  224/  265]
train() client id: f_00001-1-7 loss: 0.405621  [  256/  265]
train() client id: f_00001-2-0 loss: 0.440257  [   32/  265]
train() client id: f_00001-2-1 loss: 0.359654  [   64/  265]
train() client id: f_00001-2-2 loss: 0.427237  [   96/  265]
train() client id: f_00001-2-3 loss: 0.443209  [  128/  265]
train() client id: f_00001-2-4 loss: 0.337445  [  160/  265]
train() client id: f_00001-2-5 loss: 0.324468  [  192/  265]
train() client id: f_00001-2-6 loss: 0.385049  [  224/  265]
train() client id: f_00001-2-7 loss: 0.366922  [  256/  265]
train() client id: f_00001-3-0 loss: 0.333754  [   32/  265]
train() client id: f_00001-3-1 loss: 0.595338  [   64/  265]
train() client id: f_00001-3-2 loss: 0.376678  [   96/  265]
train() client id: f_00001-3-3 loss: 0.390380  [  128/  265]
train() client id: f_00001-3-4 loss: 0.313735  [  160/  265]
train() client id: f_00001-3-5 loss: 0.373763  [  192/  265]
train() client id: f_00001-3-6 loss: 0.366520  [  224/  265]
train() client id: f_00001-3-7 loss: 0.304754  [  256/  265]
train() client id: f_00001-4-0 loss: 0.372808  [   32/  265]
train() client id: f_00001-4-1 loss: 0.378463  [   64/  265]
train() client id: f_00001-4-2 loss: 0.260678  [   96/  265]
train() client id: f_00001-4-3 loss: 0.384484  [  128/  265]
train() client id: f_00001-4-4 loss: 0.391422  [  160/  265]
train() client id: f_00001-4-5 loss: 0.394846  [  192/  265]
train() client id: f_00001-4-6 loss: 0.344038  [  224/  265]
train() client id: f_00001-4-7 loss: 0.463752  [  256/  265]
train() client id: f_00001-5-0 loss: 0.260711  [   32/  265]
train() client id: f_00001-5-1 loss: 0.367737  [   64/  265]
train() client id: f_00001-5-2 loss: 0.311206  [   96/  265]
train() client id: f_00001-5-3 loss: 0.516452  [  128/  265]
train() client id: f_00001-5-4 loss: 0.358693  [  160/  265]
train() client id: f_00001-5-5 loss: 0.306655  [  192/  265]
train() client id: f_00001-5-6 loss: 0.423381  [  224/  265]
train() client id: f_00001-5-7 loss: 0.451587  [  256/  265]
train() client id: f_00001-6-0 loss: 0.433357  [   32/  265]
train() client id: f_00001-6-1 loss: 0.378853  [   64/  265]
train() client id: f_00001-6-2 loss: 0.355819  [   96/  265]
train() client id: f_00001-6-3 loss: 0.295726  [  128/  265]
train() client id: f_00001-6-4 loss: 0.419071  [  160/  265]
train() client id: f_00001-6-5 loss: 0.409583  [  192/  265]
train() client id: f_00001-6-6 loss: 0.271006  [  224/  265]
train() client id: f_00001-6-7 loss: 0.430184  [  256/  265]
train() client id: f_00001-7-0 loss: 0.357861  [   32/  265]
train() client id: f_00001-7-1 loss: 0.410546  [   64/  265]
train() client id: f_00001-7-2 loss: 0.360803  [   96/  265]
train() client id: f_00001-7-3 loss: 0.374394  [  128/  265]
train() client id: f_00001-7-4 loss: 0.424600  [  160/  265]
train() client id: f_00001-7-5 loss: 0.313057  [  192/  265]
train() client id: f_00001-7-6 loss: 0.345233  [  224/  265]
train() client id: f_00001-7-7 loss: 0.392347  [  256/  265]
train() client id: f_00001-8-0 loss: 0.374546  [   32/  265]
train() client id: f_00001-8-1 loss: 0.357397  [   64/  265]
train() client id: f_00001-8-2 loss: 0.357381  [   96/  265]
train() client id: f_00001-8-3 loss: 0.340262  [  128/  265]
train() client id: f_00001-8-4 loss: 0.495351  [  160/  265]
train() client id: f_00001-8-5 loss: 0.359484  [  192/  265]
train() client id: f_00001-8-6 loss: 0.366936  [  224/  265]
train() client id: f_00001-8-7 loss: 0.292961  [  256/  265]
train() client id: f_00001-9-0 loss: 0.391246  [   32/  265]
train() client id: f_00001-9-1 loss: 0.383355  [   64/  265]
train() client id: f_00001-9-2 loss: 0.390386  [   96/  265]
train() client id: f_00001-9-3 loss: 0.433883  [  128/  265]
train() client id: f_00001-9-4 loss: 0.275926  [  160/  265]
train() client id: f_00001-9-5 loss: 0.360251  [  192/  265]
train() client id: f_00001-9-6 loss: 0.322180  [  224/  265]
train() client id: f_00001-9-7 loss: 0.319461  [  256/  265]
train() client id: f_00001-10-0 loss: 0.333123  [   32/  265]
train() client id: f_00001-10-1 loss: 0.363471  [   64/  265]
train() client id: f_00001-10-2 loss: 0.280592  [   96/  265]
train() client id: f_00001-10-3 loss: 0.383482  [  128/  265]
train() client id: f_00001-10-4 loss: 0.359740  [  160/  265]
train() client id: f_00001-10-5 loss: 0.452768  [  192/  265]
train() client id: f_00001-10-6 loss: 0.330230  [  224/  265]
train() client id: f_00001-10-7 loss: 0.454150  [  256/  265]
train() client id: f_00001-11-0 loss: 0.479311  [   32/  265]
train() client id: f_00001-11-1 loss: 0.410389  [   64/  265]
train() client id: f_00001-11-2 loss: 0.293244  [   96/  265]
train() client id: f_00001-11-3 loss: 0.344344  [  128/  265]
train() client id: f_00001-11-4 loss: 0.275193  [  160/  265]
train() client id: f_00001-11-5 loss: 0.387747  [  192/  265]
train() client id: f_00001-11-6 loss: 0.428598  [  224/  265]
train() client id: f_00001-11-7 loss: 0.330541  [  256/  265]
train() client id: f_00001-12-0 loss: 0.277228  [   32/  265]
train() client id: f_00001-12-1 loss: 0.345393  [   64/  265]
train() client id: f_00001-12-2 loss: 0.344471  [   96/  265]
train() client id: f_00001-12-3 loss: 0.367618  [  128/  265]
train() client id: f_00001-12-4 loss: 0.279752  [  160/  265]
train() client id: f_00001-12-5 loss: 0.345235  [  192/  265]
train() client id: f_00001-12-6 loss: 0.417521  [  224/  265]
train() client id: f_00001-12-7 loss: 0.567378  [  256/  265]
train() client id: f_00002-0-0 loss: 0.982269  [   32/  124]
train() client id: f_00002-0-1 loss: 1.277951  [   64/  124]
train() client id: f_00002-0-2 loss: 1.356587  [   96/  124]
train() client id: f_00002-1-0 loss: 1.226964  [   32/  124]
train() client id: f_00002-1-1 loss: 1.093613  [   64/  124]
train() client id: f_00002-1-2 loss: 1.022620  [   96/  124]
train() client id: f_00002-2-0 loss: 1.033457  [   32/  124]
train() client id: f_00002-2-1 loss: 1.135559  [   64/  124]
train() client id: f_00002-2-2 loss: 1.031023  [   96/  124]
train() client id: f_00002-3-0 loss: 1.173424  [   32/  124]
train() client id: f_00002-3-1 loss: 1.058596  [   64/  124]
train() client id: f_00002-3-2 loss: 1.050834  [   96/  124]
train() client id: f_00002-4-0 loss: 1.255859  [   32/  124]
train() client id: f_00002-4-1 loss: 0.976187  [   64/  124]
train() client id: f_00002-4-2 loss: 0.885558  [   96/  124]
train() client id: f_00002-5-0 loss: 1.180147  [   32/  124]
train() client id: f_00002-5-1 loss: 0.928971  [   64/  124]
train() client id: f_00002-5-2 loss: 0.831364  [   96/  124]
train() client id: f_00002-6-0 loss: 0.912637  [   32/  124]
train() client id: f_00002-6-1 loss: 1.019930  [   64/  124]
train() client id: f_00002-6-2 loss: 0.906198  [   96/  124]
train() client id: f_00002-7-0 loss: 0.950524  [   32/  124]
train() client id: f_00002-7-1 loss: 0.785504  [   64/  124]
train() client id: f_00002-7-2 loss: 0.944544  [   96/  124]
train() client id: f_00002-8-0 loss: 0.933252  [   32/  124]
train() client id: f_00002-8-1 loss: 1.077898  [   64/  124]
train() client id: f_00002-8-2 loss: 0.851921  [   96/  124]
train() client id: f_00002-9-0 loss: 0.941346  [   32/  124]
train() client id: f_00002-9-1 loss: 0.781143  [   64/  124]
train() client id: f_00002-9-2 loss: 0.845210  [   96/  124]
train() client id: f_00002-10-0 loss: 0.792140  [   32/  124]
train() client id: f_00002-10-1 loss: 0.983590  [   64/  124]
train() client id: f_00002-10-2 loss: 0.921239  [   96/  124]
train() client id: f_00002-11-0 loss: 0.883235  [   32/  124]
train() client id: f_00002-11-1 loss: 1.006065  [   64/  124]
train() client id: f_00002-11-2 loss: 0.973230  [   96/  124]
train() client id: f_00002-12-0 loss: 0.738008  [   32/  124]
train() client id: f_00002-12-1 loss: 1.090945  [   64/  124]
train() client id: f_00002-12-2 loss: 1.006879  [   96/  124]
train() client id: f_00003-0-0 loss: 0.375557  [   32/   43]
train() client id: f_00003-1-0 loss: 0.355062  [   32/   43]
train() client id: f_00003-2-0 loss: 0.714444  [   32/   43]
train() client id: f_00003-3-0 loss: 0.510789  [   32/   43]
train() client id: f_00003-4-0 loss: 0.726175  [   32/   43]
train() client id: f_00003-5-0 loss: 0.619098  [   32/   43]
train() client id: f_00003-6-0 loss: 0.594815  [   32/   43]
train() client id: f_00003-7-0 loss: 0.417805  [   32/   43]
train() client id: f_00003-8-0 loss: 0.453709  [   32/   43]
train() client id: f_00003-9-0 loss: 0.593694  [   32/   43]
train() client id: f_00003-10-0 loss: 0.533702  [   32/   43]
train() client id: f_00003-11-0 loss: 0.668286  [   32/   43]
train() client id: f_00003-12-0 loss: 0.785126  [   32/   43]
train() client id: f_00004-0-0 loss: 0.897424  [   32/  306]
train() client id: f_00004-0-1 loss: 0.857890  [   64/  306]
train() client id: f_00004-0-2 loss: 0.834167  [   96/  306]
train() client id: f_00004-0-3 loss: 0.831489  [  128/  306]
train() client id: f_00004-0-4 loss: 0.841010  [  160/  306]
train() client id: f_00004-0-5 loss: 0.898523  [  192/  306]
train() client id: f_00004-0-6 loss: 0.805898  [  224/  306]
train() client id: f_00004-0-7 loss: 0.864478  [  256/  306]
train() client id: f_00004-0-8 loss: 0.915706  [  288/  306]
train() client id: f_00004-1-0 loss: 0.861218  [   32/  306]
train() client id: f_00004-1-1 loss: 0.741892  [   64/  306]
train() client id: f_00004-1-2 loss: 0.721576  [   96/  306]
train() client id: f_00004-1-3 loss: 0.842444  [  128/  306]
train() client id: f_00004-1-4 loss: 0.868545  [  160/  306]
train() client id: f_00004-1-5 loss: 1.006304  [  192/  306]
train() client id: f_00004-1-6 loss: 0.866952  [  224/  306]
train() client id: f_00004-1-7 loss: 0.884823  [  256/  306]
train() client id: f_00004-1-8 loss: 0.904265  [  288/  306]
train() client id: f_00004-2-0 loss: 0.800260  [   32/  306]
train() client id: f_00004-2-1 loss: 0.871139  [   64/  306]
train() client id: f_00004-2-2 loss: 0.861183  [   96/  306]
train() client id: f_00004-2-3 loss: 0.888548  [  128/  306]
train() client id: f_00004-2-4 loss: 0.833389  [  160/  306]
train() client id: f_00004-2-5 loss: 0.848809  [  192/  306]
train() client id: f_00004-2-6 loss: 0.921487  [  224/  306]
train() client id: f_00004-2-7 loss: 0.875761  [  256/  306]
train() client id: f_00004-2-8 loss: 0.812678  [  288/  306]
train() client id: f_00004-3-0 loss: 0.943120  [   32/  306]
train() client id: f_00004-3-1 loss: 0.843180  [   64/  306]
train() client id: f_00004-3-2 loss: 0.692209  [   96/  306]
train() client id: f_00004-3-3 loss: 0.823309  [  128/  306]
train() client id: f_00004-3-4 loss: 0.964850  [  160/  306]
train() client id: f_00004-3-5 loss: 0.786452  [  192/  306]
train() client id: f_00004-3-6 loss: 1.046917  [  224/  306]
train() client id: f_00004-3-7 loss: 0.895304  [  256/  306]
train() client id: f_00004-3-8 loss: 0.704632  [  288/  306]
train() client id: f_00004-4-0 loss: 0.912879  [   32/  306]
train() client id: f_00004-4-1 loss: 0.870655  [   64/  306]
train() client id: f_00004-4-2 loss: 0.847291  [   96/  306]
train() client id: f_00004-4-3 loss: 0.663455  [  128/  306]
train() client id: f_00004-4-4 loss: 0.849176  [  160/  306]
train() client id: f_00004-4-5 loss: 0.923916  [  192/  306]
train() client id: f_00004-4-6 loss: 0.808897  [  224/  306]
train() client id: f_00004-4-7 loss: 0.871897  [  256/  306]
train() client id: f_00004-4-8 loss: 0.853520  [  288/  306]
train() client id: f_00004-5-0 loss: 0.903812  [   32/  306]
train() client id: f_00004-5-1 loss: 0.927492  [   64/  306]
train() client id: f_00004-5-2 loss: 0.735390  [   96/  306]
train() client id: f_00004-5-3 loss: 0.881541  [  128/  306]
train() client id: f_00004-5-4 loss: 0.791546  [  160/  306]
train() client id: f_00004-5-5 loss: 0.954133  [  192/  306]
train() client id: f_00004-5-6 loss: 0.829484  [  224/  306]
train() client id: f_00004-5-7 loss: 0.745265  [  256/  306]
train() client id: f_00004-5-8 loss: 0.808167  [  288/  306]
train() client id: f_00004-6-0 loss: 0.876819  [   32/  306]
train() client id: f_00004-6-1 loss: 0.756050  [   64/  306]
train() client id: f_00004-6-2 loss: 0.795327  [   96/  306]
train() client id: f_00004-6-3 loss: 0.911260  [  128/  306]
train() client id: f_00004-6-4 loss: 0.760233  [  160/  306]
train() client id: f_00004-6-5 loss: 0.922000  [  192/  306]
train() client id: f_00004-6-6 loss: 0.971263  [  224/  306]
train() client id: f_00004-6-7 loss: 0.810546  [  256/  306]
train() client id: f_00004-6-8 loss: 0.745417  [  288/  306]
train() client id: f_00004-7-0 loss: 0.850392  [   32/  306]
train() client id: f_00004-7-1 loss: 0.916171  [   64/  306]
train() client id: f_00004-7-2 loss: 0.726886  [   96/  306]
train() client id: f_00004-7-3 loss: 0.799092  [  128/  306]
train() client id: f_00004-7-4 loss: 0.903061  [  160/  306]
train() client id: f_00004-7-5 loss: 0.755116  [  192/  306]
train() client id: f_00004-7-6 loss: 0.880231  [  224/  306]
train() client id: f_00004-7-7 loss: 1.054245  [  256/  306]
train() client id: f_00004-7-8 loss: 0.800313  [  288/  306]
train() client id: f_00004-8-0 loss: 0.997390  [   32/  306]
train() client id: f_00004-8-1 loss: 0.925662  [   64/  306]
train() client id: f_00004-8-2 loss: 0.926659  [   96/  306]
train() client id: f_00004-8-3 loss: 0.744411  [  128/  306]
train() client id: f_00004-8-4 loss: 0.810337  [  160/  306]
train() client id: f_00004-8-5 loss: 0.870223  [  192/  306]
train() client id: f_00004-8-6 loss: 0.720895  [  224/  306]
train() client id: f_00004-8-7 loss: 0.878691  [  256/  306]
train() client id: f_00004-8-8 loss: 0.822803  [  288/  306]
train() client id: f_00004-9-0 loss: 0.864709  [   32/  306]
train() client id: f_00004-9-1 loss: 0.758317  [   64/  306]
train() client id: f_00004-9-2 loss: 0.773418  [   96/  306]
train() client id: f_00004-9-3 loss: 0.869414  [  128/  306]
train() client id: f_00004-9-4 loss: 0.888461  [  160/  306]
train() client id: f_00004-9-5 loss: 0.861734  [  192/  306]
train() client id: f_00004-9-6 loss: 0.718908  [  224/  306]
train() client id: f_00004-9-7 loss: 0.977405  [  256/  306]
train() client id: f_00004-9-8 loss: 0.987483  [  288/  306]
train() client id: f_00004-10-0 loss: 0.903039  [   32/  306]
train() client id: f_00004-10-1 loss: 0.799995  [   64/  306]
train() client id: f_00004-10-2 loss: 0.800748  [   96/  306]
train() client id: f_00004-10-3 loss: 1.009466  [  128/  306]
train() client id: f_00004-10-4 loss: 0.683521  [  160/  306]
train() client id: f_00004-10-5 loss: 0.794908  [  192/  306]
train() client id: f_00004-10-6 loss: 0.930887  [  224/  306]
train() client id: f_00004-10-7 loss: 0.891921  [  256/  306]
train() client id: f_00004-10-8 loss: 0.840126  [  288/  306]
train() client id: f_00004-11-0 loss: 0.826756  [   32/  306]
train() client id: f_00004-11-1 loss: 0.921423  [   64/  306]
train() client id: f_00004-11-2 loss: 0.931800  [   96/  306]
train() client id: f_00004-11-3 loss: 0.798009  [  128/  306]
train() client id: f_00004-11-4 loss: 0.894803  [  160/  306]
train() client id: f_00004-11-5 loss: 0.850192  [  192/  306]
train() client id: f_00004-11-6 loss: 0.848543  [  224/  306]
train() client id: f_00004-11-7 loss: 0.874780  [  256/  306]
train() client id: f_00004-11-8 loss: 0.767202  [  288/  306]
train() client id: f_00004-12-0 loss: 0.822879  [   32/  306]
train() client id: f_00004-12-1 loss: 0.819154  [   64/  306]
train() client id: f_00004-12-2 loss: 0.883528  [   96/  306]
train() client id: f_00004-12-3 loss: 0.982669  [  128/  306]
train() client id: f_00004-12-4 loss: 0.877910  [  160/  306]
train() client id: f_00004-12-5 loss: 0.792797  [  192/  306]
train() client id: f_00004-12-6 loss: 0.781488  [  224/  306]
train() client id: f_00004-12-7 loss: 0.733147  [  256/  306]
train() client id: f_00004-12-8 loss: 0.919018  [  288/  306]
train() client id: f_00005-0-0 loss: 1.051113  [   32/  146]
train() client id: f_00005-0-1 loss: 0.896700  [   64/  146]
train() client id: f_00005-0-2 loss: 0.747019  [   96/  146]
train() client id: f_00005-0-3 loss: 0.798894  [  128/  146]
train() client id: f_00005-1-0 loss: 0.779411  [   32/  146]
train() client id: f_00005-1-1 loss: 0.942930  [   64/  146]
train() client id: f_00005-1-2 loss: 0.840726  [   96/  146]
train() client id: f_00005-1-3 loss: 0.879435  [  128/  146]
train() client id: f_00005-2-0 loss: 0.645116  [   32/  146]
train() client id: f_00005-2-1 loss: 0.913828  [   64/  146]
train() client id: f_00005-2-2 loss: 1.064431  [   96/  146]
train() client id: f_00005-2-3 loss: 0.800808  [  128/  146]
train() client id: f_00005-3-0 loss: 0.962318  [   32/  146]
train() client id: f_00005-3-1 loss: 0.651240  [   64/  146]
train() client id: f_00005-3-2 loss: 0.767771  [   96/  146]
train() client id: f_00005-3-3 loss: 0.951148  [  128/  146]
train() client id: f_00005-4-0 loss: 0.972905  [   32/  146]
train() client id: f_00005-4-1 loss: 0.769114  [   64/  146]
train() client id: f_00005-4-2 loss: 0.940630  [   96/  146]
train() client id: f_00005-4-3 loss: 0.730341  [  128/  146]
train() client id: f_00005-5-0 loss: 1.091866  [   32/  146]
train() client id: f_00005-5-1 loss: 0.872617  [   64/  146]
train() client id: f_00005-5-2 loss: 0.623459  [   96/  146]
train() client id: f_00005-5-3 loss: 0.962963  [  128/  146]
train() client id: f_00005-6-0 loss: 0.781023  [   32/  146]
train() client id: f_00005-6-1 loss: 0.966032  [   64/  146]
train() client id: f_00005-6-2 loss: 0.635282  [   96/  146]
train() client id: f_00005-6-3 loss: 0.958546  [  128/  146]
train() client id: f_00005-7-0 loss: 0.815283  [   32/  146]
train() client id: f_00005-7-1 loss: 1.017418  [   64/  146]
train() client id: f_00005-7-2 loss: 0.882186  [   96/  146]
train() client id: f_00005-7-3 loss: 0.918782  [  128/  146]
train() client id: f_00005-8-0 loss: 0.805596  [   32/  146]
train() client id: f_00005-8-1 loss: 0.831289  [   64/  146]
train() client id: f_00005-8-2 loss: 0.805223  [   96/  146]
train() client id: f_00005-8-3 loss: 0.994475  [  128/  146]
train() client id: f_00005-9-0 loss: 0.893928  [   32/  146]
train() client id: f_00005-9-1 loss: 1.049100  [   64/  146]
train() client id: f_00005-9-2 loss: 0.713248  [   96/  146]
train() client id: f_00005-9-3 loss: 0.880859  [  128/  146]
train() client id: f_00005-10-0 loss: 0.890871  [   32/  146]
train() client id: f_00005-10-1 loss: 0.829168  [   64/  146]
train() client id: f_00005-10-2 loss: 0.990401  [   96/  146]
train() client id: f_00005-10-3 loss: 0.915429  [  128/  146]
train() client id: f_00005-11-0 loss: 0.723535  [   32/  146]
train() client id: f_00005-11-1 loss: 0.826445  [   64/  146]
train() client id: f_00005-11-2 loss: 1.028270  [   96/  146]
train() client id: f_00005-11-3 loss: 0.948869  [  128/  146]
train() client id: f_00005-12-0 loss: 0.985340  [   32/  146]
train() client id: f_00005-12-1 loss: 0.729088  [   64/  146]
train() client id: f_00005-12-2 loss: 0.921700  [   96/  146]
train() client id: f_00005-12-3 loss: 0.926576  [  128/  146]
train() client id: f_00006-0-0 loss: 0.466875  [   32/   54]
train() client id: f_00006-1-0 loss: 0.518373  [   32/   54]
train() client id: f_00006-2-0 loss: 0.530563  [   32/   54]
train() client id: f_00006-3-0 loss: 0.506164  [   32/   54]
train() client id: f_00006-4-0 loss: 0.504230  [   32/   54]
train() client id: f_00006-5-0 loss: 0.523830  [   32/   54]
train() client id: f_00006-6-0 loss: 0.510463  [   32/   54]
train() client id: f_00006-7-0 loss: 0.575507  [   32/   54]
train() client id: f_00006-8-0 loss: 0.528321  [   32/   54]
train() client id: f_00006-9-0 loss: 0.502642  [   32/   54]
train() client id: f_00006-10-0 loss: 0.510319  [   32/   54]
train() client id: f_00006-11-0 loss: 0.499143  [   32/   54]
train() client id: f_00006-12-0 loss: 0.567334  [   32/   54]
train() client id: f_00007-0-0 loss: 0.802680  [   32/  179]
train() client id: f_00007-0-1 loss: 0.622229  [   64/  179]
train() client id: f_00007-0-2 loss: 0.507658  [   96/  179]
train() client id: f_00007-0-3 loss: 0.669644  [  128/  179]
train() client id: f_00007-0-4 loss: 0.734044  [  160/  179]
train() client id: f_00007-1-0 loss: 0.618476  [   32/  179]
train() client id: f_00007-1-1 loss: 0.606118  [   64/  179]
train() client id: f_00007-1-2 loss: 0.514045  [   96/  179]
train() client id: f_00007-1-3 loss: 0.622902  [  128/  179]
train() client id: f_00007-1-4 loss: 0.783189  [  160/  179]
train() client id: f_00007-2-0 loss: 0.583542  [   32/  179]
train() client id: f_00007-2-1 loss: 0.785221  [   64/  179]
train() client id: f_00007-2-2 loss: 0.614407  [   96/  179]
train() client id: f_00007-2-3 loss: 0.564656  [  128/  179]
train() client id: f_00007-2-4 loss: 0.564689  [  160/  179]
train() client id: f_00007-3-0 loss: 0.796119  [   32/  179]
train() client id: f_00007-3-1 loss: 0.562247  [   64/  179]
train() client id: f_00007-3-2 loss: 0.740811  [   96/  179]
train() client id: f_00007-3-3 loss: 0.555230  [  128/  179]
train() client id: f_00007-3-4 loss: 0.472368  [  160/  179]
train() client id: f_00007-4-0 loss: 0.702890  [   32/  179]
train() client id: f_00007-4-1 loss: 0.590428  [   64/  179]
train() client id: f_00007-4-2 loss: 0.612440  [   96/  179]
train() client id: f_00007-4-3 loss: 0.631884  [  128/  179]
train() client id: f_00007-4-4 loss: 0.425981  [  160/  179]
train() client id: f_00007-5-0 loss: 0.607541  [   32/  179]
train() client id: f_00007-5-1 loss: 0.672132  [   64/  179]
train() client id: f_00007-5-2 loss: 0.659724  [   96/  179]
train() client id: f_00007-5-3 loss: 0.609434  [  128/  179]
train() client id: f_00007-5-4 loss: 0.605408  [  160/  179]
train() client id: f_00007-6-0 loss: 0.624428  [   32/  179]
train() client id: f_00007-6-1 loss: 0.770492  [   64/  179]
train() client id: f_00007-6-2 loss: 0.571975  [   96/  179]
train() client id: f_00007-6-3 loss: 0.460407  [  128/  179]
train() client id: f_00007-6-4 loss: 0.457254  [  160/  179]
train() client id: f_00007-7-0 loss: 0.709582  [   32/  179]
train() client id: f_00007-7-1 loss: 0.645859  [   64/  179]
train() client id: f_00007-7-2 loss: 0.540068  [   96/  179]
train() client id: f_00007-7-3 loss: 0.524940  [  128/  179]
train() client id: f_00007-7-4 loss: 0.575757  [  160/  179]
train() client id: f_00007-8-0 loss: 0.716697  [   32/  179]
train() client id: f_00007-8-1 loss: 0.801443  [   64/  179]
train() client id: f_00007-8-2 loss: 0.600782  [   96/  179]
train() client id: f_00007-8-3 loss: 0.469405  [  128/  179]
train() client id: f_00007-8-4 loss: 0.434692  [  160/  179]
train() client id: f_00007-9-0 loss: 0.741749  [   32/  179]
train() client id: f_00007-9-1 loss: 0.548667  [   64/  179]
train() client id: f_00007-9-2 loss: 0.565279  [   96/  179]
train() client id: f_00007-9-3 loss: 0.438837  [  128/  179]
train() client id: f_00007-9-4 loss: 0.760730  [  160/  179]
train() client id: f_00007-10-0 loss: 0.556939  [   32/  179]
train() client id: f_00007-10-1 loss: 0.546036  [   64/  179]
train() client id: f_00007-10-2 loss: 0.520328  [   96/  179]
train() client id: f_00007-10-3 loss: 0.603827  [  128/  179]
train() client id: f_00007-10-4 loss: 0.803285  [  160/  179]
train() client id: f_00007-11-0 loss: 0.538671  [   32/  179]
train() client id: f_00007-11-1 loss: 0.433911  [   64/  179]
train() client id: f_00007-11-2 loss: 0.610128  [   96/  179]
train() client id: f_00007-11-3 loss: 0.738613  [  128/  179]
train() client id: f_00007-11-4 loss: 0.686710  [  160/  179]
train() client id: f_00007-12-0 loss: 0.633033  [   32/  179]
train() client id: f_00007-12-1 loss: 0.534776  [   64/  179]
train() client id: f_00007-12-2 loss: 0.547814  [   96/  179]
train() client id: f_00007-12-3 loss: 0.724542  [  128/  179]
train() client id: f_00007-12-4 loss: 0.636042  [  160/  179]
train() client id: f_00008-0-0 loss: 0.688889  [   32/  130]
train() client id: f_00008-0-1 loss: 0.822576  [   64/  130]
train() client id: f_00008-0-2 loss: 0.621780  [   96/  130]
train() client id: f_00008-0-3 loss: 0.619105  [  128/  130]
train() client id: f_00008-1-0 loss: 0.659600  [   32/  130]
train() client id: f_00008-1-1 loss: 0.681355  [   64/  130]
train() client id: f_00008-1-2 loss: 0.767875  [   96/  130]
train() client id: f_00008-1-3 loss: 0.655429  [  128/  130]
train() client id: f_00008-2-0 loss: 0.691405  [   32/  130]
train() client id: f_00008-2-1 loss: 0.725125  [   64/  130]
train() client id: f_00008-2-2 loss: 0.655059  [   96/  130]
train() client id: f_00008-2-3 loss: 0.640257  [  128/  130]
train() client id: f_00008-3-0 loss: 0.733711  [   32/  130]
train() client id: f_00008-3-1 loss: 0.652069  [   64/  130]
train() client id: f_00008-3-2 loss: 0.709369  [   96/  130]
train() client id: f_00008-3-3 loss: 0.650025  [  128/  130]
train() client id: f_00008-4-0 loss: 0.558432  [   32/  130]
train() client id: f_00008-4-1 loss: 0.724171  [   64/  130]
train() client id: f_00008-4-2 loss: 0.759131  [   96/  130]
train() client id: f_00008-4-3 loss: 0.709341  [  128/  130]
train() client id: f_00008-5-0 loss: 0.612778  [   32/  130]
train() client id: f_00008-5-1 loss: 0.593137  [   64/  130]
train() client id: f_00008-5-2 loss: 0.739096  [   96/  130]
train() client id: f_00008-5-3 loss: 0.774677  [  128/  130]
train() client id: f_00008-6-0 loss: 0.593435  [   32/  130]
train() client id: f_00008-6-1 loss: 0.665477  [   64/  130]
train() client id: f_00008-6-2 loss: 0.760248  [   96/  130]
train() client id: f_00008-6-3 loss: 0.726219  [  128/  130]
train() client id: f_00008-7-0 loss: 0.817419  [   32/  130]
train() client id: f_00008-7-1 loss: 0.709673  [   64/  130]
train() client id: f_00008-7-2 loss: 0.682994  [   96/  130]
train() client id: f_00008-7-3 loss: 0.537152  [  128/  130]
train() client id: f_00008-8-0 loss: 0.734427  [   32/  130]
train() client id: f_00008-8-1 loss: 0.774324  [   64/  130]
train() client id: f_00008-8-2 loss: 0.631649  [   96/  130]
train() client id: f_00008-8-3 loss: 0.608472  [  128/  130]
train() client id: f_00008-9-0 loss: 0.701545  [   32/  130]
train() client id: f_00008-9-1 loss: 0.688880  [   64/  130]
train() client id: f_00008-9-2 loss: 0.762145  [   96/  130]
train() client id: f_00008-9-3 loss: 0.586213  [  128/  130]
train() client id: f_00008-10-0 loss: 0.682863  [   32/  130]
train() client id: f_00008-10-1 loss: 0.683874  [   64/  130]
train() client id: f_00008-10-2 loss: 0.731440  [   96/  130]
train() client id: f_00008-10-3 loss: 0.625558  [  128/  130]
train() client id: f_00008-11-0 loss: 0.810870  [   32/  130]
train() client id: f_00008-11-1 loss: 0.676621  [   64/  130]
train() client id: f_00008-11-2 loss: 0.577751  [   96/  130]
train() client id: f_00008-11-3 loss: 0.670454  [  128/  130]
train() client id: f_00008-12-0 loss: 0.712047  [   32/  130]
train() client id: f_00008-12-1 loss: 0.601434  [   64/  130]
train() client id: f_00008-12-2 loss: 0.703464  [   96/  130]
train() client id: f_00008-12-3 loss: 0.677704  [  128/  130]
train() client id: f_00009-0-0 loss: 1.128656  [   32/  118]
train() client id: f_00009-0-1 loss: 1.086384  [   64/  118]
train() client id: f_00009-0-2 loss: 0.820775  [   96/  118]
train() client id: f_00009-1-0 loss: 1.019368  [   32/  118]
train() client id: f_00009-1-1 loss: 0.983640  [   64/  118]
train() client id: f_00009-1-2 loss: 0.842692  [   96/  118]
train() client id: f_00009-2-0 loss: 0.870462  [   32/  118]
train() client id: f_00009-2-1 loss: 0.936697  [   64/  118]
train() client id: f_00009-2-2 loss: 0.977839  [   96/  118]
train() client id: f_00009-3-0 loss: 0.953280  [   32/  118]
train() client id: f_00009-3-1 loss: 0.937934  [   64/  118]
train() client id: f_00009-3-2 loss: 0.733799  [   96/  118]
train() client id: f_00009-4-0 loss: 0.796091  [   32/  118]
train() client id: f_00009-4-1 loss: 0.918093  [   64/  118]
train() client id: f_00009-4-2 loss: 0.863645  [   96/  118]
train() client id: f_00009-5-0 loss: 0.944047  [   32/  118]
train() client id: f_00009-5-1 loss: 0.795265  [   64/  118]
train() client id: f_00009-5-2 loss: 0.719860  [   96/  118]
train() client id: f_00009-6-0 loss: 0.847519  [   32/  118]
train() client id: f_00009-6-1 loss: 0.887319  [   64/  118]
train() client id: f_00009-6-2 loss: 0.736160  [   96/  118]
train() client id: f_00009-7-0 loss: 0.811551  [   32/  118]
train() client id: f_00009-7-1 loss: 0.918074  [   64/  118]
train() client id: f_00009-7-2 loss: 0.667994  [   96/  118]
train() client id: f_00009-8-0 loss: 0.856712  [   32/  118]
train() client id: f_00009-8-1 loss: 0.761444  [   64/  118]
train() client id: f_00009-8-2 loss: 0.734220  [   96/  118]
train() client id: f_00009-9-0 loss: 0.768365  [   32/  118]
train() client id: f_00009-9-1 loss: 0.865426  [   64/  118]
train() client id: f_00009-9-2 loss: 0.733342  [   96/  118]
train() client id: f_00009-10-0 loss: 0.786644  [   32/  118]
train() client id: f_00009-10-1 loss: 0.753534  [   64/  118]
train() client id: f_00009-10-2 loss: 0.744154  [   96/  118]
train() client id: f_00009-11-0 loss: 0.831666  [   32/  118]
train() client id: f_00009-11-1 loss: 0.763802  [   64/  118]
train() client id: f_00009-11-2 loss: 0.794428  [   96/  118]
train() client id: f_00009-12-0 loss: 0.837641  [   32/  118]
train() client id: f_00009-12-1 loss: 0.718669  [   64/  118]
train() client id: f_00009-12-2 loss: 0.791104  [   96/  118]
At round 42 accuracy: 0.6472148541114059
At round 42 training accuracy: 0.5982562038900067
At round 42 training loss: 0.8188626483957253
update_location
xs = [  -3.9056584     4.20031788  230.00902392   18.81129433    0.97929623
    3.95640986 -192.44319194 -171.32485185  214.66397685 -157.06087855]
ys = [ 222.5879595   205.55583871    1.32061395 -192.45517586  184.35018685
  167.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [244.05051502 228.62818177 250.81047646 217.69901127 209.72827757
 195.3899751  216.8900013  198.37560615 237.46429822 186.23676175]
dists_bs = [177.58909768 181.7867245  440.67770376 415.43486141 176.22751969
 179.10062273 178.47941057 174.21642051 420.2648364  171.93692248]
uav_gains = [8.31942576e-12 1.09578607e-11 7.30282910e-12 1.30831832e-11
 1.47787344e-11 1.82276138e-11 1.32493055e-11 1.74589294e-11
 9.39389080e-12 2.07944683e-11]
bs_gains = [5.55812671e-11 5.20619013e-11 4.36269716e-12 5.14619051e-12
 5.67920630e-11 5.42777976e-11 5.48084274e-11 5.86468436e-11
 4.98229610e-12 6.08499840e-11]
Round 43
-------------------------------
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.31748257 10.95769314  5.24195078  1.89681827 12.63633198  6.07983027
  2.34717104  7.46063269  5.5099153   4.93011902]
obj_prev = 62.377945058813665
eta_min = 4.6045000649351174e-18	eta_max = 0.9345899157842099
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 14.450064093228786	eta = 0.9090909090909091
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 27.618160282034836	eta = 0.47564435026761315
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 21.01662000340652	eta = 0.6250492182285267
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 19.820837826510346	eta = 0.6627581547216592
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 19.755289483233774	eta = 0.6649571961010281
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 19.755074608659463	eta = 0.6649644287942615
eta = 0.6649644287942615
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [0.03366898 0.07081173 0.03313455 0.01149021 0.08176751 0.03901326
 0.01442956 0.04783132 0.03473783 0.03153126]
ene_total = [1.80568477 3.13015273 1.80324111 0.85888841 3.56508653 1.85089364
 0.97553732 2.28873162 1.93281417 1.54404431]
ti_comp = [0.58057304 0.62215632 0.57583732 0.59457466 0.62339857 0.62275708
 0.59491807 0.60189896 0.55960141 0.62435462]
ti_coms = [0.11299635 0.07141307 0.11773207 0.09899474 0.07017082 0.07081231
 0.09865133 0.09167044 0.13396798 0.06921478]
t_total = [27.84981956 27.84981956 27.84981956 27.84981956 27.84981956 27.84981956
 27.84981956 27.84981956 27.84981956 27.84981956]
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [7.07711056e-06 5.73319437e-05 6.85683199e-06 2.68194986e-07
 8.79206213e-05 9.56928073e-06 5.30548435e-07 1.88786089e-05
 8.36624401e-06 5.02621941e-06]
ene_total = [0.45401354 0.2890566  0.47302065 0.39751757 0.28529668 0.2847264
 0.39614917 0.36885463 0.53827549 0.27812918]
optimize_network iter = 0 obj = 3.765039909361509
eta = 0.6649644287942615
freqs = [28996330.2200803  56908313.47423983 28770753.14460626  9662542.53169571
 65582048.83208953 31323013.60893005 12127351.29718995 39733680.81777115
 31038012.71809572 25251084.61040274]
eta_min = 0.6649644287942639	eta_max = 0.6649644287943239
af = 0.006654514151181828	bf = 1.2587071102671177	zeta = 0.007319965566300011	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [1.65314891e-06 1.33922227e-05 1.60169383e-06 6.26479190e-08
 2.05374606e-05 2.23529729e-06 1.23931308e-07 4.40987202e-06
 1.95427881e-06 1.17407932e-06]
ene_total = [1.6724149  1.05878427 1.74248852 1.46497738 1.04145823 1.04824288
 1.45990451 1.35723249 1.98280693 1.02444478]
ti_comp = [0.58057304 0.62215632 0.57583732 0.59457466 0.62339857 0.62275708
 0.59491807 0.60189896 0.55960141 0.62435462]
ti_coms = [0.11299635 0.07141307 0.11773207 0.09899474 0.07017082 0.07081231
 0.09865133 0.09167044 0.13396798 0.06921478]
t_total = [27.84981956 27.84981956 27.84981956 27.84981956 27.84981956 27.84981956
 27.84981956 27.84981956 27.84981956 27.84981956]
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [7.07711056e-06 5.73319437e-05 6.85683199e-06 2.68194986e-07
 8.79206213e-05 9.56928073e-06 5.30548435e-07 1.88786089e-05
 8.36624401e-06 5.02621941e-06]
ene_total = [0.45401354 0.2890566  0.47302065 0.39751757 0.28529668 0.2847264
 0.39614917 0.36885463 0.53827549 0.27812918]
optimize_network iter = 1 obj = 3.765039909362207
eta = 0.6649644287943239
freqs = [28996330.22008007 56908313.47423852 28770753.14460608  9662542.53169558
 65582048.832088   31323013.60892932 12127351.29718979 39733680.81777052
 31038012.71809573 25251084.61040214]
Done!
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [6.88604689e-06 5.57841296e-05 6.67171527e-06 2.60954415e-07
 8.55469920e-05 9.31093491e-06 5.16225000e-07 1.83689353e-05
 8.14037707e-06 4.89052449e-06]
ene_total = [0.01130652 0.00719709 0.01177988 0.00989973 0.00710263 0.00709054
 0.00986565 0.00918541 0.01340494 0.00692637]
At round 43 energy consumption: 0.09375876535404022
At round 43 eta: 0.6649644287943239
At round 43 a_n: 13.110585589604415
At round 43 local rounds: 13.360704448743727
At round 43 global rounds: 40.15433760708037
gradient difference: 0.42047548294067383
train() client id: f_00000-0-0 loss: 0.746006  [   32/  126]
train() client id: f_00000-0-1 loss: 1.154966  [   64/  126]
train() client id: f_00000-0-2 loss: 1.133971  [   96/  126]
train() client id: f_00000-1-0 loss: 1.089061  [   32/  126]
train() client id: f_00000-1-1 loss: 1.007115  [   64/  126]
train() client id: f_00000-1-2 loss: 0.942945  [   96/  126]
train() client id: f_00000-2-0 loss: 0.874473  [   32/  126]
train() client id: f_00000-2-1 loss: 0.822940  [   64/  126]
train() client id: f_00000-2-2 loss: 1.002227  [   96/  126]
train() client id: f_00000-3-0 loss: 0.860196  [   32/  126]
train() client id: f_00000-3-1 loss: 0.709559  [   64/  126]
train() client id: f_00000-3-2 loss: 0.796669  [   96/  126]
train() client id: f_00000-4-0 loss: 0.818320  [   32/  126]
train() client id: f_00000-4-1 loss: 0.864989  [   64/  126]
train() client id: f_00000-4-2 loss: 0.725028  [   96/  126]
train() client id: f_00000-5-0 loss: 0.774034  [   32/  126]
train() client id: f_00000-5-1 loss: 0.723852  [   64/  126]
train() client id: f_00000-5-2 loss: 0.883724  [   96/  126]
train() client id: f_00000-6-0 loss: 0.816575  [   32/  126]
train() client id: f_00000-6-1 loss: 0.767363  [   64/  126]
train() client id: f_00000-6-2 loss: 0.711893  [   96/  126]
train() client id: f_00000-7-0 loss: 0.788071  [   32/  126]
train() client id: f_00000-7-1 loss: 0.805532  [   64/  126]
train() client id: f_00000-7-2 loss: 0.716049  [   96/  126]
train() client id: f_00000-8-0 loss: 0.784884  [   32/  126]
train() client id: f_00000-8-1 loss: 0.756954  [   64/  126]
train() client id: f_00000-8-2 loss: 0.749865  [   96/  126]
train() client id: f_00000-9-0 loss: 0.756663  [   32/  126]
train() client id: f_00000-9-1 loss: 0.853879  [   64/  126]
train() client id: f_00000-9-2 loss: 0.764410  [   96/  126]
train() client id: f_00000-10-0 loss: 0.882667  [   32/  126]
train() client id: f_00000-10-1 loss: 0.820382  [   64/  126]
train() client id: f_00000-10-2 loss: 0.754739  [   96/  126]
train() client id: f_00000-11-0 loss: 0.785931  [   32/  126]
train() client id: f_00000-11-1 loss: 0.828183  [   64/  126]
train() client id: f_00000-11-2 loss: 0.866071  [   96/  126]
train() client id: f_00000-12-0 loss: 0.747059  [   32/  126]
train() client id: f_00000-12-1 loss: 0.872091  [   64/  126]
train() client id: f_00000-12-2 loss: 0.787761  [   96/  126]
train() client id: f_00001-0-0 loss: 0.341781  [   32/  265]
train() client id: f_00001-0-1 loss: 0.194282  [   64/  265]
train() client id: f_00001-0-2 loss: 0.315938  [   96/  265]
train() client id: f_00001-0-3 loss: 0.310849  [  128/  265]
train() client id: f_00001-0-4 loss: 0.476822  [  160/  265]
train() client id: f_00001-0-5 loss: 0.261243  [  192/  265]
train() client id: f_00001-0-6 loss: 0.206674  [  224/  265]
train() client id: f_00001-0-7 loss: 0.258574  [  256/  265]
train() client id: f_00001-1-0 loss: 0.362748  [   32/  265]
train() client id: f_00001-1-1 loss: 0.323045  [   64/  265]
train() client id: f_00001-1-2 loss: 0.164473  [   96/  265]
train() client id: f_00001-1-3 loss: 0.291419  [  128/  265]
train() client id: f_00001-1-4 loss: 0.228103  [  160/  265]
train() client id: f_00001-1-5 loss: 0.261416  [  192/  265]
train() client id: f_00001-1-6 loss: 0.260791  [  224/  265]
train() client id: f_00001-1-7 loss: 0.303503  [  256/  265]
train() client id: f_00001-2-0 loss: 0.259600  [   32/  265]
train() client id: f_00001-2-1 loss: 0.405464  [   64/  265]
train() client id: f_00001-2-2 loss: 0.276027  [   96/  265]
train() client id: f_00001-2-3 loss: 0.236969  [  128/  265]
train() client id: f_00001-2-4 loss: 0.245080  [  160/  265]
train() client id: f_00001-2-5 loss: 0.238215  [  192/  265]
train() client id: f_00001-2-6 loss: 0.211362  [  224/  265]
train() client id: f_00001-2-7 loss: 0.312315  [  256/  265]
train() client id: f_00001-3-0 loss: 0.238676  [   32/  265]
train() client id: f_00001-3-1 loss: 0.162193  [   64/  265]
train() client id: f_00001-3-2 loss: 0.265417  [   96/  265]
train() client id: f_00001-3-3 loss: 0.304542  [  128/  265]
train() client id: f_00001-3-4 loss: 0.321526  [  160/  265]
train() client id: f_00001-3-5 loss: 0.163072  [  192/  265]
train() client id: f_00001-3-6 loss: 0.314633  [  224/  265]
train() client id: f_00001-3-7 loss: 0.242951  [  256/  265]
train() client id: f_00001-4-0 loss: 0.181490  [   32/  265]
train() client id: f_00001-4-1 loss: 0.157761  [   64/  265]
train() client id: f_00001-4-2 loss: 0.337531  [   96/  265]
train() client id: f_00001-4-3 loss: 0.266985  [  128/  265]
train() client id: f_00001-4-4 loss: 0.345245  [  160/  265]
train() client id: f_00001-4-5 loss: 0.247123  [  192/  265]
train() client id: f_00001-4-6 loss: 0.213143  [  224/  265]
train() client id: f_00001-4-7 loss: 0.255221  [  256/  265]
train() client id: f_00001-5-0 loss: 0.168871  [   32/  265]
train() client id: f_00001-5-1 loss: 0.301677  [   64/  265]
train() client id: f_00001-5-2 loss: 0.318186  [   96/  265]
train() client id: f_00001-5-3 loss: 0.328183  [  128/  265]
train() client id: f_00001-5-4 loss: 0.242705  [  160/  265]
train() client id: f_00001-5-5 loss: 0.234557  [  192/  265]
train() client id: f_00001-5-6 loss: 0.252632  [  224/  265]
train() client id: f_00001-5-7 loss: 0.153497  [  256/  265]
train() client id: f_00001-6-0 loss: 0.295320  [   32/  265]
train() client id: f_00001-6-1 loss: 0.231946  [   64/  265]
train() client id: f_00001-6-2 loss: 0.145130  [   96/  265]
train() client id: f_00001-6-3 loss: 0.250384  [  128/  265]
train() client id: f_00001-6-4 loss: 0.302863  [  160/  265]
train() client id: f_00001-6-5 loss: 0.316584  [  192/  265]
train() client id: f_00001-6-6 loss: 0.203051  [  224/  265]
train() client id: f_00001-6-7 loss: 0.203722  [  256/  265]
train() client id: f_00001-7-0 loss: 0.165270  [   32/  265]
train() client id: f_00001-7-1 loss: 0.234681  [   64/  265]
train() client id: f_00001-7-2 loss: 0.365616  [   96/  265]
train() client id: f_00001-7-3 loss: 0.264839  [  128/  265]
train() client id: f_00001-7-4 loss: 0.296587  [  160/  265]
train() client id: f_00001-7-5 loss: 0.241505  [  192/  265]
train() client id: f_00001-7-6 loss: 0.214126  [  224/  265]
train() client id: f_00001-7-7 loss: 0.146197  [  256/  265]
train() client id: f_00001-8-0 loss: 0.457281  [   32/  265]
train() client id: f_00001-8-1 loss: 0.279751  [   64/  265]
train() client id: f_00001-8-2 loss: 0.169050  [   96/  265]
train() client id: f_00001-8-3 loss: 0.153601  [  128/  265]
train() client id: f_00001-8-4 loss: 0.236338  [  160/  265]
train() client id: f_00001-8-5 loss: 0.151474  [  192/  265]
train() client id: f_00001-8-6 loss: 0.207573  [  224/  265]
train() client id: f_00001-8-7 loss: 0.247139  [  256/  265]
train() client id: f_00001-9-0 loss: 0.297609  [   32/  265]
train() client id: f_00001-9-1 loss: 0.318024  [   64/  265]
train() client id: f_00001-9-2 loss: 0.229982  [   96/  265]
train() client id: f_00001-9-3 loss: 0.270998  [  128/  265]
train() client id: f_00001-9-4 loss: 0.152889  [  160/  265]
train() client id: f_00001-9-5 loss: 0.139043  [  192/  265]
train() client id: f_00001-9-6 loss: 0.188323  [  224/  265]
train() client id: f_00001-9-7 loss: 0.193625  [  256/  265]
train() client id: f_00001-10-0 loss: 0.365119  [   32/  265]
train() client id: f_00001-10-1 loss: 0.264498  [   64/  265]
train() client id: f_00001-10-2 loss: 0.197506  [   96/  265]
train() client id: f_00001-10-3 loss: 0.236032  [  128/  265]
train() client id: f_00001-10-4 loss: 0.136252  [  160/  265]
train() client id: f_00001-10-5 loss: 0.212650  [  192/  265]
train() client id: f_00001-10-6 loss: 0.304362  [  224/  265]
train() client id: f_00001-10-7 loss: 0.135914  [  256/  265]
train() client id: f_00001-11-0 loss: 0.351495  [   32/  265]
train() client id: f_00001-11-1 loss: 0.119234  [   64/  265]
train() client id: f_00001-11-2 loss: 0.221973  [   96/  265]
train() client id: f_00001-11-3 loss: 0.250145  [  128/  265]
train() client id: f_00001-11-4 loss: 0.136677  [  160/  265]
train() client id: f_00001-11-5 loss: 0.256266  [  192/  265]
train() client id: f_00001-11-6 loss: 0.191775  [  224/  265]
train() client id: f_00001-11-7 loss: 0.309536  [  256/  265]
train() client id: f_00001-12-0 loss: 0.210966  [   32/  265]
train() client id: f_00001-12-1 loss: 0.172820  [   64/  265]
train() client id: f_00001-12-2 loss: 0.257467  [   96/  265]
train() client id: f_00001-12-3 loss: 0.357351  [  128/  265]
train() client id: f_00001-12-4 loss: 0.203701  [  160/  265]
train() client id: f_00001-12-5 loss: 0.180945  [  192/  265]
train() client id: f_00001-12-6 loss: 0.230626  [  224/  265]
train() client id: f_00001-12-7 loss: 0.226416  [  256/  265]
train() client id: f_00002-0-0 loss: 1.093021  [   32/  124]
train() client id: f_00002-0-1 loss: 1.204498  [   64/  124]
train() client id: f_00002-0-2 loss: 1.145693  [   96/  124]
train() client id: f_00002-1-0 loss: 1.155051  [   32/  124]
train() client id: f_00002-1-1 loss: 1.050070  [   64/  124]
train() client id: f_00002-1-2 loss: 1.103431  [   96/  124]
train() client id: f_00002-2-0 loss: 1.125805  [   32/  124]
train() client id: f_00002-2-1 loss: 0.869145  [   64/  124]
train() client id: f_00002-2-2 loss: 1.036895  [   96/  124]
train() client id: f_00002-3-0 loss: 1.080193  [   32/  124]
train() client id: f_00002-3-1 loss: 1.036193  [   64/  124]
train() client id: f_00002-3-2 loss: 0.951679  [   96/  124]
train() client id: f_00002-4-0 loss: 0.942090  [   32/  124]
train() client id: f_00002-4-1 loss: 1.052731  [   64/  124]
train() client id: f_00002-4-2 loss: 0.955503  [   96/  124]
train() client id: f_00002-5-0 loss: 0.895141  [   32/  124]
train() client id: f_00002-5-1 loss: 0.947864  [   64/  124]
train() client id: f_00002-5-2 loss: 1.025464  [   96/  124]
train() client id: f_00002-6-0 loss: 1.056640  [   32/  124]
train() client id: f_00002-6-1 loss: 0.956212  [   64/  124]
train() client id: f_00002-6-2 loss: 0.856493  [   96/  124]
train() client id: f_00002-7-0 loss: 0.976892  [   32/  124]
train() client id: f_00002-7-1 loss: 1.086938  [   64/  124]
train() client id: f_00002-7-2 loss: 0.810169  [   96/  124]
train() client id: f_00002-8-0 loss: 0.839358  [   32/  124]
train() client id: f_00002-8-1 loss: 1.025974  [   64/  124]
train() client id: f_00002-8-2 loss: 0.952708  [   96/  124]
train() client id: f_00002-9-0 loss: 0.827175  [   32/  124]
train() client id: f_00002-9-1 loss: 0.805963  [   64/  124]
train() client id: f_00002-9-2 loss: 0.968156  [   96/  124]
train() client id: f_00002-10-0 loss: 0.939888  [   32/  124]
train() client id: f_00002-10-1 loss: 0.702948  [   64/  124]
train() client id: f_00002-10-2 loss: 0.816143  [   96/  124]
train() client id: f_00002-11-0 loss: 0.978637  [   32/  124]
train() client id: f_00002-11-1 loss: 0.845596  [   64/  124]
train() client id: f_00002-11-2 loss: 0.762265  [   96/  124]
train() client id: f_00002-12-0 loss: 0.978287  [   32/  124]
train() client id: f_00002-12-1 loss: 0.683918  [   64/  124]
train() client id: f_00002-12-2 loss: 1.015314  [   96/  124]
train() client id: f_00003-0-0 loss: 0.491430  [   32/   43]
train() client id: f_00003-1-0 loss: 0.471978  [   32/   43]
train() client id: f_00003-2-0 loss: 0.446990  [   32/   43]
train() client id: f_00003-3-0 loss: 0.606544  [   32/   43]
train() client id: f_00003-4-0 loss: 0.504821  [   32/   43]
train() client id: f_00003-5-0 loss: 0.664585  [   32/   43]
train() client id: f_00003-6-0 loss: 0.407350  [   32/   43]
train() client id: f_00003-7-0 loss: 0.538001  [   32/   43]
train() client id: f_00003-8-0 loss: 0.578447  [   32/   43]
train() client id: f_00003-9-0 loss: 0.460376  [   32/   43]
train() client id: f_00003-10-0 loss: 0.664051  [   32/   43]
train() client id: f_00003-11-0 loss: 0.412533  [   32/   43]
train() client id: f_00003-12-0 loss: 0.524011  [   32/   43]
train() client id: f_00004-0-0 loss: 0.743527  [   32/  306]
train() client id: f_00004-0-1 loss: 0.712292  [   64/  306]
train() client id: f_00004-0-2 loss: 0.742916  [   96/  306]
train() client id: f_00004-0-3 loss: 0.814131  [  128/  306]
train() client id: f_00004-0-4 loss: 0.688715  [  160/  306]
train() client id: f_00004-0-5 loss: 0.672902  [  192/  306]
train() client id: f_00004-0-6 loss: 0.675930  [  224/  306]
train() client id: f_00004-0-7 loss: 0.780670  [  256/  306]
train() client id: f_00004-0-8 loss: 0.776523  [  288/  306]
train() client id: f_00004-1-0 loss: 0.785555  [   32/  306]
train() client id: f_00004-1-1 loss: 0.732616  [   64/  306]
train() client id: f_00004-1-2 loss: 0.655804  [   96/  306]
train() client id: f_00004-1-3 loss: 0.624189  [  128/  306]
train() client id: f_00004-1-4 loss: 0.613266  [  160/  306]
train() client id: f_00004-1-5 loss: 0.828121  [  192/  306]
train() client id: f_00004-1-6 loss: 0.783204  [  224/  306]
train() client id: f_00004-1-7 loss: 0.753596  [  256/  306]
train() client id: f_00004-1-8 loss: 0.785896  [  288/  306]
train() client id: f_00004-2-0 loss: 0.857943  [   32/  306]
train() client id: f_00004-2-1 loss: 0.645985  [   64/  306]
train() client id: f_00004-2-2 loss: 0.686937  [   96/  306]
train() client id: f_00004-2-3 loss: 0.642602  [  128/  306]
train() client id: f_00004-2-4 loss: 0.792935  [  160/  306]
train() client id: f_00004-2-5 loss: 0.765345  [  192/  306]
train() client id: f_00004-2-6 loss: 0.664798  [  224/  306]
train() client id: f_00004-2-7 loss: 0.748756  [  256/  306]
train() client id: f_00004-2-8 loss: 0.721541  [  288/  306]
train() client id: f_00004-3-0 loss: 0.712082  [   32/  306]
train() client id: f_00004-3-1 loss: 0.786170  [   64/  306]
train() client id: f_00004-3-2 loss: 0.740013  [   96/  306]
train() client id: f_00004-3-3 loss: 0.700387  [  128/  306]
train() client id: f_00004-3-4 loss: 0.739461  [  160/  306]
train() client id: f_00004-3-5 loss: 0.749950  [  192/  306]
train() client id: f_00004-3-6 loss: 0.676635  [  224/  306]
train() client id: f_00004-3-7 loss: 0.741663  [  256/  306]
train() client id: f_00004-3-8 loss: 0.715493  [  288/  306]
train() client id: f_00004-4-0 loss: 0.742911  [   32/  306]
train() client id: f_00004-4-1 loss: 0.753058  [   64/  306]
train() client id: f_00004-4-2 loss: 0.690103  [   96/  306]
train() client id: f_00004-4-3 loss: 0.620898  [  128/  306]
train() client id: f_00004-4-4 loss: 0.775316  [  160/  306]
train() client id: f_00004-4-5 loss: 0.715110  [  192/  306]
train() client id: f_00004-4-6 loss: 0.812280  [  224/  306]
train() client id: f_00004-4-7 loss: 0.691860  [  256/  306]
train() client id: f_00004-4-8 loss: 0.766277  [  288/  306]
train() client id: f_00004-5-0 loss: 0.669239  [   32/  306]
train() client id: f_00004-5-1 loss: 0.778044  [   64/  306]
train() client id: f_00004-5-2 loss: 0.854211  [   96/  306]
train() client id: f_00004-5-3 loss: 0.776177  [  128/  306]
train() client id: f_00004-5-4 loss: 0.710211  [  160/  306]
train() client id: f_00004-5-5 loss: 0.642088  [  192/  306]
train() client id: f_00004-5-6 loss: 0.757938  [  224/  306]
train() client id: f_00004-5-7 loss: 0.639189  [  256/  306]
train() client id: f_00004-5-8 loss: 0.678911  [  288/  306]
train() client id: f_00004-6-0 loss: 0.765964  [   32/  306]
train() client id: f_00004-6-1 loss: 0.642555  [   64/  306]
train() client id: f_00004-6-2 loss: 0.746689  [   96/  306]
train() client id: f_00004-6-3 loss: 0.743967  [  128/  306]
train() client id: f_00004-6-4 loss: 0.728524  [  160/  306]
train() client id: f_00004-6-5 loss: 0.719288  [  192/  306]
train() client id: f_00004-6-6 loss: 0.839932  [  224/  306]
train() client id: f_00004-6-7 loss: 0.657401  [  256/  306]
train() client id: f_00004-6-8 loss: 0.772100  [  288/  306]
train() client id: f_00004-7-0 loss: 0.636755  [   32/  306]
train() client id: f_00004-7-1 loss: 0.794768  [   64/  306]
train() client id: f_00004-7-2 loss: 0.839535  [   96/  306]
train() client id: f_00004-7-3 loss: 0.700203  [  128/  306]
train() client id: f_00004-7-4 loss: 0.819155  [  160/  306]
train() client id: f_00004-7-5 loss: 0.620266  [  192/  306]
train() client id: f_00004-7-6 loss: 0.718452  [  224/  306]
train() client id: f_00004-7-7 loss: 0.601247  [  256/  306]
train() client id: f_00004-7-8 loss: 0.706588  [  288/  306]
train() client id: f_00004-8-0 loss: 0.690446  [   32/  306]
train() client id: f_00004-8-1 loss: 0.677557  [   64/  306]
train() client id: f_00004-8-2 loss: 0.716902  [   96/  306]
train() client id: f_00004-8-3 loss: 0.774635  [  128/  306]
train() client id: f_00004-8-4 loss: 0.818517  [  160/  306]
train() client id: f_00004-8-5 loss: 0.624621  [  192/  306]
train() client id: f_00004-8-6 loss: 0.750926  [  224/  306]
train() client id: f_00004-8-7 loss: 0.849397  [  256/  306]
train() client id: f_00004-8-8 loss: 0.659049  [  288/  306]
train() client id: f_00004-9-0 loss: 0.731995  [   32/  306]
train() client id: f_00004-9-1 loss: 0.729081  [   64/  306]
train() client id: f_00004-9-2 loss: 0.615076  [   96/  306]
train() client id: f_00004-9-3 loss: 0.919838  [  128/  306]
train() client id: f_00004-9-4 loss: 0.689491  [  160/  306]
train() client id: f_00004-9-5 loss: 0.817398  [  192/  306]
train() client id: f_00004-9-6 loss: 0.694298  [  224/  306]
train() client id: f_00004-9-7 loss: 0.734082  [  256/  306]
train() client id: f_00004-9-8 loss: 0.729734  [  288/  306]
train() client id: f_00004-10-0 loss: 0.758433  [   32/  306]
train() client id: f_00004-10-1 loss: 0.657272  [   64/  306]
train() client id: f_00004-10-2 loss: 0.783766  [   96/  306]
train() client id: f_00004-10-3 loss: 0.757568  [  128/  306]
train() client id: f_00004-10-4 loss: 0.725206  [  160/  306]
train() client id: f_00004-10-5 loss: 0.819811  [  192/  306]
train() client id: f_00004-10-6 loss: 0.666600  [  224/  306]
train() client id: f_00004-10-7 loss: 0.723385  [  256/  306]
train() client id: f_00004-10-8 loss: 0.727889  [  288/  306]
train() client id: f_00004-11-0 loss: 0.729299  [   32/  306]
train() client id: f_00004-11-1 loss: 0.708502  [   64/  306]
train() client id: f_00004-11-2 loss: 0.748173  [   96/  306]
train() client id: f_00004-11-3 loss: 0.834208  [  128/  306]
train() client id: f_00004-11-4 loss: 0.681853  [  160/  306]
train() client id: f_00004-11-5 loss: 0.694465  [  192/  306]
train() client id: f_00004-11-6 loss: 0.735732  [  224/  306]
train() client id: f_00004-11-7 loss: 0.720899  [  256/  306]
train() client id: f_00004-11-8 loss: 0.732853  [  288/  306]
train() client id: f_00004-12-0 loss: 0.746377  [   32/  306]
train() client id: f_00004-12-1 loss: 0.578068  [   64/  306]
train() client id: f_00004-12-2 loss: 0.690574  [   96/  306]
train() client id: f_00004-12-3 loss: 0.868097  [  128/  306]
train() client id: f_00004-12-4 loss: 0.714970  [  160/  306]
train() client id: f_00004-12-5 loss: 0.745542  [  192/  306]
train() client id: f_00004-12-6 loss: 0.853376  [  224/  306]
train() client id: f_00004-12-7 loss: 0.657820  [  256/  306]
train() client id: f_00004-12-8 loss: 0.654484  [  288/  306]
train() client id: f_00005-0-0 loss: 0.691902  [   32/  146]
train() client id: f_00005-0-1 loss: 0.606923  [   64/  146]
train() client id: f_00005-0-2 loss: 0.654943  [   96/  146]
train() client id: f_00005-0-3 loss: 0.500570  [  128/  146]
train() client id: f_00005-1-0 loss: 0.462465  [   32/  146]
train() client id: f_00005-1-1 loss: 0.624612  [   64/  146]
train() client id: f_00005-1-2 loss: 0.667842  [   96/  146]
train() client id: f_00005-1-3 loss: 0.426978  [  128/  146]
train() client id: f_00005-2-0 loss: 0.573673  [   32/  146]
train() client id: f_00005-2-1 loss: 0.654294  [   64/  146]
train() client id: f_00005-2-2 loss: 0.558744  [   96/  146]
train() client id: f_00005-2-3 loss: 0.519446  [  128/  146]
train() client id: f_00005-3-0 loss: 0.706830  [   32/  146]
train() client id: f_00005-3-1 loss: 0.516453  [   64/  146]
train() client id: f_00005-3-2 loss: 0.494124  [   96/  146]
train() client id: f_00005-3-3 loss: 0.556984  [  128/  146]
train() client id: f_00005-4-0 loss: 0.601649  [   32/  146]
train() client id: f_00005-4-1 loss: 0.811898  [   64/  146]
train() client id: f_00005-4-2 loss: 0.320803  [   96/  146]
train() client id: f_00005-4-3 loss: 0.577612  [  128/  146]
train() client id: f_00005-5-0 loss: 0.686836  [   32/  146]
train() client id: f_00005-5-1 loss: 0.621985  [   64/  146]
train() client id: f_00005-5-2 loss: 0.337855  [   96/  146]
train() client id: f_00005-5-3 loss: 0.717389  [  128/  146]
train() client id: f_00005-6-0 loss: 0.762834  [   32/  146]
train() client id: f_00005-6-1 loss: 0.380389  [   64/  146]
train() client id: f_00005-6-2 loss: 0.516430  [   96/  146]
train() client id: f_00005-6-3 loss: 0.581348  [  128/  146]
train() client id: f_00005-7-0 loss: 0.418188  [   32/  146]
train() client id: f_00005-7-1 loss: 0.533690  [   64/  146]
train() client id: f_00005-7-2 loss: 0.591418  [   96/  146]
train() client id: f_00005-7-3 loss: 0.521380  [  128/  146]
train() client id: f_00005-8-0 loss: 0.641111  [   32/  146]
train() client id: f_00005-8-1 loss: 0.601036  [   64/  146]
train() client id: f_00005-8-2 loss: 0.539491  [   96/  146]
train() client id: f_00005-8-3 loss: 0.418165  [  128/  146]
train() client id: f_00005-9-0 loss: 0.448995  [   32/  146]
train() client id: f_00005-9-1 loss: 0.613083  [   64/  146]
train() client id: f_00005-9-2 loss: 0.802767  [   96/  146]
train() client id: f_00005-9-3 loss: 0.482820  [  128/  146]
train() client id: f_00005-10-0 loss: 0.682706  [   32/  146]
train() client id: f_00005-10-1 loss: 0.376003  [   64/  146]
train() client id: f_00005-10-2 loss: 0.493677  [   96/  146]
train() client id: f_00005-10-3 loss: 0.644895  [  128/  146]
train() client id: f_00005-11-0 loss: 0.642280  [   32/  146]
train() client id: f_00005-11-1 loss: 0.521866  [   64/  146]
train() client id: f_00005-11-2 loss: 0.599175  [   96/  146]
train() client id: f_00005-11-3 loss: 0.551864  [  128/  146]
train() client id: f_00005-12-0 loss: 0.393315  [   32/  146]
train() client id: f_00005-12-1 loss: 0.551353  [   64/  146]
train() client id: f_00005-12-2 loss: 0.619479  [   96/  146]
train() client id: f_00005-12-3 loss: 0.759116  [  128/  146]
train() client id: f_00006-0-0 loss: 0.461698  [   32/   54]
train() client id: f_00006-1-0 loss: 0.493799  [   32/   54]
train() client id: f_00006-2-0 loss: 0.528568  [   32/   54]
train() client id: f_00006-3-0 loss: 0.512157  [   32/   54]
train() client id: f_00006-4-0 loss: 0.488538  [   32/   54]
train() client id: f_00006-5-0 loss: 0.466560  [   32/   54]
train() client id: f_00006-6-0 loss: 0.475340  [   32/   54]
train() client id: f_00006-7-0 loss: 0.450721  [   32/   54]
train() client id: f_00006-8-0 loss: 0.466391  [   32/   54]
train() client id: f_00006-9-0 loss: 0.413890  [   32/   54]
train() client id: f_00006-10-0 loss: 0.471377  [   32/   54]
train() client id: f_00006-11-0 loss: 0.462734  [   32/   54]
train() client id: f_00006-12-0 loss: 0.491426  [   32/   54]
train() client id: f_00007-0-0 loss: 0.509370  [   32/  179]
train() client id: f_00007-0-1 loss: 0.477960  [   64/  179]
train() client id: f_00007-0-2 loss: 0.672436  [   96/  179]
train() client id: f_00007-0-3 loss: 0.606145  [  128/  179]
train() client id: f_00007-0-4 loss: 0.428492  [  160/  179]
train() client id: f_00007-1-0 loss: 0.583362  [   32/  179]
train() client id: f_00007-1-1 loss: 0.424612  [   64/  179]
train() client id: f_00007-1-2 loss: 0.496299  [   96/  179]
train() client id: f_00007-1-3 loss: 0.671119  [  128/  179]
train() client id: f_00007-1-4 loss: 0.520513  [  160/  179]
train() client id: f_00007-2-0 loss: 0.571309  [   32/  179]
train() client id: f_00007-2-1 loss: 0.436538  [   64/  179]
train() client id: f_00007-2-2 loss: 0.763568  [   96/  179]
train() client id: f_00007-2-3 loss: 0.459204  [  128/  179]
train() client id: f_00007-2-4 loss: 0.385859  [  160/  179]
train() client id: f_00007-3-0 loss: 0.596596  [   32/  179]
train() client id: f_00007-3-1 loss: 0.415493  [   64/  179]
train() client id: f_00007-3-2 loss: 0.450474  [   96/  179]
train() client id: f_00007-3-3 loss: 0.731614  [  128/  179]
train() client id: f_00007-3-4 loss: 0.339683  [  160/  179]
train() client id: f_00007-4-0 loss: 0.778329  [   32/  179]
train() client id: f_00007-4-1 loss: 0.423257  [   64/  179]
train() client id: f_00007-4-2 loss: 0.550796  [   96/  179]
train() client id: f_00007-4-3 loss: 0.363430  [  128/  179]
train() client id: f_00007-4-4 loss: 0.435529  [  160/  179]
train() client id: f_00007-5-0 loss: 0.530683  [   32/  179]
train() client id: f_00007-5-1 loss: 0.663543  [   64/  179]
train() client id: f_00007-5-2 loss: 0.382199  [   96/  179]
train() client id: f_00007-5-3 loss: 0.498120  [  128/  179]
train() client id: f_00007-5-4 loss: 0.445732  [  160/  179]
train() client id: f_00007-6-0 loss: 0.428725  [   32/  179]
train() client id: f_00007-6-1 loss: 0.444102  [   64/  179]
train() client id: f_00007-6-2 loss: 0.492580  [   96/  179]
train() client id: f_00007-6-3 loss: 0.689732  [  128/  179]
train() client id: f_00007-6-4 loss: 0.347434  [  160/  179]
train() client id: f_00007-7-0 loss: 0.366376  [   32/  179]
train() client id: f_00007-7-1 loss: 0.562909  [   64/  179]
train() client id: f_00007-7-2 loss: 0.462411  [   96/  179]
train() client id: f_00007-7-3 loss: 0.591176  [  128/  179]
train() client id: f_00007-7-4 loss: 0.320141  [  160/  179]
train() client id: f_00007-8-0 loss: 0.593719  [   32/  179]
train() client id: f_00007-8-1 loss: 0.444525  [   64/  179]
train() client id: f_00007-8-2 loss: 0.321792  [   96/  179]
train() client id: f_00007-8-3 loss: 0.612055  [  128/  179]
train() client id: f_00007-8-4 loss: 0.436732  [  160/  179]
train() client id: f_00007-9-0 loss: 0.445038  [   32/  179]
train() client id: f_00007-9-1 loss: 0.642105  [   64/  179]
train() client id: f_00007-9-2 loss: 0.344022  [   96/  179]
train() client id: f_00007-9-3 loss: 0.333708  [  128/  179]
train() client id: f_00007-9-4 loss: 0.478677  [  160/  179]
train() client id: f_00007-10-0 loss: 0.378761  [   32/  179]
train() client id: f_00007-10-1 loss: 0.421772  [   64/  179]
train() client id: f_00007-10-2 loss: 0.432402  [   96/  179]
train() client id: f_00007-10-3 loss: 0.651212  [  128/  179]
train() client id: f_00007-10-4 loss: 0.550246  [  160/  179]
train() client id: f_00007-11-0 loss: 0.404114  [   32/  179]
train() client id: f_00007-11-1 loss: 0.511017  [   64/  179]
train() client id: f_00007-11-2 loss: 0.485340  [   96/  179]
train() client id: f_00007-11-3 loss: 0.417012  [  128/  179]
train() client id: f_00007-11-4 loss: 0.329303  [  160/  179]
train() client id: f_00007-12-0 loss: 0.424136  [   32/  179]
train() client id: f_00007-12-1 loss: 0.302185  [   64/  179]
train() client id: f_00007-12-2 loss: 0.397044  [   96/  179]
train() client id: f_00007-12-3 loss: 0.492398  [  128/  179]
train() client id: f_00007-12-4 loss: 0.520887  [  160/  179]
train() client id: f_00008-0-0 loss: 0.793240  [   32/  130]
train() client id: f_00008-0-1 loss: 0.760451  [   64/  130]
train() client id: f_00008-0-2 loss: 0.624379  [   96/  130]
train() client id: f_00008-0-3 loss: 0.735761  [  128/  130]
train() client id: f_00008-1-0 loss: 0.809707  [   32/  130]
train() client id: f_00008-1-1 loss: 0.696586  [   64/  130]
train() client id: f_00008-1-2 loss: 0.687799  [   96/  130]
train() client id: f_00008-1-3 loss: 0.718084  [  128/  130]
train() client id: f_00008-2-0 loss: 0.694739  [   32/  130]
train() client id: f_00008-2-1 loss: 0.585584  [   64/  130]
train() client id: f_00008-2-2 loss: 0.881752  [   96/  130]
train() client id: f_00008-2-3 loss: 0.713771  [  128/  130]
train() client id: f_00008-3-0 loss: 0.672116  [   32/  130]
train() client id: f_00008-3-1 loss: 0.666606  [   64/  130]
train() client id: f_00008-3-2 loss: 0.963728  [   96/  130]
train() client id: f_00008-3-3 loss: 0.611295  [  128/  130]
train() client id: f_00008-4-0 loss: 0.616822  [   32/  130]
train() client id: f_00008-4-1 loss: 0.737214  [   64/  130]
train() client id: f_00008-4-2 loss: 0.719046  [   96/  130]
train() client id: f_00008-4-3 loss: 0.790091  [  128/  130]
train() client id: f_00008-5-0 loss: 0.732018  [   32/  130]
train() client id: f_00008-5-1 loss: 0.699574  [   64/  130]
train() client id: f_00008-5-2 loss: 0.670034  [   96/  130]
train() client id: f_00008-5-3 loss: 0.787345  [  128/  130]
train() client id: f_00008-6-0 loss: 0.700009  [   32/  130]
train() client id: f_00008-6-1 loss: 0.712383  [   64/  130]
train() client id: f_00008-6-2 loss: 0.773176  [   96/  130]
train() client id: f_00008-6-3 loss: 0.674959  [  128/  130]
train() client id: f_00008-7-0 loss: 0.745170  [   32/  130]
train() client id: f_00008-7-1 loss: 0.721158  [   64/  130]
train() client id: f_00008-7-2 loss: 0.637936  [   96/  130]
train() client id: f_00008-7-3 loss: 0.803569  [  128/  130]
train() client id: f_00008-8-0 loss: 0.857372  [   32/  130]
train() client id: f_00008-8-1 loss: 0.728626  [   64/  130]
train() client id: f_00008-8-2 loss: 0.679227  [   96/  130]
train() client id: f_00008-8-3 loss: 0.644261  [  128/  130]
train() client id: f_00008-9-0 loss: 0.575680  [   32/  130]
train() client id: f_00008-9-1 loss: 0.814967  [   64/  130]
train() client id: f_00008-9-2 loss: 0.831868  [   96/  130]
train() client id: f_00008-9-3 loss: 0.671848  [  128/  130]
train() client id: f_00008-10-0 loss: 0.778539  [   32/  130]
train() client id: f_00008-10-1 loss: 0.689069  [   64/  130]
train() client id: f_00008-10-2 loss: 0.666074  [   96/  130]
train() client id: f_00008-10-3 loss: 0.727331  [  128/  130]
train() client id: f_00008-11-0 loss: 0.695971  [   32/  130]
train() client id: f_00008-11-1 loss: 0.783595  [   64/  130]
train() client id: f_00008-11-2 loss: 0.721475  [   96/  130]
train() client id: f_00008-11-3 loss: 0.703055  [  128/  130]
train() client id: f_00008-12-0 loss: 0.739188  [   32/  130]
train() client id: f_00008-12-1 loss: 0.738998  [   64/  130]
train() client id: f_00008-12-2 loss: 0.661741  [   96/  130]
train() client id: f_00008-12-3 loss: 0.765764  [  128/  130]
train() client id: f_00009-0-0 loss: 1.018376  [   32/  118]
train() client id: f_00009-0-1 loss: 1.075215  [   64/  118]
train() client id: f_00009-0-2 loss: 1.080154  [   96/  118]
train() client id: f_00009-1-0 loss: 1.144484  [   32/  118]
train() client id: f_00009-1-1 loss: 1.061566  [   64/  118]
train() client id: f_00009-1-2 loss: 0.990003  [   96/  118]
train() client id: f_00009-2-0 loss: 0.971485  [   32/  118]
train() client id: f_00009-2-1 loss: 1.082580  [   64/  118]
train() client id: f_00009-2-2 loss: 1.116486  [   96/  118]
train() client id: f_00009-3-0 loss: 0.972204  [   32/  118]
train() client id: f_00009-3-1 loss: 0.891860  [   64/  118]
train() client id: f_00009-3-2 loss: 1.036245  [   96/  118]
train() client id: f_00009-4-0 loss: 0.980957  [   32/  118]
train() client id: f_00009-4-1 loss: 0.978593  [   64/  118]
train() client id: f_00009-4-2 loss: 0.761600  [   96/  118]
train() client id: f_00009-5-0 loss: 0.781777  [   32/  118]
train() client id: f_00009-5-1 loss: 1.035761  [   64/  118]
train() client id: f_00009-5-2 loss: 0.985219  [   96/  118]
train() client id: f_00009-6-0 loss: 0.948663  [   32/  118]
train() client id: f_00009-6-1 loss: 0.883044  [   64/  118]
train() client id: f_00009-6-2 loss: 0.853075  [   96/  118]
train() client id: f_00009-7-0 loss: 0.906821  [   32/  118]
train() client id: f_00009-7-1 loss: 0.853757  [   64/  118]
train() client id: f_00009-7-2 loss: 0.743494  [   96/  118]
train() client id: f_00009-8-0 loss: 0.841049  [   32/  118]
train() client id: f_00009-8-1 loss: 0.883038  [   64/  118]
train() client id: f_00009-8-2 loss: 0.726504  [   96/  118]
train() client id: f_00009-9-0 loss: 0.867626  [   32/  118]
train() client id: f_00009-9-1 loss: 0.814814  [   64/  118]
train() client id: f_00009-9-2 loss: 0.867275  [   96/  118]
train() client id: f_00009-10-0 loss: 0.831842  [   32/  118]
train() client id: f_00009-10-1 loss: 0.721555  [   64/  118]
train() client id: f_00009-10-2 loss: 0.845718  [   96/  118]
train() client id: f_00009-11-0 loss: 0.759676  [   32/  118]
train() client id: f_00009-11-1 loss: 0.852295  [   64/  118]
train() client id: f_00009-11-2 loss: 0.829484  [   96/  118]
train() client id: f_00009-12-0 loss: 0.970244  [   32/  118]
train() client id: f_00009-12-1 loss: 0.768146  [   64/  118]
train() client id: f_00009-12-2 loss: 0.776018  [   96/  118]
At round 43 accuracy: 0.6472148541114059
At round 43 training accuracy: 0.5861837692823608
At round 43 training loss: 0.8467957532860751
update_location
xs = [  -3.9056584     4.20031788  235.00902392   18.81129433    0.97929623
    3.95640986 -197.44319194 -176.32485185  219.66397685 -162.06087855]
ys = [ 227.5879595   210.55583871    1.32061395 -197.45517586  189.35018685
  172.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [248.61925403 233.13387545 255.40357347 222.13151795 214.1365272
 199.70073579 221.33843901 202.709471   241.99366252 190.47241326]
dists_bs = [178.99376305 182.69365504 445.27736169 419.86387774 176.56341796
 178.96975881 179.03723612 174.18269057 424.90489817 171.48735842]
uav_gains = [7.62245662e-12 1.01432215e-11 6.66462561e-12 1.21951248e-11
 1.38244935e-11 1.71271443e-11 1.23513336e-11 1.63936996e-11
 8.64631835e-12 1.95639647e-11]
bs_gains = [5.43685774e-11 5.13414801e-11 4.23768198e-12 4.99562983e-12
 5.64900616e-11 5.43889979e-11 5.43316212e-11 5.86786482e-11
 4.83144700e-12 6.12976995e-11]
Round 44
-------------------------------
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.18624546 10.67895549  5.11317553  1.85111052 12.31468286  5.9249859
  2.28997512  7.27266655  5.37158258  4.80445503]
obj_prev = 60.80783503027712
eta_min = 1.6905775058448652e-18	eta_max = 0.9354514077356543
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 14.082134182864616	eta = 0.9090909090909091
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 27.112785949573404	eta = 0.4721735416659381
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 20.558519671827195	eta = 0.622707294620243
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 19.371299283019138	eta = 0.6608715285021035
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 19.305821021572196	eta = 0.6631129622477986
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 19.305603441902093	eta = 0.6631204357204616
eta = 0.6631204357204616
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [0.03389812 0.07129367 0.03336005 0.01156841 0.08232401 0.03927877
 0.01452777 0.04815685 0.03497425 0.03174586]
ene_total = [1.77121153 3.05330088 1.77028457 0.8430904  3.47724339 1.80411205
 0.95686313 2.2367863  1.88825588 1.50445532]
ti_comp = [0.59818702 0.64270661 0.59304455 0.61337277 0.644077   0.64353969
 0.6137322  0.62114382 0.5788735  0.64520803]
ti_coms = [0.11613574 0.07161615 0.12127821 0.10094999 0.07024576 0.07078307
 0.10059056 0.09317894 0.13544926 0.06911473]
t_total = [27.79981537 27.79981537 27.79981537 27.79981537 27.79981537 27.79981537
 27.79981537 27.79981537 27.79981537 27.79981537]
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [6.80350798e-06 5.48286200e-05 6.59759497e-06 2.57188538e-07
 8.40589437e-05 9.14540875e-06 5.08766715e-07 1.80913087e-05
 7.97917407e-06 4.80332220e-06]
ene_total = [0.45223865 0.28084755 0.47224396 0.39288439 0.2766519  0.27582751
 0.39149537 0.36333529 0.52744819 0.26916572]
optimize_network iter = 0 obj = 3.7021385292782734
eta = 0.6631204357204616
freqs = [28334048.79448884 55463616.087887   28126095.25989743  9430159.49291534
 63908512.8336163  30517755.77420575 11835590.3817379  38764656.20847187
 30208889.06833503 24601258.06337142]
eta_min = 0.6631204357204632	eta_max = 0.6660036163441293
af = 0.006161926359054621	bf = 1.2446435035998953	zeta = 0.006778118994960084	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [1.57849488e-06 1.27208928e-05 1.53072061e-06 5.96708039e-08
 1.95026760e-05 2.12184375e-06 1.18039937e-07 4.19739907e-06
 1.85126341e-06 1.11442796e-06]
ene_total = [1.67509603 1.03465718 1.74925201 1.45587345 1.015872   1.02111426
 1.45069832 1.34439885 1.95366795 0.99690874]
ti_comp = [0.59207349 0.63659309 0.58693102 0.60725925 0.63796347 0.63742617
 0.60761867 0.6150303  0.57275998 0.63909451]
ti_coms = [0.11613574 0.07161615 0.12127821 0.10094999 0.07024576 0.07078307
 0.10059056 0.09317894 0.13544926 0.06911473]
t_total = [27.79981537 27.79981537 27.79981537 27.79981537 27.79981537 27.79981537
 27.79981537 27.79981537 27.79981537 27.79981537]
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [6.72701829e-06 5.41347322e-05 6.52458862e-06 2.54167082e-07
 8.29917378e-05 9.02944383e-06 5.02783784e-07 1.78742687e-05
 7.89490544e-06 4.74218027e-06]
ene_total = [0.45613954 0.28324469 0.47631768 0.39627579 0.27899817 0.278204
 0.39487466 0.36646322 0.53199801 0.27148686]
optimize_network iter = 1 obj = 3.734002631064773
eta = 0.6660036163441293
freqs = [28324287.6160973  55404880.64581718 28118923.39438503  9424501.49550652
 63839529.8841224  30485058.77113441 11828419.4116198  38736518.87654436
 30208889.06833503 24574290.61426852]
eta_min = 0.6660036163441323	eta_max = 0.6660036163441293
af = 0.006150348470560822	bf = 1.2446435035998953	zeta = 0.006765383317616905	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [1.57740747e-06 1.26939645e-05 1.52994007e-06 5.95992217e-08
 1.94605963e-05 2.11729946e-06 1.17896944e-07 4.19130791e-06
 1.85126341e-06 1.11198606e-06]
ene_total = [1.67509587 1.0346533  1.74925189 1.45587344 1.01586593 1.0211136
 1.4506983  1.34439798 1.95366795 0.99690839]
ti_comp = [0.59207349 0.63659309 0.58693102 0.60725925 0.63796347 0.63742617
 0.60761867 0.6150303  0.57275998 0.63909451]
ti_coms = [0.11613574 0.07161615 0.12127821 0.10094999 0.07024576 0.07078307
 0.10059056 0.09317894 0.13544926 0.06911473]
t_total = [27.79981537 27.79981537 27.79981537 27.79981537 27.79981537 27.79981537
 27.79981537 27.79981537 27.79981537 27.79981537]
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [6.72701829e-06 5.41347322e-05 6.52458862e-06 2.54167082e-07
 8.29917378e-05 9.02944383e-06 5.02783784e-07 1.78742687e-05
 7.89490544e-06 4.74218027e-06]
ene_total = [0.45613954 0.28324469 0.47631768 0.39627579 0.27899817 0.278204
 0.39487466 0.36646322 0.53199801 0.27148686]
optimize_network iter = 2 obj = 3.734002631064773
eta = 0.6660036163441293
freqs = [28324287.6160973  55404880.64581718 28118923.39438503  9424501.49550652
 63839529.8841224  30485058.77113441 11828419.4116198  38736518.87654436
 30208889.06833503 24574290.61426852]
Done!
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [6.57055255e-06 5.28755963e-05 6.37283125e-06 2.48255333e-07
 8.10614081e-05 8.81942529e-06 4.91089385e-07 1.74585257e-05
 7.71127547e-06 4.63188047e-06]
ene_total = [0.01162015 0.00721449 0.01213419 0.01009525 0.00710564 0.00708713
 0.01005955 0.00933535 0.01355264 0.0069161 ]
At round 44 energy consumption: 0.09512048272349369
At round 44 eta: 0.6660036163441293
At round 44 a_n: 12.768039742635098
At round 44 local rounds: 13.309571309784454
At round 44 global rounds: 39.25367528264245
gradient difference: 0.4115578532218933
train() client id: f_00000-0-0 loss: 1.421867  [   32/  126]
train() client id: f_00000-0-1 loss: 1.172125  [   64/  126]
train() client id: f_00000-0-2 loss: 1.127825  [   96/  126]
train() client id: f_00000-1-0 loss: 0.967714  [   32/  126]
train() client id: f_00000-1-1 loss: 1.024290  [   64/  126]
train() client id: f_00000-1-2 loss: 1.147430  [   96/  126]
train() client id: f_00000-2-0 loss: 1.150469  [   32/  126]
train() client id: f_00000-2-1 loss: 0.996487  [   64/  126]
train() client id: f_00000-2-2 loss: 0.859555  [   96/  126]
train() client id: f_00000-3-0 loss: 0.900069  [   32/  126]
train() client id: f_00000-3-1 loss: 0.983769  [   64/  126]
train() client id: f_00000-3-2 loss: 1.014735  [   96/  126]
train() client id: f_00000-4-0 loss: 0.997843  [   32/  126]
train() client id: f_00000-4-1 loss: 0.896848  [   64/  126]
train() client id: f_00000-4-2 loss: 0.902717  [   96/  126]
train() client id: f_00000-5-0 loss: 0.956305  [   32/  126]
train() client id: f_00000-5-1 loss: 0.889880  [   64/  126]
train() client id: f_00000-5-2 loss: 0.885419  [   96/  126]
train() client id: f_00000-6-0 loss: 0.922398  [   32/  126]
train() client id: f_00000-6-1 loss: 0.909178  [   64/  126]
train() client id: f_00000-6-2 loss: 0.885434  [   96/  126]
train() client id: f_00000-7-0 loss: 0.883739  [   32/  126]
train() client id: f_00000-7-1 loss: 0.857888  [   64/  126]
train() client id: f_00000-7-2 loss: 0.866908  [   96/  126]
train() client id: f_00000-8-0 loss: 0.888783  [   32/  126]
train() client id: f_00000-8-1 loss: 0.824107  [   64/  126]
train() client id: f_00000-8-2 loss: 0.852130  [   96/  126]
train() client id: f_00000-9-0 loss: 0.995364  [   32/  126]
train() client id: f_00000-9-1 loss: 0.781230  [   64/  126]
train() client id: f_00000-9-2 loss: 0.777336  [   96/  126]
train() client id: f_00000-10-0 loss: 0.876456  [   32/  126]
train() client id: f_00000-10-1 loss: 0.846987  [   64/  126]
train() client id: f_00000-10-2 loss: 0.836757  [   96/  126]
train() client id: f_00000-11-0 loss: 0.844288  [   32/  126]
train() client id: f_00000-11-1 loss: 0.872147  [   64/  126]
train() client id: f_00000-11-2 loss: 0.793770  [   96/  126]
train() client id: f_00000-12-0 loss: 0.897104  [   32/  126]
train() client id: f_00000-12-1 loss: 0.903313  [   64/  126]
train() client id: f_00000-12-2 loss: 0.772410  [   96/  126]
train() client id: f_00001-0-0 loss: 0.577200  [   32/  265]
train() client id: f_00001-0-1 loss: 0.423390  [   64/  265]
train() client id: f_00001-0-2 loss: 0.483632  [   96/  265]
train() client id: f_00001-0-3 loss: 0.361135  [  128/  265]
train() client id: f_00001-0-4 loss: 0.448369  [  160/  265]
train() client id: f_00001-0-5 loss: 0.376625  [  192/  265]
train() client id: f_00001-0-6 loss: 0.460911  [  224/  265]
train() client id: f_00001-0-7 loss: 0.334200  [  256/  265]
train() client id: f_00001-1-0 loss: 0.410760  [   32/  265]
train() client id: f_00001-1-1 loss: 0.468345  [   64/  265]
train() client id: f_00001-1-2 loss: 0.395943  [   96/  265]
train() client id: f_00001-1-3 loss: 0.373755  [  128/  265]
train() client id: f_00001-1-4 loss: 0.493954  [  160/  265]
train() client id: f_00001-1-5 loss: 0.324714  [  192/  265]
train() client id: f_00001-1-6 loss: 0.430461  [  224/  265]
train() client id: f_00001-1-7 loss: 0.444981  [  256/  265]
train() client id: f_00001-2-0 loss: 0.342570  [   32/  265]
train() client id: f_00001-2-1 loss: 0.323399  [   64/  265]
train() client id: f_00001-2-2 loss: 0.461352  [   96/  265]
train() client id: f_00001-2-3 loss: 0.335570  [  128/  265]
train() client id: f_00001-2-4 loss: 0.600527  [  160/  265]
train() client id: f_00001-2-5 loss: 0.425825  [  192/  265]
train() client id: f_00001-2-6 loss: 0.445669  [  224/  265]
train() client id: f_00001-2-7 loss: 0.442875  [  256/  265]
train() client id: f_00001-3-0 loss: 0.452538  [   32/  265]
train() client id: f_00001-3-1 loss: 0.363815  [   64/  265]
train() client id: f_00001-3-2 loss: 0.384208  [   96/  265]
train() client id: f_00001-3-3 loss: 0.371599  [  128/  265]
train() client id: f_00001-3-4 loss: 0.538617  [  160/  265]
train() client id: f_00001-3-5 loss: 0.335508  [  192/  265]
train() client id: f_00001-3-6 loss: 0.348078  [  224/  265]
train() client id: f_00001-3-7 loss: 0.543116  [  256/  265]
train() client id: f_00001-4-0 loss: 0.405787  [   32/  265]
train() client id: f_00001-4-1 loss: 0.327812  [   64/  265]
train() client id: f_00001-4-2 loss: 0.458326  [   96/  265]
train() client id: f_00001-4-3 loss: 0.458101  [  128/  265]
train() client id: f_00001-4-4 loss: 0.555231  [  160/  265]
train() client id: f_00001-4-5 loss: 0.366096  [  192/  265]
train() client id: f_00001-4-6 loss: 0.326109  [  224/  265]
train() client id: f_00001-4-7 loss: 0.407149  [  256/  265]
train() client id: f_00001-5-0 loss: 0.345951  [   32/  265]
train() client id: f_00001-5-1 loss: 0.418936  [   64/  265]
train() client id: f_00001-5-2 loss: 0.343141  [   96/  265]
train() client id: f_00001-5-3 loss: 0.607858  [  128/  265]
train() client id: f_00001-5-4 loss: 0.355907  [  160/  265]
train() client id: f_00001-5-5 loss: 0.406581  [  192/  265]
train() client id: f_00001-5-6 loss: 0.333302  [  224/  265]
train() client id: f_00001-5-7 loss: 0.459177  [  256/  265]
train() client id: f_00001-6-0 loss: 0.322897  [   32/  265]
train() client id: f_00001-6-1 loss: 0.525497  [   64/  265]
train() client id: f_00001-6-2 loss: 0.405509  [   96/  265]
train() client id: f_00001-6-3 loss: 0.366024  [  128/  265]
train() client id: f_00001-6-4 loss: 0.412420  [  160/  265]
train() client id: f_00001-6-5 loss: 0.309209  [  192/  265]
train() client id: f_00001-6-6 loss: 0.538194  [  224/  265]
train() client id: f_00001-6-7 loss: 0.383834  [  256/  265]
train() client id: f_00001-7-0 loss: 0.352258  [   32/  265]
train() client id: f_00001-7-1 loss: 0.544278  [   64/  265]
train() client id: f_00001-7-2 loss: 0.446622  [   96/  265]
train() client id: f_00001-7-3 loss: 0.379455  [  128/  265]
train() client id: f_00001-7-4 loss: 0.329182  [  160/  265]
train() client id: f_00001-7-5 loss: 0.379076  [  192/  265]
train() client id: f_00001-7-6 loss: 0.405263  [  224/  265]
train() client id: f_00001-7-7 loss: 0.364472  [  256/  265]
train() client id: f_00001-8-0 loss: 0.471215  [   32/  265]
train() client id: f_00001-8-1 loss: 0.374952  [   64/  265]
train() client id: f_00001-8-2 loss: 0.400273  [   96/  265]
train() client id: f_00001-8-3 loss: 0.350320  [  128/  265]
train() client id: f_00001-8-4 loss: 0.364950  [  160/  265]
train() client id: f_00001-8-5 loss: 0.306206  [  192/  265]
train() client id: f_00001-8-6 loss: 0.361179  [  224/  265]
train() client id: f_00001-8-7 loss: 0.502474  [  256/  265]
train() client id: f_00001-9-0 loss: 0.415696  [   32/  265]
train() client id: f_00001-9-1 loss: 0.366265  [   64/  265]
train() client id: f_00001-9-2 loss: 0.440045  [   96/  265]
train() client id: f_00001-9-3 loss: 0.407858  [  128/  265]
train() client id: f_00001-9-4 loss: 0.338596  [  160/  265]
train() client id: f_00001-9-5 loss: 0.396622  [  192/  265]
train() client id: f_00001-9-6 loss: 0.476339  [  224/  265]
train() client id: f_00001-9-7 loss: 0.378662  [  256/  265]
train() client id: f_00001-10-0 loss: 0.426416  [   32/  265]
train() client id: f_00001-10-1 loss: 0.517213  [   64/  265]
train() client id: f_00001-10-2 loss: 0.309985  [   96/  265]
train() client id: f_00001-10-3 loss: 0.402211  [  128/  265]
train() client id: f_00001-10-4 loss: 0.415004  [  160/  265]
train() client id: f_00001-10-5 loss: 0.328767  [  192/  265]
train() client id: f_00001-10-6 loss: 0.379707  [  224/  265]
train() client id: f_00001-10-7 loss: 0.427623  [  256/  265]
train() client id: f_00001-11-0 loss: 0.355764  [   32/  265]
train() client id: f_00001-11-1 loss: 0.316974  [   64/  265]
train() client id: f_00001-11-2 loss: 0.438844  [   96/  265]
train() client id: f_00001-11-3 loss: 0.296655  [  128/  265]
train() client id: f_00001-11-4 loss: 0.560145  [  160/  265]
train() client id: f_00001-11-5 loss: 0.318139  [  192/  265]
train() client id: f_00001-11-6 loss: 0.403378  [  224/  265]
train() client id: f_00001-11-7 loss: 0.415785  [  256/  265]
train() client id: f_00001-12-0 loss: 0.489234  [   32/  265]
train() client id: f_00001-12-1 loss: 0.401755  [   64/  265]
train() client id: f_00001-12-2 loss: 0.343128  [   96/  265]
train() client id: f_00001-12-3 loss: 0.390880  [  128/  265]
train() client id: f_00001-12-4 loss: 0.403862  [  160/  265]
train() client id: f_00001-12-5 loss: 0.401784  [  192/  265]
train() client id: f_00001-12-6 loss: 0.309109  [  224/  265]
train() client id: f_00001-12-7 loss: 0.460848  [  256/  265]
train() client id: f_00002-0-0 loss: 0.947429  [   32/  124]
train() client id: f_00002-0-1 loss: 1.250249  [   64/  124]
train() client id: f_00002-0-2 loss: 1.014899  [   96/  124]
train() client id: f_00002-1-0 loss: 0.948226  [   32/  124]
train() client id: f_00002-1-1 loss: 1.007108  [   64/  124]
train() client id: f_00002-1-2 loss: 0.967628  [   96/  124]
train() client id: f_00002-2-0 loss: 0.967350  [   32/  124]
train() client id: f_00002-2-1 loss: 0.901177  [   64/  124]
train() client id: f_00002-2-2 loss: 0.930170  [   96/  124]
train() client id: f_00002-3-0 loss: 0.959608  [   32/  124]
train() client id: f_00002-3-1 loss: 0.891753  [   64/  124]
train() client id: f_00002-3-2 loss: 0.906474  [   96/  124]
train() client id: f_00002-4-0 loss: 0.824375  [   32/  124]
train() client id: f_00002-4-1 loss: 0.960047  [   64/  124]
train() client id: f_00002-4-2 loss: 0.812832  [   96/  124]
train() client id: f_00002-5-0 loss: 0.837702  [   32/  124]
train() client id: f_00002-5-1 loss: 0.948266  [   64/  124]
train() client id: f_00002-5-2 loss: 0.743180  [   96/  124]
train() client id: f_00002-6-0 loss: 0.844790  [   32/  124]
train() client id: f_00002-6-1 loss: 0.856859  [   64/  124]
train() client id: f_00002-6-2 loss: 0.821211  [   96/  124]
train() client id: f_00002-7-0 loss: 0.693582  [   32/  124]
train() client id: f_00002-7-1 loss: 0.818985  [   64/  124]
train() client id: f_00002-7-2 loss: 0.967868  [   96/  124]
train() client id: f_00002-8-0 loss: 0.906228  [   32/  124]
train() client id: f_00002-8-1 loss: 0.859225  [   64/  124]
train() client id: f_00002-8-2 loss: 0.728329  [   96/  124]
train() client id: f_00002-9-0 loss: 0.868475  [   32/  124]
train() client id: f_00002-9-1 loss: 0.860192  [   64/  124]
train() client id: f_00002-9-2 loss: 0.728720  [   96/  124]
train() client id: f_00002-10-0 loss: 0.837423  [   32/  124]
train() client id: f_00002-10-1 loss: 0.789836  [   64/  124]
train() client id: f_00002-10-2 loss: 0.858299  [   96/  124]
train() client id: f_00002-11-0 loss: 0.877870  [   32/  124]
train() client id: f_00002-11-1 loss: 0.782638  [   64/  124]
train() client id: f_00002-11-2 loss: 0.834419  [   96/  124]
train() client id: f_00002-12-0 loss: 0.811603  [   32/  124]
train() client id: f_00002-12-1 loss: 0.833925  [   64/  124]
train() client id: f_00002-12-2 loss: 0.904194  [   96/  124]
train() client id: f_00003-0-0 loss: 0.766576  [   32/   43]
train() client id: f_00003-1-0 loss: 0.673756  [   32/   43]
train() client id: f_00003-2-0 loss: 0.813273  [   32/   43]
train() client id: f_00003-3-0 loss: 0.838871  [   32/   43]
train() client id: f_00003-4-0 loss: 0.719906  [   32/   43]
train() client id: f_00003-5-0 loss: 0.819369  [   32/   43]
train() client id: f_00003-6-0 loss: 0.685774  [   32/   43]
train() client id: f_00003-7-0 loss: 0.579826  [   32/   43]
train() client id: f_00003-8-0 loss: 0.679437  [   32/   43]
train() client id: f_00003-9-0 loss: 0.778896  [   32/   43]
train() client id: f_00003-10-0 loss: 0.782139  [   32/   43]
train() client id: f_00003-11-0 loss: 0.570014  [   32/   43]
train() client id: f_00003-12-0 loss: 0.750583  [   32/   43]
train() client id: f_00004-0-0 loss: 0.917634  [   32/  306]
train() client id: f_00004-0-1 loss: 0.843414  [   64/  306]
train() client id: f_00004-0-2 loss: 0.884046  [   96/  306]
train() client id: f_00004-0-3 loss: 0.970323  [  128/  306]
train() client id: f_00004-0-4 loss: 1.075154  [  160/  306]
train() client id: f_00004-0-5 loss: 0.871289  [  192/  306]
train() client id: f_00004-0-6 loss: 0.946971  [  224/  306]
train() client id: f_00004-0-7 loss: 0.877174  [  256/  306]
train() client id: f_00004-0-8 loss: 0.959102  [  288/  306]
train() client id: f_00004-1-0 loss: 1.018794  [   32/  306]
train() client id: f_00004-1-1 loss: 0.910129  [   64/  306]
train() client id: f_00004-1-2 loss: 0.937993  [   96/  306]
train() client id: f_00004-1-3 loss: 0.805090  [  128/  306]
train() client id: f_00004-1-4 loss: 0.940907  [  160/  306]
train() client id: f_00004-1-5 loss: 0.901798  [  192/  306]
train() client id: f_00004-1-6 loss: 0.854164  [  224/  306]
train() client id: f_00004-1-7 loss: 0.835083  [  256/  306]
train() client id: f_00004-1-8 loss: 0.999523  [  288/  306]
train() client id: f_00004-2-0 loss: 0.958837  [   32/  306]
train() client id: f_00004-2-1 loss: 0.844760  [   64/  306]
train() client id: f_00004-2-2 loss: 0.875218  [   96/  306]
train() client id: f_00004-2-3 loss: 0.958611  [  128/  306]
train() client id: f_00004-2-4 loss: 0.897102  [  160/  306]
train() client id: f_00004-2-5 loss: 0.879481  [  192/  306]
train() client id: f_00004-2-6 loss: 0.874418  [  224/  306]
train() client id: f_00004-2-7 loss: 0.952715  [  256/  306]
train() client id: f_00004-2-8 loss: 1.007592  [  288/  306]
train() client id: f_00004-3-0 loss: 0.950367  [   32/  306]
train() client id: f_00004-3-1 loss: 0.893993  [   64/  306]
train() client id: f_00004-3-2 loss: 0.871323  [   96/  306]
train() client id: f_00004-3-3 loss: 0.914709  [  128/  306]
train() client id: f_00004-3-4 loss: 0.845980  [  160/  306]
train() client id: f_00004-3-5 loss: 0.859665  [  192/  306]
train() client id: f_00004-3-6 loss: 0.951895  [  224/  306]
train() client id: f_00004-3-7 loss: 0.973518  [  256/  306]
train() client id: f_00004-3-8 loss: 0.968190  [  288/  306]
train() client id: f_00004-4-0 loss: 0.743104  [   32/  306]
train() client id: f_00004-4-1 loss: 0.908296  [   64/  306]
train() client id: f_00004-4-2 loss: 1.043383  [   96/  306]
train() client id: f_00004-4-3 loss: 0.827886  [  128/  306]
train() client id: f_00004-4-4 loss: 0.822565  [  160/  306]
train() client id: f_00004-4-5 loss: 0.921336  [  192/  306]
train() client id: f_00004-4-6 loss: 0.923993  [  224/  306]
train() client id: f_00004-4-7 loss: 1.001339  [  256/  306]
train() client id: f_00004-4-8 loss: 0.933323  [  288/  306]
train() client id: f_00004-5-0 loss: 0.933095  [   32/  306]
train() client id: f_00004-5-1 loss: 0.909925  [   64/  306]
train() client id: f_00004-5-2 loss: 0.859810  [   96/  306]
train() client id: f_00004-5-3 loss: 1.029885  [  128/  306]
train() client id: f_00004-5-4 loss: 0.824205  [  160/  306]
train() client id: f_00004-5-5 loss: 0.935865  [  192/  306]
train() client id: f_00004-5-6 loss: 0.949661  [  224/  306]
train() client id: f_00004-5-7 loss: 0.928994  [  256/  306]
train() client id: f_00004-5-8 loss: 0.817207  [  288/  306]
train() client id: f_00004-6-0 loss: 0.932170  [   32/  306]
train() client id: f_00004-6-1 loss: 0.913078  [   64/  306]
train() client id: f_00004-6-2 loss: 0.926213  [   96/  306]
train() client id: f_00004-6-3 loss: 0.965236  [  128/  306]
train() client id: f_00004-6-4 loss: 0.926686  [  160/  306]
train() client id: f_00004-6-5 loss: 0.902841  [  192/  306]
train() client id: f_00004-6-6 loss: 0.719488  [  224/  306]
train() client id: f_00004-6-7 loss: 0.978710  [  256/  306]
train() client id: f_00004-6-8 loss: 0.890707  [  288/  306]
train() client id: f_00004-7-0 loss: 0.897938  [   32/  306]
train() client id: f_00004-7-1 loss: 1.024438  [   64/  306]
train() client id: f_00004-7-2 loss: 0.918601  [   96/  306]
train() client id: f_00004-7-3 loss: 0.864841  [  128/  306]
train() client id: f_00004-7-4 loss: 0.837555  [  160/  306]
train() client id: f_00004-7-5 loss: 0.881396  [  192/  306]
train() client id: f_00004-7-6 loss: 0.956918  [  224/  306]
train() client id: f_00004-7-7 loss: 0.821731  [  256/  306]
train() client id: f_00004-7-8 loss: 0.931902  [  288/  306]
train() client id: f_00004-8-0 loss: 0.800016  [   32/  306]
train() client id: f_00004-8-1 loss: 1.064864  [   64/  306]
train() client id: f_00004-8-2 loss: 0.914593  [   96/  306]
train() client id: f_00004-8-3 loss: 0.839317  [  128/  306]
train() client id: f_00004-8-4 loss: 0.991365  [  160/  306]
train() client id: f_00004-8-5 loss: 0.774601  [  192/  306]
train() client id: f_00004-8-6 loss: 0.873366  [  224/  306]
train() client id: f_00004-8-7 loss: 1.000695  [  256/  306]
train() client id: f_00004-8-8 loss: 0.837861  [  288/  306]
train() client id: f_00004-9-0 loss: 0.901379  [   32/  306]
train() client id: f_00004-9-1 loss: 0.913672  [   64/  306]
train() client id: f_00004-9-2 loss: 0.901003  [   96/  306]
train() client id: f_00004-9-3 loss: 0.972861  [  128/  306]
train() client id: f_00004-9-4 loss: 0.813882  [  160/  306]
train() client id: f_00004-9-5 loss: 0.894410  [  192/  306]
train() client id: f_00004-9-6 loss: 0.941090  [  224/  306]
train() client id: f_00004-9-7 loss: 0.888088  [  256/  306]
train() client id: f_00004-9-8 loss: 0.981976  [  288/  306]
train() client id: f_00004-10-0 loss: 0.957598  [   32/  306]
train() client id: f_00004-10-1 loss: 0.840571  [   64/  306]
train() client id: f_00004-10-2 loss: 0.945660  [   96/  306]
train() client id: f_00004-10-3 loss: 0.859655  [  128/  306]
train() client id: f_00004-10-4 loss: 0.810264  [  160/  306]
train() client id: f_00004-10-5 loss: 0.900722  [  192/  306]
train() client id: f_00004-10-6 loss: 0.857922  [  224/  306]
train() client id: f_00004-10-7 loss: 0.953933  [  256/  306]
train() client id: f_00004-10-8 loss: 0.955211  [  288/  306]
train() client id: f_00004-11-0 loss: 0.817794  [   32/  306]
train() client id: f_00004-11-1 loss: 0.985206  [   64/  306]
train() client id: f_00004-11-2 loss: 0.873901  [   96/  306]
train() client id: f_00004-11-3 loss: 0.828491  [  128/  306]
train() client id: f_00004-11-4 loss: 0.884412  [  160/  306]
train() client id: f_00004-11-5 loss: 0.820068  [  192/  306]
train() client id: f_00004-11-6 loss: 0.868576  [  224/  306]
train() client id: f_00004-11-7 loss: 0.901445  [  256/  306]
train() client id: f_00004-11-8 loss: 1.017074  [  288/  306]
train() client id: f_00004-12-0 loss: 0.924549  [   32/  306]
train() client id: f_00004-12-1 loss: 0.844082  [   64/  306]
train() client id: f_00004-12-2 loss: 0.914441  [   96/  306]
train() client id: f_00004-12-3 loss: 0.943871  [  128/  306]
train() client id: f_00004-12-4 loss: 0.918910  [  160/  306]
train() client id: f_00004-12-5 loss: 0.864317  [  192/  306]
train() client id: f_00004-12-6 loss: 0.847351  [  224/  306]
train() client id: f_00004-12-7 loss: 0.950424  [  256/  306]
train() client id: f_00004-12-8 loss: 0.844998  [  288/  306]
train() client id: f_00005-0-0 loss: 0.578648  [   32/  146]
train() client id: f_00005-0-1 loss: 0.613807  [   64/  146]
train() client id: f_00005-0-2 loss: 0.400692  [   96/  146]
train() client id: f_00005-0-3 loss: 0.201680  [  128/  146]
train() client id: f_00005-1-0 loss: 0.335640  [   32/  146]
train() client id: f_00005-1-1 loss: 0.462637  [   64/  146]
train() client id: f_00005-1-2 loss: 0.486395  [   96/  146]
train() client id: f_00005-1-3 loss: 0.476221  [  128/  146]
train() client id: f_00005-2-0 loss: 0.364049  [   32/  146]
train() client id: f_00005-2-1 loss: 0.519354  [   64/  146]
train() client id: f_00005-2-2 loss: 0.550304  [   96/  146]
train() client id: f_00005-2-3 loss: 0.319941  [  128/  146]
train() client id: f_00005-3-0 loss: 0.321618  [   32/  146]
train() client id: f_00005-3-1 loss: 0.687180  [   64/  146]
train() client id: f_00005-3-2 loss: 0.292557  [   96/  146]
train() client id: f_00005-3-3 loss: 0.383533  [  128/  146]
train() client id: f_00005-4-0 loss: 0.643577  [   32/  146]
train() client id: f_00005-4-1 loss: 0.343691  [   64/  146]
train() client id: f_00005-4-2 loss: 0.304586  [   96/  146]
train() client id: f_00005-4-3 loss: 0.485681  [  128/  146]
train() client id: f_00005-5-0 loss: 0.179091  [   32/  146]
train() client id: f_00005-5-1 loss: 0.299386  [   64/  146]
train() client id: f_00005-5-2 loss: 0.555035  [   96/  146]
train() client id: f_00005-5-3 loss: 0.698164  [  128/  146]
train() client id: f_00005-6-0 loss: 0.494685  [   32/  146]
train() client id: f_00005-6-1 loss: 0.254760  [   64/  146]
train() client id: f_00005-6-2 loss: 0.322368  [   96/  146]
train() client id: f_00005-6-3 loss: 0.533662  [  128/  146]
train() client id: f_00005-7-0 loss: 0.433550  [   32/  146]
train() client id: f_00005-7-1 loss: 0.461076  [   64/  146]
train() client id: f_00005-7-2 loss: 0.286856  [   96/  146]
train() client id: f_00005-7-3 loss: 0.583888  [  128/  146]
train() client id: f_00005-8-0 loss: 0.471214  [   32/  146]
train() client id: f_00005-8-1 loss: 0.501316  [   64/  146]
train() client id: f_00005-8-2 loss: 0.313588  [   96/  146]
train() client id: f_00005-8-3 loss: 0.469340  [  128/  146]
train() client id: f_00005-9-0 loss: 0.414374  [   32/  146]
train() client id: f_00005-9-1 loss: 0.577288  [   64/  146]
train() client id: f_00005-9-2 loss: 0.419213  [   96/  146]
train() client id: f_00005-9-3 loss: 0.348630  [  128/  146]
train() client id: f_00005-10-0 loss: 0.556687  [   32/  146]
train() client id: f_00005-10-1 loss: 0.404354  [   64/  146]
train() client id: f_00005-10-2 loss: 0.472992  [   96/  146]
train() client id: f_00005-10-3 loss: 0.325735  [  128/  146]
train() client id: f_00005-11-0 loss: 0.491958  [   32/  146]
train() client id: f_00005-11-1 loss: 0.495739  [   64/  146]
train() client id: f_00005-11-2 loss: 0.242705  [   96/  146]
train() client id: f_00005-11-3 loss: 0.142273  [  128/  146]
train() client id: f_00005-12-0 loss: 0.308753  [   32/  146]
train() client id: f_00005-12-1 loss: 0.398825  [   64/  146]
train() client id: f_00005-12-2 loss: 0.628075  [   96/  146]
train() client id: f_00005-12-3 loss: 0.335600  [  128/  146]
train() client id: f_00006-0-0 loss: 0.477930  [   32/   54]
train() client id: f_00006-1-0 loss: 0.521778  [   32/   54]
train() client id: f_00006-2-0 loss: 0.473241  [   32/   54]
train() client id: f_00006-3-0 loss: 0.488015  [   32/   54]
train() client id: f_00006-4-0 loss: 0.472884  [   32/   54]
train() client id: f_00006-5-0 loss: 0.531968  [   32/   54]
train() client id: f_00006-6-0 loss: 0.471271  [   32/   54]
train() client id: f_00006-7-0 loss: 0.474453  [   32/   54]
train() client id: f_00006-8-0 loss: 0.476787  [   32/   54]
train() client id: f_00006-9-0 loss: 0.490970  [   32/   54]
train() client id: f_00006-10-0 loss: 0.471843  [   32/   54]
train() client id: f_00006-11-0 loss: 0.500534  [   32/   54]
train() client id: f_00006-12-0 loss: 0.461142  [   32/   54]
train() client id: f_00007-0-0 loss: 0.586973  [   32/  179]
train() client id: f_00007-0-1 loss: 0.533036  [   64/  179]
train() client id: f_00007-0-2 loss: 0.579787  [   96/  179]
train() client id: f_00007-0-3 loss: 0.573340  [  128/  179]
train() client id: f_00007-0-4 loss: 0.725561  [  160/  179]
train() client id: f_00007-1-0 loss: 0.563583  [   32/  179]
train() client id: f_00007-1-1 loss: 0.516734  [   64/  179]
train() client id: f_00007-1-2 loss: 0.615893  [   96/  179]
train() client id: f_00007-1-3 loss: 0.476432  [  128/  179]
train() client id: f_00007-1-4 loss: 0.691296  [  160/  179]
train() client id: f_00007-2-0 loss: 0.608307  [   32/  179]
train() client id: f_00007-2-1 loss: 0.401631  [   64/  179]
train() client id: f_00007-2-2 loss: 0.581342  [   96/  179]
train() client id: f_00007-2-3 loss: 0.471086  [  128/  179]
train() client id: f_00007-2-4 loss: 0.556805  [  160/  179]
train() client id: f_00007-3-0 loss: 0.504326  [   32/  179]
train() client id: f_00007-3-1 loss: 0.479392  [   64/  179]
train() client id: f_00007-3-2 loss: 0.656801  [   96/  179]
train() client id: f_00007-3-3 loss: 0.656842  [  128/  179]
train() client id: f_00007-3-4 loss: 0.429191  [  160/  179]
train() client id: f_00007-4-0 loss: 0.481437  [   32/  179]
train() client id: f_00007-4-1 loss: 0.409523  [   64/  179]
train() client id: f_00007-4-2 loss: 0.633963  [   96/  179]
train() client id: f_00007-4-3 loss: 0.692847  [  128/  179]
train() client id: f_00007-4-4 loss: 0.614195  [  160/  179]
train() client id: f_00007-5-0 loss: 0.519164  [   32/  179]
train() client id: f_00007-5-1 loss: 0.477017  [   64/  179]
train() client id: f_00007-5-2 loss: 0.584620  [   96/  179]
train() client id: f_00007-5-3 loss: 0.613289  [  128/  179]
train() client id: f_00007-5-4 loss: 0.531695  [  160/  179]
train() client id: f_00007-6-0 loss: 0.607403  [   32/  179]
train() client id: f_00007-6-1 loss: 0.587982  [   64/  179]
train() client id: f_00007-6-2 loss: 0.496913  [   96/  179]
train() client id: f_00007-6-3 loss: 0.559927  [  128/  179]
train() client id: f_00007-6-4 loss: 0.451511  [  160/  179]
train() client id: f_00007-7-0 loss: 0.587101  [   32/  179]
train() client id: f_00007-7-1 loss: 0.493452  [   64/  179]
train() client id: f_00007-7-2 loss: 0.593711  [   96/  179]
train() client id: f_00007-7-3 loss: 0.556172  [  128/  179]
train() client id: f_00007-7-4 loss: 0.476875  [  160/  179]
train() client id: f_00007-8-0 loss: 0.675929  [   32/  179]
train() client id: f_00007-8-1 loss: 0.598938  [   64/  179]
train() client id: f_00007-8-2 loss: 0.549593  [   96/  179]
train() client id: f_00007-8-3 loss: 0.579112  [  128/  179]
train() client id: f_00007-8-4 loss: 0.385466  [  160/  179]
train() client id: f_00007-9-0 loss: 0.492945  [   32/  179]
train() client id: f_00007-9-1 loss: 0.560522  [   64/  179]
train() client id: f_00007-9-2 loss: 0.533964  [   96/  179]
train() client id: f_00007-9-3 loss: 0.342279  [  128/  179]
train() client id: f_00007-9-4 loss: 0.662226  [  160/  179]
train() client id: f_00007-10-0 loss: 0.419302  [   32/  179]
train() client id: f_00007-10-1 loss: 0.565592  [   64/  179]
train() client id: f_00007-10-2 loss: 0.555322  [   96/  179]
train() client id: f_00007-10-3 loss: 0.441170  [  128/  179]
train() client id: f_00007-10-4 loss: 0.691176  [  160/  179]
train() client id: f_00007-11-0 loss: 0.564421  [   32/  179]
train() client id: f_00007-11-1 loss: 0.659638  [   64/  179]
train() client id: f_00007-11-2 loss: 0.500011  [   96/  179]
train() client id: f_00007-11-3 loss: 0.588246  [  128/  179]
train() client id: f_00007-11-4 loss: 0.471298  [  160/  179]
train() client id: f_00007-12-0 loss: 0.473124  [   32/  179]
train() client id: f_00007-12-1 loss: 0.390498  [   64/  179]
train() client id: f_00007-12-2 loss: 0.657355  [   96/  179]
train() client id: f_00007-12-3 loss: 0.651628  [  128/  179]
train() client id: f_00007-12-4 loss: 0.491555  [  160/  179]
train() client id: f_00008-0-0 loss: 0.739389  [   32/  130]
train() client id: f_00008-0-1 loss: 0.725980  [   64/  130]
train() client id: f_00008-0-2 loss: 0.633510  [   96/  130]
train() client id: f_00008-0-3 loss: 0.663906  [  128/  130]
train() client id: f_00008-1-0 loss: 0.779125  [   32/  130]
train() client id: f_00008-1-1 loss: 0.649588  [   64/  130]
train() client id: f_00008-1-2 loss: 0.666702  [   96/  130]
train() client id: f_00008-1-3 loss: 0.635914  [  128/  130]
train() client id: f_00008-2-0 loss: 0.660432  [   32/  130]
train() client id: f_00008-2-1 loss: 0.648572  [   64/  130]
train() client id: f_00008-2-2 loss: 0.894782  [   96/  130]
train() client id: f_00008-2-3 loss: 0.552003  [  128/  130]
train() client id: f_00008-3-0 loss: 0.827545  [   32/  130]
train() client id: f_00008-3-1 loss: 0.622428  [   64/  130]
train() client id: f_00008-3-2 loss: 0.581347  [   96/  130]
train() client id: f_00008-3-3 loss: 0.698311  [  128/  130]
train() client id: f_00008-4-0 loss: 0.745498  [   32/  130]
train() client id: f_00008-4-1 loss: 0.783267  [   64/  130]
train() client id: f_00008-4-2 loss: 0.631675  [   96/  130]
train() client id: f_00008-4-3 loss: 0.599024  [  128/  130]
train() client id: f_00008-5-0 loss: 0.691877  [   32/  130]
train() client id: f_00008-5-1 loss: 0.659124  [   64/  130]
train() client id: f_00008-5-2 loss: 0.641146  [   96/  130]
train() client id: f_00008-5-3 loss: 0.741695  [  128/  130]
train() client id: f_00008-6-0 loss: 0.657596  [   32/  130]
train() client id: f_00008-6-1 loss: 0.751943  [   64/  130]
train() client id: f_00008-6-2 loss: 0.728227  [   96/  130]
train() client id: f_00008-6-3 loss: 0.613824  [  128/  130]
train() client id: f_00008-7-0 loss: 0.683761  [   32/  130]
train() client id: f_00008-7-1 loss: 0.722430  [   64/  130]
train() client id: f_00008-7-2 loss: 0.708519  [   96/  130]
train() client id: f_00008-7-3 loss: 0.603470  [  128/  130]
train() client id: f_00008-8-0 loss: 0.708453  [   32/  130]
train() client id: f_00008-8-1 loss: 0.557912  [   64/  130]
train() client id: f_00008-8-2 loss: 0.543032  [   96/  130]
train() client id: f_00008-8-3 loss: 0.938118  [  128/  130]
train() client id: f_00008-9-0 loss: 0.670158  [   32/  130]
train() client id: f_00008-9-1 loss: 0.684628  [   64/  130]
train() client id: f_00008-9-2 loss: 0.694239  [   96/  130]
train() client id: f_00008-9-3 loss: 0.671511  [  128/  130]
train() client id: f_00008-10-0 loss: 0.659157  [   32/  130]
train() client id: f_00008-10-1 loss: 0.835114  [   64/  130]
train() client id: f_00008-10-2 loss: 0.570251  [   96/  130]
train() client id: f_00008-10-3 loss: 0.674659  [  128/  130]
train() client id: f_00008-11-0 loss: 0.705042  [   32/  130]
train() client id: f_00008-11-1 loss: 0.658612  [   64/  130]
train() client id: f_00008-11-2 loss: 0.496673  [   96/  130]
train() client id: f_00008-11-3 loss: 0.896087  [  128/  130]
train() client id: f_00008-12-0 loss: 0.621971  [   32/  130]
train() client id: f_00008-12-1 loss: 0.620013  [   64/  130]
train() client id: f_00008-12-2 loss: 0.682770  [   96/  130]
train() client id: f_00008-12-3 loss: 0.775653  [  128/  130]
train() client id: f_00009-0-0 loss: 0.976271  [   32/  118]
train() client id: f_00009-0-1 loss: 1.245562  [   64/  118]
train() client id: f_00009-0-2 loss: 1.044328  [   96/  118]
train() client id: f_00009-1-0 loss: 1.071608  [   32/  118]
train() client id: f_00009-1-1 loss: 0.939634  [   64/  118]
train() client id: f_00009-1-2 loss: 0.945730  [   96/  118]
train() client id: f_00009-2-0 loss: 0.940518  [   32/  118]
train() client id: f_00009-2-1 loss: 0.839327  [   64/  118]
train() client id: f_00009-2-2 loss: 1.177436  [   96/  118]
train() client id: f_00009-3-0 loss: 0.910066  [   32/  118]
train() client id: f_00009-3-1 loss: 1.116724  [   64/  118]
train() client id: f_00009-3-2 loss: 0.849499  [   96/  118]
train() client id: f_00009-4-0 loss: 0.958298  [   32/  118]
train() client id: f_00009-4-1 loss: 1.066360  [   64/  118]
train() client id: f_00009-4-2 loss: 0.803348  [   96/  118]
train() client id: f_00009-5-0 loss: 0.831692  [   32/  118]
train() client id: f_00009-5-1 loss: 0.923709  [   64/  118]
train() client id: f_00009-5-2 loss: 0.841117  [   96/  118]
train() client id: f_00009-6-0 loss: 0.791683  [   32/  118]
train() client id: f_00009-6-1 loss: 0.801442  [   64/  118]
train() client id: f_00009-6-2 loss: 0.922485  [   96/  118]
train() client id: f_00009-7-0 loss: 0.989927  [   32/  118]
train() client id: f_00009-7-1 loss: 0.773653  [   64/  118]
train() client id: f_00009-7-2 loss: 0.798534  [   96/  118]
train() client id: f_00009-8-0 loss: 0.834121  [   32/  118]
train() client id: f_00009-8-1 loss: 0.674624  [   64/  118]
train() client id: f_00009-8-2 loss: 0.899536  [   96/  118]
train() client id: f_00009-9-0 loss: 0.893578  [   32/  118]
train() client id: f_00009-9-1 loss: 0.956232  [   64/  118]
train() client id: f_00009-9-2 loss: 0.694055  [   96/  118]
train() client id: f_00009-10-0 loss: 0.785425  [   32/  118]
train() client id: f_00009-10-1 loss: 0.860937  [   64/  118]
train() client id: f_00009-10-2 loss: 0.771835  [   96/  118]
train() client id: f_00009-11-0 loss: 0.697638  [   32/  118]
train() client id: f_00009-11-1 loss: 0.791383  [   64/  118]
train() client id: f_00009-11-2 loss: 0.792532  [   96/  118]
train() client id: f_00009-12-0 loss: 0.832115  [   32/  118]
train() client id: f_00009-12-1 loss: 0.933372  [   64/  118]
train() client id: f_00009-12-2 loss: 0.705360  [   96/  118]
At round 44 accuracy: 0.6472148541114059
At round 44 training accuracy: 0.5895372233400402
At round 44 training loss: 0.8310502997620571
update_location
xs = [  -3.9056584     4.20031788  240.00902392   18.81129433    0.97929623
    3.95640986 -202.44319194 -181.32485185  224.66397685 -167.06087855]
ys = [ 232.5879595   215.55583871    1.32061395 -202.45517586  194.35018685
  177.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [253.20429118 237.6593408  260.01168355 226.58764977 218.57024991
 204.04294985 225.80995661 207.07336418 246.5412186  194.74431699]
dists_bs = [180.52602805 183.73222358 449.88556219 424.3055828  177.03994586
 178.97853524 179.73247853 174.29245025 429.55303777 171.18271783]
uav_gains = [6.96478461e-12 9.36092802e-12 6.06859569e-12 1.13384472e-11
 1.29057031e-11 1.60769736e-11 1.14854179e-11 1.53749023e-11
 7.93441177e-12 1.83978984e-11]
bs_gains = [5.30863138e-11 5.05330089e-11 4.11726034e-12 4.85057916e-12
 5.60653496e-11 5.43815306e-11 5.37452031e-11 5.85752397e-11
 4.68648315e-12 6.16036319e-11]
Round 45
-------------------------------
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.05514024 10.40024199  4.98453475  1.80545975 11.99306906  5.77018381
  2.2328339   7.08469715  5.23316744  4.67883957]
obj_prev = 59.2381676466099
eta_min = 5.885406011502381e-19	eta_max = 0.9363452566333951
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 13.714204272500444	eta = 0.9090909090909091
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 26.6122320197388	eta = 0.4684860112559707
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 20.101928982645475	eta = 0.6202120423522212
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 18.922786406509566	eta = 0.6588595443458035
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 18.857340066002127	eta = 0.6611461842396013
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 18.857119499819667	eta = 0.6611539174721296
eta = 0.6611539174721296
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [0.03414319 0.0718091  0.03360124 0.01165204 0.08291919 0.03956275
 0.0146328  0.04850501 0.0352271  0.03197537]
ene_total = [1.73700778 2.97656775 1.73758614 0.82737842 3.38956642 1.75748632
 0.93827146 2.18480132 1.84341633 1.46503756]
ti_comp = [0.61689568 0.66459269 0.61132264 0.63338719 0.66608943 0.66565651
 0.63376527 0.64167568 0.59949544 0.66739459]
ti_coms = [0.11954586 0.07184885 0.1251189  0.10305434 0.0703521  0.07078503
 0.10267627 0.09476586 0.1369461  0.06904695]
t_total = [27.74981117 27.74981117 27.74981117 27.74981117 27.74981117 27.74981117
 27.74981117 27.74981117 27.74981117 27.74981117]
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [6.53685718e-06 5.23971089e-05 6.34460979e-06 2.46460673e-07
 8.03119208e-05 8.73451965e-06 4.87534362e-07 1.73224277e-05
 7.60219691e-06 4.58734577e-06]
ene_total = [0.45070637 0.27270761 0.47169887 0.38832777 0.26811959 0.26705379
 0.38691223 0.35773942 0.51631227 0.26034825]
optimize_network iter = 0 obj = 3.639926174690373
eta = 0.6611539174721296
freqs = [27673393.660334   54024893.67528107 27482409.80128485  9198198.6140707
 62243282.60071961 29717090.05574555 11544335.26103987 37795583.8923452
 29380628.13581736 23955372.57683051]
eta_min = 0.6611539174721324	eta_max = 0.6721599416563945
af = 0.005695477061159853	bf = 1.2310181153532935	zeta = 0.006265024767275839	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [1.50574262e-06 1.20694943e-05 1.46145910e-06 5.67713705e-08
 1.84995754e-05 2.01196663e-06 1.12301867e-07 3.99016180e-06
 1.75113998e-06 1.05667936e-06]
ene_total = [1.67921435 1.010801   1.75748063 1.44739012 0.99068256 0.99444728
 1.44208793 1.33153211 1.92363253 0.96990197]
ti_comp = [0.5929754  0.64067241 0.58740236 0.60946691 0.64216915 0.64173622
 0.60984499 0.6177554  0.57557516 0.64347431]
ti_coms = [0.11954586 0.07184885 0.1251189  0.10305434 0.0703521  0.07078503
 0.10267627 0.09476586 0.1369461  0.06904695]
t_total = [27.74981117 27.74981117 27.74981117 27.74981117 27.74981117 27.74981117
 27.74981117 27.74981117 27.74981117 27.74981117]
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [6.26134418e-06 4.98993487e-05 6.08167179e-06 2.35577818e-07
 7.64706350e-05 8.31715417e-06 4.65984701e-07 1.65407536e-05
 7.29886402e-06 4.36729864e-06]
ene_total = [0.46582645 0.28176549 0.48752418 0.40136402 0.27697112 0.27600289
 0.39990055 0.36971876 0.53363374 0.26907992]
optimize_network iter = 1 obj = 3.761787113624325
eta = 0.6721599416563945
freqs = [27640992.43136498 53805865.01782712 27460330.69138759  9177789.36033805
 61985734.40435877 29594844.6264841  11518450.63917791 37692613.12413883
 29380628.13581735 23854513.15446249]
eta_min = 0.6721599416563961	eta_max = 0.6721599416563635
af = 0.0056547282477792775	bf = 1.2310181153532935	zeta = 0.006220201072557206	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [1.50221871e-06 1.19718279e-05 1.45911179e-06 5.65197178e-08
 1.83467983e-05 1.99544766e-06 1.11798827e-07 3.96844972e-06
 1.75113998e-06 1.04780021e-06]
ene_total = [1.67921385 1.01078729 1.7574803  1.44739009 0.9906611  0.99444496
 1.44208786 1.33152906 1.92363253 0.96990072]
ti_comp = [0.5929754  0.64067241 0.58740236 0.60946691 0.64216915 0.64173622
 0.60984499 0.6177554  0.57557516 0.64347431]
ti_coms = [0.11954586 0.07184885 0.1251189  0.10305434 0.0703521  0.07078503
 0.10267627 0.09476586 0.1369461  0.06904695]
t_total = [27.74981117 27.74981117 27.74981117 27.74981117 27.74981117 27.74981117
 27.74981117 27.74981117 27.74981117 27.74981117]
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [6.26134418e-06 4.98993487e-05 6.08167179e-06 2.35577818e-07
 7.64706350e-05 8.31715417e-06 4.65984701e-07 1.65407536e-05
 7.29886402e-06 4.36729864e-06]
ene_total = [0.46582645 0.28176549 0.48752418 0.40136402 0.27697112 0.27600289
 0.39990055 0.36971876 0.53363374 0.26907992]
optimize_network iter = 2 obj = 3.761787113623971
eta = 0.6721599416563635
freqs = [27640992.43136505 53805865.01782771 27460330.69138763  9177789.3603381
 61985734.40435947 29594844.62648444 11518450.63917798 37692613.1241391
 29380628.13581733 23854513.15446277]
Done!
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [6.25736035e-06 4.98675999e-05 6.07780228e-06 2.35427930e-07
 7.64219800e-05 8.31186232e-06 4.65688215e-07 1.65302294e-05
 7.29422007e-06 4.36451991e-06]
ene_total = [0.01196084 0.00723475 0.01251797 0.01030567 0.00711163 0.00708682
 0.01026809 0.00949312 0.0137019  0.00690906]
At round 45 energy consumption: 0.09658985266196866
At round 45 eta: 0.6721599416563635
At round 45 a_n: 12.425493895665781
At round 45 local rounds: 13.008276614299735
At round 45 global rounds: 38.94594152753551
gradient difference: 0.42940860986709595
train() client id: f_00000-0-0 loss: 1.016423  [   32/  126]
train() client id: f_00000-0-1 loss: 1.063631  [   64/  126]
train() client id: f_00000-0-2 loss: 1.115277  [   96/  126]
train() client id: f_00000-1-0 loss: 0.886947  [   32/  126]
train() client id: f_00000-1-1 loss: 1.071180  [   64/  126]
train() client id: f_00000-1-2 loss: 1.146683  [   96/  126]
train() client id: f_00000-2-0 loss: 0.968266  [   32/  126]
train() client id: f_00000-2-1 loss: 0.768810  [   64/  126]
train() client id: f_00000-2-2 loss: 1.032910  [   96/  126]
train() client id: f_00000-3-0 loss: 1.001341  [   32/  126]
train() client id: f_00000-3-1 loss: 0.927709  [   64/  126]
train() client id: f_00000-3-2 loss: 0.915517  [   96/  126]
train() client id: f_00000-4-0 loss: 1.043103  [   32/  126]
train() client id: f_00000-4-1 loss: 0.897673  [   64/  126]
train() client id: f_00000-4-2 loss: 0.928210  [   96/  126]
train() client id: f_00000-5-0 loss: 0.975574  [   32/  126]
train() client id: f_00000-5-1 loss: 0.896696  [   64/  126]
train() client id: f_00000-5-2 loss: 0.838604  [   96/  126]
train() client id: f_00000-6-0 loss: 1.050132  [   32/  126]
train() client id: f_00000-6-1 loss: 0.850850  [   64/  126]
train() client id: f_00000-6-2 loss: 0.860560  [   96/  126]
train() client id: f_00000-7-0 loss: 0.906582  [   32/  126]
train() client id: f_00000-7-1 loss: 0.854836  [   64/  126]
train() client id: f_00000-7-2 loss: 0.810663  [   96/  126]
train() client id: f_00000-8-0 loss: 0.966478  [   32/  126]
train() client id: f_00000-8-1 loss: 0.882421  [   64/  126]
train() client id: f_00000-8-2 loss: 0.813241  [   96/  126]
train() client id: f_00000-9-0 loss: 0.932070  [   32/  126]
train() client id: f_00000-9-1 loss: 1.016781  [   64/  126]
train() client id: f_00000-9-2 loss: 0.838783  [   96/  126]
train() client id: f_00000-10-0 loss: 0.709593  [   32/  126]
train() client id: f_00000-10-1 loss: 0.978494  [   64/  126]
train() client id: f_00000-10-2 loss: 1.065479  [   96/  126]
train() client id: f_00000-11-0 loss: 0.940484  [   32/  126]
train() client id: f_00000-11-1 loss: 0.934806  [   64/  126]
train() client id: f_00000-11-2 loss: 0.851951  [   96/  126]
train() client id: f_00000-12-0 loss: 0.944028  [   32/  126]
train() client id: f_00000-12-1 loss: 0.824608  [   64/  126]
train() client id: f_00000-12-2 loss: 0.940044  [   96/  126]
train() client id: f_00001-0-0 loss: 0.424152  [   32/  265]
train() client id: f_00001-0-1 loss: 0.429618  [   64/  265]
train() client id: f_00001-0-2 loss: 0.446206  [   96/  265]
train() client id: f_00001-0-3 loss: 0.400816  [  128/  265]
train() client id: f_00001-0-4 loss: 0.371072  [  160/  265]
train() client id: f_00001-0-5 loss: 0.341899  [  192/  265]
train() client id: f_00001-0-6 loss: 0.479632  [  224/  265]
train() client id: f_00001-0-7 loss: 0.306321  [  256/  265]
train() client id: f_00001-1-0 loss: 0.376358  [   32/  265]
train() client id: f_00001-1-1 loss: 0.457204  [   64/  265]
train() client id: f_00001-1-2 loss: 0.427904  [   96/  265]
train() client id: f_00001-1-3 loss: 0.304054  [  128/  265]
train() client id: f_00001-1-4 loss: 0.309463  [  160/  265]
train() client id: f_00001-1-5 loss: 0.447768  [  192/  265]
train() client id: f_00001-1-6 loss: 0.432992  [  224/  265]
train() client id: f_00001-1-7 loss: 0.361444  [  256/  265]
train() client id: f_00001-2-0 loss: 0.545460  [   32/  265]
train() client id: f_00001-2-1 loss: 0.386403  [   64/  265]
train() client id: f_00001-2-2 loss: 0.382436  [   96/  265]
train() client id: f_00001-2-3 loss: 0.308208  [  128/  265]
train() client id: f_00001-2-4 loss: 0.543637  [  160/  265]
train() client id: f_00001-2-5 loss: 0.339976  [  192/  265]
train() client id: f_00001-2-6 loss: 0.288922  [  224/  265]
train() client id: f_00001-2-7 loss: 0.269760  [  256/  265]
train() client id: f_00001-3-0 loss: 0.335707  [   32/  265]
train() client id: f_00001-3-1 loss: 0.404919  [   64/  265]
train() client id: f_00001-3-2 loss: 0.287292  [   96/  265]
train() client id: f_00001-3-3 loss: 0.372339  [  128/  265]
train() client id: f_00001-3-4 loss: 0.305392  [  160/  265]
train() client id: f_00001-3-5 loss: 0.503608  [  192/  265]
train() client id: f_00001-3-6 loss: 0.399470  [  224/  265]
train() client id: f_00001-3-7 loss: 0.363548  [  256/  265]
train() client id: f_00001-4-0 loss: 0.335256  [   32/  265]
train() client id: f_00001-4-1 loss: 0.291625  [   64/  265]
train() client id: f_00001-4-2 loss: 0.428085  [   96/  265]
train() client id: f_00001-4-3 loss: 0.407585  [  128/  265]
train() client id: f_00001-4-4 loss: 0.281062  [  160/  265]
train() client id: f_00001-4-5 loss: 0.334323  [  192/  265]
train() client id: f_00001-4-6 loss: 0.588806  [  224/  265]
train() client id: f_00001-4-7 loss: 0.299390  [  256/  265]
train() client id: f_00001-5-0 loss: 0.473982  [   32/  265]
train() client id: f_00001-5-1 loss: 0.341683  [   64/  265]
train() client id: f_00001-5-2 loss: 0.402569  [   96/  265]
train() client id: f_00001-5-3 loss: 0.364733  [  128/  265]
train() client id: f_00001-5-4 loss: 0.445434  [  160/  265]
train() client id: f_00001-5-5 loss: 0.356560  [  192/  265]
train() client id: f_00001-5-6 loss: 0.272024  [  224/  265]
train() client id: f_00001-5-7 loss: 0.295458  [  256/  265]
train() client id: f_00001-6-0 loss: 0.322989  [   32/  265]
train() client id: f_00001-6-1 loss: 0.322601  [   64/  265]
train() client id: f_00001-6-2 loss: 0.481830  [   96/  265]
train() client id: f_00001-6-3 loss: 0.306543  [  128/  265]
train() client id: f_00001-6-4 loss: 0.386342  [  160/  265]
train() client id: f_00001-6-5 loss: 0.307196  [  192/  265]
train() client id: f_00001-6-6 loss: 0.399205  [  224/  265]
train() client id: f_00001-6-7 loss: 0.383819  [  256/  265]
train() client id: f_00001-7-0 loss: 0.297235  [   32/  265]
train() client id: f_00001-7-1 loss: 0.365763  [   64/  265]
train() client id: f_00001-7-2 loss: 0.261273  [   96/  265]
train() client id: f_00001-7-3 loss: 0.396719  [  128/  265]
train() client id: f_00001-7-4 loss: 0.382565  [  160/  265]
train() client id: f_00001-7-5 loss: 0.329962  [  192/  265]
train() client id: f_00001-7-6 loss: 0.459679  [  224/  265]
train() client id: f_00001-7-7 loss: 0.397218  [  256/  265]
train() client id: f_00001-8-0 loss: 0.260388  [   32/  265]
train() client id: f_00001-8-1 loss: 0.342385  [   64/  265]
train() client id: f_00001-8-2 loss: 0.364334  [   96/  265]
train() client id: f_00001-8-3 loss: 0.449912  [  128/  265]
train() client id: f_00001-8-4 loss: 0.313726  [  160/  265]
train() client id: f_00001-8-5 loss: 0.376664  [  192/  265]
train() client id: f_00001-8-6 loss: 0.268298  [  224/  265]
train() client id: f_00001-8-7 loss: 0.414006  [  256/  265]
train() client id: f_00001-9-0 loss: 0.430450  [   32/  265]
train() client id: f_00001-9-1 loss: 0.375745  [   64/  265]
train() client id: f_00001-9-2 loss: 0.368483  [   96/  265]
train() client id: f_00001-9-3 loss: 0.308589  [  128/  265]
train() client id: f_00001-9-4 loss: 0.472555  [  160/  265]
train() client id: f_00001-9-5 loss: 0.369823  [  192/  265]
train() client id: f_00001-9-6 loss: 0.262414  [  224/  265]
train() client id: f_00001-9-7 loss: 0.270172  [  256/  265]
train() client id: f_00001-10-0 loss: 0.346229  [   32/  265]
train() client id: f_00001-10-1 loss: 0.306108  [   64/  265]
train() client id: f_00001-10-2 loss: 0.264645  [   96/  265]
train() client id: f_00001-10-3 loss: 0.442321  [  128/  265]
train() client id: f_00001-10-4 loss: 0.288796  [  160/  265]
train() client id: f_00001-10-5 loss: 0.402806  [  192/  265]
train() client id: f_00001-10-6 loss: 0.507779  [  224/  265]
train() client id: f_00001-10-7 loss: 0.294242  [  256/  265]
train() client id: f_00001-11-0 loss: 0.321640  [   32/  265]
train() client id: f_00001-11-1 loss: 0.305491  [   64/  265]
train() client id: f_00001-11-2 loss: 0.390672  [   96/  265]
train() client id: f_00001-11-3 loss: 0.451357  [  128/  265]
train() client id: f_00001-11-4 loss: 0.334771  [  160/  265]
train() client id: f_00001-11-5 loss: 0.356332  [  192/  265]
train() client id: f_00001-11-6 loss: 0.424024  [  224/  265]
train() client id: f_00001-11-7 loss: 0.278053  [  256/  265]
train() client id: f_00001-12-0 loss: 0.318158  [   32/  265]
train() client id: f_00001-12-1 loss: 0.413439  [   64/  265]
train() client id: f_00001-12-2 loss: 0.266466  [   96/  265]
train() client id: f_00001-12-3 loss: 0.342950  [  128/  265]
train() client id: f_00001-12-4 loss: 0.446872  [  160/  265]
train() client id: f_00001-12-5 loss: 0.337199  [  192/  265]
train() client id: f_00001-12-6 loss: 0.373747  [  224/  265]
train() client id: f_00001-12-7 loss: 0.350870  [  256/  265]
train() client id: f_00002-0-0 loss: 1.239215  [   32/  124]
train() client id: f_00002-0-1 loss: 1.199420  [   64/  124]
train() client id: f_00002-0-2 loss: 1.021325  [   96/  124]
train() client id: f_00002-1-0 loss: 1.002277  [   32/  124]
train() client id: f_00002-1-1 loss: 1.173477  [   64/  124]
train() client id: f_00002-1-2 loss: 1.174762  [   96/  124]
train() client id: f_00002-2-0 loss: 1.072683  [   32/  124]
train() client id: f_00002-2-1 loss: 1.052300  [   64/  124]
train() client id: f_00002-2-2 loss: 1.001454  [   96/  124]
train() client id: f_00002-3-0 loss: 1.198823  [   32/  124]
train() client id: f_00002-3-1 loss: 0.955665  [   64/  124]
train() client id: f_00002-3-2 loss: 0.926784  [   96/  124]
train() client id: f_00002-4-0 loss: 1.104488  [   32/  124]
train() client id: f_00002-4-1 loss: 1.053408  [   64/  124]
train() client id: f_00002-4-2 loss: 0.811161  [   96/  124]
train() client id: f_00002-5-0 loss: 0.893119  [   32/  124]
train() client id: f_00002-5-1 loss: 0.872779  [   64/  124]
train() client id: f_00002-5-2 loss: 0.921188  [   96/  124]
train() client id: f_00002-6-0 loss: 1.012110  [   32/  124]
train() client id: f_00002-6-1 loss: 1.035120  [   64/  124]
train() client id: f_00002-6-2 loss: 0.726316  [   96/  124]
train() client id: f_00002-7-0 loss: 0.847486  [   32/  124]
train() client id: f_00002-7-1 loss: 1.037589  [   64/  124]
train() client id: f_00002-7-2 loss: 0.787868  [   96/  124]
train() client id: f_00002-8-0 loss: 0.888443  [   32/  124]
train() client id: f_00002-8-1 loss: 0.884461  [   64/  124]
train() client id: f_00002-8-2 loss: 0.847685  [   96/  124]
train() client id: f_00002-9-0 loss: 0.715607  [   32/  124]
train() client id: f_00002-9-1 loss: 0.924694  [   64/  124]
train() client id: f_00002-9-2 loss: 0.679168  [   96/  124]
train() client id: f_00002-10-0 loss: 0.963654  [   32/  124]
train() client id: f_00002-10-1 loss: 0.739074  [   64/  124]
train() client id: f_00002-10-2 loss: 0.706491  [   96/  124]
train() client id: f_00002-11-0 loss: 0.754223  [   32/  124]
train() client id: f_00002-11-1 loss: 0.780159  [   64/  124]
train() client id: f_00002-11-2 loss: 0.778830  [   96/  124]
train() client id: f_00002-12-0 loss: 0.766610  [   32/  124]
train() client id: f_00002-12-1 loss: 0.778442  [   64/  124]
train() client id: f_00002-12-2 loss: 0.810634  [   96/  124]
train() client id: f_00003-0-0 loss: 0.627437  [   32/   43]
train() client id: f_00003-1-0 loss: 0.434770  [   32/   43]
train() client id: f_00003-2-0 loss: 0.515082  [   32/   43]
train() client id: f_00003-3-0 loss: 0.486508  [   32/   43]
train() client id: f_00003-4-0 loss: 0.652283  [   32/   43]
train() client id: f_00003-5-0 loss: 0.694867  [   32/   43]
train() client id: f_00003-6-0 loss: 0.618738  [   32/   43]
train() client id: f_00003-7-0 loss: 0.728660  [   32/   43]
train() client id: f_00003-8-0 loss: 0.578512  [   32/   43]
train() client id: f_00003-9-0 loss: 0.604587  [   32/   43]
train() client id: f_00003-10-0 loss: 0.626421  [   32/   43]
train() client id: f_00003-11-0 loss: 0.652956  [   32/   43]
train() client id: f_00003-12-0 loss: 0.624378  [   32/   43]
train() client id: f_00004-0-0 loss: 0.774601  [   32/  306]
train() client id: f_00004-0-1 loss: 0.816185  [   64/  306]
train() client id: f_00004-0-2 loss: 0.840019  [   96/  306]
train() client id: f_00004-0-3 loss: 0.735290  [  128/  306]
train() client id: f_00004-0-4 loss: 0.924003  [  160/  306]
train() client id: f_00004-0-5 loss: 0.774677  [  192/  306]
train() client id: f_00004-0-6 loss: 1.039624  [  224/  306]
train() client id: f_00004-0-7 loss: 0.796462  [  256/  306]
train() client id: f_00004-0-8 loss: 0.882171  [  288/  306]
train() client id: f_00004-1-0 loss: 0.850905  [   32/  306]
train() client id: f_00004-1-1 loss: 0.776802  [   64/  306]
train() client id: f_00004-1-2 loss: 0.921429  [   96/  306]
train() client id: f_00004-1-3 loss: 0.925795  [  128/  306]
train() client id: f_00004-1-4 loss: 0.730590  [  160/  306]
train() client id: f_00004-1-5 loss: 0.890776  [  192/  306]
train() client id: f_00004-1-6 loss: 0.820448  [  224/  306]
train() client id: f_00004-1-7 loss: 0.798693  [  256/  306]
train() client id: f_00004-1-8 loss: 0.838758  [  288/  306]
train() client id: f_00004-2-0 loss: 0.931806  [   32/  306]
train() client id: f_00004-2-1 loss: 0.847676  [   64/  306]
train() client id: f_00004-2-2 loss: 0.796677  [   96/  306]
train() client id: f_00004-2-3 loss: 0.897197  [  128/  306]
train() client id: f_00004-2-4 loss: 0.784809  [  160/  306]
train() client id: f_00004-2-5 loss: 0.796584  [  192/  306]
train() client id: f_00004-2-6 loss: 0.946832  [  224/  306]
train() client id: f_00004-2-7 loss: 0.819034  [  256/  306]
train() client id: f_00004-2-8 loss: 0.891190  [  288/  306]
train() client id: f_00004-3-0 loss: 0.734037  [   32/  306]
train() client id: f_00004-3-1 loss: 0.809082  [   64/  306]
train() client id: f_00004-3-2 loss: 0.825612  [   96/  306]
train() client id: f_00004-3-3 loss: 0.879219  [  128/  306]
train() client id: f_00004-3-4 loss: 1.030151  [  160/  306]
train() client id: f_00004-3-5 loss: 0.839932  [  192/  306]
train() client id: f_00004-3-6 loss: 0.849499  [  224/  306]
train() client id: f_00004-3-7 loss: 0.780114  [  256/  306]
train() client id: f_00004-3-8 loss: 0.878037  [  288/  306]
train() client id: f_00004-4-0 loss: 0.734288  [   32/  306]
train() client id: f_00004-4-1 loss: 0.882782  [   64/  306]
train() client id: f_00004-4-2 loss: 0.922211  [   96/  306]
train() client id: f_00004-4-3 loss: 0.890764  [  128/  306]
train() client id: f_00004-4-4 loss: 0.827125  [  160/  306]
train() client id: f_00004-4-5 loss: 0.856426  [  192/  306]
train() client id: f_00004-4-6 loss: 0.799850  [  224/  306]
train() client id: f_00004-4-7 loss: 0.781664  [  256/  306]
train() client id: f_00004-4-8 loss: 0.872647  [  288/  306]
train() client id: f_00004-5-0 loss: 0.871826  [   32/  306]
train() client id: f_00004-5-1 loss: 0.825097  [   64/  306]
train() client id: f_00004-5-2 loss: 0.895873  [   96/  306]
train() client id: f_00004-5-3 loss: 0.887204  [  128/  306]
train() client id: f_00004-5-4 loss: 0.771572  [  160/  306]
train() client id: f_00004-5-5 loss: 0.918585  [  192/  306]
train() client id: f_00004-5-6 loss: 0.876791  [  224/  306]
train() client id: f_00004-5-7 loss: 0.786581  [  256/  306]
train() client id: f_00004-5-8 loss: 0.870971  [  288/  306]
train() client id: f_00004-6-0 loss: 0.860132  [   32/  306]
train() client id: f_00004-6-1 loss: 0.762344  [   64/  306]
train() client id: f_00004-6-2 loss: 0.705001  [   96/  306]
train() client id: f_00004-6-3 loss: 0.969630  [  128/  306]
train() client id: f_00004-6-4 loss: 0.726903  [  160/  306]
train() client id: f_00004-6-5 loss: 0.888403  [  192/  306]
train() client id: f_00004-6-6 loss: 0.830983  [  224/  306]
train() client id: f_00004-6-7 loss: 0.876165  [  256/  306]
train() client id: f_00004-6-8 loss: 0.855740  [  288/  306]
train() client id: f_00004-7-0 loss: 0.909488  [   32/  306]
train() client id: f_00004-7-1 loss: 0.888711  [   64/  306]
train() client id: f_00004-7-2 loss: 0.739769  [   96/  306]
train() client id: f_00004-7-3 loss: 0.924672  [  128/  306]
train() client id: f_00004-7-4 loss: 0.843247  [  160/  306]
train() client id: f_00004-7-5 loss: 0.664853  [  192/  306]
train() client id: f_00004-7-6 loss: 0.717178  [  224/  306]
train() client id: f_00004-7-7 loss: 0.927330  [  256/  306]
train() client id: f_00004-7-8 loss: 0.897744  [  288/  306]
train() client id: f_00004-8-0 loss: 0.742922  [   32/  306]
train() client id: f_00004-8-1 loss: 0.847139  [   64/  306]
train() client id: f_00004-8-2 loss: 0.843737  [   96/  306]
train() client id: f_00004-8-3 loss: 0.895404  [  128/  306]
train() client id: f_00004-8-4 loss: 0.862788  [  160/  306]
train() client id: f_00004-8-5 loss: 0.740502  [  192/  306]
train() client id: f_00004-8-6 loss: 0.788391  [  224/  306]
train() client id: f_00004-8-7 loss: 0.926560  [  256/  306]
train() client id: f_00004-8-8 loss: 0.883657  [  288/  306]
train() client id: f_00004-9-0 loss: 0.957992  [   32/  306]
train() client id: f_00004-9-1 loss: 0.734510  [   64/  306]
train() client id: f_00004-9-2 loss: 0.847211  [   96/  306]
train() client id: f_00004-9-3 loss: 0.804048  [  128/  306]
train() client id: f_00004-9-4 loss: 0.914287  [  160/  306]
train() client id: f_00004-9-5 loss: 0.789170  [  192/  306]
train() client id: f_00004-9-6 loss: 0.738822  [  224/  306]
train() client id: f_00004-9-7 loss: 0.927054  [  256/  306]
train() client id: f_00004-9-8 loss: 0.875422  [  288/  306]
train() client id: f_00004-10-0 loss: 0.847486  [   32/  306]
train() client id: f_00004-10-1 loss: 0.821391  [   64/  306]
train() client id: f_00004-10-2 loss: 0.730832  [   96/  306]
train() client id: f_00004-10-3 loss: 0.771584  [  128/  306]
train() client id: f_00004-10-4 loss: 0.914187  [  160/  306]
train() client id: f_00004-10-5 loss: 0.945352  [  192/  306]
train() client id: f_00004-10-6 loss: 0.910453  [  224/  306]
train() client id: f_00004-10-7 loss: 0.801676  [  256/  306]
train() client id: f_00004-10-8 loss: 0.822732  [  288/  306]
train() client id: f_00004-11-0 loss: 0.799536  [   32/  306]
train() client id: f_00004-11-1 loss: 0.821714  [   64/  306]
train() client id: f_00004-11-2 loss: 0.832462  [   96/  306]
train() client id: f_00004-11-3 loss: 0.782396  [  128/  306]
train() client id: f_00004-11-4 loss: 0.919714  [  160/  306]
train() client id: f_00004-11-5 loss: 0.916068  [  192/  306]
train() client id: f_00004-11-6 loss: 0.802847  [  224/  306]
train() client id: f_00004-11-7 loss: 0.798400  [  256/  306]
train() client id: f_00004-11-8 loss: 0.898109  [  288/  306]
train() client id: f_00004-12-0 loss: 0.849434  [   32/  306]
train() client id: f_00004-12-1 loss: 0.855090  [   64/  306]
train() client id: f_00004-12-2 loss: 0.858269  [   96/  306]
train() client id: f_00004-12-3 loss: 0.831134  [  128/  306]
train() client id: f_00004-12-4 loss: 0.831243  [  160/  306]
train() client id: f_00004-12-5 loss: 0.896060  [  192/  306]
train() client id: f_00004-12-6 loss: 0.781484  [  224/  306]
train() client id: f_00004-12-7 loss: 0.836094  [  256/  306]
train() client id: f_00004-12-8 loss: 0.806878  [  288/  306]
train() client id: f_00005-0-0 loss: 0.648357  [   32/  146]
train() client id: f_00005-0-1 loss: 0.564064  [   64/  146]
train() client id: f_00005-0-2 loss: 0.687690  [   96/  146]
train() client id: f_00005-0-3 loss: 0.800457  [  128/  146]
train() client id: f_00005-1-0 loss: 0.890282  [   32/  146]
train() client id: f_00005-1-1 loss: 0.698947  [   64/  146]
train() client id: f_00005-1-2 loss: 0.595294  [   96/  146]
train() client id: f_00005-1-3 loss: 0.570372  [  128/  146]
train() client id: f_00005-2-0 loss: 0.889271  [   32/  146]
train() client id: f_00005-2-1 loss: 0.528644  [   64/  146]
train() client id: f_00005-2-2 loss: 0.625853  [   96/  146]
train() client id: f_00005-2-3 loss: 0.843399  [  128/  146]
train() client id: f_00005-3-0 loss: 0.725854  [   32/  146]
train() client id: f_00005-3-1 loss: 0.531232  [   64/  146]
train() client id: f_00005-3-2 loss: 0.840974  [   96/  146]
train() client id: f_00005-3-3 loss: 0.652922  [  128/  146]
train() client id: f_00005-4-0 loss: 0.594738  [   32/  146]
train() client id: f_00005-4-1 loss: 0.925705  [   64/  146]
train() client id: f_00005-4-2 loss: 0.693360  [   96/  146]
train() client id: f_00005-4-3 loss: 0.579830  [  128/  146]
train() client id: f_00005-5-0 loss: 0.660843  [   32/  146]
train() client id: f_00005-5-1 loss: 0.717831  [   64/  146]
train() client id: f_00005-5-2 loss: 0.659989  [   96/  146]
train() client id: f_00005-5-3 loss: 0.715698  [  128/  146]
train() client id: f_00005-6-0 loss: 0.574330  [   32/  146]
train() client id: f_00005-6-1 loss: 0.567884  [   64/  146]
train() client id: f_00005-6-2 loss: 0.794419  [   96/  146]
train() client id: f_00005-6-3 loss: 0.894056  [  128/  146]
train() client id: f_00005-7-0 loss: 0.768549  [   32/  146]
train() client id: f_00005-7-1 loss: 0.873570  [   64/  146]
train() client id: f_00005-7-2 loss: 0.603554  [   96/  146]
train() client id: f_00005-7-3 loss: 0.590488  [  128/  146]
train() client id: f_00005-8-0 loss: 0.773580  [   32/  146]
train() client id: f_00005-8-1 loss: 0.798421  [   64/  146]
train() client id: f_00005-8-2 loss: 0.469919  [   96/  146]
train() client id: f_00005-8-3 loss: 0.754426  [  128/  146]
train() client id: f_00005-9-0 loss: 0.621038  [   32/  146]
train() client id: f_00005-9-1 loss: 0.556220  [   64/  146]
train() client id: f_00005-9-2 loss: 0.800702  [   96/  146]
train() client id: f_00005-9-3 loss: 0.950318  [  128/  146]
train() client id: f_00005-10-0 loss: 0.672797  [   32/  146]
train() client id: f_00005-10-1 loss: 0.740894  [   64/  146]
train() client id: f_00005-10-2 loss: 0.759231  [   96/  146]
train() client id: f_00005-10-3 loss: 0.623862  [  128/  146]
train() client id: f_00005-11-0 loss: 0.821442  [   32/  146]
train() client id: f_00005-11-1 loss: 0.841781  [   64/  146]
train() client id: f_00005-11-2 loss: 0.473729  [   96/  146]
train() client id: f_00005-11-3 loss: 0.712126  [  128/  146]
train() client id: f_00005-12-0 loss: 0.439937  [   32/  146]
train() client id: f_00005-12-1 loss: 0.708946  [   64/  146]
train() client id: f_00005-12-2 loss: 0.466980  [   96/  146]
train() client id: f_00005-12-3 loss: 0.899753  [  128/  146]
train() client id: f_00006-0-0 loss: 0.501236  [   32/   54]
train() client id: f_00006-1-0 loss: 0.503670  [   32/   54]
train() client id: f_00006-2-0 loss: 0.518186  [   32/   54]
train() client id: f_00006-3-0 loss: 0.563267  [   32/   54]
train() client id: f_00006-4-0 loss: 0.470885  [   32/   54]
train() client id: f_00006-5-0 loss: 0.560621  [   32/   54]
train() client id: f_00006-6-0 loss: 0.500352  [   32/   54]
train() client id: f_00006-7-0 loss: 0.494410  [   32/   54]
train() client id: f_00006-8-0 loss: 0.440159  [   32/   54]
train() client id: f_00006-9-0 loss: 0.505818  [   32/   54]
train() client id: f_00006-10-0 loss: 0.496150  [   32/   54]
train() client id: f_00006-11-0 loss: 0.455999  [   32/   54]
train() client id: f_00006-12-0 loss: 0.512661  [   32/   54]
train() client id: f_00007-0-0 loss: 0.596548  [   32/  179]
train() client id: f_00007-0-1 loss: 0.541598  [   64/  179]
train() client id: f_00007-0-2 loss: 0.684230  [   96/  179]
train() client id: f_00007-0-3 loss: 0.597845  [  128/  179]
train() client id: f_00007-0-4 loss: 0.617259  [  160/  179]
train() client id: f_00007-1-0 loss: 0.625188  [   32/  179]
train() client id: f_00007-1-1 loss: 0.547269  [   64/  179]
train() client id: f_00007-1-2 loss: 0.609099  [   96/  179]
train() client id: f_00007-1-3 loss: 0.714118  [  128/  179]
train() client id: f_00007-1-4 loss: 0.560647  [  160/  179]
train() client id: f_00007-2-0 loss: 0.583727  [   32/  179]
train() client id: f_00007-2-1 loss: 0.694012  [   64/  179]
train() client id: f_00007-2-2 loss: 0.732854  [   96/  179]
train() client id: f_00007-2-3 loss: 0.513326  [  128/  179]
train() client id: f_00007-2-4 loss: 0.470407  [  160/  179]
train() client id: f_00007-3-0 loss: 0.544208  [   32/  179]
train() client id: f_00007-3-1 loss: 0.474222  [   64/  179]
train() client id: f_00007-3-2 loss: 0.733777  [   96/  179]
train() client id: f_00007-3-3 loss: 0.569672  [  128/  179]
train() client id: f_00007-3-4 loss: 0.588166  [  160/  179]
train() client id: f_00007-4-0 loss: 0.494190  [   32/  179]
train() client id: f_00007-4-1 loss: 0.469354  [   64/  179]
train() client id: f_00007-4-2 loss: 0.736512  [   96/  179]
train() client id: f_00007-4-3 loss: 0.469399  [  128/  179]
train() client id: f_00007-4-4 loss: 0.689111  [  160/  179]
train() client id: f_00007-5-0 loss: 0.723660  [   32/  179]
train() client id: f_00007-5-1 loss: 0.633895  [   64/  179]
train() client id: f_00007-5-2 loss: 0.488759  [   96/  179]
train() client id: f_00007-5-3 loss: 0.504090  [  128/  179]
train() client id: f_00007-5-4 loss: 0.580477  [  160/  179]
train() client id: f_00007-6-0 loss: 0.533451  [   32/  179]
train() client id: f_00007-6-1 loss: 0.420451  [   64/  179]
train() client id: f_00007-6-2 loss: 0.492632  [   96/  179]
train() client id: f_00007-6-3 loss: 0.913584  [  128/  179]
train() client id: f_00007-6-4 loss: 0.433543  [  160/  179]
train() client id: f_00007-7-0 loss: 0.528188  [   32/  179]
train() client id: f_00007-7-1 loss: 0.718758  [   64/  179]
train() client id: f_00007-7-2 loss: 0.521133  [   96/  179]
train() client id: f_00007-7-3 loss: 0.676603  [  128/  179]
train() client id: f_00007-7-4 loss: 0.422408  [  160/  179]
train() client id: f_00007-8-0 loss: 0.481111  [   32/  179]
train() client id: f_00007-8-1 loss: 0.672544  [   64/  179]
train() client id: f_00007-8-2 loss: 0.603637  [   96/  179]
train() client id: f_00007-8-3 loss: 0.503546  [  128/  179]
train() client id: f_00007-8-4 loss: 0.560403  [  160/  179]
train() client id: f_00007-9-0 loss: 0.602690  [   32/  179]
train() client id: f_00007-9-1 loss: 0.493351  [   64/  179]
train() client id: f_00007-9-2 loss: 0.755385  [   96/  179]
train() client id: f_00007-9-3 loss: 0.489544  [  128/  179]
train() client id: f_00007-9-4 loss: 0.556090  [  160/  179]
train() client id: f_00007-10-0 loss: 0.527463  [   32/  179]
train() client id: f_00007-10-1 loss: 0.516310  [   64/  179]
train() client id: f_00007-10-2 loss: 0.753217  [   96/  179]
train() client id: f_00007-10-3 loss: 0.491412  [  128/  179]
train() client id: f_00007-10-4 loss: 0.630663  [  160/  179]
train() client id: f_00007-11-0 loss: 0.629689  [   32/  179]
train() client id: f_00007-11-1 loss: 0.431594  [   64/  179]
train() client id: f_00007-11-2 loss: 0.743289  [   96/  179]
train() client id: f_00007-11-3 loss: 0.434515  [  128/  179]
train() client id: f_00007-11-4 loss: 0.654516  [  160/  179]
train() client id: f_00007-12-0 loss: 0.692637  [   32/  179]
train() client id: f_00007-12-1 loss: 0.405039  [   64/  179]
train() client id: f_00007-12-2 loss: 0.568626  [   96/  179]
train() client id: f_00007-12-3 loss: 0.591938  [  128/  179]
train() client id: f_00007-12-4 loss: 0.554377  [  160/  179]
train() client id: f_00008-0-0 loss: 0.780814  [   32/  130]
train() client id: f_00008-0-1 loss: 0.606671  [   64/  130]
train() client id: f_00008-0-2 loss: 0.562088  [   96/  130]
train() client id: f_00008-0-3 loss: 0.624376  [  128/  130]
train() client id: f_00008-1-0 loss: 0.658665  [   32/  130]
train() client id: f_00008-1-1 loss: 0.630796  [   64/  130]
train() client id: f_00008-1-2 loss: 0.608917  [   96/  130]
train() client id: f_00008-1-3 loss: 0.688381  [  128/  130]
train() client id: f_00008-2-0 loss: 0.646960  [   32/  130]
train() client id: f_00008-2-1 loss: 0.702798  [   64/  130]
train() client id: f_00008-2-2 loss: 0.594106  [   96/  130]
train() client id: f_00008-2-3 loss: 0.627656  [  128/  130]
train() client id: f_00008-3-0 loss: 0.708749  [   32/  130]
train() client id: f_00008-3-1 loss: 0.585889  [   64/  130]
train() client id: f_00008-3-2 loss: 0.662829  [   96/  130]
train() client id: f_00008-3-3 loss: 0.623180  [  128/  130]
train() client id: f_00008-4-0 loss: 0.624698  [   32/  130]
train() client id: f_00008-4-1 loss: 0.625547  [   64/  130]
train() client id: f_00008-4-2 loss: 0.534269  [   96/  130]
train() client id: f_00008-4-3 loss: 0.798207  [  128/  130]
train() client id: f_00008-5-0 loss: 0.665290  [   32/  130]
train() client id: f_00008-5-1 loss: 0.663291  [   64/  130]
train() client id: f_00008-5-2 loss: 0.596675  [   96/  130]
train() client id: f_00008-5-3 loss: 0.650934  [  128/  130]
train() client id: f_00008-6-0 loss: 0.620452  [   32/  130]
train() client id: f_00008-6-1 loss: 0.651015  [   64/  130]
train() client id: f_00008-6-2 loss: 0.679548  [   96/  130]
train() client id: f_00008-6-3 loss: 0.591663  [  128/  130]
train() client id: f_00008-7-0 loss: 0.587168  [   32/  130]
train() client id: f_00008-7-1 loss: 0.710770  [   64/  130]
train() client id: f_00008-7-2 loss: 0.638009  [   96/  130]
train() client id: f_00008-7-3 loss: 0.635879  [  128/  130]
train() client id: f_00008-8-0 loss: 0.485872  [   32/  130]
train() client id: f_00008-8-1 loss: 0.696541  [   64/  130]
train() client id: f_00008-8-2 loss: 0.722439  [   96/  130]
train() client id: f_00008-8-3 loss: 0.677050  [  128/  130]
train() client id: f_00008-9-0 loss: 0.650950  [   32/  130]
train() client id: f_00008-9-1 loss: 0.554576  [   64/  130]
train() client id: f_00008-9-2 loss: 0.781235  [   96/  130]
train() client id: f_00008-9-3 loss: 0.594525  [  128/  130]
train() client id: f_00008-10-0 loss: 0.694764  [   32/  130]
train() client id: f_00008-10-1 loss: 0.611233  [   64/  130]
train() client id: f_00008-10-2 loss: 0.634438  [   96/  130]
train() client id: f_00008-10-3 loss: 0.608973  [  128/  130]
train() client id: f_00008-11-0 loss: 0.710743  [   32/  130]
train() client id: f_00008-11-1 loss: 0.557584  [   64/  130]
train() client id: f_00008-11-2 loss: 0.696504  [   96/  130]
train() client id: f_00008-11-3 loss: 0.599165  [  128/  130]
train() client id: f_00008-12-0 loss: 0.659985  [   32/  130]
train() client id: f_00008-12-1 loss: 0.534048  [   64/  130]
train() client id: f_00008-12-2 loss: 0.656131  [   96/  130]
train() client id: f_00008-12-3 loss: 0.733526  [  128/  130]
train() client id: f_00009-0-0 loss: 1.256596  [   32/  118]
train() client id: f_00009-0-1 loss: 0.961412  [   64/  118]
train() client id: f_00009-0-2 loss: 1.068290  [   96/  118]
train() client id: f_00009-1-0 loss: 1.107412  [   32/  118]
train() client id: f_00009-1-1 loss: 1.017703  [   64/  118]
train() client id: f_00009-1-2 loss: 1.031875  [   96/  118]
train() client id: f_00009-2-0 loss: 0.950323  [   32/  118]
train() client id: f_00009-2-1 loss: 1.129174  [   64/  118]
train() client id: f_00009-2-2 loss: 0.959593  [   96/  118]
train() client id: f_00009-3-0 loss: 0.874016  [   32/  118]
train() client id: f_00009-3-1 loss: 1.119485  [   64/  118]
train() client id: f_00009-3-2 loss: 0.954664  [   96/  118]
train() client id: f_00009-4-0 loss: 1.048970  [   32/  118]
train() client id: f_00009-4-1 loss: 0.917185  [   64/  118]
train() client id: f_00009-4-2 loss: 1.000749  [   96/  118]
train() client id: f_00009-5-0 loss: 0.981262  [   32/  118]
train() client id: f_00009-5-1 loss: 0.959111  [   64/  118]
train() client id: f_00009-5-2 loss: 0.830754  [   96/  118]
train() client id: f_00009-6-0 loss: 0.949496  [   32/  118]
train() client id: f_00009-6-1 loss: 0.779321  [   64/  118]
train() client id: f_00009-6-2 loss: 0.897336  [   96/  118]
train() client id: f_00009-7-0 loss: 0.954184  [   32/  118]
train() client id: f_00009-7-1 loss: 0.899620  [   64/  118]
train() client id: f_00009-7-2 loss: 1.000143  [   96/  118]
train() client id: f_00009-8-0 loss: 0.853696  [   32/  118]
train() client id: f_00009-8-1 loss: 1.021429  [   64/  118]
train() client id: f_00009-8-2 loss: 0.736772  [   96/  118]
train() client id: f_00009-9-0 loss: 0.929895  [   32/  118]
train() client id: f_00009-9-1 loss: 0.837781  [   64/  118]
train() client id: f_00009-9-2 loss: 0.981704  [   96/  118]
train() client id: f_00009-10-0 loss: 0.974685  [   32/  118]
train() client id: f_00009-10-1 loss: 0.824969  [   64/  118]
train() client id: f_00009-10-2 loss: 0.837804  [   96/  118]
train() client id: f_00009-11-0 loss: 0.788001  [   32/  118]
train() client id: f_00009-11-1 loss: 0.817679  [   64/  118]
train() client id: f_00009-11-2 loss: 0.993995  [   96/  118]
train() client id: f_00009-12-0 loss: 0.764276  [   32/  118]
train() client id: f_00009-12-1 loss: 0.818225  [   64/  118]
train() client id: f_00009-12-2 loss: 1.075774  [   96/  118]
At round 45 accuracy: 0.6472148541114059
At round 45 training accuracy: 0.5922199865861838
At round 45 training loss: 0.835812442306335
update_location
xs = [  -3.9056584     4.20031788  245.00902392   18.81129433    0.97929623
    3.95640986 -207.44319194 -186.32485185  229.66397685 -172.06087855]
ys = [ 237.5879595   220.55583871    1.32061395 -207.45517586  199.35018685
  182.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [257.80475687 242.20346954 264.63402242 231.06603988 223.02792654
 208.41465133 230.30320976 211.46542666 251.1059781  199.05013887]
dists_bs = [182.18267316 184.90021192 454.50204544 428.75958223 177.65597175
 179.12693148 180.56355047 174.54542885 434.20899579 171.02377515]
uav_gains = [6.34845528e-12 8.61260857e-12 5.51598422e-12 1.05126925e-11
 1.20199406e-11 1.50715939e-11 1.06508766e-11 1.43975841e-11
 7.26046655e-12 1.72893462e-11]
bs_gains = [5.17457027e-11 4.96442942e-11 4.00123195e-12 4.71080726e-12
 5.55227049e-11 5.42554792e-11 5.30554308e-11 5.83378396e-11
 4.54713029e-12 6.17640715e-11]
Round 46
-------------------------------
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.92415982 10.12154902  4.85601333  1.75987582 11.67148702  5.61542073
  2.1757575   6.89673295  5.09466844  4.55326949]
obj_prev = 57.668934107649186
eta_min = 1.934063127362615e-19	eta_max = 0.9372721317732582
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 13.346274362136274	eta = 0.9090909090909091
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 26.11640265875819	eta = 0.46457304443429304
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 19.646759764324543	eta = 0.617556118077178
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 18.475231020287687	eta = 0.6567158310241378
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 18.40977991233988	eta = 0.6590506106332402
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 18.40955607686498	eta = 0.6590586238034545
eta = 0.6590586238034545
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [0.03440512 0.07235997 0.03385901 0.01174143 0.08355529 0.03986625
 0.01474505 0.04887711 0.03549735 0.03222067]
ene_total = [1.70303953 2.89994011 1.70508731 0.81177669 3.30204155 1.7110066
 0.91978726 2.13279846 1.79829675 1.42578181]
ti_comp = [0.63680768 0.68794036 0.63078466 0.65472313 0.68956148 0.68923291
 0.65512248 0.66360668 0.62159244 0.69103952]
ti_coms = [0.12324342 0.07211074 0.12926644 0.10532797 0.07048962 0.07081819
 0.10492862 0.09644442 0.13845866 0.06901158]
t_total = [27.69980698 27.69980698 27.69980698 27.69980698 27.69980698 27.69980698
 27.69980698 27.69980698 27.69980698 27.69980698]
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [6.27671845e-06 5.00349702e-05 6.09733991e-06 2.36008472e-07
 7.66753725e-05 8.33612381e-06 4.66846244e-07 1.65720103e-05
 7.23529432e-06 4.37802116e-06]
ene_total = [0.44938525 0.26462869 0.47132941 0.38387283 0.25969147 0.25839833
 0.38242583 0.35209241 0.50487161 0.25166996]
optimize_network iter = 0 obj = 3.578365787451716
eta = 0.6590586238034545
freqs = [27013744.23254671 52591749.45149608 26838799.91285678  8966713.64201406
 60585816.6007772  28920738.74380525 11253659.91650407 36826870.45355151
 28553553.02519245 23313187.49305714]
eta_min = 0.6590586238034565	eta_max = 0.6785433277513145
af = 0.005254252612989919	bf = 1.2178222333661453	zeta = 0.005779677874288912	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [1.43481355e-06 1.14376411e-05 1.39380888e-06 5.39498714e-08
 1.75274491e-05 1.90557908e-06 1.06717757e-07 3.78824463e-06
 1.65393723e-06 1.00078475e-06]
ene_total = [1.6846925  0.98717606 1.76700979 1.43963457 0.96585086 0.96820657
 1.43418346 1.3187243  1.89268496 0.94339011]
ti_comp = [0.59337097 0.64450365 0.58734795 0.61128643 0.64612477 0.6457962
 0.61168578 0.62016997 0.57815574 0.64760281]
ti_coms = [0.12324342 0.07211074 0.12926644 0.10532797 0.07048962 0.07081819
 0.10492862 0.09644442 0.13845866 0.06901158]
t_total = [27.69980698 27.69980698 27.69980698 27.69980698 27.69980698 27.69980698
 27.69980698 27.69980698 27.69980698 27.69980698]
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [5.81719976e-06 4.58713790e-05 5.65886160e-06 2.17856639e-07
 7.02726667e-05 7.64051325e-06 4.30902936e-07 1.52683660e-05
 6.72969396e-06 4.01128493e-06]
ene_total = [0.47660643 0.28050789 0.49988152 0.40714012 0.27518486 0.27403394
 0.40560472 0.37338368 0.53545424 0.26691045]
optimize_network iter = 1 obj = 3.7947078371699785
eta = 0.6785433277513145
freqs = [26965339.44106933 52213419.54907353 26809447.82144539  8932753.44599161
 60140450.82785589 28709062.73531702 11210552.63537973 36652528.58326961
 28553553.02519245 23138485.42973953]
eta_min = 0.6785433277513172	eta_max = 0.6785433277513145
af = 0.005187942505206725	bf = 1.2178222333661453	zeta = 0.005706736755727398	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [1.42967619e-06 1.12736748e-05 1.39076189e-06 5.35419898e-08
 1.72707080e-05 1.87778663e-06 1.05901756e-07 3.75246172e-06
 1.65393723e-06 9.85841782e-07]
ene_total = [1.6846918  0.98715364 1.76700937 1.43963452 0.96581577 0.96820277
 1.43418335 1.31871941 1.89268496 0.94338807]
ti_comp = [0.59337097 0.64450365 0.58734795 0.61128643 0.64612477 0.6457962
 0.61168578 0.62016997 0.57815574 0.64760281]
ti_coms = [0.12324342 0.07211074 0.12926644 0.10532797 0.07048962 0.07081819
 0.10492862 0.09644442 0.13845866 0.06901158]
t_total = [27.69980698 27.69980698 27.69980698 27.69980698 27.69980698 27.69980698
 27.69980698 27.69980698 27.69980698 27.69980698]
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [5.81719976e-06 4.58713790e-05 5.65886160e-06 2.17856639e-07
 7.02726667e-05 7.64051325e-06 4.30902936e-07 1.52683660e-05
 6.72969396e-06 4.01128493e-06]
ene_total = [0.47660643 0.28050789 0.49988152 0.40714012 0.27518486 0.27403394
 0.40560472 0.37338368 0.53545424 0.26691045]
optimize_network iter = 2 obj = 3.7947078371699785
eta = 0.6785433277513145
freqs = [26965339.44106933 52213419.54907353 26809447.82144539  8932753.44599161
 60140450.82785589 28709062.73531702 11210552.63537973 36652528.58326961
 28553553.02519245 23138485.42973953]
Done!
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [5.49709926e-06 4.33472348e-05 5.34747390e-06 2.05868737e-07
 6.64058034e-05 7.22008208e-06 4.07191829e-07 1.44282003e-05
 6.35938204e-06 3.79055772e-06]
ene_total = [0.01232984 0.00725442 0.01293199 0.010533   0.00711537 0.00708904
 0.01049327 0.00965887 0.01385222 0.00690495]
At round 46 energy consumption: 0.0981629748231883
At round 46 eta: 0.6785433277513145
At round 46 a_n: 12.082948048696464
At round 46 local rounds: 12.69876962905909
At round 46 global rounds: 38.653712827752926
gradient difference: 0.4250982701778412
train() client id: f_00000-0-0 loss: 1.000754  [   32/  126]
train() client id: f_00000-0-1 loss: 1.336813  [   64/  126]
train() client id: f_00000-0-2 loss: 1.266915  [   96/  126]
train() client id: f_00000-1-0 loss: 0.877018  [   32/  126]
train() client id: f_00000-1-1 loss: 1.063650  [   64/  126]
train() client id: f_00000-1-2 loss: 1.084713  [   96/  126]
train() client id: f_00000-2-0 loss: 0.974723  [   32/  126]
train() client id: f_00000-2-1 loss: 1.067150  [   64/  126]
train() client id: f_00000-2-2 loss: 1.001178  [   96/  126]
train() client id: f_00000-3-0 loss: 0.934092  [   32/  126]
train() client id: f_00000-3-1 loss: 0.927121  [   64/  126]
train() client id: f_00000-3-2 loss: 0.948684  [   96/  126]
train() client id: f_00000-4-0 loss: 0.823133  [   32/  126]
train() client id: f_00000-4-1 loss: 1.010164  [   64/  126]
train() client id: f_00000-4-2 loss: 0.990991  [   96/  126]
train() client id: f_00000-5-0 loss: 0.866351  [   32/  126]
train() client id: f_00000-5-1 loss: 0.821059  [   64/  126]
train() client id: f_00000-5-2 loss: 0.831838  [   96/  126]
train() client id: f_00000-6-0 loss: 0.810957  [   32/  126]
train() client id: f_00000-6-1 loss: 0.852975  [   64/  126]
train() client id: f_00000-6-2 loss: 0.819069  [   96/  126]
train() client id: f_00000-7-0 loss: 0.867907  [   32/  126]
train() client id: f_00000-7-1 loss: 0.795095  [   64/  126]
train() client id: f_00000-7-2 loss: 0.828045  [   96/  126]
train() client id: f_00000-8-0 loss: 0.942600  [   32/  126]
train() client id: f_00000-8-1 loss: 0.750004  [   64/  126]
train() client id: f_00000-8-2 loss: 0.784258  [   96/  126]
train() client id: f_00000-9-0 loss: 0.794969  [   32/  126]
train() client id: f_00000-9-1 loss: 0.659447  [   64/  126]
train() client id: f_00000-9-2 loss: 0.891685  [   96/  126]
train() client id: f_00000-10-0 loss: 0.818801  [   32/  126]
train() client id: f_00000-10-1 loss: 0.797217  [   64/  126]
train() client id: f_00000-10-2 loss: 0.818692  [   96/  126]
train() client id: f_00000-11-0 loss: 0.736744  [   32/  126]
train() client id: f_00000-11-1 loss: 0.844977  [   64/  126]
train() client id: f_00000-11-2 loss: 0.918160  [   96/  126]
train() client id: f_00001-0-0 loss: 0.412843  [   32/  265]
train() client id: f_00001-0-1 loss: 0.406834  [   64/  265]
train() client id: f_00001-0-2 loss: 0.480178  [   96/  265]
train() client id: f_00001-0-3 loss: 0.319341  [  128/  265]
train() client id: f_00001-0-4 loss: 0.517387  [  160/  265]
train() client id: f_00001-0-5 loss: 0.351694  [  192/  265]
train() client id: f_00001-0-6 loss: 0.458462  [  224/  265]
train() client id: f_00001-0-7 loss: 0.553673  [  256/  265]
train() client id: f_00001-1-0 loss: 0.546116  [   32/  265]
train() client id: f_00001-1-1 loss: 0.469104  [   64/  265]
train() client id: f_00001-1-2 loss: 0.356345  [   96/  265]
train() client id: f_00001-1-3 loss: 0.424724  [  128/  265]
train() client id: f_00001-1-4 loss: 0.321813  [  160/  265]
train() client id: f_00001-1-5 loss: 0.439409  [  192/  265]
train() client id: f_00001-1-6 loss: 0.473267  [  224/  265]
train() client id: f_00001-1-7 loss: 0.404594  [  256/  265]
train() client id: f_00001-2-0 loss: 0.510675  [   32/  265]
train() client id: f_00001-2-1 loss: 0.380805  [   64/  265]
train() client id: f_00001-2-2 loss: 0.495870  [   96/  265]
train() client id: f_00001-2-3 loss: 0.415861  [  128/  265]
train() client id: f_00001-2-4 loss: 0.361496  [  160/  265]
train() client id: f_00001-2-5 loss: 0.415336  [  192/  265]
train() client id: f_00001-2-6 loss: 0.373339  [  224/  265]
train() client id: f_00001-2-7 loss: 0.407895  [  256/  265]
train() client id: f_00001-3-0 loss: 0.373042  [   32/  265]
train() client id: f_00001-3-1 loss: 0.378653  [   64/  265]
train() client id: f_00001-3-2 loss: 0.459654  [   96/  265]
train() client id: f_00001-3-3 loss: 0.443259  [  128/  265]
train() client id: f_00001-3-4 loss: 0.365613  [  160/  265]
train() client id: f_00001-3-5 loss: 0.551532  [  192/  265]
train() client id: f_00001-3-6 loss: 0.385266  [  224/  265]
train() client id: f_00001-3-7 loss: 0.443214  [  256/  265]
train() client id: f_00001-4-0 loss: 0.428365  [   32/  265]
train() client id: f_00001-4-1 loss: 0.387084  [   64/  265]
train() client id: f_00001-4-2 loss: 0.479299  [   96/  265]
train() client id: f_00001-4-3 loss: 0.314710  [  128/  265]
train() client id: f_00001-4-4 loss: 0.322438  [  160/  265]
train() client id: f_00001-4-5 loss: 0.353925  [  192/  265]
train() client id: f_00001-4-6 loss: 0.450947  [  224/  265]
train() client id: f_00001-4-7 loss: 0.632315  [  256/  265]
train() client id: f_00001-5-0 loss: 0.340702  [   32/  265]
train() client id: f_00001-5-1 loss: 0.437087  [   64/  265]
train() client id: f_00001-5-2 loss: 0.501470  [   96/  265]
train() client id: f_00001-5-3 loss: 0.324648  [  128/  265]
train() client id: f_00001-5-4 loss: 0.512161  [  160/  265]
train() client id: f_00001-5-5 loss: 0.410516  [  192/  265]
train() client id: f_00001-5-6 loss: 0.392493  [  224/  265]
train() client id: f_00001-5-7 loss: 0.450866  [  256/  265]
train() client id: f_00001-6-0 loss: 0.470232  [   32/  265]
train() client id: f_00001-6-1 loss: 0.364939  [   64/  265]
train() client id: f_00001-6-2 loss: 0.324817  [   96/  265]
train() client id: f_00001-6-3 loss: 0.329454  [  128/  265]
train() client id: f_00001-6-4 loss: 0.452292  [  160/  265]
train() client id: f_00001-6-5 loss: 0.505300  [  192/  265]
train() client id: f_00001-6-6 loss: 0.480993  [  224/  265]
train() client id: f_00001-6-7 loss: 0.331346  [  256/  265]
train() client id: f_00001-7-0 loss: 0.370323  [   32/  265]
train() client id: f_00001-7-1 loss: 0.389015  [   64/  265]
train() client id: f_00001-7-2 loss: 0.415864  [   96/  265]
train() client id: f_00001-7-3 loss: 0.424302  [  128/  265]
train() client id: f_00001-7-4 loss: 0.364057  [  160/  265]
train() client id: f_00001-7-5 loss: 0.432866  [  192/  265]
train() client id: f_00001-7-6 loss: 0.561759  [  224/  265]
train() client id: f_00001-7-7 loss: 0.378050  [  256/  265]
train() client id: f_00001-8-0 loss: 0.312154  [   32/  265]
train() client id: f_00001-8-1 loss: 0.431673  [   64/  265]
train() client id: f_00001-8-2 loss: 0.521396  [   96/  265]
train() client id: f_00001-8-3 loss: 0.384646  [  128/  265]
train() client id: f_00001-8-4 loss: 0.413059  [  160/  265]
train() client id: f_00001-8-5 loss: 0.367366  [  192/  265]
train() client id: f_00001-8-6 loss: 0.376769  [  224/  265]
train() client id: f_00001-8-7 loss: 0.407822  [  256/  265]
train() client id: f_00001-9-0 loss: 0.447591  [   32/  265]
train() client id: f_00001-9-1 loss: 0.433173  [   64/  265]
train() client id: f_00001-9-2 loss: 0.465349  [   96/  265]
train() client id: f_00001-9-3 loss: 0.386707  [  128/  265]
train() client id: f_00001-9-4 loss: 0.325876  [  160/  265]
train() client id: f_00001-9-5 loss: 0.375665  [  192/  265]
train() client id: f_00001-9-6 loss: 0.446597  [  224/  265]
train() client id: f_00001-9-7 loss: 0.447361  [  256/  265]
train() client id: f_00001-10-0 loss: 0.400757  [   32/  265]
train() client id: f_00001-10-1 loss: 0.387576  [   64/  265]
train() client id: f_00001-10-2 loss: 0.514851  [   96/  265]
train() client id: f_00001-10-3 loss: 0.603691  [  128/  265]
train() client id: f_00001-10-4 loss: 0.366178  [  160/  265]
train() client id: f_00001-10-5 loss: 0.301350  [  192/  265]
train() client id: f_00001-10-6 loss: 0.370471  [  224/  265]
train() client id: f_00001-10-7 loss: 0.397426  [  256/  265]
train() client id: f_00001-11-0 loss: 0.399256  [   32/  265]
train() client id: f_00001-11-1 loss: 0.491392  [   64/  265]
train() client id: f_00001-11-2 loss: 0.320156  [   96/  265]
train() client id: f_00001-11-3 loss: 0.401193  [  128/  265]
train() client id: f_00001-11-4 loss: 0.392602  [  160/  265]
train() client id: f_00001-11-5 loss: 0.427639  [  192/  265]
train() client id: f_00001-11-6 loss: 0.401618  [  224/  265]
train() client id: f_00001-11-7 loss: 0.503301  [  256/  265]
train() client id: f_00002-0-0 loss: 1.158634  [   32/  124]
train() client id: f_00002-0-1 loss: 1.154898  [   64/  124]
train() client id: f_00002-0-2 loss: 1.233154  [   96/  124]
train() client id: f_00002-1-0 loss: 1.163536  [   32/  124]
train() client id: f_00002-1-1 loss: 1.223431  [   64/  124]
train() client id: f_00002-1-2 loss: 1.058449  [   96/  124]
train() client id: f_00002-2-0 loss: 1.055247  [   32/  124]
train() client id: f_00002-2-1 loss: 1.088439  [   64/  124]
train() client id: f_00002-2-2 loss: 1.127609  [   96/  124]
train() client id: f_00002-3-0 loss: 1.150435  [   32/  124]
train() client id: f_00002-3-1 loss: 1.073936  [   64/  124]
train() client id: f_00002-3-2 loss: 0.879889  [   96/  124]
train() client id: f_00002-4-0 loss: 0.851853  [   32/  124]
train() client id: f_00002-4-1 loss: 1.058376  [   64/  124]
train() client id: f_00002-4-2 loss: 1.084717  [   96/  124]
train() client id: f_00002-5-0 loss: 1.032857  [   32/  124]
train() client id: f_00002-5-1 loss: 1.031415  [   64/  124]
train() client id: f_00002-5-2 loss: 1.098740  [   96/  124]
train() client id: f_00002-6-0 loss: 0.876195  [   32/  124]
train() client id: f_00002-6-1 loss: 0.970386  [   64/  124]
train() client id: f_00002-6-2 loss: 1.079613  [   96/  124]
train() client id: f_00002-7-0 loss: 0.872654  [   32/  124]
train() client id: f_00002-7-1 loss: 1.190112  [   64/  124]
train() client id: f_00002-7-2 loss: 1.004905  [   96/  124]
train() client id: f_00002-8-0 loss: 0.885333  [   32/  124]
train() client id: f_00002-8-1 loss: 0.950954  [   64/  124]
train() client id: f_00002-8-2 loss: 1.112610  [   96/  124]
train() client id: f_00002-9-0 loss: 0.893102  [   32/  124]
train() client id: f_00002-9-1 loss: 1.165355  [   64/  124]
train() client id: f_00002-9-2 loss: 0.931429  [   96/  124]
train() client id: f_00002-10-0 loss: 0.984871  [   32/  124]
train() client id: f_00002-10-1 loss: 1.069631  [   64/  124]
train() client id: f_00002-10-2 loss: 0.944557  [   96/  124]
train() client id: f_00002-11-0 loss: 0.997041  [   32/  124]
train() client id: f_00002-11-1 loss: 0.960462  [   64/  124]
train() client id: f_00002-11-2 loss: 0.826588  [   96/  124]
train() client id: f_00003-0-0 loss: 0.641537  [   32/   43]
train() client id: f_00003-1-0 loss: 0.583243  [   32/   43]
train() client id: f_00003-2-0 loss: 0.669404  [   32/   43]
train() client id: f_00003-3-0 loss: 0.772181  [   32/   43]
train() client id: f_00003-4-0 loss: 0.669383  [   32/   43]
train() client id: f_00003-5-0 loss: 0.943215  [   32/   43]
train() client id: f_00003-6-0 loss: 0.741199  [   32/   43]
train() client id: f_00003-7-0 loss: 0.623131  [   32/   43]
train() client id: f_00003-8-0 loss: 0.582256  [   32/   43]
train() client id: f_00003-9-0 loss: 0.582481  [   32/   43]
train() client id: f_00003-10-0 loss: 0.691027  [   32/   43]
train() client id: f_00003-11-0 loss: 0.765647  [   32/   43]
train() client id: f_00004-0-0 loss: 0.849659  [   32/  306]
train() client id: f_00004-0-1 loss: 0.910949  [   64/  306]
train() client id: f_00004-0-2 loss: 0.900206  [   96/  306]
train() client id: f_00004-0-3 loss: 0.970002  [  128/  306]
train() client id: f_00004-0-4 loss: 0.984898  [  160/  306]
train() client id: f_00004-0-5 loss: 0.975507  [  192/  306]
train() client id: f_00004-0-6 loss: 0.945199  [  224/  306]
train() client id: f_00004-0-7 loss: 0.751033  [  256/  306]
train() client id: f_00004-0-8 loss: 0.711451  [  288/  306]
train() client id: f_00004-1-0 loss: 1.013899  [   32/  306]
train() client id: f_00004-1-1 loss: 0.795843  [   64/  306]
train() client id: f_00004-1-2 loss: 0.969515  [   96/  306]
train() client id: f_00004-1-3 loss: 0.943337  [  128/  306]
train() client id: f_00004-1-4 loss: 0.933966  [  160/  306]
train() client id: f_00004-1-5 loss: 0.753256  [  192/  306]
train() client id: f_00004-1-6 loss: 0.774141  [  224/  306]
train() client id: f_00004-1-7 loss: 0.766760  [  256/  306]
train() client id: f_00004-1-8 loss: 0.868283  [  288/  306]
train() client id: f_00004-2-0 loss: 0.865988  [   32/  306]
train() client id: f_00004-2-1 loss: 0.773372  [   64/  306]
train() client id: f_00004-2-2 loss: 0.950100  [   96/  306]
train() client id: f_00004-2-3 loss: 0.799671  [  128/  306]
train() client id: f_00004-2-4 loss: 0.895598  [  160/  306]
train() client id: f_00004-2-5 loss: 0.807248  [  192/  306]
train() client id: f_00004-2-6 loss: 0.721061  [  224/  306]
train() client id: f_00004-2-7 loss: 0.945819  [  256/  306]
train() client id: f_00004-2-8 loss: 1.089225  [  288/  306]
train() client id: f_00004-3-0 loss: 0.933718  [   32/  306]
train() client id: f_00004-3-1 loss: 0.878024  [   64/  306]
train() client id: f_00004-3-2 loss: 0.994959  [   96/  306]
train() client id: f_00004-3-3 loss: 0.917410  [  128/  306]
train() client id: f_00004-3-4 loss: 0.993746  [  160/  306]
train() client id: f_00004-3-5 loss: 0.665029  [  192/  306]
train() client id: f_00004-3-6 loss: 0.797208  [  224/  306]
train() client id: f_00004-3-7 loss: 0.842736  [  256/  306]
train() client id: f_00004-3-8 loss: 0.869725  [  288/  306]
train() client id: f_00004-4-0 loss: 0.846213  [   32/  306]
train() client id: f_00004-4-1 loss: 0.891957  [   64/  306]
train() client id: f_00004-4-2 loss: 0.815820  [   96/  306]
train() client id: f_00004-4-3 loss: 0.929898  [  128/  306]
train() client id: f_00004-4-4 loss: 0.748275  [  160/  306]
train() client id: f_00004-4-5 loss: 0.946340  [  192/  306]
train() client id: f_00004-4-6 loss: 0.815003  [  224/  306]
train() client id: f_00004-4-7 loss: 0.843341  [  256/  306]
train() client id: f_00004-4-8 loss: 0.812718  [  288/  306]
train() client id: f_00004-5-0 loss: 1.012018  [   32/  306]
train() client id: f_00004-5-1 loss: 0.741931  [   64/  306]
train() client id: f_00004-5-2 loss: 0.846324  [   96/  306]
train() client id: f_00004-5-3 loss: 0.902782  [  128/  306]
train() client id: f_00004-5-4 loss: 0.815455  [  160/  306]
train() client id: f_00004-5-5 loss: 0.949757  [  192/  306]
train() client id: f_00004-5-6 loss: 0.858087  [  224/  306]
train() client id: f_00004-5-7 loss: 0.933124  [  256/  306]
train() client id: f_00004-5-8 loss: 0.796839  [  288/  306]
train() client id: f_00004-6-0 loss: 0.794409  [   32/  306]
train() client id: f_00004-6-1 loss: 0.874380  [   64/  306]
train() client id: f_00004-6-2 loss: 0.873008  [   96/  306]
train() client id: f_00004-6-3 loss: 0.793563  [  128/  306]
train() client id: f_00004-6-4 loss: 0.895271  [  160/  306]
train() client id: f_00004-6-5 loss: 0.845134  [  192/  306]
train() client id: f_00004-6-6 loss: 0.757606  [  224/  306]
train() client id: f_00004-6-7 loss: 0.907188  [  256/  306]
train() client id: f_00004-6-8 loss: 0.931223  [  288/  306]
train() client id: f_00004-7-0 loss: 0.847179  [   32/  306]
train() client id: f_00004-7-1 loss: 0.941613  [   64/  306]
train() client id: f_00004-7-2 loss: 0.904068  [   96/  306]
train() client id: f_00004-7-3 loss: 0.911905  [  128/  306]
train() client id: f_00004-7-4 loss: 0.812908  [  160/  306]
train() client id: f_00004-7-5 loss: 0.743150  [  192/  306]
train() client id: f_00004-7-6 loss: 0.904269  [  224/  306]
train() client id: f_00004-7-7 loss: 0.753259  [  256/  306]
train() client id: f_00004-7-8 loss: 0.885316  [  288/  306]
train() client id: f_00004-8-0 loss: 0.939731  [   32/  306]
train() client id: f_00004-8-1 loss: 0.825895  [   64/  306]
train() client id: f_00004-8-2 loss: 0.804228  [   96/  306]
train() client id: f_00004-8-3 loss: 0.857162  [  128/  306]
train() client id: f_00004-8-4 loss: 0.873821  [  160/  306]
train() client id: f_00004-8-5 loss: 0.829167  [  192/  306]
train() client id: f_00004-8-6 loss: 0.842244  [  224/  306]
train() client id: f_00004-8-7 loss: 0.964012  [  256/  306]
train() client id: f_00004-8-8 loss: 0.875184  [  288/  306]
train() client id: f_00004-9-0 loss: 0.802758  [   32/  306]
train() client id: f_00004-9-1 loss: 0.869097  [   64/  306]
train() client id: f_00004-9-2 loss: 0.977420  [   96/  306]
train() client id: f_00004-9-3 loss: 0.906784  [  128/  306]
train() client id: f_00004-9-4 loss: 0.810738  [  160/  306]
train() client id: f_00004-9-5 loss: 0.973621  [  192/  306]
train() client id: f_00004-9-6 loss: 0.844784  [  224/  306]
train() client id: f_00004-9-7 loss: 0.847268  [  256/  306]
train() client id: f_00004-9-8 loss: 0.769625  [  288/  306]
train() client id: f_00004-10-0 loss: 0.740321  [   32/  306]
train() client id: f_00004-10-1 loss: 0.733534  [   64/  306]
train() client id: f_00004-10-2 loss: 0.775873  [   96/  306]
train() client id: f_00004-10-3 loss: 0.903027  [  128/  306]
train() client id: f_00004-10-4 loss: 1.027128  [  160/  306]
train() client id: f_00004-10-5 loss: 0.884550  [  192/  306]
train() client id: f_00004-10-6 loss: 0.797323  [  224/  306]
train() client id: f_00004-10-7 loss: 0.914312  [  256/  306]
train() client id: f_00004-10-8 loss: 0.902513  [  288/  306]
train() client id: f_00004-11-0 loss: 0.942569  [   32/  306]
train() client id: f_00004-11-1 loss: 0.757004  [   64/  306]
train() client id: f_00004-11-2 loss: 0.873336  [   96/  306]
train() client id: f_00004-11-3 loss: 0.832613  [  128/  306]
train() client id: f_00004-11-4 loss: 0.962910  [  160/  306]
train() client id: f_00004-11-5 loss: 0.884945  [  192/  306]
train() client id: f_00004-11-6 loss: 0.833346  [  224/  306]
train() client id: f_00004-11-7 loss: 0.867254  [  256/  306]
train() client id: f_00004-11-8 loss: 0.776418  [  288/  306]
train() client id: f_00005-0-0 loss: 0.508262  [   32/  146]
train() client id: f_00005-0-1 loss: 0.193816  [   64/  146]
train() client id: f_00005-0-2 loss: 0.429997  [   96/  146]
train() client id: f_00005-0-3 loss: 0.627930  [  128/  146]
train() client id: f_00005-1-0 loss: 0.454669  [   32/  146]
train() client id: f_00005-1-1 loss: 0.449533  [   64/  146]
train() client id: f_00005-1-2 loss: 0.694988  [   96/  146]
train() client id: f_00005-1-3 loss: 0.330311  [  128/  146]
train() client id: f_00005-2-0 loss: 0.473285  [   32/  146]
train() client id: f_00005-2-1 loss: 0.410578  [   64/  146]
train() client id: f_00005-2-2 loss: 0.460155  [   96/  146]
train() client id: f_00005-2-3 loss: 0.340128  [  128/  146]
train() client id: f_00005-3-0 loss: 0.551810  [   32/  146]
train() client id: f_00005-3-1 loss: 0.305525  [   64/  146]
train() client id: f_00005-3-2 loss: 0.194903  [   96/  146]
train() client id: f_00005-3-3 loss: 0.505042  [  128/  146]
train() client id: f_00005-4-0 loss: 0.357963  [   32/  146]
train() client id: f_00005-4-1 loss: 0.397896  [   64/  146]
train() client id: f_00005-4-2 loss: 0.590775  [   96/  146]
train() client id: f_00005-4-3 loss: 0.315989  [  128/  146]
train() client id: f_00005-5-0 loss: 0.300553  [   32/  146]
train() client id: f_00005-5-1 loss: 0.282465  [   64/  146]
train() client id: f_00005-5-2 loss: 0.414346  [   96/  146]
train() client id: f_00005-5-3 loss: 0.664857  [  128/  146]
train() client id: f_00005-6-0 loss: 0.503544  [   32/  146]
train() client id: f_00005-6-1 loss: 0.597337  [   64/  146]
train() client id: f_00005-6-2 loss: 0.493057  [   96/  146]
train() client id: f_00005-6-3 loss: 0.280521  [  128/  146]
train() client id: f_00005-7-0 loss: 0.483098  [   32/  146]
train() client id: f_00005-7-1 loss: 0.321696  [   64/  146]
train() client id: f_00005-7-2 loss: 0.563425  [   96/  146]
train() client id: f_00005-7-3 loss: 0.507657  [  128/  146]
train() client id: f_00005-8-0 loss: 0.364935  [   32/  146]
train() client id: f_00005-8-1 loss: 0.245519  [   64/  146]
train() client id: f_00005-8-2 loss: 0.366097  [   96/  146]
train() client id: f_00005-8-3 loss: 0.704162  [  128/  146]
train() client id: f_00005-9-0 loss: 0.322239  [   32/  146]
train() client id: f_00005-9-1 loss: 0.401472  [   64/  146]
train() client id: f_00005-9-2 loss: 0.246907  [   96/  146]
train() client id: f_00005-9-3 loss: 0.739992  [  128/  146]
train() client id: f_00005-10-0 loss: 0.459833  [   32/  146]
train() client id: f_00005-10-1 loss: 0.323928  [   64/  146]
train() client id: f_00005-10-2 loss: 0.218760  [   96/  146]
train() client id: f_00005-10-3 loss: 0.713877  [  128/  146]
train() client id: f_00005-11-0 loss: 0.339558  [   32/  146]
train() client id: f_00005-11-1 loss: 0.386785  [   64/  146]
train() client id: f_00005-11-2 loss: 0.377160  [   96/  146]
train() client id: f_00005-11-3 loss: 0.542279  [  128/  146]
train() client id: f_00006-0-0 loss: 0.518880  [   32/   54]
train() client id: f_00006-1-0 loss: 0.539090  [   32/   54]
train() client id: f_00006-2-0 loss: 0.534842  [   32/   54]
train() client id: f_00006-3-0 loss: 0.532848  [   32/   54]
train() client id: f_00006-4-0 loss: 0.560383  [   32/   54]
train() client id: f_00006-5-0 loss: 0.548092  [   32/   54]
train() client id: f_00006-6-0 loss: 0.541037  [   32/   54]
train() client id: f_00006-7-0 loss: 0.535402  [   32/   54]
train() client id: f_00006-8-0 loss: 0.585298  [   32/   54]
train() client id: f_00006-9-0 loss: 0.518581  [   32/   54]
train() client id: f_00006-10-0 loss: 0.579280  [   32/   54]
train() client id: f_00006-11-0 loss: 0.569568  [   32/   54]
train() client id: f_00007-0-0 loss: 0.534486  [   32/  179]
train() client id: f_00007-0-1 loss: 0.647073  [   64/  179]
train() client id: f_00007-0-2 loss: 0.572488  [   96/  179]
train() client id: f_00007-0-3 loss: 0.504230  [  128/  179]
train() client id: f_00007-0-4 loss: 0.544580  [  160/  179]
train() client id: f_00007-1-0 loss: 0.457739  [   32/  179]
train() client id: f_00007-1-1 loss: 0.569315  [   64/  179]
train() client id: f_00007-1-2 loss: 0.680880  [   96/  179]
train() client id: f_00007-1-3 loss: 0.740143  [  128/  179]
train() client id: f_00007-1-4 loss: 0.502421  [  160/  179]
train() client id: f_00007-2-0 loss: 0.633385  [   32/  179]
train() client id: f_00007-2-1 loss: 0.478742  [   64/  179]
train() client id: f_00007-2-2 loss: 0.513221  [   96/  179]
train() client id: f_00007-2-3 loss: 0.695231  [  128/  179]
train() client id: f_00007-2-4 loss: 0.663929  [  160/  179]
train() client id: f_00007-3-0 loss: 0.633856  [   32/  179]
train() client id: f_00007-3-1 loss: 0.504738  [   64/  179]
train() client id: f_00007-3-2 loss: 0.505304  [   96/  179]
train() client id: f_00007-3-3 loss: 0.558510  [  128/  179]
train() client id: f_00007-3-4 loss: 0.680557  [  160/  179]
train() client id: f_00007-4-0 loss: 0.688783  [   32/  179]
train() client id: f_00007-4-1 loss: 0.400142  [   64/  179]
train() client id: f_00007-4-2 loss: 0.606657  [   96/  179]
train() client id: f_00007-4-3 loss: 0.548899  [  128/  179]
train() client id: f_00007-4-4 loss: 0.369290  [  160/  179]
train() client id: f_00007-5-0 loss: 0.644529  [   32/  179]
train() client id: f_00007-5-1 loss: 0.632178  [   64/  179]
train() client id: f_00007-5-2 loss: 0.548445  [   96/  179]
train() client id: f_00007-5-3 loss: 0.474020  [  128/  179]
train() client id: f_00007-5-4 loss: 0.610728  [  160/  179]
train() client id: f_00007-6-0 loss: 0.618018  [   32/  179]
train() client id: f_00007-6-1 loss: 0.442754  [   64/  179]
train() client id: f_00007-6-2 loss: 0.553496  [   96/  179]
train() client id: f_00007-6-3 loss: 0.617675  [  128/  179]
train() client id: f_00007-6-4 loss: 0.607966  [  160/  179]
train() client id: f_00007-7-0 loss: 0.947192  [   32/  179]
train() client id: f_00007-7-1 loss: 0.436917  [   64/  179]
train() client id: f_00007-7-2 loss: 0.443354  [   96/  179]
train() client id: f_00007-7-3 loss: 0.484011  [  128/  179]
train() client id: f_00007-7-4 loss: 0.594007  [  160/  179]
train() client id: f_00007-8-0 loss: 0.521787  [   32/  179]
train() client id: f_00007-8-1 loss: 0.582838  [   64/  179]
train() client id: f_00007-8-2 loss: 0.766196  [   96/  179]
train() client id: f_00007-8-3 loss: 0.497090  [  128/  179]
train() client id: f_00007-8-4 loss: 0.543156  [  160/  179]
train() client id: f_00007-9-0 loss: 0.650429  [   32/  179]
train() client id: f_00007-9-1 loss: 0.486227  [   64/  179]
train() client id: f_00007-9-2 loss: 0.423973  [   96/  179]
train() client id: f_00007-9-3 loss: 0.657026  [  128/  179]
train() client id: f_00007-9-4 loss: 0.474639  [  160/  179]
train() client id: f_00007-10-0 loss: 0.648975  [   32/  179]
train() client id: f_00007-10-1 loss: 0.640099  [   64/  179]
train() client id: f_00007-10-2 loss: 0.413486  [   96/  179]
train() client id: f_00007-10-3 loss: 0.597060  [  128/  179]
train() client id: f_00007-10-4 loss: 0.609422  [  160/  179]
train() client id: f_00007-11-0 loss: 0.544187  [   32/  179]
train() client id: f_00007-11-1 loss: 0.584114  [   64/  179]
train() client id: f_00007-11-2 loss: 0.857600  [   96/  179]
train() client id: f_00007-11-3 loss: 0.399775  [  128/  179]
train() client id: f_00007-11-4 loss: 0.446769  [  160/  179]
train() client id: f_00008-0-0 loss: 0.648695  [   32/  130]
train() client id: f_00008-0-1 loss: 0.708603  [   64/  130]
train() client id: f_00008-0-2 loss: 0.663592  [   96/  130]
train() client id: f_00008-0-3 loss: 0.559868  [  128/  130]
train() client id: f_00008-1-0 loss: 0.537260  [   32/  130]
train() client id: f_00008-1-1 loss: 0.677627  [   64/  130]
train() client id: f_00008-1-2 loss: 0.768248  [   96/  130]
train() client id: f_00008-1-3 loss: 0.597975  [  128/  130]
train() client id: f_00008-2-0 loss: 0.682504  [   32/  130]
train() client id: f_00008-2-1 loss: 0.733902  [   64/  130]
train() client id: f_00008-2-2 loss: 0.584832  [   96/  130]
train() client id: f_00008-2-3 loss: 0.584879  [  128/  130]
train() client id: f_00008-3-0 loss: 0.620022  [   32/  130]
train() client id: f_00008-3-1 loss: 0.583826  [   64/  130]
train() client id: f_00008-3-2 loss: 0.634878  [   96/  130]
train() client id: f_00008-3-3 loss: 0.734838  [  128/  130]
train() client id: f_00008-4-0 loss: 0.618458  [   32/  130]
train() client id: f_00008-4-1 loss: 0.598091  [   64/  130]
train() client id: f_00008-4-2 loss: 0.694644  [   96/  130]
train() client id: f_00008-4-3 loss: 0.652682  [  128/  130]
train() client id: f_00008-5-0 loss: 0.551403  [   32/  130]
train() client id: f_00008-5-1 loss: 0.725370  [   64/  130]
train() client id: f_00008-5-2 loss: 0.657827  [   96/  130]
train() client id: f_00008-5-3 loss: 0.654607  [  128/  130]
train() client id: f_00008-6-0 loss: 0.679898  [   32/  130]
train() client id: f_00008-6-1 loss: 0.583423  [   64/  130]
train() client id: f_00008-6-2 loss: 0.719789  [   96/  130]
train() client id: f_00008-6-3 loss: 0.617329  [  128/  130]
train() client id: f_00008-7-0 loss: 0.643570  [   32/  130]
train() client id: f_00008-7-1 loss: 0.574846  [   64/  130]
train() client id: f_00008-7-2 loss: 0.672281  [   96/  130]
train() client id: f_00008-7-3 loss: 0.668887  [  128/  130]
train() client id: f_00008-8-0 loss: 0.820412  [   32/  130]
train() client id: f_00008-8-1 loss: 0.615802  [   64/  130]
train() client id: f_00008-8-2 loss: 0.548925  [   96/  130]
train() client id: f_00008-8-3 loss: 0.614740  [  128/  130]
train() client id: f_00008-9-0 loss: 0.628062  [   32/  130]
train() client id: f_00008-9-1 loss: 0.682093  [   64/  130]
train() client id: f_00008-9-2 loss: 0.626436  [   96/  130]
train() client id: f_00008-9-3 loss: 0.658419  [  128/  130]
train() client id: f_00008-10-0 loss: 0.672539  [   32/  130]
train() client id: f_00008-10-1 loss: 0.566330  [   64/  130]
train() client id: f_00008-10-2 loss: 0.622957  [   96/  130]
train() client id: f_00008-10-3 loss: 0.669751  [  128/  130]
train() client id: f_00008-11-0 loss: 0.694038  [   32/  130]
train() client id: f_00008-11-1 loss: 0.708678  [   64/  130]
train() client id: f_00008-11-2 loss: 0.571751  [   96/  130]
train() client id: f_00008-11-3 loss: 0.619322  [  128/  130]
train() client id: f_00009-0-0 loss: 0.812153  [   32/  118]
train() client id: f_00009-0-1 loss: 0.869437  [   64/  118]
train() client id: f_00009-0-2 loss: 1.002204  [   96/  118]
train() client id: f_00009-1-0 loss: 1.036728  [   32/  118]
train() client id: f_00009-1-1 loss: 0.873830  [   64/  118]
train() client id: f_00009-1-2 loss: 0.638945  [   96/  118]
train() client id: f_00009-2-0 loss: 0.791665  [   32/  118]
train() client id: f_00009-2-1 loss: 0.749806  [   64/  118]
train() client id: f_00009-2-2 loss: 0.844462  [   96/  118]
train() client id: f_00009-3-0 loss: 0.746832  [   32/  118]
train() client id: f_00009-3-1 loss: 0.777867  [   64/  118]
train() client id: f_00009-3-2 loss: 0.818555  [   96/  118]
train() client id: f_00009-4-0 loss: 0.773882  [   32/  118]
train() client id: f_00009-4-1 loss: 0.755338  [   64/  118]
train() client id: f_00009-4-2 loss: 0.751643  [   96/  118]
train() client id: f_00009-5-0 loss: 0.826285  [   32/  118]
train() client id: f_00009-5-1 loss: 0.721738  [   64/  118]
train() client id: f_00009-5-2 loss: 0.641554  [   96/  118]
train() client id: f_00009-6-0 loss: 0.774808  [   32/  118]
train() client id: f_00009-6-1 loss: 0.569855  [   64/  118]
train() client id: f_00009-6-2 loss: 0.746289  [   96/  118]
train() client id: f_00009-7-0 loss: 0.600092  [   32/  118]
train() client id: f_00009-7-1 loss: 0.750983  [   64/  118]
train() client id: f_00009-7-2 loss: 0.669573  [   96/  118]
train() client id: f_00009-8-0 loss: 0.522005  [   32/  118]
train() client id: f_00009-8-1 loss: 0.749590  [   64/  118]
train() client id: f_00009-8-2 loss: 0.697150  [   96/  118]
train() client id: f_00009-9-0 loss: 0.736640  [   32/  118]
train() client id: f_00009-9-1 loss: 0.663382  [   64/  118]
train() client id: f_00009-9-2 loss: 0.535852  [   96/  118]
train() client id: f_00009-10-0 loss: 0.776580  [   32/  118]
train() client id: f_00009-10-1 loss: 0.526419  [   64/  118]
train() client id: f_00009-10-2 loss: 0.509713  [   96/  118]
train() client id: f_00009-11-0 loss: 0.589934  [   32/  118]
train() client id: f_00009-11-1 loss: 0.472957  [   64/  118]
train() client id: f_00009-11-2 loss: 0.643254  [   96/  118]
At round 46 accuracy: 0.6472148541114059
At round 46 training accuracy: 0.590878604963112
At round 46 training loss: 0.8341943200765914
update_location
xs = [  -3.9056584     4.20031788  250.00902392   18.81129433    0.97929623
    3.95640986 -212.44319194 -191.32485185  234.66397685 -177.06087855]
ys = [ 242.5879595   225.55583871    1.32061395 -212.45517586  204.35018685
  187.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [262.41983969 246.76523063 269.26985732 235.56541882 227.50814905
 212.81402303 234.81695071 215.88393917 255.68701962 203.38772473]
dists_bs = [183.9603381  186.19518456 459.12656158 433.22549684 178.41005064
 179.41460111 181.52858639 174.94100506 438.8725234  171.01093665]
uav_gains = [5.77502052e-12 7.90037418e-12 5.00729947e-12 9.71837173e-12
 1.11658603e-11 1.41063225e-11 9.84803495e-12 1.34576919e-11
 6.62667355e-12 1.62319850e-11]
bs_gains = [5.03577512e-11 4.86835724e-11 3.88940630e-12 4.57609321e-12
 5.48681098e-11 5.40122526e-11 5.22694600e-11 5.79692334e-11
 4.41312865e-12 6.17770556e-11]
Round 47
-------------------------------
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.7932913   9.84287299  4.72759004  1.71436679 11.34993316  5.46069336
  2.11875437  6.70878283  4.95608411  4.42774153]
obj_prev = 56.1001104698108
eta_min = 5.969830954362688e-20	eta_max = 0.9382327126425405
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 12.978344451772108	eta = 0.9090909090909091
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 25.625037120999544	eta = 0.4604284044719558
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 19.192856434060143	eta = 0.6147336638864522
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 18.0285169675076	eta = 0.6544351361468406
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 17.963027394397052	eta = 0.6568210745943969
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 17.962800013672382	eta = 0.6568293889135345
eta = 0.6568293889135345
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [0.0346847  0.07294799 0.03413415 0.01183684 0.08423428 0.04019021
 0.01486487 0.0492743  0.03578581 0.0324825 ]
ene_total = [1.66925633 2.82340307 1.67271292 0.79630461 3.21465228 1.66466235
 0.90143074 2.08079959 1.75289976 1.38667837]
ti_comp = [0.65804615 0.71288711 0.65155892 0.67749658 0.71463044 0.71440598
 0.67791976 0.68705877 0.6453014  0.71627974]
ti_coms = [0.12724232 0.07240136 0.13372954 0.10779188 0.07065803 0.07088248
 0.1073687  0.09822969 0.13998706 0.06900873]
t_total = [27.64980278 27.64980278 27.64980278 27.64980278 27.64980278 27.64980278
 27.64980278 27.64980278 27.64980278 27.64980278]
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [6.02256253e-06 4.77395682e-05 5.85517725e-06 2.25825789e-07
 7.31448314e-05 7.94970091e-06 4.46690869e-07 1.58399461e-05
 6.87839679e-06 4.17506343e-06]
ene_total = [0.44822894 0.2566042  0.4710644  0.37954037 0.25136048 0.24985529
 0.37805813 0.34642191 0.493133   0.24312494]
optimize_network iter = 0 obj = 3.5173916587449052
eta = 0.6568293889135345
freqs = [26354310.22875877 51163774.23769676 26194218.65211249  8735722.05600123
 58935554.78523942 28128413.29923534 10963593.51009882 35858870.20763274
 27727978.21502854 22674452.1605431 ]
eta_min = 0.6568293889135383	eta_max = 0.6851522482532448
af = 0.0048373581457900995	bf = 1.2050321534900845	zeta = 0.00532109396036911	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [1.36561793e-06 1.08249620e-05 1.32766327e-06 5.12060679e-08
 1.65856133e-05 1.80259716e-06 1.01287294e-07 3.59171272e-06
 1.55967862e-06 9.46696933e-07]
ene_total = [1.69139003 0.96374279 1.77760827 1.4326949  0.9413374  0.94235589
 1.42707694 1.30607207 1.86080934 0.91733759]
ti_comp = [0.59323403 0.64807499 0.5867468  0.61268446 0.64981832 0.64959386
 0.61310764 0.62224665 0.58048928 0.65146762]
ti_coms = [0.12724232 0.07240136 0.13372954 0.10779188 0.07065803 0.07088248
 0.1073687  0.09822969 0.13998706 0.06900873]
t_total = [27.64980278 27.64980278 27.64980278 27.64980278 27.64980278 27.64980278
 27.64980278 27.64980278 27.64980278 27.64980278]
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [5.39431995e-06 4.20498333e-05 5.25582343e-06 2.01005902e-07
 6.43958017e-05 6.99925668e-06 3.97543809e-07 1.40575984e-05
 6.18755144e-06 3.67398631e-06]
ene_total = [0.4885263  0.27946926 0.51341709 0.41368185 0.27363643 0.27229513
 0.41206534 0.3775167  0.53746741 0.26497658]
optimize_network iter = 1 obj = 3.8330520913049613
eta = 0.6851522482532448
freqs = [26297444.52410097 50627871.73999517 26166161.04026021  8689617.22720056
 58304024.4554098  27827872.27421467 10905010.46336445 35617179.03212754
 27727978.21502853 22426331.23656411]
eta_min = 0.6851522482532453	eta_max = 0.6851522482532441
af = 0.0047490407946122245	bf = 1.2050321534900845	zeta = 0.005223944874073448	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [1.35973100e-06 1.05993827e-05 1.32482057e-06 5.06669901e-08
 1.62320679e-05 1.76428286e-06 1.00207745e-07 3.54345912e-06
 1.55967862e-06 9.26091353e-07]
ene_total = [1.69138925 0.96371281 1.77760789 1.43269482 0.94129041 0.9423508
 1.4270768  1.30606566 1.86080934 0.91733486]
ti_comp = [0.59323403 0.64807499 0.5867468  0.61268446 0.64981832 0.64959386
 0.61310764 0.62224665 0.58048928 0.65146762]
ti_coms = [0.12724232 0.07240136 0.13372954 0.10779188 0.07065803 0.07088248
 0.1073687  0.09822969 0.13998706 0.06900873]
t_total = [27.64980278 27.64980278 27.64980278 27.64980278 27.64980278 27.64980278
 27.64980278 27.64980278 27.64980278 27.64980278]
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [5.39431995e-06 4.20498333e-05 5.25582343e-06 2.01005902e-07
 6.43958017e-05 6.99925668e-06 3.97543809e-07 1.40575984e-05
 6.18755144e-06 3.67398631e-06]
ene_total = [0.4885263  0.27946926 0.51341709 0.41368185 0.27363643 0.27229513
 0.41206534 0.3775167  0.53746741 0.26497658]
optimize_network iter = 2 obj = 3.8330520913049524
eta = 0.6851522482532441
freqs = [26297444.52410097 50627871.73999519 26166161.04026021  8689617.22720056
 58304024.45540981 27827872.27421467 10905010.46336445 35617179.03212754
 27727978.21502853 22426331.23656412]
Done!
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [5.22816025e-06 4.07545842e-05 5.09392980e-06 1.94814375e-07
 6.24122362e-05 6.78366056e-06 3.85298380e-07 1.36245862e-05
 5.99695805e-06 3.56081756e-06]
ene_total = [0.01272946 0.00728089 0.01337805 0.01077938 0.00712821 0.00709503
 0.01073726 0.00983659 0.0140047  0.00690443]
At round 47 energy consumption: 0.09987401390775219
At round 47 eta: 0.6851522482532441
At round 47 a_n: 11.740402201727147
At round 47 local rounds: 12.381380128537735
At round 47 global rounds: 38.37711395955986
gradient difference: 0.4784102439880371
train() client id: f_00000-0-0 loss: 0.861435  [   32/  126]
train() client id: f_00000-0-1 loss: 1.045741  [   64/  126]
train() client id: f_00000-0-2 loss: 1.229020  [   96/  126]
train() client id: f_00000-1-0 loss: 0.936157  [   32/  126]
train() client id: f_00000-1-1 loss: 0.941079  [   64/  126]
train() client id: f_00000-1-2 loss: 1.072726  [   96/  126]
train() client id: f_00000-2-0 loss: 0.934563  [   32/  126]
train() client id: f_00000-2-1 loss: 0.757908  [   64/  126]
train() client id: f_00000-2-2 loss: 0.857448  [   96/  126]
train() client id: f_00000-3-0 loss: 0.775154  [   32/  126]
train() client id: f_00000-3-1 loss: 0.979250  [   64/  126]
train() client id: f_00000-3-2 loss: 0.776000  [   96/  126]
train() client id: f_00000-4-0 loss: 0.854077  [   32/  126]
train() client id: f_00000-4-1 loss: 0.789812  [   64/  126]
train() client id: f_00000-4-2 loss: 0.861856  [   96/  126]
train() client id: f_00000-5-0 loss: 0.641362  [   32/  126]
train() client id: f_00000-5-1 loss: 0.833364  [   64/  126]
train() client id: f_00000-5-2 loss: 0.866067  [   96/  126]
train() client id: f_00000-6-0 loss: 0.818747  [   32/  126]
train() client id: f_00000-6-1 loss: 0.701066  [   64/  126]
train() client id: f_00000-6-2 loss: 0.861464  [   96/  126]
train() client id: f_00000-7-0 loss: 0.642366  [   32/  126]
train() client id: f_00000-7-1 loss: 0.845700  [   64/  126]
train() client id: f_00000-7-2 loss: 0.811127  [   96/  126]
train() client id: f_00000-8-0 loss: 0.639635  [   32/  126]
train() client id: f_00000-8-1 loss: 0.799837  [   64/  126]
train() client id: f_00000-8-2 loss: 0.752275  [   96/  126]
train() client id: f_00000-9-0 loss: 0.700455  [   32/  126]
train() client id: f_00000-9-1 loss: 0.859856  [   64/  126]
train() client id: f_00000-9-2 loss: 0.714919  [   96/  126]
train() client id: f_00000-10-0 loss: 0.632872  [   32/  126]
train() client id: f_00000-10-1 loss: 0.834535  [   64/  126]
train() client id: f_00000-10-2 loss: 0.829323  [   96/  126]
train() client id: f_00000-11-0 loss: 0.624731  [   32/  126]
train() client id: f_00000-11-1 loss: 0.701580  [   64/  126]
train() client id: f_00000-11-2 loss: 0.801771  [   96/  126]
train() client id: f_00001-0-0 loss: 0.292646  [   32/  265]
train() client id: f_00001-0-1 loss: 0.465912  [   64/  265]
train() client id: f_00001-0-2 loss: 0.378331  [   96/  265]
train() client id: f_00001-0-3 loss: 0.391217  [  128/  265]
train() client id: f_00001-0-4 loss: 0.249201  [  160/  265]
train() client id: f_00001-0-5 loss: 0.306664  [  192/  265]
train() client id: f_00001-0-6 loss: 0.305611  [  224/  265]
train() client id: f_00001-0-7 loss: 0.255424  [  256/  265]
train() client id: f_00001-1-0 loss: 0.260187  [   32/  265]
train() client id: f_00001-1-1 loss: 0.418720  [   64/  265]
train() client id: f_00001-1-2 loss: 0.307843  [   96/  265]
train() client id: f_00001-1-3 loss: 0.253641  [  128/  265]
train() client id: f_00001-1-4 loss: 0.308655  [  160/  265]
train() client id: f_00001-1-5 loss: 0.209626  [  192/  265]
train() client id: f_00001-1-6 loss: 0.388317  [  224/  265]
train() client id: f_00001-1-7 loss: 0.397218  [  256/  265]
train() client id: f_00001-2-0 loss: 0.326849  [   32/  265]
train() client id: f_00001-2-1 loss: 0.284406  [   64/  265]
train() client id: f_00001-2-2 loss: 0.195260  [   96/  265]
train() client id: f_00001-2-3 loss: 0.331420  [  128/  265]
train() client id: f_00001-2-4 loss: 0.328904  [  160/  265]
train() client id: f_00001-2-5 loss: 0.355941  [  192/  265]
train() client id: f_00001-2-6 loss: 0.205106  [  224/  265]
train() client id: f_00001-2-7 loss: 0.387177  [  256/  265]
train() client id: f_00001-3-0 loss: 0.225427  [   32/  265]
train() client id: f_00001-3-1 loss: 0.220779  [   64/  265]
train() client id: f_00001-3-2 loss: 0.364256  [   96/  265]
train() client id: f_00001-3-3 loss: 0.406094  [  128/  265]
train() client id: f_00001-3-4 loss: 0.290884  [  160/  265]
train() client id: f_00001-3-5 loss: 0.326406  [  192/  265]
train() client id: f_00001-3-6 loss: 0.352686  [  224/  265]
train() client id: f_00001-3-7 loss: 0.221508  [  256/  265]
train() client id: f_00001-4-0 loss: 0.271423  [   32/  265]
train() client id: f_00001-4-1 loss: 0.238646  [   64/  265]
train() client id: f_00001-4-2 loss: 0.224742  [   96/  265]
train() client id: f_00001-4-3 loss: 0.338026  [  128/  265]
train() client id: f_00001-4-4 loss: 0.270577  [  160/  265]
train() client id: f_00001-4-5 loss: 0.444560  [  192/  265]
train() client id: f_00001-4-6 loss: 0.384988  [  224/  265]
train() client id: f_00001-4-7 loss: 0.212408  [  256/  265]
train() client id: f_00001-5-0 loss: 0.361980  [   32/  265]
train() client id: f_00001-5-1 loss: 0.340251  [   64/  265]
train() client id: f_00001-5-2 loss: 0.248441  [   96/  265]
train() client id: f_00001-5-3 loss: 0.259451  [  128/  265]
train() client id: f_00001-5-4 loss: 0.310286  [  160/  265]
train() client id: f_00001-5-5 loss: 0.236577  [  192/  265]
train() client id: f_00001-5-6 loss: 0.364417  [  224/  265]
train() client id: f_00001-5-7 loss: 0.216060  [  256/  265]
train() client id: f_00001-6-0 loss: 0.179261  [   32/  265]
train() client id: f_00001-6-1 loss: 0.265456  [   64/  265]
train() client id: f_00001-6-2 loss: 0.319833  [   96/  265]
train() client id: f_00001-6-3 loss: 0.339649  [  128/  265]
train() client id: f_00001-6-4 loss: 0.363986  [  160/  265]
train() client id: f_00001-6-5 loss: 0.258169  [  192/  265]
train() client id: f_00001-6-6 loss: 0.294820  [  224/  265]
train() client id: f_00001-6-7 loss: 0.194421  [  256/  265]
train() client id: f_00001-7-0 loss: 0.279394  [   32/  265]
train() client id: f_00001-7-1 loss: 0.176979  [   64/  265]
train() client id: f_00001-7-2 loss: 0.214140  [   96/  265]
train() client id: f_00001-7-3 loss: 0.267901  [  128/  265]
train() client id: f_00001-7-4 loss: 0.267019  [  160/  265]
train() client id: f_00001-7-5 loss: 0.286757  [  192/  265]
train() client id: f_00001-7-6 loss: 0.455088  [  224/  265]
train() client id: f_00001-7-7 loss: 0.259309  [  256/  265]
train() client id: f_00001-8-0 loss: 0.184738  [   32/  265]
train() client id: f_00001-8-1 loss: 0.201997  [   64/  265]
train() client id: f_00001-8-2 loss: 0.343459  [   96/  265]
train() client id: f_00001-8-3 loss: 0.337120  [  128/  265]
train() client id: f_00001-8-4 loss: 0.229332  [  160/  265]
train() client id: f_00001-8-5 loss: 0.410515  [  192/  265]
train() client id: f_00001-8-6 loss: 0.198138  [  224/  265]
train() client id: f_00001-8-7 loss: 0.317969  [  256/  265]
train() client id: f_00001-9-0 loss: 0.191589  [   32/  265]
train() client id: f_00001-9-1 loss: 0.178998  [   64/  265]
train() client id: f_00001-9-2 loss: 0.395585  [   96/  265]
train() client id: f_00001-9-3 loss: 0.347669  [  128/  265]
train() client id: f_00001-9-4 loss: 0.222408  [  160/  265]
train() client id: f_00001-9-5 loss: 0.446519  [  192/  265]
train() client id: f_00001-9-6 loss: 0.189503  [  224/  265]
train() client id: f_00001-9-7 loss: 0.249782  [  256/  265]
train() client id: f_00001-10-0 loss: 0.384968  [   32/  265]
train() client id: f_00001-10-1 loss: 0.291669  [   64/  265]
train() client id: f_00001-10-2 loss: 0.197797  [   96/  265]
train() client id: f_00001-10-3 loss: 0.178563  [  128/  265]
train() client id: f_00001-10-4 loss: 0.387148  [  160/  265]
train() client id: f_00001-10-5 loss: 0.267594  [  192/  265]
train() client id: f_00001-10-6 loss: 0.260282  [  224/  265]
train() client id: f_00001-10-7 loss: 0.275251  [  256/  265]
train() client id: f_00001-11-0 loss: 0.168937  [   32/  265]
train() client id: f_00001-11-1 loss: 0.185581  [   64/  265]
train() client id: f_00001-11-2 loss: 0.321367  [   96/  265]
train() client id: f_00001-11-3 loss: 0.251549  [  128/  265]
train() client id: f_00001-11-4 loss: 0.400980  [  160/  265]
train() client id: f_00001-11-5 loss: 0.206637  [  192/  265]
train() client id: f_00001-11-6 loss: 0.190619  [  224/  265]
train() client id: f_00001-11-7 loss: 0.502177  [  256/  265]
train() client id: f_00002-0-0 loss: 1.304776  [   32/  124]
train() client id: f_00002-0-1 loss: 1.268499  [   64/  124]
train() client id: f_00002-0-2 loss: 1.486515  [   96/  124]
train() client id: f_00002-1-0 loss: 1.316941  [   32/  124]
train() client id: f_00002-1-1 loss: 1.258658  [   64/  124]
train() client id: f_00002-1-2 loss: 1.403132  [   96/  124]
train() client id: f_00002-2-0 loss: 1.293574  [   32/  124]
train() client id: f_00002-2-1 loss: 1.381615  [   64/  124]
train() client id: f_00002-2-2 loss: 1.305525  [   96/  124]
train() client id: f_00002-3-0 loss: 1.315714  [   32/  124]
train() client id: f_00002-3-1 loss: 1.339240  [   64/  124]
train() client id: f_00002-3-2 loss: 1.226342  [   96/  124]
train() client id: f_00002-4-0 loss: 1.314311  [   32/  124]
train() client id: f_00002-4-1 loss: 1.068923  [   64/  124]
train() client id: f_00002-4-2 loss: 1.250529  [   96/  124]
train() client id: f_00002-5-0 loss: 1.207049  [   32/  124]
train() client id: f_00002-5-1 loss: 1.160577  [   64/  124]
train() client id: f_00002-5-2 loss: 1.040312  [   96/  124]
train() client id: f_00002-6-0 loss: 1.046340  [   32/  124]
train() client id: f_00002-6-1 loss: 1.130604  [   64/  124]
train() client id: f_00002-6-2 loss: 1.345883  [   96/  124]
train() client id: f_00002-7-0 loss: 1.363378  [   32/  124]
train() client id: f_00002-7-1 loss: 1.088498  [   64/  124]
train() client id: f_00002-7-2 loss: 0.976933  [   96/  124]
train() client id: f_00002-8-0 loss: 1.221565  [   32/  124]
train() client id: f_00002-8-1 loss: 1.152296  [   64/  124]
train() client id: f_00002-8-2 loss: 1.185081  [   96/  124]
train() client id: f_00002-9-0 loss: 1.345433  [   32/  124]
train() client id: f_00002-9-1 loss: 1.080999  [   64/  124]
train() client id: f_00002-9-2 loss: 0.951012  [   96/  124]
train() client id: f_00002-10-0 loss: 0.955045  [   32/  124]
train() client id: f_00002-10-1 loss: 1.078892  [   64/  124]
train() client id: f_00002-10-2 loss: 1.178252  [   96/  124]
train() client id: f_00002-11-0 loss: 1.185813  [   32/  124]
train() client id: f_00002-11-1 loss: 1.027365  [   64/  124]
train() client id: f_00002-11-2 loss: 1.034380  [   96/  124]
train() client id: f_00003-0-0 loss: 0.374829  [   32/   43]
train() client id: f_00003-1-0 loss: 0.583597  [   32/   43]
train() client id: f_00003-2-0 loss: 0.641597  [   32/   43]
train() client id: f_00003-3-0 loss: 0.361535  [   32/   43]
train() client id: f_00003-4-0 loss: 0.483335  [   32/   43]
train() client id: f_00003-5-0 loss: 0.533561  [   32/   43]
train() client id: f_00003-6-0 loss: 0.498721  [   32/   43]
train() client id: f_00003-7-0 loss: 0.380620  [   32/   43]
train() client id: f_00003-8-0 loss: 0.430546  [   32/   43]
train() client id: f_00003-9-0 loss: 0.599275  [   32/   43]
train() client id: f_00003-10-0 loss: 0.455153  [   32/   43]
train() client id: f_00003-11-0 loss: 0.547516  [   32/   43]
train() client id: f_00004-0-0 loss: 0.931521  [   32/  306]
train() client id: f_00004-0-1 loss: 0.902885  [   64/  306]
train() client id: f_00004-0-2 loss: 0.923056  [   96/  306]
train() client id: f_00004-0-3 loss: 0.813008  [  128/  306]
train() client id: f_00004-0-4 loss: 0.956039  [  160/  306]
train() client id: f_00004-0-5 loss: 0.972693  [  192/  306]
train() client id: f_00004-0-6 loss: 0.850923  [  224/  306]
train() client id: f_00004-0-7 loss: 0.945755  [  256/  306]
train() client id: f_00004-0-8 loss: 0.914573  [  288/  306]
train() client id: f_00004-1-0 loss: 0.883881  [   32/  306]
train() client id: f_00004-1-1 loss: 0.899259  [   64/  306]
train() client id: f_00004-1-2 loss: 1.004995  [   96/  306]
train() client id: f_00004-1-3 loss: 0.793381  [  128/  306]
train() client id: f_00004-1-4 loss: 0.924593  [  160/  306]
train() client id: f_00004-1-5 loss: 0.878073  [  192/  306]
train() client id: f_00004-1-6 loss: 0.916389  [  224/  306]
train() client id: f_00004-1-7 loss: 0.944101  [  256/  306]
train() client id: f_00004-1-8 loss: 0.825424  [  288/  306]
train() client id: f_00004-2-0 loss: 0.967634  [   32/  306]
train() client id: f_00004-2-1 loss: 0.966119  [   64/  306]
train() client id: f_00004-2-2 loss: 0.821660  [   96/  306]
train() client id: f_00004-2-3 loss: 0.811158  [  128/  306]
train() client id: f_00004-2-4 loss: 0.872894  [  160/  306]
train() client id: f_00004-2-5 loss: 0.977931  [  192/  306]
train() client id: f_00004-2-6 loss: 0.928599  [  224/  306]
train() client id: f_00004-2-7 loss: 0.866993  [  256/  306]
train() client id: f_00004-2-8 loss: 0.884120  [  288/  306]
train() client id: f_00004-3-0 loss: 0.941406  [   32/  306]
train() client id: f_00004-3-1 loss: 1.036427  [   64/  306]
train() client id: f_00004-3-2 loss: 0.932870  [   96/  306]
train() client id: f_00004-3-3 loss: 0.809563  [  128/  306]
train() client id: f_00004-3-4 loss: 0.820698  [  160/  306]
train() client id: f_00004-3-5 loss: 0.963946  [  192/  306]
train() client id: f_00004-3-6 loss: 0.899844  [  224/  306]
train() client id: f_00004-3-7 loss: 0.953144  [  256/  306]
train() client id: f_00004-3-8 loss: 0.816938  [  288/  306]
train() client id: f_00004-4-0 loss: 0.881770  [   32/  306]
train() client id: f_00004-4-1 loss: 0.923741  [   64/  306]
train() client id: f_00004-4-2 loss: 0.791335  [   96/  306]
train() client id: f_00004-4-3 loss: 0.924267  [  128/  306]
train() client id: f_00004-4-4 loss: 0.961744  [  160/  306]
train() client id: f_00004-4-5 loss: 0.881429  [  192/  306]
train() client id: f_00004-4-6 loss: 0.975662  [  224/  306]
train() client id: f_00004-4-7 loss: 1.080646  [  256/  306]
train() client id: f_00004-4-8 loss: 0.753594  [  288/  306]
train() client id: f_00004-5-0 loss: 1.073270  [   32/  306]
train() client id: f_00004-5-1 loss: 0.980542  [   64/  306]
train() client id: f_00004-5-2 loss: 0.866451  [   96/  306]
train() client id: f_00004-5-3 loss: 0.918985  [  128/  306]
train() client id: f_00004-5-4 loss: 0.749918  [  160/  306]
train() client id: f_00004-5-5 loss: 0.917892  [  192/  306]
train() client id: f_00004-5-6 loss: 0.978188  [  224/  306]
train() client id: f_00004-5-7 loss: 0.918438  [  256/  306]
train() client id: f_00004-5-8 loss: 0.823679  [  288/  306]
train() client id: f_00004-6-0 loss: 0.942523  [   32/  306]
train() client id: f_00004-6-1 loss: 0.866774  [   64/  306]
train() client id: f_00004-6-2 loss: 0.952942  [   96/  306]
train() client id: f_00004-6-3 loss: 0.906990  [  128/  306]
train() client id: f_00004-6-4 loss: 0.848668  [  160/  306]
train() client id: f_00004-6-5 loss: 0.944981  [  192/  306]
train() client id: f_00004-6-6 loss: 0.911598  [  224/  306]
train() client id: f_00004-6-7 loss: 0.896457  [  256/  306]
train() client id: f_00004-6-8 loss: 0.914818  [  288/  306]
train() client id: f_00004-7-0 loss: 0.938258  [   32/  306]
train() client id: f_00004-7-1 loss: 0.868949  [   64/  306]
train() client id: f_00004-7-2 loss: 0.864984  [   96/  306]
train() client id: f_00004-7-3 loss: 0.956520  [  128/  306]
train() client id: f_00004-7-4 loss: 0.835401  [  160/  306]
train() client id: f_00004-7-5 loss: 0.923473  [  192/  306]
train() client id: f_00004-7-6 loss: 0.839049  [  224/  306]
train() client id: f_00004-7-7 loss: 0.962599  [  256/  306]
train() client id: f_00004-7-8 loss: 0.861077  [  288/  306]
train() client id: f_00004-8-0 loss: 0.848192  [   32/  306]
train() client id: f_00004-8-1 loss: 0.863601  [   64/  306]
train() client id: f_00004-8-2 loss: 0.843062  [   96/  306]
train() client id: f_00004-8-3 loss: 0.889695  [  128/  306]
train() client id: f_00004-8-4 loss: 0.952538  [  160/  306]
train() client id: f_00004-8-5 loss: 0.990567  [  192/  306]
train() client id: f_00004-8-6 loss: 1.016912  [  224/  306]
train() client id: f_00004-8-7 loss: 0.898928  [  256/  306]
train() client id: f_00004-8-8 loss: 0.822965  [  288/  306]
train() client id: f_00004-9-0 loss: 0.958570  [   32/  306]
train() client id: f_00004-9-1 loss: 0.945359  [   64/  306]
train() client id: f_00004-9-2 loss: 0.873388  [   96/  306]
train() client id: f_00004-9-3 loss: 0.886378  [  128/  306]
train() client id: f_00004-9-4 loss: 0.851087  [  160/  306]
train() client id: f_00004-9-5 loss: 0.834268  [  192/  306]
train() client id: f_00004-9-6 loss: 0.897189  [  224/  306]
train() client id: f_00004-9-7 loss: 0.859100  [  256/  306]
train() client id: f_00004-9-8 loss: 0.877038  [  288/  306]
train() client id: f_00004-10-0 loss: 0.847272  [   32/  306]
train() client id: f_00004-10-1 loss: 0.877823  [   64/  306]
train() client id: f_00004-10-2 loss: 0.834372  [   96/  306]
train() client id: f_00004-10-3 loss: 0.980525  [  128/  306]
train() client id: f_00004-10-4 loss: 0.988880  [  160/  306]
train() client id: f_00004-10-5 loss: 0.905594  [  192/  306]
train() client id: f_00004-10-6 loss: 0.835166  [  224/  306]
train() client id: f_00004-10-7 loss: 0.835282  [  256/  306]
train() client id: f_00004-10-8 loss: 0.939243  [  288/  306]
train() client id: f_00004-11-0 loss: 0.787480  [   32/  306]
train() client id: f_00004-11-1 loss: 0.925973  [   64/  306]
train() client id: f_00004-11-2 loss: 0.836076  [   96/  306]
train() client id: f_00004-11-3 loss: 0.914703  [  128/  306]
train() client id: f_00004-11-4 loss: 0.916997  [  160/  306]
train() client id: f_00004-11-5 loss: 0.878094  [  192/  306]
train() client id: f_00004-11-6 loss: 0.795841  [  224/  306]
train() client id: f_00004-11-7 loss: 1.105639  [  256/  306]
train() client id: f_00004-11-8 loss: 0.864889  [  288/  306]
train() client id: f_00005-0-0 loss: 0.560774  [   32/  146]
train() client id: f_00005-0-1 loss: 0.530904  [   64/  146]
train() client id: f_00005-0-2 loss: 0.370297  [   96/  146]
train() client id: f_00005-0-3 loss: 0.703827  [  128/  146]
train() client id: f_00005-1-0 loss: 0.332332  [   32/  146]
train() client id: f_00005-1-1 loss: 0.815515  [   64/  146]
train() client id: f_00005-1-2 loss: 0.506603  [   96/  146]
train() client id: f_00005-1-3 loss: 0.404054  [  128/  146]
train() client id: f_00005-2-0 loss: 0.521541  [   32/  146]
train() client id: f_00005-2-1 loss: 0.620196  [   64/  146]
train() client id: f_00005-2-2 loss: 0.464409  [   96/  146]
train() client id: f_00005-2-3 loss: 0.566680  [  128/  146]
train() client id: f_00005-3-0 loss: 0.360861  [   32/  146]
train() client id: f_00005-3-1 loss: 0.449306  [   64/  146]
train() client id: f_00005-3-2 loss: 0.641874  [   96/  146]
train() client id: f_00005-3-3 loss: 0.369657  [  128/  146]
train() client id: f_00005-4-0 loss: 0.277247  [   32/  146]
train() client id: f_00005-4-1 loss: 0.355938  [   64/  146]
train() client id: f_00005-4-2 loss: 0.888165  [   96/  146]
train() client id: f_00005-4-3 loss: 0.669692  [  128/  146]
train() client id: f_00005-5-0 loss: 0.578611  [   32/  146]
train() client id: f_00005-5-1 loss: 0.582809  [   64/  146]
train() client id: f_00005-5-2 loss: 0.339288  [   96/  146]
train() client id: f_00005-5-3 loss: 0.419097  [  128/  146]
train() client id: f_00005-6-0 loss: 0.634927  [   32/  146]
train() client id: f_00005-6-1 loss: 0.685943  [   64/  146]
train() client id: f_00005-6-2 loss: 0.460659  [   96/  146]
train() client id: f_00005-6-3 loss: 0.391486  [  128/  146]
train() client id: f_00005-7-0 loss: 0.661524  [   32/  146]
train() client id: f_00005-7-1 loss: 0.463411  [   64/  146]
train() client id: f_00005-7-2 loss: 0.431395  [   96/  146]
train() client id: f_00005-7-3 loss: 0.311400  [  128/  146]
train() client id: f_00005-8-0 loss: 0.434767  [   32/  146]
train() client id: f_00005-8-1 loss: 0.530706  [   64/  146]
train() client id: f_00005-8-2 loss: 0.664562  [   96/  146]
train() client id: f_00005-8-3 loss: 0.510152  [  128/  146]
train() client id: f_00005-9-0 loss: 0.535178  [   32/  146]
train() client id: f_00005-9-1 loss: 0.774812  [   64/  146]
train() client id: f_00005-9-2 loss: 0.426789  [   96/  146]
train() client id: f_00005-9-3 loss: 0.451838  [  128/  146]
train() client id: f_00005-10-0 loss: 0.605665  [   32/  146]
train() client id: f_00005-10-1 loss: 0.628110  [   64/  146]
train() client id: f_00005-10-2 loss: 0.618369  [   96/  146]
train() client id: f_00005-10-3 loss: 0.255703  [  128/  146]
train() client id: f_00005-11-0 loss: 0.780457  [   32/  146]
train() client id: f_00005-11-1 loss: 0.552589  [   64/  146]
train() client id: f_00005-11-2 loss: 0.499552  [   96/  146]
train() client id: f_00005-11-3 loss: 0.288098  [  128/  146]
train() client id: f_00006-0-0 loss: 0.490143  [   32/   54]
train() client id: f_00006-1-0 loss: 0.388619  [   32/   54]
train() client id: f_00006-2-0 loss: 0.477881  [   32/   54]
train() client id: f_00006-3-0 loss: 0.387349  [   32/   54]
train() client id: f_00006-4-0 loss: 0.494335  [   32/   54]
train() client id: f_00006-5-0 loss: 0.486402  [   32/   54]
train() client id: f_00006-6-0 loss: 0.502951  [   32/   54]
train() client id: f_00006-7-0 loss: 0.449727  [   32/   54]
train() client id: f_00006-8-0 loss: 0.490320  [   32/   54]
train() client id: f_00006-9-0 loss: 0.509600  [   32/   54]
train() client id: f_00006-10-0 loss: 0.422335  [   32/   54]
train() client id: f_00006-11-0 loss: 0.510539  [   32/   54]
train() client id: f_00007-0-0 loss: 0.915311  [   32/  179]
train() client id: f_00007-0-1 loss: 0.750404  [   64/  179]
train() client id: f_00007-0-2 loss: 0.680977  [   96/  179]
train() client id: f_00007-0-3 loss: 0.633529  [  128/  179]
train() client id: f_00007-0-4 loss: 0.670239  [  160/  179]
train() client id: f_00007-1-0 loss: 0.810900  [   32/  179]
train() client id: f_00007-1-1 loss: 0.671793  [   64/  179]
train() client id: f_00007-1-2 loss: 0.712352  [   96/  179]
train() client id: f_00007-1-3 loss: 0.638456  [  128/  179]
train() client id: f_00007-1-4 loss: 0.664052  [  160/  179]
train() client id: f_00007-2-0 loss: 0.835494  [   32/  179]
train() client id: f_00007-2-1 loss: 0.644857  [   64/  179]
train() client id: f_00007-2-2 loss: 0.644284  [   96/  179]
train() client id: f_00007-2-3 loss: 0.596590  [  128/  179]
train() client id: f_00007-2-4 loss: 0.764235  [  160/  179]
train() client id: f_00007-3-0 loss: 0.554900  [   32/  179]
train() client id: f_00007-3-1 loss: 0.794651  [   64/  179]
train() client id: f_00007-3-2 loss: 0.806713  [   96/  179]
train() client id: f_00007-3-3 loss: 0.730523  [  128/  179]
train() client id: f_00007-3-4 loss: 0.509586  [  160/  179]
train() client id: f_00007-4-0 loss: 0.783068  [   32/  179]
train() client id: f_00007-4-1 loss: 0.531125  [   64/  179]
train() client id: f_00007-4-2 loss: 0.647766  [   96/  179]
train() client id: f_00007-4-3 loss: 0.706063  [  128/  179]
train() client id: f_00007-4-4 loss: 0.858616  [  160/  179]
train() client id: f_00007-5-0 loss: 0.737231  [   32/  179]
train() client id: f_00007-5-1 loss: 0.790625  [   64/  179]
train() client id: f_00007-5-2 loss: 0.642668  [   96/  179]
train() client id: f_00007-5-3 loss: 0.536445  [  128/  179]
train() client id: f_00007-5-4 loss: 0.732049  [  160/  179]
train() client id: f_00007-6-0 loss: 0.624605  [   32/  179]
train() client id: f_00007-6-1 loss: 0.664930  [   64/  179]
train() client id: f_00007-6-2 loss: 0.841557  [   96/  179]
train() client id: f_00007-6-3 loss: 0.700915  [  128/  179]
train() client id: f_00007-6-4 loss: 0.554677  [  160/  179]
train() client id: f_00007-7-0 loss: 0.791222  [   32/  179]
train() client id: f_00007-7-1 loss: 0.548433  [   64/  179]
train() client id: f_00007-7-2 loss: 0.665629  [   96/  179]
train() client id: f_00007-7-3 loss: 0.821001  [  128/  179]
train() client id: f_00007-7-4 loss: 0.667356  [  160/  179]
train() client id: f_00007-8-0 loss: 0.546069  [   32/  179]
train() client id: f_00007-8-1 loss: 0.546526  [   64/  179]
train() client id: f_00007-8-2 loss: 0.908777  [   96/  179]
train() client id: f_00007-8-3 loss: 0.727340  [  128/  179]
train() client id: f_00007-8-4 loss: 0.740959  [  160/  179]
train() client id: f_00007-9-0 loss: 0.736064  [   32/  179]
train() client id: f_00007-9-1 loss: 0.551080  [   64/  179]
train() client id: f_00007-9-2 loss: 0.655455  [   96/  179]
train() client id: f_00007-9-3 loss: 0.539154  [  128/  179]
train() client id: f_00007-9-4 loss: 0.888904  [  160/  179]
train() client id: f_00007-10-0 loss: 0.567713  [   32/  179]
train() client id: f_00007-10-1 loss: 0.633207  [   64/  179]
train() client id: f_00007-10-2 loss: 0.885055  [   96/  179]
train() client id: f_00007-10-3 loss: 0.614476  [  128/  179]
train() client id: f_00007-10-4 loss: 0.633684  [  160/  179]
train() client id: f_00007-11-0 loss: 0.525109  [   32/  179]
train() client id: f_00007-11-1 loss: 0.727547  [   64/  179]
train() client id: f_00007-11-2 loss: 0.784410  [   96/  179]
train() client id: f_00007-11-3 loss: 0.656325  [  128/  179]
train() client id: f_00007-11-4 loss: 0.730796  [  160/  179]
train() client id: f_00008-0-0 loss: 0.754976  [   32/  130]
train() client id: f_00008-0-1 loss: 0.778325  [   64/  130]
train() client id: f_00008-0-2 loss: 0.698397  [   96/  130]
train() client id: f_00008-0-3 loss: 0.690301  [  128/  130]
train() client id: f_00008-1-0 loss: 0.571065  [   32/  130]
train() client id: f_00008-1-1 loss: 0.692588  [   64/  130]
train() client id: f_00008-1-2 loss: 0.879012  [   96/  130]
train() client id: f_00008-1-3 loss: 0.743715  [  128/  130]
train() client id: f_00008-2-0 loss: 0.680238  [   32/  130]
train() client id: f_00008-2-1 loss: 0.794062  [   64/  130]
train() client id: f_00008-2-2 loss: 0.748839  [   96/  130]
train() client id: f_00008-2-3 loss: 0.698049  [  128/  130]
train() client id: f_00008-3-0 loss: 0.774185  [   32/  130]
train() client id: f_00008-3-1 loss: 0.704737  [   64/  130]
train() client id: f_00008-3-2 loss: 0.649307  [   96/  130]
train() client id: f_00008-3-3 loss: 0.748900  [  128/  130]
train() client id: f_00008-4-0 loss: 0.759054  [   32/  130]
train() client id: f_00008-4-1 loss: 0.630500  [   64/  130]
train() client id: f_00008-4-2 loss: 0.742586  [   96/  130]
train() client id: f_00008-4-3 loss: 0.764630  [  128/  130]
train() client id: f_00008-5-0 loss: 0.730746  [   32/  130]
train() client id: f_00008-5-1 loss: 0.836946  [   64/  130]
train() client id: f_00008-5-2 loss: 0.668695  [   96/  130]
train() client id: f_00008-5-3 loss: 0.695355  [  128/  130]
train() client id: f_00008-6-0 loss: 0.679307  [   32/  130]
train() client id: f_00008-6-1 loss: 0.635698  [   64/  130]
train() client id: f_00008-6-2 loss: 0.873121  [   96/  130]
train() client id: f_00008-6-3 loss: 0.743419  [  128/  130]
train() client id: f_00008-7-0 loss: 0.616603  [   32/  130]
train() client id: f_00008-7-1 loss: 0.731566  [   64/  130]
train() client id: f_00008-7-2 loss: 0.851640  [   96/  130]
train() client id: f_00008-7-3 loss: 0.730139  [  128/  130]
train() client id: f_00008-8-0 loss: 0.746086  [   32/  130]
train() client id: f_00008-8-1 loss: 0.676619  [   64/  130]
train() client id: f_00008-8-2 loss: 0.637676  [   96/  130]
train() client id: f_00008-8-3 loss: 0.828827  [  128/  130]
train() client id: f_00008-9-0 loss: 0.729516  [   32/  130]
train() client id: f_00008-9-1 loss: 0.695267  [   64/  130]
train() client id: f_00008-9-2 loss: 0.717753  [   96/  130]
train() client id: f_00008-9-3 loss: 0.721625  [  128/  130]
train() client id: f_00008-10-0 loss: 0.742983  [   32/  130]
train() client id: f_00008-10-1 loss: 0.636793  [   64/  130]
train() client id: f_00008-10-2 loss: 0.782701  [   96/  130]
train() client id: f_00008-10-3 loss: 0.778988  [  128/  130]
train() client id: f_00008-11-0 loss: 0.622060  [   32/  130]
train() client id: f_00008-11-1 loss: 0.795643  [   64/  130]
train() client id: f_00008-11-2 loss: 0.849340  [   96/  130]
train() client id: f_00008-11-3 loss: 0.680347  [  128/  130]
train() client id: f_00009-0-0 loss: 0.965269  [   32/  118]
train() client id: f_00009-0-1 loss: 0.849260  [   64/  118]
train() client id: f_00009-0-2 loss: 0.936572  [   96/  118]
train() client id: f_00009-1-0 loss: 0.848159  [   32/  118]
train() client id: f_00009-1-1 loss: 0.979335  [   64/  118]
train() client id: f_00009-1-2 loss: 0.787088  [   96/  118]
train() client id: f_00009-2-0 loss: 0.851048  [   32/  118]
train() client id: f_00009-2-1 loss: 0.770009  [   64/  118]
train() client id: f_00009-2-2 loss: 1.038745  [   96/  118]
train() client id: f_00009-3-0 loss: 0.934732  [   32/  118]
train() client id: f_00009-3-1 loss: 0.905720  [   64/  118]
train() client id: f_00009-3-2 loss: 0.700125  [   96/  118]
train() client id: f_00009-4-0 loss: 0.948779  [   32/  118]
train() client id: f_00009-4-1 loss: 0.649881  [   64/  118]
train() client id: f_00009-4-2 loss: 0.917539  [   96/  118]
train() client id: f_00009-5-0 loss: 0.843448  [   32/  118]
train() client id: f_00009-5-1 loss: 0.803363  [   64/  118]
train() client id: f_00009-5-2 loss: 0.766191  [   96/  118]
train() client id: f_00009-6-0 loss: 1.064357  [   32/  118]
train() client id: f_00009-6-1 loss: 0.701181  [   64/  118]
train() client id: f_00009-6-2 loss: 0.656013  [   96/  118]
train() client id: f_00009-7-0 loss: 0.809044  [   32/  118]
train() client id: f_00009-7-1 loss: 0.695482  [   64/  118]
train() client id: f_00009-7-2 loss: 1.008601  [   96/  118]
train() client id: f_00009-8-0 loss: 0.746319  [   32/  118]
train() client id: f_00009-8-1 loss: 0.779720  [   64/  118]
train() client id: f_00009-8-2 loss: 0.895820  [   96/  118]
train() client id: f_00009-9-0 loss: 0.825247  [   32/  118]
train() client id: f_00009-9-1 loss: 0.817469  [   64/  118]
train() client id: f_00009-9-2 loss: 0.736838  [   96/  118]
train() client id: f_00009-10-0 loss: 0.700454  [   32/  118]
train() client id: f_00009-10-1 loss: 0.975182  [   64/  118]
train() client id: f_00009-10-2 loss: 0.957291  [   96/  118]
train() client id: f_00009-11-0 loss: 0.875153  [   32/  118]
train() client id: f_00009-11-1 loss: 0.759980  [   64/  118]
train() client id: f_00009-11-2 loss: 0.752953  [   96/  118]
At round 47 accuracy: 0.6472148541114059
At round 47 training accuracy: 0.5895372233400402
At round 47 training loss: 0.825465969783736
update_location
xs = [  -3.9056584     4.20031788  255.00902392   18.81129433    0.97929623
    3.95640986 -217.44319194 -196.32485185  239.66397685 -182.06087855]
ys = [ 247.5879595   230.55583871    1.32061395 -217.45517586  209.35018685
  192.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [267.04878179 251.343664   273.91850303 240.08460655 232.00961134
 217.23938387 239.35002039 220.3273104  260.28348348 207.75508503]
dists_bs = [185.85555033 187.61451208 463.75887031 437.70296191 179.3004407
 179.84087577 182.62546262 175.47821451 443.54338182 171.14423519]
uav_gains = [5.24535404e-12 7.22652115e-12 4.54225750e-12 8.95681473e-12
 1.03431723e-11 1.31774062e-11 9.07808384e-12 1.25521670e-11
 6.03484339e-12 1.52201927e-11]
bs_gains = [4.89330838e-11 4.76593468e-11 3.78160221e-12 4.44622592e-12
 5.41085986e-11 5.36545480e-11 5.13951767e-11 5.74736933e-11
 4.28423215e-12 6.16424250e-11]
Round 48
-------------------------------
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.66251584  9.56421031  4.59923771  1.66893827 11.02840386  5.30599829
  2.06183055  6.52085571  4.81741298  4.30225233]
obj_prev = 54.53165585566928
eta_min = 1.7212465954185627e-20	eta_max = 0.9392276891852349
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 12.610414541407936	eta = 0.909090909090909
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 25.13769025130001	eta = 0.4560487898791294
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 18.739989943316214	eta = 0.6117406281506839
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 17.582476098023903	eta = 0.6520135819067145
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 17.516919047777254	eta = 0.6544537420190021
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 17.516687862363067	eta = 0.6544623795057578
eta = 0.6544623795057578
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [0.03498261 0.07357454 0.03442733 0.01193851 0.08495777 0.0405354
 0.01499255 0.04969752 0.03609317 0.03276149]
ene_total = [1.63559168 2.74693988 1.64037262 0.78097472 3.12737952 1.61844234
 0.88321539 2.02882541 1.7072293  1.347717  ]
ti_comp = [0.68075038 0.73958322 0.67379048 0.70183595 0.74144644 0.74132564
 0.70228536 0.71216487 0.67077194 0.74326503]
ti_coms = [0.13155303 0.07272019 0.13851293 0.11046746 0.07085697 0.07097777
 0.11001805 0.10013853 0.14153147 0.06903838]
t_total = [27.59979858 27.59979858 27.59979858 27.59979858 27.59979858 27.59979858
 27.59979858 27.59979858 27.59979858 27.59979858]
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [5.77378821e-06 4.55080801e-05 5.61746411e-06 2.15903231e-07
 6.97155574e-05 7.57470152e-06 4.27050309e-07 1.51259614e-05
 6.53138483e-06 3.97817259e-06]
ene_total = [0.4471766  0.2486291  0.47081906 0.37534488 0.24312091 0.24142
 0.37382507 0.34075668 0.48110627 0.23470829]
optimize_network iter = 0 obj = 3.4569068484449996
eta = 0.6544623795057578
freqs = [25694153.73552614 49740538.88381484 25547504.06148466  8505199.59149158
 57291910.54818947 27339810.52250793 10674113.26419133 34891861.85896511
 26904202.71882671 22038902.98735746]
eta_min = 0.6544623795057585	eta_max = 0.691983191142256
af = 0.004443917503062845	bf = 1.1926074070739463	zeta = 0.00488830925336913	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [1.29805932e-06 1.02310970e-05 1.26291463e-06 4.85392243e-08
 1.56734064e-05 1.70293948e-06 9.60091734e-08 3.40060882e-06
 1.46838170e-06 8.94370180e-07]
ene_total = [1.6991017  0.94046194 1.78898032 1.42663193 0.91710237 0.91685829
 1.42083414 1.2936725  1.8279896  0.89170773]
ti_comp = [0.59254498 0.65137783 0.58558508 0.61363055 0.65324104 0.65312024
 0.61407996 0.62395948 0.58256655 0.65505963]
ti_coms = [0.13155303 0.07272019 0.13851293 0.11046746 0.07085697 0.07097777
 0.11001805 0.10013853 0.14153147 0.06903838]
t_total = [27.59979858 27.59979858 27.59979858 27.59979858 27.59979858 27.59979858
 27.59979858 27.59979858 27.59979858 27.59979858]
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [4.99236055e-06 3.84333847e-05 4.87216508e-06 1.85024130e-07
 5.88375125e-05 6.39306671e-06 3.65905026e-07 1.29087227e-05
 5.67251856e-06 3.35522234e-06]
ene_total = [0.50161926 0.27864598 0.52814309 0.42106603 0.27232185 0.27078333
 0.41935993 0.38218116 0.53967909 0.26327534]
optimize_network iter = 1 obj = 3.877075061802886
eta = 0.691983191142256
freqs = [25637255.917499   49049571.69376329 25530186.95114224  8448581.49608057
 56476825.65917366 26951443.47001215 10602081.72934842 34587483.18659778
 26904202.71882672 21718183.03696004]
eta_min = 0.6919831911422635	eta_max = 0.6919831911422551
af = 0.004337069151020109	bf = 1.1926074070739463	zeta = 0.004770776066122121	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [1.29231677e-06 9.94882221e-06 1.26120311e-06 4.78951357e-08
 1.52306115e-05 1.65490197e-06 9.47177586e-08 3.34153728e-06
 1.46838170e-06 8.68529037e-07]
ene_total = [1.69910096 0.94042549 1.7889801  1.42663185 0.91704519 0.91685209
 1.42083397 1.29366487 1.8279896  0.8917044 ]
ti_comp = [0.59254498 0.65137783 0.58558508 0.61363055 0.65324104 0.65312024
 0.61407996 0.62395948 0.58256655 0.65505963]
ti_coms = [0.13155303 0.07272019 0.13851293 0.11046746 0.07085697 0.07097777
 0.11001805 0.10013853 0.14153147 0.06903838]
t_total = [27.59979858 27.59979858 27.59979858 27.59979858 27.59979858 27.59979858
 27.59979858 27.59979858 27.59979858 27.59979858]
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [4.99236055e-06 3.84333847e-05 4.87216508e-06 1.85024130e-07
 5.88375125e-05 6.39306671e-06 3.65905026e-07 1.29087227e-05
 5.67251856e-06 3.35522234e-06]
ene_total = [0.50161926 0.27864598 0.52814309 0.42106603 0.27232185 0.27078333
 0.41935993 0.38218116 0.53967909 0.26327534]
optimize_network iter = 2 obj = 3.8770750618028753
eta = 0.6919831911422551
freqs = [25637255.91749901 49049571.69376332 25530186.95114225  8448581.49608057
 56476825.6591737  26951443.47001216 10602081.72934843 34587483.18659779
 26904202.71882672 21718183.03696005]
Done!
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [4.96895282e-06 3.82531817e-05 4.84932092e-06 1.84156606e-07
 5.85616405e-05 6.36309147e-06 3.64189404e-07 1.28481975e-05
 5.64592177e-06 3.33949068e-06]
ene_total = [0.01316027 0.00731027 0.01385614 0.01104693 0.00714426 0.00710414
 0.01100217 0.0100267  0.01415879 0.00690718]
At round 48 energy consumption: 0.10171685606737808
At round 48 eta: 0.6919831911422551
At round 48 a_n: 11.397856354757828
At round 48 local rounds: 12.056529567513225
At round 48 global rounds: 38.11610880998822
gradient difference: 0.47160881757736206
train() client id: f_00000-0-0 loss: 1.060352  [   32/  126]
train() client id: f_00000-0-1 loss: 1.351200  [   64/  126]
train() client id: f_00000-0-2 loss: 1.277577  [   96/  126]
train() client id: f_00000-1-0 loss: 0.971913  [   32/  126]
train() client id: f_00000-1-1 loss: 1.100267  [   64/  126]
train() client id: f_00000-1-2 loss: 0.975487  [   96/  126]
train() client id: f_00000-2-0 loss: 1.002646  [   32/  126]
train() client id: f_00000-2-1 loss: 1.000408  [   64/  126]
train() client id: f_00000-2-2 loss: 1.045093  [   96/  126]
train() client id: f_00000-3-0 loss: 1.060799  [   32/  126]
train() client id: f_00000-3-1 loss: 0.986573  [   64/  126]
train() client id: f_00000-3-2 loss: 0.857656  [   96/  126]
train() client id: f_00000-4-0 loss: 0.766273  [   32/  126]
train() client id: f_00000-4-1 loss: 1.039779  [   64/  126]
train() client id: f_00000-4-2 loss: 0.850776  [   96/  126]
train() client id: f_00000-5-0 loss: 0.699957  [   32/  126]
train() client id: f_00000-5-1 loss: 0.867431  [   64/  126]
train() client id: f_00000-5-2 loss: 1.068635  [   96/  126]
train() client id: f_00000-6-0 loss: 0.757041  [   32/  126]
train() client id: f_00000-6-1 loss: 0.887869  [   64/  126]
train() client id: f_00000-6-2 loss: 0.892997  [   96/  126]
train() client id: f_00000-7-0 loss: 0.821628  [   32/  126]
train() client id: f_00000-7-1 loss: 0.777323  [   64/  126]
train() client id: f_00000-7-2 loss: 0.792533  [   96/  126]
train() client id: f_00000-8-0 loss: 0.663370  [   32/  126]
train() client id: f_00000-8-1 loss: 0.766997  [   64/  126]
train() client id: f_00000-8-2 loss: 0.796953  [   96/  126]
train() client id: f_00000-9-0 loss: 0.891755  [   32/  126]
train() client id: f_00000-9-1 loss: 0.763710  [   64/  126]
train() client id: f_00000-9-2 loss: 0.684101  [   96/  126]
train() client id: f_00000-10-0 loss: 0.866618  [   32/  126]
train() client id: f_00000-10-1 loss: 0.805797  [   64/  126]
train() client id: f_00000-10-2 loss: 0.767523  [   96/  126]
train() client id: f_00000-11-0 loss: 0.852130  [   32/  126]
train() client id: f_00000-11-1 loss: 0.769378  [   64/  126]
train() client id: f_00000-11-2 loss: 0.820898  [   96/  126]
train() client id: f_00001-0-0 loss: 0.380654  [   32/  265]
train() client id: f_00001-0-1 loss: 0.257080  [   64/  265]
train() client id: f_00001-0-2 loss: 0.431495  [   96/  265]
train() client id: f_00001-0-3 loss: 0.300409  [  128/  265]
train() client id: f_00001-0-4 loss: 0.336130  [  160/  265]
train() client id: f_00001-0-5 loss: 0.283534  [  192/  265]
train() client id: f_00001-0-6 loss: 0.296439  [  224/  265]
train() client id: f_00001-0-7 loss: 0.211381  [  256/  265]
train() client id: f_00001-1-0 loss: 0.458227  [   32/  265]
train() client id: f_00001-1-1 loss: 0.240146  [   64/  265]
train() client id: f_00001-1-2 loss: 0.299819  [   96/  265]
train() client id: f_00001-1-3 loss: 0.321475  [  128/  265]
train() client id: f_00001-1-4 loss: 0.247547  [  160/  265]
train() client id: f_00001-1-5 loss: 0.232421  [  192/  265]
train() client id: f_00001-1-6 loss: 0.235981  [  224/  265]
train() client id: f_00001-1-7 loss: 0.362698  [  256/  265]
train() client id: f_00001-2-0 loss: 0.386285  [   32/  265]
train() client id: f_00001-2-1 loss: 0.219827  [   64/  265]
train() client id: f_00001-2-2 loss: 0.361015  [   96/  265]
train() client id: f_00001-2-3 loss: 0.366816  [  128/  265]
train() client id: f_00001-2-4 loss: 0.204025  [  160/  265]
train() client id: f_00001-2-5 loss: 0.293215  [  192/  265]
train() client id: f_00001-2-6 loss: 0.192855  [  224/  265]
train() client id: f_00001-2-7 loss: 0.259167  [  256/  265]
train() client id: f_00001-3-0 loss: 0.415337  [   32/  265]
train() client id: f_00001-3-1 loss: 0.308501  [   64/  265]
train() client id: f_00001-3-2 loss: 0.262298  [   96/  265]
train() client id: f_00001-3-3 loss: 0.314194  [  128/  265]
train() client id: f_00001-3-4 loss: 0.239078  [  160/  265]
train() client id: f_00001-3-5 loss: 0.279181  [  192/  265]
train() client id: f_00001-3-6 loss: 0.206255  [  224/  265]
train() client id: f_00001-3-7 loss: 0.260427  [  256/  265]
train() client id: f_00001-4-0 loss: 0.245348  [   32/  265]
train() client id: f_00001-4-1 loss: 0.247879  [   64/  265]
train() client id: f_00001-4-2 loss: 0.240421  [   96/  265]
train() client id: f_00001-4-3 loss: 0.378037  [  128/  265]
train() client id: f_00001-4-4 loss: 0.370873  [  160/  265]
train() client id: f_00001-4-5 loss: 0.260370  [  192/  265]
train() client id: f_00001-4-6 loss: 0.180397  [  224/  265]
train() client id: f_00001-4-7 loss: 0.324754  [  256/  265]
train() client id: f_00001-5-0 loss: 0.368908  [   32/  265]
train() client id: f_00001-5-1 loss: 0.221895  [   64/  265]
train() client id: f_00001-5-2 loss: 0.255015  [   96/  265]
train() client id: f_00001-5-3 loss: 0.191066  [  128/  265]
train() client id: f_00001-5-4 loss: 0.323390  [  160/  265]
train() client id: f_00001-5-5 loss: 0.248050  [  192/  265]
train() client id: f_00001-5-6 loss: 0.193825  [  224/  265]
train() client id: f_00001-5-7 loss: 0.348366  [  256/  265]
train() client id: f_00001-6-0 loss: 0.268714  [   32/  265]
train() client id: f_00001-6-1 loss: 0.303714  [   64/  265]
train() client id: f_00001-6-2 loss: 0.250146  [   96/  265]
train() client id: f_00001-6-3 loss: 0.166863  [  128/  265]
train() client id: f_00001-6-4 loss: 0.171185  [  160/  265]
train() client id: f_00001-6-5 loss: 0.366019  [  192/  265]
train() client id: f_00001-6-6 loss: 0.174905  [  224/  265]
train() client id: f_00001-6-7 loss: 0.378351  [  256/  265]
train() client id: f_00001-7-0 loss: 0.178151  [   32/  265]
train() client id: f_00001-7-1 loss: 0.180418  [   64/  265]
train() client id: f_00001-7-2 loss: 0.294815  [   96/  265]
train() client id: f_00001-7-3 loss: 0.224913  [  128/  265]
train() client id: f_00001-7-4 loss: 0.262898  [  160/  265]
train() client id: f_00001-7-5 loss: 0.426358  [  192/  265]
train() client id: f_00001-7-6 loss: 0.258837  [  224/  265]
train() client id: f_00001-7-7 loss: 0.244566  [  256/  265]
train() client id: f_00001-8-0 loss: 0.159190  [   32/  265]
train() client id: f_00001-8-1 loss: 0.384680  [   64/  265]
train() client id: f_00001-8-2 loss: 0.311544  [   96/  265]
train() client id: f_00001-8-3 loss: 0.175705  [  128/  265]
train() client id: f_00001-8-4 loss: 0.441380  [  160/  265]
train() client id: f_00001-8-5 loss: 0.181470  [  192/  265]
train() client id: f_00001-8-6 loss: 0.265902  [  224/  265]
train() client id: f_00001-8-7 loss: 0.223457  [  256/  265]
train() client id: f_00001-9-0 loss: 0.166237  [   32/  265]
train() client id: f_00001-9-1 loss: 0.166195  [   64/  265]
train() client id: f_00001-9-2 loss: 0.201954  [   96/  265]
train() client id: f_00001-9-3 loss: 0.460850  [  128/  265]
train() client id: f_00001-9-4 loss: 0.186754  [  160/  265]
train() client id: f_00001-9-5 loss: 0.350878  [  192/  265]
train() client id: f_00001-9-6 loss: 0.165006  [  224/  265]
train() client id: f_00001-9-7 loss: 0.386125  [  256/  265]
train() client id: f_00001-10-0 loss: 0.396773  [   32/  265]
train() client id: f_00001-10-1 loss: 0.308117  [   64/  265]
train() client id: f_00001-10-2 loss: 0.307621  [   96/  265]
train() client id: f_00001-10-3 loss: 0.167852  [  128/  265]
train() client id: f_00001-10-4 loss: 0.377284  [  160/  265]
train() client id: f_00001-10-5 loss: 0.146401  [  192/  265]
train() client id: f_00001-10-6 loss: 0.183567  [  224/  265]
train() client id: f_00001-10-7 loss: 0.223267  [  256/  265]
train() client id: f_00001-11-0 loss: 0.317490  [   32/  265]
train() client id: f_00001-11-1 loss: 0.169323  [   64/  265]
train() client id: f_00001-11-2 loss: 0.248530  [   96/  265]
train() client id: f_00001-11-3 loss: 0.272350  [  128/  265]
train() client id: f_00001-11-4 loss: 0.305037  [  160/  265]
train() client id: f_00001-11-5 loss: 0.276314  [  192/  265]
train() client id: f_00001-11-6 loss: 0.333205  [  224/  265]
train() client id: f_00001-11-7 loss: 0.181066  [  256/  265]
train() client id: f_00002-0-0 loss: 1.126915  [   32/  124]
train() client id: f_00002-0-1 loss: 1.106957  [   64/  124]
train() client id: f_00002-0-2 loss: 1.000184  [   96/  124]
train() client id: f_00002-1-0 loss: 1.086890  [   32/  124]
train() client id: f_00002-1-1 loss: 1.150889  [   64/  124]
train() client id: f_00002-1-2 loss: 1.029201  [   96/  124]
train() client id: f_00002-2-0 loss: 1.069533  [   32/  124]
train() client id: f_00002-2-1 loss: 1.244608  [   64/  124]
train() client id: f_00002-2-2 loss: 1.031911  [   96/  124]
train() client id: f_00002-3-0 loss: 1.032182  [   32/  124]
train() client id: f_00002-3-1 loss: 0.997842  [   64/  124]
train() client id: f_00002-3-2 loss: 1.210515  [   96/  124]
train() client id: f_00002-4-0 loss: 1.108097  [   32/  124]
train() client id: f_00002-4-1 loss: 1.067676  [   64/  124]
train() client id: f_00002-4-2 loss: 0.986648  [   96/  124]
train() client id: f_00002-5-0 loss: 1.019233  [   32/  124]
train() client id: f_00002-5-1 loss: 1.137002  [   64/  124]
train() client id: f_00002-5-2 loss: 0.977946  [   96/  124]
train() client id: f_00002-6-0 loss: 0.968557  [   32/  124]
train() client id: f_00002-6-1 loss: 0.868968  [   64/  124]
train() client id: f_00002-6-2 loss: 1.289047  [   96/  124]
train() client id: f_00002-7-0 loss: 1.119284  [   32/  124]
train() client id: f_00002-7-1 loss: 0.910769  [   64/  124]
train() client id: f_00002-7-2 loss: 1.002717  [   96/  124]
train() client id: f_00002-8-0 loss: 1.089626  [   32/  124]
train() client id: f_00002-8-1 loss: 0.934697  [   64/  124]
train() client id: f_00002-8-2 loss: 1.142868  [   96/  124]
train() client id: f_00002-9-0 loss: 1.065183  [   32/  124]
train() client id: f_00002-9-1 loss: 1.026983  [   64/  124]
train() client id: f_00002-9-2 loss: 0.905125  [   96/  124]
train() client id: f_00002-10-0 loss: 1.036147  [   32/  124]
train() client id: f_00002-10-1 loss: 1.009259  [   64/  124]
train() client id: f_00002-10-2 loss: 0.922429  [   96/  124]
train() client id: f_00002-11-0 loss: 1.168981  [   32/  124]
train() client id: f_00002-11-1 loss: 0.943490  [   64/  124]
train() client id: f_00002-11-2 loss: 0.888957  [   96/  124]
train() client id: f_00003-0-0 loss: 0.747944  [   32/   43]
train() client id: f_00003-1-0 loss: 0.586058  [   32/   43]
train() client id: f_00003-2-0 loss: 0.725971  [   32/   43]
train() client id: f_00003-3-0 loss: 0.814201  [   32/   43]
train() client id: f_00003-4-0 loss: 0.598647  [   32/   43]
train() client id: f_00003-5-0 loss: 0.667057  [   32/   43]
train() client id: f_00003-6-0 loss: 0.730993  [   32/   43]
train() client id: f_00003-7-0 loss: 0.660931  [   32/   43]
train() client id: f_00003-8-0 loss: 0.819940  [   32/   43]
train() client id: f_00003-9-0 loss: 0.630219  [   32/   43]
train() client id: f_00003-10-0 loss: 0.587738  [   32/   43]
train() client id: f_00003-11-0 loss: 0.711898  [   32/   43]
train() client id: f_00004-0-0 loss: 0.800815  [   32/  306]
train() client id: f_00004-0-1 loss: 0.900229  [   64/  306]
train() client id: f_00004-0-2 loss: 0.838879  [   96/  306]
train() client id: f_00004-0-3 loss: 1.079113  [  128/  306]
train() client id: f_00004-0-4 loss: 0.747421  [  160/  306]
train() client id: f_00004-0-5 loss: 0.925573  [  192/  306]
train() client id: f_00004-0-6 loss: 0.817699  [  224/  306]
train() client id: f_00004-0-7 loss: 0.825614  [  256/  306]
train() client id: f_00004-0-8 loss: 0.850904  [  288/  306]
train() client id: f_00004-1-0 loss: 0.858860  [   32/  306]
train() client id: f_00004-1-1 loss: 0.840388  [   64/  306]
train() client id: f_00004-1-2 loss: 0.859985  [   96/  306]
train() client id: f_00004-1-3 loss: 0.735813  [  128/  306]
train() client id: f_00004-1-4 loss: 0.824515  [  160/  306]
train() client id: f_00004-1-5 loss: 0.944328  [  192/  306]
train() client id: f_00004-1-6 loss: 0.839351  [  224/  306]
train() client id: f_00004-1-7 loss: 0.859977  [  256/  306]
train() client id: f_00004-1-8 loss: 1.007345  [  288/  306]
train() client id: f_00004-2-0 loss: 0.729375  [   32/  306]
train() client id: f_00004-2-1 loss: 0.832142  [   64/  306]
train() client id: f_00004-2-2 loss: 0.938162  [   96/  306]
train() client id: f_00004-2-3 loss: 0.798277  [  128/  306]
train() client id: f_00004-2-4 loss: 0.859168  [  160/  306]
train() client id: f_00004-2-5 loss: 0.816829  [  192/  306]
train() client id: f_00004-2-6 loss: 0.814502  [  224/  306]
train() client id: f_00004-2-7 loss: 0.988919  [  256/  306]
train() client id: f_00004-2-8 loss: 0.996644  [  288/  306]
train() client id: f_00004-3-0 loss: 0.762626  [   32/  306]
train() client id: f_00004-3-1 loss: 0.771694  [   64/  306]
train() client id: f_00004-3-2 loss: 0.850767  [   96/  306]
train() client id: f_00004-3-3 loss: 0.936007  [  128/  306]
train() client id: f_00004-3-4 loss: 0.893588  [  160/  306]
train() client id: f_00004-3-5 loss: 0.934590  [  192/  306]
train() client id: f_00004-3-6 loss: 0.912798  [  224/  306]
train() client id: f_00004-3-7 loss: 0.966074  [  256/  306]
train() client id: f_00004-3-8 loss: 0.824881  [  288/  306]
train() client id: f_00004-4-0 loss: 0.877859  [   32/  306]
train() client id: f_00004-4-1 loss: 0.909534  [   64/  306]
train() client id: f_00004-4-2 loss: 0.936895  [   96/  306]
train() client id: f_00004-4-3 loss: 0.897548  [  128/  306]
train() client id: f_00004-4-4 loss: 0.939344  [  160/  306]
train() client id: f_00004-4-5 loss: 0.892033  [  192/  306]
train() client id: f_00004-4-6 loss: 0.871473  [  224/  306]
train() client id: f_00004-4-7 loss: 0.820493  [  256/  306]
train() client id: f_00004-4-8 loss: 0.626918  [  288/  306]
train() client id: f_00004-5-0 loss: 1.020827  [   32/  306]
train() client id: f_00004-5-1 loss: 0.837225  [   64/  306]
train() client id: f_00004-5-2 loss: 0.953298  [   96/  306]
train() client id: f_00004-5-3 loss: 0.789566  [  128/  306]
train() client id: f_00004-5-4 loss: 0.880431  [  160/  306]
train() client id: f_00004-5-5 loss: 0.728375  [  192/  306]
train() client id: f_00004-5-6 loss: 0.876262  [  224/  306]
train() client id: f_00004-5-7 loss: 0.906757  [  256/  306]
train() client id: f_00004-5-8 loss: 0.784583  [  288/  306]
train() client id: f_00004-6-0 loss: 0.948926  [   32/  306]
train() client id: f_00004-6-1 loss: 0.808382  [   64/  306]
train() client id: f_00004-6-2 loss: 0.789293  [   96/  306]
train() client id: f_00004-6-3 loss: 0.920374  [  128/  306]
train() client id: f_00004-6-4 loss: 0.947865  [  160/  306]
train() client id: f_00004-6-5 loss: 0.778870  [  192/  306]
train() client id: f_00004-6-6 loss: 0.808002  [  224/  306]
train() client id: f_00004-6-7 loss: 0.839940  [  256/  306]
train() client id: f_00004-6-8 loss: 0.825547  [  288/  306]
train() client id: f_00004-7-0 loss: 0.781707  [   32/  306]
train() client id: f_00004-7-1 loss: 0.738642  [   64/  306]
train() client id: f_00004-7-2 loss: 0.868973  [   96/  306]
train() client id: f_00004-7-3 loss: 1.043244  [  128/  306]
train() client id: f_00004-7-4 loss: 0.829729  [  160/  306]
train() client id: f_00004-7-5 loss: 0.896739  [  192/  306]
train() client id: f_00004-7-6 loss: 0.760761  [  224/  306]
train() client id: f_00004-7-7 loss: 0.894479  [  256/  306]
train() client id: f_00004-7-8 loss: 0.878026  [  288/  306]
train() client id: f_00004-8-0 loss: 0.775979  [   32/  306]
train() client id: f_00004-8-1 loss: 0.950899  [   64/  306]
train() client id: f_00004-8-2 loss: 0.834220  [   96/  306]
train() client id: f_00004-8-3 loss: 0.897845  [  128/  306]
train() client id: f_00004-8-4 loss: 0.947788  [  160/  306]
train() client id: f_00004-8-5 loss: 0.868259  [  192/  306]
train() client id: f_00004-8-6 loss: 0.898223  [  224/  306]
train() client id: f_00004-8-7 loss: 0.809737  [  256/  306]
train() client id: f_00004-8-8 loss: 0.710962  [  288/  306]
train() client id: f_00004-9-0 loss: 0.836825  [   32/  306]
train() client id: f_00004-9-1 loss: 0.941996  [   64/  306]
train() client id: f_00004-9-2 loss: 0.800311  [   96/  306]
train() client id: f_00004-9-3 loss: 0.823845  [  128/  306]
train() client id: f_00004-9-4 loss: 0.799317  [  160/  306]
train() client id: f_00004-9-5 loss: 0.825944  [  192/  306]
train() client id: f_00004-9-6 loss: 1.012525  [  224/  306]
train() client id: f_00004-9-7 loss: 0.795538  [  256/  306]
train() client id: f_00004-9-8 loss: 0.903584  [  288/  306]
train() client id: f_00004-10-0 loss: 0.921254  [   32/  306]
train() client id: f_00004-10-1 loss: 0.943875  [   64/  306]
train() client id: f_00004-10-2 loss: 0.752331  [   96/  306]
train() client id: f_00004-10-3 loss: 0.891911  [  128/  306]
train() client id: f_00004-10-4 loss: 0.809815  [  160/  306]
train() client id: f_00004-10-5 loss: 0.780686  [  192/  306]
train() client id: f_00004-10-6 loss: 0.828572  [  224/  306]
train() client id: f_00004-10-7 loss: 0.844444  [  256/  306]
train() client id: f_00004-10-8 loss: 0.838407  [  288/  306]
train() client id: f_00004-11-0 loss: 0.839446  [   32/  306]
train() client id: f_00004-11-1 loss: 0.896429  [   64/  306]
train() client id: f_00004-11-2 loss: 0.765046  [   96/  306]
train() client id: f_00004-11-3 loss: 0.913638  [  128/  306]
train() client id: f_00004-11-4 loss: 0.876498  [  160/  306]
train() client id: f_00004-11-5 loss: 0.820448  [  192/  306]
train() client id: f_00004-11-6 loss: 0.823752  [  224/  306]
train() client id: f_00004-11-7 loss: 0.921134  [  256/  306]
train() client id: f_00004-11-8 loss: 0.838777  [  288/  306]
train() client id: f_00005-0-0 loss: 0.783334  [   32/  146]
train() client id: f_00005-0-1 loss: 1.107933  [   64/  146]
train() client id: f_00005-0-2 loss: 0.640792  [   96/  146]
train() client id: f_00005-0-3 loss: 0.868232  [  128/  146]
train() client id: f_00005-1-0 loss: 0.740777  [   32/  146]
train() client id: f_00005-1-1 loss: 0.735983  [   64/  146]
train() client id: f_00005-1-2 loss: 1.057367  [   96/  146]
train() client id: f_00005-1-3 loss: 0.837643  [  128/  146]
train() client id: f_00005-2-0 loss: 0.843385  [   32/  146]
train() client id: f_00005-2-1 loss: 0.578401  [   64/  146]
train() client id: f_00005-2-2 loss: 0.919329  [   96/  146]
train() client id: f_00005-2-3 loss: 1.022346  [  128/  146]
train() client id: f_00005-3-0 loss: 0.874430  [   32/  146]
train() client id: f_00005-3-1 loss: 0.970849  [   64/  146]
train() client id: f_00005-3-2 loss: 0.724528  [   96/  146]
train() client id: f_00005-3-3 loss: 0.841994  [  128/  146]
train() client id: f_00005-4-0 loss: 0.909494  [   32/  146]
train() client id: f_00005-4-1 loss: 0.951816  [   64/  146]
train() client id: f_00005-4-2 loss: 0.810458  [   96/  146]
train() client id: f_00005-4-3 loss: 0.748808  [  128/  146]
train() client id: f_00005-5-0 loss: 0.838088  [   32/  146]
train() client id: f_00005-5-1 loss: 0.873446  [   64/  146]
train() client id: f_00005-5-2 loss: 0.915639  [   96/  146]
train() client id: f_00005-5-3 loss: 0.899931  [  128/  146]
train() client id: f_00005-6-0 loss: 1.047391  [   32/  146]
train() client id: f_00005-6-1 loss: 0.862501  [   64/  146]
train() client id: f_00005-6-2 loss: 0.717303  [   96/  146]
train() client id: f_00005-6-3 loss: 0.876298  [  128/  146]
train() client id: f_00005-7-0 loss: 0.941487  [   32/  146]
train() client id: f_00005-7-1 loss: 0.840491  [   64/  146]
train() client id: f_00005-7-2 loss: 0.854346  [   96/  146]
train() client id: f_00005-7-3 loss: 0.552605  [  128/  146]
train() client id: f_00005-8-0 loss: 0.672739  [   32/  146]
train() client id: f_00005-8-1 loss: 1.116354  [   64/  146]
train() client id: f_00005-8-2 loss: 0.618692  [   96/  146]
train() client id: f_00005-8-3 loss: 0.944892  [  128/  146]
train() client id: f_00005-9-0 loss: 0.736185  [   32/  146]
train() client id: f_00005-9-1 loss: 0.783720  [   64/  146]
train() client id: f_00005-9-2 loss: 0.986816  [   96/  146]
train() client id: f_00005-9-3 loss: 0.811852  [  128/  146]
train() client id: f_00005-10-0 loss: 0.613949  [   32/  146]
train() client id: f_00005-10-1 loss: 0.815462  [   64/  146]
train() client id: f_00005-10-2 loss: 1.070654  [   96/  146]
train() client id: f_00005-10-3 loss: 0.723413  [  128/  146]
train() client id: f_00005-11-0 loss: 0.791342  [   32/  146]
train() client id: f_00005-11-1 loss: 0.831959  [   64/  146]
train() client id: f_00005-11-2 loss: 0.967871  [   96/  146]
train() client id: f_00005-11-3 loss: 0.911661  [  128/  146]
train() client id: f_00006-0-0 loss: 0.405018  [   32/   54]
train() client id: f_00006-1-0 loss: 0.429222  [   32/   54]
train() client id: f_00006-2-0 loss: 0.469016  [   32/   54]
train() client id: f_00006-3-0 loss: 0.487495  [   32/   54]
train() client id: f_00006-4-0 loss: 0.420761  [   32/   54]
train() client id: f_00006-5-0 loss: 0.459220  [   32/   54]
train() client id: f_00006-6-0 loss: 0.487060  [   32/   54]
train() client id: f_00006-7-0 loss: 0.409787  [   32/   54]
train() client id: f_00006-8-0 loss: 0.466364  [   32/   54]
train() client id: f_00006-9-0 loss: 0.401077  [   32/   54]
train() client id: f_00006-10-0 loss: 0.446432  [   32/   54]
train() client id: f_00006-11-0 loss: 0.509314  [   32/   54]
train() client id: f_00007-0-0 loss: 0.432168  [   32/  179]
train() client id: f_00007-0-1 loss: 0.607439  [   64/  179]
train() client id: f_00007-0-2 loss: 0.591903  [   96/  179]
train() client id: f_00007-0-3 loss: 0.477411  [  128/  179]
train() client id: f_00007-0-4 loss: 0.748610  [  160/  179]
train() client id: f_00007-1-0 loss: 0.478345  [   32/  179]
train() client id: f_00007-1-1 loss: 0.856403  [   64/  179]
train() client id: f_00007-1-2 loss: 0.432079  [   96/  179]
train() client id: f_00007-1-3 loss: 0.447311  [  128/  179]
train() client id: f_00007-1-4 loss: 0.508792  [  160/  179]
train() client id: f_00007-2-0 loss: 0.466265  [   32/  179]
train() client id: f_00007-2-1 loss: 0.451335  [   64/  179]
train() client id: f_00007-2-2 loss: 0.614800  [   96/  179]
train() client id: f_00007-2-3 loss: 0.716502  [  128/  179]
train() client id: f_00007-2-4 loss: 0.473252  [  160/  179]
train() client id: f_00007-3-0 loss: 0.514942  [   32/  179]
train() client id: f_00007-3-1 loss: 0.744559  [   64/  179]
train() client id: f_00007-3-2 loss: 0.363031  [   96/  179]
train() client id: f_00007-3-3 loss: 0.487577  [  128/  179]
train() client id: f_00007-3-4 loss: 0.542232  [  160/  179]
train() client id: f_00007-4-0 loss: 0.653697  [   32/  179]
train() client id: f_00007-4-1 loss: 0.550050  [   64/  179]
train() client id: f_00007-4-2 loss: 0.555049  [   96/  179]
train() client id: f_00007-4-3 loss: 0.382281  [  128/  179]
train() client id: f_00007-4-4 loss: 0.491175  [  160/  179]
train() client id: f_00007-5-0 loss: 0.454948  [   32/  179]
train() client id: f_00007-5-1 loss: 0.430849  [   64/  179]
train() client id: f_00007-5-2 loss: 0.477723  [   96/  179]
train() client id: f_00007-5-3 loss: 0.616511  [  128/  179]
train() client id: f_00007-5-4 loss: 0.519974  [  160/  179]
train() client id: f_00007-6-0 loss: 0.566115  [   32/  179]
train() client id: f_00007-6-1 loss: 0.353989  [   64/  179]
train() client id: f_00007-6-2 loss: 0.595236  [   96/  179]
train() client id: f_00007-6-3 loss: 0.460862  [  128/  179]
train() client id: f_00007-6-4 loss: 0.451019  [  160/  179]
train() client id: f_00007-7-0 loss: 0.343902  [   32/  179]
train() client id: f_00007-7-1 loss: 0.333610  [   64/  179]
train() client id: f_00007-7-2 loss: 0.641253  [   96/  179]
train() client id: f_00007-7-3 loss: 0.366074  [  128/  179]
train() client id: f_00007-7-4 loss: 0.742119  [  160/  179]
train() client id: f_00007-8-0 loss: 0.439524  [   32/  179]
train() client id: f_00007-8-1 loss: 0.580061  [   64/  179]
train() client id: f_00007-8-2 loss: 0.426966  [   96/  179]
train() client id: f_00007-8-3 loss: 0.611608  [  128/  179]
train() client id: f_00007-8-4 loss: 0.452399  [  160/  179]
train() client id: f_00007-9-0 loss: 0.701690  [   32/  179]
train() client id: f_00007-9-1 loss: 0.362546  [   64/  179]
train() client id: f_00007-9-2 loss: 0.347614  [   96/  179]
train() client id: f_00007-9-3 loss: 0.436023  [  128/  179]
train() client id: f_00007-9-4 loss: 0.585037  [  160/  179]
train() client id: f_00007-10-0 loss: 0.548656  [   32/  179]
train() client id: f_00007-10-1 loss: 0.398857  [   64/  179]
train() client id: f_00007-10-2 loss: 0.558660  [   96/  179]
train() client id: f_00007-10-3 loss: 0.448146  [  128/  179]
train() client id: f_00007-10-4 loss: 0.441407  [  160/  179]
train() client id: f_00007-11-0 loss: 0.519330  [   32/  179]
train() client id: f_00007-11-1 loss: 0.666771  [   64/  179]
train() client id: f_00007-11-2 loss: 0.332823  [   96/  179]
train() client id: f_00007-11-3 loss: 0.453515  [  128/  179]
train() client id: f_00007-11-4 loss: 0.447525  [  160/  179]
train() client id: f_00008-0-0 loss: 0.721469  [   32/  130]
train() client id: f_00008-0-1 loss: 0.704849  [   64/  130]
train() client id: f_00008-0-2 loss: 0.842723  [   96/  130]
train() client id: f_00008-0-3 loss: 0.755209  [  128/  130]
train() client id: f_00008-1-0 loss: 0.702730  [   32/  130]
train() client id: f_00008-1-1 loss: 0.835955  [   64/  130]
train() client id: f_00008-1-2 loss: 0.806537  [   96/  130]
train() client id: f_00008-1-3 loss: 0.667630  [  128/  130]
train() client id: f_00008-2-0 loss: 0.650334  [   32/  130]
train() client id: f_00008-2-1 loss: 0.745259  [   64/  130]
train() client id: f_00008-2-2 loss: 0.890109  [   96/  130]
train() client id: f_00008-2-3 loss: 0.720402  [  128/  130]
train() client id: f_00008-3-0 loss: 0.824536  [   32/  130]
train() client id: f_00008-3-1 loss: 0.712684  [   64/  130]
train() client id: f_00008-3-2 loss: 0.741951  [   96/  130]
train() client id: f_00008-3-3 loss: 0.729527  [  128/  130]
train() client id: f_00008-4-0 loss: 0.783327  [   32/  130]
train() client id: f_00008-4-1 loss: 0.736441  [   64/  130]
train() client id: f_00008-4-2 loss: 0.762129  [   96/  130]
train() client id: f_00008-4-3 loss: 0.729164  [  128/  130]
train() client id: f_00008-5-0 loss: 0.668946  [   32/  130]
train() client id: f_00008-5-1 loss: 0.705608  [   64/  130]
train() client id: f_00008-5-2 loss: 0.815246  [   96/  130]
train() client id: f_00008-5-3 loss: 0.804472  [  128/  130]
train() client id: f_00008-6-0 loss: 0.687516  [   32/  130]
train() client id: f_00008-6-1 loss: 0.790063  [   64/  130]
train() client id: f_00008-6-2 loss: 0.808381  [   96/  130]
train() client id: f_00008-6-3 loss: 0.708578  [  128/  130]
train() client id: f_00008-7-0 loss: 0.775638  [   32/  130]
train() client id: f_00008-7-1 loss: 0.652734  [   64/  130]
train() client id: f_00008-7-2 loss: 0.819698  [   96/  130]
train() client id: f_00008-7-3 loss: 0.762633  [  128/  130]
train() client id: f_00008-8-0 loss: 0.744630  [   32/  130]
train() client id: f_00008-8-1 loss: 0.682364  [   64/  130]
train() client id: f_00008-8-2 loss: 0.718721  [   96/  130]
train() client id: f_00008-8-3 loss: 0.855041  [  128/  130]
train() client id: f_00008-9-0 loss: 0.833924  [   32/  130]
train() client id: f_00008-9-1 loss: 0.765084  [   64/  130]
train() client id: f_00008-9-2 loss: 0.623641  [   96/  130]
train() client id: f_00008-9-3 loss: 0.703852  [  128/  130]
train() client id: f_00008-10-0 loss: 0.605419  [   32/  130]
train() client id: f_00008-10-1 loss: 0.776437  [   64/  130]
train() client id: f_00008-10-2 loss: 0.719076  [   96/  130]
train() client id: f_00008-10-3 loss: 0.901106  [  128/  130]
train() client id: f_00008-11-0 loss: 0.737392  [   32/  130]
train() client id: f_00008-11-1 loss: 0.755877  [   64/  130]
train() client id: f_00008-11-2 loss: 0.748712  [   96/  130]
train() client id: f_00008-11-3 loss: 0.759610  [  128/  130]
train() client id: f_00009-0-0 loss: 1.059455  [   32/  118]
train() client id: f_00009-0-1 loss: 1.111137  [   64/  118]
train() client id: f_00009-0-2 loss: 1.024653  [   96/  118]
train() client id: f_00009-1-0 loss: 1.086281  [   32/  118]
train() client id: f_00009-1-1 loss: 1.015005  [   64/  118]
train() client id: f_00009-1-2 loss: 0.953820  [   96/  118]
train() client id: f_00009-2-0 loss: 1.039658  [   32/  118]
train() client id: f_00009-2-1 loss: 0.978806  [   64/  118]
train() client id: f_00009-2-2 loss: 1.038306  [   96/  118]
train() client id: f_00009-3-0 loss: 0.982023  [   32/  118]
train() client id: f_00009-3-1 loss: 0.940538  [   64/  118]
train() client id: f_00009-3-2 loss: 0.970337  [   96/  118]
train() client id: f_00009-4-0 loss: 1.010461  [   32/  118]
train() client id: f_00009-4-1 loss: 1.069787  [   64/  118]
train() client id: f_00009-4-2 loss: 0.818822  [   96/  118]
train() client id: f_00009-5-0 loss: 0.872454  [   32/  118]
train() client id: f_00009-5-1 loss: 1.031592  [   64/  118]
train() client id: f_00009-5-2 loss: 0.841727  [   96/  118]
train() client id: f_00009-6-0 loss: 0.821181  [   32/  118]
train() client id: f_00009-6-1 loss: 0.998954  [   64/  118]
train() client id: f_00009-6-2 loss: 0.915347  [   96/  118]
train() client id: f_00009-7-0 loss: 0.894277  [   32/  118]
train() client id: f_00009-7-1 loss: 0.923297  [   64/  118]
train() client id: f_00009-7-2 loss: 0.798336  [   96/  118]
train() client id: f_00009-8-0 loss: 0.999040  [   32/  118]
train() client id: f_00009-8-1 loss: 0.861075  [   64/  118]
train() client id: f_00009-8-2 loss: 0.910753  [   96/  118]
train() client id: f_00009-9-0 loss: 0.877014  [   32/  118]
train() client id: f_00009-9-1 loss: 0.953118  [   64/  118]
train() client id: f_00009-9-2 loss: 0.852326  [   96/  118]
train() client id: f_00009-10-0 loss: 1.088057  [   32/  118]
train() client id: f_00009-10-1 loss: 0.811081  [   64/  118]
train() client id: f_00009-10-2 loss: 0.731543  [   96/  118]
train() client id: f_00009-11-0 loss: 0.883166  [   32/  118]
train() client id: f_00009-11-1 loss: 1.101738  [   64/  118]
train() client id: f_00009-11-2 loss: 0.828975  [   96/  118]
At round 48 accuracy: 0.649867374005305
At round 48 training accuracy: 0.5902079141515761
At round 48 training loss: 0.8392586970659757
update_location
xs = [  -3.9056584     4.20031788  260.00902392   18.81129433    0.97929623
    3.95640986 -222.44319194 -201.32485185  244.66397685 -187.06087855]
ys = [ 252.5879595   235.55583871    1.32061395 -222.45517586  214.35018685
  197.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [271.6908748  255.93787492 278.57931822 244.62250522 236.53110075
 221.68917748 243.90134108 224.79406626 264.89456684 212.15038096]
dists_bs = [187.86475237 189.15539519 468.39874042 442.19162659 180.32512278
 180.40477296 183.85181945 176.15576144 448.22134189 171.42332987]
uav_gains = [4.75955329e-12 6.59322551e-12 4.11980872e-12 8.22994915e-12
 9.55255008e-12 1.22821086e-11 8.34286394e-12 1.16790101e-11
 5.48618892e-12 1.42491555e-11]
bs_gains = [4.74818064e-11 4.65802305e-11 3.67764727e-12 4.32100377e-12
 5.32520858e-11 5.31862805e-11 5.04410218e-11 5.68568637e-11
 4.16020754e-12 6.13618281e-11]
Round 49
-------------------------------
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.53180875  9.28555743  4.47092374  1.6235926  10.7068955   5.1513321
  2.0049889   6.33296008  4.67865356  4.17679842]
obj_prev = 52.96351106757935
eta_min = 4.606985121778528e-21	eta_max = 0.9400638118405741
af = 11.129531482767057	bf = 1.180489388448874	zeta = 12.242484631043764	eta = 0.909090909090909
af = 11.129531482767057	bf = 1.180489388448874	zeta = 24.65371738730223	eta = 0.45143421204703454
af = 11.129531482767057	bf = 1.180489388448874	zeta = 18.287853905358034	eta = 0.608575043324591
af = 11.129531482767057	bf = 1.180489388448874	zeta = 17.136885852504452	eta = 0.6494488892881634
af = 11.129531482767057	bf = 1.180489388448874	zeta = 17.071238826404898	eta = 0.6519463289068677
af = 11.129531482767057	bf = 1.180489388448874	zeta = 17.07100360444631	eta = 0.6519553120982448
eta = 0.6519553120982448
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [0.03529932 0.07424063 0.03473901 0.01204659 0.08572692 0.04090239
 0.01512828 0.05014745 0.03641993 0.03305809]
ene_total = [1.60196419 2.67053184 1.60796312 0.76579092 3.04020143 1.57233453
 0.86514606 1.976894   1.66129063 1.30888688]
ti_comp = [0.70507767 0.76819322 0.69764277 0.72788409 0.77017386 0.77015604
 0.7283619  0.73907045 0.6981679  0.77215943]
ti_coms = [0.13618224 0.07306669 0.14361714 0.11337582 0.07108605 0.07110387
 0.11289801 0.10218946 0.14309201 0.06910048]
t_total = [27.54979439 27.54979439 27.54979439 27.54979439 27.54979439 27.54979439
 27.54979439 27.54979439 27.54979439 27.54979439]
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [5.52974468e-06 4.33375200e-05 5.38351868e-06 2.06228308e-07
 6.63825764e-05 7.21055148e-06 4.07900477e-07 1.44296163e-05
 6.19409234e-06 3.78703601e-06]
ene_total = [0.44615416 0.2406999  0.47049736 0.37129277 0.23496834 0.23308889
 0.36973465 0.33512519 0.46880422 0.22641604]
optimize_network iter = 0 obj = 3.396781522582919
eta = 0.6519553120982448
freqs = [25032220.23451398 48321587.59240132 24897421.60469212  8275076.45541563
 55654263.85367705 26554609.35010472 10385139.39706882 33926025.18152012
 26082502.95796476 21406261.0487775 ]
eta_min = 0.6519553120982468	eta_max = 0.6979914102198819
af = 0.004073073698070901	bf = 1.180489388448874	zeta = 0.004480381067877992	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [1.23203954e-06 9.65569680e-06 1.19946005e-06 4.59481302e-08
 1.47901871e-05 1.60652707e-06 9.08811424e-08 3.21495093e-06
 1.38005769e-06 8.43760128e-07]
ene_total = [1.70755859 0.91729463 1.80077061 1.42147114 0.89310582 0.89167624
 1.41548626 1.28161791 1.79420944 0.86646288]
ti_comp = [0.59380367 0.65691922 0.58636878 0.6166101  0.65889986 0.65888204
 0.6170879  0.62779645 0.5868939  0.66088543]
ti_coms = [0.13618224 0.07306669 0.14361714 0.11337582 0.07108605 0.07110387
 0.11289801 0.10218946 0.14309201 0.06910048]
t_total = [27.54979439 27.54979439 27.54979439 27.54979439 27.54979439 27.54979439
 27.54979439 27.54979439 27.54979439 27.54979439]
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [4.62920539e-06 3.51879741e-05 4.52484764e-06 1.70633650e-07
 5.38525197e-05 5.84956806e-06 3.37417651e-07 1.18741342e-05
 5.20464897e-06 3.06954669e-06]
ene_total = [0.51412882 0.27708296 0.54218432 0.42788873 0.2703124  0.26856798
 0.42609178 0.38611291 0.54022815 0.26090226]
optimize_network iter = 1 obj = 3.9135003002771818
eta = 0.6979914102198819
freqs = [24982229.19460944 47493855.941672   24897421.60469214  8210343.90468077
 54677108.13463603 26088475.84559726 10302681.03588401 33568945.36426646
 26078780.65421337 21021289.55092173]
eta_min = 0.697991410219886	eta_max = 0.6979914102198779
af = 0.003953064498406836	bf = 1.180489388448874	zeta = 0.004348370948247519	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [1.22712352e-06 9.32773270e-06 1.19946005e-06 4.52320749e-08
 1.42753859e-05 1.55062088e-06 8.94436732e-08 3.14763077e-06
 1.37966381e-06 8.13684556e-07]
ene_total = [1.70755797 0.91725351 1.80077061 1.42147105 0.89304128 0.89166923
 1.41548608 1.28160947 1.79420939 0.86645911]
ti_comp = [0.59380367 0.65691922 0.58636878 0.6166101  0.65889986 0.65888204
 0.6170879  0.62779645 0.5868939  0.66088543]
ti_coms = [0.13618224 0.07306669 0.14361714 0.11337582 0.07108605 0.07110387
 0.11289801 0.10218946 0.14309201 0.06910048]
t_total = [27.54979439 27.54979439 27.54979439 27.54979439 27.54979439 27.54979439
 27.54979439 27.54979439 27.54979439 27.54979439]
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [4.62920539e-06 3.51879741e-05 4.52484764e-06 1.70633650e-07
 5.38525197e-05 5.84956806e-06 3.37417651e-07 1.18741342e-05
 5.20464897e-06 3.06954669e-06]
ene_total = [0.51412882 0.27708296 0.54218432 0.42788873 0.2703124  0.26856798
 0.42609178 0.38611291 0.54022815 0.26090226]
optimize_network iter = 2 obj = 3.9135003002771303
eta = 0.6979914102198779
freqs = [24982229.19460944 47493855.94167206 24897421.60469212  8210343.90468077
 54677108.13463611 26088475.8455973  10302681.03588402 33568945.36426648
 26078780.65421337 21021289.55092177]
Done!
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [4.32509460e-06 3.28763371e-05 4.22759253e-06 1.59424052e-07
 5.03147350e-05 5.46528683e-06 3.15251352e-07 1.10940754e-05
 4.86273502e-06 2.86789605e-06]
ene_total = [0.01362255 0.00733955 0.01436594 0.01133774 0.00715892 0.00711585
 0.01129012 0.01023004 0.01431406 0.00691292]
At round 49 energy consumption: 0.10368768459529902
At round 49 eta: 0.6979914102198779
At round 49 a_n: 11.055310507788512
At round 49 local rounds: 11.773444044723618
At round 49 global rounds: 37.74017276480797
gradient difference: 0.4828665852546692
train() client id: f_00000-0-0 loss: 1.234429  [   32/  126]
train() client id: f_00000-0-1 loss: 1.113115  [   64/  126]
train() client id: f_00000-0-2 loss: 1.129658  [   96/  126]
train() client id: f_00000-1-0 loss: 1.134318  [   32/  126]
train() client id: f_00000-1-1 loss: 1.025503  [   64/  126]
train() client id: f_00000-1-2 loss: 0.981720  [   96/  126]
train() client id: f_00000-2-0 loss: 0.957658  [   32/  126]
train() client id: f_00000-2-1 loss: 0.988640  [   64/  126]
train() client id: f_00000-2-2 loss: 0.954650  [   96/  126]
train() client id: f_00000-3-0 loss: 0.730202  [   32/  126]
train() client id: f_00000-3-1 loss: 0.986545  [   64/  126]
train() client id: f_00000-3-2 loss: 1.007908  [   96/  126]
train() client id: f_00000-4-0 loss: 0.826798  [   32/  126]
train() client id: f_00000-4-1 loss: 1.004857  [   64/  126]
train() client id: f_00000-4-2 loss: 0.863324  [   96/  126]
train() client id: f_00000-5-0 loss: 0.813050  [   32/  126]
train() client id: f_00000-5-1 loss: 0.943951  [   64/  126]
train() client id: f_00000-5-2 loss: 0.858971  [   96/  126]
train() client id: f_00000-6-0 loss: 0.794132  [   32/  126]
train() client id: f_00000-6-1 loss: 0.910305  [   64/  126]
train() client id: f_00000-6-2 loss: 0.934863  [   96/  126]
train() client id: f_00000-7-0 loss: 0.798296  [   32/  126]
train() client id: f_00000-7-1 loss: 0.884999  [   64/  126]
train() client id: f_00000-7-2 loss: 0.755135  [   96/  126]
train() client id: f_00000-8-0 loss: 0.901226  [   32/  126]
train() client id: f_00000-8-1 loss: 0.989243  [   64/  126]
train() client id: f_00000-8-2 loss: 0.752494  [   96/  126]
train() client id: f_00000-9-0 loss: 0.814947  [   32/  126]
train() client id: f_00000-9-1 loss: 0.916380  [   64/  126]
train() client id: f_00000-9-2 loss: 0.890307  [   96/  126]
train() client id: f_00000-10-0 loss: 0.926881  [   32/  126]
train() client id: f_00000-10-1 loss: 0.777488  [   64/  126]
train() client id: f_00000-10-2 loss: 0.828493  [   96/  126]
train() client id: f_00001-0-0 loss: 0.581166  [   32/  265]
train() client id: f_00001-0-1 loss: 0.547542  [   64/  265]
train() client id: f_00001-0-2 loss: 0.440542  [   96/  265]
train() client id: f_00001-0-3 loss: 0.422147  [  128/  265]
train() client id: f_00001-0-4 loss: 0.519042  [  160/  265]
train() client id: f_00001-0-5 loss: 0.536555  [  192/  265]
train() client id: f_00001-0-6 loss: 0.512793  [  224/  265]
train() client id: f_00001-0-7 loss: 0.511433  [  256/  265]
train() client id: f_00001-1-0 loss: 0.445475  [   32/  265]
train() client id: f_00001-1-1 loss: 0.518045  [   64/  265]
train() client id: f_00001-1-2 loss: 0.574383  [   96/  265]
train() client id: f_00001-1-3 loss: 0.397898  [  128/  265]
train() client id: f_00001-1-4 loss: 0.418088  [  160/  265]
train() client id: f_00001-1-5 loss: 0.557939  [  192/  265]
train() client id: f_00001-1-6 loss: 0.478899  [  224/  265]
train() client id: f_00001-1-7 loss: 0.680033  [  256/  265]
train() client id: f_00001-2-0 loss: 0.489712  [   32/  265]
train() client id: f_00001-2-1 loss: 0.549935  [   64/  265]
train() client id: f_00001-2-2 loss: 0.607838  [   96/  265]
train() client id: f_00001-2-3 loss: 0.487491  [  128/  265]
train() client id: f_00001-2-4 loss: 0.406174  [  160/  265]
train() client id: f_00001-2-5 loss: 0.495411  [  192/  265]
train() client id: f_00001-2-6 loss: 0.516470  [  224/  265]
train() client id: f_00001-2-7 loss: 0.495868  [  256/  265]
train() client id: f_00001-3-0 loss: 0.567193  [   32/  265]
train() client id: f_00001-3-1 loss: 0.521006  [   64/  265]
train() client id: f_00001-3-2 loss: 0.444984  [   96/  265]
train() client id: f_00001-3-3 loss: 0.488255  [  128/  265]
train() client id: f_00001-3-4 loss: 0.431366  [  160/  265]
train() client id: f_00001-3-5 loss: 0.684754  [  192/  265]
train() client id: f_00001-3-6 loss: 0.428182  [  224/  265]
train() client id: f_00001-3-7 loss: 0.419070  [  256/  265]
train() client id: f_00001-4-0 loss: 0.409954  [   32/  265]
train() client id: f_00001-4-1 loss: 0.519965  [   64/  265]
train() client id: f_00001-4-2 loss: 0.639412  [   96/  265]
train() client id: f_00001-4-3 loss: 0.476183  [  128/  265]
train() client id: f_00001-4-4 loss: 0.522225  [  160/  265]
train() client id: f_00001-4-5 loss: 0.453134  [  192/  265]
train() client id: f_00001-4-6 loss: 0.499062  [  224/  265]
train() client id: f_00001-4-7 loss: 0.483669  [  256/  265]
train() client id: f_00001-5-0 loss: 0.518176  [   32/  265]
train() client id: f_00001-5-1 loss: 0.511798  [   64/  265]
train() client id: f_00001-5-2 loss: 0.455182  [   96/  265]
train() client id: f_00001-5-3 loss: 0.648824  [  128/  265]
train() client id: f_00001-5-4 loss: 0.552979  [  160/  265]
train() client id: f_00001-5-5 loss: 0.496916  [  192/  265]
train() client id: f_00001-5-6 loss: 0.408690  [  224/  265]
train() client id: f_00001-5-7 loss: 0.394279  [  256/  265]
train() client id: f_00001-6-0 loss: 0.557603  [   32/  265]
train() client id: f_00001-6-1 loss: 0.430371  [   64/  265]
train() client id: f_00001-6-2 loss: 0.483397  [   96/  265]
train() client id: f_00001-6-3 loss: 0.519148  [  128/  265]
train() client id: f_00001-6-4 loss: 0.404486  [  160/  265]
train() client id: f_00001-6-5 loss: 0.485929  [  192/  265]
train() client id: f_00001-6-6 loss: 0.543430  [  224/  265]
train() client id: f_00001-6-7 loss: 0.407591  [  256/  265]
train() client id: f_00001-7-0 loss: 0.522367  [   32/  265]
train() client id: f_00001-7-1 loss: 0.466796  [   64/  265]
train() client id: f_00001-7-2 loss: 0.420607  [   96/  265]
train() client id: f_00001-7-3 loss: 0.517849  [  128/  265]
train() client id: f_00001-7-4 loss: 0.513836  [  160/  265]
train() client id: f_00001-7-5 loss: 0.532412  [  192/  265]
train() client id: f_00001-7-6 loss: 0.453210  [  224/  265]
train() client id: f_00001-7-7 loss: 0.495150  [  256/  265]
train() client id: f_00001-8-0 loss: 0.427634  [   32/  265]
train() client id: f_00001-8-1 loss: 0.474399  [   64/  265]
train() client id: f_00001-8-2 loss: 0.380137  [   96/  265]
train() client id: f_00001-8-3 loss: 0.511123  [  128/  265]
train() client id: f_00001-8-4 loss: 0.390354  [  160/  265]
train() client id: f_00001-8-5 loss: 0.550447  [  192/  265]
train() client id: f_00001-8-6 loss: 0.585657  [  224/  265]
train() client id: f_00001-8-7 loss: 0.669204  [  256/  265]
train() client id: f_00001-9-0 loss: 0.488871  [   32/  265]
train() client id: f_00001-9-1 loss: 0.654171  [   64/  265]
train() client id: f_00001-9-2 loss: 0.494292  [   96/  265]
train() client id: f_00001-9-3 loss: 0.403887  [  128/  265]
train() client id: f_00001-9-4 loss: 0.399280  [  160/  265]
train() client id: f_00001-9-5 loss: 0.613341  [  192/  265]
train() client id: f_00001-9-6 loss: 0.420817  [  224/  265]
train() client id: f_00001-9-7 loss: 0.512721  [  256/  265]
train() client id: f_00001-10-0 loss: 0.554829  [   32/  265]
train() client id: f_00001-10-1 loss: 0.472446  [   64/  265]
train() client id: f_00001-10-2 loss: 0.509501  [   96/  265]
train() client id: f_00001-10-3 loss: 0.397890  [  128/  265]
train() client id: f_00001-10-4 loss: 0.497703  [  160/  265]
train() client id: f_00001-10-5 loss: 0.519384  [  192/  265]
train() client id: f_00001-10-6 loss: 0.533504  [  224/  265]
train() client id: f_00001-10-7 loss: 0.504657  [  256/  265]
train() client id: f_00002-0-0 loss: 1.358485  [   32/  124]
train() client id: f_00002-0-1 loss: 1.442144  [   64/  124]
train() client id: f_00002-0-2 loss: 1.187502  [   96/  124]
train() client id: f_00002-1-0 loss: 1.386961  [   32/  124]
train() client id: f_00002-1-1 loss: 1.299277  [   64/  124]
train() client id: f_00002-1-2 loss: 1.176862  [   96/  124]
train() client id: f_00002-2-0 loss: 1.336475  [   32/  124]
train() client id: f_00002-2-1 loss: 1.216391  [   64/  124]
train() client id: f_00002-2-2 loss: 1.334349  [   96/  124]
train() client id: f_00002-3-0 loss: 1.374759  [   32/  124]
train() client id: f_00002-3-1 loss: 1.218710  [   64/  124]
train() client id: f_00002-3-2 loss: 1.138999  [   96/  124]
train() client id: f_00002-4-0 loss: 1.084847  [   32/  124]
train() client id: f_00002-4-1 loss: 1.229814  [   64/  124]
train() client id: f_00002-4-2 loss: 1.271970  [   96/  124]
train() client id: f_00002-5-0 loss: 1.281360  [   32/  124]
train() client id: f_00002-5-1 loss: 1.124420  [   64/  124]
train() client id: f_00002-5-2 loss: 1.094267  [   96/  124]
train() client id: f_00002-6-0 loss: 0.979774  [   32/  124]
train() client id: f_00002-6-1 loss: 1.356097  [   64/  124]
train() client id: f_00002-6-2 loss: 1.218006  [   96/  124]
train() client id: f_00002-7-0 loss: 1.170138  [   32/  124]
train() client id: f_00002-7-1 loss: 1.130017  [   64/  124]
train() client id: f_00002-7-2 loss: 1.031912  [   96/  124]
train() client id: f_00002-8-0 loss: 1.009650  [   32/  124]
train() client id: f_00002-8-1 loss: 1.426500  [   64/  124]
train() client id: f_00002-8-2 loss: 0.907027  [   96/  124]
train() client id: f_00002-9-0 loss: 1.095478  [   32/  124]
train() client id: f_00002-9-1 loss: 1.129439  [   64/  124]
train() client id: f_00002-9-2 loss: 1.102776  [   96/  124]
train() client id: f_00002-10-0 loss: 1.083631  [   32/  124]
train() client id: f_00002-10-1 loss: 1.321682  [   64/  124]
train() client id: f_00002-10-2 loss: 0.961410  [   96/  124]
train() client id: f_00003-0-0 loss: 0.630720  [   32/   43]
train() client id: f_00003-1-0 loss: 0.740118  [   32/   43]
train() client id: f_00003-2-0 loss: 0.600878  [   32/   43]
train() client id: f_00003-3-0 loss: 0.676883  [   32/   43]
train() client id: f_00003-4-0 loss: 0.449177  [   32/   43]
train() client id: f_00003-5-0 loss: 0.509019  [   32/   43]
train() client id: f_00003-6-0 loss: 0.548448  [   32/   43]
train() client id: f_00003-7-0 loss: 0.618250  [   32/   43]
train() client id: f_00003-8-0 loss: 0.614293  [   32/   43]
train() client id: f_00003-9-0 loss: 0.740147  [   32/   43]
train() client id: f_00003-10-0 loss: 0.534603  [   32/   43]
train() client id: f_00004-0-0 loss: 0.856813  [   32/  306]
train() client id: f_00004-0-1 loss: 0.875135  [   64/  306]
train() client id: f_00004-0-2 loss: 0.931634  [   96/  306]
train() client id: f_00004-0-3 loss: 0.863379  [  128/  306]
train() client id: f_00004-0-4 loss: 0.895936  [  160/  306]
train() client id: f_00004-0-5 loss: 0.805732  [  192/  306]
train() client id: f_00004-0-6 loss: 0.955343  [  224/  306]
train() client id: f_00004-0-7 loss: 0.893951  [  256/  306]
train() client id: f_00004-0-8 loss: 0.825531  [  288/  306]
train() client id: f_00004-1-0 loss: 0.804658  [   32/  306]
train() client id: f_00004-1-1 loss: 0.988872  [   64/  306]
train() client id: f_00004-1-2 loss: 1.060950  [   96/  306]
train() client id: f_00004-1-3 loss: 0.900893  [  128/  306]
train() client id: f_00004-1-4 loss: 0.819130  [  160/  306]
train() client id: f_00004-1-5 loss: 0.809746  [  192/  306]
train() client id: f_00004-1-6 loss: 0.806281  [  224/  306]
train() client id: f_00004-1-7 loss: 0.867340  [  256/  306]
train() client id: f_00004-1-8 loss: 0.846933  [  288/  306]
train() client id: f_00004-2-0 loss: 0.944987  [   32/  306]
train() client id: f_00004-2-1 loss: 0.945293  [   64/  306]
train() client id: f_00004-2-2 loss: 0.873877  [   96/  306]
train() client id: f_00004-2-3 loss: 0.843411  [  128/  306]
train() client id: f_00004-2-4 loss: 0.865645  [  160/  306]
train() client id: f_00004-2-5 loss: 0.850941  [  192/  306]
train() client id: f_00004-2-6 loss: 0.870430  [  224/  306]
train() client id: f_00004-2-7 loss: 0.870986  [  256/  306]
train() client id: f_00004-2-8 loss: 0.762420  [  288/  306]
train() client id: f_00004-3-0 loss: 0.855889  [   32/  306]
train() client id: f_00004-3-1 loss: 0.788938  [   64/  306]
train() client id: f_00004-3-2 loss: 0.830626  [   96/  306]
train() client id: f_00004-3-3 loss: 0.878148  [  128/  306]
train() client id: f_00004-3-4 loss: 0.960070  [  160/  306]
train() client id: f_00004-3-5 loss: 0.864395  [  192/  306]
train() client id: f_00004-3-6 loss: 1.026834  [  224/  306]
train() client id: f_00004-3-7 loss: 0.807816  [  256/  306]
train() client id: f_00004-3-8 loss: 0.890719  [  288/  306]
train() client id: f_00004-4-0 loss: 0.880207  [   32/  306]
train() client id: f_00004-4-1 loss: 0.869805  [   64/  306]
train() client id: f_00004-4-2 loss: 0.910513  [   96/  306]
train() client id: f_00004-4-3 loss: 0.951701  [  128/  306]
train() client id: f_00004-4-4 loss: 0.831886  [  160/  306]
train() client id: f_00004-4-5 loss: 0.817047  [  192/  306]
train() client id: f_00004-4-6 loss: 0.984538  [  224/  306]
train() client id: f_00004-4-7 loss: 0.855338  [  256/  306]
train() client id: f_00004-4-8 loss: 0.778883  [  288/  306]
train() client id: f_00004-5-0 loss: 0.929919  [   32/  306]
train() client id: f_00004-5-1 loss: 0.738848  [   64/  306]
train() client id: f_00004-5-2 loss: 0.801825  [   96/  306]
train() client id: f_00004-5-3 loss: 0.777391  [  128/  306]
train() client id: f_00004-5-4 loss: 0.893335  [  160/  306]
train() client id: f_00004-5-5 loss: 0.958649  [  192/  306]
train() client id: f_00004-5-6 loss: 0.935115  [  224/  306]
train() client id: f_00004-5-7 loss: 0.816545  [  256/  306]
train() client id: f_00004-5-8 loss: 0.932772  [  288/  306]
train() client id: f_00004-6-0 loss: 0.886477  [   32/  306]
train() client id: f_00004-6-1 loss: 0.708021  [   64/  306]
train() client id: f_00004-6-2 loss: 0.943530  [   96/  306]
train() client id: f_00004-6-3 loss: 0.978887  [  128/  306]
train() client id: f_00004-6-4 loss: 0.798457  [  160/  306]
train() client id: f_00004-6-5 loss: 0.963406  [  192/  306]
train() client id: f_00004-6-6 loss: 0.973653  [  224/  306]
train() client id: f_00004-6-7 loss: 0.903648  [  256/  306]
train() client id: f_00004-6-8 loss: 0.742675  [  288/  306]
train() client id: f_00004-7-0 loss: 0.773724  [   32/  306]
train() client id: f_00004-7-1 loss: 0.852356  [   64/  306]
train() client id: f_00004-7-2 loss: 0.844225  [   96/  306]
train() client id: f_00004-7-3 loss: 0.857459  [  128/  306]
train() client id: f_00004-7-4 loss: 0.766497  [  160/  306]
train() client id: f_00004-7-5 loss: 0.867466  [  192/  306]
train() client id: f_00004-7-6 loss: 1.051997  [  224/  306]
train() client id: f_00004-7-7 loss: 0.904208  [  256/  306]
train() client id: f_00004-7-8 loss: 0.874499  [  288/  306]
train() client id: f_00004-8-0 loss: 0.819940  [   32/  306]
train() client id: f_00004-8-1 loss: 0.830587  [   64/  306]
train() client id: f_00004-8-2 loss: 0.854240  [   96/  306]
train() client id: f_00004-8-3 loss: 0.809753  [  128/  306]
train() client id: f_00004-8-4 loss: 0.883612  [  160/  306]
train() client id: f_00004-8-5 loss: 0.944004  [  192/  306]
train() client id: f_00004-8-6 loss: 0.933721  [  224/  306]
train() client id: f_00004-8-7 loss: 0.874083  [  256/  306]
train() client id: f_00004-8-8 loss: 0.801181  [  288/  306]
train() client id: f_00004-9-0 loss: 0.862885  [   32/  306]
train() client id: f_00004-9-1 loss: 0.805090  [   64/  306]
train() client id: f_00004-9-2 loss: 0.917689  [   96/  306]
train() client id: f_00004-9-3 loss: 0.871198  [  128/  306]
train() client id: f_00004-9-4 loss: 0.925795  [  160/  306]
train() client id: f_00004-9-5 loss: 0.805534  [  192/  306]
train() client id: f_00004-9-6 loss: 0.870470  [  224/  306]
train() client id: f_00004-9-7 loss: 0.892618  [  256/  306]
train() client id: f_00004-9-8 loss: 0.896111  [  288/  306]
train() client id: f_00004-10-0 loss: 0.835993  [   32/  306]
train() client id: f_00004-10-1 loss: 0.968129  [   64/  306]
train() client id: f_00004-10-2 loss: 0.882770  [   96/  306]
train() client id: f_00004-10-3 loss: 0.823954  [  128/  306]
train() client id: f_00004-10-4 loss: 0.827868  [  160/  306]
train() client id: f_00004-10-5 loss: 0.825451  [  192/  306]
train() client id: f_00004-10-6 loss: 0.908496  [  224/  306]
train() client id: f_00004-10-7 loss: 0.863659  [  256/  306]
train() client id: f_00004-10-8 loss: 0.796493  [  288/  306]
train() client id: f_00005-0-0 loss: 0.640856  [   32/  146]
train() client id: f_00005-0-1 loss: 0.613490  [   64/  146]
train() client id: f_00005-0-2 loss: 0.616345  [   96/  146]
train() client id: f_00005-0-3 loss: 0.541575  [  128/  146]
train() client id: f_00005-1-0 loss: 0.383145  [   32/  146]
train() client id: f_00005-1-1 loss: 0.641627  [   64/  146]
train() client id: f_00005-1-2 loss: 0.492307  [   96/  146]
train() client id: f_00005-1-3 loss: 0.741003  [  128/  146]
train() client id: f_00005-2-0 loss: 0.473151  [   32/  146]
train() client id: f_00005-2-1 loss: 0.683490  [   64/  146]
train() client id: f_00005-2-2 loss: 0.715338  [   96/  146]
train() client id: f_00005-2-3 loss: 0.249651  [  128/  146]
train() client id: f_00005-3-0 loss: 0.568890  [   32/  146]
train() client id: f_00005-3-1 loss: 0.464588  [   64/  146]
train() client id: f_00005-3-2 loss: 0.924592  [   96/  146]
train() client id: f_00005-3-3 loss: 0.557181  [  128/  146]
train() client id: f_00005-4-0 loss: 0.943971  [   32/  146]
train() client id: f_00005-4-1 loss: 0.607979  [   64/  146]
train() client id: f_00005-4-2 loss: 0.564534  [   96/  146]
train() client id: f_00005-4-3 loss: 0.324193  [  128/  146]
train() client id: f_00005-5-0 loss: 0.686150  [   32/  146]
train() client id: f_00005-5-1 loss: 0.487228  [   64/  146]
train() client id: f_00005-5-2 loss: 0.743698  [   96/  146]
train() client id: f_00005-5-3 loss: 0.510409  [  128/  146]
train() client id: f_00005-6-0 loss: 0.581993  [   32/  146]
train() client id: f_00005-6-1 loss: 0.549447  [   64/  146]
train() client id: f_00005-6-2 loss: 0.596640  [   96/  146]
train() client id: f_00005-6-3 loss: 0.563810  [  128/  146]
train() client id: f_00005-7-0 loss: 0.593568  [   32/  146]
train() client id: f_00005-7-1 loss: 0.558285  [   64/  146]
train() client id: f_00005-7-2 loss: 0.603558  [   96/  146]
train() client id: f_00005-7-3 loss: 0.464774  [  128/  146]
train() client id: f_00005-8-0 loss: 0.427105  [   32/  146]
train() client id: f_00005-8-1 loss: 0.687259  [   64/  146]
train() client id: f_00005-8-2 loss: 0.445707  [   96/  146]
train() client id: f_00005-8-3 loss: 0.660634  [  128/  146]
train() client id: f_00005-9-0 loss: 0.484481  [   32/  146]
train() client id: f_00005-9-1 loss: 0.735500  [   64/  146]
train() client id: f_00005-9-2 loss: 0.778654  [   96/  146]
train() client id: f_00005-9-3 loss: 0.399318  [  128/  146]
train() client id: f_00005-10-0 loss: 0.796937  [   32/  146]
train() client id: f_00005-10-1 loss: 0.511323  [   64/  146]
train() client id: f_00005-10-2 loss: 0.451390  [   96/  146]
train() client id: f_00005-10-3 loss: 0.571192  [  128/  146]
train() client id: f_00006-0-0 loss: 0.484637  [   32/   54]
train() client id: f_00006-1-0 loss: 0.430065  [   32/   54]
train() client id: f_00006-2-0 loss: 0.434464  [   32/   54]
train() client id: f_00006-3-0 loss: 0.423024  [   32/   54]
train() client id: f_00006-4-0 loss: 0.414317  [   32/   54]
train() client id: f_00006-5-0 loss: 0.448198  [   32/   54]
train() client id: f_00006-6-0 loss: 0.493968  [   32/   54]
train() client id: f_00006-7-0 loss: 0.415836  [   32/   54]
train() client id: f_00006-8-0 loss: 0.478182  [   32/   54]
train() client id: f_00006-9-0 loss: 0.413877  [   32/   54]
train() client id: f_00006-10-0 loss: 0.414690  [   32/   54]
train() client id: f_00007-0-0 loss: 0.599490  [   32/  179]
train() client id: f_00007-0-1 loss: 0.692317  [   64/  179]
train() client id: f_00007-0-2 loss: 0.485363  [   96/  179]
train() client id: f_00007-0-3 loss: 0.601806  [  128/  179]
train() client id: f_00007-0-4 loss: 0.731935  [  160/  179]
train() client id: f_00007-1-0 loss: 0.465324  [   32/  179]
train() client id: f_00007-1-1 loss: 0.690890  [   64/  179]
train() client id: f_00007-1-2 loss: 0.616255  [   96/  179]
train() client id: f_00007-1-3 loss: 0.512148  [  128/  179]
train() client id: f_00007-1-4 loss: 0.666572  [  160/  179]
train() client id: f_00007-2-0 loss: 0.439985  [   32/  179]
train() client id: f_00007-2-1 loss: 0.687331  [   64/  179]
train() client id: f_00007-2-2 loss: 0.551527  [   96/  179]
train() client id: f_00007-2-3 loss: 0.672294  [  128/  179]
train() client id: f_00007-2-4 loss: 0.714197  [  160/  179]
train() client id: f_00007-3-0 loss: 0.747337  [   32/  179]
train() client id: f_00007-3-1 loss: 0.513921  [   64/  179]
train() client id: f_00007-3-2 loss: 0.618110  [   96/  179]
train() client id: f_00007-3-3 loss: 0.557312  [  128/  179]
train() client id: f_00007-3-4 loss: 0.507091  [  160/  179]
train() client id: f_00007-4-0 loss: 0.564629  [   32/  179]
train() client id: f_00007-4-1 loss: 0.521096  [   64/  179]
train() client id: f_00007-4-2 loss: 0.438948  [   96/  179]
train() client id: f_00007-4-3 loss: 0.781641  [  128/  179]
train() client id: f_00007-4-4 loss: 0.541171  [  160/  179]
train() client id: f_00007-5-0 loss: 0.461109  [   32/  179]
train() client id: f_00007-5-1 loss: 0.552889  [   64/  179]
train() client id: f_00007-5-2 loss: 0.705110  [   96/  179]
train() client id: f_00007-5-3 loss: 0.518668  [  128/  179]
train() client id: f_00007-5-4 loss: 0.771446  [  160/  179]
train() client id: f_00007-6-0 loss: 0.556219  [   32/  179]
train() client id: f_00007-6-1 loss: 0.603289  [   64/  179]
train() client id: f_00007-6-2 loss: 0.498811  [   96/  179]
train() client id: f_00007-6-3 loss: 0.638291  [  128/  179]
train() client id: f_00007-6-4 loss: 0.634339  [  160/  179]
train() client id: f_00007-7-0 loss: 0.628965  [   32/  179]
train() client id: f_00007-7-1 loss: 0.652553  [   64/  179]
train() client id: f_00007-7-2 loss: 0.502117  [   96/  179]
train() client id: f_00007-7-3 loss: 0.569972  [  128/  179]
train() client id: f_00007-7-4 loss: 0.634519  [  160/  179]
train() client id: f_00007-8-0 loss: 0.636759  [   32/  179]
train() client id: f_00007-8-1 loss: 0.521752  [   64/  179]
train() client id: f_00007-8-2 loss: 0.704981  [   96/  179]
train() client id: f_00007-8-3 loss: 0.579626  [  128/  179]
train() client id: f_00007-8-4 loss: 0.563526  [  160/  179]
train() client id: f_00007-9-0 loss: 0.528974  [   32/  179]
train() client id: f_00007-9-1 loss: 0.426932  [   64/  179]
train() client id: f_00007-9-2 loss: 0.705162  [   96/  179]
train() client id: f_00007-9-3 loss: 0.601888  [  128/  179]
train() client id: f_00007-9-4 loss: 0.560590  [  160/  179]
train() client id: f_00007-10-0 loss: 0.645473  [   32/  179]
train() client id: f_00007-10-1 loss: 0.674693  [   64/  179]
train() client id: f_00007-10-2 loss: 0.563812  [   96/  179]
train() client id: f_00007-10-3 loss: 0.429170  [  128/  179]
train() client id: f_00007-10-4 loss: 0.472801  [  160/  179]
train() client id: f_00008-0-0 loss: 0.892814  [   32/  130]
train() client id: f_00008-0-1 loss: 0.635964  [   64/  130]
train() client id: f_00008-0-2 loss: 0.617332  [   96/  130]
train() client id: f_00008-0-3 loss: 0.576038  [  128/  130]
train() client id: f_00008-1-0 loss: 0.690697  [   32/  130]
train() client id: f_00008-1-1 loss: 0.676524  [   64/  130]
train() client id: f_00008-1-2 loss: 0.621436  [   96/  130]
train() client id: f_00008-1-3 loss: 0.683825  [  128/  130]
train() client id: f_00008-2-0 loss: 0.758553  [   32/  130]
train() client id: f_00008-2-1 loss: 0.674747  [   64/  130]
train() client id: f_00008-2-2 loss: 0.654208  [   96/  130]
train() client id: f_00008-2-3 loss: 0.622557  [  128/  130]
train() client id: f_00008-3-0 loss: 0.724628  [   32/  130]
train() client id: f_00008-3-1 loss: 0.758934  [   64/  130]
train() client id: f_00008-3-2 loss: 0.595779  [   96/  130]
train() client id: f_00008-3-3 loss: 0.655061  [  128/  130]
train() client id: f_00008-4-0 loss: 0.637158  [   32/  130]
train() client id: f_00008-4-1 loss: 0.805527  [   64/  130]
train() client id: f_00008-4-2 loss: 0.692370  [   96/  130]
train() client id: f_00008-4-3 loss: 0.610554  [  128/  130]
train() client id: f_00008-5-0 loss: 0.756283  [   32/  130]
train() client id: f_00008-5-1 loss: 0.746115  [   64/  130]
train() client id: f_00008-5-2 loss: 0.575442  [   96/  130]
train() client id: f_00008-5-3 loss: 0.635816  [  128/  130]
train() client id: f_00008-6-0 loss: 0.703801  [   32/  130]
train() client id: f_00008-6-1 loss: 0.686522  [   64/  130]
train() client id: f_00008-6-2 loss: 0.673231  [   96/  130]
train() client id: f_00008-6-3 loss: 0.674511  [  128/  130]
train() client id: f_00008-7-0 loss: 0.628779  [   32/  130]
train() client id: f_00008-7-1 loss: 0.740355  [   64/  130]
train() client id: f_00008-7-2 loss: 0.739503  [   96/  130]
train() client id: f_00008-7-3 loss: 0.632657  [  128/  130]
train() client id: f_00008-8-0 loss: 0.632007  [   32/  130]
train() client id: f_00008-8-1 loss: 0.745714  [   64/  130]
train() client id: f_00008-8-2 loss: 0.639447  [   96/  130]
train() client id: f_00008-8-3 loss: 0.725239  [  128/  130]
train() client id: f_00008-9-0 loss: 0.615114  [   32/  130]
train() client id: f_00008-9-1 loss: 0.737477  [   64/  130]
train() client id: f_00008-9-2 loss: 0.750248  [   96/  130]
train() client id: f_00008-9-3 loss: 0.602726  [  128/  130]
train() client id: f_00008-10-0 loss: 0.714870  [   32/  130]
train() client id: f_00008-10-1 loss: 0.567201  [   64/  130]
train() client id: f_00008-10-2 loss: 0.772746  [   96/  130]
train() client id: f_00008-10-3 loss: 0.654965  [  128/  130]
train() client id: f_00009-0-0 loss: 1.166020  [   32/  118]
train() client id: f_00009-0-1 loss: 1.054437  [   64/  118]
train() client id: f_00009-0-2 loss: 1.153479  [   96/  118]
train() client id: f_00009-1-0 loss: 1.053199  [   32/  118]
train() client id: f_00009-1-1 loss: 1.065069  [   64/  118]
train() client id: f_00009-1-2 loss: 1.003149  [   96/  118]
train() client id: f_00009-2-0 loss: 1.096949  [   32/  118]
train() client id: f_00009-2-1 loss: 1.026339  [   64/  118]
train() client id: f_00009-2-2 loss: 0.976708  [   96/  118]
train() client id: f_00009-3-0 loss: 0.957567  [   32/  118]
train() client id: f_00009-3-1 loss: 1.154037  [   64/  118]
train() client id: f_00009-3-2 loss: 0.873645  [   96/  118]
train() client id: f_00009-4-0 loss: 1.049767  [   32/  118]
train() client id: f_00009-4-1 loss: 0.846557  [   64/  118]
train() client id: f_00009-4-2 loss: 1.007297  [   96/  118]
train() client id: f_00009-5-0 loss: 0.933565  [   32/  118]
train() client id: f_00009-5-1 loss: 1.051612  [   64/  118]
train() client id: f_00009-5-2 loss: 0.908731  [   96/  118]
train() client id: f_00009-6-0 loss: 0.943922  [   32/  118]
train() client id: f_00009-6-1 loss: 0.995869  [   64/  118]
train() client id: f_00009-6-2 loss: 0.839036  [   96/  118]
train() client id: f_00009-7-0 loss: 1.027180  [   32/  118]
train() client id: f_00009-7-1 loss: 0.991982  [   64/  118]
train() client id: f_00009-7-2 loss: 0.755906  [   96/  118]
train() client id: f_00009-8-0 loss: 1.043814  [   32/  118]
train() client id: f_00009-8-1 loss: 0.920230  [   64/  118]
train() client id: f_00009-8-2 loss: 0.830312  [   96/  118]
train() client id: f_00009-9-0 loss: 0.809634  [   32/  118]
train() client id: f_00009-9-1 loss: 0.855577  [   64/  118]
train() client id: f_00009-9-2 loss: 0.982039  [   96/  118]
train() client id: f_00009-10-0 loss: 0.826846  [   32/  118]
train() client id: f_00009-10-1 loss: 1.043252  [   64/  118]
train() client id: f_00009-10-2 loss: 0.837026  [   96/  118]
At round 49 accuracy: 0.649867374005305
At round 49 training accuracy: 0.5868544600938967
At round 49 training loss: 0.8317111481260946
update_location
xs = [  -3.9056584     4.20031788  265.00902392   18.81129433    0.97929623
    3.95640986 -227.44319194 -206.32485185  249.66397685 -192.06087855]
ys = [ 257.5879595   240.55583871    1.32061395 -227.45517586  219.35018685
  202.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [276.34545599 260.54702878 283.25170216 249.17809258 241.07149041
 226.1619617  248.46990985 229.28284006 269.51951935 216.57191167]
dists_bs = [189.98432772 190.81488913 473.04594942 446.69115324 181.48182216
 181.10500714 185.2050848  176.97203396 452.90618355 171.84751034]
uav_gains = [4.31692826e-12 6.00226608e-12 3.73822966e-12 7.54002475e-12
 8.79545935e-12 1.14187632e-11 7.64459119e-12 1.08372987e-11
 4.98118754e-12 1.33149743e-11]
bs_gains = [4.60133985e-11 4.54548008e-11 3.57737746e-12 4.20023415e-12
 5.23071814e-11 5.26124836e-11 4.94158134e-11 5.61256116e-11
 4.04083377e-12 6.09386741e-11]
Round 50
-------------------------------
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.40113982  9.00691085  4.34261068  1.57832816 10.38540444  4.99669129
  1.94822841  6.14510341  4.53980435  4.05137628]
obj_prev = 51.395597693307295
eta_min = 1.1367158248091635e-21	eta_max = 0.9395757044516898
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 11.874554720679596	eta = 0.909090909090909
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 24.172264619780965	eta = 0.44658826617504477
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 17.836063247717608	eta = 0.6052372430028105
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 16.691468667850707	eta = 0.6467405571604739
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 16.62571758399257	eta = 0.6492982748886553
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 16.62547813342434	eta = 0.6493076264898318
eta = 0.6493076264898318
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [0.03563512 0.07494687 0.03506948 0.01216119 0.08654243 0.04129149
 0.01527219 0.05062449 0.03676639 0.03337257]
ene_total = [1.56827942 2.59415826 1.57537116 0.75074681 2.95309342 1.52632602
 0.84721729 1.92501916 1.61509008 1.2701765 ]
ti_comp = [0.73120545 0.79889761 0.72329946 0.75580078 0.8009931  0.80107742
 0.7563088  0.76793556 0.72766908 0.80314305]
ti_coms = [0.14113249 0.07344032 0.14903847 0.11653715 0.07134483 0.07126051
 0.11602913 0.10440237 0.14466885 0.06919488]
t_total = [27.49979019 27.49979019 27.49979019 27.49979019 27.49979019 27.49979019
 27.49979019 27.49979019 27.49979019 27.49979019]
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [5.28975756e-06 4.12247795e-05 5.15266296e-06 1.96785757e-07
 6.31407441e-05 6.85665898e-06 3.89211744e-07 1.37503103e-05
 5.86631230e-06 3.60133211e-06]
ene_total = [0.44507619 0.2328146  0.46999487 0.36738071 0.22689962 0.22485949
 0.36578528 0.32955399 0.45624248 0.21824512]
optimize_network iter = 0 obj = 3.3368523467960083
eta = 0.6493076264898318
freqs = [24367376.98591928 46906432.60757383 24242711.66907599  8045235.73109414
 54021956.04098655 25772468.52031963 10096532.76307652 32961419.31716986
 25263125.77228831 20776230.43734453]
eta_min = 0.6493076264898333	eta_max = 0.6979381775060964
af = 0.0037239897789525765	bf = 1.1686004695034842	zeta = 0.004096388756847835	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [1.16746392e-06 9.09842125e-06 1.13720677e-06 4.34311531e-08
 1.39353344e-05 1.51328333e-06 8.59000931e-08 3.03473097e-06
 1.29471112e-06 7.94823816e-07]
ene_total = [1.71643177 0.89420252 1.81257148 1.41719511 0.86930783 0.86677177
 1.41102229 1.26998969 1.7594524  0.84156459]
ti_comp = [0.61023825 0.67793041 0.60233226 0.63483358 0.6800259  0.68011022
 0.6353416  0.64696837 0.60670188 0.68217585]
ti_coms = [0.14113249 0.07344032 0.14903847 0.11653715 0.07134483 0.07126051
 0.11602913 0.10440237 0.14466885 0.06919488]
t_total = [27.49979019 27.49979019 27.49979019 27.49979019 27.49979019 27.49979019
 27.49979019 27.49979019 27.49979019 27.49979019]
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [4.38601091e-06 3.30616333e-05 4.29091311e-06 1.61080327e-07
 5.05906534e-05 5.49359804e-06 3.18510394e-07 1.11879346e-05
 4.87344749e-06 2.88277172e-06]
ene_total = [0.51669831 0.2699979  0.54563033 0.42652599 0.26297008 0.26101093
 0.42467242 0.38251687 0.52965907 0.25335527]
optimize_network iter = 1 obj = 3.8730371713241745
eta = 0.6979381775060964
freqs = [24314579.32952386 46031634.30360291 24242711.66907599  7976349.99245549
 52989718.17503818 25279541.36753655 10008802.12815238 32581091.06223162
 25232695.58126215 20369543.75424322]
eta_min = 0.6979381775061025	eta_max = 0.697938177506091
af = 0.0036043043465234475	bf = 1.1686004695034842	zeta = 0.003964734781175793	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [1.16241023e-06 8.76221731e-06 1.13720677e-06 4.26905959e-08
 1.34078766e-05 1.45595045e-06 8.44137753e-08 2.96510198e-06
 1.29159396e-06 7.64011626e-07]
ene_total = [1.71643115 0.89416163 1.81257148 1.41719502 0.86924368 0.8667648
 1.41102211 1.26998122 1.75945202 0.84156084]
ti_comp = [0.61023825 0.67793041 0.60233226 0.63483358 0.6800259  0.68011022
 0.6353416  0.64696837 0.60670188 0.68217585]
ti_coms = [0.14113249 0.07344032 0.14903847 0.11653715 0.07134483 0.07126051
 0.11602913 0.10440237 0.14466885 0.06919488]
t_total = [27.49979019 27.49979019 27.49979019 27.49979019 27.49979019 27.49979019
 27.49979019 27.49979019 27.49979019 27.49979019]
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [4.38601091e-06 3.30616333e-05 4.29091311e-06 1.61080327e-07
 5.05906534e-05 5.49359804e-06 3.18510394e-07 1.11879346e-05
 4.87344749e-06 2.88277172e-06]
ene_total = [0.51669831 0.2699979  0.54563033 0.42652599 0.26297008 0.26101093
 0.42467242 0.38251687 0.52965907 0.25335527]
optimize_network iter = 2 obj = 3.873037171324106
eta = 0.697938177506091
freqs = [24314579.32952385 46031634.30360299 24242711.66907597  7976349.99245549
 52989718.17503828 25279541.3675366  10008802.12815239 32581091.06223165
 25232695.58126213 20369543.75424326]
Done!
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [4.09700746e-06 3.08831330e-05 4.00817585e-06 1.50466407e-07
 4.72571292e-05 5.13161335e-06 2.97523077e-07 1.04507382e-05
 4.55232582e-06 2.69281985e-06]
ene_total = [0.01411735 0.00737491 0.01490785 0.01165387 0.00718174 0.00713118
 0.01160321 0.01045069 0.01447144 0.00692218]
At round 50 energy consumption: 0.10581441892494492
At round 50 eta: 0.697938177506091
At round 50 a_n: 10.712764660819193
At round 50 local rounds: 11.775941462244061
At round 50 global rounds: 36.59949614457299
gradient difference: 0.5075965523719788
train() client id: f_00000-0-0 loss: 1.429469  [   32/  126]
train() client id: f_00000-0-1 loss: 1.230811  [   64/  126]
train() client id: f_00000-0-2 loss: 1.250343  [   96/  126]
train() client id: f_00000-1-0 loss: 1.197213  [   32/  126]
train() client id: f_00000-1-1 loss: 1.289840  [   64/  126]
train() client id: f_00000-1-2 loss: 0.987212  [   96/  126]
train() client id: f_00000-2-0 loss: 1.006264  [   32/  126]
train() client id: f_00000-2-1 loss: 1.218198  [   64/  126]
train() client id: f_00000-2-2 loss: 1.098167  [   96/  126]
train() client id: f_00000-3-0 loss: 1.069143  [   32/  126]
train() client id: f_00000-3-1 loss: 0.928367  [   64/  126]
train() client id: f_00000-3-2 loss: 1.169457  [   96/  126]
train() client id: f_00000-4-0 loss: 0.958974  [   32/  126]
train() client id: f_00000-4-1 loss: 1.023145  [   64/  126]
train() client id: f_00000-4-2 loss: 0.948991  [   96/  126]
train() client id: f_00000-5-0 loss: 0.962181  [   32/  126]
train() client id: f_00000-5-1 loss: 0.926352  [   64/  126]
train() client id: f_00000-5-2 loss: 0.936142  [   96/  126]
train() client id: f_00000-6-0 loss: 0.895538  [   32/  126]
train() client id: f_00000-6-1 loss: 0.977882  [   64/  126]
train() client id: f_00000-6-2 loss: 0.799929  [   96/  126]
train() client id: f_00000-7-0 loss: 0.939991  [   32/  126]
train() client id: f_00000-7-1 loss: 0.930366  [   64/  126]
train() client id: f_00000-7-2 loss: 0.790783  [   96/  126]
train() client id: f_00000-8-0 loss: 0.852539  [   32/  126]
train() client id: f_00000-8-1 loss: 0.819271  [   64/  126]
train() client id: f_00000-8-2 loss: 0.965886  [   96/  126]
train() client id: f_00000-9-0 loss: 0.906330  [   32/  126]
train() client id: f_00000-9-1 loss: 0.845855  [   64/  126]
train() client id: f_00000-9-2 loss: 0.899007  [   96/  126]
train() client id: f_00000-10-0 loss: 0.836709  [   32/  126]
train() client id: f_00000-10-1 loss: 0.879909  [   64/  126]
train() client id: f_00000-10-2 loss: 0.889205  [   96/  126]
train() client id: f_00001-0-0 loss: 0.393281  [   32/  265]
train() client id: f_00001-0-1 loss: 0.461895  [   64/  265]
train() client id: f_00001-0-2 loss: 0.649349  [   96/  265]
train() client id: f_00001-0-3 loss: 0.391987  [  128/  265]
train() client id: f_00001-0-4 loss: 0.523392  [  160/  265]
train() client id: f_00001-0-5 loss: 0.479400  [  192/  265]
train() client id: f_00001-0-6 loss: 0.463645  [  224/  265]
train() client id: f_00001-0-7 loss: 0.482534  [  256/  265]
train() client id: f_00001-1-0 loss: 0.488177  [   32/  265]
train() client id: f_00001-1-1 loss: 0.430362  [   64/  265]
train() client id: f_00001-1-2 loss: 0.509309  [   96/  265]
train() client id: f_00001-1-3 loss: 0.424205  [  128/  265]
train() client id: f_00001-1-4 loss: 0.552613  [  160/  265]
train() client id: f_00001-1-5 loss: 0.507443  [  192/  265]
train() client id: f_00001-1-6 loss: 0.493178  [  224/  265]
train() client id: f_00001-1-7 loss: 0.396645  [  256/  265]
train() client id: f_00001-2-0 loss: 0.511537  [   32/  265]
train() client id: f_00001-2-1 loss: 0.380626  [   64/  265]
train() client id: f_00001-2-2 loss: 0.520317  [   96/  265]
train() client id: f_00001-2-3 loss: 0.437851  [  128/  265]
train() client id: f_00001-2-4 loss: 0.514386  [  160/  265]
train() client id: f_00001-2-5 loss: 0.387592  [  192/  265]
train() client id: f_00001-2-6 loss: 0.515823  [  224/  265]
train() client id: f_00001-2-7 loss: 0.489219  [  256/  265]
train() client id: f_00001-3-0 loss: 0.485708  [   32/  265]
train() client id: f_00001-3-1 loss: 0.542837  [   64/  265]
train() client id: f_00001-3-2 loss: 0.456182  [   96/  265]
train() client id: f_00001-3-3 loss: 0.399600  [  128/  265]
train() client id: f_00001-3-4 loss: 0.375125  [  160/  265]
train() client id: f_00001-3-5 loss: 0.598710  [  192/  265]
train() client id: f_00001-3-6 loss: 0.374931  [  224/  265]
train() client id: f_00001-3-7 loss: 0.510202  [  256/  265]
train() client id: f_00001-4-0 loss: 0.433288  [   32/  265]
train() client id: f_00001-4-1 loss: 0.363997  [   64/  265]
train() client id: f_00001-4-2 loss: 0.565187  [   96/  265]
train() client id: f_00001-4-3 loss: 0.488691  [  128/  265]
train() client id: f_00001-4-4 loss: 0.411240  [  160/  265]
train() client id: f_00001-4-5 loss: 0.387930  [  192/  265]
train() client id: f_00001-4-6 loss: 0.574283  [  224/  265]
train() client id: f_00001-4-7 loss: 0.493161  [  256/  265]
train() client id: f_00001-5-0 loss: 0.460941  [   32/  265]
train() client id: f_00001-5-1 loss: 0.491900  [   64/  265]
train() client id: f_00001-5-2 loss: 0.349232  [   96/  265]
train() client id: f_00001-5-3 loss: 0.368572  [  128/  265]
train() client id: f_00001-5-4 loss: 0.512696  [  160/  265]
train() client id: f_00001-5-5 loss: 0.479938  [  192/  265]
train() client id: f_00001-5-6 loss: 0.496600  [  224/  265]
train() client id: f_00001-5-7 loss: 0.554815  [  256/  265]
train() client id: f_00001-6-0 loss: 0.458748  [   32/  265]
train() client id: f_00001-6-1 loss: 0.471249  [   64/  265]
train() client id: f_00001-6-2 loss: 0.533653  [   96/  265]
train() client id: f_00001-6-3 loss: 0.359181  [  128/  265]
train() client id: f_00001-6-4 loss: 0.540878  [  160/  265]
train() client id: f_00001-6-5 loss: 0.406733  [  192/  265]
train() client id: f_00001-6-6 loss: 0.406841  [  224/  265]
train() client id: f_00001-6-7 loss: 0.530205  [  256/  265]
train() client id: f_00001-7-0 loss: 0.512804  [   32/  265]
train() client id: f_00001-7-1 loss: 0.456880  [   64/  265]
train() client id: f_00001-7-2 loss: 0.497521  [   96/  265]
train() client id: f_00001-7-3 loss: 0.503609  [  128/  265]
train() client id: f_00001-7-4 loss: 0.425254  [  160/  265]
train() client id: f_00001-7-5 loss: 0.416221  [  192/  265]
train() client id: f_00001-7-6 loss: 0.533802  [  224/  265]
train() client id: f_00001-7-7 loss: 0.359846  [  256/  265]
train() client id: f_00001-8-0 loss: 0.551078  [   32/  265]
train() client id: f_00001-8-1 loss: 0.372456  [   64/  265]
train() client id: f_00001-8-2 loss: 0.433824  [   96/  265]
train() client id: f_00001-8-3 loss: 0.502461  [  128/  265]
train() client id: f_00001-8-4 loss: 0.351020  [  160/  265]
train() client id: f_00001-8-5 loss: 0.368035  [  192/  265]
train() client id: f_00001-8-6 loss: 0.493136  [  224/  265]
train() client id: f_00001-8-7 loss: 0.544252  [  256/  265]
train() client id: f_00001-9-0 loss: 0.432381  [   32/  265]
train() client id: f_00001-9-1 loss: 0.464215  [   64/  265]
train() client id: f_00001-9-2 loss: 0.400025  [   96/  265]
train() client id: f_00001-9-3 loss: 0.498030  [  128/  265]
train() client id: f_00001-9-4 loss: 0.381615  [  160/  265]
train() client id: f_00001-9-5 loss: 0.506178  [  192/  265]
train() client id: f_00001-9-6 loss: 0.572291  [  224/  265]
train() client id: f_00001-9-7 loss: 0.454728  [  256/  265]
train() client id: f_00001-10-0 loss: 0.374423  [   32/  265]
train() client id: f_00001-10-1 loss: 0.460433  [   64/  265]
train() client id: f_00001-10-2 loss: 0.505315  [   96/  265]
train() client id: f_00001-10-3 loss: 0.607895  [  128/  265]
train() client id: f_00001-10-4 loss: 0.419761  [  160/  265]
train() client id: f_00001-10-5 loss: 0.423507  [  192/  265]
train() client id: f_00001-10-6 loss: 0.478102  [  224/  265]
train() client id: f_00001-10-7 loss: 0.363779  [  256/  265]
train() client id: f_00002-0-0 loss: 1.157040  [   32/  124]
train() client id: f_00002-0-1 loss: 1.349706  [   64/  124]
train() client id: f_00002-0-2 loss: 1.200846  [   96/  124]
train() client id: f_00002-1-0 loss: 0.971340  [   32/  124]
train() client id: f_00002-1-1 loss: 1.368101  [   64/  124]
train() client id: f_00002-1-2 loss: 1.155553  [   96/  124]
train() client id: f_00002-2-0 loss: 1.196487  [   32/  124]
train() client id: f_00002-2-1 loss: 1.040362  [   64/  124]
train() client id: f_00002-2-2 loss: 1.121946  [   96/  124]
train() client id: f_00002-3-0 loss: 1.171580  [   32/  124]
train() client id: f_00002-3-1 loss: 1.231755  [   64/  124]
train() client id: f_00002-3-2 loss: 1.117659  [   96/  124]
train() client id: f_00002-4-0 loss: 0.968050  [   32/  124]
train() client id: f_00002-4-1 loss: 1.161205  [   64/  124]
train() client id: f_00002-4-2 loss: 1.229738  [   96/  124]
train() client id: f_00002-5-0 loss: 0.922427  [   32/  124]
train() client id: f_00002-5-1 loss: 1.312060  [   64/  124]
train() client id: f_00002-5-2 loss: 1.045311  [   96/  124]
train() client id: f_00002-6-0 loss: 1.109487  [   32/  124]
train() client id: f_00002-6-1 loss: 1.098031  [   64/  124]
train() client id: f_00002-6-2 loss: 1.075981  [   96/  124]
train() client id: f_00002-7-0 loss: 1.100302  [   32/  124]
train() client id: f_00002-7-1 loss: 1.031488  [   64/  124]
train() client id: f_00002-7-2 loss: 1.131097  [   96/  124]
train() client id: f_00002-8-0 loss: 1.024748  [   32/  124]
train() client id: f_00002-8-1 loss: 1.097669  [   64/  124]
train() client id: f_00002-8-2 loss: 1.166123  [   96/  124]
train() client id: f_00002-9-0 loss: 1.005369  [   32/  124]
train() client id: f_00002-9-1 loss: 1.203444  [   64/  124]
train() client id: f_00002-9-2 loss: 1.174923  [   96/  124]
train() client id: f_00002-10-0 loss: 1.032636  [   32/  124]
train() client id: f_00002-10-1 loss: 0.906981  [   64/  124]
train() client id: f_00002-10-2 loss: 1.343941  [   96/  124]
train() client id: f_00003-0-0 loss: 0.578034  [   32/   43]
train() client id: f_00003-1-0 loss: 0.687646  [   32/   43]
train() client id: f_00003-2-0 loss: 0.558296  [   32/   43]
train() client id: f_00003-3-0 loss: 0.742251  [   32/   43]
train() client id: f_00003-4-0 loss: 0.717075  [   32/   43]
train() client id: f_00003-5-0 loss: 0.898110  [   32/   43]
train() client id: f_00003-6-0 loss: 0.782260  [   32/   43]
train() client id: f_00003-7-0 loss: 0.701395  [   32/   43]
train() client id: f_00003-8-0 loss: 0.837584  [   32/   43]
train() client id: f_00003-9-0 loss: 0.548440  [   32/   43]
train() client id: f_00003-10-0 loss: 0.779012  [   32/   43]
train() client id: f_00004-0-0 loss: 0.906359  [   32/  306]
train() client id: f_00004-0-1 loss: 1.101164  [   64/  306]
train() client id: f_00004-0-2 loss: 0.985428  [   96/  306]
train() client id: f_00004-0-3 loss: 0.820174  [  128/  306]
train() client id: f_00004-0-4 loss: 0.918296  [  160/  306]
train() client id: f_00004-0-5 loss: 0.927475  [  192/  306]
train() client id: f_00004-0-6 loss: 0.780488  [  224/  306]
train() client id: f_00004-0-7 loss: 0.916740  [  256/  306]
train() client id: f_00004-0-8 loss: 0.879853  [  288/  306]
train() client id: f_00004-1-0 loss: 0.996415  [   32/  306]
train() client id: f_00004-1-1 loss: 1.031972  [   64/  306]
train() client id: f_00004-1-2 loss: 0.861794  [   96/  306]
train() client id: f_00004-1-3 loss: 1.022734  [  128/  306]
train() client id: f_00004-1-4 loss: 0.925543  [  160/  306]
train() client id: f_00004-1-5 loss: 0.806122  [  192/  306]
train() client id: f_00004-1-6 loss: 0.894496  [  224/  306]
train() client id: f_00004-1-7 loss: 0.798782  [  256/  306]
train() client id: f_00004-1-8 loss: 0.823245  [  288/  306]
train() client id: f_00004-2-0 loss: 0.960374  [   32/  306]
train() client id: f_00004-2-1 loss: 0.782534  [   64/  306]
train() client id: f_00004-2-2 loss: 0.924395  [   96/  306]
train() client id: f_00004-2-3 loss: 0.804322  [  128/  306]
train() client id: f_00004-2-4 loss: 0.895971  [  160/  306]
train() client id: f_00004-2-5 loss: 0.897665  [  192/  306]
train() client id: f_00004-2-6 loss: 0.944424  [  224/  306]
train() client id: f_00004-2-7 loss: 1.051685  [  256/  306]
train() client id: f_00004-2-8 loss: 1.013538  [  288/  306]
train() client id: f_00004-3-0 loss: 0.766439  [   32/  306]
train() client id: f_00004-3-1 loss: 0.932140  [   64/  306]
train() client id: f_00004-3-2 loss: 0.823225  [   96/  306]
train() client id: f_00004-3-3 loss: 1.028934  [  128/  306]
train() client id: f_00004-3-4 loss: 0.887366  [  160/  306]
train() client id: f_00004-3-5 loss: 1.028721  [  192/  306]
train() client id: f_00004-3-6 loss: 0.914658  [  224/  306]
train() client id: f_00004-3-7 loss: 0.799837  [  256/  306]
train() client id: f_00004-3-8 loss: 0.962478  [  288/  306]
train() client id: f_00004-4-0 loss: 0.871655  [   32/  306]
train() client id: f_00004-4-1 loss: 0.902065  [   64/  306]
train() client id: f_00004-4-2 loss: 1.021890  [   96/  306]
train() client id: f_00004-4-3 loss: 0.894044  [  128/  306]
train() client id: f_00004-4-4 loss: 0.985178  [  160/  306]
train() client id: f_00004-4-5 loss: 0.885438  [  192/  306]
train() client id: f_00004-4-6 loss: 0.933786  [  224/  306]
train() client id: f_00004-4-7 loss: 0.847064  [  256/  306]
train() client id: f_00004-4-8 loss: 0.851613  [  288/  306]
train() client id: f_00004-5-0 loss: 0.870183  [   32/  306]
train() client id: f_00004-5-1 loss: 0.884451  [   64/  306]
train() client id: f_00004-5-2 loss: 0.953777  [   96/  306]
train() client id: f_00004-5-3 loss: 1.007599  [  128/  306]
train() client id: f_00004-5-4 loss: 0.726348  [  160/  306]
train() client id: f_00004-5-5 loss: 0.890825  [  192/  306]
train() client id: f_00004-5-6 loss: 0.931443  [  224/  306]
train() client id: f_00004-5-7 loss: 1.018045  [  256/  306]
train() client id: f_00004-5-8 loss: 0.862326  [  288/  306]
train() client id: f_00004-6-0 loss: 0.883457  [   32/  306]
train() client id: f_00004-6-1 loss: 0.968123  [   64/  306]
train() client id: f_00004-6-2 loss: 0.785057  [   96/  306]
train() client id: f_00004-6-3 loss: 1.082184  [  128/  306]
train() client id: f_00004-6-4 loss: 0.902034  [  160/  306]
train() client id: f_00004-6-5 loss: 0.955299  [  192/  306]
train() client id: f_00004-6-6 loss: 0.996299  [  224/  306]
train() client id: f_00004-6-7 loss: 0.753515  [  256/  306]
train() client id: f_00004-6-8 loss: 0.801840  [  288/  306]
train() client id: f_00004-7-0 loss: 0.878914  [   32/  306]
train() client id: f_00004-7-1 loss: 1.002542  [   64/  306]
train() client id: f_00004-7-2 loss: 0.955117  [   96/  306]
train() client id: f_00004-7-3 loss: 0.847072  [  128/  306]
train() client id: f_00004-7-4 loss: 0.814756  [  160/  306]
train() client id: f_00004-7-5 loss: 0.822967  [  192/  306]
train() client id: f_00004-7-6 loss: 0.804354  [  224/  306]
train() client id: f_00004-7-7 loss: 0.941118  [  256/  306]
train() client id: f_00004-7-8 loss: 0.935524  [  288/  306]
train() client id: f_00004-8-0 loss: 0.872443  [   32/  306]
train() client id: f_00004-8-1 loss: 1.049471  [   64/  306]
train() client id: f_00004-8-2 loss: 0.890309  [   96/  306]
train() client id: f_00004-8-3 loss: 0.950437  [  128/  306]
train() client id: f_00004-8-4 loss: 0.812051  [  160/  306]
train() client id: f_00004-8-5 loss: 0.947768  [  192/  306]
train() client id: f_00004-8-6 loss: 0.891585  [  224/  306]
train() client id: f_00004-8-7 loss: 0.760007  [  256/  306]
train() client id: f_00004-8-8 loss: 0.894457  [  288/  306]
train() client id: f_00004-9-0 loss: 0.822740  [   32/  306]
train() client id: f_00004-9-1 loss: 0.956999  [   64/  306]
train() client id: f_00004-9-2 loss: 0.914431  [   96/  306]
train() client id: f_00004-9-3 loss: 0.835436  [  128/  306]
train() client id: f_00004-9-4 loss: 0.891541  [  160/  306]
train() client id: f_00004-9-5 loss: 0.869936  [  192/  306]
train() client id: f_00004-9-6 loss: 0.947375  [  224/  306]
train() client id: f_00004-9-7 loss: 0.853856  [  256/  306]
train() client id: f_00004-9-8 loss: 0.955255  [  288/  306]
train() client id: f_00004-10-0 loss: 0.919669  [   32/  306]
train() client id: f_00004-10-1 loss: 0.916931  [   64/  306]
train() client id: f_00004-10-2 loss: 0.990812  [   96/  306]
train() client id: f_00004-10-3 loss: 0.849464  [  128/  306]
train() client id: f_00004-10-4 loss: 0.853213  [  160/  306]
train() client id: f_00004-10-5 loss: 0.790036  [  192/  306]
train() client id: f_00004-10-6 loss: 0.915392  [  224/  306]
train() client id: f_00004-10-7 loss: 0.909468  [  256/  306]
train() client id: f_00004-10-8 loss: 0.878347  [  288/  306]
train() client id: f_00005-0-0 loss: 0.772131  [   32/  146]
train() client id: f_00005-0-1 loss: 0.593545  [   64/  146]
train() client id: f_00005-0-2 loss: 0.597506  [   96/  146]
train() client id: f_00005-0-3 loss: 0.640268  [  128/  146]
train() client id: f_00005-1-0 loss: 0.798806  [   32/  146]
train() client id: f_00005-1-1 loss: 0.641144  [   64/  146]
train() client id: f_00005-1-2 loss: 0.685147  [   96/  146]
train() client id: f_00005-1-3 loss: 0.441972  [  128/  146]
train() client id: f_00005-2-0 loss: 0.839835  [   32/  146]
train() client id: f_00005-2-1 loss: 0.480084  [   64/  146]
train() client id: f_00005-2-2 loss: 0.473625  [   96/  146]
train() client id: f_00005-2-3 loss: 0.747674  [  128/  146]
train() client id: f_00005-3-0 loss: 0.646884  [   32/  146]
train() client id: f_00005-3-1 loss: 0.559132  [   64/  146]
train() client id: f_00005-3-2 loss: 0.671103  [   96/  146]
train() client id: f_00005-3-3 loss: 0.660692  [  128/  146]
train() client id: f_00005-4-0 loss: 0.529180  [   32/  146]
train() client id: f_00005-4-1 loss: 0.927002  [   64/  146]
train() client id: f_00005-4-2 loss: 0.495281  [   96/  146]
train() client id: f_00005-4-3 loss: 0.630960  [  128/  146]
train() client id: f_00005-5-0 loss: 0.371888  [   32/  146]
train() client id: f_00005-5-1 loss: 0.762917  [   64/  146]
train() client id: f_00005-5-2 loss: 0.916067  [   96/  146]
train() client id: f_00005-5-3 loss: 0.645671  [  128/  146]
train() client id: f_00005-6-0 loss: 0.569246  [   32/  146]
train() client id: f_00005-6-1 loss: 0.691747  [   64/  146]
train() client id: f_00005-6-2 loss: 0.658624  [   96/  146]
train() client id: f_00005-6-3 loss: 0.575259  [  128/  146]
train() client id: f_00005-7-0 loss: 0.595379  [   32/  146]
train() client id: f_00005-7-1 loss: 0.601284  [   64/  146]
train() client id: f_00005-7-2 loss: 0.868088  [   96/  146]
train() client id: f_00005-7-3 loss: 0.481099  [  128/  146]
train() client id: f_00005-8-0 loss: 0.957921  [   32/  146]
train() client id: f_00005-8-1 loss: 0.573148  [   64/  146]
train() client id: f_00005-8-2 loss: 0.405987  [   96/  146]
train() client id: f_00005-8-3 loss: 0.729321  [  128/  146]
train() client id: f_00005-9-0 loss: 0.616003  [   32/  146]
train() client id: f_00005-9-1 loss: 0.631918  [   64/  146]
train() client id: f_00005-9-2 loss: 0.566509  [   96/  146]
train() client id: f_00005-9-3 loss: 0.910197  [  128/  146]
train() client id: f_00005-10-0 loss: 0.580713  [   32/  146]
train() client id: f_00005-10-1 loss: 0.708722  [   64/  146]
train() client id: f_00005-10-2 loss: 0.499213  [   96/  146]
train() client id: f_00005-10-3 loss: 0.633302  [  128/  146]
train() client id: f_00006-0-0 loss: 0.458765  [   32/   54]
train() client id: f_00006-1-0 loss: 0.495393  [   32/   54]
train() client id: f_00006-2-0 loss: 0.506191  [   32/   54]
train() client id: f_00006-3-0 loss: 0.506942  [   32/   54]
train() client id: f_00006-4-0 loss: 0.457467  [   32/   54]
train() client id: f_00006-5-0 loss: 0.511119  [   32/   54]
train() client id: f_00006-6-0 loss: 0.521638  [   32/   54]
train() client id: f_00006-7-0 loss: 0.549145  [   32/   54]
train() client id: f_00006-8-0 loss: 0.500876  [   32/   54]
train() client id: f_00006-9-0 loss: 0.439985  [   32/   54]
train() client id: f_00006-10-0 loss: 0.531718  [   32/   54]
train() client id: f_00007-0-0 loss: 0.867811  [   32/  179]
train() client id: f_00007-0-1 loss: 0.547201  [   64/  179]
train() client id: f_00007-0-2 loss: 0.580599  [   96/  179]
train() client id: f_00007-0-3 loss: 0.642030  [  128/  179]
train() client id: f_00007-0-4 loss: 0.771690  [  160/  179]
train() client id: f_00007-1-0 loss: 0.685567  [   32/  179]
train() client id: f_00007-1-1 loss: 0.823466  [   64/  179]
train() client id: f_00007-1-2 loss: 0.505176  [   96/  179]
train() client id: f_00007-1-3 loss: 0.581466  [  128/  179]
train() client id: f_00007-1-4 loss: 0.796363  [  160/  179]
train() client id: f_00007-2-0 loss: 0.749372  [   32/  179]
train() client id: f_00007-2-1 loss: 0.618991  [   64/  179]
train() client id: f_00007-2-2 loss: 0.587965  [   96/  179]
train() client id: f_00007-2-3 loss: 0.576413  [  128/  179]
train() client id: f_00007-2-4 loss: 0.695065  [  160/  179]
train() client id: f_00007-3-0 loss: 0.614492  [   32/  179]
train() client id: f_00007-3-1 loss: 0.669450  [   64/  179]
train() client id: f_00007-3-2 loss: 0.746374  [   96/  179]
train() client id: f_00007-3-3 loss: 0.612908  [  128/  179]
train() client id: f_00007-3-4 loss: 0.668204  [  160/  179]
train() client id: f_00007-4-0 loss: 0.674453  [   32/  179]
train() client id: f_00007-4-1 loss: 0.614964  [   64/  179]
train() client id: f_00007-4-2 loss: 0.522596  [   96/  179]
train() client id: f_00007-4-3 loss: 0.590439  [  128/  179]
train() client id: f_00007-4-4 loss: 0.677774  [  160/  179]
train() client id: f_00007-5-0 loss: 0.727706  [   32/  179]
train() client id: f_00007-5-1 loss: 0.832239  [   64/  179]
train() client id: f_00007-5-2 loss: 0.595955  [   96/  179]
train() client id: f_00007-5-3 loss: 0.557469  [  128/  179]
train() client id: f_00007-5-4 loss: 0.442094  [  160/  179]
train() client id: f_00007-6-0 loss: 0.722925  [   32/  179]
train() client id: f_00007-6-1 loss: 0.570490  [   64/  179]
train() client id: f_00007-6-2 loss: 0.642340  [   96/  179]
train() client id: f_00007-6-3 loss: 0.615371  [  128/  179]
train() client id: f_00007-6-4 loss: 0.623911  [  160/  179]
train() client id: f_00007-7-0 loss: 0.611311  [   32/  179]
train() client id: f_00007-7-1 loss: 0.620473  [   64/  179]
train() client id: f_00007-7-2 loss: 0.768058  [   96/  179]
train() client id: f_00007-7-3 loss: 0.476183  [  128/  179]
train() client id: f_00007-7-4 loss: 0.690839  [  160/  179]
train() client id: f_00007-8-0 loss: 0.685978  [   32/  179]
train() client id: f_00007-8-1 loss: 0.566351  [   64/  179]
train() client id: f_00007-8-2 loss: 0.481727  [   96/  179]
train() client id: f_00007-8-3 loss: 0.874012  [  128/  179]
train() client id: f_00007-8-4 loss: 0.487578  [  160/  179]
train() client id: f_00007-9-0 loss: 0.469061  [   32/  179]
train() client id: f_00007-9-1 loss: 0.702371  [   64/  179]
train() client id: f_00007-9-2 loss: 0.579305  [   96/  179]
train() client id: f_00007-9-3 loss: 0.573517  [  128/  179]
train() client id: f_00007-9-4 loss: 0.508964  [  160/  179]
train() client id: f_00007-10-0 loss: 0.531219  [   32/  179]
train() client id: f_00007-10-1 loss: 0.693101  [   64/  179]
train() client id: f_00007-10-2 loss: 0.770979  [   96/  179]
train() client id: f_00007-10-3 loss: 0.445863  [  128/  179]
train() client id: f_00007-10-4 loss: 0.549969  [  160/  179]
train() client id: f_00008-0-0 loss: 0.828948  [   32/  130]
train() client id: f_00008-0-1 loss: 0.760061  [   64/  130]
train() client id: f_00008-0-2 loss: 0.702422  [   96/  130]
train() client id: f_00008-0-3 loss: 0.807864  [  128/  130]
train() client id: f_00008-1-0 loss: 0.701927  [   32/  130]
train() client id: f_00008-1-1 loss: 0.847879  [   64/  130]
train() client id: f_00008-1-2 loss: 0.684053  [   96/  130]
train() client id: f_00008-1-3 loss: 0.866772  [  128/  130]
train() client id: f_00008-2-0 loss: 0.763274  [   32/  130]
train() client id: f_00008-2-1 loss: 0.797944  [   64/  130]
train() client id: f_00008-2-2 loss: 0.770826  [   96/  130]
train() client id: f_00008-2-3 loss: 0.766757  [  128/  130]
train() client id: f_00008-3-0 loss: 0.742127  [   32/  130]
train() client id: f_00008-3-1 loss: 0.688295  [   64/  130]
train() client id: f_00008-3-2 loss: 0.838784  [   96/  130]
train() client id: f_00008-3-3 loss: 0.784782  [  128/  130]
train() client id: f_00008-4-0 loss: 0.851810  [   32/  130]
train() client id: f_00008-4-1 loss: 0.818965  [   64/  130]
train() client id: f_00008-4-2 loss: 0.680248  [   96/  130]
train() client id: f_00008-4-3 loss: 0.740681  [  128/  130]
train() client id: f_00008-5-0 loss: 0.774861  [   32/  130]
train() client id: f_00008-5-1 loss: 0.791423  [   64/  130]
train() client id: f_00008-5-2 loss: 0.804305  [   96/  130]
train() client id: f_00008-5-3 loss: 0.717276  [  128/  130]
train() client id: f_00008-6-0 loss: 0.707280  [   32/  130]
train() client id: f_00008-6-1 loss: 0.687098  [   64/  130]
train() client id: f_00008-6-2 loss: 0.907053  [   96/  130]
train() client id: f_00008-6-3 loss: 0.737648  [  128/  130]
train() client id: f_00008-7-0 loss: 0.859971  [   32/  130]
train() client id: f_00008-7-1 loss: 0.719852  [   64/  130]
train() client id: f_00008-7-2 loss: 0.849842  [   96/  130]
train() client id: f_00008-7-3 loss: 0.652872  [  128/  130]
train() client id: f_00008-8-0 loss: 0.781940  [   32/  130]
train() client id: f_00008-8-1 loss: 0.726722  [   64/  130]
train() client id: f_00008-8-2 loss: 0.756129  [   96/  130]
train() client id: f_00008-8-3 loss: 0.810392  [  128/  130]
train() client id: f_00008-9-0 loss: 0.730135  [   32/  130]
train() client id: f_00008-9-1 loss: 0.790612  [   64/  130]
train() client id: f_00008-9-2 loss: 0.870861  [   96/  130]
train() client id: f_00008-9-3 loss: 0.677090  [  128/  130]
train() client id: f_00008-10-0 loss: 0.792590  [   32/  130]
train() client id: f_00008-10-1 loss: 0.718338  [   64/  130]
train() client id: f_00008-10-2 loss: 0.739736  [   96/  130]
train() client id: f_00008-10-3 loss: 0.816691  [  128/  130]
train() client id: f_00009-0-0 loss: 1.031305  [   32/  118]
train() client id: f_00009-0-1 loss: 1.069404  [   64/  118]
train() client id: f_00009-0-2 loss: 0.923443  [   96/  118]
train() client id: f_00009-1-0 loss: 0.958492  [   32/  118]
train() client id: f_00009-1-1 loss: 0.950299  [   64/  118]
train() client id: f_00009-1-2 loss: 0.883488  [   96/  118]
train() client id: f_00009-2-0 loss: 0.940036  [   32/  118]
train() client id: f_00009-2-1 loss: 0.987236  [   64/  118]
train() client id: f_00009-2-2 loss: 0.865549  [   96/  118]
train() client id: f_00009-3-0 loss: 0.845251  [   32/  118]
train() client id: f_00009-3-1 loss: 1.054290  [   64/  118]
train() client id: f_00009-3-2 loss: 0.846083  [   96/  118]
train() client id: f_00009-4-0 loss: 0.788963  [   32/  118]
train() client id: f_00009-4-1 loss: 0.841562  [   64/  118]
train() client id: f_00009-4-2 loss: 0.883140  [   96/  118]
train() client id: f_00009-5-0 loss: 0.924963  [   32/  118]
train() client id: f_00009-5-1 loss: 0.928549  [   64/  118]
train() client id: f_00009-5-2 loss: 0.730527  [   96/  118]
train() client id: f_00009-6-0 loss: 0.795809  [   32/  118]
train() client id: f_00009-6-1 loss: 0.916556  [   64/  118]
train() client id: f_00009-6-2 loss: 0.777225  [   96/  118]
train() client id: f_00009-7-0 loss: 0.740444  [   32/  118]
train() client id: f_00009-7-1 loss: 0.754777  [   64/  118]
train() client id: f_00009-7-2 loss: 0.991162  [   96/  118]
train() client id: f_00009-8-0 loss: 0.923335  [   32/  118]
train() client id: f_00009-8-1 loss: 0.738793  [   64/  118]
train() client id: f_00009-8-2 loss: 0.889586  [   96/  118]
train() client id: f_00009-9-0 loss: 0.932876  [   32/  118]
train() client id: f_00009-9-1 loss: 0.658922  [   64/  118]
train() client id: f_00009-9-2 loss: 0.788036  [   96/  118]
train() client id: f_00009-10-0 loss: 0.874829  [   32/  118]
train() client id: f_00009-10-1 loss: 0.842642  [   64/  118]
train() client id: f_00009-10-2 loss: 0.860240  [   96/  118]
At round 50 accuracy: 0.649867374005305
At round 50 training accuracy: 0.596244131455399
At round 50 training loss: 0.818324789225309
update_location
xs = [  -3.9056584     4.20031788  270.00902392   18.81129433    0.97929623
    3.95640986 -232.44319194 -211.32485185  254.66397685 -197.06087855]
ys = [ 262.5879595   245.55583871    1.32061395 -232.45517586  224.35018685
  207.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [281.0119048  265.17034637 287.93509167 253.75041592 245.62973224
 230.65639906 253.05479253 233.79236357 274.1576391  221.01810268]
dists_bs = [192.21062503 192.58992783 477.70028313 451.20121691 182.76803233
 181.94000418 186.68249879 177.9251228  457.59769543 172.41570577]
uav_gains = [3.91606458e-12 5.45480845e-12 3.39525697e-12 6.88931259e-12
 8.07391748e-12 1.05867714e-11 6.98555204e-12 1.00271385e-11
 4.51953049e-12 1.24147554e-11]
bs_gains = [4.45366340e-11 4.42914694e-11 3.48063665e-12 4.08373311e-12
 5.12830001e-11 5.19391842e-11 4.83285750e-11 5.52878520e-11
 3.92590119e-12 6.03780354e-11]
Round 51
-------------------------------
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.27047392  8.72826714  4.21425716  1.53313884 10.06392708  4.84207237
  1.89154359  5.95729153  4.4008638   3.92598233]
obj_prev = 49.827817776908454
eta_min = 2.5652490042097954e-22	eta_max = 0.9391007800776031
af = 10.460568009377656	bf = 1.156843673852587	zeta = 11.506624810315422	eta = 0.9090909090909091
af = 10.460568009377656	bf = 1.156843673852587	zeta = 23.692265208499137	eta = 0.44151827262279386
af = 10.460568009377656	bf = 1.156843673852587	zeta = 17.38415562977596	eta = 0.6017300024316722
af = 10.460568009377656	bf = 1.156843673852587	zeta = 16.245893349082294	eta = 0.6438899840474798
af = 10.460568009377656	bf = 1.156843673852587	zeta = 16.180034455220028	eta = 0.6465108611683364
af = 10.460568009377656	bf = 1.156843673852587	zeta = 16.179790636766835	eta = 0.6465206036478086
eta = 0.6465206036478086
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [0.03599007 0.0756934  0.0354188  0.01228233 0.08740446 0.04170278
 0.01542432 0.05112875 0.03713261 0.03370499]
ene_total = [1.53443253 2.51779645 1.54247681 0.73582445 2.86602817 1.48040303
 0.82941199 1.87320864 1.56863488 1.23157367]
ti_comp = [0.75933363 0.83189525 0.75096662 0.78576566 0.83410293 0.83428835
 0.78630533 0.79893754 0.75947361 0.83641438]
ti_coms = [0.14640211 0.07384048 0.15476912 0.11997008 0.07163281 0.07144739
 0.11943041 0.10679819 0.14626213 0.06932135]
t_total = [27.449786 27.449786 27.449786 27.449786 27.449786 27.449786 27.449786
 27.449786 27.449786 27.449786]
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [5.05315743e-06 3.91666828e-05 4.92425175e-06 1.87558063e-07
 5.99848321e-05 6.51242403e-06 3.70949938e-07 1.30872950e-05
 5.54780444e-06 3.42073551e-06]
ene_total = [0.44384841 0.22497254 0.46920207 0.3635944  0.21891275 0.21673023
 0.36196441 0.32406582 0.44343917 0.21019324]
optimize_network iter = 0 obj = 3.2769230299910435
eta = 0.6465206036478086
freqs = [23698456.65782105 45494550.69374827 23582139.4675787   7815514.32290755
 52394286.76461308 24993025.32151997  9808095.66767819 31997964.51374336
 24446282.03194968 20148497.51725332]
eta_min = 0.6465206036478099	eta_max = 0.6980699554597802
af = 0.0033958499648260683	bf = 1.156843673852587	zeta = 0.003735434961308675	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [1.10424650e-06 8.55894019e-06 1.07607725e-06 4.09863212e-08
 1.31082480e-05 1.42313425e-06 8.10622219e-08 2.85991478e-06
 1.21233975e-06 7.47519793e-07]
ene_total = [1.7253386  0.8711479  1.82393243 1.41373717 0.84566865 0.84210666
 1.4073824  1.2588513  1.7237018  0.81697375]
ti_comp = [0.62724647 0.69980809 0.61887946 0.6536785  0.70201577 0.70220119
 0.65421817 0.66685038 0.62738645 0.70432722]
ti_coms = [0.14640211 0.07384048 0.15476912 0.11997008 0.07163281 0.07144739
 0.11943041 0.10679819 0.14626213 0.06932135]
t_total = [27.449786 27.449786 27.449786 27.449786 27.449786 27.449786 27.449786
 27.449786 27.449786 27.449786]
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [4.14484175e-06 3.09779353e-05 4.05812745e-06 1.51687479e-07
 4.73961769e-05 5.14527051e-06 2.99922360e-07 1.05141713e-05
 4.55022169e-06 2.70003775e-06]
ene_total = [0.51959565 0.26309218 0.54927951 0.42567061 0.25584167 0.25368468
 0.42376107 0.37930318 0.51911337 0.24605454]
optimize_network iter = 1 obj = 3.8353964400634717
eta = 0.6980699554597802
freqs = [23642854.46224663 44569165.40272366 23582139.46757871  7742332.9461439
 51302927.41865983 24471403.78525164  9714909.2256883  31593097.86188618
 24387978.69085684 19718556.51146085]
eta_min = 0.698069955459783	eta_max = 0.6980699554597762
af = 0.0032766452472554383	bf = 1.156843673852587	zeta = 0.003604309771980982	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [1.09907093e-06 8.21429384e-06 1.07607725e-06 4.02223555e-08
 1.25678526e-05 1.36435058e-06 7.95291995e-08 2.78800028e-06
 1.20656389e-06 7.15958094e-07]
ene_total = [1.72533799 0.87110728 1.82393243 1.41373708 0.84560497 0.84209973
 1.40738222 1.25884283 1.72370112 0.81697003]
ti_comp = [0.62724647 0.69980809 0.61887946 0.6536785  0.70201577 0.70220119
 0.65421817 0.66685038 0.62738645 0.70432722]
ti_coms = [0.14640211 0.07384048 0.15476912 0.11997008 0.07163281 0.07144739
 0.11943041 0.10679819 0.14626213 0.06932135]
t_total = [27.449786 27.449786 27.449786 27.449786 27.449786 27.449786 27.449786
 27.449786 27.449786 27.449786]
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [4.14484175e-06 3.09779353e-05 4.05812745e-06 1.51687479e-07
 4.73961769e-05 5.14527051e-06 2.99922360e-07 1.05141713e-05
 4.55022169e-06 2.70003775e-06]
ene_total = [0.51959565 0.26309218 0.54927951 0.42567061 0.25584167 0.25368468
 0.42376107 0.37930318 0.51911337 0.24605454]
optimize_network iter = 2 obj = 3.8353964400634206
eta = 0.6980699554597762
freqs = [23642854.46224662 44569165.40272372 23582139.46757869  7742332.9461439
 51302927.41865991 24471403.78525168  9714909.2256883  31593097.8618862
 24387978.69085682 19718556.51146088]
Done!
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [3.87376305e-06 2.89519331e-05 3.79272000e-06 1.41766896e-07
 4.42963978e-05 4.80876231e-06 2.80307000e-07 9.82652913e-06
 4.25263056e-06 2.52345135e-06]
ene_total = [0.01464408 0.007413   0.0154807  0.01199715 0.00720758 0.00714955
 0.01194332 0.01068965 0.01463047 0.00693466]
At round 51 energy consumption: 0.10809015491855846
At round 51 eta: 0.6980699554597762
At round 51 a_n: 10.370218813849878
At round 51 local rounds: 11.76975943561131
At round 51 global rounds: 35.48094949322612
gradient difference: 0.49656981229782104
train() client id: f_00000-0-0 loss: 1.269447  [   32/  126]
train() client id: f_00000-0-1 loss: 1.343284  [   64/  126]
train() client id: f_00000-0-2 loss: 1.058555  [   96/  126]
train() client id: f_00000-1-0 loss: 1.019251  [   32/  126]
train() client id: f_00000-1-1 loss: 1.227819  [   64/  126]
train() client id: f_00000-1-2 loss: 0.941747  [   96/  126]
train() client id: f_00000-2-0 loss: 1.061433  [   32/  126]
train() client id: f_00000-2-1 loss: 1.039733  [   64/  126]
train() client id: f_00000-2-2 loss: 0.926835  [   96/  126]
train() client id: f_00000-3-0 loss: 0.912820  [   32/  126]
train() client id: f_00000-3-1 loss: 1.079404  [   64/  126]
train() client id: f_00000-3-2 loss: 1.005690  [   96/  126]
train() client id: f_00000-4-0 loss: 0.803599  [   32/  126]
train() client id: f_00000-4-1 loss: 0.933096  [   64/  126]
train() client id: f_00000-4-2 loss: 1.036208  [   96/  126]
train() client id: f_00000-5-0 loss: 0.924546  [   32/  126]
train() client id: f_00000-5-1 loss: 0.921745  [   64/  126]
train() client id: f_00000-5-2 loss: 0.887583  [   96/  126]
train() client id: f_00000-6-0 loss: 0.888376  [   32/  126]
train() client id: f_00000-6-1 loss: 0.849315  [   64/  126]
train() client id: f_00000-6-2 loss: 0.974654  [   96/  126]
train() client id: f_00000-7-0 loss: 0.921969  [   32/  126]
train() client id: f_00000-7-1 loss: 0.822057  [   64/  126]
train() client id: f_00000-7-2 loss: 0.823781  [   96/  126]
train() client id: f_00000-8-0 loss: 0.863171  [   32/  126]
train() client id: f_00000-8-1 loss: 0.777638  [   64/  126]
train() client id: f_00000-8-2 loss: 0.805348  [   96/  126]
train() client id: f_00000-9-0 loss: 0.693264  [   32/  126]
train() client id: f_00000-9-1 loss: 0.829709  [   64/  126]
train() client id: f_00000-9-2 loss: 0.923991  [   96/  126]
train() client id: f_00000-10-0 loss: 0.729407  [   32/  126]
train() client id: f_00000-10-1 loss: 0.872914  [   64/  126]
train() client id: f_00000-10-2 loss: 0.771955  [   96/  126]
train() client id: f_00001-0-0 loss: 0.556477  [   32/  265]
train() client id: f_00001-0-1 loss: 0.529465  [   64/  265]
train() client id: f_00001-0-2 loss: 0.534895  [   96/  265]
train() client id: f_00001-0-3 loss: 0.484719  [  128/  265]
train() client id: f_00001-0-4 loss: 0.535019  [  160/  265]
train() client id: f_00001-0-5 loss: 0.462998  [  192/  265]
train() client id: f_00001-0-6 loss: 0.436988  [  224/  265]
train() client id: f_00001-0-7 loss: 0.589208  [  256/  265]
train() client id: f_00001-1-0 loss: 0.488282  [   32/  265]
train() client id: f_00001-1-1 loss: 0.472100  [   64/  265]
train() client id: f_00001-1-2 loss: 0.456036  [   96/  265]
train() client id: f_00001-1-3 loss: 0.521995  [  128/  265]
train() client id: f_00001-1-4 loss: 0.435962  [  160/  265]
train() client id: f_00001-1-5 loss: 0.489718  [  192/  265]
train() client id: f_00001-1-6 loss: 0.614645  [  224/  265]
train() client id: f_00001-1-7 loss: 0.558384  [  256/  265]
train() client id: f_00001-2-0 loss: 0.442476  [   32/  265]
train() client id: f_00001-2-1 loss: 0.512419  [   64/  265]
train() client id: f_00001-2-2 loss: 0.511795  [   96/  265]
train() client id: f_00001-2-3 loss: 0.514531  [  128/  265]
train() client id: f_00001-2-4 loss: 0.424339  [  160/  265]
train() client id: f_00001-2-5 loss: 0.478255  [  192/  265]
train() client id: f_00001-2-6 loss: 0.543982  [  224/  265]
train() client id: f_00001-2-7 loss: 0.589197  [  256/  265]
train() client id: f_00001-3-0 loss: 0.477759  [   32/  265]
train() client id: f_00001-3-1 loss: 0.496292  [   64/  265]
train() client id: f_00001-3-2 loss: 0.619174  [   96/  265]
train() client id: f_00001-3-3 loss: 0.451830  [  128/  265]
train() client id: f_00001-3-4 loss: 0.406815  [  160/  265]
train() client id: f_00001-3-5 loss: 0.712991  [  192/  265]
train() client id: f_00001-3-6 loss: 0.419208  [  224/  265]
train() client id: f_00001-3-7 loss: 0.418321  [  256/  265]
train() client id: f_00001-4-0 loss: 0.421314  [   32/  265]
train() client id: f_00001-4-1 loss: 0.585260  [   64/  265]
train() client id: f_00001-4-2 loss: 0.570089  [   96/  265]
train() client id: f_00001-4-3 loss: 0.464710  [  128/  265]
train() client id: f_00001-4-4 loss: 0.395422  [  160/  265]
train() client id: f_00001-4-5 loss: 0.531896  [  192/  265]
train() client id: f_00001-4-6 loss: 0.505995  [  224/  265]
train() client id: f_00001-4-7 loss: 0.484990  [  256/  265]
train() client id: f_00001-5-0 loss: 0.441053  [   32/  265]
train() client id: f_00001-5-1 loss: 0.414590  [   64/  265]
train() client id: f_00001-5-2 loss: 0.597235  [   96/  265]
train() client id: f_00001-5-3 loss: 0.479931  [  128/  265]
train() client id: f_00001-5-4 loss: 0.486079  [  160/  265]
train() client id: f_00001-5-5 loss: 0.594094  [  192/  265]
train() client id: f_00001-5-6 loss: 0.449625  [  224/  265]
train() client id: f_00001-5-7 loss: 0.543642  [  256/  265]
train() client id: f_00001-6-0 loss: 0.550103  [   32/  265]
train() client id: f_00001-6-1 loss: 0.544722  [   64/  265]
train() client id: f_00001-6-2 loss: 0.452574  [   96/  265]
train() client id: f_00001-6-3 loss: 0.395349  [  128/  265]
train() client id: f_00001-6-4 loss: 0.622837  [  160/  265]
train() client id: f_00001-6-5 loss: 0.500203  [  192/  265]
train() client id: f_00001-6-6 loss: 0.481095  [  224/  265]
train() client id: f_00001-6-7 loss: 0.450920  [  256/  265]
train() client id: f_00001-7-0 loss: 0.502273  [   32/  265]
train() client id: f_00001-7-1 loss: 0.511172  [   64/  265]
train() client id: f_00001-7-2 loss: 0.533010  [   96/  265]
train() client id: f_00001-7-3 loss: 0.433492  [  128/  265]
train() client id: f_00001-7-4 loss: 0.510189  [  160/  265]
train() client id: f_00001-7-5 loss: 0.471859  [  192/  265]
train() client id: f_00001-7-6 loss: 0.516241  [  224/  265]
train() client id: f_00001-7-7 loss: 0.440860  [  256/  265]
train() client id: f_00001-8-0 loss: 0.516218  [   32/  265]
train() client id: f_00001-8-1 loss: 0.465742  [   64/  265]
train() client id: f_00001-8-2 loss: 0.473111  [   96/  265]
train() client id: f_00001-8-3 loss: 0.429441  [  128/  265]
train() client id: f_00001-8-4 loss: 0.526239  [  160/  265]
train() client id: f_00001-8-5 loss: 0.490400  [  192/  265]
train() client id: f_00001-8-6 loss: 0.537339  [  224/  265]
train() client id: f_00001-8-7 loss: 0.560098  [  256/  265]
train() client id: f_00001-9-0 loss: 0.471876  [   32/  265]
train() client id: f_00001-9-1 loss: 0.468757  [   64/  265]
train() client id: f_00001-9-2 loss: 0.461936  [   96/  265]
train() client id: f_00001-9-3 loss: 0.473366  [  128/  265]
train() client id: f_00001-9-4 loss: 0.750853  [  160/  265]
train() client id: f_00001-9-5 loss: 0.572690  [  192/  265]
train() client id: f_00001-9-6 loss: 0.399407  [  224/  265]
train() client id: f_00001-9-7 loss: 0.408002  [  256/  265]
train() client id: f_00001-10-0 loss: 0.525821  [   32/  265]
train() client id: f_00001-10-1 loss: 0.555896  [   64/  265]
train() client id: f_00001-10-2 loss: 0.440439  [   96/  265]
train() client id: f_00001-10-3 loss: 0.476590  [  128/  265]
train() client id: f_00001-10-4 loss: 0.558539  [  160/  265]
train() client id: f_00001-10-5 loss: 0.469043  [  192/  265]
train() client id: f_00001-10-6 loss: 0.500934  [  224/  265]
train() client id: f_00001-10-7 loss: 0.482729  [  256/  265]
train() client id: f_00002-0-0 loss: 0.983151  [   32/  124]
train() client id: f_00002-0-1 loss: 1.058320  [   64/  124]
train() client id: f_00002-0-2 loss: 0.796173  [   96/  124]
train() client id: f_00002-1-0 loss: 0.831120  [   32/  124]
train() client id: f_00002-1-1 loss: 0.880383  [   64/  124]
train() client id: f_00002-1-2 loss: 0.972117  [   96/  124]
train() client id: f_00002-2-0 loss: 0.824034  [   32/  124]
train() client id: f_00002-2-1 loss: 0.973159  [   64/  124]
train() client id: f_00002-2-2 loss: 0.742057  [   96/  124]
train() client id: f_00002-3-0 loss: 0.884871  [   32/  124]
train() client id: f_00002-3-1 loss: 0.685626  [   64/  124]
train() client id: f_00002-3-2 loss: 0.851512  [   96/  124]
train() client id: f_00002-4-0 loss: 1.021619  [   32/  124]
train() client id: f_00002-4-1 loss: 0.863721  [   64/  124]
train() client id: f_00002-4-2 loss: 0.641823  [   96/  124]
train() client id: f_00002-5-0 loss: 0.669422  [   32/  124]
train() client id: f_00002-5-1 loss: 0.810133  [   64/  124]
train() client id: f_00002-5-2 loss: 0.986493  [   96/  124]
train() client id: f_00002-6-0 loss: 0.733579  [   32/  124]
train() client id: f_00002-6-1 loss: 0.759407  [   64/  124]
train() client id: f_00002-6-2 loss: 0.651245  [   96/  124]
train() client id: f_00002-7-0 loss: 0.734988  [   32/  124]
train() client id: f_00002-7-1 loss: 0.598231  [   64/  124]
train() client id: f_00002-7-2 loss: 0.837838  [   96/  124]
train() client id: f_00002-8-0 loss: 0.605939  [   32/  124]
train() client id: f_00002-8-1 loss: 0.901142  [   64/  124]
train() client id: f_00002-8-2 loss: 0.789706  [   96/  124]
train() client id: f_00002-9-0 loss: 0.758966  [   32/  124]
train() client id: f_00002-9-1 loss: 0.609622  [   64/  124]
train() client id: f_00002-9-2 loss: 0.710740  [   96/  124]
train() client id: f_00002-10-0 loss: 0.894730  [   32/  124]
train() client id: f_00002-10-1 loss: 0.615748  [   64/  124]
train() client id: f_00002-10-2 loss: 0.616748  [   96/  124]
train() client id: f_00003-0-0 loss: 0.551251  [   32/   43]
train() client id: f_00003-1-0 loss: 0.564786  [   32/   43]
train() client id: f_00003-2-0 loss: 0.725827  [   32/   43]
train() client id: f_00003-3-0 loss: 0.510724  [   32/   43]
train() client id: f_00003-4-0 loss: 0.786345  [   32/   43]
train() client id: f_00003-5-0 loss: 0.622886  [   32/   43]
train() client id: f_00003-6-0 loss: 0.704812  [   32/   43]
train() client id: f_00003-7-0 loss: 0.641857  [   32/   43]
train() client id: f_00003-8-0 loss: 0.448637  [   32/   43]
train() client id: f_00003-9-0 loss: 0.632973  [   32/   43]
train() client id: f_00003-10-0 loss: 0.530189  [   32/   43]
train() client id: f_00004-0-0 loss: 0.966504  [   32/  306]
train() client id: f_00004-0-1 loss: 0.880795  [   64/  306]
train() client id: f_00004-0-2 loss: 0.899738  [   96/  306]
train() client id: f_00004-0-3 loss: 1.012794  [  128/  306]
train() client id: f_00004-0-4 loss: 0.778661  [  160/  306]
train() client id: f_00004-0-5 loss: 0.833041  [  192/  306]
train() client id: f_00004-0-6 loss: 0.859957  [  224/  306]
train() client id: f_00004-0-7 loss: 0.971948  [  256/  306]
train() client id: f_00004-0-8 loss: 0.822247  [  288/  306]
train() client id: f_00004-1-0 loss: 0.910695  [   32/  306]
train() client id: f_00004-1-1 loss: 0.752591  [   64/  306]
train() client id: f_00004-1-2 loss: 0.778396  [   96/  306]
train() client id: f_00004-1-3 loss: 0.956966  [  128/  306]
train() client id: f_00004-1-4 loss: 1.030428  [  160/  306]
train() client id: f_00004-1-5 loss: 0.948609  [  192/  306]
train() client id: f_00004-1-6 loss: 1.024883  [  224/  306]
train() client id: f_00004-1-7 loss: 0.866924  [  256/  306]
train() client id: f_00004-1-8 loss: 0.750671  [  288/  306]
train() client id: f_00004-2-0 loss: 1.045358  [   32/  306]
train() client id: f_00004-2-1 loss: 0.840066  [   64/  306]
train() client id: f_00004-2-2 loss: 0.892449  [   96/  306]
train() client id: f_00004-2-3 loss: 0.979297  [  128/  306]
train() client id: f_00004-2-4 loss: 0.618255  [  160/  306]
train() client id: f_00004-2-5 loss: 0.870525  [  192/  306]
train() client id: f_00004-2-6 loss: 0.777983  [  224/  306]
train() client id: f_00004-2-7 loss: 0.911844  [  256/  306]
train() client id: f_00004-2-8 loss: 0.959899  [  288/  306]
train() client id: f_00004-3-0 loss: 0.914212  [   32/  306]
train() client id: f_00004-3-1 loss: 0.755630  [   64/  306]
train() client id: f_00004-3-2 loss: 1.030282  [   96/  306]
train() client id: f_00004-3-3 loss: 0.819913  [  128/  306]
train() client id: f_00004-3-4 loss: 0.817013  [  160/  306]
train() client id: f_00004-3-5 loss: 0.921757  [  192/  306]
train() client id: f_00004-3-6 loss: 0.825211  [  224/  306]
train() client id: f_00004-3-7 loss: 0.891019  [  256/  306]
train() client id: f_00004-3-8 loss: 0.961604  [  288/  306]
train() client id: f_00004-4-0 loss: 0.791036  [   32/  306]
train() client id: f_00004-4-1 loss: 0.872724  [   64/  306]
train() client id: f_00004-4-2 loss: 0.840207  [   96/  306]
train() client id: f_00004-4-3 loss: 0.809881  [  128/  306]
train() client id: f_00004-4-4 loss: 0.963712  [  160/  306]
train() client id: f_00004-4-5 loss: 0.887681  [  192/  306]
train() client id: f_00004-4-6 loss: 0.977676  [  224/  306]
train() client id: f_00004-4-7 loss: 0.929280  [  256/  306]
train() client id: f_00004-4-8 loss: 0.876519  [  288/  306]
train() client id: f_00004-5-0 loss: 0.845707  [   32/  306]
train() client id: f_00004-5-1 loss: 1.009655  [   64/  306]
train() client id: f_00004-5-2 loss: 0.801042  [   96/  306]
train() client id: f_00004-5-3 loss: 0.877438  [  128/  306]
train() client id: f_00004-5-4 loss: 0.855970  [  160/  306]
train() client id: f_00004-5-5 loss: 0.818848  [  192/  306]
train() client id: f_00004-5-6 loss: 0.982399  [  224/  306]
train() client id: f_00004-5-7 loss: 0.984571  [  256/  306]
train() client id: f_00004-5-8 loss: 0.901816  [  288/  306]
train() client id: f_00004-6-0 loss: 0.834414  [   32/  306]
train() client id: f_00004-6-1 loss: 0.931499  [   64/  306]
train() client id: f_00004-6-2 loss: 0.970467  [   96/  306]
train() client id: f_00004-6-3 loss: 0.898464  [  128/  306]
train() client id: f_00004-6-4 loss: 0.888247  [  160/  306]
train() client id: f_00004-6-5 loss: 0.873731  [  192/  306]
train() client id: f_00004-6-6 loss: 0.787809  [  224/  306]
train() client id: f_00004-6-7 loss: 0.878341  [  256/  306]
train() client id: f_00004-6-8 loss: 0.839215  [  288/  306]
train() client id: f_00004-7-0 loss: 0.928519  [   32/  306]
train() client id: f_00004-7-1 loss: 0.939048  [   64/  306]
train() client id: f_00004-7-2 loss: 0.803331  [   96/  306]
train() client id: f_00004-7-3 loss: 0.882196  [  128/  306]
train() client id: f_00004-7-4 loss: 0.897629  [  160/  306]
train() client id: f_00004-7-5 loss: 0.929368  [  192/  306]
train() client id: f_00004-7-6 loss: 0.901177  [  224/  306]
train() client id: f_00004-7-7 loss: 0.941243  [  256/  306]
train() client id: f_00004-7-8 loss: 0.762401  [  288/  306]
train() client id: f_00004-8-0 loss: 0.847779  [   32/  306]
train() client id: f_00004-8-1 loss: 0.982652  [   64/  306]
train() client id: f_00004-8-2 loss: 0.927934  [   96/  306]
train() client id: f_00004-8-3 loss: 0.734209  [  128/  306]
train() client id: f_00004-8-4 loss: 0.785702  [  160/  306]
train() client id: f_00004-8-5 loss: 0.939960  [  192/  306]
train() client id: f_00004-8-6 loss: 0.820955  [  224/  306]
train() client id: f_00004-8-7 loss: 0.875325  [  256/  306]
train() client id: f_00004-8-8 loss: 0.949077  [  288/  306]
train() client id: f_00004-9-0 loss: 0.962742  [   32/  306]
train() client id: f_00004-9-1 loss: 0.810547  [   64/  306]
train() client id: f_00004-9-2 loss: 0.870170  [   96/  306]
train() client id: f_00004-9-3 loss: 0.774452  [  128/  306]
train() client id: f_00004-9-4 loss: 0.897784  [  160/  306]
train() client id: f_00004-9-5 loss: 0.861151  [  192/  306]
train() client id: f_00004-9-6 loss: 0.865976  [  224/  306]
train() client id: f_00004-9-7 loss: 0.847055  [  256/  306]
train() client id: f_00004-9-8 loss: 0.969717  [  288/  306]
train() client id: f_00004-10-0 loss: 0.877782  [   32/  306]
train() client id: f_00004-10-1 loss: 0.915581  [   64/  306]
train() client id: f_00004-10-2 loss: 0.974236  [   96/  306]
train() client id: f_00004-10-3 loss: 0.833622  [  128/  306]
train() client id: f_00004-10-4 loss: 1.009913  [  160/  306]
train() client id: f_00004-10-5 loss: 0.964160  [  192/  306]
train() client id: f_00004-10-6 loss: 0.869376  [  224/  306]
train() client id: f_00004-10-7 loss: 0.741453  [  256/  306]
train() client id: f_00004-10-8 loss: 0.772924  [  288/  306]
train() client id: f_00005-0-0 loss: 0.781168  [   32/  146]
train() client id: f_00005-0-1 loss: 0.557617  [   64/  146]
train() client id: f_00005-0-2 loss: 0.790593  [   96/  146]
train() client id: f_00005-0-3 loss: 0.405719  [  128/  146]
train() client id: f_00005-1-0 loss: 0.775364  [   32/  146]
train() client id: f_00005-1-1 loss: 0.746325  [   64/  146]
train() client id: f_00005-1-2 loss: 0.446086  [   96/  146]
train() client id: f_00005-1-3 loss: 0.498868  [  128/  146]
train() client id: f_00005-2-0 loss: 0.677819  [   32/  146]
train() client id: f_00005-2-1 loss: 0.535370  [   64/  146]
train() client id: f_00005-2-2 loss: 0.791637  [   96/  146]
train() client id: f_00005-2-3 loss: 0.487849  [  128/  146]
train() client id: f_00005-3-0 loss: 0.254076  [   32/  146]
train() client id: f_00005-3-1 loss: 0.756081  [   64/  146]
train() client id: f_00005-3-2 loss: 0.737271  [   96/  146]
train() client id: f_00005-3-3 loss: 0.599005  [  128/  146]
train() client id: f_00005-4-0 loss: 0.558282  [   32/  146]
train() client id: f_00005-4-1 loss: 0.593519  [   64/  146]
train() client id: f_00005-4-2 loss: 0.653999  [   96/  146]
train() client id: f_00005-4-3 loss: 0.608813  [  128/  146]
train() client id: f_00005-5-0 loss: 0.700855  [   32/  146]
train() client id: f_00005-5-1 loss: 0.720724  [   64/  146]
train() client id: f_00005-5-2 loss: 0.526386  [   96/  146]
train() client id: f_00005-5-3 loss: 0.601903  [  128/  146]
train() client id: f_00005-6-0 loss: 0.696640  [   32/  146]
train() client id: f_00005-6-1 loss: 0.679216  [   64/  146]
train() client id: f_00005-6-2 loss: 0.587902  [   96/  146]
train() client id: f_00005-6-3 loss: 0.465035  [  128/  146]
train() client id: f_00005-7-0 loss: 0.397424  [   32/  146]
train() client id: f_00005-7-1 loss: 0.722276  [   64/  146]
train() client id: f_00005-7-2 loss: 0.607371  [   96/  146]
train() client id: f_00005-7-3 loss: 0.628544  [  128/  146]
train() client id: f_00005-8-0 loss: 0.765170  [   32/  146]
train() client id: f_00005-8-1 loss: 0.593129  [   64/  146]
train() client id: f_00005-8-2 loss: 0.497270  [   96/  146]
train() client id: f_00005-8-3 loss: 0.458042  [  128/  146]
train() client id: f_00005-9-0 loss: 0.450652  [   32/  146]
train() client id: f_00005-9-1 loss: 0.648579  [   64/  146]
train() client id: f_00005-9-2 loss: 0.677306  [   96/  146]
train() client id: f_00005-9-3 loss: 0.560685  [  128/  146]
train() client id: f_00005-10-0 loss: 0.575053  [   32/  146]
train() client id: f_00005-10-1 loss: 0.656641  [   64/  146]
train() client id: f_00005-10-2 loss: 0.591416  [   96/  146]
train() client id: f_00005-10-3 loss: 0.673110  [  128/  146]
train() client id: f_00006-0-0 loss: 0.549438  [   32/   54]
train() client id: f_00006-1-0 loss: 0.503613  [   32/   54]
train() client id: f_00006-2-0 loss: 0.573252  [   32/   54]
train() client id: f_00006-3-0 loss: 0.503199  [   32/   54]
train() client id: f_00006-4-0 loss: 0.492471  [   32/   54]
train() client id: f_00006-5-0 loss: 0.474104  [   32/   54]
train() client id: f_00006-6-0 loss: 0.550120  [   32/   54]
train() client id: f_00006-7-0 loss: 0.508020  [   32/   54]
train() client id: f_00006-8-0 loss: 0.437995  [   32/   54]
train() client id: f_00006-9-0 loss: 0.518642  [   32/   54]
train() client id: f_00006-10-0 loss: 0.510178  [   32/   54]
train() client id: f_00007-0-0 loss: 0.658566  [   32/  179]
train() client id: f_00007-0-1 loss: 0.744119  [   64/  179]
train() client id: f_00007-0-2 loss: 0.835916  [   96/  179]
train() client id: f_00007-0-3 loss: 0.523075  [  128/  179]
train() client id: f_00007-0-4 loss: 0.617369  [  160/  179]
train() client id: f_00007-1-0 loss: 0.505486  [   32/  179]
train() client id: f_00007-1-1 loss: 0.836995  [   64/  179]
train() client id: f_00007-1-2 loss: 0.692429  [   96/  179]
train() client id: f_00007-1-3 loss: 0.717329  [  128/  179]
train() client id: f_00007-1-4 loss: 0.595438  [  160/  179]
train() client id: f_00007-2-0 loss: 0.468251  [   32/  179]
train() client id: f_00007-2-1 loss: 0.687654  [   64/  179]
train() client id: f_00007-2-2 loss: 0.686110  [   96/  179]
train() client id: f_00007-2-3 loss: 0.524937  [  128/  179]
train() client id: f_00007-2-4 loss: 0.634852  [  160/  179]
train() client id: f_00007-3-0 loss: 0.513121  [   32/  179]
train() client id: f_00007-3-1 loss: 0.614375  [   64/  179]
train() client id: f_00007-3-2 loss: 0.570515  [   96/  179]
train() client id: f_00007-3-3 loss: 0.536571  [  128/  179]
train() client id: f_00007-3-4 loss: 0.934520  [  160/  179]
train() client id: f_00007-4-0 loss: 0.833633  [   32/  179]
train() client id: f_00007-4-1 loss: 0.535969  [   64/  179]
train() client id: f_00007-4-2 loss: 0.458706  [   96/  179]
train() client id: f_00007-4-3 loss: 0.556516  [  128/  179]
train() client id: f_00007-4-4 loss: 0.826097  [  160/  179]
train() client id: f_00007-5-0 loss: 0.747112  [   32/  179]
train() client id: f_00007-5-1 loss: 0.697591  [   64/  179]
train() client id: f_00007-5-2 loss: 0.671786  [   96/  179]
train() client id: f_00007-5-3 loss: 0.491134  [  128/  179]
train() client id: f_00007-5-4 loss: 0.567135  [  160/  179]
train() client id: f_00007-6-0 loss: 0.540175  [   32/  179]
train() client id: f_00007-6-1 loss: 0.651482  [   64/  179]
train() client id: f_00007-6-2 loss: 0.821259  [   96/  179]
train() client id: f_00007-6-3 loss: 0.663548  [  128/  179]
train() client id: f_00007-6-4 loss: 0.470912  [  160/  179]
train() client id: f_00007-7-0 loss: 0.621879  [   32/  179]
train() client id: f_00007-7-1 loss: 0.471053  [   64/  179]
train() client id: f_00007-7-2 loss: 0.772686  [   96/  179]
train() client id: f_00007-7-3 loss: 0.541910  [  128/  179]
train() client id: f_00007-7-4 loss: 0.646726  [  160/  179]
train() client id: f_00007-8-0 loss: 0.703456  [   32/  179]
train() client id: f_00007-8-1 loss: 0.547432  [   64/  179]
train() client id: f_00007-8-2 loss: 0.447163  [   96/  179]
train() client id: f_00007-8-3 loss: 0.473092  [  128/  179]
train() client id: f_00007-8-4 loss: 0.667155  [  160/  179]
train() client id: f_00007-9-0 loss: 0.567213  [   32/  179]
train() client id: f_00007-9-1 loss: 0.721431  [   64/  179]
train() client id: f_00007-9-2 loss: 0.581777  [   96/  179]
train() client id: f_00007-9-3 loss: 0.705637  [  128/  179]
train() client id: f_00007-9-4 loss: 0.577259  [  160/  179]
train() client id: f_00007-10-0 loss: 0.637797  [   32/  179]
train() client id: f_00007-10-1 loss: 0.684348  [   64/  179]
train() client id: f_00007-10-2 loss: 0.447801  [   96/  179]
train() client id: f_00007-10-3 loss: 0.670341  [  128/  179]
train() client id: f_00007-10-4 loss: 0.559606  [  160/  179]
train() client id: f_00008-0-0 loss: 0.684043  [   32/  130]
train() client id: f_00008-0-1 loss: 0.653826  [   64/  130]
train() client id: f_00008-0-2 loss: 0.729671  [   96/  130]
train() client id: f_00008-0-3 loss: 0.686124  [  128/  130]
train() client id: f_00008-1-0 loss: 0.731632  [   32/  130]
train() client id: f_00008-1-1 loss: 0.683673  [   64/  130]
train() client id: f_00008-1-2 loss: 0.760542  [   96/  130]
train() client id: f_00008-1-3 loss: 0.568641  [  128/  130]
train() client id: f_00008-2-0 loss: 0.645151  [   32/  130]
train() client id: f_00008-2-1 loss: 0.589506  [   64/  130]
train() client id: f_00008-2-2 loss: 0.846305  [   96/  130]
train() client id: f_00008-2-3 loss: 0.689404  [  128/  130]
train() client id: f_00008-3-0 loss: 0.801346  [   32/  130]
train() client id: f_00008-3-1 loss: 0.706758  [   64/  130]
train() client id: f_00008-3-2 loss: 0.646619  [   96/  130]
train() client id: f_00008-3-3 loss: 0.612535  [  128/  130]
train() client id: f_00008-4-0 loss: 0.744331  [   32/  130]
train() client id: f_00008-4-1 loss: 0.817749  [   64/  130]
train() client id: f_00008-4-2 loss: 0.655664  [   96/  130]
train() client id: f_00008-4-3 loss: 0.552944  [  128/  130]
train() client id: f_00008-5-0 loss: 0.763823  [   32/  130]
train() client id: f_00008-5-1 loss: 0.600263  [   64/  130]
train() client id: f_00008-5-2 loss: 0.647019  [   96/  130]
train() client id: f_00008-5-3 loss: 0.761193  [  128/  130]
train() client id: f_00008-6-0 loss: 0.712913  [   32/  130]
train() client id: f_00008-6-1 loss: 0.799617  [   64/  130]
train() client id: f_00008-6-2 loss: 0.567732  [   96/  130]
train() client id: f_00008-6-3 loss: 0.686136  [  128/  130]
train() client id: f_00008-7-0 loss: 0.693946  [   32/  130]
train() client id: f_00008-7-1 loss: 0.752364  [   64/  130]
train() client id: f_00008-7-2 loss: 0.570442  [   96/  130]
train() client id: f_00008-7-3 loss: 0.749646  [  128/  130]
train() client id: f_00008-8-0 loss: 0.701247  [   32/  130]
train() client id: f_00008-8-1 loss: 0.781813  [   64/  130]
train() client id: f_00008-8-2 loss: 0.637360  [   96/  130]
train() client id: f_00008-8-3 loss: 0.644653  [  128/  130]
train() client id: f_00008-9-0 loss: 0.694658  [   32/  130]
train() client id: f_00008-9-1 loss: 0.601457  [   64/  130]
train() client id: f_00008-9-2 loss: 0.739703  [   96/  130]
train() client id: f_00008-9-3 loss: 0.715697  [  128/  130]
train() client id: f_00008-10-0 loss: 0.588756  [   32/  130]
train() client id: f_00008-10-1 loss: 0.714980  [   64/  130]
train() client id: f_00008-10-2 loss: 0.761692  [   96/  130]
train() client id: f_00008-10-3 loss: 0.705203  [  128/  130]
train() client id: f_00009-0-0 loss: 0.975497  [   32/  118]
train() client id: f_00009-0-1 loss: 0.873869  [   64/  118]
train() client id: f_00009-0-2 loss: 0.932955  [   96/  118]
train() client id: f_00009-1-0 loss: 0.828620  [   32/  118]
train() client id: f_00009-1-1 loss: 1.009233  [   64/  118]
train() client id: f_00009-1-2 loss: 0.960683  [   96/  118]
train() client id: f_00009-2-0 loss: 0.726382  [   32/  118]
train() client id: f_00009-2-1 loss: 0.768222  [   64/  118]
train() client id: f_00009-2-2 loss: 0.894197  [   96/  118]
train() client id: f_00009-3-0 loss: 0.861454  [   32/  118]
train() client id: f_00009-3-1 loss: 0.782081  [   64/  118]
train() client id: f_00009-3-2 loss: 0.901079  [   96/  118]
train() client id: f_00009-4-0 loss: 0.811389  [   32/  118]
train() client id: f_00009-4-1 loss: 0.766706  [   64/  118]
train() client id: f_00009-4-2 loss: 0.965930  [   96/  118]
train() client id: f_00009-5-0 loss: 0.797627  [   32/  118]
train() client id: f_00009-5-1 loss: 0.892300  [   64/  118]
train() client id: f_00009-5-2 loss: 0.689811  [   96/  118]
train() client id: f_00009-6-0 loss: 0.983922  [   32/  118]
train() client id: f_00009-6-1 loss: 0.651184  [   64/  118]
train() client id: f_00009-6-2 loss: 0.719120  [   96/  118]
train() client id: f_00009-7-0 loss: 0.825504  [   32/  118]
train() client id: f_00009-7-1 loss: 0.870571  [   64/  118]
train() client id: f_00009-7-2 loss: 0.703104  [   96/  118]
train() client id: f_00009-8-0 loss: 0.918403  [   32/  118]
train() client id: f_00009-8-1 loss: 0.808365  [   64/  118]
train() client id: f_00009-8-2 loss: 0.740535  [   96/  118]
train() client id: f_00009-9-0 loss: 0.817617  [   32/  118]
train() client id: f_00009-9-1 loss: 0.810284  [   64/  118]
train() client id: f_00009-9-2 loss: 0.769251  [   96/  118]
train() client id: f_00009-10-0 loss: 0.693582  [   32/  118]
train() client id: f_00009-10-1 loss: 0.922706  [   64/  118]
train() client id: f_00009-10-2 loss: 0.944980  [   96/  118]
At round 51 accuracy: 0.6472148541114059
At round 51 training accuracy: 0.5922199865861838
At round 51 training loss: 0.8264865968022442
update_location
xs = [  -3.9056584     4.20031788  275.00902392   18.81129433    0.97929623
    3.95640986 -237.44319194 -216.32485185  259.66397685 -202.06087855]
ys = [ 267.5879595   250.55583871    1.32061395 -237.45517586  229.35018685
  212.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [285.68963971 269.80709958 292.62895834 258.33858662 250.20485053
 235.1712481  257.65511821 238.32145892 278.80826897 225.48749521]
dists_bs = [194.53998039 194.4773475  482.36153531 455.72150476 184.18104004
 182.90791844 188.28113892 179.01284267 462.29567447 173.12649821]
uav_gains = [3.55494025e-12 4.95127103e-12 3.08824037e-12 6.27981480e-12
 7.39020348e-12 9.78653336e-12 6.36780404e-12 9.24954034e-12
 4.10015262e-12 1.15466699e-11]
bs_gains = [4.30595271e-11 4.30983677e-11 3.38727622e-12 3.97132493e-12
 5.01889727e-11 5.11732577e-11 4.71883724e-11 5.43523537e-11
 3.81521100e-12 5.96865065e-11]
Round 52
-------------------------------
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [4.13977177 8.44962292 4.08581891 1.48801359 9.74245982 4.68747181
 1.834924   5.76952797 4.26183039 3.80061293]
obj_prev = 48.26005410532363
eta_min = 5.2478248998997527e-23	eta_max = 0.9386552346452276
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 11.138694899951254	eta = 0.9090909090909091
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 23.212442703241933	eta = 0.43623527270004686
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 16.931595727685714	eta = 0.598058590314986
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 15.799778450901695	eta = 0.6409005230136667
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 15.733820172197483	eta = 0.6435872637324471
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 15.733571912722388	eta = 0.6435974188731335
eta = 0.6435974188731335
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [0.03636401 0.07647987 0.03578681 0.01240994 0.0883126  0.04213608
 0.01558458 0.05165999 0.03751843 0.03405519]
ene_total = [1.50031121 2.44142183 1.50915709 0.72099371 2.77897583 1.43455086
 0.8117007  1.8214624  1.52193281 1.19306548]
ti_comp = [0.78968755 0.8674062  0.78087534 0.81798185 0.86972334 0.87000865
 0.81855413 0.83227438 0.79380079 0.87219318]
ti_coms = [0.15198525 0.0742666  0.16079746 0.12369095 0.07194946 0.07166414
 0.12311867 0.10939841 0.147872   0.06947962]
t_total = [27.3997818 27.3997818 27.3997818 27.3997818 27.3997818 27.3997818
 27.3997818 27.3997818 27.3997818 27.3997818]
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [4.81930945e-06 3.71600544e-05 4.69770128e-06 1.78526131e-07
 5.69096341e-05 6.17725006e-06 3.53077655e-07 1.24396969e-05
 5.23830472e-06 3.24492313e-06]
ene_total = [0.44237055 0.21717421 0.46800782 0.35990781 0.2110067  0.20870035
 0.35824773 0.31867769 0.43041446 0.20225874]
optimize_network iter = 0 obj = 3.2167660550334363
eta = 0.6435974188731335
freqs = [23024303.73895735 44085381.74515043 22914544.46834145  7585706.59027375
 50770513.42376783 24215895.58340987  9519576.07846752 31035429.18235327
 23632141.3100814  19522731.20042242]
eta_min = 0.6435974188731354	eta_max = 0.698454705717541
af = 0.003087860896766924	bf = 1.1451029605675662	zeta = 0.003396646986443617	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [1.04231482e-06 8.03693475e-06 1.01601355e-06 3.86114307e-08
 1.23083516e-05 1.33600869e-06 7.63632379e-08 2.69044364e-06
 1.13293465e-06 7.01808322e-07]
ene_total = [1.7338512  0.84809379 1.83437118 1.41097682 0.82214886 0.81764254
 1.40445301 1.24824095 1.68694076 0.79265082]
ti_comp = [0.6447458  0.72246444 0.63593359 0.6730401  0.72478158 0.7250669
 0.67361237 0.68733263 0.64885904 0.72725142]
ti_coms = [0.15198525 0.0742666  0.16079746 0.12369095 0.07194946 0.07166414
 0.12311867 0.10939841 0.147872   0.06947962]
t_total = [27.3997818 27.3997818 27.3997818 27.3997818 27.3997818 27.3997818
 27.3997818 27.3997818 27.3997818 27.3997818]
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [3.90488746e-06 2.89320090e-05 3.82574001e-06 1.42428669e-07
 4.42612736e-05 4.80370504e-06 2.81601583e-07 9.85142590e-06
 4.23451096e-06 2.52087196e-06]
ene_total = [0.52281539 0.2563997  0.55311808 0.42538119 0.24895817 0.24662
 0.4234179  0.3765627  0.50868118 0.23902886]
optimize_network iter = 1 obj = 3.8009831688814426
eta = 0.698454705717541
freqs = [22965892.7521965  43105303.11724808 22914544.46834147  7508078.8429111
 49615295.72405244 23663342.1290412   9420739.8305015  30604647.12122058
 23544761.66323882 19067718.62692055]
eta_min = 0.698454705717542	eta_max = 0.6984547057175405
af = 0.002969273754740996	bf = 1.1451029605675662	zeta = 0.003266201130215096	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [1.03703298e-06 7.68356267e-06 1.01601355e-06 3.78252199e-08
 1.17546026e-05 1.27573473e-06 7.47857989e-08 2.61627349e-06
 1.12457210e-06 6.69475724e-07]
ene_total = [1.73385059 0.84805348 1.83437118 1.41097673 0.8220857  0.81763566
 1.40445283 1.24823249 1.68693981 0.79264713]
ti_comp = [0.6447458  0.72246444 0.63593359 0.6730401  0.72478158 0.7250669
 0.67361237 0.68733263 0.64885904 0.72725142]
ti_coms = [0.15198525 0.0742666  0.16079746 0.12369095 0.07194946 0.07166414
 0.12311867 0.10939841 0.147872   0.06947962]
t_total = [27.3997818 27.3997818 27.3997818 27.3997818 27.3997818 27.3997818
 27.3997818 27.3997818 27.3997818 27.3997818]
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [3.90488746e-06 2.89320090e-05 3.82574001e-06 1.42428669e-07
 4.42612736e-05 4.80370504e-06 2.81601583e-07 9.85142590e-06
 4.23451096e-06 2.52087196e-06]
ene_total = [0.52281539 0.2563997  0.55311808 0.42538119 0.24895817 0.24662
 0.4234179  0.3765627  0.50868118 0.23902886]
optimize_network iter = 2 obj = 3.800983168881437
eta = 0.6984547057175405
freqs = [22965892.7521965  43105303.11724809 22914544.46834146  7508078.8429111
 49615295.72405244 23663342.12904121  9420739.8305015  30604647.12122059
 23544761.66323882 19067718.62692055]
Done!
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [3.65510535e-06 2.70813288e-05 3.58102069e-06 1.33318001e-07
 4.14300335e-05 4.49642870e-06 2.63588507e-07 9.22126437e-06
 3.96364398e-06 2.35962053e-06]
ene_total = [0.01520218 0.00745374 0.01608333 0.01236923 0.00723638 0.00717091
 0.01231213 0.01094906 0.01479116 0.00695032]
At round 52 energy consumption: 0.11051844173128197
At round 52 eta: 0.6984547057175405
At round 52 a_n: 10.027672966880559
At round 52 local rounds: 11.751716534008594
At round 52 global rounds: 34.39025251090811
gradient difference: 0.461601585149765
train() client id: f_00000-0-0 loss: 1.200127  [   32/  126]
train() client id: f_00000-0-1 loss: 0.975585  [   64/  126]
train() client id: f_00000-0-2 loss: 1.148553  [   96/  126]
train() client id: f_00000-1-0 loss: 1.059330  [   32/  126]
train() client id: f_00000-1-1 loss: 1.013727  [   64/  126]
train() client id: f_00000-1-2 loss: 0.954796  [   96/  126]
train() client id: f_00000-2-0 loss: 0.998899  [   32/  126]
train() client id: f_00000-2-1 loss: 1.046801  [   64/  126]
train() client id: f_00000-2-2 loss: 1.034287  [   96/  126]
train() client id: f_00000-3-0 loss: 0.999554  [   32/  126]
train() client id: f_00000-3-1 loss: 0.782918  [   64/  126]
train() client id: f_00000-3-2 loss: 1.023870  [   96/  126]
train() client id: f_00000-4-0 loss: 0.959950  [   32/  126]
train() client id: f_00000-4-1 loss: 0.924461  [   64/  126]
train() client id: f_00000-4-2 loss: 0.905456  [   96/  126]
train() client id: f_00000-5-0 loss: 0.909312  [   32/  126]
train() client id: f_00000-5-1 loss: 0.700293  [   64/  126]
train() client id: f_00000-5-2 loss: 0.984008  [   96/  126]
train() client id: f_00000-6-0 loss: 0.876399  [   32/  126]
train() client id: f_00000-6-1 loss: 0.842893  [   64/  126]
train() client id: f_00000-6-2 loss: 0.835525  [   96/  126]
train() client id: f_00000-7-0 loss: 0.906847  [   32/  126]
train() client id: f_00000-7-1 loss: 0.857319  [   64/  126]
train() client id: f_00000-7-2 loss: 0.925067  [   96/  126]
train() client id: f_00000-8-0 loss: 0.780654  [   32/  126]
train() client id: f_00000-8-1 loss: 0.924188  [   64/  126]
train() client id: f_00000-8-2 loss: 0.895207  [   96/  126]
train() client id: f_00000-9-0 loss: 0.838848  [   32/  126]
train() client id: f_00000-9-1 loss: 0.770943  [   64/  126]
train() client id: f_00000-9-2 loss: 0.834648  [   96/  126]
train() client id: f_00000-10-0 loss: 0.879241  [   32/  126]
train() client id: f_00000-10-1 loss: 0.862547  [   64/  126]
train() client id: f_00000-10-2 loss: 0.824625  [   96/  126]
train() client id: f_00001-0-0 loss: 0.426641  [   32/  265]
train() client id: f_00001-0-1 loss: 0.528548  [   64/  265]
train() client id: f_00001-0-2 loss: 0.477917  [   96/  265]
train() client id: f_00001-0-3 loss: 0.444787  [  128/  265]
train() client id: f_00001-0-4 loss: 0.399420  [  160/  265]
train() client id: f_00001-0-5 loss: 0.535309  [  192/  265]
train() client id: f_00001-0-6 loss: 0.447458  [  224/  265]
train() client id: f_00001-0-7 loss: 0.425513  [  256/  265]
train() client id: f_00001-1-0 loss: 0.446366  [   32/  265]
train() client id: f_00001-1-1 loss: 0.347790  [   64/  265]
train() client id: f_00001-1-2 loss: 0.367723  [   96/  265]
train() client id: f_00001-1-3 loss: 0.433046  [  128/  265]
train() client id: f_00001-1-4 loss: 0.409694  [  160/  265]
train() client id: f_00001-1-5 loss: 0.587649  [  192/  265]
train() client id: f_00001-1-6 loss: 0.530298  [  224/  265]
train() client id: f_00001-1-7 loss: 0.501447  [  256/  265]
train() client id: f_00001-2-0 loss: 0.555093  [   32/  265]
train() client id: f_00001-2-1 loss: 0.454985  [   64/  265]
train() client id: f_00001-2-2 loss: 0.496148  [   96/  265]
train() client id: f_00001-2-3 loss: 0.342092  [  128/  265]
train() client id: f_00001-2-4 loss: 0.429437  [  160/  265]
train() client id: f_00001-2-5 loss: 0.415515  [  192/  265]
train() client id: f_00001-2-6 loss: 0.492925  [  224/  265]
train() client id: f_00001-2-7 loss: 0.382728  [  256/  265]
train() client id: f_00001-3-0 loss: 0.648236  [   32/  265]
train() client id: f_00001-3-1 loss: 0.473111  [   64/  265]
train() client id: f_00001-3-2 loss: 0.415513  [   96/  265]
train() client id: f_00001-3-3 loss: 0.423811  [  128/  265]
train() client id: f_00001-3-4 loss: 0.410776  [  160/  265]
train() client id: f_00001-3-5 loss: 0.357142  [  192/  265]
train() client id: f_00001-3-6 loss: 0.426898  [  224/  265]
train() client id: f_00001-3-7 loss: 0.377961  [  256/  265]
train() client id: f_00001-4-0 loss: 0.437350  [   32/  265]
train() client id: f_00001-4-1 loss: 0.430962  [   64/  265]
train() client id: f_00001-4-2 loss: 0.450668  [   96/  265]
train() client id: f_00001-4-3 loss: 0.430968  [  128/  265]
train() client id: f_00001-4-4 loss: 0.409048  [  160/  265]
train() client id: f_00001-4-5 loss: 0.445208  [  192/  265]
train() client id: f_00001-4-6 loss: 0.408442  [  224/  265]
train() client id: f_00001-4-7 loss: 0.370339  [  256/  265]
train() client id: f_00001-5-0 loss: 0.344443  [   32/  265]
train() client id: f_00001-5-1 loss: 0.415875  [   64/  265]
train() client id: f_00001-5-2 loss: 0.480957  [   96/  265]
train() client id: f_00001-5-3 loss: 0.470304  [  128/  265]
train() client id: f_00001-5-4 loss: 0.520559  [  160/  265]
train() client id: f_00001-5-5 loss: 0.386773  [  192/  265]
train() client id: f_00001-5-6 loss: 0.405875  [  224/  265]
train() client id: f_00001-5-7 loss: 0.467125  [  256/  265]
train() client id: f_00001-6-0 loss: 0.496124  [   32/  265]
train() client id: f_00001-6-1 loss: 0.420222  [   64/  265]
train() client id: f_00001-6-2 loss: 0.328985  [   96/  265]
train() client id: f_00001-6-3 loss: 0.463687  [  128/  265]
train() client id: f_00001-6-4 loss: 0.409202  [  160/  265]
train() client id: f_00001-6-5 loss: 0.382172  [  192/  265]
train() client id: f_00001-6-6 loss: 0.473369  [  224/  265]
train() client id: f_00001-6-7 loss: 0.466531  [  256/  265]
train() client id: f_00001-7-0 loss: 0.414195  [   32/  265]
train() client id: f_00001-7-1 loss: 0.521047  [   64/  265]
train() client id: f_00001-7-2 loss: 0.488410  [   96/  265]
train() client id: f_00001-7-3 loss: 0.490439  [  128/  265]
train() client id: f_00001-7-4 loss: 0.342019  [  160/  265]
train() client id: f_00001-7-5 loss: 0.408174  [  192/  265]
train() client id: f_00001-7-6 loss: 0.410218  [  224/  265]
train() client id: f_00001-7-7 loss: 0.369947  [  256/  265]
train() client id: f_00001-8-0 loss: 0.398773  [   32/  265]
train() client id: f_00001-8-1 loss: 0.568279  [   64/  265]
train() client id: f_00001-8-2 loss: 0.531840  [   96/  265]
train() client id: f_00001-8-3 loss: 0.404621  [  128/  265]
train() client id: f_00001-8-4 loss: 0.402374  [  160/  265]
train() client id: f_00001-8-5 loss: 0.328461  [  192/  265]
train() client id: f_00001-8-6 loss: 0.342973  [  224/  265]
train() client id: f_00001-8-7 loss: 0.454977  [  256/  265]
train() client id: f_00001-9-0 loss: 0.423290  [   32/  265]
train() client id: f_00001-9-1 loss: 0.334633  [   64/  265]
train() client id: f_00001-9-2 loss: 0.510855  [   96/  265]
train() client id: f_00001-9-3 loss: 0.456115  [  128/  265]
train() client id: f_00001-9-4 loss: 0.533540  [  160/  265]
train() client id: f_00001-9-5 loss: 0.495502  [  192/  265]
train() client id: f_00001-9-6 loss: 0.343953  [  224/  265]
train() client id: f_00001-9-7 loss: 0.325439  [  256/  265]
train() client id: f_00001-10-0 loss: 0.377738  [   32/  265]
train() client id: f_00001-10-1 loss: 0.377182  [   64/  265]
train() client id: f_00001-10-2 loss: 0.593526  [   96/  265]
train() client id: f_00001-10-3 loss: 0.483960  [  128/  265]
train() client id: f_00001-10-4 loss: 0.395499  [  160/  265]
train() client id: f_00001-10-5 loss: 0.327768  [  192/  265]
train() client id: f_00001-10-6 loss: 0.383587  [  224/  265]
train() client id: f_00001-10-7 loss: 0.388932  [  256/  265]
train() client id: f_00002-0-0 loss: 1.320650  [   32/  124]
train() client id: f_00002-0-1 loss: 1.182350  [   64/  124]
train() client id: f_00002-0-2 loss: 1.207244  [   96/  124]
train() client id: f_00002-1-0 loss: 1.159112  [   32/  124]
train() client id: f_00002-1-1 loss: 1.291813  [   64/  124]
train() client id: f_00002-1-2 loss: 1.059876  [   96/  124]
train() client id: f_00002-2-0 loss: 1.104646  [   32/  124]
train() client id: f_00002-2-1 loss: 1.277999  [   64/  124]
train() client id: f_00002-2-2 loss: 0.987954  [   96/  124]
train() client id: f_00002-3-0 loss: 1.226696  [   32/  124]
train() client id: f_00002-3-1 loss: 1.160345  [   64/  124]
train() client id: f_00002-3-2 loss: 0.995655  [   96/  124]
train() client id: f_00002-4-0 loss: 1.161459  [   32/  124]
train() client id: f_00002-4-1 loss: 1.198981  [   64/  124]
train() client id: f_00002-4-2 loss: 1.137360  [   96/  124]
train() client id: f_00002-5-0 loss: 1.135082  [   32/  124]
train() client id: f_00002-5-1 loss: 0.880091  [   64/  124]
train() client id: f_00002-5-2 loss: 1.227622  [   96/  124]
train() client id: f_00002-6-0 loss: 1.096859  [   32/  124]
train() client id: f_00002-6-1 loss: 1.015082  [   64/  124]
train() client id: f_00002-6-2 loss: 1.095771  [   96/  124]
train() client id: f_00002-7-0 loss: 0.985712  [   32/  124]
train() client id: f_00002-7-1 loss: 1.188616  [   64/  124]
train() client id: f_00002-7-2 loss: 1.126426  [   96/  124]
train() client id: f_00002-8-0 loss: 0.940105  [   32/  124]
train() client id: f_00002-8-1 loss: 1.237901  [   64/  124]
train() client id: f_00002-8-2 loss: 1.102461  [   96/  124]
train() client id: f_00002-9-0 loss: 1.019185  [   32/  124]
train() client id: f_00002-9-1 loss: 0.959208  [   64/  124]
train() client id: f_00002-9-2 loss: 1.293280  [   96/  124]
train() client id: f_00002-10-0 loss: 1.068103  [   32/  124]
train() client id: f_00002-10-1 loss: 1.066886  [   64/  124]
train() client id: f_00002-10-2 loss: 1.017880  [   96/  124]
train() client id: f_00003-0-0 loss: 0.627393  [   32/   43]
train() client id: f_00003-1-0 loss: 0.779303  [   32/   43]
train() client id: f_00003-2-0 loss: 0.652404  [   32/   43]
train() client id: f_00003-3-0 loss: 0.659317  [   32/   43]
train() client id: f_00003-4-0 loss: 0.708021  [   32/   43]
train() client id: f_00003-5-0 loss: 0.730902  [   32/   43]
train() client id: f_00003-6-0 loss: 0.689098  [   32/   43]
train() client id: f_00003-7-0 loss: 0.744772  [   32/   43]
train() client id: f_00003-8-0 loss: 0.718400  [   32/   43]
train() client id: f_00003-9-0 loss: 0.684067  [   32/   43]
train() client id: f_00003-10-0 loss: 0.642180  [   32/   43]
train() client id: f_00004-0-0 loss: 0.937252  [   32/  306]
train() client id: f_00004-0-1 loss: 1.088029  [   64/  306]
train() client id: f_00004-0-2 loss: 0.976277  [   96/  306]
train() client id: f_00004-0-3 loss: 1.034581  [  128/  306]
train() client id: f_00004-0-4 loss: 0.843093  [  160/  306]
train() client id: f_00004-0-5 loss: 0.839338  [  192/  306]
train() client id: f_00004-0-6 loss: 0.947005  [  224/  306]
train() client id: f_00004-0-7 loss: 0.962503  [  256/  306]
train() client id: f_00004-0-8 loss: 0.996869  [  288/  306]
train() client id: f_00004-1-0 loss: 0.885062  [   32/  306]
train() client id: f_00004-1-1 loss: 1.044477  [   64/  306]
train() client id: f_00004-1-2 loss: 0.948359  [   96/  306]
train() client id: f_00004-1-3 loss: 0.947939  [  128/  306]
train() client id: f_00004-1-4 loss: 1.004601  [  160/  306]
train() client id: f_00004-1-5 loss: 0.895472  [  192/  306]
train() client id: f_00004-1-6 loss: 0.903536  [  224/  306]
train() client id: f_00004-1-7 loss: 0.938300  [  256/  306]
train() client id: f_00004-1-8 loss: 1.018955  [  288/  306]
train() client id: f_00004-2-0 loss: 1.043548  [   32/  306]
train() client id: f_00004-2-1 loss: 0.984234  [   64/  306]
train() client id: f_00004-2-2 loss: 0.909671  [   96/  306]
train() client id: f_00004-2-3 loss: 0.967988  [  128/  306]
train() client id: f_00004-2-4 loss: 0.963706  [  160/  306]
train() client id: f_00004-2-5 loss: 0.982856  [  192/  306]
train() client id: f_00004-2-6 loss: 0.830154  [  224/  306]
train() client id: f_00004-2-7 loss: 0.876328  [  256/  306]
train() client id: f_00004-2-8 loss: 0.924754  [  288/  306]
train() client id: f_00004-3-0 loss: 1.040150  [   32/  306]
train() client id: f_00004-3-1 loss: 0.958894  [   64/  306]
train() client id: f_00004-3-2 loss: 1.155689  [   96/  306]
train() client id: f_00004-3-3 loss: 0.932194  [  128/  306]
train() client id: f_00004-3-4 loss: 0.906337  [  160/  306]
train() client id: f_00004-3-5 loss: 0.924490  [  192/  306]
train() client id: f_00004-3-6 loss: 0.835652  [  224/  306]
train() client id: f_00004-3-7 loss: 0.963106  [  256/  306]
train() client id: f_00004-3-8 loss: 0.876713  [  288/  306]
train() client id: f_00004-4-0 loss: 1.139390  [   32/  306]
train() client id: f_00004-4-1 loss: 0.953305  [   64/  306]
train() client id: f_00004-4-2 loss: 0.863903  [   96/  306]
train() client id: f_00004-4-3 loss: 0.911491  [  128/  306]
train() client id: f_00004-4-4 loss: 1.012691  [  160/  306]
train() client id: f_00004-4-5 loss: 0.935290  [  192/  306]
train() client id: f_00004-4-6 loss: 0.834733  [  224/  306]
train() client id: f_00004-4-7 loss: 0.954205  [  256/  306]
train() client id: f_00004-4-8 loss: 0.891517  [  288/  306]
train() client id: f_00004-5-0 loss: 0.885607  [   32/  306]
train() client id: f_00004-5-1 loss: 0.924075  [   64/  306]
train() client id: f_00004-5-2 loss: 0.847611  [   96/  306]
train() client id: f_00004-5-3 loss: 0.986272  [  128/  306]
train() client id: f_00004-5-4 loss: 0.861521  [  160/  306]
train() client id: f_00004-5-5 loss: 1.048774  [  192/  306]
train() client id: f_00004-5-6 loss: 0.920926  [  224/  306]
train() client id: f_00004-5-7 loss: 0.850935  [  256/  306]
train() client id: f_00004-5-8 loss: 1.115995  [  288/  306]
train() client id: f_00004-6-0 loss: 0.970494  [   32/  306]
train() client id: f_00004-6-1 loss: 0.937221  [   64/  306]
train() client id: f_00004-6-2 loss: 0.896697  [   96/  306]
train() client id: f_00004-6-3 loss: 0.952557  [  128/  306]
train() client id: f_00004-6-4 loss: 0.978528  [  160/  306]
train() client id: f_00004-6-5 loss: 0.853284  [  192/  306]
train() client id: f_00004-6-6 loss: 0.992099  [  224/  306]
train() client id: f_00004-6-7 loss: 0.999914  [  256/  306]
train() client id: f_00004-6-8 loss: 0.886983  [  288/  306]
train() client id: f_00004-7-0 loss: 0.957762  [   32/  306]
train() client id: f_00004-7-1 loss: 0.993555  [   64/  306]
train() client id: f_00004-7-2 loss: 0.882367  [   96/  306]
train() client id: f_00004-7-3 loss: 0.849456  [  128/  306]
train() client id: f_00004-7-4 loss: 1.077644  [  160/  306]
train() client id: f_00004-7-5 loss: 0.877494  [  192/  306]
train() client id: f_00004-7-6 loss: 0.941867  [  224/  306]
train() client id: f_00004-7-7 loss: 0.949391  [  256/  306]
train() client id: f_00004-7-8 loss: 0.962630  [  288/  306]
train() client id: f_00004-8-0 loss: 0.994621  [   32/  306]
train() client id: f_00004-8-1 loss: 0.952750  [   64/  306]
train() client id: f_00004-8-2 loss: 0.937858  [   96/  306]
train() client id: f_00004-8-3 loss: 0.954716  [  128/  306]
train() client id: f_00004-8-4 loss: 0.906276  [  160/  306]
train() client id: f_00004-8-5 loss: 0.930754  [  192/  306]
train() client id: f_00004-8-6 loss: 0.865016  [  224/  306]
train() client id: f_00004-8-7 loss: 0.882370  [  256/  306]
train() client id: f_00004-8-8 loss: 0.888834  [  288/  306]
train() client id: f_00004-9-0 loss: 0.847354  [   32/  306]
train() client id: f_00004-9-1 loss: 0.879650  [   64/  306]
train() client id: f_00004-9-2 loss: 0.934444  [   96/  306]
train() client id: f_00004-9-3 loss: 0.977789  [  128/  306]
train() client id: f_00004-9-4 loss: 1.019273  [  160/  306]
train() client id: f_00004-9-5 loss: 0.942065  [  192/  306]
train() client id: f_00004-9-6 loss: 0.862357  [  224/  306]
train() client id: f_00004-9-7 loss: 1.061755  [  256/  306]
train() client id: f_00004-9-8 loss: 0.862392  [  288/  306]
train() client id: f_00004-10-0 loss: 0.911896  [   32/  306]
train() client id: f_00004-10-1 loss: 0.910055  [   64/  306]
train() client id: f_00004-10-2 loss: 0.999623  [   96/  306]
train() client id: f_00004-10-3 loss: 1.095089  [  128/  306]
train() client id: f_00004-10-4 loss: 0.903463  [  160/  306]
train() client id: f_00004-10-5 loss: 0.868819  [  192/  306]
train() client id: f_00004-10-6 loss: 0.807523  [  224/  306]
train() client id: f_00004-10-7 loss: 0.865330  [  256/  306]
train() client id: f_00004-10-8 loss: 0.959069  [  288/  306]
train() client id: f_00005-0-0 loss: 0.721159  [   32/  146]
train() client id: f_00005-0-1 loss: 0.588495  [   64/  146]
train() client id: f_00005-0-2 loss: 0.622457  [   96/  146]
train() client id: f_00005-0-3 loss: 0.393558  [  128/  146]
train() client id: f_00005-1-0 loss: 0.460694  [   32/  146]
train() client id: f_00005-1-1 loss: 0.754987  [   64/  146]
train() client id: f_00005-1-2 loss: 0.457285  [   96/  146]
train() client id: f_00005-1-3 loss: 0.744942  [  128/  146]
train() client id: f_00005-2-0 loss: 0.223281  [   32/  146]
train() client id: f_00005-2-1 loss: 0.459458  [   64/  146]
train() client id: f_00005-2-2 loss: 0.743075  [   96/  146]
train() client id: f_00005-2-3 loss: 0.707805  [  128/  146]
train() client id: f_00005-3-0 loss: 0.600372  [   32/  146]
train() client id: f_00005-3-1 loss: 0.664896  [   64/  146]
train() client id: f_00005-3-2 loss: 0.451021  [   96/  146]
train() client id: f_00005-3-3 loss: 0.562996  [  128/  146]
train() client id: f_00005-4-0 loss: 0.497591  [   32/  146]
train() client id: f_00005-4-1 loss: 0.305092  [   64/  146]
train() client id: f_00005-4-2 loss: 0.633418  [   96/  146]
train() client id: f_00005-4-3 loss: 0.678908  [  128/  146]
train() client id: f_00005-5-0 loss: 0.423664  [   32/  146]
train() client id: f_00005-5-1 loss: 0.480919  [   64/  146]
train() client id: f_00005-5-2 loss: 0.649011  [   96/  146]
train() client id: f_00005-5-3 loss: 0.723855  [  128/  146]
train() client id: f_00005-6-0 loss: 0.386478  [   32/  146]
train() client id: f_00005-6-1 loss: 0.676952  [   64/  146]
train() client id: f_00005-6-2 loss: 0.712423  [   96/  146]
train() client id: f_00005-6-3 loss: 0.573299  [  128/  146]
train() client id: f_00005-7-0 loss: 0.307688  [   32/  146]
train() client id: f_00005-7-1 loss: 0.531705  [   64/  146]
train() client id: f_00005-7-2 loss: 0.902928  [   96/  146]
train() client id: f_00005-7-3 loss: 0.418479  [  128/  146]
train() client id: f_00005-8-0 loss: 0.584233  [   32/  146]
train() client id: f_00005-8-1 loss: 0.368083  [   64/  146]
train() client id: f_00005-8-2 loss: 0.479398  [   96/  146]
train() client id: f_00005-8-3 loss: 0.752793  [  128/  146]
train() client id: f_00005-9-0 loss: 0.508454  [   32/  146]
train() client id: f_00005-9-1 loss: 0.425497  [   64/  146]
train() client id: f_00005-9-2 loss: 0.733322  [   96/  146]
train() client id: f_00005-9-3 loss: 0.515740  [  128/  146]
train() client id: f_00005-10-0 loss: 0.618899  [   32/  146]
train() client id: f_00005-10-1 loss: 0.717656  [   64/  146]
train() client id: f_00005-10-2 loss: 0.497480  [   96/  146]
train() client id: f_00005-10-3 loss: 0.476138  [  128/  146]
train() client id: f_00006-0-0 loss: 0.533063  [   32/   54]
train() client id: f_00006-1-0 loss: 0.496339  [   32/   54]
train() client id: f_00006-2-0 loss: 0.502712  [   32/   54]
train() client id: f_00006-3-0 loss: 0.448653  [   32/   54]
train() client id: f_00006-4-0 loss: 0.545825  [   32/   54]
train() client id: f_00006-5-0 loss: 0.473146  [   32/   54]
train() client id: f_00006-6-0 loss: 0.501716  [   32/   54]
train() client id: f_00006-7-0 loss: 0.536281  [   32/   54]
train() client id: f_00006-8-0 loss: 0.506222  [   32/   54]
train() client id: f_00006-9-0 loss: 0.496665  [   32/   54]
train() client id: f_00006-10-0 loss: 0.525241  [   32/   54]
train() client id: f_00007-0-0 loss: 0.555520  [   32/  179]
train() client id: f_00007-0-1 loss: 0.529087  [   64/  179]
train() client id: f_00007-0-2 loss: 0.538783  [   96/  179]
train() client id: f_00007-0-3 loss: 0.741834  [  128/  179]
train() client id: f_00007-0-4 loss: 0.568457  [  160/  179]
train() client id: f_00007-1-0 loss: 0.667659  [   32/  179]
train() client id: f_00007-1-1 loss: 0.740006  [   64/  179]
train() client id: f_00007-1-2 loss: 0.514088  [   96/  179]
train() client id: f_00007-1-3 loss: 0.555421  [  128/  179]
train() client id: f_00007-1-4 loss: 0.523858  [  160/  179]
train() client id: f_00007-2-0 loss: 0.779931  [   32/  179]
train() client id: f_00007-2-1 loss: 0.496457  [   64/  179]
train() client id: f_00007-2-2 loss: 0.539216  [   96/  179]
train() client id: f_00007-2-3 loss: 0.513984  [  128/  179]
train() client id: f_00007-2-4 loss: 0.507953  [  160/  179]
train() client id: f_00007-3-0 loss: 0.577503  [   32/  179]
train() client id: f_00007-3-1 loss: 0.606454  [   64/  179]
train() client id: f_00007-3-2 loss: 0.511432  [   96/  179]
train() client id: f_00007-3-3 loss: 0.645686  [  128/  179]
train() client id: f_00007-3-4 loss: 0.525019  [  160/  179]
train() client id: f_00007-4-0 loss: 0.447208  [   32/  179]
train() client id: f_00007-4-1 loss: 0.559366  [   64/  179]
train() client id: f_00007-4-2 loss: 0.776674  [   96/  179]
train() client id: f_00007-4-3 loss: 0.598437  [  128/  179]
train() client id: f_00007-4-4 loss: 0.540221  [  160/  179]
train() client id: f_00007-5-0 loss: 0.733929  [   32/  179]
train() client id: f_00007-5-1 loss: 0.481601  [   64/  179]
train() client id: f_00007-5-2 loss: 0.706681  [   96/  179]
train() client id: f_00007-5-3 loss: 0.571175  [  128/  179]
train() client id: f_00007-5-4 loss: 0.446795  [  160/  179]
train() client id: f_00007-6-0 loss: 0.610126  [   32/  179]
train() client id: f_00007-6-1 loss: 0.413504  [   64/  179]
train() client id: f_00007-6-2 loss: 0.539455  [   96/  179]
train() client id: f_00007-6-3 loss: 0.631344  [  128/  179]
train() client id: f_00007-6-4 loss: 0.789858  [  160/  179]
train() client id: f_00007-7-0 loss: 0.522415  [   32/  179]
train() client id: f_00007-7-1 loss: 0.437044  [   64/  179]
train() client id: f_00007-7-2 loss: 0.514937  [   96/  179]
train() client id: f_00007-7-3 loss: 0.972685  [  128/  179]
train() client id: f_00007-7-4 loss: 0.500863  [  160/  179]
train() client id: f_00007-8-0 loss: 0.513979  [   32/  179]
train() client id: f_00007-8-1 loss: 0.662815  [   64/  179]
train() client id: f_00007-8-2 loss: 0.746531  [   96/  179]
train() client id: f_00007-8-3 loss: 0.640338  [  128/  179]
train() client id: f_00007-8-4 loss: 0.394247  [  160/  179]
train() client id: f_00007-9-0 loss: 0.676378  [   32/  179]
train() client id: f_00007-9-1 loss: 0.416410  [   64/  179]
train() client id: f_00007-9-2 loss: 0.782010  [   96/  179]
train() client id: f_00007-9-3 loss: 0.478342  [  128/  179]
train() client id: f_00007-9-4 loss: 0.542694  [  160/  179]
train() client id: f_00007-10-0 loss: 0.483430  [   32/  179]
train() client id: f_00007-10-1 loss: 0.690378  [   64/  179]
train() client id: f_00007-10-2 loss: 0.689331  [   96/  179]
train() client id: f_00007-10-3 loss: 0.522603  [  128/  179]
train() client id: f_00007-10-4 loss: 0.531762  [  160/  179]
train() client id: f_00008-0-0 loss: 0.665502  [   32/  130]
train() client id: f_00008-0-1 loss: 0.726849  [   64/  130]
train() client id: f_00008-0-2 loss: 0.717679  [   96/  130]
train() client id: f_00008-0-3 loss: 0.630467  [  128/  130]
train() client id: f_00008-1-0 loss: 0.663267  [   32/  130]
train() client id: f_00008-1-1 loss: 0.636650  [   64/  130]
train() client id: f_00008-1-2 loss: 0.685994  [   96/  130]
train() client id: f_00008-1-3 loss: 0.796370  [  128/  130]
train() client id: f_00008-2-0 loss: 0.592658  [   32/  130]
train() client id: f_00008-2-1 loss: 0.817258  [   64/  130]
train() client id: f_00008-2-2 loss: 0.745650  [   96/  130]
train() client id: f_00008-2-3 loss: 0.627393  [  128/  130]
train() client id: f_00008-3-0 loss: 0.632832  [   32/  130]
train() client id: f_00008-3-1 loss: 0.740185  [   64/  130]
train() client id: f_00008-3-2 loss: 0.713603  [   96/  130]
train() client id: f_00008-3-3 loss: 0.688465  [  128/  130]
train() client id: f_00008-4-0 loss: 0.716259  [   32/  130]
train() client id: f_00008-4-1 loss: 0.710102  [   64/  130]
train() client id: f_00008-4-2 loss: 0.679997  [   96/  130]
train() client id: f_00008-4-3 loss: 0.672849  [  128/  130]
train() client id: f_00008-5-0 loss: 0.655526  [   32/  130]
train() client id: f_00008-5-1 loss: 0.711568  [   64/  130]
train() client id: f_00008-5-2 loss: 0.750552  [   96/  130]
train() client id: f_00008-5-3 loss: 0.668319  [  128/  130]
train() client id: f_00008-6-0 loss: 0.688313  [   32/  130]
train() client id: f_00008-6-1 loss: 0.785420  [   64/  130]
train() client id: f_00008-6-2 loss: 0.619442  [   96/  130]
train() client id: f_00008-6-3 loss: 0.635946  [  128/  130]
train() client id: f_00008-7-0 loss: 0.633930  [   32/  130]
train() client id: f_00008-7-1 loss: 0.675527  [   64/  130]
train() client id: f_00008-7-2 loss: 0.822487  [   96/  130]
train() client id: f_00008-7-3 loss: 0.653123  [  128/  130]
train() client id: f_00008-8-0 loss: 0.654399  [   32/  130]
train() client id: f_00008-8-1 loss: 0.712820  [   64/  130]
train() client id: f_00008-8-2 loss: 0.667371  [   96/  130]
train() client id: f_00008-8-3 loss: 0.755850  [  128/  130]
train() client id: f_00008-9-0 loss: 0.667572  [   32/  130]
train() client id: f_00008-9-1 loss: 0.763352  [   64/  130]
train() client id: f_00008-9-2 loss: 0.640961  [   96/  130]
train() client id: f_00008-9-3 loss: 0.709594  [  128/  130]
train() client id: f_00008-10-0 loss: 0.702218  [   32/  130]
train() client id: f_00008-10-1 loss: 0.671715  [   64/  130]
train() client id: f_00008-10-2 loss: 0.817309  [   96/  130]
train() client id: f_00008-10-3 loss: 0.597907  [  128/  130]
train() client id: f_00009-0-0 loss: 0.966607  [   32/  118]
train() client id: f_00009-0-1 loss: 0.905930  [   64/  118]
train() client id: f_00009-0-2 loss: 1.021168  [   96/  118]
train() client id: f_00009-1-0 loss: 0.735300  [   32/  118]
train() client id: f_00009-1-1 loss: 0.866991  [   64/  118]
train() client id: f_00009-1-2 loss: 0.998909  [   96/  118]
train() client id: f_00009-2-0 loss: 0.865293  [   32/  118]
train() client id: f_00009-2-1 loss: 0.775184  [   64/  118]
train() client id: f_00009-2-2 loss: 0.869593  [   96/  118]
train() client id: f_00009-3-0 loss: 0.874433  [   32/  118]
train() client id: f_00009-3-1 loss: 0.573764  [   64/  118]
train() client id: f_00009-3-2 loss: 0.901441  [   96/  118]
train() client id: f_00009-4-0 loss: 0.796354  [   32/  118]
train() client id: f_00009-4-1 loss: 0.734030  [   64/  118]
train() client id: f_00009-4-2 loss: 0.848881  [   96/  118]
train() client id: f_00009-5-0 loss: 0.768910  [   32/  118]
train() client id: f_00009-5-1 loss: 0.778167  [   64/  118]
train() client id: f_00009-5-2 loss: 0.762137  [   96/  118]
train() client id: f_00009-6-0 loss: 0.856252  [   32/  118]
train() client id: f_00009-6-1 loss: 0.584032  [   64/  118]
train() client id: f_00009-6-2 loss: 0.773893  [   96/  118]
train() client id: f_00009-7-0 loss: 0.733784  [   32/  118]
train() client id: f_00009-7-1 loss: 0.652389  [   64/  118]
train() client id: f_00009-7-2 loss: 0.674786  [   96/  118]
train() client id: f_00009-8-0 loss: 0.667575  [   32/  118]
train() client id: f_00009-8-1 loss: 0.720580  [   64/  118]
train() client id: f_00009-8-2 loss: 0.843710  [   96/  118]
train() client id: f_00009-9-0 loss: 0.514760  [   32/  118]
train() client id: f_00009-9-1 loss: 0.849004  [   64/  118]
train() client id: f_00009-9-2 loss: 0.712386  [   96/  118]
train() client id: f_00009-10-0 loss: 0.750377  [   32/  118]
train() client id: f_00009-10-1 loss: 0.702944  [   64/  118]
train() client id: f_00009-10-2 loss: 0.679490  [   96/  118]
At round 52 accuracy: 0.6472148541114059
At round 52 training accuracy: 0.5888665325285044
At round 52 training loss: 0.8398476852724117
update_location
xs = [  -3.9056584     4.20031788  280.00902392   18.81129433    0.97929623
    3.95640986 -242.44319194 -221.32485185  264.66397685 -207.06087855]
ys = [ 272.5879595   255.55583871    1.32061395 -242.45517586  234.35018685
  217.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [290.37811527 274.45660744 297.33280595 262.94177511 254.79593618
 239.70535547 262.27007427 242.86903117 283.47079323 229.97873659]
dists_bs = [196.96873753 196.4739094  487.0295073  460.25171554 185.71795115
 184.00665242 189.99794523 180.23275607 466.99992549 173.97813991]
uav_gains = [3.23107212e-12 4.49127941e-12 2.81429478e-12 5.71301961e-12
 6.74655076e-12 9.01929979e-12 5.79291830e-12 8.50621989e-12
 3.72132531e-12 1.07099613e-11]
bs_gains = [4.15893030e-11 4.18832527e-11 3.29715465e-12 3.86284178e-12
 4.90346648e-11 5.03222692e-11 4.60041651e-11 5.33285356e-11
 3.70857455e-12 5.88720247e-11]
Round 53
-------------------------------
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [4.00899084 8.17097489 3.95724982 1.44293614 9.42099912 4.53288609
 1.77835404 5.58181324 4.12270253 3.67526442]
obj_prev = 46.692171130454035
eta_min = 9.633908147429921e-24	eta_max = 0.9382560279743308
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 10.770764989587082	eta = 0.9090909090909091
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 22.73132098521351	eta = 0.4307538722609915
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 16.477782321424517	eta = 0.5942307250446651
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 15.352697592199675	eta = 0.6377774640049659
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 15.286662235887265	eta = 0.6405325364618376
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 15.286409541549396	eta = 0.6405431248831895
eta = 0.6405431248831895
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [0.03675654 0.07730543 0.03617311 0.0125439  0.08926589 0.04259092
 0.0157528  0.05221763 0.03792342 0.0344228 ]
ene_total = [1.46579906 2.36500809 1.47528965 0.70621205 2.69190422 1.38875393
 0.79404133 1.76977105 1.47499189 1.15463827]
ti_comp = [0.82252146 0.90567545 0.81328504 0.85268029 0.90809929 0.90848315
 0.85328562 0.86816903 0.8308949  0.91072419]
ti_coms = [0.15787206 0.07471807 0.16710848 0.12771323 0.07229423 0.07191036
 0.1271079  0.11222449 0.14949862 0.06966933]
t_total = [27.3497776 27.3497776 27.3497776 27.3497776 27.3497776 27.3497776
 27.3497776 27.3497776 27.3497776 27.3497776]
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [4.58764259e-06 3.52017978e-05 4.47251598e-06 1.69670107e-07
 5.39100853e-05 5.85055712e-06 3.35555868e-07 1.18065477e-05
 4.93753617e-06 3.07358121e-06]
ene_total = [0.44053948 0.20942103 0.46630285 0.35628294 0.2031812  0.20076965
 0.35459888 0.31339904 0.41719008 0.19444042]
optimize_network iter = 0 obj = 3.1561255646288915
eta = 0.6405431248831895
freqs = [22343821.0822694  42678329.73940749 22238886.46292163  7355570.59611931
 49149853.28596357 23440674.99681275  9230675.16871088 30073423.9891545
 22820828.02431027 18898584.29975834]
eta_min = 0.6405431248831939	eta_max = 0.6991595698398908
af = 0.002799252834533405	bf = 1.1332441370306174	zeta = 0.0030791781179867455	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [9.81614110e-07 7.53209970e-06 9.56980564e-07 3.63041731e-08
 1.15350966e-05 1.25183889e-06 7.17986128e-08 2.52623730e-06
 1.05648055e-06 6.57651642e-07]
ene_total = [1.74150662 0.82500401 1.84338565 1.40873715 0.79870952 0.79334107
 1.40206398 1.23816416 1.64915218 0.76855593]
ti_comp = [0.6626492  0.74580319 0.65341278 0.69280803 0.74822703 0.7486109
 0.69341336 0.70829677 0.67102264 0.75085193]
ti_coms = [0.15787206 0.07471807 0.16710848 0.12771323 0.07229423 0.07191036
 0.1271079  0.11222449 0.14949862 0.06966933]
t_total = [27.3497776 27.3497776 27.3497776 27.3497776 27.3497776 27.3497776
 27.3497776 27.3497776 27.3497776 27.3497776]
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [3.66565189e-06 2.69213035e-05 3.59332702e-06 1.33286597e-07
 4.11817149e-05 4.46841209e-06 2.63514067e-07 9.19889168e-06
 3.92611172e-06 2.34500816e-06]
ene_total = [0.52634448 0.2499491  0.55712907 0.42570072 0.24234523 0.239842
 0.42368735 0.37437551 0.49844265 0.23230136]
optimize_network iter = 1 obj = 3.770117456851034
eta = 0.6991595698398908
freqs = [22282599.06518497 41639030.99946182 22238886.46292161  7273350.56555898
 47925539.38367743 22854708.69293805  9126001.3904321  29615341.56378339
 22703099.87437893 18416482.47146711]
eta_min = 0.6991595698399091	eta_max = 0.6991595698398846
af = 0.002681430697799304	bf = 1.1332441370306174	zeta = 0.0029495737675792346	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [9.76242239e-07 7.16972436e-06 9.56980564e-07 3.54970983e-08
 1.09675798e-05 1.19003461e-06 7.01794852e-08 2.44986346e-06
 1.04560832e-06 6.24526300e-07]
ene_total = [1.74150603 0.82496404 1.84338565 1.40873707 0.79864692 0.79333425
 1.4020638  1.23815573 1.64915098 0.76855228]
ti_comp = [0.6626492  0.74580319 0.65341278 0.69280803 0.74822703 0.7486109
 0.69341336 0.70829677 0.67102264 0.75085193]
ti_coms = [0.15787206 0.07471807 0.16710848 0.12771323 0.07229423 0.07191036
 0.1271079  0.11222449 0.14949862 0.06966933]
t_total = [27.3497776 27.3497776 27.3497776 27.3497776 27.3497776 27.3497776
 27.3497776 27.3497776 27.3497776 27.3497776]
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [3.66565189e-06 2.69213035e-05 3.59332702e-06 1.33286597e-07
 4.11817149e-05 4.46841209e-06 2.63514067e-07 9.19889168e-06
 3.92611172e-06 2.34500816e-06]
ene_total = [0.52634448 0.2499491  0.55712907 0.42570072 0.24234523 0.239842
 0.42368735 0.37437551 0.49844265 0.23230136]
optimize_network iter = 2 obj = 3.770117456850956
eta = 0.6991595698398846
freqs = [22282599.06518495 41639030.99946193 22238886.46292159  7273350.56555898
 47925539.38367755 22854708.6929381   9126001.39043211 29615341.56378343
 22703099.87437893 18416482.47146716]
Done!
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [3.44084355e-06 2.52702648e-05 3.37295424e-06 1.25112351e-07
 3.86561088e-05 4.19437181e-06 2.47353187e-07 8.63473895e-06
 3.68532982e-06 2.20119270e-06]
ene_total = [0.01579065 0.00749708 0.01671422 0.01277145 0.00726808 0.00719523
 0.01271104 0.01123108 0.01495355 0.00696913]
At round 53 energy consumption: 0.11310150513411767
At round 53 eta: 0.6991595698398846
At round 53 a_n: 9.685127119911243
At round 53 local rounds: 11.718687629159392
At round 53 global rounds: 33.33219860622975
gradient difference: 0.5688074231147766
train() client id: f_00000-0-0 loss: 1.016428  [   32/  126]
train() client id: f_00000-0-1 loss: 0.944968  [   64/  126]
train() client id: f_00000-0-2 loss: 1.001469  [   96/  126]
train() client id: f_00000-1-0 loss: 1.027993  [   32/  126]
train() client id: f_00000-1-1 loss: 0.722751  [   64/  126]
train() client id: f_00000-1-2 loss: 0.953116  [   96/  126]
train() client id: f_00000-2-0 loss: 0.928215  [   32/  126]
train() client id: f_00000-2-1 loss: 0.739650  [   64/  126]
train() client id: f_00000-2-2 loss: 0.889043  [   96/  126]
train() client id: f_00000-3-0 loss: 0.784417  [   32/  126]
train() client id: f_00000-3-1 loss: 0.748873  [   64/  126]
train() client id: f_00000-3-2 loss: 0.931431  [   96/  126]
train() client id: f_00000-4-0 loss: 0.797740  [   32/  126]
train() client id: f_00000-4-1 loss: 0.827570  [   64/  126]
train() client id: f_00000-4-2 loss: 0.774955  [   96/  126]
train() client id: f_00000-5-0 loss: 0.837200  [   32/  126]
train() client id: f_00000-5-1 loss: 0.920430  [   64/  126]
train() client id: f_00000-5-2 loss: 0.735450  [   96/  126]
train() client id: f_00000-6-0 loss: 0.880020  [   32/  126]
train() client id: f_00000-6-1 loss: 0.844974  [   64/  126]
train() client id: f_00000-6-2 loss: 0.848032  [   96/  126]
train() client id: f_00000-7-0 loss: 0.751729  [   32/  126]
train() client id: f_00000-7-1 loss: 0.929863  [   64/  126]
train() client id: f_00000-7-2 loss: 0.890317  [   96/  126]
train() client id: f_00000-8-0 loss: 0.747531  [   32/  126]
train() client id: f_00000-8-1 loss: 0.939607  [   64/  126]
train() client id: f_00000-8-2 loss: 0.899630  [   96/  126]
train() client id: f_00000-9-0 loss: 0.705386  [   32/  126]
train() client id: f_00000-9-1 loss: 0.854216  [   64/  126]
train() client id: f_00000-9-2 loss: 1.085932  [   96/  126]
train() client id: f_00000-10-0 loss: 0.838931  [   32/  126]
train() client id: f_00000-10-1 loss: 0.971029  [   64/  126]
train() client id: f_00000-10-2 loss: 0.943576  [   96/  126]
train() client id: f_00001-0-0 loss: 0.458922  [   32/  265]
train() client id: f_00001-0-1 loss: 0.572972  [   64/  265]
train() client id: f_00001-0-2 loss: 0.542919  [   96/  265]
train() client id: f_00001-0-3 loss: 0.617295  [  128/  265]
train() client id: f_00001-0-4 loss: 0.440186  [  160/  265]
train() client id: f_00001-0-5 loss: 0.457515  [  192/  265]
train() client id: f_00001-0-6 loss: 0.449784  [  224/  265]
train() client id: f_00001-0-7 loss: 0.372375  [  256/  265]
train() client id: f_00001-1-0 loss: 0.398454  [   32/  265]
train() client id: f_00001-1-1 loss: 0.437930  [   64/  265]
train() client id: f_00001-1-2 loss: 0.545108  [   96/  265]
train() client id: f_00001-1-3 loss: 0.504262  [  128/  265]
train() client id: f_00001-1-4 loss: 0.464269  [  160/  265]
train() client id: f_00001-1-5 loss: 0.412182  [  192/  265]
train() client id: f_00001-1-6 loss: 0.458092  [  224/  265]
train() client id: f_00001-1-7 loss: 0.639640  [  256/  265]
train() client id: f_00001-2-0 loss: 0.545886  [   32/  265]
train() client id: f_00001-2-1 loss: 0.517474  [   64/  265]
train() client id: f_00001-2-2 loss: 0.420749  [   96/  265]
train() client id: f_00001-2-3 loss: 0.564308  [  128/  265]
train() client id: f_00001-2-4 loss: 0.460851  [  160/  265]
train() client id: f_00001-2-5 loss: 0.447195  [  192/  265]
train() client id: f_00001-2-6 loss: 0.375017  [  224/  265]
train() client id: f_00001-2-7 loss: 0.476670  [  256/  265]
train() client id: f_00001-3-0 loss: 0.462963  [   32/  265]
train() client id: f_00001-3-1 loss: 0.414448  [   64/  265]
train() client id: f_00001-3-2 loss: 0.439988  [   96/  265]
train() client id: f_00001-3-3 loss: 0.424037  [  128/  265]
train() client id: f_00001-3-4 loss: 0.493892  [  160/  265]
train() client id: f_00001-3-5 loss: 0.587200  [  192/  265]
train() client id: f_00001-3-6 loss: 0.446462  [  224/  265]
train() client id: f_00001-3-7 loss: 0.494810  [  256/  265]
train() client id: f_00001-4-0 loss: 0.385381  [   32/  265]
train() client id: f_00001-4-1 loss: 0.525031  [   64/  265]
train() client id: f_00001-4-2 loss: 0.436137  [   96/  265]
train() client id: f_00001-4-3 loss: 0.505153  [  128/  265]
train() client id: f_00001-4-4 loss: 0.485899  [  160/  265]
train() client id: f_00001-4-5 loss: 0.402894  [  192/  265]
train() client id: f_00001-4-6 loss: 0.469355  [  224/  265]
train() client id: f_00001-4-7 loss: 0.456323  [  256/  265]
train() client id: f_00001-5-0 loss: 0.385898  [   32/  265]
train() client id: f_00001-5-1 loss: 0.395681  [   64/  265]
train() client id: f_00001-5-2 loss: 0.463121  [   96/  265]
train() client id: f_00001-5-3 loss: 0.486110  [  128/  265]
train() client id: f_00001-5-4 loss: 0.580852  [  160/  265]
train() client id: f_00001-5-5 loss: 0.600535  [  192/  265]
train() client id: f_00001-5-6 loss: 0.448905  [  224/  265]
train() client id: f_00001-5-7 loss: 0.365507  [  256/  265]
train() client id: f_00001-6-0 loss: 0.361920  [   32/  265]
train() client id: f_00001-6-1 loss: 0.445726  [   64/  265]
train() client id: f_00001-6-2 loss: 0.481224  [   96/  265]
train() client id: f_00001-6-3 loss: 0.608032  [  128/  265]
train() client id: f_00001-6-4 loss: 0.473249  [  160/  265]
train() client id: f_00001-6-5 loss: 0.358451  [  192/  265]
train() client id: f_00001-6-6 loss: 0.444709  [  224/  265]
train() client id: f_00001-6-7 loss: 0.483866  [  256/  265]
train() client id: f_00001-7-0 loss: 0.449542  [   32/  265]
train() client id: f_00001-7-1 loss: 0.499972  [   64/  265]
train() client id: f_00001-7-2 loss: 0.390476  [   96/  265]
train() client id: f_00001-7-3 loss: 0.587828  [  128/  265]
train() client id: f_00001-7-4 loss: 0.499865  [  160/  265]
train() client id: f_00001-7-5 loss: 0.464891  [  192/  265]
train() client id: f_00001-7-6 loss: 0.409452  [  224/  265]
train() client id: f_00001-7-7 loss: 0.410488  [  256/  265]
train() client id: f_00001-8-0 loss: 0.426848  [   32/  265]
train() client id: f_00001-8-1 loss: 0.447704  [   64/  265]
train() client id: f_00001-8-2 loss: 0.447001  [   96/  265]
train() client id: f_00001-8-3 loss: 0.485950  [  128/  265]
train() client id: f_00001-8-4 loss: 0.536653  [  160/  265]
train() client id: f_00001-8-5 loss: 0.540076  [  192/  265]
train() client id: f_00001-8-6 loss: 0.376302  [  224/  265]
train() client id: f_00001-8-7 loss: 0.397732  [  256/  265]
train() client id: f_00001-9-0 loss: 0.434377  [   32/  265]
train() client id: f_00001-9-1 loss: 0.468072  [   64/  265]
train() client id: f_00001-9-2 loss: 0.399678  [   96/  265]
train() client id: f_00001-9-3 loss: 0.435995  [  128/  265]
train() client id: f_00001-9-4 loss: 0.483364  [  160/  265]
train() client id: f_00001-9-5 loss: 0.592370  [  192/  265]
train() client id: f_00001-9-6 loss: 0.372888  [  224/  265]
train() client id: f_00001-9-7 loss: 0.519484  [  256/  265]
train() client id: f_00001-10-0 loss: 0.412602  [   32/  265]
train() client id: f_00001-10-1 loss: 0.641447  [   64/  265]
train() client id: f_00001-10-2 loss: 0.473749  [   96/  265]
train() client id: f_00001-10-3 loss: 0.380670  [  128/  265]
train() client id: f_00001-10-4 loss: 0.432676  [  160/  265]
train() client id: f_00001-10-5 loss: 0.514610  [  192/  265]
train() client id: f_00001-10-6 loss: 0.386524  [  224/  265]
train() client id: f_00001-10-7 loss: 0.469282  [  256/  265]
train() client id: f_00002-0-0 loss: 1.182454  [   32/  124]
train() client id: f_00002-0-1 loss: 1.089751  [   64/  124]
train() client id: f_00002-0-2 loss: 1.192732  [   96/  124]
train() client id: f_00002-1-0 loss: 1.138319  [   32/  124]
train() client id: f_00002-1-1 loss: 1.009306  [   64/  124]
train() client id: f_00002-1-2 loss: 1.154898  [   96/  124]
train() client id: f_00002-2-0 loss: 0.917544  [   32/  124]
train() client id: f_00002-2-1 loss: 1.077517  [   64/  124]
train() client id: f_00002-2-2 loss: 1.231954  [   96/  124]
train() client id: f_00002-3-0 loss: 1.030984  [   32/  124]
train() client id: f_00002-3-1 loss: 1.108525  [   64/  124]
train() client id: f_00002-3-2 loss: 1.042180  [   96/  124]
train() client id: f_00002-4-0 loss: 1.018645  [   32/  124]
train() client id: f_00002-4-1 loss: 1.098064  [   64/  124]
train() client id: f_00002-4-2 loss: 1.069762  [   96/  124]
train() client id: f_00002-5-0 loss: 0.956734  [   32/  124]
train() client id: f_00002-5-1 loss: 1.025644  [   64/  124]
train() client id: f_00002-5-2 loss: 1.109758  [   96/  124]
train() client id: f_00002-6-0 loss: 0.770163  [   32/  124]
train() client id: f_00002-6-1 loss: 1.208162  [   64/  124]
train() client id: f_00002-6-2 loss: 0.992203  [   96/  124]
train() client id: f_00002-7-0 loss: 0.927478  [   32/  124]
train() client id: f_00002-7-1 loss: 1.126925  [   64/  124]
train() client id: f_00002-7-2 loss: 1.052033  [   96/  124]
train() client id: f_00002-8-0 loss: 1.016160  [   32/  124]
train() client id: f_00002-8-1 loss: 0.909170  [   64/  124]
train() client id: f_00002-8-2 loss: 0.896391  [   96/  124]
train() client id: f_00002-9-0 loss: 0.986186  [   32/  124]
train() client id: f_00002-9-1 loss: 1.028381  [   64/  124]
train() client id: f_00002-9-2 loss: 0.913430  [   96/  124]
train() client id: f_00002-10-0 loss: 1.135483  [   32/  124]
train() client id: f_00002-10-1 loss: 1.023931  [   64/  124]
train() client id: f_00002-10-2 loss: 0.942074  [   96/  124]
train() client id: f_00003-0-0 loss: 0.804773  [   32/   43]
train() client id: f_00003-1-0 loss: 0.863895  [   32/   43]
train() client id: f_00003-2-0 loss: 0.739321  [   32/   43]
train() client id: f_00003-3-0 loss: 0.786345  [   32/   43]
train() client id: f_00003-4-0 loss: 0.760909  [   32/   43]
train() client id: f_00003-5-0 loss: 0.663022  [   32/   43]
train() client id: f_00003-6-0 loss: 0.560967  [   32/   43]
train() client id: f_00003-7-0 loss: 0.809610  [   32/   43]
train() client id: f_00003-8-0 loss: 0.565793  [   32/   43]
train() client id: f_00003-9-0 loss: 0.556804  [   32/   43]
train() client id: f_00003-10-0 loss: 0.748294  [   32/   43]
train() client id: f_00004-0-0 loss: 0.830618  [   32/  306]
train() client id: f_00004-0-1 loss: 0.716334  [   64/  306]
train() client id: f_00004-0-2 loss: 0.862418  [   96/  306]
train() client id: f_00004-0-3 loss: 0.924267  [  128/  306]
train() client id: f_00004-0-4 loss: 0.898550  [  160/  306]
train() client id: f_00004-0-5 loss: 0.811822  [  192/  306]
train() client id: f_00004-0-6 loss: 0.838943  [  224/  306]
train() client id: f_00004-0-7 loss: 0.915322  [  256/  306]
train() client id: f_00004-0-8 loss: 0.853016  [  288/  306]
train() client id: f_00004-1-0 loss: 0.931598  [   32/  306]
train() client id: f_00004-1-1 loss: 0.702046  [   64/  306]
train() client id: f_00004-1-2 loss: 0.733125  [   96/  306]
train() client id: f_00004-1-3 loss: 0.707242  [  128/  306]
train() client id: f_00004-1-4 loss: 0.898029  [  160/  306]
train() client id: f_00004-1-5 loss: 0.904165  [  192/  306]
train() client id: f_00004-1-6 loss: 0.869224  [  224/  306]
train() client id: f_00004-1-7 loss: 0.978556  [  256/  306]
train() client id: f_00004-1-8 loss: 0.836971  [  288/  306]
train() client id: f_00004-2-0 loss: 0.816580  [   32/  306]
train() client id: f_00004-2-1 loss: 0.681009  [   64/  306]
train() client id: f_00004-2-2 loss: 0.702071  [   96/  306]
train() client id: f_00004-2-3 loss: 0.843288  [  128/  306]
train() client id: f_00004-2-4 loss: 0.859972  [  160/  306]
train() client id: f_00004-2-5 loss: 0.934042  [  192/  306]
train() client id: f_00004-2-6 loss: 0.967813  [  224/  306]
train() client id: f_00004-2-7 loss: 0.993062  [  256/  306]
train() client id: f_00004-2-8 loss: 0.767543  [  288/  306]
train() client id: f_00004-3-0 loss: 0.860919  [   32/  306]
train() client id: f_00004-3-1 loss: 0.846880  [   64/  306]
train() client id: f_00004-3-2 loss: 0.894010  [   96/  306]
train() client id: f_00004-3-3 loss: 0.862340  [  128/  306]
train() client id: f_00004-3-4 loss: 0.931829  [  160/  306]
train() client id: f_00004-3-5 loss: 0.825845  [  192/  306]
train() client id: f_00004-3-6 loss: 0.813987  [  224/  306]
train() client id: f_00004-3-7 loss: 0.795290  [  256/  306]
train() client id: f_00004-3-8 loss: 0.669137  [  288/  306]
train() client id: f_00004-4-0 loss: 0.888568  [   32/  306]
train() client id: f_00004-4-1 loss: 0.815062  [   64/  306]
train() client id: f_00004-4-2 loss: 0.810354  [   96/  306]
train() client id: f_00004-4-3 loss: 0.739788  [  128/  306]
train() client id: f_00004-4-4 loss: 0.687778  [  160/  306]
train() client id: f_00004-4-5 loss: 0.879952  [  192/  306]
train() client id: f_00004-4-6 loss: 0.928579  [  224/  306]
train() client id: f_00004-4-7 loss: 0.940182  [  256/  306]
train() client id: f_00004-4-8 loss: 0.808442  [  288/  306]
train() client id: f_00004-5-0 loss: 0.783007  [   32/  306]
train() client id: f_00004-5-1 loss: 0.951855  [   64/  306]
train() client id: f_00004-5-2 loss: 0.815802  [   96/  306]
train() client id: f_00004-5-3 loss: 0.875788  [  128/  306]
train() client id: f_00004-5-4 loss: 0.822811  [  160/  306]
train() client id: f_00004-5-5 loss: 0.758277  [  192/  306]
train() client id: f_00004-5-6 loss: 0.804839  [  224/  306]
train() client id: f_00004-5-7 loss: 0.823674  [  256/  306]
train() client id: f_00004-5-8 loss: 0.902250  [  288/  306]
train() client id: f_00004-6-0 loss: 0.828477  [   32/  306]
train() client id: f_00004-6-1 loss: 0.759721  [   64/  306]
train() client id: f_00004-6-2 loss: 0.918482  [   96/  306]
train() client id: f_00004-6-3 loss: 0.822562  [  128/  306]
train() client id: f_00004-6-4 loss: 0.747104  [  160/  306]
train() client id: f_00004-6-5 loss: 0.886594  [  192/  306]
train() client id: f_00004-6-6 loss: 0.897280  [  224/  306]
train() client id: f_00004-6-7 loss: 0.874895  [  256/  306]
train() client id: f_00004-6-8 loss: 0.849553  [  288/  306]
train() client id: f_00004-7-0 loss: 0.760261  [   32/  306]
train() client id: f_00004-7-1 loss: 0.768532  [   64/  306]
train() client id: f_00004-7-2 loss: 0.890595  [   96/  306]
train() client id: f_00004-7-3 loss: 0.867520  [  128/  306]
train() client id: f_00004-7-4 loss: 0.920977  [  160/  306]
train() client id: f_00004-7-5 loss: 0.867467  [  192/  306]
train() client id: f_00004-7-6 loss: 0.754417  [  224/  306]
train() client id: f_00004-7-7 loss: 0.858416  [  256/  306]
train() client id: f_00004-7-8 loss: 0.854495  [  288/  306]
train() client id: f_00004-8-0 loss: 0.790247  [   32/  306]
train() client id: f_00004-8-1 loss: 0.847253  [   64/  306]
train() client id: f_00004-8-2 loss: 0.873689  [   96/  306]
train() client id: f_00004-8-3 loss: 0.820122  [  128/  306]
train() client id: f_00004-8-4 loss: 0.853030  [  160/  306]
train() client id: f_00004-8-5 loss: 0.950641  [  192/  306]
train() client id: f_00004-8-6 loss: 0.830989  [  224/  306]
train() client id: f_00004-8-7 loss: 0.750148  [  256/  306]
train() client id: f_00004-8-8 loss: 0.834661  [  288/  306]
train() client id: f_00004-9-0 loss: 0.800219  [   32/  306]
train() client id: f_00004-9-1 loss: 0.848671  [   64/  306]
train() client id: f_00004-9-2 loss: 0.834062  [   96/  306]
train() client id: f_00004-9-3 loss: 0.771169  [  128/  306]
train() client id: f_00004-9-4 loss: 0.837021  [  160/  306]
train() client id: f_00004-9-5 loss: 0.747636  [  192/  306]
train() client id: f_00004-9-6 loss: 0.860272  [  224/  306]
train() client id: f_00004-9-7 loss: 0.902754  [  256/  306]
train() client id: f_00004-9-8 loss: 0.917951  [  288/  306]
train() client id: f_00004-10-0 loss: 0.846370  [   32/  306]
train() client id: f_00004-10-1 loss: 0.839010  [   64/  306]
train() client id: f_00004-10-2 loss: 0.932777  [   96/  306]
train() client id: f_00004-10-3 loss: 0.754671  [  128/  306]
train() client id: f_00004-10-4 loss: 0.911195  [  160/  306]
train() client id: f_00004-10-5 loss: 0.964720  [  192/  306]
train() client id: f_00004-10-6 loss: 0.674226  [  224/  306]
train() client id: f_00004-10-7 loss: 0.856996  [  256/  306]
train() client id: f_00004-10-8 loss: 0.747648  [  288/  306]
train() client id: f_00005-0-0 loss: 0.735386  [   32/  146]
train() client id: f_00005-0-1 loss: 0.466399  [   64/  146]
train() client id: f_00005-0-2 loss: 0.786851  [   96/  146]
train() client id: f_00005-0-3 loss: 0.599408  [  128/  146]
train() client id: f_00005-1-0 loss: 0.990951  [   32/  146]
train() client id: f_00005-1-1 loss: 0.559506  [   64/  146]
train() client id: f_00005-1-2 loss: 0.771825  [   96/  146]
train() client id: f_00005-1-3 loss: 0.574410  [  128/  146]
train() client id: f_00005-2-0 loss: 0.561496  [   32/  146]
train() client id: f_00005-2-1 loss: 0.710861  [   64/  146]
train() client id: f_00005-2-2 loss: 0.669780  [   96/  146]
train() client id: f_00005-2-3 loss: 0.716091  [  128/  146]
train() client id: f_00005-3-0 loss: 0.548635  [   32/  146]
train() client id: f_00005-3-1 loss: 0.482185  [   64/  146]
train() client id: f_00005-3-2 loss: 0.768745  [   96/  146]
train() client id: f_00005-3-3 loss: 0.797753  [  128/  146]
train() client id: f_00005-4-0 loss: 0.859744  [   32/  146]
train() client id: f_00005-4-1 loss: 0.923466  [   64/  146]
train() client id: f_00005-4-2 loss: 0.637854  [   96/  146]
train() client id: f_00005-4-3 loss: 0.465711  [  128/  146]
train() client id: f_00005-5-0 loss: 0.682409  [   32/  146]
train() client id: f_00005-5-1 loss: 0.678596  [   64/  146]
train() client id: f_00005-5-2 loss: 0.804747  [   96/  146]
train() client id: f_00005-5-3 loss: 0.683265  [  128/  146]
train() client id: f_00005-6-0 loss: 0.657781  [   32/  146]
train() client id: f_00005-6-1 loss: 0.586907  [   64/  146]
train() client id: f_00005-6-2 loss: 0.714044  [   96/  146]
train() client id: f_00005-6-3 loss: 0.702824  [  128/  146]
train() client id: f_00005-7-0 loss: 0.805022  [   32/  146]
train() client id: f_00005-7-1 loss: 0.886669  [   64/  146]
train() client id: f_00005-7-2 loss: 0.515155  [   96/  146]
train() client id: f_00005-7-3 loss: 0.659001  [  128/  146]
train() client id: f_00005-8-0 loss: 0.764114  [   32/  146]
train() client id: f_00005-8-1 loss: 0.467735  [   64/  146]
train() client id: f_00005-8-2 loss: 0.581265  [   96/  146]
train() client id: f_00005-8-3 loss: 0.824719  [  128/  146]
train() client id: f_00005-9-0 loss: 0.838590  [   32/  146]
train() client id: f_00005-9-1 loss: 0.667833  [   64/  146]
train() client id: f_00005-9-2 loss: 0.583561  [   96/  146]
train() client id: f_00005-9-3 loss: 0.714097  [  128/  146]
train() client id: f_00005-10-0 loss: 0.673460  [   32/  146]
train() client id: f_00005-10-1 loss: 1.020298  [   64/  146]
train() client id: f_00005-10-2 loss: 0.605706  [   96/  146]
train() client id: f_00005-10-3 loss: 0.589217  [  128/  146]
train() client id: f_00006-0-0 loss: 0.459145  [   32/   54]
train() client id: f_00006-1-0 loss: 0.409797  [   32/   54]
train() client id: f_00006-2-0 loss: 0.476176  [   32/   54]
train() client id: f_00006-3-0 loss: 0.449477  [   32/   54]
train() client id: f_00006-4-0 loss: 0.387761  [   32/   54]
train() client id: f_00006-5-0 loss: 0.403061  [   32/   54]
train() client id: f_00006-6-0 loss: 0.504148  [   32/   54]
train() client id: f_00006-7-0 loss: 0.507902  [   32/   54]
train() client id: f_00006-8-0 loss: 0.410363  [   32/   54]
train() client id: f_00006-9-0 loss: 0.452654  [   32/   54]
train() client id: f_00006-10-0 loss: 0.459425  [   32/   54]
train() client id: f_00007-0-0 loss: 0.674875  [   32/  179]
train() client id: f_00007-0-1 loss: 0.631039  [   64/  179]
train() client id: f_00007-0-2 loss: 0.669512  [   96/  179]
train() client id: f_00007-0-3 loss: 0.528392  [  128/  179]
train() client id: f_00007-0-4 loss: 0.818765  [  160/  179]
train() client id: f_00007-1-0 loss: 0.582504  [   32/  179]
train() client id: f_00007-1-1 loss: 0.745466  [   64/  179]
train() client id: f_00007-1-2 loss: 0.707328  [   96/  179]
train() client id: f_00007-1-3 loss: 0.654953  [  128/  179]
train() client id: f_00007-1-4 loss: 0.480311  [  160/  179]
train() client id: f_00007-2-0 loss: 0.496947  [   32/  179]
train() client id: f_00007-2-1 loss: 0.673249  [   64/  179]
train() client id: f_00007-2-2 loss: 0.725145  [   96/  179]
train() client id: f_00007-2-3 loss: 0.598146  [  128/  179]
train() client id: f_00007-2-4 loss: 0.755249  [  160/  179]
train() client id: f_00007-3-0 loss: 0.676173  [   32/  179]
train() client id: f_00007-3-1 loss: 0.627449  [   64/  179]
train() client id: f_00007-3-2 loss: 0.625921  [   96/  179]
train() client id: f_00007-3-3 loss: 0.587291  [  128/  179]
train() client id: f_00007-3-4 loss: 0.669079  [  160/  179]
train() client id: f_00007-4-0 loss: 0.660636  [   32/  179]
train() client id: f_00007-4-1 loss: 0.555403  [   64/  179]
train() client id: f_00007-4-2 loss: 0.467073  [   96/  179]
train() client id: f_00007-4-3 loss: 0.841002  [  128/  179]
train() client id: f_00007-4-4 loss: 0.556780  [  160/  179]
train() client id: f_00007-5-0 loss: 0.655415  [   32/  179]
train() client id: f_00007-5-1 loss: 0.687574  [   64/  179]
train() client id: f_00007-5-2 loss: 0.730705  [   96/  179]
train() client id: f_00007-5-3 loss: 0.543383  [  128/  179]
train() client id: f_00007-5-4 loss: 0.539625  [  160/  179]
train() client id: f_00007-6-0 loss: 0.550996  [   32/  179]
train() client id: f_00007-6-1 loss: 0.508678  [   64/  179]
train() client id: f_00007-6-2 loss: 0.565299  [   96/  179]
train() client id: f_00007-6-3 loss: 0.474029  [  128/  179]
train() client id: f_00007-6-4 loss: 0.581654  [  160/  179]
train() client id: f_00007-7-0 loss: 0.785171  [   32/  179]
train() client id: f_00007-7-1 loss: 0.497956  [   64/  179]
train() client id: f_00007-7-2 loss: 0.670155  [   96/  179]
train() client id: f_00007-7-3 loss: 0.463240  [  128/  179]
train() client id: f_00007-7-4 loss: 0.699782  [  160/  179]
train() client id: f_00007-8-0 loss: 0.649941  [   32/  179]
train() client id: f_00007-8-1 loss: 0.528305  [   64/  179]
train() client id: f_00007-8-2 loss: 0.800472  [   96/  179]
train() client id: f_00007-8-3 loss: 0.477051  [  128/  179]
train() client id: f_00007-8-4 loss: 0.553933  [  160/  179]
train() client id: f_00007-9-0 loss: 0.662545  [   32/  179]
train() client id: f_00007-9-1 loss: 0.552341  [   64/  179]
train() client id: f_00007-9-2 loss: 0.606341  [   96/  179]
train() client id: f_00007-9-3 loss: 0.489646  [  128/  179]
train() client id: f_00007-9-4 loss: 0.697680  [  160/  179]
train() client id: f_00007-10-0 loss: 0.470442  [   32/  179]
train() client id: f_00007-10-1 loss: 0.449926  [   64/  179]
train() client id: f_00007-10-2 loss: 0.769483  [   96/  179]
train() client id: f_00007-10-3 loss: 0.698947  [  128/  179]
train() client id: f_00007-10-4 loss: 0.610246  [  160/  179]
train() client id: f_00008-0-0 loss: 0.745261  [   32/  130]
train() client id: f_00008-0-1 loss: 0.807088  [   64/  130]
train() client id: f_00008-0-2 loss: 0.716627  [   96/  130]
train() client id: f_00008-0-3 loss: 0.898883  [  128/  130]
train() client id: f_00008-1-0 loss: 0.820018  [   32/  130]
train() client id: f_00008-1-1 loss: 0.731006  [   64/  130]
train() client id: f_00008-1-2 loss: 0.749488  [   96/  130]
train() client id: f_00008-1-3 loss: 0.890352  [  128/  130]
train() client id: f_00008-2-0 loss: 0.762672  [   32/  130]
train() client id: f_00008-2-1 loss: 0.884807  [   64/  130]
train() client id: f_00008-2-2 loss: 0.800775  [   96/  130]
train() client id: f_00008-2-3 loss: 0.736085  [  128/  130]
train() client id: f_00008-3-0 loss: 0.841405  [   32/  130]
train() client id: f_00008-3-1 loss: 0.667332  [   64/  130]
train() client id: f_00008-3-2 loss: 0.773028  [   96/  130]
train() client id: f_00008-3-3 loss: 0.899732  [  128/  130]
train() client id: f_00008-4-0 loss: 0.713668  [   32/  130]
train() client id: f_00008-4-1 loss: 0.785666  [   64/  130]
train() client id: f_00008-4-2 loss: 0.800743  [   96/  130]
train() client id: f_00008-4-3 loss: 0.876196  [  128/  130]
train() client id: f_00008-5-0 loss: 0.825813  [   32/  130]
train() client id: f_00008-5-1 loss: 0.745268  [   64/  130]
train() client id: f_00008-5-2 loss: 0.851018  [   96/  130]
train() client id: f_00008-5-3 loss: 0.764325  [  128/  130]
train() client id: f_00008-6-0 loss: 0.861479  [   32/  130]
train() client id: f_00008-6-1 loss: 0.827470  [   64/  130]
train() client id: f_00008-6-2 loss: 0.759378  [   96/  130]
train() client id: f_00008-6-3 loss: 0.727172  [  128/  130]
train() client id: f_00008-7-0 loss: 0.876322  [   32/  130]
train() client id: f_00008-7-1 loss: 0.708122  [   64/  130]
train() client id: f_00008-7-2 loss: 0.885920  [   96/  130]
train() client id: f_00008-7-3 loss: 0.699593  [  128/  130]
train() client id: f_00008-8-0 loss: 0.845507  [   32/  130]
train() client id: f_00008-8-1 loss: 0.736905  [   64/  130]
train() client id: f_00008-8-2 loss: 0.799400  [   96/  130]
train() client id: f_00008-8-3 loss: 0.787602  [  128/  130]
train() client id: f_00008-9-0 loss: 0.751635  [   32/  130]
train() client id: f_00008-9-1 loss: 0.798846  [   64/  130]
train() client id: f_00008-9-2 loss: 0.817644  [   96/  130]
train() client id: f_00008-9-3 loss: 0.799565  [  128/  130]
train() client id: f_00008-10-0 loss: 0.728485  [   32/  130]
train() client id: f_00008-10-1 loss: 0.747702  [   64/  130]
train() client id: f_00008-10-2 loss: 0.903603  [   96/  130]
train() client id: f_00008-10-3 loss: 0.774740  [  128/  130]
train() client id: f_00009-0-0 loss: 1.166708  [   32/  118]
train() client id: f_00009-0-1 loss: 1.002394  [   64/  118]
train() client id: f_00009-0-2 loss: 0.899559  [   96/  118]
train() client id: f_00009-1-0 loss: 0.989299  [   32/  118]
train() client id: f_00009-1-1 loss: 0.988166  [   64/  118]
train() client id: f_00009-1-2 loss: 0.957229  [   96/  118]
train() client id: f_00009-2-0 loss: 0.892896  [   32/  118]
train() client id: f_00009-2-1 loss: 1.032632  [   64/  118]
train() client id: f_00009-2-2 loss: 0.768119  [   96/  118]
train() client id: f_00009-3-0 loss: 0.915801  [   32/  118]
train() client id: f_00009-3-1 loss: 0.798952  [   64/  118]
train() client id: f_00009-3-2 loss: 0.849908  [   96/  118]
train() client id: f_00009-4-0 loss: 0.903774  [   32/  118]
train() client id: f_00009-4-1 loss: 0.871668  [   64/  118]
train() client id: f_00009-4-2 loss: 0.829582  [   96/  118]
train() client id: f_00009-5-0 loss: 0.977257  [   32/  118]
train() client id: f_00009-5-1 loss: 0.886160  [   64/  118]
train() client id: f_00009-5-2 loss: 0.867382  [   96/  118]
train() client id: f_00009-6-0 loss: 0.902267  [   32/  118]
train() client id: f_00009-6-1 loss: 0.862449  [   64/  118]
train() client id: f_00009-6-2 loss: 0.839053  [   96/  118]
train() client id: f_00009-7-0 loss: 0.905229  [   32/  118]
train() client id: f_00009-7-1 loss: 0.768737  [   64/  118]
train() client id: f_00009-7-2 loss: 0.719388  [   96/  118]
train() client id: f_00009-8-0 loss: 0.925620  [   32/  118]
train() client id: f_00009-8-1 loss: 0.771190  [   64/  118]
train() client id: f_00009-8-2 loss: 0.696510  [   96/  118]
train() client id: f_00009-9-0 loss: 0.721830  [   32/  118]
train() client id: f_00009-9-1 loss: 0.761285  [   64/  118]
train() client id: f_00009-9-2 loss: 0.970034  [   96/  118]
train() client id: f_00009-10-0 loss: 0.929941  [   32/  118]
train() client id: f_00009-10-1 loss: 0.788611  [   64/  118]
train() client id: f_00009-10-2 loss: 0.702182  [   96/  118]
At round 53 accuracy: 0.6472148541114059
At round 53 training accuracy: 0.5888665325285044
At round 53 training loss: 0.8168635304994746
update_location
xs = [  -3.9056584     4.20031788  285.00902392   18.81129433    0.97929623
    3.95640986 -247.44319194 -226.32485185  269.66397685 -212.06087855]
ys = [ 277.5879595   260.55583871    1.32061395 -247.45517586  239.35018685
  222.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [295.07681953 279.11823257 302.04616822 267.55920626 259.4021414
 244.2576487  266.89890179 247.43406156 288.14463449 234.49057138]
dists_bs = [199.49326595 198.57632151 491.70400773 464.79155911 187.3757168
 185.23387822 191.82974512 181.58219868 471.71026083 174.96857419]
uav_gains = [2.94167056e-12 4.07370156e-12 2.57043737e-12 5.18972845e-12
 6.14485954e-12 8.28695143e-12 5.26179225e-12 7.79933417e-12
 3.38079010e-12 9.90488682e-12]
bs_gains = [4.01323889e-11 4.06534300e-11 3.21013717e-12 3.75812338e-12
 4.78296084e-11 4.93943086e-11 4.47846749e-11 5.22262602e-11
 3.60581283e-12 5.79436618e-11]
Round 54
-------------------------------
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.87808648 7.89231983 3.82850312 1.39788506 9.0995415  4.37831172
 1.72181282 5.39414421 3.98347864 3.54993313]
obj_prev = 45.124016510283404
eta_min = 1.5688388831448793e-24	eta_max = 0.9379204733020944
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 10.402835079222914	eta = 0.9090909090909091
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 22.247241049856704	eta = 0.4250919373822522
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 16.022057938092566	eta = 0.5902564349620266
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 14.904186500929747	eta = 0.6345279427832983
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 14.838111741409746	eta = 0.6373535234204303
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 14.83785471002873	eta = 0.6373645640903601
eta = 0.6373645640903601
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [0.03716704 0.07816878 0.03657709 0.01268399 0.09026281 0.04306657
 0.01592873 0.0528008  0.03834695 0.03480723]
ene_total = [1.43077897 2.28852743 1.44075621 0.69142509 2.60477925 1.34299583
 0.77637967 1.71811452 1.42782004 1.11627769]
ti_comp = [0.85812284 0.94697766 0.84848758 0.89012496 0.94950542 0.94998635
 0.89076325 0.90687475 0.8710298  0.95228188]
ti_coms = [0.16404911 0.07519428 0.17368437 0.13204699 0.07266652 0.0721856
 0.13140869 0.11529719 0.15114215 0.06989007]
t_total = [27.29977341 27.29977341 27.29977341 27.29977341 27.29977341 27.29977341
 27.29977341 27.29977341 27.29977341 27.29977341]
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [4.35767693e-06 3.32889784e-05 4.24831188e-06 1.60970289e-07
 5.09813926e-05 5.53179598e-06 3.18345717e-07 1.11868234e-05
 4.64522066e-06 2.90641279e-06]
ene_total = [0.4382524  0.20171506 0.46398296 0.35267029 0.19543653 0.19293825
 0.35096977 0.30823006 0.40378868 0.18673731]
optimize_network iter = 0 obj = 3.094721297443309
eta = 0.6373645640903601
freqs = [21656013.90690634 41272766.08711902 21554285.58755618  7124836.67893045
 47531488.32308537 22666941.75621849  8941057.85172046 29111404.29436724
 22012419.35744325 18275695.9412735 ]
eta_min = 0.6373645640903616	eta_max = 0.700248974656232
af = 0.0025292806273427074	bf = 1.1211163846456331	zeta = 0.002782208690076978	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [9.22110454e-07 7.04414656e-06 8.98968156e-07 3.40622741e-08
 1.07879670e-05 1.17056105e-06 6.73638541e-08 2.36719862e-06
 9.82956424e-07 6.15014298e-07]
ene_total = [1.74781835 0.80184327 1.85046646 1.40678466 0.77531226 0.76916407
 1.39998808 1.22858694 1.61031875 0.74464912]
ti_comp = [0.68086857 0.76972339 0.67123331 0.71287069 0.77225115 0.77273208
 0.71350898 0.72962048 0.69377552 0.77502761]
ti_coms = [0.16404911 0.07519428 0.17368437 0.13204699 0.07266652 0.0721856
 0.13140869 0.11529719 0.15114215 0.06989007]
t_total = [27.29977341 27.29977341 27.29977341 27.29977341 27.29977341 27.29977341
 27.29977341 27.29977341 27.29977341 27.29977341]
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [3.42697873e-06 2.49456369e-05 3.36081341e-06 1.24253947e-07
 3.81569359e-05 4.13930121e-06 2.45645457e-07 8.55640634e-06
 3.62508715e-06 2.17239112e-06]
ene_total = [0.53016278 0.24376304 0.56129271 0.42665538 0.23602256 0.23336954
 0.42459695 0.37280828 0.48846606 0.22588899]
optimize_network iter = 1 obj = 3.7430263059334705
eta = 0.700248974656232
freqs = [21591991.81873228 40169519.43771622 21554285.58755618  7037900.85535273
 46232598.93038776 22044962.3818897   8830387.52189105 28624734.01565526
 21863003.51871486 17764389.24875431]
eta_min = 0.7002489746562424	eta_max = 0.7002489746562317
af = 0.0024124103758265034	bf = 1.1211163846456331	zeta = 0.002653651413409154	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [9.16666407e-07 6.67259097e-06 8.98968156e-07 3.32361034e-08
 1.02064191e-05 1.10720219e-06 6.57065467e-08 2.28871285e-06
 9.69657495e-07 5.81082673e-07]
ene_total = [1.74781777 0.80180369 1.85046646 1.40678457 0.7752503  0.76915732
 1.3999879  1.22857858 1.61031734 0.7446455 ]
ti_comp = [0.68086857 0.76972339 0.67123331 0.71287069 0.77225115 0.77273208
 0.71350898 0.72962048 0.69377552 0.77502761]
ti_coms = [0.16404911 0.07519428 0.17368437 0.13204699 0.07266652 0.0721856
 0.13140869 0.11529719 0.15114215 0.06989007]
t_total = [27.29977341 27.29977341 27.29977341 27.29977341 27.29977341 27.29977341
 27.29977341 27.29977341 27.29977341 27.29977341]
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [3.42697873e-06 2.49456369e-05 3.36081341e-06 1.24253947e-07
 3.81569359e-05 4.13930121e-06 2.45645457e-07 8.55640634e-06
 3.62508715e-06 2.17239112e-06]
ene_total = [0.53016278 0.24376304 0.56129271 0.42665538 0.23602256 0.23336954
 0.42459695 0.37280828 0.48846606 0.22588899]
optimize_network iter = 2 obj = 3.743026305933466
eta = 0.7002489746562317
freqs = [21591991.81873227 40169519.4377162  21554285.58755618  7037900.85535273
 46232598.93038776 22044962.38188971  8830387.52189105 28624734.01565526
 21863003.51871486 17764389.24875431]
Done!
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [3.23086379e-06 2.35180785e-05 3.16848489e-06 1.17143295e-07
 3.59733374e-05 3.90242234e-06 2.31587959e-07 8.06675079e-06
 3.41763510e-06 2.04807218e-06]
ene_total = [0.01640814 0.00754295 0.01737161 0.01320482 0.00730263 0.00722246
 0.0131411  0.01153779 0.01511763 0.00699105]
At round 54 energy consumption: 0.11584017098855656
At round 54 eta: 0.7002489746562317
At round 54 a_n: 9.342581272941924
At round 54 local rounds: 11.66770515392171
At round 54 global rounds: 32.310572111651304
gradient difference: 0.464343786239624
train() client id: f_00000-0-0 loss: 1.022466  [   32/  126]
train() client id: f_00000-0-1 loss: 1.263595  [   64/  126]
train() client id: f_00000-0-2 loss: 1.172585  [   96/  126]
train() client id: f_00000-1-0 loss: 0.996519  [   32/  126]
train() client id: f_00000-1-1 loss: 0.934964  [   64/  126]
train() client id: f_00000-1-2 loss: 0.993763  [   96/  126]
train() client id: f_00000-2-0 loss: 0.979042  [   32/  126]
train() client id: f_00000-2-1 loss: 1.062480  [   64/  126]
train() client id: f_00000-2-2 loss: 0.946119  [   96/  126]
train() client id: f_00000-3-0 loss: 0.838728  [   32/  126]
train() client id: f_00000-3-1 loss: 1.105652  [   64/  126]
train() client id: f_00000-3-2 loss: 0.858884  [   96/  126]
train() client id: f_00000-4-0 loss: 0.839296  [   32/  126]
train() client id: f_00000-4-1 loss: 0.899094  [   64/  126]
train() client id: f_00000-4-2 loss: 0.838955  [   96/  126]
train() client id: f_00000-5-0 loss: 0.856898  [   32/  126]
train() client id: f_00000-5-1 loss: 0.833180  [   64/  126]
train() client id: f_00000-5-2 loss: 0.835492  [   96/  126]
train() client id: f_00000-6-0 loss: 0.865440  [   32/  126]
train() client id: f_00000-6-1 loss: 0.863923  [   64/  126]
train() client id: f_00000-6-2 loss: 0.749213  [   96/  126]
train() client id: f_00000-7-0 loss: 0.850532  [   32/  126]
train() client id: f_00000-7-1 loss: 0.787114  [   64/  126]
train() client id: f_00000-7-2 loss: 0.690244  [   96/  126]
train() client id: f_00000-8-0 loss: 0.755493  [   32/  126]
train() client id: f_00000-8-1 loss: 0.738910  [   64/  126]
train() client id: f_00000-8-2 loss: 0.772814  [   96/  126]
train() client id: f_00000-9-0 loss: 0.797269  [   32/  126]
train() client id: f_00000-9-1 loss: 0.714704  [   64/  126]
train() client id: f_00000-9-2 loss: 0.826374  [   96/  126]
train() client id: f_00000-10-0 loss: 0.981454  [   32/  126]
train() client id: f_00000-10-1 loss: 0.711345  [   64/  126]
train() client id: f_00000-10-2 loss: 0.717232  [   96/  126]
train() client id: f_00001-0-0 loss: 0.348031  [   32/  265]
train() client id: f_00001-0-1 loss: 0.424047  [   64/  265]
train() client id: f_00001-0-2 loss: 0.518782  [   96/  265]
train() client id: f_00001-0-3 loss: 0.508393  [  128/  265]
train() client id: f_00001-0-4 loss: 0.485082  [  160/  265]
train() client id: f_00001-0-5 loss: 0.434235  [  192/  265]
train() client id: f_00001-0-6 loss: 0.445467  [  224/  265]
train() client id: f_00001-0-7 loss: 0.515370  [  256/  265]
train() client id: f_00001-1-0 loss: 0.432882  [   32/  265]
train() client id: f_00001-1-1 loss: 0.561819  [   64/  265]
train() client id: f_00001-1-2 loss: 0.428636  [   96/  265]
train() client id: f_00001-1-3 loss: 0.404269  [  128/  265]
train() client id: f_00001-1-4 loss: 0.461953  [  160/  265]
train() client id: f_00001-1-5 loss: 0.559137  [  192/  265]
train() client id: f_00001-1-6 loss: 0.367714  [  224/  265]
train() client id: f_00001-1-7 loss: 0.413500  [  256/  265]
train() client id: f_00001-2-0 loss: 0.471331  [   32/  265]
train() client id: f_00001-2-1 loss: 0.366834  [   64/  265]
train() client id: f_00001-2-2 loss: 0.513524  [   96/  265]
train() client id: f_00001-2-3 loss: 0.354618  [  128/  265]
train() client id: f_00001-2-4 loss: 0.497448  [  160/  265]
train() client id: f_00001-2-5 loss: 0.446092  [  192/  265]
train() client id: f_00001-2-6 loss: 0.384592  [  224/  265]
train() client id: f_00001-2-7 loss: 0.471004  [  256/  265]
train() client id: f_00001-3-0 loss: 0.404170  [   32/  265]
train() client id: f_00001-3-1 loss: 0.527365  [   64/  265]
train() client id: f_00001-3-2 loss: 0.462633  [   96/  265]
train() client id: f_00001-3-3 loss: 0.447258  [  128/  265]
train() client id: f_00001-3-4 loss: 0.458689  [  160/  265]
train() client id: f_00001-3-5 loss: 0.357299  [  192/  265]
train() client id: f_00001-3-6 loss: 0.447966  [  224/  265]
train() client id: f_00001-3-7 loss: 0.415854  [  256/  265]
train() client id: f_00001-4-0 loss: 0.497231  [   32/  265]
train() client id: f_00001-4-1 loss: 0.337514  [   64/  265]
train() client id: f_00001-4-2 loss: 0.453289  [   96/  265]
train() client id: f_00001-4-3 loss: 0.401895  [  128/  265]
train() client id: f_00001-4-4 loss: 0.447714  [  160/  265]
train() client id: f_00001-4-5 loss: 0.496831  [  192/  265]
train() client id: f_00001-4-6 loss: 0.525513  [  224/  265]
train() client id: f_00001-4-7 loss: 0.379200  [  256/  265]
train() client id: f_00001-5-0 loss: 0.572282  [   32/  265]
train() client id: f_00001-5-1 loss: 0.387074  [   64/  265]
train() client id: f_00001-5-2 loss: 0.391796  [   96/  265]
train() client id: f_00001-5-3 loss: 0.336430  [  128/  265]
train() client id: f_00001-5-4 loss: 0.447168  [  160/  265]
train() client id: f_00001-5-5 loss: 0.650124  [  192/  265]
train() client id: f_00001-5-6 loss: 0.358001  [  224/  265]
train() client id: f_00001-5-7 loss: 0.377586  [  256/  265]
train() client id: f_00001-6-0 loss: 0.477286  [   32/  265]
train() client id: f_00001-6-1 loss: 0.372667  [   64/  265]
train() client id: f_00001-6-2 loss: 0.428087  [   96/  265]
train() client id: f_00001-6-3 loss: 0.373570  [  128/  265]
train() client id: f_00001-6-4 loss: 0.514793  [  160/  265]
train() client id: f_00001-6-5 loss: 0.429080  [  192/  265]
train() client id: f_00001-6-6 loss: 0.428364  [  224/  265]
train() client id: f_00001-6-7 loss: 0.457328  [  256/  265]
train() client id: f_00001-7-0 loss: 0.414208  [   32/  265]
train() client id: f_00001-7-1 loss: 0.395278  [   64/  265]
train() client id: f_00001-7-2 loss: 0.392683  [   96/  265]
train() client id: f_00001-7-3 loss: 0.386160  [  128/  265]
train() client id: f_00001-7-4 loss: 0.543933  [  160/  265]
train() client id: f_00001-7-5 loss: 0.392480  [  192/  265]
train() client id: f_00001-7-6 loss: 0.577900  [  224/  265]
train() client id: f_00001-7-7 loss: 0.402594  [  256/  265]
train() client id: f_00001-8-0 loss: 0.335396  [   32/  265]
train() client id: f_00001-8-1 loss: 0.671264  [   64/  265]
train() client id: f_00001-8-2 loss: 0.393728  [   96/  265]
train() client id: f_00001-8-3 loss: 0.386269  [  128/  265]
train() client id: f_00001-8-4 loss: 0.484780  [  160/  265]
train() client id: f_00001-8-5 loss: 0.574023  [  192/  265]
train() client id: f_00001-8-6 loss: 0.335183  [  224/  265]
train() client id: f_00001-8-7 loss: 0.326609  [  256/  265]
train() client id: f_00001-9-0 loss: 0.412651  [   32/  265]
train() client id: f_00001-9-1 loss: 0.510091  [   64/  265]
train() client id: f_00001-9-2 loss: 0.374593  [   96/  265]
train() client id: f_00001-9-3 loss: 0.362078  [  128/  265]
train() client id: f_00001-9-4 loss: 0.459705  [  160/  265]
train() client id: f_00001-9-5 loss: 0.443205  [  192/  265]
train() client id: f_00001-9-6 loss: 0.452472  [  224/  265]
train() client id: f_00001-9-7 loss: 0.448952  [  256/  265]
train() client id: f_00001-10-0 loss: 0.545898  [   32/  265]
train() client id: f_00001-10-1 loss: 0.431927  [   64/  265]
train() client id: f_00001-10-2 loss: 0.373685  [   96/  265]
train() client id: f_00001-10-3 loss: 0.360548  [  128/  265]
train() client id: f_00001-10-4 loss: 0.443809  [  160/  265]
train() client id: f_00001-10-5 loss: 0.557069  [  192/  265]
train() client id: f_00001-10-6 loss: 0.413000  [  224/  265]
train() client id: f_00001-10-7 loss: 0.388435  [  256/  265]
train() client id: f_00002-0-0 loss: 1.078474  [   32/  124]
train() client id: f_00002-0-1 loss: 1.213895  [   64/  124]
train() client id: f_00002-0-2 loss: 1.259459  [   96/  124]
train() client id: f_00002-1-0 loss: 1.043257  [   32/  124]
train() client id: f_00002-1-1 loss: 1.108186  [   64/  124]
train() client id: f_00002-1-2 loss: 1.266082  [   96/  124]
train() client id: f_00002-2-0 loss: 1.140977  [   32/  124]
train() client id: f_00002-2-1 loss: 1.018922  [   64/  124]
train() client id: f_00002-2-2 loss: 1.306264  [   96/  124]
train() client id: f_00002-3-0 loss: 1.204077  [   32/  124]
train() client id: f_00002-3-1 loss: 1.037938  [   64/  124]
train() client id: f_00002-3-2 loss: 1.204300  [   96/  124]
train() client id: f_00002-4-0 loss: 1.168883  [   32/  124]
train() client id: f_00002-4-1 loss: 0.992564  [   64/  124]
train() client id: f_00002-4-2 loss: 1.187865  [   96/  124]
train() client id: f_00002-5-0 loss: 1.215572  [   32/  124]
train() client id: f_00002-5-1 loss: 1.070342  [   64/  124]
train() client id: f_00002-5-2 loss: 0.946893  [   96/  124]
train() client id: f_00002-6-0 loss: 1.018689  [   32/  124]
train() client id: f_00002-6-1 loss: 1.081755  [   64/  124]
train() client id: f_00002-6-2 loss: 1.093832  [   96/  124]
train() client id: f_00002-7-0 loss: 1.013817  [   32/  124]
train() client id: f_00002-7-1 loss: 1.105353  [   64/  124]
train() client id: f_00002-7-2 loss: 1.082518  [   96/  124]
train() client id: f_00002-8-0 loss: 0.980884  [   32/  124]
train() client id: f_00002-8-1 loss: 1.120889  [   64/  124]
train() client id: f_00002-8-2 loss: 1.005821  [   96/  124]
train() client id: f_00002-9-0 loss: 1.061914  [   32/  124]
train() client id: f_00002-9-1 loss: 0.915425  [   64/  124]
train() client id: f_00002-9-2 loss: 1.130355  [   96/  124]
train() client id: f_00002-10-0 loss: 1.085695  [   32/  124]
train() client id: f_00002-10-1 loss: 0.927632  [   64/  124]
train() client id: f_00002-10-2 loss: 1.160245  [   96/  124]
train() client id: f_00003-0-0 loss: 0.630461  [   32/   43]
train() client id: f_00003-1-0 loss: 0.393628  [   32/   43]
train() client id: f_00003-2-0 loss: 0.507935  [   32/   43]
train() client id: f_00003-3-0 loss: 0.481725  [   32/   43]
train() client id: f_00003-4-0 loss: 0.671533  [   32/   43]
train() client id: f_00003-5-0 loss: 0.671350  [   32/   43]
train() client id: f_00003-6-0 loss: 0.856109  [   32/   43]
train() client id: f_00003-7-0 loss: 0.402803  [   32/   43]
train() client id: f_00003-8-0 loss: 0.679270  [   32/   43]
train() client id: f_00003-9-0 loss: 0.726113  [   32/   43]
train() client id: f_00003-10-0 loss: 0.425680  [   32/   43]
train() client id: f_00004-0-0 loss: 0.761982  [   32/  306]
train() client id: f_00004-0-1 loss: 0.919397  [   64/  306]
train() client id: f_00004-0-2 loss: 0.856302  [   96/  306]
train() client id: f_00004-0-3 loss: 0.913863  [  128/  306]
train() client id: f_00004-0-4 loss: 0.848116  [  160/  306]
train() client id: f_00004-0-5 loss: 0.980034  [  192/  306]
train() client id: f_00004-0-6 loss: 0.964597  [  224/  306]
train() client id: f_00004-0-7 loss: 1.018137  [  256/  306]
train() client id: f_00004-0-8 loss: 0.996949  [  288/  306]
train() client id: f_00004-1-0 loss: 0.961580  [   32/  306]
train() client id: f_00004-1-1 loss: 1.032896  [   64/  306]
train() client id: f_00004-1-2 loss: 0.867646  [   96/  306]
train() client id: f_00004-1-3 loss: 0.927566  [  128/  306]
train() client id: f_00004-1-4 loss: 0.863745  [  160/  306]
train() client id: f_00004-1-5 loss: 0.929987  [  192/  306]
train() client id: f_00004-1-6 loss: 1.002029  [  224/  306]
train() client id: f_00004-1-7 loss: 0.744557  [  256/  306]
train() client id: f_00004-1-8 loss: 0.895958  [  288/  306]
train() client id: f_00004-2-0 loss: 0.850440  [   32/  306]
train() client id: f_00004-2-1 loss: 1.054176  [   64/  306]
train() client id: f_00004-2-2 loss: 0.823159  [   96/  306]
train() client id: f_00004-2-3 loss: 0.951866  [  128/  306]
train() client id: f_00004-2-4 loss: 0.832076  [  160/  306]
train() client id: f_00004-2-5 loss: 0.963956  [  192/  306]
train() client id: f_00004-2-6 loss: 0.891877  [  224/  306]
train() client id: f_00004-2-7 loss: 0.886621  [  256/  306]
train() client id: f_00004-2-8 loss: 0.900754  [  288/  306]
train() client id: f_00004-3-0 loss: 0.838469  [   32/  306]
train() client id: f_00004-3-1 loss: 0.931884  [   64/  306]
train() client id: f_00004-3-2 loss: 0.809426  [   96/  306]
train() client id: f_00004-3-3 loss: 0.866454  [  128/  306]
train() client id: f_00004-3-4 loss: 0.913660  [  160/  306]
train() client id: f_00004-3-5 loss: 0.907379  [  192/  306]
train() client id: f_00004-3-6 loss: 0.952429  [  224/  306]
train() client id: f_00004-3-7 loss: 0.977778  [  256/  306]
train() client id: f_00004-3-8 loss: 0.982168  [  288/  306]
train() client id: f_00004-4-0 loss: 1.001004  [   32/  306]
train() client id: f_00004-4-1 loss: 0.837528  [   64/  306]
train() client id: f_00004-4-2 loss: 0.825117  [   96/  306]
train() client id: f_00004-4-3 loss: 0.918313  [  128/  306]
train() client id: f_00004-4-4 loss: 0.956882  [  160/  306]
train() client id: f_00004-4-5 loss: 0.845874  [  192/  306]
train() client id: f_00004-4-6 loss: 0.900580  [  224/  306]
train() client id: f_00004-4-7 loss: 0.971088  [  256/  306]
train() client id: f_00004-4-8 loss: 0.907023  [  288/  306]
train() client id: f_00004-5-0 loss: 0.763118  [   32/  306]
train() client id: f_00004-5-1 loss: 0.918510  [   64/  306]
train() client id: f_00004-5-2 loss: 1.063178  [   96/  306]
train() client id: f_00004-5-3 loss: 0.961655  [  128/  306]
train() client id: f_00004-5-4 loss: 0.931542  [  160/  306]
train() client id: f_00004-5-5 loss: 0.878533  [  192/  306]
train() client id: f_00004-5-6 loss: 0.816205  [  224/  306]
train() client id: f_00004-5-7 loss: 0.871208  [  256/  306]
train() client id: f_00004-5-8 loss: 0.920700  [  288/  306]
train() client id: f_00004-6-0 loss: 0.931033  [   32/  306]
train() client id: f_00004-6-1 loss: 0.893204  [   64/  306]
train() client id: f_00004-6-2 loss: 0.871318  [   96/  306]
train() client id: f_00004-6-3 loss: 0.861376  [  128/  306]
train() client id: f_00004-6-4 loss: 0.857213  [  160/  306]
train() client id: f_00004-6-5 loss: 0.758166  [  192/  306]
train() client id: f_00004-6-6 loss: 1.062557  [  224/  306]
train() client id: f_00004-6-7 loss: 0.800366  [  256/  306]
train() client id: f_00004-6-8 loss: 0.974486  [  288/  306]
train() client id: f_00004-7-0 loss: 0.933205  [   32/  306]
train() client id: f_00004-7-1 loss: 0.828284  [   64/  306]
train() client id: f_00004-7-2 loss: 0.863852  [   96/  306]
train() client id: f_00004-7-3 loss: 0.849060  [  128/  306]
train() client id: f_00004-7-4 loss: 0.953850  [  160/  306]
train() client id: f_00004-7-5 loss: 0.939470  [  192/  306]
train() client id: f_00004-7-6 loss: 0.958777  [  224/  306]
train() client id: f_00004-7-7 loss: 0.837884  [  256/  306]
train() client id: f_00004-7-8 loss: 0.964408  [  288/  306]
train() client id: f_00004-8-0 loss: 0.942569  [   32/  306]
train() client id: f_00004-8-1 loss: 0.935521  [   64/  306]
train() client id: f_00004-8-2 loss: 0.886487  [   96/  306]
train() client id: f_00004-8-3 loss: 0.856296  [  128/  306]
train() client id: f_00004-8-4 loss: 0.865121  [  160/  306]
train() client id: f_00004-8-5 loss: 0.958131  [  192/  306]
train() client id: f_00004-8-6 loss: 0.926255  [  224/  306]
train() client id: f_00004-8-7 loss: 0.861193  [  256/  306]
train() client id: f_00004-8-8 loss: 0.888062  [  288/  306]
train() client id: f_00004-9-0 loss: 0.970011  [   32/  306]
train() client id: f_00004-9-1 loss: 0.880959  [   64/  306]
train() client id: f_00004-9-2 loss: 0.835850  [   96/  306]
train() client id: f_00004-9-3 loss: 0.970005  [  128/  306]
train() client id: f_00004-9-4 loss: 0.982981  [  160/  306]
train() client id: f_00004-9-5 loss: 0.984632  [  192/  306]
train() client id: f_00004-9-6 loss: 0.822852  [  224/  306]
train() client id: f_00004-9-7 loss: 0.835529  [  256/  306]
train() client id: f_00004-9-8 loss: 0.861326  [  288/  306]
train() client id: f_00004-10-0 loss: 0.844316  [   32/  306]
train() client id: f_00004-10-1 loss: 0.930084  [   64/  306]
train() client id: f_00004-10-2 loss: 0.839737  [   96/  306]
train() client id: f_00004-10-3 loss: 0.978513  [  128/  306]
train() client id: f_00004-10-4 loss: 0.860955  [  160/  306]
train() client id: f_00004-10-5 loss: 0.860043  [  192/  306]
train() client id: f_00004-10-6 loss: 0.970568  [  224/  306]
train() client id: f_00004-10-7 loss: 0.886238  [  256/  306]
train() client id: f_00004-10-8 loss: 0.916233  [  288/  306]
train() client id: f_00005-0-0 loss: 0.751909  [   32/  146]
train() client id: f_00005-0-1 loss: 0.592575  [   64/  146]
train() client id: f_00005-0-2 loss: 0.612035  [   96/  146]
train() client id: f_00005-0-3 loss: 0.840325  [  128/  146]
train() client id: f_00005-1-0 loss: 0.828214  [   32/  146]
train() client id: f_00005-1-1 loss: 0.816379  [   64/  146]
train() client id: f_00005-1-2 loss: 0.619259  [   96/  146]
train() client id: f_00005-1-3 loss: 0.668429  [  128/  146]
train() client id: f_00005-2-0 loss: 0.517503  [   32/  146]
train() client id: f_00005-2-1 loss: 0.601233  [   64/  146]
train() client id: f_00005-2-2 loss: 0.942615  [   96/  146]
train() client id: f_00005-2-3 loss: 0.842201  [  128/  146]
train() client id: f_00005-3-0 loss: 0.724442  [   32/  146]
train() client id: f_00005-3-1 loss: 0.678608  [   64/  146]
train() client id: f_00005-3-2 loss: 0.795478  [   96/  146]
train() client id: f_00005-3-3 loss: 0.673440  [  128/  146]
train() client id: f_00005-4-0 loss: 0.782421  [   32/  146]
train() client id: f_00005-4-1 loss: 0.558905  [   64/  146]
train() client id: f_00005-4-2 loss: 0.797476  [   96/  146]
train() client id: f_00005-4-3 loss: 0.646324  [  128/  146]
train() client id: f_00005-5-0 loss: 0.683516  [   32/  146]
train() client id: f_00005-5-1 loss: 0.640371  [   64/  146]
train() client id: f_00005-5-2 loss: 0.810475  [   96/  146]
train() client id: f_00005-5-3 loss: 0.848694  [  128/  146]
train() client id: f_00005-6-0 loss: 0.635471  [   32/  146]
train() client id: f_00005-6-1 loss: 0.629461  [   64/  146]
train() client id: f_00005-6-2 loss: 0.850024  [   96/  146]
train() client id: f_00005-6-3 loss: 0.776519  [  128/  146]
train() client id: f_00005-7-0 loss: 0.694778  [   32/  146]
train() client id: f_00005-7-1 loss: 0.785937  [   64/  146]
train() client id: f_00005-7-2 loss: 0.738226  [   96/  146]
train() client id: f_00005-7-3 loss: 0.622061  [  128/  146]
train() client id: f_00005-8-0 loss: 0.922049  [   32/  146]
train() client id: f_00005-8-1 loss: 0.819835  [   64/  146]
train() client id: f_00005-8-2 loss: 0.569700  [   96/  146]
train() client id: f_00005-8-3 loss: 0.618158  [  128/  146]
train() client id: f_00005-9-0 loss: 0.705289  [   32/  146]
train() client id: f_00005-9-1 loss: 0.603998  [   64/  146]
train() client id: f_00005-9-2 loss: 0.678614  [   96/  146]
train() client id: f_00005-9-3 loss: 0.709924  [  128/  146]
train() client id: f_00005-10-0 loss: 0.706238  [   32/  146]
train() client id: f_00005-10-1 loss: 0.661163  [   64/  146]
train() client id: f_00005-10-2 loss: 0.869839  [   96/  146]
train() client id: f_00005-10-3 loss: 0.597936  [  128/  146]
train() client id: f_00006-0-0 loss: 0.463584  [   32/   54]
train() client id: f_00006-1-0 loss: 0.498900  [   32/   54]
train() client id: f_00006-2-0 loss: 0.559350  [   32/   54]
train() client id: f_00006-3-0 loss: 0.532244  [   32/   54]
train() client id: f_00006-4-0 loss: 0.472614  [   32/   54]
train() client id: f_00006-5-0 loss: 0.536596  [   32/   54]
train() client id: f_00006-6-0 loss: 0.474569  [   32/   54]
train() client id: f_00006-7-0 loss: 0.446616  [   32/   54]
train() client id: f_00006-8-0 loss: 0.476929  [   32/   54]
train() client id: f_00006-9-0 loss: 0.521498  [   32/   54]
train() client id: f_00006-10-0 loss: 0.473122  [   32/   54]
train() client id: f_00007-0-0 loss: 0.757817  [   32/  179]
train() client id: f_00007-0-1 loss: 0.501442  [   64/  179]
train() client id: f_00007-0-2 loss: 0.571169  [   96/  179]
train() client id: f_00007-0-3 loss: 0.580728  [  128/  179]
train() client id: f_00007-0-4 loss: 0.500835  [  160/  179]
train() client id: f_00007-1-0 loss: 0.403494  [   32/  179]
train() client id: f_00007-1-1 loss: 0.487937  [   64/  179]
train() client id: f_00007-1-2 loss: 0.484981  [   96/  179]
train() client id: f_00007-1-3 loss: 0.627348  [  128/  179]
train() client id: f_00007-1-4 loss: 0.719543  [  160/  179]
train() client id: f_00007-2-0 loss: 0.372527  [   32/  179]
train() client id: f_00007-2-1 loss: 0.578128  [   64/  179]
train() client id: f_00007-2-2 loss: 0.456009  [   96/  179]
train() client id: f_00007-2-3 loss: 0.601095  [  128/  179]
train() client id: f_00007-2-4 loss: 0.645072  [  160/  179]
train() client id: f_00007-3-0 loss: 0.397557  [   32/  179]
train() client id: f_00007-3-1 loss: 0.506703  [   64/  179]
train() client id: f_00007-3-2 loss: 0.507950  [   96/  179]
train() client id: f_00007-3-3 loss: 0.595162  [  128/  179]
train() client id: f_00007-3-4 loss: 0.517849  [  160/  179]
train() client id: f_00007-4-0 loss: 0.641419  [   32/  179]
train() client id: f_00007-4-1 loss: 0.507644  [   64/  179]
train() client id: f_00007-4-2 loss: 0.394603  [   96/  179]
train() client id: f_00007-4-3 loss: 0.738384  [  128/  179]
train() client id: f_00007-4-4 loss: 0.476807  [  160/  179]
train() client id: f_00007-5-0 loss: 0.568640  [   32/  179]
train() client id: f_00007-5-1 loss: 0.495256  [   64/  179]
train() client id: f_00007-5-2 loss: 0.724382  [   96/  179]
train() client id: f_00007-5-3 loss: 0.485686  [  128/  179]
train() client id: f_00007-5-4 loss: 0.482525  [  160/  179]
train() client id: f_00007-6-0 loss: 0.449023  [   32/  179]
train() client id: f_00007-6-1 loss: 0.564160  [   64/  179]
train() client id: f_00007-6-2 loss: 0.579234  [   96/  179]
train() client id: f_00007-6-3 loss: 0.498836  [  128/  179]
train() client id: f_00007-6-4 loss: 0.502242  [  160/  179]
train() client id: f_00007-7-0 loss: 0.592222  [   32/  179]
train() client id: f_00007-7-1 loss: 0.444669  [   64/  179]
train() client id: f_00007-7-2 loss: 0.524235  [   96/  179]
train() client id: f_00007-7-3 loss: 0.607265  [  128/  179]
train() client id: f_00007-7-4 loss: 0.442940  [  160/  179]
train() client id: f_00007-8-0 loss: 0.441544  [   32/  179]
train() client id: f_00007-8-1 loss: 0.857075  [   64/  179]
train() client id: f_00007-8-2 loss: 0.556086  [   96/  179]
train() client id: f_00007-8-3 loss: 0.469147  [  128/  179]
train() client id: f_00007-8-4 loss: 0.337936  [  160/  179]
train() client id: f_00007-9-0 loss: 0.461006  [   32/  179]
train() client id: f_00007-9-1 loss: 0.395901  [   64/  179]
train() client id: f_00007-9-2 loss: 0.497664  [   96/  179]
train() client id: f_00007-9-3 loss: 0.456955  [  128/  179]
train() client id: f_00007-9-4 loss: 0.742651  [  160/  179]
train() client id: f_00007-10-0 loss: 0.455200  [   32/  179]
train() client id: f_00007-10-1 loss: 0.536288  [   64/  179]
train() client id: f_00007-10-2 loss: 0.353011  [   96/  179]
train() client id: f_00007-10-3 loss: 0.642075  [  128/  179]
train() client id: f_00007-10-4 loss: 0.589937  [  160/  179]
train() client id: f_00008-0-0 loss: 0.735288  [   32/  130]
train() client id: f_00008-0-1 loss: 0.726530  [   64/  130]
train() client id: f_00008-0-2 loss: 0.670299  [   96/  130]
train() client id: f_00008-0-3 loss: 0.751933  [  128/  130]
train() client id: f_00008-1-0 loss: 0.654833  [   32/  130]
train() client id: f_00008-1-1 loss: 0.825066  [   64/  130]
train() client id: f_00008-1-2 loss: 0.723705  [   96/  130]
train() client id: f_00008-1-3 loss: 0.700683  [  128/  130]
train() client id: f_00008-2-0 loss: 0.674773  [   32/  130]
train() client id: f_00008-2-1 loss: 0.748466  [   64/  130]
train() client id: f_00008-2-2 loss: 0.754335  [   96/  130]
train() client id: f_00008-2-3 loss: 0.742022  [  128/  130]
train() client id: f_00008-3-0 loss: 0.633651  [   32/  130]
train() client id: f_00008-3-1 loss: 0.711673  [   64/  130]
train() client id: f_00008-3-2 loss: 0.836856  [   96/  130]
train() client id: f_00008-3-3 loss: 0.760261  [  128/  130]
train() client id: f_00008-4-0 loss: 0.628124  [   32/  130]
train() client id: f_00008-4-1 loss: 0.821555  [   64/  130]
train() client id: f_00008-4-2 loss: 0.778203  [   96/  130]
train() client id: f_00008-4-3 loss: 0.708606  [  128/  130]
train() client id: f_00008-5-0 loss: 0.744797  [   32/  130]
train() client id: f_00008-5-1 loss: 0.758954  [   64/  130]
train() client id: f_00008-5-2 loss: 0.701562  [   96/  130]
train() client id: f_00008-5-3 loss: 0.724833  [  128/  130]
train() client id: f_00008-6-0 loss: 0.688939  [   32/  130]
train() client id: f_00008-6-1 loss: 0.795554  [   64/  130]
train() client id: f_00008-6-2 loss: 0.710287  [   96/  130]
train() client id: f_00008-6-3 loss: 0.741634  [  128/  130]
train() client id: f_00008-7-0 loss: 0.655310  [   32/  130]
train() client id: f_00008-7-1 loss: 0.712426  [   64/  130]
train() client id: f_00008-7-2 loss: 0.820714  [   96/  130]
train() client id: f_00008-7-3 loss: 0.733865  [  128/  130]
train() client id: f_00008-8-0 loss: 0.692072  [   32/  130]
train() client id: f_00008-8-1 loss: 0.831120  [   64/  130]
train() client id: f_00008-8-2 loss: 0.626606  [   96/  130]
train() client id: f_00008-8-3 loss: 0.786339  [  128/  130]
train() client id: f_00008-9-0 loss: 0.729486  [   32/  130]
train() client id: f_00008-9-1 loss: 0.711294  [   64/  130]
train() client id: f_00008-9-2 loss: 0.723079  [   96/  130]
train() client id: f_00008-9-3 loss: 0.765641  [  128/  130]
train() client id: f_00008-10-0 loss: 0.770458  [   32/  130]
train() client id: f_00008-10-1 loss: 0.753878  [   64/  130]
train() client id: f_00008-10-2 loss: 0.612232  [   96/  130]
train() client id: f_00008-10-3 loss: 0.802347  [  128/  130]
train() client id: f_00009-0-0 loss: 1.200092  [   32/  118]
train() client id: f_00009-0-1 loss: 0.925035  [   64/  118]
train() client id: f_00009-0-2 loss: 1.050039  [   96/  118]
train() client id: f_00009-1-0 loss: 1.085234  [   32/  118]
train() client id: f_00009-1-1 loss: 0.880735  [   64/  118]
train() client id: f_00009-1-2 loss: 1.136864  [   96/  118]
train() client id: f_00009-2-0 loss: 0.975461  [   32/  118]
train() client id: f_00009-2-1 loss: 0.991612  [   64/  118]
train() client id: f_00009-2-2 loss: 0.924405  [   96/  118]
train() client id: f_00009-3-0 loss: 1.083182  [   32/  118]
train() client id: f_00009-3-1 loss: 0.814304  [   64/  118]
train() client id: f_00009-3-2 loss: 0.912768  [   96/  118]
train() client id: f_00009-4-0 loss: 0.947021  [   32/  118]
train() client id: f_00009-4-1 loss: 1.055614  [   64/  118]
train() client id: f_00009-4-2 loss: 0.828563  [   96/  118]
train() client id: f_00009-5-0 loss: 0.961589  [   32/  118]
train() client id: f_00009-5-1 loss: 0.764104  [   64/  118]
train() client id: f_00009-5-2 loss: 1.096442  [   96/  118]
train() client id: f_00009-6-0 loss: 1.006654  [   32/  118]
train() client id: f_00009-6-1 loss: 0.954104  [   64/  118]
train() client id: f_00009-6-2 loss: 0.711681  [   96/  118]
train() client id: f_00009-7-0 loss: 0.949961  [   32/  118]
train() client id: f_00009-7-1 loss: 0.866857  [   64/  118]
train() client id: f_00009-7-2 loss: 0.929497  [   96/  118]
train() client id: f_00009-8-0 loss: 0.788353  [   32/  118]
train() client id: f_00009-8-1 loss: 0.916611  [   64/  118]
train() client id: f_00009-8-2 loss: 0.867248  [   96/  118]
train() client id: f_00009-9-0 loss: 0.851115  [   32/  118]
train() client id: f_00009-9-1 loss: 0.771468  [   64/  118]
train() client id: f_00009-9-2 loss: 1.071265  [   96/  118]
train() client id: f_00009-10-0 loss: 1.007516  [   32/  118]
train() client id: f_00009-10-1 loss: 0.754646  [   64/  118]
train() client id: f_00009-10-2 loss: 1.016201  [   96/  118]
At round 54 accuracy: 0.6472148541114059
At round 54 training accuracy: 0.5881958417169685
At round 54 training loss: 0.8361808572237672
update_location
xs = [  -3.9056584     4.20031788  290.00902392   18.81129433    0.97929623
    3.95640986 -252.44319194 -231.32485185  274.66397685 -217.06087855]
ys = [ 282.5879595   265.55583871    1.32061395 -252.45517586  244.35018685
  227.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [299.78527152 283.79137785 306.76860657 272.19015525 264.02267484
 248.82712967 271.54089139 252.01560138 292.82925085 239.02183343]
dists_bs = [202.10997688 200.78125871 496.38485217 469.34075593 189.15115944
 186.58706051 193.77327739 183.05830601 476.42650004 176.09545916]
uav_gains = [2.68378500e-12 3.69674554e-12 2.35370123e-12 4.70996853e-12
 5.58646308e-12 7.59172095e-12 4.77454866e-12 7.13118064e-12
 3.07591018e-12 9.13258179e-12]
bs_gains = [3.86944233e-11 3.94156962e-11 3.12609541e-12 3.65701662e-12
 4.65831512e-11 4.83978224e-11 4.35382740e-11 5.10556319e-11
 3.50675591e-12 5.69113968e-11]
Round 55
-------------------------------
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.74701293 7.6136546  3.69953246 1.35283391 8.77808353 4.22374522
 1.66527442 5.20651362 3.84415712 3.42461541]
obj_prev = 43.55542321522954
eta_min = 2.2364048538474024e-25	eta_max = 0.9376658335407816
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 10.034905168858742	eta = 0.909090909090909
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 21.758383945021716	eta = 0.4192701574551497
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 15.56372063459429	eta = 0.5861478290943805
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 14.453751471416155	eta = 0.6311607806900414
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 14.387691546337155	eta = 0.6340587044987988
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 14.387430378338086	eta = 0.6340702142568856
eta = 0.6340702142568856
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [0.03759466 0.07906812 0.03699791 0.01282992 0.09130131 0.04356206
 0.016112   0.05340828 0.03878814 0.03520769]
ene_total = [1.39513647 2.21195089 1.40544566 0.67656761 2.51756531 1.29725943
 0.75865034 1.66646122 1.38042471 1.07796874]
ti_comp = [0.89681784 0.99162304 0.88681254 0.93061922 0.99425194 0.99482833
 0.93128983 0.94868174 0.91451495 0.99717632]
ti_coms = [0.17049984 0.07569464 0.18050515 0.13669846 0.07306574 0.07248935
 0.13602786 0.11863595 0.15280274 0.07014136]
t_total = [27.24976921 27.24976921 27.24976921 27.24976921 27.24976921 27.24976921
 27.24976921 27.24976921 27.24976921 27.24976921]
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [4.12904738e-06 3.14189100e-05 4.02483533e-06 1.52408097e-07
 4.81191659e-05 5.22046259e-06 3.01410426e-07 1.05794898e-05
 4.36109102e-06 2.74314532e-06]
ene_total = [0.43540987 0.19405873 0.46095183 0.34900978 0.18777323 0.1852064
 0.34730146 0.30316045 0.39023318 0.17914848]
optimize_network iter = 0 obj = 3.032253401302107
eta = 0.6340702142568856
freqs = [20960028.75390409 39868035.25301737 20860054.05324307  6893217.87674232
 45914572.57827595 21894260.42256957  8650365.72831919 28148681.6418485
 21206945.13364052 17653694.93957705]
eta_min = 0.6340702142568873	eta_max = 0.7017829950267004
af = 0.0022772242984259367	bf = 1.1085543441262717	zeta = 0.0025049467282685306	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [8.63792940e-07 6.57280727e-06 8.41991876e-07 3.18836346e-08
 1.00664856e-05 1.09211601e-06 6.30547859e-08 2.21321959e-06
 9.12336258e-07 5.73863496e-07]
ene_total = [1.75228827 0.77857718 1.85510894 1.40483142 0.75191939 0.74507368
 1.39794292 1.21943002 1.57042295 0.72089048]
ti_comp = [0.69931807 0.79412327 0.68931277 0.73311945 0.79675218 0.79732856
 0.73379006 0.75118197 0.71701518 0.79967655]
ti_coms = [0.17049984 0.07569464 0.18050515 0.13669846 0.07306574 0.07248935
 0.13602786 0.11863595 0.15280274 0.07014136]
t_total = [27.24976921 27.24976921 27.24976921 27.24976921 27.24976921 27.24976921
 27.24976921 27.24976921 27.24976921 27.24976921]
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [3.18905741e-06 2.30071054e-05 3.12847447e-06 1.15333663e-07
 3.51898948e-05 3.81666577e-06 2.28001571e-07 7.92444745e-06
 3.33175889e-06 2.00316859e-06]
ene_total = [0.53424404 0.23785794 0.56558689 0.42825434 0.23000374 0.22721516
 0.42615699 0.3719125  0.47880679 0.21980253]
optimize_network iter = 1 obj = 3.71984092060831
eta = 0.7017829950267004
freqs = [20893243.25277815 38696173.21796749 20860054.05324307  6801485.54855279
 44535695.14880739 21233696.28865842  8533594.53110815 27632357.80406622
 21024470.67520079 17111091.4088647 ]
eta_min = 0.7017829950267082	eta_max = 0.7017829950267004
af = 0.0021615562993590274	bf = 1.1085543441262717	zeta = 0.0023777119292949303	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [8.58297057e-07 6.19209011e-06 8.41991876e-07 3.10406904e-08
 9.47094369e-06 1.02721042e-06 6.13639241e-08 2.13277124e-06
 8.96703470e-07 5.39129116e-07]
ene_total = [1.75228771 0.77853806 1.85510894 1.40483134 0.75185818 0.74506701
 1.39794275 1.21942176 1.57042134 0.72088691]
ti_comp = [0.69931807 0.79412327 0.68931277 0.73311945 0.79675218 0.79732856
 0.73379006 0.75118197 0.71701518 0.79967655]
ti_coms = [0.17049984 0.07569464 0.18050515 0.13669846 0.07306574 0.07248935
 0.13602786 0.11863595 0.15280274 0.07014136]
t_total = [27.24976921 27.24976921 27.24976921 27.24976921 27.24976921 27.24976921
 27.24976921 27.24976921 27.24976921 27.24976921]
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [3.18905741e-06 2.30071054e-05 3.12847447e-06 1.15333663e-07
 3.51898948e-05 3.81666577e-06 2.28001571e-07 7.92444745e-06
 3.33175889e-06 2.00316859e-06]
ene_total = [0.53424404 0.23785794 0.56558689 0.42825434 0.23000374 0.22721516
 0.42615699 0.3719125  0.47880679 0.21980253]
optimize_network iter = 2 obj = 3.71984092060831
eta = 0.7017829950267004
freqs = [20893243.25277815 38696173.21796749 20860054.05324307  6801485.54855279
 44535695.14880739 21233696.28865842  8533594.53110815 27632357.80406622
 21024470.67520079 17111091.4088647 ]
Done!
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [3.02513636e-06 2.18245150e-05 2.96766745e-06 1.09405386e-07
 3.33810956e-05 3.62048496e-06 2.16282040e-07 7.51712216e-06
 3.16050283e-06 1.90020353e-06]
ene_total = [0.01705301 0.00759129 0.01805348 0.01366996 0.00733996 0.00725256
 0.013603   0.01187111 0.01528343 0.00701604]
At round 55 energy consumption: 0.11873383165749558
At round 55 eta: 0.7017829950267004
At round 55 a_n: 9.000035425972609
At round 55 local rounds: 11.596049668428781
At round 55 global rounds: 31.328130579872187
gradient difference: 0.43061479926109314
train() client id: f_00000-0-0 loss: 1.068347  [   32/  126]
train() client id: f_00000-0-1 loss: 1.290653  [   64/  126]
train() client id: f_00000-0-2 loss: 1.003033  [   96/  126]
train() client id: f_00000-1-0 loss: 1.128518  [   32/  126]
train() client id: f_00000-1-1 loss: 0.975669  [   64/  126]
train() client id: f_00000-1-2 loss: 0.996135  [   96/  126]
train() client id: f_00000-2-0 loss: 0.987646  [   32/  126]
train() client id: f_00000-2-1 loss: 1.006338  [   64/  126]
train() client id: f_00000-2-2 loss: 0.976865  [   96/  126]
train() client id: f_00000-3-0 loss: 0.956383  [   32/  126]
train() client id: f_00000-3-1 loss: 0.916077  [   64/  126]
train() client id: f_00000-3-2 loss: 0.974344  [   96/  126]
train() client id: f_00000-4-0 loss: 1.036328  [   32/  126]
train() client id: f_00000-4-1 loss: 0.813103  [   64/  126]
train() client id: f_00000-4-2 loss: 1.039465  [   96/  126]
train() client id: f_00000-5-0 loss: 0.864199  [   32/  126]
train() client id: f_00000-5-1 loss: 0.963831  [   64/  126]
train() client id: f_00000-5-2 loss: 0.808305  [   96/  126]
train() client id: f_00000-6-0 loss: 0.892482  [   32/  126]
train() client id: f_00000-6-1 loss: 0.890559  [   64/  126]
train() client id: f_00000-6-2 loss: 0.905578  [   96/  126]
train() client id: f_00000-7-0 loss: 0.993918  [   32/  126]
train() client id: f_00000-7-1 loss: 0.856281  [   64/  126]
train() client id: f_00000-7-2 loss: 0.835201  [   96/  126]
train() client id: f_00000-8-0 loss: 1.026302  [   32/  126]
train() client id: f_00000-8-1 loss: 0.740502  [   64/  126]
train() client id: f_00000-8-2 loss: 0.991417  [   96/  126]
train() client id: f_00000-9-0 loss: 0.748024  [   32/  126]
train() client id: f_00000-9-1 loss: 1.006442  [   64/  126]
train() client id: f_00000-9-2 loss: 0.989668  [   96/  126]
train() client id: f_00000-10-0 loss: 0.806749  [   32/  126]
train() client id: f_00000-10-1 loss: 1.007786  [   64/  126]
train() client id: f_00000-10-2 loss: 0.899564  [   96/  126]
train() client id: f_00001-0-0 loss: 0.480188  [   32/  265]
train() client id: f_00001-0-1 loss: 0.472039  [   64/  265]
train() client id: f_00001-0-2 loss: 0.422721  [   96/  265]
train() client id: f_00001-0-3 loss: 0.531326  [  128/  265]
train() client id: f_00001-0-4 loss: 0.382852  [  160/  265]
train() client id: f_00001-0-5 loss: 0.459023  [  192/  265]
train() client id: f_00001-0-6 loss: 0.462683  [  224/  265]
train() client id: f_00001-0-7 loss: 0.566063  [  256/  265]
train() client id: f_00001-1-0 loss: 0.459282  [   32/  265]
train() client id: f_00001-1-1 loss: 0.487670  [   64/  265]
train() client id: f_00001-1-2 loss: 0.382950  [   96/  265]
train() client id: f_00001-1-3 loss: 0.537061  [  128/  265]
train() client id: f_00001-1-4 loss: 0.400344  [  160/  265]
train() client id: f_00001-1-5 loss: 0.645842  [  192/  265]
train() client id: f_00001-1-6 loss: 0.350934  [  224/  265]
train() client id: f_00001-1-7 loss: 0.437073  [  256/  265]
train() client id: f_00001-2-0 loss: 0.359447  [   32/  265]
train() client id: f_00001-2-1 loss: 0.503605  [   64/  265]
train() client id: f_00001-2-2 loss: 0.393393  [   96/  265]
train() client id: f_00001-2-3 loss: 0.431056  [  128/  265]
train() client id: f_00001-2-4 loss: 0.562232  [  160/  265]
train() client id: f_00001-2-5 loss: 0.357799  [  192/  265]
train() client id: f_00001-2-6 loss: 0.572758  [  224/  265]
train() client id: f_00001-2-7 loss: 0.465831  [  256/  265]
train() client id: f_00001-3-0 loss: 0.501209  [   32/  265]
train() client id: f_00001-3-1 loss: 0.440952  [   64/  265]
train() client id: f_00001-3-2 loss: 0.408076  [   96/  265]
train() client id: f_00001-3-3 loss: 0.500077  [  128/  265]
train() client id: f_00001-3-4 loss: 0.384163  [  160/  265]
train() client id: f_00001-3-5 loss: 0.478681  [  192/  265]
train() client id: f_00001-3-6 loss: 0.408766  [  224/  265]
train() client id: f_00001-3-7 loss: 0.568413  [  256/  265]
train() client id: f_00001-4-0 loss: 0.483106  [   32/  265]
train() client id: f_00001-4-1 loss: 0.464254  [   64/  265]
train() client id: f_00001-4-2 loss: 0.366371  [   96/  265]
train() client id: f_00001-4-3 loss: 0.461124  [  128/  265]
train() client id: f_00001-4-4 loss: 0.574906  [  160/  265]
train() client id: f_00001-4-5 loss: 0.438604  [  192/  265]
train() client id: f_00001-4-6 loss: 0.441727  [  224/  265]
train() client id: f_00001-4-7 loss: 0.383193  [  256/  265]
train() client id: f_00001-5-0 loss: 0.404099  [   32/  265]
train() client id: f_00001-5-1 loss: 0.489651  [   64/  265]
train() client id: f_00001-5-2 loss: 0.409175  [   96/  265]
train() client id: f_00001-5-3 loss: 0.427482  [  128/  265]
train() client id: f_00001-5-4 loss: 0.462896  [  160/  265]
train() client id: f_00001-5-5 loss: 0.434967  [  192/  265]
train() client id: f_00001-5-6 loss: 0.665166  [  224/  265]
train() client id: f_00001-5-7 loss: 0.353923  [  256/  265]
train() client id: f_00001-6-0 loss: 0.518124  [   32/  265]
train() client id: f_00001-6-1 loss: 0.436859  [   64/  265]
train() client id: f_00001-6-2 loss: 0.578251  [   96/  265]
train() client id: f_00001-6-3 loss: 0.389557  [  128/  265]
train() client id: f_00001-6-4 loss: 0.388447  [  160/  265]
train() client id: f_00001-6-5 loss: 0.500884  [  192/  265]
train() client id: f_00001-6-6 loss: 0.413716  [  224/  265]
train() client id: f_00001-6-7 loss: 0.358175  [  256/  265]
train() client id: f_00001-7-0 loss: 0.502579  [   32/  265]
train() client id: f_00001-7-1 loss: 0.496173  [   64/  265]
train() client id: f_00001-7-2 loss: 0.367816  [   96/  265]
train() client id: f_00001-7-3 loss: 0.373794  [  128/  265]
train() client id: f_00001-7-4 loss: 0.362186  [  160/  265]
train() client id: f_00001-7-5 loss: 0.530557  [  192/  265]
train() client id: f_00001-7-6 loss: 0.595011  [  224/  265]
train() client id: f_00001-7-7 loss: 0.400967  [  256/  265]
train() client id: f_00001-8-0 loss: 0.458412  [   32/  265]
train() client id: f_00001-8-1 loss: 0.360909  [   64/  265]
train() client id: f_00001-8-2 loss: 0.431489  [   96/  265]
train() client id: f_00001-8-3 loss: 0.441991  [  128/  265]
train() client id: f_00001-8-4 loss: 0.550812  [  160/  265]
train() client id: f_00001-8-5 loss: 0.343848  [  192/  265]
train() client id: f_00001-8-6 loss: 0.438174  [  224/  265]
train() client id: f_00001-8-7 loss: 0.517007  [  256/  265]
train() client id: f_00001-9-0 loss: 0.449851  [   32/  265]
train() client id: f_00001-9-1 loss: 0.357510  [   64/  265]
train() client id: f_00001-9-2 loss: 0.614173  [   96/  265]
train() client id: f_00001-9-3 loss: 0.366610  [  128/  265]
train() client id: f_00001-9-4 loss: 0.430139  [  160/  265]
train() client id: f_00001-9-5 loss: 0.419468  [  192/  265]
train() client id: f_00001-9-6 loss: 0.418615  [  224/  265]
train() client id: f_00001-9-7 loss: 0.456805  [  256/  265]
train() client id: f_00001-10-0 loss: 0.479802  [   32/  265]
train() client id: f_00001-10-1 loss: 0.403572  [   64/  265]
train() client id: f_00001-10-2 loss: 0.399969  [   96/  265]
train() client id: f_00001-10-3 loss: 0.483893  [  128/  265]
train() client id: f_00001-10-4 loss: 0.381618  [  160/  265]
train() client id: f_00001-10-5 loss: 0.468615  [  192/  265]
train() client id: f_00001-10-6 loss: 0.494489  [  224/  265]
train() client id: f_00001-10-7 loss: 0.501909  [  256/  265]
train() client id: f_00002-0-0 loss: 1.200010  [   32/  124]
train() client id: f_00002-0-1 loss: 0.994881  [   64/  124]
train() client id: f_00002-0-2 loss: 1.149316  [   96/  124]
train() client id: f_00002-1-0 loss: 1.029728  [   32/  124]
train() client id: f_00002-1-1 loss: 1.019783  [   64/  124]
train() client id: f_00002-1-2 loss: 1.084163  [   96/  124]
train() client id: f_00002-2-0 loss: 1.068966  [   32/  124]
train() client id: f_00002-2-1 loss: 1.102818  [   64/  124]
train() client id: f_00002-2-2 loss: 0.949331  [   96/  124]
train() client id: f_00002-3-0 loss: 0.915126  [   32/  124]
train() client id: f_00002-3-1 loss: 1.044186  [   64/  124]
train() client id: f_00002-3-2 loss: 0.896459  [   96/  124]
train() client id: f_00002-4-0 loss: 0.950706  [   32/  124]
train() client id: f_00002-4-1 loss: 0.917415  [   64/  124]
train() client id: f_00002-4-2 loss: 0.915442  [   96/  124]
train() client id: f_00002-5-0 loss: 0.947056  [   32/  124]
train() client id: f_00002-5-1 loss: 0.954741  [   64/  124]
train() client id: f_00002-5-2 loss: 0.886859  [   96/  124]
train() client id: f_00002-6-0 loss: 0.803211  [   32/  124]
train() client id: f_00002-6-1 loss: 1.051740  [   64/  124]
train() client id: f_00002-6-2 loss: 0.793089  [   96/  124]
train() client id: f_00002-7-0 loss: 0.777263  [   32/  124]
train() client id: f_00002-7-1 loss: 0.658592  [   64/  124]
train() client id: f_00002-7-2 loss: 0.857066  [   96/  124]
train() client id: f_00002-8-0 loss: 0.982059  [   32/  124]
train() client id: f_00002-8-1 loss: 0.819046  [   64/  124]
train() client id: f_00002-8-2 loss: 0.687947  [   96/  124]
train() client id: f_00002-9-0 loss: 1.062227  [   32/  124]
train() client id: f_00002-9-1 loss: 0.757127  [   64/  124]
train() client id: f_00002-9-2 loss: 0.819832  [   96/  124]
train() client id: f_00002-10-0 loss: 0.885404  [   32/  124]
train() client id: f_00002-10-1 loss: 0.805855  [   64/  124]
train() client id: f_00002-10-2 loss: 0.743631  [   96/  124]
train() client id: f_00003-0-0 loss: 0.919026  [   32/   43]
train() client id: f_00003-1-0 loss: 0.761691  [   32/   43]
train() client id: f_00003-2-0 loss: 0.713802  [   32/   43]
train() client id: f_00003-3-0 loss: 0.989471  [   32/   43]
train() client id: f_00003-4-0 loss: 0.830762  [   32/   43]
train() client id: f_00003-5-0 loss: 0.950617  [   32/   43]
train() client id: f_00003-6-0 loss: 1.088560  [   32/   43]
train() client id: f_00003-7-0 loss: 0.841285  [   32/   43]
train() client id: f_00003-8-0 loss: 0.913939  [   32/   43]
train() client id: f_00003-9-0 loss: 0.800086  [   32/   43]
train() client id: f_00003-10-0 loss: 0.938667  [   32/   43]
train() client id: f_00004-0-0 loss: 0.878869  [   32/  306]
train() client id: f_00004-0-1 loss: 0.924961  [   64/  306]
train() client id: f_00004-0-2 loss: 0.876577  [   96/  306]
train() client id: f_00004-0-3 loss: 0.809231  [  128/  306]
train() client id: f_00004-0-4 loss: 0.965999  [  160/  306]
train() client id: f_00004-0-5 loss: 0.852369  [  192/  306]
train() client id: f_00004-0-6 loss: 0.817564  [  224/  306]
train() client id: f_00004-0-7 loss: 0.924125  [  256/  306]
train() client id: f_00004-0-8 loss: 0.822095  [  288/  306]
train() client id: f_00004-1-0 loss: 0.882569  [   32/  306]
train() client id: f_00004-1-1 loss: 0.840097  [   64/  306]
train() client id: f_00004-1-2 loss: 0.927996  [   96/  306]
train() client id: f_00004-1-3 loss: 0.804155  [  128/  306]
train() client id: f_00004-1-4 loss: 0.829981  [  160/  306]
train() client id: f_00004-1-5 loss: 0.906273  [  192/  306]
train() client id: f_00004-1-6 loss: 0.804025  [  224/  306]
train() client id: f_00004-1-7 loss: 0.883421  [  256/  306]
train() client id: f_00004-1-8 loss: 0.855919  [  288/  306]
train() client id: f_00004-2-0 loss: 0.906347  [   32/  306]
train() client id: f_00004-2-1 loss: 0.824737  [   64/  306]
train() client id: f_00004-2-2 loss: 0.864797  [   96/  306]
train() client id: f_00004-2-3 loss: 0.970143  [  128/  306]
train() client id: f_00004-2-4 loss: 0.693929  [  160/  306]
train() client id: f_00004-2-5 loss: 0.826972  [  192/  306]
train() client id: f_00004-2-6 loss: 0.893646  [  224/  306]
train() client id: f_00004-2-7 loss: 0.912476  [  256/  306]
train() client id: f_00004-2-8 loss: 0.792491  [  288/  306]
train() client id: f_00004-3-0 loss: 0.994938  [   32/  306]
train() client id: f_00004-3-1 loss: 0.919146  [   64/  306]
train() client id: f_00004-3-2 loss: 0.907104  [   96/  306]
train() client id: f_00004-3-3 loss: 0.783845  [  128/  306]
train() client id: f_00004-3-4 loss: 0.813031  [  160/  306]
train() client id: f_00004-3-5 loss: 0.833317  [  192/  306]
train() client id: f_00004-3-6 loss: 0.788403  [  224/  306]
train() client id: f_00004-3-7 loss: 0.776763  [  256/  306]
train() client id: f_00004-3-8 loss: 0.876577  [  288/  306]
train() client id: f_00004-4-0 loss: 0.782405  [   32/  306]
train() client id: f_00004-4-1 loss: 0.961681  [   64/  306]
train() client id: f_00004-4-2 loss: 0.777615  [   96/  306]
train() client id: f_00004-4-3 loss: 0.857738  [  128/  306]
train() client id: f_00004-4-4 loss: 0.871978  [  160/  306]
train() client id: f_00004-4-5 loss: 0.951591  [  192/  306]
train() client id: f_00004-4-6 loss: 0.824335  [  224/  306]
train() client id: f_00004-4-7 loss: 0.874570  [  256/  306]
train() client id: f_00004-4-8 loss: 0.809644  [  288/  306]
train() client id: f_00004-5-0 loss: 0.833112  [   32/  306]
train() client id: f_00004-5-1 loss: 0.929121  [   64/  306]
train() client id: f_00004-5-2 loss: 0.934312  [   96/  306]
train() client id: f_00004-5-3 loss: 0.837616  [  128/  306]
train() client id: f_00004-5-4 loss: 0.922773  [  160/  306]
train() client id: f_00004-5-5 loss: 0.737079  [  192/  306]
train() client id: f_00004-5-6 loss: 0.889939  [  224/  306]
train() client id: f_00004-5-7 loss: 0.831752  [  256/  306]
train() client id: f_00004-5-8 loss: 0.810097  [  288/  306]
train() client id: f_00004-6-0 loss: 0.933269  [   32/  306]
train() client id: f_00004-6-1 loss: 0.807842  [   64/  306]
train() client id: f_00004-6-2 loss: 0.879640  [   96/  306]
train() client id: f_00004-6-3 loss: 0.701953  [  128/  306]
train() client id: f_00004-6-4 loss: 0.871770  [  160/  306]
train() client id: f_00004-6-5 loss: 0.881481  [  192/  306]
train() client id: f_00004-6-6 loss: 0.956856  [  224/  306]
train() client id: f_00004-6-7 loss: 0.847407  [  256/  306]
train() client id: f_00004-6-8 loss: 0.724998  [  288/  306]
train() client id: f_00004-7-0 loss: 0.768085  [   32/  306]
train() client id: f_00004-7-1 loss: 0.873373  [   64/  306]
train() client id: f_00004-7-2 loss: 0.950269  [   96/  306]
train() client id: f_00004-7-3 loss: 0.788040  [  128/  306]
train() client id: f_00004-7-4 loss: 0.908797  [  160/  306]
train() client id: f_00004-7-5 loss: 0.803792  [  192/  306]
train() client id: f_00004-7-6 loss: 0.806370  [  224/  306]
train() client id: f_00004-7-7 loss: 0.939041  [  256/  306]
train() client id: f_00004-7-8 loss: 0.848021  [  288/  306]
train() client id: f_00004-8-0 loss: 0.962654  [   32/  306]
train() client id: f_00004-8-1 loss: 0.794455  [   64/  306]
train() client id: f_00004-8-2 loss: 0.861883  [   96/  306]
train() client id: f_00004-8-3 loss: 0.839523  [  128/  306]
train() client id: f_00004-8-4 loss: 0.958460  [  160/  306]
train() client id: f_00004-8-5 loss: 0.775943  [  192/  306]
train() client id: f_00004-8-6 loss: 0.770520  [  224/  306]
train() client id: f_00004-8-7 loss: 0.939548  [  256/  306]
train() client id: f_00004-8-8 loss: 0.689172  [  288/  306]
train() client id: f_00004-9-0 loss: 0.844744  [   32/  306]
train() client id: f_00004-9-1 loss: 0.767541  [   64/  306]
train() client id: f_00004-9-2 loss: 0.876392  [   96/  306]
train() client id: f_00004-9-3 loss: 0.969109  [  128/  306]
train() client id: f_00004-9-4 loss: 0.746966  [  160/  306]
train() client id: f_00004-9-5 loss: 0.879514  [  192/  306]
train() client id: f_00004-9-6 loss: 0.979681  [  224/  306]
train() client id: f_00004-9-7 loss: 0.807194  [  256/  306]
train() client id: f_00004-9-8 loss: 0.759967  [  288/  306]
train() client id: f_00004-10-0 loss: 0.794472  [   32/  306]
train() client id: f_00004-10-1 loss: 0.848449  [   64/  306]
train() client id: f_00004-10-2 loss: 0.919358  [   96/  306]
train() client id: f_00004-10-3 loss: 0.732455  [  128/  306]
train() client id: f_00004-10-4 loss: 0.893827  [  160/  306]
train() client id: f_00004-10-5 loss: 0.986107  [  192/  306]
train() client id: f_00004-10-6 loss: 0.852348  [  224/  306]
train() client id: f_00004-10-7 loss: 0.858352  [  256/  306]
train() client id: f_00004-10-8 loss: 0.760946  [  288/  306]
train() client id: f_00005-0-0 loss: 0.611986  [   32/  146]
train() client id: f_00005-0-1 loss: 1.069530  [   64/  146]
train() client id: f_00005-0-2 loss: 0.699018  [   96/  146]
train() client id: f_00005-0-3 loss: 0.584203  [  128/  146]
train() client id: f_00005-1-0 loss: 0.953759  [   32/  146]
train() client id: f_00005-1-1 loss: 0.786218  [   64/  146]
train() client id: f_00005-1-2 loss: 0.672682  [   96/  146]
train() client id: f_00005-1-3 loss: 0.589247  [  128/  146]
train() client id: f_00005-2-0 loss: 0.586123  [   32/  146]
train() client id: f_00005-2-1 loss: 0.523679  [   64/  146]
train() client id: f_00005-2-2 loss: 0.700001  [   96/  146]
train() client id: f_00005-2-3 loss: 0.862220  [  128/  146]
train() client id: f_00005-3-0 loss: 0.677911  [   32/  146]
train() client id: f_00005-3-1 loss: 0.717838  [   64/  146]
train() client id: f_00005-3-2 loss: 0.485552  [   96/  146]
train() client id: f_00005-3-3 loss: 0.766049  [  128/  146]
train() client id: f_00005-4-0 loss: 0.710437  [   32/  146]
train() client id: f_00005-4-1 loss: 0.723455  [   64/  146]
train() client id: f_00005-4-2 loss: 0.677050  [   96/  146]
train() client id: f_00005-4-3 loss: 0.769591  [  128/  146]
train() client id: f_00005-5-0 loss: 0.894944  [   32/  146]
train() client id: f_00005-5-1 loss: 0.525534  [   64/  146]
train() client id: f_00005-5-2 loss: 0.751725  [   96/  146]
train() client id: f_00005-5-3 loss: 0.637718  [  128/  146]
train() client id: f_00005-6-0 loss: 0.792422  [   32/  146]
train() client id: f_00005-6-1 loss: 0.876146  [   64/  146]
train() client id: f_00005-6-2 loss: 0.503974  [   96/  146]
train() client id: f_00005-6-3 loss: 0.696176  [  128/  146]
train() client id: f_00005-7-0 loss: 0.578599  [   32/  146]
train() client id: f_00005-7-1 loss: 0.903380  [   64/  146]
train() client id: f_00005-7-2 loss: 0.681784  [   96/  146]
train() client id: f_00005-7-3 loss: 0.802146  [  128/  146]
train() client id: f_00005-8-0 loss: 0.872973  [   32/  146]
train() client id: f_00005-8-1 loss: 0.529719  [   64/  146]
train() client id: f_00005-8-2 loss: 0.715773  [   96/  146]
train() client id: f_00005-8-3 loss: 0.593202  [  128/  146]
train() client id: f_00005-9-0 loss: 0.811566  [   32/  146]
train() client id: f_00005-9-1 loss: 0.680270  [   64/  146]
train() client id: f_00005-9-2 loss: 0.712068  [   96/  146]
train() client id: f_00005-9-3 loss: 0.588567  [  128/  146]
train() client id: f_00005-10-0 loss: 0.640917  [   32/  146]
train() client id: f_00005-10-1 loss: 0.637399  [   64/  146]
train() client id: f_00005-10-2 loss: 0.676777  [   96/  146]
train() client id: f_00005-10-3 loss: 1.069648  [  128/  146]
train() client id: f_00006-0-0 loss: 0.524978  [   32/   54]
train() client id: f_00006-1-0 loss: 0.517752  [   32/   54]
train() client id: f_00006-2-0 loss: 0.559921  [   32/   54]
train() client id: f_00006-3-0 loss: 0.572044  [   32/   54]
train() client id: f_00006-4-0 loss: 0.557570  [   32/   54]
train() client id: f_00006-5-0 loss: 0.570725  [   32/   54]
train() client id: f_00006-6-0 loss: 0.527808  [   32/   54]
train() client id: f_00006-7-0 loss: 0.549762  [   32/   54]
train() client id: f_00006-8-0 loss: 0.522202  [   32/   54]
train() client id: f_00006-9-0 loss: 0.463693  [   32/   54]
train() client id: f_00006-10-0 loss: 0.504437  [   32/   54]
train() client id: f_00007-0-0 loss: 0.492299  [   32/  179]
train() client id: f_00007-0-1 loss: 0.717400  [   64/  179]
train() client id: f_00007-0-2 loss: 0.374057  [   96/  179]
train() client id: f_00007-0-3 loss: 0.461459  [  128/  179]
train() client id: f_00007-0-4 loss: 0.606437  [  160/  179]
train() client id: f_00007-1-0 loss: 0.370049  [   32/  179]
train() client id: f_00007-1-1 loss: 0.503863  [   64/  179]
train() client id: f_00007-1-2 loss: 0.488363  [   96/  179]
train() client id: f_00007-1-3 loss: 0.635042  [  128/  179]
train() client id: f_00007-1-4 loss: 0.510394  [  160/  179]
train() client id: f_00007-2-0 loss: 0.486461  [   32/  179]
train() client id: f_00007-2-1 loss: 0.364308  [   64/  179]
train() client id: f_00007-2-2 loss: 0.532521  [   96/  179]
train() client id: f_00007-2-3 loss: 0.682260  [  128/  179]
train() client id: f_00007-2-4 loss: 0.445043  [  160/  179]
train() client id: f_00007-3-0 loss: 0.494857  [   32/  179]
train() client id: f_00007-3-1 loss: 0.509100  [   64/  179]
train() client id: f_00007-3-2 loss: 0.611481  [   96/  179]
train() client id: f_00007-3-3 loss: 0.428105  [  128/  179]
train() client id: f_00007-3-4 loss: 0.364515  [  160/  179]
train() client id: f_00007-4-0 loss: 0.336302  [   32/  179]
train() client id: f_00007-4-1 loss: 0.723003  [   64/  179]
train() client id: f_00007-4-2 loss: 0.596863  [   96/  179]
train() client id: f_00007-4-3 loss: 0.556413  [  128/  179]
train() client id: f_00007-4-4 loss: 0.337163  [  160/  179]
train() client id: f_00007-5-0 loss: 0.566123  [   32/  179]
train() client id: f_00007-5-1 loss: 0.723190  [   64/  179]
train() client id: f_00007-5-2 loss: 0.358910  [   96/  179]
train() client id: f_00007-5-3 loss: 0.404237  [  128/  179]
train() client id: f_00007-5-4 loss: 0.463468  [  160/  179]
train() client id: f_00007-6-0 loss: 0.364959  [   32/  179]
train() client id: f_00007-6-1 loss: 0.531049  [   64/  179]
train() client id: f_00007-6-2 loss: 0.502477  [   96/  179]
train() client id: f_00007-6-3 loss: 0.490608  [  128/  179]
train() client id: f_00007-6-4 loss: 0.482499  [  160/  179]
train() client id: f_00007-7-0 loss: 0.662024  [   32/  179]
train() client id: f_00007-7-1 loss: 0.341746  [   64/  179]
train() client id: f_00007-7-2 loss: 0.460391  [   96/  179]
train() client id: f_00007-7-3 loss: 0.624090  [  128/  179]
train() client id: f_00007-7-4 loss: 0.403732  [  160/  179]
train() client id: f_00007-8-0 loss: 0.472139  [   32/  179]
train() client id: f_00007-8-1 loss: 0.688940  [   64/  179]
train() client id: f_00007-8-2 loss: 0.418108  [   96/  179]
train() client id: f_00007-8-3 loss: 0.508080  [  128/  179]
train() client id: f_00007-8-4 loss: 0.399096  [  160/  179]
train() client id: f_00007-9-0 loss: 0.461114  [   32/  179]
train() client id: f_00007-9-1 loss: 0.384637  [   64/  179]
train() client id: f_00007-9-2 loss: 0.446321  [   96/  179]
train() client id: f_00007-9-3 loss: 0.581589  [  128/  179]
train() client id: f_00007-9-4 loss: 0.577414  [  160/  179]
train() client id: f_00007-10-0 loss: 0.406536  [   32/  179]
train() client id: f_00007-10-1 loss: 0.612140  [   64/  179]
train() client id: f_00007-10-2 loss: 0.485512  [   96/  179]
train() client id: f_00007-10-3 loss: 0.403697  [  128/  179]
train() client id: f_00007-10-4 loss: 0.462233  [  160/  179]
train() client id: f_00008-0-0 loss: 0.688774  [   32/  130]
train() client id: f_00008-0-1 loss: 0.660562  [   64/  130]
train() client id: f_00008-0-2 loss: 0.767472  [   96/  130]
train() client id: f_00008-0-3 loss: 0.551847  [  128/  130]
train() client id: f_00008-1-0 loss: 0.671676  [   32/  130]
train() client id: f_00008-1-1 loss: 0.676619  [   64/  130]
train() client id: f_00008-1-2 loss: 0.626692  [   96/  130]
train() client id: f_00008-1-3 loss: 0.680241  [  128/  130]
train() client id: f_00008-2-0 loss: 0.692954  [   32/  130]
train() client id: f_00008-2-1 loss: 0.727475  [   64/  130]
train() client id: f_00008-2-2 loss: 0.657457  [   96/  130]
train() client id: f_00008-2-3 loss: 0.625517  [  128/  130]
train() client id: f_00008-3-0 loss: 0.621988  [   32/  130]
train() client id: f_00008-3-1 loss: 0.597816  [   64/  130]
train() client id: f_00008-3-2 loss: 0.828028  [   96/  130]
train() client id: f_00008-3-3 loss: 0.619390  [  128/  130]
train() client id: f_00008-4-0 loss: 0.651599  [   32/  130]
train() client id: f_00008-4-1 loss: 0.704110  [   64/  130]
train() client id: f_00008-4-2 loss: 0.737367  [   96/  130]
train() client id: f_00008-4-3 loss: 0.606888  [  128/  130]
train() client id: f_00008-5-0 loss: 0.629148  [   32/  130]
train() client id: f_00008-5-1 loss: 0.607838  [   64/  130]
train() client id: f_00008-5-2 loss: 0.651441  [   96/  130]
train() client id: f_00008-5-3 loss: 0.819701  [  128/  130]
train() client id: f_00008-6-0 loss: 0.744677  [   32/  130]
train() client id: f_00008-6-1 loss: 0.660929  [   64/  130]
train() client id: f_00008-6-2 loss: 0.656659  [   96/  130]
train() client id: f_00008-6-3 loss: 0.647080  [  128/  130]
train() client id: f_00008-7-0 loss: 0.761479  [   32/  130]
train() client id: f_00008-7-1 loss: 0.661950  [   64/  130]
train() client id: f_00008-7-2 loss: 0.691490  [   96/  130]
train() client id: f_00008-7-3 loss: 0.572509  [  128/  130]
train() client id: f_00008-8-0 loss: 0.669364  [   32/  130]
train() client id: f_00008-8-1 loss: 0.696077  [   64/  130]
train() client id: f_00008-8-2 loss: 0.634924  [   96/  130]
train() client id: f_00008-8-3 loss: 0.703281  [  128/  130]
train() client id: f_00008-9-0 loss: 0.655001  [   32/  130]
train() client id: f_00008-9-1 loss: 0.623230  [   64/  130]
train() client id: f_00008-9-2 loss: 0.751716  [   96/  130]
train() client id: f_00008-9-3 loss: 0.687780  [  128/  130]
train() client id: f_00008-10-0 loss: 0.605347  [   32/  130]
train() client id: f_00008-10-1 loss: 0.764657  [   64/  130]
train() client id: f_00008-10-2 loss: 0.761414  [   96/  130]
train() client id: f_00008-10-3 loss: 0.579903  [  128/  130]
train() client id: f_00009-0-0 loss: 0.849016  [   32/  118]
train() client id: f_00009-0-1 loss: 1.020279  [   64/  118]
train() client id: f_00009-0-2 loss: 0.727706  [   96/  118]
train() client id: f_00009-1-0 loss: 0.721020  [   32/  118]
train() client id: f_00009-1-1 loss: 0.861679  [   64/  118]
train() client id: f_00009-1-2 loss: 0.945241  [   96/  118]
train() client id: f_00009-2-0 loss: 0.792100  [   32/  118]
train() client id: f_00009-2-1 loss: 0.955107  [   64/  118]
train() client id: f_00009-2-2 loss: 0.897504  [   96/  118]
train() client id: f_00009-3-0 loss: 0.846187  [   32/  118]
train() client id: f_00009-3-1 loss: 0.758233  [   64/  118]
train() client id: f_00009-3-2 loss: 0.869308  [   96/  118]
train() client id: f_00009-4-0 loss: 0.808711  [   32/  118]
train() client id: f_00009-4-1 loss: 0.717202  [   64/  118]
train() client id: f_00009-4-2 loss: 0.916386  [   96/  118]
train() client id: f_00009-5-0 loss: 0.858059  [   32/  118]
train() client id: f_00009-5-1 loss: 0.744608  [   64/  118]
train() client id: f_00009-5-2 loss: 0.635784  [   96/  118]
train() client id: f_00009-6-0 loss: 0.724860  [   32/  118]
train() client id: f_00009-6-1 loss: 0.659244  [   64/  118]
train() client id: f_00009-6-2 loss: 0.807566  [   96/  118]
train() client id: f_00009-7-0 loss: 0.686473  [   32/  118]
train() client id: f_00009-7-1 loss: 0.796120  [   64/  118]
train() client id: f_00009-7-2 loss: 0.766826  [   96/  118]
train() client id: f_00009-8-0 loss: 0.703164  [   32/  118]
train() client id: f_00009-8-1 loss: 0.804758  [   64/  118]
train() client id: f_00009-8-2 loss: 0.667396  [   96/  118]
train() client id: f_00009-9-0 loss: 0.655130  [   32/  118]
train() client id: f_00009-9-1 loss: 0.664168  [   64/  118]
train() client id: f_00009-9-2 loss: 0.866876  [   96/  118]
train() client id: f_00009-10-0 loss: 0.927500  [   32/  118]
train() client id: f_00009-10-1 loss: 0.735848  [   64/  118]
train() client id: f_00009-10-2 loss: 0.590549  [   96/  118]
At round 55 accuracy: 0.6472148541114059
At round 55 training accuracy: 0.5848423876592891
At round 55 training loss: 0.8359528645086871
update_location
xs = [  -3.9056584     4.20031788  295.00902392   18.81129433    0.97929623
    3.95640986 -257.44319194 -236.32485185  279.66397685 -222.06087855]
ys = [ 287.5879595   270.55583871    1.32061395 -257.45517586  249.35018685
  232.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [304.50301906 288.47548341 311.49970821 276.83394368 268.65679724
 253.41286859 276.19537943 256.61276636 297.52413334 243.57143847]
dists_bs = [204.81533719 203.08538164 501.07186281 473.89903665 191.04099818
 188.06348039 195.82521531 184.65804049 481.14846951 177.35619392]
uav_gains = [2.45442945e-12 3.35809681e-12 2.16122255e-12 4.27299009e-12
 5.07197201e-12 6.93588738e-12 4.33052174e-12 6.50389323e-12
 2.80382037e-12 8.39484877e-12]
bs_gains = [3.72802792e-11 3.81762994e-11 3.04490705e-12 3.55937522e-12
 4.53043251e-11 4.73414518e-11 4.22728929e-11 4.98268075e-11
 3.41124240e-12 5.57858785e-11]
Round 56
-------------------------------
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.61572451 7.33497615 3.57029297 1.30775167 8.45662183 4.06918315
 1.60870824 5.01890964 3.70473634 3.29930761]
obj_prev = 41.986212112103765
eta_min = 2.748442212366637e-26	eta_max = 0.9375089458427088
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 9.666975258494574	eta = 0.909090909090909
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 21.262798910283387	eta = 0.41331150066296846
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 15.102037364580543	eta = 0.5819187910708925
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 14.000878826985186	eta = 0.6276862641626415
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 13.93490538719279	eta = 0.6306579830804683
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 13.934640394529897	eta = 0.6306699761950073
eta = 0.6306699761950073
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [0.03803835 0.0800013  0.03743457 0.01298134 0.09237885 0.04407619
 0.01630215 0.05403861 0.03924592 0.03562322]
ene_total = [1.3587627  2.13524871 1.36925661 0.66156516 2.4302258  1.25152696
 0.7407784  1.61476761 1.33281256 1.03969587]
ti_comp = [0.93897812 1.03996482 0.92863396 0.97451355 1.04269211 1.04336226
 0.97521529 0.99392524 0.96170282 1.04576067]
ti_coms = [0.17720525 0.07621855 0.18754941 0.14166982 0.07349126 0.07282111
 0.14096808 0.12225813 0.15448054 0.0704227 ]
t_total = [27.19976501 27.19976501 27.19976501 27.19976501 27.19976501 27.19976501
 27.19976501 27.19976501 27.19976501 27.19976501]
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [3.90152269e-06 2.95892377e-05 3.80197612e-06 1.43967031e-07
 4.53195465e-05 4.91611198e-06 2.84717213e-07 9.98355239e-06
 4.08490276e-06 2.58353803e-06]
ene_total = [0.43191848 0.18645457 0.45712329 0.34523224 0.18019189 0.17757424
 0.34352564 0.29816856 0.37654616 0.17167282]
optimize_network iter = 0 obj = 2.96840789621886
eta = 0.6306699761950073
freqs = [20255185.25779217 38463462.35602392 20155718.00296714  6660421.60236042
 44298241.69414774 21122186.81412296  8358231.69852659 27184444.26259059
 20404389.67271167 17032203.97092095]
eta_min = 0.6306699761950091	eta_max = 0.7038160577019408
af = 0.0020423891099591697	bf = 1.0953806736156944	zeta = 0.0022466280209550867	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [8.06674524e-07 6.11783813e-06 7.86092385e-07 2.97664645e-08
 9.37021941e-06 1.01644989e-06 5.88678167e-08 2.06418827e-06
 8.44589985e-07 5.34169471e-07]
ene_total = [1.75441871 0.75517231 1.85682433 1.40253966 0.72849401 0.72103243
 1.3955953  1.21056458 1.52944703 0.69724031]
ti_comp = [0.71791719 0.81890388 0.70757302 0.75345262 0.82163117 0.82230133
 0.75415436 0.77286431 0.74064189 0.82469974]
ti_coms = [0.17720525 0.07621855 0.18754941 0.14166982 0.07349126 0.07282111
 0.14096808 0.12225813 0.15448054 0.0704227 ]
t_total = [27.19976501 27.19976501 27.19976501 27.19976501 27.19976501 27.19976501
 27.19976501 27.19976501 27.19976501 27.19976501]
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [2.95240886e-06 2.11098588e-05 2.89691726e-06 1.06538614e-07
 3.22867276e-05 3.50114543e-06 2.10607755e-07 7.30409071e-06
 3.04668167e-06 1.83767148e-06]
ene_total = [0.53855695 0.23224405 0.56998772 0.43049024 0.22429635 0.22138528
 0.42836106 0.37172334 0.46950708 0.21404676]
optimize_network iter = 1 obj = 3.70059882926673
eta = 0.7038160577019408
freqs = [20185710.85684811 37218665.59464449 20155718.00296712  6563875.93252434
 42834368.86978544 20420657.36097779  8235336.89300485 26637755.85065275
 20187518.35748624 16456368.51125106]
eta_min = 0.7038160577019519	eta_max = 0.703816057701941
af = 0.0019282538903956608	bf = 1.0953806736156944	zeta = 0.0021210792794352272	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [8.01150298e-07 5.72826139e-06 7.86092385e-07 2.89097636e-08
 8.76115832e-06 9.50052597e-07 5.71494239e-08 1.98200003e-06
 8.26731677e-07 4.98660964e-07]
ene_total = [1.75441816 0.75513375 1.85682433 1.40253957 0.72843371 0.72102585
 1.39559513 1.21055645 1.52944526 0.6972368 ]
ti_comp = [0.71791719 0.81890388 0.70757302 0.75345262 0.82163117 0.82230133
 0.75415436 0.77286431 0.74064189 0.82469974]
ti_coms = [0.17720525 0.07621855 0.18754941 0.14166982 0.07349126 0.07282111
 0.14096808 0.12225813 0.15448054 0.0704227 ]
t_total = [27.19976501 27.19976501 27.19976501 27.19976501 27.19976501 27.19976501
 27.19976501 27.19976501 27.19976501 27.19976501]
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [2.95240886e-06 2.11098588e-05 2.89691726e-06 1.06538614e-07
 3.22867276e-05 3.50114543e-06 2.10607755e-07 7.30409071e-06
 3.04668167e-06 1.83767148e-06]
ene_total = [0.53855695 0.23224405 0.56998772 0.43049024 0.22429635 0.22138528
 0.42836106 0.37172334 0.46950708 0.21404676]
optimize_network iter = 2 obj = 3.7005988292667333
eta = 0.703816057701941
freqs = [20185710.85684812 37218665.5946445  20155718.00296713  6563875.93252435
 42834368.86978545 20420657.36097779  8235336.89300485 26637755.85065275
 20187518.35748625 16456368.51125106]
Done!
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [2.82371805e-06 2.01897137e-05 2.77064524e-06 1.01894765e-07
 3.08794005e-05 3.34853607e-06 2.01427698e-07 6.98571701e-06
 2.91388167e-06 1.75757030e-06]
ene_total = [0.01772335 0.00764205 0.01875771 0.01416708 0.00738001 0.00728546
 0.01409701 0.0122328  0.01545097 0.00704403]
At round 56 energy consumption: 0.12178045716881733
At round 56 eta: 0.703816057701941
At round 56 a_n: 8.65748957900329
At round 56 local rounds: 11.50132443724633
At round 56 global rounds: 30.386642017599986
gradient difference: 0.5666335821151733
train() client id: f_00000-0-0 loss: 1.242628  [   32/  126]
train() client id: f_00000-0-1 loss: 1.111049  [   64/  126]
train() client id: f_00000-0-2 loss: 1.251618  [   96/  126]
train() client id: f_00000-1-0 loss: 1.219596  [   32/  126]
train() client id: f_00000-1-1 loss: 1.203585  [   64/  126]
train() client id: f_00000-1-2 loss: 0.859964  [   96/  126]
train() client id: f_00000-2-0 loss: 0.991850  [   32/  126]
train() client id: f_00000-2-1 loss: 1.138229  [   64/  126]
train() client id: f_00000-2-2 loss: 1.028113  [   96/  126]
train() client id: f_00000-3-0 loss: 0.985831  [   32/  126]
train() client id: f_00000-3-1 loss: 1.090631  [   64/  126]
train() client id: f_00000-3-2 loss: 0.970479  [   96/  126]
train() client id: f_00000-4-0 loss: 0.884769  [   32/  126]
train() client id: f_00000-4-1 loss: 0.978488  [   64/  126]
train() client id: f_00000-4-2 loss: 0.772569  [   96/  126]
train() client id: f_00000-5-0 loss: 0.906039  [   32/  126]
train() client id: f_00000-5-1 loss: 0.960985  [   64/  126]
train() client id: f_00000-5-2 loss: 0.867909  [   96/  126]
train() client id: f_00000-6-0 loss: 0.881341  [   32/  126]
train() client id: f_00000-6-1 loss: 0.979037  [   64/  126]
train() client id: f_00000-6-2 loss: 0.856202  [   96/  126]
train() client id: f_00000-7-0 loss: 0.922145  [   32/  126]
train() client id: f_00000-7-1 loss: 0.735378  [   64/  126]
train() client id: f_00000-7-2 loss: 1.033801  [   96/  126]
train() client id: f_00000-8-0 loss: 0.858828  [   32/  126]
train() client id: f_00000-8-1 loss: 0.817344  [   64/  126]
train() client id: f_00000-8-2 loss: 0.958094  [   96/  126]
train() client id: f_00000-9-0 loss: 0.921217  [   32/  126]
train() client id: f_00000-9-1 loss: 0.834705  [   64/  126]
train() client id: f_00000-9-2 loss: 0.816098  [   96/  126]
train() client id: f_00000-10-0 loss: 0.818082  [   32/  126]
train() client id: f_00000-10-1 loss: 0.902395  [   64/  126]
train() client id: f_00000-10-2 loss: 0.781793  [   96/  126]
train() client id: f_00001-0-0 loss: 0.417986  [   32/  265]
train() client id: f_00001-0-1 loss: 0.420815  [   64/  265]
train() client id: f_00001-0-2 loss: 0.345262  [   96/  265]
train() client id: f_00001-0-3 loss: 0.308507  [  128/  265]
train() client id: f_00001-0-4 loss: 0.489231  [  160/  265]
train() client id: f_00001-0-5 loss: 0.412784  [  192/  265]
train() client id: f_00001-0-6 loss: 0.368076  [  224/  265]
train() client id: f_00001-0-7 loss: 0.370120  [  256/  265]
train() client id: f_00001-1-0 loss: 0.445467  [   32/  265]
train() client id: f_00001-1-1 loss: 0.301352  [   64/  265]
train() client id: f_00001-1-2 loss: 0.309757  [   96/  265]
train() client id: f_00001-1-3 loss: 0.400234  [  128/  265]
train() client id: f_00001-1-4 loss: 0.351731  [  160/  265]
train() client id: f_00001-1-5 loss: 0.390870  [  192/  265]
train() client id: f_00001-1-6 loss: 0.429666  [  224/  265]
train() client id: f_00001-1-7 loss: 0.469039  [  256/  265]
train() client id: f_00001-2-0 loss: 0.362980  [   32/  265]
train() client id: f_00001-2-1 loss: 0.432656  [   64/  265]
train() client id: f_00001-2-2 loss: 0.444731  [   96/  265]
train() client id: f_00001-2-3 loss: 0.326250  [  128/  265]
train() client id: f_00001-2-4 loss: 0.343773  [  160/  265]
train() client id: f_00001-2-5 loss: 0.429013  [  192/  265]
train() client id: f_00001-2-6 loss: 0.351333  [  224/  265]
train() client id: f_00001-2-7 loss: 0.362524  [  256/  265]
train() client id: f_00001-3-0 loss: 0.426923  [   32/  265]
train() client id: f_00001-3-1 loss: 0.373178  [   64/  265]
train() client id: f_00001-3-2 loss: 0.380115  [   96/  265]
train() client id: f_00001-3-3 loss: 0.364223  [  128/  265]
train() client id: f_00001-3-4 loss: 0.336194  [  160/  265]
train() client id: f_00001-3-5 loss: 0.473754  [  192/  265]
train() client id: f_00001-3-6 loss: 0.290322  [  224/  265]
train() client id: f_00001-3-7 loss: 0.348945  [  256/  265]
train() client id: f_00001-4-0 loss: 0.395956  [   32/  265]
train() client id: f_00001-4-1 loss: 0.331114  [   64/  265]
train() client id: f_00001-4-2 loss: 0.476230  [   96/  265]
train() client id: f_00001-4-3 loss: 0.291876  [  128/  265]
train() client id: f_00001-4-4 loss: 0.265892  [  160/  265]
train() client id: f_00001-4-5 loss: 0.445845  [  192/  265]
train() client id: f_00001-4-6 loss: 0.278267  [  224/  265]
train() client id: f_00001-4-7 loss: 0.493097  [  256/  265]
train() client id: f_00001-5-0 loss: 0.308767  [   32/  265]
train() client id: f_00001-5-1 loss: 0.377823  [   64/  265]
train() client id: f_00001-5-2 loss: 0.492469  [   96/  265]
train() client id: f_00001-5-3 loss: 0.380345  [  128/  265]
train() client id: f_00001-5-4 loss: 0.355306  [  160/  265]
train() client id: f_00001-5-5 loss: 0.406328  [  192/  265]
train() client id: f_00001-5-6 loss: 0.343727  [  224/  265]
train() client id: f_00001-5-7 loss: 0.283298  [  256/  265]
train() client id: f_00001-6-0 loss: 0.349892  [   32/  265]
train() client id: f_00001-6-1 loss: 0.443866  [   64/  265]
train() client id: f_00001-6-2 loss: 0.275609  [   96/  265]
train() client id: f_00001-6-3 loss: 0.399126  [  128/  265]
train() client id: f_00001-6-4 loss: 0.362227  [  160/  265]
train() client id: f_00001-6-5 loss: 0.340688  [  192/  265]
train() client id: f_00001-6-6 loss: 0.357346  [  224/  265]
train() client id: f_00001-6-7 loss: 0.397980  [  256/  265]
train() client id: f_00001-7-0 loss: 0.303131  [   32/  265]
train() client id: f_00001-7-1 loss: 0.452612  [   64/  265]
train() client id: f_00001-7-2 loss: 0.428512  [   96/  265]
train() client id: f_00001-7-3 loss: 0.368373  [  128/  265]
train() client id: f_00001-7-4 loss: 0.420620  [  160/  265]
train() client id: f_00001-7-5 loss: 0.272261  [  192/  265]
train() client id: f_00001-7-6 loss: 0.276169  [  224/  265]
train() client id: f_00001-7-7 loss: 0.390947  [  256/  265]
train() client id: f_00001-8-0 loss: 0.380198  [   32/  265]
train() client id: f_00001-8-1 loss: 0.388057  [   64/  265]
train() client id: f_00001-8-2 loss: 0.371455  [   96/  265]
train() client id: f_00001-8-3 loss: 0.308719  [  128/  265]
train() client id: f_00001-8-4 loss: 0.304812  [  160/  265]
train() client id: f_00001-8-5 loss: 0.480985  [  192/  265]
train() client id: f_00001-8-6 loss: 0.346504  [  224/  265]
train() client id: f_00001-8-7 loss: 0.312318  [  256/  265]
train() client id: f_00001-9-0 loss: 0.356155  [   32/  265]
train() client id: f_00001-9-1 loss: 0.361865  [   64/  265]
train() client id: f_00001-9-2 loss: 0.451263  [   96/  265]
train() client id: f_00001-9-3 loss: 0.321119  [  128/  265]
train() client id: f_00001-9-4 loss: 0.264335  [  160/  265]
train() client id: f_00001-9-5 loss: 0.472247  [  192/  265]
train() client id: f_00001-9-6 loss: 0.368632  [  224/  265]
train() client id: f_00001-9-7 loss: 0.299748  [  256/  265]
train() client id: f_00001-10-0 loss: 0.428869  [   32/  265]
train() client id: f_00001-10-1 loss: 0.310113  [   64/  265]
train() client id: f_00001-10-2 loss: 0.328834  [   96/  265]
train() client id: f_00001-10-3 loss: 0.277680  [  128/  265]
train() client id: f_00001-10-4 loss: 0.364917  [  160/  265]
train() client id: f_00001-10-5 loss: 0.365572  [  192/  265]
train() client id: f_00001-10-6 loss: 0.334613  [  224/  265]
train() client id: f_00001-10-7 loss: 0.389193  [  256/  265]
train() client id: f_00002-0-0 loss: 1.232933  [   32/  124]
train() client id: f_00002-0-1 loss: 1.197560  [   64/  124]
train() client id: f_00002-0-2 loss: 1.069310  [   96/  124]
train() client id: f_00002-1-0 loss: 1.104861  [   32/  124]
train() client id: f_00002-1-1 loss: 1.081405  [   64/  124]
train() client id: f_00002-1-2 loss: 1.216152  [   96/  124]
train() client id: f_00002-2-0 loss: 0.901516  [   32/  124]
train() client id: f_00002-2-1 loss: 1.152613  [   64/  124]
train() client id: f_00002-2-2 loss: 1.116036  [   96/  124]
train() client id: f_00002-3-0 loss: 1.075349  [   32/  124]
train() client id: f_00002-3-1 loss: 0.975280  [   64/  124]
train() client id: f_00002-3-2 loss: 1.225092  [   96/  124]
train() client id: f_00002-4-0 loss: 1.113159  [   32/  124]
train() client id: f_00002-4-1 loss: 1.073250  [   64/  124]
train() client id: f_00002-4-2 loss: 0.923435  [   96/  124]
train() client id: f_00002-5-0 loss: 1.191224  [   32/  124]
train() client id: f_00002-5-1 loss: 1.090881  [   64/  124]
train() client id: f_00002-5-2 loss: 0.817644  [   96/  124]
train() client id: f_00002-6-0 loss: 1.021633  [   32/  124]
train() client id: f_00002-6-1 loss: 1.104237  [   64/  124]
train() client id: f_00002-6-2 loss: 1.039101  [   96/  124]
train() client id: f_00002-7-0 loss: 1.357327  [   32/  124]
train() client id: f_00002-7-1 loss: 0.856023  [   64/  124]
train() client id: f_00002-7-2 loss: 0.979012  [   96/  124]
train() client id: f_00002-8-0 loss: 1.090936  [   32/  124]
train() client id: f_00002-8-1 loss: 0.957662  [   64/  124]
train() client id: f_00002-8-2 loss: 0.928154  [   96/  124]
train() client id: f_00002-9-0 loss: 0.959661  [   32/  124]
train() client id: f_00002-9-1 loss: 1.001080  [   64/  124]
train() client id: f_00002-9-2 loss: 0.929740  [   96/  124]
train() client id: f_00002-10-0 loss: 0.954821  [   32/  124]
train() client id: f_00002-10-1 loss: 1.197293  [   64/  124]
train() client id: f_00002-10-2 loss: 0.951373  [   96/  124]
train() client id: f_00003-0-0 loss: 0.480212  [   32/   43]
train() client id: f_00003-1-0 loss: 0.631805  [   32/   43]
train() client id: f_00003-2-0 loss: 0.748533  [   32/   43]
train() client id: f_00003-3-0 loss: 0.490770  [   32/   43]
train() client id: f_00003-4-0 loss: 0.738122  [   32/   43]
train() client id: f_00003-5-0 loss: 0.379452  [   32/   43]
train() client id: f_00003-6-0 loss: 0.366039  [   32/   43]
train() client id: f_00003-7-0 loss: 0.478385  [   32/   43]
train() client id: f_00003-8-0 loss: 0.562713  [   32/   43]
train() client id: f_00003-9-0 loss: 0.530432  [   32/   43]
train() client id: f_00003-10-0 loss: 0.681070  [   32/   43]
train() client id: f_00004-0-0 loss: 1.063143  [   32/  306]
train() client id: f_00004-0-1 loss: 0.867139  [   64/  306]
train() client id: f_00004-0-2 loss: 1.007334  [   96/  306]
train() client id: f_00004-0-3 loss: 0.867331  [  128/  306]
train() client id: f_00004-0-4 loss: 0.992229  [  160/  306]
train() client id: f_00004-0-5 loss: 0.955408  [  192/  306]
train() client id: f_00004-0-6 loss: 0.877625  [  224/  306]
train() client id: f_00004-0-7 loss: 0.953331  [  256/  306]
train() client id: f_00004-0-8 loss: 0.851553  [  288/  306]
train() client id: f_00004-1-0 loss: 1.138726  [   32/  306]
train() client id: f_00004-1-1 loss: 0.806309  [   64/  306]
train() client id: f_00004-1-2 loss: 0.977829  [   96/  306]
train() client id: f_00004-1-3 loss: 0.890649  [  128/  306]
train() client id: f_00004-1-4 loss: 1.058682  [  160/  306]
train() client id: f_00004-1-5 loss: 0.978271  [  192/  306]
train() client id: f_00004-1-6 loss: 0.912893  [  224/  306]
train() client id: f_00004-1-7 loss: 0.843717  [  256/  306]
train() client id: f_00004-1-8 loss: 0.795149  [  288/  306]
train() client id: f_00004-2-0 loss: 1.168555  [   32/  306]
train() client id: f_00004-2-1 loss: 1.015562  [   64/  306]
train() client id: f_00004-2-2 loss: 0.901007  [   96/  306]
train() client id: f_00004-2-3 loss: 0.833645  [  128/  306]
train() client id: f_00004-2-4 loss: 0.906587  [  160/  306]
train() client id: f_00004-2-5 loss: 0.897947  [  192/  306]
train() client id: f_00004-2-6 loss: 0.802554  [  224/  306]
train() client id: f_00004-2-7 loss: 0.899550  [  256/  306]
train() client id: f_00004-2-8 loss: 0.898304  [  288/  306]
train() client id: f_00004-3-0 loss: 0.999431  [   32/  306]
train() client id: f_00004-3-1 loss: 0.874542  [   64/  306]
train() client id: f_00004-3-2 loss: 0.849790  [   96/  306]
train() client id: f_00004-3-3 loss: 0.911506  [  128/  306]
train() client id: f_00004-3-4 loss: 0.946888  [  160/  306]
train() client id: f_00004-3-5 loss: 0.789016  [  192/  306]
train() client id: f_00004-3-6 loss: 1.050272  [  224/  306]
train() client id: f_00004-3-7 loss: 0.857119  [  256/  306]
train() client id: f_00004-3-8 loss: 0.992542  [  288/  306]
train() client id: f_00004-4-0 loss: 0.950822  [   32/  306]
train() client id: f_00004-4-1 loss: 1.014634  [   64/  306]
train() client id: f_00004-4-2 loss: 0.795338  [   96/  306]
train() client id: f_00004-4-3 loss: 0.951577  [  128/  306]
train() client id: f_00004-4-4 loss: 0.924461  [  160/  306]
train() client id: f_00004-4-5 loss: 0.754105  [  192/  306]
train() client id: f_00004-4-6 loss: 0.874427  [  224/  306]
train() client id: f_00004-4-7 loss: 0.904911  [  256/  306]
train() client id: f_00004-4-8 loss: 1.023833  [  288/  306]
train() client id: f_00004-5-0 loss: 0.949988  [   32/  306]
train() client id: f_00004-5-1 loss: 0.844582  [   64/  306]
train() client id: f_00004-5-2 loss: 0.915850  [   96/  306]
train() client id: f_00004-5-3 loss: 0.943921  [  128/  306]
train() client id: f_00004-5-4 loss: 0.884770  [  160/  306]
train() client id: f_00004-5-5 loss: 0.916668  [  192/  306]
train() client id: f_00004-5-6 loss: 0.932855  [  224/  306]
train() client id: f_00004-5-7 loss: 1.024069  [  256/  306]
train() client id: f_00004-5-8 loss: 0.875546  [  288/  306]
train() client id: f_00004-6-0 loss: 0.953774  [   32/  306]
train() client id: f_00004-6-1 loss: 0.792935  [   64/  306]
train() client id: f_00004-6-2 loss: 0.955300  [   96/  306]
train() client id: f_00004-6-3 loss: 0.904802  [  128/  306]
train() client id: f_00004-6-4 loss: 1.000530  [  160/  306]
train() client id: f_00004-6-5 loss: 0.885108  [  192/  306]
train() client id: f_00004-6-6 loss: 0.912352  [  224/  306]
train() client id: f_00004-6-7 loss: 0.751827  [  256/  306]
train() client id: f_00004-6-8 loss: 1.043224  [  288/  306]
train() client id: f_00004-7-0 loss: 0.871496  [   32/  306]
train() client id: f_00004-7-1 loss: 0.962168  [   64/  306]
train() client id: f_00004-7-2 loss: 0.845044  [   96/  306]
train() client id: f_00004-7-3 loss: 0.987151  [  128/  306]
train() client id: f_00004-7-4 loss: 0.788845  [  160/  306]
train() client id: f_00004-7-5 loss: 0.817073  [  192/  306]
train() client id: f_00004-7-6 loss: 0.918582  [  224/  306]
train() client id: f_00004-7-7 loss: 1.029368  [  256/  306]
train() client id: f_00004-7-8 loss: 0.926758  [  288/  306]
train() client id: f_00004-8-0 loss: 1.002872  [   32/  306]
train() client id: f_00004-8-1 loss: 0.956814  [   64/  306]
train() client id: f_00004-8-2 loss: 0.852882  [   96/  306]
train() client id: f_00004-8-3 loss: 0.840491  [  128/  306]
train() client id: f_00004-8-4 loss: 0.965624  [  160/  306]
train() client id: f_00004-8-5 loss: 0.882879  [  192/  306]
train() client id: f_00004-8-6 loss: 0.860887  [  224/  306]
train() client id: f_00004-8-7 loss: 0.848540  [  256/  306]
train() client id: f_00004-8-8 loss: 0.937864  [  288/  306]
train() client id: f_00004-9-0 loss: 0.906003  [   32/  306]
train() client id: f_00004-9-1 loss: 0.974830  [   64/  306]
train() client id: f_00004-9-2 loss: 0.772244  [   96/  306]
train() client id: f_00004-9-3 loss: 0.877174  [  128/  306]
train() client id: f_00004-9-4 loss: 0.916817  [  160/  306]
train() client id: f_00004-9-5 loss: 1.041998  [  192/  306]
train() client id: f_00004-9-6 loss: 0.913610  [  224/  306]
train() client id: f_00004-9-7 loss: 0.965698  [  256/  306]
train() client id: f_00004-9-8 loss: 0.842788  [  288/  306]
train() client id: f_00004-10-0 loss: 1.015827  [   32/  306]
train() client id: f_00004-10-1 loss: 0.843846  [   64/  306]
train() client id: f_00004-10-2 loss: 0.817285  [   96/  306]
train() client id: f_00004-10-3 loss: 0.948195  [  128/  306]
train() client id: f_00004-10-4 loss: 0.809015  [  160/  306]
train() client id: f_00004-10-5 loss: 0.834696  [  192/  306]
train() client id: f_00004-10-6 loss: 1.063149  [  224/  306]
train() client id: f_00004-10-7 loss: 0.961911  [  256/  306]
train() client id: f_00004-10-8 loss: 0.860332  [  288/  306]
train() client id: f_00005-0-0 loss: -0.027535  [   32/  146]
train() client id: f_00005-0-1 loss: 0.379501  [   64/  146]
train() client id: f_00005-0-2 loss: 0.205749  [   96/  146]
train() client id: f_00005-0-3 loss: 0.295147  [  128/  146]
train() client id: f_00005-1-0 loss: 0.104518  [   32/  146]
train() client id: f_00005-1-1 loss: 0.446398  [   64/  146]
train() client id: f_00005-1-2 loss: 0.248522  [   96/  146]
train() client id: f_00005-1-3 loss: 0.013832  [  128/  146]
train() client id: f_00005-2-0 loss: 0.162428  [   32/  146]
train() client id: f_00005-2-1 loss: 0.238202  [   64/  146]
train() client id: f_00005-2-2 loss: 0.213493  [   96/  146]
train() client id: f_00005-2-3 loss: 0.194444  [  128/  146]
train() client id: f_00005-3-0 loss: 0.045460  [   32/  146]
train() client id: f_00005-3-1 loss: 0.158644  [   64/  146]
train() client id: f_00005-3-2 loss: 0.427697  [   96/  146]
train() client id: f_00005-3-3 loss: 0.110106  [  128/  146]
train() client id: f_00005-4-0 loss: 0.262787  [   32/  146]
train() client id: f_00005-4-1 loss: 0.264098  [   64/  146]
train() client id: f_00005-4-2 loss: -0.097164  [   96/  146]
train() client id: f_00005-4-3 loss: 0.465451  [  128/  146]
train() client id: f_00005-5-0 loss: 0.430987  [   32/  146]
train() client id: f_00005-5-1 loss: 0.097240  [   64/  146]
train() client id: f_00005-5-2 loss: 0.318719  [   96/  146]
train() client id: f_00005-5-3 loss: 0.165507  [  128/  146]
train() client id: f_00005-6-0 loss: 0.038989  [   32/  146]
train() client id: f_00005-6-1 loss: 0.136480  [   64/  146]
train() client id: f_00005-6-2 loss: 0.300954  [   96/  146]
train() client id: f_00005-6-3 loss: 0.316446  [  128/  146]
train() client id: f_00005-7-0 loss: 0.118020  [   32/  146]
train() client id: f_00005-7-1 loss: 0.488137  [   64/  146]
train() client id: f_00005-7-2 loss: 0.254689  [   96/  146]
train() client id: f_00005-7-3 loss: 0.033509  [  128/  146]
train() client id: f_00005-8-0 loss: 0.384807  [   32/  146]
train() client id: f_00005-8-1 loss: 0.261024  [   64/  146]
train() client id: f_00005-8-2 loss: 0.028696  [   96/  146]
train() client id: f_00005-8-3 loss: 0.325483  [  128/  146]
train() client id: f_00005-9-0 loss: 0.160430  [   32/  146]
train() client id: f_00005-9-1 loss: 0.105832  [   64/  146]
train() client id: f_00005-9-2 loss: 0.190545  [   96/  146]
train() client id: f_00005-9-3 loss: 0.331689  [  128/  146]
train() client id: f_00005-10-0 loss: 0.133380  [   32/  146]
train() client id: f_00005-10-1 loss: -0.036073  [   64/  146]
train() client id: f_00005-10-2 loss: 0.316527  [   96/  146]
train() client id: f_00005-10-3 loss: 0.235103  [  128/  146]
train() client id: f_00006-0-0 loss: 0.563580  [   32/   54]
train() client id: f_00006-1-0 loss: 0.523763  [   32/   54]
train() client id: f_00006-2-0 loss: 0.474879  [   32/   54]
train() client id: f_00006-3-0 loss: 0.561497  [   32/   54]
train() client id: f_00006-4-0 loss: 0.522768  [   32/   54]
train() client id: f_00006-5-0 loss: 0.498378  [   32/   54]
train() client id: f_00006-6-0 loss: 0.570573  [   32/   54]
train() client id: f_00006-7-0 loss: 0.531191  [   32/   54]
train() client id: f_00006-8-0 loss: 0.572711  [   32/   54]
train() client id: f_00006-9-0 loss: 0.472614  [   32/   54]
train() client id: f_00006-10-0 loss: 0.476707  [   32/   54]
train() client id: f_00007-0-0 loss: 0.468771  [   32/  179]
train() client id: f_00007-0-1 loss: 0.633442  [   64/  179]
train() client id: f_00007-0-2 loss: 0.740673  [   96/  179]
train() client id: f_00007-0-3 loss: 0.646153  [  128/  179]
train() client id: f_00007-0-4 loss: 0.710266  [  160/  179]
train() client id: f_00007-1-0 loss: 0.460556  [   32/  179]
train() client id: f_00007-1-1 loss: 0.783095  [   64/  179]
train() client id: f_00007-1-2 loss: 0.465797  [   96/  179]
train() client id: f_00007-1-3 loss: 0.582272  [  128/  179]
train() client id: f_00007-1-4 loss: 0.643921  [  160/  179]
train() client id: f_00007-2-0 loss: 0.665125  [   32/  179]
train() client id: f_00007-2-1 loss: 0.548510  [   64/  179]
train() client id: f_00007-2-2 loss: 0.723532  [   96/  179]
train() client id: f_00007-2-3 loss: 0.582843  [  128/  179]
train() client id: f_00007-2-4 loss: 0.444259  [  160/  179]
train() client id: f_00007-3-0 loss: 0.433758  [   32/  179]
train() client id: f_00007-3-1 loss: 0.734003  [   64/  179]
train() client id: f_00007-3-2 loss: 0.641008  [   96/  179]
train() client id: f_00007-3-3 loss: 0.639024  [  128/  179]
train() client id: f_00007-3-4 loss: 0.567247  [  160/  179]
train() client id: f_00007-4-0 loss: 0.528301  [   32/  179]
train() client id: f_00007-4-1 loss: 0.511589  [   64/  179]
train() client id: f_00007-4-2 loss: 0.868566  [   96/  179]
train() client id: f_00007-4-3 loss: 0.632800  [  128/  179]
train() client id: f_00007-4-4 loss: 0.583076  [  160/  179]
train() client id: f_00007-5-0 loss: 0.456455  [   32/  179]
train() client id: f_00007-5-1 loss: 0.796151  [   64/  179]
train() client id: f_00007-5-2 loss: 0.445579  [   96/  179]
train() client id: f_00007-5-3 loss: 0.651203  [  128/  179]
train() client id: f_00007-5-4 loss: 0.727369  [  160/  179]
train() client id: f_00007-6-0 loss: 0.693979  [   32/  179]
train() client id: f_00007-6-1 loss: 0.555193  [   64/  179]
train() client id: f_00007-6-2 loss: 0.671345  [   96/  179]
train() client id: f_00007-6-3 loss: 0.616233  [  128/  179]
train() client id: f_00007-6-4 loss: 0.535526  [  160/  179]
train() client id: f_00007-7-0 loss: 0.754013  [   32/  179]
train() client id: f_00007-7-1 loss: 0.507284  [   64/  179]
train() client id: f_00007-7-2 loss: 0.585004  [   96/  179]
train() client id: f_00007-7-3 loss: 0.623848  [  128/  179]
train() client id: f_00007-7-4 loss: 0.615358  [  160/  179]
train() client id: f_00007-8-0 loss: 0.721455  [   32/  179]
train() client id: f_00007-8-1 loss: 0.455172  [   64/  179]
train() client id: f_00007-8-2 loss: 0.574617  [   96/  179]
train() client id: f_00007-8-3 loss: 0.495452  [  128/  179]
train() client id: f_00007-8-4 loss: 0.728827  [  160/  179]
train() client id: f_00007-9-0 loss: 0.474357  [   32/  179]
train() client id: f_00007-9-1 loss: 0.567582  [   64/  179]
train() client id: f_00007-9-2 loss: 0.751655  [   96/  179]
train() client id: f_00007-9-3 loss: 0.711309  [  128/  179]
train() client id: f_00007-9-4 loss: 0.613652  [  160/  179]
train() client id: f_00007-10-0 loss: 0.552319  [   32/  179]
train() client id: f_00007-10-1 loss: 0.610989  [   64/  179]
train() client id: f_00007-10-2 loss: 0.763658  [   96/  179]
train() client id: f_00007-10-3 loss: 0.523038  [  128/  179]
train() client id: f_00007-10-4 loss: 0.662490  [  160/  179]
train() client id: f_00008-0-0 loss: 0.702949  [   32/  130]
train() client id: f_00008-0-1 loss: 0.832840  [   64/  130]
train() client id: f_00008-0-2 loss: 0.804602  [   96/  130]
train() client id: f_00008-0-3 loss: 0.881084  [  128/  130]
train() client id: f_00008-1-0 loss: 0.859424  [   32/  130]
train() client id: f_00008-1-1 loss: 0.747604  [   64/  130]
train() client id: f_00008-1-2 loss: 0.721258  [   96/  130]
train() client id: f_00008-1-3 loss: 0.889572  [  128/  130]
train() client id: f_00008-2-0 loss: 0.786536  [   32/  130]
train() client id: f_00008-2-1 loss: 0.880026  [   64/  130]
train() client id: f_00008-2-2 loss: 0.722752  [   96/  130]
train() client id: f_00008-2-3 loss: 0.820313  [  128/  130]
train() client id: f_00008-3-0 loss: 0.782093  [   32/  130]
train() client id: f_00008-3-1 loss: 0.901231  [   64/  130]
train() client id: f_00008-3-2 loss: 0.776555  [   96/  130]
train() client id: f_00008-3-3 loss: 0.742405  [  128/  130]
train() client id: f_00008-4-0 loss: 0.677650  [   32/  130]
train() client id: f_00008-4-1 loss: 0.901374  [   64/  130]
train() client id: f_00008-4-2 loss: 0.813851  [   96/  130]
train() client id: f_00008-4-3 loss: 0.815447  [  128/  130]
train() client id: f_00008-5-0 loss: 0.708955  [   32/  130]
train() client id: f_00008-5-1 loss: 0.967386  [   64/  130]
train() client id: f_00008-5-2 loss: 0.684150  [   96/  130]
train() client id: f_00008-5-3 loss: 0.841687  [  128/  130]
train() client id: f_00008-6-0 loss: 0.882774  [   32/  130]
train() client id: f_00008-6-1 loss: 0.779271  [   64/  130]
train() client id: f_00008-6-2 loss: 0.829824  [   96/  130]
train() client id: f_00008-6-3 loss: 0.678825  [  128/  130]
train() client id: f_00008-7-0 loss: 0.860452  [   32/  130]
train() client id: f_00008-7-1 loss: 0.774047  [   64/  130]
train() client id: f_00008-7-2 loss: 0.728522  [   96/  130]
train() client id: f_00008-7-3 loss: 0.835954  [  128/  130]
train() client id: f_00008-8-0 loss: 0.802687  [   32/  130]
train() client id: f_00008-8-1 loss: 0.769891  [   64/  130]
train() client id: f_00008-8-2 loss: 0.734192  [   96/  130]
train() client id: f_00008-8-3 loss: 0.858270  [  128/  130]
train() client id: f_00008-9-0 loss: 0.815188  [   32/  130]
train() client id: f_00008-9-1 loss: 0.767447  [   64/  130]
train() client id: f_00008-9-2 loss: 0.840058  [   96/  130]
train() client id: f_00008-9-3 loss: 0.723682  [  128/  130]
train() client id: f_00008-10-0 loss: 0.815913  [   32/  130]
train() client id: f_00008-10-1 loss: 0.873268  [   64/  130]
train() client id: f_00008-10-2 loss: 0.650633  [   96/  130]
train() client id: f_00008-10-3 loss: 0.855810  [  128/  130]
train() client id: f_00009-0-0 loss: 1.067867  [   32/  118]
train() client id: f_00009-0-1 loss: 1.069587  [   64/  118]
train() client id: f_00009-0-2 loss: 0.970654  [   96/  118]
train() client id: f_00009-1-0 loss: 1.174679  [   32/  118]
train() client id: f_00009-1-1 loss: 0.984166  [   64/  118]
train() client id: f_00009-1-2 loss: 0.905672  [   96/  118]
train() client id: f_00009-2-0 loss: 0.927520  [   32/  118]
train() client id: f_00009-2-1 loss: 1.004468  [   64/  118]
train() client id: f_00009-2-2 loss: 1.048955  [   96/  118]
train() client id: f_00009-3-0 loss: 0.843376  [   32/  118]
train() client id: f_00009-3-1 loss: 1.162777  [   64/  118]
train() client id: f_00009-3-2 loss: 1.042985  [   96/  118]
train() client id: f_00009-4-0 loss: 0.992058  [   32/  118]
train() client id: f_00009-4-1 loss: 0.883407  [   64/  118]
train() client id: f_00009-4-2 loss: 0.883137  [   96/  118]
train() client id: f_00009-5-0 loss: 1.028693  [   32/  118]
train() client id: f_00009-5-1 loss: 0.991081  [   64/  118]
train() client id: f_00009-5-2 loss: 0.936993  [   96/  118]
train() client id: f_00009-6-0 loss: 1.027750  [   32/  118]
train() client id: f_00009-6-1 loss: 0.879138  [   64/  118]
train() client id: f_00009-6-2 loss: 0.983684  [   96/  118]
train() client id: f_00009-7-0 loss: 0.881148  [   32/  118]
train() client id: f_00009-7-1 loss: 0.990697  [   64/  118]
train() client id: f_00009-7-2 loss: 0.976762  [   96/  118]
train() client id: f_00009-8-0 loss: 0.948835  [   32/  118]
train() client id: f_00009-8-1 loss: 0.805857  [   64/  118]
train() client id: f_00009-8-2 loss: 1.003562  [   96/  118]
train() client id: f_00009-9-0 loss: 1.039989  [   32/  118]
train() client id: f_00009-9-1 loss: 0.937599  [   64/  118]
train() client id: f_00009-9-2 loss: 0.907105  [   96/  118]
train() client id: f_00009-10-0 loss: 0.783963  [   32/  118]
train() client id: f_00009-10-1 loss: 0.948747  [   64/  118]
train() client id: f_00009-10-2 loss: 1.088370  [   96/  118]
At round 56 accuracy: 0.6472148541114059
At round 56 training accuracy: 0.5902079141515761
At round 56 training loss: 0.8274090349987372
update_location
xs = [  -3.9056584     4.20031788  300.00902392   18.81129433    0.97929623
    3.95640986 -262.44319194 -241.32485185  284.66397685 -227.06087855]
ys = [ 292.5879595   275.55583871    1.32061395 -262.45517586  254.35018685
  237.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [309.22963669 293.17002391 316.23908432 281.48993611 273.30381733
 258.0139986  280.86174452 261.22473155 302.22880354 248.13837757]
dists_bs = [207.60588129 205.4853538  505.76486823 478.46614164 193.04187331
 189.66025984 197.98218826 186.37821878 485.87600216 178.74794631]
uav_gains = [2.25068246e-12 3.05507187e-12 1.99030200e-12 3.87733494e-12
 4.60120712e-12 6.32147895e-12 3.92831823e-12 5.91917228e-12
 2.56156197e-12 7.69388670e-12]
bs_gains = [3.58940992e-11 3.69409145e-11 2.96645552e-12 3.46505949e-12
 4.40017369e-11 4.62338788e-11 4.09959494e-11 4.85498225e-11
 3.31911903e-12 5.45781883e-11]
Round 57
-------------------------------
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.48417658 7.05628152 3.44074209 1.26260335 8.13515315 3.91462212
 1.55207962 4.8313157  3.56521466 3.17400612]
obj_prev = 40.416194911724524
eta_min = 2.86092118758865e-27	eta_max = 0.9374658932840896
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 9.299045348130402	eta = 0.909090909090909
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 20.758435476455084	eta = 0.4072405937720066
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 14.636258285641127	eta = 0.5775846137877274
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 13.545044928479468	eta = 0.6241158766062833
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 13.479247501397975	eta = 0.6271624278976032
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 13.478979113930322	eta = 0.6271749156783474
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 13.478979109435532	eta = 0.6271749158874893
eta = 0.6271749158874893
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [0.03849692 0.08096575 0.03788586 0.01313784 0.09349252 0.04460754
 0.01649868 0.05469007 0.03971905 0.03605267]
ene_total = [1.32155705 2.05839072 1.33209932 0.6463361  2.34272368 1.20578025
 0.72268136 1.56297838 1.28498919 1.00144307]
ti_comp = [0.9850295  1.0924086  0.97437897 1.022215   1.09523156 1.09599371
 1.02294619 1.0429956  1.01299828 1.09844051]
ti_coms = [0.18414452 0.07676542 0.19479505 0.14695901 0.07394246 0.07318031
 0.14622782 0.12617841 0.15617573 0.07073351]
t_total = [27.14976082 27.14976082 27.14976082 27.14976082 27.14976082 27.14976082
 27.14976082 27.14976082 27.14976082 27.14976082]
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [3.67501858e-06 2.77980148e-05 3.57977419e-06 1.35633586e-07
 4.25793239e-05 4.61837109e-06 2.68239096e-07 9.39810980e-06
 3.81644508e-06 2.42738867e-06]
ene_total = [0.42769316 0.17890492 0.45242289 0.34126134 0.17269288 0.17004157
 0.3395665  0.29322113 0.36274925 0.16430889]
optimize_network iter = 0 obj = 2.90286252803159
eta = 0.6271749158874893
freqs = [19540999.15308214 37058362.31216387 19441028.73626838  6426161.89579366
 42681624.07611285 20350273.65967661  8064295.3890079  26217785.78934446
 19604695.49055661 16410844.32139915]
eta_min = 0.6271749158874926	eta_max = 0.7063960382409014
af = 0.0018241050152146168	bf = 1.0814089668242106	zeta = 0.002006515516736079	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [7.50791650e-07 5.67902365e-06 7.31333600e-07 2.77094010e-08
 8.69878619e-06 9.43514808e-07 5.48001783e-08 1.91999638e-06
 7.79684519e-07 4.95905832e-07]
ene_total = [1.75372365 0.73159623 1.85514936 1.39952837 0.7050001  0.6970034
 1.39256766 1.20180996 1.48737301 0.67365931]
ti_comp = [0.73659322 0.84397231 0.72594269 0.77377872 0.84679527 0.84755742
 0.77450991 0.79455932 0.764562   0.85000422]
ti_coms = [0.18414452 0.07676542 0.19479505 0.14695901 0.07394246 0.07318031
 0.14622782 0.12617841 0.15617573 0.07073351]
t_total = [27.14976082 27.14976082 27.14976082 27.14976082 27.14976082 27.14976082
 27.14976082 27.14976082 27.14976082 27.14976082]
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [2.71785248e-06 1.92597612e-05 2.66704329e-06 9.78907100e-08
 2.94562262e-05 3.19366957e-06 1.93507141e-07 6.69693547e-06
 2.77060309e-06 1.67638409e-06]
ene_total = [0.54306642 0.22692583 0.57447011 0.4333404  0.21890245 0.21588069
 0.43118717 0.37225931 0.46059653 0.20862108]
optimize_network iter = 1 obj = 3.685249982672195
eta = 0.7063960382409014
freqs = [19468958.52365561 35736956.5886699  19441028.73626838  6324869.42089178
 41128502.02279741 19605756.94700696  7935360.67409761 25640506.16051596
 19352208.45479425 15800135.59527822]
eta_min = 0.7063960382409034	eta_max = 0.7063960382409006
af = 0.0017119208968572406	bf = 1.0814089668242106	zeta = 0.0018831129865429649	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [7.45266057e-07 5.28124555e-06 7.31333600e-07 2.68427459e-08
 8.07723219e-06 8.75740514e-07 5.30618587e-08 1.83637587e-06
 7.59730875e-07 4.59683582e-07]
ene_total = [1.75372312 0.73155835 1.85514936 1.39952829 0.7049409  0.69699695
 1.39256749 1.20180199 1.48737111 0.67365586]
ti_comp = [0.73659322 0.84397231 0.72594269 0.77377872 0.84679527 0.84755742
 0.77450991 0.79455932 0.764562   0.85000422]
ti_coms = [0.18414452 0.07676542 0.19479505 0.14695901 0.07394246 0.07318031
 0.14622782 0.12617841 0.15617573 0.07073351]
t_total = [27.14976082 27.14976082 27.14976082 27.14976082 27.14976082 27.14976082
 27.14976082 27.14976082 27.14976082 27.14976082]
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [2.71785248e-06 1.92597612e-05 2.66704329e-06 9.78907100e-08
 2.94562262e-05 3.19366957e-06 1.93507141e-07 6.69693547e-06
 2.77060309e-06 1.67638409e-06]
ene_total = [0.54306642 0.22692583 0.57447011 0.4333404  0.21890245 0.21588069
 0.43118717 0.37225931 0.46059653 0.20862108]
optimize_network iter = 2 obj = 3.6852499826721847
eta = 0.7063960382409006
freqs = [19468958.52365561 35736956.5886699  19441028.73626836  6324869.42089178
 41128502.02279742 19605756.94700696  7935360.67409761 25640506.16051596
 19352208.45479425 15800135.59527823]
Done!
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [2.62674960e-06 1.86141707e-05 2.57764354e-06 9.46093966e-08
 2.84688484e-05 3.08661721e-06 1.87020749e-07 6.47245305e-06
 2.67773200e-06 1.62019141e-06]
ene_total = [0.01841708 0.00769516 0.01948208 0.014696   0.00742271 0.00732112
 0.01462297 0.01262431 0.01562025 0.00707497]
At round 57 energy consumption: 0.12497665052505343
At round 57 eta: 0.7063960382409006
At round 57 a_n: 8.314943732033974
At round 57 local rounds: 11.38151016928333
At round 57 global rounds: 29.486964437171718
gradient difference: 0.49488741159439087
train() client id: f_00000-0-0 loss: 1.145327  [   32/  126]
train() client id: f_00000-0-1 loss: 1.142423  [   64/  126]
train() client id: f_00000-0-2 loss: 0.977920  [   96/  126]
train() client id: f_00000-1-0 loss: 1.023487  [   32/  126]
train() client id: f_00000-1-1 loss: 1.011260  [   64/  126]
train() client id: f_00000-1-2 loss: 0.963291  [   96/  126]
train() client id: f_00000-2-0 loss: 1.171128  [   32/  126]
train() client id: f_00000-2-1 loss: 1.042940  [   64/  126]
train() client id: f_00000-2-2 loss: 0.858895  [   96/  126]
train() client id: f_00000-3-0 loss: 0.932223  [   32/  126]
train() client id: f_00000-3-1 loss: 0.794841  [   64/  126]
train() client id: f_00000-3-2 loss: 1.077620  [   96/  126]
train() client id: f_00000-4-0 loss: 0.939814  [   32/  126]
train() client id: f_00000-4-1 loss: 0.930380  [   64/  126]
train() client id: f_00000-4-2 loss: 0.896739  [   96/  126]
train() client id: f_00000-5-0 loss: 0.929870  [   32/  126]
train() client id: f_00000-5-1 loss: 0.886138  [   64/  126]
train() client id: f_00000-5-2 loss: 0.938146  [   96/  126]
train() client id: f_00000-6-0 loss: 0.949439  [   32/  126]
train() client id: f_00000-6-1 loss: 0.864780  [   64/  126]
train() client id: f_00000-6-2 loss: 0.895043  [   96/  126]
train() client id: f_00000-7-0 loss: 0.925395  [   32/  126]
train() client id: f_00000-7-1 loss: 0.811605  [   64/  126]
train() client id: f_00000-7-2 loss: 0.835873  [   96/  126]
train() client id: f_00000-8-0 loss: 0.979914  [   32/  126]
train() client id: f_00000-8-1 loss: 0.793724  [   64/  126]
train() client id: f_00000-8-2 loss: 0.942451  [   96/  126]
train() client id: f_00000-9-0 loss: 0.982412  [   32/  126]
train() client id: f_00000-9-1 loss: 0.836369  [   64/  126]
train() client id: f_00000-9-2 loss: 0.979032  [   96/  126]
train() client id: f_00000-10-0 loss: 0.872304  [   32/  126]
train() client id: f_00000-10-1 loss: 0.887572  [   64/  126]
train() client id: f_00000-10-2 loss: 1.031762  [   96/  126]
train() client id: f_00001-0-0 loss: 0.429668  [   32/  265]
train() client id: f_00001-0-1 loss: 0.480943  [   64/  265]
train() client id: f_00001-0-2 loss: 0.379611  [   96/  265]
train() client id: f_00001-0-3 loss: 0.400364  [  128/  265]
train() client id: f_00001-0-4 loss: 0.322311  [  160/  265]
train() client id: f_00001-0-5 loss: 0.482464  [  192/  265]
train() client id: f_00001-0-6 loss: 0.496583  [  224/  265]
train() client id: f_00001-0-7 loss: 0.447274  [  256/  265]
train() client id: f_00001-1-0 loss: 0.340427  [   32/  265]
train() client id: f_00001-1-1 loss: 0.373960  [   64/  265]
train() client id: f_00001-1-2 loss: 0.485209  [   96/  265]
train() client id: f_00001-1-3 loss: 0.537125  [  128/  265]
train() client id: f_00001-1-4 loss: 0.419574  [  160/  265]
train() client id: f_00001-1-5 loss: 0.449130  [  192/  265]
train() client id: f_00001-1-6 loss: 0.378270  [  224/  265]
train() client id: f_00001-1-7 loss: 0.373190  [  256/  265]
train() client id: f_00001-2-0 loss: 0.398665  [   32/  265]
train() client id: f_00001-2-1 loss: 0.440890  [   64/  265]
train() client id: f_00001-2-2 loss: 0.360794  [   96/  265]
train() client id: f_00001-2-3 loss: 0.407625  [  128/  265]
train() client id: f_00001-2-4 loss: 0.522140  [  160/  265]
train() client id: f_00001-2-5 loss: 0.426917  [  192/  265]
train() client id: f_00001-2-6 loss: 0.323420  [  224/  265]
train() client id: f_00001-2-7 loss: 0.348062  [  256/  265]
train() client id: f_00001-3-0 loss: 0.337540  [   32/  265]
train() client id: f_00001-3-1 loss: 0.367140  [   64/  265]
train() client id: f_00001-3-2 loss: 0.532746  [   96/  265]
train() client id: f_00001-3-3 loss: 0.516063  [  128/  265]
train() client id: f_00001-3-4 loss: 0.341608  [  160/  265]
train() client id: f_00001-3-5 loss: 0.322220  [  192/  265]
train() client id: f_00001-3-6 loss: 0.333509  [  224/  265]
train() client id: f_00001-3-7 loss: 0.520570  [  256/  265]
train() client id: f_00001-4-0 loss: 0.350577  [   32/  265]
train() client id: f_00001-4-1 loss: 0.326301  [   64/  265]
train() client id: f_00001-4-2 loss: 0.447119  [   96/  265]
train() client id: f_00001-4-3 loss: 0.403041  [  128/  265]
train() client id: f_00001-4-4 loss: 0.478641  [  160/  265]
train() client id: f_00001-4-5 loss: 0.498710  [  192/  265]
train() client id: f_00001-4-6 loss: 0.390526  [  224/  265]
train() client id: f_00001-4-7 loss: 0.349207  [  256/  265]
train() client id: f_00001-5-0 loss: 0.403664  [   32/  265]
train() client id: f_00001-5-1 loss: 0.431720  [   64/  265]
train() client id: f_00001-5-2 loss: 0.472089  [   96/  265]
train() client id: f_00001-5-3 loss: 0.353871  [  128/  265]
train() client id: f_00001-5-4 loss: 0.461266  [  160/  265]
train() client id: f_00001-5-5 loss: 0.436948  [  192/  265]
train() client id: f_00001-5-6 loss: 0.365419  [  224/  265]
train() client id: f_00001-5-7 loss: 0.304200  [  256/  265]
train() client id: f_00001-6-0 loss: 0.452356  [   32/  265]
train() client id: f_00001-6-1 loss: 0.378254  [   64/  265]
train() client id: f_00001-6-2 loss: 0.372076  [   96/  265]
train() client id: f_00001-6-3 loss: 0.302094  [  128/  265]
train() client id: f_00001-6-4 loss: 0.382648  [  160/  265]
train() client id: f_00001-6-5 loss: 0.434601  [  192/  265]
train() client id: f_00001-6-6 loss: 0.425644  [  224/  265]
train() client id: f_00001-6-7 loss: 0.375318  [  256/  265]
train() client id: f_00001-7-0 loss: 0.378327  [   32/  265]
train() client id: f_00001-7-1 loss: 0.485770  [   64/  265]
train() client id: f_00001-7-2 loss: 0.294135  [   96/  265]
train() client id: f_00001-7-3 loss: 0.442971  [  128/  265]
train() client id: f_00001-7-4 loss: 0.407946  [  160/  265]
train() client id: f_00001-7-5 loss: 0.412494  [  192/  265]
train() client id: f_00001-7-6 loss: 0.411236  [  224/  265]
train() client id: f_00001-7-7 loss: 0.349366  [  256/  265]
train() client id: f_00001-8-0 loss: 0.298254  [   32/  265]
train() client id: f_00001-8-1 loss: 0.392400  [   64/  265]
train() client id: f_00001-8-2 loss: 0.384939  [   96/  265]
train() client id: f_00001-8-3 loss: 0.544117  [  128/  265]
train() client id: f_00001-8-4 loss: 0.499465  [  160/  265]
train() client id: f_00001-8-5 loss: 0.393351  [  192/  265]
train() client id: f_00001-8-6 loss: 0.363156  [  224/  265]
train() client id: f_00001-8-7 loss: 0.310466  [  256/  265]
train() client id: f_00001-9-0 loss: 0.423502  [   32/  265]
train() client id: f_00001-9-1 loss: 0.345395  [   64/  265]
train() client id: f_00001-9-2 loss: 0.448777  [   96/  265]
train() client id: f_00001-9-3 loss: 0.405761  [  128/  265]
train() client id: f_00001-9-4 loss: 0.423709  [  160/  265]
train() client id: f_00001-9-5 loss: 0.371671  [  192/  265]
train() client id: f_00001-9-6 loss: 0.379608  [  224/  265]
train() client id: f_00001-9-7 loss: 0.387675  [  256/  265]
train() client id: f_00001-10-0 loss: 0.484183  [   32/  265]
train() client id: f_00001-10-1 loss: 0.343225  [   64/  265]
train() client id: f_00001-10-2 loss: 0.430744  [   96/  265]
train() client id: f_00001-10-3 loss: 0.392868  [  128/  265]
train() client id: f_00001-10-4 loss: 0.492733  [  160/  265]
train() client id: f_00001-10-5 loss: 0.361120  [  192/  265]
train() client id: f_00001-10-6 loss: 0.323462  [  224/  265]
train() client id: f_00001-10-7 loss: 0.351426  [  256/  265]
train() client id: f_00002-0-0 loss: 1.113297  [   32/  124]
train() client id: f_00002-0-1 loss: 1.292684  [   64/  124]
train() client id: f_00002-0-2 loss: 1.259951  [   96/  124]
train() client id: f_00002-1-0 loss: 1.514125  [   32/  124]
train() client id: f_00002-1-1 loss: 0.997239  [   64/  124]
train() client id: f_00002-1-2 loss: 1.090553  [   96/  124]
train() client id: f_00002-2-0 loss: 1.256391  [   32/  124]
train() client id: f_00002-2-1 loss: 1.205599  [   64/  124]
train() client id: f_00002-2-2 loss: 1.065615  [   96/  124]
train() client id: f_00002-3-0 loss: 1.218106  [   32/  124]
train() client id: f_00002-3-1 loss: 1.126778  [   64/  124]
train() client id: f_00002-3-2 loss: 0.988312  [   96/  124]
train() client id: f_00002-4-0 loss: 1.062381  [   32/  124]
train() client id: f_00002-4-1 loss: 1.166378  [   64/  124]
train() client id: f_00002-4-2 loss: 1.046603  [   96/  124]
train() client id: f_00002-5-0 loss: 1.029666  [   32/  124]
train() client id: f_00002-5-1 loss: 0.923597  [   64/  124]
train() client id: f_00002-5-2 loss: 1.092100  [   96/  124]
train() client id: f_00002-6-0 loss: 0.954088  [   32/  124]
train() client id: f_00002-6-1 loss: 1.091608  [   64/  124]
train() client id: f_00002-6-2 loss: 1.021757  [   96/  124]
train() client id: f_00002-7-0 loss: 1.046887  [   32/  124]
train() client id: f_00002-7-1 loss: 1.065990  [   64/  124]
train() client id: f_00002-7-2 loss: 1.041346  [   96/  124]
train() client id: f_00002-8-0 loss: 0.898284  [   32/  124]
train() client id: f_00002-8-1 loss: 1.174792  [   64/  124]
train() client id: f_00002-8-2 loss: 1.082511  [   96/  124]
train() client id: f_00002-9-0 loss: 1.072605  [   32/  124]
train() client id: f_00002-9-1 loss: 1.078064  [   64/  124]
train() client id: f_00002-9-2 loss: 0.958218  [   96/  124]
train() client id: f_00002-10-0 loss: 1.307745  [   32/  124]
train() client id: f_00002-10-1 loss: 1.048437  [   64/  124]
train() client id: f_00002-10-2 loss: 0.923412  [   96/  124]
train() client id: f_00003-0-0 loss: 0.637399  [   32/   43]
train() client id: f_00003-1-0 loss: 0.682029  [   32/   43]
train() client id: f_00003-2-0 loss: 0.463856  [   32/   43]
train() client id: f_00003-3-0 loss: 0.426837  [   32/   43]
train() client id: f_00003-4-0 loss: 0.336699  [   32/   43]
train() client id: f_00003-5-0 loss: 0.529341  [   32/   43]
train() client id: f_00003-6-0 loss: 0.500156  [   32/   43]
train() client id: f_00003-7-0 loss: 0.548214  [   32/   43]
train() client id: f_00003-8-0 loss: 0.551220  [   32/   43]
train() client id: f_00003-9-0 loss: 0.578460  [   32/   43]
train() client id: f_00003-10-0 loss: 0.629778  [   32/   43]
train() client id: f_00004-0-0 loss: 0.889933  [   32/  306]
train() client id: f_00004-0-1 loss: 0.910899  [   64/  306]
train() client id: f_00004-0-2 loss: 0.757698  [   96/  306]
train() client id: f_00004-0-3 loss: 0.871015  [  128/  306]
train() client id: f_00004-0-4 loss: 0.945660  [  160/  306]
train() client id: f_00004-0-5 loss: 0.847453  [  192/  306]
train() client id: f_00004-0-6 loss: 0.734081  [  224/  306]
train() client id: f_00004-0-7 loss: 0.807329  [  256/  306]
train() client id: f_00004-0-8 loss: 0.946593  [  288/  306]
train() client id: f_00004-1-0 loss: 0.822717  [   32/  306]
train() client id: f_00004-1-1 loss: 0.846785  [   64/  306]
train() client id: f_00004-1-2 loss: 0.895007  [   96/  306]
train() client id: f_00004-1-3 loss: 0.919350  [  128/  306]
train() client id: f_00004-1-4 loss: 0.740559  [  160/  306]
train() client id: f_00004-1-5 loss: 0.791981  [  192/  306]
train() client id: f_00004-1-6 loss: 0.879605  [  224/  306]
train() client id: f_00004-1-7 loss: 0.906759  [  256/  306]
train() client id: f_00004-1-8 loss: 0.884175  [  288/  306]
train() client id: f_00004-2-0 loss: 0.840360  [   32/  306]
train() client id: f_00004-2-1 loss: 0.851952  [   64/  306]
train() client id: f_00004-2-2 loss: 0.805658  [   96/  306]
train() client id: f_00004-2-3 loss: 0.877472  [  128/  306]
train() client id: f_00004-2-4 loss: 0.824810  [  160/  306]
train() client id: f_00004-2-5 loss: 0.835410  [  192/  306]
train() client id: f_00004-2-6 loss: 1.023433  [  224/  306]
train() client id: f_00004-2-7 loss: 0.861123  [  256/  306]
train() client id: f_00004-2-8 loss: 0.826945  [  288/  306]
train() client id: f_00004-3-0 loss: 0.858340  [   32/  306]
train() client id: f_00004-3-1 loss: 0.817643  [   64/  306]
train() client id: f_00004-3-2 loss: 0.768395  [   96/  306]
train() client id: f_00004-3-3 loss: 0.910086  [  128/  306]
train() client id: f_00004-3-4 loss: 0.959225  [  160/  306]
train() client id: f_00004-3-5 loss: 0.803727  [  192/  306]
train() client id: f_00004-3-6 loss: 0.891702  [  224/  306]
train() client id: f_00004-3-7 loss: 0.776612  [  256/  306]
train() client id: f_00004-3-8 loss: 0.922782  [  288/  306]
train() client id: f_00004-4-0 loss: 0.830279  [   32/  306]
train() client id: f_00004-4-1 loss: 0.926285  [   64/  306]
train() client id: f_00004-4-2 loss: 0.975583  [   96/  306]
train() client id: f_00004-4-3 loss: 0.851218  [  128/  306]
train() client id: f_00004-4-4 loss: 0.812854  [  160/  306]
train() client id: f_00004-4-5 loss: 0.833489  [  192/  306]
train() client id: f_00004-4-6 loss: 0.842650  [  224/  306]
train() client id: f_00004-4-7 loss: 0.882508  [  256/  306]
train() client id: f_00004-4-8 loss: 0.786034  [  288/  306]
train() client id: f_00004-5-0 loss: 0.778197  [   32/  306]
train() client id: f_00004-5-1 loss: 0.888999  [   64/  306]
train() client id: f_00004-5-2 loss: 0.872257  [   96/  306]
train() client id: f_00004-5-3 loss: 0.866044  [  128/  306]
train() client id: f_00004-5-4 loss: 0.872280  [  160/  306]
train() client id: f_00004-5-5 loss: 0.775702  [  192/  306]
train() client id: f_00004-5-6 loss: 0.911199  [  224/  306]
train() client id: f_00004-5-7 loss: 0.960813  [  256/  306]
train() client id: f_00004-5-8 loss: 0.807818  [  288/  306]
train() client id: f_00004-6-0 loss: 0.802751  [   32/  306]
train() client id: f_00004-6-1 loss: 0.885832  [   64/  306]
train() client id: f_00004-6-2 loss: 0.926178  [   96/  306]
train() client id: f_00004-6-3 loss: 0.796536  [  128/  306]
train() client id: f_00004-6-4 loss: 0.837165  [  160/  306]
train() client id: f_00004-6-5 loss: 0.755851  [  192/  306]
train() client id: f_00004-6-6 loss: 0.762202  [  224/  306]
train() client id: f_00004-6-7 loss: 0.958675  [  256/  306]
train() client id: f_00004-6-8 loss: 1.031913  [  288/  306]
train() client id: f_00004-7-0 loss: 0.943549  [   32/  306]
train() client id: f_00004-7-1 loss: 0.795044  [   64/  306]
train() client id: f_00004-7-2 loss: 0.968308  [   96/  306]
train() client id: f_00004-7-3 loss: 0.774713  [  128/  306]
train() client id: f_00004-7-4 loss: 0.815221  [  160/  306]
train() client id: f_00004-7-5 loss: 0.891890  [  192/  306]
train() client id: f_00004-7-6 loss: 0.858532  [  224/  306]
train() client id: f_00004-7-7 loss: 0.724121  [  256/  306]
train() client id: f_00004-7-8 loss: 0.821239  [  288/  306]
train() client id: f_00004-8-0 loss: 0.961339  [   32/  306]
train() client id: f_00004-8-1 loss: 0.722031  [   64/  306]
train() client id: f_00004-8-2 loss: 0.938797  [   96/  306]
train() client id: f_00004-8-3 loss: 0.772713  [  128/  306]
train() client id: f_00004-8-4 loss: 0.851619  [  160/  306]
train() client id: f_00004-8-5 loss: 0.976891  [  192/  306]
train() client id: f_00004-8-6 loss: 0.881297  [  224/  306]
train() client id: f_00004-8-7 loss: 0.758848  [  256/  306]
train() client id: f_00004-8-8 loss: 0.869791  [  288/  306]
train() client id: f_00004-9-0 loss: 0.835108  [   32/  306]
train() client id: f_00004-9-1 loss: 0.865872  [   64/  306]
train() client id: f_00004-9-2 loss: 0.857937  [   96/  306]
train() client id: f_00004-9-3 loss: 0.826722  [  128/  306]
train() client id: f_00004-9-4 loss: 1.062282  [  160/  306]
train() client id: f_00004-9-5 loss: 0.811817  [  192/  306]
train() client id: f_00004-9-6 loss: 0.858225  [  224/  306]
train() client id: f_00004-9-7 loss: 0.917562  [  256/  306]
train() client id: f_00004-9-8 loss: 0.735268  [  288/  306]
train() client id: f_00004-10-0 loss: 0.823667  [   32/  306]
train() client id: f_00004-10-1 loss: 0.902766  [   64/  306]
train() client id: f_00004-10-2 loss: 0.865677  [   96/  306]
train() client id: f_00004-10-3 loss: 0.771446  [  128/  306]
train() client id: f_00004-10-4 loss: 0.892776  [  160/  306]
train() client id: f_00004-10-5 loss: 0.860469  [  192/  306]
train() client id: f_00004-10-6 loss: 0.786982  [  224/  306]
train() client id: f_00004-10-7 loss: 0.884953  [  256/  306]
train() client id: f_00004-10-8 loss: 1.022038  [  288/  306]
train() client id: f_00005-0-0 loss: 0.379011  [   32/  146]
train() client id: f_00005-0-1 loss: 0.714457  [   64/  146]
train() client id: f_00005-0-2 loss: 0.670088  [   96/  146]
train() client id: f_00005-0-3 loss: 0.628677  [  128/  146]
train() client id: f_00005-1-0 loss: 0.871866  [   32/  146]
train() client id: f_00005-1-1 loss: 0.332578  [   64/  146]
train() client id: f_00005-1-2 loss: 0.639029  [   96/  146]
train() client id: f_00005-1-3 loss: 0.533303  [  128/  146]
train() client id: f_00005-2-0 loss: 0.559139  [   32/  146]
train() client id: f_00005-2-1 loss: 0.499301  [   64/  146]
train() client id: f_00005-2-2 loss: 0.702405  [   96/  146]
train() client id: f_00005-2-3 loss: 0.528186  [  128/  146]
train() client id: f_00005-3-0 loss: 0.471487  [   32/  146]
train() client id: f_00005-3-1 loss: 0.703129  [   64/  146]
train() client id: f_00005-3-2 loss: 0.454970  [   96/  146]
train() client id: f_00005-3-3 loss: 0.479384  [  128/  146]
train() client id: f_00005-4-0 loss: 0.736358  [   32/  146]
train() client id: f_00005-4-1 loss: 0.552169  [   64/  146]
train() client id: f_00005-4-2 loss: 0.416830  [   96/  146]
train() client id: f_00005-4-3 loss: 0.389410  [  128/  146]
train() client id: f_00005-5-0 loss: 0.535458  [   32/  146]
train() client id: f_00005-5-1 loss: 0.752358  [   64/  146]
train() client id: f_00005-5-2 loss: 0.524987  [   96/  146]
train() client id: f_00005-5-3 loss: 0.399108  [  128/  146]
train() client id: f_00005-6-0 loss: 0.678076  [   32/  146]
train() client id: f_00005-6-1 loss: 0.537081  [   64/  146]
train() client id: f_00005-6-2 loss: 0.646474  [   96/  146]
train() client id: f_00005-6-3 loss: 0.528235  [  128/  146]
train() client id: f_00005-7-0 loss: 0.760069  [   32/  146]
train() client id: f_00005-7-1 loss: 0.550897  [   64/  146]
train() client id: f_00005-7-2 loss: 0.468197  [   96/  146]
train() client id: f_00005-7-3 loss: 0.258663  [  128/  146]
train() client id: f_00005-8-0 loss: 0.634993  [   32/  146]
train() client id: f_00005-8-1 loss: 0.425310  [   64/  146]
train() client id: f_00005-8-2 loss: 0.548643  [   96/  146]
train() client id: f_00005-8-3 loss: 0.598701  [  128/  146]
train() client id: f_00005-9-0 loss: 0.733922  [   32/  146]
train() client id: f_00005-9-1 loss: 0.484653  [   64/  146]
train() client id: f_00005-9-2 loss: 0.781026  [   96/  146]
train() client id: f_00005-9-3 loss: 0.346219  [  128/  146]
train() client id: f_00005-10-0 loss: 0.354829  [   32/  146]
train() client id: f_00005-10-1 loss: 0.526828  [   64/  146]
train() client id: f_00005-10-2 loss: 0.411959  [   96/  146]
train() client id: f_00005-10-3 loss: 0.595208  [  128/  146]
train() client id: f_00006-0-0 loss: 0.557038  [   32/   54]
train() client id: f_00006-1-0 loss: 0.578798  [   32/   54]
train() client id: f_00006-2-0 loss: 0.542317  [   32/   54]
train() client id: f_00006-3-0 loss: 0.601185  [   32/   54]
train() client id: f_00006-4-0 loss: 0.505281  [   32/   54]
train() client id: f_00006-5-0 loss: 0.562239  [   32/   54]
train() client id: f_00006-6-0 loss: 0.594316  [   32/   54]
train() client id: f_00006-7-0 loss: 0.560504  [   32/   54]
train() client id: f_00006-8-0 loss: 0.497884  [   32/   54]
train() client id: f_00006-9-0 loss: 0.530922  [   32/   54]
train() client id: f_00006-10-0 loss: 0.509326  [   32/   54]
train() client id: f_00007-0-0 loss: 0.559815  [   32/  179]
train() client id: f_00007-0-1 loss: 0.509044  [   64/  179]
train() client id: f_00007-0-2 loss: 0.400131  [   96/  179]
train() client id: f_00007-0-3 loss: 0.687853  [  128/  179]
train() client id: f_00007-0-4 loss: 0.392125  [  160/  179]
train() client id: f_00007-1-0 loss: 0.497162  [   32/  179]
train() client id: f_00007-1-1 loss: 0.527922  [   64/  179]
train() client id: f_00007-1-2 loss: 0.429458  [   96/  179]
train() client id: f_00007-1-3 loss: 0.654542  [  128/  179]
train() client id: f_00007-1-4 loss: 0.464280  [  160/  179]
train() client id: f_00007-2-0 loss: 0.469315  [   32/  179]
train() client id: f_00007-2-1 loss: 0.630990  [   64/  179]
train() client id: f_00007-2-2 loss: 0.511559  [   96/  179]
train() client id: f_00007-2-3 loss: 0.512608  [  128/  179]
train() client id: f_00007-2-4 loss: 0.366749  [  160/  179]
train() client id: f_00007-3-0 loss: 0.371337  [   32/  179]
train() client id: f_00007-3-1 loss: 0.598515  [   64/  179]
train() client id: f_00007-3-2 loss: 0.513718  [   96/  179]
train() client id: f_00007-3-3 loss: 0.610121  [  128/  179]
train() client id: f_00007-3-4 loss: 0.437692  [  160/  179]
train() client id: f_00007-4-0 loss: 0.440158  [   32/  179]
train() client id: f_00007-4-1 loss: 0.332931  [   64/  179]
train() client id: f_00007-4-2 loss: 0.451330  [   96/  179]
train() client id: f_00007-4-3 loss: 0.602212  [  128/  179]
train() client id: f_00007-4-4 loss: 0.640078  [  160/  179]
train() client id: f_00007-5-0 loss: 0.362685  [   32/  179]
train() client id: f_00007-5-1 loss: 0.600856  [   64/  179]
train() client id: f_00007-5-2 loss: 0.510387  [   96/  179]
train() client id: f_00007-5-3 loss: 0.369079  [  128/  179]
train() client id: f_00007-5-4 loss: 0.434278  [  160/  179]
train() client id: f_00007-6-0 loss: 0.437362  [   32/  179]
train() client id: f_00007-6-1 loss: 0.397966  [   64/  179]
train() client id: f_00007-6-2 loss: 0.387421  [   96/  179]
train() client id: f_00007-6-3 loss: 0.507952  [  128/  179]
train() client id: f_00007-6-4 loss: 0.617041  [  160/  179]
train() client id: f_00007-7-0 loss: 0.431429  [   32/  179]
train() client id: f_00007-7-1 loss: 0.581084  [   64/  179]
train() client id: f_00007-7-2 loss: 0.373743  [   96/  179]
train() client id: f_00007-7-3 loss: 0.557356  [  128/  179]
train() client id: f_00007-7-4 loss: 0.472374  [  160/  179]
train() client id: f_00007-8-0 loss: 0.383089  [   32/  179]
train() client id: f_00007-8-1 loss: 0.377647  [   64/  179]
train() client id: f_00007-8-2 loss: 0.542563  [   96/  179]
train() client id: f_00007-8-3 loss: 0.589902  [  128/  179]
train() client id: f_00007-8-4 loss: 0.495178  [  160/  179]
train() client id: f_00007-9-0 loss: 0.296696  [   32/  179]
train() client id: f_00007-9-1 loss: 0.352720  [   64/  179]
train() client id: f_00007-9-2 loss: 0.409705  [   96/  179]
train() client id: f_00007-9-3 loss: 0.754328  [  128/  179]
train() client id: f_00007-9-4 loss: 0.415371  [  160/  179]
train() client id: f_00007-10-0 loss: 0.561611  [   32/  179]
train() client id: f_00007-10-1 loss: 0.270522  [   64/  179]
train() client id: f_00007-10-2 loss: 0.611957  [   96/  179]
train() client id: f_00007-10-3 loss: 0.462913  [  128/  179]
train() client id: f_00007-10-4 loss: 0.404138  [  160/  179]
train() client id: f_00008-0-0 loss: 0.692670  [   32/  130]
train() client id: f_00008-0-1 loss: 0.647975  [   64/  130]
train() client id: f_00008-0-2 loss: 0.598562  [   96/  130]
train() client id: f_00008-0-3 loss: 0.624175  [  128/  130]
train() client id: f_00008-1-0 loss: 0.617651  [   32/  130]
train() client id: f_00008-1-1 loss: 0.630107  [   64/  130]
train() client id: f_00008-1-2 loss: 0.684050  [   96/  130]
train() client id: f_00008-1-3 loss: 0.631378  [  128/  130]
train() client id: f_00008-2-0 loss: 0.619625  [   32/  130]
train() client id: f_00008-2-1 loss: 0.533039  [   64/  130]
train() client id: f_00008-2-2 loss: 0.720710  [   96/  130]
train() client id: f_00008-2-3 loss: 0.699541  [  128/  130]
train() client id: f_00008-3-0 loss: 0.753408  [   32/  130]
train() client id: f_00008-3-1 loss: 0.565486  [   64/  130]
train() client id: f_00008-3-2 loss: 0.730308  [   96/  130]
train() client id: f_00008-3-3 loss: 0.520739  [  128/  130]
train() client id: f_00008-4-0 loss: 0.589267  [   32/  130]
train() client id: f_00008-4-1 loss: 0.650062  [   64/  130]
train() client id: f_00008-4-2 loss: 0.655007  [   96/  130]
train() client id: f_00008-4-3 loss: 0.678047  [  128/  130]
train() client id: f_00008-5-0 loss: 0.756092  [   32/  130]
train() client id: f_00008-5-1 loss: 0.583919  [   64/  130]
train() client id: f_00008-5-2 loss: 0.600807  [   96/  130]
train() client id: f_00008-5-3 loss: 0.641521  [  128/  130]
train() client id: f_00008-6-0 loss: 0.539997  [   32/  130]
train() client id: f_00008-6-1 loss: 0.724917  [   64/  130]
train() client id: f_00008-6-2 loss: 0.644237  [   96/  130]
train() client id: f_00008-6-3 loss: 0.667701  [  128/  130]
train() client id: f_00008-7-0 loss: 0.747327  [   32/  130]
train() client id: f_00008-7-1 loss: 0.523108  [   64/  130]
train() client id: f_00008-7-2 loss: 0.633736  [   96/  130]
train() client id: f_00008-7-3 loss: 0.613254  [  128/  130]
train() client id: f_00008-8-0 loss: 0.585504  [   32/  130]
train() client id: f_00008-8-1 loss: 0.650418  [   64/  130]
train() client id: f_00008-8-2 loss: 0.581873  [   96/  130]
train() client id: f_00008-8-3 loss: 0.752772  [  128/  130]
train() client id: f_00008-9-0 loss: 0.581840  [   32/  130]
train() client id: f_00008-9-1 loss: 0.644889  [   64/  130]
train() client id: f_00008-9-2 loss: 0.632145  [   96/  130]
train() client id: f_00008-9-3 loss: 0.722682  [  128/  130]
train() client id: f_00008-10-0 loss: 0.590623  [   32/  130]
train() client id: f_00008-10-1 loss: 0.540735  [   64/  130]
train() client id: f_00008-10-2 loss: 0.723657  [   96/  130]
train() client id: f_00008-10-3 loss: 0.692948  [  128/  130]
train() client id: f_00009-0-0 loss: 0.770675  [   32/  118]
train() client id: f_00009-0-1 loss: 0.812462  [   64/  118]
train() client id: f_00009-0-2 loss: 0.997301  [   96/  118]
train() client id: f_00009-1-0 loss: 0.929703  [   32/  118]
train() client id: f_00009-1-1 loss: 0.802749  [   64/  118]
train() client id: f_00009-1-2 loss: 0.897440  [   96/  118]
train() client id: f_00009-2-0 loss: 0.649459  [   32/  118]
train() client id: f_00009-2-1 loss: 0.818271  [   64/  118]
train() client id: f_00009-2-2 loss: 0.931137  [   96/  118]
train() client id: f_00009-3-0 loss: 0.822507  [   32/  118]
train() client id: f_00009-3-1 loss: 0.762885  [   64/  118]
train() client id: f_00009-3-2 loss: 0.780833  [   96/  118]
train() client id: f_00009-4-0 loss: 0.782597  [   32/  118]
train() client id: f_00009-4-1 loss: 0.603989  [   64/  118]
train() client id: f_00009-4-2 loss: 0.857741  [   96/  118]
train() client id: f_00009-5-0 loss: 0.555416  [   32/  118]
train() client id: f_00009-5-1 loss: 0.579612  [   64/  118]
train() client id: f_00009-5-2 loss: 0.962020  [   96/  118]
train() client id: f_00009-6-0 loss: 0.700024  [   32/  118]
train() client id: f_00009-6-1 loss: 0.672801  [   64/  118]
train() client id: f_00009-6-2 loss: 0.701697  [   96/  118]
train() client id: f_00009-7-0 loss: 0.734088  [   32/  118]
train() client id: f_00009-7-1 loss: 0.547755  [   64/  118]
train() client id: f_00009-7-2 loss: 0.605308  [   96/  118]
train() client id: f_00009-8-0 loss: 0.656629  [   32/  118]
train() client id: f_00009-8-1 loss: 0.610081  [   64/  118]
train() client id: f_00009-8-2 loss: 0.734906  [   96/  118]
train() client id: f_00009-9-0 loss: 0.783606  [   32/  118]
train() client id: f_00009-9-1 loss: 0.584350  [   64/  118]
train() client id: f_00009-9-2 loss: 0.645563  [   96/  118]
train() client id: f_00009-10-0 loss: 0.673546  [   32/  118]
train() client id: f_00009-10-1 loss: 0.615961  [   64/  118]
train() client id: f_00009-10-2 loss: 0.746381  [   96/  118]
At round 57 accuracy: 0.6472148541114059
At round 57 training accuracy: 0.5888665325285044
At round 57 training loss: 0.8286589351176828
update_location
xs = [  -3.9056584     4.20031788  305.00902392   18.81129433    0.97929623
    3.95640986 -267.44319194 -246.32485185  289.66397685 -232.06087855]
ys = [ 297.5879595   280.55583871    1.32061395 -267.45517586  259.35018685
  242.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [313.96472382 297.87450596 320.98636839 286.15753684 277.96308826
 262.62971077 285.53940438 265.85072671 306.94281138 252.721711  ]
dists_bs = [210.47822106 207.97785702 510.4637031  483.04182061 195.15036951
 191.37438614 200.24080201 188.21553855 490.60893718 180.26768178]
uav_gains = [2.06976062e-12 2.78476917e-12 1.83844324e-12 3.52095540e-12
 4.17321580e-12 5.75002013e-12 3.56593276e-12 5.37807949e-12
 2.34619438e-12 7.03198726e-12]
bs_gains = [3.45393392e-11 3.57146344e-11 2.89062969e-12 3.37393595e-12
 4.26834807e-11 4.50836859e-11 3.97142964e-11 4.72344384e-11
 3.23024015e-12 5.32996109e-11]
Round 58
-------------------------------
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.3523265  6.77756785 3.31084036 1.21735079 7.81367428 3.76005882
 1.49535056 4.64371042 3.4255904  3.04870737]
obj_prev = 38.84517734952248
eta_min = 2.470941254922374e-28	eta_max = 0.9375517380484754
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 8.931115437766236	eta = 0.909090909090909
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 20.2431781023763	eta = 0.40108306173335817
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 14.165631330058979	eta = 0.5731615953668162
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 13.085726257080514	eta = 0.6204619975235665
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 13.020212302687447	eta = 0.623583983407007
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 13.019941071601606	eta = 0.6235969738929089
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 13.01994106692125	eta = 0.623596974117077
eta = 0.623596974117077
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [0.03896902 0.08195865 0.03835046 0.01329895 0.09463905 0.04515458
 0.01670101 0.05536075 0.04020613 0.0364948 ]
ene_total = [1.28342906 1.9813468  1.29389683 0.63079383 2.25502199 1.16000083
 0.7042715  1.51102723 1.23695895 0.96319404]
ti_comp = [1.03546301 1.14942413 1.02453888 1.07419893 1.1523401  1.15319241
 1.07495744 1.09635059 1.06887034 1.15568559]
ti_coms = [0.19129579 0.07733467 0.20221992 0.15255987 0.0744187  0.07356639
 0.15180136 0.13040821 0.15788846 0.07107321]
t_total = [27.09975662 27.09975662 27.09975662 27.09975662 27.09975662 27.09975662
 27.09975662 27.09975662 27.09975662 27.09975662]
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [3.44960441e-06 2.60437692e-05 3.35842002e-06 1.27398049e-07
 3.98960382e-05 4.32694980e-06 2.51956506e-07 8.82240566e-06
 3.55555046e-06 2.27453939e-06]
ene_total = [0.42265881 0.17141173 0.4467888  0.33701571 0.16527621 0.16260767
 0.33534287 0.28827359 0.34886259 0.15705475]
optimize_network iter = 0 obj = 2.83529273119976
eta = 0.623596974117077
freqs = [18817195.6470524  35652050.01875965 18715963.43024579  6190171.56284024
 41063853.09945352 19578076.71969618  7768218.52091367 25247740.71552675
 18807768.58899782 15789240.96805261]
eta_min = 0.6235969741170779	eta_max = 0.7095637697055857
af = 0.0016217254572042322	bf = 1.0664469018281375	zeta = 0.0017838980029246557	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [6.96202700e-07 5.25618019e-06 6.77799772e-07 2.57116048e-08
 8.05185933e-06 8.73269445e-07 5.08501203e-08 1.78054696e-06
 7.17584839e-07 4.59049871e-07]
ene_total = [1.74973877 0.70781749 1.84965402 1.39538178 0.68140253 0.67295033
 1.38844638 1.19293349 1.4441827  0.65010868]
ti_comp = [0.75528321 0.86924434 0.74435908 0.79401913 0.8721603  0.87301261
 0.79477765 0.81617079 0.78869054 0.8755058 ]
ti_coms = [0.19129579 0.07733467 0.20221992 0.15255987 0.0744187  0.07356639
 0.15180136 0.13040821 0.15788846 0.07107321]
t_total = [27.09975662 27.09975662 27.09975662 27.09975662 27.09975662 27.09975662
 27.09975662 27.09975662 27.09975662 27.09975662]
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [2.48645699e-06 1.74639655e-05 2.43999568e-06 8.94195368e-08
 2.67091848e-05 2.89538630e-06 1.76757958e-07 6.10500434e-06
 2.50441200e-06 1.51990678e-06]
ene_total = [0.54773486 0.22190259 0.5790084  0.43676858 0.21381912 0.21069725
 0.43459951 0.37352246 0.45209302 0.2035201 ]
optimize_network iter = 1 obj = 3.6736658995998672
eta = 0.7095637697055857
freqs = [18742766.932176   34251295.00082695 18715963.43024578  6084298.06393133
 39418319.48246514 18789072.00779956  7633454.28294466 24640242.31966294
 18518666.61499257 15142443.926058  ]
eta_min = 0.7095637697056919	eta_max = 0.7095637697055788
af = 0.0015119964848909963	bf = 1.0664469018281375	zeta = 0.001663196133380096	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [6.90706132e-07 4.85126751e-06 6.77799772e-07 2.48396102e-08
 7.41947189e-06 8.04301492e-07 4.91011128e-08 1.69589257e-06
 6.95693805e-07 4.22210775e-07]
ene_total = [1.74973827 0.70778046 1.84965402 1.3953817  0.68134469 0.67294402
 1.38844622 1.19292575 1.44418069 0.65010531]
ti_comp = [0.75528321 0.86924434 0.74435908 0.79401913 0.8721603  0.87301261
 0.79477765 0.81617079 0.78869054 0.8755058 ]
ti_coms = [0.19129579 0.07733467 0.20221992 0.15255987 0.0744187  0.07356639
 0.15180136 0.13040821 0.15788846 0.07107321]
t_total = [27.09975662 27.09975662 27.09975662 27.09975662 27.09975662 27.09975662
 27.09975662 27.09975662 27.09975662 27.09975662]
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [2.48645699e-06 1.74639655e-05 2.43999568e-06 8.94195368e-08
 2.67091848e-05 2.89538630e-06 1.76757958e-07 6.10500434e-06
 2.50441200e-06 1.51990678e-06]
ene_total = [0.54773486 0.22190259 0.5790084  0.43676858 0.21381912 0.21069725
 0.43459951 0.37352246 0.45209302 0.2035201 ]
optimize_network iter = 2 obj = 3.67366589959978
eta = 0.7095637697055788
freqs = [18742766.93217598 34251295.00082704 18715963.43024575  6084298.06393133
 39418319.48246525 18789072.00779961  7633454.28294467 24640242.31966297
 18518666.61499257 15142443.92605804]
Done!
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [2.43444879e-06 1.70986789e-05 2.38895930e-06 8.75491851e-08
 2.61505198e-05 2.83482469e-06 1.73060784e-07 5.97730848e-06
 2.45202824e-06 1.48811552e-06]
ene_total = [0.01913201 0.00775057 0.02022438 0.01525607 0.00746802 0.00735947
 0.01518031 0.0130468  0.0157913  0.00710881]
At round 58 energy consumption: 0.12831774437155147
At round 58 eta: 0.7095637697055788
At round 58 a_n: 7.972397885064655
At round 58 local rounds: 11.234997825322132
At round 58 global rounds: 28.629154577598474
gradient difference: 0.6440115571022034
train() client id: f_00000-0-0 loss: 0.602558  [   32/  126]
train() client id: f_00000-0-1 loss: 0.755988  [   64/  126]
train() client id: f_00000-0-2 loss: 0.960603  [   96/  126]
train() client id: f_00000-1-0 loss: 0.625237  [   32/  126]
train() client id: f_00000-1-1 loss: 0.663717  [   64/  126]
train() client id: f_00000-1-2 loss: 1.013932  [   96/  126]
train() client id: f_00000-2-0 loss: 0.684676  [   32/  126]
train() client id: f_00000-2-1 loss: 0.745206  [   64/  126]
train() client id: f_00000-2-2 loss: 0.693044  [   96/  126]
train() client id: f_00000-3-0 loss: 0.857568  [   32/  126]
train() client id: f_00000-3-1 loss: 0.657070  [   64/  126]
train() client id: f_00000-3-2 loss: 0.645704  [   96/  126]
train() client id: f_00000-4-0 loss: 0.733068  [   32/  126]
train() client id: f_00000-4-1 loss: 0.544779  [   64/  126]
train() client id: f_00000-4-2 loss: 0.592878  [   96/  126]
train() client id: f_00000-5-0 loss: 0.510884  [   32/  126]
train() client id: f_00000-5-1 loss: 0.745089  [   64/  126]
train() client id: f_00000-5-2 loss: 0.773799  [   96/  126]
train() client id: f_00000-6-0 loss: 0.629941  [   32/  126]
train() client id: f_00000-6-1 loss: 0.702341  [   64/  126]
train() client id: f_00000-6-2 loss: 0.665283  [   96/  126]
train() client id: f_00000-7-0 loss: 0.755895  [   32/  126]
train() client id: f_00000-7-1 loss: 0.637495  [   64/  126]
train() client id: f_00000-7-2 loss: 0.593837  [   96/  126]
train() client id: f_00000-8-0 loss: 0.698463  [   32/  126]
train() client id: f_00000-8-1 loss: 0.562284  [   64/  126]
train() client id: f_00000-8-2 loss: 0.615058  [   96/  126]
train() client id: f_00000-9-0 loss: 0.601652  [   32/  126]
train() client id: f_00000-9-1 loss: 0.728565  [   64/  126]
train() client id: f_00000-9-2 loss: 0.734667  [   96/  126]
train() client id: f_00000-10-0 loss: 0.644901  [   32/  126]
train() client id: f_00000-10-1 loss: 0.737319  [   64/  126]
train() client id: f_00000-10-2 loss: 0.696573  [   96/  126]
train() client id: f_00001-0-0 loss: 0.570277  [   32/  265]
train() client id: f_00001-0-1 loss: 0.399377  [   64/  265]
train() client id: f_00001-0-2 loss: 0.531345  [   96/  265]
train() client id: f_00001-0-3 loss: 0.500236  [  128/  265]
train() client id: f_00001-0-4 loss: 0.469187  [  160/  265]
train() client id: f_00001-0-5 loss: 0.412048  [  192/  265]
train() client id: f_00001-0-6 loss: 0.501340  [  224/  265]
train() client id: f_00001-0-7 loss: 0.555802  [  256/  265]
train() client id: f_00001-1-0 loss: 0.408419  [   32/  265]
train() client id: f_00001-1-1 loss: 0.486514  [   64/  265]
train() client id: f_00001-1-2 loss: 0.398542  [   96/  265]
train() client id: f_00001-1-3 loss: 0.405690  [  128/  265]
train() client id: f_00001-1-4 loss: 0.470886  [  160/  265]
train() client id: f_00001-1-5 loss: 0.602855  [  192/  265]
train() client id: f_00001-1-6 loss: 0.653652  [  224/  265]
train() client id: f_00001-1-7 loss: 0.455066  [  256/  265]
train() client id: f_00001-2-0 loss: 0.464753  [   32/  265]
train() client id: f_00001-2-1 loss: 0.437378  [   64/  265]
train() client id: f_00001-2-2 loss: 0.487462  [   96/  265]
train() client id: f_00001-2-3 loss: 0.546531  [  128/  265]
train() client id: f_00001-2-4 loss: 0.482316  [  160/  265]
train() client id: f_00001-2-5 loss: 0.404751  [  192/  265]
train() client id: f_00001-2-6 loss: 0.440463  [  224/  265]
train() client id: f_00001-2-7 loss: 0.511122  [  256/  265]
train() client id: f_00001-3-0 loss: 0.506249  [   32/  265]
train() client id: f_00001-3-1 loss: 0.375926  [   64/  265]
train() client id: f_00001-3-2 loss: 0.385206  [   96/  265]
train() client id: f_00001-3-3 loss: 0.387922  [  128/  265]
train() client id: f_00001-3-4 loss: 0.493999  [  160/  265]
train() client id: f_00001-3-5 loss: 0.485684  [  192/  265]
train() client id: f_00001-3-6 loss: 0.667355  [  224/  265]
train() client id: f_00001-3-7 loss: 0.444198  [  256/  265]
train() client id: f_00001-4-0 loss: 0.381223  [   32/  265]
train() client id: f_00001-4-1 loss: 0.502854  [   64/  265]
train() client id: f_00001-4-2 loss: 0.413527  [   96/  265]
train() client id: f_00001-4-3 loss: 0.641677  [  128/  265]
train() client id: f_00001-4-4 loss: 0.432306  [  160/  265]
train() client id: f_00001-4-5 loss: 0.564740  [  192/  265]
train() client id: f_00001-4-6 loss: 0.462725  [  224/  265]
train() client id: f_00001-4-7 loss: 0.378110  [  256/  265]
train() client id: f_00001-5-0 loss: 0.432159  [   32/  265]
train() client id: f_00001-5-1 loss: 0.548396  [   64/  265]
train() client id: f_00001-5-2 loss: 0.479425  [   96/  265]
train() client id: f_00001-5-3 loss: 0.616131  [  128/  265]
train() client id: f_00001-5-4 loss: 0.392533  [  160/  265]
train() client id: f_00001-5-5 loss: 0.455058  [  192/  265]
train() client id: f_00001-5-6 loss: 0.400331  [  224/  265]
train() client id: f_00001-5-7 loss: 0.433213  [  256/  265]
train() client id: f_00001-6-0 loss: 0.523888  [   32/  265]
train() client id: f_00001-6-1 loss: 0.495814  [   64/  265]
train() client id: f_00001-6-2 loss: 0.463081  [   96/  265]
train() client id: f_00001-6-3 loss: 0.441797  [  128/  265]
train() client id: f_00001-6-4 loss: 0.442103  [  160/  265]
train() client id: f_00001-6-5 loss: 0.427628  [  192/  265]
train() client id: f_00001-6-6 loss: 0.379380  [  224/  265]
train() client id: f_00001-6-7 loss: 0.502706  [  256/  265]
train() client id: f_00001-7-0 loss: 0.470860  [   32/  265]
train() client id: f_00001-7-1 loss: 0.446537  [   64/  265]
train() client id: f_00001-7-2 loss: 0.518140  [   96/  265]
train() client id: f_00001-7-3 loss: 0.548713  [  128/  265]
train() client id: f_00001-7-4 loss: 0.377785  [  160/  265]
train() client id: f_00001-7-5 loss: 0.443114  [  192/  265]
train() client id: f_00001-7-6 loss: 0.455463  [  224/  265]
train() client id: f_00001-7-7 loss: 0.466063  [  256/  265]
train() client id: f_00001-8-0 loss: 0.529360  [   32/  265]
train() client id: f_00001-8-1 loss: 0.421515  [   64/  265]
train() client id: f_00001-8-2 loss: 0.463829  [   96/  265]
train() client id: f_00001-8-3 loss: 0.512818  [  128/  265]
train() client id: f_00001-8-4 loss: 0.398381  [  160/  265]
train() client id: f_00001-8-5 loss: 0.389931  [  192/  265]
train() client id: f_00001-8-6 loss: 0.587592  [  224/  265]
train() client id: f_00001-8-7 loss: 0.436378  [  256/  265]
train() client id: f_00001-9-0 loss: 0.490774  [   32/  265]
train() client id: f_00001-9-1 loss: 0.411342  [   64/  265]
train() client id: f_00001-9-2 loss: 0.485667  [   96/  265]
train() client id: f_00001-9-3 loss: 0.476938  [  128/  265]
train() client id: f_00001-9-4 loss: 0.612395  [  160/  265]
train() client id: f_00001-9-5 loss: 0.375658  [  192/  265]
train() client id: f_00001-9-6 loss: 0.386727  [  224/  265]
train() client id: f_00001-9-7 loss: 0.495764  [  256/  265]
train() client id: f_00001-10-0 loss: 0.525827  [   32/  265]
train() client id: f_00001-10-1 loss: 0.524584  [   64/  265]
train() client id: f_00001-10-2 loss: 0.607679  [   96/  265]
train() client id: f_00001-10-3 loss: 0.413483  [  128/  265]
train() client id: f_00001-10-4 loss: 0.469706  [  160/  265]
train() client id: f_00001-10-5 loss: 0.363961  [  192/  265]
train() client id: f_00001-10-6 loss: 0.414195  [  224/  265]
train() client id: f_00001-10-7 loss: 0.423414  [  256/  265]
train() client id: f_00002-0-0 loss: 1.215471  [   32/  124]
train() client id: f_00002-0-1 loss: 1.076955  [   64/  124]
train() client id: f_00002-0-2 loss: 1.156958  [   96/  124]
train() client id: f_00002-1-0 loss: 1.190051  [   32/  124]
train() client id: f_00002-1-1 loss: 0.983888  [   64/  124]
train() client id: f_00002-1-2 loss: 0.949551  [   96/  124]
train() client id: f_00002-2-0 loss: 1.117585  [   32/  124]
train() client id: f_00002-2-1 loss: 1.145215  [   64/  124]
train() client id: f_00002-2-2 loss: 1.099489  [   96/  124]
train() client id: f_00002-3-0 loss: 0.899118  [   32/  124]
train() client id: f_00002-3-1 loss: 1.069299  [   64/  124]
train() client id: f_00002-3-2 loss: 1.179577  [   96/  124]
train() client id: f_00002-4-0 loss: 1.091812  [   32/  124]
train() client id: f_00002-4-1 loss: 0.842096  [   64/  124]
train() client id: f_00002-4-2 loss: 1.028390  [   96/  124]
train() client id: f_00002-5-0 loss: 1.067180  [   32/  124]
train() client id: f_00002-5-1 loss: 1.038644  [   64/  124]
train() client id: f_00002-5-2 loss: 1.095273  [   96/  124]
train() client id: f_00002-6-0 loss: 1.038302  [   32/  124]
train() client id: f_00002-6-1 loss: 1.245918  [   64/  124]
train() client id: f_00002-6-2 loss: 1.013026  [   96/  124]
train() client id: f_00002-7-0 loss: 1.003639  [   32/  124]
train() client id: f_00002-7-1 loss: 1.196888  [   64/  124]
train() client id: f_00002-7-2 loss: 0.948336  [   96/  124]
train() client id: f_00002-8-0 loss: 1.026991  [   32/  124]
train() client id: f_00002-8-1 loss: 1.145554  [   64/  124]
train() client id: f_00002-8-2 loss: 0.903082  [   96/  124]
train() client id: f_00002-9-0 loss: 0.824070  [   32/  124]
train() client id: f_00002-9-1 loss: 1.232732  [   64/  124]
train() client id: f_00002-9-2 loss: 1.025057  [   96/  124]
train() client id: f_00002-10-0 loss: 1.119873  [   32/  124]
train() client id: f_00002-10-1 loss: 1.057798  [   64/  124]
train() client id: f_00002-10-2 loss: 0.988005  [   96/  124]
train() client id: f_00003-0-0 loss: 0.523808  [   32/   43]
train() client id: f_00003-1-0 loss: 0.586906  [   32/   43]
train() client id: f_00003-2-0 loss: 0.698960  [   32/   43]
train() client id: f_00003-3-0 loss: 0.384095  [   32/   43]
train() client id: f_00003-4-0 loss: 0.489928  [   32/   43]
train() client id: f_00003-5-0 loss: 0.447661  [   32/   43]
train() client id: f_00003-6-0 loss: 0.661715  [   32/   43]
train() client id: f_00003-7-0 loss: 0.753659  [   32/   43]
train() client id: f_00003-8-0 loss: 0.547365  [   32/   43]
train() client id: f_00003-9-0 loss: 0.393440  [   32/   43]
train() client id: f_00003-10-0 loss: 0.510216  [   32/   43]
train() client id: f_00004-0-0 loss: 0.699156  [   32/  306]
train() client id: f_00004-0-1 loss: 0.665836  [   64/  306]
train() client id: f_00004-0-2 loss: 0.588019  [   96/  306]
train() client id: f_00004-0-3 loss: 0.617130  [  128/  306]
train() client id: f_00004-0-4 loss: 0.836260  [  160/  306]
train() client id: f_00004-0-5 loss: 0.551691  [  192/  306]
train() client id: f_00004-0-6 loss: 0.654014  [  224/  306]
train() client id: f_00004-0-7 loss: 0.705088  [  256/  306]
train() client id: f_00004-0-8 loss: 0.735654  [  288/  306]
train() client id: f_00004-1-0 loss: 0.667125  [   32/  306]
train() client id: f_00004-1-1 loss: 0.674636  [   64/  306]
train() client id: f_00004-1-2 loss: 0.865943  [   96/  306]
train() client id: f_00004-1-3 loss: 0.621743  [  128/  306]
train() client id: f_00004-1-4 loss: 0.567879  [  160/  306]
train() client id: f_00004-1-5 loss: 0.555845  [  192/  306]
train() client id: f_00004-1-6 loss: 0.649863  [  224/  306]
train() client id: f_00004-1-7 loss: 0.781595  [  256/  306]
train() client id: f_00004-1-8 loss: 0.756079  [  288/  306]
train() client id: f_00004-2-0 loss: 0.723182  [   32/  306]
train() client id: f_00004-2-1 loss: 0.608999  [   64/  306]
train() client id: f_00004-2-2 loss: 0.687799  [   96/  306]
train() client id: f_00004-2-3 loss: 0.811008  [  128/  306]
train() client id: f_00004-2-4 loss: 0.560438  [  160/  306]
train() client id: f_00004-2-5 loss: 0.706168  [  192/  306]
train() client id: f_00004-2-6 loss: 0.670706  [  224/  306]
train() client id: f_00004-2-7 loss: 0.599545  [  256/  306]
train() client id: f_00004-2-8 loss: 0.756541  [  288/  306]
train() client id: f_00004-3-0 loss: 0.788944  [   32/  306]
train() client id: f_00004-3-1 loss: 0.560269  [   64/  306]
train() client id: f_00004-3-2 loss: 0.724262  [   96/  306]
train() client id: f_00004-3-3 loss: 0.624188  [  128/  306]
train() client id: f_00004-3-4 loss: 0.628321  [  160/  306]
train() client id: f_00004-3-5 loss: 0.749186  [  192/  306]
train() client id: f_00004-3-6 loss: 0.639768  [  224/  306]
train() client id: f_00004-3-7 loss: 0.700093  [  256/  306]
train() client id: f_00004-3-8 loss: 0.723616  [  288/  306]
train() client id: f_00004-4-0 loss: 0.746937  [   32/  306]
train() client id: f_00004-4-1 loss: 0.666870  [   64/  306]
train() client id: f_00004-4-2 loss: 0.633102  [   96/  306]
train() client id: f_00004-4-3 loss: 0.715813  [  128/  306]
train() client id: f_00004-4-4 loss: 0.706445  [  160/  306]
train() client id: f_00004-4-5 loss: 0.626351  [  192/  306]
train() client id: f_00004-4-6 loss: 0.695815  [  224/  306]
train() client id: f_00004-4-7 loss: 0.656031  [  256/  306]
train() client id: f_00004-4-8 loss: 0.697420  [  288/  306]
train() client id: f_00004-5-0 loss: 0.748285  [   32/  306]
train() client id: f_00004-5-1 loss: 0.810597  [   64/  306]
train() client id: f_00004-5-2 loss: 0.649192  [   96/  306]
train() client id: f_00004-5-3 loss: 0.738486  [  128/  306]
train() client id: f_00004-5-4 loss: 0.692553  [  160/  306]
train() client id: f_00004-5-5 loss: 0.702905  [  192/  306]
train() client id: f_00004-5-6 loss: 0.750887  [  224/  306]
train() client id: f_00004-5-7 loss: 0.545199  [  256/  306]
train() client id: f_00004-5-8 loss: 0.618613  [  288/  306]
train() client id: f_00004-6-0 loss: 0.752671  [   32/  306]
train() client id: f_00004-6-1 loss: 0.715320  [   64/  306]
train() client id: f_00004-6-2 loss: 0.814790  [   96/  306]
train() client id: f_00004-6-3 loss: 0.658489  [  128/  306]
train() client id: f_00004-6-4 loss: 0.654891  [  160/  306]
train() client id: f_00004-6-5 loss: 0.647945  [  192/  306]
train() client id: f_00004-6-6 loss: 0.678593  [  224/  306]
train() client id: f_00004-6-7 loss: 0.697692  [  256/  306]
train() client id: f_00004-6-8 loss: 0.653752  [  288/  306]
train() client id: f_00004-7-0 loss: 0.770465  [   32/  306]
train() client id: f_00004-7-1 loss: 0.590194  [   64/  306]
train() client id: f_00004-7-2 loss: 0.665319  [   96/  306]
train() client id: f_00004-7-3 loss: 0.636525  [  128/  306]
train() client id: f_00004-7-4 loss: 0.759613  [  160/  306]
train() client id: f_00004-7-5 loss: 0.715339  [  192/  306]
train() client id: f_00004-7-6 loss: 0.673102  [  224/  306]
train() client id: f_00004-7-7 loss: 0.728804  [  256/  306]
train() client id: f_00004-7-8 loss: 0.732135  [  288/  306]
train() client id: f_00004-8-0 loss: 0.656972  [   32/  306]
train() client id: f_00004-8-1 loss: 0.560849  [   64/  306]
train() client id: f_00004-8-2 loss: 0.598097  [   96/  306]
train() client id: f_00004-8-3 loss: 0.657436  [  128/  306]
train() client id: f_00004-8-4 loss: 0.669991  [  160/  306]
train() client id: f_00004-8-5 loss: 0.870324  [  192/  306]
train() client id: f_00004-8-6 loss: 0.798290  [  224/  306]
train() client id: f_00004-8-7 loss: 0.770634  [  256/  306]
train() client id: f_00004-8-8 loss: 0.654829  [  288/  306]
train() client id: f_00004-9-0 loss: 0.691646  [   32/  306]
train() client id: f_00004-9-1 loss: 0.524668  [   64/  306]
train() client id: f_00004-9-2 loss: 0.617123  [   96/  306]
train() client id: f_00004-9-3 loss: 0.822361  [  128/  306]
train() client id: f_00004-9-4 loss: 0.809626  [  160/  306]
train() client id: f_00004-9-5 loss: 0.701731  [  192/  306]
train() client id: f_00004-9-6 loss: 0.626443  [  224/  306]
train() client id: f_00004-9-7 loss: 0.742590  [  256/  306]
train() client id: f_00004-9-8 loss: 0.668334  [  288/  306]
train() client id: f_00004-10-0 loss: 0.736135  [   32/  306]
train() client id: f_00004-10-1 loss: 0.698511  [   64/  306]
train() client id: f_00004-10-2 loss: 0.703094  [   96/  306]
train() client id: f_00004-10-3 loss: 0.727565  [  128/  306]
train() client id: f_00004-10-4 loss: 0.715757  [  160/  306]
train() client id: f_00004-10-5 loss: 0.649155  [  192/  306]
train() client id: f_00004-10-6 loss: 0.768836  [  224/  306]
train() client id: f_00004-10-7 loss: 0.660673  [  256/  306]
train() client id: f_00004-10-8 loss: 0.702878  [  288/  306]
train() client id: f_00005-0-0 loss: 0.617219  [   32/  146]
train() client id: f_00005-0-1 loss: 0.518234  [   64/  146]
train() client id: f_00005-0-2 loss: 0.590565  [   96/  146]
train() client id: f_00005-0-3 loss: 0.580296  [  128/  146]
train() client id: f_00005-1-0 loss: 0.715793  [   32/  146]
train() client id: f_00005-1-1 loss: 0.762198  [   64/  146]
train() client id: f_00005-1-2 loss: 0.465286  [   96/  146]
train() client id: f_00005-1-3 loss: 0.397749  [  128/  146]
train() client id: f_00005-2-0 loss: 0.337038  [   32/  146]
train() client id: f_00005-2-1 loss: 0.574176  [   64/  146]
train() client id: f_00005-2-2 loss: 0.832672  [   96/  146]
train() client id: f_00005-2-3 loss: 0.597319  [  128/  146]
train() client id: f_00005-3-0 loss: 0.478815  [   32/  146]
train() client id: f_00005-3-1 loss: 0.450038  [   64/  146]
train() client id: f_00005-3-2 loss: 0.647734  [   96/  146]
train() client id: f_00005-3-3 loss: 0.619330  [  128/  146]
train() client id: f_00005-4-0 loss: 0.604664  [   32/  146]
train() client id: f_00005-4-1 loss: 0.641531  [   64/  146]
train() client id: f_00005-4-2 loss: 0.397807  [   96/  146]
train() client id: f_00005-4-3 loss: 0.598427  [  128/  146]
train() client id: f_00005-5-0 loss: 0.658502  [   32/  146]
train() client id: f_00005-5-1 loss: 0.503710  [   64/  146]
train() client id: f_00005-5-2 loss: 0.692536  [   96/  146]
train() client id: f_00005-5-3 loss: 0.395961  [  128/  146]
train() client id: f_00005-6-0 loss: 0.565425  [   32/  146]
train() client id: f_00005-6-1 loss: 0.573492  [   64/  146]
train() client id: f_00005-6-2 loss: 0.658993  [   96/  146]
train() client id: f_00005-6-3 loss: 0.466956  [  128/  146]
train() client id: f_00005-7-0 loss: 0.679675  [   32/  146]
train() client id: f_00005-7-1 loss: 0.388854  [   64/  146]
train() client id: f_00005-7-2 loss: 0.802684  [   96/  146]
train() client id: f_00005-7-3 loss: 0.498124  [  128/  146]
train() client id: f_00005-8-0 loss: 0.603324  [   32/  146]
train() client id: f_00005-8-1 loss: 0.554911  [   64/  146]
train() client id: f_00005-8-2 loss: 0.652763  [   96/  146]
train() client id: f_00005-8-3 loss: 0.537921  [  128/  146]
train() client id: f_00005-9-0 loss: 0.696198  [   32/  146]
train() client id: f_00005-9-1 loss: 0.751505  [   64/  146]
train() client id: f_00005-9-2 loss: 0.469601  [   96/  146]
train() client id: f_00005-9-3 loss: 0.465328  [  128/  146]
train() client id: f_00005-10-0 loss: 0.647375  [   32/  146]
train() client id: f_00005-10-1 loss: 0.577641  [   64/  146]
train() client id: f_00005-10-2 loss: 0.435901  [   96/  146]
train() client id: f_00005-10-3 loss: 0.650624  [  128/  146]
train() client id: f_00006-0-0 loss: 0.610588  [   32/   54]
train() client id: f_00006-1-0 loss: 0.580627  [   32/   54]
train() client id: f_00006-2-0 loss: 0.514187  [   32/   54]
train() client id: f_00006-3-0 loss: 0.551303  [   32/   54]
train() client id: f_00006-4-0 loss: 0.559053  [   32/   54]
train() client id: f_00006-5-0 loss: 0.590766  [   32/   54]
train() client id: f_00006-6-0 loss: 0.558110  [   32/   54]
train() client id: f_00006-7-0 loss: 0.588005  [   32/   54]
train() client id: f_00006-8-0 loss: 0.538023  [   32/   54]
train() client id: f_00006-9-0 loss: 0.495968  [   32/   54]
train() client id: f_00006-10-0 loss: 0.503952  [   32/   54]
train() client id: f_00007-0-0 loss: 0.526451  [   32/  179]
train() client id: f_00007-0-1 loss: 0.775278  [   64/  179]
train() client id: f_00007-0-2 loss: 0.642936  [   96/  179]
train() client id: f_00007-0-3 loss: 0.579270  [  128/  179]
train() client id: f_00007-0-4 loss: 0.697883  [  160/  179]
train() client id: f_00007-1-0 loss: 0.537167  [   32/  179]
train() client id: f_00007-1-1 loss: 0.854576  [   64/  179]
train() client id: f_00007-1-2 loss: 0.522706  [   96/  179]
train() client id: f_00007-1-3 loss: 0.666822  [  128/  179]
train() client id: f_00007-1-4 loss: 0.606992  [  160/  179]
train() client id: f_00007-2-0 loss: 0.732090  [   32/  179]
train() client id: f_00007-2-1 loss: 0.454378  [   64/  179]
train() client id: f_00007-2-2 loss: 0.457148  [   96/  179]
train() client id: f_00007-2-3 loss: 0.679718  [  128/  179]
train() client id: f_00007-2-4 loss: 0.915833  [  160/  179]
train() client id: f_00007-3-0 loss: 0.516307  [   32/  179]
train() client id: f_00007-3-1 loss: 0.500910  [   64/  179]
train() client id: f_00007-3-2 loss: 0.548121  [   96/  179]
train() client id: f_00007-3-3 loss: 0.481545  [  128/  179]
train() client id: f_00007-3-4 loss: 0.859152  [  160/  179]
train() client id: f_00007-4-0 loss: 0.628920  [   32/  179]
train() client id: f_00007-4-1 loss: 0.549861  [   64/  179]
train() client id: f_00007-4-2 loss: 0.681782  [   96/  179]
train() client id: f_00007-4-3 loss: 0.758939  [  128/  179]
train() client id: f_00007-4-4 loss: 0.483177  [  160/  179]
train() client id: f_00007-5-0 loss: 0.745182  [   32/  179]
train() client id: f_00007-5-1 loss: 0.518439  [   64/  179]
train() client id: f_00007-5-2 loss: 0.521319  [   96/  179]
train() client id: f_00007-5-3 loss: 0.670376  [  128/  179]
train() client id: f_00007-5-4 loss: 0.515411  [  160/  179]
train() client id: f_00007-6-0 loss: 0.886940  [   32/  179]
train() client id: f_00007-6-1 loss: 0.614832  [   64/  179]
train() client id: f_00007-6-2 loss: 0.571337  [   96/  179]
train() client id: f_00007-6-3 loss: 0.608714  [  128/  179]
train() client id: f_00007-6-4 loss: 0.450861  [  160/  179]
train() client id: f_00007-7-0 loss: 0.662721  [   32/  179]
train() client id: f_00007-7-1 loss: 0.673836  [   64/  179]
train() client id: f_00007-7-2 loss: 0.724090  [   96/  179]
train() client id: f_00007-7-3 loss: 0.500884  [  128/  179]
train() client id: f_00007-7-4 loss: 0.555252  [  160/  179]
train() client id: f_00007-8-0 loss: 0.672372  [   32/  179]
train() client id: f_00007-8-1 loss: 0.662912  [   64/  179]
train() client id: f_00007-8-2 loss: 0.583129  [   96/  179]
train() client id: f_00007-8-3 loss: 0.518992  [  128/  179]
train() client id: f_00007-8-4 loss: 0.663813  [  160/  179]
train() client id: f_00007-9-0 loss: 0.451531  [   32/  179]
train() client id: f_00007-9-1 loss: 0.608444  [   64/  179]
train() client id: f_00007-9-2 loss: 0.832413  [   96/  179]
train() client id: f_00007-9-3 loss: 0.527850  [  128/  179]
train() client id: f_00007-9-4 loss: 0.532766  [  160/  179]
train() client id: f_00007-10-0 loss: 0.633919  [   32/  179]
train() client id: f_00007-10-1 loss: 0.669442  [   64/  179]
train() client id: f_00007-10-2 loss: 0.783639  [   96/  179]
train() client id: f_00007-10-3 loss: 0.431081  [  128/  179]
train() client id: f_00007-10-4 loss: 0.580356  [  160/  179]
train() client id: f_00008-0-0 loss: 0.580852  [   32/  130]
train() client id: f_00008-0-1 loss: 0.597226  [   64/  130]
train() client id: f_00008-0-2 loss: 0.746543  [   96/  130]
train() client id: f_00008-0-3 loss: 0.756683  [  128/  130]
train() client id: f_00008-1-0 loss: 0.748871  [   32/  130]
train() client id: f_00008-1-1 loss: 0.736322  [   64/  130]
train() client id: f_00008-1-2 loss: 0.564686  [   96/  130]
train() client id: f_00008-1-3 loss: 0.606058  [  128/  130]
train() client id: f_00008-2-0 loss: 0.558441  [   32/  130]
train() client id: f_00008-2-1 loss: 0.622202  [   64/  130]
train() client id: f_00008-2-2 loss: 0.781192  [   96/  130]
train() client id: f_00008-2-3 loss: 0.714294  [  128/  130]
train() client id: f_00008-3-0 loss: 0.751320  [   32/  130]
train() client id: f_00008-3-1 loss: 0.691791  [   64/  130]
train() client id: f_00008-3-2 loss: 0.586470  [   96/  130]
train() client id: f_00008-3-3 loss: 0.638368  [  128/  130]
train() client id: f_00008-4-0 loss: 0.603476  [   32/  130]
train() client id: f_00008-4-1 loss: 0.600571  [   64/  130]
train() client id: f_00008-4-2 loss: 0.754974  [   96/  130]
train() client id: f_00008-4-3 loss: 0.682558  [  128/  130]
train() client id: f_00008-5-0 loss: 0.742600  [   32/  130]
train() client id: f_00008-5-1 loss: 0.640618  [   64/  130]
train() client id: f_00008-5-2 loss: 0.690084  [   96/  130]
train() client id: f_00008-5-3 loss: 0.590977  [  128/  130]
train() client id: f_00008-6-0 loss: 0.684406  [   32/  130]
train() client id: f_00008-6-1 loss: 0.613105  [   64/  130]
train() client id: f_00008-6-2 loss: 0.743710  [   96/  130]
train() client id: f_00008-6-3 loss: 0.625767  [  128/  130]
train() client id: f_00008-7-0 loss: 0.630048  [   32/  130]
train() client id: f_00008-7-1 loss: 0.705024  [   64/  130]
train() client id: f_00008-7-2 loss: 0.716731  [   96/  130]
train() client id: f_00008-7-3 loss: 0.620125  [  128/  130]
train() client id: f_00008-8-0 loss: 0.675192  [   32/  130]
train() client id: f_00008-8-1 loss: 0.634861  [   64/  130]
train() client id: f_00008-8-2 loss: 0.685155  [   96/  130]
train() client id: f_00008-8-3 loss: 0.636345  [  128/  130]
train() client id: f_00008-9-0 loss: 0.629453  [   32/  130]
train() client id: f_00008-9-1 loss: 0.699032  [   64/  130]
train() client id: f_00008-9-2 loss: 0.595969  [   96/  130]
train() client id: f_00008-9-3 loss: 0.733788  [  128/  130]
train() client id: f_00008-10-0 loss: 0.734526  [   32/  130]
train() client id: f_00008-10-1 loss: 0.604411  [   64/  130]
train() client id: f_00008-10-2 loss: 0.673527  [   96/  130]
train() client id: f_00008-10-3 loss: 0.623720  [  128/  130]
train() client id: f_00009-0-0 loss: 0.937045  [   32/  118]
train() client id: f_00009-0-1 loss: 0.919235  [   64/  118]
train() client id: f_00009-0-2 loss: 0.891763  [   96/  118]
train() client id: f_00009-1-0 loss: 0.852576  [   32/  118]
train() client id: f_00009-1-1 loss: 0.924440  [   64/  118]
train() client id: f_00009-1-2 loss: 0.973167  [   96/  118]
train() client id: f_00009-2-0 loss: 0.896688  [   32/  118]
train() client id: f_00009-2-1 loss: 0.752186  [   64/  118]
train() client id: f_00009-2-2 loss: 0.905784  [   96/  118]
train() client id: f_00009-3-0 loss: 0.774222  [   32/  118]
train() client id: f_00009-3-1 loss: 0.725731  [   64/  118]
train() client id: f_00009-3-2 loss: 0.886279  [   96/  118]
train() client id: f_00009-4-0 loss: 0.925940  [   32/  118]
train() client id: f_00009-4-1 loss: 0.805193  [   64/  118]
train() client id: f_00009-4-2 loss: 0.690629  [   96/  118]
train() client id: f_00009-5-0 loss: 0.746732  [   32/  118]
train() client id: f_00009-5-1 loss: 0.650744  [   64/  118]
train() client id: f_00009-5-2 loss: 0.880207  [   96/  118]
train() client id: f_00009-6-0 loss: 0.770690  [   32/  118]
train() client id: f_00009-6-1 loss: 0.687091  [   64/  118]
train() client id: f_00009-6-2 loss: 0.816778  [   96/  118]
train() client id: f_00009-7-0 loss: 0.707129  [   32/  118]
train() client id: f_00009-7-1 loss: 0.853874  [   64/  118]
train() client id: f_00009-7-2 loss: 0.629434  [   96/  118]
train() client id: f_00009-8-0 loss: 0.815906  [   32/  118]
train() client id: f_00009-8-1 loss: 0.666418  [   64/  118]
train() client id: f_00009-8-2 loss: 0.664662  [   96/  118]
train() client id: f_00009-9-0 loss: 0.626436  [   32/  118]
train() client id: f_00009-9-1 loss: 0.832958  [   64/  118]
train() client id: f_00009-9-2 loss: 0.795168  [   96/  118]
train() client id: f_00009-10-0 loss: 0.694251  [   32/  118]
train() client id: f_00009-10-1 loss: 0.763207  [   64/  118]
train() client id: f_00009-10-2 loss: 0.655659  [   96/  118]
At round 58 accuracy: 0.6472148541114059
At round 58 training accuracy: 0.5881958417169685
At round 58 training loss: 0.8260858789152971
update_location
xs = [  -3.9056584     4.20031788  310.00902392   18.81129433    0.97929623
    3.95640986 -272.44319194 -251.32485185  294.66397685 -237.06087855]
ys = [ 302.5879595   285.55583871    1.32061395 -272.45517586  264.35018685
  247.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [318.70790295 302.58846589 325.74121466 290.83618696 282.63400416
 267.25924958 290.22781289 270.490032   311.66573316 257.32056272]
dists_bs = [213.4290541  210.55960534 515.16820789 487.62583218 197.36303755
 193.20273595 202.59765721 190.16660451 495.34711972 181.91219277]
uav_gains = [1.90906750e-12 2.54420374e-12 1.70337274e-12 3.20136059e-12
 3.78635653e-12 5.22235086e-12 3.24089386e-12 4.88091679e-12
 2.15488058e-12 6.41123405e-12]
bs_gains = [3.32188179e-11 3.45019724e-11 2.81732361e-12 3.28587708e-12
 4.13570723e-11 4.38992321e-11 3.84341882e-11 4.58900129e-11
 3.14446733e-12 5.19614211e-11]
Round 59
-------------------------------
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.22013439 6.49883235 3.18055184 1.17195349 7.49218211 3.60548997
 1.43848068 4.45606788 3.28586187 2.92340788]
obj_prev = 37.272962461679306
eta_min = 1.7285175423879787e-29	eta_max = 0.9377803249311158
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 8.56318552740206	eta = 0.909090909090909
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 19.714881852065698	eta = 0.3948648627079844
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 13.689416383079331	eta = 0.5686666179167633
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 12.622409124377576	eta = 0.6167375846489944
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 12.557303683729339	eta = 0.6199351637809642
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 12.557030281762911	eta = 0.6199486615180114
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 12.557030276907934	eta = 0.6199486617577047
eta = 0.6199486617577047
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [0.0394532  0.08297697 0.03882696 0.01346419 0.09581492 0.04571561
 0.01690851 0.0560486  0.04070568 0.03694824]
ene_total = [1.24429988 1.90408729 1.25458552 0.61484934 2.16708445 1.11417022
 0.68545833 1.4588381  1.18872479 0.92493235]
ti_comp = [1.09084903 1.21156017 1.07968336 1.13102359 1.21456653 1.21550713
 1.13180693 1.15453068 1.12986699 1.21804473]
ti_coms = [0.19863687 0.07792573 0.20980254 0.1584623  0.07491936 0.07397877
 0.15767897 0.13495521 0.1596189  0.07144116]
t_total = [27.04975243 27.04975243 27.04975243 27.04975243 27.04975243 27.04975243
 27.04975243 27.04975243 27.04975243 27.04975243]
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [3.22550318e-06 2.43255565e-05 3.13824884e-06 1.19255159e-07
 3.72680608e-05 4.04164983e-06 2.35858602e-07 8.25587810e-06
 3.30210249e-06 2.12488134e-06]
ene_total = [0.4167514  0.16397633 0.44017198 0.33241142 0.15794132 0.15527123
 0.33077065 0.2832709  0.3349044  0.14990784]
optimize_network iter = 0 obj = 2.7653774631169483
eta = 0.6199486617577047
freqs = [18083713.04857494 34243850.94632931 17980716.29606591  5952213.52469169
 39444079.62545728 18805161.01539245  7469699.35866398 24273324.57197437
 18013484.95120145 15167027.70060071]
eta_min = 0.6199486617577048	eta_max = 0.7133529451356779
af = 0.0014346255173217418	bf = 1.0502994844473539	zeta = 0.001578088069053916	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [6.42985407e-07 4.84915903e-06 6.25591760e-07 2.37728263e-08
 7.42917241e-06 8.05679522e-07 4.70170484e-08 1.64576156e-06
 6.58255038e-07 4.23582807e-07]
ene_total = [1.74202974 0.68380564 1.83994709 1.38965906 0.65766716 0.64883763
 1.38279153 1.18365265 1.39985765 0.62655025]
ti_comp = [0.77393525 0.89464639 0.76276958 0.81410981 0.89765275 0.89859335
 0.81489315 0.8376169  0.81295321 0.90113095]
ti_coms = [0.19863687 0.07792573 0.20980254 0.1584623  0.07491936 0.07397877
 0.15767897 0.13495521 0.1596189  0.07144116]
t_total = [27.04975243 27.04975243 27.04975243 27.04975243 27.04975243 27.04975243
 27.04975243 27.04975243 27.04975243 27.04975243]
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [2.25947941e-06 1.57304375e-05 2.21709516e-06 8.11606270e-08
 2.40576692e-05 2.60758319e-06 1.60430136e-07 5.53062528e-06
 2.24908028e-06 1.36891415e-06]
ene_total = [0.55252348 0.21716916 0.58357692 0.44072701 0.20903928 0.20582665
 0.43855056 0.37549926 0.44400411 0.19873447]
optimize_network iter = 1 obj = 3.6656508882711805
eta = 0.7133529451356779
freqs = [18007133.55773868 32762205.35417381 17980716.2960659   5842034.71544074
 37704373.23913116 17970837.74832523  7329456.31607533 23636668.595279
 17687093.31024747 14483474.75860257]
eta_min = 0.7133529451356788	eta_max = 0.7133529451356644
af = 0.0013279300383138625	bf = 1.0502994844473539	zeta = 0.0014607230421452488	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [6.37551209e-07 4.43861511e-06 6.25591760e-07 2.29008752e-08
 6.78828762e-06 7.35774715e-07 4.52681388e-08 1.56056161e-06
 6.34616915e-07 3.86262812e-07]
ene_total = [1.74202926 0.68376964 1.83994709 1.38965899 0.65761096 0.6488315
 1.38279138 1.18364518 1.39985558 0.62654698]
ti_comp = [0.77393525 0.89464639 0.76276958 0.81410981 0.89765275 0.89859335
 0.81489315 0.8376169  0.81295321 0.90113095]
ti_coms = [0.19863687 0.07792573 0.20980254 0.1584623  0.07491936 0.07397877
 0.15767897 0.13495521 0.1596189  0.07144116]
t_total = [27.04975243 27.04975243 27.04975243 27.04975243 27.04975243 27.04975243
 27.04975243 27.04975243 27.04975243 27.04975243]
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [2.25947941e-06 1.57304375e-05 2.21709516e-06 8.11606270e-08
 2.40576692e-05 2.60758319e-06 1.60430136e-07 5.53062528e-06
 2.24908028e-06 1.36891415e-06]
ene_total = [0.55252348 0.21716916 0.58357692 0.44072701 0.20903928 0.20582665
 0.43855056 0.37549926 0.44400411 0.19873447]
optimize_network iter = 2 obj = 3.6656508882710073
eta = 0.7133529451356644
freqs = [18007133.55773862 32762205.35417396 17980716.29606583  5842034.71544073
 37704373.23913135 17970837.74832532  7329456.31607533 23636668.59527903
 17687093.31024747 14483474.75860265]
Done!
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [2.24710003e-06 1.56442526e-05 2.20494800e-06 8.07159591e-08
 2.39258606e-05 2.59329661e-06 1.59551162e-07 5.50032375e-06
 2.23675788e-06 1.36141406e-06]
ene_total = [0.01986593 0.00780822 0.02098246 0.01584631 0.00751586 0.00740047
 0.01576806 0.01350102 0.01596413 0.00714548]
At round 59 energy consumption: 0.131797934436514
At round 59 eta: 0.7133529451356644
At round 59 a_n: 7.62985203809534
At round 59 local rounds: 11.060599497015481
At round 59 global rounds: 27.812593047006303
gradient difference: 0.49410533905029297
train() client id: f_00000-0-0 loss: 1.114949  [   32/  126]
train() client id: f_00000-0-1 loss: 1.235427  [   64/  126]
train() client id: f_00000-0-2 loss: 1.039362  [   96/  126]
train() client id: f_00000-1-0 loss: 1.021817  [   32/  126]
train() client id: f_00000-1-1 loss: 0.951445  [   64/  126]
train() client id: f_00000-1-2 loss: 1.173752  [   96/  126]
train() client id: f_00000-2-0 loss: 1.064114  [   32/  126]
train() client id: f_00000-2-1 loss: 1.031520  [   64/  126]
train() client id: f_00000-2-2 loss: 0.893455  [   96/  126]
train() client id: f_00000-3-0 loss: 0.863914  [   32/  126]
train() client id: f_00000-3-1 loss: 1.004799  [   64/  126]
train() client id: f_00000-3-2 loss: 0.958060  [   96/  126]
train() client id: f_00000-4-0 loss: 0.908245  [   32/  126]
train() client id: f_00000-4-1 loss: 0.982405  [   64/  126]
train() client id: f_00000-4-2 loss: 0.784013  [   96/  126]
train() client id: f_00000-5-0 loss: 0.791319  [   32/  126]
train() client id: f_00000-5-1 loss: 0.979787  [   64/  126]
train() client id: f_00000-5-2 loss: 0.904351  [   96/  126]
train() client id: f_00000-6-0 loss: 0.835641  [   32/  126]
train() client id: f_00000-6-1 loss: 1.004131  [   64/  126]
train() client id: f_00000-6-2 loss: 0.868199  [   96/  126]
train() client id: f_00000-7-0 loss: 0.831080  [   32/  126]
train() client id: f_00000-7-1 loss: 0.926041  [   64/  126]
train() client id: f_00000-7-2 loss: 0.891166  [   96/  126]
train() client id: f_00000-8-0 loss: 0.922044  [   32/  126]
train() client id: f_00000-8-1 loss: 0.827582  [   64/  126]
train() client id: f_00000-8-2 loss: 0.897323  [   96/  126]
train() client id: f_00000-9-0 loss: 0.923290  [   32/  126]
train() client id: f_00000-9-1 loss: 0.738131  [   64/  126]
train() client id: f_00000-9-2 loss: 0.850801  [   96/  126]
train() client id: f_00000-10-0 loss: 0.850271  [   32/  126]
train() client id: f_00000-10-1 loss: 0.821502  [   64/  126]
train() client id: f_00000-10-2 loss: 0.909034  [   96/  126]
train() client id: f_00001-0-0 loss: 0.399972  [   32/  265]
train() client id: f_00001-0-1 loss: 0.592065  [   64/  265]
train() client id: f_00001-0-2 loss: 0.450578  [   96/  265]
train() client id: f_00001-0-3 loss: 0.419617  [  128/  265]
train() client id: f_00001-0-4 loss: 0.446248  [  160/  265]
train() client id: f_00001-0-5 loss: 0.519095  [  192/  265]
train() client id: f_00001-0-6 loss: 0.447018  [  224/  265]
train() client id: f_00001-0-7 loss: 0.408917  [  256/  265]
train() client id: f_00001-1-0 loss: 0.459401  [   32/  265]
train() client id: f_00001-1-1 loss: 0.498885  [   64/  265]
train() client id: f_00001-1-2 loss: 0.462211  [   96/  265]
train() client id: f_00001-1-3 loss: 0.394097  [  128/  265]
train() client id: f_00001-1-4 loss: 0.449610  [  160/  265]
train() client id: f_00001-1-5 loss: 0.527210  [  192/  265]
train() client id: f_00001-1-6 loss: 0.432518  [  224/  265]
train() client id: f_00001-1-7 loss: 0.468778  [  256/  265]
train() client id: f_00001-2-0 loss: 0.553227  [   32/  265]
train() client id: f_00001-2-1 loss: 0.465289  [   64/  265]
train() client id: f_00001-2-2 loss: 0.339301  [   96/  265]
train() client id: f_00001-2-3 loss: 0.506061  [  128/  265]
train() client id: f_00001-2-4 loss: 0.423313  [  160/  265]
train() client id: f_00001-2-5 loss: 0.425941  [  192/  265]
train() client id: f_00001-2-6 loss: 0.398838  [  224/  265]
train() client id: f_00001-2-7 loss: 0.512267  [  256/  265]
train() client id: f_00001-3-0 loss: 0.482764  [   32/  265]
train() client id: f_00001-3-1 loss: 0.443497  [   64/  265]
train() client id: f_00001-3-2 loss: 0.566255  [   96/  265]
train() client id: f_00001-3-3 loss: 0.402642  [  128/  265]
train() client id: f_00001-3-4 loss: 0.507271  [  160/  265]
train() client id: f_00001-3-5 loss: 0.356736  [  192/  265]
train() client id: f_00001-3-6 loss: 0.452475  [  224/  265]
train() client id: f_00001-3-7 loss: 0.358575  [  256/  265]
train() client id: f_00001-4-0 loss: 0.540469  [   32/  265]
train() client id: f_00001-4-1 loss: 0.472121  [   64/  265]
train() client id: f_00001-4-2 loss: 0.404578  [   96/  265]
train() client id: f_00001-4-3 loss: 0.433330  [  128/  265]
train() client id: f_00001-4-4 loss: 0.396566  [  160/  265]
train() client id: f_00001-4-5 loss: 0.408767  [  192/  265]
train() client id: f_00001-4-6 loss: 0.506814  [  224/  265]
train() client id: f_00001-4-7 loss: 0.425307  [  256/  265]
train() client id: f_00001-5-0 loss: 0.454644  [   32/  265]
train() client id: f_00001-5-1 loss: 0.430418  [   64/  265]
train() client id: f_00001-5-2 loss: 0.535569  [   96/  265]
train() client id: f_00001-5-3 loss: 0.500431  [  128/  265]
train() client id: f_00001-5-4 loss: 0.458476  [  160/  265]
train() client id: f_00001-5-5 loss: 0.340466  [  192/  265]
train() client id: f_00001-5-6 loss: 0.448508  [  224/  265]
train() client id: f_00001-5-7 loss: 0.469903  [  256/  265]
train() client id: f_00001-6-0 loss: 0.502773  [   32/  265]
train() client id: f_00001-6-1 loss: 0.516217  [   64/  265]
train() client id: f_00001-6-2 loss: 0.417694  [   96/  265]
train() client id: f_00001-6-3 loss: 0.453654  [  128/  265]
train() client id: f_00001-6-4 loss: 0.446064  [  160/  265]
train() client id: f_00001-6-5 loss: 0.513501  [  192/  265]
train() client id: f_00001-6-6 loss: 0.394180  [  224/  265]
train() client id: f_00001-6-7 loss: 0.378992  [  256/  265]
train() client id: f_00001-7-0 loss: 0.376448  [   32/  265]
train() client id: f_00001-7-1 loss: 0.359720  [   64/  265]
train() client id: f_00001-7-2 loss: 0.536704  [   96/  265]
train() client id: f_00001-7-3 loss: 0.444571  [  128/  265]
train() client id: f_00001-7-4 loss: 0.557033  [  160/  265]
train() client id: f_00001-7-5 loss: 0.402276  [  192/  265]
train() client id: f_00001-7-6 loss: 0.406618  [  224/  265]
train() client id: f_00001-7-7 loss: 0.532340  [  256/  265]
train() client id: f_00001-8-0 loss: 0.418964  [   32/  265]
train() client id: f_00001-8-1 loss: 0.358581  [   64/  265]
train() client id: f_00001-8-2 loss: 0.367734  [   96/  265]
train() client id: f_00001-8-3 loss: 0.348569  [  128/  265]
train() client id: f_00001-8-4 loss: 0.435593  [  160/  265]
train() client id: f_00001-8-5 loss: 0.362392  [  192/  265]
train() client id: f_00001-8-6 loss: 0.654125  [  224/  265]
train() client id: f_00001-8-7 loss: 0.477636  [  256/  265]
train() client id: f_00001-9-0 loss: 0.414698  [   32/  265]
train() client id: f_00001-9-1 loss: 0.512609  [   64/  265]
train() client id: f_00001-9-2 loss: 0.702684  [   96/  265]
train() client id: f_00001-9-3 loss: 0.382188  [  128/  265]
train() client id: f_00001-9-4 loss: 0.352063  [  160/  265]
train() client id: f_00001-9-5 loss: 0.373649  [  192/  265]
train() client id: f_00001-9-6 loss: 0.521029  [  224/  265]
train() client id: f_00001-9-7 loss: 0.367060  [  256/  265]
train() client id: f_00001-10-0 loss: 0.506001  [   32/  265]
train() client id: f_00001-10-1 loss: 0.428808  [   64/  265]
train() client id: f_00001-10-2 loss: 0.539493  [   96/  265]
train() client id: f_00001-10-3 loss: 0.404276  [  128/  265]
train() client id: f_00001-10-4 loss: 0.481799  [  160/  265]
train() client id: f_00001-10-5 loss: 0.453132  [  192/  265]
train() client id: f_00001-10-6 loss: 0.460201  [  224/  265]
train() client id: f_00001-10-7 loss: 0.360909  [  256/  265]
train() client id: f_00002-0-0 loss: 1.125310  [   32/  124]
train() client id: f_00002-0-1 loss: 1.154412  [   64/  124]
train() client id: f_00002-0-2 loss: 1.175536  [   96/  124]
train() client id: f_00002-1-0 loss: 1.323672  [   32/  124]
train() client id: f_00002-1-1 loss: 1.115485  [   64/  124]
train() client id: f_00002-1-2 loss: 1.088972  [   96/  124]
train() client id: f_00002-2-0 loss: 1.088928  [   32/  124]
train() client id: f_00002-2-1 loss: 1.008471  [   64/  124]
train() client id: f_00002-2-2 loss: 1.033278  [   96/  124]
train() client id: f_00002-3-0 loss: 0.930882  [   32/  124]
train() client id: f_00002-3-1 loss: 1.406319  [   64/  124]
train() client id: f_00002-3-2 loss: 1.052817  [   96/  124]
train() client id: f_00002-4-0 loss: 1.082405  [   32/  124]
train() client id: f_00002-4-1 loss: 0.971756  [   64/  124]
train() client id: f_00002-4-2 loss: 1.265508  [   96/  124]
train() client id: f_00002-5-0 loss: 1.101054  [   32/  124]
train() client id: f_00002-5-1 loss: 0.908123  [   64/  124]
train() client id: f_00002-5-2 loss: 1.172308  [   96/  124]
train() client id: f_00002-6-0 loss: 1.031058  [   32/  124]
train() client id: f_00002-6-1 loss: 1.017841  [   64/  124]
train() client id: f_00002-6-2 loss: 0.987036  [   96/  124]
train() client id: f_00002-7-0 loss: 0.998331  [   32/  124]
train() client id: f_00002-7-1 loss: 1.322920  [   64/  124]
train() client id: f_00002-7-2 loss: 0.864207  [   96/  124]
train() client id: f_00002-8-0 loss: 0.926322  [   32/  124]
train() client id: f_00002-8-1 loss: 0.942271  [   64/  124]
train() client id: f_00002-8-2 loss: 1.066521  [   96/  124]
train() client id: f_00002-9-0 loss: 1.013730  [   32/  124]
train() client id: f_00002-9-1 loss: 1.119428  [   64/  124]
train() client id: f_00002-9-2 loss: 0.998819  [   96/  124]
train() client id: f_00002-10-0 loss: 1.049757  [   32/  124]
train() client id: f_00002-10-1 loss: 1.109432  [   64/  124]
train() client id: f_00002-10-2 loss: 1.031903  [   96/  124]
train() client id: f_00003-0-0 loss: 0.828294  [   32/   43]
train() client id: f_00003-1-0 loss: 0.861736  [   32/   43]
train() client id: f_00003-2-0 loss: 0.940752  [   32/   43]
train() client id: f_00003-3-0 loss: 0.431805  [   32/   43]
train() client id: f_00003-4-0 loss: 0.793127  [   32/   43]
train() client id: f_00003-5-0 loss: 0.619506  [   32/   43]
train() client id: f_00003-6-0 loss: 0.583166  [   32/   43]
train() client id: f_00003-7-0 loss: 0.691474  [   32/   43]
train() client id: f_00003-8-0 loss: 0.749936  [   32/   43]
train() client id: f_00003-9-0 loss: 0.433210  [   32/   43]
train() client id: f_00003-10-0 loss: 0.518114  [   32/   43]
train() client id: f_00004-0-0 loss: 0.780903  [   32/  306]
train() client id: f_00004-0-1 loss: 0.841582  [   64/  306]
train() client id: f_00004-0-2 loss: 0.785923  [   96/  306]
train() client id: f_00004-0-3 loss: 0.802754  [  128/  306]
train() client id: f_00004-0-4 loss: 1.017717  [  160/  306]
train() client id: f_00004-0-5 loss: 0.854479  [  192/  306]
train() client id: f_00004-0-6 loss: 0.746013  [  224/  306]
train() client id: f_00004-0-7 loss: 0.713471  [  256/  306]
train() client id: f_00004-0-8 loss: 0.698312  [  288/  306]
train() client id: f_00004-1-0 loss: 0.754200  [   32/  306]
train() client id: f_00004-1-1 loss: 0.849697  [   64/  306]
train() client id: f_00004-1-2 loss: 0.860244  [   96/  306]
train() client id: f_00004-1-3 loss: 0.698424  [  128/  306]
train() client id: f_00004-1-4 loss: 0.794665  [  160/  306]
train() client id: f_00004-1-5 loss: 0.865048  [  192/  306]
train() client id: f_00004-1-6 loss: 0.756132  [  224/  306]
train() client id: f_00004-1-7 loss: 0.825964  [  256/  306]
train() client id: f_00004-1-8 loss: 0.699423  [  288/  306]
train() client id: f_00004-2-0 loss: 0.745953  [   32/  306]
train() client id: f_00004-2-1 loss: 0.760617  [   64/  306]
train() client id: f_00004-2-2 loss: 0.929852  [   96/  306]
train() client id: f_00004-2-3 loss: 0.761899  [  128/  306]
train() client id: f_00004-2-4 loss: 0.656444  [  160/  306]
train() client id: f_00004-2-5 loss: 0.809539  [  192/  306]
train() client id: f_00004-2-6 loss: 0.754525  [  224/  306]
train() client id: f_00004-2-7 loss: 0.898067  [  256/  306]
train() client id: f_00004-2-8 loss: 0.852213  [  288/  306]
train() client id: f_00004-3-0 loss: 0.830834  [   32/  306]
train() client id: f_00004-3-1 loss: 0.771723  [   64/  306]
train() client id: f_00004-3-2 loss: 0.718042  [   96/  306]
train() client id: f_00004-3-3 loss: 0.808122  [  128/  306]
train() client id: f_00004-3-4 loss: 0.719312  [  160/  306]
train() client id: f_00004-3-5 loss: 0.715486  [  192/  306]
train() client id: f_00004-3-6 loss: 0.870703  [  224/  306]
train() client id: f_00004-3-7 loss: 0.915466  [  256/  306]
train() client id: f_00004-3-8 loss: 0.740659  [  288/  306]
train() client id: f_00004-4-0 loss: 0.735420  [   32/  306]
train() client id: f_00004-4-1 loss: 0.805889  [   64/  306]
train() client id: f_00004-4-2 loss: 0.760210  [   96/  306]
train() client id: f_00004-4-3 loss: 0.911776  [  128/  306]
train() client id: f_00004-4-4 loss: 0.706148  [  160/  306]
train() client id: f_00004-4-5 loss: 0.812876  [  192/  306]
train() client id: f_00004-4-6 loss: 0.855748  [  224/  306]
train() client id: f_00004-4-7 loss: 0.788315  [  256/  306]
train() client id: f_00004-4-8 loss: 0.793994  [  288/  306]
train() client id: f_00004-5-0 loss: 0.825762  [   32/  306]
train() client id: f_00004-5-1 loss: 0.928620  [   64/  306]
train() client id: f_00004-5-2 loss: 0.755183  [   96/  306]
train() client id: f_00004-5-3 loss: 0.816467  [  128/  306]
train() client id: f_00004-5-4 loss: 0.772799  [  160/  306]
train() client id: f_00004-5-5 loss: 0.689423  [  192/  306]
train() client id: f_00004-5-6 loss: 0.780961  [  224/  306]
train() client id: f_00004-5-7 loss: 0.765738  [  256/  306]
train() client id: f_00004-5-8 loss: 0.856448  [  288/  306]
train() client id: f_00004-6-0 loss: 0.705469  [   32/  306]
train() client id: f_00004-6-1 loss: 0.747279  [   64/  306]
train() client id: f_00004-6-2 loss: 0.923009  [   96/  306]
train() client id: f_00004-6-3 loss: 0.945562  [  128/  306]
train() client id: f_00004-6-4 loss: 0.757785  [  160/  306]
train() client id: f_00004-6-5 loss: 0.800744  [  192/  306]
train() client id: f_00004-6-6 loss: 0.718098  [  224/  306]
train() client id: f_00004-6-7 loss: 0.740542  [  256/  306]
train() client id: f_00004-6-8 loss: 0.767649  [  288/  306]
train() client id: f_00004-7-0 loss: 0.860628  [   32/  306]
train() client id: f_00004-7-1 loss: 0.921714  [   64/  306]
train() client id: f_00004-7-2 loss: 0.703344  [   96/  306]
train() client id: f_00004-7-3 loss: 0.823860  [  128/  306]
train() client id: f_00004-7-4 loss: 0.788995  [  160/  306]
train() client id: f_00004-7-5 loss: 0.900366  [  192/  306]
train() client id: f_00004-7-6 loss: 0.785572  [  224/  306]
train() client id: f_00004-7-7 loss: 0.673909  [  256/  306]
train() client id: f_00004-7-8 loss: 0.797258  [  288/  306]
train() client id: f_00004-8-0 loss: 0.782778  [   32/  306]
train() client id: f_00004-8-1 loss: 0.860432  [   64/  306]
train() client id: f_00004-8-2 loss: 0.859084  [   96/  306]
train() client id: f_00004-8-3 loss: 0.874401  [  128/  306]
train() client id: f_00004-8-4 loss: 0.776506  [  160/  306]
train() client id: f_00004-8-5 loss: 0.773896  [  192/  306]
train() client id: f_00004-8-6 loss: 0.726823  [  224/  306]
train() client id: f_00004-8-7 loss: 0.956091  [  256/  306]
train() client id: f_00004-8-8 loss: 0.732868  [  288/  306]
train() client id: f_00004-9-0 loss: 0.846372  [   32/  306]
train() client id: f_00004-9-1 loss: 0.891977  [   64/  306]
train() client id: f_00004-9-2 loss: 0.737368  [   96/  306]
train() client id: f_00004-9-3 loss: 0.706080  [  128/  306]
train() client id: f_00004-9-4 loss: 0.772375  [  160/  306]
train() client id: f_00004-9-5 loss: 0.805134  [  192/  306]
train() client id: f_00004-9-6 loss: 0.741517  [  224/  306]
train() client id: f_00004-9-7 loss: 0.884542  [  256/  306]
train() client id: f_00004-9-8 loss: 0.853798  [  288/  306]
train() client id: f_00004-10-0 loss: 0.740638  [   32/  306]
train() client id: f_00004-10-1 loss: 0.908461  [   64/  306]
train() client id: f_00004-10-2 loss: 0.821784  [   96/  306]
train() client id: f_00004-10-3 loss: 0.679369  [  128/  306]
train() client id: f_00004-10-4 loss: 0.805442  [  160/  306]
train() client id: f_00004-10-5 loss: 0.844025  [  192/  306]
train() client id: f_00004-10-6 loss: 0.817677  [  224/  306]
train() client id: f_00004-10-7 loss: 0.809220  [  256/  306]
train() client id: f_00004-10-8 loss: 0.808285  [  288/  306]
train() client id: f_00005-0-0 loss: 0.537509  [   32/  146]
train() client id: f_00005-0-1 loss: 0.349330  [   64/  146]
train() client id: f_00005-0-2 loss: 0.999980  [   96/  146]
train() client id: f_00005-0-3 loss: 0.446805  [  128/  146]
train() client id: f_00005-1-0 loss: 0.506689  [   32/  146]
train() client id: f_00005-1-1 loss: 0.407788  [   64/  146]
train() client id: f_00005-1-2 loss: 0.399434  [   96/  146]
train() client id: f_00005-1-3 loss: 0.776256  [  128/  146]
train() client id: f_00005-2-0 loss: 0.722625  [   32/  146]
train() client id: f_00005-2-1 loss: 0.608958  [   64/  146]
train() client id: f_00005-2-2 loss: 0.588584  [   96/  146]
train() client id: f_00005-2-3 loss: 0.378489  [  128/  146]
train() client id: f_00005-3-0 loss: 0.842397  [   32/  146]
train() client id: f_00005-3-1 loss: 0.411295  [   64/  146]
train() client id: f_00005-3-2 loss: 0.469504  [   96/  146]
train() client id: f_00005-3-3 loss: 0.652104  [  128/  146]
train() client id: f_00005-4-0 loss: 0.712584  [   32/  146]
train() client id: f_00005-4-1 loss: 0.339372  [   64/  146]
train() client id: f_00005-4-2 loss: 0.699244  [   96/  146]
train() client id: f_00005-4-3 loss: 0.561570  [  128/  146]
train() client id: f_00005-5-0 loss: 0.701670  [   32/  146]
train() client id: f_00005-5-1 loss: 0.728438  [   64/  146]
train() client id: f_00005-5-2 loss: 0.335615  [   96/  146]
train() client id: f_00005-5-3 loss: 0.380216  [  128/  146]
train() client id: f_00005-6-0 loss: 0.513524  [   32/  146]
train() client id: f_00005-6-1 loss: 0.533036  [   64/  146]
train() client id: f_00005-6-2 loss: 0.578121  [   96/  146]
train() client id: f_00005-6-3 loss: 0.586068  [  128/  146]
train() client id: f_00005-7-0 loss: 0.542526  [   32/  146]
train() client id: f_00005-7-1 loss: 0.496004  [   64/  146]
train() client id: f_00005-7-2 loss: 0.578204  [   96/  146]
train() client id: f_00005-7-3 loss: 0.579160  [  128/  146]
train() client id: f_00005-8-0 loss: 0.586776  [   32/  146]
train() client id: f_00005-8-1 loss: 0.491247  [   64/  146]
train() client id: f_00005-8-2 loss: 0.601732  [   96/  146]
train() client id: f_00005-8-3 loss: 0.477199  [  128/  146]
train() client id: f_00005-9-0 loss: 0.611440  [   32/  146]
train() client id: f_00005-9-1 loss: 0.434630  [   64/  146]
train() client id: f_00005-9-2 loss: 0.622187  [   96/  146]
train() client id: f_00005-9-3 loss: 0.388463  [  128/  146]
train() client id: f_00005-10-0 loss: 0.471893  [   32/  146]
train() client id: f_00005-10-1 loss: 0.459562  [   64/  146]
train() client id: f_00005-10-2 loss: 0.411451  [   96/  146]
train() client id: f_00005-10-3 loss: 0.663714  [  128/  146]
train() client id: f_00006-0-0 loss: 0.571929  [   32/   54]
train() client id: f_00006-1-0 loss: 0.501020  [   32/   54]
train() client id: f_00006-2-0 loss: 0.520133  [   32/   54]
train() client id: f_00006-3-0 loss: 0.609583  [   32/   54]
train() client id: f_00006-4-0 loss: 0.587027  [   32/   54]
train() client id: f_00006-5-0 loss: 0.562312  [   32/   54]
train() client id: f_00006-6-0 loss: 0.589124  [   32/   54]
train() client id: f_00006-7-0 loss: 0.524519  [   32/   54]
train() client id: f_00006-8-0 loss: 0.512109  [   32/   54]
train() client id: f_00006-9-0 loss: 0.608587  [   32/   54]
train() client id: f_00006-10-0 loss: 0.596609  [   32/   54]
train() client id: f_00007-0-0 loss: 0.679797  [   32/  179]
train() client id: f_00007-0-1 loss: 0.537683  [   64/  179]
train() client id: f_00007-0-2 loss: 0.541971  [   96/  179]
train() client id: f_00007-0-3 loss: 0.732524  [  128/  179]
train() client id: f_00007-0-4 loss: 0.631460  [  160/  179]
train() client id: f_00007-1-0 loss: 0.476473  [   32/  179]
train() client id: f_00007-1-1 loss: 0.696306  [   64/  179]
train() client id: f_00007-1-2 loss: 0.538175  [   96/  179]
train() client id: f_00007-1-3 loss: 0.535722  [  128/  179]
train() client id: f_00007-1-4 loss: 0.636061  [  160/  179]
train() client id: f_00007-2-0 loss: 0.534950  [   32/  179]
train() client id: f_00007-2-1 loss: 0.756890  [   64/  179]
train() client id: f_00007-2-2 loss: 0.519447  [   96/  179]
train() client id: f_00007-2-3 loss: 0.630798  [  128/  179]
train() client id: f_00007-2-4 loss: 0.518611  [  160/  179]
train() client id: f_00007-3-0 loss: 0.532905  [   32/  179]
train() client id: f_00007-3-1 loss: 0.552984  [   64/  179]
train() client id: f_00007-3-2 loss: 0.731319  [   96/  179]
train() client id: f_00007-3-3 loss: 0.522542  [  128/  179]
train() client id: f_00007-3-4 loss: 0.432329  [  160/  179]
train() client id: f_00007-4-0 loss: 0.530004  [   32/  179]
train() client id: f_00007-4-1 loss: 0.687330  [   64/  179]
train() client id: f_00007-4-2 loss: 0.491897  [   96/  179]
train() client id: f_00007-4-3 loss: 0.504003  [  128/  179]
train() client id: f_00007-4-4 loss: 0.660385  [  160/  179]
train() client id: f_00007-5-0 loss: 0.541541  [   32/  179]
train() client id: f_00007-5-1 loss: 0.490324  [   64/  179]
train() client id: f_00007-5-2 loss: 0.772624  [   96/  179]
train() client id: f_00007-5-3 loss: 0.650918  [  128/  179]
train() client id: f_00007-5-4 loss: 0.491823  [  160/  179]
train() client id: f_00007-6-0 loss: 0.411172  [   32/  179]
train() client id: f_00007-6-1 loss: 0.504666  [   64/  179]
train() client id: f_00007-6-2 loss: 0.522794  [   96/  179]
train() client id: f_00007-6-3 loss: 0.710842  [  128/  179]
train() client id: f_00007-6-4 loss: 0.494891  [  160/  179]
train() client id: f_00007-7-0 loss: 0.470987  [   32/  179]
train() client id: f_00007-7-1 loss: 0.586830  [   64/  179]
train() client id: f_00007-7-2 loss: 0.474605  [   96/  179]
train() client id: f_00007-7-3 loss: 0.765885  [  128/  179]
train() client id: f_00007-7-4 loss: 0.497198  [  160/  179]
train() client id: f_00007-8-0 loss: 0.464679  [   32/  179]
train() client id: f_00007-8-1 loss: 0.490120  [   64/  179]
train() client id: f_00007-8-2 loss: 0.501846  [   96/  179]
train() client id: f_00007-8-3 loss: 0.600198  [  128/  179]
train() client id: f_00007-8-4 loss: 0.483515  [  160/  179]
train() client id: f_00007-9-0 loss: 0.549268  [   32/  179]
train() client id: f_00007-9-1 loss: 0.608069  [   64/  179]
train() client id: f_00007-9-2 loss: 0.563448  [   96/  179]
train() client id: f_00007-9-3 loss: 0.524825  [  128/  179]
train() client id: f_00007-9-4 loss: 0.592143  [  160/  179]
train() client id: f_00007-10-0 loss: 0.627715  [   32/  179]
train() client id: f_00007-10-1 loss: 0.400403  [   64/  179]
train() client id: f_00007-10-2 loss: 0.752943  [   96/  179]
train() client id: f_00007-10-3 loss: 0.452139  [  128/  179]
train() client id: f_00007-10-4 loss: 0.710937  [  160/  179]
train() client id: f_00008-0-0 loss: 0.859870  [   32/  130]
train() client id: f_00008-0-1 loss: 0.663242  [   64/  130]
train() client id: f_00008-0-2 loss: 0.723809  [   96/  130]
train() client id: f_00008-0-3 loss: 0.765207  [  128/  130]
train() client id: f_00008-1-0 loss: 0.912056  [   32/  130]
train() client id: f_00008-1-1 loss: 0.714800  [   64/  130]
train() client id: f_00008-1-2 loss: 0.640177  [   96/  130]
train() client id: f_00008-1-3 loss: 0.710930  [  128/  130]
train() client id: f_00008-2-0 loss: 0.781243  [   32/  130]
train() client id: f_00008-2-1 loss: 0.703276  [   64/  130]
train() client id: f_00008-2-2 loss: 0.749601  [   96/  130]
train() client id: f_00008-2-3 loss: 0.775805  [  128/  130]
train() client id: f_00008-3-0 loss: 0.709087  [   32/  130]
train() client id: f_00008-3-1 loss: 0.778795  [   64/  130]
train() client id: f_00008-3-2 loss: 0.723470  [   96/  130]
train() client id: f_00008-3-3 loss: 0.755067  [  128/  130]
train() client id: f_00008-4-0 loss: 0.668144  [   32/  130]
train() client id: f_00008-4-1 loss: 0.709305  [   64/  130]
train() client id: f_00008-4-2 loss: 0.871839  [   96/  130]
train() client id: f_00008-4-3 loss: 0.766826  [  128/  130]
train() client id: f_00008-5-0 loss: 0.722209  [   32/  130]
train() client id: f_00008-5-1 loss: 0.699418  [   64/  130]
train() client id: f_00008-5-2 loss: 0.752722  [   96/  130]
train() client id: f_00008-5-3 loss: 0.814858  [  128/  130]
train() client id: f_00008-6-0 loss: 0.718740  [   32/  130]
train() client id: f_00008-6-1 loss: 0.840463  [   64/  130]
train() client id: f_00008-6-2 loss: 0.756066  [   96/  130]
train() client id: f_00008-6-3 loss: 0.695565  [  128/  130]
train() client id: f_00008-7-0 loss: 0.764026  [   32/  130]
train() client id: f_00008-7-1 loss: 0.801322  [   64/  130]
train() client id: f_00008-7-2 loss: 0.761726  [   96/  130]
train() client id: f_00008-7-3 loss: 0.681035  [  128/  130]
train() client id: f_00008-8-0 loss: 0.741991  [   32/  130]
train() client id: f_00008-8-1 loss: 0.665298  [   64/  130]
train() client id: f_00008-8-2 loss: 0.750871  [   96/  130]
train() client id: f_00008-8-3 loss: 0.833462  [  128/  130]
train() client id: f_00008-9-0 loss: 0.705979  [   32/  130]
train() client id: f_00008-9-1 loss: 0.745100  [   64/  130]
train() client id: f_00008-9-2 loss: 0.803238  [   96/  130]
train() client id: f_00008-9-3 loss: 0.711173  [  128/  130]
train() client id: f_00008-10-0 loss: 0.744065  [   32/  130]
train() client id: f_00008-10-1 loss: 0.682611  [   64/  130]
train() client id: f_00008-10-2 loss: 0.838188  [   96/  130]
train() client id: f_00008-10-3 loss: 0.729180  [  128/  130]
train() client id: f_00009-0-0 loss: 1.122277  [   32/  118]
train() client id: f_00009-0-1 loss: 0.915900  [   64/  118]
train() client id: f_00009-0-2 loss: 1.100104  [   96/  118]
train() client id: f_00009-1-0 loss: 1.076136  [   32/  118]
train() client id: f_00009-1-1 loss: 0.955723  [   64/  118]
train() client id: f_00009-1-2 loss: 0.995571  [   96/  118]
train() client id: f_00009-2-0 loss: 1.063016  [   32/  118]
train() client id: f_00009-2-1 loss: 1.029805  [   64/  118]
train() client id: f_00009-2-2 loss: 0.952178  [   96/  118]
train() client id: f_00009-3-0 loss: 1.013375  [   32/  118]
train() client id: f_00009-3-1 loss: 0.926064  [   64/  118]
train() client id: f_00009-3-2 loss: 0.911520  [   96/  118]
train() client id: f_00009-4-0 loss: 0.922706  [   32/  118]
train() client id: f_00009-4-1 loss: 0.830124  [   64/  118]
train() client id: f_00009-4-2 loss: 0.984164  [   96/  118]
train() client id: f_00009-5-0 loss: 0.926456  [   32/  118]
train() client id: f_00009-5-1 loss: 0.956806  [   64/  118]
train() client id: f_00009-5-2 loss: 0.922442  [   96/  118]
train() client id: f_00009-6-0 loss: 0.878910  [   32/  118]
train() client id: f_00009-6-1 loss: 1.012406  [   64/  118]
train() client id: f_00009-6-2 loss: 0.713949  [   96/  118]
train() client id: f_00009-7-0 loss: 0.949982  [   32/  118]
train() client id: f_00009-7-1 loss: 0.917971  [   64/  118]
train() client id: f_00009-7-2 loss: 0.838286  [   96/  118]
train() client id: f_00009-8-0 loss: 0.855836  [   32/  118]
train() client id: f_00009-8-1 loss: 0.754628  [   64/  118]
train() client id: f_00009-8-2 loss: 1.027055  [   96/  118]
train() client id: f_00009-9-0 loss: 0.797333  [   32/  118]
train() client id: f_00009-9-1 loss: 0.866801  [   64/  118]
train() client id: f_00009-9-2 loss: 0.837125  [   96/  118]
train() client id: f_00009-10-0 loss: 0.832741  [   32/  118]
train() client id: f_00009-10-1 loss: 0.899781  [   64/  118]
train() client id: f_00009-10-2 loss: 0.852638  [   96/  118]
At round 59 accuracy: 0.6445623342175066
At round 59 training accuracy: 0.5922199865861838
At round 59 training loss: 0.815495960830163
update_location
xs = [  -3.9056584     4.20031788  315.00902392   18.81129433    0.97929623
    3.95640986 -277.44319194 -256.32485185  299.66397685 -242.06087855]
ys = [ 307.5879595   290.55583871    1.32061395 -277.45517586  269.35018685
  252.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [323.45881808 307.31146753 330.50329676 295.5253617  287.31599707
 271.90190877 294.92645743 275.14197413 316.3971697  261.93411534]
dists_bs = [216.45517025 213.22735703 519.87822868 492.21794357 199.67641438
 195.1420987  205.04936632 192.2279532  500.09040061 183.67812789]
uav_gains = [1.76622191e-12 2.33041781e-12 1.58304528e-12 2.91576877e-12
 3.43842860e-12 4.73853249e-12 2.95041862e-12 4.42719338e-12
 1.98494725e-12 5.83324094e-12]
bs_gains = [3.19347696e-11 3.33068759e-11 2.74643623e-12 3.20076101e-12
 4.00294047e-11 4.26885463e-11 3.71612640e-11 4.45253937e-11
 3.06166898e-12 5.05746914e-11]
Round 60
-------------------------------
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.0875637  6.22007234 3.04984448 1.12636964 7.17067363 3.45091238
 1.38142813 4.26835796 3.14602737 2.7981042 ]
obj_prev = 35.69935382138897
eta_min = 9.518668131305417e-31	eta_max = 0.9381641580347129
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 8.195255617037894	eta = 0.909090909090909
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 19.17140763760524	eta = 0.38861165126506014
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 13.206898476870052	eta = 0.5641167297661407
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 12.154598615650826	eta = 0.6129558543819038
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 12.090043572188366	eta = 0.6162287451356823
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 12.089768790589424	eta = 0.6162427510544748
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 12.089768785575135	eta = 0.6162427513100645
eta = 0.6162427513100645
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [0.03994795 0.08401752 0.03931386 0.01363303 0.09701646 0.0462889
 0.01712055 0.05675146 0.04121614 0.03741158]
ene_total = [1.20410292 1.8265834  1.21411492 0.59841362 2.07887597 1.06827008
 0.66615118 1.4063269  1.14028819 0.8866416 ]
ti_comp = [1.15185544 1.27946326 1.14047878 1.19334864 1.28255751 1.28358448
 1.19415401 1.21817826 1.19663411 1.28616459]
ti_coms = [0.20614588 0.07853805 0.21752254 0.16465267 0.07544381 0.07441683
 0.16384731 0.13982306 0.16136721 0.07183672]
t_total = [26.99974823 26.99974823 26.99974823 26.99974823 26.99974823 26.99974823
 26.99974823 26.99974823 26.99974823 26.99974823]
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [3.00308521e-06 2.26429994e-05 2.91972948e-06 1.11204574e-07
 3.46946537e-05 3.76237107e-06 2.19944230e-07 7.69820333e-06
 3.05604171e-06 1.97835807e-06]
ene_total = [0.40991842 0.15659935 0.43253582 0.32736424 0.15068698 0.14803017
 0.32576518 0.27814891 0.32089064 0.14286493]
optimize_network iter = 0 obj = 2.6928046428038024
eta = 0.6162427513100645
freqs = [17340697.3279414  32833111.65400368 17235681.79847541  5712090.78528441
 37821484.27558679 18031106.89702021  7168485.47995044 23293576.56497024
 17221697.8440477  14543852.06878811]
eta_min = 0.6162427513100649	eta_max = 0.7177903732762834
af = 0.0012621994794922648	bf = 1.0327722521438556	zeta = 0.0013884194274414914	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [5.91233479e-07 4.45784864e-06 5.74822789e-07 2.18934405e-08
 6.83052241e-06 7.40718154e-07 4.33015993e-08 1.51558655e-06
 6.01659308e-07 3.89489955e-07]
ene_total = [1.73019841 0.65953123 1.82567951 1.3819049  0.63376085 0.62463054
 1.37514743 1.17363935 1.35437921 0.60294663]
ti_comp = [0.79250899 0.92011681 0.78113233 0.8340022  0.92321106 0.92423803
 0.83480756 0.85883181 0.83728766 0.92681814]
ti_coms = [0.20614588 0.07853805 0.21752254 0.16465267 0.07544381 0.07441683
 0.16384731 0.13982306 0.16136721 0.07183672]
t_total = [26.99974823 26.99974823 26.99974823 26.99974823 26.99974823 26.99974823
 26.99974823 26.99974823 26.99974823 26.99974823]
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [2.03829682e-06 1.40674660e-05 1.99976986e-06 7.31535092e-08
 2.15142627e-05 2.33160556e-06 1.44601475e-07 4.97630551e-06
 2.00560251e-06 1.22411200e-06]
ene_total = [0.55739348 0.21271672 0.58815051 0.44515884 0.2045524  0.20125725
 0.44298339 0.37816177 0.43632847 0.19425169]
optimize_network iter = 1 obj = 3.6609545155387906
eta = 0.7177903732762834
freqs = [17262263.50383285 31270462.24446457 17235681.79847541  5597996.92086191
 35987511.86898271 17151433.11384352  7023260.57451295 22629570.05842017
 16857767.37652736 13823527.31845897]
eta_min = 0.7177903732762844	eta_max = 0.7177903732762764
af = 0.001159170611255296	bf = 1.0327722521438556	zeta = 0.0012750876723808255	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [5.85897150e-07 4.04361532e-06 5.74822789e-07 2.10275717e-08
 6.18415585e-06 6.70207126e-07 4.15648943e-08 1.43041150e-06
 5.76499351e-07 3.51864226e-07]
ene_total = [1.73019796 0.65949647 1.82567951 1.38190483 0.6337066  0.62462463
 1.37514729 1.1736322  1.3543771  0.60294347]
ti_comp = [0.79250899 0.92011681 0.78113233 0.8340022  0.92321106 0.92423803
 0.83480756 0.85883181 0.83728766 0.92681814]
ti_coms = [0.20614588 0.07853805 0.21752254 0.16465267 0.07544381 0.07441683
 0.16384731 0.13982306 0.16136721 0.07183672]
t_total = [26.99974823 26.99974823 26.99974823 26.99974823 26.99974823 26.99974823
 26.99974823 26.99974823 26.99974823 26.99974823]
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [2.03829682e-06 1.40674660e-05 1.99976986e-06 7.31535092e-08
 2.15142627e-05 2.33160556e-06 1.44601475e-07 4.97630551e-06
 2.00560251e-06 1.22411200e-06]
ene_total = [0.55739348 0.21271672 0.58815051 0.44515884 0.2045524  0.20125725
 0.44298339 0.37816177 0.43632847 0.19425169]
optimize_network iter = 2 obj = 3.6609545155387
eta = 0.7177903732762764
freqs = [17262263.50383282 31270462.24446465 17235681.79847537  5597996.92086191
 35987511.8689828  17151433.11384356  7023260.57451294 22629570.05842018
 16857767.37652736 13823527.31845901]
Done!
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [1.87731017e-06 1.29564040e-05 1.84182611e-06 6.73757745e-08
 1.98150455e-05 2.14745310e-06 1.33180711e-07 4.58327210e-06
 1.84719809e-06 1.12743045e-06]
ene_total = [0.02061646 0.00786676 0.0217541  0.01646533 0.0075642  0.00744383
 0.01638486 0.01398689 0.01613857 0.0071848 ]
At round 60 energy consumption: 0.135405804121047
At round 60 eta: 0.7177903732762764
At round 60 a_n: 7.287306191126021
At round 60 local rounds: 10.857538900724403
At round 60 global rounds: 27.03611541063686
gradient difference: 0.5013759136199951
train() client id: f_00000-0-0 loss: 0.920411  [   32/  126]
train() client id: f_00000-0-1 loss: 0.756399  [   64/  126]
train() client id: f_00000-0-2 loss: 0.813308  [   96/  126]
train() client id: f_00000-1-0 loss: 0.907459  [   32/  126]
train() client id: f_00000-1-1 loss: 0.770428  [   64/  126]
train() client id: f_00000-1-2 loss: 0.822028  [   96/  126]
train() client id: f_00000-2-0 loss: 0.690273  [   32/  126]
train() client id: f_00000-2-1 loss: 0.962855  [   64/  126]
train() client id: f_00000-2-2 loss: 0.760660  [   96/  126]
train() client id: f_00000-3-0 loss: 0.781095  [   32/  126]
train() client id: f_00000-3-1 loss: 0.812407  [   64/  126]
train() client id: f_00000-3-2 loss: 0.743602  [   96/  126]
train() client id: f_00000-4-0 loss: 0.897664  [   32/  126]
train() client id: f_00000-4-1 loss: 0.684020  [   64/  126]
train() client id: f_00000-4-2 loss: 0.835218  [   96/  126]
train() client id: f_00000-5-0 loss: 0.793973  [   32/  126]
train() client id: f_00000-5-1 loss: 0.782397  [   64/  126]
train() client id: f_00000-5-2 loss: 0.743184  [   96/  126]
train() client id: f_00000-6-0 loss: 0.829154  [   32/  126]
train() client id: f_00000-6-1 loss: 0.851632  [   64/  126]
train() client id: f_00000-6-2 loss: 0.773333  [   96/  126]
train() client id: f_00000-7-0 loss: 0.904095  [   32/  126]
train() client id: f_00000-7-1 loss: 0.740299  [   64/  126]
train() client id: f_00000-7-2 loss: 0.742253  [   96/  126]
train() client id: f_00000-8-0 loss: 0.827639  [   32/  126]
train() client id: f_00000-8-1 loss: 0.796054  [   64/  126]
train() client id: f_00000-8-2 loss: 0.833974  [   96/  126]
train() client id: f_00000-9-0 loss: 0.732322  [   32/  126]
train() client id: f_00000-9-1 loss: 0.825032  [   64/  126]
train() client id: f_00000-9-2 loss: 0.793012  [   96/  126]
train() client id: f_00001-0-0 loss: 0.495201  [   32/  265]
train() client id: f_00001-0-1 loss: 0.541456  [   64/  265]
train() client id: f_00001-0-2 loss: 0.452238  [   96/  265]
train() client id: f_00001-0-3 loss: 0.599034  [  128/  265]
train() client id: f_00001-0-4 loss: 0.558830  [  160/  265]
train() client id: f_00001-0-5 loss: 0.560244  [  192/  265]
train() client id: f_00001-0-6 loss: 0.630378  [  224/  265]
train() client id: f_00001-0-7 loss: 0.457541  [  256/  265]
train() client id: f_00001-1-0 loss: 0.573737  [   32/  265]
train() client id: f_00001-1-1 loss: 0.425244  [   64/  265]
train() client id: f_00001-1-2 loss: 0.542093  [   96/  265]
train() client id: f_00001-1-3 loss: 0.673596  [  128/  265]
train() client id: f_00001-1-4 loss: 0.478193  [  160/  265]
train() client id: f_00001-1-5 loss: 0.471605  [  192/  265]
train() client id: f_00001-1-6 loss: 0.519511  [  224/  265]
train() client id: f_00001-1-7 loss: 0.488517  [  256/  265]
train() client id: f_00001-2-0 loss: 0.584502  [   32/  265]
train() client id: f_00001-2-1 loss: 0.454517  [   64/  265]
train() client id: f_00001-2-2 loss: 0.463639  [   96/  265]
train() client id: f_00001-2-3 loss: 0.644272  [  128/  265]
train() client id: f_00001-2-4 loss: 0.596758  [  160/  265]
train() client id: f_00001-2-5 loss: 0.438111  [  192/  265]
train() client id: f_00001-2-6 loss: 0.469641  [  224/  265]
train() client id: f_00001-2-7 loss: 0.574612  [  256/  265]
train() client id: f_00001-3-0 loss: 0.493485  [   32/  265]
train() client id: f_00001-3-1 loss: 0.497187  [   64/  265]
train() client id: f_00001-3-2 loss: 0.585712  [   96/  265]
train() client id: f_00001-3-3 loss: 0.454668  [  128/  265]
train() client id: f_00001-3-4 loss: 0.548371  [  160/  265]
train() client id: f_00001-3-5 loss: 0.568740  [  192/  265]
train() client id: f_00001-3-6 loss: 0.610005  [  224/  265]
train() client id: f_00001-3-7 loss: 0.434955  [  256/  265]
train() client id: f_00001-4-0 loss: 0.419279  [   32/  265]
train() client id: f_00001-4-1 loss: 0.487753  [   64/  265]
train() client id: f_00001-4-2 loss: 0.411380  [   96/  265]
train() client id: f_00001-4-3 loss: 0.547784  [  128/  265]
train() client id: f_00001-4-4 loss: 0.600489  [  160/  265]
train() client id: f_00001-4-5 loss: 0.482913  [  192/  265]
train() client id: f_00001-4-6 loss: 0.595500  [  224/  265]
train() client id: f_00001-4-7 loss: 0.632708  [  256/  265]
train() client id: f_00001-5-0 loss: 0.429980  [   32/  265]
train() client id: f_00001-5-1 loss: 0.595725  [   64/  265]
train() client id: f_00001-5-2 loss: 0.454588  [   96/  265]
train() client id: f_00001-5-3 loss: 0.486079  [  128/  265]
train() client id: f_00001-5-4 loss: 0.689041  [  160/  265]
train() client id: f_00001-5-5 loss: 0.563829  [  192/  265]
train() client id: f_00001-5-6 loss: 0.439890  [  224/  265]
train() client id: f_00001-5-7 loss: 0.499012  [  256/  265]
train() client id: f_00001-6-0 loss: 0.532731  [   32/  265]
train() client id: f_00001-6-1 loss: 0.642540  [   64/  265]
train() client id: f_00001-6-2 loss: 0.436723  [   96/  265]
train() client id: f_00001-6-3 loss: 0.514872  [  128/  265]
train() client id: f_00001-6-4 loss: 0.412984  [  160/  265]
train() client id: f_00001-6-5 loss: 0.542794  [  192/  265]
train() client id: f_00001-6-6 loss: 0.515860  [  224/  265]
train() client id: f_00001-6-7 loss: 0.575191  [  256/  265]
train() client id: f_00001-7-0 loss: 0.473939  [   32/  265]
train() client id: f_00001-7-1 loss: 0.605194  [   64/  265]
train() client id: f_00001-7-2 loss: 0.537759  [   96/  265]
train() client id: f_00001-7-3 loss: 0.433105  [  128/  265]
train() client id: f_00001-7-4 loss: 0.554981  [  160/  265]
train() client id: f_00001-7-5 loss: 0.653921  [  192/  265]
train() client id: f_00001-7-6 loss: 0.458903  [  224/  265]
train() client id: f_00001-7-7 loss: 0.438110  [  256/  265]
train() client id: f_00001-8-0 loss: 0.500709  [   32/  265]
train() client id: f_00001-8-1 loss: 0.422316  [   64/  265]
train() client id: f_00001-8-2 loss: 0.632725  [   96/  265]
train() client id: f_00001-8-3 loss: 0.502921  [  128/  265]
train() client id: f_00001-8-4 loss: 0.488855  [  160/  265]
train() client id: f_00001-8-5 loss: 0.425577  [  192/  265]
train() client id: f_00001-8-6 loss: 0.552549  [  224/  265]
train() client id: f_00001-8-7 loss: 0.558963  [  256/  265]
train() client id: f_00001-9-0 loss: 0.578996  [   32/  265]
train() client id: f_00001-9-1 loss: 0.568712  [   64/  265]
train() client id: f_00001-9-2 loss: 0.420643  [   96/  265]
train() client id: f_00001-9-3 loss: 0.465463  [  128/  265]
train() client id: f_00001-9-4 loss: 0.621066  [  160/  265]
train() client id: f_00001-9-5 loss: 0.440306  [  192/  265]
train() client id: f_00001-9-6 loss: 0.516410  [  224/  265]
train() client id: f_00001-9-7 loss: 0.554311  [  256/  265]
train() client id: f_00002-0-0 loss: 0.911155  [   32/  124]
train() client id: f_00002-0-1 loss: 1.190617  [   64/  124]
train() client id: f_00002-0-2 loss: 1.063817  [   96/  124]
train() client id: f_00002-1-0 loss: 0.979811  [   32/  124]
train() client id: f_00002-1-1 loss: 1.177405  [   64/  124]
train() client id: f_00002-1-2 loss: 1.104378  [   96/  124]
train() client id: f_00002-2-0 loss: 1.133711  [   32/  124]
train() client id: f_00002-2-1 loss: 0.938865  [   64/  124]
train() client id: f_00002-2-2 loss: 1.064279  [   96/  124]
train() client id: f_00002-3-0 loss: 0.927537  [   32/  124]
train() client id: f_00002-3-1 loss: 1.007932  [   64/  124]
train() client id: f_00002-3-2 loss: 1.207396  [   96/  124]
train() client id: f_00002-4-0 loss: 0.880569  [   32/  124]
train() client id: f_00002-4-1 loss: 1.082222  [   64/  124]
train() client id: f_00002-4-2 loss: 1.072220  [   96/  124]
train() client id: f_00002-5-0 loss: 0.932230  [   32/  124]
train() client id: f_00002-5-1 loss: 1.094809  [   64/  124]
train() client id: f_00002-5-2 loss: 0.985207  [   96/  124]
train() client id: f_00002-6-0 loss: 0.933048  [   32/  124]
train() client id: f_00002-6-1 loss: 0.949602  [   64/  124]
train() client id: f_00002-6-2 loss: 1.069282  [   96/  124]
train() client id: f_00002-7-0 loss: 1.129443  [   32/  124]
train() client id: f_00002-7-1 loss: 0.973623  [   64/  124]
train() client id: f_00002-7-2 loss: 0.807057  [   96/  124]
train() client id: f_00002-8-0 loss: 0.925459  [   32/  124]
train() client id: f_00002-8-1 loss: 0.756304  [   64/  124]
train() client id: f_00002-8-2 loss: 1.095828  [   96/  124]
train() client id: f_00002-9-0 loss: 1.010789  [   32/  124]
train() client id: f_00002-9-1 loss: 1.143486  [   64/  124]
train() client id: f_00002-9-2 loss: 0.785637  [   96/  124]
train() client id: f_00003-0-0 loss: 0.844084  [   32/   43]
train() client id: f_00003-1-0 loss: 0.868111  [   32/   43]
train() client id: f_00003-2-0 loss: 1.001554  [   32/   43]
train() client id: f_00003-3-0 loss: 1.078295  [   32/   43]
train() client id: f_00003-4-0 loss: 0.838945  [   32/   43]
train() client id: f_00003-5-0 loss: 0.770602  [   32/   43]
train() client id: f_00003-6-0 loss: 1.031684  [   32/   43]
train() client id: f_00003-7-0 loss: 0.865444  [   32/   43]
train() client id: f_00003-8-0 loss: 0.721154  [   32/   43]
train() client id: f_00003-9-0 loss: 0.724899  [   32/   43]
train() client id: f_00004-0-0 loss: 0.791956  [   32/  306]
train() client id: f_00004-0-1 loss: 0.919847  [   64/  306]
train() client id: f_00004-0-2 loss: 0.746041  [   96/  306]
train() client id: f_00004-0-3 loss: 0.757514  [  128/  306]
train() client id: f_00004-0-4 loss: 0.765848  [  160/  306]
train() client id: f_00004-0-5 loss: 0.846153  [  192/  306]
train() client id: f_00004-0-6 loss: 0.829248  [  224/  306]
train() client id: f_00004-0-7 loss: 0.754793  [  256/  306]
train() client id: f_00004-0-8 loss: 0.706936  [  288/  306]
train() client id: f_00004-1-0 loss: 0.699763  [   32/  306]
train() client id: f_00004-1-1 loss: 0.847037  [   64/  306]
train() client id: f_00004-1-2 loss: 0.854432  [   96/  306]
train() client id: f_00004-1-3 loss: 0.813426  [  128/  306]
train() client id: f_00004-1-4 loss: 0.793472  [  160/  306]
train() client id: f_00004-1-5 loss: 0.853668  [  192/  306]
train() client id: f_00004-1-6 loss: 0.713287  [  224/  306]
train() client id: f_00004-1-7 loss: 0.833726  [  256/  306]
train() client id: f_00004-1-8 loss: 0.707032  [  288/  306]
train() client id: f_00004-2-0 loss: 0.762185  [   32/  306]
train() client id: f_00004-2-1 loss: 0.668345  [   64/  306]
train() client id: f_00004-2-2 loss: 0.851266  [   96/  306]
train() client id: f_00004-2-3 loss: 0.915482  [  128/  306]
train() client id: f_00004-2-4 loss: 0.811003  [  160/  306]
train() client id: f_00004-2-5 loss: 0.611574  [  192/  306]
train() client id: f_00004-2-6 loss: 0.901239  [  224/  306]
train() client id: f_00004-2-7 loss: 0.709930  [  256/  306]
train() client id: f_00004-2-8 loss: 0.837738  [  288/  306]
train() client id: f_00004-3-0 loss: 0.768441  [   32/  306]
train() client id: f_00004-3-1 loss: 0.687299  [   64/  306]
train() client id: f_00004-3-2 loss: 0.899187  [   96/  306]
train() client id: f_00004-3-3 loss: 0.679414  [  128/  306]
train() client id: f_00004-3-4 loss: 0.951432  [  160/  306]
train() client id: f_00004-3-5 loss: 0.650790  [  192/  306]
train() client id: f_00004-3-6 loss: 0.732532  [  224/  306]
train() client id: f_00004-3-7 loss: 0.871942  [  256/  306]
train() client id: f_00004-3-8 loss: 0.810620  [  288/  306]
train() client id: f_00004-4-0 loss: 0.888185  [   32/  306]
train() client id: f_00004-4-1 loss: 0.777789  [   64/  306]
train() client id: f_00004-4-2 loss: 0.658865  [   96/  306]
train() client id: f_00004-4-3 loss: 0.687939  [  128/  306]
train() client id: f_00004-4-4 loss: 0.750569  [  160/  306]
train() client id: f_00004-4-5 loss: 0.763704  [  192/  306]
train() client id: f_00004-4-6 loss: 0.870354  [  224/  306]
train() client id: f_00004-4-7 loss: 0.862656  [  256/  306]
train() client id: f_00004-4-8 loss: 0.891022  [  288/  306]
train() client id: f_00004-5-0 loss: 0.701868  [   32/  306]
train() client id: f_00004-5-1 loss: 0.821061  [   64/  306]
train() client id: f_00004-5-2 loss: 0.807178  [   96/  306]
train() client id: f_00004-5-3 loss: 0.813837  [  128/  306]
train() client id: f_00004-5-4 loss: 0.698791  [  160/  306]
train() client id: f_00004-5-5 loss: 0.772810  [  192/  306]
train() client id: f_00004-5-6 loss: 0.843327  [  224/  306]
train() client id: f_00004-5-7 loss: 0.811783  [  256/  306]
train() client id: f_00004-5-8 loss: 0.898542  [  288/  306]
train() client id: f_00004-6-0 loss: 0.866578  [   32/  306]
train() client id: f_00004-6-1 loss: 0.821586  [   64/  306]
train() client id: f_00004-6-2 loss: 0.758060  [   96/  306]
train() client id: f_00004-6-3 loss: 0.712795  [  128/  306]
train() client id: f_00004-6-4 loss: 0.730340  [  160/  306]
train() client id: f_00004-6-5 loss: 0.828627  [  192/  306]
train() client id: f_00004-6-6 loss: 0.744740  [  224/  306]
train() client id: f_00004-6-7 loss: 0.913190  [  256/  306]
train() client id: f_00004-6-8 loss: 0.755345  [  288/  306]
train() client id: f_00004-7-0 loss: 0.746582  [   32/  306]
train() client id: f_00004-7-1 loss: 0.704022  [   64/  306]
train() client id: f_00004-7-2 loss: 0.803692  [   96/  306]
train() client id: f_00004-7-3 loss: 0.791392  [  128/  306]
train() client id: f_00004-7-4 loss: 0.800455  [  160/  306]
train() client id: f_00004-7-5 loss: 0.809664  [  192/  306]
train() client id: f_00004-7-6 loss: 0.797520  [  224/  306]
train() client id: f_00004-7-7 loss: 0.874049  [  256/  306]
train() client id: f_00004-7-8 loss: 0.789279  [  288/  306]
train() client id: f_00004-8-0 loss: 0.680335  [   32/  306]
train() client id: f_00004-8-1 loss: 0.807908  [   64/  306]
train() client id: f_00004-8-2 loss: 0.810021  [   96/  306]
train() client id: f_00004-8-3 loss: 0.838829  [  128/  306]
train() client id: f_00004-8-4 loss: 0.688579  [  160/  306]
train() client id: f_00004-8-5 loss: 0.812602  [  192/  306]
train() client id: f_00004-8-6 loss: 0.730237  [  224/  306]
train() client id: f_00004-8-7 loss: 0.694281  [  256/  306]
train() client id: f_00004-8-8 loss: 0.961275  [  288/  306]
train() client id: f_00004-9-0 loss: 0.658268  [   32/  306]
train() client id: f_00004-9-1 loss: 0.651806  [   64/  306]
train() client id: f_00004-9-2 loss: 0.978900  [   96/  306]
train() client id: f_00004-9-3 loss: 0.816183  [  128/  306]
train() client id: f_00004-9-4 loss: 0.830208  [  160/  306]
train() client id: f_00004-9-5 loss: 0.825763  [  192/  306]
train() client id: f_00004-9-6 loss: 0.799061  [  224/  306]
train() client id: f_00004-9-7 loss: 0.859845  [  256/  306]
train() client id: f_00004-9-8 loss: 0.752002  [  288/  306]
train() client id: f_00005-0-0 loss: 0.301252  [   32/  146]
train() client id: f_00005-0-1 loss: 0.613556  [   64/  146]
train() client id: f_00005-0-2 loss: 0.529408  [   96/  146]
train() client id: f_00005-0-3 loss: 0.625862  [  128/  146]
train() client id: f_00005-1-0 loss: 0.463949  [   32/  146]
train() client id: f_00005-1-1 loss: 0.385047  [   64/  146]
train() client id: f_00005-1-2 loss: 0.488045  [   96/  146]
train() client id: f_00005-1-3 loss: 0.723621  [  128/  146]
train() client id: f_00005-2-0 loss: 0.271207  [   32/  146]
train() client id: f_00005-2-1 loss: 0.691385  [   64/  146]
train() client id: f_00005-2-2 loss: 0.409134  [   96/  146]
train() client id: f_00005-2-3 loss: 0.478594  [  128/  146]
train() client id: f_00005-3-0 loss: 0.568359  [   32/  146]
train() client id: f_00005-3-1 loss: 0.574581  [   64/  146]
train() client id: f_00005-3-2 loss: 0.461399  [   96/  146]
train() client id: f_00005-3-3 loss: 0.404879  [  128/  146]
train() client id: f_00005-4-0 loss: 0.320711  [   32/  146]
train() client id: f_00005-4-1 loss: 0.599170  [   64/  146]
train() client id: f_00005-4-2 loss: 0.377396  [   96/  146]
train() client id: f_00005-4-3 loss: 0.554072  [  128/  146]
train() client id: f_00005-5-0 loss: 0.377156  [   32/  146]
train() client id: f_00005-5-1 loss: 0.452191  [   64/  146]
train() client id: f_00005-5-2 loss: 0.323680  [   96/  146]
train() client id: f_00005-5-3 loss: 0.777494  [  128/  146]
train() client id: f_00005-6-0 loss: 0.330705  [   32/  146]
train() client id: f_00005-6-1 loss: 0.463822  [   64/  146]
train() client id: f_00005-6-2 loss: 0.590644  [   96/  146]
train() client id: f_00005-6-3 loss: 0.381336  [  128/  146]
train() client id: f_00005-7-0 loss: 0.566651  [   32/  146]
train() client id: f_00005-7-1 loss: 0.531538  [   64/  146]
train() client id: f_00005-7-2 loss: 0.708311  [   96/  146]
train() client id: f_00005-7-3 loss: 0.248935  [  128/  146]
train() client id: f_00005-8-0 loss: 0.292299  [   32/  146]
train() client id: f_00005-8-1 loss: 0.411579  [   64/  146]
train() client id: f_00005-8-2 loss: 0.757756  [   96/  146]
train() client id: f_00005-8-3 loss: 0.396531  [  128/  146]
train() client id: f_00005-9-0 loss: 0.484570  [   32/  146]
train() client id: f_00005-9-1 loss: 0.580881  [   64/  146]
train() client id: f_00005-9-2 loss: 0.363651  [   96/  146]
train() client id: f_00005-9-3 loss: 0.594087  [  128/  146]
train() client id: f_00006-0-0 loss: 0.505123  [   32/   54]
train() client id: f_00006-1-0 loss: 0.480830  [   32/   54]
train() client id: f_00006-2-0 loss: 0.512259  [   32/   54]
train() client id: f_00006-3-0 loss: 0.418737  [   32/   54]
train() client id: f_00006-4-0 loss: 0.425772  [   32/   54]
train() client id: f_00006-5-0 loss: 0.458126  [   32/   54]
train() client id: f_00006-6-0 loss: 0.502959  [   32/   54]
train() client id: f_00006-7-0 loss: 0.382506  [   32/   54]
train() client id: f_00006-8-0 loss: 0.399964  [   32/   54]
train() client id: f_00006-9-0 loss: 0.459388  [   32/   54]
train() client id: f_00007-0-0 loss: 0.636238  [   32/  179]
train() client id: f_00007-0-1 loss: 0.358415  [   64/  179]
train() client id: f_00007-0-2 loss: 0.565310  [   96/  179]
train() client id: f_00007-0-3 loss: 0.379781  [  128/  179]
train() client id: f_00007-0-4 loss: 0.474082  [  160/  179]
train() client id: f_00007-1-0 loss: 0.533118  [   32/  179]
train() client id: f_00007-1-1 loss: 0.464482  [   64/  179]
train() client id: f_00007-1-2 loss: 0.485521  [   96/  179]
train() client id: f_00007-1-3 loss: 0.556299  [  128/  179]
train() client id: f_00007-1-4 loss: 0.366014  [  160/  179]
train() client id: f_00007-2-0 loss: 0.292419  [   32/  179]
train() client id: f_00007-2-1 loss: 0.275756  [   64/  179]
train() client id: f_00007-2-2 loss: 0.633070  [   96/  179]
train() client id: f_00007-2-3 loss: 0.551434  [  128/  179]
train() client id: f_00007-2-4 loss: 0.445644  [  160/  179]
train() client id: f_00007-3-0 loss: 0.441926  [   32/  179]
train() client id: f_00007-3-1 loss: 0.479169  [   64/  179]
train() client id: f_00007-3-2 loss: 0.562753  [   96/  179]
train() client id: f_00007-3-3 loss: 0.616533  [  128/  179]
train() client id: f_00007-3-4 loss: 0.287756  [  160/  179]
train() client id: f_00007-4-0 loss: 0.358926  [   32/  179]
train() client id: f_00007-4-1 loss: 0.416427  [   64/  179]
train() client id: f_00007-4-2 loss: 0.508282  [   96/  179]
train() client id: f_00007-4-3 loss: 0.485934  [  128/  179]
train() client id: f_00007-4-4 loss: 0.332673  [  160/  179]
train() client id: f_00007-5-0 loss: 0.348279  [   32/  179]
train() client id: f_00007-5-1 loss: 0.339978  [   64/  179]
train() client id: f_00007-5-2 loss: 0.446180  [   96/  179]
train() client id: f_00007-5-3 loss: 0.519259  [  128/  179]
train() client id: f_00007-5-4 loss: 0.472307  [  160/  179]
train() client id: f_00007-6-0 loss: 0.380611  [   32/  179]
train() client id: f_00007-6-1 loss: 0.289108  [   64/  179]
train() client id: f_00007-6-2 loss: 0.534146  [   96/  179]
train() client id: f_00007-6-3 loss: 0.376846  [  128/  179]
train() client id: f_00007-6-4 loss: 0.411171  [  160/  179]
train() client id: f_00007-7-0 loss: 0.300885  [   32/  179]
train() client id: f_00007-7-1 loss: 0.626584  [   64/  179]
train() client id: f_00007-7-2 loss: 0.335824  [   96/  179]
train() client id: f_00007-7-3 loss: 0.516717  [  128/  179]
train() client id: f_00007-7-4 loss: 0.379779  [  160/  179]
train() client id: f_00007-8-0 loss: 0.549458  [   32/  179]
train() client id: f_00007-8-1 loss: 0.462350  [   64/  179]
train() client id: f_00007-8-2 loss: 0.365148  [   96/  179]
train() client id: f_00007-8-3 loss: 0.302774  [  128/  179]
train() client id: f_00007-8-4 loss: 0.486195  [  160/  179]
train() client id: f_00007-9-0 loss: 0.399745  [   32/  179]
train() client id: f_00007-9-1 loss: 0.350281  [   64/  179]
train() client id: f_00007-9-2 loss: 0.540495  [   96/  179]
train() client id: f_00007-9-3 loss: 0.368315  [  128/  179]
train() client id: f_00007-9-4 loss: 0.633078  [  160/  179]
train() client id: f_00008-0-0 loss: 0.783875  [   32/  130]
train() client id: f_00008-0-1 loss: 0.746011  [   64/  130]
train() client id: f_00008-0-2 loss: 0.768589  [   96/  130]
train() client id: f_00008-0-3 loss: 0.654047  [  128/  130]
train() client id: f_00008-1-0 loss: 0.809991  [   32/  130]
train() client id: f_00008-1-1 loss: 0.729635  [   64/  130]
train() client id: f_00008-1-2 loss: 0.686601  [   96/  130]
train() client id: f_00008-1-3 loss: 0.731502  [  128/  130]
train() client id: f_00008-2-0 loss: 0.725731  [   32/  130]
train() client id: f_00008-2-1 loss: 0.807900  [   64/  130]
train() client id: f_00008-2-2 loss: 0.746637  [   96/  130]
train() client id: f_00008-2-3 loss: 0.664016  [  128/  130]
train() client id: f_00008-3-0 loss: 0.891832  [   32/  130]
train() client id: f_00008-3-1 loss: 0.691451  [   64/  130]
train() client id: f_00008-3-2 loss: 0.633003  [   96/  130]
train() client id: f_00008-3-3 loss: 0.716801  [  128/  130]
train() client id: f_00008-4-0 loss: 0.669633  [   32/  130]
train() client id: f_00008-4-1 loss: 0.810467  [   64/  130]
train() client id: f_00008-4-2 loss: 0.736156  [   96/  130]
train() client id: f_00008-4-3 loss: 0.700693  [  128/  130]
train() client id: f_00008-5-0 loss: 0.798635  [   32/  130]
train() client id: f_00008-5-1 loss: 0.752426  [   64/  130]
train() client id: f_00008-5-2 loss: 0.715447  [   96/  130]
train() client id: f_00008-5-3 loss: 0.686250  [  128/  130]
train() client id: f_00008-6-0 loss: 0.770044  [   32/  130]
train() client id: f_00008-6-1 loss: 0.725793  [   64/  130]
train() client id: f_00008-6-2 loss: 0.758913  [   96/  130]
train() client id: f_00008-6-3 loss: 0.657196  [  128/  130]
train() client id: f_00008-7-0 loss: 0.650458  [   32/  130]
train() client id: f_00008-7-1 loss: 0.739350  [   64/  130]
train() client id: f_00008-7-2 loss: 0.773129  [   96/  130]
train() client id: f_00008-7-3 loss: 0.752095  [  128/  130]
train() client id: f_00008-8-0 loss: 0.764084  [   32/  130]
train() client id: f_00008-8-1 loss: 0.862162  [   64/  130]
train() client id: f_00008-8-2 loss: 0.669452  [   96/  130]
train() client id: f_00008-8-3 loss: 0.641225  [  128/  130]
train() client id: f_00008-9-0 loss: 0.761207  [   32/  130]
train() client id: f_00008-9-1 loss: 0.737140  [   64/  130]
train() client id: f_00008-9-2 loss: 0.725684  [   96/  130]
train() client id: f_00008-9-3 loss: 0.679041  [  128/  130]
train() client id: f_00009-0-0 loss: 1.119319  [   32/  118]
train() client id: f_00009-0-1 loss: 1.078348  [   64/  118]
train() client id: f_00009-0-2 loss: 1.227316  [   96/  118]
train() client id: f_00009-1-0 loss: 1.125877  [   32/  118]
train() client id: f_00009-1-1 loss: 1.036160  [   64/  118]
train() client id: f_00009-1-2 loss: 1.048755  [   96/  118]
train() client id: f_00009-2-0 loss: 1.079836  [   32/  118]
train() client id: f_00009-2-1 loss: 1.012848  [   64/  118]
train() client id: f_00009-2-2 loss: 1.221636  [   96/  118]
train() client id: f_00009-3-0 loss: 1.041720  [   32/  118]
train() client id: f_00009-3-1 loss: 1.036683  [   64/  118]
train() client id: f_00009-3-2 loss: 1.049336  [   96/  118]
train() client id: f_00009-4-0 loss: 0.873452  [   32/  118]
train() client id: f_00009-4-1 loss: 0.918792  [   64/  118]
train() client id: f_00009-4-2 loss: 1.071933  [   96/  118]
train() client id: f_00009-5-0 loss: 0.921970  [   32/  118]
train() client id: f_00009-5-1 loss: 0.964905  [   64/  118]
train() client id: f_00009-5-2 loss: 0.949838  [   96/  118]
train() client id: f_00009-6-0 loss: 1.023703  [   32/  118]
train() client id: f_00009-6-1 loss: 0.792665  [   64/  118]
train() client id: f_00009-6-2 loss: 1.030823  [   96/  118]
train() client id: f_00009-7-0 loss: 0.908617  [   32/  118]
train() client id: f_00009-7-1 loss: 0.799731  [   64/  118]
train() client id: f_00009-7-2 loss: 1.027981  [   96/  118]
train() client id: f_00009-8-0 loss: 0.792400  [   32/  118]
train() client id: f_00009-8-1 loss: 0.830353  [   64/  118]
train() client id: f_00009-8-2 loss: 0.954433  [   96/  118]
train() client id: f_00009-9-0 loss: 0.995546  [   32/  118]
train() client id: f_00009-9-1 loss: 0.801941  [   64/  118]
train() client id: f_00009-9-2 loss: 0.920459  [   96/  118]
At round 60 accuracy: 0.6445623342175066
At round 60 training accuracy: 0.5915492957746479
At round 60 training loss: 0.8159767022674953
update_location
xs = [  -3.9056584     4.20031788  320.00902392   18.81129433    0.97929623
    3.95640986 -282.44319194 -261.32485185  304.66397685 -247.06087855]
ys = [ 312.5879595   295.55583871    1.32061395 -282.45517586  274.35018685
  257.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [328.2171333  312.04310033 335.27230636 300.22456789 292.0085342
 276.55702758 299.63485647 279.80592283 321.13674465 266.56160558]
dists_bs = [219.55345664 215.9779252  524.5936169  496.81793016 202.08704146
 197.18919898 207.59256862 194.39607637 504.83863615 185.5620205 ]
uav_gains = [1.63906976e-12 2.14056491e-12 1.47563907e-12 2.66124979e-12
 3.12682385e-12 4.29784027e-12 2.69155883e-12 4.01567151e-12
 1.83392238e-12 5.29895911e-12]
bs_gains = [3.06888987e-11 3.21327477e-11 2.67787117e-12 3.11847132e-12
 3.87067209e-11 4.14592405e-11 3.59005447e-11 4.31488375e-11
 2.98171993e-12 4.91501249e-11]
Round 61
-------------------------------
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.95458156 5.94128521 2.91869021 1.0805571  6.84914591 3.29632295
 1.32415061 4.08054691 3.00608516 2.67279298]
obj_prev = 34.12415860899734
eta_min = 3.98916673942854e-32	eta_max = 0.9387143479457988
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 7.8273257066737205	eta = 0.9090909090909091
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 18.61065565296208	eta = 0.3823481974584871
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 12.717399503431304	eta = 0.5595287496088128
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 11.681826444107466	eta = 0.60912997436458
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 11.617979436161402	eta = 0.6124774692131586
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 11.617704178505617	eta = 0.6124919806097139
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 11.617704173351733	eta = 0.6124919808814296
eta = 0.6124919808814296
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [0.04045173 0.08507706 0.03980964 0.01380496 0.09823992 0.04687264
 0.01733646 0.05746714 0.04173591 0.03788337]
ene_total = [1.16278399 1.74880761 1.17244705 0.58140006 1.9903631  1.02228243
 0.6462616  1.35340359 1.09164915 0.84830559]
ti_comp = [1.21927116 1.35390191 1.20771185 1.26195871 1.35708161 1.35819303
 1.2627831  1.28806187 1.26993947 1.36081379]
ti_coms = [0.21380186 0.07917111 0.22536118 0.17111432 0.07599142 0.07487999
 0.17028993 0.14501116 0.16313355 0.07225923]
t_total = [26.94974403 26.94974403 26.94974403 26.94974403 26.94974403 26.94974403
 26.94974403 26.94974403 26.94974403 26.94974403]
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [2.78285633e-06 2.09963099e-05 2.70344885e-06 1.03251135e-07
 3.21760040e-05 3.48911535e-06 2.04222466e-07 7.14933098e-06
 2.81736924e-06 1.83496748e-06]
ene_total = [0.40211875 0.14928058 0.4238552  0.32179204 0.14351122 0.14088165
 0.32024363 0.27283609 0.3068348  0.13592205]
optimize_network iter = 0 obj = 2.617276020137961
eta = 0.6124919808814296
freqs = [16588488.88317586 31419209.64704492 16481431.96089219  5469654.50966602
 36195288.80600585 17255515.63488949  6864384.21294392 22307602.23984509
 16432245.4685148  13919379.90342778]
eta_min = 0.6124919808814304	eta_max = 0.7228965206792303
af = 0.0011038579011319113	bf = 1.0136743134601105	zeta = 0.0012142436912451024	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [5.41052682e-07 4.08217617e-06 5.25614002e-07 2.00744476e-08
 6.25577148e-06 6.78366036e-07 3.97056477e-08 1.38999799e-06
 5.47762803e-07 3.56760810e-07]
ene_total = [1.71388696 0.63496578 1.80654562 1.37166028 0.60965148 0.60029516
 1.36505352 1.16252622 1.30772847 0.57926126]
ti_comp = [0.8109757  0.94560644 0.79941638 0.85366324 0.94878614 0.94989756
 0.85448763 0.8797664  0.861644   0.95251833]
ti_coms = [0.21380186 0.07917111 0.22536118 0.17111432 0.07599142 0.07487999
 0.17028993 0.14501116 0.16313355 0.07225923]
t_total = [26.94974403 26.94974403 26.94974403 26.94974403 26.94974403 26.94974403
 26.94974403 26.94974403 26.94974403 26.94974403]
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [1.82433614e-06 1.24831928e-05 1.78948422e-06 6.54396715e-08
 1.90913448e-05 2.06877816e-06 1.29353631e-07 4.44460588e-06
 1.77493813e-06 1.08619612e-06]
ene_total = [0.56230711 0.20853359 0.59270505 0.45000056 0.20034537 0.19697486
 0.44783425 0.38146927 0.42905758 0.19005691]
optimize_network iter = 1 obj = 3.659284534384905
eta = 0.7228965206792303
freqs = [16508552.89543737 29777055.50547944 16481431.96089219  5352148.75441332
 34268839.33358462 16331361.10328626  6714818.53144301 21618818.75231183
 16031043.09450184 13163002.59895364]
eta_min = 0.7228965206792295	eta_max = 0.7228965206792275
af = 0.0010051578008740583	bf = 1.0136743134601105	zeta = 0.0011056735809614642	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [5.35850837e-07 3.66661011e-06 5.25614002e-07 1.92211852e-08
 5.60758124e-06 6.07649264e-07 3.79942326e-08 1.30548627e-06
 5.21341470e-07 3.19041588e-07]
ene_total = [1.71388654 0.63493247 1.80654562 1.37166021 0.60959952 0.60028949
 1.36505338 1.16251944 1.30772635 0.57925823]
ti_comp = [0.8109757  0.94560644 0.79941638 0.85366324 0.94878614 0.94989756
 0.85448763 0.8797664  0.861644   0.95251833]
ti_coms = [0.21380186 0.07917111 0.22536118 0.17111432 0.07599142 0.07487999
 0.17028993 0.14501116 0.16313355 0.07225923]
t_total = [26.94974403 26.94974403 26.94974403 26.94974403 26.94974403 26.94974403
 26.94974403 26.94974403 26.94974403 26.94974403]
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [1.82433614e-06 1.24831928e-05 1.78948422e-06 6.54396715e-08
 1.90913448e-05 2.06877816e-06 1.29353631e-07 4.44460588e-06
 1.77493813e-06 1.08619612e-06]
ene_total = [0.56230711 0.20853359 0.59270505 0.45000056 0.20034537 0.19697486
 0.44783425 0.38146927 0.42905758 0.19005691]
optimize_network iter = 2 obj = 3.6592845343848683
eta = 0.7228965206792275
freqs = [16508552.89543735 29777055.50547947 16481431.96089218  5352148.75441332
 34268839.33358465 16331361.10328627  6714818.531443   21618818.75231183
 16031043.09450183 13163002.59895365]
Done!
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [1.71695361e-06 1.17484177e-05 1.68415312e-06 6.15878170e-08
 1.79676062e-05 1.94700750e-06 1.21739727e-07 4.18299125e-06
 1.67046323e-06 1.02226136e-06]
ene_total = [0.0213819  0.00792886 0.0225378  0.01711149 0.00761711 0.00748995
 0.01702911 0.0145053  0.01631503 0.00722695]
At round 61 energy consumption: 0.13914349848735766
At round 61 eta: 0.7228965206792275
At round 61 a_n: 6.944760344156705
At round 61 local rounds: 10.625424748609518
At round 61 global rounds: 26.298140351714242
gradient difference: 0.48440104722976685
train() client id: f_00000-0-0 loss: 1.197399  [   32/  126]
train() client id: f_00000-0-1 loss: 1.260022  [   64/  126]
train() client id: f_00000-0-2 loss: 1.337267  [   96/  126]
train() client id: f_00000-1-0 loss: 1.130525  [   32/  126]
train() client id: f_00000-1-1 loss: 1.250881  [   64/  126]
train() client id: f_00000-1-2 loss: 1.085212  [   96/  126]
train() client id: f_00000-2-0 loss: 0.898965  [   32/  126]
train() client id: f_00000-2-1 loss: 1.175913  [   64/  126]
train() client id: f_00000-2-2 loss: 1.136488  [   96/  126]
train() client id: f_00000-3-0 loss: 1.054595  [   32/  126]
train() client id: f_00000-3-1 loss: 0.956661  [   64/  126]
train() client id: f_00000-3-2 loss: 0.934289  [   96/  126]
train() client id: f_00000-4-0 loss: 1.050858  [   32/  126]
train() client id: f_00000-4-1 loss: 0.826899  [   64/  126]
train() client id: f_00000-4-2 loss: 1.020690  [   96/  126]
train() client id: f_00000-5-0 loss: 0.854279  [   32/  126]
train() client id: f_00000-5-1 loss: 0.895885  [   64/  126]
train() client id: f_00000-5-2 loss: 0.768364  [   96/  126]
train() client id: f_00000-6-0 loss: 0.795868  [   32/  126]
train() client id: f_00000-6-1 loss: 0.904190  [   64/  126]
train() client id: f_00000-6-2 loss: 0.880662  [   96/  126]
train() client id: f_00000-7-0 loss: 0.879773  [   32/  126]
train() client id: f_00000-7-1 loss: 0.897705  [   64/  126]
train() client id: f_00000-7-2 loss: 0.847355  [   96/  126]
train() client id: f_00000-8-0 loss: 0.798577  [   32/  126]
train() client id: f_00000-8-1 loss: 0.920391  [   64/  126]
train() client id: f_00000-8-2 loss: 0.826634  [   96/  126]
train() client id: f_00000-9-0 loss: 0.838980  [   32/  126]
train() client id: f_00000-9-1 loss: 0.773714  [   64/  126]
train() client id: f_00000-9-2 loss: 0.846053  [   96/  126]
train() client id: f_00001-0-0 loss: 0.368375  [   32/  265]
train() client id: f_00001-0-1 loss: 0.393275  [   64/  265]
train() client id: f_00001-0-2 loss: 0.337921  [   96/  265]
train() client id: f_00001-0-3 loss: 0.378044  [  128/  265]
train() client id: f_00001-0-4 loss: 0.446770  [  160/  265]
train() client id: f_00001-0-5 loss: 0.484929  [  192/  265]
train() client id: f_00001-0-6 loss: 0.397640  [  224/  265]
train() client id: f_00001-0-7 loss: 0.531576  [  256/  265]
train() client id: f_00001-1-0 loss: 0.350251  [   32/  265]
train() client id: f_00001-1-1 loss: 0.369669  [   64/  265]
train() client id: f_00001-1-2 loss: 0.387506  [   96/  265]
train() client id: f_00001-1-3 loss: 0.467475  [  128/  265]
train() client id: f_00001-1-4 loss: 0.310621  [  160/  265]
train() client id: f_00001-1-5 loss: 0.326520  [  192/  265]
train() client id: f_00001-1-6 loss: 0.429245  [  224/  265]
train() client id: f_00001-1-7 loss: 0.570872  [  256/  265]
train() client id: f_00001-2-0 loss: 0.300822  [   32/  265]
train() client id: f_00001-2-1 loss: 0.378057  [   64/  265]
train() client id: f_00001-2-2 loss: 0.404411  [   96/  265]
train() client id: f_00001-2-3 loss: 0.477479  [  128/  265]
train() client id: f_00001-2-4 loss: 0.519914  [  160/  265]
train() client id: f_00001-2-5 loss: 0.398023  [  192/  265]
train() client id: f_00001-2-6 loss: 0.435595  [  224/  265]
train() client id: f_00001-2-7 loss: 0.293971  [  256/  265]
train() client id: f_00001-3-0 loss: 0.298515  [   32/  265]
train() client id: f_00001-3-1 loss: 0.335748  [   64/  265]
train() client id: f_00001-3-2 loss: 0.407322  [   96/  265]
train() client id: f_00001-3-3 loss: 0.463167  [  128/  265]
train() client id: f_00001-3-4 loss: 0.307856  [  160/  265]
train() client id: f_00001-3-5 loss: 0.380949  [  192/  265]
train() client id: f_00001-3-6 loss: 0.499158  [  224/  265]
train() client id: f_00001-3-7 loss: 0.382484  [  256/  265]
train() client id: f_00001-4-0 loss: 0.364441  [   32/  265]
train() client id: f_00001-4-1 loss: 0.516508  [   64/  265]
train() client id: f_00001-4-2 loss: 0.438261  [   96/  265]
train() client id: f_00001-4-3 loss: 0.368091  [  128/  265]
train() client id: f_00001-4-4 loss: 0.371977  [  160/  265]
train() client id: f_00001-4-5 loss: 0.315236  [  192/  265]
train() client id: f_00001-4-6 loss: 0.351831  [  224/  265]
train() client id: f_00001-4-7 loss: 0.436476  [  256/  265]
train() client id: f_00001-5-0 loss: 0.319009  [   32/  265]
train() client id: f_00001-5-1 loss: 0.460039  [   64/  265]
train() client id: f_00001-5-2 loss: 0.354030  [   96/  265]
train() client id: f_00001-5-3 loss: 0.284496  [  128/  265]
train() client id: f_00001-5-4 loss: 0.483530  [  160/  265]
train() client id: f_00001-5-5 loss: 0.416604  [  192/  265]
train() client id: f_00001-5-6 loss: 0.321812  [  224/  265]
train() client id: f_00001-5-7 loss: 0.384542  [  256/  265]
train() client id: f_00001-6-0 loss: 0.319336  [   32/  265]
train() client id: f_00001-6-1 loss: 0.425050  [   64/  265]
train() client id: f_00001-6-2 loss: 0.410904  [   96/  265]
train() client id: f_00001-6-3 loss: 0.373532  [  128/  265]
train() client id: f_00001-6-4 loss: 0.563100  [  160/  265]
train() client id: f_00001-6-5 loss: 0.348731  [  192/  265]
train() client id: f_00001-6-6 loss: 0.361226  [  224/  265]
train() client id: f_00001-6-7 loss: 0.334621  [  256/  265]
train() client id: f_00001-7-0 loss: 0.434427  [   32/  265]
train() client id: f_00001-7-1 loss: 0.555556  [   64/  265]
train() client id: f_00001-7-2 loss: 0.289325  [   96/  265]
train() client id: f_00001-7-3 loss: 0.346430  [  128/  265]
train() client id: f_00001-7-4 loss: 0.311347  [  160/  265]
train() client id: f_00001-7-5 loss: 0.499650  [  192/  265]
train() client id: f_00001-7-6 loss: 0.289138  [  224/  265]
train() client id: f_00001-7-7 loss: 0.418619  [  256/  265]
train() client id: f_00001-8-0 loss: 0.447767  [   32/  265]
train() client id: f_00001-8-1 loss: 0.413661  [   64/  265]
train() client id: f_00001-8-2 loss: 0.370749  [   96/  265]
train() client id: f_00001-8-3 loss: 0.402930  [  128/  265]
train() client id: f_00001-8-4 loss: 0.302462  [  160/  265]
train() client id: f_00001-8-5 loss: 0.326427  [  192/  265]
train() client id: f_00001-8-6 loss: 0.375404  [  224/  265]
train() client id: f_00001-8-7 loss: 0.427461  [  256/  265]
train() client id: f_00001-9-0 loss: 0.352386  [   32/  265]
train() client id: f_00001-9-1 loss: 0.478205  [   64/  265]
train() client id: f_00001-9-2 loss: 0.354882  [   96/  265]
train() client id: f_00001-9-3 loss: 0.371091  [  128/  265]
train() client id: f_00001-9-4 loss: 0.294764  [  160/  265]
train() client id: f_00001-9-5 loss: 0.373609  [  192/  265]
train() client id: f_00001-9-6 loss: 0.376872  [  224/  265]
train() client id: f_00001-9-7 loss: 0.455742  [  256/  265]
train() client id: f_00002-0-0 loss: 1.032044  [   32/  124]
train() client id: f_00002-0-1 loss: 1.224398  [   64/  124]
train() client id: f_00002-0-2 loss: 1.389572  [   96/  124]
train() client id: f_00002-1-0 loss: 1.070703  [   32/  124]
train() client id: f_00002-1-1 loss: 1.385197  [   64/  124]
train() client id: f_00002-1-2 loss: 1.248212  [   96/  124]
train() client id: f_00002-2-0 loss: 1.277096  [   32/  124]
train() client id: f_00002-2-1 loss: 1.168376  [   64/  124]
train() client id: f_00002-2-2 loss: 1.115459  [   96/  124]
train() client id: f_00002-3-0 loss: 1.361929  [   32/  124]
train() client id: f_00002-3-1 loss: 1.246932  [   64/  124]
train() client id: f_00002-3-2 loss: 1.037123  [   96/  124]
train() client id: f_00002-4-0 loss: 1.345731  [   32/  124]
train() client id: f_00002-4-1 loss: 1.006954  [   64/  124]
train() client id: f_00002-4-2 loss: 1.232246  [   96/  124]
train() client id: f_00002-5-0 loss: 1.060220  [   32/  124]
train() client id: f_00002-5-1 loss: 1.195659  [   64/  124]
train() client id: f_00002-5-2 loss: 1.224048  [   96/  124]
train() client id: f_00002-6-0 loss: 1.105251  [   32/  124]
train() client id: f_00002-6-1 loss: 1.003874  [   64/  124]
train() client id: f_00002-6-2 loss: 1.167727  [   96/  124]
train() client id: f_00002-7-0 loss: 1.093713  [   32/  124]
train() client id: f_00002-7-1 loss: 1.254555  [   64/  124]
train() client id: f_00002-7-2 loss: 1.058702  [   96/  124]
train() client id: f_00002-8-0 loss: 1.187037  [   32/  124]
train() client id: f_00002-8-1 loss: 1.195155  [   64/  124]
train() client id: f_00002-8-2 loss: 1.033591  [   96/  124]
train() client id: f_00002-9-0 loss: 1.131864  [   32/  124]
train() client id: f_00002-9-1 loss: 1.164549  [   64/  124]
train() client id: f_00002-9-2 loss: 1.223982  [   96/  124]
train() client id: f_00003-0-0 loss: 0.789507  [   32/   43]
train() client id: f_00003-1-0 loss: 0.723501  [   32/   43]
train() client id: f_00003-2-0 loss: 0.697205  [   32/   43]
train() client id: f_00003-3-0 loss: 0.741757  [   32/   43]
train() client id: f_00003-4-0 loss: 0.744213  [   32/   43]
train() client id: f_00003-5-0 loss: 0.744642  [   32/   43]
train() client id: f_00003-6-0 loss: 0.668397  [   32/   43]
train() client id: f_00003-7-0 loss: 0.847927  [   32/   43]
train() client id: f_00003-8-0 loss: 0.616218  [   32/   43]
train() client id: f_00003-9-0 loss: 0.796674  [   32/   43]
train() client id: f_00004-0-0 loss: 0.878682  [   32/  306]
train() client id: f_00004-0-1 loss: 0.803190  [   64/  306]
train() client id: f_00004-0-2 loss: 0.908525  [   96/  306]
train() client id: f_00004-0-3 loss: 0.802267  [  128/  306]
train() client id: f_00004-0-4 loss: 1.041944  [  160/  306]
train() client id: f_00004-0-5 loss: 0.796021  [  192/  306]
train() client id: f_00004-0-6 loss: 0.985821  [  224/  306]
train() client id: f_00004-0-7 loss: 0.952717  [  256/  306]
train() client id: f_00004-0-8 loss: 0.927456  [  288/  306]
train() client id: f_00004-1-0 loss: 0.983366  [   32/  306]
train() client id: f_00004-1-1 loss: 0.876349  [   64/  306]
train() client id: f_00004-1-2 loss: 1.061732  [   96/  306]
train() client id: f_00004-1-3 loss: 0.844373  [  128/  306]
train() client id: f_00004-1-4 loss: 0.874799  [  160/  306]
train() client id: f_00004-1-5 loss: 0.842335  [  192/  306]
train() client id: f_00004-1-6 loss: 0.824258  [  224/  306]
train() client id: f_00004-1-7 loss: 0.845653  [  256/  306]
train() client id: f_00004-1-8 loss: 0.914045  [  288/  306]
train() client id: f_00004-2-0 loss: 0.871871  [   32/  306]
train() client id: f_00004-2-1 loss: 1.053404  [   64/  306]
train() client id: f_00004-2-2 loss: 1.042796  [   96/  306]
train() client id: f_00004-2-3 loss: 0.795085  [  128/  306]
train() client id: f_00004-2-4 loss: 0.880639  [  160/  306]
train() client id: f_00004-2-5 loss: 0.887726  [  192/  306]
train() client id: f_00004-2-6 loss: 0.828670  [  224/  306]
train() client id: f_00004-2-7 loss: 0.839249  [  256/  306]
train() client id: f_00004-2-8 loss: 0.864272  [  288/  306]
train() client id: f_00004-3-0 loss: 0.879275  [   32/  306]
train() client id: f_00004-3-1 loss: 0.920480  [   64/  306]
train() client id: f_00004-3-2 loss: 0.998451  [   96/  306]
train() client id: f_00004-3-3 loss: 0.978591  [  128/  306]
train() client id: f_00004-3-4 loss: 0.952116  [  160/  306]
train() client id: f_00004-3-5 loss: 0.759808  [  192/  306]
train() client id: f_00004-3-6 loss: 0.895554  [  224/  306]
train() client id: f_00004-3-7 loss: 0.847528  [  256/  306]
train() client id: f_00004-3-8 loss: 0.809849  [  288/  306]
train() client id: f_00004-4-0 loss: 0.906536  [   32/  306]
train() client id: f_00004-4-1 loss: 0.769181  [   64/  306]
train() client id: f_00004-4-2 loss: 0.915176  [   96/  306]
train() client id: f_00004-4-3 loss: 0.988423  [  128/  306]
train() client id: f_00004-4-4 loss: 0.942229  [  160/  306]
train() client id: f_00004-4-5 loss: 0.836226  [  192/  306]
train() client id: f_00004-4-6 loss: 0.975171  [  224/  306]
train() client id: f_00004-4-7 loss: 0.847592  [  256/  306]
train() client id: f_00004-4-8 loss: 0.899340  [  288/  306]
train() client id: f_00004-5-0 loss: 0.884885  [   32/  306]
train() client id: f_00004-5-1 loss: 0.836523  [   64/  306]
train() client id: f_00004-5-2 loss: 0.949180  [   96/  306]
train() client id: f_00004-5-3 loss: 0.846316  [  128/  306]
train() client id: f_00004-5-4 loss: 0.888502  [  160/  306]
train() client id: f_00004-5-5 loss: 0.947959  [  192/  306]
train() client id: f_00004-5-6 loss: 0.959584  [  224/  306]
train() client id: f_00004-5-7 loss: 0.888251  [  256/  306]
train() client id: f_00004-5-8 loss: 0.876277  [  288/  306]
train() client id: f_00004-6-0 loss: 0.936863  [   32/  306]
train() client id: f_00004-6-1 loss: 0.826737  [   64/  306]
train() client id: f_00004-6-2 loss: 0.838736  [   96/  306]
train() client id: f_00004-6-3 loss: 0.955163  [  128/  306]
train() client id: f_00004-6-4 loss: 0.944222  [  160/  306]
train() client id: f_00004-6-5 loss: 0.705568  [  192/  306]
train() client id: f_00004-6-6 loss: 0.990946  [  224/  306]
train() client id: f_00004-6-7 loss: 0.849245  [  256/  306]
train() client id: f_00004-6-8 loss: 0.938687  [  288/  306]
train() client id: f_00004-7-0 loss: 0.946280  [   32/  306]
train() client id: f_00004-7-1 loss: 0.885292  [   64/  306]
train() client id: f_00004-7-2 loss: 0.937737  [   96/  306]
train() client id: f_00004-7-3 loss: 0.845266  [  128/  306]
train() client id: f_00004-7-4 loss: 0.867543  [  160/  306]
train() client id: f_00004-7-5 loss: 0.871307  [  192/  306]
train() client id: f_00004-7-6 loss: 0.828425  [  224/  306]
train() client id: f_00004-7-7 loss: 0.931316  [  256/  306]
train() client id: f_00004-7-8 loss: 0.806901  [  288/  306]
train() client id: f_00004-8-0 loss: 0.734854  [   32/  306]
train() client id: f_00004-8-1 loss: 0.825956  [   64/  306]
train() client id: f_00004-8-2 loss: 0.968217  [   96/  306]
train() client id: f_00004-8-3 loss: 0.822301  [  128/  306]
train() client id: f_00004-8-4 loss: 0.896013  [  160/  306]
train() client id: f_00004-8-5 loss: 0.909165  [  192/  306]
train() client id: f_00004-8-6 loss: 0.798456  [  224/  306]
train() client id: f_00004-8-7 loss: 0.986993  [  256/  306]
train() client id: f_00004-8-8 loss: 0.957459  [  288/  306]
train() client id: f_00004-9-0 loss: 0.892518  [   32/  306]
train() client id: f_00004-9-1 loss: 0.891771  [   64/  306]
train() client id: f_00004-9-2 loss: 0.871417  [   96/  306]
train() client id: f_00004-9-3 loss: 0.854239  [  128/  306]
train() client id: f_00004-9-4 loss: 0.819707  [  160/  306]
train() client id: f_00004-9-5 loss: 0.949073  [  192/  306]
train() client id: f_00004-9-6 loss: 0.931352  [  224/  306]
train() client id: f_00004-9-7 loss: 0.813707  [  256/  306]
train() client id: f_00004-9-8 loss: 0.832770  [  288/  306]
train() client id: f_00005-0-0 loss: 0.387923  [   32/  146]
train() client id: f_00005-0-1 loss: 0.547472  [   64/  146]
train() client id: f_00005-0-2 loss: 0.765190  [   96/  146]
train() client id: f_00005-0-3 loss: 0.587131  [  128/  146]
train() client id: f_00005-1-0 loss: 0.874117  [   32/  146]
train() client id: f_00005-1-1 loss: 0.557615  [   64/  146]
train() client id: f_00005-1-2 loss: 0.464047  [   96/  146]
train() client id: f_00005-1-3 loss: 0.393063  [  128/  146]
train() client id: f_00005-2-0 loss: 0.516221  [   32/  146]
train() client id: f_00005-2-1 loss: 0.868062  [   64/  146]
train() client id: f_00005-2-2 loss: 0.358325  [   96/  146]
train() client id: f_00005-2-3 loss: 0.505376  [  128/  146]
train() client id: f_00005-3-0 loss: 0.574117  [   32/  146]
train() client id: f_00005-3-1 loss: 0.724349  [   64/  146]
train() client id: f_00005-3-2 loss: 0.482096  [   96/  146]
train() client id: f_00005-3-3 loss: 0.494344  [  128/  146]
train() client id: f_00005-4-0 loss: 0.620835  [   32/  146]
train() client id: f_00005-4-1 loss: 0.702247  [   64/  146]
train() client id: f_00005-4-2 loss: 0.557668  [   96/  146]
train() client id: f_00005-4-3 loss: 0.355896  [  128/  146]
train() client id: f_00005-5-0 loss: 0.299130  [   32/  146]
train() client id: f_00005-5-1 loss: 0.487738  [   64/  146]
train() client id: f_00005-5-2 loss: 0.764463  [   96/  146]
train() client id: f_00005-5-3 loss: 0.583188  [  128/  146]
train() client id: f_00005-6-0 loss: 0.644573  [   32/  146]
train() client id: f_00005-6-1 loss: 0.427315  [   64/  146]
train() client id: f_00005-6-2 loss: 0.511143  [   96/  146]
train() client id: f_00005-6-3 loss: 0.452221  [  128/  146]
train() client id: f_00005-7-0 loss: 0.370463  [   32/  146]
train() client id: f_00005-7-1 loss: 0.589685  [   64/  146]
train() client id: f_00005-7-2 loss: 0.651161  [   96/  146]
train() client id: f_00005-7-3 loss: 0.475435  [  128/  146]
train() client id: f_00005-8-0 loss: 0.471767  [   32/  146]
train() client id: f_00005-8-1 loss: 0.546315  [   64/  146]
train() client id: f_00005-8-2 loss: 0.553412  [   96/  146]
train() client id: f_00005-8-3 loss: 0.367185  [  128/  146]
train() client id: f_00005-9-0 loss: 0.572166  [   32/  146]
train() client id: f_00005-9-1 loss: 0.579514  [   64/  146]
train() client id: f_00005-9-2 loss: 0.328538  [   96/  146]
train() client id: f_00005-9-3 loss: 0.571981  [  128/  146]
train() client id: f_00006-0-0 loss: 0.483323  [   32/   54]
train() client id: f_00006-1-0 loss: 0.427250  [   32/   54]
train() client id: f_00006-2-0 loss: 0.531078  [   32/   54]
train() client id: f_00006-3-0 loss: 0.527277  [   32/   54]
train() client id: f_00006-4-0 loss: 0.421267  [   32/   54]
train() client id: f_00006-5-0 loss: 0.448469  [   32/   54]
train() client id: f_00006-6-0 loss: 0.516350  [   32/   54]
train() client id: f_00006-7-0 loss: 0.476534  [   32/   54]
train() client id: f_00006-8-0 loss: 0.528216  [   32/   54]
train() client id: f_00006-9-0 loss: 0.474410  [   32/   54]
train() client id: f_00007-0-0 loss: 0.543021  [   32/  179]
train() client id: f_00007-0-1 loss: 0.461305  [   64/  179]
train() client id: f_00007-0-2 loss: 0.521100  [   96/  179]
train() client id: f_00007-0-3 loss: 0.364126  [  128/  179]
train() client id: f_00007-0-4 loss: 0.317502  [  160/  179]
train() client id: f_00007-1-0 loss: 0.438150  [   32/  179]
train() client id: f_00007-1-1 loss: 0.561639  [   64/  179]
train() client id: f_00007-1-2 loss: 0.401083  [   96/  179]
train() client id: f_00007-1-3 loss: 0.403201  [  128/  179]
train() client id: f_00007-1-4 loss: 0.345407  [  160/  179]
train() client id: f_00007-2-0 loss: 0.491533  [   32/  179]
train() client id: f_00007-2-1 loss: 0.481286  [   64/  179]
train() client id: f_00007-2-2 loss: 0.444043  [   96/  179]
train() client id: f_00007-2-3 loss: 0.411638  [  128/  179]
train() client id: f_00007-2-4 loss: 0.281273  [  160/  179]
train() client id: f_00007-3-0 loss: 0.625642  [   32/  179]
train() client id: f_00007-3-1 loss: 0.378381  [   64/  179]
train() client id: f_00007-3-2 loss: 0.379968  [   96/  179]
train() client id: f_00007-3-3 loss: 0.271931  [  128/  179]
train() client id: f_00007-3-4 loss: 0.590775  [  160/  179]
train() client id: f_00007-4-0 loss: 0.509145  [   32/  179]
train() client id: f_00007-4-1 loss: 0.588830  [   64/  179]
train() client id: f_00007-4-2 loss: 0.237408  [   96/  179]
train() client id: f_00007-4-3 loss: 0.478807  [  128/  179]
train() client id: f_00007-4-4 loss: 0.371927  [  160/  179]
train() client id: f_00007-5-0 loss: 0.433819  [   32/  179]
train() client id: f_00007-5-1 loss: 0.371299  [   64/  179]
train() client id: f_00007-5-2 loss: 0.478723  [   96/  179]
train() client id: f_00007-5-3 loss: 0.538567  [  128/  179]
train() client id: f_00007-5-4 loss: 0.266019  [  160/  179]
train() client id: f_00007-6-0 loss: 0.277569  [   32/  179]
train() client id: f_00007-6-1 loss: 0.267850  [   64/  179]
train() client id: f_00007-6-2 loss: 0.558249  [   96/  179]
train() client id: f_00007-6-3 loss: 0.396800  [  128/  179]
train() client id: f_00007-6-4 loss: 0.449259  [  160/  179]
train() client id: f_00007-7-0 loss: 0.289734  [   32/  179]
train() client id: f_00007-7-1 loss: 0.233783  [   64/  179]
train() client id: f_00007-7-2 loss: 0.540382  [   96/  179]
train() client id: f_00007-7-3 loss: 0.476324  [  128/  179]
train() client id: f_00007-7-4 loss: 0.326042  [  160/  179]
train() client id: f_00007-8-0 loss: 0.282155  [   32/  179]
train() client id: f_00007-8-1 loss: 0.548189  [   64/  179]
train() client id: f_00007-8-2 loss: 0.434452  [   96/  179]
train() client id: f_00007-8-3 loss: 0.333392  [  128/  179]
train() client id: f_00007-8-4 loss: 0.513490  [  160/  179]
train() client id: f_00007-9-0 loss: 0.364215  [   32/  179]
train() client id: f_00007-9-1 loss: 0.440265  [   64/  179]
train() client id: f_00007-9-2 loss: 0.355202  [   96/  179]
train() client id: f_00007-9-3 loss: 0.458929  [  128/  179]
train() client id: f_00007-9-4 loss: 0.506902  [  160/  179]
train() client id: f_00008-0-0 loss: 0.830365  [   32/  130]
train() client id: f_00008-0-1 loss: 0.670529  [   64/  130]
train() client id: f_00008-0-2 loss: 0.734989  [   96/  130]
train() client id: f_00008-0-3 loss: 0.802189  [  128/  130]
train() client id: f_00008-1-0 loss: 0.789869  [   32/  130]
train() client id: f_00008-1-1 loss: 0.719541  [   64/  130]
train() client id: f_00008-1-2 loss: 0.734703  [   96/  130]
train() client id: f_00008-1-3 loss: 0.808001  [  128/  130]
train() client id: f_00008-2-0 loss: 0.680649  [   32/  130]
train() client id: f_00008-2-1 loss: 0.828282  [   64/  130]
train() client id: f_00008-2-2 loss: 0.765153  [   96/  130]
train() client id: f_00008-2-3 loss: 0.757489  [  128/  130]
train() client id: f_00008-3-0 loss: 0.754630  [   32/  130]
train() client id: f_00008-3-1 loss: 0.828222  [   64/  130]
train() client id: f_00008-3-2 loss: 0.658314  [   96/  130]
train() client id: f_00008-3-3 loss: 0.818211  [  128/  130]
train() client id: f_00008-4-0 loss: 0.809209  [   32/  130]
train() client id: f_00008-4-1 loss: 0.749575  [   64/  130]
train() client id: f_00008-4-2 loss: 0.801618  [   96/  130]
train() client id: f_00008-4-3 loss: 0.692213  [  128/  130]
train() client id: f_00008-5-0 loss: 0.757491  [   32/  130]
train() client id: f_00008-5-1 loss: 0.717219  [   64/  130]
train() client id: f_00008-5-2 loss: 0.818887  [   96/  130]
train() client id: f_00008-5-3 loss: 0.754521  [  128/  130]
train() client id: f_00008-6-0 loss: 0.769051  [   32/  130]
train() client id: f_00008-6-1 loss: 0.757538  [   64/  130]
train() client id: f_00008-6-2 loss: 0.789724  [   96/  130]
train() client id: f_00008-6-3 loss: 0.719989  [  128/  130]
train() client id: f_00008-7-0 loss: 0.652230  [   32/  130]
train() client id: f_00008-7-1 loss: 0.773569  [   64/  130]
train() client id: f_00008-7-2 loss: 0.694867  [   96/  130]
train() client id: f_00008-7-3 loss: 0.924234  [  128/  130]
train() client id: f_00008-8-0 loss: 0.737559  [   32/  130]
train() client id: f_00008-8-1 loss: 0.775863  [   64/  130]
train() client id: f_00008-8-2 loss: 0.701934  [   96/  130]
train() client id: f_00008-8-3 loss: 0.813153  [  128/  130]
train() client id: f_00008-9-0 loss: 0.666124  [   32/  130]
train() client id: f_00008-9-1 loss: 0.718317  [   64/  130]
train() client id: f_00008-9-2 loss: 0.743973  [   96/  130]
train() client id: f_00008-9-3 loss: 0.920523  [  128/  130]
train() client id: f_00009-0-0 loss: 0.950293  [   32/  118]
train() client id: f_00009-0-1 loss: 0.848032  [   64/  118]
train() client id: f_00009-0-2 loss: 1.090891  [   96/  118]
train() client id: f_00009-1-0 loss: 0.922906  [   32/  118]
train() client id: f_00009-1-1 loss: 0.825021  [   64/  118]
train() client id: f_00009-1-2 loss: 0.953092  [   96/  118]
train() client id: f_00009-2-0 loss: 0.872152  [   32/  118]
train() client id: f_00009-2-1 loss: 0.991614  [   64/  118]
train() client id: f_00009-2-2 loss: 0.827952  [   96/  118]
train() client id: f_00009-3-0 loss: 0.913258  [   32/  118]
train() client id: f_00009-3-1 loss: 0.758878  [   64/  118]
train() client id: f_00009-3-2 loss: 0.889249  [   96/  118]
train() client id: f_00009-4-0 loss: 0.928109  [   32/  118]
train() client id: f_00009-4-1 loss: 0.765481  [   64/  118]
train() client id: f_00009-4-2 loss: 0.740545  [   96/  118]
train() client id: f_00009-5-0 loss: 0.821411  [   32/  118]
train() client id: f_00009-5-1 loss: 0.961465  [   64/  118]
train() client id: f_00009-5-2 loss: 0.599541  [   96/  118]
train() client id: f_00009-6-0 loss: 0.779837  [   32/  118]
train() client id: f_00009-6-1 loss: 0.711507  [   64/  118]
train() client id: f_00009-6-2 loss: 0.766433  [   96/  118]
train() client id: f_00009-7-0 loss: 0.776156  [   32/  118]
train() client id: f_00009-7-1 loss: 0.842811  [   64/  118]
train() client id: f_00009-7-2 loss: 0.769337  [   96/  118]
train() client id: f_00009-8-0 loss: 0.761875  [   32/  118]
train() client id: f_00009-8-1 loss: 0.725522  [   64/  118]
train() client id: f_00009-8-2 loss: 0.777096  [   96/  118]
train() client id: f_00009-9-0 loss: 0.677146  [   32/  118]
train() client id: f_00009-9-1 loss: 0.706820  [   64/  118]
train() client id: f_00009-9-2 loss: 0.948757  [   96/  118]
At round 61 accuracy: 0.6445623342175066
At round 61 training accuracy: 0.5888665325285044
At round 61 training loss: 0.8358968828234089
update_location
xs = [  -3.9056584     4.20031788  325.00902392   18.81129433    0.97929623
    3.95640986 -287.44319194 -266.32485185  309.66397685 -252.06087855]
ys = [ 317.5879595   300.55583871    1.32061395 -287.45517586  279.35018685
  262.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [332.98253135 316.78297753 340.04795198 304.93334177 296.71111525
 281.22398726 304.3525573  284.48128755 325.88410291 271.20231996]
dists_bs = [222.7209014  218.80818669 529.31422911 501.42557522 204.59148124
 199.34071762 210.22394361 196.66744272 509.59168784 187.56031626]
uav_gains = [1.52568368e-12 1.97196846e-12 1.37954414e-12 2.43484695e-12
 2.84867986e-12 3.89882946e-12 2.46132727e-12 3.64447161e-12
 1.69955459e-12 4.80856961e-12]
bs_gains = [2.94824353e-11 3.09824743e-11 2.61153645e-12 3.03889670e-12
 3.73946054e-11 4.02184414e-11 3.46564429e-11 4.17679507e-11
 2.90450111e-12 4.76979163e-11]
Round 62
-------------------------------
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.82115899 5.66246847 2.78706489 1.03447434 6.52759612 3.14171865
 1.26660636 3.89259813 2.86603349 2.54747095]
obj_prev = 32.54719040436046
eta_min = 1.2218214947720352e-33	eta_max = 0.9394406220974982
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 7.459395796309552	eta = 0.9090909090909091
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 18.0305957827678	eta = 0.37609788314465736
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 12.220288063290287	eta = 0.5549189078534793
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 11.203657475325056	eta = 0.605272779953424
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 11.140690513111194	eta = 0.6086937697223753
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 11.140415786273087	eta = 0.6087087803393881
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 11.14041578100363	eta = 0.6087087806273096
eta = 0.6087087806273096
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [0.040963   0.08615235 0.0403128  0.01397944 0.09948158 0.04746507
 0.01755557 0.05819347 0.04226342 0.03836218]
ene_total = [1.12030085 1.67073395 1.1295553  0.56372656 1.90151444 0.97618988
 0.62570552 1.29997444 1.04280632 0.80990852]
ti_comp = [1.2940368  1.43579767 1.28232047 1.33779388 1.43906047 1.44025443
 1.33863419 1.36510734 1.35070396 1.44291406]
ti_coms = [0.22158526 0.0798244  0.23330159 0.17782818 0.0765616  0.07536764
 0.17698787 0.15051473 0.1649181  0.07270801]
t_total = [26.89973984 26.89973984 26.89973984 26.89973984 26.89973984 26.89973984
 26.89973984 26.89973984 26.89973984 26.89973984]
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [2.56544140e-06 1.93862966e-05 2.49009332e-06 9.54048993e-08
 2.97132334e-05 3.22198749e-06 1.88712718e-07 6.60950921e-06
 2.58614832e-06 1.69476237e-06]
ene_total = [0.39332206 0.14201894 0.41411524 0.31561678 0.13641131 0.13382205
 0.31412702 0.26725561 0.29274776 0.12907455]
optimize_network iter = 0 obj = 2.5385113123879783
eta = 0.6087087806273096
freqs = [15827603.24540961 30001562.15823117 15718689.94891933  5224809.85098799
 34564766.11963843 16478014.31267369  6557270.26566169 21314613.85120587
 15644958.54527248 13293299.23888385]
eta_min = 0.6087087806273109	eta_max = 0.7286862650795968
af = 0.0009590243179496368	bf = 0.9928211124535382	zeta = 0.0010549267497446007	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [4.92556679e-07 3.72210798e-06 4.78090085e-07 1.83174406e-08
 5.70484736e-06 6.18611464e-07 3.62322484e-08 1.26900498e-06
 4.96532343e-07 3.25389044e-07]
ene_total = [1.6927798  0.61008177 1.78228252 1.35847294 0.58530795 0.57579846
 1.35205494 1.14991453 1.25988629 0.5554585 ]
ti_comp = [0.82931765 0.97107852 0.81760132 0.87307473 0.97434132 0.97553528
 0.87391504 0.90038819 0.88598481 0.97819491]
ti_coms = [0.22158526 0.0798244  0.23330159 0.17782818 0.0765616  0.07536764
 0.17698787 0.15051473 0.1649181  0.07270801]
t_total = [26.89973984 26.89973984 26.89973984 26.89973984 26.89973984 26.89973984
 26.89973984 26.89973984 26.89973984 26.89973984]
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [1.61900639e-06 1.09851926e-05 1.58767144e-06 5.80605679e-08
 1.68004452e-05 1.82033517e-06 1.14768207e-07 3.93802316e-06
 1.55795981e-06 9.55815292e-07]
ene_total = [0.56722855 0.204606   0.59721779 0.45518453 0.19640314 0.19296354
 0.45303505 0.38537017 0.42217726 0.18613361]
optimize_network iter = 1 obj = 3.660319648720293
eta = 0.7286862650795968
freqs = [15746566.89412167 28283149.87566576 15718689.94891934  5104500.91249742
 32549667.43739824 15511225.99197442  6404139.63058966 20604377.17176979
 15207342.34336579 12502384.68143362]
eta_min = 0.7286862650796062	eta_max = 0.7286862650795745
af = 0.0008653145558439159	bf = 0.9928211124535382	zeta = 0.0009518460114283075	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [4.87525871e-07 3.30793358e-06 4.78090085e-07 1.74835808e-08
 5.05906076e-06 5.48151319e-07 3.45597587e-08 1.18584348e-06
 4.69143123e-07 2.87821399e-07]
ene_total = [1.69277942 0.61005013 1.78228252 1.35847287 0.58525862 0.57579307
 1.35205481 1.14990818 1.25988419 0.55545563]
ti_comp = [0.82931765 0.97107852 0.81760132 0.87307473 0.97434132 0.97553528
 0.87391504 0.90038819 0.88598481 0.97819491]
ti_coms = [0.22158526 0.0798244  0.23330159 0.17782818 0.0765616  0.07536764
 0.17698787 0.15051473 0.1649181  0.07270801]
t_total = [26.89973984 26.89973984 26.89973984 26.89973984 26.89973984 26.89973984
 26.89973984 26.89973984 26.89973984 26.89973984]
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [1.61900639e-06 1.09851926e-05 1.58767144e-06 5.80605679e-08
 1.68004452e-05 1.82033517e-06 1.14768207e-07 3.93802316e-06
 1.55795981e-06 9.55815292e-07]
ene_total = [0.56722855 0.204606   0.59721779 0.45518453 0.19640314 0.19296354
 0.45303505 0.38537017 0.42217726 0.18613361]
optimize_network iter = 2 obj = 3.6603196487199923
eta = 0.7286862650795745
freqs = [15746566.89412155 28283149.87566597 15718689.9489192   5104500.91249741
 32549667.4373985  15511225.99197455  6404139.63058965 20604377.17176981
 15207342.34336578 12502384.68143372]
Done!
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [1.56211252e-06 1.05991595e-05 1.53187873e-06 5.60202486e-08
 1.62100570e-05 1.75636636e-06 1.10735112e-07 3.79963621e-06
 1.50321120e-06 9.22226774e-07]
ene_total = [0.02216009 0.00799304 0.02333169 0.01778287 0.00767237 0.00753852
 0.0176989  0.01505527 0.01649331 0.00727172]
At round 62 energy consumption: 0.1429977884216537
At round 62 eta: 0.7286862650795745
At round 62 a_n: 6.602214497187386
At round 62 local rounds: 10.364211052331338
At round 62 global rounds: 25.59678869996595
gradient difference: 0.5390576124191284
train() client id: f_00000-0-0 loss: 0.805762  [   32/  126]
train() client id: f_00000-0-1 loss: 1.048401  [   64/  126]
train() client id: f_00000-0-2 loss: 1.026430  [   96/  126]
train() client id: f_00000-1-0 loss: 0.975772  [   32/  126]
train() client id: f_00000-1-1 loss: 0.959399  [   64/  126]
train() client id: f_00000-1-2 loss: 1.173074  [   96/  126]
train() client id: f_00000-2-0 loss: 1.134792  [   32/  126]
train() client id: f_00000-2-1 loss: 1.001779  [   64/  126]
train() client id: f_00000-2-2 loss: 0.888448  [   96/  126]
train() client id: f_00000-3-0 loss: 1.044342  [   32/  126]
train() client id: f_00000-3-1 loss: 0.956395  [   64/  126]
train() client id: f_00000-3-2 loss: 0.849803  [   96/  126]
train() client id: f_00000-4-0 loss: 0.801109  [   32/  126]
train() client id: f_00000-4-1 loss: 0.997625  [   64/  126]
train() client id: f_00000-4-2 loss: 0.875348  [   96/  126]
train() client id: f_00000-5-0 loss: 0.901868  [   32/  126]
train() client id: f_00000-5-1 loss: 0.936109  [   64/  126]
train() client id: f_00000-5-2 loss: 0.997370  [   96/  126]
train() client id: f_00000-6-0 loss: 0.794325  [   32/  126]
train() client id: f_00000-6-1 loss: 0.933564  [   64/  126]
train() client id: f_00000-6-2 loss: 0.961607  [   96/  126]
train() client id: f_00000-7-0 loss: 0.832109  [   32/  126]
train() client id: f_00000-7-1 loss: 1.002108  [   64/  126]
train() client id: f_00000-7-2 loss: 0.855084  [   96/  126]
train() client id: f_00000-8-0 loss: 0.938569  [   32/  126]
train() client id: f_00000-8-1 loss: 0.871899  [   64/  126]
train() client id: f_00000-8-2 loss: 0.781542  [   96/  126]
train() client id: f_00000-9-0 loss: 0.878326  [   32/  126]
train() client id: f_00000-9-1 loss: 0.872642  [   64/  126]
train() client id: f_00000-9-2 loss: 0.847799  [   96/  126]
train() client id: f_00001-0-0 loss: 0.474673  [   32/  265]
train() client id: f_00001-0-1 loss: 0.481283  [   64/  265]
train() client id: f_00001-0-2 loss: 0.403809  [   96/  265]
train() client id: f_00001-0-3 loss: 0.445393  [  128/  265]
train() client id: f_00001-0-4 loss: 0.639510  [  160/  265]
train() client id: f_00001-0-5 loss: 0.403431  [  192/  265]
train() client id: f_00001-0-6 loss: 0.321585  [  224/  265]
train() client id: f_00001-0-7 loss: 0.379734  [  256/  265]
train() client id: f_00001-1-0 loss: 0.466175  [   32/  265]
train() client id: f_00001-1-1 loss: 0.542600  [   64/  265]
train() client id: f_00001-1-2 loss: 0.348334  [   96/  265]
train() client id: f_00001-1-3 loss: 0.386119  [  128/  265]
train() client id: f_00001-1-4 loss: 0.412386  [  160/  265]
train() client id: f_00001-1-5 loss: 0.334142  [  192/  265]
train() client id: f_00001-1-6 loss: 0.508590  [  224/  265]
train() client id: f_00001-1-7 loss: 0.401514  [  256/  265]
train() client id: f_00001-2-0 loss: 0.423911  [   32/  265]
train() client id: f_00001-2-1 loss: 0.492529  [   64/  265]
train() client id: f_00001-2-2 loss: 0.320747  [   96/  265]
train() client id: f_00001-2-3 loss: 0.457801  [  128/  265]
train() client id: f_00001-2-4 loss: 0.403502  [  160/  265]
train() client id: f_00001-2-5 loss: 0.405328  [  192/  265]
train() client id: f_00001-2-6 loss: 0.480605  [  224/  265]
train() client id: f_00001-2-7 loss: 0.422277  [  256/  265]
train() client id: f_00001-3-0 loss: 0.364610  [   32/  265]
train() client id: f_00001-3-1 loss: 0.742512  [   64/  265]
train() client id: f_00001-3-2 loss: 0.395631  [   96/  265]
train() client id: f_00001-3-3 loss: 0.312512  [  128/  265]
train() client id: f_00001-3-4 loss: 0.413627  [  160/  265]
train() client id: f_00001-3-5 loss: 0.371243  [  192/  265]
train() client id: f_00001-3-6 loss: 0.384373  [  224/  265]
train() client id: f_00001-3-7 loss: 0.362624  [  256/  265]
train() client id: f_00001-4-0 loss: 0.390381  [   32/  265]
train() client id: f_00001-4-1 loss: 0.357439  [   64/  265]
train() client id: f_00001-4-2 loss: 0.514216  [   96/  265]
train() client id: f_00001-4-3 loss: 0.396015  [  128/  265]
train() client id: f_00001-4-4 loss: 0.334311  [  160/  265]
train() client id: f_00001-4-5 loss: 0.446595  [  192/  265]
train() client id: f_00001-4-6 loss: 0.551488  [  224/  265]
train() client id: f_00001-4-7 loss: 0.340704  [  256/  265]
train() client id: f_00001-5-0 loss: 0.317794  [   32/  265]
train() client id: f_00001-5-1 loss: 0.444771  [   64/  265]
train() client id: f_00001-5-2 loss: 0.447779  [   96/  265]
train() client id: f_00001-5-3 loss: 0.360654  [  128/  265]
train() client id: f_00001-5-4 loss: 0.567059  [  160/  265]
train() client id: f_00001-5-5 loss: 0.370559  [  192/  265]
train() client id: f_00001-5-6 loss: 0.306676  [  224/  265]
train() client id: f_00001-5-7 loss: 0.401310  [  256/  265]
train() client id: f_00001-6-0 loss: 0.550858  [   32/  265]
train() client id: f_00001-6-1 loss: 0.442949  [   64/  265]
train() client id: f_00001-6-2 loss: 0.423974  [   96/  265]
train() client id: f_00001-6-3 loss: 0.394748  [  128/  265]
train() client id: f_00001-6-4 loss: 0.339270  [  160/  265]
train() client id: f_00001-6-5 loss: 0.473762  [  192/  265]
train() client id: f_00001-6-6 loss: 0.320001  [  224/  265]
train() client id: f_00001-6-7 loss: 0.325492  [  256/  265]
train() client id: f_00001-7-0 loss: 0.354709  [   32/  265]
train() client id: f_00001-7-1 loss: 0.367369  [   64/  265]
train() client id: f_00001-7-2 loss: 0.539487  [   96/  265]
train() client id: f_00001-7-3 loss: 0.519702  [  128/  265]
train() client id: f_00001-7-4 loss: 0.367953  [  160/  265]
train() client id: f_00001-7-5 loss: 0.414407  [  192/  265]
train() client id: f_00001-7-6 loss: 0.321821  [  224/  265]
train() client id: f_00001-7-7 loss: 0.365830  [  256/  265]
train() client id: f_00001-8-0 loss: 0.367047  [   32/  265]
train() client id: f_00001-8-1 loss: 0.375322  [   64/  265]
train() client id: f_00001-8-2 loss: 0.430937  [   96/  265]
train() client id: f_00001-8-3 loss: 0.326906  [  128/  265]
train() client id: f_00001-8-4 loss: 0.480353  [  160/  265]
train() client id: f_00001-8-5 loss: 0.455808  [  192/  265]
train() client id: f_00001-8-6 loss: 0.419854  [  224/  265]
train() client id: f_00001-8-7 loss: 0.385422  [  256/  265]
train() client id: f_00001-9-0 loss: 0.309473  [   32/  265]
train() client id: f_00001-9-1 loss: 0.389803  [   64/  265]
train() client id: f_00001-9-2 loss: 0.480501  [   96/  265]
train() client id: f_00001-9-3 loss: 0.337075  [  128/  265]
train() client id: f_00001-9-4 loss: 0.403038  [  160/  265]
train() client id: f_00001-9-5 loss: 0.429644  [  192/  265]
train() client id: f_00001-9-6 loss: 0.483564  [  224/  265]
train() client id: f_00001-9-7 loss: 0.384878  [  256/  265]
train() client id: f_00002-0-0 loss: 1.021455  [   32/  124]
train() client id: f_00002-0-1 loss: 0.955740  [   64/  124]
train() client id: f_00002-0-2 loss: 0.937163  [   96/  124]
train() client id: f_00002-1-0 loss: 1.020515  [   32/  124]
train() client id: f_00002-1-1 loss: 1.137006  [   64/  124]
train() client id: f_00002-1-2 loss: 0.991860  [   96/  124]
train() client id: f_00002-2-0 loss: 0.930205  [   32/  124]
train() client id: f_00002-2-1 loss: 0.810152  [   64/  124]
train() client id: f_00002-2-2 loss: 1.122780  [   96/  124]
train() client id: f_00002-3-0 loss: 0.985829  [   32/  124]
train() client id: f_00002-3-1 loss: 1.157649  [   64/  124]
train() client id: f_00002-3-2 loss: 0.810084  [   96/  124]
train() client id: f_00002-4-0 loss: 0.830427  [   32/  124]
train() client id: f_00002-4-1 loss: 1.028882  [   64/  124]
train() client id: f_00002-4-2 loss: 0.825789  [   96/  124]
train() client id: f_00002-5-0 loss: 0.976900  [   32/  124]
train() client id: f_00002-5-1 loss: 0.910440  [   64/  124]
train() client id: f_00002-5-2 loss: 0.886058  [   96/  124]
train() client id: f_00002-6-0 loss: 0.951909  [   32/  124]
train() client id: f_00002-6-1 loss: 0.865454  [   64/  124]
train() client id: f_00002-6-2 loss: 0.945270  [   96/  124]
train() client id: f_00002-7-0 loss: 0.863661  [   32/  124]
train() client id: f_00002-7-1 loss: 0.919360  [   64/  124]
train() client id: f_00002-7-2 loss: 0.819191  [   96/  124]
train() client id: f_00002-8-0 loss: 0.915017  [   32/  124]
train() client id: f_00002-8-1 loss: 0.733943  [   64/  124]
train() client id: f_00002-8-2 loss: 0.913385  [   96/  124]
train() client id: f_00002-9-0 loss: 0.907105  [   32/  124]
train() client id: f_00002-9-1 loss: 0.981221  [   64/  124]
train() client id: f_00002-9-2 loss: 0.907327  [   96/  124]
train() client id: f_00003-0-0 loss: 0.640840  [   32/   43]
train() client id: f_00003-1-0 loss: 0.591625  [   32/   43]
train() client id: f_00003-2-0 loss: 0.595342  [   32/   43]
train() client id: f_00003-3-0 loss: 0.517255  [   32/   43]
train() client id: f_00003-4-0 loss: 0.527443  [   32/   43]
train() client id: f_00003-5-0 loss: 0.743792  [   32/   43]
train() client id: f_00003-6-0 loss: 0.637206  [   32/   43]
train() client id: f_00003-7-0 loss: 0.609293  [   32/   43]
train() client id: f_00003-8-0 loss: 0.601031  [   32/   43]
train() client id: f_00003-9-0 loss: 0.609609  [   32/   43]
train() client id: f_00004-0-0 loss: 0.986621  [   32/  306]
train() client id: f_00004-0-1 loss: 0.762280  [   64/  306]
train() client id: f_00004-0-2 loss: 0.844245  [   96/  306]
train() client id: f_00004-0-3 loss: 1.040020  [  128/  306]
train() client id: f_00004-0-4 loss: 0.873199  [  160/  306]
train() client id: f_00004-0-5 loss: 0.922345  [  192/  306]
train() client id: f_00004-0-6 loss: 0.764799  [  224/  306]
train() client id: f_00004-0-7 loss: 0.779319  [  256/  306]
train() client id: f_00004-0-8 loss: 0.780763  [  288/  306]
train() client id: f_00004-1-0 loss: 1.105103  [   32/  306]
train() client id: f_00004-1-1 loss: 0.831291  [   64/  306]
train() client id: f_00004-1-2 loss: 0.862799  [   96/  306]
train() client id: f_00004-1-3 loss: 0.966819  [  128/  306]
train() client id: f_00004-1-4 loss: 0.759009  [  160/  306]
train() client id: f_00004-1-5 loss: 0.880114  [  192/  306]
train() client id: f_00004-1-6 loss: 0.922465  [  224/  306]
train() client id: f_00004-1-7 loss: 0.790752  [  256/  306]
train() client id: f_00004-1-8 loss: 0.685666  [  288/  306]
train() client id: f_00004-2-0 loss: 0.885638  [   32/  306]
train() client id: f_00004-2-1 loss: 0.948333  [   64/  306]
train() client id: f_00004-2-2 loss: 0.899136  [   96/  306]
train() client id: f_00004-2-3 loss: 0.802613  [  128/  306]
train() client id: f_00004-2-4 loss: 0.947516  [  160/  306]
train() client id: f_00004-2-5 loss: 0.729085  [  192/  306]
train() client id: f_00004-2-6 loss: 0.918797  [  224/  306]
train() client id: f_00004-2-7 loss: 0.872170  [  256/  306]
train() client id: f_00004-2-8 loss: 0.776227  [  288/  306]
train() client id: f_00004-3-0 loss: 0.855627  [   32/  306]
train() client id: f_00004-3-1 loss: 0.908216  [   64/  306]
train() client id: f_00004-3-2 loss: 0.797183  [   96/  306]
train() client id: f_00004-3-3 loss: 0.762352  [  128/  306]
train() client id: f_00004-3-4 loss: 0.964673  [  160/  306]
train() client id: f_00004-3-5 loss: 0.894910  [  192/  306]
train() client id: f_00004-3-6 loss: 0.852161  [  224/  306]
train() client id: f_00004-3-7 loss: 0.866477  [  256/  306]
train() client id: f_00004-3-8 loss: 0.789715  [  288/  306]
train() client id: f_00004-4-0 loss: 0.836448  [   32/  306]
train() client id: f_00004-4-1 loss: 0.904382  [   64/  306]
train() client id: f_00004-4-2 loss: 0.791339  [   96/  306]
train() client id: f_00004-4-3 loss: 0.978640  [  128/  306]
train() client id: f_00004-4-4 loss: 1.025819  [  160/  306]
train() client id: f_00004-4-5 loss: 0.892912  [  192/  306]
train() client id: f_00004-4-6 loss: 0.728658  [  224/  306]
train() client id: f_00004-4-7 loss: 0.835963  [  256/  306]
train() client id: f_00004-4-8 loss: 0.869339  [  288/  306]
train() client id: f_00004-5-0 loss: 0.930540  [   32/  306]
train() client id: f_00004-5-1 loss: 0.777444  [   64/  306]
train() client id: f_00004-5-2 loss: 0.934849  [   96/  306]
train() client id: f_00004-5-3 loss: 0.828045  [  128/  306]
train() client id: f_00004-5-4 loss: 1.007203  [  160/  306]
train() client id: f_00004-5-5 loss: 0.776236  [  192/  306]
train() client id: f_00004-5-6 loss: 0.735802  [  224/  306]
train() client id: f_00004-5-7 loss: 0.720166  [  256/  306]
train() client id: f_00004-5-8 loss: 1.005710  [  288/  306]
train() client id: f_00004-6-0 loss: 0.896664  [   32/  306]
train() client id: f_00004-6-1 loss: 1.005875  [   64/  306]
train() client id: f_00004-6-2 loss: 0.795141  [   96/  306]
train() client id: f_00004-6-3 loss: 0.874243  [  128/  306]
train() client id: f_00004-6-4 loss: 0.776440  [  160/  306]
train() client id: f_00004-6-5 loss: 0.761058  [  192/  306]
train() client id: f_00004-6-6 loss: 0.893473  [  224/  306]
train() client id: f_00004-6-7 loss: 0.825713  [  256/  306]
train() client id: f_00004-6-8 loss: 0.800009  [  288/  306]
train() client id: f_00004-7-0 loss: 0.841535  [   32/  306]
train() client id: f_00004-7-1 loss: 0.744196  [   64/  306]
train() client id: f_00004-7-2 loss: 0.822502  [   96/  306]
train() client id: f_00004-7-3 loss: 0.830462  [  128/  306]
train() client id: f_00004-7-4 loss: 0.883119  [  160/  306]
train() client id: f_00004-7-5 loss: 0.899533  [  192/  306]
train() client id: f_00004-7-6 loss: 0.826366  [  224/  306]
train() client id: f_00004-7-7 loss: 0.916470  [  256/  306]
train() client id: f_00004-7-8 loss: 0.930164  [  288/  306]
train() client id: f_00004-8-0 loss: 0.785476  [   32/  306]
train() client id: f_00004-8-1 loss: 0.917694  [   64/  306]
train() client id: f_00004-8-2 loss: 0.845384  [   96/  306]
train() client id: f_00004-8-3 loss: 0.876003  [  128/  306]
train() client id: f_00004-8-4 loss: 0.973675  [  160/  306]
train() client id: f_00004-8-5 loss: 0.900710  [  192/  306]
train() client id: f_00004-8-6 loss: 0.817463  [  224/  306]
train() client id: f_00004-8-7 loss: 0.886912  [  256/  306]
train() client id: f_00004-8-8 loss: 0.717140  [  288/  306]
train() client id: f_00004-9-0 loss: 0.866359  [   32/  306]
train() client id: f_00004-9-1 loss: 0.852361  [   64/  306]
train() client id: f_00004-9-2 loss: 0.908838  [   96/  306]
train() client id: f_00004-9-3 loss: 0.880543  [  128/  306]
train() client id: f_00004-9-4 loss: 1.010348  [  160/  306]
train() client id: f_00004-9-5 loss: 0.825782  [  192/  306]
train() client id: f_00004-9-6 loss: 0.794611  [  224/  306]
train() client id: f_00004-9-7 loss: 0.832732  [  256/  306]
train() client id: f_00004-9-8 loss: 0.728072  [  288/  306]
train() client id: f_00005-0-0 loss: 0.577218  [   32/  146]
train() client id: f_00005-0-1 loss: 0.584452  [   64/  146]
train() client id: f_00005-0-2 loss: 0.605900  [   96/  146]
train() client id: f_00005-0-3 loss: 0.427939  [  128/  146]
train() client id: f_00005-1-0 loss: 0.597866  [   32/  146]
train() client id: f_00005-1-1 loss: 0.604604  [   64/  146]
train() client id: f_00005-1-2 loss: 0.337241  [   96/  146]
train() client id: f_00005-1-3 loss: 0.782270  [  128/  146]
train() client id: f_00005-2-0 loss: 0.569033  [   32/  146]
train() client id: f_00005-2-1 loss: 0.404339  [   64/  146]
train() client id: f_00005-2-2 loss: 0.799964  [   96/  146]
train() client id: f_00005-2-3 loss: 0.588842  [  128/  146]
train() client id: f_00005-3-0 loss: 0.560503  [   32/  146]
train() client id: f_00005-3-1 loss: 0.731580  [   64/  146]
train() client id: f_00005-3-2 loss: 0.440115  [   96/  146]
train() client id: f_00005-3-3 loss: 0.691994  [  128/  146]
train() client id: f_00005-4-0 loss: 0.748723  [   32/  146]
train() client id: f_00005-4-1 loss: 0.562703  [   64/  146]
train() client id: f_00005-4-2 loss: 0.507141  [   96/  146]
train() client id: f_00005-4-3 loss: 0.456793  [  128/  146]
train() client id: f_00005-5-0 loss: 0.513969  [   32/  146]
train() client id: f_00005-5-1 loss: 0.537779  [   64/  146]
train() client id: f_00005-5-2 loss: 0.659951  [   96/  146]
train() client id: f_00005-5-3 loss: 0.653991  [  128/  146]
train() client id: f_00005-6-0 loss: 0.500029  [   32/  146]
train() client id: f_00005-6-1 loss: 0.371445  [   64/  146]
train() client id: f_00005-6-2 loss: 0.627516  [   96/  146]
train() client id: f_00005-6-3 loss: 0.717725  [  128/  146]
train() client id: f_00005-7-0 loss: 0.712728  [   32/  146]
train() client id: f_00005-7-1 loss: 0.269249  [   64/  146]
train() client id: f_00005-7-2 loss: 0.802218  [   96/  146]
train() client id: f_00005-7-3 loss: 0.608915  [  128/  146]
train() client id: f_00005-8-0 loss: 0.614719  [   32/  146]
train() client id: f_00005-8-1 loss: 0.611042  [   64/  146]
train() client id: f_00005-8-2 loss: 0.436649  [   96/  146]
train() client id: f_00005-8-3 loss: 0.786167  [  128/  146]
train() client id: f_00005-9-0 loss: 0.409657  [   32/  146]
train() client id: f_00005-9-1 loss: 0.563328  [   64/  146]
train() client id: f_00005-9-2 loss: 0.581023  [   96/  146]
train() client id: f_00005-9-3 loss: 0.626743  [  128/  146]
train() client id: f_00006-0-0 loss: 0.536555  [   32/   54]
train() client id: f_00006-1-0 loss: 0.452772  [   32/   54]
train() client id: f_00006-2-0 loss: 0.461237  [   32/   54]
train() client id: f_00006-3-0 loss: 0.583611  [   32/   54]
train() client id: f_00006-4-0 loss: 0.559824  [   32/   54]
train() client id: f_00006-5-0 loss: 0.574237  [   32/   54]
train() client id: f_00006-6-0 loss: 0.535691  [   32/   54]
train() client id: f_00006-7-0 loss: 0.466416  [   32/   54]
train() client id: f_00006-8-0 loss: 0.561073  [   32/   54]
train() client id: f_00006-9-0 loss: 0.515419  [   32/   54]
train() client id: f_00007-0-0 loss: 0.470539  [   32/  179]
train() client id: f_00007-0-1 loss: 0.400376  [   64/  179]
train() client id: f_00007-0-2 loss: 0.724025  [   96/  179]
train() client id: f_00007-0-3 loss: 0.390159  [  128/  179]
train() client id: f_00007-0-4 loss: 0.525729  [  160/  179]
train() client id: f_00007-1-0 loss: 0.377339  [   32/  179]
train() client id: f_00007-1-1 loss: 0.509894  [   64/  179]
train() client id: f_00007-1-2 loss: 0.652196  [   96/  179]
train() client id: f_00007-1-3 loss: 0.328582  [  128/  179]
train() client id: f_00007-1-4 loss: 0.549506  [  160/  179]
train() client id: f_00007-2-0 loss: 0.388380  [   32/  179]
train() client id: f_00007-2-1 loss: 0.520466  [   64/  179]
train() client id: f_00007-2-2 loss: 0.593244  [   96/  179]
train() client id: f_00007-2-3 loss: 0.531051  [  128/  179]
train() client id: f_00007-2-4 loss: 0.398491  [  160/  179]
train() client id: f_00007-3-0 loss: 0.493957  [   32/  179]
train() client id: f_00007-3-1 loss: 0.303628  [   64/  179]
train() client id: f_00007-3-2 loss: 0.610221  [   96/  179]
train() client id: f_00007-3-3 loss: 0.425584  [  128/  179]
train() client id: f_00007-3-4 loss: 0.504599  [  160/  179]
train() client id: f_00007-4-0 loss: 0.714243  [   32/  179]
train() client id: f_00007-4-1 loss: 0.279955  [   64/  179]
train() client id: f_00007-4-2 loss: 0.451617  [   96/  179]
train() client id: f_00007-4-3 loss: 0.281856  [  128/  179]
train() client id: f_00007-4-4 loss: 0.422471  [  160/  179]
train() client id: f_00007-5-0 loss: 0.355355  [   32/  179]
train() client id: f_00007-5-1 loss: 0.443374  [   64/  179]
train() client id: f_00007-5-2 loss: 0.628799  [   96/  179]
train() client id: f_00007-5-3 loss: 0.358040  [  128/  179]
train() client id: f_00007-5-4 loss: 0.473354  [  160/  179]
train() client id: f_00007-6-0 loss: 0.317616  [   32/  179]
train() client id: f_00007-6-1 loss: 0.605187  [   64/  179]
train() client id: f_00007-6-2 loss: 0.505116  [   96/  179]
train() client id: f_00007-6-3 loss: 0.296501  [  128/  179]
train() client id: f_00007-6-4 loss: 0.278770  [  160/  179]
train() client id: f_00007-7-0 loss: 0.424153  [   32/  179]
train() client id: f_00007-7-1 loss: 0.370365  [   64/  179]
train() client id: f_00007-7-2 loss: 0.477036  [   96/  179]
train() client id: f_00007-7-3 loss: 0.350513  [  128/  179]
train() client id: f_00007-7-4 loss: 0.362866  [  160/  179]
train() client id: f_00007-8-0 loss: 0.331536  [   32/  179]
train() client id: f_00007-8-1 loss: 0.384595  [   64/  179]
train() client id: f_00007-8-2 loss: 0.547289  [   96/  179]
train() client id: f_00007-8-3 loss: 0.300648  [  128/  179]
train() client id: f_00007-8-4 loss: 0.458416  [  160/  179]
train() client id: f_00007-9-0 loss: 0.278305  [   32/  179]
train() client id: f_00007-9-1 loss: 0.542836  [   64/  179]
train() client id: f_00007-9-2 loss: 0.274846  [   96/  179]
train() client id: f_00007-9-3 loss: 0.437463  [  128/  179]
train() client id: f_00007-9-4 loss: 0.451777  [  160/  179]
train() client id: f_00008-0-0 loss: 0.571390  [   32/  130]
train() client id: f_00008-0-1 loss: 0.598718  [   64/  130]
train() client id: f_00008-0-2 loss: 0.453298  [   96/  130]
train() client id: f_00008-0-3 loss: 0.618026  [  128/  130]
train() client id: f_00008-1-0 loss: 0.607233  [   32/  130]
train() client id: f_00008-1-1 loss: 0.585045  [   64/  130]
train() client id: f_00008-1-2 loss: 0.529742  [   96/  130]
train() client id: f_00008-1-3 loss: 0.550713  [  128/  130]
train() client id: f_00008-2-0 loss: 0.566681  [   32/  130]
train() client id: f_00008-2-1 loss: 0.494109  [   64/  130]
train() client id: f_00008-2-2 loss: 0.594102  [   96/  130]
train() client id: f_00008-2-3 loss: 0.600806  [  128/  130]
train() client id: f_00008-3-0 loss: 0.629276  [   32/  130]
train() client id: f_00008-3-1 loss: 0.480710  [   64/  130]
train() client id: f_00008-3-2 loss: 0.578879  [   96/  130]
train() client id: f_00008-3-3 loss: 0.576365  [  128/  130]
train() client id: f_00008-4-0 loss: 0.594334  [   32/  130]
train() client id: f_00008-4-1 loss: 0.464370  [   64/  130]
train() client id: f_00008-4-2 loss: 0.641257  [   96/  130]
train() client id: f_00008-4-3 loss: 0.536079  [  128/  130]
train() client id: f_00008-5-0 loss: 0.516120  [   32/  130]
train() client id: f_00008-5-1 loss: 0.675138  [   64/  130]
train() client id: f_00008-5-2 loss: 0.606285  [   96/  130]
train() client id: f_00008-5-3 loss: 0.478639  [  128/  130]
train() client id: f_00008-6-0 loss: 0.543082  [   32/  130]
train() client id: f_00008-6-1 loss: 0.572399  [   64/  130]
train() client id: f_00008-6-2 loss: 0.624994  [   96/  130]
train() client id: f_00008-6-3 loss: 0.537073  [  128/  130]
train() client id: f_00008-7-0 loss: 0.460775  [   32/  130]
train() client id: f_00008-7-1 loss: 0.556301  [   64/  130]
train() client id: f_00008-7-2 loss: 0.690756  [   96/  130]
train() client id: f_00008-7-3 loss: 0.546439  [  128/  130]
train() client id: f_00008-8-0 loss: 0.615327  [   32/  130]
train() client id: f_00008-8-1 loss: 0.596776  [   64/  130]
train() client id: f_00008-8-2 loss: 0.639439  [   96/  130]
train() client id: f_00008-8-3 loss: 0.436909  [  128/  130]
train() client id: f_00008-9-0 loss: 0.517512  [   32/  130]
train() client id: f_00008-9-1 loss: 0.684990  [   64/  130]
train() client id: f_00008-9-2 loss: 0.606335  [   96/  130]
train() client id: f_00008-9-3 loss: 0.469965  [  128/  130]
train() client id: f_00009-0-0 loss: 0.773699  [   32/  118]
train() client id: f_00009-0-1 loss: 0.745341  [   64/  118]
train() client id: f_00009-0-2 loss: 0.752632  [   96/  118]
train() client id: f_00009-1-0 loss: 0.786604  [   32/  118]
train() client id: f_00009-1-1 loss: 0.769927  [   64/  118]
train() client id: f_00009-1-2 loss: 0.600332  [   96/  118]
train() client id: f_00009-2-0 loss: 0.643237  [   32/  118]
train() client id: f_00009-2-1 loss: 0.761836  [   64/  118]
train() client id: f_00009-2-2 loss: 0.646703  [   96/  118]
train() client id: f_00009-3-0 loss: 0.700240  [   32/  118]
train() client id: f_00009-3-1 loss: 0.608539  [   64/  118]
train() client id: f_00009-3-2 loss: 0.746807  [   96/  118]
train() client id: f_00009-4-0 loss: 0.570389  [   32/  118]
train() client id: f_00009-4-1 loss: 0.761498  [   64/  118]
train() client id: f_00009-4-2 loss: 0.698600  [   96/  118]
train() client id: f_00009-5-0 loss: 0.643851  [   32/  118]
train() client id: f_00009-5-1 loss: 0.543111  [   64/  118]
train() client id: f_00009-5-2 loss: 0.811552  [   96/  118]
train() client id: f_00009-6-0 loss: 0.642707  [   32/  118]
train() client id: f_00009-6-1 loss: 0.664198  [   64/  118]
train() client id: f_00009-6-2 loss: 0.512524  [   96/  118]
train() client id: f_00009-7-0 loss: 0.548784  [   32/  118]
train() client id: f_00009-7-1 loss: 0.679526  [   64/  118]
train() client id: f_00009-7-2 loss: 0.524499  [   96/  118]
train() client id: f_00009-8-0 loss: 0.621504  [   32/  118]
train() client id: f_00009-8-1 loss: 0.678892  [   64/  118]
train() client id: f_00009-8-2 loss: 0.605022  [   96/  118]
train() client id: f_00009-9-0 loss: 0.508293  [   32/  118]
train() client id: f_00009-9-1 loss: 0.731400  [   64/  118]
train() client id: f_00009-9-2 loss: 0.511523  [   96/  118]
At round 62 accuracy: 0.6445623342175066
At round 62 training accuracy: 0.5861837692823608
At round 62 training loss: 0.8269608270565141
update_location
xs = [  -3.9056584     4.20031788  330.00902392   18.81129433    0.97929623
    3.95640986 -292.44319194 -271.32485185  314.66397685 -257.06087855]
ys = [ 322.5879595   305.55583871    1.32061395 -292.45517586  284.35018685
  267.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [337.75471245 321.53073452 344.82995793 309.65124686 301.42327014
 285.90220796 309.07913397 289.16751458 330.63890923 275.85559109]
dists_bs = [225.95459613 221.71508958 534.03992676 506.04066956 207.18633175
 201.59331142 212.94022257 199.03851774 514.34942217 189.66939928]
uav_gains = [1.42435425e-12 1.82215782e-12 1.29334638e-12 2.23367367e-12
 2.60101956e-12 3.53945382e-12 2.25679855e-12 3.31121347e-12
 1.57981843e-12 4.36146341e-12]
bs_gains = [2.83161877e-11 2.98584584e-11 2.54734430e-12 2.96193078e-12
 3.60979885e-11 3.89727411e-11 3.34327819e-11 4.03896532e-11
 2.82989921e-12 4.62276411e-11]
Round 63
-------------------------------
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.68727091 5.38361967 2.65494812 0.98808132 6.20602153 2.98709652
 1.20875503 3.70447299 2.72587058 2.42213493]
obj_prev = 30.968271608412685
eta_min = 2.604478585360512e-35	eta_max = 0.9403513878285256
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 7.091465885945381	eta = 0.9090909090909091
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 17.42929397319813	eta = 0.36988229006606876
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 11.714987188776163	eta = 0.5503025368408222
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 10.719694765497792	eta = 0.601396523881516
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 10.657792616593055	eta = 0.6048895302207599
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 10.657519519314839	eta = 0.6049050304207853
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 10.657519513957864	eta = 0.6049050307248393
eta = 0.6049050307248393
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [0.04148026 0.08724024 0.04082185 0.01415596 0.10073778 0.04806443
 0.01777726 0.05892831 0.0427971  0.0388466 ]
ene_total = [1.07662235 1.59233833 1.08542302 0.54531731 1.81230098 0.92997575
 0.60440513 1.2459445  0.99375704 0.77143509]
ti_comp = [1.37728495 1.52626582 1.36543421 1.42198973 1.52960948 1.53088406
 1.42284286 1.45043831 1.44004221 1.53358086]
ti_coms = [0.22947828 0.08049741 0.24132901 0.1847735  0.07715375 0.07587917
 0.18392037 0.15632492 0.16672102 0.07318237]
t_total = [26.84973564 26.84973564 26.84973564 26.84973564 26.84973564 26.84973564
 26.84973564 26.84973564 26.84973564 26.84973564]
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [2.35156460e-06 1.78143569e-05 2.28042807e-06 8.76809665e-08
 2.73083858e-05 2.96119390e-06 1.73444402e-07 6.07929826e-06
 2.36250370e-06 1.55784967e-06]
ene_total = [0.38350781 0.13481247 0.40330974 0.30876628 0.12938372 0.12684698
 0.3073421  0.26132755 0.2786378  0.12231706]
optimize_network iter = 0 obj = 2.4562515021459133
eta = 0.6049050307248393
freqs = [15058707.65852771 28579633.50514324 14948302.02718542  4977519.32111269
 32929248.53672925 15698259.84497274  6247090.26876788 20313966.31628257
 14859667.46433555 12665323.49976712]
eta_min = 0.6049050307248401	eta_max = 0.735169781083726
af = 0.0008271317221307269	bf = 0.9700368260492027	zeta = 0.0009098448943437997	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [4.45862869e-07 3.37764920e-06 4.32375195e-07 1.66245432e-08
 5.17774221e-06 5.61450199e-07 3.28855174e-08 1.15265103e-06
 4.47936952e-07 2.95372418e-07]
ene_total = [1.66660366 0.58485258 1.75266791 1.34190688 0.5607002  0.55110835
 1.3357123  1.1353834  1.21083327 0.53150371]
ti_comp = [0.84752723 0.9965081  0.83567649 0.89223201 0.99985176 1.00112634
 0.89308514 0.92068059 0.91028449 1.00382314]
ti_coms = [0.22947828 0.08049741 0.24132901 0.1847735  0.07715375 0.07587917
 0.18392037 0.15632492 0.16672102 0.07318237]
t_total = [26.84973564 26.84973564 26.84973564 26.84973564 26.84973564 26.84973564
 26.84973564 26.84973564 26.84973564 26.84973564]
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [1.42363747e-06 9.58012233e-06 1.39567334e-06 5.10557738e-08
 1.46517044e-05 1.58736166e-06 1.00923117e-07 3.45888573e-06
 1.35541058e-06 8.33540515e-07]
ene_total = [0.57212459 0.20091879 0.60166777 0.4606414  0.19270948 0.18920625
 0.4585158  0.38980408 0.41566912 0.18246434]
optimize_network iter = 1 obj = 3.6637216068534983
eta = 0.735169781083726
freqs = [14977014.44528698 26790042.71364152 14948302.02718542  4855109.38680831
 30831466.09028835 14691709.46586062  6091289.81349057 19586300.25463695
 14387143.5153923  11842221.20609098]
eta_min = 0.7351697810837274	eta_max = 0.735169781083714
af = 0.0007390421746511915	bf = 0.9700368260492027	zeta = 0.0008129463921163107	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [4.41038396e-07 2.96789166e-06 4.32375195e-07 1.58169176e-08
 4.53905178e-06 4.91759633e-07 3.12656630e-08 1.07155188e-06
 4.19901921e-07 2.58228221e-07]
ene_total = [1.66660331 0.58482282 1.75266791 1.34190682 0.56065381 0.55110329
 1.33571218 1.13537751 1.21083123 0.53150101]
ti_comp = [0.84752723 0.9965081  0.83567649 0.89223201 0.99985176 1.00112634
 0.89308514 0.92068059 0.91028449 1.00382314]
ti_coms = [0.22947828 0.08049741 0.24132901 0.1847735  0.07715375 0.07587917
 0.18392037 0.15632492 0.16672102 0.07318237]
t_total = [26.84973564 26.84973564 26.84973564 26.84973564 26.84973564 26.84973564
 26.84973564 26.84973564 26.84973564 26.84973564]
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [1.42363747e-06 9.58012233e-06 1.39567334e-06 5.10557738e-08
 1.46517044e-05 1.58736166e-06 1.00923117e-07 3.45888573e-06
 1.35541058e-06 8.33540515e-07]
ene_total = [0.57212459 0.20091879 0.60166777 0.4606414  0.19270948 0.18920625
 0.4585158  0.38980408 0.41566912 0.18246434]
optimize_network iter = 2 obj = 3.6637216068533323
eta = 0.735169781083714
freqs = [14977014.44528692 26790042.71364162 14948302.02718534  4855109.38680831
 30831466.09028849 14691709.46586069  6091289.81349056 19586300.25463695
 14387143.51539229 11842221.20609103]
Done!
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [1.41315906e-06 9.50960965e-06 1.38540075e-06 5.06799874e-08
 1.45438633e-05 1.57567819e-06 1.00180291e-07 3.43342726e-06
 1.34543434e-06 8.27405398e-07]
ene_total = [0.02294924 0.00805925 0.02413429 0.0184774  0.00772992 0.00758949
 0.01839214 0.01563593 0.01667345 0.00731906]
At round 63 energy consumption: 0.14696016345683638
At round 63 eta: 0.735169781083714
At round 63 a_n: 6.259668650218071
At round 63 local rounds: 10.074148866251232
At round 63 global rounds: 24.929989199134315
gradient difference: 0.5269817113876343
train() client id: f_00000-0-0 loss: 1.209895  [   32/  126]
train() client id: f_00000-0-1 loss: 0.986508  [   64/  126]
train() client id: f_00000-0-2 loss: 1.179257  [   96/  126]
train() client id: f_00000-1-0 loss: 1.187494  [   32/  126]
train() client id: f_00000-1-1 loss: 1.071324  [   64/  126]
train() client id: f_00000-1-2 loss: 0.815617  [   96/  126]
train() client id: f_00000-2-0 loss: 0.998335  [   32/  126]
train() client id: f_00000-2-1 loss: 0.991642  [   64/  126]
train() client id: f_00000-2-2 loss: 0.921526  [   96/  126]
train() client id: f_00000-3-0 loss: 1.024781  [   32/  126]
train() client id: f_00000-3-1 loss: 0.924122  [   64/  126]
train() client id: f_00000-3-2 loss: 0.987651  [   96/  126]
train() client id: f_00000-4-0 loss: 0.916465  [   32/  126]
train() client id: f_00000-4-1 loss: 0.921968  [   64/  126]
train() client id: f_00000-4-2 loss: 1.005864  [   96/  126]
train() client id: f_00000-5-0 loss: 1.046919  [   32/  126]
train() client id: f_00000-5-1 loss: 0.886979  [   64/  126]
train() client id: f_00000-5-2 loss: 0.823058  [   96/  126]
train() client id: f_00000-6-0 loss: 0.846871  [   32/  126]
train() client id: f_00000-6-1 loss: 0.949665  [   64/  126]
train() client id: f_00000-6-2 loss: 1.008980  [   96/  126]
train() client id: f_00000-7-0 loss: 0.956064  [   32/  126]
train() client id: f_00000-7-1 loss: 0.819969  [   64/  126]
train() client id: f_00000-7-2 loss: 0.785804  [   96/  126]
train() client id: f_00000-8-0 loss: 0.838519  [   32/  126]
train() client id: f_00000-8-1 loss: 0.854488  [   64/  126]
train() client id: f_00000-8-2 loss: 0.871232  [   96/  126]
train() client id: f_00000-9-0 loss: 0.780036  [   32/  126]
train() client id: f_00000-9-1 loss: 0.934373  [   64/  126]
train() client id: f_00000-9-2 loss: 0.975663  [   96/  126]
train() client id: f_00001-0-0 loss: 0.385774  [   32/  265]
train() client id: f_00001-0-1 loss: 0.492346  [   64/  265]
train() client id: f_00001-0-2 loss: 0.421847  [   96/  265]
train() client id: f_00001-0-3 loss: 0.442647  [  128/  265]
train() client id: f_00001-0-4 loss: 0.385284  [  160/  265]
train() client id: f_00001-0-5 loss: 0.378961  [  192/  265]
train() client id: f_00001-0-6 loss: 0.404230  [  224/  265]
train() client id: f_00001-0-7 loss: 0.557899  [  256/  265]
train() client id: f_00001-1-0 loss: 0.485272  [   32/  265]
train() client id: f_00001-1-1 loss: 0.304847  [   64/  265]
train() client id: f_00001-1-2 loss: 0.413789  [   96/  265]
train() client id: f_00001-1-3 loss: 0.432595  [  128/  265]
train() client id: f_00001-1-4 loss: 0.446700  [  160/  265]
train() client id: f_00001-1-5 loss: 0.489328  [  192/  265]
train() client id: f_00001-1-6 loss: 0.417587  [  224/  265]
train() client id: f_00001-1-7 loss: 0.419219  [  256/  265]
train() client id: f_00001-2-0 loss: 0.407665  [   32/  265]
train() client id: f_00001-2-1 loss: 0.430005  [   64/  265]
train() client id: f_00001-2-2 loss: 0.538795  [   96/  265]
train() client id: f_00001-2-3 loss: 0.379043  [  128/  265]
train() client id: f_00001-2-4 loss: 0.349853  [  160/  265]
train() client id: f_00001-2-5 loss: 0.391638  [  192/  265]
train() client id: f_00001-2-6 loss: 0.388143  [  224/  265]
train() client id: f_00001-2-7 loss: 0.477367  [  256/  265]
train() client id: f_00001-3-0 loss: 0.324901  [   32/  265]
train() client id: f_00001-3-1 loss: 0.536975  [   64/  265]
train() client id: f_00001-3-2 loss: 0.353986  [   96/  265]
train() client id: f_00001-3-3 loss: 0.395733  [  128/  265]
train() client id: f_00001-3-4 loss: 0.452031  [  160/  265]
train() client id: f_00001-3-5 loss: 0.439823  [  192/  265]
train() client id: f_00001-3-6 loss: 0.370402  [  224/  265]
train() client id: f_00001-3-7 loss: 0.373545  [  256/  265]
train() client id: f_00001-4-0 loss: 0.458708  [   32/  265]
train() client id: f_00001-4-1 loss: 0.476774  [   64/  265]
train() client id: f_00001-4-2 loss: 0.390941  [   96/  265]
train() client id: f_00001-4-3 loss: 0.379200  [  128/  265]
train() client id: f_00001-4-4 loss: 0.323861  [  160/  265]
train() client id: f_00001-4-5 loss: 0.313597  [  192/  265]
train() client id: f_00001-4-6 loss: 0.479447  [  224/  265]
train() client id: f_00001-4-7 loss: 0.401755  [  256/  265]
train() client id: f_00001-5-0 loss: 0.317298  [   32/  265]
train() client id: f_00001-5-1 loss: 0.418543  [   64/  265]
train() client id: f_00001-5-2 loss: 0.347862  [   96/  265]
train() client id: f_00001-5-3 loss: 0.368392  [  128/  265]
train() client id: f_00001-5-4 loss: 0.393420  [  160/  265]
train() client id: f_00001-5-5 loss: 0.510501  [  192/  265]
train() client id: f_00001-5-6 loss: 0.312706  [  224/  265]
train() client id: f_00001-5-7 loss: 0.643392  [  256/  265]
train() client id: f_00001-6-0 loss: 0.365011  [   32/  265]
train() client id: f_00001-6-1 loss: 0.377596  [   64/  265]
train() client id: f_00001-6-2 loss: 0.480837  [   96/  265]
train() client id: f_00001-6-3 loss: 0.402046  [  128/  265]
train() client id: f_00001-6-4 loss: 0.348388  [  160/  265]
train() client id: f_00001-6-5 loss: 0.545186  [  192/  265]
train() client id: f_00001-6-6 loss: 0.362883  [  224/  265]
train() client id: f_00001-6-7 loss: 0.357558  [  256/  265]
train() client id: f_00001-7-0 loss: 0.315100  [   32/  265]
train() client id: f_00001-7-1 loss: 0.616524  [   64/  265]
train() client id: f_00001-7-2 loss: 0.507533  [   96/  265]
train() client id: f_00001-7-3 loss: 0.394194  [  128/  265]
train() client id: f_00001-7-4 loss: 0.371487  [  160/  265]
train() client id: f_00001-7-5 loss: 0.402281  [  192/  265]
train() client id: f_00001-7-6 loss: 0.322608  [  224/  265]
train() client id: f_00001-7-7 loss: 0.368055  [  256/  265]
train() client id: f_00001-8-0 loss: 0.425206  [   32/  265]
train() client id: f_00001-8-1 loss: 0.476740  [   64/  265]
train() client id: f_00001-8-2 loss: 0.395219  [   96/  265]
train() client id: f_00001-8-3 loss: 0.438289  [  128/  265]
train() client id: f_00001-8-4 loss: 0.391979  [  160/  265]
train() client id: f_00001-8-5 loss: 0.390704  [  192/  265]
train() client id: f_00001-8-6 loss: 0.363630  [  224/  265]
train() client id: f_00001-8-7 loss: 0.432076  [  256/  265]
train() client id: f_00001-9-0 loss: 0.325431  [   32/  265]
train() client id: f_00001-9-1 loss: 0.350967  [   64/  265]
train() client id: f_00001-9-2 loss: 0.374538  [   96/  265]
train() client id: f_00001-9-3 loss: 0.426128  [  128/  265]
train() client id: f_00001-9-4 loss: 0.487709  [  160/  265]
train() client id: f_00001-9-5 loss: 0.475687  [  192/  265]
train() client id: f_00001-9-6 loss: 0.371796  [  224/  265]
train() client id: f_00001-9-7 loss: 0.402303  [  256/  265]
train() client id: f_00002-0-0 loss: 0.969506  [   32/  124]
train() client id: f_00002-0-1 loss: 0.904236  [   64/  124]
train() client id: f_00002-0-2 loss: 1.212497  [   96/  124]
train() client id: f_00002-1-0 loss: 1.116589  [   32/  124]
train() client id: f_00002-1-1 loss: 0.932798  [   64/  124]
train() client id: f_00002-1-2 loss: 0.861827  [   96/  124]
train() client id: f_00002-2-0 loss: 0.754014  [   32/  124]
train() client id: f_00002-2-1 loss: 1.097246  [   64/  124]
train() client id: f_00002-2-2 loss: 1.102917  [   96/  124]
train() client id: f_00002-3-0 loss: 1.023947  [   32/  124]
train() client id: f_00002-3-1 loss: 0.992278  [   64/  124]
train() client id: f_00002-3-2 loss: 0.885524  [   96/  124]
train() client id: f_00002-4-0 loss: 0.888422  [   32/  124]
train() client id: f_00002-4-1 loss: 1.035244  [   64/  124]
train() client id: f_00002-4-2 loss: 0.803691  [   96/  124]
train() client id: f_00002-5-0 loss: 0.821648  [   32/  124]
train() client id: f_00002-5-1 loss: 1.021457  [   64/  124]
train() client id: f_00002-5-2 loss: 0.869415  [   96/  124]
train() client id: f_00002-6-0 loss: 0.689647  [   32/  124]
train() client id: f_00002-6-1 loss: 0.943174  [   64/  124]
train() client id: f_00002-6-2 loss: 0.829150  [   96/  124]
train() client id: f_00002-7-0 loss: 0.925403  [   32/  124]
train() client id: f_00002-7-1 loss: 0.858782  [   64/  124]
train() client id: f_00002-7-2 loss: 0.829771  [   96/  124]
train() client id: f_00002-8-0 loss: 0.855837  [   32/  124]
train() client id: f_00002-8-1 loss: 0.748405  [   64/  124]
train() client id: f_00002-8-2 loss: 0.969163  [   96/  124]
train() client id: f_00002-9-0 loss: 0.875796  [   32/  124]
train() client id: f_00002-9-1 loss: 0.969873  [   64/  124]
train() client id: f_00002-9-2 loss: 0.788830  [   96/  124]
train() client id: f_00003-0-0 loss: 0.833156  [   32/   43]
train() client id: f_00003-1-0 loss: 0.610090  [   32/   43]
train() client id: f_00003-2-0 loss: 0.719158  [   32/   43]
train() client id: f_00003-3-0 loss: 0.792982  [   32/   43]
train() client id: f_00003-4-0 loss: 0.551602  [   32/   43]
train() client id: f_00003-5-0 loss: 0.751724  [   32/   43]
train() client id: f_00003-6-0 loss: 0.629580  [   32/   43]
train() client id: f_00003-7-0 loss: 0.724385  [   32/   43]
train() client id: f_00003-8-0 loss: 0.582307  [   32/   43]
train() client id: f_00003-9-0 loss: 0.546250  [   32/   43]
train() client id: f_00004-0-0 loss: 0.975889  [   32/  306]
train() client id: f_00004-0-1 loss: 0.969661  [   64/  306]
train() client id: f_00004-0-2 loss: 0.992234  [   96/  306]
train() client id: f_00004-0-3 loss: 0.912987  [  128/  306]
train() client id: f_00004-0-4 loss: 0.988881  [  160/  306]
train() client id: f_00004-0-5 loss: 1.003360  [  192/  306]
train() client id: f_00004-0-6 loss: 1.078920  [  224/  306]
train() client id: f_00004-0-7 loss: 0.984114  [  256/  306]
train() client id: f_00004-0-8 loss: 1.059063  [  288/  306]
train() client id: f_00004-1-0 loss: 0.986751  [   32/  306]
train() client id: f_00004-1-1 loss: 0.959341  [   64/  306]
train() client id: f_00004-1-2 loss: 0.891234  [   96/  306]
train() client id: f_00004-1-3 loss: 1.001176  [  128/  306]
train() client id: f_00004-1-4 loss: 1.025408  [  160/  306]
train() client id: f_00004-1-5 loss: 1.096849  [  192/  306]
train() client id: f_00004-1-6 loss: 0.899323  [  224/  306]
train() client id: f_00004-1-7 loss: 0.869949  [  256/  306]
train() client id: f_00004-1-8 loss: 1.066810  [  288/  306]
train() client id: f_00004-2-0 loss: 1.049305  [   32/  306]
train() client id: f_00004-2-1 loss: 1.027537  [   64/  306]
train() client id: f_00004-2-2 loss: 0.935756  [   96/  306]
train() client id: f_00004-2-3 loss: 0.964314  [  128/  306]
train() client id: f_00004-2-4 loss: 0.943350  [  160/  306]
train() client id: f_00004-2-5 loss: 1.136644  [  192/  306]
train() client id: f_00004-2-6 loss: 0.906126  [  224/  306]
train() client id: f_00004-2-7 loss: 0.861173  [  256/  306]
train() client id: f_00004-2-8 loss: 0.908045  [  288/  306]
train() client id: f_00004-3-0 loss: 0.918361  [   32/  306]
train() client id: f_00004-3-1 loss: 1.040832  [   64/  306]
train() client id: f_00004-3-2 loss: 1.020317  [   96/  306]
train() client id: f_00004-3-3 loss: 0.902738  [  128/  306]
train() client id: f_00004-3-4 loss: 1.008184  [  160/  306]
train() client id: f_00004-3-5 loss: 1.003257  [  192/  306]
train() client id: f_00004-3-6 loss: 0.817206  [  224/  306]
train() client id: f_00004-3-7 loss: 0.899003  [  256/  306]
train() client id: f_00004-3-8 loss: 1.094516  [  288/  306]
train() client id: f_00004-4-0 loss: 1.014446  [   32/  306]
train() client id: f_00004-4-1 loss: 0.809907  [   64/  306]
train() client id: f_00004-4-2 loss: 1.028457  [   96/  306]
train() client id: f_00004-4-3 loss: 0.801874  [  128/  306]
train() client id: f_00004-4-4 loss: 1.125100  [  160/  306]
train() client id: f_00004-4-5 loss: 0.937662  [  192/  306]
train() client id: f_00004-4-6 loss: 1.033243  [  224/  306]
train() client id: f_00004-4-7 loss: 0.911230  [  256/  306]
train() client id: f_00004-4-8 loss: 0.984038  [  288/  306]
train() client id: f_00004-5-0 loss: 0.986847  [   32/  306]
train() client id: f_00004-5-1 loss: 1.018968  [   64/  306]
train() client id: f_00004-5-2 loss: 0.927831  [   96/  306]
train() client id: f_00004-5-3 loss: 1.071116  [  128/  306]
train() client id: f_00004-5-4 loss: 1.027715  [  160/  306]
train() client id: f_00004-5-5 loss: 0.884387  [  192/  306]
train() client id: f_00004-5-6 loss: 0.891875  [  224/  306]
train() client id: f_00004-5-7 loss: 0.934016  [  256/  306]
train() client id: f_00004-5-8 loss: 0.990133  [  288/  306]
train() client id: f_00004-6-0 loss: 0.959900  [   32/  306]
train() client id: f_00004-6-1 loss: 1.040584  [   64/  306]
train() client id: f_00004-6-2 loss: 0.920862  [   96/  306]
train() client id: f_00004-6-3 loss: 0.844263  [  128/  306]
train() client id: f_00004-6-4 loss: 0.827130  [  160/  306]
train() client id: f_00004-6-5 loss: 0.924057  [  192/  306]
train() client id: f_00004-6-6 loss: 1.148841  [  224/  306]
train() client id: f_00004-6-7 loss: 0.994743  [  256/  306]
train() client id: f_00004-6-8 loss: 1.023615  [  288/  306]
train() client id: f_00004-7-0 loss: 1.030818  [   32/  306]
train() client id: f_00004-7-1 loss: 1.063353  [   64/  306]
train() client id: f_00004-7-2 loss: 1.135562  [   96/  306]
train() client id: f_00004-7-3 loss: 0.904108  [  128/  306]
train() client id: f_00004-7-4 loss: 0.868675  [  160/  306]
train() client id: f_00004-7-5 loss: 0.851967  [  192/  306]
train() client id: f_00004-7-6 loss: 0.866625  [  224/  306]
train() client id: f_00004-7-7 loss: 1.023608  [  256/  306]
train() client id: f_00004-7-8 loss: 0.936312  [  288/  306]
train() client id: f_00004-8-0 loss: 1.034153  [   32/  306]
train() client id: f_00004-8-1 loss: 0.964211  [   64/  306]
train() client id: f_00004-8-2 loss: 0.849886  [   96/  306]
train() client id: f_00004-8-3 loss: 0.977508  [  128/  306]
train() client id: f_00004-8-4 loss: 1.016129  [  160/  306]
train() client id: f_00004-8-5 loss: 0.967558  [  192/  306]
train() client id: f_00004-8-6 loss: 0.889361  [  224/  306]
train() client id: f_00004-8-7 loss: 1.046824  [  256/  306]
train() client id: f_00004-8-8 loss: 0.917905  [  288/  306]
train() client id: f_00004-9-0 loss: 0.953025  [   32/  306]
train() client id: f_00004-9-1 loss: 0.792001  [   64/  306]
train() client id: f_00004-9-2 loss: 0.838101  [   96/  306]
train() client id: f_00004-9-3 loss: 0.984390  [  128/  306]
train() client id: f_00004-9-4 loss: 0.981735  [  160/  306]
train() client id: f_00004-9-5 loss: 1.067692  [  192/  306]
train() client id: f_00004-9-6 loss: 0.918239  [  224/  306]
train() client id: f_00004-9-7 loss: 0.961183  [  256/  306]
train() client id: f_00004-9-8 loss: 1.038620  [  288/  306]
train() client id: f_00005-0-0 loss: 0.459872  [   32/  146]
train() client id: f_00005-0-1 loss: 0.573370  [   64/  146]
train() client id: f_00005-0-2 loss: 0.576109  [   96/  146]
train() client id: f_00005-0-3 loss: 0.789225  [  128/  146]
train() client id: f_00005-1-0 loss: 0.626266  [   32/  146]
train() client id: f_00005-1-1 loss: 0.396931  [   64/  146]
train() client id: f_00005-1-2 loss: 0.314519  [   96/  146]
train() client id: f_00005-1-3 loss: 0.961133  [  128/  146]
train() client id: f_00005-2-0 loss: 0.729440  [   32/  146]
train() client id: f_00005-2-1 loss: 0.625826  [   64/  146]
train() client id: f_00005-2-2 loss: 0.659358  [   96/  146]
train() client id: f_00005-2-3 loss: 0.555316  [  128/  146]
train() client id: f_00005-3-0 loss: 0.723262  [   32/  146]
train() client id: f_00005-3-1 loss: 0.645609  [   64/  146]
train() client id: f_00005-3-2 loss: 0.667589  [   96/  146]
train() client id: f_00005-3-3 loss: 0.330918  [  128/  146]
train() client id: f_00005-4-0 loss: 0.667502  [   32/  146]
train() client id: f_00005-4-1 loss: 0.494082  [   64/  146]
train() client id: f_00005-4-2 loss: 0.514713  [   96/  146]
train() client id: f_00005-4-3 loss: 0.588582  [  128/  146]
train() client id: f_00005-5-0 loss: 0.601906  [   32/  146]
train() client id: f_00005-5-1 loss: 0.583795  [   64/  146]
train() client id: f_00005-5-2 loss: 0.319086  [   96/  146]
train() client id: f_00005-5-3 loss: 0.651783  [  128/  146]
train() client id: f_00005-6-0 loss: 0.595086  [   32/  146]
train() client id: f_00005-6-1 loss: 0.585069  [   64/  146]
train() client id: f_00005-6-2 loss: 0.677505  [   96/  146]
train() client id: f_00005-6-3 loss: 0.651126  [  128/  146]
train() client id: f_00005-7-0 loss: 0.774668  [   32/  146]
train() client id: f_00005-7-1 loss: 0.647749  [   64/  146]
train() client id: f_00005-7-2 loss: 0.418641  [   96/  146]
train() client id: f_00005-7-3 loss: 0.452202  [  128/  146]
train() client id: f_00005-8-0 loss: 0.844451  [   32/  146]
train() client id: f_00005-8-1 loss: 0.677603  [   64/  146]
train() client id: f_00005-8-2 loss: 0.552491  [   96/  146]
train() client id: f_00005-8-3 loss: 0.342563  [  128/  146]
train() client id: f_00005-9-0 loss: 0.549927  [   32/  146]
train() client id: f_00005-9-1 loss: 0.595089  [   64/  146]
train() client id: f_00005-9-2 loss: 0.599092  [   96/  146]
train() client id: f_00005-9-3 loss: 0.389081  [  128/  146]
train() client id: f_00006-0-0 loss: 0.508441  [   32/   54]
train() client id: f_00006-1-0 loss: 0.450013  [   32/   54]
train() client id: f_00006-2-0 loss: 0.504165  [   32/   54]
train() client id: f_00006-3-0 loss: 0.538307  [   32/   54]
train() client id: f_00006-4-0 loss: 0.437284  [   32/   54]
train() client id: f_00006-5-0 loss: 0.442773  [   32/   54]
train() client id: f_00006-6-0 loss: 0.430505  [   32/   54]
train() client id: f_00006-7-0 loss: 0.501647  [   32/   54]
train() client id: f_00006-8-0 loss: 0.481741  [   32/   54]
train() client id: f_00006-9-0 loss: 0.507808  [   32/   54]
train() client id: f_00007-0-0 loss: 0.426591  [   32/  179]
train() client id: f_00007-0-1 loss: 0.649682  [   64/  179]
train() client id: f_00007-0-2 loss: 0.536559  [   96/  179]
train() client id: f_00007-0-3 loss: 0.443036  [  128/  179]
train() client id: f_00007-0-4 loss: 0.685228  [  160/  179]
train() client id: f_00007-1-0 loss: 0.786858  [   32/  179]
train() client id: f_00007-1-1 loss: 0.387091  [   64/  179]
train() client id: f_00007-1-2 loss: 0.441732  [   96/  179]
train() client id: f_00007-1-3 loss: 0.479452  [  128/  179]
train() client id: f_00007-1-4 loss: 0.372225  [  160/  179]
train() client id: f_00007-2-0 loss: 0.461571  [   32/  179]
train() client id: f_00007-2-1 loss: 0.431678  [   64/  179]
train() client id: f_00007-2-2 loss: 0.475211  [   96/  179]
train() client id: f_00007-2-3 loss: 0.456760  [  128/  179]
train() client id: f_00007-2-4 loss: 0.507117  [  160/  179]
train() client id: f_00007-3-0 loss: 0.529187  [   32/  179]
train() client id: f_00007-3-1 loss: 0.760833  [   64/  179]
train() client id: f_00007-3-2 loss: 0.452129  [   96/  179]
train() client id: f_00007-3-3 loss: 0.537854  [  128/  179]
train() client id: f_00007-3-4 loss: 0.355764  [  160/  179]
train() client id: f_00007-4-0 loss: 0.315786  [   32/  179]
train() client id: f_00007-4-1 loss: 0.510823  [   64/  179]
train() client id: f_00007-4-2 loss: 0.393122  [   96/  179]
train() client id: f_00007-4-3 loss: 0.440851  [  128/  179]
train() client id: f_00007-4-4 loss: 0.702017  [  160/  179]
train() client id: f_00007-5-0 loss: 0.292572  [   32/  179]
train() client id: f_00007-5-1 loss: 0.642458  [   64/  179]
train() client id: f_00007-5-2 loss: 0.442274  [   96/  179]
train() client id: f_00007-5-3 loss: 0.414852  [  128/  179]
train() client id: f_00007-5-4 loss: 0.650034  [  160/  179]
train() client id: f_00007-6-0 loss: 0.515279  [   32/  179]
train() client id: f_00007-6-1 loss: 0.502929  [   64/  179]
train() client id: f_00007-6-2 loss: 0.397979  [   96/  179]
train() client id: f_00007-6-3 loss: 0.587455  [  128/  179]
train() client id: f_00007-6-4 loss: 0.331619  [  160/  179]
train() client id: f_00007-7-0 loss: 0.329091  [   32/  179]
train() client id: f_00007-7-1 loss: 0.595469  [   64/  179]
train() client id: f_00007-7-2 loss: 0.383603  [   96/  179]
train() client id: f_00007-7-3 loss: 0.762099  [  128/  179]
train() client id: f_00007-7-4 loss: 0.451654  [  160/  179]
train() client id: f_00007-8-0 loss: 0.613278  [   32/  179]
train() client id: f_00007-8-1 loss: 0.349177  [   64/  179]
train() client id: f_00007-8-2 loss: 0.644463  [   96/  179]
train() client id: f_00007-8-3 loss: 0.520085  [  128/  179]
train() client id: f_00007-8-4 loss: 0.310582  [  160/  179]
train() client id: f_00007-9-0 loss: 0.529546  [   32/  179]
train() client id: f_00007-9-1 loss: 0.499902  [   64/  179]
train() client id: f_00007-9-2 loss: 0.343721  [   96/  179]
train() client id: f_00007-9-3 loss: 0.347198  [  128/  179]
train() client id: f_00007-9-4 loss: 0.685831  [  160/  179]
train() client id: f_00008-0-0 loss: 0.840814  [   32/  130]
train() client id: f_00008-0-1 loss: 0.810716  [   64/  130]
train() client id: f_00008-0-2 loss: 0.754505  [   96/  130]
train() client id: f_00008-0-3 loss: 0.831347  [  128/  130]
train() client id: f_00008-1-0 loss: 0.803342  [   32/  130]
train() client id: f_00008-1-1 loss: 0.787697  [   64/  130]
train() client id: f_00008-1-2 loss: 0.864384  [   96/  130]
train() client id: f_00008-1-3 loss: 0.786196  [  128/  130]
train() client id: f_00008-2-0 loss: 0.761089  [   32/  130]
train() client id: f_00008-2-1 loss: 0.704711  [   64/  130]
train() client id: f_00008-2-2 loss: 0.840747  [   96/  130]
train() client id: f_00008-2-3 loss: 0.927577  [  128/  130]
train() client id: f_00008-3-0 loss: 0.923711  [   32/  130]
train() client id: f_00008-3-1 loss: 0.931928  [   64/  130]
train() client id: f_00008-3-2 loss: 0.642788  [   96/  130]
train() client id: f_00008-3-3 loss: 0.732835  [  128/  130]
train() client id: f_00008-4-0 loss: 0.861172  [   32/  130]
train() client id: f_00008-4-1 loss: 0.869514  [   64/  130]
train() client id: f_00008-4-2 loss: 0.782190  [   96/  130]
train() client id: f_00008-4-3 loss: 0.702926  [  128/  130]
train() client id: f_00008-5-0 loss: 0.816293  [   32/  130]
train() client id: f_00008-5-1 loss: 0.927316  [   64/  130]
train() client id: f_00008-5-2 loss: 0.832398  [   96/  130]
train() client id: f_00008-5-3 loss: 0.643745  [  128/  130]
train() client id: f_00008-6-0 loss: 0.725401  [   32/  130]
train() client id: f_00008-6-1 loss: 0.873273  [   64/  130]
train() client id: f_00008-6-2 loss: 0.778691  [   96/  130]
train() client id: f_00008-6-3 loss: 0.814008  [  128/  130]
train() client id: f_00008-7-0 loss: 0.807399  [   32/  130]
train() client id: f_00008-7-1 loss: 0.779745  [   64/  130]
train() client id: f_00008-7-2 loss: 0.806081  [   96/  130]
train() client id: f_00008-7-3 loss: 0.761714  [  128/  130]
train() client id: f_00008-8-0 loss: 0.738119  [   32/  130]
train() client id: f_00008-8-1 loss: 0.863774  [   64/  130]
train() client id: f_00008-8-2 loss: 0.754857  [   96/  130]
train() client id: f_00008-8-3 loss: 0.853773  [  128/  130]
train() client id: f_00008-9-0 loss: 0.745994  [   32/  130]
train() client id: f_00008-9-1 loss: 0.861255  [   64/  130]
train() client id: f_00008-9-2 loss: 0.803197  [   96/  130]
train() client id: f_00008-9-3 loss: 0.798066  [  128/  130]
train() client id: f_00009-0-0 loss: 1.144466  [   32/  118]
train() client id: f_00009-0-1 loss: 1.070444  [   64/  118]
train() client id: f_00009-0-2 loss: 1.173813  [   96/  118]
train() client id: f_00009-1-0 loss: 1.020341  [   32/  118]
train() client id: f_00009-1-1 loss: 1.135923  [   64/  118]
train() client id: f_00009-1-2 loss: 1.028009  [   96/  118]
train() client id: f_00009-2-0 loss: 1.102740  [   32/  118]
train() client id: f_00009-2-1 loss: 1.020164  [   64/  118]
train() client id: f_00009-2-2 loss: 1.039505  [   96/  118]
train() client id: f_00009-3-0 loss: 1.056635  [   32/  118]
train() client id: f_00009-3-1 loss: 0.984075  [   64/  118]
train() client id: f_00009-3-2 loss: 1.016469  [   96/  118]
train() client id: f_00009-4-0 loss: 1.106347  [   32/  118]
train() client id: f_00009-4-1 loss: 1.023578  [   64/  118]
train() client id: f_00009-4-2 loss: 0.929898  [   96/  118]
train() client id: f_00009-5-0 loss: 1.005895  [   32/  118]
train() client id: f_00009-5-1 loss: 0.907366  [   64/  118]
train() client id: f_00009-5-2 loss: 0.908638  [   96/  118]
train() client id: f_00009-6-0 loss: 1.086954  [   32/  118]
train() client id: f_00009-6-1 loss: 0.877625  [   64/  118]
train() client id: f_00009-6-2 loss: 0.993422  [   96/  118]
train() client id: f_00009-7-0 loss: 1.104667  [   32/  118]
train() client id: f_00009-7-1 loss: 0.926461  [   64/  118]
train() client id: f_00009-7-2 loss: 0.851760  [   96/  118]
train() client id: f_00009-8-0 loss: 1.075434  [   32/  118]
train() client id: f_00009-8-1 loss: 0.897435  [   64/  118]
train() client id: f_00009-8-2 loss: 0.833830  [   96/  118]
train() client id: f_00009-9-0 loss: 0.992254  [   32/  118]
train() client id: f_00009-9-1 loss: 0.979793  [   64/  118]
train() client id: f_00009-9-2 loss: 0.924965  [   96/  118]
At round 63 accuracy: 0.6445623342175066
At round 63 training accuracy: 0.5861837692823608
At round 63 training loss: 0.8283234435234615
update_location
xs = [  -3.9056584     4.20031788  335.00902392   18.81129433    0.97929623
    3.95640986 -297.44319194 -276.32485185  319.66397685 -262.06087855]
ys = [ 327.5879595   310.55583871    1.32061395 -297.45517586  289.35018685
  272.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [342.53339308 326.28602732 349.61806322 314.37787206 306.14455678
 290.59114581 313.81418542 293.86408424 335.40084685 280.5207941 ]
dists_bs = [229.25173742 224.69565936 538.77057605 510.6630112  209.86823946
 203.94363122 215.73819853 201.50578171 519.11171038 191.88561647]
uav_gains = [1.33357588e-12 1.68888635e-12 1.21580973e-12 2.05498399e-12
 2.38086830e-12 3.21721303e-12 2.07518341e-12 3.01317070e-12
 1.47290941e-12 3.95629760e-12]
bs_gains = [2.71905946e-11 2.87626551e-11 2.48521091e-12 2.88747186e-12
 3.48211629e-11 3.77281641e-11 3.22328236e-11 3.90201606e-11
 2.75780638e-12 4.47481726e-11]
Round 64
-------------------------------
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.55289596 5.10473648 2.52232289 0.94134025 5.8844195  2.83245373
 1.15055847 3.5161317  2.58559463 2.29678183]
obj_prev = 29.387235422694697
eta_min = 3.6403482068472025e-37	eta_max = 0.941453835955468
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 6.723535975581212	eta = 0.909090909090909
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 16.80493378903063	eta = 0.36372088751318643
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 11.200979803524296	eta = 0.5456938178232738
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 10.229583039555175	eta = 0.5975126658351408
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 10.168941452928603	eta = 0.601075879986136
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 10.168671162537121	eta = 0.6010918570034193
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 10.168671157124352	eta = 0.6010918573233796
eta = 0.6010918573233796
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [0.04200208 0.08833771 0.04133538 0.01433404 0.10200505 0.04866908
 0.01800089 0.05966962 0.04333548 0.03933528]
ene_total = [1.03172726 1.51359867 1.04004204 0.52610415 1.72269629 0.88362426
 0.58229031 1.19121994 0.94449752 0.7328707 ]
ti_comp = [1.47039382 1.62666926 1.45842809 1.51593046 1.63009164 1.63144497
 1.51679335 1.54542978 1.53931646 1.63417731]
ti_coms = [0.23746513 0.08118968 0.24943086 0.19192848 0.0777673  0.07641398
 0.19106559 0.16242916 0.16854248 0.07368164]
t_total = [26.79973145 26.79973145 26.79973145 26.79973145 26.79973145 26.79973145
 26.79973145 26.79973145 26.79973145 26.79973145]
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [2.14202768e-06 1.62824548e-05 2.07527568e-06 8.00990907e-08
 2.49643934e-05 2.70703886e-06 1.58456196e-07 5.55957146e-06
 2.14661920e-06 1.42438847e-06]
ene_total = [0.37266401 0.1276584  0.3914396  0.30117553 0.12242423 0.11995133
 0.29982271 0.25497116 0.26451064 0.11564362]
optimize_network iter = 0 obj = 2.3702612355707346
eta = 0.6010918573233796
freqs = [14282595.45433634 27152940.78877741 14171209.70092077  4727803.67422734
 31288134.07679844 14915942.00582812  5933864.17259814 19305186.95139418
 14076208.69920771 12035193.86722137]
eta_min = 0.6010918573233806	eta_max = 0.7423534842131878
af = 0.0007076189579367038	bf = 0.9451563237723394	zeta = 0.0007783808537303743	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [4.01088465e-07 3.04884240e-06 3.88589348e-07 1.49983222e-08
 4.67451020e-06 5.06885170e-07 2.96704627e-08 1.04101362e-06
 4.01948215e-07 2.66712607e-07]
ene_total = [1.63512592 0.5592525  1.71751674 1.3215506  0.53579916 0.52619371
 1.31561006 1.11849968 1.16054979 0.50736327]
ti_comp = [0.86560563 1.02188107 0.8536399  0.91114227 1.02530345 1.02665678
 0.91200517 0.9406416  0.93452827 1.02938912]
ti_coms = [0.23746513 0.08118968 0.24943086 0.19192848 0.0777673  0.07641398
 0.19106559 0.16242916 0.16854248 0.07368164]
t_total = [26.79973145 26.79973145 26.79973145 26.79973145 26.79973145 26.79973145
 26.79973145 26.79973145 26.79973145 26.79973145]
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [1.23942807e-06 8.27345194e-06 1.21469007e-06 4.44613668e-08
 1.26534594e-05 1.37074871e-06 8.78894039e-08 3.00926595e-06
 1.16787135e-06 7.19841221e-07]
ene_total = [0.57696514 0.19745596 0.60603597 0.46630224 0.18924751 0.18568541
 0.46420685 0.394704   0.40951193 0.17903122]
optimize_network iter = 1 obj = 3.6691462360745932
eta = 0.7423534842131878
freqs = [14200721.70636506 25299122.79358197 14171209.70092077  4604073.01677737
 29115814.90637482 13873547.37129865  5776388.64214124 18564736.76689159
 13570968.78990086 11183104.37494726]
eta_min = 0.7423534842131893	eta_max = 0.7423534842131851
af = 0.0006257175094953287	bf = 0.9451563237723394	zeta = 0.0006882892604448616	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [3.96503235e-07 2.64674534e-06 3.88589348e-07 1.42235570e-08
 4.04794575e-06 4.38513790e-07 2.81165434e-08 9.62688933e-07
 3.73611652e-07 2.30283129e-07]
ene_total = [1.63512561 0.55922482 1.71751674 1.32155055 0.53575602 0.526189
 1.31560995 1.11849429 1.16054784 0.50736076]
ti_comp = [0.86560563 1.02188107 0.8536399  0.91114227 1.02530345 1.02665678
 0.91200517 0.9406416  0.93452827 1.02938912]
ti_coms = [0.23746513 0.08118968 0.24943086 0.19192848 0.0777673  0.07641398
 0.19106559 0.16242916 0.16854248 0.07368164]
t_total = [26.79973145 26.79973145 26.79973145 26.79973145 26.79973145 26.79973145
 26.79973145 26.79973145 26.79973145 26.79973145]
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [1.23942807e-06 8.27345194e-06 1.21469007e-06 4.44613668e-08
 1.26534594e-05 1.37074871e-06 8.78894039e-08 3.00926595e-06
 1.16787135e-06 7.19841221e-07]
ene_total = [0.57696514 0.19745596 0.60603597 0.46630224 0.18924751 0.18568541
 0.46420685 0.394704   0.40951193 0.17903122]
optimize_network iter = 2 obj = 3.669146236074556
eta = 0.7423534842131851
freqs = [14200721.70636504 25299122.79358199 14171209.70092075  4604073.01677736
 29115814.90637484 13873547.37129866  5776388.64214123 18564736.76689158
 13570968.78990086 11183104.37494727]
Done!
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [1.14341502e-06 7.63254395e-06 1.12059337e-06 4.10171399e-08
 1.16732514e-05 1.26456283e-06 8.10809978e-08 2.77615133e-06
 1.07740148e-06 6.64078283e-07]
ene_total = [0.02374766 0.0081266  0.02494421 0.01919289 0.0077884  0.00764266
 0.01910664 0.01624569 0.01685533 0.00736883]
At round 64 energy consumption: 0.15101890456706119
At round 64 eta: 0.7423534842131851
At round 64 a_n: 5.917122803248752
At round 64 local rounds: 9.755733857653984
At round 64 global rounds: 24.295568799375985
gradient difference: 0.4784475862979889
train() client id: f_00000-0-0 loss: 1.133897  [   32/  126]
train() client id: f_00000-0-1 loss: 1.242487  [   64/  126]
train() client id: f_00000-0-2 loss: 0.832088  [   96/  126]
train() client id: f_00000-1-0 loss: 0.980048  [   32/  126]
train() client id: f_00000-1-1 loss: 0.989327  [   64/  126]
train() client id: f_00000-1-2 loss: 1.033985  [   96/  126]
train() client id: f_00000-2-0 loss: 0.846356  [   32/  126]
train() client id: f_00000-2-1 loss: 0.831568  [   64/  126]
train() client id: f_00000-2-2 loss: 0.921420  [   96/  126]
train() client id: f_00000-3-0 loss: 0.894217  [   32/  126]
train() client id: f_00000-3-1 loss: 0.918694  [   64/  126]
train() client id: f_00000-3-2 loss: 0.869872  [   96/  126]
train() client id: f_00000-4-0 loss: 0.841691  [   32/  126]
train() client id: f_00000-4-1 loss: 0.868694  [   64/  126]
train() client id: f_00000-4-2 loss: 0.829076  [   96/  126]
train() client id: f_00000-5-0 loss: 0.694746  [   32/  126]
train() client id: f_00000-5-1 loss: 0.806171  [   64/  126]
train() client id: f_00000-5-2 loss: 0.886494  [   96/  126]
train() client id: f_00000-6-0 loss: 0.885603  [   32/  126]
train() client id: f_00000-6-1 loss: 0.709926  [   64/  126]
train() client id: f_00000-6-2 loss: 0.742022  [   96/  126]
train() client id: f_00000-7-0 loss: 0.815218  [   32/  126]
train() client id: f_00000-7-1 loss: 0.662014  [   64/  126]
train() client id: f_00000-7-2 loss: 0.769344  [   96/  126]
train() client id: f_00000-8-0 loss: 0.872668  [   32/  126]
train() client id: f_00000-8-1 loss: 0.782370  [   64/  126]
train() client id: f_00000-8-2 loss: 0.687723  [   96/  126]
train() client id: f_00001-0-0 loss: 0.541183  [   32/  265]
train() client id: f_00001-0-1 loss: 0.380410  [   64/  265]
train() client id: f_00001-0-2 loss: 0.484940  [   96/  265]
train() client id: f_00001-0-3 loss: 0.470198  [  128/  265]
train() client id: f_00001-0-4 loss: 0.505477  [  160/  265]
train() client id: f_00001-0-5 loss: 0.524473  [  192/  265]
train() client id: f_00001-0-6 loss: 0.450054  [  224/  265]
train() client id: f_00001-0-7 loss: 0.519239  [  256/  265]
train() client id: f_00001-1-0 loss: 0.435061  [   32/  265]
train() client id: f_00001-1-1 loss: 0.462629  [   64/  265]
train() client id: f_00001-1-2 loss: 0.518078  [   96/  265]
train() client id: f_00001-1-3 loss: 0.463118  [  128/  265]
train() client id: f_00001-1-4 loss: 0.455692  [  160/  265]
train() client id: f_00001-1-5 loss: 0.610412  [  192/  265]
train() client id: f_00001-1-6 loss: 0.499991  [  224/  265]
train() client id: f_00001-1-7 loss: 0.470880  [  256/  265]
train() client id: f_00001-2-0 loss: 0.402234  [   32/  265]
train() client id: f_00001-2-1 loss: 0.488502  [   64/  265]
train() client id: f_00001-2-2 loss: 0.383008  [   96/  265]
train() client id: f_00001-2-3 loss: 0.447069  [  128/  265]
train() client id: f_00001-2-4 loss: 0.544737  [  160/  265]
train() client id: f_00001-2-5 loss: 0.532654  [  192/  265]
train() client id: f_00001-2-6 loss: 0.420899  [  224/  265]
train() client id: f_00001-2-7 loss: 0.623853  [  256/  265]
train() client id: f_00001-3-0 loss: 0.372128  [   32/  265]
train() client id: f_00001-3-1 loss: 0.405292  [   64/  265]
train() client id: f_00001-3-2 loss: 0.508359  [   96/  265]
train() client id: f_00001-3-3 loss: 0.446843  [  128/  265]
train() client id: f_00001-3-4 loss: 0.458177  [  160/  265]
train() client id: f_00001-3-5 loss: 0.556552  [  192/  265]
train() client id: f_00001-3-6 loss: 0.511782  [  224/  265]
train() client id: f_00001-3-7 loss: 0.515630  [  256/  265]
train() client id: f_00001-4-0 loss: 0.606190  [   32/  265]
train() client id: f_00001-4-1 loss: 0.424622  [   64/  265]
train() client id: f_00001-4-2 loss: 0.548659  [   96/  265]
train() client id: f_00001-4-3 loss: 0.446783  [  128/  265]
train() client id: f_00001-4-4 loss: 0.388300  [  160/  265]
train() client id: f_00001-4-5 loss: 0.477656  [  192/  265]
train() client id: f_00001-4-6 loss: 0.451466  [  224/  265]
train() client id: f_00001-4-7 loss: 0.497487  [  256/  265]
train() client id: f_00001-5-0 loss: 0.505981  [   32/  265]
train() client id: f_00001-5-1 loss: 0.503665  [   64/  265]
train() client id: f_00001-5-2 loss: 0.411573  [   96/  265]
train() client id: f_00001-5-3 loss: 0.529005  [  128/  265]
train() client id: f_00001-5-4 loss: 0.404765  [  160/  265]
train() client id: f_00001-5-5 loss: 0.542385  [  192/  265]
train() client id: f_00001-5-6 loss: 0.382704  [  224/  265]
train() client id: f_00001-5-7 loss: 0.547177  [  256/  265]
train() client id: f_00001-6-0 loss: 0.449073  [   32/  265]
train() client id: f_00001-6-1 loss: 0.466371  [   64/  265]
train() client id: f_00001-6-2 loss: 0.605659  [   96/  265]
train() client id: f_00001-6-3 loss: 0.544354  [  128/  265]
train() client id: f_00001-6-4 loss: 0.485747  [  160/  265]
train() client id: f_00001-6-5 loss: 0.503905  [  192/  265]
train() client id: f_00001-6-6 loss: 0.370059  [  224/  265]
train() client id: f_00001-6-7 loss: 0.388792  [  256/  265]
train() client id: f_00001-7-0 loss: 0.403173  [   32/  265]
train() client id: f_00001-7-1 loss: 0.466243  [   64/  265]
train() client id: f_00001-7-2 loss: 0.611093  [   96/  265]
train() client id: f_00001-7-3 loss: 0.377897  [  128/  265]
train() client id: f_00001-7-4 loss: 0.524213  [  160/  265]
train() client id: f_00001-7-5 loss: 0.505231  [  192/  265]
train() client id: f_00001-7-6 loss: 0.378085  [  224/  265]
train() client id: f_00001-7-7 loss: 0.521924  [  256/  265]
train() client id: f_00001-8-0 loss: 0.508070  [   32/  265]
train() client id: f_00001-8-1 loss: 0.461115  [   64/  265]
train() client id: f_00001-8-2 loss: 0.622291  [   96/  265]
train() client id: f_00001-8-3 loss: 0.436472  [  128/  265]
train() client id: f_00001-8-4 loss: 0.578497  [  160/  265]
train() client id: f_00001-8-5 loss: 0.443486  [  192/  265]
train() client id: f_00001-8-6 loss: 0.402164  [  224/  265]
train() client id: f_00001-8-7 loss: 0.382210  [  256/  265]
train() client id: f_00002-0-0 loss: 1.268532  [   32/  124]
train() client id: f_00002-0-1 loss: 1.273465  [   64/  124]
train() client id: f_00002-0-2 loss: 1.066975  [   96/  124]
train() client id: f_00002-1-0 loss: 1.292454  [   32/  124]
train() client id: f_00002-1-1 loss: 1.115319  [   64/  124]
train() client id: f_00002-1-2 loss: 1.113314  [   96/  124]
train() client id: f_00002-2-0 loss: 1.175909  [   32/  124]
train() client id: f_00002-2-1 loss: 1.211827  [   64/  124]
train() client id: f_00002-2-2 loss: 1.096284  [   96/  124]
train() client id: f_00002-3-0 loss: 1.383724  [   32/  124]
train() client id: f_00002-3-1 loss: 1.141009  [   64/  124]
train() client id: f_00002-3-2 loss: 0.870279  [   96/  124]
train() client id: f_00002-4-0 loss: 1.258646  [   32/  124]
train() client id: f_00002-4-1 loss: 0.990733  [   64/  124]
train() client id: f_00002-4-2 loss: 1.131401  [   96/  124]
train() client id: f_00002-5-0 loss: 1.371714  [   32/  124]
train() client id: f_00002-5-1 loss: 1.032813  [   64/  124]
train() client id: f_00002-5-2 loss: 0.909269  [   96/  124]
train() client id: f_00002-6-0 loss: 1.108294  [   32/  124]
train() client id: f_00002-6-1 loss: 1.029689  [   64/  124]
train() client id: f_00002-6-2 loss: 1.120600  [   96/  124]
train() client id: f_00002-7-0 loss: 1.272526  [   32/  124]
train() client id: f_00002-7-1 loss: 1.079015  [   64/  124]
train() client id: f_00002-7-2 loss: 0.914871  [   96/  124]
train() client id: f_00002-8-0 loss: 1.005321  [   32/  124]
train() client id: f_00002-8-1 loss: 1.089676  [   64/  124]
train() client id: f_00002-8-2 loss: 1.212948  [   96/  124]
train() client id: f_00003-0-0 loss: 0.667196  [   32/   43]
train() client id: f_00003-1-0 loss: 0.666256  [   32/   43]
train() client id: f_00003-2-0 loss: 0.571667  [   32/   43]
train() client id: f_00003-3-0 loss: 0.861272  [   32/   43]
train() client id: f_00003-4-0 loss: 0.612176  [   32/   43]
train() client id: f_00003-5-0 loss: 0.843124  [   32/   43]
train() client id: f_00003-6-0 loss: 0.765789  [   32/   43]
train() client id: f_00003-7-0 loss: 0.757684  [   32/   43]
train() client id: f_00003-8-0 loss: 0.902159  [   32/   43]
train() client id: f_00004-0-0 loss: 0.853298  [   32/  306]
train() client id: f_00004-0-1 loss: 0.850023  [   64/  306]
train() client id: f_00004-0-2 loss: 0.628983  [   96/  306]
train() client id: f_00004-0-3 loss: 0.830437  [  128/  306]
train() client id: f_00004-0-4 loss: 0.755751  [  160/  306]
train() client id: f_00004-0-5 loss: 0.653776  [  192/  306]
train() client id: f_00004-0-6 loss: 0.584900  [  224/  306]
train() client id: f_00004-0-7 loss: 0.853328  [  256/  306]
train() client id: f_00004-0-8 loss: 0.762470  [  288/  306]
train() client id: f_00004-1-0 loss: 0.621712  [   32/  306]
train() client id: f_00004-1-1 loss: 0.793740  [   64/  306]
train() client id: f_00004-1-2 loss: 0.761874  [   96/  306]
train() client id: f_00004-1-3 loss: 0.810398  [  128/  306]
train() client id: f_00004-1-4 loss: 0.697928  [  160/  306]
train() client id: f_00004-1-5 loss: 0.868852  [  192/  306]
train() client id: f_00004-1-6 loss: 0.699469  [  224/  306]
train() client id: f_00004-1-7 loss: 0.717494  [  256/  306]
train() client id: f_00004-1-8 loss: 0.753168  [  288/  306]
train() client id: f_00004-2-0 loss: 0.706963  [   32/  306]
train() client id: f_00004-2-1 loss: 0.693881  [   64/  306]
train() client id: f_00004-2-2 loss: 0.763529  [   96/  306]
train() client id: f_00004-2-3 loss: 0.733114  [  128/  306]
train() client id: f_00004-2-4 loss: 0.928729  [  160/  306]
train() client id: f_00004-2-5 loss: 0.775480  [  192/  306]
train() client id: f_00004-2-6 loss: 0.766249  [  224/  306]
train() client id: f_00004-2-7 loss: 0.779774  [  256/  306]
train() client id: f_00004-2-8 loss: 0.602133  [  288/  306]
train() client id: f_00004-3-0 loss: 0.812633  [   32/  306]
train() client id: f_00004-3-1 loss: 0.675702  [   64/  306]
train() client id: f_00004-3-2 loss: 0.894852  [   96/  306]
train() client id: f_00004-3-3 loss: 0.689533  [  128/  306]
train() client id: f_00004-3-4 loss: 0.745979  [  160/  306]
train() client id: f_00004-3-5 loss: 0.549993  [  192/  306]
train() client id: f_00004-3-6 loss: 0.768726  [  224/  306]
train() client id: f_00004-3-7 loss: 0.704310  [  256/  306]
train() client id: f_00004-3-8 loss: 0.824901  [  288/  306]
train() client id: f_00004-4-0 loss: 0.716157  [   32/  306]
train() client id: f_00004-4-1 loss: 0.691060  [   64/  306]
train() client id: f_00004-4-2 loss: 0.821061  [   96/  306]
train() client id: f_00004-4-3 loss: 0.778247  [  128/  306]
train() client id: f_00004-4-4 loss: 0.608520  [  160/  306]
train() client id: f_00004-4-5 loss: 0.824234  [  192/  306]
train() client id: f_00004-4-6 loss: 0.732462  [  224/  306]
train() client id: f_00004-4-7 loss: 0.705031  [  256/  306]
train() client id: f_00004-4-8 loss: 0.758307  [  288/  306]
train() client id: f_00004-5-0 loss: 0.791853  [   32/  306]
train() client id: f_00004-5-1 loss: 0.669105  [   64/  306]
train() client id: f_00004-5-2 loss: 0.617874  [   96/  306]
train() client id: f_00004-5-3 loss: 0.654760  [  128/  306]
train() client id: f_00004-5-4 loss: 0.809286  [  160/  306]
train() client id: f_00004-5-5 loss: 0.766784  [  192/  306]
train() client id: f_00004-5-6 loss: 0.798306  [  224/  306]
train() client id: f_00004-5-7 loss: 0.722316  [  256/  306]
train() client id: f_00004-5-8 loss: 0.855243  [  288/  306]
train() client id: f_00004-6-0 loss: 0.796228  [   32/  306]
train() client id: f_00004-6-1 loss: 0.768967  [   64/  306]
train() client id: f_00004-6-2 loss: 0.672427  [   96/  306]
train() client id: f_00004-6-3 loss: 0.832317  [  128/  306]
train() client id: f_00004-6-4 loss: 0.627670  [  160/  306]
train() client id: f_00004-6-5 loss: 0.731061  [  192/  306]
train() client id: f_00004-6-6 loss: 0.651924  [  224/  306]
train() client id: f_00004-6-7 loss: 0.872339  [  256/  306]
train() client id: f_00004-6-8 loss: 0.745831  [  288/  306]
train() client id: f_00004-7-0 loss: 0.750416  [   32/  306]
train() client id: f_00004-7-1 loss: 0.678646  [   64/  306]
train() client id: f_00004-7-2 loss: 0.837218  [   96/  306]
train() client id: f_00004-7-3 loss: 0.777757  [  128/  306]
train() client id: f_00004-7-4 loss: 0.800163  [  160/  306]
train() client id: f_00004-7-5 loss: 0.662090  [  192/  306]
train() client id: f_00004-7-6 loss: 0.851957  [  224/  306]
train() client id: f_00004-7-7 loss: 0.686163  [  256/  306]
train() client id: f_00004-7-8 loss: 0.658981  [  288/  306]
train() client id: f_00004-8-0 loss: 0.743410  [   32/  306]
train() client id: f_00004-8-1 loss: 0.757775  [   64/  306]
train() client id: f_00004-8-2 loss: 0.619716  [   96/  306]
train() client id: f_00004-8-3 loss: 0.701951  [  128/  306]
train() client id: f_00004-8-4 loss: 0.814851  [  160/  306]
train() client id: f_00004-8-5 loss: 0.728911  [  192/  306]
train() client id: f_00004-8-6 loss: 0.613190  [  224/  306]
train() client id: f_00004-8-7 loss: 0.771317  [  256/  306]
train() client id: f_00004-8-8 loss: 0.932732  [  288/  306]
train() client id: f_00005-0-0 loss: 0.854909  [   32/  146]
train() client id: f_00005-0-1 loss: 0.643623  [   64/  146]
train() client id: f_00005-0-2 loss: 0.654966  [   96/  146]
train() client id: f_00005-0-3 loss: 0.996713  [  128/  146]
train() client id: f_00005-1-0 loss: 0.825957  [   32/  146]
train() client id: f_00005-1-1 loss: 0.726097  [   64/  146]
train() client id: f_00005-1-2 loss: 0.560198  [   96/  146]
train() client id: f_00005-1-3 loss: 0.931650  [  128/  146]
train() client id: f_00005-2-0 loss: 0.615201  [   32/  146]
train() client id: f_00005-2-1 loss: 0.717019  [   64/  146]
train() client id: f_00005-2-2 loss: 0.953123  [   96/  146]
train() client id: f_00005-2-3 loss: 0.850912  [  128/  146]
train() client id: f_00005-3-0 loss: 0.815798  [   32/  146]
train() client id: f_00005-3-1 loss: 0.697512  [   64/  146]
train() client id: f_00005-3-2 loss: 1.052842  [   96/  146]
train() client id: f_00005-3-3 loss: 0.685516  [  128/  146]
train() client id: f_00005-4-0 loss: 0.697213  [   32/  146]
train() client id: f_00005-4-1 loss: 0.949065  [   64/  146]
train() client id: f_00005-4-2 loss: 0.691403  [   96/  146]
train() client id: f_00005-4-3 loss: 0.878026  [  128/  146]
train() client id: f_00005-5-0 loss: 0.870173  [   32/  146]
train() client id: f_00005-5-1 loss: 0.788026  [   64/  146]
train() client id: f_00005-5-2 loss: 0.786652  [   96/  146]
train() client id: f_00005-5-3 loss: 0.727620  [  128/  146]
train() client id: f_00005-6-0 loss: 0.797899  [   32/  146]
train() client id: f_00005-6-1 loss: 0.740296  [   64/  146]
train() client id: f_00005-6-2 loss: 0.960909  [   96/  146]
train() client id: f_00005-6-3 loss: 0.770551  [  128/  146]
train() client id: f_00005-7-0 loss: 0.736928  [   32/  146]
train() client id: f_00005-7-1 loss: 0.660891  [   64/  146]
train() client id: f_00005-7-2 loss: 0.756357  [   96/  146]
train() client id: f_00005-7-3 loss: 0.921710  [  128/  146]
train() client id: f_00005-8-0 loss: 0.757256  [   32/  146]
train() client id: f_00005-8-1 loss: 0.982964  [   64/  146]
train() client id: f_00005-8-2 loss: 0.785190  [   96/  146]
train() client id: f_00005-8-3 loss: 0.697614  [  128/  146]
train() client id: f_00006-0-0 loss: 0.510769  [   32/   54]
train() client id: f_00006-1-0 loss: 0.516219  [   32/   54]
train() client id: f_00006-2-0 loss: 0.533193  [   32/   54]
train() client id: f_00006-3-0 loss: 0.457966  [   32/   54]
train() client id: f_00006-4-0 loss: 0.473430  [   32/   54]
train() client id: f_00006-5-0 loss: 0.449721  [   32/   54]
train() client id: f_00006-6-0 loss: 0.506742  [   32/   54]
train() client id: f_00006-7-0 loss: 0.413722  [   32/   54]
train() client id: f_00006-8-0 loss: 0.470947  [   32/   54]
train() client id: f_00007-0-0 loss: 0.306527  [   32/  179]
train() client id: f_00007-0-1 loss: 0.507420  [   64/  179]
train() client id: f_00007-0-2 loss: 0.340143  [   96/  179]
train() client id: f_00007-0-3 loss: 0.478709  [  128/  179]
train() client id: f_00007-0-4 loss: 0.364738  [  160/  179]
train() client id: f_00007-1-0 loss: 0.344006  [   32/  179]
train() client id: f_00007-1-1 loss: 0.530168  [   64/  179]
train() client id: f_00007-1-2 loss: 0.297065  [   96/  179]
train() client id: f_00007-1-3 loss: 0.283688  [  128/  179]
train() client id: f_00007-1-4 loss: 0.503035  [  160/  179]
train() client id: f_00007-2-0 loss: 0.463664  [   32/  179]
train() client id: f_00007-2-1 loss: 0.257295  [   64/  179]
train() client id: f_00007-2-2 loss: 0.324171  [   96/  179]
train() client id: f_00007-2-3 loss: 0.446523  [  128/  179]
train() client id: f_00007-2-4 loss: 0.323674  [  160/  179]
train() client id: f_00007-3-0 loss: 0.291437  [   32/  179]
train() client id: f_00007-3-1 loss: 0.528449  [   64/  179]
train() client id: f_00007-3-2 loss: 0.321486  [   96/  179]
train() client id: f_00007-3-3 loss: 0.224126  [  128/  179]
train() client id: f_00007-3-4 loss: 0.448513  [  160/  179]
train() client id: f_00007-4-0 loss: 0.218424  [   32/  179]
train() client id: f_00007-4-1 loss: 0.299198  [   64/  179]
train() client id: f_00007-4-2 loss: 0.457861  [   96/  179]
train() client id: f_00007-4-3 loss: 0.282320  [  128/  179]
train() client id: f_00007-4-4 loss: 0.364536  [  160/  179]
train() client id: f_00007-5-0 loss: 0.323259  [   32/  179]
train() client id: f_00007-5-1 loss: 0.249075  [   64/  179]
train() client id: f_00007-5-2 loss: 0.131076  [   96/  179]
train() client id: f_00007-5-3 loss: 0.448237  [  128/  179]
train() client id: f_00007-5-4 loss: 0.367194  [  160/  179]
train() client id: f_00007-6-0 loss: 0.358996  [   32/  179]
train() client id: f_00007-6-1 loss: 0.390673  [   64/  179]
train() client id: f_00007-6-2 loss: 0.443279  [   96/  179]
train() client id: f_00007-6-3 loss: 0.231432  [  128/  179]
train() client id: f_00007-6-4 loss: 0.203818  [  160/  179]
train() client id: f_00007-7-0 loss: 0.163661  [   32/  179]
train() client id: f_00007-7-1 loss: 0.210509  [   64/  179]
train() client id: f_00007-7-2 loss: 0.416698  [   96/  179]
train() client id: f_00007-7-3 loss: 0.186848  [  128/  179]
train() client id: f_00007-7-4 loss: 0.425389  [  160/  179]
train() client id: f_00007-8-0 loss: 0.642734  [   32/  179]
train() client id: f_00007-8-1 loss: 0.267748  [   64/  179]
train() client id: f_00007-8-2 loss: 0.246724  [   96/  179]
train() client id: f_00007-8-3 loss: 0.167039  [  128/  179]
train() client id: f_00007-8-4 loss: 0.277809  [  160/  179]
train() client id: f_00008-0-0 loss: 0.669309  [   32/  130]
train() client id: f_00008-0-1 loss: 0.718410  [   64/  130]
train() client id: f_00008-0-2 loss: 0.842429  [   96/  130]
train() client id: f_00008-0-3 loss: 0.650158  [  128/  130]
train() client id: f_00008-1-0 loss: 0.672003  [   32/  130]
train() client id: f_00008-1-1 loss: 0.737896  [   64/  130]
train() client id: f_00008-1-2 loss: 0.669147  [   96/  130]
train() client id: f_00008-1-3 loss: 0.814416  [  128/  130]
train() client id: f_00008-2-0 loss: 0.758466  [   32/  130]
train() client id: f_00008-2-1 loss: 0.615280  [   64/  130]
train() client id: f_00008-2-2 loss: 0.822276  [   96/  130]
train() client id: f_00008-2-3 loss: 0.691564  [  128/  130]
train() client id: f_00008-3-0 loss: 0.750513  [   32/  130]
train() client id: f_00008-3-1 loss: 0.790670  [   64/  130]
train() client id: f_00008-3-2 loss: 0.615444  [   96/  130]
train() client id: f_00008-3-3 loss: 0.701133  [  128/  130]
train() client id: f_00008-4-0 loss: 0.759202  [   32/  130]
train() client id: f_00008-4-1 loss: 0.697049  [   64/  130]
train() client id: f_00008-4-2 loss: 0.719486  [   96/  130]
train() client id: f_00008-4-3 loss: 0.664593  [  128/  130]
train() client id: f_00008-5-0 loss: 0.783299  [   32/  130]
train() client id: f_00008-5-1 loss: 0.675393  [   64/  130]
train() client id: f_00008-5-2 loss: 0.689919  [   96/  130]
train() client id: f_00008-5-3 loss: 0.734394  [  128/  130]
train() client id: f_00008-6-0 loss: 0.633049  [   32/  130]
train() client id: f_00008-6-1 loss: 0.761343  [   64/  130]
train() client id: f_00008-6-2 loss: 0.792682  [   96/  130]
train() client id: f_00008-6-3 loss: 0.684143  [  128/  130]
train() client id: f_00008-7-0 loss: 0.783119  [   32/  130]
train() client id: f_00008-7-1 loss: 0.631055  [   64/  130]
train() client id: f_00008-7-2 loss: 0.776728  [   96/  130]
train() client id: f_00008-7-3 loss: 0.674070  [  128/  130]
train() client id: f_00008-8-0 loss: 0.687793  [   32/  130]
train() client id: f_00008-8-1 loss: 0.737120  [   64/  130]
train() client id: f_00008-8-2 loss: 0.695395  [   96/  130]
train() client id: f_00008-8-3 loss: 0.768825  [  128/  130]
train() client id: f_00009-0-0 loss: 1.161529  [   32/  118]
train() client id: f_00009-0-1 loss: 0.886112  [   64/  118]
train() client id: f_00009-0-2 loss: 1.139310  [   96/  118]
train() client id: f_00009-1-0 loss: 1.039783  [   32/  118]
train() client id: f_00009-1-1 loss: 1.148437  [   64/  118]
train() client id: f_00009-1-2 loss: 0.937715  [   96/  118]
train() client id: f_00009-2-0 loss: 0.968842  [   32/  118]
train() client id: f_00009-2-1 loss: 1.017917  [   64/  118]
train() client id: f_00009-2-2 loss: 1.108078  [   96/  118]
train() client id: f_00009-3-0 loss: 0.956941  [   32/  118]
train() client id: f_00009-3-1 loss: 1.012327  [   64/  118]
train() client id: f_00009-3-2 loss: 1.083035  [   96/  118]
train() client id: f_00009-4-0 loss: 0.881910  [   32/  118]
train() client id: f_00009-4-1 loss: 0.975472  [   64/  118]
train() client id: f_00009-4-2 loss: 0.934860  [   96/  118]
train() client id: f_00009-5-0 loss: 0.990852  [   32/  118]
train() client id: f_00009-5-1 loss: 0.930604  [   64/  118]
train() client id: f_00009-5-2 loss: 0.786316  [   96/  118]
train() client id: f_00009-6-0 loss: 0.950048  [   32/  118]
train() client id: f_00009-6-1 loss: 0.853765  [   64/  118]
train() client id: f_00009-6-2 loss: 1.007057  [   96/  118]
train() client id: f_00009-7-0 loss: 1.005126  [   32/  118]
train() client id: f_00009-7-1 loss: 0.711745  [   64/  118]
train() client id: f_00009-7-2 loss: 0.861898  [   96/  118]
train() client id: f_00009-8-0 loss: 0.903160  [   32/  118]
train() client id: f_00009-8-1 loss: 0.776045  [   64/  118]
train() client id: f_00009-8-2 loss: 1.006518  [   96/  118]
At round 64 accuracy: 0.6445623342175066
At round 64 training accuracy: 0.5949027498323273
At round 64 training loss: 0.816822898945245
update_location
xs = [  -3.9056584     4.20031788  340.00902392   18.81129433    0.97929623
    3.95640986 -302.44319194 -281.32485185  324.66397685 -267.06087855]
ys = [ 332.5879595   315.55583871    1.32061395 -302.45517586  294.35018685
  277.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [347.31830497 331.04853121 354.41202063 319.11282989 310.87455913
 295.29029028 318.55733376 298.57050846 340.16961628 285.19734345]
dists_bs = [232.60962728 227.74700376 543.50604767 515.29240511 212.63391028
 206.38833839 218.61473469 204.06574573 523.87842829 194.20530012]
uav_gains = [1.25202968e-12 1.57013546e-12 1.14585755e-12 1.89621938e-12
 2.18534457e-12 2.92930714e-12 1.91387845e-12 2.74742002e-12
 1.37723251e-12 3.59110754e-12]
bs_gains = [2.61057735e-11 2.76966092e-11 2.42505626e-12 2.81542271e-12
 3.35678104e-11 3.64901503e-11 3.10593012e-11 3.76649830e-11
 2.68811993e-12 4.32676259e-11]
Round 65
-------------------------------
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.41801623 4.82581662 2.38917526 0.89421619 5.56278746 2.67778749
 1.09198131 3.3275343  2.44520381 2.17140866]
obj_prev = 27.80392733965525
eta_min = 3.100306852884529e-39	eta_max = 0.942754072368237
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 6.3556060652170405	eta = 0.9090909090909091
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 16.15583264303265	eta = 0.3576308212219315
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 10.677811898063974	eta = 0.5411055889361986
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 9.733010612728588	eta = 0.5936317061131898
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 9.673834452832171	eta = 0.597263031926322
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 9.673568210715082	eta = 0.5972794701806058
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 9.673568205281397	eta = 0.5972794705161003
eta = 0.5972794705161003
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [0.04252711 0.08944194 0.04185208 0.01451322 0.10328012 0.04927744
 0.0182259  0.0604155  0.04387718 0.03982698]
ene_total = [0.98560292 1.43449508 0.99341101 0.50602748 1.63267669 0.83712064
 0.55929955 1.13571028 0.89502304 0.69420151]
ti_comp = [1.57505947 1.73869087 1.56299493 1.62132055 1.74218994 1.74362015
 1.62219032 1.65177999 1.65020898 1.7463865 ]
ti_coms = [0.24553217 0.08190077 0.25759671 0.19927109 0.0784017  0.07697149
 0.19840132 0.16881166 0.17038266 0.07420514]
t_total = [26.74972725 26.74972725 26.74972725 26.74972725 26.74972725 26.74972725
 26.74972725 26.74972725 26.74972725 26.74972725]
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [1.93768734e-06 1.47930885e-05 1.87549493e-06 7.26831216e-08
 2.26850246e-05 2.45991889e-06 1.43794955e-07 5.05150367e-06
 1.93873362e-06 1.29458708e-06]
ene_total = [0.36078585 0.12055314 0.37851121 0.29278761 0.11552795 0.1131294
 0.29151072 0.24810709 0.25036961 0.10904772]
optimize_network iter = 0 obj = 2.280330304694935
eta = 0.5972794705161003
freqs = [13500159.94622198 25721057.81080511 13388423.45249819  4475740.44373556
 29640890.63331935 14130785.41729783  5617683.65648641 18287997.60708139
 13294430.26761132 11402680.79036755]
eta_min = 0.5972794705161009	eta_max = 0.7502409697515174
af = 0.0005999271736932453	bf = 0.9180266431472464	zeta = 0.0006599198910625699	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [3.58346965e-07 2.73576559e-06 3.46845386e-07 1.34416814e-08
 4.19526387e-06 4.54926060e-07 2.65927760e-08 9.34201802e-07
 3.58540459e-07 2.39414968e-07]
ene_total = [1.59815174 0.53325669 1.67667707 1.29702355 0.51057679 0.50102436
 1.29136322 1.09882803 1.10901597 0.48300462]
ti_comp = [0.8835615  1.0471929  0.87149697 0.92982258 1.05069198 1.05212218
 0.93069235 0.96028202 0.95871102 1.05488853]
ti_coms = [0.24553217 0.08190077 0.25759671 0.19927109 0.0784017  0.07697149
 0.19840132 0.16881166 0.17038266 0.07420514]
t_total = [26.74972725 26.74972725 26.74972725 26.74972725 26.74972725 26.74972725
 26.74972725 26.74972725 26.74972725 26.74972725]
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [1.06740436e-06 7.06927946e-06 1.04574117e-06 3.83085836e-08
 1.08119582e-05 1.17116240e-06 7.57285947e-08 2.59091048e-06
 9.95739029e-07 6.15068927e-07]
ene_total = [0.58172358 0.19420128 0.61030556 0.47210056 0.18600018 0.18238342
 0.47004085 0.39999859 0.40368272 0.1758164 ]
optimize_network iter = 1 obj = 3.676253139763781
eta = 0.7502409697515174
freqs = [13418605.79030925 23811832.48313907 13388423.4524982   4351530.1809025
 27404358.81401067 13057508.37108007  5459605.33203652 17539930.50999551
 12759371.1105045  10525653.53083322]
eta_min = 0.7502409697515208	eta_max = 0.7502409697515149
af = 0.00052469220506583	bf = 0.9180266431472464	zeta = 0.000577161425572413	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [3.54030510e-07 2.34469776e-06 3.46845386e-07 1.27059696e-08
 3.58604780e-06 3.88444377e-07 2.51172226e-08 8.59338214e-07
 3.30260967e-07 2.04002507e-07]
ene_total = [1.59815146 0.53323124 1.67667707 1.2970235  0.51053714 0.50102003
 1.29136313 1.09882316 1.10901413 0.48300231]
ti_comp = [0.8835615  1.0471929  0.87149697 0.92982258 1.05069198 1.05212218
 0.93069235 0.96028202 0.95871102 1.05488853]
ti_coms = [0.24553217 0.08190077 0.25759671 0.19927109 0.0784017  0.07697149
 0.19840132 0.16881166 0.17038266 0.07420514]
t_total = [26.74972725 26.74972725 26.74972725 26.74972725 26.74972725 26.74972725
 26.74972725 26.74972725 26.74972725 26.74972725]
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [1.06740436e-06 7.06927946e-06 1.04574117e-06 3.83085836e-08
 1.08119582e-05 1.17116240e-06 7.57285947e-08 2.59091048e-06
 9.95739029e-07 6.15068927e-07]
ene_total = [0.58172358 0.19420128 0.61030556 0.47210056 0.18600018 0.18238342
 0.47004085 0.39999859 0.40368272 0.1758164 ]
optimize_network iter = 2 obj = 3.6762531397637437
eta = 0.7502409697515149
freqs = [13418605.79030923 23811832.48313908 13388423.45249818  4351530.1809025
 27404358.81401069 13057508.37108008  5459605.33203651 17539930.5099955
 12759371.11050449 10525653.53083322]
Done!
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [1.02093442e-06 6.76151514e-06 1.00021434e-06 3.66408019e-08
 1.03412546e-05 1.12017531e-06 7.24317156e-08 2.47811400e-06
 9.52389074e-07 5.88291620e-07]
ene_total = [0.02455424 0.00819684 0.02576067 0.01992715 0.00785051 0.00769827
 0.0198402  0.01688364 0.01703922 0.0074211 ]
At round 65 energy consumption: 0.15517184374296747
At round 65 eta: 0.7502409697515149
At round 65 a_n: 5.574576956279433
At round 65 local rounds: 9.409653576790653
At round 65 global rounds: 23.691326785509258
gradient difference: 0.4953983724117279
train() client id: f_00000-0-0 loss: 0.937795  [   32/  126]
train() client id: f_00000-0-1 loss: 1.208470  [   64/  126]
train() client id: f_00000-0-2 loss: 1.151524  [   96/  126]
train() client id: f_00000-1-0 loss: 1.020344  [   32/  126]
train() client id: f_00000-1-1 loss: 1.152092  [   64/  126]
train() client id: f_00000-1-2 loss: 1.113907  [   96/  126]
train() client id: f_00000-2-0 loss: 1.144656  [   32/  126]
train() client id: f_00000-2-1 loss: 0.916063  [   64/  126]
train() client id: f_00000-2-2 loss: 0.924587  [   96/  126]
train() client id: f_00000-3-0 loss: 0.949983  [   32/  126]
train() client id: f_00000-3-1 loss: 1.142669  [   64/  126]
train() client id: f_00000-3-2 loss: 0.916536  [   96/  126]
train() client id: f_00000-4-0 loss: 0.935282  [   32/  126]
train() client id: f_00000-4-1 loss: 0.940874  [   64/  126]
train() client id: f_00000-4-2 loss: 1.044729  [   96/  126]
train() client id: f_00000-5-0 loss: 0.919584  [   32/  126]
train() client id: f_00000-5-1 loss: 0.892497  [   64/  126]
train() client id: f_00000-5-2 loss: 0.935492  [   96/  126]
train() client id: f_00000-6-0 loss: 0.947593  [   32/  126]
train() client id: f_00000-6-1 loss: 0.955312  [   64/  126]
train() client id: f_00000-6-2 loss: 0.827082  [   96/  126]
train() client id: f_00000-7-0 loss: 0.863675  [   32/  126]
train() client id: f_00000-7-1 loss: 1.001509  [   64/  126]
train() client id: f_00000-7-2 loss: 0.814416  [   96/  126]
train() client id: f_00000-8-0 loss: 0.866955  [   32/  126]
train() client id: f_00000-8-1 loss: 0.938481  [   64/  126]
train() client id: f_00000-8-2 loss: 0.914234  [   96/  126]
train() client id: f_00001-0-0 loss: 0.522911  [   32/  265]
train() client id: f_00001-0-1 loss: 0.565024  [   64/  265]
train() client id: f_00001-0-2 loss: 0.458163  [   96/  265]
train() client id: f_00001-0-3 loss: 0.503120  [  128/  265]
train() client id: f_00001-0-4 loss: 0.545300  [  160/  265]
train() client id: f_00001-0-5 loss: 0.608396  [  192/  265]
train() client id: f_00001-0-6 loss: 0.424158  [  224/  265]
train() client id: f_00001-0-7 loss: 0.651018  [  256/  265]
train() client id: f_00001-1-0 loss: 0.544846  [   32/  265]
train() client id: f_00001-1-1 loss: 0.521074  [   64/  265]
train() client id: f_00001-1-2 loss: 0.444544  [   96/  265]
train() client id: f_00001-1-3 loss: 0.565531  [  128/  265]
train() client id: f_00001-1-4 loss: 0.605925  [  160/  265]
train() client id: f_00001-1-5 loss: 0.426360  [  192/  265]
train() client id: f_00001-1-6 loss: 0.522522  [  224/  265]
train() client id: f_00001-1-7 loss: 0.527862  [  256/  265]
train() client id: f_00001-2-0 loss: 0.563649  [   32/  265]
train() client id: f_00001-2-1 loss: 0.495762  [   64/  265]
train() client id: f_00001-2-2 loss: 0.512556  [   96/  265]
train() client id: f_00001-2-3 loss: 0.620918  [  128/  265]
train() client id: f_00001-2-4 loss: 0.508997  [  160/  265]
train() client id: f_00001-2-5 loss: 0.425942  [  192/  265]
train() client id: f_00001-2-6 loss: 0.514044  [  224/  265]
train() client id: f_00001-2-7 loss: 0.568388  [  256/  265]
train() client id: f_00001-3-0 loss: 0.505423  [   32/  265]
train() client id: f_00001-3-1 loss: 0.595114  [   64/  265]
train() client id: f_00001-3-2 loss: 0.586104  [   96/  265]
train() client id: f_00001-3-3 loss: 0.424817  [  128/  265]
train() client id: f_00001-3-4 loss: 0.593663  [  160/  265]
train() client id: f_00001-3-5 loss: 0.552334  [  192/  265]
train() client id: f_00001-3-6 loss: 0.480967  [  224/  265]
train() client id: f_00001-3-7 loss: 0.476612  [  256/  265]
train() client id: f_00001-4-0 loss: 0.652802  [   32/  265]
train() client id: f_00001-4-1 loss: 0.439366  [   64/  265]
train() client id: f_00001-4-2 loss: 0.443488  [   96/  265]
train() client id: f_00001-4-3 loss: 0.540117  [  128/  265]
train() client id: f_00001-4-4 loss: 0.523136  [  160/  265]
train() client id: f_00001-4-5 loss: 0.537061  [  192/  265]
train() client id: f_00001-4-6 loss: 0.576008  [  224/  265]
train() client id: f_00001-4-7 loss: 0.495309  [  256/  265]
train() client id: f_00001-5-0 loss: 0.447327  [   32/  265]
train() client id: f_00001-5-1 loss: 0.675793  [   64/  265]
train() client id: f_00001-5-2 loss: 0.435257  [   96/  265]
train() client id: f_00001-5-3 loss: 0.481293  [  128/  265]
train() client id: f_00001-5-4 loss: 0.596931  [  160/  265]
train() client id: f_00001-5-5 loss: 0.484033  [  192/  265]
train() client id: f_00001-5-6 loss: 0.407400  [  224/  265]
train() client id: f_00001-5-7 loss: 0.599777  [  256/  265]
train() client id: f_00001-6-0 loss: 0.629796  [   32/  265]
train() client id: f_00001-6-1 loss: 0.523971  [   64/  265]
train() client id: f_00001-6-2 loss: 0.482232  [   96/  265]
train() client id: f_00001-6-3 loss: 0.500936  [  128/  265]
train() client id: f_00001-6-4 loss: 0.476695  [  160/  265]
train() client id: f_00001-6-5 loss: 0.442963  [  192/  265]
train() client id: f_00001-6-6 loss: 0.521448  [  224/  265]
train() client id: f_00001-6-7 loss: 0.556034  [  256/  265]
train() client id: f_00001-7-0 loss: 0.468082  [   32/  265]
train() client id: f_00001-7-1 loss: 0.510998  [   64/  265]
train() client id: f_00001-7-2 loss: 0.522779  [   96/  265]
train() client id: f_00001-7-3 loss: 0.660281  [  128/  265]
train() client id: f_00001-7-4 loss: 0.478100  [  160/  265]
train() client id: f_00001-7-5 loss: 0.490966  [  192/  265]
train() client id: f_00001-7-6 loss: 0.538158  [  224/  265]
train() client id: f_00001-7-7 loss: 0.515816  [  256/  265]
train() client id: f_00001-8-0 loss: 0.479887  [   32/  265]
train() client id: f_00001-8-1 loss: 0.539280  [   64/  265]
train() client id: f_00001-8-2 loss: 0.438993  [   96/  265]
train() client id: f_00001-8-3 loss: 0.652339  [  128/  265]
train() client id: f_00001-8-4 loss: 0.620900  [  160/  265]
train() client id: f_00001-8-5 loss: 0.578798  [  192/  265]
train() client id: f_00001-8-6 loss: 0.465102  [  224/  265]
train() client id: f_00001-8-7 loss: 0.461136  [  256/  265]
train() client id: f_00002-0-0 loss: 0.876680  [   32/  124]
train() client id: f_00002-0-1 loss: 0.912491  [   64/  124]
train() client id: f_00002-0-2 loss: 1.139701  [   96/  124]
train() client id: f_00002-1-0 loss: 1.077764  [   32/  124]
train() client id: f_00002-1-1 loss: 1.048440  [   64/  124]
train() client id: f_00002-1-2 loss: 0.981191  [   96/  124]
train() client id: f_00002-2-0 loss: 0.971544  [   32/  124]
train() client id: f_00002-2-1 loss: 0.922826  [   64/  124]
train() client id: f_00002-2-2 loss: 1.081801  [   96/  124]
train() client id: f_00002-3-0 loss: 0.798582  [   32/  124]
train() client id: f_00002-3-1 loss: 0.828402  [   64/  124]
train() client id: f_00002-3-2 loss: 0.996529  [   96/  124]
train() client id: f_00002-4-0 loss: 0.880113  [   32/  124]
train() client id: f_00002-4-1 loss: 0.982268  [   64/  124]
train() client id: f_00002-4-2 loss: 1.076467  [   96/  124]
train() client id: f_00002-5-0 loss: 0.760350  [   32/  124]
train() client id: f_00002-5-1 loss: 0.895676  [   64/  124]
train() client id: f_00002-5-2 loss: 0.938669  [   96/  124]
train() client id: f_00002-6-0 loss: 0.751991  [   32/  124]
train() client id: f_00002-6-1 loss: 0.920959  [   64/  124]
train() client id: f_00002-6-2 loss: 0.725224  [   96/  124]
train() client id: f_00002-7-0 loss: 0.847641  [   32/  124]
train() client id: f_00002-7-1 loss: 0.729470  [   64/  124]
train() client id: f_00002-7-2 loss: 0.869785  [   96/  124]
train() client id: f_00002-8-0 loss: 0.930078  [   32/  124]
train() client id: f_00002-8-1 loss: 0.871909  [   64/  124]
train() client id: f_00002-8-2 loss: 0.596421  [   96/  124]
train() client id: f_00003-0-0 loss: 0.554450  [   32/   43]
train() client id: f_00003-1-0 loss: 0.444300  [   32/   43]
train() client id: f_00003-2-0 loss: 0.546815  [   32/   43]
train() client id: f_00003-3-0 loss: 0.480534  [   32/   43]
train() client id: f_00003-4-0 loss: 0.712578  [   32/   43]
train() client id: f_00003-5-0 loss: 0.440058  [   32/   43]
train() client id: f_00003-6-0 loss: 0.457890  [   32/   43]
train() client id: f_00003-7-0 loss: 0.325048  [   32/   43]
train() client id: f_00003-8-0 loss: 0.300878  [   32/   43]
train() client id: f_00004-0-0 loss: 0.440011  [   32/  306]
train() client id: f_00004-0-1 loss: 0.823858  [   64/  306]
train() client id: f_00004-0-2 loss: 0.658737  [   96/  306]
train() client id: f_00004-0-3 loss: 0.583027  [  128/  306]
train() client id: f_00004-0-4 loss: 0.577724  [  160/  306]
train() client id: f_00004-0-5 loss: 0.564027  [  192/  306]
train() client id: f_00004-0-6 loss: 0.679658  [  224/  306]
train() client id: f_00004-0-7 loss: 0.654614  [  256/  306]
train() client id: f_00004-0-8 loss: 0.676511  [  288/  306]
train() client id: f_00004-1-0 loss: 0.614409  [   32/  306]
train() client id: f_00004-1-1 loss: 0.628268  [   64/  306]
train() client id: f_00004-1-2 loss: 0.489606  [   96/  306]
train() client id: f_00004-1-3 loss: 0.655817  [  128/  306]
train() client id: f_00004-1-4 loss: 0.722531  [  160/  306]
train() client id: f_00004-1-5 loss: 0.674913  [  192/  306]
train() client id: f_00004-1-6 loss: 0.679275  [  224/  306]
train() client id: f_00004-1-7 loss: 0.763956  [  256/  306]
train() client id: f_00004-1-8 loss: 0.549271  [  288/  306]
train() client id: f_00004-2-0 loss: 0.673967  [   32/  306]
train() client id: f_00004-2-1 loss: 0.640437  [   64/  306]
train() client id: f_00004-2-2 loss: 0.575279  [   96/  306]
train() client id: f_00004-2-3 loss: 0.630977  [  128/  306]
train() client id: f_00004-2-4 loss: 0.532458  [  160/  306]
train() client id: f_00004-2-5 loss: 0.576094  [  192/  306]
train() client id: f_00004-2-6 loss: 0.759668  [  224/  306]
train() client id: f_00004-2-7 loss: 0.665842  [  256/  306]
train() client id: f_00004-2-8 loss: 0.698469  [  288/  306]
train() client id: f_00004-3-0 loss: 0.676342  [   32/  306]
train() client id: f_00004-3-1 loss: 0.718893  [   64/  306]
train() client id: f_00004-3-2 loss: 0.693933  [   96/  306]
train() client id: f_00004-3-3 loss: 0.587106  [  128/  306]
train() client id: f_00004-3-4 loss: 0.577103  [  160/  306]
train() client id: f_00004-3-5 loss: 0.631332  [  192/  306]
train() client id: f_00004-3-6 loss: 0.694596  [  224/  306]
train() client id: f_00004-3-7 loss: 0.577833  [  256/  306]
train() client id: f_00004-3-8 loss: 0.638175  [  288/  306]
train() client id: f_00004-4-0 loss: 0.615668  [   32/  306]
train() client id: f_00004-4-1 loss: 0.685586  [   64/  306]
train() client id: f_00004-4-2 loss: 0.768506  [   96/  306]
train() client id: f_00004-4-3 loss: 0.610725  [  128/  306]
train() client id: f_00004-4-4 loss: 0.680686  [  160/  306]
train() client id: f_00004-4-5 loss: 0.612126  [  192/  306]
train() client id: f_00004-4-6 loss: 0.545383  [  224/  306]
train() client id: f_00004-4-7 loss: 0.580444  [  256/  306]
train() client id: f_00004-4-8 loss: 0.732074  [  288/  306]
train() client id: f_00004-5-0 loss: 0.658704  [   32/  306]
train() client id: f_00004-5-1 loss: 0.675691  [   64/  306]
train() client id: f_00004-5-2 loss: 0.547155  [   96/  306]
train() client id: f_00004-5-3 loss: 0.612777  [  128/  306]
train() client id: f_00004-5-4 loss: 0.639289  [  160/  306]
train() client id: f_00004-5-5 loss: 0.616076  [  192/  306]
train() client id: f_00004-5-6 loss: 0.726632  [  224/  306]
train() client id: f_00004-5-7 loss: 0.705075  [  256/  306]
train() client id: f_00004-5-8 loss: 0.744139  [  288/  306]
train() client id: f_00004-6-0 loss: 0.645179  [   32/  306]
train() client id: f_00004-6-1 loss: 0.711654  [   64/  306]
train() client id: f_00004-6-2 loss: 0.633420  [   96/  306]
train() client id: f_00004-6-3 loss: 0.697133  [  128/  306]
train() client id: f_00004-6-4 loss: 0.693845  [  160/  306]
train() client id: f_00004-6-5 loss: 0.660969  [  192/  306]
train() client id: f_00004-6-6 loss: 0.717570  [  224/  306]
train() client id: f_00004-6-7 loss: 0.533476  [  256/  306]
train() client id: f_00004-6-8 loss: 0.713065  [  288/  306]
train() client id: f_00004-7-0 loss: 0.650455  [   32/  306]
train() client id: f_00004-7-1 loss: 0.561369  [   64/  306]
train() client id: f_00004-7-2 loss: 0.754080  [   96/  306]
train() client id: f_00004-7-3 loss: 0.648213  [  128/  306]
train() client id: f_00004-7-4 loss: 0.708426  [  160/  306]
train() client id: f_00004-7-5 loss: 0.678833  [  192/  306]
train() client id: f_00004-7-6 loss: 0.550553  [  224/  306]
train() client id: f_00004-7-7 loss: 0.733293  [  256/  306]
train() client id: f_00004-7-8 loss: 0.664396  [  288/  306]
train() client id: f_00004-8-0 loss: 0.623864  [   32/  306]
train() client id: f_00004-8-1 loss: 0.657653  [   64/  306]
train() client id: f_00004-8-2 loss: 0.598844  [   96/  306]
train() client id: f_00004-8-3 loss: 0.630088  [  128/  306]
train() client id: f_00004-8-4 loss: 0.725821  [  160/  306]
train() client id: f_00004-8-5 loss: 0.734290  [  192/  306]
train() client id: f_00004-8-6 loss: 0.696392  [  224/  306]
train() client id: f_00004-8-7 loss: 0.693360  [  256/  306]
train() client id: f_00004-8-8 loss: 0.675943  [  288/  306]
train() client id: f_00005-0-0 loss: 0.424750  [   32/  146]
train() client id: f_00005-0-1 loss: 0.708464  [   64/  146]
train() client id: f_00005-0-2 loss: 1.104321  [   96/  146]
train() client id: f_00005-0-3 loss: 0.659488  [  128/  146]
train() client id: f_00005-1-0 loss: 0.571906  [   32/  146]
train() client id: f_00005-1-1 loss: 0.729006  [   64/  146]
train() client id: f_00005-1-2 loss: 0.726522  [   96/  146]
train() client id: f_00005-1-3 loss: 0.760956  [  128/  146]
train() client id: f_00005-2-0 loss: 0.436873  [   32/  146]
train() client id: f_00005-2-1 loss: 0.624281  [   64/  146]
train() client id: f_00005-2-2 loss: 1.023553  [   96/  146]
train() client id: f_00005-2-3 loss: 0.710281  [  128/  146]
train() client id: f_00005-3-0 loss: 0.723789  [   32/  146]
train() client id: f_00005-3-1 loss: 0.906011  [   64/  146]
train() client id: f_00005-3-2 loss: 0.549698  [   96/  146]
train() client id: f_00005-3-3 loss: 0.740022  [  128/  146]
train() client id: f_00005-4-0 loss: 0.918276  [   32/  146]
train() client id: f_00005-4-1 loss: 0.603460  [   64/  146]
train() client id: f_00005-4-2 loss: 0.784784  [   96/  146]
train() client id: f_00005-4-3 loss: 0.675865  [  128/  146]
train() client id: f_00005-5-0 loss: 0.760983  [   32/  146]
train() client id: f_00005-5-1 loss: 0.711582  [   64/  146]
train() client id: f_00005-5-2 loss: 0.673227  [   96/  146]
train() client id: f_00005-5-3 loss: 0.792071  [  128/  146]
train() client id: f_00005-6-0 loss: 0.749735  [   32/  146]
train() client id: f_00005-6-1 loss: 0.575332  [   64/  146]
train() client id: f_00005-6-2 loss: 0.990170  [   96/  146]
train() client id: f_00005-6-3 loss: 0.649004  [  128/  146]
train() client id: f_00005-7-0 loss: 0.518468  [   32/  146]
train() client id: f_00005-7-1 loss: 0.777981  [   64/  146]
train() client id: f_00005-7-2 loss: 0.855204  [   96/  146]
train() client id: f_00005-7-3 loss: 0.756734  [  128/  146]
train() client id: f_00005-8-0 loss: 0.518555  [   32/  146]
train() client id: f_00005-8-1 loss: 0.753290  [   64/  146]
train() client id: f_00005-8-2 loss: 0.701472  [   96/  146]
train() client id: f_00005-8-3 loss: 0.809595  [  128/  146]
train() client id: f_00006-0-0 loss: 0.538249  [   32/   54]
train() client id: f_00006-1-0 loss: 0.496660  [   32/   54]
train() client id: f_00006-2-0 loss: 0.491190  [   32/   54]
train() client id: f_00006-3-0 loss: 0.547489  [   32/   54]
train() client id: f_00006-4-0 loss: 0.475794  [   32/   54]
train() client id: f_00006-5-0 loss: 0.497298  [   32/   54]
train() client id: f_00006-6-0 loss: 0.521196  [   32/   54]
train() client id: f_00006-7-0 loss: 0.489748  [   32/   54]
train() client id: f_00006-8-0 loss: 0.503992  [   32/   54]
train() client id: f_00007-0-0 loss: 0.674389  [   32/  179]
train() client id: f_00007-0-1 loss: 0.811840  [   64/  179]
train() client id: f_00007-0-2 loss: 0.563822  [   96/  179]
train() client id: f_00007-0-3 loss: 0.647763  [  128/  179]
train() client id: f_00007-0-4 loss: 0.543853  [  160/  179]
train() client id: f_00007-1-0 loss: 0.545324  [   32/  179]
train() client id: f_00007-1-1 loss: 0.695991  [   64/  179]
train() client id: f_00007-1-2 loss: 0.773557  [   96/  179]
train() client id: f_00007-1-3 loss: 0.615204  [  128/  179]
train() client id: f_00007-1-4 loss: 0.446700  [  160/  179]
train() client id: f_00007-2-0 loss: 0.691665  [   32/  179]
train() client id: f_00007-2-1 loss: 0.464124  [   64/  179]
train() client id: f_00007-2-2 loss: 0.691516  [   96/  179]
train() client id: f_00007-2-3 loss: 0.700182  [  128/  179]
train() client id: f_00007-2-4 loss: 0.564042  [  160/  179]
train() client id: f_00007-3-0 loss: 0.500973  [   32/  179]
train() client id: f_00007-3-1 loss: 0.827771  [   64/  179]
train() client id: f_00007-3-2 loss: 0.425459  [   96/  179]
train() client id: f_00007-3-3 loss: 0.638146  [  128/  179]
train() client id: f_00007-3-4 loss: 0.663607  [  160/  179]
train() client id: f_00007-4-0 loss: 0.648485  [   32/  179]
train() client id: f_00007-4-1 loss: 0.487158  [   64/  179]
train() client id: f_00007-4-2 loss: 0.822299  [   96/  179]
train() client id: f_00007-4-3 loss: 0.565461  [  128/  179]
train() client id: f_00007-4-4 loss: 0.577094  [  160/  179]
train() client id: f_00007-5-0 loss: 0.428748  [   32/  179]
train() client id: f_00007-5-1 loss: 0.729467  [   64/  179]
train() client id: f_00007-5-2 loss: 0.572235  [   96/  179]
train() client id: f_00007-5-3 loss: 0.686732  [  128/  179]
train() client id: f_00007-5-4 loss: 0.679790  [  160/  179]
train() client id: f_00007-6-0 loss: 0.568223  [   32/  179]
train() client id: f_00007-6-1 loss: 0.544465  [   64/  179]
train() client id: f_00007-6-2 loss: 0.766649  [   96/  179]
train() client id: f_00007-6-3 loss: 0.546232  [  128/  179]
train() client id: f_00007-6-4 loss: 0.644735  [  160/  179]
train() client id: f_00007-7-0 loss: 0.641572  [   32/  179]
train() client id: f_00007-7-1 loss: 0.543997  [   64/  179]
train() client id: f_00007-7-2 loss: 0.639163  [   96/  179]
train() client id: f_00007-7-3 loss: 0.472484  [  128/  179]
train() client id: f_00007-7-4 loss: 0.692881  [  160/  179]
train() client id: f_00007-8-0 loss: 0.733346  [   32/  179]
train() client id: f_00007-8-1 loss: 0.519764  [   64/  179]
train() client id: f_00007-8-2 loss: 0.481538  [   96/  179]
train() client id: f_00007-8-3 loss: 0.774688  [  128/  179]
train() client id: f_00007-8-4 loss: 0.506281  [  160/  179]
train() client id: f_00008-0-0 loss: 0.697767  [   32/  130]
train() client id: f_00008-0-1 loss: 0.635755  [   64/  130]
train() client id: f_00008-0-2 loss: 0.687596  [   96/  130]
train() client id: f_00008-0-3 loss: 0.708077  [  128/  130]
train() client id: f_00008-1-0 loss: 0.690950  [   32/  130]
train() client id: f_00008-1-1 loss: 0.586955  [   64/  130]
train() client id: f_00008-1-2 loss: 0.679770  [   96/  130]
train() client id: f_00008-1-3 loss: 0.770976  [  128/  130]
train() client id: f_00008-2-0 loss: 0.640865  [   32/  130]
train() client id: f_00008-2-1 loss: 0.679346  [   64/  130]
train() client id: f_00008-2-2 loss: 0.604783  [   96/  130]
train() client id: f_00008-2-3 loss: 0.780888  [  128/  130]
train() client id: f_00008-3-0 loss: 0.759725  [   32/  130]
train() client id: f_00008-3-1 loss: 0.682363  [   64/  130]
train() client id: f_00008-3-2 loss: 0.671590  [   96/  130]
train() client id: f_00008-3-3 loss: 0.577732  [  128/  130]
train() client id: f_00008-4-0 loss: 0.603286  [   32/  130]
train() client id: f_00008-4-1 loss: 0.829425  [   64/  130]
train() client id: f_00008-4-2 loss: 0.659456  [   96/  130]
train() client id: f_00008-4-3 loss: 0.622121  [  128/  130]
train() client id: f_00008-5-0 loss: 0.691693  [   32/  130]
train() client id: f_00008-5-1 loss: 0.638499  [   64/  130]
train() client id: f_00008-5-2 loss: 0.742532  [   96/  130]
train() client id: f_00008-5-3 loss: 0.677602  [  128/  130]
train() client id: f_00008-6-0 loss: 0.696250  [   32/  130]
train() client id: f_00008-6-1 loss: 0.652825  [   64/  130]
train() client id: f_00008-6-2 loss: 0.662319  [   96/  130]
train() client id: f_00008-6-3 loss: 0.739234  [  128/  130]
train() client id: f_00008-7-0 loss: 0.729953  [   32/  130]
train() client id: f_00008-7-1 loss: 0.704028  [   64/  130]
train() client id: f_00008-7-2 loss: 0.626103  [   96/  130]
train() client id: f_00008-7-3 loss: 0.673761  [  128/  130]
train() client id: f_00008-8-0 loss: 0.823676  [   32/  130]
train() client id: f_00008-8-1 loss: 0.689632  [   64/  130]
train() client id: f_00008-8-2 loss: 0.621204  [   96/  130]
train() client id: f_00008-8-3 loss: 0.617805  [  128/  130]
train() client id: f_00009-0-0 loss: 1.107203  [   32/  118]
train() client id: f_00009-0-1 loss: 0.672065  [   64/  118]
train() client id: f_00009-0-2 loss: 0.876938  [   96/  118]
train() client id: f_00009-1-0 loss: 0.812523  [   32/  118]
train() client id: f_00009-1-1 loss: 0.735409  [   64/  118]
train() client id: f_00009-1-2 loss: 1.067507  [   96/  118]
train() client id: f_00009-2-0 loss: 0.750048  [   32/  118]
train() client id: f_00009-2-1 loss: 0.849270  [   64/  118]
train() client id: f_00009-2-2 loss: 0.828377  [   96/  118]
train() client id: f_00009-3-0 loss: 0.727053  [   32/  118]
train() client id: f_00009-3-1 loss: 0.939168  [   64/  118]
train() client id: f_00009-3-2 loss: 0.743136  [   96/  118]
train() client id: f_00009-4-0 loss: 0.925512  [   32/  118]
train() client id: f_00009-4-1 loss: 0.624445  [   64/  118]
train() client id: f_00009-4-2 loss: 0.867255  [   96/  118]
train() client id: f_00009-5-0 loss: 0.698351  [   32/  118]
train() client id: f_00009-5-1 loss: 0.882850  [   64/  118]
train() client id: f_00009-5-2 loss: 0.689523  [   96/  118]
train() client id: f_00009-6-0 loss: 0.742877  [   32/  118]
train() client id: f_00009-6-1 loss: 0.563753  [   64/  118]
train() client id: f_00009-6-2 loss: 0.870544  [   96/  118]
train() client id: f_00009-7-0 loss: 0.716800  [   32/  118]
train() client id: f_00009-7-1 loss: 0.669131  [   64/  118]
train() client id: f_00009-7-2 loss: 0.735570  [   96/  118]
train() client id: f_00009-8-0 loss: 0.740431  [   32/  118]
train() client id: f_00009-8-1 loss: 0.542973  [   64/  118]
train() client id: f_00009-8-2 loss: 0.778551  [   96/  118]
At round 65 accuracy: 0.6472148541114059
At round 65 training accuracy: 0.5942320590207915
At round 65 training loss: 0.8293156956094916
update_location
xs = [  -3.9056584     4.20031788  345.00902392   18.81129433    0.97929623
    3.95640986 -307.44319194 -286.32485185  329.66397685 -272.06087855]
ys = [ 337.5879595   320.55583871    1.32061395 -307.45517586  299.35018685
  282.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [352.10919409 335.81793937 359.21159587 323.85575486 315.61288533
 299.99916173 323.30822262 303.28632848 344.94493416 289.88469   ]
dists_bs = [236.02567297 230.86631653 548.24621667 519.92866292 215.48011896
 208.92411955 221.5667713  206.71496584 528.64945606 196.62478831]
uav_gains = [1.17856502e-12 1.46410888e-12 1.08255453e-12 1.75503525e-12
 2.01172458e-12 2.67278145e-12 1.77049497e-12 2.51097274e-12
 1.29138642e-12 3.26345136e-12]
bs_gains = [2.50615657e-11 2.66614938e-11 2.36680394e-12 2.74569035e-12
 3.23410344e-11 3.52635523e-11 2.99144570e-11 3.63289386e-11
 2.62074203e-12 4.17933260e-11]
Round 66
-------------------------------
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.28261697 4.54685792 2.2554939  0.84667744 5.24112296 2.52309512
 1.03299146 3.13864151 2.30469629 2.04601254]
obj_prev = 26.218206122007363
eta_min = 1.468265776943342e-41	eta_max = 0.944257265961695
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 5.987676154852869	eta = 0.9090909090909091
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 15.480452461148737	eta = 0.3516267998379439
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 10.14509351030873	eta = 0.5365492149900848
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 9.229709829328627	eta = 0.5897630651031099
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 9.172211188857027	eta = 0.5934601642807859
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 9.17195028532138	eta = 0.5934770457345999
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 9.17195027990423	eta = 0.5934770460851201
eta = 0.5934770460851201
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [0.04305412 0.09055032 0.04237072 0.01469307 0.10456    0.0498881
 0.01845176 0.06116418 0.04442091 0.04032052]
ene_total = [0.9382438  1.35500986 0.94553393 0.48503672 1.54222126 0.79045116
 0.53538055 1.0793304  0.84532808 0.65541452]
ti_comp = [1.69339463 1.86443235 1.68124436 1.74028295 1.86800619 1.86951147
 1.74115692 1.77160865 1.77482088 1.87231038]
ti_coms = [0.25366796 0.08263025 0.26581824 0.20677964 0.07905641 0.07755113
 0.20590568 0.17545395 0.17224172 0.07475222]
t_total = [26.69972305 26.69972305 26.69972305 26.69972305 26.69972305 26.69972305
 26.69972305 26.69972305 26.69972305 26.69972305]
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [1.73943290e-06 1.33492469e-05 1.68196045e-06 6.54603068e-08
 2.04748185e-05 2.22031557e-06 1.29514340e-07 4.55654761e-06
 1.73913543e-06 1.16869917e-06]
ene_total = [0.34787422 0.11349245 0.36453489 0.28355414 0.10868941 0.10637493
 0.28235657 0.24065936 0.23621585 0.10252241]
optimize_network iter = 0 obj = 2.186274239551174
eta = 0.5934770460851201
freqs = [12712369.24332196 24283617.19415748 12600999.04046834  4221460.42224169
 27987058.04437446 13342550.5059045   5298707.90102926 17262328.31372014
 12514196.1057277  10767584.65528894]
eta_min = 0.5934770460851205	eta_max = 0.7588338965075888
af = 0.0005034964568992424	bf = 0.8885079592597985	zeta = 0.0005538461025891666	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [3.17745128e-07 2.43852935e-06 3.07246539e-07 1.19577441e-08
 3.74016947e-06 4.05588772e-07 2.36586018e-08 8.32352201e-07
 3.17690788e-07 2.13488239e-07]
ene_total = [1.55552023 0.5068411  1.6300255  1.26798066 0.48500598 0.47557109
 1.26262219 1.07594073 1.05621167 0.45839627]
ti_comp = [0.90140949 1.07244721 0.88925922 0.94829781 1.07602105 1.07752633
 0.94917178 0.97962351 0.98283574 1.08032524]
ti_coms = [0.25366796 0.08263025 0.26581824 0.20677964 0.07905641 0.07755113
 0.20590568 0.17545395 0.17224172 0.07475222]
t_total = [26.69972305 26.69972305 26.69972305 26.69972305 26.69972305 26.69972305
 26.69972305 26.69972305 26.69972305 26.69972305]
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [9.08390094e-07 5.97022622e-06 8.89637979e-07 3.26227732e-08
 9.13119788e-06 9.89026201e-07 6.44906348e-08 2.20518843e-06
 8.39214901e-07 5.19447888e-07]
ene_total = [0.58637686 0.1911386  0.6144619  0.47797382 0.18295069 0.17928302
 0.47595438 0.40561441 0.39815774 0.17280245]
optimize_network iter = 1 obj = 3.6847138773635306
eta = 0.7588338965075888
freqs = [12631650.06371988 22329634.8077465  12600999.04046835  4097654.84679903
 25698769.41757561 12244375.3475877   5141153.96101874 16512221.31849963
 11952921.87158274  9870499.99002256]
eta_min = 0.7588338965075906	eta_max = 0.7588338965075868
af = 0.00043529367837054833	bf = 0.8885079592597985	zeta = 0.0004788230462076032	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [3.13722794e-07 2.06188515e-06 3.07246539e-07 1.12666437e-08
 3.15356247e-06 3.41571386e-07 2.22725702e-08 7.61586768e-07
 2.89832359e-07 1.79397204e-07]
ene_total = [1.55551998 0.506818   1.6300255  1.26798062 0.48497001 0.47556716
 1.2626221  1.0759364  1.05620996 0.45839418]
ti_comp = [0.90140949 1.07244721 0.88925922 0.94829781 1.07602105 1.07752633
 0.94917178 0.97962351 0.98283574 1.08032524]
ti_coms = [0.25366796 0.08263025 0.26581824 0.20677964 0.07905641 0.07755113
 0.20590568 0.17545395 0.17224172 0.07475222]
t_total = [26.69972305 26.69972305 26.69972305 26.69972305 26.69972305 26.69972305
 26.69972305 26.69972305 26.69972305 26.69972305]
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [9.08390094e-07 5.97022622e-06 8.89637979e-07 3.26227732e-08
 9.13119788e-06 9.89026201e-07 6.44906348e-08 2.20518843e-06
 8.39214901e-07 5.19447888e-07]
ene_total = [0.58637686 0.1911386  0.6144619  0.47797382 0.18295069 0.17928302
 0.47595438 0.40561441 0.39815774 0.17280245]
optimize_network iter = 2 obj = 3.6847138773635004
eta = 0.7588338965075868
freqs = [12631650.06371987 22329634.80774652 12600999.04046834  4097654.84679903
 25698769.41757564 12244375.34758771  5141153.96101873 16512221.31849963
 11952921.87158274  9870499.99002257]
Done!
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [9.04697167e-07 5.94595514e-06 8.86021287e-07 3.24901501e-08
 9.09407634e-06 9.85005461e-07 6.42284576e-08 2.19622356e-06
 8.35803196e-07 5.17336149e-07]
ene_total = [0.0253677  0.00826897 0.02658271 0.020678   0.00791473 0.0077561
 0.02059063 0.01754759 0.01722501 0.00747574]
At round 66 energy consumption: 0.15940718136832308
At round 66 eta: 0.7588338965075868
At round 66 a_n: 5.2320311093101175
At round 66 local rounds: 9.036737525084137
At round 66 global rounds: 23.115093189100694
gradient difference: 0.5194600820541382
train() client id: f_00000-0-0 loss: 1.149231  [   32/  126]
train() client id: f_00000-0-1 loss: 1.001873  [   64/  126]
train() client id: f_00000-0-2 loss: 0.862556  [   96/  126]
train() client id: f_00000-1-0 loss: 1.015969  [   32/  126]
train() client id: f_00000-1-1 loss: 0.875066  [   64/  126]
train() client id: f_00000-1-2 loss: 1.084842  [   96/  126]
train() client id: f_00000-2-0 loss: 0.847322  [   32/  126]
train() client id: f_00000-2-1 loss: 1.031231  [   64/  126]
train() client id: f_00000-2-2 loss: 0.902256  [   96/  126]
train() client id: f_00000-3-0 loss: 1.081578  [   32/  126]
train() client id: f_00000-3-1 loss: 0.736666  [   64/  126]
train() client id: f_00000-3-2 loss: 0.895211  [   96/  126]
train() client id: f_00000-4-0 loss: 0.787306  [   32/  126]
train() client id: f_00000-4-1 loss: 0.890105  [   64/  126]
train() client id: f_00000-4-2 loss: 0.880490  [   96/  126]
train() client id: f_00000-5-0 loss: 0.795351  [   32/  126]
train() client id: f_00000-5-1 loss: 0.865181  [   64/  126]
train() client id: f_00000-5-2 loss: 0.911483  [   96/  126]
train() client id: f_00000-6-0 loss: 0.935458  [   32/  126]
train() client id: f_00000-6-1 loss: 0.768258  [   64/  126]
train() client id: f_00000-6-2 loss: 0.829782  [   96/  126]
train() client id: f_00000-7-0 loss: 0.778103  [   32/  126]
train() client id: f_00000-7-1 loss: 0.797613  [   64/  126]
train() client id: f_00000-7-2 loss: 0.716265  [   96/  126]
train() client id: f_00000-8-0 loss: 0.934774  [   32/  126]
train() client id: f_00000-8-1 loss: 0.774105  [   64/  126]
train() client id: f_00000-8-2 loss: 0.828155  [   96/  126]
train() client id: f_00001-0-0 loss: 0.586782  [   32/  265]
train() client id: f_00001-0-1 loss: 0.467084  [   64/  265]
train() client id: f_00001-0-2 loss: 0.555148  [   96/  265]
train() client id: f_00001-0-3 loss: 0.742892  [  128/  265]
train() client id: f_00001-0-4 loss: 0.466538  [  160/  265]
train() client id: f_00001-0-5 loss: 0.427854  [  192/  265]
train() client id: f_00001-0-6 loss: 0.600335  [  224/  265]
train() client id: f_00001-0-7 loss: 0.496413  [  256/  265]
train() client id: f_00001-1-0 loss: 0.557918  [   32/  265]
train() client id: f_00001-1-1 loss: 0.557474  [   64/  265]
train() client id: f_00001-1-2 loss: 0.511817  [   96/  265]
train() client id: f_00001-1-3 loss: 0.646022  [  128/  265]
train() client id: f_00001-1-4 loss: 0.550344  [  160/  265]
train() client id: f_00001-1-5 loss: 0.425831  [  192/  265]
train() client id: f_00001-1-6 loss: 0.530667  [  224/  265]
train() client id: f_00001-1-7 loss: 0.498861  [  256/  265]
train() client id: f_00001-2-0 loss: 0.521355  [   32/  265]
train() client id: f_00001-2-1 loss: 0.652913  [   64/  265]
train() client id: f_00001-2-2 loss: 0.543947  [   96/  265]
train() client id: f_00001-2-3 loss: 0.462026  [  128/  265]
train() client id: f_00001-2-4 loss: 0.422255  [  160/  265]
train() client id: f_00001-2-5 loss: 0.488534  [  192/  265]
train() client id: f_00001-2-6 loss: 0.534570  [  224/  265]
train() client id: f_00001-2-7 loss: 0.656188  [  256/  265]
train() client id: f_00001-3-0 loss: 0.535618  [   32/  265]
train() client id: f_00001-3-1 loss: 0.598203  [   64/  265]
train() client id: f_00001-3-2 loss: 0.509349  [   96/  265]
train() client id: f_00001-3-3 loss: 0.573918  [  128/  265]
train() client id: f_00001-3-4 loss: 0.516787  [  160/  265]
train() client id: f_00001-3-5 loss: 0.551377  [  192/  265]
train() client id: f_00001-3-6 loss: 0.489703  [  224/  265]
train() client id: f_00001-3-7 loss: 0.507005  [  256/  265]
train() client id: f_00001-4-0 loss: 0.506347  [   32/  265]
train() client id: f_00001-4-1 loss: 0.445424  [   64/  265]
train() client id: f_00001-4-2 loss: 0.536156  [   96/  265]
train() client id: f_00001-4-3 loss: 0.511632  [  128/  265]
train() client id: f_00001-4-4 loss: 0.501424  [  160/  265]
train() client id: f_00001-4-5 loss: 0.523964  [  192/  265]
train() client id: f_00001-4-6 loss: 0.743132  [  224/  265]
train() client id: f_00001-4-7 loss: 0.516357  [  256/  265]
train() client id: f_00001-5-0 loss: 0.499012  [   32/  265]
train() client id: f_00001-5-1 loss: 0.491930  [   64/  265]
train() client id: f_00001-5-2 loss: 0.511788  [   96/  265]
train() client id: f_00001-5-3 loss: 0.613030  [  128/  265]
train() client id: f_00001-5-4 loss: 0.492462  [  160/  265]
train() client id: f_00001-5-5 loss: 0.532852  [  192/  265]
train() client id: f_00001-5-6 loss: 0.535228  [  224/  265]
train() client id: f_00001-5-7 loss: 0.479880  [  256/  265]
train() client id: f_00001-6-0 loss: 0.485196  [   32/  265]
train() client id: f_00001-6-1 loss: 0.543914  [   64/  265]
train() client id: f_00001-6-2 loss: 0.659659  [   96/  265]
train() client id: f_00001-6-3 loss: 0.510032  [  128/  265]
train() client id: f_00001-6-4 loss: 0.441150  [  160/  265]
train() client id: f_00001-6-5 loss: 0.510892  [  192/  265]
train() client id: f_00001-6-6 loss: 0.521306  [  224/  265]
train() client id: f_00001-6-7 loss: 0.617395  [  256/  265]
train() client id: f_00001-7-0 loss: 0.442420  [   32/  265]
train() client id: f_00001-7-1 loss: 0.444600  [   64/  265]
train() client id: f_00001-7-2 loss: 0.483457  [   96/  265]
train() client id: f_00001-7-3 loss: 0.475759  [  128/  265]
train() client id: f_00001-7-4 loss: 0.601966  [  160/  265]
train() client id: f_00001-7-5 loss: 0.681222  [  192/  265]
train() client id: f_00001-7-6 loss: 0.428046  [  224/  265]
train() client id: f_00001-7-7 loss: 0.606453  [  256/  265]
train() client id: f_00001-8-0 loss: 0.462081  [   32/  265]
train() client id: f_00001-8-1 loss: 0.724025  [   64/  265]
train() client id: f_00001-8-2 loss: 0.532742  [   96/  265]
train() client id: f_00001-8-3 loss: 0.448814  [  128/  265]
train() client id: f_00001-8-4 loss: 0.455374  [  160/  265]
train() client id: f_00001-8-5 loss: 0.646575  [  192/  265]
train() client id: f_00001-8-6 loss: 0.504615  [  224/  265]
train() client id: f_00001-8-7 loss: 0.529019  [  256/  265]
train() client id: f_00002-0-0 loss: 1.223097  [   32/  124]
train() client id: f_00002-0-1 loss: 1.359079  [   64/  124]
train() client id: f_00002-0-2 loss: 0.980276  [   96/  124]
train() client id: f_00002-1-0 loss: 1.195021  [   32/  124]
train() client id: f_00002-1-1 loss: 1.290108  [   64/  124]
train() client id: f_00002-1-2 loss: 1.267732  [   96/  124]
train() client id: f_00002-2-0 loss: 1.286843  [   32/  124]
train() client id: f_00002-2-1 loss: 1.179561  [   64/  124]
train() client id: f_00002-2-2 loss: 1.200805  [   96/  124]
train() client id: f_00002-3-0 loss: 1.030198  [   32/  124]
train() client id: f_00002-3-1 loss: 1.310054  [   64/  124]
train() client id: f_00002-3-2 loss: 1.140180  [   96/  124]
train() client id: f_00002-4-0 loss: 1.201867  [   32/  124]
train() client id: f_00002-4-1 loss: 1.392466  [   64/  124]
train() client id: f_00002-4-2 loss: 1.000543  [   96/  124]
train() client id: f_00002-5-0 loss: 1.175054  [   32/  124]
train() client id: f_00002-5-1 loss: 1.255381  [   64/  124]
train() client id: f_00002-5-2 loss: 1.055501  [   96/  124]
train() client id: f_00002-6-0 loss: 1.224971  [   32/  124]
train() client id: f_00002-6-1 loss: 1.010087  [   64/  124]
train() client id: f_00002-6-2 loss: 1.302325  [   96/  124]
train() client id: f_00002-7-0 loss: 1.135079  [   32/  124]
train() client id: f_00002-7-1 loss: 0.988221  [   64/  124]
train() client id: f_00002-7-2 loss: 1.359917  [   96/  124]
train() client id: f_00002-8-0 loss: 1.319472  [   32/  124]
train() client id: f_00002-8-1 loss: 1.164862  [   64/  124]
train() client id: f_00002-8-2 loss: 1.176481  [   96/  124]
train() client id: f_00003-0-0 loss: 0.841902  [   32/   43]
train() client id: f_00003-1-0 loss: 0.897683  [   32/   43]
train() client id: f_00003-2-0 loss: 0.678727  [   32/   43]
train() client id: f_00003-3-0 loss: 0.871615  [   32/   43]
train() client id: f_00003-4-0 loss: 0.989275  [   32/   43]
train() client id: f_00003-5-0 loss: 0.992230  [   32/   43]
train() client id: f_00003-6-0 loss: 0.874173  [   32/   43]
train() client id: f_00003-7-0 loss: 0.591153  [   32/   43]
train() client id: f_00003-8-0 loss: 0.704349  [   32/   43]
train() client id: f_00004-0-0 loss: 0.818224  [   32/  306]
train() client id: f_00004-0-1 loss: 0.783691  [   64/  306]
train() client id: f_00004-0-2 loss: 0.708188  [   96/  306]
train() client id: f_00004-0-3 loss: 0.735373  [  128/  306]
train() client id: f_00004-0-4 loss: 1.008201  [  160/  306]
train() client id: f_00004-0-5 loss: 0.689248  [  192/  306]
train() client id: f_00004-0-6 loss: 0.919666  [  224/  306]
train() client id: f_00004-0-7 loss: 0.770736  [  256/  306]
train() client id: f_00004-0-8 loss: 0.869619  [  288/  306]
train() client id: f_00004-1-0 loss: 0.765788  [   32/  306]
train() client id: f_00004-1-1 loss: 0.679506  [   64/  306]
train() client id: f_00004-1-2 loss: 0.840798  [   96/  306]
train() client id: f_00004-1-3 loss: 0.895021  [  128/  306]
train() client id: f_00004-1-4 loss: 0.653237  [  160/  306]
train() client id: f_00004-1-5 loss: 0.820608  [  192/  306]
train() client id: f_00004-1-6 loss: 0.730809  [  224/  306]
train() client id: f_00004-1-7 loss: 0.903897  [  256/  306]
train() client id: f_00004-1-8 loss: 0.870311  [  288/  306]
train() client id: f_00004-2-0 loss: 0.754020  [   32/  306]
train() client id: f_00004-2-1 loss: 0.917379  [   64/  306]
train() client id: f_00004-2-2 loss: 0.666783  [   96/  306]
train() client id: f_00004-2-3 loss: 0.770309  [  128/  306]
train() client id: f_00004-2-4 loss: 0.733325  [  160/  306]
train() client id: f_00004-2-5 loss: 0.834397  [  192/  306]
train() client id: f_00004-2-6 loss: 0.836022  [  224/  306]
train() client id: f_00004-2-7 loss: 0.847635  [  256/  306]
train() client id: f_00004-2-8 loss: 0.901672  [  288/  306]
train() client id: f_00004-3-0 loss: 0.688466  [   32/  306]
train() client id: f_00004-3-1 loss: 0.797392  [   64/  306]
train() client id: f_00004-3-2 loss: 0.724622  [   96/  306]
train() client id: f_00004-3-3 loss: 0.818792  [  128/  306]
train() client id: f_00004-3-4 loss: 0.856612  [  160/  306]
train() client id: f_00004-3-5 loss: 0.721167  [  192/  306]
train() client id: f_00004-3-6 loss: 0.845107  [  224/  306]
train() client id: f_00004-3-7 loss: 0.775299  [  256/  306]
train() client id: f_00004-3-8 loss: 0.832393  [  288/  306]
train() client id: f_00004-4-0 loss: 0.721384  [   32/  306]
train() client id: f_00004-4-1 loss: 0.862444  [   64/  306]
train() client id: f_00004-4-2 loss: 0.905357  [   96/  306]
train() client id: f_00004-4-3 loss: 0.932987  [  128/  306]
train() client id: f_00004-4-4 loss: 0.766906  [  160/  306]
train() client id: f_00004-4-5 loss: 0.745018  [  192/  306]
train() client id: f_00004-4-6 loss: 0.835759  [  224/  306]
train() client id: f_00004-4-7 loss: 0.681880  [  256/  306]
train() client id: f_00004-4-8 loss: 0.742300  [  288/  306]
train() client id: f_00004-5-0 loss: 0.716552  [   32/  306]
train() client id: f_00004-5-1 loss: 0.778763  [   64/  306]
train() client id: f_00004-5-2 loss: 0.851639  [   96/  306]
train() client id: f_00004-5-3 loss: 0.763599  [  128/  306]
train() client id: f_00004-5-4 loss: 0.740346  [  160/  306]
train() client id: f_00004-5-5 loss: 0.738414  [  192/  306]
train() client id: f_00004-5-6 loss: 0.933175  [  224/  306]
train() client id: f_00004-5-7 loss: 0.730966  [  256/  306]
train() client id: f_00004-5-8 loss: 0.918158  [  288/  306]
train() client id: f_00004-6-0 loss: 0.832972  [   32/  306]
train() client id: f_00004-6-1 loss: 0.683410  [   64/  306]
train() client id: f_00004-6-2 loss: 0.842275  [   96/  306]
train() client id: f_00004-6-3 loss: 0.734909  [  128/  306]
train() client id: f_00004-6-4 loss: 0.676859  [  160/  306]
train() client id: f_00004-6-5 loss: 0.915422  [  192/  306]
train() client id: f_00004-6-6 loss: 0.773768  [  224/  306]
train() client id: f_00004-6-7 loss: 0.885467  [  256/  306]
train() client id: f_00004-6-8 loss: 0.823462  [  288/  306]
train() client id: f_00004-7-0 loss: 0.964081  [   32/  306]
train() client id: f_00004-7-1 loss: 0.873629  [   64/  306]
train() client id: f_00004-7-2 loss: 0.747357  [   96/  306]
train() client id: f_00004-7-3 loss: 0.797705  [  128/  306]
train() client id: f_00004-7-4 loss: 0.976563  [  160/  306]
train() client id: f_00004-7-5 loss: 0.721297  [  192/  306]
train() client id: f_00004-7-6 loss: 0.775916  [  224/  306]
train() client id: f_00004-7-7 loss: 0.605535  [  256/  306]
train() client id: f_00004-7-8 loss: 0.720618  [  288/  306]
train() client id: f_00004-8-0 loss: 0.765384  [   32/  306]
train() client id: f_00004-8-1 loss: 0.791719  [   64/  306]
train() client id: f_00004-8-2 loss: 0.724716  [   96/  306]
train() client id: f_00004-8-3 loss: 0.697652  [  128/  306]
train() client id: f_00004-8-4 loss: 0.744055  [  160/  306]
train() client id: f_00004-8-5 loss: 0.788702  [  192/  306]
train() client id: f_00004-8-6 loss: 0.855911  [  224/  306]
train() client id: f_00004-8-7 loss: 0.859330  [  256/  306]
train() client id: f_00004-8-8 loss: 0.821811  [  288/  306]
train() client id: f_00005-0-0 loss: 0.765918  [   32/  146]
train() client id: f_00005-0-1 loss: 0.323969  [   64/  146]
train() client id: f_00005-0-2 loss: 0.323792  [   96/  146]
train() client id: f_00005-0-3 loss: 0.618693  [  128/  146]
train() client id: f_00005-1-0 loss: 0.599442  [   32/  146]
train() client id: f_00005-1-1 loss: 0.445671  [   64/  146]
train() client id: f_00005-1-2 loss: 0.437217  [   96/  146]
train() client id: f_00005-1-3 loss: 0.881064  [  128/  146]
train() client id: f_00005-2-0 loss: 0.612256  [   32/  146]
train() client id: f_00005-2-1 loss: 0.370839  [   64/  146]
train() client id: f_00005-2-2 loss: 0.762240  [   96/  146]
train() client id: f_00005-2-3 loss: 0.430445  [  128/  146]
train() client id: f_00005-3-0 loss: 0.560717  [   32/  146]
train() client id: f_00005-3-1 loss: 0.659357  [   64/  146]
train() client id: f_00005-3-2 loss: 0.810966  [   96/  146]
train() client id: f_00005-3-3 loss: 0.431734  [  128/  146]
train() client id: f_00005-4-0 loss: 0.555736  [   32/  146]
train() client id: f_00005-4-1 loss: 0.340806  [   64/  146]
train() client id: f_00005-4-2 loss: 0.770404  [   96/  146]
train() client id: f_00005-4-3 loss: 0.593071  [  128/  146]
train() client id: f_00005-5-0 loss: 0.667308  [   32/  146]
train() client id: f_00005-5-1 loss: 0.361748  [   64/  146]
train() client id: f_00005-5-2 loss: 0.723027  [   96/  146]
train() client id: f_00005-5-3 loss: 0.413065  [  128/  146]
train() client id: f_00005-6-0 loss: 0.693976  [   32/  146]
train() client id: f_00005-6-1 loss: 0.336184  [   64/  146]
train() client id: f_00005-6-2 loss: 0.650196  [   96/  146]
train() client id: f_00005-6-3 loss: 0.571476  [  128/  146]
train() client id: f_00005-7-0 loss: 0.661112  [   32/  146]
train() client id: f_00005-7-1 loss: 0.883066  [   64/  146]
train() client id: f_00005-7-2 loss: 0.214958  [   96/  146]
train() client id: f_00005-7-3 loss: 0.489635  [  128/  146]
train() client id: f_00005-8-0 loss: 0.622028  [   32/  146]
train() client id: f_00005-8-1 loss: 0.697038  [   64/  146]
train() client id: f_00005-8-2 loss: 0.469335  [   96/  146]
train() client id: f_00005-8-3 loss: 0.619247  [  128/  146]
train() client id: f_00006-0-0 loss: 0.460632  [   32/   54]
train() client id: f_00006-1-0 loss: 0.410389  [   32/   54]
train() client id: f_00006-2-0 loss: 0.461316  [   32/   54]
train() client id: f_00006-3-0 loss: 0.430131  [   32/   54]
train() client id: f_00006-4-0 loss: 0.517468  [   32/   54]
train() client id: f_00006-5-0 loss: 0.504313  [   32/   54]
train() client id: f_00006-6-0 loss: 0.511574  [   32/   54]
train() client id: f_00006-7-0 loss: 0.455423  [   32/   54]
train() client id: f_00006-8-0 loss: 0.530470  [   32/   54]
train() client id: f_00007-0-0 loss: 0.575971  [   32/  179]
train() client id: f_00007-0-1 loss: 0.586890  [   64/  179]
train() client id: f_00007-0-2 loss: 0.736024  [   96/  179]
train() client id: f_00007-0-3 loss: 0.960031  [  128/  179]
train() client id: f_00007-0-4 loss: 0.580640  [  160/  179]
train() client id: f_00007-1-0 loss: 0.488513  [   32/  179]
train() client id: f_00007-1-1 loss: 0.661862  [   64/  179]
train() client id: f_00007-1-2 loss: 0.922084  [   96/  179]
train() client id: f_00007-1-3 loss: 0.733700  [  128/  179]
train() client id: f_00007-1-4 loss: 0.564898  [  160/  179]
train() client id: f_00007-2-0 loss: 0.533996  [   32/  179]
train() client id: f_00007-2-1 loss: 0.626654  [   64/  179]
train() client id: f_00007-2-2 loss: 0.747188  [   96/  179]
train() client id: f_00007-2-3 loss: 0.629782  [  128/  179]
train() client id: f_00007-2-4 loss: 0.573037  [  160/  179]
train() client id: f_00007-3-0 loss: 0.571907  [   32/  179]
train() client id: f_00007-3-1 loss: 0.526320  [   64/  179]
train() client id: f_00007-3-2 loss: 0.685432  [   96/  179]
train() client id: f_00007-3-3 loss: 0.635813  [  128/  179]
train() client id: f_00007-3-4 loss: 0.584298  [  160/  179]
train() client id: f_00007-4-0 loss: 0.491428  [   32/  179]
train() client id: f_00007-4-1 loss: 0.701108  [   64/  179]
train() client id: f_00007-4-2 loss: 0.754820  [   96/  179]
train() client id: f_00007-4-3 loss: 0.512959  [  128/  179]
train() client id: f_00007-4-4 loss: 0.814429  [  160/  179]
train() client id: f_00007-5-0 loss: 0.853551  [   32/  179]
train() client id: f_00007-5-1 loss: 0.557854  [   64/  179]
train() client id: f_00007-5-2 loss: 0.529691  [   96/  179]
train() client id: f_00007-5-3 loss: 0.711765  [  128/  179]
train() client id: f_00007-5-4 loss: 0.541305  [  160/  179]
train() client id: f_00007-6-0 loss: 0.552758  [   32/  179]
train() client id: f_00007-6-1 loss: 0.884561  [   64/  179]
train() client id: f_00007-6-2 loss: 0.585844  [   96/  179]
train() client id: f_00007-6-3 loss: 0.515674  [  128/  179]
train() client id: f_00007-6-4 loss: 0.526987  [  160/  179]
train() client id: f_00007-7-0 loss: 0.656893  [   32/  179]
train() client id: f_00007-7-1 loss: 0.765398  [   64/  179]
train() client id: f_00007-7-2 loss: 0.653122  [   96/  179]
train() client id: f_00007-7-3 loss: 0.557564  [  128/  179]
train() client id: f_00007-7-4 loss: 0.571721  [  160/  179]
train() client id: f_00007-8-0 loss: 0.524722  [   32/  179]
train() client id: f_00007-8-1 loss: 0.522441  [   64/  179]
train() client id: f_00007-8-2 loss: 0.621525  [   96/  179]
train() client id: f_00007-8-3 loss: 0.729077  [  128/  179]
train() client id: f_00007-8-4 loss: 0.769425  [  160/  179]
train() client id: f_00008-0-0 loss: 0.781374  [   32/  130]
train() client id: f_00008-0-1 loss: 0.817103  [   64/  130]
train() client id: f_00008-0-2 loss: 0.908208  [   96/  130]
train() client id: f_00008-0-3 loss: 0.771366  [  128/  130]
train() client id: f_00008-1-0 loss: 0.808404  [   32/  130]
train() client id: f_00008-1-1 loss: 0.792780  [   64/  130]
train() client id: f_00008-1-2 loss: 0.856023  [   96/  130]
train() client id: f_00008-1-3 loss: 0.828324  [  128/  130]
train() client id: f_00008-2-0 loss: 0.786586  [   32/  130]
train() client id: f_00008-2-1 loss: 0.817117  [   64/  130]
train() client id: f_00008-2-2 loss: 0.821644  [   96/  130]
train() client id: f_00008-2-3 loss: 0.834364  [  128/  130]
train() client id: f_00008-3-0 loss: 0.795829  [   32/  130]
train() client id: f_00008-3-1 loss: 0.784816  [   64/  130]
train() client id: f_00008-3-2 loss: 0.783270  [   96/  130]
train() client id: f_00008-3-3 loss: 0.921933  [  128/  130]
train() client id: f_00008-4-0 loss: 0.871585  [   32/  130]
train() client id: f_00008-4-1 loss: 0.773126  [   64/  130]
train() client id: f_00008-4-2 loss: 0.829869  [   96/  130]
train() client id: f_00008-4-3 loss: 0.778885  [  128/  130]
train() client id: f_00008-5-0 loss: 0.860834  [   32/  130]
train() client id: f_00008-5-1 loss: 0.897422  [   64/  130]
train() client id: f_00008-5-2 loss: 0.798239  [   96/  130]
train() client id: f_00008-5-3 loss: 0.718311  [  128/  130]
train() client id: f_00008-6-0 loss: 0.811820  [   32/  130]
train() client id: f_00008-6-1 loss: 0.856522  [   64/  130]
train() client id: f_00008-6-2 loss: 0.764154  [   96/  130]
train() client id: f_00008-6-3 loss: 0.840144  [  128/  130]
train() client id: f_00008-7-0 loss: 0.757929  [   32/  130]
train() client id: f_00008-7-1 loss: 0.734628  [   64/  130]
train() client id: f_00008-7-2 loss: 0.926227  [   96/  130]
train() client id: f_00008-7-3 loss: 0.827360  [  128/  130]
train() client id: f_00008-8-0 loss: 0.874941  [   32/  130]
train() client id: f_00008-8-1 loss: 0.812875  [   64/  130]
train() client id: f_00008-8-2 loss: 0.780257  [   96/  130]
train() client id: f_00008-8-3 loss: 0.795071  [  128/  130]
train() client id: f_00009-0-0 loss: 0.795448  [   32/  118]
train() client id: f_00009-0-1 loss: 0.984903  [   64/  118]
train() client id: f_00009-0-2 loss: 0.978637  [   96/  118]
train() client id: f_00009-1-0 loss: 0.886550  [   32/  118]
train() client id: f_00009-1-1 loss: 1.078426  [   64/  118]
train() client id: f_00009-1-2 loss: 0.870678  [   96/  118]
train() client id: f_00009-2-0 loss: 0.867850  [   32/  118]
train() client id: f_00009-2-1 loss: 0.910214  [   64/  118]
train() client id: f_00009-2-2 loss: 0.826465  [   96/  118]
train() client id: f_00009-3-0 loss: 1.022491  [   32/  118]
train() client id: f_00009-3-1 loss: 0.811819  [   64/  118]
train() client id: f_00009-3-2 loss: 0.760829  [   96/  118]
train() client id: f_00009-4-0 loss: 0.831876  [   32/  118]
train() client id: f_00009-4-1 loss: 0.831753  [   64/  118]
train() client id: f_00009-4-2 loss: 0.772791  [   96/  118]
train() client id: f_00009-5-0 loss: 0.863432  [   32/  118]
train() client id: f_00009-5-1 loss: 0.749175  [   64/  118]
train() client id: f_00009-5-2 loss: 0.831653  [   96/  118]
train() client id: f_00009-6-0 loss: 0.634712  [   32/  118]
train() client id: f_00009-6-1 loss: 0.808325  [   64/  118]
train() client id: f_00009-6-2 loss: 0.803090  [   96/  118]
train() client id: f_00009-7-0 loss: 0.825072  [   32/  118]
train() client id: f_00009-7-1 loss: 0.759307  [   64/  118]
train() client id: f_00009-7-2 loss: 0.647526  [   96/  118]
train() client id: f_00009-8-0 loss: 0.741083  [   32/  118]
train() client id: f_00009-8-1 loss: 0.893212  [   64/  118]
train() client id: f_00009-8-2 loss: 0.708780  [   96/  118]
At round 66 accuracy: 0.6472148541114059
At round 66 training accuracy: 0.5982562038900067
At round 66 training loss: 0.8001799645960471
update_location
xs = [  -3.9056584     4.20031788  350.00902392   18.81129433    0.97929623
    3.95640986 -312.44319194 -291.32485185  334.66397685 -277.06087855]
ys = [ 342.5879595   325.55583871    1.32061395 -312.45517586  304.35018685
  287.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [356.90581973 340.59396176 364.01656672 328.606302   320.35916602
 304.71730923 328.06651571 308.01111272 349.72653227 294.58231834]
dists_bs = [239.49738599 234.05088014 552.99096225 524.57160263 218.40371686
 211.54769968 224.59133123 209.45005519 533.42467806 199.14044332]
uav_gains = [1.11218118e-12 1.36922033e-12 1.02508951e-12 1.62931179e-12
 1.85748367e-12 2.44465100e-12 1.64287124e-12 2.30088160e-12
 1.21414591e-12 2.97056458e-12]
bs_gains = [2.40575783e-11 2.56581480e-11 2.31038093e-12 2.67818585e-12
 3.11433994e-11 3.40526430e-11 2.88000814e-11 3.50161782e-11
 2.55557949e-12 4.03317968e-11]
Round 67
-------------------------------
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.14668611 4.26785824 2.12126966 0.79869584 4.91942362 2.36837405
 0.97356032 2.94941557 2.16407019 1.92059068]
obj_prev = 24.629944275071566
eta_min = 3.445448803028318e-44	eta_max = 0.9459678027745403
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 5.6197462444887005	eta = 0.9090909090909091
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 14.777404826319295	eta = 0.34572107093955495
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 9.602497695143875	eta = 0.5320345168992939
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 8.719456153128892	eta = 0.585915008062652
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 8.663852506476184	eta = 0.5896753457475884
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 8.663598265580141	eta = 0.5896926502882287
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 8.66359826021894	eta = 0.5896926506531418
eta = 0.5896926506531418
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [0.04358199 0.09166053 0.04289021 0.01487322 0.10584197 0.05049976
 0.01867799 0.06191409 0.04496554 0.04081488]
ene_total = [0.88965002 1.27512756 0.89641868 0.46309031 1.45131187 0.74360323
 0.51049018 1.02200221 0.79540655 0.61649765]
ti_comp = [1.82806619 2.00655166 1.81584028 1.87549598 2.01019846 2.01177704
 1.87637173 1.90759375 1.91580954 2.01460717]
ti_coms = [0.26186319 0.08337772 0.27408909 0.2144334  0.07973092 0.07815233
 0.21355765 0.18233562 0.17411984 0.07532221]
t_total = [26.64971886 26.64971886 26.64971886 26.64971886 26.64971886 26.64971886
 26.64971886 26.64971886 26.64971886 26.64971886]
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [1.54816501e-06 1.19543613e-05 1.49554405e-06 5.84605015e-08
 1.83390085e-05 1.98878720e-06 1.15673257e-07 4.07639963e-06
 1.54815642e-06 1.04701937e-06]
ene_total = [0.33393439 0.10647147 0.34952357 0.27343533 0.10190267 0.09968125
 0.27231935 0.23255711 0.22204852 0.09606041]
optimize_network iter = 0 obj = 2.0879340736516
eta = 0.5896926506531418
freqs = [11920243.00940963 22840310.78845544 11810016.9072595   3965142.48672103
 26326248.17150466 12551033.48678927  4977157.22105609 16228322.10506473
 11735389.30408466 10129735.66511689]
eta_min = 0.589692650653142	eta_max = 0.7681327803953306
af = 0.00041776276143721885	bf = 0.8564740523773019	zeta = 0.00045953903758094077	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [2.79380519e-07 2.15727370e-06 2.69884586e-07 1.05497316e-08
 3.30944163e-06 3.58894819e-07 2.08742959e-08 7.35623554e-07
 2.79378970e-07 1.88944211e-07]
ene_total = [1.50710005 0.47998245 1.57746246 1.23411502 0.45906057 0.44980564
 1.22907549 1.04942654 1.00211652 0.43350782]
ti_comp = [0.91916882 1.09765429 0.90694292 0.96659861 1.10130109 1.10287968
 0.96747436 0.99869639 1.00691217 1.1057098 ]
ti_coms = [0.26186319 0.08337772 0.27408909 0.2144334  0.07973092 0.07815233
 0.21355765 0.18233562 0.17411984 0.07532221]
t_total = [26.64971886 26.64971886 26.64971886 26.64971886 26.64971886 26.64971886
 26.64971886 26.64971886 26.64971886 26.64971886]
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [7.62987513e-07 4.97740325e-06 7.46967171e-07 2.74226484e-08
 7.61287159e-06 8.24515199e-07 5.42124102e-08 1.85305581e-06
 6.98301926e-07 4.33071962e-07]
ene_total = [0.59090555 0.18825224 0.61849267 0.4838647  0.18008278 0.17636757
 0.4818892  0.411478   0.39291321 0.16997262]
optimize_network iter = 1 obj = 3.694218537284266
eta = 0.7681327803953306
freqs = [11840881.82931954 20853986.15252445 11810016.9072595   3842652.168879
 24000712.97449805 11434929.97175277  4821288.08222465 15482045.45566182
 11152199.96866208  9218274.46340382]
eta_min = 0.7681327803953335	eta_max = 0.7681327803953281
af = 0.0003568274848523184	bf = 0.8564740523773019	zeta = 0.00039251023333755023	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [2.75672850e-07 1.79837142e-06 2.69884586e-07 9.90799918e-09
 2.75058500e-06 2.97903243e-07 1.95873319e-08 6.69522327e-07
 2.52301484e-07 1.56472000e-07]
ene_total = [1.50709984 0.47996179 1.57746246 1.23411499 0.4590284  0.44980213
 1.22907542 1.04942273 1.00211496 0.43350595]
ti_comp = [0.91916882 1.09765429 0.90694292 0.96659861 1.10130109 1.10287968
 0.96747436 0.99869639 1.00691217 1.1057098 ]
ti_coms = [0.26186319 0.08337772 0.27408909 0.2144334  0.07973092 0.07815233
 0.21355765 0.18233562 0.17411984 0.07532221]
t_total = [26.64971886 26.64971886 26.64971886 26.64971886 26.64971886 26.64971886
 26.64971886 26.64971886 26.64971886 26.64971886]
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [7.62987513e-07 4.97740325e-06 7.46967171e-07 2.74226484e-08
 7.61287159e-06 8.24515199e-07 5.42124102e-08 1.85305581e-06
 6.98301926e-07 4.33071962e-07]
ene_total = [0.59090555 0.18825224 0.61849267 0.4838647  0.18008278 0.17636757
 0.4818892  0.411478   0.39291321 0.16997262]
optimize_network iter = 2 obj = 3.6942185372842253
eta = 0.7681327803953281
freqs = [11840881.82931953 20853986.15252446 11810016.90725948  3842652.168879
 24000712.97449807 11434929.97175278  4821288.08222465 15482045.45566182
 11152199.96866208  9218274.46340384]
Done!
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [7.06640672e-07 4.60982063e-06 6.91803437e-07 2.53974782e-08
 7.05065889e-06 7.63624521e-07 5.02088085e-08 1.71620712e-06
 6.46732134e-07 4.01089477e-07]
ene_total = [0.02618703 0.00834238 0.0274096  0.02144336 0.00798014 0.007816
 0.02135582 0.01823528 0.01741263 0.00753262]
At round 67 energy consumption: 0.16371485787111706
At round 67 eta: 0.7681327803953281
At round 67 a_n: 4.8894852623407985
At round 67 local rounds: 8.637912231455728
At round 67 global rounds: 22.564772710133866
gradient difference: 0.4656331539154053
train() client id: f_00000-0-0 loss: 1.275425  [   32/  126]
train() client id: f_00000-0-1 loss: 1.055078  [   64/  126]
train() client id: f_00000-0-2 loss: 1.167153  [   96/  126]
train() client id: f_00000-1-0 loss: 1.173852  [   32/  126]
train() client id: f_00000-1-1 loss: 1.155913  [   64/  126]
train() client id: f_00000-1-2 loss: 0.894065  [   96/  126]
train() client id: f_00000-2-0 loss: 1.137472  [   32/  126]
train() client id: f_00000-2-1 loss: 1.056431  [   64/  126]
train() client id: f_00000-2-2 loss: 0.939689  [   96/  126]
train() client id: f_00000-3-0 loss: 0.995713  [   32/  126]
train() client id: f_00000-3-1 loss: 0.889796  [   64/  126]
train() client id: f_00000-3-2 loss: 1.027756  [   96/  126]
train() client id: f_00000-4-0 loss: 1.026579  [   32/  126]
train() client id: f_00000-4-1 loss: 0.915722  [   64/  126]
train() client id: f_00000-4-2 loss: 0.940586  [   96/  126]
train() client id: f_00000-5-0 loss: 0.886093  [   32/  126]
train() client id: f_00000-5-1 loss: 0.935440  [   64/  126]
train() client id: f_00000-5-2 loss: 0.800111  [   96/  126]
train() client id: f_00000-6-0 loss: 0.853916  [   32/  126]
train() client id: f_00000-6-1 loss: 0.923961  [   64/  126]
train() client id: f_00000-6-2 loss: 0.825942  [   96/  126]
train() client id: f_00000-7-0 loss: 0.859965  [   32/  126]
train() client id: f_00000-7-1 loss: 0.891592  [   64/  126]
train() client id: f_00000-7-2 loss: 0.777449  [   96/  126]
train() client id: f_00001-0-0 loss: 0.570268  [   32/  265]
train() client id: f_00001-0-1 loss: 0.467013  [   64/  265]
train() client id: f_00001-0-2 loss: 0.446920  [   96/  265]
train() client id: f_00001-0-3 loss: 0.548873  [  128/  265]
train() client id: f_00001-0-4 loss: 0.378242  [  160/  265]
train() client id: f_00001-0-5 loss: 0.490234  [  192/  265]
train() client id: f_00001-0-6 loss: 0.422311  [  224/  265]
train() client id: f_00001-0-7 loss: 0.418543  [  256/  265]
train() client id: f_00001-1-0 loss: 0.475765  [   32/  265]
train() client id: f_00001-1-1 loss: 0.371598  [   64/  265]
train() client id: f_00001-1-2 loss: 0.498410  [   96/  265]
train() client id: f_00001-1-3 loss: 0.418858  [  128/  265]
train() client id: f_00001-1-4 loss: 0.578936  [  160/  265]
train() client id: f_00001-1-5 loss: 0.392108  [  192/  265]
train() client id: f_00001-1-6 loss: 0.456773  [  224/  265]
train() client id: f_00001-1-7 loss: 0.482061  [  256/  265]
train() client id: f_00001-2-0 loss: 0.394290  [   32/  265]
train() client id: f_00001-2-1 loss: 0.516147  [   64/  265]
train() client id: f_00001-2-2 loss: 0.542538  [   96/  265]
train() client id: f_00001-2-3 loss: 0.467565  [  128/  265]
train() client id: f_00001-2-4 loss: 0.380645  [  160/  265]
train() client id: f_00001-2-5 loss: 0.338626  [  192/  265]
train() client id: f_00001-2-6 loss: 0.514649  [  224/  265]
train() client id: f_00001-2-7 loss: 0.475236  [  256/  265]
train() client id: f_00001-3-0 loss: 0.487772  [   32/  265]
train() client id: f_00001-3-1 loss: 0.485812  [   64/  265]
train() client id: f_00001-3-2 loss: 0.411994  [   96/  265]
train() client id: f_00001-3-3 loss: 0.566482  [  128/  265]
train() client id: f_00001-3-4 loss: 0.342746  [  160/  265]
train() client id: f_00001-3-5 loss: 0.501105  [  192/  265]
train() client id: f_00001-3-6 loss: 0.377293  [  224/  265]
train() client id: f_00001-3-7 loss: 0.369648  [  256/  265]
train() client id: f_00001-4-0 loss: 0.456725  [   32/  265]
train() client id: f_00001-4-1 loss: 0.475551  [   64/  265]
train() client id: f_00001-4-2 loss: 0.352183  [   96/  265]
train() client id: f_00001-4-3 loss: 0.511885  [  128/  265]
train() client id: f_00001-4-4 loss: 0.513315  [  160/  265]
train() client id: f_00001-4-5 loss: 0.351103  [  192/  265]
train() client id: f_00001-4-6 loss: 0.345387  [  224/  265]
train() client id: f_00001-4-7 loss: 0.552951  [  256/  265]
train() client id: f_00001-5-0 loss: 0.366209  [   32/  265]
train() client id: f_00001-5-1 loss: 0.425807  [   64/  265]
train() client id: f_00001-5-2 loss: 0.341322  [   96/  265]
train() client id: f_00001-5-3 loss: 0.478242  [  128/  265]
train() client id: f_00001-5-4 loss: 0.385276  [  160/  265]
train() client id: f_00001-5-5 loss: 0.590051  [  192/  265]
train() client id: f_00001-5-6 loss: 0.502815  [  224/  265]
train() client id: f_00001-5-7 loss: 0.382388  [  256/  265]
train() client id: f_00001-6-0 loss: 0.390989  [   32/  265]
train() client id: f_00001-6-1 loss: 0.416947  [   64/  265]
train() client id: f_00001-6-2 loss: 0.530809  [   96/  265]
train() client id: f_00001-6-3 loss: 0.332604  [  128/  265]
train() client id: f_00001-6-4 loss: 0.440383  [  160/  265]
train() client id: f_00001-6-5 loss: 0.359816  [  192/  265]
train() client id: f_00001-6-6 loss: 0.441489  [  224/  265]
train() client id: f_00001-6-7 loss: 0.576451  [  256/  265]
train() client id: f_00001-7-0 loss: 0.499958  [   32/  265]
train() client id: f_00001-7-1 loss: 0.455870  [   64/  265]
train() client id: f_00001-7-2 loss: 0.343588  [   96/  265]
train() client id: f_00001-7-3 loss: 0.561815  [  128/  265]
train() client id: f_00001-7-4 loss: 0.460991  [  160/  265]
train() client id: f_00001-7-5 loss: 0.344221  [  192/  265]
train() client id: f_00001-7-6 loss: 0.347352  [  224/  265]
train() client id: f_00001-7-7 loss: 0.494897  [  256/  265]
train() client id: f_00002-0-0 loss: 0.967884  [   32/  124]
train() client id: f_00002-0-1 loss: 1.208369  [   64/  124]
train() client id: f_00002-0-2 loss: 1.029327  [   96/  124]
train() client id: f_00002-1-0 loss: 1.019879  [   32/  124]
train() client id: f_00002-1-1 loss: 1.120271  [   64/  124]
train() client id: f_00002-1-2 loss: 1.042504  [   96/  124]
train() client id: f_00002-2-0 loss: 0.959772  [   32/  124]
train() client id: f_00002-2-1 loss: 0.934477  [   64/  124]
train() client id: f_00002-2-2 loss: 0.927702  [   96/  124]
train() client id: f_00002-3-0 loss: 1.166864  [   32/  124]
train() client id: f_00002-3-1 loss: 0.991127  [   64/  124]
train() client id: f_00002-3-2 loss: 0.871128  [   96/  124]
train() client id: f_00002-4-0 loss: 1.087353  [   32/  124]
train() client id: f_00002-4-1 loss: 0.922990  [   64/  124]
train() client id: f_00002-4-2 loss: 0.944532  [   96/  124]
train() client id: f_00002-5-0 loss: 0.701601  [   32/  124]
train() client id: f_00002-5-1 loss: 1.057433  [   64/  124]
train() client id: f_00002-5-2 loss: 1.135985  [   96/  124]
train() client id: f_00002-6-0 loss: 0.837794  [   32/  124]
train() client id: f_00002-6-1 loss: 0.985552  [   64/  124]
train() client id: f_00002-6-2 loss: 0.910390  [   96/  124]
train() client id: f_00002-7-0 loss: 1.024236  [   32/  124]
train() client id: f_00002-7-1 loss: 0.943562  [   64/  124]
train() client id: f_00002-7-2 loss: 0.784445  [   96/  124]
train() client id: f_00003-0-0 loss: 0.709950  [   32/   43]
train() client id: f_00003-1-0 loss: 0.589937  [   32/   43]
train() client id: f_00003-2-0 loss: 0.833626  [   32/   43]
train() client id: f_00003-3-0 loss: 1.024484  [   32/   43]
train() client id: f_00003-4-0 loss: 0.917372  [   32/   43]
train() client id: f_00003-5-0 loss: 0.631910  [   32/   43]
train() client id: f_00003-6-0 loss: 0.863458  [   32/   43]
train() client id: f_00003-7-0 loss: 0.783934  [   32/   43]
train() client id: f_00004-0-0 loss: 0.959132  [   32/  306]
train() client id: f_00004-0-1 loss: 0.872559  [   64/  306]
train() client id: f_00004-0-2 loss: 0.653079  [   96/  306]
train() client id: f_00004-0-3 loss: 0.936260  [  128/  306]
train() client id: f_00004-0-4 loss: 0.914488  [  160/  306]
train() client id: f_00004-0-5 loss: 0.710423  [  192/  306]
train() client id: f_00004-0-6 loss: 0.859766  [  224/  306]
train() client id: f_00004-0-7 loss: 0.774988  [  256/  306]
train() client id: f_00004-0-8 loss: 0.661366  [  288/  306]
train() client id: f_00004-1-0 loss: 0.932069  [   32/  306]
train() client id: f_00004-1-1 loss: 0.674762  [   64/  306]
train() client id: f_00004-1-2 loss: 0.757461  [   96/  306]
train() client id: f_00004-1-3 loss: 0.875913  [  128/  306]
train() client id: f_00004-1-4 loss: 0.775673  [  160/  306]
train() client id: f_00004-1-5 loss: 0.853095  [  192/  306]
train() client id: f_00004-1-6 loss: 0.887960  [  224/  306]
train() client id: f_00004-1-7 loss: 0.834584  [  256/  306]
train() client id: f_00004-1-8 loss: 0.797971  [  288/  306]
train() client id: f_00004-2-0 loss: 0.882584  [   32/  306]
train() client id: f_00004-2-1 loss: 0.794156  [   64/  306]
train() client id: f_00004-2-2 loss: 0.677163  [   96/  306]
train() client id: f_00004-2-3 loss: 0.772997  [  128/  306]
train() client id: f_00004-2-4 loss: 0.947413  [  160/  306]
train() client id: f_00004-2-5 loss: 0.847546  [  192/  306]
train() client id: f_00004-2-6 loss: 0.868441  [  224/  306]
train() client id: f_00004-2-7 loss: 0.899701  [  256/  306]
train() client id: f_00004-2-8 loss: 0.669193  [  288/  306]
train() client id: f_00004-3-0 loss: 0.791843  [   32/  306]
train() client id: f_00004-3-1 loss: 0.725351  [   64/  306]
train() client id: f_00004-3-2 loss: 0.943674  [   96/  306]
train() client id: f_00004-3-3 loss: 0.793011  [  128/  306]
train() client id: f_00004-3-4 loss: 0.865353  [  160/  306]
train() client id: f_00004-3-5 loss: 0.924921  [  192/  306]
train() client id: f_00004-3-6 loss: 0.854779  [  224/  306]
train() client id: f_00004-3-7 loss: 0.813394  [  256/  306]
train() client id: f_00004-3-8 loss: 0.711040  [  288/  306]
train() client id: f_00004-4-0 loss: 0.851471  [   32/  306]
train() client id: f_00004-4-1 loss: 0.737593  [   64/  306]
train() client id: f_00004-4-2 loss: 0.816626  [   96/  306]
train() client id: f_00004-4-3 loss: 0.898782  [  128/  306]
train() client id: f_00004-4-4 loss: 0.776286  [  160/  306]
train() client id: f_00004-4-5 loss: 0.770821  [  192/  306]
train() client id: f_00004-4-6 loss: 0.819525  [  224/  306]
train() client id: f_00004-4-7 loss: 0.763172  [  256/  306]
train() client id: f_00004-4-8 loss: 1.025759  [  288/  306]
train() client id: f_00004-5-0 loss: 0.799341  [   32/  306]
train() client id: f_00004-5-1 loss: 0.738731  [   64/  306]
train() client id: f_00004-5-2 loss: 0.824547  [   96/  306]
train() client id: f_00004-5-3 loss: 0.967091  [  128/  306]
train() client id: f_00004-5-4 loss: 0.886510  [  160/  306]
train() client id: f_00004-5-5 loss: 0.767678  [  192/  306]
train() client id: f_00004-5-6 loss: 0.718635  [  224/  306]
train() client id: f_00004-5-7 loss: 0.856221  [  256/  306]
train() client id: f_00004-5-8 loss: 0.817538  [  288/  306]
train() client id: f_00004-6-0 loss: 0.880124  [   32/  306]
train() client id: f_00004-6-1 loss: 0.772291  [   64/  306]
train() client id: f_00004-6-2 loss: 0.812688  [   96/  306]
train() client id: f_00004-6-3 loss: 0.863475  [  128/  306]
train() client id: f_00004-6-4 loss: 0.944346  [  160/  306]
train() client id: f_00004-6-5 loss: 0.819338  [  192/  306]
train() client id: f_00004-6-6 loss: 0.756173  [  224/  306]
train() client id: f_00004-6-7 loss: 0.867002  [  256/  306]
train() client id: f_00004-6-8 loss: 0.636917  [  288/  306]
train() client id: f_00004-7-0 loss: 0.826357  [   32/  306]
train() client id: f_00004-7-1 loss: 0.881589  [   64/  306]
train() client id: f_00004-7-2 loss: 0.980810  [   96/  306]
train() client id: f_00004-7-3 loss: 0.742477  [  128/  306]
train() client id: f_00004-7-4 loss: 0.868223  [  160/  306]
train() client id: f_00004-7-5 loss: 0.749604  [  192/  306]
train() client id: f_00004-7-6 loss: 0.715117  [  224/  306]
train() client id: f_00004-7-7 loss: 0.775319  [  256/  306]
train() client id: f_00004-7-8 loss: 0.736678  [  288/  306]
train() client id: f_00005-0-0 loss: 0.627722  [   32/  146]
train() client id: f_00005-0-1 loss: 0.412208  [   64/  146]
train() client id: f_00005-0-2 loss: 0.687991  [   96/  146]
train() client id: f_00005-0-3 loss: 0.444481  [  128/  146]
train() client id: f_00005-1-0 loss: 0.780141  [   32/  146]
train() client id: f_00005-1-1 loss: 0.662793  [   64/  146]
train() client id: f_00005-1-2 loss: 0.353156  [   96/  146]
train() client id: f_00005-1-3 loss: 0.635284  [  128/  146]
train() client id: f_00005-2-0 loss: 0.661118  [   32/  146]
train() client id: f_00005-2-1 loss: 0.450030  [   64/  146]
train() client id: f_00005-2-2 loss: 0.554978  [   96/  146]
train() client id: f_00005-2-3 loss: 0.522857  [  128/  146]
train() client id: f_00005-3-0 loss: 0.467849  [   32/  146]
train() client id: f_00005-3-1 loss: 0.448361  [   64/  146]
train() client id: f_00005-3-2 loss: 0.548736  [   96/  146]
train() client id: f_00005-3-3 loss: 0.634873  [  128/  146]
train() client id: f_00005-4-0 loss: 0.388946  [   32/  146]
train() client id: f_00005-4-1 loss: 0.567788  [   64/  146]
train() client id: f_00005-4-2 loss: 0.523590  [   96/  146]
train() client id: f_00005-4-3 loss: 0.435787  [  128/  146]
train() client id: f_00005-5-0 loss: 0.705048  [   32/  146]
train() client id: f_00005-5-1 loss: 0.313737  [   64/  146]
train() client id: f_00005-5-2 loss: 0.571409  [   96/  146]
train() client id: f_00005-5-3 loss: 0.524997  [  128/  146]
train() client id: f_00005-6-0 loss: 0.671391  [   32/  146]
train() client id: f_00005-6-1 loss: 0.395702  [   64/  146]
train() client id: f_00005-6-2 loss: 0.596648  [   96/  146]
train() client id: f_00005-6-3 loss: 0.693764  [  128/  146]
train() client id: f_00005-7-0 loss: 0.645364  [   32/  146]
train() client id: f_00005-7-1 loss: 0.390916  [   64/  146]
train() client id: f_00005-7-2 loss: 0.403376  [   96/  146]
train() client id: f_00005-7-3 loss: 0.697343  [  128/  146]
train() client id: f_00006-0-0 loss: 0.435807  [   32/   54]
train() client id: f_00006-1-0 loss: 0.442519  [   32/   54]
train() client id: f_00006-2-0 loss: 0.462213  [   32/   54]
train() client id: f_00006-3-0 loss: 0.428447  [   32/   54]
train() client id: f_00006-4-0 loss: 0.460116  [   32/   54]
train() client id: f_00006-5-0 loss: 0.396377  [   32/   54]
train() client id: f_00006-6-0 loss: 0.464217  [   32/   54]
train() client id: f_00006-7-0 loss: 0.396138  [   32/   54]
train() client id: f_00007-0-0 loss: 0.553990  [   32/  179]
train() client id: f_00007-0-1 loss: 0.421217  [   64/  179]
train() client id: f_00007-0-2 loss: 0.457857  [   96/  179]
train() client id: f_00007-0-3 loss: 0.475806  [  128/  179]
train() client id: f_00007-0-4 loss: 0.428381  [  160/  179]
train() client id: f_00007-1-0 loss: 0.506053  [   32/  179]
train() client id: f_00007-1-1 loss: 0.428365  [   64/  179]
train() client id: f_00007-1-2 loss: 0.392530  [   96/  179]
train() client id: f_00007-1-3 loss: 0.369356  [  128/  179]
train() client id: f_00007-1-4 loss: 0.396179  [  160/  179]
train() client id: f_00007-2-0 loss: 0.307370  [   32/  179]
train() client id: f_00007-2-1 loss: 0.445896  [   64/  179]
train() client id: f_00007-2-2 loss: 0.325222  [   96/  179]
train() client id: f_00007-2-3 loss: 0.537033  [  128/  179]
train() client id: f_00007-2-4 loss: 0.625204  [  160/  179]
train() client id: f_00007-3-0 loss: 0.450276  [   32/  179]
train() client id: f_00007-3-1 loss: 0.481246  [   64/  179]
train() client id: f_00007-3-2 loss: 0.262234  [   96/  179]
train() client id: f_00007-3-3 loss: 0.618129  [  128/  179]
train() client id: f_00007-3-4 loss: 0.397265  [  160/  179]
train() client id: f_00007-4-0 loss: 0.268934  [   32/  179]
train() client id: f_00007-4-1 loss: 0.372694  [   64/  179]
train() client id: f_00007-4-2 loss: 0.517961  [   96/  179]
train() client id: f_00007-4-3 loss: 0.441473  [  128/  179]
train() client id: f_00007-4-4 loss: 0.481916  [  160/  179]
train() client id: f_00007-5-0 loss: 0.415028  [   32/  179]
train() client id: f_00007-5-1 loss: 0.443301  [   64/  179]
train() client id: f_00007-5-2 loss: 0.430458  [   96/  179]
train() client id: f_00007-5-3 loss: 0.520773  [  128/  179]
train() client id: f_00007-5-4 loss: 0.336439  [  160/  179]
train() client id: f_00007-6-0 loss: 0.450764  [   32/  179]
train() client id: f_00007-6-1 loss: 0.395166  [   64/  179]
train() client id: f_00007-6-2 loss: 0.342869  [   96/  179]
train() client id: f_00007-6-3 loss: 0.418853  [  128/  179]
train() client id: f_00007-6-4 loss: 0.351041  [  160/  179]
train() client id: f_00007-7-0 loss: 0.363424  [   32/  179]
train() client id: f_00007-7-1 loss: 0.353293  [   64/  179]
train() client id: f_00007-7-2 loss: 0.266279  [   96/  179]
train() client id: f_00007-7-3 loss: 0.560293  [  128/  179]
train() client id: f_00007-7-4 loss: 0.496670  [  160/  179]
train() client id: f_00008-0-0 loss: 0.845859  [   32/  130]
train() client id: f_00008-0-1 loss: 0.650849  [   64/  130]
train() client id: f_00008-0-2 loss: 0.901718  [   96/  130]
train() client id: f_00008-0-3 loss: 0.607178  [  128/  130]
train() client id: f_00008-1-0 loss: 0.811483  [   32/  130]
train() client id: f_00008-1-1 loss: 0.695717  [   64/  130]
train() client id: f_00008-1-2 loss: 0.623155  [   96/  130]
train() client id: f_00008-1-3 loss: 0.871516  [  128/  130]
train() client id: f_00008-2-0 loss: 0.773323  [   32/  130]
train() client id: f_00008-2-1 loss: 0.696979  [   64/  130]
train() client id: f_00008-2-2 loss: 0.778193  [   96/  130]
train() client id: f_00008-2-3 loss: 0.764129  [  128/  130]
train() client id: f_00008-3-0 loss: 0.649107  [   32/  130]
train() client id: f_00008-3-1 loss: 0.806487  [   64/  130]
train() client id: f_00008-3-2 loss: 0.856640  [   96/  130]
train() client id: f_00008-3-3 loss: 0.665662  [  128/  130]
train() client id: f_00008-4-0 loss: 0.810175  [   32/  130]
train() client id: f_00008-4-1 loss: 0.762723  [   64/  130]
train() client id: f_00008-4-2 loss: 0.727808  [   96/  130]
train() client id: f_00008-4-3 loss: 0.697035  [  128/  130]
train() client id: f_00008-5-0 loss: 0.747716  [   32/  130]
train() client id: f_00008-5-1 loss: 0.663476  [   64/  130]
train() client id: f_00008-5-2 loss: 0.811774  [   96/  130]
train() client id: f_00008-5-3 loss: 0.786924  [  128/  130]
train() client id: f_00008-6-0 loss: 0.758706  [   32/  130]
train() client id: f_00008-6-1 loss: 0.692296  [   64/  130]
train() client id: f_00008-6-2 loss: 0.809093  [   96/  130]
train() client id: f_00008-6-3 loss: 0.750593  [  128/  130]
train() client id: f_00008-7-0 loss: 0.761387  [   32/  130]
train() client id: f_00008-7-1 loss: 0.665335  [   64/  130]
train() client id: f_00008-7-2 loss: 0.814958  [   96/  130]
train() client id: f_00008-7-3 loss: 0.758592  [  128/  130]
train() client id: f_00009-0-0 loss: 0.816640  [   32/  118]
train() client id: f_00009-0-1 loss: 1.070301  [   64/  118]
train() client id: f_00009-0-2 loss: 0.894797  [   96/  118]
train() client id: f_00009-1-0 loss: 0.876464  [   32/  118]
train() client id: f_00009-1-1 loss: 0.922514  [   64/  118]
train() client id: f_00009-1-2 loss: 1.003589  [   96/  118]
train() client id: f_00009-2-0 loss: 1.003623  [   32/  118]
train() client id: f_00009-2-1 loss: 0.715351  [   64/  118]
train() client id: f_00009-2-2 loss: 0.815799  [   96/  118]
train() client id: f_00009-3-0 loss: 1.069130  [   32/  118]
train() client id: f_00009-3-1 loss: 0.825103  [   64/  118]
train() client id: f_00009-3-2 loss: 0.867611  [   96/  118]
train() client id: f_00009-4-0 loss: 0.936591  [   32/  118]
train() client id: f_00009-4-1 loss: 0.737005  [   64/  118]
train() client id: f_00009-4-2 loss: 0.990568  [   96/  118]
train() client id: f_00009-5-0 loss: 0.785835  [   32/  118]
train() client id: f_00009-5-1 loss: 0.817044  [   64/  118]
train() client id: f_00009-5-2 loss: 0.980212  [   96/  118]
train() client id: f_00009-6-0 loss: 0.850556  [   32/  118]
train() client id: f_00009-6-1 loss: 0.935469  [   64/  118]
train() client id: f_00009-6-2 loss: 0.822717  [   96/  118]
train() client id: f_00009-7-0 loss: 0.732808  [   32/  118]
train() client id: f_00009-7-1 loss: 0.932672  [   64/  118]
train() client id: f_00009-7-2 loss: 0.898088  [   96/  118]
At round 67 accuracy: 0.6472148541114059
At round 67 training accuracy: 0.5935613682092555
At round 67 training loss: 0.825508760333581
update_location
xs = [  -3.9056584     4.20031788  355.00902392   18.81129433    0.97929623
    3.95640986 -317.44319194 -296.32485185  339.66397685 -282.06087855]
ys = [ 347.5879595   330.55583871    1.32061395 -317.45517586  309.35018685
  292.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [361.70795368 345.37632399 368.8267223  333.36414546 325.11305283
 309.44430849 332.83189548 312.74445491 354.51415648 299.28974434]
dists_bs = [243.02238063 237.29806759 557.74016761 529.22104837 221.40163822
 214.25585347 227.68552432 212.26769452 538.20398264 201.74866778]
uav_gains = [1.05200963e-12 1.28407742e-12 9.72759726e-13 1.51715294e-12
 1.72031827e-12 2.24199922e-12 1.52907249e-12 2.11432101e-12
 1.14444349e-12 2.70950730e-12]
bs_gains = [2.30932218e-11 2.46871139e-11 2.25571746e-12 2.61282415e-12
 2.99769719e-11 3.28611333e-11 2.77175544e-11 3.37302187e-11
 2.49254349e-12 3.88887688e-11]
Round 68
-------------------------------
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.01021389 3.98881553 1.98649518 0.75024676 4.59768714 2.21362175
 0.91366287 2.75982094 2.02332361 1.79514037]
obj_prev = 23.039028040351884
eta_min = nan	eta_max = nan
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 5.251816334124527	eta = 0.9090909090909091
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 14.045450908523552	eta = 0.3399234753417846
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 9.049757744866326	eta = 0.5275697560275713
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 8.202066092537866	eta = 0.5820946127112314
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 8.148578542585753	eta = 0.5859155017793716
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 8.148332307464996	eta = 0.5859332076078638
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 8.148332302200442	eta = 0.5859332079864292
eta = 0.5859332079864292
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [0.04410974 0.09277049 0.04340959 0.01505332 0.10712366 0.05111129
 0.01890417 0.06266384 0.04551005 0.04130912]
ene_total = [0.83982608 1.19483484 0.84607581 0.44015542 1.35993302 0.69656538
 0.48459427 0.96365584 0.74525191 0.57743974]
ti_comp = [1.98249026 2.16845803 1.97019612 2.03038779 2.17217608 2.17382626
 2.03126319 2.06316582 2.07658364 2.17668634]
ti_coms = [0.27011056 0.08414279 0.2824047  0.22221303 0.08042474 0.07877456
 0.22133763 0.18943501 0.17601719 0.07591448]
t_total = [26.59971466 26.59971466 26.59971466 26.59971466 26.59971466 26.59971466
 26.59971466 26.59971466 26.59971466 26.59971466]
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [1.36477616e-06 1.06122506e-05 1.31709789e-06 5.17153298e-08
 1.62834394e-05 1.76595985e-06 1.02334201e-07 3.61295660e-06
 1.36616489e-06 9.29878567e-07]
ene_total = [0.31897471 0.09948487 0.33349161 0.26239966 0.0951614  0.09304137
 0.26136655 0.22373596 0.20786507 0.08965418]
optimize_network iter = 0 obj = 1.9851753783046104
eta = 0.5859332079864292
freqs = [11124831.81396149 21390888.52316821 11016564.88463251  3707007.23591558
 24658143.18962516 11756065.47830876  4653305.14511856 15186331.26526243
 10957914.22550441  9488993.01846851]
eta_min = 0.5859332079864303	eta_max = 0.7781376768963073
af = 0.0003421552139497174	bf = 0.8218123015887785	zeta = 0.00037637073534468917	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [2.43339611e-07 1.89216446e-06 2.34838575e-07 9.22084412e-09
 2.90333750e-06 3.14870668e-07 1.82461896e-08 6.44190219e-07
 2.43587222e-07 1.65797363e-07]
ene_total = [1.45278485 0.4526582  1.5189076  1.19515856 0.43271528 0.42370069
 1.19045077 1.01889828 0.94670988 0.40830993]
ti_comp = [0.93686199 1.12282976 0.92456785 0.98475952 1.12654781 1.12819799
 0.98563492 1.01753754 1.03095536 1.13105807]
ti_coms = [0.27011056 0.08414279 0.2824047  0.22221303 0.08042474 0.07877456
 0.22133763 0.18943501 0.17601719 0.07591448]
t_total = [26.59971466 26.59971466 26.59971466 26.59971466 26.59971466 26.59971466
 26.59971466 26.59971466 26.59971466 26.59971466]
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [6.31568265e-07 4.09043688e-06 6.18083830e-07 2.27198204e-08
 6.25640601e-06 6.77560105e-07 4.49168218e-08 1.53503439e-06
 5.72809530e-07 3.55906665e-07]
ene_total = [0.59529372 0.18552719 0.62238771 0.48972193 0.17738094 0.17362127
 0.48779318 0.41751781 0.38792591 0.16731103]
optimize_network iter = 1 obj = 3.7044806936267416
eta = 0.7781376768963073
freqs = [11047352.84864209 19386314.72442644 11016564.8846325   3586753.80166813
 22311825.119887   10629940.50483284  4500294.94608312 14449934.82166095
 10357782.55225607  8569597.11605592]
eta_min = 0.7781376768963095	eta_max = 0.7781376768963063
af = 0.0002885807068502459	bf = 0.8218123015887785	zeta = 0.0003174387775352705	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [2.39961934e-07 1.55414576e-06 2.34838575e-07 8.63230839e-09
 2.37709741e-06 2.57436357e-07 1.70659737e-08 5.83230415e-07
 2.17636778e-07 1.35225369e-07]
ene_total = [1.45278467 0.45264002 1.5189076  1.19515853 0.43268697 0.4236976
 1.19045071 1.018895   0.94670848 0.40830829]
ti_comp = [0.93686199 1.12282976 0.92456785 0.98475952 1.12654781 1.12819799
 0.98563492 1.01753754 1.03095536 1.13105807]
ti_coms = [0.27011056 0.08414279 0.2824047  0.22221303 0.08042474 0.07877456
 0.22133763 0.18943501 0.17601719 0.07591448]
t_total = [26.59971466 26.59971466 26.59971466 26.59971466 26.59971466 26.59971466
 26.59971466 26.59971466 26.59971466 26.59971466]
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [6.31568265e-07 4.09043688e-06 6.18083830e-07 2.27198204e-08
 6.25640601e-06 6.77560105e-07 4.49168218e-08 1.53503439e-06
 5.72809530e-07 3.55906665e-07]
ene_total = [0.59529372 0.18552719 0.62238771 0.48972193 0.17738094 0.17362127
 0.48779318 0.41751781 0.38792591 0.16731103]
optimize_network iter = 2 obj = 3.7044806936267247
eta = 0.7781376768963063
freqs = [11047352.84864208 19386314.72442644 11016564.88463249  3586753.80166813
 22311825.119887   10629940.50483284  4500294.94608312 14449934.82166094
 10357782.55225607  8569597.11605592]
Done!
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [6.15101785e-07 3.98378951e-06 6.01968921e-07 2.21274609e-08
 6.09328669e-06 6.59894509e-07 4.37457339e-08 1.49501241e-06
 5.57875029e-07 3.46627335e-07]
ene_total = [0.02701167 0.00841826 0.02824107 0.02222133 0.00804857 0.00787812
 0.02213381 0.018945   0.01760228 0.00759179]
At round 68 energy consumption: 0.16809188719852586
At round 68 eta: 0.7781376768963063
At round 68 a_n: 4.546939415371483
At round 68 local rounds: 8.214162669625793
At round 68 global rounds: 22.038375844715002
gradient difference: 0.6460168957710266
train() client id: f_00000-0-0 loss: 1.137325  [   32/  126]
train() client id: f_00000-0-1 loss: 0.975306  [   64/  126]
train() client id: f_00000-0-2 loss: 1.151113  [   96/  126]
train() client id: f_00000-1-0 loss: 1.125362  [   32/  126]
train() client id: f_00000-1-1 loss: 1.048213  [   64/  126]
train() client id: f_00000-1-2 loss: 0.994912  [   96/  126]
train() client id: f_00000-2-0 loss: 0.895492  [   32/  126]
train() client id: f_00000-2-1 loss: 0.991984  [   64/  126]
train() client id: f_00000-2-2 loss: 1.034335  [   96/  126]
train() client id: f_00000-3-0 loss: 0.987498  [   32/  126]
train() client id: f_00000-3-1 loss: 1.043248  [   64/  126]
train() client id: f_00000-3-2 loss: 0.973168  [   96/  126]
train() client id: f_00000-4-0 loss: 0.929052  [   32/  126]
train() client id: f_00000-4-1 loss: 1.133121  [   64/  126]
train() client id: f_00000-4-2 loss: 0.938969  [   96/  126]
train() client id: f_00000-5-0 loss: 0.902852  [   32/  126]
train() client id: f_00000-5-1 loss: 0.968533  [   64/  126]
train() client id: f_00000-5-2 loss: 0.973199  [   96/  126]
train() client id: f_00000-6-0 loss: 0.982847  [   32/  126]
train() client id: f_00000-6-1 loss: 0.980357  [   64/  126]
train() client id: f_00000-6-2 loss: 0.990732  [   96/  126]
train() client id: f_00000-7-0 loss: 0.973700  [   32/  126]
train() client id: f_00000-7-1 loss: 0.851862  [   64/  126]
train() client id: f_00000-7-2 loss: 0.953350  [   96/  126]
train() client id: f_00001-0-0 loss: 0.404700  [   32/  265]
train() client id: f_00001-0-1 loss: 0.434949  [   64/  265]
train() client id: f_00001-0-2 loss: 0.423705  [   96/  265]
train() client id: f_00001-0-3 loss: 0.409064  [  128/  265]
train() client id: f_00001-0-4 loss: 0.364156  [  160/  265]
train() client id: f_00001-0-5 loss: 0.380570  [  192/  265]
train() client id: f_00001-0-6 loss: 0.453665  [  224/  265]
train() client id: f_00001-0-7 loss: 0.592078  [  256/  265]
train() client id: f_00001-1-0 loss: 0.447834  [   32/  265]
train() client id: f_00001-1-1 loss: 0.373281  [   64/  265]
train() client id: f_00001-1-2 loss: 0.485209  [   96/  265]
train() client id: f_00001-1-3 loss: 0.434433  [  128/  265]
train() client id: f_00001-1-4 loss: 0.339438  [  160/  265]
train() client id: f_00001-1-5 loss: 0.538750  [  192/  265]
train() client id: f_00001-1-6 loss: 0.409773  [  224/  265]
train() client id: f_00001-1-7 loss: 0.452425  [  256/  265]
train() client id: f_00001-2-0 loss: 0.452677  [   32/  265]
train() client id: f_00001-2-1 loss: 0.344133  [   64/  265]
train() client id: f_00001-2-2 loss: 0.329416  [   96/  265]
train() client id: f_00001-2-3 loss: 0.468477  [  128/  265]
train() client id: f_00001-2-4 loss: 0.438099  [  160/  265]
train() client id: f_00001-2-5 loss: 0.430613  [  192/  265]
train() client id: f_00001-2-6 loss: 0.609995  [  224/  265]
train() client id: f_00001-2-7 loss: 0.328232  [  256/  265]
train() client id: f_00001-3-0 loss: 0.392987  [   32/  265]
train() client id: f_00001-3-1 loss: 0.472415  [   64/  265]
train() client id: f_00001-3-2 loss: 0.606361  [   96/  265]
train() client id: f_00001-3-3 loss: 0.433259  [  128/  265]
train() client id: f_00001-3-4 loss: 0.332269  [  160/  265]
train() client id: f_00001-3-5 loss: 0.416988  [  192/  265]
train() client id: f_00001-3-6 loss: 0.315050  [  224/  265]
train() client id: f_00001-3-7 loss: 0.351066  [  256/  265]
train() client id: f_00001-4-0 loss: 0.370372  [   32/  265]
train() client id: f_00001-4-1 loss: 0.501998  [   64/  265]
train() client id: f_00001-4-2 loss: 0.517581  [   96/  265]
train() client id: f_00001-4-3 loss: 0.380751  [  128/  265]
train() client id: f_00001-4-4 loss: 0.332909  [  160/  265]
train() client id: f_00001-4-5 loss: 0.470286  [  192/  265]
train() client id: f_00001-4-6 loss: 0.508683  [  224/  265]
train() client id: f_00001-4-7 loss: 0.305957  [  256/  265]
train() client id: f_00001-5-0 loss: 0.363621  [   32/  265]
train() client id: f_00001-5-1 loss: 0.442008  [   64/  265]
train() client id: f_00001-5-2 loss: 0.420298  [   96/  265]
train() client id: f_00001-5-3 loss: 0.457305  [  128/  265]
train() client id: f_00001-5-4 loss: 0.365503  [  160/  265]
train() client id: f_00001-5-5 loss: 0.435214  [  192/  265]
train() client id: f_00001-5-6 loss: 0.387377  [  224/  265]
train() client id: f_00001-5-7 loss: 0.407543  [  256/  265]
train() client id: f_00001-6-0 loss: 0.517895  [   32/  265]
train() client id: f_00001-6-1 loss: 0.484789  [   64/  265]
train() client id: f_00001-6-2 loss: 0.461872  [   96/  265]
train() client id: f_00001-6-3 loss: 0.370886  [  128/  265]
train() client id: f_00001-6-4 loss: 0.504105  [  160/  265]
train() client id: f_00001-6-5 loss: 0.316145  [  192/  265]
train() client id: f_00001-6-6 loss: 0.383285  [  224/  265]
train() client id: f_00001-6-7 loss: 0.319333  [  256/  265]
train() client id: f_00001-7-0 loss: 0.356676  [   32/  265]
train() client id: f_00001-7-1 loss: 0.429404  [   64/  265]
train() client id: f_00001-7-2 loss: 0.407200  [   96/  265]
train() client id: f_00001-7-3 loss: 0.373899  [  128/  265]
train() client id: f_00001-7-4 loss: 0.413202  [  160/  265]
train() client id: f_00001-7-5 loss: 0.564285  [  192/  265]
train() client id: f_00001-7-6 loss: 0.353445  [  224/  265]
train() client id: f_00001-7-7 loss: 0.476390  [  256/  265]
train() client id: f_00002-0-0 loss: 1.298237  [   32/  124]
train() client id: f_00002-0-1 loss: 1.129313  [   64/  124]
train() client id: f_00002-0-2 loss: 0.921909  [   96/  124]
train() client id: f_00002-1-0 loss: 1.133381  [   32/  124]
train() client id: f_00002-1-1 loss: 1.087977  [   64/  124]
train() client id: f_00002-1-2 loss: 1.215086  [   96/  124]
train() client id: f_00002-2-0 loss: 1.095997  [   32/  124]
train() client id: f_00002-2-1 loss: 1.038747  [   64/  124]
train() client id: f_00002-2-2 loss: 1.205202  [   96/  124]
train() client id: f_00002-3-0 loss: 1.084389  [   32/  124]
train() client id: f_00002-3-1 loss: 1.089171  [   64/  124]
train() client id: f_00002-3-2 loss: 1.269928  [   96/  124]
train() client id: f_00002-4-0 loss: 1.180352  [   32/  124]
train() client id: f_00002-4-1 loss: 1.218607  [   64/  124]
train() client id: f_00002-4-2 loss: 1.110370  [   96/  124]
train() client id: f_00002-5-0 loss: 1.200877  [   32/  124]
train() client id: f_00002-5-1 loss: 1.116803  [   64/  124]
train() client id: f_00002-5-2 loss: 1.067773  [   96/  124]
train() client id: f_00002-6-0 loss: 1.127405  [   32/  124]
train() client id: f_00002-6-1 loss: 1.041906  [   64/  124]
train() client id: f_00002-6-2 loss: 0.993251  [   96/  124]
train() client id: f_00002-7-0 loss: 1.132005  [   32/  124]
train() client id: f_00002-7-1 loss: 0.997274  [   64/  124]
train() client id: f_00002-7-2 loss: 1.059671  [   96/  124]
train() client id: f_00003-0-0 loss: 0.597463  [   32/   43]
train() client id: f_00003-1-0 loss: 0.616190  [   32/   43]
train() client id: f_00003-2-0 loss: 0.516409  [   32/   43]
train() client id: f_00003-3-0 loss: 0.774831  [   32/   43]
train() client id: f_00003-4-0 loss: 0.475050  [   32/   43]
train() client id: f_00003-5-0 loss: 0.602098  [   32/   43]
train() client id: f_00003-6-0 loss: 0.647869  [   32/   43]
train() client id: f_00003-7-0 loss: 0.586677  [   32/   43]
train() client id: f_00004-0-0 loss: 0.862472  [   32/  306]
train() client id: f_00004-0-1 loss: 0.961077  [   64/  306]
train() client id: f_00004-0-2 loss: 0.888066  [   96/  306]
train() client id: f_00004-0-3 loss: 1.089012  [  128/  306]
train() client id: f_00004-0-4 loss: 0.927185  [  160/  306]
train() client id: f_00004-0-5 loss: 0.788012  [  192/  306]
train() client id: f_00004-0-6 loss: 0.725577  [  224/  306]
train() client id: f_00004-0-7 loss: 0.873434  [  256/  306]
train() client id: f_00004-0-8 loss: 0.985785  [  288/  306]
train() client id: f_00004-1-0 loss: 0.937867  [   32/  306]
train() client id: f_00004-1-1 loss: 0.937927  [   64/  306]
train() client id: f_00004-1-2 loss: 1.006876  [   96/  306]
train() client id: f_00004-1-3 loss: 0.709798  [  128/  306]
train() client id: f_00004-1-4 loss: 0.885206  [  160/  306]
train() client id: f_00004-1-5 loss: 0.967440  [  192/  306]
train() client id: f_00004-1-6 loss: 0.889281  [  224/  306]
train() client id: f_00004-1-7 loss: 0.886922  [  256/  306]
train() client id: f_00004-1-8 loss: 0.814780  [  288/  306]
train() client id: f_00004-2-0 loss: 0.811285  [   32/  306]
train() client id: f_00004-2-1 loss: 0.831190  [   64/  306]
train() client id: f_00004-2-2 loss: 0.826126  [   96/  306]
train() client id: f_00004-2-3 loss: 0.830196  [  128/  306]
train() client id: f_00004-2-4 loss: 0.916224  [  160/  306]
train() client id: f_00004-2-5 loss: 0.960584  [  192/  306]
train() client id: f_00004-2-6 loss: 1.010291  [  224/  306]
train() client id: f_00004-2-7 loss: 0.943965  [  256/  306]
train() client id: f_00004-2-8 loss: 0.962060  [  288/  306]
train() client id: f_00004-3-0 loss: 0.855273  [   32/  306]
train() client id: f_00004-3-1 loss: 0.816550  [   64/  306]
train() client id: f_00004-3-2 loss: 0.955904  [   96/  306]
train() client id: f_00004-3-3 loss: 0.835494  [  128/  306]
train() client id: f_00004-3-4 loss: 0.864505  [  160/  306]
train() client id: f_00004-3-5 loss: 1.105827  [  192/  306]
train() client id: f_00004-3-6 loss: 0.874182  [  224/  306]
train() client id: f_00004-3-7 loss: 0.942261  [  256/  306]
train() client id: f_00004-3-8 loss: 0.787171  [  288/  306]
train() client id: f_00004-4-0 loss: 0.824171  [   32/  306]
train() client id: f_00004-4-1 loss: 0.856308  [   64/  306]
train() client id: f_00004-4-2 loss: 0.842270  [   96/  306]
train() client id: f_00004-4-3 loss: 0.884107  [  128/  306]
train() client id: f_00004-4-4 loss: 0.880683  [  160/  306]
train() client id: f_00004-4-5 loss: 0.852971  [  192/  306]
train() client id: f_00004-4-6 loss: 0.996916  [  224/  306]
train() client id: f_00004-4-7 loss: 0.910669  [  256/  306]
train() client id: f_00004-4-8 loss: 0.974477  [  288/  306]
train() client id: f_00004-5-0 loss: 0.879961  [   32/  306]
train() client id: f_00004-5-1 loss: 0.926508  [   64/  306]
train() client id: f_00004-5-2 loss: 0.891899  [   96/  306]
train() client id: f_00004-5-3 loss: 0.766114  [  128/  306]
train() client id: f_00004-5-4 loss: 0.809110  [  160/  306]
train() client id: f_00004-5-5 loss: 1.003828  [  192/  306]
train() client id: f_00004-5-6 loss: 0.943833  [  224/  306]
train() client id: f_00004-5-7 loss: 0.911629  [  256/  306]
train() client id: f_00004-5-8 loss: 0.888703  [  288/  306]
train() client id: f_00004-6-0 loss: 0.844066  [   32/  306]
train() client id: f_00004-6-1 loss: 0.966177  [   64/  306]
train() client id: f_00004-6-2 loss: 0.822748  [   96/  306]
train() client id: f_00004-6-3 loss: 0.968058  [  128/  306]
train() client id: f_00004-6-4 loss: 0.950055  [  160/  306]
train() client id: f_00004-6-5 loss: 0.778516  [  192/  306]
train() client id: f_00004-6-6 loss: 0.843759  [  224/  306]
train() client id: f_00004-6-7 loss: 0.909533  [  256/  306]
train() client id: f_00004-6-8 loss: 0.889239  [  288/  306]
train() client id: f_00004-7-0 loss: 0.894124  [   32/  306]
train() client id: f_00004-7-1 loss: 0.893194  [   64/  306]
train() client id: f_00004-7-2 loss: 0.873520  [   96/  306]
train() client id: f_00004-7-3 loss: 0.865403  [  128/  306]
train() client id: f_00004-7-4 loss: 0.944412  [  160/  306]
train() client id: f_00004-7-5 loss: 0.868960  [  192/  306]
train() client id: f_00004-7-6 loss: 0.908496  [  224/  306]
train() client id: f_00004-7-7 loss: 0.934766  [  256/  306]
train() client id: f_00004-7-8 loss: 0.942512  [  288/  306]
train() client id: f_00005-0-0 loss: 0.566932  [   32/  146]
train() client id: f_00005-0-1 loss: 0.726059  [   64/  146]
train() client id: f_00005-0-2 loss: 0.529675  [   96/  146]
train() client id: f_00005-0-3 loss: 0.262381  [  128/  146]
train() client id: f_00005-1-0 loss: 0.610077  [   32/  146]
train() client id: f_00005-1-1 loss: 0.358863  [   64/  146]
train() client id: f_00005-1-2 loss: 0.777967  [   96/  146]
train() client id: f_00005-1-3 loss: 0.482031  [  128/  146]
train() client id: f_00005-2-0 loss: 0.386095  [   32/  146]
train() client id: f_00005-2-1 loss: 0.448214  [   64/  146]
train() client id: f_00005-2-2 loss: 0.705575  [   96/  146]
train() client id: f_00005-2-3 loss: 0.518066  [  128/  146]
train() client id: f_00005-3-0 loss: 0.844100  [   32/  146]
train() client id: f_00005-3-1 loss: 0.519618  [   64/  146]
train() client id: f_00005-3-2 loss: 0.368177  [   96/  146]
train() client id: f_00005-3-3 loss: 0.486559  [  128/  146]
train() client id: f_00005-4-0 loss: 0.474527  [   32/  146]
train() client id: f_00005-4-1 loss: 0.554601  [   64/  146]
train() client id: f_00005-4-2 loss: 0.636647  [   96/  146]
train() client id: f_00005-4-3 loss: 0.299326  [  128/  146]
train() client id: f_00005-5-0 loss: 0.496736  [   32/  146]
train() client id: f_00005-5-1 loss: 0.453614  [   64/  146]
train() client id: f_00005-5-2 loss: 0.687093  [   96/  146]
train() client id: f_00005-5-3 loss: 0.477295  [  128/  146]
train() client id: f_00005-6-0 loss: 0.696445  [   32/  146]
train() client id: f_00005-6-1 loss: 0.628607  [   64/  146]
train() client id: f_00005-6-2 loss: 0.386836  [   96/  146]
train() client id: f_00005-6-3 loss: 0.267834  [  128/  146]
train() client id: f_00005-7-0 loss: 0.488410  [   32/  146]
train() client id: f_00005-7-1 loss: 0.450522  [   64/  146]
train() client id: f_00005-7-2 loss: 0.483857  [   96/  146]
train() client id: f_00005-7-3 loss: 0.474721  [  128/  146]
train() client id: f_00006-0-0 loss: 0.589255  [   32/   54]
train() client id: f_00006-1-0 loss: 0.492570  [   32/   54]
train() client id: f_00006-2-0 loss: 0.574722  [   32/   54]
train() client id: f_00006-3-0 loss: 0.543843  [   32/   54]
train() client id: f_00006-4-0 loss: 0.524974  [   32/   54]
train() client id: f_00006-5-0 loss: 0.597955  [   32/   54]
train() client id: f_00006-6-0 loss: 0.588100  [   32/   54]
train() client id: f_00006-7-0 loss: 0.484180  [   32/   54]
train() client id: f_00007-0-0 loss: 0.389276  [   32/  179]
train() client id: f_00007-0-1 loss: 0.418469  [   64/  179]
train() client id: f_00007-0-2 loss: 0.494358  [   96/  179]
train() client id: f_00007-0-3 loss: 0.787321  [  128/  179]
train() client id: f_00007-0-4 loss: 0.502632  [  160/  179]
train() client id: f_00007-1-0 loss: 0.617669  [   32/  179]
train() client id: f_00007-1-1 loss: 0.728089  [   64/  179]
train() client id: f_00007-1-2 loss: 0.330057  [   96/  179]
train() client id: f_00007-1-3 loss: 0.419467  [  128/  179]
train() client id: f_00007-1-4 loss: 0.430393  [  160/  179]
train() client id: f_00007-2-0 loss: 0.617230  [   32/  179]
train() client id: f_00007-2-1 loss: 0.529749  [   64/  179]
train() client id: f_00007-2-2 loss: 0.447446  [   96/  179]
train() client id: f_00007-2-3 loss: 0.461414  [  128/  179]
train() client id: f_00007-2-4 loss: 0.382790  [  160/  179]
train() client id: f_00007-3-0 loss: 0.530222  [   32/  179]
train() client id: f_00007-3-1 loss: 0.471949  [   64/  179]
train() client id: f_00007-3-2 loss: 0.494997  [   96/  179]
train() client id: f_00007-3-3 loss: 0.328562  [  128/  179]
train() client id: f_00007-3-4 loss: 0.474599  [  160/  179]
train() client id: f_00007-4-0 loss: 0.565261  [   32/  179]
train() client id: f_00007-4-1 loss: 0.429946  [   64/  179]
train() client id: f_00007-4-2 loss: 0.423710  [   96/  179]
train() client id: f_00007-4-3 loss: 0.441179  [  128/  179]
train() client id: f_00007-4-4 loss: 0.388595  [  160/  179]
train() client id: f_00007-5-0 loss: 0.539627  [   32/  179]
train() client id: f_00007-5-1 loss: 0.448607  [   64/  179]
train() client id: f_00007-5-2 loss: 0.519241  [   96/  179]
train() client id: f_00007-5-3 loss: 0.390660  [  128/  179]
train() client id: f_00007-5-4 loss: 0.309340  [  160/  179]
train() client id: f_00007-6-0 loss: 0.477932  [   32/  179]
train() client id: f_00007-6-1 loss: 0.419064  [   64/  179]
train() client id: f_00007-6-2 loss: 0.392380  [   96/  179]
train() client id: f_00007-6-3 loss: 0.272907  [  128/  179]
train() client id: f_00007-6-4 loss: 0.629068  [  160/  179]
train() client id: f_00007-7-0 loss: 0.459351  [   32/  179]
train() client id: f_00007-7-1 loss: 0.294959  [   64/  179]
train() client id: f_00007-7-2 loss: 0.347028  [   96/  179]
train() client id: f_00007-7-3 loss: 0.400860  [  128/  179]
train() client id: f_00007-7-4 loss: 0.576925  [  160/  179]
train() client id: f_00008-0-0 loss: 0.719872  [   32/  130]
train() client id: f_00008-0-1 loss: 0.676667  [   64/  130]
train() client id: f_00008-0-2 loss: 0.615688  [   96/  130]
train() client id: f_00008-0-3 loss: 0.707811  [  128/  130]
train() client id: f_00008-1-0 loss: 0.796788  [   32/  130]
train() client id: f_00008-1-1 loss: 0.624496  [   64/  130]
train() client id: f_00008-1-2 loss: 0.649788  [   96/  130]
train() client id: f_00008-1-3 loss: 0.674877  [  128/  130]
train() client id: f_00008-2-0 loss: 0.665042  [   32/  130]
train() client id: f_00008-2-1 loss: 0.685570  [   64/  130]
train() client id: f_00008-2-2 loss: 0.765116  [   96/  130]
train() client id: f_00008-2-3 loss: 0.589056  [  128/  130]
train() client id: f_00008-3-0 loss: 0.718405  [   32/  130]
train() client id: f_00008-3-1 loss: 0.679745  [   64/  130]
train() client id: f_00008-3-2 loss: 0.641605  [   96/  130]
train() client id: f_00008-3-3 loss: 0.710919  [  128/  130]
train() client id: f_00008-4-0 loss: 0.736164  [   32/  130]
train() client id: f_00008-4-1 loss: 0.719334  [   64/  130]
train() client id: f_00008-4-2 loss: 0.645713  [   96/  130]
train() client id: f_00008-4-3 loss: 0.624586  [  128/  130]
train() client id: f_00008-5-0 loss: 0.712795  [   32/  130]
train() client id: f_00008-5-1 loss: 0.601587  [   64/  130]
train() client id: f_00008-5-2 loss: 0.753646  [   96/  130]
train() client id: f_00008-5-3 loss: 0.689996  [  128/  130]
train() client id: f_00008-6-0 loss: 0.627511  [   32/  130]
train() client id: f_00008-6-1 loss: 0.666170  [   64/  130]
train() client id: f_00008-6-2 loss: 0.698666  [   96/  130]
train() client id: f_00008-6-3 loss: 0.709367  [  128/  130]
train() client id: f_00008-7-0 loss: 0.751313  [   32/  130]
train() client id: f_00008-7-1 loss: 0.700908  [   64/  130]
train() client id: f_00008-7-2 loss: 0.642378  [   96/  130]
train() client id: f_00008-7-3 loss: 0.664543  [  128/  130]
train() client id: f_00009-0-0 loss: 0.745717  [   32/  118]
train() client id: f_00009-0-1 loss: 0.956969  [   64/  118]
train() client id: f_00009-0-2 loss: 0.638408  [   96/  118]
train() client id: f_00009-1-0 loss: 0.820061  [   32/  118]
train() client id: f_00009-1-1 loss: 0.928569  [   64/  118]
train() client id: f_00009-1-2 loss: 0.822343  [   96/  118]
train() client id: f_00009-2-0 loss: 0.756069  [   32/  118]
train() client id: f_00009-2-1 loss: 0.715820  [   64/  118]
train() client id: f_00009-2-2 loss: 0.803444  [   96/  118]
train() client id: f_00009-3-0 loss: 0.708797  [   32/  118]
train() client id: f_00009-3-1 loss: 0.985101  [   64/  118]
train() client id: f_00009-3-2 loss: 0.702560  [   96/  118]
train() client id: f_00009-4-0 loss: 0.777296  [   32/  118]
train() client id: f_00009-4-1 loss: 0.756237  [   64/  118]
train() client id: f_00009-4-2 loss: 0.707641  [   96/  118]
train() client id: f_00009-5-0 loss: 0.673202  [   32/  118]
train() client id: f_00009-5-1 loss: 0.869702  [   64/  118]
train() client id: f_00009-5-2 loss: 0.765345  [   96/  118]
train() client id: f_00009-6-0 loss: 0.830684  [   32/  118]
train() client id: f_00009-6-1 loss: 0.718034  [   64/  118]
train() client id: f_00009-6-2 loss: 0.746387  [   96/  118]
train() client id: f_00009-7-0 loss: 0.672068  [   32/  118]
train() client id: f_00009-7-1 loss: 0.858457  [   64/  118]
train() client id: f_00009-7-2 loss: 0.679646  [   96/  118]
At round 68 accuracy: 0.6472148541114059
At round 68 training accuracy: 0.5922199865861838
At round 68 training loss: 0.823981389707496
update_location
xs = [  -3.9056584     4.20031788  360.00902392   18.81129433    0.97929623
    3.95640986 -322.44319194 -301.32485185  344.66397685 -287.06087855]
ys = [ 352.5879595   335.55583871    1.32061395 -322.45517586  314.35018685
  297.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [366.51537942 350.16476631 373.64186238 338.12897722 329.87421692
 314.17975995 337.60406184 317.48597228 359.3075659  304.00651284]
dists_bs = [246.59837202 240.60534339 562.4937198  533.87683018 224.47090518
 217.04541519 230.84655055 215.16464081 542.987262   204.44591885]
uav_gains = [9.97297579e-13 1.20746354e-12 9.24956572e-13 1.41687733e-12
 1.59815306e-12 2.06205032e-12 1.42738234e-12 1.94864202e-12
 1.08135149e-12 2.47729216e-12]
bs_gains = [2.21677447e-11 2.37486723e-11 2.20274684e-12 2.54952386e-12
 2.88433648e-11 3.16921979e-11 2.66678863e-11 3.24739829e-11
 2.43154935e-12 3.74692015e-11]
Round 69
-------------------------------
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.8731924  3.7097278  1.85116443 0.70130907 4.2759113  2.0588358
 0.85327763 2.56982483 1.88245466 1.66965903]
obj_prev = 21.445356959883735
eta_min = 1.2228941532952692e-50	eta_max = 0.9500254409851863
af = 4.439896748873053	bf = 0.784423253509249	zeta = 4.883886423760359	eta = 0.9090909090909091
af = 4.439896748873053	bf = 0.784423253509249	zeta = 13.283496720526747	eta = 0.3342415662294824
af = 4.439896748873053	bf = 0.784423253509249	zeta = 8.486662978303045	eta = 0.5231616667498248
af = 4.439896748873053	bf = 0.784423253509249	zeta = 7.677394178566005	eta = 0.5783077754778437
af = 4.439896748873053	bf = 0.784423253509249	zeta = 7.626245838238497	eta = 0.5821864181995182
af = 4.439896748873053	bf = 0.784423253509249	zeta = 7.626008957353865	eta = 0.5822045022110287
eta = 0.5822045022110287
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [0.04463654 0.09387843 0.04392802 0.0152331  0.10840302 0.0517217
 0.01912994 0.06341222 0.04605357 0.04180247]
ene_total = [0.78877956 1.11412037 0.79451743 0.41620728 1.26807169 0.64932725
 0.45766691 0.90423052 0.69485736 0.53823058]
ti_comp = [2.16111529 2.35459485 2.14875789 2.20941898 2.35838257 2.36010268
 2.21029221 2.24279005 2.26158603 2.36299155]
ti_coms = [0.27840468 0.08492511 0.29076208 0.23010099 0.0811374  0.07941728
 0.22922776 0.19672992 0.17793394 0.07652842]
t_total = [26.54971046 26.54971046 26.54971046 26.54971046 26.54971046 26.54971046
 26.54971046 26.54971046 26.54971046 26.54971046]
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [1.19013336e-06 9.32706721e-06 1.14743992e-06 4.52573450e-08
 1.43144819e-05 1.55251797e-06 8.95615827e-08 3.16826659e-06
 1.19355847e-06 8.17638912e-07]
ene_total = [0.30300549 0.09252695 0.3164538  0.25042331 0.088459   0.08644808
 0.24947344 0.214139   0.19366152 0.08329608]
optimize_network iter = 0 obj = 1.8778866839980972
eta = 0.5822045022110287
freqs = [10327199.39136077 19935155.93888518 10221724.11669465  3447309.92299876
 22982492.36545908 10957510.88478662  4327469.55320673 14136905.79686903
 10181697.58740329  8845243.50076053]
eta_min = 0.5822045022110292	eta_max = 0.7888487440198617
af = 0.000276093861788328	bf = 0.784423253509249	zeta = 0.00030370324796716086	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [2.09696425e-07 1.64338949e-06 2.02174023e-07 7.97415130e-09
 2.52215070e-06 2.73547049e-07 1.57803692e-08 5.58235069e-07
 2.10299915e-07 1.44064491e-07]
ene_total = [1.3924886  0.42484648 1.45429544 1.1508812  0.40594569 0.39722982
 1.14651401 0.98399884 0.88997085 0.38277432]
ti_comp = [0.9545136  1.14799316 0.9421562  1.00281729 1.15178087 1.15350099
 1.00369052 1.03618836 1.05498434 1.15638985]
ti_coms = [0.27840468 0.08492511 0.29076208 0.23010099 0.0811374  0.07941728
 0.22922776 0.19672992 0.17793394 0.07652842]
t_total = [26.54971046 26.54971046 26.54971046 26.54971046 26.54971046 26.54971046
 26.54971046 26.54971046 26.54971046 26.54971046]
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [5.14272772e-07 3.30754040e-06 5.03112813e-07 1.85185886e-08
 5.05907005e-06 5.47858951e-07 3.66123645e-08 1.25120311e-06
 4.62364171e-07 2.87795246e-07]
ene_total = [0.59952879 0.1829493  0.62613904 0.49550078 0.17483054 0.17102929
 0.49362075 0.42366592 0.3831736  0.1648028 ]
optimize_network iter = 1 obj = 3.715240812383447
eta = 0.7888487440198617
freqs = [10252122.85547994 17928004.42999087 10221724.11669466  3330213.08308708
 20633691.92866442  9830152.63900685  4178489.52262742 13416514.38117568
  9570237.57402436  7925070.10389261]
eta_min = 0.7888487440198628	eta_max = 0.7888487440198605
af = 0.00022982602808494754	bf = 0.784423253509249	zeta = 0.0002528086308934423	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [2.06658611e-07 1.32912288e-06 2.02174023e-07 7.44162632e-09
 2.03296858e-06 2.20155092e-07 1.47125432e-08 5.02791340e-07
 1.85799332e-07 1.15649455e-07]
ene_total = [1.39248845 0.42483077 1.45429544 1.15088117 0.40592122 0.39722715
 1.14651395 0.98399607 0.88996963 0.38277289]
ti_comp = [0.9545136  1.14799316 0.9421562  1.00281729 1.15178087 1.15350099
 1.00369052 1.03618836 1.05498434 1.15638985]
ti_coms = [0.27840468 0.08492511 0.29076208 0.23010099 0.0811374  0.07941728
 0.22922776 0.19672992 0.17793394 0.07652842]
t_total = [26.54971046 26.54971046 26.54971046 26.54971046 26.54971046 26.54971046
 26.54971046 26.54971046 26.54971046 26.54971046]
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [5.14272772e-07 3.30754040e-06 5.03112813e-07 1.85185886e-08
 5.05907005e-06 5.47858951e-07 3.66123645e-08 1.25120311e-06
 4.62364171e-07 2.87795246e-07]
ene_total = [0.59952879 0.1829493  0.62613904 0.49550078 0.17483054 0.17102929
 0.49362075 0.42366592 0.3831736  0.1648028 ]
optimize_network iter = 2 obj = 3.715240812383425
eta = 0.7888487440198605
freqs = [10252122.85547993 17928004.42999088 10221724.11669465  3330213.08308708
 20633691.92866442  9830152.63900685  4178489.52262742 13416514.38117567
  9570237.57402436  7925070.10389262]
Done!
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [4.63517562e-07 2.98110875e-06 4.53459015e-07 1.66909304e-08
 4.55977439e-06 4.93789014e-07 3.29989742e-08 1.12771791e-06
 4.16731985e-07 2.59391820e-07]
ene_total = [0.02784093 0.00849549 0.02907666 0.02301012 0.0081183  0.00794222
 0.02292281 0.01967412 0.01779381 0.0076531 ]
At round 69 energy consumption: 0.17252756466419225
At round 69 eta: 0.7888487440198605
At round 69 a_n: 4.204393568402164
At round 69 local rounds: 7.766500565656417
At round 69 global rounds: 21.53403916195109
gradient difference: 0.7443931698799133
train() client id: f_00000-0-0 loss: 1.064797  [   32/  126]
train() client id: f_00000-0-1 loss: 0.889910  [   64/  126]
train() client id: f_00000-0-2 loss: 0.858874  [   96/  126]
train() client id: f_00000-1-0 loss: 0.915669  [   32/  126]
train() client id: f_00000-1-1 loss: 0.867425  [   64/  126]
train() client id: f_00000-1-2 loss: 0.904154  [   96/  126]
train() client id: f_00000-2-0 loss: 0.804125  [   32/  126]
train() client id: f_00000-2-1 loss: 0.943226  [   64/  126]
train() client id: f_00000-2-2 loss: 0.970891  [   96/  126]
train() client id: f_00000-3-0 loss: 0.761717  [   32/  126]
train() client id: f_00000-3-1 loss: 0.870703  [   64/  126]
train() client id: f_00000-3-2 loss: 1.036509  [   96/  126]
train() client id: f_00000-4-0 loss: 1.026510  [   32/  126]
train() client id: f_00000-4-1 loss: 0.738114  [   64/  126]
train() client id: f_00000-4-2 loss: 0.695868  [   96/  126]
train() client id: f_00000-5-0 loss: 0.989427  [   32/  126]
train() client id: f_00000-5-1 loss: 0.788517  [   64/  126]
train() client id: f_00000-5-2 loss: 0.794010  [   96/  126]
train() client id: f_00000-6-0 loss: 0.981936  [   32/  126]
train() client id: f_00000-6-1 loss: 0.880846  [   64/  126]
train() client id: f_00000-6-2 loss: 0.849032  [   96/  126]
train() client id: f_00001-0-0 loss: 0.483558  [   32/  265]
train() client id: f_00001-0-1 loss: 0.416859  [   64/  265]
train() client id: f_00001-0-2 loss: 0.432897  [   96/  265]
train() client id: f_00001-0-3 loss: 0.333302  [  128/  265]
train() client id: f_00001-0-4 loss: 0.350335  [  160/  265]
train() client id: f_00001-0-5 loss: 0.540457  [  192/  265]
train() client id: f_00001-0-6 loss: 0.607521  [  224/  265]
train() client id: f_00001-0-7 loss: 0.366188  [  256/  265]
train() client id: f_00001-1-0 loss: 0.461437  [   32/  265]
train() client id: f_00001-1-1 loss: 0.343320  [   64/  265]
train() client id: f_00001-1-2 loss: 0.674160  [   96/  265]
train() client id: f_00001-1-3 loss: 0.397679  [  128/  265]
train() client id: f_00001-1-4 loss: 0.470316  [  160/  265]
train() client id: f_00001-1-5 loss: 0.427539  [  192/  265]
train() client id: f_00001-1-6 loss: 0.368597  [  224/  265]
train() client id: f_00001-1-7 loss: 0.342350  [  256/  265]
train() client id: f_00001-2-0 loss: 0.394856  [   32/  265]
train() client id: f_00001-2-1 loss: 0.307134  [   64/  265]
train() client id: f_00001-2-2 loss: 0.341920  [   96/  265]
train() client id: f_00001-2-3 loss: 0.402296  [  128/  265]
train() client id: f_00001-2-4 loss: 0.505931  [  160/  265]
train() client id: f_00001-2-5 loss: 0.359876  [  192/  265]
train() client id: f_00001-2-6 loss: 0.531144  [  224/  265]
train() client id: f_00001-2-7 loss: 0.583161  [  256/  265]
train() client id: f_00001-3-0 loss: 0.306496  [   32/  265]
train() client id: f_00001-3-1 loss: 0.363761  [   64/  265]
train() client id: f_00001-3-2 loss: 0.516623  [   96/  265]
train() client id: f_00001-3-3 loss: 0.468518  [  128/  265]
train() client id: f_00001-3-4 loss: 0.348560  [  160/  265]
train() client id: f_00001-3-5 loss: 0.453809  [  192/  265]
train() client id: f_00001-3-6 loss: 0.603613  [  224/  265]
train() client id: f_00001-3-7 loss: 0.343694  [  256/  265]
train() client id: f_00001-4-0 loss: 0.424863  [   32/  265]
train() client id: f_00001-4-1 loss: 0.403873  [   64/  265]
train() client id: f_00001-4-2 loss: 0.427515  [   96/  265]
train() client id: f_00001-4-3 loss: 0.411452  [  128/  265]
train() client id: f_00001-4-4 loss: 0.406710  [  160/  265]
train() client id: f_00001-4-5 loss: 0.473910  [  192/  265]
train() client id: f_00001-4-6 loss: 0.380752  [  224/  265]
train() client id: f_00001-4-7 loss: 0.435405  [  256/  265]
train() client id: f_00001-5-0 loss: 0.317259  [   32/  265]
train() client id: f_00001-5-1 loss: 0.366903  [   64/  265]
train() client id: f_00001-5-2 loss: 0.635453  [   96/  265]
train() client id: f_00001-5-3 loss: 0.370291  [  128/  265]
train() client id: f_00001-5-4 loss: 0.431741  [  160/  265]
train() client id: f_00001-5-5 loss: 0.448532  [  192/  265]
train() client id: f_00001-5-6 loss: 0.391167  [  224/  265]
train() client id: f_00001-5-7 loss: 0.398319  [  256/  265]
train() client id: f_00001-6-0 loss: 0.471332  [   32/  265]
train() client id: f_00001-6-1 loss: 0.371704  [   64/  265]
train() client id: f_00001-6-2 loss: 0.373151  [   96/  265]
train() client id: f_00001-6-3 loss: 0.471270  [  128/  265]
train() client id: f_00001-6-4 loss: 0.457608  [  160/  265]
train() client id: f_00001-6-5 loss: 0.435561  [  192/  265]
train() client id: f_00001-6-6 loss: 0.328025  [  224/  265]
train() client id: f_00001-6-7 loss: 0.434452  [  256/  265]
train() client id: f_00002-0-0 loss: 0.853151  [   32/  124]
train() client id: f_00002-0-1 loss: 1.252207  [   64/  124]
train() client id: f_00002-0-2 loss: 1.030257  [   96/  124]
train() client id: f_00002-1-0 loss: 1.071942  [   32/  124]
train() client id: f_00002-1-1 loss: 0.836306  [   64/  124]
train() client id: f_00002-1-2 loss: 1.118120  [   96/  124]
train() client id: f_00002-2-0 loss: 1.124292  [   32/  124]
train() client id: f_00002-2-1 loss: 1.036721  [   64/  124]
train() client id: f_00002-2-2 loss: 1.080516  [   96/  124]
train() client id: f_00002-3-0 loss: 1.027483  [   32/  124]
train() client id: f_00002-3-1 loss: 0.915051  [   64/  124]
train() client id: f_00002-3-2 loss: 0.882949  [   96/  124]
train() client id: f_00002-4-0 loss: 1.026373  [   32/  124]
train() client id: f_00002-4-1 loss: 0.948220  [   64/  124]
train() client id: f_00002-4-2 loss: 0.910122  [   96/  124]
train() client id: f_00002-5-0 loss: 0.891587  [   32/  124]
train() client id: f_00002-5-1 loss: 1.031180  [   64/  124]
train() client id: f_00002-5-2 loss: 1.033267  [   96/  124]
train() client id: f_00002-6-0 loss: 0.909819  [   32/  124]
train() client id: f_00002-6-1 loss: 0.996554  [   64/  124]
train() client id: f_00002-6-2 loss: 1.256463  [   96/  124]
train() client id: f_00003-0-0 loss: 0.504418  [   32/   43]
train() client id: f_00003-1-0 loss: 0.612010  [   32/   43]
train() client id: f_00003-2-0 loss: 0.659209  [   32/   43]
train() client id: f_00003-3-0 loss: 0.584416  [   32/   43]
train() client id: f_00003-4-0 loss: 0.587618  [   32/   43]
train() client id: f_00003-5-0 loss: 0.806626  [   32/   43]
train() client id: f_00003-6-0 loss: 0.815570  [   32/   43]
train() client id: f_00004-0-0 loss: 0.819994  [   32/  306]
train() client id: f_00004-0-1 loss: 0.710230  [   64/  306]
train() client id: f_00004-0-2 loss: 0.961401  [   96/  306]
train() client id: f_00004-0-3 loss: 0.865502  [  128/  306]
train() client id: f_00004-0-4 loss: 0.980839  [  160/  306]
train() client id: f_00004-0-5 loss: 0.682352  [  192/  306]
train() client id: f_00004-0-6 loss: 0.869885  [  224/  306]
train() client id: f_00004-0-7 loss: 0.896158  [  256/  306]
train() client id: f_00004-0-8 loss: 0.741952  [  288/  306]
train() client id: f_00004-1-0 loss: 0.889114  [   32/  306]
train() client id: f_00004-1-1 loss: 0.806657  [   64/  306]
train() client id: f_00004-1-2 loss: 0.748503  [   96/  306]
train() client id: f_00004-1-3 loss: 0.775788  [  128/  306]
train() client id: f_00004-1-4 loss: 1.052215  [  160/  306]
train() client id: f_00004-1-5 loss: 0.943933  [  192/  306]
train() client id: f_00004-1-6 loss: 0.749012  [  224/  306]
train() client id: f_00004-1-7 loss: 0.775422  [  256/  306]
train() client id: f_00004-1-8 loss: 0.778475  [  288/  306]
train() client id: f_00004-2-0 loss: 0.943198  [   32/  306]
train() client id: f_00004-2-1 loss: 0.916883  [   64/  306]
train() client id: f_00004-2-2 loss: 0.783851  [   96/  306]
train() client id: f_00004-2-3 loss: 0.823033  [  128/  306]
train() client id: f_00004-2-4 loss: 0.741388  [  160/  306]
train() client id: f_00004-2-5 loss: 0.656540  [  192/  306]
train() client id: f_00004-2-6 loss: 0.833616  [  224/  306]
train() client id: f_00004-2-7 loss: 0.809811  [  256/  306]
train() client id: f_00004-2-8 loss: 0.928180  [  288/  306]
train() client id: f_00004-3-0 loss: 0.920055  [   32/  306]
train() client id: f_00004-3-1 loss: 0.786085  [   64/  306]
train() client id: f_00004-3-2 loss: 0.909950  [   96/  306]
train() client id: f_00004-3-3 loss: 0.849431  [  128/  306]
train() client id: f_00004-3-4 loss: 0.826882  [  160/  306]
train() client id: f_00004-3-5 loss: 0.889809  [  192/  306]
train() client id: f_00004-3-6 loss: 0.815638  [  224/  306]
train() client id: f_00004-3-7 loss: 0.783859  [  256/  306]
train() client id: f_00004-3-8 loss: 0.810546  [  288/  306]
train() client id: f_00004-4-0 loss: 0.725298  [   32/  306]
train() client id: f_00004-4-1 loss: 0.850967  [   64/  306]
train() client id: f_00004-4-2 loss: 0.983747  [   96/  306]
train() client id: f_00004-4-3 loss: 0.776564  [  128/  306]
train() client id: f_00004-4-4 loss: 0.903736  [  160/  306]
train() client id: f_00004-4-5 loss: 0.930791  [  192/  306]
train() client id: f_00004-4-6 loss: 0.822532  [  224/  306]
train() client id: f_00004-4-7 loss: 0.766539  [  256/  306]
train() client id: f_00004-4-8 loss: 0.908496  [  288/  306]
train() client id: f_00004-5-0 loss: 0.855088  [   32/  306]
train() client id: f_00004-5-1 loss: 0.917028  [   64/  306]
train() client id: f_00004-5-2 loss: 0.839348  [   96/  306]
train() client id: f_00004-5-3 loss: 0.926174  [  128/  306]
train() client id: f_00004-5-4 loss: 0.893704  [  160/  306]
train() client id: f_00004-5-5 loss: 0.809743  [  192/  306]
train() client id: f_00004-5-6 loss: 0.787598  [  224/  306]
train() client id: f_00004-5-7 loss: 0.785177  [  256/  306]
train() client id: f_00004-5-8 loss: 0.706644  [  288/  306]
train() client id: f_00004-6-0 loss: 0.887172  [   32/  306]
train() client id: f_00004-6-1 loss: 0.824907  [   64/  306]
train() client id: f_00004-6-2 loss: 0.824623  [   96/  306]
train() client id: f_00004-6-3 loss: 0.846776  [  128/  306]
train() client id: f_00004-6-4 loss: 0.880688  [  160/  306]
train() client id: f_00004-6-5 loss: 0.750275  [  192/  306]
train() client id: f_00004-6-6 loss: 0.864475  [  224/  306]
train() client id: f_00004-6-7 loss: 0.892965  [  256/  306]
train() client id: f_00004-6-8 loss: 0.883573  [  288/  306]
train() client id: f_00005-0-0 loss: 0.524896  [   32/  146]
train() client id: f_00005-0-1 loss: 0.740843  [   64/  146]
train() client id: f_00005-0-2 loss: 0.686008  [   96/  146]
train() client id: f_00005-0-3 loss: 0.842119  [  128/  146]
train() client id: f_00005-1-0 loss: 0.711387  [   32/  146]
train() client id: f_00005-1-1 loss: 0.852202  [   64/  146]
train() client id: f_00005-1-2 loss: 0.627078  [   96/  146]
train() client id: f_00005-1-3 loss: 0.849308  [  128/  146]
train() client id: f_00005-2-0 loss: 0.637179  [   32/  146]
train() client id: f_00005-2-1 loss: 0.684825  [   64/  146]
train() client id: f_00005-2-2 loss: 0.676631  [   96/  146]
train() client id: f_00005-2-3 loss: 0.814225  [  128/  146]
train() client id: f_00005-3-0 loss: 0.706904  [   32/  146]
train() client id: f_00005-3-1 loss: 0.763011  [   64/  146]
train() client id: f_00005-3-2 loss: 0.905273  [   96/  146]
train() client id: f_00005-3-3 loss: 0.681641  [  128/  146]
train() client id: f_00005-4-0 loss: 1.106030  [   32/  146]
train() client id: f_00005-4-1 loss: 0.604510  [   64/  146]
train() client id: f_00005-4-2 loss: 0.527270  [   96/  146]
train() client id: f_00005-4-3 loss: 0.581774  [  128/  146]
train() client id: f_00005-5-0 loss: 0.598171  [   32/  146]
train() client id: f_00005-5-1 loss: 0.678546  [   64/  146]
train() client id: f_00005-5-2 loss: 0.683841  [   96/  146]
train() client id: f_00005-5-3 loss: 0.829330  [  128/  146]
train() client id: f_00005-6-0 loss: 0.951636  [   32/  146]
train() client id: f_00005-6-1 loss: 0.515634  [   64/  146]
train() client id: f_00005-6-2 loss: 0.752029  [   96/  146]
train() client id: f_00005-6-3 loss: 0.557306  [  128/  146]
train() client id: f_00006-0-0 loss: 0.564093  [   32/   54]
train() client id: f_00006-1-0 loss: 0.538577  [   32/   54]
train() client id: f_00006-2-0 loss: 0.508400  [   32/   54]
train() client id: f_00006-3-0 loss: 0.609902  [   32/   54]
train() client id: f_00006-4-0 loss: 0.539787  [   32/   54]
train() client id: f_00006-5-0 loss: 0.523931  [   32/   54]
train() client id: f_00006-6-0 loss: 0.459349  [   32/   54]
train() client id: f_00007-0-0 loss: 0.381076  [   32/  179]
train() client id: f_00007-0-1 loss: 0.493031  [   64/  179]
train() client id: f_00007-0-2 loss: 0.368214  [   96/  179]
train() client id: f_00007-0-3 loss: 0.469326  [  128/  179]
train() client id: f_00007-0-4 loss: 0.850982  [  160/  179]
train() client id: f_00007-1-0 loss: 0.638985  [   32/  179]
train() client id: f_00007-1-1 loss: 0.356341  [   64/  179]
train() client id: f_00007-1-2 loss: 0.492463  [   96/  179]
train() client id: f_00007-1-3 loss: 0.511211  [  128/  179]
train() client id: f_00007-1-4 loss: 0.416360  [  160/  179]
train() client id: f_00007-2-0 loss: 0.363928  [   32/  179]
train() client id: f_00007-2-1 loss: 0.582010  [   64/  179]
train() client id: f_00007-2-2 loss: 0.382277  [   96/  179]
train() client id: f_00007-2-3 loss: 0.352642  [  128/  179]
train() client id: f_00007-2-4 loss: 0.666920  [  160/  179]
train() client id: f_00007-3-0 loss: 0.519838  [   32/  179]
train() client id: f_00007-3-1 loss: 0.451653  [   64/  179]
train() client id: f_00007-3-2 loss: 0.517504  [   96/  179]
train() client id: f_00007-3-3 loss: 0.324263  [  128/  179]
train() client id: f_00007-3-4 loss: 0.589767  [  160/  179]
train() client id: f_00007-4-0 loss: 0.575384  [   32/  179]
train() client id: f_00007-4-1 loss: 0.456281  [   64/  179]
train() client id: f_00007-4-2 loss: 0.296028  [   96/  179]
train() client id: f_00007-4-3 loss: 0.587160  [  128/  179]
train() client id: f_00007-4-4 loss: 0.489100  [  160/  179]
train() client id: f_00007-5-0 loss: 0.396816  [   32/  179]
train() client id: f_00007-5-1 loss: 0.419526  [   64/  179]
train() client id: f_00007-5-2 loss: 0.499820  [   96/  179]
train() client id: f_00007-5-3 loss: 0.396984  [  128/  179]
train() client id: f_00007-5-4 loss: 0.571120  [  160/  179]
train() client id: f_00007-6-0 loss: 0.667819  [   32/  179]
train() client id: f_00007-6-1 loss: 0.480444  [   64/  179]
train() client id: f_00007-6-2 loss: 0.331611  [   96/  179]
train() client id: f_00007-6-3 loss: 0.355852  [  128/  179]
train() client id: f_00007-6-4 loss: 0.374279  [  160/  179]
train() client id: f_00008-0-0 loss: 0.829846  [   32/  130]
train() client id: f_00008-0-1 loss: 0.780987  [   64/  130]
train() client id: f_00008-0-2 loss: 0.726396  [   96/  130]
train() client id: f_00008-0-3 loss: 0.620691  [  128/  130]
train() client id: f_00008-1-0 loss: 0.675623  [   32/  130]
train() client id: f_00008-1-1 loss: 0.829834  [   64/  130]
train() client id: f_00008-1-2 loss: 0.759249  [   96/  130]
train() client id: f_00008-1-3 loss: 0.687848  [  128/  130]
train() client id: f_00008-2-0 loss: 0.700325  [   32/  130]
train() client id: f_00008-2-1 loss: 0.697687  [   64/  130]
train() client id: f_00008-2-2 loss: 0.797531  [   96/  130]
train() client id: f_00008-2-3 loss: 0.718714  [  128/  130]
train() client id: f_00008-3-0 loss: 0.811063  [   32/  130]
train() client id: f_00008-3-1 loss: 0.664400  [   64/  130]
train() client id: f_00008-3-2 loss: 0.734232  [   96/  130]
train() client id: f_00008-3-3 loss: 0.748490  [  128/  130]
train() client id: f_00008-4-0 loss: 0.692980  [   32/  130]
train() client id: f_00008-4-1 loss: 0.701661  [   64/  130]
train() client id: f_00008-4-2 loss: 0.740559  [   96/  130]
train() client id: f_00008-4-3 loss: 0.819113  [  128/  130]
train() client id: f_00008-5-0 loss: 0.709417  [   32/  130]
train() client id: f_00008-5-1 loss: 0.686974  [   64/  130]
train() client id: f_00008-5-2 loss: 0.735924  [   96/  130]
train() client id: f_00008-5-3 loss: 0.822519  [  128/  130]
train() client id: f_00008-6-0 loss: 0.756516  [   32/  130]
train() client id: f_00008-6-1 loss: 0.688307  [   64/  130]
train() client id: f_00008-6-2 loss: 0.753702  [   96/  130]
train() client id: f_00008-6-3 loss: 0.710477  [  128/  130]
train() client id: f_00009-0-0 loss: 0.935086  [   32/  118]
train() client id: f_00009-0-1 loss: 0.909415  [   64/  118]
train() client id: f_00009-0-2 loss: 0.993251  [   96/  118]
train() client id: f_00009-1-0 loss: 1.093268  [   32/  118]
train() client id: f_00009-1-1 loss: 0.853745  [   64/  118]
train() client id: f_00009-1-2 loss: 1.036899  [   96/  118]
train() client id: f_00009-2-0 loss: 0.971239  [   32/  118]
train() client id: f_00009-2-1 loss: 0.869638  [   64/  118]
train() client id: f_00009-2-2 loss: 0.948628  [   96/  118]
train() client id: f_00009-3-0 loss: 0.828351  [   32/  118]
train() client id: f_00009-3-1 loss: 1.034419  [   64/  118]
train() client id: f_00009-3-2 loss: 0.988911  [   96/  118]
train() client id: f_00009-4-0 loss: 0.895170  [   32/  118]
train() client id: f_00009-4-1 loss: 0.773105  [   64/  118]
train() client id: f_00009-4-2 loss: 1.032749  [   96/  118]
train() client id: f_00009-5-0 loss: 0.947652  [   32/  118]
train() client id: f_00009-5-1 loss: 0.915242  [   64/  118]
train() client id: f_00009-5-2 loss: 0.832286  [   96/  118]
train() client id: f_00009-6-0 loss: 0.983751  [   32/  118]
train() client id: f_00009-6-1 loss: 0.966652  [   64/  118]
train() client id: f_00009-6-2 loss: 0.865182  [   96/  118]
At round 69 accuracy: 0.6472148541114059
At round 69 training accuracy: 0.5902079141515761
At round 69 training loss: 0.8283330739442266
update_location
xs = [  -3.9056584     4.20031788  365.00902392   18.81129433    0.97929623
    3.95640986 -327.44319194 -306.32485185  349.66397685 -292.06087855]
ys = [ 357.5879595   340.55583871    1.32061395 -327.45517586  319.35018685
  302.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [371.32789142 354.95904264 378.4617967  342.90050597 334.64234768
 318.92328712 342.382731   322.23530396 364.10653205 308.73219566]
dists_bs = [250.22317375 243.97026388 567.25150952 538.53878371 227.60863152
 219.91328691 234.07170231 218.13773441 547.77441202 207.22872031]
uav_gains = [9.47393004e-13 1.13831941e-12 8.81152900e-13 1.32700416e-12
 1.48913730e-12 1.90221719e-12 1.33628878e-12 1.80140553e-12
 1.02406515e-12 2.27098709e-12]
bs_gains = [2.12802637e-11 2.28428757e-11 2.15140531e-12 2.48820711e-12
 2.77437810e-11 3.05485056e-11 2.56517576e-11 3.12498438e-11
 2.37251633e-12 3.60773175e-11]
Round 70
-------------------------------
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.73561523 3.43059311 1.71527241 0.65186493 3.95409396 1.90401388
 0.79238643 2.37939759 1.74146138 1.54414415]
obj_prev = 19.848843077614966
eta_min = 1.1829730818327539e-54	eta_max = 0.9523787254678187
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 4.515956513396188	eta = 0.9090909090909091
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 12.490584420921488	eta = 0.32868077856324013
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 7.913053448586266	eta = 0.518815529157309
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 7.145329232998529	eta = 0.5745592509885677
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 7.096743771047384	eta = 0.5784927770574487
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 7.096517585671171	eta = 0.5785112152005008
eta = 0.5785112152005008
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [0.04516167 0.09498287 0.04444481 0.01541231 0.10967833 0.05233018
 0.019355   0.06415824 0.04659537 0.04229426]
ene_total = [0.7365201  1.0329747  0.74175636 0.39122836 1.17571715 0.60187953
 0.42968967 0.84367489 0.64421599 0.49886084]
ti_comp = [2.36984321 2.57086067 2.35742536 2.41850329 2.57471655 2.57650502
 2.41937284 2.45238669 2.47671475 2.57942159]
ti_coms = [0.28674182 0.08572436 0.29915967 0.23808174 0.08186847 0.08008
 0.23721219 0.20419834 0.17987028 0.07716343]
t_total = [26.49970627 26.49970627 26.49970627 26.49970627 26.49970627 26.49970627
 26.49970627 26.49970627 26.49970627 26.49970627]
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [1.02506317e-06 8.10324115e-06 9.87341466e-07 3.91192234e-08
 1.24389480e-05 1.34919525e-06 7.74201308e-08 2.74447586e-06
 1.03075701e-06 7.10688982e-07]
ene_total = [0.28603811 0.08559175 0.29842467 0.23748935 0.08178872 0.07989408
 0.23662235 0.2037173  0.17943272 0.0769784 ]
optimize_network iter = 0 obj = 1.7659774351917215
eta = 0.5785112152005008
freqs = [ 9528407.84474552 18472970.62942295  9426557.92578857  3186333.13230265
 21299107.60121082 10155265.18397391  4000003.44569053 13080775.34726803
  9406688.61857887  8198399.6009876 ]
eta_min = 0.578511215200502	eta_max = 0.8002666857305597
af = 0.00021898789766085596	bf = 0.7442198316471329	zeta = 0.00024088668742694157	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [1.78511656e-07 1.41115497e-06 1.71942534e-07 6.81249461e-09
 2.16620522e-06 2.34958278e-07 1.34824820e-08 4.77942182e-07
 1.79503221e-07 1.23764340e-07]
ene_total = [1.32614126 0.39652609 1.38357144 1.10108858 0.3787282  0.37036751
 1.09706736 0.9444053  0.83187829 0.35687372]
ti_comp = [0.9721494  1.17316687 0.95973155 1.02080948 1.17702275 1.17881122
 1.02167903 1.05469288 1.07902094 1.18172779]
ti_coms = [0.28674182 0.08572436 0.29915967 0.23808174 0.08186847 0.08008
 0.23721219 0.20419834 0.17987028 0.07716343]
t_total = [26.49970627 26.49970627 26.49970627 26.49970627 26.49970627 26.49970627
 26.49970627 26.49970627 26.49970627 26.49970627]
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [4.11016540e-07 2.62561918e-06 4.01956633e-07 1.48159555e-08
 4.01613458e-06 4.34894323e-07 2.92931455e-08 1.00119980e-06
 3.66424074e-07 2.28467704e-07]
ene_total = [0.60360123 0.18050535 0.62974066 0.50116324 0.17241798 0.16857786
 0.49933314 0.42985932 0.37863531 0.16243412]
optimize_network iter = 1 obj = 3.726268206640733
eta = 0.8002666857305597
freqs = [ 9456245.97160665 16480383.53188894  9426557.92578857  3073300.22792621
 18967836.56712048  9036283.02009842  3856208.50116259 12382497.36052094
  8790118.03381017  7285272.29487025]
eta_min = 0.800266685730563	eta_max = 0.8002666857305585
af = 0.00017982620690873804	bf = 0.7442198316471329	zeta = 0.00019780882759961186	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [1.75818035e-07 1.12314508e-06 1.71942534e-07 6.33773081e-09
 1.71795736e-06 1.86032089e-07 1.25305500e-08 4.28277123e-07
 1.56742989e-07 9.77302347e-08]
ene_total = [1.32614114 0.39651277 1.38357144 1.10108855 0.37870747 0.37036524
 1.09706732 0.944403   0.83187723 0.35687251]
ti_comp = [0.9721494  1.17316687 0.95973155 1.02080948 1.17702275 1.17881122
 1.02167903 1.05469288 1.07902094 1.18172779]
ti_coms = [0.28674182 0.08572436 0.29915967 0.23808174 0.08186847 0.08008
 0.23721219 0.20419834 0.17987028 0.07716343]
t_total = [26.49970627 26.49970627 26.49970627 26.49970627 26.49970627 26.49970627
 26.49970627 26.49970627 26.49970627 26.49970627]
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [4.11016540e-07 2.62561918e-06 4.01956633e-07 1.48159555e-08
 4.01613458e-06 4.34894323e-07 2.92931455e-08 1.00119980e-06
 3.66424074e-07 2.28467704e-07]
ene_total = [0.60360123 0.18050535 0.62974066 0.50116324 0.17241798 0.16857786
 0.49933314 0.42985932 0.37863531 0.16243412]
optimize_network iter = 2 obj = 3.7262682066407105
eta = 0.8002666857305585
freqs = [ 9456245.97160664 16480383.53188894  9426557.92578855  3073300.22792621
 18967836.56712048  9036283.02009842  3856208.50116259 12382497.36052093
  8790118.03381016  7285272.29487025]
Done!
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [3.94344793e-07 2.51911821e-06 3.85652375e-07 1.42149873e-08
 3.85323121e-06 4.17254039e-07 2.81049502e-08 9.60588899e-07
 3.51561096e-07 2.19200545e-07]
ene_total = [0.02867458 0.00857495 0.02991635 0.02380819 0.0081907  0.00800842
 0.02372125 0.02042079 0.01798738 0.00771656]
At round 70 energy consumption: 0.1770191732645484
At round 70 eta: 0.8002666857305585
At round 70 a_n: 3.8618477214328486
At round 70 local rounds: 7.295939587071601
At round 70 global rounds: 21.05003656390746
gradient difference: 0.7773406505584717
train() client id: f_00000-0-0 loss: 1.060687  [   32/  126]
train() client id: f_00000-0-1 loss: 0.801669  [   64/  126]
train() client id: f_00000-0-2 loss: 1.206156  [   96/  126]
train() client id: f_00000-1-0 loss: 0.965179  [   32/  126]
train() client id: f_00000-1-1 loss: 1.085849  [   64/  126]
train() client id: f_00000-1-2 loss: 0.802859  [   96/  126]
train() client id: f_00000-2-0 loss: 0.894917  [   32/  126]
train() client id: f_00000-2-1 loss: 0.922036  [   64/  126]
train() client id: f_00000-2-2 loss: 0.866611  [   96/  126]
train() client id: f_00000-3-0 loss: 0.716771  [   32/  126]
train() client id: f_00000-3-1 loss: 1.051023  [   64/  126]
train() client id: f_00000-3-2 loss: 0.989574  [   96/  126]
train() client id: f_00000-4-0 loss: 0.910960  [   32/  126]
train() client id: f_00000-4-1 loss: 0.895409  [   64/  126]
train() client id: f_00000-4-2 loss: 0.895719  [   96/  126]
train() client id: f_00000-5-0 loss: 0.848646  [   32/  126]
train() client id: f_00000-5-1 loss: 1.009395  [   64/  126]
train() client id: f_00000-5-2 loss: 0.891608  [   96/  126]
train() client id: f_00000-6-0 loss: 1.067303  [   32/  126]
train() client id: f_00000-6-1 loss: 0.763535  [   64/  126]
train() client id: f_00000-6-2 loss: 0.906401  [   96/  126]
train() client id: f_00001-0-0 loss: 0.415571  [   32/  265]
train() client id: f_00001-0-1 loss: 0.461041  [   64/  265]
train() client id: f_00001-0-2 loss: 0.569059  [   96/  265]
train() client id: f_00001-0-3 loss: 0.485542  [  128/  265]
train() client id: f_00001-0-4 loss: 0.481103  [  160/  265]
train() client id: f_00001-0-5 loss: 0.412875  [  192/  265]
train() client id: f_00001-0-6 loss: 0.523180  [  224/  265]
train() client id: f_00001-0-7 loss: 0.483933  [  256/  265]
train() client id: f_00001-1-0 loss: 0.404180  [   32/  265]
train() client id: f_00001-1-1 loss: 0.360582  [   64/  265]
train() client id: f_00001-1-2 loss: 0.649998  [   96/  265]
train() client id: f_00001-1-3 loss: 0.487017  [  128/  265]
train() client id: f_00001-1-4 loss: 0.524711  [  160/  265]
train() client id: f_00001-1-5 loss: 0.524008  [  192/  265]
train() client id: f_00001-1-6 loss: 0.396710  [  224/  265]
train() client id: f_00001-1-7 loss: 0.413088  [  256/  265]
train() client id: f_00001-2-0 loss: 0.380711  [   32/  265]
train() client id: f_00001-2-1 loss: 0.689254  [   64/  265]
train() client id: f_00001-2-2 loss: 0.439332  [   96/  265]
train() client id: f_00001-2-3 loss: 0.528722  [  128/  265]
train() client id: f_00001-2-4 loss: 0.529445  [  160/  265]
train() client id: f_00001-2-5 loss: 0.363688  [  192/  265]
train() client id: f_00001-2-6 loss: 0.410208  [  224/  265]
train() client id: f_00001-2-7 loss: 0.398767  [  256/  265]
train() client id: f_00001-3-0 loss: 0.494780  [   32/  265]
train() client id: f_00001-3-1 loss: 0.401000  [   64/  265]
train() client id: f_00001-3-2 loss: 0.444846  [   96/  265]
train() client id: f_00001-3-3 loss: 0.574338  [  128/  265]
train() client id: f_00001-3-4 loss: 0.543048  [  160/  265]
train() client id: f_00001-3-5 loss: 0.371593  [  192/  265]
train() client id: f_00001-3-6 loss: 0.393422  [  224/  265]
train() client id: f_00001-3-7 loss: 0.367858  [  256/  265]
train() client id: f_00001-4-0 loss: 0.576400  [   32/  265]
train() client id: f_00001-4-1 loss: 0.428758  [   64/  265]
train() client id: f_00001-4-2 loss: 0.514237  [   96/  265]
train() client id: f_00001-4-3 loss: 0.446752  [  128/  265]
train() client id: f_00001-4-4 loss: 0.501150  [  160/  265]
train() client id: f_00001-4-5 loss: 0.357714  [  192/  265]
train() client id: f_00001-4-6 loss: 0.433046  [  224/  265]
train() client id: f_00001-4-7 loss: 0.438175  [  256/  265]
train() client id: f_00001-5-0 loss: 0.415147  [   32/  265]
train() client id: f_00001-5-1 loss: 0.410260  [   64/  265]
train() client id: f_00001-5-2 loss: 0.364422  [   96/  265]
train() client id: f_00001-5-3 loss: 0.418901  [  128/  265]
train() client id: f_00001-5-4 loss: 0.366017  [  160/  265]
train() client id: f_00001-5-5 loss: 0.417996  [  192/  265]
train() client id: f_00001-5-6 loss: 0.655004  [  224/  265]
train() client id: f_00001-5-7 loss: 0.538019  [  256/  265]
train() client id: f_00001-6-0 loss: 0.455497  [   32/  265]
train() client id: f_00001-6-1 loss: 0.453969  [   64/  265]
train() client id: f_00001-6-2 loss: 0.552418  [   96/  265]
train() client id: f_00001-6-3 loss: 0.659111  [  128/  265]
train() client id: f_00001-6-4 loss: 0.352145  [  160/  265]
train() client id: f_00001-6-5 loss: 0.404198  [  192/  265]
train() client id: f_00001-6-6 loss: 0.409622  [  224/  265]
train() client id: f_00001-6-7 loss: 0.384106  [  256/  265]
train() client id: f_00002-0-0 loss: 0.568862  [   32/  124]
train() client id: f_00002-0-1 loss: 0.965593  [   64/  124]
train() client id: f_00002-0-2 loss: 0.681244  [   96/  124]
train() client id: f_00002-1-0 loss: 0.642160  [   32/  124]
train() client id: f_00002-1-1 loss: 0.740397  [   64/  124]
train() client id: f_00002-1-2 loss: 0.888245  [   96/  124]
train() client id: f_00002-2-0 loss: 0.684458  [   32/  124]
train() client id: f_00002-2-1 loss: 0.818797  [   64/  124]
train() client id: f_00002-2-2 loss: 0.803550  [   96/  124]
train() client id: f_00002-3-0 loss: 0.799774  [   32/  124]
train() client id: f_00002-3-1 loss: 0.576048  [   64/  124]
train() client id: f_00002-3-2 loss: 0.708985  [   96/  124]
train() client id: f_00002-4-0 loss: 0.666514  [   32/  124]
train() client id: f_00002-4-1 loss: 0.771386  [   64/  124]
train() client id: f_00002-4-2 loss: 0.664953  [   96/  124]
train() client id: f_00002-5-0 loss: 0.637388  [   32/  124]
train() client id: f_00002-5-1 loss: 0.808727  [   64/  124]
train() client id: f_00002-5-2 loss: 0.775920  [   96/  124]
train() client id: f_00002-6-0 loss: 0.687686  [   32/  124]
train() client id: f_00002-6-1 loss: 0.925737  [   64/  124]
train() client id: f_00002-6-2 loss: 0.595187  [   96/  124]
train() client id: f_00003-0-0 loss: 0.468325  [   32/   43]
train() client id: f_00003-1-0 loss: 0.560063  [   32/   43]
train() client id: f_00003-2-0 loss: 0.418757  [   32/   43]
train() client id: f_00003-3-0 loss: 0.524020  [   32/   43]
train() client id: f_00003-4-0 loss: 0.407207  [   32/   43]
train() client id: f_00003-5-0 loss: 0.336286  [   32/   43]
train() client id: f_00003-6-0 loss: 0.309512  [   32/   43]
train() client id: f_00004-0-0 loss: 0.714626  [   32/  306]
train() client id: f_00004-0-1 loss: 1.142464  [   64/  306]
train() client id: f_00004-0-2 loss: 0.713299  [   96/  306]
train() client id: f_00004-0-3 loss: 0.760940  [  128/  306]
train() client id: f_00004-0-4 loss: 0.659469  [  160/  306]
train() client id: f_00004-0-5 loss: 0.797951  [  192/  306]
train() client id: f_00004-0-6 loss: 0.764944  [  224/  306]
train() client id: f_00004-0-7 loss: 0.719869  [  256/  306]
train() client id: f_00004-0-8 loss: 0.700969  [  288/  306]
train() client id: f_00004-1-0 loss: 0.804478  [   32/  306]
train() client id: f_00004-1-1 loss: 0.823530  [   64/  306]
train() client id: f_00004-1-2 loss: 0.697977  [   96/  306]
train() client id: f_00004-1-3 loss: 0.755937  [  128/  306]
train() client id: f_00004-1-4 loss: 0.663860  [  160/  306]
train() client id: f_00004-1-5 loss: 0.782079  [  192/  306]
train() client id: f_00004-1-6 loss: 0.699864  [  224/  306]
train() client id: f_00004-1-7 loss: 0.781970  [  256/  306]
train() client id: f_00004-1-8 loss: 0.975386  [  288/  306]
train() client id: f_00004-2-0 loss: 0.851300  [   32/  306]
train() client id: f_00004-2-1 loss: 0.673355  [   64/  306]
train() client id: f_00004-2-2 loss: 0.606112  [   96/  306]
train() client id: f_00004-2-3 loss: 0.859411  [  128/  306]
train() client id: f_00004-2-4 loss: 0.712408  [  160/  306]
train() client id: f_00004-2-5 loss: 0.998684  [  192/  306]
train() client id: f_00004-2-6 loss: 0.737974  [  224/  306]
train() client id: f_00004-2-7 loss: 0.814604  [  256/  306]
train() client id: f_00004-2-8 loss: 0.840793  [  288/  306]
train() client id: f_00004-3-0 loss: 0.762166  [   32/  306]
train() client id: f_00004-3-1 loss: 0.804135  [   64/  306]
train() client id: f_00004-3-2 loss: 0.778111  [   96/  306]
train() client id: f_00004-3-3 loss: 0.695280  [  128/  306]
train() client id: f_00004-3-4 loss: 0.669951  [  160/  306]
train() client id: f_00004-3-5 loss: 0.953208  [  192/  306]
train() client id: f_00004-3-6 loss: 0.893838  [  224/  306]
train() client id: f_00004-3-7 loss: 0.791943  [  256/  306]
train() client id: f_00004-3-8 loss: 0.774794  [  288/  306]
train() client id: f_00004-4-0 loss: 0.809954  [   32/  306]
train() client id: f_00004-4-1 loss: 0.762778  [   64/  306]
train() client id: f_00004-4-2 loss: 0.795349  [   96/  306]
train() client id: f_00004-4-3 loss: 0.793930  [  128/  306]
train() client id: f_00004-4-4 loss: 0.813626  [  160/  306]
train() client id: f_00004-4-5 loss: 0.849864  [  192/  306]
train() client id: f_00004-4-6 loss: 0.708334  [  224/  306]
train() client id: f_00004-4-7 loss: 0.724795  [  256/  306]
train() client id: f_00004-4-8 loss: 0.778065  [  288/  306]
train() client id: f_00004-5-0 loss: 0.859966  [   32/  306]
train() client id: f_00004-5-1 loss: 0.823604  [   64/  306]
train() client id: f_00004-5-2 loss: 0.764857  [   96/  306]
train() client id: f_00004-5-3 loss: 0.659889  [  128/  306]
train() client id: f_00004-5-4 loss: 0.796276  [  160/  306]
train() client id: f_00004-5-5 loss: 0.834228  [  192/  306]
train() client id: f_00004-5-6 loss: 0.861095  [  224/  306]
train() client id: f_00004-5-7 loss: 0.689059  [  256/  306]
train() client id: f_00004-5-8 loss: 0.850340  [  288/  306]
train() client id: f_00004-6-0 loss: 1.043234  [   32/  306]
train() client id: f_00004-6-1 loss: 0.684680  [   64/  306]
train() client id: f_00004-6-2 loss: 0.808462  [   96/  306]
train() client id: f_00004-6-3 loss: 0.911482  [  128/  306]
train() client id: f_00004-6-4 loss: 0.784074  [  160/  306]
train() client id: f_00004-6-5 loss: 0.673284  [  192/  306]
train() client id: f_00004-6-6 loss: 0.769294  [  224/  306]
train() client id: f_00004-6-7 loss: 0.710531  [  256/  306]
train() client id: f_00004-6-8 loss: 0.793833  [  288/  306]
train() client id: f_00005-0-0 loss: 0.697585  [   32/  146]
train() client id: f_00005-0-1 loss: 0.665804  [   64/  146]
train() client id: f_00005-0-2 loss: 0.909808  [   96/  146]
train() client id: f_00005-0-3 loss: 0.809064  [  128/  146]
train() client id: f_00005-1-0 loss: 0.511267  [   32/  146]
train() client id: f_00005-1-1 loss: 0.861826  [   64/  146]
train() client id: f_00005-1-2 loss: 0.801333  [   96/  146]
train() client id: f_00005-1-3 loss: 0.693883  [  128/  146]
train() client id: f_00005-2-0 loss: 0.898727  [   32/  146]
train() client id: f_00005-2-1 loss: 0.774718  [   64/  146]
train() client id: f_00005-2-2 loss: 0.592228  [   96/  146]
train() client id: f_00005-2-3 loss: 0.755643  [  128/  146]
train() client id: f_00005-3-0 loss: 0.921654  [   32/  146]
train() client id: f_00005-3-1 loss: 0.530926  [   64/  146]
train() client id: f_00005-3-2 loss: 0.729234  [   96/  146]
train() client id: f_00005-3-3 loss: 0.889583  [  128/  146]
train() client id: f_00005-4-0 loss: 0.734009  [   32/  146]
train() client id: f_00005-4-1 loss: 0.827510  [   64/  146]
train() client id: f_00005-4-2 loss: 0.764890  [   96/  146]
train() client id: f_00005-4-3 loss: 0.679851  [  128/  146]
train() client id: f_00005-5-0 loss: 0.716824  [   32/  146]
train() client id: f_00005-5-1 loss: 0.796605  [   64/  146]
train() client id: f_00005-5-2 loss: 0.768237  [   96/  146]
train() client id: f_00005-5-3 loss: 0.771356  [  128/  146]
train() client id: f_00005-6-0 loss: 0.668575  [   32/  146]
train() client id: f_00005-6-1 loss: 0.743416  [   64/  146]
train() client id: f_00005-6-2 loss: 0.833483  [   96/  146]
train() client id: f_00005-6-3 loss: 0.916439  [  128/  146]
train() client id: f_00006-0-0 loss: 0.491594  [   32/   54]
train() client id: f_00006-1-0 loss: 0.487906  [   32/   54]
train() client id: f_00006-2-0 loss: 0.513853  [   32/   54]
train() client id: f_00006-3-0 loss: 0.534856  [   32/   54]
train() client id: f_00006-4-0 loss: 0.526816  [   32/   54]
train() client id: f_00006-5-0 loss: 0.480707  [   32/   54]
train() client id: f_00006-6-0 loss: 0.588062  [   32/   54]
train() client id: f_00007-0-0 loss: 0.775168  [   32/  179]
train() client id: f_00007-0-1 loss: 0.545853  [   64/  179]
train() client id: f_00007-0-2 loss: 0.710757  [   96/  179]
train() client id: f_00007-0-3 loss: 0.652788  [  128/  179]
train() client id: f_00007-0-4 loss: 0.683291  [  160/  179]
train() client id: f_00007-1-0 loss: 0.575326  [   32/  179]
train() client id: f_00007-1-1 loss: 0.742893  [   64/  179]
train() client id: f_00007-1-2 loss: 0.769370  [   96/  179]
train() client id: f_00007-1-3 loss: 0.512653  [  128/  179]
train() client id: f_00007-1-4 loss: 0.812199  [  160/  179]
train() client id: f_00007-2-0 loss: 0.503371  [   32/  179]
train() client id: f_00007-2-1 loss: 0.667140  [   64/  179]
train() client id: f_00007-2-2 loss: 0.573263  [   96/  179]
train() client id: f_00007-2-3 loss: 0.848310  [  128/  179]
train() client id: f_00007-2-4 loss: 0.778346  [  160/  179]
train() client id: f_00007-3-0 loss: 0.855888  [   32/  179]
train() client id: f_00007-3-1 loss: 0.506460  [   64/  179]
train() client id: f_00007-3-2 loss: 0.522966  [   96/  179]
train() client id: f_00007-3-3 loss: 0.577467  [  128/  179]
train() client id: f_00007-3-4 loss: 0.697941  [  160/  179]
train() client id: f_00007-4-0 loss: 0.723954  [   32/  179]
train() client id: f_00007-4-1 loss: 0.734694  [   64/  179]
train() client id: f_00007-4-2 loss: 0.568776  [   96/  179]
train() client id: f_00007-4-3 loss: 0.633126  [  128/  179]
train() client id: f_00007-4-4 loss: 0.620142  [  160/  179]
train() client id: f_00007-5-0 loss: 0.702317  [   32/  179]
train() client id: f_00007-5-1 loss: 0.551948  [   64/  179]
train() client id: f_00007-5-2 loss: 0.634962  [   96/  179]
train() client id: f_00007-5-3 loss: 0.518274  [  128/  179]
train() client id: f_00007-5-4 loss: 0.837080  [  160/  179]
train() client id: f_00007-6-0 loss: 0.560464  [   32/  179]
train() client id: f_00007-6-1 loss: 0.506259  [   64/  179]
train() client id: f_00007-6-2 loss: 0.718387  [   96/  179]
train() client id: f_00007-6-3 loss: 0.945361  [  128/  179]
train() client id: f_00007-6-4 loss: 0.564706  [  160/  179]
train() client id: f_00008-0-0 loss: 0.766104  [   32/  130]
train() client id: f_00008-0-1 loss: 0.671457  [   64/  130]
train() client id: f_00008-0-2 loss: 0.685474  [   96/  130]
train() client id: f_00008-0-3 loss: 0.665323  [  128/  130]
train() client id: f_00008-1-0 loss: 0.745159  [   32/  130]
train() client id: f_00008-1-1 loss: 0.730914  [   64/  130]
train() client id: f_00008-1-2 loss: 0.656451  [   96/  130]
train() client id: f_00008-1-3 loss: 0.723914  [  128/  130]
train() client id: f_00008-2-0 loss: 0.766717  [   32/  130]
train() client id: f_00008-2-1 loss: 0.656971  [   64/  130]
train() client id: f_00008-2-2 loss: 0.681066  [   96/  130]
train() client id: f_00008-2-3 loss: 0.727067  [  128/  130]
train() client id: f_00008-3-0 loss: 0.683589  [   32/  130]
train() client id: f_00008-3-1 loss: 0.664579  [   64/  130]
train() client id: f_00008-3-2 loss: 0.670748  [   96/  130]
train() client id: f_00008-3-3 loss: 0.830334  [  128/  130]
train() client id: f_00008-4-0 loss: 0.736118  [   32/  130]
train() client id: f_00008-4-1 loss: 0.607026  [   64/  130]
train() client id: f_00008-4-2 loss: 0.708079  [   96/  130]
train() client id: f_00008-4-3 loss: 0.807295  [  128/  130]
train() client id: f_00008-5-0 loss: 0.620221  [   32/  130]
train() client id: f_00008-5-1 loss: 0.711692  [   64/  130]
train() client id: f_00008-5-2 loss: 0.740102  [   96/  130]
train() client id: f_00008-5-3 loss: 0.756957  [  128/  130]
train() client id: f_00008-6-0 loss: 0.626246  [   32/  130]
train() client id: f_00008-6-1 loss: 0.635027  [   64/  130]
train() client id: f_00008-6-2 loss: 0.855971  [   96/  130]
train() client id: f_00008-6-3 loss: 0.748073  [  128/  130]
train() client id: f_00009-0-0 loss: 0.875235  [   32/  118]
train() client id: f_00009-0-1 loss: 0.947853  [   64/  118]
train() client id: f_00009-0-2 loss: 0.909673  [   96/  118]
train() client id: f_00009-1-0 loss: 0.940359  [   32/  118]
train() client id: f_00009-1-1 loss: 0.819803  [   64/  118]
train() client id: f_00009-1-2 loss: 0.830391  [   96/  118]
train() client id: f_00009-2-0 loss: 0.895310  [   32/  118]
train() client id: f_00009-2-1 loss: 0.997818  [   64/  118]
train() client id: f_00009-2-2 loss: 0.779634  [   96/  118]
train() client id: f_00009-3-0 loss: 0.863305  [   32/  118]
train() client id: f_00009-3-1 loss: 0.798911  [   64/  118]
train() client id: f_00009-3-2 loss: 1.016687  [   96/  118]
train() client id: f_00009-4-0 loss: 0.853581  [   32/  118]
train() client id: f_00009-4-1 loss: 0.726069  [   64/  118]
train() client id: f_00009-4-2 loss: 0.889952  [   96/  118]
train() client id: f_00009-5-0 loss: 0.779126  [   32/  118]
train() client id: f_00009-5-1 loss: 0.956412  [   64/  118]
train() client id: f_00009-5-2 loss: 0.812920  [   96/  118]
train() client id: f_00009-6-0 loss: 0.754852  [   32/  118]
train() client id: f_00009-6-1 loss: 0.967693  [   64/  118]
train() client id: f_00009-6-2 loss: 0.908018  [   96/  118]
At round 70 accuracy: 0.6472148541114059
At round 70 training accuracy: 0.5902079141515761
At round 70 training loss: 0.8296368453783254
update_location
xs = [  -3.9056584     4.20031788  370.00902392   18.81129433    0.97929623
    3.95640986 -332.44319194 -311.32485185  354.66397685 -297.06087855]
ys = [ 362.5879595   345.55583871    1.32061395 -332.45517586  324.35018685
  307.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [376.14529445 359.75891974 383.2863444  347.67845598 339.41715149
 323.67453495 347.16763445 326.99210944 368.91083807 313.46638962]
dists_bs = [253.89469525 247.39047687 572.01343104 543.20675007 230.81202527
 222.85644543 237.35836564 221.18390469 552.5653321  210.09367269]
uav_gains = [9.01731222e-13 1.07572493e-12 8.40891865e-13 1.24623615e-12
 1.39163386e-12 1.76012882e-12 1.25446710e-12 1.67039793e-12
 9.71887012e-13 2.08779148e-12]
bs_gains = [2.04297913e-11 2.19695797e-11 2.10163190e-12 2.42879938e-12
 2.66790571e-11 2.94322543e-11 2.46695578e-11 3.00596713e-11
 2.31536738e-12 3.47166460e-11]
Round 71
-------------------------------
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.59747704 3.1514096  1.57881481 0.60189948 3.63223305 1.7491537
 0.73097414 2.18851291 1.6003418  1.41859333]
obj_prev = 18.24940985510058
eta_min = 2.2153155362897003e-59	eta_max = 0.954951968660827
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 4.14802660303202	eta = 0.909090909090909
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 11.665880506041221	eta = 0.323244634087488
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 7.328813927254386	eta = 0.5145352730897301
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 6.605790167817666	eta = 0.5708527185521312
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 6.559990536518958	eta = 0.5748382188192438
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 6.55977636966569	eta = 0.5748569864243457
eta = 0.5748569864243457
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [0.04568455 0.09608258 0.0449594  0.01559076 0.11094819 0.05293607
 0.01957909 0.06490107 0.04713485 0.04278394]
ene_total = [0.68305847 0.95139002 0.68780539 0.3652074  1.08286069 0.55421391
 0.40065058 0.78194702 0.59332082 0.45932206]
ti_comp = [2.6166745  2.82525406 2.60419719 2.66565241 2.82917673 2.83103203
 2.66651706 2.69997525 2.72996789 2.83397532]
ti_coms = [0.29511977 0.0865402  0.30759708 0.24614186 0.08261753 0.08076224
 0.24527721 0.21181901 0.18182637 0.07781895]
t_total = [26.44970207 26.44970207 26.44970207 26.44970207 26.44970207 26.44970207
 26.44970207 26.44970207 26.44970207 26.44970207]
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [8.70339434e-07 6.94542833e-06 8.37517236e-07 3.33330274e-08
 1.06640102e-05 1.15676577e-06 6.59734165e-08 2.34377520e-06
 8.78195905e-07 6.09439114e-07]
ene_total = [0.26808419 0.07867313 0.27941784 0.2235868  0.07514369 0.07337205
 0.22280168 0.19243014 0.16517258 0.0706935 ]
optimize_network iter = 0 obj = 1.6493755938248962
eta = 0.5748569864243457
freqs = [ 8729505.68014144 17004237.96072038  8632103.28335727  2924379.60099508
 19607858.17168588  9349252.32733673  3671285.85880158 12018826.22316111
  8632858.47902665  7548397.32371458]
eta_min = 0.5748569864243458	eta_max = 0.8123930873707683
af = 0.00017023437323749214	bf = 0.7011262629418353	zeta = 0.00018725781056124136	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [1.49832217e-07 1.19568169e-06 1.44181752e-07 5.73840643e-09
 1.83584959e-06 1.99141592e-07 1.13575726e-08 4.03489744e-07
 1.51184738e-07 1.04917243e-07]
ene_total = [1.25368474 0.36767638 1.30668851 1.04561886 0.35103995 0.34308906
 1.04194602 0.8998312  0.77241077 0.33058187]
ti_comp = [0.98979546 1.19837503 0.97731815 1.03877337 1.2022977  1.20415299
 1.03963802 1.07309621 1.10308885 1.20709628]
ti_coms = [0.29511977 0.0865402  0.30759708 0.24614186 0.08261753 0.08076224
 0.24527721 0.21181901 0.18182637 0.07781895]
t_total = [26.44970207 26.44970207 26.44970207 26.44970207 26.44970207 26.44970207
 26.44970207 26.44970207 26.44970207 26.44970207]
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [3.21501828e-07 2.04039780e-06 3.14308302e-07 1.16018294e-08
 3.12106616e-06 3.37954282e-07 2.29392754e-08 7.84231878e-07
 2.84296680e-07 1.77551736e-07]
ene_total = [0.60750428 0.17818314 0.63318841 0.50667793 0.17013065 0.16625427
 0.5048983  0.436041   0.37429151 0.16019228]
optimize_network iter = 1 obj = 3.7373617634764136
eta = 0.8123930873707683
freqs = [ 8660759.82156696 15044717.2754607   8632103.28335727  2816297.66515272
 17315710.58589685  8249014.9990354   3533804.43880649 11348678.03144383
  8017957.73562188  6650755.80408876]
eta_min = 0.8123930873707704	eta_max = 0.8123930873707671
af = 0.0001378387179910577	bf = 0.7011262629418353	zeta = 0.0001516225897901635	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [1.47481618e-07 9.35985870e-07 1.44181752e-07 5.32207417e-09
 1.43171779e-06 1.55028805e-07 1.05228684e-08 3.59748455e-07
 1.30414606e-07 8.14478020e-08]
ene_total = [1.25368464 0.36766535 1.30668851 1.04561884 0.35102278 0.34308719
 1.04194599 0.89982934 0.77240989 0.33058088]
ti_comp = [0.98979546 1.19837503 0.97731815 1.03877337 1.2022977  1.20415299
 1.03963802 1.07309621 1.10308885 1.20709628]
ti_coms = [0.29511977 0.0865402  0.30759708 0.24614186 0.08261753 0.08076224
 0.24527721 0.21181901 0.18182637 0.07781895]
t_total = [26.44970207 26.44970207 26.44970207 26.44970207 26.44970207 26.44970207
 26.44970207 26.44970207 26.44970207 26.44970207]
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [3.21501828e-07 2.04039780e-06 3.14308302e-07 1.16018294e-08
 3.12106616e-06 3.37954282e-07 2.29392754e-08 7.84231878e-07
 2.84296680e-07 1.77551736e-07]
ene_total = [0.60750428 0.17818314 0.63318841 0.50667793 0.17013065 0.16625427
 0.5048983  0.436041   0.37429151 0.16019228]
optimize_network iter = 2 obj = 3.7373617634763896
eta = 0.8123930873707671
freqs = [ 8660759.82156695 15044717.2754607   8632103.28335726  2816297.66515272
 17315710.58589685  8249014.9990354   3533804.43880649 11348678.03144382
  8017957.73562187  6650755.80408876]
Done!
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [2.83533115e-07 1.79943097e-06 2.77189130e-07 1.02316770e-08
 2.75247459e-06 2.98042568e-07 2.02301936e-08 6.91615688e-07
 2.50721820e-07 1.56583237e-07]
ene_total = [0.02951226 0.00865582 0.03075999 0.0246142  0.00826451 0.00807652
 0.02452774 0.02118259 0.01818289 0.00778205]
At round 71 energy consumption: 0.18155856217199728
At round 71 eta: 0.8123930873707671
At round 71 a_n: 3.5193018744635296
At round 71 local rounds: 6.803476774877765
At round 71 global rounds: 20.584783723108377
gradient difference: 0.8088682889938354
train() client id: f_00000-0-0 loss: 1.026854  [   32/  126]
train() client id: f_00000-0-1 loss: 0.805825  [   64/  126]
train() client id: f_00000-0-2 loss: 0.793076  [   96/  126]
train() client id: f_00000-1-0 loss: 0.823792  [   32/  126]
train() client id: f_00000-1-1 loss: 1.001788  [   64/  126]
train() client id: f_00000-1-2 loss: 0.760913  [   96/  126]
train() client id: f_00000-2-0 loss: 0.769426  [   32/  126]
train() client id: f_00000-2-1 loss: 0.929376  [   64/  126]
train() client id: f_00000-2-2 loss: 1.009467  [   96/  126]
train() client id: f_00000-3-0 loss: 0.899160  [   32/  126]
train() client id: f_00000-3-1 loss: 0.832492  [   64/  126]
train() client id: f_00000-3-2 loss: 0.794776  [   96/  126]
train() client id: f_00000-4-0 loss: 0.988221  [   32/  126]
train() client id: f_00000-4-1 loss: 0.814833  [   64/  126]
train() client id: f_00000-4-2 loss: 0.884830  [   96/  126]
train() client id: f_00000-5-0 loss: 0.836490  [   32/  126]
train() client id: f_00000-5-1 loss: 0.907960  [   64/  126]
train() client id: f_00000-5-2 loss: 0.907821  [   96/  126]
train() client id: f_00001-0-0 loss: 0.581093  [   32/  265]
train() client id: f_00001-0-1 loss: 0.525524  [   64/  265]
train() client id: f_00001-0-2 loss: 0.619128  [   96/  265]
train() client id: f_00001-0-3 loss: 0.538976  [  128/  265]
train() client id: f_00001-0-4 loss: 0.523230  [  160/  265]
train() client id: f_00001-0-5 loss: 0.538720  [  192/  265]
train() client id: f_00001-0-6 loss: 0.655470  [  224/  265]
train() client id: f_00001-0-7 loss: 0.513837  [  256/  265]
train() client id: f_00001-1-0 loss: 0.633582  [   32/  265]
train() client id: f_00001-1-1 loss: 0.535226  [   64/  265]
train() client id: f_00001-1-2 loss: 0.585812  [   96/  265]
train() client id: f_00001-1-3 loss: 0.672430  [  128/  265]
train() client id: f_00001-1-4 loss: 0.499021  [  160/  265]
train() client id: f_00001-1-5 loss: 0.484782  [  192/  265]
train() client id: f_00001-1-6 loss: 0.572352  [  224/  265]
train() client id: f_00001-1-7 loss: 0.540701  [  256/  265]
train() client id: f_00001-2-0 loss: 0.568874  [   32/  265]
train() client id: f_00001-2-1 loss: 0.564807  [   64/  265]
train() client id: f_00001-2-2 loss: 0.541819  [   96/  265]
train() client id: f_00001-2-3 loss: 0.519327  [  128/  265]
train() client id: f_00001-2-4 loss: 0.496413  [  160/  265]
train() client id: f_00001-2-5 loss: 0.500155  [  192/  265]
train() client id: f_00001-2-6 loss: 0.622663  [  224/  265]
train() client id: f_00001-2-7 loss: 0.664114  [  256/  265]
train() client id: f_00001-3-0 loss: 0.559847  [   32/  265]
train() client id: f_00001-3-1 loss: 0.510551  [   64/  265]
train() client id: f_00001-3-2 loss: 0.563600  [   96/  265]
train() client id: f_00001-3-3 loss: 0.578509  [  128/  265]
train() client id: f_00001-3-4 loss: 0.552686  [  160/  265]
train() client id: f_00001-3-5 loss: 0.587592  [  192/  265]
train() client id: f_00001-3-6 loss: 0.542693  [  224/  265]
train() client id: f_00001-3-7 loss: 0.571161  [  256/  265]
train() client id: f_00001-4-0 loss: 0.692755  [   32/  265]
train() client id: f_00001-4-1 loss: 0.574932  [   64/  265]
train() client id: f_00001-4-2 loss: 0.525631  [   96/  265]
train() client id: f_00001-4-3 loss: 0.509456  [  128/  265]
train() client id: f_00001-4-4 loss: 0.521838  [  160/  265]
train() client id: f_00001-4-5 loss: 0.545631  [  192/  265]
train() client id: f_00001-4-6 loss: 0.537903  [  224/  265]
train() client id: f_00001-4-7 loss: 0.464130  [  256/  265]
train() client id: f_00001-5-0 loss: 0.491922  [   32/  265]
train() client id: f_00001-5-1 loss: 0.648825  [   64/  265]
train() client id: f_00001-5-2 loss: 0.504904  [   96/  265]
train() client id: f_00001-5-3 loss: 0.628433  [  128/  265]
train() client id: f_00001-5-4 loss: 0.524126  [  160/  265]
train() client id: f_00001-5-5 loss: 0.682535  [  192/  265]
train() client id: f_00001-5-6 loss: 0.480026  [  224/  265]
train() client id: f_00001-5-7 loss: 0.575171  [  256/  265]
train() client id: f_00002-0-0 loss: 1.121239  [   32/  124]
train() client id: f_00002-0-1 loss: 1.048696  [   64/  124]
train() client id: f_00002-0-2 loss: 1.145712  [   96/  124]
train() client id: f_00002-1-0 loss: 1.022833  [   32/  124]
train() client id: f_00002-1-1 loss: 1.225882  [   64/  124]
train() client id: f_00002-1-2 loss: 0.977294  [   96/  124]
train() client id: f_00002-2-0 loss: 1.111645  [   32/  124]
train() client id: f_00002-2-1 loss: 1.133486  [   64/  124]
train() client id: f_00002-2-2 loss: 1.202076  [   96/  124]
train() client id: f_00002-3-0 loss: 1.066458  [   32/  124]
train() client id: f_00002-3-1 loss: 1.043661  [   64/  124]
train() client id: f_00002-3-2 loss: 1.050030  [   96/  124]
train() client id: f_00002-4-0 loss: 1.056513  [   32/  124]
train() client id: f_00002-4-1 loss: 1.028424  [   64/  124]
train() client id: f_00002-4-2 loss: 1.222290  [   96/  124]
train() client id: f_00002-5-0 loss: 0.927305  [   32/  124]
train() client id: f_00002-5-1 loss: 0.996866  [   64/  124]
train() client id: f_00002-5-2 loss: 1.208595  [   96/  124]
train() client id: f_00003-0-0 loss: 0.797630  [   32/   43]
train() client id: f_00003-1-0 loss: 0.660939  [   32/   43]
train() client id: f_00003-2-0 loss: 0.821756  [   32/   43]
train() client id: f_00003-3-0 loss: 0.884756  [   32/   43]
train() client id: f_00003-4-0 loss: 0.779664  [   32/   43]
train() client id: f_00003-5-0 loss: 0.635642  [   32/   43]
train() client id: f_00004-0-0 loss: 0.863573  [   32/  306]
train() client id: f_00004-0-1 loss: 0.767608  [   64/  306]
train() client id: f_00004-0-2 loss: 0.878342  [   96/  306]
train() client id: f_00004-0-3 loss: 0.876352  [  128/  306]
train() client id: f_00004-0-4 loss: 0.750008  [  160/  306]
train() client id: f_00004-0-5 loss: 0.845816  [  192/  306]
train() client id: f_00004-0-6 loss: 1.004241  [  224/  306]
train() client id: f_00004-0-7 loss: 0.853480  [  256/  306]
train() client id: f_00004-0-8 loss: 0.796919  [  288/  306]
train() client id: f_00004-1-0 loss: 0.842673  [   32/  306]
train() client id: f_00004-1-1 loss: 0.721839  [   64/  306]
train() client id: f_00004-1-2 loss: 0.872064  [   96/  306]
train() client id: f_00004-1-3 loss: 0.816659  [  128/  306]
train() client id: f_00004-1-4 loss: 0.785212  [  160/  306]
train() client id: f_00004-1-5 loss: 0.866927  [  192/  306]
train() client id: f_00004-1-6 loss: 0.873717  [  224/  306]
train() client id: f_00004-1-7 loss: 0.867728  [  256/  306]
train() client id: f_00004-1-8 loss: 0.807236  [  288/  306]
train() client id: f_00004-2-0 loss: 0.913435  [   32/  306]
train() client id: f_00004-2-1 loss: 0.861250  [   64/  306]
train() client id: f_00004-2-2 loss: 0.844002  [   96/  306]
train() client id: f_00004-2-3 loss: 0.847415  [  128/  306]
train() client id: f_00004-2-4 loss: 0.758835  [  160/  306]
train() client id: f_00004-2-5 loss: 0.815631  [  192/  306]
train() client id: f_00004-2-6 loss: 0.812715  [  224/  306]
train() client id: f_00004-2-7 loss: 0.832809  [  256/  306]
train() client id: f_00004-2-8 loss: 0.879769  [  288/  306]
train() client id: f_00004-3-0 loss: 0.975014  [   32/  306]
train() client id: f_00004-3-1 loss: 0.882328  [   64/  306]
train() client id: f_00004-3-2 loss: 0.824795  [   96/  306]
train() client id: f_00004-3-3 loss: 0.731681  [  128/  306]
train() client id: f_00004-3-4 loss: 0.892131  [  160/  306]
train() client id: f_00004-3-5 loss: 0.855609  [  192/  306]
train() client id: f_00004-3-6 loss: 0.771892  [  224/  306]
train() client id: f_00004-3-7 loss: 0.945131  [  256/  306]
train() client id: f_00004-3-8 loss: 0.733577  [  288/  306]
train() client id: f_00004-4-0 loss: 0.842814  [   32/  306]
train() client id: f_00004-4-1 loss: 0.900223  [   64/  306]
train() client id: f_00004-4-2 loss: 0.822034  [   96/  306]
train() client id: f_00004-4-3 loss: 0.721018  [  128/  306]
train() client id: f_00004-4-4 loss: 0.779995  [  160/  306]
train() client id: f_00004-4-5 loss: 0.722793  [  192/  306]
train() client id: f_00004-4-6 loss: 0.945651  [  224/  306]
train() client id: f_00004-4-7 loss: 0.787852  [  256/  306]
train() client id: f_00004-4-8 loss: 0.940153  [  288/  306]
train() client id: f_00004-5-0 loss: 0.738695  [   32/  306]
train() client id: f_00004-5-1 loss: 0.753358  [   64/  306]
train() client id: f_00004-5-2 loss: 0.737864  [   96/  306]
train() client id: f_00004-5-3 loss: 0.873760  [  128/  306]
train() client id: f_00004-5-4 loss: 0.855990  [  160/  306]
train() client id: f_00004-5-5 loss: 0.872787  [  192/  306]
train() client id: f_00004-5-6 loss: 0.890123  [  224/  306]
train() client id: f_00004-5-7 loss: 0.818078  [  256/  306]
train() client id: f_00004-5-8 loss: 0.911552  [  288/  306]
train() client id: f_00005-0-0 loss: 0.541436  [   32/  146]
train() client id: f_00005-0-1 loss: 0.587876  [   64/  146]
train() client id: f_00005-0-2 loss: 0.410568  [   96/  146]
train() client id: f_00005-0-3 loss: 0.268761  [  128/  146]
train() client id: f_00005-1-0 loss: 0.606226  [   32/  146]
train() client id: f_00005-1-1 loss: 0.345370  [   64/  146]
train() client id: f_00005-1-2 loss: 0.410414  [   96/  146]
train() client id: f_00005-1-3 loss: 0.499820  [  128/  146]
train() client id: f_00005-2-0 loss: 0.464229  [   32/  146]
train() client id: f_00005-2-1 loss: 0.572280  [   64/  146]
train() client id: f_00005-2-2 loss: 0.447650  [   96/  146]
train() client id: f_00005-2-3 loss: 0.368992  [  128/  146]
train() client id: f_00005-3-0 loss: 0.496894  [   32/  146]
train() client id: f_00005-3-1 loss: 0.458685  [   64/  146]
train() client id: f_00005-3-2 loss: 0.557038  [   96/  146]
train() client id: f_00005-3-3 loss: 0.417570  [  128/  146]
train() client id: f_00005-4-0 loss: 0.328121  [   32/  146]
train() client id: f_00005-4-1 loss: 0.567021  [   64/  146]
train() client id: f_00005-4-2 loss: 0.461563  [   96/  146]
train() client id: f_00005-4-3 loss: 0.490925  [  128/  146]
train() client id: f_00005-5-0 loss: 0.722775  [   32/  146]
train() client id: f_00005-5-1 loss: 0.334850  [   64/  146]
train() client id: f_00005-5-2 loss: 0.384701  [   96/  146]
train() client id: f_00005-5-3 loss: 0.371501  [  128/  146]
train() client id: f_00006-0-0 loss: 0.476170  [   32/   54]
train() client id: f_00006-1-0 loss: 0.473610  [   32/   54]
train() client id: f_00006-2-0 loss: 0.384045  [   32/   54]
train() client id: f_00006-3-0 loss: 0.425609  [   32/   54]
train() client id: f_00006-4-0 loss: 0.442262  [   32/   54]
train() client id: f_00006-5-0 loss: 0.456002  [   32/   54]
train() client id: f_00007-0-0 loss: 0.793727  [   32/  179]
train() client id: f_00007-0-1 loss: 0.856854  [   64/  179]
train() client id: f_00007-0-2 loss: 0.564335  [   96/  179]
train() client id: f_00007-0-3 loss: 0.458575  [  128/  179]
train() client id: f_00007-0-4 loss: 0.712878  [  160/  179]
train() client id: f_00007-1-0 loss: 0.539784  [   32/  179]
train() client id: f_00007-1-1 loss: 0.744505  [   64/  179]
train() client id: f_00007-1-2 loss: 0.698001  [   96/  179]
train() client id: f_00007-1-3 loss: 0.742668  [  128/  179]
train() client id: f_00007-1-4 loss: 0.667037  [  160/  179]
train() client id: f_00007-2-0 loss: 0.572616  [   32/  179]
train() client id: f_00007-2-1 loss: 0.584365  [   64/  179]
train() client id: f_00007-2-2 loss: 0.680255  [   96/  179]
train() client id: f_00007-2-3 loss: 0.710664  [  128/  179]
train() client id: f_00007-2-4 loss: 0.662991  [  160/  179]
train() client id: f_00007-3-0 loss: 0.578460  [   32/  179]
train() client id: f_00007-3-1 loss: 0.935954  [   64/  179]
train() client id: f_00007-3-2 loss: 0.471932  [   96/  179]
train() client id: f_00007-3-3 loss: 0.682371  [  128/  179]
train() client id: f_00007-3-4 loss: 0.710568  [  160/  179]
train() client id: f_00007-4-0 loss: 0.633240  [   32/  179]
train() client id: f_00007-4-1 loss: 0.677227  [   64/  179]
train() client id: f_00007-4-2 loss: 0.606248  [   96/  179]
train() client id: f_00007-4-3 loss: 0.893533  [  128/  179]
train() client id: f_00007-4-4 loss: 0.573473  [  160/  179]
train() client id: f_00007-5-0 loss: 0.811669  [   32/  179]
train() client id: f_00007-5-1 loss: 0.503107  [   64/  179]
train() client id: f_00007-5-2 loss: 0.599907  [   96/  179]
train() client id: f_00007-5-3 loss: 0.750110  [  128/  179]
train() client id: f_00007-5-4 loss: 0.595418  [  160/  179]
train() client id: f_00008-0-0 loss: 0.751457  [   32/  130]
train() client id: f_00008-0-1 loss: 0.739086  [   64/  130]
train() client id: f_00008-0-2 loss: 0.640306  [   96/  130]
train() client id: f_00008-0-3 loss: 0.798664  [  128/  130]
train() client id: f_00008-1-0 loss: 0.847604  [   32/  130]
train() client id: f_00008-1-1 loss: 0.727476  [   64/  130]
train() client id: f_00008-1-2 loss: 0.739461  [   96/  130]
train() client id: f_00008-1-3 loss: 0.621061  [  128/  130]
train() client id: f_00008-2-0 loss: 0.719575  [   32/  130]
train() client id: f_00008-2-1 loss: 0.749780  [   64/  130]
train() client id: f_00008-2-2 loss: 0.629657  [   96/  130]
train() client id: f_00008-2-3 loss: 0.836590  [  128/  130]
train() client id: f_00008-3-0 loss: 0.705064  [   32/  130]
train() client id: f_00008-3-1 loss: 0.792316  [   64/  130]
train() client id: f_00008-3-2 loss: 0.683068  [   96/  130]
train() client id: f_00008-3-3 loss: 0.756633  [  128/  130]
train() client id: f_00008-4-0 loss: 0.778085  [   32/  130]
train() client id: f_00008-4-1 loss: 0.735241  [   64/  130]
train() client id: f_00008-4-2 loss: 0.686822  [   96/  130]
train() client id: f_00008-4-3 loss: 0.710254  [  128/  130]
train() client id: f_00008-5-0 loss: 0.711337  [   32/  130]
train() client id: f_00008-5-1 loss: 0.725127  [   64/  130]
train() client id: f_00008-5-2 loss: 0.722519  [   96/  130]
train() client id: f_00008-5-3 loss: 0.776084  [  128/  130]
train() client id: f_00009-0-0 loss: 0.799756  [   32/  118]
train() client id: f_00009-0-1 loss: 0.721050  [   64/  118]
train() client id: f_00009-0-2 loss: 0.967694  [   96/  118]
train() client id: f_00009-1-0 loss: 0.705401  [   32/  118]
train() client id: f_00009-1-1 loss: 0.747576  [   64/  118]
train() client id: f_00009-1-2 loss: 0.812889  [   96/  118]
train() client id: f_00009-2-0 loss: 0.770275  [   32/  118]
train() client id: f_00009-2-1 loss: 0.644874  [   64/  118]
train() client id: f_00009-2-2 loss: 0.757271  [   96/  118]
train() client id: f_00009-3-0 loss: 0.746389  [   32/  118]
train() client id: f_00009-3-1 loss: 0.768849  [   64/  118]
train() client id: f_00009-3-2 loss: 0.777993  [   96/  118]
train() client id: f_00009-4-0 loss: 0.636611  [   32/  118]
train() client id: f_00009-4-1 loss: 0.639899  [   64/  118]
train() client id: f_00009-4-2 loss: 0.810354  [   96/  118]
train() client id: f_00009-5-0 loss: 0.659869  [   32/  118]
train() client id: f_00009-5-1 loss: 0.729277  [   64/  118]
train() client id: f_00009-5-2 loss: 0.743070  [   96/  118]
At round 71 accuracy: 0.6472148541114059
At round 71 training accuracy: 0.590878604963112
At round 71 training loss: 0.8249064843078108
update_location
xs = [  -3.9056584     4.20031788  375.00902392   18.81129433    0.97929623
    3.95640986 -337.44319194 -316.32485185  359.66397685 -302.06087855]
ys = [ 367.5879595   350.55583871    1.32061395 -337.45517586  329.35018685
  312.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [380.96740298 364.56417641 388.11533343 352.46256611 344.19835066
 328.43316837 351.95851791 331.75606724 373.72027804 318.20871485]
dists_bs = [257.61093895 250.86372084 576.77938202 547.88057556 234.07839046
 225.87194774 240.70402087 224.30017435 557.35992501 213.03746171]
uav_gains = [8.59823011e-13 1.01888229e-12 8.03777175e-13 1.17344140e-12
 1.30420376e-12 1.63364151e-12 1.18076150e-12 1.55363346e-12
 9.24212893e-13 1.92508765e-12]
bs_gains = [1.96152589e-11 2.11284714e-11 2.05336828e-12 2.37122932e-12
 2.56497054e-11 2.83452084e-11 2.37214220e-11 2.89048802e-11
 2.26002902e-12 3.33900711e-11]
Round 72
-------------------------------
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.45877326 2.87217542 1.44178771 0.55140047 3.31032656 1.59425308
 0.66902829 1.99714791 1.45909395 1.29300424]
obj_prev = 16.646990883599326
eta_min = 4.971520837874105e-65	eta_max = 0.9577477121135591
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 3.7800966926678474	eta = 0.9090909090909091
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 10.808661784694127	eta = 0.3179349680138224
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 6.733867507222867	eta = 0.5103236045412168
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 6.058721545799979	eta = 0.5671908690326205
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 6.015928897442357	eta = 0.5712254245973457
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 6.015728044345185	eta = 0.5712444966689002
eta = 0.5712444966689002
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [0.04620474 0.09717663 0.04547133 0.01576828 0.11221151 0.05353882
 0.01980203 0.06564007 0.04767156 0.04327111]
ene_total = [0.62840586 0.86936    0.63267678 0.33813836 0.98949538 0.50632297
 0.37054313 0.71901402 0.54216497 0.41960659]
ti_comp = [2.91272976 3.128895   2.90019242 2.96199725 3.13288317 3.13480384
 2.96285607 2.99669538 3.03246495 3.13777295]
ti_coms = [0.3035376  0.08737236 0.31607494 0.25427011 0.08338419 0.08146352
 0.25341129 0.21957198 0.18380241 0.07849441]
t_total = [26.39969788 26.39969788 26.39969788 26.39969788 26.39969788 26.39969788
 26.39969788 26.39969788 26.39969788 26.39969788]
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [7.26673296e-07 5.85846172e-06 6.98617374e-07 2.79295552e-08
 8.99712661e-06 9.76035873e-07 5.52825571e-08 1.96834785e-06
 7.36319715e-07 5.14317079e-07]
ene_total = [0.24915505 0.07176489 0.2594457  0.20870969 0.06851708 0.06687473
 0.20800497 0.18024477 0.15087435 0.06443384]
optimize_network iter = 0 obj = 1.52802506642978
eta = 0.5712444966689002
freqs = [ 7931518.36924844 15528906.23802998  7839364.44612375  2661765.47781446
 17908664.85509679  8539421.8514933   3341713.30591816 10952075.17012927
  7860199.06137994  6895193.77630764]
eta_min = 0.5712444966689012	eta_max = 0.8252306516812821
af = 0.0001292173845627228	bf = 0.6550768027395515	zeta = 0.00014213912301899509	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [1.23691134e-07 9.97201600e-07 1.18915578e-07 4.75404610e-09
 1.53145134e-06 1.66136535e-07 9.40995383e-09 3.35043518e-07
 1.25333105e-07 8.75447921e-08]
ene_total = [1.17506927 0.33827729 1.22360405 0.98433881 0.32285882 0.31537062
 0.98101428 0.85002707 0.71154662 0.30387348]
ti_comp = [1.00747753 1.22364276 0.99494019 1.05674501 1.22763094 1.22955161
 1.05760384 1.09144314 1.12721272 1.23252071]
ti_coms = [0.3035376  0.08737236 0.31607494 0.25427011 0.08338419 0.08146352
 0.25341129 0.21957198 0.18380241 0.07849441]
t_total = [26.39969788 26.39969788 26.39969788 26.39969788 26.39969788 26.39969788
 26.39969788 26.39969788 26.39969788 26.39969788]
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [2.45233236e-07 1.54655995e-06 2.39667719e-07 8.85938112e-09
 2.36574025e-06 2.56155376e-07 1.75175625e-08 5.99094442e-07
 2.15157618e-07 1.34584796e-07]
ene_total = [0.61123365 0.17597144 0.63647978 0.51201976 0.16795702 0.16404692
 0.51029054 0.44216059 0.37012424 0.15806563]
optimize_network iter = 1 obj = 3.7483495686155184
eta = 0.8252306516812821
freqs = [ 7866677.00292003 13622203.68026247  7839364.44612375  2559495.62396928
 15678688.91426082  7468996.16234366  3211640.19547591 10315922.15655571
  7254268.29694454  6022043.97636097]
eta_min = 0.8252306516812807	eta_max = 0.8252306516812821
af = 0.00010312038831994186	bf = 0.6550768027395515	zeta = 0.00011343242715193606	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [1.21677013e-07 7.67354369e-07 1.18915578e-07 4.39574606e-09
 1.17380585e-06 1.27096235e-07 8.69166314e-09 2.97251805e-07
 1.06754438e-07 6.67767395e-08]
ene_total = [1.17506919 0.33826839 1.22360405 0.98433879 0.32284498 0.31536911
 0.98101425 0.85002561 0.7115459  0.30387267]
ti_comp = [1.00747753 1.22364276 0.99494019 1.05674501 1.22763094 1.22955161
 1.05760384 1.09144314 1.12721272 1.23252071]
ti_coms = [0.3035376  0.08737236 0.31607494 0.25427011 0.08338419 0.08146352
 0.25341129 0.21957198 0.18380241 0.07849441]
t_total = [26.39969788 26.39969788 26.39969788 26.39969788 26.39969788 26.39969788
 26.39969788 26.39969788 26.39969788 26.39969788]
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [2.45233236e-07 1.54655995e-06 2.39667719e-07 8.85938112e-09
 2.36574025e-06 2.56155376e-07 1.75175625e-08 5.99094442e-07
 2.15157618e-07 1.34584796e-07]
ene_total = [0.61123365 0.17597144 0.63647978 0.51201976 0.16795702 0.16404692
 0.51029054 0.44216059 0.37012424 0.15806563]
optimize_network iter = 2 obj = 3.7483495686155184
eta = 0.8252306516812821
freqs = [ 7866677.00292003 13622203.68026247  7839364.44612375  2559495.62396928
 15678688.91426082  7468996.16234366  3211640.19547591 10315922.15655571
  7254268.29694454  6022043.97636097]
Done!
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [2.33923815e-07 1.47523724e-06 2.28614962e-07 8.45081303e-09
 2.25663941e-06 2.44342258e-07 1.67097050e-08 5.71466002e-07
 2.05235193e-07 1.28378148e-07]
ene_total = [0.03035399 0.00873871 0.03160772 0.02542702 0.00834068 0.0081466
 0.02534115 0.02195777 0.01838045 0.00784957]
At round 72 energy consumption: 0.18614365042570813
At round 72 eta: 0.8252306516812821
At round 72 a_n: 3.176756027494214
At round 72 local rounds: 6.290079613751617
At round 72 global rounds: 20.136836970093626
gradient difference: 0.6998321413993835
train() client id: f_00000-0-0 loss: 0.957394  [   32/  126]
train() client id: f_00000-0-1 loss: 1.026118  [   64/  126]
train() client id: f_00000-0-2 loss: 0.923425  [   96/  126]
train() client id: f_00000-1-0 loss: 0.784307  [   32/  126]
train() client id: f_00000-1-1 loss: 0.920849  [   64/  126]
train() client id: f_00000-1-2 loss: 1.143177  [   96/  126]
train() client id: f_00000-2-0 loss: 0.869064  [   32/  126]
train() client id: f_00000-2-1 loss: 0.892540  [   64/  126]
train() client id: f_00000-2-2 loss: 0.997421  [   96/  126]
train() client id: f_00000-3-0 loss: 0.966689  [   32/  126]
train() client id: f_00000-3-1 loss: 1.024531  [   64/  126]
train() client id: f_00000-3-2 loss: 0.846901  [   96/  126]
train() client id: f_00000-4-0 loss: 0.830293  [   32/  126]
train() client id: f_00000-4-1 loss: 0.946714  [   64/  126]
train() client id: f_00000-4-2 loss: 0.857073  [   96/  126]
train() client id: f_00000-5-0 loss: 0.947872  [   32/  126]
train() client id: f_00000-5-1 loss: 0.862444  [   64/  126]
train() client id: f_00000-5-2 loss: 0.869171  [   96/  126]
train() client id: f_00001-0-0 loss: 0.474346  [   32/  265]
train() client id: f_00001-0-1 loss: 0.569328  [   64/  265]
train() client id: f_00001-0-2 loss: 0.410801  [   96/  265]
train() client id: f_00001-0-3 loss: 0.469425  [  128/  265]
train() client id: f_00001-0-4 loss: 0.338221  [  160/  265]
train() client id: f_00001-0-5 loss: 0.396675  [  192/  265]
train() client id: f_00001-0-6 loss: 0.438265  [  224/  265]
train() client id: f_00001-0-7 loss: 0.515180  [  256/  265]
train() client id: f_00001-1-0 loss: 0.462988  [   32/  265]
train() client id: f_00001-1-1 loss: 0.407988  [   64/  265]
train() client id: f_00001-1-2 loss: 0.470671  [   96/  265]
train() client id: f_00001-1-3 loss: 0.391984  [  128/  265]
train() client id: f_00001-1-4 loss: 0.434212  [  160/  265]
train() client id: f_00001-1-5 loss: 0.484349  [  192/  265]
train() client id: f_00001-1-6 loss: 0.515659  [  224/  265]
train() client id: f_00001-1-7 loss: 0.449845  [  256/  265]
train() client id: f_00001-2-0 loss: 0.379350  [   32/  265]
train() client id: f_00001-2-1 loss: 0.600404  [   64/  265]
train() client id: f_00001-2-2 loss: 0.405086  [   96/  265]
train() client id: f_00001-2-3 loss: 0.445482  [  128/  265]
train() client id: f_00001-2-4 loss: 0.420748  [  160/  265]
train() client id: f_00001-2-5 loss: 0.407752  [  192/  265]
train() client id: f_00001-2-6 loss: 0.377748  [  224/  265]
train() client id: f_00001-2-7 loss: 0.511653  [  256/  265]
train() client id: f_00001-3-0 loss: 0.519763  [   32/  265]
train() client id: f_00001-3-1 loss: 0.434760  [   64/  265]
train() client id: f_00001-3-2 loss: 0.412700  [   96/  265]
train() client id: f_00001-3-3 loss: 0.525285  [  128/  265]
train() client id: f_00001-3-4 loss: 0.447785  [  160/  265]
train() client id: f_00001-3-5 loss: 0.433771  [  192/  265]
train() client id: f_00001-3-6 loss: 0.452372  [  224/  265]
train() client id: f_00001-3-7 loss: 0.369407  [  256/  265]
train() client id: f_00001-4-0 loss: 0.439918  [   32/  265]
train() client id: f_00001-4-1 loss: 0.615924  [   64/  265]
train() client id: f_00001-4-2 loss: 0.378085  [   96/  265]
train() client id: f_00001-4-3 loss: 0.453435  [  128/  265]
train() client id: f_00001-4-4 loss: 0.489016  [  160/  265]
train() client id: f_00001-4-5 loss: 0.384635  [  192/  265]
train() client id: f_00001-4-6 loss: 0.412957  [  224/  265]
train() client id: f_00001-4-7 loss: 0.410612  [  256/  265]
train() client id: f_00001-5-0 loss: 0.451221  [   32/  265]
train() client id: f_00001-5-1 loss: 0.428621  [   64/  265]
train() client id: f_00001-5-2 loss: 0.343498  [   96/  265]
train() client id: f_00001-5-3 loss: 0.490660  [  128/  265]
train() client id: f_00001-5-4 loss: 0.482020  [  160/  265]
train() client id: f_00001-5-5 loss: 0.416747  [  192/  265]
train() client id: f_00001-5-6 loss: 0.469945  [  224/  265]
train() client id: f_00001-5-7 loss: 0.493384  [  256/  265]
train() client id: f_00002-0-0 loss: 0.717742  [   32/  124]
train() client id: f_00002-0-1 loss: 0.785358  [   64/  124]
train() client id: f_00002-0-2 loss: 0.771328  [   96/  124]
train() client id: f_00002-1-0 loss: 0.763957  [   32/  124]
train() client id: f_00002-1-1 loss: 0.694296  [   64/  124]
train() client id: f_00002-1-2 loss: 0.741267  [   96/  124]
train() client id: f_00002-2-0 loss: 0.859170  [   32/  124]
train() client id: f_00002-2-1 loss: 0.825264  [   64/  124]
train() client id: f_00002-2-2 loss: 0.643685  [   96/  124]
train() client id: f_00002-3-0 loss: 0.795703  [   32/  124]
train() client id: f_00002-3-1 loss: 0.568523  [   64/  124]
train() client id: f_00002-3-2 loss: 0.747076  [   96/  124]
train() client id: f_00002-4-0 loss: 0.544721  [   32/  124]
train() client id: f_00002-4-1 loss: 0.808360  [   64/  124]
train() client id: f_00002-4-2 loss: 0.759348  [   96/  124]
train() client id: f_00002-5-0 loss: 0.673573  [   32/  124]
train() client id: f_00002-5-1 loss: 0.784737  [   64/  124]
train() client id: f_00002-5-2 loss: 0.812541  [   96/  124]
train() client id: f_00003-0-0 loss: 0.845401  [   32/   43]
train() client id: f_00003-1-0 loss: 0.457866  [   32/   43]
train() client id: f_00003-2-0 loss: 0.777603  [   32/   43]
train() client id: f_00003-3-0 loss: 0.735101  [   32/   43]
train() client id: f_00003-4-0 loss: 0.729790  [   32/   43]
train() client id: f_00003-5-0 loss: 0.812399  [   32/   43]
train() client id: f_00004-0-0 loss: 0.668917  [   32/  306]
train() client id: f_00004-0-1 loss: 0.797662  [   64/  306]
train() client id: f_00004-0-2 loss: 0.948331  [   96/  306]
train() client id: f_00004-0-3 loss: 0.870475  [  128/  306]
train() client id: f_00004-0-4 loss: 0.698237  [  160/  306]
train() client id: f_00004-0-5 loss: 0.820187  [  192/  306]
train() client id: f_00004-0-6 loss: 0.784903  [  224/  306]
train() client id: f_00004-0-7 loss: 0.698427  [  256/  306]
train() client id: f_00004-0-8 loss: 0.756188  [  288/  306]
train() client id: f_00004-1-0 loss: 0.782098  [   32/  306]
train() client id: f_00004-1-1 loss: 0.713638  [   64/  306]
train() client id: f_00004-1-2 loss: 0.870289  [   96/  306]
train() client id: f_00004-1-3 loss: 0.673230  [  128/  306]
train() client id: f_00004-1-4 loss: 0.893568  [  160/  306]
train() client id: f_00004-1-5 loss: 0.693352  [  192/  306]
train() client id: f_00004-1-6 loss: 0.819205  [  224/  306]
train() client id: f_00004-1-7 loss: 0.740677  [  256/  306]
train() client id: f_00004-1-8 loss: 0.705790  [  288/  306]
train() client id: f_00004-2-0 loss: 0.714565  [   32/  306]
train() client id: f_00004-2-1 loss: 0.682722  [   64/  306]
train() client id: f_00004-2-2 loss: 0.686332  [   96/  306]
train() client id: f_00004-2-3 loss: 0.780040  [  128/  306]
train() client id: f_00004-2-4 loss: 0.757371  [  160/  306]
train() client id: f_00004-2-5 loss: 0.905018  [  192/  306]
train() client id: f_00004-2-6 loss: 0.874530  [  224/  306]
train() client id: f_00004-2-7 loss: 0.833309  [  256/  306]
train() client id: f_00004-2-8 loss: 0.785074  [  288/  306]
train() client id: f_00004-3-0 loss: 0.852687  [   32/  306]
train() client id: f_00004-3-1 loss: 0.844004  [   64/  306]
train() client id: f_00004-3-2 loss: 0.691890  [   96/  306]
train() client id: f_00004-3-3 loss: 0.816705  [  128/  306]
train() client id: f_00004-3-4 loss: 0.816774  [  160/  306]
train() client id: f_00004-3-5 loss: 0.786710  [  192/  306]
train() client id: f_00004-3-6 loss: 0.736671  [  224/  306]
train() client id: f_00004-3-7 loss: 0.763451  [  256/  306]
train() client id: f_00004-3-8 loss: 0.721907  [  288/  306]
train() client id: f_00004-4-0 loss: 0.937536  [   32/  306]
train() client id: f_00004-4-1 loss: 0.694326  [   64/  306]
train() client id: f_00004-4-2 loss: 0.802079  [   96/  306]
train() client id: f_00004-4-3 loss: 0.723618  [  128/  306]
train() client id: f_00004-4-4 loss: 0.751356  [  160/  306]
train() client id: f_00004-4-5 loss: 0.758642  [  192/  306]
train() client id: f_00004-4-6 loss: 0.862948  [  224/  306]
train() client id: f_00004-4-7 loss: 0.641548  [  256/  306]
train() client id: f_00004-4-8 loss: 0.880573  [  288/  306]
train() client id: f_00004-5-0 loss: 0.770236  [   32/  306]
train() client id: f_00004-5-1 loss: 0.703120  [   64/  306]
train() client id: f_00004-5-2 loss: 0.714522  [   96/  306]
train() client id: f_00004-5-3 loss: 0.874848  [  128/  306]
train() client id: f_00004-5-4 loss: 0.769811  [  160/  306]
train() client id: f_00004-5-5 loss: 0.724288  [  192/  306]
train() client id: f_00004-5-6 loss: 0.774802  [  224/  306]
train() client id: f_00004-5-7 loss: 0.838484  [  256/  306]
train() client id: f_00004-5-8 loss: 0.900828  [  288/  306]
train() client id: f_00005-0-0 loss: 0.742847  [   32/  146]
train() client id: f_00005-0-1 loss: 0.901200  [   64/  146]
train() client id: f_00005-0-2 loss: 0.499182  [   96/  146]
train() client id: f_00005-0-3 loss: 0.841506  [  128/  146]
train() client id: f_00005-1-0 loss: 0.750040  [   32/  146]
train() client id: f_00005-1-1 loss: 0.873789  [   64/  146]
train() client id: f_00005-1-2 loss: 0.737534  [   96/  146]
train() client id: f_00005-1-3 loss: 0.648104  [  128/  146]
train() client id: f_00005-2-0 loss: 0.597878  [   32/  146]
train() client id: f_00005-2-1 loss: 0.822616  [   64/  146]
train() client id: f_00005-2-2 loss: 0.871446  [   96/  146]
train() client id: f_00005-2-3 loss: 0.780721  [  128/  146]
train() client id: f_00005-3-0 loss: 0.496307  [   32/  146]
train() client id: f_00005-3-1 loss: 0.834473  [   64/  146]
train() client id: f_00005-3-2 loss: 0.715416  [   96/  146]
train() client id: f_00005-3-3 loss: 0.912843  [  128/  146]
train() client id: f_00005-4-0 loss: 0.921915  [   32/  146]
train() client id: f_00005-4-1 loss: 0.640397  [   64/  146]
train() client id: f_00005-4-2 loss: 0.780872  [   96/  146]
train() client id: f_00005-4-3 loss: 0.628916  [  128/  146]
train() client id: f_00005-5-0 loss: 0.672357  [   32/  146]
train() client id: f_00005-5-1 loss: 1.026969  [   64/  146]
train() client id: f_00005-5-2 loss: 0.582376  [   96/  146]
train() client id: f_00005-5-3 loss: 0.802246  [  128/  146]
train() client id: f_00006-0-0 loss: 0.548389  [   32/   54]
train() client id: f_00006-1-0 loss: 0.496243  [   32/   54]
train() client id: f_00006-2-0 loss: 0.525540  [   32/   54]
train() client id: f_00006-3-0 loss: 0.437094  [   32/   54]
train() client id: f_00006-4-0 loss: 0.497302  [   32/   54]
train() client id: f_00006-5-0 loss: 0.543873  [   32/   54]
train() client id: f_00007-0-0 loss: 0.730598  [   32/  179]
train() client id: f_00007-0-1 loss: 0.665974  [   64/  179]
train() client id: f_00007-0-2 loss: 0.726152  [   96/  179]
train() client id: f_00007-0-3 loss: 0.788609  [  128/  179]
train() client id: f_00007-0-4 loss: 0.691401  [  160/  179]
train() client id: f_00007-1-0 loss: 0.686958  [   32/  179]
train() client id: f_00007-1-1 loss: 0.742846  [   64/  179]
train() client id: f_00007-1-2 loss: 0.774798  [   96/  179]
train() client id: f_00007-1-3 loss: 0.553672  [  128/  179]
train() client id: f_00007-1-4 loss: 0.874315  [  160/  179]
train() client id: f_00007-2-0 loss: 0.613632  [   32/  179]
train() client id: f_00007-2-1 loss: 0.791814  [   64/  179]
train() client id: f_00007-2-2 loss: 0.770560  [   96/  179]
train() client id: f_00007-2-3 loss: 0.732332  [  128/  179]
train() client id: f_00007-2-4 loss: 0.525334  [  160/  179]
train() client id: f_00007-3-0 loss: 0.586044  [   32/  179]
train() client id: f_00007-3-1 loss: 0.792468  [   64/  179]
train() client id: f_00007-3-2 loss: 0.672830  [   96/  179]
train() client id: f_00007-3-3 loss: 0.812382  [  128/  179]
train() client id: f_00007-3-4 loss: 0.750156  [  160/  179]
train() client id: f_00007-4-0 loss: 0.911895  [   32/  179]
train() client id: f_00007-4-1 loss: 0.669715  [   64/  179]
train() client id: f_00007-4-2 loss: 0.597038  [   96/  179]
train() client id: f_00007-4-3 loss: 0.817625  [  128/  179]
train() client id: f_00007-4-4 loss: 0.503371  [  160/  179]
train() client id: f_00007-5-0 loss: 0.550184  [   32/  179]
train() client id: f_00007-5-1 loss: 0.817157  [   64/  179]
train() client id: f_00007-5-2 loss: 0.633328  [   96/  179]
train() client id: f_00007-5-3 loss: 0.830545  [  128/  179]
train() client id: f_00007-5-4 loss: 0.754783  [  160/  179]
train() client id: f_00008-0-0 loss: 0.653516  [   32/  130]
train() client id: f_00008-0-1 loss: 0.754548  [   64/  130]
train() client id: f_00008-0-2 loss: 0.706666  [   96/  130]
train() client id: f_00008-0-3 loss: 0.666831  [  128/  130]
train() client id: f_00008-1-0 loss: 0.725969  [   32/  130]
train() client id: f_00008-1-1 loss: 0.638609  [   64/  130]
train() client id: f_00008-1-2 loss: 0.804036  [   96/  130]
train() client id: f_00008-1-3 loss: 0.664007  [  128/  130]
train() client id: f_00008-2-0 loss: 0.726797  [   32/  130]
train() client id: f_00008-2-1 loss: 0.765098  [   64/  130]
train() client id: f_00008-2-2 loss: 0.657240  [   96/  130]
train() client id: f_00008-2-3 loss: 0.643660  [  128/  130]
train() client id: f_00008-3-0 loss: 0.742125  [   32/  130]
train() client id: f_00008-3-1 loss: 0.724131  [   64/  130]
train() client id: f_00008-3-2 loss: 0.639300  [   96/  130]
train() client id: f_00008-3-3 loss: 0.725949  [  128/  130]
train() client id: f_00008-4-0 loss: 0.675501  [   32/  130]
train() client id: f_00008-4-1 loss: 0.687407  [   64/  130]
train() client id: f_00008-4-2 loss: 0.633183  [   96/  130]
train() client id: f_00008-4-3 loss: 0.787250  [  128/  130]
train() client id: f_00008-5-0 loss: 0.715961  [   32/  130]
train() client id: f_00008-5-1 loss: 0.603045  [   64/  130]
train() client id: f_00008-5-2 loss: 0.767004  [   96/  130]
train() client id: f_00008-5-3 loss: 0.741782  [  128/  130]
train() client id: f_00009-0-0 loss: 1.111337  [   32/  118]
train() client id: f_00009-0-1 loss: 0.717395  [   64/  118]
train() client id: f_00009-0-2 loss: 0.890936  [   96/  118]
train() client id: f_00009-1-0 loss: 0.892640  [   32/  118]
train() client id: f_00009-1-1 loss: 0.902839  [   64/  118]
train() client id: f_00009-1-2 loss: 0.735967  [   96/  118]
train() client id: f_00009-2-0 loss: 0.795298  [   32/  118]
train() client id: f_00009-2-1 loss: 0.737833  [   64/  118]
train() client id: f_00009-2-2 loss: 0.836027  [   96/  118]
train() client id: f_00009-3-0 loss: 0.824044  [   32/  118]
train() client id: f_00009-3-1 loss: 0.658357  [   64/  118]
train() client id: f_00009-3-2 loss: 0.840753  [   96/  118]
train() client id: f_00009-4-0 loss: 0.891400  [   32/  118]
train() client id: f_00009-4-1 loss: 0.714091  [   64/  118]
train() client id: f_00009-4-2 loss: 0.731012  [   96/  118]
train() client id: f_00009-5-0 loss: 0.912005  [   32/  118]
train() client id: f_00009-5-1 loss: 0.662774  [   64/  118]
train() client id: f_00009-5-2 loss: 0.854614  [   96/  118]
At round 72 accuracy: 0.649867374005305
At round 72 training accuracy: 0.5841716968477532
At round 72 training loss: 0.8417632537581845
update_location
xs = [  -3.9056584     4.20031788  380.00902392   18.81129433    0.97929623
    3.95640986 -342.44319194 -321.32485185  364.66397685 -307.06087855]
ys = [ 372.5879595   355.55583871    1.32061395 -342.45517586  334.35018685
  317.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [385.79404056 369.37460269 392.94860005 357.25258889 348.98568232
 333.19887093 356.75514046 336.52687362 378.53465625 322.95881315]
dists_bs = [261.36999725 254.38782364 581.5492634  552.56011151 237.40512789
 228.95693543 244.1062424  227.48366256 562.15809678 216.05686492]
uav_gains = [8.21244196e-13 9.67100320e-13 7.69464620e-13 1.10763527e-12
 1.22558846e-12 1.52083801e-12 1.11416668e-12 1.44934733e-12
 8.80519312e-13 1.78047100e-12]
bs_gains = [1.88355381e-11 2.03190964e-11 2.00655865e-12 2.31542863e-12
 2.46559544e-11 2.72887370e-11 2.28072648e-11 2.77864773e-11
 2.20643109e-12 3.20998859e-11]
Round 73
-------------------------------
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.3194998  2.59288881 1.30418738 0.50035792 2.98837254 1.43930991
 0.6065387  1.80528297 1.31771582 1.16737463]
obj_prev = 15.041528473465261
eta_min = 6.730086322824428e-72	eta_max = 0.960768447983313
af = 3.101969802094253	bf = 0.606014338290774	zeta = 3.4121667823036788	eta = 0.9090909090909091
af = 3.101969802094253	bf = 0.606014338290774	zeta = 9.918300016635595	eta = 0.3127521648761819
af = 3.101969802094253	bf = 0.606014338290774	zeta = 6.128169132936568	eta = 0.5061821458912337
af = 3.101969802094253	bf = 0.606014338290774	zeta = 5.504089108696306	eta = 0.5635755055624424
af = 3.101969802094253	bf = 0.606014338290774	zeta = 5.464521898505789	eta = 0.5676562121459247
af = 3.101969802094253	bf = 0.606014338290774	zeta = 5.464335618649047	eta = 0.5676755636142928
eta = 0.5676755636142928
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [0.0467219  0.09826431 0.04598028 0.01594477 0.11346746 0.05413807
 0.02002367 0.06637476 0.04820513 0.04375543]
ene_total = [0.57257327 0.78687954 0.57638183 0.31001941 0.89561579 0.45820012
 0.33936518 0.65485131 0.49074166 0.3797075 ]
ti_comp = [3.27393193 3.49770683 3.26133269 3.32347001 3.50175933 3.50374398
 3.32432233 3.35848843 3.40012884 3.50673809]
ti_coms = [0.31199547 0.08822057 0.32459471 0.26245738 0.08416807 0.08218342
 0.26160506 0.22743896 0.18579855 0.07918931]
t_total = [26.34969368 26.34969368 26.34969368 26.34969368 26.34969368 26.34969368
 26.34969368 26.34969368 26.34969368 26.34969368]
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [5.94705746e-07 4.84730784e-06 5.71221622e-07 2.29377929e-08
 7.44597339e-06 8.07836791e-07 4.54051187e-08 1.62032140e-06
 6.05576618e-07 4.25764209e-07]
ene_total = [0.22926124 0.06486083 0.2385191  0.19285604 0.06190211 0.060395
 0.19222992 0.16713595 0.13653078 0.05819209]
optimize_network iter = 0 obj = 1.401883055865988
eta = 0.5676755636142928
freqs = [ 7135441.14428652 14046961.66554712  7049308.38877608  2398814.24536511
 16201493.85610097  7725745.89318307  3011692.04068683  9881641.69424418
  7088721.37557428  6238764.68717198]
eta_min = 0.5676755636142934	eta_max = 0.8387833537635655
af = 9.530769728321942e-05	bf = 0.606014338290774	zeta = 0.00010483846701154136	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [1.00107717e-07 8.15954657e-07 9.61545994e-08 3.86115336e-09
 1.25339196e-06 1.35984388e-07 7.64311227e-09 2.72751152e-07
 1.01937628e-07 7.16695331e-08]
ene_total = [1.09025033 0.30830925 1.13427738 0.91713951 0.29416335 0.28718908
 0.91416128 0.79477943 0.64926391 0.27672413]
ti_comp = [1.02522061 1.24899552 1.01262137 1.0747587  1.25304801 1.25503266
 1.07561102 1.10977712 1.15141753 1.25802677]
ti_coms = [0.31199547 0.08822057 0.32459471 0.26245738 0.08416807 0.08218342
 0.26160506 0.22743896 0.18579855 0.07918931]
t_total = [26.34969368 26.34969368 26.34969368 26.34969368 26.34969368 26.34969368
 26.34969368 26.34969368 26.34969368 26.34969368]
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [1.81535984e-07 1.13789349e-06 1.77360386e-07 6.56552073e-09
 1.74066261e-06 1.88466540e-07 1.29824466e-08 4.44194635e-07
 1.58070248e-07 9.90266306e-08]
ene_total = [0.61478717 0.17386006 0.63961375 0.51716947 0.16588653 0.16194522
 0.51549012 0.44817469 0.36611709 0.1560436 ]
optimize_network iter = 1 obj = 3.7590876976936305
eta = 0.8387833537635655
freqs = [ 7074978.59491438 12213971.70314103  7049308.38877607  2303188.06336389
 14058067.6277585   6696837.19701407  2890083.78088009  9285155.45638792
  6499537.15984087  5399630.45377395]
eta_min = 0.8387833537635758	eta_max = 0.8387833537635625
af = 7.4931901688053e-05	bf = 0.606014338290774	zeta = 8.242509185685831e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [9.84183688e-08 6.16900397e-07 9.61545994e-08 3.55944769e-09
 9.43687147e-07 1.02175717e-07 7.03833579e-09 2.40816781e-07
 8.56965960e-08 5.36865431e-08]
ene_total = [1.09025027 0.30830229 1.13427738 0.9171395  0.29415253 0.2871879
 0.91416126 0.79477832 0.64926334 0.2767235 ]
ti_comp = [1.02522061 1.24899552 1.01262137 1.0747587  1.25304801 1.25503266
 1.07561102 1.10977712 1.15141753 1.25802677]
ti_coms = [0.31199547 0.08822057 0.32459471 0.26245738 0.08416807 0.08218342
 0.26160506 0.22743896 0.18579855 0.07918931]
t_total = [26.34969368 26.34969368 26.34969368 26.34969368 26.34969368 26.34969368
 26.34969368 26.34969368 26.34969368 26.34969368]
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [1.81535984e-07 1.13789349e-06 1.77360386e-07 6.56552073e-09
 1.74066261e-06 1.88466540e-07 1.29824466e-08 4.44194635e-07
 1.58070248e-07 9.90266306e-08]
ene_total = [0.61478717 0.17386006 0.63961375 0.51716947 0.16588653 0.16194522
 0.51549012 0.44817469 0.36611709 0.1560436 ]
optimize_network iter = 2 obj = 3.759087697693561
eta = 0.8387833537635625
freqs = [ 7074978.59491436 12213971.70314105  7049308.38877605  2303188.06336388
 14058067.62775852  6696837.19701408  2890083.78088009  9285155.45638791
  6499537.15984087  5399630.45377396]
Done!
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [1.57674265e-07 9.88324819e-07 1.54047521e-07 5.70252590e-09
 1.51186388e-06 1.63693844e-07 1.12759888e-08 3.85808151e-07
 1.37292946e-07 8.60102267e-08]
ene_total = [0.0311997  0.00882304 0.03245962 0.02624574 0.00841832 0.00821851
 0.02616052 0.02274428 0.01857999 0.00791902]
At round 73 energy consumption: 0.19076875112517003
At round 73 eta: 0.8387833537635625
At round 73 a_n: 2.834210180524895
At round 73 local rounds: 5.756677690695127
At round 73 global rounds: 19.70488843214887
gradient difference: 0.6428208947181702
train() client id: f_00000-0-0 loss: 1.104061  [   32/  126]
train() client id: f_00000-0-1 loss: 1.145351  [   64/  126]
train() client id: f_00000-0-2 loss: 1.015527  [   96/  126]
train() client id: f_00000-1-0 loss: 0.950392  [   32/  126]
train() client id: f_00000-1-1 loss: 1.115620  [   64/  126]
train() client id: f_00000-1-2 loss: 1.106690  [   96/  126]
train() client id: f_00000-2-0 loss: 0.940519  [   32/  126]
train() client id: f_00000-2-1 loss: 0.967985  [   64/  126]
train() client id: f_00000-2-2 loss: 1.109861  [   96/  126]
train() client id: f_00000-3-0 loss: 0.943779  [   32/  126]
train() client id: f_00000-3-1 loss: 0.948327  [   64/  126]
train() client id: f_00000-3-2 loss: 1.131748  [   96/  126]
train() client id: f_00000-4-0 loss: 1.016912  [   32/  126]
train() client id: f_00000-4-1 loss: 0.870536  [   64/  126]
train() client id: f_00000-4-2 loss: 1.002978  [   96/  126]
train() client id: f_00001-0-0 loss: 0.390875  [   32/  265]
train() client id: f_00001-0-1 loss: 0.397514  [   64/  265]
train() client id: f_00001-0-2 loss: 0.472398  [   96/  265]
train() client id: f_00001-0-3 loss: 0.483821  [  128/  265]
train() client id: f_00001-0-4 loss: 0.472872  [  160/  265]
train() client id: f_00001-0-5 loss: 0.510990  [  192/  265]
train() client id: f_00001-0-6 loss: 0.464967  [  224/  265]
train() client id: f_00001-0-7 loss: 0.490817  [  256/  265]
train() client id: f_00001-1-0 loss: 0.394942  [   32/  265]
train() client id: f_00001-1-1 loss: 0.507255  [   64/  265]
train() client id: f_00001-1-2 loss: 0.609535  [   96/  265]
train() client id: f_00001-1-3 loss: 0.361737  [  128/  265]
train() client id: f_00001-1-4 loss: 0.453882  [  160/  265]
train() client id: f_00001-1-5 loss: 0.427184  [  192/  265]
train() client id: f_00001-1-6 loss: 0.473698  [  224/  265]
train() client id: f_00001-1-7 loss: 0.494265  [  256/  265]
train() client id: f_00001-2-0 loss: 0.398436  [   32/  265]
train() client id: f_00001-2-1 loss: 0.409587  [   64/  265]
train() client id: f_00001-2-2 loss: 0.386611  [   96/  265]
train() client id: f_00001-2-3 loss: 0.502090  [  128/  265]
train() client id: f_00001-2-4 loss: 0.596851  [  160/  265]
train() client id: f_00001-2-5 loss: 0.457984  [  192/  265]
train() client id: f_00001-2-6 loss: 0.457282  [  224/  265]
train() client id: f_00001-2-7 loss: 0.443421  [  256/  265]
train() client id: f_00001-3-0 loss: 0.467607  [   32/  265]
train() client id: f_00001-3-1 loss: 0.361860  [   64/  265]
train() client id: f_00001-3-2 loss: 0.561531  [   96/  265]
train() client id: f_00001-3-3 loss: 0.570709  [  128/  265]
train() client id: f_00001-3-4 loss: 0.368167  [  160/  265]
train() client id: f_00001-3-5 loss: 0.506144  [  192/  265]
train() client id: f_00001-3-6 loss: 0.376520  [  224/  265]
train() client id: f_00001-3-7 loss: 0.431916  [  256/  265]
train() client id: f_00001-4-0 loss: 0.504586  [   32/  265]
train() client id: f_00001-4-1 loss: 0.448820  [   64/  265]
train() client id: f_00001-4-2 loss: 0.373887  [   96/  265]
train() client id: f_00001-4-3 loss: 0.408109  [  128/  265]
train() client id: f_00001-4-4 loss: 0.507396  [  160/  265]
train() client id: f_00001-4-5 loss: 0.482930  [  192/  265]
train() client id: f_00001-4-6 loss: 0.497256  [  224/  265]
train() client id: f_00001-4-7 loss: 0.391988  [  256/  265]
train() client id: f_00002-0-0 loss: 0.880507  [   32/  124]
train() client id: f_00002-0-1 loss: 0.740168  [   64/  124]
train() client id: f_00002-0-2 loss: 1.268774  [   96/  124]
train() client id: f_00002-1-0 loss: 0.992996  [   32/  124]
train() client id: f_00002-1-1 loss: 0.991853  [   64/  124]
train() client id: f_00002-1-2 loss: 0.799832  [   96/  124]
train() client id: f_00002-2-0 loss: 0.835011  [   32/  124]
train() client id: f_00002-2-1 loss: 1.006830  [   64/  124]
train() client id: f_00002-2-2 loss: 0.916093  [   96/  124]
train() client id: f_00002-3-0 loss: 0.828043  [   32/  124]
train() client id: f_00002-3-1 loss: 0.900769  [   64/  124]
train() client id: f_00002-3-2 loss: 0.775516  [   96/  124]
train() client id: f_00002-4-0 loss: 0.784542  [   32/  124]
train() client id: f_00002-4-1 loss: 1.071139  [   64/  124]
train() client id: f_00002-4-2 loss: 0.773268  [   96/  124]
train() client id: f_00003-0-0 loss: 0.628221  [   32/   43]
train() client id: f_00003-1-0 loss: 0.561744  [   32/   43]
train() client id: f_00003-2-0 loss: 0.341150  [   32/   43]
train() client id: f_00003-3-0 loss: 0.409244  [   32/   43]
train() client id: f_00003-4-0 loss: 0.346610  [   32/   43]
train() client id: f_00004-0-0 loss: 0.724603  [   32/  306]
train() client id: f_00004-0-1 loss: 0.809911  [   64/  306]
train() client id: f_00004-0-2 loss: 0.628514  [   96/  306]
train() client id: f_00004-0-3 loss: 0.874152  [  128/  306]
train() client id: f_00004-0-4 loss: 0.797214  [  160/  306]
train() client id: f_00004-0-5 loss: 0.702837  [  192/  306]
train() client id: f_00004-0-6 loss: 0.566571  [  224/  306]
train() client id: f_00004-0-7 loss: 0.616410  [  256/  306]
train() client id: f_00004-0-8 loss: 0.762038  [  288/  306]
train() client id: f_00004-1-0 loss: 0.826333  [   32/  306]
train() client id: f_00004-1-1 loss: 0.687775  [   64/  306]
train() client id: f_00004-1-2 loss: 0.658231  [   96/  306]
train() client id: f_00004-1-3 loss: 0.674433  [  128/  306]
train() client id: f_00004-1-4 loss: 0.855016  [  160/  306]
train() client id: f_00004-1-5 loss: 0.677333  [  192/  306]
train() client id: f_00004-1-6 loss: 0.650784  [  224/  306]
train() client id: f_00004-1-7 loss: 0.629934  [  256/  306]
train() client id: f_00004-1-8 loss: 0.812117  [  288/  306]
train() client id: f_00004-2-0 loss: 0.693521  [   32/  306]
train() client id: f_00004-2-1 loss: 0.594000  [   64/  306]
train() client id: f_00004-2-2 loss: 0.656558  [   96/  306]
train() client id: f_00004-2-3 loss: 0.698770  [  128/  306]
train() client id: f_00004-2-4 loss: 0.868253  [  160/  306]
train() client id: f_00004-2-5 loss: 0.753782  [  192/  306]
train() client id: f_00004-2-6 loss: 0.705320  [  224/  306]
train() client id: f_00004-2-7 loss: 0.787655  [  256/  306]
train() client id: f_00004-2-8 loss: 0.678695  [  288/  306]
train() client id: f_00004-3-0 loss: 0.698813  [   32/  306]
train() client id: f_00004-3-1 loss: 0.505664  [   64/  306]
train() client id: f_00004-3-2 loss: 0.842153  [   96/  306]
train() client id: f_00004-3-3 loss: 0.705703  [  128/  306]
train() client id: f_00004-3-4 loss: 0.929644  [  160/  306]
train() client id: f_00004-3-5 loss: 0.705163  [  192/  306]
train() client id: f_00004-3-6 loss: 0.888695  [  224/  306]
train() client id: f_00004-3-7 loss: 0.573790  [  256/  306]
train() client id: f_00004-3-8 loss: 0.691133  [  288/  306]
train() client id: f_00004-4-0 loss: 0.764355  [   32/  306]
train() client id: f_00004-4-1 loss: 0.658929  [   64/  306]
train() client id: f_00004-4-2 loss: 0.651924  [   96/  306]
train() client id: f_00004-4-3 loss: 0.769308  [  128/  306]
train() client id: f_00004-4-4 loss: 0.673049  [  160/  306]
train() client id: f_00004-4-5 loss: 0.752716  [  192/  306]
train() client id: f_00004-4-6 loss: 0.710343  [  224/  306]
train() client id: f_00004-4-7 loss: 0.759602  [  256/  306]
train() client id: f_00004-4-8 loss: 0.737221  [  288/  306]
train() client id: f_00005-0-0 loss: 0.805948  [   32/  146]
train() client id: f_00005-0-1 loss: 0.568994  [   64/  146]
train() client id: f_00005-0-2 loss: 0.759295  [   96/  146]
train() client id: f_00005-0-3 loss: 0.455664  [  128/  146]
train() client id: f_00005-1-0 loss: 0.940390  [   32/  146]
train() client id: f_00005-1-1 loss: 0.855812  [   64/  146]
train() client id: f_00005-1-2 loss: 0.432447  [   96/  146]
train() client id: f_00005-1-3 loss: 0.484829  [  128/  146]
train() client id: f_00005-2-0 loss: 0.856422  [   32/  146]
train() client id: f_00005-2-1 loss: 0.750818  [   64/  146]
train() client id: f_00005-2-2 loss: 0.521667  [   96/  146]
train() client id: f_00005-2-3 loss: 0.422867  [  128/  146]
train() client id: f_00005-3-0 loss: 0.515135  [   32/  146]
train() client id: f_00005-3-1 loss: 0.685066  [   64/  146]
train() client id: f_00005-3-2 loss: 0.709293  [   96/  146]
train() client id: f_00005-3-3 loss: 0.571374  [  128/  146]
train() client id: f_00005-4-0 loss: 0.413451  [   32/  146]
train() client id: f_00005-4-1 loss: 0.771668  [   64/  146]
train() client id: f_00005-4-2 loss: 0.390615  [   96/  146]
train() client id: f_00005-4-3 loss: 1.065584  [  128/  146]
train() client id: f_00006-0-0 loss: 0.476079  [   32/   54]
train() client id: f_00006-1-0 loss: 0.465450  [   32/   54]
train() client id: f_00006-2-0 loss: 0.425696  [   32/   54]
train() client id: f_00006-3-0 loss: 0.486375  [   32/   54]
train() client id: f_00006-4-0 loss: 0.470510  [   32/   54]
train() client id: f_00007-0-0 loss: 0.810552  [   32/  179]
train() client id: f_00007-0-1 loss: 0.609313  [   64/  179]
train() client id: f_00007-0-2 loss: 0.465480  [   96/  179]
train() client id: f_00007-0-3 loss: 0.670735  [  128/  179]
train() client id: f_00007-0-4 loss: 0.464985  [  160/  179]
train() client id: f_00007-1-0 loss: 0.625806  [   32/  179]
train() client id: f_00007-1-1 loss: 0.497993  [   64/  179]
train() client id: f_00007-1-2 loss: 0.714452  [   96/  179]
train() client id: f_00007-1-3 loss: 0.737342  [  128/  179]
train() client id: f_00007-1-4 loss: 0.419623  [  160/  179]
train() client id: f_00007-2-0 loss: 0.437964  [   32/  179]
train() client id: f_00007-2-1 loss: 0.504827  [   64/  179]
train() client id: f_00007-2-2 loss: 0.550988  [   96/  179]
train() client id: f_00007-2-3 loss: 0.703727  [  128/  179]
train() client id: f_00007-2-4 loss: 0.876647  [  160/  179]
train() client id: f_00007-3-0 loss: 0.599643  [   32/  179]
train() client id: f_00007-3-1 loss: 0.572337  [   64/  179]
train() client id: f_00007-3-2 loss: 0.623219  [   96/  179]
train() client id: f_00007-3-3 loss: 0.587090  [  128/  179]
train() client id: f_00007-3-4 loss: 0.564740  [  160/  179]
train() client id: f_00007-4-0 loss: 0.601448  [   32/  179]
train() client id: f_00007-4-1 loss: 0.563977  [   64/  179]
train() client id: f_00007-4-2 loss: 0.512560  [   96/  179]
train() client id: f_00007-4-3 loss: 0.715230  [  128/  179]
train() client id: f_00007-4-4 loss: 0.623940  [  160/  179]
train() client id: f_00008-0-0 loss: 0.687901  [   32/  130]
train() client id: f_00008-0-1 loss: 0.627327  [   64/  130]
train() client id: f_00008-0-2 loss: 0.672404  [   96/  130]
train() client id: f_00008-0-3 loss: 0.776645  [  128/  130]
train() client id: f_00008-1-0 loss: 0.811419  [   32/  130]
train() client id: f_00008-1-1 loss: 0.784586  [   64/  130]
train() client id: f_00008-1-2 loss: 0.603995  [   96/  130]
train() client id: f_00008-1-3 loss: 0.591488  [  128/  130]
train() client id: f_00008-2-0 loss: 0.738273  [   32/  130]
train() client id: f_00008-2-1 loss: 0.921464  [   64/  130]
train() client id: f_00008-2-2 loss: 0.550567  [   96/  130]
train() client id: f_00008-2-3 loss: 0.558992  [  128/  130]
train() client id: f_00008-3-0 loss: 0.616346  [   32/  130]
train() client id: f_00008-3-1 loss: 0.735545  [   64/  130]
train() client id: f_00008-3-2 loss: 0.739898  [   96/  130]
train() client id: f_00008-3-3 loss: 0.653653  [  128/  130]
train() client id: f_00008-4-0 loss: 0.721166  [   32/  130]
train() client id: f_00008-4-1 loss: 0.729009  [   64/  130]
train() client id: f_00008-4-2 loss: 0.653855  [   96/  130]
train() client id: f_00008-4-3 loss: 0.699058  [  128/  130]
train() client id: f_00009-0-0 loss: 0.735575  [   32/  118]
train() client id: f_00009-0-1 loss: 0.965843  [   64/  118]
train() client id: f_00009-0-2 loss: 0.659426  [   96/  118]
train() client id: f_00009-1-0 loss: 0.760502  [   32/  118]
train() client id: f_00009-1-1 loss: 0.895945  [   64/  118]
train() client id: f_00009-1-2 loss: 0.710495  [   96/  118]
train() client id: f_00009-2-0 loss: 0.640820  [   32/  118]
train() client id: f_00009-2-1 loss: 0.860979  [   64/  118]
train() client id: f_00009-2-2 loss: 0.740359  [   96/  118]
train() client id: f_00009-3-0 loss: 0.763198  [   32/  118]
train() client id: f_00009-3-1 loss: 0.714955  [   64/  118]
train() client id: f_00009-3-2 loss: 0.740787  [   96/  118]
train() client id: f_00009-4-0 loss: 0.676009  [   32/  118]
train() client id: f_00009-4-1 loss: 0.671124  [   64/  118]
train() client id: f_00009-4-2 loss: 0.810525  [   96/  118]
At round 73 accuracy: 0.649867374005305
At round 73 training accuracy: 0.5902079141515761
At round 73 training loss: 0.8244571605003936
update_location
xs = [  -3.9056584     4.20031788  385.00902392   18.81129433    0.97929623
    3.95640986 -347.44319194 -326.32485185  369.66397685 -312.06087855]
ys = [ 377.5879595   360.55583871    1.32061395 -347.45517586  339.35018685
  322.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [390.6250393  374.18999919 397.78598834 362.04828963 353.77889752
 337.97134361 361.5572737  341.30424139 383.35378667 327.71634652]
dists_bs = [265.17004933 257.96070089 586.32297926 557.24521406 240.78973528
 232.10863791 247.56269812 230.73158702 566.95975654 219.14875693]
uav_gains = [7.85626588e-13 9.19780429e-13 7.37654771e-13 1.04796295e-12
 1.15469134e-12 1.42001853e-12 1.05381008e-12 1.35598266e-12
 8.40352469e-13 1.65176343e-12]
bs_gains = [1.80894587e-11 1.95408815e-11 1.96114959e-12 2.26133192e-12
 2.36977859e-11 2.62638529e-11 2.19268126e-11 2.67051078e-11
 2.15450662e-12 3.08478466e-11]
Round 74
-------------------------------
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.17965282 2.31354803 1.16601007 0.44876366 2.6663691  1.28432213
 0.54349708 1.6129016  1.17620535 1.04170236]
obj_prev = 13.43297219536657
eta_min = 1.985153234470665e-80	eta_max = 0.9640166902686983
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 3.044236871939507	eta = 0.9090909090909091
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 8.994246026442418	eta = 0.3076953929504865
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 5.5116993178884695	eta = 0.5021115822515108
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 4.941875446838901	eta = 0.5600076519876258
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 4.905748712654075	eta = 0.5641316397354368
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 4.905578223300182	eta = 0.5641512456685993
eta = 0.5641512456685993
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [0.04723579 0.09934512 0.04648602 0.01612015 0.11471549 0.05473354
 0.02024391 0.06710481 0.04873534 0.04423669]
ene_total = [0.51557111 0.70394459 0.51893062 0.28085198 0.80121777 0.40983951
 0.30711799 0.58944182 0.4390443  0.33961853]
ti_comp = [3.7239081  3.95531799 3.71124405 3.77370601 3.95943373 3.96148105
 3.77455137 3.80899893 3.85658756 3.96449942]
ti_coms = [0.32049446 0.08908456 0.3331585  0.27069654 0.08496883 0.08292151
 0.26985118 0.23540363 0.187815   0.07990314]
t_total = [26.29968948 26.29968948 26.29968948 26.29968948 26.29968948 26.29968948
 26.29968948 26.29968948 26.29968948 26.29968948]
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [4.75002260e-07 3.91702877e-06 4.55835322e-07 1.83844731e-08
 6.01838646e-06 6.53018300e-07 3.63942310e-08 1.30172529e-06
 4.86413493e-07 3.44232011e-07]
ene_total = [0.20841223 0.05795483 0.2166472  0.17602698 0.05529214 0.05392593
 0.17547739 0.15308527 0.12213433 0.05196115]
optimize_network iter = 0 obj = 1.2709174591621142
eta = 0.5641512456685993
freqs = [ 6342233.67091708 12558423.2934157   6262861.66135149  2135851.43012001
 14486350.74522172  6908216.2161498   2681631.30754287  8808720.47508642
  6318453.65727492  5579101.94228267]
eta_min = 0.5641512456686001	eta_max = 0.8530565293483942
af = 6.786276083478417e-05	bf = 0.5538889445570291	zeta = 7.464903691826259e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [7.90879612e-08 6.52185990e-07 7.58966627e-08 3.06101806e-09
 1.00206242e-06 1.08727664e-07 6.05964597e-09 2.16737493e-07
 8.09879336e-08 5.73146914e-08]
ene_total = [0.99918599 0.27775315 1.03866771 0.84393208 0.2649327  0.25852208
 0.84129665 0.73390845 0.58554042 0.2491103 ]
ti_comp = [1.04304861 1.27445851 1.03038457 1.09284653 1.27857424 1.28062156
 1.09369189 1.12813944 1.17572807 1.28363993]
ti_coms = [0.32049446 0.08908456 0.3331585  0.27069654 0.08496883 0.08292151
 0.26985118 0.23540363 0.187815   0.07990314]
t_total = [26.29968948 26.29968948 26.29968948 26.29968948 26.29968948 26.29968948
 26.29968948 26.29968948 26.29968948 26.29968948]
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [1.29575823e-07 8.07435167e-07 1.26557476e-07 4.69146470e-09
 1.23519014e-06 1.33732939e-07 9.27711067e-09 3.17581333e-07
 1.12005080e-07 7.02717911e-08]
ene_total = [0.61816447 0.17183973 0.64259052 0.52211305 0.16390964 0.15993959
 0.52048262 0.45404696 0.36225519 0.1541166 ]
optimize_network iter = 1 obj = 3.769458367807797
eta = 0.8530565293483942
freqs = [ 6286609.35895016 10821081.10112869  6262861.66135149  2047669.00897199
 12455063.70509777  5933111.71394756  2569503.70041527  8257350.63294458
  5754226.36383451  4783979.0224429 ]
eta_min = 0.8530565293483974	eta_max = 0.8530565293483936
af = 5.2542085376783516e-05	bf = 0.5538889445570291	zeta = 5.7796293914461874e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [7.77067690e-08 4.84219790e-07 7.58966627e-08 2.81347673e-09
 7.40744934e-07 8.01997961e-08 5.56349384e-09 1.90453887e-07
 6.71695741e-08 4.21420732e-08]
ene_total = [0.99918595 0.27774791 1.03866771 0.84393207 0.26492456 0.25852119
 0.84129664 0.73390763 0.58553999 0.24910983]
ti_comp = [1.04304861 1.27445851 1.03038457 1.09284653 1.27857424 1.28062156
 1.09369189 1.12813944 1.17572807 1.28363993]
ti_coms = [0.32049446 0.08908456 0.3331585  0.27069654 0.08496883 0.08292151
 0.26985118 0.23540363 0.187815   0.07990314]
t_total = [26.29968948 26.29968948 26.29968948 26.29968948 26.29968948 26.29968948
 26.29968948 26.29968948 26.29968948 26.29968948]
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [1.29575823e-07 8.07435167e-07 1.26557476e-07 4.69146470e-09
 1.23519014e-06 1.33732939e-07 9.27711067e-09 3.17581333e-07
 1.12005080e-07 7.02717911e-08]
ene_total = [0.61816447 0.17183973 0.64259052 0.52211305 0.16390964 0.15993959
 0.52048262 0.45404696 0.36225519 0.1541166 ]
optimize_network iter = 2 obj = 3.7694583678077827
eta = 0.8530565293483936
freqs = [ 6286609.35895015 10821081.10112868  6262861.66135148  2047669.00897199
 12455063.70509776  5933111.71394756  2569503.70041527  8257350.63294457
  5754226.3638345   4783979.0224429 ]
Done!
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [1.24492590e-07 7.75759650e-07 1.21592652e-07 4.50741950e-09
 1.18673388e-06 1.28486623e-07 8.91317151e-09 3.05122682e-07
 1.07611143e-07 6.75150431e-08]
ene_total = [0.03204957 0.00890923 0.03331597 0.02706966 0.00849807 0.00829228
 0.02698513 0.02354067 0.01878161 0.00799038]
At round 74 energy consumption: 0.1954325657499382
At round 74 eta: 0.8530565293483936
At round 74 a_n: 2.4916643335555797
At round 74 local rounds: 5.2041580597357475
At round 74 global rounds: 19.287758537054202
gradient difference: 0.9455734491348267
train() client id: f_00000-0-0 loss: 0.741734  [   32/  126]
train() client id: f_00000-0-1 loss: 0.841038  [   64/  126]
train() client id: f_00000-0-2 loss: 0.945452  [   96/  126]
train() client id: f_00000-1-0 loss: 1.018171  [   32/  126]
train() client id: f_00000-1-1 loss: 0.906710  [   64/  126]
train() client id: f_00000-1-2 loss: 0.747917  [   96/  126]
train() client id: f_00000-2-0 loss: 0.989340  [   32/  126]
train() client id: f_00000-2-1 loss: 0.848281  [   64/  126]
train() client id: f_00000-2-2 loss: 0.759552  [   96/  126]
train() client id: f_00000-3-0 loss: 0.800817  [   32/  126]
train() client id: f_00000-3-1 loss: 0.948360  [   64/  126]
train() client id: f_00000-3-2 loss: 0.783099  [   96/  126]
train() client id: f_00000-4-0 loss: 0.887099  [   32/  126]
train() client id: f_00000-4-1 loss: 0.863541  [   64/  126]
train() client id: f_00000-4-2 loss: 0.737891  [   96/  126]
train() client id: f_00001-0-0 loss: 0.584320  [   32/  265]
train() client id: f_00001-0-1 loss: 0.504677  [   64/  265]
train() client id: f_00001-0-2 loss: 0.691019  [   96/  265]
train() client id: f_00001-0-3 loss: 0.653852  [  128/  265]
train() client id: f_00001-0-4 loss: 0.675923  [  160/  265]
train() client id: f_00001-0-5 loss: 0.542158  [  192/  265]
train() client id: f_00001-0-6 loss: 0.615653  [  224/  265]
train() client id: f_00001-0-7 loss: 0.479491  [  256/  265]
train() client id: f_00001-1-0 loss: 0.621639  [   32/  265]
train() client id: f_00001-1-1 loss: 0.586104  [   64/  265]
train() client id: f_00001-1-2 loss: 0.592809  [   96/  265]
train() client id: f_00001-1-3 loss: 0.541485  [  128/  265]
train() client id: f_00001-1-4 loss: 0.607038  [  160/  265]
train() client id: f_00001-1-5 loss: 0.647930  [  192/  265]
train() client id: f_00001-1-6 loss: 0.527799  [  224/  265]
train() client id: f_00001-1-7 loss: 0.573969  [  256/  265]
train() client id: f_00001-2-0 loss: 0.491177  [   32/  265]
train() client id: f_00001-2-1 loss: 0.591487  [   64/  265]
train() client id: f_00001-2-2 loss: 0.484195  [   96/  265]
train() client id: f_00001-2-3 loss: 0.651687  [  128/  265]
train() client id: f_00001-2-4 loss: 0.641022  [  160/  265]
train() client id: f_00001-2-5 loss: 0.624417  [  192/  265]
train() client id: f_00001-2-6 loss: 0.618705  [  224/  265]
train() client id: f_00001-2-7 loss: 0.548460  [  256/  265]
train() client id: f_00001-3-0 loss: 0.633886  [   32/  265]
train() client id: f_00001-3-1 loss: 0.554008  [   64/  265]
train() client id: f_00001-3-2 loss: 0.639926  [   96/  265]
train() client id: f_00001-3-3 loss: 0.574224  [  128/  265]
train() client id: f_00001-3-4 loss: 0.519759  [  160/  265]
train() client id: f_00001-3-5 loss: 0.493959  [  192/  265]
train() client id: f_00001-3-6 loss: 0.612518  [  224/  265]
train() client id: f_00001-3-7 loss: 0.617883  [  256/  265]
train() client id: f_00001-4-0 loss: 0.518985  [   32/  265]
train() client id: f_00001-4-1 loss: 0.558594  [   64/  265]
train() client id: f_00001-4-2 loss: 0.499387  [   96/  265]
train() client id: f_00001-4-3 loss: 0.596250  [  128/  265]
train() client id: f_00001-4-4 loss: 0.555511  [  160/  265]
train() client id: f_00001-4-5 loss: 0.672812  [  192/  265]
train() client id: f_00001-4-6 loss: 0.718184  [  224/  265]
train() client id: f_00001-4-7 loss: 0.637024  [  256/  265]
train() client id: f_00002-0-0 loss: 1.001779  [   32/  124]
train() client id: f_00002-0-1 loss: 1.059076  [   64/  124]
train() client id: f_00002-0-2 loss: 1.184486  [   96/  124]
train() client id: f_00002-1-0 loss: 1.221501  [   32/  124]
train() client id: f_00002-1-1 loss: 0.968577  [   64/  124]
train() client id: f_00002-1-2 loss: 1.123116  [   96/  124]
train() client id: f_00002-2-0 loss: 1.003145  [   32/  124]
train() client id: f_00002-2-1 loss: 1.023928  [   64/  124]
train() client id: f_00002-2-2 loss: 1.062697  [   96/  124]
train() client id: f_00002-3-0 loss: 1.092642  [   32/  124]
train() client id: f_00002-3-1 loss: 0.975490  [   64/  124]
train() client id: f_00002-3-2 loss: 1.120565  [   96/  124]
train() client id: f_00002-4-0 loss: 0.922332  [   32/  124]
train() client id: f_00002-4-1 loss: 1.114852  [   64/  124]
train() client id: f_00002-4-2 loss: 1.107454  [   96/  124]
train() client id: f_00003-0-0 loss: 0.604700  [   32/   43]
train() client id: f_00003-1-0 loss: 0.732850  [   32/   43]
train() client id: f_00003-2-0 loss: 0.876437  [   32/   43]
train() client id: f_00003-3-0 loss: 0.611593  [   32/   43]
train() client id: f_00003-4-0 loss: 0.839038  [   32/   43]
train() client id: f_00004-0-0 loss: 0.864173  [   32/  306]
train() client id: f_00004-0-1 loss: 0.778529  [   64/  306]
train() client id: f_00004-0-2 loss: 0.640357  [   96/  306]
train() client id: f_00004-0-3 loss: 0.978532  [  128/  306]
train() client id: f_00004-0-4 loss: 0.769974  [  160/  306]
train() client id: f_00004-0-5 loss: 0.870115  [  192/  306]
train() client id: f_00004-0-6 loss: 0.944018  [  224/  306]
train() client id: f_00004-0-7 loss: 0.788209  [  256/  306]
train() client id: f_00004-0-8 loss: 0.719061  [  288/  306]
train() client id: f_00004-1-0 loss: 0.828395  [   32/  306]
train() client id: f_00004-1-1 loss: 0.746989  [   64/  306]
train() client id: f_00004-1-2 loss: 0.905546  [   96/  306]
train() client id: f_00004-1-3 loss: 0.739941  [  128/  306]
train() client id: f_00004-1-4 loss: 0.770899  [  160/  306]
train() client id: f_00004-1-5 loss: 0.976386  [  192/  306]
train() client id: f_00004-1-6 loss: 0.746637  [  224/  306]
train() client id: f_00004-1-7 loss: 0.852108  [  256/  306]
train() client id: f_00004-1-8 loss: 0.756895  [  288/  306]
train() client id: f_00004-2-0 loss: 0.766501  [   32/  306]
train() client id: f_00004-2-1 loss: 0.763749  [   64/  306]
train() client id: f_00004-2-2 loss: 0.936131  [   96/  306]
train() client id: f_00004-2-3 loss: 0.854489  [  128/  306]
train() client id: f_00004-2-4 loss: 0.797034  [  160/  306]
train() client id: f_00004-2-5 loss: 0.749897  [  192/  306]
train() client id: f_00004-2-6 loss: 0.773980  [  224/  306]
train() client id: f_00004-2-7 loss: 0.862809  [  256/  306]
train() client id: f_00004-2-8 loss: 0.852638  [  288/  306]
train() client id: f_00004-3-0 loss: 0.856192  [   32/  306]
train() client id: f_00004-3-1 loss: 0.796848  [   64/  306]
train() client id: f_00004-3-2 loss: 0.760923  [   96/  306]
train() client id: f_00004-3-3 loss: 0.778827  [  128/  306]
train() client id: f_00004-3-4 loss: 0.646866  [  160/  306]
train() client id: f_00004-3-5 loss: 0.830672  [  192/  306]
train() client id: f_00004-3-6 loss: 0.747805  [  224/  306]
train() client id: f_00004-3-7 loss: 1.168891  [  256/  306]
train() client id: f_00004-3-8 loss: 0.869461  [  288/  306]
train() client id: f_00004-4-0 loss: 0.764268  [   32/  306]
train() client id: f_00004-4-1 loss: 0.793083  [   64/  306]
train() client id: f_00004-4-2 loss: 0.697401  [   96/  306]
train() client id: f_00004-4-3 loss: 0.906814  [  128/  306]
train() client id: f_00004-4-4 loss: 0.759668  [  160/  306]
train() client id: f_00004-4-5 loss: 0.919363  [  192/  306]
train() client id: f_00004-4-6 loss: 0.850363  [  224/  306]
train() client id: f_00004-4-7 loss: 0.762134  [  256/  306]
train() client id: f_00004-4-8 loss: 0.928112  [  288/  306]
train() client id: f_00005-0-0 loss: 0.187761  [   32/  146]
train() client id: f_00005-0-1 loss: -0.069017  [   64/  146]
train() client id: f_00005-0-2 loss: 0.328406  [   96/  146]
train() client id: f_00005-0-3 loss: 0.293256  [  128/  146]
train() client id: f_00005-1-0 loss: 0.284940  [   32/  146]
train() client id: f_00005-1-1 loss: 0.170950  [   64/  146]
train() client id: f_00005-1-2 loss: 0.420245  [   96/  146]
train() client id: f_00005-1-3 loss: 0.064115  [  128/  146]
train() client id: f_00005-2-0 loss: 0.189145  [   32/  146]
train() client id: f_00005-2-1 loss: 0.117377  [   64/  146]
train() client id: f_00005-2-2 loss: 0.176239  [   96/  146]
train() client id: f_00005-2-3 loss: 0.223246  [  128/  146]
train() client id: f_00005-3-0 loss: 0.387732  [   32/  146]
train() client id: f_00005-3-1 loss: -0.113006  [   64/  146]
train() client id: f_00005-3-2 loss: 0.418103  [   96/  146]
train() client id: f_00005-3-3 loss: 0.250392  [  128/  146]
train() client id: f_00005-4-0 loss: 0.086006  [   32/  146]
train() client id: f_00005-4-1 loss: 0.320342  [   64/  146]
train() client id: f_00005-4-2 loss: 0.064165  [   96/  146]
train() client id: f_00005-4-3 loss: 0.454992  [  128/  146]
train() client id: f_00006-0-0 loss: 0.463861  [   32/   54]
train() client id: f_00006-1-0 loss: 0.428952  [   32/   54]
train() client id: f_00006-2-0 loss: 0.477395  [   32/   54]
train() client id: f_00006-3-0 loss: 0.472423  [   32/   54]
train() client id: f_00006-4-0 loss: 0.440607  [   32/   54]
train() client id: f_00007-0-0 loss: 0.489439  [   32/  179]
train() client id: f_00007-0-1 loss: 0.786417  [   64/  179]
train() client id: f_00007-0-2 loss: 0.679326  [   96/  179]
train() client id: f_00007-0-3 loss: 0.565080  [  128/  179]
train() client id: f_00007-0-4 loss: 0.530426  [  160/  179]
train() client id: f_00007-1-0 loss: 0.487062  [   32/  179]
train() client id: f_00007-1-1 loss: 0.461269  [   64/  179]
train() client id: f_00007-1-2 loss: 0.532691  [   96/  179]
train() client id: f_00007-1-3 loss: 0.479465  [  128/  179]
train() client id: f_00007-1-4 loss: 1.000933  [  160/  179]
train() client id: f_00007-2-0 loss: 0.525622  [   32/  179]
train() client id: f_00007-2-1 loss: 0.449022  [   64/  179]
train() client id: f_00007-2-2 loss: 0.582007  [   96/  179]
train() client id: f_00007-2-3 loss: 0.798610  [  128/  179]
train() client id: f_00007-2-4 loss: 0.703844  [  160/  179]
train() client id: f_00007-3-0 loss: 0.669359  [   32/  179]
train() client id: f_00007-3-1 loss: 0.582518  [   64/  179]
train() client id: f_00007-3-2 loss: 0.468161  [   96/  179]
train() client id: f_00007-3-3 loss: 0.534724  [  128/  179]
train() client id: f_00007-3-4 loss: 0.717950  [  160/  179]
train() client id: f_00007-4-0 loss: 0.416463  [   32/  179]
train() client id: f_00007-4-1 loss: 0.403021  [   64/  179]
train() client id: f_00007-4-2 loss: 0.682910  [   96/  179]
train() client id: f_00007-4-3 loss: 0.798225  [  128/  179]
train() client id: f_00007-4-4 loss: 0.676292  [  160/  179]
train() client id: f_00008-0-0 loss: 0.779454  [   32/  130]
train() client id: f_00008-0-1 loss: 0.782737  [   64/  130]
train() client id: f_00008-0-2 loss: 0.703410  [   96/  130]
train() client id: f_00008-0-3 loss: 0.597883  [  128/  130]
train() client id: f_00008-1-0 loss: 0.676004  [   32/  130]
train() client id: f_00008-1-1 loss: 0.789747  [   64/  130]
train() client id: f_00008-1-2 loss: 0.677661  [   96/  130]
train() client id: f_00008-1-3 loss: 0.754080  [  128/  130]
train() client id: f_00008-2-0 loss: 0.719342  [   32/  130]
train() client id: f_00008-2-1 loss: 0.728265  [   64/  130]
train() client id: f_00008-2-2 loss: 0.669626  [   96/  130]
train() client id: f_00008-2-3 loss: 0.789348  [  128/  130]
train() client id: f_00008-3-0 loss: 0.686906  [   32/  130]
train() client id: f_00008-3-1 loss: 0.797837  [   64/  130]
train() client id: f_00008-3-2 loss: 0.645214  [   96/  130]
train() client id: f_00008-3-3 loss: 0.751432  [  128/  130]
train() client id: f_00008-4-0 loss: 0.680330  [   32/  130]
train() client id: f_00008-4-1 loss: 0.819855  [   64/  130]
train() client id: f_00008-4-2 loss: 0.730292  [   96/  130]
train() client id: f_00008-4-3 loss: 0.629576  [  128/  130]
train() client id: f_00009-0-0 loss: 0.636861  [   32/  118]
train() client id: f_00009-0-1 loss: 0.771645  [   64/  118]
train() client id: f_00009-0-2 loss: 0.762167  [   96/  118]
train() client id: f_00009-1-0 loss: 0.698983  [   32/  118]
train() client id: f_00009-1-1 loss: 0.776240  [   64/  118]
train() client id: f_00009-1-2 loss: 0.717658  [   96/  118]
train() client id: f_00009-2-0 loss: 0.610746  [   32/  118]
train() client id: f_00009-2-1 loss: 0.827584  [   64/  118]
train() client id: f_00009-2-2 loss: 0.740577  [   96/  118]
train() client id: f_00009-3-0 loss: 0.749203  [   32/  118]
train() client id: f_00009-3-1 loss: 0.762480  [   64/  118]
train() client id: f_00009-3-2 loss: 0.657343  [   96/  118]
train() client id: f_00009-4-0 loss: 0.700103  [   32/  118]
train() client id: f_00009-4-1 loss: 0.718112  [   64/  118]
train() client id: f_00009-4-2 loss: 0.637343  [   96/  118]
At round 74 accuracy: 0.649867374005305
At round 74 training accuracy: 0.5949027498323273
At round 74 training loss: 0.819276447482732
update_location
xs = [  -3.9056584     4.20031788  390.00902392   18.81129433    0.97929623
    3.95640986 -352.44319194 -331.32485185  374.66397685 -317.06087855]
ys = [ 382.5879595   365.55583871    1.32061395 -352.45517586  344.35018685
  327.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [395.46023937 379.01017649 402.62734974 366.84944567 358.57776033
 342.75030358 366.36470093 346.08789882 388.17749229 332.48099579]
dists_bs = [269.00935793 261.58035399 591.10043668 561.93574397 244.22980671
 235.32437464 251.07114812 234.04126509 571.76481643 222.31011324]
uav_gains = [7.52650140e-13 8.76404134e-13 7.08086688e-13 9.93683329e-13
 1.09055942e-12 1.32968634e-12 9.98935416e-13 1.27217390e-12
 8.03318596e-13 1.53701405e-12]
bs_gains = [1.73758236e-11 1.87931569e-11 1.91709000e-12 2.20887654e-12
 2.27749704e-11 2.52712502e-11 2.10796318e-11 2.56610996e-11
 2.10419163e-12 2.96352278e-11]
Round 75
-------------------------------
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.03922851 2.03415137 1.02725189 0.39661097 2.34431442 1.12928775
 0.47989662 1.41999007 1.0345605  0.91598534]
obj_prev = 11.821277437641127
eta_min = 2.632614097881737e-91	eta_max = 0.9674950324086027
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 2.6763069615753383	eta = 0.9090909090909091
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 8.03601399311375	eta = 0.3027628287842397
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 4.8844582536009025	eta = 0.4981118073659859
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 4.372075946270912	eta = 0.5564876636646818
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 4.3396007499684774	eta = 0.5606521126909028
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 4.3394472210647415	eta = 0.5606719484671788
eta = 0.5606719484671788
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [0.04774628 0.10041876 0.0469884  0.01629436 0.11595525 0.05532505
 0.02046269 0.06783003 0.04926203 0.04471477]
ene_total = [0.45740881 0.62055196 0.46033181 0.25063986 0.70629813 0.3612359
 0.2738053  0.5227749  0.38706651 0.29933403]
ti_comp = [4.2992849  4.53835717 4.28655232 4.34933896 4.54253515 4.54464388
 4.35017711 4.38486951 4.43846938 4.54768586]
ti_coms = [0.32903639 0.08996413 0.34176898 0.27898233 0.08578614 0.08367741
 0.27814418 0.24345178 0.18985191 0.08063543]
t_total = [26.24968529 26.24968529 26.24968529 26.24968529 26.24968529 26.24968529
 26.24968529 26.24968529 26.24968529 26.24968529]
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [3.68049423e-07 3.07275006e-06 3.52887078e-07 1.42937379e-08
 4.72231164e-06 5.12443289e-07 2.82979099e-08 1.01445480e-06
 3.79271804e-07 2.70179295e-07]
ene_total = [0.18661622 0.05104089 0.19383747 0.15822588 0.04868069 0.04746084
 0.1577506  0.13808028 0.10767732 0.0457342 ]
optimize_network iter = 0 obj = 1.1351043989055445
eta = 0.5606719484671788
freqs = [ 5552816.27091243 11063338.14405196  5480908.36118572  1873200.15342277
 12763274.63448044  6086841.3557449   2351937.65383736  7734555.17253756
  5549439.33843933  4916211.22498621]
eta_min = 0.5606719484671795	eta_max = 0.8680569114774389
af = 4.622705191075305e-05	bf = 0.4986564559927847	zeta = 5.084975710182836e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [6.06251059e-08 5.06143430e-07 5.81275641e-08 2.35446469e-09
 7.77859235e-07 8.44096653e-08 4.66123208e-09 1.67101009e-07
 6.24736566e-08 4.45039370e-08]
ene_total = [0.90183473 0.24659032 0.93673252 0.76464342 0.23514662 0.22934794
 0.76234626 0.66726472 0.52035367 0.22100929]
ti_comp = [1.06098413 1.30005639 1.04825154 1.11103819 1.30423438 1.30634311
 1.11187634 1.14656874 1.20016861 1.30938509]
ti_coms = [0.32903639 0.08996413 0.34176898 0.27898233 0.08578614 0.08367741
 0.27814418 0.24345178 0.18985191 0.08063543]
t_total = [26.24968529 26.24968529 26.24968529 26.24968529 26.24968529 26.24968529
 26.24968529 26.24968529 26.24968529 26.24968529]
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [8.83797899e-08 5.47611225e-07 8.62964911e-08 3.20336552e-09
 8.37745486e-07 9.06991369e-08 6.33471974e-09 2.16979151e-07
 7.58585669e-08 4.76617893e-08]
ene_total = [0.62136672 0.16990208 0.64541139 0.52684109 0.16201769 0.15802138
 0.52525835 0.45974786 0.35852513 0.15227597]
optimize_network iter = 1 obj = 3.7793676563367
eta = 0.8680569114774389
freqs = [ 5502474.32221998  9444523.43544286  5480908.36118572  1793229.33817215
 10870816.12174629  5178356.71698614  2250264.85743831  7233513.5913764
  5018771.87691086  4175523.98380345]
eta_min = 0.8680569114774714	eta_max = 0.8680569114774374
af = 3.523192231617816e-05	bf = 0.4986564559927847	zeta = 3.8755114547795976e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [5.95308319e-08 3.68859801e-07 5.81275641e-08 2.15772197e-09
 5.64288349e-07 6.10930969e-08 4.26693859e-09 1.46152750e-07
 5.10967904e-08 3.21040135e-08]
ene_total = [0.9018347  0.24658655 0.93673252 0.76464342 0.23514077 0.2293473
 0.76234625 0.66726414 0.52035336 0.22100895]
ti_comp = [1.06098413 1.30005639 1.04825154 1.11103819 1.30423438 1.30634311
 1.11187634 1.14656874 1.20016861 1.30938509]
ti_coms = [0.32903639 0.08996413 0.34176898 0.27898233 0.08578614 0.08367741
 0.27814418 0.24345178 0.18985191 0.08063543]
t_total = [26.24968529 26.24968529 26.24968529 26.24968529 26.24968529 26.24968529
 26.24968529 26.24968529 26.24968529 26.24968529]
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [8.83797899e-08 5.47611225e-07 8.62964911e-08 3.20336552e-09
 8.37745486e-07 9.06991369e-08 6.33471974e-09 2.16979151e-07
 7.58585669e-08 4.76617893e-08]
ene_total = [0.62136672 0.16990208 0.64541139 0.52684109 0.16201769 0.15802138
 0.52525835 0.45974786 0.35852513 0.15227597]
optimize_network iter = 2 obj = 3.779367656336656
eta = 0.8680569114774374
freqs = [ 5502474.32221997  9444523.43544286  5480908.3611857   1793229.33817215
 10870816.12174629  5178356.71698613  2250264.85743831  7233513.59137638
  5018771.87691085  4175523.98380344]
Done!
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [7.62986036e-08 4.72754821e-07 7.45000840e-08 2.76547745e-09
 7.23228816e-07 7.83009046e-08 5.46878728e-09 1.87318914e-07
 6.54889850e-08 4.11466013e-08]
ene_total = [0.03290372 0.00899689 0.03417697 0.02789824 0.00857934 0.00836782
 0.02781442 0.02434537 0.01898526 0.00806358]
At round 75 energy consumption: 0.20013159600162664
At round 75 eta: 0.8680569114774374
At round 75 a_n: 2.1491184865862607
At round 75 local rounds: 4.633363424333758
At round 75 global rounds: 18.884386908447258
gradient difference: 0.7391544580459595
train() client id: f_00000-0-0 loss: 1.245679  [   32/  126]
train() client id: f_00000-0-1 loss: 0.998994  [   64/  126]
train() client id: f_00000-0-2 loss: 1.079807  [   96/  126]
train() client id: f_00000-1-0 loss: 1.050526  [   32/  126]
train() client id: f_00000-1-1 loss: 1.089096  [   64/  126]
train() client id: f_00000-1-2 loss: 1.110234  [   96/  126]
train() client id: f_00000-2-0 loss: 0.894041  [   32/  126]
train() client id: f_00000-2-1 loss: 1.079200  [   64/  126]
train() client id: f_00000-2-2 loss: 0.976957  [   96/  126]
train() client id: f_00000-3-0 loss: 1.023174  [   32/  126]
train() client id: f_00000-3-1 loss: 1.009841  [   64/  126]
train() client id: f_00000-3-2 loss: 0.981148  [   96/  126]
train() client id: f_00001-0-0 loss: 0.439692  [   32/  265]
train() client id: f_00001-0-1 loss: 0.454805  [   64/  265]
train() client id: f_00001-0-2 loss: 0.342656  [   96/  265]
train() client id: f_00001-0-3 loss: 0.412427  [  128/  265]
train() client id: f_00001-0-4 loss: 0.453561  [  160/  265]
train() client id: f_00001-0-5 loss: 0.511244  [  192/  265]
train() client id: f_00001-0-6 loss: 0.468940  [  224/  265]
train() client id: f_00001-0-7 loss: 0.441765  [  256/  265]
train() client id: f_00001-1-0 loss: 0.518181  [   32/  265]
train() client id: f_00001-1-1 loss: 0.466865  [   64/  265]
train() client id: f_00001-1-2 loss: 0.355679  [   96/  265]
train() client id: f_00001-1-3 loss: 0.649439  [  128/  265]
train() client id: f_00001-1-4 loss: 0.367869  [  160/  265]
train() client id: f_00001-1-5 loss: 0.393782  [  192/  265]
train() client id: f_00001-1-6 loss: 0.442946  [  224/  265]
train() client id: f_00001-1-7 loss: 0.350886  [  256/  265]
train() client id: f_00001-2-0 loss: 0.345068  [   32/  265]
train() client id: f_00001-2-1 loss: 0.370288  [   64/  265]
train() client id: f_00001-2-2 loss: 0.331682  [   96/  265]
train() client id: f_00001-2-3 loss: 0.391295  [  128/  265]
train() client id: f_00001-2-4 loss: 0.618326  [  160/  265]
train() client id: f_00001-2-5 loss: 0.449705  [  192/  265]
train() client id: f_00001-2-6 loss: 0.427453  [  224/  265]
train() client id: f_00001-2-7 loss: 0.577244  [  256/  265]
train() client id: f_00001-3-0 loss: 0.434824  [   32/  265]
train() client id: f_00001-3-1 loss: 0.450488  [   64/  265]
train() client id: f_00001-3-2 loss: 0.456402  [   96/  265]
train() client id: f_00001-3-3 loss: 0.454480  [  128/  265]
train() client id: f_00001-3-4 loss: 0.360675  [  160/  265]
train() client id: f_00001-3-5 loss: 0.495574  [  192/  265]
train() client id: f_00001-3-6 loss: 0.484058  [  224/  265]
train() client id: f_00001-3-7 loss: 0.364472  [  256/  265]
train() client id: f_00002-0-0 loss: 0.937078  [   32/  124]
train() client id: f_00002-0-1 loss: 1.176593  [   64/  124]
train() client id: f_00002-0-2 loss: 1.102647  [   96/  124]
train() client id: f_00002-1-0 loss: 1.022435  [   32/  124]
train() client id: f_00002-1-1 loss: 1.027883  [   64/  124]
train() client id: f_00002-1-2 loss: 0.974632  [   96/  124]
train() client id: f_00002-2-0 loss: 1.064607  [   32/  124]
train() client id: f_00002-2-1 loss: 0.882648  [   64/  124]
train() client id: f_00002-2-2 loss: 0.837302  [   96/  124]
train() client id: f_00002-3-0 loss: 0.866490  [   32/  124]
train() client id: f_00002-3-1 loss: 1.045193  [   64/  124]
train() client id: f_00002-3-2 loss: 1.084352  [   96/  124]
train() client id: f_00003-0-0 loss: 0.579030  [   32/   43]
train() client id: f_00003-1-0 loss: 0.668978  [   32/   43]
train() client id: f_00003-2-0 loss: 0.604852  [   32/   43]
train() client id: f_00003-3-0 loss: 0.579168  [   32/   43]
train() client id: f_00004-0-0 loss: 0.739000  [   32/  306]
train() client id: f_00004-0-1 loss: 0.675349  [   64/  306]
train() client id: f_00004-0-2 loss: 0.860685  [   96/  306]
train() client id: f_00004-0-3 loss: 0.693990  [  128/  306]
train() client id: f_00004-0-4 loss: 0.828271  [  160/  306]
train() client id: f_00004-0-5 loss: 0.843719  [  192/  306]
train() client id: f_00004-0-6 loss: 0.737072  [  224/  306]
train() client id: f_00004-0-7 loss: 0.691432  [  256/  306]
train() client id: f_00004-0-8 loss: 0.922230  [  288/  306]
train() client id: f_00004-1-0 loss: 0.631123  [   32/  306]
train() client id: f_00004-1-1 loss: 0.786750  [   64/  306]
train() client id: f_00004-1-2 loss: 0.838899  [   96/  306]
train() client id: f_00004-1-3 loss: 0.753136  [  128/  306]
train() client id: f_00004-1-4 loss: 0.809939  [  160/  306]
train() client id: f_00004-1-5 loss: 0.895558  [  192/  306]
train() client id: f_00004-1-6 loss: 0.733183  [  224/  306]
train() client id: f_00004-1-7 loss: 0.887491  [  256/  306]
train() client id: f_00004-1-8 loss: 0.726934  [  288/  306]
train() client id: f_00004-2-0 loss: 0.716814  [   32/  306]
train() client id: f_00004-2-1 loss: 0.838394  [   64/  306]
train() client id: f_00004-2-2 loss: 0.878101  [   96/  306]
train() client id: f_00004-2-3 loss: 0.568181  [  128/  306]
train() client id: f_00004-2-4 loss: 0.710885  [  160/  306]
train() client id: f_00004-2-5 loss: 0.832091  [  192/  306]
train() client id: f_00004-2-6 loss: 0.721053  [  224/  306]
train() client id: f_00004-2-7 loss: 0.843122  [  256/  306]
train() client id: f_00004-2-8 loss: 0.818450  [  288/  306]
train() client id: f_00004-3-0 loss: 0.945878  [   32/  306]
train() client id: f_00004-3-1 loss: 0.866997  [   64/  306]
train() client id: f_00004-3-2 loss: 0.734409  [   96/  306]
train() client id: f_00004-3-3 loss: 0.865200  [  128/  306]
train() client id: f_00004-3-4 loss: 0.688294  [  160/  306]
train() client id: f_00004-3-5 loss: 0.640897  [  192/  306]
train() client id: f_00004-3-6 loss: 0.684545  [  224/  306]
train() client id: f_00004-3-7 loss: 0.853797  [  256/  306]
train() client id: f_00004-3-8 loss: 0.688049  [  288/  306]
train() client id: f_00005-0-0 loss: 0.181454  [   32/  146]
train() client id: f_00005-0-1 loss: 0.387970  [   64/  146]
train() client id: f_00005-0-2 loss: 0.530355  [   96/  146]
train() client id: f_00005-0-3 loss: 0.249337  [  128/  146]
train() client id: f_00005-1-0 loss: 0.189713  [   32/  146]
train() client id: f_00005-1-1 loss: 0.591459  [   64/  146]
train() client id: f_00005-1-2 loss: 0.525737  [   96/  146]
train() client id: f_00005-1-3 loss: 0.389446  [  128/  146]
train() client id: f_00005-2-0 loss: 0.459949  [   32/  146]
train() client id: f_00005-2-1 loss: 0.288013  [   64/  146]
train() client id: f_00005-2-2 loss: 0.397743  [   96/  146]
train() client id: f_00005-2-3 loss: 0.383423  [  128/  146]
train() client id: f_00005-3-0 loss: 0.083542  [   32/  146]
train() client id: f_00005-3-1 loss: 0.629046  [   64/  146]
train() client id: f_00005-3-2 loss: 0.655854  [   96/  146]
train() client id: f_00005-3-3 loss: 0.344940  [  128/  146]
train() client id: f_00006-0-0 loss: 0.510682  [   32/   54]
train() client id: f_00006-1-0 loss: 0.549285  [   32/   54]
train() client id: f_00006-2-0 loss: 0.498281  [   32/   54]
train() client id: f_00006-3-0 loss: 0.518501  [   32/   54]
train() client id: f_00007-0-0 loss: 0.622023  [   32/  179]
train() client id: f_00007-0-1 loss: 0.515803  [   64/  179]
train() client id: f_00007-0-2 loss: 0.534949  [   96/  179]
train() client id: f_00007-0-3 loss: 0.450378  [  128/  179]
train() client id: f_00007-0-4 loss: 0.703253  [  160/  179]
train() client id: f_00007-1-0 loss: 0.390830  [   32/  179]
train() client id: f_00007-1-1 loss: 0.559715  [   64/  179]
train() client id: f_00007-1-2 loss: 0.597514  [   96/  179]
train() client id: f_00007-1-3 loss: 0.648246  [  128/  179]
train() client id: f_00007-1-4 loss: 0.622909  [  160/  179]
train() client id: f_00007-2-0 loss: 0.511152  [   32/  179]
train() client id: f_00007-2-1 loss: 0.704806  [   64/  179]
train() client id: f_00007-2-2 loss: 0.482624  [   96/  179]
train() client id: f_00007-2-3 loss: 0.536997  [  128/  179]
train() client id: f_00007-2-4 loss: 0.545609  [  160/  179]
train() client id: f_00007-3-0 loss: 0.452331  [   32/  179]
train() client id: f_00007-3-1 loss: 0.556388  [   64/  179]
train() client id: f_00007-3-2 loss: 0.523715  [   96/  179]
train() client id: f_00007-3-3 loss: 0.558024  [  128/  179]
train() client id: f_00007-3-4 loss: 0.479038  [  160/  179]
train() client id: f_00008-0-0 loss: 0.904179  [   32/  130]
train() client id: f_00008-0-1 loss: 0.802280  [   64/  130]
train() client id: f_00008-0-2 loss: 0.758487  [   96/  130]
train() client id: f_00008-0-3 loss: 0.800354  [  128/  130]
train() client id: f_00008-1-0 loss: 0.786904  [   32/  130]
train() client id: f_00008-1-1 loss: 0.891621  [   64/  130]
train() client id: f_00008-1-2 loss: 0.779896  [   96/  130]
train() client id: f_00008-1-3 loss: 0.821717  [  128/  130]
train() client id: f_00008-2-0 loss: 1.033387  [   32/  130]
train() client id: f_00008-2-1 loss: 0.738348  [   64/  130]
train() client id: f_00008-2-2 loss: 0.745205  [   96/  130]
train() client id: f_00008-2-3 loss: 0.797610  [  128/  130]
train() client id: f_00008-3-0 loss: 0.862114  [   32/  130]
train() client id: f_00008-3-1 loss: 0.698157  [   64/  130]
train() client id: f_00008-3-2 loss: 0.999652  [   96/  130]
train() client id: f_00008-3-3 loss: 0.743883  [  128/  130]
train() client id: f_00009-0-0 loss: 0.878604  [   32/  118]
train() client id: f_00009-0-1 loss: 0.906192  [   64/  118]
train() client id: f_00009-0-2 loss: 0.927696  [   96/  118]
train() client id: f_00009-1-0 loss: 1.021211  [   32/  118]
train() client id: f_00009-1-1 loss: 0.894527  [   64/  118]
train() client id: f_00009-1-2 loss: 0.835956  [   96/  118]
train() client id: f_00009-2-0 loss: 1.076897  [   32/  118]
train() client id: f_00009-2-1 loss: 0.812738  [   64/  118]
train() client id: f_00009-2-2 loss: 0.860506  [   96/  118]
train() client id: f_00009-3-0 loss: 0.784887  [   32/  118]
train() client id: f_00009-3-1 loss: 1.021762  [   64/  118]
train() client id: f_00009-3-2 loss: 0.923789  [   96/  118]
At round 75 accuracy: 0.649867374005305
At round 75 training accuracy: 0.5902079141515761
At round 75 training loss: 0.819747729147244
update_location
xs = [  -3.9056584     4.20031788  395.00902392   18.81129433    0.97929623
    3.95640986 -357.44319194 -336.32485185  379.66397685 -322.06087855]
ys = [ 387.5879595   370.55583871    1.32061395 -357.45517586  349.35018685
  332.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [400.29948853 383.83495447 407.47254263 371.65584557 363.38204698
 347.53548324 371.17721644 350.87758866 393.00560465 337.25245937]
dists_bs = [272.88626614 265.24486796 595.88154568 566.63156646 247.72303154
 238.60155659 254.62944319 237.41011412 576.57319143 225.53801284]
uav_gains = [7.22036186e-13 8.36522048e-13 6.80532523e-13 9.44154307e-13
 1.03236608e-12 1.24853056e-12 9.48887642e-13 1.19672866e-12
 7.69075581e-13 1.43449114e-12]
bs_gains = [1.66934225e-11 1.80751751e-11 1.87433090e-12 2.15800250e-12
 2.18870991e-11 2.43113414e-11 2.02651559e-11 2.46545045e-11
 2.05542502e-12 2.84628764e-11]
Round 76
-------------------------------
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.89822293 1.75469717 0.88790864 0.34389424 2.02220671 0.97420485
 0.41573163 1.22653712 0.89277918 0.79022156]
obj_prev = 10.206404031521155
eta_min = 1.1840901563596855e-105	eta_max = 0.9712061927430241
af = 2.098524592010151	bf = 0.440277105517973	zeta = 2.308377051211166	eta = 0.9090909090909091
af = 2.098524592010151	bf = 0.440277105517973	zeta = 7.043166478768837	eta = 0.29795186559000353
af = 2.098524592010151	bf = 0.440277105517973	zeta = 4.246460455912228	eta = 0.49418206381468455
af = 2.098524592010151	bf = 0.440277105517973	zeta = 3.794695110359663	eta = 0.5530153361415251
af = 2.098524592010151	bf = 0.440277105517973	zeta = 3.766078122653466	eta = 0.557217488237231
af = 2.098524592010151	bf = 0.440277105517973	zeta = 3.765942672970741	eta = 0.5572375296819754
eta = 0.5572375296819754
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [0.0482533  0.10148511 0.04748737 0.0164674  0.11718658 0.05591255
 0.02067998 0.06855032 0.04978515 0.0451896 ]
ene_total = [0.39809468 0.53669908 0.4005926  0.21938841 0.6108545  0.31238457
 0.23943255 0.45484532 0.3348021  0.25884885]
ti_comp = [5.06004778 5.30681242 5.04724229 5.11036031 5.31105174 5.31322071
 5.11119117 5.14610005 5.20576198 5.31628572]
ti_coms = [0.33762368 0.09085904 0.35042917 0.28731115 0.08661972 0.08445075
 0.2864803  0.25157141 0.19190948 0.08138574]
t_total = [26.19968109 26.19968109 26.19968109 26.19968109 26.19968109 26.19968109
 26.19968109 26.19968109 26.19968109 26.19968109]
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [2.74253247e-07 2.31963433e-06 2.62727860e-07 1.06868990e-08
 3.56576398e-06 3.86983308e-07 2.11585747e-08 7.60242285e-07
 2.84584225e-07 2.04069817e-07]
ene_total = [0.16388001 0.0441132  0.1700956  0.13945761 0.04206153 0.04099331
 0.13905437 0.12211358 0.09315205 0.0395047 ]
optimize_network iter = 0 obj = 0.9944259543834252
eta = 0.5572375296819754
freqs = [ 4768067.39444     9561776.65897993  4704288.96039803  1611177.51564226
 11032332.74977597  5261643.95832219  2023010.29856296  6660414.59689484
  4781734.99264716  4250109.82064523]
eta_min = 0.5572375296819757	eta_max = 0.8837926289249317
af = 2.9732682841315033e-05	bf = 0.440277105517973	zeta = 3.2705951125446536e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [4.47003242e-08 3.78075401e-07 4.28218103e-08 1.74184938e-09
 5.81181107e-07 6.30741094e-08 3.44861970e-09 1.23911301e-07
 4.63841622e-08 3.32611812e-08]
ene_total = [0.79815367 0.21480247 0.82842621 0.67921232 0.20478538 0.19964562
 0.67724819 0.59472538 0.45368094 0.19239915]
ti_comp = [1.07904836 1.325813   1.06624286 1.12936088 1.33005232 1.33222129
 1.13019174 1.16510062 1.22476255 1.33528629]
ti_coms = [0.33762368 0.09085904 0.35042917 0.28731115 0.08661972 0.08445075
 0.2864803  0.25157141 0.19190948 0.08138574]
t_total = [26.19968109 26.19968109 26.19968109 26.19968109 26.19968109 26.19968109
 26.19968109 26.19968109 26.19968109 26.19968109]
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [5.68571582e-08 3.50371208e-07 5.55019335e-08 2.06298094e-09
 5.36021441e-07 5.80311479e-08 4.07973874e-09 1.39825878e-07
 4.84709564e-08 3.04966807e-08]
ene_total = [0.62439628 0.16803958 0.64807852 0.53134818 0.16020288 0.15618279
 0.52981165 0.46525429 0.35491489 0.15051391]
optimize_network iter = 1 obj = 3.7887429688072136
eta = 0.8837926289249317
freqs = [4723436.47190019 8085223.78253195 4704288.96039802 1540154.03017325
 9306387.77331266 4433073.47420022 1932725.03801711 6214669.51457857
 4293583.32001691 3574670.85257318]
eta_min = 0.8837926289249345	eta_max = 0.8837926289249313
af = 2.229825334963228e-05	bf = 0.440277105517973	zeta = 2.452807868459551e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [4.38674166e-08 2.70324445e-07 4.28218103e-08 1.59166668e-09
 4.13560520e-07 4.47731935e-08 3.14767049e-09 1.07880876e-07
 3.73971494e-08 2.35293258e-08]
ene_total = [0.79815365 0.21479992 0.82842621 0.67921232 0.20478142 0.19964519
 0.67724818 0.594725   0.45368072 0.19239892]
ti_comp = [1.07904836 1.325813   1.06624286 1.12936088 1.33005232 1.33222129
 1.13019174 1.16510062 1.22476255 1.33528629]
ti_coms = [0.33762368 0.09085904 0.35042917 0.28731115 0.08661972 0.08445075
 0.2864803  0.25157141 0.19190948 0.08138574]
t_total = [26.19968109 26.19968109 26.19968109 26.19968109 26.19968109 26.19968109
 26.19968109 26.19968109 26.19968109 26.19968109]
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [5.68571582e-08 3.50371208e-07 5.55019335e-08 2.06298094e-09
 5.36021441e-07 5.80311479e-08 4.07973874e-09 1.39825878e-07
 4.84709564e-08 3.04966807e-08]
ene_total = [0.62439628 0.16803958 0.64807852 0.53134818 0.16020288 0.15618279
 0.52981165 0.46525429 0.35491489 0.15051391]
optimize_network iter = 2 obj = 3.788742968807199
eta = 0.8837926289249313
freqs = [4723436.47190018 8085223.78253194 4704288.96039801 1540154.03017325
 9306387.77331265 4433073.47420021 1932725.03801711 6214669.51457855
 4293583.3200169  3574670.85257317]
Done!
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [5.62233473e-08 3.46465471e-07 5.48832299e-08 2.03998402e-09
 5.30046183e-07 5.73842500e-08 4.03426016e-09 1.38267179e-07
 4.79306301e-08 3.01567212e-08]
ene_total = [0.03376242 0.00908625 0.03504297 0.02873112 0.0086625  0.00844513
 0.02864803 0.02515728 0.019191   0.0081386 ]
At round 76 energy consumption: 0.20486531204581118
At round 76 eta: 0.8837926289249313
At round 76 a_n: 1.8065726396169453
At round 76 local rounds: 4.045092362532633
At round 76 global rounds: 18.49382243745926
gradient difference: 0.8905469179153442
train() client id: f_00000-0-0 loss: 1.067473  [   32/  126]
train() client id: f_00000-0-1 loss: 1.087952  [   64/  126]
train() client id: f_00000-0-2 loss: 0.788007  [   96/  126]
train() client id: f_00000-1-0 loss: 0.960559  [   32/  126]
train() client id: f_00000-1-1 loss: 0.913613  [   64/  126]
train() client id: f_00000-1-2 loss: 1.049241  [   96/  126]
train() client id: f_00000-2-0 loss: 1.037597  [   32/  126]
train() client id: f_00000-2-1 loss: 0.888510  [   64/  126]
train() client id: f_00000-2-2 loss: 1.004923  [   96/  126]
train() client id: f_00000-3-0 loss: 0.823267  [   32/  126]
train() client id: f_00000-3-1 loss: 0.942209  [   64/  126]
train() client id: f_00000-3-2 loss: 1.084640  [   96/  126]
train() client id: f_00001-0-0 loss: 0.593655  [   32/  265]
train() client id: f_00001-0-1 loss: 0.563471  [   64/  265]
train() client id: f_00001-0-2 loss: 0.544069  [   96/  265]
train() client id: f_00001-0-3 loss: 0.678794  [  128/  265]
train() client id: f_00001-0-4 loss: 0.850169  [  160/  265]
train() client id: f_00001-0-5 loss: 0.574849  [  192/  265]
train() client id: f_00001-0-6 loss: 0.495474  [  224/  265]
train() client id: f_00001-0-7 loss: 0.533331  [  256/  265]
train() client id: f_00001-1-0 loss: 0.518553  [   32/  265]
train() client id: f_00001-1-1 loss: 0.622552  [   64/  265]
train() client id: f_00001-1-2 loss: 0.631722  [   96/  265]
train() client id: f_00001-1-3 loss: 0.518985  [  128/  265]
train() client id: f_00001-1-4 loss: 0.654709  [  160/  265]
train() client id: f_00001-1-5 loss: 0.629472  [  192/  265]
train() client id: f_00001-1-6 loss: 0.599689  [  224/  265]
train() client id: f_00001-1-7 loss: 0.677931  [  256/  265]
train() client id: f_00001-2-0 loss: 0.503141  [   32/  265]
train() client id: f_00001-2-1 loss: 0.672423  [   64/  265]
train() client id: f_00001-2-2 loss: 0.569161  [   96/  265]
train() client id: f_00001-2-3 loss: 0.702479  [  128/  265]
train() client id: f_00001-2-4 loss: 0.600881  [  160/  265]
train() client id: f_00001-2-5 loss: 0.555867  [  192/  265]
train() client id: f_00001-2-6 loss: 0.596440  [  224/  265]
train() client id: f_00001-2-7 loss: 0.598780  [  256/  265]
train() client id: f_00001-3-0 loss: 0.597697  [   32/  265]
train() client id: f_00001-3-1 loss: 0.529165  [   64/  265]
train() client id: f_00001-3-2 loss: 0.588523  [   96/  265]
train() client id: f_00001-3-3 loss: 0.587978  [  128/  265]
train() client id: f_00001-3-4 loss: 0.576295  [  160/  265]
train() client id: f_00001-3-5 loss: 0.759976  [  192/  265]
train() client id: f_00001-3-6 loss: 0.498035  [  224/  265]
train() client id: f_00001-3-7 loss: 0.737938  [  256/  265]
train() client id: f_00002-0-0 loss: 0.967780  [   32/  124]
train() client id: f_00002-0-1 loss: 0.786192  [   64/  124]
train() client id: f_00002-0-2 loss: 0.587801  [   96/  124]
train() client id: f_00002-1-0 loss: 0.730576  [   32/  124]
train() client id: f_00002-1-1 loss: 0.640798  [   64/  124]
train() client id: f_00002-1-2 loss: 0.945893  [   96/  124]
train() client id: f_00002-2-0 loss: 0.941542  [   32/  124]
train() client id: f_00002-2-1 loss: 0.616319  [   64/  124]
train() client id: f_00002-2-2 loss: 0.639483  [   96/  124]
train() client id: f_00002-3-0 loss: 0.711536  [   32/  124]
train() client id: f_00002-3-1 loss: 0.803475  [   64/  124]
train() client id: f_00002-3-2 loss: 0.796708  [   96/  124]
train() client id: f_00003-0-0 loss: 0.590773  [   32/   43]
train() client id: f_00003-1-0 loss: 0.580069  [   32/   43]
train() client id: f_00003-2-0 loss: 0.809042  [   32/   43]
train() client id: f_00003-3-0 loss: 0.431253  [   32/   43]
train() client id: f_00004-0-0 loss: 0.998668  [   32/  306]
train() client id: f_00004-0-1 loss: 0.829304  [   64/  306]
train() client id: f_00004-0-2 loss: 0.965743  [   96/  306]
train() client id: f_00004-0-3 loss: 0.749845  [  128/  306]
train() client id: f_00004-0-4 loss: 1.084428  [  160/  306]
train() client id: f_00004-0-5 loss: 1.014443  [  192/  306]
train() client id: f_00004-0-6 loss: 0.966928  [  224/  306]
train() client id: f_00004-0-7 loss: 0.897489  [  256/  306]
train() client id: f_00004-0-8 loss: 0.844572  [  288/  306]
train() client id: f_00004-1-0 loss: 1.033695  [   32/  306]
train() client id: f_00004-1-1 loss: 1.012577  [   64/  306]
train() client id: f_00004-1-2 loss: 0.860962  [   96/  306]
train() client id: f_00004-1-3 loss: 0.802940  [  128/  306]
train() client id: f_00004-1-4 loss: 0.975363  [  160/  306]
train() client id: f_00004-1-5 loss: 0.943209  [  192/  306]
train() client id: f_00004-1-6 loss: 0.903534  [  224/  306]
train() client id: f_00004-1-7 loss: 0.933600  [  256/  306]
train() client id: f_00004-1-8 loss: 0.980758  [  288/  306]
train() client id: f_00004-2-0 loss: 0.953469  [   32/  306]
train() client id: f_00004-2-1 loss: 0.807300  [   64/  306]
train() client id: f_00004-2-2 loss: 0.955586  [   96/  306]
train() client id: f_00004-2-3 loss: 1.044086  [  128/  306]
train() client id: f_00004-2-4 loss: 0.978294  [  160/  306]
train() client id: f_00004-2-5 loss: 0.978498  [  192/  306]
train() client id: f_00004-2-6 loss: 0.908973  [  224/  306]
train() client id: f_00004-2-7 loss: 0.919121  [  256/  306]
train() client id: f_00004-2-8 loss: 0.798634  [  288/  306]
train() client id: f_00004-3-0 loss: 0.954609  [   32/  306]
train() client id: f_00004-3-1 loss: 0.871861  [   64/  306]
train() client id: f_00004-3-2 loss: 0.926610  [   96/  306]
train() client id: f_00004-3-3 loss: 1.038189  [  128/  306]
train() client id: f_00004-3-4 loss: 1.020635  [  160/  306]
train() client id: f_00004-3-5 loss: 0.928828  [  192/  306]
train() client id: f_00004-3-6 loss: 0.690229  [  224/  306]
train() client id: f_00004-3-7 loss: 1.018169  [  256/  306]
train() client id: f_00004-3-8 loss: 0.866387  [  288/  306]
train() client id: f_00005-0-0 loss: 0.728890  [   32/  146]
train() client id: f_00005-0-1 loss: 0.656544  [   64/  146]
train() client id: f_00005-0-2 loss: 0.428036  [   96/  146]
train() client id: f_00005-0-3 loss: 0.726212  [  128/  146]
train() client id: f_00005-1-0 loss: 0.492721  [   32/  146]
train() client id: f_00005-1-1 loss: 0.651568  [   64/  146]
train() client id: f_00005-1-2 loss: 0.873531  [   96/  146]
train() client id: f_00005-1-3 loss: 0.473409  [  128/  146]
train() client id: f_00005-2-0 loss: 0.957874  [   32/  146]
train() client id: f_00005-2-1 loss: 0.477489  [   64/  146]
train() client id: f_00005-2-2 loss: 0.482047  [   96/  146]
train() client id: f_00005-2-3 loss: 0.403536  [  128/  146]
train() client id: f_00005-3-0 loss: 0.627648  [   32/  146]
train() client id: f_00005-3-1 loss: 0.911426  [   64/  146]
train() client id: f_00005-3-2 loss: 0.527051  [   96/  146]
train() client id: f_00005-3-3 loss: 0.520107  [  128/  146]
train() client id: f_00006-0-0 loss: 0.430803  [   32/   54]
train() client id: f_00006-1-0 loss: 0.427662  [   32/   54]
train() client id: f_00006-2-0 loss: 0.456591  [   32/   54]
train() client id: f_00006-3-0 loss: 0.476115  [   32/   54]
train() client id: f_00007-0-0 loss: 0.649465  [   32/  179]
train() client id: f_00007-0-1 loss: 0.683178  [   64/  179]
train() client id: f_00007-0-2 loss: 0.948871  [   96/  179]
train() client id: f_00007-0-3 loss: 0.659834  [  128/  179]
train() client id: f_00007-0-4 loss: 0.693885  [  160/  179]
train() client id: f_00007-1-0 loss: 0.555548  [   32/  179]
train() client id: f_00007-1-1 loss: 0.871618  [   64/  179]
train() client id: f_00007-1-2 loss: 0.586788  [   96/  179]
train() client id: f_00007-1-3 loss: 0.768490  [  128/  179]
train() client id: f_00007-1-4 loss: 0.757359  [  160/  179]
train() client id: f_00007-2-0 loss: 0.946168  [   32/  179]
train() client id: f_00007-2-1 loss: 0.597557  [   64/  179]
train() client id: f_00007-2-2 loss: 0.799775  [   96/  179]
train() client id: f_00007-2-3 loss: 0.767137  [  128/  179]
train() client id: f_00007-2-4 loss: 0.547424  [  160/  179]
train() client id: f_00007-3-0 loss: 0.678943  [   32/  179]
train() client id: f_00007-3-1 loss: 0.735677  [   64/  179]
train() client id: f_00007-3-2 loss: 0.820543  [   96/  179]
train() client id: f_00007-3-3 loss: 0.775102  [  128/  179]
train() client id: f_00007-3-4 loss: 0.558233  [  160/  179]
train() client id: f_00008-0-0 loss: 0.771517  [   32/  130]
train() client id: f_00008-0-1 loss: 0.745335  [   64/  130]
train() client id: f_00008-0-2 loss: 0.760678  [   96/  130]
train() client id: f_00008-0-3 loss: 0.975231  [  128/  130]
train() client id: f_00008-1-0 loss: 0.717080  [   32/  130]
train() client id: f_00008-1-1 loss: 0.892146  [   64/  130]
train() client id: f_00008-1-2 loss: 0.774030  [   96/  130]
train() client id: f_00008-1-3 loss: 0.869573  [  128/  130]
train() client id: f_00008-2-0 loss: 0.781976  [   32/  130]
train() client id: f_00008-2-1 loss: 0.813640  [   64/  130]
train() client id: f_00008-2-2 loss: 0.799957  [   96/  130]
train() client id: f_00008-2-3 loss: 0.850884  [  128/  130]
train() client id: f_00008-3-0 loss: 0.751605  [   32/  130]
train() client id: f_00008-3-1 loss: 0.886296  [   64/  130]
train() client id: f_00008-3-2 loss: 0.781454  [   96/  130]
train() client id: f_00008-3-3 loss: 0.786449  [  128/  130]
train() client id: f_00009-0-0 loss: 0.749254  [   32/  118]
train() client id: f_00009-0-1 loss: 0.812387  [   64/  118]
train() client id: f_00009-0-2 loss: 0.615054  [   96/  118]
train() client id: f_00009-1-0 loss: 0.763195  [   32/  118]
train() client id: f_00009-1-1 loss: 0.731482  [   64/  118]
train() client id: f_00009-1-2 loss: 0.780007  [   96/  118]
train() client id: f_00009-2-0 loss: 0.789222  [   32/  118]
train() client id: f_00009-2-1 loss: 0.638427  [   64/  118]
train() client id: f_00009-2-2 loss: 0.730293  [   96/  118]
train() client id: f_00009-3-0 loss: 0.659717  [   32/  118]
train() client id: f_00009-3-1 loss: 0.915022  [   64/  118]
train() client id: f_00009-3-2 loss: 0.619821  [   96/  118]
At round 76 accuracy: 0.649867374005305
At round 76 training accuracy: 0.5942320590207915
At round 76 training loss: 0.8198402475763055
update_location
xs = [  -3.9056584     4.20031788  400.00902392   18.81129433    0.97929623
    3.95640986 -362.44319194 -341.32485185  384.66397685 -327.06087855]
ys = [ 392.5879595   375.55583871    1.32061395 -362.45517586  354.35018685
  337.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [405.14264168 388.66416179 412.32143194 376.46728849 368.19154517
 352.32662917 375.99462486 355.67306722 397.83796332 342.03045206]
dists_bs = [276.79919408 268.95240911 600.66621906 571.33255103 251.26719289
 241.93768684 258.23552284 240.83565102 581.38479929 228.82963973]
uav_gains = [6.93541635e-13 7.99744288e-13 6.54792901e-13 8.98819590e-13
 9.79395095e-13 1.17540781e-12 9.03099454e-13 1.12860915e-12
 7.37325739e-13 1.34266847e-12]
bs_gains = [1.60410424e-11 1.73861274e-11 1.83282543e-12 2.10865230e-12
 2.10336130e-11 2.33842916e-11 1.94827084e-11 2.36851376e-11
 2.00814839e-12 2.73312636e-11]
Round 77
-------------------------------
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.7566319  1.47518382 0.74797579 0.2906086  1.70004424 0.81907155
 0.35099723 1.03253353 0.75085927 0.66440907]
obj_prev = 8.588314982517188
eta_min = nan	eta_max = nan
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 1.9404471408469977	eta = 0.9090909090909091
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 6.015300613417023	eta = 0.2932593013524188
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 3.5977300389560636	eta = 0.49032107362544536
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 3.2097433156161213	eta = 0.5495900082517465
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 3.1851865241926465	eta = 0.5538271752429276
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 3.1850702185308974	eta = 0.5538473987330524
eta = 0.5538473987330524
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [0.04875685 0.10254417 0.04798293 0.01663924 0.1184095  0.05649604
 0.02089579 0.06926569 0.05030469 0.04566118]
ene_total = [0.33763567 0.45238391 0.33971862 0.18710392 0.51488509 0.26328129
 0.20400615 0.38565213 0.28224513 0.2181583 ]
ti_comp = [6.11168561 6.36617567 6.09880235 6.16226391 6.37047552 6.37270362
 6.16308753 6.19819213 6.2639569  6.37579113]
ti_coms = [0.34625918 0.09176912 0.35914244 0.29568088 0.08746927 0.08524118
 0.29485726 0.25975266 0.19398789 0.08215366]
t_total = [26.1496769 26.1496769 26.1496769 26.1496769 26.1496769 26.1496769
 26.1496769 26.1496769 26.1496769 26.1496769]
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [1.93938935e-07 1.66286030e-06 1.85631348e-07 7.58228597e-09
 2.55679530e-06 2.77515029e-07 1.50127409e-08 5.40635308e-07
 2.02771968e-07 1.46370394e-07]
ene_total = [0.14020895 0.03716612 0.14542564 0.1197279  0.03542864 0.0345172
 0.11939442 0.10518191 0.0785509  0.03326647]
optimize_network iter = 0 obj = 0.848868136074443
eta = 0.5538473987330524
freqs = [3988822.08036756 8053828.5629115  3933799.78027688 1350091.76188237
 9293615.50887199 4432658.36750978 1695237.49554636 5587571.86903341
 4015408.34280281 3580824.62742287]
eta_min = 0.5538473987330529	eta_max = 0.9002731777032497
af = 1.770021082250513e-05	bf = 0.3787142685879881	zeta = 1.9470231904755644e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [3.12834926e-08 2.68229162e-07 2.99434299e-08 1.22306739e-09
 4.12426142e-07 4.47648089e-08 2.42164354e-09 8.72076598e-08
 3.27083129e-08 2.36104067e-08]
ene_total = [0.68809723 0.18237166 0.71369919 0.58758591 0.17382975 0.1693947
 0.58594921 0.51618999 0.38549919 0.16325868]
ti_comp = [1.09726106 1.35175113 1.08437781 1.14783937 1.35605097 1.35827907
 1.14866299 1.18376759 1.24953236 1.36136658]
ti_coms = [0.34625918 0.09176912 0.35914244 0.29568088 0.08746927 0.08524118
 0.29485726 0.25975266 0.19398789 0.08215366]
t_total = [26.1496769 26.1496769 26.1496769 26.1496769 26.1496769 26.1496769
 26.1496769 26.1496769 26.1496769 26.1496769]
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [3.38201293e-08 2.07313558e-07 3.30055808e-08 1.22836293e-09
 3.17172712e-07 3.43371396e-08 2.42929128e-09 8.33124237e-08
 2.86430070e-08 1.80459360e-08]
ene_total = [0.6272565  0.16624547 0.65059478 0.53563224 0.1584582  0.15441683
 0.53414025 0.47054899 0.35141372 0.14882344]
optimize_network iter = 1 obj = 3.7975304096863765
eta = 0.9002731777032497
freqs = [3950315.32823697 6744042.83153091 3933799.78027688 1288719.87693548
 7762767.85413468 3697728.61147668 1617231.97677077 5201849.38696053
 3579043.95899813 2981797.2351148 ]
eta_min = 0.8902239180004755	eta_max = 0.9002731777032479
af = 1.3057149041253039e-05	bf = 0.3787142685879881	zeta = 1.4362863945378344e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [3.06824073e-08 1.88079677e-07 2.99434299e-08 1.11439939e-09
 2.87746454e-07 3.11514510e-08 2.20390950e-09 7.55829668e-08
 2.59856015e-08 1.63716925e-08]
ene_total = [0.68809721 0.18237007 0.71369919 0.58758591 0.17382727 0.16939443
 0.58594921 0.51618975 0.38549905 0.16325854]
ti_comp = [1.09726106 1.35175113 1.08437781 1.14783937 1.35605097 1.35827907
 1.14866299 1.18376759 1.24953236 1.36136658]
ti_coms = [0.34625918 0.09176912 0.35914244 0.29568088 0.08746927 0.08524118
 0.29485726 0.25975266 0.19398789 0.08215366]
t_total = [26.1496769 26.1496769 26.1496769 26.1496769 26.1496769 26.1496769
 26.1496769 26.1496769 26.1496769 26.1496769]
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [3.38201293e-08 2.07313558e-07 3.30055808e-08 1.22836293e-09
 3.17172712e-07 3.43371396e-08 2.42929128e-09 8.33124237e-08
 2.86430070e-08 1.80459360e-08]
ene_total = [0.6272565  0.16624547 0.65059478 0.53563224 0.1584582  0.15441683
 0.53414025 0.47054899 0.35141372 0.14882344]
optimize_network iter = 2 obj = 3.7975304096863085
eta = 0.9002731777032479
freqs = [3950315.32823696 6744042.83153091 3933799.78027686 1288719.87693548
 7762767.85413468 3697728.61147668 1617231.97677076 5201849.38696052
 3579043.95899813 2981797.2351148 ]
Done!
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [2.94934334e-08 1.80791402e-07 2.87830921e-08 1.07121530e-09
 2.76595992e-07 2.99443012e-08 2.11850581e-09 7.26540515e-08
 2.49786335e-08 1.57372731e-08]
ene_total = [0.03462595 0.00917709 0.03591427 0.02956809 0.0087472  0.00852415
 0.02948573 0.02597534 0.01939881 0.00821538]
At round 77 energy consumption: 0.20963201619321536
At round 77 eta: 0.9002731777032479
At round 77 a_n: 1.4640267926476263
At round 77 local rounds: 3.4401009464594003
At round 77 global rounds: 18.115213119307235
gradient difference: 1.0380113124847412
train() client id: f_00000-0-0 loss: 0.846716  [   32/  126]
train() client id: f_00000-0-1 loss: 0.656908  [   64/  126]
train() client id: f_00000-0-2 loss: 0.830099  [   96/  126]
train() client id: f_00000-1-0 loss: 0.806201  [   32/  126]
train() client id: f_00000-1-1 loss: 0.809188  [   64/  126]
train() client id: f_00000-1-2 loss: 0.725575  [   96/  126]
train() client id: f_00000-2-0 loss: 0.769132  [   32/  126]
train() client id: f_00000-2-1 loss: 0.813527  [   64/  126]
train() client id: f_00000-2-2 loss: 0.858368  [   96/  126]
train() client id: f_00001-0-0 loss: 0.593171  [   32/  265]
train() client id: f_00001-0-1 loss: 0.392686  [   64/  265]
train() client id: f_00001-0-2 loss: 0.484094  [   96/  265]
train() client id: f_00001-0-3 loss: 0.492562  [  128/  265]
train() client id: f_00001-0-4 loss: 0.441441  [  160/  265]
train() client id: f_00001-0-5 loss: 0.372163  [  192/  265]
train() client id: f_00001-0-6 loss: 0.475813  [  224/  265]
train() client id: f_00001-0-7 loss: 0.451352  [  256/  265]
train() client id: f_00001-1-0 loss: 0.591625  [   32/  265]
train() client id: f_00001-1-1 loss: 0.414869  [   64/  265]
train() client id: f_00001-1-2 loss: 0.393188  [   96/  265]
train() client id: f_00001-1-3 loss: 0.365102  [  128/  265]
train() client id: f_00001-1-4 loss: 0.410257  [  160/  265]
train() client id: f_00001-1-5 loss: 0.393015  [  192/  265]
train() client id: f_00001-1-6 loss: 0.625966  [  224/  265]
train() client id: f_00001-1-7 loss: 0.458559  [  256/  265]
train() client id: f_00001-2-0 loss: 0.412206  [   32/  265]
train() client id: f_00001-2-1 loss: 0.480133  [   64/  265]
train() client id: f_00001-2-2 loss: 0.566957  [   96/  265]
train() client id: f_00001-2-3 loss: 0.459852  [  128/  265]
train() client id: f_00001-2-4 loss: 0.390596  [  160/  265]
train() client id: f_00001-2-5 loss: 0.375131  [  192/  265]
train() client id: f_00001-2-6 loss: 0.637031  [  224/  265]
train() client id: f_00001-2-7 loss: 0.351157  [  256/  265]
train() client id: f_00002-0-0 loss: 0.805152  [   32/  124]
train() client id: f_00002-0-1 loss: 0.796097  [   64/  124]
train() client id: f_00002-0-2 loss: 0.777418  [   96/  124]
train() client id: f_00002-1-0 loss: 0.826522  [   32/  124]
train() client id: f_00002-1-1 loss: 0.894628  [   64/  124]
train() client id: f_00002-1-2 loss: 0.799741  [   96/  124]
train() client id: f_00002-2-0 loss: 0.987455  [   32/  124]
train() client id: f_00002-2-1 loss: 0.808868  [   64/  124]
train() client id: f_00002-2-2 loss: 0.859646  [   96/  124]
train() client id: f_00003-0-0 loss: 0.833052  [   32/   43]
train() client id: f_00003-1-0 loss: 0.668871  [   32/   43]
train() client id: f_00003-2-0 loss: 0.454461  [   32/   43]
train() client id: f_00004-0-0 loss: 0.893983  [   32/  306]
train() client id: f_00004-0-1 loss: 0.985457  [   64/  306]
train() client id: f_00004-0-2 loss: 0.872198  [   96/  306]
train() client id: f_00004-0-3 loss: 1.082459  [  128/  306]
train() client id: f_00004-0-4 loss: 1.022072  [  160/  306]
train() client id: f_00004-0-5 loss: 0.899198  [  192/  306]
train() client id: f_00004-0-6 loss: 0.760558  [  224/  306]
train() client id: f_00004-0-7 loss: 0.843858  [  256/  306]
train() client id: f_00004-0-8 loss: 0.970088  [  288/  306]
train() client id: f_00004-1-0 loss: 0.847723  [   32/  306]
train() client id: f_00004-1-1 loss: 0.916346  [   64/  306]
train() client id: f_00004-1-2 loss: 1.007324  [   96/  306]
train() client id: f_00004-1-3 loss: 1.002681  [  128/  306]
train() client id: f_00004-1-4 loss: 0.883023  [  160/  306]
train() client id: f_00004-1-5 loss: 0.999509  [  192/  306]
train() client id: f_00004-1-6 loss: 0.839383  [  224/  306]
train() client id: f_00004-1-7 loss: 0.882538  [  256/  306]
train() client id: f_00004-1-8 loss: 0.892124  [  288/  306]
train() client id: f_00004-2-0 loss: 0.856779  [   32/  306]
train() client id: f_00004-2-1 loss: 0.920133  [   64/  306]
train() client id: f_00004-2-2 loss: 0.882089  [   96/  306]
train() client id: f_00004-2-3 loss: 0.941830  [  128/  306]
train() client id: f_00004-2-4 loss: 1.024756  [  160/  306]
train() client id: f_00004-2-5 loss: 0.947372  [  192/  306]
train() client id: f_00004-2-6 loss: 0.865726  [  224/  306]
train() client id: f_00004-2-7 loss: 0.946656  [  256/  306]
train() client id: f_00004-2-8 loss: 0.979774  [  288/  306]
train() client id: f_00005-0-0 loss: 0.675650  [   32/  146]
train() client id: f_00005-0-1 loss: 0.650129  [   64/  146]
train() client id: f_00005-0-2 loss: 0.886252  [   96/  146]
train() client id: f_00005-0-3 loss: 0.566256  [  128/  146]
train() client id: f_00005-1-0 loss: 0.645273  [   32/  146]
train() client id: f_00005-1-1 loss: 0.728357  [   64/  146]
train() client id: f_00005-1-2 loss: 0.765702  [   96/  146]
train() client id: f_00005-1-3 loss: 0.920420  [  128/  146]
train() client id: f_00005-2-0 loss: 0.534234  [   32/  146]
train() client id: f_00005-2-1 loss: 0.675450  [   64/  146]
train() client id: f_00005-2-2 loss: 1.027316  [   96/  146]
train() client id: f_00005-2-3 loss: 0.786634  [  128/  146]
train() client id: f_00006-0-0 loss: 0.444174  [   32/   54]
train() client id: f_00006-1-0 loss: 0.492056  [   32/   54]
train() client id: f_00006-2-0 loss: 0.403676  [   32/   54]
train() client id: f_00007-0-0 loss: 0.632917  [   32/  179]
train() client id: f_00007-0-1 loss: 0.794541  [   64/  179]
train() client id: f_00007-0-2 loss: 1.000145  [   96/  179]
train() client id: f_00007-0-3 loss: 0.657888  [  128/  179]
train() client id: f_00007-0-4 loss: 0.826920  [  160/  179]
train() client id: f_00007-1-0 loss: 0.627551  [   32/  179]
train() client id: f_00007-1-1 loss: 0.693020  [   64/  179]
train() client id: f_00007-1-2 loss: 0.976185  [   96/  179]
train() client id: f_00007-1-3 loss: 0.665116  [  128/  179]
train() client id: f_00007-1-4 loss: 0.875020  [  160/  179]
train() client id: f_00007-2-0 loss: 0.715600  [   32/  179]
train() client id: f_00007-2-1 loss: 0.982129  [   64/  179]
train() client id: f_00007-2-2 loss: 0.766378  [   96/  179]
train() client id: f_00007-2-3 loss: 0.769443  [  128/  179]
train() client id: f_00007-2-4 loss: 0.585395  [  160/  179]
train() client id: f_00008-0-0 loss: 0.790712  [   32/  130]
train() client id: f_00008-0-1 loss: 0.717682  [   64/  130]
train() client id: f_00008-0-2 loss: 0.846146  [   96/  130]
train() client id: f_00008-0-3 loss: 0.788621  [  128/  130]
train() client id: f_00008-1-0 loss: 0.722301  [   32/  130]
train() client id: f_00008-1-1 loss: 0.734924  [   64/  130]
train() client id: f_00008-1-2 loss: 0.890498  [   96/  130]
train() client id: f_00008-1-3 loss: 0.745125  [  128/  130]
train() client id: f_00008-2-0 loss: 0.653564  [   32/  130]
train() client id: f_00008-2-1 loss: 0.808359  [   64/  130]
train() client id: f_00008-2-2 loss: 0.737721  [   96/  130]
train() client id: f_00008-2-3 loss: 0.952539  [  128/  130]
train() client id: f_00009-0-0 loss: 1.042843  [   32/  118]
train() client id: f_00009-0-1 loss: 0.740446  [   64/  118]
train() client id: f_00009-0-2 loss: 0.738676  [   96/  118]
train() client id: f_00009-1-0 loss: 0.944579  [   32/  118]
train() client id: f_00009-1-1 loss: 0.830884  [   64/  118]
train() client id: f_00009-1-2 loss: 0.848424  [   96/  118]
train() client id: f_00009-2-0 loss: 0.898241  [   32/  118]
train() client id: f_00009-2-1 loss: 0.863786  [   64/  118]
train() client id: f_00009-2-2 loss: 0.857567  [   96/  118]
At round 77 accuracy: 0.6472148541114059
At round 77 training accuracy: 0.5902079141515761
At round 77 training loss: 0.8281305429565441
update_location
xs = [  -3.9056584     4.20031788  405.00902392   18.81129433    0.97929623
    3.95640986 -367.44319194 -346.32485185  389.66397685 -332.06087855]
ys = [ 397.5879595   380.55583871    1.32061395 -367.45517586  359.35018685
  342.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [409.98956048 393.49763537 417.17388877 381.28358352 373.00605331
 357.12350123 380.8167405  360.47410346 402.67441541 346.81470401]
dists_bs = [280.74663567 272.7012225  605.45437232 576.0385713  254.86016576
 245.33036058 261.88741317 244.31549137 586.1995604  232.18228357]
uav_gains = [6.66953995e-13 7.65732155e-13 6.30692965e-13 8.57197052e-13
 9.31026172e-13 1.10932372e-12 8.61079374e-13 1.06691429e-12
 7.07809580e-13 1.26020837e-12]
bs_gains = [1.54174772e-11 1.67251587e-11 1.79252867e-12 2.06077086e-12
 2.02138293e-11 2.24900517e-11 1.87315249e-11 2.27526125e-11
 1.96230594e-12 2.62405338e-11]
Round 78
-------------------------------
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.61445089 1.1956097  0.60744834 0.2367497  1.37782529 0.66388603
 0.28568902 0.83797174 0.60879864 0.53854598]
obj_prev = 6.96697533321086
eta_min = nan	eta_max = nan
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 1.572517230482826	eta = 0.909090909090909
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 4.952035710772682	eta = 0.2886815043580717
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 2.9382966582345253	eta = 0.48652715668257346
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 2.6172340285407683	eta = 0.5462106571408896
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 2.596934549245399	eta = 0.5504802264023727
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 2.5968383965379944	eta = 0.5505006089430082
eta = 0.5505006089430082
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [0.049257   0.10359608 0.04847515 0.01680993 0.11962415 0.05707558
 0.02111014 0.06997622 0.05082072 0.04612957]
ene_total = [0.27603738 0.36760475 0.27771398 0.15379303 0.4183885  0.21392216
 0.16753295 0.31519772 0.22938986 0.17725807]
ti_comp = [7.65842292 7.92067487 7.64545672 7.70927839 7.9250345  7.92732068
 7.71009494 7.74538131 7.81728173 7.93043026]
ti_coms = [0.35494613 0.09269418 0.36791233 0.30409066 0.08833455 0.08604837
 0.30327411 0.26798773 0.19608732 0.08293879]
t_total = [26.0996727 26.0996727 26.0996727 26.0996727 26.0996727 26.0996727
 26.0996727 26.0996727 26.0996727 26.0996727]
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.27351857e-07 1.10760673e-06 1.21795299e-07 4.99516971e-09
 1.70346924e-06 1.84917516e-07 9.89086254e-09 3.56981594e-07
 1.34242758e-07 9.75494579e-08]
ene_total = [0.11560694 0.03019425 0.11983004 0.09904284 0.02877625 0.02802669
 0.0987769  0.0872852  0.06386639 0.02701362]
optimize_network iter = 0 obj = 0.6984191265921221
eta = 0.5505006089430082
freqs = [3215871.18363491 6539599.20133967 3170192.94839821 1090240.14955988
 7547232.16704463 3599928.48687781 1368993.7919406  4517286.87955715
 3250536.39445521 2908390.39625094]
eta_min = 0.5505006089430086	eta_max = 0.917509374892285
af = 9.439588067529733e-06	bf = 0.3139333373586834	zeta = 1.0383546874282708e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [2.03340174e-08 1.76849361e-07 1.94468128e-08 7.97568798e-10
 2.71989541e-07 2.95254116e-08 1.57925432e-09 5.69985401e-08
 2.14342739e-08 1.55755277e-08]
ene_total = [0.57161603 0.14928029 0.59249717 0.48971657 0.14226095 0.1385753
 0.48840159 0.43157626 0.31578514 0.13356732]
ti_comp = [1.26571924 1.52797119 1.25275304 1.31657471 1.53233081 1.534617
 1.31739126 1.35267763 1.42457805 1.53772658]
ti_coms = [0.35494613 0.09269418 0.36791233 0.30409066 0.08833455 0.08604837
 0.30327411 0.26798773 0.19608732 0.08293879]
t_total = [26.0996727 26.0996727 26.0996727 26.0996727 26.0996727 26.0996727
 26.0996727 26.0996727 26.0996727 26.0996727]
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.89785459e-08 1.21152772e-07 1.84654624e-08 6.97172764e-10
 1.85474954e-07 2.00855912e-08 1.37904294e-09 4.76427188e-08
 1.64544595e-08 1.05612140e-08]
ene_total = [0.57161601 0.14927939 0.59249715 0.48971657 0.14225955 0.13857515
 0.48840158 0.43157611 0.31578506 0.13356724]
optimize_network iter = 1 obj = 3.4532738111991463
eta = 0.909090909090909
freqs = [3106837.34109527 5412726.23484139 3089168.32072196 1019314.72951421
 6232384.57566306 2969191.46464435 1279276.24484534 4129943.16102444
 2848016.20722417 2394904.72079761]
eta_min = 0.9009430532547689	eta_max = 0.9090909090909057
af = 6.772388892494084e-06	bf = 0.3139333373586834	zeta = 7.449627781743493e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.89785459e-08 1.21152772e-07 1.84654624e-08 6.97172764e-10
 1.85474954e-07 2.00855912e-08 1.37904294e-09 4.76427188e-08
 1.64544595e-08 1.05612140e-08]
ene_total = [0.57161601 0.14927939 0.59249715 0.48971657 0.14225955 0.13857515
 0.48840158 0.43157611 0.31578506 0.13356724]
ti_comp = [1.26571924 1.52797119 1.25275304 1.31657471 1.53233081 1.534617
 1.31739126 1.35267763 1.42457805 1.53772658]
ti_coms = [0.35494613 0.09269418 0.36791233 0.30409066 0.08833455 0.08604837
 0.30327411 0.26798773 0.19608732 0.08293879]
t_total = [26.0996727 26.0996727 26.0996727 26.0996727 26.0996727 26.0996727
 26.0996727 26.0996727 26.0996727 26.0996727]
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.89785459e-08 1.21152772e-07 1.84654624e-08 6.97172764e-10
 1.85474954e-07 2.00855912e-08 1.37904294e-09 4.76427188e-08
 1.64544595e-08 1.05612140e-08]
ene_total = [0.57161601 0.14927939 0.59249715 0.48971657 0.14225955 0.13857515
 0.48840158 0.43157611 0.31578506 0.13356724]
optimize_network iter = 2 obj = 3.4532738111990233
eta = 0.9090909090909057
freqs = [3106837.34109525 5412726.23484139 3089168.32072194 1019314.7295142
 6232384.57566307 2969191.46464435 1279276.24484533 4129943.16102442
 2848016.20722416 2394904.72079761]
Done!
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.82431083e-08 1.16457981e-07 1.77499073e-08 6.70156624e-10
 1.78287614e-07 1.93072545e-08 1.32560365e-09 4.57965159e-08
 1.58168328e-08 1.01519565e-08]
ene_total = [0.03549463 0.00926953 0.03679125 0.03040907 0.00883363 0.00860486
 0.03032741 0.02679882 0.01960875 0.00829389]
At round 78 energy consumption: 0.21443183922587233
At round 78 eta: 0.9090909090909057
At round 78 a_n: 1.1214809456783108
At round 78 local rounds: 3.120939520577876
At round 78 global rounds: 16.104294719123292
gradient difference: 1.0617284774780273
train() client id: f_00000-0-0 loss: 1.032154  [   32/  126]
train() client id: f_00000-0-1 loss: 0.774027  [   64/  126]
train() client id: f_00000-0-2 loss: 0.659826  [   96/  126]
train() client id: f_00000-1-0 loss: 0.842846  [   32/  126]
train() client id: f_00000-1-1 loss: 0.827069  [   64/  126]
train() client id: f_00000-1-2 loss: 0.868498  [   96/  126]
train() client id: f_00000-2-0 loss: 0.963672  [   32/  126]
train() client id: f_00000-2-1 loss: 0.958621  [   64/  126]
train() client id: f_00000-2-2 loss: 0.676473  [   96/  126]
train() client id: f_00001-0-0 loss: 0.604160  [   32/  265]
train() client id: f_00001-0-1 loss: 0.484329  [   64/  265]
train() client id: f_00001-0-2 loss: 0.532862  [   96/  265]
train() client id: f_00001-0-3 loss: 0.607155  [  128/  265]
train() client id: f_00001-0-4 loss: 0.716386  [  160/  265]
train() client id: f_00001-0-5 loss: 0.541345  [  192/  265]
train() client id: f_00001-0-6 loss: 0.618288  [  224/  265]
train() client id: f_00001-0-7 loss: 0.556327  [  256/  265]
train() client id: f_00001-1-0 loss: 0.669823  [   32/  265]
train() client id: f_00001-1-1 loss: 0.517309  [   64/  265]
train() client id: f_00001-1-2 loss: 0.644331  [   96/  265]
train() client id: f_00001-1-3 loss: 0.596296  [  128/  265]
train() client id: f_00001-1-4 loss: 0.531690  [  160/  265]
train() client id: f_00001-1-5 loss: 0.506571  [  192/  265]
train() client id: f_00001-1-6 loss: 0.613567  [  224/  265]
train() client id: f_00001-1-7 loss: 0.592895  [  256/  265]
train() client id: f_00001-2-0 loss: 0.656215  [   32/  265]
train() client id: f_00001-2-1 loss: 0.677579  [   64/  265]
train() client id: f_00001-2-2 loss: 0.538702  [   96/  265]
train() client id: f_00001-2-3 loss: 0.571503  [  128/  265]
train() client id: f_00001-2-4 loss: 0.625608  [  160/  265]
train() client id: f_00001-2-5 loss: 0.493649  [  192/  265]
train() client id: f_00001-2-6 loss: 0.599328  [  224/  265]
train() client id: f_00001-2-7 loss: 0.558306  [  256/  265]
train() client id: f_00002-0-0 loss: 0.829661  [   32/  124]
train() client id: f_00002-0-1 loss: 0.983389  [   64/  124]
train() client id: f_00002-0-2 loss: 0.939399  [   96/  124]
train() client id: f_00002-1-0 loss: 0.929526  [   32/  124]
train() client id: f_00002-1-1 loss: 0.919801  [   64/  124]
train() client id: f_00002-1-2 loss: 0.843079  [   96/  124]
train() client id: f_00002-2-0 loss: 0.794373  [   32/  124]
train() client id: f_00002-2-1 loss: 0.987393  [   64/  124]
train() client id: f_00002-2-2 loss: 1.038608  [   96/  124]
train() client id: f_00003-0-0 loss: 1.070159  [   32/   43]
train() client id: f_00003-1-0 loss: 0.880210  [   32/   43]
train() client id: f_00003-2-0 loss: 0.764551  [   32/   43]
train() client id: f_00004-0-0 loss: 0.817847  [   32/  306]
train() client id: f_00004-0-1 loss: 0.781872  [   64/  306]
train() client id: f_00004-0-2 loss: 0.788285  [   96/  306]
train() client id: f_00004-0-3 loss: 0.818967  [  128/  306]
train() client id: f_00004-0-4 loss: 0.855402  [  160/  306]
train() client id: f_00004-0-5 loss: 0.767678  [  192/  306]
train() client id: f_00004-0-6 loss: 0.573840  [  224/  306]
train() client id: f_00004-0-7 loss: 0.774293  [  256/  306]
train() client id: f_00004-0-8 loss: 0.794172  [  288/  306]
train() client id: f_00004-1-0 loss: 0.731655  [   32/  306]
train() client id: f_00004-1-1 loss: 0.779833  [   64/  306]
train() client id: f_00004-1-2 loss: 0.781832  [   96/  306]
train() client id: f_00004-1-3 loss: 0.753149  [  128/  306]
train() client id: f_00004-1-4 loss: 0.722932  [  160/  306]
train() client id: f_00004-1-5 loss: 0.762267  [  192/  306]
train() client id: f_00004-1-6 loss: 0.863817  [  224/  306]
train() client id: f_00004-1-7 loss: 0.848344  [  256/  306]
train() client id: f_00004-1-8 loss: 0.740518  [  288/  306]
train() client id: f_00004-2-0 loss: 0.871426  [   32/  306]
train() client id: f_00004-2-1 loss: 0.657302  [   64/  306]
train() client id: f_00004-2-2 loss: 0.761117  [   96/  306]
train() client id: f_00004-2-3 loss: 0.745803  [  128/  306]
train() client id: f_00004-2-4 loss: 0.796883  [  160/  306]
train() client id: f_00004-2-5 loss: 0.745791  [  192/  306]
train() client id: f_00004-2-6 loss: 0.792727  [  224/  306]
train() client id: f_00004-2-7 loss: 0.880768  [  256/  306]
train() client id: f_00004-2-8 loss: 0.769361  [  288/  306]
train() client id: f_00005-0-0 loss: 0.676336  [   32/  146]
train() client id: f_00005-0-1 loss: 0.418683  [   64/  146]
train() client id: f_00005-0-2 loss: 0.390431  [   96/  146]
train() client id: f_00005-0-3 loss: 0.857326  [  128/  146]
train() client id: f_00005-1-0 loss: 0.688951  [   32/  146]
train() client id: f_00005-1-1 loss: 0.670539  [   64/  146]
train() client id: f_00005-1-2 loss: 0.856486  [   96/  146]
train() client id: f_00005-1-3 loss: 0.262257  [  128/  146]
train() client id: f_00005-2-0 loss: 0.546262  [   32/  146]
train() client id: f_00005-2-1 loss: 0.750325  [   64/  146]
train() client id: f_00005-2-2 loss: 0.562054  [   96/  146]
train() client id: f_00005-2-3 loss: 0.467954  [  128/  146]
train() client id: f_00006-0-0 loss: 0.452940  [   32/   54]
train() client id: f_00006-1-0 loss: 0.505091  [   32/   54]
train() client id: f_00006-2-0 loss: 0.485791  [   32/   54]
train() client id: f_00007-0-0 loss: 0.721548  [   32/  179]
train() client id: f_00007-0-1 loss: 0.755579  [   64/  179]
train() client id: f_00007-0-2 loss: 1.040574  [   96/  179]
train() client id: f_00007-0-3 loss: 0.731125  [  128/  179]
train() client id: f_00007-0-4 loss: 0.714862  [  160/  179]
train() client id: f_00007-1-0 loss: 0.769956  [   32/  179]
train() client id: f_00007-1-1 loss: 0.957277  [   64/  179]
train() client id: f_00007-1-2 loss: 0.805769  [   96/  179]
train() client id: f_00007-1-3 loss: 0.685465  [  128/  179]
train() client id: f_00007-1-4 loss: 0.835960  [  160/  179]
train() client id: f_00007-2-0 loss: 0.792268  [   32/  179]
train() client id: f_00007-2-1 loss: 0.825058  [   64/  179]
train() client id: f_00007-2-2 loss: 0.824270  [   96/  179]
train() client id: f_00007-2-3 loss: 0.844559  [  128/  179]
train() client id: f_00007-2-4 loss: 0.927841  [  160/  179]
train() client id: f_00008-0-0 loss: 0.575970  [   32/  130]
train() client id: f_00008-0-1 loss: 0.552928  [   64/  130]
train() client id: f_00008-0-2 loss: 0.624754  [   96/  130]
train() client id: f_00008-0-3 loss: 0.743691  [  128/  130]
train() client id: f_00008-1-0 loss: 0.579703  [   32/  130]
train() client id: f_00008-1-1 loss: 0.649312  [   64/  130]
train() client id: f_00008-1-2 loss: 0.675215  [   96/  130]
train() client id: f_00008-1-3 loss: 0.606492  [  128/  130]
train() client id: f_00008-2-0 loss: 0.732166  [   32/  130]
train() client id: f_00008-2-1 loss: 0.687896  [   64/  130]
train() client id: f_00008-2-2 loss: 0.516839  [   96/  130]
train() client id: f_00008-2-3 loss: 0.579009  [  128/  130]
train() client id: f_00009-0-0 loss: 0.915734  [   32/  118]
train() client id: f_00009-0-1 loss: 0.716011  [   64/  118]
train() client id: f_00009-0-2 loss: 0.669983  [   96/  118]
train() client id: f_00009-1-0 loss: 0.745792  [   32/  118]
train() client id: f_00009-1-1 loss: 0.632760  [   64/  118]
train() client id: f_00009-1-2 loss: 0.790839  [   96/  118]
train() client id: f_00009-2-0 loss: 0.764395  [   32/  118]
train() client id: f_00009-2-1 loss: 0.762454  [   64/  118]
train() client id: f_00009-2-2 loss: 0.678191  [   96/  118]
At round 78 accuracy: 0.6472148541114059
At round 78 training accuracy: 0.5922199865861838
At round 78 training loss: 0.8216183027259634
update_location
xs = [  -3.9056584     4.20031788  410.00902392   18.81129433    0.97929623
    3.95640986 -372.44319194 -351.32485185  394.66397685 -337.06087855]
ys = [ 402.5879595   385.55583871    1.32061395 -372.45517586  364.35018685
  347.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [414.84011293 398.33521992 422.02979008 386.10454908 377.82537987
 361.92587174 385.64338677 365.28047824 407.51481519 351.60495973]
dists_bs = [284.72715542 276.48962935 610.24592354 580.74950485 258.49991481
 248.77726449 265.58322443 247.84734786 591.0173977  235.59333944]
uav_gains = [6.42087105e-13 7.34190954e-13 6.08078996e-13 8.18868515e-13
 8.86722082e-13 1.04941536e-12 8.22401288e-13 1.01086288e-12
 6.80300468e-13 1.18594342e-12]
bs_gains = [1.48215354e-11 1.60913809e-11 1.75339758e-12 2.01430537e-12
 1.94269648e-11 2.16283883e-11 1.80107706e-11 2.18563739e-11
 1.91784428e-12 2.51905507e-11]
Round 79
-------------------------------
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.47167495 0.91597325 0.46632083 0.18231339 1.05554823 0.50864652
 0.2198029  0.64284551 0.46659513 0.41263047]
obj_prev = 5.342351171013073
eta_min = 2.6352711032412936e-201	eta_max = 0.9959613407486563
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 1.2045873201186579	eta = 0.909090909090909
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 3.853002462740199	eta = 0.28421455540603213
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 2.268192123899988	eta = 0.48279833546161194
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 2.0171814832442236	eta = 0.5428759836545998
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 2.0013314548574064	eta = 0.5471754212767701
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 2.0012564070127876	eta = 0.5471959405544855
eta = 0.5471959405544855
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [0.04975385 0.10464104 0.04896411 0.01697949 0.12083078 0.05765129
 0.02132308 0.07068206 0.05133334 0.04659488]
ene_total = [0.21330398 0.28236012 0.21458127 0.11946231 0.32136361 0.16430363
 0.13001973 0.24348682 0.17623076 0.13614417]
ti_comp = [10.15400802 10.42406192 10.14095348 10.20515528 10.42848067 10.43082397
 10.20596501 10.24142523 10.31948805 10.43395522]
ti_coms = [0.36368797 0.09363407 0.37674251 0.31254071 0.08921532 0.08687202
 0.31173098 0.27627076 0.19820794 0.08374077]
t_total = [26.0496685 26.0496685 26.0496685 26.0496685 26.0496685 26.0496685
 26.0496685 26.0496685 26.0496685 26.0496685]
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [7.46595073e-08 6.59040563e-07 7.13437549e-08 2.93774855e-09
 1.01384295e-06 1.10070217e-07 5.81730210e-09 2.10419888e-07
 7.93893490e-08 5.80759835e-08]
ene_total = [0.09007648 0.02319242 0.09330975 0.07740842 0.02209888 0.02151627
 0.07720788 0.06842579 0.04909128 0.02074061]
optimize_network iter = 0 obj = 0.5430677938747402
eta = 0.5471959405544855
freqs = [2449961.18756652 5019206.37355413 2414176.71155869  831907.42340227
 5793307.05286581 2763505.9294842  1044638.0630291  3450792.08881117
 2487203.73595765 2232848.20767479]
eta_min = 0.5471959405544858	eta_max = 0.9355133020148048
af = 4.251199729612188e-06	bf = 0.24590073836681936	zeta = 4.6763197025734075e-06	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [1.18016929e-08 1.04176878e-07 1.12775602e-08 4.64380326e-10
 1.60261749e-07 1.73991893e-08 9.19561561e-10 3.32618174e-08
 1.25493558e-08 9.18027656e-09]
ene_total = [0.44865618 0.11551099 0.46476064 0.3855593  0.11006058 0.10716806
 0.3845604  0.34081604 0.24451523 0.10330516]
ti_comp = [1.74794103 2.01799493 1.73488648 1.79908829 2.02241368 2.02475698
 1.79989802 1.83535824 1.91342105 2.02788823]
ti_coms = [0.36368797 0.09363407 0.37674251 0.31254071 0.08921532 0.08687202
 0.31173098 0.27627076 0.19820794 0.08374077]
t_total = [26.0496685 26.0496685 26.0496685 26.0496685 26.0496685 26.0496685
 26.0496685 26.0496685 26.0496685 26.0496685]
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [9.95139913e-09 6.94582032e-08 9.62828620e-09 3.73358873e-10
 1.06475663e-07 1.15382202e-08 7.38773900e-10 2.58787602e-08
 9.12084391e-09 6.07273117e-09]
ene_total = [0.44865616 0.11551056 0.46476062 0.3855593  0.11005992 0.10716799
 0.38456039 0.34081595 0.24451519 0.10330512]
optimize_network iter = 1 obj = 2.7049111966856447
eta = 0.909090909090909
freqs = [2249723.37868762 4098369.93259797 2230673.32764145  745935.59638761
 4722117.45608519 2250428.94068191  936334.90519755 3043809.99590222
 2120401.76543593 1816031.36876833]
eta_min = 0.9090909090910358	eta_max = 0.909090909090905
af = 2.9326740829202082e-06	bf = 0.24590073836681936	zeta = 3.2259414912122293e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [9.95139913e-09 6.94582032e-08 9.62828620e-09 3.73358873e-10
 1.06475663e-07 1.15382202e-08 7.38773900e-10 2.58787602e-08
 9.12084391e-09 6.07273117e-09]
ene_total = [0.44865616 0.11551056 0.46476062 0.3855593  0.11005992 0.10716799
 0.38456039 0.34081595 0.24451519 0.10330512]
ti_comp = [1.74794103 2.01799493 1.73488648 1.79908829 2.02241368 2.02475698
 1.79989802 1.83535824 1.91342105 2.02788823]
ti_coms = [0.36368797 0.09363407 0.37674251 0.31254071 0.08921532 0.08687202
 0.31173098 0.27627076 0.19820794 0.08374077]
t_total = [26.0496685 26.0496685 26.0496685 26.0496685 26.0496685 26.0496685
 26.0496685 26.0496685 26.0496685 26.0496685]
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [9.95139913e-09 6.94582032e-08 9.62828620e-09 3.73358873e-10
 1.06475663e-07 1.15382202e-08 7.38773900e-10 2.58787602e-08
 9.12084391e-09 6.07273117e-09]
ene_total = [0.44865616 0.11551056 0.46476062 0.3855593  0.11005992 0.10716799
 0.38456039 0.34081595 0.24451519 0.10330512]
optimize_network iter = 2 obj = 2.7049111966855257
eta = 0.909090909090905
freqs = [2249723.37868761 4098369.93259797 2230673.32764143  745935.59638761
 4722117.4560852  2250428.94068191  936334.90519755 3043809.99590221
 2120401.76543593 1816031.36876833]
Done!
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [9.56577248e-09 6.67666285e-08 9.25518050e-09 3.58890845e-10
 1.02349625e-07 1.10911026e-08 7.10145674e-10 2.48759324e-08
 8.76740211e-09 5.83740678e-09]
ene_total = [0.03636881 0.00936347 0.03767426 0.03125407 0.00892163 0.00868721
 0.0311731  0.0276271  0.0198208  0.00837408]
At round 79 energy consumption: 0.2192645430104472
At round 79 eta: 0.909090909090905
At round 79 a_n: 0.7789350987089918
At round 79 local rounds: 3.120939520577902
At round 79 global rounds: 12.336290402460858
gradient difference: 0.794734001159668
train() client id: f_00000-0-0 loss: 1.113301  [   32/  126]
train() client id: f_00000-0-1 loss: 1.200948  [   64/  126]
train() client id: f_00000-0-2 loss: 0.962777  [   96/  126]
train() client id: f_00000-1-0 loss: 1.079802  [   32/  126]
train() client id: f_00000-1-1 loss: 1.116399  [   64/  126]
train() client id: f_00000-1-2 loss: 1.188401  [   96/  126]
train() client id: f_00000-2-0 loss: 1.121822  [   32/  126]
train() client id: f_00000-2-1 loss: 1.149790  [   64/  126]
train() client id: f_00000-2-2 loss: 1.015843  [   96/  126]
train() client id: f_00001-0-0 loss: 0.531844  [   32/  265]
train() client id: f_00001-0-1 loss: 0.577478  [   64/  265]
train() client id: f_00001-0-2 loss: 0.520727  [   96/  265]
train() client id: f_00001-0-3 loss: 0.647813  [  128/  265]
train() client id: f_00001-0-4 loss: 0.632083  [  160/  265]
train() client id: f_00001-0-5 loss: 0.482608  [  192/  265]
train() client id: f_00001-0-6 loss: 0.690995  [  224/  265]
train() client id: f_00001-0-7 loss: 0.517314  [  256/  265]
train() client id: f_00001-1-0 loss: 0.617357  [   32/  265]
train() client id: f_00001-1-1 loss: 0.480267  [   64/  265]
train() client id: f_00001-1-2 loss: 0.688841  [   96/  265]
train() client id: f_00001-1-3 loss: 0.487469  [  128/  265]
train() client id: f_00001-1-4 loss: 0.556744  [  160/  265]
train() client id: f_00001-1-5 loss: 0.595058  [  192/  265]
train() client id: f_00001-1-6 loss: 0.570267  [  224/  265]
train() client id: f_00001-1-7 loss: 0.648750  [  256/  265]
train() client id: f_00001-2-0 loss: 0.566324  [   32/  265]
train() client id: f_00001-2-1 loss: 0.500535  [   64/  265]
train() client id: f_00001-2-2 loss: 0.708932  [   96/  265]
train() client id: f_00001-2-3 loss: 0.558774  [  128/  265]
train() client id: f_00001-2-4 loss: 0.556625  [  160/  265]
train() client id: f_00001-2-5 loss: 0.724869  [  192/  265]
train() client id: f_00001-2-6 loss: 0.503133  [  224/  265]
train() client id: f_00001-2-7 loss: 0.562287  [  256/  265]
train() client id: f_00002-0-0 loss: 0.908799  [   32/  124]
train() client id: f_00002-0-1 loss: 0.707599  [   64/  124]
train() client id: f_00002-0-2 loss: 0.850786  [   96/  124]
train() client id: f_00002-1-0 loss: 0.948467  [   32/  124]
train() client id: f_00002-1-1 loss: 0.784492  [   64/  124]
train() client id: f_00002-1-2 loss: 0.850176  [   96/  124]
train() client id: f_00002-2-0 loss: 0.667843  [   32/  124]
train() client id: f_00002-2-1 loss: 0.837002  [   64/  124]
train() client id: f_00002-2-2 loss: 0.674038  [   96/  124]
train() client id: f_00003-0-0 loss: 0.669174  [   32/   43]
train() client id: f_00003-1-0 loss: 0.561399  [   32/   43]
train() client id: f_00003-2-0 loss: 0.858467  [   32/   43]
train() client id: f_00004-0-0 loss: 0.810943  [   32/  306]
train() client id: f_00004-0-1 loss: 0.717427  [   64/  306]
train() client id: f_00004-0-2 loss: 0.783874  [   96/  306]
train() client id: f_00004-0-3 loss: 0.855820  [  128/  306]
train() client id: f_00004-0-4 loss: 0.846891  [  160/  306]
train() client id: f_00004-0-5 loss: 0.826981  [  192/  306]
train() client id: f_00004-0-6 loss: 0.725839  [  224/  306]
train() client id: f_00004-0-7 loss: 0.842521  [  256/  306]
train() client id: f_00004-0-8 loss: 0.842294  [  288/  306]
train() client id: f_00004-1-0 loss: 0.811386  [   32/  306]
train() client id: f_00004-1-1 loss: 0.911077  [   64/  306]
train() client id: f_00004-1-2 loss: 0.744184  [   96/  306]
train() client id: f_00004-1-3 loss: 0.797026  [  128/  306]
train() client id: f_00004-1-4 loss: 0.917608  [  160/  306]
train() client id: f_00004-1-5 loss: 0.751298  [  192/  306]
train() client id: f_00004-1-6 loss: 0.853433  [  224/  306]
train() client id: f_00004-1-7 loss: 0.833298  [  256/  306]
train() client id: f_00004-1-8 loss: 0.690696  [  288/  306]
train() client id: f_00004-2-0 loss: 0.749218  [   32/  306]
train() client id: f_00004-2-1 loss: 0.950758  [   64/  306]
train() client id: f_00004-2-2 loss: 0.838019  [   96/  306]
train() client id: f_00004-2-3 loss: 0.812389  [  128/  306]
train() client id: f_00004-2-4 loss: 0.851085  [  160/  306]
train() client id: f_00004-2-5 loss: 0.769373  [  192/  306]
train() client id: f_00004-2-6 loss: 0.809059  [  224/  306]
train() client id: f_00004-2-7 loss: 0.756940  [  256/  306]
train() client id: f_00004-2-8 loss: 0.772524  [  288/  306]
train() client id: f_00005-0-0 loss: 0.806769  [   32/  146]
train() client id: f_00005-0-1 loss: 0.515900  [   64/  146]
train() client id: f_00005-0-2 loss: 1.017523  [   96/  146]
train() client id: f_00005-0-3 loss: 0.850559  [  128/  146]
train() client id: f_00005-1-0 loss: 0.899336  [   32/  146]
train() client id: f_00005-1-1 loss: 0.775069  [   64/  146]
train() client id: f_00005-1-2 loss: 0.793943  [   96/  146]
train() client id: f_00005-1-3 loss: 0.926699  [  128/  146]
train() client id: f_00005-2-0 loss: 0.869156  [   32/  146]
train() client id: f_00005-2-1 loss: 0.781284  [   64/  146]
train() client id: f_00005-2-2 loss: 0.803307  [   96/  146]
train() client id: f_00005-2-3 loss: 0.698217  [  128/  146]
train() client id: f_00006-0-0 loss: 0.458991  [   32/   54]
train() client id: f_00006-1-0 loss: 0.528670  [   32/   54]
train() client id: f_00006-2-0 loss: 0.504182  [   32/   54]
train() client id: f_00007-0-0 loss: 0.429484  [   32/  179]
train() client id: f_00007-0-1 loss: 0.711591  [   64/  179]
train() client id: f_00007-0-2 loss: 0.611254  [   96/  179]
train() client id: f_00007-0-3 loss: 0.580060  [  128/  179]
train() client id: f_00007-0-4 loss: 0.688291  [  160/  179]
train() client id: f_00007-1-0 loss: 0.454602  [   32/  179]
train() client id: f_00007-1-1 loss: 0.447181  [   64/  179]
train() client id: f_00007-1-2 loss: 0.619618  [   96/  179]
train() client id: f_00007-1-3 loss: 0.743860  [  128/  179]
train() client id: f_00007-1-4 loss: 0.617296  [  160/  179]
train() client id: f_00007-2-0 loss: 0.550459  [   32/  179]
train() client id: f_00007-2-1 loss: 0.612360  [   64/  179]
train() client id: f_00007-2-2 loss: 0.756511  [   96/  179]
train() client id: f_00007-2-3 loss: 0.525137  [  128/  179]
train() client id: f_00007-2-4 loss: 0.444507  [  160/  179]
train() client id: f_00008-0-0 loss: 0.869867  [   32/  130]
train() client id: f_00008-0-1 loss: 0.762791  [   64/  130]
train() client id: f_00008-0-2 loss: 0.786265  [   96/  130]
train() client id: f_00008-0-3 loss: 0.755063  [  128/  130]
train() client id: f_00008-1-0 loss: 0.811991  [   32/  130]
train() client id: f_00008-1-1 loss: 0.768565  [   64/  130]
train() client id: f_00008-1-2 loss: 0.765016  [   96/  130]
train() client id: f_00008-1-3 loss: 0.806466  [  128/  130]
train() client id: f_00008-2-0 loss: 0.815780  [   32/  130]
train() client id: f_00008-2-1 loss: 0.693846  [   64/  130]
train() client id: f_00008-2-2 loss: 0.829927  [   96/  130]
train() client id: f_00008-2-3 loss: 0.832388  [  128/  130]
train() client id: f_00009-0-0 loss: 0.943565  [   32/  118]
train() client id: f_00009-0-1 loss: 0.657653  [   64/  118]
train() client id: f_00009-0-2 loss: 0.994123  [   96/  118]
train() client id: f_00009-1-0 loss: 0.849318  [   32/  118]
train() client id: f_00009-1-1 loss: 0.758850  [   64/  118]
train() client id: f_00009-1-2 loss: 0.828303  [   96/  118]
train() client id: f_00009-2-0 loss: 0.755594  [   32/  118]
train() client id: f_00009-2-1 loss: 0.782845  [   64/  118]
train() client id: f_00009-2-2 loss: 0.964959  [   96/  118]
At round 79 accuracy: 0.6445623342175066
At round 79 training accuracy: 0.5868544600938967
At round 79 training loss: 0.8282939594008301
update_location
xs = [  -3.9056584     4.20031788  415.00902392   18.81129433    0.97929623
    3.95640986 -377.44319194 -356.32485185  399.66397685 -342.06087855]
ys = [ 407.5879595   390.55583871    1.32061395 -377.45517586  369.35018685
  352.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [419.69417305 403.17676746 426.88901831 390.93001238 382.6493428
 366.73352471 390.47439568 370.09198357 412.35902362 356.40097712]
dists_bs = [288.73938531 280.31602439 615.04079331 585.46523308 262.18449197
 252.27617572 269.32114847 251.42902847 595.83823656 239.06030698]
uav_gains = [6.18777499e-13 7.04863808e-13 5.86815526e-13 7.83470858e-13
 8.46017284e-13 9.94934833e-13 7.86695341e-13 9.59778197e-13
 6.54600040e-13 1.11885798e-12]
bs_gains = [1.42520458e-11 1.54838835e-11 1.71539091e-12 1.96920521e-12
 1.86721568e-11 2.07989120e-11 1.73195580e-11 2.09957270e-11
 1.87471239e-12 2.41809390e-11]
Round 80
-------------------------------
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.32829867 0.63627291 0.3245873  0.12729558 0.73321141 0.35335129
 0.15333479 0.44714953 0.32424655 0.28666076]
obj_prev = 3.7144087846743608
eta_min = 2.74115185868049e-289	eta_max = 0.9972962516128097
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 0.8366574097544858	eta = 0.9090909090909091
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 2.7178337538132036	eta = 0.2798543671643672
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 1.5874476546409413	eta = 0.4791324255686323
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 1.4095987989406396	eta = 0.539584487304448
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 1.398385343651186	eta = 0.5439113393776205
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 1.39833229498431	eta = 0.5439319737944585
eta = 0.5439319737944585
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [0.05024753 0.10567934 0.04944995 0.01714797 0.12202973 0.05822333
 0.02153466 0.0713834  0.05184269 0.04705721]
ene_total = [0.14943821 0.1966487  0.15032166 0.08411788 0.22380943 0.11442236
 0.09147288 0.17052575 0.12276251 0.0948129 ]
ti_comp = [14.85036606 15.12826574 14.83721764 14.90182223 15.13274303 15.13514254
 14.90262547 14.93825677 15.02250443 15.13829514]
ti_coms = [0.37248833 0.09458864 0.38563675 0.32103215 0.09011135 0.08771185
 0.32022891 0.28459761 0.20034996 0.08455925]
t_total = [25.99966431 25.99966431 25.99966431 25.99966431 25.99966431 25.99966431
 25.99966431 25.99966431 25.99966431 25.99966431]
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [3.59542268e-08 3.22308656e-07 3.43299174e-08 1.41918577e-09
 4.95954233e-07 5.38515457e-08 2.81040133e-09 1.01876001e-07
 3.85885102e-08 2.84187337e-08]
ene_total = [0.06361869 0.01615569 0.06586436 0.05483025 0.01539129 0.01498072
 0.05469306 0.04860763 0.03421856 0.01444223]
optimize_network iter = 0 obj = 0.38280246883118135
eta = 0.5439319737944585
freqs = [1691794.45393543 3492777.65574166 1666416.00768811  575364.79907189
 4031976.3853505  1923448.44978197  722512.19806255 2389281.50597988
 1725501.02801081 1554244.18120876]
eta_min = 0.5439319737944591	eta_max = 0.9542982433752891
af = 1.4269447035111872e-06	bf = 0.17458309747545425	zeta = 1.569639173862306e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [5.62757246e-09 5.04479022e-08 5.37333478e-09 2.22131623e-10
 7.76269894e-08 8.42886922e-09 4.39885336e-10 1.59456795e-08
 6.03989174e-09 4.44811354e-09]
ene_total = [0.3191587  0.08104669 0.33042464 0.27506954 0.07721065 0.07515409
 0.2743813  0.24385151 0.17166562 0.07245282]
ti_comp = [2.66191833 2.93981801 2.64876991 2.7133745  2.9442953  2.9466948
 2.71417774 2.74980904 2.8340567  2.9498474 ]
ti_coms = [0.37248833 0.09458864 0.38563675 0.32103215 0.09011135 0.08771185
 0.32022891 0.28459761 0.20034996 0.08455925]
t_total = [25.99966431 25.99966431 25.99966431 25.99966431 25.99966431 25.99966431
 25.99966431 25.99966431 25.99966431 25.99966431]
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [4.29089854e-09 3.27282506e-08 4.13050023e-09 1.64139063e-10
 5.02374527e-08 5.44771420e-09 3.24885833e-10 1.15286932e-08
 4.15756503e-09 2.86994110e-09]
ene_total = [0.31915869 0.08104654 0.33042463 0.27506954 0.07721041 0.07515407
 0.2743813  0.24385147 0.17166561 0.07245281]
optimize_network iter = 1 obj = 1.9204150651233343
eta = 0.9090909090909091
freqs = [1477274.39950027 2813265.88968522 1461042.34829177  494588.56272364
 3243585.97051685 1546333.09614372  620927.40517717 2031588.97688467
 1431594.99214584 1248440.38657298]
eta_min = 0.9090909090909186	eta_max = 0.9090909090909067
af = 9.470450164325744e-07	bf = 0.17458309747545425	zeta = 1.0417495180758319e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [4.29089854e-09 3.27282506e-08 4.13050023e-09 1.64139063e-10
 5.02374527e-08 5.44771420e-09 3.24885833e-10 1.15286932e-08
 4.15756503e-09 2.86994110e-09]
ene_total = [0.31915869 0.08104654 0.33042463 0.27506954 0.07721041 0.07515407
 0.2743813  0.24385147 0.17166561 0.07245281]
ti_comp = [2.66191833 2.93981801 2.64876991 2.7133745  2.9442953  2.9466948
 2.71417774 2.74980904 2.8340567  2.9498474 ]
ti_coms = [0.37248833 0.09458864 0.38563675 0.32103215 0.09011135 0.08771185
 0.32022891 0.28459761 0.20034996 0.08455925]
t_total = [25.99966431 25.99966431 25.99966431 25.99966431 25.99966431 25.99966431
 25.99966431 25.99966431 25.99966431 25.99966431]
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [4.29089854e-09 3.27282506e-08 4.13050023e-09 1.64139063e-10
 5.02374527e-08 5.44771420e-09 3.24885833e-10 1.15286932e-08
 4.15756503e-09 2.86994110e-09]
ene_total = [0.31915869 0.08104654 0.33042463 0.27506954 0.07721041 0.07515407
 0.2743813  0.24385147 0.17166561 0.07245281]
optimize_network iter = 2 obj = 1.9204150651232854
eta = 0.9090909090909067
freqs = [1477274.39950027 2813265.88968522 1461042.34829176  494588.56272364
 3243585.97051684 1546333.09614371  620927.40517717 2031588.97688466
 1431594.99214584 1248440.38657297]
Done!
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [4.12462194e-09 3.14599982e-08 3.97043922e-09 1.57778511e-10
 4.82907013e-08 5.23660984e-09 3.12296182e-10 1.10819449e-08
 3.99645523e-09 2.75872802e-09]
ene_total = [0.03724884 0.0094589  0.03856368 0.03210322 0.00901118 0.00877119
 0.03202289 0.02845977 0.020035   0.00845593]
At round 80 energy consumption: 0.22413059127783166
At round 80 eta: 0.9090909090909067
At round 80 a_n: 0.43638925173967635
At round 80 local rounds: 3.120939520577836
At round 80 global rounds: 8.568286085798688
gradient difference: 1.224853515625
train() client id: f_00000-0-0 loss: 0.964303  [   32/  126]
train() client id: f_00000-0-1 loss: 0.954541  [   64/  126]
train() client id: f_00000-0-2 loss: 0.804722  [   96/  126]
train() client id: f_00000-1-0 loss: 0.821690  [   32/  126]
train() client id: f_00000-1-1 loss: 1.001877  [   64/  126]
train() client id: f_00000-1-2 loss: 0.993587  [   96/  126]
train() client id: f_00000-2-0 loss: 0.906662  [   32/  126]
train() client id: f_00000-2-1 loss: 0.879330  [   64/  126]
train() client id: f_00000-2-2 loss: 1.120208  [   96/  126]
train() client id: f_00001-0-0 loss: 0.664669  [   32/  265]
train() client id: f_00001-0-1 loss: 0.618729  [   64/  265]
train() client id: f_00001-0-2 loss: 0.558066  [   96/  265]
train() client id: f_00001-0-3 loss: 0.514930  [  128/  265]
train() client id: f_00001-0-4 loss: 0.516873  [  160/  265]
train() client id: f_00001-0-5 loss: 0.559374  [  192/  265]
train() client id: f_00001-0-6 loss: 0.582057  [  224/  265]
train() client id: f_00001-0-7 loss: 0.485478  [  256/  265]
train() client id: f_00001-1-0 loss: 0.595967  [   32/  265]
train() client id: f_00001-1-1 loss: 0.622007  [   64/  265]
train() client id: f_00001-1-2 loss: 0.527099  [   96/  265]
train() client id: f_00001-1-3 loss: 0.506400  [  128/  265]
train() client id: f_00001-1-4 loss: 0.550493  [  160/  265]
train() client id: f_00001-1-5 loss: 0.590804  [  192/  265]
train() client id: f_00001-1-6 loss: 0.618484  [  224/  265]
train() client id: f_00001-1-7 loss: 0.530818  [  256/  265]
train() client id: f_00001-2-0 loss: 0.697173  [   32/  265]
train() client id: f_00001-2-1 loss: 0.576805  [   64/  265]
train() client id: f_00001-2-2 loss: 0.558956  [   96/  265]
train() client id: f_00001-2-3 loss: 0.548500  [  128/  265]
train() client id: f_00001-2-4 loss: 0.630294  [  160/  265]
train() client id: f_00001-2-5 loss: 0.536038  [  192/  265]
train() client id: f_00001-2-6 loss: 0.464540  [  224/  265]
train() client id: f_00001-2-7 loss: 0.536105  [  256/  265]
train() client id: f_00002-0-0 loss: 1.222618  [   32/  124]
train() client id: f_00002-0-1 loss: 1.014718  [   64/  124]
train() client id: f_00002-0-2 loss: 0.824959  [   96/  124]
train() client id: f_00002-1-0 loss: 1.004872  [   32/  124]
train() client id: f_00002-1-1 loss: 0.833192  [   64/  124]
train() client id: f_00002-1-2 loss: 1.036557  [   96/  124]
train() client id: f_00002-2-0 loss: 1.320387  [   32/  124]
train() client id: f_00002-2-1 loss: 0.824113  [   64/  124]
train() client id: f_00002-2-2 loss: 1.043080  [   96/  124]
train() client id: f_00003-0-0 loss: 0.536303  [   32/   43]
train() client id: f_00003-1-0 loss: 0.313427  [   32/   43]
train() client id: f_00003-2-0 loss: 0.627336  [   32/   43]
train() client id: f_00004-0-0 loss: 0.860600  [   32/  306]
train() client id: f_00004-0-1 loss: 0.796849  [   64/  306]
train() client id: f_00004-0-2 loss: 0.870057  [   96/  306]
train() client id: f_00004-0-3 loss: 0.940603  [  128/  306]
train() client id: f_00004-0-4 loss: 0.920328  [  160/  306]
train() client id: f_00004-0-5 loss: 0.797746  [  192/  306]
train() client id: f_00004-0-6 loss: 0.817984  [  224/  306]
train() client id: f_00004-0-7 loss: 0.946121  [  256/  306]
train() client id: f_00004-0-8 loss: 0.789946  [  288/  306]
train() client id: f_00004-1-0 loss: 0.943651  [   32/  306]
train() client id: f_00004-1-1 loss: 0.914942  [   64/  306]
train() client id: f_00004-1-2 loss: 0.890631  [   96/  306]
train() client id: f_00004-1-3 loss: 0.792248  [  128/  306]
train() client id: f_00004-1-4 loss: 0.749205  [  160/  306]
train() client id: f_00004-1-5 loss: 0.838296  [  192/  306]
train() client id: f_00004-1-6 loss: 0.753810  [  224/  306]
train() client id: f_00004-1-7 loss: 0.967321  [  256/  306]
train() client id: f_00004-1-8 loss: 0.750575  [  288/  306]
train() client id: f_00004-2-0 loss: 0.900693  [   32/  306]
train() client id: f_00004-2-1 loss: 0.792655  [   64/  306]
train() client id: f_00004-2-2 loss: 0.765248  [   96/  306]
train() client id: f_00004-2-3 loss: 0.891310  [  128/  306]
train() client id: f_00004-2-4 loss: 0.887281  [  160/  306]
train() client id: f_00004-2-5 loss: 0.952844  [  192/  306]
train() client id: f_00004-2-6 loss: 0.783675  [  224/  306]
train() client id: f_00004-2-7 loss: 0.765269  [  256/  306]
train() client id: f_00004-2-8 loss: 0.956295  [  288/  306]
train() client id: f_00005-0-0 loss: 0.801041  [   32/  146]
train() client id: f_00005-0-1 loss: 0.564876  [   64/  146]
train() client id: f_00005-0-2 loss: 0.661476  [   96/  146]
train() client id: f_00005-0-3 loss: 0.937170  [  128/  146]
train() client id: f_00005-1-0 loss: 0.698680  [   32/  146]
train() client id: f_00005-1-1 loss: 0.675684  [   64/  146]
train() client id: f_00005-1-2 loss: 0.585146  [   96/  146]
train() client id: f_00005-1-3 loss: 0.960266  [  128/  146]
train() client id: f_00005-2-0 loss: 0.605460  [   32/  146]
train() client id: f_00005-2-1 loss: 0.856660  [   64/  146]
train() client id: f_00005-2-2 loss: 0.625363  [   96/  146]
train() client id: f_00005-2-3 loss: 0.909674  [  128/  146]
train() client id: f_00006-0-0 loss: 0.429223  [   32/   54]
train() client id: f_00006-1-0 loss: 0.475941  [   32/   54]
train() client id: f_00006-2-0 loss: 0.535480  [   32/   54]
train() client id: f_00007-0-0 loss: 1.030375  [   32/  179]
train() client id: f_00007-0-1 loss: 0.802188  [   64/  179]
train() client id: f_00007-0-2 loss: 0.784127  [   96/  179]
train() client id: f_00007-0-3 loss: 0.926043  [  128/  179]
train() client id: f_00007-0-4 loss: 0.643741  [  160/  179]
train() client id: f_00007-1-0 loss: 0.764793  [   32/  179]
train() client id: f_00007-1-1 loss: 0.688921  [   64/  179]
train() client id: f_00007-1-2 loss: 0.864756  [   96/  179]
train() client id: f_00007-1-3 loss: 1.168151  [  128/  179]
train() client id: f_00007-1-4 loss: 0.683185  [  160/  179]
train() client id: f_00007-2-0 loss: 0.829356  [   32/  179]
train() client id: f_00007-2-1 loss: 0.940952  [   64/  179]
train() client id: f_00007-2-2 loss: 0.778161  [   96/  179]
train() client id: f_00007-2-3 loss: 0.798867  [  128/  179]
train() client id: f_00007-2-4 loss: 0.776249  [  160/  179]
train() client id: f_00008-0-0 loss: 0.653017  [   32/  130]
train() client id: f_00008-0-1 loss: 0.698816  [   64/  130]
train() client id: f_00008-0-2 loss: 0.692650  [   96/  130]
train() client id: f_00008-0-3 loss: 0.743810  [  128/  130]
train() client id: f_00008-1-0 loss: 0.698687  [   32/  130]
train() client id: f_00008-1-1 loss: 0.623532  [   64/  130]
train() client id: f_00008-1-2 loss: 0.765751  [   96/  130]
train() client id: f_00008-1-3 loss: 0.719489  [  128/  130]
train() client id: f_00008-2-0 loss: 0.742704  [   32/  130]
train() client id: f_00008-2-1 loss: 0.674556  [   64/  130]
train() client id: f_00008-2-2 loss: 0.714526  [   96/  130]
train() client id: f_00008-2-3 loss: 0.712378  [  128/  130]
train() client id: f_00009-0-0 loss: 0.740732  [   32/  118]
train() client id: f_00009-0-1 loss: 0.498844  [   64/  118]
train() client id: f_00009-0-2 loss: 0.539377  [   96/  118]
train() client id: f_00009-1-0 loss: 0.380627  [   32/  118]
train() client id: f_00009-1-1 loss: 0.486533  [   64/  118]
train() client id: f_00009-1-2 loss: 0.684049  [   96/  118]
train() client id: f_00009-2-0 loss: 0.534986  [   32/  118]
train() client id: f_00009-2-1 loss: 0.566099  [   64/  118]
train() client id: f_00009-2-2 loss: 0.553500  [   96/  118]
At round 80 accuracy: 0.6445623342175066
At round 80 training accuracy: 0.590878604963112
At round 80 training loss: 0.8254743425716478
update_location
xs = [  -3.9056584     4.20031788  420.00902392   18.81129433    0.97929623
    3.95640986 -382.44319194 -361.32485185  404.66397685 -347.06087855]
ys = [ 412.5879595   395.55583871    1.32061395 -382.45517586  374.35018685
  357.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [424.55162052 408.02213691 431.75146113 395.7598089  387.47776893
 371.54625506 395.30960727 374.90842191 417.20690806 361.20252668]
dists_bs = [292.78202169 284.1788731  619.83890462 590.18564105 265.91203375
 255.82496036 273.09945612 255.05843424 600.66200473 242.58078894]
uav_gains = [5.96881287e-13 6.77526367e-13 5.66782867e-13 7.50688319e-13
 8.08508001e-13 9.45234363e-13 7.53640053e-13 9.13074203e-13
 6.30534284e-13 1.05807042e-12]
bs_gains = [1.37078628e-11 1.49017434e-11 1.67846913e-12 1.92542185e-12
 1.79484814e-11 2.00011019e-11 1.66569601e-11 2.01698633e-11
 1.83286144e-12 2.32111230e-11]
Round 81
-------------------------------
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.18431614 0.35650715 0.18224127 0.07169206 0.41081325 0.19799863
 0.08628049 0.25087914 0.1817507  0.16063512]
obj_prev = 2.0831139657376503
eta_min = 0.0	eta_max = inf
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.4687274993903175	eta = 0.909090909090909
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 1.5461570538938256	eta = 0.2755967820109389
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.8960917211906734	eta = 0.47552711230325256
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.7944965020417399	eta = 0.5363345306638816
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.7881017357421507	eta = 0.5406864230991465
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.7880715228657817	eta = 0.54070715179138
eta = 0.54070715179138
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [0.05073821 0.10671132 0.04993285 0.01731542 0.12322138 0.0587919
 0.02174495 0.07208048 0.05234895 0.04751674]
ene_total = [0.08444145 0.1104692  0.08493497 0.04776518 0.12572503 0.06427522
 0.05189808 0.09632167 0.06897994 0.05326078]
ti_comp = [26.93025964 27.2160528  26.91701177 26.98204373 27.22058811 27.22304297
 26.98284085 27.01864488 27.10909704 27.22621665]
ti_coms = [0.38135093 0.09555778 0.3945988  0.32956684 0.09102246 0.0885676
 0.32876972 0.2929657  0.20251354 0.08539392]
t_total = [25.94966011 25.94966011 25.94966011 25.94966011 25.94966011 25.94966011
 25.94966011 25.94966011 25.94966011 25.94966011]
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.12565281e-08 1.02532430e-07 1.07395299e-08 4.45686613e-10
 1.57813146e-07 1.71379604e-08 8.82632352e-10 3.20631268e-08
 1.22003941e-08 9.04576573e-09]
ene_total = [0.03623342 0.00907936 0.03749214 0.03131323 0.00864849 0.00841512
 0.03123749 0.02783567 0.01924149 0.00811357]
optimize_network iter = 0 obj = 0.21760996983692812
eta = 0.54070715179138
freqs = [ 942029.79379861 1960448.1887988   927533.22358696  320869.35975109
 2263385.64084244 1079818.64242409  402940.3153214  1333902.54699206
  965523.69021721  872628.40406785]
eta_min = 0.5407071517913803	eta_max = 0.9738786232505544
af = 2.5132315156122307e-07	bf = 0.0999465483120546	zeta = 2.764554667173454e-07	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.74483827e-09 1.58932227e-08 1.66470005e-09 6.90844506e-11
 2.44621090e-08 2.65650021e-09 1.36814006e-10 4.97000232e-09
 1.89114391e-09 1.40215510e-09]
ene_total = [0.1830592  0.0458705  0.18941855 0.15820137 0.04369346 0.04251496
 0.15781873 0.14063181 0.09721221 0.04099149]
ti_comp = [5.02451092 5.31030408 5.01126305 5.07629501 5.3148394  5.31729426
 5.07709213 5.11289616 5.20334832 5.32046794]
ti_coms = [0.38135093 0.09555778 0.3945988  0.32956684 0.09102246 0.0885676
 0.32876972 0.2929657  0.20251354 0.08539392]
t_total = [25.94966011 25.94966011 25.94966011 25.94966011 25.94966011 25.94966011
 25.94966011 25.94966011 25.94966011 25.94966011]
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.20434277e-09 1.00305536e-08 1.15397584e-09 4.68962074e-11
 1.54173613e-08 1.67302812e-09 9.28490047e-11 3.33465732e-09
 1.23336131e-09 8.82210910e-10]
ene_total = [0.1830592  0.04587047 0.18941855 0.15820137 0.04369342 0.04251495
 0.15781873 0.14063181 0.09721221 0.04099149]
optimize_network iter = 1 obj = 1.0994121997748156
eta = 0.909090909090909
freqs = [ 782640.11306705 1557441.83511423  772253.41470056  264366.82466173
 1796869.89992426  856934.27544998  331943.42290144 1092625.69941991
  779732.80435494  692177.58267536]
eta_min = 0.9090909090909166	eta_max = 0.9090909090908771
af = 1.605687647826223e-07	bf = 0.0999465483120546	zeta = 1.7662564126088454e-07	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.20434277e-09 1.00305536e-08 1.15397584e-09 4.68962074e-11
 1.54173613e-08 1.67302812e-09 9.28490047e-11 3.33465732e-09
 1.23336131e-09 8.82210910e-10]
ene_total = [0.1830592  0.04587047 0.18941855 0.15820137 0.04369342 0.04251495
 0.15781873 0.14063181 0.09721221 0.04099149]
ti_comp = [5.02451092 5.31030408 5.01126305 5.07629501 5.3148394  5.31729426
 5.07709213 5.11289616 5.20334832 5.32046794]
ti_coms = [0.38135093 0.09555778 0.3945988  0.32956684 0.09102246 0.0885676
 0.32876972 0.2929657  0.20251354 0.08539392]
t_total = [25.94966011 25.94966011 25.94966011 25.94966011 25.94966011 25.94966011
 25.94966011 25.94966011 25.94966011 25.94966011]
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.20434277e-09 1.00305536e-08 1.15397584e-09 4.68962074e-11
 1.54173613e-08 1.67302812e-09 9.28490047e-11 3.33465732e-09
 1.23336131e-09 8.82210910e-10]
ene_total = [0.1830592  0.04587047 0.18941855 0.15820137 0.04369342 0.04251495
 0.15781873 0.14063181 0.09721221 0.04099149]
optimize_network iter = 2 obj = 1.09941219977443
eta = 0.9090909090908771
freqs = [ 782640.11306704 1557441.83511425  772253.41470055  264366.82466173
 1796869.89992429  856934.27544999  331943.42290144 1092625.69941991
  779732.80435495  692177.58267537]
Done!
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.15767328e-09 9.64185965e-09 1.10925813e-09 4.50789326e-11
 1.48199232e-08 1.60819661e-09 8.92510132e-11 3.20543602e-09
 1.18556733e-09 8.48024357e-10]
ene_total = [0.03813509 0.00955579 0.03945988 0.03295668 0.00910226 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
At round 81 energy consumption: 0.22903076238567763
At round 81 eta: 0.9090909090908771
At round 81 a_n: 0.09384340477035735
At round 81 local rounds: 3.120939520578907
At round 81 global rounds: 4.80028176913475
gradient difference: 0.9565939903259277
train() client id: f_00000-0-0 loss: 0.768548  [   32/  126]
train() client id: f_00000-0-1 loss: 0.927073  [   64/  126]
train() client id: f_00000-0-2 loss: 0.976281  [   96/  126]
train() client id: f_00000-1-0 loss: 0.874469  [   32/  126]
train() client id: f_00000-1-1 loss: 0.836236  [   64/  126]
train() client id: f_00000-1-2 loss: 0.766387  [   96/  126]
train() client id: f_00000-2-0 loss: 0.786139  [   32/  126]
train() client id: f_00000-2-1 loss: 0.722081  [   64/  126]
train() client id: f_00000-2-2 loss: 0.741675  [   96/  126]
train() client id: f_00001-0-0 loss: 0.535674  [   32/  265]
train() client id: f_00001-0-1 loss: 0.658337  [   64/  265]
train() client id: f_00001-0-2 loss: 0.494757  [   96/  265]
train() client id: f_00001-0-3 loss: 0.557692  [  128/  265]
train() client id: f_00001-0-4 loss: 0.542115  [  160/  265]
train() client id: f_00001-0-5 loss: 0.695945  [  192/  265]
train() client id: f_00001-0-6 loss: 0.455796  [  224/  265]
train() client id: f_00001-0-7 loss: 0.719258  [  256/  265]
train() client id: f_00001-1-0 loss: 0.570916  [   32/  265]
train() client id: f_00001-1-1 loss: 0.672752  [   64/  265]
train() client id: f_00001-1-2 loss: 0.554042  [   96/  265]
train() client id: f_00001-1-3 loss: 0.536085  [  128/  265]
train() client id: f_00001-1-4 loss: 0.546625  [  160/  265]
train() client id: f_00001-1-5 loss: 0.623384  [  192/  265]
train() client id: f_00001-1-6 loss: 0.634516  [  224/  265]
train() client id: f_00001-1-7 loss: 0.505439  [  256/  265]
train() client id: f_00001-2-0 loss: 0.547390  [   32/  265]
train() client id: f_00001-2-1 loss: 0.728737  [   64/  265]
train() client id: f_00001-2-2 loss: 0.500640  [   96/  265]
train() client id: f_00001-2-3 loss: 0.657758  [  128/  265]
train() client id: f_00001-2-4 loss: 0.532924  [  160/  265]
train() client id: f_00001-2-5 loss: 0.577334  [  192/  265]
train() client id: f_00001-2-6 loss: 0.620072  [  224/  265]
train() client id: f_00001-2-7 loss: 0.470582  [  256/  265]
train() client id: f_00002-0-0 loss: 1.107762  [   32/  124]
train() client id: f_00002-0-1 loss: 0.673663  [   64/  124]
train() client id: f_00002-0-2 loss: 0.907781  [   96/  124]
train() client id: f_00002-1-0 loss: 1.006406  [   32/  124]
train() client id: f_00002-1-1 loss: 0.902967  [   64/  124]
train() client id: f_00002-1-2 loss: 0.888745  [   96/  124]
train() client id: f_00002-2-0 loss: 0.852802  [   32/  124]
train() client id: f_00002-2-1 loss: 0.996081  [   64/  124]
train() client id: f_00002-2-2 loss: 1.014036  [   96/  124]
train() client id: f_00003-0-0 loss: 0.474603  [   32/   43]
train() client id: f_00003-1-0 loss: 0.306890  [   32/   43]
train() client id: f_00003-2-0 loss: 0.445627  [   32/   43]
train() client id: f_00004-0-0 loss: 0.759968  [   32/  306]
train() client id: f_00004-0-1 loss: 0.759538  [   64/  306]
train() client id: f_00004-0-2 loss: 0.763911  [   96/  306]
train() client id: f_00004-0-3 loss: 0.685594  [  128/  306]
train() client id: f_00004-0-4 loss: 0.732034  [  160/  306]
train() client id: f_00004-0-5 loss: 0.745413  [  192/  306]
train() client id: f_00004-0-6 loss: 0.847868  [  224/  306]
train() client id: f_00004-0-7 loss: 0.568340  [  256/  306]
train() client id: f_00004-0-8 loss: 0.781151  [  288/  306]
train() client id: f_00004-1-0 loss: 0.865150  [   32/  306]
train() client id: f_00004-1-1 loss: 0.718607  [   64/  306]
train() client id: f_00004-1-2 loss: 0.786359  [   96/  306]
train() client id: f_00004-1-3 loss: 0.669766  [  128/  306]
train() client id: f_00004-1-4 loss: 0.665042  [  160/  306]
train() client id: f_00004-1-5 loss: 0.775421  [  192/  306]
train() client id: f_00004-1-6 loss: 0.653207  [  224/  306]
train() client id: f_00004-1-7 loss: 0.739953  [  256/  306]
train() client id: f_00004-1-8 loss: 0.913302  [  288/  306]
train() client id: f_00004-2-0 loss: 0.627087  [   32/  306]
train() client id: f_00004-2-1 loss: 0.752090  [   64/  306]
train() client id: f_00004-2-2 loss: 0.776811  [   96/  306]
train() client id: f_00004-2-3 loss: 0.633626  [  128/  306]
train() client id: f_00004-2-4 loss: 0.841553  [  160/  306]
train() client id: f_00004-2-5 loss: 0.889797  [  192/  306]
train() client id: f_00004-2-6 loss: 0.722258  [  224/  306]
train() client id: f_00004-2-7 loss: 0.778287  [  256/  306]
train() client id: f_00004-2-8 loss: 0.607802  [  288/  306]
train() client id: f_00005-0-0 loss: 0.504885  [   32/  146]
train() client id: f_00005-0-1 loss: 0.467256  [   64/  146]
train() client id: f_00005-0-2 loss: 0.294401  [   96/  146]
train() client id: f_00005-0-3 loss: 0.474234  [  128/  146]
train() client id: f_00005-1-0 loss: 0.755172  [   32/  146]
train() client id: f_00005-1-1 loss: 0.394942  [   64/  146]
train() client id: f_00005-1-2 loss: 0.254761  [   96/  146]
train() client id: f_00005-1-3 loss: 0.250805  [  128/  146]
train() client id: f_00005-2-0 loss: 0.315391  [   32/  146]
train() client id: f_00005-2-1 loss: 0.335854  [   64/  146]
train() client id: f_00005-2-2 loss: 0.703964  [   96/  146]
train() client id: f_00005-2-3 loss: 0.442454  [  128/  146]
train() client id: f_00006-0-0 loss: 0.439407  [   32/   54]
train() client id: f_00006-1-0 loss: 0.450191  [   32/   54]
train() client id: f_00006-2-0 loss: 0.445460  [   32/   54]
train() client id: f_00007-0-0 loss: 0.796587  [   32/  179]
train() client id: f_00007-0-1 loss: 0.561445  [   64/  179]
train() client id: f_00007-0-2 loss: 0.896944  [   96/  179]
train() client id: f_00007-0-3 loss: 0.834323  [  128/  179]
train() client id: f_00007-0-4 loss: 0.542527  [  160/  179]
train() client id: f_00007-1-0 loss: 0.811282  [   32/  179]
train() client id: f_00007-1-1 loss: 0.658565  [   64/  179]
train() client id: f_00007-1-2 loss: 0.729050  [   96/  179]
train() client id: f_00007-1-3 loss: 0.648977  [  128/  179]
train() client id: f_00007-1-4 loss: 0.736906  [  160/  179]
train() client id: f_00007-2-0 loss: 0.732632  [   32/  179]
train() client id: f_00007-2-1 loss: 0.637636  [   64/  179]
train() client id: f_00007-2-2 loss: 0.839464  [   96/  179]
train() client id: f_00007-2-3 loss: 0.716123  [  128/  179]
train() client id: f_00007-2-4 loss: 0.663530  [  160/  179]
train() client id: f_00008-0-0 loss: 0.785703  [   32/  130]
train() client id: f_00008-0-1 loss: 0.668691  [   64/  130]
train() client id: f_00008-0-2 loss: 0.598699  [   96/  130]
train() client id: f_00008-0-3 loss: 0.718710  [  128/  130]
train() client id: f_00008-1-0 loss: 0.761935  [   32/  130]
train() client id: f_00008-1-1 loss: 0.688616  [   64/  130]
train() client id: f_00008-1-2 loss: 0.656624  [   96/  130]
train() client id: f_00008-1-3 loss: 0.674471  [  128/  130]
train() client id: f_00008-2-0 loss: 0.750740  [   32/  130]
train() client id: f_00008-2-1 loss: 0.622020  [   64/  130]
train() client id: f_00008-2-2 loss: 0.620431  [   96/  130]
train() client id: f_00008-2-3 loss: 0.785721  [  128/  130]
train() client id: f_00009-0-0 loss: 0.576472  [   32/  118]
train() client id: f_00009-0-1 loss: 0.835649  [   64/  118]
train() client id: f_00009-0-2 loss: 0.911208  [   96/  118]
train() client id: f_00009-1-0 loss: 0.789576  [   32/  118]
train() client id: f_00009-1-1 loss: 0.874558  [   64/  118]
train() client id: f_00009-1-2 loss: 0.763521  [   96/  118]
train() client id: f_00009-2-0 loss: 0.651346  [   32/  118]
train() client id: f_00009-2-1 loss: 0.584582  [   64/  118]
train() client id: f_00009-2-2 loss: 1.024814  [   96/  118]
At round 81 accuracy: 0.6445623342175066
At round 81 training accuracy: 0.5955734406438632
At round 81 training loss: 0.8069663028242122
update_location
xs = [  -3.9056584     4.20031788  425.00902392   18.81129433    0.97929623
    3.95640986 -387.44319194 -366.32485185  409.66397685 -352.06087855]
ys = [ 417.5879595   400.55583871    1.32061395 -387.45517586  379.35018685
  362.81415074   -2.62498432    0.82234798   17.56900603    4.00148178]
dists_uav = [429.4123404  412.87119371 436.61701116 400.5937819  392.31049346
 376.36386803 400.1488692  379.72960556 422.05834182 366.00939068]
dists_bs = [296.85382231 288.07670906 624.64018276 594.91061737 269.68075861
 259.42157167 276.91649437 258.73355676 605.48863217 246.15248922]
uav_gains = [5.76271498e-13 6.51982263e-13 5.47875000e-13 7.20245856e-13
 7.73843588e-13 8.99752934e-13 7.22955533e-13 8.70243248e-13
 6.07950194e-13 1.00281652e-12]
bs_gains = [1.31878706e-11 1.43440329e-11 1.64259431e-12 1.88290873e-12
 1.72549694e-11 1.92343292e-11 1.60220239e-11 1.93778844e-11
 1.79224469e-12 2.22803616e-11]
Round 82
-------------------------------
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.03972093 0.07667447 0.03927574 0.01549837 0.08835219 0.04258691
 0.01863559 0.05403005 0.03910533 0.03455187]
obj_prev = 0.44843144567355053
eta_min = 0.0	eta_max = inf
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.10079758902614538	eta = 0.9090909090909091
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.3375882886973583	eta = 0.2714376502678347
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.19414841544868844	eta = 0.47198001400206313
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.17188140883090305	eta = 0.5331243935293785
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.17048248758463935	eta = 0.5374990307814271
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.17047588964991173	eta = 0.5375198336265012
eta = 0.5375198336265012
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [0.05122607 0.10773737 0.05041296 0.01748191 0.12440617 0.05935719
 0.02195403 0.07277355 0.0528523  0.04797362]
ene_total = [0.01831375 0.02382031 0.0184197  0.01040877 0.02710943 0.01385921
 0.01130017 0.020882   0.01487805 0.0114845 ]
ti_comp = [127.24870803 127.54244623 127.23535515 127.30084038 127.54703912
 127.54954855 127.30163179 127.33761381 127.43428871 127.5527431 ]
ti_coms = [0.39027955 0.09654136 0.40363243 0.3381472  0.09194846 0.08943903
 0.3373558  0.30137378 0.20469887 0.08624449]
t_total = [25.89965591 25.89965591 25.89965591 25.89965591 25.89965591 25.89965591
 25.89965591 25.89965591 25.89965591 25.89965591]
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [5.18855480e-10 4.80473758e-09 4.94641987e-10 2.06055571e-11
 7.39716068e-09 8.03419283e-10 4.08088986e-11 1.48554980e-09
 5.68197076e-10 4.24138750e-10]
ene_total = [0.00791929 0.00195895 0.00819024 0.00686146 0.00186576 0.00181484
 0.0068454  0.00611528 0.00415361 0.00175002]
optimize_network iter = 0 obj = 0.04747485012945179
eta = 0.5375198336265012
freqs = [201283.26753271 422358.89209453 198109.08366522  68663.77799187
 487687.42285783 232682.88526082  86228.39443656 285750.39151138
 207370.78109251 188054.06037245]
eta_min = 0.537519833626502	eta_max = 0.9942699445819028
af = 2.5030539272411267e-09	bf = 0.021956175032555886	zeta = 2.7533593199652398e-09	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [7.96601739e-11 7.37674065e-10 7.59426628e-11 3.16358278e-12
 1.13569024e-09 1.23349415e-10 6.26541318e-12 2.28077296e-10
 8.72356169e-11 6.51182610e-11]
ene_total = [0.04028768 0.00996575 0.04166607 0.03490617 0.00949163 0.00923259
 0.03482448 0.03111014 0.0211306  0.00890282]
ti_comp = [24.69954088 24.99327907 24.686188   24.75167323 24.99787197 25.0003814
 24.75246463 24.78844665 24.88512156 25.00357594]
ti_coms = [0.39027955 0.09654136 0.40363243 0.3381472  0.09194846 0.08943903
 0.3373558  0.30137378 0.20469887 0.08624449]
t_total = [25.89965591 25.89965591 25.89965591 25.89965591 25.89965591 25.89965591
 25.89965591 25.89965591 25.89965591 25.89965591]
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [4.98379232e-11 4.52811249e-10 4.75535621e-11 1.97252403e-12
 6.96922143e-10 7.56817841e-11 3.90634363e-12 1.41868681e-10
 5.39233154e-11 3.99454976e-11]
ene_total = [0.04028768 0.00996575 0.04166607 0.03490617 0.00949163 0.00923259
 0.03482448 0.03111014 0.0211306  0.00890282]
optimize_network iter = 1 obj = 0.24151792697303298
eta = 0.909090909090909
freqs = [159208.78105798 330908.549687   156766.40742107  54218.71812527
 382035.51668375 182260.08745128  68086.44578208 225366.34992481
 163038.03730914 147287.27775164]
eta_min = 0.909090909090918	eta_max = 0.9684934797724077
af = 1.5403473503468153e-09	bf = 0.021956175032555886	zeta = 1.694382085381497e-09	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [4.98379232e-11 4.52811249e-10 4.75535621e-11 1.97252403e-12
 6.96922143e-10 7.56817841e-11 3.90634363e-12 1.41868681e-10
 5.39233154e-11 3.99454976e-11]
ene_total = [0.04028768 0.00996575 0.04166607 0.03490617 0.00949163 0.00923259
 0.03482448 0.03111014 0.0211306  0.00890282]
ti_comp = [24.69954088 24.99327907 24.686188   24.75167323 24.99787197 25.0003814
 24.75246463 24.78844665 24.88512156 25.00357594]
ti_coms = [0.39027955 0.09654136 0.40363243 0.3381472  0.09194846 0.08943903
 0.3373558  0.30137378 0.20469887 0.08624449]
t_total = [25.89965591 25.89965591 25.89965591 25.89965591 25.89965591 25.89965591
 25.89965591 25.89965591 25.89965591 25.89965591]
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [4.98379232e-11 4.52811249e-10 4.75535621e-11 1.97252403e-12
 6.96922143e-10 7.56817841e-11 3.90634363e-12 1.41868681e-10
 5.39233154e-11 3.99454976e-11]
ene_total = [0.04028768 0.00996575 0.04166607 0.03490617 0.00949163 0.00923259
 0.03482448 0.03111014 0.0211306  0.00890282]
optimize_network iter = 2 obj = 0.24151792697305713
eta = 0.909090909090918
freqs = [159208.78105798 330908.549687   156766.40742107  54218.71812527
 382035.51668375 182260.08745128  68086.44578208 225366.34992481
 163038.03730914 147287.27775163]
Done!
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [4.79066540e-11 4.35264361e-10 4.57108141e-11 1.89608676e-12
 6.69915714e-10 7.27490395e-11 3.75496892e-12 1.36371128e-10
 5.18337331e-11 3.83975697e-11]
ene_total = [0.03902796 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
At round 82 energy consumption: 0.23396609732358067
At round 82 eta: 0.909090909090918
At round 82 a_n: -0.2487024421989581
At round 82 local rounds: 3.1209395205774326
At round 82 global rounds: 1.0322774524740326
gradient difference: 0.9645732045173645
train() client id: f_00000-0-0 loss: 0.944816  [   32/  126]
train() client id: f_00000-0-1 loss: 1.183252  [   64/  126]
train() client id: f_00000-0-2 loss: 0.799648  [   96/  126]
train() client id: f_00000-1-0 loss: 0.910900  [   32/  126]
train() client id: f_00000-1-1 loss: 0.985669  [   64/  126]
train() client id: f_00000-1-2 loss: 0.805651  [   96/  126]
train() client id: f_00000-2-0 loss: 0.965994  [   32/  126]
train() client id: f_00000-2-1 loss: 1.011836  [   64/  126]
train() client id: f_00000-2-2 loss: 0.917776  [   96/  126]
train() client id: f_00001-0-0 loss: 0.533959  [   32/  265]
train() client id: f_00001-0-1 loss: 0.698589  [   64/  265]
train() client id: f_00001-0-2 loss: 0.559894  [   96/  265]
train() client id: f_00001-0-3 loss: 0.713241  [  128/  265]
train() client id: f_00001-0-4 loss: 0.571885  [  160/  265]
train() client id: f_00001-0-5 loss: 0.612553  [  192/  265]
train() client id: f_00001-0-6 loss: 0.588744  [  224/  265]
train() client id: f_00001-0-7 loss: 0.507108  [  256/  265]
train() client id: f_00001-1-0 loss: 0.733688  [   32/  265]
train() client id: f_00001-1-1 loss: 0.625103  [   64/  265]
train() client id: f_00001-1-2 loss: 0.548225  [   96/  265]
train() client id: f_00001-1-3 loss: 0.525912  [  128/  265]
train() client id: f_00001-1-4 loss: 0.748108  [  160/  265]
train() client id: f_00001-1-5 loss: 0.496375  [  192/  265]
train() client id: f_00001-1-6 loss: 0.651779  [  224/  265]
train() client id: f_00001-1-7 loss: 0.496856  [  256/  265]
train() client id: f_00001-2-0 loss: 0.584431  [   32/  265]
train() client id: f_00001-2-1 loss: 0.605880  [   64/  265]
train() client id: f_00001-2-2 loss: 0.572141  [   96/  265]
train() client id: f_00001-2-3 loss: 0.519377  [  128/  265]
train() client id: f_00001-2-4 loss: 0.638733  [  160/  265]
train() client id: f_00001-2-5 loss: 0.670089  [  192/  265]
train() client id: f_00001-2-6 loss: 0.577522  [  224/  265]
train() client id: f_00001-2-7 loss: 0.654387  [  256/  265]
train() client id: f_00002-0-0 loss: 0.714064  [   32/  124]
train() client id: f_00002-0-1 loss: 0.856799  [   64/  124]
train() client id: f_00002-0-2 loss: 0.759802  [   96/  124]
train() client id: f_00002-1-0 loss: 0.934396  [   32/  124]
train() client id: f_00002-1-1 loss: 0.650744  [   64/  124]
train() client id: f_00002-1-2 loss: 0.673241  [   96/  124]
train() client id: f_00002-2-0 loss: 0.758894  [   32/  124]
train() client id: f_00002-2-1 loss: 0.868217  [   64/  124]
train() client id: f_00002-2-2 loss: 0.750203  [   96/  124]
train() client id: f_00003-0-0 loss: 0.590024  [   32/   43]
train() client id: f_00003-1-0 loss: 0.706777  [   32/   43]
train() client id: f_00003-2-0 loss: 0.503693  [   32/   43]
train() client id: f_00004-0-0 loss: 0.786758  [   32/  306]
train() client id: f_00004-0-1 loss: 0.906165  [   64/  306]
train() client id: f_00004-0-2 loss: 1.027641  [   96/  306]
train() client id: f_00004-0-3 loss: 0.974698  [  128/  306]
train() client id: f_00004-0-4 loss: 0.871190  [  160/  306]
train() client id: f_00004-0-5 loss: 0.799945  [  192/  306]
train() client id: f_00004-0-6 loss: 1.134622  [  224/  306]
train() client id: f_00004-0-7 loss: 1.123500  [  256/  306]
train() client id: f_00004-0-8 loss: 0.928437  [  288/  306]
train() client id: f_00004-1-0 loss: 0.844445  [   32/  306]
train() client id: f_00004-1-1 loss: 0.957432  [   64/  306]
train() client id: f_00004-1-2 loss: 0.976104  [   96/  306]
train() client id: f_00004-1-3 loss: 0.946001  [  128/  306]
train() client id: f_00004-1-4 loss: 1.169226  [  160/  306]
train() client id: f_00004-1-5 loss: 0.979795  [  192/  306]
train() client id: f_00004-1-6 loss: 0.750087  [  224/  306]
train() client id: f_00004-1-7 loss: 0.999409  [  256/  306]
train() client id: f_00004-1-8 loss: 0.986861  [  288/  306]
train() client id: f_00004-2-0 loss: 1.015456  [   32/  306]
train() client id: f_00004-2-1 loss: 0.924282  [   64/  306]
train() client id: f_00004-2-2 loss: 1.017245  [   96/  306]
train() client id: f_00004-2-3 loss: 1.042471  [  128/  306]
train() client id: f_00004-2-4 loss: 0.855006  [  160/  306]
train() client id: f_00004-2-5 loss: 0.922266  [  192/  306]
train() client id: f_00004-2-6 loss: 0.888606  [  224/  306]
train() client id: f_00004-2-7 loss: 0.971478  [  256/  306]
train() client id: f_00004-2-8 loss: 0.809615  [  288/  306]
train() client id: f_00005-0-0 loss: 0.639039  [   32/  146]
train() client id: f_00005-0-1 loss: 1.012791  [   64/  146]
train() client id: f_00005-0-2 loss: 0.832745  [   96/  146]
train() client id: f_00005-0-3 loss: 0.519218  [  128/  146]
train() client id: f_00005-1-0 loss: 0.501080  [   32/  146]
train() client id: f_00005-1-1 loss: 0.740131  [   64/  146]
train() client id: f_00005-1-2 loss: 0.587283  [   96/  146]
train() client id: f_00005-1-3 loss: 0.748521  [  128/  146]
train() client id: f_00005-2-0 loss: 0.824038  [   32/  146]
train() client id: f_00005-2-1 loss: 0.796700  [   64/  146]
train() client id: f_00005-2-2 loss: 0.830791  [   96/  146]
train() client id: f_00005-2-3 loss: 0.342697  [  128/  146]
train() client id: f_00006-0-0 loss: 0.465150  [   32/   54]
train() client id: f_00006-1-0 loss: 0.508558  [   32/   54]
train() client id: f_00006-2-0 loss: 0.432840  [   32/   54]
train() client id: f_00007-0-0 loss: 0.581730  [   32/  179]
train() client id: f_00007-0-1 loss: 0.846426  [   64/  179]
train() client id: f_00007-0-2 loss: 0.786246  [   96/  179]
train() client id: f_00007-0-3 loss: 0.698601  [  128/  179]
train() client id: f_00007-0-4 loss: 0.658297  [  160/  179]
train() client id: f_00007-1-0 loss: 0.681018  [   32/  179]
train() client id: f_00007-1-1 loss: 0.603347  [   64/  179]
train() client id: f_00007-1-2 loss: 0.683764  [   96/  179]
train() client id: f_00007-1-3 loss: 0.676179  [  128/  179]
train() client id: f_00007-1-4 loss: 0.916283  [  160/  179]
train() client id: f_00007-2-0 loss: 0.621756  [   32/  179]
train() client id: f_00007-2-1 loss: 0.622199  [   64/  179]
train() client id: f_00007-2-2 loss: 0.790053  [   96/  179]
train() client id: f_00007-2-3 loss: 0.879923  [  128/  179]
train() client id: f_00007-2-4 loss: 0.760575  [  160/  179]
train() client id: f_00008-0-0 loss: 0.686552  [   32/  130]
train() client id: f_00008-0-1 loss: 0.514983  [   64/  130]
train() client id: f_00008-0-2 loss: 0.744739  [   96/  130]
train() client id: f_00008-0-3 loss: 0.752894  [  128/  130]
train() client id: f_00008-1-0 loss: 0.588363  [   32/  130]
train() client id: f_00008-1-1 loss: 0.777543  [   64/  130]
train() client id: f_00008-1-2 loss: 0.707099  [   96/  130]
train() client id: f_00008-1-3 loss: 0.631312  [  128/  130]
train() client id: f_00008-2-0 loss: 0.655774  [   32/  130]
train() client id: f_00008-2-1 loss: 0.652609  [   64/  130]
train() client id: f_00008-2-2 loss: 0.766248  [   96/  130]
train() client id: f_00008-2-3 loss: 0.625388  [  128/  130]
train() client id: f_00009-0-0 loss: 0.524573  [   32/  118]
train() client id: f_00009-0-1 loss: 0.439127  [   64/  118]
train() client id: f_00009-0-2 loss: 0.478116  [   96/  118]
train() client id: f_00009-1-0 loss: 0.538844  [   32/  118]
train() client id: f_00009-1-1 loss: 0.550131  [   64/  118]
train() client id: f_00009-1-2 loss: 0.373673  [   96/  118]
train() client id: f_00009-2-0 loss: 0.476195  [   32/  118]
train() client id: f_00009-2-1 loss: 0.483481  [   64/  118]
train() client id: f_00009-2-2 loss: 0.566452  [   96/  118]
At round 82 accuracy: 0.6445623342175066
At round 82 training accuracy: 0.5928906773977196
At round 82 training loss: 0.8199195727972463
Done!
