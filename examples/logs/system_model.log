v = 22.697160301531774
a_0 = 27.840057009285058	a_alpha = -0.3425458469693173
['f_00000', 'f_00001', 'f_00002', 'f_00003', 'f_00004', 'f_00005', 'f_00006', 'f_00007', 'f_00008', 'f_00009']
10
dict_keys(['x', 'y'])
id = f_00000, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 126
id = f_00001, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 265
id = f_00002, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 124
id = f_00003, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 43
id = f_00004, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 306
id = f_00005, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 146
id = f_00006, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 54
id = f_00007, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 179
id = f_00008, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 130
id = f_00009, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 118
BaseFederated generated!
num_samples = [126 265 124  43 306 146  54 179 130 118]
msize = 502400
xs = 6.094342 -20.799682 15.009024 18.811294 -39.020704 -26.043590 2.556808 -6.324852 -0.336023 -17.060879 
ys = 17.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 7.375016 -19.177652 17.569006 -0.998518 
dists_uav = 101.717635 103.318009 101.128704 104.221562 107.749902 104.755057 100.304178 102.018558 101.532177 101.449843 
dists_bs = 239.945225 221.811133 257.425636 246.587507 214.313391 243.153064 244.181399 257.208604 235.142558 236.474617 
uav_gains = -100.184927 -100.354426 -100.121880 -100.448968 -100.810468 -100.504406 -100.032992 -100.217001 -100.165112 -100.156304 
bs_gains = -106.210198 -105.254593 -107.065309 -106.542249 -104.836440 -106.371692 -106.423011 -107.055053 -105.964334 -106.033026 
SystemModel __init__!
t_co_uav = [0.06351163 0.06396501 0.06334461 0.06422068 0.06521719 0.06437154
 0.0631106  0.06359693 0.06345904 0.0634357 ]
t_co_bs = [0.08476871 0.08051968 0.08895491 0.08634835 0.0787878  0.08552995
 0.08577462 0.08890235 0.0836346  0.08394849]
difference = [-0.02125708 -0.01655467 -0.0256103  -0.02212767 -0.01357061 -0.0211584
 -0.02266402 -0.02530541 -0.02017555 -0.0205128 ]
decs_opt = [1 0 1 1 0 0 1 1 0 0]
af = 6.796163711028166	bf = 20.32894796997589	zeta = 7.475780082130983	eta = 0.9090909090909091
af = 6.796163711028166	bf = 20.32894796997589	zeta = 230.74360710778967	eta = 0.02945331312192499
af = 6.796163711028166	bf = 20.32894796997589	zeta = 45.629002909774904	eta = 0.1489439452461113
af = 6.796163711028166	bf = 20.32894796997589	zeta = 39.092727771299245	eta = 0.173847262610764
af = 6.796163711028166	bf = 20.32894796997589	zeta = 38.999286881910415	eta = 0.17426379440236703
af = 6.796163711028166	bf = 20.32894796997589	zeta = 38.99926333319505	eta = 0.17426389962713645
eta_opt = 0.17426389962713645
initialize_feasible_solution eta = 0.17426389962713645, tau = 10.000839233398438
ti_comp = [0.03604337 0.0758055  0.03547125 0.01230051 0.0875339  0.04176454
 0.01544716 0.05120447 0.0371876  0.0337549 ]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [3.35654071 5.27057237 3.33162034 2.5799458  5.60760928 4.29178968
 2.64860954 3.87057992 4.07357378 3.96842191]
system_model train() tau = 30	t0 = 0.050004196166992185	t_min = 10.000839233398438
Round 0
-------------------------------
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.86489914 22.7015787  10.6948057   3.82750737 26.17399534 12.62305496
  4.75731633 15.36016845 11.26073498 10.24387519]
obj_prev = 128.5079361631131
eta_min = 2.1972428731623064e-09	eta_max = 0.9187482009227209
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 29.903120328523933	eta = 0.9090909090909091
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 50.862560519193266	eta = 0.5344727942639534
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 40.950101804889684	eta = 0.6638482847646219
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 39.18020327767913	eta = 0.6938364931761265
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 39.094800455198204	eta = 0.6953521830931377
af = 27.184654844112664	bf = 2.0328947969975895	zeta = 39.094588160812094	eta = 0.6953559590470941
eta = 0.6953559590470941
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [0.02998124 0.06305577 0.02950534 0.01023169 0.07281157 0.03474016
 0.0128491  0.04259239 0.03093302 0.02807767]
ene_total = [3.3202555  6.49821862 3.27523947 1.52191262 7.37393057 3.95636804
 1.75096148 4.4735121  3.5911297  3.33306006]
ti_comp = [0.26476791 0.24775986 0.26493494 0.26405887 0.24949175 0.2427496
 0.26516894 0.26468261 0.24464495 0.24433105]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [30. 30. 30. 30. 30. 30. 30. 30. 30. 30.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.40269086e-05 2.55265831e-04 2.28719695e-05 9.60109636e-07
 3.87586715e-04 4.44691157e-05 1.88561461e-06 6.89326925e-05
 3.09082577e-05 2.31742490e-05]
ene_total = [0.58260011 0.75916097 0.58096823 0.58697181 0.75542628 0.78568377
 0.5769119  0.5874834  0.76712376 0.76928555]
optimize_network iter = 0 obj = 6.7516157792990965
eta = 0.6953559590470941
freqs = [5.66179550e+07 1.27251793e+08 5.56841306e+07 1.93738838e+07
 1.45919802e+08 7.15555497e+07 2.42281409e+07 8.04593666e+07
 6.32202327e+07 5.74582420e+07]
eta_min = 0.6595211103034527	eta_max = 0.6953559590470929
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 0.07249356690995133	eta = 0.909090909090909
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 22.43093651594189	eta = 0.002938051320263521
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 2.424233800512977	eta = 0.027185184296772417
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 2.333929777668074	eta = 0.028237028926918685
af = 0.0659032426454103	bf = 2.0328947969975895	zeta = 2.333881523150504	eta = 0.02823761274584649
eta = 0.02823761274584649
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.35890421e-04 2.50613865e-03 2.24551507e-04 9.42612594e-06
 3.80523333e-03 4.36587104e-04 1.85125116e-05 6.76764628e-04
 3.03449855e-04 2.27519213e-04]
ene_total = [0.18871274 0.3024796  0.18790939 0.18425611 0.33473573 0.25754286
 0.18133617 0.20158775 0.24829862 0.24702256]
ti_comp = [0.30338297 0.28637492 0.30355    0.30267393 0.28810681 0.28136466
 0.303784   0.30329767 0.28326001 0.28294611]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [30. 30. 30. 30. 30. 30. 30. 30. 30. 30.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.75152158e-05 2.87283940e-04 2.61968029e-05 1.09875040e-06
 4.37019626e-04 4.97694879e-05 2.16020605e-06 7.89340962e-05
 3.46659600e-05 2.59825795e-05]
ene_total = [0.52156762 0.68187855 0.52009413 0.52520532 0.6799609  0.70342523
 0.51621531 0.52646951 0.68669249 0.68854913]
optimize_network iter = 1 obj = 6.050058200424808
eta = 0.6595211103034527
freqs = [5.66070564e+07 1.26125273e+08 5.56778794e+07 1.93635364e+07
 1.44763522e+08 7.07252580e+07 2.42281409e+07 8.04405789e+07
 6.25531698e+07 5.68420211e+07]
eta_min = 0.6595211103034622	eta_max = 0.6595211103034516
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 0.07141177968017555	eta = 0.909090909090909
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 22.429905462588092	eta = 0.0028943412096646293
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 2.4193533145705937	eta = 0.026833534117679286
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 2.3303118142037405	eta = 0.02785884674898469
af = 0.06491979970925049	bf = 2.0328947969975895	zeta = 2.3302654452731733	eta = 0.02785940109996355
eta = 0.02785940109996355
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.36690987e-04 2.47126971e-03 2.25349755e-04 9.45165461e-06
 3.75932384e-03 4.28126361e-04 1.85824929e-05 6.79005729e-04
 2.98203014e-04 2.23506966e-04]
ene_total = [0.18866225 0.30136334 0.18785915 0.18418516 0.33329075 0.25720037
 0.18126762 0.2015735  0.24805176 0.24681156]
ti_comp = [0.30338297 0.28637492 0.30355    0.30267393 0.28810681 0.28136466
 0.303784   0.30329767 0.28326001 0.28294611]
ti_coms = [0.06351163 0.08051968 0.06334461 0.06422068 0.0787878  0.08552995
 0.0631106  0.06359693 0.0836346  0.08394849]
t_total = [30. 30. 30. 30. 30. 30. 30. 30. 30. 30.]
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.75152158e-05 2.87283940e-04 2.61968029e-05 1.09875040e-06
 4.37019626e-04 4.97694879e-05 2.16020605e-06 7.89340962e-05
 3.46659600e-05 2.59825795e-05]
ene_total = [0.52156762 0.68187855 0.52009413 0.52520532 0.6799609  0.70342523
 0.51621531 0.52646951 0.68669249 0.68854913]
optimize_network iter = 2 obj = 6.0500582004249726
eta = 0.6595211103034622
freqs = [5.66070564e+07 1.26125273e+08 5.56778794e+07 1.93635364e+07
 1.44763522e+08 7.07252580e+07 2.42281409e+07 8.04405789e+07
 6.25531698e+07 5.68420211e+07]
Done!
ene_coms = [0.00635116 0.00805197 0.00633446 0.00642207 0.00787878 0.00855299
 0.00631106 0.00635969 0.00836346 0.00839485]
ene_comp = [2.62436988e-05 2.74008143e-04 2.49862116e-05 1.04797559e-06
 4.16824332e-04 4.74695695e-05 2.06037988e-06 7.52864401e-05
 3.30639970e-05 2.47818878e-05]
ene_total = [0.00637741 0.00832598 0.00635945 0.00642312 0.0082956  0.00860046
 0.00631312 0.00643498 0.00839652 0.00841963]
At round 0 energy consumption: 0.07394626955167252
At round 0 eta: 0.6595211103034622
At round 0 a_n: 27.840057009285058
At round 0 local rounds: 13.62985484868938
At round 0 global rounds: 81.76735137411414
gradient difference: 1.2180449962615967
train() client id: f_00000-0-0 loss: 0.915154  [   32/  126]
train() client id: f_00000-0-1 loss: 1.024049  [   64/  126]
train() client id: f_00000-0-2 loss: 1.037554  [   96/  126]
train() client id: f_00000-1-0 loss: 0.953083  [   32/  126]
train() client id: f_00000-1-1 loss: 0.913700  [   64/  126]
train() client id: f_00000-1-2 loss: 1.091630  [   96/  126]
train() client id: f_00000-2-0 loss: 0.840723  [   32/  126]
train() client id: f_00000-2-1 loss: 1.119783  [   64/  126]
train() client id: f_00000-2-2 loss: 1.006068  [   96/  126]
train() client id: f_00000-3-0 loss: 0.940316  [   32/  126]
train() client id: f_00000-3-1 loss: 0.998449  [   64/  126]
train() client id: f_00000-3-2 loss: 1.077232  [   96/  126]
train() client id: f_00000-4-0 loss: 0.957064  [   32/  126]
train() client id: f_00000-4-1 loss: 1.022875  [   64/  126]
train() client id: f_00000-4-2 loss: 1.005921  [   96/  126]
train() client id: f_00000-5-0 loss: 0.908290  [   32/  126]
train() client id: f_00000-5-1 loss: 0.987143  [   64/  126]
train() client id: f_00000-5-2 loss: 1.015151  [   96/  126]
train() client id: f_00000-6-0 loss: 1.035977  [   32/  126]
train() client id: f_00000-6-1 loss: 0.947592  [   64/  126]
train() client id: f_00000-6-2 loss: 1.040318  [   96/  126]
train() client id: f_00000-7-0 loss: 1.086987  [   32/  126]
train() client id: f_00000-7-1 loss: 0.945843  [   64/  126]
train() client id: f_00000-7-2 loss: 1.137508  [   96/  126]
train() client id: f_00000-8-0 loss: 1.108787  [   32/  126]
train() client id: f_00000-8-1 loss: 1.068445  [   64/  126]
train() client id: f_00000-8-2 loss: 0.971549  [   96/  126]
train() client id: f_00000-9-0 loss: 1.080107  [   32/  126]
train() client id: f_00000-9-1 loss: 1.196435  [   64/  126]
train() client id: f_00000-9-2 loss: 0.952683  [   96/  126]
train() client id: f_00000-10-0 loss: 1.128314  [   32/  126]
train() client id: f_00000-10-1 loss: 1.083959  [   64/  126]
train() client id: f_00000-10-2 loss: 0.979289  [   96/  126]
train() client id: f_00000-11-0 loss: 1.073194  [   32/  126]
train() client id: f_00000-11-1 loss: 1.100329  [   64/  126]
train() client id: f_00000-11-2 loss: 1.024931  [   96/  126]
train() client id: f_00000-12-0 loss: 1.085278  [   32/  126]
train() client id: f_00000-12-1 loss: 1.069157  [   64/  126]
train() client id: f_00000-12-2 loss: 1.015781  [   96/  126]
train() client id: f_00001-0-0 loss: 0.953391  [   32/  265]
train() client id: f_00001-0-1 loss: 1.074700  [   64/  265]
train() client id: f_00001-0-2 loss: 0.815915  [   96/  265]
train() client id: f_00001-0-3 loss: 1.007392  [  128/  265]
train() client id: f_00001-0-4 loss: 0.954046  [  160/  265]
train() client id: f_00001-0-5 loss: 0.899197  [  192/  265]
train() client id: f_00001-0-6 loss: 0.991891  [  224/  265]
train() client id: f_00001-0-7 loss: 0.935052  [  256/  265]
train() client id: f_00001-1-0 loss: 0.922357  [   32/  265]
train() client id: f_00001-1-1 loss: 1.032122  [   64/  265]
train() client id: f_00001-1-2 loss: 0.954323  [   96/  265]
train() client id: f_00001-1-3 loss: 0.976829  [  128/  265]
train() client id: f_00001-1-4 loss: 0.888005  [  160/  265]
train() client id: f_00001-1-5 loss: 0.836827  [  192/  265]
train() client id: f_00001-1-6 loss: 0.994806  [  224/  265]
train() client id: f_00001-1-7 loss: 1.084189  [  256/  265]
train() client id: f_00001-2-0 loss: 0.985169  [   32/  265]
train() client id: f_00001-2-1 loss: 0.931075  [   64/  265]
train() client id: f_00001-2-2 loss: 1.024337  [   96/  265]
train() client id: f_00001-2-3 loss: 0.939333  [  128/  265]
train() client id: f_00001-2-4 loss: 0.972546  [  160/  265]
train() client id: f_00001-2-5 loss: 1.014132  [  192/  265]
train() client id: f_00001-2-6 loss: 0.919280  [  224/  265]
train() client id: f_00001-2-7 loss: 1.069548  [  256/  265]
train() client id: f_00001-3-0 loss: 1.031788  [   32/  265]
train() client id: f_00001-3-1 loss: 1.008658  [   64/  265]
train() client id: f_00001-3-2 loss: 1.040611  [   96/  265]
train() client id: f_00001-3-3 loss: 0.993258  [  128/  265]
train() client id: f_00001-3-4 loss: 1.055952  [  160/  265]
train() client id: f_00001-3-5 loss: 0.943526  [  192/  265]
train() client id: f_00001-3-6 loss: 0.973910  [  224/  265]
train() client id: f_00001-3-7 loss: 1.008761  [  256/  265]
train() client id: f_00001-4-0 loss: 1.088070  [   32/  265]
train() client id: f_00001-4-1 loss: 0.994473  [   64/  265]
train() client id: f_00001-4-2 loss: 1.030423  [   96/  265]
train() client id: f_00001-4-3 loss: 0.914494  [  128/  265]
train() client id: f_00001-4-4 loss: 1.081943  [  160/  265]
train() client id: f_00001-4-5 loss: 1.077410  [  192/  265]
train() client id: f_00001-4-6 loss: 0.999200  [  224/  265]
train() client id: f_00001-4-7 loss: 0.981909  [  256/  265]
train() client id: f_00001-5-0 loss: 0.952983  [   32/  265]
train() client id: f_00001-5-1 loss: 0.990507  [   64/  265]
train() client id: f_00001-5-2 loss: 1.124300  [   96/  265]
train() client id: f_00001-5-3 loss: 1.068147  [  128/  265]
train() client id: f_00001-5-4 loss: 0.947385  [  160/  265]
train() client id: f_00001-5-5 loss: 1.036836  [  192/  265]
train() client id: f_00001-5-6 loss: 1.087364  [  224/  265]
train() client id: f_00001-5-7 loss: 1.184426  [  256/  265]
train() client id: f_00001-6-0 loss: 1.208444  [   32/  265]
train() client id: f_00001-6-1 loss: 1.053590  [   64/  265]
train() client id: f_00001-6-2 loss: 1.014154  [   96/  265]
train() client id: f_00001-6-3 loss: 1.084456  [  128/  265]
train() client id: f_00001-6-4 loss: 1.075684  [  160/  265]
train() client id: f_00001-6-5 loss: 1.004956  [  192/  265]
train() client id: f_00001-6-6 loss: 1.082352  [  224/  265]
train() client id: f_00001-6-7 loss: 1.074673  [  256/  265]
train() client id: f_00001-7-0 loss: 1.091456  [   32/  265]
train() client id: f_00001-7-1 loss: 1.005782  [   64/  265]
train() client id: f_00001-7-2 loss: 1.054314  [   96/  265]
train() client id: f_00001-7-3 loss: 1.004906  [  128/  265]
train() client id: f_00001-7-4 loss: 1.137760  [  160/  265]
train() client id: f_00001-7-5 loss: 1.042360  [  192/  265]
train() client id: f_00001-7-6 loss: 1.201450  [  224/  265]
train() client id: f_00001-7-7 loss: 1.193758  [  256/  265]
train() client id: f_00001-8-0 loss: 1.024876  [   32/  265]
train() client id: f_00001-8-1 loss: 1.180124  [   64/  265]
train() client id: f_00001-8-2 loss: 1.182948  [   96/  265]
train() client id: f_00001-8-3 loss: 1.111898  [  128/  265]
train() client id: f_00001-8-4 loss: 1.151792  [  160/  265]
train() client id: f_00001-8-5 loss: 1.040267  [  192/  265]
train() client id: f_00001-8-6 loss: 1.105443  [  224/  265]
train() client id: f_00001-8-7 loss: 1.082235  [  256/  265]
train() client id: f_00001-9-0 loss: 1.163144  [   32/  265]
train() client id: f_00001-9-1 loss: 1.102028  [   64/  265]
train() client id: f_00001-9-2 loss: 1.049566  [   96/  265]
train() client id: f_00001-9-3 loss: 1.267190  [  128/  265]
train() client id: f_00001-9-4 loss: 1.090909  [  160/  265]
train() client id: f_00001-9-5 loss: 1.187045  [  192/  265]
train() client id: f_00001-9-6 loss: 1.220462  [  224/  265]
train() client id: f_00001-9-7 loss: 1.074419  [  256/  265]
train() client id: f_00001-10-0 loss: 1.051576  [   32/  265]
train() client id: f_00001-10-1 loss: 1.313710  [   64/  265]
train() client id: f_00001-10-2 loss: 1.101994  [   96/  265]
train() client id: f_00001-10-3 loss: 1.105906  [  128/  265]
train() client id: f_00001-10-4 loss: 1.123671  [  160/  265]
train() client id: f_00001-10-5 loss: 1.315527  [  192/  265]
train() client id: f_00001-10-6 loss: 1.115960  [  224/  265]
train() client id: f_00001-10-7 loss: 1.198471  [  256/  265]
train() client id: f_00001-11-0 loss: 1.084528  [   32/  265]
train() client id: f_00001-11-1 loss: 1.223140  [   64/  265]
train() client id: f_00001-11-2 loss: 1.252226  [   96/  265]
train() client id: f_00001-11-3 loss: 1.251604  [  128/  265]
train() client id: f_00001-11-4 loss: 1.139555  [  160/  265]
train() client id: f_00001-11-5 loss: 1.254957  [  192/  265]
train() client id: f_00001-11-6 loss: 1.057303  [  224/  265]
train() client id: f_00001-11-7 loss: 1.189449  [  256/  265]
train() client id: f_00001-12-0 loss: 1.146554  [   32/  265]
train() client id: f_00001-12-1 loss: 1.195273  [   64/  265]
train() client id: f_00001-12-2 loss: 1.084565  [   96/  265]
train() client id: f_00001-12-3 loss: 1.275063  [  128/  265]
train() client id: f_00001-12-4 loss: 1.198234  [  160/  265]
train() client id: f_00001-12-5 loss: 1.207741  [  192/  265]
train() client id: f_00001-12-6 loss: 1.211153  [  224/  265]
train() client id: f_00001-12-7 loss: 1.247199  [  256/  265]
train() client id: f_00002-0-0 loss: 1.084094  [   32/  124]
train() client id: f_00002-0-1 loss: 1.007647  [   64/  124]
train() client id: f_00002-0-2 loss: 1.031679  [   96/  124]
train() client id: f_00002-1-0 loss: 0.978053  [   32/  124]
train() client id: f_00002-1-1 loss: 1.098888  [   64/  124]
train() client id: f_00002-1-2 loss: 1.110605  [   96/  124]
train() client id: f_00002-2-0 loss: 1.050591  [   32/  124]
train() client id: f_00002-2-1 loss: 0.999529  [   64/  124]
train() client id: f_00002-2-2 loss: 1.200637  [   96/  124]
train() client id: f_00002-3-0 loss: 1.088490  [   32/  124]
train() client id: f_00002-3-1 loss: 1.083827  [   64/  124]
train() client id: f_00002-3-2 loss: 1.085608  [   96/  124]
train() client id: f_00002-4-0 loss: 1.068372  [   32/  124]
train() client id: f_00002-4-1 loss: 1.096821  [   64/  124]
train() client id: f_00002-4-2 loss: 1.119953  [   96/  124]
train() client id: f_00002-5-0 loss: 1.061272  [   32/  124]
train() client id: f_00002-5-1 loss: 1.100825  [   64/  124]
train() client id: f_00002-5-2 loss: 1.172575  [   96/  124]
train() client id: f_00002-6-0 loss: 1.096533  [   32/  124]
train() client id: f_00002-6-1 loss: 1.117991  [   64/  124]
train() client id: f_00002-6-2 loss: 1.123824  [   96/  124]
train() client id: f_00002-7-0 loss: 1.100413  [   32/  124]
train() client id: f_00002-7-1 loss: 1.243055  [   64/  124]
train() client id: f_00002-7-2 loss: 1.057507  [   96/  124]
train() client id: f_00002-8-0 loss: 1.255074  [   32/  124]
train() client id: f_00002-8-1 loss: 1.106546  [   64/  124]
train() client id: f_00002-8-2 loss: 1.033130  [   96/  124]
train() client id: f_00002-9-0 loss: 1.080468  [   32/  124]
train() client id: f_00002-9-1 loss: 1.226805  [   64/  124]
train() client id: f_00002-9-2 loss: 1.095447  [   96/  124]
train() client id: f_00002-10-0 loss: 1.210243  [   32/  124]
train() client id: f_00002-10-1 loss: 1.084389  [   64/  124]
train() client id: f_00002-10-2 loss: 1.080691  [   96/  124]
train() client id: f_00002-11-0 loss: 1.085918  [   32/  124]
train() client id: f_00002-11-1 loss: 1.155743  [   64/  124]
train() client id: f_00002-11-2 loss: 1.153887  [   96/  124]
train() client id: f_00002-12-0 loss: 1.100024  [   32/  124]
train() client id: f_00002-12-1 loss: 1.164723  [   64/  124]
train() client id: f_00002-12-2 loss: 1.263962  [   96/  124]
train() client id: f_00003-0-0 loss: 0.918654  [   32/   43]
train() client id: f_00003-1-0 loss: 1.044880  [   32/   43]
train() client id: f_00003-2-0 loss: 1.021041  [   32/   43]
train() client id: f_00003-3-0 loss: 0.980072  [   32/   43]
train() client id: f_00003-4-0 loss: 1.009820  [   32/   43]
train() client id: f_00003-5-0 loss: 0.989134  [   32/   43]
train() client id: f_00003-6-0 loss: 0.947777  [   32/   43]
train() client id: f_00003-7-0 loss: 1.081183  [   32/   43]
train() client id: f_00003-8-0 loss: 0.991359  [   32/   43]
train() client id: f_00003-9-0 loss: 1.018946  [   32/   43]
train() client id: f_00003-10-0 loss: 1.016216  [   32/   43]
train() client id: f_00003-11-0 loss: 1.090301  [   32/   43]
train() client id: f_00003-12-0 loss: 1.032265  [   32/   43]
train() client id: f_00004-0-0 loss: 1.010215  [   32/  306]
train() client id: f_00004-0-1 loss: 1.090059  [   64/  306]
train() client id: f_00004-0-2 loss: 1.029271  [   96/  306]
train() client id: f_00004-0-3 loss: 1.033229  [  128/  306]
train() client id: f_00004-0-4 loss: 1.101494  [  160/  306]
train() client id: f_00004-0-5 loss: 1.082597  [  192/  306]
train() client id: f_00004-0-6 loss: 1.165408  [  224/  306]
train() client id: f_00004-0-7 loss: 1.155738  [  256/  306]
train() client id: f_00004-0-8 loss: 1.112240  [  288/  306]
train() client id: f_00004-1-0 loss: 1.127492  [   32/  306]
train() client id: f_00004-1-1 loss: 1.103527  [   64/  306]
train() client id: f_00004-1-2 loss: 1.078602  [   96/  306]
train() client id: f_00004-1-3 loss: 1.181465  [  128/  306]
train() client id: f_00004-1-4 loss: 1.028537  [  160/  306]
train() client id: f_00004-1-5 loss: 1.067625  [  192/  306]
train() client id: f_00004-1-6 loss: 1.038098  [  224/  306]
train() client id: f_00004-1-7 loss: 1.171225  [  256/  306]
train() client id: f_00004-1-8 loss: 1.152300  [  288/  306]
train() client id: f_00004-2-0 loss: 1.067921  [   32/  306]
train() client id: f_00004-2-1 loss: 1.108030  [   64/  306]
train() client id: f_00004-2-2 loss: 1.059295  [   96/  306]
train() client id: f_00004-2-3 loss: 1.067691  [  128/  306]
train() client id: f_00004-2-4 loss: 1.244099  [  160/  306]
train() client id: f_00004-2-5 loss: 1.065426  [  192/  306]
train() client id: f_00004-2-6 loss: 1.174587  [  224/  306]
train() client id: f_00004-2-7 loss: 1.156186  [  256/  306]
train() client id: f_00004-2-8 loss: 1.069329  [  288/  306]
train() client id: f_00004-3-0 loss: 1.204860  [   32/  306]
train() client id: f_00004-3-1 loss: 1.097907  [   64/  306]
train() client id: f_00004-3-2 loss: 1.184853  [   96/  306]
train() client id: f_00004-3-3 loss: 1.126408  [  128/  306]
train() client id: f_00004-3-4 loss: 1.062863  [  160/  306]
train() client id: f_00004-3-5 loss: 1.170828  [  192/  306]
train() client id: f_00004-3-6 loss: 1.049928  [  224/  306]
train() client id: f_00004-3-7 loss: 1.096282  [  256/  306]
train() client id: f_00004-3-8 loss: 1.162201  [  288/  306]
train() client id: f_00004-4-0 loss: 1.124816  [   32/  306]
train() client id: f_00004-4-1 loss: 1.092249  [   64/  306]
train() client id: f_00004-4-2 loss: 1.148729  [   96/  306]
train() client id: f_00004-4-3 loss: 0.982135  [  128/  306]
train() client id: f_00004-4-4 loss: 1.131884  [  160/  306]
train() client id: f_00004-4-5 loss: 1.273193  [  192/  306]
train() client id: f_00004-4-6 loss: 1.182167  [  224/  306]
train() client id: f_00004-4-7 loss: 1.165372  [  256/  306]
train() client id: f_00004-4-8 loss: 1.163186  [  288/  306]
train() client id: f_00004-5-0 loss: 1.281227  [   32/  306]
train() client id: f_00004-5-1 loss: 1.049282  [   64/  306]
train() client id: f_00004-5-2 loss: 1.097203  [   96/  306]
train() client id: f_00004-5-3 loss: 1.126688  [  128/  306]
train() client id: f_00004-5-4 loss: 1.035429  [  160/  306]
train() client id: f_00004-5-5 loss: 1.186749  [  192/  306]
train() client id: f_00004-5-6 loss: 1.184062  [  224/  306]
train() client id: f_00004-5-7 loss: 1.142045  [  256/  306]
train() client id: f_00004-5-8 loss: 1.152622  [  288/  306]
train() client id: f_00004-6-0 loss: 1.172651  [   32/  306]
train() client id: f_00004-6-1 loss: 1.238904  [   64/  306]
train() client id: f_00004-6-2 loss: 1.069662  [   96/  306]
train() client id: f_00004-6-3 loss: 1.211274  [  128/  306]
train() client id: f_00004-6-4 loss: 1.126734  [  160/  306]
train() client id: f_00004-6-5 loss: 1.188720  [  192/  306]
train() client id: f_00004-6-6 loss: 1.179917  [  224/  306]
train() client id: f_00004-6-7 loss: 1.153402  [  256/  306]
train() client id: f_00004-6-8 loss: 1.152658  [  288/  306]
train() client id: f_00004-7-0 loss: 1.175835  [   32/  306]
train() client id: f_00004-7-1 loss: 1.134882  [   64/  306]
train() client id: f_00004-7-2 loss: 1.145992  [   96/  306]
train() client id: f_00004-7-3 loss: 1.174234  [  128/  306]
train() client id: f_00004-7-4 loss: 1.146372  [  160/  306]
train() client id: f_00004-7-5 loss: 1.195814  [  192/  306]
train() client id: f_00004-7-6 loss: 1.186068  [  224/  306]
train() client id: f_00004-7-7 loss: 1.213581  [  256/  306]
train() client id: f_00004-7-8 loss: 1.193465  [  288/  306]
train() client id: f_00004-8-0 loss: 1.107868  [   32/  306]
train() client id: f_00004-8-1 loss: 1.191699  [   64/  306]
train() client id: f_00004-8-2 loss: 1.127662  [   96/  306]
train() client id: f_00004-8-3 loss: 1.199785  [  128/  306]
train() client id: f_00004-8-4 loss: 1.100258  [  160/  306]
train() client id: f_00004-8-5 loss: 1.243179  [  192/  306]
train() client id: f_00004-8-6 loss: 1.184172  [  224/  306]
train() client id: f_00004-8-7 loss: 1.220060  [  256/  306]
train() client id: f_00004-8-8 loss: 1.161312  [  288/  306]
train() client id: f_00004-9-0 loss: 1.182883  [   32/  306]
train() client id: f_00004-9-1 loss: 1.254832  [   64/  306]
train() client id: f_00004-9-2 loss: 1.174779  [   96/  306]
train() client id: f_00004-9-3 loss: 1.088712  [  128/  306]
train() client id: f_00004-9-4 loss: 1.093908  [  160/  306]
train() client id: f_00004-9-5 loss: 1.232404  [  192/  306]
train() client id: f_00004-9-6 loss: 1.163625  [  224/  306]
train() client id: f_00004-9-7 loss: 1.228469  [  256/  306]
train() client id: f_00004-9-8 loss: 1.241584  [  288/  306]
train() client id: f_00004-10-0 loss: 1.077825  [   32/  306]
train() client id: f_00004-10-1 loss: 1.122262  [   64/  306]
train() client id: f_00004-10-2 loss: 1.145676  [   96/  306]
train() client id: f_00004-10-3 loss: 1.151231  [  128/  306]
train() client id: f_00004-10-4 loss: 1.221618  [  160/  306]
train() client id: f_00004-10-5 loss: 1.364283  [  192/  306]
train() client id: f_00004-10-6 loss: 1.268805  [  224/  306]
train() client id: f_00004-10-7 loss: 1.200815  [  256/  306]
train() client id: f_00004-10-8 loss: 1.243549  [  288/  306]
train() client id: f_00004-11-0 loss: 1.162267  [   32/  306]
train() client id: f_00004-11-1 loss: 1.262228  [   64/  306]
train() client id: f_00004-11-2 loss: 1.299282  [   96/  306]
train() client id: f_00004-11-3 loss: 1.193905  [  128/  306]
train() client id: f_00004-11-4 loss: 1.264838  [  160/  306]
train() client id: f_00004-11-5 loss: 1.203286  [  192/  306]
train() client id: f_00004-11-6 loss: 1.146482  [  224/  306]
train() client id: f_00004-11-7 loss: 1.220289  [  256/  306]
train() client id: f_00004-11-8 loss: 1.186302  [  288/  306]
train() client id: f_00004-12-0 loss: 1.314563  [   32/  306]
train() client id: f_00004-12-1 loss: 1.206542  [   64/  306]
train() client id: f_00004-12-2 loss: 1.199875  [   96/  306]
train() client id: f_00004-12-3 loss: 1.116347  [  128/  306]
train() client id: f_00004-12-4 loss: 1.107811  [  160/  306]
train() client id: f_00004-12-5 loss: 1.238276  [  192/  306]
train() client id: f_00004-12-6 loss: 1.242398  [  224/  306]
train() client id: f_00004-12-7 loss: 1.270953  [  256/  306]
train() client id: f_00004-12-8 loss: 1.234915  [  288/  306]
train() client id: f_00005-0-0 loss: 1.247884  [   32/  146]
train() client id: f_00005-0-1 loss: 1.123020  [   64/  146]
train() client id: f_00005-0-2 loss: 1.183697  [   96/  146]
train() client id: f_00005-0-3 loss: 1.113910  [  128/  146]
train() client id: f_00005-1-0 loss: 1.378928  [   32/  146]
train() client id: f_00005-1-1 loss: 1.186663  [   64/  146]
train() client id: f_00005-1-2 loss: 1.139774  [   96/  146]
train() client id: f_00005-1-3 loss: 1.106863  [  128/  146]
train() client id: f_00005-2-0 loss: 1.134156  [   32/  146]
train() client id: f_00005-2-1 loss: 1.211580  [   64/  146]
train() client id: f_00005-2-2 loss: 1.204538  [   96/  146]
train() client id: f_00005-2-3 loss: 1.125180  [  128/  146]
train() client id: f_00005-3-0 loss: 1.274081  [   32/  146]
train() client id: f_00005-3-1 loss: 1.242010  [   64/  146]
train() client id: f_00005-3-2 loss: 1.180946  [   96/  146]
train() client id: f_00005-3-3 loss: 1.210477  [  128/  146]
train() client id: f_00005-4-0 loss: 1.150045  [   32/  146]
train() client id: f_00005-4-1 loss: 1.233341  [   64/  146]
train() client id: f_00005-4-2 loss: 1.256552  [   96/  146]
train() client id: f_00005-4-3 loss: 1.145381  [  128/  146]
train() client id: f_00005-5-0 loss: 1.276311  [   32/  146]
train() client id: f_00005-5-1 loss: 1.249197  [   64/  146]
train() client id: f_00005-5-2 loss: 1.239034  [   96/  146]
train() client id: f_00005-5-3 loss: 1.166754  [  128/  146]
train() client id: f_00005-6-0 loss: 1.289248  [   32/  146]
train() client id: f_00005-6-1 loss: 1.196621  [   64/  146]
train() client id: f_00005-6-2 loss: 1.221463  [   96/  146]
train() client id: f_00005-6-3 loss: 1.265227  [  128/  146]
train() client id: f_00005-7-0 loss: 1.343043  [   32/  146]
train() client id: f_00005-7-1 loss: 1.161075  [   64/  146]
train() client id: f_00005-7-2 loss: 1.227934  [   96/  146]
train() client id: f_00005-7-3 loss: 1.388128  [  128/  146]
train() client id: f_00005-8-0 loss: 1.309771  [   32/  146]
train() client id: f_00005-8-1 loss: 1.222908  [   64/  146]
train() client id: f_00005-8-2 loss: 1.248492  [   96/  146]
train() client id: f_00005-8-3 loss: 1.350114  [  128/  146]
train() client id: f_00005-9-0 loss: 1.260631  [   32/  146]
train() client id: f_00005-9-1 loss: 1.282295  [   64/  146]
train() client id: f_00005-9-2 loss: 1.378326  [   96/  146]
train() client id: f_00005-9-3 loss: 1.362523  [  128/  146]
train() client id: f_00005-10-0 loss: 1.381902  [   32/  146]
train() client id: f_00005-10-1 loss: 1.363118  [   64/  146]
train() client id: f_00005-10-2 loss: 1.349108  [   96/  146]
train() client id: f_00005-10-3 loss: 1.240367  [  128/  146]
train() client id: f_00005-11-0 loss: 1.409414  [   32/  146]
train() client id: f_00005-11-1 loss: 1.309019  [   64/  146]
train() client id: f_00005-11-2 loss: 1.464888  [   96/  146]
train() client id: f_00005-11-3 loss: 1.236740  [  128/  146]
train() client id: f_00005-12-0 loss: 1.253490  [   32/  146]
train() client id: f_00005-12-1 loss: 1.193912  [   64/  146]
train() client id: f_00005-12-2 loss: 1.583561  [   96/  146]
train() client id: f_00005-12-3 loss: 1.359762  [  128/  146]
train() client id: f_00006-0-0 loss: 0.923303  [   32/   54]
train() client id: f_00006-1-0 loss: 0.876045  [   32/   54]
train() client id: f_00006-2-0 loss: 0.951702  [   32/   54]
train() client id: f_00006-3-0 loss: 0.896121  [   32/   54]
train() client id: f_00006-4-0 loss: 0.984479  [   32/   54]
train() client id: f_00006-5-0 loss: 0.948518  [   32/   54]
train() client id: f_00006-6-0 loss: 0.858019  [   32/   54]
train() client id: f_00006-7-0 loss: 0.903363  [   32/   54]
train() client id: f_00006-8-0 loss: 0.937765  [   32/   54]
train() client id: f_00006-9-0 loss: 0.918377  [   32/   54]
train() client id: f_00006-10-0 loss: 0.977148  [   32/   54]
train() client id: f_00006-11-0 loss: 0.973651  [   32/   54]
train() client id: f_00006-12-0 loss: 0.970128  [   32/   54]
train() client id: f_00007-0-0 loss: 1.082362  [   32/  179]
train() client id: f_00007-0-1 loss: 1.040050  [   64/  179]
train() client id: f_00007-0-2 loss: 1.077603  [   96/  179]
train() client id: f_00007-0-3 loss: 1.139140  [  128/  179]
train() client id: f_00007-0-4 loss: 1.200512  [  160/  179]
train() client id: f_00007-1-0 loss: 1.078221  [   32/  179]
train() client id: f_00007-1-1 loss: 1.073679  [   64/  179]
train() client id: f_00007-1-2 loss: 1.132921  [   96/  179]
train() client id: f_00007-1-3 loss: 1.109282  [  128/  179]
train() client id: f_00007-1-4 loss: 1.170696  [  160/  179]
train() client id: f_00007-2-0 loss: 1.126760  [   32/  179]
train() client id: f_00007-2-1 loss: 1.052257  [   64/  179]
train() client id: f_00007-2-2 loss: 1.116944  [   96/  179]
train() client id: f_00007-2-3 loss: 1.151248  [  128/  179]
train() client id: f_00007-2-4 loss: 1.229096  [  160/  179]
train() client id: f_00007-3-0 loss: 1.183977  [   32/  179]
train() client id: f_00007-3-1 loss: 1.136236  [   64/  179]
train() client id: f_00007-3-2 loss: 1.150606  [   96/  179]
train() client id: f_00007-3-3 loss: 1.201500  [  128/  179]
train() client id: f_00007-3-4 loss: 1.091213  [  160/  179]
train() client id: f_00007-4-0 loss: 1.246035  [   32/  179]
train() client id: f_00007-4-1 loss: 1.095453  [   64/  179]
train() client id: f_00007-4-2 loss: 1.163018  [   96/  179]
train() client id: f_00007-4-3 loss: 1.108594  [  128/  179]
train() client id: f_00007-4-4 loss: 1.186666  [  160/  179]
train() client id: f_00007-5-0 loss: 1.152583  [   32/  179]
train() client id: f_00007-5-1 loss: 1.179056  [   64/  179]
train() client id: f_00007-5-2 loss: 1.236361  [   96/  179]
train() client id: f_00007-5-3 loss: 1.220033  [  128/  179]
train() client id: f_00007-5-4 loss: 1.167470  [  160/  179]
train() client id: f_00007-6-0 loss: 1.160395  [   32/  179]
train() client id: f_00007-6-1 loss: 1.202526  [   64/  179]
train() client id: f_00007-6-2 loss: 1.228591  [   96/  179]
train() client id: f_00007-6-3 loss: 1.246870  [  128/  179]
train() client id: f_00007-6-4 loss: 1.176473  [  160/  179]
train() client id: f_00007-7-0 loss: 1.257504  [   32/  179]
train() client id: f_00007-7-1 loss: 1.147098  [   64/  179]
train() client id: f_00007-7-2 loss: 1.295044  [   96/  179]
train() client id: f_00007-7-3 loss: 1.211405  [  128/  179]
train() client id: f_00007-7-4 loss: 1.257075  [  160/  179]
train() client id: f_00007-8-0 loss: 1.260241  [   32/  179]
train() client id: f_00007-8-1 loss: 1.248614  [   64/  179]
train() client id: f_00007-8-2 loss: 1.248274  [   96/  179]
train() client id: f_00007-8-3 loss: 1.357416  [  128/  179]
train() client id: f_00007-8-4 loss: 1.192680  [  160/  179]
train() client id: f_00007-9-0 loss: 1.280506  [   32/  179]
train() client id: f_00007-9-1 loss: 1.307789  [   64/  179]
train() client id: f_00007-9-2 loss: 1.314136  [   96/  179]
train() client id: f_00007-9-3 loss: 1.220435  [  128/  179]
train() client id: f_00007-9-4 loss: 1.323161  [  160/  179]
train() client id: f_00007-10-0 loss: 1.275461  [   32/  179]
train() client id: f_00007-10-1 loss: 1.283104  [   64/  179]
train() client id: f_00007-10-2 loss: 1.294271  [   96/  179]
train() client id: f_00007-10-3 loss: 1.244592  [  128/  179]
train() client id: f_00007-10-4 loss: 1.347935  [  160/  179]
train() client id: f_00007-11-0 loss: 1.305825  [   32/  179]
train() client id: f_00007-11-1 loss: 1.357441  [   64/  179]
train() client id: f_00007-11-2 loss: 1.304526  [   96/  179]
train() client id: f_00007-11-3 loss: 1.320010  [  128/  179]
train() client id: f_00007-11-4 loss: 1.342491  [  160/  179]
train() client id: f_00007-12-0 loss: 1.303072  [   32/  179]
train() client id: f_00007-12-1 loss: 1.367937  [   64/  179]
train() client id: f_00007-12-2 loss: 1.434319  [   96/  179]
train() client id: f_00007-12-3 loss: 1.299858  [  128/  179]
train() client id: f_00007-12-4 loss: 1.315953  [  160/  179]
train() client id: f_00008-0-0 loss: 1.112438  [   32/  130]
train() client id: f_00008-0-1 loss: 1.060722  [   64/  130]
train() client id: f_00008-0-2 loss: 1.038401  [   96/  130]
train() client id: f_00008-0-3 loss: 1.140161  [  128/  130]
train() client id: f_00008-1-0 loss: 1.093080  [   32/  130]
train() client id: f_00008-1-1 loss: 0.999325  [   64/  130]
train() client id: f_00008-1-2 loss: 1.162343  [   96/  130]
train() client id: f_00008-1-3 loss: 1.063861  [  128/  130]
train() client id: f_00008-2-0 loss: 1.073009  [   32/  130]
train() client id: f_00008-2-1 loss: 1.006412  [   64/  130]
train() client id: f_00008-2-2 loss: 1.130638  [   96/  130]
train() client id: f_00008-2-3 loss: 1.088476  [  128/  130]
train() client id: f_00008-3-0 loss: 1.051709  [   32/  130]
train() client id: f_00008-3-1 loss: 1.084032  [   64/  130]
train() client id: f_00008-3-2 loss: 1.193762  [   96/  130]
train() client id: f_00008-3-3 loss: 0.975980  [  128/  130]
train() client id: f_00008-4-0 loss: 1.097117  [   32/  130]
train() client id: f_00008-4-1 loss: 1.082170  [   64/  130]
train() client id: f_00008-4-2 loss: 1.060487  [   96/  130]
train() client id: f_00008-4-3 loss: 1.074700  [  128/  130]
train() client id: f_00008-5-0 loss: 1.036486  [   32/  130]
train() client id: f_00008-5-1 loss: 1.136222  [   64/  130]
train() client id: f_00008-5-2 loss: 1.084614  [   96/  130]
train() client id: f_00008-5-3 loss: 1.056761  [  128/  130]
train() client id: f_00008-6-0 loss: 1.085726  [   32/  130]
train() client id: f_00008-6-1 loss: 1.051856  [   64/  130]
train() client id: f_00008-6-2 loss: 1.024003  [   96/  130]
train() client id: f_00008-6-3 loss: 1.174244  [  128/  130]
train() client id: f_00008-7-0 loss: 1.098464  [   32/  130]
train() client id: f_00008-7-1 loss: 1.021067  [   64/  130]
train() client id: f_00008-7-2 loss: 1.088795  [   96/  130]
train() client id: f_00008-7-3 loss: 1.109100  [  128/  130]
train() client id: f_00008-8-0 loss: 1.018854  [   32/  130]
train() client id: f_00008-8-1 loss: 1.194629  [   64/  130]
train() client id: f_00008-8-2 loss: 1.093826  [   96/  130]
train() client id: f_00008-8-3 loss: 1.065806  [  128/  130]
train() client id: f_00008-9-0 loss: 1.174202  [   32/  130]
train() client id: f_00008-9-1 loss: 1.110123  [   64/  130]
train() client id: f_00008-9-2 loss: 1.109004  [   96/  130]
train() client id: f_00008-9-3 loss: 0.988137  [  128/  130]
train() client id: f_00008-10-0 loss: 1.147811  [   32/  130]
train() client id: f_00008-10-1 loss: 1.118171  [   64/  130]
train() client id: f_00008-10-2 loss: 1.033867  [   96/  130]
train() client id: f_00008-10-3 loss: 1.121382  [  128/  130]
train() client id: f_00008-11-0 loss: 1.131076  [   32/  130]
train() client id: f_00008-11-1 loss: 1.058079  [   64/  130]
train() client id: f_00008-11-2 loss: 1.153923  [   96/  130]
train() client id: f_00008-11-3 loss: 1.116436  [  128/  130]
train() client id: f_00008-12-0 loss: 1.067288  [   32/  130]
train() client id: f_00008-12-1 loss: 1.114003  [   64/  130]
train() client id: f_00008-12-2 loss: 1.084825  [   96/  130]
train() client id: f_00008-12-3 loss: 1.210090  [  128/  130]
train() client id: f_00009-0-0 loss: 1.097942  [   32/  118]
train() client id: f_00009-0-1 loss: 0.971297  [   64/  118]
train() client id: f_00009-0-2 loss: 0.946640  [   96/  118]
train() client id: f_00009-1-0 loss: 0.955566  [   32/  118]
train() client id: f_00009-1-1 loss: 1.131222  [   64/  118]
train() client id: f_00009-1-2 loss: 0.911011  [   96/  118]
train() client id: f_00009-2-0 loss: 0.922993  [   32/  118]
train() client id: f_00009-2-1 loss: 0.994737  [   64/  118]
train() client id: f_00009-2-2 loss: 0.919097  [   96/  118]
train() client id: f_00009-3-0 loss: 0.985699  [   32/  118]
train() client id: f_00009-3-1 loss: 0.993773  [   64/  118]
train() client id: f_00009-3-2 loss: 0.958782  [   96/  118]
train() client id: f_00009-4-0 loss: 0.866812  [   32/  118]
train() client id: f_00009-4-1 loss: 0.950904  [   64/  118]
train() client id: f_00009-4-2 loss: 1.207072  [   96/  118]
train() client id: f_00009-5-0 loss: 0.949599  [   32/  118]
train() client id: f_00009-5-1 loss: 0.918411  [   64/  118]
train() client id: f_00009-5-2 loss: 1.016487  [   96/  118]
train() client id: f_00009-6-0 loss: 1.182815  [   32/  118]
train() client id: f_00009-6-1 loss: 0.896303  [   64/  118]
train() client id: f_00009-6-2 loss: 0.925463  [   96/  118]
train() client id: f_00009-7-0 loss: 0.918275  [   32/  118]
train() client id: f_00009-7-1 loss: 1.088060  [   64/  118]
train() client id: f_00009-7-2 loss: 0.989495  [   96/  118]
train() client id: f_00009-8-0 loss: 1.083570  [   32/  118]
train() client id: f_00009-8-1 loss: 1.026687  [   64/  118]
train() client id: f_00009-8-2 loss: 0.906995  [   96/  118]
train() client id: f_00009-9-0 loss: 1.005479  [   32/  118]
train() client id: f_00009-9-1 loss: 1.046751  [   64/  118]
train() client id: f_00009-9-2 loss: 1.031324  [   96/  118]
train() client id: f_00009-10-0 loss: 0.969857  [   32/  118]
train() client id: f_00009-10-1 loss: 0.857500  [   64/  118]
train() client id: f_00009-10-2 loss: 1.058501  [   96/  118]
train() client id: f_00009-11-0 loss: 0.901514  [   32/  118]
train() client id: f_00009-11-1 loss: 1.031407  [   64/  118]
train() client id: f_00009-11-2 loss: 1.031169  [   96/  118]
train() client id: f_00009-12-0 loss: 1.141570  [   32/  118]
train() client id: f_00009-12-1 loss: 0.886522  [   64/  118]
train() client id: f_00009-12-2 loss: 1.014047  [   96/  118]
At round 0 accuracy: 0.246684350132626
At round 0 training accuracy: 0.215962441314554
At round 0 training loss: 1.1942722852947514
update_location
xs = 1.094342 -15.799682 20.009024 18.811294 -34.020704 -21.043590 2.556808 -6.324852 4.663977 -17.060879 
ys = 17.587959 15.555839 1.320614 17.544824 9.350187 -17.185849 2.375016 -14.177652 17.569006 4.001482 
dists_uav = 101.540799 102.428580 101.990711 103.255439 106.041663 103.625219 100.060871 101.197873 101.638687 101.523817 
dists_bs = 236.194343 225.315738 261.137988 249.709732 217.520166 246.247796 247.640476 253.455104 238.880017 232.777274 
uav_gains = -100.166034 -100.260551 -100.214037 -100.347849 -100.636950 -100.386663 -100.006623 -100.129304 -100.176496 -100.164218 
bs_gains = -106.018605 -105.445222 -107.239421 -106.695252 -105.017046 -106.525485 -106.594065 -106.876288 -106.156094 -105.841395 
Round 1
-------------------------------
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.86475814 22.70386864 10.69549308  3.82673858 26.17607366 12.62512831
  4.75712203 15.35951406 11.26321529 10.24142851]
obj_prev = 128.51334030366516
eta_min = 2.311289869413348e-09	eta_max = 0.9179068358889176
af = 27.184654844112664	bf = 2.038244896144109	zeta = 29.903120328523933	eta = 0.9090909090909091
af = 27.184654844112664	bf = 2.038244896144109	zeta = 50.92141160980498	eta = 0.5338550913006942
af = 27.184654844112664	bf = 2.038244896144109	zeta = 40.97475341706718	eta = 0.6634488941863763
af = 27.184654844112664	bf = 2.038244896144109	zeta = 39.19821510905128	eta = 0.693517670855259
af = 27.184654844112664	bf = 2.038244896144109	zeta = 39.11235513228073	eta = 0.6950400903288041
af = 27.184654844112664	bf = 2.038244896144109	zeta = 39.11214097403846	eta = 0.6950438960157429
eta = 0.6950438960157429
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [0.03001828 0.06313368 0.0295418  0.01024433 0.07290153 0.03478308
 0.01286498 0.04264501 0.03097124 0.02811235]
ene_total = [3.31978164 6.5061149  3.27744721 1.51901348 7.38134405 3.96296862
 1.74998819 4.47161385 3.59899574 3.32487331]
ti_comp = [0.26460659 0.24673409 0.26447904 0.26412078 0.24854123 0.24180084
 0.26502657 0.26470385 0.24355148 0.24498963]
ti_coms = [0.06346149 0.08133399 0.06358904 0.06394729 0.07952685 0.08626724
 0.06304151 0.06336423 0.0845166  0.08307845]
t_total = [29.9499958 29.9499958 29.9499958 29.9499958 29.9499958 29.9499958
 29.9499958 29.9499958 29.9499958 29.9499958]
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.41454867e-05 2.58347910e-04 2.30360589e-05 9.63220790e-07
 3.92006301e-04 4.49850892e-05 1.89464606e-06 6.91773975e-05
 3.13021490e-05 2.31353594e-05]
ene_total = [0.58155701 0.76609949 0.58262017 0.58387567 0.76180371 0.79165776
 0.57569161 0.58478017 0.77442671 0.76055195]
optimize_network iter = 0 obj = 6.763064260913875
eta = 0.6950438960157429
freqs = [5.67224657e+07 1.27938699e+08 5.58490306e+07 1.93932720e+07
 1.46658825e+08 7.19250640e+07 2.42711055e+07 8.05523076e+07
 6.35825283e+07 5.73745792e+07]
eta_min = 0.6596845988039914	eta_max = 0.6950438960157337
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 0.07315436494122368	eta = 0.9090909090909091
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 22.49041741434538	eta = 0.0029569912778037635
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 2.432729834459089	eta = 0.02733717784291988
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 2.3416402766936963	eta = 0.028400591154114474
af = 0.06650396812838516	bf = 2.038244896144109	zeta = 2.3415910621216036	eta = 0.028401188065745816
eta = 0.028401188065745816
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.36378699e-04 2.52916596e-03 2.25517659e-04 9.42970754e-06
 3.83765052e-03 4.40393562e-04 1.85481443e-05 6.77230634e-04
 3.06440760e-04 2.26489788e-04]
ene_total = [0.18861483 0.30552365 0.1886691  0.18350388 0.33783864 0.25980791
 0.18116974 0.20096826 0.2509534  0.24454163]
ti_comp = [0.30264569 0.28477319 0.30251814 0.30215989 0.28658033 0.27983995
 0.30306567 0.30274295 0.28159058 0.28302874]
ti_coms = [0.06346149 0.08133399 0.06358904 0.06394729 0.07952685 0.08626724
 0.06304151 0.06336423 0.0845166  0.08307845]
t_total = [29.9499958 29.9499958 29.9499958 29.9499958 29.9499958 29.9499958
 29.9499958 29.9499958 29.9499958 29.9499958]
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.76000760e-05 2.90006048e-04 2.63287260e-05 1.10052366e-06
 4.40899051e-04 5.02234785e-05 2.16658346e-06 7.90821828e-05
 3.50155897e-05 2.59210505e-05]
ene_total = [0.52141495 0.68909041 0.52235439 0.52322131 0.68665088 0.70983183
 0.51589861 0.52483088 0.69426636 0.68175732]
optimize_network iter = 1 obj = 6.069316940771649
eta = 0.6596845988039914
freqs = [5.67111680e+07 1.26759149e+08 5.58345224e+07 1.93849279e+07
 1.45447947e+08 7.10682616e+07 2.42711055e+07 8.05399831e+07
 6.28865503e+07 5.67915891e+07]
eta_min = 0.659684598803993	eta_max = 0.6596845988039878
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 0.072022402521563	eta = 0.9090909090909091
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 22.489338538927885	eta = 0.002911375595591902
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 2.427629129214501	eta = 0.026970722420201213
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 2.3378597367303393	eta = 0.028006347153576604
af = 0.06547491138323909	bf = 2.038244896144109	zeta = 2.337812514296872	eta = 0.028006912865265216
eta = 0.028006912865265216
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.37212058e-04 2.49249066e-03 2.26285293e-04 9.45857837e-06
 3.78935809e-03 4.31651518e-04 1.86209531e-05 6.79681002e-04
 3.00945552e-04 2.22781478e-04]
ene_total = [0.18856219 0.30434926 0.18861456 0.18343028 0.3363184  0.25945213
 0.18109834 0.20095692 0.25069421 0.24433622]
ti_comp = [0.30264569 0.28477319 0.30251814 0.30215989 0.28658033 0.27983995
 0.30306567 0.30274295 0.28159058 0.28302874]
ti_coms = [0.06346149 0.08133399 0.06358904 0.06394729 0.07952685 0.08626724
 0.06304151 0.06336423 0.0845166  0.08307845]
t_total = [29.9499958 29.9499958 29.9499958 29.9499958 29.9499958 29.9499958
 29.9499958 29.9499958 29.9499958 29.9499958]
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.76000760e-05 2.90006048e-04 2.63287260e-05 1.10052366e-06
 4.40899051e-04 5.02234785e-05 2.16658346e-06 7.90821828e-05
 3.50155897e-05 2.59210505e-05]
ene_total = [0.52141495 0.68909041 0.52235439 0.52322131 0.68665088 0.70983183
 0.51589861 0.52483088 0.69426636 0.68175732]
optimize_network iter = 2 obj = 6.069316940771676
eta = 0.659684598803993
freqs = [5.67111680e+07 1.26759149e+08 5.58345224e+07 1.93849279e+07
 1.45447947e+08 7.10682616e+07 2.42711055e+07 8.05399831e+07
 6.28865503e+07 5.67915891e+07]
Done!
ene_coms = [0.00634615 0.0081334  0.0063589  0.00639473 0.00795269 0.00862672
 0.00630415 0.00633642 0.00845166 0.00830784]
ene_comp = [2.63403224e-05 2.76769267e-04 2.51270008e-05 1.05029232e-06
 4.20775042e-04 4.79311221e-05 2.06769383e-06 7.54726250e-05
 3.34173689e-05 2.47379328e-05]
ene_total = [0.00637249 0.00841017 0.00638403 0.00639578 0.00837346 0.00867465
 0.00630622 0.0064119  0.00848508 0.00833258]
At round 1 energy consumption: 0.07414635829145368
At round 1 eta: 0.659684598803993
At round 1 a_n: 27.840057009285058
At round 1 local rounds: 13.621738685881681
At round 1 global rounds: 81.80663264560978
gradient difference: 0.4354081153869629
train() client id: f_00000-0-0 loss: 1.420821  [   32/  126]
train() client id: f_00000-0-1 loss: 1.228583  [   64/  126]
train() client id: f_00000-0-2 loss: 1.256168  [   96/  126]
train() client id: f_00000-1-0 loss: 1.250021  [   32/  126]
train() client id: f_00000-1-1 loss: 1.163367  [   64/  126]
train() client id: f_00000-1-2 loss: 1.185983  [   96/  126]
train() client id: f_00000-2-0 loss: 1.147216  [   32/  126]
train() client id: f_00000-2-1 loss: 1.066338  [   64/  126]
train() client id: f_00000-2-2 loss: 1.094933  [   96/  126]
train() client id: f_00000-3-0 loss: 1.080676  [   32/  126]
train() client id: f_00000-3-1 loss: 1.104011  [   64/  126]
train() client id: f_00000-3-2 loss: 1.010602  [   96/  126]
train() client id: f_00000-4-0 loss: 1.017190  [   32/  126]
train() client id: f_00000-4-1 loss: 0.961008  [   64/  126]
train() client id: f_00000-4-2 loss: 1.070657  [   96/  126]
train() client id: f_00000-5-0 loss: 1.004391  [   32/  126]
train() client id: f_00000-5-1 loss: 0.961435  [   64/  126]
train() client id: f_00000-5-2 loss: 0.976344  [   96/  126]
train() client id: f_00000-6-0 loss: 1.018880  [   32/  126]
train() client id: f_00000-6-1 loss: 0.982277  [   64/  126]
train() client id: f_00000-6-2 loss: 0.879993  [   96/  126]
train() client id: f_00000-7-0 loss: 0.955892  [   32/  126]
train() client id: f_00000-7-1 loss: 0.906565  [   64/  126]
train() client id: f_00000-7-2 loss: 0.847982  [   96/  126]
train() client id: f_00000-8-0 loss: 0.906778  [   32/  126]
train() client id: f_00000-8-1 loss: 0.909082  [   64/  126]
train() client id: f_00000-8-2 loss: 0.929712  [   96/  126]
train() client id: f_00000-9-0 loss: 0.936037  [   32/  126]
train() client id: f_00000-9-1 loss: 0.881193  [   64/  126]
train() client id: f_00000-9-2 loss: 0.940400  [   96/  126]
train() client id: f_00000-10-0 loss: 0.925919  [   32/  126]
train() client id: f_00000-10-1 loss: 0.933762  [   64/  126]
train() client id: f_00000-10-2 loss: 0.865729  [   96/  126]
train() client id: f_00000-11-0 loss: 0.807432  [   32/  126]
train() client id: f_00000-11-1 loss: 0.800555  [   64/  126]
train() client id: f_00000-11-2 loss: 0.940668  [   96/  126]
train() client id: f_00000-12-0 loss: 0.856051  [   32/  126]
train() client id: f_00000-12-1 loss: 0.823459  [   64/  126]
train() client id: f_00000-12-2 loss: 0.827552  [   96/  126]
train() client id: f_00001-0-0 loss: 1.217238  [   32/  265]
train() client id: f_00001-0-1 loss: 1.107072  [   64/  265]
train() client id: f_00001-0-2 loss: 1.142498  [   96/  265]
train() client id: f_00001-0-3 loss: 1.071860  [  128/  265]
train() client id: f_00001-0-4 loss: 1.066053  [  160/  265]
train() client id: f_00001-0-5 loss: 1.058076  [  192/  265]
train() client id: f_00001-0-6 loss: 1.010175  [  224/  265]
train() client id: f_00001-0-7 loss: 1.000950  [  256/  265]
train() client id: f_00001-1-0 loss: 1.000454  [   32/  265]
train() client id: f_00001-1-1 loss: 0.936788  [   64/  265]
train() client id: f_00001-1-2 loss: 0.947012  [   96/  265]
train() client id: f_00001-1-3 loss: 0.895833  [  128/  265]
train() client id: f_00001-1-4 loss: 0.930977  [  160/  265]
train() client id: f_00001-1-5 loss: 0.972451  [  192/  265]
train() client id: f_00001-1-6 loss: 0.844026  [  224/  265]
train() client id: f_00001-1-7 loss: 0.800163  [  256/  265]
train() client id: f_00001-2-0 loss: 0.766771  [   32/  265]
train() client id: f_00001-2-1 loss: 0.877201  [   64/  265]
train() client id: f_00001-2-2 loss: 0.884784  [   96/  265]
train() client id: f_00001-2-3 loss: 0.745994  [  128/  265]
train() client id: f_00001-2-4 loss: 0.840396  [  160/  265]
train() client id: f_00001-2-5 loss: 0.757024  [  192/  265]
train() client id: f_00001-2-6 loss: 0.739025  [  224/  265]
train() client id: f_00001-2-7 loss: 0.767911  [  256/  265]
train() client id: f_00001-3-0 loss: 0.711689  [   32/  265]
train() client id: f_00001-3-1 loss: 0.717996  [   64/  265]
train() client id: f_00001-3-2 loss: 0.700659  [   96/  265]
train() client id: f_00001-3-3 loss: 0.670843  [  128/  265]
train() client id: f_00001-3-4 loss: 0.704753  [  160/  265]
train() client id: f_00001-3-5 loss: 0.756452  [  192/  265]
train() client id: f_00001-3-6 loss: 0.760905  [  224/  265]
train() client id: f_00001-3-7 loss: 0.717474  [  256/  265]
train() client id: f_00001-4-0 loss: 0.688336  [   32/  265]
train() client id: f_00001-4-1 loss: 0.668257  [   64/  265]
train() client id: f_00001-4-2 loss: 0.572827  [   96/  265]
train() client id: f_00001-4-3 loss: 0.740942  [  128/  265]
train() client id: f_00001-4-4 loss: 0.671265  [  160/  265]
train() client id: f_00001-4-5 loss: 0.733186  [  192/  265]
train() client id: f_00001-4-6 loss: 0.589554  [  224/  265]
train() client id: f_00001-4-7 loss: 0.560180  [  256/  265]
train() client id: f_00001-5-0 loss: 0.571865  [   32/  265]
train() client id: f_00001-5-1 loss: 0.692560  [   64/  265]
train() client id: f_00001-5-2 loss: 0.699914  [   96/  265]
train() client id: f_00001-5-3 loss: 0.621393  [  128/  265]
train() client id: f_00001-5-4 loss: 0.530091  [  160/  265]
train() client id: f_00001-5-5 loss: 0.545444  [  192/  265]
train() client id: f_00001-5-6 loss: 0.626187  [  224/  265]
train() client id: f_00001-5-7 loss: 0.552971  [  256/  265]
train() client id: f_00001-6-0 loss: 0.553554  [   32/  265]
train() client id: f_00001-6-1 loss: 0.604034  [   64/  265]
train() client id: f_00001-6-2 loss: 0.504334  [   96/  265]
train() client id: f_00001-6-3 loss: 0.554742  [  128/  265]
train() client id: f_00001-6-4 loss: 0.570909  [  160/  265]
train() client id: f_00001-6-5 loss: 0.630794  [  192/  265]
train() client id: f_00001-6-6 loss: 0.670746  [  224/  265]
train() client id: f_00001-6-7 loss: 0.481365  [  256/  265]
train() client id: f_00001-7-0 loss: 0.531676  [   32/  265]
train() client id: f_00001-7-1 loss: 0.477628  [   64/  265]
train() client id: f_00001-7-2 loss: 0.504349  [   96/  265]
train() client id: f_00001-7-3 loss: 0.599941  [  128/  265]
train() client id: f_00001-7-4 loss: 0.526816  [  160/  265]
train() client id: f_00001-7-5 loss: 0.694516  [  192/  265]
train() client id: f_00001-7-6 loss: 0.493516  [  224/  265]
train() client id: f_00001-7-7 loss: 0.545313  [  256/  265]
train() client id: f_00001-8-0 loss: 0.514611  [   32/  265]
train() client id: f_00001-8-1 loss: 0.563858  [   64/  265]
train() client id: f_00001-8-2 loss: 0.506716  [   96/  265]
train() client id: f_00001-8-3 loss: 0.473596  [  128/  265]
train() client id: f_00001-8-4 loss: 0.556039  [  160/  265]
train() client id: f_00001-8-5 loss: 0.629901  [  192/  265]
train() client id: f_00001-8-6 loss: 0.481742  [  224/  265]
train() client id: f_00001-8-7 loss: 0.415517  [  256/  265]
train() client id: f_00001-9-0 loss: 0.459013  [   32/  265]
train() client id: f_00001-9-1 loss: 0.606477  [   64/  265]
train() client id: f_00001-9-2 loss: 0.499189  [   96/  265]
train() client id: f_00001-9-3 loss: 0.487469  [  128/  265]
train() client id: f_00001-9-4 loss: 0.472987  [  160/  265]
train() client id: f_00001-9-5 loss: 0.456190  [  192/  265]
train() client id: f_00001-9-6 loss: 0.618071  [  224/  265]
train() client id: f_00001-9-7 loss: 0.510714  [  256/  265]
train() client id: f_00001-10-0 loss: 0.411612  [   32/  265]
train() client id: f_00001-10-1 loss: 0.598720  [   64/  265]
train() client id: f_00001-10-2 loss: 0.507309  [   96/  265]
train() client id: f_00001-10-3 loss: 0.453432  [  128/  265]
train() client id: f_00001-10-4 loss: 0.508689  [  160/  265]
train() client id: f_00001-10-5 loss: 0.453460  [  192/  265]
train() client id: f_00001-10-6 loss: 0.368203  [  224/  265]
train() client id: f_00001-10-7 loss: 0.591630  [  256/  265]
train() client id: f_00001-11-0 loss: 0.538327  [   32/  265]
train() client id: f_00001-11-1 loss: 0.482264  [   64/  265]
train() client id: f_00001-11-2 loss: 0.451780  [   96/  265]
train() client id: f_00001-11-3 loss: 0.508006  [  128/  265]
train() client id: f_00001-11-4 loss: 0.403287  [  160/  265]
train() client id: f_00001-11-5 loss: 0.465184  [  192/  265]
train() client id: f_00001-11-6 loss: 0.396079  [  224/  265]
train() client id: f_00001-11-7 loss: 0.636669  [  256/  265]
train() client id: f_00001-12-0 loss: 0.511880  [   32/  265]
train() client id: f_00001-12-1 loss: 0.517833  [   64/  265]
train() client id: f_00001-12-2 loss: 0.437416  [   96/  265]
train() client id: f_00001-12-3 loss: 0.396495  [  128/  265]
train() client id: f_00001-12-4 loss: 0.531749  [  160/  265]
train() client id: f_00001-12-5 loss: 0.522549  [  192/  265]
train() client id: f_00001-12-6 loss: 0.518775  [  224/  265]
train() client id: f_00001-12-7 loss: 0.416509  [  256/  265]
train() client id: f_00002-0-0 loss: 1.228368  [   32/  124]
train() client id: f_00002-0-1 loss: 1.229634  [   64/  124]
train() client id: f_00002-0-2 loss: 1.200834  [   96/  124]
train() client id: f_00002-1-0 loss: 1.119566  [   32/  124]
train() client id: f_00002-1-1 loss: 1.183690  [   64/  124]
train() client id: f_00002-1-2 loss: 1.156187  [   96/  124]
train() client id: f_00002-2-0 loss: 1.139012  [   32/  124]
train() client id: f_00002-2-1 loss: 1.200072  [   64/  124]
train() client id: f_00002-2-2 loss: 1.014088  [   96/  124]
train() client id: f_00002-3-0 loss: 1.054564  [   32/  124]
train() client id: f_00002-3-1 loss: 1.054747  [   64/  124]
train() client id: f_00002-3-2 loss: 1.097946  [   96/  124]
train() client id: f_00002-4-0 loss: 1.038175  [   32/  124]
train() client id: f_00002-4-1 loss: 1.020730  [   64/  124]
train() client id: f_00002-4-2 loss: 1.030627  [   96/  124]
train() client id: f_00002-5-0 loss: 1.005891  [   32/  124]
train() client id: f_00002-5-1 loss: 1.003207  [   64/  124]
train() client id: f_00002-5-2 loss: 1.016683  [   96/  124]
train() client id: f_00002-6-0 loss: 1.034174  [   32/  124]
train() client id: f_00002-6-1 loss: 0.933544  [   64/  124]
train() client id: f_00002-6-2 loss: 0.914227  [   96/  124]
train() client id: f_00002-7-0 loss: 0.976587  [   32/  124]
train() client id: f_00002-7-1 loss: 0.982551  [   64/  124]
train() client id: f_00002-7-2 loss: 0.860783  [   96/  124]
train() client id: f_00002-8-0 loss: 0.965400  [   32/  124]
train() client id: f_00002-8-1 loss: 0.960613  [   64/  124]
train() client id: f_00002-8-2 loss: 0.881762  [   96/  124]
train() client id: f_00002-9-0 loss: 0.789503  [   32/  124]
train() client id: f_00002-9-1 loss: 0.932986  [   64/  124]
train() client id: f_00002-9-2 loss: 0.958741  [   96/  124]
train() client id: f_00002-10-0 loss: 0.924207  [   32/  124]
train() client id: f_00002-10-1 loss: 0.987500  [   64/  124]
train() client id: f_00002-10-2 loss: 0.835088  [   96/  124]
train() client id: f_00002-11-0 loss: 0.827054  [   32/  124]
train() client id: f_00002-11-1 loss: 0.849577  [   64/  124]
train() client id: f_00002-11-2 loss: 0.946822  [   96/  124]
train() client id: f_00002-12-0 loss: 0.887239  [   32/  124]
train() client id: f_00002-12-1 loss: 0.900084  [   64/  124]
train() client id: f_00002-12-2 loss: 0.877384  [   96/  124]
train() client id: f_00003-0-0 loss: 1.050405  [   32/   43]
train() client id: f_00003-1-0 loss: 1.006367  [   32/   43]
train() client id: f_00003-2-0 loss: 0.993292  [   32/   43]
train() client id: f_00003-3-0 loss: 1.005363  [   32/   43]
train() client id: f_00003-4-0 loss: 1.077152  [   32/   43]
train() client id: f_00003-5-0 loss: 1.034562  [   32/   43]
train() client id: f_00003-6-0 loss: 1.020371  [   32/   43]
train() client id: f_00003-7-0 loss: 1.036669  [   32/   43]
train() client id: f_00003-8-0 loss: 1.071984  [   32/   43]
train() client id: f_00003-9-0 loss: 1.077724  [   32/   43]
train() client id: f_00003-10-0 loss: 1.024044  [   32/   43]
train() client id: f_00003-11-0 loss: 1.007045  [   32/   43]
train() client id: f_00003-12-0 loss: 1.027912  [   32/   43]
train() client id: f_00004-0-0 loss: 1.022245  [   32/  306]
train() client id: f_00004-0-1 loss: 1.088460  [   64/  306]
train() client id: f_00004-0-2 loss: 1.075545  [   96/  306]
train() client id: f_00004-0-3 loss: 1.004464  [  128/  306]
train() client id: f_00004-0-4 loss: 0.993775  [  160/  306]
train() client id: f_00004-0-5 loss: 1.046392  [  192/  306]
train() client id: f_00004-0-6 loss: 1.035090  [  224/  306]
train() client id: f_00004-0-7 loss: 1.020759  [  256/  306]
train() client id: f_00004-0-8 loss: 1.065013  [  288/  306]
train() client id: f_00004-1-0 loss: 0.961953  [   32/  306]
train() client id: f_00004-1-1 loss: 1.053907  [   64/  306]
train() client id: f_00004-1-2 loss: 1.077907  [   96/  306]
train() client id: f_00004-1-3 loss: 0.985345  [  128/  306]
train() client id: f_00004-1-4 loss: 1.038769  [  160/  306]
train() client id: f_00004-1-5 loss: 1.001816  [  192/  306]
train() client id: f_00004-1-6 loss: 1.014616  [  224/  306]
train() client id: f_00004-1-7 loss: 1.060350  [  256/  306]
train() client id: f_00004-1-8 loss: 0.987763  [  288/  306]
train() client id: f_00004-2-0 loss: 1.015624  [   32/  306]
train() client id: f_00004-2-1 loss: 0.965613  [   64/  306]
train() client id: f_00004-2-2 loss: 0.974750  [   96/  306]
train() client id: f_00004-2-3 loss: 0.994514  [  128/  306]
train() client id: f_00004-2-4 loss: 0.989336  [  160/  306]
train() client id: f_00004-2-5 loss: 0.996711  [  192/  306]
train() client id: f_00004-2-6 loss: 1.033821  [  224/  306]
train() client id: f_00004-2-7 loss: 1.035490  [  256/  306]
train() client id: f_00004-2-8 loss: 0.944847  [  288/  306]
train() client id: f_00004-3-0 loss: 0.981917  [   32/  306]
train() client id: f_00004-3-1 loss: 1.016830  [   64/  306]
train() client id: f_00004-3-2 loss: 1.062869  [   96/  306]
train() client id: f_00004-3-3 loss: 0.926243  [  128/  306]
train() client id: f_00004-3-4 loss: 0.958115  [  160/  306]
train() client id: f_00004-3-5 loss: 0.982980  [  192/  306]
train() client id: f_00004-3-6 loss: 0.997233  [  224/  306]
train() client id: f_00004-3-7 loss: 1.007461  [  256/  306]
train() client id: f_00004-3-8 loss: 0.916227  [  288/  306]
train() client id: f_00004-4-0 loss: 1.115493  [   32/  306]
train() client id: f_00004-4-1 loss: 0.958454  [   64/  306]
train() client id: f_00004-4-2 loss: 1.071187  [   96/  306]
train() client id: f_00004-4-3 loss: 0.956117  [  128/  306]
train() client id: f_00004-4-4 loss: 0.905555  [  160/  306]
train() client id: f_00004-4-5 loss: 0.972039  [  192/  306]
train() client id: f_00004-4-6 loss: 0.925121  [  224/  306]
train() client id: f_00004-4-7 loss: 0.935655  [  256/  306]
train() client id: f_00004-4-8 loss: 0.911987  [  288/  306]
train() client id: f_00004-5-0 loss: 0.983662  [   32/  306]
train() client id: f_00004-5-1 loss: 0.882696  [   64/  306]
train() client id: f_00004-5-2 loss: 0.971268  [   96/  306]
train() client id: f_00004-5-3 loss: 1.007330  [  128/  306]
train() client id: f_00004-5-4 loss: 0.943413  [  160/  306]
train() client id: f_00004-5-5 loss: 1.011133  [  192/  306]
train() client id: f_00004-5-6 loss: 0.998081  [  224/  306]
train() client id: f_00004-5-7 loss: 0.877354  [  256/  306]
train() client id: f_00004-5-8 loss: 0.950102  [  288/  306]
train() client id: f_00004-6-0 loss: 1.022136  [   32/  306]
train() client id: f_00004-6-1 loss: 0.937125  [   64/  306]
train() client id: f_00004-6-2 loss: 0.956795  [   96/  306]
train() client id: f_00004-6-3 loss: 0.886374  [  128/  306]
train() client id: f_00004-6-4 loss: 0.913001  [  160/  306]
train() client id: f_00004-6-5 loss: 0.977137  [  192/  306]
train() client id: f_00004-6-6 loss: 1.026831  [  224/  306]
train() client id: f_00004-6-7 loss: 0.947689  [  256/  306]
train() client id: f_00004-6-8 loss: 0.987588  [  288/  306]
train() client id: f_00004-7-0 loss: 0.981314  [   32/  306]
train() client id: f_00004-7-1 loss: 1.003545  [   64/  306]
train() client id: f_00004-7-2 loss: 0.950844  [   96/  306]
train() client id: f_00004-7-3 loss: 0.911875  [  128/  306]
train() client id: f_00004-7-4 loss: 0.957309  [  160/  306]
train() client id: f_00004-7-5 loss: 1.002166  [  192/  306]
train() client id: f_00004-7-6 loss: 0.972349  [  224/  306]
train() client id: f_00004-7-7 loss: 1.057181  [  256/  306]
train() client id: f_00004-7-8 loss: 0.879185  [  288/  306]
train() client id: f_00004-8-0 loss: 0.969440  [   32/  306]
train() client id: f_00004-8-1 loss: 0.943359  [   64/  306]
train() client id: f_00004-8-2 loss: 0.927845  [   96/  306]
train() client id: f_00004-8-3 loss: 0.972678  [  128/  306]
train() client id: f_00004-8-4 loss: 0.950364  [  160/  306]
train() client id: f_00004-8-5 loss: 0.926712  [  192/  306]
train() client id: f_00004-8-6 loss: 1.022944  [  224/  306]
train() client id: f_00004-8-7 loss: 1.025172  [  256/  306]
train() client id: f_00004-8-8 loss: 0.887987  [  288/  306]
train() client id: f_00004-9-0 loss: 0.995455  [   32/  306]
train() client id: f_00004-9-1 loss: 0.981255  [   64/  306]
train() client id: f_00004-9-2 loss: 0.976586  [   96/  306]
train() client id: f_00004-9-3 loss: 0.994637  [  128/  306]
train() client id: f_00004-9-4 loss: 0.910161  [  160/  306]
train() client id: f_00004-9-5 loss: 0.903296  [  192/  306]
train() client id: f_00004-9-6 loss: 0.957980  [  224/  306]
train() client id: f_00004-9-7 loss: 0.972420  [  256/  306]
train() client id: f_00004-9-8 loss: 1.007071  [  288/  306]
train() client id: f_00004-10-0 loss: 0.936658  [   32/  306]
train() client id: f_00004-10-1 loss: 0.882253  [   64/  306]
train() client id: f_00004-10-2 loss: 0.931843  [   96/  306]
train() client id: f_00004-10-3 loss: 1.036532  [  128/  306]
train() client id: f_00004-10-4 loss: 0.989070  [  160/  306]
train() client id: f_00004-10-5 loss: 0.908594  [  192/  306]
train() client id: f_00004-10-6 loss: 0.924961  [  224/  306]
train() client id: f_00004-10-7 loss: 1.013675  [  256/  306]
train() client id: f_00004-10-8 loss: 0.999925  [  288/  306]
train() client id: f_00004-11-0 loss: 1.022918  [   32/  306]
train() client id: f_00004-11-1 loss: 0.936300  [   64/  306]
train() client id: f_00004-11-2 loss: 0.972094  [   96/  306]
train() client id: f_00004-11-3 loss: 1.000516  [  128/  306]
train() client id: f_00004-11-4 loss: 0.873788  [  160/  306]
train() client id: f_00004-11-5 loss: 0.968898  [  192/  306]
train() client id: f_00004-11-6 loss: 0.960087  [  224/  306]
train() client id: f_00004-11-7 loss: 1.052980  [  256/  306]
train() client id: f_00004-11-8 loss: 0.862298  [  288/  306]
train() client id: f_00004-12-0 loss: 0.949601  [   32/  306]
train() client id: f_00004-12-1 loss: 0.921326  [   64/  306]
train() client id: f_00004-12-2 loss: 0.986914  [   96/  306]
train() client id: f_00004-12-3 loss: 0.947188  [  128/  306]
train() client id: f_00004-12-4 loss: 0.946660  [  160/  306]
train() client id: f_00004-12-5 loss: 0.945594  [  192/  306]
train() client id: f_00004-12-6 loss: 0.963501  [  224/  306]
train() client id: f_00004-12-7 loss: 1.083469  [  256/  306]
train() client id: f_00004-12-8 loss: 0.896912  [  288/  306]
train() client id: f_00005-0-0 loss: 1.119304  [   32/  146]
train() client id: f_00005-0-1 loss: 1.146136  [   64/  146]
train() client id: f_00005-0-2 loss: 1.100002  [   96/  146]
train() client id: f_00005-0-3 loss: 1.112286  [  128/  146]
train() client id: f_00005-1-0 loss: 1.106416  [   32/  146]
train() client id: f_00005-1-1 loss: 1.083590  [   64/  146]
train() client id: f_00005-1-2 loss: 1.044368  [   96/  146]
train() client id: f_00005-1-3 loss: 1.038909  [  128/  146]
train() client id: f_00005-2-0 loss: 1.060313  [   32/  146]
train() client id: f_00005-2-1 loss: 1.045194  [   64/  146]
train() client id: f_00005-2-2 loss: 0.993296  [   96/  146]
train() client id: f_00005-2-3 loss: 1.000275  [  128/  146]
train() client id: f_00005-3-0 loss: 0.925326  [   32/  146]
train() client id: f_00005-3-1 loss: 1.025299  [   64/  146]
train() client id: f_00005-3-2 loss: 1.026744  [   96/  146]
train() client id: f_00005-3-3 loss: 1.050543  [  128/  146]
train() client id: f_00005-4-0 loss: 1.011852  [   32/  146]
train() client id: f_00005-4-1 loss: 0.952426  [   64/  146]
train() client id: f_00005-4-2 loss: 0.999736  [   96/  146]
train() client id: f_00005-4-3 loss: 0.960530  [  128/  146]
train() client id: f_00005-5-0 loss: 0.976699  [   32/  146]
train() client id: f_00005-5-1 loss: 0.927993  [   64/  146]
train() client id: f_00005-5-2 loss: 0.905659  [   96/  146]
train() client id: f_00005-5-3 loss: 1.012207  [  128/  146]
train() client id: f_00005-6-0 loss: 0.959445  [   32/  146]
train() client id: f_00005-6-1 loss: 0.923786  [   64/  146]
train() client id: f_00005-6-2 loss: 0.869830  [   96/  146]
train() client id: f_00005-6-3 loss: 0.951827  [  128/  146]
train() client id: f_00005-7-0 loss: 0.887872  [   32/  146]
train() client id: f_00005-7-1 loss: 0.999855  [   64/  146]
train() client id: f_00005-7-2 loss: 0.869594  [   96/  146]
train() client id: f_00005-7-3 loss: 0.889104  [  128/  146]
train() client id: f_00005-8-0 loss: 0.890230  [   32/  146]
train() client id: f_00005-8-1 loss: 0.831401  [   64/  146]
train() client id: f_00005-8-2 loss: 0.895134  [   96/  146]
train() client id: f_00005-8-3 loss: 0.971336  [  128/  146]
train() client id: f_00005-9-0 loss: 0.867568  [   32/  146]
train() client id: f_00005-9-1 loss: 0.930881  [   64/  146]
train() client id: f_00005-9-2 loss: 0.893742  [   96/  146]
train() client id: f_00005-9-3 loss: 0.817204  [  128/  146]
train() client id: f_00005-10-0 loss: 0.805620  [   32/  146]
train() client id: f_00005-10-1 loss: 0.873813  [   64/  146]
train() client id: f_00005-10-2 loss: 0.918250  [   96/  146]
train() client id: f_00005-10-3 loss: 0.905131  [  128/  146]
train() client id: f_00005-11-0 loss: 0.899747  [   32/  146]
train() client id: f_00005-11-1 loss: 0.916020  [   64/  146]
train() client id: f_00005-11-2 loss: 0.842887  [   96/  146]
train() client id: f_00005-11-3 loss: 0.952737  [  128/  146]
train() client id: f_00005-12-0 loss: 0.863845  [   32/  146]
train() client id: f_00005-12-1 loss: 0.821711  [   64/  146]
train() client id: f_00005-12-2 loss: 0.952630  [   96/  146]
train() client id: f_00005-12-3 loss: 0.849107  [  128/  146]
train() client id: f_00006-0-0 loss: 0.986211  [   32/   54]
train() client id: f_00006-1-0 loss: 0.986041  [   32/   54]
train() client id: f_00006-2-0 loss: 1.010897  [   32/   54]
train() client id: f_00006-3-0 loss: 0.980163  [   32/   54]
train() client id: f_00006-4-0 loss: 1.006922  [   32/   54]
train() client id: f_00006-5-0 loss: 1.066872  [   32/   54]
train() client id: f_00006-6-0 loss: 0.986541  [   32/   54]
train() client id: f_00006-7-0 loss: 1.032580  [   32/   54]
train() client id: f_00006-8-0 loss: 1.004373  [   32/   54]
train() client id: f_00006-9-0 loss: 1.017403  [   32/   54]
train() client id: f_00006-10-0 loss: 0.972161  [   32/   54]
train() client id: f_00006-11-0 loss: 1.010996  [   32/   54]
train() client id: f_00006-12-0 loss: 1.025448  [   32/   54]
train() client id: f_00007-0-0 loss: 1.271679  [   32/  179]
train() client id: f_00007-0-1 loss: 1.284497  [   64/  179]
train() client id: f_00007-0-2 loss: 1.176825  [   96/  179]
train() client id: f_00007-0-3 loss: 1.127212  [  128/  179]
train() client id: f_00007-0-4 loss: 1.161160  [  160/  179]
train() client id: f_00007-1-0 loss: 1.156561  [   32/  179]
train() client id: f_00007-1-1 loss: 1.161128  [   64/  179]
train() client id: f_00007-1-2 loss: 0.996605  [   96/  179]
train() client id: f_00007-1-3 loss: 1.099987  [  128/  179]
train() client id: f_00007-1-4 loss: 1.065644  [  160/  179]
train() client id: f_00007-2-0 loss: 0.993367  [   32/  179]
train() client id: f_00007-2-1 loss: 1.059138  [   64/  179]
train() client id: f_00007-2-2 loss: 1.051988  [   96/  179]
train() client id: f_00007-2-3 loss: 0.972841  [  128/  179]
train() client id: f_00007-2-4 loss: 0.966943  [  160/  179]
train() client id: f_00007-3-0 loss: 0.882309  [   32/  179]
train() client id: f_00007-3-1 loss: 1.006145  [   64/  179]
train() client id: f_00007-3-2 loss: 0.910367  [   96/  179]
train() client id: f_00007-3-3 loss: 0.929422  [  128/  179]
train() client id: f_00007-3-4 loss: 0.927231  [  160/  179]
train() client id: f_00007-4-0 loss: 0.897677  [   32/  179]
train() client id: f_00007-4-1 loss: 0.918104  [   64/  179]
train() client id: f_00007-4-2 loss: 0.876681  [   96/  179]
train() client id: f_00007-4-3 loss: 0.851121  [  128/  179]
train() client id: f_00007-4-4 loss: 0.899755  [  160/  179]
train() client id: f_00007-5-0 loss: 0.788271  [   32/  179]
train() client id: f_00007-5-1 loss: 0.917468  [   64/  179]
train() client id: f_00007-5-2 loss: 0.874625  [   96/  179]
train() client id: f_00007-5-3 loss: 0.814903  [  128/  179]
train() client id: f_00007-5-4 loss: 0.802592  [  160/  179]
train() client id: f_00007-6-0 loss: 0.864970  [   32/  179]
train() client id: f_00007-6-1 loss: 0.763357  [   64/  179]
train() client id: f_00007-6-2 loss: 0.803806  [   96/  179]
train() client id: f_00007-6-3 loss: 0.775767  [  128/  179]
train() client id: f_00007-6-4 loss: 0.812699  [  160/  179]
train() client id: f_00007-7-0 loss: 0.789140  [   32/  179]
train() client id: f_00007-7-1 loss: 0.748311  [   64/  179]
train() client id: f_00007-7-2 loss: 0.750616  [   96/  179]
train() client id: f_00007-7-3 loss: 0.784217  [  128/  179]
train() client id: f_00007-7-4 loss: 0.806235  [  160/  179]
train() client id: f_00007-8-0 loss: 0.876917  [   32/  179]
train() client id: f_00007-8-1 loss: 0.754215  [   64/  179]
train() client id: f_00007-8-2 loss: 0.681505  [   96/  179]
train() client id: f_00007-8-3 loss: 0.776254  [  128/  179]
train() client id: f_00007-8-4 loss: 0.722914  [  160/  179]
train() client id: f_00007-9-0 loss: 0.755386  [   32/  179]
train() client id: f_00007-9-1 loss: 0.686990  [   64/  179]
train() client id: f_00007-9-2 loss: 0.742432  [   96/  179]
train() client id: f_00007-9-3 loss: 0.751086  [  128/  179]
train() client id: f_00007-9-4 loss: 0.756775  [  160/  179]
train() client id: f_00007-10-0 loss: 0.695925  [   32/  179]
train() client id: f_00007-10-1 loss: 0.820295  [   64/  179]
train() client id: f_00007-10-2 loss: 0.667531  [   96/  179]
train() client id: f_00007-10-3 loss: 0.691830  [  128/  179]
train() client id: f_00007-10-4 loss: 0.664634  [  160/  179]
train() client id: f_00007-11-0 loss: 0.725065  [   32/  179]
train() client id: f_00007-11-1 loss: 0.632784  [   64/  179]
train() client id: f_00007-11-2 loss: 0.720023  [   96/  179]
train() client id: f_00007-11-3 loss: 0.815969  [  128/  179]
train() client id: f_00007-11-4 loss: 0.691100  [  160/  179]
train() client id: f_00007-12-0 loss: 0.687487  [   32/  179]
train() client id: f_00007-12-1 loss: 0.848843  [   64/  179]
train() client id: f_00007-12-2 loss: 0.705360  [   96/  179]
train() client id: f_00007-12-3 loss: 0.676103  [  128/  179]
train() client id: f_00007-12-4 loss: 0.696606  [  160/  179]
train() client id: f_00008-0-0 loss: 1.106795  [   32/  130]
train() client id: f_00008-0-1 loss: 1.081794  [   64/  130]
train() client id: f_00008-0-2 loss: 0.997467  [   96/  130]
train() client id: f_00008-0-3 loss: 1.133722  [  128/  130]
train() client id: f_00008-1-0 loss: 0.990816  [   32/  130]
train() client id: f_00008-1-1 loss: 0.979120  [   64/  130]
train() client id: f_00008-1-2 loss: 1.098086  [   96/  130]
train() client id: f_00008-1-3 loss: 1.081554  [  128/  130]
train() client id: f_00008-2-0 loss: 0.951058  [   32/  130]
train() client id: f_00008-2-1 loss: 1.055508  [   64/  130]
train() client id: f_00008-2-2 loss: 1.036488  [   96/  130]
train() client id: f_00008-2-3 loss: 0.976591  [  128/  130]
train() client id: f_00008-3-0 loss: 0.972948  [   32/  130]
train() client id: f_00008-3-1 loss: 1.038452  [   64/  130]
train() client id: f_00008-3-2 loss: 0.948370  [   96/  130]
train() client id: f_00008-3-3 loss: 0.962683  [  128/  130]
train() client id: f_00008-4-0 loss: 0.950802  [   32/  130]
train() client id: f_00008-4-1 loss: 0.956980  [   64/  130]
train() client id: f_00008-4-2 loss: 0.982157  [   96/  130]
train() client id: f_00008-4-3 loss: 0.936175  [  128/  130]
train() client id: f_00008-5-0 loss: 0.982673  [   32/  130]
train() client id: f_00008-5-1 loss: 0.955804  [   64/  130]
train() client id: f_00008-5-2 loss: 0.966916  [   96/  130]
train() client id: f_00008-5-3 loss: 0.843755  [  128/  130]
train() client id: f_00008-6-0 loss: 0.936136  [   32/  130]
train() client id: f_00008-6-1 loss: 0.942870  [   64/  130]
train() client id: f_00008-6-2 loss: 0.917796  [   96/  130]
train() client id: f_00008-6-3 loss: 0.870680  [  128/  130]
train() client id: f_00008-7-0 loss: 0.895085  [   32/  130]
train() client id: f_00008-7-1 loss: 0.846601  [   64/  130]
train() client id: f_00008-7-2 loss: 0.911795  [   96/  130]
train() client id: f_00008-7-3 loss: 0.931675  [  128/  130]
train() client id: f_00008-8-0 loss: 0.857304  [   32/  130]
train() client id: f_00008-8-1 loss: 0.894499  [   64/  130]
train() client id: f_00008-8-2 loss: 0.953767  [   96/  130]
train() client id: f_00008-8-3 loss: 0.854298  [  128/  130]
train() client id: f_00008-9-0 loss: 0.927359  [   32/  130]
train() client id: f_00008-9-1 loss: 0.832082  [   64/  130]
train() client id: f_00008-9-2 loss: 0.919301  [   96/  130]
train() client id: f_00008-9-3 loss: 0.832276  [  128/  130]
train() client id: f_00008-10-0 loss: 0.864301  [   32/  130]
train() client id: f_00008-10-1 loss: 0.845554  [   64/  130]
train() client id: f_00008-10-2 loss: 0.851409  [   96/  130]
train() client id: f_00008-10-3 loss: 0.908949  [  128/  130]
train() client id: f_00008-11-0 loss: 0.968792  [   32/  130]
train() client id: f_00008-11-1 loss: 0.843089  [   64/  130]
train() client id: f_00008-11-2 loss: 0.815477  [   96/  130]
train() client id: f_00008-11-3 loss: 0.816038  [  128/  130]
train() client id: f_00008-12-0 loss: 0.828290  [   32/  130]
train() client id: f_00008-12-1 loss: 0.916420  [   64/  130]
train() client id: f_00008-12-2 loss: 0.808632  [   96/  130]
train() client id: f_00008-12-3 loss: 0.862160  [  128/  130]
train() client id: f_00009-0-0 loss: 0.904644  [   32/  118]
train() client id: f_00009-0-1 loss: 0.891716  [   64/  118]
train() client id: f_00009-0-2 loss: 0.878308  [   96/  118]
train() client id: f_00009-1-0 loss: 0.935942  [   32/  118]
train() client id: f_00009-1-1 loss: 0.797953  [   64/  118]
train() client id: f_00009-1-2 loss: 0.935629  [   96/  118]
train() client id: f_00009-2-0 loss: 0.914268  [   32/  118]
train() client id: f_00009-2-1 loss: 0.903898  [   64/  118]
train() client id: f_00009-2-2 loss: 0.844723  [   96/  118]
train() client id: f_00009-3-0 loss: 0.892094  [   32/  118]
train() client id: f_00009-3-1 loss: 0.863627  [   64/  118]
train() client id: f_00009-3-2 loss: 0.751090  [   96/  118]
train() client id: f_00009-4-0 loss: 0.870784  [   32/  118]
train() client id: f_00009-4-1 loss: 0.738957  [   64/  118]
train() client id: f_00009-4-2 loss: 0.864279  [   96/  118]
train() client id: f_00009-5-0 loss: 0.832072  [   32/  118]
train() client id: f_00009-5-1 loss: 0.776683  [   64/  118]
train() client id: f_00009-5-2 loss: 0.908842  [   96/  118]
train() client id: f_00009-6-0 loss: 0.884745  [   32/  118]
train() client id: f_00009-6-1 loss: 0.730542  [   64/  118]
train() client id: f_00009-6-2 loss: 0.845809  [   96/  118]
train() client id: f_00009-7-0 loss: 0.766269  [   32/  118]
train() client id: f_00009-7-1 loss: 0.763851  [   64/  118]
train() client id: f_00009-7-2 loss: 0.781705  [   96/  118]
train() client id: f_00009-8-0 loss: 0.722415  [   32/  118]
train() client id: f_00009-8-1 loss: 0.808348  [   64/  118]
train() client id: f_00009-8-2 loss: 0.908521  [   96/  118]
train() client id: f_00009-9-0 loss: 0.773798  [   32/  118]
train() client id: f_00009-9-1 loss: 0.783109  [   64/  118]
train() client id: f_00009-9-2 loss: 0.801913  [   96/  118]
train() client id: f_00009-10-0 loss: 0.733334  [   32/  118]
train() client id: f_00009-10-1 loss: 0.733652  [   64/  118]
train() client id: f_00009-10-2 loss: 0.845538  [   96/  118]
train() client id: f_00009-11-0 loss: 0.782308  [   32/  118]
train() client id: f_00009-11-1 loss: 0.692412  [   64/  118]
train() client id: f_00009-11-2 loss: 0.899318  [   96/  118]
train() client id: f_00009-12-0 loss: 0.741756  [   32/  118]
train() client id: f_00009-12-1 loss: 0.786586  [   64/  118]
train() client id: f_00009-12-2 loss: 0.826536  [   96/  118]
At round 1 accuracy: 0.41644562334217505
At round 1 training accuracy: 0.35412474849094566
At round 1 training loss: 1.079692482586851
update_location
xs = -3.905658 -10.799682 25.009024 18.811294 -29.020704 -16.043590 2.556808 -6.324852 9.663977 -12.060879 
ys = 17.587959 15.555839 1.320614 12.544824 9.350187 -17.185849 -2.624984 -9.177652 17.569006 4.001482 
dists_uav = 101.609992 101.777292 103.088289 102.524326 104.544858 102.726580 100.067117 100.619248 101.990502 100.804150 
dists_bs = 232.490482 228.875916 264.892693 252.892273 220.793604 249.404372 251.151459 249.745296 242.662940 236.198329 
uav_gains = -100.173430 -100.191293 -100.330258 -100.270696 -100.482597 -100.292094 -100.007301 -100.067044 -100.214014 -100.086978 
bs_gains = -105.826404 -105.635862 -107.413019 -106.849255 -105.198681 -106.680373 -106.765259 -106.696984 -106.347156 -106.018810 
Round 2
-------------------------------
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.73113195 22.42682549 10.56475866  3.77907907 25.85610411 12.47188502
  4.69859503 15.17007359 11.12712296 10.11765294]
obj_prev = 126.94322881666007
eta_min = 1.8621250529904804e-09	eta_max = 0.9180913765363099
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 29.535190418159765	eta = 0.9090909090909091
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 50.41602139462105	eta = 0.5325722332838158
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 40.52128855107027	eta = 0.6626189360582114
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 38.752941123532246	eta = 0.6928551054183996
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 38.66719520504412	eta = 0.6943915369355615
af = 26.850173107417966	bf = 2.0241798455930224	zeta = 38.6669798224976	eta = 0.694395404830551
eta = 0.694395404830551
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [0.0300953  0.06329568 0.0296176  0.01027062 0.07308859 0.03487234
 0.01289799 0.04275444 0.03105071 0.02818449]
ene_total = [3.27908415 6.43447607 3.23987046 1.49764204 7.29869581 3.92072634
 1.72777469 4.41559147 3.56239059 3.2907282 ]
ti_comp = [0.26882441 0.25014105 0.26840555 0.26856527 0.25202169 0.24528326
 0.26926224 0.26910548 0.24689208 0.24842218]
ti_coms = [0.06348111 0.08216448 0.06389997 0.06374025 0.08028384 0.08702226
 0.06304328 0.06320005 0.08541344 0.08388335]
t_total = [29.89999161 29.89999161 29.89999161 29.89999161 29.89999161 29.89999161
 29.89999161 29.89999161 29.89999161 29.89999161]
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.35742980e-05 2.53298284e-04 2.25396212e-05 9.38793768e-07
 3.84196142e-04 4.40541962e-05 1.84967284e-06 6.74494733e-05
 3.06958933e-05 2.26740888e-05]
ene_total = [0.57330776 0.76208585 0.57698348 0.57360274 0.75694218 0.78696767
 0.5674136  0.57472659 0.77128999 0.75680081]
optimize_network iter = 0 obj = 6.700120669142431
eta = 0.694395404830551
freqs = [5.59757626e+07 1.26519974e+08 5.51732252e+07 1.91212720e+07
 1.45004571e+08 7.10858434e+07 2.39506050e+07 7.94380680e+07
 6.28831617e+07 5.67270011e+07]
eta_min = 0.6637595643039359	eta_max = 0.6943954048305468
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 0.07059948298279546	eta = 0.909090909090909
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 22.33326679569504	eta = 0.0028738002708385358
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 2.406693151543204	eta = 0.026667856733220745
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 2.3186248233022884	eta = 0.02768078195366155
af = 0.06418134816617768	bf = 2.0241798455930224	zeta = 2.3185795201975767	eta = 0.02768132281299047
eta = 0.02768132281299047
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.31855663e-04 2.49121488e-03 2.21679510e-04 9.23313400e-06
 3.77860887e-03 4.33277584e-04 1.81917241e-05 6.63372559e-04
 3.01897292e-04 2.23002013e-04]
ene_total = [0.18608375 0.30281643 0.18698052 0.18052076 0.33390587 0.25835522
 0.17880307 0.19749234 0.25008995 0.2435316 ]
ti_comp = [0.30213693 0.28345356 0.30171807 0.30187779 0.28533421 0.27859578
 0.30257476 0.302418   0.2802046  0.2817347 ]
ti_coms = [0.06348111 0.08216448 0.06389997 0.06374025 0.08028384 0.08702226
 0.06304328 0.06320005 0.08541344 0.08388335]
t_total = [29.89999161 29.89999161 29.89999161 29.89999161 29.89999161 29.89999161
 29.89999161 29.89999161 29.89999161 29.89999161]
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.64813595e-05 2.79904609e-04 2.53103811e-05 1.05433666e-06
 4.25297547e-04 4.84557995e-05 2.07851021e-06 7.57844495e-05
 3.38154992e-05 2.50150645e-05]
ene_total = [0.52130977 0.69482585 0.52463943 0.52134959 0.69133623 0.71562472
 0.51573361 0.52304321 0.70127064 0.68803794]
optimize_network iter = 1 obj = 6.097170992212053
eta = 0.6637595643039359
freqs = [5.59657273e+07 1.25464071e+08 5.51538439e+07 1.91158103e+07
 1.43920617e+08 7.03288863e+07 2.39506050e+07 7.94329736e+07
 6.22620637e+07 5.62078659e+07]
eta_min = 0.6637595643039447	eta_max = 0.6637595643039276
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 0.06961124159514927	eta = 0.909090909090909
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 22.33232490105157	eta = 0.0028336927384439636
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 2.4022158004882264	eta = 0.026343572834638552
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 2.3153036098319895	eta = 0.027332461555343614
af = 0.06328294690468114	bf = 2.0241798455930224	zeta = 2.315259971692997	eta = 0.02733297671898439
eta = 0.02733297671898439
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.32590817e-04 2.45845542e-03 2.22305891e-04 9.26043939e-06
 3.73546925e-03 4.25596504e-04 1.82559506e-05 6.65629235e-04
 2.97007961e-04 2.19712069e-04]
ene_total = [0.18603789 0.30178187 0.18693126 0.18045688 0.33256672 0.25804555
 0.17874085 0.19748541 0.24986216 0.24335138]
ti_comp = [0.30213693 0.28345356 0.30171807 0.30187779 0.28533421 0.27859578
 0.30257476 0.302418   0.2802046  0.2817347 ]
ti_coms = [0.06348111 0.08216448 0.06389997 0.06374025 0.08028384 0.08702226
 0.06304328 0.06320005 0.08541344 0.08388335]
t_total = [29.89999161 29.89999161 29.89999161 29.89999161 29.89999161 29.89999161
 29.89999161 29.89999161 29.89999161 29.89999161]
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.64813595e-05 2.79904609e-04 2.53103811e-05 1.05433666e-06
 4.25297547e-04 4.84557995e-05 2.07851021e-06 7.57844495e-05
 3.38154992e-05 2.50150645e-05]
ene_total = [0.52130977 0.69482585 0.52463943 0.52134959 0.69133623 0.71562472
 0.51573361 0.52304321 0.70127064 0.68803794]
optimize_network iter = 2 obj = 6.09717099221221
eta = 0.6637595643039447
freqs = [5.59657273e+07 1.25464071e+08 5.51538439e+07 1.91158103e+07
 1.43920617e+08 7.03288863e+07 2.39506050e+07 7.94329736e+07
 6.22620637e+07 5.62078659e+07]
Done!
ene_coms = [0.00634811 0.00821645 0.00639    0.00637402 0.00802838 0.00870223
 0.00630433 0.00632    0.00854134 0.00838833]
ene_comp = [2.56524120e-05 2.71142739e-04 2.45180888e-05 1.02133270e-06
 4.11984434e-04 4.69389849e-05 2.01344649e-06 7.34121646e-05
 3.27569707e-05 2.42320165e-05]
ene_total = [0.00637376 0.00848759 0.00641452 0.00637505 0.00844037 0.00874916
 0.00630634 0.00639342 0.0085741  0.00841257]
At round 2 energy consumption: 0.07452687465895509
At round 2 eta: 0.6637595643039447
At round 2 a_n: 27.49751116231574
At round 2 local rounds: 13.420089837852764
At round 2 global rounds: 81.77931100223806
gradient difference: 0.532572865486145
train() client id: f_00000-0-0 loss: 1.179778  [   32/  126]
train() client id: f_00000-0-1 loss: 1.156502  [   64/  126]
train() client id: f_00000-0-2 loss: 1.188378  [   96/  126]
train() client id: f_00000-1-0 loss: 1.175611  [   32/  126]
train() client id: f_00000-1-1 loss: 1.006787  [   64/  126]
train() client id: f_00000-1-2 loss: 1.086386  [   96/  126]
train() client id: f_00000-2-0 loss: 1.093510  [   32/  126]
train() client id: f_00000-2-1 loss: 0.987743  [   64/  126]
train() client id: f_00000-2-2 loss: 1.002391  [   96/  126]
train() client id: f_00000-3-0 loss: 1.010767  [   32/  126]
train() client id: f_00000-3-1 loss: 0.999844  [   64/  126]
train() client id: f_00000-3-2 loss: 0.985002  [   96/  126]
train() client id: f_00000-4-0 loss: 0.974765  [   32/  126]
train() client id: f_00000-4-1 loss: 0.965013  [   64/  126]
train() client id: f_00000-4-2 loss: 0.947079  [   96/  126]
train() client id: f_00000-5-0 loss: 0.962356  [   32/  126]
train() client id: f_00000-5-1 loss: 0.964873  [   64/  126]
train() client id: f_00000-5-2 loss: 0.891513  [   96/  126]
train() client id: f_00000-6-0 loss: 0.953341  [   32/  126]
train() client id: f_00000-6-1 loss: 0.892128  [   64/  126]
train() client id: f_00000-6-2 loss: 0.966443  [   96/  126]
train() client id: f_00000-7-0 loss: 0.892043  [   32/  126]
train() client id: f_00000-7-1 loss: 0.907095  [   64/  126]
train() client id: f_00000-7-2 loss: 1.033678  [   96/  126]
train() client id: f_00000-8-0 loss: 1.001873  [   32/  126]
train() client id: f_00000-8-1 loss: 0.905502  [   64/  126]
train() client id: f_00000-8-2 loss: 0.949578  [   96/  126]
train() client id: f_00000-9-0 loss: 0.968656  [   32/  126]
train() client id: f_00000-9-1 loss: 0.907018  [   64/  126]
train() client id: f_00000-9-2 loss: 0.915893  [   96/  126]
train() client id: f_00000-10-0 loss: 0.859148  [   32/  126]
train() client id: f_00000-10-1 loss: 1.008161  [   64/  126]
train() client id: f_00000-10-2 loss: 1.019956  [   96/  126]
train() client id: f_00000-11-0 loss: 1.010875  [   32/  126]
train() client id: f_00000-11-1 loss: 0.949435  [   64/  126]
train() client id: f_00000-11-2 loss: 0.875245  [   96/  126]
train() client id: f_00000-12-0 loss: 0.880604  [   32/  126]
train() client id: f_00000-12-1 loss: 1.049180  [   64/  126]
train() client id: f_00000-12-2 loss: 0.842936  [   96/  126]
train() client id: f_00001-0-0 loss: 0.934782  [   32/  265]
train() client id: f_00001-0-1 loss: 0.862857  [   64/  265]
train() client id: f_00001-0-2 loss: 0.929405  [   96/  265]
train() client id: f_00001-0-3 loss: 0.977539  [  128/  265]
train() client id: f_00001-0-4 loss: 0.883284  [  160/  265]
train() client id: f_00001-0-5 loss: 0.816570  [  192/  265]
train() client id: f_00001-0-6 loss: 0.933152  [  224/  265]
train() client id: f_00001-0-7 loss: 0.855564  [  256/  265]
train() client id: f_00001-1-0 loss: 0.851062  [   32/  265]
train() client id: f_00001-1-1 loss: 0.845017  [   64/  265]
train() client id: f_00001-1-2 loss: 0.835985  [   96/  265]
train() client id: f_00001-1-3 loss: 0.791029  [  128/  265]
train() client id: f_00001-1-4 loss: 0.878710  [  160/  265]
train() client id: f_00001-1-5 loss: 0.788158  [  192/  265]
train() client id: f_00001-1-6 loss: 0.723422  [  224/  265]
train() client id: f_00001-1-7 loss: 0.782778  [  256/  265]
train() client id: f_00001-2-0 loss: 0.788092  [   32/  265]
train() client id: f_00001-2-1 loss: 0.738268  [   64/  265]
train() client id: f_00001-2-2 loss: 0.788552  [   96/  265]
train() client id: f_00001-2-3 loss: 0.733466  [  128/  265]
train() client id: f_00001-2-4 loss: 0.754428  [  160/  265]
train() client id: f_00001-2-5 loss: 0.738506  [  192/  265]
train() client id: f_00001-2-6 loss: 0.705024  [  224/  265]
train() client id: f_00001-2-7 loss: 0.705108  [  256/  265]
train() client id: f_00001-3-0 loss: 0.595811  [   32/  265]
train() client id: f_00001-3-1 loss: 0.756025  [   64/  265]
train() client id: f_00001-3-2 loss: 0.708251  [   96/  265]
train() client id: f_00001-3-3 loss: 0.663871  [  128/  265]
train() client id: f_00001-3-4 loss: 0.694572  [  160/  265]
train() client id: f_00001-3-5 loss: 0.707746  [  192/  265]
train() client id: f_00001-3-6 loss: 0.696172  [  224/  265]
train() client id: f_00001-3-7 loss: 0.719107  [  256/  265]
train() client id: f_00001-4-0 loss: 0.725155  [   32/  265]
train() client id: f_00001-4-1 loss: 0.628436  [   64/  265]
train() client id: f_00001-4-2 loss: 0.676207  [   96/  265]
train() client id: f_00001-4-3 loss: 0.602117  [  128/  265]
train() client id: f_00001-4-4 loss: 0.679644  [  160/  265]
train() client id: f_00001-4-5 loss: 0.640277  [  192/  265]
train() client id: f_00001-4-6 loss: 0.714230  [  224/  265]
train() client id: f_00001-4-7 loss: 0.726101  [  256/  265]
train() client id: f_00001-5-0 loss: 0.763653  [   32/  265]
train() client id: f_00001-5-1 loss: 0.687435  [   64/  265]
train() client id: f_00001-5-2 loss: 0.599861  [   96/  265]
train() client id: f_00001-5-3 loss: 0.589985  [  128/  265]
train() client id: f_00001-5-4 loss: 0.578160  [  160/  265]
train() client id: f_00001-5-5 loss: 0.671363  [  192/  265]
train() client id: f_00001-5-6 loss: 0.630839  [  224/  265]
train() client id: f_00001-5-7 loss: 0.611289  [  256/  265]
train() client id: f_00001-6-0 loss: 0.762442  [   32/  265]
train() client id: f_00001-6-1 loss: 0.557820  [   64/  265]
train() client id: f_00001-6-2 loss: 0.546253  [   96/  265]
train() client id: f_00001-6-3 loss: 0.690215  [  128/  265]
train() client id: f_00001-6-4 loss: 0.570222  [  160/  265]
train() client id: f_00001-6-5 loss: 0.670732  [  192/  265]
train() client id: f_00001-6-6 loss: 0.712527  [  224/  265]
train() client id: f_00001-6-7 loss: 0.606771  [  256/  265]
train() client id: f_00001-7-0 loss: 0.574579  [   32/  265]
train() client id: f_00001-7-1 loss: 0.649563  [   64/  265]
train() client id: f_00001-7-2 loss: 0.660137  [   96/  265]
train() client id: f_00001-7-3 loss: 0.676996  [  128/  265]
train() client id: f_00001-7-4 loss: 0.672584  [  160/  265]
train() client id: f_00001-7-5 loss: 0.574144  [  192/  265]
train() client id: f_00001-7-6 loss: 0.557523  [  224/  265]
train() client id: f_00001-7-7 loss: 0.616671  [  256/  265]
train() client id: f_00001-8-0 loss: 0.593049  [   32/  265]
train() client id: f_00001-8-1 loss: 0.644208  [   64/  265]
train() client id: f_00001-8-2 loss: 0.581539  [   96/  265]
train() client id: f_00001-8-3 loss: 0.570315  [  128/  265]
train() client id: f_00001-8-4 loss: 0.659157  [  160/  265]
train() client id: f_00001-8-5 loss: 0.551483  [  192/  265]
train() client id: f_00001-8-6 loss: 0.600873  [  224/  265]
train() client id: f_00001-8-7 loss: 0.779055  [  256/  265]
train() client id: f_00001-9-0 loss: 0.643369  [   32/  265]
train() client id: f_00001-9-1 loss: 0.702182  [   64/  265]
train() client id: f_00001-9-2 loss: 0.748543  [   96/  265]
train() client id: f_00001-9-3 loss: 0.554138  [  128/  265]
train() client id: f_00001-9-4 loss: 0.520836  [  160/  265]
train() client id: f_00001-9-5 loss: 0.558837  [  192/  265]
train() client id: f_00001-9-6 loss: 0.613406  [  224/  265]
train() client id: f_00001-9-7 loss: 0.616224  [  256/  265]
train() client id: f_00001-10-0 loss: 0.573385  [   32/  265]
train() client id: f_00001-10-1 loss: 0.498112  [   64/  265]
train() client id: f_00001-10-2 loss: 0.724154  [   96/  265]
train() client id: f_00001-10-3 loss: 0.674891  [  128/  265]
train() client id: f_00001-10-4 loss: 0.584929  [  160/  265]
train() client id: f_00001-10-5 loss: 0.552799  [  192/  265]
train() client id: f_00001-10-6 loss: 0.657146  [  224/  265]
train() client id: f_00001-10-7 loss: 0.586259  [  256/  265]
train() client id: f_00001-11-0 loss: 0.733160  [   32/  265]
train() client id: f_00001-11-1 loss: 0.542444  [   64/  265]
train() client id: f_00001-11-2 loss: 0.570836  [   96/  265]
train() client id: f_00001-11-3 loss: 0.589131  [  128/  265]
train() client id: f_00001-11-4 loss: 0.562778  [  160/  265]
train() client id: f_00001-11-5 loss: 0.621603  [  192/  265]
train() client id: f_00001-11-6 loss: 0.626783  [  224/  265]
train() client id: f_00001-11-7 loss: 0.679376  [  256/  265]
train() client id: f_00001-12-0 loss: 0.706278  [   32/  265]
train() client id: f_00001-12-1 loss: 0.612207  [   64/  265]
train() client id: f_00001-12-2 loss: 0.534519  [   96/  265]
train() client id: f_00001-12-3 loss: 0.635280  [  128/  265]
train() client id: f_00001-12-4 loss: 0.512176  [  160/  265]
train() client id: f_00001-12-5 loss: 0.731532  [  192/  265]
train() client id: f_00001-12-6 loss: 0.600861  [  224/  265]
train() client id: f_00001-12-7 loss: 0.524229  [  256/  265]
train() client id: f_00002-0-0 loss: 1.192923  [   32/  124]
train() client id: f_00002-0-1 loss: 1.057930  [   64/  124]
train() client id: f_00002-0-2 loss: 1.158620  [   96/  124]
train() client id: f_00002-1-0 loss: 1.127423  [   32/  124]
train() client id: f_00002-1-1 loss: 1.146688  [   64/  124]
train() client id: f_00002-1-2 loss: 1.104076  [   96/  124]
train() client id: f_00002-2-0 loss: 1.143189  [   32/  124]
train() client id: f_00002-2-1 loss: 1.069702  [   64/  124]
train() client id: f_00002-2-2 loss: 1.077259  [   96/  124]
train() client id: f_00002-3-0 loss: 1.095937  [   32/  124]
train() client id: f_00002-3-1 loss: 1.054035  [   64/  124]
train() client id: f_00002-3-2 loss: 1.108244  [   96/  124]
train() client id: f_00002-4-0 loss: 1.101544  [   32/  124]
train() client id: f_00002-4-1 loss: 1.088872  [   64/  124]
train() client id: f_00002-4-2 loss: 1.054839  [   96/  124]
train() client id: f_00002-5-0 loss: 1.020321  [   32/  124]
train() client id: f_00002-5-1 loss: 1.044542  [   64/  124]
train() client id: f_00002-5-2 loss: 1.092591  [   96/  124]
train() client id: f_00002-6-0 loss: 1.021975  [   32/  124]
train() client id: f_00002-6-1 loss: 0.980809  [   64/  124]
train() client id: f_00002-6-2 loss: 1.076853  [   96/  124]
train() client id: f_00002-7-0 loss: 1.071264  [   32/  124]
train() client id: f_00002-7-1 loss: 1.076319  [   64/  124]
train() client id: f_00002-7-2 loss: 0.963475  [   96/  124]
train() client id: f_00002-8-0 loss: 1.026051  [   32/  124]
train() client id: f_00002-8-1 loss: 1.051001  [   64/  124]
train() client id: f_00002-8-2 loss: 0.983487  [   96/  124]
train() client id: f_00002-9-0 loss: 1.058617  [   32/  124]
train() client id: f_00002-9-1 loss: 1.039714  [   64/  124]
train() client id: f_00002-9-2 loss: 0.968031  [   96/  124]
train() client id: f_00002-10-0 loss: 0.958133  [   32/  124]
train() client id: f_00002-10-1 loss: 1.067943  [   64/  124]
train() client id: f_00002-10-2 loss: 1.097765  [   96/  124]
train() client id: f_00002-11-0 loss: 0.972088  [   32/  124]
train() client id: f_00002-11-1 loss: 1.102874  [   64/  124]
train() client id: f_00002-11-2 loss: 0.966147  [   96/  124]
train() client id: f_00002-12-0 loss: 1.020455  [   32/  124]
train() client id: f_00002-12-1 loss: 0.943535  [   64/  124]
train() client id: f_00002-12-2 loss: 1.213009  [   96/  124]
train() client id: f_00003-0-0 loss: 1.050726  [   32/   43]
train() client id: f_00003-1-0 loss: 1.056615  [   32/   43]
train() client id: f_00003-2-0 loss: 1.061664  [   32/   43]
train() client id: f_00003-3-0 loss: 1.053550  [   32/   43]
train() client id: f_00003-4-0 loss: 1.020597  [   32/   43]
train() client id: f_00003-5-0 loss: 1.064886  [   32/   43]
train() client id: f_00003-6-0 loss: 1.064463  [   32/   43]
train() client id: f_00003-7-0 loss: 1.090992  [   32/   43]
train() client id: f_00003-8-0 loss: 1.085877  [   32/   43]
train() client id: f_00003-9-0 loss: 1.071300  [   32/   43]
train() client id: f_00003-10-0 loss: 1.027622  [   32/   43]
train() client id: f_00003-11-0 loss: 1.085404  [   32/   43]
train() client id: f_00003-12-0 loss: 1.082601  [   32/   43]
train() client id: f_00004-0-0 loss: 1.142115  [   32/  306]
train() client id: f_00004-0-1 loss: 1.049331  [   64/  306]
train() client id: f_00004-0-2 loss: 1.086609  [   96/  306]
train() client id: f_00004-0-3 loss: 1.096297  [  128/  306]
train() client id: f_00004-0-4 loss: 1.021154  [  160/  306]
train() client id: f_00004-0-5 loss: 0.938684  [  192/  306]
train() client id: f_00004-0-6 loss: 1.091693  [  224/  306]
train() client id: f_00004-0-7 loss: 1.120047  [  256/  306]
train() client id: f_00004-0-8 loss: 1.058212  [  288/  306]
train() client id: f_00004-1-0 loss: 1.099320  [   32/  306]
train() client id: f_00004-1-1 loss: 1.032180  [   64/  306]
train() client id: f_00004-1-2 loss: 1.014796  [   96/  306]
train() client id: f_00004-1-3 loss: 1.039594  [  128/  306]
train() client id: f_00004-1-4 loss: 1.096911  [  160/  306]
train() client id: f_00004-1-5 loss: 1.126022  [  192/  306]
train() client id: f_00004-1-6 loss: 1.097465  [  224/  306]
train() client id: f_00004-1-7 loss: 0.985947  [  256/  306]
train() client id: f_00004-1-8 loss: 1.023648  [  288/  306]
train() client id: f_00004-2-0 loss: 1.002910  [   32/  306]
train() client id: f_00004-2-1 loss: 0.965094  [   64/  306]
train() client id: f_00004-2-2 loss: 1.022699  [   96/  306]
train() client id: f_00004-2-3 loss: 1.120663  [  128/  306]
train() client id: f_00004-2-4 loss: 1.060364  [  160/  306]
train() client id: f_00004-2-5 loss: 1.053047  [  192/  306]
train() client id: f_00004-2-6 loss: 1.028004  [  224/  306]
train() client id: f_00004-2-7 loss: 1.146426  [  256/  306]
train() client id: f_00004-2-8 loss: 1.101496  [  288/  306]
train() client id: f_00004-3-0 loss: 1.085280  [   32/  306]
train() client id: f_00004-3-1 loss: 1.015803  [   64/  306]
train() client id: f_00004-3-2 loss: 1.010407  [   96/  306]
train() client id: f_00004-3-3 loss: 1.087330  [  128/  306]
train() client id: f_00004-3-4 loss: 1.100659  [  160/  306]
train() client id: f_00004-3-5 loss: 1.113906  [  192/  306]
train() client id: f_00004-3-6 loss: 1.042753  [  224/  306]
train() client id: f_00004-3-7 loss: 1.045578  [  256/  306]
train() client id: f_00004-3-8 loss: 0.941957  [  288/  306]
train() client id: f_00004-4-0 loss: 0.964267  [   32/  306]
train() client id: f_00004-4-1 loss: 1.000318  [   64/  306]
train() client id: f_00004-4-2 loss: 1.058229  [   96/  306]
train() client id: f_00004-4-3 loss: 1.065745  [  128/  306]
train() client id: f_00004-4-4 loss: 1.011288  [  160/  306]
train() client id: f_00004-4-5 loss: 1.085061  [  192/  306]
train() client id: f_00004-4-6 loss: 1.119144  [  224/  306]
train() client id: f_00004-4-7 loss: 1.027842  [  256/  306]
train() client id: f_00004-4-8 loss: 1.117507  [  288/  306]
train() client id: f_00004-5-0 loss: 1.081426  [   32/  306]
train() client id: f_00004-5-1 loss: 1.045237  [   64/  306]
train() client id: f_00004-5-2 loss: 1.071274  [   96/  306]
train() client id: f_00004-5-3 loss: 1.067632  [  128/  306]
train() client id: f_00004-5-4 loss: 1.031968  [  160/  306]
train() client id: f_00004-5-5 loss: 1.035115  [  192/  306]
train() client id: f_00004-5-6 loss: 0.960022  [  224/  306]
train() client id: f_00004-5-7 loss: 1.079411  [  256/  306]
train() client id: f_00004-5-8 loss: 1.101390  [  288/  306]
train() client id: f_00004-6-0 loss: 0.980322  [   32/  306]
train() client id: f_00004-6-1 loss: 0.990494  [   64/  306]
train() client id: f_00004-6-2 loss: 1.190949  [   96/  306]
train() client id: f_00004-6-3 loss: 1.134015  [  128/  306]
train() client id: f_00004-6-4 loss: 1.023044  [  160/  306]
train() client id: f_00004-6-5 loss: 0.997085  [  192/  306]
train() client id: f_00004-6-6 loss: 1.030604  [  224/  306]
train() client id: f_00004-6-7 loss: 1.103192  [  256/  306]
train() client id: f_00004-6-8 loss: 1.023117  [  288/  306]
train() client id: f_00004-7-0 loss: 1.022320  [   32/  306]
train() client id: f_00004-7-1 loss: 1.070394  [   64/  306]
train() client id: f_00004-7-2 loss: 1.100559  [   96/  306]
train() client id: f_00004-7-3 loss: 1.068984  [  128/  306]
train() client id: f_00004-7-4 loss: 1.059594  [  160/  306]
train() client id: f_00004-7-5 loss: 0.951735  [  192/  306]
train() client id: f_00004-7-6 loss: 1.048212  [  224/  306]
train() client id: f_00004-7-7 loss: 1.074000  [  256/  306]
train() client id: f_00004-7-8 loss: 1.090384  [  288/  306]
train() client id: f_00004-8-0 loss: 1.032699  [   32/  306]
train() client id: f_00004-8-1 loss: 1.059003  [   64/  306]
train() client id: f_00004-8-2 loss: 1.073450  [   96/  306]
train() client id: f_00004-8-3 loss: 1.034657  [  128/  306]
train() client id: f_00004-8-4 loss: 1.042003  [  160/  306]
train() client id: f_00004-8-5 loss: 0.926819  [  192/  306]
train() client id: f_00004-8-6 loss: 1.028313  [  224/  306]
train() client id: f_00004-8-7 loss: 1.082422  [  256/  306]
train() client id: f_00004-8-8 loss: 1.133205  [  288/  306]
train() client id: f_00004-9-0 loss: 1.057017  [   32/  306]
train() client id: f_00004-9-1 loss: 1.025843  [   64/  306]
train() client id: f_00004-9-2 loss: 1.068856  [   96/  306]
train() client id: f_00004-9-3 loss: 1.107042  [  128/  306]
train() client id: f_00004-9-4 loss: 0.957717  [  160/  306]
train() client id: f_00004-9-5 loss: 1.033271  [  192/  306]
train() client id: f_00004-9-6 loss: 0.972886  [  224/  306]
train() client id: f_00004-9-7 loss: 1.080037  [  256/  306]
train() client id: f_00004-9-8 loss: 1.121522  [  288/  306]
train() client id: f_00004-10-0 loss: 1.151943  [   32/  306]
train() client id: f_00004-10-1 loss: 0.971075  [   64/  306]
train() client id: f_00004-10-2 loss: 1.036798  [   96/  306]
train() client id: f_00004-10-3 loss: 1.092340  [  128/  306]
train() client id: f_00004-10-4 loss: 0.995932  [  160/  306]
train() client id: f_00004-10-5 loss: 1.029743  [  192/  306]
train() client id: f_00004-10-6 loss: 1.012562  [  224/  306]
train() client id: f_00004-10-7 loss: 1.035511  [  256/  306]
train() client id: f_00004-10-8 loss: 1.132199  [  288/  306]
train() client id: f_00004-11-0 loss: 1.069981  [   32/  306]
train() client id: f_00004-11-1 loss: 0.975437  [   64/  306]
train() client id: f_00004-11-2 loss: 0.931826  [   96/  306]
train() client id: f_00004-11-3 loss: 1.102833  [  128/  306]
train() client id: f_00004-11-4 loss: 1.069969  [  160/  306]
train() client id: f_00004-11-5 loss: 0.982638  [  192/  306]
train() client id: f_00004-11-6 loss: 1.137801  [  224/  306]
train() client id: f_00004-11-7 loss: 1.075241  [  256/  306]
train() client id: f_00004-11-8 loss: 1.086122  [  288/  306]
train() client id: f_00004-12-0 loss: 1.014187  [   32/  306]
train() client id: f_00004-12-1 loss: 1.070499  [   64/  306]
train() client id: f_00004-12-2 loss: 1.126304  [   96/  306]
train() client id: f_00004-12-3 loss: 1.010589  [  128/  306]
train() client id: f_00004-12-4 loss: 1.061766  [  160/  306]
train() client id: f_00004-12-5 loss: 1.146069  [  192/  306]
train() client id: f_00004-12-6 loss: 1.046281  [  224/  306]
train() client id: f_00004-12-7 loss: 0.964865  [  256/  306]
train() client id: f_00004-12-8 loss: 0.984279  [  288/  306]
train() client id: f_00005-0-0 loss: 1.085377  [   32/  146]
train() client id: f_00005-0-1 loss: 1.076161  [   64/  146]
train() client id: f_00005-0-2 loss: 1.053884  [   96/  146]
train() client id: f_00005-0-3 loss: 0.999685  [  128/  146]
train() client id: f_00005-1-0 loss: 1.047303  [   32/  146]
train() client id: f_00005-1-1 loss: 0.984448  [   64/  146]
train() client id: f_00005-1-2 loss: 0.958918  [   96/  146]
train() client id: f_00005-1-3 loss: 1.104298  [  128/  146]
train() client id: f_00005-2-0 loss: 0.993649  [   32/  146]
train() client id: f_00005-2-1 loss: 1.078309  [   64/  146]
train() client id: f_00005-2-2 loss: 0.963070  [   96/  146]
train() client id: f_00005-2-3 loss: 0.991877  [  128/  146]
train() client id: f_00005-3-0 loss: 0.986122  [   32/  146]
train() client id: f_00005-3-1 loss: 0.990311  [   64/  146]
train() client id: f_00005-3-2 loss: 0.880207  [   96/  146]
train() client id: f_00005-3-3 loss: 0.995239  [  128/  146]
train() client id: f_00005-4-0 loss: 1.025778  [   32/  146]
train() client id: f_00005-4-1 loss: 0.922436  [   64/  146]
train() client id: f_00005-4-2 loss: 0.935305  [   96/  146]
train() client id: f_00005-4-3 loss: 0.910790  [  128/  146]
train() client id: f_00005-5-0 loss: 0.959685  [   32/  146]
train() client id: f_00005-5-1 loss: 0.916320  [   64/  146]
train() client id: f_00005-5-2 loss: 0.998851  [   96/  146]
train() client id: f_00005-5-3 loss: 0.856186  [  128/  146]
train() client id: f_00005-6-0 loss: 0.934498  [   32/  146]
train() client id: f_00005-6-1 loss: 0.888091  [   64/  146]
train() client id: f_00005-6-2 loss: 0.982283  [   96/  146]
train() client id: f_00005-6-3 loss: 0.966082  [  128/  146]
train() client id: f_00005-7-0 loss: 0.813532  [   32/  146]
train() client id: f_00005-7-1 loss: 0.965707  [   64/  146]
train() client id: f_00005-7-2 loss: 0.884665  [   96/  146]
train() client id: f_00005-7-3 loss: 0.901563  [  128/  146]
train() client id: f_00005-8-0 loss: 1.018684  [   32/  146]
train() client id: f_00005-8-1 loss: 0.812867  [   64/  146]
train() client id: f_00005-8-2 loss: 0.880375  [   96/  146]
train() client id: f_00005-8-3 loss: 0.922493  [  128/  146]
train() client id: f_00005-9-0 loss: 1.054878  [   32/  146]
train() client id: f_00005-9-1 loss: 0.886331  [   64/  146]
train() client id: f_00005-9-2 loss: 0.826266  [   96/  146]
train() client id: f_00005-9-3 loss: 0.992574  [  128/  146]
train() client id: f_00005-10-0 loss: 1.001504  [   32/  146]
train() client id: f_00005-10-1 loss: 0.825955  [   64/  146]
train() client id: f_00005-10-2 loss: 0.824374  [   96/  146]
train() client id: f_00005-10-3 loss: 0.929588  [  128/  146]
train() client id: f_00005-11-0 loss: 0.855016  [   32/  146]
train() client id: f_00005-11-1 loss: 1.002852  [   64/  146]
train() client id: f_00005-11-2 loss: 0.844686  [   96/  146]
train() client id: f_00005-11-3 loss: 0.956392  [  128/  146]
train() client id: f_00005-12-0 loss: 0.862083  [   32/  146]
train() client id: f_00005-12-1 loss: 0.955909  [   64/  146]
train() client id: f_00005-12-2 loss: 0.824374  [   96/  146]
train() client id: f_00005-12-3 loss: 0.890833  [  128/  146]
train() client id: f_00006-0-0 loss: 1.058756  [   32/   54]
train() client id: f_00006-1-0 loss: 1.018383  [   32/   54]
train() client id: f_00006-2-0 loss: 1.016255  [   32/   54]
train() client id: f_00006-3-0 loss: 1.034530  [   32/   54]
train() client id: f_00006-4-0 loss: 1.024019  [   32/   54]
train() client id: f_00006-5-0 loss: 1.067842  [   32/   54]
train() client id: f_00006-6-0 loss: 1.053776  [   32/   54]
train() client id: f_00006-7-0 loss: 1.062496  [   32/   54]
train() client id: f_00006-8-0 loss: 1.013827  [   32/   54]
train() client id: f_00006-9-0 loss: 1.035489  [   32/   54]
train() client id: f_00006-10-0 loss: 1.052601  [   32/   54]
train() client id: f_00006-11-0 loss: 1.029469  [   32/   54]
train() client id: f_00006-12-0 loss: 1.022006  [   32/   54]
train() client id: f_00007-0-0 loss: 1.061033  [   32/  179]
train() client id: f_00007-0-1 loss: 1.030008  [   64/  179]
train() client id: f_00007-0-2 loss: 1.015082  [   96/  179]
train() client id: f_00007-0-3 loss: 1.096137  [  128/  179]
train() client id: f_00007-0-4 loss: 1.104557  [  160/  179]
train() client id: f_00007-1-0 loss: 0.974733  [   32/  179]
train() client id: f_00007-1-1 loss: 0.942510  [   64/  179]
train() client id: f_00007-1-2 loss: 0.985159  [   96/  179]
train() client id: f_00007-1-3 loss: 0.969125  [  128/  179]
train() client id: f_00007-1-4 loss: 1.048314  [  160/  179]
train() client id: f_00007-2-0 loss: 0.961531  [   32/  179]
train() client id: f_00007-2-1 loss: 0.913088  [   64/  179]
train() client id: f_00007-2-2 loss: 0.976015  [   96/  179]
train() client id: f_00007-2-3 loss: 0.906945  [  128/  179]
train() client id: f_00007-2-4 loss: 0.887300  [  160/  179]
train() client id: f_00007-3-0 loss: 0.904602  [   32/  179]
train() client id: f_00007-3-1 loss: 0.932203  [   64/  179]
train() client id: f_00007-3-2 loss: 0.890477  [   96/  179]
train() client id: f_00007-3-3 loss: 0.830019  [  128/  179]
train() client id: f_00007-3-4 loss: 0.891198  [  160/  179]
train() client id: f_00007-4-0 loss: 0.878090  [   32/  179]
train() client id: f_00007-4-1 loss: 0.919552  [   64/  179]
train() client id: f_00007-4-2 loss: 0.833147  [   96/  179]
train() client id: f_00007-4-3 loss: 0.830683  [  128/  179]
train() client id: f_00007-4-4 loss: 0.790396  [  160/  179]
train() client id: f_00007-5-0 loss: 0.831967  [   32/  179]
train() client id: f_00007-5-1 loss: 0.770228  [   64/  179]
train() client id: f_00007-5-2 loss: 0.861252  [   96/  179]
train() client id: f_00007-5-3 loss: 0.900085  [  128/  179]
train() client id: f_00007-5-4 loss: 0.824412  [  160/  179]
train() client id: f_00007-6-0 loss: 0.731370  [   32/  179]
train() client id: f_00007-6-1 loss: 0.819949  [   64/  179]
train() client id: f_00007-6-2 loss: 0.978347  [   96/  179]
train() client id: f_00007-6-3 loss: 0.763543  [  128/  179]
train() client id: f_00007-6-4 loss: 0.759645  [  160/  179]
train() client id: f_00007-7-0 loss: 0.902580  [   32/  179]
train() client id: f_00007-7-1 loss: 0.782334  [   64/  179]
train() client id: f_00007-7-2 loss: 0.816699  [   96/  179]
train() client id: f_00007-7-3 loss: 0.804314  [  128/  179]
train() client id: f_00007-7-4 loss: 0.766419  [  160/  179]
train() client id: f_00007-8-0 loss: 0.784409  [   32/  179]
train() client id: f_00007-8-1 loss: 0.846831  [   64/  179]
train() client id: f_00007-8-2 loss: 0.780273  [   96/  179]
train() client id: f_00007-8-3 loss: 0.739235  [  128/  179]
train() client id: f_00007-8-4 loss: 0.898701  [  160/  179]
train() client id: f_00007-9-0 loss: 0.795377  [   32/  179]
train() client id: f_00007-9-1 loss: 0.777090  [   64/  179]
train() client id: f_00007-9-2 loss: 0.902601  [   96/  179]
train() client id: f_00007-9-3 loss: 0.800604  [  128/  179]
train() client id: f_00007-9-4 loss: 0.770672  [  160/  179]
train() client id: f_00007-10-0 loss: 0.774672  [   32/  179]
train() client id: f_00007-10-1 loss: 0.926784  [   64/  179]
train() client id: f_00007-10-2 loss: 0.715609  [   96/  179]
train() client id: f_00007-10-3 loss: 0.740223  [  128/  179]
train() client id: f_00007-10-4 loss: 0.752004  [  160/  179]
train() client id: f_00007-11-0 loss: 0.799148  [   32/  179]
train() client id: f_00007-11-1 loss: 0.691504  [   64/  179]
train() client id: f_00007-11-2 loss: 0.787802  [   96/  179]
train() client id: f_00007-11-3 loss: 0.899451  [  128/  179]
train() client id: f_00007-11-4 loss: 0.827867  [  160/  179]
train() client id: f_00007-12-0 loss: 0.751694  [   32/  179]
train() client id: f_00007-12-1 loss: 0.771375  [   64/  179]
train() client id: f_00007-12-2 loss: 0.791396  [   96/  179]
train() client id: f_00007-12-3 loss: 0.781719  [  128/  179]
train() client id: f_00007-12-4 loss: 0.828027  [  160/  179]
train() client id: f_00008-0-0 loss: 0.975960  [   32/  130]
train() client id: f_00008-0-1 loss: 1.066042  [   64/  130]
train() client id: f_00008-0-2 loss: 0.963425  [   96/  130]
train() client id: f_00008-0-3 loss: 0.985748  [  128/  130]
train() client id: f_00008-1-0 loss: 1.033785  [   32/  130]
train() client id: f_00008-1-1 loss: 0.965860  [   64/  130]
train() client id: f_00008-1-2 loss: 1.002933  [   96/  130]
train() client id: f_00008-1-3 loss: 0.959904  [  128/  130]
train() client id: f_00008-2-0 loss: 0.981039  [   32/  130]
train() client id: f_00008-2-1 loss: 0.976149  [   64/  130]
train() client id: f_00008-2-2 loss: 0.997213  [   96/  130]
train() client id: f_00008-2-3 loss: 0.972064  [  128/  130]
train() client id: f_00008-3-0 loss: 0.970623  [   32/  130]
train() client id: f_00008-3-1 loss: 0.931755  [   64/  130]
train() client id: f_00008-3-2 loss: 0.958940  [   96/  130]
train() client id: f_00008-3-3 loss: 1.029596  [  128/  130]
train() client id: f_00008-4-0 loss: 0.952633  [   32/  130]
train() client id: f_00008-4-1 loss: 1.002411  [   64/  130]
train() client id: f_00008-4-2 loss: 0.958159  [   96/  130]
train() client id: f_00008-4-3 loss: 0.976914  [  128/  130]
train() client id: f_00008-5-0 loss: 0.997654  [   32/  130]
train() client id: f_00008-5-1 loss: 0.945526  [   64/  130]
train() client id: f_00008-5-2 loss: 0.932279  [   96/  130]
train() client id: f_00008-5-3 loss: 1.008450  [  128/  130]
train() client id: f_00008-6-0 loss: 0.928657  [   32/  130]
train() client id: f_00008-6-1 loss: 0.897466  [   64/  130]
train() client id: f_00008-6-2 loss: 1.032690  [   96/  130]
train() client id: f_00008-6-3 loss: 0.999273  [  128/  130]
train() client id: f_00008-7-0 loss: 0.857846  [   32/  130]
train() client id: f_00008-7-1 loss: 1.005359  [   64/  130]
train() client id: f_00008-7-2 loss: 1.074187  [   96/  130]
train() client id: f_00008-7-3 loss: 0.926843  [  128/  130]
train() client id: f_00008-8-0 loss: 0.963304  [   32/  130]
train() client id: f_00008-8-1 loss: 0.911998  [   64/  130]
train() client id: f_00008-8-2 loss: 0.948716  [   96/  130]
train() client id: f_00008-8-3 loss: 1.016592  [  128/  130]
train() client id: f_00008-9-0 loss: 1.025549  [   32/  130]
train() client id: f_00008-9-1 loss: 0.934803  [   64/  130]
train() client id: f_00008-9-2 loss: 0.966319  [   96/  130]
train() client id: f_00008-9-3 loss: 0.925273  [  128/  130]
train() client id: f_00008-10-0 loss: 0.953581  [   32/  130]
train() client id: f_00008-10-1 loss: 0.901305  [   64/  130]
train() client id: f_00008-10-2 loss: 1.060677  [   96/  130]
train() client id: f_00008-10-3 loss: 0.950649  [  128/  130]
train() client id: f_00008-11-0 loss: 0.895568  [   32/  130]
train() client id: f_00008-11-1 loss: 0.935057  [   64/  130]
train() client id: f_00008-11-2 loss: 0.908547  [   96/  130]
train() client id: f_00008-11-3 loss: 1.135259  [  128/  130]
train() client id: f_00008-12-0 loss: 0.970711  [   32/  130]
train() client id: f_00008-12-1 loss: 1.061947  [   64/  130]
train() client id: f_00008-12-2 loss: 0.901339  [   96/  130]
train() client id: f_00008-12-3 loss: 0.938004  [  128/  130]
train() client id: f_00009-0-0 loss: 1.027358  [   32/  118]
train() client id: f_00009-0-1 loss: 0.856606  [   64/  118]
train() client id: f_00009-0-2 loss: 0.989782  [   96/  118]
train() client id: f_00009-1-0 loss: 0.916319  [   32/  118]
train() client id: f_00009-1-1 loss: 0.923725  [   64/  118]
train() client id: f_00009-1-2 loss: 0.893088  [   96/  118]
train() client id: f_00009-2-0 loss: 0.898280  [   32/  118]
train() client id: f_00009-2-1 loss: 0.925499  [   64/  118]
train() client id: f_00009-2-2 loss: 0.904526  [   96/  118]
train() client id: f_00009-3-0 loss: 0.900564  [   32/  118]
train() client id: f_00009-3-1 loss: 0.763945  [   64/  118]
train() client id: f_00009-3-2 loss: 0.903341  [   96/  118]
train() client id: f_00009-4-0 loss: 0.823024  [   32/  118]
train() client id: f_00009-4-1 loss: 0.867738  [   64/  118]
train() client id: f_00009-4-2 loss: 0.797711  [   96/  118]
train() client id: f_00009-5-0 loss: 0.963207  [   32/  118]
train() client id: f_00009-5-1 loss: 0.783890  [   64/  118]
train() client id: f_00009-5-2 loss: 0.809778  [   96/  118]
train() client id: f_00009-6-0 loss: 0.865103  [   32/  118]
train() client id: f_00009-6-1 loss: 0.804968  [   64/  118]
train() client id: f_00009-6-2 loss: 0.824720  [   96/  118]
train() client id: f_00009-7-0 loss: 0.836639  [   32/  118]
train() client id: f_00009-7-1 loss: 0.792541  [   64/  118]
train() client id: f_00009-7-2 loss: 0.854642  [   96/  118]
train() client id: f_00009-8-0 loss: 0.903558  [   32/  118]
train() client id: f_00009-8-1 loss: 0.761271  [   64/  118]
train() client id: f_00009-8-2 loss: 0.758910  [   96/  118]
train() client id: f_00009-9-0 loss: 0.841350  [   32/  118]
train() client id: f_00009-9-1 loss: 0.836818  [   64/  118]
train() client id: f_00009-9-2 loss: 0.822783  [   96/  118]
train() client id: f_00009-10-0 loss: 0.783987  [   32/  118]
train() client id: f_00009-10-1 loss: 0.888792  [   64/  118]
train() client id: f_00009-10-2 loss: 0.734623  [   96/  118]
train() client id: f_00009-11-0 loss: 0.854355  [   32/  118]
train() client id: f_00009-11-1 loss: 0.788863  [   64/  118]
train() client id: f_00009-11-2 loss: 0.764913  [   96/  118]
train() client id: f_00009-12-0 loss: 0.775704  [   32/  118]
train() client id: f_00009-12-1 loss: 0.620967  [   64/  118]
train() client id: f_00009-12-2 loss: 0.905708  [   96/  118]
At round 2 accuracy: 0.5278514588859416
At round 2 training accuracy: 0.4909456740442656
At round 2 training loss: 1.0143718907564194
update_location
xs = -3.905658 -5.799682 30.009024 18.811294 -24.020704 -11.043590 2.556808 -6.324852 14.663977 -7.060879 
ys = 22.587959 15.555839 1.320614 7.544824 9.350187 -17.185849 2.375016 -4.177652 17.569006 4.001482 
dists_uav = 102.593714 101.368735 104.414010 102.033275 103.268680 102.065245 100.060871 100.286871 102.585097 100.328799 
dists_bs = 229.134685 232.489114 268.687977 256.132883 224.130784 252.620476 247.640476 246.081159 246.489233 239.674867 
uav_gains = -100.278042 -100.147620 -100.468999 -100.218567 -100.349241 -100.221968 -100.006623 -100.031119 -100.277130 -100.035657 
bs_gains = -105.649603 -105.826333 -107.586010 -107.004089 -105.381101 -106.836179 -106.594065 -106.517253 -106.537402 -106.196489 
Round 3
-------------------------------
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.59821531 22.14976829 10.43417884  3.73162013 25.53613015 12.31863725
  4.64005817 14.98083601 10.99100861  9.99386679]
obj_prev = 125.37431955255585
eta_min = 1.4920442712219943e-09	eta_max = 0.9182779578592448
af = 26.51569137072327	bf = 2.011304995713791	zeta = 29.1672605077956	eta = 0.9090909090909091
af = 26.51569137072327	bf = 2.011304995713791	zeta = 49.923723386827525	eta = 0.531124074325744
af = 26.51569137072327	bf = 2.011304995713791	zeta = 40.07321095605609	eta = 0.6616812263883753
af = 26.51569137072327	bf = 2.011304995713791	zeta = 38.31157890911957	eta = 0.692106462999664
af = 26.51569137072327	bf = 2.011304995713791	zeta = 38.22584319738169	eta = 0.6936587699009733
af = 26.51569137072327	bf = 2.011304995713791	zeta = 38.22562613755516	eta = 0.6936627087626083
eta = 0.6936627087626083
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [0.03018242 0.0634789  0.02970333 0.01030035 0.07330016 0.03497328
 0.01293532 0.0428782  0.03114059 0.02826607]
ene_total = [3.24068386 6.36286468 3.20278272 1.47684991 7.21615316 3.87841894
 1.70546401 4.36028936 3.52564504 3.25647446]
ti_comp = [0.27298014 0.25372925 0.27246494 0.27313894 0.25568174 0.24894535
 0.27369854 0.27363436 0.25041517 0.25203535]
ti_coms = [0.06375991 0.0830108  0.06427511 0.0636011  0.08105831 0.0877947
 0.06304151 0.06310569 0.08632488 0.08470469]
t_total = [29.84998741 29.84998741 29.84998741 29.84998741 29.84998741 29.84998741
 29.84998741 29.84998741 29.84998741 29.84998741]
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.30610989e-05 2.48328765e-04 2.20634935e-05 9.15521600e-07
 3.76526319e-04 4.31400861e-05 1.80578817e-06 6.58033963e-05
 3.00981129e-05 2.22204868e-05]
ene_total = [0.56723758 0.75785384 0.57171611 0.56386684 0.75191015 0.7820715
 0.55898529 0.56522719 0.76788634 0.75282607]
optimize_network iter = 0 obj = 6.639580919785389
eta = 0.6936627087626083
freqs = [5.52831753e+07 1.25091797e+08 5.45085400e+07 1.88555117e+07
 1.43342577e+08 7.02428831e+07 2.36306014e+07 7.83494392e+07
 6.21779230e+07 5.60756135e+07]
eta_min = 0.6679443667432773	eta_max = 0.6936627087626027
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 0.06810536589768883	eta = 0.9090909090909091
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 22.189266299545174	eta = 0.002790266616394072
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 2.3821060289713065	eta = 0.025991273371083312
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 2.2969923954010767	eta = 0.026954363942109684
af = 0.06191396899789893	bf = 2.011304995713791	zeta = 2.2969507848071644	eta = 0.02695485223602507
eta = 0.02695485223602507
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.27830406e-04 2.45334550e-03 2.17974637e-04 9.04482732e-06
 3.71986367e-03 4.26199261e-04 1.78401495e-05 6.50099743e-04
 2.97352060e-04 2.19525641e-04]
ene_total = [0.18429415 0.3001259  0.18545689 0.17774529 0.33002202 0.25690445
 0.17642907 0.19425277 0.24920683 0.24251343]
ti_comp = [0.30125093 0.28200003 0.30073572 0.30140973 0.28395252 0.27721613
 0.30196932 0.30190514 0.27868595 0.28030614]
ti_coms = [0.06375991 0.0830108  0.06427511 0.0636011  0.08105831 0.0877947
 0.06304151 0.06310569 0.08632488 0.08470469]
t_total = [29.84998741 29.84998741 29.84998741 29.84998741 29.84998741 29.84998741
 29.84998741 29.84998741 29.84998741 29.84998741]
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.54305861e-05 2.69985736e-04 2.43218458e-05 1.00970000e-06
 4.09991014e-04 4.67221780e-05 1.99231109e-06 7.25971523e-05
 3.26363375e-05 2.41258334e-05]
ene_total = [0.52349772 0.70092769 0.52762029 0.52020198 0.69640997 0.72179151
 0.51570607 0.52200485 0.70861965 0.69467407]
optimize_network iter = 1 obj = 6.13145379564688
eta = 0.6679443667432773
freqs = [5.52695580e+07 1.24176825e+08 5.44854450e+07 1.88518958e+07
 1.42403130e+08 6.95950244e+07 2.36306014e+07 7.83477189e+07
 6.16413456e+07 5.56279733e+07]
eta_min = 0.6679443667432772	eta_max = 0.6679443667432751
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 0.06726955400122223	eta = 0.9090909090909091
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 22.188469685723824	eta = 0.002756122475650405
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 2.378298126804526	eta = 0.025713403762074868
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 2.294165378733591	eta = 0.026656378205336263
af = 0.061154140001111114	bf = 2.011304995713791	zeta = 2.2941251141107752	eta = 0.02665684605646063
eta = 0.02665684605646063
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.28418763e-04 2.42502502e-03 2.18460004e-04 9.06917459e-06
 3.68255925e-03 4.19660878e-04 1.78950352e-05 6.52071157e-04
 2.93141173e-04 2.16699410e-04]
ene_total = [0.18425414 0.2992439  0.18541365 0.17769155 0.32888024 0.25664338
 0.17637659 0.19424829 0.24901305 0.24236033]
ti_comp = [0.30125093 0.28200003 0.30073572 0.30140973 0.28395252 0.27721613
 0.30196932 0.30190514 0.27868595 0.28030614]
ti_coms = [0.06375991 0.0830108  0.06427511 0.0636011  0.08105831 0.0877947
 0.06304151 0.06310569 0.08632488 0.08470469]
t_total = [29.84998741 29.84998741 29.84998741 29.84998741 29.84998741 29.84998741
 29.84998741 29.84998741 29.84998741 29.84998741]
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.54305861e-05 2.69985736e-04 2.43218458e-05 1.00970000e-06
 4.09991014e-04 4.67221780e-05 1.99231109e-06 7.25971523e-05
 3.26363375e-05 2.41258334e-05]
ene_total = [0.52349772 0.70092769 0.52762029 0.52020198 0.69640997 0.72179151
 0.51570607 0.52200485 0.70861965 0.69467407]
optimize_network iter = 2 obj = 6.1314537956468795
eta = 0.6679443667432772
freqs = [5.52695580e+07 1.24176825e+08 5.44854450e+07 1.88518958e+07
 1.42403130e+08 6.95950244e+07 2.36306014e+07 7.83477189e+07
 6.16413456e+07 5.56279733e+07]
Done!
ene_coms = [0.00637599 0.00830108 0.00642751 0.00636011 0.00810583 0.00877947
 0.00630415 0.00631057 0.00863249 0.00847047]
ene_comp = [2.50181899e-05 2.65607500e-04 2.39274295e-05 9.93326158e-07
 4.03342377e-04 4.59645058e-05 1.96000269e-06 7.14198774e-05
 3.21070889e-05 2.37345958e-05]
ene_total = [0.00640101 0.00856669 0.00645144 0.0063611  0.00850917 0.00882543
 0.00630611 0.00638199 0.00866459 0.0084942 ]
At round 3 energy consumption: 0.07496174437238437
At round 3 eta: 0.6679443667432772
At round 3 a_n: 27.154965315346423
At round 3 local rounds: 13.214290123550223
At round 3 global rounds: 81.77836059884595
gradient difference: 0.4899936616420746
train() client id: f_00000-0-0 loss: 1.104478  [   32/  126]
train() client id: f_00000-0-1 loss: 1.292173  [   64/  126]
train() client id: f_00000-0-2 loss: 1.134353  [   96/  126]
train() client id: f_00000-1-0 loss: 1.087111  [   32/  126]
train() client id: f_00000-1-1 loss: 1.091858  [   64/  126]
train() client id: f_00000-1-2 loss: 1.200151  [   96/  126]
train() client id: f_00000-2-0 loss: 1.111712  [   32/  126]
train() client id: f_00000-2-1 loss: 1.051373  [   64/  126]
train() client id: f_00000-2-2 loss: 1.026739  [   96/  126]
train() client id: f_00000-3-0 loss: 1.063961  [   32/  126]
train() client id: f_00000-3-1 loss: 1.030366  [   64/  126]
train() client id: f_00000-3-2 loss: 0.985536  [   96/  126]
train() client id: f_00000-4-0 loss: 1.007189  [   32/  126]
train() client id: f_00000-4-1 loss: 0.983264  [   64/  126]
train() client id: f_00000-4-2 loss: 0.978024  [   96/  126]
train() client id: f_00000-5-0 loss: 1.014109  [   32/  126]
train() client id: f_00000-5-1 loss: 0.932656  [   64/  126]
train() client id: f_00000-5-2 loss: 0.945554  [   96/  126]
train() client id: f_00000-6-0 loss: 1.056076  [   32/  126]
train() client id: f_00000-6-1 loss: 0.955597  [   64/  126]
train() client id: f_00000-6-2 loss: 0.908687  [   96/  126]
train() client id: f_00000-7-0 loss: 0.951189  [   32/  126]
train() client id: f_00000-7-1 loss: 1.002266  [   64/  126]
train() client id: f_00000-7-2 loss: 0.977210  [   96/  126]
train() client id: f_00000-8-0 loss: 0.920091  [   32/  126]
train() client id: f_00000-8-1 loss: 1.002522  [   64/  126]
train() client id: f_00000-8-2 loss: 1.058318  [   96/  126]
train() client id: f_00000-9-0 loss: 1.068065  [   32/  126]
train() client id: f_00000-9-1 loss: 0.937954  [   64/  126]
train() client id: f_00000-9-2 loss: 0.995350  [   96/  126]
train() client id: f_00000-10-0 loss: 0.927832  [   32/  126]
train() client id: f_00000-10-1 loss: 1.033263  [   64/  126]
train() client id: f_00000-10-2 loss: 0.922715  [   96/  126]
train() client id: f_00000-11-0 loss: 1.047612  [   32/  126]
train() client id: f_00000-11-1 loss: 0.947563  [   64/  126]
train() client id: f_00000-11-2 loss: 1.012693  [   96/  126]
train() client id: f_00000-12-0 loss: 0.904091  [   32/  126]
train() client id: f_00000-12-1 loss: 1.069366  [   64/  126]
train() client id: f_00000-12-2 loss: 0.962649  [   96/  126]
train() client id: f_00001-0-0 loss: 0.789516  [   32/  265]
train() client id: f_00001-0-1 loss: 0.831302  [   64/  265]
train() client id: f_00001-0-2 loss: 0.847967  [   96/  265]
train() client id: f_00001-0-3 loss: 0.784453  [  128/  265]
train() client id: f_00001-0-4 loss: 0.758707  [  160/  265]
train() client id: f_00001-0-5 loss: 0.764020  [  192/  265]
train() client id: f_00001-0-6 loss: 0.836158  [  224/  265]
train() client id: f_00001-0-7 loss: 0.756809  [  256/  265]
train() client id: f_00001-1-0 loss: 0.790204  [   32/  265]
train() client id: f_00001-1-1 loss: 0.860303  [   64/  265]
train() client id: f_00001-1-2 loss: 0.710212  [   96/  265]
train() client id: f_00001-1-3 loss: 0.780124  [  128/  265]
train() client id: f_00001-1-4 loss: 0.683932  [  160/  265]
train() client id: f_00001-1-5 loss: 0.727049  [  192/  265]
train() client id: f_00001-1-6 loss: 0.708907  [  224/  265]
train() client id: f_00001-1-7 loss: 0.639775  [  256/  265]
train() client id: f_00001-2-0 loss: 0.625195  [   32/  265]
train() client id: f_00001-2-1 loss: 0.657895  [   64/  265]
train() client id: f_00001-2-2 loss: 0.702995  [   96/  265]
train() client id: f_00001-2-3 loss: 0.802924  [  128/  265]
train() client id: f_00001-2-4 loss: 0.723529  [  160/  265]
train() client id: f_00001-2-5 loss: 0.734762  [  192/  265]
train() client id: f_00001-2-6 loss: 0.657154  [  224/  265]
train() client id: f_00001-2-7 loss: 0.709950  [  256/  265]
train() client id: f_00001-3-0 loss: 0.745045  [   32/  265]
train() client id: f_00001-3-1 loss: 0.708251  [   64/  265]
train() client id: f_00001-3-2 loss: 0.654769  [   96/  265]
train() client id: f_00001-3-3 loss: 0.664689  [  128/  265]
train() client id: f_00001-3-4 loss: 0.640192  [  160/  265]
train() client id: f_00001-3-5 loss: 0.655994  [  192/  265]
train() client id: f_00001-3-6 loss: 0.656656  [  224/  265]
train() client id: f_00001-3-7 loss: 0.699537  [  256/  265]
train() client id: f_00001-4-0 loss: 0.766109  [   32/  265]
train() client id: f_00001-4-1 loss: 0.661957  [   64/  265]
train() client id: f_00001-4-2 loss: 0.687633  [   96/  265]
train() client id: f_00001-4-3 loss: 0.613405  [  128/  265]
train() client id: f_00001-4-4 loss: 0.634760  [  160/  265]
train() client id: f_00001-4-5 loss: 0.598238  [  192/  265]
train() client id: f_00001-4-6 loss: 0.598841  [  224/  265]
train() client id: f_00001-4-7 loss: 0.725874  [  256/  265]
train() client id: f_00001-5-0 loss: 0.659601  [   32/  265]
train() client id: f_00001-5-1 loss: 0.714421  [   64/  265]
train() client id: f_00001-5-2 loss: 0.580396  [   96/  265]
train() client id: f_00001-5-3 loss: 0.715313  [  128/  265]
train() client id: f_00001-5-4 loss: 0.578429  [  160/  265]
train() client id: f_00001-5-5 loss: 0.671548  [  192/  265]
train() client id: f_00001-5-6 loss: 0.675734  [  224/  265]
train() client id: f_00001-5-7 loss: 0.611089  [  256/  265]
train() client id: f_00001-6-0 loss: 0.579050  [   32/  265]
train() client id: f_00001-6-1 loss: 0.710019  [   64/  265]
train() client id: f_00001-6-2 loss: 0.698503  [   96/  265]
train() client id: f_00001-6-3 loss: 0.671402  [  128/  265]
train() client id: f_00001-6-4 loss: 0.548106  [  160/  265]
train() client id: f_00001-6-5 loss: 0.570771  [  192/  265]
train() client id: f_00001-6-6 loss: 0.687101  [  224/  265]
train() client id: f_00001-6-7 loss: 0.623051  [  256/  265]
train() client id: f_00001-7-0 loss: 0.564687  [   32/  265]
train() client id: f_00001-7-1 loss: 0.753206  [   64/  265]
train() client id: f_00001-7-2 loss: 0.552569  [   96/  265]
train() client id: f_00001-7-3 loss: 0.695910  [  128/  265]
train() client id: f_00001-7-4 loss: 0.623158  [  160/  265]
train() client id: f_00001-7-5 loss: 0.661821  [  192/  265]
train() client id: f_00001-7-6 loss: 0.605270  [  224/  265]
train() client id: f_00001-7-7 loss: 0.661725  [  256/  265]
train() client id: f_00001-8-0 loss: 0.662765  [   32/  265]
train() client id: f_00001-8-1 loss: 0.646785  [   64/  265]
train() client id: f_00001-8-2 loss: 0.658592  [   96/  265]
train() client id: f_00001-8-3 loss: 0.577856  [  128/  265]
train() client id: f_00001-8-4 loss: 0.594077  [  160/  265]
train() client id: f_00001-8-5 loss: 0.617525  [  192/  265]
train() client id: f_00001-8-6 loss: 0.580545  [  224/  265]
train() client id: f_00001-8-7 loss: 0.676949  [  256/  265]
train() client id: f_00001-9-0 loss: 0.538324  [   32/  265]
train() client id: f_00001-9-1 loss: 0.769303  [   64/  265]
train() client id: f_00001-9-2 loss: 0.560019  [   96/  265]
train() client id: f_00001-9-3 loss: 0.601783  [  128/  265]
train() client id: f_00001-9-4 loss: 0.725192  [  160/  265]
train() client id: f_00001-9-5 loss: 0.671718  [  192/  265]
train() client id: f_00001-9-6 loss: 0.606919  [  224/  265]
train() client id: f_00001-9-7 loss: 0.620132  [  256/  265]
train() client id: f_00001-10-0 loss: 0.528790  [   32/  265]
train() client id: f_00001-10-1 loss: 0.621482  [   64/  265]
train() client id: f_00001-10-2 loss: 0.536706  [   96/  265]
train() client id: f_00001-10-3 loss: 0.707418  [  128/  265]
train() client id: f_00001-10-4 loss: 0.568840  [  160/  265]
train() client id: f_00001-10-5 loss: 0.778226  [  192/  265]
train() client id: f_00001-10-6 loss: 0.740204  [  224/  265]
train() client id: f_00001-10-7 loss: 0.562508  [  256/  265]
train() client id: f_00001-11-0 loss: 0.687052  [   32/  265]
train() client id: f_00001-11-1 loss: 0.594423  [   64/  265]
train() client id: f_00001-11-2 loss: 0.685467  [   96/  265]
train() client id: f_00001-11-3 loss: 0.530023  [  128/  265]
train() client id: f_00001-11-4 loss: 0.554521  [  160/  265]
train() client id: f_00001-11-5 loss: 0.715071  [  192/  265]
train() client id: f_00001-11-6 loss: 0.641261  [  224/  265]
train() client id: f_00001-11-7 loss: 0.619727  [  256/  265]
train() client id: f_00001-12-0 loss: 0.550375  [   32/  265]
train() client id: f_00001-12-1 loss: 0.508194  [   64/  265]
train() client id: f_00001-12-2 loss: 0.563589  [   96/  265]
train() client id: f_00001-12-3 loss: 0.594653  [  128/  265]
train() client id: f_00001-12-4 loss: 0.618161  [  160/  265]
train() client id: f_00001-12-5 loss: 0.661065  [  192/  265]
train() client id: f_00001-12-6 loss: 0.819287  [  224/  265]
train() client id: f_00001-12-7 loss: 0.621077  [  256/  265]
train() client id: f_00002-0-0 loss: 1.165990  [   32/  124]
train() client id: f_00002-0-1 loss: 1.111541  [   64/  124]
train() client id: f_00002-0-2 loss: 1.062387  [   96/  124]
train() client id: f_00002-1-0 loss: 1.087078  [   32/  124]
train() client id: f_00002-1-1 loss: 1.059709  [   64/  124]
train() client id: f_00002-1-2 loss: 1.052417  [   96/  124]
train() client id: f_00002-2-0 loss: 1.028729  [   32/  124]
train() client id: f_00002-2-1 loss: 1.008942  [   64/  124]
train() client id: f_00002-2-2 loss: 1.010512  [   96/  124]
train() client id: f_00002-3-0 loss: 0.984908  [   32/  124]
train() client id: f_00002-3-1 loss: 0.990153  [   64/  124]
train() client id: f_00002-3-2 loss: 0.967462  [   96/  124]
train() client id: f_00002-4-0 loss: 0.902626  [   32/  124]
train() client id: f_00002-4-1 loss: 0.992595  [   64/  124]
train() client id: f_00002-4-2 loss: 0.954962  [   96/  124]
train() client id: f_00002-5-0 loss: 0.931719  [   32/  124]
train() client id: f_00002-5-1 loss: 0.886601  [   64/  124]
train() client id: f_00002-5-2 loss: 0.886734  [   96/  124]
train() client id: f_00002-6-0 loss: 0.907264  [   32/  124]
train() client id: f_00002-6-1 loss: 0.854886  [   64/  124]
train() client id: f_00002-6-2 loss: 0.970132  [   96/  124]
train() client id: f_00002-7-0 loss: 0.870790  [   32/  124]
train() client id: f_00002-7-1 loss: 0.842484  [   64/  124]
train() client id: f_00002-7-2 loss: 0.860018  [   96/  124]
train() client id: f_00002-8-0 loss: 0.873436  [   32/  124]
train() client id: f_00002-8-1 loss: 0.830957  [   64/  124]
train() client id: f_00002-8-2 loss: 0.858015  [   96/  124]
train() client id: f_00002-9-0 loss: 0.889969  [   32/  124]
train() client id: f_00002-9-1 loss: 0.818949  [   64/  124]
train() client id: f_00002-9-2 loss: 0.849071  [   96/  124]
train() client id: f_00002-10-0 loss: 0.852289  [   32/  124]
train() client id: f_00002-10-1 loss: 0.870083  [   64/  124]
train() client id: f_00002-10-2 loss: 0.754571  [   96/  124]
train() client id: f_00002-11-0 loss: 0.843076  [   32/  124]
train() client id: f_00002-11-1 loss: 0.765362  [   64/  124]
train() client id: f_00002-11-2 loss: 0.913757  [   96/  124]
train() client id: f_00002-12-0 loss: 0.784286  [   32/  124]
train() client id: f_00002-12-1 loss: 0.794290  [   64/  124]
train() client id: f_00002-12-2 loss: 0.807637  [   96/  124]
train() client id: f_00003-0-0 loss: 1.064860  [   32/   43]
train() client id: f_00003-1-0 loss: 1.064857  [   32/   43]
train() client id: f_00003-2-0 loss: 1.044675  [   32/   43]
train() client id: f_00003-3-0 loss: 1.059189  [   32/   43]
train() client id: f_00003-4-0 loss: 1.071196  [   32/   43]
train() client id: f_00003-5-0 loss: 1.053683  [   32/   43]
train() client id: f_00003-6-0 loss: 1.078994  [   32/   43]
train() client id: f_00003-7-0 loss: 1.007001  [   32/   43]
train() client id: f_00003-8-0 loss: 1.052757  [   32/   43]
train() client id: f_00003-9-0 loss: 1.029818  [   32/   43]
train() client id: f_00003-10-0 loss: 1.083901  [   32/   43]
train() client id: f_00003-11-0 loss: 1.070569  [   32/   43]
train() client id: f_00003-12-0 loss: 1.057619  [   32/   43]
train() client id: f_00004-0-0 loss: 1.144171  [   32/  306]
train() client id: f_00004-0-1 loss: 1.117018  [   64/  306]
train() client id: f_00004-0-2 loss: 1.018454  [   96/  306]
train() client id: f_00004-0-3 loss: 1.002768  [  128/  306]
train() client id: f_00004-0-4 loss: 1.023058  [  160/  306]
train() client id: f_00004-0-5 loss: 1.069331  [  192/  306]
train() client id: f_00004-0-6 loss: 1.024298  [  224/  306]
train() client id: f_00004-0-7 loss: 1.008003  [  256/  306]
train() client id: f_00004-0-8 loss: 1.011269  [  288/  306]
train() client id: f_00004-1-0 loss: 1.034614  [   32/  306]
train() client id: f_00004-1-1 loss: 1.117864  [   64/  306]
train() client id: f_00004-1-2 loss: 1.090826  [   96/  306]
train() client id: f_00004-1-3 loss: 0.952770  [  128/  306]
train() client id: f_00004-1-4 loss: 1.020328  [  160/  306]
train() client id: f_00004-1-5 loss: 0.970813  [  192/  306]
train() client id: f_00004-1-6 loss: 0.997994  [  224/  306]
train() client id: f_00004-1-7 loss: 1.124623  [  256/  306]
train() client id: f_00004-1-8 loss: 1.025117  [  288/  306]
train() client id: f_00004-2-0 loss: 1.065847  [   32/  306]
train() client id: f_00004-2-1 loss: 0.990488  [   64/  306]
train() client id: f_00004-2-2 loss: 1.088466  [   96/  306]
train() client id: f_00004-2-3 loss: 1.023992  [  128/  306]
train() client id: f_00004-2-4 loss: 0.996327  [  160/  306]
train() client id: f_00004-2-5 loss: 1.052662  [  192/  306]
train() client id: f_00004-2-6 loss: 0.984142  [  224/  306]
train() client id: f_00004-2-7 loss: 1.074848  [  256/  306]
train() client id: f_00004-2-8 loss: 1.050778  [  288/  306]
train() client id: f_00004-3-0 loss: 1.073129  [   32/  306]
train() client id: f_00004-3-1 loss: 1.052937  [   64/  306]
train() client id: f_00004-3-2 loss: 0.939698  [   96/  306]
train() client id: f_00004-3-3 loss: 1.087959  [  128/  306]
train() client id: f_00004-3-4 loss: 1.045447  [  160/  306]
train() client id: f_00004-3-5 loss: 1.037034  [  192/  306]
train() client id: f_00004-3-6 loss: 0.958347  [  224/  306]
train() client id: f_00004-3-7 loss: 1.059309  [  256/  306]
train() client id: f_00004-3-8 loss: 1.064725  [  288/  306]
train() client id: f_00004-4-0 loss: 1.052293  [   32/  306]
train() client id: f_00004-4-1 loss: 1.081244  [   64/  306]
train() client id: f_00004-4-2 loss: 0.931855  [   96/  306]
train() client id: f_00004-4-3 loss: 1.077037  [  128/  306]
train() client id: f_00004-4-4 loss: 1.011229  [  160/  306]
train() client id: f_00004-4-5 loss: 0.984052  [  192/  306]
train() client id: f_00004-4-6 loss: 0.929508  [  224/  306]
train() client id: f_00004-4-7 loss: 1.189437  [  256/  306]
train() client id: f_00004-4-8 loss: 1.072731  [  288/  306]
train() client id: f_00004-5-0 loss: 0.966232  [   32/  306]
train() client id: f_00004-5-1 loss: 1.087096  [   64/  306]
train() client id: f_00004-5-2 loss: 1.114921  [   96/  306]
train() client id: f_00004-5-3 loss: 1.065295  [  128/  306]
train() client id: f_00004-5-4 loss: 1.082490  [  160/  306]
train() client id: f_00004-5-5 loss: 1.060983  [  192/  306]
train() client id: f_00004-5-6 loss: 1.003636  [  224/  306]
train() client id: f_00004-5-7 loss: 0.936078  [  256/  306]
train() client id: f_00004-5-8 loss: 0.944436  [  288/  306]
train() client id: f_00004-6-0 loss: 1.103810  [   32/  306]
train() client id: f_00004-6-1 loss: 1.102947  [   64/  306]
train() client id: f_00004-6-2 loss: 1.038096  [   96/  306]
train() client id: f_00004-6-3 loss: 0.954510  [  128/  306]
train() client id: f_00004-6-4 loss: 0.974023  [  160/  306]
train() client id: f_00004-6-5 loss: 0.978780  [  192/  306]
train() client id: f_00004-6-6 loss: 1.019750  [  224/  306]
train() client id: f_00004-6-7 loss: 1.021232  [  256/  306]
train() client id: f_00004-6-8 loss: 1.085940  [  288/  306]
train() client id: f_00004-7-0 loss: 1.027257  [   32/  306]
train() client id: f_00004-7-1 loss: 0.955511  [   64/  306]
train() client id: f_00004-7-2 loss: 1.165797  [   96/  306]
train() client id: f_00004-7-3 loss: 0.995741  [  128/  306]
train() client id: f_00004-7-4 loss: 1.016918  [  160/  306]
train() client id: f_00004-7-5 loss: 0.961223  [  192/  306]
train() client id: f_00004-7-6 loss: 1.012697  [  224/  306]
train() client id: f_00004-7-7 loss: 1.035018  [  256/  306]
train() client id: f_00004-7-8 loss: 1.120375  [  288/  306]
train() client id: f_00004-8-0 loss: 0.948422  [   32/  306]
train() client id: f_00004-8-1 loss: 0.991802  [   64/  306]
train() client id: f_00004-8-2 loss: 1.070335  [   96/  306]
train() client id: f_00004-8-3 loss: 1.043104  [  128/  306]
train() client id: f_00004-8-4 loss: 1.091772  [  160/  306]
train() client id: f_00004-8-5 loss: 0.957648  [  192/  306]
train() client id: f_00004-8-6 loss: 1.006184  [  224/  306]
train() client id: f_00004-8-7 loss: 1.062468  [  256/  306]
train() client id: f_00004-8-8 loss: 1.070206  [  288/  306]
train() client id: f_00004-9-0 loss: 1.080987  [   32/  306]
train() client id: f_00004-9-1 loss: 1.048408  [   64/  306]
train() client id: f_00004-9-2 loss: 1.115754  [   96/  306]
train() client id: f_00004-9-3 loss: 1.000426  [  128/  306]
train() client id: f_00004-9-4 loss: 1.072075  [  160/  306]
train() client id: f_00004-9-5 loss: 1.003210  [  192/  306]
train() client id: f_00004-9-6 loss: 1.013739  [  224/  306]
train() client id: f_00004-9-7 loss: 0.983904  [  256/  306]
train() client id: f_00004-9-8 loss: 0.979957  [  288/  306]
train() client id: f_00004-10-0 loss: 1.009966  [   32/  306]
train() client id: f_00004-10-1 loss: 1.016972  [   64/  306]
train() client id: f_00004-10-2 loss: 0.978984  [   96/  306]
train() client id: f_00004-10-3 loss: 0.994754  [  128/  306]
train() client id: f_00004-10-4 loss: 0.986934  [  160/  306]
train() client id: f_00004-10-5 loss: 1.022126  [  192/  306]
train() client id: f_00004-10-6 loss: 1.072247  [  224/  306]
train() client id: f_00004-10-7 loss: 1.042946  [  256/  306]
train() client id: f_00004-10-8 loss: 1.065804  [  288/  306]
train() client id: f_00004-11-0 loss: 0.952595  [   32/  306]
train() client id: f_00004-11-1 loss: 0.967751  [   64/  306]
train() client id: f_00004-11-2 loss: 1.069568  [   96/  306]
train() client id: f_00004-11-3 loss: 1.029834  [  128/  306]
train() client id: f_00004-11-4 loss: 0.968381  [  160/  306]
train() client id: f_00004-11-5 loss: 1.120472  [  192/  306]
train() client id: f_00004-11-6 loss: 1.073005  [  224/  306]
train() client id: f_00004-11-7 loss: 0.991123  [  256/  306]
train() client id: f_00004-11-8 loss: 1.067467  [  288/  306]
train() client id: f_00004-12-0 loss: 0.965980  [   32/  306]
train() client id: f_00004-12-1 loss: 1.061462  [   64/  306]
train() client id: f_00004-12-2 loss: 1.058350  [   96/  306]
train() client id: f_00004-12-3 loss: 1.008960  [  128/  306]
train() client id: f_00004-12-4 loss: 1.051506  [  160/  306]
train() client id: f_00004-12-5 loss: 0.982229  [  192/  306]
train() client id: f_00004-12-6 loss: 1.063243  [  224/  306]
train() client id: f_00004-12-7 loss: 0.952362  [  256/  306]
train() client id: f_00004-12-8 loss: 1.103201  [  288/  306]
train() client id: f_00005-0-0 loss: 0.938835  [   32/  146]
train() client id: f_00005-0-1 loss: 0.994642  [   64/  146]
train() client id: f_00005-0-2 loss: 0.971741  [   96/  146]
train() client id: f_00005-0-3 loss: 0.960916  [  128/  146]
train() client id: f_00005-1-0 loss: 0.948418  [   32/  146]
train() client id: f_00005-1-1 loss: 0.925298  [   64/  146]
train() client id: f_00005-1-2 loss: 0.945255  [   96/  146]
train() client id: f_00005-1-3 loss: 0.985340  [  128/  146]
train() client id: f_00005-2-0 loss: 0.926473  [   32/  146]
train() client id: f_00005-2-1 loss: 0.964240  [   64/  146]
train() client id: f_00005-2-2 loss: 0.875794  [   96/  146]
train() client id: f_00005-2-3 loss: 0.925765  [  128/  146]
train() client id: f_00005-3-0 loss: 0.962409  [   32/  146]
train() client id: f_00005-3-1 loss: 0.858527  [   64/  146]
train() client id: f_00005-3-2 loss: 0.918182  [   96/  146]
train() client id: f_00005-3-3 loss: 0.869689  [  128/  146]
train() client id: f_00005-4-0 loss: 0.807296  [   32/  146]
train() client id: f_00005-4-1 loss: 0.889467  [   64/  146]
train() client id: f_00005-4-2 loss: 0.874453  [   96/  146]
train() client id: f_00005-4-3 loss: 0.957667  [  128/  146]
train() client id: f_00005-5-0 loss: 0.917616  [   32/  146]
train() client id: f_00005-5-1 loss: 0.964935  [   64/  146]
train() client id: f_00005-5-2 loss: 0.903154  [   96/  146]
train() client id: f_00005-5-3 loss: 0.792159  [  128/  146]
train() client id: f_00005-6-0 loss: 0.853851  [   32/  146]
train() client id: f_00005-6-1 loss: 0.944481  [   64/  146]
train() client id: f_00005-6-2 loss: 0.837272  [   96/  146]
train() client id: f_00005-6-3 loss: 0.890446  [  128/  146]
train() client id: f_00005-7-0 loss: 0.787486  [   32/  146]
train() client id: f_00005-7-1 loss: 0.799262  [   64/  146]
train() client id: f_00005-7-2 loss: 0.977860  [   96/  146]
train() client id: f_00005-7-3 loss: 0.913443  [  128/  146]
train() client id: f_00005-8-0 loss: 0.957294  [   32/  146]
train() client id: f_00005-8-1 loss: 0.678679  [   64/  146]
train() client id: f_00005-8-2 loss: 0.908364  [   96/  146]
train() client id: f_00005-8-3 loss: 0.979377  [  128/  146]
train() client id: f_00005-9-0 loss: 0.716507  [   32/  146]
train() client id: f_00005-9-1 loss: 0.939832  [   64/  146]
train() client id: f_00005-9-2 loss: 0.781260  [   96/  146]
train() client id: f_00005-9-3 loss: 0.981254  [  128/  146]
train() client id: f_00005-10-0 loss: 0.735629  [   32/  146]
train() client id: f_00005-10-1 loss: 0.713040  [   64/  146]
train() client id: f_00005-10-2 loss: 1.054003  [   96/  146]
train() client id: f_00005-10-3 loss: 0.910625  [  128/  146]
train() client id: f_00005-11-0 loss: 0.876362  [   32/  146]
train() client id: f_00005-11-1 loss: 0.848415  [   64/  146]
train() client id: f_00005-11-2 loss: 0.774807  [   96/  146]
train() client id: f_00005-11-3 loss: 0.776379  [  128/  146]
train() client id: f_00005-12-0 loss: 0.897133  [   32/  146]
train() client id: f_00005-12-1 loss: 0.879404  [   64/  146]
train() client id: f_00005-12-2 loss: 0.770131  [   96/  146]
train() client id: f_00005-12-3 loss: 0.857477  [  128/  146]
train() client id: f_00006-0-0 loss: 1.019964  [   32/   54]
train() client id: f_00006-1-0 loss: 1.026506  [   32/   54]
train() client id: f_00006-2-0 loss: 1.051655  [   32/   54]
train() client id: f_00006-3-0 loss: 1.054807  [   32/   54]
train() client id: f_00006-4-0 loss: 1.052424  [   32/   54]
train() client id: f_00006-5-0 loss: 1.033405  [   32/   54]
train() client id: f_00006-6-0 loss: 1.066528  [   32/   54]
train() client id: f_00006-7-0 loss: 1.040774  [   32/   54]
train() client id: f_00006-8-0 loss: 1.036814  [   32/   54]
train() client id: f_00006-9-0 loss: 1.055774  [   32/   54]
train() client id: f_00006-10-0 loss: 1.054812  [   32/   54]
train() client id: f_00006-11-0 loss: 1.056879  [   32/   54]
train() client id: f_00006-12-0 loss: 1.028176  [   32/   54]
train() client id: f_00007-0-0 loss: 1.000928  [   32/  179]
train() client id: f_00007-0-1 loss: 0.967471  [   64/  179]
train() client id: f_00007-0-2 loss: 0.950555  [   96/  179]
train() client id: f_00007-0-3 loss: 0.910368  [  128/  179]
train() client id: f_00007-0-4 loss: 0.911445  [  160/  179]
train() client id: f_00007-1-0 loss: 0.880421  [   32/  179]
train() client id: f_00007-1-1 loss: 0.980227  [   64/  179]
train() client id: f_00007-1-2 loss: 0.872618  [   96/  179]
train() client id: f_00007-1-3 loss: 0.827610  [  128/  179]
train() client id: f_00007-1-4 loss: 0.874266  [  160/  179]
train() client id: f_00007-2-0 loss: 0.791086  [   32/  179]
train() client id: f_00007-2-1 loss: 0.869871  [   64/  179]
train() client id: f_00007-2-2 loss: 0.776854  [   96/  179]
train() client id: f_00007-2-3 loss: 0.924768  [  128/  179]
train() client id: f_00007-2-4 loss: 0.808646  [  160/  179]
train() client id: f_00007-3-0 loss: 0.799813  [   32/  179]
train() client id: f_00007-3-1 loss: 0.742484  [   64/  179]
train() client id: f_00007-3-2 loss: 0.827885  [   96/  179]
train() client id: f_00007-3-3 loss: 0.842247  [  128/  179]
train() client id: f_00007-3-4 loss: 0.715115  [  160/  179]
train() client id: f_00007-4-0 loss: 0.676756  [   32/  179]
train() client id: f_00007-4-1 loss: 0.814559  [   64/  179]
train() client id: f_00007-4-2 loss: 0.712644  [   96/  179]
train() client id: f_00007-4-3 loss: 0.755907  [  128/  179]
train() client id: f_00007-4-4 loss: 0.921372  [  160/  179]
train() client id: f_00007-5-0 loss: 0.745204  [   32/  179]
train() client id: f_00007-5-1 loss: 0.779529  [   64/  179]
train() client id: f_00007-5-2 loss: 0.707096  [   96/  179]
train() client id: f_00007-5-3 loss: 0.691872  [  128/  179]
train() client id: f_00007-5-4 loss: 0.781052  [  160/  179]
train() client id: f_00007-6-0 loss: 0.661977  [   32/  179]
train() client id: f_00007-6-1 loss: 0.703370  [   64/  179]
train() client id: f_00007-6-2 loss: 0.713774  [   96/  179]
train() client id: f_00007-6-3 loss: 0.783817  [  128/  179]
train() client id: f_00007-6-4 loss: 0.706335  [  160/  179]
train() client id: f_00007-7-0 loss: 0.698451  [   32/  179]
train() client id: f_00007-7-1 loss: 0.755895  [   64/  179]
train() client id: f_00007-7-2 loss: 0.634483  [   96/  179]
train() client id: f_00007-7-3 loss: 0.651411  [  128/  179]
train() client id: f_00007-7-4 loss: 0.886513  [  160/  179]
train() client id: f_00007-8-0 loss: 0.622539  [   32/  179]
train() client id: f_00007-8-1 loss: 0.704838  [   64/  179]
train() client id: f_00007-8-2 loss: 0.863893  [   96/  179]
train() client id: f_00007-8-3 loss: 0.685887  [  128/  179]
train() client id: f_00007-8-4 loss: 0.697077  [  160/  179]
train() client id: f_00007-9-0 loss: 0.696990  [   32/  179]
train() client id: f_00007-9-1 loss: 0.755476  [   64/  179]
train() client id: f_00007-9-2 loss: 0.712416  [   96/  179]
train() client id: f_00007-9-3 loss: 0.723987  [  128/  179]
train() client id: f_00007-9-4 loss: 0.631625  [  160/  179]
train() client id: f_00007-10-0 loss: 0.718946  [   32/  179]
train() client id: f_00007-10-1 loss: 0.770204  [   64/  179]
train() client id: f_00007-10-2 loss: 0.687546  [   96/  179]
train() client id: f_00007-10-3 loss: 0.700892  [  128/  179]
train() client id: f_00007-10-4 loss: 0.663406  [  160/  179]
train() client id: f_00007-11-0 loss: 0.752280  [   32/  179]
train() client id: f_00007-11-1 loss: 0.661626  [   64/  179]
train() client id: f_00007-11-2 loss: 0.711384  [   96/  179]
train() client id: f_00007-11-3 loss: 0.658068  [  128/  179]
train() client id: f_00007-11-4 loss: 0.600368  [  160/  179]
train() client id: f_00007-12-0 loss: 0.674390  [   32/  179]
train() client id: f_00007-12-1 loss: 0.655937  [   64/  179]
train() client id: f_00007-12-2 loss: 0.732674  [   96/  179]
train() client id: f_00007-12-3 loss: 0.793313  [  128/  179]
train() client id: f_00007-12-4 loss: 0.595269  [  160/  179]
train() client id: f_00008-0-0 loss: 0.935904  [   32/  130]
train() client id: f_00008-0-1 loss: 0.969277  [   64/  130]
train() client id: f_00008-0-2 loss: 0.952143  [   96/  130]
train() client id: f_00008-0-3 loss: 0.907903  [  128/  130]
train() client id: f_00008-1-0 loss: 0.944799  [   32/  130]
train() client id: f_00008-1-1 loss: 0.886490  [   64/  130]
train() client id: f_00008-1-2 loss: 0.914454  [   96/  130]
train() client id: f_00008-1-3 loss: 0.958869  [  128/  130]
train() client id: f_00008-2-0 loss: 0.945253  [   32/  130]
train() client id: f_00008-2-1 loss: 0.937934  [   64/  130]
train() client id: f_00008-2-2 loss: 0.979862  [   96/  130]
train() client id: f_00008-2-3 loss: 0.898933  [  128/  130]
train() client id: f_00008-3-0 loss: 0.980301  [   32/  130]
train() client id: f_00008-3-1 loss: 0.871015  [   64/  130]
train() client id: f_00008-3-2 loss: 1.024988  [   96/  130]
train() client id: f_00008-3-3 loss: 0.876804  [  128/  130]
train() client id: f_00008-4-0 loss: 0.836796  [   32/  130]
train() client id: f_00008-4-1 loss: 0.890305  [   64/  130]
train() client id: f_00008-4-2 loss: 1.019903  [   96/  130]
train() client id: f_00008-4-3 loss: 0.998871  [  128/  130]
train() client id: f_00008-5-0 loss: 0.966349  [   32/  130]
train() client id: f_00008-5-1 loss: 0.898557  [   64/  130]
train() client id: f_00008-5-2 loss: 0.891334  [   96/  130]
train() client id: f_00008-5-3 loss: 0.977085  [  128/  130]
train() client id: f_00008-6-0 loss: 0.931273  [   32/  130]
train() client id: f_00008-6-1 loss: 1.047826  [   64/  130]
train() client id: f_00008-6-2 loss: 0.841763  [   96/  130]
train() client id: f_00008-6-3 loss: 0.914552  [  128/  130]
train() client id: f_00008-7-0 loss: 0.944207  [   32/  130]
train() client id: f_00008-7-1 loss: 0.975294  [   64/  130]
train() client id: f_00008-7-2 loss: 0.921590  [   96/  130]
train() client id: f_00008-7-3 loss: 0.933454  [  128/  130]
train() client id: f_00008-8-0 loss: 0.958793  [   32/  130]
train() client id: f_00008-8-1 loss: 0.921407  [   64/  130]
train() client id: f_00008-8-2 loss: 0.946404  [   96/  130]
train() client id: f_00008-8-3 loss: 0.947289  [  128/  130]
train() client id: f_00008-9-0 loss: 1.023606  [   32/  130]
train() client id: f_00008-9-1 loss: 0.949849  [   64/  130]
train() client id: f_00008-9-2 loss: 0.975066  [   96/  130]
train() client id: f_00008-9-3 loss: 0.834633  [  128/  130]
train() client id: f_00008-10-0 loss: 0.875425  [   32/  130]
train() client id: f_00008-10-1 loss: 0.934014  [   64/  130]
train() client id: f_00008-10-2 loss: 1.035547  [   96/  130]
train() client id: f_00008-10-3 loss: 0.944920  [  128/  130]
train() client id: f_00008-11-0 loss: 0.935345  [   32/  130]
train() client id: f_00008-11-1 loss: 0.935042  [   64/  130]
train() client id: f_00008-11-2 loss: 1.007709  [   96/  130]
train() client id: f_00008-11-3 loss: 0.922559  [  128/  130]
train() client id: f_00008-12-0 loss: 1.001229  [   32/  130]
train() client id: f_00008-12-1 loss: 0.895171  [   64/  130]
train() client id: f_00008-12-2 loss: 0.967638  [   96/  130]
train() client id: f_00008-12-3 loss: 0.924296  [  128/  130]
train() client id: f_00009-0-0 loss: 1.054582  [   32/  118]
train() client id: f_00009-0-1 loss: 0.937903  [   64/  118]
train() client id: f_00009-0-2 loss: 1.061620  [   96/  118]
train() client id: f_00009-1-0 loss: 0.913282  [   32/  118]
train() client id: f_00009-1-1 loss: 0.951925  [   64/  118]
train() client id: f_00009-1-2 loss: 1.069356  [   96/  118]
train() client id: f_00009-2-0 loss: 0.956335  [   32/  118]
train() client id: f_00009-2-1 loss: 0.900023  [   64/  118]
train() client id: f_00009-2-2 loss: 0.975136  [   96/  118]
train() client id: f_00009-3-0 loss: 0.931048  [   32/  118]
train() client id: f_00009-3-1 loss: 0.843814  [   64/  118]
train() client id: f_00009-3-2 loss: 0.954137  [   96/  118]
train() client id: f_00009-4-0 loss: 0.830945  [   32/  118]
train() client id: f_00009-4-1 loss: 0.824934  [   64/  118]
train() client id: f_00009-4-2 loss: 0.928219  [   96/  118]
train() client id: f_00009-5-0 loss: 0.965586  [   32/  118]
train() client id: f_00009-5-1 loss: 0.780537  [   64/  118]
train() client id: f_00009-5-2 loss: 0.784860  [   96/  118]
train() client id: f_00009-6-0 loss: 0.888402  [   32/  118]
train() client id: f_00009-6-1 loss: 0.875488  [   64/  118]
train() client id: f_00009-6-2 loss: 0.827874  [   96/  118]
train() client id: f_00009-7-0 loss: 0.808254  [   32/  118]
train() client id: f_00009-7-1 loss: 0.729538  [   64/  118]
train() client id: f_00009-7-2 loss: 0.808372  [   96/  118]
train() client id: f_00009-8-0 loss: 0.841368  [   32/  118]
train() client id: f_00009-8-1 loss: 0.799379  [   64/  118]
train() client id: f_00009-8-2 loss: 0.757680  [   96/  118]
train() client id: f_00009-9-0 loss: 0.784108  [   32/  118]
train() client id: f_00009-9-1 loss: 0.745376  [   64/  118]
train() client id: f_00009-9-2 loss: 0.863002  [   96/  118]
train() client id: f_00009-10-0 loss: 0.682029  [   32/  118]
train() client id: f_00009-10-1 loss: 0.722618  [   64/  118]
train() client id: f_00009-10-2 loss: 0.884856  [   96/  118]
train() client id: f_00009-11-0 loss: 0.780885  [   32/  118]
train() client id: f_00009-11-1 loss: 0.821673  [   64/  118]
train() client id: f_00009-11-2 loss: 0.838702  [   96/  118]
train() client id: f_00009-12-0 loss: 0.759851  [   32/  118]
train() client id: f_00009-12-1 loss: 0.821698  [   64/  118]
train() client id: f_00009-12-2 loss: 0.802615  [   96/  118]
At round 3 accuracy: 0.5994694960212201
At round 3 training accuracy: 0.5580147551978538
At round 3 training loss: 0.9709357656027124
update_location
xs = -3.905658 -0.799682 35.009024 18.811294 -19.020704 -6.043590 2.556808 -6.324852 19.663977 -2.060879 
ys = 27.587959 15.555839 1.320614 2.544824 9.350187 -17.185849 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 103.809198 101.205848 105.959312 101.785760 102.221393 101.645848 100.067117 100.203194 103.418286 100.101244 
dists_bs = 225.839729 236.152898 272.522145 259.429384 227.528902 255.893863 251.151459 242.464760 250.356909 243.204509 
uav_gains = -100.405923 -100.130159 -100.628515 -100.192196 -100.238567 -100.177261 -100.007301 -100.022055 -100.364959 -100.011003 
bs_gains = -105.473469 -106.016471 -107.758309 -107.159596 -105.564083 -106.992736 -106.765259 -106.337221 -106.726727 -106.374265 
Round 4
-------------------------------
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.46545628 21.87269455 10.30373985  3.68435765 25.21614886 12.16538239
  4.58153103 14.79179615 10.85487015  9.87006751]
obj_prev = 123.8060444419656
eta_min = 1.1887003308068872e-09	eta_max = 0.9184674772800265
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 28.799330597431425	eta = 0.9090909090909091
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 49.438331310801296	eta = 0.5295730850913346
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 39.62791236585137	eta = 0.660675974861339
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 37.87221877758381	eta = 0.6913038232004766
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 37.78643716203915	eta = 0.6928732000256066
af = 26.181209634028566	bf = 1.9990579578134051	zeta = 37.78621816141451	eta = 0.6928772157665561
eta = 0.6928772157665561
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [0.03027591 0.06367553 0.02979534 0.01033226 0.07352722 0.03508161
 0.01297539 0.04301102 0.03123705 0.02835363]
ene_total = [3.202787   6.29122282 3.16613857 1.4566637  7.13363507 3.83605127
 1.68315721 4.30566359 3.48877145 3.22212748]
ti_comp = [0.2772398  0.25747119 0.276632   0.27781288 0.25949396 0.25275957
 0.27830054 0.2782619  0.25409312 0.25580165]
ti_coms = [0.06410402 0.08387264 0.06471183 0.06353094 0.08184987 0.08858426
 0.06304328 0.06308193 0.0872507  0.08554218]
t_total = [29.79998322 29.79998322 29.79998322 29.79998322 29.79998322 29.79998322
 29.79998322 29.79998322 29.79998322 29.79998322]
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.25663118e-05 2.43411145e-04 2.16033125e-05 8.93224701e-07
 3.68952017e-04 4.22380117e-05 1.76284161e-06 6.42261255e-05
 2.95055929e-05 2.17720570e-05]
ene_total = [0.56161072 0.75347478 0.56683292 0.55471554 0.74677556 0.77704532
 0.5505341  0.55632463 0.76429157 0.74870068]
optimize_network iter = 0 obj = 6.580305820795663
eta = 0.6928772157665561
freqs = [5.46023917e+07 1.23655643e+08 5.38537534e+07 1.85957107e+07
 1.41674235e+08 6.93972005e+07 2.33118325e+07 7.72851390e+07
 6.14677254e+07 5.54211297e+07]
eta_min = 0.6721669962857192	eta_max = 0.692877215766553
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 0.06566942832741904	eta = 0.9090909090909091
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 22.05222718616278	eta = 0.002707185981428379
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 2.358376629903991	eta = 0.02531380252868433
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 2.276155607764052	eta = 0.02622820693542063
af = 0.05969948029765367	bf = 1.9990579578134051	zeta = 2.276117460225635	eta = 0.02622864651797696
eta = 0.02622864651797696
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.23933498e-04 2.41545493e-03 2.14377315e-04 8.86378481e-06
 3.66124143e-03 4.19142738e-04 1.74933012e-05 6.37338573e-04
 2.92794440e-04 2.16051827e-04]
ene_total = [0.18267388 0.2974487  0.18408433 0.17517407 0.32618133 0.25545427
 0.17406893 0.19124253 0.24830343 0.241486  ]
ti_comp = [0.30025765 0.28048904 0.29964984 0.30083073 0.28251181 0.27577742
 0.30131839 0.30127974 0.27711097 0.27881949]
ti_coms = [0.06410402 0.08387264 0.06471183 0.06353094 0.08184987 0.08858426
 0.06304328 0.06308193 0.0872507  0.08554218]
t_total = [29.79998322 29.79998322 29.79998322 29.79998322 29.79998322 29.79998322
 29.79998322 29.79998322 29.79998322 29.79998322]
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.44184640e-05 2.60315910e-04 2.33685526e-05 9.66842755e-07
 3.95080819e-04 4.50335349e-05 1.90864377e-06 6.95367030e-05
 3.14859996e-05 2.32591919e-05]
ene_total = [0.52628353 0.70725804 0.53116872 0.51967847 0.70173644 0.72818562
 0.51576709 0.52161422 0.71617089 0.70152458]
optimize_network iter = 1 obj = 6.169387607456715
eta = 0.6721669962857192
freqs = [5.45864375e+07 1.22896148e+08 5.38289512e+07 1.85932175e+07
 1.40894199e+08 6.88656172e+07 2.33118325e+07 7.72843192e+07
 6.10236134e+07 5.50512470e+07]
eta_min = 0.6721669962857293	eta_max = 0.6721669962857161
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 0.06499207998561081	eta = 0.909090909090909
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 22.05158160424028	eta = 0.0026793411075088985
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 2.3552731564287246	eta = 0.02508571412048698
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 2.273849552272878	eta = 0.0259840010165886
af = 0.059083709077828	bf = 1.9990579578134051	zeta = 2.2738124460945865	eta = 0.025984425047592612
eta = 0.025984425047592612
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.24377690e-04 2.39200478e-03 2.14730208e-04 8.88417650e-06
 3.63033979e-03 4.13806558e-04 1.75382481e-05 6.38962582e-04
 2.89320240e-04 2.13725309e-04]
ene_total = [0.1826403  0.29672859 0.18404788 0.17513071 0.32524889 0.25524333
 0.17402652 0.19123929 0.24814553 0.24136141]
ti_comp = [0.30025765 0.28048904 0.29964984 0.30083073 0.28251181 0.27577742
 0.30131839 0.30127974 0.27711097 0.27881949]
ti_coms = [0.06410402 0.08387264 0.06471183 0.06353094 0.08184987 0.08858426
 0.06304328 0.06308193 0.0872507  0.08554218]
t_total = [29.79998322 29.79998322 29.79998322 29.79998322 29.79998322 29.79998322
 29.79998322 29.79998322 29.79998322 29.79998322]
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.44184640e-05 2.60315910e-04 2.33685526e-05 9.66842755e-07
 3.95080819e-04 4.50335349e-05 1.90864377e-06 6.95367030e-05
 3.14859996e-05 2.32591919e-05]
ene_total = [0.52628353 0.70725804 0.53116872 0.51967847 0.70173644 0.72818562
 0.51576709 0.52161422 0.71617089 0.70152458]
optimize_network iter = 2 obj = 6.169387607456903
eta = 0.6721669962857293
freqs = [5.45864375e+07 1.22896148e+08 5.38289512e+07 1.85932175e+07
 1.40894199e+08 6.88656172e+07 2.33118325e+07 7.72843192e+07
 6.10236134e+07 5.50512470e+07]
Done!
ene_coms = [0.0064104  0.00838726 0.00647118 0.00635309 0.00818499 0.00885843
 0.00630433 0.00630819 0.00872507 0.00855422]
ene_comp = [2.44035723e-05 2.60157155e-04 2.33543012e-05 9.66253122e-07
 3.94839877e-04 4.50060710e-05 1.90747978e-06 6.94942958e-05
 3.14667978e-05 2.32450072e-05]
ene_total = [0.00643481 0.00864742 0.00649454 0.00635406 0.00857983 0.00890343
 0.00630624 0.00637769 0.00875654 0.00857746]
At round 4 energy consumption: 0.07543200489545869
At round 4 eta: 0.6721669962857293
At round 4 a_n: 26.812419468377104
At round 4 local rounds: 13.007932941342311
At round 4 global rounds: 81.78682184099438
gradient difference: 0.42325329780578613
train() client id: f_00000-0-0 loss: 1.320891  [   32/  126]
train() client id: f_00000-0-1 loss: 1.321781  [   64/  126]
train() client id: f_00000-0-2 loss: 1.322237  [   96/  126]
train() client id: f_00000-1-0 loss: 1.171284  [   32/  126]
train() client id: f_00000-1-1 loss: 1.133872  [   64/  126]
train() client id: f_00000-1-2 loss: 1.124993  [   96/  126]
train() client id: f_00000-2-0 loss: 1.214204  [   32/  126]
train() client id: f_00000-2-1 loss: 1.080992  [   64/  126]
train() client id: f_00000-2-2 loss: 1.061503  [   96/  126]
train() client id: f_00000-3-0 loss: 1.068694  [   32/  126]
train() client id: f_00000-3-1 loss: 1.092723  [   64/  126]
train() client id: f_00000-3-2 loss: 1.036137  [   96/  126]
train() client id: f_00000-4-0 loss: 1.031603  [   32/  126]
train() client id: f_00000-4-1 loss: 0.960317  [   64/  126]
train() client id: f_00000-4-2 loss: 0.999131  [   96/  126]
train() client id: f_00000-5-0 loss: 0.988852  [   32/  126]
train() client id: f_00000-5-1 loss: 0.930766  [   64/  126]
train() client id: f_00000-5-2 loss: 0.938199  [   96/  126]
train() client id: f_00000-6-0 loss: 0.913499  [   32/  126]
train() client id: f_00000-6-1 loss: 0.961571  [   64/  126]
train() client id: f_00000-6-2 loss: 0.923139  [   96/  126]
train() client id: f_00000-7-0 loss: 0.904635  [   32/  126]
train() client id: f_00000-7-1 loss: 0.923489  [   64/  126]
train() client id: f_00000-7-2 loss: 1.032422  [   96/  126]
train() client id: f_00000-8-0 loss: 0.866567  [   32/  126]
train() client id: f_00000-8-1 loss: 0.983650  [   64/  126]
train() client id: f_00000-8-2 loss: 0.949347  [   96/  126]
train() client id: f_00000-9-0 loss: 0.919465  [   32/  126]
train() client id: f_00000-9-1 loss: 0.890797  [   64/  126]
train() client id: f_00000-9-2 loss: 0.921270  [   96/  126]
train() client id: f_00000-10-0 loss: 0.894889  [   32/  126]
train() client id: f_00000-10-1 loss: 0.976605  [   64/  126]
train() client id: f_00000-10-2 loss: 0.863472  [   96/  126]
train() client id: f_00000-11-0 loss: 0.992525  [   32/  126]
train() client id: f_00000-11-1 loss: 0.954809  [   64/  126]
train() client id: f_00000-11-2 loss: 0.784500  [   96/  126]
train() client id: f_00000-12-0 loss: 0.922424  [   32/  126]
train() client id: f_00000-12-1 loss: 0.871479  [   64/  126]
train() client id: f_00000-12-2 loss: 0.963325  [   96/  126]
train() client id: f_00001-0-0 loss: 0.765257  [   32/  265]
train() client id: f_00001-0-1 loss: 0.708135  [   64/  265]
train() client id: f_00001-0-2 loss: 0.700949  [   96/  265]
train() client id: f_00001-0-3 loss: 0.737086  [  128/  265]
train() client id: f_00001-0-4 loss: 0.651925  [  160/  265]
train() client id: f_00001-0-5 loss: 0.687013  [  192/  265]
train() client id: f_00001-0-6 loss: 0.679582  [  224/  265]
train() client id: f_00001-0-7 loss: 0.721956  [  256/  265]
train() client id: f_00001-1-0 loss: 0.776665  [   32/  265]
train() client id: f_00001-1-1 loss: 0.682613  [   64/  265]
train() client id: f_00001-1-2 loss: 0.682045  [   96/  265]
train() client id: f_00001-1-3 loss: 0.616572  [  128/  265]
train() client id: f_00001-1-4 loss: 0.638441  [  160/  265]
train() client id: f_00001-1-5 loss: 0.620959  [  192/  265]
train() client id: f_00001-1-6 loss: 0.628772  [  224/  265]
train() client id: f_00001-1-7 loss: 0.610566  [  256/  265]
train() client id: f_00001-2-0 loss: 0.586477  [   32/  265]
train() client id: f_00001-2-1 loss: 0.610095  [   64/  265]
train() client id: f_00001-2-2 loss: 0.586740  [   96/  265]
train() client id: f_00001-2-3 loss: 0.683686  [  128/  265]
train() client id: f_00001-2-4 loss: 0.586246  [  160/  265]
train() client id: f_00001-2-5 loss: 0.702464  [  192/  265]
train() client id: f_00001-2-6 loss: 0.665703  [  224/  265]
train() client id: f_00001-2-7 loss: 0.613106  [  256/  265]
train() client id: f_00001-3-0 loss: 0.623734  [   32/  265]
train() client id: f_00001-3-1 loss: 0.647248  [   64/  265]
train() client id: f_00001-3-2 loss: 0.570152  [   96/  265]
train() client id: f_00001-3-3 loss: 0.627056  [  128/  265]
train() client id: f_00001-3-4 loss: 0.636342  [  160/  265]
train() client id: f_00001-3-5 loss: 0.504051  [  192/  265]
train() client id: f_00001-3-6 loss: 0.618338  [  224/  265]
train() client id: f_00001-3-7 loss: 0.577048  [  256/  265]
train() client id: f_00001-4-0 loss: 0.653603  [   32/  265]
train() client id: f_00001-4-1 loss: 0.550088  [   64/  265]
train() client id: f_00001-4-2 loss: 0.493334  [   96/  265]
train() client id: f_00001-4-3 loss: 0.659737  [  128/  265]
train() client id: f_00001-4-4 loss: 0.674892  [  160/  265]
train() client id: f_00001-4-5 loss: 0.676102  [  192/  265]
train() client id: f_00001-4-6 loss: 0.512383  [  224/  265]
train() client id: f_00001-4-7 loss: 0.581526  [  256/  265]
train() client id: f_00001-5-0 loss: 0.643348  [   32/  265]
train() client id: f_00001-5-1 loss: 0.644717  [   64/  265]
train() client id: f_00001-5-2 loss: 0.609256  [   96/  265]
train() client id: f_00001-5-3 loss: 0.521779  [  128/  265]
train() client id: f_00001-5-4 loss: 0.552172  [  160/  265]
train() client id: f_00001-5-5 loss: 0.523322  [  192/  265]
train() client id: f_00001-5-6 loss: 0.614240  [  224/  265]
train() client id: f_00001-5-7 loss: 0.609005  [  256/  265]
train() client id: f_00001-6-0 loss: 0.506687  [   32/  265]
train() client id: f_00001-6-1 loss: 0.588402  [   64/  265]
train() client id: f_00001-6-2 loss: 0.586913  [   96/  265]
train() client id: f_00001-6-3 loss: 0.568679  [  128/  265]
train() client id: f_00001-6-4 loss: 0.523690  [  160/  265]
train() client id: f_00001-6-5 loss: 0.547920  [  192/  265]
train() client id: f_00001-6-6 loss: 0.589855  [  224/  265]
train() client id: f_00001-6-7 loss: 0.681378  [  256/  265]
train() client id: f_00001-7-0 loss: 0.565574  [   32/  265]
train() client id: f_00001-7-1 loss: 0.551166  [   64/  265]
train() client id: f_00001-7-2 loss: 0.575053  [   96/  265]
train() client id: f_00001-7-3 loss: 0.490267  [  128/  265]
train() client id: f_00001-7-4 loss: 0.627156  [  160/  265]
train() client id: f_00001-7-5 loss: 0.624318  [  192/  265]
train() client id: f_00001-7-6 loss: 0.590592  [  224/  265]
train() client id: f_00001-7-7 loss: 0.546207  [  256/  265]
train() client id: f_00001-8-0 loss: 0.499852  [   32/  265]
train() client id: f_00001-8-1 loss: 0.602375  [   64/  265]
train() client id: f_00001-8-2 loss: 0.480196  [   96/  265]
train() client id: f_00001-8-3 loss: 0.689538  [  128/  265]
train() client id: f_00001-8-4 loss: 0.620425  [  160/  265]
train() client id: f_00001-8-5 loss: 0.595972  [  192/  265]
train() client id: f_00001-8-6 loss: 0.557299  [  224/  265]
train() client id: f_00001-8-7 loss: 0.553449  [  256/  265]
train() client id: f_00001-9-0 loss: 0.606407  [   32/  265]
train() client id: f_00001-9-1 loss: 0.496578  [   64/  265]
train() client id: f_00001-9-2 loss: 0.547690  [   96/  265]
train() client id: f_00001-9-3 loss: 0.803917  [  128/  265]
train() client id: f_00001-9-4 loss: 0.477203  [  160/  265]
train() client id: f_00001-9-5 loss: 0.495404  [  192/  265]
train() client id: f_00001-9-6 loss: 0.598015  [  224/  265]
train() client id: f_00001-9-7 loss: 0.554739  [  256/  265]
train() client id: f_00001-10-0 loss: 0.586228  [   32/  265]
train() client id: f_00001-10-1 loss: 0.646201  [   64/  265]
train() client id: f_00001-10-2 loss: 0.589503  [   96/  265]
train() client id: f_00001-10-3 loss: 0.543312  [  128/  265]
train() client id: f_00001-10-4 loss: 0.591766  [  160/  265]
train() client id: f_00001-10-5 loss: 0.490584  [  192/  265]
train() client id: f_00001-10-6 loss: 0.575991  [  224/  265]
train() client id: f_00001-10-7 loss: 0.483438  [  256/  265]
train() client id: f_00001-11-0 loss: 0.643583  [   32/  265]
train() client id: f_00001-11-1 loss: 0.527376  [   64/  265]
train() client id: f_00001-11-2 loss: 0.582141  [   96/  265]
train() client id: f_00001-11-3 loss: 0.649139  [  128/  265]
train() client id: f_00001-11-4 loss: 0.648672  [  160/  265]
train() client id: f_00001-11-5 loss: 0.480145  [  192/  265]
train() client id: f_00001-11-6 loss: 0.495818  [  224/  265]
train() client id: f_00001-11-7 loss: 0.546849  [  256/  265]
train() client id: f_00001-12-0 loss: 0.595954  [   32/  265]
train() client id: f_00001-12-1 loss: 0.473161  [   64/  265]
train() client id: f_00001-12-2 loss: 0.513078  [   96/  265]
train() client id: f_00001-12-3 loss: 0.498984  [  128/  265]
train() client id: f_00001-12-4 loss: 0.754717  [  160/  265]
train() client id: f_00001-12-5 loss: 0.531957  [  192/  265]
train() client id: f_00001-12-6 loss: 0.483272  [  224/  265]
train() client id: f_00001-12-7 loss: 0.723368  [  256/  265]
train() client id: f_00002-0-0 loss: 1.319568  [   32/  124]
train() client id: f_00002-0-1 loss: 1.130778  [   64/  124]
train() client id: f_00002-0-2 loss: 1.251165  [   96/  124]
train() client id: f_00002-1-0 loss: 1.260836  [   32/  124]
train() client id: f_00002-1-1 loss: 1.227824  [   64/  124]
train() client id: f_00002-1-2 loss: 1.133098  [   96/  124]
train() client id: f_00002-2-0 loss: 1.238541  [   32/  124]
train() client id: f_00002-2-1 loss: 1.144354  [   64/  124]
train() client id: f_00002-2-2 loss: 1.114458  [   96/  124]
train() client id: f_00002-3-0 loss: 1.091473  [   32/  124]
train() client id: f_00002-3-1 loss: 1.146764  [   64/  124]
train() client id: f_00002-3-2 loss: 1.142942  [   96/  124]
train() client id: f_00002-4-0 loss: 1.119463  [   32/  124]
train() client id: f_00002-4-1 loss: 1.111962  [   64/  124]
train() client id: f_00002-4-2 loss: 1.029361  [   96/  124]
train() client id: f_00002-5-0 loss: 1.054670  [   32/  124]
train() client id: f_00002-5-1 loss: 1.109519  [   64/  124]
train() client id: f_00002-5-2 loss: 1.124842  [   96/  124]
train() client id: f_00002-6-0 loss: 1.132596  [   32/  124]
train() client id: f_00002-6-1 loss: 1.013289  [   64/  124]
train() client id: f_00002-6-2 loss: 1.073593  [   96/  124]
train() client id: f_00002-7-0 loss: 1.050079  [   32/  124]
train() client id: f_00002-7-1 loss: 0.967691  [   64/  124]
train() client id: f_00002-7-2 loss: 1.082201  [   96/  124]
train() client id: f_00002-8-0 loss: 1.070216  [   32/  124]
train() client id: f_00002-8-1 loss: 1.100506  [   64/  124]
train() client id: f_00002-8-2 loss: 0.952890  [   96/  124]
train() client id: f_00002-9-0 loss: 1.011648  [   32/  124]
train() client id: f_00002-9-1 loss: 1.101321  [   64/  124]
train() client id: f_00002-9-2 loss: 0.979591  [   96/  124]
train() client id: f_00002-10-0 loss: 0.978503  [   32/  124]
train() client id: f_00002-10-1 loss: 0.936618  [   64/  124]
train() client id: f_00002-10-2 loss: 1.019961  [   96/  124]
train() client id: f_00002-11-0 loss: 1.000454  [   32/  124]
train() client id: f_00002-11-1 loss: 0.924918  [   64/  124]
train() client id: f_00002-11-2 loss: 1.100698  [   96/  124]
train() client id: f_00002-12-0 loss: 1.014641  [   32/  124]
train() client id: f_00002-12-1 loss: 1.133085  [   64/  124]
train() client id: f_00002-12-2 loss: 0.921856  [   96/  124]
train() client id: f_00003-0-0 loss: 1.058002  [   32/   43]
train() client id: f_00003-1-0 loss: 1.089068  [   32/   43]
train() client id: f_00003-2-0 loss: 1.066965  [   32/   43]
train() client id: f_00003-3-0 loss: 1.047814  [   32/   43]
train() client id: f_00003-4-0 loss: 1.052004  [   32/   43]
train() client id: f_00003-5-0 loss: 1.021340  [   32/   43]
train() client id: f_00003-6-0 loss: 1.054136  [   32/   43]
train() client id: f_00003-7-0 loss: 1.054495  [   32/   43]
train() client id: f_00003-8-0 loss: 1.060772  [   32/   43]
train() client id: f_00003-9-0 loss: 1.053218  [   32/   43]
train() client id: f_00003-10-0 loss: 1.049495  [   32/   43]
train() client id: f_00003-11-0 loss: 1.038392  [   32/   43]
train() client id: f_00003-12-0 loss: 1.071204  [   32/   43]
train() client id: f_00004-0-0 loss: 0.849833  [   32/  306]
train() client id: f_00004-0-1 loss: 1.045561  [   64/  306]
train() client id: f_00004-0-2 loss: 0.765281  [   96/  306]
train() client id: f_00004-0-3 loss: 0.923681  [  128/  306]
train() client id: f_00004-0-4 loss: 0.900766  [  160/  306]
train() client id: f_00004-0-5 loss: 1.008216  [  192/  306]
train() client id: f_00004-0-6 loss: 0.923623  [  224/  306]
train() client id: f_00004-0-7 loss: 0.853156  [  256/  306]
train() client id: f_00004-0-8 loss: 0.870473  [  288/  306]
train() client id: f_00004-1-0 loss: 0.920670  [   32/  306]
train() client id: f_00004-1-1 loss: 0.919450  [   64/  306]
train() client id: f_00004-1-2 loss: 0.984134  [   96/  306]
train() client id: f_00004-1-3 loss: 0.931259  [  128/  306]
train() client id: f_00004-1-4 loss: 0.837592  [  160/  306]
train() client id: f_00004-1-5 loss: 0.917690  [  192/  306]
train() client id: f_00004-1-6 loss: 0.818943  [  224/  306]
train() client id: f_00004-1-7 loss: 0.927254  [  256/  306]
train() client id: f_00004-1-8 loss: 0.875246  [  288/  306]
train() client id: f_00004-2-0 loss: 0.942658  [   32/  306]
train() client id: f_00004-2-1 loss: 0.890684  [   64/  306]
train() client id: f_00004-2-2 loss: 0.861005  [   96/  306]
train() client id: f_00004-2-3 loss: 0.929248  [  128/  306]
train() client id: f_00004-2-4 loss: 0.875686  [  160/  306]
train() client id: f_00004-2-5 loss: 0.963678  [  192/  306]
train() client id: f_00004-2-6 loss: 0.873094  [  224/  306]
train() client id: f_00004-2-7 loss: 0.845473  [  256/  306]
train() client id: f_00004-2-8 loss: 0.938136  [  288/  306]
train() client id: f_00004-3-0 loss: 0.879145  [   32/  306]
train() client id: f_00004-3-1 loss: 0.963648  [   64/  306]
train() client id: f_00004-3-2 loss: 0.899402  [   96/  306]
train() client id: f_00004-3-3 loss: 0.946495  [  128/  306]
train() client id: f_00004-3-4 loss: 0.867959  [  160/  306]
train() client id: f_00004-3-5 loss: 0.954295  [  192/  306]
train() client id: f_00004-3-6 loss: 0.803374  [  224/  306]
train() client id: f_00004-3-7 loss: 0.938383  [  256/  306]
train() client id: f_00004-3-8 loss: 0.917908  [  288/  306]
train() client id: f_00004-4-0 loss: 0.844538  [   32/  306]
train() client id: f_00004-4-1 loss: 0.849351  [   64/  306]
train() client id: f_00004-4-2 loss: 0.881758  [   96/  306]
train() client id: f_00004-4-3 loss: 0.876928  [  128/  306]
train() client id: f_00004-4-4 loss: 0.973635  [  160/  306]
train() client id: f_00004-4-5 loss: 0.876522  [  192/  306]
train() client id: f_00004-4-6 loss: 0.901481  [  224/  306]
train() client id: f_00004-4-7 loss: 1.007973  [  256/  306]
train() client id: f_00004-4-8 loss: 0.924847  [  288/  306]
train() client id: f_00004-5-0 loss: 0.918660  [   32/  306]
train() client id: f_00004-5-1 loss: 0.946695  [   64/  306]
train() client id: f_00004-5-2 loss: 0.934046  [   96/  306]
train() client id: f_00004-5-3 loss: 0.931357  [  128/  306]
train() client id: f_00004-5-4 loss: 0.963673  [  160/  306]
train() client id: f_00004-5-5 loss: 0.968353  [  192/  306]
train() client id: f_00004-5-6 loss: 0.818526  [  224/  306]
train() client id: f_00004-5-7 loss: 0.834395  [  256/  306]
train() client id: f_00004-5-8 loss: 0.860287  [  288/  306]
train() client id: f_00004-6-0 loss: 0.828477  [   32/  306]
train() client id: f_00004-6-1 loss: 0.916048  [   64/  306]
train() client id: f_00004-6-2 loss: 0.850622  [   96/  306]
train() client id: f_00004-6-3 loss: 0.885542  [  128/  306]
train() client id: f_00004-6-4 loss: 0.900050  [  160/  306]
train() client id: f_00004-6-5 loss: 0.882944  [  192/  306]
train() client id: f_00004-6-6 loss: 0.883763  [  224/  306]
train() client id: f_00004-6-7 loss: 0.991659  [  256/  306]
train() client id: f_00004-6-8 loss: 0.904851  [  288/  306]
train() client id: f_00004-7-0 loss: 0.823246  [   32/  306]
train() client id: f_00004-7-1 loss: 0.967626  [   64/  306]
train() client id: f_00004-7-2 loss: 0.929333  [   96/  306]
train() client id: f_00004-7-3 loss: 0.948862  [  128/  306]
train() client id: f_00004-7-4 loss: 0.796230  [  160/  306]
train() client id: f_00004-7-5 loss: 0.926267  [  192/  306]
train() client id: f_00004-7-6 loss: 0.910266  [  224/  306]
train() client id: f_00004-7-7 loss: 0.928060  [  256/  306]
train() client id: f_00004-7-8 loss: 0.887732  [  288/  306]
train() client id: f_00004-8-0 loss: 0.854510  [   32/  306]
train() client id: f_00004-8-1 loss: 0.965763  [   64/  306]
train() client id: f_00004-8-2 loss: 0.867668  [   96/  306]
train() client id: f_00004-8-3 loss: 0.921891  [  128/  306]
train() client id: f_00004-8-4 loss: 0.837506  [  160/  306]
train() client id: f_00004-8-5 loss: 0.920114  [  192/  306]
train() client id: f_00004-8-6 loss: 0.983534  [  224/  306]
train() client id: f_00004-8-7 loss: 0.939306  [  256/  306]
train() client id: f_00004-8-8 loss: 0.850981  [  288/  306]
train() client id: f_00004-9-0 loss: 0.967507  [   32/  306]
train() client id: f_00004-9-1 loss: 0.807495  [   64/  306]
train() client id: f_00004-9-2 loss: 0.938346  [   96/  306]
train() client id: f_00004-9-3 loss: 0.875442  [  128/  306]
train() client id: f_00004-9-4 loss: 0.964709  [  160/  306]
train() client id: f_00004-9-5 loss: 1.006636  [  192/  306]
train() client id: f_00004-9-6 loss: 0.855247  [  224/  306]
train() client id: f_00004-9-7 loss: 0.814534  [  256/  306]
train() client id: f_00004-9-8 loss: 0.881298  [  288/  306]
train() client id: f_00004-10-0 loss: 0.827573  [   32/  306]
train() client id: f_00004-10-1 loss: 0.939101  [   64/  306]
train() client id: f_00004-10-2 loss: 0.890407  [   96/  306]
train() client id: f_00004-10-3 loss: 0.909444  [  128/  306]
train() client id: f_00004-10-4 loss: 0.950862  [  160/  306]
train() client id: f_00004-10-5 loss: 0.803805  [  192/  306]
train() client id: f_00004-10-6 loss: 0.932526  [  224/  306]
train() client id: f_00004-10-7 loss: 0.847127  [  256/  306]
train() client id: f_00004-10-8 loss: 0.992269  [  288/  306]
train() client id: f_00004-11-0 loss: 0.906074  [   32/  306]
train() client id: f_00004-11-1 loss: 0.806332  [   64/  306]
train() client id: f_00004-11-2 loss: 0.905419  [   96/  306]
train() client id: f_00004-11-3 loss: 0.786394  [  128/  306]
train() client id: f_00004-11-4 loss: 1.028199  [  160/  306]
train() client id: f_00004-11-5 loss: 1.010867  [  192/  306]
train() client id: f_00004-11-6 loss: 0.859499  [  224/  306]
train() client id: f_00004-11-7 loss: 0.905416  [  256/  306]
train() client id: f_00004-11-8 loss: 0.888938  [  288/  306]
train() client id: f_00004-12-0 loss: 0.865880  [   32/  306]
train() client id: f_00004-12-1 loss: 0.856017  [   64/  306]
train() client id: f_00004-12-2 loss: 0.907013  [   96/  306]
train() client id: f_00004-12-3 loss: 0.879868  [  128/  306]
train() client id: f_00004-12-4 loss: 0.924729  [  160/  306]
train() client id: f_00004-12-5 loss: 0.970172  [  192/  306]
train() client id: f_00004-12-6 loss: 0.968852  [  224/  306]
train() client id: f_00004-12-7 loss: 0.922229  [  256/  306]
train() client id: f_00004-12-8 loss: 0.824604  [  288/  306]
train() client id: f_00005-0-0 loss: 0.895768  [   32/  146]
train() client id: f_00005-0-1 loss: 0.928599  [   64/  146]
train() client id: f_00005-0-2 loss: 1.064048  [   96/  146]
train() client id: f_00005-0-3 loss: 0.849841  [  128/  146]
train() client id: f_00005-1-0 loss: 0.871840  [   32/  146]
train() client id: f_00005-1-1 loss: 0.870079  [   64/  146]
train() client id: f_00005-1-2 loss: 0.909100  [   96/  146]
train() client id: f_00005-1-3 loss: 0.960735  [  128/  146]
train() client id: f_00005-2-0 loss: 0.918276  [   32/  146]
train() client id: f_00005-2-1 loss: 0.828855  [   64/  146]
train() client id: f_00005-2-2 loss: 0.876072  [   96/  146]
train() client id: f_00005-2-3 loss: 0.950208  [  128/  146]
train() client id: f_00005-3-0 loss: 0.962548  [   32/  146]
train() client id: f_00005-3-1 loss: 0.864209  [   64/  146]
train() client id: f_00005-3-2 loss: 0.923954  [   96/  146]
train() client id: f_00005-3-3 loss: 0.834681  [  128/  146]
train() client id: f_00005-4-0 loss: 0.766787  [   32/  146]
train() client id: f_00005-4-1 loss: 0.776479  [   64/  146]
train() client id: f_00005-4-2 loss: 0.917072  [   96/  146]
train() client id: f_00005-4-3 loss: 0.866762  [  128/  146]
train() client id: f_00005-5-0 loss: 0.797392  [   32/  146]
train() client id: f_00005-5-1 loss: 0.840019  [   64/  146]
train() client id: f_00005-5-2 loss: 0.846818  [   96/  146]
train() client id: f_00005-5-3 loss: 0.824692  [  128/  146]
train() client id: f_00005-6-0 loss: 0.873143  [   32/  146]
train() client id: f_00005-6-1 loss: 0.902009  [   64/  146]
train() client id: f_00005-6-2 loss: 0.926206  [   96/  146]
train() client id: f_00005-6-3 loss: 0.712758  [  128/  146]
train() client id: f_00005-7-0 loss: 0.917951  [   32/  146]
train() client id: f_00005-7-1 loss: 0.831099  [   64/  146]
train() client id: f_00005-7-2 loss: 0.803993  [   96/  146]
train() client id: f_00005-7-3 loss: 0.854127  [  128/  146]
train() client id: f_00005-8-0 loss: 0.711676  [   32/  146]
train() client id: f_00005-8-1 loss: 1.030993  [   64/  146]
train() client id: f_00005-8-2 loss: 0.846667  [   96/  146]
train() client id: f_00005-8-3 loss: 0.893956  [  128/  146]
train() client id: f_00005-9-0 loss: 0.795186  [   32/  146]
train() client id: f_00005-9-1 loss: 0.997928  [   64/  146]
train() client id: f_00005-9-2 loss: 0.780730  [   96/  146]
train() client id: f_00005-9-3 loss: 0.796119  [  128/  146]
train() client id: f_00005-10-0 loss: 0.831147  [   32/  146]
train() client id: f_00005-10-1 loss: 0.871497  [   64/  146]
train() client id: f_00005-10-2 loss: 0.766708  [   96/  146]
train() client id: f_00005-10-3 loss: 0.813721  [  128/  146]
train() client id: f_00005-11-0 loss: 0.731042  [   32/  146]
train() client id: f_00005-11-1 loss: 0.726730  [   64/  146]
train() client id: f_00005-11-2 loss: 0.814422  [   96/  146]
train() client id: f_00005-11-3 loss: 0.947395  [  128/  146]
train() client id: f_00005-12-0 loss: 0.892404  [   32/  146]
train() client id: f_00005-12-1 loss: 0.773214  [   64/  146]
train() client id: f_00005-12-2 loss: 0.781872  [   96/  146]
train() client id: f_00005-12-3 loss: 0.885056  [  128/  146]
train() client id: f_00006-0-0 loss: 0.994315  [   32/   54]
train() client id: f_00006-1-0 loss: 1.028711  [   32/   54]
train() client id: f_00006-2-0 loss: 1.005249  [   32/   54]
train() client id: f_00006-3-0 loss: 1.007245  [   32/   54]
train() client id: f_00006-4-0 loss: 0.998916  [   32/   54]
train() client id: f_00006-5-0 loss: 1.023185  [   32/   54]
train() client id: f_00006-6-0 loss: 1.000435  [   32/   54]
train() client id: f_00006-7-0 loss: 0.999984  [   32/   54]
train() client id: f_00006-8-0 loss: 1.016773  [   32/   54]
train() client id: f_00006-9-0 loss: 1.024092  [   32/   54]
train() client id: f_00006-10-0 loss: 0.986179  [   32/   54]
train() client id: f_00006-11-0 loss: 1.009950  [   32/   54]
train() client id: f_00006-12-0 loss: 1.032114  [   32/   54]
train() client id: f_00007-0-0 loss: 0.952570  [   32/  179]
train() client id: f_00007-0-1 loss: 0.952570  [   64/  179]
train() client id: f_00007-0-2 loss: 0.917096  [   96/  179]
train() client id: f_00007-0-3 loss: 0.921217  [  128/  179]
train() client id: f_00007-0-4 loss: 0.961537  [  160/  179]
train() client id: f_00007-1-0 loss: 0.893697  [   32/  179]
train() client id: f_00007-1-1 loss: 0.866895  [   64/  179]
train() client id: f_00007-1-2 loss: 0.901603  [   96/  179]
train() client id: f_00007-1-3 loss: 0.911773  [  128/  179]
train() client id: f_00007-1-4 loss: 0.923246  [  160/  179]
train() client id: f_00007-2-0 loss: 0.806776  [   32/  179]
train() client id: f_00007-2-1 loss: 0.885362  [   64/  179]
train() client id: f_00007-2-2 loss: 0.944073  [   96/  179]
train() client id: f_00007-2-3 loss: 0.867805  [  128/  179]
train() client id: f_00007-2-4 loss: 0.797235  [  160/  179]
train() client id: f_00007-3-0 loss: 0.823055  [   32/  179]
train() client id: f_00007-3-1 loss: 0.791422  [   64/  179]
train() client id: f_00007-3-2 loss: 0.840813  [   96/  179]
train() client id: f_00007-3-3 loss: 0.832411  [  128/  179]
train() client id: f_00007-3-4 loss: 0.919735  [  160/  179]
train() client id: f_00007-4-0 loss: 0.822627  [   32/  179]
train() client id: f_00007-4-1 loss: 0.802257  [   64/  179]
train() client id: f_00007-4-2 loss: 0.886204  [   96/  179]
train() client id: f_00007-4-3 loss: 0.782131  [  128/  179]
train() client id: f_00007-4-4 loss: 0.815230  [  160/  179]
train() client id: f_00007-5-0 loss: 0.784144  [   32/  179]
train() client id: f_00007-5-1 loss: 0.817131  [   64/  179]
train() client id: f_00007-5-2 loss: 0.727481  [   96/  179]
train() client id: f_00007-5-3 loss: 0.859219  [  128/  179]
train() client id: f_00007-5-4 loss: 0.825472  [  160/  179]
train() client id: f_00007-6-0 loss: 0.796782  [   32/  179]
train() client id: f_00007-6-1 loss: 0.825594  [   64/  179]
train() client id: f_00007-6-2 loss: 0.776773  [   96/  179]
train() client id: f_00007-6-3 loss: 0.784095  [  128/  179]
train() client id: f_00007-6-4 loss: 0.825790  [  160/  179]
train() client id: f_00007-7-0 loss: 0.801436  [   32/  179]
train() client id: f_00007-7-1 loss: 0.770460  [   64/  179]
train() client id: f_00007-7-2 loss: 0.826588  [   96/  179]
train() client id: f_00007-7-3 loss: 0.734864  [  128/  179]
train() client id: f_00007-7-4 loss: 0.920933  [  160/  179]
train() client id: f_00007-8-0 loss: 0.856693  [   32/  179]
train() client id: f_00007-8-1 loss: 0.754684  [   64/  179]
train() client id: f_00007-8-2 loss: 0.774264  [   96/  179]
train() client id: f_00007-8-3 loss: 0.868841  [  128/  179]
train() client id: f_00007-8-4 loss: 0.768269  [  160/  179]
train() client id: f_00007-9-0 loss: 0.754154  [   32/  179]
train() client id: f_00007-9-1 loss: 0.873895  [   64/  179]
train() client id: f_00007-9-2 loss: 0.853664  [   96/  179]
train() client id: f_00007-9-3 loss: 0.758614  [  128/  179]
train() client id: f_00007-9-4 loss: 0.770330  [  160/  179]
train() client id: f_00007-10-0 loss: 0.772283  [   32/  179]
train() client id: f_00007-10-1 loss: 0.826016  [   64/  179]
train() client id: f_00007-10-2 loss: 0.826313  [   96/  179]
train() client id: f_00007-10-3 loss: 0.695713  [  128/  179]
train() client id: f_00007-10-4 loss: 0.830201  [  160/  179]
train() client id: f_00007-11-0 loss: 0.911640  [   32/  179]
train() client id: f_00007-11-1 loss: 0.769347  [   64/  179]
train() client id: f_00007-11-2 loss: 0.744615  [   96/  179]
train() client id: f_00007-11-3 loss: 0.829591  [  128/  179]
train() client id: f_00007-11-4 loss: 0.773459  [  160/  179]
train() client id: f_00007-12-0 loss: 0.808878  [   32/  179]
train() client id: f_00007-12-1 loss: 0.897836  [   64/  179]
train() client id: f_00007-12-2 loss: 0.760309  [   96/  179]
train() client id: f_00007-12-3 loss: 0.783542  [  128/  179]
train() client id: f_00007-12-4 loss: 0.763190  [  160/  179]
train() client id: f_00008-0-0 loss: 0.911467  [   32/  130]
train() client id: f_00008-0-1 loss: 0.977115  [   64/  130]
train() client id: f_00008-0-2 loss: 0.973726  [   96/  130]
train() client id: f_00008-0-3 loss: 0.843502  [  128/  130]
train() client id: f_00008-1-0 loss: 0.977567  [   32/  130]
train() client id: f_00008-1-1 loss: 0.922608  [   64/  130]
train() client id: f_00008-1-2 loss: 0.906049  [   96/  130]
train() client id: f_00008-1-3 loss: 0.881570  [  128/  130]
train() client id: f_00008-2-0 loss: 0.954932  [   32/  130]
train() client id: f_00008-2-1 loss: 0.947699  [   64/  130]
train() client id: f_00008-2-2 loss: 0.838052  [   96/  130]
train() client id: f_00008-2-3 loss: 0.942032  [  128/  130]
train() client id: f_00008-3-0 loss: 0.934906  [   32/  130]
train() client id: f_00008-3-1 loss: 0.887486  [   64/  130]
train() client id: f_00008-3-2 loss: 0.858470  [   96/  130]
train() client id: f_00008-3-3 loss: 0.992394  [  128/  130]
train() client id: f_00008-4-0 loss: 0.868254  [   32/  130]
train() client id: f_00008-4-1 loss: 0.975527  [   64/  130]
train() client id: f_00008-4-2 loss: 0.898381  [   96/  130]
train() client id: f_00008-4-3 loss: 0.915893  [  128/  130]
train() client id: f_00008-5-0 loss: 0.889084  [   32/  130]
train() client id: f_00008-5-1 loss: 0.945243  [   64/  130]
train() client id: f_00008-5-2 loss: 0.962920  [   96/  130]
train() client id: f_00008-5-3 loss: 0.823098  [  128/  130]
train() client id: f_00008-6-0 loss: 0.925428  [   32/  130]
train() client id: f_00008-6-1 loss: 0.916439  [   64/  130]
train() client id: f_00008-6-2 loss: 0.874116  [   96/  130]
train() client id: f_00008-6-3 loss: 0.887380  [  128/  130]
train() client id: f_00008-7-0 loss: 0.863077  [   32/  130]
train() client id: f_00008-7-1 loss: 0.805037  [   64/  130]
train() client id: f_00008-7-2 loss: 0.963809  [   96/  130]
train() client id: f_00008-7-3 loss: 0.987046  [  128/  130]
train() client id: f_00008-8-0 loss: 1.029199  [   32/  130]
train() client id: f_00008-8-1 loss: 0.774377  [   64/  130]
train() client id: f_00008-8-2 loss: 0.891512  [   96/  130]
train() client id: f_00008-8-3 loss: 0.941451  [  128/  130]
train() client id: f_00008-9-0 loss: 0.913537  [   32/  130]
train() client id: f_00008-9-1 loss: 0.898758  [   64/  130]
train() client id: f_00008-9-2 loss: 0.909386  [   96/  130]
train() client id: f_00008-9-3 loss: 0.920733  [  128/  130]
train() client id: f_00008-10-0 loss: 0.873981  [   32/  130]
train() client id: f_00008-10-1 loss: 0.923509  [   64/  130]
train() client id: f_00008-10-2 loss: 0.833708  [   96/  130]
train() client id: f_00008-10-3 loss: 1.014873  [  128/  130]
train() client id: f_00008-11-0 loss: 0.938893  [   32/  130]
train() client id: f_00008-11-1 loss: 0.853145  [   64/  130]
train() client id: f_00008-11-2 loss: 0.904563  [   96/  130]
train() client id: f_00008-11-3 loss: 0.905847  [  128/  130]
train() client id: f_00008-12-0 loss: 1.012352  [   32/  130]
train() client id: f_00008-12-1 loss: 0.835925  [   64/  130]
train() client id: f_00008-12-2 loss: 0.890242  [   96/  130]
train() client id: f_00008-12-3 loss: 0.905480  [  128/  130]
train() client id: f_00009-0-0 loss: 1.194711  [   32/  118]
train() client id: f_00009-0-1 loss: 1.164020  [   64/  118]
train() client id: f_00009-0-2 loss: 1.054761  [   96/  118]
train() client id: f_00009-1-0 loss: 1.216193  [   32/  118]
train() client id: f_00009-1-1 loss: 1.069408  [   64/  118]
train() client id: f_00009-1-2 loss: 1.110207  [   96/  118]
train() client id: f_00009-2-0 loss: 1.077340  [   32/  118]
train() client id: f_00009-2-1 loss: 1.031443  [   64/  118]
train() client id: f_00009-2-2 loss: 1.068469  [   96/  118]
train() client id: f_00009-3-0 loss: 1.011418  [   32/  118]
train() client id: f_00009-3-1 loss: 1.096629  [   64/  118]
train() client id: f_00009-3-2 loss: 1.021678  [   96/  118]
train() client id: f_00009-4-0 loss: 1.053457  [   32/  118]
train() client id: f_00009-4-1 loss: 1.006850  [   64/  118]
train() client id: f_00009-4-2 loss: 1.005712  [   96/  118]
train() client id: f_00009-5-0 loss: 0.940762  [   32/  118]
train() client id: f_00009-5-1 loss: 0.998266  [   64/  118]
train() client id: f_00009-5-2 loss: 1.023073  [   96/  118]
train() client id: f_00009-6-0 loss: 0.947084  [   32/  118]
train() client id: f_00009-6-1 loss: 0.928754  [   64/  118]
train() client id: f_00009-6-2 loss: 1.013407  [   96/  118]
train() client id: f_00009-7-0 loss: 0.924675  [   32/  118]
train() client id: f_00009-7-1 loss: 0.989164  [   64/  118]
train() client id: f_00009-7-2 loss: 0.937257  [   96/  118]
train() client id: f_00009-8-0 loss: 0.879741  [   32/  118]
train() client id: f_00009-8-1 loss: 0.913153  [   64/  118]
train() client id: f_00009-8-2 loss: 1.043403  [   96/  118]
train() client id: f_00009-9-0 loss: 0.899985  [   32/  118]
train() client id: f_00009-9-1 loss: 0.877325  [   64/  118]
train() client id: f_00009-9-2 loss: 0.982945  [   96/  118]
train() client id: f_00009-10-0 loss: 0.812434  [   32/  118]
train() client id: f_00009-10-1 loss: 0.965531  [   64/  118]
train() client id: f_00009-10-2 loss: 0.858387  [   96/  118]
train() client id: f_00009-11-0 loss: 0.895939  [   32/  118]
train() client id: f_00009-11-1 loss: 0.782661  [   64/  118]
train() client id: f_00009-11-2 loss: 1.026496  [   96/  118]
train() client id: f_00009-12-0 loss: 0.849687  [   32/  118]
train() client id: f_00009-12-1 loss: 0.928795  [   64/  118]
train() client id: f_00009-12-2 loss: 1.013245  [   96/  118]
At round 4 accuracy: 0.6286472148541115
At round 4 training accuracy: 0.5774647887323944
At round 4 training loss: 0.938765901455557
update_location
xs = -3.905658 4.200318 40.009024 18.811294 -14.020704 -1.043590 -2.443192 -6.324852 24.663977 2.939121 
ys = 32.587959 15.555839 1.320614 -2.455176 9.350187 -17.185849 -2.624984 -4.177652 17.569006 4.001482 
dists_uav = 105.248417 101.289816 107.714744 101.783558 101.410089 101.471388 100.064278 100.286871 104.484361 100.123176 
dists_bs = 222.608317 239.864951 276.393577 262.779674 230.985269 259.222362 247.641852 246.081159 254.264078 246.784976 
uav_gains = -100.555422 -100.139164 -100.806925 -100.191961 -100.152049 -100.158610 -100.006993 -100.031119 -100.476312 -100.013382 
bs_gains = -105.298218 -106.206129 -107.929842 -107.315629 -105.747419 -107.149888 -106.594132 -106.517253 -106.915039 -106.551983 
Round 5
-------------------------------
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.33284149 21.59560189 10.17342765  3.63728595 24.89615745 12.01211791
  4.52299688 14.602885   10.71870557  9.74625265]
obj_prev = 122.23827245410807
eta_min = 9.414027209246992e-10	eta_max = 0.9186608067299383
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 28.431400687067253	eta = 0.909090909090909
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 48.95841824133416	eta = 0.5279322499743717
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 39.18478015448109	eta = 0.6596114051281231
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 37.434409794184106	eta = 0.6904537306569067
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 37.34853700150222	eta = 0.6920412410342677
af = 25.846727897333864	bf = 1.9873090114183967	zeta = 37.348315836191034	eta = 0.6920453390909805
eta = 0.6920453390909805
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [0.03037504 0.06388402 0.0298929  0.01036609 0.07376796 0.03519648
 0.01301788 0.04315185 0.03133933 0.02844647]
ene_total = [3.16534696 6.21953031 3.1298897  1.43707177 7.05111483 3.79361781
 1.66080998 4.25148272 3.45176733 3.18768443]
ti_comp = [0.28160493 0.26136622 0.28090865 0.2825856  0.2634578  0.25672527
 0.28307344 0.28301023 0.25792521 0.25972041]
ti_coms = [0.06451099 0.0847497  0.06520727 0.06353032 0.08265812 0.08939065
 0.06304248 0.06310569 0.08819071 0.08639551]
t_total = [29.74997902 29.74997902 29.74997902 29.74997902 29.74997902 29.74997902
 29.74997902 29.74997902 29.74997902 29.74997902]
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.20876875e-05 2.38538142e-04 2.11569937e-05 8.71815183e-07
 3.61460861e-04 4.13466617e-05 1.72068806e-06 6.27009232e-05
 2.89175573e-05 2.13281397e-05]
ene_total = [0.55639501 0.74895917 0.56229982 0.54614218 0.74154691 0.77190053
 0.54202196 0.54780676 0.76051829 0.74443548]
optimize_network iter = 0 obj = 6.522026104246858
eta = 0.6920453390909805
freqs = [5.39320171e+07 1.22211702e+08 5.32075115e+07 1.83414978e+07
 1.39999582e+08 6.85489188e+07 2.29938135e+07 7.62372573e+07
 6.07527495e+07 5.47636387e+07]
eta_min = 0.6764484296472902	eta_max = 0.6920453390909633
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 0.06328991478194519	eta = 0.9090909090909091
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 21.92072085717903	eta = 0.0026247442563715508
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 2.335363763739976	eta = 0.024636969648472683
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 2.25597545648311	eta = 0.025503950408706712
af = 0.05753628616540472	bf = 1.9873090114183967	zeta = 2.255940548856704	eta = 0.025504345047817744
eta = 0.025504345047817744
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.20148930e-04 2.37751991e-03 2.10872665e-04 8.68941936e-06
 3.60269593e-03 4.12103953e-04 1.71501718e-05 6.24942794e-04
 2.88222535e-04 2.12578485e-04]
ene_total = [0.1812087  0.29478227 0.18284802 0.17280114 0.32238    0.25400243
 0.17170585 0.1883868  0.24737814 0.2404472 ]
ti_comp = [0.29913458 0.27889587 0.2984383  0.30011526 0.28098745 0.27425492
 0.3006031  0.30053989 0.27545486 0.27725007]
ti_coms = [0.06451099 0.0847497  0.06520727 0.06353032 0.08265812 0.08939065
 0.06304248 0.06310569 0.08819071 0.08639551]
t_total = [29.74997902 29.74997902 29.74997902 29.74997902 29.74997902 29.74997902
 29.74997902 29.74997902 29.74997902 29.74997902]
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.34412342e-05 2.50873933e-04 2.24469855e-05 9.25616981e-07
 3.80533165e-04 4.33862178e-05 1.82724368e-06 6.65820168e-05
 3.03620624e-05 2.24132480e-05]
ene_total = [0.52968454 0.71386454 0.53529951 0.51981962 0.70736071 0.73485767
 0.51590233 0.52171707 0.72397543 0.70863849]
optimize_network iter = 1 obj = 6.211119913967213
eta = 0.6764484296472902
freqs = [5.39156214e+07 1.21622656e+08 5.31836109e+07 1.83396515e+07
 1.39394357e+08 6.81410966e+07 2.29938135e+07 7.62362643e+07
 6.04092728e+07 5.44779861e+07]
eta_min = 0.6764484296473243	eta_max = 0.6764484296472864
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 0.06277694578067981	eta = 0.909090909090909
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 21.92023194550156	eta = 0.002603528596394288
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 2.332999905489733	eta = 0.024462045872963308
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 2.2542173952325193	eta = 0.02531696846560011
af = 0.05706995070970891	bf = 1.9873090114183967	zeta = 2.2541832396466193	eta = 0.02531735207056884
eta = 0.02531735207056884
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.20456386e-04 2.35937920e-03 2.11105834e-04 8.70509514e-06
 3.57877769e-03 4.08031791e-04 1.71845703e-05 6.26179946e-04
 2.85544288e-04 2.10788545e-04]
ene_total = [0.18118229 0.29423306 0.18281927 0.17276841 0.32166859 0.25384311
 0.17167384 0.18838425 0.24725795 0.24035246]
ti_comp = [0.29913458 0.27889587 0.2984383  0.30011526 0.28098745 0.27425492
 0.3006031  0.30053989 0.27545486 0.27725007]
ti_coms = [0.06451099 0.0847497  0.06520727 0.06353032 0.08265812 0.08939065
 0.06304248 0.06310569 0.08819071 0.08639551]
t_total = [29.74997902 29.74997902 29.74997902 29.74997902 29.74997902 29.74997902
 29.74997902 29.74997902 29.74997902 29.74997902]
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.34412342e-05 2.50873933e-04 2.24469855e-05 9.25616981e-07
 3.80533165e-04 4.33862178e-05 1.82724368e-06 6.65820168e-05
 3.03620624e-05 2.24132480e-05]
ene_total = [0.52968454 0.71386454 0.53529951 0.51981962 0.70736071 0.73485767
 0.51590233 0.52171707 0.72397543 0.70863849]
optimize_network iter = 2 obj = 6.2111199139678615
eta = 0.6764484296473243
freqs = [5.39156214e+07 1.21622656e+08 5.31836109e+07 1.83396515e+07
 1.39394357e+08 6.81410966e+07 2.29938135e+07 7.62362643e+07
 6.04092728e+07 5.44779861e+07]
Done!
ene_coms = [0.0064511  0.00847497 0.00652073 0.00635303 0.00826581 0.00893907
 0.00630425 0.00631057 0.00881907 0.00863955]
ene_comp = [2.19761204e-05 2.35193919e-04 2.10440137e-05 8.67764468e-07
 3.56749245e-04 4.06745112e-05 1.71303809e-06 6.24205363e-05
 2.84643859e-05 2.10123849e-05]
ene_total = [0.00647308 0.00871016 0.00654177 0.0063539  0.00862256 0.00897974
 0.00630596 0.00637299 0.00884754 0.00866056]
At round 5 energy consumption: 0.07586826097698268
At round 5 eta: 0.6764484296473243
At round 5 a_n: 26.46987362140779
At round 5 local rounds: 12.80002140651856
At round 5 global rounds: 81.81036980458869
gradient difference: 0.3938519358634949
train() client id: f_00000-0-0 loss: 1.577524  [   32/  126]
train() client id: f_00000-0-1 loss: 1.360910  [   64/  126]
train() client id: f_00000-0-2 loss: 1.311937  [   96/  126]
train() client id: f_00000-1-0 loss: 1.227304  [   32/  126]
train() client id: f_00000-1-1 loss: 1.335350  [   64/  126]
train() client id: f_00000-1-2 loss: 1.129238  [   96/  126]
train() client id: f_00000-2-0 loss: 1.176344  [   32/  126]
train() client id: f_00000-2-1 loss: 1.158575  [   64/  126]
train() client id: f_00000-2-2 loss: 1.082238  [   96/  126]
train() client id: f_00000-3-0 loss: 1.056985  [   32/  126]
train() client id: f_00000-3-1 loss: 1.020758  [   64/  126]
train() client id: f_00000-3-2 loss: 1.078068  [   96/  126]
train() client id: f_00000-4-0 loss: 1.049539  [   32/  126]
train() client id: f_00000-4-1 loss: 0.975156  [   64/  126]
train() client id: f_00000-4-2 loss: 0.958732  [   96/  126]
train() client id: f_00000-5-0 loss: 0.954056  [   32/  126]
train() client id: f_00000-5-1 loss: 0.981056  [   64/  126]
train() client id: f_00000-5-2 loss: 0.925636  [   96/  126]
train() client id: f_00000-6-0 loss: 0.901896  [   32/  126]
train() client id: f_00000-6-1 loss: 0.897219  [   64/  126]
train() client id: f_00000-6-2 loss: 0.955733  [   96/  126]
train() client id: f_00000-7-0 loss: 0.930531  [   32/  126]
train() client id: f_00000-7-1 loss: 0.855253  [   64/  126]
train() client id: f_00000-7-2 loss: 0.878424  [   96/  126]
train() client id: f_00000-8-0 loss: 0.942323  [   32/  126]
train() client id: f_00000-8-1 loss: 0.885004  [   64/  126]
train() client id: f_00000-8-2 loss: 0.831992  [   96/  126]
train() client id: f_00000-9-0 loss: 0.836709  [   32/  126]
train() client id: f_00000-9-1 loss: 0.804223  [   64/  126]
train() client id: f_00000-9-2 loss: 0.919087  [   96/  126]
train() client id: f_00000-10-0 loss: 0.774902  [   32/  126]
train() client id: f_00000-10-1 loss: 0.959610  [   64/  126]
train() client id: f_00000-10-2 loss: 0.850979  [   96/  126]
train() client id: f_00000-11-0 loss: 0.794460  [   32/  126]
train() client id: f_00000-11-1 loss: 0.864264  [   64/  126]
train() client id: f_00000-11-2 loss: 0.780530  [   96/  126]
train() client id: f_00001-0-0 loss: 0.503228  [   32/  265]
train() client id: f_00001-0-1 loss: 0.531227  [   64/  265]
train() client id: f_00001-0-2 loss: 0.547863  [   96/  265]
train() client id: f_00001-0-3 loss: 0.497680  [  128/  265]
train() client id: f_00001-0-4 loss: 0.497660  [  160/  265]
train() client id: f_00001-0-5 loss: 0.438396  [  192/  265]
train() client id: f_00001-0-6 loss: 0.651161  [  224/  265]
train() client id: f_00001-0-7 loss: 0.434882  [  256/  265]
train() client id: f_00001-1-0 loss: 0.465076  [   32/  265]
train() client id: f_00001-1-1 loss: 0.575167  [   64/  265]
train() client id: f_00001-1-2 loss: 0.538637  [   96/  265]
train() client id: f_00001-1-3 loss: 0.557109  [  128/  265]
train() client id: f_00001-1-4 loss: 0.454394  [  160/  265]
train() client id: f_00001-1-5 loss: 0.487435  [  192/  265]
train() client id: f_00001-1-6 loss: 0.415079  [  224/  265]
train() client id: f_00001-1-7 loss: 0.375587  [  256/  265]
train() client id: f_00001-2-0 loss: 0.430719  [   32/  265]
train() client id: f_00001-2-1 loss: 0.508026  [   64/  265]
train() client id: f_00001-2-2 loss: 0.475262  [   96/  265]
train() client id: f_00001-2-3 loss: 0.503560  [  128/  265]
train() client id: f_00001-2-4 loss: 0.442800  [  160/  265]
train() client id: f_00001-2-5 loss: 0.372377  [  192/  265]
train() client id: f_00001-2-6 loss: 0.465180  [  224/  265]
train() client id: f_00001-2-7 loss: 0.400880  [  256/  265]
train() client id: f_00001-3-0 loss: 0.359896  [   32/  265]
train() client id: f_00001-3-1 loss: 0.418647  [   64/  265]
train() client id: f_00001-3-2 loss: 0.442838  [   96/  265]
train() client id: f_00001-3-3 loss: 0.356565  [  128/  265]
train() client id: f_00001-3-4 loss: 0.482870  [  160/  265]
train() client id: f_00001-3-5 loss: 0.480241  [  192/  265]
train() client id: f_00001-3-6 loss: 0.378912  [  224/  265]
train() client id: f_00001-3-7 loss: 0.385866  [  256/  265]
train() client id: f_00001-4-0 loss: 0.360465  [   32/  265]
train() client id: f_00001-4-1 loss: 0.460475  [   64/  265]
train() client id: f_00001-4-2 loss: 0.413060  [   96/  265]
train() client id: f_00001-4-3 loss: 0.400849  [  128/  265]
train() client id: f_00001-4-4 loss: 0.358568  [  160/  265]
train() client id: f_00001-4-5 loss: 0.355927  [  192/  265]
train() client id: f_00001-4-6 loss: 0.334657  [  224/  265]
train() client id: f_00001-4-7 loss: 0.469503  [  256/  265]
train() client id: f_00001-5-0 loss: 0.477834  [   32/  265]
train() client id: f_00001-5-1 loss: 0.432214  [   64/  265]
train() client id: f_00001-5-2 loss: 0.334351  [   96/  265]
train() client id: f_00001-5-3 loss: 0.375322  [  128/  265]
train() client id: f_00001-5-4 loss: 0.346531  [  160/  265]
train() client id: f_00001-5-5 loss: 0.364059  [  192/  265]
train() client id: f_00001-5-6 loss: 0.355410  [  224/  265]
train() client id: f_00001-5-7 loss: 0.452192  [  256/  265]
train() client id: f_00001-6-0 loss: 0.321138  [   32/  265]
train() client id: f_00001-6-1 loss: 0.378910  [   64/  265]
train() client id: f_00001-6-2 loss: 0.479924  [   96/  265]
train() client id: f_00001-6-3 loss: 0.379012  [  128/  265]
train() client id: f_00001-6-4 loss: 0.346805  [  160/  265]
train() client id: f_00001-6-5 loss: 0.454522  [  192/  265]
train() client id: f_00001-6-6 loss: 0.278134  [  224/  265]
train() client id: f_00001-6-7 loss: 0.404991  [  256/  265]
train() client id: f_00001-7-0 loss: 0.300161  [   32/  265]
train() client id: f_00001-7-1 loss: 0.270151  [   64/  265]
train() client id: f_00001-7-2 loss: 0.451829  [   96/  265]
train() client id: f_00001-7-3 loss: 0.491102  [  128/  265]
train() client id: f_00001-7-4 loss: 0.440677  [  160/  265]
train() client id: f_00001-7-5 loss: 0.281667  [  192/  265]
train() client id: f_00001-7-6 loss: 0.404933  [  224/  265]
train() client id: f_00001-7-7 loss: 0.355697  [  256/  265]
train() client id: f_00001-8-0 loss: 0.336716  [   32/  265]
train() client id: f_00001-8-1 loss: 0.444394  [   64/  265]
train() client id: f_00001-8-2 loss: 0.277239  [   96/  265]
train() client id: f_00001-8-3 loss: 0.347965  [  128/  265]
train() client id: f_00001-8-4 loss: 0.335307  [  160/  265]
train() client id: f_00001-8-5 loss: 0.366971  [  192/  265]
train() client id: f_00001-8-6 loss: 0.415759  [  224/  265]
train() client id: f_00001-8-7 loss: 0.419375  [  256/  265]
train() client id: f_00001-9-0 loss: 0.402021  [   32/  265]
train() client id: f_00001-9-1 loss: 0.277031  [   64/  265]
train() client id: f_00001-9-2 loss: 0.279623  [   96/  265]
train() client id: f_00001-9-3 loss: 0.287937  [  128/  265]
train() client id: f_00001-9-4 loss: 0.273406  [  160/  265]
train() client id: f_00001-9-5 loss: 0.558591  [  192/  265]
train() client id: f_00001-9-6 loss: 0.289743  [  224/  265]
train() client id: f_00001-9-7 loss: 0.441615  [  256/  265]
train() client id: f_00001-10-0 loss: 0.386026  [   32/  265]
train() client id: f_00001-10-1 loss: 0.326137  [   64/  265]
train() client id: f_00001-10-2 loss: 0.274200  [   96/  265]
train() client id: f_00001-10-3 loss: 0.314116  [  128/  265]
train() client id: f_00001-10-4 loss: 0.458275  [  160/  265]
train() client id: f_00001-10-5 loss: 0.343707  [  192/  265]
train() client id: f_00001-10-6 loss: 0.332174  [  224/  265]
train() client id: f_00001-10-7 loss: 0.414573  [  256/  265]
train() client id: f_00001-11-0 loss: 0.409452  [   32/  265]
train() client id: f_00001-11-1 loss: 0.336677  [   64/  265]
train() client id: f_00001-11-2 loss: 0.293371  [   96/  265]
train() client id: f_00001-11-3 loss: 0.322288  [  128/  265]
train() client id: f_00001-11-4 loss: 0.455647  [  160/  265]
train() client id: f_00001-11-5 loss: 0.301283  [  192/  265]
train() client id: f_00001-11-6 loss: 0.345252  [  224/  265]
train() client id: f_00001-11-7 loss: 0.349440  [  256/  265]
train() client id: f_00002-0-0 loss: 1.350481  [   32/  124]
train() client id: f_00002-0-1 loss: 1.302906  [   64/  124]
train() client id: f_00002-0-2 loss: 1.305731  [   96/  124]
train() client id: f_00002-1-0 loss: 1.287723  [   32/  124]
train() client id: f_00002-1-1 loss: 1.279563  [   64/  124]
train() client id: f_00002-1-2 loss: 1.233878  [   96/  124]
train() client id: f_00002-2-0 loss: 1.235045  [   32/  124]
train() client id: f_00002-2-1 loss: 1.214924  [   64/  124]
train() client id: f_00002-2-2 loss: 1.173607  [   96/  124]
train() client id: f_00002-3-0 loss: 1.200112  [   32/  124]
train() client id: f_00002-3-1 loss: 1.196075  [   64/  124]
train() client id: f_00002-3-2 loss: 1.208191  [   96/  124]
train() client id: f_00002-4-0 loss: 1.105762  [   32/  124]
train() client id: f_00002-4-1 loss: 1.186610  [   64/  124]
train() client id: f_00002-4-2 loss: 1.167768  [   96/  124]
train() client id: f_00002-5-0 loss: 1.091584  [   32/  124]
train() client id: f_00002-5-1 loss: 1.138767  [   64/  124]
train() client id: f_00002-5-2 loss: 1.097767  [   96/  124]
train() client id: f_00002-6-0 loss: 1.122435  [   32/  124]
train() client id: f_00002-6-1 loss: 1.106576  [   64/  124]
train() client id: f_00002-6-2 loss: 1.114586  [   96/  124]
train() client id: f_00002-7-0 loss: 1.085309  [   32/  124]
train() client id: f_00002-7-1 loss: 1.114624  [   64/  124]
train() client id: f_00002-7-2 loss: 1.205470  [   96/  124]
train() client id: f_00002-8-0 loss: 1.063558  [   32/  124]
train() client id: f_00002-8-1 loss: 1.096075  [   64/  124]
train() client id: f_00002-8-2 loss: 1.162643  [   96/  124]
train() client id: f_00002-9-0 loss: 1.067967  [   32/  124]
train() client id: f_00002-9-1 loss: 1.204933  [   64/  124]
train() client id: f_00002-9-2 loss: 1.103961  [   96/  124]
train() client id: f_00002-10-0 loss: 1.192875  [   32/  124]
train() client id: f_00002-10-1 loss: 1.112821  [   64/  124]
train() client id: f_00002-10-2 loss: 1.045766  [   96/  124]
train() client id: f_00002-11-0 loss: 1.022594  [   32/  124]
train() client id: f_00002-11-1 loss: 1.184281  [   64/  124]
train() client id: f_00002-11-2 loss: 1.065013  [   96/  124]
train() client id: f_00003-0-0 loss: 0.991651  [   32/   43]
train() client id: f_00003-1-0 loss: 1.073553  [   32/   43]
train() client id: f_00003-2-0 loss: 0.990639  [   32/   43]
train() client id: f_00003-3-0 loss: 1.044877  [   32/   43]
train() client id: f_00003-4-0 loss: 1.012208  [   32/   43]
train() client id: f_00003-5-0 loss: 1.052607  [   32/   43]
train() client id: f_00003-6-0 loss: 0.983455  [   32/   43]
train() client id: f_00003-7-0 loss: 1.046474  [   32/   43]
train() client id: f_00003-8-0 loss: 0.983293  [   32/   43]
train() client id: f_00003-9-0 loss: 1.029836  [   32/   43]
train() client id: f_00003-10-0 loss: 0.989236  [   32/   43]
train() client id: f_00003-11-0 loss: 1.009250  [   32/   43]
train() client id: f_00004-0-0 loss: 1.002056  [   32/  306]
train() client id: f_00004-0-1 loss: 1.069373  [   64/  306]
train() client id: f_00004-0-2 loss: 1.028401  [   96/  306]
train() client id: f_00004-0-3 loss: 1.168161  [  128/  306]
train() client id: f_00004-0-4 loss: 1.122987  [  160/  306]
train() client id: f_00004-0-5 loss: 1.115504  [  192/  306]
train() client id: f_00004-0-6 loss: 1.052436  [  224/  306]
train() client id: f_00004-0-7 loss: 1.039945  [  256/  306]
train() client id: f_00004-0-8 loss: 0.946832  [  288/  306]
train() client id: f_00004-1-0 loss: 1.029149  [   32/  306]
train() client id: f_00004-1-1 loss: 1.097738  [   64/  306]
train() client id: f_00004-1-2 loss: 0.998683  [   96/  306]
train() client id: f_00004-1-3 loss: 1.011770  [  128/  306]
train() client id: f_00004-1-4 loss: 1.041124  [  160/  306]
train() client id: f_00004-1-5 loss: 1.004254  [  192/  306]
train() client id: f_00004-1-6 loss: 1.081025  [  224/  306]
train() client id: f_00004-1-7 loss: 1.153247  [  256/  306]
train() client id: f_00004-1-8 loss: 1.083484  [  288/  306]
train() client id: f_00004-2-0 loss: 1.052680  [   32/  306]
train() client id: f_00004-2-1 loss: 1.056297  [   64/  306]
train() client id: f_00004-2-2 loss: 1.066737  [   96/  306]
train() client id: f_00004-2-3 loss: 1.073873  [  128/  306]
train() client id: f_00004-2-4 loss: 1.065576  [  160/  306]
train() client id: f_00004-2-5 loss: 1.059974  [  192/  306]
train() client id: f_00004-2-6 loss: 1.015272  [  224/  306]
train() client id: f_00004-2-7 loss: 1.111750  [  256/  306]
train() client id: f_00004-2-8 loss: 1.011453  [  288/  306]
train() client id: f_00004-3-0 loss: 0.987966  [   32/  306]
train() client id: f_00004-3-1 loss: 1.132825  [   64/  306]
train() client id: f_00004-3-2 loss: 1.030155  [   96/  306]
train() client id: f_00004-3-3 loss: 1.066966  [  128/  306]
train() client id: f_00004-3-4 loss: 1.049424  [  160/  306]
train() client id: f_00004-3-5 loss: 1.013066  [  192/  306]
train() client id: f_00004-3-6 loss: 1.141157  [  224/  306]
train() client id: f_00004-3-7 loss: 1.185739  [  256/  306]
train() client id: f_00004-3-8 loss: 0.937622  [  288/  306]
train() client id: f_00004-4-0 loss: 0.996666  [   32/  306]
train() client id: f_00004-4-1 loss: 1.130464  [   64/  306]
train() client id: f_00004-4-2 loss: 1.078053  [   96/  306]
train() client id: f_00004-4-3 loss: 0.947717  [  128/  306]
train() client id: f_00004-4-4 loss: 1.121997  [  160/  306]
train() client id: f_00004-4-5 loss: 0.936957  [  192/  306]
train() client id: f_00004-4-6 loss: 1.022816  [  224/  306]
train() client id: f_00004-4-7 loss: 1.089024  [  256/  306]
train() client id: f_00004-4-8 loss: 1.070624  [  288/  306]
train() client id: f_00004-5-0 loss: 0.916054  [   32/  306]
train() client id: f_00004-5-1 loss: 1.116429  [   64/  306]
train() client id: f_00004-5-2 loss: 1.120962  [   96/  306]
train() client id: f_00004-5-3 loss: 0.972987  [  128/  306]
train() client id: f_00004-5-4 loss: 1.073738  [  160/  306]
train() client id: f_00004-5-5 loss: 1.151516  [  192/  306]
train() client id: f_00004-5-6 loss: 1.134970  [  224/  306]
train() client id: f_00004-5-7 loss: 1.057465  [  256/  306]
train() client id: f_00004-5-8 loss: 0.935119  [  288/  306]
train() client id: f_00004-6-0 loss: 0.932451  [   32/  306]
train() client id: f_00004-6-1 loss: 0.983027  [   64/  306]
train() client id: f_00004-6-2 loss: 1.077316  [   96/  306]
train() client id: f_00004-6-3 loss: 1.015079  [  128/  306]
train() client id: f_00004-6-4 loss: 1.062030  [  160/  306]
train() client id: f_00004-6-5 loss: 1.095643  [  192/  306]
train() client id: f_00004-6-6 loss: 1.007262  [  224/  306]
train() client id: f_00004-6-7 loss: 1.086947  [  256/  306]
train() client id: f_00004-6-8 loss: 1.131078  [  288/  306]
train() client id: f_00004-7-0 loss: 1.046656  [   32/  306]
train() client id: f_00004-7-1 loss: 0.963273  [   64/  306]
train() client id: f_00004-7-2 loss: 1.058019  [   96/  306]
train() client id: f_00004-7-3 loss: 1.084613  [  128/  306]
train() client id: f_00004-7-4 loss: 1.131852  [  160/  306]
train() client id: f_00004-7-5 loss: 1.093238  [  192/  306]
train() client id: f_00004-7-6 loss: 0.967744  [  224/  306]
train() client id: f_00004-7-7 loss: 1.043713  [  256/  306]
train() client id: f_00004-7-8 loss: 0.901246  [  288/  306]
train() client id: f_00004-8-0 loss: 1.100394  [   32/  306]
train() client id: f_00004-8-1 loss: 1.114108  [   64/  306]
train() client id: f_00004-8-2 loss: 1.011694  [   96/  306]
train() client id: f_00004-8-3 loss: 1.028681  [  128/  306]
train() client id: f_00004-8-4 loss: 1.065901  [  160/  306]
train() client id: f_00004-8-5 loss: 0.975465  [  192/  306]
train() client id: f_00004-8-6 loss: 0.966002  [  224/  306]
train() client id: f_00004-8-7 loss: 1.071936  [  256/  306]
train() client id: f_00004-8-8 loss: 1.058073  [  288/  306]
train() client id: f_00004-9-0 loss: 1.013847  [   32/  306]
train() client id: f_00004-9-1 loss: 1.124760  [   64/  306]
train() client id: f_00004-9-2 loss: 1.014723  [   96/  306]
train() client id: f_00004-9-3 loss: 1.040597  [  128/  306]
train() client id: f_00004-9-4 loss: 1.151994  [  160/  306]
train() client id: f_00004-9-5 loss: 0.991024  [  192/  306]
train() client id: f_00004-9-6 loss: 0.938651  [  224/  306]
train() client id: f_00004-9-7 loss: 1.039620  [  256/  306]
train() client id: f_00004-9-8 loss: 1.023670  [  288/  306]
train() client id: f_00004-10-0 loss: 1.014158  [   32/  306]
train() client id: f_00004-10-1 loss: 0.916339  [   64/  306]
train() client id: f_00004-10-2 loss: 1.059512  [   96/  306]
train() client id: f_00004-10-3 loss: 1.050243  [  128/  306]
train() client id: f_00004-10-4 loss: 1.037765  [  160/  306]
train() client id: f_00004-10-5 loss: 1.000201  [  192/  306]
train() client id: f_00004-10-6 loss: 1.053407  [  224/  306]
train() client id: f_00004-10-7 loss: 1.146962  [  256/  306]
train() client id: f_00004-10-8 loss: 1.010837  [  288/  306]
train() client id: f_00004-11-0 loss: 1.004129  [   32/  306]
train() client id: f_00004-11-1 loss: 1.069287  [   64/  306]
train() client id: f_00004-11-2 loss: 1.059082  [   96/  306]
train() client id: f_00004-11-3 loss: 0.967317  [  128/  306]
train() client id: f_00004-11-4 loss: 0.998135  [  160/  306]
train() client id: f_00004-11-5 loss: 1.129141  [  192/  306]
train() client id: f_00004-11-6 loss: 0.933512  [  224/  306]
train() client id: f_00004-11-7 loss: 1.079963  [  256/  306]
train() client id: f_00004-11-8 loss: 1.033210  [  288/  306]
train() client id: f_00005-0-0 loss: 0.854360  [   32/  146]
train() client id: f_00005-0-1 loss: 0.860266  [   64/  146]
train() client id: f_00005-0-2 loss: 0.734267  [   96/  146]
train() client id: f_00005-0-3 loss: 0.930781  [  128/  146]
train() client id: f_00005-1-0 loss: 0.947503  [   32/  146]
train() client id: f_00005-1-1 loss: 0.795827  [   64/  146]
train() client id: f_00005-1-2 loss: 0.859484  [   96/  146]
train() client id: f_00005-1-3 loss: 0.747364  [  128/  146]
train() client id: f_00005-2-0 loss: 0.735079  [   32/  146]
train() client id: f_00005-2-1 loss: 0.810658  [   64/  146]
train() client id: f_00005-2-2 loss: 0.852062  [   96/  146]
train() client id: f_00005-2-3 loss: 0.869075  [  128/  146]
train() client id: f_00005-3-0 loss: 0.793553  [   32/  146]
train() client id: f_00005-3-1 loss: 0.764041  [   64/  146]
train() client id: f_00005-3-2 loss: 0.958435  [   96/  146]
train() client id: f_00005-3-3 loss: 0.762182  [  128/  146]
train() client id: f_00005-4-0 loss: 0.678793  [   32/  146]
train() client id: f_00005-4-1 loss: 0.686124  [   64/  146]
train() client id: f_00005-4-2 loss: 0.835967  [   96/  146]
train() client id: f_00005-4-3 loss: 0.970657  [  128/  146]
train() client id: f_00005-5-0 loss: 0.716291  [   32/  146]
train() client id: f_00005-5-1 loss: 0.665775  [   64/  146]
train() client id: f_00005-5-2 loss: 0.900633  [   96/  146]
train() client id: f_00005-5-3 loss: 0.811213  [  128/  146]
train() client id: f_00005-6-0 loss: 0.836584  [   32/  146]
train() client id: f_00005-6-1 loss: 0.846728  [   64/  146]
train() client id: f_00005-6-2 loss: 0.796779  [   96/  146]
train() client id: f_00005-6-3 loss: 0.711856  [  128/  146]
train() client id: f_00005-7-0 loss: 0.841807  [   32/  146]
train() client id: f_00005-7-1 loss: 0.745744  [   64/  146]
train() client id: f_00005-7-2 loss: 0.881182  [   96/  146]
train() client id: f_00005-7-3 loss: 0.697267  [  128/  146]
train() client id: f_00005-8-0 loss: 0.808239  [   32/  146]
train() client id: f_00005-8-1 loss: 0.892458  [   64/  146]
train() client id: f_00005-8-2 loss: 0.631224  [   96/  146]
train() client id: f_00005-8-3 loss: 0.800523  [  128/  146]
train() client id: f_00005-9-0 loss: 0.784491  [   32/  146]
train() client id: f_00005-9-1 loss: 0.660758  [   64/  146]
train() client id: f_00005-9-2 loss: 0.823231  [   96/  146]
train() client id: f_00005-9-3 loss: 0.860892  [  128/  146]
train() client id: f_00005-10-0 loss: 0.800932  [   32/  146]
train() client id: f_00005-10-1 loss: 0.741638  [   64/  146]
train() client id: f_00005-10-2 loss: 0.834380  [   96/  146]
train() client id: f_00005-10-3 loss: 0.565690  [  128/  146]
train() client id: f_00005-11-0 loss: 0.707022  [   32/  146]
train() client id: f_00005-11-1 loss: 0.744871  [   64/  146]
train() client id: f_00005-11-2 loss: 0.839329  [   96/  146]
train() client id: f_00005-11-3 loss: 0.784377  [  128/  146]
train() client id: f_00006-0-0 loss: 0.920991  [   32/   54]
train() client id: f_00006-1-0 loss: 0.959790  [   32/   54]
train() client id: f_00006-2-0 loss: 0.935461  [   32/   54]
train() client id: f_00006-3-0 loss: 0.932706  [   32/   54]
train() client id: f_00006-4-0 loss: 0.937341  [   32/   54]
train() client id: f_00006-5-0 loss: 0.938028  [   32/   54]
train() client id: f_00006-6-0 loss: 0.930729  [   32/   54]
train() client id: f_00006-7-0 loss: 0.938874  [   32/   54]
train() client id: f_00006-8-0 loss: 0.925526  [   32/   54]
train() client id: f_00006-9-0 loss: 0.911740  [   32/   54]
train() client id: f_00006-10-0 loss: 0.953140  [   32/   54]
train() client id: f_00006-11-0 loss: 0.934605  [   32/   54]
train() client id: f_00007-0-0 loss: 0.846574  [   32/  179]
train() client id: f_00007-0-1 loss: 0.845537  [   64/  179]
train() client id: f_00007-0-2 loss: 0.841506  [   96/  179]
train() client id: f_00007-0-3 loss: 0.790607  [  128/  179]
train() client id: f_00007-0-4 loss: 0.890511  [  160/  179]
train() client id: f_00007-1-0 loss: 0.859191  [   32/  179]
train() client id: f_00007-1-1 loss: 0.759473  [   64/  179]
train() client id: f_00007-1-2 loss: 0.779219  [   96/  179]
train() client id: f_00007-1-3 loss: 0.802376  [  128/  179]
train() client id: f_00007-1-4 loss: 0.823082  [  160/  179]
train() client id: f_00007-2-0 loss: 0.839346  [   32/  179]
train() client id: f_00007-2-1 loss: 0.829684  [   64/  179]
train() client id: f_00007-2-2 loss: 0.709087  [   96/  179]
train() client id: f_00007-2-3 loss: 0.813238  [  128/  179]
train() client id: f_00007-2-4 loss: 0.746659  [  160/  179]
train() client id: f_00007-3-0 loss: 0.786841  [   32/  179]
train() client id: f_00007-3-1 loss: 0.700314  [   64/  179]
train() client id: f_00007-3-2 loss: 0.695367  [   96/  179]
train() client id: f_00007-3-3 loss: 0.764860  [  128/  179]
train() client id: f_00007-3-4 loss: 0.774593  [  160/  179]
train() client id: f_00007-4-0 loss: 0.870416  [   32/  179]
train() client id: f_00007-4-1 loss: 0.660616  [   64/  179]
train() client id: f_00007-4-2 loss: 0.837721  [   96/  179]
train() client id: f_00007-4-3 loss: 0.667709  [  128/  179]
train() client id: f_00007-4-4 loss: 0.723329  [  160/  179]
train() client id: f_00007-5-0 loss: 0.713914  [   32/  179]
train() client id: f_00007-5-1 loss: 0.758367  [   64/  179]
train() client id: f_00007-5-2 loss: 0.695334  [   96/  179]
train() client id: f_00007-5-3 loss: 0.640933  [  128/  179]
train() client id: f_00007-5-4 loss: 0.824195  [  160/  179]
train() client id: f_00007-6-0 loss: 0.703960  [   32/  179]
train() client id: f_00007-6-1 loss: 0.810809  [   64/  179]
train() client id: f_00007-6-2 loss: 0.724321  [   96/  179]
train() client id: f_00007-6-3 loss: 0.735061  [  128/  179]
train() client id: f_00007-6-4 loss: 0.700841  [  160/  179]
train() client id: f_00007-7-0 loss: 0.634931  [   32/  179]
train() client id: f_00007-7-1 loss: 0.695815  [   64/  179]
train() client id: f_00007-7-2 loss: 0.746472  [   96/  179]
train() client id: f_00007-7-3 loss: 0.709144  [  128/  179]
train() client id: f_00007-7-4 loss: 0.803319  [  160/  179]
train() client id: f_00007-8-0 loss: 0.695125  [   32/  179]
train() client id: f_00007-8-1 loss: 0.692437  [   64/  179]
train() client id: f_00007-8-2 loss: 0.871764  [   96/  179]
train() client id: f_00007-8-3 loss: 0.681018  [  128/  179]
train() client id: f_00007-8-4 loss: 0.615001  [  160/  179]
train() client id: f_00007-9-0 loss: 0.687703  [   32/  179]
train() client id: f_00007-9-1 loss: 0.680425  [   64/  179]
train() client id: f_00007-9-2 loss: 0.813067  [   96/  179]
train() client id: f_00007-9-3 loss: 0.620299  [  128/  179]
train() client id: f_00007-9-4 loss: 0.742036  [  160/  179]
train() client id: f_00007-10-0 loss: 0.731726  [   32/  179]
train() client id: f_00007-10-1 loss: 0.872620  [   64/  179]
train() client id: f_00007-10-2 loss: 0.694827  [   96/  179]
train() client id: f_00007-10-3 loss: 0.618666  [  128/  179]
train() client id: f_00007-10-4 loss: 0.680898  [  160/  179]
train() client id: f_00007-11-0 loss: 0.692116  [   32/  179]
train() client id: f_00007-11-1 loss: 0.627899  [   64/  179]
train() client id: f_00007-11-2 loss: 0.755084  [   96/  179]
train() client id: f_00007-11-3 loss: 0.731749  [  128/  179]
train() client id: f_00007-11-4 loss: 0.741018  [  160/  179]
train() client id: f_00008-0-0 loss: 0.864492  [   32/  130]
train() client id: f_00008-0-1 loss: 1.015529  [   64/  130]
train() client id: f_00008-0-2 loss: 0.836509  [   96/  130]
train() client id: f_00008-0-3 loss: 0.786996  [  128/  130]
train() client id: f_00008-1-0 loss: 0.888824  [   32/  130]
train() client id: f_00008-1-1 loss: 0.872848  [   64/  130]
train() client id: f_00008-1-2 loss: 0.897679  [   96/  130]
train() client id: f_00008-1-3 loss: 0.840728  [  128/  130]
train() client id: f_00008-2-0 loss: 0.962967  [   32/  130]
train() client id: f_00008-2-1 loss: 0.904622  [   64/  130]
train() client id: f_00008-2-2 loss: 0.782563  [   96/  130]
train() client id: f_00008-2-3 loss: 0.807686  [  128/  130]
train() client id: f_00008-3-0 loss: 0.850206  [   32/  130]
train() client id: f_00008-3-1 loss: 0.847665  [   64/  130]
train() client id: f_00008-3-2 loss: 0.871888  [   96/  130]
train() client id: f_00008-3-3 loss: 0.919521  [  128/  130]
train() client id: f_00008-4-0 loss: 0.858947  [   32/  130]
train() client id: f_00008-4-1 loss: 0.889883  [   64/  130]
train() client id: f_00008-4-2 loss: 0.832566  [   96/  130]
train() client id: f_00008-4-3 loss: 0.899822  [  128/  130]
train() client id: f_00008-5-0 loss: 0.866085  [   32/  130]
train() client id: f_00008-5-1 loss: 0.892133  [   64/  130]
train() client id: f_00008-5-2 loss: 0.828433  [   96/  130]
train() client id: f_00008-5-3 loss: 0.862083  [  128/  130]
train() client id: f_00008-6-0 loss: 0.888883  [   32/  130]
train() client id: f_00008-6-1 loss: 0.902457  [   64/  130]
train() client id: f_00008-6-2 loss: 0.841285  [   96/  130]
train() client id: f_00008-6-3 loss: 0.846193  [  128/  130]
train() client id: f_00008-7-0 loss: 0.874329  [   32/  130]
train() client id: f_00008-7-1 loss: 0.979691  [   64/  130]
train() client id: f_00008-7-2 loss: 0.851064  [   96/  130]
train() client id: f_00008-7-3 loss: 0.766590  [  128/  130]
train() client id: f_00008-8-0 loss: 0.829884  [   32/  130]
train() client id: f_00008-8-1 loss: 0.883178  [   64/  130]
train() client id: f_00008-8-2 loss: 0.891252  [   96/  130]
train() client id: f_00008-8-3 loss: 0.875111  [  128/  130]
train() client id: f_00008-9-0 loss: 0.815873  [   32/  130]
train() client id: f_00008-9-1 loss: 0.869272  [   64/  130]
train() client id: f_00008-9-2 loss: 0.907417  [   96/  130]
train() client id: f_00008-9-3 loss: 0.889085  [  128/  130]
train() client id: f_00008-10-0 loss: 0.855257  [   32/  130]
train() client id: f_00008-10-1 loss: 0.891535  [   64/  130]
train() client id: f_00008-10-2 loss: 0.839507  [   96/  130]
train() client id: f_00008-10-3 loss: 0.888941  [  128/  130]
train() client id: f_00008-11-0 loss: 0.831184  [   32/  130]
train() client id: f_00008-11-1 loss: 0.854353  [   64/  130]
train() client id: f_00008-11-2 loss: 0.884048  [   96/  130]
train() client id: f_00008-11-3 loss: 0.844456  [  128/  130]
train() client id: f_00009-0-0 loss: 1.008959  [   32/  118]
train() client id: f_00009-0-1 loss: 1.069291  [   64/  118]
train() client id: f_00009-0-2 loss: 1.119088  [   96/  118]
train() client id: f_00009-1-0 loss: 1.025888  [   32/  118]
train() client id: f_00009-1-1 loss: 0.918947  [   64/  118]
train() client id: f_00009-1-2 loss: 0.981157  [   96/  118]
train() client id: f_00009-2-0 loss: 0.893090  [   32/  118]
train() client id: f_00009-2-1 loss: 0.960175  [   64/  118]
train() client id: f_00009-2-2 loss: 0.983418  [   96/  118]
train() client id: f_00009-3-0 loss: 1.003768  [   32/  118]
train() client id: f_00009-3-1 loss: 0.851767  [   64/  118]
train() client id: f_00009-3-2 loss: 0.932400  [   96/  118]
train() client id: f_00009-4-0 loss: 0.941674  [   32/  118]
train() client id: f_00009-4-1 loss: 0.904016  [   64/  118]
train() client id: f_00009-4-2 loss: 0.900181  [   96/  118]
train() client id: f_00009-5-0 loss: 0.825368  [   32/  118]
train() client id: f_00009-5-1 loss: 0.848219  [   64/  118]
train() client id: f_00009-5-2 loss: 0.921845  [   96/  118]
train() client id: f_00009-6-0 loss: 0.774016  [   32/  118]
train() client id: f_00009-6-1 loss: 0.851987  [   64/  118]
train() client id: f_00009-6-2 loss: 1.019258  [   96/  118]
train() client id: f_00009-7-0 loss: 0.782036  [   32/  118]
train() client id: f_00009-7-1 loss: 0.806437  [   64/  118]
train() client id: f_00009-7-2 loss: 0.909045  [   96/  118]
train() client id: f_00009-8-0 loss: 0.900161  [   32/  118]
train() client id: f_00009-8-1 loss: 0.826044  [   64/  118]
train() client id: f_00009-8-2 loss: 0.757132  [   96/  118]
train() client id: f_00009-9-0 loss: 1.008501  [   32/  118]
train() client id: f_00009-9-1 loss: 0.832015  [   64/  118]
train() client id: f_00009-9-2 loss: 0.669231  [   96/  118]
train() client id: f_00009-10-0 loss: 0.721290  [   32/  118]
train() client id: f_00009-10-1 loss: 0.857130  [   64/  118]
train() client id: f_00009-10-2 loss: 0.843477  [   96/  118]
train() client id: f_00009-11-0 loss: 0.829402  [   32/  118]
train() client id: f_00009-11-1 loss: 0.865961  [   64/  118]
train() client id: f_00009-11-2 loss: 0.835095  [   96/  118]
At round 5 accuracy: 0.6312997347480106
At round 5 training accuracy: 0.5788061703554661
At round 5 training loss: 0.9176655911918579
update_location
xs = -3.905658 4.200318 45.009024 18.811294 -9.020704 3.956410 -7.443192 -1.324852 29.663977 -2.060879 
ys = 37.587959 20.555839 1.320614 -7.455176 9.350187 -17.185849 -2.624984 -4.177652 17.569006 4.001482 
dists_uav = 106.902333 102.177224 109.670216 102.026685 100.840464 101.543127 100.310975 100.095994 105.776280 100.101244 
dists_bs = 219.443256 236.570820 280.300731 266.181722 234.497308 262.603879 244.184191 249.534944 258.208949 243.204509 
uav_gains = -100.724721 -100.233874 -101.002277 -100.217866 -100.090889 -100.166283 -100.033728 -100.010434 -100.609743 -100.011003 
bs_gains = -105.124082 -106.037972 -108.100538 -107.472050 -105.930919 -107.307491 -106.423150 -106.686738 -107.102255 -106.374265 
Round 6
-------------------------------
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.20035717 21.31407924 10.04322819  3.59039783 24.57615325 11.85884139
  4.4646498  14.41376646 10.58251293  9.61787463]
obj_prev = 120.66186087163511
eta_min = 7.409354424179371e-10	eta_max = 0.91885879376386
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 28.063470776703085	eta = 0.909090909090909
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 48.384419986505044	eta = 0.5272822567213747
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 38.70245321762515	eta = 0.6591893805073021
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 36.96801720495558	eta = 0.6901167032896544
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 36.88278882583632	eta = 0.6917114180576249
af = 25.512246160639165	bf = 1.9670068663541114	zeta = 36.88256856080987	eta = 0.6917155490018552
eta = 0.6917155490018552
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [0.03041438 0.06396674 0.02993161 0.01037951 0.07386349 0.03524206
 0.01303473 0.04320773 0.03137991 0.0284833 ]
ene_total = [3.12833392 6.13288912 3.09403261 1.41867893 6.96750532 3.75135634
 1.63958615 4.19625148 3.41497814 3.13895655]
ti_comp = [0.28546116 0.26646804 0.2846808  0.28683999 0.2669565  0.2602256
 0.28732669 0.28738774 0.26129447 0.26489704]
ti_coms = [0.06497807 0.08397118 0.06575843 0.06359924 0.08348272 0.09021363
 0.06311253 0.06305149 0.08914475 0.08554218]
t_total = [29.69997482 29.69997482 29.69997482 29.69997482 29.69997482 29.69997482
 29.69997482 29.69997482 29.69997482 29.69997482]
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.15785733e-05 2.30384101e-04 2.06801576e-05 8.49436889e-07
 3.53417801e-04 4.03983548e-05 1.67661934e-06 6.10418604e-05
 2.82862087e-05 2.05824434e-05]
ene_total = [0.55252256 0.73118701 0.55906002 0.53908008 0.73747448 0.76799073
 0.53502532 0.53953919 0.7579054  0.72672045]
optimize_network iter = 0 obj = 6.446505230599096
eta = 0.6917155490018552
freqs = [5.32723555e+07 1.20027048e+08 5.25704736e+07 1.80928566e+07
 1.38343675e+08 6.77144291e+07 2.26827740e+07 7.51732237e+07
 6.00470262e+07 5.37629723e+07]
eta_min = 0.6801612059033121	eta_max = 0.6917155490018475
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 0.06073437862890649	eta = 0.9090909090909091
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 21.69496157536947	eta = 0.002544972079761971
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 2.3026899172665374	eta = 0.023977640700475235
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 2.226369694175787	eta = 0.024799597131268097
af = 0.05521307148082408	bf = 1.9670068663541114	zeta = 2.2263378919337997	eta = 0.024799951382431865
eta = 0.024799951382431865
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.16436107e-04 2.31078474e-03 2.07424872e-04 8.51997075e-06
 3.54482996e-03 4.05201146e-04 1.68167264e-05 6.12258394e-04
 2.83714628e-04 2.06444785e-04]
ene_total = [0.17988639 0.28688359 0.18173569 0.17062183 0.31863717 0.25255426
 0.16954014 0.18532952 0.24643571 0.23471359]
ti_comp = [0.29859544 0.27960232 0.29781508 0.29997427 0.28009078 0.27335988
 0.30046097 0.30052202 0.27442876 0.27803133]
ti_coms = [0.06497807 0.08397118 0.06575843 0.06359924 0.08348272 0.09021363
 0.06311253 0.06305149 0.08914475 0.08554218]
t_total = [29.69997482 29.69997482 29.69997482 29.69997482 29.69997482 29.69997482
 29.69997482 29.69997482 29.69997482 29.69997482]
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.25514581e-05 2.39268435e-04 2.16073258e-05 8.88109865e-07
 3.67109805e-04 4.18618440e-05 1.75321215e-06 6.38316142e-05
 2.93224432e-05 2.13642617e-05]
ene_total = [0.53264186 0.70549825 0.53893943 0.51960869 0.71195131 0.74036622
 0.5157035  0.52027595 0.73061032 0.70053117]
optimize_network iter = 1 obj = 6.216126697450631
eta = 0.6801612059033121
freqs = [5.32566467e+07 1.19616625e+08 5.25486356e+07 1.80913467e+07
 1.37882470e+08 6.74069266e+07 2.26825634e+07 7.51732237e+07
 5.97860936e+07 5.35642116e+07]
eta_min = 0.6801612059033234	eta_max = 0.680161205903311
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 0.06036643683170785	eta = 0.909090909090909
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 21.694610889380957	eta = 0.0025295949864110306
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 2.300984648578189	eta = 0.023850041316801688
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 2.2251002879015918	eta = 0.024663418200206223
af = 0.05487857893791622	bf = 1.9670068663541114	zeta = 2.225068999906738	eta = 0.02466376500693525
eta = 0.02466376500693525
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.16630671e-04 2.29842707e-03 2.07561279e-04 8.53123712e-06
 3.52647900e-03 4.02127407e-04 1.68414620e-05 6.13170349e-04
 2.81673164e-04 2.05226390e-04]
ene_total = [0.17986649 0.2865125  0.18171397 0.17059831 0.31810109 0.25243665
 0.16951713 0.18532807 0.24634661 0.23464818]
ti_comp = [0.29859544 0.27960232 0.29781508 0.29997427 0.28009078 0.27335988
 0.30046097 0.30052202 0.27442876 0.27803133]
ti_coms = [0.06497807 0.08397118 0.06575843 0.06359924 0.08348272 0.09021363
 0.06311253 0.06305149 0.08914475 0.08554218]
t_total = [29.69997482 29.69997482 29.69997482 29.69997482 29.69997482 29.69997482
 29.69997482 29.69997482 29.69997482 29.69997482]
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.25514581e-05 2.39268435e-04 2.16073258e-05 8.88109865e-07
 3.67109805e-04 4.18618440e-05 1.75321215e-06 6.38316142e-05
 2.93224432e-05 2.13642617e-05]
ene_total = [0.53264186 0.70549825 0.53893943 0.51960869 0.71195131 0.74036622
 0.5157035  0.52027595 0.73061032 0.70053117]
optimize_network iter = 2 obj = 6.216126697450847
eta = 0.6801612059033234
freqs = [5.32566467e+07 1.19616625e+08 5.25486356e+07 1.80913467e+07
 1.37882470e+08 6.74069266e+07 2.26825634e+07 7.51732237e+07
 5.97860936e+07 5.35642116e+07]
Done!
ene_coms = [0.00649781 0.00839712 0.00657584 0.00635992 0.00834827 0.00902136
 0.00631125 0.00630515 0.00891447 0.00855422]
ene_comp = [2.14422044e-05 2.27499377e-04 2.05445117e-05 8.44425808e-07
 3.49052528e-04 3.98027573e-05 1.66697572e-06 6.06918856e-05
 2.78801405e-05 2.03134033e-05]
ene_total = [0.00651925 0.00862462 0.00659639 0.00636077 0.00869732 0.00906117
 0.00631292 0.00636584 0.00894236 0.00857453]
At round 6 energy consumption: 0.07605515902799734
At round 6 eta: 0.6801612059033234
At round 6 a_n: 26.12732777443847
At round 6 local rounds: 12.620787147614399
At round 6 global rounds: 81.68905166188517
gradient difference: 0.4390849173069
train() client id: f_00000-0-0 loss: 1.312751  [   32/  126]
train() client id: f_00000-0-1 loss: 1.446486  [   64/  126]
train() client id: f_00000-0-2 loss: 1.256084  [   96/  126]
train() client id: f_00000-1-0 loss: 1.307395  [   32/  126]
train() client id: f_00000-1-1 loss: 1.171526  [   64/  126]
train() client id: f_00000-1-2 loss: 1.202190  [   96/  126]
train() client id: f_00000-2-0 loss: 1.115332  [   32/  126]
train() client id: f_00000-2-1 loss: 1.101896  [   64/  126]
train() client id: f_00000-2-2 loss: 1.069927  [   96/  126]
train() client id: f_00000-3-0 loss: 1.104395  [   32/  126]
train() client id: f_00000-3-1 loss: 1.053930  [   64/  126]
train() client id: f_00000-3-2 loss: 0.984752  [   96/  126]
train() client id: f_00000-4-0 loss: 1.007705  [   32/  126]
train() client id: f_00000-4-1 loss: 1.011557  [   64/  126]
train() client id: f_00000-4-2 loss: 0.952372  [   96/  126]
train() client id: f_00000-5-0 loss: 0.988750  [   32/  126]
train() client id: f_00000-5-1 loss: 0.982727  [   64/  126]
train() client id: f_00000-5-2 loss: 0.949423  [   96/  126]
train() client id: f_00000-6-0 loss: 0.941688  [   32/  126]
train() client id: f_00000-6-1 loss: 0.978580  [   64/  126]
train() client id: f_00000-6-2 loss: 0.985033  [   96/  126]
train() client id: f_00000-7-0 loss: 0.921285  [   32/  126]
train() client id: f_00000-7-1 loss: 0.924036  [   64/  126]
train() client id: f_00000-7-2 loss: 0.983572  [   96/  126]
train() client id: f_00000-8-0 loss: 0.970818  [   32/  126]
train() client id: f_00000-8-1 loss: 0.948884  [   64/  126]
train() client id: f_00000-8-2 loss: 0.871570  [   96/  126]
train() client id: f_00000-9-0 loss: 0.901590  [   32/  126]
train() client id: f_00000-9-1 loss: 0.916624  [   64/  126]
train() client id: f_00000-9-2 loss: 0.988270  [   96/  126]
train() client id: f_00000-10-0 loss: 1.022684  [   32/  126]
train() client id: f_00000-10-1 loss: 0.880278  [   64/  126]
train() client id: f_00000-10-2 loss: 0.944908  [   96/  126]
train() client id: f_00000-11-0 loss: 0.980895  [   32/  126]
train() client id: f_00000-11-1 loss: 0.931684  [   64/  126]
train() client id: f_00000-11-2 loss: 0.997989  [   96/  126]
train() client id: f_00001-0-0 loss: 0.617095  [   32/  265]
train() client id: f_00001-0-1 loss: 0.611265  [   64/  265]
train() client id: f_00001-0-2 loss: 0.623660  [   96/  265]
train() client id: f_00001-0-3 loss: 0.603337  [  128/  265]
train() client id: f_00001-0-4 loss: 0.663751  [  160/  265]
train() client id: f_00001-0-5 loss: 0.532402  [  192/  265]
train() client id: f_00001-0-6 loss: 0.510927  [  224/  265]
train() client id: f_00001-0-7 loss: 0.547153  [  256/  265]
train() client id: f_00001-1-0 loss: 0.512307  [   32/  265]
train() client id: f_00001-1-1 loss: 0.677400  [   64/  265]
train() client id: f_00001-1-2 loss: 0.636584  [   96/  265]
train() client id: f_00001-1-3 loss: 0.621123  [  128/  265]
train() client id: f_00001-1-4 loss: 0.530226  [  160/  265]
train() client id: f_00001-1-5 loss: 0.552310  [  192/  265]
train() client id: f_00001-1-6 loss: 0.494668  [  224/  265]
train() client id: f_00001-1-7 loss: 0.527296  [  256/  265]
train() client id: f_00001-2-0 loss: 0.558754  [   32/  265]
train() client id: f_00001-2-1 loss: 0.626669  [   64/  265]
train() client id: f_00001-2-2 loss: 0.547567  [   96/  265]
train() client id: f_00001-2-3 loss: 0.476863  [  128/  265]
train() client id: f_00001-2-4 loss: 0.635216  [  160/  265]
train() client id: f_00001-2-5 loss: 0.544994  [  192/  265]
train() client id: f_00001-2-6 loss: 0.523801  [  224/  265]
train() client id: f_00001-2-7 loss: 0.495551  [  256/  265]
train() client id: f_00001-3-0 loss: 0.676056  [   32/  265]
train() client id: f_00001-3-1 loss: 0.492369  [   64/  265]
train() client id: f_00001-3-2 loss: 0.566154  [   96/  265]
train() client id: f_00001-3-3 loss: 0.549474  [  128/  265]
train() client id: f_00001-3-4 loss: 0.524016  [  160/  265]
train() client id: f_00001-3-5 loss: 0.509881  [  192/  265]
train() client id: f_00001-3-6 loss: 0.523165  [  224/  265]
train() client id: f_00001-3-7 loss: 0.454501  [  256/  265]
train() client id: f_00001-4-0 loss: 0.536656  [   32/  265]
train() client id: f_00001-4-1 loss: 0.559694  [   64/  265]
train() client id: f_00001-4-2 loss: 0.523195  [   96/  265]
train() client id: f_00001-4-3 loss: 0.502014  [  128/  265]
train() client id: f_00001-4-4 loss: 0.461965  [  160/  265]
train() client id: f_00001-4-5 loss: 0.498597  [  192/  265]
train() client id: f_00001-4-6 loss: 0.533568  [  224/  265]
train() client id: f_00001-4-7 loss: 0.551042  [  256/  265]
train() client id: f_00001-5-0 loss: 0.484257  [   32/  265]
train() client id: f_00001-5-1 loss: 0.481099  [   64/  265]
train() client id: f_00001-5-2 loss: 0.514652  [   96/  265]
train() client id: f_00001-5-3 loss: 0.551438  [  128/  265]
train() client id: f_00001-5-4 loss: 0.673545  [  160/  265]
train() client id: f_00001-5-5 loss: 0.424562  [  192/  265]
train() client id: f_00001-5-6 loss: 0.500538  [  224/  265]
train() client id: f_00001-5-7 loss: 0.549803  [  256/  265]
train() client id: f_00001-6-0 loss: 0.504016  [   32/  265]
train() client id: f_00001-6-1 loss: 0.449737  [   64/  265]
train() client id: f_00001-6-2 loss: 0.473424  [   96/  265]
train() client id: f_00001-6-3 loss: 0.525791  [  128/  265]
train() client id: f_00001-6-4 loss: 0.497175  [  160/  265]
train() client id: f_00001-6-5 loss: 0.489202  [  192/  265]
train() client id: f_00001-6-6 loss: 0.731339  [  224/  265]
train() client id: f_00001-6-7 loss: 0.474747  [  256/  265]
train() client id: f_00001-7-0 loss: 0.611646  [   32/  265]
train() client id: f_00001-7-1 loss: 0.410513  [   64/  265]
train() client id: f_00001-7-2 loss: 0.529014  [   96/  265]
train() client id: f_00001-7-3 loss: 0.487840  [  128/  265]
train() client id: f_00001-7-4 loss: 0.654820  [  160/  265]
train() client id: f_00001-7-5 loss: 0.591831  [  192/  265]
train() client id: f_00001-7-6 loss: 0.404626  [  224/  265]
train() client id: f_00001-7-7 loss: 0.437993  [  256/  265]
train() client id: f_00001-8-0 loss: 0.595383  [   32/  265]
train() client id: f_00001-8-1 loss: 0.548169  [   64/  265]
train() client id: f_00001-8-2 loss: 0.489302  [   96/  265]
train() client id: f_00001-8-3 loss: 0.560151  [  128/  265]
train() client id: f_00001-8-4 loss: 0.461755  [  160/  265]
train() client id: f_00001-8-5 loss: 0.506061  [  192/  265]
train() client id: f_00001-8-6 loss: 0.480606  [  224/  265]
train() client id: f_00001-8-7 loss: 0.471779  [  256/  265]
train() client id: f_00001-9-0 loss: 0.600107  [   32/  265]
train() client id: f_00001-9-1 loss: 0.500458  [   64/  265]
train() client id: f_00001-9-2 loss: 0.565307  [   96/  265]
train() client id: f_00001-9-3 loss: 0.461136  [  128/  265]
train() client id: f_00001-9-4 loss: 0.535131  [  160/  265]
train() client id: f_00001-9-5 loss: 0.471859  [  192/  265]
train() client id: f_00001-9-6 loss: 0.476520  [  224/  265]
train() client id: f_00001-9-7 loss: 0.435935  [  256/  265]
train() client id: f_00001-10-0 loss: 0.608026  [   32/  265]
train() client id: f_00001-10-1 loss: 0.560970  [   64/  265]
train() client id: f_00001-10-2 loss: 0.401127  [   96/  265]
train() client id: f_00001-10-3 loss: 0.527734  [  128/  265]
train() client id: f_00001-10-4 loss: 0.451768  [  160/  265]
train() client id: f_00001-10-5 loss: 0.449001  [  192/  265]
train() client id: f_00001-10-6 loss: 0.579328  [  224/  265]
train() client id: f_00001-10-7 loss: 0.467082  [  256/  265]
train() client id: f_00001-11-0 loss: 0.582626  [   32/  265]
train() client id: f_00001-11-1 loss: 0.412462  [   64/  265]
train() client id: f_00001-11-2 loss: 0.520199  [   96/  265]
train() client id: f_00001-11-3 loss: 0.591943  [  128/  265]
train() client id: f_00001-11-4 loss: 0.469695  [  160/  265]
train() client id: f_00001-11-5 loss: 0.545906  [  192/  265]
train() client id: f_00001-11-6 loss: 0.423290  [  224/  265]
train() client id: f_00001-11-7 loss: 0.556233  [  256/  265]
train() client id: f_00002-0-0 loss: 1.300599  [   32/  124]
train() client id: f_00002-0-1 loss: 1.214548  [   64/  124]
train() client id: f_00002-0-2 loss: 1.303672  [   96/  124]
train() client id: f_00002-1-0 loss: 1.255537  [   32/  124]
train() client id: f_00002-1-1 loss: 1.228744  [   64/  124]
train() client id: f_00002-1-2 loss: 1.199376  [   96/  124]
train() client id: f_00002-2-0 loss: 1.282760  [   32/  124]
train() client id: f_00002-2-1 loss: 1.164638  [   64/  124]
train() client id: f_00002-2-2 loss: 1.090728  [   96/  124]
train() client id: f_00002-3-0 loss: 1.085114  [   32/  124]
train() client id: f_00002-3-1 loss: 1.159134  [   64/  124]
train() client id: f_00002-3-2 loss: 1.192215  [   96/  124]
train() client id: f_00002-4-0 loss: 1.097772  [   32/  124]
train() client id: f_00002-4-1 loss: 1.143436  [   64/  124]
train() client id: f_00002-4-2 loss: 1.147416  [   96/  124]
train() client id: f_00002-5-0 loss: 1.108757  [   32/  124]
train() client id: f_00002-5-1 loss: 1.095225  [   64/  124]
train() client id: f_00002-5-2 loss: 1.088675  [   96/  124]
train() client id: f_00002-6-0 loss: 1.085348  [   32/  124]
train() client id: f_00002-6-1 loss: 1.102193  [   64/  124]
train() client id: f_00002-6-2 loss: 0.995741  [   96/  124]
train() client id: f_00002-7-0 loss: 0.982277  [   32/  124]
train() client id: f_00002-7-1 loss: 1.041557  [   64/  124]
train() client id: f_00002-7-2 loss: 1.124904  [   96/  124]
train() client id: f_00002-8-0 loss: 0.988716  [   32/  124]
train() client id: f_00002-8-1 loss: 1.094657  [   64/  124]
train() client id: f_00002-8-2 loss: 1.076498  [   96/  124]
train() client id: f_00002-9-0 loss: 1.051897  [   32/  124]
train() client id: f_00002-9-1 loss: 1.130646  [   64/  124]
train() client id: f_00002-9-2 loss: 0.951244  [   96/  124]
train() client id: f_00002-10-0 loss: 1.120850  [   32/  124]
train() client id: f_00002-10-1 loss: 1.030915  [   64/  124]
train() client id: f_00002-10-2 loss: 1.035872  [   96/  124]
train() client id: f_00002-11-0 loss: 1.056359  [   32/  124]
train() client id: f_00002-11-1 loss: 1.031095  [   64/  124]
train() client id: f_00002-11-2 loss: 0.989403  [   96/  124]
train() client id: f_00003-0-0 loss: 1.058334  [   32/   43]
train() client id: f_00003-1-0 loss: 0.993282  [   32/   43]
train() client id: f_00003-2-0 loss: 1.037849  [   32/   43]
train() client id: f_00003-3-0 loss: 1.023507  [   32/   43]
train() client id: f_00003-4-0 loss: 1.065299  [   32/   43]
train() client id: f_00003-5-0 loss: 1.000503  [   32/   43]
train() client id: f_00003-6-0 loss: 1.048783  [   32/   43]
train() client id: f_00003-7-0 loss: 0.990730  [   32/   43]
train() client id: f_00003-8-0 loss: 1.033028  [   32/   43]
train() client id: f_00003-9-0 loss: 0.989567  [   32/   43]
train() client id: f_00003-10-0 loss: 1.023857  [   32/   43]
train() client id: f_00003-11-0 loss: 1.117047  [   32/   43]
train() client id: f_00004-0-0 loss: 0.964884  [   32/  306]
train() client id: f_00004-0-1 loss: 0.881160  [   64/  306]
train() client id: f_00004-0-2 loss: 0.846138  [   96/  306]
train() client id: f_00004-0-3 loss: 0.929001  [  128/  306]
train() client id: f_00004-0-4 loss: 0.875980  [  160/  306]
train() client id: f_00004-0-5 loss: 1.015616  [  192/  306]
train() client id: f_00004-0-6 loss: 0.998755  [  224/  306]
train() client id: f_00004-0-7 loss: 0.889649  [  256/  306]
train() client id: f_00004-0-8 loss: 0.899301  [  288/  306]
train() client id: f_00004-1-0 loss: 0.961249  [   32/  306]
train() client id: f_00004-1-1 loss: 0.877507  [   64/  306]
train() client id: f_00004-1-2 loss: 0.951357  [   96/  306]
train() client id: f_00004-1-3 loss: 0.853316  [  128/  306]
train() client id: f_00004-1-4 loss: 1.021841  [  160/  306]
train() client id: f_00004-1-5 loss: 0.972648  [  192/  306]
train() client id: f_00004-1-6 loss: 0.949298  [  224/  306]
train() client id: f_00004-1-7 loss: 0.895418  [  256/  306]
train() client id: f_00004-1-8 loss: 0.880459  [  288/  306]
train() client id: f_00004-2-0 loss: 0.843861  [   32/  306]
train() client id: f_00004-2-1 loss: 0.893868  [   64/  306]
train() client id: f_00004-2-2 loss: 0.938882  [   96/  306]
train() client id: f_00004-2-3 loss: 0.916385  [  128/  306]
train() client id: f_00004-2-4 loss: 1.006474  [  160/  306]
train() client id: f_00004-2-5 loss: 0.956816  [  192/  306]
train() client id: f_00004-2-6 loss: 0.823708  [  224/  306]
train() client id: f_00004-2-7 loss: 0.957427  [  256/  306]
train() client id: f_00004-2-8 loss: 0.922954  [  288/  306]
train() client id: f_00004-3-0 loss: 0.909099  [   32/  306]
train() client id: f_00004-3-1 loss: 0.920692  [   64/  306]
train() client id: f_00004-3-2 loss: 0.945575  [   96/  306]
train() client id: f_00004-3-3 loss: 0.935375  [  128/  306]
train() client id: f_00004-3-4 loss: 0.959433  [  160/  306]
train() client id: f_00004-3-5 loss: 0.949501  [  192/  306]
train() client id: f_00004-3-6 loss: 0.920903  [  224/  306]
train() client id: f_00004-3-7 loss: 0.865325  [  256/  306]
train() client id: f_00004-3-8 loss: 0.905564  [  288/  306]
train() client id: f_00004-4-0 loss: 0.936772  [   32/  306]
train() client id: f_00004-4-1 loss: 0.880100  [   64/  306]
train() client id: f_00004-4-2 loss: 0.915100  [   96/  306]
train() client id: f_00004-4-3 loss: 0.888871  [  128/  306]
train() client id: f_00004-4-4 loss: 0.882116  [  160/  306]
train() client id: f_00004-4-5 loss: 0.959847  [  192/  306]
train() client id: f_00004-4-6 loss: 1.009390  [  224/  306]
train() client id: f_00004-4-7 loss: 0.996608  [  256/  306]
train() client id: f_00004-4-8 loss: 0.828509  [  288/  306]
train() client id: f_00004-5-0 loss: 0.904026  [   32/  306]
train() client id: f_00004-5-1 loss: 0.970723  [   64/  306]
train() client id: f_00004-5-2 loss: 0.855990  [   96/  306]
train() client id: f_00004-5-3 loss: 0.902467  [  128/  306]
train() client id: f_00004-5-4 loss: 0.932012  [  160/  306]
train() client id: f_00004-5-5 loss: 0.833713  [  192/  306]
train() client id: f_00004-5-6 loss: 0.964502  [  224/  306]
train() client id: f_00004-5-7 loss: 0.954091  [  256/  306]
train() client id: f_00004-5-8 loss: 0.883935  [  288/  306]
train() client id: f_00004-6-0 loss: 1.030092  [   32/  306]
train() client id: f_00004-6-1 loss: 0.783491  [   64/  306]
train() client id: f_00004-6-2 loss: 0.954651  [   96/  306]
train() client id: f_00004-6-3 loss: 0.878846  [  128/  306]
train() client id: f_00004-6-4 loss: 0.855997  [  160/  306]
train() client id: f_00004-6-5 loss: 0.911532  [  192/  306]
train() client id: f_00004-6-6 loss: 1.029522  [  224/  306]
train() client id: f_00004-6-7 loss: 1.003919  [  256/  306]
train() client id: f_00004-6-8 loss: 0.880016  [  288/  306]
train() client id: f_00004-7-0 loss: 0.859119  [   32/  306]
train() client id: f_00004-7-1 loss: 0.772468  [   64/  306]
train() client id: f_00004-7-2 loss: 0.834294  [   96/  306]
train() client id: f_00004-7-3 loss: 1.092295  [  128/  306]
train() client id: f_00004-7-4 loss: 0.841684  [  160/  306]
train() client id: f_00004-7-5 loss: 0.972523  [  192/  306]
train() client id: f_00004-7-6 loss: 0.947805  [  224/  306]
train() client id: f_00004-7-7 loss: 0.921431  [  256/  306]
train() client id: f_00004-7-8 loss: 0.991212  [  288/  306]
train() client id: f_00004-8-0 loss: 0.998477  [   32/  306]
train() client id: f_00004-8-1 loss: 0.903091  [   64/  306]
train() client id: f_00004-8-2 loss: 0.972569  [   96/  306]
train() client id: f_00004-8-3 loss: 0.899279  [  128/  306]
train() client id: f_00004-8-4 loss: 0.851974  [  160/  306]
train() client id: f_00004-8-5 loss: 1.065742  [  192/  306]
train() client id: f_00004-8-6 loss: 0.853055  [  224/  306]
train() client id: f_00004-8-7 loss: 0.886443  [  256/  306]
train() client id: f_00004-8-8 loss: 0.841953  [  288/  306]
train() client id: f_00004-9-0 loss: 0.945116  [   32/  306]
train() client id: f_00004-9-1 loss: 0.945642  [   64/  306]
train() client id: f_00004-9-2 loss: 0.821382  [   96/  306]
train() client id: f_00004-9-3 loss: 0.893731  [  128/  306]
train() client id: f_00004-9-4 loss: 1.022541  [  160/  306]
train() client id: f_00004-9-5 loss: 0.962807  [  192/  306]
train() client id: f_00004-9-6 loss: 0.879268  [  224/  306]
train() client id: f_00004-9-7 loss: 0.983792  [  256/  306]
train() client id: f_00004-9-8 loss: 0.883817  [  288/  306]
train() client id: f_00004-10-0 loss: 0.953002  [   32/  306]
train() client id: f_00004-10-1 loss: 0.850995  [   64/  306]
train() client id: f_00004-10-2 loss: 0.966930  [   96/  306]
train() client id: f_00004-10-3 loss: 0.858720  [  128/  306]
train() client id: f_00004-10-4 loss: 0.830676  [  160/  306]
train() client id: f_00004-10-5 loss: 0.939305  [  192/  306]
train() client id: f_00004-10-6 loss: 0.914804  [  224/  306]
train() client id: f_00004-10-7 loss: 0.970726  [  256/  306]
train() client id: f_00004-10-8 loss: 0.941779  [  288/  306]
train() client id: f_00004-11-0 loss: 0.946502  [   32/  306]
train() client id: f_00004-11-1 loss: 0.893485  [   64/  306]
train() client id: f_00004-11-2 loss: 0.961473  [   96/  306]
train() client id: f_00004-11-3 loss: 0.869207  [  128/  306]
train() client id: f_00004-11-4 loss: 0.902261  [  160/  306]
train() client id: f_00004-11-5 loss: 0.871631  [  192/  306]
train() client id: f_00004-11-6 loss: 1.008169  [  224/  306]
train() client id: f_00004-11-7 loss: 0.911163  [  256/  306]
train() client id: f_00004-11-8 loss: 0.927532  [  288/  306]
train() client id: f_00005-0-0 loss: 0.842964  [   32/  146]
train() client id: f_00005-0-1 loss: 0.812908  [   64/  146]
train() client id: f_00005-0-2 loss: 0.860918  [   96/  146]
train() client id: f_00005-0-3 loss: 0.787660  [  128/  146]
train() client id: f_00005-1-0 loss: 0.751717  [   32/  146]
train() client id: f_00005-1-1 loss: 0.943651  [   64/  146]
train() client id: f_00005-1-2 loss: 0.823919  [   96/  146]
train() client id: f_00005-1-3 loss: 0.741940  [  128/  146]
train() client id: f_00005-2-0 loss: 0.829477  [   32/  146]
train() client id: f_00005-2-1 loss: 0.780450  [   64/  146]
train() client id: f_00005-2-2 loss: 0.760657  [   96/  146]
train() client id: f_00005-2-3 loss: 0.824548  [  128/  146]
train() client id: f_00005-3-0 loss: 0.801509  [   32/  146]
train() client id: f_00005-3-1 loss: 0.720332  [   64/  146]
train() client id: f_00005-3-2 loss: 0.701234  [   96/  146]
train() client id: f_00005-3-3 loss: 0.976422  [  128/  146]
train() client id: f_00005-4-0 loss: 0.794637  [   32/  146]
train() client id: f_00005-4-1 loss: 0.899743  [   64/  146]
train() client id: f_00005-4-2 loss: 0.689566  [   96/  146]
train() client id: f_00005-4-3 loss: 0.767770  [  128/  146]
train() client id: f_00005-5-0 loss: 0.838328  [   32/  146]
train() client id: f_00005-5-1 loss: 0.755454  [   64/  146]
train() client id: f_00005-5-2 loss: 0.902853  [   96/  146]
train() client id: f_00005-5-3 loss: 0.767182  [  128/  146]
train() client id: f_00005-6-0 loss: 0.829140  [   32/  146]
train() client id: f_00005-6-1 loss: 0.745862  [   64/  146]
train() client id: f_00005-6-2 loss: 0.757578  [   96/  146]
train() client id: f_00005-6-3 loss: 0.811810  [  128/  146]
train() client id: f_00005-7-0 loss: 0.712490  [   32/  146]
train() client id: f_00005-7-1 loss: 1.083116  [   64/  146]
train() client id: f_00005-7-2 loss: 0.687915  [   96/  146]
train() client id: f_00005-7-3 loss: 0.683479  [  128/  146]
train() client id: f_00005-8-0 loss: 0.793522  [   32/  146]
train() client id: f_00005-8-1 loss: 0.730527  [   64/  146]
train() client id: f_00005-8-2 loss: 0.712382  [   96/  146]
train() client id: f_00005-8-3 loss: 0.785110  [  128/  146]
train() client id: f_00005-9-0 loss: 0.769290  [   32/  146]
train() client id: f_00005-9-1 loss: 0.803385  [   64/  146]
train() client id: f_00005-9-2 loss: 0.763282  [   96/  146]
train() client id: f_00005-9-3 loss: 0.765892  [  128/  146]
train() client id: f_00005-10-0 loss: 0.779867  [   32/  146]
train() client id: f_00005-10-1 loss: 0.895441  [   64/  146]
train() client id: f_00005-10-2 loss: 0.636038  [   96/  146]
train() client id: f_00005-10-3 loss: 0.915369  [  128/  146]
train() client id: f_00005-11-0 loss: 0.654647  [   32/  146]
train() client id: f_00005-11-1 loss: 0.818029  [   64/  146]
train() client id: f_00005-11-2 loss: 1.008704  [   96/  146]
train() client id: f_00005-11-3 loss: 0.649903  [  128/  146]
train() client id: f_00006-0-0 loss: 0.921533  [   32/   54]
train() client id: f_00006-1-0 loss: 0.891243  [   32/   54]
train() client id: f_00006-2-0 loss: 0.916141  [   32/   54]
train() client id: f_00006-3-0 loss: 0.882409  [   32/   54]
train() client id: f_00006-4-0 loss: 0.905641  [   32/   54]
train() client id: f_00006-5-0 loss: 0.871761  [   32/   54]
train() client id: f_00006-6-0 loss: 0.917774  [   32/   54]
train() client id: f_00006-7-0 loss: 0.901573  [   32/   54]
train() client id: f_00006-8-0 loss: 0.882114  [   32/   54]
train() client id: f_00006-9-0 loss: 0.927104  [   32/   54]
train() client id: f_00006-10-0 loss: 0.895435  [   32/   54]
train() client id: f_00006-11-0 loss: 0.907772  [   32/   54]
train() client id: f_00007-0-0 loss: 0.843712  [   32/  179]
train() client id: f_00007-0-1 loss: 0.717082  [   64/  179]
train() client id: f_00007-0-2 loss: 0.851018  [   96/  179]
train() client id: f_00007-0-3 loss: 0.807959  [  128/  179]
train() client id: f_00007-0-4 loss: 0.735973  [  160/  179]
train() client id: f_00007-1-0 loss: 0.682171  [   32/  179]
train() client id: f_00007-1-1 loss: 0.757732  [   64/  179]
train() client id: f_00007-1-2 loss: 0.735746  [   96/  179]
train() client id: f_00007-1-3 loss: 0.723290  [  128/  179]
train() client id: f_00007-1-4 loss: 0.837806  [  160/  179]
train() client id: f_00007-2-0 loss: 0.712726  [   32/  179]
train() client id: f_00007-2-1 loss: 0.649594  [   64/  179]
train() client id: f_00007-2-2 loss: 0.860453  [   96/  179]
train() client id: f_00007-2-3 loss: 0.730062  [  128/  179]
train() client id: f_00007-2-4 loss: 0.753850  [  160/  179]
train() client id: f_00007-3-0 loss: 0.681009  [   32/  179]
train() client id: f_00007-3-1 loss: 0.690042  [   64/  179]
train() client id: f_00007-3-2 loss: 0.685480  [   96/  179]
train() client id: f_00007-3-3 loss: 0.701258  [  128/  179]
train() client id: f_00007-3-4 loss: 0.797963  [  160/  179]
train() client id: f_00007-4-0 loss: 0.631969  [   32/  179]
train() client id: f_00007-4-1 loss: 0.721388  [   64/  179]
train() client id: f_00007-4-2 loss: 0.658161  [   96/  179]
train() client id: f_00007-4-3 loss: 0.845232  [  128/  179]
train() client id: f_00007-4-4 loss: 0.673624  [  160/  179]
train() client id: f_00007-5-0 loss: 0.721587  [   32/  179]
train() client id: f_00007-5-1 loss: 0.665142  [   64/  179]
train() client id: f_00007-5-2 loss: 0.651384  [   96/  179]
train() client id: f_00007-5-3 loss: 0.670551  [  128/  179]
train() client id: f_00007-5-4 loss: 0.705637  [  160/  179]
train() client id: f_00007-6-0 loss: 0.640015  [   32/  179]
train() client id: f_00007-6-1 loss: 0.594544  [   64/  179]
train() client id: f_00007-6-2 loss: 0.771487  [   96/  179]
train() client id: f_00007-6-3 loss: 0.778802  [  128/  179]
train() client id: f_00007-6-4 loss: 0.648751  [  160/  179]
train() client id: f_00007-7-0 loss: 0.629278  [   32/  179]
train() client id: f_00007-7-1 loss: 0.575513  [   64/  179]
train() client id: f_00007-7-2 loss: 0.631018  [   96/  179]
train() client id: f_00007-7-3 loss: 0.862940  [  128/  179]
train() client id: f_00007-7-4 loss: 0.640547  [  160/  179]
train() client id: f_00007-8-0 loss: 0.635378  [   32/  179]
train() client id: f_00007-8-1 loss: 0.706806  [   64/  179]
train() client id: f_00007-8-2 loss: 0.624267  [   96/  179]
train() client id: f_00007-8-3 loss: 0.629212  [  128/  179]
train() client id: f_00007-8-4 loss: 0.659670  [  160/  179]
train() client id: f_00007-9-0 loss: 0.693215  [   32/  179]
train() client id: f_00007-9-1 loss: 0.762824  [   64/  179]
train() client id: f_00007-9-2 loss: 0.633075  [   96/  179]
train() client id: f_00007-9-3 loss: 0.634017  [  128/  179]
train() client id: f_00007-9-4 loss: 0.626972  [  160/  179]
train() client id: f_00007-10-0 loss: 0.618709  [   32/  179]
train() client id: f_00007-10-1 loss: 0.550611  [   64/  179]
train() client id: f_00007-10-2 loss: 0.698538  [   96/  179]
train() client id: f_00007-10-3 loss: 0.778704  [  128/  179]
train() client id: f_00007-10-4 loss: 0.695575  [  160/  179]
train() client id: f_00007-11-0 loss: 0.625498  [   32/  179]
train() client id: f_00007-11-1 loss: 0.702024  [   64/  179]
train() client id: f_00007-11-2 loss: 0.607281  [   96/  179]
train() client id: f_00007-11-3 loss: 0.686967  [  128/  179]
train() client id: f_00007-11-4 loss: 0.639366  [  160/  179]
train() client id: f_00008-0-0 loss: 0.826403  [   32/  130]
train() client id: f_00008-0-1 loss: 0.841862  [   64/  130]
train() client id: f_00008-0-2 loss: 0.939902  [   96/  130]
train() client id: f_00008-0-3 loss: 0.869352  [  128/  130]
train() client id: f_00008-1-0 loss: 0.815544  [   32/  130]
train() client id: f_00008-1-1 loss: 0.842992  [   64/  130]
train() client id: f_00008-1-2 loss: 0.967259  [   96/  130]
train() client id: f_00008-1-3 loss: 0.809966  [  128/  130]
train() client id: f_00008-2-0 loss: 0.902776  [   32/  130]
train() client id: f_00008-2-1 loss: 0.786794  [   64/  130]
train() client id: f_00008-2-2 loss: 0.853260  [   96/  130]
train() client id: f_00008-2-3 loss: 0.926123  [  128/  130]
train() client id: f_00008-3-0 loss: 0.818437  [   32/  130]
train() client id: f_00008-3-1 loss: 0.869874  [   64/  130]
train() client id: f_00008-3-2 loss: 0.868544  [   96/  130]
train() client id: f_00008-3-3 loss: 0.910681  [  128/  130]
train() client id: f_00008-4-0 loss: 0.940184  [   32/  130]
train() client id: f_00008-4-1 loss: 0.873630  [   64/  130]
train() client id: f_00008-4-2 loss: 0.830895  [   96/  130]
train() client id: f_00008-4-3 loss: 0.819963  [  128/  130]
train() client id: f_00008-5-0 loss: 0.802877  [   32/  130]
train() client id: f_00008-5-1 loss: 0.901752  [   64/  130]
train() client id: f_00008-5-2 loss: 0.812539  [   96/  130]
train() client id: f_00008-5-3 loss: 0.955299  [  128/  130]
train() client id: f_00008-6-0 loss: 0.835984  [   32/  130]
train() client id: f_00008-6-1 loss: 0.950923  [   64/  130]
train() client id: f_00008-6-2 loss: 0.854006  [   96/  130]
train() client id: f_00008-6-3 loss: 0.821240  [  128/  130]
train() client id: f_00008-7-0 loss: 0.848222  [   32/  130]
train() client id: f_00008-7-1 loss: 0.874205  [   64/  130]
train() client id: f_00008-7-2 loss: 0.852246  [   96/  130]
train() client id: f_00008-7-3 loss: 0.858271  [  128/  130]
train() client id: f_00008-8-0 loss: 0.946104  [   32/  130]
train() client id: f_00008-8-1 loss: 0.822877  [   64/  130]
train() client id: f_00008-8-2 loss: 0.854506  [   96/  130]
train() client id: f_00008-8-3 loss: 0.815431  [  128/  130]
train() client id: f_00008-9-0 loss: 0.783732  [   32/  130]
train() client id: f_00008-9-1 loss: 0.912505  [   64/  130]
train() client id: f_00008-9-2 loss: 0.879655  [   96/  130]
train() client id: f_00008-9-3 loss: 0.865732  [  128/  130]
train() client id: f_00008-10-0 loss: 0.821276  [   32/  130]
train() client id: f_00008-10-1 loss: 1.014367  [   64/  130]
train() client id: f_00008-10-2 loss: 0.842277  [   96/  130]
train() client id: f_00008-10-3 loss: 0.795699  [  128/  130]
train() client id: f_00008-11-0 loss: 0.970999  [   32/  130]
train() client id: f_00008-11-1 loss: 0.830339  [   64/  130]
train() client id: f_00008-11-2 loss: 0.844909  [   96/  130]
train() client id: f_00008-11-3 loss: 0.760464  [  128/  130]
train() client id: f_00009-0-0 loss: 1.094060  [   32/  118]
train() client id: f_00009-0-1 loss: 1.063103  [   64/  118]
train() client id: f_00009-0-2 loss: 1.151374  [   96/  118]
train() client id: f_00009-1-0 loss: 1.078337  [   32/  118]
train() client id: f_00009-1-1 loss: 1.023100  [   64/  118]
train() client id: f_00009-1-2 loss: 1.018061  [   96/  118]
train() client id: f_00009-2-0 loss: 1.046715  [   32/  118]
train() client id: f_00009-2-1 loss: 0.968380  [   64/  118]
train() client id: f_00009-2-2 loss: 1.067852  [   96/  118]
train() client id: f_00009-3-0 loss: 1.001391  [   32/  118]
train() client id: f_00009-3-1 loss: 0.882422  [   64/  118]
train() client id: f_00009-3-2 loss: 1.076805  [   96/  118]
train() client id: f_00009-4-0 loss: 0.971123  [   32/  118]
train() client id: f_00009-4-1 loss: 1.020019  [   64/  118]
train() client id: f_00009-4-2 loss: 0.894389  [   96/  118]
train() client id: f_00009-5-0 loss: 0.977059  [   32/  118]
train() client id: f_00009-5-1 loss: 0.896442  [   64/  118]
train() client id: f_00009-5-2 loss: 0.922866  [   96/  118]
train() client id: f_00009-6-0 loss: 0.941580  [   32/  118]
train() client id: f_00009-6-1 loss: 0.867805  [   64/  118]
train() client id: f_00009-6-2 loss: 0.904257  [   96/  118]
train() client id: f_00009-7-0 loss: 0.845245  [   32/  118]
train() client id: f_00009-7-1 loss: 0.991792  [   64/  118]
train() client id: f_00009-7-2 loss: 0.885082  [   96/  118]
train() client id: f_00009-8-0 loss: 1.001126  [   32/  118]
train() client id: f_00009-8-1 loss: 0.907133  [   64/  118]
train() client id: f_00009-8-2 loss: 0.857556  [   96/  118]
train() client id: f_00009-9-0 loss: 0.923939  [   32/  118]
train() client id: f_00009-9-1 loss: 0.894904  [   64/  118]
train() client id: f_00009-9-2 loss: 0.916449  [   96/  118]
train() client id: f_00009-10-0 loss: 0.869329  [   32/  118]
train() client id: f_00009-10-1 loss: 1.006291  [   64/  118]
train() client id: f_00009-10-2 loss: 0.803373  [   96/  118]
train() client id: f_00009-11-0 loss: 0.864363  [   32/  118]
train() client id: f_00009-11-1 loss: 0.901826  [   64/  118]
train() client id: f_00009-11-2 loss: 0.869405  [   96/  118]
At round 6 accuracy: 0.6312997347480106
At round 6 training accuracy: 0.5761234071093226
At round 6 training loss: 0.9068601712691098
update_location
xs = -3.905658 4.200318 50.009024 18.811294 -4.020704 3.956410 -12.443192 3.675148 34.663977 2.939121 
ys = 42.587959 25.555839 1.320614 -12.455176 9.350187 -12.185849 -2.624984 -4.177652 17.569006 4.001482 
dists_uav = 108.761153 103.299291 111.815234 102.513395 100.516626 100.817400 100.805375 100.154678 107.285886 100.123176 
dists_bs = 216.347457 233.337334 284.242133 269.633568 238.062556 258.967061 240.780711 253.040391 262.189819 246.784976 
uav_gains = -100.911898 -100.352459 -101.212603 -100.269538 -100.055965 -100.088405 -100.087110 -100.016797 -100.763608 -100.013382 
bs_gains = -104.951309 -105.870618 -108.270336 -107.628730 -106.114410 -107.137906 -106.252467 -106.856375 -107.288302 -106.551983 
Round 7
-------------------------------
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.06798939 21.0326556   9.91312762  3.54368464 24.2561337  11.70105956
  4.40648092 14.22483624 10.44629039  9.49400071]
obj_prev = 119.08625877018483
eta_min = 5.793901722590025e-10	eta_max = 0.9200929959618319
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 27.695540866338916	eta = 0.909090909090909
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 47.81923698001299	eta = 0.5265195769323551
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 38.22376664223015	eta = 0.6586939654483899
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 36.504271658621605	eta = 0.6897210457833629
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 36.41961812780936	eta = 0.6913242290346582
af = 25.177764423944467	bf = 1.9475061075022868	zeta = 36.41939846351146	eta = 0.6913283987699586
eta = 0.6913283987699586
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [0.03046057 0.06406391 0.02997707 0.01039528 0.07397568 0.03529559
 0.01305453 0.04327336 0.03142758 0.02852657]
ene_total = [3.09168549 6.04664955 3.05847947 1.40079217 6.88392674 3.69460946
 1.61888686 4.14166343 3.37804549 3.10465979]
ti_comp = [0.28943977 0.27173208 0.28857988 0.29120492 0.27061872 0.2656134
 0.2916892  0.29187392 0.26482942 0.26854656]
ti_coms = [0.0655023  0.08320999 0.06636219 0.06373715 0.08432335 0.08932867
 0.06325287 0.06306815 0.09011265 0.08639551]
t_total = [29.64997063 29.64997063 29.64997063 29.64997063 29.64997063 29.64997063
 29.64997063 29.64997063 29.64997063 29.64997063]
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.10852063e-05 2.22555556e-04 2.02169483e-05 8.27924223e-07
 3.45486619e-04 3.89530298e-05 1.63426776e-06 5.94499024e-05
 2.76617870e-05 2.01182160e-05]
ene_total = [0.54893267 0.71368309 0.55604324 0.53249539 0.73325245 0.74945808
 0.52851731 0.53180388 0.75506382 0.72338265]
optimize_network iter = 0 obj = 6.3726325729552675
eta = 0.6913283987699586
freqs = [5.26198833e+07 1.17880646e+08 5.19389521e+07 1.78487289e+07
 1.36678791e+08 6.64416515e+07 2.23774684e+07 7.41302190e+07
 5.93355081e+07 5.31128927e+07]
eta_min = 0.6839075543866936	eta_max = 0.6913283987699287
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 0.058240867111970324	eta = 0.9090909090909091
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 21.478076657689165	eta = 0.0024651296143925966
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 2.271073836819586	eta = 0.023313307551114187
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 2.1977536965308997	eta = 0.02409107213089359
af = 0.052946242829063926	bf = 1.9475061075022868	zeta = 2.1977247993541873	eta = 0.0240913888966546
eta = 0.0240913888966546
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.12822560e-04 2.24635427e-03 2.04058838e-04 8.35661507e-06
 3.48715330e-03 3.93170615e-04 1.64954064e-05 6.00054856e-04
 2.79202975e-04 2.03062288e-04]
ene_total = [0.17868869 0.2792033  0.18072909 0.16862269 0.31492846 0.24640612
 0.16755818 0.18248854 0.24546632 0.23363341]
ti_comp = [0.29797302 0.28026532 0.29711312 0.29973816 0.27915197 0.27414664
 0.30022245 0.30040716 0.27336266 0.27707981]
ti_coms = [0.0655023  0.08320999 0.06636219 0.06373715 0.08432335 0.08932867
 0.06325287 0.06306815 0.09011265 0.08639551]
t_total = [29.64997063 29.64997063 29.64997063 29.64997063 29.64997063 29.64997063
 29.64997063 29.64997063 29.64997063 29.64997063]
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.16912899e-05 2.28100645e-04 2.07945251e-05 8.52018260e-07
 3.54005908e-04 3.98676209e-05 1.68198679e-06 6.11879833e-05
 2.83060505e-05 2.06045860e-05]
ene_total = [0.53609492 0.69738041 0.54303623 0.51999606 0.71673297 0.7319378
 0.5161133  0.5194606  0.73738988 0.7064396 ]
optimize_network iter = 1 obj = 6.224581758881931
eta = 0.6839075543866936
freqs = [5.26073161e+07 1.17632966e+08 5.19221168e+07 1.78475642e+07
 1.36374531e+08 6.62555801e+07 2.23770658e+07 7.41302190e+07
 5.91638860e+07 5.29821614e+07]
eta_min = 0.6839075543866936	eta_max = 0.6839075543866819
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 0.05801261670554691	eta = 0.9090909090909091
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 21.4778591118164	eta = 0.0024554934542136467
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 2.270009752860956	eta = 0.023232826375798606
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 2.196960837665713	eta = 0.024005317507445147
af = 0.052738742459588095	bf = 1.9475061075022868	zeta = 2.196932244064364	eta = 0.024005629942423928
eta = 0.024005629942423928
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.12924512e-04 2.23906549e-03 2.04121753e-04 8.36352164e-06
 3.47496787e-03 3.91345733e-04 1.65106003e-05 6.00629173e-04
 2.77855860e-04 2.02257286e-04]
ene_total = [0.17867568 0.27898621 0.18071488 0.16860805 0.31457886 0.24633625
 0.16754386 0.18248768 0.24540916 0.23359161]
ti_comp = [0.29797302 0.28026532 0.29711312 0.29973816 0.27915197 0.27414664
 0.30022245 0.30040716 0.27336266 0.27707981]
ti_coms = [0.0655023  0.08320999 0.06636219 0.06373715 0.08432335 0.08932867
 0.06325287 0.06306815 0.09011265 0.08639551]
t_total = [29.64997063 29.64997063 29.64997063 29.64997063 29.64997063 29.64997063
 29.64997063 29.64997063 29.64997063 29.64997063]
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.16912899e-05 2.28100645e-04 2.07945251e-05 8.52018260e-07
 3.54005908e-04 3.98676209e-05 1.68198679e-06 6.11879833e-05
 2.83060505e-05 2.06045860e-05]
ene_total = [0.53609492 0.69738041 0.54303623 0.51999606 0.71673297 0.7319378
 0.5161133  0.5194606  0.73738988 0.7064396 ]
optimize_network iter = 2 obj = 6.224581758881931
eta = 0.6839075543866936
freqs = [5.26073161e+07 1.17632966e+08 5.19221168e+07 1.78475642e+07
 1.36374531e+08 6.62555801e+07 2.23770658e+07 7.41302190e+07
 5.91638860e+07 5.29821614e+07]
Done!
ene_coms = [0.00655023 0.008321   0.00663622 0.00637372 0.00843233 0.00893287
 0.00632529 0.00630682 0.00901127 0.00863955]
ene_comp = [2.09225246e-05 2.20016485e-04 2.00575422e-05 8.21821712e-07
 3.41459515e-04 3.84546646e-05 1.62237516e-06 5.90194079e-05
 2.73028502e-05 1.98743347e-05]
ene_total = [0.00657115 0.00854102 0.00665628 0.00637454 0.00877379 0.00897132
 0.00632691 0.00636583 0.00903857 0.00865943]
At round 7 energy consumption: 0.07627883514893437
At round 7 eta: 0.6839075543866936
At round 7 a_n: 25.784781927469155
At round 7 local rounds: 12.440921153911754
At round 7 global rounds: 81.57354686993413
gradient difference: 0.5093237161636353
train() client id: f_00000-0-0 loss: 1.430601  [   32/  126]
train() client id: f_00000-0-1 loss: 1.302216  [   64/  126]
train() client id: f_00000-0-2 loss: 1.194716  [   96/  126]
train() client id: f_00000-1-0 loss: 1.153256  [   32/  126]
train() client id: f_00000-1-1 loss: 1.255559  [   64/  126]
train() client id: f_00000-1-2 loss: 1.215163  [   96/  126]
train() client id: f_00000-2-0 loss: 1.160930  [   32/  126]
train() client id: f_00000-2-1 loss: 1.128673  [   64/  126]
train() client id: f_00000-2-2 loss: 1.063594  [   96/  126]
train() client id: f_00000-3-0 loss: 1.070363  [   32/  126]
train() client id: f_00000-3-1 loss: 0.964266  [   64/  126]
train() client id: f_00000-3-2 loss: 1.053928  [   96/  126]
train() client id: f_00000-4-0 loss: 0.953510  [   32/  126]
train() client id: f_00000-4-1 loss: 1.024441  [   64/  126]
train() client id: f_00000-4-2 loss: 0.930531  [   96/  126]
train() client id: f_00000-5-0 loss: 0.993148  [   32/  126]
train() client id: f_00000-5-1 loss: 0.898401  [   64/  126]
train() client id: f_00000-5-2 loss: 0.910277  [   96/  126]
train() client id: f_00000-6-0 loss: 0.957725  [   32/  126]
train() client id: f_00000-6-1 loss: 0.973596  [   64/  126]
train() client id: f_00000-6-2 loss: 0.787400  [   96/  126]
train() client id: f_00000-7-0 loss: 0.915028  [   32/  126]
train() client id: f_00000-7-1 loss: 0.884889  [   64/  126]
train() client id: f_00000-7-2 loss: 0.854005  [   96/  126]
train() client id: f_00000-8-0 loss: 0.871189  [   32/  126]
train() client id: f_00000-8-1 loss: 0.900654  [   64/  126]
train() client id: f_00000-8-2 loss: 0.787892  [   96/  126]
train() client id: f_00000-9-0 loss: 0.861296  [   32/  126]
train() client id: f_00000-9-1 loss: 0.944388  [   64/  126]
train() client id: f_00000-9-2 loss: 0.792167  [   96/  126]
train() client id: f_00000-10-0 loss: 0.803814  [   32/  126]
train() client id: f_00000-10-1 loss: 0.779153  [   64/  126]
train() client id: f_00000-10-2 loss: 0.820426  [   96/  126]
train() client id: f_00000-11-0 loss: 0.866950  [   32/  126]
train() client id: f_00000-11-1 loss: 0.956288  [   64/  126]
train() client id: f_00000-11-2 loss: 0.777434  [   96/  126]
train() client id: f_00001-0-0 loss: 0.696445  [   32/  265]
train() client id: f_00001-0-1 loss: 0.488879  [   64/  265]
train() client id: f_00001-0-2 loss: 0.632768  [   96/  265]
train() client id: f_00001-0-3 loss: 0.530052  [  128/  265]
train() client id: f_00001-0-4 loss: 0.523278  [  160/  265]
train() client id: f_00001-0-5 loss: 0.595517  [  192/  265]
train() client id: f_00001-0-6 loss: 0.457790  [  224/  265]
train() client id: f_00001-0-7 loss: 0.460942  [  256/  265]
train() client id: f_00001-1-0 loss: 0.504818  [   32/  265]
train() client id: f_00001-1-1 loss: 0.512434  [   64/  265]
train() client id: f_00001-1-2 loss: 0.515076  [   96/  265]
train() client id: f_00001-1-3 loss: 0.486524  [  128/  265]
train() client id: f_00001-1-4 loss: 0.550990  [  160/  265]
train() client id: f_00001-1-5 loss: 0.473527  [  192/  265]
train() client id: f_00001-1-6 loss: 0.531562  [  224/  265]
train() client id: f_00001-1-7 loss: 0.626137  [  256/  265]
train() client id: f_00001-2-0 loss: 0.603896  [   32/  265]
train() client id: f_00001-2-1 loss: 0.585025  [   64/  265]
train() client id: f_00001-2-2 loss: 0.531826  [   96/  265]
train() client id: f_00001-2-3 loss: 0.572401  [  128/  265]
train() client id: f_00001-2-4 loss: 0.439488  [  160/  265]
train() client id: f_00001-2-5 loss: 0.430065  [  192/  265]
train() client id: f_00001-2-6 loss: 0.482318  [  224/  265]
train() client id: f_00001-2-7 loss: 0.406121  [  256/  265]
train() client id: f_00001-3-0 loss: 0.593887  [   32/  265]
train() client id: f_00001-3-1 loss: 0.425890  [   64/  265]
train() client id: f_00001-3-2 loss: 0.504148  [   96/  265]
train() client id: f_00001-3-3 loss: 0.412287  [  128/  265]
train() client id: f_00001-3-4 loss: 0.426377  [  160/  265]
train() client id: f_00001-3-5 loss: 0.664777  [  192/  265]
train() client id: f_00001-3-6 loss: 0.401849  [  224/  265]
train() client id: f_00001-3-7 loss: 0.482827  [  256/  265]
train() client id: f_00001-4-0 loss: 0.468199  [   32/  265]
train() client id: f_00001-4-1 loss: 0.498330  [   64/  265]
train() client id: f_00001-4-2 loss: 0.576491  [   96/  265]
train() client id: f_00001-4-3 loss: 0.472028  [  128/  265]
train() client id: f_00001-4-4 loss: 0.635832  [  160/  265]
train() client id: f_00001-4-5 loss: 0.405847  [  192/  265]
train() client id: f_00001-4-6 loss: 0.448534  [  224/  265]
train() client id: f_00001-4-7 loss: 0.393554  [  256/  265]
train() client id: f_00001-5-0 loss: 0.567310  [   32/  265]
train() client id: f_00001-5-1 loss: 0.456893  [   64/  265]
train() client id: f_00001-5-2 loss: 0.499852  [   96/  265]
train() client id: f_00001-5-3 loss: 0.453448  [  128/  265]
train() client id: f_00001-5-4 loss: 0.438418  [  160/  265]
train() client id: f_00001-5-5 loss: 0.578331  [  192/  265]
train() client id: f_00001-5-6 loss: 0.393530  [  224/  265]
train() client id: f_00001-5-7 loss: 0.466708  [  256/  265]
train() client id: f_00001-6-0 loss: 0.403398  [   32/  265]
train() client id: f_00001-6-1 loss: 0.435499  [   64/  265]
train() client id: f_00001-6-2 loss: 0.501222  [   96/  265]
train() client id: f_00001-6-3 loss: 0.502581  [  128/  265]
train() client id: f_00001-6-4 loss: 0.391327  [  160/  265]
train() client id: f_00001-6-5 loss: 0.502519  [  192/  265]
train() client id: f_00001-6-6 loss: 0.583350  [  224/  265]
train() client id: f_00001-6-7 loss: 0.495183  [  256/  265]
train() client id: f_00001-7-0 loss: 0.446137  [   32/  265]
train() client id: f_00001-7-1 loss: 0.592297  [   64/  265]
train() client id: f_00001-7-2 loss: 0.515757  [   96/  265]
train() client id: f_00001-7-3 loss: 0.432842  [  128/  265]
train() client id: f_00001-7-4 loss: 0.479809  [  160/  265]
train() client id: f_00001-7-5 loss: 0.545609  [  192/  265]
train() client id: f_00001-7-6 loss: 0.391611  [  224/  265]
train() client id: f_00001-7-7 loss: 0.381745  [  256/  265]
train() client id: f_00001-8-0 loss: 0.449165  [   32/  265]
train() client id: f_00001-8-1 loss: 0.539140  [   64/  265]
train() client id: f_00001-8-2 loss: 0.542745  [   96/  265]
train() client id: f_00001-8-3 loss: 0.430852  [  128/  265]
train() client id: f_00001-8-4 loss: 0.514042  [  160/  265]
train() client id: f_00001-8-5 loss: 0.500821  [  192/  265]
train() client id: f_00001-8-6 loss: 0.362537  [  224/  265]
train() client id: f_00001-8-7 loss: 0.423578  [  256/  265]
train() client id: f_00001-9-0 loss: 0.483185  [   32/  265]
train() client id: f_00001-9-1 loss: 0.487551  [   64/  265]
train() client id: f_00001-9-2 loss: 0.426433  [   96/  265]
train() client id: f_00001-9-3 loss: 0.447670  [  128/  265]
train() client id: f_00001-9-4 loss: 0.475424  [  160/  265]
train() client id: f_00001-9-5 loss: 0.487048  [  192/  265]
train() client id: f_00001-9-6 loss: 0.376002  [  224/  265]
train() client id: f_00001-9-7 loss: 0.516694  [  256/  265]
train() client id: f_00001-10-0 loss: 0.507141  [   32/  265]
train() client id: f_00001-10-1 loss: 0.431663  [   64/  265]
train() client id: f_00001-10-2 loss: 0.457515  [   96/  265]
train() client id: f_00001-10-3 loss: 0.399708  [  128/  265]
train() client id: f_00001-10-4 loss: 0.442542  [  160/  265]
train() client id: f_00001-10-5 loss: 0.382522  [  192/  265]
train() client id: f_00001-10-6 loss: 0.506776  [  224/  265]
train() client id: f_00001-10-7 loss: 0.620167  [  256/  265]
train() client id: f_00001-11-0 loss: 0.448856  [   32/  265]
train() client id: f_00001-11-1 loss: 0.424350  [   64/  265]
train() client id: f_00001-11-2 loss: 0.356116  [   96/  265]
train() client id: f_00001-11-3 loss: 0.420247  [  128/  265]
train() client id: f_00001-11-4 loss: 0.456098  [  160/  265]
train() client id: f_00001-11-5 loss: 0.375880  [  192/  265]
train() client id: f_00001-11-6 loss: 0.502636  [  224/  265]
train() client id: f_00001-11-7 loss: 0.699568  [  256/  265]
train() client id: f_00002-0-0 loss: 1.253051  [   32/  124]
train() client id: f_00002-0-1 loss: 1.157774  [   64/  124]
train() client id: f_00002-0-2 loss: 1.184324  [   96/  124]
train() client id: f_00002-1-0 loss: 1.171260  [   32/  124]
train() client id: f_00002-1-1 loss: 1.215691  [   64/  124]
train() client id: f_00002-1-2 loss: 1.118380  [   96/  124]
train() client id: f_00002-2-0 loss: 1.191880  [   32/  124]
train() client id: f_00002-2-1 loss: 1.154594  [   64/  124]
train() client id: f_00002-2-2 loss: 1.061773  [   96/  124]
train() client id: f_00002-3-0 loss: 1.093026  [   32/  124]
train() client id: f_00002-3-1 loss: 1.171788  [   64/  124]
train() client id: f_00002-3-2 loss: 1.135586  [   96/  124]
train() client id: f_00002-4-0 loss: 1.114441  [   32/  124]
train() client id: f_00002-4-1 loss: 1.141004  [   64/  124]
train() client id: f_00002-4-2 loss: 1.102972  [   96/  124]
train() client id: f_00002-5-0 loss: 0.995085  [   32/  124]
train() client id: f_00002-5-1 loss: 1.119462  [   64/  124]
train() client id: f_00002-5-2 loss: 1.162825  [   96/  124]
train() client id: f_00002-6-0 loss: 1.021093  [   32/  124]
train() client id: f_00002-6-1 loss: 1.058224  [   64/  124]
train() client id: f_00002-6-2 loss: 1.151324  [   96/  124]
train() client id: f_00002-7-0 loss: 1.090916  [   32/  124]
train() client id: f_00002-7-1 loss: 1.120862  [   64/  124]
train() client id: f_00002-7-2 loss: 1.042501  [   96/  124]
train() client id: f_00002-8-0 loss: 1.134901  [   32/  124]
train() client id: f_00002-8-1 loss: 1.024420  [   64/  124]
train() client id: f_00002-8-2 loss: 1.006498  [   96/  124]
train() client id: f_00002-9-0 loss: 1.126771  [   32/  124]
train() client id: f_00002-9-1 loss: 1.107021  [   64/  124]
train() client id: f_00002-9-2 loss: 1.039569  [   96/  124]
train() client id: f_00002-10-0 loss: 1.127331  [   32/  124]
train() client id: f_00002-10-1 loss: 1.034912  [   64/  124]
train() client id: f_00002-10-2 loss: 1.035048  [   96/  124]
train() client id: f_00002-11-0 loss: 1.112637  [   32/  124]
train() client id: f_00002-11-1 loss: 1.080508  [   64/  124]
train() client id: f_00002-11-2 loss: 1.073019  [   96/  124]
train() client id: f_00003-0-0 loss: 0.926568  [   32/   43]
train() client id: f_00003-1-0 loss: 0.924111  [   32/   43]
train() client id: f_00003-2-0 loss: 0.847758  [   32/   43]
train() client id: f_00003-3-0 loss: 0.933752  [   32/   43]
train() client id: f_00003-4-0 loss: 0.944241  [   32/   43]
train() client id: f_00003-5-0 loss: 0.913047  [   32/   43]
train() client id: f_00003-6-0 loss: 0.850634  [   32/   43]
train() client id: f_00003-7-0 loss: 0.903457  [   32/   43]
train() client id: f_00003-8-0 loss: 0.903502  [   32/   43]
train() client id: f_00003-9-0 loss: 0.889883  [   32/   43]
train() client id: f_00003-10-0 loss: 0.905323  [   32/   43]
train() client id: f_00003-11-0 loss: 0.931673  [   32/   43]
train() client id: f_00004-0-0 loss: 0.943915  [   32/  306]
train() client id: f_00004-0-1 loss: 0.988748  [   64/  306]
train() client id: f_00004-0-2 loss: 1.051898  [   96/  306]
train() client id: f_00004-0-3 loss: 0.835585  [  128/  306]
train() client id: f_00004-0-4 loss: 0.848469  [  160/  306]
train() client id: f_00004-0-5 loss: 0.902288  [  192/  306]
train() client id: f_00004-0-6 loss: 0.833040  [  224/  306]
train() client id: f_00004-0-7 loss: 0.866795  [  256/  306]
train() client id: f_00004-0-8 loss: 0.990276  [  288/  306]
train() client id: f_00004-1-0 loss: 0.860219  [   32/  306]
train() client id: f_00004-1-1 loss: 0.935563  [   64/  306]
train() client id: f_00004-1-2 loss: 0.805137  [   96/  306]
train() client id: f_00004-1-3 loss: 0.896068  [  128/  306]
train() client id: f_00004-1-4 loss: 0.933639  [  160/  306]
train() client id: f_00004-1-5 loss: 0.970100  [  192/  306]
train() client id: f_00004-1-6 loss: 0.934661  [  224/  306]
train() client id: f_00004-1-7 loss: 0.996203  [  256/  306]
train() client id: f_00004-1-8 loss: 0.937451  [  288/  306]
train() client id: f_00004-2-0 loss: 0.943999  [   32/  306]
train() client id: f_00004-2-1 loss: 0.854304  [   64/  306]
train() client id: f_00004-2-2 loss: 1.034447  [   96/  306]
train() client id: f_00004-2-3 loss: 0.941128  [  128/  306]
train() client id: f_00004-2-4 loss: 0.923060  [  160/  306]
train() client id: f_00004-2-5 loss: 0.935681  [  192/  306]
train() client id: f_00004-2-6 loss: 0.861370  [  224/  306]
train() client id: f_00004-2-7 loss: 0.913816  [  256/  306]
train() client id: f_00004-2-8 loss: 0.863539  [  288/  306]
train() client id: f_00004-3-0 loss: 0.972016  [   32/  306]
train() client id: f_00004-3-1 loss: 0.999492  [   64/  306]
train() client id: f_00004-3-2 loss: 0.796605  [   96/  306]
train() client id: f_00004-3-3 loss: 0.844798  [  128/  306]
train() client id: f_00004-3-4 loss: 0.883336  [  160/  306]
train() client id: f_00004-3-5 loss: 0.983174  [  192/  306]
train() client id: f_00004-3-6 loss: 1.044339  [  224/  306]
train() client id: f_00004-3-7 loss: 0.817659  [  256/  306]
train() client id: f_00004-3-8 loss: 0.867501  [  288/  306]
train() client id: f_00004-4-0 loss: 0.902353  [   32/  306]
train() client id: f_00004-4-1 loss: 0.843499  [   64/  306]
train() client id: f_00004-4-2 loss: 0.864036  [   96/  306]
train() client id: f_00004-4-3 loss: 0.970272  [  128/  306]
train() client id: f_00004-4-4 loss: 0.957589  [  160/  306]
train() client id: f_00004-4-5 loss: 0.965274  [  192/  306]
train() client id: f_00004-4-6 loss: 1.001367  [  224/  306]
train() client id: f_00004-4-7 loss: 0.853037  [  256/  306]
train() client id: f_00004-4-8 loss: 0.898885  [  288/  306]
train() client id: f_00004-5-0 loss: 0.905957  [   32/  306]
train() client id: f_00004-5-1 loss: 0.941089  [   64/  306]
train() client id: f_00004-5-2 loss: 0.919586  [   96/  306]
train() client id: f_00004-5-3 loss: 0.880377  [  128/  306]
train() client id: f_00004-5-4 loss: 0.979780  [  160/  306]
train() client id: f_00004-5-5 loss: 0.951984  [  192/  306]
train() client id: f_00004-5-6 loss: 0.882802  [  224/  306]
train() client id: f_00004-5-7 loss: 0.848700  [  256/  306]
train() client id: f_00004-5-8 loss: 0.926310  [  288/  306]
train() client id: f_00004-6-0 loss: 0.872143  [   32/  306]
train() client id: f_00004-6-1 loss: 0.939777  [   64/  306]
train() client id: f_00004-6-2 loss: 0.946315  [   96/  306]
train() client id: f_00004-6-3 loss: 0.897612  [  128/  306]
train() client id: f_00004-6-4 loss: 0.949181  [  160/  306]
train() client id: f_00004-6-5 loss: 0.888053  [  192/  306]
train() client id: f_00004-6-6 loss: 0.907271  [  224/  306]
train() client id: f_00004-6-7 loss: 0.983827  [  256/  306]
train() client id: f_00004-6-8 loss: 0.873660  [  288/  306]
train() client id: f_00004-7-0 loss: 0.983697  [   32/  306]
train() client id: f_00004-7-1 loss: 0.856909  [   64/  306]
train() client id: f_00004-7-2 loss: 0.835664  [   96/  306]
train() client id: f_00004-7-3 loss: 0.904583  [  128/  306]
train() client id: f_00004-7-4 loss: 1.007473  [  160/  306]
train() client id: f_00004-7-5 loss: 0.937585  [  192/  306]
train() client id: f_00004-7-6 loss: 0.950141  [  224/  306]
train() client id: f_00004-7-7 loss: 0.868457  [  256/  306]
train() client id: f_00004-7-8 loss: 0.941125  [  288/  306]
train() client id: f_00004-8-0 loss: 0.788070  [   32/  306]
train() client id: f_00004-8-1 loss: 0.905383  [   64/  306]
train() client id: f_00004-8-2 loss: 0.963856  [   96/  306]
train() client id: f_00004-8-3 loss: 0.902870  [  128/  306]
train() client id: f_00004-8-4 loss: 0.983016  [  160/  306]
train() client id: f_00004-8-5 loss: 0.955606  [  192/  306]
train() client id: f_00004-8-6 loss: 0.943564  [  224/  306]
train() client id: f_00004-8-7 loss: 0.920559  [  256/  306]
train() client id: f_00004-8-8 loss: 0.937370  [  288/  306]
train() client id: f_00004-9-0 loss: 0.914603  [   32/  306]
train() client id: f_00004-9-1 loss: 0.970089  [   64/  306]
train() client id: f_00004-9-2 loss: 0.904842  [   96/  306]
train() client id: f_00004-9-3 loss: 1.063947  [  128/  306]
train() client id: f_00004-9-4 loss: 0.969697  [  160/  306]
train() client id: f_00004-9-5 loss: 0.905687  [  192/  306]
train() client id: f_00004-9-6 loss: 0.780512  [  224/  306]
train() client id: f_00004-9-7 loss: 0.997425  [  256/  306]
train() client id: f_00004-9-8 loss: 0.901208  [  288/  306]
train() client id: f_00004-10-0 loss: 0.930772  [   32/  306]
train() client id: f_00004-10-1 loss: 0.931344  [   64/  306]
train() client id: f_00004-10-2 loss: 0.887556  [   96/  306]
train() client id: f_00004-10-3 loss: 0.897588  [  128/  306]
train() client id: f_00004-10-4 loss: 0.884287  [  160/  306]
train() client id: f_00004-10-5 loss: 0.976287  [  192/  306]
train() client id: f_00004-10-6 loss: 0.939341  [  224/  306]
train() client id: f_00004-10-7 loss: 0.888461  [  256/  306]
train() client id: f_00004-10-8 loss: 0.922099  [  288/  306]
train() client id: f_00004-11-0 loss: 1.065123  [   32/  306]
train() client id: f_00004-11-1 loss: 0.937825  [   64/  306]
train() client id: f_00004-11-2 loss: 1.008433  [   96/  306]
train() client id: f_00004-11-3 loss: 0.954253  [  128/  306]
train() client id: f_00004-11-4 loss: 0.878835  [  160/  306]
train() client id: f_00004-11-5 loss: 0.844793  [  192/  306]
train() client id: f_00004-11-6 loss: 0.812501  [  224/  306]
train() client id: f_00004-11-7 loss: 0.871530  [  256/  306]
train() client id: f_00004-11-8 loss: 0.990661  [  288/  306]
train() client id: f_00005-0-0 loss: 1.061966  [   32/  146]
train() client id: f_00005-0-1 loss: 0.857572  [   64/  146]
train() client id: f_00005-0-2 loss: 0.904884  [   96/  146]
train() client id: f_00005-0-3 loss: 0.908792  [  128/  146]
train() client id: f_00005-1-0 loss: 0.921468  [   32/  146]
train() client id: f_00005-1-1 loss: 0.963901  [   64/  146]
train() client id: f_00005-1-2 loss: 1.044571  [   96/  146]
train() client id: f_00005-1-3 loss: 0.818170  [  128/  146]
train() client id: f_00005-2-0 loss: 0.913253  [   32/  146]
train() client id: f_00005-2-1 loss: 0.936221  [   64/  146]
train() client id: f_00005-2-2 loss: 0.899119  [   96/  146]
train() client id: f_00005-2-3 loss: 0.927576  [  128/  146]
train() client id: f_00005-3-0 loss: 0.781733  [   32/  146]
train() client id: f_00005-3-1 loss: 0.829588  [   64/  146]
train() client id: f_00005-3-2 loss: 0.805768  [   96/  146]
train() client id: f_00005-3-3 loss: 1.103358  [  128/  146]
train() client id: f_00005-4-0 loss: 0.828951  [   32/  146]
train() client id: f_00005-4-1 loss: 0.947004  [   64/  146]
train() client id: f_00005-4-2 loss: 0.858840  [   96/  146]
train() client id: f_00005-4-3 loss: 1.057523  [  128/  146]
train() client id: f_00005-5-0 loss: 1.088248  [   32/  146]
train() client id: f_00005-5-1 loss: 0.925568  [   64/  146]
train() client id: f_00005-5-2 loss: 0.971428  [   96/  146]
train() client id: f_00005-5-3 loss: 0.691905  [  128/  146]
train() client id: f_00005-6-0 loss: 0.962692  [   32/  146]
train() client id: f_00005-6-1 loss: 0.823710  [   64/  146]
train() client id: f_00005-6-2 loss: 1.028354  [   96/  146]
train() client id: f_00005-6-3 loss: 0.921759  [  128/  146]
train() client id: f_00005-7-0 loss: 0.958676  [   32/  146]
train() client id: f_00005-7-1 loss: 0.935851  [   64/  146]
train() client id: f_00005-7-2 loss: 0.885978  [   96/  146]
train() client id: f_00005-7-3 loss: 0.916824  [  128/  146]
train() client id: f_00005-8-0 loss: 0.918593  [   32/  146]
train() client id: f_00005-8-1 loss: 0.824751  [   64/  146]
train() client id: f_00005-8-2 loss: 0.874812  [   96/  146]
train() client id: f_00005-8-3 loss: 0.921434  [  128/  146]
train() client id: f_00005-9-0 loss: 0.816475  [   32/  146]
train() client id: f_00005-9-1 loss: 0.862716  [   64/  146]
train() client id: f_00005-9-2 loss: 1.056527  [   96/  146]
train() client id: f_00005-9-3 loss: 0.813563  [  128/  146]
train() client id: f_00005-10-0 loss: 0.912760  [   32/  146]
train() client id: f_00005-10-1 loss: 1.061017  [   64/  146]
train() client id: f_00005-10-2 loss: 0.904427  [   96/  146]
train() client id: f_00005-10-3 loss: 0.815232  [  128/  146]
train() client id: f_00005-11-0 loss: 0.820801  [   32/  146]
train() client id: f_00005-11-1 loss: 0.952451  [   64/  146]
train() client id: f_00005-11-2 loss: 1.056677  [   96/  146]
train() client id: f_00005-11-3 loss: 0.923745  [  128/  146]
train() client id: f_00006-0-0 loss: 0.847927  [   32/   54]
train() client id: f_00006-1-0 loss: 0.858137  [   32/   54]
train() client id: f_00006-2-0 loss: 0.855334  [   32/   54]
train() client id: f_00006-3-0 loss: 0.868473  [   32/   54]
train() client id: f_00006-4-0 loss: 0.849145  [   32/   54]
train() client id: f_00006-5-0 loss: 0.874091  [   32/   54]
train() client id: f_00006-6-0 loss: 0.829857  [   32/   54]
train() client id: f_00006-7-0 loss: 0.876014  [   32/   54]
train() client id: f_00006-8-0 loss: 0.872401  [   32/   54]
train() client id: f_00006-9-0 loss: 0.870196  [   32/   54]
train() client id: f_00006-10-0 loss: 0.868447  [   32/   54]
train() client id: f_00006-11-0 loss: 0.874994  [   32/   54]
train() client id: f_00007-0-0 loss: 0.856987  [   32/  179]
train() client id: f_00007-0-1 loss: 0.782198  [   64/  179]
train() client id: f_00007-0-2 loss: 0.803910  [   96/  179]
train() client id: f_00007-0-3 loss: 0.766305  [  128/  179]
train() client id: f_00007-0-4 loss: 0.789019  [  160/  179]
train() client id: f_00007-1-0 loss: 0.816724  [   32/  179]
train() client id: f_00007-1-1 loss: 0.700157  [   64/  179]
train() client id: f_00007-1-2 loss: 0.686520  [   96/  179]
train() client id: f_00007-1-3 loss: 0.805596  [  128/  179]
train() client id: f_00007-1-4 loss: 0.745911  [  160/  179]
train() client id: f_00007-2-0 loss: 0.673863  [   32/  179]
train() client id: f_00007-2-1 loss: 0.725863  [   64/  179]
train() client id: f_00007-2-2 loss: 0.665497  [   96/  179]
train() client id: f_00007-2-3 loss: 0.786128  [  128/  179]
train() client id: f_00007-2-4 loss: 0.887984  [  160/  179]
train() client id: f_00007-3-0 loss: 0.651426  [   32/  179]
train() client id: f_00007-3-1 loss: 0.768972  [   64/  179]
train() client id: f_00007-3-2 loss: 0.718326  [   96/  179]
train() client id: f_00007-3-3 loss: 0.768211  [  128/  179]
train() client id: f_00007-3-4 loss: 0.827966  [  160/  179]
train() client id: f_00007-4-0 loss: 0.714324  [   32/  179]
train() client id: f_00007-4-1 loss: 0.783454  [   64/  179]
train() client id: f_00007-4-2 loss: 0.688647  [   96/  179]
train() client id: f_00007-4-3 loss: 0.622249  [  128/  179]
train() client id: f_00007-4-4 loss: 0.804747  [  160/  179]
train() client id: f_00007-5-0 loss: 0.685834  [   32/  179]
train() client id: f_00007-5-1 loss: 0.800281  [   64/  179]
train() client id: f_00007-5-2 loss: 0.623087  [   96/  179]
train() client id: f_00007-5-3 loss: 0.620044  [  128/  179]
train() client id: f_00007-5-4 loss: 0.760636  [  160/  179]
train() client id: f_00007-6-0 loss: 0.616746  [   32/  179]
train() client id: f_00007-6-1 loss: 0.827655  [   64/  179]
train() client id: f_00007-6-2 loss: 0.690913  [   96/  179]
train() client id: f_00007-6-3 loss: 0.670123  [  128/  179]
train() client id: f_00007-6-4 loss: 0.676932  [  160/  179]
train() client id: f_00007-7-0 loss: 0.747422  [   32/  179]
train() client id: f_00007-7-1 loss: 0.737004  [   64/  179]
train() client id: f_00007-7-2 loss: 0.669257  [   96/  179]
train() client id: f_00007-7-3 loss: 0.741538  [  128/  179]
train() client id: f_00007-7-4 loss: 0.607496  [  160/  179]
train() client id: f_00007-8-0 loss: 0.598849  [   32/  179]
train() client id: f_00007-8-1 loss: 0.784429  [   64/  179]
train() client id: f_00007-8-2 loss: 0.679893  [   96/  179]
train() client id: f_00007-8-3 loss: 0.729143  [  128/  179]
train() client id: f_00007-8-4 loss: 0.736092  [  160/  179]
train() client id: f_00007-9-0 loss: 0.608273  [   32/  179]
train() client id: f_00007-9-1 loss: 0.756097  [   64/  179]
train() client id: f_00007-9-2 loss: 0.663839  [   96/  179]
train() client id: f_00007-9-3 loss: 0.681107  [  128/  179]
train() client id: f_00007-9-4 loss: 0.875532  [  160/  179]
train() client id: f_00007-10-0 loss: 0.751834  [   32/  179]
train() client id: f_00007-10-1 loss: 0.745193  [   64/  179]
train() client id: f_00007-10-2 loss: 0.681750  [   96/  179]
train() client id: f_00007-10-3 loss: 0.721013  [  128/  179]
train() client id: f_00007-10-4 loss: 0.671574  [  160/  179]
train() client id: f_00007-11-0 loss: 0.679258  [   32/  179]
train() client id: f_00007-11-1 loss: 0.659479  [   64/  179]
train() client id: f_00007-11-2 loss: 0.742201  [   96/  179]
train() client id: f_00007-11-3 loss: 0.837203  [  128/  179]
train() client id: f_00007-11-4 loss: 0.668843  [  160/  179]
train() client id: f_00008-0-0 loss: 0.951247  [   32/  130]
train() client id: f_00008-0-1 loss: 1.004620  [   64/  130]
train() client id: f_00008-0-2 loss: 0.909816  [   96/  130]
train() client id: f_00008-0-3 loss: 0.918351  [  128/  130]
train() client id: f_00008-1-0 loss: 0.822452  [   32/  130]
train() client id: f_00008-1-1 loss: 1.038380  [   64/  130]
train() client id: f_00008-1-2 loss: 0.950664  [   96/  130]
train() client id: f_00008-1-3 loss: 0.972429  [  128/  130]
train() client id: f_00008-2-0 loss: 0.992246  [   32/  130]
train() client id: f_00008-2-1 loss: 0.866046  [   64/  130]
train() client id: f_00008-2-2 loss: 0.945301  [   96/  130]
train() client id: f_00008-2-3 loss: 0.958227  [  128/  130]
train() client id: f_00008-3-0 loss: 0.932676  [   32/  130]
train() client id: f_00008-3-1 loss: 0.854476  [   64/  130]
train() client id: f_00008-3-2 loss: 1.026577  [   96/  130]
train() client id: f_00008-3-3 loss: 0.973126  [  128/  130]
train() client id: f_00008-4-0 loss: 0.941515  [   32/  130]
train() client id: f_00008-4-1 loss: 0.972215  [   64/  130]
train() client id: f_00008-4-2 loss: 0.930450  [   96/  130]
train() client id: f_00008-4-3 loss: 0.910064  [  128/  130]
train() client id: f_00008-5-0 loss: 0.875046  [   32/  130]
train() client id: f_00008-5-1 loss: 1.051422  [   64/  130]
train() client id: f_00008-5-2 loss: 0.936507  [   96/  130]
train() client id: f_00008-5-3 loss: 0.927352  [  128/  130]
train() client id: f_00008-6-0 loss: 0.831653  [   32/  130]
train() client id: f_00008-6-1 loss: 0.979019  [   64/  130]
train() client id: f_00008-6-2 loss: 0.960174  [   96/  130]
train() client id: f_00008-6-3 loss: 1.033688  [  128/  130]
train() client id: f_00008-7-0 loss: 0.999377  [   32/  130]
train() client id: f_00008-7-1 loss: 0.995587  [   64/  130]
train() client id: f_00008-7-2 loss: 0.825317  [   96/  130]
train() client id: f_00008-7-3 loss: 0.984075  [  128/  130]
train() client id: f_00008-8-0 loss: 0.886882  [   32/  130]
train() client id: f_00008-8-1 loss: 1.023395  [   64/  130]
train() client id: f_00008-8-2 loss: 0.955974  [   96/  130]
train() client id: f_00008-8-3 loss: 0.939238  [  128/  130]
train() client id: f_00008-9-0 loss: 1.045548  [   32/  130]
train() client id: f_00008-9-1 loss: 0.941872  [   64/  130]
train() client id: f_00008-9-2 loss: 0.856458  [   96/  130]
train() client id: f_00008-9-3 loss: 0.961998  [  128/  130]
train() client id: f_00008-10-0 loss: 1.032861  [   32/  130]
train() client id: f_00008-10-1 loss: 0.942011  [   64/  130]
train() client id: f_00008-10-2 loss: 0.941878  [   96/  130]
train() client id: f_00008-10-3 loss: 0.893062  [  128/  130]
train() client id: f_00008-11-0 loss: 0.850160  [   32/  130]
train() client id: f_00008-11-1 loss: 0.872983  [   64/  130]
train() client id: f_00008-11-2 loss: 1.012401  [   96/  130]
train() client id: f_00008-11-3 loss: 1.080314  [  128/  130]
train() client id: f_00009-0-0 loss: 1.026877  [   32/  118]
train() client id: f_00009-0-1 loss: 1.147004  [   64/  118]
train() client id: f_00009-0-2 loss: 1.170681  [   96/  118]
train() client id: f_00009-1-0 loss: 1.015426  [   32/  118]
train() client id: f_00009-1-1 loss: 1.091302  [   64/  118]
train() client id: f_00009-1-2 loss: 1.050318  [   96/  118]
train() client id: f_00009-2-0 loss: 0.995151  [   32/  118]
train() client id: f_00009-2-1 loss: 1.061291  [   64/  118]
train() client id: f_00009-2-2 loss: 1.084626  [   96/  118]
train() client id: f_00009-3-0 loss: 0.961206  [   32/  118]
train() client id: f_00009-3-1 loss: 1.023324  [   64/  118]
train() client id: f_00009-3-2 loss: 0.972840  [   96/  118]
train() client id: f_00009-4-0 loss: 1.055453  [   32/  118]
train() client id: f_00009-4-1 loss: 0.931154  [   64/  118]
train() client id: f_00009-4-2 loss: 1.044712  [   96/  118]
train() client id: f_00009-5-0 loss: 1.001950  [   32/  118]
train() client id: f_00009-5-1 loss: 0.870964  [   64/  118]
train() client id: f_00009-5-2 loss: 0.949754  [   96/  118]
train() client id: f_00009-6-0 loss: 0.902768  [   32/  118]
train() client id: f_00009-6-1 loss: 1.000906  [   64/  118]
train() client id: f_00009-6-2 loss: 0.972830  [   96/  118]
train() client id: f_00009-7-0 loss: 0.942692  [   32/  118]
train() client id: f_00009-7-1 loss: 0.939884  [   64/  118]
train() client id: f_00009-7-2 loss: 0.902181  [   96/  118]
train() client id: f_00009-8-0 loss: 0.946030  [   32/  118]
train() client id: f_00009-8-1 loss: 0.843317  [   64/  118]
train() client id: f_00009-8-2 loss: 0.915557  [   96/  118]
train() client id: f_00009-9-0 loss: 0.840856  [   32/  118]
train() client id: f_00009-9-1 loss: 0.980757  [   64/  118]
train() client id: f_00009-9-2 loss: 0.895079  [   96/  118]
train() client id: f_00009-10-0 loss: 0.823537  [   32/  118]
train() client id: f_00009-10-1 loss: 0.988594  [   64/  118]
train() client id: f_00009-10-2 loss: 0.960136  [   96/  118]
train() client id: f_00009-11-0 loss: 0.887031  [   32/  118]
train() client id: f_00009-11-1 loss: 0.920858  [   64/  118]
train() client id: f_00009-11-2 loss: 0.958004  [   96/  118]
At round 7 accuracy: 0.6312997347480106
At round 7 training accuracy: 0.5774647887323944
At round 7 training loss: 0.8906506223407948
update_location
xs = -3.905658 4.200318 55.009024 18.811294 0.979296 3.956410 -17.443192 -1.324852 39.663977 -2.060879 
ys = 47.587959 30.555839 1.320614 -17.455176 9.350187 -7.185849 -2.624984 -4.177652 17.569006 4.001482 
dists_uav = 110.814566 104.648468 114.139111 103.240244 100.440953 100.335884 101.543860 100.095994 109.004133 100.101244 
dists_bs = 213.323936 230.167047 288.216377 273.133324 241.678657 255.376350 237.433744 249.534944 266.205073 243.204509 
uav_gains = -101.114990 -100.493352 -101.435968 -100.346251 -100.047788 -100.036424 -100.166362 -100.010434 -100.936129 -100.011003 
bs_gains = -104.780168 -105.704267 -108.439181 -107.785550 -106.297732 -106.968118 -106.082247 -106.686738 -107.473116 -106.374265 
Round 8
-------------------------------
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.93572436 20.7513301   9.78311252  3.49713647 23.93609638 11.54337877
  4.34848004 14.03581921 10.31003617  9.36568175]
obj_prev = 117.50679575560986
eta_min = 4.500137844098843e-10	eta_max = 0.9201793140385103
af = 24.843282687249765	bf = 1.924183044657018	zeta = 27.327610955974745	eta = 0.909090909090909
af = 24.843282687249765	bf = 1.924183044657018	zeta = 47.21200862959305	eta = 0.5262068572883742
af = 24.843282687249765	bf = 1.924183044657018	zeta = 37.72761010314322	eta = 0.6584907609925703
af = 24.843282687249765	bf = 1.924183044657018	zeta = 36.02779691629381	eta = 0.689558752231509
af = 24.843282687249765	bf = 1.924183044657018	zeta = 35.944047891502834	eta = 0.6911654124832922
af = 24.843282687249765	bf = 1.924183044657018	zeta = 35.94383021535565	eta = 0.6911695981870181
eta = 0.6911695981870181
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [0.03047953 0.06410378 0.02999573 0.01040174 0.07402172 0.03531755
 0.01306266 0.04330029 0.03144714 0.02854432]
ene_total = [3.05537056 5.9604125  3.0232152  1.38370094 6.79982988 3.63829942
 1.59895401 4.0866222  3.34115314 3.05627237]
ti_comp = [0.29321835 0.27683249 0.29228351 0.29535599 0.2741193  0.27083978
 0.29583662 0.2962475  0.2682047  0.2737568 ]
ti_coms = [0.06608063 0.08246649 0.06701547 0.06394299 0.08517969 0.0884592
 0.06346236 0.06305149 0.09109429 0.08554218]
t_total = [29.59996643 29.59996643 29.59996643 29.59996643 29.59996643 29.59996643
 29.59996643 29.59996643 29.59996643 29.59996643]
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.05836579e-05 2.14830890e-04 1.97446596e-05 8.06319238e-07
 3.37348113e-04 3.75341750e-05 1.59173447e-06 5.78153199e-05
 2.70203220e-05 1.93958759e-05]
ene_total = [0.54608481 0.69707832 0.55371718 0.52684511 0.72952358 0.73184161
 0.52295022 0.52419719 0.75268396 0.70631616]
optimize_network iter = 0 obj = 6.291238143797367
eta = 0.6911695981870181
freqs = [5.19741187e+07 1.15780803e+08 5.13127289e+07 1.76088264e+07
 1.35017343e+08 6.52000814e+07 2.20774830e+07 7.30812690e+07
 5.86252507e+07 5.21344532e+07]
eta_min = 0.6875884070281042	eta_max = 0.6911695981870177
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 0.05581615321591653	eta = 0.909090909090909
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 21.219211967217113	eta = 0.002391321484860486
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 2.2358034144344674	eta = 0.022695178449689362
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 2.1654164527348314	eta = 0.02343288627225956
af = 0.05074195746901502	bf = 1.924183044657018	zeta = 2.1653901507160183	eta = 0.02343317090097433
eta = 0.02343317090097433
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.09174714e-04 2.18314889e-03 2.00648668e-04 8.19395643e-06
 3.42819024e-03 3.81428819e-04 1.61754827e-05 5.87529343e-04
 2.74585215e-04 1.97104266e-04]
ene_total = [0.17760768 0.27172476 0.17982108 0.16680246 0.3112301  0.24039767
 0.16575821 0.17957311 0.24447922 0.22799587]
ti_comp = [0.29738478 0.28099891 0.29644993 0.29952241 0.27828572 0.27500621
 0.30000305 0.30041392 0.27237112 0.27792323]
ti_coms = [0.06608063 0.08246649 0.06701547 0.06394299 0.08517969 0.0884592
 0.06346236 0.06305149 0.09109429 0.08554218]
t_total = [29.59996643 29.59996643 29.59996643 29.59996643 29.59996643 29.59996643
 29.59996643 29.59996643 29.59996643 29.59996643]
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.08671664e-05 2.17429093e-04 2.00148185e-05 8.17590855e-07
 3.41327869e-04 3.79632066e-05 1.61405844e-06 5.86284344e-05
 2.73210417e-05 1.96239144e-05]
ene_total = [0.5398481  0.68929927 0.54739189 0.52080677 0.72148512 0.72348741
 0.51695743 0.51825451 0.74408039 0.69823819]
optimize_network iter = 1 obj = 6.219849079279121
eta = 0.6875884070281042
freqs = [5.19666731e+07 1.15668296e+08 5.13030792e+07 1.76080893e+07
 1.34866351e+08 6.51153626e+07 2.20770578e+07 7.30812690e+07
 5.85403612e+07 5.20751181e+07]
eta_min = 0.6875884070281044	eta_max = 0.6875884070280862
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 0.055709725023689025	eta = 0.909090909090909
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 21.21911053031574	eta = 0.0023867732106222467
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 2.23530448131582	eta = 0.022656960154787298
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 2.1650443530624273	eta = 0.023392224965439173
af = 0.05064520456699002	bf = 1.924183044657018	zeta = 2.1650181860209075	eta = 0.023392507690695646
eta = 0.023392507690695646
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.09211545e-04 2.17991631e-03 2.00666013e-04 8.19706147e-06
 3.42210961e-03 3.80614259e-04 1.61823437e-05 5.87801193e-04
 2.73917274e-04 1.96746859e-04]
ene_total = [0.17760125 0.27162923 0.17981404 0.16679559 0.31105873 0.24036644
 0.16575149 0.17957271 0.24445164 0.22797706]
ti_comp = [0.29738478 0.28099891 0.29644993 0.29952241 0.27828572 0.27500621
 0.30000305 0.30041392 0.27237112 0.27792323]
ti_coms = [0.06608063 0.08246649 0.06701547 0.06394299 0.08517969 0.0884592
 0.06346236 0.06305149 0.09109429 0.08554218]
t_total = [29.59996643 29.59996643 29.59996643 29.59996643 29.59996643 29.59996643
 29.59996643 29.59996643 29.59996643 29.59996643]
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.08671664e-05 2.17429093e-04 2.00148185e-05 8.17590855e-07
 3.41327869e-04 3.79632066e-05 1.61405844e-06 5.86284344e-05
 2.73210417e-05 1.96239144e-05]
ene_total = [0.5398481  0.68929927 0.54739189 0.52080677 0.72148512 0.72348741
 0.51695743 0.51825451 0.74408039 0.69823819]
optimize_network iter = 2 obj = 6.219849079279126
eta = 0.6875884070281044
freqs = [5.19666731e+07 1.15668296e+08 5.13030792e+07 1.76080893e+07
 1.34866351e+08 6.51153626e+07 2.20770578e+07 7.30812690e+07
 5.85403612e+07 5.20751181e+07]
Done!
ene_coms = [0.00660806 0.00824665 0.00670155 0.0063943  0.00851797 0.00884592
 0.00634624 0.00630515 0.00910943 0.00855422]
ene_comp = [2.04160455e-05 2.12728559e-04 1.95821241e-05 7.99915605e-07
 3.33948805e-04 3.71424915e-05 1.57916459e-06 5.73609639e-05
 2.67303963e-05 1.91996709e-05]
ene_total = [0.00662848 0.00845938 0.00672113 0.0063951  0.00885192 0.00888306
 0.00634781 0.00636251 0.00913616 0.00857342]
At round 8 energy consumption: 0.07635896655888749
At round 8 eta: 0.6875884070281044
At round 8 a_n: 25.442236080499836
At round 8 local rounds: 12.265156719696218
At round 8 global rounds: 81.4381945256065
gradient difference: 0.4514181613922119
train() client id: f_00000-0-0 loss: 1.317731  [   32/  126]
train() client id: f_00000-0-1 loss: 1.313786  [   64/  126]
train() client id: f_00000-0-2 loss: 1.272936  [   96/  126]
train() client id: f_00000-1-0 loss: 1.194035  [   32/  126]
train() client id: f_00000-1-1 loss: 1.166416  [   64/  126]
train() client id: f_00000-1-2 loss: 1.149445  [   96/  126]
train() client id: f_00000-2-0 loss: 1.131583  [   32/  126]
train() client id: f_00000-2-1 loss: 1.025599  [   64/  126]
train() client id: f_00000-2-2 loss: 1.186438  [   96/  126]
train() client id: f_00000-3-0 loss: 1.121447  [   32/  126]
train() client id: f_00000-3-1 loss: 1.061051  [   64/  126]
train() client id: f_00000-3-2 loss: 1.051096  [   96/  126]
train() client id: f_00000-4-0 loss: 1.000389  [   32/  126]
train() client id: f_00000-4-1 loss: 1.057948  [   64/  126]
train() client id: f_00000-4-2 loss: 1.057107  [   96/  126]
train() client id: f_00000-5-0 loss: 1.018674  [   32/  126]
train() client id: f_00000-5-1 loss: 0.977999  [   64/  126]
train() client id: f_00000-5-2 loss: 1.040877  [   96/  126]
train() client id: f_00000-6-0 loss: 0.977255  [   32/  126]
train() client id: f_00000-6-1 loss: 1.040229  [   64/  126]
train() client id: f_00000-6-2 loss: 1.019838  [   96/  126]
train() client id: f_00000-7-0 loss: 1.024456  [   32/  126]
train() client id: f_00000-7-1 loss: 0.933220  [   64/  126]
train() client id: f_00000-7-2 loss: 1.022886  [   96/  126]
train() client id: f_00000-8-0 loss: 0.975148  [   32/  126]
train() client id: f_00000-8-1 loss: 1.024651  [   64/  126]
train() client id: f_00000-8-2 loss: 0.940819  [   96/  126]
train() client id: f_00000-9-0 loss: 0.970328  [   32/  126]
train() client id: f_00000-9-1 loss: 0.954748  [   64/  126]
train() client id: f_00000-9-2 loss: 1.058475  [   96/  126]
train() client id: f_00000-10-0 loss: 0.993292  [   32/  126]
train() client id: f_00000-10-1 loss: 0.940803  [   64/  126]
train() client id: f_00000-10-2 loss: 1.002149  [   96/  126]
train() client id: f_00000-11-0 loss: 1.071362  [   32/  126]
train() client id: f_00000-11-1 loss: 0.938314  [   64/  126]
train() client id: f_00000-11-2 loss: 0.993439  [   96/  126]
train() client id: f_00001-0-0 loss: 0.339129  [   32/  265]
train() client id: f_00001-0-1 loss: 0.400479  [   64/  265]
train() client id: f_00001-0-2 loss: 0.485244  [   96/  265]
train() client id: f_00001-0-3 loss: 0.334399  [  128/  265]
train() client id: f_00001-0-4 loss: 0.413378  [  160/  265]
train() client id: f_00001-0-5 loss: 0.458771  [  192/  265]
train() client id: f_00001-0-6 loss: 0.329340  [  224/  265]
train() client id: f_00001-0-7 loss: 0.415461  [  256/  265]
train() client id: f_00001-1-0 loss: 0.402790  [   32/  265]
train() client id: f_00001-1-1 loss: 0.430260  [   64/  265]
train() client id: f_00001-1-2 loss: 0.452651  [   96/  265]
train() client id: f_00001-1-3 loss: 0.377236  [  128/  265]
train() client id: f_00001-1-4 loss: 0.348195  [  160/  265]
train() client id: f_00001-1-5 loss: 0.352022  [  192/  265]
train() client id: f_00001-1-6 loss: 0.385588  [  224/  265]
train() client id: f_00001-1-7 loss: 0.264100  [  256/  265]
train() client id: f_00001-2-0 loss: 0.330394  [   32/  265]
train() client id: f_00001-2-1 loss: 0.387934  [   64/  265]
train() client id: f_00001-2-2 loss: 0.395745  [   96/  265]
train() client id: f_00001-2-3 loss: 0.277445  [  128/  265]
train() client id: f_00001-2-4 loss: 0.275626  [  160/  265]
train() client id: f_00001-2-5 loss: 0.523189  [  192/  265]
train() client id: f_00001-2-6 loss: 0.312594  [  224/  265]
train() client id: f_00001-2-7 loss: 0.342163  [  256/  265]
train() client id: f_00001-3-0 loss: 0.259034  [   32/  265]
train() client id: f_00001-3-1 loss: 0.416113  [   64/  265]
train() client id: f_00001-3-2 loss: 0.316933  [   96/  265]
train() client id: f_00001-3-3 loss: 0.285670  [  128/  265]
train() client id: f_00001-3-4 loss: 0.360391  [  160/  265]
train() client id: f_00001-3-5 loss: 0.378601  [  192/  265]
train() client id: f_00001-3-6 loss: 0.382038  [  224/  265]
train() client id: f_00001-3-7 loss: 0.326420  [  256/  265]
train() client id: f_00001-4-0 loss: 0.310188  [   32/  265]
train() client id: f_00001-4-1 loss: 0.358328  [   64/  265]
train() client id: f_00001-4-2 loss: 0.346882  [   96/  265]
train() client id: f_00001-4-3 loss: 0.374050  [  128/  265]
train() client id: f_00001-4-4 loss: 0.284996  [  160/  265]
train() client id: f_00001-4-5 loss: 0.302094  [  192/  265]
train() client id: f_00001-4-6 loss: 0.306818  [  224/  265]
train() client id: f_00001-4-7 loss: 0.270279  [  256/  265]
train() client id: f_00001-5-0 loss: 0.349914  [   32/  265]
train() client id: f_00001-5-1 loss: 0.336454  [   64/  265]
train() client id: f_00001-5-2 loss: 0.301974  [   96/  265]
train() client id: f_00001-5-3 loss: 0.304435  [  128/  265]
train() client id: f_00001-5-4 loss: 0.391813  [  160/  265]
train() client id: f_00001-5-5 loss: 0.287146  [  192/  265]
train() client id: f_00001-5-6 loss: 0.239550  [  224/  265]
train() client id: f_00001-5-7 loss: 0.283631  [  256/  265]
train() client id: f_00001-6-0 loss: 0.296001  [   32/  265]
train() client id: f_00001-6-1 loss: 0.225763  [   64/  265]
train() client id: f_00001-6-2 loss: 0.412495  [   96/  265]
train() client id: f_00001-6-3 loss: 0.276231  [  128/  265]
train() client id: f_00001-6-4 loss: 0.221757  [  160/  265]
train() client id: f_00001-6-5 loss: 0.296665  [  192/  265]
train() client id: f_00001-6-6 loss: 0.303521  [  224/  265]
train() client id: f_00001-6-7 loss: 0.442916  [  256/  265]
train() client id: f_00001-7-0 loss: 0.302945  [   32/  265]
train() client id: f_00001-7-1 loss: 0.221664  [   64/  265]
train() client id: f_00001-7-2 loss: 0.409690  [   96/  265]
train() client id: f_00001-7-3 loss: 0.330882  [  128/  265]
train() client id: f_00001-7-4 loss: 0.268484  [  160/  265]
train() client id: f_00001-7-5 loss: 0.382142  [  192/  265]
train() client id: f_00001-7-6 loss: 0.284066  [  224/  265]
train() client id: f_00001-7-7 loss: 0.230811  [  256/  265]
train() client id: f_00001-8-0 loss: 0.259835  [   32/  265]
train() client id: f_00001-8-1 loss: 0.283161  [   64/  265]
train() client id: f_00001-8-2 loss: 0.276869  [   96/  265]
train() client id: f_00001-8-3 loss: 0.342044  [  128/  265]
train() client id: f_00001-8-4 loss: 0.275864  [  160/  265]
train() client id: f_00001-8-5 loss: 0.304395  [  192/  265]
train() client id: f_00001-8-6 loss: 0.262120  [  224/  265]
train() client id: f_00001-8-7 loss: 0.260118  [  256/  265]
train() client id: f_00001-9-0 loss: 0.264581  [   32/  265]
train() client id: f_00001-9-1 loss: 0.353529  [   64/  265]
train() client id: f_00001-9-2 loss: 0.246325  [   96/  265]
train() client id: f_00001-9-3 loss: 0.381980  [  128/  265]
train() client id: f_00001-9-4 loss: 0.334775  [  160/  265]
train() client id: f_00001-9-5 loss: 0.327058  [  192/  265]
train() client id: f_00001-9-6 loss: 0.251004  [  224/  265]
train() client id: f_00001-9-7 loss: 0.199929  [  256/  265]
train() client id: f_00001-10-0 loss: 0.207494  [   32/  265]
train() client id: f_00001-10-1 loss: 0.248590  [   64/  265]
train() client id: f_00001-10-2 loss: 0.244999  [   96/  265]
train() client id: f_00001-10-3 loss: 0.362234  [  128/  265]
train() client id: f_00001-10-4 loss: 0.245332  [  160/  265]
train() client id: f_00001-10-5 loss: 0.239773  [  192/  265]
train() client id: f_00001-10-6 loss: 0.414450  [  224/  265]
train() client id: f_00001-10-7 loss: 0.358441  [  256/  265]
train() client id: f_00001-11-0 loss: 0.324854  [   32/  265]
train() client id: f_00001-11-1 loss: 0.341469  [   64/  265]
train() client id: f_00001-11-2 loss: 0.214399  [   96/  265]
train() client id: f_00001-11-3 loss: 0.475210  [  128/  265]
train() client id: f_00001-11-4 loss: 0.310335  [  160/  265]
train() client id: f_00001-11-5 loss: 0.183332  [  192/  265]
train() client id: f_00001-11-6 loss: 0.264140  [  224/  265]
train() client id: f_00001-11-7 loss: 0.197679  [  256/  265]
train() client id: f_00002-0-0 loss: 1.255416  [   32/  124]
train() client id: f_00002-0-1 loss: 1.319057  [   64/  124]
train() client id: f_00002-0-2 loss: 1.177980  [   96/  124]
train() client id: f_00002-1-0 loss: 1.175683  [   32/  124]
train() client id: f_00002-1-1 loss: 1.279261  [   64/  124]
train() client id: f_00002-1-2 loss: 1.170475  [   96/  124]
train() client id: f_00002-2-0 loss: 1.147167  [   32/  124]
train() client id: f_00002-2-1 loss: 1.237878  [   64/  124]
train() client id: f_00002-2-2 loss: 1.193842  [   96/  124]
train() client id: f_00002-3-0 loss: 1.145913  [   32/  124]
train() client id: f_00002-3-1 loss: 1.140163  [   64/  124]
train() client id: f_00002-3-2 loss: 1.219547  [   96/  124]
train() client id: f_00002-4-0 loss: 1.105084  [   32/  124]
train() client id: f_00002-4-1 loss: 1.052116  [   64/  124]
train() client id: f_00002-4-2 loss: 1.187783  [   96/  124]
train() client id: f_00002-5-0 loss: 1.079796  [   32/  124]
train() client id: f_00002-5-1 loss: 1.144383  [   64/  124]
train() client id: f_00002-5-2 loss: 1.080719  [   96/  124]
train() client id: f_00002-6-0 loss: 1.135962  [   32/  124]
train() client id: f_00002-6-1 loss: 1.022940  [   64/  124]
train() client id: f_00002-6-2 loss: 1.034254  [   96/  124]
train() client id: f_00002-7-0 loss: 1.111114  [   32/  124]
train() client id: f_00002-7-1 loss: 1.034503  [   64/  124]
train() client id: f_00002-7-2 loss: 1.050548  [   96/  124]
train() client id: f_00002-8-0 loss: 1.043974  [   32/  124]
train() client id: f_00002-8-1 loss: 1.003165  [   64/  124]
train() client id: f_00002-8-2 loss: 1.050592  [   96/  124]
train() client id: f_00002-9-0 loss: 0.963922  [   32/  124]
train() client id: f_00002-9-1 loss: 1.021101  [   64/  124]
train() client id: f_00002-9-2 loss: 1.134605  [   96/  124]
train() client id: f_00002-10-0 loss: 1.120014  [   32/  124]
train() client id: f_00002-10-1 loss: 0.985307  [   64/  124]
train() client id: f_00002-10-2 loss: 1.021321  [   96/  124]
train() client id: f_00002-11-0 loss: 1.062859  [   32/  124]
train() client id: f_00002-11-1 loss: 1.062802  [   64/  124]
train() client id: f_00002-11-2 loss: 1.009536  [   96/  124]
train() client id: f_00003-0-0 loss: 0.964291  [   32/   43]
train() client id: f_00003-1-0 loss: 0.900422  [   32/   43]
train() client id: f_00003-2-0 loss: 0.972311  [   32/   43]
train() client id: f_00003-3-0 loss: 0.961698  [   32/   43]
train() client id: f_00003-4-0 loss: 0.834820  [   32/   43]
train() client id: f_00003-5-0 loss: 0.942370  [   32/   43]
train() client id: f_00003-6-0 loss: 0.954223  [   32/   43]
train() client id: f_00003-7-0 loss: 0.938520  [   32/   43]
train() client id: f_00003-8-0 loss: 0.970228  [   32/   43]
train() client id: f_00003-9-0 loss: 0.997084  [   32/   43]
train() client id: f_00003-10-0 loss: 0.966751  [   32/   43]
train() client id: f_00003-11-0 loss: 0.920496  [   32/   43]
train() client id: f_00004-0-0 loss: 0.899407  [   32/  306]
train() client id: f_00004-0-1 loss: 0.891168  [   64/  306]
train() client id: f_00004-0-2 loss: 0.867028  [   96/  306]
train() client id: f_00004-0-3 loss: 0.865755  [  128/  306]
train() client id: f_00004-0-4 loss: 0.852250  [  160/  306]
train() client id: f_00004-0-5 loss: 0.826163  [  192/  306]
train() client id: f_00004-0-6 loss: 0.916298  [  224/  306]
train() client id: f_00004-0-7 loss: 0.726246  [  256/  306]
train() client id: f_00004-0-8 loss: 0.918227  [  288/  306]
train() client id: f_00004-1-0 loss: 0.926589  [   32/  306]
train() client id: f_00004-1-1 loss: 0.925472  [   64/  306]
train() client id: f_00004-1-2 loss: 0.915315  [   96/  306]
train() client id: f_00004-1-3 loss: 0.664691  [  128/  306]
train() client id: f_00004-1-4 loss: 0.910682  [  160/  306]
train() client id: f_00004-1-5 loss: 0.778749  [  192/  306]
train() client id: f_00004-1-6 loss: 0.970473  [  224/  306]
train() client id: f_00004-1-7 loss: 0.862412  [  256/  306]
train() client id: f_00004-1-8 loss: 0.820675  [  288/  306]
train() client id: f_00004-2-0 loss: 0.948512  [   32/  306]
train() client id: f_00004-2-1 loss: 0.884169  [   64/  306]
train() client id: f_00004-2-2 loss: 0.928379  [   96/  306]
train() client id: f_00004-2-3 loss: 0.869279  [  128/  306]
train() client id: f_00004-2-4 loss: 0.826983  [  160/  306]
train() client id: f_00004-2-5 loss: 0.852683  [  192/  306]
train() client id: f_00004-2-6 loss: 0.919387  [  224/  306]
train() client id: f_00004-2-7 loss: 0.777976  [  256/  306]
train() client id: f_00004-2-8 loss: 0.846086  [  288/  306]
train() client id: f_00004-3-0 loss: 0.858196  [   32/  306]
train() client id: f_00004-3-1 loss: 0.950734  [   64/  306]
train() client id: f_00004-3-2 loss: 0.948036  [   96/  306]
train() client id: f_00004-3-3 loss: 0.885273  [  128/  306]
train() client id: f_00004-3-4 loss: 0.808153  [  160/  306]
train() client id: f_00004-3-5 loss: 0.789990  [  192/  306]
train() client id: f_00004-3-6 loss: 0.917526  [  224/  306]
train() client id: f_00004-3-7 loss: 0.932241  [  256/  306]
train() client id: f_00004-3-8 loss: 0.736159  [  288/  306]
train() client id: f_00004-4-0 loss: 0.917091  [   32/  306]
train() client id: f_00004-4-1 loss: 0.834419  [   64/  306]
train() client id: f_00004-4-2 loss: 0.916943  [   96/  306]
train() client id: f_00004-4-3 loss: 0.842122  [  128/  306]
train() client id: f_00004-4-4 loss: 0.822000  [  160/  306]
train() client id: f_00004-4-5 loss: 0.900743  [  192/  306]
train() client id: f_00004-4-6 loss: 0.909854  [  224/  306]
train() client id: f_00004-4-7 loss: 0.823718  [  256/  306]
train() client id: f_00004-4-8 loss: 0.822816  [  288/  306]
train() client id: f_00004-5-0 loss: 0.789656  [   32/  306]
train() client id: f_00004-5-1 loss: 0.818456  [   64/  306]
train() client id: f_00004-5-2 loss: 0.942194  [   96/  306]
train() client id: f_00004-5-3 loss: 0.972528  [  128/  306]
train() client id: f_00004-5-4 loss: 0.808823  [  160/  306]
train() client id: f_00004-5-5 loss: 0.786031  [  192/  306]
train() client id: f_00004-5-6 loss: 0.942590  [  224/  306]
train() client id: f_00004-5-7 loss: 0.888618  [  256/  306]
train() client id: f_00004-5-8 loss: 0.881813  [  288/  306]
train() client id: f_00004-6-0 loss: 0.824006  [   32/  306]
train() client id: f_00004-6-1 loss: 0.831208  [   64/  306]
train() client id: f_00004-6-2 loss: 0.790827  [   96/  306]
train() client id: f_00004-6-3 loss: 0.903829  [  128/  306]
train() client id: f_00004-6-4 loss: 0.917941  [  160/  306]
train() client id: f_00004-6-5 loss: 0.883361  [  192/  306]
train() client id: f_00004-6-6 loss: 0.979032  [  224/  306]
train() client id: f_00004-6-7 loss: 0.871001  [  256/  306]
train() client id: f_00004-6-8 loss: 0.829697  [  288/  306]
train() client id: f_00004-7-0 loss: 0.784307  [   32/  306]
train() client id: f_00004-7-1 loss: 0.950033  [   64/  306]
train() client id: f_00004-7-2 loss: 0.869123  [   96/  306]
train() client id: f_00004-7-3 loss: 0.826712  [  128/  306]
train() client id: f_00004-7-4 loss: 0.833631  [  160/  306]
train() client id: f_00004-7-5 loss: 0.945348  [  192/  306]
train() client id: f_00004-7-6 loss: 0.920423  [  224/  306]
train() client id: f_00004-7-7 loss: 0.827905  [  256/  306]
train() client id: f_00004-7-8 loss: 0.924572  [  288/  306]
train() client id: f_00004-8-0 loss: 0.806270  [   32/  306]
train() client id: f_00004-8-1 loss: 0.825376  [   64/  306]
train() client id: f_00004-8-2 loss: 0.808419  [   96/  306]
train() client id: f_00004-8-3 loss: 0.830706  [  128/  306]
train() client id: f_00004-8-4 loss: 0.976851  [  160/  306]
train() client id: f_00004-8-5 loss: 0.879334  [  192/  306]
train() client id: f_00004-8-6 loss: 0.819389  [  224/  306]
train() client id: f_00004-8-7 loss: 0.875245  [  256/  306]
train() client id: f_00004-8-8 loss: 0.965065  [  288/  306]
train() client id: f_00004-9-0 loss: 0.836023  [   32/  306]
train() client id: f_00004-9-1 loss: 0.807382  [   64/  306]
train() client id: f_00004-9-2 loss: 0.920572  [   96/  306]
train() client id: f_00004-9-3 loss: 0.999288  [  128/  306]
train() client id: f_00004-9-4 loss: 0.874557  [  160/  306]
train() client id: f_00004-9-5 loss: 0.838546  [  192/  306]
train() client id: f_00004-9-6 loss: 0.782696  [  224/  306]
train() client id: f_00004-9-7 loss: 0.898145  [  256/  306]
train() client id: f_00004-9-8 loss: 0.909646  [  288/  306]
train() client id: f_00004-10-0 loss: 0.786797  [   32/  306]
train() client id: f_00004-10-1 loss: 0.911425  [   64/  306]
train() client id: f_00004-10-2 loss: 0.907252  [   96/  306]
train() client id: f_00004-10-3 loss: 0.806834  [  128/  306]
train() client id: f_00004-10-4 loss: 0.864759  [  160/  306]
train() client id: f_00004-10-5 loss: 0.932238  [  192/  306]
train() client id: f_00004-10-6 loss: 0.891819  [  224/  306]
train() client id: f_00004-10-7 loss: 0.868413  [  256/  306]
train() client id: f_00004-10-8 loss: 0.875792  [  288/  306]
train() client id: f_00004-11-0 loss: 0.862242  [   32/  306]
train() client id: f_00004-11-1 loss: 0.860136  [   64/  306]
train() client id: f_00004-11-2 loss: 0.833997  [   96/  306]
train() client id: f_00004-11-3 loss: 0.794713  [  128/  306]
train() client id: f_00004-11-4 loss: 0.850472  [  160/  306]
train() client id: f_00004-11-5 loss: 0.965623  [  192/  306]
train() client id: f_00004-11-6 loss: 0.802584  [  224/  306]
train() client id: f_00004-11-7 loss: 0.977639  [  256/  306]
train() client id: f_00004-11-8 loss: 0.917237  [  288/  306]
train() client id: f_00005-0-0 loss: 0.814943  [   32/  146]
train() client id: f_00005-0-1 loss: 0.825430  [   64/  146]
train() client id: f_00005-0-2 loss: 0.683931  [   96/  146]
train() client id: f_00005-0-3 loss: 0.856040  [  128/  146]
train() client id: f_00005-1-0 loss: 0.772606  [   32/  146]
train() client id: f_00005-1-1 loss: 0.899472  [   64/  146]
train() client id: f_00005-1-2 loss: 0.713049  [   96/  146]
train() client id: f_00005-1-3 loss: 0.771343  [  128/  146]
train() client id: f_00005-2-0 loss: 0.647446  [   32/  146]
train() client id: f_00005-2-1 loss: 0.801293  [   64/  146]
train() client id: f_00005-2-2 loss: 0.980532  [   96/  146]
train() client id: f_00005-2-3 loss: 0.713173  [  128/  146]
train() client id: f_00005-3-0 loss: 0.857636  [   32/  146]
train() client id: f_00005-3-1 loss: 0.837527  [   64/  146]
train() client id: f_00005-3-2 loss: 0.772669  [   96/  146]
train() client id: f_00005-3-3 loss: 0.698665  [  128/  146]
train() client id: f_00005-4-0 loss: 0.817718  [   32/  146]
train() client id: f_00005-4-1 loss: 0.793202  [   64/  146]
train() client id: f_00005-4-2 loss: 0.952896  [   96/  146]
train() client id: f_00005-4-3 loss: 0.629732  [  128/  146]
train() client id: f_00005-5-0 loss: 0.533203  [   32/  146]
train() client id: f_00005-5-1 loss: 0.692192  [   64/  146]
train() client id: f_00005-5-2 loss: 1.018985  [   96/  146]
train() client id: f_00005-5-3 loss: 0.843905  [  128/  146]
train() client id: f_00005-6-0 loss: 0.682120  [   32/  146]
train() client id: f_00005-6-1 loss: 0.741608  [   64/  146]
train() client id: f_00005-6-2 loss: 1.007049  [   96/  146]
train() client id: f_00005-6-3 loss: 0.838910  [  128/  146]
train() client id: f_00005-7-0 loss: 0.944886  [   32/  146]
train() client id: f_00005-7-1 loss: 0.729847  [   64/  146]
train() client id: f_00005-7-2 loss: 0.797126  [   96/  146]
train() client id: f_00005-7-3 loss: 0.680952  [  128/  146]
train() client id: f_00005-8-0 loss: 0.631133  [   32/  146]
train() client id: f_00005-8-1 loss: 0.602514  [   64/  146]
train() client id: f_00005-8-2 loss: 0.982088  [   96/  146]
train() client id: f_00005-8-3 loss: 0.782072  [  128/  146]
train() client id: f_00005-9-0 loss: 0.912568  [   32/  146]
train() client id: f_00005-9-1 loss: 0.752954  [   64/  146]
train() client id: f_00005-9-2 loss: 0.727079  [   96/  146]
train() client id: f_00005-9-3 loss: 0.715869  [  128/  146]
train() client id: f_00005-10-0 loss: 0.806770  [   32/  146]
train() client id: f_00005-10-1 loss: 0.654031  [   64/  146]
train() client id: f_00005-10-2 loss: 0.956901  [   96/  146]
train() client id: f_00005-10-3 loss: 0.844121  [  128/  146]
train() client id: f_00005-11-0 loss: 0.829021  [   32/  146]
train() client id: f_00005-11-1 loss: 0.924956  [   64/  146]
train() client id: f_00005-11-2 loss: 0.699309  [   96/  146]
train() client id: f_00005-11-3 loss: 0.769455  [  128/  146]
train() client id: f_00006-0-0 loss: 0.808732  [   32/   54]
train() client id: f_00006-1-0 loss: 0.795157  [   32/   54]
train() client id: f_00006-2-0 loss: 0.802628  [   32/   54]
train() client id: f_00006-3-0 loss: 0.822309  [   32/   54]
train() client id: f_00006-4-0 loss: 0.776120  [   32/   54]
train() client id: f_00006-5-0 loss: 0.827866  [   32/   54]
train() client id: f_00006-6-0 loss: 0.798121  [   32/   54]
train() client id: f_00006-7-0 loss: 0.765956  [   32/   54]
train() client id: f_00006-8-0 loss: 0.799542  [   32/   54]
train() client id: f_00006-9-0 loss: 0.820723  [   32/   54]
train() client id: f_00006-10-0 loss: 0.820158  [   32/   54]
train() client id: f_00006-11-0 loss: 0.821183  [   32/   54]
train() client id: f_00007-0-0 loss: 0.786764  [   32/  179]
train() client id: f_00007-0-1 loss: 0.843792  [   64/  179]
train() client id: f_00007-0-2 loss: 0.805076  [   96/  179]
train() client id: f_00007-0-3 loss: 0.747262  [  128/  179]
train() client id: f_00007-0-4 loss: 0.844373  [  160/  179]
train() client id: f_00007-1-0 loss: 0.868251  [   32/  179]
train() client id: f_00007-1-1 loss: 0.796441  [   64/  179]
train() client id: f_00007-1-2 loss: 0.763414  [   96/  179]
train() client id: f_00007-1-3 loss: 0.806022  [  128/  179]
train() client id: f_00007-1-4 loss: 0.683645  [  160/  179]
train() client id: f_00007-2-0 loss: 0.726031  [   32/  179]
train() client id: f_00007-2-1 loss: 0.857363  [   64/  179]
train() client id: f_00007-2-2 loss: 0.717681  [   96/  179]
train() client id: f_00007-2-3 loss: 0.666207  [  128/  179]
train() client id: f_00007-2-4 loss: 0.784754  [  160/  179]
train() client id: f_00007-3-0 loss: 0.659918  [   32/  179]
train() client id: f_00007-3-1 loss: 0.652071  [   64/  179]
train() client id: f_00007-3-2 loss: 0.757341  [   96/  179]
train() client id: f_00007-3-3 loss: 0.897654  [  128/  179]
train() client id: f_00007-3-4 loss: 0.800850  [  160/  179]
train() client id: f_00007-4-0 loss: 0.764061  [   32/  179]
train() client id: f_00007-4-1 loss: 0.656367  [   64/  179]
train() client id: f_00007-4-2 loss: 0.631653  [   96/  179]
train() client id: f_00007-4-3 loss: 0.833566  [  128/  179]
train() client id: f_00007-4-4 loss: 0.825720  [  160/  179]
train() client id: f_00007-5-0 loss: 0.689580  [   32/  179]
train() client id: f_00007-5-1 loss: 0.767898  [   64/  179]
train() client id: f_00007-5-2 loss: 0.628160  [   96/  179]
train() client id: f_00007-5-3 loss: 0.623813  [  128/  179]
train() client id: f_00007-5-4 loss: 0.764982  [  160/  179]
train() client id: f_00007-6-0 loss: 0.682686  [   32/  179]
train() client id: f_00007-6-1 loss: 0.761051  [   64/  179]
train() client id: f_00007-6-2 loss: 0.687155  [   96/  179]
train() client id: f_00007-6-3 loss: 0.750642  [  128/  179]
train() client id: f_00007-6-4 loss: 0.707850  [  160/  179]
train() client id: f_00007-7-0 loss: 0.693793  [   32/  179]
train() client id: f_00007-7-1 loss: 0.742933  [   64/  179]
train() client id: f_00007-7-2 loss: 0.818139  [   96/  179]
train() client id: f_00007-7-3 loss: 0.616766  [  128/  179]
train() client id: f_00007-7-4 loss: 0.761525  [  160/  179]
train() client id: f_00007-8-0 loss: 0.697991  [   32/  179]
train() client id: f_00007-8-1 loss: 0.742018  [   64/  179]
train() client id: f_00007-8-2 loss: 0.765700  [   96/  179]
train() client id: f_00007-8-3 loss: 0.597054  [  128/  179]
train() client id: f_00007-8-4 loss: 0.742318  [  160/  179]
train() client id: f_00007-9-0 loss: 0.621149  [   32/  179]
train() client id: f_00007-9-1 loss: 0.691379  [   64/  179]
train() client id: f_00007-9-2 loss: 0.847366  [   96/  179]
train() client id: f_00007-9-3 loss: 0.668057  [  128/  179]
train() client id: f_00007-9-4 loss: 0.737257  [  160/  179]
train() client id: f_00007-10-0 loss: 0.729797  [   32/  179]
train() client id: f_00007-10-1 loss: 0.755844  [   64/  179]
train() client id: f_00007-10-2 loss: 0.769206  [   96/  179]
train() client id: f_00007-10-3 loss: 0.704271  [  128/  179]
train() client id: f_00007-10-4 loss: 0.650034  [  160/  179]
train() client id: f_00007-11-0 loss: 0.658523  [   32/  179]
train() client id: f_00007-11-1 loss: 0.831127  [   64/  179]
train() client id: f_00007-11-2 loss: 0.750787  [   96/  179]
train() client id: f_00007-11-3 loss: 0.598477  [  128/  179]
train() client id: f_00007-11-4 loss: 0.760693  [  160/  179]
train() client id: f_00008-0-0 loss: 0.640047  [   32/  130]
train() client id: f_00008-0-1 loss: 0.778220  [   64/  130]
train() client id: f_00008-0-2 loss: 0.698650  [   96/  130]
train() client id: f_00008-0-3 loss: 0.665370  [  128/  130]
train() client id: f_00008-1-0 loss: 0.634377  [   32/  130]
train() client id: f_00008-1-1 loss: 0.739057  [   64/  130]
train() client id: f_00008-1-2 loss: 0.803771  [   96/  130]
train() client id: f_00008-1-3 loss: 0.619361  [  128/  130]
train() client id: f_00008-2-0 loss: 0.575422  [   32/  130]
train() client id: f_00008-2-1 loss: 0.776157  [   64/  130]
train() client id: f_00008-2-2 loss: 0.800630  [   96/  130]
train() client id: f_00008-2-3 loss: 0.637400  [  128/  130]
train() client id: f_00008-3-0 loss: 0.686949  [   32/  130]
train() client id: f_00008-3-1 loss: 0.677074  [   64/  130]
train() client id: f_00008-3-2 loss: 0.652687  [   96/  130]
train() client id: f_00008-3-3 loss: 0.758254  [  128/  130]
train() client id: f_00008-4-0 loss: 0.636117  [   32/  130]
train() client id: f_00008-4-1 loss: 0.671253  [   64/  130]
train() client id: f_00008-4-2 loss: 0.829917  [   96/  130]
train() client id: f_00008-4-3 loss: 0.626340  [  128/  130]
train() client id: f_00008-5-0 loss: 0.584623  [   32/  130]
train() client id: f_00008-5-1 loss: 0.721514  [   64/  130]
train() client id: f_00008-5-2 loss: 0.641411  [   96/  130]
train() client id: f_00008-5-3 loss: 0.817688  [  128/  130]
train() client id: f_00008-6-0 loss: 0.689282  [   32/  130]
train() client id: f_00008-6-1 loss: 0.656180  [   64/  130]
train() client id: f_00008-6-2 loss: 0.712639  [   96/  130]
train() client id: f_00008-6-3 loss: 0.672197  [  128/  130]
train() client id: f_00008-7-0 loss: 0.603626  [   32/  130]
train() client id: f_00008-7-1 loss: 0.826462  [   64/  130]
train() client id: f_00008-7-2 loss: 0.568931  [   96/  130]
train() client id: f_00008-7-3 loss: 0.721513  [  128/  130]
train() client id: f_00008-8-0 loss: 0.704604  [   32/  130]
train() client id: f_00008-8-1 loss: 0.604840  [   64/  130]
train() client id: f_00008-8-2 loss: 0.748896  [   96/  130]
train() client id: f_00008-8-3 loss: 0.690489  [  128/  130]
train() client id: f_00008-9-0 loss: 0.761662  [   32/  130]
train() client id: f_00008-9-1 loss: 0.634615  [   64/  130]
train() client id: f_00008-9-2 loss: 0.680620  [   96/  130]
train() client id: f_00008-9-3 loss: 0.673342  [  128/  130]
train() client id: f_00008-10-0 loss: 0.651952  [   32/  130]
train() client id: f_00008-10-1 loss: 0.632386  [   64/  130]
train() client id: f_00008-10-2 loss: 0.650144  [   96/  130]
train() client id: f_00008-10-3 loss: 0.809362  [  128/  130]
train() client id: f_00008-11-0 loss: 0.532320  [   32/  130]
train() client id: f_00008-11-1 loss: 0.778753  [   64/  130]
train() client id: f_00008-11-2 loss: 0.622695  [   96/  130]
train() client id: f_00008-11-3 loss: 0.776178  [  128/  130]
train() client id: f_00009-0-0 loss: 1.019369  [   32/  118]
train() client id: f_00009-0-1 loss: 1.101223  [   64/  118]
train() client id: f_00009-0-2 loss: 1.118685  [   96/  118]
train() client id: f_00009-1-0 loss: 1.012747  [   32/  118]
train() client id: f_00009-1-1 loss: 1.127439  [   64/  118]
train() client id: f_00009-1-2 loss: 0.987407  [   96/  118]
train() client id: f_00009-2-0 loss: 1.153627  [   32/  118]
train() client id: f_00009-2-1 loss: 0.972919  [   64/  118]
train() client id: f_00009-2-2 loss: 0.893232  [   96/  118]
train() client id: f_00009-3-0 loss: 0.939565  [   32/  118]
train() client id: f_00009-3-1 loss: 0.952311  [   64/  118]
train() client id: f_00009-3-2 loss: 0.953028  [   96/  118]
train() client id: f_00009-4-0 loss: 0.972507  [   32/  118]
train() client id: f_00009-4-1 loss: 1.012848  [   64/  118]
train() client id: f_00009-4-2 loss: 0.812106  [   96/  118]
train() client id: f_00009-5-0 loss: 0.883591  [   32/  118]
train() client id: f_00009-5-1 loss: 0.965428  [   64/  118]
train() client id: f_00009-5-2 loss: 0.905319  [   96/  118]
train() client id: f_00009-6-0 loss: 0.945188  [   32/  118]
train() client id: f_00009-6-1 loss: 0.820300  [   64/  118]
train() client id: f_00009-6-2 loss: 0.886299  [   96/  118]
train() client id: f_00009-7-0 loss: 0.891870  [   32/  118]
train() client id: f_00009-7-1 loss: 0.845350  [   64/  118]
train() client id: f_00009-7-2 loss: 0.928316  [   96/  118]
train() client id: f_00009-8-0 loss: 0.818430  [   32/  118]
train() client id: f_00009-8-1 loss: 0.936170  [   64/  118]
train() client id: f_00009-8-2 loss: 0.847888  [   96/  118]
train() client id: f_00009-9-0 loss: 0.918108  [   32/  118]
train() client id: f_00009-9-1 loss: 0.839539  [   64/  118]
train() client id: f_00009-9-2 loss: 0.768680  [   96/  118]
train() client id: f_00009-10-0 loss: 0.756441  [   32/  118]
train() client id: f_00009-10-1 loss: 0.841866  [   64/  118]
train() client id: f_00009-10-2 loss: 0.874010  [   96/  118]
train() client id: f_00009-11-0 loss: 0.866666  [   32/  118]
train() client id: f_00009-11-1 loss: 0.864879  [   64/  118]
train() client id: f_00009-11-2 loss: 0.872752  [   96/  118]
At round 8 accuracy: 0.6312997347480106
At round 8 training accuracy: 0.5700871898054997
At round 8 training loss: 0.8933343068473151
update_location
xs = -3.905658 4.200318 60.009024 18.811294 0.979296 3.956410 -22.443192 -1.324852 44.663977 2.939121 
ys = 52.587959 35.555839 1.320614 -22.455176 14.350187 -2.185849 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 113.051969 106.216102 116.631158 104.202206 101.029139 100.102103 102.521156 100.012157 110.921327 100.123176 
dists_bs = 210.375810 227.062608 292.222125 276.679172 238.279406 251.833719 234.145713 245.969331 270.253179 246.784976 
uav_gains = -101.332045 -100.654797 -101.670508 -100.446952 -100.111185 -100.011096 -100.270360 -100.001336 -101.125447 -100.013382 
bs_gains = -104.610942 -105.539137 -108.607026 -107.942400 -106.125481 -106.798248 -105.912673 -106.511726 -107.656642 -106.551983 
Round 9
-------------------------------
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.80354861 20.47010187  9.65317002  3.45074232 23.61178753 11.38579797
  4.29063581 13.84678522 10.17374854  9.24174878]
obj_prev = 115.9280666591941
eta_min = 3.3564500217132474e-10	eta_max = 0.9202772988105504
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 26.95968104561057	eta = 0.909090909090909
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 46.612772647926946	eta = 0.5257958185768866
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 37.23476446800202	eta = 0.6582236063723965
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 35.55373220830825	eta = 0.6893453774967627
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 35.470824829161245	eta = 0.690956612049402
af = 24.50880095055506	bf = 1.9015865607893707	zeta = 35.47060887359621	eta = 0.690960818797758
eta = 0.690960818797758
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [0.03050446 0.06415621 0.03002026 0.01041025 0.07408226 0.03534644
 0.01307334 0.0433357  0.03147286 0.02856767]
ene_total = [3.01933378 5.87455757 2.98817258 1.36705087 6.70212565 3.5823017
 1.57947799 4.03156066 3.30411446 3.0219136 ]
ti_comp = [0.29712306 0.28209195 0.2961178  0.29961782 0.27945843 0.27622759
 0.30009367 0.30080535 0.27174348 0.27743751]
ti_coms = [0.06670996 0.08174107 0.06771522 0.0642152  0.08437459 0.08760544
 0.06373935 0.06302768 0.09208954 0.08639551]
t_total = [29.54996223 29.54996223 29.54996223 29.54996223 29.54996223 29.54996223
 29.54996223 29.54996223 29.54996223 29.54996223]
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [2.00954316e-05 2.07402819e-04 1.92838817e-05 7.85468046e-07
 3.25378074e-04 3.61728644e-05 1.55069375e-06 5.62142497e-05
 2.63857967e-05 1.89310232e-05]
ene_total = [0.54344022 0.68073341 0.5515389  0.52160987 0.71170427 0.71445587
 0.51780721 0.51646678 0.75008018 0.70322868]
optimize_network iter = 0 obj = 6.211065406906146
eta = 0.690960818797758
freqs = [5.13330407e+07 1.13715060e+08 5.06897303e+07 1.73725523e+07
 1.32546121e+08 6.39806453e+07 2.17820992e+07 7.20327985e+07
 5.79091277e+07 5.14848716e+07]
eta_min = 0.6909608187976134	eta_max = 0.6909608187977386
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 0.05319201668668125	eta = 0.9090909090909091
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 20.968149575428694	eta = 0.002306182461743776
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 2.200288580777995	eta = 0.021977289355824242
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 2.1330781928113622	eta = 0.022669763803801746
af = 0.04835637880607386	bf = 1.9015865607893707	zeta = 2.1330546285065273	eta = 0.022670014241468774
eta = 0.022670014241468774
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [2.05846218e-04 2.12451699e-03 1.97533160e-04 8.04588971e-06
 3.33298867e-03 3.70534332e-04 1.58844283e-05 5.75826930e-04
 2.70281154e-04 1.93918678e-04]
ene_total = [0.17661037 0.26448822 0.17897858 0.16512347 0.30228746 0.23450342
 0.1641027  0.17665537 0.24344476 0.22686028]
ti_comp = [0.29712306 0.28209195 0.2961178  0.29961782 0.27945843 0.27622759
 0.30009367 0.30080535 0.27174348 0.27743751]
ti_coms = [0.06670996 0.08174107 0.06771522 0.0642152  0.08437459 0.08760544
 0.06373935 0.06302768 0.09208954 0.08639551]
t_total = [29.54996223 29.54996223 29.54996223 29.54996223 29.54996223 29.54996223
 29.54996223 29.54996223 29.54996223 29.54996223]
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [2.00954316e-05 2.07402819e-04 1.92838817e-05 7.85468046e-07
 3.25378074e-04 3.61728644e-05 1.55069375e-06 5.62142497e-05
 2.63857967e-05 1.89310232e-05]
ene_total = [0.54344022 0.68073341 0.5515389  0.52160987 0.71170427 0.71445587
 0.51780721 0.51646678 0.75008018 0.70322868]
optimize_network iter = 1 obj = 6.211065406903268
eta = 0.6909608187976134
freqs = [5.13330407e+07 1.13715060e+08 5.06897303e+07 1.73725523e+07
 1.32546121e+08 6.39806453e+07 2.17820992e+07 7.20327985e+07
 5.79091277e+07 5.14848716e+07]
Done!
ene_coms = [0.006671   0.00817411 0.00677152 0.00642152 0.00843746 0.00876054
 0.00637394 0.00630277 0.00920895 0.00863955]
ene_comp = [1.99212129e-05 2.05604726e-04 1.91166988e-05 7.78658377e-07
 3.22557186e-04 3.58592613e-05 1.53724990e-06 5.57268965e-05
 2.61570432e-05 1.87668994e-05]
ene_total = [0.00669092 0.00837971 0.00679064 0.0064223  0.00876002 0.0087964
 0.00637547 0.00635849 0.00923511 0.00865832]
At round 9 energy consumption: 0.07646738157410429
At round 9 eta: 0.6909608187976134
At round 9 a_n: 25.09969023353052
At round 9 local rounds: 12.104944645341616
At round 9 global rounds: 81.21847247936174
gradient difference: 0.3553544282913208
train() client id: f_00000-0-0 loss: 1.204651  [   32/  126]
train() client id: f_00000-0-1 loss: 1.401591  [   64/  126]
train() client id: f_00000-0-2 loss: 1.287379  [   96/  126]
train() client id: f_00000-1-0 loss: 1.253867  [   32/  126]
train() client id: f_00000-1-1 loss: 1.148516  [   64/  126]
train() client id: f_00000-1-2 loss: 1.115155  [   96/  126]
train() client id: f_00000-2-0 loss: 1.237063  [   32/  126]
train() client id: f_00000-2-1 loss: 1.052297  [   64/  126]
train() client id: f_00000-2-2 loss: 1.115736  [   96/  126]
train() client id: f_00000-3-0 loss: 1.079940  [   32/  126]
train() client id: f_00000-3-1 loss: 1.098003  [   64/  126]
train() client id: f_00000-3-2 loss: 0.954854  [   96/  126]
train() client id: f_00000-4-0 loss: 1.118740  [   32/  126]
train() client id: f_00000-4-1 loss: 0.976041  [   64/  126]
train() client id: f_00000-4-2 loss: 0.918118  [   96/  126]
train() client id: f_00000-5-0 loss: 0.926349  [   32/  126]
train() client id: f_00000-5-1 loss: 0.959988  [   64/  126]
train() client id: f_00000-5-2 loss: 0.939819  [   96/  126]
train() client id: f_00000-6-0 loss: 0.900713  [   32/  126]
train() client id: f_00000-6-1 loss: 0.942825  [   64/  126]
train() client id: f_00000-6-2 loss: 0.918424  [   96/  126]
train() client id: f_00000-7-0 loss: 0.897626  [   32/  126]
train() client id: f_00000-7-1 loss: 0.897653  [   64/  126]
train() client id: f_00000-7-2 loss: 0.967098  [   96/  126]
train() client id: f_00000-8-0 loss: 0.929997  [   32/  126]
train() client id: f_00000-8-1 loss: 0.898709  [   64/  126]
train() client id: f_00000-8-2 loss: 0.884004  [   96/  126]
train() client id: f_00000-9-0 loss: 0.938905  [   32/  126]
train() client id: f_00000-9-1 loss: 0.805625  [   64/  126]
train() client id: f_00000-9-2 loss: 0.920038  [   96/  126]
train() client id: f_00000-10-0 loss: 0.863020  [   32/  126]
train() client id: f_00000-10-1 loss: 0.798649  [   64/  126]
train() client id: f_00000-10-2 loss: 0.904667  [   96/  126]
train() client id: f_00000-11-0 loss: 0.775772  [   32/  126]
train() client id: f_00000-11-1 loss: 0.860936  [   64/  126]
train() client id: f_00000-11-2 loss: 0.923904  [   96/  126]
train() client id: f_00001-0-0 loss: 0.588580  [   32/  265]
train() client id: f_00001-0-1 loss: 0.604231  [   64/  265]
train() client id: f_00001-0-2 loss: 0.634375  [   96/  265]
train() client id: f_00001-0-3 loss: 0.721533  [  128/  265]
train() client id: f_00001-0-4 loss: 0.594349  [  160/  265]
train() client id: f_00001-0-5 loss: 0.614930  [  192/  265]
train() client id: f_00001-0-6 loss: 0.628427  [  224/  265]
train() client id: f_00001-0-7 loss: 0.527202  [  256/  265]
train() client id: f_00001-1-0 loss: 0.626097  [   32/  265]
train() client id: f_00001-1-1 loss: 0.737193  [   64/  265]
train() client id: f_00001-1-2 loss: 0.530925  [   96/  265]
train() client id: f_00001-1-3 loss: 0.604972  [  128/  265]
train() client id: f_00001-1-4 loss: 0.605873  [  160/  265]
train() client id: f_00001-1-5 loss: 0.592045  [  192/  265]
train() client id: f_00001-1-6 loss: 0.525538  [  224/  265]
train() client id: f_00001-1-7 loss: 0.572345  [  256/  265]
train() client id: f_00001-2-0 loss: 0.580135  [   32/  265]
train() client id: f_00001-2-1 loss: 0.683772  [   64/  265]
train() client id: f_00001-2-2 loss: 0.556132  [   96/  265]
train() client id: f_00001-2-3 loss: 0.554027  [  128/  265]
train() client id: f_00001-2-4 loss: 0.504923  [  160/  265]
train() client id: f_00001-2-5 loss: 0.611918  [  192/  265]
train() client id: f_00001-2-6 loss: 0.566166  [  224/  265]
train() client id: f_00001-2-7 loss: 0.564583  [  256/  265]
train() client id: f_00001-3-0 loss: 0.652266  [   32/  265]
train() client id: f_00001-3-1 loss: 0.610024  [   64/  265]
train() client id: f_00001-3-2 loss: 0.709197  [   96/  265]
train() client id: f_00001-3-3 loss: 0.612463  [  128/  265]
train() client id: f_00001-3-4 loss: 0.542019  [  160/  265]
train() client id: f_00001-3-5 loss: 0.505890  [  192/  265]
train() client id: f_00001-3-6 loss: 0.535637  [  224/  265]
train() client id: f_00001-3-7 loss: 0.496315  [  256/  265]
train() client id: f_00001-4-0 loss: 0.540427  [   32/  265]
train() client id: f_00001-4-1 loss: 0.547961  [   64/  265]
train() client id: f_00001-4-2 loss: 0.540375  [   96/  265]
train() client id: f_00001-4-3 loss: 0.490749  [  128/  265]
train() client id: f_00001-4-4 loss: 0.575576  [  160/  265]
train() client id: f_00001-4-5 loss: 0.567135  [  192/  265]
train() client id: f_00001-4-6 loss: 0.671030  [  224/  265]
train() client id: f_00001-4-7 loss: 0.611871  [  256/  265]
train() client id: f_00001-5-0 loss: 0.555840  [   32/  265]
train() client id: f_00001-5-1 loss: 0.550416  [   64/  265]
train() client id: f_00001-5-2 loss: 0.539178  [   96/  265]
train() client id: f_00001-5-3 loss: 0.544615  [  128/  265]
train() client id: f_00001-5-4 loss: 0.602743  [  160/  265]
train() client id: f_00001-5-5 loss: 0.515541  [  192/  265]
train() client id: f_00001-5-6 loss: 0.480940  [  224/  265]
train() client id: f_00001-5-7 loss: 0.797896  [  256/  265]
train() client id: f_00001-6-0 loss: 0.532408  [   32/  265]
train() client id: f_00001-6-1 loss: 0.650940  [   64/  265]
train() client id: f_00001-6-2 loss: 0.555588  [   96/  265]
train() client id: f_00001-6-3 loss: 0.678008  [  128/  265]
train() client id: f_00001-6-4 loss: 0.619993  [  160/  265]
train() client id: f_00001-6-5 loss: 0.479566  [  192/  265]
train() client id: f_00001-6-6 loss: 0.472427  [  224/  265]
train() client id: f_00001-6-7 loss: 0.588972  [  256/  265]
train() client id: f_00001-7-0 loss: 0.475947  [   32/  265]
train() client id: f_00001-7-1 loss: 0.587330  [   64/  265]
train() client id: f_00001-7-2 loss: 0.475125  [   96/  265]
train() client id: f_00001-7-3 loss: 0.476790  [  128/  265]
train() client id: f_00001-7-4 loss: 0.749057  [  160/  265]
train() client id: f_00001-7-5 loss: 0.583263  [  192/  265]
train() client id: f_00001-7-6 loss: 0.574794  [  224/  265]
train() client id: f_00001-7-7 loss: 0.571160  [  256/  265]
train() client id: f_00001-8-0 loss: 0.466972  [   32/  265]
train() client id: f_00001-8-1 loss: 0.538455  [   64/  265]
train() client id: f_00001-8-2 loss: 0.593911  [   96/  265]
train() client id: f_00001-8-3 loss: 0.596656  [  128/  265]
train() client id: f_00001-8-4 loss: 0.545861  [  160/  265]
train() client id: f_00001-8-5 loss: 0.512022  [  192/  265]
train() client id: f_00001-8-6 loss: 0.644838  [  224/  265]
train() client id: f_00001-8-7 loss: 0.557263  [  256/  265]
train() client id: f_00001-9-0 loss: 0.599604  [   32/  265]
train() client id: f_00001-9-1 loss: 0.614604  [   64/  265]
train() client id: f_00001-9-2 loss: 0.685162  [   96/  265]
train() client id: f_00001-9-3 loss: 0.482244  [  128/  265]
train() client id: f_00001-9-4 loss: 0.605415  [  160/  265]
train() client id: f_00001-9-5 loss: 0.474283  [  192/  265]
train() client id: f_00001-9-6 loss: 0.475600  [  224/  265]
train() client id: f_00001-9-7 loss: 0.624652  [  256/  265]
train() client id: f_00001-10-0 loss: 0.593917  [   32/  265]
train() client id: f_00001-10-1 loss: 0.683294  [   64/  265]
train() client id: f_00001-10-2 loss: 0.637015  [   96/  265]
train() client id: f_00001-10-3 loss: 0.463985  [  128/  265]
train() client id: f_00001-10-4 loss: 0.478385  [  160/  265]
train() client id: f_00001-10-5 loss: 0.501799  [  192/  265]
train() client id: f_00001-10-6 loss: 0.478464  [  224/  265]
train() client id: f_00001-10-7 loss: 0.730418  [  256/  265]
train() client id: f_00001-11-0 loss: 0.496140  [   32/  265]
train() client id: f_00001-11-1 loss: 0.584634  [   64/  265]
train() client id: f_00001-11-2 loss: 0.572246  [   96/  265]
train() client id: f_00001-11-3 loss: 0.679632  [  128/  265]
train() client id: f_00001-11-4 loss: 0.542969  [  160/  265]
train() client id: f_00001-11-5 loss: 0.671917  [  192/  265]
train() client id: f_00001-11-6 loss: 0.485083  [  224/  265]
train() client id: f_00001-11-7 loss: 0.534385  [  256/  265]
train() client id: f_00002-0-0 loss: 1.369656  [   32/  124]
train() client id: f_00002-0-1 loss: 1.296255  [   64/  124]
train() client id: f_00002-0-2 loss: 1.154341  [   96/  124]
train() client id: f_00002-1-0 loss: 1.204250  [   32/  124]
train() client id: f_00002-1-1 loss: 1.293911  [   64/  124]
train() client id: f_00002-1-2 loss: 1.253203  [   96/  124]
train() client id: f_00002-2-0 loss: 1.226972  [   32/  124]
train() client id: f_00002-2-1 loss: 1.176731  [   64/  124]
train() client id: f_00002-2-2 loss: 1.201849  [   96/  124]
train() client id: f_00002-3-0 loss: 1.161903  [   32/  124]
train() client id: f_00002-3-1 loss: 1.160360  [   64/  124]
train() client id: f_00002-3-2 loss: 1.203014  [   96/  124]
train() client id: f_00002-4-0 loss: 1.170074  [   32/  124]
train() client id: f_00002-4-1 loss: 1.097661  [   64/  124]
train() client id: f_00002-4-2 loss: 1.207520  [   96/  124]
train() client id: f_00002-5-0 loss: 1.064271  [   32/  124]
train() client id: f_00002-5-1 loss: 1.139785  [   64/  124]
train() client id: f_00002-5-2 loss: 1.041906  [   96/  124]
train() client id: f_00002-6-0 loss: 1.125643  [   32/  124]
train() client id: f_00002-6-1 loss: 1.053109  [   64/  124]
train() client id: f_00002-6-2 loss: 1.148383  [   96/  124]
train() client id: f_00002-7-0 loss: 1.007655  [   32/  124]
train() client id: f_00002-7-1 loss: 1.174580  [   64/  124]
train() client id: f_00002-7-2 loss: 1.126622  [   96/  124]
train() client id: f_00002-8-0 loss: 1.033576  [   32/  124]
train() client id: f_00002-8-1 loss: 1.138400  [   64/  124]
train() client id: f_00002-8-2 loss: 0.971928  [   96/  124]
train() client id: f_00002-9-0 loss: 0.967148  [   32/  124]
train() client id: f_00002-9-1 loss: 1.156011  [   64/  124]
train() client id: f_00002-9-2 loss: 0.995621  [   96/  124]
train() client id: f_00002-10-0 loss: 1.022115  [   32/  124]
train() client id: f_00002-10-1 loss: 0.900144  [   64/  124]
train() client id: f_00002-10-2 loss: 1.143666  [   96/  124]
train() client id: f_00002-11-0 loss: 1.023075  [   32/  124]
train() client id: f_00002-11-1 loss: 0.983645  [   64/  124]
train() client id: f_00002-11-2 loss: 1.006061  [   96/  124]
train() client id: f_00003-0-0 loss: 0.996205  [   32/   43]
train() client id: f_00003-1-0 loss: 1.073447  [   32/   43]
train() client id: f_00003-2-0 loss: 0.925466  [   32/   43]
train() client id: f_00003-3-0 loss: 1.075090  [   32/   43]
train() client id: f_00003-4-0 loss: 0.985157  [   32/   43]
train() client id: f_00003-5-0 loss: 1.077371  [   32/   43]
train() client id: f_00003-6-0 loss: 0.998941  [   32/   43]
train() client id: f_00003-7-0 loss: 0.943784  [   32/   43]
train() client id: f_00003-8-0 loss: 1.082813  [   32/   43]
train() client id: f_00003-9-0 loss: 0.944745  [   32/   43]
train() client id: f_00003-10-0 loss: 0.954921  [   32/   43]
train() client id: f_00003-11-0 loss: 1.022583  [   32/   43]
train() client id: f_00004-0-0 loss: 0.892758  [   32/  306]
train() client id: f_00004-0-1 loss: 0.823910  [   64/  306]
train() client id: f_00004-0-2 loss: 0.888394  [   96/  306]
train() client id: f_00004-0-3 loss: 0.968803  [  128/  306]
train() client id: f_00004-0-4 loss: 0.979022  [  160/  306]
train() client id: f_00004-0-5 loss: 0.890939  [  192/  306]
train() client id: f_00004-0-6 loss: 0.924787  [  224/  306]
train() client id: f_00004-0-7 loss: 0.857975  [  256/  306]
train() client id: f_00004-0-8 loss: 0.836068  [  288/  306]
train() client id: f_00004-1-0 loss: 0.857767  [   32/  306]
train() client id: f_00004-1-1 loss: 0.994239  [   64/  306]
train() client id: f_00004-1-2 loss: 0.756087  [   96/  306]
train() client id: f_00004-1-3 loss: 0.806568  [  128/  306]
train() client id: f_00004-1-4 loss: 0.915255  [  160/  306]
train() client id: f_00004-1-5 loss: 0.791888  [  192/  306]
train() client id: f_00004-1-6 loss: 1.015436  [  224/  306]
train() client id: f_00004-1-7 loss: 0.867176  [  256/  306]
train() client id: f_00004-1-8 loss: 1.009088  [  288/  306]
train() client id: f_00004-2-0 loss: 0.866503  [   32/  306]
train() client id: f_00004-2-1 loss: 0.949826  [   64/  306]
train() client id: f_00004-2-2 loss: 0.881966  [   96/  306]
train() client id: f_00004-2-3 loss: 0.897793  [  128/  306]
train() client id: f_00004-2-4 loss: 0.943148  [  160/  306]
train() client id: f_00004-2-5 loss: 0.928191  [  192/  306]
train() client id: f_00004-2-6 loss: 0.893516  [  224/  306]
train() client id: f_00004-2-7 loss: 0.869305  [  256/  306]
train() client id: f_00004-2-8 loss: 0.857680  [  288/  306]
train() client id: f_00004-3-0 loss: 0.848391  [   32/  306]
train() client id: f_00004-3-1 loss: 0.990614  [   64/  306]
train() client id: f_00004-3-2 loss: 0.874351  [   96/  306]
train() client id: f_00004-3-3 loss: 0.836603  [  128/  306]
train() client id: f_00004-3-4 loss: 0.815597  [  160/  306]
train() client id: f_00004-3-5 loss: 0.891144  [  192/  306]
train() client id: f_00004-3-6 loss: 0.957882  [  224/  306]
train() client id: f_00004-3-7 loss: 0.864809  [  256/  306]
train() client id: f_00004-3-8 loss: 0.967449  [  288/  306]
train() client id: f_00004-4-0 loss: 0.855884  [   32/  306]
train() client id: f_00004-4-1 loss: 0.813152  [   64/  306]
train() client id: f_00004-4-2 loss: 0.946232  [   96/  306]
train() client id: f_00004-4-3 loss: 0.923424  [  128/  306]
train() client id: f_00004-4-4 loss: 0.902847  [  160/  306]
train() client id: f_00004-4-5 loss: 0.962213  [  192/  306]
train() client id: f_00004-4-6 loss: 0.917144  [  224/  306]
train() client id: f_00004-4-7 loss: 0.963901  [  256/  306]
train() client id: f_00004-4-8 loss: 0.764986  [  288/  306]
train() client id: f_00004-5-0 loss: 0.946153  [   32/  306]
train() client id: f_00004-5-1 loss: 0.895575  [   64/  306]
train() client id: f_00004-5-2 loss: 0.876070  [   96/  306]
train() client id: f_00004-5-3 loss: 0.943924  [  128/  306]
train() client id: f_00004-5-4 loss: 0.966305  [  160/  306]
train() client id: f_00004-5-5 loss: 0.836423  [  192/  306]
train() client id: f_00004-5-6 loss: 0.927274  [  224/  306]
train() client id: f_00004-5-7 loss: 0.785702  [  256/  306]
train() client id: f_00004-5-8 loss: 0.881278  [  288/  306]
train() client id: f_00004-6-0 loss: 0.809091  [   32/  306]
train() client id: f_00004-6-1 loss: 0.860916  [   64/  306]
train() client id: f_00004-6-2 loss: 0.866476  [   96/  306]
train() client id: f_00004-6-3 loss: 0.944419  [  128/  306]
train() client id: f_00004-6-4 loss: 1.009566  [  160/  306]
train() client id: f_00004-6-5 loss: 0.856357  [  192/  306]
train() client id: f_00004-6-6 loss: 0.849432  [  224/  306]
train() client id: f_00004-6-7 loss: 0.889959  [  256/  306]
train() client id: f_00004-6-8 loss: 0.982571  [  288/  306]
train() client id: f_00004-7-0 loss: 0.775270  [   32/  306]
train() client id: f_00004-7-1 loss: 0.928982  [   64/  306]
train() client id: f_00004-7-2 loss: 0.950793  [   96/  306]
train() client id: f_00004-7-3 loss: 0.846154  [  128/  306]
train() client id: f_00004-7-4 loss: 0.845794  [  160/  306]
train() client id: f_00004-7-5 loss: 0.991142  [  192/  306]
train() client id: f_00004-7-6 loss: 0.837857  [  224/  306]
train() client id: f_00004-7-7 loss: 0.953899  [  256/  306]
train() client id: f_00004-7-8 loss: 1.010170  [  288/  306]
train() client id: f_00004-8-0 loss: 0.849985  [   32/  306]
train() client id: f_00004-8-1 loss: 0.993171  [   64/  306]
train() client id: f_00004-8-2 loss: 0.816407  [   96/  306]
train() client id: f_00004-8-3 loss: 0.866633  [  128/  306]
train() client id: f_00004-8-4 loss: 0.893134  [  160/  306]
train() client id: f_00004-8-5 loss: 0.717935  [  192/  306]
train() client id: f_00004-8-6 loss: 0.950359  [  224/  306]
train() client id: f_00004-8-7 loss: 0.994450  [  256/  306]
train() client id: f_00004-8-8 loss: 0.927644  [  288/  306]
train() client id: f_00004-9-0 loss: 0.923929  [   32/  306]
train() client id: f_00004-9-1 loss: 0.927534  [   64/  306]
train() client id: f_00004-9-2 loss: 0.916803  [   96/  306]
train() client id: f_00004-9-3 loss: 0.890127  [  128/  306]
train() client id: f_00004-9-4 loss: 0.844807  [  160/  306]
train() client id: f_00004-9-5 loss: 0.883675  [  192/  306]
train() client id: f_00004-9-6 loss: 0.884995  [  224/  306]
train() client id: f_00004-9-7 loss: 0.864965  [  256/  306]
train() client id: f_00004-9-8 loss: 0.866866  [  288/  306]
train() client id: f_00004-10-0 loss: 0.845461  [   32/  306]
train() client id: f_00004-10-1 loss: 0.989122  [   64/  306]
train() client id: f_00004-10-2 loss: 0.893580  [   96/  306]
train() client id: f_00004-10-3 loss: 0.828217  [  128/  306]
train() client id: f_00004-10-4 loss: 0.938043  [  160/  306]
train() client id: f_00004-10-5 loss: 0.952291  [  192/  306]
train() client id: f_00004-10-6 loss: 0.840441  [  224/  306]
train() client id: f_00004-10-7 loss: 0.890033  [  256/  306]
train() client id: f_00004-10-8 loss: 0.804849  [  288/  306]
train() client id: f_00004-11-0 loss: 0.817588  [   32/  306]
train() client id: f_00004-11-1 loss: 0.908538  [   64/  306]
train() client id: f_00004-11-2 loss: 0.816418  [   96/  306]
train() client id: f_00004-11-3 loss: 0.988194  [  128/  306]
train() client id: f_00004-11-4 loss: 0.891117  [  160/  306]
train() client id: f_00004-11-5 loss: 0.901431  [  192/  306]
train() client id: f_00004-11-6 loss: 0.812982  [  224/  306]
train() client id: f_00004-11-7 loss: 0.907637  [  256/  306]
train() client id: f_00004-11-8 loss: 0.952593  [  288/  306]
train() client id: f_00005-0-0 loss: 0.900978  [   32/  146]
train() client id: f_00005-0-1 loss: 0.668984  [   64/  146]
train() client id: f_00005-0-2 loss: 0.687757  [   96/  146]
train() client id: f_00005-0-3 loss: 0.955468  [  128/  146]
train() client id: f_00005-1-0 loss: 0.739911  [   32/  146]
train() client id: f_00005-1-1 loss: 0.750319  [   64/  146]
train() client id: f_00005-1-2 loss: 0.755678  [   96/  146]
train() client id: f_00005-1-3 loss: 0.885942  [  128/  146]
train() client id: f_00005-2-0 loss: 0.785992  [   32/  146]
train() client id: f_00005-2-1 loss: 0.801020  [   64/  146]
train() client id: f_00005-2-2 loss: 0.717876  [   96/  146]
train() client id: f_00005-2-3 loss: 0.867860  [  128/  146]
train() client id: f_00005-3-0 loss: 1.078038  [   32/  146]
train() client id: f_00005-3-1 loss: 0.721873  [   64/  146]
train() client id: f_00005-3-2 loss: 0.776224  [   96/  146]
train() client id: f_00005-3-3 loss: 0.721699  [  128/  146]
train() client id: f_00005-4-0 loss: 0.688936  [   32/  146]
train() client id: f_00005-4-1 loss: 0.827265  [   64/  146]
train() client id: f_00005-4-2 loss: 0.797825  [   96/  146]
train() client id: f_00005-4-3 loss: 0.872747  [  128/  146]
train() client id: f_00005-5-0 loss: 0.698576  [   32/  146]
train() client id: f_00005-5-1 loss: 0.647661  [   64/  146]
train() client id: f_00005-5-2 loss: 1.010284  [   96/  146]
train() client id: f_00005-5-3 loss: 0.878085  [  128/  146]
train() client id: f_00005-6-0 loss: 0.753701  [   32/  146]
train() client id: f_00005-6-1 loss: 0.765222  [   64/  146]
train() client id: f_00005-6-2 loss: 0.626899  [   96/  146]
train() client id: f_00005-6-3 loss: 0.760936  [  128/  146]
train() client id: f_00005-7-0 loss: 0.678165  [   32/  146]
train() client id: f_00005-7-1 loss: 0.775622  [   64/  146]
train() client id: f_00005-7-2 loss: 0.882173  [   96/  146]
train() client id: f_00005-7-3 loss: 0.843792  [  128/  146]
train() client id: f_00005-8-0 loss: 0.805502  [   32/  146]
train() client id: f_00005-8-1 loss: 0.737610  [   64/  146]
train() client id: f_00005-8-2 loss: 0.822747  [   96/  146]
train() client id: f_00005-8-3 loss: 0.717013  [  128/  146]
train() client id: f_00005-9-0 loss: 1.002215  [   32/  146]
train() client id: f_00005-9-1 loss: 0.708553  [   64/  146]
train() client id: f_00005-9-2 loss: 0.712057  [   96/  146]
train() client id: f_00005-9-3 loss: 0.812361  [  128/  146]
train() client id: f_00005-10-0 loss: 0.909332  [   32/  146]
train() client id: f_00005-10-1 loss: 0.643617  [   64/  146]
train() client id: f_00005-10-2 loss: 0.779320  [   96/  146]
train() client id: f_00005-10-3 loss: 0.707373  [  128/  146]
train() client id: f_00005-11-0 loss: 0.732168  [   32/  146]
train() client id: f_00005-11-1 loss: 1.008151  [   64/  146]
train() client id: f_00005-11-2 loss: 0.700900  [   96/  146]
train() client id: f_00005-11-3 loss: 0.572807  [  128/  146]
train() client id: f_00006-0-0 loss: 0.791120  [   32/   54]
train() client id: f_00006-1-0 loss: 0.731537  [   32/   54]
train() client id: f_00006-2-0 loss: 0.768796  [   32/   54]
train() client id: f_00006-3-0 loss: 0.749490  [   32/   54]
train() client id: f_00006-4-0 loss: 0.783786  [   32/   54]
train() client id: f_00006-5-0 loss: 0.788803  [   32/   54]
train() client id: f_00006-6-0 loss: 0.756090  [   32/   54]
train() client id: f_00006-7-0 loss: 0.786983  [   32/   54]
train() client id: f_00006-8-0 loss: 0.764689  [   32/   54]
train() client id: f_00006-9-0 loss: 0.754684  [   32/   54]
train() client id: f_00006-10-0 loss: 0.785935  [   32/   54]
train() client id: f_00006-11-0 loss: 0.752630  [   32/   54]
train() client id: f_00007-0-0 loss: 0.631552  [   32/  179]
train() client id: f_00007-0-1 loss: 0.662165  [   64/  179]
train() client id: f_00007-0-2 loss: 0.677272  [   96/  179]
train() client id: f_00007-0-3 loss: 0.824374  [  128/  179]
train() client id: f_00007-0-4 loss: 0.585627  [  160/  179]
train() client id: f_00007-1-0 loss: 0.771972  [   32/  179]
train() client id: f_00007-1-1 loss: 0.601706  [   64/  179]
train() client id: f_00007-1-2 loss: 0.636981  [   96/  179]
train() client id: f_00007-1-3 loss: 0.704522  [  128/  179]
train() client id: f_00007-1-4 loss: 0.639376  [  160/  179]
train() client id: f_00007-2-0 loss: 0.651536  [   32/  179]
train() client id: f_00007-2-1 loss: 0.563563  [   64/  179]
train() client id: f_00007-2-2 loss: 0.680868  [   96/  179]
train() client id: f_00007-2-3 loss: 0.615302  [  128/  179]
train() client id: f_00007-2-4 loss: 0.846130  [  160/  179]
train() client id: f_00007-3-0 loss: 0.688591  [   32/  179]
train() client id: f_00007-3-1 loss: 0.553588  [   64/  179]
train() client id: f_00007-3-2 loss: 0.754701  [   96/  179]
train() client id: f_00007-3-3 loss: 0.540103  [  128/  179]
train() client id: f_00007-3-4 loss: 0.552658  [  160/  179]
train() client id: f_00007-4-0 loss: 0.775541  [   32/  179]
train() client id: f_00007-4-1 loss: 0.614109  [   64/  179]
train() client id: f_00007-4-2 loss: 0.653778  [   96/  179]
train() client id: f_00007-4-3 loss: 0.532937  [  128/  179]
train() client id: f_00007-4-4 loss: 0.589948  [  160/  179]
train() client id: f_00007-5-0 loss: 0.525937  [   32/  179]
train() client id: f_00007-5-1 loss: 0.735116  [   64/  179]
train() client id: f_00007-5-2 loss: 0.530497  [   96/  179]
train() client id: f_00007-5-3 loss: 0.733373  [  128/  179]
train() client id: f_00007-5-4 loss: 0.663603  [  160/  179]
train() client id: f_00007-6-0 loss: 0.583877  [   32/  179]
train() client id: f_00007-6-1 loss: 0.581498  [   64/  179]
train() client id: f_00007-6-2 loss: 0.599601  [   96/  179]
train() client id: f_00007-6-3 loss: 0.731118  [  128/  179]
train() client id: f_00007-6-4 loss: 0.659501  [  160/  179]
train() client id: f_00007-7-0 loss: 0.509356  [   32/  179]
train() client id: f_00007-7-1 loss: 0.586595  [   64/  179]
train() client id: f_00007-7-2 loss: 0.597063  [   96/  179]
train() client id: f_00007-7-3 loss: 0.667192  [  128/  179]
train() client id: f_00007-7-4 loss: 0.577672  [  160/  179]
train() client id: f_00007-8-0 loss: 0.501461  [   32/  179]
train() client id: f_00007-8-1 loss: 0.491250  [   64/  179]
train() client id: f_00007-8-2 loss: 0.816510  [   96/  179]
train() client id: f_00007-8-3 loss: 0.674093  [  128/  179]
train() client id: f_00007-8-4 loss: 0.577762  [  160/  179]
train() client id: f_00007-9-0 loss: 0.580161  [   32/  179]
train() client id: f_00007-9-1 loss: 0.494294  [   64/  179]
train() client id: f_00007-9-2 loss: 0.635945  [   96/  179]
train() client id: f_00007-9-3 loss: 0.563426  [  128/  179]
train() client id: f_00007-9-4 loss: 0.662440  [  160/  179]
train() client id: f_00007-10-0 loss: 0.555567  [   32/  179]
train() client id: f_00007-10-1 loss: 0.667932  [   64/  179]
train() client id: f_00007-10-2 loss: 0.498046  [   96/  179]
train() client id: f_00007-10-3 loss: 0.496267  [  128/  179]
train() client id: f_00007-10-4 loss: 0.738408  [  160/  179]
train() client id: f_00007-11-0 loss: 0.644405  [   32/  179]
train() client id: f_00007-11-1 loss: 0.666699  [   64/  179]
train() client id: f_00007-11-2 loss: 0.502036  [   96/  179]
train() client id: f_00007-11-3 loss: 0.558270  [  128/  179]
train() client id: f_00007-11-4 loss: 0.741026  [  160/  179]
train() client id: f_00008-0-0 loss: 0.652428  [   32/  130]
train() client id: f_00008-0-1 loss: 0.782380  [   64/  130]
train() client id: f_00008-0-2 loss: 0.827248  [   96/  130]
train() client id: f_00008-0-3 loss: 0.691679  [  128/  130]
train() client id: f_00008-1-0 loss: 0.632005  [   32/  130]
train() client id: f_00008-1-1 loss: 0.865184  [   64/  130]
train() client id: f_00008-1-2 loss: 0.754995  [   96/  130]
train() client id: f_00008-1-3 loss: 0.692233  [  128/  130]
train() client id: f_00008-2-0 loss: 0.643617  [   32/  130]
train() client id: f_00008-2-1 loss: 0.728797  [   64/  130]
train() client id: f_00008-2-2 loss: 0.767291  [   96/  130]
train() client id: f_00008-2-3 loss: 0.763007  [  128/  130]
train() client id: f_00008-3-0 loss: 0.838991  [   32/  130]
train() client id: f_00008-3-1 loss: 0.612518  [   64/  130]
train() client id: f_00008-3-2 loss: 0.739908  [   96/  130]
train() client id: f_00008-3-3 loss: 0.712176  [  128/  130]
train() client id: f_00008-4-0 loss: 0.704212  [   32/  130]
train() client id: f_00008-4-1 loss: 0.682711  [   64/  130]
train() client id: f_00008-4-2 loss: 0.722052  [   96/  130]
train() client id: f_00008-4-3 loss: 0.775531  [  128/  130]
train() client id: f_00008-5-0 loss: 0.821713  [   32/  130]
train() client id: f_00008-5-1 loss: 0.672659  [   64/  130]
train() client id: f_00008-5-2 loss: 0.632917  [   96/  130]
train() client id: f_00008-5-3 loss: 0.759156  [  128/  130]
train() client id: f_00008-6-0 loss: 0.795545  [   32/  130]
train() client id: f_00008-6-1 loss: 0.669427  [   64/  130]
train() client id: f_00008-6-2 loss: 0.665641  [   96/  130]
train() client id: f_00008-6-3 loss: 0.741748  [  128/  130]
train() client id: f_00008-7-0 loss: 0.574999  [   32/  130]
train() client id: f_00008-7-1 loss: 0.647857  [   64/  130]
train() client id: f_00008-7-2 loss: 0.823837  [   96/  130]
train() client id: f_00008-7-3 loss: 0.855637  [  128/  130]
train() client id: f_00008-8-0 loss: 0.714686  [   32/  130]
train() client id: f_00008-8-1 loss: 0.740089  [   64/  130]
train() client id: f_00008-8-2 loss: 0.737759  [   96/  130]
train() client id: f_00008-8-3 loss: 0.714449  [  128/  130]
train() client id: f_00008-9-0 loss: 0.742660  [   32/  130]
train() client id: f_00008-9-1 loss: 0.631963  [   64/  130]
train() client id: f_00008-9-2 loss: 0.796946  [   96/  130]
train() client id: f_00008-9-3 loss: 0.721416  [  128/  130]
train() client id: f_00008-10-0 loss: 0.826559  [   32/  130]
train() client id: f_00008-10-1 loss: 0.697092  [   64/  130]
train() client id: f_00008-10-2 loss: 0.716524  [   96/  130]
train() client id: f_00008-10-3 loss: 0.655024  [  128/  130]
train() client id: f_00008-11-0 loss: 0.597229  [   32/  130]
train() client id: f_00008-11-1 loss: 0.740816  [   64/  130]
train() client id: f_00008-11-2 loss: 0.748562  [   96/  130]
train() client id: f_00008-11-3 loss: 0.778070  [  128/  130]
train() client id: f_00009-0-0 loss: 1.279743  [   32/  118]
train() client id: f_00009-0-1 loss: 1.183107  [   64/  118]
train() client id: f_00009-0-2 loss: 1.073129  [   96/  118]
train() client id: f_00009-1-0 loss: 1.185589  [   32/  118]
train() client id: f_00009-1-1 loss: 1.121498  [   64/  118]
train() client id: f_00009-1-2 loss: 1.105146  [   96/  118]
train() client id: f_00009-2-0 loss: 1.058722  [   32/  118]
train() client id: f_00009-2-1 loss: 1.159836  [   64/  118]
train() client id: f_00009-2-2 loss: 1.089960  [   96/  118]
train() client id: f_00009-3-0 loss: 0.953859  [   32/  118]
train() client id: f_00009-3-1 loss: 1.069500  [   64/  118]
train() client id: f_00009-3-2 loss: 1.146853  [   96/  118]
train() client id: f_00009-4-0 loss: 1.082363  [   32/  118]
train() client id: f_00009-4-1 loss: 1.000244  [   64/  118]
train() client id: f_00009-4-2 loss: 0.963185  [   96/  118]
train() client id: f_00009-5-0 loss: 1.018803  [   32/  118]
train() client id: f_00009-5-1 loss: 0.913488  [   64/  118]
train() client id: f_00009-5-2 loss: 1.050991  [   96/  118]
train() client id: f_00009-6-0 loss: 0.898833  [   32/  118]
train() client id: f_00009-6-1 loss: 0.869242  [   64/  118]
train() client id: f_00009-6-2 loss: 1.020734  [   96/  118]
train() client id: f_00009-7-0 loss: 1.021673  [   32/  118]
train() client id: f_00009-7-1 loss: 0.900786  [   64/  118]
train() client id: f_00009-7-2 loss: 0.861007  [   96/  118]
train() client id: f_00009-8-0 loss: 0.970956  [   32/  118]
train() client id: f_00009-8-1 loss: 0.849197  [   64/  118]
train() client id: f_00009-8-2 loss: 0.947569  [   96/  118]
train() client id: f_00009-9-0 loss: 0.888312  [   32/  118]
train() client id: f_00009-9-1 loss: 0.991252  [   64/  118]
train() client id: f_00009-9-2 loss: 0.980736  [   96/  118]
train() client id: f_00009-10-0 loss: 0.881786  [   32/  118]
train() client id: f_00009-10-1 loss: 1.014244  [   64/  118]
train() client id: f_00009-10-2 loss: 0.857604  [   96/  118]
train() client id: f_00009-11-0 loss: 0.835069  [   32/  118]
train() client id: f_00009-11-1 loss: 0.926143  [   64/  118]
train() client id: f_00009-11-2 loss: 1.021961  [   96/  118]
At round 9 accuracy: 0.6312997347480106
At round 9 training accuracy: 0.5774647887323944
At round 9 training loss: 0.8804820209348493
update_location
xs = -3.905658 4.200318 65.009024 18.811294 0.979296 3.956410 -27.443192 -6.324852 49.663977 2.939121 
ys = 57.587959 40.555839 1.320614 -27.455176 19.350187 2.814151 -2.624984 0.822348 17.569006 -0.998518 
dists_uav = 115.462666 107.992679 119.280833 105.392844 101.859652 100.117794 103.730513 100.203194 113.027344 100.048166 
dists_bs = 207.506291 224.026754 296.258098 280.269363 234.937390 248.341224 230.919134 242.464760 274.332682 250.275467 
uav_gains = -101.561164 -100.834906 -101.914463 -100.570312 -100.200075 -100.012798 -100.397690 -100.022055 -101.329680 -100.005244 
bs_gains = -104.443935 -105.375456 -108.773826 -108.099177 -105.953719 -106.628426 -105.743937 -106.337221 -107.838831 -106.722771 
Round 10
-------------------------------
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.67144915 20.18897006  9.52328793  3.40449035 23.28757639 11.22831614
  4.23293599 13.65794809 10.03742584  9.11771254]
obj_prev = 114.35011248363482
eta_min = 2.485131023397881e-10	eta_max = 0.9203875705067922
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 26.5917511352464	eta = 0.909090909090909
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 46.02197555438805	eta = 0.5252777379209097
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 36.745409717816976	eta = 0.6578867782263103
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 35.08220734696584	eta = 0.6890763450194114
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 35.00007514490989	eta = 0.6906933517648766
af = 24.17431921386036	bf = 1.8797572485696508	zeta = 34.99986062549486	eta = 0.690697585128414
eta = 0.690697585128414
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [0.0305359  0.06422234 0.03005121 0.01042098 0.07415862 0.03538287
 0.01308682 0.04338037 0.0315053  0.02859712]
ene_total = [2.98353072 5.7890883  2.95330941 1.35079969 6.60483527 3.5266156
 1.5604156  3.97716561 3.26691924 2.98718119]
ti_comp = [0.30116936 0.28752245 0.30009808 0.30400477 0.28497028 0.28178895
 0.30447482 0.30547464 0.27545826 0.28132541]
ti_coms = [0.06738721 0.08103412 0.06845849 0.0645518  0.08358629 0.08676762
 0.06408176 0.06308193 0.09309831 0.08723116]
t_total = [29.49995804 29.49995804 29.49995804 29.49995804 29.49995804 29.49995804
 29.49995804 29.49995804 29.49995804 29.49995804]
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [1.96196272e-05 2.00260269e-04 1.88338590e-05 7.65323874e-07
 3.13881583e-04 3.48667396e-05 1.51105132e-06 5.46776000e-05
 2.57584149e-05 1.84683789e-05]
ene_total = [0.54095026 0.66464151 0.5494621  0.51674598 0.69416402 0.6972951
 0.51304332 0.50929606 0.74723805 0.69969276]
optimize_network iter = 0 obj = 6.132529161591935
eta = 0.690697585128414
freqs = [5.06955673e+07 1.11682299e+08 5.00689748e+07 1.71395055e+07
 1.30116415e+08 6.27825752e+07 2.14908014e+07 7.10048625e+07
 5.71870607e+07 5.08256882e+07]
eta_min = 0.6906975851284028	eta_max = 0.6906975851284034
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 0.050678425323249086	eta = 0.9090909090909091
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 20.72563143256374	eta = 0.002222913974819694
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 2.1660187348301907	eta = 0.02127003566846808
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 2.101859368191834	eta = 0.021919304614581324
af = 0.04607129574840826	bf = 1.8797572485696508	zeta = 2.101838286505997	eta = 0.02191952446779107
eta = 0.02191952446779107
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [2.02550289e-04 2.06745903e-03 1.94438128e-04 7.90109671e-06
 3.24046959e-03 3.59959346e-04 1.55998826e-05 5.64483901e-04
 2.65926276e-04 1.90664962e-04]
ene_total = [0.17569726 0.2574448  0.17820355 0.16359333 0.29359605 0.22873723
 0.16259842 0.17396099 0.24238129 0.22562536]
ti_comp = [0.30116936 0.28752245 0.30009808 0.30400477 0.28497028 0.28178895
 0.30447482 0.30547464 0.27545826 0.28132541]
ti_coms = [0.06738721 0.08103412 0.06845849 0.0645518  0.08358629 0.08676762
 0.06408176 0.06308193 0.09309831 0.08723116]
t_total = [29.49995804 29.49995804 29.49995804 29.49995804 29.49995804 29.49995804
 29.49995804 29.49995804 29.49995804 29.49995804]
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [1.96196272e-05 2.00260269e-04 1.88338590e-05 7.65323874e-07
 3.13881583e-04 3.48667396e-05 1.51105132e-06 5.46776000e-05
 2.57584149e-05 1.84683789e-05]
ene_total = [0.54095026 0.66464151 0.5494621  0.51674598 0.69416402 0.6972951
 0.51304332 0.50929606 0.74723805 0.69969276]
optimize_network iter = 1 obj = 6.132529161591719
eta = 0.6906975851284028
freqs = [5.06955673e+07 1.11682299e+08 5.00689748e+07 1.71395055e+07
 1.30116415e+08 6.27825752e+07 2.14908014e+07 7.10048625e+07
 5.71870607e+07 5.08256882e+07]
Done!
ene_coms = [0.00673872 0.00810341 0.00684585 0.00645518 0.00835863 0.00867676
 0.00640818 0.00630819 0.00930983 0.00872312]
ene_comp = [1.94295065e-05 1.98319681e-04 1.86513526e-05 7.57907632e-07
 3.10839966e-04 3.45288693e-05 1.49640873e-06 5.41477560e-05
 2.55088073e-05 1.82894141e-05]
ene_total = [0.00675815 0.00830173 0.0068645  0.00645594 0.00866947 0.00871129
 0.00640967 0.00636234 0.00933534 0.00874141]
At round 10 energy consumption: 0.07660983999268792
At round 10 eta: 0.6906975851284028
At round 10 a_n: 24.7571443865612
At round 10 local rounds: 12.11742183622639
At round 10 global rounds: 80.0418722784263
gradient difference: 0.42519450187683105
train() client id: f_00000-0-0 loss: 1.540802  [   32/  126]
train() client id: f_00000-0-1 loss: 1.391635  [   64/  126]
train() client id: f_00000-0-2 loss: 1.310962  [   96/  126]
train() client id: f_00000-1-0 loss: 1.308355  [   32/  126]
train() client id: f_00000-1-1 loss: 1.262763  [   64/  126]
train() client id: f_00000-1-2 loss: 1.143010  [   96/  126]
train() client id: f_00000-2-0 loss: 1.132865  [   32/  126]
train() client id: f_00000-2-1 loss: 1.189193  [   64/  126]
train() client id: f_00000-2-2 loss: 1.191764  [   96/  126]
train() client id: f_00000-3-0 loss: 1.137934  [   32/  126]
train() client id: f_00000-3-1 loss: 0.964851  [   64/  126]
train() client id: f_00000-3-2 loss: 1.066522  [   96/  126]
train() client id: f_00000-4-0 loss: 0.961797  [   32/  126]
train() client id: f_00000-4-1 loss: 0.999544  [   64/  126]
train() client id: f_00000-4-2 loss: 1.011357  [   96/  126]
train() client id: f_00000-5-0 loss: 0.913815  [   32/  126]
train() client id: f_00000-5-1 loss: 0.979571  [   64/  126]
train() client id: f_00000-5-2 loss: 0.923458  [   96/  126]
train() client id: f_00000-6-0 loss: 0.909418  [   32/  126]
train() client id: f_00000-6-1 loss: 0.873193  [   64/  126]
train() client id: f_00000-6-2 loss: 0.890060  [   96/  126]
train() client id: f_00000-7-0 loss: 0.858602  [   32/  126]
train() client id: f_00000-7-1 loss: 0.815183  [   64/  126]
train() client id: f_00000-7-2 loss: 0.868079  [   96/  126]
train() client id: f_00000-8-0 loss: 0.806124  [   32/  126]
train() client id: f_00000-8-1 loss: 0.843618  [   64/  126]
train() client id: f_00000-8-2 loss: 0.905541  [   96/  126]
train() client id: f_00000-9-0 loss: 0.877208  [   32/  126]
train() client id: f_00000-9-1 loss: 0.803008  [   64/  126]
train() client id: f_00000-9-2 loss: 0.771954  [   96/  126]
train() client id: f_00000-10-0 loss: 0.831498  [   32/  126]
train() client id: f_00000-10-1 loss: 0.846040  [   64/  126]
train() client id: f_00000-10-2 loss: 0.777620  [   96/  126]
train() client id: f_00000-11-0 loss: 0.825859  [   32/  126]
train() client id: f_00000-11-1 loss: 0.759929  [   64/  126]
train() client id: f_00000-11-2 loss: 0.821329  [   96/  126]
train() client id: f_00001-0-0 loss: 0.646917  [   32/  265]
train() client id: f_00001-0-1 loss: 0.481895  [   64/  265]
train() client id: f_00001-0-2 loss: 0.474639  [   96/  265]
train() client id: f_00001-0-3 loss: 0.571855  [  128/  265]
train() client id: f_00001-0-4 loss: 0.474002  [  160/  265]
train() client id: f_00001-0-5 loss: 0.622786  [  192/  265]
train() client id: f_00001-0-6 loss: 0.513286  [  224/  265]
train() client id: f_00001-0-7 loss: 0.616677  [  256/  265]
train() client id: f_00001-1-0 loss: 0.507928  [   32/  265]
train() client id: f_00001-1-1 loss: 0.588610  [   64/  265]
train() client id: f_00001-1-2 loss: 0.468764  [   96/  265]
train() client id: f_00001-1-3 loss: 0.465911  [  128/  265]
train() client id: f_00001-1-4 loss: 0.617984  [  160/  265]
train() client id: f_00001-1-5 loss: 0.438618  [  192/  265]
train() client id: f_00001-1-6 loss: 0.610635  [  224/  265]
train() client id: f_00001-1-7 loss: 0.451580  [  256/  265]
train() client id: f_00001-2-0 loss: 0.549724  [   32/  265]
train() client id: f_00001-2-1 loss: 0.603024  [   64/  265]
train() client id: f_00001-2-2 loss: 0.556741  [   96/  265]
train() client id: f_00001-2-3 loss: 0.441248  [  128/  265]
train() client id: f_00001-2-4 loss: 0.538424  [  160/  265]
train() client id: f_00001-2-5 loss: 0.484557  [  192/  265]
train() client id: f_00001-2-6 loss: 0.503476  [  224/  265]
train() client id: f_00001-2-7 loss: 0.516375  [  256/  265]
train() client id: f_00001-3-0 loss: 0.548238  [   32/  265]
train() client id: f_00001-3-1 loss: 0.489269  [   64/  265]
train() client id: f_00001-3-2 loss: 0.620638  [   96/  265]
train() client id: f_00001-3-3 loss: 0.435353  [  128/  265]
train() client id: f_00001-3-4 loss: 0.483075  [  160/  265]
train() client id: f_00001-3-5 loss: 0.483300  [  192/  265]
train() client id: f_00001-3-6 loss: 0.462411  [  224/  265]
train() client id: f_00001-3-7 loss: 0.618012  [  256/  265]
train() client id: f_00001-4-0 loss: 0.608421  [   32/  265]
train() client id: f_00001-4-1 loss: 0.463323  [   64/  265]
train() client id: f_00001-4-2 loss: 0.602693  [   96/  265]
train() client id: f_00001-4-3 loss: 0.428973  [  128/  265]
train() client id: f_00001-4-4 loss: 0.483709  [  160/  265]
train() client id: f_00001-4-5 loss: 0.596505  [  192/  265]
train() client id: f_00001-4-6 loss: 0.414166  [  224/  265]
train() client id: f_00001-4-7 loss: 0.510090  [  256/  265]
train() client id: f_00001-5-0 loss: 0.502097  [   32/  265]
train() client id: f_00001-5-1 loss: 0.574491  [   64/  265]
train() client id: f_00001-5-2 loss: 0.532147  [   96/  265]
train() client id: f_00001-5-3 loss: 0.415651  [  128/  265]
train() client id: f_00001-5-4 loss: 0.543685  [  160/  265]
train() client id: f_00001-5-5 loss: 0.505427  [  192/  265]
train() client id: f_00001-5-6 loss: 0.548132  [  224/  265]
train() client id: f_00001-5-7 loss: 0.443059  [  256/  265]
train() client id: f_00001-6-0 loss: 0.461916  [   32/  265]
train() client id: f_00001-6-1 loss: 0.548756  [   64/  265]
train() client id: f_00001-6-2 loss: 0.430094  [   96/  265]
train() client id: f_00001-6-3 loss: 0.586835  [  128/  265]
train() client id: f_00001-6-4 loss: 0.506277  [  160/  265]
train() client id: f_00001-6-5 loss: 0.538043  [  192/  265]
train() client id: f_00001-6-6 loss: 0.459331  [  224/  265]
train() client id: f_00001-6-7 loss: 0.527043  [  256/  265]
train() client id: f_00001-7-0 loss: 0.474942  [   32/  265]
train() client id: f_00001-7-1 loss: 0.420492  [   64/  265]
train() client id: f_00001-7-2 loss: 0.482374  [   96/  265]
train() client id: f_00001-7-3 loss: 0.536120  [  128/  265]
train() client id: f_00001-7-4 loss: 0.500248  [  160/  265]
train() client id: f_00001-7-5 loss: 0.591279  [  192/  265]
train() client id: f_00001-7-6 loss: 0.477437  [  224/  265]
train() client id: f_00001-7-7 loss: 0.481146  [  256/  265]
train() client id: f_00001-8-0 loss: 0.526784  [   32/  265]
train() client id: f_00001-8-1 loss: 0.604121  [   64/  265]
train() client id: f_00001-8-2 loss: 0.491478  [   96/  265]
train() client id: f_00001-8-3 loss: 0.425818  [  128/  265]
train() client id: f_00001-8-4 loss: 0.456201  [  160/  265]
train() client id: f_00001-8-5 loss: 0.578106  [  192/  265]
train() client id: f_00001-8-6 loss: 0.396166  [  224/  265]
train() client id: f_00001-8-7 loss: 0.550784  [  256/  265]
train() client id: f_00001-9-0 loss: 0.489705  [   32/  265]
train() client id: f_00001-9-1 loss: 0.556767  [   64/  265]
train() client id: f_00001-9-2 loss: 0.462240  [   96/  265]
train() client id: f_00001-9-3 loss: 0.503446  [  128/  265]
train() client id: f_00001-9-4 loss: 0.411775  [  160/  265]
train() client id: f_00001-9-5 loss: 0.586721  [  192/  265]
train() client id: f_00001-9-6 loss: 0.601563  [  224/  265]
train() client id: f_00001-9-7 loss: 0.412437  [  256/  265]
train() client id: f_00001-10-0 loss: 0.408922  [   32/  265]
train() client id: f_00001-10-1 loss: 0.520421  [   64/  265]
train() client id: f_00001-10-2 loss: 0.585300  [   96/  265]
train() client id: f_00001-10-3 loss: 0.460100  [  128/  265]
train() client id: f_00001-10-4 loss: 0.478379  [  160/  265]
train() client id: f_00001-10-5 loss: 0.563143  [  192/  265]
train() client id: f_00001-10-6 loss: 0.517314  [  224/  265]
train() client id: f_00001-10-7 loss: 0.417620  [  256/  265]
train() client id: f_00001-11-0 loss: 0.464058  [   32/  265]
train() client id: f_00001-11-1 loss: 0.539364  [   64/  265]
train() client id: f_00001-11-2 loss: 0.509044  [   96/  265]
train() client id: f_00001-11-3 loss: 0.463573  [  128/  265]
train() client id: f_00001-11-4 loss: 0.467315  [  160/  265]
train() client id: f_00001-11-5 loss: 0.568373  [  192/  265]
train() client id: f_00001-11-6 loss: 0.466124  [  224/  265]
train() client id: f_00001-11-7 loss: 0.412610  [  256/  265]
train() client id: f_00002-0-0 loss: 1.244674  [   32/  124]
train() client id: f_00002-0-1 loss: 1.262146  [   64/  124]
train() client id: f_00002-0-2 loss: 1.276409  [   96/  124]
train() client id: f_00002-1-0 loss: 1.313643  [   32/  124]
train() client id: f_00002-1-1 loss: 1.200703  [   64/  124]
train() client id: f_00002-1-2 loss: 1.200094  [   96/  124]
train() client id: f_00002-2-0 loss: 1.247486  [   32/  124]
train() client id: f_00002-2-1 loss: 1.135272  [   64/  124]
train() client id: f_00002-2-2 loss: 1.155313  [   96/  124]
train() client id: f_00002-3-0 loss: 1.109031  [   32/  124]
train() client id: f_00002-3-1 loss: 1.120674  [   64/  124]
train() client id: f_00002-3-2 loss: 1.137353  [   96/  124]
train() client id: f_00002-4-0 loss: 1.142256  [   32/  124]
train() client id: f_00002-4-1 loss: 1.020982  [   64/  124]
train() client id: f_00002-4-2 loss: 1.171032  [   96/  124]
train() client id: f_00002-5-0 loss: 1.116686  [   32/  124]
train() client id: f_00002-5-1 loss: 1.050630  [   64/  124]
train() client id: f_00002-5-2 loss: 1.041357  [   96/  124]
train() client id: f_00002-6-0 loss: 1.114111  [   32/  124]
train() client id: f_00002-6-1 loss: 0.956173  [   64/  124]
train() client id: f_00002-6-2 loss: 1.086992  [   96/  124]
train() client id: f_00002-7-0 loss: 1.092169  [   32/  124]
train() client id: f_00002-7-1 loss: 1.054621  [   64/  124]
train() client id: f_00002-7-2 loss: 1.085266  [   96/  124]
train() client id: f_00002-8-0 loss: 1.029240  [   32/  124]
train() client id: f_00002-8-1 loss: 1.114864  [   64/  124]
train() client id: f_00002-8-2 loss: 0.972958  [   96/  124]
train() client id: f_00002-9-0 loss: 1.071552  [   32/  124]
train() client id: f_00002-9-1 loss: 0.988000  [   64/  124]
train() client id: f_00002-9-2 loss: 1.024792  [   96/  124]
train() client id: f_00002-10-0 loss: 1.044160  [   32/  124]
train() client id: f_00002-10-1 loss: 1.027304  [   64/  124]
train() client id: f_00002-10-2 loss: 0.968619  [   96/  124]
train() client id: f_00002-11-0 loss: 0.934997  [   32/  124]
train() client id: f_00002-11-1 loss: 1.137195  [   64/  124]
train() client id: f_00002-11-2 loss: 1.114769  [   96/  124]
train() client id: f_00003-0-0 loss: 0.927403  [   32/   43]
train() client id: f_00003-1-0 loss: 0.877695  [   32/   43]
train() client id: f_00003-2-0 loss: 0.942512  [   32/   43]
train() client id: f_00003-3-0 loss: 0.942992  [   32/   43]
train() client id: f_00003-4-0 loss: 0.877404  [   32/   43]
train() client id: f_00003-5-0 loss: 0.910979  [   32/   43]
train() client id: f_00003-6-0 loss: 0.845437  [   32/   43]
train() client id: f_00003-7-0 loss: 0.937167  [   32/   43]
train() client id: f_00003-8-0 loss: 0.938359  [   32/   43]
train() client id: f_00003-9-0 loss: 0.941551  [   32/   43]
train() client id: f_00003-10-0 loss: 0.872363  [   32/   43]
train() client id: f_00003-11-0 loss: 0.933853  [   32/   43]
train() client id: f_00004-0-0 loss: 0.913732  [   32/  306]
train() client id: f_00004-0-1 loss: 0.924558  [   64/  306]
train() client id: f_00004-0-2 loss: 0.849364  [   96/  306]
train() client id: f_00004-0-3 loss: 0.880225  [  128/  306]
train() client id: f_00004-0-4 loss: 0.879077  [  160/  306]
train() client id: f_00004-0-5 loss: 0.846489  [  192/  306]
train() client id: f_00004-0-6 loss: 0.780654  [  224/  306]
train() client id: f_00004-0-7 loss: 0.855229  [  256/  306]
train() client id: f_00004-0-8 loss: 0.846508  [  288/  306]
train() client id: f_00004-1-0 loss: 0.925451  [   32/  306]
train() client id: f_00004-1-1 loss: 0.700754  [   64/  306]
train() client id: f_00004-1-2 loss: 0.873595  [   96/  306]
train() client id: f_00004-1-3 loss: 0.857729  [  128/  306]
train() client id: f_00004-1-4 loss: 0.855125  [  160/  306]
train() client id: f_00004-1-5 loss: 0.887212  [  192/  306]
train() client id: f_00004-1-6 loss: 0.925713  [  224/  306]
train() client id: f_00004-1-7 loss: 0.907973  [  256/  306]
train() client id: f_00004-1-8 loss: 0.783994  [  288/  306]
train() client id: f_00004-2-0 loss: 0.961377  [   32/  306]
train() client id: f_00004-2-1 loss: 0.848372  [   64/  306]
train() client id: f_00004-2-2 loss: 0.856463  [   96/  306]
train() client id: f_00004-2-3 loss: 0.834270  [  128/  306]
train() client id: f_00004-2-4 loss: 0.913782  [  160/  306]
train() client id: f_00004-2-5 loss: 0.851825  [  192/  306]
train() client id: f_00004-2-6 loss: 0.770306  [  224/  306]
train() client id: f_00004-2-7 loss: 0.911692  [  256/  306]
train() client id: f_00004-2-8 loss: 0.818339  [  288/  306]
train() client id: f_00004-3-0 loss: 0.877832  [   32/  306]
train() client id: f_00004-3-1 loss: 0.834546  [   64/  306]
train() client id: f_00004-3-2 loss: 0.805542  [   96/  306]
train() client id: f_00004-3-3 loss: 0.886635  [  128/  306]
train() client id: f_00004-3-4 loss: 0.816237  [  160/  306]
train() client id: f_00004-3-5 loss: 0.932689  [  192/  306]
train() client id: f_00004-3-6 loss: 0.869169  [  224/  306]
train() client id: f_00004-3-7 loss: 0.863546  [  256/  306]
train() client id: f_00004-3-8 loss: 0.829812  [  288/  306]
train() client id: f_00004-4-0 loss: 0.865416  [   32/  306]
train() client id: f_00004-4-1 loss: 0.808858  [   64/  306]
train() client id: f_00004-4-2 loss: 0.831717  [   96/  306]
train() client id: f_00004-4-3 loss: 0.888673  [  128/  306]
train() client id: f_00004-4-4 loss: 0.898831  [  160/  306]
train() client id: f_00004-4-5 loss: 0.808243  [  192/  306]
train() client id: f_00004-4-6 loss: 0.837172  [  224/  306]
train() client id: f_00004-4-7 loss: 0.956579  [  256/  306]
train() client id: f_00004-4-8 loss: 0.854380  [  288/  306]
train() client id: f_00004-5-0 loss: 0.803260  [   32/  306]
train() client id: f_00004-5-1 loss: 0.984991  [   64/  306]
train() client id: f_00004-5-2 loss: 0.856159  [   96/  306]
train() client id: f_00004-5-3 loss: 0.870953  [  128/  306]
train() client id: f_00004-5-4 loss: 0.809089  [  160/  306]
train() client id: f_00004-5-5 loss: 0.780175  [  192/  306]
train() client id: f_00004-5-6 loss: 0.950323  [  224/  306]
train() client id: f_00004-5-7 loss: 0.855540  [  256/  306]
train() client id: f_00004-5-8 loss: 0.813966  [  288/  306]
train() client id: f_00004-6-0 loss: 0.878256  [   32/  306]
train() client id: f_00004-6-1 loss: 0.855449  [   64/  306]
train() client id: f_00004-6-2 loss: 0.742676  [   96/  306]
train() client id: f_00004-6-3 loss: 0.971111  [  128/  306]
train() client id: f_00004-6-4 loss: 0.722984  [  160/  306]
train() client id: f_00004-6-5 loss: 0.815303  [  192/  306]
train() client id: f_00004-6-6 loss: 0.910016  [  224/  306]
train() client id: f_00004-6-7 loss: 1.003097  [  256/  306]
train() client id: f_00004-6-8 loss: 0.936189  [  288/  306]
train() client id: f_00004-7-0 loss: 0.871861  [   32/  306]
train() client id: f_00004-7-1 loss: 0.705939  [   64/  306]
train() client id: f_00004-7-2 loss: 0.950533  [   96/  306]
train() client id: f_00004-7-3 loss: 0.917533  [  128/  306]
train() client id: f_00004-7-4 loss: 0.819533  [  160/  306]
train() client id: f_00004-7-5 loss: 0.895378  [  192/  306]
train() client id: f_00004-7-6 loss: 1.006556  [  224/  306]
train() client id: f_00004-7-7 loss: 0.862011  [  256/  306]
train() client id: f_00004-7-8 loss: 0.785924  [  288/  306]
train() client id: f_00004-8-0 loss: 0.819394  [   32/  306]
train() client id: f_00004-8-1 loss: 0.886979  [   64/  306]
train() client id: f_00004-8-2 loss: 0.847129  [   96/  306]
train() client id: f_00004-8-3 loss: 0.846516  [  128/  306]
train() client id: f_00004-8-4 loss: 0.805251  [  160/  306]
train() client id: f_00004-8-5 loss: 0.886410  [  192/  306]
train() client id: f_00004-8-6 loss: 0.898911  [  224/  306]
train() client id: f_00004-8-7 loss: 0.847929  [  256/  306]
train() client id: f_00004-8-8 loss: 0.893447  [  288/  306]
train() client id: f_00004-9-0 loss: 0.857331  [   32/  306]
train() client id: f_00004-9-1 loss: 0.790763  [   64/  306]
train() client id: f_00004-9-2 loss: 0.799290  [   96/  306]
train() client id: f_00004-9-3 loss: 0.931607  [  128/  306]
train() client id: f_00004-9-4 loss: 0.831541  [  160/  306]
train() client id: f_00004-9-5 loss: 0.841276  [  192/  306]
train() client id: f_00004-9-6 loss: 0.871172  [  224/  306]
train() client id: f_00004-9-7 loss: 0.936044  [  256/  306]
train() client id: f_00004-9-8 loss: 0.863559  [  288/  306]
train() client id: f_00004-10-0 loss: 0.860624  [   32/  306]
train() client id: f_00004-10-1 loss: 0.831946  [   64/  306]
train() client id: f_00004-10-2 loss: 1.009905  [   96/  306]
train() client id: f_00004-10-3 loss: 0.964285  [  128/  306]
train() client id: f_00004-10-4 loss: 0.771460  [  160/  306]
train() client id: f_00004-10-5 loss: 0.905619  [  192/  306]
train() client id: f_00004-10-6 loss: 0.784588  [  224/  306]
train() client id: f_00004-10-7 loss: 0.913096  [  256/  306]
train() client id: f_00004-10-8 loss: 0.788710  [  288/  306]
train() client id: f_00004-11-0 loss: 0.888890  [   32/  306]
train() client id: f_00004-11-1 loss: 0.811672  [   64/  306]
train() client id: f_00004-11-2 loss: 0.911291  [   96/  306]
train() client id: f_00004-11-3 loss: 0.790461  [  128/  306]
train() client id: f_00004-11-4 loss: 0.794987  [  160/  306]
train() client id: f_00004-11-5 loss: 0.863226  [  192/  306]
train() client id: f_00004-11-6 loss: 0.820405  [  224/  306]
train() client id: f_00004-11-7 loss: 0.951121  [  256/  306]
train() client id: f_00004-11-8 loss: 0.915822  [  288/  306]
train() client id: f_00005-0-0 loss: 1.054726  [   32/  146]
train() client id: f_00005-0-1 loss: 0.962300  [   64/  146]
train() client id: f_00005-0-2 loss: 0.862384  [   96/  146]
train() client id: f_00005-0-3 loss: 0.781084  [  128/  146]
train() client id: f_00005-1-0 loss: 0.942506  [   32/  146]
train() client id: f_00005-1-1 loss: 0.915134  [   64/  146]
train() client id: f_00005-1-2 loss: 0.899175  [   96/  146]
train() client id: f_00005-1-3 loss: 0.854597  [  128/  146]
train() client id: f_00005-2-0 loss: 0.904619  [   32/  146]
train() client id: f_00005-2-1 loss: 0.843634  [   64/  146]
train() client id: f_00005-2-2 loss: 0.892556  [   96/  146]
train() client id: f_00005-2-3 loss: 1.024435  [  128/  146]
train() client id: f_00005-3-0 loss: 0.881396  [   32/  146]
train() client id: f_00005-3-1 loss: 1.076631  [   64/  146]
train() client id: f_00005-3-2 loss: 0.896181  [   96/  146]
train() client id: f_00005-3-3 loss: 0.893006  [  128/  146]
train() client id: f_00005-4-0 loss: 0.918223  [   32/  146]
train() client id: f_00005-4-1 loss: 1.179511  [   64/  146]
train() client id: f_00005-4-2 loss: 0.869951  [   96/  146]
train() client id: f_00005-4-3 loss: 0.701030  [  128/  146]
train() client id: f_00005-5-0 loss: 0.919453  [   32/  146]
train() client id: f_00005-5-1 loss: 1.049422  [   64/  146]
train() client id: f_00005-5-2 loss: 0.857161  [   96/  146]
train() client id: f_00005-5-3 loss: 0.949673  [  128/  146]
train() client id: f_00005-6-0 loss: 0.820577  [   32/  146]
train() client id: f_00005-6-1 loss: 0.844414  [   64/  146]
train() client id: f_00005-6-2 loss: 1.218834  [   96/  146]
train() client id: f_00005-6-3 loss: 0.849018  [  128/  146]
train() client id: f_00005-7-0 loss: 0.849537  [   32/  146]
train() client id: f_00005-7-1 loss: 1.069434  [   64/  146]
train() client id: f_00005-7-2 loss: 0.938917  [   96/  146]
train() client id: f_00005-7-3 loss: 0.848610  [  128/  146]
train() client id: f_00005-8-0 loss: 1.036749  [   32/  146]
train() client id: f_00005-8-1 loss: 0.817217  [   64/  146]
train() client id: f_00005-8-2 loss: 1.084899  [   96/  146]
train() client id: f_00005-8-3 loss: 0.928884  [  128/  146]
train() client id: f_00005-9-0 loss: 0.795416  [   32/  146]
train() client id: f_00005-9-1 loss: 0.681804  [   64/  146]
train() client id: f_00005-9-2 loss: 1.404022  [   96/  146]
train() client id: f_00005-9-3 loss: 1.025444  [  128/  146]
train() client id: f_00005-10-0 loss: 0.847056  [   32/  146]
train() client id: f_00005-10-1 loss: 1.047070  [   64/  146]
train() client id: f_00005-10-2 loss: 0.898841  [   96/  146]
train() client id: f_00005-10-3 loss: 0.993820  [  128/  146]
train() client id: f_00005-11-0 loss: 0.938027  [   32/  146]
train() client id: f_00005-11-1 loss: 0.908037  [   64/  146]
train() client id: f_00005-11-2 loss: 0.907948  [   96/  146]
train() client id: f_00005-11-3 loss: 1.030932  [  128/  146]
train() client id: f_00006-0-0 loss: 0.750377  [   32/   54]
train() client id: f_00006-1-0 loss: 0.762852  [   32/   54]
train() client id: f_00006-2-0 loss: 0.751617  [   32/   54]
train() client id: f_00006-3-0 loss: 0.772600  [   32/   54]
train() client id: f_00006-4-0 loss: 0.752172  [   32/   54]
train() client id: f_00006-5-0 loss: 0.786095  [   32/   54]
train() client id: f_00006-6-0 loss: 0.754708  [   32/   54]
train() client id: f_00006-7-0 loss: 0.795333  [   32/   54]
train() client id: f_00006-8-0 loss: 0.753791  [   32/   54]
train() client id: f_00006-9-0 loss: 0.709020  [   32/   54]
train() client id: f_00006-10-0 loss: 0.782565  [   32/   54]
train() client id: f_00006-11-0 loss: 0.753136  [   32/   54]
train() client id: f_00007-0-0 loss: 0.535417  [   32/  179]
train() client id: f_00007-0-1 loss: 0.594117  [   64/  179]
train() client id: f_00007-0-2 loss: 0.655869  [   96/  179]
train() client id: f_00007-0-3 loss: 0.734470  [  128/  179]
train() client id: f_00007-0-4 loss: 0.519587  [  160/  179]
train() client id: f_00007-1-0 loss: 0.555069  [   32/  179]
train() client id: f_00007-1-1 loss: 0.569928  [   64/  179]
train() client id: f_00007-1-2 loss: 0.492919  [   96/  179]
train() client id: f_00007-1-3 loss: 0.571274  [  128/  179]
train() client id: f_00007-1-4 loss: 0.805326  [  160/  179]
train() client id: f_00007-2-0 loss: 0.525516  [   32/  179]
train() client id: f_00007-2-1 loss: 0.489885  [   64/  179]
train() client id: f_00007-2-2 loss: 0.598898  [   96/  179]
train() client id: f_00007-2-3 loss: 0.679619  [  128/  179]
train() client id: f_00007-2-4 loss: 0.533183  [  160/  179]
train() client id: f_00007-3-0 loss: 0.513020  [   32/  179]
train() client id: f_00007-3-1 loss: 0.760911  [   64/  179]
train() client id: f_00007-3-2 loss: 0.525732  [   96/  179]
train() client id: f_00007-3-3 loss: 0.527449  [  128/  179]
train() client id: f_00007-3-4 loss: 0.461965  [  160/  179]
train() client id: f_00007-4-0 loss: 0.632000  [   32/  179]
train() client id: f_00007-4-1 loss: 0.594632  [   64/  179]
train() client id: f_00007-4-2 loss: 0.571246  [   96/  179]
train() client id: f_00007-4-3 loss: 0.430493  [  128/  179]
train() client id: f_00007-4-4 loss: 0.499599  [  160/  179]
train() client id: f_00007-5-0 loss: 0.438464  [   32/  179]
train() client id: f_00007-5-1 loss: 0.527546  [   64/  179]
train() client id: f_00007-5-2 loss: 0.646907  [   96/  179]
train() client id: f_00007-5-3 loss: 0.575025  [  128/  179]
train() client id: f_00007-5-4 loss: 0.547710  [  160/  179]
train() client id: f_00007-6-0 loss: 0.572374  [   32/  179]
train() client id: f_00007-6-1 loss: 0.414402  [   64/  179]
train() client id: f_00007-6-2 loss: 0.430127  [   96/  179]
train() client id: f_00007-6-3 loss: 0.671986  [  128/  179]
train() client id: f_00007-6-4 loss: 0.626468  [  160/  179]
train() client id: f_00007-7-0 loss: 0.500522  [   32/  179]
train() client id: f_00007-7-1 loss: 0.421559  [   64/  179]
train() client id: f_00007-7-2 loss: 0.701203  [   96/  179]
train() client id: f_00007-7-3 loss: 0.404746  [  128/  179]
train() client id: f_00007-7-4 loss: 0.583372  [  160/  179]
train() client id: f_00007-8-0 loss: 0.577281  [   32/  179]
train() client id: f_00007-8-1 loss: 0.501201  [   64/  179]
train() client id: f_00007-8-2 loss: 0.615042  [   96/  179]
train() client id: f_00007-8-3 loss: 0.494087  [  128/  179]
train() client id: f_00007-8-4 loss: 0.399377  [  160/  179]
train() client id: f_00007-9-0 loss: 0.480485  [   32/  179]
train() client id: f_00007-9-1 loss: 0.564334  [   64/  179]
train() client id: f_00007-9-2 loss: 0.510668  [   96/  179]
train() client id: f_00007-9-3 loss: 0.530876  [  128/  179]
train() client id: f_00007-9-4 loss: 0.467812  [  160/  179]
train() client id: f_00007-10-0 loss: 0.397054  [   32/  179]
train() client id: f_00007-10-1 loss: 0.639039  [   64/  179]
train() client id: f_00007-10-2 loss: 0.472508  [   96/  179]
train() client id: f_00007-10-3 loss: 0.567811  [  128/  179]
train() client id: f_00007-10-4 loss: 0.553940  [  160/  179]
train() client id: f_00007-11-0 loss: 0.477241  [   32/  179]
train() client id: f_00007-11-1 loss: 0.574376  [   64/  179]
train() client id: f_00007-11-2 loss: 0.384150  [   96/  179]
train() client id: f_00007-11-3 loss: 0.529756  [  128/  179]
train() client id: f_00007-11-4 loss: 0.661336  [  160/  179]
train() client id: f_00008-0-0 loss: 0.787638  [   32/  130]
train() client id: f_00008-0-1 loss: 0.787630  [   64/  130]
train() client id: f_00008-0-2 loss: 0.777721  [   96/  130]
train() client id: f_00008-0-3 loss: 0.726219  [  128/  130]
train() client id: f_00008-1-0 loss: 0.817831  [   32/  130]
train() client id: f_00008-1-1 loss: 0.662656  [   64/  130]
train() client id: f_00008-1-2 loss: 0.823649  [   96/  130]
train() client id: f_00008-1-3 loss: 0.795014  [  128/  130]
train() client id: f_00008-2-0 loss: 0.960471  [   32/  130]
train() client id: f_00008-2-1 loss: 0.718953  [   64/  130]
train() client id: f_00008-2-2 loss: 0.727579  [   96/  130]
train() client id: f_00008-2-3 loss: 0.691701  [  128/  130]
train() client id: f_00008-3-0 loss: 0.726217  [   32/  130]
train() client id: f_00008-3-1 loss: 0.854231  [   64/  130]
train() client id: f_00008-3-2 loss: 0.666293  [   96/  130]
train() client id: f_00008-3-3 loss: 0.842199  [  128/  130]
train() client id: f_00008-4-0 loss: 0.864000  [   32/  130]
train() client id: f_00008-4-1 loss: 0.770256  [   64/  130]
train() client id: f_00008-4-2 loss: 0.713974  [   96/  130]
train() client id: f_00008-4-3 loss: 0.738247  [  128/  130]
train() client id: f_00008-5-0 loss: 0.815684  [   32/  130]
train() client id: f_00008-5-1 loss: 0.749112  [   64/  130]
train() client id: f_00008-5-2 loss: 0.710724  [   96/  130]
train() client id: f_00008-5-3 loss: 0.773572  [  128/  130]
train() client id: f_00008-6-0 loss: 0.841623  [   32/  130]
train() client id: f_00008-6-1 loss: 0.747571  [   64/  130]
train() client id: f_00008-6-2 loss: 0.756009  [   96/  130]
train() client id: f_00008-6-3 loss: 0.709874  [  128/  130]
train() client id: f_00008-7-0 loss: 0.707446  [   32/  130]
train() client id: f_00008-7-1 loss: 0.819688  [   64/  130]
train() client id: f_00008-7-2 loss: 0.777980  [   96/  130]
train() client id: f_00008-7-3 loss: 0.776317  [  128/  130]
train() client id: f_00008-8-0 loss: 0.745061  [   32/  130]
train() client id: f_00008-8-1 loss: 0.714837  [   64/  130]
train() client id: f_00008-8-2 loss: 0.751394  [   96/  130]
train() client id: f_00008-8-3 loss: 0.870649  [  128/  130]
train() client id: f_00008-9-0 loss: 0.769376  [   32/  130]
train() client id: f_00008-9-1 loss: 0.734934  [   64/  130]
train() client id: f_00008-9-2 loss: 0.718392  [   96/  130]
train() client id: f_00008-9-3 loss: 0.858187  [  128/  130]
train() client id: f_00008-10-0 loss: 0.624870  [   32/  130]
train() client id: f_00008-10-1 loss: 0.773667  [   64/  130]
train() client id: f_00008-10-2 loss: 0.927716  [   96/  130]
train() client id: f_00008-10-3 loss: 0.756335  [  128/  130]
train() client id: f_00008-11-0 loss: 0.748380  [   32/  130]
train() client id: f_00008-11-1 loss: 0.705003  [   64/  130]
train() client id: f_00008-11-2 loss: 0.976925  [   96/  130]
train() client id: f_00008-11-3 loss: 0.614457  [  128/  130]
train() client id: f_00009-0-0 loss: 1.166470  [   32/  118]
train() client id: f_00009-0-1 loss: 0.916523  [   64/  118]
train() client id: f_00009-0-2 loss: 1.050543  [   96/  118]
train() client id: f_00009-1-0 loss: 0.978093  [   32/  118]
train() client id: f_00009-1-1 loss: 0.988359  [   64/  118]
train() client id: f_00009-1-2 loss: 0.954975  [   96/  118]
train() client id: f_00009-2-0 loss: 1.044464  [   32/  118]
train() client id: f_00009-2-1 loss: 0.885377  [   64/  118]
train() client id: f_00009-2-2 loss: 0.949301  [   96/  118]
train() client id: f_00009-3-0 loss: 0.859496  [   32/  118]
train() client id: f_00009-3-1 loss: 0.968150  [   64/  118]
train() client id: f_00009-3-2 loss: 0.871542  [   96/  118]
train() client id: f_00009-4-0 loss: 0.859885  [   32/  118]
train() client id: f_00009-4-1 loss: 0.786258  [   64/  118]
train() client id: f_00009-4-2 loss: 0.920624  [   96/  118]
train() client id: f_00009-5-0 loss: 0.758270  [   32/  118]
train() client id: f_00009-5-1 loss: 0.972979  [   64/  118]
train() client id: f_00009-5-2 loss: 0.815496  [   96/  118]
train() client id: f_00009-6-0 loss: 0.851757  [   32/  118]
train() client id: f_00009-6-1 loss: 0.906540  [   64/  118]
train() client id: f_00009-6-2 loss: 0.802057  [   96/  118]
train() client id: f_00009-7-0 loss: 0.892313  [   32/  118]
train() client id: f_00009-7-1 loss: 0.776732  [   64/  118]
train() client id: f_00009-7-2 loss: 0.760128  [   96/  118]
train() client id: f_00009-8-0 loss: 0.831278  [   32/  118]
train() client id: f_00009-8-1 loss: 0.753170  [   64/  118]
train() client id: f_00009-8-2 loss: 0.834450  [   96/  118]
train() client id: f_00009-9-0 loss: 0.713934  [   32/  118]
train() client id: f_00009-9-1 loss: 0.867697  [   64/  118]
train() client id: f_00009-9-2 loss: 0.806368  [   96/  118]
train() client id: f_00009-10-0 loss: 0.698349  [   32/  118]
train() client id: f_00009-10-1 loss: 0.717730  [   64/  118]
train() client id: f_00009-10-2 loss: 0.834643  [   96/  118]
train() client id: f_00009-11-0 loss: 0.683922  [   32/  118]
train() client id: f_00009-11-1 loss: 0.690877  [   64/  118]
train() client id: f_00009-11-2 loss: 0.834820  [   96/  118]
At round 10 accuracy: 0.6312997347480106
At round 10 training accuracy: 0.574111334674715
At round 10 training loss: 0.8728130639214032
update_location
xs = -3.905658 4.200318 70.009024 18.811294 0.979296 3.956410 -32.443192 -11.324852 54.663977 -2.060879 
ys = 62.587959 45.555839 1.320614 -32.455176 24.350187 7.814151 -2.624984 0.822348 17.569006 -0.998518 
dists_uav = 118.036040 109.968073 122.077875 106.804509 102.926627 100.382838 105.163926 100.642578 115.311840 100.026218 
dists_bs = 204.718686 221.062310 300.323078 283.902215 231.655086 244.901010 227.756621 239.013407 278.442202 246.745655 
uav_gains = -101.800535 -101.031727 -102.166196 -100.714781 -100.313218 -100.041504 -100.546703 -100.069561 -101.546969 -100.002862 
bs_gains = -104.279469 -105.213471 -108.939543 -108.255785 -105.782631 -106.458795 -105.576247 -106.162882 -108.019641 -106.550046 
Round 11
-------------------------------
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.53941363 19.90793381  9.39345483  3.35836813 22.96346205 11.07093228
  4.17536772 13.46928103  9.90106646  8.98947369]
obj_prev = 112.76875362310263
eta_min = 1.8259794231621306e-10	eta_max = 0.9205107351620903
af = 23.839837477165666	bf = 1.854557298226717	zeta = 26.223821224882236	eta = 0.909090909090909
af = 23.839837477165666	bf = 1.854557298226717	zeta = 45.394101441493795	eta = 0.5251747852723034
af = 23.839837477165666	bf = 1.854557298226717	zeta = 36.24067924685511	eta = 0.6578198304391449
af = 23.839837477165666	bf = 1.854557298226717	zeta = 34.59948643670541	eta = 0.6890228709254712
af = 23.839837477165666	bf = 1.854557298226717	zeta = 34.51842070411875	eta = 0.6906410255994443
af = 23.839837477165666	bf = 1.854557298226717	zeta = 34.51820885585075	eta = 0.6906452642639038
eta = 0.6906452642639038
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [0.03054215 0.06423548 0.03005736 0.01042312 0.0741738  0.03539011
 0.01308949 0.04338925 0.03151175 0.02860297]
ene_total = [2.94794512 5.70363107 2.91862696 1.33519898 6.50745954 3.47131979
 1.5419756  3.92315854 3.2297488  2.93914446]
ti_comp = [0.30504778 0.29281109 0.30391469 0.30820672 0.29034205 0.28721115
 0.30867006 0.30995051 0.27903667 0.28677106]
ti_coms = [0.0681094  0.08034609 0.06924248 0.06495046 0.08281513 0.08594602
 0.06448711 0.06320667 0.09412051 0.08638612]
t_total = [29.44995384 29.44995384 29.44995384 29.44995384 29.44995384 29.44995384
 29.44995384 29.44995384 29.44995384 29.44995384]
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [1.91356519e-05 1.93210162e-04 1.83750713e-05 7.45055334e-07
 3.02560183e-04 3.35832973e-05 1.47115905e-06 5.31424719e-05
 2.51174115e-05 1.77845446e-05]
ene_total = [0.53903666 0.6493481  0.54791909 0.51265458 0.677464   0.68094541
 0.50905514 0.50302767 0.7447912  0.68317183]
optimize_network iter = 0 obj = 6.04741367166706
eta = 0.6906452642639038
freqs = [5.00612622e+07 1.09687587e+08 4.94503204e+07 1.69092939e+07
 1.27735205e+08 6.16099244e+07 2.12030516e+07 6.99938362e+07
 5.64652429e+07 4.98707394e+07]
eta_min = 0.6906452642638832	eta_max = 0.6906452642639023
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 0.04826057494319857	eta = 0.909090909090909
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 20.44612752124682	eta = 0.0021457975307437047
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 2.1286929945616637	eta = 0.020610416842846153
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 2.067483129885282	eta = 0.02122060843649869
af = 0.04387324994836233	bf = 1.854557298226717	zeta = 2.0674642356817103	eta = 0.021220802368025433
eta = 0.021220802368025433
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [1.99188226e-04 2.01117734e-03 1.91271134e-04 7.75548441e-06
 3.14943156e-03 3.49577711e-04 1.53136962e-05 5.53174501e-04
 2.61453995e-04 1.85124181e-04]
ene_total = [0.17486013 0.25058139 0.17748901 0.16220541 0.28513266 0.22310299
 0.16123818 0.17146061 0.24129522 0.22009864]
ti_comp = [0.30504778 0.29281109 0.30391469 0.30820672 0.29034205 0.28721115
 0.30867006 0.30995051 0.27903667 0.28677106]
ti_coms = [0.0681094  0.08034609 0.06924248 0.06495046 0.08281513 0.08594602
 0.06448711 0.06320667 0.09412051 0.08638612]
t_total = [29.44995384 29.44995384 29.44995384 29.44995384 29.44995384 29.44995384
 29.44995384 29.44995384 29.44995384 29.44995384]
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [1.91356519e-05 1.93210162e-04 1.83750713e-05 7.45055334e-07
 3.02560183e-04 3.35832973e-05 1.47115905e-06 5.31424719e-05
 2.51174115e-05 1.77845446e-05]
ene_total = [0.53903666 0.6493481  0.54791909 0.51265458 0.677464   0.68094541
 0.50905514 0.50302767 0.7447912  0.68317183]
optimize_network iter = 1 obj = 6.047413671666661
eta = 0.6906452642638832
freqs = [5.00612622e+07 1.09687587e+08 4.94503204e+07 1.69092939e+07
 1.27735205e+08 6.16099244e+07 2.12030516e+07 6.99938362e+07
 5.64652429e+07 4.98707394e+07]
Done!
ene_coms = [0.00681094 0.00803461 0.00692425 0.00649505 0.00828151 0.0085946
 0.00644871 0.00632067 0.00941205 0.00863861]
ene_comp = [1.89463426e-05 1.91298731e-04 1.81932864e-05 7.37684489e-07
 2.99566950e-04 3.32510571e-05 1.45660485e-06 5.26167325e-05
 2.48689246e-05 1.76086018e-05]
ene_total = [0.00682989 0.00822591 0.00694244 0.00649578 0.00858108 0.00862785
 0.00645017 0.00637328 0.00943692 0.00865622]
At round 11 energy consumption: 0.0766195440375612
At round 11 eta: 0.6906452642638832
At round 11 a_n: 24.414598539591886
At round 11 local rounds: 12.119902394804324
At round 11 global rounds: 78.921043447086
gradient difference: 0.4289214611053467
train() client id: f_00000-0-0 loss: 1.313430  [   32/  126]
train() client id: f_00000-0-1 loss: 1.336180  [   64/  126]
train() client id: f_00000-0-2 loss: 1.232432  [   96/  126]
train() client id: f_00000-1-0 loss: 1.215445  [   32/  126]
train() client id: f_00000-1-1 loss: 1.311800  [   64/  126]
train() client id: f_00000-1-2 loss: 1.127080  [   96/  126]
train() client id: f_00000-2-0 loss: 1.074494  [   32/  126]
train() client id: f_00000-2-1 loss: 1.181434  [   64/  126]
train() client id: f_00000-2-2 loss: 1.112077  [   96/  126]
train() client id: f_00000-3-0 loss: 1.067803  [   32/  126]
train() client id: f_00000-3-1 loss: 1.095686  [   64/  126]
train() client id: f_00000-3-2 loss: 1.017818  [   96/  126]
train() client id: f_00000-4-0 loss: 0.969081  [   32/  126]
train() client id: f_00000-4-1 loss: 0.946465  [   64/  126]
train() client id: f_00000-4-2 loss: 1.023398  [   96/  126]
train() client id: f_00000-5-0 loss: 0.986442  [   32/  126]
train() client id: f_00000-5-1 loss: 0.917494  [   64/  126]
train() client id: f_00000-5-2 loss: 0.995817  [   96/  126]
train() client id: f_00000-6-0 loss: 0.956020  [   32/  126]
train() client id: f_00000-6-1 loss: 0.929733  [   64/  126]
train() client id: f_00000-6-2 loss: 0.961533  [   96/  126]
train() client id: f_00000-7-0 loss: 0.831253  [   32/  126]
train() client id: f_00000-7-1 loss: 0.876140  [   64/  126]
train() client id: f_00000-7-2 loss: 0.987259  [   96/  126]
train() client id: f_00000-8-0 loss: 0.864263  [   32/  126]
train() client id: f_00000-8-1 loss: 0.936023  [   64/  126]
train() client id: f_00000-8-2 loss: 0.908483  [   96/  126]
train() client id: f_00000-9-0 loss: 0.850924  [   32/  126]
train() client id: f_00000-9-1 loss: 0.846640  [   64/  126]
train() client id: f_00000-9-2 loss: 1.010569  [   96/  126]
train() client id: f_00000-10-0 loss: 0.913266  [   32/  126]
train() client id: f_00000-10-1 loss: 0.792825  [   64/  126]
train() client id: f_00000-10-2 loss: 0.975523  [   96/  126]
train() client id: f_00000-11-0 loss: 0.819472  [   32/  126]
train() client id: f_00000-11-1 loss: 0.849422  [   64/  126]
train() client id: f_00000-11-2 loss: 0.978222  [   96/  126]
train() client id: f_00001-0-0 loss: 0.565744  [   32/  265]
train() client id: f_00001-0-1 loss: 0.564164  [   64/  265]
train() client id: f_00001-0-2 loss: 0.719473  [   96/  265]
train() client id: f_00001-0-3 loss: 0.573489  [  128/  265]
train() client id: f_00001-0-4 loss: 0.605587  [  160/  265]
train() client id: f_00001-0-5 loss: 0.603043  [  192/  265]
train() client id: f_00001-0-6 loss: 0.504109  [  224/  265]
train() client id: f_00001-0-7 loss: 0.577920  [  256/  265]
train() client id: f_00001-1-0 loss: 0.659754  [   32/  265]
train() client id: f_00001-1-1 loss: 0.595545  [   64/  265]
train() client id: f_00001-1-2 loss: 0.523916  [   96/  265]
train() client id: f_00001-1-3 loss: 0.564382  [  128/  265]
train() client id: f_00001-1-4 loss: 0.578841  [  160/  265]
train() client id: f_00001-1-5 loss: 0.489184  [  192/  265]
train() client id: f_00001-1-6 loss: 0.548750  [  224/  265]
train() client id: f_00001-1-7 loss: 0.657346  [  256/  265]
train() client id: f_00001-2-0 loss: 0.552453  [   32/  265]
train() client id: f_00001-2-1 loss: 0.549450  [   64/  265]
train() client id: f_00001-2-2 loss: 0.612949  [   96/  265]
train() client id: f_00001-2-3 loss: 0.502896  [  128/  265]
train() client id: f_00001-2-4 loss: 0.583396  [  160/  265]
train() client id: f_00001-2-5 loss: 0.538982  [  192/  265]
train() client id: f_00001-2-6 loss: 0.515534  [  224/  265]
train() client id: f_00001-2-7 loss: 0.617194  [  256/  265]
train() client id: f_00001-3-0 loss: 0.526493  [   32/  265]
train() client id: f_00001-3-1 loss: 0.490353  [   64/  265]
train() client id: f_00001-3-2 loss: 0.581638  [   96/  265]
train() client id: f_00001-3-3 loss: 0.534032  [  128/  265]
train() client id: f_00001-3-4 loss: 0.602707  [  160/  265]
train() client id: f_00001-3-5 loss: 0.643421  [  192/  265]
train() client id: f_00001-3-6 loss: 0.569876  [  224/  265]
train() client id: f_00001-3-7 loss: 0.491328  [  256/  265]
train() client id: f_00001-4-0 loss: 0.531564  [   32/  265]
train() client id: f_00001-4-1 loss: 0.591864  [   64/  265]
train() client id: f_00001-4-2 loss: 0.584404  [   96/  265]
train() client id: f_00001-4-3 loss: 0.514954  [  128/  265]
train() client id: f_00001-4-4 loss: 0.587478  [  160/  265]
train() client id: f_00001-4-5 loss: 0.557284  [  192/  265]
train() client id: f_00001-4-6 loss: 0.622797  [  224/  265]
train() client id: f_00001-4-7 loss: 0.461606  [  256/  265]
train() client id: f_00001-5-0 loss: 0.622285  [   32/  265]
train() client id: f_00001-5-1 loss: 0.593636  [   64/  265]
train() client id: f_00001-5-2 loss: 0.489673  [   96/  265]
train() client id: f_00001-5-3 loss: 0.520950  [  128/  265]
train() client id: f_00001-5-4 loss: 0.598773  [  160/  265]
train() client id: f_00001-5-5 loss: 0.475714  [  192/  265]
train() client id: f_00001-5-6 loss: 0.558688  [  224/  265]
train() client id: f_00001-5-7 loss: 0.527401  [  256/  265]
train() client id: f_00001-6-0 loss: 0.523289  [   32/  265]
train() client id: f_00001-6-1 loss: 0.466352  [   64/  265]
train() client id: f_00001-6-2 loss: 0.539764  [   96/  265]
train() client id: f_00001-6-3 loss: 0.614555  [  128/  265]
train() client id: f_00001-6-4 loss: 0.515338  [  160/  265]
train() client id: f_00001-6-5 loss: 0.575029  [  192/  265]
train() client id: f_00001-6-6 loss: 0.521296  [  224/  265]
train() client id: f_00001-6-7 loss: 0.553737  [  256/  265]
train() client id: f_00001-7-0 loss: 0.593245  [   32/  265]
train() client id: f_00001-7-1 loss: 0.622618  [   64/  265]
train() client id: f_00001-7-2 loss: 0.580916  [   96/  265]
train() client id: f_00001-7-3 loss: 0.538238  [  128/  265]
train() client id: f_00001-7-4 loss: 0.542820  [  160/  265]
train() client id: f_00001-7-5 loss: 0.568587  [  192/  265]
train() client id: f_00001-7-6 loss: 0.453070  [  224/  265]
train() client id: f_00001-7-7 loss: 0.524046  [  256/  265]
train() client id: f_00001-8-0 loss: 0.515332  [   32/  265]
train() client id: f_00001-8-1 loss: 0.630542  [   64/  265]
train() client id: f_00001-8-2 loss: 0.519688  [   96/  265]
train() client id: f_00001-8-3 loss: 0.578731  [  128/  265]
train() client id: f_00001-8-4 loss: 0.540022  [  160/  265]
train() client id: f_00001-8-5 loss: 0.571613  [  192/  265]
train() client id: f_00001-8-6 loss: 0.531866  [  224/  265]
train() client id: f_00001-8-7 loss: 0.535781  [  256/  265]
train() client id: f_00001-9-0 loss: 0.514188  [   32/  265]
train() client id: f_00001-9-1 loss: 0.713526  [   64/  265]
train() client id: f_00001-9-2 loss: 0.466892  [   96/  265]
train() client id: f_00001-9-3 loss: 0.519436  [  128/  265]
train() client id: f_00001-9-4 loss: 0.650553  [  160/  265]
train() client id: f_00001-9-5 loss: 0.598343  [  192/  265]
train() client id: f_00001-9-6 loss: 0.463444  [  224/  265]
train() client id: f_00001-9-7 loss: 0.450286  [  256/  265]
train() client id: f_00001-10-0 loss: 0.611709  [   32/  265]
train() client id: f_00001-10-1 loss: 0.637817  [   64/  265]
train() client id: f_00001-10-2 loss: 0.529489  [   96/  265]
train() client id: f_00001-10-3 loss: 0.493429  [  128/  265]
train() client id: f_00001-10-4 loss: 0.521474  [  160/  265]
train() client id: f_00001-10-5 loss: 0.565072  [  192/  265]
train() client id: f_00001-10-6 loss: 0.472035  [  224/  265]
train() client id: f_00001-10-7 loss: 0.542644  [  256/  265]
train() client id: f_00001-11-0 loss: 0.726518  [   32/  265]
train() client id: f_00001-11-1 loss: 0.456987  [   64/  265]
train() client id: f_00001-11-2 loss: 0.454882  [   96/  265]
train() client id: f_00001-11-3 loss: 0.655300  [  128/  265]
train() client id: f_00001-11-4 loss: 0.564833  [  160/  265]
train() client id: f_00001-11-5 loss: 0.525006  [  192/  265]
train() client id: f_00001-11-6 loss: 0.517927  [  224/  265]
train() client id: f_00001-11-7 loss: 0.528103  [  256/  265]
train() client id: f_00002-0-0 loss: 1.184228  [   32/  124]
train() client id: f_00002-0-1 loss: 1.130829  [   64/  124]
train() client id: f_00002-0-2 loss: 1.112474  [   96/  124]
train() client id: f_00002-1-0 loss: 1.094945  [   32/  124]
train() client id: f_00002-1-1 loss: 1.049430  [   64/  124]
train() client id: f_00002-1-2 loss: 1.128371  [   96/  124]
train() client id: f_00002-2-0 loss: 0.964482  [   32/  124]
train() client id: f_00002-2-1 loss: 1.009859  [   64/  124]
train() client id: f_00002-2-2 loss: 1.143862  [   96/  124]
train() client id: f_00002-3-0 loss: 1.087205  [   32/  124]
train() client id: f_00002-3-1 loss: 0.982442  [   64/  124]
train() client id: f_00002-3-2 loss: 0.987542  [   96/  124]
train() client id: f_00002-4-0 loss: 0.970570  [   32/  124]
train() client id: f_00002-4-1 loss: 1.027197  [   64/  124]
train() client id: f_00002-4-2 loss: 0.885210  [   96/  124]
train() client id: f_00002-5-0 loss: 0.966000  [   32/  124]
train() client id: f_00002-5-1 loss: 1.044193  [   64/  124]
train() client id: f_00002-5-2 loss: 0.965362  [   96/  124]
train() client id: f_00002-6-0 loss: 0.930141  [   32/  124]
train() client id: f_00002-6-1 loss: 0.955629  [   64/  124]
train() client id: f_00002-6-2 loss: 0.999136  [   96/  124]
train() client id: f_00002-7-0 loss: 0.854495  [   32/  124]
train() client id: f_00002-7-1 loss: 1.054911  [   64/  124]
train() client id: f_00002-7-2 loss: 0.895406  [   96/  124]
train() client id: f_00002-8-0 loss: 0.884404  [   32/  124]
train() client id: f_00002-8-1 loss: 0.863220  [   64/  124]
train() client id: f_00002-8-2 loss: 0.961230  [   96/  124]
train() client id: f_00002-9-0 loss: 0.979674  [   32/  124]
train() client id: f_00002-9-1 loss: 0.909466  [   64/  124]
train() client id: f_00002-9-2 loss: 0.869662  [   96/  124]
train() client id: f_00002-10-0 loss: 0.952922  [   32/  124]
train() client id: f_00002-10-1 loss: 0.976338  [   64/  124]
train() client id: f_00002-10-2 loss: 0.881699  [   96/  124]
train() client id: f_00002-11-0 loss: 0.903273  [   32/  124]
train() client id: f_00002-11-1 loss: 0.829551  [   64/  124]
train() client id: f_00002-11-2 loss: 1.000653  [   96/  124]
train() client id: f_00003-0-0 loss: 0.887056  [   32/   43]
train() client id: f_00003-1-0 loss: 0.902616  [   32/   43]
train() client id: f_00003-2-0 loss: 0.950044  [   32/   43]
train() client id: f_00003-3-0 loss: 0.917089  [   32/   43]
train() client id: f_00003-4-0 loss: 0.973168  [   32/   43]
train() client id: f_00003-5-0 loss: 0.923125  [   32/   43]
train() client id: f_00003-6-0 loss: 0.901346  [   32/   43]
train() client id: f_00003-7-0 loss: 0.901009  [   32/   43]
train() client id: f_00003-8-0 loss: 0.855441  [   32/   43]
train() client id: f_00003-9-0 loss: 0.886937  [   32/   43]
train() client id: f_00003-10-0 loss: 0.931536  [   32/   43]
train() client id: f_00003-11-0 loss: 0.893404  [   32/   43]
train() client id: f_00004-0-0 loss: 0.865160  [   32/  306]
train() client id: f_00004-0-1 loss: 0.838179  [   64/  306]
train() client id: f_00004-0-2 loss: 0.930970  [   96/  306]
train() client id: f_00004-0-3 loss: 0.937444  [  128/  306]
train() client id: f_00004-0-4 loss: 0.894238  [  160/  306]
train() client id: f_00004-0-5 loss: 0.946103  [  192/  306]
train() client id: f_00004-0-6 loss: 0.791252  [  224/  306]
train() client id: f_00004-0-7 loss: 0.955555  [  256/  306]
train() client id: f_00004-0-8 loss: 0.985248  [  288/  306]
train() client id: f_00004-1-0 loss: 0.932536  [   32/  306]
train() client id: f_00004-1-1 loss: 0.931456  [   64/  306]
train() client id: f_00004-1-2 loss: 0.752410  [   96/  306]
train() client id: f_00004-1-3 loss: 0.821557  [  128/  306]
train() client id: f_00004-1-4 loss: 0.947544  [  160/  306]
train() client id: f_00004-1-5 loss: 0.915517  [  192/  306]
train() client id: f_00004-1-6 loss: 0.955549  [  224/  306]
train() client id: f_00004-1-7 loss: 0.890142  [  256/  306]
train() client id: f_00004-1-8 loss: 0.923566  [  288/  306]
train() client id: f_00004-2-0 loss: 0.850745  [   32/  306]
train() client id: f_00004-2-1 loss: 0.903191  [   64/  306]
train() client id: f_00004-2-2 loss: 0.920453  [   96/  306]
train() client id: f_00004-2-3 loss: 0.843889  [  128/  306]
train() client id: f_00004-2-4 loss: 0.919264  [  160/  306]
train() client id: f_00004-2-5 loss: 0.882174  [  192/  306]
train() client id: f_00004-2-6 loss: 0.899861  [  224/  306]
train() client id: f_00004-2-7 loss: 0.990146  [  256/  306]
train() client id: f_00004-2-8 loss: 0.902844  [  288/  306]
train() client id: f_00004-3-0 loss: 0.886624  [   32/  306]
train() client id: f_00004-3-1 loss: 0.870033  [   64/  306]
train() client id: f_00004-3-2 loss: 0.957054  [   96/  306]
train() client id: f_00004-3-3 loss: 0.933324  [  128/  306]
train() client id: f_00004-3-4 loss: 0.893700  [  160/  306]
train() client id: f_00004-3-5 loss: 0.802181  [  192/  306]
train() client id: f_00004-3-6 loss: 0.908459  [  224/  306]
train() client id: f_00004-3-7 loss: 0.955206  [  256/  306]
train() client id: f_00004-3-8 loss: 0.820627  [  288/  306]
train() client id: f_00004-4-0 loss: 0.814194  [   32/  306]
train() client id: f_00004-4-1 loss: 0.947044  [   64/  306]
train() client id: f_00004-4-2 loss: 0.815943  [   96/  306]
train() client id: f_00004-4-3 loss: 0.879824  [  128/  306]
train() client id: f_00004-4-4 loss: 0.836536  [  160/  306]
train() client id: f_00004-4-5 loss: 0.964746  [  192/  306]
train() client id: f_00004-4-6 loss: 0.993170  [  224/  306]
train() client id: f_00004-4-7 loss: 0.929906  [  256/  306]
train() client id: f_00004-4-8 loss: 0.889032  [  288/  306]
train() client id: f_00004-5-0 loss: 1.009075  [   32/  306]
train() client id: f_00004-5-1 loss: 0.925699  [   64/  306]
train() client id: f_00004-5-2 loss: 1.023349  [   96/  306]
train() client id: f_00004-5-3 loss: 0.842826  [  128/  306]
train() client id: f_00004-5-4 loss: 0.903684  [  160/  306]
train() client id: f_00004-5-5 loss: 0.767519  [  192/  306]
train() client id: f_00004-5-6 loss: 0.834520  [  224/  306]
train() client id: f_00004-5-7 loss: 0.851840  [  256/  306]
train() client id: f_00004-5-8 loss: 0.916014  [  288/  306]
train() client id: f_00004-6-0 loss: 0.825405  [   32/  306]
train() client id: f_00004-6-1 loss: 0.892264  [   64/  306]
train() client id: f_00004-6-2 loss: 0.966582  [   96/  306]
train() client id: f_00004-6-3 loss: 0.906152  [  128/  306]
train() client id: f_00004-6-4 loss: 0.919932  [  160/  306]
train() client id: f_00004-6-5 loss: 0.933342  [  192/  306]
train() client id: f_00004-6-6 loss: 0.848664  [  224/  306]
train() client id: f_00004-6-7 loss: 0.833703  [  256/  306]
train() client id: f_00004-6-8 loss: 0.965345  [  288/  306]
train() client id: f_00004-7-0 loss: 0.976747  [   32/  306]
train() client id: f_00004-7-1 loss: 0.850212  [   64/  306]
train() client id: f_00004-7-2 loss: 0.835422  [   96/  306]
train() client id: f_00004-7-3 loss: 0.848844  [  128/  306]
train() client id: f_00004-7-4 loss: 0.913082  [  160/  306]
train() client id: f_00004-7-5 loss: 0.932957  [  192/  306]
train() client id: f_00004-7-6 loss: 0.894027  [  224/  306]
train() client id: f_00004-7-7 loss: 0.842438  [  256/  306]
train() client id: f_00004-7-8 loss: 1.031768  [  288/  306]
train() client id: f_00004-8-0 loss: 0.844425  [   32/  306]
train() client id: f_00004-8-1 loss: 0.838422  [   64/  306]
train() client id: f_00004-8-2 loss: 0.923242  [   96/  306]
train() client id: f_00004-8-3 loss: 0.775207  [  128/  306]
train() client id: f_00004-8-4 loss: 0.944894  [  160/  306]
train() client id: f_00004-8-5 loss: 0.854588  [  192/  306]
train() client id: f_00004-8-6 loss: 0.956235  [  224/  306]
train() client id: f_00004-8-7 loss: 0.907247  [  256/  306]
train() client id: f_00004-8-8 loss: 0.956309  [  288/  306]
train() client id: f_00004-9-0 loss: 0.982079  [   32/  306]
train() client id: f_00004-9-1 loss: 0.963546  [   64/  306]
train() client id: f_00004-9-2 loss: 0.821077  [   96/  306]
train() client id: f_00004-9-3 loss: 0.864702  [  128/  306]
train() client id: f_00004-9-4 loss: 0.853498  [  160/  306]
train() client id: f_00004-9-5 loss: 0.857425  [  192/  306]
train() client id: f_00004-9-6 loss: 0.824851  [  224/  306]
train() client id: f_00004-9-7 loss: 0.880698  [  256/  306]
train() client id: f_00004-9-8 loss: 0.952145  [  288/  306]
train() client id: f_00004-10-0 loss: 1.024134  [   32/  306]
train() client id: f_00004-10-1 loss: 0.787309  [   64/  306]
train() client id: f_00004-10-2 loss: 1.052349  [   96/  306]
train() client id: f_00004-10-3 loss: 0.872579  [  128/  306]
train() client id: f_00004-10-4 loss: 0.834667  [  160/  306]
train() client id: f_00004-10-5 loss: 0.844710  [  192/  306]
train() client id: f_00004-10-6 loss: 0.874899  [  224/  306]
train() client id: f_00004-10-7 loss: 0.788346  [  256/  306]
train() client id: f_00004-10-8 loss: 0.928079  [  288/  306]
train() client id: f_00004-11-0 loss: 0.868012  [   32/  306]
train() client id: f_00004-11-1 loss: 0.928858  [   64/  306]
train() client id: f_00004-11-2 loss: 0.933657  [   96/  306]
train() client id: f_00004-11-3 loss: 0.891665  [  128/  306]
train() client id: f_00004-11-4 loss: 0.888193  [  160/  306]
train() client id: f_00004-11-5 loss: 0.892204  [  192/  306]
train() client id: f_00004-11-6 loss: 0.879853  [  224/  306]
train() client id: f_00004-11-7 loss: 0.928345  [  256/  306]
train() client id: f_00004-11-8 loss: 0.838674  [  288/  306]
train() client id: f_00005-0-0 loss: 0.644355  [   32/  146]
train() client id: f_00005-0-1 loss: 0.721791  [   64/  146]
train() client id: f_00005-0-2 loss: 0.632037  [   96/  146]
train() client id: f_00005-0-3 loss: 0.802737  [  128/  146]
train() client id: f_00005-1-0 loss: 0.627054  [   32/  146]
train() client id: f_00005-1-1 loss: 0.685390  [   64/  146]
train() client id: f_00005-1-2 loss: 0.899344  [   96/  146]
train() client id: f_00005-1-3 loss: 0.472594  [  128/  146]
train() client id: f_00005-2-0 loss: 0.621984  [   32/  146]
train() client id: f_00005-2-1 loss: 0.680739  [   64/  146]
train() client id: f_00005-2-2 loss: 0.782264  [   96/  146]
train() client id: f_00005-2-3 loss: 0.750922  [  128/  146]
train() client id: f_00005-3-0 loss: 0.655730  [   32/  146]
train() client id: f_00005-3-1 loss: 0.553561  [   64/  146]
train() client id: f_00005-3-2 loss: 0.654223  [   96/  146]
train() client id: f_00005-3-3 loss: 0.852463  [  128/  146]
train() client id: f_00005-4-0 loss: 0.804557  [   32/  146]
train() client id: f_00005-4-1 loss: 0.605431  [   64/  146]
train() client id: f_00005-4-2 loss: 0.739242  [   96/  146]
train() client id: f_00005-4-3 loss: 0.564302  [  128/  146]
train() client id: f_00005-5-0 loss: 0.749616  [   32/  146]
train() client id: f_00005-5-1 loss: 0.772360  [   64/  146]
train() client id: f_00005-5-2 loss: 0.513778  [   96/  146]
train() client id: f_00005-5-3 loss: 0.643483  [  128/  146]
train() client id: f_00005-6-0 loss: 0.556675  [   32/  146]
train() client id: f_00005-6-1 loss: 0.763252  [   64/  146]
train() client id: f_00005-6-2 loss: 0.826879  [   96/  146]
train() client id: f_00005-6-3 loss: 0.571546  [  128/  146]
train() client id: f_00005-7-0 loss: 0.691007  [   32/  146]
train() client id: f_00005-7-1 loss: 0.737130  [   64/  146]
train() client id: f_00005-7-2 loss: 0.514630  [   96/  146]
train() client id: f_00005-7-3 loss: 0.673184  [  128/  146]
train() client id: f_00005-8-0 loss: 0.756136  [   32/  146]
train() client id: f_00005-8-1 loss: 0.564596  [   64/  146]
train() client id: f_00005-8-2 loss: 0.530226  [   96/  146]
train() client id: f_00005-8-3 loss: 0.635605  [  128/  146]
train() client id: f_00005-9-0 loss: 0.555629  [   32/  146]
train() client id: f_00005-9-1 loss: 0.531842  [   64/  146]
train() client id: f_00005-9-2 loss: 0.741013  [   96/  146]
train() client id: f_00005-9-3 loss: 0.496608  [  128/  146]
train() client id: f_00005-10-0 loss: 0.637304  [   32/  146]
train() client id: f_00005-10-1 loss: 0.685965  [   64/  146]
train() client id: f_00005-10-2 loss: 0.598923  [   96/  146]
train() client id: f_00005-10-3 loss: 0.641208  [  128/  146]
train() client id: f_00005-11-0 loss: 0.646630  [   32/  146]
train() client id: f_00005-11-1 loss: 0.657513  [   64/  146]
train() client id: f_00005-11-2 loss: 0.560022  [   96/  146]
train() client id: f_00005-11-3 loss: 0.680131  [  128/  146]
train() client id: f_00006-0-0 loss: 0.726552  [   32/   54]
train() client id: f_00006-1-0 loss: 0.750798  [   32/   54]
train() client id: f_00006-2-0 loss: 0.719515  [   32/   54]
train() client id: f_00006-3-0 loss: 0.738214  [   32/   54]
train() client id: f_00006-4-0 loss: 0.723728  [   32/   54]
train() client id: f_00006-5-0 loss: 0.729714  [   32/   54]
train() client id: f_00006-6-0 loss: 0.758063  [   32/   54]
train() client id: f_00006-7-0 loss: 0.695017  [   32/   54]
train() client id: f_00006-8-0 loss: 0.721293  [   32/   54]
train() client id: f_00006-9-0 loss: 0.697542  [   32/   54]
train() client id: f_00006-10-0 loss: 0.756058  [   32/   54]
train() client id: f_00006-11-0 loss: 0.758123  [   32/   54]
train() client id: f_00007-0-0 loss: 0.604985  [   32/  179]
train() client id: f_00007-0-1 loss: 0.533994  [   64/  179]
train() client id: f_00007-0-2 loss: 0.559140  [   96/  179]
train() client id: f_00007-0-3 loss: 0.518061  [  128/  179]
train() client id: f_00007-0-4 loss: 0.499816  [  160/  179]
train() client id: f_00007-1-0 loss: 0.584577  [   32/  179]
train() client id: f_00007-1-1 loss: 0.445131  [   64/  179]
train() client id: f_00007-1-2 loss: 0.568334  [   96/  179]
train() client id: f_00007-1-3 loss: 0.588453  [  128/  179]
train() client id: f_00007-1-4 loss: 0.496850  [  160/  179]
train() client id: f_00007-2-0 loss: 0.560538  [   32/  179]
train() client id: f_00007-2-1 loss: 0.500868  [   64/  179]
train() client id: f_00007-2-2 loss: 0.430960  [   96/  179]
train() client id: f_00007-2-3 loss: 0.482830  [  128/  179]
train() client id: f_00007-2-4 loss: 0.578015  [  160/  179]
train() client id: f_00007-3-0 loss: 0.472339  [   32/  179]
train() client id: f_00007-3-1 loss: 0.422990  [   64/  179]
train() client id: f_00007-3-2 loss: 0.582013  [   96/  179]
train() client id: f_00007-3-3 loss: 0.486425  [  128/  179]
train() client id: f_00007-3-4 loss: 0.464034  [  160/  179]
train() client id: f_00007-4-0 loss: 0.496817  [   32/  179]
train() client id: f_00007-4-1 loss: 0.627471  [   64/  179]
train() client id: f_00007-4-2 loss: 0.393712  [   96/  179]
train() client id: f_00007-4-3 loss: 0.605884  [  128/  179]
train() client id: f_00007-4-4 loss: 0.447138  [  160/  179]
train() client id: f_00007-5-0 loss: 0.558127  [   32/  179]
train() client id: f_00007-5-1 loss: 0.516687  [   64/  179]
train() client id: f_00007-5-2 loss: 0.458766  [   96/  179]
train() client id: f_00007-5-3 loss: 0.568382  [  128/  179]
train() client id: f_00007-5-4 loss: 0.436358  [  160/  179]
train() client id: f_00007-6-0 loss: 0.674195  [   32/  179]
train() client id: f_00007-6-1 loss: 0.542491  [   64/  179]
train() client id: f_00007-6-2 loss: 0.411367  [   96/  179]
train() client id: f_00007-6-3 loss: 0.456914  [  128/  179]
train() client id: f_00007-6-4 loss: 0.371384  [  160/  179]
train() client id: f_00007-7-0 loss: 0.553255  [   32/  179]
train() client id: f_00007-7-1 loss: 0.446983  [   64/  179]
train() client id: f_00007-7-2 loss: 0.438156  [   96/  179]
train() client id: f_00007-7-3 loss: 0.583143  [  128/  179]
train() client id: f_00007-7-4 loss: 0.470375  [  160/  179]
train() client id: f_00007-8-0 loss: 0.363288  [   32/  179]
train() client id: f_00007-8-1 loss: 0.760575  [   64/  179]
train() client id: f_00007-8-2 loss: 0.384142  [   96/  179]
train() client id: f_00007-8-3 loss: 0.465627  [  128/  179]
train() client id: f_00007-8-4 loss: 0.423586  [  160/  179]
train() client id: f_00007-9-0 loss: 0.382531  [   32/  179]
train() client id: f_00007-9-1 loss: 0.409227  [   64/  179]
train() client id: f_00007-9-2 loss: 0.485462  [   96/  179]
train() client id: f_00007-9-3 loss: 0.428305  [  128/  179]
train() client id: f_00007-9-4 loss: 0.513019  [  160/  179]
train() client id: f_00007-10-0 loss: 0.613004  [   32/  179]
train() client id: f_00007-10-1 loss: 0.584059  [   64/  179]
train() client id: f_00007-10-2 loss: 0.359442  [   96/  179]
train() client id: f_00007-10-3 loss: 0.372798  [  128/  179]
train() client id: f_00007-10-4 loss: 0.530517  [  160/  179]
train() client id: f_00007-11-0 loss: 0.451704  [   32/  179]
train() client id: f_00007-11-1 loss: 0.450723  [   64/  179]
train() client id: f_00007-11-2 loss: 0.542088  [   96/  179]
train() client id: f_00007-11-3 loss: 0.493239  [  128/  179]
train() client id: f_00007-11-4 loss: 0.520659  [  160/  179]
train() client id: f_00008-0-0 loss: 0.981127  [   32/  130]
train() client id: f_00008-0-1 loss: 0.988407  [   64/  130]
train() client id: f_00008-0-2 loss: 1.053730  [   96/  130]
train() client id: f_00008-0-3 loss: 0.820491  [  128/  130]
train() client id: f_00008-1-0 loss: 0.977211  [   32/  130]
train() client id: f_00008-1-1 loss: 0.942565  [   64/  130]
train() client id: f_00008-1-2 loss: 0.978142  [   96/  130]
train() client id: f_00008-1-3 loss: 0.916163  [  128/  130]
train() client id: f_00008-2-0 loss: 0.975284  [   32/  130]
train() client id: f_00008-2-1 loss: 0.919059  [   64/  130]
train() client id: f_00008-2-2 loss: 1.038829  [   96/  130]
train() client id: f_00008-2-3 loss: 0.920436  [  128/  130]
train() client id: f_00008-3-0 loss: 0.927964  [   32/  130]
train() client id: f_00008-3-1 loss: 0.992742  [   64/  130]
train() client id: f_00008-3-2 loss: 0.914143  [   96/  130]
train() client id: f_00008-3-3 loss: 0.985432  [  128/  130]
train() client id: f_00008-4-0 loss: 0.973618  [   32/  130]
train() client id: f_00008-4-1 loss: 1.056860  [   64/  130]
train() client id: f_00008-4-2 loss: 0.850110  [   96/  130]
train() client id: f_00008-4-3 loss: 0.969529  [  128/  130]
train() client id: f_00008-5-0 loss: 0.873848  [   32/  130]
train() client id: f_00008-5-1 loss: 0.974134  [   64/  130]
train() client id: f_00008-5-2 loss: 1.000073  [   96/  130]
train() client id: f_00008-5-3 loss: 1.007228  [  128/  130]
train() client id: f_00008-6-0 loss: 1.108286  [   32/  130]
train() client id: f_00008-6-1 loss: 0.922714  [   64/  130]
train() client id: f_00008-6-2 loss: 0.889230  [   96/  130]
train() client id: f_00008-6-3 loss: 0.934264  [  128/  130]
train() client id: f_00008-7-0 loss: 1.050742  [   32/  130]
train() client id: f_00008-7-1 loss: 0.904709  [   64/  130]
train() client id: f_00008-7-2 loss: 0.903104  [   96/  130]
train() client id: f_00008-7-3 loss: 0.993887  [  128/  130]
train() client id: f_00008-8-0 loss: 0.874049  [   32/  130]
train() client id: f_00008-8-1 loss: 1.032631  [   64/  130]
train() client id: f_00008-8-2 loss: 0.964336  [   96/  130]
train() client id: f_00008-8-3 loss: 0.951537  [  128/  130]
train() client id: f_00008-9-0 loss: 1.051605  [   32/  130]
train() client id: f_00008-9-1 loss: 0.834103  [   64/  130]
train() client id: f_00008-9-2 loss: 0.973709  [   96/  130]
train() client id: f_00008-9-3 loss: 0.999804  [  128/  130]
train() client id: f_00008-10-0 loss: 0.987005  [   32/  130]
train() client id: f_00008-10-1 loss: 0.994531  [   64/  130]
train() client id: f_00008-10-2 loss: 0.936483  [   96/  130]
train() client id: f_00008-10-3 loss: 0.945035  [  128/  130]
train() client id: f_00008-11-0 loss: 0.944548  [   32/  130]
train() client id: f_00008-11-1 loss: 0.928549  [   64/  130]
train() client id: f_00008-11-2 loss: 0.956775  [   96/  130]
train() client id: f_00008-11-3 loss: 1.034964  [  128/  130]
train() client id: f_00009-0-0 loss: 1.034113  [   32/  118]
train() client id: f_00009-0-1 loss: 1.020669  [   64/  118]
train() client id: f_00009-0-2 loss: 1.077893  [   96/  118]
train() client id: f_00009-1-0 loss: 1.075943  [   32/  118]
train() client id: f_00009-1-1 loss: 1.076625  [   64/  118]
train() client id: f_00009-1-2 loss: 0.943331  [   96/  118]
train() client id: f_00009-2-0 loss: 1.018455  [   32/  118]
train() client id: f_00009-2-1 loss: 0.996024  [   64/  118]
train() client id: f_00009-2-2 loss: 0.872402  [   96/  118]
train() client id: f_00009-3-0 loss: 0.964679  [   32/  118]
train() client id: f_00009-3-1 loss: 0.908111  [   64/  118]
train() client id: f_00009-3-2 loss: 0.956837  [   96/  118]
train() client id: f_00009-4-0 loss: 0.962836  [   32/  118]
train() client id: f_00009-4-1 loss: 0.973849  [   64/  118]
train() client id: f_00009-4-2 loss: 0.867801  [   96/  118]
train() client id: f_00009-5-0 loss: 0.872214  [   32/  118]
train() client id: f_00009-5-1 loss: 0.932827  [   64/  118]
train() client id: f_00009-5-2 loss: 0.990766  [   96/  118]
train() client id: f_00009-6-0 loss: 0.898413  [   32/  118]
train() client id: f_00009-6-1 loss: 0.709942  [   64/  118]
train() client id: f_00009-6-2 loss: 1.064946  [   96/  118]
train() client id: f_00009-7-0 loss: 0.974027  [   32/  118]
train() client id: f_00009-7-1 loss: 0.901398  [   64/  118]
train() client id: f_00009-7-2 loss: 0.723966  [   96/  118]
train() client id: f_00009-8-0 loss: 0.787432  [   32/  118]
train() client id: f_00009-8-1 loss: 0.888130  [   64/  118]
train() client id: f_00009-8-2 loss: 0.868632  [   96/  118]
train() client id: f_00009-9-0 loss: 0.960292  [   32/  118]
train() client id: f_00009-9-1 loss: 0.845910  [   64/  118]
train() client id: f_00009-9-2 loss: 0.792472  [   96/  118]
train() client id: f_00009-10-0 loss: 0.838349  [   32/  118]
train() client id: f_00009-10-1 loss: 0.912141  [   64/  118]
train() client id: f_00009-10-2 loss: 0.829818  [   96/  118]
train() client id: f_00009-11-0 loss: 0.822419  [   32/  118]
train() client id: f_00009-11-1 loss: 0.923713  [   64/  118]
train() client id: f_00009-11-2 loss: 0.754138  [   96/  118]
At round 11 accuracy: 0.6312997347480106
At round 11 training accuracy: 0.5761234071093226
At round 11 training loss: 0.8698481919678258
update_location
xs = -3.905658 4.200318 75.009024 18.811294 0.979296 3.956410 -37.443192 -16.324852 59.663977 -2.060879 
ys = 67.587959 50.555839 1.320614 -37.455176 29.350187 12.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 120.761693 112.131777 125.012390 108.428571 104.222802 100.895271 106.812374 101.327080 117.764426 100.101244 
dists_bs = 202.016386 218.172187 304.415902 287.576111 228.435069 241.515313 224.660879 235.617608 282.580431 243.204509 
uav_gains = -102.048464 -101.243300 -102.424207 -100.878644 -100.449097 -100.096788 -100.715580 -100.143157 -101.775517 -100.011003 
bs_gains = -104.117884 -105.053442 -109.104145 -108.412138 -105.612417 -106.289510 -105.409828 -105.988876 -108.199037 -106.374265 
Round 12
-------------------------------
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.40743039 19.62699225  9.26366007  3.31236285 22.63944362 10.91364539
  4.11791779 13.28077427  9.76466884  8.86129599]
obj_prev = 111.18819146740302
eta_min = 1.3309903798194387e-10	eta_max = 0.9206473861818304
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 25.855891314518058	eta = 0.9090909090909091
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 44.77490344422677	eta = 0.5249672010962586
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 35.73954375539702	eta = 0.6576848294802706
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 34.11938254858583	eta = 0.6889150384535667
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 34.039315064938776	eta = 0.6905355085914164
af = 23.50535574047096	bf = 1.8301460856680722	zeta = 34.039105597683566	eta = 0.6905397579562299
eta = 0.6905397579562299
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [0.03055476 0.064262   0.03006976 0.01042742 0.07420442 0.03540472
 0.0130949  0.04340716 0.03152475 0.02861478]
ene_total = [2.91251884 5.6185508  2.88405621 1.31991488 6.41048895 3.41632264
 1.52386453 3.86970026 3.19241569 2.89127279]
ti_comp = [0.30907982 0.29827603 0.3078889  0.3125449  0.295892   0.29281253
 0.31300078 0.31455258 0.28279738 0.29241128]
ti_coms = [0.06887363 0.07967742 0.07006456 0.06540855 0.08206146 0.08514092
 0.06495268 0.06340088 0.09515608 0.08554218]
t_total = [29.39994965 29.39994965 29.39994965 29.39994965 29.39994965 29.39994965
 29.39994965 29.39994965 29.39994965 29.39994965]
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [1.86627385e-05 1.86425773e-04 1.79259442e-05 7.25413394e-07
 2.91677471e-04 3.23507450e-05 1.43250282e-06 5.16627645e-05
 2.44841111e-05 1.71262716e-05]
ene_total = [0.53720059 0.6342901  0.54640715 0.5088514  0.66102208 0.66480406
 0.50536026 0.49719653 0.74209725 0.66674105]
optimize_network iter = 0 obj = 5.96397047673148
eta = 0.6905397579562299
freqs = [4.94285918e+07 1.07722361e+08 4.88321676e+07 1.66814724e+07
 1.25391054e+08 6.04562965e+07 2.09183144e+07 6.89982586e+07
 5.57373503e+07 4.89289881e+07]
eta_min = 0.6905397579563017	eta_max = 0.6905397579562269
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 0.04594401078389732	eta = 0.909090909090909
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 20.175396261636216	eta = 0.002070208782478131
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 2.0926044394311156	eta = 0.019959473345172857
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 2.034227485351597	eta = 0.020532257494100595
af = 0.04176728253081574	bf = 1.8301460856680722	zeta = 2.0342105771628316	eta = 0.02053242815651352
eta = 0.02053242815651352
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [1.95847449e-04 1.95635877e-03 1.88115503e-04 7.61251424e-06
 3.06087387e-03 3.39489881e-04 1.50327361e-05 5.42150909e-04
 2.56937143e-04 1.79723710e-04]
ene_total = [0.17408175 0.24390138 0.17681862 0.16093954 0.27690584 0.21759158
 0.16000151 0.16914252 0.24017663 0.21465121]
ti_comp = [0.30907982 0.29827603 0.3078889  0.3125449  0.295892   0.29281253
 0.31300078 0.31455258 0.28279738 0.29241128]
ti_coms = [0.06887363 0.07967742 0.07006456 0.06540855 0.08206146 0.08514092
 0.06495268 0.06340088 0.09515608 0.08554218]
t_total = [29.39994965 29.39994965 29.39994965 29.39994965 29.39994965 29.39994965
 29.39994965 29.39994965 29.39994965 29.39994965]
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [1.86627385e-05 1.86425773e-04 1.79259442e-05 7.25413394e-07
 2.91677471e-04 3.23507450e-05 1.43250282e-06 5.16627645e-05
 2.44841111e-05 1.71262716e-05]
ene_total = [0.53720059 0.6342901  0.54640715 0.5088514  0.66102208 0.66480406
 0.50536026 0.49719653 0.74209725 0.66674105]
optimize_network iter = 1 obj = 5.963970476732851
eta = 0.6905397579563017
freqs = [4.94285918e+07 1.07722361e+08 4.88321676e+07 1.66814724e+07
 1.25391054e+08 6.04562965e+07 2.09183144e+07 6.89982586e+07
 5.57373503e+07 4.89289881e+07]
Done!
ene_coms = [0.00688736 0.00796774 0.00700646 0.00654086 0.00820615 0.00851409
 0.00649527 0.00634009 0.00951561 0.00855422]
ene_comp = [1.84704838e-05 1.84505302e-04 1.77412796e-05 7.17940527e-07
 2.88672747e-04 3.20174828e-05 1.41774584e-06 5.11305590e-05
 2.42318873e-05 1.69498448e-05]
ene_total = [0.00690583 0.00815225 0.0070242  0.00654157 0.00849482 0.00854611
 0.00649669 0.00639122 0.00953984 0.00857117]
At round 12 energy consumption: 0.07666369174488584
At round 12 eta: 0.6905397579563017
At round 12 a_n: 24.072052692622567
At round 12 local rounds: 12.124905073660063
At round 12 global rounds: 77.78722246725123
gradient difference: 0.4172072410583496
train() client id: f_00000-0-0 loss: 1.620379  [   32/  126]
train() client id: f_00000-0-1 loss: 1.397483  [   64/  126]
train() client id: f_00000-0-2 loss: 1.271710  [   96/  126]
train() client id: f_00000-1-0 loss: 1.437161  [   32/  126]
train() client id: f_00000-1-1 loss: 1.336123  [   64/  126]
train() client id: f_00000-1-2 loss: 1.228163  [   96/  126]
train() client id: f_00000-2-0 loss: 1.274589  [   32/  126]
train() client id: f_00000-2-1 loss: 1.107684  [   64/  126]
train() client id: f_00000-2-2 loss: 1.077000  [   96/  126]
train() client id: f_00000-3-0 loss: 1.195981  [   32/  126]
train() client id: f_00000-3-1 loss: 1.002661  [   64/  126]
train() client id: f_00000-3-2 loss: 1.072552  [   96/  126]
train() client id: f_00000-4-0 loss: 1.026529  [   32/  126]
train() client id: f_00000-4-1 loss: 1.093535  [   64/  126]
train() client id: f_00000-4-2 loss: 0.980541  [   96/  126]
train() client id: f_00000-5-0 loss: 0.911861  [   32/  126]
train() client id: f_00000-5-1 loss: 0.962981  [   64/  126]
train() client id: f_00000-5-2 loss: 1.039561  [   96/  126]
train() client id: f_00000-6-0 loss: 0.935127  [   32/  126]
train() client id: f_00000-6-1 loss: 0.996074  [   64/  126]
train() client id: f_00000-6-2 loss: 0.913868  [   96/  126]
train() client id: f_00000-7-0 loss: 0.858996  [   32/  126]
train() client id: f_00000-7-1 loss: 0.862067  [   64/  126]
train() client id: f_00000-7-2 loss: 0.858991  [   96/  126]
train() client id: f_00000-8-0 loss: 0.860125  [   32/  126]
train() client id: f_00000-8-1 loss: 0.819509  [   64/  126]
train() client id: f_00000-8-2 loss: 0.850188  [   96/  126]
train() client id: f_00000-9-0 loss: 0.830117  [   32/  126]
train() client id: f_00000-9-1 loss: 0.851690  [   64/  126]
train() client id: f_00000-9-2 loss: 0.857333  [   96/  126]
train() client id: f_00000-10-0 loss: 0.721755  [   32/  126]
train() client id: f_00000-10-1 loss: 0.881391  [   64/  126]
train() client id: f_00000-10-2 loss: 0.778998  [   96/  126]
train() client id: f_00000-11-0 loss: 0.812879  [   32/  126]
train() client id: f_00000-11-1 loss: 0.839972  [   64/  126]
train() client id: f_00000-11-2 loss: 0.884486  [   96/  126]
train() client id: f_00001-0-0 loss: 0.580446  [   32/  265]
train() client id: f_00001-0-1 loss: 0.627719  [   64/  265]
train() client id: f_00001-0-2 loss: 0.532853  [   96/  265]
train() client id: f_00001-0-3 loss: 0.611220  [  128/  265]
train() client id: f_00001-0-4 loss: 0.521562  [  160/  265]
train() client id: f_00001-0-5 loss: 0.557976  [  192/  265]
train() client id: f_00001-0-6 loss: 0.468370  [  224/  265]
train() client id: f_00001-0-7 loss: 0.468697  [  256/  265]
train() client id: f_00001-1-0 loss: 0.678149  [   32/  265]
train() client id: f_00001-1-1 loss: 0.556156  [   64/  265]
train() client id: f_00001-1-2 loss: 0.609452  [   96/  265]
train() client id: f_00001-1-3 loss: 0.544036  [  128/  265]
train() client id: f_00001-1-4 loss: 0.472907  [  160/  265]
train() client id: f_00001-1-5 loss: 0.462723  [  192/  265]
train() client id: f_00001-1-6 loss: 0.489624  [  224/  265]
train() client id: f_00001-1-7 loss: 0.497975  [  256/  265]
train() client id: f_00001-2-0 loss: 0.669817  [   32/  265]
train() client id: f_00001-2-1 loss: 0.579246  [   64/  265]
train() client id: f_00001-2-2 loss: 0.464649  [   96/  265]
train() client id: f_00001-2-3 loss: 0.626836  [  128/  265]
train() client id: f_00001-2-4 loss: 0.468320  [  160/  265]
train() client id: f_00001-2-5 loss: 0.560545  [  192/  265]
train() client id: f_00001-2-6 loss: 0.442533  [  224/  265]
train() client id: f_00001-2-7 loss: 0.435822  [  256/  265]
train() client id: f_00001-3-0 loss: 0.565145  [   32/  265]
train() client id: f_00001-3-1 loss: 0.491958  [   64/  265]
train() client id: f_00001-3-2 loss: 0.492323  [   96/  265]
train() client id: f_00001-3-3 loss: 0.607880  [  128/  265]
train() client id: f_00001-3-4 loss: 0.441117  [  160/  265]
train() client id: f_00001-3-5 loss: 0.573359  [  192/  265]
train() client id: f_00001-3-6 loss: 0.579833  [  224/  265]
train() client id: f_00001-3-7 loss: 0.431807  [  256/  265]
train() client id: f_00001-4-0 loss: 0.490556  [   32/  265]
train() client id: f_00001-4-1 loss: 0.532095  [   64/  265]
train() client id: f_00001-4-2 loss: 0.477115  [   96/  265]
train() client id: f_00001-4-3 loss: 0.578383  [  128/  265]
train() client id: f_00001-4-4 loss: 0.491998  [  160/  265]
train() client id: f_00001-4-5 loss: 0.441467  [  192/  265]
train() client id: f_00001-4-6 loss: 0.525893  [  224/  265]
train() client id: f_00001-4-7 loss: 0.557878  [  256/  265]
train() client id: f_00001-5-0 loss: 0.506760  [   32/  265]
train() client id: f_00001-5-1 loss: 0.619747  [   64/  265]
train() client id: f_00001-5-2 loss: 0.486040  [   96/  265]
train() client id: f_00001-5-3 loss: 0.412482  [  128/  265]
train() client id: f_00001-5-4 loss: 0.490038  [  160/  265]
train() client id: f_00001-5-5 loss: 0.533547  [  192/  265]
train() client id: f_00001-5-6 loss: 0.476731  [  224/  265]
train() client id: f_00001-5-7 loss: 0.554426  [  256/  265]
train() client id: f_00001-6-0 loss: 0.489310  [   32/  265]
train() client id: f_00001-6-1 loss: 0.534664  [   64/  265]
train() client id: f_00001-6-2 loss: 0.477127  [   96/  265]
train() client id: f_00001-6-3 loss: 0.479669  [  128/  265]
train() client id: f_00001-6-4 loss: 0.488080  [  160/  265]
train() client id: f_00001-6-5 loss: 0.489436  [  192/  265]
train() client id: f_00001-6-6 loss: 0.505920  [  224/  265]
train() client id: f_00001-6-7 loss: 0.578264  [  256/  265]
train() client id: f_00001-7-0 loss: 0.425517  [   32/  265]
train() client id: f_00001-7-1 loss: 0.653291  [   64/  265]
train() client id: f_00001-7-2 loss: 0.521123  [   96/  265]
train() client id: f_00001-7-3 loss: 0.416188  [  128/  265]
train() client id: f_00001-7-4 loss: 0.421760  [  160/  265]
train() client id: f_00001-7-5 loss: 0.617466  [  192/  265]
train() client id: f_00001-7-6 loss: 0.609504  [  224/  265]
train() client id: f_00001-7-7 loss: 0.406295  [  256/  265]
train() client id: f_00001-8-0 loss: 0.622809  [   32/  265]
train() client id: f_00001-8-1 loss: 0.415593  [   64/  265]
train() client id: f_00001-8-2 loss: 0.677484  [   96/  265]
train() client id: f_00001-8-3 loss: 0.463297  [  128/  265]
train() client id: f_00001-8-4 loss: 0.398675  [  160/  265]
train() client id: f_00001-8-5 loss: 0.428084  [  192/  265]
train() client id: f_00001-8-6 loss: 0.528676  [  224/  265]
train() client id: f_00001-8-7 loss: 0.499182  [  256/  265]
train() client id: f_00001-9-0 loss: 0.499737  [   32/  265]
train() client id: f_00001-9-1 loss: 0.475735  [   64/  265]
train() client id: f_00001-9-2 loss: 0.589917  [   96/  265]
train() client id: f_00001-9-3 loss: 0.713336  [  128/  265]
train() client id: f_00001-9-4 loss: 0.489767  [  160/  265]
train() client id: f_00001-9-5 loss: 0.409238  [  192/  265]
train() client id: f_00001-9-6 loss: 0.414121  [  224/  265]
train() client id: f_00001-9-7 loss: 0.475743  [  256/  265]
train() client id: f_00001-10-0 loss: 0.550388  [   32/  265]
train() client id: f_00001-10-1 loss: 0.416203  [   64/  265]
train() client id: f_00001-10-2 loss: 0.454435  [   96/  265]
train() client id: f_00001-10-3 loss: 0.525121  [  128/  265]
train() client id: f_00001-10-4 loss: 0.668219  [  160/  265]
train() client id: f_00001-10-5 loss: 0.607970  [  192/  265]
train() client id: f_00001-10-6 loss: 0.425197  [  224/  265]
train() client id: f_00001-10-7 loss: 0.411742  [  256/  265]
train() client id: f_00001-11-0 loss: 0.622763  [   32/  265]
train() client id: f_00001-11-1 loss: 0.408110  [   64/  265]
train() client id: f_00001-11-2 loss: 0.486331  [   96/  265]
train() client id: f_00001-11-3 loss: 0.469925  [  128/  265]
train() client id: f_00001-11-4 loss: 0.637354  [  160/  265]
train() client id: f_00001-11-5 loss: 0.558840  [  192/  265]
train() client id: f_00001-11-6 loss: 0.467060  [  224/  265]
train() client id: f_00001-11-7 loss: 0.411882  [  256/  265]
train() client id: f_00002-0-0 loss: 1.239597  [   32/  124]
train() client id: f_00002-0-1 loss: 1.180513  [   64/  124]
train() client id: f_00002-0-2 loss: 0.998184  [   96/  124]
train() client id: f_00002-1-0 loss: 1.099746  [   32/  124]
train() client id: f_00002-1-1 loss: 1.140273  [   64/  124]
train() client id: f_00002-1-2 loss: 1.031308  [   96/  124]
train() client id: f_00002-2-0 loss: 1.051327  [   32/  124]
train() client id: f_00002-2-1 loss: 0.960966  [   64/  124]
train() client id: f_00002-2-2 loss: 1.077812  [   96/  124]
train() client id: f_00002-3-0 loss: 1.051923  [   32/  124]
train() client id: f_00002-3-1 loss: 1.022367  [   64/  124]
train() client id: f_00002-3-2 loss: 1.063056  [   96/  124]
train() client id: f_00002-4-0 loss: 1.000135  [   32/  124]
train() client id: f_00002-4-1 loss: 0.915069  [   64/  124]
train() client id: f_00002-4-2 loss: 0.954295  [   96/  124]
train() client id: f_00002-5-0 loss: 0.960842  [   32/  124]
train() client id: f_00002-5-1 loss: 1.083406  [   64/  124]
train() client id: f_00002-5-2 loss: 0.898878  [   96/  124]
train() client id: f_00002-6-0 loss: 0.839655  [   32/  124]
train() client id: f_00002-6-1 loss: 0.998923  [   64/  124]
train() client id: f_00002-6-2 loss: 0.931872  [   96/  124]
train() client id: f_00002-7-0 loss: 0.895636  [   32/  124]
train() client id: f_00002-7-1 loss: 0.882890  [   64/  124]
train() client id: f_00002-7-2 loss: 0.921020  [   96/  124]
train() client id: f_00002-8-0 loss: 0.874575  [   32/  124]
train() client id: f_00002-8-1 loss: 0.861324  [   64/  124]
train() client id: f_00002-8-2 loss: 1.026435  [   96/  124]
train() client id: f_00002-9-0 loss: 0.892920  [   32/  124]
train() client id: f_00002-9-1 loss: 0.926097  [   64/  124]
train() client id: f_00002-9-2 loss: 0.846240  [   96/  124]
train() client id: f_00002-10-0 loss: 0.969463  [   32/  124]
train() client id: f_00002-10-1 loss: 0.849372  [   64/  124]
train() client id: f_00002-10-2 loss: 0.889628  [   96/  124]
train() client id: f_00002-11-0 loss: 0.883791  [   32/  124]
train() client id: f_00002-11-1 loss: 0.816375  [   64/  124]
train() client id: f_00002-11-2 loss: 0.855755  [   96/  124]
train() client id: f_00003-0-0 loss: 0.867366  [   32/   43]
train() client id: f_00003-1-0 loss: 0.866527  [   32/   43]
train() client id: f_00003-2-0 loss: 0.984779  [   32/   43]
train() client id: f_00003-3-0 loss: 0.903677  [   32/   43]
train() client id: f_00003-4-0 loss: 0.951992  [   32/   43]
train() client id: f_00003-5-0 loss: 0.893039  [   32/   43]
train() client id: f_00003-6-0 loss: 0.900896  [   32/   43]
train() client id: f_00003-7-0 loss: 1.097267  [   32/   43]
train() client id: f_00003-8-0 loss: 0.939770  [   32/   43]
train() client id: f_00003-9-0 loss: 0.888443  [   32/   43]
train() client id: f_00003-10-0 loss: 0.899211  [   32/   43]
train() client id: f_00003-11-0 loss: 0.913564  [   32/   43]
train() client id: f_00004-0-0 loss: 1.140723  [   32/  306]
train() client id: f_00004-0-1 loss: 0.944836  [   64/  306]
train() client id: f_00004-0-2 loss: 0.898661  [   96/  306]
train() client id: f_00004-0-3 loss: 0.859557  [  128/  306]
train() client id: f_00004-0-4 loss: 0.943596  [  160/  306]
train() client id: f_00004-0-5 loss: 1.024636  [  192/  306]
train() client id: f_00004-0-6 loss: 1.010919  [  224/  306]
train() client id: f_00004-0-7 loss: 0.931559  [  256/  306]
train() client id: f_00004-0-8 loss: 0.991494  [  288/  306]
train() client id: f_00004-1-0 loss: 1.004915  [   32/  306]
train() client id: f_00004-1-1 loss: 1.060473  [   64/  306]
train() client id: f_00004-1-2 loss: 0.931224  [   96/  306]
train() client id: f_00004-1-3 loss: 0.926748  [  128/  306]
train() client id: f_00004-1-4 loss: 0.947821  [  160/  306]
train() client id: f_00004-1-5 loss: 0.876159  [  192/  306]
train() client id: f_00004-1-6 loss: 1.041355  [  224/  306]
train() client id: f_00004-1-7 loss: 1.031638  [  256/  306]
train() client id: f_00004-1-8 loss: 1.001451  [  288/  306]
train() client id: f_00004-2-0 loss: 1.013013  [   32/  306]
train() client id: f_00004-2-1 loss: 0.936204  [   64/  306]
train() client id: f_00004-2-2 loss: 1.000216  [   96/  306]
train() client id: f_00004-2-3 loss: 0.971619  [  128/  306]
train() client id: f_00004-2-4 loss: 0.922316  [  160/  306]
train() client id: f_00004-2-5 loss: 1.034284  [  192/  306]
train() client id: f_00004-2-6 loss: 0.856809  [  224/  306]
train() client id: f_00004-2-7 loss: 0.957998  [  256/  306]
train() client id: f_00004-2-8 loss: 1.006344  [  288/  306]
train() client id: f_00004-3-0 loss: 1.004732  [   32/  306]
train() client id: f_00004-3-1 loss: 0.867368  [   64/  306]
train() client id: f_00004-3-2 loss: 0.975829  [   96/  306]
train() client id: f_00004-3-3 loss: 0.953623  [  128/  306]
train() client id: f_00004-3-4 loss: 0.868490  [  160/  306]
train() client id: f_00004-3-5 loss: 1.006603  [  192/  306]
train() client id: f_00004-3-6 loss: 1.000219  [  224/  306]
train() client id: f_00004-3-7 loss: 0.928055  [  256/  306]
train() client id: f_00004-3-8 loss: 0.995782  [  288/  306]
train() client id: f_00004-4-0 loss: 0.898096  [   32/  306]
train() client id: f_00004-4-1 loss: 1.002588  [   64/  306]
train() client id: f_00004-4-2 loss: 0.892714  [   96/  306]
train() client id: f_00004-4-3 loss: 0.927317  [  128/  306]
train() client id: f_00004-4-4 loss: 0.929479  [  160/  306]
train() client id: f_00004-4-5 loss: 0.987271  [  192/  306]
train() client id: f_00004-4-6 loss: 1.026170  [  224/  306]
train() client id: f_00004-4-7 loss: 1.116423  [  256/  306]
train() client id: f_00004-4-8 loss: 0.941797  [  288/  306]
train() client id: f_00004-5-0 loss: 1.020375  [   32/  306]
train() client id: f_00004-5-1 loss: 1.006707  [   64/  306]
train() client id: f_00004-5-2 loss: 0.901454  [   96/  306]
train() client id: f_00004-5-3 loss: 0.995341  [  128/  306]
train() client id: f_00004-5-4 loss: 0.915571  [  160/  306]
train() client id: f_00004-5-5 loss: 0.901304  [  192/  306]
train() client id: f_00004-5-6 loss: 0.996338  [  224/  306]
train() client id: f_00004-5-7 loss: 0.946267  [  256/  306]
train() client id: f_00004-5-8 loss: 0.944161  [  288/  306]
train() client id: f_00004-6-0 loss: 1.015816  [   32/  306]
train() client id: f_00004-6-1 loss: 0.946130  [   64/  306]
train() client id: f_00004-6-2 loss: 0.969811  [   96/  306]
train() client id: f_00004-6-3 loss: 1.013360  [  128/  306]
train() client id: f_00004-6-4 loss: 1.013751  [  160/  306]
train() client id: f_00004-6-5 loss: 0.936706  [  192/  306]
train() client id: f_00004-6-6 loss: 0.908743  [  224/  306]
train() client id: f_00004-6-7 loss: 0.963182  [  256/  306]
train() client id: f_00004-6-8 loss: 0.900984  [  288/  306]
train() client id: f_00004-7-0 loss: 0.923264  [   32/  306]
train() client id: f_00004-7-1 loss: 0.863360  [   64/  306]
train() client id: f_00004-7-2 loss: 1.075368  [   96/  306]
train() client id: f_00004-7-3 loss: 0.852069  [  128/  306]
train() client id: f_00004-7-4 loss: 1.057341  [  160/  306]
train() client id: f_00004-7-5 loss: 0.901639  [  192/  306]
train() client id: f_00004-7-6 loss: 0.976087  [  224/  306]
train() client id: f_00004-7-7 loss: 0.938976  [  256/  306]
train() client id: f_00004-7-8 loss: 0.973836  [  288/  306]
train() client id: f_00004-8-0 loss: 1.039025  [   32/  306]
train() client id: f_00004-8-1 loss: 0.996543  [   64/  306]
train() client id: f_00004-8-2 loss: 0.872293  [   96/  306]
train() client id: f_00004-8-3 loss: 0.918856  [  128/  306]
train() client id: f_00004-8-4 loss: 0.885647  [  160/  306]
train() client id: f_00004-8-5 loss: 0.988883  [  192/  306]
train() client id: f_00004-8-6 loss: 0.932984  [  224/  306]
train() client id: f_00004-8-7 loss: 0.904740  [  256/  306]
train() client id: f_00004-8-8 loss: 1.013205  [  288/  306]
train() client id: f_00004-9-0 loss: 0.920698  [   32/  306]
train() client id: f_00004-9-1 loss: 0.863080  [   64/  306]
train() client id: f_00004-9-2 loss: 0.876108  [   96/  306]
train() client id: f_00004-9-3 loss: 0.997322  [  128/  306]
train() client id: f_00004-9-4 loss: 1.035181  [  160/  306]
train() client id: f_00004-9-5 loss: 1.052285  [  192/  306]
train() client id: f_00004-9-6 loss: 0.861059  [  224/  306]
train() client id: f_00004-9-7 loss: 0.919317  [  256/  306]
train() client id: f_00004-9-8 loss: 0.999433  [  288/  306]
train() client id: f_00004-10-0 loss: 0.926732  [   32/  306]
train() client id: f_00004-10-1 loss: 0.877610  [   64/  306]
train() client id: f_00004-10-2 loss: 0.998644  [   96/  306]
train() client id: f_00004-10-3 loss: 0.924545  [  128/  306]
train() client id: f_00004-10-4 loss: 0.946315  [  160/  306]
train() client id: f_00004-10-5 loss: 0.975769  [  192/  306]
train() client id: f_00004-10-6 loss: 0.965184  [  224/  306]
train() client id: f_00004-10-7 loss: 0.922569  [  256/  306]
train() client id: f_00004-10-8 loss: 0.967799  [  288/  306]
train() client id: f_00004-11-0 loss: 0.934194  [   32/  306]
train() client id: f_00004-11-1 loss: 0.918700  [   64/  306]
train() client id: f_00004-11-2 loss: 0.916755  [   96/  306]
train() client id: f_00004-11-3 loss: 1.029610  [  128/  306]
train() client id: f_00004-11-4 loss: 1.011749  [  160/  306]
train() client id: f_00004-11-5 loss: 0.979662  [  192/  306]
train() client id: f_00004-11-6 loss: 0.829105  [  224/  306]
train() client id: f_00004-11-7 loss: 0.993828  [  256/  306]
train() client id: f_00004-11-8 loss: 0.975851  [  288/  306]
train() client id: f_00005-0-0 loss: 0.777845  [   32/  146]
train() client id: f_00005-0-1 loss: 0.805906  [   64/  146]
train() client id: f_00005-0-2 loss: 0.648540  [   96/  146]
train() client id: f_00005-0-3 loss: 0.633894  [  128/  146]
train() client id: f_00005-1-0 loss: 0.597750  [   32/  146]
train() client id: f_00005-1-1 loss: 0.885653  [   64/  146]
train() client id: f_00005-1-2 loss: 0.777978  [   96/  146]
train() client id: f_00005-1-3 loss: 0.573742  [  128/  146]
train() client id: f_00005-2-0 loss: 0.855024  [   32/  146]
train() client id: f_00005-2-1 loss: 0.582794  [   64/  146]
train() client id: f_00005-2-2 loss: 0.708339  [   96/  146]
train() client id: f_00005-2-3 loss: 0.768618  [  128/  146]
train() client id: f_00005-3-0 loss: 0.718495  [   32/  146]
train() client id: f_00005-3-1 loss: 0.754926  [   64/  146]
train() client id: f_00005-3-2 loss: 0.515205  [   96/  146]
train() client id: f_00005-3-3 loss: 0.843339  [  128/  146]
train() client id: f_00005-4-0 loss: 0.690613  [   32/  146]
train() client id: f_00005-4-1 loss: 0.847152  [   64/  146]
train() client id: f_00005-4-2 loss: 0.650633  [   96/  146]
train() client id: f_00005-4-3 loss: 0.576497  [  128/  146]
train() client id: f_00005-5-0 loss: 0.860111  [   32/  146]
train() client id: f_00005-5-1 loss: 0.678140  [   64/  146]
train() client id: f_00005-5-2 loss: 0.712938  [   96/  146]
train() client id: f_00005-5-3 loss: 0.620705  [  128/  146]
train() client id: f_00005-6-0 loss: 0.764062  [   32/  146]
train() client id: f_00005-6-1 loss: 0.643755  [   64/  146]
train() client id: f_00005-6-2 loss: 0.638404  [   96/  146]
train() client id: f_00005-6-3 loss: 0.740125  [  128/  146]
train() client id: f_00005-7-0 loss: 0.512361  [   32/  146]
train() client id: f_00005-7-1 loss: 0.818624  [   64/  146]
train() client id: f_00005-7-2 loss: 0.772297  [   96/  146]
train() client id: f_00005-7-3 loss: 0.659263  [  128/  146]
train() client id: f_00005-8-0 loss: 0.700121  [   32/  146]
train() client id: f_00005-8-1 loss: 0.858047  [   64/  146]
train() client id: f_00005-8-2 loss: 0.515780  [   96/  146]
train() client id: f_00005-8-3 loss: 0.763786  [  128/  146]
train() client id: f_00005-9-0 loss: 0.717413  [   32/  146]
train() client id: f_00005-9-1 loss: 0.770674  [   64/  146]
train() client id: f_00005-9-2 loss: 0.650406  [   96/  146]
train() client id: f_00005-9-3 loss: 0.680927  [  128/  146]
train() client id: f_00005-10-0 loss: 0.824518  [   32/  146]
train() client id: f_00005-10-1 loss: 0.588265  [   64/  146]
train() client id: f_00005-10-2 loss: 0.720989  [   96/  146]
train() client id: f_00005-10-3 loss: 0.647252  [  128/  146]
train() client id: f_00005-11-0 loss: 0.827276  [   32/  146]
train() client id: f_00005-11-1 loss: 0.764432  [   64/  146]
train() client id: f_00005-11-2 loss: 0.619159  [   96/  146]
train() client id: f_00005-11-3 loss: 0.601430  [  128/  146]
train() client id: f_00006-0-0 loss: 0.706771  [   32/   54]
train() client id: f_00006-1-0 loss: 0.665638  [   32/   54]
train() client id: f_00006-2-0 loss: 0.698089  [   32/   54]
train() client id: f_00006-3-0 loss: 0.677288  [   32/   54]
train() client id: f_00006-4-0 loss: 0.671784  [   32/   54]
train() client id: f_00006-5-0 loss: 0.665377  [   32/   54]
train() client id: f_00006-6-0 loss: 0.697025  [   32/   54]
train() client id: f_00006-7-0 loss: 0.661927  [   32/   54]
train() client id: f_00006-8-0 loss: 0.662002  [   32/   54]
train() client id: f_00006-9-0 loss: 0.674577  [   32/   54]
train() client id: f_00006-10-0 loss: 0.666955  [   32/   54]
train() client id: f_00006-11-0 loss: 0.664234  [   32/   54]
train() client id: f_00007-0-0 loss: 0.567685  [   32/  179]
train() client id: f_00007-0-1 loss: 0.589158  [   64/  179]
train() client id: f_00007-0-2 loss: 0.684721  [   96/  179]
train() client id: f_00007-0-3 loss: 0.752320  [  128/  179]
train() client id: f_00007-0-4 loss: 0.590630  [  160/  179]
train() client id: f_00007-1-0 loss: 0.587837  [   32/  179]
train() client id: f_00007-1-1 loss: 0.615987  [   64/  179]
train() client id: f_00007-1-2 loss: 0.612597  [   96/  179]
train() client id: f_00007-1-3 loss: 0.665331  [  128/  179]
train() client id: f_00007-1-4 loss: 0.557990  [  160/  179]
train() client id: f_00007-2-0 loss: 0.590662  [   32/  179]
train() client id: f_00007-2-1 loss: 0.516476  [   64/  179]
train() client id: f_00007-2-2 loss: 0.676902  [   96/  179]
train() client id: f_00007-2-3 loss: 0.640028  [  128/  179]
train() client id: f_00007-2-4 loss: 0.634552  [  160/  179]
train() client id: f_00007-3-0 loss: 0.670335  [   32/  179]
train() client id: f_00007-3-1 loss: 0.687385  [   64/  179]
train() client id: f_00007-3-2 loss: 0.541484  [   96/  179]
train() client id: f_00007-3-3 loss: 0.506050  [  128/  179]
train() client id: f_00007-3-4 loss: 0.543161  [  160/  179]
train() client id: f_00007-4-0 loss: 0.711381  [   32/  179]
train() client id: f_00007-4-1 loss: 0.562971  [   64/  179]
train() client id: f_00007-4-2 loss: 0.644318  [   96/  179]
train() client id: f_00007-4-3 loss: 0.477744  [  128/  179]
train() client id: f_00007-4-4 loss: 0.470144  [  160/  179]
train() client id: f_00007-5-0 loss: 0.524218  [   32/  179]
train() client id: f_00007-5-1 loss: 0.557478  [   64/  179]
train() client id: f_00007-5-2 loss: 0.607715  [   96/  179]
train() client id: f_00007-5-3 loss: 0.726351  [  128/  179]
train() client id: f_00007-5-4 loss: 0.476790  [  160/  179]
train() client id: f_00007-6-0 loss: 0.632269  [   32/  179]
train() client id: f_00007-6-1 loss: 0.542848  [   64/  179]
train() client id: f_00007-6-2 loss: 0.590816  [   96/  179]
train() client id: f_00007-6-3 loss: 0.447599  [  128/  179]
train() client id: f_00007-6-4 loss: 0.568823  [  160/  179]
train() client id: f_00007-7-0 loss: 0.960265  [   32/  179]
train() client id: f_00007-7-1 loss: 0.447756  [   64/  179]
train() client id: f_00007-7-2 loss: 0.523132  [   96/  179]
train() client id: f_00007-7-3 loss: 0.529664  [  128/  179]
train() client id: f_00007-7-4 loss: 0.463976  [  160/  179]
train() client id: f_00007-8-0 loss: 0.525571  [   32/  179]
train() client id: f_00007-8-1 loss: 0.623551  [   64/  179]
train() client id: f_00007-8-2 loss: 0.688066  [   96/  179]
train() client id: f_00007-8-3 loss: 0.533884  [  128/  179]
train() client id: f_00007-8-4 loss: 0.539038  [  160/  179]
train() client id: f_00007-9-0 loss: 0.463370  [   32/  179]
train() client id: f_00007-9-1 loss: 0.520906  [   64/  179]
train() client id: f_00007-9-2 loss: 0.544160  [   96/  179]
train() client id: f_00007-9-3 loss: 0.675029  [  128/  179]
train() client id: f_00007-9-4 loss: 0.516749  [  160/  179]
train() client id: f_00007-10-0 loss: 0.621553  [   32/  179]
train() client id: f_00007-10-1 loss: 0.667779  [   64/  179]
train() client id: f_00007-10-2 loss: 0.437230  [   96/  179]
train() client id: f_00007-10-3 loss: 0.444803  [  128/  179]
train() client id: f_00007-10-4 loss: 0.602465  [  160/  179]
train() client id: f_00007-11-0 loss: 0.610497  [   32/  179]
train() client id: f_00007-11-1 loss: 0.453210  [   64/  179]
train() client id: f_00007-11-2 loss: 0.511812  [   96/  179]
train() client id: f_00007-11-3 loss: 0.691972  [  128/  179]
train() client id: f_00007-11-4 loss: 0.528116  [  160/  179]
train() client id: f_00008-0-0 loss: 0.755246  [   32/  130]
train() client id: f_00008-0-1 loss: 0.640276  [   64/  130]
train() client id: f_00008-0-2 loss: 0.807537  [   96/  130]
train() client id: f_00008-0-3 loss: 0.662633  [  128/  130]
train() client id: f_00008-1-0 loss: 0.746120  [   32/  130]
train() client id: f_00008-1-1 loss: 0.742780  [   64/  130]
train() client id: f_00008-1-2 loss: 0.648909  [   96/  130]
train() client id: f_00008-1-3 loss: 0.739066  [  128/  130]
train() client id: f_00008-2-0 loss: 0.620453  [   32/  130]
train() client id: f_00008-2-1 loss: 0.850920  [   64/  130]
train() client id: f_00008-2-2 loss: 0.621329  [   96/  130]
train() client id: f_00008-2-3 loss: 0.777022  [  128/  130]
train() client id: f_00008-3-0 loss: 0.697237  [   32/  130]
train() client id: f_00008-3-1 loss: 0.831519  [   64/  130]
train() client id: f_00008-3-2 loss: 0.675901  [   96/  130]
train() client id: f_00008-3-3 loss: 0.686434  [  128/  130]
train() client id: f_00008-4-0 loss: 0.676307  [   32/  130]
train() client id: f_00008-4-1 loss: 0.591967  [   64/  130]
train() client id: f_00008-4-2 loss: 0.774829  [   96/  130]
train() client id: f_00008-4-3 loss: 0.786982  [  128/  130]
train() client id: f_00008-5-0 loss: 0.622348  [   32/  130]
train() client id: f_00008-5-1 loss: 0.692374  [   64/  130]
train() client id: f_00008-5-2 loss: 0.749333  [   96/  130]
train() client id: f_00008-5-3 loss: 0.823062  [  128/  130]
train() client id: f_00008-6-0 loss: 0.685209  [   32/  130]
train() client id: f_00008-6-1 loss: 0.766900  [   64/  130]
train() client id: f_00008-6-2 loss: 0.717274  [   96/  130]
train() client id: f_00008-6-3 loss: 0.682982  [  128/  130]
train() client id: f_00008-7-0 loss: 0.818347  [   32/  130]
train() client id: f_00008-7-1 loss: 0.650840  [   64/  130]
train() client id: f_00008-7-2 loss: 0.720086  [   96/  130]
train() client id: f_00008-7-3 loss: 0.648698  [  128/  130]
train() client id: f_00008-8-0 loss: 0.785830  [   32/  130]
train() client id: f_00008-8-1 loss: 0.630000  [   64/  130]
train() client id: f_00008-8-2 loss: 0.700384  [   96/  130]
train() client id: f_00008-8-3 loss: 0.734278  [  128/  130]
train() client id: f_00008-9-0 loss: 0.754292  [   32/  130]
train() client id: f_00008-9-1 loss: 0.695224  [   64/  130]
train() client id: f_00008-9-2 loss: 0.751987  [   96/  130]
train() client id: f_00008-9-3 loss: 0.647396  [  128/  130]
train() client id: f_00008-10-0 loss: 0.880000  [   32/  130]
train() client id: f_00008-10-1 loss: 0.619494  [   64/  130]
train() client id: f_00008-10-2 loss: 0.712360  [   96/  130]
train() client id: f_00008-10-3 loss: 0.673279  [  128/  130]
train() client id: f_00008-11-0 loss: 0.771114  [   32/  130]
train() client id: f_00008-11-1 loss: 0.789311  [   64/  130]
train() client id: f_00008-11-2 loss: 0.670737  [   96/  130]
train() client id: f_00008-11-3 loss: 0.656730  [  128/  130]
train() client id: f_00009-0-0 loss: 1.192647  [   32/  118]
train() client id: f_00009-0-1 loss: 1.259409  [   64/  118]
train() client id: f_00009-0-2 loss: 1.219397  [   96/  118]
train() client id: f_00009-1-0 loss: 1.196165  [   32/  118]
train() client id: f_00009-1-1 loss: 1.079350  [   64/  118]
train() client id: f_00009-1-2 loss: 1.172238  [   96/  118]
train() client id: f_00009-2-0 loss: 1.101665  [   32/  118]
train() client id: f_00009-2-1 loss: 1.135131  [   64/  118]
train() client id: f_00009-2-2 loss: 1.157146  [   96/  118]
train() client id: f_00009-3-0 loss: 1.052938  [   32/  118]
train() client id: f_00009-3-1 loss: 1.080384  [   64/  118]
train() client id: f_00009-3-2 loss: 1.029834  [   96/  118]
train() client id: f_00009-4-0 loss: 1.139966  [   32/  118]
train() client id: f_00009-4-1 loss: 1.023786  [   64/  118]
train() client id: f_00009-4-2 loss: 1.042896  [   96/  118]
train() client id: f_00009-5-0 loss: 1.071125  [   32/  118]
train() client id: f_00009-5-1 loss: 1.031675  [   64/  118]
train() client id: f_00009-5-2 loss: 1.031840  [   96/  118]
train() client id: f_00009-6-0 loss: 0.905492  [   32/  118]
train() client id: f_00009-6-1 loss: 1.015508  [   64/  118]
train() client id: f_00009-6-2 loss: 1.170507  [   96/  118]
train() client id: f_00009-7-0 loss: 1.100209  [   32/  118]
train() client id: f_00009-7-1 loss: 0.974851  [   64/  118]
train() client id: f_00009-7-2 loss: 0.977326  [   96/  118]
train() client id: f_00009-8-0 loss: 0.899671  [   32/  118]
train() client id: f_00009-8-1 loss: 1.077419  [   64/  118]
train() client id: f_00009-8-2 loss: 0.980807  [   96/  118]
train() client id: f_00009-9-0 loss: 0.945161  [   32/  118]
train() client id: f_00009-9-1 loss: 1.025050  [   64/  118]
train() client id: f_00009-9-2 loss: 0.977214  [   96/  118]
train() client id: f_00009-10-0 loss: 1.036466  [   32/  118]
train() client id: f_00009-10-1 loss: 0.964213  [   64/  118]
train() client id: f_00009-10-2 loss: 0.938032  [   96/  118]
train() client id: f_00009-11-0 loss: 0.957103  [   32/  118]
train() client id: f_00009-11-1 loss: 1.008539  [   64/  118]
train() client id: f_00009-11-2 loss: 1.065399  [   96/  118]
At round 12 accuracy: 0.6312997347480106
At round 12 training accuracy: 0.5727699530516432
At round 12 training loss: 0.870510815715939
update_location
xs = -3.905658 4.200318 80.009024 18.811294 0.979296 3.956410 -42.443192 -21.324852 64.663977 -7.060879 
ys = 72.587959 55.555839 1.320614 -42.455176 34.350187 17.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 123.629552 114.473114 128.074931 110.255643 105.739748 101.651351 108.666071 102.251775 120.374831 100.328799 
dists_bs = 199.402858 215.359378 308.535462 291.289497 225.280009 238.186456 221.634705 232.279800 286.746124 239.674867 
uav_gains = -102.303383 -101.467697 -102.687136 -101.060085 -100.605992 -100.177849 -100.902402 -100.241793 -102.013616 -100.035657 
bs_gains = -103.959538 -104.895645 -109.267602 -108.568154 -105.443294 -106.120737 -105.244917 -105.815380 -108.376991 -106.196489 
Round 13
-------------------------------
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.27548856 19.34614452  9.13389385  3.26646163 22.31552022 10.75645448
  4.0605729  13.0924169   9.62823145  8.73319217]
obj_prev = 109.60837667978073
eta_min = 9.621283683779976e-11	eta_max = 0.9207981058157783
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 25.487961404153893	eta = 0.9090909090909091
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 44.16384428532155	eta = 0.5246570894979226
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 35.24177796016038	eta = 0.6574831164866354
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 33.64173104871985	eta = 0.6887539160877387
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 33.56259776728117	eta = 0.6903778475206297
af = 23.170874003776266	bf = 1.8064747675059596	zeta = 33.56239040785321	eta = 0.6903821129008305
eta = 0.6903821129008305
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [0.0305736  0.06430162 0.03008831 0.01043385 0.07425017 0.03542655
 0.01310297 0.04343393 0.03154419 0.02863242]
ene_total = [2.87721447 5.53384282 2.84956312 1.30490774 6.3139176  3.36162512
 1.50604088 3.81675253 3.15491369 2.84361242]
ti_comp = [0.31327504 0.30392366 0.31202998 0.31702896 0.30162658 0.29859963
 0.31747675 0.31928923 0.2867473  0.29824756]
ti_coms = [0.06967721 0.0790286  0.07092227 0.06592329 0.08132567 0.08435263
 0.0654755  0.06366303 0.09620495 0.08470469]
t_total = [29.34994545 29.34994545 29.34994545 29.34994545 29.34994545 29.34994545
 29.34994545 29.34994545 29.34994545 29.34994545]
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [1.81998670e-05 1.79894040e-04 1.74856014e-05 7.06343005e-07
 2.81211599e-04 3.11665129e-05 1.39497231e-06 5.02341124e-05
 2.38583025e-05 1.64930279e-05]
ene_total = [0.53540984 0.61947247 0.54489739 0.50529856 0.64484267 0.64887784
 0.50191943 0.49177151 0.73915545 0.65045154]
optimize_network iter = 0 obj = 5.882096701797337
eta = 0.6903821129008305
freqs = [4.87967396e+07 1.05785811e+08 4.82138059e+07 1.64556699e+07
 1.23082943e+08 5.93211620e+07 2.06361124e+07 6.80165848e+07
 5.50034670e+07 4.80010972e+07]
eta_min = 0.6903821129008529	eta_max = 0.6903821129008285
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 0.04372443773870311	eta = 0.909090909090909
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 19.912896282792712	eta = 0.0019961681258650003
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 2.05768598057864	eta = 0.019317568000433452
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 2.002030029818999	eta = 0.019854591720065342
af = 0.03974948885336646	bf = 1.8064747675059596	zeta = 2.0020149221690455	eta = 0.019854741547231138
eta = 0.019854741547231138
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [1.92521000e-04 1.90294690e-03 1.84965388e-04 7.47180525e-06
 2.97469966e-03 3.29684180e-04 1.47562323e-05 5.31384189e-04
 2.52376804e-04 1.74465793e-04]
ene_total = [0.17335085 0.23740048 0.17618224 0.15978245 0.26890909 0.21220106
 0.15887471 0.16699434 0.23902414 0.20929556]
ti_comp = [0.31327504 0.30392366 0.31202998 0.31702896 0.30162658 0.29859963
 0.31747675 0.31928923 0.2867473  0.29824756]
ti_coms = [0.06967721 0.0790286  0.07092227 0.06592329 0.08132567 0.08435263
 0.0654755  0.06366303 0.09620495 0.08470469]
t_total = [29.34994545 29.34994545 29.34994545 29.34994545 29.34994545 29.34994545
 29.34994545 29.34994545 29.34994545 29.34994545]
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [1.81998670e-05 1.79894040e-04 1.74856014e-05 7.06343005e-07
 2.81211599e-04 3.11665129e-05 1.39497231e-06 5.02341124e-05
 2.38583025e-05 1.64930279e-05]
ene_total = [0.53540984 0.61947247 0.54489739 0.50529856 0.64484267 0.64887784
 0.50191943 0.49177151 0.73915545 0.65045154]
optimize_network iter = 1 obj = 5.882096701797758
eta = 0.6903821129008529
freqs = [4.87967396e+07 1.05785811e+08 4.82138059e+07 1.64556699e+07
 1.23082943e+08 5.93211620e+07 2.06361124e+07 6.80165848e+07
 5.50034670e+07 4.80010972e+07]
Done!
ene_coms = [0.00696772 0.00790286 0.00709223 0.00659233 0.00813257 0.00843526
 0.00654755 0.0063663  0.00962049 0.00847047]
ene_comp = [1.80012808e-05 1.77931142e-04 1.72948088e-05 6.98635807e-07
 2.78143184e-04 3.08264423e-05 1.37975120e-06 4.96859874e-05
 2.35979748e-05 1.63130657e-05]
ene_total = [0.00698572 0.00808079 0.00710952 0.00659303 0.00841071 0.00846609
 0.00654893 0.00641599 0.00964409 0.00848678]
At round 13 energy consumption: 0.07674165610159829
At round 13 eta: 0.6903821129008529
At round 13 a_n: 23.72950684565325
At round 13 local rounds: 12.132381383768099
At round 13 global rounds: 76.641266006878
gradient difference: 0.38414281606674194
train() client id: f_00000-0-0 loss: 1.289819  [   32/  126]
train() client id: f_00000-0-1 loss: 1.380058  [   64/  126]
train() client id: f_00000-0-2 loss: 1.323670  [   96/  126]
train() client id: f_00000-1-0 loss: 1.315533  [   32/  126]
train() client id: f_00000-1-1 loss: 1.258082  [   64/  126]
train() client id: f_00000-1-2 loss: 1.070924  [   96/  126]
train() client id: f_00000-2-0 loss: 1.180353  [   32/  126]
train() client id: f_00000-2-1 loss: 1.015965  [   64/  126]
train() client id: f_00000-2-2 loss: 1.077146  [   96/  126]
train() client id: f_00000-3-0 loss: 1.083717  [   32/  126]
train() client id: f_00000-3-1 loss: 1.115363  [   64/  126]
train() client id: f_00000-3-2 loss: 0.973680  [   96/  126]
train() client id: f_00000-4-0 loss: 0.888430  [   32/  126]
train() client id: f_00000-4-1 loss: 0.979790  [   64/  126]
train() client id: f_00000-4-2 loss: 1.015636  [   96/  126]
train() client id: f_00000-5-0 loss: 0.940252  [   32/  126]
train() client id: f_00000-5-1 loss: 0.994471  [   64/  126]
train() client id: f_00000-5-2 loss: 0.891517  [   96/  126]
train() client id: f_00000-6-0 loss: 0.840610  [   32/  126]
train() client id: f_00000-6-1 loss: 0.875330  [   64/  126]
train() client id: f_00000-6-2 loss: 0.931189  [   96/  126]
train() client id: f_00000-7-0 loss: 0.945566  [   32/  126]
train() client id: f_00000-7-1 loss: 0.803347  [   64/  126]
train() client id: f_00000-7-2 loss: 0.887208  [   96/  126]
train() client id: f_00000-8-0 loss: 0.875078  [   32/  126]
train() client id: f_00000-8-1 loss: 0.869837  [   64/  126]
train() client id: f_00000-8-2 loss: 0.909215  [   96/  126]
train() client id: f_00000-9-0 loss: 0.837376  [   32/  126]
train() client id: f_00000-9-1 loss: 0.882884  [   64/  126]
train() client id: f_00000-9-2 loss: 0.800870  [   96/  126]
train() client id: f_00000-10-0 loss: 0.903699  [   32/  126]
train() client id: f_00000-10-1 loss: 0.820324  [   64/  126]
train() client id: f_00000-10-2 loss: 0.759314  [   96/  126]
train() client id: f_00000-11-0 loss: 0.881666  [   32/  126]
train() client id: f_00000-11-1 loss: 0.791200  [   64/  126]
train() client id: f_00000-11-2 loss: 0.829349  [   96/  126]
train() client id: f_00001-0-0 loss: 0.498072  [   32/  265]
train() client id: f_00001-0-1 loss: 0.480074  [   64/  265]
train() client id: f_00001-0-2 loss: 0.559209  [   96/  265]
train() client id: f_00001-0-3 loss: 0.576576  [  128/  265]
train() client id: f_00001-0-4 loss: 0.709085  [  160/  265]
train() client id: f_00001-0-5 loss: 0.529938  [  192/  265]
train() client id: f_00001-0-6 loss: 0.580721  [  224/  265]
train() client id: f_00001-0-7 loss: 0.571088  [  256/  265]
train() client id: f_00001-1-0 loss: 0.501072  [   32/  265]
train() client id: f_00001-1-1 loss: 0.593361  [   64/  265]
train() client id: f_00001-1-2 loss: 0.569724  [   96/  265]
train() client id: f_00001-1-3 loss: 0.593758  [  128/  265]
train() client id: f_00001-1-4 loss: 0.578219  [  160/  265]
train() client id: f_00001-1-5 loss: 0.513568  [  192/  265]
train() client id: f_00001-1-6 loss: 0.460379  [  224/  265]
train() client id: f_00001-1-7 loss: 0.589484  [  256/  265]
train() client id: f_00001-2-0 loss: 0.604352  [   32/  265]
train() client id: f_00001-2-1 loss: 0.785839  [   64/  265]
train() client id: f_00001-2-2 loss: 0.517879  [   96/  265]
train() client id: f_00001-2-3 loss: 0.470575  [  128/  265]
train() client id: f_00001-2-4 loss: 0.513569  [  160/  265]
train() client id: f_00001-2-5 loss: 0.462496  [  192/  265]
train() client id: f_00001-2-6 loss: 0.524554  [  224/  265]
train() client id: f_00001-2-7 loss: 0.495271  [  256/  265]
train() client id: f_00001-3-0 loss: 0.518583  [   32/  265]
train() client id: f_00001-3-1 loss: 0.746647  [   64/  265]
train() client id: f_00001-3-2 loss: 0.547618  [   96/  265]
train() client id: f_00001-3-3 loss: 0.450293  [  128/  265]
train() client id: f_00001-3-4 loss: 0.440487  [  160/  265]
train() client id: f_00001-3-5 loss: 0.487570  [  192/  265]
train() client id: f_00001-3-6 loss: 0.569261  [  224/  265]
train() client id: f_00001-3-7 loss: 0.553024  [  256/  265]
train() client id: f_00001-4-0 loss: 0.574962  [   32/  265]
train() client id: f_00001-4-1 loss: 0.503166  [   64/  265]
train() client id: f_00001-4-2 loss: 0.460045  [   96/  265]
train() client id: f_00001-4-3 loss: 0.501463  [  128/  265]
train() client id: f_00001-4-4 loss: 0.436512  [  160/  265]
train() client id: f_00001-4-5 loss: 0.626347  [  192/  265]
train() client id: f_00001-4-6 loss: 0.447587  [  224/  265]
train() client id: f_00001-4-7 loss: 0.694919  [  256/  265]
train() client id: f_00001-5-0 loss: 0.560042  [   32/  265]
train() client id: f_00001-5-1 loss: 0.581789  [   64/  265]
train() client id: f_00001-5-2 loss: 0.588617  [   96/  265]
train() client id: f_00001-5-3 loss: 0.542059  [  128/  265]
train() client id: f_00001-5-4 loss: 0.512756  [  160/  265]
train() client id: f_00001-5-5 loss: 0.500021  [  192/  265]
train() client id: f_00001-5-6 loss: 0.499792  [  224/  265]
train() client id: f_00001-5-7 loss: 0.492431  [  256/  265]
train() client id: f_00001-6-0 loss: 0.514795  [   32/  265]
train() client id: f_00001-6-1 loss: 0.638338  [   64/  265]
train() client id: f_00001-6-2 loss: 0.494945  [   96/  265]
train() client id: f_00001-6-3 loss: 0.505134  [  128/  265]
train() client id: f_00001-6-4 loss: 0.436614  [  160/  265]
train() client id: f_00001-6-5 loss: 0.653523  [  192/  265]
train() client id: f_00001-6-6 loss: 0.428492  [  224/  265]
train() client id: f_00001-6-7 loss: 0.592584  [  256/  265]
train() client id: f_00001-7-0 loss: 0.507523  [   32/  265]
train() client id: f_00001-7-1 loss: 0.655503  [   64/  265]
train() client id: f_00001-7-2 loss: 0.499357  [   96/  265]
train() client id: f_00001-7-3 loss: 0.439621  [  128/  265]
train() client id: f_00001-7-4 loss: 0.575696  [  160/  265]
train() client id: f_00001-7-5 loss: 0.640415  [  192/  265]
train() client id: f_00001-7-6 loss: 0.444929  [  224/  265]
train() client id: f_00001-7-7 loss: 0.495860  [  256/  265]
train() client id: f_00001-8-0 loss: 0.492216  [   32/  265]
train() client id: f_00001-8-1 loss: 0.562477  [   64/  265]
train() client id: f_00001-8-2 loss: 0.603377  [   96/  265]
train() client id: f_00001-8-3 loss: 0.619336  [  128/  265]
train() client id: f_00001-8-4 loss: 0.524240  [  160/  265]
train() client id: f_00001-8-5 loss: 0.504553  [  192/  265]
train() client id: f_00001-8-6 loss: 0.429998  [  224/  265]
train() client id: f_00001-8-7 loss: 0.509980  [  256/  265]
train() client id: f_00001-9-0 loss: 0.564115  [   32/  265]
train() client id: f_00001-9-1 loss: 0.565591  [   64/  265]
train() client id: f_00001-9-2 loss: 0.580824  [   96/  265]
train() client id: f_00001-9-3 loss: 0.476048  [  128/  265]
train() client id: f_00001-9-4 loss: 0.487984  [  160/  265]
train() client id: f_00001-9-5 loss: 0.566220  [  192/  265]
train() client id: f_00001-9-6 loss: 0.516488  [  224/  265]
train() client id: f_00001-9-7 loss: 0.497438  [  256/  265]
train() client id: f_00001-10-0 loss: 0.499018  [   32/  265]
train() client id: f_00001-10-1 loss: 0.434971  [   64/  265]
train() client id: f_00001-10-2 loss: 0.544475  [   96/  265]
train() client id: f_00001-10-3 loss: 0.623057  [  128/  265]
train() client id: f_00001-10-4 loss: 0.556822  [  160/  265]
train() client id: f_00001-10-5 loss: 0.565551  [  192/  265]
train() client id: f_00001-10-6 loss: 0.614063  [  224/  265]
train() client id: f_00001-10-7 loss: 0.424719  [  256/  265]
train() client id: f_00001-11-0 loss: 0.612349  [   32/  265]
train() client id: f_00001-11-1 loss: 0.507925  [   64/  265]
train() client id: f_00001-11-2 loss: 0.584236  [   96/  265]
train() client id: f_00001-11-3 loss: 0.577058  [  128/  265]
train() client id: f_00001-11-4 loss: 0.501269  [  160/  265]
train() client id: f_00001-11-5 loss: 0.484720  [  192/  265]
train() client id: f_00001-11-6 loss: 0.429472  [  224/  265]
train() client id: f_00001-11-7 loss: 0.453831  [  256/  265]
train() client id: f_00002-0-0 loss: 1.317308  [   32/  124]
train() client id: f_00002-0-1 loss: 1.301491  [   64/  124]
train() client id: f_00002-0-2 loss: 1.342457  [   96/  124]
train() client id: f_00002-1-0 loss: 1.226207  [   32/  124]
train() client id: f_00002-1-1 loss: 1.250849  [   64/  124]
train() client id: f_00002-1-2 loss: 1.223161  [   96/  124]
train() client id: f_00002-2-0 loss: 1.228122  [   32/  124]
train() client id: f_00002-2-1 loss: 1.192528  [   64/  124]
train() client id: f_00002-2-2 loss: 1.152634  [   96/  124]
train() client id: f_00002-3-0 loss: 1.136999  [   32/  124]
train() client id: f_00002-3-1 loss: 1.195815  [   64/  124]
train() client id: f_00002-3-2 loss: 1.116569  [   96/  124]
train() client id: f_00002-4-0 loss: 1.195497  [   32/  124]
train() client id: f_00002-4-1 loss: 1.195557  [   64/  124]
train() client id: f_00002-4-2 loss: 1.016604  [   96/  124]
train() client id: f_00002-5-0 loss: 1.111883  [   32/  124]
train() client id: f_00002-5-1 loss: 1.056728  [   64/  124]
train() client id: f_00002-5-2 loss: 1.117143  [   96/  124]
train() client id: f_00002-6-0 loss: 1.233562  [   32/  124]
train() client id: f_00002-6-1 loss: 1.014528  [   64/  124]
train() client id: f_00002-6-2 loss: 1.065770  [   96/  124]
train() client id: f_00002-7-0 loss: 0.979039  [   32/  124]
train() client id: f_00002-7-1 loss: 1.087747  [   64/  124]
train() client id: f_00002-7-2 loss: 1.162575  [   96/  124]
train() client id: f_00002-8-0 loss: 1.068373  [   32/  124]
train() client id: f_00002-8-1 loss: 1.081542  [   64/  124]
train() client id: f_00002-8-2 loss: 0.968107  [   96/  124]
train() client id: f_00002-9-0 loss: 1.198934  [   32/  124]
train() client id: f_00002-9-1 loss: 0.933630  [   64/  124]
train() client id: f_00002-9-2 loss: 1.015620  [   96/  124]
train() client id: f_00002-10-0 loss: 1.066533  [   32/  124]
train() client id: f_00002-10-1 loss: 1.080882  [   64/  124]
train() client id: f_00002-10-2 loss: 1.045050  [   96/  124]
train() client id: f_00002-11-0 loss: 1.005337  [   32/  124]
train() client id: f_00002-11-1 loss: 1.160864  [   64/  124]
train() client id: f_00002-11-2 loss: 0.935902  [   96/  124]
train() client id: f_00003-0-0 loss: 0.913091  [   32/   43]
train() client id: f_00003-1-0 loss: 0.989422  [   32/   43]
train() client id: f_00003-2-0 loss: 0.851934  [   32/   43]
train() client id: f_00003-3-0 loss: 0.968666  [   32/   43]
train() client id: f_00003-4-0 loss: 0.889133  [   32/   43]
train() client id: f_00003-5-0 loss: 0.983245  [   32/   43]
train() client id: f_00003-6-0 loss: 0.815223  [   32/   43]
train() client id: f_00003-7-0 loss: 0.956317  [   32/   43]
train() client id: f_00003-8-0 loss: 0.987005  [   32/   43]
train() client id: f_00003-9-0 loss: 0.975633  [   32/   43]
train() client id: f_00003-10-0 loss: 0.875220  [   32/   43]
train() client id: f_00003-11-0 loss: 0.858933  [   32/   43]
train() client id: f_00004-0-0 loss: 0.789077  [   32/  306]
train() client id: f_00004-0-1 loss: 0.982817  [   64/  306]
train() client id: f_00004-0-2 loss: 0.901254  [   96/  306]
train() client id: f_00004-0-3 loss: 0.849509  [  128/  306]
train() client id: f_00004-0-4 loss: 0.906290  [  160/  306]
train() client id: f_00004-0-5 loss: 0.869403  [  192/  306]
train() client id: f_00004-0-6 loss: 0.839931  [  224/  306]
train() client id: f_00004-0-7 loss: 0.953816  [  256/  306]
train() client id: f_00004-0-8 loss: 0.925998  [  288/  306]
train() client id: f_00004-1-0 loss: 0.794435  [   32/  306]
train() client id: f_00004-1-1 loss: 0.877824  [   64/  306]
train() client id: f_00004-1-2 loss: 0.950462  [   96/  306]
train() client id: f_00004-1-3 loss: 0.833296  [  128/  306]
train() client id: f_00004-1-4 loss: 0.838489  [  160/  306]
train() client id: f_00004-1-5 loss: 0.901520  [  192/  306]
train() client id: f_00004-1-6 loss: 1.059249  [  224/  306]
train() client id: f_00004-1-7 loss: 0.864021  [  256/  306]
train() client id: f_00004-1-8 loss: 0.930627  [  288/  306]
train() client id: f_00004-2-0 loss: 0.884431  [   32/  306]
train() client id: f_00004-2-1 loss: 0.807721  [   64/  306]
train() client id: f_00004-2-2 loss: 0.968042  [   96/  306]
train() client id: f_00004-2-3 loss: 0.694131  [  128/  306]
train() client id: f_00004-2-4 loss: 0.918700  [  160/  306]
train() client id: f_00004-2-5 loss: 0.906889  [  192/  306]
train() client id: f_00004-2-6 loss: 0.986661  [  224/  306]
train() client id: f_00004-2-7 loss: 0.948619  [  256/  306]
train() client id: f_00004-2-8 loss: 0.834683  [  288/  306]
train() client id: f_00004-3-0 loss: 0.877323  [   32/  306]
train() client id: f_00004-3-1 loss: 0.963455  [   64/  306]
train() client id: f_00004-3-2 loss: 0.874378  [   96/  306]
train() client id: f_00004-3-3 loss: 0.846418  [  128/  306]
train() client id: f_00004-3-4 loss: 0.853113  [  160/  306]
train() client id: f_00004-3-5 loss: 0.956999  [  192/  306]
train() client id: f_00004-3-6 loss: 0.941086  [  224/  306]
train() client id: f_00004-3-7 loss: 0.862602  [  256/  306]
train() client id: f_00004-3-8 loss: 0.915016  [  288/  306]
train() client id: f_00004-4-0 loss: 0.850047  [   32/  306]
train() client id: f_00004-4-1 loss: 0.969682  [   64/  306]
train() client id: f_00004-4-2 loss: 0.821742  [   96/  306]
train() client id: f_00004-4-3 loss: 0.865708  [  128/  306]
train() client id: f_00004-4-4 loss: 0.872222  [  160/  306]
train() client id: f_00004-4-5 loss: 0.908335  [  192/  306]
train() client id: f_00004-4-6 loss: 0.859444  [  224/  306]
train() client id: f_00004-4-7 loss: 0.909916  [  256/  306]
train() client id: f_00004-4-8 loss: 0.874165  [  288/  306]
train() client id: f_00004-5-0 loss: 0.872696  [   32/  306]
train() client id: f_00004-5-1 loss: 0.857367  [   64/  306]
train() client id: f_00004-5-2 loss: 0.863793  [   96/  306]
train() client id: f_00004-5-3 loss: 0.893882  [  128/  306]
train() client id: f_00004-5-4 loss: 0.887284  [  160/  306]
train() client id: f_00004-5-5 loss: 0.965235  [  192/  306]
train() client id: f_00004-5-6 loss: 0.928237  [  224/  306]
train() client id: f_00004-5-7 loss: 0.838249  [  256/  306]
train() client id: f_00004-5-8 loss: 0.880723  [  288/  306]
train() client id: f_00004-6-0 loss: 0.959761  [   32/  306]
train() client id: f_00004-6-1 loss: 0.760824  [   64/  306]
train() client id: f_00004-6-2 loss: 0.902762  [   96/  306]
train() client id: f_00004-6-3 loss: 1.030656  [  128/  306]
train() client id: f_00004-6-4 loss: 0.873394  [  160/  306]
train() client id: f_00004-6-5 loss: 0.862317  [  192/  306]
train() client id: f_00004-6-6 loss: 0.808656  [  224/  306]
train() client id: f_00004-6-7 loss: 0.847149  [  256/  306]
train() client id: f_00004-6-8 loss: 0.998509  [  288/  306]
train() client id: f_00004-7-0 loss: 0.938561  [   32/  306]
train() client id: f_00004-7-1 loss: 0.769731  [   64/  306]
train() client id: f_00004-7-2 loss: 0.891836  [   96/  306]
train() client id: f_00004-7-3 loss: 0.848802  [  128/  306]
train() client id: f_00004-7-4 loss: 0.978413  [  160/  306]
train() client id: f_00004-7-5 loss: 0.880931  [  192/  306]
train() client id: f_00004-7-6 loss: 0.880527  [  224/  306]
train() client id: f_00004-7-7 loss: 0.908074  [  256/  306]
train() client id: f_00004-7-8 loss: 0.894884  [  288/  306]
train() client id: f_00004-8-0 loss: 0.836346  [   32/  306]
train() client id: f_00004-8-1 loss: 0.907565  [   64/  306]
train() client id: f_00004-8-2 loss: 0.931650  [   96/  306]
train() client id: f_00004-8-3 loss: 0.902196  [  128/  306]
train() client id: f_00004-8-4 loss: 0.844273  [  160/  306]
train() client id: f_00004-8-5 loss: 0.841220  [  192/  306]
train() client id: f_00004-8-6 loss: 0.905366  [  224/  306]
train() client id: f_00004-8-7 loss: 0.835317  [  256/  306]
train() client id: f_00004-8-8 loss: 0.910162  [  288/  306]
train() client id: f_00004-9-0 loss: 0.926720  [   32/  306]
train() client id: f_00004-9-1 loss: 0.854952  [   64/  306]
train() client id: f_00004-9-2 loss: 0.856183  [   96/  306]
train() client id: f_00004-9-3 loss: 0.844104  [  128/  306]
train() client id: f_00004-9-4 loss: 0.882124  [  160/  306]
train() client id: f_00004-9-5 loss: 0.934765  [  192/  306]
train() client id: f_00004-9-6 loss: 0.938013  [  224/  306]
train() client id: f_00004-9-7 loss: 0.938741  [  256/  306]
train() client id: f_00004-9-8 loss: 0.917398  [  288/  306]
train() client id: f_00004-10-0 loss: 0.882891  [   32/  306]
train() client id: f_00004-10-1 loss: 0.870753  [   64/  306]
train() client id: f_00004-10-2 loss: 0.964309  [   96/  306]
train() client id: f_00004-10-3 loss: 0.955166  [  128/  306]
train() client id: f_00004-10-4 loss: 0.932345  [  160/  306]
train() client id: f_00004-10-5 loss: 0.793515  [  192/  306]
train() client id: f_00004-10-6 loss: 0.944882  [  224/  306]
train() client id: f_00004-10-7 loss: 0.877329  [  256/  306]
train() client id: f_00004-10-8 loss: 0.845492  [  288/  306]
train() client id: f_00004-11-0 loss: 0.913444  [   32/  306]
train() client id: f_00004-11-1 loss: 0.934027  [   64/  306]
train() client id: f_00004-11-2 loss: 0.867123  [   96/  306]
train() client id: f_00004-11-3 loss: 0.874355  [  128/  306]
train() client id: f_00004-11-4 loss: 0.893826  [  160/  306]
train() client id: f_00004-11-5 loss: 0.873147  [  192/  306]
train() client id: f_00004-11-6 loss: 0.925436  [  224/  306]
train() client id: f_00004-11-7 loss: 0.924117  [  256/  306]
train() client id: f_00004-11-8 loss: 0.806825  [  288/  306]
train() client id: f_00005-0-0 loss: 0.801143  [   32/  146]
train() client id: f_00005-0-1 loss: 0.767334  [   64/  146]
train() client id: f_00005-0-2 loss: 0.731700  [   96/  146]
train() client id: f_00005-0-3 loss: 0.793164  [  128/  146]
train() client id: f_00005-1-0 loss: 0.712734  [   32/  146]
train() client id: f_00005-1-1 loss: 0.739481  [   64/  146]
train() client id: f_00005-1-2 loss: 0.815021  [   96/  146]
train() client id: f_00005-1-3 loss: 0.847493  [  128/  146]
train() client id: f_00005-2-0 loss: 0.949003  [   32/  146]
train() client id: f_00005-2-1 loss: 0.751896  [   64/  146]
train() client id: f_00005-2-2 loss: 0.823858  [   96/  146]
train() client id: f_00005-2-3 loss: 0.677424  [  128/  146]
train() client id: f_00005-3-0 loss: 0.814516  [   32/  146]
train() client id: f_00005-3-1 loss: 0.768134  [   64/  146]
train() client id: f_00005-3-2 loss: 0.798281  [   96/  146]
train() client id: f_00005-3-3 loss: 0.799400  [  128/  146]
train() client id: f_00005-4-0 loss: 0.968051  [   32/  146]
train() client id: f_00005-4-1 loss: 0.855974  [   64/  146]
train() client id: f_00005-4-2 loss: 0.535712  [   96/  146]
train() client id: f_00005-4-3 loss: 0.822933  [  128/  146]
train() client id: f_00005-5-0 loss: 0.645204  [   32/  146]
train() client id: f_00005-5-1 loss: 0.756319  [   64/  146]
train() client id: f_00005-5-2 loss: 1.144106  [   96/  146]
train() client id: f_00005-5-3 loss: 0.680151  [  128/  146]
train() client id: f_00005-6-0 loss: 0.846184  [   32/  146]
train() client id: f_00005-6-1 loss: 0.749437  [   64/  146]
train() client id: f_00005-6-2 loss: 0.682169  [   96/  146]
train() client id: f_00005-6-3 loss: 0.941209  [  128/  146]
train() client id: f_00005-7-0 loss: 0.924273  [   32/  146]
train() client id: f_00005-7-1 loss: 0.724449  [   64/  146]
train() client id: f_00005-7-2 loss: 0.733658  [   96/  146]
train() client id: f_00005-7-3 loss: 0.809045  [  128/  146]
train() client id: f_00005-8-0 loss: 0.818239  [   32/  146]
train() client id: f_00005-8-1 loss: 0.635881  [   64/  146]
train() client id: f_00005-8-2 loss: 0.936733  [   96/  146]
train() client id: f_00005-8-3 loss: 0.737606  [  128/  146]
train() client id: f_00005-9-0 loss: 0.683086  [   32/  146]
train() client id: f_00005-9-1 loss: 1.046225  [   64/  146]
train() client id: f_00005-9-2 loss: 0.630544  [   96/  146]
train() client id: f_00005-9-3 loss: 0.712033  [  128/  146]
train() client id: f_00005-10-0 loss: 0.827075  [   32/  146]
train() client id: f_00005-10-1 loss: 0.730675  [   64/  146]
train() client id: f_00005-10-2 loss: 0.819943  [   96/  146]
train() client id: f_00005-10-3 loss: 0.689792  [  128/  146]
train() client id: f_00005-11-0 loss: 0.739802  [   32/  146]
train() client id: f_00005-11-1 loss: 0.933332  [   64/  146]
train() client id: f_00005-11-2 loss: 0.564650  [   96/  146]
train() client id: f_00005-11-3 loss: 0.940508  [  128/  146]
train() client id: f_00006-0-0 loss: 0.680755  [   32/   54]
train() client id: f_00006-1-0 loss: 0.649096  [   32/   54]
train() client id: f_00006-2-0 loss: 0.680290  [   32/   54]
train() client id: f_00006-3-0 loss: 0.680362  [   32/   54]
train() client id: f_00006-4-0 loss: 0.716454  [   32/   54]
train() client id: f_00006-5-0 loss: 0.642978  [   32/   54]
train() client id: f_00006-6-0 loss: 0.704243  [   32/   54]
train() client id: f_00006-7-0 loss: 0.702465  [   32/   54]
train() client id: f_00006-8-0 loss: 0.714285  [   32/   54]
train() client id: f_00006-9-0 loss: 0.683616  [   32/   54]
train() client id: f_00006-10-0 loss: 0.676574  [   32/   54]
train() client id: f_00006-11-0 loss: 0.720210  [   32/   54]
train() client id: f_00007-0-0 loss: 0.682572  [   32/  179]
train() client id: f_00007-0-1 loss: 0.708947  [   64/  179]
train() client id: f_00007-0-2 loss: 0.662920  [   96/  179]
train() client id: f_00007-0-3 loss: 0.740273  [  128/  179]
train() client id: f_00007-0-4 loss: 0.716514  [  160/  179]
train() client id: f_00007-1-0 loss: 0.757532  [   32/  179]
train() client id: f_00007-1-1 loss: 0.670585  [   64/  179]
train() client id: f_00007-1-2 loss: 0.590831  [   96/  179]
train() client id: f_00007-1-3 loss: 0.547211  [  128/  179]
train() client id: f_00007-1-4 loss: 0.861569  [  160/  179]
train() client id: f_00007-2-0 loss: 0.767947  [   32/  179]
train() client id: f_00007-2-1 loss: 0.673147  [   64/  179]
train() client id: f_00007-2-2 loss: 0.610416  [   96/  179]
train() client id: f_00007-2-3 loss: 0.655078  [  128/  179]
train() client id: f_00007-2-4 loss: 0.615503  [  160/  179]
train() client id: f_00007-3-0 loss: 0.669755  [   32/  179]
train() client id: f_00007-3-1 loss: 0.684384  [   64/  179]
train() client id: f_00007-3-2 loss: 0.685946  [   96/  179]
train() client id: f_00007-3-3 loss: 0.537056  [  128/  179]
train() client id: f_00007-3-4 loss: 0.552692  [  160/  179]
train() client id: f_00007-4-0 loss: 0.695245  [   32/  179]
train() client id: f_00007-4-1 loss: 0.516660  [   64/  179]
train() client id: f_00007-4-2 loss: 0.611791  [   96/  179]
train() client id: f_00007-4-3 loss: 0.862341  [  128/  179]
train() client id: f_00007-4-4 loss: 0.555678  [  160/  179]
train() client id: f_00007-5-0 loss: 0.534382  [   32/  179]
train() client id: f_00007-5-1 loss: 0.612024  [   64/  179]
train() client id: f_00007-5-2 loss: 0.685890  [   96/  179]
train() client id: f_00007-5-3 loss: 0.818772  [  128/  179]
train() client id: f_00007-5-4 loss: 0.614340  [  160/  179]
train() client id: f_00007-6-0 loss: 0.664626  [   32/  179]
train() client id: f_00007-6-1 loss: 0.539076  [   64/  179]
train() client id: f_00007-6-2 loss: 0.539046  [   96/  179]
train() client id: f_00007-6-3 loss: 0.689463  [  128/  179]
train() client id: f_00007-6-4 loss: 0.664364  [  160/  179]
train() client id: f_00007-7-0 loss: 0.608660  [   32/  179]
train() client id: f_00007-7-1 loss: 0.608046  [   64/  179]
train() client id: f_00007-7-2 loss: 0.609258  [   96/  179]
train() client id: f_00007-7-3 loss: 0.780093  [  128/  179]
train() client id: f_00007-7-4 loss: 0.528826  [  160/  179]
train() client id: f_00007-8-0 loss: 0.749774  [   32/  179]
train() client id: f_00007-8-1 loss: 0.602625  [   64/  179]
train() client id: f_00007-8-2 loss: 0.507235  [   96/  179]
train() client id: f_00007-8-3 loss: 0.603548  [  128/  179]
train() client id: f_00007-8-4 loss: 0.661535  [  160/  179]
train() client id: f_00007-9-0 loss: 0.700951  [   32/  179]
train() client id: f_00007-9-1 loss: 0.585161  [   64/  179]
train() client id: f_00007-9-2 loss: 0.766814  [   96/  179]
train() client id: f_00007-9-3 loss: 0.659657  [  128/  179]
train() client id: f_00007-9-4 loss: 0.513765  [  160/  179]
train() client id: f_00007-10-0 loss: 0.827793  [   32/  179]
train() client id: f_00007-10-1 loss: 0.722631  [   64/  179]
train() client id: f_00007-10-2 loss: 0.499355  [   96/  179]
train() client id: f_00007-10-3 loss: 0.670892  [  128/  179]
train() client id: f_00007-10-4 loss: 0.500932  [  160/  179]
train() client id: f_00007-11-0 loss: 0.572145  [   32/  179]
train() client id: f_00007-11-1 loss: 0.794125  [   64/  179]
train() client id: f_00007-11-2 loss: 0.612929  [   96/  179]
train() client id: f_00007-11-3 loss: 0.584825  [  128/  179]
train() client id: f_00007-11-4 loss: 0.667031  [  160/  179]
train() client id: f_00008-0-0 loss: 0.827394  [   32/  130]
train() client id: f_00008-0-1 loss: 0.835702  [   64/  130]
train() client id: f_00008-0-2 loss: 0.962679  [   96/  130]
train() client id: f_00008-0-3 loss: 0.760661  [  128/  130]
train() client id: f_00008-1-0 loss: 0.814668  [   32/  130]
train() client id: f_00008-1-1 loss: 0.778861  [   64/  130]
train() client id: f_00008-1-2 loss: 0.933765  [   96/  130]
train() client id: f_00008-1-3 loss: 0.855956  [  128/  130]
train() client id: f_00008-2-0 loss: 0.901731  [   32/  130]
train() client id: f_00008-2-1 loss: 0.687677  [   64/  130]
train() client id: f_00008-2-2 loss: 0.920885  [   96/  130]
train() client id: f_00008-2-3 loss: 0.819252  [  128/  130]
train() client id: f_00008-3-0 loss: 0.769136  [   32/  130]
train() client id: f_00008-3-1 loss: 0.869450  [   64/  130]
train() client id: f_00008-3-2 loss: 0.803521  [   96/  130]
train() client id: f_00008-3-3 loss: 0.931264  [  128/  130]
train() client id: f_00008-4-0 loss: 0.889562  [   32/  130]
train() client id: f_00008-4-1 loss: 0.811890  [   64/  130]
train() client id: f_00008-4-2 loss: 0.911618  [   96/  130]
train() client id: f_00008-4-3 loss: 0.754603  [  128/  130]
train() client id: f_00008-5-0 loss: 0.708161  [   32/  130]
train() client id: f_00008-5-1 loss: 0.934640  [   64/  130]
train() client id: f_00008-5-2 loss: 0.847177  [   96/  130]
train() client id: f_00008-5-3 loss: 0.882892  [  128/  130]
train() client id: f_00008-6-0 loss: 0.823146  [   32/  130]
train() client id: f_00008-6-1 loss: 0.880947  [   64/  130]
train() client id: f_00008-6-2 loss: 0.828178  [   96/  130]
train() client id: f_00008-6-3 loss: 0.797799  [  128/  130]
train() client id: f_00008-7-0 loss: 0.800950  [   32/  130]
train() client id: f_00008-7-1 loss: 0.793607  [   64/  130]
train() client id: f_00008-7-2 loss: 0.796949  [   96/  130]
train() client id: f_00008-7-3 loss: 0.926375  [  128/  130]
train() client id: f_00008-8-0 loss: 0.817906  [   32/  130]
train() client id: f_00008-8-1 loss: 0.830458  [   64/  130]
train() client id: f_00008-8-2 loss: 0.886847  [   96/  130]
train() client id: f_00008-8-3 loss: 0.793968  [  128/  130]
train() client id: f_00008-9-0 loss: 0.703744  [   32/  130]
train() client id: f_00008-9-1 loss: 0.728627  [   64/  130]
train() client id: f_00008-9-2 loss: 0.929175  [   96/  130]
train() client id: f_00008-9-3 loss: 0.953720  [  128/  130]
train() client id: f_00008-10-0 loss: 0.855862  [   32/  130]
train() client id: f_00008-10-1 loss: 0.758569  [   64/  130]
train() client id: f_00008-10-2 loss: 0.824972  [   96/  130]
train() client id: f_00008-10-3 loss: 0.930510  [  128/  130]
train() client id: f_00008-11-0 loss: 0.851063  [   32/  130]
train() client id: f_00008-11-1 loss: 0.758821  [   64/  130]
train() client id: f_00008-11-2 loss: 0.916714  [   96/  130]
train() client id: f_00008-11-3 loss: 0.839224  [  128/  130]
train() client id: f_00009-0-0 loss: 1.262657  [   32/  118]
train() client id: f_00009-0-1 loss: 1.171567  [   64/  118]
train() client id: f_00009-0-2 loss: 0.961693  [   96/  118]
train() client id: f_00009-1-0 loss: 1.188993  [   32/  118]
train() client id: f_00009-1-1 loss: 1.150714  [   64/  118]
train() client id: f_00009-1-2 loss: 1.037871  [   96/  118]
train() client id: f_00009-2-0 loss: 1.199271  [   32/  118]
train() client id: f_00009-2-1 loss: 1.002440  [   64/  118]
train() client id: f_00009-2-2 loss: 1.002802  [   96/  118]
train() client id: f_00009-3-0 loss: 0.979855  [   32/  118]
train() client id: f_00009-3-1 loss: 0.996521  [   64/  118]
train() client id: f_00009-3-2 loss: 0.982224  [   96/  118]
train() client id: f_00009-4-0 loss: 0.974862  [   32/  118]
train() client id: f_00009-4-1 loss: 0.903303  [   64/  118]
train() client id: f_00009-4-2 loss: 0.927209  [   96/  118]
train() client id: f_00009-5-0 loss: 1.045148  [   32/  118]
train() client id: f_00009-5-1 loss: 0.880897  [   64/  118]
train() client id: f_00009-5-2 loss: 0.930426  [   96/  118]
train() client id: f_00009-6-0 loss: 1.048048  [   32/  118]
train() client id: f_00009-6-1 loss: 0.831423  [   64/  118]
train() client id: f_00009-6-2 loss: 0.862327  [   96/  118]
train() client id: f_00009-7-0 loss: 0.866993  [   32/  118]
train() client id: f_00009-7-1 loss: 0.903175  [   64/  118]
train() client id: f_00009-7-2 loss: 0.888262  [   96/  118]
train() client id: f_00009-8-0 loss: 0.836584  [   32/  118]
train() client id: f_00009-8-1 loss: 0.646308  [   64/  118]
train() client id: f_00009-8-2 loss: 0.830011  [   96/  118]
train() client id: f_00009-9-0 loss: 0.756668  [   32/  118]
train() client id: f_00009-9-1 loss: 0.936896  [   64/  118]
train() client id: f_00009-9-2 loss: 0.820327  [   96/  118]
train() client id: f_00009-10-0 loss: 0.814059  [   32/  118]
train() client id: f_00009-10-1 loss: 0.871819  [   64/  118]
train() client id: f_00009-10-2 loss: 0.823532  [   96/  118]
train() client id: f_00009-11-0 loss: 0.760784  [   32/  118]
train() client id: f_00009-11-1 loss: 0.942001  [   64/  118]
train() client id: f_00009-11-2 loss: 0.800235  [   96/  118]
At round 13 accuracy: 0.6339522546419099
At round 13 training accuracy: 0.5754527162977867
At round 13 training loss: 0.8647818963707373
update_location
xs = -3.905658 4.200318 85.009024 18.811294 0.979296 3.956410 -47.443192 -26.324852 69.663977 -12.060879 
ys = 77.587959 60.555839 1.320614 -47.455176 39.350187 22.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 126.629955 116.981418 131.256536 112.275814 107.468117 102.645695 110.714710 103.410222 123.133016 100.804150 
dists_bs = 196.881638 212.626950 312.680702 295.040884 222.192674 234.916856 218.680988 229.002520 290.938102 236.198329 
uav_gains = -102.563866 -101.703071 -102.953772 -101.257239 -100.782036 -100.283542 -101.105202 -100.364113 -102.259670 -100.086978 
bs_gains = -103.804805 -104.740372 -109.429889 -108.723761 -105.275492 -105.952656 -105.081768 -105.642586 -108.553476 -106.018810 
Round 14
-------------------------------
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.14357806 19.06538973  9.00414718  3.22065168 21.99169094 10.5993586
  4.00331986 12.90419715  9.4917528   8.60518442]
obj_prev = 108.02927042739734
eta_min = 6.894580279323513e-11	eta_max = 0.920963466545226
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 25.120031493789718	eta = 0.9090909090909091
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 43.56050107516789	eta = 0.5242453990066629
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 34.74720289395531	eta = 0.6572152681404528
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 33.166400765192755	eta = 0.6885399603277947
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 33.08814089362885	eta = 0.6901684908951391
af = 22.83639226708156	bf = 1.7835048992303524	zeta = 33.08793538155577	eta = 0.6901727775922599
eta = 0.6901727775922599
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [0.03059863 0.06435425 0.03011293 0.01044239 0.07431095 0.03545555
 0.0131137  0.04346948 0.03157001 0.02865586]
ene_total = [2.84199716 5.44950306 2.81511635 1.29013739 6.21774054 3.30722781
 1.48846279 3.76427446 3.1172366  2.79623923]
ti_comp = [0.31764397 0.30976147 0.31634818 0.32166983 0.30755338 0.30458011
 0.32210904 0.32417046 0.29089448 0.30427822]
ti_coms = [0.07051759 0.0784001  0.07181339 0.06649173 0.08060818 0.08358146
 0.06605252 0.06399111 0.09726708 0.08388335]
t_total = [29.29994125 29.29994125 29.29994125 29.29994125 29.29994125 29.29994125
 29.29994125 29.29994125 29.29994125 29.29994125]
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [1.77461660e-05 1.73602902e-04 1.70533033e-05 6.87794728e-07
 2.71142377e-04 3.00282236e-05 1.35846854e-06 4.88524522e-05
 2.32398452e-05 1.58846785e-05]
ene_total = [0.53363372 0.6048986  0.54336256 0.50195733 0.62892872 0.63317194
 0.49869262 0.4867173  0.73596393 0.63438309]
optimize_network iter = 0 obj = 5.8017097996113725
eta = 0.6901727775922599
freqs = [4.81649713e+07 1.03877111e+08 4.75946046e+07 1.62315316e+07
 1.20809836e+08 5.82039824e+07 2.03559896e+07 6.70472512e+07
 5.42636806e+07 4.70882466e+07]
eta_min = 0.6901727775922659	eta_max = 0.6901727775922564
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 0.041597769241550876	eta = 0.9090909090909091
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 19.658200800192578	eta = 0.0019236833645317572
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 2.023882036535733	eta = 0.01868495948542783
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 1.9708396260580614	eta = 0.01918783921124654
af = 0.03781615385595534	bf = 1.7835048992303524	zeta = 1.9708261485414031	eta = 0.01918797042749958
eta = 0.01918797042749958
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [1.89202933e-04 1.85088870e-03 1.81815891e-04 7.33300812e-06
 2.89081782e-03 3.20149601e-04 1.44834795e-05 5.20846430e-04
 2.47774470e-04 1.69356454e-04]
ene_total = [0.17265704 0.23107452 0.17557065 0.15872099 0.26113616 0.20692952
 0.15784421 0.16500284 0.23783644 0.20405377]
ti_comp = [0.31764397 0.30976147 0.31634818 0.32166983 0.30755338 0.30458011
 0.32210904 0.32417046 0.29089448 0.30427822]
ti_coms = [0.07051759 0.0784001  0.07181339 0.06649173 0.08060818 0.08358146
 0.06605252 0.06399111 0.09726708 0.08388335]
t_total = [29.29994125 29.29994125 29.29994125 29.29994125 29.29994125 29.29994125
 29.29994125 29.29994125 29.29994125 29.29994125]
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [1.77461660e-05 1.73602902e-04 1.70533033e-05 6.87794728e-07
 2.71142377e-04 3.00282236e-05 1.35846854e-06 4.88524522e-05
 2.32398452e-05 1.58846785e-05]
ene_total = [0.53363372 0.6048986  0.54336256 0.50195733 0.62892872 0.63317194
 0.49869262 0.4867173  0.73596393 0.63438309]
optimize_network iter = 1 obj = 5.801709799611484
eta = 0.6901727775922659
freqs = [4.81649713e+07 1.03877111e+08 4.75946046e+07 1.62315316e+07
 1.20809836e+08 5.82039824e+07 2.03559896e+07 6.70472512e+07
 5.42636806e+07 4.70882466e+07]
Done!
ene_coms = [0.00705176 0.00784001 0.00718134 0.00664917 0.00806082 0.00835815
 0.00660525 0.00639911 0.00972671 0.00838833]
ene_comp = [1.75381754e-05 1.71568221e-04 1.68534331e-05 6.79733555e-07
 2.67964503e-04 2.96762833e-05 1.34254685e-06 4.82798860e-05
 2.29674669e-05 1.56985050e-05]
ene_total = [0.0070693  0.00801158 0.00719819 0.00664985 0.00832878 0.00838782
 0.00660659 0.00644739 0.00974968 0.00840403]
At round 14 energy consumption: 0.0768532209744656
At round 14 eta: 0.6901727775922659
At round 14 a_n: 23.386960998683932
At round 14 local rounds: 12.142311740979952
At round 14 global rounds: 75.48388039288098
gradient difference: 0.5144508481025696
train() client id: f_00000-0-0 loss: 1.128293  [   32/  126]
train() client id: f_00000-0-1 loss: 0.918837  [   64/  126]
train() client id: f_00000-0-2 loss: 1.212235  [   96/  126]
train() client id: f_00000-1-0 loss: 1.120686  [   32/  126]
train() client id: f_00000-1-1 loss: 1.118335  [   64/  126]
train() client id: f_00000-1-2 loss: 0.877450  [   96/  126]
train() client id: f_00000-2-0 loss: 0.984634  [   32/  126]
train() client id: f_00000-2-1 loss: 0.908888  [   64/  126]
train() client id: f_00000-2-2 loss: 1.107944  [   96/  126]
train() client id: f_00000-3-0 loss: 0.901273  [   32/  126]
train() client id: f_00000-3-1 loss: 0.966390  [   64/  126]
train() client id: f_00000-3-2 loss: 1.045402  [   96/  126]
train() client id: f_00000-4-0 loss: 0.942325  [   32/  126]
train() client id: f_00000-4-1 loss: 0.917170  [   64/  126]
train() client id: f_00000-4-2 loss: 0.967326  [   96/  126]
train() client id: f_00000-5-0 loss: 1.008346  [   32/  126]
train() client id: f_00000-5-1 loss: 0.842337  [   64/  126]
train() client id: f_00000-5-2 loss: 0.895094  [   96/  126]
train() client id: f_00000-6-0 loss: 0.855624  [   32/  126]
train() client id: f_00000-6-1 loss: 0.857492  [   64/  126]
train() client id: f_00000-6-2 loss: 0.930318  [   96/  126]
train() client id: f_00000-7-0 loss: 0.908055  [   32/  126]
train() client id: f_00000-7-1 loss: 0.870686  [   64/  126]
train() client id: f_00000-7-2 loss: 0.859485  [   96/  126]
train() client id: f_00000-8-0 loss: 0.828109  [   32/  126]
train() client id: f_00000-8-1 loss: 0.900021  [   64/  126]
train() client id: f_00000-8-2 loss: 0.952740  [   96/  126]
train() client id: f_00000-9-0 loss: 0.889061  [   32/  126]
train() client id: f_00000-9-1 loss: 0.936803  [   64/  126]
train() client id: f_00000-9-2 loss: 0.861848  [   96/  126]
train() client id: f_00000-10-0 loss: 0.867175  [   32/  126]
train() client id: f_00000-10-1 loss: 0.854434  [   64/  126]
train() client id: f_00000-10-2 loss: 1.013281  [   96/  126]
train() client id: f_00000-11-0 loss: 0.842404  [   32/  126]
train() client id: f_00000-11-1 loss: 0.855449  [   64/  126]
train() client id: f_00000-11-2 loss: 0.920791  [   96/  126]
train() client id: f_00001-0-0 loss: 0.626410  [   32/  265]
train() client id: f_00001-0-1 loss: 0.570541  [   64/  265]
train() client id: f_00001-0-2 loss: 0.587975  [   96/  265]
train() client id: f_00001-0-3 loss: 0.622260  [  128/  265]
train() client id: f_00001-0-4 loss: 0.597376  [  160/  265]
train() client id: f_00001-0-5 loss: 0.621770  [  192/  265]
train() client id: f_00001-0-6 loss: 0.490688  [  224/  265]
train() client id: f_00001-0-7 loss: 0.534809  [  256/  265]
train() client id: f_00001-1-0 loss: 0.530518  [   32/  265]
train() client id: f_00001-1-1 loss: 0.547142  [   64/  265]
train() client id: f_00001-1-2 loss: 0.715387  [   96/  265]
train() client id: f_00001-1-3 loss: 0.653626  [  128/  265]
train() client id: f_00001-1-4 loss: 0.486080  [  160/  265]
train() client id: f_00001-1-5 loss: 0.484235  [  192/  265]
train() client id: f_00001-1-6 loss: 0.590922  [  224/  265]
train() client id: f_00001-1-7 loss: 0.566620  [  256/  265]
train() client id: f_00001-2-0 loss: 0.494271  [   32/  265]
train() client id: f_00001-2-1 loss: 0.585472  [   64/  265]
train() client id: f_00001-2-2 loss: 0.709777  [   96/  265]
train() client id: f_00001-2-3 loss: 0.663068  [  128/  265]
train() client id: f_00001-2-4 loss: 0.470512  [  160/  265]
train() client id: f_00001-2-5 loss: 0.475265  [  192/  265]
train() client id: f_00001-2-6 loss: 0.588010  [  224/  265]
train() client id: f_00001-2-7 loss: 0.535129  [  256/  265]
train() client id: f_00001-3-0 loss: 0.541585  [   32/  265]
train() client id: f_00001-3-1 loss: 0.594725  [   64/  265]
train() client id: f_00001-3-2 loss: 0.598881  [   96/  265]
train() client id: f_00001-3-3 loss: 0.526904  [  128/  265]
train() client id: f_00001-3-4 loss: 0.580178  [  160/  265]
train() client id: f_00001-3-5 loss: 0.602397  [  192/  265]
train() client id: f_00001-3-6 loss: 0.477370  [  224/  265]
train() client id: f_00001-3-7 loss: 0.548028  [  256/  265]
train() client id: f_00001-4-0 loss: 0.526667  [   32/  265]
train() client id: f_00001-4-1 loss: 0.543434  [   64/  265]
train() client id: f_00001-4-2 loss: 0.528253  [   96/  265]
train() client id: f_00001-4-3 loss: 0.510898  [  128/  265]
train() client id: f_00001-4-4 loss: 0.725798  [  160/  265]
train() client id: f_00001-4-5 loss: 0.512657  [  192/  265]
train() client id: f_00001-4-6 loss: 0.463324  [  224/  265]
train() client id: f_00001-4-7 loss: 0.579343  [  256/  265]
train() client id: f_00001-5-0 loss: 0.536754  [   32/  265]
train() client id: f_00001-5-1 loss: 0.568703  [   64/  265]
train() client id: f_00001-5-2 loss: 0.779060  [   96/  265]
train() client id: f_00001-5-3 loss: 0.534900  [  128/  265]
train() client id: f_00001-5-4 loss: 0.510293  [  160/  265]
train() client id: f_00001-5-5 loss: 0.463090  [  192/  265]
train() client id: f_00001-5-6 loss: 0.567736  [  224/  265]
train() client id: f_00001-5-7 loss: 0.473899  [  256/  265]
train() client id: f_00001-6-0 loss: 0.633995  [   32/  265]
train() client id: f_00001-6-1 loss: 0.566067  [   64/  265]
train() client id: f_00001-6-2 loss: 0.464925  [   96/  265]
train() client id: f_00001-6-3 loss: 0.525745  [  128/  265]
train() client id: f_00001-6-4 loss: 0.549486  [  160/  265]
train() client id: f_00001-6-5 loss: 0.541016  [  192/  265]
train() client id: f_00001-6-6 loss: 0.520918  [  224/  265]
train() client id: f_00001-6-7 loss: 0.528939  [  256/  265]
train() client id: f_00001-7-0 loss: 0.512188  [   32/  265]
train() client id: f_00001-7-1 loss: 0.607251  [   64/  265]
train() client id: f_00001-7-2 loss: 0.507360  [   96/  265]
train() client id: f_00001-7-3 loss: 0.584715  [  128/  265]
train() client id: f_00001-7-4 loss: 0.571439  [  160/  265]
train() client id: f_00001-7-5 loss: 0.565083  [  192/  265]
train() client id: f_00001-7-6 loss: 0.518810  [  224/  265]
train() client id: f_00001-7-7 loss: 0.553837  [  256/  265]
train() client id: f_00001-8-0 loss: 0.522233  [   32/  265]
train() client id: f_00001-8-1 loss: 0.585404  [   64/  265]
train() client id: f_00001-8-2 loss: 0.454596  [   96/  265]
train() client id: f_00001-8-3 loss: 0.504256  [  128/  265]
train() client id: f_00001-8-4 loss: 0.522483  [  160/  265]
train() client id: f_00001-8-5 loss: 0.524080  [  192/  265]
train() client id: f_00001-8-6 loss: 0.686007  [  224/  265]
train() client id: f_00001-8-7 loss: 0.621475  [  256/  265]
train() client id: f_00001-9-0 loss: 0.604644  [   32/  265]
train() client id: f_00001-9-1 loss: 0.510272  [   64/  265]
train() client id: f_00001-9-2 loss: 0.523467  [   96/  265]
train() client id: f_00001-9-3 loss: 0.568232  [  128/  265]
train() client id: f_00001-9-4 loss: 0.612892  [  160/  265]
train() client id: f_00001-9-5 loss: 0.456057  [  192/  265]
train() client id: f_00001-9-6 loss: 0.513245  [  224/  265]
train() client id: f_00001-9-7 loss: 0.566106  [  256/  265]
train() client id: f_00001-10-0 loss: 0.453494  [   32/  265]
train() client id: f_00001-10-1 loss: 0.525103  [   64/  265]
train() client id: f_00001-10-2 loss: 0.567042  [   96/  265]
train() client id: f_00001-10-3 loss: 0.504835  [  128/  265]
train() client id: f_00001-10-4 loss: 0.603740  [  160/  265]
train() client id: f_00001-10-5 loss: 0.638510  [  192/  265]
train() client id: f_00001-10-6 loss: 0.537607  [  224/  265]
train() client id: f_00001-10-7 loss: 0.534974  [  256/  265]
train() client id: f_00001-11-0 loss: 0.772915  [   32/  265]
train() client id: f_00001-11-1 loss: 0.529283  [   64/  265]
train() client id: f_00001-11-2 loss: 0.519184  [   96/  265]
train() client id: f_00001-11-3 loss: 0.500885  [  128/  265]
train() client id: f_00001-11-4 loss: 0.588178  [  160/  265]
train() client id: f_00001-11-5 loss: 0.543019  [  192/  265]
train() client id: f_00001-11-6 loss: 0.468303  [  224/  265]
train() client id: f_00001-11-7 loss: 0.521547  [  256/  265]
train() client id: f_00002-0-0 loss: 1.250479  [   32/  124]
train() client id: f_00002-0-1 loss: 1.327174  [   64/  124]
train() client id: f_00002-0-2 loss: 1.261115  [   96/  124]
train() client id: f_00002-1-0 loss: 1.314961  [   32/  124]
train() client id: f_00002-1-1 loss: 1.233195  [   64/  124]
train() client id: f_00002-1-2 loss: 1.175651  [   96/  124]
train() client id: f_00002-2-0 loss: 1.230485  [   32/  124]
train() client id: f_00002-2-1 loss: 1.195245  [   64/  124]
train() client id: f_00002-2-2 loss: 1.147691  [   96/  124]
train() client id: f_00002-3-0 loss: 1.203350  [   32/  124]
train() client id: f_00002-3-1 loss: 1.249190  [   64/  124]
train() client id: f_00002-3-2 loss: 1.153713  [   96/  124]
train() client id: f_00002-4-0 loss: 1.154327  [   32/  124]
train() client id: f_00002-4-1 loss: 1.132828  [   64/  124]
train() client id: f_00002-4-2 loss: 1.204301  [   96/  124]
train() client id: f_00002-5-0 loss: 1.150820  [   32/  124]
train() client id: f_00002-5-1 loss: 1.226640  [   64/  124]
train() client id: f_00002-5-2 loss: 1.043764  [   96/  124]
train() client id: f_00002-6-0 loss: 1.097469  [   32/  124]
train() client id: f_00002-6-1 loss: 1.092248  [   64/  124]
train() client id: f_00002-6-2 loss: 1.209477  [   96/  124]
train() client id: f_00002-7-0 loss: 1.160477  [   32/  124]
train() client id: f_00002-7-1 loss: 1.153694  [   64/  124]
train() client id: f_00002-7-2 loss: 1.114818  [   96/  124]
train() client id: f_00002-8-0 loss: 1.067919  [   32/  124]
train() client id: f_00002-8-1 loss: 1.134674  [   64/  124]
train() client id: f_00002-8-2 loss: 1.041960  [   96/  124]
train() client id: f_00002-9-0 loss: 1.193700  [   32/  124]
train() client id: f_00002-9-1 loss: 1.115781  [   64/  124]
train() client id: f_00002-9-2 loss: 1.048267  [   96/  124]
train() client id: f_00002-10-0 loss: 1.296661  [   32/  124]
train() client id: f_00002-10-1 loss: 0.953498  [   64/  124]
train() client id: f_00002-10-2 loss: 1.045429  [   96/  124]
train() client id: f_00002-11-0 loss: 1.086257  [   32/  124]
train() client id: f_00002-11-1 loss: 1.156787  [   64/  124]
train() client id: f_00002-11-2 loss: 1.139755  [   96/  124]
train() client id: f_00003-0-0 loss: 0.895650  [   32/   43]
train() client id: f_00003-1-0 loss: 0.901380  [   32/   43]
train() client id: f_00003-2-0 loss: 0.909260  [   32/   43]
train() client id: f_00003-3-0 loss: 0.867986  [   32/   43]
train() client id: f_00003-4-0 loss: 0.955895  [   32/   43]
train() client id: f_00003-5-0 loss: 0.982202  [   32/   43]
train() client id: f_00003-6-0 loss: 0.932173  [   32/   43]
train() client id: f_00003-7-0 loss: 0.993696  [   32/   43]
train() client id: f_00003-8-0 loss: 0.851569  [   32/   43]
train() client id: f_00003-9-0 loss: 0.906315  [   32/   43]
train() client id: f_00003-10-0 loss: 0.916539  [   32/   43]
train() client id: f_00003-11-0 loss: 0.812353  [   32/   43]
train() client id: f_00004-0-0 loss: 1.072314  [   32/  306]
train() client id: f_00004-0-1 loss: 1.083965  [   64/  306]
train() client id: f_00004-0-2 loss: 1.029008  [   96/  306]
train() client id: f_00004-0-3 loss: 1.048230  [  128/  306]
train() client id: f_00004-0-4 loss: 1.080263  [  160/  306]
train() client id: f_00004-0-5 loss: 1.123842  [  192/  306]
train() client id: f_00004-0-6 loss: 1.022902  [  224/  306]
train() client id: f_00004-0-7 loss: 1.023522  [  256/  306]
train() client id: f_00004-0-8 loss: 1.051018  [  288/  306]
train() client id: f_00004-1-0 loss: 0.992790  [   32/  306]
train() client id: f_00004-1-1 loss: 1.029042  [   64/  306]
train() client id: f_00004-1-2 loss: 0.972654  [   96/  306]
train() client id: f_00004-1-3 loss: 1.205590  [  128/  306]
train() client id: f_00004-1-4 loss: 1.087080  [  160/  306]
train() client id: f_00004-1-5 loss: 1.231312  [  192/  306]
train() client id: f_00004-1-6 loss: 1.045826  [  224/  306]
train() client id: f_00004-1-7 loss: 0.962517  [  256/  306]
train() client id: f_00004-1-8 loss: 0.867127  [  288/  306]
train() client id: f_00004-2-0 loss: 1.101790  [   32/  306]
train() client id: f_00004-2-1 loss: 1.016836  [   64/  306]
train() client id: f_00004-2-2 loss: 0.990896  [   96/  306]
train() client id: f_00004-2-3 loss: 1.035025  [  128/  306]
train() client id: f_00004-2-4 loss: 1.003860  [  160/  306]
train() client id: f_00004-2-5 loss: 1.038580  [  192/  306]
train() client id: f_00004-2-6 loss: 0.948758  [  224/  306]
train() client id: f_00004-2-7 loss: 1.085981  [  256/  306]
train() client id: f_00004-2-8 loss: 1.017220  [  288/  306]
train() client id: f_00004-3-0 loss: 1.042536  [   32/  306]
train() client id: f_00004-3-1 loss: 0.974345  [   64/  306]
train() client id: f_00004-3-2 loss: 1.001815  [   96/  306]
train() client id: f_00004-3-3 loss: 1.065879  [  128/  306]
train() client id: f_00004-3-4 loss: 1.055827  [  160/  306]
train() client id: f_00004-3-5 loss: 1.048937  [  192/  306]
train() client id: f_00004-3-6 loss: 0.985595  [  224/  306]
train() client id: f_00004-3-7 loss: 1.149789  [  256/  306]
train() client id: f_00004-3-8 loss: 0.973142  [  288/  306]
train() client id: f_00004-4-0 loss: 1.056161  [   32/  306]
train() client id: f_00004-4-1 loss: 1.051764  [   64/  306]
train() client id: f_00004-4-2 loss: 0.915660  [   96/  306]
train() client id: f_00004-4-3 loss: 1.012482  [  128/  306]
train() client id: f_00004-4-4 loss: 1.127061  [  160/  306]
train() client id: f_00004-4-5 loss: 0.975062  [  192/  306]
train() client id: f_00004-4-6 loss: 1.024168  [  224/  306]
train() client id: f_00004-4-7 loss: 1.017798  [  256/  306]
train() client id: f_00004-4-8 loss: 1.053605  [  288/  306]
train() client id: f_00004-5-0 loss: 1.022329  [   32/  306]
train() client id: f_00004-5-1 loss: 1.102146  [   64/  306]
train() client id: f_00004-5-2 loss: 0.960974  [   96/  306]
train() client id: f_00004-5-3 loss: 0.976725  [  128/  306]
train() client id: f_00004-5-4 loss: 1.078670  [  160/  306]
train() client id: f_00004-5-5 loss: 0.961882  [  192/  306]
train() client id: f_00004-5-6 loss: 0.966277  [  224/  306]
train() client id: f_00004-5-7 loss: 0.938902  [  256/  306]
train() client id: f_00004-5-8 loss: 1.107705  [  288/  306]
train() client id: f_00004-6-0 loss: 0.944955  [   32/  306]
train() client id: f_00004-6-1 loss: 0.974541  [   64/  306]
train() client id: f_00004-6-2 loss: 1.070026  [   96/  306]
train() client id: f_00004-6-3 loss: 1.073141  [  128/  306]
train() client id: f_00004-6-4 loss: 0.987831  [  160/  306]
train() client id: f_00004-6-5 loss: 1.049901  [  192/  306]
train() client id: f_00004-6-6 loss: 0.910439  [  224/  306]
train() client id: f_00004-6-7 loss: 1.002167  [  256/  306]
train() client id: f_00004-6-8 loss: 1.035098  [  288/  306]
train() client id: f_00004-7-0 loss: 0.824218  [   32/  306]
train() client id: f_00004-7-1 loss: 1.159356  [   64/  306]
train() client id: f_00004-7-2 loss: 1.102405  [   96/  306]
train() client id: f_00004-7-3 loss: 1.012587  [  128/  306]
train() client id: f_00004-7-4 loss: 0.962928  [  160/  306]
train() client id: f_00004-7-5 loss: 0.975129  [  192/  306]
train() client id: f_00004-7-6 loss: 1.056502  [  224/  306]
train() client id: f_00004-7-7 loss: 1.026723  [  256/  306]
train() client id: f_00004-7-8 loss: 0.976655  [  288/  306]
train() client id: f_00004-8-0 loss: 1.062731  [   32/  306]
train() client id: f_00004-8-1 loss: 1.007117  [   64/  306]
train() client id: f_00004-8-2 loss: 1.074440  [   96/  306]
train() client id: f_00004-8-3 loss: 1.042938  [  128/  306]
train() client id: f_00004-8-4 loss: 1.090060  [  160/  306]
train() client id: f_00004-8-5 loss: 1.145054  [  192/  306]
train() client id: f_00004-8-6 loss: 0.856068  [  224/  306]
train() client id: f_00004-8-7 loss: 0.905341  [  256/  306]
train() client id: f_00004-8-8 loss: 0.919637  [  288/  306]
train() client id: f_00004-9-0 loss: 0.955687  [   32/  306]
train() client id: f_00004-9-1 loss: 0.976022  [   64/  306]
train() client id: f_00004-9-2 loss: 0.990335  [   96/  306]
train() client id: f_00004-9-3 loss: 1.074126  [  128/  306]
train() client id: f_00004-9-4 loss: 1.002700  [  160/  306]
train() client id: f_00004-9-5 loss: 0.982944  [  192/  306]
train() client id: f_00004-9-6 loss: 0.971838  [  224/  306]
train() client id: f_00004-9-7 loss: 1.016515  [  256/  306]
train() client id: f_00004-9-8 loss: 1.007338  [  288/  306]
train() client id: f_00004-10-0 loss: 0.945474  [   32/  306]
train() client id: f_00004-10-1 loss: 1.009555  [   64/  306]
train() client id: f_00004-10-2 loss: 0.933768  [   96/  306]
train() client id: f_00004-10-3 loss: 0.935520  [  128/  306]
train() client id: f_00004-10-4 loss: 1.190888  [  160/  306]
train() client id: f_00004-10-5 loss: 1.020101  [  192/  306]
train() client id: f_00004-10-6 loss: 0.988668  [  224/  306]
train() client id: f_00004-10-7 loss: 0.928589  [  256/  306]
train() client id: f_00004-10-8 loss: 0.960432  [  288/  306]
train() client id: f_00004-11-0 loss: 0.961891  [   32/  306]
train() client id: f_00004-11-1 loss: 0.923514  [   64/  306]
train() client id: f_00004-11-2 loss: 0.992645  [   96/  306]
train() client id: f_00004-11-3 loss: 0.928913  [  128/  306]
train() client id: f_00004-11-4 loss: 1.085048  [  160/  306]
train() client id: f_00004-11-5 loss: 1.045524  [  192/  306]
train() client id: f_00004-11-6 loss: 0.994249  [  224/  306]
train() client id: f_00004-11-7 loss: 1.026939  [  256/  306]
train() client id: f_00004-11-8 loss: 0.973128  [  288/  306]
train() client id: f_00005-0-0 loss: 0.937424  [   32/  146]
train() client id: f_00005-0-1 loss: 0.523193  [   64/  146]
train() client id: f_00005-0-2 loss: 0.457664  [   96/  146]
train() client id: f_00005-0-3 loss: 0.453342  [  128/  146]
train() client id: f_00005-1-0 loss: 0.711013  [   32/  146]
train() client id: f_00005-1-1 loss: 0.462561  [   64/  146]
train() client id: f_00005-1-2 loss: 0.501587  [   96/  146]
train() client id: f_00005-1-3 loss: 0.660257  [  128/  146]
train() client id: f_00005-2-0 loss: 0.638427  [   32/  146]
train() client id: f_00005-2-1 loss: 0.656267  [   64/  146]
train() client id: f_00005-2-2 loss: 0.576462  [   96/  146]
train() client id: f_00005-2-3 loss: 0.395798  [  128/  146]
train() client id: f_00005-3-0 loss: 0.543431  [   32/  146]
train() client id: f_00005-3-1 loss: 0.574404  [   64/  146]
train() client id: f_00005-3-2 loss: 0.722907  [   96/  146]
train() client id: f_00005-3-3 loss: 0.497129  [  128/  146]
train() client id: f_00005-4-0 loss: 0.410395  [   32/  146]
train() client id: f_00005-4-1 loss: 0.599623  [   64/  146]
train() client id: f_00005-4-2 loss: 0.631145  [   96/  146]
train() client id: f_00005-4-3 loss: 0.446205  [  128/  146]
train() client id: f_00005-5-0 loss: 0.470494  [   32/  146]
train() client id: f_00005-5-1 loss: 0.525463  [   64/  146]
train() client id: f_00005-5-2 loss: 0.776146  [   96/  146]
train() client id: f_00005-5-3 loss: 0.464410  [  128/  146]
train() client id: f_00005-6-0 loss: 0.489672  [   32/  146]
train() client id: f_00005-6-1 loss: 0.676859  [   64/  146]
train() client id: f_00005-6-2 loss: 0.497579  [   96/  146]
train() client id: f_00005-6-3 loss: 0.549554  [  128/  146]
train() client id: f_00005-7-0 loss: 0.712184  [   32/  146]
train() client id: f_00005-7-1 loss: 0.539673  [   64/  146]
train() client id: f_00005-7-2 loss: 0.641337  [   96/  146]
train() client id: f_00005-7-3 loss: 0.358326  [  128/  146]
train() client id: f_00005-8-0 loss: 0.535177  [   32/  146]
train() client id: f_00005-8-1 loss: 0.618063  [   64/  146]
train() client id: f_00005-8-2 loss: 0.535651  [   96/  146]
train() client id: f_00005-8-3 loss: 0.449771  [  128/  146]
train() client id: f_00005-9-0 loss: 0.583597  [   32/  146]
train() client id: f_00005-9-1 loss: 0.731660  [   64/  146]
train() client id: f_00005-9-2 loss: 0.586291  [   96/  146]
train() client id: f_00005-9-3 loss: 0.384096  [  128/  146]
train() client id: f_00005-10-0 loss: 0.582931  [   32/  146]
train() client id: f_00005-10-1 loss: 0.523211  [   64/  146]
train() client id: f_00005-10-2 loss: 0.528632  [   96/  146]
train() client id: f_00005-10-3 loss: 0.512799  [  128/  146]
train() client id: f_00005-11-0 loss: 0.427289  [   32/  146]
train() client id: f_00005-11-1 loss: 0.717748  [   64/  146]
train() client id: f_00005-11-2 loss: 0.504273  [   96/  146]
train() client id: f_00005-11-3 loss: 0.399317  [  128/  146]
train() client id: f_00006-0-0 loss: 0.622986  [   32/   54]
train() client id: f_00006-1-0 loss: 0.604259  [   32/   54]
train() client id: f_00006-2-0 loss: 0.630062  [   32/   54]
train() client id: f_00006-3-0 loss: 0.655603  [   32/   54]
train() client id: f_00006-4-0 loss: 0.591510  [   32/   54]
train() client id: f_00006-5-0 loss: 0.623183  [   32/   54]
train() client id: f_00006-6-0 loss: 0.668188  [   32/   54]
train() client id: f_00006-7-0 loss: 0.638622  [   32/   54]
train() client id: f_00006-8-0 loss: 0.654855  [   32/   54]
train() client id: f_00006-9-0 loss: 0.635556  [   32/   54]
train() client id: f_00006-10-0 loss: 0.663019  [   32/   54]
train() client id: f_00006-11-0 loss: 0.588966  [   32/   54]
train() client id: f_00007-0-0 loss: 0.661454  [   32/  179]
train() client id: f_00007-0-1 loss: 0.668742  [   64/  179]
train() client id: f_00007-0-2 loss: 0.526403  [   96/  179]
train() client id: f_00007-0-3 loss: 0.634044  [  128/  179]
train() client id: f_00007-0-4 loss: 0.668153  [  160/  179]
train() client id: f_00007-1-0 loss: 0.635394  [   32/  179]
train() client id: f_00007-1-1 loss: 0.570831  [   64/  179]
train() client id: f_00007-1-2 loss: 0.635772  [   96/  179]
train() client id: f_00007-1-3 loss: 0.679514  [  128/  179]
train() client id: f_00007-1-4 loss: 0.601113  [  160/  179]
train() client id: f_00007-2-0 loss: 0.570893  [   32/  179]
train() client id: f_00007-2-1 loss: 0.670959  [   64/  179]
train() client id: f_00007-2-2 loss: 0.507104  [   96/  179]
train() client id: f_00007-2-3 loss: 0.654859  [  128/  179]
train() client id: f_00007-2-4 loss: 0.632474  [  160/  179]
train() client id: f_00007-3-0 loss: 0.565553  [   32/  179]
train() client id: f_00007-3-1 loss: 0.485435  [   64/  179]
train() client id: f_00007-3-2 loss: 0.486438  [   96/  179]
train() client id: f_00007-3-3 loss: 0.660016  [  128/  179]
train() client id: f_00007-3-4 loss: 0.576445  [  160/  179]
train() client id: f_00007-4-0 loss: 0.652459  [   32/  179]
train() client id: f_00007-4-1 loss: 0.559479  [   64/  179]
train() client id: f_00007-4-2 loss: 0.527206  [   96/  179]
train() client id: f_00007-4-3 loss: 0.613708  [  128/  179]
train() client id: f_00007-4-4 loss: 0.521114  [  160/  179]
train() client id: f_00007-5-0 loss: 0.660065  [   32/  179]
train() client id: f_00007-5-1 loss: 0.544379  [   64/  179]
train() client id: f_00007-5-2 loss: 0.701769  [   96/  179]
train() client id: f_00007-5-3 loss: 0.539779  [  128/  179]
train() client id: f_00007-5-4 loss: 0.467896  [  160/  179]
train() client id: f_00007-6-0 loss: 0.610764  [   32/  179]
train() client id: f_00007-6-1 loss: 0.544714  [   64/  179]
train() client id: f_00007-6-2 loss: 0.557550  [   96/  179]
train() client id: f_00007-6-3 loss: 0.526989  [  128/  179]
train() client id: f_00007-6-4 loss: 0.589201  [  160/  179]
train() client id: f_00007-7-0 loss: 0.610332  [   32/  179]
train() client id: f_00007-7-1 loss: 0.454168  [   64/  179]
train() client id: f_00007-7-2 loss: 0.571835  [   96/  179]
train() client id: f_00007-7-3 loss: 0.633612  [  128/  179]
train() client id: f_00007-7-4 loss: 0.522194  [  160/  179]
train() client id: f_00007-8-0 loss: 0.444314  [   32/  179]
train() client id: f_00007-8-1 loss: 0.549558  [   64/  179]
train() client id: f_00007-8-2 loss: 0.589422  [   96/  179]
train() client id: f_00007-8-3 loss: 0.738083  [  128/  179]
train() client id: f_00007-8-4 loss: 0.541377  [  160/  179]
train() client id: f_00007-9-0 loss: 0.448078  [   32/  179]
train() client id: f_00007-9-1 loss: 0.479450  [   64/  179]
train() client id: f_00007-9-2 loss: 0.790589  [   96/  179]
train() client id: f_00007-9-3 loss: 0.547016  [  128/  179]
train() client id: f_00007-9-4 loss: 0.431471  [  160/  179]
train() client id: f_00007-10-0 loss: 0.615192  [   32/  179]
train() client id: f_00007-10-1 loss: 0.430731  [   64/  179]
train() client id: f_00007-10-2 loss: 0.576729  [   96/  179]
train() client id: f_00007-10-3 loss: 0.539916  [  128/  179]
train() client id: f_00007-10-4 loss: 0.595438  [  160/  179]
train() client id: f_00007-11-0 loss: 0.484546  [   32/  179]
train() client id: f_00007-11-1 loss: 0.426791  [   64/  179]
train() client id: f_00007-11-2 loss: 0.621555  [   96/  179]
train() client id: f_00007-11-3 loss: 0.797689  [  128/  179]
train() client id: f_00007-11-4 loss: 0.420032  [  160/  179]
train() client id: f_00008-0-0 loss: 0.649403  [   32/  130]
train() client id: f_00008-0-1 loss: 0.855481  [   64/  130]
train() client id: f_00008-0-2 loss: 0.848824  [   96/  130]
train() client id: f_00008-0-3 loss: 0.745067  [  128/  130]
train() client id: f_00008-1-0 loss: 0.807131  [   32/  130]
train() client id: f_00008-1-1 loss: 0.742153  [   64/  130]
train() client id: f_00008-1-2 loss: 0.735521  [   96/  130]
train() client id: f_00008-1-3 loss: 0.840850  [  128/  130]
train() client id: f_00008-2-0 loss: 0.686360  [   32/  130]
train() client id: f_00008-2-1 loss: 0.802761  [   64/  130]
train() client id: f_00008-2-2 loss: 0.783537  [   96/  130]
train() client id: f_00008-2-3 loss: 0.790526  [  128/  130]
train() client id: f_00008-3-0 loss: 0.749079  [   32/  130]
train() client id: f_00008-3-1 loss: 0.705953  [   64/  130]
train() client id: f_00008-3-2 loss: 0.884547  [   96/  130]
train() client id: f_00008-3-3 loss: 0.740871  [  128/  130]
train() client id: f_00008-4-0 loss: 0.897011  [   32/  130]
train() client id: f_00008-4-1 loss: 0.707257  [   64/  130]
train() client id: f_00008-4-2 loss: 0.854468  [   96/  130]
train() client id: f_00008-4-3 loss: 0.660298  [  128/  130]
train() client id: f_00008-5-0 loss: 0.709429  [   32/  130]
train() client id: f_00008-5-1 loss: 0.942458  [   64/  130]
train() client id: f_00008-5-2 loss: 0.720486  [   96/  130]
train() client id: f_00008-5-3 loss: 0.677922  [  128/  130]
train() client id: f_00008-6-0 loss: 0.978011  [   32/  130]
train() client id: f_00008-6-1 loss: 0.632043  [   64/  130]
train() client id: f_00008-6-2 loss: 0.725743  [   96/  130]
train() client id: f_00008-6-3 loss: 0.719647  [  128/  130]
train() client id: f_00008-7-0 loss: 0.823130  [   32/  130]
train() client id: f_00008-7-1 loss: 0.851067  [   64/  130]
train() client id: f_00008-7-2 loss: 0.666493  [   96/  130]
train() client id: f_00008-7-3 loss: 0.773425  [  128/  130]
train() client id: f_00008-8-0 loss: 0.758061  [   32/  130]
train() client id: f_00008-8-1 loss: 0.803355  [   64/  130]
train() client id: f_00008-8-2 loss: 0.695442  [   96/  130]
train() client id: f_00008-8-3 loss: 0.833181  [  128/  130]
train() client id: f_00008-9-0 loss: 0.781702  [   32/  130]
train() client id: f_00008-9-1 loss: 0.791297  [   64/  130]
train() client id: f_00008-9-2 loss: 0.844370  [   96/  130]
train() client id: f_00008-9-3 loss: 0.700907  [  128/  130]
train() client id: f_00008-10-0 loss: 0.707830  [   32/  130]
train() client id: f_00008-10-1 loss: 0.883637  [   64/  130]
train() client id: f_00008-10-2 loss: 0.854535  [   96/  130]
train() client id: f_00008-10-3 loss: 0.676945  [  128/  130]
train() client id: f_00008-11-0 loss: 0.768670  [   32/  130]
train() client id: f_00008-11-1 loss: 0.674745  [   64/  130]
train() client id: f_00008-11-2 loss: 0.710240  [   96/  130]
train() client id: f_00008-11-3 loss: 0.966468  [  128/  130]
train() client id: f_00009-0-0 loss: 1.102507  [   32/  118]
train() client id: f_00009-0-1 loss: 1.220896  [   64/  118]
train() client id: f_00009-0-2 loss: 1.010708  [   96/  118]
train() client id: f_00009-1-0 loss: 1.149686  [   32/  118]
train() client id: f_00009-1-1 loss: 1.138236  [   64/  118]
train() client id: f_00009-1-2 loss: 1.061352  [   96/  118]
train() client id: f_00009-2-0 loss: 1.044750  [   32/  118]
train() client id: f_00009-2-1 loss: 0.976028  [   64/  118]
train() client id: f_00009-2-2 loss: 1.102532  [   96/  118]
train() client id: f_00009-3-0 loss: 1.049585  [   32/  118]
train() client id: f_00009-3-1 loss: 0.941741  [   64/  118]
train() client id: f_00009-3-2 loss: 1.074472  [   96/  118]
train() client id: f_00009-4-0 loss: 0.868121  [   32/  118]
train() client id: f_00009-4-1 loss: 1.013616  [   64/  118]
train() client id: f_00009-4-2 loss: 0.994724  [   96/  118]
train() client id: f_00009-5-0 loss: 0.998638  [   32/  118]
train() client id: f_00009-5-1 loss: 0.949062  [   64/  118]
train() client id: f_00009-5-2 loss: 0.898339  [   96/  118]
train() client id: f_00009-6-0 loss: 0.909833  [   32/  118]
train() client id: f_00009-6-1 loss: 0.912166  [   64/  118]
train() client id: f_00009-6-2 loss: 0.969821  [   96/  118]
train() client id: f_00009-7-0 loss: 0.920968  [   32/  118]
train() client id: f_00009-7-1 loss: 0.877041  [   64/  118]
train() client id: f_00009-7-2 loss: 0.938348  [   96/  118]
train() client id: f_00009-8-0 loss: 1.049522  [   32/  118]
train() client id: f_00009-8-1 loss: 0.881462  [   64/  118]
train() client id: f_00009-8-2 loss: 0.766622  [   96/  118]
train() client id: f_00009-9-0 loss: 0.852836  [   32/  118]
train() client id: f_00009-9-1 loss: 0.861030  [   64/  118]
train() client id: f_00009-9-2 loss: 0.934305  [   96/  118]
train() client id: f_00009-10-0 loss: 1.076968  [   32/  118]
train() client id: f_00009-10-1 loss: 0.837867  [   64/  118]
train() client id: f_00009-10-2 loss: 0.866687  [   96/  118]
train() client id: f_00009-11-0 loss: 0.994794  [   32/  118]
train() client id: f_00009-11-1 loss: 0.869771  [   64/  118]
train() client id: f_00009-11-2 loss: 0.935890  [   96/  118]
At round 14 accuracy: 0.636604774535809
At round 14 training accuracy: 0.5767940979208585
At round 14 training loss: 0.8587519501851238
update_location
xs = -3.905658 4.200318 90.009024 18.811294 0.979296 3.956410 -52.443192 -31.324852 74.663977 -17.060879 
ys = 82.587959 65.555839 1.320614 -52.455176 44.350187 27.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 129.753710 119.646189 134.548758 114.478864 109.397889 103.871460 112.947682 104.794669 126.029280 101.523817 
dists_bs = 194.456316 209.978043 316.850614 298.828839 219.175926 231.709022 215.802702 225.788402 295.155246 232.777274 
uav_gains = -102.828633 -101.947677 -103.223044 -101.468243 -100.975281 -100.412433 -101.322024 -100.508511 -102.512212 -100.164218 
bs_gains = -103.654077 -104.587928 -109.590987 -108.878889 -105.109259 -105.785462 -104.920652 -105.470705 -108.728473 -105.841395 
Round 15
-------------------------------
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.01168963 18.78472696  8.87441189  3.17492049 21.66795492 10.44235678
  3.94614583 12.71610262  9.35523145  8.4772718 ]
obj_prev = 106.45081237176021
eta_min = 4.89584141592709e-11	eta_max = 0.9211440323876784
af = 22.501910530386866	bf = 1.761176745733534	zeta = 24.752101583425553	eta = 0.9090909090909091
af = 22.501910530386866	bf = 1.761176745733534	zeta = 42.96421672758091	eta = 0.5237360818902523
af = 22.501910530386866	bf = 1.761176745733534	zeta = 34.25554170777676	eta = 0.6568838035709252
af = 22.501910530386866	bf = 1.761176745733534	zeta = 32.69318907763576	eta = 0.6882751779568552
af = 22.501910530386866	bf = 1.761176745733534	zeta = 32.6157469047972	eta = 0.6899094046832094
af = 22.501910530386866	bf = 1.761176745733534	zeta = 32.61554299983585	eta = 0.6899137178399916
eta = 0.6899137178399916
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [0.0306296  0.06441941 0.03014342 0.01045296 0.07438618 0.03549145
 0.01312697 0.04351349 0.03160197 0.02868487]
ene_total = [2.80683503 5.36552507 2.7806876  1.27556609 6.12194985 3.25313134
 1.47109077 3.71222193 3.07937974 2.74915557]
ti_comp = [0.32219551 0.3157955  0.32085207 0.32647704 0.31367854 0.31076018
 0.32690731 0.32920522 0.2952455  0.31050951]
ti_coms = [0.07139244 0.07779246 0.07273588 0.06711092 0.07990942 0.08282778
 0.06668064 0.06438274 0.09834245 0.08307845]
t_total = [29.24993706 29.24993706 29.24993706 29.24993706 29.24993706 29.24993706
 29.24993706 29.24993706 29.24993706 29.24993706]
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [1.73007617e-05 1.67539923e-04 1.66283024e-05 6.69719050e-07
 2.61449152e-04 2.89334621e-05 1.32289255e-06 4.75136331e-05
 2.26285138e-05 1.52999065e-05]
ene_total = [0.5318468  0.59057441 0.54178073 0.49879221 0.61328584 0.61769428
 0.4956431  0.48199869 0.73252479 0.61854397]
optimize_network iter = 0 obj = 5.7226848172315465
eta = 0.6899137178399916
freqs = [4.75326373e+07 1.01995448e+08 4.69740154e+07 1.60087223e+07
 1.18570726e+08 5.71042387e+07 2.00775158e+07 6.60886950e+07
 5.35181277e+07 4.61899994e+07]
eta_min = 0.6899137178400166	eta_max = 0.6899137178399891
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 0.03956004889123434	eta = 0.9090909090909091
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 19.410648956797782	eta = 0.0018527809600934662
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 1.9911157923761436	eta = 0.018062074012930685
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 1.9405838032428373	eta = 0.018532402852232132
af = 0.03596368081021303	bf = 1.761176745733534	zeta = 1.9405717992779243	eta = 0.01853251748973932
eta = 0.01853251748973932
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [1.85887615e-04 1.80012864e-03 1.78662392e-04 7.19578012e-06
 2.80913409e-03 3.10874884e-04 1.42137870e-05 5.10509081e-04
 2.43131519e-04 1.64389475e-04]
ene_total = [0.1719908  0.2249194  0.17497549 0.15774238 0.25358093 0.2017751
 0.15689688 0.1631543  0.23661227 0.19892425]
ti_comp = [0.32219551 0.3157955  0.32085207 0.32647704 0.31367854 0.31076018
 0.32690731 0.32920522 0.2952455  0.31050951]
ti_coms = [0.07139244 0.07779246 0.07273588 0.06711092 0.07990942 0.08282778
 0.06668064 0.06438274 0.09834245 0.08307845]
t_total = [29.24993706 29.24993706 29.24993706 29.24993706 29.24993706 29.24993706
 29.24993706 29.24993706 29.24993706 29.24993706]
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [1.73007617e-05 1.67539923e-04 1.66283024e-05 6.69719050e-07
 2.61449152e-04 2.89334621e-05 1.32289255e-06 4.75136331e-05
 2.26285138e-05 1.52999065e-05]
ene_total = [0.5318468  0.59057441 0.54178073 0.49879221 0.61328584 0.61769428
 0.4956431  0.48199869 0.73252479 0.61854397]
optimize_network iter = 1 obj = 5.722684817232005
eta = 0.6899137178400166
freqs = [4.75326373e+07 1.01995448e+08 4.69740154e+07 1.60087223e+07
 1.18570726e+08 5.71042387e+07 2.00775158e+07 6.60886950e+07
 5.35181277e+07 4.61899994e+07]
Done!
ene_coms = [0.00713924 0.00777925 0.00727359 0.00671109 0.00799094 0.00828278
 0.00666806 0.00643827 0.00983425 0.00830784]
ene_comp = [1.70806981e-05 1.65408836e-04 1.64167924e-05 6.61200306e-07
 2.58123551e-04 2.85654321e-05 1.30606552e-06 4.69092656e-05
 2.23406819e-05 1.51052936e-05]
ene_total = [0.00715633 0.00794465 0.00729    0.00671175 0.00824907 0.00831134
 0.00666937 0.00648518 0.00985659 0.00832295]
At round 15 energy consumption: 0.07699723536707746
At round 15 eta: 0.6899137178400166
At round 15 a_n: 23.044415151714617
At round 15 local rounds: 12.154605075633594
At round 15 global rounds: 74.31613869273092
gradient difference: 0.46718040108680725
train() client id: f_00000-0-0 loss: 1.201526  [   32/  126]
train() client id: f_00000-0-1 loss: 1.195368  [   64/  126]
train() client id: f_00000-0-2 loss: 1.212761  [   96/  126]
train() client id: f_00000-1-0 loss: 1.166910  [   32/  126]
train() client id: f_00000-1-1 loss: 1.113200  [   64/  126]
train() client id: f_00000-1-2 loss: 1.119044  [   96/  126]
train() client id: f_00000-2-0 loss: 1.017024  [   32/  126]
train() client id: f_00000-2-1 loss: 0.949445  [   64/  126]
train() client id: f_00000-2-2 loss: 1.079279  [   96/  126]
train() client id: f_00000-3-0 loss: 1.077543  [   32/  126]
train() client id: f_00000-3-1 loss: 0.897919  [   64/  126]
train() client id: f_00000-3-2 loss: 1.053771  [   96/  126]
train() client id: f_00000-4-0 loss: 1.008616  [   32/  126]
train() client id: f_00000-4-1 loss: 0.945678  [   64/  126]
train() client id: f_00000-4-2 loss: 0.924710  [   96/  126]
train() client id: f_00000-5-0 loss: 0.919972  [   32/  126]
train() client id: f_00000-5-1 loss: 0.973546  [   64/  126]
train() client id: f_00000-5-2 loss: 0.887286  [   96/  126]
train() client id: f_00000-6-0 loss: 0.875522  [   32/  126]
train() client id: f_00000-6-1 loss: 0.817194  [   64/  126]
train() client id: f_00000-6-2 loss: 0.936805  [   96/  126]
train() client id: f_00000-7-0 loss: 0.879943  [   32/  126]
train() client id: f_00000-7-1 loss: 0.882457  [   64/  126]
train() client id: f_00000-7-2 loss: 0.791936  [   96/  126]
train() client id: f_00000-8-0 loss: 0.879153  [   32/  126]
train() client id: f_00000-8-1 loss: 0.927733  [   64/  126]
train() client id: f_00000-8-2 loss: 0.842515  [   96/  126]
train() client id: f_00000-9-0 loss: 0.929480  [   32/  126]
train() client id: f_00000-9-1 loss: 0.891179  [   64/  126]
train() client id: f_00000-9-2 loss: 0.744438  [   96/  126]
train() client id: f_00000-10-0 loss: 0.906116  [   32/  126]
train() client id: f_00000-10-1 loss: 0.877641  [   64/  126]
train() client id: f_00000-10-2 loss: 0.797918  [   96/  126]
train() client id: f_00000-11-0 loss: 0.836407  [   32/  126]
train() client id: f_00000-11-1 loss: 0.883499  [   64/  126]
train() client id: f_00000-11-2 loss: 0.907878  [   96/  126]
train() client id: f_00001-0-0 loss: 0.583722  [   32/  265]
train() client id: f_00001-0-1 loss: 0.513009  [   64/  265]
train() client id: f_00001-0-2 loss: 0.550730  [   96/  265]
train() client id: f_00001-0-3 loss: 0.585976  [  128/  265]
train() client id: f_00001-0-4 loss: 0.489751  [  160/  265]
train() client id: f_00001-0-5 loss: 0.525478  [  192/  265]
train() client id: f_00001-0-6 loss: 0.487728  [  224/  265]
train() client id: f_00001-0-7 loss: 0.614338  [  256/  265]
train() client id: f_00001-1-0 loss: 0.551142  [   32/  265]
train() client id: f_00001-1-1 loss: 0.508738  [   64/  265]
train() client id: f_00001-1-2 loss: 0.590580  [   96/  265]
train() client id: f_00001-1-3 loss: 0.613922  [  128/  265]
train() client id: f_00001-1-4 loss: 0.467470  [  160/  265]
train() client id: f_00001-1-5 loss: 0.547582  [  192/  265]
train() client id: f_00001-1-6 loss: 0.518917  [  224/  265]
train() client id: f_00001-1-7 loss: 0.452159  [  256/  265]
train() client id: f_00001-2-0 loss: 0.619291  [   32/  265]
train() client id: f_00001-2-1 loss: 0.503883  [   64/  265]
train() client id: f_00001-2-2 loss: 0.439345  [   96/  265]
train() client id: f_00001-2-3 loss: 0.526029  [  128/  265]
train() client id: f_00001-2-4 loss: 0.444313  [  160/  265]
train() client id: f_00001-2-5 loss: 0.436455  [  192/  265]
train() client id: f_00001-2-6 loss: 0.746254  [  224/  265]
train() client id: f_00001-2-7 loss: 0.477399  [  256/  265]
train() client id: f_00001-3-0 loss: 0.448075  [   32/  265]
train() client id: f_00001-3-1 loss: 0.604793  [   64/  265]
train() client id: f_00001-3-2 loss: 0.510104  [   96/  265]
train() client id: f_00001-3-3 loss: 0.559013  [  128/  265]
train() client id: f_00001-3-4 loss: 0.618141  [  160/  265]
train() client id: f_00001-3-5 loss: 0.480090  [  192/  265]
train() client id: f_00001-3-6 loss: 0.457303  [  224/  265]
train() client id: f_00001-3-7 loss: 0.474025  [  256/  265]
train() client id: f_00001-4-0 loss: 0.531164  [   32/  265]
train() client id: f_00001-4-1 loss: 0.527188  [   64/  265]
train() client id: f_00001-4-2 loss: 0.420962  [   96/  265]
train() client id: f_00001-4-3 loss: 0.480228  [  128/  265]
train() client id: f_00001-4-4 loss: 0.511852  [  160/  265]
train() client id: f_00001-4-5 loss: 0.600063  [  192/  265]
train() client id: f_00001-4-6 loss: 0.476695  [  224/  265]
train() client id: f_00001-4-7 loss: 0.568771  [  256/  265]
train() client id: f_00001-5-0 loss: 0.557042  [   32/  265]
train() client id: f_00001-5-1 loss: 0.476801  [   64/  265]
train() client id: f_00001-5-2 loss: 0.481129  [   96/  265]
train() client id: f_00001-5-3 loss: 0.544988  [  128/  265]
train() client id: f_00001-5-4 loss: 0.419574  [  160/  265]
train() client id: f_00001-5-5 loss: 0.476931  [  192/  265]
train() client id: f_00001-5-6 loss: 0.541985  [  224/  265]
train() client id: f_00001-5-7 loss: 0.540016  [  256/  265]
train() client id: f_00001-6-0 loss: 0.491573  [   32/  265]
train() client id: f_00001-6-1 loss: 0.403366  [   64/  265]
train() client id: f_00001-6-2 loss: 0.700781  [   96/  265]
train() client id: f_00001-6-3 loss: 0.453354  [  128/  265]
train() client id: f_00001-6-4 loss: 0.475872  [  160/  265]
train() client id: f_00001-6-5 loss: 0.481373  [  192/  265]
train() client id: f_00001-6-6 loss: 0.470738  [  224/  265]
train() client id: f_00001-6-7 loss: 0.589231  [  256/  265]
train() client id: f_00001-7-0 loss: 0.584917  [   32/  265]
train() client id: f_00001-7-1 loss: 0.477504  [   64/  265]
train() client id: f_00001-7-2 loss: 0.470317  [   96/  265]
train() client id: f_00001-7-3 loss: 0.517255  [  128/  265]
train() client id: f_00001-7-4 loss: 0.463284  [  160/  265]
train() client id: f_00001-7-5 loss: 0.488254  [  192/  265]
train() client id: f_00001-7-6 loss: 0.474331  [  224/  265]
train() client id: f_00001-7-7 loss: 0.577373  [  256/  265]
train() client id: f_00001-8-0 loss: 0.622570  [   32/  265]
train() client id: f_00001-8-1 loss: 0.411789  [   64/  265]
train() client id: f_00001-8-2 loss: 0.401229  [   96/  265]
train() client id: f_00001-8-3 loss: 0.503087  [  128/  265]
train() client id: f_00001-8-4 loss: 0.574167  [  160/  265]
train() client id: f_00001-8-5 loss: 0.609313  [  192/  265]
train() client id: f_00001-8-6 loss: 0.513084  [  224/  265]
train() client id: f_00001-8-7 loss: 0.416195  [  256/  265]
train() client id: f_00001-9-0 loss: 0.623925  [   32/  265]
train() client id: f_00001-9-1 loss: 0.480450  [   64/  265]
train() client id: f_00001-9-2 loss: 0.601778  [   96/  265]
train() client id: f_00001-9-3 loss: 0.514659  [  128/  265]
train() client id: f_00001-9-4 loss: 0.414963  [  160/  265]
train() client id: f_00001-9-5 loss: 0.417202  [  192/  265]
train() client id: f_00001-9-6 loss: 0.449458  [  224/  265]
train() client id: f_00001-9-7 loss: 0.483977  [  256/  265]
train() client id: f_00001-10-0 loss: 0.526828  [   32/  265]
train() client id: f_00001-10-1 loss: 0.628656  [   64/  265]
train() client id: f_00001-10-2 loss: 0.498153  [   96/  265]
train() client id: f_00001-10-3 loss: 0.418520  [  128/  265]
train() client id: f_00001-10-4 loss: 0.531128  [  160/  265]
train() client id: f_00001-10-5 loss: 0.566484  [  192/  265]
train() client id: f_00001-10-6 loss: 0.465309  [  224/  265]
train() client id: f_00001-10-7 loss: 0.409729  [  256/  265]
train() client id: f_00001-11-0 loss: 0.453783  [   32/  265]
train() client id: f_00001-11-1 loss: 0.607489  [   64/  265]
train() client id: f_00001-11-2 loss: 0.452554  [   96/  265]
train() client id: f_00001-11-3 loss: 0.459927  [  128/  265]
train() client id: f_00001-11-4 loss: 0.651123  [  160/  265]
train() client id: f_00001-11-5 loss: 0.416280  [  192/  265]
train() client id: f_00001-11-6 loss: 0.428913  [  224/  265]
train() client id: f_00001-11-7 loss: 0.418604  [  256/  265]
train() client id: f_00002-0-0 loss: 1.216790  [   32/  124]
train() client id: f_00002-0-1 loss: 1.068512  [   64/  124]
train() client id: f_00002-0-2 loss: 1.155714  [   96/  124]
train() client id: f_00002-1-0 loss: 1.202802  [   32/  124]
train() client id: f_00002-1-1 loss: 1.096262  [   64/  124]
train() client id: f_00002-1-2 loss: 1.062976  [   96/  124]
train() client id: f_00002-2-0 loss: 1.164473  [   32/  124]
train() client id: f_00002-2-1 loss: 1.102914  [   64/  124]
train() client id: f_00002-2-2 loss: 1.098803  [   96/  124]
train() client id: f_00002-3-0 loss: 1.016637  [   32/  124]
train() client id: f_00002-3-1 loss: 1.063410  [   64/  124]
train() client id: f_00002-3-2 loss: 1.156719  [   96/  124]
train() client id: f_00002-4-0 loss: 0.994453  [   32/  124]
train() client id: f_00002-4-1 loss: 1.140106  [   64/  124]
train() client id: f_00002-4-2 loss: 1.070281  [   96/  124]
train() client id: f_00002-5-0 loss: 0.942901  [   32/  124]
train() client id: f_00002-5-1 loss: 1.074052  [   64/  124]
train() client id: f_00002-5-2 loss: 1.098933  [   96/  124]
train() client id: f_00002-6-0 loss: 0.993954  [   32/  124]
train() client id: f_00002-6-1 loss: 0.991812  [   64/  124]
train() client id: f_00002-6-2 loss: 1.065908  [   96/  124]
train() client id: f_00002-7-0 loss: 0.956826  [   32/  124]
train() client id: f_00002-7-1 loss: 0.975476  [   64/  124]
train() client id: f_00002-7-2 loss: 1.006565  [   96/  124]
train() client id: f_00002-8-0 loss: 1.107173  [   32/  124]
train() client id: f_00002-8-1 loss: 0.876958  [   64/  124]
train() client id: f_00002-8-2 loss: 1.000448  [   96/  124]
train() client id: f_00002-9-0 loss: 0.888088  [   32/  124]
train() client id: f_00002-9-1 loss: 1.120736  [   64/  124]
train() client id: f_00002-9-2 loss: 0.939346  [   96/  124]
train() client id: f_00002-10-0 loss: 0.938673  [   32/  124]
train() client id: f_00002-10-1 loss: 0.978528  [   64/  124]
train() client id: f_00002-10-2 loss: 0.996406  [   96/  124]
train() client id: f_00002-11-0 loss: 0.871507  [   32/  124]
train() client id: f_00002-11-1 loss: 1.105395  [   64/  124]
train() client id: f_00002-11-2 loss: 0.980473  [   96/  124]
train() client id: f_00003-0-0 loss: 1.053046  [   32/   43]
train() client id: f_00003-1-0 loss: 0.969016  [   32/   43]
train() client id: f_00003-2-0 loss: 0.884949  [   32/   43]
train() client id: f_00003-3-0 loss: 0.994275  [   32/   43]
train() client id: f_00003-4-0 loss: 1.017535  [   32/   43]
train() client id: f_00003-5-0 loss: 1.020568  [   32/   43]
train() client id: f_00003-6-0 loss: 1.042204  [   32/   43]
train() client id: f_00003-7-0 loss: 1.020044  [   32/   43]
train() client id: f_00003-8-0 loss: 0.888941  [   32/   43]
train() client id: f_00003-9-0 loss: 0.960706  [   32/   43]
train() client id: f_00003-10-0 loss: 1.062530  [   32/   43]
train() client id: f_00003-11-0 loss: 0.973777  [   32/   43]
train() client id: f_00004-0-0 loss: 0.912197  [   32/  306]
train() client id: f_00004-0-1 loss: 0.995702  [   64/  306]
train() client id: f_00004-0-2 loss: 0.789559  [   96/  306]
train() client id: f_00004-0-3 loss: 0.945262  [  128/  306]
train() client id: f_00004-0-4 loss: 0.920631  [  160/  306]
train() client id: f_00004-0-5 loss: 0.942353  [  192/  306]
train() client id: f_00004-0-6 loss: 0.870338  [  224/  306]
train() client id: f_00004-0-7 loss: 0.904371  [  256/  306]
train() client id: f_00004-0-8 loss: 0.878024  [  288/  306]
train() client id: f_00004-1-0 loss: 0.929584  [   32/  306]
train() client id: f_00004-1-1 loss: 0.942656  [   64/  306]
train() client id: f_00004-1-2 loss: 1.001490  [   96/  306]
train() client id: f_00004-1-3 loss: 0.870405  [  128/  306]
train() client id: f_00004-1-4 loss: 0.781325  [  160/  306]
train() client id: f_00004-1-5 loss: 0.910498  [  192/  306]
train() client id: f_00004-1-6 loss: 0.986095  [  224/  306]
train() client id: f_00004-1-7 loss: 0.829498  [  256/  306]
train() client id: f_00004-1-8 loss: 0.945996  [  288/  306]
train() client id: f_00004-2-0 loss: 0.931334  [   32/  306]
train() client id: f_00004-2-1 loss: 0.925109  [   64/  306]
train() client id: f_00004-2-2 loss: 0.875278  [   96/  306]
train() client id: f_00004-2-3 loss: 0.923066  [  128/  306]
train() client id: f_00004-2-4 loss: 0.844343  [  160/  306]
train() client id: f_00004-2-5 loss: 0.876342  [  192/  306]
train() client id: f_00004-2-6 loss: 0.963644  [  224/  306]
train() client id: f_00004-2-7 loss: 0.841005  [  256/  306]
train() client id: f_00004-2-8 loss: 0.928374  [  288/  306]
train() client id: f_00004-3-0 loss: 0.839776  [   32/  306]
train() client id: f_00004-3-1 loss: 0.908351  [   64/  306]
train() client id: f_00004-3-2 loss: 0.856003  [   96/  306]
train() client id: f_00004-3-3 loss: 0.969235  [  128/  306]
train() client id: f_00004-3-4 loss: 0.901415  [  160/  306]
train() client id: f_00004-3-5 loss: 0.906426  [  192/  306]
train() client id: f_00004-3-6 loss: 0.836711  [  224/  306]
train() client id: f_00004-3-7 loss: 0.888956  [  256/  306]
train() client id: f_00004-3-8 loss: 0.983574  [  288/  306]
train() client id: f_00004-4-0 loss: 0.877538  [   32/  306]
train() client id: f_00004-4-1 loss: 0.863808  [   64/  306]
train() client id: f_00004-4-2 loss: 0.855170  [   96/  306]
train() client id: f_00004-4-3 loss: 0.834373  [  128/  306]
train() client id: f_00004-4-4 loss: 0.989092  [  160/  306]
train() client id: f_00004-4-5 loss: 0.776276  [  192/  306]
train() client id: f_00004-4-6 loss: 0.900014  [  224/  306]
train() client id: f_00004-4-7 loss: 1.047460  [  256/  306]
train() client id: f_00004-4-8 loss: 0.941841  [  288/  306]
train() client id: f_00004-5-0 loss: 0.857809  [   32/  306]
train() client id: f_00004-5-1 loss: 0.857671  [   64/  306]
train() client id: f_00004-5-2 loss: 0.945940  [   96/  306]
train() client id: f_00004-5-3 loss: 0.901050  [  128/  306]
train() client id: f_00004-5-4 loss: 0.881178  [  160/  306]
train() client id: f_00004-5-5 loss: 0.883629  [  192/  306]
train() client id: f_00004-5-6 loss: 0.838544  [  224/  306]
train() client id: f_00004-5-7 loss: 0.998058  [  256/  306]
train() client id: f_00004-5-8 loss: 0.949280  [  288/  306]
train() client id: f_00004-6-0 loss: 0.942898  [   32/  306]
train() client id: f_00004-6-1 loss: 0.841217  [   64/  306]
train() client id: f_00004-6-2 loss: 0.873471  [   96/  306]
train() client id: f_00004-6-3 loss: 0.903075  [  128/  306]
train() client id: f_00004-6-4 loss: 0.891829  [  160/  306]
train() client id: f_00004-6-5 loss: 0.889262  [  192/  306]
train() client id: f_00004-6-6 loss: 0.950502  [  224/  306]
train() client id: f_00004-6-7 loss: 0.747384  [  256/  306]
train() client id: f_00004-6-8 loss: 0.956369  [  288/  306]
train() client id: f_00004-7-0 loss: 0.865366  [   32/  306]
train() client id: f_00004-7-1 loss: 0.893326  [   64/  306]
train() client id: f_00004-7-2 loss: 0.790349  [   96/  306]
train() client id: f_00004-7-3 loss: 0.903924  [  128/  306]
train() client id: f_00004-7-4 loss: 0.921285  [  160/  306]
train() client id: f_00004-7-5 loss: 0.942297  [  192/  306]
train() client id: f_00004-7-6 loss: 1.005553  [  224/  306]
train() client id: f_00004-7-7 loss: 0.822491  [  256/  306]
train() client id: f_00004-7-8 loss: 0.918746  [  288/  306]
train() client id: f_00004-8-0 loss: 0.904445  [   32/  306]
train() client id: f_00004-8-1 loss: 0.902520  [   64/  306]
train() client id: f_00004-8-2 loss: 1.003796  [   96/  306]
train() client id: f_00004-8-3 loss: 0.849592  [  128/  306]
train() client id: f_00004-8-4 loss: 0.887578  [  160/  306]
train() client id: f_00004-8-5 loss: 0.877915  [  192/  306]
train() client id: f_00004-8-6 loss: 0.991920  [  224/  306]
train() client id: f_00004-8-7 loss: 0.834733  [  256/  306]
train() client id: f_00004-8-8 loss: 0.879346  [  288/  306]
train() client id: f_00004-9-0 loss: 1.036604  [   32/  306]
train() client id: f_00004-9-1 loss: 1.003229  [   64/  306]
train() client id: f_00004-9-2 loss: 0.958886  [   96/  306]
train() client id: f_00004-9-3 loss: 0.853525  [  128/  306]
train() client id: f_00004-9-4 loss: 0.868484  [  160/  306]
train() client id: f_00004-9-5 loss: 0.854181  [  192/  306]
train() client id: f_00004-9-6 loss: 0.858760  [  224/  306]
train() client id: f_00004-9-7 loss: 0.872312  [  256/  306]
train() client id: f_00004-9-8 loss: 0.819875  [  288/  306]
train() client id: f_00004-10-0 loss: 1.007349  [   32/  306]
train() client id: f_00004-10-1 loss: 0.865643  [   64/  306]
train() client id: f_00004-10-2 loss: 0.822588  [   96/  306]
train() client id: f_00004-10-3 loss: 0.761139  [  128/  306]
train() client id: f_00004-10-4 loss: 0.965407  [  160/  306]
train() client id: f_00004-10-5 loss: 0.926282  [  192/  306]
train() client id: f_00004-10-6 loss: 0.957715  [  224/  306]
train() client id: f_00004-10-7 loss: 0.936533  [  256/  306]
train() client id: f_00004-10-8 loss: 0.869135  [  288/  306]
train() client id: f_00004-11-0 loss: 0.875600  [   32/  306]
train() client id: f_00004-11-1 loss: 0.962055  [   64/  306]
train() client id: f_00004-11-2 loss: 0.885041  [   96/  306]
train() client id: f_00004-11-3 loss: 0.799390  [  128/  306]
train() client id: f_00004-11-4 loss: 0.993540  [  160/  306]
train() client id: f_00004-11-5 loss: 0.890095  [  192/  306]
train() client id: f_00004-11-6 loss: 0.924553  [  224/  306]
train() client id: f_00004-11-7 loss: 0.834435  [  256/  306]
train() client id: f_00004-11-8 loss: 0.898555  [  288/  306]
train() client id: f_00005-0-0 loss: 0.871335  [   32/  146]
train() client id: f_00005-0-1 loss: 0.711741  [   64/  146]
train() client id: f_00005-0-2 loss: 0.857686  [   96/  146]
train() client id: f_00005-0-3 loss: 0.926506  [  128/  146]
train() client id: f_00005-1-0 loss: 1.121134  [   32/  146]
train() client id: f_00005-1-1 loss: 0.710340  [   64/  146]
train() client id: f_00005-1-2 loss: 0.615550  [   96/  146]
train() client id: f_00005-1-3 loss: 0.764907  [  128/  146]
train() client id: f_00005-2-0 loss: 0.946002  [   32/  146]
train() client id: f_00005-2-1 loss: 0.922912  [   64/  146]
train() client id: f_00005-2-2 loss: 0.869180  [   96/  146]
train() client id: f_00005-2-3 loss: 0.721488  [  128/  146]
train() client id: f_00005-3-0 loss: 0.981431  [   32/  146]
train() client id: f_00005-3-1 loss: 0.867007  [   64/  146]
train() client id: f_00005-3-2 loss: 0.789263  [   96/  146]
train() client id: f_00005-3-3 loss: 0.708815  [  128/  146]
train() client id: f_00005-4-0 loss: 0.873037  [   32/  146]
train() client id: f_00005-4-1 loss: 1.009328  [   64/  146]
train() client id: f_00005-4-2 loss: 0.761204  [   96/  146]
train() client id: f_00005-4-3 loss: 0.742551  [  128/  146]
train() client id: f_00005-5-0 loss: 0.843653  [   32/  146]
train() client id: f_00005-5-1 loss: 0.920391  [   64/  146]
train() client id: f_00005-5-2 loss: 0.819157  [   96/  146]
train() client id: f_00005-5-3 loss: 0.716525  [  128/  146]
train() client id: f_00005-6-0 loss: 0.870856  [   32/  146]
train() client id: f_00005-6-1 loss: 0.763197  [   64/  146]
train() client id: f_00005-6-2 loss: 0.758950  [   96/  146]
train() client id: f_00005-6-3 loss: 0.869811  [  128/  146]
train() client id: f_00005-7-0 loss: 0.788925  [   32/  146]
train() client id: f_00005-7-1 loss: 0.781564  [   64/  146]
train() client id: f_00005-7-2 loss: 0.812488  [   96/  146]
train() client id: f_00005-7-3 loss: 0.820516  [  128/  146]
train() client id: f_00005-8-0 loss: 0.616247  [   32/  146]
train() client id: f_00005-8-1 loss: 0.862044  [   64/  146]
train() client id: f_00005-8-2 loss: 0.867095  [   96/  146]
train() client id: f_00005-8-3 loss: 1.145993  [  128/  146]
train() client id: f_00005-9-0 loss: 1.081986  [   32/  146]
train() client id: f_00005-9-1 loss: 0.688993  [   64/  146]
train() client id: f_00005-9-2 loss: 0.777491  [   96/  146]
train() client id: f_00005-9-3 loss: 0.877180  [  128/  146]
train() client id: f_00005-10-0 loss: 0.897575  [   32/  146]
train() client id: f_00005-10-1 loss: 0.749051  [   64/  146]
train() client id: f_00005-10-2 loss: 0.844365  [   96/  146]
train() client id: f_00005-10-3 loss: 0.753565  [  128/  146]
train() client id: f_00005-11-0 loss: 0.643511  [   32/  146]
train() client id: f_00005-11-1 loss: 0.824803  [   64/  146]
train() client id: f_00005-11-2 loss: 0.847116  [   96/  146]
train() client id: f_00005-11-3 loss: 0.886670  [  128/  146]
train() client id: f_00006-0-0 loss: 0.683816  [   32/   54]
train() client id: f_00006-1-0 loss: 0.710826  [   32/   54]
train() client id: f_00006-2-0 loss: 0.715412  [   32/   54]
train() client id: f_00006-3-0 loss: 0.682465  [   32/   54]
train() client id: f_00006-4-0 loss: 0.673605  [   32/   54]
train() client id: f_00006-5-0 loss: 0.641477  [   32/   54]
train() client id: f_00006-6-0 loss: 0.672699  [   32/   54]
train() client id: f_00006-7-0 loss: 0.681188  [   32/   54]
train() client id: f_00006-8-0 loss: 0.688123  [   32/   54]
train() client id: f_00006-9-0 loss: 0.677977  [   32/   54]
train() client id: f_00006-10-0 loss: 0.687652  [   32/   54]
train() client id: f_00006-11-0 loss: 0.691131  [   32/   54]
train() client id: f_00007-0-0 loss: 0.704639  [   32/  179]
train() client id: f_00007-0-1 loss: 0.743776  [   64/  179]
train() client id: f_00007-0-2 loss: 0.728248  [   96/  179]
train() client id: f_00007-0-3 loss: 0.837551  [  128/  179]
train() client id: f_00007-0-4 loss: 0.706288  [  160/  179]
train() client id: f_00007-1-0 loss: 0.618738  [   32/  179]
train() client id: f_00007-1-1 loss: 0.707203  [   64/  179]
train() client id: f_00007-1-2 loss: 0.772278  [   96/  179]
train() client id: f_00007-1-3 loss: 0.848852  [  128/  179]
train() client id: f_00007-1-4 loss: 0.775231  [  160/  179]
train() client id: f_00007-2-0 loss: 0.725035  [   32/  179]
train() client id: f_00007-2-1 loss: 0.677160  [   64/  179]
train() client id: f_00007-2-2 loss: 0.779873  [   96/  179]
train() client id: f_00007-2-3 loss: 0.608573  [  128/  179]
train() client id: f_00007-2-4 loss: 0.750532  [  160/  179]
train() client id: f_00007-3-0 loss: 0.697246  [   32/  179]
train() client id: f_00007-3-1 loss: 0.826537  [   64/  179]
train() client id: f_00007-3-2 loss: 0.683521  [   96/  179]
train() client id: f_00007-3-3 loss: 0.767233  [  128/  179]
train() client id: f_00007-3-4 loss: 0.608314  [  160/  179]
train() client id: f_00007-4-0 loss: 0.848570  [   32/  179]
train() client id: f_00007-4-1 loss: 0.616827  [   64/  179]
train() client id: f_00007-4-2 loss: 0.586872  [   96/  179]
train() client id: f_00007-4-3 loss: 0.774047  [  128/  179]
train() client id: f_00007-4-4 loss: 0.729673  [  160/  179]
train() client id: f_00007-5-0 loss: 0.605525  [   32/  179]
train() client id: f_00007-5-1 loss: 0.815453  [   64/  179]
train() client id: f_00007-5-2 loss: 0.718795  [   96/  179]
train() client id: f_00007-5-3 loss: 0.675994  [  128/  179]
train() client id: f_00007-5-4 loss: 0.695927  [  160/  179]
train() client id: f_00007-6-0 loss: 0.824726  [   32/  179]
train() client id: f_00007-6-1 loss: 0.657037  [   64/  179]
train() client id: f_00007-6-2 loss: 0.658927  [   96/  179]
train() client id: f_00007-6-3 loss: 0.591233  [  128/  179]
train() client id: f_00007-6-4 loss: 0.753277  [  160/  179]
train() client id: f_00007-7-0 loss: 0.689344  [   32/  179]
train() client id: f_00007-7-1 loss: 0.678564  [   64/  179]
train() client id: f_00007-7-2 loss: 0.667789  [   96/  179]
train() client id: f_00007-7-3 loss: 0.649783  [  128/  179]
train() client id: f_00007-7-4 loss: 0.593394  [  160/  179]
train() client id: f_00007-8-0 loss: 0.591004  [   32/  179]
train() client id: f_00007-8-1 loss: 0.660316  [   64/  179]
train() client id: f_00007-8-2 loss: 0.875217  [   96/  179]
train() client id: f_00007-8-3 loss: 0.816678  [  128/  179]
train() client id: f_00007-8-4 loss: 0.581866  [  160/  179]
train() client id: f_00007-9-0 loss: 0.805988  [   32/  179]
train() client id: f_00007-9-1 loss: 0.586764  [   64/  179]
train() client id: f_00007-9-2 loss: 0.747476  [   96/  179]
train() client id: f_00007-9-3 loss: 0.738387  [  128/  179]
train() client id: f_00007-9-4 loss: 0.604702  [  160/  179]
train() client id: f_00007-10-0 loss: 0.583703  [   32/  179]
train() client id: f_00007-10-1 loss: 0.813588  [   64/  179]
train() client id: f_00007-10-2 loss: 0.769954  [   96/  179]
train() client id: f_00007-10-3 loss: 0.576897  [  128/  179]
train() client id: f_00007-10-4 loss: 0.690848  [  160/  179]
train() client id: f_00007-11-0 loss: 0.676488  [   32/  179]
train() client id: f_00007-11-1 loss: 0.582476  [   64/  179]
train() client id: f_00007-11-2 loss: 0.799529  [   96/  179]
train() client id: f_00007-11-3 loss: 0.792634  [  128/  179]
train() client id: f_00007-11-4 loss: 0.675469  [  160/  179]
train() client id: f_00008-0-0 loss: 0.481731  [   32/  130]
train() client id: f_00008-0-1 loss: 0.695600  [   64/  130]
train() client id: f_00008-0-2 loss: 0.694627  [   96/  130]
train() client id: f_00008-0-3 loss: 0.538071  [  128/  130]
train() client id: f_00008-1-0 loss: 0.538091  [   32/  130]
train() client id: f_00008-1-1 loss: 0.553729  [   64/  130]
train() client id: f_00008-1-2 loss: 0.574620  [   96/  130]
train() client id: f_00008-1-3 loss: 0.690996  [  128/  130]
train() client id: f_00008-2-0 loss: 0.503109  [   32/  130]
train() client id: f_00008-2-1 loss: 0.426058  [   64/  130]
train() client id: f_00008-2-2 loss: 0.677718  [   96/  130]
train() client id: f_00008-2-3 loss: 0.787911  [  128/  130]
train() client id: f_00008-3-0 loss: 0.603892  [   32/  130]
train() client id: f_00008-3-1 loss: 0.542569  [   64/  130]
train() client id: f_00008-3-2 loss: 0.621203  [   96/  130]
train() client id: f_00008-3-3 loss: 0.582632  [  128/  130]
train() client id: f_00008-4-0 loss: 0.630686  [   32/  130]
train() client id: f_00008-4-1 loss: 0.604650  [   64/  130]
train() client id: f_00008-4-2 loss: 0.591321  [   96/  130]
train() client id: f_00008-4-3 loss: 0.542816  [  128/  130]
train() client id: f_00008-5-0 loss: 0.628630  [   32/  130]
train() client id: f_00008-5-1 loss: 0.534023  [   64/  130]
train() client id: f_00008-5-2 loss: 0.736241  [   96/  130]
train() client id: f_00008-5-3 loss: 0.480117  [  128/  130]
train() client id: f_00008-6-0 loss: 0.593164  [   32/  130]
train() client id: f_00008-6-1 loss: 0.633449  [   64/  130]
train() client id: f_00008-6-2 loss: 0.645651  [   96/  130]
train() client id: f_00008-6-3 loss: 0.500847  [  128/  130]
train() client id: f_00008-7-0 loss: 0.624281  [   32/  130]
train() client id: f_00008-7-1 loss: 0.672642  [   64/  130]
train() client id: f_00008-7-2 loss: 0.623300  [   96/  130]
train() client id: f_00008-7-3 loss: 0.454042  [  128/  130]
train() client id: f_00008-8-0 loss: 0.740703  [   32/  130]
train() client id: f_00008-8-1 loss: 0.521375  [   64/  130]
train() client id: f_00008-8-2 loss: 0.565865  [   96/  130]
train() client id: f_00008-8-3 loss: 0.543501  [  128/  130]
train() client id: f_00008-9-0 loss: 0.512193  [   32/  130]
train() client id: f_00008-9-1 loss: 0.657212  [   64/  130]
train() client id: f_00008-9-2 loss: 0.621610  [   96/  130]
train() client id: f_00008-9-3 loss: 0.526237  [  128/  130]
train() client id: f_00008-10-0 loss: 0.575736  [   32/  130]
train() client id: f_00008-10-1 loss: 0.522240  [   64/  130]
train() client id: f_00008-10-2 loss: 0.610604  [   96/  130]
train() client id: f_00008-10-3 loss: 0.650530  [  128/  130]
train() client id: f_00008-11-0 loss: 0.620222  [   32/  130]
train() client id: f_00008-11-1 loss: 0.536469  [   64/  130]
train() client id: f_00008-11-2 loss: 0.504318  [   96/  130]
train() client id: f_00008-11-3 loss: 0.700758  [  128/  130]
train() client id: f_00009-0-0 loss: 1.086976  [   32/  118]
train() client id: f_00009-0-1 loss: 0.944840  [   64/  118]
train() client id: f_00009-0-2 loss: 1.160112  [   96/  118]
train() client id: f_00009-1-0 loss: 0.971080  [   32/  118]
train() client id: f_00009-1-1 loss: 1.002844  [   64/  118]
train() client id: f_00009-1-2 loss: 1.017762  [   96/  118]
train() client id: f_00009-2-0 loss: 0.958280  [   32/  118]
train() client id: f_00009-2-1 loss: 1.022834  [   64/  118]
train() client id: f_00009-2-2 loss: 0.954914  [   96/  118]
train() client id: f_00009-3-0 loss: 1.007815  [   32/  118]
train() client id: f_00009-3-1 loss: 0.980449  [   64/  118]
train() client id: f_00009-3-2 loss: 0.928296  [   96/  118]
train() client id: f_00009-4-0 loss: 0.930844  [   32/  118]
train() client id: f_00009-4-1 loss: 0.825235  [   64/  118]
train() client id: f_00009-4-2 loss: 0.932483  [   96/  118]
train() client id: f_00009-5-0 loss: 0.919548  [   32/  118]
train() client id: f_00009-5-1 loss: 0.818098  [   64/  118]
train() client id: f_00009-5-2 loss: 0.983821  [   96/  118]
train() client id: f_00009-6-0 loss: 1.068486  [   32/  118]
train() client id: f_00009-6-1 loss: 0.965777  [   64/  118]
train() client id: f_00009-6-2 loss: 0.665755  [   96/  118]
train() client id: f_00009-7-0 loss: 0.965077  [   32/  118]
train() client id: f_00009-7-1 loss: 0.816500  [   64/  118]
train() client id: f_00009-7-2 loss: 0.854038  [   96/  118]
train() client id: f_00009-8-0 loss: 0.908819  [   32/  118]
train() client id: f_00009-8-1 loss: 0.924176  [   64/  118]
train() client id: f_00009-8-2 loss: 0.758004  [   96/  118]
train() client id: f_00009-9-0 loss: 0.862278  [   32/  118]
train() client id: f_00009-9-1 loss: 0.790803  [   64/  118]
train() client id: f_00009-9-2 loss: 0.917511  [   96/  118]
train() client id: f_00009-10-0 loss: 0.814343  [   32/  118]
train() client id: f_00009-10-1 loss: 0.796439  [   64/  118]
train() client id: f_00009-10-2 loss: 0.899703  [   96/  118]
train() client id: f_00009-11-0 loss: 0.873751  [   32/  118]
train() client id: f_00009-11-1 loss: 0.813887  [   64/  118]
train() client id: f_00009-11-2 loss: 0.874101  [   96/  118]
At round 15 accuracy: 0.636604774535809
At round 15 training accuracy: 0.5801475519785378
At round 15 training loss: 0.8558552207306558
update_location
xs = -3.905658 4.200318 95.009024 18.811294 0.979296 3.956410 -57.443192 -36.324852 79.663977 -22.060879 
ys = 87.587959 70.555839 1.320614 -57.455176 49.350187 32.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 132.992123 122.457213 137.943679 116.854448 111.518608 105.320566 115.354284 106.396293 129.054326 102.482653 
dists_bs = 192.130525 207.415855 321.044237 302.651989 216.232718 228.565554 213.002906 222.640183 299.396491 229.414185 
uav_gains = -103.096545 -102.199894 -103.494024 -101.691278 -101.183759 -100.562863 -101.550966 -100.673201 -102.769906 -100.266282 
bs_gains = -103.507757 -104.438634 -109.750876 -109.033478 -104.944859 -105.619361 -104.761854 -105.299959 -108.901967 -105.664427 
Round 16
-------------------------------
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.87981483 18.50415528  8.7446806   3.12925602 21.34431125 10.28544805
  3.88903844 12.52812053  9.21866598  8.34945337]
obj_prev = 104.87294435893298
eta_min = 3.443571724151839e-11	eta_max = 0.9213403601232922
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 24.38417167306138	eta = 0.9090909090909091
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 42.374357946194124	eta = 0.5231330896350052
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 33.76652681787571	eta = 0.6564912320788918
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 32.22189998010197	eta = 0.6879615667412922
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 32.14522467977385	eta = 0.6896025463975112
af = 22.167428793692164	bf = 1.7394327346185519	zeta = 32.14502216089842	eta = 0.6896068910058766
eta = 0.6896068910058766
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [0.03066631 0.06449661 0.03017954 0.01046549 0.07447533 0.03553398
 0.0131427  0.04356563 0.03163984 0.02871924]
ene_total = [2.771699   5.28190218 2.74625142 1.26115748 6.02653747 3.19933593
 1.45388682 3.66054961 3.04133893 2.70236334]
ti_comp = [0.32693877 0.3220322  0.32555044 0.33146051 0.32000855 0.31714645
 0.33188162 0.33440318 0.29980737 0.31694807]
ti_coms = [0.07229963 0.0772062  0.07368796 0.06777789 0.07922985 0.08209195
 0.06735678 0.06483522 0.09943103 0.08229033]
t_total = [29.19993286 29.19993286 29.19993286 29.19993286 29.19993286 29.19993286
 29.19993286 29.19993286 29.19993286 29.19993286]
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [1.68628817e-05 1.61693297e-04 1.62099429e-05 6.52070823e-07
 2.52112341e-04 2.78799409e-05 1.28815427e-06 4.62137600e-05
 2.20241139e-05 1.47374657e-05]
ene_total = [0.53002626 0.57650519 0.54013261 0.49576897 0.59791914 0.60245213
 0.49273557 0.47757895 0.72884031 0.60294189]
optimize_network iter = 0 obj = 5.644901027509804
eta = 0.6896068910058766
freqs = [4.68991664e+07 1.00139996e+08 4.63515646e+07 1.57869290e+07
 1.16364588e+08 5.60214049e+07 1.98002903e+07 6.51393823e+07
 5.27669565e+07 4.53059144e+07]
eta_min = 0.6896068910059152	eta_max = 0.6896068910058611
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 0.03760750116688597	eta = 0.909090909090909
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 19.169603857786115	eta = 0.0017834816868453646
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 1.9593134418751286	eta = 0.01744929458133157
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 1.9111928987478155	eta = 0.017888637743914634
af = 0.03418863742444179	bf = 1.7394327346185519	zeta = 1.9111822247051171	eta = 0.01788873765279858
eta = 0.01788873765279858
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [1.82570215e-04 1.75061300e-03 1.75501009e-04 7.05980817e-06
 2.72955744e-03 3.01849167e-04 1.39465251e-05 5.00344854e-04
 2.38449589e-04 1.59558866e-04]
ene_total = [0.17134348 0.21893109 0.17438925 0.15683431 0.24623749 0.19673596
 0.1560201  0.1614347  0.23535039 0.19390545]
ti_comp = [0.32693877 0.3220322  0.32555044 0.33146051 0.32000855 0.31714645
 0.33188162 0.33440318 0.29980737 0.31694807]
ti_coms = [0.07229963 0.0772062  0.07368796 0.06777789 0.07922985 0.08209195
 0.06735678 0.06483522 0.09943103 0.08229033]
t_total = [29.19993286 29.19993286 29.19993286 29.19993286 29.19993286 29.19993286
 29.19993286 29.19993286 29.19993286 29.19993286]
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [1.68628817e-05 1.61693297e-04 1.62099429e-05 6.52070823e-07
 2.52112341e-04 2.78799409e-05 1.28815427e-06 4.62137600e-05
 2.20241139e-05 1.47374657e-05]
ene_total = [0.53002626 0.57650519 0.54013261 0.49576897 0.59791914 0.60245213
 0.49273557 0.47757895 0.72884031 0.60294189]
optimize_network iter = 1 obj = 5.644901027510502
eta = 0.6896068910059152
freqs = [4.68991664e+07 1.00139996e+08 4.63515646e+07 1.57869290e+07
 1.16364588e+08 5.60214049e+07 1.98002903e+07 6.51393823e+07
 5.27669565e+07 4.53059144e+07]
Done!
ene_coms = [0.00722996 0.00772062 0.0073688  0.00677779 0.00792298 0.00820919
 0.00673568 0.00648352 0.0099431  0.00822903]
ene_comp = [1.66284604e-05 1.59445500e-04 1.59845985e-05 6.43005986e-07
 2.48607572e-04 2.74923648e-05 1.27024685e-06 4.55713142e-05
 2.17179432e-05 1.45325912e-05]
ene_total = [0.00724659 0.00788007 0.00738478 0.00677843 0.00817159 0.00823669
 0.00673695 0.00652909 0.00996482 0.00824357]
At round 16 energy consumption: 0.0771725767342164
At round 16 eta: 0.6896068910059152
At round 16 a_n: 22.701869304745298
At round 16 local rounds: 12.169171106060379
At round 16 global rounds: 73.13908926108901
gradient difference: 0.4523835778236389
train() client id: f_00000-0-0 loss: 1.469382  [   32/  126]
train() client id: f_00000-0-1 loss: 1.410147  [   64/  126]
train() client id: f_00000-0-2 loss: 1.364793  [   96/  126]
train() client id: f_00000-1-0 loss: 1.322264  [   32/  126]
train() client id: f_00000-1-1 loss: 1.132251  [   64/  126]
train() client id: f_00000-1-2 loss: 1.267133  [   96/  126]
train() client id: f_00000-2-0 loss: 1.246623  [   32/  126]
train() client id: f_00000-2-1 loss: 1.121792  [   64/  126]
train() client id: f_00000-2-2 loss: 1.113342  [   96/  126]
train() client id: f_00000-3-0 loss: 1.220020  [   32/  126]
train() client id: f_00000-3-1 loss: 0.979039  [   64/  126]
train() client id: f_00000-3-2 loss: 1.020356  [   96/  126]
train() client id: f_00000-4-0 loss: 0.970653  [   32/  126]
train() client id: f_00000-4-1 loss: 1.001867  [   64/  126]
train() client id: f_00000-4-2 loss: 0.981283  [   96/  126]
train() client id: f_00000-5-0 loss: 0.935544  [   32/  126]
train() client id: f_00000-5-1 loss: 0.974211  [   64/  126]
train() client id: f_00000-5-2 loss: 1.012366  [   96/  126]
train() client id: f_00000-6-0 loss: 0.918191  [   32/  126]
train() client id: f_00000-6-1 loss: 0.963206  [   64/  126]
train() client id: f_00000-6-2 loss: 0.948871  [   96/  126]
train() client id: f_00000-7-0 loss: 0.794901  [   32/  126]
train() client id: f_00000-7-1 loss: 1.016131  [   64/  126]
train() client id: f_00000-7-2 loss: 0.849501  [   96/  126]
train() client id: f_00000-8-0 loss: 0.869635  [   32/  126]
train() client id: f_00000-8-1 loss: 0.932219  [   64/  126]
train() client id: f_00000-8-2 loss: 0.881782  [   96/  126]
train() client id: f_00000-9-0 loss: 0.847658  [   32/  126]
train() client id: f_00000-9-1 loss: 0.892427  [   64/  126]
train() client id: f_00000-9-2 loss: 0.828596  [   96/  126]
train() client id: f_00000-10-0 loss: 0.911410  [   32/  126]
train() client id: f_00000-10-1 loss: 0.827439  [   64/  126]
train() client id: f_00000-10-2 loss: 0.844275  [   96/  126]
train() client id: f_00000-11-0 loss: 0.776980  [   32/  126]
train() client id: f_00000-11-1 loss: 0.898123  [   64/  126]
train() client id: f_00000-11-2 loss: 0.887872  [   96/  126]
train() client id: f_00001-0-0 loss: 0.555254  [   32/  265]
train() client id: f_00001-0-1 loss: 0.518256  [   64/  265]
train() client id: f_00001-0-2 loss: 0.488140  [   96/  265]
train() client id: f_00001-0-3 loss: 0.587880  [  128/  265]
train() client id: f_00001-0-4 loss: 0.582574  [  160/  265]
train() client id: f_00001-0-5 loss: 0.642842  [  192/  265]
train() client id: f_00001-0-6 loss: 0.602835  [  224/  265]
train() client id: f_00001-0-7 loss: 0.542746  [  256/  265]
train() client id: f_00001-1-0 loss: 0.596000  [   32/  265]
train() client id: f_00001-1-1 loss: 0.610696  [   64/  265]
train() client id: f_00001-1-2 loss: 0.513861  [   96/  265]
train() client id: f_00001-1-3 loss: 0.477412  [  128/  265]
train() client id: f_00001-1-4 loss: 0.641098  [  160/  265]
train() client id: f_00001-1-5 loss: 0.480543  [  192/  265]
train() client id: f_00001-1-6 loss: 0.472214  [  224/  265]
train() client id: f_00001-1-7 loss: 0.588040  [  256/  265]
train() client id: f_00001-2-0 loss: 0.497242  [   32/  265]
train() client id: f_00001-2-1 loss: 0.479882  [   64/  265]
train() client id: f_00001-2-2 loss: 0.708624  [   96/  265]
train() client id: f_00001-2-3 loss: 0.523894  [  128/  265]
train() client id: f_00001-2-4 loss: 0.584457  [  160/  265]
train() client id: f_00001-2-5 loss: 0.475683  [  192/  265]
train() client id: f_00001-2-6 loss: 0.531820  [  224/  265]
train() client id: f_00001-2-7 loss: 0.594656  [  256/  265]
train() client id: f_00001-3-0 loss: 0.526084  [   32/  265]
train() client id: f_00001-3-1 loss: 0.583819  [   64/  265]
train() client id: f_00001-3-2 loss: 0.637926  [   96/  265]
train() client id: f_00001-3-3 loss: 0.611891  [  128/  265]
train() client id: f_00001-3-4 loss: 0.453650  [  160/  265]
train() client id: f_00001-3-5 loss: 0.530567  [  192/  265]
train() client id: f_00001-3-6 loss: 0.551773  [  224/  265]
train() client id: f_00001-3-7 loss: 0.469438  [  256/  265]
train() client id: f_00001-4-0 loss: 0.505422  [   32/  265]
train() client id: f_00001-4-1 loss: 0.706504  [   64/  265]
train() client id: f_00001-4-2 loss: 0.582754  [   96/  265]
train() client id: f_00001-4-3 loss: 0.537563  [  128/  265]
train() client id: f_00001-4-4 loss: 0.450131  [  160/  265]
train() client id: f_00001-4-5 loss: 0.524017  [  192/  265]
train() client id: f_00001-4-6 loss: 0.462865  [  224/  265]
train() client id: f_00001-4-7 loss: 0.578312  [  256/  265]
train() client id: f_00001-5-0 loss: 0.623814  [   32/  265]
train() client id: f_00001-5-1 loss: 0.508915  [   64/  265]
train() client id: f_00001-5-2 loss: 0.520907  [   96/  265]
train() client id: f_00001-5-3 loss: 0.446352  [  128/  265]
train() client id: f_00001-5-4 loss: 0.530605  [  160/  265]
train() client id: f_00001-5-5 loss: 0.601174  [  192/  265]
train() client id: f_00001-5-6 loss: 0.507839  [  224/  265]
train() client id: f_00001-5-7 loss: 0.591885  [  256/  265]
train() client id: f_00001-6-0 loss: 0.563792  [   32/  265]
train() client id: f_00001-6-1 loss: 0.539209  [   64/  265]
train() client id: f_00001-6-2 loss: 0.609869  [   96/  265]
train() client id: f_00001-6-3 loss: 0.444116  [  128/  265]
train() client id: f_00001-6-4 loss: 0.503579  [  160/  265]
train() client id: f_00001-6-5 loss: 0.567023  [  192/  265]
train() client id: f_00001-6-6 loss: 0.649036  [  224/  265]
train() client id: f_00001-6-7 loss: 0.448320  [  256/  265]
train() client id: f_00001-7-0 loss: 0.445204  [   32/  265]
train() client id: f_00001-7-1 loss: 0.504707  [   64/  265]
train() client id: f_00001-7-2 loss: 0.594979  [   96/  265]
train() client id: f_00001-7-3 loss: 0.521672  [  128/  265]
train() client id: f_00001-7-4 loss: 0.632475  [  160/  265]
train() client id: f_00001-7-5 loss: 0.569134  [  192/  265]
train() client id: f_00001-7-6 loss: 0.516935  [  224/  265]
train() client id: f_00001-7-7 loss: 0.533193  [  256/  265]
train() client id: f_00001-8-0 loss: 0.549338  [   32/  265]
train() client id: f_00001-8-1 loss: 0.603855  [   64/  265]
train() client id: f_00001-8-2 loss: 0.499873  [   96/  265]
train() client id: f_00001-8-3 loss: 0.565611  [  128/  265]
train() client id: f_00001-8-4 loss: 0.530682  [  160/  265]
train() client id: f_00001-8-5 loss: 0.526845  [  192/  265]
train() client id: f_00001-8-6 loss: 0.506992  [  224/  265]
train() client id: f_00001-8-7 loss: 0.533731  [  256/  265]
train() client id: f_00001-9-0 loss: 0.517206  [   32/  265]
train() client id: f_00001-9-1 loss: 0.515631  [   64/  265]
train() client id: f_00001-9-2 loss: 0.500523  [   96/  265]
train() client id: f_00001-9-3 loss: 0.583315  [  128/  265]
train() client id: f_00001-9-4 loss: 0.491429  [  160/  265]
train() client id: f_00001-9-5 loss: 0.510391  [  192/  265]
train() client id: f_00001-9-6 loss: 0.501604  [  224/  265]
train() client id: f_00001-9-7 loss: 0.700431  [  256/  265]
train() client id: f_00001-10-0 loss: 0.596833  [   32/  265]
train() client id: f_00001-10-1 loss: 0.585865  [   64/  265]
train() client id: f_00001-10-2 loss: 0.581046  [   96/  265]
train() client id: f_00001-10-3 loss: 0.470004  [  128/  265]
train() client id: f_00001-10-4 loss: 0.456086  [  160/  265]
train() client id: f_00001-10-5 loss: 0.477417  [  192/  265]
train() client id: f_00001-10-6 loss: 0.501448  [  224/  265]
train() client id: f_00001-10-7 loss: 0.591798  [  256/  265]
train() client id: f_00001-11-0 loss: 0.487364  [   32/  265]
train() client id: f_00001-11-1 loss: 0.542382  [   64/  265]
train() client id: f_00001-11-2 loss: 0.438023  [   96/  265]
train() client id: f_00001-11-3 loss: 0.542703  [  128/  265]
train() client id: f_00001-11-4 loss: 0.582640  [  160/  265]
train() client id: f_00001-11-5 loss: 0.611265  [  192/  265]
train() client id: f_00001-11-6 loss: 0.544775  [  224/  265]
train() client id: f_00001-11-7 loss: 0.579704  [  256/  265]
train() client id: f_00002-0-0 loss: 1.199535  [   32/  124]
train() client id: f_00002-0-1 loss: 1.320635  [   64/  124]
train() client id: f_00002-0-2 loss: 1.173644  [   96/  124]
train() client id: f_00002-1-0 loss: 1.095940  [   32/  124]
train() client id: f_00002-1-1 loss: 1.271157  [   64/  124]
train() client id: f_00002-1-2 loss: 1.088257  [   96/  124]
train() client id: f_00002-2-0 loss: 1.189503  [   32/  124]
train() client id: f_00002-2-1 loss: 1.149273  [   64/  124]
train() client id: f_00002-2-2 loss: 1.172645  [   96/  124]
train() client id: f_00002-3-0 loss: 1.116234  [   32/  124]
train() client id: f_00002-3-1 loss: 1.163773  [   64/  124]
train() client id: f_00002-3-2 loss: 1.050602  [   96/  124]
train() client id: f_00002-4-0 loss: 1.086486  [   32/  124]
train() client id: f_00002-4-1 loss: 1.079935  [   64/  124]
train() client id: f_00002-4-2 loss: 1.007813  [   96/  124]
train() client id: f_00002-5-0 loss: 1.107930  [   32/  124]
train() client id: f_00002-5-1 loss: 1.121963  [   64/  124]
train() client id: f_00002-5-2 loss: 0.921999  [   96/  124]
train() client id: f_00002-6-0 loss: 1.022663  [   32/  124]
train() client id: f_00002-6-1 loss: 0.993474  [   64/  124]
train() client id: f_00002-6-2 loss: 0.945251  [   96/  124]
train() client id: f_00002-7-0 loss: 1.052763  [   32/  124]
train() client id: f_00002-7-1 loss: 1.153225  [   64/  124]
train() client id: f_00002-7-2 loss: 0.908981  [   96/  124]
train() client id: f_00002-8-0 loss: 0.983276  [   32/  124]
train() client id: f_00002-8-1 loss: 0.967415  [   64/  124]
train() client id: f_00002-8-2 loss: 1.024501  [   96/  124]
train() client id: f_00002-9-0 loss: 1.082037  [   32/  124]
train() client id: f_00002-9-1 loss: 0.979593  [   64/  124]
train() client id: f_00002-9-2 loss: 1.008985  [   96/  124]
train() client id: f_00002-10-0 loss: 0.987899  [   32/  124]
train() client id: f_00002-10-1 loss: 1.088453  [   64/  124]
train() client id: f_00002-10-2 loss: 0.962356  [   96/  124]
train() client id: f_00002-11-0 loss: 1.043530  [   32/  124]
train() client id: f_00002-11-1 loss: 0.878313  [   64/  124]
train() client id: f_00002-11-2 loss: 1.088469  [   96/  124]
train() client id: f_00003-0-0 loss: 0.816694  [   32/   43]
train() client id: f_00003-1-0 loss: 0.822395  [   32/   43]
train() client id: f_00003-2-0 loss: 0.940382  [   32/   43]
train() client id: f_00003-3-0 loss: 0.900511  [   32/   43]
train() client id: f_00003-4-0 loss: 0.904948  [   32/   43]
train() client id: f_00003-5-0 loss: 0.886585  [   32/   43]
train() client id: f_00003-6-0 loss: 0.920947  [   32/   43]
train() client id: f_00003-7-0 loss: 0.873410  [   32/   43]
train() client id: f_00003-8-0 loss: 0.804961  [   32/   43]
train() client id: f_00003-9-0 loss: 0.821550  [   32/   43]
train() client id: f_00003-10-0 loss: 0.907627  [   32/   43]
train() client id: f_00003-11-0 loss: 0.854561  [   32/   43]
train() client id: f_00004-0-0 loss: 0.940347  [   32/  306]
train() client id: f_00004-0-1 loss: 0.854243  [   64/  306]
train() client id: f_00004-0-2 loss: 0.830423  [   96/  306]
train() client id: f_00004-0-3 loss: 0.977834  [  128/  306]
train() client id: f_00004-0-4 loss: 0.941775  [  160/  306]
train() client id: f_00004-0-5 loss: 0.768088  [  192/  306]
train() client id: f_00004-0-6 loss: 0.941469  [  224/  306]
train() client id: f_00004-0-7 loss: 0.770631  [  256/  306]
train() client id: f_00004-0-8 loss: 0.881570  [  288/  306]
train() client id: f_00004-1-0 loss: 0.858387  [   32/  306]
train() client id: f_00004-1-1 loss: 0.876773  [   64/  306]
train() client id: f_00004-1-2 loss: 0.900331  [   96/  306]
train() client id: f_00004-1-3 loss: 0.862592  [  128/  306]
train() client id: f_00004-1-4 loss: 0.895615  [  160/  306]
train() client id: f_00004-1-5 loss: 0.855019  [  192/  306]
train() client id: f_00004-1-6 loss: 0.861677  [  224/  306]
train() client id: f_00004-1-7 loss: 0.930992  [  256/  306]
train() client id: f_00004-1-8 loss: 0.920482  [  288/  306]
train() client id: f_00004-2-0 loss: 0.914628  [   32/  306]
train() client id: f_00004-2-1 loss: 0.930697  [   64/  306]
train() client id: f_00004-2-2 loss: 0.899180  [   96/  306]
train() client id: f_00004-2-3 loss: 0.897368  [  128/  306]
train() client id: f_00004-2-4 loss: 0.863200  [  160/  306]
train() client id: f_00004-2-5 loss: 0.986535  [  192/  306]
train() client id: f_00004-2-6 loss: 0.794785  [  224/  306]
train() client id: f_00004-2-7 loss: 0.834322  [  256/  306]
train() client id: f_00004-2-8 loss: 0.844543  [  288/  306]
train() client id: f_00004-3-0 loss: 0.797005  [   32/  306]
train() client id: f_00004-3-1 loss: 0.898194  [   64/  306]
train() client id: f_00004-3-2 loss: 0.869078  [   96/  306]
train() client id: f_00004-3-3 loss: 0.787624  [  128/  306]
train() client id: f_00004-3-4 loss: 0.861848  [  160/  306]
train() client id: f_00004-3-5 loss: 0.876805  [  192/  306]
train() client id: f_00004-3-6 loss: 1.034612  [  224/  306]
train() client id: f_00004-3-7 loss: 0.946303  [  256/  306]
train() client id: f_00004-3-8 loss: 0.909957  [  288/  306]
train() client id: f_00004-4-0 loss: 0.837968  [   32/  306]
train() client id: f_00004-4-1 loss: 0.861552  [   64/  306]
train() client id: f_00004-4-2 loss: 0.960239  [   96/  306]
train() client id: f_00004-4-3 loss: 0.859715  [  128/  306]
train() client id: f_00004-4-4 loss: 0.885089  [  160/  306]
train() client id: f_00004-4-5 loss: 0.958583  [  192/  306]
train() client id: f_00004-4-6 loss: 0.839586  [  224/  306]
train() client id: f_00004-4-7 loss: 0.774577  [  256/  306]
train() client id: f_00004-4-8 loss: 0.984546  [  288/  306]
train() client id: f_00004-5-0 loss: 0.904545  [   32/  306]
train() client id: f_00004-5-1 loss: 0.824907  [   64/  306]
train() client id: f_00004-5-2 loss: 0.870969  [   96/  306]
train() client id: f_00004-5-3 loss: 0.921177  [  128/  306]
train() client id: f_00004-5-4 loss: 0.861415  [  160/  306]
train() client id: f_00004-5-5 loss: 0.885286  [  192/  306]
train() client id: f_00004-5-6 loss: 0.934618  [  224/  306]
train() client id: f_00004-5-7 loss: 0.924377  [  256/  306]
train() client id: f_00004-5-8 loss: 0.881595  [  288/  306]
train() client id: f_00004-6-0 loss: 0.855515  [   32/  306]
train() client id: f_00004-6-1 loss: 0.817978  [   64/  306]
train() client id: f_00004-6-2 loss: 0.864589  [   96/  306]
train() client id: f_00004-6-3 loss: 0.862460  [  128/  306]
train() client id: f_00004-6-4 loss: 0.902139  [  160/  306]
train() client id: f_00004-6-5 loss: 0.925163  [  192/  306]
train() client id: f_00004-6-6 loss: 0.921962  [  224/  306]
train() client id: f_00004-6-7 loss: 0.964955  [  256/  306]
train() client id: f_00004-6-8 loss: 0.878967  [  288/  306]
train() client id: f_00004-7-0 loss: 0.858103  [   32/  306]
train() client id: f_00004-7-1 loss: 0.886040  [   64/  306]
train() client id: f_00004-7-2 loss: 0.931211  [   96/  306]
train() client id: f_00004-7-3 loss: 0.940040  [  128/  306]
train() client id: f_00004-7-4 loss: 0.843078  [  160/  306]
train() client id: f_00004-7-5 loss: 0.839491  [  192/  306]
train() client id: f_00004-7-6 loss: 0.894077  [  224/  306]
train() client id: f_00004-7-7 loss: 0.904400  [  256/  306]
train() client id: f_00004-7-8 loss: 0.899063  [  288/  306]
train() client id: f_00004-8-0 loss: 0.962724  [   32/  306]
train() client id: f_00004-8-1 loss: 0.977239  [   64/  306]
train() client id: f_00004-8-2 loss: 0.931258  [   96/  306]
train() client id: f_00004-8-3 loss: 0.797752  [  128/  306]
train() client id: f_00004-8-4 loss: 0.890390  [  160/  306]
train() client id: f_00004-8-5 loss: 0.824123  [  192/  306]
train() client id: f_00004-8-6 loss: 0.893417  [  224/  306]
train() client id: f_00004-8-7 loss: 0.839180  [  256/  306]
train() client id: f_00004-8-8 loss: 0.860445  [  288/  306]
train() client id: f_00004-9-0 loss: 0.893447  [   32/  306]
train() client id: f_00004-9-1 loss: 0.934428  [   64/  306]
train() client id: f_00004-9-2 loss: 0.836518  [   96/  306]
train() client id: f_00004-9-3 loss: 0.805588  [  128/  306]
train() client id: f_00004-9-4 loss: 0.869167  [  160/  306]
train() client id: f_00004-9-5 loss: 0.864398  [  192/  306]
train() client id: f_00004-9-6 loss: 0.930372  [  224/  306]
train() client id: f_00004-9-7 loss: 0.999943  [  256/  306]
train() client id: f_00004-9-8 loss: 0.847877  [  288/  306]
train() client id: f_00004-10-0 loss: 0.760163  [   32/  306]
train() client id: f_00004-10-1 loss: 0.898802  [   64/  306]
train() client id: f_00004-10-2 loss: 0.904914  [   96/  306]
train() client id: f_00004-10-3 loss: 0.890712  [  128/  306]
train() client id: f_00004-10-4 loss: 0.974828  [  160/  306]
train() client id: f_00004-10-5 loss: 0.946545  [  192/  306]
train() client id: f_00004-10-6 loss: 0.914456  [  224/  306]
train() client id: f_00004-10-7 loss: 0.857752  [  256/  306]
train() client id: f_00004-10-8 loss: 0.864203  [  288/  306]
train() client id: f_00004-11-0 loss: 0.881318  [   32/  306]
train() client id: f_00004-11-1 loss: 0.901937  [   64/  306]
train() client id: f_00004-11-2 loss: 0.965525  [   96/  306]
train() client id: f_00004-11-3 loss: 0.932387  [  128/  306]
train() client id: f_00004-11-4 loss: 0.886138  [  160/  306]
train() client id: f_00004-11-5 loss: 0.859298  [  192/  306]
train() client id: f_00004-11-6 loss: 0.727098  [  224/  306]
train() client id: f_00004-11-7 loss: 0.884361  [  256/  306]
train() client id: f_00004-11-8 loss: 0.919462  [  288/  306]
train() client id: f_00005-0-0 loss: 0.857657  [   32/  146]
train() client id: f_00005-0-1 loss: 0.591492  [   64/  146]
train() client id: f_00005-0-2 loss: 0.673438  [   96/  146]
train() client id: f_00005-0-3 loss: 0.539599  [  128/  146]
train() client id: f_00005-1-0 loss: 0.551669  [   32/  146]
train() client id: f_00005-1-1 loss: 0.766644  [   64/  146]
train() client id: f_00005-1-2 loss: 0.593964  [   96/  146]
train() client id: f_00005-1-3 loss: 0.788317  [  128/  146]
train() client id: f_00005-2-0 loss: 0.606227  [   32/  146]
train() client id: f_00005-2-1 loss: 0.597981  [   64/  146]
train() client id: f_00005-2-2 loss: 0.605115  [   96/  146]
train() client id: f_00005-2-3 loss: 0.652150  [  128/  146]
train() client id: f_00005-3-0 loss: 0.641025  [   32/  146]
train() client id: f_00005-3-1 loss: 0.536235  [   64/  146]
train() client id: f_00005-3-2 loss: 0.831587  [   96/  146]
train() client id: f_00005-3-3 loss: 0.573440  [  128/  146]
train() client id: f_00005-4-0 loss: 0.675916  [   32/  146]
train() client id: f_00005-4-1 loss: 0.372043  [   64/  146]
train() client id: f_00005-4-2 loss: 0.864851  [   96/  146]
train() client id: f_00005-4-3 loss: 0.557492  [  128/  146]
train() client id: f_00005-5-0 loss: 0.841750  [   32/  146]
train() client id: f_00005-5-1 loss: 0.630425  [   64/  146]
train() client id: f_00005-5-2 loss: 0.620316  [   96/  146]
train() client id: f_00005-5-3 loss: 0.362760  [  128/  146]
train() client id: f_00005-6-0 loss: 0.580160  [   32/  146]
train() client id: f_00005-6-1 loss: 0.634638  [   64/  146]
train() client id: f_00005-6-2 loss: 0.696771  [   96/  146]
train() client id: f_00005-6-3 loss: 0.619927  [  128/  146]
train() client id: f_00005-7-0 loss: 0.660497  [   32/  146]
train() client id: f_00005-7-1 loss: 0.709125  [   64/  146]
train() client id: f_00005-7-2 loss: 0.582191  [   96/  146]
train() client id: f_00005-7-3 loss: 0.614272  [  128/  146]
train() client id: f_00005-8-0 loss: 0.517038  [   32/  146]
train() client id: f_00005-8-1 loss: 0.859342  [   64/  146]
train() client id: f_00005-8-2 loss: 0.670817  [   96/  146]
train() client id: f_00005-8-3 loss: 0.640408  [  128/  146]
train() client id: f_00005-9-0 loss: 0.484477  [   32/  146]
train() client id: f_00005-9-1 loss: 0.618792  [   64/  146]
train() client id: f_00005-9-2 loss: 0.658878  [   96/  146]
train() client id: f_00005-9-3 loss: 0.713451  [  128/  146]
train() client id: f_00005-10-0 loss: 0.612931  [   32/  146]
train() client id: f_00005-10-1 loss: 0.843489  [   64/  146]
train() client id: f_00005-10-2 loss: 0.475136  [   96/  146]
train() client id: f_00005-10-3 loss: 0.444354  [  128/  146]
train() client id: f_00005-11-0 loss: 0.761666  [   32/  146]
train() client id: f_00005-11-1 loss: 0.590401  [   64/  146]
train() client id: f_00005-11-2 loss: 0.468134  [   96/  146]
train() client id: f_00005-11-3 loss: 0.729925  [  128/  146]
train() client id: f_00006-0-0 loss: 0.662949  [   32/   54]
train() client id: f_00006-1-0 loss: 0.665181  [   32/   54]
train() client id: f_00006-2-0 loss: 0.601555  [   32/   54]
train() client id: f_00006-3-0 loss: 0.600949  [   32/   54]
train() client id: f_00006-4-0 loss: 0.683526  [   32/   54]
train() client id: f_00006-5-0 loss: 0.639248  [   32/   54]
train() client id: f_00006-6-0 loss: 0.640610  [   32/   54]
train() client id: f_00006-7-0 loss: 0.625059  [   32/   54]
train() client id: f_00006-8-0 loss: 0.676331  [   32/   54]
train() client id: f_00006-9-0 loss: 0.603245  [   32/   54]
train() client id: f_00006-10-0 loss: 0.601980  [   32/   54]
train() client id: f_00006-11-0 loss: 0.673658  [   32/   54]
train() client id: f_00007-0-0 loss: 0.713532  [   32/  179]
train() client id: f_00007-0-1 loss: 0.535540  [   64/  179]
train() client id: f_00007-0-2 loss: 0.743882  [   96/  179]
train() client id: f_00007-0-3 loss: 0.680344  [  128/  179]
train() client id: f_00007-0-4 loss: 0.678639  [  160/  179]
train() client id: f_00007-1-0 loss: 0.614012  [   32/  179]
train() client id: f_00007-1-1 loss: 0.626243  [   64/  179]
train() client id: f_00007-1-2 loss: 0.644797  [   96/  179]
train() client id: f_00007-1-3 loss: 0.727045  [  128/  179]
train() client id: f_00007-1-4 loss: 0.661380  [  160/  179]
train() client id: f_00007-2-0 loss: 0.791679  [   32/  179]
train() client id: f_00007-2-1 loss: 0.522040  [   64/  179]
train() client id: f_00007-2-2 loss: 0.680103  [   96/  179]
train() client id: f_00007-2-3 loss: 0.578021  [  128/  179]
train() client id: f_00007-2-4 loss: 0.525426  [  160/  179]
train() client id: f_00007-3-0 loss: 0.645233  [   32/  179]
train() client id: f_00007-3-1 loss: 0.758220  [   64/  179]
train() client id: f_00007-3-2 loss: 0.523516  [   96/  179]
train() client id: f_00007-3-3 loss: 0.578811  [  128/  179]
train() client id: f_00007-3-4 loss: 0.698007  [  160/  179]
train() client id: f_00007-4-0 loss: 0.543437  [   32/  179]
train() client id: f_00007-4-1 loss: 0.536754  [   64/  179]
train() client id: f_00007-4-2 loss: 0.573086  [   96/  179]
train() client id: f_00007-4-3 loss: 0.722432  [  128/  179]
train() client id: f_00007-4-4 loss: 0.794556  [  160/  179]
train() client id: f_00007-5-0 loss: 0.684199  [   32/  179]
train() client id: f_00007-5-1 loss: 0.692589  [   64/  179]
train() client id: f_00007-5-2 loss: 0.663590  [   96/  179]
train() client id: f_00007-5-3 loss: 0.553415  [  128/  179]
train() client id: f_00007-5-4 loss: 0.480034  [  160/  179]
train() client id: f_00007-6-0 loss: 0.649794  [   32/  179]
train() client id: f_00007-6-1 loss: 0.667798  [   64/  179]
train() client id: f_00007-6-2 loss: 0.684028  [   96/  179]
train() client id: f_00007-6-3 loss: 0.624064  [  128/  179]
train() client id: f_00007-6-4 loss: 0.494721  [  160/  179]
train() client id: f_00007-7-0 loss: 0.564122  [   32/  179]
train() client id: f_00007-7-1 loss: 0.601188  [   64/  179]
train() client id: f_00007-7-2 loss: 0.615122  [   96/  179]
train() client id: f_00007-7-3 loss: 0.639187  [  128/  179]
train() client id: f_00007-7-4 loss: 0.623815  [  160/  179]
train() client id: f_00007-8-0 loss: 0.579307  [   32/  179]
train() client id: f_00007-8-1 loss: 0.621498  [   64/  179]
train() client id: f_00007-8-2 loss: 0.568158  [   96/  179]
train() client id: f_00007-8-3 loss: 0.644380  [  128/  179]
train() client id: f_00007-8-4 loss: 0.587973  [  160/  179]
train() client id: f_00007-9-0 loss: 0.571039  [   32/  179]
train() client id: f_00007-9-1 loss: 0.818345  [   64/  179]
train() client id: f_00007-9-2 loss: 0.605412  [   96/  179]
train() client id: f_00007-9-3 loss: 0.557178  [  128/  179]
train() client id: f_00007-9-4 loss: 0.555729  [  160/  179]
train() client id: f_00007-10-0 loss: 0.500120  [   32/  179]
train() client id: f_00007-10-1 loss: 0.625891  [   64/  179]
train() client id: f_00007-10-2 loss: 0.723816  [   96/  179]
train() client id: f_00007-10-3 loss: 0.563539  [  128/  179]
train() client id: f_00007-10-4 loss: 0.589697  [  160/  179]
train() client id: f_00007-11-0 loss: 0.641049  [   32/  179]
train() client id: f_00007-11-1 loss: 0.590533  [   64/  179]
train() client id: f_00007-11-2 loss: 0.475657  [   96/  179]
train() client id: f_00007-11-3 loss: 0.772629  [  128/  179]
train() client id: f_00007-11-4 loss: 0.482669  [  160/  179]
train() client id: f_00008-0-0 loss: 0.605474  [   32/  130]
train() client id: f_00008-0-1 loss: 0.643594  [   64/  130]
train() client id: f_00008-0-2 loss: 0.680632  [   96/  130]
train() client id: f_00008-0-3 loss: 0.683446  [  128/  130]
train() client id: f_00008-1-0 loss: 0.645340  [   32/  130]
train() client id: f_00008-1-1 loss: 0.681945  [   64/  130]
train() client id: f_00008-1-2 loss: 0.728298  [   96/  130]
train() client id: f_00008-1-3 loss: 0.600980  [  128/  130]
train() client id: f_00008-2-0 loss: 0.845619  [   32/  130]
train() client id: f_00008-2-1 loss: 0.515133  [   64/  130]
train() client id: f_00008-2-2 loss: 0.649392  [   96/  130]
train() client id: f_00008-2-3 loss: 0.674646  [  128/  130]
train() client id: f_00008-3-0 loss: 0.581503  [   32/  130]
train() client id: f_00008-3-1 loss: 0.623675  [   64/  130]
train() client id: f_00008-3-2 loss: 0.724072  [   96/  130]
train() client id: f_00008-3-3 loss: 0.746855  [  128/  130]
train() client id: f_00008-4-0 loss: 0.636055  [   32/  130]
train() client id: f_00008-4-1 loss: 0.620966  [   64/  130]
train() client id: f_00008-4-2 loss: 0.750609  [   96/  130]
train() client id: f_00008-4-3 loss: 0.632633  [  128/  130]
train() client id: f_00008-5-0 loss: 0.614285  [   32/  130]
train() client id: f_00008-5-1 loss: 0.755433  [   64/  130]
train() client id: f_00008-5-2 loss: 0.728842  [   96/  130]
train() client id: f_00008-5-3 loss: 0.562715  [  128/  130]
train() client id: f_00008-6-0 loss: 0.653934  [   32/  130]
train() client id: f_00008-6-1 loss: 0.691473  [   64/  130]
train() client id: f_00008-6-2 loss: 0.767298  [   96/  130]
train() client id: f_00008-6-3 loss: 0.563667  [  128/  130]
train() client id: f_00008-7-0 loss: 0.643532  [   32/  130]
train() client id: f_00008-7-1 loss: 0.695654  [   64/  130]
train() client id: f_00008-7-2 loss: 0.707560  [   96/  130]
train() client id: f_00008-7-3 loss: 0.612542  [  128/  130]
train() client id: f_00008-8-0 loss: 0.723546  [   32/  130]
train() client id: f_00008-8-1 loss: 0.659722  [   64/  130]
train() client id: f_00008-8-2 loss: 0.693930  [   96/  130]
train() client id: f_00008-8-3 loss: 0.608481  [  128/  130]
train() client id: f_00008-9-0 loss: 0.804473  [   32/  130]
train() client id: f_00008-9-1 loss: 0.675475  [   64/  130]
train() client id: f_00008-9-2 loss: 0.601554  [   96/  130]
train() client id: f_00008-9-3 loss: 0.600559  [  128/  130]
train() client id: f_00008-10-0 loss: 0.507262  [   32/  130]
train() client id: f_00008-10-1 loss: 0.721835  [   64/  130]
train() client id: f_00008-10-2 loss: 0.827093  [   96/  130]
train() client id: f_00008-10-3 loss: 0.579687  [  128/  130]
train() client id: f_00008-11-0 loss: 0.694139  [   32/  130]
train() client id: f_00008-11-1 loss: 0.644082  [   64/  130]
train() client id: f_00008-11-2 loss: 0.684903  [   96/  130]
train() client id: f_00008-11-3 loss: 0.661822  [  128/  130]
train() client id: f_00009-0-0 loss: 0.994707  [   32/  118]
train() client id: f_00009-0-1 loss: 1.035464  [   64/  118]
train() client id: f_00009-0-2 loss: 1.043369  [   96/  118]
train() client id: f_00009-1-0 loss: 1.052755  [   32/  118]
train() client id: f_00009-1-1 loss: 1.093258  [   64/  118]
train() client id: f_00009-1-2 loss: 0.911342  [   96/  118]
train() client id: f_00009-2-0 loss: 1.020504  [   32/  118]
train() client id: f_00009-2-1 loss: 1.021313  [   64/  118]
train() client id: f_00009-2-2 loss: 0.885790  [   96/  118]
train() client id: f_00009-3-0 loss: 0.922235  [   32/  118]
train() client id: f_00009-3-1 loss: 0.934782  [   64/  118]
train() client id: f_00009-3-2 loss: 0.988646  [   96/  118]
train() client id: f_00009-4-0 loss: 0.842473  [   32/  118]
train() client id: f_00009-4-1 loss: 0.939158  [   64/  118]
train() client id: f_00009-4-2 loss: 0.949496  [   96/  118]
train() client id: f_00009-5-0 loss: 0.900452  [   32/  118]
train() client id: f_00009-5-1 loss: 0.800015  [   64/  118]
train() client id: f_00009-5-2 loss: 0.953192  [   96/  118]
train() client id: f_00009-6-0 loss: 0.873872  [   32/  118]
train() client id: f_00009-6-1 loss: 0.970119  [   64/  118]
train() client id: f_00009-6-2 loss: 0.872269  [   96/  118]
train() client id: f_00009-7-0 loss: 0.949195  [   32/  118]
train() client id: f_00009-7-1 loss: 0.947673  [   64/  118]
train() client id: f_00009-7-2 loss: 0.793966  [   96/  118]
train() client id: f_00009-8-0 loss: 0.878979  [   32/  118]
train() client id: f_00009-8-1 loss: 0.785236  [   64/  118]
train() client id: f_00009-8-2 loss: 0.963512  [   96/  118]
train() client id: f_00009-9-0 loss: 0.886121  [   32/  118]
train() client id: f_00009-9-1 loss: 0.883105  [   64/  118]
train() client id: f_00009-9-2 loss: 0.903478  [   96/  118]
train() client id: f_00009-10-0 loss: 0.763463  [   32/  118]
train() client id: f_00009-10-1 loss: 0.834210  [   64/  118]
train() client id: f_00009-10-2 loss: 0.859347  [   96/  118]
train() client id: f_00009-11-0 loss: 1.020253  [   32/  118]
train() client id: f_00009-11-1 loss: 0.804788  [   64/  118]
train() client id: f_00009-11-2 loss: 0.764844  [   96/  118]
At round 16 accuracy: 0.636604774535809
At round 16 training accuracy: 0.5781354795439303
At round 16 training loss: 0.8509186060962171
update_location
xs = -3.905658 4.200318 100.009024 18.811294 0.979296 3.956410 -62.443192 -41.324852 84.663977 -27.060879 
ys = 92.587959 75.555839 1.320614 -62.455176 54.350187 37.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 136.337025 125.404655 141.433903 119.392269 113.819602 106.983939 117.923885 108.205451 132.199315 103.674023 
dists_bs = 189.907920 204.943639 325.260653 306.509018 213.366094 225.489144 210.284736 219.560697 303.660828 226.111647 
uav_gains = -103.366606 -102.458239 -103.765913 -101.924604 -101.405528 -100.733006 -101.790212 -100.856278 -103.031556 -100.391775 
bs_gains = -103.366265 -104.292824 -109.909542 -109.187470 -104.782571 -105.454577 -104.605676 -105.130588 -109.073945 -105.488101 
Round 17
-------------------------------
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.74794599 18.22367373  8.61494673  3.08364678 21.02075903 10.12863146
  3.83198593 12.340238    9.082055    8.2217282 ]
obj_prev = 103.29561085136496
eta_min = 2.3980574083554427e-11	eta_max = 0.9215530004474818
af = 21.83294705699746	bf = 1.718217883710131	zeta = 24.01624176269721	eta = 0.9090909090909091
af = 21.83294705699746	bf = 1.718217883710131	zeta = 41.790319927079516	eta = 0.5224402946685754
af = 21.83294705699746	bf = 1.718217883710131	zeta = 33.27990204277066	eta = 0.6560400036315671
af = 21.83294705699746	bf = 1.718217883710131	zeta = 31.752345685579005	eta = 0.6876010759391976
af = 21.83294705699746	bf = 1.718217883710131	zeta = 31.67639108760098	eta = 0.6892498263649575
af = 21.83294705699746	bf = 1.718217883710131	zeta = 31.676189751909728	eta = 0.6892542072766557
eta = 0.6892542072766557
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [0.03070852 0.06458539 0.03022109 0.01047989 0.07457784 0.03558289
 0.0131608  0.0436256  0.0316834  0.02875878]
ene_total = [2.73656278 5.19862747 2.71178509 1.24687687 5.93149526 3.1458414
 1.43681478 3.6092118  3.0031103  2.655864  ]
ti_comp = [0.33188313 0.32847851 0.33045231 0.33663064 0.32655044 0.32374602
 0.33704245 0.33977473 0.30458757 0.32360102]
ti_coms = [0.07323726 0.07664188 0.07466807 0.06848974 0.07856994 0.08137436
 0.06807793 0.06534565 0.10053281 0.08151936]
t_total = [29.14992867 29.14992867 29.14992867 29.14992867 29.14992867 29.14992867
 29.14992867 29.14992867 29.14992867 29.14992867]
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [1.64318511e-05 1.56051844e-04 1.57976562e-05 6.34809284e-07
 2.43113417e-04 2.68654979e-05 1.25417250e-06 4.49492172e-05
 2.14264829e-05 1.41961788e-05]
ene_total = [0.52815183 0.56269552 0.53840142 0.492855   0.58283307 0.5874521
 0.48993648 0.47342069 0.72491292 0.58758383]
optimize_network iter = 0 obj = 5.56824286220838
eta = 0.6892542072766557
freqs = [4.62640631e+07 9.83099137e+07 4.57268501e+07 1.55658631e+07
 1.14190388e+08 5.49549501e+07 1.95239441e+07 6.41978302e+07
 5.20103250e+07 4.44355465e+07]
eta_min = 0.6892542072766561	eta_max = 0.6892542072766527
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 0.03573652736322809	eta = 0.9090909090909091
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 18.93445726929715	eta = 0.0017158005474531947
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 1.9284046028512083	eta = 0.016846958413371853
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 1.882600477836571	eta = 0.017256848986739414
af = 0.03248775214838917	bf = 1.718217883710131	zeta = 1.8825910021965795	eta = 0.01725693584558884
eta = 0.01725693584558884
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [1.79246673e-04 1.70228989e-03 1.72328565e-04 6.92481033e-06
 2.65200012e-03 2.93061998e-04 1.36811274e-05 4.90328058e-04
 2.33730561e-04 1.54858866e-04]
ene_total = [0.17070727 0.21310565 0.17380525 0.15598508 0.23910014 0.1918103
 0.15520187 0.15983003 0.23404959 0.18899581]
ti_comp = [0.33188313 0.32847851 0.33045231 0.33663064 0.32655044 0.32374602
 0.33704245 0.33977473 0.30458757 0.32360102]
ti_coms = [0.07323726 0.07664188 0.07466807 0.06848974 0.07856994 0.08137436
 0.06807793 0.06534565 0.10053281 0.08151936]
t_total = [29.14992867 29.14992867 29.14992867 29.14992867 29.14992867 29.14992867
 29.14992867 29.14992867 29.14992867 29.14992867]
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [1.64318511e-05 1.56051844e-04 1.57976562e-05 6.34809284e-07
 2.43113417e-04 2.68654979e-05 1.25417250e-06 4.49492172e-05
 2.14264829e-05 1.41961788e-05]
ene_total = [0.52815183 0.56269552 0.53840142 0.492855   0.58283307 0.5874521
 0.48993648 0.47342069 0.72491292 0.58758383]
optimize_network iter = 1 obj = 5.56824286220839
eta = 0.6892542072766561
freqs = [4.62640631e+07 9.83099137e+07 4.57268501e+07 1.55658631e+07
 1.14190388e+08 5.49549501e+07 1.95239441e+07 6.41978302e+07
 5.20103250e+07 4.44355465e+07]
Done!
ene_coms = [0.00732373 0.00766419 0.00746681 0.00684897 0.00785699 0.00813744
 0.00680779 0.00653457 0.01005328 0.00815194]
ene_comp = [1.61811483e-05 1.53670942e-04 1.55566295e-05 6.25123921e-07
 2.39404205e-04 2.64556077e-05 1.23503744e-06 4.42634215e-05
 2.10995764e-05 1.39795860e-05]
ene_total = [0.00733991 0.00781786 0.00748236 0.0068496  0.0080964  0.00816389
 0.00680903 0.00657883 0.01007438 0.00816592]
At round 17 energy consumption: 0.07737817222069016
At round 17 eta: 0.6892542072766561
At round 17 a_n: 22.359323457775982
At round 17 local rounds: 12.185922115151007
At round 17 global rounds: 71.9537447693859
gradient difference: 0.4258708357810974
train() client id: f_00000-0-0 loss: 1.350222  [   32/  126]
train() client id: f_00000-0-1 loss: 1.459741  [   64/  126]
train() client id: f_00000-0-2 loss: 1.249335  [   96/  126]
train() client id: f_00000-1-0 loss: 1.158011  [   32/  126]
train() client id: f_00000-1-1 loss: 1.281737  [   64/  126]
train() client id: f_00000-1-2 loss: 1.265723  [   96/  126]
train() client id: f_00000-2-0 loss: 1.127714  [   32/  126]
train() client id: f_00000-2-1 loss: 1.238881  [   64/  126]
train() client id: f_00000-2-2 loss: 1.046724  [   96/  126]
train() client id: f_00000-3-0 loss: 1.075924  [   32/  126]
train() client id: f_00000-3-1 loss: 1.088536  [   64/  126]
train() client id: f_00000-3-2 loss: 0.954424  [   96/  126]
train() client id: f_00000-4-0 loss: 0.961941  [   32/  126]
train() client id: f_00000-4-1 loss: 0.957857  [   64/  126]
train() client id: f_00000-4-2 loss: 1.000448  [   96/  126]
train() client id: f_00000-5-0 loss: 0.897661  [   32/  126]
train() client id: f_00000-5-1 loss: 0.882041  [   64/  126]
train() client id: f_00000-5-2 loss: 0.985181  [   96/  126]
train() client id: f_00000-6-0 loss: 0.913289  [   32/  126]
train() client id: f_00000-6-1 loss: 0.855013  [   64/  126]
train() client id: f_00000-6-2 loss: 0.928917  [   96/  126]
train() client id: f_00000-7-0 loss: 0.904264  [   32/  126]
train() client id: f_00000-7-1 loss: 0.893632  [   64/  126]
train() client id: f_00000-7-2 loss: 0.906826  [   96/  126]
train() client id: f_00000-8-0 loss: 0.819244  [   32/  126]
train() client id: f_00000-8-1 loss: 0.864613  [   64/  126]
train() client id: f_00000-8-2 loss: 0.936096  [   96/  126]
train() client id: f_00000-9-0 loss: 0.834389  [   32/  126]
train() client id: f_00000-9-1 loss: 0.909290  [   64/  126]
train() client id: f_00000-9-2 loss: 0.840043  [   96/  126]
train() client id: f_00000-10-0 loss: 0.878556  [   32/  126]
train() client id: f_00000-10-1 loss: 0.849312  [   64/  126]
train() client id: f_00000-10-2 loss: 0.851929  [   96/  126]
train() client id: f_00000-11-0 loss: 0.818055  [   32/  126]
train() client id: f_00000-11-1 loss: 0.832823  [   64/  126]
train() client id: f_00000-11-2 loss: 0.887102  [   96/  126]
train() client id: f_00001-0-0 loss: 0.432811  [   32/  265]
train() client id: f_00001-0-1 loss: 0.433581  [   64/  265]
train() client id: f_00001-0-2 loss: 0.399750  [   96/  265]
train() client id: f_00001-0-3 loss: 0.401962  [  128/  265]
train() client id: f_00001-0-4 loss: 0.407493  [  160/  265]
train() client id: f_00001-0-5 loss: 0.639670  [  192/  265]
train() client id: f_00001-0-6 loss: 0.502641  [  224/  265]
train() client id: f_00001-0-7 loss: 0.364465  [  256/  265]
train() client id: f_00001-1-0 loss: 0.394574  [   32/  265]
train() client id: f_00001-1-1 loss: 0.437851  [   64/  265]
train() client id: f_00001-1-2 loss: 0.356150  [   96/  265]
train() client id: f_00001-1-3 loss: 0.449305  [  128/  265]
train() client id: f_00001-1-4 loss: 0.418141  [  160/  265]
train() client id: f_00001-1-5 loss: 0.498824  [  192/  265]
train() client id: f_00001-1-6 loss: 0.479329  [  224/  265]
train() client id: f_00001-1-7 loss: 0.408755  [  256/  265]
train() client id: f_00001-2-0 loss: 0.358814  [   32/  265]
train() client id: f_00001-2-1 loss: 0.495320  [   64/  265]
train() client id: f_00001-2-2 loss: 0.351635  [   96/  265]
train() client id: f_00001-2-3 loss: 0.413396  [  128/  265]
train() client id: f_00001-2-4 loss: 0.604459  [  160/  265]
train() client id: f_00001-2-5 loss: 0.340457  [  192/  265]
train() client id: f_00001-2-6 loss: 0.452210  [  224/  265]
train() client id: f_00001-2-7 loss: 0.408279  [  256/  265]
train() client id: f_00001-3-0 loss: 0.427543  [   32/  265]
train() client id: f_00001-3-1 loss: 0.431566  [   64/  265]
train() client id: f_00001-3-2 loss: 0.510962  [   96/  265]
train() client id: f_00001-3-3 loss: 0.327180  [  128/  265]
train() client id: f_00001-3-4 loss: 0.478640  [  160/  265]
train() client id: f_00001-3-5 loss: 0.400186  [  192/  265]
train() client id: f_00001-3-6 loss: 0.347934  [  224/  265]
train() client id: f_00001-3-7 loss: 0.445237  [  256/  265]
train() client id: f_00001-4-0 loss: 0.450168  [   32/  265]
train() client id: f_00001-4-1 loss: 0.445752  [   64/  265]
train() client id: f_00001-4-2 loss: 0.320483  [   96/  265]
train() client id: f_00001-4-3 loss: 0.513122  [  128/  265]
train() client id: f_00001-4-4 loss: 0.345391  [  160/  265]
train() client id: f_00001-4-5 loss: 0.380304  [  192/  265]
train() client id: f_00001-4-6 loss: 0.322631  [  224/  265]
train() client id: f_00001-4-7 loss: 0.504741  [  256/  265]
train() client id: f_00001-5-0 loss: 0.401940  [   32/  265]
train() client id: f_00001-5-1 loss: 0.340961  [   64/  265]
train() client id: f_00001-5-2 loss: 0.423437  [   96/  265]
train() client id: f_00001-5-3 loss: 0.362147  [  128/  265]
train() client id: f_00001-5-4 loss: 0.473934  [  160/  265]
train() client id: f_00001-5-5 loss: 0.544382  [  192/  265]
train() client id: f_00001-5-6 loss: 0.320504  [  224/  265]
train() client id: f_00001-5-7 loss: 0.393992  [  256/  265]
train() client id: f_00001-6-0 loss: 0.398519  [   32/  265]
train() client id: f_00001-6-1 loss: 0.438593  [   64/  265]
train() client id: f_00001-6-2 loss: 0.364736  [   96/  265]
train() client id: f_00001-6-3 loss: 0.371910  [  128/  265]
train() client id: f_00001-6-4 loss: 0.394424  [  160/  265]
train() client id: f_00001-6-5 loss: 0.511211  [  192/  265]
train() client id: f_00001-6-6 loss: 0.432009  [  224/  265]
train() client id: f_00001-6-7 loss: 0.377157  [  256/  265]
train() client id: f_00001-7-0 loss: 0.328468  [   32/  265]
train() client id: f_00001-7-1 loss: 0.429736  [   64/  265]
train() client id: f_00001-7-2 loss: 0.366960  [   96/  265]
train() client id: f_00001-7-3 loss: 0.472861  [  128/  265]
train() client id: f_00001-7-4 loss: 0.330249  [  160/  265]
train() client id: f_00001-7-5 loss: 0.440127  [  192/  265]
train() client id: f_00001-7-6 loss: 0.469614  [  224/  265]
train() client id: f_00001-7-7 loss: 0.434609  [  256/  265]
train() client id: f_00001-8-0 loss: 0.355244  [   32/  265]
train() client id: f_00001-8-1 loss: 0.313125  [   64/  265]
train() client id: f_00001-8-2 loss: 0.465795  [   96/  265]
train() client id: f_00001-8-3 loss: 0.310843  [  128/  265]
train() client id: f_00001-8-4 loss: 0.561032  [  160/  265]
train() client id: f_00001-8-5 loss: 0.441925  [  192/  265]
train() client id: f_00001-8-6 loss: 0.452089  [  224/  265]
train() client id: f_00001-8-7 loss: 0.320661  [  256/  265]
train() client id: f_00001-9-0 loss: 0.488788  [   32/  265]
train() client id: f_00001-9-1 loss: 0.307907  [   64/  265]
train() client id: f_00001-9-2 loss: 0.460679  [   96/  265]
train() client id: f_00001-9-3 loss: 0.302100  [  128/  265]
train() client id: f_00001-9-4 loss: 0.359917  [  160/  265]
train() client id: f_00001-9-5 loss: 0.512565  [  192/  265]
train() client id: f_00001-9-6 loss: 0.391900  [  224/  265]
train() client id: f_00001-9-7 loss: 0.349012  [  256/  265]
train() client id: f_00001-10-0 loss: 0.302133  [   32/  265]
train() client id: f_00001-10-1 loss: 0.394026  [   64/  265]
train() client id: f_00001-10-2 loss: 0.411989  [   96/  265]
train() client id: f_00001-10-3 loss: 0.384255  [  128/  265]
train() client id: f_00001-10-4 loss: 0.421863  [  160/  265]
train() client id: f_00001-10-5 loss: 0.458780  [  192/  265]
train() client id: f_00001-10-6 loss: 0.562450  [  224/  265]
train() client id: f_00001-10-7 loss: 0.318324  [  256/  265]
train() client id: f_00001-11-0 loss: 0.365199  [   32/  265]
train() client id: f_00001-11-1 loss: 0.464579  [   64/  265]
train() client id: f_00001-11-2 loss: 0.424696  [   96/  265]
train() client id: f_00001-11-3 loss: 0.315488  [  128/  265]
train() client id: f_00001-11-4 loss: 0.361583  [  160/  265]
train() client id: f_00001-11-5 loss: 0.367662  [  192/  265]
train() client id: f_00001-11-6 loss: 0.493874  [  224/  265]
train() client id: f_00001-11-7 loss: 0.321738  [  256/  265]
train() client id: f_00002-0-0 loss: 1.157895  [   32/  124]
train() client id: f_00002-0-1 loss: 1.297947  [   64/  124]
train() client id: f_00002-0-2 loss: 1.151148  [   96/  124]
train() client id: f_00002-1-0 loss: 1.014312  [   32/  124]
train() client id: f_00002-1-1 loss: 1.172137  [   64/  124]
train() client id: f_00002-1-2 loss: 1.134969  [   96/  124]
train() client id: f_00002-2-0 loss: 1.106936  [   32/  124]
train() client id: f_00002-2-1 loss: 1.044795  [   64/  124]
train() client id: f_00002-2-2 loss: 1.051827  [   96/  124]
train() client id: f_00002-3-0 loss: 1.008277  [   32/  124]
train() client id: f_00002-3-1 loss: 1.163593  [   64/  124]
train() client id: f_00002-3-2 loss: 1.051780  [   96/  124]
train() client id: f_00002-4-0 loss: 1.129752  [   32/  124]
train() client id: f_00002-4-1 loss: 1.028274  [   64/  124]
train() client id: f_00002-4-2 loss: 0.997480  [   96/  124]
train() client id: f_00002-5-0 loss: 1.072898  [   32/  124]
train() client id: f_00002-5-1 loss: 0.956583  [   64/  124]
train() client id: f_00002-5-2 loss: 1.002033  [   96/  124]
train() client id: f_00002-6-0 loss: 1.007216  [   32/  124]
train() client id: f_00002-6-1 loss: 0.975848  [   64/  124]
train() client id: f_00002-6-2 loss: 0.912485  [   96/  124]
train() client id: f_00002-7-0 loss: 0.897660  [   32/  124]
train() client id: f_00002-7-1 loss: 1.003910  [   64/  124]
train() client id: f_00002-7-2 loss: 0.979380  [   96/  124]
train() client id: f_00002-8-0 loss: 0.898811  [   32/  124]
train() client id: f_00002-8-1 loss: 0.939857  [   64/  124]
train() client id: f_00002-8-2 loss: 0.997086  [   96/  124]
train() client id: f_00002-9-0 loss: 0.982946  [   32/  124]
train() client id: f_00002-9-1 loss: 0.972402  [   64/  124]
train() client id: f_00002-9-2 loss: 0.902464  [   96/  124]
train() client id: f_00002-10-0 loss: 0.834981  [   32/  124]
train() client id: f_00002-10-1 loss: 0.965279  [   64/  124]
train() client id: f_00002-10-2 loss: 1.077717  [   96/  124]
train() client id: f_00002-11-0 loss: 0.953864  [   32/  124]
train() client id: f_00002-11-1 loss: 0.830904  [   64/  124]
train() client id: f_00002-11-2 loss: 0.949022  [   96/  124]
train() client id: f_00003-0-0 loss: 0.746953  [   32/   43]
train() client id: f_00003-1-0 loss: 0.764896  [   32/   43]
train() client id: f_00003-2-0 loss: 0.747061  [   32/   43]
train() client id: f_00003-3-0 loss: 0.837850  [   32/   43]
train() client id: f_00003-4-0 loss: 0.939674  [   32/   43]
train() client id: f_00003-5-0 loss: 0.934968  [   32/   43]
train() client id: f_00003-6-0 loss: 0.784957  [   32/   43]
train() client id: f_00003-7-0 loss: 0.923068  [   32/   43]
train() client id: f_00003-8-0 loss: 1.015877  [   32/   43]
train() client id: f_00003-9-0 loss: 0.870127  [   32/   43]
train() client id: f_00003-10-0 loss: 0.777089  [   32/   43]
train() client id: f_00003-11-0 loss: 0.919826  [   32/   43]
train() client id: f_00004-0-0 loss: 0.840902  [   32/  306]
train() client id: f_00004-0-1 loss: 0.947253  [   64/  306]
train() client id: f_00004-0-2 loss: 0.905959  [   96/  306]
train() client id: f_00004-0-3 loss: 0.883840  [  128/  306]
train() client id: f_00004-0-4 loss: 0.897222  [  160/  306]
train() client id: f_00004-0-5 loss: 0.803735  [  192/  306]
train() client id: f_00004-0-6 loss: 1.005621  [  224/  306]
train() client id: f_00004-0-7 loss: 0.920803  [  256/  306]
train() client id: f_00004-0-8 loss: 0.866404  [  288/  306]
train() client id: f_00004-1-0 loss: 0.888232  [   32/  306]
train() client id: f_00004-1-1 loss: 0.918441  [   64/  306]
train() client id: f_00004-1-2 loss: 0.928053  [   96/  306]
train() client id: f_00004-1-3 loss: 0.835850  [  128/  306]
train() client id: f_00004-1-4 loss: 0.792060  [  160/  306]
train() client id: f_00004-1-5 loss: 0.932141  [  192/  306]
train() client id: f_00004-1-6 loss: 0.949184  [  224/  306]
train() client id: f_00004-1-7 loss: 0.975337  [  256/  306]
train() client id: f_00004-1-8 loss: 0.903962  [  288/  306]
train() client id: f_00004-2-0 loss: 0.910703  [   32/  306]
train() client id: f_00004-2-1 loss: 0.900496  [   64/  306]
train() client id: f_00004-2-2 loss: 0.831199  [   96/  306]
train() client id: f_00004-2-3 loss: 0.791159  [  128/  306]
train() client id: f_00004-2-4 loss: 0.810227  [  160/  306]
train() client id: f_00004-2-5 loss: 0.983482  [  192/  306]
train() client id: f_00004-2-6 loss: 0.961704  [  224/  306]
train() client id: f_00004-2-7 loss: 0.872377  [  256/  306]
train() client id: f_00004-2-8 loss: 1.011189  [  288/  306]
train() client id: f_00004-3-0 loss: 0.867648  [   32/  306]
train() client id: f_00004-3-1 loss: 1.003067  [   64/  306]
train() client id: f_00004-3-2 loss: 0.969624  [   96/  306]
train() client id: f_00004-3-3 loss: 0.852169  [  128/  306]
train() client id: f_00004-3-4 loss: 0.810084  [  160/  306]
train() client id: f_00004-3-5 loss: 0.959505  [  192/  306]
train() client id: f_00004-3-6 loss: 0.915683  [  224/  306]
train() client id: f_00004-3-7 loss: 0.766429  [  256/  306]
train() client id: f_00004-3-8 loss: 0.833844  [  288/  306]
train() client id: f_00004-4-0 loss: 0.787731  [   32/  306]
train() client id: f_00004-4-1 loss: 0.840166  [   64/  306]
train() client id: f_00004-4-2 loss: 0.892810  [   96/  306]
train() client id: f_00004-4-3 loss: 0.827066  [  128/  306]
train() client id: f_00004-4-4 loss: 1.029948  [  160/  306]
train() client id: f_00004-4-5 loss: 0.919971  [  192/  306]
train() client id: f_00004-4-6 loss: 0.812319  [  224/  306]
train() client id: f_00004-4-7 loss: 0.888440  [  256/  306]
train() client id: f_00004-4-8 loss: 0.986275  [  288/  306]
train() client id: f_00004-5-0 loss: 0.917711  [   32/  306]
train() client id: f_00004-5-1 loss: 0.933423  [   64/  306]
train() client id: f_00004-5-2 loss: 0.938042  [   96/  306]
train() client id: f_00004-5-3 loss: 0.873932  [  128/  306]
train() client id: f_00004-5-4 loss: 0.828243  [  160/  306]
train() client id: f_00004-5-5 loss: 0.893136  [  192/  306]
train() client id: f_00004-5-6 loss: 0.871247  [  224/  306]
train() client id: f_00004-5-7 loss: 0.800348  [  256/  306]
train() client id: f_00004-5-8 loss: 0.885746  [  288/  306]
train() client id: f_00004-6-0 loss: 0.808313  [   32/  306]
train() client id: f_00004-6-1 loss: 0.855575  [   64/  306]
train() client id: f_00004-6-2 loss: 0.975554  [   96/  306]
train() client id: f_00004-6-3 loss: 0.943251  [  128/  306]
train() client id: f_00004-6-4 loss: 0.873706  [  160/  306]
train() client id: f_00004-6-5 loss: 0.872256  [  192/  306]
train() client id: f_00004-6-6 loss: 0.970447  [  224/  306]
train() client id: f_00004-6-7 loss: 0.914220  [  256/  306]
train() client id: f_00004-6-8 loss: 0.721548  [  288/  306]
train() client id: f_00004-7-0 loss: 0.875991  [   32/  306]
train() client id: f_00004-7-1 loss: 0.915203  [   64/  306]
train() client id: f_00004-7-2 loss: 0.812795  [   96/  306]
train() client id: f_00004-7-3 loss: 0.947428  [  128/  306]
train() client id: f_00004-7-4 loss: 0.893147  [  160/  306]
train() client id: f_00004-7-5 loss: 0.941139  [  192/  306]
train() client id: f_00004-7-6 loss: 0.799180  [  224/  306]
train() client id: f_00004-7-7 loss: 0.909322  [  256/  306]
train() client id: f_00004-7-8 loss: 0.848312  [  288/  306]
train() client id: f_00004-8-0 loss: 0.882625  [   32/  306]
train() client id: f_00004-8-1 loss: 0.821219  [   64/  306]
train() client id: f_00004-8-2 loss: 0.876374  [   96/  306]
train() client id: f_00004-8-3 loss: 0.860471  [  128/  306]
train() client id: f_00004-8-4 loss: 0.934340  [  160/  306]
train() client id: f_00004-8-5 loss: 0.894703  [  192/  306]
train() client id: f_00004-8-6 loss: 0.875916  [  224/  306]
train() client id: f_00004-8-7 loss: 0.931184  [  256/  306]
train() client id: f_00004-8-8 loss: 0.754247  [  288/  306]
train() client id: f_00004-9-0 loss: 0.807922  [   32/  306]
train() client id: f_00004-9-1 loss: 0.879283  [   64/  306]
train() client id: f_00004-9-2 loss: 0.803155  [   96/  306]
train() client id: f_00004-9-3 loss: 0.899826  [  128/  306]
train() client id: f_00004-9-4 loss: 0.921654  [  160/  306]
train() client id: f_00004-9-5 loss: 0.980960  [  192/  306]
train() client id: f_00004-9-6 loss: 0.925427  [  224/  306]
train() client id: f_00004-9-7 loss: 0.834803  [  256/  306]
train() client id: f_00004-9-8 loss: 0.833527  [  288/  306]
train() client id: f_00004-10-0 loss: 0.903858  [   32/  306]
train() client id: f_00004-10-1 loss: 0.853272  [   64/  306]
train() client id: f_00004-10-2 loss: 0.855411  [   96/  306]
train() client id: f_00004-10-3 loss: 0.935553  [  128/  306]
train() client id: f_00004-10-4 loss: 0.827206  [  160/  306]
train() client id: f_00004-10-5 loss: 0.986567  [  192/  306]
train() client id: f_00004-10-6 loss: 0.819547  [  224/  306]
train() client id: f_00004-10-7 loss: 0.831006  [  256/  306]
train() client id: f_00004-10-8 loss: 0.899316  [  288/  306]
train() client id: f_00004-11-0 loss: 0.903845  [   32/  306]
train() client id: f_00004-11-1 loss: 0.873404  [   64/  306]
train() client id: f_00004-11-2 loss: 0.813977  [   96/  306]
train() client id: f_00004-11-3 loss: 0.936137  [  128/  306]
train() client id: f_00004-11-4 loss: 0.897434  [  160/  306]
train() client id: f_00004-11-5 loss: 0.854544  [  192/  306]
train() client id: f_00004-11-6 loss: 0.797409  [  224/  306]
train() client id: f_00004-11-7 loss: 0.916048  [  256/  306]
train() client id: f_00004-11-8 loss: 0.781342  [  288/  306]
train() client id: f_00005-0-0 loss: 0.721379  [   32/  146]
train() client id: f_00005-0-1 loss: 0.549814  [   64/  146]
train() client id: f_00005-0-2 loss: 0.546808  [   96/  146]
train() client id: f_00005-0-3 loss: 0.564809  [  128/  146]
train() client id: f_00005-1-0 loss: 0.806555  [   32/  146]
train() client id: f_00005-1-1 loss: 0.428021  [   64/  146]
train() client id: f_00005-1-2 loss: 0.538350  [   96/  146]
train() client id: f_00005-1-3 loss: 0.589015  [  128/  146]
train() client id: f_00005-2-0 loss: 0.599617  [   32/  146]
train() client id: f_00005-2-1 loss: 0.425783  [   64/  146]
train() client id: f_00005-2-2 loss: 0.714189  [   96/  146]
train() client id: f_00005-2-3 loss: 0.653922  [  128/  146]
train() client id: f_00005-3-0 loss: 0.563425  [   32/  146]
train() client id: f_00005-3-1 loss: 0.555602  [   64/  146]
train() client id: f_00005-3-2 loss: 0.482954  [   96/  146]
train() client id: f_00005-3-3 loss: 0.739969  [  128/  146]
train() client id: f_00005-4-0 loss: 0.604043  [   32/  146]
train() client id: f_00005-4-1 loss: 0.672803  [   64/  146]
train() client id: f_00005-4-2 loss: 0.734996  [   96/  146]
train() client id: f_00005-4-3 loss: 0.387788  [  128/  146]
train() client id: f_00005-5-0 loss: 0.838332  [   32/  146]
train() client id: f_00005-5-1 loss: 0.272774  [   64/  146]
train() client id: f_00005-5-2 loss: 0.761517  [   96/  146]
train() client id: f_00005-5-3 loss: 0.531255  [  128/  146]
train() client id: f_00005-6-0 loss: 0.318058  [   32/  146]
train() client id: f_00005-6-1 loss: 0.759947  [   64/  146]
train() client id: f_00005-6-2 loss: 0.593692  [   96/  146]
train() client id: f_00005-6-3 loss: 0.624946  [  128/  146]
train() client id: f_00005-7-0 loss: 0.486322  [   32/  146]
train() client id: f_00005-7-1 loss: 0.790046  [   64/  146]
train() client id: f_00005-7-2 loss: 0.485043  [   96/  146]
train() client id: f_00005-7-3 loss: 0.723881  [  128/  146]
train() client id: f_00005-8-0 loss: 0.593923  [   32/  146]
train() client id: f_00005-8-1 loss: 0.617013  [   64/  146]
train() client id: f_00005-8-2 loss: 0.413131  [   96/  146]
train() client id: f_00005-8-3 loss: 0.723344  [  128/  146]
train() client id: f_00005-9-0 loss: 0.733074  [   32/  146]
train() client id: f_00005-9-1 loss: 0.611021  [   64/  146]
train() client id: f_00005-9-2 loss: 0.467688  [   96/  146]
train() client id: f_00005-9-3 loss: 0.532854  [  128/  146]
train() client id: f_00005-10-0 loss: 0.598748  [   32/  146]
train() client id: f_00005-10-1 loss: 0.638763  [   64/  146]
train() client id: f_00005-10-2 loss: 0.438700  [   96/  146]
train() client id: f_00005-10-3 loss: 0.591090  [  128/  146]
train() client id: f_00005-11-0 loss: 0.541346  [   32/  146]
train() client id: f_00005-11-1 loss: 0.452462  [   64/  146]
train() client id: f_00005-11-2 loss: 0.644960  [   96/  146]
train() client id: f_00005-11-3 loss: 0.630854  [  128/  146]
train() client id: f_00006-0-0 loss: 0.586395  [   32/   54]
train() client id: f_00006-1-0 loss: 0.634677  [   32/   54]
train() client id: f_00006-2-0 loss: 0.617922  [   32/   54]
train() client id: f_00006-3-0 loss: 0.555805  [   32/   54]
train() client id: f_00006-4-0 loss: 0.622842  [   32/   54]
train() client id: f_00006-5-0 loss: 0.581107  [   32/   54]
train() client id: f_00006-6-0 loss: 0.599943  [   32/   54]
train() client id: f_00006-7-0 loss: 0.590251  [   32/   54]
train() client id: f_00006-8-0 loss: 0.631574  [   32/   54]
train() client id: f_00006-9-0 loss: 0.618248  [   32/   54]
train() client id: f_00006-10-0 loss: 0.577478  [   32/   54]
train() client id: f_00006-11-0 loss: 0.548810  [   32/   54]
train() client id: f_00007-0-0 loss: 0.790618  [   32/  179]
train() client id: f_00007-0-1 loss: 0.686112  [   64/  179]
train() client id: f_00007-0-2 loss: 0.637620  [   96/  179]
train() client id: f_00007-0-3 loss: 0.779464  [  128/  179]
train() client id: f_00007-0-4 loss: 0.895904  [  160/  179]
train() client id: f_00007-1-0 loss: 0.748000  [   32/  179]
train() client id: f_00007-1-1 loss: 0.693994  [   64/  179]
train() client id: f_00007-1-2 loss: 0.838466  [   96/  179]
train() client id: f_00007-1-3 loss: 0.707702  [  128/  179]
train() client id: f_00007-1-4 loss: 0.759099  [  160/  179]
train() client id: f_00007-2-0 loss: 0.853117  [   32/  179]
train() client id: f_00007-2-1 loss: 0.642260  [   64/  179]
train() client id: f_00007-2-2 loss: 0.626468  [   96/  179]
train() client id: f_00007-2-3 loss: 0.688678  [  128/  179]
train() client id: f_00007-2-4 loss: 0.855297  [  160/  179]
train() client id: f_00007-3-0 loss: 0.686338  [   32/  179]
train() client id: f_00007-3-1 loss: 0.786732  [   64/  179]
train() client id: f_00007-3-2 loss: 0.617789  [   96/  179]
train() client id: f_00007-3-3 loss: 0.757058  [  128/  179]
train() client id: f_00007-3-4 loss: 0.841230  [  160/  179]
train() client id: f_00007-4-0 loss: 0.664107  [   32/  179]
train() client id: f_00007-4-1 loss: 0.601972  [   64/  179]
train() client id: f_00007-4-2 loss: 0.838532  [   96/  179]
train() client id: f_00007-4-3 loss: 0.610292  [  128/  179]
train() client id: f_00007-4-4 loss: 0.951638  [  160/  179]
train() client id: f_00007-5-0 loss: 0.680354  [   32/  179]
train() client id: f_00007-5-1 loss: 0.727726  [   64/  179]
train() client id: f_00007-5-2 loss: 0.608493  [   96/  179]
train() client id: f_00007-5-3 loss: 0.791678  [  128/  179]
train() client id: f_00007-5-4 loss: 0.761380  [  160/  179]
train() client id: f_00007-6-0 loss: 0.684947  [   32/  179]
train() client id: f_00007-6-1 loss: 0.786858  [   64/  179]
train() client id: f_00007-6-2 loss: 0.586924  [   96/  179]
train() client id: f_00007-6-3 loss: 0.796900  [  128/  179]
train() client id: f_00007-6-4 loss: 0.686224  [  160/  179]
train() client id: f_00007-7-0 loss: 0.791532  [   32/  179]
train() client id: f_00007-7-1 loss: 0.858645  [   64/  179]
train() client id: f_00007-7-2 loss: 0.661939  [   96/  179]
train() client id: f_00007-7-3 loss: 0.584131  [  128/  179]
train() client id: f_00007-7-4 loss: 0.581209  [  160/  179]
train() client id: f_00007-8-0 loss: 0.710190  [   32/  179]
train() client id: f_00007-8-1 loss: 0.736368  [   64/  179]
train() client id: f_00007-8-2 loss: 0.668265  [   96/  179]
train() client id: f_00007-8-3 loss: 0.880914  [  128/  179]
train() client id: f_00007-8-4 loss: 0.645611  [  160/  179]
train() client id: f_00007-9-0 loss: 0.675748  [   32/  179]
train() client id: f_00007-9-1 loss: 0.685198  [   64/  179]
train() client id: f_00007-9-2 loss: 0.584322  [   96/  179]
train() client id: f_00007-9-3 loss: 0.795834  [  128/  179]
train() client id: f_00007-9-4 loss: 0.721710  [  160/  179]
train() client id: f_00007-10-0 loss: 0.669603  [   32/  179]
train() client id: f_00007-10-1 loss: 0.569551  [   64/  179]
train() client id: f_00007-10-2 loss: 0.861151  [   96/  179]
train() client id: f_00007-10-3 loss: 0.781514  [  128/  179]
train() client id: f_00007-10-4 loss: 0.662454  [  160/  179]
train() client id: f_00007-11-0 loss: 0.593927  [   32/  179]
train() client id: f_00007-11-1 loss: 0.713667  [   64/  179]
train() client id: f_00007-11-2 loss: 0.738945  [   96/  179]
train() client id: f_00007-11-3 loss: 0.776851  [  128/  179]
train() client id: f_00007-11-4 loss: 0.716722  [  160/  179]
train() client id: f_00008-0-0 loss: 0.670984  [   32/  130]
train() client id: f_00008-0-1 loss: 0.791141  [   64/  130]
train() client id: f_00008-0-2 loss: 0.821364  [   96/  130]
train() client id: f_00008-0-3 loss: 0.836874  [  128/  130]
train() client id: f_00008-1-0 loss: 0.767603  [   32/  130]
train() client id: f_00008-1-1 loss: 0.823063  [   64/  130]
train() client id: f_00008-1-2 loss: 0.740596  [   96/  130]
train() client id: f_00008-1-3 loss: 0.801873  [  128/  130]
train() client id: f_00008-2-0 loss: 0.671575  [   32/  130]
train() client id: f_00008-2-1 loss: 0.862553  [   64/  130]
train() client id: f_00008-2-2 loss: 0.798474  [   96/  130]
train() client id: f_00008-2-3 loss: 0.803234  [  128/  130]
train() client id: f_00008-3-0 loss: 0.850856  [   32/  130]
train() client id: f_00008-3-1 loss: 0.825781  [   64/  130]
train() client id: f_00008-3-2 loss: 0.760605  [   96/  130]
train() client id: f_00008-3-3 loss: 0.699433  [  128/  130]
train() client id: f_00008-4-0 loss: 0.767150  [   32/  130]
train() client id: f_00008-4-1 loss: 0.797462  [   64/  130]
train() client id: f_00008-4-2 loss: 0.827498  [   96/  130]
train() client id: f_00008-4-3 loss: 0.744205  [  128/  130]
train() client id: f_00008-5-0 loss: 0.860729  [   32/  130]
train() client id: f_00008-5-1 loss: 0.822849  [   64/  130]
train() client id: f_00008-5-2 loss: 0.637579  [   96/  130]
train() client id: f_00008-5-3 loss: 0.776428  [  128/  130]
train() client id: f_00008-6-0 loss: 0.817688  [   32/  130]
train() client id: f_00008-6-1 loss: 0.756538  [   64/  130]
train() client id: f_00008-6-2 loss: 0.827992  [   96/  130]
train() client id: f_00008-6-3 loss: 0.728538  [  128/  130]
train() client id: f_00008-7-0 loss: 0.808235  [   32/  130]
train() client id: f_00008-7-1 loss: 0.724017  [   64/  130]
train() client id: f_00008-7-2 loss: 0.770601  [   96/  130]
train() client id: f_00008-7-3 loss: 0.830646  [  128/  130]
train() client id: f_00008-8-0 loss: 0.671074  [   32/  130]
train() client id: f_00008-8-1 loss: 0.767799  [   64/  130]
train() client id: f_00008-8-2 loss: 0.799616  [   96/  130]
train() client id: f_00008-8-3 loss: 0.862895  [  128/  130]
train() client id: f_00008-9-0 loss: 0.844203  [   32/  130]
train() client id: f_00008-9-1 loss: 0.766093  [   64/  130]
train() client id: f_00008-9-2 loss: 0.792792  [   96/  130]
train() client id: f_00008-9-3 loss: 0.737147  [  128/  130]
train() client id: f_00008-10-0 loss: 0.852818  [   32/  130]
train() client id: f_00008-10-1 loss: 0.775131  [   64/  130]
train() client id: f_00008-10-2 loss: 0.767434  [   96/  130]
train() client id: f_00008-10-3 loss: 0.738898  [  128/  130]
train() client id: f_00008-11-0 loss: 0.857572  [   32/  130]
train() client id: f_00008-11-1 loss: 0.675325  [   64/  130]
train() client id: f_00008-11-2 loss: 0.846321  [   96/  130]
train() client id: f_00008-11-3 loss: 0.747005  [  128/  130]
train() client id: f_00009-0-0 loss: 1.172806  [   32/  118]
train() client id: f_00009-0-1 loss: 1.149657  [   64/  118]
train() client id: f_00009-0-2 loss: 1.079287  [   96/  118]
train() client id: f_00009-1-0 loss: 1.181565  [   32/  118]
train() client id: f_00009-1-1 loss: 1.019119  [   64/  118]
train() client id: f_00009-1-2 loss: 1.166602  [   96/  118]
train() client id: f_00009-2-0 loss: 1.132488  [   32/  118]
train() client id: f_00009-2-1 loss: 0.927614  [   64/  118]
train() client id: f_00009-2-2 loss: 1.025474  [   96/  118]
train() client id: f_00009-3-0 loss: 0.961997  [   32/  118]
train() client id: f_00009-3-1 loss: 1.016580  [   64/  118]
train() client id: f_00009-3-2 loss: 1.043273  [   96/  118]
train() client id: f_00009-4-0 loss: 1.035491  [   32/  118]
train() client id: f_00009-4-1 loss: 1.121744  [   64/  118]
train() client id: f_00009-4-2 loss: 0.956441  [   96/  118]
train() client id: f_00009-5-0 loss: 0.882956  [   32/  118]
train() client id: f_00009-5-1 loss: 1.066726  [   64/  118]
train() client id: f_00009-5-2 loss: 1.058210  [   96/  118]
train() client id: f_00009-6-0 loss: 1.030702  [   32/  118]
train() client id: f_00009-6-1 loss: 0.854553  [   64/  118]
train() client id: f_00009-6-2 loss: 1.066844  [   96/  118]
train() client id: f_00009-7-0 loss: 0.935804  [   32/  118]
train() client id: f_00009-7-1 loss: 1.001938  [   64/  118]
train() client id: f_00009-7-2 loss: 1.009797  [   96/  118]
train() client id: f_00009-8-0 loss: 0.949332  [   32/  118]
train() client id: f_00009-8-1 loss: 1.106548  [   64/  118]
train() client id: f_00009-8-2 loss: 0.891611  [   96/  118]
train() client id: f_00009-9-0 loss: 1.017805  [   32/  118]
train() client id: f_00009-9-1 loss: 0.797208  [   64/  118]
train() client id: f_00009-9-2 loss: 1.061784  [   96/  118]
train() client id: f_00009-10-0 loss: 1.067987  [   32/  118]
train() client id: f_00009-10-1 loss: 0.937399  [   64/  118]
train() client id: f_00009-10-2 loss: 0.876705  [   96/  118]
train() client id: f_00009-11-0 loss: 0.817411  [   32/  118]
train() client id: f_00009-11-1 loss: 1.105526  [   64/  118]
train() client id: f_00009-11-2 loss: 0.966008  [   96/  118]
At round 17 accuracy: 0.6392572944297082
At round 17 training accuracy: 0.5734406438631791
At round 17 training loss: 0.8517367135852463
update_location
xs = -3.905658 4.200318 105.009024 18.811294 0.979296 3.956410 -67.443192 -46.324852 89.663977 -32.060879 
ys = 97.587959 80.555839 1.320614 -67.455176 59.350187 42.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 139.780771 128.479126 145.012548 122.082208 116.290170 108.851756 120.646072 110.211924 135.455892 105.090018 
dists_bs = 187.792166 202.564690 329.498987 310.398663 210.579182 222.482573 207.651395 216.552876 307.947298 222.872353 
uav_gains = -103.637956 -102.721371 -104.038043 -102.166582 -101.638713 -100.920940 -102.038061 -101.055779 -103.296101 -100.539069 
bs_gains = -103.230028 -104.150844 -110.066974 -109.340815 -104.622691 -105.291347 -104.452435 -104.962850 -109.244398 -105.312633 
Round 18
-------------------------------
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.61607623 17.94328127  8.48520447  3.03808191 20.69729735  9.97190604
  3.77497723 12.15244223  8.94539714  8.09409538]
obj_prev = 101.71875926654823
eta_min = 1.6526197130074664e-11	eta_max = 0.921782499053972
af = 21.498465320302763	bf = 1.69748013632556	zeta = 23.64831185233304	eta = 0.9090909090909091
af = 21.498465320302763	bf = 1.69748013632556	zeta = 41.211530046727255	eta = 0.5216614208675814
af = 21.498465320302763	bf = 1.69748013632556	zeta = 32.795424289084025	eta = 0.6555324648584754
af = 21.498465320302763	bf = 1.69748013632556	zeta = 31.284347894451706	eta = 0.6871955711793989
af = 21.498465320302763	bf = 1.69748013632556	zeta = 31.2090722305542	eta = 0.6888530732821788
af = 21.498465320302763	bf = 1.69748013632556	zeta = 31.208871892116978	eta = 0.6888574952218328
eta = 0.6888574952218328
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [0.03075603 0.06468531 0.03026784 0.01049611 0.07469322 0.03563794
 0.01318116 0.04369309 0.03173241 0.02880327]
ene_total = [2.70140281 5.11569384 2.67726863 1.23269158 5.83681499 3.09264715
 1.41984069 3.55816331 2.96469034 2.60965854]
ti_comp = [0.33703834 0.33514192 0.33556709 0.34199829 0.33331177 0.33056654
 0.34240076 0.34533101 0.3095942  0.33047606]
ti_coms = [0.07420365 0.07610006 0.0756749  0.0692437  0.07793021 0.08067545
 0.06884123 0.06591098 0.10164779 0.08076593]
t_total = [29.09992447 29.09992447 29.09992447 29.09992447 29.09992447 29.09992447
 29.09992447 29.09992447 29.09992447 29.09992447]
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [1.60070873e-05 1.50605001e-04 1.53909571e-05 6.17897996e-07
 2.34434896e-04 2.58880941e-05 1.22087479e-06 4.37166851e-05
 2.08354902e-05 1.36749361e-05]
ene_total = [0.52620567 0.54914919 0.53657281 0.49001957 0.56803143 0.57270002
 0.4872143  0.4694866  0.72074503 0.57247604]
optimize_network iter = 0 obj = 5.492600646974247
eta = 0.6888574952218328
freqs = [4.56269040e+07 9.65043471e+07 4.50995376e+07 1.53452611e+07
 1.12047077e+08 5.39043395e+07 1.92481414e+07 6.32626258e+07
 5.12483990e+07 4.35784487e+07]
eta_min = 0.6888574952218411	eta_max = 0.6888574952218237
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 0.033943701105718926	eta = 0.909090909090909
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 18.704633302137236	eta = 0.001649746862055949
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 1.8983226351773008	eta = 0.016255355925430464
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 1.8547436567121627	eta = 0.016637291080325793
af = 0.03085791009610811	bf = 1.69748013632556	zeta = 1.8547352592532886	eta = 0.016637366406961725
eta = 0.016637366406961725
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [1.75913676e-04 1.65510932e-03 1.69142568e-04 6.79053635e-06
 2.57637780e-03 2.84503340e-04 1.34170925e-05 4.80434863e-04
 2.28976553e-04 1.50283948e-04]
ene_total = [0.17007518 0.20743924 0.17321761 0.15518366 0.23216334 0.18699632
 0.15443092 0.1583265  0.23270867 0.18419382]
ti_comp = [0.33703834 0.33514192 0.33556709 0.34199829 0.33331177 0.33056654
 0.34240076 0.34533101 0.3095942  0.33047606]
ti_coms = [0.07420365 0.07610006 0.0756749  0.0692437  0.07793021 0.08067545
 0.06884123 0.06591098 0.10164779 0.08076593]
t_total = [29.09992447 29.09992447 29.09992447 29.09992447 29.09992447 29.09992447
 29.09992447 29.09992447 29.09992447 29.09992447]
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [1.60070873e-05 1.50605001e-04 1.53909571e-05 6.17897996e-07
 2.34434896e-04 2.58880941e-05 1.22087479e-06 4.37166851e-05
 2.08354902e-05 1.36749361e-05]
ene_total = [0.52620567 0.54914919 0.53657281 0.49001957 0.56803143 0.57270002
 0.4872143  0.4694866  0.72074503 0.57247604]
optimize_network iter = 1 obj = 5.492600646974393
eta = 0.6888574952218411
freqs = [4.56269040e+07 9.65043471e+07 4.50995376e+07 1.53452611e+07
 1.12047077e+08 5.39043395e+07 1.92481414e+07 6.32626258e+07
 5.12483990e+07 4.35784487e+07]
Done!
ene_coms = [0.00742036 0.00761001 0.00756749 0.00692437 0.00779302 0.00806755
 0.00688412 0.0065911  0.01016478 0.00807659]
ene_comp = [1.57385166e-05 1.48078115e-04 1.51327241e-05 6.07530761e-07
 2.30501493e-04 2.54537376e-05 1.20039067e-06 4.29831965e-05
 2.04859075e-05 1.34454948e-05]
ene_total = [0.0074361  0.00775808 0.00758262 0.00692498 0.00802352 0.008093
 0.00688532 0.00663408 0.01018527 0.00809004]
At round 18 energy consumption: 0.07761301768692713
At round 18 eta: 0.6888574952218411
At round 18 a_n: 22.016777610806663
At round 18 local rounds: 12.204774532627562
At round 18 global rounds: 70.7610733753795
gradient difference: 0.4814021587371826
train() client id: f_00000-0-0 loss: 1.311627  [   32/  126]
train() client id: f_00000-0-1 loss: 1.125472  [   64/  126]
train() client id: f_00000-0-2 loss: 1.208023  [   96/  126]
train() client id: f_00000-1-0 loss: 1.135524  [   32/  126]
train() client id: f_00000-1-1 loss: 1.303636  [   64/  126]
train() client id: f_00000-1-2 loss: 0.974005  [   96/  126]
train() client id: f_00000-2-0 loss: 1.041599  [   32/  126]
train() client id: f_00000-2-1 loss: 1.026965  [   64/  126]
train() client id: f_00000-2-2 loss: 1.055841  [   96/  126]
train() client id: f_00000-3-0 loss: 0.847661  [   32/  126]
train() client id: f_00000-3-1 loss: 1.071646  [   64/  126]
train() client id: f_00000-3-2 loss: 0.953532  [   96/  126]
train() client id: f_00000-4-0 loss: 0.945471  [   32/  126]
train() client id: f_00000-4-1 loss: 0.905069  [   64/  126]
train() client id: f_00000-4-2 loss: 0.918399  [   96/  126]
train() client id: f_00000-5-0 loss: 0.903527  [   32/  126]
train() client id: f_00000-5-1 loss: 0.962740  [   64/  126]
train() client id: f_00000-5-2 loss: 0.873898  [   96/  126]
train() client id: f_00000-6-0 loss: 0.910923  [   32/  126]
train() client id: f_00000-6-1 loss: 0.899632  [   64/  126]
train() client id: f_00000-6-2 loss: 0.795817  [   96/  126]
train() client id: f_00000-7-0 loss: 0.855399  [   32/  126]
train() client id: f_00000-7-1 loss: 0.783505  [   64/  126]
train() client id: f_00000-7-2 loss: 0.942751  [   96/  126]
train() client id: f_00000-8-0 loss: 0.893764  [   32/  126]
train() client id: f_00000-8-1 loss: 0.897658  [   64/  126]
train() client id: f_00000-8-2 loss: 0.774272  [   96/  126]
train() client id: f_00000-9-0 loss: 0.884076  [   32/  126]
train() client id: f_00000-9-1 loss: 0.801927  [   64/  126]
train() client id: f_00000-9-2 loss: 0.814451  [   96/  126]
train() client id: f_00000-10-0 loss: 0.768195  [   32/  126]
train() client id: f_00000-10-1 loss: 0.823797  [   64/  126]
train() client id: f_00000-10-2 loss: 0.824813  [   96/  126]
train() client id: f_00000-11-0 loss: 0.858182  [   32/  126]
train() client id: f_00000-11-1 loss: 0.796867  [   64/  126]
train() client id: f_00000-11-2 loss: 0.801963  [   96/  126]
train() client id: f_00001-0-0 loss: 0.584797  [   32/  265]
train() client id: f_00001-0-1 loss: 0.542181  [   64/  265]
train() client id: f_00001-0-2 loss: 0.619358  [   96/  265]
train() client id: f_00001-0-3 loss: 0.469741  [  128/  265]
train() client id: f_00001-0-4 loss: 0.613511  [  160/  265]
train() client id: f_00001-0-5 loss: 0.505579  [  192/  265]
train() client id: f_00001-0-6 loss: 0.593730  [  224/  265]
train() client id: f_00001-0-7 loss: 0.578799  [  256/  265]
train() client id: f_00001-1-0 loss: 0.517289  [   32/  265]
train() client id: f_00001-1-1 loss: 0.634234  [   64/  265]
train() client id: f_00001-1-2 loss: 0.477452  [   96/  265]
train() client id: f_00001-1-3 loss: 0.540124  [  128/  265]
train() client id: f_00001-1-4 loss: 0.464252  [  160/  265]
train() client id: f_00001-1-5 loss: 0.529871  [  192/  265]
train() client id: f_00001-1-6 loss: 0.517208  [  224/  265]
train() client id: f_00001-1-7 loss: 0.688845  [  256/  265]
train() client id: f_00001-2-0 loss: 0.523986  [   32/  265]
train() client id: f_00001-2-1 loss: 0.667694  [   64/  265]
train() client id: f_00001-2-2 loss: 0.556346  [   96/  265]
train() client id: f_00001-2-3 loss: 0.521829  [  128/  265]
train() client id: f_00001-2-4 loss: 0.530163  [  160/  265]
train() client id: f_00001-2-5 loss: 0.453868  [  192/  265]
train() client id: f_00001-2-6 loss: 0.526047  [  224/  265]
train() client id: f_00001-2-7 loss: 0.601372  [  256/  265]
train() client id: f_00001-3-0 loss: 0.538471  [   32/  265]
train() client id: f_00001-3-1 loss: 0.569964  [   64/  265]
train() client id: f_00001-3-2 loss: 0.518088  [   96/  265]
train() client id: f_00001-3-3 loss: 0.614558  [  128/  265]
train() client id: f_00001-3-4 loss: 0.457420  [  160/  265]
train() client id: f_00001-3-5 loss: 0.580620  [  192/  265]
train() client id: f_00001-3-6 loss: 0.440513  [  224/  265]
train() client id: f_00001-3-7 loss: 0.581249  [  256/  265]
train() client id: f_00001-4-0 loss: 0.577961  [   32/  265]
train() client id: f_00001-4-1 loss: 0.457599  [   64/  265]
train() client id: f_00001-4-2 loss: 0.673142  [   96/  265]
train() client id: f_00001-4-3 loss: 0.543272  [  128/  265]
train() client id: f_00001-4-4 loss: 0.449062  [  160/  265]
train() client id: f_00001-4-5 loss: 0.537809  [  192/  265]
train() client id: f_00001-4-6 loss: 0.558547  [  224/  265]
train() client id: f_00001-4-7 loss: 0.540568  [  256/  265]
train() client id: f_00001-5-0 loss: 0.505199  [   32/  265]
train() client id: f_00001-5-1 loss: 0.512589  [   64/  265]
train() client id: f_00001-5-2 loss: 0.430382  [   96/  265]
train() client id: f_00001-5-3 loss: 0.582025  [  128/  265]
train() client id: f_00001-5-4 loss: 0.555677  [  160/  265]
train() client id: f_00001-5-5 loss: 0.510534  [  192/  265]
train() client id: f_00001-5-6 loss: 0.504874  [  224/  265]
train() client id: f_00001-5-7 loss: 0.635941  [  256/  265]
train() client id: f_00001-6-0 loss: 0.611356  [   32/  265]
train() client id: f_00001-6-1 loss: 0.639789  [   64/  265]
train() client id: f_00001-6-2 loss: 0.521393  [   96/  265]
train() client id: f_00001-6-3 loss: 0.539964  [  128/  265]
train() client id: f_00001-6-4 loss: 0.499994  [  160/  265]
train() client id: f_00001-6-5 loss: 0.472879  [  192/  265]
train() client id: f_00001-6-6 loss: 0.432997  [  224/  265]
train() client id: f_00001-6-7 loss: 0.449195  [  256/  265]
train() client id: f_00001-7-0 loss: 0.519379  [   32/  265]
train() client id: f_00001-7-1 loss: 0.532007  [   64/  265]
train() client id: f_00001-7-2 loss: 0.512821  [   96/  265]
train() client id: f_00001-7-3 loss: 0.640834  [  128/  265]
train() client id: f_00001-7-4 loss: 0.599824  [  160/  265]
train() client id: f_00001-7-5 loss: 0.441145  [  192/  265]
train() client id: f_00001-7-6 loss: 0.510482  [  224/  265]
train() client id: f_00001-7-7 loss: 0.552258  [  256/  265]
train() client id: f_00001-8-0 loss: 0.433492  [   32/  265]
train() client id: f_00001-8-1 loss: 0.452770  [   64/  265]
train() client id: f_00001-8-2 loss: 0.446521  [   96/  265]
train() client id: f_00001-8-3 loss: 0.442640  [  128/  265]
train() client id: f_00001-8-4 loss: 0.713106  [  160/  265]
train() client id: f_00001-8-5 loss: 0.574604  [  192/  265]
train() client id: f_00001-8-6 loss: 0.692953  [  224/  265]
train() client id: f_00001-8-7 loss: 0.547202  [  256/  265]
train() client id: f_00001-9-0 loss: 0.650455  [   32/  265]
train() client id: f_00001-9-1 loss: 0.632678  [   64/  265]
train() client id: f_00001-9-2 loss: 0.499167  [   96/  265]
train() client id: f_00001-9-3 loss: 0.622123  [  128/  265]
train() client id: f_00001-9-4 loss: 0.504045  [  160/  265]
train() client id: f_00001-9-5 loss: 0.445416  [  192/  265]
train() client id: f_00001-9-6 loss: 0.451924  [  224/  265]
train() client id: f_00001-9-7 loss: 0.496411  [  256/  265]
train() client id: f_00001-10-0 loss: 0.508721  [   32/  265]
train() client id: f_00001-10-1 loss: 0.621994  [   64/  265]
train() client id: f_00001-10-2 loss: 0.608561  [   96/  265]
train() client id: f_00001-10-3 loss: 0.478355  [  128/  265]
train() client id: f_00001-10-4 loss: 0.511099  [  160/  265]
train() client id: f_00001-10-5 loss: 0.432826  [  192/  265]
train() client id: f_00001-10-6 loss: 0.581049  [  224/  265]
train() client id: f_00001-10-7 loss: 0.570522  [  256/  265]
train() client id: f_00001-11-0 loss: 0.444813  [   32/  265]
train() client id: f_00001-11-1 loss: 0.568945  [   64/  265]
train() client id: f_00001-11-2 loss: 0.504060  [   96/  265]
train() client id: f_00001-11-3 loss: 0.594956  [  128/  265]
train() client id: f_00001-11-4 loss: 0.654079  [  160/  265]
train() client id: f_00001-11-5 loss: 0.501494  [  192/  265]
train() client id: f_00001-11-6 loss: 0.450715  [  224/  265]
train() client id: f_00001-11-7 loss: 0.590472  [  256/  265]
train() client id: f_00002-0-0 loss: 1.293807  [   32/  124]
train() client id: f_00002-0-1 loss: 1.196875  [   64/  124]
train() client id: f_00002-0-2 loss: 1.092555  [   96/  124]
train() client id: f_00002-1-0 loss: 1.229470  [   32/  124]
train() client id: f_00002-1-1 loss: 1.090755  [   64/  124]
train() client id: f_00002-1-2 loss: 1.117577  [   96/  124]
train() client id: f_00002-2-0 loss: 1.057089  [   32/  124]
train() client id: f_00002-2-1 loss: 1.145970  [   64/  124]
train() client id: f_00002-2-2 loss: 1.039780  [   96/  124]
train() client id: f_00002-3-0 loss: 1.004822  [   32/  124]
train() client id: f_00002-3-1 loss: 1.151121  [   64/  124]
train() client id: f_00002-3-2 loss: 1.042239  [   96/  124]
train() client id: f_00002-4-0 loss: 0.966139  [   32/  124]
train() client id: f_00002-4-1 loss: 1.124750  [   64/  124]
train() client id: f_00002-4-2 loss: 0.982506  [   96/  124]
train() client id: f_00002-5-0 loss: 0.915739  [   32/  124]
train() client id: f_00002-5-1 loss: 0.979831  [   64/  124]
train() client id: f_00002-5-2 loss: 1.146948  [   96/  124]
train() client id: f_00002-6-0 loss: 0.988913  [   32/  124]
train() client id: f_00002-6-1 loss: 0.972648  [   64/  124]
train() client id: f_00002-6-2 loss: 0.945988  [   96/  124]
train() client id: f_00002-7-0 loss: 0.996417  [   32/  124]
train() client id: f_00002-7-1 loss: 1.071446  [   64/  124]
train() client id: f_00002-7-2 loss: 0.963257  [   96/  124]
train() client id: f_00002-8-0 loss: 1.010577  [   32/  124]
train() client id: f_00002-8-1 loss: 1.010028  [   64/  124]
train() client id: f_00002-8-2 loss: 0.856485  [   96/  124]
train() client id: f_00002-9-0 loss: 0.959586  [   32/  124]
train() client id: f_00002-9-1 loss: 0.911337  [   64/  124]
train() client id: f_00002-9-2 loss: 1.043108  [   96/  124]
train() client id: f_00002-10-0 loss: 1.031155  [   32/  124]
train() client id: f_00002-10-1 loss: 0.912555  [   64/  124]
train() client id: f_00002-10-2 loss: 0.909149  [   96/  124]
train() client id: f_00002-11-0 loss: 0.915413  [   32/  124]
train() client id: f_00002-11-1 loss: 0.960226  [   64/  124]
train() client id: f_00002-11-2 loss: 0.983325  [   96/  124]
train() client id: f_00003-0-0 loss: 0.766800  [   32/   43]
train() client id: f_00003-1-0 loss: 0.726873  [   32/   43]
train() client id: f_00003-2-0 loss: 0.800203  [   32/   43]
train() client id: f_00003-3-0 loss: 0.724134  [   32/   43]
train() client id: f_00003-4-0 loss: 0.755198  [   32/   43]
train() client id: f_00003-5-0 loss: 0.756726  [   32/   43]
train() client id: f_00003-6-0 loss: 0.727784  [   32/   43]
train() client id: f_00003-7-0 loss: 0.864824  [   32/   43]
train() client id: f_00003-8-0 loss: 0.838868  [   32/   43]
train() client id: f_00003-9-0 loss: 0.694521  [   32/   43]
train() client id: f_00003-10-0 loss: 0.867858  [   32/   43]
train() client id: f_00003-11-0 loss: 0.749544  [   32/   43]
train() client id: f_00004-0-0 loss: 1.101710  [   32/  306]
train() client id: f_00004-0-1 loss: 1.029117  [   64/  306]
train() client id: f_00004-0-2 loss: 0.921195  [   96/  306]
train() client id: f_00004-0-3 loss: 1.027313  [  128/  306]
train() client id: f_00004-0-4 loss: 0.911778  [  160/  306]
train() client id: f_00004-0-5 loss: 0.851345  [  192/  306]
train() client id: f_00004-0-6 loss: 0.983430  [  224/  306]
train() client id: f_00004-0-7 loss: 0.880357  [  256/  306]
train() client id: f_00004-0-8 loss: 1.012864  [  288/  306]
train() client id: f_00004-1-0 loss: 1.011009  [   32/  306]
train() client id: f_00004-1-1 loss: 0.966120  [   64/  306]
train() client id: f_00004-1-2 loss: 0.952581  [   96/  306]
train() client id: f_00004-1-3 loss: 0.885028  [  128/  306]
train() client id: f_00004-1-4 loss: 1.061923  [  160/  306]
train() client id: f_00004-1-5 loss: 0.977013  [  192/  306]
train() client id: f_00004-1-6 loss: 1.004508  [  224/  306]
train() client id: f_00004-1-7 loss: 0.851283  [  256/  306]
train() client id: f_00004-1-8 loss: 0.959640  [  288/  306]
train() client id: f_00004-2-0 loss: 1.004032  [   32/  306]
train() client id: f_00004-2-1 loss: 0.891289  [   64/  306]
train() client id: f_00004-2-2 loss: 0.969607  [   96/  306]
train() client id: f_00004-2-3 loss: 0.908672  [  128/  306]
train() client id: f_00004-2-4 loss: 1.016468  [  160/  306]
train() client id: f_00004-2-5 loss: 0.943363  [  192/  306]
train() client id: f_00004-2-6 loss: 1.082839  [  224/  306]
train() client id: f_00004-2-7 loss: 1.041162  [  256/  306]
train() client id: f_00004-2-8 loss: 0.901274  [  288/  306]
train() client id: f_00004-3-0 loss: 0.967988  [   32/  306]
train() client id: f_00004-3-1 loss: 0.968175  [   64/  306]
train() client id: f_00004-3-2 loss: 1.016527  [   96/  306]
train() client id: f_00004-3-3 loss: 0.927691  [  128/  306]
train() client id: f_00004-3-4 loss: 0.963727  [  160/  306]
train() client id: f_00004-3-5 loss: 0.927671  [  192/  306]
train() client id: f_00004-3-6 loss: 1.001994  [  224/  306]
train() client id: f_00004-3-7 loss: 0.947200  [  256/  306]
train() client id: f_00004-3-8 loss: 1.000795  [  288/  306]
train() client id: f_00004-4-0 loss: 0.908273  [   32/  306]
train() client id: f_00004-4-1 loss: 1.027120  [   64/  306]
train() client id: f_00004-4-2 loss: 0.909087  [   96/  306]
train() client id: f_00004-4-3 loss: 1.001255  [  128/  306]
train() client id: f_00004-4-4 loss: 1.144102  [  160/  306]
train() client id: f_00004-4-5 loss: 0.976495  [  192/  306]
train() client id: f_00004-4-6 loss: 0.872887  [  224/  306]
train() client id: f_00004-4-7 loss: 0.817853  [  256/  306]
train() client id: f_00004-4-8 loss: 1.044021  [  288/  306]
train() client id: f_00004-5-0 loss: 0.932766  [   32/  306]
train() client id: f_00004-5-1 loss: 0.896446  [   64/  306]
train() client id: f_00004-5-2 loss: 0.947332  [   96/  306]
train() client id: f_00004-5-3 loss: 0.990705  [  128/  306]
train() client id: f_00004-5-4 loss: 1.085371  [  160/  306]
train() client id: f_00004-5-5 loss: 0.964099  [  192/  306]
train() client id: f_00004-5-6 loss: 0.919312  [  224/  306]
train() client id: f_00004-5-7 loss: 1.054756  [  256/  306]
train() client id: f_00004-5-8 loss: 0.900536  [  288/  306]
train() client id: f_00004-6-0 loss: 1.024849  [   32/  306]
train() client id: f_00004-6-1 loss: 1.030535  [   64/  306]
train() client id: f_00004-6-2 loss: 0.949857  [   96/  306]
train() client id: f_00004-6-3 loss: 0.918198  [  128/  306]
train() client id: f_00004-6-4 loss: 1.099128  [  160/  306]
train() client id: f_00004-6-5 loss: 0.937380  [  192/  306]
train() client id: f_00004-6-6 loss: 0.905763  [  224/  306]
train() client id: f_00004-6-7 loss: 0.884302  [  256/  306]
train() client id: f_00004-6-8 loss: 0.930983  [  288/  306]
train() client id: f_00004-7-0 loss: 1.058581  [   32/  306]
train() client id: f_00004-7-1 loss: 0.924058  [   64/  306]
train() client id: f_00004-7-2 loss: 1.024613  [   96/  306]
train() client id: f_00004-7-3 loss: 1.061084  [  128/  306]
train() client id: f_00004-7-4 loss: 0.908119  [  160/  306]
train() client id: f_00004-7-5 loss: 0.913337  [  192/  306]
train() client id: f_00004-7-6 loss: 0.906636  [  224/  306]
train() client id: f_00004-7-7 loss: 0.953611  [  256/  306]
train() client id: f_00004-7-8 loss: 0.879863  [  288/  306]
train() client id: f_00004-8-0 loss: 0.904584  [   32/  306]
train() client id: f_00004-8-1 loss: 1.107803  [   64/  306]
train() client id: f_00004-8-2 loss: 0.938554  [   96/  306]
train() client id: f_00004-8-3 loss: 0.995940  [  128/  306]
train() client id: f_00004-8-4 loss: 1.012102  [  160/  306]
train() client id: f_00004-8-5 loss: 1.057533  [  192/  306]
train() client id: f_00004-8-6 loss: 0.886741  [  224/  306]
train() client id: f_00004-8-7 loss: 0.929656  [  256/  306]
train() client id: f_00004-8-8 loss: 0.911791  [  288/  306]
train() client id: f_00004-9-0 loss: 1.045233  [   32/  306]
train() client id: f_00004-9-1 loss: 0.931424  [   64/  306]
train() client id: f_00004-9-2 loss: 0.916137  [   96/  306]
train() client id: f_00004-9-3 loss: 0.921166  [  128/  306]
train() client id: f_00004-9-4 loss: 0.946255  [  160/  306]
train() client id: f_00004-9-5 loss: 0.975531  [  192/  306]
train() client id: f_00004-9-6 loss: 0.980444  [  224/  306]
train() client id: f_00004-9-7 loss: 0.956059  [  256/  306]
train() client id: f_00004-9-8 loss: 1.088083  [  288/  306]
train() client id: f_00004-10-0 loss: 0.990710  [   32/  306]
train() client id: f_00004-10-1 loss: 0.937078  [   64/  306]
train() client id: f_00004-10-2 loss: 1.032402  [   96/  306]
train() client id: f_00004-10-3 loss: 0.990251  [  128/  306]
train() client id: f_00004-10-4 loss: 0.930539  [  160/  306]
train() client id: f_00004-10-5 loss: 0.848854  [  192/  306]
train() client id: f_00004-10-6 loss: 1.026655  [  224/  306]
train() client id: f_00004-10-7 loss: 1.013475  [  256/  306]
train() client id: f_00004-10-8 loss: 0.943640  [  288/  306]
train() client id: f_00004-11-0 loss: 0.881701  [   32/  306]
train() client id: f_00004-11-1 loss: 1.115446  [   64/  306]
train() client id: f_00004-11-2 loss: 0.978889  [   96/  306]
train() client id: f_00004-11-3 loss: 0.963171  [  128/  306]
train() client id: f_00004-11-4 loss: 0.960376  [  160/  306]
train() client id: f_00004-11-5 loss: 1.028390  [  192/  306]
train() client id: f_00004-11-6 loss: 0.912042  [  224/  306]
train() client id: f_00004-11-7 loss: 0.869457  [  256/  306]
train() client id: f_00004-11-8 loss: 1.022389  [  288/  306]
train() client id: f_00005-0-0 loss: 0.686065  [   32/  146]
train() client id: f_00005-0-1 loss: 0.530004  [   64/  146]
train() client id: f_00005-0-2 loss: 0.772371  [   96/  146]
train() client id: f_00005-0-3 loss: 0.584797  [  128/  146]
train() client id: f_00005-1-0 loss: 0.659742  [   32/  146]
train() client id: f_00005-1-1 loss: 0.636565  [   64/  146]
train() client id: f_00005-1-2 loss: 0.603411  [   96/  146]
train() client id: f_00005-1-3 loss: 0.468809  [  128/  146]
train() client id: f_00005-2-0 loss: 0.656503  [   32/  146]
train() client id: f_00005-2-1 loss: 0.902612  [   64/  146]
train() client id: f_00005-2-2 loss: 0.627541  [   96/  146]
train() client id: f_00005-2-3 loss: 0.387333  [  128/  146]
train() client id: f_00005-3-0 loss: 0.574969  [   32/  146]
train() client id: f_00005-3-1 loss: 0.625670  [   64/  146]
train() client id: f_00005-3-2 loss: 0.694888  [   96/  146]
train() client id: f_00005-3-3 loss: 0.625434  [  128/  146]
train() client id: f_00005-4-0 loss: 0.518934  [   32/  146]
train() client id: f_00005-4-1 loss: 0.545570  [   64/  146]
train() client id: f_00005-4-2 loss: 0.612013  [   96/  146]
train() client id: f_00005-4-3 loss: 0.813495  [  128/  146]
train() client id: f_00005-5-0 loss: 0.538473  [   32/  146]
train() client id: f_00005-5-1 loss: 0.675785  [   64/  146]
train() client id: f_00005-5-2 loss: 0.509819  [   96/  146]
train() client id: f_00005-5-3 loss: 0.516401  [  128/  146]
train() client id: f_00005-6-0 loss: 0.597303  [   32/  146]
train() client id: f_00005-6-1 loss: 0.513098  [   64/  146]
train() client id: f_00005-6-2 loss: 0.547902  [   96/  146]
train() client id: f_00005-6-3 loss: 0.680780  [  128/  146]
train() client id: f_00005-7-0 loss: 0.697623  [   32/  146]
train() client id: f_00005-7-1 loss: 0.710564  [   64/  146]
train() client id: f_00005-7-2 loss: 0.472299  [   96/  146]
train() client id: f_00005-7-3 loss: 0.644389  [  128/  146]
train() client id: f_00005-8-0 loss: 0.535887  [   32/  146]
train() client id: f_00005-8-1 loss: 0.571531  [   64/  146]
train() client id: f_00005-8-2 loss: 0.664543  [   96/  146]
train() client id: f_00005-8-3 loss: 0.730660  [  128/  146]
train() client id: f_00005-9-0 loss: 0.835447  [   32/  146]
train() client id: f_00005-9-1 loss: 0.544338  [   64/  146]
train() client id: f_00005-9-2 loss: 0.487516  [   96/  146]
train() client id: f_00005-9-3 loss: 0.673576  [  128/  146]
train() client id: f_00005-10-0 loss: 0.558406  [   32/  146]
train() client id: f_00005-10-1 loss: 0.656557  [   64/  146]
train() client id: f_00005-10-2 loss: 0.660339  [   96/  146]
train() client id: f_00005-10-3 loss: 0.519452  [  128/  146]
train() client id: f_00005-11-0 loss: 0.794203  [   32/  146]
train() client id: f_00005-11-1 loss: 0.485251  [   64/  146]
train() client id: f_00005-11-2 loss: 0.775920  [   96/  146]
train() client id: f_00005-11-3 loss: 0.479653  [  128/  146]
train() client id: f_00006-0-0 loss: 0.678087  [   32/   54]
train() client id: f_00006-1-0 loss: 0.619953  [   32/   54]
train() client id: f_00006-2-0 loss: 0.660911  [   32/   54]
train() client id: f_00006-3-0 loss: 0.663068  [   32/   54]
train() client id: f_00006-4-0 loss: 0.655270  [   32/   54]
train() client id: f_00006-5-0 loss: 0.693626  [   32/   54]
train() client id: f_00006-6-0 loss: 0.699701  [   32/   54]
train() client id: f_00006-7-0 loss: 0.621436  [   32/   54]
train() client id: f_00006-8-0 loss: 0.654850  [   32/   54]
train() client id: f_00006-9-0 loss: 0.687276  [   32/   54]
train() client id: f_00006-10-0 loss: 0.612479  [   32/   54]
train() client id: f_00006-11-0 loss: 0.604105  [   32/   54]
train() client id: f_00007-0-0 loss: 0.512634  [   32/  179]
train() client id: f_00007-0-1 loss: 0.595103  [   64/  179]
train() client id: f_00007-0-2 loss: 0.614123  [   96/  179]
train() client id: f_00007-0-3 loss: 0.455722  [  128/  179]
train() client id: f_00007-0-4 loss: 0.673293  [  160/  179]
train() client id: f_00007-1-0 loss: 0.616604  [   32/  179]
train() client id: f_00007-1-1 loss: 0.508094  [   64/  179]
train() client id: f_00007-1-2 loss: 0.618964  [   96/  179]
train() client id: f_00007-1-3 loss: 0.549715  [  128/  179]
train() client id: f_00007-1-4 loss: 0.619587  [  160/  179]
train() client id: f_00007-2-0 loss: 0.467660  [   32/  179]
train() client id: f_00007-2-1 loss: 0.714748  [   64/  179]
train() client id: f_00007-2-2 loss: 0.640992  [   96/  179]
train() client id: f_00007-2-3 loss: 0.581626  [  128/  179]
train() client id: f_00007-2-4 loss: 0.467434  [  160/  179]
train() client id: f_00007-3-0 loss: 0.541548  [   32/  179]
train() client id: f_00007-3-1 loss: 0.709549  [   64/  179]
train() client id: f_00007-3-2 loss: 0.513316  [   96/  179]
train() client id: f_00007-3-3 loss: 0.634773  [  128/  179]
train() client id: f_00007-3-4 loss: 0.508590  [  160/  179]
train() client id: f_00007-4-0 loss: 0.502215  [   32/  179]
train() client id: f_00007-4-1 loss: 0.618836  [   64/  179]
train() client id: f_00007-4-2 loss: 0.432625  [   96/  179]
train() client id: f_00007-4-3 loss: 0.548884  [  128/  179]
train() client id: f_00007-4-4 loss: 0.707243  [  160/  179]
train() client id: f_00007-5-0 loss: 0.502383  [   32/  179]
train() client id: f_00007-5-1 loss: 0.745681  [   64/  179]
train() client id: f_00007-5-2 loss: 0.457470  [   96/  179]
train() client id: f_00007-5-3 loss: 0.504421  [  128/  179]
train() client id: f_00007-5-4 loss: 0.511302  [  160/  179]
train() client id: f_00007-6-0 loss: 0.609053  [   32/  179]
train() client id: f_00007-6-1 loss: 0.608378  [   64/  179]
train() client id: f_00007-6-2 loss: 0.584003  [   96/  179]
train() client id: f_00007-6-3 loss: 0.505502  [  128/  179]
train() client id: f_00007-6-4 loss: 0.505025  [  160/  179]
train() client id: f_00007-7-0 loss: 0.507544  [   32/  179]
train() client id: f_00007-7-1 loss: 0.474740  [   64/  179]
train() client id: f_00007-7-2 loss: 0.701594  [   96/  179]
train() client id: f_00007-7-3 loss: 0.530860  [  128/  179]
train() client id: f_00007-7-4 loss: 0.588322  [  160/  179]
train() client id: f_00007-8-0 loss: 0.692482  [   32/  179]
train() client id: f_00007-8-1 loss: 0.548746  [   64/  179]
train() client id: f_00007-8-2 loss: 0.433806  [   96/  179]
train() client id: f_00007-8-3 loss: 0.434658  [  128/  179]
train() client id: f_00007-8-4 loss: 0.579889  [  160/  179]
train() client id: f_00007-9-0 loss: 0.488762  [   32/  179]
train() client id: f_00007-9-1 loss: 0.558781  [   64/  179]
train() client id: f_00007-9-2 loss: 0.520804  [   96/  179]
train() client id: f_00007-9-3 loss: 0.589886  [  128/  179]
train() client id: f_00007-9-4 loss: 0.629313  [  160/  179]
train() client id: f_00007-10-0 loss: 0.397083  [   32/  179]
train() client id: f_00007-10-1 loss: 0.745189  [   64/  179]
train() client id: f_00007-10-2 loss: 0.700187  [   96/  179]
train() client id: f_00007-10-3 loss: 0.508076  [  128/  179]
train() client id: f_00007-10-4 loss: 0.410433  [  160/  179]
train() client id: f_00007-11-0 loss: 0.501092  [   32/  179]
train() client id: f_00007-11-1 loss: 0.674514  [   64/  179]
train() client id: f_00007-11-2 loss: 0.594017  [   96/  179]
train() client id: f_00007-11-3 loss: 0.478688  [  128/  179]
train() client id: f_00007-11-4 loss: 0.389272  [  160/  179]
train() client id: f_00008-0-0 loss: 0.737274  [   32/  130]
train() client id: f_00008-0-1 loss: 0.836963  [   64/  130]
train() client id: f_00008-0-2 loss: 0.848141  [   96/  130]
train() client id: f_00008-0-3 loss: 0.872206  [  128/  130]
train() client id: f_00008-1-0 loss: 0.902902  [   32/  130]
train() client id: f_00008-1-1 loss: 0.864714  [   64/  130]
train() client id: f_00008-1-2 loss: 0.715481  [   96/  130]
train() client id: f_00008-1-3 loss: 0.752612  [  128/  130]
train() client id: f_00008-2-0 loss: 0.760352  [   32/  130]
train() client id: f_00008-2-1 loss: 0.843838  [   64/  130]
train() client id: f_00008-2-2 loss: 0.870991  [   96/  130]
train() client id: f_00008-2-3 loss: 0.790583  [  128/  130]
train() client id: f_00008-3-0 loss: 0.815198  [   32/  130]
train() client id: f_00008-3-1 loss: 0.774936  [   64/  130]
train() client id: f_00008-3-2 loss: 0.872815  [   96/  130]
train() client id: f_00008-3-3 loss: 0.823887  [  128/  130]
train() client id: f_00008-4-0 loss: 0.869755  [   32/  130]
train() client id: f_00008-4-1 loss: 0.846963  [   64/  130]
train() client id: f_00008-4-2 loss: 0.709830  [   96/  130]
train() client id: f_00008-4-3 loss: 0.833767  [  128/  130]
train() client id: f_00008-5-0 loss: 0.834172  [   32/  130]
train() client id: f_00008-5-1 loss: 0.793046  [   64/  130]
train() client id: f_00008-5-2 loss: 0.744164  [   96/  130]
train() client id: f_00008-5-3 loss: 0.861500  [  128/  130]
train() client id: f_00008-6-0 loss: 0.790142  [   32/  130]
train() client id: f_00008-6-1 loss: 0.882725  [   64/  130]
train() client id: f_00008-6-2 loss: 0.798742  [   96/  130]
train() client id: f_00008-6-3 loss: 0.818791  [  128/  130]
train() client id: f_00008-7-0 loss: 0.868684  [   32/  130]
train() client id: f_00008-7-1 loss: 0.811208  [   64/  130]
train() client id: f_00008-7-2 loss: 0.820480  [   96/  130]
train() client id: f_00008-7-3 loss: 0.785517  [  128/  130]
train() client id: f_00008-8-0 loss: 0.741441  [   32/  130]
train() client id: f_00008-8-1 loss: 0.920680  [   64/  130]
train() client id: f_00008-8-2 loss: 0.747545  [   96/  130]
train() client id: f_00008-8-3 loss: 0.872462  [  128/  130]
train() client id: f_00008-9-0 loss: 0.918929  [   32/  130]
train() client id: f_00008-9-1 loss: 0.714777  [   64/  130]
train() client id: f_00008-9-2 loss: 0.812350  [   96/  130]
train() client id: f_00008-9-3 loss: 0.789629  [  128/  130]
train() client id: f_00008-10-0 loss: 0.787585  [   32/  130]
train() client id: f_00008-10-1 loss: 0.677526  [   64/  130]
train() client id: f_00008-10-2 loss: 0.832063  [   96/  130]
train() client id: f_00008-10-3 loss: 0.965513  [  128/  130]
train() client id: f_00008-11-0 loss: 0.765545  [   32/  130]
train() client id: f_00008-11-1 loss: 0.814467  [   64/  130]
train() client id: f_00008-11-2 loss: 0.871965  [   96/  130]
train() client id: f_00008-11-3 loss: 0.833620  [  128/  130]
train() client id: f_00009-0-0 loss: 1.233105  [   32/  118]
train() client id: f_00009-0-1 loss: 1.092102  [   64/  118]
train() client id: f_00009-0-2 loss: 1.029596  [   96/  118]
train() client id: f_00009-1-0 loss: 1.109301  [   32/  118]
train() client id: f_00009-1-1 loss: 1.056602  [   64/  118]
train() client id: f_00009-1-2 loss: 0.936542  [   96/  118]
train() client id: f_00009-2-0 loss: 1.044843  [   32/  118]
train() client id: f_00009-2-1 loss: 0.928240  [   64/  118]
train() client id: f_00009-2-2 loss: 1.060750  [   96/  118]
train() client id: f_00009-3-0 loss: 1.074312  [   32/  118]
train() client id: f_00009-3-1 loss: 0.979402  [   64/  118]
train() client id: f_00009-3-2 loss: 0.980523  [   96/  118]
train() client id: f_00009-4-0 loss: 1.019749  [   32/  118]
train() client id: f_00009-4-1 loss: 0.844103  [   64/  118]
train() client id: f_00009-4-2 loss: 0.987219  [   96/  118]
train() client id: f_00009-5-0 loss: 1.010552  [   32/  118]
train() client id: f_00009-5-1 loss: 0.899967  [   64/  118]
train() client id: f_00009-5-2 loss: 0.912848  [   96/  118]
train() client id: f_00009-6-0 loss: 0.891699  [   32/  118]
train() client id: f_00009-6-1 loss: 0.931755  [   64/  118]
train() client id: f_00009-6-2 loss: 0.887484  [   96/  118]
train() client id: f_00009-7-0 loss: 0.865381  [   32/  118]
train() client id: f_00009-7-1 loss: 0.981851  [   64/  118]
train() client id: f_00009-7-2 loss: 0.891998  [   96/  118]
train() client id: f_00009-8-0 loss: 0.923333  [   32/  118]
train() client id: f_00009-8-1 loss: 0.903269  [   64/  118]
train() client id: f_00009-8-2 loss: 0.903882  [   96/  118]
train() client id: f_00009-9-0 loss: 0.835009  [   32/  118]
train() client id: f_00009-9-1 loss: 1.005687  [   64/  118]
train() client id: f_00009-9-2 loss: 0.873653  [   96/  118]
train() client id: f_00009-10-0 loss: 0.852042  [   32/  118]
train() client id: f_00009-10-1 loss: 0.894898  [   64/  118]
train() client id: f_00009-10-2 loss: 0.949494  [   96/  118]
train() client id: f_00009-11-0 loss: 0.982224  [   32/  118]
train() client id: f_00009-11-1 loss: 0.948512  [   64/  118]
train() client id: f_00009-11-2 loss: 0.875107  [   96/  118]
At round 18 accuracy: 0.6419098143236074
At round 18 training accuracy: 0.5814889336016097
At round 18 training loss: 0.8441948173418158
update_location
xs = -3.905658 4.200318 110.009024 18.811294 0.979296 3.956410 -72.443192 -51.324852 94.663977 -37.060879 
ys = 102.587959 85.555839 1.320614 -72.455176 64.350187 47.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 143.316236 131.671729 148.673230 124.914440 118.919744 110.913688 123.510755 112.405145 138.816204 106.721697 
dists_bs = 185.786914 200.282331 333.758405 314.319713 207.875188 219.548712 205.106153 213.619748 312.254989 219.699100 
uav_gains = -103.909864 -102.988095 -104.309866 -102.415692 -101.881538 -101.124699 -102.292940 -101.269740 -103.562615 -100.706358 
bs_gains = -103.099483 -104.013053 -110.223161 -109.493464 -104.465533 -105.129924 -104.302462 -104.797018 -109.413322 -105.138251 
Round 19
-------------------------------
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.48419946 17.66297686  8.35544877  2.99255128 20.37392527  9.81527082
  3.718002   11.96472075  8.80869105  7.96655398]
obj_prev = 100.14234023128081
eta_min = 1.126497835179761e-11	eta_max = 0.9220293976525566
af = 21.163983583608065	bf = 1.67717061299475	zeta = 23.280381941968873	eta = 0.9090909090909091
af = 21.163983583608065	bf = 1.67717061299475	zeta = 40.637450630966356	eta = 0.5207999826514902
af = 21.163983583608065	bf = 1.67717061299475	zeta = 32.312864825181585	eta = 0.6549708203871438
af = 21.163983583608065	bf = 1.67717061299475	zeta = 30.81773875501804	eta = 0.6867468035811661
af = 21.163983583608065	bf = 1.67717061299475	zeta = 30.743104385934746	eta = 0.6884140039315868
af = 21.163983583608065	bf = 1.67717061299475	zeta = 30.74290487459993	eta = 0.6884184715119078
eta = 0.6884184715119078
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [0.03080864 0.06479595 0.03031961 0.01051406 0.07482098 0.0356989
 0.0132037  0.04376783 0.03178669 0.02885253]
ene_total = [2.66619823 5.03309398 2.64268475 1.21857109 5.74248846 3.03975216
 1.40293291 3.50736005 2.92607578 2.56374747]
ti_comp = [0.34241459 0.34203063 0.34090455 0.34757484 0.34030078 0.33761632
 0.34796803 0.35108386 0.31483598 0.33758153]
ti_coms = [0.07519738 0.07558134 0.07670742 0.07003712 0.07731119 0.07999565
 0.06964393 0.0665281  0.10277599 0.08003044]
t_total = [29.04992027 29.04992027 29.04992027 29.04992027 29.04992027 29.04992027
 29.04992027 29.04992027 29.04992027 29.04992027]
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [1.55880946e-05 1.45342807e-04 1.49894390e-05 6.01304716e-07
 2.26060309e-04 2.49458102e-05 1.18819712e-06 4.25131485e-05
 2.02510366e-05 1.31726936e-05]
ene_total = [0.52417228 0.53586919 0.53463478 0.48723401 0.55351726 0.55820092
 0.48453973 0.46574004 0.71633895 0.557624  ]
optimize_network iter = 0 obj = 5.417871154985847
eta = 0.6884184715119078
freqs = [4.49873339e+07 9.47224336e+07 4.44693580e+07 1.51248853e+07
 1.09933602e+08 5.28690360e+07 1.89725795e+07 6.23324407e+07
 5.04813506e+07 4.27341730e+07]
eta_min = 0.6884184715119183	eta_max = 0.6884184715118975
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 0.03222576338096967	eta = 0.909090909090909
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 18.47959117596394	eta = 0.0015853244938805354
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 1.8690048705094464	eta = 0.015674730970695012
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 1.8275633379854408	eta = 0.016030168650925096
af = 0.02929614852815424	bf = 1.67717061299475	zeta = 1.827555909002531	eta = 0.016030233813281204
eta = 0.016030233813281204
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [1.72568617e-04 1.60902329e-03 1.65941177e-04 6.65676762e-06
 2.50260959e-03 2.76163578e-04 1.31539832e-05 4.70643491e-04
 2.24189901e-04 1.45828825e-04]
ene_total = [0.16944101 0.20192804 0.17262124 0.15441975 0.22542177 0.18229224
 0.15369677 0.1569107  0.23132642 0.17949795]
ti_comp = [0.34241459 0.34203063 0.34090455 0.34757484 0.34030078 0.33761632
 0.34796803 0.35108386 0.31483598 0.33758153]
ti_coms = [0.07519738 0.07558134 0.07670742 0.07003712 0.07731119 0.07999565
 0.06964393 0.0665281  0.10277599 0.08003044]
t_total = [29.04992027 29.04992027 29.04992027 29.04992027 29.04992027 29.04992027
 29.04992027 29.04992027 29.04992027 29.04992027]
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [1.55880946e-05 1.45342807e-04 1.49894390e-05 6.01304716e-07
 2.26060309e-04 2.49458102e-05 1.18819712e-06 4.25131485e-05
 2.02510366e-05 1.31726936e-05]
ene_total = [0.52417228 0.53586919 0.53463478 0.48723401 0.55351726 0.55820092
 0.48453973 0.46574004 0.71633895 0.557624  ]
optimize_network iter = 1 obj = 5.4178711549860274
eta = 0.6884184715119183
freqs = [4.49873339e+07 9.47224336e+07 4.44693580e+07 1.51248853e+07
 1.09933602e+08 5.28690360e+07 1.89725795e+07 6.23324407e+07
 5.04813506e+07 4.27341730e+07]
Done!
ene_coms = [0.00751974 0.00755813 0.00767074 0.00700371 0.00773112 0.00799956
 0.00696439 0.00665281 0.0102776  0.00800304]
ene_comp = [1.53003832e-05 1.42660197e-04 1.47127771e-05 5.90206361e-07
 2.21887886e-04 2.44853823e-05 1.16626642e-06 4.17284781e-05
 1.98772608e-05 1.29295635e-05]
ene_total = [0.00753504 0.00770079 0.00768545 0.0070043  0.00795301 0.00802405
 0.00696556 0.00669454 0.01029748 0.00801597]
At round 19 energy consumption: 0.07787619467639152
At round 19 eta: 0.6884184715119183
At round 19 a_n: 21.674231763837348
At round 19 local rounds: 12.225650330368824
At round 19 global rounds: 69.56199190949924
gradient difference: 0.4118976593017578
train() client id: f_00000-0-0 loss: 1.336961  [   32/  126]
train() client id: f_00000-0-1 loss: 1.436316  [   64/  126]
train() client id: f_00000-0-2 loss: 1.240828  [   96/  126]
train() client id: f_00000-1-0 loss: 1.428273  [   32/  126]
train() client id: f_00000-1-1 loss: 1.179920  [   64/  126]
train() client id: f_00000-1-2 loss: 1.017045  [   96/  126]
train() client id: f_00000-2-0 loss: 1.183233  [   32/  126]
train() client id: f_00000-2-1 loss: 1.070913  [   64/  126]
train() client id: f_00000-2-2 loss: 1.059839  [   96/  126]
train() client id: f_00000-3-0 loss: 1.052819  [   32/  126]
train() client id: f_00000-3-1 loss: 1.028202  [   64/  126]
train() client id: f_00000-3-2 loss: 1.067416  [   96/  126]
train() client id: f_00000-4-0 loss: 1.117919  [   32/  126]
train() client id: f_00000-4-1 loss: 0.937717  [   64/  126]
train() client id: f_00000-4-2 loss: 0.902404  [   96/  126]
train() client id: f_00000-5-0 loss: 0.981269  [   32/  126]
train() client id: f_00000-5-1 loss: 0.974936  [   64/  126]
train() client id: f_00000-5-2 loss: 0.930950  [   96/  126]
train() client id: f_00000-6-0 loss: 0.874866  [   32/  126]
train() client id: f_00000-6-1 loss: 0.946306  [   64/  126]
train() client id: f_00000-6-2 loss: 0.911995  [   96/  126]
train() client id: f_00000-7-0 loss: 0.900876  [   32/  126]
train() client id: f_00000-7-1 loss: 0.872532  [   64/  126]
train() client id: f_00000-7-2 loss: 0.870956  [   96/  126]
train() client id: f_00000-8-0 loss: 0.861527  [   32/  126]
train() client id: f_00000-8-1 loss: 0.848463  [   64/  126]
train() client id: f_00000-8-2 loss: 0.906643  [   96/  126]
train() client id: f_00000-9-0 loss: 0.888160  [   32/  126]
train() client id: f_00000-9-1 loss: 0.926308  [   64/  126]
train() client id: f_00000-9-2 loss: 0.840064  [   96/  126]
train() client id: f_00000-10-0 loss: 0.816572  [   32/  126]
train() client id: f_00000-10-1 loss: 0.878722  [   64/  126]
train() client id: f_00000-10-2 loss: 0.960307  [   96/  126]
train() client id: f_00000-11-0 loss: 0.979976  [   32/  126]
train() client id: f_00000-11-1 loss: 0.807085  [   64/  126]
train() client id: f_00000-11-2 loss: 0.822172  [   96/  126]
train() client id: f_00001-0-0 loss: 0.388237  [   32/  265]
train() client id: f_00001-0-1 loss: 0.469059  [   64/  265]
train() client id: f_00001-0-2 loss: 0.552608  [   96/  265]
train() client id: f_00001-0-3 loss: 0.473901  [  128/  265]
train() client id: f_00001-0-4 loss: 0.588084  [  160/  265]
train() client id: f_00001-0-5 loss: 0.391072  [  192/  265]
train() client id: f_00001-0-6 loss: 0.369739  [  224/  265]
train() client id: f_00001-0-7 loss: 0.419304  [  256/  265]
train() client id: f_00001-1-0 loss: 0.551592  [   32/  265]
train() client id: f_00001-1-1 loss: 0.531267  [   64/  265]
train() client id: f_00001-1-2 loss: 0.425333  [   96/  265]
train() client id: f_00001-1-3 loss: 0.383322  [  128/  265]
train() client id: f_00001-1-4 loss: 0.453828  [  160/  265]
train() client id: f_00001-1-5 loss: 0.370680  [  192/  265]
train() client id: f_00001-1-6 loss: 0.482634  [  224/  265]
train() client id: f_00001-1-7 loss: 0.371199  [  256/  265]
train() client id: f_00001-2-0 loss: 0.588552  [   32/  265]
train() client id: f_00001-2-1 loss: 0.357370  [   64/  265]
train() client id: f_00001-2-2 loss: 0.462815  [   96/  265]
train() client id: f_00001-2-3 loss: 0.457358  [  128/  265]
train() client id: f_00001-2-4 loss: 0.378469  [  160/  265]
train() client id: f_00001-2-5 loss: 0.352591  [  192/  265]
train() client id: f_00001-2-6 loss: 0.435668  [  224/  265]
train() client id: f_00001-2-7 loss: 0.411677  [  256/  265]
train() client id: f_00001-3-0 loss: 0.534892  [   32/  265]
train() client id: f_00001-3-1 loss: 0.350570  [   64/  265]
train() client id: f_00001-3-2 loss: 0.451929  [   96/  265]
train() client id: f_00001-3-3 loss: 0.355637  [  128/  265]
train() client id: f_00001-3-4 loss: 0.537217  [  160/  265]
train() client id: f_00001-3-5 loss: 0.415138  [  192/  265]
train() client id: f_00001-3-6 loss: 0.401318  [  224/  265]
train() client id: f_00001-3-7 loss: 0.419887  [  256/  265]
train() client id: f_00001-4-0 loss: 0.470351  [   32/  265]
train() client id: f_00001-4-1 loss: 0.420634  [   64/  265]
train() client id: f_00001-4-2 loss: 0.413890  [   96/  265]
train() client id: f_00001-4-3 loss: 0.486802  [  128/  265]
train() client id: f_00001-4-4 loss: 0.379524  [  160/  265]
train() client id: f_00001-4-5 loss: 0.454972  [  192/  265]
train() client id: f_00001-4-6 loss: 0.349218  [  224/  265]
train() client id: f_00001-4-7 loss: 0.468447  [  256/  265]
train() client id: f_00001-5-0 loss: 0.450361  [   32/  265]
train() client id: f_00001-5-1 loss: 0.536652  [   64/  265]
train() client id: f_00001-5-2 loss: 0.410068  [   96/  265]
train() client id: f_00001-5-3 loss: 0.415657  [  128/  265]
train() client id: f_00001-5-4 loss: 0.395908  [  160/  265]
train() client id: f_00001-5-5 loss: 0.369753  [  192/  265]
train() client id: f_00001-5-6 loss: 0.448531  [  224/  265]
train() client id: f_00001-5-7 loss: 0.393481  [  256/  265]
train() client id: f_00001-6-0 loss: 0.401297  [   32/  265]
train() client id: f_00001-6-1 loss: 0.427132  [   64/  265]
train() client id: f_00001-6-2 loss: 0.482053  [   96/  265]
train() client id: f_00001-6-3 loss: 0.342318  [  128/  265]
train() client id: f_00001-6-4 loss: 0.399333  [  160/  265]
train() client id: f_00001-6-5 loss: 0.407648  [  192/  265]
train() client id: f_00001-6-6 loss: 0.489120  [  224/  265]
train() client id: f_00001-6-7 loss: 0.462979  [  256/  265]
train() client id: f_00001-7-0 loss: 0.423987  [   32/  265]
train() client id: f_00001-7-1 loss: 0.430291  [   64/  265]
train() client id: f_00001-7-2 loss: 0.388741  [   96/  265]
train() client id: f_00001-7-3 loss: 0.382178  [  128/  265]
train() client id: f_00001-7-4 loss: 0.389237  [  160/  265]
train() client id: f_00001-7-5 loss: 0.381450  [  192/  265]
train() client id: f_00001-7-6 loss: 0.320537  [  224/  265]
train() client id: f_00001-7-7 loss: 0.628533  [  256/  265]
train() client id: f_00001-8-0 loss: 0.560661  [   32/  265]
train() client id: f_00001-8-1 loss: 0.362760  [   64/  265]
train() client id: f_00001-8-2 loss: 0.415692  [   96/  265]
train() client id: f_00001-8-3 loss: 0.402701  [  128/  265]
train() client id: f_00001-8-4 loss: 0.412234  [  160/  265]
train() client id: f_00001-8-5 loss: 0.511772  [  192/  265]
train() client id: f_00001-8-6 loss: 0.329448  [  224/  265]
train() client id: f_00001-8-7 loss: 0.332538  [  256/  265]
train() client id: f_00001-9-0 loss: 0.441282  [   32/  265]
train() client id: f_00001-9-1 loss: 0.392291  [   64/  265]
train() client id: f_00001-9-2 loss: 0.458693  [   96/  265]
train() client id: f_00001-9-3 loss: 0.523809  [  128/  265]
train() client id: f_00001-9-4 loss: 0.356249  [  160/  265]
train() client id: f_00001-9-5 loss: 0.457266  [  192/  265]
train() client id: f_00001-9-6 loss: 0.329088  [  224/  265]
train() client id: f_00001-9-7 loss: 0.366866  [  256/  265]
train() client id: f_00001-10-0 loss: 0.377481  [   32/  265]
train() client id: f_00001-10-1 loss: 0.339302  [   64/  265]
train() client id: f_00001-10-2 loss: 0.369728  [   96/  265]
train() client id: f_00001-10-3 loss: 0.440638  [  128/  265]
train() client id: f_00001-10-4 loss: 0.377349  [  160/  265]
train() client id: f_00001-10-5 loss: 0.536294  [  192/  265]
train() client id: f_00001-10-6 loss: 0.567700  [  224/  265]
train() client id: f_00001-10-7 loss: 0.395866  [  256/  265]
train() client id: f_00001-11-0 loss: 0.425185  [   32/  265]
train() client id: f_00001-11-1 loss: 0.407315  [   64/  265]
train() client id: f_00001-11-2 loss: 0.450476  [   96/  265]
train() client id: f_00001-11-3 loss: 0.602627  [  128/  265]
train() client id: f_00001-11-4 loss: 0.398523  [  160/  265]
train() client id: f_00001-11-5 loss: 0.316945  [  192/  265]
train() client id: f_00001-11-6 loss: 0.334706  [  224/  265]
train() client id: f_00001-11-7 loss: 0.428903  [  256/  265]
train() client id: f_00002-0-0 loss: 1.209849  [   32/  124]
train() client id: f_00002-0-1 loss: 1.166454  [   64/  124]
train() client id: f_00002-0-2 loss: 1.202741  [   96/  124]
train() client id: f_00002-1-0 loss: 1.271397  [   32/  124]
train() client id: f_00002-1-1 loss: 1.098135  [   64/  124]
train() client id: f_00002-1-2 loss: 1.152645  [   96/  124]
train() client id: f_00002-2-0 loss: 1.152170  [   32/  124]
train() client id: f_00002-2-1 loss: 1.151049  [   64/  124]
train() client id: f_00002-2-2 loss: 1.128976  [   96/  124]
train() client id: f_00002-3-0 loss: 0.976242  [   32/  124]
train() client id: f_00002-3-1 loss: 1.129416  [   64/  124]
train() client id: f_00002-3-2 loss: 1.165352  [   96/  124]
train() client id: f_00002-4-0 loss: 0.987677  [   32/  124]
train() client id: f_00002-4-1 loss: 1.006558  [   64/  124]
train() client id: f_00002-4-2 loss: 1.058894  [   96/  124]
train() client id: f_00002-5-0 loss: 1.002229  [   32/  124]
train() client id: f_00002-5-1 loss: 0.923984  [   64/  124]
train() client id: f_00002-5-2 loss: 1.131267  [   96/  124]
train() client id: f_00002-6-0 loss: 1.057225  [   32/  124]
train() client id: f_00002-6-1 loss: 0.999684  [   64/  124]
train() client id: f_00002-6-2 loss: 0.930062  [   96/  124]
train() client id: f_00002-7-0 loss: 1.044108  [   32/  124]
train() client id: f_00002-7-1 loss: 0.830997  [   64/  124]
train() client id: f_00002-7-2 loss: 1.118925  [   96/  124]
train() client id: f_00002-8-0 loss: 0.991265  [   32/  124]
train() client id: f_00002-8-1 loss: 1.053298  [   64/  124]
train() client id: f_00002-8-2 loss: 0.921171  [   96/  124]
train() client id: f_00002-9-0 loss: 0.839068  [   32/  124]
train() client id: f_00002-9-1 loss: 0.831104  [   64/  124]
train() client id: f_00002-9-2 loss: 1.075196  [   96/  124]
train() client id: f_00002-10-0 loss: 0.903112  [   32/  124]
train() client id: f_00002-10-1 loss: 0.948570  [   64/  124]
train() client id: f_00002-10-2 loss: 1.013760  [   96/  124]
train() client id: f_00002-11-0 loss: 0.809253  [   32/  124]
train() client id: f_00002-11-1 loss: 1.014632  [   64/  124]
train() client id: f_00002-11-2 loss: 1.009445  [   96/  124]
train() client id: f_00003-0-0 loss: 0.907247  [   32/   43]
train() client id: f_00003-1-0 loss: 0.929472  [   32/   43]
train() client id: f_00003-2-0 loss: 0.839179  [   32/   43]
train() client id: f_00003-3-0 loss: 0.900935  [   32/   43]
train() client id: f_00003-4-0 loss: 0.932522  [   32/   43]
train() client id: f_00003-5-0 loss: 0.804902  [   32/   43]
train() client id: f_00003-6-0 loss: 0.863402  [   32/   43]
train() client id: f_00003-7-0 loss: 0.861089  [   32/   43]
train() client id: f_00003-8-0 loss: 0.872187  [   32/   43]
train() client id: f_00003-9-0 loss: 0.836870  [   32/   43]
train() client id: f_00003-10-0 loss: 0.955285  [   32/   43]
train() client id: f_00003-11-0 loss: 0.907888  [   32/   43]
train() client id: f_00004-0-0 loss: 0.945361  [   32/  306]
train() client id: f_00004-0-1 loss: 0.811277  [   64/  306]
train() client id: f_00004-0-2 loss: 0.880828  [   96/  306]
train() client id: f_00004-0-3 loss: 0.976633  [  128/  306]
train() client id: f_00004-0-4 loss: 0.791320  [  160/  306]
train() client id: f_00004-0-5 loss: 0.913474  [  192/  306]
train() client id: f_00004-0-6 loss: 0.762339  [  224/  306]
train() client id: f_00004-0-7 loss: 0.762374  [  256/  306]
train() client id: f_00004-0-8 loss: 0.761144  [  288/  306]
train() client id: f_00004-1-0 loss: 0.751837  [   32/  306]
train() client id: f_00004-1-1 loss: 0.808494  [   64/  306]
train() client id: f_00004-1-2 loss: 0.895502  [   96/  306]
train() client id: f_00004-1-3 loss: 0.822212  [  128/  306]
train() client id: f_00004-1-4 loss: 0.767423  [  160/  306]
train() client id: f_00004-1-5 loss: 0.873924  [  192/  306]
train() client id: f_00004-1-6 loss: 0.744507  [  224/  306]
train() client id: f_00004-1-7 loss: 0.835952  [  256/  306]
train() client id: f_00004-1-8 loss: 0.950913  [  288/  306]
train() client id: f_00004-2-0 loss: 0.968960  [   32/  306]
train() client id: f_00004-2-1 loss: 0.872732  [   64/  306]
train() client id: f_00004-2-2 loss: 0.886970  [   96/  306]
train() client id: f_00004-2-3 loss: 0.876775  [  128/  306]
train() client id: f_00004-2-4 loss: 0.758442  [  160/  306]
train() client id: f_00004-2-5 loss: 0.795235  [  192/  306]
train() client id: f_00004-2-6 loss: 0.712807  [  224/  306]
train() client id: f_00004-2-7 loss: 0.997144  [  256/  306]
train() client id: f_00004-2-8 loss: 0.719729  [  288/  306]
train() client id: f_00004-3-0 loss: 0.869188  [   32/  306]
train() client id: f_00004-3-1 loss: 0.723218  [   64/  306]
train() client id: f_00004-3-2 loss: 0.882269  [   96/  306]
train() client id: f_00004-3-3 loss: 0.897947  [  128/  306]
train() client id: f_00004-3-4 loss: 0.811007  [  160/  306]
train() client id: f_00004-3-5 loss: 0.775925  [  192/  306]
train() client id: f_00004-3-6 loss: 0.785216  [  224/  306]
train() client id: f_00004-3-7 loss: 0.751068  [  256/  306]
train() client id: f_00004-3-8 loss: 0.915047  [  288/  306]
train() client id: f_00004-4-0 loss: 0.991374  [   32/  306]
train() client id: f_00004-4-1 loss: 0.885151  [   64/  306]
train() client id: f_00004-4-2 loss: 0.661276  [   96/  306]
train() client id: f_00004-4-3 loss: 0.808343  [  128/  306]
train() client id: f_00004-4-4 loss: 0.719437  [  160/  306]
train() client id: f_00004-4-5 loss: 0.718636  [  192/  306]
train() client id: f_00004-4-6 loss: 0.794648  [  224/  306]
train() client id: f_00004-4-7 loss: 0.918781  [  256/  306]
train() client id: f_00004-4-8 loss: 0.931282  [  288/  306]
train() client id: f_00004-5-0 loss: 0.800724  [   32/  306]
train() client id: f_00004-5-1 loss: 0.831907  [   64/  306]
train() client id: f_00004-5-2 loss: 0.779605  [   96/  306]
train() client id: f_00004-5-3 loss: 0.939466  [  128/  306]
train() client id: f_00004-5-4 loss: 0.914581  [  160/  306]
train() client id: f_00004-5-5 loss: 0.883976  [  192/  306]
train() client id: f_00004-5-6 loss: 0.807371  [  224/  306]
train() client id: f_00004-5-7 loss: 0.768427  [  256/  306]
train() client id: f_00004-5-8 loss: 0.833831  [  288/  306]
train() client id: f_00004-6-0 loss: 0.796270  [   32/  306]
train() client id: f_00004-6-1 loss: 0.878920  [   64/  306]
train() client id: f_00004-6-2 loss: 0.897910  [   96/  306]
train() client id: f_00004-6-3 loss: 0.846272  [  128/  306]
train() client id: f_00004-6-4 loss: 0.811836  [  160/  306]
train() client id: f_00004-6-5 loss: 0.796992  [  192/  306]
train() client id: f_00004-6-6 loss: 0.823281  [  224/  306]
train() client id: f_00004-6-7 loss: 0.743143  [  256/  306]
train() client id: f_00004-6-8 loss: 0.868379  [  288/  306]
train() client id: f_00004-7-0 loss: 0.738712  [   32/  306]
train() client id: f_00004-7-1 loss: 0.900464  [   64/  306]
train() client id: f_00004-7-2 loss: 0.899351  [   96/  306]
train() client id: f_00004-7-3 loss: 0.872844  [  128/  306]
train() client id: f_00004-7-4 loss: 0.767062  [  160/  306]
train() client id: f_00004-7-5 loss: 0.939982  [  192/  306]
train() client id: f_00004-7-6 loss: 0.810313  [  224/  306]
train() client id: f_00004-7-7 loss: 0.804306  [  256/  306]
train() client id: f_00004-7-8 loss: 0.783254  [  288/  306]
train() client id: f_00004-8-0 loss: 0.895518  [   32/  306]
train() client id: f_00004-8-1 loss: 0.796986  [   64/  306]
train() client id: f_00004-8-2 loss: 0.778945  [   96/  306]
train() client id: f_00004-8-3 loss: 0.941349  [  128/  306]
train() client id: f_00004-8-4 loss: 0.846714  [  160/  306]
train() client id: f_00004-8-5 loss: 0.825884  [  192/  306]
train() client id: f_00004-8-6 loss: 0.848283  [  224/  306]
train() client id: f_00004-8-7 loss: 0.831689  [  256/  306]
train() client id: f_00004-8-8 loss: 0.875233  [  288/  306]
train() client id: f_00004-9-0 loss: 0.945343  [   32/  306]
train() client id: f_00004-9-1 loss: 0.776527  [   64/  306]
train() client id: f_00004-9-2 loss: 0.834817  [   96/  306]
train() client id: f_00004-9-3 loss: 0.746975  [  128/  306]
train() client id: f_00004-9-4 loss: 0.803645  [  160/  306]
train() client id: f_00004-9-5 loss: 0.822023  [  192/  306]
train() client id: f_00004-9-6 loss: 0.947303  [  224/  306]
train() client id: f_00004-9-7 loss: 0.898878  [  256/  306]
train() client id: f_00004-9-8 loss: 0.763058  [  288/  306]
train() client id: f_00004-10-0 loss: 0.874690  [   32/  306]
train() client id: f_00004-10-1 loss: 0.882754  [   64/  306]
train() client id: f_00004-10-2 loss: 0.770437  [   96/  306]
train() client id: f_00004-10-3 loss: 0.915041  [  128/  306]
train() client id: f_00004-10-4 loss: 0.909788  [  160/  306]
train() client id: f_00004-10-5 loss: 0.825595  [  192/  306]
train() client id: f_00004-10-6 loss: 0.866421  [  224/  306]
train() client id: f_00004-10-7 loss: 0.774813  [  256/  306]
train() client id: f_00004-10-8 loss: 0.723639  [  288/  306]
train() client id: f_00004-11-0 loss: 0.753844  [   32/  306]
train() client id: f_00004-11-1 loss: 0.859902  [   64/  306]
train() client id: f_00004-11-2 loss: 0.801106  [   96/  306]
train() client id: f_00004-11-3 loss: 0.774005  [  128/  306]
train() client id: f_00004-11-4 loss: 0.741838  [  160/  306]
train() client id: f_00004-11-5 loss: 0.945941  [  192/  306]
train() client id: f_00004-11-6 loss: 0.856852  [  224/  306]
train() client id: f_00004-11-7 loss: 0.932990  [  256/  306]
train() client id: f_00004-11-8 loss: 0.867380  [  288/  306]
train() client id: f_00005-0-0 loss: 0.691559  [   32/  146]
train() client id: f_00005-0-1 loss: 0.734154  [   64/  146]
train() client id: f_00005-0-2 loss: 0.822926  [   96/  146]
train() client id: f_00005-0-3 loss: 0.881060  [  128/  146]
train() client id: f_00005-1-0 loss: 0.853972  [   32/  146]
train() client id: f_00005-1-1 loss: 0.785679  [   64/  146]
train() client id: f_00005-1-2 loss: 0.790828  [   96/  146]
train() client id: f_00005-1-3 loss: 0.742149  [  128/  146]
train() client id: f_00005-2-0 loss: 0.744345  [   32/  146]
train() client id: f_00005-2-1 loss: 0.819252  [   64/  146]
train() client id: f_00005-2-2 loss: 0.576716  [   96/  146]
train() client id: f_00005-2-3 loss: 0.826101  [  128/  146]
train() client id: f_00005-3-0 loss: 0.682245  [   32/  146]
train() client id: f_00005-3-1 loss: 0.925102  [   64/  146]
train() client id: f_00005-3-2 loss: 0.604035  [   96/  146]
train() client id: f_00005-3-3 loss: 0.830458  [  128/  146]
train() client id: f_00005-4-0 loss: 0.836220  [   32/  146]
train() client id: f_00005-4-1 loss: 0.635962  [   64/  146]
train() client id: f_00005-4-2 loss: 0.901093  [   96/  146]
train() client id: f_00005-4-3 loss: 0.690381  [  128/  146]
train() client id: f_00005-5-0 loss: 1.001449  [   32/  146]
train() client id: f_00005-5-1 loss: 0.740291  [   64/  146]
train() client id: f_00005-5-2 loss: 0.626389  [   96/  146]
train() client id: f_00005-5-3 loss: 0.660378  [  128/  146]
train() client id: f_00005-6-0 loss: 0.750115  [   32/  146]
train() client id: f_00005-6-1 loss: 0.801341  [   64/  146]
train() client id: f_00005-6-2 loss: 0.770230  [   96/  146]
train() client id: f_00005-6-3 loss: 0.854857  [  128/  146]
train() client id: f_00005-7-0 loss: 0.681918  [   32/  146]
train() client id: f_00005-7-1 loss: 0.961630  [   64/  146]
train() client id: f_00005-7-2 loss: 0.833393  [   96/  146]
train() client id: f_00005-7-3 loss: 0.767098  [  128/  146]
train() client id: f_00005-8-0 loss: 0.624519  [   32/  146]
train() client id: f_00005-8-1 loss: 0.883556  [   64/  146]
train() client id: f_00005-8-2 loss: 0.823748  [   96/  146]
train() client id: f_00005-8-3 loss: 0.804328  [  128/  146]
train() client id: f_00005-9-0 loss: 0.605851  [   32/  146]
train() client id: f_00005-9-1 loss: 0.707974  [   64/  146]
train() client id: f_00005-9-2 loss: 0.911381  [   96/  146]
train() client id: f_00005-9-3 loss: 0.819019  [  128/  146]
train() client id: f_00005-10-0 loss: 0.733412  [   32/  146]
train() client id: f_00005-10-1 loss: 1.011617  [   64/  146]
train() client id: f_00005-10-2 loss: 0.543368  [   96/  146]
train() client id: f_00005-10-3 loss: 0.869697  [  128/  146]
train() client id: f_00005-11-0 loss: 0.858768  [   32/  146]
train() client id: f_00005-11-1 loss: 0.736356  [   64/  146]
train() client id: f_00005-11-2 loss: 0.573578  [   96/  146]
train() client id: f_00005-11-3 loss: 1.031327  [  128/  146]
train() client id: f_00006-0-0 loss: 0.633250  [   32/   54]
train() client id: f_00006-1-0 loss: 0.589890  [   32/   54]
train() client id: f_00006-2-0 loss: 0.634391  [   32/   54]
train() client id: f_00006-3-0 loss: 0.664034  [   32/   54]
train() client id: f_00006-4-0 loss: 0.629904  [   32/   54]
train() client id: f_00006-5-0 loss: 0.591361  [   32/   54]
train() client id: f_00006-6-0 loss: 0.633604  [   32/   54]
train() client id: f_00006-7-0 loss: 0.596916  [   32/   54]
train() client id: f_00006-8-0 loss: 0.579440  [   32/   54]
train() client id: f_00006-9-0 loss: 0.597324  [   32/   54]
train() client id: f_00006-10-0 loss: 0.633545  [   32/   54]
train() client id: f_00006-11-0 loss: 0.582659  [   32/   54]
train() client id: f_00007-0-0 loss: 0.688043  [   32/  179]
train() client id: f_00007-0-1 loss: 0.767586  [   64/  179]
train() client id: f_00007-0-2 loss: 0.789917  [   96/  179]
train() client id: f_00007-0-3 loss: 0.801009  [  128/  179]
train() client id: f_00007-0-4 loss: 0.682688  [  160/  179]
train() client id: f_00007-1-0 loss: 0.756455  [   32/  179]
train() client id: f_00007-1-1 loss: 0.868937  [   64/  179]
train() client id: f_00007-1-2 loss: 0.586910  [   96/  179]
train() client id: f_00007-1-3 loss: 0.703545  [  128/  179]
train() client id: f_00007-1-4 loss: 0.684077  [  160/  179]
train() client id: f_00007-2-0 loss: 0.614182  [   32/  179]
train() client id: f_00007-2-1 loss: 0.682010  [   64/  179]
train() client id: f_00007-2-2 loss: 0.867477  [   96/  179]
train() client id: f_00007-2-3 loss: 0.577212  [  128/  179]
train() client id: f_00007-2-4 loss: 0.856838  [  160/  179]
train() client id: f_00007-3-0 loss: 0.681726  [   32/  179]
train() client id: f_00007-3-1 loss: 0.833821  [   64/  179]
train() client id: f_00007-3-2 loss: 0.625488  [   96/  179]
train() client id: f_00007-3-3 loss: 0.684839  [  128/  179]
train() client id: f_00007-3-4 loss: 0.755968  [  160/  179]
train() client id: f_00007-4-0 loss: 0.810721  [   32/  179]
train() client id: f_00007-4-1 loss: 0.571189  [   64/  179]
train() client id: f_00007-4-2 loss: 0.729132  [   96/  179]
train() client id: f_00007-4-3 loss: 0.566962  [  128/  179]
train() client id: f_00007-4-4 loss: 0.804495  [  160/  179]
train() client id: f_00007-5-0 loss: 0.656181  [   32/  179]
train() client id: f_00007-5-1 loss: 0.659327  [   64/  179]
train() client id: f_00007-5-2 loss: 0.836483  [   96/  179]
train() client id: f_00007-5-3 loss: 0.652492  [  128/  179]
train() client id: f_00007-5-4 loss: 0.589034  [  160/  179]
train() client id: f_00007-6-0 loss: 0.671722  [   32/  179]
train() client id: f_00007-6-1 loss: 0.780047  [   64/  179]
train() client id: f_00007-6-2 loss: 0.650102  [   96/  179]
train() client id: f_00007-6-3 loss: 0.767147  [  128/  179]
train() client id: f_00007-6-4 loss: 0.572710  [  160/  179]
train() client id: f_00007-7-0 loss: 0.751344  [   32/  179]
train() client id: f_00007-7-1 loss: 0.757071  [   64/  179]
train() client id: f_00007-7-2 loss: 0.712389  [   96/  179]
train() client id: f_00007-7-3 loss: 0.561350  [  128/  179]
train() client id: f_00007-7-4 loss: 0.575673  [  160/  179]
train() client id: f_00007-8-0 loss: 0.571885  [   32/  179]
train() client id: f_00007-8-1 loss: 0.687621  [   64/  179]
train() client id: f_00007-8-2 loss: 0.809251  [   96/  179]
train() client id: f_00007-8-3 loss: 0.620313  [  128/  179]
train() client id: f_00007-8-4 loss: 0.581098  [  160/  179]
train() client id: f_00007-9-0 loss: 0.647137  [   32/  179]
train() client id: f_00007-9-1 loss: 0.741588  [   64/  179]
train() client id: f_00007-9-2 loss: 0.543138  [   96/  179]
train() client id: f_00007-9-3 loss: 0.786450  [  128/  179]
train() client id: f_00007-9-4 loss: 0.737752  [  160/  179]
train() client id: f_00007-10-0 loss: 0.629177  [   32/  179]
train() client id: f_00007-10-1 loss: 0.661794  [   64/  179]
train() client id: f_00007-10-2 loss: 0.553466  [   96/  179]
train() client id: f_00007-10-3 loss: 0.654550  [  128/  179]
train() client id: f_00007-10-4 loss: 0.733822  [  160/  179]
train() client id: f_00007-11-0 loss: 0.589338  [   32/  179]
train() client id: f_00007-11-1 loss: 0.798886  [   64/  179]
train() client id: f_00007-11-2 loss: 0.661917  [   96/  179]
train() client id: f_00007-11-3 loss: 0.939343  [  128/  179]
train() client id: f_00007-11-4 loss: 0.554580  [  160/  179]
train() client id: f_00008-0-0 loss: 0.724317  [   32/  130]
train() client id: f_00008-0-1 loss: 0.723398  [   64/  130]
train() client id: f_00008-0-2 loss: 0.768681  [   96/  130]
train() client id: f_00008-0-3 loss: 0.786901  [  128/  130]
train() client id: f_00008-1-0 loss: 0.785346  [   32/  130]
train() client id: f_00008-1-1 loss: 0.689763  [   64/  130]
train() client id: f_00008-1-2 loss: 0.702393  [   96/  130]
train() client id: f_00008-1-3 loss: 0.808255  [  128/  130]
train() client id: f_00008-2-0 loss: 0.726246  [   32/  130]
train() client id: f_00008-2-1 loss: 0.739398  [   64/  130]
train() client id: f_00008-2-2 loss: 0.781042  [   96/  130]
train() client id: f_00008-2-3 loss: 0.758846  [  128/  130]
train() client id: f_00008-3-0 loss: 0.672784  [   32/  130]
train() client id: f_00008-3-1 loss: 0.856724  [   64/  130]
train() client id: f_00008-3-2 loss: 0.747885  [   96/  130]
train() client id: f_00008-3-3 loss: 0.685078  [  128/  130]
train() client id: f_00008-4-0 loss: 0.826925  [   32/  130]
train() client id: f_00008-4-1 loss: 0.791173  [   64/  130]
train() client id: f_00008-4-2 loss: 0.664533  [   96/  130]
train() client id: f_00008-4-3 loss: 0.718637  [  128/  130]
train() client id: f_00008-5-0 loss: 0.739302  [   32/  130]
train() client id: f_00008-5-1 loss: 0.745802  [   64/  130]
train() client id: f_00008-5-2 loss: 0.712573  [   96/  130]
train() client id: f_00008-5-3 loss: 0.739734  [  128/  130]
train() client id: f_00008-6-0 loss: 0.719264  [   32/  130]
train() client id: f_00008-6-1 loss: 0.769518  [   64/  130]
train() client id: f_00008-6-2 loss: 0.742786  [   96/  130]
train() client id: f_00008-6-3 loss: 0.749536  [  128/  130]
train() client id: f_00008-7-0 loss: 0.716300  [   32/  130]
train() client id: f_00008-7-1 loss: 0.663943  [   64/  130]
train() client id: f_00008-7-2 loss: 0.722760  [   96/  130]
train() client id: f_00008-7-3 loss: 0.865648  [  128/  130]
train() client id: f_00008-8-0 loss: 0.852024  [   32/  130]
train() client id: f_00008-8-1 loss: 0.683967  [   64/  130]
train() client id: f_00008-8-2 loss: 0.817212  [   96/  130]
train() client id: f_00008-8-3 loss: 0.647790  [  128/  130]
train() client id: f_00008-9-0 loss: 0.768974  [   32/  130]
train() client id: f_00008-9-1 loss: 0.729899  [   64/  130]
train() client id: f_00008-9-2 loss: 0.750270  [   96/  130]
train() client id: f_00008-9-3 loss: 0.756553  [  128/  130]
train() client id: f_00008-10-0 loss: 0.857553  [   32/  130]
train() client id: f_00008-10-1 loss: 0.900804  [   64/  130]
train() client id: f_00008-10-2 loss: 0.583896  [   96/  130]
train() client id: f_00008-10-3 loss: 0.665139  [  128/  130]
train() client id: f_00008-11-0 loss: 0.715409  [   32/  130]
train() client id: f_00008-11-1 loss: 0.625756  [   64/  130]
train() client id: f_00008-11-2 loss: 0.793345  [   96/  130]
train() client id: f_00008-11-3 loss: 0.871386  [  128/  130]
train() client id: f_00009-0-0 loss: 1.319850  [   32/  118]
train() client id: f_00009-0-1 loss: 1.090029  [   64/  118]
train() client id: f_00009-0-2 loss: 1.103164  [   96/  118]
train() client id: f_00009-1-0 loss: 1.017479  [   32/  118]
train() client id: f_00009-1-1 loss: 1.140664  [   64/  118]
train() client id: f_00009-1-2 loss: 1.133224  [   96/  118]
train() client id: f_00009-2-0 loss: 1.064056  [   32/  118]
train() client id: f_00009-2-1 loss: 1.163052  [   64/  118]
train() client id: f_00009-2-2 loss: 0.968912  [   96/  118]
train() client id: f_00009-3-0 loss: 1.039178  [   32/  118]
train() client id: f_00009-3-1 loss: 1.009242  [   64/  118]
train() client id: f_00009-3-2 loss: 1.072065  [   96/  118]
train() client id: f_00009-4-0 loss: 1.069497  [   32/  118]
train() client id: f_00009-4-1 loss: 0.961187  [   64/  118]
train() client id: f_00009-4-2 loss: 0.950885  [   96/  118]
train() client id: f_00009-5-0 loss: 0.946262  [   32/  118]
train() client id: f_00009-5-1 loss: 0.941708  [   64/  118]
train() client id: f_00009-5-2 loss: 1.014633  [   96/  118]
train() client id: f_00009-6-0 loss: 0.881391  [   32/  118]
train() client id: f_00009-6-1 loss: 0.906943  [   64/  118]
train() client id: f_00009-6-2 loss: 0.949525  [   96/  118]
train() client id: f_00009-7-0 loss: 1.005308  [   32/  118]
train() client id: f_00009-7-1 loss: 0.890359  [   64/  118]
train() client id: f_00009-7-2 loss: 0.902194  [   96/  118]
train() client id: f_00009-8-0 loss: 0.986449  [   32/  118]
train() client id: f_00009-8-1 loss: 0.943429  [   64/  118]
train() client id: f_00009-8-2 loss: 0.874304  [   96/  118]
train() client id: f_00009-9-0 loss: 0.746330  [   32/  118]
train() client id: f_00009-9-1 loss: 1.001641  [   64/  118]
train() client id: f_00009-9-2 loss: 0.916019  [   96/  118]
train() client id: f_00009-10-0 loss: 0.941032  [   32/  118]
train() client id: f_00009-10-1 loss: 0.863787  [   64/  118]
train() client id: f_00009-10-2 loss: 0.913905  [   96/  118]
train() client id: f_00009-11-0 loss: 0.830290  [   32/  118]
train() client id: f_00009-11-1 loss: 0.835630  [   64/  118]
train() client id: f_00009-11-2 loss: 0.914668  [   96/  118]
At round 19 accuracy: 0.6419098143236074
At round 19 training accuracy: 0.579476861167002
At round 19 training loss: 0.8465811291332683
update_location
xs = -3.905658 4.200318 115.009024 18.811294 0.979296 3.956410 -77.443192 -56.324852 99.663977 -42.060879 
ys = 107.587959 90.555839 1.320614 -77.455176 69.350187 52.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 146.936800 134.974081 152.410038 127.879510 121.698017 113.159126 126.508255 114.774410 142.272901 108.559336 
dists_bs = 183.895777 198.099900 338.038109 318.271007 205.257389 216.690513 202.652328 210.764431 316.583035 216.594790 
uav_gains = -104.181726 -103.257358 -104.580954 -102.670545 -102.132348 -101.342333 -102.553420 -101.496241 -103.830302 -100.891731 
bs_gains = -102.975068 -103.879818 -110.378098 -109.645377 -104.311426 -104.970576 -104.156104 -104.633384 -109.580713 -104.965203 
Round 20
-------------------------------
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.3523103  17.38275937  8.22567535  2.94704545 20.05064182  9.65872482
  3.66105067 11.77706151  8.67193539  7.83910309]
obj_prev = 98.5663077651687
eta_min = 7.590997972408306e-12	eta_max = 0.9222942349256907
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 22.912452031604698	eta = 0.9090909090909091
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 40.06758095330687	eta = 0.519859231611392
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 31.832010204673995	eta = 0.654357098812908
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 30.352361561329435	eta = 0.6862563825495306
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 30.27833469058294	eta = 0.6879341965062457
af = 20.82950184691336	bf = 1.6572437931277046	zeta = 30.27813585075753	eta = 0.6879387142452571
eta = 0.6879387142452571
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [0.03086617 0.06491693 0.03037623 0.01053369 0.07496069 0.03576556
 0.01322836 0.04384955 0.03184604 0.02890641]
ene_total = [2.63093078 4.95082043 2.60801881 1.20448716 5.64850744 2.98715498
 1.38606222 3.45675966 2.88726353 2.51813083]
ti_comp = [0.34802257 0.34915355 0.34647497 0.3533723  0.34752642 0.34490443
 0.35375633 0.35704591 0.32032243 0.34492651]
ti_coms = [0.07621728 0.0750863  0.07776488 0.07086754 0.07671342 0.07933541
 0.07048351 0.06719393 0.10391742 0.07931333]
t_total = [28.99991608 28.99991608 28.99991608 28.99991608 28.99991608 28.99991608
 28.99991608 28.99991608 28.99991608 28.99991608]
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [1.51744596e-05 1.40255892e-04 1.45927700e-05 5.85001217e-07
 2.17974172e-04 2.40368430e-05 1.15608361e-06 4.13358984e-05
 1.96730538e-05 1.26884708e-05]
ene_total = [0.52203848 0.52285762 0.53257769 0.48447181 0.53929285 0.54395903
 0.48188571 0.46214562 0.71169684 0.54303237]
optimize_network iter = 0 obj = 5.343958006285102
eta = 0.6879387142452571
freqs = [4.43450629e+07 9.29633038e+07 4.38361048e+07 1.49045232e+07
 1.07848904e+08 5.18485023e+07 1.86969890e+07 6.14060420e+07
 4.97093568e+07 4.19022717e+07]
eta_min = 0.6879387142452722	eta_max = 0.6879387142452453
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 0.030579617084646203	eta = 0.9090909090909091
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 18.258827212431594	eta = 0.001522532174257359
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 1.8403927678865823	eta = 0.015105282079029619
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 1.8010043726548246	eta = 0.015435638201229906
af = 0.02779965189513291	bf = 1.6572437931277046	zeta = 1.800997812209442	eta = 0.015435694428206238
eta = 0.015435694428206238
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [1.69209568e-04 1.56398576e-03 1.62723179e-04 6.52331649e-06
 2.43061804e-03 2.68033519e-04 1.28914250e-05 4.60934337e-04
 2.19373145e-04 1.41488442e-04]
ene_total = [0.16879936 0.19656835 0.17201187 0.15368379 0.21887026 0.17769628
 0.15298971 0.15556983 0.22990166 0.17490671]
ti_comp = [0.34802257 0.34915355 0.34647497 0.3533723  0.34752642 0.34490443
 0.35375633 0.35704591 0.32032243 0.34492651]
ti_coms = [0.07621728 0.0750863  0.07776488 0.07086754 0.07671342 0.07933541
 0.07048351 0.06719393 0.10391742 0.07931333]
t_total = [28.99991608 28.99991608 28.99991608 28.99991608 28.99991608 28.99991608
 28.99991608 28.99991608 28.99991608 28.99991608]
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [1.51744596e-05 1.40255892e-04 1.45927700e-05 5.85001217e-07
 2.17974172e-04 2.40368430e-05 1.15608361e-06 4.13358984e-05
 1.96730538e-05 1.26884708e-05]
ene_total = [0.52203848 0.52285762 0.53257769 0.48447181 0.53929285 0.54395903
 0.48188571 0.46214562 0.71169684 0.54303237]
optimize_network iter = 1 obj = 5.343958006285359
eta = 0.6879387142452722
freqs = [4.43450629e+07 9.29633038e+07 4.38361048e+07 1.49045232e+07
 1.07848904e+08 5.18485023e+07 1.86969890e+07 6.14060420e+07
 4.97093568e+07 4.19022717e+07]
Done!
ene_coms = [0.00762173 0.00750863 0.00777649 0.00708675 0.00767134 0.00793354
 0.00704835 0.00671939 0.01039174 0.00793133]
ene_comp = [1.48666236e-05 1.37410596e-04 1.42967344e-05 5.73133613e-07
 2.13552247e-04 2.35492206e-05 1.13263077e-06 4.04973394e-05
 1.92739572e-05 1.24310667e-05]
ene_total = [0.00763659 0.00764604 0.00779078 0.00708733 0.00788489 0.00795709
 0.00704948 0.00675989 0.01041102 0.00794376]
At round 20 energy consumption: 0.07816688572627806
At round 20 eta: 0.6879387142452722
At round 20 a_n: 21.33168591686803
At round 20 local rounds: 12.248478257840025
At round 20 global rounds: 68.35736084749131
gradient difference: 0.43649497628211975
train() client id: f_00000-0-0 loss: 1.321530  [   32/  126]
train() client id: f_00000-0-1 loss: 1.352825  [   64/  126]
train() client id: f_00000-0-2 loss: 1.434375  [   96/  126]
train() client id: f_00000-1-0 loss: 1.409190  [   32/  126]
train() client id: f_00000-1-1 loss: 1.293424  [   64/  126]
train() client id: f_00000-1-2 loss: 1.079309  [   96/  126]
train() client id: f_00000-2-0 loss: 1.249974  [   32/  126]
train() client id: f_00000-2-1 loss: 1.238928  [   64/  126]
train() client id: f_00000-2-2 loss: 1.178094  [   96/  126]
train() client id: f_00000-3-0 loss: 1.088440  [   32/  126]
train() client id: f_00000-3-1 loss: 1.167050  [   64/  126]
train() client id: f_00000-3-2 loss: 1.050562  [   96/  126]
train() client id: f_00000-4-0 loss: 1.120720  [   32/  126]
train() client id: f_00000-4-1 loss: 1.018031  [   64/  126]
train() client id: f_00000-4-2 loss: 1.082897  [   96/  126]
train() client id: f_00000-5-0 loss: 0.993530  [   32/  126]
train() client id: f_00000-5-1 loss: 0.986386  [   64/  126]
train() client id: f_00000-5-2 loss: 0.941025  [   96/  126]
train() client id: f_00000-6-0 loss: 1.032377  [   32/  126]
train() client id: f_00000-6-1 loss: 0.936166  [   64/  126]
train() client id: f_00000-6-2 loss: 0.884483  [   96/  126]
train() client id: f_00000-7-0 loss: 0.910744  [   32/  126]
train() client id: f_00000-7-1 loss: 0.861941  [   64/  126]
train() client id: f_00000-7-2 loss: 1.028438  [   96/  126]
train() client id: f_00000-8-0 loss: 0.920165  [   32/  126]
train() client id: f_00000-8-1 loss: 0.960475  [   64/  126]
train() client id: f_00000-8-2 loss: 0.942285  [   96/  126]
train() client id: f_00000-9-0 loss: 0.918646  [   32/  126]
train() client id: f_00000-9-1 loss: 0.966124  [   64/  126]
train() client id: f_00000-9-2 loss: 0.884940  [   96/  126]
train() client id: f_00000-10-0 loss: 0.962864  [   32/  126]
train() client id: f_00000-10-1 loss: 0.910697  [   64/  126]
train() client id: f_00000-10-2 loss: 0.851866  [   96/  126]
train() client id: f_00000-11-0 loss: 0.942935  [   32/  126]
train() client id: f_00000-11-1 loss: 0.833295  [   64/  126]
train() client id: f_00000-11-2 loss: 1.022264  [   96/  126]
train() client id: f_00001-0-0 loss: 0.433089  [   32/  265]
train() client id: f_00001-0-1 loss: 0.417571  [   64/  265]
train() client id: f_00001-0-2 loss: 0.428620  [   96/  265]
train() client id: f_00001-0-3 loss: 0.432425  [  128/  265]
train() client id: f_00001-0-4 loss: 0.371554  [  160/  265]
train() client id: f_00001-0-5 loss: 0.504325  [  192/  265]
train() client id: f_00001-0-6 loss: 0.431555  [  224/  265]
train() client id: f_00001-0-7 loss: 0.507994  [  256/  265]
train() client id: f_00001-1-0 loss: 0.570209  [   32/  265]
train() client id: f_00001-1-1 loss: 0.417436  [   64/  265]
train() client id: f_00001-1-2 loss: 0.477319  [   96/  265]
train() client id: f_00001-1-3 loss: 0.347992  [  128/  265]
train() client id: f_00001-1-4 loss: 0.485287  [  160/  265]
train() client id: f_00001-1-5 loss: 0.416924  [  192/  265]
train() client id: f_00001-1-6 loss: 0.405063  [  224/  265]
train() client id: f_00001-1-7 loss: 0.323283  [  256/  265]
train() client id: f_00001-2-0 loss: 0.389680  [   32/  265]
train() client id: f_00001-2-1 loss: 0.419024  [   64/  265]
train() client id: f_00001-2-2 loss: 0.446554  [   96/  265]
train() client id: f_00001-2-3 loss: 0.346297  [  128/  265]
train() client id: f_00001-2-4 loss: 0.351768  [  160/  265]
train() client id: f_00001-2-5 loss: 0.454257  [  192/  265]
train() client id: f_00001-2-6 loss: 0.504125  [  224/  265]
train() client id: f_00001-2-7 loss: 0.405487  [  256/  265]
train() client id: f_00001-3-0 loss: 0.334040  [   32/  265]
train() client id: f_00001-3-1 loss: 0.413986  [   64/  265]
train() client id: f_00001-3-2 loss: 0.332487  [   96/  265]
train() client id: f_00001-3-3 loss: 0.341704  [  128/  265]
train() client id: f_00001-3-4 loss: 0.599193  [  160/  265]
train() client id: f_00001-3-5 loss: 0.323159  [  192/  265]
train() client id: f_00001-3-6 loss: 0.490693  [  224/  265]
train() client id: f_00001-3-7 loss: 0.502914  [  256/  265]
train() client id: f_00001-4-0 loss: 0.430300  [   32/  265]
train() client id: f_00001-4-1 loss: 0.481615  [   64/  265]
train() client id: f_00001-4-2 loss: 0.506060  [   96/  265]
train() client id: f_00001-4-3 loss: 0.322502  [  128/  265]
train() client id: f_00001-4-4 loss: 0.427066  [  160/  265]
train() client id: f_00001-4-5 loss: 0.296737  [  192/  265]
train() client id: f_00001-4-6 loss: 0.385302  [  224/  265]
train() client id: f_00001-4-7 loss: 0.444501  [  256/  265]
train() client id: f_00001-5-0 loss: 0.328239  [   32/  265]
train() client id: f_00001-5-1 loss: 0.453164  [   64/  265]
train() client id: f_00001-5-2 loss: 0.417970  [   96/  265]
train() client id: f_00001-5-3 loss: 0.499934  [  128/  265]
train() client id: f_00001-5-4 loss: 0.311097  [  160/  265]
train() client id: f_00001-5-5 loss: 0.319638  [  192/  265]
train() client id: f_00001-5-6 loss: 0.351514  [  224/  265]
train() client id: f_00001-5-7 loss: 0.585723  [  256/  265]
train() client id: f_00001-6-0 loss: 0.361126  [   32/  265]
train() client id: f_00001-6-1 loss: 0.316038  [   64/  265]
train() client id: f_00001-6-2 loss: 0.310899  [   96/  265]
train() client id: f_00001-6-3 loss: 0.465828  [  128/  265]
train() client id: f_00001-6-4 loss: 0.429122  [  160/  265]
train() client id: f_00001-6-5 loss: 0.425254  [  192/  265]
train() client id: f_00001-6-6 loss: 0.449421  [  224/  265]
train() client id: f_00001-6-7 loss: 0.484863  [  256/  265]
train() client id: f_00001-7-0 loss: 0.442629  [   32/  265]
train() client id: f_00001-7-1 loss: 0.587531  [   64/  265]
train() client id: f_00001-7-2 loss: 0.309078  [   96/  265]
train() client id: f_00001-7-3 loss: 0.310789  [  128/  265]
train() client id: f_00001-7-4 loss: 0.457143  [  160/  265]
train() client id: f_00001-7-5 loss: 0.315387  [  192/  265]
train() client id: f_00001-7-6 loss: 0.438136  [  224/  265]
train() client id: f_00001-7-7 loss: 0.379884  [  256/  265]
train() client id: f_00001-8-0 loss: 0.351507  [   32/  265]
train() client id: f_00001-8-1 loss: 0.368950  [   64/  265]
train() client id: f_00001-8-2 loss: 0.475537  [   96/  265]
train() client id: f_00001-8-3 loss: 0.397672  [  128/  265]
train() client id: f_00001-8-4 loss: 0.308580  [  160/  265]
train() client id: f_00001-8-5 loss: 0.581987  [  192/  265]
train() client id: f_00001-8-6 loss: 0.353684  [  224/  265]
train() client id: f_00001-8-7 loss: 0.383553  [  256/  265]
train() client id: f_00001-9-0 loss: 0.485776  [   32/  265]
train() client id: f_00001-9-1 loss: 0.321857  [   64/  265]
train() client id: f_00001-9-2 loss: 0.378408  [   96/  265]
train() client id: f_00001-9-3 loss: 0.354054  [  128/  265]
train() client id: f_00001-9-4 loss: 0.495908  [  160/  265]
train() client id: f_00001-9-5 loss: 0.358595  [  192/  265]
train() client id: f_00001-9-6 loss: 0.451966  [  224/  265]
train() client id: f_00001-9-7 loss: 0.372100  [  256/  265]
train() client id: f_00001-10-0 loss: 0.520570  [   32/  265]
train() client id: f_00001-10-1 loss: 0.349757  [   64/  265]
train() client id: f_00001-10-2 loss: 0.302894  [   96/  265]
train() client id: f_00001-10-3 loss: 0.430528  [  128/  265]
train() client id: f_00001-10-4 loss: 0.447114  [  160/  265]
train() client id: f_00001-10-5 loss: 0.334757  [  192/  265]
train() client id: f_00001-10-6 loss: 0.395199  [  224/  265]
train() client id: f_00001-10-7 loss: 0.308922  [  256/  265]
train() client id: f_00001-11-0 loss: 0.353122  [   32/  265]
train() client id: f_00001-11-1 loss: 0.342001  [   64/  265]
train() client id: f_00001-11-2 loss: 0.474786  [   96/  265]
train() client id: f_00001-11-3 loss: 0.299735  [  128/  265]
train() client id: f_00001-11-4 loss: 0.443188  [  160/  265]
train() client id: f_00001-11-5 loss: 0.420125  [  192/  265]
train() client id: f_00001-11-6 loss: 0.344192  [  224/  265]
train() client id: f_00001-11-7 loss: 0.537683  [  256/  265]
train() client id: f_00002-0-0 loss: 1.230212  [   32/  124]
train() client id: f_00002-0-1 loss: 1.077732  [   64/  124]
train() client id: f_00002-0-2 loss: 1.063780  [   96/  124]
train() client id: f_00002-1-0 loss: 1.090602  [   32/  124]
train() client id: f_00002-1-1 loss: 1.176123  [   64/  124]
train() client id: f_00002-1-2 loss: 1.190511  [   96/  124]
train() client id: f_00002-2-0 loss: 1.066693  [   32/  124]
train() client id: f_00002-2-1 loss: 1.143641  [   64/  124]
train() client id: f_00002-2-2 loss: 0.913935  [   96/  124]
train() client id: f_00002-3-0 loss: 1.063453  [   32/  124]
train() client id: f_00002-3-1 loss: 1.090582  [   64/  124]
train() client id: f_00002-3-2 loss: 0.941835  [   96/  124]
train() client id: f_00002-4-0 loss: 0.961366  [   32/  124]
train() client id: f_00002-4-1 loss: 0.994912  [   64/  124]
train() client id: f_00002-4-2 loss: 1.082610  [   96/  124]
train() client id: f_00002-5-0 loss: 0.852938  [   32/  124]
train() client id: f_00002-5-1 loss: 1.020944  [   64/  124]
train() client id: f_00002-5-2 loss: 1.025877  [   96/  124]
train() client id: f_00002-6-0 loss: 0.984000  [   32/  124]
train() client id: f_00002-6-1 loss: 0.956598  [   64/  124]
train() client id: f_00002-6-2 loss: 1.054767  [   96/  124]
train() client id: f_00002-7-0 loss: 0.969288  [   32/  124]
train() client id: f_00002-7-1 loss: 1.063125  [   64/  124]
train() client id: f_00002-7-2 loss: 0.880557  [   96/  124]
train() client id: f_00002-8-0 loss: 0.832021  [   32/  124]
train() client id: f_00002-8-1 loss: 0.940047  [   64/  124]
train() client id: f_00002-8-2 loss: 1.005118  [   96/  124]
train() client id: f_00002-9-0 loss: 1.018139  [   32/  124]
train() client id: f_00002-9-1 loss: 1.021368  [   64/  124]
train() client id: f_00002-9-2 loss: 0.813369  [   96/  124]
train() client id: f_00002-10-0 loss: 1.025066  [   32/  124]
train() client id: f_00002-10-1 loss: 0.960279  [   64/  124]
train() client id: f_00002-10-2 loss: 0.901603  [   96/  124]
train() client id: f_00002-11-0 loss: 0.817369  [   32/  124]
train() client id: f_00002-11-1 loss: 0.988648  [   64/  124]
train() client id: f_00002-11-2 loss: 0.942824  [   96/  124]
train() client id: f_00003-0-0 loss: 0.784138  [   32/   43]
train() client id: f_00003-1-0 loss: 0.704921  [   32/   43]
train() client id: f_00003-2-0 loss: 0.902595  [   32/   43]
train() client id: f_00003-3-0 loss: 0.855283  [   32/   43]
train() client id: f_00003-4-0 loss: 0.866587  [   32/   43]
train() client id: f_00003-5-0 loss: 0.656101  [   32/   43]
train() client id: f_00003-6-0 loss: 0.795038  [   32/   43]
train() client id: f_00003-7-0 loss: 0.899347  [   32/   43]
train() client id: f_00003-8-0 loss: 0.819670  [   32/   43]
train() client id: f_00003-9-0 loss: 0.793266  [   32/   43]
train() client id: f_00003-10-0 loss: 0.862846  [   32/   43]
train() client id: f_00003-11-0 loss: 0.713048  [   32/   43]
train() client id: f_00004-0-0 loss: 0.777473  [   32/  306]
train() client id: f_00004-0-1 loss: 0.922542  [   64/  306]
train() client id: f_00004-0-2 loss: 0.911944  [   96/  306]
train() client id: f_00004-0-3 loss: 0.939751  [  128/  306]
train() client id: f_00004-0-4 loss: 0.985085  [  160/  306]
train() client id: f_00004-0-5 loss: 0.841769  [  192/  306]
train() client id: f_00004-0-6 loss: 0.804476  [  224/  306]
train() client id: f_00004-0-7 loss: 0.846941  [  256/  306]
train() client id: f_00004-0-8 loss: 0.785446  [  288/  306]
train() client id: f_00004-1-0 loss: 0.989508  [   32/  306]
train() client id: f_00004-1-1 loss: 0.866769  [   64/  306]
train() client id: f_00004-1-2 loss: 0.959717  [   96/  306]
train() client id: f_00004-1-3 loss: 0.738631  [  128/  306]
train() client id: f_00004-1-4 loss: 0.759313  [  160/  306]
train() client id: f_00004-1-5 loss: 0.888845  [  192/  306]
train() client id: f_00004-1-6 loss: 0.815153  [  224/  306]
train() client id: f_00004-1-7 loss: 0.905466  [  256/  306]
train() client id: f_00004-1-8 loss: 0.785105  [  288/  306]
train() client id: f_00004-2-0 loss: 1.026600  [   32/  306]
train() client id: f_00004-2-1 loss: 0.913486  [   64/  306]
train() client id: f_00004-2-2 loss: 0.821225  [   96/  306]
train() client id: f_00004-2-3 loss: 0.881517  [  128/  306]
train() client id: f_00004-2-4 loss: 0.809395  [  160/  306]
train() client id: f_00004-2-5 loss: 0.775910  [  192/  306]
train() client id: f_00004-2-6 loss: 0.901278  [  224/  306]
train() client id: f_00004-2-7 loss: 0.770906  [  256/  306]
train() client id: f_00004-2-8 loss: 0.876435  [  288/  306]
train() client id: f_00004-3-0 loss: 0.884160  [   32/  306]
train() client id: f_00004-3-1 loss: 0.962740  [   64/  306]
train() client id: f_00004-3-2 loss: 0.868577  [   96/  306]
train() client id: f_00004-3-3 loss: 0.829086  [  128/  306]
train() client id: f_00004-3-4 loss: 0.887242  [  160/  306]
train() client id: f_00004-3-5 loss: 0.832338  [  192/  306]
train() client id: f_00004-3-6 loss: 0.878246  [  224/  306]
train() client id: f_00004-3-7 loss: 0.856529  [  256/  306]
train() client id: f_00004-3-8 loss: 0.784881  [  288/  306]
train() client id: f_00004-4-0 loss: 0.875656  [   32/  306]
train() client id: f_00004-4-1 loss: 0.825132  [   64/  306]
train() client id: f_00004-4-2 loss: 0.730275  [   96/  306]
train() client id: f_00004-4-3 loss: 0.869964  [  128/  306]
train() client id: f_00004-4-4 loss: 0.930949  [  160/  306]
train() client id: f_00004-4-5 loss: 0.912759  [  192/  306]
train() client id: f_00004-4-6 loss: 0.874014  [  224/  306]
train() client id: f_00004-4-7 loss: 0.858253  [  256/  306]
train() client id: f_00004-4-8 loss: 0.867764  [  288/  306]
train() client id: f_00004-5-0 loss: 0.817733  [   32/  306]
train() client id: f_00004-5-1 loss: 0.784107  [   64/  306]
train() client id: f_00004-5-2 loss: 0.975273  [   96/  306]
train() client id: f_00004-5-3 loss: 0.854106  [  128/  306]
train() client id: f_00004-5-4 loss: 0.907074  [  160/  306]
train() client id: f_00004-5-5 loss: 0.846534  [  192/  306]
train() client id: f_00004-5-6 loss: 0.860913  [  224/  306]
train() client id: f_00004-5-7 loss: 0.835478  [  256/  306]
train() client id: f_00004-5-8 loss: 0.841037  [  288/  306]
train() client id: f_00004-6-0 loss: 0.878316  [   32/  306]
train() client id: f_00004-6-1 loss: 0.898485  [   64/  306]
train() client id: f_00004-6-2 loss: 0.788203  [   96/  306]
train() client id: f_00004-6-3 loss: 0.722312  [  128/  306]
train() client id: f_00004-6-4 loss: 0.942958  [  160/  306]
train() client id: f_00004-6-5 loss: 0.798307  [  192/  306]
train() client id: f_00004-6-6 loss: 0.830689  [  224/  306]
train() client id: f_00004-6-7 loss: 1.063630  [  256/  306]
train() client id: f_00004-6-8 loss: 0.702556  [  288/  306]
train() client id: f_00004-7-0 loss: 0.902914  [   32/  306]
train() client id: f_00004-7-1 loss: 0.891135  [   64/  306]
train() client id: f_00004-7-2 loss: 0.892530  [   96/  306]
train() client id: f_00004-7-3 loss: 0.892980  [  128/  306]
train() client id: f_00004-7-4 loss: 0.814348  [  160/  306]
train() client id: f_00004-7-5 loss: 0.829765  [  192/  306]
train() client id: f_00004-7-6 loss: 0.809070  [  224/  306]
train() client id: f_00004-7-7 loss: 0.819379  [  256/  306]
train() client id: f_00004-7-8 loss: 0.806653  [  288/  306]
train() client id: f_00004-8-0 loss: 0.768525  [   32/  306]
train() client id: f_00004-8-1 loss: 0.814459  [   64/  306]
train() client id: f_00004-8-2 loss: 0.837788  [   96/  306]
train() client id: f_00004-8-3 loss: 0.901470  [  128/  306]
train() client id: f_00004-8-4 loss: 0.755147  [  160/  306]
train() client id: f_00004-8-5 loss: 0.920554  [  192/  306]
train() client id: f_00004-8-6 loss: 0.902233  [  224/  306]
train() client id: f_00004-8-7 loss: 0.822630  [  256/  306]
train() client id: f_00004-8-8 loss: 0.857971  [  288/  306]
train() client id: f_00004-9-0 loss: 0.920230  [   32/  306]
train() client id: f_00004-9-1 loss: 0.844667  [   64/  306]
train() client id: f_00004-9-2 loss: 0.855061  [   96/  306]
train() client id: f_00004-9-3 loss: 0.752002  [  128/  306]
train() client id: f_00004-9-4 loss: 0.736686  [  160/  306]
train() client id: f_00004-9-5 loss: 0.894071  [  192/  306]
train() client id: f_00004-9-6 loss: 0.820999  [  224/  306]
train() client id: f_00004-9-7 loss: 0.823091  [  256/  306]
train() client id: f_00004-9-8 loss: 0.880328  [  288/  306]
train() client id: f_00004-10-0 loss: 0.912625  [   32/  306]
train() client id: f_00004-10-1 loss: 0.831877  [   64/  306]
train() client id: f_00004-10-2 loss: 0.785220  [   96/  306]
train() client id: f_00004-10-3 loss: 0.846812  [  128/  306]
train() client id: f_00004-10-4 loss: 0.738368  [  160/  306]
train() client id: f_00004-10-5 loss: 0.798403  [  192/  306]
train() client id: f_00004-10-6 loss: 0.922976  [  224/  306]
train() client id: f_00004-10-7 loss: 0.832484  [  256/  306]
train() client id: f_00004-10-8 loss: 0.903607  [  288/  306]
train() client id: f_00004-11-0 loss: 0.814560  [   32/  306]
train() client id: f_00004-11-1 loss: 0.724550  [   64/  306]
train() client id: f_00004-11-2 loss: 0.837529  [   96/  306]
train() client id: f_00004-11-3 loss: 0.865794  [  128/  306]
train() client id: f_00004-11-4 loss: 0.876267  [  160/  306]
train() client id: f_00004-11-5 loss: 0.807073  [  192/  306]
train() client id: f_00004-11-6 loss: 0.811624  [  224/  306]
train() client id: f_00004-11-7 loss: 0.889392  [  256/  306]
train() client id: f_00004-11-8 loss: 0.814558  [  288/  306]
train() client id: f_00005-0-0 loss: 0.231785  [   32/  146]
train() client id: f_00005-0-1 loss: 0.573319  [   64/  146]
train() client id: f_00005-0-2 loss: 0.561684  [   96/  146]
train() client id: f_00005-0-3 loss: 0.362180  [  128/  146]
train() client id: f_00005-1-0 loss: 0.448386  [   32/  146]
train() client id: f_00005-1-1 loss: 0.374023  [   64/  146]
train() client id: f_00005-1-2 loss: 0.458178  [   96/  146]
train() client id: f_00005-1-3 loss: 0.586142  [  128/  146]
train() client id: f_00005-2-0 loss: 0.300403  [   32/  146]
train() client id: f_00005-2-1 loss: 0.646340  [   64/  146]
train() client id: f_00005-2-2 loss: 0.381493  [   96/  146]
train() client id: f_00005-2-3 loss: 0.268880  [  128/  146]
train() client id: f_00005-3-0 loss: 0.347357  [   32/  146]
train() client id: f_00005-3-1 loss: 0.514842  [   64/  146]
train() client id: f_00005-3-2 loss: 0.347347  [   96/  146]
train() client id: f_00005-3-3 loss: 0.469294  [  128/  146]
train() client id: f_00005-4-0 loss: 0.738651  [   32/  146]
train() client id: f_00005-4-1 loss: 0.391707  [   64/  146]
train() client id: f_00005-4-2 loss: 0.360195  [   96/  146]
train() client id: f_00005-4-3 loss: 0.234267  [  128/  146]
train() client id: f_00005-5-0 loss: 0.241097  [   32/  146]
train() client id: f_00005-5-1 loss: 0.397266  [   64/  146]
train() client id: f_00005-5-2 loss: 0.565116  [   96/  146]
train() client id: f_00005-5-3 loss: 0.570042  [  128/  146]
train() client id: f_00005-6-0 loss: 0.481113  [   32/  146]
train() client id: f_00005-6-1 loss: 0.374132  [   64/  146]
train() client id: f_00005-6-2 loss: 0.473270  [   96/  146]
train() client id: f_00005-6-3 loss: 0.293986  [  128/  146]
train() client id: f_00005-7-0 loss: 0.305036  [   32/  146]
train() client id: f_00005-7-1 loss: 0.503647  [   64/  146]
train() client id: f_00005-7-2 loss: 0.472255  [   96/  146]
train() client id: f_00005-7-3 loss: 0.509823  [  128/  146]
train() client id: f_00005-8-0 loss: 0.205548  [   32/  146]
train() client id: f_00005-8-1 loss: 0.470750  [   64/  146]
train() client id: f_00005-8-2 loss: 0.557443  [   96/  146]
train() client id: f_00005-8-3 loss: 0.526365  [  128/  146]
train() client id: f_00005-9-0 loss: 0.295665  [   32/  146]
train() client id: f_00005-9-1 loss: 0.295348  [   64/  146]
train() client id: f_00005-9-2 loss: 0.347702  [   96/  146]
train() client id: f_00005-9-3 loss: 0.661459  [  128/  146]
train() client id: f_00005-10-0 loss: 0.364300  [   32/  146]
train() client id: f_00005-10-1 loss: 0.264260  [   64/  146]
train() client id: f_00005-10-2 loss: 0.599547  [   96/  146]
train() client id: f_00005-10-3 loss: 0.395569  [  128/  146]
train() client id: f_00005-11-0 loss: 0.357939  [   32/  146]
train() client id: f_00005-11-1 loss: 0.103289  [   64/  146]
train() client id: f_00005-11-2 loss: 0.433097  [   96/  146]
train() client id: f_00005-11-3 loss: 0.604482  [  128/  146]
train() client id: f_00006-0-0 loss: 0.670709  [   32/   54]
train() client id: f_00006-1-0 loss: 0.596328  [   32/   54]
train() client id: f_00006-2-0 loss: 0.674267  [   32/   54]
train() client id: f_00006-3-0 loss: 0.638335  [   32/   54]
train() client id: f_00006-4-0 loss: 0.591870  [   32/   54]
train() client id: f_00006-5-0 loss: 0.678071  [   32/   54]
train() client id: f_00006-6-0 loss: 0.596288  [   32/   54]
train() client id: f_00006-7-0 loss: 0.601755  [   32/   54]
train() client id: f_00006-8-0 loss: 0.683851  [   32/   54]
train() client id: f_00006-9-0 loss: 0.586752  [   32/   54]
train() client id: f_00006-10-0 loss: 0.675342  [   32/   54]
train() client id: f_00006-11-0 loss: 0.668932  [   32/   54]
train() client id: f_00007-0-0 loss: 0.783030  [   32/  179]
train() client id: f_00007-0-1 loss: 0.846305  [   64/  179]
train() client id: f_00007-0-2 loss: 0.799584  [   96/  179]
train() client id: f_00007-0-3 loss: 0.700405  [  128/  179]
train() client id: f_00007-0-4 loss: 0.615341  [  160/  179]
train() client id: f_00007-1-0 loss: 0.679428  [   32/  179]
train() client id: f_00007-1-1 loss: 0.801285  [   64/  179]
train() client id: f_00007-1-2 loss: 0.705866  [   96/  179]
train() client id: f_00007-1-3 loss: 0.702074  [  128/  179]
train() client id: f_00007-1-4 loss: 0.799711  [  160/  179]
train() client id: f_00007-2-0 loss: 0.648606  [   32/  179]
train() client id: f_00007-2-1 loss: 0.677264  [   64/  179]
train() client id: f_00007-2-2 loss: 0.836630  [   96/  179]
train() client id: f_00007-2-3 loss: 0.772150  [  128/  179]
train() client id: f_00007-2-4 loss: 0.732091  [  160/  179]
train() client id: f_00007-3-0 loss: 0.701860  [   32/  179]
train() client id: f_00007-3-1 loss: 0.672871  [   64/  179]
train() client id: f_00007-3-2 loss: 0.583785  [   96/  179]
train() client id: f_00007-3-3 loss: 0.804691  [  128/  179]
train() client id: f_00007-3-4 loss: 0.673883  [  160/  179]
train() client id: f_00007-4-0 loss: 0.628323  [   32/  179]
train() client id: f_00007-4-1 loss: 0.804177  [   64/  179]
train() client id: f_00007-4-2 loss: 0.662038  [   96/  179]
train() client id: f_00007-4-3 loss: 0.813487  [  128/  179]
train() client id: f_00007-4-4 loss: 0.683406  [  160/  179]
train() client id: f_00007-5-0 loss: 0.797651  [   32/  179]
train() client id: f_00007-5-1 loss: 0.673147  [   64/  179]
train() client id: f_00007-5-2 loss: 0.580416  [   96/  179]
train() client id: f_00007-5-3 loss: 0.722036  [  128/  179]
train() client id: f_00007-5-4 loss: 0.728565  [  160/  179]
train() client id: f_00007-6-0 loss: 0.730248  [   32/  179]
train() client id: f_00007-6-1 loss: 0.786265  [   64/  179]
train() client id: f_00007-6-2 loss: 0.603665  [   96/  179]
train() client id: f_00007-6-3 loss: 0.813555  [  128/  179]
train() client id: f_00007-6-4 loss: 0.579802  [  160/  179]
train() client id: f_00007-7-0 loss: 0.631820  [   32/  179]
train() client id: f_00007-7-1 loss: 0.565181  [   64/  179]
train() client id: f_00007-7-2 loss: 0.831814  [   96/  179]
train() client id: f_00007-7-3 loss: 0.669327  [  128/  179]
train() client id: f_00007-7-4 loss: 0.869833  [  160/  179]
train() client id: f_00007-8-0 loss: 0.702294  [   32/  179]
train() client id: f_00007-8-1 loss: 0.564175  [   64/  179]
train() client id: f_00007-8-2 loss: 0.753683  [   96/  179]
train() client id: f_00007-8-3 loss: 0.635992  [  128/  179]
train() client id: f_00007-8-4 loss: 0.901782  [  160/  179]
train() client id: f_00007-9-0 loss: 0.656278  [   32/  179]
train() client id: f_00007-9-1 loss: 0.702046  [   64/  179]
train() client id: f_00007-9-2 loss: 0.705226  [   96/  179]
train() client id: f_00007-9-3 loss: 0.757515  [  128/  179]
train() client id: f_00007-9-4 loss: 0.665560  [  160/  179]
train() client id: f_00007-10-0 loss: 0.665767  [   32/  179]
train() client id: f_00007-10-1 loss: 0.558920  [   64/  179]
train() client id: f_00007-10-2 loss: 0.743298  [   96/  179]
train() client id: f_00007-10-3 loss: 0.749399  [  128/  179]
train() client id: f_00007-10-4 loss: 0.662943  [  160/  179]
train() client id: f_00007-11-0 loss: 0.764514  [   32/  179]
train() client id: f_00007-11-1 loss: 0.734984  [   64/  179]
train() client id: f_00007-11-2 loss: 0.814096  [   96/  179]
train() client id: f_00007-11-3 loss: 0.569774  [  128/  179]
train() client id: f_00007-11-4 loss: 0.702699  [  160/  179]
train() client id: f_00008-0-0 loss: 0.650712  [   32/  130]
train() client id: f_00008-0-1 loss: 0.686928  [   64/  130]
train() client id: f_00008-0-2 loss: 0.608019  [   96/  130]
train() client id: f_00008-0-3 loss: 0.744531  [  128/  130]
train() client id: f_00008-1-0 loss: 0.670819  [   32/  130]
train() client id: f_00008-1-1 loss: 0.644543  [   64/  130]
train() client id: f_00008-1-2 loss: 0.607583  [   96/  130]
train() client id: f_00008-1-3 loss: 0.764992  [  128/  130]
train() client id: f_00008-2-0 loss: 0.651742  [   32/  130]
train() client id: f_00008-2-1 loss: 0.814038  [   64/  130]
train() client id: f_00008-2-2 loss: 0.718058  [   96/  130]
train() client id: f_00008-2-3 loss: 0.491128  [  128/  130]
train() client id: f_00008-3-0 loss: 0.740048  [   32/  130]
train() client id: f_00008-3-1 loss: 0.667181  [   64/  130]
train() client id: f_00008-3-2 loss: 0.612209  [   96/  130]
train() client id: f_00008-3-3 loss: 0.644734  [  128/  130]
train() client id: f_00008-4-0 loss: 0.696348  [   32/  130]
train() client id: f_00008-4-1 loss: 0.717619  [   64/  130]
train() client id: f_00008-4-2 loss: 0.590455  [   96/  130]
train() client id: f_00008-4-3 loss: 0.654046  [  128/  130]
train() client id: f_00008-5-0 loss: 0.744709  [   32/  130]
train() client id: f_00008-5-1 loss: 0.619045  [   64/  130]
train() client id: f_00008-5-2 loss: 0.653304  [   96/  130]
train() client id: f_00008-5-3 loss: 0.659633  [  128/  130]
train() client id: f_00008-6-0 loss: 0.692211  [   32/  130]
train() client id: f_00008-6-1 loss: 0.657024  [   64/  130]
train() client id: f_00008-6-2 loss: 0.757282  [   96/  130]
train() client id: f_00008-6-3 loss: 0.566974  [  128/  130]
train() client id: f_00008-7-0 loss: 0.677946  [   32/  130]
train() client id: f_00008-7-1 loss: 0.643586  [   64/  130]
train() client id: f_00008-7-2 loss: 0.605759  [   96/  130]
train() client id: f_00008-7-3 loss: 0.716967  [  128/  130]
train() client id: f_00008-8-0 loss: 0.667479  [   32/  130]
train() client id: f_00008-8-1 loss: 0.688512  [   64/  130]
train() client id: f_00008-8-2 loss: 0.524067  [   96/  130]
train() client id: f_00008-8-3 loss: 0.788720  [  128/  130]
train() client id: f_00008-9-0 loss: 0.678938  [   32/  130]
train() client id: f_00008-9-1 loss: 0.708586  [   64/  130]
train() client id: f_00008-9-2 loss: 0.604043  [   96/  130]
train() client id: f_00008-9-3 loss: 0.679083  [  128/  130]
train() client id: f_00008-10-0 loss: 0.695206  [   32/  130]
train() client id: f_00008-10-1 loss: 0.684694  [   64/  130]
train() client id: f_00008-10-2 loss: 0.672705  [   96/  130]
train() client id: f_00008-10-3 loss: 0.610549  [  128/  130]
train() client id: f_00008-11-0 loss: 0.621586  [   32/  130]
train() client id: f_00008-11-1 loss: 0.821990  [   64/  130]
train() client id: f_00008-11-2 loss: 0.571991  [   96/  130]
train() client id: f_00008-11-3 loss: 0.649053  [  128/  130]
train() client id: f_00009-0-0 loss: 1.040759  [   32/  118]
train() client id: f_00009-0-1 loss: 1.184270  [   64/  118]
train() client id: f_00009-0-2 loss: 1.140987  [   96/  118]
train() client id: f_00009-1-0 loss: 1.130771  [   32/  118]
train() client id: f_00009-1-1 loss: 1.048584  [   64/  118]
train() client id: f_00009-1-2 loss: 1.160245  [   96/  118]
train() client id: f_00009-2-0 loss: 1.171361  [   32/  118]
train() client id: f_00009-2-1 loss: 0.958049  [   64/  118]
train() client id: f_00009-2-2 loss: 0.991569  [   96/  118]
train() client id: f_00009-3-0 loss: 0.982387  [   32/  118]
train() client id: f_00009-3-1 loss: 1.006480  [   64/  118]
train() client id: f_00009-3-2 loss: 0.950238  [   96/  118]
train() client id: f_00009-4-0 loss: 0.953006  [   32/  118]
train() client id: f_00009-4-1 loss: 0.863212  [   64/  118]
train() client id: f_00009-4-2 loss: 1.085891  [   96/  118]
train() client id: f_00009-5-0 loss: 0.983267  [   32/  118]
train() client id: f_00009-5-1 loss: 0.890653  [   64/  118]
train() client id: f_00009-5-2 loss: 0.891422  [   96/  118]
train() client id: f_00009-6-0 loss: 0.938059  [   32/  118]
train() client id: f_00009-6-1 loss: 0.929243  [   64/  118]
train() client id: f_00009-6-2 loss: 0.894017  [   96/  118]
train() client id: f_00009-7-0 loss: 1.020877  [   32/  118]
train() client id: f_00009-7-1 loss: 0.917531  [   64/  118]
train() client id: f_00009-7-2 loss: 0.909181  [   96/  118]
train() client id: f_00009-8-0 loss: 0.860889  [   32/  118]
train() client id: f_00009-8-1 loss: 0.857114  [   64/  118]
train() client id: f_00009-8-2 loss: 0.995024  [   96/  118]
train() client id: f_00009-9-0 loss: 0.760325  [   32/  118]
train() client id: f_00009-9-1 loss: 1.009168  [   64/  118]
train() client id: f_00009-9-2 loss: 0.902491  [   96/  118]
train() client id: f_00009-10-0 loss: 0.907318  [   32/  118]
train() client id: f_00009-10-1 loss: 0.907232  [   64/  118]
train() client id: f_00009-10-2 loss: 0.918302  [   96/  118]
train() client id: f_00009-11-0 loss: 0.815232  [   32/  118]
train() client id: f_00009-11-1 loss: 0.879008  [   64/  118]
train() client id: f_00009-11-2 loss: 0.903981  [   96/  118]
At round 20 accuracy: 0.6419098143236074
At round 20 training accuracy: 0.5814889336016097
At round 20 training loss: 0.8271146828452366
update_location
xs = -3.905658 4.200318 120.009024 18.811294 0.979296 3.956410 -82.443192 -61.324852 104.663977 -47.060879 
ys = 112.587959 95.555839 1.320614 -82.455176 74.350187 57.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 150.636326 138.378326 156.217508 130.968396 124.615044 115.577373 129.629358 117.309052 145.819128 110.592668 
dists_bs = 182.122312 196.020735 342.337338 322.251432 202.729124 213.911009 200.293279 207.990129 320.930612 213.562431 
uav_gains = -104.453056 -103.528245 -104.850992 -102.929888 -102.389625 -101.571946 -102.818214 -101.733444 -104.098489 -101.093226 
bs_gains = -102.857227 -103.751516 -110.531779 -109.796515 -104.160711 -104.813587 -104.013718 -104.472255 -109.746571 -104.793755 
Round 21
-------------------------------
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.22040415 17.10262761  8.09588073  2.90155574 19.72744601  9.50226703
  3.60411444 11.58945304  8.53512887  7.71174179]
obj_prev = 96.99061941020106
eta_min = 5.053940464590129e-12	eta_max = 0.9225775474279215
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 22.544522121240533	eta = 0.9090909090909091
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 39.50145864601099	eta = 0.5188421089429398
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 31.352662916909924	eta = 0.6536931221610769
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 29.88807124482688	eta = 0.6857257513318464
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 29.81462162317344	eta = 0.6874150666493414
af = 20.495020110218665	bf = 1.6376576432937124	zeta = 29.81442331258453	eta = 0.6874196389895562
eta = 0.6874196389895562
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [0.03092845 0.06504793 0.03043752 0.01055495 0.07511195 0.03583773
 0.01325505 0.04393804 0.03191031 0.02896474]
ene_total = [2.59558483 4.86886559 2.57325892 1.19041384 5.55486377 2.93485376
 1.36920185 3.40632189 2.84825067 2.47280818]
ti_comp = [0.35387355 0.35652047 0.35228915 0.35940333 0.35499852 0.35244077
 0.35977839 0.36323055 0.32606389 0.35252092]
ti_coms = [0.07726245 0.07461553 0.07884685 0.07173267 0.07613748 0.07869523
 0.07135761 0.06790545 0.10507211 0.07861508]
t_total = [28.94991188 28.94991188 28.94991188 28.94991188 28.94991188 28.94991188
 28.94991188 28.94991188 28.94991188 28.94991188]
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [1.47658462e-05 1.35335454e-04 1.42006892e-05 5.68963075e-07
 2.10161950e-04 2.31595009e-05 1.12448598e-06 4.01825275e-05
 1.91015032e-05 1.22213485e-05]
ene_total = [0.51979327 0.51011572 0.53039422 0.48170862 0.52535973 0.52997772
 0.47922748 0.45866954 0.70682062 0.52870501]
optimize_network iter = 0 obj = 5.270771946473119
eta = 0.6874196389895562
freqs = [4.36998634e+07 9.12260848e+07 4.31996322e+07 1.46839871e+07
 1.05791923e+08 5.08422020e+07 1.84211324e+07 6.04822990e+07
 4.89325984e+07 4.10822992e+07]
eta_min = 0.6874196389895763	eta_max = 0.6874196389895456
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 0.029002321139164264	eta = 0.9090909090909091
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 18.041876240655995	eta = 0.0014613638924501724
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 1.8124320127004978	eta = 0.014547164420730322
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 1.775015665989668	eta = 0.01485381058620065
af = 0.02636574649014933	bf = 1.6376576432937124	zeta = 1.7750098831933823	eta = 0.01485385897835976
eta = 0.01485385897835976
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [1.65835243e-04 1.51995270e-03 1.59487963e-04 6.39002520e-06
 2.36032919e-03 2.60104392e-04 1.26291038e-05 4.51290031e-04
 2.14529013e-04 1.37257984e-04]
ene_total = [0.1681456  0.19135646 0.17138602 0.15296698 0.2125038  0.17320668
 0.15230083 0.15429171 0.22843322 0.17041859]
ti_comp = [0.35387355 0.35652047 0.35228915 0.35940333 0.35499852 0.35244077
 0.35977839 0.36323055 0.32606389 0.35252092]
ti_coms = [0.07726245 0.07461553 0.07884685 0.07173267 0.07613748 0.07869523
 0.07135761 0.06790545 0.10507211 0.07861508]
t_total = [28.94991188 28.94991188 28.94991188 28.94991188 28.94991188 28.94991188
 28.94991188 28.94991188 28.94991188 28.94991188]
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [1.47658462e-05 1.35335454e-04 1.42006892e-05 5.68963075e-07
 2.10161950e-04 2.31595009e-05 1.12448598e-06 4.01825275e-05
 1.91015032e-05 1.22213485e-05]
ene_total = [0.51979327 0.51011572 0.53039422 0.48170862 0.52535973 0.52997772
 0.47922748 0.45866954 0.70682062 0.52870501]
optimize_network iter = 1 obj = 5.270771946473455
eta = 0.6874196389895763
freqs = [4.36998634e+07 9.12260848e+07 4.31996322e+07 1.46839871e+07
 1.05791923e+08 5.08422020e+07 1.84211324e+07 6.04822990e+07
 4.89325984e+07 4.10822992e+07]
Done!
ene_coms = [0.00772625 0.00746155 0.00788468 0.00717327 0.00761375 0.00786952
 0.00713576 0.00679054 0.01050721 0.00786151]
ene_comp = [1.44371661e-05 1.32322957e-04 1.38845892e-05 5.56298252e-07
 2.05483854e-04 2.26439824e-05 1.09945550e-06 3.92880853e-05
 1.86763136e-05 1.19493076e-05]
ene_total = [0.00774068 0.00759388 0.00789857 0.00717382 0.00781923 0.00789217
 0.00713686 0.00682983 0.01052589 0.00787346]
At round 21 energy consumption: 0.07848438862700059
At round 21 eta: 0.6874196389895763
At round 21 a_n: 20.989140069898713
At round 21 local rounds: 12.273194960737252
At round 21 global rounds: 67.14798076901184
gradient difference: 0.3961276710033417
train() client id: f_00000-0-0 loss: 1.046361  [   32/  126]
train() client id: f_00000-0-1 loss: 1.242517  [   64/  126]
train() client id: f_00000-0-2 loss: 1.127757  [   96/  126]
train() client id: f_00000-1-0 loss: 1.053052  [   32/  126]
train() client id: f_00000-1-1 loss: 1.096914  [   64/  126]
train() client id: f_00000-1-2 loss: 0.957195  [   96/  126]
train() client id: f_00000-2-0 loss: 1.016555  [   32/  126]
train() client id: f_00000-2-1 loss: 0.992574  [   64/  126]
train() client id: f_00000-2-2 loss: 1.025650  [   96/  126]
train() client id: f_00000-3-0 loss: 0.917162  [   32/  126]
train() client id: f_00000-3-1 loss: 1.047475  [   64/  126]
train() client id: f_00000-3-2 loss: 0.906388  [   96/  126]
train() client id: f_00000-4-0 loss: 0.915785  [   32/  126]
train() client id: f_00000-4-1 loss: 0.902992  [   64/  126]
train() client id: f_00000-4-2 loss: 0.916408  [   96/  126]
train() client id: f_00000-5-0 loss: 0.774769  [   32/  126]
train() client id: f_00000-5-1 loss: 0.821630  [   64/  126]
train() client id: f_00000-5-2 loss: 0.904511  [   96/  126]
train() client id: f_00000-6-0 loss: 0.828546  [   32/  126]
train() client id: f_00000-6-1 loss: 0.927652  [   64/  126]
train() client id: f_00000-6-2 loss: 0.837341  [   96/  126]
train() client id: f_00000-7-0 loss: 0.857644  [   32/  126]
train() client id: f_00000-7-1 loss: 0.846503  [   64/  126]
train() client id: f_00000-7-2 loss: 0.784383  [   96/  126]
train() client id: f_00000-8-0 loss: 0.764612  [   32/  126]
train() client id: f_00000-8-1 loss: 0.778391  [   64/  126]
train() client id: f_00000-8-2 loss: 0.850563  [   96/  126]
train() client id: f_00000-9-0 loss: 0.889922  [   32/  126]
train() client id: f_00000-9-1 loss: 0.759927  [   64/  126]
train() client id: f_00000-9-2 loss: 0.827527  [   96/  126]
train() client id: f_00000-10-0 loss: 0.707611  [   32/  126]
train() client id: f_00000-10-1 loss: 0.914287  [   64/  126]
train() client id: f_00000-10-2 loss: 0.795563  [   96/  126]
train() client id: f_00000-11-0 loss: 0.759569  [   32/  126]
train() client id: f_00000-11-1 loss: 0.783191  [   64/  126]
train() client id: f_00000-11-2 loss: 0.799632  [   96/  126]
train() client id: f_00001-0-0 loss: 0.425299  [   32/  265]
train() client id: f_00001-0-1 loss: 0.367035  [   64/  265]
train() client id: f_00001-0-2 loss: 0.410470  [   96/  265]
train() client id: f_00001-0-3 loss: 0.514977  [  128/  265]
train() client id: f_00001-0-4 loss: 0.439810  [  160/  265]
train() client id: f_00001-0-5 loss: 0.322737  [  192/  265]
train() client id: f_00001-0-6 loss: 0.395127  [  224/  265]
train() client id: f_00001-0-7 loss: 0.381502  [  256/  265]
train() client id: f_00001-1-0 loss: 0.375067  [   32/  265]
train() client id: f_00001-1-1 loss: 0.370732  [   64/  265]
train() client id: f_00001-1-2 loss: 0.420700  [   96/  265]
train() client id: f_00001-1-3 loss: 0.470354  [  128/  265]
train() client id: f_00001-1-4 loss: 0.497217  [  160/  265]
train() client id: f_00001-1-5 loss: 0.318898  [  192/  265]
train() client id: f_00001-1-6 loss: 0.389623  [  224/  265]
train() client id: f_00001-1-7 loss: 0.310855  [  256/  265]
train() client id: f_00001-2-0 loss: 0.458909  [   32/  265]
train() client id: f_00001-2-1 loss: 0.361387  [   64/  265]
train() client id: f_00001-2-2 loss: 0.285303  [   96/  265]
train() client id: f_00001-2-3 loss: 0.416324  [  128/  265]
train() client id: f_00001-2-4 loss: 0.354563  [  160/  265]
train() client id: f_00001-2-5 loss: 0.398786  [  192/  265]
train() client id: f_00001-2-6 loss: 0.422297  [  224/  265]
train() client id: f_00001-2-7 loss: 0.304446  [  256/  265]
train() client id: f_00001-3-0 loss: 0.364432  [   32/  265]
train() client id: f_00001-3-1 loss: 0.286190  [   64/  265]
train() client id: f_00001-3-2 loss: 0.356141  [   96/  265]
train() client id: f_00001-3-3 loss: 0.399083  [  128/  265]
train() client id: f_00001-3-4 loss: 0.384750  [  160/  265]
train() client id: f_00001-3-5 loss: 0.292874  [  192/  265]
train() client id: f_00001-3-6 loss: 0.494755  [  224/  265]
train() client id: f_00001-3-7 loss: 0.411308  [  256/  265]
train() client id: f_00001-4-0 loss: 0.341181  [   32/  265]
train() client id: f_00001-4-1 loss: 0.320909  [   64/  265]
train() client id: f_00001-4-2 loss: 0.329641  [   96/  265]
train() client id: f_00001-4-3 loss: 0.437353  [  128/  265]
train() client id: f_00001-4-4 loss: 0.401315  [  160/  265]
train() client id: f_00001-4-5 loss: 0.321950  [  192/  265]
train() client id: f_00001-4-6 loss: 0.355809  [  224/  265]
train() client id: f_00001-4-7 loss: 0.431077  [  256/  265]
train() client id: f_00001-5-0 loss: 0.447149  [   32/  265]
train() client id: f_00001-5-1 loss: 0.321447  [   64/  265]
train() client id: f_00001-5-2 loss: 0.271271  [   96/  265]
train() client id: f_00001-5-3 loss: 0.452827  [  128/  265]
train() client id: f_00001-5-4 loss: 0.266972  [  160/  265]
train() client id: f_00001-5-5 loss: 0.401905  [  192/  265]
train() client id: f_00001-5-6 loss: 0.318941  [  224/  265]
train() client id: f_00001-5-7 loss: 0.405527  [  256/  265]
train() client id: f_00001-6-0 loss: 0.271107  [   32/  265]
train() client id: f_00001-6-1 loss: 0.335603  [   64/  265]
train() client id: f_00001-6-2 loss: 0.432090  [   96/  265]
train() client id: f_00001-6-3 loss: 0.336092  [  128/  265]
train() client id: f_00001-6-4 loss: 0.268697  [  160/  265]
train() client id: f_00001-6-5 loss: 0.451185  [  192/  265]
train() client id: f_00001-6-6 loss: 0.347952  [  224/  265]
train() client id: f_00001-6-7 loss: 0.366805  [  256/  265]
train() client id: f_00001-7-0 loss: 0.432409  [   32/  265]
train() client id: f_00001-7-1 loss: 0.405205  [   64/  265]
train() client id: f_00001-7-2 loss: 0.251377  [   96/  265]
train() client id: f_00001-7-3 loss: 0.418354  [  128/  265]
train() client id: f_00001-7-4 loss: 0.253882  [  160/  265]
train() client id: f_00001-7-5 loss: 0.262670  [  192/  265]
train() client id: f_00001-7-6 loss: 0.337600  [  224/  265]
train() client id: f_00001-7-7 loss: 0.391630  [  256/  265]
train() client id: f_00001-8-0 loss: 0.263394  [   32/  265]
train() client id: f_00001-8-1 loss: 0.331728  [   64/  265]
train() client id: f_00001-8-2 loss: 0.500610  [   96/  265]
train() client id: f_00001-8-3 loss: 0.329637  [  128/  265]
train() client id: f_00001-8-4 loss: 0.323530  [  160/  265]
train() client id: f_00001-8-5 loss: 0.307873  [  192/  265]
train() client id: f_00001-8-6 loss: 0.417879  [  224/  265]
train() client id: f_00001-8-7 loss: 0.318761  [  256/  265]
train() client id: f_00001-9-0 loss: 0.337179  [   32/  265]
train() client id: f_00001-9-1 loss: 0.316041  [   64/  265]
train() client id: f_00001-9-2 loss: 0.446733  [   96/  265]
train() client id: f_00001-9-3 loss: 0.315184  [  128/  265]
train() client id: f_00001-9-4 loss: 0.314322  [  160/  265]
train() client id: f_00001-9-5 loss: 0.351857  [  192/  265]
train() client id: f_00001-9-6 loss: 0.432605  [  224/  265]
train() client id: f_00001-9-7 loss: 0.255704  [  256/  265]
train() client id: f_00001-10-0 loss: 0.322083  [   32/  265]
train() client id: f_00001-10-1 loss: 0.288220  [   64/  265]
train() client id: f_00001-10-2 loss: 0.403137  [   96/  265]
train() client id: f_00001-10-3 loss: 0.233757  [  128/  265]
train() client id: f_00001-10-4 loss: 0.235628  [  160/  265]
train() client id: f_00001-10-5 loss: 0.468882  [  192/  265]
train() client id: f_00001-10-6 loss: 0.409898  [  224/  265]
train() client id: f_00001-10-7 loss: 0.343434  [  256/  265]
train() client id: f_00001-11-0 loss: 0.431866  [   32/  265]
train() client id: f_00001-11-1 loss: 0.272423  [   64/  265]
train() client id: f_00001-11-2 loss: 0.336694  [   96/  265]
train() client id: f_00001-11-3 loss: 0.416995  [  128/  265]
train() client id: f_00001-11-4 loss: 0.322099  [  160/  265]
train() client id: f_00001-11-5 loss: 0.250394  [  192/  265]
train() client id: f_00001-11-6 loss: 0.370296  [  224/  265]
train() client id: f_00001-11-7 loss: 0.337425  [  256/  265]
train() client id: f_00002-0-0 loss: 1.196948  [   32/  124]
train() client id: f_00002-0-1 loss: 1.203400  [   64/  124]
train() client id: f_00002-0-2 loss: 1.258497  [   96/  124]
train() client id: f_00002-1-0 loss: 1.164812  [   32/  124]
train() client id: f_00002-1-1 loss: 1.271335  [   64/  124]
train() client id: f_00002-1-2 loss: 1.149718  [   96/  124]
train() client id: f_00002-2-0 loss: 1.263541  [   32/  124]
train() client id: f_00002-2-1 loss: 1.071821  [   64/  124]
train() client id: f_00002-2-2 loss: 1.059172  [   96/  124]
train() client id: f_00002-3-0 loss: 1.089939  [   32/  124]
train() client id: f_00002-3-1 loss: 1.096243  [   64/  124]
train() client id: f_00002-3-2 loss: 1.139596  [   96/  124]
train() client id: f_00002-4-0 loss: 1.280836  [   32/  124]
train() client id: f_00002-4-1 loss: 1.032684  [   64/  124]
train() client id: f_00002-4-2 loss: 1.006995  [   96/  124]
train() client id: f_00002-5-0 loss: 0.959559  [   32/  124]
train() client id: f_00002-5-1 loss: 0.973020  [   64/  124]
train() client id: f_00002-5-2 loss: 1.184422  [   96/  124]
train() client id: f_00002-6-0 loss: 1.080145  [   32/  124]
train() client id: f_00002-6-1 loss: 0.945287  [   64/  124]
train() client id: f_00002-6-2 loss: 0.982376  [   96/  124]
train() client id: f_00002-7-0 loss: 1.025173  [   32/  124]
train() client id: f_00002-7-1 loss: 0.922750  [   64/  124]
train() client id: f_00002-7-2 loss: 0.883008  [   96/  124]
train() client id: f_00002-8-0 loss: 1.040401  [   32/  124]
train() client id: f_00002-8-1 loss: 1.091905  [   64/  124]
train() client id: f_00002-8-2 loss: 0.949109  [   96/  124]
train() client id: f_00002-9-0 loss: 1.031838  [   32/  124]
train() client id: f_00002-9-1 loss: 0.939541  [   64/  124]
train() client id: f_00002-9-2 loss: 0.997056  [   96/  124]
train() client id: f_00002-10-0 loss: 1.016229  [   32/  124]
train() client id: f_00002-10-1 loss: 0.982413  [   64/  124]
train() client id: f_00002-10-2 loss: 0.960843  [   96/  124]
train() client id: f_00002-11-0 loss: 0.904340  [   32/  124]
train() client id: f_00002-11-1 loss: 1.093166  [   64/  124]
train() client id: f_00002-11-2 loss: 0.977115  [   96/  124]
train() client id: f_00003-0-0 loss: 0.816005  [   32/   43]
train() client id: f_00003-1-0 loss: 0.965296  [   32/   43]
train() client id: f_00003-2-0 loss: 0.959360  [   32/   43]
train() client id: f_00003-3-0 loss: 0.850624  [   32/   43]
train() client id: f_00003-4-0 loss: 0.825207  [   32/   43]
train() client id: f_00003-5-0 loss: 0.897518  [   32/   43]
train() client id: f_00003-6-0 loss: 0.761416  [   32/   43]
train() client id: f_00003-7-0 loss: 0.998442  [   32/   43]
train() client id: f_00003-8-0 loss: 0.934076  [   32/   43]
train() client id: f_00003-9-0 loss: 0.742114  [   32/   43]
train() client id: f_00003-10-0 loss: 0.994441  [   32/   43]
train() client id: f_00003-11-0 loss: 0.877841  [   32/   43]
train() client id: f_00004-0-0 loss: 0.920138  [   32/  306]
train() client id: f_00004-0-1 loss: 0.963926  [   64/  306]
train() client id: f_00004-0-2 loss: 0.945328  [   96/  306]
train() client id: f_00004-0-3 loss: 0.899160  [  128/  306]
train() client id: f_00004-0-4 loss: 0.882642  [  160/  306]
train() client id: f_00004-0-5 loss: 0.748052  [  192/  306]
train() client id: f_00004-0-6 loss: 0.921085  [  224/  306]
train() client id: f_00004-0-7 loss: 1.027201  [  256/  306]
train() client id: f_00004-0-8 loss: 0.993006  [  288/  306]
train() client id: f_00004-1-0 loss: 1.000339  [   32/  306]
train() client id: f_00004-1-1 loss: 0.990054  [   64/  306]
train() client id: f_00004-1-2 loss: 0.792300  [   96/  306]
train() client id: f_00004-1-3 loss: 0.857610  [  128/  306]
train() client id: f_00004-1-4 loss: 1.041996  [  160/  306]
train() client id: f_00004-1-5 loss: 0.929049  [  192/  306]
train() client id: f_00004-1-6 loss: 0.879147  [  224/  306]
train() client id: f_00004-1-7 loss: 0.959985  [  256/  306]
train() client id: f_00004-1-8 loss: 0.826242  [  288/  306]
train() client id: f_00004-2-0 loss: 0.939629  [   32/  306]
train() client id: f_00004-2-1 loss: 1.001321  [   64/  306]
train() client id: f_00004-2-2 loss: 0.980166  [   96/  306]
train() client id: f_00004-2-3 loss: 0.931386  [  128/  306]
train() client id: f_00004-2-4 loss: 0.875534  [  160/  306]
train() client id: f_00004-2-5 loss: 0.727256  [  192/  306]
train() client id: f_00004-2-6 loss: 0.970340  [  224/  306]
train() client id: f_00004-2-7 loss: 0.783063  [  256/  306]
train() client id: f_00004-2-8 loss: 1.016053  [  288/  306]
train() client id: f_00004-3-0 loss: 0.824765  [   32/  306]
train() client id: f_00004-3-1 loss: 0.860232  [   64/  306]
train() client id: f_00004-3-2 loss: 0.935232  [   96/  306]
train() client id: f_00004-3-3 loss: 0.872957  [  128/  306]
train() client id: f_00004-3-4 loss: 0.958440  [  160/  306]
train() client id: f_00004-3-5 loss: 1.012607  [  192/  306]
train() client id: f_00004-3-6 loss: 0.830616  [  224/  306]
train() client id: f_00004-3-7 loss: 0.981383  [  256/  306]
train() client id: f_00004-3-8 loss: 0.955189  [  288/  306]
train() client id: f_00004-4-0 loss: 0.812501  [   32/  306]
train() client id: f_00004-4-1 loss: 0.894807  [   64/  306]
train() client id: f_00004-4-2 loss: 0.960710  [   96/  306]
train() client id: f_00004-4-3 loss: 0.971458  [  128/  306]
train() client id: f_00004-4-4 loss: 0.864907  [  160/  306]
train() client id: f_00004-4-5 loss: 0.811480  [  192/  306]
train() client id: f_00004-4-6 loss: 0.884825  [  224/  306]
train() client id: f_00004-4-7 loss: 0.976961  [  256/  306]
train() client id: f_00004-4-8 loss: 0.939759  [  288/  306]
train() client id: f_00004-5-0 loss: 1.032447  [   32/  306]
train() client id: f_00004-5-1 loss: 0.876577  [   64/  306]
train() client id: f_00004-5-2 loss: 0.848654  [   96/  306]
train() client id: f_00004-5-3 loss: 0.989833  [  128/  306]
train() client id: f_00004-5-4 loss: 0.835416  [  160/  306]
train() client id: f_00004-5-5 loss: 0.967439  [  192/  306]
train() client id: f_00004-5-6 loss: 0.992481  [  224/  306]
train() client id: f_00004-5-7 loss: 0.800802  [  256/  306]
train() client id: f_00004-5-8 loss: 0.902630  [  288/  306]
train() client id: f_00004-6-0 loss: 0.783472  [   32/  306]
train() client id: f_00004-6-1 loss: 0.902149  [   64/  306]
train() client id: f_00004-6-2 loss: 0.923999  [   96/  306]
train() client id: f_00004-6-3 loss: 1.090437  [  128/  306]
train() client id: f_00004-6-4 loss: 0.888250  [  160/  306]
train() client id: f_00004-6-5 loss: 0.880268  [  192/  306]
train() client id: f_00004-6-6 loss: 0.879093  [  224/  306]
train() client id: f_00004-6-7 loss: 0.981733  [  256/  306]
train() client id: f_00004-6-8 loss: 0.909854  [  288/  306]
train() client id: f_00004-7-0 loss: 0.965852  [   32/  306]
train() client id: f_00004-7-1 loss: 0.935849  [   64/  306]
train() client id: f_00004-7-2 loss: 0.912169  [   96/  306]
train() client id: f_00004-7-3 loss: 1.024818  [  128/  306]
train() client id: f_00004-7-4 loss: 0.875912  [  160/  306]
train() client id: f_00004-7-5 loss: 0.867596  [  192/  306]
train() client id: f_00004-7-6 loss: 0.858511  [  224/  306]
train() client id: f_00004-7-7 loss: 0.934432  [  256/  306]
train() client id: f_00004-7-8 loss: 0.833002  [  288/  306]
train() client id: f_00004-8-0 loss: 0.933611  [   32/  306]
train() client id: f_00004-8-1 loss: 0.987296  [   64/  306]
train() client id: f_00004-8-2 loss: 0.975007  [   96/  306]
train() client id: f_00004-8-3 loss: 0.912494  [  128/  306]
train() client id: f_00004-8-4 loss: 0.811252  [  160/  306]
train() client id: f_00004-8-5 loss: 0.964842  [  192/  306]
train() client id: f_00004-8-6 loss: 1.000011  [  224/  306]
train() client id: f_00004-8-7 loss: 0.890478  [  256/  306]
train() client id: f_00004-8-8 loss: 0.776342  [  288/  306]
train() client id: f_00004-9-0 loss: 0.900563  [   32/  306]
train() client id: f_00004-9-1 loss: 0.861004  [   64/  306]
train() client id: f_00004-9-2 loss: 0.822312  [   96/  306]
train() client id: f_00004-9-3 loss: 0.852188  [  128/  306]
train() client id: f_00004-9-4 loss: 0.962275  [  160/  306]
train() client id: f_00004-9-5 loss: 0.892025  [  192/  306]
train() client id: f_00004-9-6 loss: 0.957948  [  224/  306]
train() client id: f_00004-9-7 loss: 0.861864  [  256/  306]
train() client id: f_00004-9-8 loss: 1.070348  [  288/  306]
train() client id: f_00004-10-0 loss: 0.881769  [   32/  306]
train() client id: f_00004-10-1 loss: 0.924519  [   64/  306]
train() client id: f_00004-10-2 loss: 0.940324  [   96/  306]
train() client id: f_00004-10-3 loss: 0.951942  [  128/  306]
train() client id: f_00004-10-4 loss: 0.953704  [  160/  306]
train() client id: f_00004-10-5 loss: 0.902522  [  192/  306]
train() client id: f_00004-10-6 loss: 0.883966  [  224/  306]
train() client id: f_00004-10-7 loss: 0.855052  [  256/  306]
train() client id: f_00004-10-8 loss: 0.897491  [  288/  306]
train() client id: f_00004-11-0 loss: 0.944160  [   32/  306]
train() client id: f_00004-11-1 loss: 0.930729  [   64/  306]
train() client id: f_00004-11-2 loss: 0.959666  [   96/  306]
train() client id: f_00004-11-3 loss: 0.909571  [  128/  306]
train() client id: f_00004-11-4 loss: 0.913953  [  160/  306]
train() client id: f_00004-11-5 loss: 0.997302  [  192/  306]
train() client id: f_00004-11-6 loss: 0.936500  [  224/  306]
train() client id: f_00004-11-7 loss: 0.726631  [  256/  306]
train() client id: f_00004-11-8 loss: 0.911541  [  288/  306]
train() client id: f_00005-0-0 loss: 0.521526  [   32/  146]
train() client id: f_00005-0-1 loss: 0.428800  [   64/  146]
train() client id: f_00005-0-2 loss: 0.464662  [   96/  146]
train() client id: f_00005-0-3 loss: 0.444121  [  128/  146]
train() client id: f_00005-1-0 loss: 0.643280  [   32/  146]
train() client id: f_00005-1-1 loss: 0.395185  [   64/  146]
train() client id: f_00005-1-2 loss: 0.510543  [   96/  146]
train() client id: f_00005-1-3 loss: 0.417838  [  128/  146]
train() client id: f_00005-2-0 loss: 0.425383  [   32/  146]
train() client id: f_00005-2-1 loss: 0.472557  [   64/  146]
train() client id: f_00005-2-2 loss: 0.523119  [   96/  146]
train() client id: f_00005-2-3 loss: 0.417285  [  128/  146]
train() client id: f_00005-3-0 loss: 0.368279  [   32/  146]
train() client id: f_00005-3-1 loss: 0.507199  [   64/  146]
train() client id: f_00005-3-2 loss: 0.563295  [   96/  146]
train() client id: f_00005-3-3 loss: 0.567076  [  128/  146]
train() client id: f_00005-4-0 loss: 0.566266  [   32/  146]
train() client id: f_00005-4-1 loss: 0.457691  [   64/  146]
train() client id: f_00005-4-2 loss: 0.533889  [   96/  146]
train() client id: f_00005-4-3 loss: 0.427571  [  128/  146]
train() client id: f_00005-5-0 loss: 0.658607  [   32/  146]
train() client id: f_00005-5-1 loss: 0.350163  [   64/  146]
train() client id: f_00005-5-2 loss: 0.557871  [   96/  146]
train() client id: f_00005-5-3 loss: 0.284206  [  128/  146]
train() client id: f_00005-6-0 loss: 0.352311  [   32/  146]
train() client id: f_00005-6-1 loss: 0.409183  [   64/  146]
train() client id: f_00005-6-2 loss: 0.638901  [   96/  146]
train() client id: f_00005-6-3 loss: 0.456692  [  128/  146]
train() client id: f_00005-7-0 loss: 0.745304  [   32/  146]
train() client id: f_00005-7-1 loss: 0.441744  [   64/  146]
train() client id: f_00005-7-2 loss: 0.325656  [   96/  146]
train() client id: f_00005-7-3 loss: 0.335241  [  128/  146]
train() client id: f_00005-8-0 loss: 0.320389  [   32/  146]
train() client id: f_00005-8-1 loss: 0.614734  [   64/  146]
train() client id: f_00005-8-2 loss: 0.685617  [   96/  146]
train() client id: f_00005-8-3 loss: 0.253432  [  128/  146]
train() client id: f_00005-9-0 loss: 0.354384  [   32/  146]
train() client id: f_00005-9-1 loss: 0.733926  [   64/  146]
train() client id: f_00005-9-2 loss: 0.471249  [   96/  146]
train() client id: f_00005-9-3 loss: 0.378711  [  128/  146]
train() client id: f_00005-10-0 loss: 0.258147  [   32/  146]
train() client id: f_00005-10-1 loss: 0.399281  [   64/  146]
train() client id: f_00005-10-2 loss: 0.673499  [   96/  146]
train() client id: f_00005-10-3 loss: 0.498046  [  128/  146]
train() client id: f_00005-11-0 loss: 0.436679  [   32/  146]
train() client id: f_00005-11-1 loss: 0.435590  [   64/  146]
train() client id: f_00005-11-2 loss: 0.314466  [   96/  146]
train() client id: f_00005-11-3 loss: 0.628515  [  128/  146]
train() client id: f_00006-0-0 loss: 0.500429  [   32/   54]
train() client id: f_00006-1-0 loss: 0.543862  [   32/   54]
train() client id: f_00006-2-0 loss: 0.584849  [   32/   54]
train() client id: f_00006-3-0 loss: 0.574470  [   32/   54]
train() client id: f_00006-4-0 loss: 0.496338  [   32/   54]
train() client id: f_00006-5-0 loss: 0.541649  [   32/   54]
train() client id: f_00006-6-0 loss: 0.538165  [   32/   54]
train() client id: f_00006-7-0 loss: 0.546072  [   32/   54]
train() client id: f_00006-8-0 loss: 0.589619  [   32/   54]
train() client id: f_00006-9-0 loss: 0.546320  [   32/   54]
train() client id: f_00006-10-0 loss: 0.574462  [   32/   54]
train() client id: f_00006-11-0 loss: 0.541892  [   32/   54]
train() client id: f_00007-0-0 loss: 0.653606  [   32/  179]
train() client id: f_00007-0-1 loss: 0.584594  [   64/  179]
train() client id: f_00007-0-2 loss: 0.620432  [   96/  179]
train() client id: f_00007-0-3 loss: 0.701608  [  128/  179]
train() client id: f_00007-0-4 loss: 0.540506  [  160/  179]
train() client id: f_00007-1-0 loss: 0.564967  [   32/  179]
train() client id: f_00007-1-1 loss: 0.733873  [   64/  179]
train() client id: f_00007-1-2 loss: 0.570880  [   96/  179]
train() client id: f_00007-1-3 loss: 0.620211  [  128/  179]
train() client id: f_00007-1-4 loss: 0.552496  [  160/  179]
train() client id: f_00007-2-0 loss: 0.530289  [   32/  179]
train() client id: f_00007-2-1 loss: 0.728066  [   64/  179]
train() client id: f_00007-2-2 loss: 0.570642  [   96/  179]
train() client id: f_00007-2-3 loss: 0.587920  [  128/  179]
train() client id: f_00007-2-4 loss: 0.597933  [  160/  179]
train() client id: f_00007-3-0 loss: 0.552316  [   32/  179]
train() client id: f_00007-3-1 loss: 0.520943  [   64/  179]
train() client id: f_00007-3-2 loss: 0.669152  [   96/  179]
train() client id: f_00007-3-3 loss: 0.453887  [  128/  179]
train() client id: f_00007-3-4 loss: 0.704179  [  160/  179]
train() client id: f_00007-4-0 loss: 0.554265  [   32/  179]
train() client id: f_00007-4-1 loss: 0.592697  [   64/  179]
train() client id: f_00007-4-2 loss: 0.594983  [   96/  179]
train() client id: f_00007-4-3 loss: 0.710974  [  128/  179]
train() client id: f_00007-4-4 loss: 0.442024  [  160/  179]
train() client id: f_00007-5-0 loss: 0.446926  [   32/  179]
train() client id: f_00007-5-1 loss: 0.546522  [   64/  179]
train() client id: f_00007-5-2 loss: 0.633600  [   96/  179]
train() client id: f_00007-5-3 loss: 0.721064  [  128/  179]
train() client id: f_00007-5-4 loss: 0.522841  [  160/  179]
train() client id: f_00007-6-0 loss: 0.523328  [   32/  179]
train() client id: f_00007-6-1 loss: 0.583152  [   64/  179]
train() client id: f_00007-6-2 loss: 0.670730  [   96/  179]
train() client id: f_00007-6-3 loss: 0.510679  [  128/  179]
train() client id: f_00007-6-4 loss: 0.660492  [  160/  179]
train() client id: f_00007-7-0 loss: 0.595932  [   32/  179]
train() client id: f_00007-7-1 loss: 0.438892  [   64/  179]
train() client id: f_00007-7-2 loss: 0.758353  [   96/  179]
train() client id: f_00007-7-3 loss: 0.718503  [  128/  179]
train() client id: f_00007-7-4 loss: 0.441775  [  160/  179]
train() client id: f_00007-8-0 loss: 0.737074  [   32/  179]
train() client id: f_00007-8-1 loss: 0.816152  [   64/  179]
train() client id: f_00007-8-2 loss: 0.422510  [   96/  179]
train() client id: f_00007-8-3 loss: 0.418569  [  128/  179]
train() client id: f_00007-8-4 loss: 0.455964  [  160/  179]
train() client id: f_00007-9-0 loss: 0.431476  [   32/  179]
train() client id: f_00007-9-1 loss: 0.609528  [   64/  179]
train() client id: f_00007-9-2 loss: 0.643340  [   96/  179]
train() client id: f_00007-9-3 loss: 0.517568  [  128/  179]
train() client id: f_00007-9-4 loss: 0.636802  [  160/  179]
train() client id: f_00007-10-0 loss: 0.583526  [   32/  179]
train() client id: f_00007-10-1 loss: 0.590897  [   64/  179]
train() client id: f_00007-10-2 loss: 0.603361  [   96/  179]
train() client id: f_00007-10-3 loss: 0.602505  [  128/  179]
train() client id: f_00007-10-4 loss: 0.551044  [  160/  179]
train() client id: f_00007-11-0 loss: 0.520953  [   32/  179]
train() client id: f_00007-11-1 loss: 0.787160  [   64/  179]
train() client id: f_00007-11-2 loss: 0.518167  [   96/  179]
train() client id: f_00007-11-3 loss: 0.545839  [  128/  179]
train() client id: f_00007-11-4 loss: 0.548162  [  160/  179]
train() client id: f_00008-0-0 loss: 0.749281  [   32/  130]
train() client id: f_00008-0-1 loss: 0.865066  [   64/  130]
train() client id: f_00008-0-2 loss: 0.814341  [   96/  130]
train() client id: f_00008-0-3 loss: 0.670160  [  128/  130]
train() client id: f_00008-1-0 loss: 0.734621  [   32/  130]
train() client id: f_00008-1-1 loss: 0.731362  [   64/  130]
train() client id: f_00008-1-2 loss: 0.788714  [   96/  130]
train() client id: f_00008-1-3 loss: 0.816057  [  128/  130]
train() client id: f_00008-2-0 loss: 0.756855  [   32/  130]
train() client id: f_00008-2-1 loss: 0.724788  [   64/  130]
train() client id: f_00008-2-2 loss: 0.794485  [   96/  130]
train() client id: f_00008-2-3 loss: 0.814136  [  128/  130]
train() client id: f_00008-3-0 loss: 0.685595  [   32/  130]
train() client id: f_00008-3-1 loss: 0.881988  [   64/  130]
train() client id: f_00008-3-2 loss: 0.732034  [   96/  130]
train() client id: f_00008-3-3 loss: 0.785247  [  128/  130]
train() client id: f_00008-4-0 loss: 0.785396  [   32/  130]
train() client id: f_00008-4-1 loss: 0.706251  [   64/  130]
train() client id: f_00008-4-2 loss: 0.746817  [   96/  130]
train() client id: f_00008-4-3 loss: 0.808543  [  128/  130]
train() client id: f_00008-5-0 loss: 0.720838  [   32/  130]
train() client id: f_00008-5-1 loss: 0.787657  [   64/  130]
train() client id: f_00008-5-2 loss: 0.716403  [   96/  130]
train() client id: f_00008-5-3 loss: 0.821509  [  128/  130]
train() client id: f_00008-6-0 loss: 0.738617  [   32/  130]
train() client id: f_00008-6-1 loss: 0.843882  [   64/  130]
train() client id: f_00008-6-2 loss: 0.742512  [   96/  130]
train() client id: f_00008-6-3 loss: 0.754183  [  128/  130]
train() client id: f_00008-7-0 loss: 0.731610  [   32/  130]
train() client id: f_00008-7-1 loss: 0.747831  [   64/  130]
train() client id: f_00008-7-2 loss: 0.831852  [   96/  130]
train() client id: f_00008-7-3 loss: 0.771717  [  128/  130]
train() client id: f_00008-8-0 loss: 0.638016  [   32/  130]
train() client id: f_00008-8-1 loss: 0.812962  [   64/  130]
train() client id: f_00008-8-2 loss: 0.807758  [   96/  130]
train() client id: f_00008-8-3 loss: 0.820393  [  128/  130]
train() client id: f_00008-9-0 loss: 0.683343  [   32/  130]
train() client id: f_00008-9-1 loss: 0.755027  [   64/  130]
train() client id: f_00008-9-2 loss: 0.871821  [   96/  130]
train() client id: f_00008-9-3 loss: 0.732689  [  128/  130]
train() client id: f_00008-10-0 loss: 0.754861  [   32/  130]
train() client id: f_00008-10-1 loss: 0.756650  [   64/  130]
train() client id: f_00008-10-2 loss: 0.766088  [   96/  130]
train() client id: f_00008-10-3 loss: 0.794164  [  128/  130]
train() client id: f_00008-11-0 loss: 0.666360  [   32/  130]
train() client id: f_00008-11-1 loss: 0.809318  [   64/  130]
train() client id: f_00008-11-2 loss: 0.765918  [   96/  130]
train() client id: f_00008-11-3 loss: 0.787329  [  128/  130]
train() client id: f_00009-0-0 loss: 1.130876  [   32/  118]
train() client id: f_00009-0-1 loss: 1.044341  [   64/  118]
train() client id: f_00009-0-2 loss: 1.175650  [   96/  118]
train() client id: f_00009-1-0 loss: 1.015366  [   32/  118]
train() client id: f_00009-1-1 loss: 1.082129  [   64/  118]
train() client id: f_00009-1-2 loss: 0.968472  [   96/  118]
train() client id: f_00009-2-0 loss: 1.074677  [   32/  118]
train() client id: f_00009-2-1 loss: 1.005182  [   64/  118]
train() client id: f_00009-2-2 loss: 0.946822  [   96/  118]
train() client id: f_00009-3-0 loss: 1.025599  [   32/  118]
train() client id: f_00009-3-1 loss: 0.929183  [   64/  118]
train() client id: f_00009-3-2 loss: 0.943101  [   96/  118]
train() client id: f_00009-4-0 loss: 0.949145  [   32/  118]
train() client id: f_00009-4-1 loss: 0.823584  [   64/  118]
train() client id: f_00009-4-2 loss: 0.952124  [   96/  118]
train() client id: f_00009-5-0 loss: 0.762816  [   32/  118]
train() client id: f_00009-5-1 loss: 0.958458  [   64/  118]
train() client id: f_00009-5-2 loss: 0.920146  [   96/  118]
train() client id: f_00009-6-0 loss: 0.923660  [   32/  118]
train() client id: f_00009-6-1 loss: 0.788555  [   64/  118]
train() client id: f_00009-6-2 loss: 0.853940  [   96/  118]
train() client id: f_00009-7-0 loss: 0.869277  [   32/  118]
train() client id: f_00009-7-1 loss: 0.971627  [   64/  118]
train() client id: f_00009-7-2 loss: 0.749310  [   96/  118]
train() client id: f_00009-8-0 loss: 0.888167  [   32/  118]
train() client id: f_00009-8-1 loss: 0.723273  [   64/  118]
train() client id: f_00009-8-2 loss: 0.870757  [   96/  118]
train() client id: f_00009-9-0 loss: 0.814019  [   32/  118]
train() client id: f_00009-9-1 loss: 0.725679  [   64/  118]
train() client id: f_00009-9-2 loss: 0.949883  [   96/  118]
train() client id: f_00009-10-0 loss: 0.817799  [   32/  118]
train() client id: f_00009-10-1 loss: 0.719766  [   64/  118]
train() client id: f_00009-10-2 loss: 0.841636  [   96/  118]
train() client id: f_00009-11-0 loss: 0.799630  [   32/  118]
train() client id: f_00009-11-1 loss: 0.753314  [   64/  118]
train() client id: f_00009-11-2 loss: 0.890904  [   96/  118]
At round 21 accuracy: 0.6419098143236074
At round 21 training accuracy: 0.5781354795439303
At round 21 training loss: 0.8423922320481798
update_location
xs = -3.905658 4.200318 125.009024 18.811294 0.979296 3.956410 -87.443192 -66.324852 109.663977 -52.060879 
ys = 117.587959 100.555839 1.320614 -87.455176 79.350187 62.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 154.409140 141.877128 160.090600 134.172548 127.661314 118.157821 132.865354 119.998593 149.448512 112.811112 
dists_bs = 180.469987 194.048157 346.655367 326.259923 200.293783 211.213308 198.032396 205.300127 325.296938 210.605130 
uav_gains = -104.723484 -103.799974 -105.119779 -103.192604 -102.651992 -101.811734 -103.086179 -101.979618 -104.366625 -101.308886 
bs_gains = -102.746399 -103.628526 -110.684201 -109.946844 -104.013748 -104.659255 -103.875674 -104.313957 -109.910898 -104.624190 
Round 22
-------------------------------
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.08847719 16.82258033  7.96606227  2.85607422 19.40433678  9.34589642
  3.54718528 11.40188453  8.39827017  7.58446914]
obj_prev = 95.4152363247567
eta_min = 3.3224549121166505e-12	eta_max = 0.9228798704318716
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 22.17659221087636	eta = 0.909090909090909
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 38.93866072382321	eta = 0.5177512014733847
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 30.874641848632802	eta = 0.6529804773886541
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 29.424734721470024	eta = 0.6851561641714188
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 29.351835344463147	eta = 0.6868578450692009
af = 20.160538373523963	bf = 1.6183737102877358	zeta = 29.351637432902866	eta = 0.6868624763988199
eta = 0.6868624763988199
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [0.03099536 0.06518865 0.03050337 0.01057778 0.07527445 0.03591526
 0.01328373 0.04403309 0.03197934 0.0290274 ]
ene_total = [2.56014735 4.7872217  2.53839595 1.17632747 5.46154932 2.88284624
 1.35232746 3.3560089  2.80903437 2.42777865]
ti_comp = [0.35997945 0.36414214 0.35835849 0.36568135 0.36272784 0.36023616
 0.36604767 0.36965205 0.33207169 0.36037562]
ti_coms = [0.07833233 0.07416965 0.07995329 0.07263043 0.07558394 0.07807562
 0.07226411 0.06865973 0.1062401  0.07793616]
t_total = [28.89990768 28.89990768 28.89990768 28.89990768 28.89990768 28.89990768
 28.89990768 28.89990768 28.89990768 28.89990768]
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [1.43619908e-05 1.30573243e-04 1.38130026e-05 5.53169442e-07
 2.02610025e-04 2.23121998e-05 1.09336312e-06 3.90509212e-05
 1.85363747e-05 1.17704659e-05]
ene_total = [0.51742795 0.49764382 0.5280795  0.47892225 0.51171862 0.51625955
 0.47654255 0.45527992 0.70171191 0.51464497]
optimize_network iter = 0 obj = 5.19823104157566
eta = 0.6868624763988199
freqs = [4.30515682e+07 8.95099024e+07 4.25598540e+07 1.44631133e+07
 1.03761604e+08 4.98496017e+07 1.81448029e+07 5.95601869e+07
 4.81512593e+07 4.02738129e+07]
eta_min = 0.6868624763988167	eta_max = 0.6868624763987461
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 0.02749108426556509	eta = 0.9090909090909091
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 17.828312615008755	eta = 0.0014018093201842298
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 1.7850725781134285	eta = 0.014000492245133406
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 1.749550246261795	eta = 0.014284753947636938
af = 0.024991894786877352	bf = 1.6183737102877358	zeta = 1.7495451585936168	eta = 0.014284795487626767
eta = 0.014284795487626767
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [1.62444973e-04 1.47688208e-03 1.56235501e-04 6.25676453e-06
 2.29167254e-03 2.52367846e-04 1.23667633e-05 4.41695438e-04
 2.09660410e-04 1.33132867e-04]
ene_total = [0.16747587 0.18628874 0.17074105 0.15226131 0.20631753 0.16882169
 0.151622   0.15306496 0.22691989 0.16603211]
ti_comp = [0.35997945 0.36414214 0.35835849 0.36568135 0.36272784 0.36023616
 0.36604767 0.36965205 0.33207169 0.36037562]
ti_coms = [0.07833233 0.07416965 0.07995329 0.07263043 0.07558394 0.07807562
 0.07226411 0.06865973 0.1062401  0.07793616]
t_total = [28.89990768 28.89990768 28.89990768 28.89990768 28.89990768 28.89990768
 28.89990768 28.89990768 28.89990768 28.89990768]
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [1.43619908e-05 1.30573243e-04 1.38130026e-05 5.53169442e-07
 2.02610025e-04 2.23121998e-05 1.09336312e-06 3.90509212e-05
 1.85363747e-05 1.17704659e-05]
ene_total = [0.51742795 0.49764382 0.5280795  0.47892225 0.51171862 0.51625955
 0.47654255 0.45527992 0.70171191 0.51464497]
optimize_network iter = 1 obj = 5.198231041575606
eta = 0.6868624763988167
freqs = [4.30515682e+07 8.95099024e+07 4.25598540e+07 1.44631133e+07
 1.03761604e+08 4.98496017e+07 1.81448029e+07 5.95601869e+07
 4.81512593e+07 4.02738129e+07]
Done!
ene_coms = [0.00783323 0.00741696 0.00799533 0.00726304 0.00755839 0.00780756
 0.00722641 0.00686597 0.01062401 0.00779362]
ene_comp = [1.40119877e-05 1.27391160e-04 1.34763784e-05 5.39688647e-07
 1.97672399e-04 2.17684492e-05 1.06671775e-06 3.80992464e-05
 1.80846414e-05 1.14836185e-05]
ene_total = [0.00784724 0.00754436 0.00800881 0.00726358 0.00775607 0.00782933
 0.00722748 0.00690407 0.01064209 0.0078051 ]
At round 22 energy consumption: 0.0788281303900666
At round 22 eta: 0.6868624763988167
At round 22 a_n: 20.646594222929394
At round 22 local rounds: 12.299746037466248
At round 22 global rounds: 65.93458997020494
gradient difference: 0.38995975255966187
train() client id: f_00000-0-0 loss: 1.278953  [   32/  126]
train() client id: f_00000-0-1 loss: 1.362619  [   64/  126]
train() client id: f_00000-0-2 loss: 1.253358  [   96/  126]
train() client id: f_00000-1-0 loss: 1.013112  [   32/  126]
train() client id: f_00000-1-1 loss: 1.137427  [   64/  126]
train() client id: f_00000-1-2 loss: 1.230941  [   96/  126]
train() client id: f_00000-2-0 loss: 1.149918  [   32/  126]
train() client id: f_00000-2-1 loss: 1.007592  [   64/  126]
train() client id: f_00000-2-2 loss: 1.075376  [   96/  126]
train() client id: f_00000-3-0 loss: 1.050440  [   32/  126]
train() client id: f_00000-3-1 loss: 1.091179  [   64/  126]
train() client id: f_00000-3-2 loss: 0.886079  [   96/  126]
train() client id: f_00000-4-0 loss: 1.017815  [   32/  126]
train() client id: f_00000-4-1 loss: 0.859250  [   64/  126]
train() client id: f_00000-4-2 loss: 0.979378  [   96/  126]
train() client id: f_00000-5-0 loss: 0.954761  [   32/  126]
train() client id: f_00000-5-1 loss: 0.973538  [   64/  126]
train() client id: f_00000-5-2 loss: 0.909265  [   96/  126]
train() client id: f_00000-6-0 loss: 0.943909  [   32/  126]
train() client id: f_00000-6-1 loss: 0.827034  [   64/  126]
train() client id: f_00000-6-2 loss: 0.878152  [   96/  126]
train() client id: f_00000-7-0 loss: 0.892374  [   32/  126]
train() client id: f_00000-7-1 loss: 0.838384  [   64/  126]
train() client id: f_00000-7-2 loss: 0.820747  [   96/  126]
train() client id: f_00000-8-0 loss: 0.863451  [   32/  126]
train() client id: f_00000-8-1 loss: 0.870716  [   64/  126]
train() client id: f_00000-8-2 loss: 0.878060  [   96/  126]
train() client id: f_00000-9-0 loss: 0.900025  [   32/  126]
train() client id: f_00000-9-1 loss: 0.891469  [   64/  126]
train() client id: f_00000-9-2 loss: 0.807142  [   96/  126]
train() client id: f_00000-10-0 loss: 0.747484  [   32/  126]
train() client id: f_00000-10-1 loss: 0.882001  [   64/  126]
train() client id: f_00000-10-2 loss: 0.894285  [   96/  126]
train() client id: f_00000-11-0 loss: 0.872814  [   32/  126]
train() client id: f_00000-11-1 loss: 0.850304  [   64/  126]
train() client id: f_00000-11-2 loss: 0.870677  [   96/  126]
train() client id: f_00001-0-0 loss: 0.554406  [   32/  265]
train() client id: f_00001-0-1 loss: 0.444884  [   64/  265]
train() client id: f_00001-0-2 loss: 0.430575  [   96/  265]
train() client id: f_00001-0-3 loss: 0.454068  [  128/  265]
train() client id: f_00001-0-4 loss: 0.636583  [  160/  265]
train() client id: f_00001-0-5 loss: 0.528486  [  192/  265]
train() client id: f_00001-0-6 loss: 0.586643  [  224/  265]
train() client id: f_00001-0-7 loss: 0.435129  [  256/  265]
train() client id: f_00001-1-0 loss: 0.474418  [   32/  265]
train() client id: f_00001-1-1 loss: 0.411175  [   64/  265]
train() client id: f_00001-1-2 loss: 0.543838  [   96/  265]
train() client id: f_00001-1-3 loss: 0.535114  [  128/  265]
train() client id: f_00001-1-4 loss: 0.463547  [  160/  265]
train() client id: f_00001-1-5 loss: 0.537139  [  192/  265]
train() client id: f_00001-1-6 loss: 0.557809  [  224/  265]
train() client id: f_00001-1-7 loss: 0.406479  [  256/  265]
train() client id: f_00001-2-0 loss: 0.459451  [   32/  265]
train() client id: f_00001-2-1 loss: 0.466816  [   64/  265]
train() client id: f_00001-2-2 loss: 0.482898  [   96/  265]
train() client id: f_00001-2-3 loss: 0.746107  [  128/  265]
train() client id: f_00001-2-4 loss: 0.482238  [  160/  265]
train() client id: f_00001-2-5 loss: 0.379343  [  192/  265]
train() client id: f_00001-2-6 loss: 0.398377  [  224/  265]
train() client id: f_00001-2-7 loss: 0.462857  [  256/  265]
train() client id: f_00001-3-0 loss: 0.495137  [   32/  265]
train() client id: f_00001-3-1 loss: 0.395523  [   64/  265]
train() client id: f_00001-3-2 loss: 0.464764  [   96/  265]
train() client id: f_00001-3-3 loss: 0.466311  [  128/  265]
train() client id: f_00001-3-4 loss: 0.462734  [  160/  265]
train() client id: f_00001-3-5 loss: 0.437153  [  192/  265]
train() client id: f_00001-3-6 loss: 0.528093  [  224/  265]
train() client id: f_00001-3-7 loss: 0.462301  [  256/  265]
train() client id: f_00001-4-0 loss: 0.396806  [   32/  265]
train() client id: f_00001-4-1 loss: 0.548916  [   64/  265]
train() client id: f_00001-4-2 loss: 0.411332  [   96/  265]
train() client id: f_00001-4-3 loss: 0.489237  [  128/  265]
train() client id: f_00001-4-4 loss: 0.382155  [  160/  265]
train() client id: f_00001-4-5 loss: 0.621699  [  192/  265]
train() client id: f_00001-4-6 loss: 0.513309  [  224/  265]
train() client id: f_00001-4-7 loss: 0.495564  [  256/  265]
train() client id: f_00001-5-0 loss: 0.535726  [   32/  265]
train() client id: f_00001-5-1 loss: 0.440107  [   64/  265]
train() client id: f_00001-5-2 loss: 0.506458  [   96/  265]
train() client id: f_00001-5-3 loss: 0.589517  [  128/  265]
train() client id: f_00001-5-4 loss: 0.395066  [  160/  265]
train() client id: f_00001-5-5 loss: 0.454345  [  192/  265]
train() client id: f_00001-5-6 loss: 0.452835  [  224/  265]
train() client id: f_00001-5-7 loss: 0.466370  [  256/  265]
train() client id: f_00001-6-0 loss: 0.466489  [   32/  265]
train() client id: f_00001-6-1 loss: 0.468563  [   64/  265]
train() client id: f_00001-6-2 loss: 0.503646  [   96/  265]
train() client id: f_00001-6-3 loss: 0.550062  [  128/  265]
train() client id: f_00001-6-4 loss: 0.424902  [  160/  265]
train() client id: f_00001-6-5 loss: 0.506910  [  192/  265]
train() client id: f_00001-6-6 loss: 0.398743  [  224/  265]
train() client id: f_00001-6-7 loss: 0.504754  [  256/  265]
train() client id: f_00001-7-0 loss: 0.542531  [   32/  265]
train() client id: f_00001-7-1 loss: 0.474034  [   64/  265]
train() client id: f_00001-7-2 loss: 0.432296  [   96/  265]
train() client id: f_00001-7-3 loss: 0.393926  [  128/  265]
train() client id: f_00001-7-4 loss: 0.535252  [  160/  265]
train() client id: f_00001-7-5 loss: 0.532044  [  192/  265]
train() client id: f_00001-7-6 loss: 0.437599  [  224/  265]
train() client id: f_00001-7-7 loss: 0.466711  [  256/  265]
train() client id: f_00001-8-0 loss: 0.480000  [   32/  265]
train() client id: f_00001-8-1 loss: 0.499327  [   64/  265]
train() client id: f_00001-8-2 loss: 0.462103  [   96/  265]
train() client id: f_00001-8-3 loss: 0.452713  [  128/  265]
train() client id: f_00001-8-4 loss: 0.442278  [  160/  265]
train() client id: f_00001-8-5 loss: 0.671023  [  192/  265]
train() client id: f_00001-8-6 loss: 0.383428  [  224/  265]
train() client id: f_00001-8-7 loss: 0.421439  [  256/  265]
train() client id: f_00001-9-0 loss: 0.379303  [   32/  265]
train() client id: f_00001-9-1 loss: 0.469615  [   64/  265]
train() client id: f_00001-9-2 loss: 0.476261  [   96/  265]
train() client id: f_00001-9-3 loss: 0.385866  [  128/  265]
train() client id: f_00001-9-4 loss: 0.461895  [  160/  265]
train() client id: f_00001-9-5 loss: 0.731371  [  192/  265]
train() client id: f_00001-9-6 loss: 0.452536  [  224/  265]
train() client id: f_00001-9-7 loss: 0.455672  [  256/  265]
train() client id: f_00001-10-0 loss: 0.448314  [   32/  265]
train() client id: f_00001-10-1 loss: 0.579247  [   64/  265]
train() client id: f_00001-10-2 loss: 0.438085  [   96/  265]
train() client id: f_00001-10-3 loss: 0.390289  [  128/  265]
train() client id: f_00001-10-4 loss: 0.578184  [  160/  265]
train() client id: f_00001-10-5 loss: 0.387646  [  192/  265]
train() client id: f_00001-10-6 loss: 0.496806  [  224/  265]
train() client id: f_00001-10-7 loss: 0.498651  [  256/  265]
train() client id: f_00001-11-0 loss: 0.458710  [   32/  265]
train() client id: f_00001-11-1 loss: 0.445669  [   64/  265]
train() client id: f_00001-11-2 loss: 0.510521  [   96/  265]
train() client id: f_00001-11-3 loss: 0.461612  [  128/  265]
train() client id: f_00001-11-4 loss: 0.452208  [  160/  265]
train() client id: f_00001-11-5 loss: 0.520542  [  192/  265]
train() client id: f_00001-11-6 loss: 0.542508  [  224/  265]
train() client id: f_00001-11-7 loss: 0.411988  [  256/  265]
train() client id: f_00002-0-0 loss: 1.268398  [   32/  124]
train() client id: f_00002-0-1 loss: 1.105535  [   64/  124]
train() client id: f_00002-0-2 loss: 1.208951  [   96/  124]
train() client id: f_00002-1-0 loss: 1.144377  [   32/  124]
train() client id: f_00002-1-1 loss: 1.143600  [   64/  124]
train() client id: f_00002-1-2 loss: 1.150043  [   96/  124]
train() client id: f_00002-2-0 loss: 1.118021  [   32/  124]
train() client id: f_00002-2-1 loss: 1.185056  [   64/  124]
train() client id: f_00002-2-2 loss: 1.220094  [   96/  124]
train() client id: f_00002-3-0 loss: 1.132667  [   32/  124]
train() client id: f_00002-3-1 loss: 1.041312  [   64/  124]
train() client id: f_00002-3-2 loss: 1.138217  [   96/  124]
train() client id: f_00002-4-0 loss: 1.196059  [   32/  124]
train() client id: f_00002-4-1 loss: 0.914728  [   64/  124]
train() client id: f_00002-4-2 loss: 1.160068  [   96/  124]
train() client id: f_00002-5-0 loss: 1.017083  [   32/  124]
train() client id: f_00002-5-1 loss: 1.049782  [   64/  124]
train() client id: f_00002-5-2 loss: 0.997520  [   96/  124]
train() client id: f_00002-6-0 loss: 0.964561  [   32/  124]
train() client id: f_00002-6-1 loss: 1.117363  [   64/  124]
train() client id: f_00002-6-2 loss: 0.998386  [   96/  124]
train() client id: f_00002-7-0 loss: 1.039753  [   32/  124]
train() client id: f_00002-7-1 loss: 1.065705  [   64/  124]
train() client id: f_00002-7-2 loss: 0.952998  [   96/  124]
train() client id: f_00002-8-0 loss: 0.924118  [   32/  124]
train() client id: f_00002-8-1 loss: 1.104727  [   64/  124]
train() client id: f_00002-8-2 loss: 1.033604  [   96/  124]
train() client id: f_00002-9-0 loss: 1.069701  [   32/  124]
train() client id: f_00002-9-1 loss: 1.011346  [   64/  124]
train() client id: f_00002-9-2 loss: 0.995880  [   96/  124]
train() client id: f_00002-10-0 loss: 1.130184  [   32/  124]
train() client id: f_00002-10-1 loss: 1.054596  [   64/  124]
train() client id: f_00002-10-2 loss: 0.964715  [   96/  124]
train() client id: f_00002-11-0 loss: 1.093289  [   32/  124]
train() client id: f_00002-11-1 loss: 0.975490  [   64/  124]
train() client id: f_00002-11-2 loss: 0.955269  [   96/  124]
train() client id: f_00003-0-0 loss: 0.850270  [   32/   43]
train() client id: f_00003-1-0 loss: 0.855891  [   32/   43]
train() client id: f_00003-2-0 loss: 0.887146  [   32/   43]
train() client id: f_00003-3-0 loss: 0.798501  [   32/   43]
train() client id: f_00003-4-0 loss: 0.692235  [   32/   43]
train() client id: f_00003-5-0 loss: 0.758510  [   32/   43]
train() client id: f_00003-6-0 loss: 0.782817  [   32/   43]
train() client id: f_00003-7-0 loss: 0.776277  [   32/   43]
train() client id: f_00003-8-0 loss: 0.807965  [   32/   43]
train() client id: f_00003-9-0 loss: 0.874757  [   32/   43]
train() client id: f_00003-10-0 loss: 0.722761  [   32/   43]
train() client id: f_00003-11-0 loss: 0.830046  [   32/   43]
train() client id: f_00004-0-0 loss: 0.848337  [   32/  306]
train() client id: f_00004-0-1 loss: 0.939430  [   64/  306]
train() client id: f_00004-0-2 loss: 0.956264  [   96/  306]
train() client id: f_00004-0-3 loss: 0.932467  [  128/  306]
train() client id: f_00004-0-4 loss: 0.955950  [  160/  306]
train() client id: f_00004-0-5 loss: 1.017983  [  192/  306]
train() client id: f_00004-0-6 loss: 0.864617  [  224/  306]
train() client id: f_00004-0-7 loss: 0.883336  [  256/  306]
train() client id: f_00004-0-8 loss: 0.796783  [  288/  306]
train() client id: f_00004-1-0 loss: 0.998236  [   32/  306]
train() client id: f_00004-1-1 loss: 0.931162  [   64/  306]
train() client id: f_00004-1-2 loss: 0.847500  [   96/  306]
train() client id: f_00004-1-3 loss: 0.970318  [  128/  306]
train() client id: f_00004-1-4 loss: 0.900089  [  160/  306]
train() client id: f_00004-1-5 loss: 0.883612  [  192/  306]
train() client id: f_00004-1-6 loss: 0.935536  [  224/  306]
train() client id: f_00004-1-7 loss: 0.832078  [  256/  306]
train() client id: f_00004-1-8 loss: 0.885099  [  288/  306]
train() client id: f_00004-2-0 loss: 0.893534  [   32/  306]
train() client id: f_00004-2-1 loss: 0.760505  [   64/  306]
train() client id: f_00004-2-2 loss: 0.890787  [   96/  306]
train() client id: f_00004-2-3 loss: 0.994101  [  128/  306]
train() client id: f_00004-2-4 loss: 0.901637  [  160/  306]
train() client id: f_00004-2-5 loss: 0.927397  [  192/  306]
train() client id: f_00004-2-6 loss: 0.885709  [  224/  306]
train() client id: f_00004-2-7 loss: 0.964946  [  256/  306]
train() client id: f_00004-2-8 loss: 0.916595  [  288/  306]
train() client id: f_00004-3-0 loss: 0.983856  [   32/  306]
train() client id: f_00004-3-1 loss: 0.782576  [   64/  306]
train() client id: f_00004-3-2 loss: 0.815288  [   96/  306]
train() client id: f_00004-3-3 loss: 1.017961  [  128/  306]
train() client id: f_00004-3-4 loss: 0.828977  [  160/  306]
train() client id: f_00004-3-5 loss: 0.942316  [  192/  306]
train() client id: f_00004-3-6 loss: 0.865753  [  224/  306]
train() client id: f_00004-3-7 loss: 0.961395  [  256/  306]
train() client id: f_00004-3-8 loss: 0.900435  [  288/  306]
train() client id: f_00004-4-0 loss: 0.823042  [   32/  306]
train() client id: f_00004-4-1 loss: 0.947153  [   64/  306]
train() client id: f_00004-4-2 loss: 0.936567  [   96/  306]
train() client id: f_00004-4-3 loss: 0.988107  [  128/  306]
train() client id: f_00004-4-4 loss: 0.897077  [  160/  306]
train() client id: f_00004-4-5 loss: 0.868682  [  192/  306]
train() client id: f_00004-4-6 loss: 0.899839  [  224/  306]
train() client id: f_00004-4-7 loss: 0.820362  [  256/  306]
train() client id: f_00004-4-8 loss: 0.864179  [  288/  306]
train() client id: f_00004-5-0 loss: 0.879967  [   32/  306]
train() client id: f_00004-5-1 loss: 0.951190  [   64/  306]
train() client id: f_00004-5-2 loss: 0.919369  [   96/  306]
train() client id: f_00004-5-3 loss: 0.946036  [  128/  306]
train() client id: f_00004-5-4 loss: 0.859231  [  160/  306]
train() client id: f_00004-5-5 loss: 0.779704  [  192/  306]
train() client id: f_00004-5-6 loss: 0.941065  [  224/  306]
train() client id: f_00004-5-7 loss: 0.924915  [  256/  306]
train() client id: f_00004-5-8 loss: 0.880419  [  288/  306]
train() client id: f_00004-6-0 loss: 0.903429  [   32/  306]
train() client id: f_00004-6-1 loss: 0.889203  [   64/  306]
train() client id: f_00004-6-2 loss: 0.936120  [   96/  306]
train() client id: f_00004-6-3 loss: 0.956747  [  128/  306]
train() client id: f_00004-6-4 loss: 0.881728  [  160/  306]
train() client id: f_00004-6-5 loss: 0.883381  [  192/  306]
train() client id: f_00004-6-6 loss: 0.829170  [  224/  306]
train() client id: f_00004-6-7 loss: 0.843405  [  256/  306]
train() client id: f_00004-6-8 loss: 0.890222  [  288/  306]
train() client id: f_00004-7-0 loss: 0.907485  [   32/  306]
train() client id: f_00004-7-1 loss: 0.789571  [   64/  306]
train() client id: f_00004-7-2 loss: 0.931902  [   96/  306]
train() client id: f_00004-7-3 loss: 0.986586  [  128/  306]
train() client id: f_00004-7-4 loss: 0.881238  [  160/  306]
train() client id: f_00004-7-5 loss: 0.851348  [  192/  306]
train() client id: f_00004-7-6 loss: 0.878148  [  224/  306]
train() client id: f_00004-7-7 loss: 0.868054  [  256/  306]
train() client id: f_00004-7-8 loss: 0.911884  [  288/  306]
train() client id: f_00004-8-0 loss: 0.893902  [   32/  306]
train() client id: f_00004-8-1 loss: 0.829980  [   64/  306]
train() client id: f_00004-8-2 loss: 0.870577  [   96/  306]
train() client id: f_00004-8-3 loss: 0.946982  [  128/  306]
train() client id: f_00004-8-4 loss: 1.040037  [  160/  306]
train() client id: f_00004-8-5 loss: 0.837075  [  192/  306]
train() client id: f_00004-8-6 loss: 0.929639  [  224/  306]
train() client id: f_00004-8-7 loss: 0.827618  [  256/  306]
train() client id: f_00004-8-8 loss: 0.869564  [  288/  306]
train() client id: f_00004-9-0 loss: 0.979493  [   32/  306]
train() client id: f_00004-9-1 loss: 0.890288  [   64/  306]
train() client id: f_00004-9-2 loss: 0.874121  [   96/  306]
train() client id: f_00004-9-3 loss: 0.794606  [  128/  306]
train() client id: f_00004-9-4 loss: 0.890768  [  160/  306]
train() client id: f_00004-9-5 loss: 0.756359  [  192/  306]
train() client id: f_00004-9-6 loss: 0.924402  [  224/  306]
train() client id: f_00004-9-7 loss: 0.867159  [  256/  306]
train() client id: f_00004-9-8 loss: 0.888985  [  288/  306]
train() client id: f_00004-10-0 loss: 0.870316  [   32/  306]
train() client id: f_00004-10-1 loss: 0.968360  [   64/  306]
train() client id: f_00004-10-2 loss: 0.894543  [   96/  306]
train() client id: f_00004-10-3 loss: 0.881820  [  128/  306]
train() client id: f_00004-10-4 loss: 0.805092  [  160/  306]
train() client id: f_00004-10-5 loss: 0.962779  [  192/  306]
train() client id: f_00004-10-6 loss: 0.874597  [  224/  306]
train() client id: f_00004-10-7 loss: 0.780662  [  256/  306]
train() client id: f_00004-10-8 loss: 0.924102  [  288/  306]
train() client id: f_00004-11-0 loss: 0.906138  [   32/  306]
train() client id: f_00004-11-1 loss: 0.819784  [   64/  306]
train() client id: f_00004-11-2 loss: 0.860607  [   96/  306]
train() client id: f_00004-11-3 loss: 0.745318  [  128/  306]
train() client id: f_00004-11-4 loss: 0.904493  [  160/  306]
train() client id: f_00004-11-5 loss: 0.867671  [  192/  306]
train() client id: f_00004-11-6 loss: 0.899514  [  224/  306]
train() client id: f_00004-11-7 loss: 0.965608  [  256/  306]
train() client id: f_00004-11-8 loss: 0.881841  [  288/  306]
train() client id: f_00005-0-0 loss: 0.361576  [   32/  146]
train() client id: f_00005-0-1 loss: 0.751212  [   64/  146]
train() client id: f_00005-0-2 loss: 0.365570  [   96/  146]
train() client id: f_00005-0-3 loss: 0.374131  [  128/  146]
train() client id: f_00005-1-0 loss: 0.498089  [   32/  146]
train() client id: f_00005-1-1 loss: 0.373316  [   64/  146]
train() client id: f_00005-1-2 loss: 0.604117  [   96/  146]
train() client id: f_00005-1-3 loss: 0.506468  [  128/  146]
train() client id: f_00005-2-0 loss: 0.665986  [   32/  146]
train() client id: f_00005-2-1 loss: 0.432809  [   64/  146]
train() client id: f_00005-2-2 loss: 0.573600  [   96/  146]
train() client id: f_00005-2-3 loss: 0.321452  [  128/  146]
train() client id: f_00005-3-0 loss: 0.509089  [   32/  146]
train() client id: f_00005-3-1 loss: 0.449874  [   64/  146]
train() client id: f_00005-3-2 loss: 0.502695  [   96/  146]
train() client id: f_00005-3-3 loss: 0.448403  [  128/  146]
train() client id: f_00005-4-0 loss: 0.566800  [   32/  146]
train() client id: f_00005-4-1 loss: 0.602852  [   64/  146]
train() client id: f_00005-4-2 loss: 0.383588  [   96/  146]
train() client id: f_00005-4-3 loss: 0.467765  [  128/  146]
train() client id: f_00005-5-0 loss: 0.497005  [   32/  146]
train() client id: f_00005-5-1 loss: 0.543997  [   64/  146]
train() client id: f_00005-5-2 loss: 0.577219  [   96/  146]
train() client id: f_00005-5-3 loss: 0.295493  [  128/  146]
train() client id: f_00005-6-0 loss: 0.620909  [   32/  146]
train() client id: f_00005-6-1 loss: 0.509325  [   64/  146]
train() client id: f_00005-6-2 loss: 0.448670  [   96/  146]
train() client id: f_00005-6-3 loss: 0.486986  [  128/  146]
train() client id: f_00005-7-0 loss: 0.565018  [   32/  146]
train() client id: f_00005-7-1 loss: 0.680371  [   64/  146]
train() client id: f_00005-7-2 loss: 0.369418  [   96/  146]
train() client id: f_00005-7-3 loss: 0.318300  [  128/  146]
train() client id: f_00005-8-0 loss: 0.627820  [   32/  146]
train() client id: f_00005-8-1 loss: 0.634254  [   64/  146]
train() client id: f_00005-8-2 loss: 0.302905  [   96/  146]
train() client id: f_00005-8-3 loss: 0.455432  [  128/  146]
train() client id: f_00005-9-0 loss: 0.437848  [   32/  146]
train() client id: f_00005-9-1 loss: 0.327630  [   64/  146]
train() client id: f_00005-9-2 loss: 0.630384  [   96/  146]
train() client id: f_00005-9-3 loss: 0.491534  [  128/  146]
train() client id: f_00005-10-0 loss: 0.346046  [   32/  146]
train() client id: f_00005-10-1 loss: 0.379804  [   64/  146]
train() client id: f_00005-10-2 loss: 0.463960  [   96/  146]
train() client id: f_00005-10-3 loss: 0.735427  [  128/  146]
train() client id: f_00005-11-0 loss: 0.739033  [   32/  146]
train() client id: f_00005-11-1 loss: 0.256683  [   64/  146]
train() client id: f_00005-11-2 loss: 0.627374  [   96/  146]
train() client id: f_00005-11-3 loss: 0.403908  [  128/  146]
train() client id: f_00006-0-0 loss: 0.609501  [   32/   54]
train() client id: f_00006-1-0 loss: 0.563606  [   32/   54]
train() client id: f_00006-2-0 loss: 0.571807  [   32/   54]
train() client id: f_00006-3-0 loss: 0.578596  [   32/   54]
train() client id: f_00006-4-0 loss: 0.565545  [   32/   54]
train() client id: f_00006-5-0 loss: 0.559895  [   32/   54]
train() client id: f_00006-6-0 loss: 0.560811  [   32/   54]
train() client id: f_00006-7-0 loss: 0.560115  [   32/   54]
train() client id: f_00006-8-0 loss: 0.526740  [   32/   54]
train() client id: f_00006-9-0 loss: 0.599325  [   32/   54]
train() client id: f_00006-10-0 loss: 0.531881  [   32/   54]
train() client id: f_00006-11-0 loss: 0.564423  [   32/   54]
train() client id: f_00007-0-0 loss: 0.513024  [   32/  179]
train() client id: f_00007-0-1 loss: 0.510368  [   64/  179]
train() client id: f_00007-0-2 loss: 0.623464  [   96/  179]
train() client id: f_00007-0-3 loss: 0.737596  [  128/  179]
train() client id: f_00007-0-4 loss: 0.768384  [  160/  179]
train() client id: f_00007-1-0 loss: 0.774226  [   32/  179]
train() client id: f_00007-1-1 loss: 0.590001  [   64/  179]
train() client id: f_00007-1-2 loss: 0.652839  [   96/  179]
train() client id: f_00007-1-3 loss: 0.608013  [  128/  179]
train() client id: f_00007-1-4 loss: 0.556794  [  160/  179]
train() client id: f_00007-2-0 loss: 0.654938  [   32/  179]
train() client id: f_00007-2-1 loss: 0.683949  [   64/  179]
train() client id: f_00007-2-2 loss: 0.733810  [   96/  179]
train() client id: f_00007-2-3 loss: 0.516678  [  128/  179]
train() client id: f_00007-2-4 loss: 0.504643  [  160/  179]
train() client id: f_00007-3-0 loss: 0.566526  [   32/  179]
train() client id: f_00007-3-1 loss: 0.720551  [   64/  179]
train() client id: f_00007-3-2 loss: 0.467989  [   96/  179]
train() client id: f_00007-3-3 loss: 0.635749  [  128/  179]
train() client id: f_00007-3-4 loss: 0.507762  [  160/  179]
train() client id: f_00007-4-0 loss: 0.620853  [   32/  179]
train() client id: f_00007-4-1 loss: 0.576341  [   64/  179]
train() client id: f_00007-4-2 loss: 0.576363  [   96/  179]
train() client id: f_00007-4-3 loss: 0.590747  [  128/  179]
train() client id: f_00007-4-4 loss: 0.680036  [  160/  179]
train() client id: f_00007-5-0 loss: 0.469743  [   32/  179]
train() client id: f_00007-5-1 loss: 0.562385  [   64/  179]
train() client id: f_00007-5-2 loss: 0.615210  [   96/  179]
train() client id: f_00007-5-3 loss: 0.630125  [  128/  179]
train() client id: f_00007-5-4 loss: 0.813024  [  160/  179]
train() client id: f_00007-6-0 loss: 0.563168  [   32/  179]
train() client id: f_00007-6-1 loss: 0.541308  [   64/  179]
train() client id: f_00007-6-2 loss: 0.635166  [   96/  179]
train() client id: f_00007-6-3 loss: 0.564200  [  128/  179]
train() client id: f_00007-6-4 loss: 0.692479  [  160/  179]
train() client id: f_00007-7-0 loss: 0.558317  [   32/  179]
train() client id: f_00007-7-1 loss: 0.582691  [   64/  179]
train() client id: f_00007-7-2 loss: 0.467763  [   96/  179]
train() client id: f_00007-7-3 loss: 0.652272  [  128/  179]
train() client id: f_00007-7-4 loss: 0.813462  [  160/  179]
train() client id: f_00007-8-0 loss: 0.471516  [   32/  179]
train() client id: f_00007-8-1 loss: 0.627419  [   64/  179]
train() client id: f_00007-8-2 loss: 0.659369  [   96/  179]
train() client id: f_00007-8-3 loss: 0.491243  [  128/  179]
train() client id: f_00007-8-4 loss: 0.562573  [  160/  179]
train() client id: f_00007-9-0 loss: 0.558015  [   32/  179]
train() client id: f_00007-9-1 loss: 0.547815  [   64/  179]
train() client id: f_00007-9-2 loss: 0.553823  [   96/  179]
train() client id: f_00007-9-3 loss: 0.534780  [  128/  179]
train() client id: f_00007-9-4 loss: 0.878106  [  160/  179]
train() client id: f_00007-10-0 loss: 0.683720  [   32/  179]
train() client id: f_00007-10-1 loss: 0.748490  [   64/  179]
train() client id: f_00007-10-2 loss: 0.466756  [   96/  179]
train() client id: f_00007-10-3 loss: 0.519472  [  128/  179]
train() client id: f_00007-10-4 loss: 0.657263  [  160/  179]
train() client id: f_00007-11-0 loss: 0.684824  [   32/  179]
train() client id: f_00007-11-1 loss: 0.540602  [   64/  179]
train() client id: f_00007-11-2 loss: 0.682905  [   96/  179]
train() client id: f_00007-11-3 loss: 0.591774  [  128/  179]
train() client id: f_00007-11-4 loss: 0.572053  [  160/  179]
train() client id: f_00008-0-0 loss: 0.891389  [   32/  130]
train() client id: f_00008-0-1 loss: 0.751951  [   64/  130]
train() client id: f_00008-0-2 loss: 0.677068  [   96/  130]
train() client id: f_00008-0-3 loss: 0.741676  [  128/  130]
train() client id: f_00008-1-0 loss: 0.780448  [   32/  130]
train() client id: f_00008-1-1 loss: 0.728888  [   64/  130]
train() client id: f_00008-1-2 loss: 0.802709  [   96/  130]
train() client id: f_00008-1-3 loss: 0.720225  [  128/  130]
train() client id: f_00008-2-0 loss: 0.750061  [   32/  130]
train() client id: f_00008-2-1 loss: 0.764849  [   64/  130]
train() client id: f_00008-2-2 loss: 0.804865  [   96/  130]
train() client id: f_00008-2-3 loss: 0.708707  [  128/  130]
train() client id: f_00008-3-0 loss: 0.699734  [   32/  130]
train() client id: f_00008-3-1 loss: 0.732853  [   64/  130]
train() client id: f_00008-3-2 loss: 0.773635  [   96/  130]
train() client id: f_00008-3-3 loss: 0.863133  [  128/  130]
train() client id: f_00008-4-0 loss: 0.809855  [   32/  130]
train() client id: f_00008-4-1 loss: 0.710961  [   64/  130]
train() client id: f_00008-4-2 loss: 0.745925  [   96/  130]
train() client id: f_00008-4-3 loss: 0.789316  [  128/  130]
train() client id: f_00008-5-0 loss: 0.869809  [   32/  130]
train() client id: f_00008-5-1 loss: 0.819500  [   64/  130]
train() client id: f_00008-5-2 loss: 0.715276  [   96/  130]
train() client id: f_00008-5-3 loss: 0.668965  [  128/  130]
train() client id: f_00008-6-0 loss: 0.684700  [   32/  130]
train() client id: f_00008-6-1 loss: 0.825176  [   64/  130]
train() client id: f_00008-6-2 loss: 0.795933  [   96/  130]
train() client id: f_00008-6-3 loss: 0.715851  [  128/  130]
train() client id: f_00008-7-0 loss: 0.811171  [   32/  130]
train() client id: f_00008-7-1 loss: 0.781540  [   64/  130]
train() client id: f_00008-7-2 loss: 0.804787  [   96/  130]
train() client id: f_00008-7-3 loss: 0.673868  [  128/  130]
train() client id: f_00008-8-0 loss: 0.762669  [   32/  130]
train() client id: f_00008-8-1 loss: 0.733054  [   64/  130]
train() client id: f_00008-8-2 loss: 0.744379  [   96/  130]
train() client id: f_00008-8-3 loss: 0.822833  [  128/  130]
train() client id: f_00008-9-0 loss: 0.756503  [   32/  130]
train() client id: f_00008-9-1 loss: 0.803028  [   64/  130]
train() client id: f_00008-9-2 loss: 0.786273  [   96/  130]
train() client id: f_00008-9-3 loss: 0.704983  [  128/  130]
train() client id: f_00008-10-0 loss: 0.826749  [   32/  130]
train() client id: f_00008-10-1 loss: 0.772005  [   64/  130]
train() client id: f_00008-10-2 loss: 0.734702  [   96/  130]
train() client id: f_00008-10-3 loss: 0.743030  [  128/  130]
train() client id: f_00008-11-0 loss: 0.856813  [   32/  130]
train() client id: f_00008-11-1 loss: 0.733761  [   64/  130]
train() client id: f_00008-11-2 loss: 0.815902  [   96/  130]
train() client id: f_00008-11-3 loss: 0.666978  [  128/  130]
train() client id: f_00009-0-0 loss: 1.166743  [   32/  118]
train() client id: f_00009-0-1 loss: 1.043454  [   64/  118]
train() client id: f_00009-0-2 loss: 1.123052  [   96/  118]
train() client id: f_00009-1-0 loss: 1.086489  [   32/  118]
train() client id: f_00009-1-1 loss: 0.974419  [   64/  118]
train() client id: f_00009-1-2 loss: 0.989497  [   96/  118]
train() client id: f_00009-2-0 loss: 0.979804  [   32/  118]
train() client id: f_00009-2-1 loss: 0.907450  [   64/  118]
train() client id: f_00009-2-2 loss: 1.046291  [   96/  118]
train() client id: f_00009-3-0 loss: 0.932195  [   32/  118]
train() client id: f_00009-3-1 loss: 0.890184  [   64/  118]
train() client id: f_00009-3-2 loss: 0.878074  [   96/  118]
train() client id: f_00009-4-0 loss: 0.894858  [   32/  118]
train() client id: f_00009-4-1 loss: 0.903630  [   64/  118]
train() client id: f_00009-4-2 loss: 1.036522  [   96/  118]
train() client id: f_00009-5-0 loss: 0.840191  [   32/  118]
train() client id: f_00009-5-1 loss: 0.890713  [   64/  118]
train() client id: f_00009-5-2 loss: 0.891543  [   96/  118]
train() client id: f_00009-6-0 loss: 0.783714  [   32/  118]
train() client id: f_00009-6-1 loss: 0.884474  [   64/  118]
train() client id: f_00009-6-2 loss: 0.895842  [   96/  118]
train() client id: f_00009-7-0 loss: 0.877894  [   32/  118]
train() client id: f_00009-7-1 loss: 0.804338  [   64/  118]
train() client id: f_00009-7-2 loss: 0.817673  [   96/  118]
train() client id: f_00009-8-0 loss: 0.665786  [   32/  118]
train() client id: f_00009-8-1 loss: 0.871927  [   64/  118]
train() client id: f_00009-8-2 loss: 0.795741  [   96/  118]
train() client id: f_00009-9-0 loss: 0.768453  [   32/  118]
train() client id: f_00009-9-1 loss: 0.811219  [   64/  118]
train() client id: f_00009-9-2 loss: 0.761741  [   96/  118]
train() client id: f_00009-10-0 loss: 0.711223  [   32/  118]
train() client id: f_00009-10-1 loss: 0.862384  [   64/  118]
train() client id: f_00009-10-2 loss: 0.798216  [   96/  118]
train() client id: f_00009-11-0 loss: 0.719846  [   32/  118]
train() client id: f_00009-11-1 loss: 0.818733  [   64/  118]
train() client id: f_00009-11-2 loss: 0.752909  [   96/  118]
At round 22 accuracy: 0.6419098143236074
At round 22 training accuracy: 0.5835010060362174
At round 22 training loss: 0.8350821793041063
update_location
xs = -3.905658 4.200318 130.009024 18.811294 0.979296 3.956410 -92.443192 -71.324852 114.663977 -57.060879 
ys = 122.587959 105.555839 1.320614 -92.455176 84.350187 67.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 158.249998 145.463665 164.024664 137.483906 130.827799 120.890083 136.208055 122.832857 153.155142 115.203974 
dists_bs = 178.942157 192.185446 350.991501 330.295457 197.954796 208.600582 195.873075 202.697782 329.681266 207.726093 
uav_gains = -104.992754 -104.071889 -105.387228 -103.457709 -102.918216 -102.060004 -103.356314 -102.233161 -104.634275 -101.536807 
bs_gains = -102.643014 -103.511233 -110.835364 -110.096332 -103.870908 -104.507894 -103.742352 -104.158831 -110.073699 -104.456809 
Round 23
-------------------------------
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.95652636 16.54261616  7.83621819  2.81059366 19.08131306  9.18961195
  3.4902559  11.21434588  8.261358    7.45728421]
obj_prev = 93.84012336038582
eta_min = 2.1552750513700054e-12	eta_max = 0.9232017387246043
af = 19.826056636829264	bf = 1.599357197144509	zeta = 21.808662300512193	eta = 0.909090909090909
af = 19.826056636829264	bf = 1.599357197144509	zeta = 38.37880442012573	eta = 0.5165886987983539
af = 19.826056636829264	bf = 1.599357197144509	zeta = 30.397782640654498	eta = 0.6522204882902731
af = 19.826056636829264	bf = 1.599357197144509	zeta = 28.962231155820284	eta = 0.6845486637463356
af = 19.826056636829264	bf = 1.599357197144509	zeta = 28.889857955433257	eta = 0.686263555446129
af = 19.826056636829264	bf = 1.599357197144509	zeta = 28.889660323483184	eta = 0.6862682501224668
eta = 0.6862682501224668
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [0.03106678 0.06533886 0.03057366 0.01060215 0.07544789 0.03599801
 0.01331433 0.04413455 0.03205303 0.02909429]
ene_total = [2.52460799 4.70588087 2.50342372 1.16220666 5.36855605 2.83112977
 1.33541705 3.30578544 2.76961187 2.3830409 ]
ti_comp = [0.36635294 0.37203036 0.36469503 0.37222065 0.37072619 0.36830251
 0.37257852 0.37632559 0.33835818 0.3685025 ]
ti_coms = [0.07942667 0.07374925 0.08108457 0.07355896 0.07505341 0.0774771
 0.07320109 0.06945402 0.10742142 0.07727711]
t_total = [28.84990349 28.84990349 28.84990349 28.84990349 28.84990349 28.84990349
 28.84990349 28.84990349 28.84990349 28.84990349]
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [1.39626984e-05 1.25961536e-04 1.34295805e-05 5.37602806e-07
 1.95305657e-04 2.14934581e-05 1.06268058e-06 3.79392448e-05
 1.79776854e-05 1.13350184e-05]
ene_total = [0.51493605 0.48544134 0.52563117 0.47609264 0.49836942 0.50280622
 0.47381058 0.45194692 0.69637198 0.5008545 ]
optimize_network iter = 0 obj = 5.126260825771548
eta = 0.6862682501224668
freqs = [4.24000684e+07 8.78138843e+07 4.19167432e+07 1.42417608e+07
 1.01756896e+08 4.88701724e+07 1.78678231e+07 5.86387868e+07
 4.73655258e+07 3.94763747e+07]
eta_min = 0.686268250122481	eta_max = 0.6862682501224266
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 0.026043258509324534	eta = 0.9090909090909091
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 17.617751045101738	eta = 0.0013438542463973541
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 1.7582687679572628	eta = 0.01346534158224146
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 1.7245653152193903	eta = 0.013728496882659091
af = 0.023675689553931394	bf = 1.599357197144509	zeta = 1.7245608478782077	eta = 0.013728532445265985
eta = 0.013728532445265985
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [1.59038678e-04 1.43473385e-03 1.52966329e-04 6.12343236e-06
 2.22458099e-03 2.44815942e-04 1.21042015e-05 4.32137624e-04
 2.04770398e-04 1.29108736e-04]
ene_total = [0.16678716 0.18136157 0.17007523 0.15155948 0.20030673 0.16453951
 0.15094588 0.151879   0.22536052 0.16174577]
ti_comp = [0.36635294 0.37203036 0.36469503 0.37222065 0.37072619 0.36830251
 0.37257852 0.37632559 0.33835818 0.3685025 ]
ti_coms = [0.07942667 0.07374925 0.08108457 0.07355896 0.07505341 0.0774771
 0.07320109 0.06945402 0.10742142 0.07727711]
t_total = [28.84990349 28.84990349 28.84990349 28.84990349 28.84990349 28.84990349
 28.84990349 28.84990349 28.84990349 28.84990349]
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [1.39626984e-05 1.25961536e-04 1.34295805e-05 5.37602806e-07
 1.95305657e-04 2.14934581e-05 1.06268058e-06 3.79392448e-05
 1.79776854e-05 1.13350184e-05]
ene_total = [0.51493605 0.48544134 0.52563117 0.47609264 0.49836942 0.50280622
 0.47381058 0.45194692 0.69637198 0.5008545 ]
optimize_network iter = 1 obj = 5.126260825771778
eta = 0.686268250122481
freqs = [4.24000684e+07 8.78138843e+07 4.19167432e+07 1.42417608e+07
 1.01756896e+08 4.88701724e+07 1.78678231e+07 5.86387868e+07
 4.73655258e+07 3.94763747e+07]
Done!
ene_coms = [0.00794267 0.00737492 0.00810846 0.0073559  0.00750534 0.00774771
 0.00732011 0.0069454  0.01074214 0.00772771]
ene_comp = [1.35911095e-05 1.22609325e-04 1.30721794e-05 5.23295595e-07
 1.90107992e-04 2.09214532e-05 1.03439949e-06 3.69295686e-05
 1.74992457e-05 1.10333598e-05]
ene_total = [0.00795626 0.00749753 0.00812153 0.00735642 0.00769545 0.00776863
 0.00732114 0.00698233 0.01075964 0.00773874]
At round 23 energy consumption: 0.07919768178596374
At round 23 eta: 0.686268250122481
At round 23 a_n: 20.30404837596008
At round 23 local rounds: 12.328087095105714
At round 23 global rounds: 64.71786290003097
gradient difference: 0.48199230432510376
train() client id: f_00000-0-0 loss: 1.195278  [   32/  126]
train() client id: f_00000-0-1 loss: 1.219987  [   64/  126]
train() client id: f_00000-0-2 loss: 0.970650  [   96/  126]
train() client id: f_00000-1-0 loss: 1.229103  [   32/  126]
train() client id: f_00000-1-1 loss: 1.120499  [   64/  126]
train() client id: f_00000-1-2 loss: 0.996696  [   96/  126]
train() client id: f_00000-2-0 loss: 1.109921  [   32/  126]
train() client id: f_00000-2-1 loss: 1.006638  [   64/  126]
train() client id: f_00000-2-2 loss: 0.953570  [   96/  126]
train() client id: f_00000-3-0 loss: 0.974435  [   32/  126]
train() client id: f_00000-3-1 loss: 0.927033  [   64/  126]
train() client id: f_00000-3-2 loss: 0.985598  [   96/  126]
train() client id: f_00000-4-0 loss: 1.000272  [   32/  126]
train() client id: f_00000-4-1 loss: 0.917863  [   64/  126]
train() client id: f_00000-4-2 loss: 0.854273  [   96/  126]
train() client id: f_00000-5-0 loss: 0.918862  [   32/  126]
train() client id: f_00000-5-1 loss: 0.911811  [   64/  126]
train() client id: f_00000-5-2 loss: 0.926174  [   96/  126]
train() client id: f_00000-6-0 loss: 0.841281  [   32/  126]
train() client id: f_00000-6-1 loss: 0.913418  [   64/  126]
train() client id: f_00000-6-2 loss: 0.942953  [   96/  126]
train() client id: f_00000-7-0 loss: 0.822615  [   32/  126]
train() client id: f_00000-7-1 loss: 0.890790  [   64/  126]
train() client id: f_00000-7-2 loss: 0.815161  [   96/  126]
train() client id: f_00000-8-0 loss: 0.854894  [   32/  126]
train() client id: f_00000-8-1 loss: 0.848028  [   64/  126]
train() client id: f_00000-8-2 loss: 0.860780  [   96/  126]
train() client id: f_00000-9-0 loss: 0.748791  [   32/  126]
train() client id: f_00000-9-1 loss: 0.828416  [   64/  126]
train() client id: f_00000-9-2 loss: 0.802360  [   96/  126]
train() client id: f_00000-10-0 loss: 0.834763  [   32/  126]
train() client id: f_00000-10-1 loss: 0.923517  [   64/  126]
train() client id: f_00000-10-2 loss: 0.677793  [   96/  126]
train() client id: f_00000-11-0 loss: 0.827539  [   32/  126]
train() client id: f_00000-11-1 loss: 0.873817  [   64/  126]
train() client id: f_00000-11-2 loss: 0.746608  [   96/  126]
train() client id: f_00001-0-0 loss: 0.527389  [   32/  265]
train() client id: f_00001-0-1 loss: 0.484876  [   64/  265]
train() client id: f_00001-0-2 loss: 0.615097  [   96/  265]
train() client id: f_00001-0-3 loss: 0.438926  [  128/  265]
train() client id: f_00001-0-4 loss: 0.446397  [  160/  265]
train() client id: f_00001-0-5 loss: 0.413835  [  192/  265]
train() client id: f_00001-0-6 loss: 0.585594  [  224/  265]
train() client id: f_00001-0-7 loss: 0.462637  [  256/  265]
train() client id: f_00001-1-0 loss: 0.467321  [   32/  265]
train() client id: f_00001-1-1 loss: 0.462688  [   64/  265]
train() client id: f_00001-1-2 loss: 0.522252  [   96/  265]
train() client id: f_00001-1-3 loss: 0.462630  [  128/  265]
train() client id: f_00001-1-4 loss: 0.526941  [  160/  265]
train() client id: f_00001-1-5 loss: 0.499793  [  192/  265]
train() client id: f_00001-1-6 loss: 0.491961  [  224/  265]
train() client id: f_00001-1-7 loss: 0.485593  [  256/  265]
train() client id: f_00001-2-0 loss: 0.425381  [   32/  265]
train() client id: f_00001-2-1 loss: 0.643466  [   64/  265]
train() client id: f_00001-2-2 loss: 0.439772  [   96/  265]
train() client id: f_00001-2-3 loss: 0.505099  [  128/  265]
train() client id: f_00001-2-4 loss: 0.410130  [  160/  265]
train() client id: f_00001-2-5 loss: 0.425286  [  192/  265]
train() client id: f_00001-2-6 loss: 0.477052  [  224/  265]
train() client id: f_00001-2-7 loss: 0.540480  [  256/  265]
train() client id: f_00001-3-0 loss: 0.462177  [   32/  265]
train() client id: f_00001-3-1 loss: 0.656148  [   64/  265]
train() client id: f_00001-3-2 loss: 0.439651  [   96/  265]
train() client id: f_00001-3-3 loss: 0.480908  [  128/  265]
train() client id: f_00001-3-4 loss: 0.374088  [  160/  265]
train() client id: f_00001-3-5 loss: 0.449821  [  192/  265]
train() client id: f_00001-3-6 loss: 0.457865  [  224/  265]
train() client id: f_00001-3-7 loss: 0.506546  [  256/  265]
train() client id: f_00001-4-0 loss: 0.454778  [   32/  265]
train() client id: f_00001-4-1 loss: 0.438987  [   64/  265]
train() client id: f_00001-4-2 loss: 0.487377  [   96/  265]
train() client id: f_00001-4-3 loss: 0.434807  [  128/  265]
train() client id: f_00001-4-4 loss: 0.477892  [  160/  265]
train() client id: f_00001-4-5 loss: 0.520280  [  192/  265]
train() client id: f_00001-4-6 loss: 0.481295  [  224/  265]
train() client id: f_00001-4-7 loss: 0.514365  [  256/  265]
train() client id: f_00001-5-0 loss: 0.454815  [   32/  265]
train() client id: f_00001-5-1 loss: 0.471102  [   64/  265]
train() client id: f_00001-5-2 loss: 0.377311  [   96/  265]
train() client id: f_00001-5-3 loss: 0.440160  [  128/  265]
train() client id: f_00001-5-4 loss: 0.502906  [  160/  265]
train() client id: f_00001-5-5 loss: 0.414068  [  192/  265]
train() client id: f_00001-5-6 loss: 0.489287  [  224/  265]
train() client id: f_00001-5-7 loss: 0.572967  [  256/  265]
train() client id: f_00001-6-0 loss: 0.469079  [   32/  265]
train() client id: f_00001-6-1 loss: 0.417476  [   64/  265]
train() client id: f_00001-6-2 loss: 0.465020  [   96/  265]
train() client id: f_00001-6-3 loss: 0.507931  [  128/  265]
train() client id: f_00001-6-4 loss: 0.398399  [  160/  265]
train() client id: f_00001-6-5 loss: 0.443547  [  192/  265]
train() client id: f_00001-6-6 loss: 0.479724  [  224/  265]
train() client id: f_00001-6-7 loss: 0.599358  [  256/  265]
train() client id: f_00001-7-0 loss: 0.493115  [   32/  265]
train() client id: f_00001-7-1 loss: 0.488509  [   64/  265]
train() client id: f_00001-7-2 loss: 0.421316  [   96/  265]
train() client id: f_00001-7-3 loss: 0.509140  [  128/  265]
train() client id: f_00001-7-4 loss: 0.494062  [  160/  265]
train() client id: f_00001-7-5 loss: 0.534172  [  192/  265]
train() client id: f_00001-7-6 loss: 0.399147  [  224/  265]
train() client id: f_00001-7-7 loss: 0.376354  [  256/  265]
train() client id: f_00001-8-0 loss: 0.370184  [   32/  265]
train() client id: f_00001-8-1 loss: 0.681373  [   64/  265]
train() client id: f_00001-8-2 loss: 0.525247  [   96/  265]
train() client id: f_00001-8-3 loss: 0.430770  [  128/  265]
train() client id: f_00001-8-4 loss: 0.388102  [  160/  265]
train() client id: f_00001-8-5 loss: 0.441056  [  192/  265]
train() client id: f_00001-8-6 loss: 0.542790  [  224/  265]
train() client id: f_00001-8-7 loss: 0.382303  [  256/  265]
train() client id: f_00001-9-0 loss: 0.560643  [   32/  265]
train() client id: f_00001-9-1 loss: 0.387137  [   64/  265]
train() client id: f_00001-9-2 loss: 0.451986  [   96/  265]
train() client id: f_00001-9-3 loss: 0.428972  [  128/  265]
train() client id: f_00001-9-4 loss: 0.433388  [  160/  265]
train() client id: f_00001-9-5 loss: 0.466853  [  192/  265]
train() client id: f_00001-9-6 loss: 0.517060  [  224/  265]
train() client id: f_00001-9-7 loss: 0.460293  [  256/  265]
train() client id: f_00001-10-0 loss: 0.581185  [   32/  265]
train() client id: f_00001-10-1 loss: 0.524497  [   64/  265]
train() client id: f_00001-10-2 loss: 0.445440  [   96/  265]
train() client id: f_00001-10-3 loss: 0.397017  [  128/  265]
train() client id: f_00001-10-4 loss: 0.384769  [  160/  265]
train() client id: f_00001-10-5 loss: 0.483414  [  192/  265]
train() client id: f_00001-10-6 loss: 0.524611  [  224/  265]
train() client id: f_00001-10-7 loss: 0.429324  [  256/  265]
train() client id: f_00001-11-0 loss: 0.537268  [   32/  265]
train() client id: f_00001-11-1 loss: 0.532887  [   64/  265]
train() client id: f_00001-11-2 loss: 0.405015  [   96/  265]
train() client id: f_00001-11-3 loss: 0.389347  [  128/  265]
train() client id: f_00001-11-4 loss: 0.469267  [  160/  265]
train() client id: f_00001-11-5 loss: 0.499133  [  192/  265]
train() client id: f_00001-11-6 loss: 0.376447  [  224/  265]
train() client id: f_00001-11-7 loss: 0.561746  [  256/  265]
train() client id: f_00002-0-0 loss: 1.220584  [   32/  124]
train() client id: f_00002-0-1 loss: 1.258936  [   64/  124]
train() client id: f_00002-0-2 loss: 1.116961  [   96/  124]
train() client id: f_00002-1-0 loss: 1.161072  [   32/  124]
train() client id: f_00002-1-1 loss: 1.186538  [   64/  124]
train() client id: f_00002-1-2 loss: 1.203809  [   96/  124]
train() client id: f_00002-2-0 loss: 1.097844  [   32/  124]
train() client id: f_00002-2-1 loss: 1.248262  [   64/  124]
train() client id: f_00002-2-2 loss: 1.110246  [   96/  124]
train() client id: f_00002-3-0 loss: 1.152388  [   32/  124]
train() client id: f_00002-3-1 loss: 1.019483  [   64/  124]
train() client id: f_00002-3-2 loss: 1.258999  [   96/  124]
train() client id: f_00002-4-0 loss: 1.105931  [   32/  124]
train() client id: f_00002-4-1 loss: 0.982732  [   64/  124]
train() client id: f_00002-4-2 loss: 1.148358  [   96/  124]
train() client id: f_00002-5-0 loss: 1.031062  [   32/  124]
train() client id: f_00002-5-1 loss: 0.972884  [   64/  124]
train() client id: f_00002-5-2 loss: 1.063915  [   96/  124]
train() client id: f_00002-6-0 loss: 1.078453  [   32/  124]
train() client id: f_00002-6-1 loss: 1.124270  [   64/  124]
train() client id: f_00002-6-2 loss: 0.975104  [   96/  124]
train() client id: f_00002-7-0 loss: 1.006467  [   32/  124]
train() client id: f_00002-7-1 loss: 1.107609  [   64/  124]
train() client id: f_00002-7-2 loss: 1.010311  [   96/  124]
train() client id: f_00002-8-0 loss: 0.982077  [   32/  124]
train() client id: f_00002-8-1 loss: 1.115459  [   64/  124]
train() client id: f_00002-8-2 loss: 0.980588  [   96/  124]
train() client id: f_00002-9-0 loss: 0.937788  [   32/  124]
train() client id: f_00002-9-1 loss: 1.021067  [   64/  124]
train() client id: f_00002-9-2 loss: 1.031753  [   96/  124]
train() client id: f_00002-10-0 loss: 1.129921  [   32/  124]
train() client id: f_00002-10-1 loss: 0.945826  [   64/  124]
train() client id: f_00002-10-2 loss: 0.904037  [   96/  124]
train() client id: f_00002-11-0 loss: 1.014077  [   32/  124]
train() client id: f_00002-11-1 loss: 1.166069  [   64/  124]
train() client id: f_00002-11-2 loss: 0.872901  [   96/  124]
train() client id: f_00003-0-0 loss: 0.697301  [   32/   43]
train() client id: f_00003-1-0 loss: 0.665092  [   32/   43]
train() client id: f_00003-2-0 loss: 0.742718  [   32/   43]
train() client id: f_00003-3-0 loss: 0.708221  [   32/   43]
train() client id: f_00003-4-0 loss: 0.815600  [   32/   43]
train() client id: f_00003-5-0 loss: 0.798861  [   32/   43]
train() client id: f_00003-6-0 loss: 0.658579  [   32/   43]
train() client id: f_00003-7-0 loss: 0.834737  [   32/   43]
train() client id: f_00003-8-0 loss: 0.774574  [   32/   43]
train() client id: f_00003-9-0 loss: 0.740996  [   32/   43]
train() client id: f_00003-10-0 loss: 0.783726  [   32/   43]
train() client id: f_00003-11-0 loss: 0.939614  [   32/   43]
train() client id: f_00004-0-0 loss: 0.896380  [   32/  306]
train() client id: f_00004-0-1 loss: 0.770412  [   64/  306]
train() client id: f_00004-0-2 loss: 0.969819  [   96/  306]
train() client id: f_00004-0-3 loss: 1.006150  [  128/  306]
train() client id: f_00004-0-4 loss: 0.813802  [  160/  306]
train() client id: f_00004-0-5 loss: 0.955140  [  192/  306]
train() client id: f_00004-0-6 loss: 0.936742  [  224/  306]
train() client id: f_00004-0-7 loss: 0.846495  [  256/  306]
train() client id: f_00004-0-8 loss: 0.932517  [  288/  306]
train() client id: f_00004-1-0 loss: 0.958362  [   32/  306]
train() client id: f_00004-1-1 loss: 1.033947  [   64/  306]
train() client id: f_00004-1-2 loss: 0.870648  [   96/  306]
train() client id: f_00004-1-3 loss: 0.866570  [  128/  306]
train() client id: f_00004-1-4 loss: 0.773586  [  160/  306]
train() client id: f_00004-1-5 loss: 0.881051  [  192/  306]
train() client id: f_00004-1-6 loss: 0.860565  [  224/  306]
train() client id: f_00004-1-7 loss: 0.841609  [  256/  306]
train() client id: f_00004-1-8 loss: 0.943792  [  288/  306]
train() client id: f_00004-2-0 loss: 0.892565  [   32/  306]
train() client id: f_00004-2-1 loss: 0.999430  [   64/  306]
train() client id: f_00004-2-2 loss: 0.955933  [   96/  306]
train() client id: f_00004-2-3 loss: 0.805897  [  128/  306]
train() client id: f_00004-2-4 loss: 0.880224  [  160/  306]
train() client id: f_00004-2-5 loss: 0.891750  [  192/  306]
train() client id: f_00004-2-6 loss: 0.863830  [  224/  306]
train() client id: f_00004-2-7 loss: 0.871621  [  256/  306]
train() client id: f_00004-2-8 loss: 0.925464  [  288/  306]
train() client id: f_00004-3-0 loss: 0.910429  [   32/  306]
train() client id: f_00004-3-1 loss: 0.904828  [   64/  306]
train() client id: f_00004-3-2 loss: 0.882754  [   96/  306]
train() client id: f_00004-3-3 loss: 0.887526  [  128/  306]
train() client id: f_00004-3-4 loss: 0.695805  [  160/  306]
train() client id: f_00004-3-5 loss: 0.938471  [  192/  306]
train() client id: f_00004-3-6 loss: 0.986944  [  224/  306]
train() client id: f_00004-3-7 loss: 0.946274  [  256/  306]
train() client id: f_00004-3-8 loss: 0.960623  [  288/  306]
train() client id: f_00004-4-0 loss: 0.813294  [   32/  306]
train() client id: f_00004-4-1 loss: 0.814493  [   64/  306]
train() client id: f_00004-4-2 loss: 1.028021  [   96/  306]
train() client id: f_00004-4-3 loss: 0.971152  [  128/  306]
train() client id: f_00004-4-4 loss: 0.806850  [  160/  306]
train() client id: f_00004-4-5 loss: 0.876451  [  192/  306]
train() client id: f_00004-4-6 loss: 0.946672  [  224/  306]
train() client id: f_00004-4-7 loss: 0.850739  [  256/  306]
train() client id: f_00004-4-8 loss: 0.885315  [  288/  306]
train() client id: f_00004-5-0 loss: 0.828308  [   32/  306]
train() client id: f_00004-5-1 loss: 0.906480  [   64/  306]
train() client id: f_00004-5-2 loss: 0.788342  [   96/  306]
train() client id: f_00004-5-3 loss: 0.981415  [  128/  306]
train() client id: f_00004-5-4 loss: 0.920588  [  160/  306]
train() client id: f_00004-5-5 loss: 0.889592  [  192/  306]
train() client id: f_00004-5-6 loss: 0.881092  [  224/  306]
train() client id: f_00004-5-7 loss: 1.037889  [  256/  306]
train() client id: f_00004-5-8 loss: 0.791533  [  288/  306]
train() client id: f_00004-6-0 loss: 0.871655  [   32/  306]
train() client id: f_00004-6-1 loss: 0.936640  [   64/  306]
train() client id: f_00004-6-2 loss: 0.828246  [   96/  306]
train() client id: f_00004-6-3 loss: 0.869611  [  128/  306]
train() client id: f_00004-6-4 loss: 0.883731  [  160/  306]
train() client id: f_00004-6-5 loss: 0.993732  [  192/  306]
train() client id: f_00004-6-6 loss: 0.827084  [  224/  306]
train() client id: f_00004-6-7 loss: 0.951543  [  256/  306]
train() client id: f_00004-6-8 loss: 0.905503  [  288/  306]
train() client id: f_00004-7-0 loss: 0.893842  [   32/  306]
train() client id: f_00004-7-1 loss: 0.775146  [   64/  306]
train() client id: f_00004-7-2 loss: 0.977453  [   96/  306]
train() client id: f_00004-7-3 loss: 0.972241  [  128/  306]
train() client id: f_00004-7-4 loss: 0.860111  [  160/  306]
train() client id: f_00004-7-5 loss: 0.875615  [  192/  306]
train() client id: f_00004-7-6 loss: 0.932263  [  224/  306]
train() client id: f_00004-7-7 loss: 0.799003  [  256/  306]
train() client id: f_00004-7-8 loss: 0.854590  [  288/  306]
train() client id: f_00004-8-0 loss: 0.923268  [   32/  306]
train() client id: f_00004-8-1 loss: 0.830612  [   64/  306]
train() client id: f_00004-8-2 loss: 0.911100  [   96/  306]
train() client id: f_00004-8-3 loss: 0.786125  [  128/  306]
train() client id: f_00004-8-4 loss: 0.880810  [  160/  306]
train() client id: f_00004-8-5 loss: 0.982111  [  192/  306]
train() client id: f_00004-8-6 loss: 0.825615  [  224/  306]
train() client id: f_00004-8-7 loss: 0.940907  [  256/  306]
train() client id: f_00004-8-8 loss: 0.929312  [  288/  306]
train() client id: f_00004-9-0 loss: 0.867902  [   32/  306]
train() client id: f_00004-9-1 loss: 0.807681  [   64/  306]
train() client id: f_00004-9-2 loss: 0.794201  [   96/  306]
train() client id: f_00004-9-3 loss: 0.912516  [  128/  306]
train() client id: f_00004-9-4 loss: 0.865994  [  160/  306]
train() client id: f_00004-9-5 loss: 0.962668  [  192/  306]
train() client id: f_00004-9-6 loss: 0.928267  [  224/  306]
train() client id: f_00004-9-7 loss: 0.996570  [  256/  306]
train() client id: f_00004-9-8 loss: 0.837180  [  288/  306]
train() client id: f_00004-10-0 loss: 0.846706  [   32/  306]
train() client id: f_00004-10-1 loss: 0.908092  [   64/  306]
train() client id: f_00004-10-2 loss: 0.879603  [   96/  306]
train() client id: f_00004-10-3 loss: 0.964646  [  128/  306]
train() client id: f_00004-10-4 loss: 0.948200  [  160/  306]
train() client id: f_00004-10-5 loss: 0.948991  [  192/  306]
train() client id: f_00004-10-6 loss: 0.860830  [  224/  306]
train() client id: f_00004-10-7 loss: 0.863121  [  256/  306]
train() client id: f_00004-10-8 loss: 0.883479  [  288/  306]
train() client id: f_00004-11-0 loss: 0.769841  [   32/  306]
train() client id: f_00004-11-1 loss: 0.909425  [   64/  306]
train() client id: f_00004-11-2 loss: 0.874312  [   96/  306]
train() client id: f_00004-11-3 loss: 0.851171  [  128/  306]
train() client id: f_00004-11-4 loss: 0.942629  [  160/  306]
train() client id: f_00004-11-5 loss: 0.957679  [  192/  306]
train() client id: f_00004-11-6 loss: 0.932506  [  224/  306]
train() client id: f_00004-11-7 loss: 0.950496  [  256/  306]
train() client id: f_00004-11-8 loss: 0.813041  [  288/  306]
train() client id: f_00005-0-0 loss: 0.844529  [   32/  146]
train() client id: f_00005-0-1 loss: 0.546710  [   64/  146]
train() client id: f_00005-0-2 loss: 0.681538  [   96/  146]
train() client id: f_00005-0-3 loss: 0.662347  [  128/  146]
train() client id: f_00005-1-0 loss: 0.819393  [   32/  146]
train() client id: f_00005-1-1 loss: 0.634514  [   64/  146]
train() client id: f_00005-1-2 loss: 0.788257  [   96/  146]
train() client id: f_00005-1-3 loss: 0.718752  [  128/  146]
train() client id: f_00005-2-0 loss: 0.903576  [   32/  146]
train() client id: f_00005-2-1 loss: 0.535897  [   64/  146]
train() client id: f_00005-2-2 loss: 0.564323  [   96/  146]
train() client id: f_00005-2-3 loss: 0.825323  [  128/  146]
train() client id: f_00005-3-0 loss: 0.794710  [   32/  146]
train() client id: f_00005-3-1 loss: 0.749383  [   64/  146]
train() client id: f_00005-3-2 loss: 0.744802  [   96/  146]
train() client id: f_00005-3-3 loss: 0.598377  [  128/  146]
train() client id: f_00005-4-0 loss: 0.804305  [   32/  146]
train() client id: f_00005-4-1 loss: 0.707711  [   64/  146]
train() client id: f_00005-4-2 loss: 0.800660  [   96/  146]
train() client id: f_00005-4-3 loss: 0.545499  [  128/  146]
train() client id: f_00005-5-0 loss: 0.954371  [   32/  146]
train() client id: f_00005-5-1 loss: 0.668019  [   64/  146]
train() client id: f_00005-5-2 loss: 0.565999  [   96/  146]
train() client id: f_00005-5-3 loss: 0.704178  [  128/  146]
train() client id: f_00005-6-0 loss: 0.539340  [   32/  146]
train() client id: f_00005-6-1 loss: 0.836237  [   64/  146]
train() client id: f_00005-6-2 loss: 0.726490  [   96/  146]
train() client id: f_00005-6-3 loss: 0.782552  [  128/  146]
train() client id: f_00005-7-0 loss: 0.583680  [   32/  146]
train() client id: f_00005-7-1 loss: 0.736218  [   64/  146]
train() client id: f_00005-7-2 loss: 0.660983  [   96/  146]
train() client id: f_00005-7-3 loss: 0.983303  [  128/  146]
train() client id: f_00005-8-0 loss: 0.628118  [   32/  146]
train() client id: f_00005-8-1 loss: 0.527327  [   64/  146]
train() client id: f_00005-8-2 loss: 0.937965  [   96/  146]
train() client id: f_00005-8-3 loss: 0.717185  [  128/  146]
train() client id: f_00005-9-0 loss: 0.524920  [   32/  146]
train() client id: f_00005-9-1 loss: 0.656475  [   64/  146]
train() client id: f_00005-9-2 loss: 0.957511  [   96/  146]
train() client id: f_00005-9-3 loss: 0.667449  [  128/  146]
train() client id: f_00005-10-0 loss: 0.616688  [   32/  146]
train() client id: f_00005-10-1 loss: 0.648774  [   64/  146]
train() client id: f_00005-10-2 loss: 0.702710  [   96/  146]
train() client id: f_00005-10-3 loss: 0.919953  [  128/  146]
train() client id: f_00005-11-0 loss: 0.689510  [   32/  146]
train() client id: f_00005-11-1 loss: 0.885820  [   64/  146]
train() client id: f_00005-11-2 loss: 0.756307  [   96/  146]
train() client id: f_00005-11-3 loss: 0.440157  [  128/  146]
train() client id: f_00006-0-0 loss: 0.629985  [   32/   54]
train() client id: f_00006-1-0 loss: 0.545647  [   32/   54]
train() client id: f_00006-2-0 loss: 0.626141  [   32/   54]
train() client id: f_00006-3-0 loss: 0.575241  [   32/   54]
train() client id: f_00006-4-0 loss: 0.584325  [   32/   54]
train() client id: f_00006-5-0 loss: 0.576173  [   32/   54]
train() client id: f_00006-6-0 loss: 0.624834  [   32/   54]
train() client id: f_00006-7-0 loss: 0.583618  [   32/   54]
train() client id: f_00006-8-0 loss: 0.530548  [   32/   54]
train() client id: f_00006-9-0 loss: 0.565859  [   32/   54]
train() client id: f_00006-10-0 loss: 0.586325  [   32/   54]
train() client id: f_00006-11-0 loss: 0.582699  [   32/   54]
train() client id: f_00007-0-0 loss: 0.696892  [   32/  179]
train() client id: f_00007-0-1 loss: 0.601269  [   64/  179]
train() client id: f_00007-0-2 loss: 0.830730  [   96/  179]
train() client id: f_00007-0-3 loss: 0.528239  [  128/  179]
train() client id: f_00007-0-4 loss: 0.518222  [  160/  179]
train() client id: f_00007-1-0 loss: 0.605155  [   32/  179]
train() client id: f_00007-1-1 loss: 0.596837  [   64/  179]
train() client id: f_00007-1-2 loss: 0.740966  [   96/  179]
train() client id: f_00007-1-3 loss: 0.695657  [  128/  179]
train() client id: f_00007-1-4 loss: 0.483761  [  160/  179]
train() client id: f_00007-2-0 loss: 0.641225  [   32/  179]
train() client id: f_00007-2-1 loss: 0.604060  [   64/  179]
train() client id: f_00007-2-2 loss: 0.634976  [   96/  179]
train() client id: f_00007-2-3 loss: 0.484342  [  128/  179]
train() client id: f_00007-2-4 loss: 0.692629  [  160/  179]
train() client id: f_00007-3-0 loss: 0.487684  [   32/  179]
train() client id: f_00007-3-1 loss: 0.489415  [   64/  179]
train() client id: f_00007-3-2 loss: 0.674213  [   96/  179]
train() client id: f_00007-3-3 loss: 0.740965  [  128/  179]
train() client id: f_00007-3-4 loss: 0.782503  [  160/  179]
train() client id: f_00007-4-0 loss: 0.593467  [   32/  179]
train() client id: f_00007-4-1 loss: 0.783359  [   64/  179]
train() client id: f_00007-4-2 loss: 0.488658  [   96/  179]
train() client id: f_00007-4-3 loss: 0.727536  [  128/  179]
train() client id: f_00007-4-4 loss: 0.585884  [  160/  179]
train() client id: f_00007-5-0 loss: 0.520870  [   32/  179]
train() client id: f_00007-5-1 loss: 0.805341  [   64/  179]
train() client id: f_00007-5-2 loss: 0.564641  [   96/  179]
train() client id: f_00007-5-3 loss: 0.500199  [  128/  179]
train() client id: f_00007-5-4 loss: 0.677360  [  160/  179]
train() client id: f_00007-6-0 loss: 0.793916  [   32/  179]
train() client id: f_00007-6-1 loss: 0.643814  [   64/  179]
train() client id: f_00007-6-2 loss: 0.481880  [   96/  179]
train() client id: f_00007-6-3 loss: 0.635661  [  128/  179]
train() client id: f_00007-6-4 loss: 0.504511  [  160/  179]
train() client id: f_00007-7-0 loss: 0.485341  [   32/  179]
train() client id: f_00007-7-1 loss: 0.685289  [   64/  179]
train() client id: f_00007-7-2 loss: 0.535501  [   96/  179]
train() client id: f_00007-7-3 loss: 0.658416  [  128/  179]
train() client id: f_00007-7-4 loss: 0.607936  [  160/  179]
train() client id: f_00007-8-0 loss: 0.718408  [   32/  179]
train() client id: f_00007-8-1 loss: 0.654309  [   64/  179]
train() client id: f_00007-8-2 loss: 0.640183  [   96/  179]
train() client id: f_00007-8-3 loss: 0.553717  [  128/  179]
train() client id: f_00007-8-4 loss: 0.488380  [  160/  179]
train() client id: f_00007-9-0 loss: 0.786830  [   32/  179]
train() client id: f_00007-9-1 loss: 0.488332  [   64/  179]
train() client id: f_00007-9-2 loss: 0.632544  [   96/  179]
train() client id: f_00007-9-3 loss: 0.473042  [  128/  179]
train() client id: f_00007-9-4 loss: 0.734962  [  160/  179]
train() client id: f_00007-10-0 loss: 0.728206  [   32/  179]
train() client id: f_00007-10-1 loss: 0.657425  [   64/  179]
train() client id: f_00007-10-2 loss: 0.699084  [   96/  179]
train() client id: f_00007-10-3 loss: 0.552545  [  128/  179]
train() client id: f_00007-10-4 loss: 0.480911  [  160/  179]
train() client id: f_00007-11-0 loss: 0.642184  [   32/  179]
train() client id: f_00007-11-1 loss: 0.630718  [   64/  179]
train() client id: f_00007-11-2 loss: 0.646037  [   96/  179]
train() client id: f_00007-11-3 loss: 0.560306  [  128/  179]
train() client id: f_00007-11-4 loss: 0.640751  [  160/  179]
train() client id: f_00008-0-0 loss: 0.964176  [   32/  130]
train() client id: f_00008-0-1 loss: 0.765451  [   64/  130]
train() client id: f_00008-0-2 loss: 0.827562  [   96/  130]
train() client id: f_00008-0-3 loss: 0.838522  [  128/  130]
train() client id: f_00008-1-0 loss: 0.794553  [   32/  130]
train() client id: f_00008-1-1 loss: 0.865614  [   64/  130]
train() client id: f_00008-1-2 loss: 0.939985  [   96/  130]
train() client id: f_00008-1-3 loss: 0.787579  [  128/  130]
train() client id: f_00008-2-0 loss: 0.801936  [   32/  130]
train() client id: f_00008-2-1 loss: 0.851915  [   64/  130]
train() client id: f_00008-2-2 loss: 0.884635  [   96/  130]
train() client id: f_00008-2-3 loss: 0.849495  [  128/  130]
train() client id: f_00008-3-0 loss: 0.954086  [   32/  130]
train() client id: f_00008-3-1 loss: 0.849930  [   64/  130]
train() client id: f_00008-3-2 loss: 0.658298  [   96/  130]
train() client id: f_00008-3-3 loss: 0.892516  [  128/  130]
train() client id: f_00008-4-0 loss: 0.755354  [   32/  130]
train() client id: f_00008-4-1 loss: 0.939817  [   64/  130]
train() client id: f_00008-4-2 loss: 0.693192  [   96/  130]
train() client id: f_00008-4-3 loss: 0.998262  [  128/  130]
train() client id: f_00008-5-0 loss: 1.025182  [   32/  130]
train() client id: f_00008-5-1 loss: 0.763776  [   64/  130]
train() client id: f_00008-5-2 loss: 0.808620  [   96/  130]
train() client id: f_00008-5-3 loss: 0.785577  [  128/  130]
train() client id: f_00008-6-0 loss: 0.764510  [   32/  130]
train() client id: f_00008-6-1 loss: 0.787167  [   64/  130]
train() client id: f_00008-6-2 loss: 0.809614  [   96/  130]
train() client id: f_00008-6-3 loss: 0.983651  [  128/  130]
train() client id: f_00008-7-0 loss: 0.896691  [   32/  130]
train() client id: f_00008-7-1 loss: 0.805728  [   64/  130]
train() client id: f_00008-7-2 loss: 0.800404  [   96/  130]
train() client id: f_00008-7-3 loss: 0.868568  [  128/  130]
train() client id: f_00008-8-0 loss: 0.788084  [   32/  130]
train() client id: f_00008-8-1 loss: 0.858300  [   64/  130]
train() client id: f_00008-8-2 loss: 0.749714  [   96/  130]
train() client id: f_00008-8-3 loss: 0.981191  [  128/  130]
train() client id: f_00008-9-0 loss: 0.782673  [   32/  130]
train() client id: f_00008-9-1 loss: 0.980723  [   64/  130]
train() client id: f_00008-9-2 loss: 0.809292  [   96/  130]
train() client id: f_00008-9-3 loss: 0.777776  [  128/  130]
train() client id: f_00008-10-0 loss: 0.937977  [   32/  130]
train() client id: f_00008-10-1 loss: 0.728145  [   64/  130]
train() client id: f_00008-10-2 loss: 0.818056  [   96/  130]
train() client id: f_00008-10-3 loss: 0.893878  [  128/  130]
train() client id: f_00008-11-0 loss: 0.797709  [   32/  130]
train() client id: f_00008-11-1 loss: 0.917055  [   64/  130]
train() client id: f_00008-11-2 loss: 0.828560  [   96/  130]
train() client id: f_00008-11-3 loss: 0.836894  [  128/  130]
train() client id: f_00009-0-0 loss: 1.142342  [   32/  118]
train() client id: f_00009-0-1 loss: 1.041350  [   64/  118]
train() client id: f_00009-0-2 loss: 1.069222  [   96/  118]
train() client id: f_00009-1-0 loss: 1.008261  [   32/  118]
train() client id: f_00009-1-1 loss: 1.093025  [   64/  118]
train() client id: f_00009-1-2 loss: 0.974585  [   96/  118]
train() client id: f_00009-2-0 loss: 0.952080  [   32/  118]
train() client id: f_00009-2-1 loss: 1.100157  [   64/  118]
train() client id: f_00009-2-2 loss: 0.907934  [   96/  118]
train() client id: f_00009-3-0 loss: 0.961730  [   32/  118]
train() client id: f_00009-3-1 loss: 0.922663  [   64/  118]
train() client id: f_00009-3-2 loss: 0.958636  [   96/  118]
train() client id: f_00009-4-0 loss: 0.933578  [   32/  118]
train() client id: f_00009-4-1 loss: 1.022850  [   64/  118]
train() client id: f_00009-4-2 loss: 0.809726  [   96/  118]
train() client id: f_00009-5-0 loss: 0.905716  [   32/  118]
train() client id: f_00009-5-1 loss: 0.820006  [   64/  118]
train() client id: f_00009-5-2 loss: 0.923186  [   96/  118]
train() client id: f_00009-6-0 loss: 0.893144  [   32/  118]
train() client id: f_00009-6-1 loss: 0.874549  [   64/  118]
train() client id: f_00009-6-2 loss: 0.962679  [   96/  118]
train() client id: f_00009-7-0 loss: 0.938492  [   32/  118]
train() client id: f_00009-7-1 loss: 0.807979  [   64/  118]
train() client id: f_00009-7-2 loss: 0.765441  [   96/  118]
train() client id: f_00009-8-0 loss: 0.850088  [   32/  118]
train() client id: f_00009-8-1 loss: 0.823859  [   64/  118]
train() client id: f_00009-8-2 loss: 0.935618  [   96/  118]
train() client id: f_00009-9-0 loss: 0.833250  [   32/  118]
train() client id: f_00009-9-1 loss: 0.749827  [   64/  118]
train() client id: f_00009-9-2 loss: 1.050694  [   96/  118]
train() client id: f_00009-10-0 loss: 0.935786  [   32/  118]
train() client id: f_00009-10-1 loss: 0.733279  [   64/  118]
train() client id: f_00009-10-2 loss: 0.925133  [   96/  118]
train() client id: f_00009-11-0 loss: 0.984948  [   32/  118]
train() client id: f_00009-11-1 loss: 0.779063  [   64/  118]
train() client id: f_00009-11-2 loss: 0.776619  [   96/  118]
At round 23 accuracy: 0.6445623342175066
At round 23 training accuracy: 0.5861837692823608
At round 23 training loss: 0.8290247865985657
update_location
xs = -3.905658 4.200318 135.009024 18.811294 0.979296 3.956410 -97.443192 -76.324852 119.663977 -62.060879 
ys = 127.587959 110.555839 1.320614 -97.455176 89.350187 72.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 162.154067 149.131607 168.015418 140.894912 134.105984 123.764105 139.649798 125.802064 156.933544 117.760624 
dists_bs = 177.542038 190.435826 355.345077 334.357056 195.715618 206.076065 193.818713 200.186511 334.082890 204.928618 
uav_gains = -105.260725 -104.343457 -105.653369 -103.724353 -103.187209 -102.315198 -103.627755 -102.492609 -104.901117 -101.775167 
bs_gains = -102.547493 -103.400021 -110.985268 -110.244953 -103.732573 -104.359831 -103.614139 -104.007234 -110.234978 -104.291932 
Round 24
-------------------------------
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.82454946 16.26273367  7.70634767  2.76510758 18.75837368  9.03341251
  3.43331975 11.02682771  8.12439108  7.33018602]
obj_prev = 92.26524913835074
eta_min = 1.3786631317009973e-12	eta_max = 0.9235436873576162
af = 19.491574900134562	bf = 1.580577038913744	zeta = 21.44073239014802	eta = 0.909090909090909
af = 19.491574900134562	bf = 1.580577038913744	zeta = 37.821548020465336	eta = 0.5153563489677266
af = 19.491574900134562	bf = 1.580577038913744	zeta = 29.921938016706985	eta = 0.65141418611493
af = 19.491574900134562	bf = 1.580577038913744	zeta = 28.500452198585428	eta = 0.6839040575328834
af = 19.491574900134562	bf = 1.580577038913744	zeta = 28.428583728430333	eta = 0.6856329912995908
af = 19.491574900134562	bf = 1.580577038913744	zeta = 28.428386266155645	eta = 0.685637753675084
eta = 0.685637753675084
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [0.03114263 0.06549838 0.0306483  0.01062804 0.07563209 0.0360859
 0.01334684 0.0442423  0.03213128 0.02916532]
ene_total = [2.48895914 4.62483507 2.46833909 1.14803217 5.27587599 2.77970132
 1.31845091 3.25561899 2.72998043 2.33859317]
ti_comp = [0.37300745 0.38019815 0.37131151 0.37903648 0.37900657 0.37665288
 0.37938621 0.38326735 0.34493694 0.37691463]
ti_coms = [0.08054563 0.07335493 0.08224157 0.07451661 0.07454651 0.07690021
 0.07416687 0.07028573 0.10861614 0.07663845]
t_total = [28.79989929 28.79989929 28.79989929 28.79989929 28.79989929 28.79989929
 28.79989929 28.79989929 28.79989929 28.79989929]
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [1.35678390e-05 1.21493121e-04 1.30503540e-05 5.22248762e-07
 1.88236947e-04 2.07018929e-05 1.03241013e-06 3.68459284e-05
 1.74254783e-05 1.09142550e-05]
ene_total = [0.51231347 0.47350677 0.52304957 0.47320176 0.48531125 0.48961862
 0.47101339 0.44864286 0.69080172 0.48733502]
optimize_network iter = 0 obj = 5.054794435766291
eta = 0.685637753675084
freqs = [41745313.04826085 86137162.39129016 41270331.48315956 14019810.25468265
 99776755.16786237 47903390.74277508 17590043.05099849 57717284.54230393
 46575586.46414848 38689551.73454573]
eta_min = 0.6856377536750926	eta_max = 0.6856377536750637
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 0.024656332621380766	eta = 0.909090909090909
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 17.409847423005747	eta = 0.0012874810038828607
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 1.7319792587002336	eta = 0.012941753040645609
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 1.7000222977574324	eta = 0.013185031671165255
af = 0.022414847837618875	bf = 1.580577038913744	zeta = 1.700018383049954	eta = 0.013185062032920516
eta = 0.013185062032920516
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [1.55616847e-04 1.39346998e-03 1.49681533e-04 5.98995210e-06
 2.15899085e-03 2.37441151e-04 1.18412674e-05 4.22605781e-04
 1.99862188e-04 1.25181464e-04]
ene_total = [0.16607727 0.17657133 0.16938779 0.15085494 0.1944668  0.16035839
 0.15026585 0.15072403 0.2237539  0.15755809]
ti_comp = [0.37300745 0.38019815 0.37131151 0.37903648 0.37900657 0.37665288
 0.37938621 0.38326735 0.34493694 0.37691463]
ti_coms = [0.08054563 0.07335493 0.08224157 0.07451661 0.07454651 0.07690021
 0.07416687 0.07028573 0.10861614 0.07663845]
t_total = [28.79989929 28.79989929 28.79989929 28.79989929 28.79989929 28.79989929
 28.79989929 28.79989929 28.79989929 28.79989929]
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [1.35678390e-05 1.21493121e-04 1.30503540e-05 5.22248762e-07
 1.88236947e-04 2.07018929e-05 1.03241013e-06 3.68459284e-05
 1.74254783e-05 1.09142550e-05]
ene_total = [0.51231347 0.47350677 0.52304957 0.47320176 0.48531125 0.48961862
 0.47101339 0.44864286 0.69080172 0.48733502]
optimize_network iter = 1 obj = 5.0547944357664285
eta = 0.6856377536750926
freqs = [41745313.04826085 86137162.3912901  41270331.48315956 14019810.25468264
 99776755.16786231 47903390.74277506 17590043.05099848 57717284.54230388
 46575586.46414861 38689551.73454572]
Done!
ene_coms = [0.00805456 0.00733549 0.00822416 0.00745166 0.00745465 0.00769002
 0.00741669 0.00702857 0.01086161 0.00766385]
ene_comp = [1.31745940e-05 1.17971811e-04 1.26721075e-05 5.07112105e-07
 1.82781160e-04 2.01018772e-05 1.00248715e-06 3.57780002e-05
 1.69204250e-05 1.05979204e-05]
ene_total = [0.00806774 0.00745347 0.00823683 0.00745217 0.00763743 0.00771012
 0.00741769 0.00706435 0.01087853 0.00767444]
At round 24 energy consumption: 0.07959277336451151
At round 24 eta: 0.6856377536750926
At round 24 a_n: 19.96150252899076
At round 24 local rounds: 12.358184869136853
At round 24 global rounds: 63.498409119903215
gradient difference: 0.5255438089370728
train() client id: f_00000-0-0 loss: 1.369252  [   32/  126]
train() client id: f_00000-0-1 loss: 0.988566  [   64/  126]
train() client id: f_00000-0-2 loss: 1.346539  [   96/  126]
train() client id: f_00000-1-0 loss: 1.035486  [   32/  126]
train() client id: f_00000-1-1 loss: 1.090875  [   64/  126]
train() client id: f_00000-1-2 loss: 1.106843  [   96/  126]
train() client id: f_00000-2-0 loss: 1.003518  [   32/  126]
train() client id: f_00000-2-1 loss: 1.087072  [   64/  126]
train() client id: f_00000-2-2 loss: 1.038247  [   96/  126]
train() client id: f_00000-3-0 loss: 1.024517  [   32/  126]
train() client id: f_00000-3-1 loss: 1.062650  [   64/  126]
train() client id: f_00000-3-2 loss: 0.898377  [   96/  126]
train() client id: f_00000-4-0 loss: 0.943010  [   32/  126]
train() client id: f_00000-4-1 loss: 0.967593  [   64/  126]
train() client id: f_00000-4-2 loss: 0.963949  [   96/  126]
train() client id: f_00000-5-0 loss: 0.950495  [   32/  126]
train() client id: f_00000-5-1 loss: 0.880313  [   64/  126]
train() client id: f_00000-5-2 loss: 0.997280  [   96/  126]
train() client id: f_00000-6-0 loss: 0.830125  [   32/  126]
train() client id: f_00000-6-1 loss: 0.916243  [   64/  126]
train() client id: f_00000-6-2 loss: 0.973827  [   96/  126]
train() client id: f_00000-7-0 loss: 0.922061  [   32/  126]
train() client id: f_00000-7-1 loss: 0.862651  [   64/  126]
train() client id: f_00000-7-2 loss: 0.973506  [   96/  126]
train() client id: f_00000-8-0 loss: 0.913574  [   32/  126]
train() client id: f_00000-8-1 loss: 0.864174  [   64/  126]
train() client id: f_00000-8-2 loss: 0.961910  [   96/  126]
train() client id: f_00000-9-0 loss: 0.963114  [   32/  126]
train() client id: f_00000-9-1 loss: 0.929753  [   64/  126]
train() client id: f_00000-9-2 loss: 0.891917  [   96/  126]
train() client id: f_00000-10-0 loss: 0.966915  [   32/  126]
train() client id: f_00000-10-1 loss: 0.854502  [   64/  126]
train() client id: f_00000-10-2 loss: 0.971331  [   96/  126]
train() client id: f_00000-11-0 loss: 0.977620  [   32/  126]
train() client id: f_00000-11-1 loss: 0.860833  [   64/  126]
train() client id: f_00000-11-2 loss: 1.018844  [   96/  126]
train() client id: f_00001-0-0 loss: 0.414002  [   32/  265]
train() client id: f_00001-0-1 loss: 0.450974  [   64/  265]
train() client id: f_00001-0-2 loss: 0.450321  [   96/  265]
train() client id: f_00001-0-3 loss: 0.301123  [  128/  265]
train() client id: f_00001-0-4 loss: 0.281911  [  160/  265]
train() client id: f_00001-0-5 loss: 0.407587  [  192/  265]
train() client id: f_00001-0-6 loss: 0.355353  [  224/  265]
train() client id: f_00001-0-7 loss: 0.336815  [  256/  265]
train() client id: f_00001-1-0 loss: 0.369380  [   32/  265]
train() client id: f_00001-1-1 loss: 0.338967  [   64/  265]
train() client id: f_00001-1-2 loss: 0.376850  [   96/  265]
train() client id: f_00001-1-3 loss: 0.321214  [  128/  265]
train() client id: f_00001-1-4 loss: 0.396287  [  160/  265]
train() client id: f_00001-1-5 loss: 0.456974  [  192/  265]
train() client id: f_00001-1-6 loss: 0.316529  [  224/  265]
train() client id: f_00001-1-7 loss: 0.347049  [  256/  265]
train() client id: f_00001-2-0 loss: 0.260676  [   32/  265]
train() client id: f_00001-2-1 loss: 0.406115  [   64/  265]
train() client id: f_00001-2-2 loss: 0.402815  [   96/  265]
train() client id: f_00001-2-3 loss: 0.323574  [  128/  265]
train() client id: f_00001-2-4 loss: 0.400956  [  160/  265]
train() client id: f_00001-2-5 loss: 0.412869  [  192/  265]
train() client id: f_00001-2-6 loss: 0.370752  [  224/  265]
train() client id: f_00001-2-7 loss: 0.274662  [  256/  265]
train() client id: f_00001-3-0 loss: 0.338441  [   32/  265]
train() client id: f_00001-3-1 loss: 0.267096  [   64/  265]
train() client id: f_00001-3-2 loss: 0.318195  [   96/  265]
train() client id: f_00001-3-3 loss: 0.333443  [  128/  265]
train() client id: f_00001-3-4 loss: 0.469532  [  160/  265]
train() client id: f_00001-3-5 loss: 0.431426  [  192/  265]
train() client id: f_00001-3-6 loss: 0.321560  [  224/  265]
train() client id: f_00001-3-7 loss: 0.321372  [  256/  265]
train() client id: f_00001-4-0 loss: 0.375045  [   32/  265]
train() client id: f_00001-4-1 loss: 0.388676  [   64/  265]
train() client id: f_00001-4-2 loss: 0.409190  [   96/  265]
train() client id: f_00001-4-3 loss: 0.348381  [  128/  265]
train() client id: f_00001-4-4 loss: 0.256424  [  160/  265]
train() client id: f_00001-4-5 loss: 0.256280  [  192/  265]
train() client id: f_00001-4-6 loss: 0.302770  [  224/  265]
train() client id: f_00001-4-7 loss: 0.425696  [  256/  265]
train() client id: f_00001-5-0 loss: 0.322236  [   32/  265]
train() client id: f_00001-5-1 loss: 0.354724  [   64/  265]
train() client id: f_00001-5-2 loss: 0.339086  [   96/  265]
train() client id: f_00001-5-3 loss: 0.254457  [  128/  265]
train() client id: f_00001-5-4 loss: 0.319740  [  160/  265]
train() client id: f_00001-5-5 loss: 0.387833  [  192/  265]
train() client id: f_00001-5-6 loss: 0.462294  [  224/  265]
train() client id: f_00001-5-7 loss: 0.295423  [  256/  265]
train() client id: f_00001-6-0 loss: 0.602539  [   32/  265]
train() client id: f_00001-6-1 loss: 0.296790  [   64/  265]
train() client id: f_00001-6-2 loss: 0.261068  [   96/  265]
train() client id: f_00001-6-3 loss: 0.363808  [  128/  265]
train() client id: f_00001-6-4 loss: 0.322156  [  160/  265]
train() client id: f_00001-6-5 loss: 0.259438  [  192/  265]
train() client id: f_00001-6-6 loss: 0.248303  [  224/  265]
train() client id: f_00001-6-7 loss: 0.360376  [  256/  265]
train() client id: f_00001-7-0 loss: 0.375386  [   32/  265]
train() client id: f_00001-7-1 loss: 0.294863  [   64/  265]
train() client id: f_00001-7-2 loss: 0.268171  [   96/  265]
train() client id: f_00001-7-3 loss: 0.417951  [  128/  265]
train() client id: f_00001-7-4 loss: 0.324938  [  160/  265]
train() client id: f_00001-7-5 loss: 0.331990  [  192/  265]
train() client id: f_00001-7-6 loss: 0.222635  [  224/  265]
train() client id: f_00001-7-7 loss: 0.377157  [  256/  265]
train() client id: f_00001-8-0 loss: 0.311733  [   32/  265]
train() client id: f_00001-8-1 loss: 0.478593  [   64/  265]
train() client id: f_00001-8-2 loss: 0.297834  [   96/  265]
train() client id: f_00001-8-3 loss: 0.324966  [  128/  265]
train() client id: f_00001-8-4 loss: 0.346257  [  160/  265]
train() client id: f_00001-8-5 loss: 0.301853  [  192/  265]
train() client id: f_00001-8-6 loss: 0.288823  [  224/  265]
train() client id: f_00001-8-7 loss: 0.336977  [  256/  265]
train() client id: f_00001-9-0 loss: 0.377361  [   32/  265]
train() client id: f_00001-9-1 loss: 0.239513  [   64/  265]
train() client id: f_00001-9-2 loss: 0.226846  [   96/  265]
train() client id: f_00001-9-3 loss: 0.370724  [  128/  265]
train() client id: f_00001-9-4 loss: 0.324066  [  160/  265]
train() client id: f_00001-9-5 loss: 0.350932  [  192/  265]
train() client id: f_00001-9-6 loss: 0.432090  [  224/  265]
train() client id: f_00001-9-7 loss: 0.360021  [  256/  265]
train() client id: f_00001-10-0 loss: 0.236857  [   32/  265]
train() client id: f_00001-10-1 loss: 0.242390  [   64/  265]
train() client id: f_00001-10-2 loss: 0.341166  [   96/  265]
train() client id: f_00001-10-3 loss: 0.313259  [  128/  265]
train() client id: f_00001-10-4 loss: 0.287915  [  160/  265]
train() client id: f_00001-10-5 loss: 0.246610  [  192/  265]
train() client id: f_00001-10-6 loss: 0.633108  [  224/  265]
train() client id: f_00001-10-7 loss: 0.369965  [  256/  265]
train() client id: f_00001-11-0 loss: 0.224363  [   32/  265]
train() client id: f_00001-11-1 loss: 0.277333  [   64/  265]
train() client id: f_00001-11-2 loss: 0.462388  [   96/  265]
train() client id: f_00001-11-3 loss: 0.258642  [  128/  265]
train() client id: f_00001-11-4 loss: 0.513349  [  160/  265]
train() client id: f_00001-11-5 loss: 0.379738  [  192/  265]
train() client id: f_00001-11-6 loss: 0.302391  [  224/  265]
train() client id: f_00001-11-7 loss: 0.249530  [  256/  265]
train() client id: f_00002-0-0 loss: 1.405022  [   32/  124]
train() client id: f_00002-0-1 loss: 1.280131  [   64/  124]
train() client id: f_00002-0-2 loss: 1.159754  [   96/  124]
train() client id: f_00002-1-0 loss: 1.138868  [   32/  124]
train() client id: f_00002-1-1 loss: 1.469634  [   64/  124]
train() client id: f_00002-1-2 loss: 1.215753  [   96/  124]
train() client id: f_00002-2-0 loss: 1.224239  [   32/  124]
train() client id: f_00002-2-1 loss: 1.115797  [   64/  124]
train() client id: f_00002-2-2 loss: 1.278467  [   96/  124]
train() client id: f_00002-3-0 loss: 1.126453  [   32/  124]
train() client id: f_00002-3-1 loss: 1.146679  [   64/  124]
train() client id: f_00002-3-2 loss: 1.279685  [   96/  124]
train() client id: f_00002-4-0 loss: 1.188061  [   32/  124]
train() client id: f_00002-4-1 loss: 1.216460  [   64/  124]
train() client id: f_00002-4-2 loss: 1.104524  [   96/  124]
train() client id: f_00002-5-0 loss: 1.143218  [   32/  124]
train() client id: f_00002-5-1 loss: 1.050851  [   64/  124]
train() client id: f_00002-5-2 loss: 1.167126  [   96/  124]
train() client id: f_00002-6-0 loss: 1.315255  [   32/  124]
train() client id: f_00002-6-1 loss: 1.247283  [   64/  124]
train() client id: f_00002-6-2 loss: 1.083714  [   96/  124]
train() client id: f_00002-7-0 loss: 1.283708  [   32/  124]
train() client id: f_00002-7-1 loss: 1.040064  [   64/  124]
train() client id: f_00002-7-2 loss: 1.091628  [   96/  124]
train() client id: f_00002-8-0 loss: 1.274591  [   32/  124]
train() client id: f_00002-8-1 loss: 1.113607  [   64/  124]
train() client id: f_00002-8-2 loss: 1.060591  [   96/  124]
train() client id: f_00002-9-0 loss: 1.091899  [   32/  124]
train() client id: f_00002-9-1 loss: 1.095138  [   64/  124]
train() client id: f_00002-9-2 loss: 1.269255  [   96/  124]
train() client id: f_00002-10-0 loss: 1.059060  [   32/  124]
train() client id: f_00002-10-1 loss: 1.227680  [   64/  124]
train() client id: f_00002-10-2 loss: 1.218754  [   96/  124]
train() client id: f_00002-11-0 loss: 1.150613  [   32/  124]
train() client id: f_00002-11-1 loss: 1.155938  [   64/  124]
train() client id: f_00002-11-2 loss: 1.199651  [   96/  124]
train() client id: f_00003-0-0 loss: 0.799427  [   32/   43]
train() client id: f_00003-1-0 loss: 0.530426  [   32/   43]
train() client id: f_00003-2-0 loss: 0.629141  [   32/   43]
train() client id: f_00003-3-0 loss: 0.644472  [   32/   43]
train() client id: f_00003-4-0 loss: 0.629874  [   32/   43]
train() client id: f_00003-5-0 loss: 0.654056  [   32/   43]
train() client id: f_00003-6-0 loss: 0.643661  [   32/   43]
train() client id: f_00003-7-0 loss: 0.608365  [   32/   43]
train() client id: f_00003-8-0 loss: 0.577472  [   32/   43]
train() client id: f_00003-9-0 loss: 0.602716  [   32/   43]
train() client id: f_00003-10-0 loss: 0.670832  [   32/   43]
train() client id: f_00003-11-0 loss: 0.545280  [   32/   43]
train() client id: f_00004-0-0 loss: 1.031222  [   32/  306]
train() client id: f_00004-0-1 loss: 1.012254  [   64/  306]
train() client id: f_00004-0-2 loss: 0.960773  [   96/  306]
train() client id: f_00004-0-3 loss: 0.936789  [  128/  306]
train() client id: f_00004-0-4 loss: 1.001781  [  160/  306]
train() client id: f_00004-0-5 loss: 0.931014  [  192/  306]
train() client id: f_00004-0-6 loss: 0.882235  [  224/  306]
train() client id: f_00004-0-7 loss: 0.987001  [  256/  306]
train() client id: f_00004-0-8 loss: 0.979540  [  288/  306]
train() client id: f_00004-1-0 loss: 1.063954  [   32/  306]
train() client id: f_00004-1-1 loss: 0.998616  [   64/  306]
train() client id: f_00004-1-2 loss: 0.991047  [   96/  306]
train() client id: f_00004-1-3 loss: 0.888290  [  128/  306]
train() client id: f_00004-1-4 loss: 0.924810  [  160/  306]
train() client id: f_00004-1-5 loss: 0.947010  [  192/  306]
train() client id: f_00004-1-6 loss: 0.895811  [  224/  306]
train() client id: f_00004-1-7 loss: 0.920713  [  256/  306]
train() client id: f_00004-1-8 loss: 0.969456  [  288/  306]
train() client id: f_00004-2-0 loss: 0.946945  [   32/  306]
train() client id: f_00004-2-1 loss: 0.923892  [   64/  306]
train() client id: f_00004-2-2 loss: 0.918305  [   96/  306]
train() client id: f_00004-2-3 loss: 1.006322  [  128/  306]
train() client id: f_00004-2-4 loss: 0.935777  [  160/  306]
train() client id: f_00004-2-5 loss: 0.886878  [  192/  306]
train() client id: f_00004-2-6 loss: 1.084110  [  224/  306]
train() client id: f_00004-2-7 loss: 0.839928  [  256/  306]
train() client id: f_00004-2-8 loss: 1.070956  [  288/  306]
train() client id: f_00004-3-0 loss: 1.006375  [   32/  306]
train() client id: f_00004-3-1 loss: 0.983379  [   64/  306]
train() client id: f_00004-3-2 loss: 1.007743  [   96/  306]
train() client id: f_00004-3-3 loss: 0.962262  [  128/  306]
train() client id: f_00004-3-4 loss: 1.053123  [  160/  306]
train() client id: f_00004-3-5 loss: 0.951858  [  192/  306]
train() client id: f_00004-3-6 loss: 0.883289  [  224/  306]
train() client id: f_00004-3-7 loss: 0.850238  [  256/  306]
train() client id: f_00004-3-8 loss: 0.890058  [  288/  306]
train() client id: f_00004-4-0 loss: 0.989553  [   32/  306]
train() client id: f_00004-4-1 loss: 0.846186  [   64/  306]
train() client id: f_00004-4-2 loss: 0.947103  [   96/  306]
train() client id: f_00004-4-3 loss: 0.903102  [  128/  306]
train() client id: f_00004-4-4 loss: 1.026838  [  160/  306]
train() client id: f_00004-4-5 loss: 0.893513  [  192/  306]
train() client id: f_00004-4-6 loss: 0.947765  [  224/  306]
train() client id: f_00004-4-7 loss: 1.023061  [  256/  306]
train() client id: f_00004-4-8 loss: 0.998229  [  288/  306]
train() client id: f_00004-5-0 loss: 0.928481  [   32/  306]
train() client id: f_00004-5-1 loss: 0.960081  [   64/  306]
train() client id: f_00004-5-2 loss: 1.031858  [   96/  306]
train() client id: f_00004-5-3 loss: 1.042512  [  128/  306]
train() client id: f_00004-5-4 loss: 1.003375  [  160/  306]
train() client id: f_00004-5-5 loss: 0.982069  [  192/  306]
train() client id: f_00004-5-6 loss: 0.874296  [  224/  306]
train() client id: f_00004-5-7 loss: 0.914198  [  256/  306]
train() client id: f_00004-5-8 loss: 0.785667  [  288/  306]
train() client id: f_00004-6-0 loss: 0.999888  [   32/  306]
train() client id: f_00004-6-1 loss: 0.987721  [   64/  306]
train() client id: f_00004-6-2 loss: 1.012533  [   96/  306]
train() client id: f_00004-6-3 loss: 0.785936  [  128/  306]
train() client id: f_00004-6-4 loss: 0.909339  [  160/  306]
train() client id: f_00004-6-5 loss: 0.988405  [  192/  306]
train() client id: f_00004-6-6 loss: 0.961021  [  224/  306]
train() client id: f_00004-6-7 loss: 0.902183  [  256/  306]
train() client id: f_00004-6-8 loss: 0.983479  [  288/  306]
train() client id: f_00004-7-0 loss: 0.920329  [   32/  306]
train() client id: f_00004-7-1 loss: 0.933147  [   64/  306]
train() client id: f_00004-7-2 loss: 0.984685  [   96/  306]
train() client id: f_00004-7-3 loss: 0.912377  [  128/  306]
train() client id: f_00004-7-4 loss: 0.907514  [  160/  306]
train() client id: f_00004-7-5 loss: 0.977646  [  192/  306]
train() client id: f_00004-7-6 loss: 0.885429  [  224/  306]
train() client id: f_00004-7-7 loss: 0.971889  [  256/  306]
train() client id: f_00004-7-8 loss: 1.073354  [  288/  306]
train() client id: f_00004-8-0 loss: 0.879675  [   32/  306]
train() client id: f_00004-8-1 loss: 0.882882  [   64/  306]
train() client id: f_00004-8-2 loss: 1.074766  [   96/  306]
train() client id: f_00004-8-3 loss: 0.924700  [  128/  306]
train() client id: f_00004-8-4 loss: 1.028513  [  160/  306]
train() client id: f_00004-8-5 loss: 0.961767  [  192/  306]
train() client id: f_00004-8-6 loss: 0.874232  [  224/  306]
train() client id: f_00004-8-7 loss: 0.913182  [  256/  306]
train() client id: f_00004-8-8 loss: 0.978548  [  288/  306]
train() client id: f_00004-9-0 loss: 0.945138  [   32/  306]
train() client id: f_00004-9-1 loss: 0.978962  [   64/  306]
train() client id: f_00004-9-2 loss: 0.929739  [   96/  306]
train() client id: f_00004-9-3 loss: 0.957593  [  128/  306]
train() client id: f_00004-9-4 loss: 0.938759  [  160/  306]
train() client id: f_00004-9-5 loss: 1.045804  [  192/  306]
train() client id: f_00004-9-6 loss: 1.050956  [  224/  306]
train() client id: f_00004-9-7 loss: 0.840780  [  256/  306]
train() client id: f_00004-9-8 loss: 0.796520  [  288/  306]
train() client id: f_00004-10-0 loss: 0.962326  [   32/  306]
train() client id: f_00004-10-1 loss: 0.902579  [   64/  306]
train() client id: f_00004-10-2 loss: 0.919647  [   96/  306]
train() client id: f_00004-10-3 loss: 0.909801  [  128/  306]
train() client id: f_00004-10-4 loss: 1.054431  [  160/  306]
train() client id: f_00004-10-5 loss: 0.865300  [  192/  306]
train() client id: f_00004-10-6 loss: 1.040334  [  224/  306]
train() client id: f_00004-10-7 loss: 0.864835  [  256/  306]
train() client id: f_00004-10-8 loss: 1.001014  [  288/  306]
train() client id: f_00004-11-0 loss: 0.888978  [   32/  306]
train() client id: f_00004-11-1 loss: 0.995201  [   64/  306]
train() client id: f_00004-11-2 loss: 0.948699  [   96/  306]
train() client id: f_00004-11-3 loss: 0.889391  [  128/  306]
train() client id: f_00004-11-4 loss: 1.017444  [  160/  306]
train() client id: f_00004-11-5 loss: 0.872023  [  192/  306]
train() client id: f_00004-11-6 loss: 0.884587  [  224/  306]
train() client id: f_00004-11-7 loss: 0.968244  [  256/  306]
train() client id: f_00004-11-8 loss: 0.957954  [  288/  306]
train() client id: f_00005-0-0 loss: 0.665937  [   32/  146]
train() client id: f_00005-0-1 loss: 0.919303  [   64/  146]
train() client id: f_00005-0-2 loss: 0.877876  [   96/  146]
train() client id: f_00005-0-3 loss: 0.911611  [  128/  146]
train() client id: f_00005-1-0 loss: 0.960502  [   32/  146]
train() client id: f_00005-1-1 loss: 0.736107  [   64/  146]
train() client id: f_00005-1-2 loss: 0.913130  [   96/  146]
train() client id: f_00005-1-3 loss: 0.879162  [  128/  146]
train() client id: f_00005-2-0 loss: 0.909721  [   32/  146]
train() client id: f_00005-2-1 loss: 0.940793  [   64/  146]
train() client id: f_00005-2-2 loss: 0.740044  [   96/  146]
train() client id: f_00005-2-3 loss: 0.957848  [  128/  146]
train() client id: f_00005-3-0 loss: 0.780058  [   32/  146]
train() client id: f_00005-3-1 loss: 0.841641  [   64/  146]
train() client id: f_00005-3-2 loss: 1.151761  [   96/  146]
train() client id: f_00005-3-3 loss: 0.728144  [  128/  146]
train() client id: f_00005-4-0 loss: 1.294961  [   32/  146]
train() client id: f_00005-4-1 loss: 0.826342  [   64/  146]
train() client id: f_00005-4-2 loss: 0.760270  [   96/  146]
train() client id: f_00005-4-3 loss: 0.673576  [  128/  146]
train() client id: f_00005-5-0 loss: 0.961390  [   32/  146]
train() client id: f_00005-5-1 loss: 0.874115  [   64/  146]
train() client id: f_00005-5-2 loss: 0.742887  [   96/  146]
train() client id: f_00005-5-3 loss: 0.738172  [  128/  146]
train() client id: f_00005-6-0 loss: 0.801029  [   32/  146]
train() client id: f_00005-6-1 loss: 1.025368  [   64/  146]
train() client id: f_00005-6-2 loss: 0.915653  [   96/  146]
train() client id: f_00005-6-3 loss: 0.846619  [  128/  146]
train() client id: f_00005-7-0 loss: 0.935346  [   32/  146]
train() client id: f_00005-7-1 loss: 0.969465  [   64/  146]
train() client id: f_00005-7-2 loss: 0.647161  [   96/  146]
train() client id: f_00005-7-3 loss: 0.869826  [  128/  146]
train() client id: f_00005-8-0 loss: 0.822675  [   32/  146]
train() client id: f_00005-8-1 loss: 0.967261  [   64/  146]
train() client id: f_00005-8-2 loss: 0.814562  [   96/  146]
train() client id: f_00005-8-3 loss: 0.829658  [  128/  146]
train() client id: f_00005-9-0 loss: 0.697088  [   32/  146]
train() client id: f_00005-9-1 loss: 1.039089  [   64/  146]
train() client id: f_00005-9-2 loss: 0.915967  [   96/  146]
train() client id: f_00005-9-3 loss: 0.951137  [  128/  146]
train() client id: f_00005-10-0 loss: 0.737708  [   32/  146]
train() client id: f_00005-10-1 loss: 1.008524  [   64/  146]
train() client id: f_00005-10-2 loss: 0.685917  [   96/  146]
train() client id: f_00005-10-3 loss: 1.045678  [  128/  146]
train() client id: f_00005-11-0 loss: 0.944137  [   32/  146]
train() client id: f_00005-11-1 loss: 0.808244  [   64/  146]
train() client id: f_00005-11-2 loss: 0.873397  [   96/  146]
train() client id: f_00005-11-3 loss: 0.963639  [  128/  146]
train() client id: f_00006-0-0 loss: 0.612695  [   32/   54]
train() client id: f_00006-1-0 loss: 0.554895  [   32/   54]
train() client id: f_00006-2-0 loss: 0.594517  [   32/   54]
train() client id: f_00006-3-0 loss: 0.542493  [   32/   54]
train() client id: f_00006-4-0 loss: 0.514829  [   32/   54]
train() client id: f_00006-5-0 loss: 0.598587  [   32/   54]
train() client id: f_00006-6-0 loss: 0.537679  [   32/   54]
train() client id: f_00006-7-0 loss: 0.556690  [   32/   54]
train() client id: f_00006-8-0 loss: 0.554748  [   32/   54]
train() client id: f_00006-9-0 loss: 0.611115  [   32/   54]
train() client id: f_00006-10-0 loss: 0.509850  [   32/   54]
train() client id: f_00006-11-0 loss: 0.595299  [   32/   54]
train() client id: f_00007-0-0 loss: 0.436570  [   32/  179]
train() client id: f_00007-0-1 loss: 0.729919  [   64/  179]
train() client id: f_00007-0-2 loss: 0.419827  [   96/  179]
train() client id: f_00007-0-3 loss: 0.484324  [  128/  179]
train() client id: f_00007-0-4 loss: 0.640481  [  160/  179]
train() client id: f_00007-1-0 loss: 0.583262  [   32/  179]
train() client id: f_00007-1-1 loss: 0.555572  [   64/  179]
train() client id: f_00007-1-2 loss: 0.598073  [   96/  179]
train() client id: f_00007-1-3 loss: 0.548493  [  128/  179]
train() client id: f_00007-1-4 loss: 0.397969  [  160/  179]
train() client id: f_00007-2-0 loss: 0.627506  [   32/  179]
train() client id: f_00007-2-1 loss: 0.679094  [   64/  179]
train() client id: f_00007-2-2 loss: 0.417131  [   96/  179]
train() client id: f_00007-2-3 loss: 0.401048  [  128/  179]
train() client id: f_00007-2-4 loss: 0.460288  [  160/  179]
train() client id: f_00007-3-0 loss: 0.620577  [   32/  179]
train() client id: f_00007-3-1 loss: 0.738394  [   64/  179]
train() client id: f_00007-3-2 loss: 0.480443  [   96/  179]
train() client id: f_00007-3-3 loss: 0.433719  [  128/  179]
train() client id: f_00007-3-4 loss: 0.354083  [  160/  179]
train() client id: f_00007-4-0 loss: 0.520581  [   32/  179]
train() client id: f_00007-4-1 loss: 0.608946  [   64/  179]
train() client id: f_00007-4-2 loss: 0.410585  [   96/  179]
train() client id: f_00007-4-3 loss: 0.642926  [  128/  179]
train() client id: f_00007-4-4 loss: 0.350842  [  160/  179]
train() client id: f_00007-5-0 loss: 0.464027  [   32/  179]
train() client id: f_00007-5-1 loss: 0.551237  [   64/  179]
train() client id: f_00007-5-2 loss: 0.341396  [   96/  179]
train() client id: f_00007-5-3 loss: 0.670032  [  128/  179]
train() client id: f_00007-5-4 loss: 0.428317  [  160/  179]
train() client id: f_00007-6-0 loss: 0.436126  [   32/  179]
train() client id: f_00007-6-1 loss: 0.455440  [   64/  179]
train() client id: f_00007-6-2 loss: 0.431350  [   96/  179]
train() client id: f_00007-6-3 loss: 0.659075  [  128/  179]
train() client id: f_00007-6-4 loss: 0.358032  [  160/  179]
train() client id: f_00007-7-0 loss: 0.582635  [   32/  179]
train() client id: f_00007-7-1 loss: 0.490598  [   64/  179]
train() client id: f_00007-7-2 loss: 0.438199  [   96/  179]
train() client id: f_00007-7-3 loss: 0.476889  [  128/  179]
train() client id: f_00007-7-4 loss: 0.425353  [  160/  179]
train() client id: f_00007-8-0 loss: 0.569129  [   32/  179]
train() client id: f_00007-8-1 loss: 0.430181  [   64/  179]
train() client id: f_00007-8-2 loss: 0.455060  [   96/  179]
train() client id: f_00007-8-3 loss: 0.409616  [  128/  179]
train() client id: f_00007-8-4 loss: 0.538406  [  160/  179]
train() client id: f_00007-9-0 loss: 0.419741  [   32/  179]
train() client id: f_00007-9-1 loss: 0.441492  [   64/  179]
train() client id: f_00007-9-2 loss: 0.327973  [   96/  179]
train() client id: f_00007-9-3 loss: 0.692023  [  128/  179]
train() client id: f_00007-9-4 loss: 0.516473  [  160/  179]
train() client id: f_00007-10-0 loss: 0.322720  [   32/  179]
train() client id: f_00007-10-1 loss: 0.448598  [   64/  179]
train() client id: f_00007-10-2 loss: 0.688178  [   96/  179]
train() client id: f_00007-10-3 loss: 0.323595  [  128/  179]
train() client id: f_00007-10-4 loss: 0.610989  [  160/  179]
train() client id: f_00007-11-0 loss: 0.521096  [   32/  179]
train() client id: f_00007-11-1 loss: 0.563047  [   64/  179]
train() client id: f_00007-11-2 loss: 0.605597  [   96/  179]
train() client id: f_00007-11-3 loss: 0.327816  [  128/  179]
train() client id: f_00007-11-4 loss: 0.323445  [  160/  179]
train() client id: f_00008-0-0 loss: 0.637224  [   32/  130]
train() client id: f_00008-0-1 loss: 0.851452  [   64/  130]
train() client id: f_00008-0-2 loss: 0.745328  [   96/  130]
train() client id: f_00008-0-3 loss: 0.768477  [  128/  130]
train() client id: f_00008-1-0 loss: 0.818378  [   32/  130]
train() client id: f_00008-1-1 loss: 0.710026  [   64/  130]
train() client id: f_00008-1-2 loss: 0.701426  [   96/  130]
train() client id: f_00008-1-3 loss: 0.795109  [  128/  130]
train() client id: f_00008-2-0 loss: 0.704534  [   32/  130]
train() client id: f_00008-2-1 loss: 0.655415  [   64/  130]
train() client id: f_00008-2-2 loss: 0.805863  [   96/  130]
train() client id: f_00008-2-3 loss: 0.820305  [  128/  130]
train() client id: f_00008-3-0 loss: 0.721815  [   32/  130]
train() client id: f_00008-3-1 loss: 0.861411  [   64/  130]
train() client id: f_00008-3-2 loss: 0.667560  [   96/  130]
train() client id: f_00008-3-3 loss: 0.777933  [  128/  130]
train() client id: f_00008-4-0 loss: 0.659570  [   32/  130]
train() client id: f_00008-4-1 loss: 0.753726  [   64/  130]
train() client id: f_00008-4-2 loss: 0.840639  [   96/  130]
train() client id: f_00008-4-3 loss: 0.761492  [  128/  130]
train() client id: f_00008-5-0 loss: 0.657664  [   32/  130]
train() client id: f_00008-5-1 loss: 0.686587  [   64/  130]
train() client id: f_00008-5-2 loss: 0.863918  [   96/  130]
train() client id: f_00008-5-3 loss: 0.804861  [  128/  130]
train() client id: f_00008-6-0 loss: 0.931365  [   32/  130]
train() client id: f_00008-6-1 loss: 0.723029  [   64/  130]
train() client id: f_00008-6-2 loss: 0.751605  [   96/  130]
train() client id: f_00008-6-3 loss: 0.610322  [  128/  130]
train() client id: f_00008-7-0 loss: 0.701396  [   32/  130]
train() client id: f_00008-7-1 loss: 0.724169  [   64/  130]
train() client id: f_00008-7-2 loss: 0.772676  [   96/  130]
train() client id: f_00008-7-3 loss: 0.822421  [  128/  130]
train() client id: f_00008-8-0 loss: 0.744682  [   32/  130]
train() client id: f_00008-8-1 loss: 0.722184  [   64/  130]
train() client id: f_00008-8-2 loss: 0.741058  [   96/  130]
train() client id: f_00008-8-3 loss: 0.809069  [  128/  130]
train() client id: f_00008-9-0 loss: 0.585744  [   32/  130]
train() client id: f_00008-9-1 loss: 0.915152  [   64/  130]
train() client id: f_00008-9-2 loss: 0.762249  [   96/  130]
train() client id: f_00008-9-3 loss: 0.757638  [  128/  130]
train() client id: f_00008-10-0 loss: 0.760485  [   32/  130]
train() client id: f_00008-10-1 loss: 0.700125  [   64/  130]
train() client id: f_00008-10-2 loss: 0.744599  [   96/  130]
train() client id: f_00008-10-3 loss: 0.763450  [  128/  130]
train() client id: f_00008-11-0 loss: 0.865474  [   32/  130]
train() client id: f_00008-11-1 loss: 0.763887  [   64/  130]
train() client id: f_00008-11-2 loss: 0.651508  [   96/  130]
train() client id: f_00008-11-3 loss: 0.710596  [  128/  130]
train() client id: f_00009-0-0 loss: 1.185434  [   32/  118]
train() client id: f_00009-0-1 loss: 1.090311  [   64/  118]
train() client id: f_00009-0-2 loss: 1.140186  [   96/  118]
train() client id: f_00009-1-0 loss: 1.154754  [   32/  118]
train() client id: f_00009-1-1 loss: 1.061315  [   64/  118]
train() client id: f_00009-1-2 loss: 1.012050  [   96/  118]
train() client id: f_00009-2-0 loss: 0.990579  [   32/  118]
train() client id: f_00009-2-1 loss: 1.025567  [   64/  118]
train() client id: f_00009-2-2 loss: 1.212757  [   96/  118]
train() client id: f_00009-3-0 loss: 1.120218  [   32/  118]
train() client id: f_00009-3-1 loss: 0.981428  [   64/  118]
train() client id: f_00009-3-2 loss: 0.995764  [   96/  118]
train() client id: f_00009-4-0 loss: 1.101287  [   32/  118]
train() client id: f_00009-4-1 loss: 0.972401  [   64/  118]
train() client id: f_00009-4-2 loss: 0.962143  [   96/  118]
train() client id: f_00009-5-0 loss: 0.966232  [   32/  118]
train() client id: f_00009-5-1 loss: 1.003408  [   64/  118]
train() client id: f_00009-5-2 loss: 1.000133  [   96/  118]
train() client id: f_00009-6-0 loss: 0.980700  [   32/  118]
train() client id: f_00009-6-1 loss: 1.036564  [   64/  118]
train() client id: f_00009-6-2 loss: 0.887349  [   96/  118]
train() client id: f_00009-7-0 loss: 0.979571  [   32/  118]
train() client id: f_00009-7-1 loss: 0.988307  [   64/  118]
train() client id: f_00009-7-2 loss: 0.950322  [   96/  118]
train() client id: f_00009-8-0 loss: 0.820704  [   32/  118]
train() client id: f_00009-8-1 loss: 0.919433  [   64/  118]
train() client id: f_00009-8-2 loss: 1.035470  [   96/  118]
train() client id: f_00009-9-0 loss: 0.925537  [   32/  118]
train() client id: f_00009-9-1 loss: 1.050907  [   64/  118]
train() client id: f_00009-9-2 loss: 0.858883  [   96/  118]
train() client id: f_00009-10-0 loss: 0.774416  [   32/  118]
train() client id: f_00009-10-1 loss: 0.891275  [   64/  118]
train() client id: f_00009-10-2 loss: 1.151002  [   96/  118]
train() client id: f_00009-11-0 loss: 0.882254  [   32/  118]
train() client id: f_00009-11-1 loss: 0.806640  [   64/  118]
train() client id: f_00009-11-2 loss: 1.072875  [   96/  118]
At round 24 accuracy: 0.6445623342175066
At round 24 training accuracy: 0.5801475519785378
At round 24 training loss: 0.8389492315760522
update_location
xs = -3.905658 4.200318 140.009024 18.811294 0.979296 3.956410 -102.443192 -81.324852 124.663977 -67.060879 
ys = 132.587959 115.555839 1.320614 -102.455176 94.350187 77.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 166.116890 152.875095 172.058917 144.398504 137.487879 126.770246 143.183442 128.896888 160.778659 120.470632 
dists_bs = 176.272672 188.802442 359.715463 338.443780 193.579712 203.643036 191.872680 197.769785 338.501133 202.216090 
uav_gains = -105.527368 -104.614262 -105.918350 -103.991810 -103.458024 -102.575895 -103.899768 -102.756643 -105.166945 -102.022256 
bs_gains = -102.460239 -103.295272 -111.133914 -110.392682 -103.599135 -104.215407 -103.491427 -103.859538 -110.394743 -104.129899 
Round 25
-------------------------------
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.69254516 15.98293133  7.57645092  2.71961017 18.43551744  8.87729699
  3.37637101 10.83932141  7.98736814  7.20317358]
obj_prev = 90.69058614039777
eta_min = 8.689629583387306e-13	eta_max = 0.9239062523538892
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 21.07280247978385	eta = 0.909090909090909
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 37.266591851058905	eta = 0.5140554102721235
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 29.4469781495679	eta = 0.65056227726107
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 28.03930224522773	eta = 0.683222891778642
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 27.967919357716607	eta = 0.6849666905290988
af = 19.15709316343986	bf = 1.5620059925242489	zeta = 27.967721963314986	eta = 0.684971524980406
eta = 0.684971524980406
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [0.03122285 0.0656671  0.03072725 0.01065542 0.07582691 0.03617885
 0.01338122 0.04435627 0.03221405 0.02924044]
ene_total = [2.45319609 4.54407619 2.43314225 1.13378689 5.18350126 2.72855748
 1.30141153 3.20547972 2.69013729 2.29443326]
ti_comp = [0.37995732 0.38865987 0.37822142 0.38614518 0.38758329 0.38530164
 0.38648714 0.39049469 0.35182285 0.38562639]
ti_coms = [0.08168984 0.07298729 0.08342574 0.07550198 0.07406386 0.07634552
 0.07516002 0.07115247 0.10982431 0.07602077]
t_total = [28.7498951 28.7498951 28.7498951 28.7498951 28.7498951 28.7498951
 28.7498951 28.7498951 28.7498951 28.7498951]
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [1.31773438e-05 1.17161273e-04 1.26753124e-05 5.07095790e-07
 1.81392801e-04 1.99362151e-05 1.00252924e-06 3.57696506e-05
 1.68798211e-05 1.05074756e-05]
ene_total = [0.50955862 0.46183769 0.52033793 0.47023356 0.47254241 0.47669679
 0.46813477 0.44534224 0.68500157 0.47408718]
optimize_network iter = 0 obj = 4.983772760330714
eta = 0.684971524980406
freqs = [41087307.98029104 84478875.62799275 40620710.39935426 13797163.01214605
 97820151.49584162 46948740.71955757 17311339.16633044 56794967.35235866
 45781631.37350148 37912918.15352243]
eta_min = 0.6849715249804152	eta_max = 0.68497152498039
af = 0.02120720489719426	bf = 1.5620059925242489	zeta = 0.023327925386913688	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [3.31926004e-06 2.95119211e-05 3.19280262e-06 1.27733086e-07
 4.56912927e-05 5.02176180e-06 2.52528528e-07 9.01006858e-06
 4.25188234e-06 2.64674311e-06]
ene_total = [1.76365275 1.58149679 1.80108751 1.6294247  1.6082219  1.64868503
 1.62207175 1.53747541 2.37101979 1.64116416]
ti_comp = [0.37995732 0.38865987 0.37822142 0.38614518 0.38758329 0.38530164
 0.38648714 0.39049469 0.35182285 0.38562639]
ti_coms = [0.08168984 0.07298729 0.08342574 0.07550198 0.07406386 0.07634552
 0.07516002 0.07115247 0.10982431 0.07602077]
t_total = [28.7498951 28.7498951 28.7498951 28.7498951 28.7498951 28.7498951
 28.7498951 28.7498951 28.7498951 28.7498951]
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [1.31773438e-05 1.17161273e-04 1.26753124e-05 5.07095790e-07
 1.81392801e-04 1.99362151e-05 1.00252924e-06 3.57696506e-05
 1.68798211e-05 1.05074756e-05]
ene_total = [0.50955862 0.46183769 0.52033793 0.47023356 0.47254241 0.47669679
 0.46813477 0.44534224 0.68500157 0.47408718]
optimize_network iter = 1 obj = 4.983772760330463
eta = 0.68497152498039
freqs = [41087307.98029105 84478875.62799287 40620710.39935425 13797163.01214607
 97820151.49584176 46948740.71955761 17311339.16633046 56794967.35235876
 45781631.37350126 37912918.15352247]
Done!
ene_coms = [0.00816898 0.00729873 0.00834257 0.0075502  0.00740639 0.00763455
 0.007516   0.00711525 0.01098243 0.00760208]
ene_comp = [1.27625416e-05 1.13473219e-04 1.22763133e-05 4.91133205e-07
 1.75682838e-04 1.93086541e-05 9.70971182e-07 3.46436777e-05
 1.63484706e-05 1.01767167e-05]
ene_total = [0.00818175 0.0074122  0.00835485 0.00755069 0.00758207 0.00765386
 0.00751697 0.00714989 0.01099878 0.00761225]
At round 25 energy consumption: 0.08001331387073954
At round 25 eta: 0.68497152498039
At round 25 a_n: 19.61895668202144
At round 25 local rounds: 12.390018469521971
At round 25 global rounds: 62.276772538737
gradient difference: 0.35778430104255676
train() client id: f_00000-0-0 loss: 1.081196  [   32/  126]
train() client id: f_00000-0-1 loss: 1.293680  [   64/  126]
train() client id: f_00000-0-2 loss: 1.119100  [   96/  126]
train() client id: f_00000-1-0 loss: 1.183725  [   32/  126]
train() client id: f_00000-1-1 loss: 1.101423  [   64/  126]
train() client id: f_00000-1-2 loss: 1.002549  [   96/  126]
train() client id: f_00000-2-0 loss: 0.981387  [   32/  126]
train() client id: f_00000-2-1 loss: 0.915640  [   64/  126]
train() client id: f_00000-2-2 loss: 1.035751  [   96/  126]
train() client id: f_00000-3-0 loss: 0.916111  [   32/  126]
train() client id: f_00000-3-1 loss: 0.991244  [   64/  126]
train() client id: f_00000-3-2 loss: 0.853563  [   96/  126]
train() client id: f_00000-4-0 loss: 1.003135  [   32/  126]
train() client id: f_00000-4-1 loss: 0.892447  [   64/  126]
train() client id: f_00000-4-2 loss: 0.843077  [   96/  126]
train() client id: f_00000-5-0 loss: 0.869817  [   32/  126]
train() client id: f_00000-5-1 loss: 0.852633  [   64/  126]
train() client id: f_00000-5-2 loss: 0.870623  [   96/  126]
train() client id: f_00000-6-0 loss: 0.861248  [   32/  126]
train() client id: f_00000-6-1 loss: 0.797018  [   64/  126]
train() client id: f_00000-6-2 loss: 0.832608  [   96/  126]
train() client id: f_00000-7-0 loss: 0.836350  [   32/  126]
train() client id: f_00000-7-1 loss: 0.842764  [   64/  126]
train() client id: f_00000-7-2 loss: 0.797576  [   96/  126]
train() client id: f_00000-8-0 loss: 0.821231  [   32/  126]
train() client id: f_00000-8-1 loss: 0.807833  [   64/  126]
train() client id: f_00000-8-2 loss: 0.740583  [   96/  126]
train() client id: f_00000-9-0 loss: 0.861736  [   32/  126]
train() client id: f_00000-9-1 loss: 0.683378  [   64/  126]
train() client id: f_00000-9-2 loss: 0.845884  [   96/  126]
train() client id: f_00000-10-0 loss: 0.769383  [   32/  126]
train() client id: f_00000-10-1 loss: 0.778750  [   64/  126]
train() client id: f_00000-10-2 loss: 0.876684  [   96/  126]
train() client id: f_00000-11-0 loss: 0.739725  [   32/  126]
train() client id: f_00000-11-1 loss: 0.843965  [   64/  126]
train() client id: f_00000-11-2 loss: 0.795876  [   96/  126]
train() client id: f_00001-0-0 loss: 0.457268  [   32/  265]
train() client id: f_00001-0-1 loss: 0.445866  [   64/  265]
train() client id: f_00001-0-2 loss: 0.582619  [   96/  265]
train() client id: f_00001-0-3 loss: 0.657225  [  128/  265]
train() client id: f_00001-0-4 loss: 0.580192  [  160/  265]
train() client id: f_00001-0-5 loss: 0.571201  [  192/  265]
train() client id: f_00001-0-6 loss: 0.441730  [  224/  265]
train() client id: f_00001-0-7 loss: 0.572673  [  256/  265]
train() client id: f_00001-1-0 loss: 0.482863  [   32/  265]
train() client id: f_00001-1-1 loss: 0.448664  [   64/  265]
train() client id: f_00001-1-2 loss: 0.699436  [   96/  265]
train() client id: f_00001-1-3 loss: 0.525225  [  128/  265]
train() client id: f_00001-1-4 loss: 0.499009  [  160/  265]
train() client id: f_00001-1-5 loss: 0.426350  [  192/  265]
train() client id: f_00001-1-6 loss: 0.631756  [  224/  265]
train() client id: f_00001-1-7 loss: 0.495037  [  256/  265]
train() client id: f_00001-2-0 loss: 0.507021  [   32/  265]
train() client id: f_00001-2-1 loss: 0.628847  [   64/  265]
train() client id: f_00001-2-2 loss: 0.445134  [   96/  265]
train() client id: f_00001-2-3 loss: 0.426434  [  128/  265]
train() client id: f_00001-2-4 loss: 0.557364  [  160/  265]
train() client id: f_00001-2-5 loss: 0.473162  [  192/  265]
train() client id: f_00001-2-6 loss: 0.550889  [  224/  265]
train() client id: f_00001-2-7 loss: 0.561733  [  256/  265]
train() client id: f_00001-3-0 loss: 0.546695  [   32/  265]
train() client id: f_00001-3-1 loss: 0.436040  [   64/  265]
train() client id: f_00001-3-2 loss: 0.508891  [   96/  265]
train() client id: f_00001-3-3 loss: 0.482851  [  128/  265]
train() client id: f_00001-3-4 loss: 0.724681  [  160/  265]
train() client id: f_00001-3-5 loss: 0.511193  [  192/  265]
train() client id: f_00001-3-6 loss: 0.420813  [  224/  265]
train() client id: f_00001-3-7 loss: 0.510528  [  256/  265]
train() client id: f_00001-4-0 loss: 0.651703  [   32/  265]
train() client id: f_00001-4-1 loss: 0.469307  [   64/  265]
train() client id: f_00001-4-2 loss: 0.510098  [   96/  265]
train() client id: f_00001-4-3 loss: 0.629998  [  128/  265]
train() client id: f_00001-4-4 loss: 0.503684  [  160/  265]
train() client id: f_00001-4-5 loss: 0.438819  [  192/  265]
train() client id: f_00001-4-6 loss: 0.448993  [  224/  265]
train() client id: f_00001-4-7 loss: 0.465839  [  256/  265]
train() client id: f_00001-5-0 loss: 0.432635  [   32/  265]
train() client id: f_00001-5-1 loss: 0.537571  [   64/  265]
train() client id: f_00001-5-2 loss: 0.548612  [   96/  265]
train() client id: f_00001-5-3 loss: 0.597485  [  128/  265]
train() client id: f_00001-5-4 loss: 0.425138  [  160/  265]
train() client id: f_00001-5-5 loss: 0.589028  [  192/  265]
train() client id: f_00001-5-6 loss: 0.589083  [  224/  265]
train() client id: f_00001-5-7 loss: 0.430317  [  256/  265]
train() client id: f_00001-6-0 loss: 0.551973  [   32/  265]
train() client id: f_00001-6-1 loss: 0.617162  [   64/  265]
train() client id: f_00001-6-2 loss: 0.537716  [   96/  265]
train() client id: f_00001-6-3 loss: 0.569912  [  128/  265]
train() client id: f_00001-6-4 loss: 0.462097  [  160/  265]
train() client id: f_00001-6-5 loss: 0.448921  [  192/  265]
train() client id: f_00001-6-6 loss: 0.417037  [  224/  265]
train() client id: f_00001-6-7 loss: 0.534485  [  256/  265]
train() client id: f_00001-7-0 loss: 0.691731  [   32/  265]
train() client id: f_00001-7-1 loss: 0.461463  [   64/  265]
train() client id: f_00001-7-2 loss: 0.534126  [   96/  265]
train() client id: f_00001-7-3 loss: 0.424228  [  128/  265]
train() client id: f_00001-7-4 loss: 0.540875  [  160/  265]
train() client id: f_00001-7-5 loss: 0.442006  [  192/  265]
train() client id: f_00001-7-6 loss: 0.461209  [  224/  265]
train() client id: f_00001-7-7 loss: 0.417288  [  256/  265]
train() client id: f_00001-8-0 loss: 0.522012  [   32/  265]
train() client id: f_00001-8-1 loss: 0.542636  [   64/  265]
train() client id: f_00001-8-2 loss: 0.488236  [   96/  265]
train() client id: f_00001-8-3 loss: 0.454172  [  128/  265]
train() client id: f_00001-8-4 loss: 0.601759  [  160/  265]
train() client id: f_00001-8-5 loss: 0.508612  [  192/  265]
train() client id: f_00001-8-6 loss: 0.505425  [  224/  265]
train() client id: f_00001-8-7 loss: 0.523350  [  256/  265]
train() client id: f_00001-9-0 loss: 0.413852  [   32/  265]
train() client id: f_00001-9-1 loss: 0.691820  [   64/  265]
train() client id: f_00001-9-2 loss: 0.440073  [   96/  265]
train() client id: f_00001-9-3 loss: 0.424508  [  128/  265]
train() client id: f_00001-9-4 loss: 0.475437  [  160/  265]
train() client id: f_00001-9-5 loss: 0.604550  [  192/  265]
train() client id: f_00001-9-6 loss: 0.551418  [  224/  265]
train() client id: f_00001-9-7 loss: 0.548933  [  256/  265]
train() client id: f_00001-10-0 loss: 0.578995  [   32/  265]
train() client id: f_00001-10-1 loss: 0.557512  [   64/  265]
train() client id: f_00001-10-2 loss: 0.441520  [   96/  265]
train() client id: f_00001-10-3 loss: 0.544019  [  128/  265]
train() client id: f_00001-10-4 loss: 0.569589  [  160/  265]
train() client id: f_00001-10-5 loss: 0.471973  [  192/  265]
train() client id: f_00001-10-6 loss: 0.472943  [  224/  265]
train() client id: f_00001-10-7 loss: 0.512842  [  256/  265]
train() client id: f_00001-11-0 loss: 0.467896  [   32/  265]
train() client id: f_00001-11-1 loss: 0.459231  [   64/  265]
train() client id: f_00001-11-2 loss: 0.762863  [   96/  265]
train() client id: f_00001-11-3 loss: 0.635439  [  128/  265]
train() client id: f_00001-11-4 loss: 0.505672  [  160/  265]
train() client id: f_00001-11-5 loss: 0.420350  [  192/  265]
train() client id: f_00001-11-6 loss: 0.424234  [  224/  265]
train() client id: f_00001-11-7 loss: 0.476863  [  256/  265]
train() client id: f_00002-0-0 loss: 1.427701  [   32/  124]
train() client id: f_00002-0-1 loss: 1.301138  [   64/  124]
train() client id: f_00002-0-2 loss: 1.036560  [   96/  124]
train() client id: f_00002-1-0 loss: 1.253256  [   32/  124]
train() client id: f_00002-1-1 loss: 1.109500  [   64/  124]
train() client id: f_00002-1-2 loss: 1.245041  [   96/  124]
train() client id: f_00002-2-0 loss: 1.241980  [   32/  124]
train() client id: f_00002-2-1 loss: 1.097524  [   64/  124]
train() client id: f_00002-2-2 loss: 1.143587  [   96/  124]
train() client id: f_00002-3-0 loss: 1.048868  [   32/  124]
train() client id: f_00002-3-1 loss: 1.139947  [   64/  124]
train() client id: f_00002-3-2 loss: 1.181304  [   96/  124]
train() client id: f_00002-4-0 loss: 1.187712  [   32/  124]
train() client id: f_00002-4-1 loss: 0.985371  [   64/  124]
train() client id: f_00002-4-2 loss: 1.029462  [   96/  124]
train() client id: f_00002-5-0 loss: 1.074984  [   32/  124]
train() client id: f_00002-5-1 loss: 1.128750  [   64/  124]
train() client id: f_00002-5-2 loss: 1.040441  [   96/  124]
train() client id: f_00002-6-0 loss: 0.973932  [   32/  124]
train() client id: f_00002-6-1 loss: 1.006995  [   64/  124]
train() client id: f_00002-6-2 loss: 1.164623  [   96/  124]
train() client id: f_00002-7-0 loss: 1.082114  [   32/  124]
train() client id: f_00002-7-1 loss: 0.982315  [   64/  124]
train() client id: f_00002-7-2 loss: 0.935996  [   96/  124]
train() client id: f_00002-8-0 loss: 1.045982  [   32/  124]
train() client id: f_00002-8-1 loss: 0.942416  [   64/  124]
train() client id: f_00002-8-2 loss: 0.838704  [   96/  124]
train() client id: f_00002-9-0 loss: 0.963183  [   32/  124]
train() client id: f_00002-9-1 loss: 1.033798  [   64/  124]
train() client id: f_00002-9-2 loss: 0.959871  [   96/  124]
train() client id: f_00002-10-0 loss: 0.855054  [   32/  124]
train() client id: f_00002-10-1 loss: 1.013344  [   64/  124]
train() client id: f_00002-10-2 loss: 0.931925  [   96/  124]
train() client id: f_00002-11-0 loss: 0.890615  [   32/  124]
train() client id: f_00002-11-1 loss: 0.933964  [   64/  124]
train() client id: f_00002-11-2 loss: 1.022668  [   96/  124]
train() client id: f_00003-0-0 loss: 0.779382  [   32/   43]
train() client id: f_00003-1-0 loss: 0.963404  [   32/   43]
train() client id: f_00003-2-0 loss: 1.120194  [   32/   43]
train() client id: f_00003-3-0 loss: 0.967456  [   32/   43]
train() client id: f_00003-4-0 loss: 0.855978  [   32/   43]
train() client id: f_00003-5-0 loss: 0.851981  [   32/   43]
train() client id: f_00003-6-0 loss: 0.873467  [   32/   43]
train() client id: f_00003-7-0 loss: 0.991443  [   32/   43]
train() client id: f_00003-8-0 loss: 0.943890  [   32/   43]
train() client id: f_00003-9-0 loss: 0.819691  [   32/   43]
train() client id: f_00003-10-0 loss: 0.719701  [   32/   43]
train() client id: f_00003-11-0 loss: 0.818439  [   32/   43]
train() client id: f_00004-0-0 loss: 0.865645  [   32/  306]
train() client id: f_00004-0-1 loss: 0.946235  [   64/  306]
train() client id: f_00004-0-2 loss: 0.853789  [   96/  306]
train() client id: f_00004-0-3 loss: 0.803316  [  128/  306]
train() client id: f_00004-0-4 loss: 0.929777  [  160/  306]
train() client id: f_00004-0-5 loss: 0.914089  [  192/  306]
train() client id: f_00004-0-6 loss: 0.780382  [  224/  306]
train() client id: f_00004-0-7 loss: 0.719540  [  256/  306]
train() client id: f_00004-0-8 loss: 0.861588  [  288/  306]
train() client id: f_00004-1-0 loss: 0.918998  [   32/  306]
train() client id: f_00004-1-1 loss: 0.839727  [   64/  306]
train() client id: f_00004-1-2 loss: 0.821478  [   96/  306]
train() client id: f_00004-1-3 loss: 0.894766  [  128/  306]
train() client id: f_00004-1-4 loss: 0.893333  [  160/  306]
train() client id: f_00004-1-5 loss: 0.716183  [  192/  306]
train() client id: f_00004-1-6 loss: 0.927248  [  224/  306]
train() client id: f_00004-1-7 loss: 0.919888  [  256/  306]
train() client id: f_00004-1-8 loss: 0.873223  [  288/  306]
train() client id: f_00004-2-0 loss: 0.836187  [   32/  306]
train() client id: f_00004-2-1 loss: 0.851363  [   64/  306]
train() client id: f_00004-2-2 loss: 0.762712  [   96/  306]
train() client id: f_00004-2-3 loss: 0.834552  [  128/  306]
train() client id: f_00004-2-4 loss: 0.900033  [  160/  306]
train() client id: f_00004-2-5 loss: 0.883158  [  192/  306]
train() client id: f_00004-2-6 loss: 0.832371  [  224/  306]
train() client id: f_00004-2-7 loss: 0.849025  [  256/  306]
train() client id: f_00004-2-8 loss: 0.925736  [  288/  306]
train() client id: f_00004-3-0 loss: 0.894431  [   32/  306]
train() client id: f_00004-3-1 loss: 0.789944  [   64/  306]
train() client id: f_00004-3-2 loss: 0.758208  [   96/  306]
train() client id: f_00004-3-3 loss: 0.929114  [  128/  306]
train() client id: f_00004-3-4 loss: 0.914845  [  160/  306]
train() client id: f_00004-3-5 loss: 0.876259  [  192/  306]
train() client id: f_00004-3-6 loss: 0.815549  [  224/  306]
train() client id: f_00004-3-7 loss: 0.946339  [  256/  306]
train() client id: f_00004-3-8 loss: 0.815162  [  288/  306]
train() client id: f_00004-4-0 loss: 0.858276  [   32/  306]
train() client id: f_00004-4-1 loss: 0.833645  [   64/  306]
train() client id: f_00004-4-2 loss: 0.871812  [   96/  306]
train() client id: f_00004-4-3 loss: 0.810987  [  128/  306]
train() client id: f_00004-4-4 loss: 0.860492  [  160/  306]
train() client id: f_00004-4-5 loss: 0.919216  [  192/  306]
train() client id: f_00004-4-6 loss: 0.971145  [  224/  306]
train() client id: f_00004-4-7 loss: 0.891537  [  256/  306]
train() client id: f_00004-4-8 loss: 0.721702  [  288/  306]
train() client id: f_00004-5-0 loss: 0.709630  [   32/  306]
train() client id: f_00004-5-1 loss: 0.755560  [   64/  306]
train() client id: f_00004-5-2 loss: 0.880992  [   96/  306]
train() client id: f_00004-5-3 loss: 0.923636  [  128/  306]
train() client id: f_00004-5-4 loss: 0.981369  [  160/  306]
train() client id: f_00004-5-5 loss: 0.875466  [  192/  306]
train() client id: f_00004-5-6 loss: 0.750636  [  224/  306]
train() client id: f_00004-5-7 loss: 0.850455  [  256/  306]
train() client id: f_00004-5-8 loss: 0.932020  [  288/  306]
train() client id: f_00004-6-0 loss: 0.896774  [   32/  306]
train() client id: f_00004-6-1 loss: 0.903977  [   64/  306]
train() client id: f_00004-6-2 loss: 0.837695  [   96/  306]
train() client id: f_00004-6-3 loss: 0.963535  [  128/  306]
train() client id: f_00004-6-4 loss: 0.822413  [  160/  306]
train() client id: f_00004-6-5 loss: 0.733781  [  192/  306]
train() client id: f_00004-6-6 loss: 0.920418  [  224/  306]
train() client id: f_00004-6-7 loss: 0.798624  [  256/  306]
train() client id: f_00004-6-8 loss: 0.789850  [  288/  306]
train() client id: f_00004-7-0 loss: 0.943066  [   32/  306]
train() client id: f_00004-7-1 loss: 0.835459  [   64/  306]
train() client id: f_00004-7-2 loss: 0.816631  [   96/  306]
train() client id: f_00004-7-3 loss: 0.727792  [  128/  306]
train() client id: f_00004-7-4 loss: 0.942243  [  160/  306]
train() client id: f_00004-7-5 loss: 0.872935  [  192/  306]
train() client id: f_00004-7-6 loss: 1.032479  [  224/  306]
train() client id: f_00004-7-7 loss: 0.747765  [  256/  306]
train() client id: f_00004-7-8 loss: 0.770668  [  288/  306]
train() client id: f_00004-8-0 loss: 0.815132  [   32/  306]
train() client id: f_00004-8-1 loss: 0.921477  [   64/  306]
train() client id: f_00004-8-2 loss: 0.774446  [   96/  306]
train() client id: f_00004-8-3 loss: 0.893112  [  128/  306]
train() client id: f_00004-8-4 loss: 0.851913  [  160/  306]
train() client id: f_00004-8-5 loss: 0.805285  [  192/  306]
train() client id: f_00004-8-6 loss: 0.886895  [  224/  306]
train() client id: f_00004-8-7 loss: 0.762139  [  256/  306]
train() client id: f_00004-8-8 loss: 0.916795  [  288/  306]
train() client id: f_00004-9-0 loss: 0.788077  [   32/  306]
train() client id: f_00004-9-1 loss: 0.766929  [   64/  306]
train() client id: f_00004-9-2 loss: 0.873896  [   96/  306]
train() client id: f_00004-9-3 loss: 0.923508  [  128/  306]
train() client id: f_00004-9-4 loss: 0.985709  [  160/  306]
train() client id: f_00004-9-5 loss: 0.774981  [  192/  306]
train() client id: f_00004-9-6 loss: 0.839930  [  224/  306]
train() client id: f_00004-9-7 loss: 0.897758  [  256/  306]
train() client id: f_00004-9-8 loss: 0.816693  [  288/  306]
train() client id: f_00004-10-0 loss: 0.901262  [   32/  306]
train() client id: f_00004-10-1 loss: 0.825616  [   64/  306]
train() client id: f_00004-10-2 loss: 0.911364  [   96/  306]
train() client id: f_00004-10-3 loss: 0.728908  [  128/  306]
train() client id: f_00004-10-4 loss: 0.823704  [  160/  306]
train() client id: f_00004-10-5 loss: 0.876830  [  192/  306]
train() client id: f_00004-10-6 loss: 0.925507  [  224/  306]
train() client id: f_00004-10-7 loss: 0.860909  [  256/  306]
train() client id: f_00004-10-8 loss: 0.820106  [  288/  306]
train() client id: f_00004-11-0 loss: 0.748249  [   32/  306]
train() client id: f_00004-11-1 loss: 0.752891  [   64/  306]
train() client id: f_00004-11-2 loss: 0.917151  [   96/  306]
train() client id: f_00004-11-3 loss: 0.921412  [  128/  306]
train() client id: f_00004-11-4 loss: 0.912646  [  160/  306]
train() client id: f_00004-11-5 loss: 0.846212  [  192/  306]
train() client id: f_00004-11-6 loss: 0.957109  [  224/  306]
train() client id: f_00004-11-7 loss: 0.741957  [  256/  306]
train() client id: f_00004-11-8 loss: 0.888414  [  288/  306]
train() client id: f_00005-0-0 loss: 0.472382  [   32/  146]
train() client id: f_00005-0-1 loss: 0.657541  [   64/  146]
train() client id: f_00005-0-2 loss: 0.735669  [   96/  146]
train() client id: f_00005-0-3 loss: 0.420883  [  128/  146]
train() client id: f_00005-1-0 loss: 0.353740  [   32/  146]
train() client id: f_00005-1-1 loss: 0.334751  [   64/  146]
train() client id: f_00005-1-2 loss: 0.567489  [   96/  146]
train() client id: f_00005-1-3 loss: 0.947610  [  128/  146]
train() client id: f_00005-2-0 loss: 0.638277  [   32/  146]
train() client id: f_00005-2-1 loss: 0.379620  [   64/  146]
train() client id: f_00005-2-2 loss: 0.554143  [   96/  146]
train() client id: f_00005-2-3 loss: 0.661914  [  128/  146]
train() client id: f_00005-3-0 loss: 0.424474  [   32/  146]
train() client id: f_00005-3-1 loss: 0.508400  [   64/  146]
train() client id: f_00005-3-2 loss: 0.516926  [   96/  146]
train() client id: f_00005-3-3 loss: 0.701449  [  128/  146]
train() client id: f_00005-4-0 loss: 0.656778  [   32/  146]
train() client id: f_00005-4-1 loss: 0.416449  [   64/  146]
train() client id: f_00005-4-2 loss: 0.683149  [   96/  146]
train() client id: f_00005-4-3 loss: 0.588388  [  128/  146]
train() client id: f_00005-5-0 loss: 0.612704  [   32/  146]
train() client id: f_00005-5-1 loss: 0.498678  [   64/  146]
train() client id: f_00005-5-2 loss: 0.592657  [   96/  146]
train() client id: f_00005-5-3 loss: 0.682181  [  128/  146]
train() client id: f_00005-6-0 loss: 0.607878  [   32/  146]
train() client id: f_00005-6-1 loss: 0.465506  [   64/  146]
train() client id: f_00005-6-2 loss: 0.627537  [   96/  146]
train() client id: f_00005-6-3 loss: 0.491722  [  128/  146]
train() client id: f_00005-7-0 loss: 0.530196  [   32/  146]
train() client id: f_00005-7-1 loss: 0.416365  [   64/  146]
train() client id: f_00005-7-2 loss: 0.658840  [   96/  146]
train() client id: f_00005-7-3 loss: 0.674616  [  128/  146]
train() client id: f_00005-8-0 loss: 0.726456  [   32/  146]
train() client id: f_00005-8-1 loss: 0.456902  [   64/  146]
train() client id: f_00005-8-2 loss: 0.356771  [   96/  146]
train() client id: f_00005-8-3 loss: 0.755703  [  128/  146]
train() client id: f_00005-9-0 loss: 0.355079  [   32/  146]
train() client id: f_00005-9-1 loss: 0.672944  [   64/  146]
train() client id: f_00005-9-2 loss: 0.351675  [   96/  146]
train() client id: f_00005-9-3 loss: 0.842759  [  128/  146]
train() client id: f_00005-10-0 loss: 0.613207  [   32/  146]
train() client id: f_00005-10-1 loss: 0.657839  [   64/  146]
train() client id: f_00005-10-2 loss: 0.437307  [   96/  146]
train() client id: f_00005-10-3 loss: 0.533598  [  128/  146]
train() client id: f_00005-11-0 loss: 0.618862  [   32/  146]
train() client id: f_00005-11-1 loss: 0.456443  [   64/  146]
train() client id: f_00005-11-2 loss: 0.597854  [   96/  146]
train() client id: f_00005-11-3 loss: 0.551733  [  128/  146]
train() client id: f_00006-0-0 loss: 0.565475  [   32/   54]
train() client id: f_00006-1-0 loss: 0.526458  [   32/   54]
train() client id: f_00006-2-0 loss: 0.556843  [   32/   54]
train() client id: f_00006-3-0 loss: 0.528288  [   32/   54]
train() client id: f_00006-4-0 loss: 0.556442  [   32/   54]
train() client id: f_00006-5-0 loss: 0.516303  [   32/   54]
train() client id: f_00006-6-0 loss: 0.522621  [   32/   54]
train() client id: f_00006-7-0 loss: 0.522454  [   32/   54]
train() client id: f_00006-8-0 loss: 0.564978  [   32/   54]
train() client id: f_00006-9-0 loss: 0.517450  [   32/   54]
train() client id: f_00006-10-0 loss: 0.551736  [   32/   54]
train() client id: f_00006-11-0 loss: 0.507655  [   32/   54]
train() client id: f_00007-0-0 loss: 0.606451  [   32/  179]
train() client id: f_00007-0-1 loss: 0.570424  [   64/  179]
train() client id: f_00007-0-2 loss: 0.723231  [   96/  179]
train() client id: f_00007-0-3 loss: 0.751681  [  128/  179]
train() client id: f_00007-0-4 loss: 0.910854  [  160/  179]
train() client id: f_00007-1-0 loss: 0.584601  [   32/  179]
train() client id: f_00007-1-1 loss: 0.875493  [   64/  179]
train() client id: f_00007-1-2 loss: 0.666562  [   96/  179]
train() client id: f_00007-1-3 loss: 0.728782  [  128/  179]
train() client id: f_00007-1-4 loss: 0.636743  [  160/  179]
train() client id: f_00007-2-0 loss: 0.679303  [   32/  179]
train() client id: f_00007-2-1 loss: 0.848724  [   64/  179]
train() client id: f_00007-2-2 loss: 0.673900  [   96/  179]
train() client id: f_00007-2-3 loss: 0.693394  [  128/  179]
train() client id: f_00007-2-4 loss: 0.691761  [  160/  179]
train() client id: f_00007-3-0 loss: 0.619695  [   32/  179]
train() client id: f_00007-3-1 loss: 0.649581  [   64/  179]
train() client id: f_00007-3-2 loss: 0.781835  [   96/  179]
train() client id: f_00007-3-3 loss: 0.607428  [  128/  179]
train() client id: f_00007-3-4 loss: 0.715844  [  160/  179]
train() client id: f_00007-4-0 loss: 0.682667  [   32/  179]
train() client id: f_00007-4-1 loss: 0.571293  [   64/  179]
train() client id: f_00007-4-2 loss: 0.897890  [   96/  179]
train() client id: f_00007-4-3 loss: 0.639846  [  128/  179]
train() client id: f_00007-4-4 loss: 0.621170  [  160/  179]
train() client id: f_00007-5-0 loss: 0.653513  [   32/  179]
train() client id: f_00007-5-1 loss: 0.555354  [   64/  179]
train() client id: f_00007-5-2 loss: 0.637399  [   96/  179]
train() client id: f_00007-5-3 loss: 0.820138  [  128/  179]
train() client id: f_00007-5-4 loss: 0.726151  [  160/  179]
train() client id: f_00007-6-0 loss: 0.737701  [   32/  179]
train() client id: f_00007-6-1 loss: 0.936589  [   64/  179]
train() client id: f_00007-6-2 loss: 0.616676  [   96/  179]
train() client id: f_00007-6-3 loss: 0.552859  [  128/  179]
train() client id: f_00007-6-4 loss: 0.653916  [  160/  179]
train() client id: f_00007-7-0 loss: 0.548118  [   32/  179]
train() client id: f_00007-7-1 loss: 0.703776  [   64/  179]
train() client id: f_00007-7-2 loss: 0.768155  [   96/  179]
train() client id: f_00007-7-3 loss: 0.886803  [  128/  179]
train() client id: f_00007-7-4 loss: 0.541289  [  160/  179]
train() client id: f_00007-8-0 loss: 0.788725  [   32/  179]
train() client id: f_00007-8-1 loss: 0.808593  [   64/  179]
train() client id: f_00007-8-2 loss: 0.714279  [   96/  179]
train() client id: f_00007-8-3 loss: 0.542905  [  128/  179]
train() client id: f_00007-8-4 loss: 0.547608  [  160/  179]
train() client id: f_00007-9-0 loss: 0.726252  [   32/  179]
train() client id: f_00007-9-1 loss: 0.750000  [   64/  179]
train() client id: f_00007-9-2 loss: 0.559363  [   96/  179]
train() client id: f_00007-9-3 loss: 0.759260  [  128/  179]
train() client id: f_00007-9-4 loss: 0.691412  [  160/  179]
train() client id: f_00007-10-0 loss: 0.728338  [   32/  179]
train() client id: f_00007-10-1 loss: 0.646193  [   64/  179]
train() client id: f_00007-10-2 loss: 0.804416  [   96/  179]
train() client id: f_00007-10-3 loss: 0.551985  [  128/  179]
train() client id: f_00007-10-4 loss: 0.676366  [  160/  179]
train() client id: f_00007-11-0 loss: 0.540402  [   32/  179]
train() client id: f_00007-11-1 loss: 0.749905  [   64/  179]
train() client id: f_00007-11-2 loss: 0.712346  [   96/  179]
train() client id: f_00007-11-3 loss: 0.725834  [  128/  179]
train() client id: f_00007-11-4 loss: 0.674528  [  160/  179]
train() client id: f_00008-0-0 loss: 0.878310  [   32/  130]
train() client id: f_00008-0-1 loss: 0.806202  [   64/  130]
train() client id: f_00008-0-2 loss: 0.701910  [   96/  130]
train() client id: f_00008-0-3 loss: 0.838161  [  128/  130]
train() client id: f_00008-1-0 loss: 0.804767  [   32/  130]
train() client id: f_00008-1-1 loss: 0.879157  [   64/  130]
train() client id: f_00008-1-2 loss: 0.684625  [   96/  130]
train() client id: f_00008-1-3 loss: 0.821436  [  128/  130]
train() client id: f_00008-2-0 loss: 0.721699  [   32/  130]
train() client id: f_00008-2-1 loss: 0.823760  [   64/  130]
train() client id: f_00008-2-2 loss: 0.845015  [   96/  130]
train() client id: f_00008-2-3 loss: 0.834085  [  128/  130]
train() client id: f_00008-3-0 loss: 0.920682  [   32/  130]
train() client id: f_00008-3-1 loss: 0.792415  [   64/  130]
train() client id: f_00008-3-2 loss: 0.727987  [   96/  130]
train() client id: f_00008-3-3 loss: 0.781874  [  128/  130]
train() client id: f_00008-4-0 loss: 0.976495  [   32/  130]
train() client id: f_00008-4-1 loss: 0.649127  [   64/  130]
train() client id: f_00008-4-2 loss: 0.814011  [   96/  130]
train() client id: f_00008-4-3 loss: 0.741132  [  128/  130]
train() client id: f_00008-5-0 loss: 0.853511  [   32/  130]
train() client id: f_00008-5-1 loss: 0.850231  [   64/  130]
train() client id: f_00008-5-2 loss: 0.784610  [   96/  130]
train() client id: f_00008-5-3 loss: 0.731515  [  128/  130]
train() client id: f_00008-6-0 loss: 0.910109  [   32/  130]
train() client id: f_00008-6-1 loss: 0.885456  [   64/  130]
train() client id: f_00008-6-2 loss: 0.757914  [   96/  130]
train() client id: f_00008-6-3 loss: 0.671655  [  128/  130]
train() client id: f_00008-7-0 loss: 0.876300  [   32/  130]
train() client id: f_00008-7-1 loss: 0.733556  [   64/  130]
train() client id: f_00008-7-2 loss: 0.801396  [   96/  130]
train() client id: f_00008-7-3 loss: 0.795886  [  128/  130]
train() client id: f_00008-8-0 loss: 0.714401  [   32/  130]
train() client id: f_00008-8-1 loss: 0.799229  [   64/  130]
train() client id: f_00008-8-2 loss: 0.867120  [   96/  130]
train() client id: f_00008-8-3 loss: 0.789677  [  128/  130]
train() client id: f_00008-9-0 loss: 0.730784  [   32/  130]
train() client id: f_00008-9-1 loss: 0.790492  [   64/  130]
train() client id: f_00008-9-2 loss: 0.857377  [   96/  130]
train() client id: f_00008-9-3 loss: 0.802925  [  128/  130]
train() client id: f_00008-10-0 loss: 0.889326  [   32/  130]
train() client id: f_00008-10-1 loss: 0.787269  [   64/  130]
train() client id: f_00008-10-2 loss: 0.763998  [   96/  130]
train() client id: f_00008-10-3 loss: 0.759701  [  128/  130]
train() client id: f_00008-11-0 loss: 0.925743  [   32/  130]
train() client id: f_00008-11-1 loss: 0.759525  [   64/  130]
train() client id: f_00008-11-2 loss: 0.753260  [   96/  130]
train() client id: f_00008-11-3 loss: 0.763825  [  128/  130]
train() client id: f_00009-0-0 loss: 0.998890  [   32/  118]
train() client id: f_00009-0-1 loss: 1.067874  [   64/  118]
train() client id: f_00009-0-2 loss: 0.994089  [   96/  118]
train() client id: f_00009-1-0 loss: 1.140901  [   32/  118]
train() client id: f_00009-1-1 loss: 0.968395  [   64/  118]
train() client id: f_00009-1-2 loss: 0.945331  [   96/  118]
train() client id: f_00009-2-0 loss: 0.981989  [   32/  118]
train() client id: f_00009-2-1 loss: 0.963171  [   64/  118]
train() client id: f_00009-2-2 loss: 0.802301  [   96/  118]
train() client id: f_00009-3-0 loss: 0.980807  [   32/  118]
train() client id: f_00009-3-1 loss: 0.812681  [   64/  118]
train() client id: f_00009-3-2 loss: 0.906625  [   96/  118]
train() client id: f_00009-4-0 loss: 0.884772  [   32/  118]
train() client id: f_00009-4-1 loss: 0.782685  [   64/  118]
train() client id: f_00009-4-2 loss: 0.882469  [   96/  118]
train() client id: f_00009-5-0 loss: 0.878459  [   32/  118]
train() client id: f_00009-5-1 loss: 0.785280  [   64/  118]
train() client id: f_00009-5-2 loss: 0.837657  [   96/  118]
train() client id: f_00009-6-0 loss: 0.851549  [   32/  118]
train() client id: f_00009-6-1 loss: 0.785156  [   64/  118]
train() client id: f_00009-6-2 loss: 0.704912  [   96/  118]
train() client id: f_00009-7-0 loss: 0.698783  [   32/  118]
train() client id: f_00009-7-1 loss: 0.765285  [   64/  118]
train() client id: f_00009-7-2 loss: 0.781880  [   96/  118]
train() client id: f_00009-8-0 loss: 0.720894  [   32/  118]
train() client id: f_00009-8-1 loss: 0.941844  [   64/  118]
train() client id: f_00009-8-2 loss: 0.673305  [   96/  118]
train() client id: f_00009-9-0 loss: 0.745957  [   32/  118]
train() client id: f_00009-9-1 loss: 0.798605  [   64/  118]
train() client id: f_00009-9-2 loss: 0.724068  [   96/  118]
train() client id: f_00009-10-0 loss: 0.694123  [   32/  118]
train() client id: f_00009-10-1 loss: 0.763436  [   64/  118]
train() client id: f_00009-10-2 loss: 0.793143  [   96/  118]
train() client id: f_00009-11-0 loss: 0.674106  [   32/  118]
train() client id: f_00009-11-1 loss: 0.606775  [   64/  118]
train() client id: f_00009-11-2 loss: 0.786686  [   96/  118]
At round 25 accuracy: 0.6445623342175066
At round 25 training accuracy: 0.5828303152246814
At round 25 training loss: 0.8393514029142003
update_location
xs = -3.905658 4.200318 145.009024 18.811294 0.979296 3.956410 -107.443192 -86.324852 129.663977 -72.060879 
ys = 137.587959 120.555839 1.320614 -107.455176 99.350187 82.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 170.134361 156.688713 176.151529 147.988106 140.966019 129.899333 146.802350 132.108502 164.685813 123.323891 
dists_bs = 175.136902 187.288336 364.102052 342.554732 191.550534 201.304812 190.038305 195.451110 342.935353 199.591974 
uav_gains = -105.792775 -104.884001 -106.182446 -104.259476 -103.729845 -102.840821 -104.171745 -103.024087 -105.431664 -102.276495 
bs_gains = -102.381634 -103.197360 -111.281307 -110.539498 -103.470993 -104.074976 -103.374611 -103.716127 -110.553002 -103.971065 
Round 26
-------------------------------
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.5605131  15.70320748  7.4465293   2.67409633 18.11274305  8.72126419
  3.31940453 10.65181908  7.8502879   7.07624586]
obj_prev = 89.11611082472227
eta_min = 5.392428774203226e-13	eta_max = 0.9242899713750001
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 20.70487256941968	eta = 0.909090909090909
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 36.71367954165409	eta = 0.5126865969778287
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 28.972791112910695	eta = 0.6496651065957375
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 27.57869875098564	eta = 0.6825054219090905
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 27.5077842638857	eta = 0.6842649064780151
af = 18.822611426745162	bf = 1.5436207515894453	zeta = 27.50758684227239	eta = 0.6842698174388544
eta = 0.6842698174388544
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [0.03130742 0.06584498 0.03081048 0.01068428 0.07603231 0.03627686
 0.01341747 0.04447642 0.03230131 0.02931965]
ene_total = [2.4173172  4.46359599 2.39783696 1.11945579 5.09142412 2.67769452
 1.2842835  3.15534052 2.65007964 2.25055859]
ti_comp = [0.3872178  0.39743138 0.38543905 0.39356432 0.39647217 0.39426467
 0.39389891 0.39802621 0.35903228 0.39465362]
ti_coms = [0.08286047 0.07264689 0.08463921 0.07651394 0.0736061  0.07581359
 0.07617935 0.07205206 0.11104599 0.07542464]
t_total = [28.6998909 28.6998909 28.6998909 28.6998909 28.6998909 28.6998909
 28.6998909 28.6998909 28.6998909 28.6998909]
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [1.27912028e-05 1.12959740e-04 1.23045006e-05 4.92135047e-07
 1.74762896e-04 1.91952252e-05 9.73020721e-07 3.47093215e-05
 1.63408045e-05 1.01140293e-05]
ene_total = [0.50667253 0.45043075 0.51750266 0.46717392 0.46006034 0.46403994
 0.4651605  0.44202165 0.67897151 0.46111082]
optimize_network iter = 0 obj = 4.913144626268169
eta = 0.6842698174388544
freqs = [40426116.40412594 82838172.3252855  39968031.32913876 13573739.87970061
 95886067.29898961 46005714.64645749 17031612.51169543 55871219.40930291
 44983852.28187121 37146055.45423073]
eta_min = 0.6842698174388597	eta_max = 0.6842698174388548
af = 0.020050708162629324	bf = 1.5436207515894453	zeta = 0.022055778978892257	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [3.21329018e-06 2.83767235e-05 3.09102528e-06 1.23629712e-07
 4.39023530e-05 4.82205074e-06 2.44433458e-07 8.71936159e-06
 4.10498900e-06 2.54075489e-06]
ene_total = [1.757659   1.54642537 1.79534967 1.62243185 1.57005655 1.60857787
 1.61536282 1.52964443 2.35549529 1.59984681]
ti_comp = [0.3872178  0.39743138 0.38543905 0.39356432 0.39647217 0.39426467
 0.39389891 0.39802621 0.35903228 0.39465362]
ti_coms = [0.08286047 0.07264689 0.08463921 0.07651394 0.0736061  0.07581359
 0.07617935 0.07205206 0.11104599 0.07542464]
t_total = [28.6998909 28.6998909 28.6998909 28.6998909 28.6998909 28.6998909
 28.6998909 28.6998909 28.6998909 28.6998909]
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [1.27912028e-05 1.12959740e-04 1.23045006e-05 4.92135047e-07
 1.74762896e-04 1.91952252e-05 9.73020721e-07 3.47093215e-05
 1.63408045e-05 1.01140293e-05]
ene_total = [0.50667253 0.45043075 0.51750266 0.46717392 0.46006034 0.46403994
 0.4651605  0.44202165 0.67897151 0.46111082]
optimize_network iter = 1 obj = 4.913144626268177
eta = 0.6842698174388548
freqs = [40426116.40412594 82838172.32528551 39968031.32913877 13573739.87970061
 95886067.29898961 46005714.64645751 17031612.51169543 55871219.40930291
 44983852.28187121 37146055.45423073]
Done!
ene_coms = [0.00828605 0.00726469 0.00846392 0.00765139 0.00736061 0.00758136
 0.00761794 0.00720521 0.0111046  0.00754246]
ene_comp = [1.23550879e-05 1.09108388e-04 1.18849799e-05 4.75355749e-07
 1.68804372e-04 1.85407658e-05 9.39845672e-07 3.35259105e-05
 1.57836663e-05 9.76919241e-06]
ene_total = [0.0082984  0.0073738  0.00847581 0.00765187 0.00752941 0.0075999
 0.00761888 0.00723873 0.01112038 0.00755223]
At round 26 energy consumption: 0.08045941191860002
At round 26 eta: 0.6842698174388548
At round 26 a_n: 19.276410835052125
At round 26 local rounds: 12.423580809938418
At round 26 global rounds: 61.05343074483861
gradient difference: 0.41730016469955444
train() client id: f_00000-0-0 loss: 1.210990  [   32/  126]
train() client id: f_00000-0-1 loss: 1.109256  [   64/  126]
train() client id: f_00000-0-2 loss: 1.387737  [   96/  126]
train() client id: f_00000-1-0 loss: 1.109588  [   32/  126]
train() client id: f_00000-1-1 loss: 1.157352  [   64/  126]
train() client id: f_00000-1-2 loss: 1.217660  [   96/  126]
train() client id: f_00000-2-0 loss: 1.000543  [   32/  126]
train() client id: f_00000-2-1 loss: 1.040497  [   64/  126]
train() client id: f_00000-2-2 loss: 1.161207  [   96/  126]
train() client id: f_00000-3-0 loss: 1.060651  [   32/  126]
train() client id: f_00000-3-1 loss: 0.951530  [   64/  126]
train() client id: f_00000-3-2 loss: 0.917768  [   96/  126]
train() client id: f_00000-4-0 loss: 0.900916  [   32/  126]
train() client id: f_00000-4-1 loss: 0.858298  [   64/  126]
train() client id: f_00000-4-2 loss: 1.133677  [   96/  126]
train() client id: f_00000-5-0 loss: 0.911014  [   32/  126]
train() client id: f_00000-5-1 loss: 1.052560  [   64/  126]
train() client id: f_00000-5-2 loss: 0.882205  [   96/  126]
train() client id: f_00000-6-0 loss: 0.950439  [   32/  126]
train() client id: f_00000-6-1 loss: 0.986063  [   64/  126]
train() client id: f_00000-6-2 loss: 0.818225  [   96/  126]
train() client id: f_00000-7-0 loss: 0.856580  [   32/  126]
train() client id: f_00000-7-1 loss: 0.881735  [   64/  126]
train() client id: f_00000-7-2 loss: 0.868779  [   96/  126]
train() client id: f_00000-8-0 loss: 0.876980  [   32/  126]
train() client id: f_00000-8-1 loss: 0.828690  [   64/  126]
train() client id: f_00000-8-2 loss: 0.990347  [   96/  126]
train() client id: f_00000-9-0 loss: 0.838621  [   32/  126]
train() client id: f_00000-9-1 loss: 0.855960  [   64/  126]
train() client id: f_00000-9-2 loss: 0.832146  [   96/  126]
train() client id: f_00000-10-0 loss: 0.939453  [   32/  126]
train() client id: f_00000-10-1 loss: 0.806022  [   64/  126]
train() client id: f_00000-10-2 loss: 0.883659  [   96/  126]
train() client id: f_00000-11-0 loss: 0.903145  [   32/  126]
train() client id: f_00000-11-1 loss: 0.851548  [   64/  126]
train() client id: f_00000-11-2 loss: 0.908012  [   96/  126]
train() client id: f_00001-0-0 loss: 0.427868  [   32/  265]
train() client id: f_00001-0-1 loss: 0.437367  [   64/  265]
train() client id: f_00001-0-2 loss: 0.600576  [   96/  265]
train() client id: f_00001-0-3 loss: 0.466803  [  128/  265]
train() client id: f_00001-0-4 loss: 0.465374  [  160/  265]
train() client id: f_00001-0-5 loss: 0.542002  [  192/  265]
train() client id: f_00001-0-6 loss: 0.462891  [  224/  265]
train() client id: f_00001-0-7 loss: 0.480361  [  256/  265]
train() client id: f_00001-1-0 loss: 0.523211  [   32/  265]
train() client id: f_00001-1-1 loss: 0.400680  [   64/  265]
train() client id: f_00001-1-2 loss: 0.509278  [   96/  265]
train() client id: f_00001-1-3 loss: 0.528323  [  128/  265]
train() client id: f_00001-1-4 loss: 0.483653  [  160/  265]
train() client id: f_00001-1-5 loss: 0.544363  [  192/  265]
train() client id: f_00001-1-6 loss: 0.462274  [  224/  265]
train() client id: f_00001-1-7 loss: 0.451202  [  256/  265]
train() client id: f_00001-2-0 loss: 0.527265  [   32/  265]
train() client id: f_00001-2-1 loss: 0.525461  [   64/  265]
train() client id: f_00001-2-2 loss: 0.471346  [   96/  265]
train() client id: f_00001-2-3 loss: 0.515320  [  128/  265]
train() client id: f_00001-2-4 loss: 0.393629  [  160/  265]
train() client id: f_00001-2-5 loss: 0.485619  [  192/  265]
train() client id: f_00001-2-6 loss: 0.479881  [  224/  265]
train() client id: f_00001-2-7 loss: 0.440777  [  256/  265]
train() client id: f_00001-3-0 loss: 0.559426  [   32/  265]
train() client id: f_00001-3-1 loss: 0.434094  [   64/  265]
train() client id: f_00001-3-2 loss: 0.457557  [   96/  265]
train() client id: f_00001-3-3 loss: 0.589575  [  128/  265]
train() client id: f_00001-3-4 loss: 0.424650  [  160/  265]
train() client id: f_00001-3-5 loss: 0.497632  [  192/  265]
train() client id: f_00001-3-6 loss: 0.394540  [  224/  265]
train() client id: f_00001-3-7 loss: 0.425180  [  256/  265]
train() client id: f_00001-4-0 loss: 0.427413  [   32/  265]
train() client id: f_00001-4-1 loss: 0.404199  [   64/  265]
train() client id: f_00001-4-2 loss: 0.366076  [   96/  265]
train() client id: f_00001-4-3 loss: 0.541059  [  128/  265]
train() client id: f_00001-4-4 loss: 0.452520  [  160/  265]
train() client id: f_00001-4-5 loss: 0.555622  [  192/  265]
train() client id: f_00001-4-6 loss: 0.504885  [  224/  265]
train() client id: f_00001-4-7 loss: 0.512196  [  256/  265]
train() client id: f_00001-5-0 loss: 0.481556  [   32/  265]
train() client id: f_00001-5-1 loss: 0.485893  [   64/  265]
train() client id: f_00001-5-2 loss: 0.484271  [   96/  265]
train() client id: f_00001-5-3 loss: 0.471448  [  128/  265]
train() client id: f_00001-5-4 loss: 0.456497  [  160/  265]
train() client id: f_00001-5-5 loss: 0.372810  [  192/  265]
train() client id: f_00001-5-6 loss: 0.497226  [  224/  265]
train() client id: f_00001-5-7 loss: 0.503650  [  256/  265]
train() client id: f_00001-6-0 loss: 0.552382  [   32/  265]
train() client id: f_00001-6-1 loss: 0.377250  [   64/  265]
train() client id: f_00001-6-2 loss: 0.427989  [   96/  265]
train() client id: f_00001-6-3 loss: 0.478303  [  128/  265]
train() client id: f_00001-6-4 loss: 0.410465  [  160/  265]
train() client id: f_00001-6-5 loss: 0.413135  [  192/  265]
train() client id: f_00001-6-6 loss: 0.504884  [  224/  265]
train() client id: f_00001-6-7 loss: 0.579421  [  256/  265]
train() client id: f_00001-7-0 loss: 0.487023  [   32/  265]
train() client id: f_00001-7-1 loss: 0.575858  [   64/  265]
train() client id: f_00001-7-2 loss: 0.414696  [   96/  265]
train() client id: f_00001-7-3 loss: 0.465747  [  128/  265]
train() client id: f_00001-7-4 loss: 0.379544  [  160/  265]
train() client id: f_00001-7-5 loss: 0.371108  [  192/  265]
train() client id: f_00001-7-6 loss: 0.418412  [  224/  265]
train() client id: f_00001-7-7 loss: 0.567231  [  256/  265]
train() client id: f_00001-8-0 loss: 0.496740  [   32/  265]
train() client id: f_00001-8-1 loss: 0.370902  [   64/  265]
train() client id: f_00001-8-2 loss: 0.478061  [   96/  265]
train() client id: f_00001-8-3 loss: 0.425714  [  128/  265]
train() client id: f_00001-8-4 loss: 0.534531  [  160/  265]
train() client id: f_00001-8-5 loss: 0.483652  [  192/  265]
train() client id: f_00001-8-6 loss: 0.495465  [  224/  265]
train() client id: f_00001-8-7 loss: 0.436293  [  256/  265]
train() client id: f_00001-9-0 loss: 0.421215  [   32/  265]
train() client id: f_00001-9-1 loss: 0.527725  [   64/  265]
train() client id: f_00001-9-2 loss: 0.405069  [   96/  265]
train() client id: f_00001-9-3 loss: 0.528371  [  128/  265]
train() client id: f_00001-9-4 loss: 0.561020  [  160/  265]
train() client id: f_00001-9-5 loss: 0.382645  [  192/  265]
train() client id: f_00001-9-6 loss: 0.428548  [  224/  265]
train() client id: f_00001-9-7 loss: 0.415318  [  256/  265]
train() client id: f_00001-10-0 loss: 0.527674  [   32/  265]
train() client id: f_00001-10-1 loss: 0.472090  [   64/  265]
train() client id: f_00001-10-2 loss: 0.365874  [   96/  265]
train() client id: f_00001-10-3 loss: 0.461890  [  128/  265]
train() client id: f_00001-10-4 loss: 0.504528  [  160/  265]
train() client id: f_00001-10-5 loss: 0.428509  [  192/  265]
train() client id: f_00001-10-6 loss: 0.510964  [  224/  265]
train() client id: f_00001-10-7 loss: 0.413341  [  256/  265]
train() client id: f_00001-11-0 loss: 0.415117  [   32/  265]
train() client id: f_00001-11-1 loss: 0.430733  [   64/  265]
train() client id: f_00001-11-2 loss: 0.401823  [   96/  265]
train() client id: f_00001-11-3 loss: 0.436008  [  128/  265]
train() client id: f_00001-11-4 loss: 0.417826  [  160/  265]
train() client id: f_00001-11-5 loss: 0.571764  [  192/  265]
train() client id: f_00001-11-6 loss: 0.476608  [  224/  265]
train() client id: f_00001-11-7 loss: 0.573592  [  256/  265]
train() client id: f_00002-0-0 loss: 1.057842  [   32/  124]
train() client id: f_00002-0-1 loss: 0.989965  [   64/  124]
train() client id: f_00002-0-2 loss: 1.179229  [   96/  124]
train() client id: f_00002-1-0 loss: 1.036046  [   32/  124]
train() client id: f_00002-1-1 loss: 1.160087  [   64/  124]
train() client id: f_00002-1-2 loss: 0.949126  [   96/  124]
train() client id: f_00002-2-0 loss: 0.855828  [   32/  124]
train() client id: f_00002-2-1 loss: 0.986617  [   64/  124]
train() client id: f_00002-2-2 loss: 1.134509  [   96/  124]
train() client id: f_00002-3-0 loss: 0.974025  [   32/  124]
train() client id: f_00002-3-1 loss: 1.086702  [   64/  124]
train() client id: f_00002-3-2 loss: 0.951221  [   96/  124]
train() client id: f_00002-4-0 loss: 0.958052  [   32/  124]
train() client id: f_00002-4-1 loss: 1.064674  [   64/  124]
train() client id: f_00002-4-2 loss: 0.783221  [   96/  124]
train() client id: f_00002-5-0 loss: 0.893383  [   32/  124]
train() client id: f_00002-5-1 loss: 0.904874  [   64/  124]
train() client id: f_00002-5-2 loss: 1.020088  [   96/  124]
train() client id: f_00002-6-0 loss: 0.866109  [   32/  124]
train() client id: f_00002-6-1 loss: 0.971297  [   64/  124]
train() client id: f_00002-6-2 loss: 0.744024  [   96/  124]
train() client id: f_00002-7-0 loss: 0.887666  [   32/  124]
train() client id: f_00002-7-1 loss: 0.884518  [   64/  124]
train() client id: f_00002-7-2 loss: 0.864387  [   96/  124]
train() client id: f_00002-8-0 loss: 0.931196  [   32/  124]
train() client id: f_00002-8-1 loss: 0.795934  [   64/  124]
train() client id: f_00002-8-2 loss: 0.924985  [   96/  124]
train() client id: f_00002-9-0 loss: 0.912004  [   32/  124]
train() client id: f_00002-9-1 loss: 0.859478  [   64/  124]
train() client id: f_00002-9-2 loss: 0.815961  [   96/  124]
train() client id: f_00002-10-0 loss: 0.818244  [   32/  124]
train() client id: f_00002-10-1 loss: 0.909775  [   64/  124]
train() client id: f_00002-10-2 loss: 0.813219  [   96/  124]
train() client id: f_00002-11-0 loss: 0.873011  [   32/  124]
train() client id: f_00002-11-1 loss: 0.879502  [   64/  124]
train() client id: f_00002-11-2 loss: 0.778241  [   96/  124]
train() client id: f_00003-0-0 loss: 0.780498  [   32/   43]
train() client id: f_00003-1-0 loss: 0.900330  [   32/   43]
train() client id: f_00003-2-0 loss: 0.669150  [   32/   43]
train() client id: f_00003-3-0 loss: 0.825240  [   32/   43]
train() client id: f_00003-4-0 loss: 0.820396  [   32/   43]
train() client id: f_00003-5-0 loss: 0.803091  [   32/   43]
train() client id: f_00003-6-0 loss: 0.795829  [   32/   43]
train() client id: f_00003-7-0 loss: 0.774690  [   32/   43]
train() client id: f_00003-8-0 loss: 0.760579  [   32/   43]
train() client id: f_00003-9-0 loss: 0.786184  [   32/   43]
train() client id: f_00003-10-0 loss: 0.767473  [   32/   43]
train() client id: f_00003-11-0 loss: 0.859313  [   32/   43]
train() client id: f_00004-0-0 loss: 0.757343  [   32/  306]
train() client id: f_00004-0-1 loss: 0.957549  [   64/  306]
train() client id: f_00004-0-2 loss: 0.878833  [   96/  306]
train() client id: f_00004-0-3 loss: 0.776329  [  128/  306]
train() client id: f_00004-0-4 loss: 0.774312  [  160/  306]
train() client id: f_00004-0-5 loss: 0.849308  [  192/  306]
train() client id: f_00004-0-6 loss: 0.840860  [  224/  306]
train() client id: f_00004-0-7 loss: 0.966482  [  256/  306]
train() client id: f_00004-0-8 loss: 0.955538  [  288/  306]
train() client id: f_00004-1-0 loss: 0.929992  [   32/  306]
train() client id: f_00004-1-1 loss: 0.832864  [   64/  306]
train() client id: f_00004-1-2 loss: 0.806769  [   96/  306]
train() client id: f_00004-1-3 loss: 0.792744  [  128/  306]
train() client id: f_00004-1-4 loss: 0.769855  [  160/  306]
train() client id: f_00004-1-5 loss: 0.975828  [  192/  306]
train() client id: f_00004-1-6 loss: 0.933231  [  224/  306]
train() client id: f_00004-1-7 loss: 0.778856  [  256/  306]
train() client id: f_00004-1-8 loss: 0.850746  [  288/  306]
train() client id: f_00004-2-0 loss: 0.808881  [   32/  306]
train() client id: f_00004-2-1 loss: 0.871578  [   64/  306]
train() client id: f_00004-2-2 loss: 1.042959  [   96/  306]
train() client id: f_00004-2-3 loss: 0.826249  [  128/  306]
train() client id: f_00004-2-4 loss: 0.911337  [  160/  306]
train() client id: f_00004-2-5 loss: 0.711700  [  192/  306]
train() client id: f_00004-2-6 loss: 0.807590  [  224/  306]
train() client id: f_00004-2-7 loss: 0.761733  [  256/  306]
train() client id: f_00004-2-8 loss: 0.775255  [  288/  306]
train() client id: f_00004-3-0 loss: 0.823987  [   32/  306]
train() client id: f_00004-3-1 loss: 0.830255  [   64/  306]
train() client id: f_00004-3-2 loss: 0.886999  [   96/  306]
train() client id: f_00004-3-3 loss: 0.745736  [  128/  306]
train() client id: f_00004-3-4 loss: 0.994553  [  160/  306]
train() client id: f_00004-3-5 loss: 0.798206  [  192/  306]
train() client id: f_00004-3-6 loss: 0.810914  [  224/  306]
train() client id: f_00004-3-7 loss: 0.897506  [  256/  306]
train() client id: f_00004-3-8 loss: 0.849667  [  288/  306]
train() client id: f_00004-4-0 loss: 0.939897  [   32/  306]
train() client id: f_00004-4-1 loss: 0.928987  [   64/  306]
train() client id: f_00004-4-2 loss: 0.832489  [   96/  306]
train() client id: f_00004-4-3 loss: 0.822947  [  128/  306]
train() client id: f_00004-4-4 loss: 0.777311  [  160/  306]
train() client id: f_00004-4-5 loss: 0.932663  [  192/  306]
train() client id: f_00004-4-6 loss: 0.787628  [  224/  306]
train() client id: f_00004-4-7 loss: 0.792675  [  256/  306]
train() client id: f_00004-4-8 loss: 0.856838  [  288/  306]
train() client id: f_00004-5-0 loss: 0.882907  [   32/  306]
train() client id: f_00004-5-1 loss: 0.936650  [   64/  306]
train() client id: f_00004-5-2 loss: 0.735859  [   96/  306]
train() client id: f_00004-5-3 loss: 0.839895  [  128/  306]
train() client id: f_00004-5-4 loss: 0.772453  [  160/  306]
train() client id: f_00004-5-5 loss: 0.921916  [  192/  306]
train() client id: f_00004-5-6 loss: 0.971060  [  224/  306]
train() client id: f_00004-5-7 loss: 0.787933  [  256/  306]
train() client id: f_00004-5-8 loss: 0.719545  [  288/  306]
train() client id: f_00004-6-0 loss: 0.910245  [   32/  306]
train() client id: f_00004-6-1 loss: 0.752961  [   64/  306]
train() client id: f_00004-6-2 loss: 0.854613  [   96/  306]
train() client id: f_00004-6-3 loss: 0.764078  [  128/  306]
train() client id: f_00004-6-4 loss: 0.827072  [  160/  306]
train() client id: f_00004-6-5 loss: 0.942557  [  192/  306]
train() client id: f_00004-6-6 loss: 0.707655  [  224/  306]
train() client id: f_00004-6-7 loss: 0.850975  [  256/  306]
train() client id: f_00004-6-8 loss: 0.938472  [  288/  306]
train() client id: f_00004-7-0 loss: 0.892305  [   32/  306]
train() client id: f_00004-7-1 loss: 0.752912  [   64/  306]
train() client id: f_00004-7-2 loss: 0.703389  [   96/  306]
train() client id: f_00004-7-3 loss: 0.828921  [  128/  306]
train() client id: f_00004-7-4 loss: 0.793412  [  160/  306]
train() client id: f_00004-7-5 loss: 0.896498  [  192/  306]
train() client id: f_00004-7-6 loss: 0.893456  [  224/  306]
train() client id: f_00004-7-7 loss: 0.825753  [  256/  306]
train() client id: f_00004-7-8 loss: 0.912109  [  288/  306]
train() client id: f_00004-8-0 loss: 0.784885  [   32/  306]
train() client id: f_00004-8-1 loss: 0.907169  [   64/  306]
train() client id: f_00004-8-2 loss: 0.841016  [   96/  306]
train() client id: f_00004-8-3 loss: 0.815336  [  128/  306]
train() client id: f_00004-8-4 loss: 1.028279  [  160/  306]
train() client id: f_00004-8-5 loss: 0.796284  [  192/  306]
train() client id: f_00004-8-6 loss: 0.843829  [  224/  306]
train() client id: f_00004-8-7 loss: 0.864984  [  256/  306]
train() client id: f_00004-8-8 loss: 0.800390  [  288/  306]
train() client id: f_00004-9-0 loss: 0.849766  [   32/  306]
train() client id: f_00004-9-1 loss: 0.871258  [   64/  306]
train() client id: f_00004-9-2 loss: 0.891319  [   96/  306]
train() client id: f_00004-9-3 loss: 0.819910  [  128/  306]
train() client id: f_00004-9-4 loss: 0.838972  [  160/  306]
train() client id: f_00004-9-5 loss: 0.855389  [  192/  306]
train() client id: f_00004-9-6 loss: 0.841638  [  224/  306]
train() client id: f_00004-9-7 loss: 0.845060  [  256/  306]
train() client id: f_00004-9-8 loss: 0.775584  [  288/  306]
train() client id: f_00004-10-0 loss: 0.842981  [   32/  306]
train() client id: f_00004-10-1 loss: 0.776395  [   64/  306]
train() client id: f_00004-10-2 loss: 0.850661  [   96/  306]
train() client id: f_00004-10-3 loss: 0.818576  [  128/  306]
train() client id: f_00004-10-4 loss: 0.986106  [  160/  306]
train() client id: f_00004-10-5 loss: 0.770606  [  192/  306]
train() client id: f_00004-10-6 loss: 0.762175  [  224/  306]
train() client id: f_00004-10-7 loss: 0.876613  [  256/  306]
train() client id: f_00004-10-8 loss: 0.917010  [  288/  306]
train() client id: f_00004-11-0 loss: 0.841091  [   32/  306]
train() client id: f_00004-11-1 loss: 0.801254  [   64/  306]
train() client id: f_00004-11-2 loss: 0.797631  [   96/  306]
train() client id: f_00004-11-3 loss: 0.773876  [  128/  306]
train() client id: f_00004-11-4 loss: 0.828279  [  160/  306]
train() client id: f_00004-11-5 loss: 0.848717  [  192/  306]
train() client id: f_00004-11-6 loss: 0.916040  [  224/  306]
train() client id: f_00004-11-7 loss: 0.879311  [  256/  306]
train() client id: f_00004-11-8 loss: 0.915621  [  288/  306]
train() client id: f_00005-0-0 loss: 0.744747  [   32/  146]
train() client id: f_00005-0-1 loss: 0.697676  [   64/  146]
train() client id: f_00005-0-2 loss: 0.442487  [   96/  146]
train() client id: f_00005-0-3 loss: 0.616187  [  128/  146]
train() client id: f_00005-1-0 loss: 0.822901  [   32/  146]
train() client id: f_00005-1-1 loss: 0.578237  [   64/  146]
train() client id: f_00005-1-2 loss: 0.671789  [   96/  146]
train() client id: f_00005-1-3 loss: 0.568794  [  128/  146]
train() client id: f_00005-2-0 loss: 0.534033  [   32/  146]
train() client id: f_00005-2-1 loss: 0.811479  [   64/  146]
train() client id: f_00005-2-2 loss: 0.674400  [   96/  146]
train() client id: f_00005-2-3 loss: 0.592825  [  128/  146]
train() client id: f_00005-3-0 loss: 0.833754  [   32/  146]
train() client id: f_00005-3-1 loss: 0.568626  [   64/  146]
train() client id: f_00005-3-2 loss: 0.577058  [   96/  146]
train() client id: f_00005-3-3 loss: 0.509026  [  128/  146]
train() client id: f_00005-4-0 loss: 0.545105  [   32/  146]
train() client id: f_00005-4-1 loss: 0.600388  [   64/  146]
train() client id: f_00005-4-2 loss: 0.492859  [   96/  146]
train() client id: f_00005-4-3 loss: 0.893205  [  128/  146]
train() client id: f_00005-5-0 loss: 0.714544  [   32/  146]
train() client id: f_00005-5-1 loss: 0.817590  [   64/  146]
train() client id: f_00005-5-2 loss: 0.518899  [   96/  146]
train() client id: f_00005-5-3 loss: 0.606260  [  128/  146]
train() client id: f_00005-6-0 loss: 0.518244  [   32/  146]
train() client id: f_00005-6-1 loss: 0.720323  [   64/  146]
train() client id: f_00005-6-2 loss: 0.607242  [   96/  146]
train() client id: f_00005-6-3 loss: 0.588661  [  128/  146]
train() client id: f_00005-7-0 loss: 0.500553  [   32/  146]
train() client id: f_00005-7-1 loss: 0.624485  [   64/  146]
train() client id: f_00005-7-2 loss: 0.781982  [   96/  146]
train() client id: f_00005-7-3 loss: 0.677995  [  128/  146]
train() client id: f_00005-8-0 loss: 0.752441  [   32/  146]
train() client id: f_00005-8-1 loss: 0.504239  [   64/  146]
train() client id: f_00005-8-2 loss: 0.556265  [   96/  146]
train() client id: f_00005-8-3 loss: 0.700438  [  128/  146]
train() client id: f_00005-9-0 loss: 0.564545  [   32/  146]
train() client id: f_00005-9-1 loss: 0.640959  [   64/  146]
train() client id: f_00005-9-2 loss: 0.737013  [   96/  146]
train() client id: f_00005-9-3 loss: 0.525523  [  128/  146]
train() client id: f_00005-10-0 loss: 0.446450  [   32/  146]
train() client id: f_00005-10-1 loss: 0.470834  [   64/  146]
train() client id: f_00005-10-2 loss: 0.867487  [   96/  146]
train() client id: f_00005-10-3 loss: 0.612070  [  128/  146]
train() client id: f_00005-11-0 loss: 0.577791  [   32/  146]
train() client id: f_00005-11-1 loss: 0.806347  [   64/  146]
train() client id: f_00005-11-2 loss: 0.647437  [   96/  146]
train() client id: f_00005-11-3 loss: 0.515241  [  128/  146]
train() client id: f_00006-0-0 loss: 0.591523  [   32/   54]
train() client id: f_00006-1-0 loss: 0.573019  [   32/   54]
train() client id: f_00006-2-0 loss: 0.588023  [   32/   54]
train() client id: f_00006-3-0 loss: 0.562253  [   32/   54]
train() client id: f_00006-4-0 loss: 0.615057  [   32/   54]
train() client id: f_00006-5-0 loss: 0.523634  [   32/   54]
train() client id: f_00006-6-0 loss: 0.613854  [   32/   54]
train() client id: f_00006-7-0 loss: 0.573389  [   32/   54]
train() client id: f_00006-8-0 loss: 0.578836  [   32/   54]
train() client id: f_00006-9-0 loss: 0.627213  [   32/   54]
train() client id: f_00006-10-0 loss: 0.532178  [   32/   54]
train() client id: f_00006-11-0 loss: 0.573550  [   32/   54]
train() client id: f_00007-0-0 loss: 0.630117  [   32/  179]
train() client id: f_00007-0-1 loss: 0.739457  [   64/  179]
train() client id: f_00007-0-2 loss: 0.593768  [   96/  179]
train() client id: f_00007-0-3 loss: 0.594460  [  128/  179]
train() client id: f_00007-0-4 loss: 0.600143  [  160/  179]
train() client id: f_00007-1-0 loss: 0.738039  [   32/  179]
train() client id: f_00007-1-1 loss: 0.589673  [   64/  179]
train() client id: f_00007-1-2 loss: 0.873586  [   96/  179]
train() client id: f_00007-1-3 loss: 0.532138  [  128/  179]
train() client id: f_00007-1-4 loss: 0.506597  [  160/  179]
train() client id: f_00007-2-0 loss: 0.634383  [   32/  179]
train() client id: f_00007-2-1 loss: 0.682286  [   64/  179]
train() client id: f_00007-2-2 loss: 0.475090  [   96/  179]
train() client id: f_00007-2-3 loss: 0.616284  [  128/  179]
train() client id: f_00007-2-4 loss: 0.575160  [  160/  179]
train() client id: f_00007-3-0 loss: 0.574715  [   32/  179]
train() client id: f_00007-3-1 loss: 0.608562  [   64/  179]
train() client id: f_00007-3-2 loss: 0.730283  [   96/  179]
train() client id: f_00007-3-3 loss: 0.633848  [  128/  179]
train() client id: f_00007-3-4 loss: 0.537178  [  160/  179]
train() client id: f_00007-4-0 loss: 0.486947  [   32/  179]
train() client id: f_00007-4-1 loss: 0.793095  [   64/  179]
train() client id: f_00007-4-2 loss: 0.689900  [   96/  179]
train() client id: f_00007-4-3 loss: 0.547411  [  128/  179]
train() client id: f_00007-4-4 loss: 0.515180  [  160/  179]
train() client id: f_00007-5-0 loss: 0.610251  [   32/  179]
train() client id: f_00007-5-1 loss: 0.540080  [   64/  179]
train() client id: f_00007-5-2 loss: 0.589519  [   96/  179]
train() client id: f_00007-5-3 loss: 0.855268  [  128/  179]
train() client id: f_00007-5-4 loss: 0.447482  [  160/  179]
train() client id: f_00007-6-0 loss: 0.845135  [   32/  179]
train() client id: f_00007-6-1 loss: 0.470409  [   64/  179]
train() client id: f_00007-6-2 loss: 0.571918  [   96/  179]
train() client id: f_00007-6-3 loss: 0.553571  [  128/  179]
train() client id: f_00007-6-4 loss: 0.665325  [  160/  179]
train() client id: f_00007-7-0 loss: 0.644541  [   32/  179]
train() client id: f_00007-7-1 loss: 0.754068  [   64/  179]
train() client id: f_00007-7-2 loss: 0.554362  [   96/  179]
train() client id: f_00007-7-3 loss: 0.645864  [  128/  179]
train() client id: f_00007-7-4 loss: 0.480908  [  160/  179]
train() client id: f_00007-8-0 loss: 0.707938  [   32/  179]
train() client id: f_00007-8-1 loss: 0.493065  [   64/  179]
train() client id: f_00007-8-2 loss: 0.604851  [   96/  179]
train() client id: f_00007-8-3 loss: 0.542679  [  128/  179]
train() client id: f_00007-8-4 loss: 0.756832  [  160/  179]
train() client id: f_00007-9-0 loss: 0.643499  [   32/  179]
train() client id: f_00007-9-1 loss: 0.614758  [   64/  179]
train() client id: f_00007-9-2 loss: 0.571627  [   96/  179]
train() client id: f_00007-9-3 loss: 0.569909  [  128/  179]
train() client id: f_00007-9-4 loss: 0.575397  [  160/  179]
train() client id: f_00007-10-0 loss: 0.846920  [   32/  179]
train() client id: f_00007-10-1 loss: 0.539648  [   64/  179]
train() client id: f_00007-10-2 loss: 0.619528  [   96/  179]
train() client id: f_00007-10-3 loss: 0.530169  [  128/  179]
train() client id: f_00007-10-4 loss: 0.557605  [  160/  179]
train() client id: f_00007-11-0 loss: 0.537488  [   32/  179]
train() client id: f_00007-11-1 loss: 0.468190  [   64/  179]
train() client id: f_00007-11-2 loss: 0.812023  [   96/  179]
train() client id: f_00007-11-3 loss: 0.738291  [  128/  179]
train() client id: f_00007-11-4 loss: 0.480926  [  160/  179]
train() client id: f_00008-0-0 loss: 0.655779  [   32/  130]
train() client id: f_00008-0-1 loss: 0.705894  [   64/  130]
train() client id: f_00008-0-2 loss: 0.836080  [   96/  130]
train() client id: f_00008-0-3 loss: 0.824578  [  128/  130]
train() client id: f_00008-1-0 loss: 0.828203  [   32/  130]
train() client id: f_00008-1-1 loss: 0.724949  [   64/  130]
train() client id: f_00008-1-2 loss: 0.711018  [   96/  130]
train() client id: f_00008-1-3 loss: 0.788209  [  128/  130]
train() client id: f_00008-2-0 loss: 0.690428  [   32/  130]
train() client id: f_00008-2-1 loss: 0.777017  [   64/  130]
train() client id: f_00008-2-2 loss: 0.842833  [   96/  130]
train() client id: f_00008-2-3 loss: 0.750762  [  128/  130]
train() client id: f_00008-3-0 loss: 0.843759  [   32/  130]
train() client id: f_00008-3-1 loss: 0.805316  [   64/  130]
train() client id: f_00008-3-2 loss: 0.720708  [   96/  130]
train() client id: f_00008-3-3 loss: 0.682910  [  128/  130]
train() client id: f_00008-4-0 loss: 0.684953  [   32/  130]
train() client id: f_00008-4-1 loss: 0.669025  [   64/  130]
train() client id: f_00008-4-2 loss: 0.908008  [   96/  130]
train() client id: f_00008-4-3 loss: 0.788766  [  128/  130]
train() client id: f_00008-5-0 loss: 0.741929  [   32/  130]
train() client id: f_00008-5-1 loss: 0.851322  [   64/  130]
train() client id: f_00008-5-2 loss: 0.704647  [   96/  130]
train() client id: f_00008-5-3 loss: 0.758690  [  128/  130]
train() client id: f_00008-6-0 loss: 0.884602  [   32/  130]
train() client id: f_00008-6-1 loss: 0.638698  [   64/  130]
train() client id: f_00008-6-2 loss: 0.724427  [   96/  130]
train() client id: f_00008-6-3 loss: 0.767132  [  128/  130]
train() client id: f_00008-7-0 loss: 0.707715  [   32/  130]
train() client id: f_00008-7-1 loss: 0.821383  [   64/  130]
train() client id: f_00008-7-2 loss: 0.766201  [   96/  130]
train() client id: f_00008-7-3 loss: 0.766306  [  128/  130]
train() client id: f_00008-8-0 loss: 0.619872  [   32/  130]
train() client id: f_00008-8-1 loss: 0.786298  [   64/  130]
train() client id: f_00008-8-2 loss: 0.791847  [   96/  130]
train() client id: f_00008-8-3 loss: 0.853774  [  128/  130]
train() client id: f_00008-9-0 loss: 0.799328  [   32/  130]
train() client id: f_00008-9-1 loss: 0.760271  [   64/  130]
train() client id: f_00008-9-2 loss: 0.706714  [   96/  130]
train() client id: f_00008-9-3 loss: 0.742570  [  128/  130]
train() client id: f_00008-10-0 loss: 0.753134  [   32/  130]
train() client id: f_00008-10-1 loss: 0.746706  [   64/  130]
train() client id: f_00008-10-2 loss: 0.730888  [   96/  130]
train() client id: f_00008-10-3 loss: 0.822218  [  128/  130]
train() client id: f_00008-11-0 loss: 0.811338  [   32/  130]
train() client id: f_00008-11-1 loss: 0.653172  [   64/  130]
train() client id: f_00008-11-2 loss: 0.763597  [   96/  130]
train() client id: f_00008-11-3 loss: 0.833500  [  128/  130]
train() client id: f_00009-0-0 loss: 1.157383  [   32/  118]
train() client id: f_00009-0-1 loss: 0.962622  [   64/  118]
train() client id: f_00009-0-2 loss: 0.933270  [   96/  118]
train() client id: f_00009-1-0 loss: 0.971015  [   32/  118]
train() client id: f_00009-1-1 loss: 1.053755  [   64/  118]
train() client id: f_00009-1-2 loss: 0.952039  [   96/  118]
train() client id: f_00009-2-0 loss: 0.923220  [   32/  118]
train() client id: f_00009-2-1 loss: 0.921122  [   64/  118]
train() client id: f_00009-2-2 loss: 1.036643  [   96/  118]
train() client id: f_00009-3-0 loss: 1.001180  [   32/  118]
train() client id: f_00009-3-1 loss: 0.889565  [   64/  118]
train() client id: f_00009-3-2 loss: 0.825127  [   96/  118]
train() client id: f_00009-4-0 loss: 0.889616  [   32/  118]
train() client id: f_00009-4-1 loss: 0.854218  [   64/  118]
train() client id: f_00009-4-2 loss: 0.898836  [   96/  118]
train() client id: f_00009-5-0 loss: 1.016504  [   32/  118]
train() client id: f_00009-5-1 loss: 0.755782  [   64/  118]
train() client id: f_00009-5-2 loss: 0.846251  [   96/  118]
train() client id: f_00009-6-0 loss: 0.879935  [   32/  118]
train() client id: f_00009-6-1 loss: 0.692074  [   64/  118]
train() client id: f_00009-6-2 loss: 0.852368  [   96/  118]
train() client id: f_00009-7-0 loss: 0.748097  [   32/  118]
train() client id: f_00009-7-1 loss: 0.882701  [   64/  118]
train() client id: f_00009-7-2 loss: 0.859883  [   96/  118]
train() client id: f_00009-8-0 loss: 0.749966  [   32/  118]
train() client id: f_00009-8-1 loss: 0.867151  [   64/  118]
train() client id: f_00009-8-2 loss: 0.740345  [   96/  118]
train() client id: f_00009-9-0 loss: 0.860379  [   32/  118]
train() client id: f_00009-9-1 loss: 0.691319  [   64/  118]
train() client id: f_00009-9-2 loss: 0.727419  [   96/  118]
train() client id: f_00009-10-0 loss: 0.778774  [   32/  118]
train() client id: f_00009-10-1 loss: 0.759598  [   64/  118]
train() client id: f_00009-10-2 loss: 0.839070  [   96/  118]
train() client id: f_00009-11-0 loss: 0.657952  [   32/  118]
train() client id: f_00009-11-1 loss: 0.871898  [   64/  118]
train() client id: f_00009-11-2 loss: 0.664447  [   96/  118]
At round 26 accuracy: 0.6445623342175066
At round 26 training accuracy: 0.5828303152246814
At round 26 training loss: 0.8477893427920478
update_location
xs = -3.905658 4.200318 150.009024 18.811294 0.979296 3.956410 -112.443192 -91.324852 134.663977 -77.060879 
ys = 142.587959 125.555839 1.320614 -112.455176 104.350187 87.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 174.202699 160.567467 180.289909 151.657612 144.533458 133.142699 150.500372 135.428597 168.650694 126.310692 
dists_bs = 174.137343 185.896420 368.504267 346.689048 189.631508 199.064736 188.318850 193.234016 347.384940 197.059800 
uav_gains = -106.057157 -105.152486 -106.446062 -104.526866 -104.001987 -103.108845 -104.443195 -103.293910 -105.695296 -102.536442 
bs_gains = -102.312033 -103.106648 -111.427450 -110.685383 -103.348553 -103.938900 -103.264085 -103.577399 -110.709767 -103.815804 
Round 27
-------------------------------
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.428454   15.42356035  7.31658542  2.62856167 17.79004913  8.56531288
  3.26241587 10.46431357  7.71314909  6.94940181]
obj_prev = 87.54180377366474
eta_min = 3.2918080146347544e-13	eta_max = 0.9246953843511696
af = 18.488129690050464	bf = 1.525402092626451	zeta = 20.336942659055513	eta = 0.909090909090909
af = 18.488129690050464	bf = 1.525402092626451	zeta = 36.16259963393917	eta = 0.5112500173438599
af = 18.488129690050464	bf = 1.525402092626451	zeta = 28.49928344660974	eta = 0.6487226152435002
af = 18.488129690050464	bf = 1.525402092626451	zeta = 27.11857262246232	eta = 0.6817515784269833
af = 18.488129690050464	bf = 1.525402092626451	zeta = 27.048110971781785	eta = 0.6835275745998821
af = 18.488129690050464	bf = 1.525402092626451	zeta = 27.047913433089164	eta = 0.6835325665983885
eta = 0.6835325665983885
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [0.03139638 0.06603207 0.03089802 0.01071464 0.07624835 0.03637993
 0.01345559 0.04460279 0.03239309 0.02940296]
ene_total = [2.38132415 4.38338617 2.36243079 1.10502586 4.99963694 2.62710832
 1.26705354 3.1051769  2.60980459 2.20696617]
ti_comp = [0.39480521 0.40653023 0.39297956 0.40131289 0.40569068 0.40355948
 0.40164054 0.40588199 0.36658327 0.40401382]
ti_coms = [0.08405931 0.07233429 0.08588495 0.07755163 0.07317384 0.07530504
 0.07722398 0.07298253 0.11228124 0.07485069]
t_total = [28.6498867 28.6498867 28.6498867 28.6498867 28.6498867 28.6498867
 28.6498867 28.6498867 28.6498867 28.6498867]
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [1.24094611e-05 1.08882722e-04 1.19380162e-05 4.77360184e-07
 1.68337651e-04 1.84778100e-05 9.43872309e-07 3.36640665e-05
 1.58085418e-05 9.73331126e-06]
ene_total = [0.50365913 0.43928163 0.51455356 0.4640106  0.44786169 0.45164645
 0.46207824 0.43865975 0.67271096 0.448405  ]
optimize_network iter = 0 obj = 4.842867033053818
eta = 0.6835325665983885
freqs = [39761858.95611374 81214212.7965591  39312506.23491908 13349480.34302717
 93973501.61782414 45073814.47997239 16750787.69960297 54945517.04012107
 44182442.51361968 36388553.66776452]
eta_min = 0.6835325665983994	eta_max = 0.6835325665983844
af = 0.018943411267469068	bf = 1.525402092626451	zeta = 0.020837752394215977	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [3.10856006e-06 2.72750345e-05 2.99046350e-06 1.19578344e-07
 4.21684466e-05 4.62867660e-06 2.36439257e-07 8.43282168e-06
 3.96002704e-06 2.43818667e-06]
ene_total = [1.75137172 1.512205   1.78937036 1.61521218 1.53279247 1.56936094
 1.60841252 1.52178181 2.33933449 1.55944202]
ti_comp = [0.39480521 0.40653023 0.39297956 0.40131289 0.40569068 0.40355948
 0.40164054 0.40588199 0.36658327 0.40401382]
ti_coms = [0.08405931 0.07233429 0.08588495 0.07755163 0.07317384 0.07530504
 0.07722398 0.07298253 0.11228124 0.07485069]
t_total = [28.6498867 28.6498867 28.6498867 28.6498867 28.6498867 28.6498867
 28.6498867 28.6498867 28.6498867 28.6498867]
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [1.24094611e-05 1.08882722e-04 1.19380162e-05 4.77360184e-07
 1.68337651e-04 1.84778100e-05 9.43872309e-07 3.36640665e-05
 1.58085418e-05 9.73331126e-06]
ene_total = [0.50365913 0.43928163 0.51455356 0.4640106  0.44786169 0.45164645
 0.46207824 0.43865975 0.67271096 0.448405  ]
optimize_network iter = 1 obj = 4.842867033053756
eta = 0.6835325665983844
freqs = [39761858.95611373 81214212.79655911 39312506.23491906 13349480.34302717
 93973501.61782415 45073814.4799724  16750787.69960297 54945517.04012109
 44182442.51361962 36388553.66776453]
Done!
ene_coms = [0.00840593 0.00723343 0.0085885  0.00775516 0.00731738 0.0075305
 0.0077224  0.00729825 0.01122812 0.00748507]
ene_comp = [1.19524010e-05 1.04872399e-04 1.14983202e-05 4.59778254e-07
 1.62137509e-04 1.77972431e-05 9.09107999e-07 3.24241657e-05
 1.52262882e-05 9.37481801e-06]
ene_total = [0.00841788 0.0073383  0.00859999 0.00775562 0.00747952 0.0075483
 0.00772331 0.00733068 0.01124335 0.00749444]
At round 27 energy consumption: 0.08093140167794016
At round 27 eta: 0.6835325665983844
At round 27 a_n: 18.93386498808281
At round 27 local rounds: 12.458880266672946
At round 27 global rounds: 59.82879433933611
gradient difference: 0.47019824385643005
train() client id: f_00000-0-0 loss: 1.215386  [   32/  126]
train() client id: f_00000-0-1 loss: 1.133366  [   64/  126]
train() client id: f_00000-0-2 loss: 1.177622  [   96/  126]
train() client id: f_00000-1-0 loss: 1.142936  [   32/  126]
train() client id: f_00000-1-1 loss: 0.975595  [   64/  126]
train() client id: f_00000-1-2 loss: 1.088149  [   96/  126]
train() client id: f_00000-2-0 loss: 1.063829  [   32/  126]
train() client id: f_00000-2-1 loss: 1.040667  [   64/  126]
train() client id: f_00000-2-2 loss: 0.885157  [   96/  126]
train() client id: f_00000-3-0 loss: 1.134565  [   32/  126]
train() client id: f_00000-3-1 loss: 0.933104  [   64/  126]
train() client id: f_00000-3-2 loss: 0.935878  [   96/  126]
train() client id: f_00000-4-0 loss: 0.978836  [   32/  126]
train() client id: f_00000-4-1 loss: 0.901173  [   64/  126]
train() client id: f_00000-4-2 loss: 0.823215  [   96/  126]
train() client id: f_00000-5-0 loss: 0.955621  [   32/  126]
train() client id: f_00000-5-1 loss: 0.936676  [   64/  126]
train() client id: f_00000-5-2 loss: 0.791720  [   96/  126]
train() client id: f_00000-6-0 loss: 0.748947  [   32/  126]
train() client id: f_00000-6-1 loss: 0.957899  [   64/  126]
train() client id: f_00000-6-2 loss: 0.888558  [   96/  126]
train() client id: f_00000-7-0 loss: 0.720260  [   32/  126]
train() client id: f_00000-7-1 loss: 0.883013  [   64/  126]
train() client id: f_00000-7-2 loss: 0.885019  [   96/  126]
train() client id: f_00000-8-0 loss: 0.818221  [   32/  126]
train() client id: f_00000-8-1 loss: 0.854324  [   64/  126]
train() client id: f_00000-8-2 loss: 0.852254  [   96/  126]
train() client id: f_00000-9-0 loss: 0.921898  [   32/  126]
train() client id: f_00000-9-1 loss: 0.704380  [   64/  126]
train() client id: f_00000-9-2 loss: 0.861718  [   96/  126]
train() client id: f_00000-10-0 loss: 0.895865  [   32/  126]
train() client id: f_00000-10-1 loss: 0.823305  [   64/  126]
train() client id: f_00000-10-2 loss: 0.740180  [   96/  126]
train() client id: f_00000-11-0 loss: 0.791945  [   32/  126]
train() client id: f_00000-11-1 loss: 0.791365  [   64/  126]
train() client id: f_00000-11-2 loss: 0.873090  [   96/  126]
train() client id: f_00001-0-0 loss: 0.326150  [   32/  265]
train() client id: f_00001-0-1 loss: 0.335059  [   64/  265]
train() client id: f_00001-0-2 loss: 0.399876  [   96/  265]
train() client id: f_00001-0-3 loss: 0.260067  [  128/  265]
train() client id: f_00001-0-4 loss: 0.253522  [  160/  265]
train() client id: f_00001-0-5 loss: 0.436015  [  192/  265]
train() client id: f_00001-0-6 loss: 0.330080  [  224/  265]
train() client id: f_00001-0-7 loss: 0.360069  [  256/  265]
train() client id: f_00001-1-0 loss: 0.311936  [   32/  265]
train() client id: f_00001-1-1 loss: 0.460706  [   64/  265]
train() client id: f_00001-1-2 loss: 0.365453  [   96/  265]
train() client id: f_00001-1-3 loss: 0.387282  [  128/  265]
train() client id: f_00001-1-4 loss: 0.253760  [  160/  265]
train() client id: f_00001-1-5 loss: 0.246335  [  192/  265]
train() client id: f_00001-1-6 loss: 0.284470  [  224/  265]
train() client id: f_00001-1-7 loss: 0.299332  [  256/  265]
train() client id: f_00001-2-0 loss: 0.288383  [   32/  265]
train() client id: f_00001-2-1 loss: 0.339908  [   64/  265]
train() client id: f_00001-2-2 loss: 0.312982  [   96/  265]
train() client id: f_00001-2-3 loss: 0.229211  [  128/  265]
train() client id: f_00001-2-4 loss: 0.266830  [  160/  265]
train() client id: f_00001-2-5 loss: 0.388366  [  192/  265]
train() client id: f_00001-2-6 loss: 0.231093  [  224/  265]
train() client id: f_00001-2-7 loss: 0.380769  [  256/  265]
train() client id: f_00001-3-0 loss: 0.310488  [   32/  265]
train() client id: f_00001-3-1 loss: 0.395540  [   64/  265]
train() client id: f_00001-3-2 loss: 0.311558  [   96/  265]
train() client id: f_00001-3-3 loss: 0.269806  [  128/  265]
train() client id: f_00001-3-4 loss: 0.230703  [  160/  265]
train() client id: f_00001-3-5 loss: 0.383992  [  192/  265]
train() client id: f_00001-3-6 loss: 0.277812  [  224/  265]
train() client id: f_00001-3-7 loss: 0.216420  [  256/  265]
train() client id: f_00001-4-0 loss: 0.381483  [   32/  265]
train() client id: f_00001-4-1 loss: 0.277337  [   64/  265]
train() client id: f_00001-4-2 loss: 0.218172  [   96/  265]
train() client id: f_00001-4-3 loss: 0.363431  [  128/  265]
train() client id: f_00001-4-4 loss: 0.244880  [  160/  265]
train() client id: f_00001-4-5 loss: 0.308327  [  192/  265]
train() client id: f_00001-4-6 loss: 0.312195  [  224/  265]
train() client id: f_00001-4-7 loss: 0.296769  [  256/  265]
train() client id: f_00001-5-0 loss: 0.221529  [   32/  265]
train() client id: f_00001-5-1 loss: 0.269760  [   64/  265]
train() client id: f_00001-5-2 loss: 0.450690  [   96/  265]
train() client id: f_00001-5-3 loss: 0.307138  [  128/  265]
train() client id: f_00001-5-4 loss: 0.193558  [  160/  265]
train() client id: f_00001-5-5 loss: 0.249700  [  192/  265]
train() client id: f_00001-5-6 loss: 0.207861  [  224/  265]
train() client id: f_00001-5-7 loss: 0.460875  [  256/  265]
train() client id: f_00001-6-0 loss: 0.260625  [   32/  265]
train() client id: f_00001-6-1 loss: 0.347528  [   64/  265]
train() client id: f_00001-6-2 loss: 0.387078  [   96/  265]
train() client id: f_00001-6-3 loss: 0.229856  [  128/  265]
train() client id: f_00001-6-4 loss: 0.312127  [  160/  265]
train() client id: f_00001-6-5 loss: 0.334512  [  192/  265]
train() client id: f_00001-6-6 loss: 0.188539  [  224/  265]
train() client id: f_00001-6-7 loss: 0.237370  [  256/  265]
train() client id: f_00001-7-0 loss: 0.237912  [   32/  265]
train() client id: f_00001-7-1 loss: 0.286162  [   64/  265]
train() client id: f_00001-7-2 loss: 0.318626  [   96/  265]
train() client id: f_00001-7-3 loss: 0.248801  [  128/  265]
train() client id: f_00001-7-4 loss: 0.330960  [  160/  265]
train() client id: f_00001-7-5 loss: 0.345708  [  192/  265]
train() client id: f_00001-7-6 loss: 0.351749  [  224/  265]
train() client id: f_00001-7-7 loss: 0.184573  [  256/  265]
train() client id: f_00001-8-0 loss: 0.307235  [   32/  265]
train() client id: f_00001-8-1 loss: 0.324650  [   64/  265]
train() client id: f_00001-8-2 loss: 0.190864  [   96/  265]
train() client id: f_00001-8-3 loss: 0.264405  [  128/  265]
train() client id: f_00001-8-4 loss: 0.268256  [  160/  265]
train() client id: f_00001-8-5 loss: 0.372887  [  192/  265]
train() client id: f_00001-8-6 loss: 0.293140  [  224/  265]
train() client id: f_00001-8-7 loss: 0.261106  [  256/  265]
train() client id: f_00001-9-0 loss: 0.239818  [   32/  265]
train() client id: f_00001-9-1 loss: 0.424208  [   64/  265]
train() client id: f_00001-9-2 loss: 0.371976  [   96/  265]
train() client id: f_00001-9-3 loss: 0.331065  [  128/  265]
train() client id: f_00001-9-4 loss: 0.198896  [  160/  265]
train() client id: f_00001-9-5 loss: 0.272615  [  192/  265]
train() client id: f_00001-9-6 loss: 0.250713  [  224/  265]
train() client id: f_00001-9-7 loss: 0.193725  [  256/  265]
train() client id: f_00001-10-0 loss: 0.317020  [   32/  265]
train() client id: f_00001-10-1 loss: 0.391644  [   64/  265]
train() client id: f_00001-10-2 loss: 0.174764  [   96/  265]
train() client id: f_00001-10-3 loss: 0.324512  [  128/  265]
train() client id: f_00001-10-4 loss: 0.205416  [  160/  265]
train() client id: f_00001-10-5 loss: 0.234955  [  192/  265]
train() client id: f_00001-10-6 loss: 0.188282  [  224/  265]
train() client id: f_00001-10-7 loss: 0.426794  [  256/  265]
train() client id: f_00001-11-0 loss: 0.323666  [   32/  265]
train() client id: f_00001-11-1 loss: 0.364421  [   64/  265]
train() client id: f_00001-11-2 loss: 0.270589  [   96/  265]
train() client id: f_00001-11-3 loss: 0.357821  [  128/  265]
train() client id: f_00001-11-4 loss: 0.278123  [  160/  265]
train() client id: f_00001-11-5 loss: 0.275669  [  192/  265]
train() client id: f_00001-11-6 loss: 0.184676  [  224/  265]
train() client id: f_00001-11-7 loss: 0.198078  [  256/  265]
train() client id: f_00002-0-0 loss: 1.217466  [   32/  124]
train() client id: f_00002-0-1 loss: 1.193980  [   64/  124]
train() client id: f_00002-0-2 loss: 1.189291  [   96/  124]
train() client id: f_00002-1-0 loss: 1.313138  [   32/  124]
train() client id: f_00002-1-1 loss: 1.210112  [   64/  124]
train() client id: f_00002-1-2 loss: 1.213224  [   96/  124]
train() client id: f_00002-2-0 loss: 1.199843  [   32/  124]
train() client id: f_00002-2-1 loss: 1.118810  [   64/  124]
train() client id: f_00002-2-2 loss: 1.197073  [   96/  124]
train() client id: f_00002-3-0 loss: 1.211531  [   32/  124]
train() client id: f_00002-3-1 loss: 1.101276  [   64/  124]
train() client id: f_00002-3-2 loss: 1.173237  [   96/  124]
train() client id: f_00002-4-0 loss: 1.051831  [   32/  124]
train() client id: f_00002-4-1 loss: 1.080926  [   64/  124]
train() client id: f_00002-4-2 loss: 1.126084  [   96/  124]
train() client id: f_00002-5-0 loss: 1.138868  [   32/  124]
train() client id: f_00002-5-1 loss: 1.109336  [   64/  124]
train() client id: f_00002-5-2 loss: 1.115440  [   96/  124]
train() client id: f_00002-6-0 loss: 1.009080  [   32/  124]
train() client id: f_00002-6-1 loss: 1.185595  [   64/  124]
train() client id: f_00002-6-2 loss: 1.152382  [   96/  124]
train() client id: f_00002-7-0 loss: 1.028807  [   32/  124]
train() client id: f_00002-7-1 loss: 1.190605  [   64/  124]
train() client id: f_00002-7-2 loss: 1.070646  [   96/  124]
train() client id: f_00002-8-0 loss: 1.204625  [   32/  124]
train() client id: f_00002-8-1 loss: 1.076364  [   64/  124]
train() client id: f_00002-8-2 loss: 1.002571  [   96/  124]
train() client id: f_00002-9-0 loss: 1.291077  [   32/  124]
train() client id: f_00002-9-1 loss: 1.080264  [   64/  124]
train() client id: f_00002-9-2 loss: 1.034595  [   96/  124]
train() client id: f_00002-10-0 loss: 1.341447  [   32/  124]
train() client id: f_00002-10-1 loss: 0.980207  [   64/  124]
train() client id: f_00002-10-2 loss: 0.885107  [   96/  124]
train() client id: f_00002-11-0 loss: 1.062435  [   32/  124]
train() client id: f_00002-11-1 loss: 1.017674  [   64/  124]
train() client id: f_00002-11-2 loss: 1.044149  [   96/  124]
train() client id: f_00003-0-0 loss: 0.590874  [   32/   43]
train() client id: f_00003-1-0 loss: 0.674807  [   32/   43]
train() client id: f_00003-2-0 loss: 0.648998  [   32/   43]
train() client id: f_00003-3-0 loss: 0.754757  [   32/   43]
train() client id: f_00003-4-0 loss: 0.642871  [   32/   43]
train() client id: f_00003-5-0 loss: 0.572519  [   32/   43]
train() client id: f_00003-6-0 loss: 0.707500  [   32/   43]
train() client id: f_00003-7-0 loss: 0.752906  [   32/   43]
train() client id: f_00003-8-0 loss: 0.740088  [   32/   43]
train() client id: f_00003-9-0 loss: 0.738530  [   32/   43]
train() client id: f_00003-10-0 loss: 0.616576  [   32/   43]
train() client id: f_00003-11-0 loss: 0.562106  [   32/   43]
train() client id: f_00004-0-0 loss: 0.721989  [   32/  306]
train() client id: f_00004-0-1 loss: 0.709854  [   64/  306]
train() client id: f_00004-0-2 loss: 0.802961  [   96/  306]
train() client id: f_00004-0-3 loss: 0.822241  [  128/  306]
train() client id: f_00004-0-4 loss: 0.880320  [  160/  306]
train() client id: f_00004-0-5 loss: 0.827421  [  192/  306]
train() client id: f_00004-0-6 loss: 0.793735  [  224/  306]
train() client id: f_00004-0-7 loss: 0.706997  [  256/  306]
train() client id: f_00004-0-8 loss: 0.825905  [  288/  306]
train() client id: f_00004-1-0 loss: 0.817755  [   32/  306]
train() client id: f_00004-1-1 loss: 0.815834  [   64/  306]
train() client id: f_00004-1-2 loss: 0.836016  [   96/  306]
train() client id: f_00004-1-3 loss: 0.681109  [  128/  306]
train() client id: f_00004-1-4 loss: 0.808483  [  160/  306]
train() client id: f_00004-1-5 loss: 0.813565  [  192/  306]
train() client id: f_00004-1-6 loss: 0.743588  [  224/  306]
train() client id: f_00004-1-7 loss: 0.808287  [  256/  306]
train() client id: f_00004-1-8 loss: 0.764013  [  288/  306]
train() client id: f_00004-2-0 loss: 0.840210  [   32/  306]
train() client id: f_00004-2-1 loss: 0.740081  [   64/  306]
train() client id: f_00004-2-2 loss: 0.729489  [   96/  306]
train() client id: f_00004-2-3 loss: 0.853590  [  128/  306]
train() client id: f_00004-2-4 loss: 0.835247  [  160/  306]
train() client id: f_00004-2-5 loss: 0.658074  [  192/  306]
train() client id: f_00004-2-6 loss: 0.750690  [  224/  306]
train() client id: f_00004-2-7 loss: 0.724299  [  256/  306]
train() client id: f_00004-2-8 loss: 0.907842  [  288/  306]
train() client id: f_00004-3-0 loss: 0.847631  [   32/  306]
train() client id: f_00004-3-1 loss: 0.714270  [   64/  306]
train() client id: f_00004-3-2 loss: 0.820394  [   96/  306]
train() client id: f_00004-3-3 loss: 0.867519  [  128/  306]
train() client id: f_00004-3-4 loss: 0.766628  [  160/  306]
train() client id: f_00004-3-5 loss: 0.814934  [  192/  306]
train() client id: f_00004-3-6 loss: 0.767752  [  224/  306]
train() client id: f_00004-3-7 loss: 0.823215  [  256/  306]
train() client id: f_00004-3-8 loss: 0.768046  [  288/  306]
train() client id: f_00004-4-0 loss: 0.891235  [   32/  306]
train() client id: f_00004-4-1 loss: 0.845944  [   64/  306]
train() client id: f_00004-4-2 loss: 0.662759  [   96/  306]
train() client id: f_00004-4-3 loss: 0.804394  [  128/  306]
train() client id: f_00004-4-4 loss: 0.837536  [  160/  306]
train() client id: f_00004-4-5 loss: 0.712474  [  192/  306]
train() client id: f_00004-4-6 loss: 0.903265  [  224/  306]
train() client id: f_00004-4-7 loss: 0.771535  [  256/  306]
train() client id: f_00004-4-8 loss: 0.664382  [  288/  306]
train() client id: f_00004-5-0 loss: 0.736231  [   32/  306]
train() client id: f_00004-5-1 loss: 0.737614  [   64/  306]
train() client id: f_00004-5-2 loss: 0.801685  [   96/  306]
train() client id: f_00004-5-3 loss: 0.754482  [  128/  306]
train() client id: f_00004-5-4 loss: 0.994473  [  160/  306]
train() client id: f_00004-5-5 loss: 0.740229  [  192/  306]
train() client id: f_00004-5-6 loss: 0.716613  [  224/  306]
train() client id: f_00004-5-7 loss: 0.840728  [  256/  306]
train() client id: f_00004-5-8 loss: 0.798894  [  288/  306]
train() client id: f_00004-6-0 loss: 0.844803  [   32/  306]
train() client id: f_00004-6-1 loss: 0.737861  [   64/  306]
train() client id: f_00004-6-2 loss: 0.824439  [   96/  306]
train() client id: f_00004-6-3 loss: 0.692707  [  128/  306]
train() client id: f_00004-6-4 loss: 0.886740  [  160/  306]
train() client id: f_00004-6-5 loss: 0.799062  [  192/  306]
train() client id: f_00004-6-6 loss: 0.756055  [  224/  306]
train() client id: f_00004-6-7 loss: 0.816810  [  256/  306]
train() client id: f_00004-6-8 loss: 0.844279  [  288/  306]
train() client id: f_00004-7-0 loss: 0.735335  [   32/  306]
train() client id: f_00004-7-1 loss: 0.804225  [   64/  306]
train() client id: f_00004-7-2 loss: 0.706733  [   96/  306]
train() client id: f_00004-7-3 loss: 0.828021  [  128/  306]
train() client id: f_00004-7-4 loss: 0.798054  [  160/  306]
train() client id: f_00004-7-5 loss: 0.854455  [  192/  306]
train() client id: f_00004-7-6 loss: 0.832733  [  224/  306]
train() client id: f_00004-7-7 loss: 0.785141  [  256/  306]
train() client id: f_00004-7-8 loss: 0.766598  [  288/  306]
train() client id: f_00004-8-0 loss: 0.780743  [   32/  306]
train() client id: f_00004-8-1 loss: 0.902280  [   64/  306]
train() client id: f_00004-8-2 loss: 0.865789  [   96/  306]
train() client id: f_00004-8-3 loss: 0.860709  [  128/  306]
train() client id: f_00004-8-4 loss: 0.749263  [  160/  306]
train() client id: f_00004-8-5 loss: 0.879557  [  192/  306]
train() client id: f_00004-8-6 loss: 0.647509  [  224/  306]
train() client id: f_00004-8-7 loss: 0.854357  [  256/  306]
train() client id: f_00004-8-8 loss: 0.747237  [  288/  306]
train() client id: f_00004-9-0 loss: 0.788250  [   32/  306]
train() client id: f_00004-9-1 loss: 0.739949  [   64/  306]
train() client id: f_00004-9-2 loss: 0.748000  [   96/  306]
train() client id: f_00004-9-3 loss: 0.764396  [  128/  306]
train() client id: f_00004-9-4 loss: 0.864432  [  160/  306]
train() client id: f_00004-9-5 loss: 0.931565  [  192/  306]
train() client id: f_00004-9-6 loss: 0.710513  [  224/  306]
train() client id: f_00004-9-7 loss: 0.796471  [  256/  306]
train() client id: f_00004-9-8 loss: 0.899246  [  288/  306]
train() client id: f_00004-10-0 loss: 0.740246  [   32/  306]
train() client id: f_00004-10-1 loss: 0.810750  [   64/  306]
train() client id: f_00004-10-2 loss: 0.846243  [   96/  306]
train() client id: f_00004-10-3 loss: 0.839551  [  128/  306]
train() client id: f_00004-10-4 loss: 0.816337  [  160/  306]
train() client id: f_00004-10-5 loss: 0.801015  [  192/  306]
train() client id: f_00004-10-6 loss: 0.835479  [  224/  306]
train() client id: f_00004-10-7 loss: 0.750348  [  256/  306]
train() client id: f_00004-10-8 loss: 0.823293  [  288/  306]
train() client id: f_00004-11-0 loss: 0.874344  [   32/  306]
train() client id: f_00004-11-1 loss: 0.648930  [   64/  306]
train() client id: f_00004-11-2 loss: 0.753423  [   96/  306]
train() client id: f_00004-11-3 loss: 0.891509  [  128/  306]
train() client id: f_00004-11-4 loss: 0.821553  [  160/  306]
train() client id: f_00004-11-5 loss: 0.927238  [  192/  306]
train() client id: f_00004-11-6 loss: 0.847268  [  224/  306]
train() client id: f_00004-11-7 loss: 0.743467  [  256/  306]
train() client id: f_00004-11-8 loss: 0.770260  [  288/  306]
train() client id: f_00005-0-0 loss: 0.912558  [   32/  146]
train() client id: f_00005-0-1 loss: 0.673128  [   64/  146]
train() client id: f_00005-0-2 loss: 0.721525  [   96/  146]
train() client id: f_00005-0-3 loss: 0.604437  [  128/  146]
train() client id: f_00005-1-0 loss: 0.820822  [   32/  146]
train() client id: f_00005-1-1 loss: 0.732837  [   64/  146]
train() client id: f_00005-1-2 loss: 0.609670  [   96/  146]
train() client id: f_00005-1-3 loss: 0.808240  [  128/  146]
train() client id: f_00005-2-0 loss: 0.855100  [   32/  146]
train() client id: f_00005-2-1 loss: 0.553856  [   64/  146]
train() client id: f_00005-2-2 loss: 0.887777  [   96/  146]
train() client id: f_00005-2-3 loss: 0.624006  [  128/  146]
train() client id: f_00005-3-0 loss: 0.696513  [   32/  146]
train() client id: f_00005-3-1 loss: 0.836607  [   64/  146]
train() client id: f_00005-3-2 loss: 0.766313  [   96/  146]
train() client id: f_00005-3-3 loss: 0.682867  [  128/  146]
train() client id: f_00005-4-0 loss: 0.938062  [   32/  146]
train() client id: f_00005-4-1 loss: 0.787580  [   64/  146]
train() client id: f_00005-4-2 loss: 0.501078  [   96/  146]
train() client id: f_00005-4-3 loss: 0.502151  [  128/  146]
train() client id: f_00005-5-0 loss: 1.079926  [   32/  146]
train() client id: f_00005-5-1 loss: 0.659530  [   64/  146]
train() client id: f_00005-5-2 loss: 0.716520  [   96/  146]
train() client id: f_00005-5-3 loss: 0.594979  [  128/  146]
train() client id: f_00005-6-0 loss: 0.497930  [   32/  146]
train() client id: f_00005-6-1 loss: 0.915963  [   64/  146]
train() client id: f_00005-6-2 loss: 0.626588  [   96/  146]
train() client id: f_00005-6-3 loss: 0.764582  [  128/  146]
train() client id: f_00005-7-0 loss: 0.796560  [   32/  146]
train() client id: f_00005-7-1 loss: 0.663074  [   64/  146]
train() client id: f_00005-7-2 loss: 0.739147  [   96/  146]
train() client id: f_00005-7-3 loss: 0.752407  [  128/  146]
train() client id: f_00005-8-0 loss: 0.635490  [   32/  146]
train() client id: f_00005-8-1 loss: 1.003909  [   64/  146]
train() client id: f_00005-8-2 loss: 0.549855  [   96/  146]
train() client id: f_00005-8-3 loss: 0.741547  [  128/  146]
train() client id: f_00005-9-0 loss: 0.511695  [   32/  146]
train() client id: f_00005-9-1 loss: 0.802976  [   64/  146]
train() client id: f_00005-9-2 loss: 0.695872  [   96/  146]
train() client id: f_00005-9-3 loss: 0.832638  [  128/  146]
train() client id: f_00005-10-0 loss: 0.435720  [   32/  146]
train() client id: f_00005-10-1 loss: 0.533046  [   64/  146]
train() client id: f_00005-10-2 loss: 0.888730  [   96/  146]
train() client id: f_00005-10-3 loss: 0.845289  [  128/  146]
train() client id: f_00005-11-0 loss: 0.636473  [   32/  146]
train() client id: f_00005-11-1 loss: 0.697216  [   64/  146]
train() client id: f_00005-11-2 loss: 0.876315  [   96/  146]
train() client id: f_00005-11-3 loss: 0.620645  [  128/  146]
train() client id: f_00006-0-0 loss: 0.511826  [   32/   54]
train() client id: f_00006-1-0 loss: 0.592665  [   32/   54]
train() client id: f_00006-2-0 loss: 0.596367  [   32/   54]
train() client id: f_00006-3-0 loss: 0.552159  [   32/   54]
train() client id: f_00006-4-0 loss: 0.602155  [   32/   54]
train() client id: f_00006-5-0 loss: 0.499860  [   32/   54]
train() client id: f_00006-6-0 loss: 0.548927  [   32/   54]
train() client id: f_00006-7-0 loss: 0.548794  [   32/   54]
train() client id: f_00006-8-0 loss: 0.562728  [   32/   54]
train() client id: f_00006-9-0 loss: 0.506291  [   32/   54]
train() client id: f_00006-10-0 loss: 0.595834  [   32/   54]
train() client id: f_00006-11-0 loss: 0.549094  [   32/   54]
train() client id: f_00007-0-0 loss: 0.718801  [   32/  179]
train() client id: f_00007-0-1 loss: 0.712855  [   64/  179]
train() client id: f_00007-0-2 loss: 0.729382  [   96/  179]
train() client id: f_00007-0-3 loss: 0.892396  [  128/  179]
train() client id: f_00007-0-4 loss: 0.575669  [  160/  179]
train() client id: f_00007-1-0 loss: 0.802321  [   32/  179]
train() client id: f_00007-1-1 loss: 0.656016  [   64/  179]
train() client id: f_00007-1-2 loss: 0.689927  [   96/  179]
train() client id: f_00007-1-3 loss: 0.627673  [  128/  179]
train() client id: f_00007-1-4 loss: 0.710912  [  160/  179]
train() client id: f_00007-2-0 loss: 0.788317  [   32/  179]
train() client id: f_00007-2-1 loss: 0.722013  [   64/  179]
train() client id: f_00007-2-2 loss: 0.654617  [   96/  179]
train() client id: f_00007-2-3 loss: 0.742431  [  128/  179]
train() client id: f_00007-2-4 loss: 0.737684  [  160/  179]
train() client id: f_00007-3-0 loss: 0.669043  [   32/  179]
train() client id: f_00007-3-1 loss: 0.679843  [   64/  179]
train() client id: f_00007-3-2 loss: 0.708403  [   96/  179]
train() client id: f_00007-3-3 loss: 0.669178  [  128/  179]
train() client id: f_00007-3-4 loss: 0.699058  [  160/  179]
train() client id: f_00007-4-0 loss: 0.710620  [   32/  179]
train() client id: f_00007-4-1 loss: 0.925656  [   64/  179]
train() client id: f_00007-4-2 loss: 0.736832  [   96/  179]
train() client id: f_00007-4-3 loss: 0.606568  [  128/  179]
train() client id: f_00007-4-4 loss: 0.651254  [  160/  179]
train() client id: f_00007-5-0 loss: 0.662765  [   32/  179]
train() client id: f_00007-5-1 loss: 0.729785  [   64/  179]
train() client id: f_00007-5-2 loss: 0.675073  [   96/  179]
train() client id: f_00007-5-3 loss: 0.693014  [  128/  179]
train() client id: f_00007-5-4 loss: 0.784429  [  160/  179]
train() client id: f_00007-6-0 loss: 0.751631  [   32/  179]
train() client id: f_00007-6-1 loss: 0.669171  [   64/  179]
train() client id: f_00007-6-2 loss: 0.785498  [   96/  179]
train() client id: f_00007-6-3 loss: 0.613870  [  128/  179]
train() client id: f_00007-6-4 loss: 0.607008  [  160/  179]
train() client id: f_00007-7-0 loss: 0.657336  [   32/  179]
train() client id: f_00007-7-1 loss: 0.823897  [   64/  179]
train() client id: f_00007-7-2 loss: 0.651616  [   96/  179]
train() client id: f_00007-7-3 loss: 0.575484  [  128/  179]
train() client id: f_00007-7-4 loss: 0.838307  [  160/  179]
train() client id: f_00007-8-0 loss: 0.929267  [   32/  179]
train() client id: f_00007-8-1 loss: 0.713280  [   64/  179]
train() client id: f_00007-8-2 loss: 0.639640  [   96/  179]
train() client id: f_00007-8-3 loss: 0.563270  [  128/  179]
train() client id: f_00007-8-4 loss: 0.763321  [  160/  179]
train() client id: f_00007-9-0 loss: 0.651931  [   32/  179]
train() client id: f_00007-9-1 loss: 0.664514  [   64/  179]
train() client id: f_00007-9-2 loss: 0.735431  [   96/  179]
train() client id: f_00007-9-3 loss: 0.990813  [  128/  179]
train() client id: f_00007-9-4 loss: 0.553294  [  160/  179]
train() client id: f_00007-10-0 loss: 0.556531  [   32/  179]
train() client id: f_00007-10-1 loss: 0.555107  [   64/  179]
train() client id: f_00007-10-2 loss: 0.655267  [   96/  179]
train() client id: f_00007-10-3 loss: 0.649287  [  128/  179]
train() client id: f_00007-10-4 loss: 0.926684  [  160/  179]
train() client id: f_00007-11-0 loss: 0.895626  [   32/  179]
train() client id: f_00007-11-1 loss: 0.572940  [   64/  179]
train() client id: f_00007-11-2 loss: 0.747759  [   96/  179]
train() client id: f_00007-11-3 loss: 0.569600  [  128/  179]
train() client id: f_00007-11-4 loss: 0.744658  [  160/  179]
train() client id: f_00008-0-0 loss: 0.715821  [   32/  130]
train() client id: f_00008-0-1 loss: 0.628096  [   64/  130]
train() client id: f_00008-0-2 loss: 0.741625  [   96/  130]
train() client id: f_00008-0-3 loss: 0.699541  [  128/  130]
train() client id: f_00008-1-0 loss: 0.742714  [   32/  130]
train() client id: f_00008-1-1 loss: 0.734955  [   64/  130]
train() client id: f_00008-1-2 loss: 0.668709  [   96/  130]
train() client id: f_00008-1-3 loss: 0.641965  [  128/  130]
train() client id: f_00008-2-0 loss: 0.631687  [   32/  130]
train() client id: f_00008-2-1 loss: 0.769254  [   64/  130]
train() client id: f_00008-2-2 loss: 0.652342  [   96/  130]
train() client id: f_00008-2-3 loss: 0.724881  [  128/  130]
train() client id: f_00008-3-0 loss: 0.805143  [   32/  130]
train() client id: f_00008-3-1 loss: 0.638048  [   64/  130]
train() client id: f_00008-3-2 loss: 0.713874  [   96/  130]
train() client id: f_00008-3-3 loss: 0.609379  [  128/  130]
train() client id: f_00008-4-0 loss: 0.690183  [   32/  130]
train() client id: f_00008-4-1 loss: 0.577119  [   64/  130]
train() client id: f_00008-4-2 loss: 0.780935  [   96/  130]
train() client id: f_00008-4-3 loss: 0.689817  [  128/  130]
train() client id: f_00008-5-0 loss: 0.618179  [   32/  130]
train() client id: f_00008-5-1 loss: 0.616136  [   64/  130]
train() client id: f_00008-5-2 loss: 0.682483  [   96/  130]
train() client id: f_00008-5-3 loss: 0.819739  [  128/  130]
train() client id: f_00008-6-0 loss: 0.710087  [   32/  130]
train() client id: f_00008-6-1 loss: 0.680660  [   64/  130]
train() client id: f_00008-6-2 loss: 0.665247  [   96/  130]
train() client id: f_00008-6-3 loss: 0.681834  [  128/  130]
train() client id: f_00008-7-0 loss: 0.687472  [   32/  130]
train() client id: f_00008-7-1 loss: 0.530086  [   64/  130]
train() client id: f_00008-7-2 loss: 0.790647  [   96/  130]
train() client id: f_00008-7-3 loss: 0.764877  [  128/  130]
train() client id: f_00008-8-0 loss: 0.729795  [   32/  130]
train() client id: f_00008-8-1 loss: 0.624207  [   64/  130]
train() client id: f_00008-8-2 loss: 0.637039  [   96/  130]
train() client id: f_00008-8-3 loss: 0.781600  [  128/  130]
train() client id: f_00008-9-0 loss: 0.610879  [   32/  130]
train() client id: f_00008-9-1 loss: 0.744401  [   64/  130]
train() client id: f_00008-9-2 loss: 0.726496  [   96/  130]
train() client id: f_00008-9-3 loss: 0.691333  [  128/  130]
train() client id: f_00008-10-0 loss: 0.730389  [   32/  130]
train() client id: f_00008-10-1 loss: 0.650586  [   64/  130]
train() client id: f_00008-10-2 loss: 0.664062  [   96/  130]
train() client id: f_00008-10-3 loss: 0.734252  [  128/  130]
train() client id: f_00008-11-0 loss: 0.764020  [   32/  130]
train() client id: f_00008-11-1 loss: 0.716221  [   64/  130]
train() client id: f_00008-11-2 loss: 0.672710  [   96/  130]
train() client id: f_00008-11-3 loss: 0.621201  [  128/  130]
train() client id: f_00009-0-0 loss: 1.342355  [   32/  118]
train() client id: f_00009-0-1 loss: 1.192336  [   64/  118]
train() client id: f_00009-0-2 loss: 1.003572  [   96/  118]
train() client id: f_00009-1-0 loss: 1.126126  [   32/  118]
train() client id: f_00009-1-1 loss: 1.082515  [   64/  118]
train() client id: f_00009-1-2 loss: 1.202049  [   96/  118]
train() client id: f_00009-2-0 loss: 1.083580  [   32/  118]
train() client id: f_00009-2-1 loss: 1.061125  [   64/  118]
train() client id: f_00009-2-2 loss: 1.083098  [   96/  118]
train() client id: f_00009-3-0 loss: 1.010190  [   32/  118]
train() client id: f_00009-3-1 loss: 1.100845  [   64/  118]
train() client id: f_00009-3-2 loss: 1.004200  [   96/  118]
train() client id: f_00009-4-0 loss: 1.090737  [   32/  118]
train() client id: f_00009-4-1 loss: 1.014730  [   64/  118]
train() client id: f_00009-4-2 loss: 1.001847  [   96/  118]
train() client id: f_00009-5-0 loss: 1.050300  [   32/  118]
train() client id: f_00009-5-1 loss: 0.969672  [   64/  118]
train() client id: f_00009-5-2 loss: 0.999366  [   96/  118]
train() client id: f_00009-6-0 loss: 0.892255  [   32/  118]
train() client id: f_00009-6-1 loss: 0.961450  [   64/  118]
train() client id: f_00009-6-2 loss: 1.098173  [   96/  118]
train() client id: f_00009-7-0 loss: 0.937810  [   32/  118]
train() client id: f_00009-7-1 loss: 1.111551  [   64/  118]
train() client id: f_00009-7-2 loss: 0.978436  [   96/  118]
train() client id: f_00009-8-0 loss: 0.840405  [   32/  118]
train() client id: f_00009-8-1 loss: 1.030146  [   64/  118]
train() client id: f_00009-8-2 loss: 0.976871  [   96/  118]
train() client id: f_00009-9-0 loss: 0.821688  [   32/  118]
train() client id: f_00009-9-1 loss: 0.994795  [   64/  118]
train() client id: f_00009-9-2 loss: 1.135888  [   96/  118]
train() client id: f_00009-10-0 loss: 0.865839  [   32/  118]
train() client id: f_00009-10-1 loss: 1.016063  [   64/  118]
train() client id: f_00009-10-2 loss: 0.954441  [   96/  118]
train() client id: f_00009-11-0 loss: 1.053264  [   32/  118]
train() client id: f_00009-11-1 loss: 0.893674  [   64/  118]
train() client id: f_00009-11-2 loss: 0.980110  [   96/  118]
At round 27 accuracy: 0.6445623342175066
At round 27 training accuracy: 0.5841716968477532
At round 27 training loss: 0.8359665134424048
update_location
xs = -3.905658 4.200318 155.009024 18.811294 0.979296 3.956410 -117.443192 -96.324852 139.663977 -82.060879 
ys = 147.587959 130.555839 1.320614 -117.455176 109.350187 92.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 178.318423 164.506747 184.470977 155.401361 148.183745 136.492197 154.271818 138.849391 172.669327 129.421790 
dists_bs = 173.276351 184.629460 372.921553 350.845903 187.826012 196.926155 186.717490 191.122038 351.849309 194.623158 
uav_gains = -106.320857 -105.419643 -106.709742 -104.793609 -104.273887 -103.378976 -104.713747 -103.565216 -105.957984 -102.800801 
bs_gains = -102.251759 -103.023487 -111.572349 -110.830319 -103.232220 -103.807554 -103.160239 -103.443761 -110.865047 -103.664506 
Round 28
-------------------------------
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.2963697  15.14398805  7.18662324  2.58300247 17.46743421  8.40944175
  3.20540132 10.27679841  7.57595044  6.8226403 ]
obj_prev = 85.96764987436393
eta_min = 1.97492431522131e-13	eta_max = 0.9251230340770827
af = 18.15364795335576	bf = 1.507335053902618	zeta = 19.96901274869134	eta = 0.909090909090909
af = 18.15364795335576	bf = 1.507335053902618	zeta = 35.613187548855024	eta = 0.509745102946827
af = 18.15364795335576	bf = 1.507335053902618	zeta = 28.02638083867362	eta = 0.6477342921247089
af = 18.15364795335576	bf = 1.507335053902618	zeta = 26.65886868799346	eta = 0.6809609277055236
af = 18.15364795335576	bf = 1.507335053902618	zeta = 26.58884556407932	eta = 0.682754274141212
af = 18.15364795335576	bf = 1.507335053902618	zeta = 26.588647822049044	eta = 0.6827593518426902
eta = 0.6827593518426902
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [0.03148978 0.0662285  0.03098994 0.01074651 0.07647517 0.03648815
 0.01349562 0.04473547 0.03248945 0.02949042]
ene_total = [2.34522213 4.30343838 2.32693545 1.09048608 4.90813228 2.57679446
 1.24971036 3.05496696 2.56930912 2.1636526 ]
ti_comp = [0.402737   0.41597589 0.40085904 0.40941141 0.41525818 0.41320547
 0.40973259 0.41408375 0.37449574 0.41372635]
ti_coms = [0.08528891 0.07205001 0.08716686 0.07861449 0.07276772 0.07482043
 0.07829331 0.07394215 0.11353016 0.07429955]
t_total = [28.59988251 28.59988251 28.59988251 28.59988251 28.59988251 28.59988251
 28.59988251 28.59988251 28.59988251 28.59988251]
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [1.20322169e-05 1.04924856e-04 1.15760063e-05 4.62767175e-07
 1.62108192e-04 1.77829383e-05 9.15076330e-07 3.26332096e-05
 1.52831670e-05 9.36476148e-06]
ene_total = [0.50052546 0.42838512 0.51150413 0.46073322 0.43594225 0.43951388
 0.4588775  0.43523713 0.66621878 0.43596797]
optimize_network iter = 0 obj = 4.772905437718752
eta = 0.6827593518426902
freqs = [39094714.05548728 79606171.90046072 38654407.63725649 13124341.80140644
 92081472.39087442 44152552.9427729  16468811.85493763 54017423.64815775
 43377597.092848   35640012.17367139]
eta_min = 0.6827593518432681	eta_max = 0.6827593518426932
af = 0.017883468190837512	bf = 1.507335053902618	zeta = 0.019671815009921264	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [3.00512114e-06 2.62056364e-05 2.89117971e-06 1.15578985e-07
 4.04875310e-05 4.44139964e-06 2.28546015e-07 8.15034742e-06
 3.81706619e-06 2.33890753e-06]
ene_total = [1.74481114 1.47881446 1.78319283 1.60772537 1.49641265 1.53101993
 1.60118017 1.51381715 2.32252357 1.51993757]
ti_comp = [0.402737   0.41597589 0.40085904 0.40941141 0.41525818 0.41320547
 0.40973259 0.41408375 0.37449574 0.41372635]
ti_coms = [0.08528891 0.07205001 0.08716686 0.07861449 0.07276772 0.07482043
 0.07829331 0.07394215 0.11353016 0.07429955]
t_total = [28.59988251 28.59988251 28.59988251 28.59988251 28.59988251 28.59988251
 28.59988251 28.59988251 28.59988251 28.59988251]
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [1.20322169e-05 1.04924856e-04 1.15760063e-05 4.62767175e-07
 1.62108192e-04 1.77829383e-05 9.15076330e-07 3.26332096e-05
 1.52831670e-05 9.36476148e-06]
ene_total = [0.50052546 0.42838512 0.51150413 0.46073322 0.43594225 0.43951388
 0.4588775  0.43523713 0.66621878 0.43596797]
optimize_network iter = 1 obj = 4.772905437718796
eta = 0.6827593518426932
freqs = [39094714.05548728 79606171.9004607  38654407.6372565  13124341.80140644
 92081472.39087439 44152552.94277289 16468811.85493763 54017423.64815772
 43377597.09284804 35640012.17367138]
Done!
ene_coms = [0.00852889 0.007205   0.00871669 0.00786145 0.00727677 0.00748204
 0.00782933 0.00739422 0.01135302 0.00742995]
ene_comp = [1.15546788e-05 1.00760567e-04 1.11165744e-05 4.44400737e-07
 1.55674395e-04 1.70771639e-05 8.78758515e-07 3.13380533e-05
 1.46766043e-05 8.99309011e-06]
ene_total = [0.00854045 0.00730576 0.0087278  0.00786189 0.00743245 0.00749912
 0.00783021 0.00742555 0.01136769 0.00743895]
At round 28 energy consumption: 0.0814298731583106
At round 28 eta: 0.6827593518426932
At round 28 a_n: 18.59131914111349
At round 28 local rounds: 12.495942598469853
At round 28 global rounds: 58.60320626975522
gradient difference: 0.4381778836250305
train() client id: f_00000-0-0 loss: 1.099567  [   32/  126]
train() client id: f_00000-0-1 loss: 0.993486  [   64/  126]
train() client id: f_00000-0-2 loss: 1.213654  [   96/  126]
train() client id: f_00000-1-0 loss: 1.120640  [   32/  126]
train() client id: f_00000-1-1 loss: 1.015589  [   64/  126]
train() client id: f_00000-1-2 loss: 1.115707  [   96/  126]
train() client id: f_00000-2-0 loss: 0.984353  [   32/  126]
train() client id: f_00000-2-1 loss: 0.983202  [   64/  126]
train() client id: f_00000-2-2 loss: 1.024110  [   96/  126]
train() client id: f_00000-3-0 loss: 1.114814  [   32/  126]
train() client id: f_00000-3-1 loss: 0.872139  [   64/  126]
train() client id: f_00000-3-2 loss: 0.965863  [   96/  126]
train() client id: f_00000-4-0 loss: 0.895952  [   32/  126]
train() client id: f_00000-4-1 loss: 0.954756  [   64/  126]
train() client id: f_00000-4-2 loss: 0.849288  [   96/  126]
train() client id: f_00000-5-0 loss: 0.796461  [   32/  126]
train() client id: f_00000-5-1 loss: 0.887702  [   64/  126]
train() client id: f_00000-5-2 loss: 0.940702  [   96/  126]
train() client id: f_00000-6-0 loss: 0.901835  [   32/  126]
train() client id: f_00000-6-1 loss: 0.928707  [   64/  126]
train() client id: f_00000-6-2 loss: 0.798950  [   96/  126]
train() client id: f_00000-7-0 loss: 0.862912  [   32/  126]
train() client id: f_00000-7-1 loss: 0.801019  [   64/  126]
train() client id: f_00000-7-2 loss: 0.920617  [   96/  126]
train() client id: f_00000-8-0 loss: 0.898221  [   32/  126]
train() client id: f_00000-8-1 loss: 0.836702  [   64/  126]
train() client id: f_00000-8-2 loss: 0.851473  [   96/  126]
train() client id: f_00000-9-0 loss: 0.801205  [   32/  126]
train() client id: f_00000-9-1 loss: 0.977042  [   64/  126]
train() client id: f_00000-9-2 loss: 0.756548  [   96/  126]
train() client id: f_00000-10-0 loss: 0.777740  [   32/  126]
train() client id: f_00000-10-1 loss: 0.731388  [   64/  126]
train() client id: f_00000-10-2 loss: 0.964243  [   96/  126]
train() client id: f_00000-11-0 loss: 0.781481  [   32/  126]
train() client id: f_00000-11-1 loss: 0.828851  [   64/  126]
train() client id: f_00000-11-2 loss: 0.827331  [   96/  126]
train() client id: f_00001-0-0 loss: 0.562380  [   32/  265]
train() client id: f_00001-0-1 loss: 0.524563  [   64/  265]
train() client id: f_00001-0-2 loss: 0.467193  [   96/  265]
train() client id: f_00001-0-3 loss: 0.607131  [  128/  265]
train() client id: f_00001-0-4 loss: 0.611337  [  160/  265]
train() client id: f_00001-0-5 loss: 0.482222  [  192/  265]
train() client id: f_00001-0-6 loss: 0.437793  [  224/  265]
train() client id: f_00001-0-7 loss: 0.622705  [  256/  265]
train() client id: f_00001-1-0 loss: 0.543316  [   32/  265]
train() client id: f_00001-1-1 loss: 0.484717  [   64/  265]
train() client id: f_00001-1-2 loss: 0.527127  [   96/  265]
train() client id: f_00001-1-3 loss: 0.513600  [  128/  265]
train() client id: f_00001-1-4 loss: 0.491766  [  160/  265]
train() client id: f_00001-1-5 loss: 0.508500  [  192/  265]
train() client id: f_00001-1-6 loss: 0.611461  [  224/  265]
train() client id: f_00001-1-7 loss: 0.578199  [  256/  265]
train() client id: f_00001-2-0 loss: 0.481163  [   32/  265]
train() client id: f_00001-2-1 loss: 0.503951  [   64/  265]
train() client id: f_00001-2-2 loss: 0.514799  [   96/  265]
train() client id: f_00001-2-3 loss: 0.556244  [  128/  265]
train() client id: f_00001-2-4 loss: 0.585518  [  160/  265]
train() client id: f_00001-2-5 loss: 0.433210  [  192/  265]
train() client id: f_00001-2-6 loss: 0.512698  [  224/  265]
train() client id: f_00001-2-7 loss: 0.595444  [  256/  265]
train() client id: f_00001-3-0 loss: 0.530193  [   32/  265]
train() client id: f_00001-3-1 loss: 0.449835  [   64/  265]
train() client id: f_00001-3-2 loss: 0.613951  [   96/  265]
train() client id: f_00001-3-3 loss: 0.448472  [  128/  265]
train() client id: f_00001-3-4 loss: 0.584210  [  160/  265]
train() client id: f_00001-3-5 loss: 0.512224  [  192/  265]
train() client id: f_00001-3-6 loss: 0.512213  [  224/  265]
train() client id: f_00001-3-7 loss: 0.543157  [  256/  265]
train() client id: f_00001-4-0 loss: 0.495921  [   32/  265]
train() client id: f_00001-4-1 loss: 0.502700  [   64/  265]
train() client id: f_00001-4-2 loss: 0.560435  [   96/  265]
train() client id: f_00001-4-3 loss: 0.407350  [  128/  265]
train() client id: f_00001-4-4 loss: 0.586803  [  160/  265]
train() client id: f_00001-4-5 loss: 0.545572  [  192/  265]
train() client id: f_00001-4-6 loss: 0.519820  [  224/  265]
train() client id: f_00001-4-7 loss: 0.568531  [  256/  265]
train() client id: f_00001-5-0 loss: 0.519704  [   32/  265]
train() client id: f_00001-5-1 loss: 0.496160  [   64/  265]
train() client id: f_00001-5-2 loss: 0.560636  [   96/  265]
train() client id: f_00001-5-3 loss: 0.565468  [  128/  265]
train() client id: f_00001-5-4 loss: 0.505799  [  160/  265]
train() client id: f_00001-5-5 loss: 0.501905  [  192/  265]
train() client id: f_00001-5-6 loss: 0.417926  [  224/  265]
train() client id: f_00001-5-7 loss: 0.556404  [  256/  265]
train() client id: f_00001-6-0 loss: 0.492278  [   32/  265]
train() client id: f_00001-6-1 loss: 0.552990  [   64/  265]
train() client id: f_00001-6-2 loss: 0.528351  [   96/  265]
train() client id: f_00001-6-3 loss: 0.544385  [  128/  265]
train() client id: f_00001-6-4 loss: 0.550126  [  160/  265]
train() client id: f_00001-6-5 loss: 0.489308  [  192/  265]
train() client id: f_00001-6-6 loss: 0.434583  [  224/  265]
train() client id: f_00001-6-7 loss: 0.565437  [  256/  265]
train() client id: f_00001-7-0 loss: 0.417894  [   32/  265]
train() client id: f_00001-7-1 loss: 0.484742  [   64/  265]
train() client id: f_00001-7-2 loss: 0.713343  [   96/  265]
train() client id: f_00001-7-3 loss: 0.496297  [  128/  265]
train() client id: f_00001-7-4 loss: 0.512199  [  160/  265]
train() client id: f_00001-7-5 loss: 0.475948  [  192/  265]
train() client id: f_00001-7-6 loss: 0.607234  [  224/  265]
train() client id: f_00001-7-7 loss: 0.459358  [  256/  265]
train() client id: f_00001-8-0 loss: 0.571927  [   32/  265]
train() client id: f_00001-8-1 loss: 0.462380  [   64/  265]
train() client id: f_00001-8-2 loss: 0.427618  [   96/  265]
train() client id: f_00001-8-3 loss: 0.535860  [  128/  265]
train() client id: f_00001-8-4 loss: 0.581128  [  160/  265]
train() client id: f_00001-8-5 loss: 0.665022  [  192/  265]
train() client id: f_00001-8-6 loss: 0.427147  [  224/  265]
train() client id: f_00001-8-7 loss: 0.496459  [  256/  265]
train() client id: f_00001-9-0 loss: 0.566327  [   32/  265]
train() client id: f_00001-9-1 loss: 0.602111  [   64/  265]
train() client id: f_00001-9-2 loss: 0.411768  [   96/  265]
train() client id: f_00001-9-3 loss: 0.615417  [  128/  265]
train() client id: f_00001-9-4 loss: 0.500263  [  160/  265]
train() client id: f_00001-9-5 loss: 0.550538  [  192/  265]
train() client id: f_00001-9-6 loss: 0.436000  [  224/  265]
train() client id: f_00001-9-7 loss: 0.487499  [  256/  265]
train() client id: f_00001-10-0 loss: 0.426242  [   32/  265]
train() client id: f_00001-10-1 loss: 0.479063  [   64/  265]
train() client id: f_00001-10-2 loss: 0.445099  [   96/  265]
train() client id: f_00001-10-3 loss: 0.451669  [  128/  265]
train() client id: f_00001-10-4 loss: 0.553921  [  160/  265]
train() client id: f_00001-10-5 loss: 0.570434  [  192/  265]
train() client id: f_00001-10-6 loss: 0.595664  [  224/  265]
train() client id: f_00001-10-7 loss: 0.595291  [  256/  265]
train() client id: f_00001-11-0 loss: 0.541937  [   32/  265]
train() client id: f_00001-11-1 loss: 0.424862  [   64/  265]
train() client id: f_00001-11-2 loss: 0.633824  [   96/  265]
train() client id: f_00001-11-3 loss: 0.461708  [  128/  265]
train() client id: f_00001-11-4 loss: 0.541576  [  160/  265]
train() client id: f_00001-11-5 loss: 0.428640  [  192/  265]
train() client id: f_00001-11-6 loss: 0.655766  [  224/  265]
train() client id: f_00001-11-7 loss: 0.480946  [  256/  265]
train() client id: f_00002-0-0 loss: 1.135124  [   32/  124]
train() client id: f_00002-0-1 loss: 1.286013  [   64/  124]
train() client id: f_00002-0-2 loss: 0.989605  [   96/  124]
train() client id: f_00002-1-0 loss: 1.001894  [   32/  124]
train() client id: f_00002-1-1 loss: 1.128987  [   64/  124]
train() client id: f_00002-1-2 loss: 1.212790  [   96/  124]
train() client id: f_00002-2-0 loss: 1.177598  [   32/  124]
train() client id: f_00002-2-1 loss: 1.129209  [   64/  124]
train() client id: f_00002-2-2 loss: 0.999566  [   96/  124]
train() client id: f_00002-3-0 loss: 1.120322  [   32/  124]
train() client id: f_00002-3-1 loss: 1.140055  [   64/  124]
train() client id: f_00002-3-2 loss: 1.029092  [   96/  124]
train() client id: f_00002-4-0 loss: 1.002899  [   32/  124]
train() client id: f_00002-4-1 loss: 1.050941  [   64/  124]
train() client id: f_00002-4-2 loss: 0.987720  [   96/  124]
train() client id: f_00002-5-0 loss: 1.059445  [   32/  124]
train() client id: f_00002-5-1 loss: 1.084204  [   64/  124]
train() client id: f_00002-5-2 loss: 1.065921  [   96/  124]
train() client id: f_00002-6-0 loss: 1.027708  [   32/  124]
train() client id: f_00002-6-1 loss: 1.053366  [   64/  124]
train() client id: f_00002-6-2 loss: 1.053479  [   96/  124]
train() client id: f_00002-7-0 loss: 1.022475  [   32/  124]
train() client id: f_00002-7-1 loss: 1.038662  [   64/  124]
train() client id: f_00002-7-2 loss: 0.967604  [   96/  124]
train() client id: f_00002-8-0 loss: 0.976649  [   32/  124]
train() client id: f_00002-8-1 loss: 1.161901  [   64/  124]
train() client id: f_00002-8-2 loss: 0.975333  [   96/  124]
train() client id: f_00002-9-0 loss: 0.928677  [   32/  124]
train() client id: f_00002-9-1 loss: 0.941114  [   64/  124]
train() client id: f_00002-9-2 loss: 1.094518  [   96/  124]
train() client id: f_00002-10-0 loss: 1.026287  [   32/  124]
train() client id: f_00002-10-1 loss: 1.122162  [   64/  124]
train() client id: f_00002-10-2 loss: 0.882494  [   96/  124]
train() client id: f_00002-11-0 loss: 1.019973  [   32/  124]
train() client id: f_00002-11-1 loss: 0.890010  [   64/  124]
train() client id: f_00002-11-2 loss: 0.934828  [   96/  124]
train() client id: f_00003-0-0 loss: 0.822098  [   32/   43]
train() client id: f_00003-1-0 loss: 0.907793  [   32/   43]
train() client id: f_00003-2-0 loss: 0.905118  [   32/   43]
train() client id: f_00003-3-0 loss: 0.799257  [   32/   43]
train() client id: f_00003-4-0 loss: 0.848078  [   32/   43]
train() client id: f_00003-5-0 loss: 0.951325  [   32/   43]
train() client id: f_00003-6-0 loss: 0.718388  [   32/   43]
train() client id: f_00003-7-0 loss: 0.872621  [   32/   43]
train() client id: f_00003-8-0 loss: 0.918620  [   32/   43]
train() client id: f_00003-9-0 loss: 0.736340  [   32/   43]
train() client id: f_00003-10-0 loss: 0.831323  [   32/   43]
train() client id: f_00003-11-0 loss: 0.904315  [   32/   43]
train() client id: f_00004-0-0 loss: 0.734013  [   32/  306]
train() client id: f_00004-0-1 loss: 0.857754  [   64/  306]
train() client id: f_00004-0-2 loss: 1.103922  [   96/  306]
train() client id: f_00004-0-3 loss: 0.879658  [  128/  306]
train() client id: f_00004-0-4 loss: 0.934497  [  160/  306]
train() client id: f_00004-0-5 loss: 0.920384  [  192/  306]
train() client id: f_00004-0-6 loss: 0.860902  [  224/  306]
train() client id: f_00004-0-7 loss: 0.914379  [  256/  306]
train() client id: f_00004-0-8 loss: 0.990773  [  288/  306]
train() client id: f_00004-1-0 loss: 1.044410  [   32/  306]
train() client id: f_00004-1-1 loss: 1.009189  [   64/  306]
train() client id: f_00004-1-2 loss: 0.850274  [   96/  306]
train() client id: f_00004-1-3 loss: 0.724307  [  128/  306]
train() client id: f_00004-1-4 loss: 0.868895  [  160/  306]
train() client id: f_00004-1-5 loss: 0.905240  [  192/  306]
train() client id: f_00004-1-6 loss: 1.085524  [  224/  306]
train() client id: f_00004-1-7 loss: 0.956841  [  256/  306]
train() client id: f_00004-1-8 loss: 0.817881  [  288/  306]
train() client id: f_00004-2-0 loss: 0.939644  [   32/  306]
train() client id: f_00004-2-1 loss: 0.811788  [   64/  306]
train() client id: f_00004-2-2 loss: 0.990770  [   96/  306]
train() client id: f_00004-2-3 loss: 0.827238  [  128/  306]
train() client id: f_00004-2-4 loss: 1.023705  [  160/  306]
train() client id: f_00004-2-5 loss: 0.846328  [  192/  306]
train() client id: f_00004-2-6 loss: 0.928130  [  224/  306]
train() client id: f_00004-2-7 loss: 0.917089  [  256/  306]
train() client id: f_00004-2-8 loss: 0.917896  [  288/  306]
train() client id: f_00004-3-0 loss: 0.864316  [   32/  306]
train() client id: f_00004-3-1 loss: 0.926183  [   64/  306]
train() client id: f_00004-3-2 loss: 0.910447  [   96/  306]
train() client id: f_00004-3-3 loss: 0.990573  [  128/  306]
train() client id: f_00004-3-4 loss: 0.948813  [  160/  306]
train() client id: f_00004-3-5 loss: 0.822861  [  192/  306]
train() client id: f_00004-3-6 loss: 0.995397  [  224/  306]
train() client id: f_00004-3-7 loss: 0.889436  [  256/  306]
train() client id: f_00004-3-8 loss: 0.915202  [  288/  306]
train() client id: f_00004-4-0 loss: 0.990418  [   32/  306]
train() client id: f_00004-4-1 loss: 0.987108  [   64/  306]
train() client id: f_00004-4-2 loss: 0.954926  [   96/  306]
train() client id: f_00004-4-3 loss: 0.889738  [  128/  306]
train() client id: f_00004-4-4 loss: 0.824722  [  160/  306]
train() client id: f_00004-4-5 loss: 0.800143  [  192/  306]
train() client id: f_00004-4-6 loss: 0.995463  [  224/  306]
train() client id: f_00004-4-7 loss: 1.022262  [  256/  306]
train() client id: f_00004-4-8 loss: 0.834124  [  288/  306]
train() client id: f_00004-5-0 loss: 0.844955  [   32/  306]
train() client id: f_00004-5-1 loss: 0.925190  [   64/  306]
train() client id: f_00004-5-2 loss: 1.004655  [   96/  306]
train() client id: f_00004-5-3 loss: 0.905369  [  128/  306]
train() client id: f_00004-5-4 loss: 0.911128  [  160/  306]
train() client id: f_00004-5-5 loss: 0.859543  [  192/  306]
train() client id: f_00004-5-6 loss: 0.831280  [  224/  306]
train() client id: f_00004-5-7 loss: 0.936878  [  256/  306]
train() client id: f_00004-5-8 loss: 0.954744  [  288/  306]
train() client id: f_00004-6-0 loss: 0.871534  [   32/  306]
train() client id: f_00004-6-1 loss: 0.874058  [   64/  306]
train() client id: f_00004-6-2 loss: 0.923113  [   96/  306]
train() client id: f_00004-6-3 loss: 0.983362  [  128/  306]
train() client id: f_00004-6-4 loss: 0.981853  [  160/  306]
train() client id: f_00004-6-5 loss: 0.919532  [  192/  306]
train() client id: f_00004-6-6 loss: 0.923958  [  224/  306]
train() client id: f_00004-6-7 loss: 0.907511  [  256/  306]
train() client id: f_00004-6-8 loss: 0.841894  [  288/  306]
train() client id: f_00004-7-0 loss: 0.893341  [   32/  306]
train() client id: f_00004-7-1 loss: 0.906272  [   64/  306]
train() client id: f_00004-7-2 loss: 0.959010  [   96/  306]
train() client id: f_00004-7-3 loss: 0.990079  [  128/  306]
train() client id: f_00004-7-4 loss: 0.963370  [  160/  306]
train() client id: f_00004-7-5 loss: 0.864248  [  192/  306]
train() client id: f_00004-7-6 loss: 0.794726  [  224/  306]
train() client id: f_00004-7-7 loss: 0.885037  [  256/  306]
train() client id: f_00004-7-8 loss: 0.805614  [  288/  306]
train() client id: f_00004-8-0 loss: 0.969651  [   32/  306]
train() client id: f_00004-8-1 loss: 0.895197  [   64/  306]
train() client id: f_00004-8-2 loss: 0.836476  [   96/  306]
train() client id: f_00004-8-3 loss: 0.883523  [  128/  306]
train() client id: f_00004-8-4 loss: 0.953338  [  160/  306]
train() client id: f_00004-8-5 loss: 0.945704  [  192/  306]
train() client id: f_00004-8-6 loss: 1.020139  [  224/  306]
train() client id: f_00004-8-7 loss: 0.804472  [  256/  306]
train() client id: f_00004-8-8 loss: 0.851929  [  288/  306]
train() client id: f_00004-9-0 loss: 0.877249  [   32/  306]
train() client id: f_00004-9-1 loss: 0.816537  [   64/  306]
train() client id: f_00004-9-2 loss: 0.916984  [   96/  306]
train() client id: f_00004-9-3 loss: 0.894022  [  128/  306]
train() client id: f_00004-9-4 loss: 1.043217  [  160/  306]
train() client id: f_00004-9-5 loss: 0.906451  [  192/  306]
train() client id: f_00004-9-6 loss: 0.853677  [  224/  306]
train() client id: f_00004-9-7 loss: 0.948288  [  256/  306]
train() client id: f_00004-9-8 loss: 0.897401  [  288/  306]
train() client id: f_00004-10-0 loss: 0.917013  [   32/  306]
train() client id: f_00004-10-1 loss: 0.937072  [   64/  306]
train() client id: f_00004-10-2 loss: 0.856365  [   96/  306]
train() client id: f_00004-10-3 loss: 0.868553  [  128/  306]
train() client id: f_00004-10-4 loss: 0.907794  [  160/  306]
train() client id: f_00004-10-5 loss: 0.862595  [  192/  306]
train() client id: f_00004-10-6 loss: 0.918137  [  224/  306]
train() client id: f_00004-10-7 loss: 0.967605  [  256/  306]
train() client id: f_00004-10-8 loss: 0.873501  [  288/  306]
train() client id: f_00004-11-0 loss: 0.850832  [   32/  306]
train() client id: f_00004-11-1 loss: 0.825646  [   64/  306]
train() client id: f_00004-11-2 loss: 0.904886  [   96/  306]
train() client id: f_00004-11-3 loss: 0.925655  [  128/  306]
train() client id: f_00004-11-4 loss: 0.938975  [  160/  306]
train() client id: f_00004-11-5 loss: 0.904381  [  192/  306]
train() client id: f_00004-11-6 loss: 0.842963  [  224/  306]
train() client id: f_00004-11-7 loss: 0.963583  [  256/  306]
train() client id: f_00004-11-8 loss: 0.960648  [  288/  306]
train() client id: f_00005-0-0 loss: 0.595460  [   32/  146]
train() client id: f_00005-0-1 loss: 1.003217  [   64/  146]
train() client id: f_00005-0-2 loss: 0.611903  [   96/  146]
train() client id: f_00005-0-3 loss: 0.670954  [  128/  146]
train() client id: f_00005-1-0 loss: 0.626796  [   32/  146]
train() client id: f_00005-1-1 loss: 0.623870  [   64/  146]
train() client id: f_00005-1-2 loss: 0.940704  [   96/  146]
train() client id: f_00005-1-3 loss: 0.689834  [  128/  146]
train() client id: f_00005-2-0 loss: 0.477064  [   32/  146]
train() client id: f_00005-2-1 loss: 0.819994  [   64/  146]
train() client id: f_00005-2-2 loss: 0.899552  [   96/  146]
train() client id: f_00005-2-3 loss: 0.767496  [  128/  146]
train() client id: f_00005-3-0 loss: 0.925017  [   32/  146]
train() client id: f_00005-3-1 loss: 0.615742  [   64/  146]
train() client id: f_00005-3-2 loss: 0.800812  [   96/  146]
train() client id: f_00005-3-3 loss: 0.602639  [  128/  146]
train() client id: f_00005-4-0 loss: 0.793723  [   32/  146]
train() client id: f_00005-4-1 loss: 0.920208  [   64/  146]
train() client id: f_00005-4-2 loss: 0.669059  [   96/  146]
train() client id: f_00005-4-3 loss: 0.558954  [  128/  146]
train() client id: f_00005-5-0 loss: 0.635556  [   32/  146]
train() client id: f_00005-5-1 loss: 0.481338  [   64/  146]
train() client id: f_00005-5-2 loss: 0.816981  [   96/  146]
train() client id: f_00005-5-3 loss: 0.965818  [  128/  146]
train() client id: f_00005-6-0 loss: 0.636516  [   32/  146]
train() client id: f_00005-6-1 loss: 0.550221  [   64/  146]
train() client id: f_00005-6-2 loss: 0.740157  [   96/  146]
train() client id: f_00005-6-3 loss: 0.922854  [  128/  146]
train() client id: f_00005-7-0 loss: 0.789265  [   32/  146]
train() client id: f_00005-7-1 loss: 0.661446  [   64/  146]
train() client id: f_00005-7-2 loss: 0.707451  [   96/  146]
train() client id: f_00005-7-3 loss: 0.750634  [  128/  146]
train() client id: f_00005-8-0 loss: 0.570832  [   32/  146]
train() client id: f_00005-8-1 loss: 0.720055  [   64/  146]
train() client id: f_00005-8-2 loss: 0.748476  [   96/  146]
train() client id: f_00005-8-3 loss: 0.833887  [  128/  146]
train() client id: f_00005-9-0 loss: 0.661175  [   32/  146]
train() client id: f_00005-9-1 loss: 0.697645  [   64/  146]
train() client id: f_00005-9-2 loss: 0.683421  [   96/  146]
train() client id: f_00005-9-3 loss: 0.747637  [  128/  146]
train() client id: f_00005-10-0 loss: 0.884075  [   32/  146]
train() client id: f_00005-10-1 loss: 0.718434  [   64/  146]
train() client id: f_00005-10-2 loss: 0.614812  [   96/  146]
train() client id: f_00005-10-3 loss: 0.607515  [  128/  146]
train() client id: f_00005-11-0 loss: 0.761099  [   32/  146]
train() client id: f_00005-11-1 loss: 0.825797  [   64/  146]
train() client id: f_00005-11-2 loss: 0.795889  [   96/  146]
train() client id: f_00005-11-3 loss: 0.575594  [  128/  146]
train() client id: f_00006-0-0 loss: 0.528106  [   32/   54]
train() client id: f_00006-1-0 loss: 0.540186  [   32/   54]
train() client id: f_00006-2-0 loss: 0.570234  [   32/   54]
train() client id: f_00006-3-0 loss: 0.580181  [   32/   54]
train() client id: f_00006-4-0 loss: 0.581884  [   32/   54]
train() client id: f_00006-5-0 loss: 0.493925  [   32/   54]
train() client id: f_00006-6-0 loss: 0.584276  [   32/   54]
train() client id: f_00006-7-0 loss: 0.497871  [   32/   54]
train() client id: f_00006-8-0 loss: 0.493904  [   32/   54]
train() client id: f_00006-9-0 loss: 0.584811  [   32/   54]
train() client id: f_00006-10-0 loss: 0.531188  [   32/   54]
train() client id: f_00006-11-0 loss: 0.543147  [   32/   54]
train() client id: f_00007-0-0 loss: 0.688788  [   32/  179]
train() client id: f_00007-0-1 loss: 0.660040  [   64/  179]
train() client id: f_00007-0-2 loss: 0.685571  [   96/  179]
train() client id: f_00007-0-3 loss: 0.475067  [  128/  179]
train() client id: f_00007-0-4 loss: 0.417050  [  160/  179]
train() client id: f_00007-1-0 loss: 0.536087  [   32/  179]
train() client id: f_00007-1-1 loss: 0.555359  [   64/  179]
train() client id: f_00007-1-2 loss: 0.650838  [   96/  179]
train() client id: f_00007-1-3 loss: 0.482971  [  128/  179]
train() client id: f_00007-1-4 loss: 0.440323  [  160/  179]
train() client id: f_00007-2-0 loss: 0.590105  [   32/  179]
train() client id: f_00007-2-1 loss: 0.679696  [   64/  179]
train() client id: f_00007-2-2 loss: 0.482004  [   96/  179]
train() client id: f_00007-2-3 loss: 0.543010  [  128/  179]
train() client id: f_00007-2-4 loss: 0.510392  [  160/  179]
train() client id: f_00007-3-0 loss: 0.590043  [   32/  179]
train() client id: f_00007-3-1 loss: 0.597755  [   64/  179]
train() client id: f_00007-3-2 loss: 0.453590  [   96/  179]
train() client id: f_00007-3-3 loss: 0.442954  [  128/  179]
train() client id: f_00007-3-4 loss: 0.660673  [  160/  179]
train() client id: f_00007-4-0 loss: 0.521153  [   32/  179]
train() client id: f_00007-4-1 loss: 0.680640  [   64/  179]
train() client id: f_00007-4-2 loss: 0.388970  [   96/  179]
train() client id: f_00007-4-3 loss: 0.393570  [  128/  179]
train() client id: f_00007-4-4 loss: 0.576014  [  160/  179]
train() client id: f_00007-5-0 loss: 0.691258  [   32/  179]
train() client id: f_00007-5-1 loss: 0.380668  [   64/  179]
train() client id: f_00007-5-2 loss: 0.501683  [   96/  179]
train() client id: f_00007-5-3 loss: 0.459536  [  128/  179]
train() client id: f_00007-5-4 loss: 0.528585  [  160/  179]
train() client id: f_00007-6-0 loss: 0.517331  [   32/  179]
train() client id: f_00007-6-1 loss: 0.391433  [   64/  179]
train() client id: f_00007-6-2 loss: 0.684711  [   96/  179]
train() client id: f_00007-6-3 loss: 0.488225  [  128/  179]
train() client id: f_00007-6-4 loss: 0.584747  [  160/  179]
train() client id: f_00007-7-0 loss: 0.624457  [   32/  179]
train() client id: f_00007-7-1 loss: 0.670294  [   64/  179]
train() client id: f_00007-7-2 loss: 0.376143  [   96/  179]
train() client id: f_00007-7-3 loss: 0.378788  [  128/  179]
train() client id: f_00007-7-4 loss: 0.399543  [  160/  179]
train() client id: f_00007-8-0 loss: 0.482798  [   32/  179]
train() client id: f_00007-8-1 loss: 0.366037  [   64/  179]
train() client id: f_00007-8-2 loss: 0.646387  [   96/  179]
train() client id: f_00007-8-3 loss: 0.370501  [  128/  179]
train() client id: f_00007-8-4 loss: 0.538556  [  160/  179]
train() client id: f_00007-9-0 loss: 0.418617  [   32/  179]
train() client id: f_00007-9-1 loss: 0.483991  [   64/  179]
train() client id: f_00007-9-2 loss: 0.604545  [   96/  179]
train() client id: f_00007-9-3 loss: 0.444593  [  128/  179]
train() client id: f_00007-9-4 loss: 0.557860  [  160/  179]
train() client id: f_00007-10-0 loss: 0.462860  [   32/  179]
train() client id: f_00007-10-1 loss: 0.440342  [   64/  179]
train() client id: f_00007-10-2 loss: 0.461140  [   96/  179]
train() client id: f_00007-10-3 loss: 0.482561  [  128/  179]
train() client id: f_00007-10-4 loss: 0.801433  [  160/  179]
train() client id: f_00007-11-0 loss: 0.655511  [   32/  179]
train() client id: f_00007-11-1 loss: 0.361449  [   64/  179]
train() client id: f_00007-11-2 loss: 0.444954  [   96/  179]
train() client id: f_00007-11-3 loss: 0.650460  [  128/  179]
train() client id: f_00007-11-4 loss: 0.506232  [  160/  179]
train() client id: f_00008-0-0 loss: 0.800196  [   32/  130]
train() client id: f_00008-0-1 loss: 0.776446  [   64/  130]
train() client id: f_00008-0-2 loss: 0.932770  [   96/  130]
train() client id: f_00008-0-3 loss: 0.601421  [  128/  130]
train() client id: f_00008-1-0 loss: 0.712710  [   32/  130]
train() client id: f_00008-1-1 loss: 0.806986  [   64/  130]
train() client id: f_00008-1-2 loss: 0.858511  [   96/  130]
train() client id: f_00008-1-3 loss: 0.728877  [  128/  130]
train() client id: f_00008-2-0 loss: 0.724153  [   32/  130]
train() client id: f_00008-2-1 loss: 0.829009  [   64/  130]
train() client id: f_00008-2-2 loss: 0.808360  [   96/  130]
train() client id: f_00008-2-3 loss: 0.732890  [  128/  130]
train() client id: f_00008-3-0 loss: 0.839226  [   32/  130]
train() client id: f_00008-3-1 loss: 0.715262  [   64/  130]
train() client id: f_00008-3-2 loss: 0.638476  [   96/  130]
train() client id: f_00008-3-3 loss: 0.909779  [  128/  130]
train() client id: f_00008-4-0 loss: 0.766992  [   32/  130]
train() client id: f_00008-4-1 loss: 0.788791  [   64/  130]
train() client id: f_00008-4-2 loss: 0.731494  [   96/  130]
train() client id: f_00008-4-3 loss: 0.809633  [  128/  130]
train() client id: f_00008-5-0 loss: 0.764979  [   32/  130]
train() client id: f_00008-5-1 loss: 0.784985  [   64/  130]
train() client id: f_00008-5-2 loss: 0.786520  [   96/  130]
train() client id: f_00008-5-3 loss: 0.732051  [  128/  130]
train() client id: f_00008-6-0 loss: 0.844615  [   32/  130]
train() client id: f_00008-6-1 loss: 0.735698  [   64/  130]
train() client id: f_00008-6-2 loss: 0.768838  [   96/  130]
train() client id: f_00008-6-3 loss: 0.764138  [  128/  130]
train() client id: f_00008-7-0 loss: 0.841558  [   32/  130]
train() client id: f_00008-7-1 loss: 0.721162  [   64/  130]
train() client id: f_00008-7-2 loss: 0.721979  [   96/  130]
train() client id: f_00008-7-3 loss: 0.793074  [  128/  130]
train() client id: f_00008-8-0 loss: 0.739314  [   32/  130]
train() client id: f_00008-8-1 loss: 0.788853  [   64/  130]
train() client id: f_00008-8-2 loss: 0.762906  [   96/  130]
train() client id: f_00008-8-3 loss: 0.811055  [  128/  130]
train() client id: f_00008-9-0 loss: 0.830360  [   32/  130]
train() client id: f_00008-9-1 loss: 0.826307  [   64/  130]
train() client id: f_00008-9-2 loss: 0.744104  [   96/  130]
train() client id: f_00008-9-3 loss: 0.710181  [  128/  130]
train() client id: f_00008-10-0 loss: 0.787903  [   32/  130]
train() client id: f_00008-10-1 loss: 0.775921  [   64/  130]
train() client id: f_00008-10-2 loss: 0.763707  [   96/  130]
train() client id: f_00008-10-3 loss: 0.777484  [  128/  130]
train() client id: f_00008-11-0 loss: 0.778910  [   32/  130]
train() client id: f_00008-11-1 loss: 0.805946  [   64/  130]
train() client id: f_00008-11-2 loss: 0.724638  [   96/  130]
train() client id: f_00008-11-3 loss: 0.803518  [  128/  130]
train() client id: f_00009-0-0 loss: 1.068397  [   32/  118]
train() client id: f_00009-0-1 loss: 1.111401  [   64/  118]
train() client id: f_00009-0-2 loss: 1.086752  [   96/  118]
train() client id: f_00009-1-0 loss: 1.030679  [   32/  118]
train() client id: f_00009-1-1 loss: 0.987570  [   64/  118]
train() client id: f_00009-1-2 loss: 1.068009  [   96/  118]
train() client id: f_00009-2-0 loss: 1.035324  [   32/  118]
train() client id: f_00009-2-1 loss: 1.027231  [   64/  118]
train() client id: f_00009-2-2 loss: 1.052302  [   96/  118]
train() client id: f_00009-3-0 loss: 1.027359  [   32/  118]
train() client id: f_00009-3-1 loss: 1.099525  [   64/  118]
train() client id: f_00009-3-2 loss: 0.915345  [   96/  118]
train() client id: f_00009-4-0 loss: 1.115754  [   32/  118]
train() client id: f_00009-4-1 loss: 0.863765  [   64/  118]
train() client id: f_00009-4-2 loss: 0.914123  [   96/  118]
train() client id: f_00009-5-0 loss: 0.883852  [   32/  118]
train() client id: f_00009-5-1 loss: 0.904576  [   64/  118]
train() client id: f_00009-5-2 loss: 0.890386  [   96/  118]
train() client id: f_00009-6-0 loss: 0.874034  [   32/  118]
train() client id: f_00009-6-1 loss: 0.803932  [   64/  118]
train() client id: f_00009-6-2 loss: 0.888811  [   96/  118]
train() client id: f_00009-7-0 loss: 1.022374  [   32/  118]
train() client id: f_00009-7-1 loss: 0.888055  [   64/  118]
train() client id: f_00009-7-2 loss: 0.869818  [   96/  118]
train() client id: f_00009-8-0 loss: 0.880195  [   32/  118]
train() client id: f_00009-8-1 loss: 0.799582  [   64/  118]
train() client id: f_00009-8-2 loss: 0.907616  [   96/  118]
train() client id: f_00009-9-0 loss: 0.887620  [   32/  118]
train() client id: f_00009-9-1 loss: 0.832229  [   64/  118]
train() client id: f_00009-9-2 loss: 0.783763  [   96/  118]
train() client id: f_00009-10-0 loss: 0.795032  [   32/  118]
train() client id: f_00009-10-1 loss: 0.913303  [   64/  118]
train() client id: f_00009-10-2 loss: 0.872918  [   96/  118]
train() client id: f_00009-11-0 loss: 0.914944  [   32/  118]
train() client id: f_00009-11-1 loss: 0.808867  [   64/  118]
train() client id: f_00009-11-2 loss: 0.857952  [   96/  118]
At round 28 accuracy: 0.6445623342175066
At round 28 training accuracy: 0.5855130784708249
At round 28 training loss: 0.836639419840655
update_location
xs = -3.905658 4.200318 160.009024 18.811294 0.979296 3.956410 -122.443192 -101.324852 144.663977 -87.060879 
ys = 152.587959 135.555839 1.320614 -122.455176 114.350187 97.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 182.478326 168.502309 188.691896 159.214116 151.910909 139.940206 158.111435 142.363625 176.738044 132.648439 
dists_bs = 172.556000 183.490043 377.353382 355.024505 186.137349 194.892411 185.237289 189.118698 356.327905 192.285679 
uav_gains = -106.584351 -105.685513 -106.974172 -105.059447 -104.545102 -103.650360 -104.983140 -103.837242 -106.219998 -103.068419 
bs_gains = -102.201101 -102.948209 -111.716010 -110.974292 -103.122398 -103.681317 -103.063455 -103.315624 -111.018855 -103.517573 
Round 29
-------------------------------
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.16426333 14.86448857  7.05664821  2.53741575 17.14489673  8.25364943
  3.14835783 10.08926783  7.43869069  6.69596016]
obj_prev = 84.39363852739328
eta_min = 1.1633239161667213e-13	eta_max = 0.9255734667757686
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 19.601082838327166	eta = 0.9090909090909091
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 35.06532785864593	eta = 0.5081705292616409
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 27.55402889869821	eta = 0.6466991191078749
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 26.19954622843339	eta = 0.6801326275385099
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 26.129948192651668	eta = 0.6819441847065052
af = 17.81916621666106	bf = 1.4894091419856037	zeta = 26.129750162898475	eta = 0.6819493529625255
eta = 0.6819493529625255
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [0.03158773 0.06643451 0.03108634 0.01077994 0.07671306 0.03660165
 0.0135376  0.04487463 0.03259051 0.02958216]
ene_total = [2.30902016 4.22374425 2.291367   1.07582749 4.81690287 2.52674818
 1.23224478 3.00469125 2.52859005 2.12061414]
ti_comp = [0.41103184 0.42578996 0.40909461 0.41788221 0.42519616 0.42322413
 0.41819745 0.42265509 0.38279172 0.42381267]
ti_coms = [0.08655269 0.07179457 0.08848992 0.07970232 0.07238837 0.0743604
 0.07938708 0.07492943 0.11479281 0.07377186]
t_total = [28.54987831 28.54987831 28.54987831 28.54987831 28.54987831 28.54987831
 28.54987831 28.54987831 28.54987831 28.54987831]
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [1.16596177e-05 1.01081200e-04 1.12186639e-05 4.48354160e-07
 1.56066329e-04 1.71096574e-05 8.86629391e-07 3.16162585e-05
 1.47648345e-05 9.00786229e-06]
ene_total = [0.49728187 0.41773503 0.50837179 0.45733327 0.42429695 0.4276389
 0.45554967 0.43173622 0.65949319 0.42379718]
optimize_network iter = 0 obj = 4.703234079287297
eta = 0.6819493529625255
freqs = [38424917.91998439 78013241.65854445 37994067.71862566 12898298.99218661
 90209018.95065838 43241454.64083756 16185653.82662158 53086584.80672698
 42569513.0112679  34900040.52352524]
eta_min = 0.6819493529625276	eta_max = 0.6819493529625246
af = 0.016869127524841664	bf = 1.4894091419856037	zeta = 0.018556040277325832	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [2.90303184e-06 2.51673727e-05 2.79324241e-06 1.11631997e-07
 3.88576653e-05 4.25999219e-06 2.20754524e-07 7.87187088e-06
 3.67617412e-06 2.24279318e-06]
ene_total = [1.73801127 1.44623117 1.77687644 1.59993898 1.46089904 1.49353974
 1.59363288 1.50568747 2.3050487  1.48132066]
ti_comp = [0.41103184 0.42578996 0.40909461 0.41788221 0.42519616 0.42322413
 0.41819745 0.42265509 0.38279172 0.42381267]
ti_coms = [0.08655269 0.07179457 0.08848992 0.07970232 0.07238837 0.0743604
 0.07938708 0.07492943 0.11479281 0.07377186]
t_total = [28.54987831 28.54987831 28.54987831 28.54987831 28.54987831 28.54987831
 28.54987831 28.54987831 28.54987831 28.54987831]
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [1.16596177e-05 1.01081200e-04 1.12186639e-05 4.48354160e-07
 1.56066329e-04 1.71096574e-05 8.86629391e-07 3.16162585e-05
 1.47648345e-05 9.00786229e-06]
ene_total = [0.49728187 0.41773503 0.50837179 0.45733327 0.42429695 0.4276389
 0.45554967 0.43173622 0.65949319 0.42379718]
optimize_network iter = 1 obj = 4.703234079287284
eta = 0.6819493529625246
freqs = [38424917.91998437 78013241.65854444 37994067.71862565 12898298.99218661
 90209018.95065837 43241454.64083755 16185653.82662157 53086584.80672697
 42569513.01126787 34900040.52352523]
Done!
ene_coms = [0.00865527 0.00717946 0.00884899 0.00797023 0.00723884 0.00743604
 0.00793871 0.00749294 0.01147928 0.00737719]
ene_comp = [1.11621458e-05 9.67684474e-05 1.07400059e-05 4.29224582e-07
 1.49407568e-04 1.63796530e-05 8.48800263e-07 3.02673121e-05
 1.41348748e-05 8.62353083e-06]
ene_total = [0.00866643 0.00727623 0.00885973 0.00797066 0.00738824 0.00745242
 0.00793956 0.00752321 0.01149342 0.00738581]
At round 29 energy consumption: 0.08195570742412175
At round 29 eta: 0.6819493529625246
At round 29 a_n: 18.248773294144172
At round 29 local rounds: 12.534813137868813
At round 29 global rounds: 57.376941264307334
gradient difference: 0.44463348388671875
train() client id: f_00000-0-0 loss: 1.391258  [   32/  126]
train() client id: f_00000-0-1 loss: 1.297944  [   64/  126]
train() client id: f_00000-0-2 loss: 1.217042  [   96/  126]
train() client id: f_00000-1-0 loss: 1.311098  [   32/  126]
train() client id: f_00000-1-1 loss: 1.150984  [   64/  126]
train() client id: f_00000-1-2 loss: 1.149737  [   96/  126]
train() client id: f_00000-2-0 loss: 1.058719  [   32/  126]
train() client id: f_00000-2-1 loss: 1.108868  [   64/  126]
train() client id: f_00000-2-2 loss: 1.249737  [   96/  126]
train() client id: f_00000-3-0 loss: 1.006092  [   32/  126]
train() client id: f_00000-3-1 loss: 0.927983  [   64/  126]
train() client id: f_00000-3-2 loss: 1.196726  [   96/  126]
train() client id: f_00000-4-0 loss: 1.048174  [   32/  126]
train() client id: f_00000-4-1 loss: 1.030439  [   64/  126]
train() client id: f_00000-4-2 loss: 0.941255  [   96/  126]
train() client id: f_00000-5-0 loss: 0.943572  [   32/  126]
train() client id: f_00000-5-1 loss: 1.054355  [   64/  126]
train() client id: f_00000-5-2 loss: 0.919809  [   96/  126]
train() client id: f_00000-6-0 loss: 0.965770  [   32/  126]
train() client id: f_00000-6-1 loss: 0.866637  [   64/  126]
train() client id: f_00000-6-2 loss: 1.008282  [   96/  126]
train() client id: f_00000-7-0 loss: 1.010867  [   32/  126]
train() client id: f_00000-7-1 loss: 0.956166  [   64/  126]
train() client id: f_00000-7-2 loss: 0.829726  [   96/  126]
train() client id: f_00000-8-0 loss: 0.876278  [   32/  126]
train() client id: f_00000-8-1 loss: 0.943805  [   64/  126]
train() client id: f_00000-8-2 loss: 0.844791  [   96/  126]
train() client id: f_00000-9-0 loss: 0.938233  [   32/  126]
train() client id: f_00000-9-1 loss: 0.858314  [   64/  126]
train() client id: f_00000-9-2 loss: 0.886268  [   96/  126]
train() client id: f_00000-10-0 loss: 0.888659  [   32/  126]
train() client id: f_00000-10-1 loss: 0.831943  [   64/  126]
train() client id: f_00000-10-2 loss: 1.014189  [   96/  126]
train() client id: f_00000-11-0 loss: 0.929638  [   32/  126]
train() client id: f_00000-11-1 loss: 0.794942  [   64/  126]
train() client id: f_00000-11-2 loss: 0.877222  [   96/  126]
train() client id: f_00001-0-0 loss: 0.532754  [   32/  265]
train() client id: f_00001-0-1 loss: 0.512366  [   64/  265]
train() client id: f_00001-0-2 loss: 0.671103  [   96/  265]
train() client id: f_00001-0-3 loss: 0.442203  [  128/  265]
train() client id: f_00001-0-4 loss: 0.450866  [  160/  265]
train() client id: f_00001-0-5 loss: 0.451343  [  192/  265]
train() client id: f_00001-0-6 loss: 0.451434  [  224/  265]
train() client id: f_00001-0-7 loss: 0.429474  [  256/  265]
train() client id: f_00001-1-0 loss: 0.540887  [   32/  265]
train() client id: f_00001-1-1 loss: 0.642024  [   64/  265]
train() client id: f_00001-1-2 loss: 0.463742  [   96/  265]
train() client id: f_00001-1-3 loss: 0.398071  [  128/  265]
train() client id: f_00001-1-4 loss: 0.408293  [  160/  265]
train() client id: f_00001-1-5 loss: 0.533426  [  192/  265]
train() client id: f_00001-1-6 loss: 0.437127  [  224/  265]
train() client id: f_00001-1-7 loss: 0.447698  [  256/  265]
train() client id: f_00001-2-0 loss: 0.395400  [   32/  265]
train() client id: f_00001-2-1 loss: 0.528265  [   64/  265]
train() client id: f_00001-2-2 loss: 0.488967  [   96/  265]
train() client id: f_00001-2-3 loss: 0.437395  [  128/  265]
train() client id: f_00001-2-4 loss: 0.450730  [  160/  265]
train() client id: f_00001-2-5 loss: 0.460595  [  192/  265]
train() client id: f_00001-2-6 loss: 0.580330  [  224/  265]
train() client id: f_00001-2-7 loss: 0.488753  [  256/  265]
train() client id: f_00001-3-0 loss: 0.440231  [   32/  265]
train() client id: f_00001-3-1 loss: 0.484012  [   64/  265]
train() client id: f_00001-3-2 loss: 0.519915  [   96/  265]
train() client id: f_00001-3-3 loss: 0.471280  [  128/  265]
train() client id: f_00001-3-4 loss: 0.451162  [  160/  265]
train() client id: f_00001-3-5 loss: 0.565019  [  192/  265]
train() client id: f_00001-3-6 loss: 0.388806  [  224/  265]
train() client id: f_00001-3-7 loss: 0.489807  [  256/  265]
train() client id: f_00001-4-0 loss: 0.588030  [   32/  265]
train() client id: f_00001-4-1 loss: 0.391521  [   64/  265]
train() client id: f_00001-4-2 loss: 0.505434  [   96/  265]
train() client id: f_00001-4-3 loss: 0.436879  [  128/  265]
train() client id: f_00001-4-4 loss: 0.430290  [  160/  265]
train() client id: f_00001-4-5 loss: 0.481141  [  192/  265]
train() client id: f_00001-4-6 loss: 0.548098  [  224/  265]
train() client id: f_00001-4-7 loss: 0.400610  [  256/  265]
train() client id: f_00001-5-0 loss: 0.439618  [   32/  265]
train() client id: f_00001-5-1 loss: 0.483604  [   64/  265]
train() client id: f_00001-5-2 loss: 0.485348  [   96/  265]
train() client id: f_00001-5-3 loss: 0.405565  [  128/  265]
train() client id: f_00001-5-4 loss: 0.390239  [  160/  265]
train() client id: f_00001-5-5 loss: 0.470206  [  192/  265]
train() client id: f_00001-5-6 loss: 0.575276  [  224/  265]
train() client id: f_00001-5-7 loss: 0.521318  [  256/  265]
train() client id: f_00001-6-0 loss: 0.403314  [   32/  265]
train() client id: f_00001-6-1 loss: 0.506749  [   64/  265]
train() client id: f_00001-6-2 loss: 0.444324  [   96/  265]
train() client id: f_00001-6-3 loss: 0.409772  [  128/  265]
train() client id: f_00001-6-4 loss: 0.435627  [  160/  265]
train() client id: f_00001-6-5 loss: 0.441958  [  192/  265]
train() client id: f_00001-6-6 loss: 0.402094  [  224/  265]
train() client id: f_00001-6-7 loss: 0.657839  [  256/  265]
train() client id: f_00001-7-0 loss: 0.510664  [   32/  265]
train() client id: f_00001-7-1 loss: 0.520121  [   64/  265]
train() client id: f_00001-7-2 loss: 0.375795  [   96/  265]
train() client id: f_00001-7-3 loss: 0.441153  [  128/  265]
train() client id: f_00001-7-4 loss: 0.389658  [  160/  265]
train() client id: f_00001-7-5 loss: 0.379641  [  192/  265]
train() client id: f_00001-7-6 loss: 0.597308  [  224/  265]
train() client id: f_00001-7-7 loss: 0.438959  [  256/  265]
train() client id: f_00001-8-0 loss: 0.624131  [   32/  265]
train() client id: f_00001-8-1 loss: 0.439542  [   64/  265]
train() client id: f_00001-8-2 loss: 0.365960  [   96/  265]
train() client id: f_00001-8-3 loss: 0.435657  [  128/  265]
train() client id: f_00001-8-4 loss: 0.457810  [  160/  265]
train() client id: f_00001-8-5 loss: 0.579348  [  192/  265]
train() client id: f_00001-8-6 loss: 0.462607  [  224/  265]
train() client id: f_00001-8-7 loss: 0.374699  [  256/  265]
train() client id: f_00001-9-0 loss: 0.495486  [   32/  265]
train() client id: f_00001-9-1 loss: 0.530353  [   64/  265]
train() client id: f_00001-9-2 loss: 0.421429  [   96/  265]
train() client id: f_00001-9-3 loss: 0.502731  [  128/  265]
train() client id: f_00001-9-4 loss: 0.513119  [  160/  265]
train() client id: f_00001-9-5 loss: 0.512246  [  192/  265]
train() client id: f_00001-9-6 loss: 0.402084  [  224/  265]
train() client id: f_00001-9-7 loss: 0.370500  [  256/  265]
train() client id: f_00001-10-0 loss: 0.433452  [   32/  265]
train() client id: f_00001-10-1 loss: 0.505651  [   64/  265]
train() client id: f_00001-10-2 loss: 0.561255  [   96/  265]
train() client id: f_00001-10-3 loss: 0.426101  [  128/  265]
train() client id: f_00001-10-4 loss: 0.499964  [  160/  265]
train() client id: f_00001-10-5 loss: 0.375159  [  192/  265]
train() client id: f_00001-10-6 loss: 0.425167  [  224/  265]
train() client id: f_00001-10-7 loss: 0.434299  [  256/  265]
train() client id: f_00001-11-0 loss: 0.419428  [   32/  265]
train() client id: f_00001-11-1 loss: 0.531401  [   64/  265]
train() client id: f_00001-11-2 loss: 0.519538  [   96/  265]
train() client id: f_00001-11-3 loss: 0.403284  [  128/  265]
train() client id: f_00001-11-4 loss: 0.421009  [  160/  265]
train() client id: f_00001-11-5 loss: 0.469488  [  192/  265]
train() client id: f_00001-11-6 loss: 0.521814  [  224/  265]
train() client id: f_00001-11-7 loss: 0.463617  [  256/  265]
train() client id: f_00002-0-0 loss: 1.223773  [   32/  124]
train() client id: f_00002-0-1 loss: 1.325294  [   64/  124]
train() client id: f_00002-0-2 loss: 1.132384  [   96/  124]
train() client id: f_00002-1-0 loss: 1.174937  [   32/  124]
train() client id: f_00002-1-1 loss: 1.230633  [   64/  124]
train() client id: f_00002-1-2 loss: 1.126951  [   96/  124]
train() client id: f_00002-2-0 loss: 1.087809  [   32/  124]
train() client id: f_00002-2-1 loss: 1.236602  [   64/  124]
train() client id: f_00002-2-2 loss: 1.163647  [   96/  124]
train() client id: f_00002-3-0 loss: 1.280058  [   32/  124]
train() client id: f_00002-3-1 loss: 1.030745  [   64/  124]
train() client id: f_00002-3-2 loss: 1.029489  [   96/  124]
train() client id: f_00002-4-0 loss: 1.054700  [   32/  124]
train() client id: f_00002-4-1 loss: 1.065635  [   64/  124]
train() client id: f_00002-4-2 loss: 1.353076  [   96/  124]
train() client id: f_00002-5-0 loss: 1.150760  [   32/  124]
train() client id: f_00002-5-1 loss: 1.079032  [   64/  124]
train() client id: f_00002-5-2 loss: 1.182068  [   96/  124]
train() client id: f_00002-6-0 loss: 1.090385  [   32/  124]
train() client id: f_00002-6-1 loss: 1.058244  [   64/  124]
train() client id: f_00002-6-2 loss: 1.108016  [   96/  124]
train() client id: f_00002-7-0 loss: 0.940387  [   32/  124]
train() client id: f_00002-7-1 loss: 1.079528  [   64/  124]
train() client id: f_00002-7-2 loss: 1.087971  [   96/  124]
train() client id: f_00002-8-0 loss: 0.969780  [   32/  124]
train() client id: f_00002-8-1 loss: 1.046174  [   64/  124]
train() client id: f_00002-8-2 loss: 1.083498  [   96/  124]
train() client id: f_00002-9-0 loss: 0.993325  [   32/  124]
train() client id: f_00002-9-1 loss: 1.213424  [   64/  124]
train() client id: f_00002-9-2 loss: 0.948411  [   96/  124]
train() client id: f_00002-10-0 loss: 0.955815  [   32/  124]
train() client id: f_00002-10-1 loss: 1.112237  [   64/  124]
train() client id: f_00002-10-2 loss: 1.108630  [   96/  124]
train() client id: f_00002-11-0 loss: 0.971522  [   32/  124]
train() client id: f_00002-11-1 loss: 1.195146  [   64/  124]
train() client id: f_00002-11-2 loss: 1.019961  [   96/  124]
train() client id: f_00003-0-0 loss: 0.883764  [   32/   43]
train() client id: f_00003-1-0 loss: 0.655042  [   32/   43]
train() client id: f_00003-2-0 loss: 0.680761  [   32/   43]
train() client id: f_00003-3-0 loss: 0.809525  [   32/   43]
train() client id: f_00003-4-0 loss: 0.646684  [   32/   43]
train() client id: f_00003-5-0 loss: 0.856999  [   32/   43]
train() client id: f_00003-6-0 loss: 0.606916  [   32/   43]
train() client id: f_00003-7-0 loss: 0.612160  [   32/   43]
train() client id: f_00003-8-0 loss: 0.764612  [   32/   43]
train() client id: f_00003-9-0 loss: 0.690200  [   32/   43]
train() client id: f_00003-10-0 loss: 0.840989  [   32/   43]
train() client id: f_00003-11-0 loss: 0.699085  [   32/   43]
train() client id: f_00004-0-0 loss: 0.718619  [   32/  306]
train() client id: f_00004-0-1 loss: 0.632179  [   64/  306]
train() client id: f_00004-0-2 loss: 0.802573  [   96/  306]
train() client id: f_00004-0-3 loss: 0.919728  [  128/  306]
train() client id: f_00004-0-4 loss: 0.556936  [  160/  306]
train() client id: f_00004-0-5 loss: 0.753145  [  192/  306]
train() client id: f_00004-0-6 loss: 0.785791  [  224/  306]
train() client id: f_00004-0-7 loss: 0.820194  [  256/  306]
train() client id: f_00004-0-8 loss: 0.713454  [  288/  306]
train() client id: f_00004-1-0 loss: 0.831762  [   32/  306]
train() client id: f_00004-1-1 loss: 0.844042  [   64/  306]
train() client id: f_00004-1-2 loss: 0.650414  [   96/  306]
train() client id: f_00004-1-3 loss: 0.645640  [  128/  306]
train() client id: f_00004-1-4 loss: 0.909272  [  160/  306]
train() client id: f_00004-1-5 loss: 0.775401  [  192/  306]
train() client id: f_00004-1-6 loss: 0.665708  [  224/  306]
train() client id: f_00004-1-7 loss: 0.645392  [  256/  306]
train() client id: f_00004-1-8 loss: 0.767159  [  288/  306]
train() client id: f_00004-2-0 loss: 0.689047  [   32/  306]
train() client id: f_00004-2-1 loss: 0.955951  [   64/  306]
train() client id: f_00004-2-2 loss: 0.799589  [   96/  306]
train() client id: f_00004-2-3 loss: 0.803100  [  128/  306]
train() client id: f_00004-2-4 loss: 0.687280  [  160/  306]
train() client id: f_00004-2-5 loss: 0.720826  [  192/  306]
train() client id: f_00004-2-6 loss: 0.669759  [  224/  306]
train() client id: f_00004-2-7 loss: 0.751041  [  256/  306]
train() client id: f_00004-2-8 loss: 0.721461  [  288/  306]
train() client id: f_00004-3-0 loss: 0.678941  [   32/  306]
train() client id: f_00004-3-1 loss: 0.618217  [   64/  306]
train() client id: f_00004-3-2 loss: 0.722982  [   96/  306]
train() client id: f_00004-3-3 loss: 0.676574  [  128/  306]
train() client id: f_00004-3-4 loss: 0.814499  [  160/  306]
train() client id: f_00004-3-5 loss: 0.984330  [  192/  306]
train() client id: f_00004-3-6 loss: 0.602486  [  224/  306]
train() client id: f_00004-3-7 loss: 0.867960  [  256/  306]
train() client id: f_00004-3-8 loss: 0.776528  [  288/  306]
train() client id: f_00004-4-0 loss: 0.855184  [   32/  306]
train() client id: f_00004-4-1 loss: 0.727241  [   64/  306]
train() client id: f_00004-4-2 loss: 0.889936  [   96/  306]
train() client id: f_00004-4-3 loss: 0.638253  [  128/  306]
train() client id: f_00004-4-4 loss: 0.668173  [  160/  306]
train() client id: f_00004-4-5 loss: 0.789998  [  192/  306]
train() client id: f_00004-4-6 loss: 0.676909  [  224/  306]
train() client id: f_00004-4-7 loss: 0.767790  [  256/  306]
train() client id: f_00004-4-8 loss: 0.787610  [  288/  306]
train() client id: f_00004-5-0 loss: 0.685662  [   32/  306]
train() client id: f_00004-5-1 loss: 0.786019  [   64/  306]
train() client id: f_00004-5-2 loss: 0.767569  [   96/  306]
train() client id: f_00004-5-3 loss: 0.699850  [  128/  306]
train() client id: f_00004-5-4 loss: 0.705155  [  160/  306]
train() client id: f_00004-5-5 loss: 0.675984  [  192/  306]
train() client id: f_00004-5-6 loss: 0.884392  [  224/  306]
train() client id: f_00004-5-7 loss: 0.756762  [  256/  306]
train() client id: f_00004-5-8 loss: 0.781589  [  288/  306]
train() client id: f_00004-6-0 loss: 0.726721  [   32/  306]
train() client id: f_00004-6-1 loss: 0.886849  [   64/  306]
train() client id: f_00004-6-2 loss: 0.692311  [   96/  306]
train() client id: f_00004-6-3 loss: 0.785259  [  128/  306]
train() client id: f_00004-6-4 loss: 0.775750  [  160/  306]
train() client id: f_00004-6-5 loss: 0.736077  [  192/  306]
train() client id: f_00004-6-6 loss: 0.721687  [  224/  306]
train() client id: f_00004-6-7 loss: 0.784288  [  256/  306]
train() client id: f_00004-6-8 loss: 0.747476  [  288/  306]
train() client id: f_00004-7-0 loss: 0.797407  [   32/  306]
train() client id: f_00004-7-1 loss: 0.734791  [   64/  306]
train() client id: f_00004-7-2 loss: 0.694475  [   96/  306]
train() client id: f_00004-7-3 loss: 0.796430  [  128/  306]
train() client id: f_00004-7-4 loss: 0.814184  [  160/  306]
train() client id: f_00004-7-5 loss: 0.720258  [  192/  306]
train() client id: f_00004-7-6 loss: 0.780360  [  224/  306]
train() client id: f_00004-7-7 loss: 0.774094  [  256/  306]
train() client id: f_00004-7-8 loss: 0.784836  [  288/  306]
train() client id: f_00004-8-0 loss: 0.703026  [   32/  306]
train() client id: f_00004-8-1 loss: 0.794451  [   64/  306]
train() client id: f_00004-8-2 loss: 0.964753  [   96/  306]
train() client id: f_00004-8-3 loss: 0.753381  [  128/  306]
train() client id: f_00004-8-4 loss: 0.695001  [  160/  306]
train() client id: f_00004-8-5 loss: 0.709390  [  192/  306]
train() client id: f_00004-8-6 loss: 0.785252  [  224/  306]
train() client id: f_00004-8-7 loss: 0.804388  [  256/  306]
train() client id: f_00004-8-8 loss: 0.684236  [  288/  306]
train() client id: f_00004-9-0 loss: 0.747905  [   32/  306]
train() client id: f_00004-9-1 loss: 0.742600  [   64/  306]
train() client id: f_00004-9-2 loss: 0.824217  [   96/  306]
train() client id: f_00004-9-3 loss: 0.779738  [  128/  306]
train() client id: f_00004-9-4 loss: 0.808505  [  160/  306]
train() client id: f_00004-9-5 loss: 0.713273  [  192/  306]
train() client id: f_00004-9-6 loss: 0.913534  [  224/  306]
train() client id: f_00004-9-7 loss: 0.689950  [  256/  306]
train() client id: f_00004-9-8 loss: 0.677230  [  288/  306]
train() client id: f_00004-10-0 loss: 0.702131  [   32/  306]
train() client id: f_00004-10-1 loss: 0.793341  [   64/  306]
train() client id: f_00004-10-2 loss: 0.766714  [   96/  306]
train() client id: f_00004-10-3 loss: 0.868966  [  128/  306]
train() client id: f_00004-10-4 loss: 0.768437  [  160/  306]
train() client id: f_00004-10-5 loss: 0.806918  [  192/  306]
train() client id: f_00004-10-6 loss: 0.718969  [  224/  306]
train() client id: f_00004-10-7 loss: 0.670539  [  256/  306]
train() client id: f_00004-10-8 loss: 0.788556  [  288/  306]
train() client id: f_00004-11-0 loss: 0.675868  [   32/  306]
train() client id: f_00004-11-1 loss: 0.775524  [   64/  306]
train() client id: f_00004-11-2 loss: 0.681110  [   96/  306]
train() client id: f_00004-11-3 loss: 0.909922  [  128/  306]
train() client id: f_00004-11-4 loss: 0.773182  [  160/  306]
train() client id: f_00004-11-5 loss: 0.782190  [  192/  306]
train() client id: f_00004-11-6 loss: 0.756960  [  224/  306]
train() client id: f_00004-11-7 loss: 0.814651  [  256/  306]
train() client id: f_00004-11-8 loss: 0.848572  [  288/  306]
train() client id: f_00005-0-0 loss: 0.668683  [   32/  146]
train() client id: f_00005-0-1 loss: 0.667321  [   64/  146]
train() client id: f_00005-0-2 loss: 0.730792  [   96/  146]
train() client id: f_00005-0-3 loss: 0.696732  [  128/  146]
train() client id: f_00005-1-0 loss: 0.786094  [   32/  146]
train() client id: f_00005-1-1 loss: 0.615559  [   64/  146]
train() client id: f_00005-1-2 loss: 0.816079  [   96/  146]
train() client id: f_00005-1-3 loss: 0.845897  [  128/  146]
train() client id: f_00005-2-0 loss: 0.892366  [   32/  146]
train() client id: f_00005-2-1 loss: 0.955326  [   64/  146]
train() client id: f_00005-2-2 loss: 0.650681  [   96/  146]
train() client id: f_00005-2-3 loss: 0.753570  [  128/  146]
train() client id: f_00005-3-0 loss: 0.882331  [   32/  146]
train() client id: f_00005-3-1 loss: 0.665962  [   64/  146]
train() client id: f_00005-3-2 loss: 0.777218  [   96/  146]
train() client id: f_00005-3-3 loss: 0.736913  [  128/  146]
train() client id: f_00005-4-0 loss: 0.790828  [   32/  146]
train() client id: f_00005-4-1 loss: 0.589128  [   64/  146]
train() client id: f_00005-4-2 loss: 0.773373  [   96/  146]
train() client id: f_00005-4-3 loss: 0.959460  [  128/  146]
train() client id: f_00005-5-0 loss: 0.893122  [   32/  146]
train() client id: f_00005-5-1 loss: 0.802434  [   64/  146]
train() client id: f_00005-5-2 loss: 0.835829  [   96/  146]
train() client id: f_00005-5-3 loss: 0.778005  [  128/  146]
train() client id: f_00005-6-0 loss: 0.760145  [   32/  146]
train() client id: f_00005-6-1 loss: 0.834218  [   64/  146]
train() client id: f_00005-6-2 loss: 0.872800  [   96/  146]
train() client id: f_00005-6-3 loss: 0.601468  [  128/  146]
train() client id: f_00005-7-0 loss: 0.520643  [   32/  146]
train() client id: f_00005-7-1 loss: 0.653897  [   64/  146]
train() client id: f_00005-7-2 loss: 0.929380  [   96/  146]
train() client id: f_00005-7-3 loss: 1.078136  [  128/  146]
train() client id: f_00005-8-0 loss: 0.619367  [   32/  146]
train() client id: f_00005-8-1 loss: 1.008702  [   64/  146]
train() client id: f_00005-8-2 loss: 0.779893  [   96/  146]
train() client id: f_00005-8-3 loss: 0.798164  [  128/  146]
train() client id: f_00005-9-0 loss: 0.890706  [   32/  146]
train() client id: f_00005-9-1 loss: 0.691543  [   64/  146]
train() client id: f_00005-9-2 loss: 0.818470  [   96/  146]
train() client id: f_00005-9-3 loss: 0.783107  [  128/  146]
train() client id: f_00005-10-0 loss: 0.890743  [   32/  146]
train() client id: f_00005-10-1 loss: 0.616218  [   64/  146]
train() client id: f_00005-10-2 loss: 0.976396  [   96/  146]
train() client id: f_00005-10-3 loss: 0.771217  [  128/  146]
train() client id: f_00005-11-0 loss: 0.827924  [   32/  146]
train() client id: f_00005-11-1 loss: 0.839581  [   64/  146]
train() client id: f_00005-11-2 loss: 0.674899  [   96/  146]
train() client id: f_00005-11-3 loss: 0.672419  [  128/  146]
train() client id: f_00006-0-0 loss: 0.578130  [   32/   54]
train() client id: f_00006-1-0 loss: 0.523778  [   32/   54]
train() client id: f_00006-2-0 loss: 0.509846  [   32/   54]
train() client id: f_00006-3-0 loss: 0.549242  [   32/   54]
train() client id: f_00006-4-0 loss: 0.603793  [   32/   54]
train() client id: f_00006-5-0 loss: 0.592118  [   32/   54]
train() client id: f_00006-6-0 loss: 0.500501  [   32/   54]
train() client id: f_00006-7-0 loss: 0.532853  [   32/   54]
train() client id: f_00006-8-0 loss: 0.584035  [   32/   54]
train() client id: f_00006-9-0 loss: 0.547654  [   32/   54]
train() client id: f_00006-10-0 loss: 0.503055  [   32/   54]
train() client id: f_00006-11-0 loss: 0.563756  [   32/   54]
train() client id: f_00007-0-0 loss: 0.506568  [   32/  179]
train() client id: f_00007-0-1 loss: 0.791980  [   64/  179]
train() client id: f_00007-0-2 loss: 0.524418  [   96/  179]
train() client id: f_00007-0-3 loss: 0.477281  [  128/  179]
train() client id: f_00007-0-4 loss: 0.650977  [  160/  179]
train() client id: f_00007-1-0 loss: 0.601926  [   32/  179]
train() client id: f_00007-1-1 loss: 0.455178  [   64/  179]
train() client id: f_00007-1-2 loss: 0.508642  [   96/  179]
train() client id: f_00007-1-3 loss: 0.534349  [  128/  179]
train() client id: f_00007-1-4 loss: 0.690702  [  160/  179]
train() client id: f_00007-2-0 loss: 0.425366  [   32/  179]
train() client id: f_00007-2-1 loss: 0.431169  [   64/  179]
train() client id: f_00007-2-2 loss: 0.612565  [   96/  179]
train() client id: f_00007-2-3 loss: 0.698170  [  128/  179]
train() client id: f_00007-2-4 loss: 0.599117  [  160/  179]
train() client id: f_00007-3-0 loss: 0.674874  [   32/  179]
train() client id: f_00007-3-1 loss: 0.558942  [   64/  179]
train() client id: f_00007-3-2 loss: 0.781085  [   96/  179]
train() client id: f_00007-3-3 loss: 0.424670  [  128/  179]
train() client id: f_00007-3-4 loss: 0.457703  [  160/  179]
train() client id: f_00007-4-0 loss: 0.651943  [   32/  179]
train() client id: f_00007-4-1 loss: 0.629772  [   64/  179]
train() client id: f_00007-4-2 loss: 0.509599  [   96/  179]
train() client id: f_00007-4-3 loss: 0.486262  [  128/  179]
train() client id: f_00007-4-4 loss: 0.553749  [  160/  179]
train() client id: f_00007-5-0 loss: 0.692961  [   32/  179]
train() client id: f_00007-5-1 loss: 0.496102  [   64/  179]
train() client id: f_00007-5-2 loss: 0.565368  [   96/  179]
train() client id: f_00007-5-3 loss: 0.528913  [  128/  179]
train() client id: f_00007-5-4 loss: 0.556854  [  160/  179]
train() client id: f_00007-6-0 loss: 0.722206  [   32/  179]
train() client id: f_00007-6-1 loss: 0.516112  [   64/  179]
train() client id: f_00007-6-2 loss: 0.516531  [   96/  179]
train() client id: f_00007-6-3 loss: 0.392328  [  128/  179]
train() client id: f_00007-6-4 loss: 0.586103  [  160/  179]
train() client id: f_00007-7-0 loss: 0.493270  [   32/  179]
train() client id: f_00007-7-1 loss: 0.505522  [   64/  179]
train() client id: f_00007-7-2 loss: 0.787221  [   96/  179]
train() client id: f_00007-7-3 loss: 0.395537  [  128/  179]
train() client id: f_00007-7-4 loss: 0.401571  [  160/  179]
train() client id: f_00007-8-0 loss: 0.589376  [   32/  179]
train() client id: f_00007-8-1 loss: 0.675330  [   64/  179]
train() client id: f_00007-8-2 loss: 0.632821  [   96/  179]
train() client id: f_00007-8-3 loss: 0.518114  [  128/  179]
train() client id: f_00007-8-4 loss: 0.384843  [  160/  179]
train() client id: f_00007-9-0 loss: 0.506552  [   32/  179]
train() client id: f_00007-9-1 loss: 0.707297  [   64/  179]
train() client id: f_00007-9-2 loss: 0.482150  [   96/  179]
train() client id: f_00007-9-3 loss: 0.405133  [  128/  179]
train() client id: f_00007-9-4 loss: 0.614938  [  160/  179]
train() client id: f_00007-10-0 loss: 0.507630  [   32/  179]
train() client id: f_00007-10-1 loss: 0.563081  [   64/  179]
train() client id: f_00007-10-2 loss: 0.511743  [   96/  179]
train() client id: f_00007-10-3 loss: 0.725251  [  128/  179]
train() client id: f_00007-10-4 loss: 0.383232  [  160/  179]
train() client id: f_00007-11-0 loss: 0.393149  [   32/  179]
train() client id: f_00007-11-1 loss: 0.490223  [   64/  179]
train() client id: f_00007-11-2 loss: 0.512407  [   96/  179]
train() client id: f_00007-11-3 loss: 0.810784  [  128/  179]
train() client id: f_00007-11-4 loss: 0.388657  [  160/  179]
train() client id: f_00008-0-0 loss: 0.801393  [   32/  130]
train() client id: f_00008-0-1 loss: 0.654021  [   64/  130]
train() client id: f_00008-0-2 loss: 0.757320  [   96/  130]
train() client id: f_00008-0-3 loss: 0.674624  [  128/  130]
train() client id: f_00008-1-0 loss: 0.709306  [   32/  130]
train() client id: f_00008-1-1 loss: 0.635832  [   64/  130]
train() client id: f_00008-1-2 loss: 0.744730  [   96/  130]
train() client id: f_00008-1-3 loss: 0.789700  [  128/  130]
train() client id: f_00008-2-0 loss: 0.816254  [   32/  130]
train() client id: f_00008-2-1 loss: 0.723831  [   64/  130]
train() client id: f_00008-2-2 loss: 0.681478  [   96/  130]
train() client id: f_00008-2-3 loss: 0.648964  [  128/  130]
train() client id: f_00008-3-0 loss: 0.635457  [   32/  130]
train() client id: f_00008-3-1 loss: 0.866011  [   64/  130]
train() client id: f_00008-3-2 loss: 0.603570  [   96/  130]
train() client id: f_00008-3-3 loss: 0.738176  [  128/  130]
train() client id: f_00008-4-0 loss: 0.654959  [   32/  130]
train() client id: f_00008-4-1 loss: 0.690344  [   64/  130]
train() client id: f_00008-4-2 loss: 0.772975  [   96/  130]
train() client id: f_00008-4-3 loss: 0.749694  [  128/  130]
train() client id: f_00008-5-0 loss: 0.702117  [   32/  130]
train() client id: f_00008-5-1 loss: 0.631672  [   64/  130]
train() client id: f_00008-5-2 loss: 0.644471  [   96/  130]
train() client id: f_00008-5-3 loss: 0.855866  [  128/  130]
train() client id: f_00008-6-0 loss: 0.653680  [   32/  130]
train() client id: f_00008-6-1 loss: 0.840714  [   64/  130]
train() client id: f_00008-6-2 loss: 0.629807  [   96/  130]
train() client id: f_00008-6-3 loss: 0.729062  [  128/  130]
train() client id: f_00008-7-0 loss: 0.684627  [   32/  130]
train() client id: f_00008-7-1 loss: 0.824588  [   64/  130]
train() client id: f_00008-7-2 loss: 0.722208  [   96/  130]
train() client id: f_00008-7-3 loss: 0.566369  [  128/  130]
train() client id: f_00008-8-0 loss: 0.682542  [   32/  130]
train() client id: f_00008-8-1 loss: 0.550697  [   64/  130]
train() client id: f_00008-8-2 loss: 0.833713  [   96/  130]
train() client id: f_00008-8-3 loss: 0.764068  [  128/  130]
train() client id: f_00008-9-0 loss: 0.763062  [   32/  130]
train() client id: f_00008-9-1 loss: 0.651924  [   64/  130]
train() client id: f_00008-9-2 loss: 0.809586  [   96/  130]
train() client id: f_00008-9-3 loss: 0.598310  [  128/  130]
train() client id: f_00008-10-0 loss: 0.712203  [   32/  130]
train() client id: f_00008-10-1 loss: 0.685783  [   64/  130]
train() client id: f_00008-10-2 loss: 0.739644  [   96/  130]
train() client id: f_00008-10-3 loss: 0.725030  [  128/  130]
train() client id: f_00008-11-0 loss: 0.688615  [   32/  130]
train() client id: f_00008-11-1 loss: 0.698515  [   64/  130]
train() client id: f_00008-11-2 loss: 0.689348  [   96/  130]
train() client id: f_00008-11-3 loss: 0.777139  [  128/  130]
train() client id: f_00009-0-0 loss: 0.985312  [   32/  118]
train() client id: f_00009-0-1 loss: 1.153857  [   64/  118]
train() client id: f_00009-0-2 loss: 1.306140  [   96/  118]
train() client id: f_00009-1-0 loss: 1.049698  [   32/  118]
train() client id: f_00009-1-1 loss: 1.085308  [   64/  118]
train() client id: f_00009-1-2 loss: 1.083330  [   96/  118]
train() client id: f_00009-2-0 loss: 0.961860  [   32/  118]
train() client id: f_00009-2-1 loss: 1.079330  [   64/  118]
train() client id: f_00009-2-2 loss: 1.039762  [   96/  118]
train() client id: f_00009-3-0 loss: 1.067496  [   32/  118]
train() client id: f_00009-3-1 loss: 0.930142  [   64/  118]
train() client id: f_00009-3-2 loss: 0.979827  [   96/  118]
train() client id: f_00009-4-0 loss: 0.962648  [   32/  118]
train() client id: f_00009-4-1 loss: 0.962303  [   64/  118]
train() client id: f_00009-4-2 loss: 0.967242  [   96/  118]
train() client id: f_00009-5-0 loss: 0.887641  [   32/  118]
train() client id: f_00009-5-1 loss: 0.821080  [   64/  118]
train() client id: f_00009-5-2 loss: 1.080994  [   96/  118]
train() client id: f_00009-6-0 loss: 0.872716  [   32/  118]
train() client id: f_00009-6-1 loss: 0.832664  [   64/  118]
train() client id: f_00009-6-2 loss: 1.028173  [   96/  118]
train() client id: f_00009-7-0 loss: 0.873878  [   32/  118]
train() client id: f_00009-7-1 loss: 0.875213  [   64/  118]
train() client id: f_00009-7-2 loss: 0.875419  [   96/  118]
train() client id: f_00009-8-0 loss: 0.758937  [   32/  118]
train() client id: f_00009-8-1 loss: 0.960201  [   64/  118]
train() client id: f_00009-8-2 loss: 0.946834  [   96/  118]
train() client id: f_00009-9-0 loss: 0.809536  [   32/  118]
train() client id: f_00009-9-1 loss: 0.880101  [   64/  118]
train() client id: f_00009-9-2 loss: 0.902821  [   96/  118]
train() client id: f_00009-10-0 loss: 0.874345  [   32/  118]
train() client id: f_00009-10-1 loss: 0.927572  [   64/  118]
train() client id: f_00009-10-2 loss: 0.793191  [   96/  118]
train() client id: f_00009-11-0 loss: 0.829819  [   32/  118]
train() client id: f_00009-11-1 loss: 0.897989  [   64/  118]
train() client id: f_00009-11-2 loss: 0.900450  [   96/  118]
At round 29 accuracy: 0.6445623342175066
At round 29 training accuracy: 0.5841716968477532
At round 29 training loss: 0.8294670983538509
update_location
xs = -3.905658 4.200318 165.009024 18.811294 0.979296 3.956410 -127.443192 -106.324852 149.663977 -92.060879 
ys = 157.587959 140.555839 1.320614 -127.455176 119.350187 102.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 186.679456 172.550243 192.950051 163.091038 155.709428 143.479625 162.014375 145.964552 180.853465 135.982415 
dists_bs = 171.978059 182.480558 381.799248 359.224096 184.568725 192.966820 183.881171 187.227483 360.820199 190.051022 
uav_gains = -106.848255 -105.950259 -107.240184 -105.324237 -104.815300 -103.922273 -105.251230 -104.109353 -106.481738 -103.338285 
bs_gains = -102.160304 -102.881124 -111.858441 -111.117292 -103.019486 -103.560573 -102.974102 -103.193408 -111.171203 -103.375425 
Round 30
-------------------------------
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.03213939 14.58505974  6.92666737  2.4917993  16.82243501  8.09793447
  3.09128315  9.90171672  7.30136856  6.56936016]
obj_prev = 82.8197638713389
eta_min = 6.720783557496832e-14	eta_max = 0.9260472326332857
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 19.233152927962998	eta = 0.9090909090909091
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 34.518956732515555	eta = 0.5065241286245609
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 27.08219396594083	eta = 0.6456155103960739
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 25.74057952697607	eta = 0.6792653779081567
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 25.671393607462342	eta = 0.6810960381552402
af = 17.48468447996636	bf = 1.4716185540757498	zeta = 25.671195205593733	eta = 0.6811013020600015
eta = 0.6811013020600015
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [0.03169041 0.06665046 0.03118739 0.01081498 0.07696242 0.03672063
 0.0135816  0.0450205  0.03269645 0.02967832]
ene_total = [2.2727313  4.14429542 2.25574611 1.06104318 4.72594168 2.47696442
 1.21464967 2.95433273 2.48764402 2.07784665]
ti_comp = [0.41970979 0.43599649 0.41770452 0.42674957 0.43552852 0.43363939
 0.42705945 0.43162177 0.39149563 0.43429663]
ti_coms = [0.08785512 0.07156842 0.08986039 0.08081534 0.07203639 0.07392552
 0.08050546 0.07594314 0.11606928 0.07326828]
t_total = [28.49987411 28.49987411 28.49987411 28.49987411 28.49987411 28.49987411
 28.49987411 28.49987411 28.49987411 28.49987411]
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [1.12918568e-05 9.73472227e-05 1.08662233e-05 4.34121306e-07
 1.50204523e-04 1.64570901e-05 8.58532087e-07 3.06128901e-05
 1.42537176e-05 8.66213655e-06]
ene_total = [0.49394235 0.40732423 0.50517807 0.45380416 0.41291985 0.4160174
 0.45208802 0.42814121 0.65253176 0.41188928]
optimize_network iter = 0 obj = 4.633836318316938
eta = 0.6811013020600015
freqs = [37752763.88864101 76434633.93405241 37331875.99521733 12671343.57363147
 88355204.53366868 42340057.1595814  15901303.6163569  52152723.73147339
 41758389.65718008 34168259.23522189]
eta_min = 0.6811013020600248	eta_max = 0.6811013020600005
af = 0.015898726867838657	bf = 1.4716185540757498	zeta = 0.017488599554622525	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [2.80235665e-06 2.41591478e-05 2.69672506e-06 1.07738059e-07
 3.72770088e-05 4.08423848e-06 2.13066208e-07 7.59735426e-06
 3.53741648e-06 2.14972581e-06]
ene_total = [1.73102112 1.4144311  1.77049778 1.59182881 1.42623245 1.45690442
 1.58574596 1.49733708 2.28689603 1.44357775]
ti_comp = [0.41970979 0.43599649 0.41770452 0.42674957 0.43552852 0.43363939
 0.42705945 0.43162177 0.39149563 0.43429663]
ti_coms = [0.08785512 0.07156842 0.08986039 0.08081534 0.07203639 0.07392552
 0.08050546 0.07594314 0.11606928 0.07326828]
t_total = [28.49987411 28.49987411 28.49987411 28.49987411 28.49987411 28.49987411
 28.49987411 28.49987411 28.49987411 28.49987411]
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [1.12918568e-05 9.73472227e-05 1.08662233e-05 4.34121306e-07
 1.50204523e-04 1.64570901e-05 8.58532087e-07 3.06128901e-05
 1.42537176e-05 8.66213655e-06]
ene_total = [0.49394235 0.40732423 0.50517807 0.45380416 0.41291985 0.4160174
 0.45208802 0.42814121 0.65253176 0.41188928]
optimize_network iter = 1 obj = 4.633836318316922
eta = 0.6811013020600005
freqs = [37752763.88864101 76434633.93405241 37331875.99521733 12671343.57363147
 88355204.53366868 42340057.1595814  15901303.6163569  52152723.73147339
 41758389.65718006 34168259.23522189]
Done!
ene_coms = [0.00878551 0.00715684 0.00898604 0.00808153 0.00720364 0.00739255
 0.00805055 0.00759431 0.01160693 0.00732683]
ene_comp = [1.07750501e-05 9.28918269e-05 1.03688971e-05 4.14252406e-07
 1.43329950e-04 1.57038807e-05 8.19238720e-07 2.92117968e-05
 1.36013522e-05 8.26568717e-06]
ene_total = [0.00879629 0.00724973 0.00899641 0.00808195 0.00734697 0.00740826
 0.00805137 0.00762353 0.01162053 0.00733509]
At round 30 energy consumption: 0.08251011673575459
At round 30 eta: 0.6811013020600005
At round 30 a_n: 17.906227447174857
At round 30 local rounds: 12.575559237281574
At round 30 global rounds: 56.15020557576531
gradient difference: 0.41903597116470337
train() client id: f_00000-0-0 loss: 1.233225  [   32/  126]
train() client id: f_00000-0-1 loss: 1.228472  [   64/  126]
train() client id: f_00000-0-2 loss: 1.260599  [   96/  126]
train() client id: f_00000-1-0 loss: 0.972005  [   32/  126]
train() client id: f_00000-1-1 loss: 1.258075  [   64/  126]
train() client id: f_00000-1-2 loss: 0.985977  [   96/  126]
train() client id: f_00000-2-0 loss: 0.940670  [   32/  126]
train() client id: f_00000-2-1 loss: 0.945654  [   64/  126]
train() client id: f_00000-2-2 loss: 1.231777  [   96/  126]
train() client id: f_00000-3-0 loss: 0.911595  [   32/  126]
train() client id: f_00000-3-1 loss: 1.084769  [   64/  126]
train() client id: f_00000-3-2 loss: 0.898542  [   96/  126]
train() client id: f_00000-4-0 loss: 1.047623  [   32/  126]
train() client id: f_00000-4-1 loss: 0.833409  [   64/  126]
train() client id: f_00000-4-2 loss: 0.910392  [   96/  126]
train() client id: f_00000-5-0 loss: 0.877772  [   32/  126]
train() client id: f_00000-5-1 loss: 0.966531  [   64/  126]
train() client id: f_00000-5-2 loss: 0.910923  [   96/  126]
train() client id: f_00000-6-0 loss: 0.813792  [   32/  126]
train() client id: f_00000-6-1 loss: 0.882596  [   64/  126]
train() client id: f_00000-6-2 loss: 0.906073  [   96/  126]
train() client id: f_00000-7-0 loss: 0.886900  [   32/  126]
train() client id: f_00000-7-1 loss: 0.738410  [   64/  126]
train() client id: f_00000-7-2 loss: 0.875870  [   96/  126]
train() client id: f_00000-8-0 loss: 0.948775  [   32/  126]
train() client id: f_00000-8-1 loss: 0.822220  [   64/  126]
train() client id: f_00000-8-2 loss: 0.849081  [   96/  126]
train() client id: f_00000-9-0 loss: 0.968502  [   32/  126]
train() client id: f_00000-9-1 loss: 0.937247  [   64/  126]
train() client id: f_00000-9-2 loss: 0.731291  [   96/  126]
train() client id: f_00000-10-0 loss: 0.853454  [   32/  126]
train() client id: f_00000-10-1 loss: 0.843753  [   64/  126]
train() client id: f_00000-10-2 loss: 0.920112  [   96/  126]
train() client id: f_00000-11-0 loss: 0.897248  [   32/  126]
train() client id: f_00000-11-1 loss: 0.784089  [   64/  126]
train() client id: f_00000-11-2 loss: 0.868619  [   96/  126]
train() client id: f_00001-0-0 loss: 0.506592  [   32/  265]
train() client id: f_00001-0-1 loss: 0.406908  [   64/  265]
train() client id: f_00001-0-2 loss: 0.412324  [   96/  265]
train() client id: f_00001-0-3 loss: 0.395421  [  128/  265]
train() client id: f_00001-0-4 loss: 0.445815  [  160/  265]
train() client id: f_00001-0-5 loss: 0.674533  [  192/  265]
train() client id: f_00001-0-6 loss: 0.456189  [  224/  265]
train() client id: f_00001-0-7 loss: 0.479556  [  256/  265]
train() client id: f_00001-1-0 loss: 0.407140  [   32/  265]
train() client id: f_00001-1-1 loss: 0.413557  [   64/  265]
train() client id: f_00001-1-2 loss: 0.426778  [   96/  265]
train() client id: f_00001-1-3 loss: 0.403081  [  128/  265]
train() client id: f_00001-1-4 loss: 0.382818  [  160/  265]
train() client id: f_00001-1-5 loss: 0.633766  [  192/  265]
train() client id: f_00001-1-6 loss: 0.492710  [  224/  265]
train() client id: f_00001-1-7 loss: 0.448780  [  256/  265]
train() client id: f_00001-2-0 loss: 0.413071  [   32/  265]
train() client id: f_00001-2-1 loss: 0.570397  [   64/  265]
train() client id: f_00001-2-2 loss: 0.364915  [   96/  265]
train() client id: f_00001-2-3 loss: 0.417536  [  128/  265]
train() client id: f_00001-2-4 loss: 0.404508  [  160/  265]
train() client id: f_00001-2-5 loss: 0.463930  [  192/  265]
train() client id: f_00001-2-6 loss: 0.394466  [  224/  265]
train() client id: f_00001-2-7 loss: 0.423065  [  256/  265]
train() client id: f_00001-3-0 loss: 0.536736  [   32/  265]
train() client id: f_00001-3-1 loss: 0.374939  [   64/  265]
train() client id: f_00001-3-2 loss: 0.561057  [   96/  265]
train() client id: f_00001-3-3 loss: 0.397461  [  128/  265]
train() client id: f_00001-3-4 loss: 0.425281  [  160/  265]
train() client id: f_00001-3-5 loss: 0.354689  [  192/  265]
train() client id: f_00001-3-6 loss: 0.478073  [  224/  265]
train() client id: f_00001-3-7 loss: 0.493659  [  256/  265]
train() client id: f_00001-4-0 loss: 0.491342  [   32/  265]
train() client id: f_00001-4-1 loss: 0.416357  [   64/  265]
train() client id: f_00001-4-2 loss: 0.451486  [   96/  265]
train() client id: f_00001-4-3 loss: 0.496516  [  128/  265]
train() client id: f_00001-4-4 loss: 0.346766  [  160/  265]
train() client id: f_00001-4-5 loss: 0.417341  [  192/  265]
train() client id: f_00001-4-6 loss: 0.486626  [  224/  265]
train() client id: f_00001-4-7 loss: 0.435260  [  256/  265]
train() client id: f_00001-5-0 loss: 0.343507  [   32/  265]
train() client id: f_00001-5-1 loss: 0.419294  [   64/  265]
train() client id: f_00001-5-2 loss: 0.404193  [   96/  265]
train() client id: f_00001-5-3 loss: 0.421987  [  128/  265]
train() client id: f_00001-5-4 loss: 0.584333  [  160/  265]
train() client id: f_00001-5-5 loss: 0.531277  [  192/  265]
train() client id: f_00001-5-6 loss: 0.475781  [  224/  265]
train() client id: f_00001-5-7 loss: 0.380254  [  256/  265]
train() client id: f_00001-6-0 loss: 0.574634  [   32/  265]
train() client id: f_00001-6-1 loss: 0.429792  [   64/  265]
train() client id: f_00001-6-2 loss: 0.477092  [   96/  265]
train() client id: f_00001-6-3 loss: 0.477246  [  128/  265]
train() client id: f_00001-6-4 loss: 0.345330  [  160/  265]
train() client id: f_00001-6-5 loss: 0.414079  [  192/  265]
train() client id: f_00001-6-6 loss: 0.484512  [  224/  265]
train() client id: f_00001-6-7 loss: 0.338170  [  256/  265]
train() client id: f_00001-7-0 loss: 0.352487  [   32/  265]
train() client id: f_00001-7-1 loss: 0.369930  [   64/  265]
train() client id: f_00001-7-2 loss: 0.503381  [   96/  265]
train() client id: f_00001-7-3 loss: 0.396762  [  128/  265]
train() client id: f_00001-7-4 loss: 0.396173  [  160/  265]
train() client id: f_00001-7-5 loss: 0.484273  [  192/  265]
train() client id: f_00001-7-6 loss: 0.489046  [  224/  265]
train() client id: f_00001-7-7 loss: 0.549787  [  256/  265]
train() client id: f_00001-8-0 loss: 0.422682  [   32/  265]
train() client id: f_00001-8-1 loss: 0.415608  [   64/  265]
train() client id: f_00001-8-2 loss: 0.435112  [   96/  265]
train() client id: f_00001-8-3 loss: 0.413384  [  128/  265]
train() client id: f_00001-8-4 loss: 0.430007  [  160/  265]
train() client id: f_00001-8-5 loss: 0.362076  [  192/  265]
train() client id: f_00001-8-6 loss: 0.423245  [  224/  265]
train() client id: f_00001-8-7 loss: 0.557117  [  256/  265]
train() client id: f_00001-9-0 loss: 0.503417  [   32/  265]
train() client id: f_00001-9-1 loss: 0.629491  [   64/  265]
train() client id: f_00001-9-2 loss: 0.603931  [   96/  265]
train() client id: f_00001-9-3 loss: 0.333960  [  128/  265]
train() client id: f_00001-9-4 loss: 0.395375  [  160/  265]
train() client id: f_00001-9-5 loss: 0.352824  [  192/  265]
train() client id: f_00001-9-6 loss: 0.358814  [  224/  265]
train() client id: f_00001-9-7 loss: 0.343931  [  256/  265]
train() client id: f_00001-10-0 loss: 0.518045  [   32/  265]
train() client id: f_00001-10-1 loss: 0.351412  [   64/  265]
train() client id: f_00001-10-2 loss: 0.483734  [   96/  265]
train() client id: f_00001-10-3 loss: 0.341076  [  128/  265]
train() client id: f_00001-10-4 loss: 0.337842  [  160/  265]
train() client id: f_00001-10-5 loss: 0.437763  [  192/  265]
train() client id: f_00001-10-6 loss: 0.556811  [  224/  265]
train() client id: f_00001-10-7 loss: 0.492286  [  256/  265]
train() client id: f_00001-11-0 loss: 0.344871  [   32/  265]
train() client id: f_00001-11-1 loss: 0.655905  [   64/  265]
train() client id: f_00001-11-2 loss: 0.347692  [   96/  265]
train() client id: f_00001-11-3 loss: 0.489878  [  128/  265]
train() client id: f_00001-11-4 loss: 0.340892  [  160/  265]
train() client id: f_00001-11-5 loss: 0.431395  [  192/  265]
train() client id: f_00001-11-6 loss: 0.544535  [  224/  265]
train() client id: f_00001-11-7 loss: 0.355147  [  256/  265]
train() client id: f_00002-0-0 loss: 1.190776  [   32/  124]
train() client id: f_00002-0-1 loss: 1.226411  [   64/  124]
train() client id: f_00002-0-2 loss: 1.364455  [   96/  124]
train() client id: f_00002-1-0 loss: 1.332262  [   32/  124]
train() client id: f_00002-1-1 loss: 1.288211  [   64/  124]
train() client id: f_00002-1-2 loss: 1.068069  [   96/  124]
train() client id: f_00002-2-0 loss: 1.078874  [   32/  124]
train() client id: f_00002-2-1 loss: 1.199844  [   64/  124]
train() client id: f_00002-2-2 loss: 1.219016  [   96/  124]
train() client id: f_00002-3-0 loss: 0.979934  [   32/  124]
train() client id: f_00002-3-1 loss: 1.196051  [   64/  124]
train() client id: f_00002-3-2 loss: 1.233458  [   96/  124]
train() client id: f_00002-4-0 loss: 1.134737  [   32/  124]
train() client id: f_00002-4-1 loss: 1.123548  [   64/  124]
train() client id: f_00002-4-2 loss: 1.054423  [   96/  124]
train() client id: f_00002-5-0 loss: 1.276360  [   32/  124]
train() client id: f_00002-5-1 loss: 1.107065  [   64/  124]
train() client id: f_00002-5-2 loss: 1.003385  [   96/  124]
train() client id: f_00002-6-0 loss: 1.052665  [   32/  124]
train() client id: f_00002-6-1 loss: 1.022074  [   64/  124]
train() client id: f_00002-6-2 loss: 1.163330  [   96/  124]
train() client id: f_00002-7-0 loss: 1.092713  [   32/  124]
train() client id: f_00002-7-1 loss: 1.019597  [   64/  124]
train() client id: f_00002-7-2 loss: 1.074296  [   96/  124]
train() client id: f_00002-8-0 loss: 0.987048  [   32/  124]
train() client id: f_00002-8-1 loss: 1.025178  [   64/  124]
train() client id: f_00002-8-2 loss: 1.130495  [   96/  124]
train() client id: f_00002-9-0 loss: 0.980053  [   32/  124]
train() client id: f_00002-9-1 loss: 0.961934  [   64/  124]
train() client id: f_00002-9-2 loss: 1.093834  [   96/  124]
train() client id: f_00002-10-0 loss: 0.968907  [   32/  124]
train() client id: f_00002-10-1 loss: 0.987382  [   64/  124]
train() client id: f_00002-10-2 loss: 0.977389  [   96/  124]
train() client id: f_00002-11-0 loss: 1.043809  [   32/  124]
train() client id: f_00002-11-1 loss: 1.009573  [   64/  124]
train() client id: f_00002-11-2 loss: 0.909586  [   96/  124]
train() client id: f_00003-0-0 loss: 0.812992  [   32/   43]
train() client id: f_00003-1-0 loss: 0.785830  [   32/   43]
train() client id: f_00003-2-0 loss: 0.693935  [   32/   43]
train() client id: f_00003-3-0 loss: 0.658595  [   32/   43]
train() client id: f_00003-4-0 loss: 0.616701  [   32/   43]
train() client id: f_00003-5-0 loss: 0.696699  [   32/   43]
train() client id: f_00003-6-0 loss: 0.834413  [   32/   43]
train() client id: f_00003-7-0 loss: 0.887401  [   32/   43]
train() client id: f_00003-8-0 loss: 0.802487  [   32/   43]
train() client id: f_00003-9-0 loss: 0.821905  [   32/   43]
train() client id: f_00003-10-0 loss: 0.766724  [   32/   43]
train() client id: f_00003-11-0 loss: 0.852521  [   32/   43]
train() client id: f_00004-0-0 loss: 0.880914  [   32/  306]
train() client id: f_00004-0-1 loss: 0.768843  [   64/  306]
train() client id: f_00004-0-2 loss: 0.811684  [   96/  306]
train() client id: f_00004-0-3 loss: 0.801539  [  128/  306]
train() client id: f_00004-0-4 loss: 0.730385  [  160/  306]
train() client id: f_00004-0-5 loss: 0.758989  [  192/  306]
train() client id: f_00004-0-6 loss: 0.706288  [  224/  306]
train() client id: f_00004-0-7 loss: 0.709253  [  256/  306]
train() client id: f_00004-0-8 loss: 0.751387  [  288/  306]
train() client id: f_00004-1-0 loss: 0.814384  [   32/  306]
train() client id: f_00004-1-1 loss: 0.585947  [   64/  306]
train() client id: f_00004-1-2 loss: 0.731952  [   96/  306]
train() client id: f_00004-1-3 loss: 0.872239  [  128/  306]
train() client id: f_00004-1-4 loss: 0.676971  [  160/  306]
train() client id: f_00004-1-5 loss: 0.762031  [  192/  306]
train() client id: f_00004-1-6 loss: 0.775053  [  224/  306]
train() client id: f_00004-1-7 loss: 0.863225  [  256/  306]
train() client id: f_00004-1-8 loss: 0.781877  [  288/  306]
train() client id: f_00004-2-0 loss: 0.778064  [   32/  306]
train() client id: f_00004-2-1 loss: 0.710804  [   64/  306]
train() client id: f_00004-2-2 loss: 0.785311  [   96/  306]
train() client id: f_00004-2-3 loss: 0.722699  [  128/  306]
train() client id: f_00004-2-4 loss: 0.801673  [  160/  306]
train() client id: f_00004-2-5 loss: 0.919148  [  192/  306]
train() client id: f_00004-2-6 loss: 0.775020  [  224/  306]
train() client id: f_00004-2-7 loss: 0.669340  [  256/  306]
train() client id: f_00004-2-8 loss: 0.746029  [  288/  306]
train() client id: f_00004-3-0 loss: 0.756606  [   32/  306]
train() client id: f_00004-3-1 loss: 0.760517  [   64/  306]
train() client id: f_00004-3-2 loss: 0.813146  [   96/  306]
train() client id: f_00004-3-3 loss: 0.765054  [  128/  306]
train() client id: f_00004-3-4 loss: 0.819244  [  160/  306]
train() client id: f_00004-3-5 loss: 0.886335  [  192/  306]
train() client id: f_00004-3-6 loss: 0.641722  [  224/  306]
train() client id: f_00004-3-7 loss: 0.789977  [  256/  306]
train() client id: f_00004-3-8 loss: 0.767556  [  288/  306]
train() client id: f_00004-4-0 loss: 0.684984  [   32/  306]
train() client id: f_00004-4-1 loss: 0.779936  [   64/  306]
train() client id: f_00004-4-2 loss: 0.690683  [   96/  306]
train() client id: f_00004-4-3 loss: 0.890320  [  128/  306]
train() client id: f_00004-4-4 loss: 0.760395  [  160/  306]
train() client id: f_00004-4-5 loss: 0.694058  [  192/  306]
train() client id: f_00004-4-6 loss: 0.754165  [  224/  306]
train() client id: f_00004-4-7 loss: 0.693805  [  256/  306]
train() client id: f_00004-4-8 loss: 0.925394  [  288/  306]
train() client id: f_00004-5-0 loss: 0.901554  [   32/  306]
train() client id: f_00004-5-1 loss: 0.814805  [   64/  306]
train() client id: f_00004-5-2 loss: 0.812600  [   96/  306]
train() client id: f_00004-5-3 loss: 0.712218  [  128/  306]
train() client id: f_00004-5-4 loss: 0.650828  [  160/  306]
train() client id: f_00004-5-5 loss: 0.783826  [  192/  306]
train() client id: f_00004-5-6 loss: 0.770915  [  224/  306]
train() client id: f_00004-5-7 loss: 0.693166  [  256/  306]
train() client id: f_00004-5-8 loss: 0.767002  [  288/  306]
train() client id: f_00004-6-0 loss: 0.685737  [   32/  306]
train() client id: f_00004-6-1 loss: 0.814379  [   64/  306]
train() client id: f_00004-6-2 loss: 0.806958  [   96/  306]
train() client id: f_00004-6-3 loss: 0.675692  [  128/  306]
train() client id: f_00004-6-4 loss: 0.826936  [  160/  306]
train() client id: f_00004-6-5 loss: 0.816298  [  192/  306]
train() client id: f_00004-6-6 loss: 0.789320  [  224/  306]
train() client id: f_00004-6-7 loss: 0.710296  [  256/  306]
train() client id: f_00004-6-8 loss: 0.733852  [  288/  306]
train() client id: f_00004-7-0 loss: 0.763496  [   32/  306]
train() client id: f_00004-7-1 loss: 0.838641  [   64/  306]
train() client id: f_00004-7-2 loss: 0.786519  [   96/  306]
train() client id: f_00004-7-3 loss: 0.932512  [  128/  306]
train() client id: f_00004-7-4 loss: 0.703957  [  160/  306]
train() client id: f_00004-7-5 loss: 0.740769  [  192/  306]
train() client id: f_00004-7-6 loss: 0.649342  [  224/  306]
train() client id: f_00004-7-7 loss: 0.668656  [  256/  306]
train() client id: f_00004-7-8 loss: 0.797394  [  288/  306]
train() client id: f_00004-8-0 loss: 0.676772  [   32/  306]
train() client id: f_00004-8-1 loss: 0.858879  [   64/  306]
train() client id: f_00004-8-2 loss: 0.780217  [   96/  306]
train() client id: f_00004-8-3 loss: 0.665342  [  128/  306]
train() client id: f_00004-8-4 loss: 0.804654  [  160/  306]
train() client id: f_00004-8-5 loss: 0.742461  [  192/  306]
train() client id: f_00004-8-6 loss: 0.729200  [  224/  306]
train() client id: f_00004-8-7 loss: 0.694411  [  256/  306]
train() client id: f_00004-8-8 loss: 0.827778  [  288/  306]
train() client id: f_00004-9-0 loss: 0.770262  [   32/  306]
train() client id: f_00004-9-1 loss: 0.704176  [   64/  306]
train() client id: f_00004-9-2 loss: 0.722184  [   96/  306]
train() client id: f_00004-9-3 loss: 0.763109  [  128/  306]
train() client id: f_00004-9-4 loss: 0.638091  [  160/  306]
train() client id: f_00004-9-5 loss: 0.828593  [  192/  306]
train() client id: f_00004-9-6 loss: 0.766749  [  224/  306]
train() client id: f_00004-9-7 loss: 0.865983  [  256/  306]
train() client id: f_00004-9-8 loss: 0.820274  [  288/  306]
train() client id: f_00004-10-0 loss: 0.960278  [   32/  306]
train() client id: f_00004-10-1 loss: 0.718492  [   64/  306]
train() client id: f_00004-10-2 loss: 0.726051  [   96/  306]
train() client id: f_00004-10-3 loss: 0.655208  [  128/  306]
train() client id: f_00004-10-4 loss: 0.785251  [  160/  306]
train() client id: f_00004-10-5 loss: 0.702865  [  192/  306]
train() client id: f_00004-10-6 loss: 0.807525  [  224/  306]
train() client id: f_00004-10-7 loss: 0.862061  [  256/  306]
train() client id: f_00004-10-8 loss: 0.737101  [  288/  306]
train() client id: f_00004-11-0 loss: 0.870368  [   32/  306]
train() client id: f_00004-11-1 loss: 0.769602  [   64/  306]
train() client id: f_00004-11-2 loss: 0.713623  [   96/  306]
train() client id: f_00004-11-3 loss: 0.662812  [  128/  306]
train() client id: f_00004-11-4 loss: 0.802326  [  160/  306]
train() client id: f_00004-11-5 loss: 0.627150  [  192/  306]
train() client id: f_00004-11-6 loss: 0.748825  [  224/  306]
train() client id: f_00004-11-7 loss: 0.858165  [  256/  306]
train() client id: f_00004-11-8 loss: 0.871530  [  288/  306]
train() client id: f_00005-0-0 loss: 0.774965  [   32/  146]
train() client id: f_00005-0-1 loss: 0.751814  [   64/  146]
train() client id: f_00005-0-2 loss: 0.725346  [   96/  146]
train() client id: f_00005-0-3 loss: 0.681722  [  128/  146]
train() client id: f_00005-1-0 loss: 0.784049  [   32/  146]
train() client id: f_00005-1-1 loss: 0.946185  [   64/  146]
train() client id: f_00005-1-2 loss: 0.409881  [   96/  146]
train() client id: f_00005-1-3 loss: 0.895799  [  128/  146]
train() client id: f_00005-2-0 loss: 0.660086  [   32/  146]
train() client id: f_00005-2-1 loss: 0.764820  [   64/  146]
train() client id: f_00005-2-2 loss: 0.852576  [   96/  146]
train() client id: f_00005-2-3 loss: 0.721056  [  128/  146]
train() client id: f_00005-3-0 loss: 0.734052  [   32/  146]
train() client id: f_00005-3-1 loss: 0.879309  [   64/  146]
train() client id: f_00005-3-2 loss: 0.654303  [   96/  146]
train() client id: f_00005-3-3 loss: 0.727977  [  128/  146]
train() client id: f_00005-4-0 loss: 0.751735  [   32/  146]
train() client id: f_00005-4-1 loss: 0.721516  [   64/  146]
train() client id: f_00005-4-2 loss: 0.830167  [   96/  146]
train() client id: f_00005-4-3 loss: 0.686540  [  128/  146]
train() client id: f_00005-5-0 loss: 0.651414  [   32/  146]
train() client id: f_00005-5-1 loss: 0.973479  [   64/  146]
train() client id: f_00005-5-2 loss: 0.637846  [   96/  146]
train() client id: f_00005-5-3 loss: 0.624666  [  128/  146]
train() client id: f_00005-6-0 loss: 0.839559  [   32/  146]
train() client id: f_00005-6-1 loss: 0.687828  [   64/  146]
train() client id: f_00005-6-2 loss: 0.611875  [   96/  146]
train() client id: f_00005-6-3 loss: 0.790887  [  128/  146]
train() client id: f_00005-7-0 loss: 0.779666  [   32/  146]
train() client id: f_00005-7-1 loss: 0.545723  [   64/  146]
train() client id: f_00005-7-2 loss: 0.695035  [   96/  146]
train() client id: f_00005-7-3 loss: 0.690904  [  128/  146]
train() client id: f_00005-8-0 loss: 0.632314  [   32/  146]
train() client id: f_00005-8-1 loss: 0.963530  [   64/  146]
train() client id: f_00005-8-2 loss: 0.616539  [   96/  146]
train() client id: f_00005-8-3 loss: 0.851283  [  128/  146]
train() client id: f_00005-9-0 loss: 0.707886  [   32/  146]
train() client id: f_00005-9-1 loss: 0.679381  [   64/  146]
train() client id: f_00005-9-2 loss: 0.664582  [   96/  146]
train() client id: f_00005-9-3 loss: 0.848083  [  128/  146]
train() client id: f_00005-10-0 loss: 0.697010  [   32/  146]
train() client id: f_00005-10-1 loss: 0.886417  [   64/  146]
train() client id: f_00005-10-2 loss: 0.834011  [   96/  146]
train() client id: f_00005-10-3 loss: 0.585532  [  128/  146]
train() client id: f_00005-11-0 loss: 0.670950  [   32/  146]
train() client id: f_00005-11-1 loss: 1.147191  [   64/  146]
train() client id: f_00005-11-2 loss: 0.634083  [   96/  146]
train() client id: f_00005-11-3 loss: 0.566770  [  128/  146]
train() client id: f_00006-0-0 loss: 0.539098  [   32/   54]
train() client id: f_00006-1-0 loss: 0.604341  [   32/   54]
train() client id: f_00006-2-0 loss: 0.595477  [   32/   54]
train() client id: f_00006-3-0 loss: 0.598267  [   32/   54]
train() client id: f_00006-4-0 loss: 0.587906  [   32/   54]
train() client id: f_00006-5-0 loss: 0.535710  [   32/   54]
train() client id: f_00006-6-0 loss: 0.528205  [   32/   54]
train() client id: f_00006-7-0 loss: 0.595460  [   32/   54]
train() client id: f_00006-8-0 loss: 0.604092  [   32/   54]
train() client id: f_00006-9-0 loss: 0.586762  [   32/   54]
train() client id: f_00006-10-0 loss: 0.538942  [   32/   54]
train() client id: f_00006-11-0 loss: 0.544761  [   32/   54]
train() client id: f_00007-0-0 loss: 0.748460  [   32/  179]
train() client id: f_00007-0-1 loss: 0.770331  [   64/  179]
train() client id: f_00007-0-2 loss: 0.613836  [   96/  179]
train() client id: f_00007-0-3 loss: 0.647715  [  128/  179]
train() client id: f_00007-0-4 loss: 0.785313  [  160/  179]
train() client id: f_00007-1-0 loss: 0.591172  [   32/  179]
train() client id: f_00007-1-1 loss: 0.764938  [   64/  179]
train() client id: f_00007-1-2 loss: 0.724055  [   96/  179]
train() client id: f_00007-1-3 loss: 0.870530  [  128/  179]
train() client id: f_00007-1-4 loss: 0.708884  [  160/  179]
train() client id: f_00007-2-0 loss: 0.873052  [   32/  179]
train() client id: f_00007-2-1 loss: 0.597311  [   64/  179]
train() client id: f_00007-2-2 loss: 0.648488  [   96/  179]
train() client id: f_00007-2-3 loss: 0.610308  [  128/  179]
train() client id: f_00007-2-4 loss: 0.754247  [  160/  179]
train() client id: f_00007-3-0 loss: 0.712211  [   32/  179]
train() client id: f_00007-3-1 loss: 0.664679  [   64/  179]
train() client id: f_00007-3-2 loss: 0.726076  [   96/  179]
train() client id: f_00007-3-3 loss: 0.539807  [  128/  179]
train() client id: f_00007-3-4 loss: 0.767019  [  160/  179]
train() client id: f_00007-4-0 loss: 0.900602  [   32/  179]
train() client id: f_00007-4-1 loss: 0.776596  [   64/  179]
train() client id: f_00007-4-2 loss: 0.642588  [   96/  179]
train() client id: f_00007-4-3 loss: 0.669418  [  128/  179]
train() client id: f_00007-4-4 loss: 0.545848  [  160/  179]
train() client id: f_00007-5-0 loss: 0.633296  [   32/  179]
train() client id: f_00007-5-1 loss: 0.806063  [   64/  179]
train() client id: f_00007-5-2 loss: 0.697962  [   96/  179]
train() client id: f_00007-5-3 loss: 0.621434  [  128/  179]
train() client id: f_00007-5-4 loss: 0.753173  [  160/  179]
train() client id: f_00007-6-0 loss: 0.683265  [   32/  179]
train() client id: f_00007-6-1 loss: 0.642605  [   64/  179]
train() client id: f_00007-6-2 loss: 0.662274  [   96/  179]
train() client id: f_00007-6-3 loss: 0.625529  [  128/  179]
train() client id: f_00007-6-4 loss: 0.756895  [  160/  179]
train() client id: f_00007-7-0 loss: 0.524531  [   32/  179]
train() client id: f_00007-7-1 loss: 0.734022  [   64/  179]
train() client id: f_00007-7-2 loss: 0.765400  [   96/  179]
train() client id: f_00007-7-3 loss: 0.636200  [  128/  179]
train() client id: f_00007-7-4 loss: 0.839457  [  160/  179]
train() client id: f_00007-8-0 loss: 0.631221  [   32/  179]
train() client id: f_00007-8-1 loss: 0.599720  [   64/  179]
train() client id: f_00007-8-2 loss: 0.767938  [   96/  179]
train() client id: f_00007-8-3 loss: 0.747342  [  128/  179]
train() client id: f_00007-8-4 loss: 0.564086  [  160/  179]
train() client id: f_00007-9-0 loss: 0.676651  [   32/  179]
train() client id: f_00007-9-1 loss: 0.783004  [   64/  179]
train() client id: f_00007-9-2 loss: 0.647924  [   96/  179]
train() client id: f_00007-9-3 loss: 0.681192  [  128/  179]
train() client id: f_00007-9-4 loss: 0.636903  [  160/  179]
train() client id: f_00007-10-0 loss: 0.756029  [   32/  179]
train() client id: f_00007-10-1 loss: 0.679319  [   64/  179]
train() client id: f_00007-10-2 loss: 0.643599  [   96/  179]
train() client id: f_00007-10-3 loss: 0.616441  [  128/  179]
train() client id: f_00007-10-4 loss: 0.686595  [  160/  179]
train() client id: f_00007-11-0 loss: 0.543479  [   32/  179]
train() client id: f_00007-11-1 loss: 0.691069  [   64/  179]
train() client id: f_00007-11-2 loss: 0.898889  [   96/  179]
train() client id: f_00007-11-3 loss: 0.648475  [  128/  179]
train() client id: f_00007-11-4 loss: 0.620974  [  160/  179]
train() client id: f_00008-0-0 loss: 0.910460  [   32/  130]
train() client id: f_00008-0-1 loss: 0.600044  [   64/  130]
train() client id: f_00008-0-2 loss: 0.629564  [   96/  130]
train() client id: f_00008-0-3 loss: 0.531443  [  128/  130]
train() client id: f_00008-1-0 loss: 0.687417  [   32/  130]
train() client id: f_00008-1-1 loss: 0.647760  [   64/  130]
train() client id: f_00008-1-2 loss: 0.580434  [   96/  130]
train() client id: f_00008-1-3 loss: 0.759928  [  128/  130]
train() client id: f_00008-2-0 loss: 0.745239  [   32/  130]
train() client id: f_00008-2-1 loss: 0.655804  [   64/  130]
train() client id: f_00008-2-2 loss: 0.587172  [   96/  130]
train() client id: f_00008-2-3 loss: 0.679531  [  128/  130]
train() client id: f_00008-3-0 loss: 0.672413  [   32/  130]
train() client id: f_00008-3-1 loss: 0.608910  [   64/  130]
train() client id: f_00008-3-2 loss: 0.610084  [   96/  130]
train() client id: f_00008-3-3 loss: 0.790465  [  128/  130]
train() client id: f_00008-4-0 loss: 0.596153  [   32/  130]
train() client id: f_00008-4-1 loss: 0.616585  [   64/  130]
train() client id: f_00008-4-2 loss: 0.587395  [   96/  130]
train() client id: f_00008-4-3 loss: 0.876761  [  128/  130]
train() client id: f_00008-5-0 loss: 0.684109  [   32/  130]
train() client id: f_00008-5-1 loss: 0.751384  [   64/  130]
train() client id: f_00008-5-2 loss: 0.658411  [   96/  130]
train() client id: f_00008-5-3 loss: 0.576279  [  128/  130]
train() client id: f_00008-6-0 loss: 0.662667  [   32/  130]
train() client id: f_00008-6-1 loss: 0.626909  [   64/  130]
train() client id: f_00008-6-2 loss: 0.661790  [   96/  130]
train() client id: f_00008-6-3 loss: 0.693795  [  128/  130]
train() client id: f_00008-7-0 loss: 0.719828  [   32/  130]
train() client id: f_00008-7-1 loss: 0.655857  [   64/  130]
train() client id: f_00008-7-2 loss: 0.664136  [   96/  130]
train() client id: f_00008-7-3 loss: 0.643325  [  128/  130]
train() client id: f_00008-8-0 loss: 0.747698  [   32/  130]
train() client id: f_00008-8-1 loss: 0.639532  [   64/  130]
train() client id: f_00008-8-2 loss: 0.585449  [   96/  130]
train() client id: f_00008-8-3 loss: 0.674592  [  128/  130]
train() client id: f_00008-9-0 loss: 0.608283  [   32/  130]
train() client id: f_00008-9-1 loss: 0.626892  [   64/  130]
train() client id: f_00008-9-2 loss: 0.504963  [   96/  130]
train() client id: f_00008-9-3 loss: 0.901286  [  128/  130]
train() client id: f_00008-10-0 loss: 0.719569  [   32/  130]
train() client id: f_00008-10-1 loss: 0.615820  [   64/  130]
train() client id: f_00008-10-2 loss: 0.615174  [   96/  130]
train() client id: f_00008-10-3 loss: 0.704662  [  128/  130]
train() client id: f_00008-11-0 loss: 0.780299  [   32/  130]
train() client id: f_00008-11-1 loss: 0.626852  [   64/  130]
train() client id: f_00008-11-2 loss: 0.640686  [   96/  130]
train() client id: f_00008-11-3 loss: 0.638147  [  128/  130]
train() client id: f_00009-0-0 loss: 1.148287  [   32/  118]
train() client id: f_00009-0-1 loss: 1.162938  [   64/  118]
train() client id: f_00009-0-2 loss: 1.107951  [   96/  118]
train() client id: f_00009-1-0 loss: 1.151036  [   32/  118]
train() client id: f_00009-1-1 loss: 1.088036  [   64/  118]
train() client id: f_00009-1-2 loss: 0.987283  [   96/  118]
train() client id: f_00009-2-0 loss: 1.065324  [   32/  118]
train() client id: f_00009-2-1 loss: 1.121236  [   64/  118]
train() client id: f_00009-2-2 loss: 0.878439  [   96/  118]
train() client id: f_00009-3-0 loss: 0.989054  [   32/  118]
train() client id: f_00009-3-1 loss: 0.907847  [   64/  118]
train() client id: f_00009-3-2 loss: 0.878459  [   96/  118]
train() client id: f_00009-4-0 loss: 0.879963  [   32/  118]
train() client id: f_00009-4-1 loss: 1.136841  [   64/  118]
train() client id: f_00009-4-2 loss: 0.837977  [   96/  118]
train() client id: f_00009-5-0 loss: 0.902412  [   32/  118]
train() client id: f_00009-5-1 loss: 1.021302  [   64/  118]
train() client id: f_00009-5-2 loss: 0.828444  [   96/  118]
train() client id: f_00009-6-0 loss: 0.949752  [   32/  118]
train() client id: f_00009-6-1 loss: 0.991145  [   64/  118]
train() client id: f_00009-6-2 loss: 0.741029  [   96/  118]
train() client id: f_00009-7-0 loss: 1.002827  [   32/  118]
train() client id: f_00009-7-1 loss: 0.849904  [   64/  118]
train() client id: f_00009-7-2 loss: 0.623462  [   96/  118]
train() client id: f_00009-8-0 loss: 0.871196  [   32/  118]
train() client id: f_00009-8-1 loss: 0.715292  [   64/  118]
train() client id: f_00009-8-2 loss: 0.969606  [   96/  118]
train() client id: f_00009-9-0 loss: 0.988925  [   32/  118]
train() client id: f_00009-9-1 loss: 0.802625  [   64/  118]
train() client id: f_00009-9-2 loss: 0.755996  [   96/  118]
train() client id: f_00009-10-0 loss: 0.776431  [   32/  118]
train() client id: f_00009-10-1 loss: 0.768046  [   64/  118]
train() client id: f_00009-10-2 loss: 0.980948  [   96/  118]
train() client id: f_00009-11-0 loss: 0.761901  [   32/  118]
train() client id: f_00009-11-1 loss: 0.712692  [   64/  118]
train() client id: f_00009-11-2 loss: 0.766981  [   96/  118]
At round 30 accuracy: 0.6445623342175066
At round 30 training accuracy: 0.5861837692823608
At round 30 training loss: 0.8294307909073904
update_location
xs = -3.905658 4.200318 170.009024 18.811294 0.979296 3.956410 -132.443192 -111.324852 154.663977 -97.060879 
ys = 162.587959 145.555839 1.320614 -132.455176 124.350187 107.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 190.919090 176.646950 197.243028 167.027658 159.574208 147.103855 165.976172 149.645912 185.012474 139.416018 
dists_bs = 171.543967 181.603173 386.258664 363.443947 183.123227 191.152648 182.651901 185.451824 365.325684 187.922856 
uav_gains = -107.113331 -106.214170 -107.508753 -105.587948 -105.084268 -104.194116 -105.517986 -104.381033 -106.743743 -103.609525 
bs_gains = -102.129572 -102.822515 -111.999650 -111.259307 -102.923875 -103.445708 -102.892537 -103.077530 -111.322105 -103.238488 
Round 31
-------------------------------
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.90000386 14.30569929  6.7966894   2.4461517  16.50004725  7.94229532
  3.0341758   9.71414062  7.16398278  6.44283898]
obj_prev = 81.24602500345188
eta_min = 3.803699202355281e-14	eta_max = 0.9265448863059464
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 18.86522301759883	eta = 0.9090909090909091
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 33.97406433952712	eta = 0.5048027981544223
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 26.610863860443008	eta = 0.6444812477044536
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 25.281958371636836	eta = 0.6783573681741374
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 25.213171638427752	eta = 0.6802080670062467
af = 17.150202743271663	bf = 1.4539623964515263	zeta = 25.212972778010624	eta = 0.680213431961071
eta = 0.680213431961071
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [0.03179805 0.06687685 0.03129332 0.01085171 0.07722383 0.03684536
 0.01362773 0.04517342 0.03280751 0.02977912]
ene_total = [2.23637289 4.06508358 2.2200981  1.04612844 4.63524192 2.42743785
 1.19692013 2.90387674 2.44646747 2.03534566]
ti_comp = [0.42879245 0.44662227 0.42670833 0.43604005 0.44628188 0.44447784
 0.43634521 0.44101196 0.40063459 0.44520477]
ti_coms = [0.08920181 0.07137199 0.09128593 0.08195421 0.07171238 0.07351642
 0.08164905 0.0769823  0.11735967 0.07278949]
t_total = [28.44986992 28.44986992 28.44986992 28.44986992 28.44986992 28.44986992
 28.44986992 28.44986992 28.44986992 28.44986992]
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [1.09291688e-05 9.37187797e-05 1.05189542e-05 4.20070663e-07
 1.44515864e-04 1.58244307e-05 8.30788735e-07 2.96229369e-05
 1.37500070e-05 8.32714570e-06]
ene_total = [0.49052461 0.39714469 0.5019487  0.45014129 0.40180415 0.40464437
 0.44848781 0.42443794 0.64533131 0.40024009]
optimize_network iter = 0 obj = 4.564704952114099
eta = 0.680213431961071
freqs = [37078600.58661079 74869583.15751041 36668274.98538263 12443483.84142069
 86519118.7997421  41447912.13753933 15615771.9937943  51215637.28090309
 40944429.42168844 33444300.55685099]
eta_min = 0.6802134319610765	eta_max = 0.6802134319610728
af = 0.014970687331339992	bf = 1.4539623964515263	zeta = 0.016467756064473992	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [2.70316510e-06 2.31799269e-05 2.60170470e-06 1.03898144e-07
 3.57438195e-05 3.91393429e-06 2.05483065e-07 7.32678673e-06
 3.40085688e-06 2.05959391e-06]
ene_total = [1.72390573 1.38338877 1.76415135 1.58337944 1.39239245 1.42109708
 1.57750334 1.48871744 2.26805167 1.40669455]
ti_comp = [0.42879245 0.44662227 0.42670833 0.43604005 0.44628188 0.44447784
 0.43634521 0.44101196 0.40063459 0.44520477]
ti_coms = [0.08920181 0.07137199 0.09128593 0.08195421 0.07171238 0.07351642
 0.08164905 0.0769823  0.11735967 0.07278949]
t_total = [28.44986992 28.44986992 28.44986992 28.44986992 28.44986992 28.44986992
 28.44986992 28.44986992 28.44986992 28.44986992]
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [1.09291688e-05 9.37187797e-05 1.05189542e-05 4.20070663e-07
 1.44515864e-04 1.58244307e-05 8.30788735e-07 2.96229369e-05
 1.37500070e-05 8.32714570e-06]
ene_total = [0.49052461 0.39714469 0.5019487  0.45014129 0.40180415 0.40464437
 0.44848781 0.42443794 0.64533131 0.40024009]
optimize_network iter = 1 obj = 4.564704952114124
eta = 0.6802134319610728
freqs = [37078600.58661079 74869583.15751038 36668274.98538262 12443483.84142068
 86519118.79974204 41447912.1375393  15615771.9937943  51215637.28090306
 40944429.42168847 33444300.55685098]
Done!
ene_coms = [0.00892018 0.0071372  0.00912859 0.00819542 0.00717124 0.00735164
 0.0081649  0.00769823 0.01173597 0.00727895]
ene_comp = [1.03936590e-05 8.91267263e-05 1.00035442e-05 3.99487948e-07
 1.37434843e-04 1.50490617e-05 7.90081565e-07 2.81714657e-05
 1.30762811e-05 7.91913038e-06]
ene_total = [0.00893058 0.00722633 0.0091386  0.00819582 0.00730867 0.00736669
 0.00816569 0.0077264  0.01174904 0.00728687]
At round 31 energy consumption: 0.08309468916898873
At round 31 eta: 0.6802134319610728
At round 31 a_n: 17.56368160020554
At round 31 local rounds: 12.618272918924527
At round 31 global rounds: 54.923137353497395
gradient difference: 0.4503632187843323
train() client id: f_00000-0-0 loss: 1.243918  [   32/  126]
train() client id: f_00000-0-1 loss: 1.114987  [   64/  126]
train() client id: f_00000-0-2 loss: 1.232847  [   96/  126]
train() client id: f_00000-1-0 loss: 1.163316  [   32/  126]
train() client id: f_00000-1-1 loss: 1.068087  [   64/  126]
train() client id: f_00000-1-2 loss: 1.193843  [   96/  126]
train() client id: f_00000-2-0 loss: 1.097231  [   32/  126]
train() client id: f_00000-2-1 loss: 1.052499  [   64/  126]
train() client id: f_00000-2-2 loss: 0.988753  [   96/  126]
train() client id: f_00000-3-0 loss: 0.996005  [   32/  126]
train() client id: f_00000-3-1 loss: 1.026655  [   64/  126]
train() client id: f_00000-3-2 loss: 0.972010  [   96/  126]
train() client id: f_00000-4-0 loss: 0.935324  [   32/  126]
train() client id: f_00000-4-1 loss: 0.950632  [   64/  126]
train() client id: f_00000-4-2 loss: 1.027740  [   96/  126]
train() client id: f_00000-5-0 loss: 0.902492  [   32/  126]
train() client id: f_00000-5-1 loss: 0.934891  [   64/  126]
train() client id: f_00000-5-2 loss: 0.942515  [   96/  126]
train() client id: f_00000-6-0 loss: 1.028358  [   32/  126]
train() client id: f_00000-6-1 loss: 0.905990  [   64/  126]
train() client id: f_00000-6-2 loss: 0.757106  [   96/  126]
train() client id: f_00000-7-0 loss: 0.888997  [   32/  126]
train() client id: f_00000-7-1 loss: 0.814229  [   64/  126]
train() client id: f_00000-7-2 loss: 0.857154  [   96/  126]
train() client id: f_00000-8-0 loss: 0.900194  [   32/  126]
train() client id: f_00000-8-1 loss: 0.912023  [   64/  126]
train() client id: f_00000-8-2 loss: 0.797265  [   96/  126]
train() client id: f_00000-9-0 loss: 0.945711  [   32/  126]
train() client id: f_00000-9-1 loss: 0.873153  [   64/  126]
train() client id: f_00000-9-2 loss: 0.731712  [   96/  126]
train() client id: f_00000-10-0 loss: 0.836245  [   32/  126]
train() client id: f_00000-10-1 loss: 0.861214  [   64/  126]
train() client id: f_00000-10-2 loss: 0.881453  [   96/  126]
train() client id: f_00000-11-0 loss: 0.951361  [   32/  126]
train() client id: f_00000-11-1 loss: 0.878689  [   64/  126]
train() client id: f_00000-11-2 loss: 0.776989  [   96/  126]
train() client id: f_00001-0-0 loss: 0.519417  [   32/  265]
train() client id: f_00001-0-1 loss: 0.479137  [   64/  265]
train() client id: f_00001-0-2 loss: 0.475565  [   96/  265]
train() client id: f_00001-0-3 loss: 0.460337  [  128/  265]
train() client id: f_00001-0-4 loss: 0.568860  [  160/  265]
train() client id: f_00001-0-5 loss: 0.782638  [  192/  265]
train() client id: f_00001-0-6 loss: 0.433449  [  224/  265]
train() client id: f_00001-0-7 loss: 0.561604  [  256/  265]
train() client id: f_00001-1-0 loss: 0.511214  [   32/  265]
train() client id: f_00001-1-1 loss: 0.571828  [   64/  265]
train() client id: f_00001-1-2 loss: 0.539052  [   96/  265]
train() client id: f_00001-1-3 loss: 0.466395  [  128/  265]
train() client id: f_00001-1-4 loss: 0.703574  [  160/  265]
train() client id: f_00001-1-5 loss: 0.443458  [  192/  265]
train() client id: f_00001-1-6 loss: 0.466561  [  224/  265]
train() client id: f_00001-1-7 loss: 0.544060  [  256/  265]
train() client id: f_00001-2-0 loss: 0.475978  [   32/  265]
train() client id: f_00001-2-1 loss: 0.575445  [   64/  265]
train() client id: f_00001-2-2 loss: 0.449698  [   96/  265]
train() client id: f_00001-2-3 loss: 0.526183  [  128/  265]
train() client id: f_00001-2-4 loss: 0.503981  [  160/  265]
train() client id: f_00001-2-5 loss: 0.582132  [  192/  265]
train() client id: f_00001-2-6 loss: 0.544196  [  224/  265]
train() client id: f_00001-2-7 loss: 0.551369  [  256/  265]
train() client id: f_00001-3-0 loss: 0.533432  [   32/  265]
train() client id: f_00001-3-1 loss: 0.600135  [   64/  265]
train() client id: f_00001-3-2 loss: 0.500539  [   96/  265]
train() client id: f_00001-3-3 loss: 0.473864  [  128/  265]
train() client id: f_00001-3-4 loss: 0.486186  [  160/  265]
train() client id: f_00001-3-5 loss: 0.502939  [  192/  265]
train() client id: f_00001-3-6 loss: 0.576649  [  224/  265]
train() client id: f_00001-3-7 loss: 0.488520  [  256/  265]
train() client id: f_00001-4-0 loss: 0.415938  [   32/  265]
train() client id: f_00001-4-1 loss: 0.436125  [   64/  265]
train() client id: f_00001-4-2 loss: 0.500393  [   96/  265]
train() client id: f_00001-4-3 loss: 0.606382  [  128/  265]
train() client id: f_00001-4-4 loss: 0.578951  [  160/  265]
train() client id: f_00001-4-5 loss: 0.637955  [  192/  265]
train() client id: f_00001-4-6 loss: 0.501174  [  224/  265]
train() client id: f_00001-4-7 loss: 0.474472  [  256/  265]
train() client id: f_00001-5-0 loss: 0.578722  [   32/  265]
train() client id: f_00001-5-1 loss: 0.499727  [   64/  265]
train() client id: f_00001-5-2 loss: 0.432902  [   96/  265]
train() client id: f_00001-5-3 loss: 0.601493  [  128/  265]
train() client id: f_00001-5-4 loss: 0.598537  [  160/  265]
train() client id: f_00001-5-5 loss: 0.421536  [  192/  265]
train() client id: f_00001-5-6 loss: 0.477808  [  224/  265]
train() client id: f_00001-5-7 loss: 0.461991  [  256/  265]
train() client id: f_00001-6-0 loss: 0.505523  [   32/  265]
train() client id: f_00001-6-1 loss: 0.632160  [   64/  265]
train() client id: f_00001-6-2 loss: 0.504046  [   96/  265]
train() client id: f_00001-6-3 loss: 0.607644  [  128/  265]
train() client id: f_00001-6-4 loss: 0.413753  [  160/  265]
train() client id: f_00001-6-5 loss: 0.484798  [  192/  265]
train() client id: f_00001-6-6 loss: 0.420741  [  224/  265]
train() client id: f_00001-6-7 loss: 0.552162  [  256/  265]
train() client id: f_00001-7-0 loss: 0.424445  [   32/  265]
train() client id: f_00001-7-1 loss: 0.591057  [   64/  265]
train() client id: f_00001-7-2 loss: 0.505708  [   96/  265]
train() client id: f_00001-7-3 loss: 0.433737  [  128/  265]
train() client id: f_00001-7-4 loss: 0.587117  [  160/  265]
train() client id: f_00001-7-5 loss: 0.521376  [  192/  265]
train() client id: f_00001-7-6 loss: 0.512814  [  224/  265]
train() client id: f_00001-7-7 loss: 0.503392  [  256/  265]
train() client id: f_00001-8-0 loss: 0.468178  [   32/  265]
train() client id: f_00001-8-1 loss: 0.571390  [   64/  265]
train() client id: f_00001-8-2 loss: 0.579041  [   96/  265]
train() client id: f_00001-8-3 loss: 0.468684  [  128/  265]
train() client id: f_00001-8-4 loss: 0.520633  [  160/  265]
train() client id: f_00001-8-5 loss: 0.488243  [  192/  265]
train() client id: f_00001-8-6 loss: 0.499505  [  224/  265]
train() client id: f_00001-8-7 loss: 0.528020  [  256/  265]
train() client id: f_00001-9-0 loss: 0.554293  [   32/  265]
train() client id: f_00001-9-1 loss: 0.538184  [   64/  265]
train() client id: f_00001-9-2 loss: 0.432409  [   96/  265]
train() client id: f_00001-9-3 loss: 0.522502  [  128/  265]
train() client id: f_00001-9-4 loss: 0.598354  [  160/  265]
train() client id: f_00001-9-5 loss: 0.493594  [  192/  265]
train() client id: f_00001-9-6 loss: 0.499115  [  224/  265]
train() client id: f_00001-9-7 loss: 0.492084  [  256/  265]
train() client id: f_00001-10-0 loss: 0.576813  [   32/  265]
train() client id: f_00001-10-1 loss: 0.554436  [   64/  265]
train() client id: f_00001-10-2 loss: 0.478226  [   96/  265]
train() client id: f_00001-10-3 loss: 0.541803  [  128/  265]
train() client id: f_00001-10-4 loss: 0.577195  [  160/  265]
train() client id: f_00001-10-5 loss: 0.481217  [  192/  265]
train() client id: f_00001-10-6 loss: 0.409786  [  224/  265]
train() client id: f_00001-10-7 loss: 0.515394  [  256/  265]
train() client id: f_00001-11-0 loss: 0.481315  [   32/  265]
train() client id: f_00001-11-1 loss: 0.527126  [   64/  265]
train() client id: f_00001-11-2 loss: 0.567019  [   96/  265]
train() client id: f_00001-11-3 loss: 0.618668  [  128/  265]
train() client id: f_00001-11-4 loss: 0.477876  [  160/  265]
train() client id: f_00001-11-5 loss: 0.490993  [  192/  265]
train() client id: f_00001-11-6 loss: 0.495887  [  224/  265]
train() client id: f_00001-11-7 loss: 0.483858  [  256/  265]
train() client id: f_00002-0-0 loss: 1.265957  [   32/  124]
train() client id: f_00002-0-1 loss: 1.362811  [   64/  124]
train() client id: f_00002-0-2 loss: 1.328153  [   96/  124]
train() client id: f_00002-1-0 loss: 1.344659  [   32/  124]
train() client id: f_00002-1-1 loss: 1.310365  [   64/  124]
train() client id: f_00002-1-2 loss: 1.236662  [   96/  124]
train() client id: f_00002-2-0 loss: 1.193792  [   32/  124]
train() client id: f_00002-2-1 loss: 1.314802  [   64/  124]
train() client id: f_00002-2-2 loss: 1.256136  [   96/  124]
train() client id: f_00002-3-0 loss: 1.278336  [   32/  124]
train() client id: f_00002-3-1 loss: 1.252236  [   64/  124]
train() client id: f_00002-3-2 loss: 1.142607  [   96/  124]
train() client id: f_00002-4-0 loss: 1.274495  [   32/  124]
train() client id: f_00002-4-1 loss: 1.197997  [   64/  124]
train() client id: f_00002-4-2 loss: 1.166607  [   96/  124]
train() client id: f_00002-5-0 loss: 1.266363  [   32/  124]
train() client id: f_00002-5-1 loss: 1.154863  [   64/  124]
train() client id: f_00002-5-2 loss: 1.028567  [   96/  124]
train() client id: f_00002-6-0 loss: 1.279142  [   32/  124]
train() client id: f_00002-6-1 loss: 1.075491  [   64/  124]
train() client id: f_00002-6-2 loss: 1.122305  [   96/  124]
train() client id: f_00002-7-0 loss: 1.139865  [   32/  124]
train() client id: f_00002-7-1 loss: 1.021825  [   64/  124]
train() client id: f_00002-7-2 loss: 1.122744  [   96/  124]
train() client id: f_00002-8-0 loss: 1.205069  [   32/  124]
train() client id: f_00002-8-1 loss: 1.187229  [   64/  124]
train() client id: f_00002-8-2 loss: 1.011026  [   96/  124]
train() client id: f_00002-9-0 loss: 1.166342  [   32/  124]
train() client id: f_00002-9-1 loss: 1.078662  [   64/  124]
train() client id: f_00002-9-2 loss: 1.062034  [   96/  124]
train() client id: f_00002-10-0 loss: 1.153584  [   32/  124]
train() client id: f_00002-10-1 loss: 0.937031  [   64/  124]
train() client id: f_00002-10-2 loss: 1.104786  [   96/  124]
train() client id: f_00002-11-0 loss: 0.983063  [   32/  124]
train() client id: f_00002-11-1 loss: 0.927769  [   64/  124]
train() client id: f_00002-11-2 loss: 1.061624  [   96/  124]
train() client id: f_00003-0-0 loss: 0.423103  [   32/   43]
train() client id: f_00003-1-0 loss: 0.641393  [   32/   43]
train() client id: f_00003-2-0 loss: 0.569965  [   32/   43]
train() client id: f_00003-3-0 loss: 0.769953  [   32/   43]
train() client id: f_00003-4-0 loss: 0.570026  [   32/   43]
train() client id: f_00003-5-0 loss: 0.618854  [   32/   43]
train() client id: f_00003-6-0 loss: 0.604864  [   32/   43]
train() client id: f_00003-7-0 loss: 0.716830  [   32/   43]
train() client id: f_00003-8-0 loss: 0.547912  [   32/   43]
train() client id: f_00003-9-0 loss: 0.663795  [   32/   43]
train() client id: f_00003-10-0 loss: 0.546591  [   32/   43]
train() client id: f_00003-11-0 loss: 0.634326  [   32/   43]
train() client id: f_00004-0-0 loss: 0.803738  [   32/  306]
train() client id: f_00004-0-1 loss: 0.859736  [   64/  306]
train() client id: f_00004-0-2 loss: 0.755800  [   96/  306]
train() client id: f_00004-0-3 loss: 0.885878  [  128/  306]
train() client id: f_00004-0-4 loss: 0.703628  [  160/  306]
train() client id: f_00004-0-5 loss: 0.602941  [  192/  306]
train() client id: f_00004-0-6 loss: 0.661289  [  224/  306]
train() client id: f_00004-0-7 loss: 0.718404  [  256/  306]
train() client id: f_00004-0-8 loss: 0.814615  [  288/  306]
train() client id: f_00004-1-0 loss: 0.837643  [   32/  306]
train() client id: f_00004-1-1 loss: 0.821123  [   64/  306]
train() client id: f_00004-1-2 loss: 0.829898  [   96/  306]
train() client id: f_00004-1-3 loss: 0.745726  [  128/  306]
train() client id: f_00004-1-4 loss: 0.778334  [  160/  306]
train() client id: f_00004-1-5 loss: 0.699764  [  192/  306]
train() client id: f_00004-1-6 loss: 0.840097  [  224/  306]
train() client id: f_00004-1-7 loss: 0.738830  [  256/  306]
train() client id: f_00004-1-8 loss: 0.580224  [  288/  306]
train() client id: f_00004-2-0 loss: 0.668117  [   32/  306]
train() client id: f_00004-2-1 loss: 0.650875  [   64/  306]
train() client id: f_00004-2-2 loss: 0.743071  [   96/  306]
train() client id: f_00004-2-3 loss: 0.748916  [  128/  306]
train() client id: f_00004-2-4 loss: 0.793723  [  160/  306]
train() client id: f_00004-2-5 loss: 0.825415  [  192/  306]
train() client id: f_00004-2-6 loss: 0.867991  [  224/  306]
train() client id: f_00004-2-7 loss: 0.744618  [  256/  306]
train() client id: f_00004-2-8 loss: 0.787520  [  288/  306]
train() client id: f_00004-3-0 loss: 0.681718  [   32/  306]
train() client id: f_00004-3-1 loss: 0.808172  [   64/  306]
train() client id: f_00004-3-2 loss: 0.757079  [   96/  306]
train() client id: f_00004-3-3 loss: 0.752432  [  128/  306]
train() client id: f_00004-3-4 loss: 0.763524  [  160/  306]
train() client id: f_00004-3-5 loss: 0.814014  [  192/  306]
train() client id: f_00004-3-6 loss: 0.659192  [  224/  306]
train() client id: f_00004-3-7 loss: 0.827087  [  256/  306]
train() client id: f_00004-3-8 loss: 0.828897  [  288/  306]
train() client id: f_00004-4-0 loss: 0.668148  [   32/  306]
train() client id: f_00004-4-1 loss: 0.819079  [   64/  306]
train() client id: f_00004-4-2 loss: 0.720432  [   96/  306]
train() client id: f_00004-4-3 loss: 0.909370  [  128/  306]
train() client id: f_00004-4-4 loss: 0.589221  [  160/  306]
train() client id: f_00004-4-5 loss: 0.712688  [  192/  306]
train() client id: f_00004-4-6 loss: 0.686366  [  224/  306]
train() client id: f_00004-4-7 loss: 0.889576  [  256/  306]
train() client id: f_00004-4-8 loss: 0.901847  [  288/  306]
train() client id: f_00004-5-0 loss: 0.723903  [   32/  306]
train() client id: f_00004-5-1 loss: 0.713073  [   64/  306]
train() client id: f_00004-5-2 loss: 0.768856  [   96/  306]
train() client id: f_00004-5-3 loss: 0.703877  [  128/  306]
train() client id: f_00004-5-4 loss: 0.845056  [  160/  306]
train() client id: f_00004-5-5 loss: 0.766112  [  192/  306]
train() client id: f_00004-5-6 loss: 0.783390  [  224/  306]
train() client id: f_00004-5-7 loss: 0.668575  [  256/  306]
train() client id: f_00004-5-8 loss: 0.891015  [  288/  306]
train() client id: f_00004-6-0 loss: 0.776836  [   32/  306]
train() client id: f_00004-6-1 loss: 0.769050  [   64/  306]
train() client id: f_00004-6-2 loss: 0.733056  [   96/  306]
train() client id: f_00004-6-3 loss: 0.724321  [  128/  306]
train() client id: f_00004-6-4 loss: 0.706808  [  160/  306]
train() client id: f_00004-6-5 loss: 0.926103  [  192/  306]
train() client id: f_00004-6-6 loss: 0.705415  [  224/  306]
train() client id: f_00004-6-7 loss: 0.773804  [  256/  306]
train() client id: f_00004-6-8 loss: 0.719468  [  288/  306]
train() client id: f_00004-7-0 loss: 0.834685  [   32/  306]
train() client id: f_00004-7-1 loss: 0.571455  [   64/  306]
train() client id: f_00004-7-2 loss: 0.801234  [   96/  306]
train() client id: f_00004-7-3 loss: 0.760169  [  128/  306]
train() client id: f_00004-7-4 loss: 0.785002  [  160/  306]
train() client id: f_00004-7-5 loss: 0.814015  [  192/  306]
train() client id: f_00004-7-6 loss: 0.677079  [  224/  306]
train() client id: f_00004-7-7 loss: 0.857935  [  256/  306]
train() client id: f_00004-7-8 loss: 0.727743  [  288/  306]
train() client id: f_00004-8-0 loss: 0.833128  [   32/  306]
train() client id: f_00004-8-1 loss: 0.711666  [   64/  306]
train() client id: f_00004-8-2 loss: 0.755876  [   96/  306]
train() client id: f_00004-8-3 loss: 0.766384  [  128/  306]
train() client id: f_00004-8-4 loss: 0.889410  [  160/  306]
train() client id: f_00004-8-5 loss: 0.644093  [  192/  306]
train() client id: f_00004-8-6 loss: 0.680157  [  224/  306]
train() client id: f_00004-8-7 loss: 0.753905  [  256/  306]
train() client id: f_00004-8-8 loss: 0.827816  [  288/  306]
train() client id: f_00004-9-0 loss: 0.734097  [   32/  306]
train() client id: f_00004-9-1 loss: 0.758095  [   64/  306]
train() client id: f_00004-9-2 loss: 0.745475  [   96/  306]
train() client id: f_00004-9-3 loss: 0.746113  [  128/  306]
train() client id: f_00004-9-4 loss: 0.914787  [  160/  306]
train() client id: f_00004-9-5 loss: 0.697429  [  192/  306]
train() client id: f_00004-9-6 loss: 0.825947  [  224/  306]
train() client id: f_00004-9-7 loss: 0.662560  [  256/  306]
train() client id: f_00004-9-8 loss: 0.854376  [  288/  306]
train() client id: f_00004-10-0 loss: 0.716143  [   32/  306]
train() client id: f_00004-10-1 loss: 0.730896  [   64/  306]
train() client id: f_00004-10-2 loss: 0.797549  [   96/  306]
train() client id: f_00004-10-3 loss: 0.685061  [  128/  306]
train() client id: f_00004-10-4 loss: 0.879423  [  160/  306]
train() client id: f_00004-10-5 loss: 0.807885  [  192/  306]
train() client id: f_00004-10-6 loss: 0.803020  [  224/  306]
train() client id: f_00004-10-7 loss: 0.807571  [  256/  306]
train() client id: f_00004-10-8 loss: 0.789908  [  288/  306]
train() client id: f_00004-11-0 loss: 0.772827  [   32/  306]
train() client id: f_00004-11-1 loss: 0.689559  [   64/  306]
train() client id: f_00004-11-2 loss: 0.767946  [   96/  306]
train() client id: f_00004-11-3 loss: 0.811817  [  128/  306]
train() client id: f_00004-11-4 loss: 0.746967  [  160/  306]
train() client id: f_00004-11-5 loss: 0.857190  [  192/  306]
train() client id: f_00004-11-6 loss: 0.738991  [  224/  306]
train() client id: f_00004-11-7 loss: 0.716348  [  256/  306]
train() client id: f_00004-11-8 loss: 0.766365  [  288/  306]
train() client id: f_00005-0-0 loss: 1.045649  [   32/  146]
train() client id: f_00005-0-1 loss: 0.966034  [   64/  146]
train() client id: f_00005-0-2 loss: 0.811733  [   96/  146]
train() client id: f_00005-0-3 loss: 0.554376  [  128/  146]
train() client id: f_00005-1-0 loss: 0.664583  [   32/  146]
train() client id: f_00005-1-1 loss: 0.689257  [   64/  146]
train() client id: f_00005-1-2 loss: 1.009821  [   96/  146]
train() client id: f_00005-1-3 loss: 0.693707  [  128/  146]
train() client id: f_00005-2-0 loss: 0.498654  [   32/  146]
train() client id: f_00005-2-1 loss: 0.804784  [   64/  146]
train() client id: f_00005-2-2 loss: 0.773673  [   96/  146]
train() client id: f_00005-2-3 loss: 1.046583  [  128/  146]
train() client id: f_00005-3-0 loss: 0.689839  [   32/  146]
train() client id: f_00005-3-1 loss: 0.650265  [   64/  146]
train() client id: f_00005-3-2 loss: 0.973944  [   96/  146]
train() client id: f_00005-3-3 loss: 0.866585  [  128/  146]
train() client id: f_00005-4-0 loss: 0.691177  [   32/  146]
train() client id: f_00005-4-1 loss: 0.801640  [   64/  146]
train() client id: f_00005-4-2 loss: 0.731138  [   96/  146]
train() client id: f_00005-4-3 loss: 0.912192  [  128/  146]
train() client id: f_00005-5-0 loss: 0.733650  [   32/  146]
train() client id: f_00005-5-1 loss: 0.731238  [   64/  146]
train() client id: f_00005-5-2 loss: 1.044889  [   96/  146]
train() client id: f_00005-5-3 loss: 0.682931  [  128/  146]
train() client id: f_00005-6-0 loss: 0.925290  [   32/  146]
train() client id: f_00005-6-1 loss: 0.802126  [   64/  146]
train() client id: f_00005-6-2 loss: 0.814353  [   96/  146]
train() client id: f_00005-6-3 loss: 0.623879  [  128/  146]
train() client id: f_00005-7-0 loss: 0.691879  [   32/  146]
train() client id: f_00005-7-1 loss: 0.628302  [   64/  146]
train() client id: f_00005-7-2 loss: 0.898895  [   96/  146]
train() client id: f_00005-7-3 loss: 0.781001  [  128/  146]
train() client id: f_00005-8-0 loss: 0.747991  [   32/  146]
train() client id: f_00005-8-1 loss: 0.708161  [   64/  146]
train() client id: f_00005-8-2 loss: 0.880741  [   96/  146]
train() client id: f_00005-8-3 loss: 0.867504  [  128/  146]
train() client id: f_00005-9-0 loss: 0.769652  [   32/  146]
train() client id: f_00005-9-1 loss: 1.000383  [   64/  146]
train() client id: f_00005-9-2 loss: 0.532254  [   96/  146]
train() client id: f_00005-9-3 loss: 1.000950  [  128/  146]
train() client id: f_00005-10-0 loss: 0.647692  [   32/  146]
train() client id: f_00005-10-1 loss: 0.743954  [   64/  146]
train() client id: f_00005-10-2 loss: 0.760208  [   96/  146]
train() client id: f_00005-10-3 loss: 1.064627  [  128/  146]
train() client id: f_00005-11-0 loss: 0.687807  [   32/  146]
train() client id: f_00005-11-1 loss: 1.015975  [   64/  146]
train() client id: f_00005-11-2 loss: 0.777681  [   96/  146]
train() client id: f_00005-11-3 loss: 0.737843  [  128/  146]
train() client id: f_00006-0-0 loss: 0.517348  [   32/   54]
train() client id: f_00006-1-0 loss: 0.573151  [   32/   54]
train() client id: f_00006-2-0 loss: 0.521174  [   32/   54]
train() client id: f_00006-3-0 loss: 0.467410  [   32/   54]
train() client id: f_00006-4-0 loss: 0.514793  [   32/   54]
train() client id: f_00006-5-0 loss: 0.504916  [   32/   54]
train() client id: f_00006-6-0 loss: 0.512098  [   32/   54]
train() client id: f_00006-7-0 loss: 0.484406  [   32/   54]
train() client id: f_00006-8-0 loss: 0.525748  [   32/   54]
train() client id: f_00006-9-0 loss: 0.572817  [   32/   54]
train() client id: f_00006-10-0 loss: 0.487704  [   32/   54]
train() client id: f_00006-11-0 loss: 0.547366  [   32/   54]
train() client id: f_00007-0-0 loss: 0.672786  [   32/  179]
train() client id: f_00007-0-1 loss: 0.958899  [   64/  179]
train() client id: f_00007-0-2 loss: 0.546843  [   96/  179]
train() client id: f_00007-0-3 loss: 0.490325  [  128/  179]
train() client id: f_00007-0-4 loss: 0.478489  [  160/  179]
train() client id: f_00007-1-0 loss: 0.608378  [   32/  179]
train() client id: f_00007-1-1 loss: 0.520519  [   64/  179]
train() client id: f_00007-1-2 loss: 0.612895  [   96/  179]
train() client id: f_00007-1-3 loss: 0.686615  [  128/  179]
train() client id: f_00007-1-4 loss: 0.707955  [  160/  179]
train() client id: f_00007-2-0 loss: 0.603717  [   32/  179]
train() client id: f_00007-2-1 loss: 0.836321  [   64/  179]
train() client id: f_00007-2-2 loss: 0.520629  [   96/  179]
train() client id: f_00007-2-3 loss: 0.489432  [  128/  179]
train() client id: f_00007-2-4 loss: 0.609469  [  160/  179]
train() client id: f_00007-3-0 loss: 0.719251  [   32/  179]
train() client id: f_00007-3-1 loss: 0.539352  [   64/  179]
train() client id: f_00007-3-2 loss: 0.436009  [   96/  179]
train() client id: f_00007-3-3 loss: 0.665763  [  128/  179]
train() client id: f_00007-3-4 loss: 0.686068  [  160/  179]
train() client id: f_00007-4-0 loss: 0.618996  [   32/  179]
train() client id: f_00007-4-1 loss: 0.609200  [   64/  179]
train() client id: f_00007-4-2 loss: 0.510179  [   96/  179]
train() client id: f_00007-4-3 loss: 0.630012  [  128/  179]
train() client id: f_00007-4-4 loss: 0.668011  [  160/  179]
train() client id: f_00007-5-0 loss: 0.783536  [   32/  179]
train() client id: f_00007-5-1 loss: 0.591233  [   64/  179]
train() client id: f_00007-5-2 loss: 0.450578  [   96/  179]
train() client id: f_00007-5-3 loss: 0.534497  [  128/  179]
train() client id: f_00007-5-4 loss: 0.537118  [  160/  179]
train() client id: f_00007-6-0 loss: 0.540743  [   32/  179]
train() client id: f_00007-6-1 loss: 0.532887  [   64/  179]
train() client id: f_00007-6-2 loss: 0.650205  [   96/  179]
train() client id: f_00007-6-3 loss: 0.573583  [  128/  179]
train() client id: f_00007-6-4 loss: 0.590885  [  160/  179]
train() client id: f_00007-7-0 loss: 0.589387  [   32/  179]
train() client id: f_00007-7-1 loss: 0.605532  [   64/  179]
train() client id: f_00007-7-2 loss: 0.700377  [   96/  179]
train() client id: f_00007-7-3 loss: 0.484942  [  128/  179]
train() client id: f_00007-7-4 loss: 0.499567  [  160/  179]
train() client id: f_00007-8-0 loss: 0.455461  [   32/  179]
train() client id: f_00007-8-1 loss: 0.434215  [   64/  179]
train() client id: f_00007-8-2 loss: 0.628552  [   96/  179]
train() client id: f_00007-8-3 loss: 0.686158  [  128/  179]
train() client id: f_00007-8-4 loss: 0.637969  [  160/  179]
train() client id: f_00007-9-0 loss: 0.602374  [   32/  179]
train() client id: f_00007-9-1 loss: 0.494740  [   64/  179]
train() client id: f_00007-9-2 loss: 0.547297  [   96/  179]
train() client id: f_00007-9-3 loss: 0.511484  [  128/  179]
train() client id: f_00007-9-4 loss: 0.567613  [  160/  179]
train() client id: f_00007-10-0 loss: 0.627037  [   32/  179]
train() client id: f_00007-10-1 loss: 0.626360  [   64/  179]
train() client id: f_00007-10-2 loss: 0.435396  [   96/  179]
train() client id: f_00007-10-3 loss: 0.448301  [  128/  179]
train() client id: f_00007-10-4 loss: 0.765850  [  160/  179]
train() client id: f_00007-11-0 loss: 0.667601  [   32/  179]
train() client id: f_00007-11-1 loss: 0.433188  [   64/  179]
train() client id: f_00007-11-2 loss: 0.484117  [   96/  179]
train() client id: f_00007-11-3 loss: 0.721079  [  128/  179]
train() client id: f_00007-11-4 loss: 0.558748  [  160/  179]
train() client id: f_00008-0-0 loss: 0.723322  [   32/  130]
train() client id: f_00008-0-1 loss: 0.700702  [   64/  130]
train() client id: f_00008-0-2 loss: 0.800706  [   96/  130]
train() client id: f_00008-0-3 loss: 0.850785  [  128/  130]
train() client id: f_00008-1-0 loss: 0.870733  [   32/  130]
train() client id: f_00008-1-1 loss: 0.644909  [   64/  130]
train() client id: f_00008-1-2 loss: 0.873991  [   96/  130]
train() client id: f_00008-1-3 loss: 0.660169  [  128/  130]
train() client id: f_00008-2-0 loss: 0.676224  [   32/  130]
train() client id: f_00008-2-1 loss: 0.768191  [   64/  130]
train() client id: f_00008-2-2 loss: 0.806522  [   96/  130]
train() client id: f_00008-2-3 loss: 0.761261  [  128/  130]
train() client id: f_00008-3-0 loss: 0.700783  [   32/  130]
train() client id: f_00008-3-1 loss: 0.845164  [   64/  130]
train() client id: f_00008-3-2 loss: 0.717133  [   96/  130]
train() client id: f_00008-3-3 loss: 0.770254  [  128/  130]
train() client id: f_00008-4-0 loss: 0.874983  [   32/  130]
train() client id: f_00008-4-1 loss: 0.746921  [   64/  130]
train() client id: f_00008-4-2 loss: 0.709063  [   96/  130]
train() client id: f_00008-4-3 loss: 0.730926  [  128/  130]
train() client id: f_00008-5-0 loss: 0.704570  [   32/  130]
train() client id: f_00008-5-1 loss: 0.786697  [   64/  130]
train() client id: f_00008-5-2 loss: 0.756144  [   96/  130]
train() client id: f_00008-5-3 loss: 0.802205  [  128/  130]
train() client id: f_00008-6-0 loss: 0.847698  [   32/  130]
train() client id: f_00008-6-1 loss: 0.679398  [   64/  130]
train() client id: f_00008-6-2 loss: 0.705012  [   96/  130]
train() client id: f_00008-6-3 loss: 0.846618  [  128/  130]
train() client id: f_00008-7-0 loss: 0.821732  [   32/  130]
train() client id: f_00008-7-1 loss: 0.760195  [   64/  130]
train() client id: f_00008-7-2 loss: 0.775266  [   96/  130]
train() client id: f_00008-7-3 loss: 0.721040  [  128/  130]
train() client id: f_00008-8-0 loss: 0.839939  [   32/  130]
train() client id: f_00008-8-1 loss: 0.838144  [   64/  130]
train() client id: f_00008-8-2 loss: 0.715310  [   96/  130]
train() client id: f_00008-8-3 loss: 0.693389  [  128/  130]
train() client id: f_00008-9-0 loss: 0.687198  [   32/  130]
train() client id: f_00008-9-1 loss: 0.805573  [   64/  130]
train() client id: f_00008-9-2 loss: 0.830500  [   96/  130]
train() client id: f_00008-9-3 loss: 0.730249  [  128/  130]
train() client id: f_00008-10-0 loss: 0.845952  [   32/  130]
train() client id: f_00008-10-1 loss: 0.779031  [   64/  130]
train() client id: f_00008-10-2 loss: 0.763325  [   96/  130]
train() client id: f_00008-10-3 loss: 0.692015  [  128/  130]
train() client id: f_00008-11-0 loss: 0.760716  [   32/  130]
train() client id: f_00008-11-1 loss: 0.744426  [   64/  130]
train() client id: f_00008-11-2 loss: 0.700385  [   96/  130]
train() client id: f_00008-11-3 loss: 0.845913  [  128/  130]
train() client id: f_00009-0-0 loss: 1.071923  [   32/  118]
train() client id: f_00009-0-1 loss: 1.093723  [   64/  118]
train() client id: f_00009-0-2 loss: 1.152891  [   96/  118]
train() client id: f_00009-1-0 loss: 1.163759  [   32/  118]
train() client id: f_00009-1-1 loss: 1.010311  [   64/  118]
train() client id: f_00009-1-2 loss: 1.111274  [   96/  118]
train() client id: f_00009-2-0 loss: 1.058691  [   32/  118]
train() client id: f_00009-2-1 loss: 1.032993  [   64/  118]
train() client id: f_00009-2-2 loss: 1.031178  [   96/  118]
train() client id: f_00009-3-0 loss: 1.116669  [   32/  118]
train() client id: f_00009-3-1 loss: 0.906106  [   64/  118]
train() client id: f_00009-3-2 loss: 1.083351  [   96/  118]
train() client id: f_00009-4-0 loss: 0.973853  [   32/  118]
train() client id: f_00009-4-1 loss: 0.888409  [   64/  118]
train() client id: f_00009-4-2 loss: 1.097848  [   96/  118]
train() client id: f_00009-5-0 loss: 0.969518  [   32/  118]
train() client id: f_00009-5-1 loss: 0.861093  [   64/  118]
train() client id: f_00009-5-2 loss: 1.083062  [   96/  118]
train() client id: f_00009-6-0 loss: 0.987638  [   32/  118]
train() client id: f_00009-6-1 loss: 0.905235  [   64/  118]
train() client id: f_00009-6-2 loss: 0.896567  [   96/  118]
train() client id: f_00009-7-0 loss: 0.948464  [   32/  118]
train() client id: f_00009-7-1 loss: 0.994375  [   64/  118]
train() client id: f_00009-7-2 loss: 0.961880  [   96/  118]
train() client id: f_00009-8-0 loss: 0.976530  [   32/  118]
train() client id: f_00009-8-1 loss: 0.854207  [   64/  118]
train() client id: f_00009-8-2 loss: 0.875832  [   96/  118]
train() client id: f_00009-9-0 loss: 0.961928  [   32/  118]
train() client id: f_00009-9-1 loss: 0.921538  [   64/  118]
train() client id: f_00009-9-2 loss: 0.869203  [   96/  118]
train() client id: f_00009-10-0 loss: 0.874045  [   32/  118]
train() client id: f_00009-10-1 loss: 0.819246  [   64/  118]
train() client id: f_00009-10-2 loss: 0.921361  [   96/  118]
train() client id: f_00009-11-0 loss: 0.974772  [   32/  118]
train() client id: f_00009-11-1 loss: 0.945242  [   64/  118]
train() client id: f_00009-11-2 loss: 0.860665  [   96/  118]
At round 31 accuracy: 0.6472148541114059
At round 31 training accuracy: 0.5841716968477532
At round 31 training loss: 0.8368196954171568
update_location
xs = -3.905658 4.200318 175.009024 18.811294 0.979296 3.956410 -137.443192 -116.324852 159.663977 -102.060879 
ys = 167.587959 150.555839 1.320614 -137.455176 129.350187 112.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 195.194719 180.789113 201.568605 171.019853 163.500550 150.806783 169.992710 153.401915 189.212197 142.942068 
dists_bs = 171.254816 180.859810 390.731168 367.683362 181.803790 189.453098 181.552056 183.795069 369.843880 185.904838 
uav_gains = -107.380487 -106.477667 -107.780992 -105.850671 -105.351902 -104.465407 -105.783494 -104.651882 -107.006694 -103.881395 
bs_gains = -102.109057 -102.772637 -112.139645 -111.400331 -102.835941 -103.337107 -102.819092 -102.968407 -111.471575 -103.107199 
Round 32
-------------------------------
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.76786425 14.02640477  6.66672465  2.4004724  16.17773155  7.78673034
  2.97703515  9.52653575  7.02653207  6.31639524]
obj_prev = 79.67242616820144
eta_min = 2.1062834512478834e-14	eta_max = 0.9270669874026624
af = 16.815721006576958	bf = 1.436444871137545	zeta = 18.497293107234654	eta = 0.9090909090909091
af = 16.815721006576958	bf = 1.436444871137545	zeta = 33.430696901951336	eta = 0.5030024069164837
af = 16.815721006576958	bf = 1.436444871137545	zeta = 26.140048449463926	eta = 0.643293413900379
af = 16.815721006576958	bf = 1.436444871137545	zeta = 24.823688418099213	eta = 0.677406222772134
af = 16.815721006576958	bf = 1.436444871137545	zeta = 24.755287540536187	eta = 0.6792779513888344
af = 16.815721006576958	bf = 1.436444871137545	zeta = 24.755088130913464	eta = 0.6792834231754543
eta = 0.6792834231754543
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [0.03191095 0.06711429 0.03140442 0.01089024 0.07749801 0.03697618
 0.01367612 0.0453338  0.03292399 0.02988485]
ene_total = [2.19996663 3.98610054 2.18445289 1.03108082 4.54479709 2.37816282
 1.17905352 2.85331091 2.40505659 1.99310631]
ti_comp = [0.43830311 0.4576972  0.43612707 0.44578269 0.45748595 0.45576916
 0.44608386 0.45085659 0.41023879 0.45656667]
ti_coms = [0.09059974 0.07120565 0.09277578 0.08312016 0.07141689 0.07313368
 0.08281899 0.07804626 0.11866406 0.07233618]
t_total = [28.39986572 28.39986572 28.39986572 28.39986572 28.39986572 28.39986572
 28.39986572 28.39986572 28.39986572 28.39986572]
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [1.05718244e-05 9.01921040e-05 1.01771547e-05 4.06206033e-07
 1.38994033e-04 1.52109415e-05 8.03407087e-07 2.86463737e-05
 1.32539102e-05 8.00248765e-06]
ene_total = [0.48705029 0.38718743 0.49871352 0.44634221 0.39094219 0.39351399
 0.44474637 0.42061388 0.63788794 0.38884464]
optimize_network iter = 0 obj = 4.495842450611667
eta = 0.6792834231754543
freqs = [36402828.40748152 73317349.05671008 36003753.27798384 12214744.53533785
 84699880.32505599 40564586.30164858 15329090.24416601 50275192.55652302
 40127838.49551824 32727809.18735842]
eta_min = 0.6792834231754692	eta_max = 0.6792834231754555
af = 0.014083508139639458	bf = 1.436444871137545	zeta = 0.015491858953603405	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [2.60553049e-06 2.22287345e-05 2.50826025e-06 1.00113487e-07
 3.42564517e-05 3.74888670e-06 1.98007609e-07 7.06018162e-06
 3.26655702e-06 1.97229209e-06]
ene_total = [1.71674703 1.35307722 1.75794987 1.57458483 1.35935733 1.38609978
 1.56889819 1.47978715 2.24850168 1.37065582]
ti_comp = [0.43830311 0.4576972  0.43612707 0.44578269 0.45748595 0.45576916
 0.44608386 0.45085659 0.41023879 0.45656667]
ti_coms = [0.09059974 0.07120565 0.09277578 0.08312016 0.07141689 0.07313368
 0.08281899 0.07804626 0.11866406 0.07233618]
t_total = [28.39986572 28.39986572 28.39986572 28.39986572 28.39986572 28.39986572
 28.39986572 28.39986572 28.39986572 28.39986572]
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [1.05718244e-05 9.01921040e-05 1.01771547e-05 4.06206033e-07
 1.38994033e-04 1.52109415e-05 8.03407087e-07 2.86463737e-05
 1.32539102e-05 8.00248765e-06]
ene_total = [0.48705029 0.38718743 0.49871352 0.44634221 0.39094219 0.39351399
 0.44474637 0.42061388 0.63788794 0.38884464]
optimize_network iter = 1 obj = 4.495842450611684
eta = 0.6792834231754555
freqs = [36402828.40748152 73317349.05671008 36003753.27798384 12214744.53533785
 84699880.32505599 40564586.30164856 15329090.24416601 50275192.55652302
 40127838.49551825 32727809.18735842]
Done!
ene_coms = [0.00905997 0.00712056 0.00927758 0.00831202 0.00714169 0.00731337
 0.0082819  0.00780463 0.01186641 0.00723362]
ene_comp = [1.00182543e-05 8.54693954e-05 9.64425066e-06 3.84935959e-07
 1.31715920e-04 1.44144544e-05 7.61338465e-07 2.71463702e-05
 1.25598987e-05 7.58345522e-06]
ene_total = [0.00906999 0.00720603 0.00928722 0.0083124  0.00727341 0.00732778
 0.00828266 0.00783177 0.01187897 0.0072412 ]
At round 32 energy consumption: 0.08371143671629988
At round 32 eta: 0.6792834231754555
At round 32 a_n: 17.221135753236222
At round 32 local rounds: 12.663073636613152
At round 32 global rounds: 53.69580806749958
gradient difference: 0.48097649216651917
train() client id: f_00000-0-0 loss: 1.132649  [   32/  126]
train() client id: f_00000-0-1 loss: 1.003826  [   64/  126]
train() client id: f_00000-0-2 loss: 1.489619  [   96/  126]
train() client id: f_00000-1-0 loss: 1.085014  [   32/  126]
train() client id: f_00000-1-1 loss: 1.103359  [   64/  126]
train() client id: f_00000-1-2 loss: 1.091270  [   96/  126]
train() client id: f_00000-2-0 loss: 1.023623  [   32/  126]
train() client id: f_00000-2-1 loss: 1.229674  [   64/  126]
train() client id: f_00000-2-2 loss: 1.033700  [   96/  126]
train() client id: f_00000-3-0 loss: 0.991643  [   32/  126]
train() client id: f_00000-3-1 loss: 1.192479  [   64/  126]
train() client id: f_00000-3-2 loss: 0.998724  [   96/  126]
train() client id: f_00000-4-0 loss: 0.892808  [   32/  126]
train() client id: f_00000-4-1 loss: 1.010724  [   64/  126]
train() client id: f_00000-4-2 loss: 1.074748  [   96/  126]
train() client id: f_00000-5-0 loss: 0.943021  [   32/  126]
train() client id: f_00000-5-1 loss: 0.990615  [   64/  126]
train() client id: f_00000-5-2 loss: 0.983564  [   96/  126]
train() client id: f_00000-6-0 loss: 1.007270  [   32/  126]
train() client id: f_00000-6-1 loss: 0.850475  [   64/  126]
train() client id: f_00000-6-2 loss: 0.976972  [   96/  126]
train() client id: f_00000-7-0 loss: 0.918063  [   32/  126]
train() client id: f_00000-7-1 loss: 0.913000  [   64/  126]
train() client id: f_00000-7-2 loss: 0.976074  [   96/  126]
train() client id: f_00000-8-0 loss: 1.011490  [   32/  126]
train() client id: f_00000-8-1 loss: 0.841098  [   64/  126]
train() client id: f_00000-8-2 loss: 0.985897  [   96/  126]
train() client id: f_00000-9-0 loss: 0.871401  [   32/  126]
train() client id: f_00000-9-1 loss: 0.837699  [   64/  126]
train() client id: f_00000-9-2 loss: 0.958323  [   96/  126]
train() client id: f_00000-10-0 loss: 0.938170  [   32/  126]
train() client id: f_00000-10-1 loss: 0.858421  [   64/  126]
train() client id: f_00000-10-2 loss: 0.915176  [   96/  126]
train() client id: f_00000-11-0 loss: 0.952221  [   32/  126]
train() client id: f_00000-11-1 loss: 0.843379  [   64/  126]
train() client id: f_00000-11-2 loss: 0.772087  [   96/  126]
train() client id: f_00001-0-0 loss: 0.568430  [   32/  265]
train() client id: f_00001-0-1 loss: 0.487899  [   64/  265]
train() client id: f_00001-0-2 loss: 0.448081  [   96/  265]
train() client id: f_00001-0-3 loss: 0.439777  [  128/  265]
train() client id: f_00001-0-4 loss: 0.557852  [  160/  265]
train() client id: f_00001-0-5 loss: 0.472369  [  192/  265]
train() client id: f_00001-0-6 loss: 0.496696  [  224/  265]
train() client id: f_00001-0-7 loss: 0.390512  [  256/  265]
train() client id: f_00001-1-0 loss: 0.455089  [   32/  265]
train() client id: f_00001-1-1 loss: 0.507838  [   64/  265]
train() client id: f_00001-1-2 loss: 0.545959  [   96/  265]
train() client id: f_00001-1-3 loss: 0.377746  [  128/  265]
train() client id: f_00001-1-4 loss: 0.384236  [  160/  265]
train() client id: f_00001-1-5 loss: 0.419833  [  192/  265]
train() client id: f_00001-1-6 loss: 0.615329  [  224/  265]
train() client id: f_00001-1-7 loss: 0.492787  [  256/  265]
train() client id: f_00001-2-0 loss: 0.445383  [   32/  265]
train() client id: f_00001-2-1 loss: 0.426569  [   64/  265]
train() client id: f_00001-2-2 loss: 0.543028  [   96/  265]
train() client id: f_00001-2-3 loss: 0.502337  [  128/  265]
train() client id: f_00001-2-4 loss: 0.437461  [  160/  265]
train() client id: f_00001-2-5 loss: 0.475087  [  192/  265]
train() client id: f_00001-2-6 loss: 0.534472  [  224/  265]
train() client id: f_00001-2-7 loss: 0.393455  [  256/  265]
train() client id: f_00001-3-0 loss: 0.426876  [   32/  265]
train() client id: f_00001-3-1 loss: 0.428061  [   64/  265]
train() client id: f_00001-3-2 loss: 0.434768  [   96/  265]
train() client id: f_00001-3-3 loss: 0.553992  [  128/  265]
train() client id: f_00001-3-4 loss: 0.467218  [  160/  265]
train() client id: f_00001-3-5 loss: 0.373803  [  192/  265]
train() client id: f_00001-3-6 loss: 0.592336  [  224/  265]
train() client id: f_00001-3-7 loss: 0.448839  [  256/  265]
train() client id: f_00001-4-0 loss: 0.469360  [   32/  265]
train() client id: f_00001-4-1 loss: 0.467032  [   64/  265]
train() client id: f_00001-4-2 loss: 0.451740  [   96/  265]
train() client id: f_00001-4-3 loss: 0.419244  [  128/  265]
train() client id: f_00001-4-4 loss: 0.454487  [  160/  265]
train() client id: f_00001-4-5 loss: 0.375569  [  192/  265]
train() client id: f_00001-4-6 loss: 0.467854  [  224/  265]
train() client id: f_00001-4-7 loss: 0.575383  [  256/  265]
train() client id: f_00001-5-0 loss: 0.407638  [   32/  265]
train() client id: f_00001-5-1 loss: 0.461503  [   64/  265]
train() client id: f_00001-5-2 loss: 0.505019  [   96/  265]
train() client id: f_00001-5-3 loss: 0.547391  [  128/  265]
train() client id: f_00001-5-4 loss: 0.367746  [  160/  265]
train() client id: f_00001-5-5 loss: 0.419888  [  192/  265]
train() client id: f_00001-5-6 loss: 0.521360  [  224/  265]
train() client id: f_00001-5-7 loss: 0.468754  [  256/  265]
train() client id: f_00001-6-0 loss: 0.442738  [   32/  265]
train() client id: f_00001-6-1 loss: 0.485513  [   64/  265]
train() client id: f_00001-6-2 loss: 0.476540  [   96/  265]
train() client id: f_00001-6-3 loss: 0.500030  [  128/  265]
train() client id: f_00001-6-4 loss: 0.454510  [  160/  265]
train() client id: f_00001-6-5 loss: 0.497193  [  192/  265]
train() client id: f_00001-6-6 loss: 0.411028  [  224/  265]
train() client id: f_00001-6-7 loss: 0.418773  [  256/  265]
train() client id: f_00001-7-0 loss: 0.362383  [   32/  265]
train() client id: f_00001-7-1 loss: 0.473278  [   64/  265]
train() client id: f_00001-7-2 loss: 0.574234  [   96/  265]
train() client id: f_00001-7-3 loss: 0.517439  [  128/  265]
train() client id: f_00001-7-4 loss: 0.369211  [  160/  265]
train() client id: f_00001-7-5 loss: 0.417789  [  192/  265]
train() client id: f_00001-7-6 loss: 0.362148  [  224/  265]
train() client id: f_00001-7-7 loss: 0.597951  [  256/  265]
train() client id: f_00001-8-0 loss: 0.508516  [   32/  265]
train() client id: f_00001-8-1 loss: 0.454837  [   64/  265]
train() client id: f_00001-8-2 loss: 0.358006  [   96/  265]
train() client id: f_00001-8-3 loss: 0.485246  [  128/  265]
train() client id: f_00001-8-4 loss: 0.437177  [  160/  265]
train() client id: f_00001-8-5 loss: 0.488478  [  192/  265]
train() client id: f_00001-8-6 loss: 0.459544  [  224/  265]
train() client id: f_00001-8-7 loss: 0.481560  [  256/  265]
train() client id: f_00001-9-0 loss: 0.423321  [   32/  265]
train() client id: f_00001-9-1 loss: 0.517143  [   64/  265]
train() client id: f_00001-9-2 loss: 0.447881  [   96/  265]
train() client id: f_00001-9-3 loss: 0.602323  [  128/  265]
train() client id: f_00001-9-4 loss: 0.370382  [  160/  265]
train() client id: f_00001-9-5 loss: 0.360218  [  192/  265]
train() client id: f_00001-9-6 loss: 0.556321  [  224/  265]
train() client id: f_00001-9-7 loss: 0.410190  [  256/  265]
train() client id: f_00001-10-0 loss: 0.504769  [   32/  265]
train() client id: f_00001-10-1 loss: 0.510576  [   64/  265]
train() client id: f_00001-10-2 loss: 0.358232  [   96/  265]
train() client id: f_00001-10-3 loss: 0.450907  [  128/  265]
train() client id: f_00001-10-4 loss: 0.527491  [  160/  265]
train() client id: f_00001-10-5 loss: 0.435259  [  192/  265]
train() client id: f_00001-10-6 loss: 0.463442  [  224/  265]
train() client id: f_00001-10-7 loss: 0.418365  [  256/  265]
train() client id: f_00001-11-0 loss: 0.362835  [   32/  265]
train() client id: f_00001-11-1 loss: 0.505792  [   64/  265]
train() client id: f_00001-11-2 loss: 0.509315  [   96/  265]
train() client id: f_00001-11-3 loss: 0.371021  [  128/  265]
train() client id: f_00001-11-4 loss: 0.532385  [  160/  265]
train() client id: f_00001-11-5 loss: 0.452308  [  192/  265]
train() client id: f_00001-11-6 loss: 0.494774  [  224/  265]
train() client id: f_00001-11-7 loss: 0.454111  [  256/  265]
train() client id: f_00002-0-0 loss: 1.162941  [   32/  124]
train() client id: f_00002-0-1 loss: 1.179603  [   64/  124]
train() client id: f_00002-0-2 loss: 1.248926  [   96/  124]
train() client id: f_00002-1-0 loss: 1.112609  [   32/  124]
train() client id: f_00002-1-1 loss: 1.182516  [   64/  124]
train() client id: f_00002-1-2 loss: 1.313046  [   96/  124]
train() client id: f_00002-2-0 loss: 1.237372  [   32/  124]
train() client id: f_00002-2-1 loss: 1.056947  [   64/  124]
train() client id: f_00002-2-2 loss: 1.020922  [   96/  124]
train() client id: f_00002-3-0 loss: 1.082262  [   32/  124]
train() client id: f_00002-3-1 loss: 1.057364  [   64/  124]
train() client id: f_00002-3-2 loss: 1.030688  [   96/  124]
train() client id: f_00002-4-0 loss: 1.047603  [   32/  124]
train() client id: f_00002-4-1 loss: 1.093014  [   64/  124]
train() client id: f_00002-4-2 loss: 1.144945  [   96/  124]
train() client id: f_00002-5-0 loss: 1.078526  [   32/  124]
train() client id: f_00002-5-1 loss: 1.024593  [   64/  124]
train() client id: f_00002-5-2 loss: 1.159984  [   96/  124]
train() client id: f_00002-6-0 loss: 1.023785  [   32/  124]
train() client id: f_00002-6-1 loss: 1.152510  [   64/  124]
train() client id: f_00002-6-2 loss: 1.034337  [   96/  124]
train() client id: f_00002-7-0 loss: 0.873581  [   32/  124]
train() client id: f_00002-7-1 loss: 1.093639  [   64/  124]
train() client id: f_00002-7-2 loss: 1.174298  [   96/  124]
train() client id: f_00002-8-0 loss: 1.034239  [   32/  124]
train() client id: f_00002-8-1 loss: 1.070038  [   64/  124]
train() client id: f_00002-8-2 loss: 0.929347  [   96/  124]
train() client id: f_00002-9-0 loss: 1.006205  [   32/  124]
train() client id: f_00002-9-1 loss: 0.941524  [   64/  124]
train() client id: f_00002-9-2 loss: 1.184468  [   96/  124]
train() client id: f_00002-10-0 loss: 0.832050  [   32/  124]
train() client id: f_00002-10-1 loss: 1.052410  [   64/  124]
train() client id: f_00002-10-2 loss: 1.291284  [   96/  124]
train() client id: f_00002-11-0 loss: 0.987889  [   32/  124]
train() client id: f_00002-11-1 loss: 0.856133  [   64/  124]
train() client id: f_00002-11-2 loss: 0.979066  [   96/  124]
train() client id: f_00003-0-0 loss: 0.704756  [   32/   43]
train() client id: f_00003-1-0 loss: 0.629210  [   32/   43]
train() client id: f_00003-2-0 loss: 0.807679  [   32/   43]
train() client id: f_00003-3-0 loss: 0.690258  [   32/   43]
train() client id: f_00003-4-0 loss: 0.799912  [   32/   43]
train() client id: f_00003-5-0 loss: 0.825329  [   32/   43]
train() client id: f_00003-6-0 loss: 0.859908  [   32/   43]
train() client id: f_00003-7-0 loss: 0.639183  [   32/   43]
train() client id: f_00003-8-0 loss: 0.744432  [   32/   43]
train() client id: f_00003-9-0 loss: 0.669267  [   32/   43]
train() client id: f_00003-10-0 loss: 0.675352  [   32/   43]
train() client id: f_00003-11-0 loss: 0.627664  [   32/   43]
train() client id: f_00004-0-0 loss: 0.792947  [   32/  306]
train() client id: f_00004-0-1 loss: 0.939045  [   64/  306]
train() client id: f_00004-0-2 loss: 0.949380  [   96/  306]
train() client id: f_00004-0-3 loss: 0.852813  [  128/  306]
train() client id: f_00004-0-4 loss: 0.992171  [  160/  306]
train() client id: f_00004-0-5 loss: 0.901347  [  192/  306]
train() client id: f_00004-0-6 loss: 0.848573  [  224/  306]
train() client id: f_00004-0-7 loss: 0.761633  [  256/  306]
train() client id: f_00004-0-8 loss: 0.917129  [  288/  306]
train() client id: f_00004-1-0 loss: 0.922447  [   32/  306]
train() client id: f_00004-1-1 loss: 0.823327  [   64/  306]
train() client id: f_00004-1-2 loss: 0.841490  [   96/  306]
train() client id: f_00004-1-3 loss: 0.867315  [  128/  306]
train() client id: f_00004-1-4 loss: 0.897254  [  160/  306]
train() client id: f_00004-1-5 loss: 0.896265  [  192/  306]
train() client id: f_00004-1-6 loss: 1.018382  [  224/  306]
train() client id: f_00004-1-7 loss: 0.880957  [  256/  306]
train() client id: f_00004-1-8 loss: 0.719680  [  288/  306]
train() client id: f_00004-2-0 loss: 0.832075  [   32/  306]
train() client id: f_00004-2-1 loss: 0.893220  [   64/  306]
train() client id: f_00004-2-2 loss: 0.857142  [   96/  306]
train() client id: f_00004-2-3 loss: 0.961631  [  128/  306]
train() client id: f_00004-2-4 loss: 0.776552  [  160/  306]
train() client id: f_00004-2-5 loss: 0.987339  [  192/  306]
train() client id: f_00004-2-6 loss: 0.864423  [  224/  306]
train() client id: f_00004-2-7 loss: 0.751696  [  256/  306]
train() client id: f_00004-2-8 loss: 0.914748  [  288/  306]
train() client id: f_00004-3-0 loss: 0.785318  [   32/  306]
train() client id: f_00004-3-1 loss: 0.969817  [   64/  306]
train() client id: f_00004-3-2 loss: 0.826459  [   96/  306]
train() client id: f_00004-3-3 loss: 0.836165  [  128/  306]
train() client id: f_00004-3-4 loss: 0.874039  [  160/  306]
train() client id: f_00004-3-5 loss: 0.984487  [  192/  306]
train() client id: f_00004-3-6 loss: 0.893515  [  224/  306]
train() client id: f_00004-3-7 loss: 0.879918  [  256/  306]
train() client id: f_00004-3-8 loss: 0.777506  [  288/  306]
train() client id: f_00004-4-0 loss: 0.824249  [   32/  306]
train() client id: f_00004-4-1 loss: 0.831421  [   64/  306]
train() client id: f_00004-4-2 loss: 1.031851  [   96/  306]
train() client id: f_00004-4-3 loss: 0.892537  [  128/  306]
train() client id: f_00004-4-4 loss: 0.916429  [  160/  306]
train() client id: f_00004-4-5 loss: 0.787495  [  192/  306]
train() client id: f_00004-4-6 loss: 0.640035  [  224/  306]
train() client id: f_00004-4-7 loss: 0.891807  [  256/  306]
train() client id: f_00004-4-8 loss: 1.002820  [  288/  306]
train() client id: f_00004-5-0 loss: 0.820248  [   32/  306]
train() client id: f_00004-5-1 loss: 1.020536  [   64/  306]
train() client id: f_00004-5-2 loss: 0.903075  [   96/  306]
train() client id: f_00004-5-3 loss: 0.975215  [  128/  306]
train() client id: f_00004-5-4 loss: 0.776216  [  160/  306]
train() client id: f_00004-5-5 loss: 0.883215  [  192/  306]
train() client id: f_00004-5-6 loss: 0.800015  [  224/  306]
train() client id: f_00004-5-7 loss: 0.789354  [  256/  306]
train() client id: f_00004-5-8 loss: 0.856778  [  288/  306]
train() client id: f_00004-6-0 loss: 0.633031  [   32/  306]
train() client id: f_00004-6-1 loss: 0.786298  [   64/  306]
train() client id: f_00004-6-2 loss: 0.782076  [   96/  306]
train() client id: f_00004-6-3 loss: 0.959923  [  128/  306]
train() client id: f_00004-6-4 loss: 0.878799  [  160/  306]
train() client id: f_00004-6-5 loss: 0.892510  [  192/  306]
train() client id: f_00004-6-6 loss: 0.976457  [  224/  306]
train() client id: f_00004-6-7 loss: 0.852290  [  256/  306]
train() client id: f_00004-6-8 loss: 0.956023  [  288/  306]
train() client id: f_00004-7-0 loss: 0.665917  [   32/  306]
train() client id: f_00004-7-1 loss: 0.853068  [   64/  306]
train() client id: f_00004-7-2 loss: 0.898565  [   96/  306]
train() client id: f_00004-7-3 loss: 0.851755  [  128/  306]
train() client id: f_00004-7-4 loss: 0.921355  [  160/  306]
train() client id: f_00004-7-5 loss: 0.928725  [  192/  306]
train() client id: f_00004-7-6 loss: 0.909069  [  224/  306]
train() client id: f_00004-7-7 loss: 0.988022  [  256/  306]
train() client id: f_00004-7-8 loss: 0.785693  [  288/  306]
train() client id: f_00004-8-0 loss: 0.795958  [   32/  306]
train() client id: f_00004-8-1 loss: 0.815777  [   64/  306]
train() client id: f_00004-8-2 loss: 0.972204  [   96/  306]
train() client id: f_00004-8-3 loss: 0.826409  [  128/  306]
train() client id: f_00004-8-4 loss: 0.923861  [  160/  306]
train() client id: f_00004-8-5 loss: 0.957824  [  192/  306]
train() client id: f_00004-8-6 loss: 0.756554  [  224/  306]
train() client id: f_00004-8-7 loss: 0.853963  [  256/  306]
train() client id: f_00004-8-8 loss: 0.911779  [  288/  306]
train() client id: f_00004-9-0 loss: 0.824791  [   32/  306]
train() client id: f_00004-9-1 loss: 0.754650  [   64/  306]
train() client id: f_00004-9-2 loss: 0.939045  [   96/  306]
train() client id: f_00004-9-3 loss: 0.819868  [  128/  306]
train() client id: f_00004-9-4 loss: 0.795311  [  160/  306]
train() client id: f_00004-9-5 loss: 0.768121  [  192/  306]
train() client id: f_00004-9-6 loss: 1.005713  [  224/  306]
train() client id: f_00004-9-7 loss: 0.959553  [  256/  306]
train() client id: f_00004-9-8 loss: 0.971582  [  288/  306]
train() client id: f_00004-10-0 loss: 0.831071  [   32/  306]
train() client id: f_00004-10-1 loss: 0.904917  [   64/  306]
train() client id: f_00004-10-2 loss: 0.848300  [   96/  306]
train() client id: f_00004-10-3 loss: 0.896240  [  128/  306]
train() client id: f_00004-10-4 loss: 0.805166  [  160/  306]
train() client id: f_00004-10-5 loss: 0.865937  [  192/  306]
train() client id: f_00004-10-6 loss: 0.905991  [  224/  306]
train() client id: f_00004-10-7 loss: 0.833974  [  256/  306]
train() client id: f_00004-10-8 loss: 0.853751  [  288/  306]
train() client id: f_00004-11-0 loss: 0.917776  [   32/  306]
train() client id: f_00004-11-1 loss: 0.828783  [   64/  306]
train() client id: f_00004-11-2 loss: 0.949429  [   96/  306]
train() client id: f_00004-11-3 loss: 0.779398  [  128/  306]
train() client id: f_00004-11-4 loss: 0.996091  [  160/  306]
train() client id: f_00004-11-5 loss: 0.757198  [  192/  306]
train() client id: f_00004-11-6 loss: 0.808470  [  224/  306]
train() client id: f_00004-11-7 loss: 0.882681  [  256/  306]
train() client id: f_00004-11-8 loss: 0.882168  [  288/  306]
train() client id: f_00005-0-0 loss: 0.641105  [   32/  146]
train() client id: f_00005-0-1 loss: 0.734669  [   64/  146]
train() client id: f_00005-0-2 loss: 0.626926  [   96/  146]
train() client id: f_00005-0-3 loss: 0.828424  [  128/  146]
train() client id: f_00005-1-0 loss: 0.744422  [   32/  146]
train() client id: f_00005-1-1 loss: 0.552051  [   64/  146]
train() client id: f_00005-1-2 loss: 0.584322  [   96/  146]
train() client id: f_00005-1-3 loss: 0.597897  [  128/  146]
train() client id: f_00005-2-0 loss: 0.812720  [   32/  146]
train() client id: f_00005-2-1 loss: 0.577309  [   64/  146]
train() client id: f_00005-2-2 loss: 0.649958  [   96/  146]
train() client id: f_00005-2-3 loss: 0.544047  [  128/  146]
train() client id: f_00005-3-0 loss: 0.874820  [   32/  146]
train() client id: f_00005-3-1 loss: 0.643021  [   64/  146]
train() client id: f_00005-3-2 loss: 0.532939  [   96/  146]
train() client id: f_00005-3-3 loss: 0.596802  [  128/  146]
train() client id: f_00005-4-0 loss: 0.790493  [   32/  146]
train() client id: f_00005-4-1 loss: 0.680513  [   64/  146]
train() client id: f_00005-4-2 loss: 0.876079  [   96/  146]
train() client id: f_00005-4-3 loss: 0.570133  [  128/  146]
train() client id: f_00005-5-0 loss: 0.550264  [   32/  146]
train() client id: f_00005-5-1 loss: 0.681513  [   64/  146]
train() client id: f_00005-5-2 loss: 0.741940  [   96/  146]
train() client id: f_00005-5-3 loss: 0.778842  [  128/  146]
train() client id: f_00005-6-0 loss: 0.499267  [   32/  146]
train() client id: f_00005-6-1 loss: 0.795642  [   64/  146]
train() client id: f_00005-6-2 loss: 0.686642  [   96/  146]
train() client id: f_00005-6-3 loss: 0.776803  [  128/  146]
train() client id: f_00005-7-0 loss: 0.750175  [   32/  146]
train() client id: f_00005-7-1 loss: 0.626198  [   64/  146]
train() client id: f_00005-7-2 loss: 0.645862  [   96/  146]
train() client id: f_00005-7-3 loss: 0.849945  [  128/  146]
train() client id: f_00005-8-0 loss: 1.015157  [   32/  146]
train() client id: f_00005-8-1 loss: 0.329255  [   64/  146]
train() client id: f_00005-8-2 loss: 0.629818  [   96/  146]
train() client id: f_00005-8-3 loss: 0.668356  [  128/  146]
train() client id: f_00005-9-0 loss: 0.612890  [   32/  146]
train() client id: f_00005-9-1 loss: 0.789060  [   64/  146]
train() client id: f_00005-9-2 loss: 0.556147  [   96/  146]
train() client id: f_00005-9-3 loss: 0.768608  [  128/  146]
train() client id: f_00005-10-0 loss: 0.870639  [   32/  146]
train() client id: f_00005-10-1 loss: 0.751538  [   64/  146]
train() client id: f_00005-10-2 loss: 0.533319  [   96/  146]
train() client id: f_00005-10-3 loss: 0.568084  [  128/  146]
train() client id: f_00005-11-0 loss: 0.585186  [   32/  146]
train() client id: f_00005-11-1 loss: 0.674338  [   64/  146]
train() client id: f_00005-11-2 loss: 0.725238  [   96/  146]
train() client id: f_00005-11-3 loss: 0.881478  [  128/  146]
train() client id: f_00006-0-0 loss: 0.580651  [   32/   54]
train() client id: f_00006-1-0 loss: 0.570343  [   32/   54]
train() client id: f_00006-2-0 loss: 0.517413  [   32/   54]
train() client id: f_00006-3-0 loss: 0.581777  [   32/   54]
train() client id: f_00006-4-0 loss: 0.582105  [   32/   54]
train() client id: f_00006-5-0 loss: 0.550278  [   32/   54]
train() client id: f_00006-6-0 loss: 0.518443  [   32/   54]
train() client id: f_00006-7-0 loss: 0.548849  [   32/   54]
train() client id: f_00006-8-0 loss: 0.574782  [   32/   54]
train() client id: f_00006-9-0 loss: 0.555316  [   32/   54]
train() client id: f_00006-10-0 loss: 0.538446  [   32/   54]
train() client id: f_00006-11-0 loss: 0.518387  [   32/   54]
train() client id: f_00007-0-0 loss: 0.588913  [   32/  179]
train() client id: f_00007-0-1 loss: 0.599570  [   64/  179]
train() client id: f_00007-0-2 loss: 0.755541  [   96/  179]
train() client id: f_00007-0-3 loss: 0.561272  [  128/  179]
train() client id: f_00007-0-4 loss: 0.821218  [  160/  179]
train() client id: f_00007-1-0 loss: 0.497774  [   32/  179]
train() client id: f_00007-1-1 loss: 0.528317  [   64/  179]
train() client id: f_00007-1-2 loss: 0.674183  [   96/  179]
train() client id: f_00007-1-3 loss: 0.676769  [  128/  179]
train() client id: f_00007-1-4 loss: 0.637660  [  160/  179]
train() client id: f_00007-2-0 loss: 0.736647  [   32/  179]
train() client id: f_00007-2-1 loss: 0.680483  [   64/  179]
train() client id: f_00007-2-2 loss: 0.642447  [   96/  179]
train() client id: f_00007-2-3 loss: 0.523743  [  128/  179]
train() client id: f_00007-2-4 loss: 0.475989  [  160/  179]
train() client id: f_00007-3-0 loss: 0.735774  [   32/  179]
train() client id: f_00007-3-1 loss: 0.490142  [   64/  179]
train() client id: f_00007-3-2 loss: 0.642807  [   96/  179]
train() client id: f_00007-3-3 loss: 0.787927  [  128/  179]
train() client id: f_00007-3-4 loss: 0.494585  [  160/  179]
train() client id: f_00007-4-0 loss: 0.491463  [   32/  179]
train() client id: f_00007-4-1 loss: 0.506951  [   64/  179]
train() client id: f_00007-4-2 loss: 0.632003  [   96/  179]
train() client id: f_00007-4-3 loss: 0.514691  [  128/  179]
train() client id: f_00007-4-4 loss: 0.964692  [  160/  179]
train() client id: f_00007-5-0 loss: 0.550196  [   32/  179]
train() client id: f_00007-5-1 loss: 0.593685  [   64/  179]
train() client id: f_00007-5-2 loss: 0.548570  [   96/  179]
train() client id: f_00007-5-3 loss: 0.632370  [  128/  179]
train() client id: f_00007-5-4 loss: 0.821747  [  160/  179]
train() client id: f_00007-6-0 loss: 0.584625  [   32/  179]
train() client id: f_00007-6-1 loss: 0.605548  [   64/  179]
train() client id: f_00007-6-2 loss: 0.536002  [   96/  179]
train() client id: f_00007-6-3 loss: 0.650326  [  128/  179]
train() client id: f_00007-6-4 loss: 0.570945  [  160/  179]
train() client id: f_00007-7-0 loss: 0.580177  [   32/  179]
train() client id: f_00007-7-1 loss: 0.662769  [   64/  179]
train() client id: f_00007-7-2 loss: 0.714506  [   96/  179]
train() client id: f_00007-7-3 loss: 0.501122  [  128/  179]
train() client id: f_00007-7-4 loss: 0.727298  [  160/  179]
train() client id: f_00007-8-0 loss: 0.655237  [   32/  179]
train() client id: f_00007-8-1 loss: 0.606596  [   64/  179]
train() client id: f_00007-8-2 loss: 0.638983  [   96/  179]
train() client id: f_00007-8-3 loss: 0.491356  [  128/  179]
train() client id: f_00007-8-4 loss: 0.773448  [  160/  179]
train() client id: f_00007-9-0 loss: 0.669314  [   32/  179]
train() client id: f_00007-9-1 loss: 0.632833  [   64/  179]
train() client id: f_00007-9-2 loss: 0.695351  [   96/  179]
train() client id: f_00007-9-3 loss: 0.560902  [  128/  179]
train() client id: f_00007-9-4 loss: 0.539465  [  160/  179]
train() client id: f_00007-10-0 loss: 0.591184  [   32/  179]
train() client id: f_00007-10-1 loss: 0.729972  [   64/  179]
train() client id: f_00007-10-2 loss: 0.659921  [   96/  179]
train() client id: f_00007-10-3 loss: 0.546385  [  128/  179]
train() client id: f_00007-10-4 loss: 0.564827  [  160/  179]
train() client id: f_00007-11-0 loss: 0.749467  [   32/  179]
train() client id: f_00007-11-1 loss: 0.588815  [   64/  179]
train() client id: f_00007-11-2 loss: 0.570872  [   96/  179]
train() client id: f_00007-11-3 loss: 0.722661  [  128/  179]
train() client id: f_00007-11-4 loss: 0.566895  [  160/  179]
train() client id: f_00008-0-0 loss: 0.793564  [   32/  130]
train() client id: f_00008-0-1 loss: 0.659576  [   64/  130]
train() client id: f_00008-0-2 loss: 0.553118  [   96/  130]
train() client id: f_00008-0-3 loss: 0.599348  [  128/  130]
train() client id: f_00008-1-0 loss: 0.814207  [   32/  130]
train() client id: f_00008-1-1 loss: 0.523340  [   64/  130]
train() client id: f_00008-1-2 loss: 0.635411  [   96/  130]
train() client id: f_00008-1-3 loss: 0.617181  [  128/  130]
train() client id: f_00008-2-0 loss: 0.714822  [   32/  130]
train() client id: f_00008-2-1 loss: 0.622870  [   64/  130]
train() client id: f_00008-2-2 loss: 0.588599  [   96/  130]
train() client id: f_00008-2-3 loss: 0.674553  [  128/  130]
train() client id: f_00008-3-0 loss: 0.800771  [   32/  130]
train() client id: f_00008-3-1 loss: 0.623576  [   64/  130]
train() client id: f_00008-3-2 loss: 0.505881  [   96/  130]
train() client id: f_00008-3-3 loss: 0.682951  [  128/  130]
train() client id: f_00008-4-0 loss: 0.682391  [   32/  130]
train() client id: f_00008-4-1 loss: 0.660177  [   64/  130]
train() client id: f_00008-4-2 loss: 0.681103  [   96/  130]
train() client id: f_00008-4-3 loss: 0.579852  [  128/  130]
train() client id: f_00008-5-0 loss: 0.644291  [   32/  130]
train() client id: f_00008-5-1 loss: 0.596902  [   64/  130]
train() client id: f_00008-5-2 loss: 0.651595  [   96/  130]
train() client id: f_00008-5-3 loss: 0.666256  [  128/  130]
train() client id: f_00008-6-0 loss: 0.524198  [   32/  130]
train() client id: f_00008-6-1 loss: 0.690069  [   64/  130]
train() client id: f_00008-6-2 loss: 0.672650  [   96/  130]
train() client id: f_00008-6-3 loss: 0.713123  [  128/  130]
train() client id: f_00008-7-0 loss: 0.815276  [   32/  130]
train() client id: f_00008-7-1 loss: 0.653651  [   64/  130]
train() client id: f_00008-7-2 loss: 0.510351  [   96/  130]
train() client id: f_00008-7-3 loss: 0.631715  [  128/  130]
train() client id: f_00008-8-0 loss: 0.536944  [   32/  130]
train() client id: f_00008-8-1 loss: 0.792103  [   64/  130]
train() client id: f_00008-8-2 loss: 0.706251  [   96/  130]
train() client id: f_00008-8-3 loss: 0.554937  [  128/  130]
train() client id: f_00008-9-0 loss: 0.591666  [   32/  130]
train() client id: f_00008-9-1 loss: 0.603127  [   64/  130]
train() client id: f_00008-9-2 loss: 0.677585  [   96/  130]
train() client id: f_00008-9-3 loss: 0.702949  [  128/  130]
train() client id: f_00008-10-0 loss: 0.762776  [   32/  130]
train() client id: f_00008-10-1 loss: 0.581950  [   64/  130]
train() client id: f_00008-10-2 loss: 0.674688  [   96/  130]
train() client id: f_00008-10-3 loss: 0.561995  [  128/  130]
train() client id: f_00008-11-0 loss: 0.693283  [   32/  130]
train() client id: f_00008-11-1 loss: 0.695912  [   64/  130]
train() client id: f_00008-11-2 loss: 0.636398  [   96/  130]
train() client id: f_00008-11-3 loss: 0.584951  [  128/  130]
train() client id: f_00009-0-0 loss: 1.120366  [   32/  118]
train() client id: f_00009-0-1 loss: 1.055057  [   64/  118]
train() client id: f_00009-0-2 loss: 1.090640  [   96/  118]
train() client id: f_00009-1-0 loss: 0.964950  [   32/  118]
train() client id: f_00009-1-1 loss: 1.169285  [   64/  118]
train() client id: f_00009-1-2 loss: 1.173843  [   96/  118]
train() client id: f_00009-2-0 loss: 1.034446  [   32/  118]
train() client id: f_00009-2-1 loss: 1.063485  [   64/  118]
train() client id: f_00009-2-2 loss: 1.011556  [   96/  118]
train() client id: f_00009-3-0 loss: 0.935778  [   32/  118]
train() client id: f_00009-3-1 loss: 0.966090  [   64/  118]
train() client id: f_00009-3-2 loss: 1.121444  [   96/  118]
train() client id: f_00009-4-0 loss: 1.008516  [   32/  118]
train() client id: f_00009-4-1 loss: 0.867006  [   64/  118]
train() client id: f_00009-4-2 loss: 1.040663  [   96/  118]
train() client id: f_00009-5-0 loss: 0.965220  [   32/  118]
train() client id: f_00009-5-1 loss: 1.035556  [   64/  118]
train() client id: f_00009-5-2 loss: 0.851276  [   96/  118]
train() client id: f_00009-6-0 loss: 0.993375  [   32/  118]
train() client id: f_00009-6-1 loss: 0.879181  [   64/  118]
train() client id: f_00009-6-2 loss: 1.031549  [   96/  118]
train() client id: f_00009-7-0 loss: 0.932701  [   32/  118]
train() client id: f_00009-7-1 loss: 0.983929  [   64/  118]
train() client id: f_00009-7-2 loss: 0.865136  [   96/  118]
train() client id: f_00009-8-0 loss: 1.041983  [   32/  118]
train() client id: f_00009-8-1 loss: 0.742283  [   64/  118]
train() client id: f_00009-8-2 loss: 0.953811  [   96/  118]
train() client id: f_00009-9-0 loss: 1.014980  [   32/  118]
train() client id: f_00009-9-1 loss: 0.975807  [   64/  118]
train() client id: f_00009-9-2 loss: 0.786163  [   96/  118]
train() client id: f_00009-10-0 loss: 0.924299  [   32/  118]
train() client id: f_00009-10-1 loss: 1.056283  [   64/  118]
train() client id: f_00009-10-2 loss: 0.928169  [   96/  118]
train() client id: f_00009-11-0 loss: 0.940283  [   32/  118]
train() client id: f_00009-11-1 loss: 0.925018  [   64/  118]
train() client id: f_00009-11-2 loss: 0.842741  [   96/  118]
At round 32 accuracy: 0.6472148541114059
At round 32 training accuracy: 0.5841716968477532
At round 32 training loss: 0.8341993108460055
update_location
xs = -3.905658 4.200318 180.009024 18.811294 0.979296 3.956410 -142.443192 -121.324852 164.663977 -107.060879 
ys = 172.587959 155.555839 1.320614 -142.455176 134.350187 117.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 199.504030 184.973678 205.924726 175.063822 167.484124 154.582752 174.060201 157.227211 193.449981 146.553893 
dists_bs = 171.111343 180.252127 395.216316 371.941671 180.613178 187.871280 180.583999 182.260462 374.374325 184.000591 
uav_gains = -107.650768 -106.741308 -108.058130 -106.112618 -105.618216 -104.735783 -106.047964 -104.921617 -107.271417 -104.153277 
bs_gains = -102.098866 -102.731710 -112.278435 -111.540355 -102.756043 -103.235150 -102.754079 -102.866448 -111.619629 -102.981997 
Round 33
-------------------------------
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.63572969 13.74717363  6.53678508  2.35476176 15.85548586  7.6312378
  2.9198615   9.33889895  6.88901514  6.19002745]
obj_prev = 78.09897687730752
eta_min = 1.1396317579661002e-14	eta_max = 0.9276141009437577
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 18.129363196870486	eta = 0.9090909090909091
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 32.88895800241827	eta = 0.5011177085230375
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 25.66977986634642	eta = 0.642048329034932
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 24.365791295624447	eta = 0.6764089485097873
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 24.297762088268055	eta = 0.678302767555703
af = 16.48123926988226	bf = 1.4190753947365375	zeta = 24.29756203224391	eta = 0.678308352418689
eta = 0.678308352418689
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [0.03202948 0.06736359 0.03152108 0.0109307  0.07778588 0.03711352
 0.01372692 0.0455022  0.03304629 0.02999586]
ene_total = [2.16353862 3.9073382  2.14884473 1.01590035 4.454601   2.32913348
 1.16104971 2.80262517 2.36340733 1.95112344]
ti_comp = [0.44826702 0.46925465 0.4459835  0.45600932 0.4691739  0.46754648
 0.45630733 0.4611897  0.42034182 0.46841538]
ti_coms = [0.09205736 0.07106973 0.09434088 0.08431506 0.07115048 0.0727779
 0.08401706 0.07913468 0.11998256 0.07190901]
t_total = [28.34986153 28.34986153 28.34986153 28.34986153 28.34986153 28.34986153
 28.34986153 28.34986153 28.34986153 28.34986153]
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [1.02201224e-05 8.67637833e-05 9.84114204e-06 3.92532807e-07
 1.33633277e-04 1.46159488e-05 7.76398040e-07 2.76833041e-05
 1.27656487e-05 7.68779460e-06]
ene_total = [0.48354488 0.3774426  0.49550619 0.44240675 0.38032544 0.3826196
 0.44086332 0.41665811 0.63019699 0.37769716]
optimize_network iter = 0 obj = 4.427261042788227
eta = 0.678308352418689
freqs = [35725893.75226643 71777219.31597894 35338835.42638909 11985166.67285503
 82896638.99614602 39689662.43089405 15041309.96971984 49331324.0982344
 39308827.86172418 32018442.92741502]
eta_min = 0.6783083524187157	eta_max = 0.6783083524186895
af = 0.013235761297397841	bf = 1.4190753947365375	zeta = 0.014559337427137626	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [2.50952835e-06 2.13046542e-05 2.41647056e-06 9.63855586e-08
 3.28133544e-05 3.58891376e-06 1.90642814e-07 6.79757383e-06
 3.13457667e-06 1.88772088e-06]
ene_total = [1.70964427 1.323468   1.75202375 1.56544914 1.327104   1.35189348
 1.55993376 1.47051205 2.22823206 1.33544537]
ti_comp = [0.44826702 0.46925465 0.4459835  0.45600932 0.4691739  0.46754648
 0.45630733 0.4611897  0.42034182 0.46841538]
ti_coms = [0.09205736 0.07106973 0.09434088 0.08431506 0.07115048 0.0727779
 0.08401706 0.07913468 0.11998256 0.07190901]
t_total = [28.34986153 28.34986153 28.34986153 28.34986153 28.34986153 28.34986153
 28.34986153 28.34986153 28.34986153 28.34986153]
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [1.02201224e-05 8.67637833e-05 9.84114204e-06 3.92532807e-07
 1.33633277e-04 1.46159488e-05 7.76398040e-07 2.76833041e-05
 1.27656487e-05 7.68779460e-06]
ene_total = [0.48354488 0.3774426  0.49550619 0.44240675 0.38032544 0.3826196
 0.44086332 0.41665811 0.63019699 0.37769716]
optimize_network iter = 1 obj = 4.427261042788233
eta = 0.6783083524186895
freqs = [35725893.75226644 71777219.31597894 35338835.4263891  11985166.67285503
 82896638.99614602 39689662.43089406 15041309.96971984 49331324.09823441
 39308827.86172419 32018442.92741502]
Done!
ene_coms = [0.00920574 0.00710697 0.00943409 0.00843151 0.00711505 0.00727779
 0.00840171 0.00791347 0.01199826 0.0071909 ]
ene_comp = [9.64912650e-06 8.19163105e-05 9.29131967e-06 3.70602088e-07
 1.26167217e-04 1.37993591e-05 7.33020858e-07 2.61366442e-05
 1.20524348e-05 7.25827927e-06]
ene_total = [0.00921539 0.00718889 0.00944338 0.00843188 0.00724122 0.00729159
 0.00840244 0.0079396  0.01201031 0.00719816]
At round 33 energy consumption: 0.08436284522778167
At round 33 eta: 0.6783083524186895
At round 33 a_n: 16.878589906266903
At round 33 local rounds: 12.71011101057834
At round 33 global rounds: 52.468225498458686
gradient difference: 0.5141167044639587
train() client id: f_00000-0-0 loss: 1.408054  [   32/  126]
train() client id: f_00000-0-1 loss: 0.916066  [   64/  126]
train() client id: f_00000-0-2 loss: 1.180199  [   96/  126]
train() client id: f_00000-1-0 loss: 1.143211  [   32/  126]
train() client id: f_00000-1-1 loss: 1.161527  [   64/  126]
train() client id: f_00000-1-2 loss: 1.069048  [   96/  126]
train() client id: f_00000-2-0 loss: 1.022466  [   32/  126]
train() client id: f_00000-2-1 loss: 0.900778  [   64/  126]
train() client id: f_00000-2-2 loss: 1.147208  [   96/  126]
train() client id: f_00000-3-0 loss: 0.914882  [   32/  126]
train() client id: f_00000-3-1 loss: 0.970456  [   64/  126]
train() client id: f_00000-3-2 loss: 1.009783  [   96/  126]
train() client id: f_00000-4-0 loss: 0.963157  [   32/  126]
train() client id: f_00000-4-1 loss: 0.905143  [   64/  126]
train() client id: f_00000-4-2 loss: 1.040181  [   96/  126]
train() client id: f_00000-5-0 loss: 0.907051  [   32/  126]
train() client id: f_00000-5-1 loss: 0.969739  [   64/  126]
train() client id: f_00000-5-2 loss: 0.937021  [   96/  126]
train() client id: f_00000-6-0 loss: 1.011346  [   32/  126]
train() client id: f_00000-6-1 loss: 0.923182  [   64/  126]
train() client id: f_00000-6-2 loss: 0.940391  [   96/  126]
train() client id: f_00000-7-0 loss: 0.856558  [   32/  126]
train() client id: f_00000-7-1 loss: 1.051575  [   64/  126]
train() client id: f_00000-7-2 loss: 0.847837  [   96/  126]
train() client id: f_00000-8-0 loss: 0.799490  [   32/  126]
train() client id: f_00000-8-1 loss: 0.926062  [   64/  126]
train() client id: f_00000-8-2 loss: 1.076951  [   96/  126]
train() client id: f_00000-9-0 loss: 0.884050  [   32/  126]
train() client id: f_00000-9-1 loss: 1.039986  [   64/  126]
train() client id: f_00000-9-2 loss: 0.834902  [   96/  126]
train() client id: f_00000-10-0 loss: 0.938554  [   32/  126]
train() client id: f_00000-10-1 loss: 0.859193  [   64/  126]
train() client id: f_00000-10-2 loss: 0.870319  [   96/  126]
train() client id: f_00000-11-0 loss: 0.884993  [   32/  126]
train() client id: f_00000-11-1 loss: 0.980637  [   64/  126]
train() client id: f_00000-11-2 loss: 0.899225  [   96/  126]
train() client id: f_00001-0-0 loss: 0.431220  [   32/  265]
train() client id: f_00001-0-1 loss: 0.423457  [   64/  265]
train() client id: f_00001-0-2 loss: 0.467256  [   96/  265]
train() client id: f_00001-0-3 loss: 0.516636  [  128/  265]
train() client id: f_00001-0-4 loss: 0.458517  [  160/  265]
train() client id: f_00001-0-5 loss: 0.423814  [  192/  265]
train() client id: f_00001-0-6 loss: 0.442490  [  224/  265]
train() client id: f_00001-0-7 loss: 0.414186  [  256/  265]
train() client id: f_00001-1-0 loss: 0.427613  [   32/  265]
train() client id: f_00001-1-1 loss: 0.532108  [   64/  265]
train() client id: f_00001-1-2 loss: 0.411888  [   96/  265]
train() client id: f_00001-1-3 loss: 0.412902  [  128/  265]
train() client id: f_00001-1-4 loss: 0.343151  [  160/  265]
train() client id: f_00001-1-5 loss: 0.519045  [  192/  265]
train() client id: f_00001-1-6 loss: 0.341881  [  224/  265]
train() client id: f_00001-1-7 loss: 0.512712  [  256/  265]
train() client id: f_00001-2-0 loss: 0.500614  [   32/  265]
train() client id: f_00001-2-1 loss: 0.440532  [   64/  265]
train() client id: f_00001-2-2 loss: 0.322638  [   96/  265]
train() client id: f_00001-2-3 loss: 0.424438  [  128/  265]
train() client id: f_00001-2-4 loss: 0.519357  [  160/  265]
train() client id: f_00001-2-5 loss: 0.348470  [  192/  265]
train() client id: f_00001-2-6 loss: 0.554868  [  224/  265]
train() client id: f_00001-2-7 loss: 0.345403  [  256/  265]
train() client id: f_00001-3-0 loss: 0.607651  [   32/  265]
train() client id: f_00001-3-1 loss: 0.342691  [   64/  265]
train() client id: f_00001-3-2 loss: 0.420004  [   96/  265]
train() client id: f_00001-3-3 loss: 0.422095  [  128/  265]
train() client id: f_00001-3-4 loss: 0.404602  [  160/  265]
train() client id: f_00001-3-5 loss: 0.386159  [  192/  265]
train() client id: f_00001-3-6 loss: 0.461728  [  224/  265]
train() client id: f_00001-3-7 loss: 0.364298  [  256/  265]
train() client id: f_00001-4-0 loss: 0.448858  [   32/  265]
train() client id: f_00001-4-1 loss: 0.444591  [   64/  265]
train() client id: f_00001-4-2 loss: 0.419047  [   96/  265]
train() client id: f_00001-4-3 loss: 0.412645  [  128/  265]
train() client id: f_00001-4-4 loss: 0.337780  [  160/  265]
train() client id: f_00001-4-5 loss: 0.522618  [  192/  265]
train() client id: f_00001-4-6 loss: 0.474980  [  224/  265]
train() client id: f_00001-4-7 loss: 0.315777  [  256/  265]
train() client id: f_00001-5-0 loss: 0.443996  [   32/  265]
train() client id: f_00001-5-1 loss: 0.320356  [   64/  265]
train() client id: f_00001-5-2 loss: 0.385155  [   96/  265]
train() client id: f_00001-5-3 loss: 0.369068  [  128/  265]
train() client id: f_00001-5-4 loss: 0.461598  [  160/  265]
train() client id: f_00001-5-5 loss: 0.411242  [  192/  265]
train() client id: f_00001-5-6 loss: 0.376031  [  224/  265]
train() client id: f_00001-5-7 loss: 0.501424  [  256/  265]
train() client id: f_00001-6-0 loss: 0.417978  [   32/  265]
train() client id: f_00001-6-1 loss: 0.513565  [   64/  265]
train() client id: f_00001-6-2 loss: 0.309854  [   96/  265]
train() client id: f_00001-6-3 loss: 0.467320  [  128/  265]
train() client id: f_00001-6-4 loss: 0.367962  [  160/  265]
train() client id: f_00001-6-5 loss: 0.430705  [  192/  265]
train() client id: f_00001-6-6 loss: 0.320438  [  224/  265]
train() client id: f_00001-6-7 loss: 0.465204  [  256/  265]
train() client id: f_00001-7-0 loss: 0.567981  [   32/  265]
train() client id: f_00001-7-1 loss: 0.402460  [   64/  265]
train() client id: f_00001-7-2 loss: 0.350588  [   96/  265]
train() client id: f_00001-7-3 loss: 0.397195  [  128/  265]
train() client id: f_00001-7-4 loss: 0.301045  [  160/  265]
train() client id: f_00001-7-5 loss: 0.456210  [  192/  265]
train() client id: f_00001-7-6 loss: 0.439052  [  224/  265]
train() client id: f_00001-7-7 loss: 0.389256  [  256/  265]
train() client id: f_00001-8-0 loss: 0.478975  [   32/  265]
train() client id: f_00001-8-1 loss: 0.416320  [   64/  265]
train() client id: f_00001-8-2 loss: 0.452912  [   96/  265]
train() client id: f_00001-8-3 loss: 0.461250  [  128/  265]
train() client id: f_00001-8-4 loss: 0.358164  [  160/  265]
train() client id: f_00001-8-5 loss: 0.414772  [  192/  265]
train() client id: f_00001-8-6 loss: 0.311981  [  224/  265]
train() client id: f_00001-8-7 loss: 0.346958  [  256/  265]
train() client id: f_00001-9-0 loss: 0.397078  [   32/  265]
train() client id: f_00001-9-1 loss: 0.563635  [   64/  265]
train() client id: f_00001-9-2 loss: 0.361999  [   96/  265]
train() client id: f_00001-9-3 loss: 0.354820  [  128/  265]
train() client id: f_00001-9-4 loss: 0.434785  [  160/  265]
train() client id: f_00001-9-5 loss: 0.467135  [  192/  265]
train() client id: f_00001-9-6 loss: 0.401922  [  224/  265]
train() client id: f_00001-9-7 loss: 0.322595  [  256/  265]
train() client id: f_00001-10-0 loss: 0.491317  [   32/  265]
train() client id: f_00001-10-1 loss: 0.356096  [   64/  265]
train() client id: f_00001-10-2 loss: 0.420123  [   96/  265]
train() client id: f_00001-10-3 loss: 0.459211  [  128/  265]
train() client id: f_00001-10-4 loss: 0.405082  [  160/  265]
train() client id: f_00001-10-5 loss: 0.450283  [  192/  265]
train() client id: f_00001-10-6 loss: 0.389417  [  224/  265]
train() client id: f_00001-10-7 loss: 0.327873  [  256/  265]
train() client id: f_00001-11-0 loss: 0.319166  [   32/  265]
train() client id: f_00001-11-1 loss: 0.398849  [   64/  265]
train() client id: f_00001-11-2 loss: 0.417853  [   96/  265]
train() client id: f_00001-11-3 loss: 0.473981  [  128/  265]
train() client id: f_00001-11-4 loss: 0.318377  [  160/  265]
train() client id: f_00001-11-5 loss: 0.552385  [  192/  265]
train() client id: f_00001-11-6 loss: 0.320252  [  224/  265]
train() client id: f_00001-11-7 loss: 0.418234  [  256/  265]
train() client id: f_00002-0-0 loss: 1.193060  [   32/  124]
train() client id: f_00002-0-1 loss: 1.093053  [   64/  124]
train() client id: f_00002-0-2 loss: 1.010454  [   96/  124]
train() client id: f_00002-1-0 loss: 1.198352  [   32/  124]
train() client id: f_00002-1-1 loss: 0.994417  [   64/  124]
train() client id: f_00002-1-2 loss: 1.012587  [   96/  124]
train() client id: f_00002-2-0 loss: 1.068765  [   32/  124]
train() client id: f_00002-2-1 loss: 1.085915  [   64/  124]
train() client id: f_00002-2-2 loss: 0.962172  [   96/  124]
train() client id: f_00002-3-0 loss: 1.163672  [   32/  124]
train() client id: f_00002-3-1 loss: 0.800654  [   64/  124]
train() client id: f_00002-3-2 loss: 1.052461  [   96/  124]
train() client id: f_00002-4-0 loss: 0.969540  [   32/  124]
train() client id: f_00002-4-1 loss: 0.984339  [   64/  124]
train() client id: f_00002-4-2 loss: 0.937928  [   96/  124]
train() client id: f_00002-5-0 loss: 1.005974  [   32/  124]
train() client id: f_00002-5-1 loss: 0.888130  [   64/  124]
train() client id: f_00002-5-2 loss: 1.030123  [   96/  124]
train() client id: f_00002-6-0 loss: 1.113450  [   32/  124]
train() client id: f_00002-6-1 loss: 0.908049  [   64/  124]
train() client id: f_00002-6-2 loss: 1.000118  [   96/  124]
train() client id: f_00002-7-0 loss: 0.949105  [   32/  124]
train() client id: f_00002-7-1 loss: 1.074655  [   64/  124]
train() client id: f_00002-7-2 loss: 0.985523  [   96/  124]
train() client id: f_00002-8-0 loss: 0.867239  [   32/  124]
train() client id: f_00002-8-1 loss: 1.097122  [   64/  124]
train() client id: f_00002-8-2 loss: 0.929948  [   96/  124]
train() client id: f_00002-9-0 loss: 0.955825  [   32/  124]
train() client id: f_00002-9-1 loss: 1.029142  [   64/  124]
train() client id: f_00002-9-2 loss: 1.050107  [   96/  124]
train() client id: f_00002-10-0 loss: 0.949473  [   32/  124]
train() client id: f_00002-10-1 loss: 1.005255  [   64/  124]
train() client id: f_00002-10-2 loss: 0.864490  [   96/  124]
train() client id: f_00002-11-0 loss: 0.890689  [   32/  124]
train() client id: f_00002-11-1 loss: 1.209020  [   64/  124]
train() client id: f_00002-11-2 loss: 0.857122  [   96/  124]
train() client id: f_00003-0-0 loss: 0.745059  [   32/   43]
train() client id: f_00003-1-0 loss: 0.840469  [   32/   43]
train() client id: f_00003-2-0 loss: 0.977922  [   32/   43]
train() client id: f_00003-3-0 loss: 0.789284  [   32/   43]
train() client id: f_00003-4-0 loss: 1.025311  [   32/   43]
train() client id: f_00003-5-0 loss: 0.806418  [   32/   43]
train() client id: f_00003-6-0 loss: 0.783117  [   32/   43]
train() client id: f_00003-7-0 loss: 0.831902  [   32/   43]
train() client id: f_00003-8-0 loss: 1.070677  [   32/   43]
train() client id: f_00003-9-0 loss: 0.804021  [   32/   43]
train() client id: f_00003-10-0 loss: 0.859741  [   32/   43]
train() client id: f_00003-11-0 loss: 0.766788  [   32/   43]
train() client id: f_00004-0-0 loss: 0.825494  [   32/  306]
train() client id: f_00004-0-1 loss: 0.993114  [   64/  306]
train() client id: f_00004-0-2 loss: 0.886976  [   96/  306]
train() client id: f_00004-0-3 loss: 0.910389  [  128/  306]
train() client id: f_00004-0-4 loss: 0.848735  [  160/  306]
train() client id: f_00004-0-5 loss: 0.755147  [  192/  306]
train() client id: f_00004-0-6 loss: 0.855832  [  224/  306]
train() client id: f_00004-0-7 loss: 0.790358  [  256/  306]
train() client id: f_00004-0-8 loss: 0.723931  [  288/  306]
train() client id: f_00004-1-0 loss: 0.845387  [   32/  306]
train() client id: f_00004-1-1 loss: 0.783653  [   64/  306]
train() client id: f_00004-1-2 loss: 0.813380  [   96/  306]
train() client id: f_00004-1-3 loss: 0.990214  [  128/  306]
train() client id: f_00004-1-4 loss: 0.882653  [  160/  306]
train() client id: f_00004-1-5 loss: 0.882024  [  192/  306]
train() client id: f_00004-1-6 loss: 0.806147  [  224/  306]
train() client id: f_00004-1-7 loss: 0.875036  [  256/  306]
train() client id: f_00004-1-8 loss: 0.848584  [  288/  306]
train() client id: f_00004-2-0 loss: 0.909802  [   32/  306]
train() client id: f_00004-2-1 loss: 0.855660  [   64/  306]
train() client id: f_00004-2-2 loss: 0.846781  [   96/  306]
train() client id: f_00004-2-3 loss: 0.880911  [  128/  306]
train() client id: f_00004-2-4 loss: 0.907214  [  160/  306]
train() client id: f_00004-2-5 loss: 0.733972  [  192/  306]
train() client id: f_00004-2-6 loss: 0.743606  [  224/  306]
train() client id: f_00004-2-7 loss: 0.802979  [  256/  306]
train() client id: f_00004-2-8 loss: 0.894386  [  288/  306]
train() client id: f_00004-3-0 loss: 0.895338  [   32/  306]
train() client id: f_00004-3-1 loss: 0.805916  [   64/  306]
train() client id: f_00004-3-2 loss: 0.870936  [   96/  306]
train() client id: f_00004-3-3 loss: 0.778816  [  128/  306]
train() client id: f_00004-3-4 loss: 0.936220  [  160/  306]
train() client id: f_00004-3-5 loss: 0.955031  [  192/  306]
train() client id: f_00004-3-6 loss: 0.719329  [  224/  306]
train() client id: f_00004-3-7 loss: 0.851765  [  256/  306]
train() client id: f_00004-3-8 loss: 0.873263  [  288/  306]
train() client id: f_00004-4-0 loss: 0.814766  [   32/  306]
train() client id: f_00004-4-1 loss: 0.907309  [   64/  306]
train() client id: f_00004-4-2 loss: 0.851125  [   96/  306]
train() client id: f_00004-4-3 loss: 0.836681  [  128/  306]
train() client id: f_00004-4-4 loss: 0.991737  [  160/  306]
train() client id: f_00004-4-5 loss: 0.829238  [  192/  306]
train() client id: f_00004-4-6 loss: 0.760812  [  224/  306]
train() client id: f_00004-4-7 loss: 0.764478  [  256/  306]
train() client id: f_00004-4-8 loss: 0.827850  [  288/  306]
train() client id: f_00004-5-0 loss: 0.854640  [   32/  306]
train() client id: f_00004-5-1 loss: 0.795417  [   64/  306]
train() client id: f_00004-5-2 loss: 0.779793  [   96/  306]
train() client id: f_00004-5-3 loss: 0.879550  [  128/  306]
train() client id: f_00004-5-4 loss: 0.865394  [  160/  306]
train() client id: f_00004-5-5 loss: 0.887398  [  192/  306]
train() client id: f_00004-5-6 loss: 0.888592  [  224/  306]
train() client id: f_00004-5-7 loss: 0.768041  [  256/  306]
train() client id: f_00004-5-8 loss: 0.953346  [  288/  306]
train() client id: f_00004-6-0 loss: 0.833945  [   32/  306]
train() client id: f_00004-6-1 loss: 0.703166  [   64/  306]
train() client id: f_00004-6-2 loss: 0.915251  [   96/  306]
train() client id: f_00004-6-3 loss: 0.976931  [  128/  306]
train() client id: f_00004-6-4 loss: 0.697992  [  160/  306]
train() client id: f_00004-6-5 loss: 0.839721  [  192/  306]
train() client id: f_00004-6-6 loss: 0.897661  [  224/  306]
train() client id: f_00004-6-7 loss: 0.809619  [  256/  306]
train() client id: f_00004-6-8 loss: 0.795298  [  288/  306]
train() client id: f_00004-7-0 loss: 0.755962  [   32/  306]
train() client id: f_00004-7-1 loss: 1.047507  [   64/  306]
train() client id: f_00004-7-2 loss: 0.837436  [   96/  306]
train() client id: f_00004-7-3 loss: 0.916074  [  128/  306]
train() client id: f_00004-7-4 loss: 0.724766  [  160/  306]
train() client id: f_00004-7-5 loss: 0.881525  [  192/  306]
train() client id: f_00004-7-6 loss: 0.905782  [  224/  306]
train() client id: f_00004-7-7 loss: 0.786359  [  256/  306]
train() client id: f_00004-7-8 loss: 0.840919  [  288/  306]
train() client id: f_00004-8-0 loss: 0.882644  [   32/  306]
train() client id: f_00004-8-1 loss: 0.846445  [   64/  306]
train() client id: f_00004-8-2 loss: 0.876347  [   96/  306]
train() client id: f_00004-8-3 loss: 0.801585  [  128/  306]
train() client id: f_00004-8-4 loss: 0.796453  [  160/  306]
train() client id: f_00004-8-5 loss: 0.755896  [  192/  306]
train() client id: f_00004-8-6 loss: 0.899550  [  224/  306]
train() client id: f_00004-8-7 loss: 0.830472  [  256/  306]
train() client id: f_00004-8-8 loss: 0.951105  [  288/  306]
train() client id: f_00004-9-0 loss: 0.917543  [   32/  306]
train() client id: f_00004-9-1 loss: 0.903895  [   64/  306]
train() client id: f_00004-9-2 loss: 0.840109  [   96/  306]
train() client id: f_00004-9-3 loss: 0.962321  [  128/  306]
train() client id: f_00004-9-4 loss: 0.865984  [  160/  306]
train() client id: f_00004-9-5 loss: 0.832193  [  192/  306]
train() client id: f_00004-9-6 loss: 0.865830  [  224/  306]
train() client id: f_00004-9-7 loss: 0.865423  [  256/  306]
train() client id: f_00004-9-8 loss: 0.657781  [  288/  306]
train() client id: f_00004-10-0 loss: 0.842840  [   32/  306]
train() client id: f_00004-10-1 loss: 0.859097  [   64/  306]
train() client id: f_00004-10-2 loss: 0.848124  [   96/  306]
train() client id: f_00004-10-3 loss: 0.729388  [  128/  306]
train() client id: f_00004-10-4 loss: 0.763041  [  160/  306]
train() client id: f_00004-10-5 loss: 0.810728  [  192/  306]
train() client id: f_00004-10-6 loss: 1.035488  [  224/  306]
train() client id: f_00004-10-7 loss: 0.911959  [  256/  306]
train() client id: f_00004-10-8 loss: 0.822089  [  288/  306]
train() client id: f_00004-11-0 loss: 0.754032  [   32/  306]
train() client id: f_00004-11-1 loss: 0.949500  [   64/  306]
train() client id: f_00004-11-2 loss: 0.779178  [   96/  306]
train() client id: f_00004-11-3 loss: 0.881762  [  128/  306]
train() client id: f_00004-11-4 loss: 0.845130  [  160/  306]
train() client id: f_00004-11-5 loss: 0.926194  [  192/  306]
train() client id: f_00004-11-6 loss: 0.737975  [  224/  306]
train() client id: f_00004-11-7 loss: 0.922695  [  256/  306]
train() client id: f_00004-11-8 loss: 0.852945  [  288/  306]
train() client id: f_00005-0-0 loss: 0.377692  [   32/  146]
train() client id: f_00005-0-1 loss: 0.235302  [   64/  146]
train() client id: f_00005-0-2 loss: 0.748729  [   96/  146]
train() client id: f_00005-0-3 loss: 0.649932  [  128/  146]
train() client id: f_00005-1-0 loss: 0.549661  [   32/  146]
train() client id: f_00005-1-1 loss: 0.430470  [   64/  146]
train() client id: f_00005-1-2 loss: 0.463208  [   96/  146]
train() client id: f_00005-1-3 loss: 0.584659  [  128/  146]
train() client id: f_00005-2-0 loss: 0.327193  [   32/  146]
train() client id: f_00005-2-1 loss: 0.355863  [   64/  146]
train() client id: f_00005-2-2 loss: 0.531563  [   96/  146]
train() client id: f_00005-2-3 loss: 0.537498  [  128/  146]
train() client id: f_00005-3-0 loss: 0.206484  [   32/  146]
train() client id: f_00005-3-1 loss: 0.471776  [   64/  146]
train() client id: f_00005-3-2 loss: 0.480914  [   96/  146]
train() client id: f_00005-3-3 loss: 0.664574  [  128/  146]
train() client id: f_00005-4-0 loss: 0.589189  [   32/  146]
train() client id: f_00005-4-1 loss: 0.426015  [   64/  146]
train() client id: f_00005-4-2 loss: 0.524022  [   96/  146]
train() client id: f_00005-4-3 loss: 0.358373  [  128/  146]
train() client id: f_00005-5-0 loss: 0.379663  [   32/  146]
train() client id: f_00005-5-1 loss: 0.532363  [   64/  146]
train() client id: f_00005-5-2 loss: 0.631338  [   96/  146]
train() client id: f_00005-5-3 loss: 0.351847  [  128/  146]
train() client id: f_00005-6-0 loss: 0.486547  [   32/  146]
train() client id: f_00005-6-1 loss: 0.498563  [   64/  146]
train() client id: f_00005-6-2 loss: 0.418858  [   96/  146]
train() client id: f_00005-6-3 loss: 0.373404  [  128/  146]
train() client id: f_00005-7-0 loss: 0.498209  [   32/  146]
train() client id: f_00005-7-1 loss: 0.330556  [   64/  146]
train() client id: f_00005-7-2 loss: 0.615713  [   96/  146]
train() client id: f_00005-7-3 loss: 0.368348  [  128/  146]
train() client id: f_00005-8-0 loss: 0.402605  [   32/  146]
train() client id: f_00005-8-1 loss: 0.661071  [   64/  146]
train() client id: f_00005-8-2 loss: 0.441362  [   96/  146]
train() client id: f_00005-8-3 loss: 0.420418  [  128/  146]
train() client id: f_00005-9-0 loss: 0.426365  [   32/  146]
train() client id: f_00005-9-1 loss: 0.506581  [   64/  146]
train() client id: f_00005-9-2 loss: 0.529651  [   96/  146]
train() client id: f_00005-9-3 loss: 0.354703  [  128/  146]
train() client id: f_00005-10-0 loss: 0.453316  [   32/  146]
train() client id: f_00005-10-1 loss: 0.249274  [   64/  146]
train() client id: f_00005-10-2 loss: 0.676275  [   96/  146]
train() client id: f_00005-10-3 loss: 0.372043  [  128/  146]
train() client id: f_00005-11-0 loss: 0.371143  [   32/  146]
train() client id: f_00005-11-1 loss: 0.207415  [   64/  146]
train() client id: f_00005-11-2 loss: 0.748601  [   96/  146]
train() client id: f_00005-11-3 loss: 0.537652  [  128/  146]
train() client id: f_00006-0-0 loss: 0.574785  [   32/   54]
train() client id: f_00006-1-0 loss: 0.490283  [   32/   54]
train() client id: f_00006-2-0 loss: 0.527380  [   32/   54]
train() client id: f_00006-3-0 loss: 0.558596  [   32/   54]
train() client id: f_00006-4-0 loss: 0.588231  [   32/   54]
train() client id: f_00006-5-0 loss: 0.537435  [   32/   54]
train() client id: f_00006-6-0 loss: 0.530975  [   32/   54]
train() client id: f_00006-7-0 loss: 0.526658  [   32/   54]
train() client id: f_00006-8-0 loss: 0.590274  [   32/   54]
train() client id: f_00006-9-0 loss: 0.519170  [   32/   54]
train() client id: f_00006-10-0 loss: 0.528521  [   32/   54]
train() client id: f_00006-11-0 loss: 0.578809  [   32/   54]
train() client id: f_00007-0-0 loss: 0.522033  [   32/  179]
train() client id: f_00007-0-1 loss: 0.669217  [   64/  179]
train() client id: f_00007-0-2 loss: 0.616225  [   96/  179]
train() client id: f_00007-0-3 loss: 0.660146  [  128/  179]
train() client id: f_00007-0-4 loss: 0.598691  [  160/  179]
train() client id: f_00007-1-0 loss: 0.484364  [   32/  179]
train() client id: f_00007-1-1 loss: 0.639277  [   64/  179]
train() client id: f_00007-1-2 loss: 0.888953  [   96/  179]
train() client id: f_00007-1-3 loss: 0.554799  [  128/  179]
train() client id: f_00007-1-4 loss: 0.506712  [  160/  179]
train() client id: f_00007-2-0 loss: 0.486681  [   32/  179]
train() client id: f_00007-2-1 loss: 0.597382  [   64/  179]
train() client id: f_00007-2-2 loss: 0.456643  [   96/  179]
train() client id: f_00007-2-3 loss: 0.626317  [  128/  179]
train() client id: f_00007-2-4 loss: 0.786779  [  160/  179]
train() client id: f_00007-3-0 loss: 0.599404  [   32/  179]
train() client id: f_00007-3-1 loss: 0.690510  [   64/  179]
train() client id: f_00007-3-2 loss: 0.587539  [   96/  179]
train() client id: f_00007-3-3 loss: 0.643828  [  128/  179]
train() client id: f_00007-3-4 loss: 0.429353  [  160/  179]
train() client id: f_00007-4-0 loss: 0.730877  [   32/  179]
train() client id: f_00007-4-1 loss: 0.447754  [   64/  179]
train() client id: f_00007-4-2 loss: 0.615856  [   96/  179]
train() client id: f_00007-4-3 loss: 0.590813  [  128/  179]
train() client id: f_00007-4-4 loss: 0.491478  [  160/  179]
train() client id: f_00007-5-0 loss: 0.499939  [   32/  179]
train() client id: f_00007-5-1 loss: 0.573538  [   64/  179]
train() client id: f_00007-5-2 loss: 0.642780  [   96/  179]
train() client id: f_00007-5-3 loss: 0.518168  [  128/  179]
train() client id: f_00007-5-4 loss: 0.672805  [  160/  179]
train() client id: f_00007-6-0 loss: 0.391986  [   32/  179]
train() client id: f_00007-6-1 loss: 0.532453  [   64/  179]
train() client id: f_00007-6-2 loss: 0.734888  [   96/  179]
train() client id: f_00007-6-3 loss: 0.633469  [  128/  179]
train() client id: f_00007-6-4 loss: 0.604845  [  160/  179]
train() client id: f_00007-7-0 loss: 0.518957  [   32/  179]
train() client id: f_00007-7-1 loss: 0.529885  [   64/  179]
train() client id: f_00007-7-2 loss: 0.411210  [   96/  179]
train() client id: f_00007-7-3 loss: 0.757928  [  128/  179]
train() client id: f_00007-7-4 loss: 0.483268  [  160/  179]
train() client id: f_00007-8-0 loss: 0.785593  [   32/  179]
train() client id: f_00007-8-1 loss: 0.406965  [   64/  179]
train() client id: f_00007-8-2 loss: 0.589144  [   96/  179]
train() client id: f_00007-8-3 loss: 0.496693  [  128/  179]
train() client id: f_00007-8-4 loss: 0.527994  [  160/  179]
train() client id: f_00007-9-0 loss: 0.577626  [   32/  179]
train() client id: f_00007-9-1 loss: 0.580250  [   64/  179]
train() client id: f_00007-9-2 loss: 0.662480  [   96/  179]
train() client id: f_00007-9-3 loss: 0.507794  [  128/  179]
train() client id: f_00007-9-4 loss: 0.526042  [  160/  179]
train() client id: f_00007-10-0 loss: 0.488416  [   32/  179]
train() client id: f_00007-10-1 loss: 0.427236  [   64/  179]
train() client id: f_00007-10-2 loss: 0.718350  [   96/  179]
train() client id: f_00007-10-3 loss: 0.610909  [  128/  179]
train() client id: f_00007-10-4 loss: 0.600913  [  160/  179]
train() client id: f_00007-11-0 loss: 0.582829  [   32/  179]
train() client id: f_00007-11-1 loss: 0.511923  [   64/  179]
train() client id: f_00007-11-2 loss: 0.490614  [   96/  179]
train() client id: f_00007-11-3 loss: 0.654673  [  128/  179]
train() client id: f_00007-11-4 loss: 0.506847  [  160/  179]
train() client id: f_00008-0-0 loss: 0.747263  [   32/  130]
train() client id: f_00008-0-1 loss: 0.774581  [   64/  130]
train() client id: f_00008-0-2 loss: 0.870469  [   96/  130]
train() client id: f_00008-0-3 loss: 0.746422  [  128/  130]
train() client id: f_00008-1-0 loss: 0.701068  [   32/  130]
train() client id: f_00008-1-1 loss: 0.772279  [   64/  130]
train() client id: f_00008-1-2 loss: 0.778656  [   96/  130]
train() client id: f_00008-1-3 loss: 0.887807  [  128/  130]
train() client id: f_00008-2-0 loss: 0.804077  [   32/  130]
train() client id: f_00008-2-1 loss: 0.704256  [   64/  130]
train() client id: f_00008-2-2 loss: 0.841482  [   96/  130]
train() client id: f_00008-2-3 loss: 0.827425  [  128/  130]
train() client id: f_00008-3-0 loss: 0.803209  [   32/  130]
train() client id: f_00008-3-1 loss: 0.944692  [   64/  130]
train() client id: f_00008-3-2 loss: 0.726426  [   96/  130]
train() client id: f_00008-3-3 loss: 0.702869  [  128/  130]
train() client id: f_00008-4-0 loss: 0.781072  [   32/  130]
train() client id: f_00008-4-1 loss: 0.789259  [   64/  130]
train() client id: f_00008-4-2 loss: 0.856509  [   96/  130]
train() client id: f_00008-4-3 loss: 0.715527  [  128/  130]
train() client id: f_00008-5-0 loss: 0.972621  [   32/  130]
train() client id: f_00008-5-1 loss: 0.786125  [   64/  130]
train() client id: f_00008-5-2 loss: 0.642799  [   96/  130]
train() client id: f_00008-5-3 loss: 0.754561  [  128/  130]
train() client id: f_00008-6-0 loss: 0.892430  [   32/  130]
train() client id: f_00008-6-1 loss: 0.751789  [   64/  130]
train() client id: f_00008-6-2 loss: 0.841877  [   96/  130]
train() client id: f_00008-6-3 loss: 0.685304  [  128/  130]
train() client id: f_00008-7-0 loss: 0.805996  [   32/  130]
train() client id: f_00008-7-1 loss: 0.770228  [   64/  130]
train() client id: f_00008-7-2 loss: 0.858852  [   96/  130]
train() client id: f_00008-7-3 loss: 0.740751  [  128/  130]
train() client id: f_00008-8-0 loss: 0.935480  [   32/  130]
train() client id: f_00008-8-1 loss: 0.758222  [   64/  130]
train() client id: f_00008-8-2 loss: 0.741277  [   96/  130]
train() client id: f_00008-8-3 loss: 0.703949  [  128/  130]
train() client id: f_00008-9-0 loss: 0.834870  [   32/  130]
train() client id: f_00008-9-1 loss: 0.804299  [   64/  130]
train() client id: f_00008-9-2 loss: 0.691911  [   96/  130]
train() client id: f_00008-9-3 loss: 0.793167  [  128/  130]
train() client id: f_00008-10-0 loss: 0.799663  [   32/  130]
train() client id: f_00008-10-1 loss: 0.756530  [   64/  130]
train() client id: f_00008-10-2 loss: 0.767183  [   96/  130]
train() client id: f_00008-10-3 loss: 0.840350  [  128/  130]
train() client id: f_00008-11-0 loss: 0.687373  [   32/  130]
train() client id: f_00008-11-1 loss: 0.821418  [   64/  130]
train() client id: f_00008-11-2 loss: 0.715354  [   96/  130]
train() client id: f_00008-11-3 loss: 0.918388  [  128/  130]
train() client id: f_00009-0-0 loss: 1.044926  [   32/  118]
train() client id: f_00009-0-1 loss: 1.115905  [   64/  118]
train() client id: f_00009-0-2 loss: 1.085528  [   96/  118]
train() client id: f_00009-1-0 loss: 1.141750  [   32/  118]
train() client id: f_00009-1-1 loss: 1.062346  [   64/  118]
train() client id: f_00009-1-2 loss: 1.027154  [   96/  118]
train() client id: f_00009-2-0 loss: 0.956527  [   32/  118]
train() client id: f_00009-2-1 loss: 1.018950  [   64/  118]
train() client id: f_00009-2-2 loss: 1.048164  [   96/  118]
train() client id: f_00009-3-0 loss: 0.920353  [   32/  118]
train() client id: f_00009-3-1 loss: 1.001681  [   64/  118]
train() client id: f_00009-3-2 loss: 0.975870  [   96/  118]
train() client id: f_00009-4-0 loss: 0.867202  [   32/  118]
train() client id: f_00009-4-1 loss: 0.900948  [   64/  118]
train() client id: f_00009-4-2 loss: 1.096877  [   96/  118]
train() client id: f_00009-5-0 loss: 0.888946  [   32/  118]
train() client id: f_00009-5-1 loss: 0.914814  [   64/  118]
train() client id: f_00009-5-2 loss: 0.914629  [   96/  118]
train() client id: f_00009-6-0 loss: 0.884341  [   32/  118]
train() client id: f_00009-6-1 loss: 0.974913  [   64/  118]
train() client id: f_00009-6-2 loss: 0.816957  [   96/  118]
train() client id: f_00009-7-0 loss: 0.921460  [   32/  118]
train() client id: f_00009-7-1 loss: 0.950823  [   64/  118]
train() client id: f_00009-7-2 loss: 0.897193  [   96/  118]
train() client id: f_00009-8-0 loss: 0.945619  [   32/  118]
train() client id: f_00009-8-1 loss: 0.916663  [   64/  118]
train() client id: f_00009-8-2 loss: 0.874583  [   96/  118]
train() client id: f_00009-9-0 loss: 0.843037  [   32/  118]
train() client id: f_00009-9-1 loss: 0.960022  [   64/  118]
train() client id: f_00009-9-2 loss: 0.934550  [   96/  118]
train() client id: f_00009-10-0 loss: 0.949385  [   32/  118]
train() client id: f_00009-10-1 loss: 0.907364  [   64/  118]
train() client id: f_00009-10-2 loss: 0.780768  [   96/  118]
train() client id: f_00009-11-0 loss: 0.859004  [   32/  118]
train() client id: f_00009-11-1 loss: 0.771707  [   64/  118]
train() client id: f_00009-11-2 loss: 0.980884  [   96/  118]
At round 33 accuracy: 0.6472148541114059
At round 33 training accuracy: 0.5875251509054326
At round 33 training loss: 0.8319923850017493
update_location
xs = -3.905658 4.200318 185.009024 18.811294 0.979296 3.956410 -147.443192 -126.324852 169.663977 -112.060879 
ys = 177.587959 160.555839 1.320614 -147.455176 139.350187 122.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 203.844886 189.197833 210.309493 179.156060 171.520942 158.426541 178.175154 161.116866 197.723380 150.245307 
dists_bs = 171.113913 179.781500 399.713681 376.218232 179.553953 186.410191 179.749862 180.851111 378.916580 182.213683 
uav_gains = -107.925351 -107.005796 -108.341492 -106.374132 -105.883343 -105.004993 -106.311732 -105.190067 -107.538881 -104.424673 
bs_gains = -102.099048 -102.699919 -112.416032 -111.679375 -102.684518 -103.140209 -102.697779 -102.772052 -111.766280 -102.863327 
Round 34
-------------------------------
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.50361085 13.46800316  6.40688418  2.3090212  15.53330799  7.47581585
  2.86265615  9.15122779  6.75143071  6.06373403]
obj_prev = 76.52569191747648
eta_min = 6.016067092016877e-15	eta_max = 0.9281867977986356
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 17.761433286506318	eta = 0.9090909090909091
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 32.349008668159414	eta = 0.4991422673511644
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 25.200112188715238	eta = 0.6407414940167677
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 23.908304317469916	eta = 0.6753618876010812
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 23.840631284811383	eta = 0.6772789419999331
af = 16.14675753318756	bf = 1.4018686060877306	zeta = 23.84043047624284	eta = 0.6772846467381501
eta = 0.6772846467381501
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [0.03215411 0.06762571 0.03164373 0.01097323 0.07808855 0.03725794
 0.01378033 0.04567925 0.03317488 0.03011258]
ene_total = [2.12711913 3.82878864 2.11331163 1.00058971 4.36464773 2.28034368
 1.14291118 2.75181183 2.3215154  1.90939155]
ti_comp = [0.45871166 0.48133195 0.45630249 0.4667549  0.48138282 0.47984682
 0.46705067 0.47204883 0.43098118 0.48078779]
ti_coms = [0.09358479 0.0709645  0.09599397 0.08554155 0.07091363 0.07244963
 0.08524578 0.08024762 0.12131527 0.07150866]
t_total = [28.29985733 28.29985733 28.29985733 28.29985733 28.29985733 28.29985733
 28.29985733 28.29985733 28.29985733 28.29985733]
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [9.87438211e-06 8.34307382e-05 9.51124209e-06 3.79057801e-07
 1.28428366e-04 1.40388386e-05 7.49775295e-07 2.67339474e-05
 1.22854563e-05 7.38273045e-06]
ene_total = [0.48003762 0.36789952 0.49236369 0.43833721 0.36994456 0.37195374
 0.43684068 0.41256138 0.62225307 0.3667911 ]
optimize_network iter = 0 obj = 4.3589825705706815
eta = 0.6772846467381501
freqs = [35048280.46691417 70248512.05463123 34674068.18486581 11754807.32564252
 81108578.19233961 38822740.19627412 14752502.84041418 48384031.59575192
 38487614.47521663 31315873.21898438]
eta_min = 0.677284646738166	eta_max = 0.6772846467381503
af = 0.012426086302136423	bf = 1.4018686060877306	zeta = 0.013668694932350066	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [2.41523466e-06 2.04068274e-05 2.32641206e-06 9.27160332e-08
 3.14130684e-05 3.43384418e-06 1.83392061e-07 6.53901742e-06
 3.00497385e-06 1.80578656e-06]
ene_total = [1.70271381 1.29453122 1.74651965 1.55598755 1.29560796 1.31845798
 1.55062412 1.46086543 2.20722871 1.30104589]
ti_comp = [0.45871166 0.48133195 0.45630249 0.4667549  0.48138282 0.47984682
 0.46705067 0.47204883 0.43098118 0.48078779]
ti_coms = [0.09358479 0.0709645  0.09599397 0.08554155 0.07091363 0.07244963
 0.08524578 0.08024762 0.12131527 0.07150866]
t_total = [28.29985733 28.29985733 28.29985733 28.29985733 28.29985733 28.29985733
 28.29985733 28.29985733 28.29985733 28.29985733]
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [9.87438211e-06 8.34307382e-05 9.51124209e-06 3.79057801e-07
 1.28428366e-04 1.40388386e-05 7.49775295e-07 2.67339474e-05
 1.22854563e-05 7.38273045e-06]
ene_total = [0.48003762 0.36789952 0.49236369 0.43833721 0.36994456 0.37195374
 0.43684068 0.41256138 0.62225307 0.3667911 ]
optimize_network iter = 1 obj = 4.358982570570683
eta = 0.6772846467381503
freqs = [35048280.46691417 70248512.05463123 34674068.18486582 11754807.32564252
 81108578.19233961 38822740.19627412 14752502.84041418 48384031.59575192
 38487614.47521663 31315873.21898438]
Done!
ene_coms = [0.00935848 0.00709645 0.0095994  0.00855415 0.00709136 0.00724496
 0.00852458 0.00802476 0.01213153 0.00715087]
ene_comp = [9.28656765e-06 7.84641698e-05 8.94504507e-06 3.56492778e-07
 1.20783123e-04 1.32031172e-05 7.05141742e-07 2.51424959e-05
 1.15541125e-05 6.94324212e-06]
ene_total = [0.00936777 0.00717491 0.00960834 0.00855451 0.00721215 0.00725817
 0.00852528 0.0080499  0.01214308 0.00715781]
At round 34 energy consumption: 0.0850519238371433
At round 34 eta: 0.6772846467381503
At round 34 a_n: 16.536044059297588
At round 34 local rounds: 12.759567346547202
At round 34 global rounds: 51.24033886878731
gradient difference: 0.513790488243103
train() client id: f_00000-0-0 loss: 1.184407  [   32/  126]
train() client id: f_00000-0-1 loss: 1.189440  [   64/  126]
train() client id: f_00000-0-2 loss: 0.904779  [   96/  126]
train() client id: f_00000-1-0 loss: 1.123416  [   32/  126]
train() client id: f_00000-1-1 loss: 0.992584  [   64/  126]
train() client id: f_00000-1-2 loss: 0.902424  [   96/  126]
train() client id: f_00000-2-0 loss: 0.975047  [   32/  126]
train() client id: f_00000-2-1 loss: 0.897688  [   64/  126]
train() client id: f_00000-2-2 loss: 1.109423  [   96/  126]
train() client id: f_00000-3-0 loss: 1.042023  [   32/  126]
train() client id: f_00000-3-1 loss: 0.803481  [   64/  126]
train() client id: f_00000-3-2 loss: 0.964659  [   96/  126]
train() client id: f_00000-4-0 loss: 0.858650  [   32/  126]
train() client id: f_00000-4-1 loss: 0.803513  [   64/  126]
train() client id: f_00000-4-2 loss: 1.016873  [   96/  126]
train() client id: f_00000-5-0 loss: 0.790405  [   32/  126]
train() client id: f_00000-5-1 loss: 0.822842  [   64/  126]
train() client id: f_00000-5-2 loss: 1.072416  [   96/  126]
train() client id: f_00000-6-0 loss: 0.890753  [   32/  126]
train() client id: f_00000-6-1 loss: 0.806881  [   64/  126]
train() client id: f_00000-6-2 loss: 0.884814  [   96/  126]
train() client id: f_00000-7-0 loss: 0.832438  [   32/  126]
train() client id: f_00000-7-1 loss: 0.944605  [   64/  126]
train() client id: f_00000-7-2 loss: 0.964942  [   96/  126]
train() client id: f_00000-8-0 loss: 0.805646  [   32/  126]
train() client id: f_00000-8-1 loss: 0.897181  [   64/  126]
train() client id: f_00000-8-2 loss: 0.939518  [   96/  126]
train() client id: f_00000-9-0 loss: 0.791322  [   32/  126]
train() client id: f_00000-9-1 loss: 0.856046  [   64/  126]
train() client id: f_00000-9-2 loss: 0.944839  [   96/  126]
train() client id: f_00000-10-0 loss: 0.845989  [   32/  126]
train() client id: f_00000-10-1 loss: 1.011553  [   64/  126]
train() client id: f_00000-10-2 loss: 0.832675  [   96/  126]
train() client id: f_00000-11-0 loss: 0.921705  [   32/  126]
train() client id: f_00000-11-1 loss: 0.859922  [   64/  126]
train() client id: f_00000-11-2 loss: 0.964537  [   96/  126]
train() client id: f_00001-0-0 loss: 0.461891  [   32/  265]
train() client id: f_00001-0-1 loss: 0.504343  [   64/  265]
train() client id: f_00001-0-2 loss: 0.377182  [   96/  265]
train() client id: f_00001-0-3 loss: 0.362141  [  128/  265]
train() client id: f_00001-0-4 loss: 0.310131  [  160/  265]
train() client id: f_00001-0-5 loss: 0.411246  [  192/  265]
train() client id: f_00001-0-6 loss: 0.458234  [  224/  265]
train() client id: f_00001-0-7 loss: 0.404491  [  256/  265]
train() client id: f_00001-1-0 loss: 0.422785  [   32/  265]
train() client id: f_00001-1-1 loss: 0.314035  [   64/  265]
train() client id: f_00001-1-2 loss: 0.489498  [   96/  265]
train() client id: f_00001-1-3 loss: 0.481474  [  128/  265]
train() client id: f_00001-1-4 loss: 0.325033  [  160/  265]
train() client id: f_00001-1-5 loss: 0.431169  [  192/  265]
train() client id: f_00001-1-6 loss: 0.372185  [  224/  265]
train() client id: f_00001-1-7 loss: 0.389171  [  256/  265]
train() client id: f_00001-2-0 loss: 0.423608  [   32/  265]
train() client id: f_00001-2-1 loss: 0.478578  [   64/  265]
train() client id: f_00001-2-2 loss: 0.374793  [   96/  265]
train() client id: f_00001-2-3 loss: 0.402124  [  128/  265]
train() client id: f_00001-2-4 loss: 0.308603  [  160/  265]
train() client id: f_00001-2-5 loss: 0.395807  [  192/  265]
train() client id: f_00001-2-6 loss: 0.415541  [  224/  265]
train() client id: f_00001-2-7 loss: 0.301452  [  256/  265]
train() client id: f_00001-3-0 loss: 0.458413  [   32/  265]
train() client id: f_00001-3-1 loss: 0.416602  [   64/  265]
train() client id: f_00001-3-2 loss: 0.434707  [   96/  265]
train() client id: f_00001-3-3 loss: 0.359846  [  128/  265]
train() client id: f_00001-3-4 loss: 0.347380  [  160/  265]
train() client id: f_00001-3-5 loss: 0.329634  [  192/  265]
train() client id: f_00001-3-6 loss: 0.431499  [  224/  265]
train() client id: f_00001-3-7 loss: 0.284542  [  256/  265]
train() client id: f_00001-4-0 loss: 0.389830  [   32/  265]
train() client id: f_00001-4-1 loss: 0.300099  [   64/  265]
train() client id: f_00001-4-2 loss: 0.377128  [   96/  265]
train() client id: f_00001-4-3 loss: 0.430901  [  128/  265]
train() client id: f_00001-4-4 loss: 0.450536  [  160/  265]
train() client id: f_00001-4-5 loss: 0.327286  [  192/  265]
train() client id: f_00001-4-6 loss: 0.389291  [  224/  265]
train() client id: f_00001-4-7 loss: 0.428853  [  256/  265]
train() client id: f_00001-5-0 loss: 0.435772  [   32/  265]
train() client id: f_00001-5-1 loss: 0.406412  [   64/  265]
train() client id: f_00001-5-2 loss: 0.343622  [   96/  265]
train() client id: f_00001-5-3 loss: 0.373061  [  128/  265]
train() client id: f_00001-5-4 loss: 0.493959  [  160/  265]
train() client id: f_00001-5-5 loss: 0.290729  [  192/  265]
train() client id: f_00001-5-6 loss: 0.329402  [  224/  265]
train() client id: f_00001-5-7 loss: 0.391185  [  256/  265]
train() client id: f_00001-6-0 loss: 0.421700  [   32/  265]
train() client id: f_00001-6-1 loss: 0.370322  [   64/  265]
train() client id: f_00001-6-2 loss: 0.278094  [   96/  265]
train() client id: f_00001-6-3 loss: 0.423659  [  128/  265]
train() client id: f_00001-6-4 loss: 0.412565  [  160/  265]
train() client id: f_00001-6-5 loss: 0.455135  [  192/  265]
train() client id: f_00001-6-6 loss: 0.282959  [  224/  265]
train() client id: f_00001-6-7 loss: 0.412192  [  256/  265]
train() client id: f_00001-7-0 loss: 0.443036  [   32/  265]
train() client id: f_00001-7-1 loss: 0.379916  [   64/  265]
train() client id: f_00001-7-2 loss: 0.276773  [   96/  265]
train() client id: f_00001-7-3 loss: 0.282797  [  128/  265]
train() client id: f_00001-7-4 loss: 0.331010  [  160/  265]
train() client id: f_00001-7-5 loss: 0.369248  [  192/  265]
train() client id: f_00001-7-6 loss: 0.358474  [  224/  265]
train() client id: f_00001-7-7 loss: 0.536603  [  256/  265]
train() client id: f_00001-8-0 loss: 0.363188  [   32/  265]
train() client id: f_00001-8-1 loss: 0.308179  [   64/  265]
train() client id: f_00001-8-2 loss: 0.283694  [   96/  265]
train() client id: f_00001-8-3 loss: 0.380435  [  128/  265]
train() client id: f_00001-8-4 loss: 0.450804  [  160/  265]
train() client id: f_00001-8-5 loss: 0.339198  [  192/  265]
train() client id: f_00001-8-6 loss: 0.404340  [  224/  265]
train() client id: f_00001-8-7 loss: 0.338240  [  256/  265]
train() client id: f_00001-9-0 loss: 0.340735  [   32/  265]
train() client id: f_00001-9-1 loss: 0.333125  [   64/  265]
train() client id: f_00001-9-2 loss: 0.292345  [   96/  265]
train() client id: f_00001-9-3 loss: 0.369924  [  128/  265]
train() client id: f_00001-9-4 loss: 0.428725  [  160/  265]
train() client id: f_00001-9-5 loss: 0.437313  [  192/  265]
train() client id: f_00001-9-6 loss: 0.332191  [  224/  265]
train() client id: f_00001-9-7 loss: 0.427068  [  256/  265]
train() client id: f_00001-10-0 loss: 0.433147  [   32/  265]
train() client id: f_00001-10-1 loss: 0.342295  [   64/  265]
train() client id: f_00001-10-2 loss: 0.363682  [   96/  265]
train() client id: f_00001-10-3 loss: 0.303848  [  128/  265]
train() client id: f_00001-10-4 loss: 0.306036  [  160/  265]
train() client id: f_00001-10-5 loss: 0.396584  [  192/  265]
train() client id: f_00001-10-6 loss: 0.376533  [  224/  265]
train() client id: f_00001-10-7 loss: 0.426382  [  256/  265]
train() client id: f_00001-11-0 loss: 0.471631  [   32/  265]
train() client id: f_00001-11-1 loss: 0.263814  [   64/  265]
train() client id: f_00001-11-2 loss: 0.384500  [   96/  265]
train() client id: f_00001-11-3 loss: 0.266888  [  128/  265]
train() client id: f_00001-11-4 loss: 0.407091  [  160/  265]
train() client id: f_00001-11-5 loss: 0.402429  [  192/  265]
train() client id: f_00001-11-6 loss: 0.286815  [  224/  265]
train() client id: f_00001-11-7 loss: 0.420046  [  256/  265]
train() client id: f_00002-0-0 loss: 1.304519  [   32/  124]
train() client id: f_00002-0-1 loss: 1.101389  [   64/  124]
train() client id: f_00002-0-2 loss: 1.277264  [   96/  124]
train() client id: f_00002-1-0 loss: 1.157112  [   32/  124]
train() client id: f_00002-1-1 loss: 1.287906  [   64/  124]
train() client id: f_00002-1-2 loss: 1.190119  [   96/  124]
train() client id: f_00002-2-0 loss: 1.198197  [   32/  124]
train() client id: f_00002-2-1 loss: 1.229159  [   64/  124]
train() client id: f_00002-2-2 loss: 1.171278  [   96/  124]
train() client id: f_00002-3-0 loss: 1.056537  [   32/  124]
train() client id: f_00002-3-1 loss: 1.051502  [   64/  124]
train() client id: f_00002-3-2 loss: 1.331420  [   96/  124]
train() client id: f_00002-4-0 loss: 1.060043  [   32/  124]
train() client id: f_00002-4-1 loss: 1.095668  [   64/  124]
train() client id: f_00002-4-2 loss: 1.150499  [   96/  124]
train() client id: f_00002-5-0 loss: 1.146065  [   32/  124]
train() client id: f_00002-5-1 loss: 1.110502  [   64/  124]
train() client id: f_00002-5-2 loss: 1.102067  [   96/  124]
train() client id: f_00002-6-0 loss: 1.070120  [   32/  124]
train() client id: f_00002-6-1 loss: 1.073066  [   64/  124]
train() client id: f_00002-6-2 loss: 1.204917  [   96/  124]
train() client id: f_00002-7-0 loss: 1.174785  [   32/  124]
train() client id: f_00002-7-1 loss: 1.165131  [   64/  124]
train() client id: f_00002-7-2 loss: 1.066501  [   96/  124]
train() client id: f_00002-8-0 loss: 0.979804  [   32/  124]
train() client id: f_00002-8-1 loss: 1.016694  [   64/  124]
train() client id: f_00002-8-2 loss: 1.026234  [   96/  124]
train() client id: f_00002-9-0 loss: 1.150494  [   32/  124]
train() client id: f_00002-9-1 loss: 1.099472  [   64/  124]
train() client id: f_00002-9-2 loss: 1.082469  [   96/  124]
train() client id: f_00002-10-0 loss: 0.947073  [   32/  124]
train() client id: f_00002-10-1 loss: 1.427571  [   64/  124]
train() client id: f_00002-10-2 loss: 1.078397  [   96/  124]
train() client id: f_00002-11-0 loss: 1.113438  [   32/  124]
train() client id: f_00002-11-1 loss: 1.173350  [   64/  124]
train() client id: f_00002-11-2 loss: 1.184704  [   96/  124]
train() client id: f_00003-0-0 loss: 0.577502  [   32/   43]
train() client id: f_00003-1-0 loss: 0.709873  [   32/   43]
train() client id: f_00003-2-0 loss: 0.835075  [   32/   43]
train() client id: f_00003-3-0 loss: 0.733331  [   32/   43]
train() client id: f_00003-4-0 loss: 0.629473  [   32/   43]
train() client id: f_00003-5-0 loss: 0.747413  [   32/   43]
train() client id: f_00003-6-0 loss: 0.755946  [   32/   43]
train() client id: f_00003-7-0 loss: 0.838068  [   32/   43]
train() client id: f_00003-8-0 loss: 0.694646  [   32/   43]
train() client id: f_00003-9-0 loss: 0.765407  [   32/   43]
train() client id: f_00003-10-0 loss: 0.736195  [   32/   43]
train() client id: f_00003-11-0 loss: 0.726610  [   32/   43]
train() client id: f_00004-0-0 loss: 0.715459  [   32/  306]
train() client id: f_00004-0-1 loss: 1.006812  [   64/  306]
train() client id: f_00004-0-2 loss: 0.816525  [   96/  306]
train() client id: f_00004-0-3 loss: 1.017445  [  128/  306]
train() client id: f_00004-0-4 loss: 0.837867  [  160/  306]
train() client id: f_00004-0-5 loss: 0.796498  [  192/  306]
train() client id: f_00004-0-6 loss: 0.770735  [  224/  306]
train() client id: f_00004-0-7 loss: 0.943611  [  256/  306]
train() client id: f_00004-0-8 loss: 0.977560  [  288/  306]
train() client id: f_00004-1-0 loss: 0.874087  [   32/  306]
train() client id: f_00004-1-1 loss: 0.944541  [   64/  306]
train() client id: f_00004-1-2 loss: 0.850648  [   96/  306]
train() client id: f_00004-1-3 loss: 0.801633  [  128/  306]
train() client id: f_00004-1-4 loss: 0.846461  [  160/  306]
train() client id: f_00004-1-5 loss: 0.853799  [  192/  306]
train() client id: f_00004-1-6 loss: 0.757302  [  224/  306]
train() client id: f_00004-1-7 loss: 1.014727  [  256/  306]
train() client id: f_00004-1-8 loss: 0.880835  [  288/  306]
train() client id: f_00004-2-0 loss: 0.905772  [   32/  306]
train() client id: f_00004-2-1 loss: 0.795562  [   64/  306]
train() client id: f_00004-2-2 loss: 0.905011  [   96/  306]
train() client id: f_00004-2-3 loss: 0.902722  [  128/  306]
train() client id: f_00004-2-4 loss: 0.726043  [  160/  306]
train() client id: f_00004-2-5 loss: 0.984083  [  192/  306]
train() client id: f_00004-2-6 loss: 0.942662  [  224/  306]
train() client id: f_00004-2-7 loss: 0.982721  [  256/  306]
train() client id: f_00004-2-8 loss: 0.783960  [  288/  306]
train() client id: f_00004-3-0 loss: 0.880198  [   32/  306]
train() client id: f_00004-3-1 loss: 0.935025  [   64/  306]
train() client id: f_00004-3-2 loss: 0.897922  [   96/  306]
train() client id: f_00004-3-3 loss: 0.913009  [  128/  306]
train() client id: f_00004-3-4 loss: 0.917364  [  160/  306]
train() client id: f_00004-3-5 loss: 0.810156  [  192/  306]
train() client id: f_00004-3-6 loss: 0.901767  [  224/  306]
train() client id: f_00004-3-7 loss: 0.862319  [  256/  306]
train() client id: f_00004-3-8 loss: 0.753575  [  288/  306]
train() client id: f_00004-4-0 loss: 0.931740  [   32/  306]
train() client id: f_00004-4-1 loss: 0.941063  [   64/  306]
train() client id: f_00004-4-2 loss: 0.821026  [   96/  306]
train() client id: f_00004-4-3 loss: 0.837601  [  128/  306]
train() client id: f_00004-4-4 loss: 0.831408  [  160/  306]
train() client id: f_00004-4-5 loss: 0.850031  [  192/  306]
train() client id: f_00004-4-6 loss: 0.822569  [  224/  306]
train() client id: f_00004-4-7 loss: 0.918724  [  256/  306]
train() client id: f_00004-4-8 loss: 0.822851  [  288/  306]
train() client id: f_00004-5-0 loss: 0.951971  [   32/  306]
train() client id: f_00004-5-1 loss: 0.827942  [   64/  306]
train() client id: f_00004-5-2 loss: 0.938885  [   96/  306]
train() client id: f_00004-5-3 loss: 0.982987  [  128/  306]
train() client id: f_00004-5-4 loss: 0.847685  [  160/  306]
train() client id: f_00004-5-5 loss: 0.836550  [  192/  306]
train() client id: f_00004-5-6 loss: 0.806650  [  224/  306]
train() client id: f_00004-5-7 loss: 0.820672  [  256/  306]
train() client id: f_00004-5-8 loss: 0.908581  [  288/  306]
train() client id: f_00004-6-0 loss: 0.790620  [   32/  306]
train() client id: f_00004-6-1 loss: 0.871181  [   64/  306]
train() client id: f_00004-6-2 loss: 0.860224  [   96/  306]
train() client id: f_00004-6-3 loss: 0.935354  [  128/  306]
train() client id: f_00004-6-4 loss: 0.921862  [  160/  306]
train() client id: f_00004-6-5 loss: 0.823998  [  192/  306]
train() client id: f_00004-6-6 loss: 0.906260  [  224/  306]
train() client id: f_00004-6-7 loss: 0.885209  [  256/  306]
train() client id: f_00004-6-8 loss: 0.786926  [  288/  306]
train() client id: f_00004-7-0 loss: 0.823515  [   32/  306]
train() client id: f_00004-7-1 loss: 0.762355  [   64/  306]
train() client id: f_00004-7-2 loss: 0.845471  [   96/  306]
train() client id: f_00004-7-3 loss: 0.906867  [  128/  306]
train() client id: f_00004-7-4 loss: 0.947997  [  160/  306]
train() client id: f_00004-7-5 loss: 0.848393  [  192/  306]
train() client id: f_00004-7-6 loss: 0.894742  [  224/  306]
train() client id: f_00004-7-7 loss: 0.880053  [  256/  306]
train() client id: f_00004-7-8 loss: 0.974018  [  288/  306]
train() client id: f_00004-8-0 loss: 0.822126  [   32/  306]
train() client id: f_00004-8-1 loss: 0.940329  [   64/  306]
train() client id: f_00004-8-2 loss: 0.804585  [   96/  306]
train() client id: f_00004-8-3 loss: 0.936246  [  128/  306]
train() client id: f_00004-8-4 loss: 0.844123  [  160/  306]
train() client id: f_00004-8-5 loss: 0.762481  [  192/  306]
train() client id: f_00004-8-6 loss: 0.988414  [  224/  306]
train() client id: f_00004-8-7 loss: 0.848274  [  256/  306]
train() client id: f_00004-8-8 loss: 0.879561  [  288/  306]
train() client id: f_00004-9-0 loss: 0.871317  [   32/  306]
train() client id: f_00004-9-1 loss: 0.957726  [   64/  306]
train() client id: f_00004-9-2 loss: 0.861257  [   96/  306]
train() client id: f_00004-9-3 loss: 0.736423  [  128/  306]
train() client id: f_00004-9-4 loss: 0.736733  [  160/  306]
train() client id: f_00004-9-5 loss: 1.008506  [  192/  306]
train() client id: f_00004-9-6 loss: 1.011709  [  224/  306]
train() client id: f_00004-9-7 loss: 0.758564  [  256/  306]
train() client id: f_00004-9-8 loss: 0.897094  [  288/  306]
train() client id: f_00004-10-0 loss: 0.842571  [   32/  306]
train() client id: f_00004-10-1 loss: 0.882705  [   64/  306]
train() client id: f_00004-10-2 loss: 0.915156  [   96/  306]
train() client id: f_00004-10-3 loss: 0.988224  [  128/  306]
train() client id: f_00004-10-4 loss: 0.895186  [  160/  306]
train() client id: f_00004-10-5 loss: 0.953046  [  192/  306]
train() client id: f_00004-10-6 loss: 0.819059  [  224/  306]
train() client id: f_00004-10-7 loss: 0.795857  [  256/  306]
train() client id: f_00004-10-8 loss: 0.844374  [  288/  306]
train() client id: f_00004-11-0 loss: 0.964818  [   32/  306]
train() client id: f_00004-11-1 loss: 0.849577  [   64/  306]
train() client id: f_00004-11-2 loss: 0.846630  [   96/  306]
train() client id: f_00004-11-3 loss: 0.883787  [  128/  306]
train() client id: f_00004-11-4 loss: 0.805246  [  160/  306]
train() client id: f_00004-11-5 loss: 0.903590  [  192/  306]
train() client id: f_00004-11-6 loss: 0.883631  [  224/  306]
train() client id: f_00004-11-7 loss: 0.910882  [  256/  306]
train() client id: f_00004-11-8 loss: 0.908302  [  288/  306]
train() client id: f_00005-0-0 loss: 0.638777  [   32/  146]
train() client id: f_00005-0-1 loss: 0.367346  [   64/  146]
train() client id: f_00005-0-2 loss: 0.587719  [   96/  146]
train() client id: f_00005-0-3 loss: 0.690832  [  128/  146]
train() client id: f_00005-1-0 loss: 0.483290  [   32/  146]
train() client id: f_00005-1-1 loss: 0.758709  [   64/  146]
train() client id: f_00005-1-2 loss: 0.499061  [   96/  146]
train() client id: f_00005-1-3 loss: 0.525524  [  128/  146]
train() client id: f_00005-2-0 loss: 0.368128  [   32/  146]
train() client id: f_00005-2-1 loss: 0.592878  [   64/  146]
train() client id: f_00005-2-2 loss: 0.608675  [   96/  146]
train() client id: f_00005-2-3 loss: 0.639520  [  128/  146]
train() client id: f_00005-3-0 loss: 0.339147  [   32/  146]
train() client id: f_00005-3-1 loss: 0.634221  [   64/  146]
train() client id: f_00005-3-2 loss: 0.606812  [   96/  146]
train() client id: f_00005-3-3 loss: 0.720137  [  128/  146]
train() client id: f_00005-4-0 loss: 0.588509  [   32/  146]
train() client id: f_00005-4-1 loss: 0.326174  [   64/  146]
train() client id: f_00005-4-2 loss: 0.672110  [   96/  146]
train() client id: f_00005-4-3 loss: 0.488911  [  128/  146]
train() client id: f_00005-5-0 loss: 0.629593  [   32/  146]
train() client id: f_00005-5-1 loss: 0.481950  [   64/  146]
train() client id: f_00005-5-2 loss: 0.372215  [   96/  146]
train() client id: f_00005-5-3 loss: 0.657167  [  128/  146]
train() client id: f_00005-6-0 loss: 0.573586  [   32/  146]
train() client id: f_00005-6-1 loss: 0.571824  [   64/  146]
train() client id: f_00005-6-2 loss: 0.581833  [   96/  146]
train() client id: f_00005-6-3 loss: 0.475504  [  128/  146]
train() client id: f_00005-7-0 loss: 0.759426  [   32/  146]
train() client id: f_00005-7-1 loss: 0.452009  [   64/  146]
train() client id: f_00005-7-2 loss: 0.502672  [   96/  146]
train() client id: f_00005-7-3 loss: 0.450465  [  128/  146]
train() client id: f_00005-8-0 loss: 0.524843  [   32/  146]
train() client id: f_00005-8-1 loss: 0.559388  [   64/  146]
train() client id: f_00005-8-2 loss: 0.680002  [   96/  146]
train() client id: f_00005-8-3 loss: 0.557314  [  128/  146]
train() client id: f_00005-9-0 loss: 0.469578  [   32/  146]
train() client id: f_00005-9-1 loss: 0.793370  [   64/  146]
train() client id: f_00005-9-2 loss: 0.446035  [   96/  146]
train() client id: f_00005-9-3 loss: 0.405640  [  128/  146]
train() client id: f_00005-10-0 loss: 0.436082  [   32/  146]
train() client id: f_00005-10-1 loss: 0.440796  [   64/  146]
train() client id: f_00005-10-2 loss: 0.735772  [   96/  146]
train() client id: f_00005-10-3 loss: 0.691715  [  128/  146]
train() client id: f_00005-11-0 loss: 0.481305  [   32/  146]
train() client id: f_00005-11-1 loss: 0.740158  [   64/  146]
train() client id: f_00005-11-2 loss: 0.436184  [   96/  146]
train() client id: f_00005-11-3 loss: 0.337658  [  128/  146]
train() client id: f_00006-0-0 loss: 0.551524  [   32/   54]
train() client id: f_00006-1-0 loss: 0.526087  [   32/   54]
train() client id: f_00006-2-0 loss: 0.546549  [   32/   54]
train() client id: f_00006-3-0 loss: 0.490245  [   32/   54]
train() client id: f_00006-4-0 loss: 0.442559  [   32/   54]
train() client id: f_00006-5-0 loss: 0.537560  [   32/   54]
train() client id: f_00006-6-0 loss: 0.492947  [   32/   54]
train() client id: f_00006-7-0 loss: 0.537671  [   32/   54]
train() client id: f_00006-8-0 loss: 0.490026  [   32/   54]
train() client id: f_00006-9-0 loss: 0.494423  [   32/   54]
train() client id: f_00006-10-0 loss: 0.480584  [   32/   54]
train() client id: f_00006-11-0 loss: 0.490379  [   32/   54]
train() client id: f_00007-0-0 loss: 0.720329  [   32/  179]
train() client id: f_00007-0-1 loss: 0.701185  [   64/  179]
train() client id: f_00007-0-2 loss: 0.724182  [   96/  179]
train() client id: f_00007-0-3 loss: 0.629638  [  128/  179]
train() client id: f_00007-0-4 loss: 0.665694  [  160/  179]
train() client id: f_00007-1-0 loss: 0.788471  [   32/  179]
train() client id: f_00007-1-1 loss: 0.538073  [   64/  179]
train() client id: f_00007-1-2 loss: 0.827082  [   96/  179]
train() client id: f_00007-1-3 loss: 0.553447  [  128/  179]
train() client id: f_00007-1-4 loss: 0.565463  [  160/  179]
train() client id: f_00007-2-0 loss: 0.600179  [   32/  179]
train() client id: f_00007-2-1 loss: 0.546359  [   64/  179]
train() client id: f_00007-2-2 loss: 0.783271  [   96/  179]
train() client id: f_00007-2-3 loss: 0.628608  [  128/  179]
train() client id: f_00007-2-4 loss: 0.645923  [  160/  179]
train() client id: f_00007-3-0 loss: 0.613788  [   32/  179]
train() client id: f_00007-3-1 loss: 0.571432  [   64/  179]
train() client id: f_00007-3-2 loss: 0.605792  [   96/  179]
train() client id: f_00007-3-3 loss: 0.584967  [  128/  179]
train() client id: f_00007-3-4 loss: 0.713337  [  160/  179]
train() client id: f_00007-4-0 loss: 0.576740  [   32/  179]
train() client id: f_00007-4-1 loss: 0.623939  [   64/  179]
train() client id: f_00007-4-2 loss: 0.681688  [   96/  179]
train() client id: f_00007-4-3 loss: 0.669872  [  128/  179]
train() client id: f_00007-4-4 loss: 0.498282  [  160/  179]
train() client id: f_00007-5-0 loss: 0.517963  [   32/  179]
train() client id: f_00007-5-1 loss: 0.536584  [   64/  179]
train() client id: f_00007-5-2 loss: 0.770114  [   96/  179]
train() client id: f_00007-5-3 loss: 0.637289  [  128/  179]
train() client id: f_00007-5-4 loss: 0.775904  [  160/  179]
train() client id: f_00007-6-0 loss: 0.571444  [   32/  179]
train() client id: f_00007-6-1 loss: 0.579895  [   64/  179]
train() client id: f_00007-6-2 loss: 0.574861  [   96/  179]
train() client id: f_00007-6-3 loss: 0.894918  [  128/  179]
train() client id: f_00007-6-4 loss: 0.491134  [  160/  179]
train() client id: f_00007-7-0 loss: 0.481449  [   32/  179]
train() client id: f_00007-7-1 loss: 0.715774  [   64/  179]
train() client id: f_00007-7-2 loss: 0.536886  [   96/  179]
train() client id: f_00007-7-3 loss: 0.688035  [  128/  179]
train() client id: f_00007-7-4 loss: 0.594883  [  160/  179]
train() client id: f_00007-8-0 loss: 0.454158  [   32/  179]
train() client id: f_00007-8-1 loss: 0.591275  [   64/  179]
train() client id: f_00007-8-2 loss: 0.576192  [   96/  179]
train() client id: f_00007-8-3 loss: 0.735820  [  128/  179]
train() client id: f_00007-8-4 loss: 0.681663  [  160/  179]
train() client id: f_00007-9-0 loss: 0.496687  [   32/  179]
train() client id: f_00007-9-1 loss: 0.480738  [   64/  179]
train() client id: f_00007-9-2 loss: 0.852739  [   96/  179]
train() client id: f_00007-9-3 loss: 0.492063  [  128/  179]
train() client id: f_00007-9-4 loss: 0.522399  [  160/  179]
train() client id: f_00007-10-0 loss: 0.801297  [   32/  179]
train() client id: f_00007-10-1 loss: 0.584172  [   64/  179]
train() client id: f_00007-10-2 loss: 0.671556  [   96/  179]
train() client id: f_00007-10-3 loss: 0.559983  [  128/  179]
train() client id: f_00007-10-4 loss: 0.461678  [  160/  179]
train() client id: f_00007-11-0 loss: 0.469713  [   32/  179]
train() client id: f_00007-11-1 loss: 0.565677  [   64/  179]
train() client id: f_00007-11-2 loss: 0.918003  [   96/  179]
train() client id: f_00007-11-3 loss: 0.497693  [  128/  179]
train() client id: f_00007-11-4 loss: 0.750200  [  160/  179]
train() client id: f_00008-0-0 loss: 0.631014  [   32/  130]
train() client id: f_00008-0-1 loss: 0.546779  [   64/  130]
train() client id: f_00008-0-2 loss: 0.661719  [   96/  130]
train() client id: f_00008-0-3 loss: 0.662962  [  128/  130]
train() client id: f_00008-1-0 loss: 0.629195  [   32/  130]
train() client id: f_00008-1-1 loss: 0.529901  [   64/  130]
train() client id: f_00008-1-2 loss: 0.633069  [   96/  130]
train() client id: f_00008-1-3 loss: 0.691234  [  128/  130]
train() client id: f_00008-2-0 loss: 0.640405  [   32/  130]
train() client id: f_00008-2-1 loss: 0.679134  [   64/  130]
train() client id: f_00008-2-2 loss: 0.589908  [   96/  130]
train() client id: f_00008-2-3 loss: 0.609425  [  128/  130]
train() client id: f_00008-3-0 loss: 0.658059  [   32/  130]
train() client id: f_00008-3-1 loss: 0.577421  [   64/  130]
train() client id: f_00008-3-2 loss: 0.676852  [   96/  130]
train() client id: f_00008-3-3 loss: 0.579360  [  128/  130]
train() client id: f_00008-4-0 loss: 0.636304  [   32/  130]
train() client id: f_00008-4-1 loss: 0.609310  [   64/  130]
train() client id: f_00008-4-2 loss: 0.622328  [   96/  130]
train() client id: f_00008-4-3 loss: 0.651039  [  128/  130]
train() client id: f_00008-5-0 loss: 0.567905  [   32/  130]
train() client id: f_00008-5-1 loss: 0.679694  [   64/  130]
train() client id: f_00008-5-2 loss: 0.695448  [   96/  130]
train() client id: f_00008-5-3 loss: 0.559815  [  128/  130]
train() client id: f_00008-6-0 loss: 0.696948  [   32/  130]
train() client id: f_00008-6-1 loss: 0.603939  [   64/  130]
train() client id: f_00008-6-2 loss: 0.579174  [   96/  130]
train() client id: f_00008-6-3 loss: 0.604543  [  128/  130]
train() client id: f_00008-7-0 loss: 0.604424  [   32/  130]
train() client id: f_00008-7-1 loss: 0.591732  [   64/  130]
train() client id: f_00008-7-2 loss: 0.690458  [   96/  130]
train() client id: f_00008-7-3 loss: 0.566612  [  128/  130]
train() client id: f_00008-8-0 loss: 0.595401  [   32/  130]
train() client id: f_00008-8-1 loss: 0.553258  [   64/  130]
train() client id: f_00008-8-2 loss: 0.681163  [   96/  130]
train() client id: f_00008-8-3 loss: 0.684779  [  128/  130]
train() client id: f_00008-9-0 loss: 0.529464  [   32/  130]
train() client id: f_00008-9-1 loss: 0.674675  [   64/  130]
train() client id: f_00008-9-2 loss: 0.672413  [   96/  130]
train() client id: f_00008-9-3 loss: 0.644199  [  128/  130]
train() client id: f_00008-10-0 loss: 0.613649  [   32/  130]
train() client id: f_00008-10-1 loss: 0.554163  [   64/  130]
train() client id: f_00008-10-2 loss: 0.573732  [   96/  130]
train() client id: f_00008-10-3 loss: 0.784629  [  128/  130]
train() client id: f_00008-11-0 loss: 0.630735  [   32/  130]
train() client id: f_00008-11-1 loss: 0.551761  [   64/  130]
train() client id: f_00008-11-2 loss: 0.583390  [   96/  130]
train() client id: f_00008-11-3 loss: 0.684229  [  128/  130]
train() client id: f_00009-0-0 loss: 0.973387  [   32/  118]
train() client id: f_00009-0-1 loss: 1.039206  [   64/  118]
train() client id: f_00009-0-2 loss: 0.982945  [   96/  118]
train() client id: f_00009-1-0 loss: 1.022523  [   32/  118]
train() client id: f_00009-1-1 loss: 0.914541  [   64/  118]
train() client id: f_00009-1-2 loss: 0.967475  [   96/  118]
train() client id: f_00009-2-0 loss: 1.045759  [   32/  118]
train() client id: f_00009-2-1 loss: 0.939688  [   64/  118]
train() client id: f_00009-2-2 loss: 0.876409  [   96/  118]
train() client id: f_00009-3-0 loss: 0.942461  [   32/  118]
train() client id: f_00009-3-1 loss: 0.821845  [   64/  118]
train() client id: f_00009-3-2 loss: 0.799539  [   96/  118]
train() client id: f_00009-4-0 loss: 0.752304  [   32/  118]
train() client id: f_00009-4-1 loss: 0.975132  [   64/  118]
train() client id: f_00009-4-2 loss: 0.858537  [   96/  118]
train() client id: f_00009-5-0 loss: 0.887601  [   32/  118]
train() client id: f_00009-5-1 loss: 0.872111  [   64/  118]
train() client id: f_00009-5-2 loss: 0.668963  [   96/  118]
train() client id: f_00009-6-0 loss: 0.920332  [   32/  118]
train() client id: f_00009-6-1 loss: 0.687698  [   64/  118]
train() client id: f_00009-6-2 loss: 0.811211  [   96/  118]
train() client id: f_00009-7-0 loss: 0.646352  [   32/  118]
train() client id: f_00009-7-1 loss: 0.855814  [   64/  118]
train() client id: f_00009-7-2 loss: 0.652650  [   96/  118]
train() client id: f_00009-8-0 loss: 0.848057  [   32/  118]
train() client id: f_00009-8-1 loss: 0.740566  [   64/  118]
train() client id: f_00009-8-2 loss: 0.745782  [   96/  118]
train() client id: f_00009-9-0 loss: 0.791015  [   32/  118]
train() client id: f_00009-9-1 loss: 0.895592  [   64/  118]
train() client id: f_00009-9-2 loss: 0.667759  [   96/  118]
train() client id: f_00009-10-0 loss: 0.840102  [   32/  118]
train() client id: f_00009-10-1 loss: 0.755631  [   64/  118]
train() client id: f_00009-10-2 loss: 0.671993  [   96/  118]
train() client id: f_00009-11-0 loss: 0.813010  [   32/  118]
train() client id: f_00009-11-1 loss: 0.688275  [   64/  118]
train() client id: f_00009-11-2 loss: 0.692432  [   96/  118]
At round 34 accuracy: 0.6472148541114059
At round 34 training accuracy: 0.5828303152246814
At round 34 training loss: 0.8250258429733408
update_location
xs = -3.905658 4.200318 190.009024 18.811294 0.979296 3.956410 -152.443192 -131.324852 174.663977 -117.060879 
ys = 182.587959 165.555839 1.320614 -152.455176 144.350187 127.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 208.215314 193.458984 214.721152 183.293332 175.607333 162.333331 182.334356 165.066329 202.030133 154.010588 
dists_bs = 171.262520 179.449007 404.222855 380.512431 178.628452 185.072691 179.051514 179.569967 383.470226 180.547598 
uav_gains = -108.205518 -107.271979 -108.632453 -106.635691 -106.147541 -105.272899 -106.575273 -105.457174 -107.810189 -104.695202 
bs_gains = -102.109604 -102.677409 -112.552443 -111.817387 -102.621677 -103.052645 -102.650443 -102.685603 -111.911545 -102.751627 
Round 35
-------------------------------
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.3715199  13.18889052  6.27703667  2.2632532  15.21119563  7.32046252
  2.80542146  8.96352054  6.61377748  5.93751329]
obj_prev = 74.95259119677627
eta_min = 3.0936410554122904e-15	eta_max = 0.9287856551033156
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 17.39350337614215	eta = 0.9090909090909091
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 31.811065698035446	eta = 0.4970684084145405
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 24.73112036289277	eta = 0.6393675484357765
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 23.451279643601314	eta = 0.6742606815832007
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 23.38394553843602	eta = 0.6762022161958229
af = 15.812275796492862	bf = 1.3848442141784587	zeta = 23.38374385979249	eta = 0.6762080482621735
eta = 0.6762080482621735
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [0.03228538 0.0679018  0.03177292 0.01101803 0.07840736 0.03741005
 0.01383659 0.04586574 0.03331032 0.03023552]
ene_total = [2.0907422  3.75044408 2.07789445 0.9851544  4.27493166 2.23178708
 1.1246433  2.70086556 2.2793763  1.86790483]
ti_comp = [0.46966707 0.49397079 0.46711134 0.47805782 0.49415416 0.49271153
 0.47835241 0.48347539 0.44219867 0.49372516]
ti_coms = [0.09519389 0.07089017 0.09774963 0.08680315 0.07070681 0.07214943
 0.08650855 0.08138558 0.1226623  0.07113581]
t_total = [28.24985313 28.24985313 28.24985313 28.24985313 28.24985313 28.24985313
 28.24985313 28.24985313 28.24985313 28.24985313]
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [9.53493184e-06 8.01901942e-05 9.18777644e-06 3.65789049e-07
 1.23374545e-04 1.34790504e-05 7.23554964e-07 2.57986237e-05
 1.18135760e-05 7.08698796e-06]
ene_total = [0.47656101 0.35854674 0.48932539 0.4341386  0.35978946 0.36150821
 0.43268317 0.40831618 0.61405008 0.35611918]
optimize_network iter = 0 obj = 4.291038018675944
eta = 0.6762080482621735
freqs = [34370497.98105174 68730577.98040815 34010002.78410535 11523739.23427672
 79334916.60468721 37963436.80656631 14462760.16316765 47433377.96218494
 37664422.59882781 30619785.51757285]
eta_min = 0.6762080482621772	eta_max = 0.6762080482621733
af = 0.011653184886415587	bf = 1.3848442141784587	zeta = 0.012818503375057147	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [2.32272370e-06 1.95344516e-05 2.23815612e-06 8.91067609e-08
 3.00542243e-05 3.28351691e-06 1.76259075e-07 6.28458343e-06
 2.87780483e-06 1.72640090e-06]
ene_total = [1.69608812 1.26623554 1.74159793 1.54622714 1.26484323 1.28577181
 1.54099513 1.45082842 2.18547748 1.2674389 ]
ti_comp = [0.46966707 0.49397079 0.46711134 0.47805782 0.49415416 0.49271153
 0.47835241 0.48347539 0.44219867 0.49372516]
ti_coms = [0.09519389 0.07089017 0.09774963 0.08680315 0.07070681 0.07214943
 0.08650855 0.08138558 0.1226623  0.07113581]
t_total = [28.24985313 28.24985313 28.24985313 28.24985313 28.24985313 28.24985313
 28.24985313 28.24985313 28.24985313 28.24985313]
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [9.53493184e-06 8.01901942e-05 9.18777644e-06 3.65789049e-07
 1.23374545e-04 1.34790504e-05 7.23554964e-07 2.57986237e-05
 1.18135760e-05 7.08698796e-06]
ene_total = [0.47656101 0.35854674 0.48932539 0.4341386  0.35978946 0.36150821
 0.43268317 0.40831618 0.61405008 0.35611918]
optimize_network iter = 1 obj = 4.291038018675939
eta = 0.6762080482621733
freqs = [34370497.98105174 68730577.98040818 34010002.78410535 11523739.23427672
 79334916.60468724 37963436.80656631 14462760.16316766 47433377.96218494
 37664422.59882782 30619785.51757286]
Done!
ene_coms = [0.00951939 0.00708902 0.00977496 0.00868031 0.00707068 0.00721494
 0.00865086 0.00813856 0.01226623 0.00711358]
ene_comp = [8.93086335e-06 7.51098884e-05 8.60570135e-06 3.42615140e-07
 1.15558372e-04 1.26251094e-05 6.77715438e-07 2.41641982e-05
 1.10651481e-05 6.63800456e-06]
ene_total = [0.00952832 0.00716413 0.00978357 0.00868066 0.00718624 0.00722757
 0.00865153 0.00816272 0.01227729 0.00712022]
At round 35 energy consumption: 0.0857822507966446
At round 35 eta: 0.6762080482621733
At round 35 a_n: 16.193498212328272
At round 35 local rounds: 12.811659702043181
At round 35 global rounds: 50.012046702878195
gradient difference: 0.4568757116794586
train() client id: f_00000-0-0 loss: 1.375098  [   32/  126]
train() client id: f_00000-0-1 loss: 1.155435  [   64/  126]
train() client id: f_00000-0-2 loss: 0.915200  [   96/  126]
train() client id: f_00000-1-0 loss: 1.128208  [   32/  126]
train() client id: f_00000-1-1 loss: 1.041859  [   64/  126]
train() client id: f_00000-1-2 loss: 0.904544  [   96/  126]
train() client id: f_00000-2-0 loss: 0.968951  [   32/  126]
train() client id: f_00000-2-1 loss: 1.008459  [   64/  126]
train() client id: f_00000-2-2 loss: 0.979821  [   96/  126]
train() client id: f_00000-3-0 loss: 0.910224  [   32/  126]
train() client id: f_00000-3-1 loss: 0.970875  [   64/  126]
train() client id: f_00000-3-2 loss: 0.938300  [   96/  126]
train() client id: f_00000-4-0 loss: 0.977629  [   32/  126]
train() client id: f_00000-4-1 loss: 0.815389  [   64/  126]
train() client id: f_00000-4-2 loss: 0.837934  [   96/  126]
train() client id: f_00000-5-0 loss: 0.855060  [   32/  126]
train() client id: f_00000-5-1 loss: 0.795437  [   64/  126]
train() client id: f_00000-5-2 loss: 0.696691  [   96/  126]
train() client id: f_00000-6-0 loss: 0.825303  [   32/  126]
train() client id: f_00000-6-1 loss: 0.814556  [   64/  126]
train() client id: f_00000-6-2 loss: 0.920701  [   96/  126]
train() client id: f_00000-7-0 loss: 0.887554  [   32/  126]
train() client id: f_00000-7-1 loss: 0.735125  [   64/  126]
train() client id: f_00000-7-2 loss: 0.883968  [   96/  126]
train() client id: f_00000-8-0 loss: 0.813966  [   32/  126]
train() client id: f_00000-8-1 loss: 0.871107  [   64/  126]
train() client id: f_00000-8-2 loss: 0.771681  [   96/  126]
train() client id: f_00000-9-0 loss: 0.824485  [   32/  126]
train() client id: f_00000-9-1 loss: 0.820732  [   64/  126]
train() client id: f_00000-9-2 loss: 0.750295  [   96/  126]
train() client id: f_00000-10-0 loss: 0.820188  [   32/  126]
train() client id: f_00000-10-1 loss: 0.746949  [   64/  126]
train() client id: f_00000-10-2 loss: 0.730794  [   96/  126]
train() client id: f_00000-11-0 loss: 0.749093  [   32/  126]
train() client id: f_00000-11-1 loss: 0.766925  [   64/  126]
train() client id: f_00000-11-2 loss: 0.709484  [   96/  126]
train() client id: f_00001-0-0 loss: 0.425288  [   32/  265]
train() client id: f_00001-0-1 loss: 0.468074  [   64/  265]
train() client id: f_00001-0-2 loss: 0.444147  [   96/  265]
train() client id: f_00001-0-3 loss: 0.399269  [  128/  265]
train() client id: f_00001-0-4 loss: 0.399307  [  160/  265]
train() client id: f_00001-0-5 loss: 0.526005  [  192/  265]
train() client id: f_00001-0-6 loss: 0.459416  [  224/  265]
train() client id: f_00001-0-7 loss: 0.327931  [  256/  265]
train() client id: f_00001-1-0 loss: 0.416479  [   32/  265]
train() client id: f_00001-1-1 loss: 0.344218  [   64/  265]
train() client id: f_00001-1-2 loss: 0.329643  [   96/  265]
train() client id: f_00001-1-3 loss: 0.514271  [  128/  265]
train() client id: f_00001-1-4 loss: 0.485453  [  160/  265]
train() client id: f_00001-1-5 loss: 0.559413  [  192/  265]
train() client id: f_00001-1-6 loss: 0.327925  [  224/  265]
train() client id: f_00001-1-7 loss: 0.333773  [  256/  265]
train() client id: f_00001-2-0 loss: 0.441549  [   32/  265]
train() client id: f_00001-2-1 loss: 0.683439  [   64/  265]
train() client id: f_00001-2-2 loss: 0.395184  [   96/  265]
train() client id: f_00001-2-3 loss: 0.366101  [  128/  265]
train() client id: f_00001-2-4 loss: 0.418153  [  160/  265]
train() client id: f_00001-2-5 loss: 0.301226  [  192/  265]
train() client id: f_00001-2-6 loss: 0.342119  [  224/  265]
train() client id: f_00001-2-7 loss: 0.355022  [  256/  265]
train() client id: f_00001-3-0 loss: 0.450662  [   32/  265]
train() client id: f_00001-3-1 loss: 0.397373  [   64/  265]
train() client id: f_00001-3-2 loss: 0.482731  [   96/  265]
train() client id: f_00001-3-3 loss: 0.519370  [  128/  265]
train() client id: f_00001-3-4 loss: 0.320295  [  160/  265]
train() client id: f_00001-3-5 loss: 0.328477  [  192/  265]
train() client id: f_00001-3-6 loss: 0.322221  [  224/  265]
train() client id: f_00001-3-7 loss: 0.445337  [  256/  265]
train() client id: f_00001-4-0 loss: 0.419234  [   32/  265]
train() client id: f_00001-4-1 loss: 0.361227  [   64/  265]
train() client id: f_00001-4-2 loss: 0.329224  [   96/  265]
train() client id: f_00001-4-3 loss: 0.396731  [  128/  265]
train() client id: f_00001-4-4 loss: 0.427525  [  160/  265]
train() client id: f_00001-4-5 loss: 0.409572  [  192/  265]
train() client id: f_00001-4-6 loss: 0.375388  [  224/  265]
train() client id: f_00001-4-7 loss: 0.433784  [  256/  265]
train() client id: f_00001-5-0 loss: 0.446335  [   32/  265]
train() client id: f_00001-5-1 loss: 0.376300  [   64/  265]
train() client id: f_00001-5-2 loss: 0.522366  [   96/  265]
train() client id: f_00001-5-3 loss: 0.364767  [  128/  265]
train() client id: f_00001-5-4 loss: 0.401382  [  160/  265]
train() client id: f_00001-5-5 loss: 0.322435  [  192/  265]
train() client id: f_00001-5-6 loss: 0.384406  [  224/  265]
train() client id: f_00001-5-7 loss: 0.369609  [  256/  265]
train() client id: f_00001-6-0 loss: 0.395212  [   32/  265]
train() client id: f_00001-6-1 loss: 0.339374  [   64/  265]
train() client id: f_00001-6-2 loss: 0.306153  [   96/  265]
train() client id: f_00001-6-3 loss: 0.494026  [  128/  265]
train() client id: f_00001-6-4 loss: 0.443353  [  160/  265]
train() client id: f_00001-6-5 loss: 0.407290  [  192/  265]
train() client id: f_00001-6-6 loss: 0.364169  [  224/  265]
train() client id: f_00001-6-7 loss: 0.422138  [  256/  265]
train() client id: f_00001-7-0 loss: 0.309068  [   32/  265]
train() client id: f_00001-7-1 loss: 0.413039  [   64/  265]
train() client id: f_00001-7-2 loss: 0.441845  [   96/  265]
train() client id: f_00001-7-3 loss: 0.293779  [  128/  265]
train() client id: f_00001-7-4 loss: 0.311103  [  160/  265]
train() client id: f_00001-7-5 loss: 0.488827  [  192/  265]
train() client id: f_00001-7-6 loss: 0.387132  [  224/  265]
train() client id: f_00001-7-7 loss: 0.462695  [  256/  265]
train() client id: f_00001-8-0 loss: 0.347240  [   32/  265]
train() client id: f_00001-8-1 loss: 0.393730  [   64/  265]
train() client id: f_00001-8-2 loss: 0.353984  [   96/  265]
train() client id: f_00001-8-3 loss: 0.373637  [  128/  265]
train() client id: f_00001-8-4 loss: 0.422705  [  160/  265]
train() client id: f_00001-8-5 loss: 0.365021  [  192/  265]
train() client id: f_00001-8-6 loss: 0.356709  [  224/  265]
train() client id: f_00001-8-7 loss: 0.519825  [  256/  265]
train() client id: f_00001-9-0 loss: 0.384007  [   32/  265]
train() client id: f_00001-9-1 loss: 0.459894  [   64/  265]
train() client id: f_00001-9-2 loss: 0.361274  [   96/  265]
train() client id: f_00001-9-3 loss: 0.341636  [  128/  265]
train() client id: f_00001-9-4 loss: 0.330771  [  160/  265]
train() client id: f_00001-9-5 loss: 0.380925  [  192/  265]
train() client id: f_00001-9-6 loss: 0.513668  [  224/  265]
train() client id: f_00001-9-7 loss: 0.361452  [  256/  265]
train() client id: f_00001-10-0 loss: 0.353323  [   32/  265]
train() client id: f_00001-10-1 loss: 0.421677  [   64/  265]
train() client id: f_00001-10-2 loss: 0.445116  [   96/  265]
train() client id: f_00001-10-3 loss: 0.281832  [  128/  265]
train() client id: f_00001-10-4 loss: 0.434588  [  160/  265]
train() client id: f_00001-10-5 loss: 0.339735  [  192/  265]
train() client id: f_00001-10-6 loss: 0.506622  [  224/  265]
train() client id: f_00001-10-7 loss: 0.335792  [  256/  265]
train() client id: f_00001-11-0 loss: 0.370512  [   32/  265]
train() client id: f_00001-11-1 loss: 0.425381  [   64/  265]
train() client id: f_00001-11-2 loss: 0.361981  [   96/  265]
train() client id: f_00001-11-3 loss: 0.297693  [  128/  265]
train() client id: f_00001-11-4 loss: 0.362484  [  160/  265]
train() client id: f_00001-11-5 loss: 0.346792  [  192/  265]
train() client id: f_00001-11-6 loss: 0.322463  [  224/  265]
train() client id: f_00001-11-7 loss: 0.632402  [  256/  265]
train() client id: f_00002-0-0 loss: 1.162887  [   32/  124]
train() client id: f_00002-0-1 loss: 1.131917  [   64/  124]
train() client id: f_00002-0-2 loss: 1.056640  [   96/  124]
train() client id: f_00002-1-0 loss: 1.226639  [   32/  124]
train() client id: f_00002-1-1 loss: 1.036057  [   64/  124]
train() client id: f_00002-1-2 loss: 1.043306  [   96/  124]
train() client id: f_00002-2-0 loss: 1.103685  [   32/  124]
train() client id: f_00002-2-1 loss: 1.062993  [   64/  124]
train() client id: f_00002-2-2 loss: 1.076794  [   96/  124]
train() client id: f_00002-3-0 loss: 0.958942  [   32/  124]
train() client id: f_00002-3-1 loss: 0.836796  [   64/  124]
train() client id: f_00002-3-2 loss: 1.129814  [   96/  124]
train() client id: f_00002-4-0 loss: 1.013354  [   32/  124]
train() client id: f_00002-4-1 loss: 1.012829  [   64/  124]
train() client id: f_00002-4-2 loss: 1.047663  [   96/  124]
train() client id: f_00002-5-0 loss: 0.934574  [   32/  124]
train() client id: f_00002-5-1 loss: 0.956409  [   64/  124]
train() client id: f_00002-5-2 loss: 0.993804  [   96/  124]
train() client id: f_00002-6-0 loss: 1.014772  [   32/  124]
train() client id: f_00002-6-1 loss: 0.963191  [   64/  124]
train() client id: f_00002-6-2 loss: 1.059517  [   96/  124]
train() client id: f_00002-7-0 loss: 0.952213  [   32/  124]
train() client id: f_00002-7-1 loss: 0.888327  [   64/  124]
train() client id: f_00002-7-2 loss: 1.020682  [   96/  124]
train() client id: f_00002-8-0 loss: 0.913253  [   32/  124]
train() client id: f_00002-8-1 loss: 0.960761  [   64/  124]
train() client id: f_00002-8-2 loss: 0.759700  [   96/  124]
train() client id: f_00002-9-0 loss: 1.008634  [   32/  124]
train() client id: f_00002-9-1 loss: 0.894712  [   64/  124]
train() client id: f_00002-9-2 loss: 0.948027  [   96/  124]
train() client id: f_00002-10-0 loss: 1.051229  [   32/  124]
train() client id: f_00002-10-1 loss: 0.986698  [   64/  124]
train() client id: f_00002-10-2 loss: 0.899507  [   96/  124]
train() client id: f_00002-11-0 loss: 1.008229  [   32/  124]
train() client id: f_00002-11-1 loss: 0.879711  [   64/  124]
train() client id: f_00002-11-2 loss: 0.946906  [   96/  124]
train() client id: f_00003-0-0 loss: 0.824514  [   32/   43]
train() client id: f_00003-1-0 loss: 0.801682  [   32/   43]
train() client id: f_00003-2-0 loss: 0.747288  [   32/   43]
train() client id: f_00003-3-0 loss: 0.720676  [   32/   43]
train() client id: f_00003-4-0 loss: 0.759803  [   32/   43]
train() client id: f_00003-5-0 loss: 0.743106  [   32/   43]
train() client id: f_00003-6-0 loss: 0.898890  [   32/   43]
train() client id: f_00003-7-0 loss: 0.655704  [   32/   43]
train() client id: f_00003-8-0 loss: 0.809756  [   32/   43]
train() client id: f_00003-9-0 loss: 0.792826  [   32/   43]
train() client id: f_00003-10-0 loss: 0.846973  [   32/   43]
train() client id: f_00003-11-0 loss: 0.770018  [   32/   43]
train() client id: f_00004-0-0 loss: 0.894791  [   32/  306]
train() client id: f_00004-0-1 loss: 0.893865  [   64/  306]
train() client id: f_00004-0-2 loss: 0.739557  [   96/  306]
train() client id: f_00004-0-3 loss: 0.890088  [  128/  306]
train() client id: f_00004-0-4 loss: 0.849867  [  160/  306]
train() client id: f_00004-0-5 loss: 0.835543  [  192/  306]
train() client id: f_00004-0-6 loss: 0.908829  [  224/  306]
train() client id: f_00004-0-7 loss: 0.658732  [  256/  306]
train() client id: f_00004-0-8 loss: 0.733364  [  288/  306]
train() client id: f_00004-1-0 loss: 0.870314  [   32/  306]
train() client id: f_00004-1-1 loss: 0.726321  [   64/  306]
train() client id: f_00004-1-2 loss: 0.829177  [   96/  306]
train() client id: f_00004-1-3 loss: 0.843161  [  128/  306]
train() client id: f_00004-1-4 loss: 0.867756  [  160/  306]
train() client id: f_00004-1-5 loss: 0.801721  [  192/  306]
train() client id: f_00004-1-6 loss: 0.860995  [  224/  306]
train() client id: f_00004-1-7 loss: 0.783683  [  256/  306]
train() client id: f_00004-1-8 loss: 0.797606  [  288/  306]
train() client id: f_00004-2-0 loss: 0.790420  [   32/  306]
train() client id: f_00004-2-1 loss: 0.741446  [   64/  306]
train() client id: f_00004-2-2 loss: 0.924321  [   96/  306]
train() client id: f_00004-2-3 loss: 0.873062  [  128/  306]
train() client id: f_00004-2-4 loss: 0.718388  [  160/  306]
train() client id: f_00004-2-5 loss: 0.707951  [  192/  306]
train() client id: f_00004-2-6 loss: 0.802129  [  224/  306]
train() client id: f_00004-2-7 loss: 0.881139  [  256/  306]
train() client id: f_00004-2-8 loss: 0.948228  [  288/  306]
train() client id: f_00004-3-0 loss: 0.871706  [   32/  306]
train() client id: f_00004-3-1 loss: 0.922845  [   64/  306]
train() client id: f_00004-3-2 loss: 0.782450  [   96/  306]
train() client id: f_00004-3-3 loss: 0.783155  [  128/  306]
train() client id: f_00004-3-4 loss: 0.671358  [  160/  306]
train() client id: f_00004-3-5 loss: 0.806866  [  192/  306]
train() client id: f_00004-3-6 loss: 0.830277  [  224/  306]
train() client id: f_00004-3-7 loss: 0.714958  [  256/  306]
train() client id: f_00004-3-8 loss: 0.861758  [  288/  306]
train() client id: f_00004-4-0 loss: 0.793083  [   32/  306]
train() client id: f_00004-4-1 loss: 0.891954  [   64/  306]
train() client id: f_00004-4-2 loss: 0.812681  [   96/  306]
train() client id: f_00004-4-3 loss: 0.836292  [  128/  306]
train() client id: f_00004-4-4 loss: 0.780104  [  160/  306]
train() client id: f_00004-4-5 loss: 0.746032  [  192/  306]
train() client id: f_00004-4-6 loss: 0.776359  [  224/  306]
train() client id: f_00004-4-7 loss: 0.835057  [  256/  306]
train() client id: f_00004-4-8 loss: 0.859041  [  288/  306]
train() client id: f_00004-5-0 loss: 0.968948  [   32/  306]
train() client id: f_00004-5-1 loss: 0.845588  [   64/  306]
train() client id: f_00004-5-2 loss: 0.765543  [   96/  306]
train() client id: f_00004-5-3 loss: 0.725057  [  128/  306]
train() client id: f_00004-5-4 loss: 0.745965  [  160/  306]
train() client id: f_00004-5-5 loss: 0.868093  [  192/  306]
train() client id: f_00004-5-6 loss: 0.920618  [  224/  306]
train() client id: f_00004-5-7 loss: 0.800621  [  256/  306]
train() client id: f_00004-5-8 loss: 0.758797  [  288/  306]
train() client id: f_00004-6-0 loss: 0.815100  [   32/  306]
train() client id: f_00004-6-1 loss: 0.844629  [   64/  306]
train() client id: f_00004-6-2 loss: 0.819503  [   96/  306]
train() client id: f_00004-6-3 loss: 0.990970  [  128/  306]
train() client id: f_00004-6-4 loss: 0.732037  [  160/  306]
train() client id: f_00004-6-5 loss: 0.797665  [  192/  306]
train() client id: f_00004-6-6 loss: 0.754712  [  224/  306]
train() client id: f_00004-6-7 loss: 0.867179  [  256/  306]
train() client id: f_00004-6-8 loss: 0.865476  [  288/  306]
train() client id: f_00004-7-0 loss: 0.839143  [   32/  306]
train() client id: f_00004-7-1 loss: 0.940016  [   64/  306]
train() client id: f_00004-7-2 loss: 0.704152  [   96/  306]
train() client id: f_00004-7-3 loss: 0.867286  [  128/  306]
train() client id: f_00004-7-4 loss: 0.759332  [  160/  306]
train() client id: f_00004-7-5 loss: 0.866205  [  192/  306]
train() client id: f_00004-7-6 loss: 0.810416  [  224/  306]
train() client id: f_00004-7-7 loss: 0.846834  [  256/  306]
train() client id: f_00004-7-8 loss: 0.820266  [  288/  306]
train() client id: f_00004-8-0 loss: 0.844115  [   32/  306]
train() client id: f_00004-8-1 loss: 0.820708  [   64/  306]
train() client id: f_00004-8-2 loss: 0.868692  [   96/  306]
train() client id: f_00004-8-3 loss: 0.824039  [  128/  306]
train() client id: f_00004-8-4 loss: 0.781166  [  160/  306]
train() client id: f_00004-8-5 loss: 0.805372  [  192/  306]
train() client id: f_00004-8-6 loss: 0.937704  [  224/  306]
train() client id: f_00004-8-7 loss: 0.784817  [  256/  306]
train() client id: f_00004-8-8 loss: 0.733117  [  288/  306]
train() client id: f_00004-9-0 loss: 0.852719  [   32/  306]
train() client id: f_00004-9-1 loss: 0.785335  [   64/  306]
train() client id: f_00004-9-2 loss: 0.871005  [   96/  306]
train() client id: f_00004-9-3 loss: 0.988746  [  128/  306]
train() client id: f_00004-9-4 loss: 0.836822  [  160/  306]
train() client id: f_00004-9-5 loss: 0.795891  [  192/  306]
train() client id: f_00004-9-6 loss: 0.839923  [  224/  306]
train() client id: f_00004-9-7 loss: 0.825534  [  256/  306]
train() client id: f_00004-9-8 loss: 0.723508  [  288/  306]
train() client id: f_00004-10-0 loss: 0.769269  [   32/  306]
train() client id: f_00004-10-1 loss: 0.762591  [   64/  306]
train() client id: f_00004-10-2 loss: 0.939445  [   96/  306]
train() client id: f_00004-10-3 loss: 0.877277  [  128/  306]
train() client id: f_00004-10-4 loss: 0.813073  [  160/  306]
train() client id: f_00004-10-5 loss: 0.810915  [  192/  306]
train() client id: f_00004-10-6 loss: 0.807100  [  224/  306]
train() client id: f_00004-10-7 loss: 0.791129  [  256/  306]
train() client id: f_00004-10-8 loss: 0.877339  [  288/  306]
train() client id: f_00004-11-0 loss: 0.815493  [   32/  306]
train() client id: f_00004-11-1 loss: 0.830740  [   64/  306]
train() client id: f_00004-11-2 loss: 0.867918  [   96/  306]
train() client id: f_00004-11-3 loss: 0.926504  [  128/  306]
train() client id: f_00004-11-4 loss: 0.835912  [  160/  306]
train() client id: f_00004-11-5 loss: 0.885611  [  192/  306]
train() client id: f_00004-11-6 loss: 0.769925  [  224/  306]
train() client id: f_00004-11-7 loss: 0.783348  [  256/  306]
train() client id: f_00004-11-8 loss: 0.788229  [  288/  306]
train() client id: f_00005-0-0 loss: 1.011220  [   32/  146]
train() client id: f_00005-0-1 loss: 0.731655  [   64/  146]
train() client id: f_00005-0-2 loss: 0.591897  [   96/  146]
train() client id: f_00005-0-3 loss: 0.942008  [  128/  146]
train() client id: f_00005-1-0 loss: 1.006306  [   32/  146]
train() client id: f_00005-1-1 loss: 0.939517  [   64/  146]
train() client id: f_00005-1-2 loss: 0.598713  [   96/  146]
train() client id: f_00005-1-3 loss: 0.723730  [  128/  146]
train() client id: f_00005-2-0 loss: 0.692804  [   32/  146]
train() client id: f_00005-2-1 loss: 0.729198  [   64/  146]
train() client id: f_00005-2-2 loss: 0.770802  [   96/  146]
train() client id: f_00005-2-3 loss: 1.007299  [  128/  146]
train() client id: f_00005-3-0 loss: 1.001532  [   32/  146]
train() client id: f_00005-3-1 loss: 0.666382  [   64/  146]
train() client id: f_00005-3-2 loss: 0.577811  [   96/  146]
train() client id: f_00005-3-3 loss: 0.859431  [  128/  146]
train() client id: f_00005-4-0 loss: 0.906881  [   32/  146]
train() client id: f_00005-4-1 loss: 0.966968  [   64/  146]
train() client id: f_00005-4-2 loss: 0.841732  [   96/  146]
train() client id: f_00005-4-3 loss: 0.671927  [  128/  146]
train() client id: f_00005-5-0 loss: 0.665092  [   32/  146]
train() client id: f_00005-5-1 loss: 1.183638  [   64/  146]
train() client id: f_00005-5-2 loss: 0.861773  [   96/  146]
train() client id: f_00005-5-3 loss: 0.741699  [  128/  146]
train() client id: f_00005-6-0 loss: 0.679924  [   32/  146]
train() client id: f_00005-6-1 loss: 0.807555  [   64/  146]
train() client id: f_00005-6-2 loss: 0.737854  [   96/  146]
train() client id: f_00005-6-3 loss: 0.948615  [  128/  146]
train() client id: f_00005-7-0 loss: 0.623223  [   32/  146]
train() client id: f_00005-7-1 loss: 0.847971  [   64/  146]
train() client id: f_00005-7-2 loss: 0.956288  [   96/  146]
train() client id: f_00005-7-3 loss: 0.895522  [  128/  146]
train() client id: f_00005-8-0 loss: 1.027484  [   32/  146]
train() client id: f_00005-8-1 loss: 0.928703  [   64/  146]
train() client id: f_00005-8-2 loss: 0.616023  [   96/  146]
train() client id: f_00005-8-3 loss: 0.674687  [  128/  146]
train() client id: f_00005-9-0 loss: 0.986632  [   32/  146]
train() client id: f_00005-9-1 loss: 0.759330  [   64/  146]
train() client id: f_00005-9-2 loss: 0.943651  [   96/  146]
train() client id: f_00005-9-3 loss: 0.683411  [  128/  146]
train() client id: f_00005-10-0 loss: 0.945929  [   32/  146]
train() client id: f_00005-10-1 loss: 0.733212  [   64/  146]
train() client id: f_00005-10-2 loss: 0.674702  [   96/  146]
train() client id: f_00005-10-3 loss: 1.081635  [  128/  146]
train() client id: f_00005-11-0 loss: 0.983128  [   32/  146]
train() client id: f_00005-11-1 loss: 1.171694  [   64/  146]
train() client id: f_00005-11-2 loss: 0.670530  [   96/  146]
train() client id: f_00005-11-3 loss: 0.643523  [  128/  146]
train() client id: f_00006-0-0 loss: 0.533541  [   32/   54]
train() client id: f_00006-1-0 loss: 0.546522  [   32/   54]
train() client id: f_00006-2-0 loss: 0.603216  [   32/   54]
train() client id: f_00006-3-0 loss: 0.605268  [   32/   54]
train() client id: f_00006-4-0 loss: 0.591633  [   32/   54]
train() client id: f_00006-5-0 loss: 0.571095  [   32/   54]
train() client id: f_00006-6-0 loss: 0.526147  [   32/   54]
train() client id: f_00006-7-0 loss: 0.583367  [   32/   54]
train() client id: f_00006-8-0 loss: 0.548320  [   32/   54]
train() client id: f_00006-9-0 loss: 0.538916  [   32/   54]
train() client id: f_00006-10-0 loss: 0.545452  [   32/   54]
train() client id: f_00006-11-0 loss: 0.594743  [   32/   54]
train() client id: f_00007-0-0 loss: 0.583695  [   32/  179]
train() client id: f_00007-0-1 loss: 0.403038  [   64/  179]
train() client id: f_00007-0-2 loss: 0.589200  [   96/  179]
train() client id: f_00007-0-3 loss: 0.444802  [  128/  179]
train() client id: f_00007-0-4 loss: 0.466282  [  160/  179]
train() client id: f_00007-1-0 loss: 0.501383  [   32/  179]
train() client id: f_00007-1-1 loss: 0.617990  [   64/  179]
train() client id: f_00007-1-2 loss: 0.539097  [   96/  179]
train() client id: f_00007-1-3 loss: 0.308935  [  128/  179]
train() client id: f_00007-1-4 loss: 0.529623  [  160/  179]
train() client id: f_00007-2-0 loss: 0.707255  [   32/  179]
train() client id: f_00007-2-1 loss: 0.358816  [   64/  179]
train() client id: f_00007-2-2 loss: 0.454258  [   96/  179]
train() client id: f_00007-2-3 loss: 0.568511  [  128/  179]
train() client id: f_00007-2-4 loss: 0.352946  [  160/  179]
train() client id: f_00007-3-0 loss: 0.373245  [   32/  179]
train() client id: f_00007-3-1 loss: 0.370938  [   64/  179]
train() client id: f_00007-3-2 loss: 0.316185  [   96/  179]
train() client id: f_00007-3-3 loss: 0.677584  [  128/  179]
train() client id: f_00007-3-4 loss: 0.414195  [  160/  179]
train() client id: f_00007-4-0 loss: 0.615529  [   32/  179]
train() client id: f_00007-4-1 loss: 0.353820  [   64/  179]
train() client id: f_00007-4-2 loss: 0.312838  [   96/  179]
train() client id: f_00007-4-3 loss: 0.405484  [  128/  179]
train() client id: f_00007-4-4 loss: 0.506629  [  160/  179]
train() client id: f_00007-5-0 loss: 0.429678  [   32/  179]
train() client id: f_00007-5-1 loss: 0.409597  [   64/  179]
train() client id: f_00007-5-2 loss: 0.473293  [   96/  179]
train() client id: f_00007-5-3 loss: 0.691017  [  128/  179]
train() client id: f_00007-5-4 loss: 0.317385  [  160/  179]
train() client id: f_00007-6-0 loss: 0.396190  [   32/  179]
train() client id: f_00007-6-1 loss: 0.383676  [   64/  179]
train() client id: f_00007-6-2 loss: 0.539506  [   96/  179]
train() client id: f_00007-6-3 loss: 0.336178  [  128/  179]
train() client id: f_00007-6-4 loss: 0.575170  [  160/  179]
train() client id: f_00007-7-0 loss: 0.303572  [   32/  179]
train() client id: f_00007-7-1 loss: 0.364167  [   64/  179]
train() client id: f_00007-7-2 loss: 0.443816  [   96/  179]
train() client id: f_00007-7-3 loss: 0.630316  [  128/  179]
train() client id: f_00007-7-4 loss: 0.452951  [  160/  179]
train() client id: f_00007-8-0 loss: 0.603393  [   32/  179]
train() client id: f_00007-8-1 loss: 0.442057  [   64/  179]
train() client id: f_00007-8-2 loss: 0.499642  [   96/  179]
train() client id: f_00007-8-3 loss: 0.272961  [  128/  179]
train() client id: f_00007-8-4 loss: 0.412097  [  160/  179]
train() client id: f_00007-9-0 loss: 0.302298  [   32/  179]
train() client id: f_00007-9-1 loss: 0.353037  [   64/  179]
train() client id: f_00007-9-2 loss: 0.443091  [   96/  179]
train() client id: f_00007-9-3 loss: 0.419021  [  128/  179]
train() client id: f_00007-9-4 loss: 0.580418  [  160/  179]
train() client id: f_00007-10-0 loss: 0.399624  [   32/  179]
train() client id: f_00007-10-1 loss: 0.657880  [   64/  179]
train() client id: f_00007-10-2 loss: 0.343274  [   96/  179]
train() client id: f_00007-10-3 loss: 0.263834  [  128/  179]
train() client id: f_00007-10-4 loss: 0.455033  [  160/  179]
train() client id: f_00007-11-0 loss: 0.379489  [   32/  179]
train() client id: f_00007-11-1 loss: 0.638492  [   64/  179]
train() client id: f_00007-11-2 loss: 0.340441  [   96/  179]
train() client id: f_00007-11-3 loss: 0.376222  [  128/  179]
train() client id: f_00007-11-4 loss: 0.468064  [  160/  179]
train() client id: f_00008-0-0 loss: 0.802820  [   32/  130]
train() client id: f_00008-0-1 loss: 0.731156  [   64/  130]
train() client id: f_00008-0-2 loss: 0.642961  [   96/  130]
train() client id: f_00008-0-3 loss: 0.684079  [  128/  130]
train() client id: f_00008-1-0 loss: 0.770067  [   32/  130]
train() client id: f_00008-1-1 loss: 0.675863  [   64/  130]
train() client id: f_00008-1-2 loss: 0.700372  [   96/  130]
train() client id: f_00008-1-3 loss: 0.758986  [  128/  130]
train() client id: f_00008-2-0 loss: 0.704859  [   32/  130]
train() client id: f_00008-2-1 loss: 0.780895  [   64/  130]
train() client id: f_00008-2-2 loss: 0.704138  [   96/  130]
train() client id: f_00008-2-3 loss: 0.681024  [  128/  130]
train() client id: f_00008-3-0 loss: 0.785688  [   32/  130]
train() client id: f_00008-3-1 loss: 0.665184  [   64/  130]
train() client id: f_00008-3-2 loss: 0.660398  [   96/  130]
train() client id: f_00008-3-3 loss: 0.789901  [  128/  130]
train() client id: f_00008-4-0 loss: 0.712731  [   32/  130]
train() client id: f_00008-4-1 loss: 0.732660  [   64/  130]
train() client id: f_00008-4-2 loss: 0.713005  [   96/  130]
train() client id: f_00008-4-3 loss: 0.714068  [  128/  130]
train() client id: f_00008-5-0 loss: 0.788148  [   32/  130]
train() client id: f_00008-5-1 loss: 0.792694  [   64/  130]
train() client id: f_00008-5-2 loss: 0.672442  [   96/  130]
train() client id: f_00008-5-3 loss: 0.639297  [  128/  130]
train() client id: f_00008-6-0 loss: 0.724084  [   32/  130]
train() client id: f_00008-6-1 loss: 0.745971  [   64/  130]
train() client id: f_00008-6-2 loss: 0.807093  [   96/  130]
train() client id: f_00008-6-3 loss: 0.590803  [  128/  130]
train() client id: f_00008-7-0 loss: 0.741640  [   32/  130]
train() client id: f_00008-7-1 loss: 0.825516  [   64/  130]
train() client id: f_00008-7-2 loss: 0.674573  [   96/  130]
train() client id: f_00008-7-3 loss: 0.632371  [  128/  130]
train() client id: f_00008-8-0 loss: 0.762086  [   32/  130]
train() client id: f_00008-8-1 loss: 0.694404  [   64/  130]
train() client id: f_00008-8-2 loss: 0.727141  [   96/  130]
train() client id: f_00008-8-3 loss: 0.715180  [  128/  130]
train() client id: f_00008-9-0 loss: 0.758401  [   32/  130]
train() client id: f_00008-9-1 loss: 0.684575  [   64/  130]
train() client id: f_00008-9-2 loss: 0.680833  [   96/  130]
train() client id: f_00008-9-3 loss: 0.730889  [  128/  130]
train() client id: f_00008-10-0 loss: 0.768078  [   32/  130]
train() client id: f_00008-10-1 loss: 0.725673  [   64/  130]
train() client id: f_00008-10-2 loss: 0.740125  [   96/  130]
train() client id: f_00008-10-3 loss: 0.665468  [  128/  130]
train() client id: f_00008-11-0 loss: 0.730962  [   32/  130]
train() client id: f_00008-11-1 loss: 0.813623  [   64/  130]
train() client id: f_00008-11-2 loss: 0.607008  [   96/  130]
train() client id: f_00008-11-3 loss: 0.755761  [  128/  130]
train() client id: f_00009-0-0 loss: 1.156801  [   32/  118]
train() client id: f_00009-0-1 loss: 1.216853  [   64/  118]
train() client id: f_00009-0-2 loss: 1.095947  [   96/  118]
train() client id: f_00009-1-0 loss: 1.200268  [   32/  118]
train() client id: f_00009-1-1 loss: 1.046277  [   64/  118]
train() client id: f_00009-1-2 loss: 1.026962  [   96/  118]
train() client id: f_00009-2-0 loss: 1.145654  [   32/  118]
train() client id: f_00009-2-1 loss: 1.047658  [   64/  118]
train() client id: f_00009-2-2 loss: 0.915999  [   96/  118]
train() client id: f_00009-3-0 loss: 1.033504  [   32/  118]
train() client id: f_00009-3-1 loss: 0.966784  [   64/  118]
train() client id: f_00009-3-2 loss: 1.053602  [   96/  118]
train() client id: f_00009-4-0 loss: 0.997659  [   32/  118]
train() client id: f_00009-4-1 loss: 0.988413  [   64/  118]
train() client id: f_00009-4-2 loss: 1.001054  [   96/  118]
train() client id: f_00009-5-0 loss: 0.912727  [   32/  118]
train() client id: f_00009-5-1 loss: 1.019253  [   64/  118]
train() client id: f_00009-5-2 loss: 0.976504  [   96/  118]
train() client id: f_00009-6-0 loss: 0.938882  [   32/  118]
train() client id: f_00009-6-1 loss: 1.101184  [   64/  118]
train() client id: f_00009-6-2 loss: 0.848242  [   96/  118]
train() client id: f_00009-7-0 loss: 0.976198  [   32/  118]
train() client id: f_00009-7-1 loss: 1.057195  [   64/  118]
train() client id: f_00009-7-2 loss: 0.925055  [   96/  118]
train() client id: f_00009-8-0 loss: 0.882508  [   32/  118]
train() client id: f_00009-8-1 loss: 0.902445  [   64/  118]
train() client id: f_00009-8-2 loss: 1.054992  [   96/  118]
train() client id: f_00009-9-0 loss: 0.940313  [   32/  118]
train() client id: f_00009-9-1 loss: 0.899615  [   64/  118]
train() client id: f_00009-9-2 loss: 0.839418  [   96/  118]
train() client id: f_00009-10-0 loss: 0.923229  [   32/  118]
train() client id: f_00009-10-1 loss: 0.972797  [   64/  118]
train() client id: f_00009-10-2 loss: 0.943449  [   96/  118]
train() client id: f_00009-11-0 loss: 0.971474  [   32/  118]
train() client id: f_00009-11-1 loss: 0.800631  [   64/  118]
train() client id: f_00009-11-2 loss: 0.965004  [   96/  118]
At round 35 accuracy: 0.649867374005305
At round 35 training accuracy: 0.5801475519785378
At round 35 training loss: 0.8387152419601901
update_location
xs = -3.905658 4.200318 195.009024 18.811294 0.979296 3.956410 -157.443192 -136.324852 179.663977 -122.060879 
ys = 187.587959 170.555839 1.320614 -157.455176 149.350187 132.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 212.613491 197.754739 219.158079 187.472657 179.739916 166.298683 186.534847 169.071409 206.368153 157.844449 
dists_bs = 171.556785 179.255417 408.743449 384.823676 177.838763 183.861476 178.490550 178.419790 388.034862 179.005709 
uav_gains = -108.492625 -107.540848 -108.932389 -106.897913 -106.411197 -105.539480 -106.839197 -105.722999 -108.086562 -104.964595 
bs_gains = -102.130480 -102.664283 -112.687682 -111.954389 -102.567799 -102.972800 -102.612286 -102.607464 -112.055440 -102.647332 
Round 36
-------------------------------
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.2394703  12.90983275  6.14725815  2.21746141 14.88914632  7.16517573
  2.74816098  8.77577622  6.47605413  5.8113634 ]
obj_prev = 73.37969937876653
eta_min = 1.5469793420595954e-15	eta_max = 0.9294112566599255
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 17.025573465777978	eta = 0.9090909090909091
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 31.275397678211032	eta = 0.4948872023642162
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 24.26289815788516	eta = 0.6379202500492736
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 22.99478274194065	eta = 0.6731002520657826
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 22.927768157082628	eta = 0.6750676277672019
af = 15.47779405979816	bf = 1.3680266359327833	zeta = 22.927565477060664	eta = 0.6750735953751348
eta = 0.6750735953751348
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [0.03242394 0.0681932  0.03190927 0.01106531 0.07874384 0.03757059
 0.01389597 0.04606258 0.03345327 0.03036527]
ene_total = [2.05444491 3.67229692 2.04263567 0.969603   4.18544742 2.18345709
 1.10625446 2.64978349 2.23698535 1.82665715]
ti_comp = [0.4811663  0.50721776 0.47844041 0.48996025 0.50753423 0.50618685
 0.49025487 0.4955151  0.45404092 0.50727357]
ti_coms = [0.09689836 0.07084691 0.09962426 0.08810442 0.07053044 0.07187782
 0.08780979 0.08254957 0.12402375 0.0707911 ]
t_total = [28.19984894 28.19984894 28.19984894 28.19984894 28.19984894 28.19984894
 28.19984894 28.19984894 28.19984894 28.19984894]
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [9.20209687e-06 7.70396484e-05 8.87104805e-06 3.52735562e-07
 1.18467487e-04 1.29360722e-05 6.97755096e-07 2.48777379e-05
 1.13502559e-05 6.80028536e-06]
ene_total = [0.47315021 0.34937218 0.48643187 0.42981882 0.34984932 0.35127413
 0.42839839 0.40391691 0.60558136 0.34567345]
optimize_network iter = 0 obj = 4.223466627519707
eta = 0.6750735953751348
freqs = [33693065.79010019 67222802.04491435 33347173.22625211 11292050.13707799
 77574909.50601687 37111387.37372465 14172192.11427163 46479487.54198894
 36839485.23777357 29929879.4291095 ]
eta_min = 0.6750735953751431	eta_max = 0.675073595375134
af = 0.01091581578850546	bf = 1.3680266359327833	zeta = 0.012007397367356006	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [2.23206564e-06 1.86867792e-05 2.15176626e-06 8.55597305e-08
 2.87355384e-05 3.13778074e-06 1.69247857e-07 6.03435770e-06
 2.75312427e-06 1.64948093e-06]
ene_total = [1.68991368 1.23854826 1.73742856 1.53620773 1.23478239 1.25381221
 1.53108527 1.44039041 2.16296411 1.23460465]
ti_comp = [0.4811663  0.50721776 0.47844041 0.48996025 0.50753423 0.50618685
 0.49025487 0.4955151  0.45404092 0.50727357]
ti_coms = [0.09689836 0.07084691 0.09962426 0.08810442 0.07053044 0.07187782
 0.08780979 0.08254957 0.12402375 0.0707911 ]
t_total = [28.19984894 28.19984894 28.19984894 28.19984894 28.19984894 28.19984894
 28.19984894 28.19984894 28.19984894 28.19984894]
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [9.20209687e-06 7.70396484e-05 8.87104805e-06 3.52735562e-07
 1.18467487e-04 1.29360722e-05 6.97755096e-07 2.48777379e-05
 1.13502559e-05 6.80028536e-06]
ene_total = [0.47315021 0.34937218 0.48643187 0.42981882 0.34984932 0.35127413
 0.42839839 0.40391691 0.60558136 0.34567345]
optimize_network iter = 1 obj = 4.223466627519696
eta = 0.675073595375134
freqs = [33693065.7901002  67222802.04491436 33347173.22625211 11292050.13707799
 77574909.50601688 37111387.37372466 14172192.11427163 46479487.54198895
 36839485.23777357 29929879.42910951]
Done!
ene_coms = [0.00968984 0.00708469 0.00996243 0.00881044 0.00705304 0.00718778
 0.00878098 0.00825496 0.01240238 0.00707911]
ene_comp = [8.58228348e-06 7.18505913e-05 8.27353268e-06 3.28976822e-07
 1.10488030e-04 1.20647544e-05 6.50757335e-07 2.32020813e-05
 1.05857518e-05 6.34224759e-06]
ene_total = [0.00969842 0.00715654 0.0099707  0.00881077 0.00716353 0.00719985
 0.00878163 0.00827816 0.01241296 0.00708545]
At round 36 energy consumption: 0.08655801199959033
At round 36 eta: 0.675073595375134
At round 36 a_n: 15.850952365358953
At round 36 local rounds: 12.866641222936334
At round 36 global rounds: 48.78320795030245
gradient difference: 0.39134451746940613
train() client id: f_00000-0-0 loss: 1.313797  [   32/  126]
train() client id: f_00000-0-1 loss: 1.042644  [   64/  126]
train() client id: f_00000-0-2 loss: 1.290097  [   96/  126]
train() client id: f_00000-1-0 loss: 1.086850  [   32/  126]
train() client id: f_00000-1-1 loss: 1.118309  [   64/  126]
train() client id: f_00000-1-2 loss: 1.154299  [   96/  126]
train() client id: f_00000-2-0 loss: 1.031566  [   32/  126]
train() client id: f_00000-2-1 loss: 1.005449  [   64/  126]
train() client id: f_00000-2-2 loss: 1.055093  [   96/  126]
train() client id: f_00000-3-0 loss: 1.062808  [   32/  126]
train() client id: f_00000-3-1 loss: 0.865983  [   64/  126]
train() client id: f_00000-3-2 loss: 0.981595  [   96/  126]
train() client id: f_00000-4-0 loss: 0.959635  [   32/  126]
train() client id: f_00000-4-1 loss: 0.939822  [   64/  126]
train() client id: f_00000-4-2 loss: 0.927945  [   96/  126]
train() client id: f_00000-5-0 loss: 0.987339  [   32/  126]
train() client id: f_00000-5-1 loss: 0.924611  [   64/  126]
train() client id: f_00000-5-2 loss: 0.948989  [   96/  126]
train() client id: f_00000-6-0 loss: 0.956590  [   32/  126]
train() client id: f_00000-6-1 loss: 0.785866  [   64/  126]
train() client id: f_00000-6-2 loss: 0.974110  [   96/  126]
train() client id: f_00000-7-0 loss: 0.863027  [   32/  126]
train() client id: f_00000-7-1 loss: 1.042903  [   64/  126]
train() client id: f_00000-7-2 loss: 0.843855  [   96/  126]
train() client id: f_00000-8-0 loss: 0.956129  [   32/  126]
train() client id: f_00000-8-1 loss: 0.904910  [   64/  126]
train() client id: f_00000-8-2 loss: 0.960922  [   96/  126]
train() client id: f_00000-9-0 loss: 1.025494  [   32/  126]
train() client id: f_00000-9-1 loss: 0.887924  [   64/  126]
train() client id: f_00000-9-2 loss: 0.917053  [   96/  126]
train() client id: f_00000-10-0 loss: 0.878191  [   32/  126]
train() client id: f_00000-10-1 loss: 1.144956  [   64/  126]
train() client id: f_00000-10-2 loss: 0.823288  [   96/  126]
train() client id: f_00000-11-0 loss: 0.848868  [   32/  126]
train() client id: f_00000-11-1 loss: 0.901959  [   64/  126]
train() client id: f_00000-11-2 loss: 1.019259  [   96/  126]
train() client id: f_00001-0-0 loss: 0.283956  [   32/  265]
train() client id: f_00001-0-1 loss: 0.282834  [   64/  265]
train() client id: f_00001-0-2 loss: 0.257020  [   96/  265]
train() client id: f_00001-0-3 loss: 0.373464  [  128/  265]
train() client id: f_00001-0-4 loss: 0.266956  [  160/  265]
train() client id: f_00001-0-5 loss: 0.277286  [  192/  265]
train() client id: f_00001-0-6 loss: 0.260250  [  224/  265]
train() client id: f_00001-0-7 loss: 0.396680  [  256/  265]
train() client id: f_00001-1-0 loss: 0.273862  [   32/  265]
train() client id: f_00001-1-1 loss: 0.335110  [   64/  265]
train() client id: f_00001-1-2 loss: 0.299125  [   96/  265]
train() client id: f_00001-1-3 loss: 0.246354  [  128/  265]
train() client id: f_00001-1-4 loss: 0.287822  [  160/  265]
train() client id: f_00001-1-5 loss: 0.268136  [  192/  265]
train() client id: f_00001-1-6 loss: 0.195245  [  224/  265]
train() client id: f_00001-1-7 loss: 0.324308  [  256/  265]
train() client id: f_00001-2-0 loss: 0.236667  [   32/  265]
train() client id: f_00001-2-1 loss: 0.226987  [   64/  265]
train() client id: f_00001-2-2 loss: 0.349427  [   96/  265]
train() client id: f_00001-2-3 loss: 0.322372  [  128/  265]
train() client id: f_00001-2-4 loss: 0.306916  [  160/  265]
train() client id: f_00001-2-5 loss: 0.309195  [  192/  265]
train() client id: f_00001-2-6 loss: 0.184795  [  224/  265]
train() client id: f_00001-2-7 loss: 0.213887  [  256/  265]
train() client id: f_00001-3-0 loss: 0.332810  [   32/  265]
train() client id: f_00001-3-1 loss: 0.201186  [   64/  265]
train() client id: f_00001-3-2 loss: 0.314061  [   96/  265]
train() client id: f_00001-3-3 loss: 0.233255  [  128/  265]
train() client id: f_00001-3-4 loss: 0.232732  [  160/  265]
train() client id: f_00001-3-5 loss: 0.309663  [  192/  265]
train() client id: f_00001-3-6 loss: 0.226593  [  224/  265]
train() client id: f_00001-3-7 loss: 0.225409  [  256/  265]
train() client id: f_00001-4-0 loss: 0.247404  [   32/  265]
train() client id: f_00001-4-1 loss: 0.362992  [   64/  265]
train() client id: f_00001-4-2 loss: 0.177587  [   96/  265]
train() client id: f_00001-4-3 loss: 0.330326  [  128/  265]
train() client id: f_00001-4-4 loss: 0.228569  [  160/  265]
train() client id: f_00001-4-5 loss: 0.275114  [  192/  265]
train() client id: f_00001-4-6 loss: 0.313759  [  224/  265]
train() client id: f_00001-4-7 loss: 0.159939  [  256/  265]
train() client id: f_00001-5-0 loss: 0.159735  [   32/  265]
train() client id: f_00001-5-1 loss: 0.307483  [   64/  265]
train() client id: f_00001-5-2 loss: 0.200001  [   96/  265]
train() client id: f_00001-5-3 loss: 0.317559  [  128/  265]
train() client id: f_00001-5-4 loss: 0.213559  [  160/  265]
train() client id: f_00001-5-5 loss: 0.312005  [  192/  265]
train() client id: f_00001-5-6 loss: 0.183428  [  224/  265]
train() client id: f_00001-5-7 loss: 0.309370  [  256/  265]
train() client id: f_00001-6-0 loss: 0.393441  [   32/  265]
train() client id: f_00001-6-1 loss: 0.183076  [   64/  265]
train() client id: f_00001-6-2 loss: 0.292132  [   96/  265]
train() client id: f_00001-6-3 loss: 0.143639  [  128/  265]
train() client id: f_00001-6-4 loss: 0.279249  [  160/  265]
train() client id: f_00001-6-5 loss: 0.220486  [  192/  265]
train() client id: f_00001-6-6 loss: 0.261925  [  224/  265]
train() client id: f_00001-6-7 loss: 0.210306  [  256/  265]
train() client id: f_00001-7-0 loss: 0.177728  [   32/  265]
train() client id: f_00001-7-1 loss: 0.343104  [   64/  265]
train() client id: f_00001-7-2 loss: 0.197672  [   96/  265]
train() client id: f_00001-7-3 loss: 0.389180  [  128/  265]
train() client id: f_00001-7-4 loss: 0.202715  [  160/  265]
train() client id: f_00001-7-5 loss: 0.362333  [  192/  265]
train() client id: f_00001-7-6 loss: 0.163221  [  224/  265]
train() client id: f_00001-7-7 loss: 0.142983  [  256/  265]
train() client id: f_00001-8-0 loss: 0.269347  [   32/  265]
train() client id: f_00001-8-1 loss: 0.281036  [   64/  265]
train() client id: f_00001-8-2 loss: 0.156225  [   96/  265]
train() client id: f_00001-8-3 loss: 0.293454  [  128/  265]
train() client id: f_00001-8-4 loss: 0.291698  [  160/  265]
train() client id: f_00001-8-5 loss: 0.297819  [  192/  265]
train() client id: f_00001-8-6 loss: 0.148951  [  224/  265]
train() client id: f_00001-8-7 loss: 0.145907  [  256/  265]
train() client id: f_00001-9-0 loss: 0.198323  [   32/  265]
train() client id: f_00001-9-1 loss: 0.240392  [   64/  265]
train() client id: f_00001-9-2 loss: 0.301652  [   96/  265]
train() client id: f_00001-9-3 loss: 0.223945  [  128/  265]
train() client id: f_00001-9-4 loss: 0.293443  [  160/  265]
train() client id: f_00001-9-5 loss: 0.279534  [  192/  265]
train() client id: f_00001-9-6 loss: 0.228991  [  224/  265]
train() client id: f_00001-9-7 loss: 0.143711  [  256/  265]
train() client id: f_00001-10-0 loss: 0.405835  [   32/  265]
train() client id: f_00001-10-1 loss: 0.163782  [   64/  265]
train() client id: f_00001-10-2 loss: 0.315136  [   96/  265]
train() client id: f_00001-10-3 loss: 0.223099  [  128/  265]
train() client id: f_00001-10-4 loss: 0.279462  [  160/  265]
train() client id: f_00001-10-5 loss: 0.163745  [  192/  265]
train() client id: f_00001-10-6 loss: 0.150890  [  224/  265]
train() client id: f_00001-10-7 loss: 0.234436  [  256/  265]
train() client id: f_00001-11-0 loss: 0.272686  [   32/  265]
train() client id: f_00001-11-1 loss: 0.201414  [   64/  265]
train() client id: f_00001-11-2 loss: 0.234207  [   96/  265]
train() client id: f_00001-11-3 loss: 0.260182  [  128/  265]
train() client id: f_00001-11-4 loss: 0.346399  [  160/  265]
train() client id: f_00001-11-5 loss: 0.211915  [  192/  265]
train() client id: f_00001-11-6 loss: 0.193610  [  224/  265]
train() client id: f_00001-11-7 loss: 0.140797  [  256/  265]
train() client id: f_00002-0-0 loss: 1.297029  [   32/  124]
train() client id: f_00002-0-1 loss: 1.106836  [   64/  124]
train() client id: f_00002-0-2 loss: 1.253358  [   96/  124]
train() client id: f_00002-1-0 loss: 1.090300  [   32/  124]
train() client id: f_00002-1-1 loss: 1.141831  [   64/  124]
train() client id: f_00002-1-2 loss: 1.307492  [   96/  124]
train() client id: f_00002-2-0 loss: 1.175485  [   32/  124]
train() client id: f_00002-2-1 loss: 1.009844  [   64/  124]
train() client id: f_00002-2-2 loss: 1.087760  [   96/  124]
train() client id: f_00002-3-0 loss: 1.219867  [   32/  124]
train() client id: f_00002-3-1 loss: 1.015180  [   64/  124]
train() client id: f_00002-3-2 loss: 1.044214  [   96/  124]
train() client id: f_00002-4-0 loss: 1.055753  [   32/  124]
train() client id: f_00002-4-1 loss: 1.082052  [   64/  124]
train() client id: f_00002-4-2 loss: 1.056382  [   96/  124]
train() client id: f_00002-5-0 loss: 0.978233  [   32/  124]
train() client id: f_00002-5-1 loss: 1.167763  [   64/  124]
train() client id: f_00002-5-2 loss: 1.032805  [   96/  124]
train() client id: f_00002-6-0 loss: 1.042289  [   32/  124]
train() client id: f_00002-6-1 loss: 0.907858  [   64/  124]
train() client id: f_00002-6-2 loss: 0.989854  [   96/  124]
train() client id: f_00002-7-0 loss: 0.912660  [   32/  124]
train() client id: f_00002-7-1 loss: 0.961049  [   64/  124]
train() client id: f_00002-7-2 loss: 0.926675  [   96/  124]
train() client id: f_00002-8-0 loss: 1.033352  [   32/  124]
train() client id: f_00002-8-1 loss: 0.955773  [   64/  124]
train() client id: f_00002-8-2 loss: 0.915748  [   96/  124]
train() client id: f_00002-9-0 loss: 0.907354  [   32/  124]
train() client id: f_00002-9-1 loss: 0.879317  [   64/  124]
train() client id: f_00002-9-2 loss: 1.067379  [   96/  124]
train() client id: f_00002-10-0 loss: 1.076114  [   32/  124]
train() client id: f_00002-10-1 loss: 0.920987  [   64/  124]
train() client id: f_00002-10-2 loss: 1.044437  [   96/  124]
train() client id: f_00002-11-0 loss: 1.067147  [   32/  124]
train() client id: f_00002-11-1 loss: 0.803954  [   64/  124]
train() client id: f_00002-11-2 loss: 0.838149  [   96/  124]
train() client id: f_00003-0-0 loss: 0.778458  [   32/   43]
train() client id: f_00003-1-0 loss: 0.880544  [   32/   43]
train() client id: f_00003-2-0 loss: 0.803506  [   32/   43]
train() client id: f_00003-3-0 loss: 0.585357  [   32/   43]
train() client id: f_00003-4-0 loss: 0.607663  [   32/   43]
train() client id: f_00003-5-0 loss: 0.619160  [   32/   43]
train() client id: f_00003-6-0 loss: 0.618779  [   32/   43]
train() client id: f_00003-7-0 loss: 0.727048  [   32/   43]
train() client id: f_00003-8-0 loss: 0.591016  [   32/   43]
train() client id: f_00003-9-0 loss: 0.625993  [   32/   43]
train() client id: f_00003-10-0 loss: 0.613245  [   32/   43]
train() client id: f_00003-11-0 loss: 0.628130  [   32/   43]
train() client id: f_00004-0-0 loss: 1.081505  [   32/  306]
train() client id: f_00004-0-1 loss: 0.939644  [   64/  306]
train() client id: f_00004-0-2 loss: 0.827824  [   96/  306]
train() client id: f_00004-0-3 loss: 0.997276  [  128/  306]
train() client id: f_00004-0-4 loss: 0.940668  [  160/  306]
train() client id: f_00004-0-5 loss: 1.002490  [  192/  306]
train() client id: f_00004-0-6 loss: 0.794967  [  224/  306]
train() client id: f_00004-0-7 loss: 0.879108  [  256/  306]
train() client id: f_00004-0-8 loss: 1.011349  [  288/  306]
train() client id: f_00004-1-0 loss: 0.919246  [   32/  306]
train() client id: f_00004-1-1 loss: 1.057683  [   64/  306]
train() client id: f_00004-1-2 loss: 0.954546  [   96/  306]
train() client id: f_00004-1-3 loss: 0.919639  [  128/  306]
train() client id: f_00004-1-4 loss: 0.932510  [  160/  306]
train() client id: f_00004-1-5 loss: 0.876572  [  192/  306]
train() client id: f_00004-1-6 loss: 0.958285  [  224/  306]
train() client id: f_00004-1-7 loss: 0.977001  [  256/  306]
train() client id: f_00004-1-8 loss: 0.934329  [  288/  306]
train() client id: f_00004-2-0 loss: 0.932923  [   32/  306]
train() client id: f_00004-2-1 loss: 0.831872  [   64/  306]
train() client id: f_00004-2-2 loss: 1.078351  [   96/  306]
train() client id: f_00004-2-3 loss: 1.002688  [  128/  306]
train() client id: f_00004-2-4 loss: 1.026237  [  160/  306]
train() client id: f_00004-2-5 loss: 0.923265  [  192/  306]
train() client id: f_00004-2-6 loss: 0.865430  [  224/  306]
train() client id: f_00004-2-7 loss: 0.996504  [  256/  306]
train() client id: f_00004-2-8 loss: 0.875254  [  288/  306]
train() client id: f_00004-3-0 loss: 0.996100  [   32/  306]
train() client id: f_00004-3-1 loss: 0.894191  [   64/  306]
train() client id: f_00004-3-2 loss: 0.954222  [   96/  306]
train() client id: f_00004-3-3 loss: 1.021010  [  128/  306]
train() client id: f_00004-3-4 loss: 0.872938  [  160/  306]
train() client id: f_00004-3-5 loss: 1.026567  [  192/  306]
train() client id: f_00004-3-6 loss: 0.872559  [  224/  306]
train() client id: f_00004-3-7 loss: 0.905447  [  256/  306]
train() client id: f_00004-3-8 loss: 0.942876  [  288/  306]
train() client id: f_00004-4-0 loss: 0.899642  [   32/  306]
train() client id: f_00004-4-1 loss: 0.998658  [   64/  306]
train() client id: f_00004-4-2 loss: 0.891241  [   96/  306]
train() client id: f_00004-4-3 loss: 0.877731  [  128/  306]
train() client id: f_00004-4-4 loss: 1.093806  [  160/  306]
train() client id: f_00004-4-5 loss: 0.916988  [  192/  306]
train() client id: f_00004-4-6 loss: 0.960434  [  224/  306]
train() client id: f_00004-4-7 loss: 0.987692  [  256/  306]
train() client id: f_00004-4-8 loss: 0.773566  [  288/  306]
train() client id: f_00004-5-0 loss: 0.941206  [   32/  306]
train() client id: f_00004-5-1 loss: 0.997463  [   64/  306]
train() client id: f_00004-5-2 loss: 1.019643  [   96/  306]
train() client id: f_00004-5-3 loss: 0.913641  [  128/  306]
train() client id: f_00004-5-4 loss: 0.931712  [  160/  306]
train() client id: f_00004-5-5 loss: 0.839449  [  192/  306]
train() client id: f_00004-5-6 loss: 0.837356  [  224/  306]
train() client id: f_00004-5-7 loss: 1.078674  [  256/  306]
train() client id: f_00004-5-8 loss: 0.898787  [  288/  306]
train() client id: f_00004-6-0 loss: 0.959949  [   32/  306]
train() client id: f_00004-6-1 loss: 0.905829  [   64/  306]
train() client id: f_00004-6-2 loss: 0.891175  [   96/  306]
train() client id: f_00004-6-3 loss: 0.874792  [  128/  306]
train() client id: f_00004-6-4 loss: 0.874672  [  160/  306]
train() client id: f_00004-6-5 loss: 1.068982  [  192/  306]
train() client id: f_00004-6-6 loss: 0.891591  [  224/  306]
train() client id: f_00004-6-7 loss: 1.066386  [  256/  306]
train() client id: f_00004-6-8 loss: 0.932498  [  288/  306]
train() client id: f_00004-7-0 loss: 0.922883  [   32/  306]
train() client id: f_00004-7-1 loss: 0.954934  [   64/  306]
train() client id: f_00004-7-2 loss: 0.823440  [   96/  306]
train() client id: f_00004-7-3 loss: 1.002491  [  128/  306]
train() client id: f_00004-7-4 loss: 0.987319  [  160/  306]
train() client id: f_00004-7-5 loss: 1.007341  [  192/  306]
train() client id: f_00004-7-6 loss: 0.934272  [  224/  306]
train() client id: f_00004-7-7 loss: 0.931427  [  256/  306]
train() client id: f_00004-7-8 loss: 0.854197  [  288/  306]
train() client id: f_00004-8-0 loss: 0.912818  [   32/  306]
train() client id: f_00004-8-1 loss: 0.919565  [   64/  306]
train() client id: f_00004-8-2 loss: 1.060510  [   96/  306]
train() client id: f_00004-8-3 loss: 0.886406  [  128/  306]
train() client id: f_00004-8-4 loss: 0.881413  [  160/  306]
train() client id: f_00004-8-5 loss: 1.022275  [  192/  306]
train() client id: f_00004-8-6 loss: 0.770744  [  224/  306]
train() client id: f_00004-8-7 loss: 0.833203  [  256/  306]
train() client id: f_00004-8-8 loss: 1.013965  [  288/  306]
train() client id: f_00004-9-0 loss: 0.914619  [   32/  306]
train() client id: f_00004-9-1 loss: 0.996647  [   64/  306]
train() client id: f_00004-9-2 loss: 0.888445  [   96/  306]
train() client id: f_00004-9-3 loss: 0.934920  [  128/  306]
train() client id: f_00004-9-4 loss: 0.924992  [  160/  306]
train() client id: f_00004-9-5 loss: 0.951349  [  192/  306]
train() client id: f_00004-9-6 loss: 0.906126  [  224/  306]
train() client id: f_00004-9-7 loss: 0.917158  [  256/  306]
train() client id: f_00004-9-8 loss: 0.897953  [  288/  306]
train() client id: f_00004-10-0 loss: 1.001771  [   32/  306]
train() client id: f_00004-10-1 loss: 0.902465  [   64/  306]
train() client id: f_00004-10-2 loss: 0.982901  [   96/  306]
train() client id: f_00004-10-3 loss: 1.016764  [  128/  306]
train() client id: f_00004-10-4 loss: 0.833876  [  160/  306]
train() client id: f_00004-10-5 loss: 0.876440  [  192/  306]
train() client id: f_00004-10-6 loss: 0.862870  [  224/  306]
train() client id: f_00004-10-7 loss: 0.863523  [  256/  306]
train() client id: f_00004-10-8 loss: 0.964968  [  288/  306]
train() client id: f_00004-11-0 loss: 0.935494  [   32/  306]
train() client id: f_00004-11-1 loss: 0.994326  [   64/  306]
train() client id: f_00004-11-2 loss: 0.874649  [   96/  306]
train() client id: f_00004-11-3 loss: 0.999883  [  128/  306]
train() client id: f_00004-11-4 loss: 0.982091  [  160/  306]
train() client id: f_00004-11-5 loss: 0.883865  [  192/  306]
train() client id: f_00004-11-6 loss: 0.871064  [  224/  306]
train() client id: f_00004-11-7 loss: 0.901296  [  256/  306]
train() client id: f_00004-11-8 loss: 0.938858  [  288/  306]
train() client id: f_00005-0-0 loss: 0.802391  [   32/  146]
train() client id: f_00005-0-1 loss: 1.060129  [   64/  146]
train() client id: f_00005-0-2 loss: 0.832112  [   96/  146]
train() client id: f_00005-0-3 loss: 0.664371  [  128/  146]
train() client id: f_00005-1-0 loss: 0.807521  [   32/  146]
train() client id: f_00005-1-1 loss: 0.826388  [   64/  146]
train() client id: f_00005-1-2 loss: 0.919301  [   96/  146]
train() client id: f_00005-1-3 loss: 0.678830  [  128/  146]
train() client id: f_00005-2-0 loss: 1.181309  [   32/  146]
train() client id: f_00005-2-1 loss: 0.844364  [   64/  146]
train() client id: f_00005-2-2 loss: 0.759960  [   96/  146]
train() client id: f_00005-2-3 loss: 0.649452  [  128/  146]
train() client id: f_00005-3-0 loss: 0.853193  [   32/  146]
train() client id: f_00005-3-1 loss: 0.814232  [   64/  146]
train() client id: f_00005-3-2 loss: 0.732252  [   96/  146]
train() client id: f_00005-3-3 loss: 0.908183  [  128/  146]
train() client id: f_00005-4-0 loss: 0.882891  [   32/  146]
train() client id: f_00005-4-1 loss: 0.915395  [   64/  146]
train() client id: f_00005-4-2 loss: 0.546048  [   96/  146]
train() client id: f_00005-4-3 loss: 0.924729  [  128/  146]
train() client id: f_00005-5-0 loss: 0.691205  [   32/  146]
train() client id: f_00005-5-1 loss: 0.814905  [   64/  146]
train() client id: f_00005-5-2 loss: 0.901307  [   96/  146]
train() client id: f_00005-5-3 loss: 0.826395  [  128/  146]
train() client id: f_00005-6-0 loss: 0.937757  [   32/  146]
train() client id: f_00005-6-1 loss: 0.894899  [   64/  146]
train() client id: f_00005-6-2 loss: 0.572660  [   96/  146]
train() client id: f_00005-6-3 loss: 0.813824  [  128/  146]
train() client id: f_00005-7-0 loss: 0.846897  [   32/  146]
train() client id: f_00005-7-1 loss: 0.578637  [   64/  146]
train() client id: f_00005-7-2 loss: 0.794887  [   96/  146]
train() client id: f_00005-7-3 loss: 1.063046  [  128/  146]
train() client id: f_00005-8-0 loss: 0.803070  [   32/  146]
train() client id: f_00005-8-1 loss: 0.867566  [   64/  146]
train() client id: f_00005-8-2 loss: 0.839594  [   96/  146]
train() client id: f_00005-8-3 loss: 0.891927  [  128/  146]
train() client id: f_00005-9-0 loss: 0.808186  [   32/  146]
train() client id: f_00005-9-1 loss: 0.801245  [   64/  146]
train() client id: f_00005-9-2 loss: 1.029861  [   96/  146]
train() client id: f_00005-9-3 loss: 0.763335  [  128/  146]
train() client id: f_00005-10-0 loss: 0.802892  [   32/  146]
train() client id: f_00005-10-1 loss: 0.734859  [   64/  146]
train() client id: f_00005-10-2 loss: 0.615271  [   96/  146]
train() client id: f_00005-10-3 loss: 1.206282  [  128/  146]
train() client id: f_00005-11-0 loss: 0.840173  [   32/  146]
train() client id: f_00005-11-1 loss: 0.636209  [   64/  146]
train() client id: f_00005-11-2 loss: 0.683779  [   96/  146]
train() client id: f_00005-11-3 loss: 0.932037  [  128/  146]
train() client id: f_00006-0-0 loss: 0.502201  [   32/   54]
train() client id: f_00006-1-0 loss: 0.531246  [   32/   54]
train() client id: f_00006-2-0 loss: 0.482754  [   32/   54]
train() client id: f_00006-3-0 loss: 0.519426  [   32/   54]
train() client id: f_00006-4-0 loss: 0.469624  [   32/   54]
train() client id: f_00006-5-0 loss: 0.480226  [   32/   54]
train() client id: f_00006-6-0 loss: 0.460869  [   32/   54]
train() client id: f_00006-7-0 loss: 0.517784  [   32/   54]
train() client id: f_00006-8-0 loss: 0.436245  [   32/   54]
train() client id: f_00006-9-0 loss: 0.456520  [   32/   54]
train() client id: f_00006-10-0 loss: 0.537014  [   32/   54]
train() client id: f_00006-11-0 loss: 0.466293  [   32/   54]
train() client id: f_00007-0-0 loss: 0.398080  [   32/  179]
train() client id: f_00007-0-1 loss: 0.475028  [   64/  179]
train() client id: f_00007-0-2 loss: 0.694106  [   96/  179]
train() client id: f_00007-0-3 loss: 0.611107  [  128/  179]
train() client id: f_00007-0-4 loss: 0.397910  [  160/  179]
train() client id: f_00007-1-0 loss: 0.390362  [   32/  179]
train() client id: f_00007-1-1 loss: 0.577881  [   64/  179]
train() client id: f_00007-1-2 loss: 0.408668  [   96/  179]
train() client id: f_00007-1-3 loss: 0.394368  [  128/  179]
train() client id: f_00007-1-4 loss: 0.536241  [  160/  179]
train() client id: f_00007-2-0 loss: 0.374021  [   32/  179]
train() client id: f_00007-2-1 loss: 0.376755  [   64/  179]
train() client id: f_00007-2-2 loss: 0.652738  [   96/  179]
train() client id: f_00007-2-3 loss: 0.583288  [  128/  179]
train() client id: f_00007-2-4 loss: 0.515825  [  160/  179]
train() client id: f_00007-3-0 loss: 0.464982  [   32/  179]
train() client id: f_00007-3-1 loss: 0.607295  [   64/  179]
train() client id: f_00007-3-2 loss: 0.495207  [   96/  179]
train() client id: f_00007-3-3 loss: 0.522577  [  128/  179]
train() client id: f_00007-3-4 loss: 0.451820  [  160/  179]
train() client id: f_00007-4-0 loss: 0.600211  [   32/  179]
train() client id: f_00007-4-1 loss: 0.415294  [   64/  179]
train() client id: f_00007-4-2 loss: 0.444467  [   96/  179]
train() client id: f_00007-4-3 loss: 0.368395  [  128/  179]
train() client id: f_00007-4-4 loss: 0.484731  [  160/  179]
train() client id: f_00007-5-0 loss: 0.524746  [   32/  179]
train() client id: f_00007-5-1 loss: 0.465924  [   64/  179]
train() client id: f_00007-5-2 loss: 0.313877  [   96/  179]
train() client id: f_00007-5-3 loss: 0.619780  [  128/  179]
train() client id: f_00007-5-4 loss: 0.402696  [  160/  179]
train() client id: f_00007-6-0 loss: 0.475945  [   32/  179]
train() client id: f_00007-6-1 loss: 0.336992  [   64/  179]
train() client id: f_00007-6-2 loss: 0.577290  [   96/  179]
train() client id: f_00007-6-3 loss: 0.612552  [  128/  179]
train() client id: f_00007-6-4 loss: 0.425563  [  160/  179]
train() client id: f_00007-7-0 loss: 0.327326  [   32/  179]
train() client id: f_00007-7-1 loss: 0.535593  [   64/  179]
train() client id: f_00007-7-2 loss: 0.535632  [   96/  179]
train() client id: f_00007-7-3 loss: 0.508490  [  128/  179]
train() client id: f_00007-7-4 loss: 0.409344  [  160/  179]
train() client id: f_00007-8-0 loss: 0.502288  [   32/  179]
train() client id: f_00007-8-1 loss: 0.582854  [   64/  179]
train() client id: f_00007-8-2 loss: 0.408933  [   96/  179]
train() client id: f_00007-8-3 loss: 0.395112  [  128/  179]
train() client id: f_00007-8-4 loss: 0.432943  [  160/  179]
train() client id: f_00007-9-0 loss: 0.434955  [   32/  179]
train() client id: f_00007-9-1 loss: 0.330861  [   64/  179]
train() client id: f_00007-9-2 loss: 0.496581  [   96/  179]
train() client id: f_00007-9-3 loss: 0.548289  [  128/  179]
train() client id: f_00007-9-4 loss: 0.330116  [  160/  179]
train() client id: f_00007-10-0 loss: 0.532143  [   32/  179]
train() client id: f_00007-10-1 loss: 0.398955  [   64/  179]
train() client id: f_00007-10-2 loss: 0.397363  [   96/  179]
train() client id: f_00007-10-3 loss: 0.483818  [  128/  179]
train() client id: f_00007-10-4 loss: 0.319494  [  160/  179]
train() client id: f_00007-11-0 loss: 0.562667  [   32/  179]
train() client id: f_00007-11-1 loss: 0.438571  [   64/  179]
train() client id: f_00007-11-2 loss: 0.302742  [   96/  179]
train() client id: f_00007-11-3 loss: 0.616859  [  128/  179]
train() client id: f_00007-11-4 loss: 0.384200  [  160/  179]
train() client id: f_00008-0-0 loss: 0.703490  [   32/  130]
train() client id: f_00008-0-1 loss: 0.660454  [   64/  130]
train() client id: f_00008-0-2 loss: 0.753722  [   96/  130]
train() client id: f_00008-0-3 loss: 0.683133  [  128/  130]
train() client id: f_00008-1-0 loss: 0.630558  [   32/  130]
train() client id: f_00008-1-1 loss: 0.654479  [   64/  130]
train() client id: f_00008-1-2 loss: 0.823597  [   96/  130]
train() client id: f_00008-1-3 loss: 0.676424  [  128/  130]
train() client id: f_00008-2-0 loss: 0.652141  [   32/  130]
train() client id: f_00008-2-1 loss: 0.705543  [   64/  130]
train() client id: f_00008-2-2 loss: 0.607875  [   96/  130]
train() client id: f_00008-2-3 loss: 0.824163  [  128/  130]
train() client id: f_00008-3-0 loss: 0.729730  [   32/  130]
train() client id: f_00008-3-1 loss: 0.717822  [   64/  130]
train() client id: f_00008-3-2 loss: 0.621782  [   96/  130]
train() client id: f_00008-3-3 loss: 0.719568  [  128/  130]
train() client id: f_00008-4-0 loss: 0.651596  [   32/  130]
train() client id: f_00008-4-1 loss: 0.651627  [   64/  130]
train() client id: f_00008-4-2 loss: 0.754265  [   96/  130]
train() client id: f_00008-4-3 loss: 0.684096  [  128/  130]
train() client id: f_00008-5-0 loss: 0.661993  [   32/  130]
train() client id: f_00008-5-1 loss: 0.718936  [   64/  130]
train() client id: f_00008-5-2 loss: 0.753734  [   96/  130]
train() client id: f_00008-5-3 loss: 0.593563  [  128/  130]
train() client id: f_00008-6-0 loss: 0.710141  [   32/  130]
train() client id: f_00008-6-1 loss: 0.761671  [   64/  130]
train() client id: f_00008-6-2 loss: 0.659549  [   96/  130]
train() client id: f_00008-6-3 loss: 0.653060  [  128/  130]
train() client id: f_00008-7-0 loss: 0.673229  [   32/  130]
train() client id: f_00008-7-1 loss: 0.710330  [   64/  130]
train() client id: f_00008-7-2 loss: 0.621720  [   96/  130]
train() client id: f_00008-7-3 loss: 0.773740  [  128/  130]
train() client id: f_00008-8-0 loss: 0.699513  [   32/  130]
train() client id: f_00008-8-1 loss: 0.647708  [   64/  130]
train() client id: f_00008-8-2 loss: 0.696843  [   96/  130]
train() client id: f_00008-8-3 loss: 0.731234  [  128/  130]
train() client id: f_00008-9-0 loss: 0.751426  [   32/  130]
train() client id: f_00008-9-1 loss: 0.676236  [   64/  130]
train() client id: f_00008-9-2 loss: 0.699699  [   96/  130]
train() client id: f_00008-9-3 loss: 0.652280  [  128/  130]
train() client id: f_00008-10-0 loss: 0.781732  [   32/  130]
train() client id: f_00008-10-1 loss: 0.695514  [   64/  130]
train() client id: f_00008-10-2 loss: 0.692908  [   96/  130]
train() client id: f_00008-10-3 loss: 0.609931  [  128/  130]
train() client id: f_00008-11-0 loss: 0.712872  [   32/  130]
train() client id: f_00008-11-1 loss: 0.685600  [   64/  130]
train() client id: f_00008-11-2 loss: 0.768088  [   96/  130]
train() client id: f_00008-11-3 loss: 0.618129  [  128/  130]
train() client id: f_00009-0-0 loss: 1.202869  [   32/  118]
train() client id: f_00009-0-1 loss: 1.045687  [   64/  118]
train() client id: f_00009-0-2 loss: 1.054031  [   96/  118]
train() client id: f_00009-1-0 loss: 1.230058  [   32/  118]
train() client id: f_00009-1-1 loss: 1.014357  [   64/  118]
train() client id: f_00009-1-2 loss: 1.094437  [   96/  118]
train() client id: f_00009-2-0 loss: 1.148498  [   32/  118]
train() client id: f_00009-2-1 loss: 1.048837  [   64/  118]
train() client id: f_00009-2-2 loss: 0.962879  [   96/  118]
train() client id: f_00009-3-0 loss: 0.937047  [   32/  118]
train() client id: f_00009-3-1 loss: 1.143567  [   64/  118]
train() client id: f_00009-3-2 loss: 0.948917  [   96/  118]
train() client id: f_00009-4-0 loss: 0.997690  [   32/  118]
train() client id: f_00009-4-1 loss: 0.969625  [   64/  118]
train() client id: f_00009-4-2 loss: 0.957425  [   96/  118]
train() client id: f_00009-5-0 loss: 1.051411  [   32/  118]
train() client id: f_00009-5-1 loss: 0.762047  [   64/  118]
train() client id: f_00009-5-2 loss: 1.019976  [   96/  118]
train() client id: f_00009-6-0 loss: 0.898478  [   32/  118]
train() client id: f_00009-6-1 loss: 0.919465  [   64/  118]
train() client id: f_00009-6-2 loss: 0.800090  [   96/  118]
train() client id: f_00009-7-0 loss: 0.859295  [   32/  118]
train() client id: f_00009-7-1 loss: 0.871829  [   64/  118]
train() client id: f_00009-7-2 loss: 0.943148  [   96/  118]
train() client id: f_00009-8-0 loss: 0.695599  [   32/  118]
train() client id: f_00009-8-1 loss: 0.851986  [   64/  118]
train() client id: f_00009-8-2 loss: 1.024761  [   96/  118]
train() client id: f_00009-9-0 loss: 0.783511  [   32/  118]
train() client id: f_00009-9-1 loss: 0.977214  [   64/  118]
train() client id: f_00009-9-2 loss: 0.741026  [   96/  118]
train() client id: f_00009-10-0 loss: 0.762159  [   32/  118]
train() client id: f_00009-10-1 loss: 0.936488  [   64/  118]
train() client id: f_00009-10-2 loss: 0.840888  [   96/  118]
train() client id: f_00009-11-0 loss: 0.723302  [   32/  118]
train() client id: f_00009-11-1 loss: 0.944846  [   64/  118]
train() client id: f_00009-11-2 loss: 0.868949  [   96/  118]
At round 36 accuracy: 0.649867374005305
At round 36 training accuracy: 0.5801475519785378
At round 36 training loss: 0.8301893295403322
update_location
xs = -3.905658 4.200318 200.009024 18.811294 0.979296 3.956410 -162.443192 -141.324852 184.663977 -127.060879 
ys = 192.587959 175.555839 1.320614 -162.455176 154.350187 137.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 217.037730 202.082892 223.618769 191.691286 183.915576 170.318506 190.773901 173.128247 210.735508 161.742013 
dists_bs = 171.995959 179.201180 413.275087 389.151402 177.186702 182.779058 178.068269 177.403129 392.610104 177.591251 
uav_gains = -108.788055 -107.813529 -109.242609 -107.161560 -106.674836 -105.804832 -107.104263 -105.987723 -108.369310 -105.232699 
bs_gains = -102.161570 -102.660603 -112.821757 -112.090380 -102.523131 -102.900999 -102.583482 -102.537975 -112.197980 -102.550863 
Round 37
-------------------------------
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.10747648 12.63082676  6.01756463  2.17165072 14.56715747  7.00995328
  2.69087946  8.58799468  6.33825934  5.68528245]
obj_prev = 71.80704525657299
eta_min = 7.508228350489047e-16	eta_max = 0.9300641933191646
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 16.65764355541381	eta = 0.909090909090909
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 30.742318165825036	eta = 0.4925884977645458
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 23.795554951531557	eta = 0.636392483972255
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 22.538890009475697	eta = 0.6718748046925549
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 22.472173027112785	eta = 0.6738695143025546
af = 15.14331232310346	bf = 1.3514443765451492	zeta = 22.47196919842037	eta = 0.6738756265369006
eta = 0.6738756265369006
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [0.0325705  0.06850145 0.03205351 0.01111533 0.07909978 0.03774042
 0.01395879 0.04627079 0.03360448 0.03050253]
ene_total = [2.01826629 3.59433964 2.00757769 0.95394728 4.09618977 2.13534693
 1.08775634 2.59856536 2.19433777 1.78564212]
ti_comp = [0.49324588 0.52112484 0.49032366 0.50250852 0.52157476 0.52032434
 0.50280454 0.5082184  0.46655987 0.52148445]
ti_coms = [0.09871374 0.07083478 0.10163597 0.08945111 0.07038486 0.07163528
 0.08915509 0.08374122 0.12539975 0.07047517]
t_total = [28.14984474 28.14984474 28.14984474 28.14984474 28.14984474 28.14984474
 28.14984474 28.14984474 28.14984474 28.14984474]
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [8.87618492e-06 7.39768305e-05 8.56132548e-06 3.39907040e-07
 1.13703226e-04 1.24094325e-05 6.72395117e-07 2.39717625e-05
 1.08957452e-05 6.52236260e-06]
ene_total = [0.46984203 0.34036317 0.48372332 0.42538889 0.34011276 0.34124206
 0.42399702 0.39936008 0.59683981 0.33544538]
optimize_network iter = 0 obj = 4.156314504851677
eta = 0.6738756265369006
freqs = [33016494.15795401 65724604.40972588 32686070.96869287 11059841.67344831
 75827849.2653193  36266244.90193002 13880926.45922257 45522544.15430705
 36013045.58118701 29245868.53629994]
eta_min = 0.673875626536905	eta_max = 0.6738756265369027
af = 0.01021278956996407	bf = 1.3514443765451492	zeta = 0.011234068526960479	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [2.14332394e-06 1.78631149e-05 2.06729513e-06 8.20770299e-08
 2.74558099e-05 2.99649386e-06 1.62362610e-07 5.78843872e-06
 2.63098523e-06 1.57494870e-06]
ene_total = [1.68434772 1.21143542 1.73418576 1.52598255 1.2053965  1.2225551
 1.52094637 1.42954967 2.1396742  1.20252205]
ti_comp = [0.49324588 0.52112484 0.49032366 0.50250852 0.52157476 0.52032434
 0.50280454 0.5082184  0.46655987 0.52148445]
ti_coms = [0.09871374 0.07083478 0.10163597 0.08945111 0.07038486 0.07163528
 0.08915509 0.08374122 0.12539975 0.07047517]
t_total = [28.14984474 28.14984474 28.14984474 28.14984474 28.14984474 28.14984474
 28.14984474 28.14984474 28.14984474 28.14984474]
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [8.87618492e-06 7.39768305e-05 8.56132548e-06 3.39907040e-07
 1.13703226e-04 1.24094325e-05 6.72395117e-07 2.39717625e-05
 1.08957452e-05 6.52236260e-06]
ene_total = [0.46984203 0.34036317 0.48372332 0.42538889 0.34011276 0.34124206
 0.42399702 0.39936008 0.59683981 0.33544538]
optimize_network iter = 1 obj = 4.156314504851704
eta = 0.6738756265369027
freqs = [33016494.15795401 65724604.40972584 32686070.96869286 11059841.67344831
 75827849.26531927 36266244.90193    13880926.45922257 45522544.15430704
 36013045.58118702 29245868.53629993]
Done!
ene_coms = [0.00987137 0.00708348 0.0101636  0.00894511 0.00703849 0.00716353
 0.00891551 0.00837412 0.01253998 0.00704752]
ene_comp = [8.24107198e-06 6.86836056e-05 7.94874151e-06 3.15585852e-07
 1.05567480e-04 1.15215069e-05 6.24283587e-07 2.22565238e-05
 1.01161277e-05 6.05567145e-06]
ene_total = [0.00987962 0.00715216 0.01017155 0.00894543 0.00714405 0.00717505
 0.00891613 0.00839638 0.01255009 0.00705357]
At round 37 energy consumption: 0.08738402800819062
At round 37 eta: 0.6738756265369027
At round 37 a_n: 15.508406518389636
At round 37 local rounds: 12.924801449949687
At round 37 global rounds: 47.55365676507614
gradient difference: 0.4338829517364502
train() client id: f_00000-0-0 loss: 1.448901  [   32/  126]
train() client id: f_00000-0-1 loss: 1.263244  [   64/  126]
train() client id: f_00000-0-2 loss: 1.106034  [   96/  126]
train() client id: f_00000-1-0 loss: 1.040269  [   32/  126]
train() client id: f_00000-1-1 loss: 1.014390  [   64/  126]
train() client id: f_00000-1-2 loss: 1.199609  [   96/  126]
train() client id: f_00000-2-0 loss: 1.161535  [   32/  126]
train() client id: f_00000-2-1 loss: 0.902470  [   64/  126]
train() client id: f_00000-2-2 loss: 1.149539  [   96/  126]
train() client id: f_00000-3-0 loss: 1.195197  [   32/  126]
train() client id: f_00000-3-1 loss: 0.895292  [   64/  126]
train() client id: f_00000-3-2 loss: 0.809659  [   96/  126]
train() client id: f_00000-4-0 loss: 0.896184  [   32/  126]
train() client id: f_00000-4-1 loss: 0.908164  [   64/  126]
train() client id: f_00000-4-2 loss: 0.889780  [   96/  126]
train() client id: f_00000-5-0 loss: 0.831854  [   32/  126]
train() client id: f_00000-5-1 loss: 0.956151  [   64/  126]
train() client id: f_00000-5-2 loss: 0.808649  [   96/  126]
train() client id: f_00000-6-0 loss: 0.851475  [   32/  126]
train() client id: f_00000-6-1 loss: 0.788112  [   64/  126]
train() client id: f_00000-6-2 loss: 0.834726  [   96/  126]
train() client id: f_00000-7-0 loss: 0.843373  [   32/  126]
train() client id: f_00000-7-1 loss: 0.810686  [   64/  126]
train() client id: f_00000-7-2 loss: 0.797396  [   96/  126]
train() client id: f_00000-8-0 loss: 0.828657  [   32/  126]
train() client id: f_00000-8-1 loss: 0.820749  [   64/  126]
train() client id: f_00000-8-2 loss: 0.669894  [   96/  126]
train() client id: f_00000-9-0 loss: 0.844781  [   32/  126]
train() client id: f_00000-9-1 loss: 0.699302  [   64/  126]
train() client id: f_00000-9-2 loss: 0.777251  [   96/  126]
train() client id: f_00000-10-0 loss: 0.736715  [   32/  126]
train() client id: f_00000-10-1 loss: 0.903156  [   64/  126]
train() client id: f_00000-10-2 loss: 0.712344  [   96/  126]
train() client id: f_00000-11-0 loss: 0.642144  [   32/  126]
train() client id: f_00000-11-1 loss: 0.886676  [   64/  126]
train() client id: f_00000-11-2 loss: 0.770057  [   96/  126]
train() client id: f_00001-0-0 loss: 0.562335  [   32/  265]
train() client id: f_00001-0-1 loss: 0.691418  [   64/  265]
train() client id: f_00001-0-2 loss: 0.495550  [   96/  265]
train() client id: f_00001-0-3 loss: 0.453924  [  128/  265]
train() client id: f_00001-0-4 loss: 0.421943  [  160/  265]
train() client id: f_00001-0-5 loss: 0.459628  [  192/  265]
train() client id: f_00001-0-6 loss: 0.452778  [  224/  265]
train() client id: f_00001-0-7 loss: 0.504159  [  256/  265]
train() client id: f_00001-1-0 loss: 0.466422  [   32/  265]
train() client id: f_00001-1-1 loss: 0.498123  [   64/  265]
train() client id: f_00001-1-2 loss: 0.530990  [   96/  265]
train() client id: f_00001-1-3 loss: 0.438683  [  128/  265]
train() client id: f_00001-1-4 loss: 0.609487  [  160/  265]
train() client id: f_00001-1-5 loss: 0.622250  [  192/  265]
train() client id: f_00001-1-6 loss: 0.471273  [  224/  265]
train() client id: f_00001-1-7 loss: 0.426986  [  256/  265]
train() client id: f_00001-2-0 loss: 0.493194  [   32/  265]
train() client id: f_00001-2-1 loss: 0.554463  [   64/  265]
train() client id: f_00001-2-2 loss: 0.544000  [   96/  265]
train() client id: f_00001-2-3 loss: 0.561845  [  128/  265]
train() client id: f_00001-2-4 loss: 0.548659  [  160/  265]
train() client id: f_00001-2-5 loss: 0.455716  [  192/  265]
train() client id: f_00001-2-6 loss: 0.417692  [  224/  265]
train() client id: f_00001-2-7 loss: 0.442924  [  256/  265]
train() client id: f_00001-3-0 loss: 0.447294  [   32/  265]
train() client id: f_00001-3-1 loss: 0.495850  [   64/  265]
train() client id: f_00001-3-2 loss: 0.425888  [   96/  265]
train() client id: f_00001-3-3 loss: 0.525804  [  128/  265]
train() client id: f_00001-3-4 loss: 0.473283  [  160/  265]
train() client id: f_00001-3-5 loss: 0.486963  [  192/  265]
train() client id: f_00001-3-6 loss: 0.611421  [  224/  265]
train() client id: f_00001-3-7 loss: 0.528638  [  256/  265]
train() client id: f_00001-4-0 loss: 0.453736  [   32/  265]
train() client id: f_00001-4-1 loss: 0.466253  [   64/  265]
train() client id: f_00001-4-2 loss: 0.427802  [   96/  265]
train() client id: f_00001-4-3 loss: 0.550903  [  128/  265]
train() client id: f_00001-4-4 loss: 0.512194  [  160/  265]
train() client id: f_00001-4-5 loss: 0.575006  [  192/  265]
train() client id: f_00001-4-6 loss: 0.502093  [  224/  265]
train() client id: f_00001-4-7 loss: 0.438219  [  256/  265]
train() client id: f_00001-5-0 loss: 0.385163  [   32/  265]
train() client id: f_00001-5-1 loss: 0.551228  [   64/  265]
train() client id: f_00001-5-2 loss: 0.542208  [   96/  265]
train() client id: f_00001-5-3 loss: 0.418461  [  128/  265]
train() client id: f_00001-5-4 loss: 0.452140  [  160/  265]
train() client id: f_00001-5-5 loss: 0.541540  [  192/  265]
train() client id: f_00001-5-6 loss: 0.458351  [  224/  265]
train() client id: f_00001-5-7 loss: 0.618639  [  256/  265]
train() client id: f_00001-6-0 loss: 0.407558  [   32/  265]
train() client id: f_00001-6-1 loss: 0.639085  [   64/  265]
train() client id: f_00001-6-2 loss: 0.395278  [   96/  265]
train() client id: f_00001-6-3 loss: 0.384820  [  128/  265]
train() client id: f_00001-6-4 loss: 0.424529  [  160/  265]
train() client id: f_00001-6-5 loss: 0.609145  [  192/  265]
train() client id: f_00001-6-6 loss: 0.598162  [  224/  265]
train() client id: f_00001-6-7 loss: 0.508272  [  256/  265]
train() client id: f_00001-7-0 loss: 0.398822  [   32/  265]
train() client id: f_00001-7-1 loss: 0.520745  [   64/  265]
train() client id: f_00001-7-2 loss: 0.614043  [   96/  265]
train() client id: f_00001-7-3 loss: 0.546675  [  128/  265]
train() client id: f_00001-7-4 loss: 0.444944  [  160/  265]
train() client id: f_00001-7-5 loss: 0.439481  [  192/  265]
train() client id: f_00001-7-6 loss: 0.473558  [  224/  265]
train() client id: f_00001-7-7 loss: 0.394246  [  256/  265]
train() client id: f_00001-8-0 loss: 0.602016  [   32/  265]
train() client id: f_00001-8-1 loss: 0.540226  [   64/  265]
train() client id: f_00001-8-2 loss: 0.423237  [   96/  265]
train() client id: f_00001-8-3 loss: 0.464195  [  128/  265]
train() client id: f_00001-8-4 loss: 0.473087  [  160/  265]
train() client id: f_00001-8-5 loss: 0.480064  [  192/  265]
train() client id: f_00001-8-6 loss: 0.402532  [  224/  265]
train() client id: f_00001-8-7 loss: 0.470008  [  256/  265]
train() client id: f_00001-9-0 loss: 0.518136  [   32/  265]
train() client id: f_00001-9-1 loss: 0.441506  [   64/  265]
train() client id: f_00001-9-2 loss: 0.450235  [   96/  265]
train() client id: f_00001-9-3 loss: 0.549957  [  128/  265]
train() client id: f_00001-9-4 loss: 0.448263  [  160/  265]
train() client id: f_00001-9-5 loss: 0.441959  [  192/  265]
train() client id: f_00001-9-6 loss: 0.631518  [  224/  265]
train() client id: f_00001-9-7 loss: 0.413820  [  256/  265]
train() client id: f_00001-10-0 loss: 0.468689  [   32/  265]
train() client id: f_00001-10-1 loss: 0.624765  [   64/  265]
train() client id: f_00001-10-2 loss: 0.396799  [   96/  265]
train() client id: f_00001-10-3 loss: 0.449554  [  128/  265]
train() client id: f_00001-10-4 loss: 0.400097  [  160/  265]
train() client id: f_00001-10-5 loss: 0.450857  [  192/  265]
train() client id: f_00001-10-6 loss: 0.533144  [  224/  265]
train() client id: f_00001-10-7 loss: 0.595695  [  256/  265]
train() client id: f_00001-11-0 loss: 0.488293  [   32/  265]
train() client id: f_00001-11-1 loss: 0.422197  [   64/  265]
train() client id: f_00001-11-2 loss: 0.531575  [   96/  265]
train() client id: f_00001-11-3 loss: 0.463363  [  128/  265]
train() client id: f_00001-11-4 loss: 0.410192  [  160/  265]
train() client id: f_00001-11-5 loss: 0.531993  [  192/  265]
train() client id: f_00001-11-6 loss: 0.563179  [  224/  265]
train() client id: f_00001-11-7 loss: 0.566119  [  256/  265]
train() client id: f_00002-0-0 loss: 1.327541  [   32/  124]
train() client id: f_00002-0-1 loss: 1.083203  [   64/  124]
train() client id: f_00002-0-2 loss: 1.296448  [   96/  124]
train() client id: f_00002-1-0 loss: 1.120377  [   32/  124]
train() client id: f_00002-1-1 loss: 1.215955  [   64/  124]
train() client id: f_00002-1-2 loss: 1.358295  [   96/  124]
train() client id: f_00002-2-0 loss: 1.161557  [   32/  124]
train() client id: f_00002-2-1 loss: 1.246237  [   64/  124]
train() client id: f_00002-2-2 loss: 1.007946  [   96/  124]
train() client id: f_00002-3-0 loss: 1.165963  [   32/  124]
train() client id: f_00002-3-1 loss: 1.158600  [   64/  124]
train() client id: f_00002-3-2 loss: 0.995350  [   96/  124]
train() client id: f_00002-4-0 loss: 1.050963  [   32/  124]
train() client id: f_00002-4-1 loss: 1.167990  [   64/  124]
train() client id: f_00002-4-2 loss: 1.072769  [   96/  124]
train() client id: f_00002-5-0 loss: 1.014030  [   32/  124]
train() client id: f_00002-5-1 loss: 1.192904  [   64/  124]
train() client id: f_00002-5-2 loss: 1.119206  [   96/  124]
train() client id: f_00002-6-0 loss: 1.113536  [   32/  124]
train() client id: f_00002-6-1 loss: 1.015269  [   64/  124]
train() client id: f_00002-6-2 loss: 1.253097  [   96/  124]
train() client id: f_00002-7-0 loss: 1.066381  [   32/  124]
train() client id: f_00002-7-1 loss: 1.089790  [   64/  124]
train() client id: f_00002-7-2 loss: 1.062321  [   96/  124]
train() client id: f_00002-8-0 loss: 1.019174  [   32/  124]
train() client id: f_00002-8-1 loss: 1.045649  [   64/  124]
train() client id: f_00002-8-2 loss: 1.114677  [   96/  124]
train() client id: f_00002-9-0 loss: 0.944850  [   32/  124]
train() client id: f_00002-9-1 loss: 1.093397  [   64/  124]
train() client id: f_00002-9-2 loss: 1.149594  [   96/  124]
train() client id: f_00002-10-0 loss: 1.007443  [   32/  124]
train() client id: f_00002-10-1 loss: 1.055578  [   64/  124]
train() client id: f_00002-10-2 loss: 1.262584  [   96/  124]
train() client id: f_00002-11-0 loss: 1.017988  [   32/  124]
train() client id: f_00002-11-1 loss: 0.936994  [   64/  124]
train() client id: f_00002-11-2 loss: 1.172881  [   96/  124]
train() client id: f_00003-0-0 loss: 0.843643  [   32/   43]
train() client id: f_00003-1-0 loss: 0.836304  [   32/   43]
train() client id: f_00003-2-0 loss: 0.686777  [   32/   43]
train() client id: f_00003-3-0 loss: 0.978597  [   32/   43]
train() client id: f_00003-4-0 loss: 0.737953  [   32/   43]
train() client id: f_00003-5-0 loss: 0.670434  [   32/   43]
train() client id: f_00003-6-0 loss: 0.691314  [   32/   43]
train() client id: f_00003-7-0 loss: 0.902064  [   32/   43]
train() client id: f_00003-8-0 loss: 0.843708  [   32/   43]
train() client id: f_00003-9-0 loss: 0.772012  [   32/   43]
train() client id: f_00003-10-0 loss: 0.959911  [   32/   43]
train() client id: f_00003-11-0 loss: 0.799282  [   32/   43]
train() client id: f_00004-0-0 loss: 0.864202  [   32/  306]
train() client id: f_00004-0-1 loss: 0.776840  [   64/  306]
train() client id: f_00004-0-2 loss: 0.914919  [   96/  306]
train() client id: f_00004-0-3 loss: 0.742803  [  128/  306]
train() client id: f_00004-0-4 loss: 0.848122  [  160/  306]
train() client id: f_00004-0-5 loss: 0.945287  [  192/  306]
train() client id: f_00004-0-6 loss: 0.749121  [  224/  306]
train() client id: f_00004-0-7 loss: 0.755738  [  256/  306]
train() client id: f_00004-0-8 loss: 0.801597  [  288/  306]
train() client id: f_00004-1-0 loss: 0.864453  [   32/  306]
train() client id: f_00004-1-1 loss: 0.787986  [   64/  306]
train() client id: f_00004-1-2 loss: 0.911950  [   96/  306]
train() client id: f_00004-1-3 loss: 0.802391  [  128/  306]
train() client id: f_00004-1-4 loss: 0.721878  [  160/  306]
train() client id: f_00004-1-5 loss: 0.820444  [  192/  306]
train() client id: f_00004-1-6 loss: 0.908345  [  224/  306]
train() client id: f_00004-1-7 loss: 0.833104  [  256/  306]
train() client id: f_00004-1-8 loss: 0.695157  [  288/  306]
train() client id: f_00004-2-0 loss: 0.824385  [   32/  306]
train() client id: f_00004-2-1 loss: 0.875214  [   64/  306]
train() client id: f_00004-2-2 loss: 0.805746  [   96/  306]
train() client id: f_00004-2-3 loss: 0.841876  [  128/  306]
train() client id: f_00004-2-4 loss: 0.750573  [  160/  306]
train() client id: f_00004-2-5 loss: 0.804197  [  192/  306]
train() client id: f_00004-2-6 loss: 1.046802  [  224/  306]
train() client id: f_00004-2-7 loss: 0.827513  [  256/  306]
train() client id: f_00004-2-8 loss: 0.752299  [  288/  306]
train() client id: f_00004-3-0 loss: 0.751022  [   32/  306]
train() client id: f_00004-3-1 loss: 0.781947  [   64/  306]
train() client id: f_00004-3-2 loss: 0.792033  [   96/  306]
train() client id: f_00004-3-3 loss: 0.861811  [  128/  306]
train() client id: f_00004-3-4 loss: 0.862896  [  160/  306]
train() client id: f_00004-3-5 loss: 0.868078  [  192/  306]
train() client id: f_00004-3-6 loss: 0.801642  [  224/  306]
train() client id: f_00004-3-7 loss: 0.843025  [  256/  306]
train() client id: f_00004-3-8 loss: 0.904036  [  288/  306]
train() client id: f_00004-4-0 loss: 0.679966  [   32/  306]
train() client id: f_00004-4-1 loss: 0.832810  [   64/  306]
train() client id: f_00004-4-2 loss: 0.943460  [   96/  306]
train() client id: f_00004-4-3 loss: 0.747134  [  128/  306]
train() client id: f_00004-4-4 loss: 0.841005  [  160/  306]
train() client id: f_00004-4-5 loss: 1.017040  [  192/  306]
train() client id: f_00004-4-6 loss: 0.823663  [  224/  306]
train() client id: f_00004-4-7 loss: 0.790456  [  256/  306]
train() client id: f_00004-4-8 loss: 0.842252  [  288/  306]
train() client id: f_00004-5-0 loss: 0.781704  [   32/  306]
train() client id: f_00004-5-1 loss: 0.782781  [   64/  306]
train() client id: f_00004-5-2 loss: 0.637663  [   96/  306]
train() client id: f_00004-5-3 loss: 0.867253  [  128/  306]
train() client id: f_00004-5-4 loss: 0.882654  [  160/  306]
train() client id: f_00004-5-5 loss: 0.824921  [  192/  306]
train() client id: f_00004-5-6 loss: 0.922161  [  224/  306]
train() client id: f_00004-5-7 loss: 0.907068  [  256/  306]
train() client id: f_00004-5-8 loss: 0.881888  [  288/  306]
train() client id: f_00004-6-0 loss: 0.738859  [   32/  306]
train() client id: f_00004-6-1 loss: 0.677534  [   64/  306]
train() client id: f_00004-6-2 loss: 0.823197  [   96/  306]
train() client id: f_00004-6-3 loss: 0.831533  [  128/  306]
train() client id: f_00004-6-4 loss: 0.781116  [  160/  306]
train() client id: f_00004-6-5 loss: 0.843101  [  192/  306]
train() client id: f_00004-6-6 loss: 0.913919  [  224/  306]
train() client id: f_00004-6-7 loss: 1.049700  [  256/  306]
train() client id: f_00004-6-8 loss: 0.734057  [  288/  306]
train() client id: f_00004-7-0 loss: 0.776498  [   32/  306]
train() client id: f_00004-7-1 loss: 0.805846  [   64/  306]
train() client id: f_00004-7-2 loss: 0.809664  [   96/  306]
train() client id: f_00004-7-3 loss: 0.843130  [  128/  306]
train() client id: f_00004-7-4 loss: 0.860326  [  160/  306]
train() client id: f_00004-7-5 loss: 0.714603  [  192/  306]
train() client id: f_00004-7-6 loss: 1.002539  [  224/  306]
train() client id: f_00004-7-7 loss: 0.796764  [  256/  306]
train() client id: f_00004-7-8 loss: 0.850406  [  288/  306]
train() client id: f_00004-8-0 loss: 0.746632  [   32/  306]
train() client id: f_00004-8-1 loss: 0.762764  [   64/  306]
train() client id: f_00004-8-2 loss: 0.790015  [   96/  306]
train() client id: f_00004-8-3 loss: 0.854692  [  128/  306]
train() client id: f_00004-8-4 loss: 0.834433  [  160/  306]
train() client id: f_00004-8-5 loss: 1.007816  [  192/  306]
train() client id: f_00004-8-6 loss: 0.853755  [  224/  306]
train() client id: f_00004-8-7 loss: 0.926169  [  256/  306]
train() client id: f_00004-8-8 loss: 0.748596  [  288/  306]
train() client id: f_00004-9-0 loss: 0.859296  [   32/  306]
train() client id: f_00004-9-1 loss: 0.801372  [   64/  306]
train() client id: f_00004-9-2 loss: 0.831946  [   96/  306]
train() client id: f_00004-9-3 loss: 0.843291  [  128/  306]
train() client id: f_00004-9-4 loss: 0.811205  [  160/  306]
train() client id: f_00004-9-5 loss: 0.710750  [  192/  306]
train() client id: f_00004-9-6 loss: 0.905522  [  224/  306]
train() client id: f_00004-9-7 loss: 0.740664  [  256/  306]
train() client id: f_00004-9-8 loss: 0.943665  [  288/  306]
train() client id: f_00004-10-0 loss: 0.900662  [   32/  306]
train() client id: f_00004-10-1 loss: 0.780002  [   64/  306]
train() client id: f_00004-10-2 loss: 0.922711  [   96/  306]
train() client id: f_00004-10-3 loss: 0.810646  [  128/  306]
train() client id: f_00004-10-4 loss: 0.724293  [  160/  306]
train() client id: f_00004-10-5 loss: 0.882159  [  192/  306]
train() client id: f_00004-10-6 loss: 0.848912  [  224/  306]
train() client id: f_00004-10-7 loss: 0.718850  [  256/  306]
train() client id: f_00004-10-8 loss: 0.877473  [  288/  306]
train() client id: f_00004-11-0 loss: 0.940526  [   32/  306]
train() client id: f_00004-11-1 loss: 0.867305  [   64/  306]
train() client id: f_00004-11-2 loss: 0.849595  [   96/  306]
train() client id: f_00004-11-3 loss: 0.915865  [  128/  306]
train() client id: f_00004-11-4 loss: 0.748142  [  160/  306]
train() client id: f_00004-11-5 loss: 0.788536  [  192/  306]
train() client id: f_00004-11-6 loss: 0.744155  [  224/  306]
train() client id: f_00004-11-7 loss: 0.839499  [  256/  306]
train() client id: f_00004-11-8 loss: 0.791437  [  288/  306]
train() client id: f_00005-0-0 loss: 0.398905  [   32/  146]
train() client id: f_00005-0-1 loss: 0.506674  [   64/  146]
train() client id: f_00005-0-2 loss: 0.685785  [   96/  146]
train() client id: f_00005-0-3 loss: 0.458893  [  128/  146]
train() client id: f_00005-1-0 loss: 0.572071  [   32/  146]
train() client id: f_00005-1-1 loss: 0.441312  [   64/  146]
train() client id: f_00005-1-2 loss: 0.572494  [   96/  146]
train() client id: f_00005-1-3 loss: 0.601601  [  128/  146]
train() client id: f_00005-2-0 loss: 0.609735  [   32/  146]
train() client id: f_00005-2-1 loss: 0.575066  [   64/  146]
train() client id: f_00005-2-2 loss: 0.552642  [   96/  146]
train() client id: f_00005-2-3 loss: 0.342702  [  128/  146]
train() client id: f_00005-3-0 loss: 0.458218  [   32/  146]
train() client id: f_00005-3-1 loss: 0.561547  [   64/  146]
train() client id: f_00005-3-2 loss: 0.516542  [   96/  146]
train() client id: f_00005-3-3 loss: 0.626083  [  128/  146]
train() client id: f_00005-4-0 loss: 0.474002  [   32/  146]
train() client id: f_00005-4-1 loss: 0.436397  [   64/  146]
train() client id: f_00005-4-2 loss: 0.625421  [   96/  146]
train() client id: f_00005-4-3 loss: 0.411372  [  128/  146]
train() client id: f_00005-5-0 loss: 0.462889  [   32/  146]
train() client id: f_00005-5-1 loss: 0.675738  [   64/  146]
train() client id: f_00005-5-2 loss: 0.598440  [   96/  146]
train() client id: f_00005-5-3 loss: 0.374992  [  128/  146]
train() client id: f_00005-6-0 loss: 0.349899  [   32/  146]
train() client id: f_00005-6-1 loss: 0.481095  [   64/  146]
train() client id: f_00005-6-2 loss: 0.532086  [   96/  146]
train() client id: f_00005-6-3 loss: 0.605811  [  128/  146]
train() client id: f_00005-7-0 loss: 0.369110  [   32/  146]
train() client id: f_00005-7-1 loss: 0.423049  [   64/  146]
train() client id: f_00005-7-2 loss: 0.515974  [   96/  146]
train() client id: f_00005-7-3 loss: 0.507542  [  128/  146]
train() client id: f_00005-8-0 loss: 0.369780  [   32/  146]
train() client id: f_00005-8-1 loss: 0.532149  [   64/  146]
train() client id: f_00005-8-2 loss: 0.602937  [   96/  146]
train() client id: f_00005-8-3 loss: 0.289622  [  128/  146]
train() client id: f_00005-9-0 loss: 0.605731  [   32/  146]
train() client id: f_00005-9-1 loss: 0.578139  [   64/  146]
train() client id: f_00005-9-2 loss: 0.539900  [   96/  146]
train() client id: f_00005-9-3 loss: 0.272791  [  128/  146]
train() client id: f_00005-10-0 loss: 0.616464  [   32/  146]
train() client id: f_00005-10-1 loss: 0.251833  [   64/  146]
train() client id: f_00005-10-2 loss: 0.470128  [   96/  146]
train() client id: f_00005-10-3 loss: 0.470736  [  128/  146]
train() client id: f_00005-11-0 loss: 0.805472  [   32/  146]
train() client id: f_00005-11-1 loss: 0.452692  [   64/  146]
train() client id: f_00005-11-2 loss: 0.241037  [   96/  146]
train() client id: f_00005-11-3 loss: 0.428914  [  128/  146]
train() client id: f_00006-0-0 loss: 0.588007  [   32/   54]
train() client id: f_00006-1-0 loss: 0.540217  [   32/   54]
train() client id: f_00006-2-0 loss: 0.550881  [   32/   54]
train() client id: f_00006-3-0 loss: 0.591749  [   32/   54]
train() client id: f_00006-4-0 loss: 0.495821  [   32/   54]
train() client id: f_00006-5-0 loss: 0.602405  [   32/   54]
train() client id: f_00006-6-0 loss: 0.554413  [   32/   54]
train() client id: f_00006-7-0 loss: 0.599958  [   32/   54]
train() client id: f_00006-8-0 loss: 0.606020  [   32/   54]
train() client id: f_00006-9-0 loss: 0.607175  [   32/   54]
train() client id: f_00006-10-0 loss: 0.608371  [   32/   54]
train() client id: f_00006-11-0 loss: 0.574217  [   32/   54]
train() client id: f_00007-0-0 loss: 0.732213  [   32/  179]
train() client id: f_00007-0-1 loss: 0.636839  [   64/  179]
train() client id: f_00007-0-2 loss: 0.722866  [   96/  179]
train() client id: f_00007-0-3 loss: 0.583193  [  128/  179]
train() client id: f_00007-0-4 loss: 0.609161  [  160/  179]
train() client id: f_00007-1-0 loss: 0.887984  [   32/  179]
train() client id: f_00007-1-1 loss: 0.601304  [   64/  179]
train() client id: f_00007-1-2 loss: 0.497851  [   96/  179]
train() client id: f_00007-1-3 loss: 0.696966  [  128/  179]
train() client id: f_00007-1-4 loss: 0.459268  [  160/  179]
train() client id: f_00007-2-0 loss: 0.746652  [   32/  179]
train() client id: f_00007-2-1 loss: 0.632670  [   64/  179]
train() client id: f_00007-2-2 loss: 0.507207  [   96/  179]
train() client id: f_00007-2-3 loss: 0.588751  [  128/  179]
train() client id: f_00007-2-4 loss: 0.723431  [  160/  179]
train() client id: f_00007-3-0 loss: 0.555017  [   32/  179]
train() client id: f_00007-3-1 loss: 0.537798  [   64/  179]
train() client id: f_00007-3-2 loss: 0.615972  [   96/  179]
train() client id: f_00007-3-3 loss: 0.887168  [  128/  179]
train() client id: f_00007-3-4 loss: 0.441385  [  160/  179]
train() client id: f_00007-4-0 loss: 0.509794  [   32/  179]
train() client id: f_00007-4-1 loss: 0.761071  [   64/  179]
train() client id: f_00007-4-2 loss: 0.523710  [   96/  179]
train() client id: f_00007-4-3 loss: 0.473838  [  128/  179]
train() client id: f_00007-4-4 loss: 0.643671  [  160/  179]
train() client id: f_00007-5-0 loss: 0.520270  [   32/  179]
train() client id: f_00007-5-1 loss: 0.524829  [   64/  179]
train() client id: f_00007-5-2 loss: 0.677029  [   96/  179]
train() client id: f_00007-5-3 loss: 0.642331  [  128/  179]
train() client id: f_00007-5-4 loss: 0.721019  [  160/  179]
train() client id: f_00007-6-0 loss: 0.433804  [   32/  179]
train() client id: f_00007-6-1 loss: 0.650795  [   64/  179]
train() client id: f_00007-6-2 loss: 0.671973  [   96/  179]
train() client id: f_00007-6-3 loss: 0.618727  [  128/  179]
train() client id: f_00007-6-4 loss: 0.719438  [  160/  179]
train() client id: f_00007-7-0 loss: 0.571041  [   32/  179]
train() client id: f_00007-7-1 loss: 0.506460  [   64/  179]
train() client id: f_00007-7-2 loss: 0.788890  [   96/  179]
train() client id: f_00007-7-3 loss: 0.473138  [  128/  179]
train() client id: f_00007-7-4 loss: 0.541479  [  160/  179]
train() client id: f_00007-8-0 loss: 0.708581  [   32/  179]
train() client id: f_00007-8-1 loss: 0.514597  [   64/  179]
train() client id: f_00007-8-2 loss: 0.470763  [   96/  179]
train() client id: f_00007-8-3 loss: 0.695219  [  128/  179]
train() client id: f_00007-8-4 loss: 0.600245  [  160/  179]
train() client id: f_00007-9-0 loss: 0.710162  [   32/  179]
train() client id: f_00007-9-1 loss: 0.635181  [   64/  179]
train() client id: f_00007-9-2 loss: 0.647676  [   96/  179]
train() client id: f_00007-9-3 loss: 0.558962  [  128/  179]
train() client id: f_00007-9-4 loss: 0.439011  [  160/  179]
train() client id: f_00007-10-0 loss: 0.715109  [   32/  179]
train() client id: f_00007-10-1 loss: 0.656323  [   64/  179]
train() client id: f_00007-10-2 loss: 0.515625  [   96/  179]
train() client id: f_00007-10-3 loss: 0.657339  [  128/  179]
train() client id: f_00007-10-4 loss: 0.435817  [  160/  179]
train() client id: f_00007-11-0 loss: 0.790259  [   32/  179]
train() client id: f_00007-11-1 loss: 0.552532  [   64/  179]
train() client id: f_00007-11-2 loss: 0.605399  [   96/  179]
train() client id: f_00007-11-3 loss: 0.677919  [  128/  179]
train() client id: f_00007-11-4 loss: 0.434307  [  160/  179]
train() client id: f_00008-0-0 loss: 0.764760  [   32/  130]
train() client id: f_00008-0-1 loss: 0.899426  [   64/  130]
train() client id: f_00008-0-2 loss: 0.744397  [   96/  130]
train() client id: f_00008-0-3 loss: 0.745094  [  128/  130]
train() client id: f_00008-1-0 loss: 0.997322  [   32/  130]
train() client id: f_00008-1-1 loss: 0.670520  [   64/  130]
train() client id: f_00008-1-2 loss: 0.738567  [   96/  130]
train() client id: f_00008-1-3 loss: 0.684401  [  128/  130]
train() client id: f_00008-2-0 loss: 0.843850  [   32/  130]
train() client id: f_00008-2-1 loss: 0.796854  [   64/  130]
train() client id: f_00008-2-2 loss: 0.796626  [   96/  130]
train() client id: f_00008-2-3 loss: 0.709853  [  128/  130]
train() client id: f_00008-3-0 loss: 0.663583  [   32/  130]
train() client id: f_00008-3-1 loss: 0.909674  [   64/  130]
train() client id: f_00008-3-2 loss: 0.765586  [   96/  130]
train() client id: f_00008-3-3 loss: 0.762321  [  128/  130]
train() client id: f_00008-4-0 loss: 0.765159  [   32/  130]
train() client id: f_00008-4-1 loss: 0.827471  [   64/  130]
train() client id: f_00008-4-2 loss: 0.801256  [   96/  130]
train() client id: f_00008-4-3 loss: 0.744394  [  128/  130]
train() client id: f_00008-5-0 loss: 0.787074  [   32/  130]
train() client id: f_00008-5-1 loss: 0.656927  [   64/  130]
train() client id: f_00008-5-2 loss: 0.857738  [   96/  130]
train() client id: f_00008-5-3 loss: 0.788516  [  128/  130]
train() client id: f_00008-6-0 loss: 0.764967  [   32/  130]
train() client id: f_00008-6-1 loss: 0.959443  [   64/  130]
train() client id: f_00008-6-2 loss: 0.705767  [   96/  130]
train() client id: f_00008-6-3 loss: 0.709635  [  128/  130]
train() client id: f_00008-7-0 loss: 0.799322  [   32/  130]
train() client id: f_00008-7-1 loss: 0.762909  [   64/  130]
train() client id: f_00008-7-2 loss: 0.801425  [   96/  130]
train() client id: f_00008-7-3 loss: 0.771854  [  128/  130]
train() client id: f_00008-8-0 loss: 0.801702  [   32/  130]
train() client id: f_00008-8-1 loss: 0.815965  [   64/  130]
train() client id: f_00008-8-2 loss: 0.693093  [   96/  130]
train() client id: f_00008-8-3 loss: 0.810010  [  128/  130]
train() client id: f_00008-9-0 loss: 0.865360  [   32/  130]
train() client id: f_00008-9-1 loss: 0.752604  [   64/  130]
train() client id: f_00008-9-2 loss: 0.808120  [   96/  130]
train() client id: f_00008-9-3 loss: 0.715104  [  128/  130]
train() client id: f_00008-10-0 loss: 0.751800  [   32/  130]
train() client id: f_00008-10-1 loss: 0.755976  [   64/  130]
train() client id: f_00008-10-2 loss: 0.815403  [   96/  130]
train() client id: f_00008-10-3 loss: 0.806085  [  128/  130]
train() client id: f_00008-11-0 loss: 0.736428  [   32/  130]
train() client id: f_00008-11-1 loss: 0.756204  [   64/  130]
train() client id: f_00008-11-2 loss: 0.820141  [   96/  130]
train() client id: f_00008-11-3 loss: 0.831008  [  128/  130]
train() client id: f_00009-0-0 loss: 1.176598  [   32/  118]
train() client id: f_00009-0-1 loss: 0.896225  [   64/  118]
train() client id: f_00009-0-2 loss: 1.026520  [   96/  118]
train() client id: f_00009-1-0 loss: 0.935168  [   32/  118]
train() client id: f_00009-1-1 loss: 1.022305  [   64/  118]
train() client id: f_00009-1-2 loss: 0.987213  [   96/  118]
train() client id: f_00009-2-0 loss: 0.872941  [   32/  118]
train() client id: f_00009-2-1 loss: 0.968188  [   64/  118]
train() client id: f_00009-2-2 loss: 0.888194  [   96/  118]
train() client id: f_00009-3-0 loss: 0.916156  [   32/  118]
train() client id: f_00009-3-1 loss: 0.727052  [   64/  118]
train() client id: f_00009-3-2 loss: 0.933890  [   96/  118]
train() client id: f_00009-4-0 loss: 0.741962  [   32/  118]
train() client id: f_00009-4-1 loss: 0.947504  [   64/  118]
train() client id: f_00009-4-2 loss: 0.831914  [   96/  118]
train() client id: f_00009-5-0 loss: 0.814427  [   32/  118]
train() client id: f_00009-5-1 loss: 0.728736  [   64/  118]
train() client id: f_00009-5-2 loss: 0.866239  [   96/  118]
train() client id: f_00009-6-0 loss: 0.749634  [   32/  118]
train() client id: f_00009-6-1 loss: 0.926236  [   64/  118]
train() client id: f_00009-6-2 loss: 0.770289  [   96/  118]
train() client id: f_00009-7-0 loss: 0.886069  [   32/  118]
train() client id: f_00009-7-1 loss: 0.709329  [   64/  118]
train() client id: f_00009-7-2 loss: 0.773567  [   96/  118]
train() client id: f_00009-8-0 loss: 0.758949  [   32/  118]
train() client id: f_00009-8-1 loss: 0.908435  [   64/  118]
train() client id: f_00009-8-2 loss: 0.693043  [   96/  118]
train() client id: f_00009-9-0 loss: 0.748300  [   32/  118]
train() client id: f_00009-9-1 loss: 0.721706  [   64/  118]
train() client id: f_00009-9-2 loss: 0.715081  [   96/  118]
train() client id: f_00009-10-0 loss: 0.719145  [   32/  118]
train() client id: f_00009-10-1 loss: 0.788491  [   64/  118]
train() client id: f_00009-10-2 loss: 0.880175  [   96/  118]
train() client id: f_00009-11-0 loss: 0.802489  [   32/  118]
train() client id: f_00009-11-1 loss: 0.741673  [   64/  118]
train() client id: f_00009-11-2 loss: 0.765031  [   96/  118]
At round 37 accuracy: 0.649867374005305
At round 37 training accuracy: 0.5861837692823608
At round 37 training loss: 0.8276973917680567
update_location
xs = -3.905658 4.200318 205.009024 18.811294 0.979296 3.956410 -167.443192 -146.324852 189.663977 -132.060879 
ys = 197.587959 180.555839 1.320614 -167.455176 159.350187 142.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 221.486469 206.441405 228.101828 195.946678 188.131446 174.389033 195.049002 177.233288 215.130412 165.698785 
dists_bs = 172.578937 179.286422 417.817409 393.495064 176.673794 181.827736 177.785659 176.522289 397.195586 176.307292 
uav_gains = -109.093156 -108.091264 -109.564280 -107.427541 -106.939127 -106.069172 -107.371372 -106.251655 -108.659792 -105.499473 
bs_gains = -102.202717 -102.666386 -112.954682 -112.225359 -102.487879 -102.837543 -102.564167 -102.477447 -112.339183 -102.462627 
Round 38
-------------------------------
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.97555343 12.35186937  5.88797184  2.12582729 14.24522634  6.85479284
  2.63358293  8.40017663  6.2003918   5.55926836]
obj_prev = 70.23466082817666
eta_min = 3.529677745883019e-16	eta_max = 0.9307450633473059
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 16.289713645049638	eta = 0.9090909090909091
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 30.212175618890367	eta = 0.49016101234197246
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 23.329211198156166	eta = 0.6347763094355793
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 22.083685449611664	eta = 0.6705778625672812
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 22.017241376491434	eta = 0.6726015459057763
af = 14.80883058640876	bf = 1.3351291140167194	zeta = 22.017036233942203	eta = 0.6726078128344527
eta = 0.6726078128344527
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [0.03272589 0.06882827 0.03220643 0.01116836 0.07947717 0.03792048
 0.01402538 0.04649155 0.03376481 0.03064806]
ene_total = [1.98224589 3.51656478 1.97276077 0.93820235 4.0071535  2.08744961
 1.06916394 2.54721356 2.15142878 1.74485306]
ti_comp = [0.50594645 0.53574993 0.50279946 0.51575345 0.53633338 0.53518151
 0.51605241 0.52164089 0.47981336 0.53641515]
ti_coms = [0.10065732 0.07085384 0.10380431 0.09085032 0.07027039 0.07142226
 0.09055136 0.08496288 0.12679041 0.07018862]
t_total = [28.09984055 28.09984055 28.09984055 28.09984055 28.09984055 28.09984055
 28.09984055 28.09984055 28.09984055 28.09984055]
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [8.55747057e-06 7.09996571e-05 8.25882640e-06 3.27313532e-07
 1.09078086e-04 1.18986925e-05 6.47495169e-07 2.30812175e-05
 1.04502883e-05 6.25297698e-06]
ene_total = [0.46667354 0.33150672 0.48123759 0.42086309 0.33056794 0.33140208
 0.41949304 0.39464461 0.58781812 0.32542594]
optimize_network iter = 0 obj = 4.0896326721833045
eta = 0.6726078128344527
freqs = [32341261.25530905 64235440.52933019 32027116.84044752 10827227.71361229
 74093064.8966756  35427679.80345908 13589106.57226709 44562788.60807764
 35185358.32378028 28567479.83964483]
eta_min = 0.6726078128344555	eta_max = 0.6726078128344546
af = 0.00954296352207343	bf = 1.3351291140167194	zeta = 0.010497259874280773	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [2.05655253e-06 1.70628135e-05 1.98478162e-06 7.86607991e-08
 2.62139160e-05 2.85952327e-06 1.55607644e-07 5.54693540e-06
 2.51143918e-06 1.50273092e-06]
ene_total = [1.67955348 1.18486183 1.73204102 1.5156187  1.17665519 1.19197501
 1.51064412 1.41831402 2.11559328 1.17116856]
ti_comp = [0.50594645 0.53574993 0.50279946 0.51575345 0.53633338 0.53518151
 0.51605241 0.52164089 0.47981336 0.53641515]
ti_coms = [0.10065732 0.07085384 0.10380431 0.09085032 0.07027039 0.07142226
 0.09055136 0.08496288 0.12679041 0.07018862]
t_total = [28.09984055 28.09984055 28.09984055 28.09984055 28.09984055 28.09984055
 28.09984055 28.09984055 28.09984055 28.09984055]
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [8.55747057e-06 7.09996571e-05 8.25882640e-06 3.27313532e-07
 1.09078086e-04 1.18986925e-05 6.47495169e-07 2.30812175e-05
 1.04502883e-05 6.25297698e-06]
ene_total = [0.46667354 0.33150672 0.48123759 0.42086309 0.33056794 0.33140208
 0.41949304 0.39464461 0.58781812 0.32542594]
optimize_network iter = 1 obj = 4.0896326721833285
eta = 0.6726078128344546
freqs = [32341261.25530905 64235440.52933017 32027116.84044754 10827227.71361229
 74093064.89667559 35427679.80345907 13589106.57226709 44562788.60807764
 35185358.32378031 28567479.83964482]
Done!
ene_coms = [0.01006573 0.00708538 0.01038043 0.00908503 0.00702704 0.00714223
 0.00905514 0.00849629 0.01267904 0.00701886]
ene_comp = [7.90743628e-06 6.56064499e-05 7.63147743e-06 3.02450459e-07
 1.00792402e-04 1.09948555e-05 5.98310768e-07 2.13279445e-05
 9.65647363e-06 5.77799440e-06]
ene_total = [0.01007364 0.00715099 0.01038806 0.00908533 0.00712783 0.00715322
 0.00905573 0.00851762 0.0126887  0.00702464]
At round 38 energy consumption: 0.08826576524222389
At round 38 eta: 0.6726078128344546
At round 38 a_n: 15.165860671420319
At round 38 local rounds: 12.986465296587173
At round 38 global rounds: 46.323221096756726
gradient difference: 0.4320608973503113
train() client id: f_00000-0-0 loss: 1.071987  [   32/  126]
train() client id: f_00000-0-1 loss: 1.116315  [   64/  126]
train() client id: f_00000-0-2 loss: 1.277215  [   96/  126]
train() client id: f_00000-1-0 loss: 1.157448  [   32/  126]
train() client id: f_00000-1-1 loss: 0.982747  [   64/  126]
train() client id: f_00000-1-2 loss: 1.067124  [   96/  126]
train() client id: f_00000-2-0 loss: 1.034874  [   32/  126]
train() client id: f_00000-2-1 loss: 0.921217  [   64/  126]
train() client id: f_00000-2-2 loss: 1.140914  [   96/  126]
train() client id: f_00000-3-0 loss: 1.102094  [   32/  126]
train() client id: f_00000-3-1 loss: 0.935855  [   64/  126]
train() client id: f_00000-3-2 loss: 0.940602  [   96/  126]
train() client id: f_00000-4-0 loss: 0.977849  [   32/  126]
train() client id: f_00000-4-1 loss: 0.839254  [   64/  126]
train() client id: f_00000-4-2 loss: 1.017302  [   96/  126]
train() client id: f_00000-5-0 loss: 0.920117  [   32/  126]
train() client id: f_00000-5-1 loss: 0.828734  [   64/  126]
train() client id: f_00000-5-2 loss: 0.876581  [   96/  126]
train() client id: f_00000-6-0 loss: 0.854200  [   32/  126]
train() client id: f_00000-6-1 loss: 0.856734  [   64/  126]
train() client id: f_00000-6-2 loss: 0.919437  [   96/  126]
train() client id: f_00000-7-0 loss: 0.834317  [   32/  126]
train() client id: f_00000-7-1 loss: 0.826244  [   64/  126]
train() client id: f_00000-7-2 loss: 0.954195  [   96/  126]
train() client id: f_00000-8-0 loss: 0.967288  [   32/  126]
train() client id: f_00000-8-1 loss: 0.898027  [   64/  126]
train() client id: f_00000-8-2 loss: 0.774649  [   96/  126]
train() client id: f_00000-9-0 loss: 0.889606  [   32/  126]
train() client id: f_00000-9-1 loss: 0.860251  [   64/  126]
train() client id: f_00000-9-2 loss: 0.886079  [   96/  126]
train() client id: f_00000-10-0 loss: 0.893347  [   32/  126]
train() client id: f_00000-10-1 loss: 0.869017  [   64/  126]
train() client id: f_00000-10-2 loss: 0.839972  [   96/  126]
train() client id: f_00000-11-0 loss: 0.946593  [   32/  126]
train() client id: f_00000-11-1 loss: 0.762577  [   64/  126]
train() client id: f_00000-11-2 loss: 0.966883  [   96/  126]
train() client id: f_00001-0-0 loss: 0.501256  [   32/  265]
train() client id: f_00001-0-1 loss: 0.412825  [   64/  265]
train() client id: f_00001-0-2 loss: 0.407176  [   96/  265]
train() client id: f_00001-0-3 loss: 0.410955  [  128/  265]
train() client id: f_00001-0-4 loss: 0.342798  [  160/  265]
train() client id: f_00001-0-5 loss: 0.388189  [  192/  265]
train() client id: f_00001-0-6 loss: 0.334323  [  224/  265]
train() client id: f_00001-0-7 loss: 0.430906  [  256/  265]
train() client id: f_00001-1-0 loss: 0.437079  [   32/  265]
train() client id: f_00001-1-1 loss: 0.319425  [   64/  265]
train() client id: f_00001-1-2 loss: 0.467232  [   96/  265]
train() client id: f_00001-1-3 loss: 0.378431  [  128/  265]
train() client id: f_00001-1-4 loss: 0.407073  [  160/  265]
train() client id: f_00001-1-5 loss: 0.400989  [  192/  265]
train() client id: f_00001-1-6 loss: 0.288032  [  224/  265]
train() client id: f_00001-1-7 loss: 0.442817  [  256/  265]
train() client id: f_00001-2-0 loss: 0.331525  [   32/  265]
train() client id: f_00001-2-1 loss: 0.375751  [   64/  265]
train() client id: f_00001-2-2 loss: 0.387397  [   96/  265]
train() client id: f_00001-2-3 loss: 0.317062  [  128/  265]
train() client id: f_00001-2-4 loss: 0.568843  [  160/  265]
train() client id: f_00001-2-5 loss: 0.431100  [  192/  265]
train() client id: f_00001-2-6 loss: 0.306545  [  224/  265]
train() client id: f_00001-2-7 loss: 0.345785  [  256/  265]
train() client id: f_00001-3-0 loss: 0.287255  [   32/  265]
train() client id: f_00001-3-1 loss: 0.367675  [   64/  265]
train() client id: f_00001-3-2 loss: 0.484392  [   96/  265]
train() client id: f_00001-3-3 loss: 0.283381  [  128/  265]
train() client id: f_00001-3-4 loss: 0.358758  [  160/  265]
train() client id: f_00001-3-5 loss: 0.445206  [  192/  265]
train() client id: f_00001-3-6 loss: 0.409934  [  224/  265]
train() client id: f_00001-3-7 loss: 0.401404  [  256/  265]
train() client id: f_00001-4-0 loss: 0.338523  [   32/  265]
train() client id: f_00001-4-1 loss: 0.279622  [   64/  265]
train() client id: f_00001-4-2 loss: 0.378848  [   96/  265]
train() client id: f_00001-4-3 loss: 0.450417  [  128/  265]
train() client id: f_00001-4-4 loss: 0.366258  [  160/  265]
train() client id: f_00001-4-5 loss: 0.358875  [  192/  265]
train() client id: f_00001-4-6 loss: 0.400425  [  224/  265]
train() client id: f_00001-4-7 loss: 0.398887  [  256/  265]
train() client id: f_00001-5-0 loss: 0.416988  [   32/  265]
train() client id: f_00001-5-1 loss: 0.356134  [   64/  265]
train() client id: f_00001-5-2 loss: 0.408363  [   96/  265]
train() client id: f_00001-5-3 loss: 0.388896  [  128/  265]
train() client id: f_00001-5-4 loss: 0.285016  [  160/  265]
train() client id: f_00001-5-5 loss: 0.319155  [  192/  265]
train() client id: f_00001-5-6 loss: 0.300117  [  224/  265]
train() client id: f_00001-5-7 loss: 0.340169  [  256/  265]
train() client id: f_00001-6-0 loss: 0.341225  [   32/  265]
train() client id: f_00001-6-1 loss: 0.412469  [   64/  265]
train() client id: f_00001-6-2 loss: 0.356642  [   96/  265]
train() client id: f_00001-6-3 loss: 0.399707  [  128/  265]
train() client id: f_00001-6-4 loss: 0.278786  [  160/  265]
train() client id: f_00001-6-5 loss: 0.383235  [  192/  265]
train() client id: f_00001-6-6 loss: 0.380716  [  224/  265]
train() client id: f_00001-6-7 loss: 0.445042  [  256/  265]
train() client id: f_00001-7-0 loss: 0.273682  [   32/  265]
train() client id: f_00001-7-1 loss: 0.382257  [   64/  265]
train() client id: f_00001-7-2 loss: 0.384358  [   96/  265]
train() client id: f_00001-7-3 loss: 0.393470  [  128/  265]
train() client id: f_00001-7-4 loss: 0.506109  [  160/  265]
train() client id: f_00001-7-5 loss: 0.361595  [  192/  265]
train() client id: f_00001-7-6 loss: 0.350023  [  224/  265]
train() client id: f_00001-7-7 loss: 0.325572  [  256/  265]
train() client id: f_00001-8-0 loss: 0.348729  [   32/  265]
train() client id: f_00001-8-1 loss: 0.305047  [   64/  265]
train() client id: f_00001-8-2 loss: 0.388668  [   96/  265]
train() client id: f_00001-8-3 loss: 0.368713  [  128/  265]
train() client id: f_00001-8-4 loss: 0.395468  [  160/  265]
train() client id: f_00001-8-5 loss: 0.423523  [  192/  265]
train() client id: f_00001-8-6 loss: 0.335523  [  224/  265]
train() client id: f_00001-8-7 loss: 0.396049  [  256/  265]
train() client id: f_00001-9-0 loss: 0.345660  [   32/  265]
train() client id: f_00001-9-1 loss: 0.280315  [   64/  265]
train() client id: f_00001-9-2 loss: 0.399348  [   96/  265]
train() client id: f_00001-9-3 loss: 0.318492  [  128/  265]
train() client id: f_00001-9-4 loss: 0.367521  [  160/  265]
train() client id: f_00001-9-5 loss: 0.548427  [  192/  265]
train() client id: f_00001-9-6 loss: 0.398543  [  224/  265]
train() client id: f_00001-9-7 loss: 0.307309  [  256/  265]
train() client id: f_00001-10-0 loss: 0.403089  [   32/  265]
train() client id: f_00001-10-1 loss: 0.339952  [   64/  265]
train() client id: f_00001-10-2 loss: 0.566859  [   96/  265]
train() client id: f_00001-10-3 loss: 0.274784  [  128/  265]
train() client id: f_00001-10-4 loss: 0.438752  [  160/  265]
train() client id: f_00001-10-5 loss: 0.296465  [  192/  265]
train() client id: f_00001-10-6 loss: 0.323912  [  224/  265]
train() client id: f_00001-10-7 loss: 0.316616  [  256/  265]
train() client id: f_00001-11-0 loss: 0.366253  [   32/  265]
train() client id: f_00001-11-1 loss: 0.361017  [   64/  265]
train() client id: f_00001-11-2 loss: 0.352340  [   96/  265]
train() client id: f_00001-11-3 loss: 0.329640  [  128/  265]
train() client id: f_00001-11-4 loss: 0.371994  [  160/  265]
train() client id: f_00001-11-5 loss: 0.327725  [  192/  265]
train() client id: f_00001-11-6 loss: 0.449110  [  224/  265]
train() client id: f_00001-11-7 loss: 0.409593  [  256/  265]
train() client id: f_00002-0-0 loss: 0.949510  [   32/  124]
train() client id: f_00002-0-1 loss: 1.157254  [   64/  124]
train() client id: f_00002-0-2 loss: 1.041308  [   96/  124]
train() client id: f_00002-1-0 loss: 0.991447  [   32/  124]
train() client id: f_00002-1-1 loss: 1.098646  [   64/  124]
train() client id: f_00002-1-2 loss: 1.061503  [   96/  124]
train() client id: f_00002-2-0 loss: 1.018457  [   32/  124]
train() client id: f_00002-2-1 loss: 1.032324  [   64/  124]
train() client id: f_00002-2-2 loss: 0.815069  [   96/  124]
train() client id: f_00002-3-0 loss: 0.967936  [   32/  124]
train() client id: f_00002-3-1 loss: 0.770805  [   64/  124]
train() client id: f_00002-3-2 loss: 1.100886  [   96/  124]
train() client id: f_00002-4-0 loss: 0.827239  [   32/  124]
train() client id: f_00002-4-1 loss: 1.039366  [   64/  124]
train() client id: f_00002-4-2 loss: 0.862418  [   96/  124]
train() client id: f_00002-5-0 loss: 0.797569  [   32/  124]
train() client id: f_00002-5-1 loss: 0.933421  [   64/  124]
train() client id: f_00002-5-2 loss: 0.867079  [   96/  124]
train() client id: f_00002-6-0 loss: 0.970559  [   32/  124]
train() client id: f_00002-6-1 loss: 0.790818  [   64/  124]
train() client id: f_00002-6-2 loss: 0.860178  [   96/  124]
train() client id: f_00002-7-0 loss: 0.857626  [   32/  124]
train() client id: f_00002-7-1 loss: 0.858930  [   64/  124]
train() client id: f_00002-7-2 loss: 0.878944  [   96/  124]
train() client id: f_00002-8-0 loss: 1.033761  [   32/  124]
train() client id: f_00002-8-1 loss: 0.768117  [   64/  124]
train() client id: f_00002-8-2 loss: 0.818133  [   96/  124]
train() client id: f_00002-9-0 loss: 0.746441  [   32/  124]
train() client id: f_00002-9-1 loss: 0.998150  [   64/  124]
train() client id: f_00002-9-2 loss: 0.745667  [   96/  124]
train() client id: f_00002-10-0 loss: 1.004870  [   32/  124]
train() client id: f_00002-10-1 loss: 0.740754  [   64/  124]
train() client id: f_00002-10-2 loss: 0.830669  [   96/  124]
train() client id: f_00002-11-0 loss: 0.817495  [   32/  124]
train() client id: f_00002-11-1 loss: 0.976036  [   64/  124]
train() client id: f_00002-11-2 loss: 0.817610  [   96/  124]
train() client id: f_00003-0-0 loss: 0.732086  [   32/   43]
train() client id: f_00003-1-0 loss: 0.800077  [   32/   43]
train() client id: f_00003-2-0 loss: 0.584589  [   32/   43]
train() client id: f_00003-3-0 loss: 0.666514  [   32/   43]
train() client id: f_00003-4-0 loss: 0.674001  [   32/   43]
train() client id: f_00003-5-0 loss: 0.682158  [   32/   43]
train() client id: f_00003-6-0 loss: 0.482921  [   32/   43]
train() client id: f_00003-7-0 loss: 0.774702  [   32/   43]
train() client id: f_00003-8-0 loss: 0.761931  [   32/   43]
train() client id: f_00003-9-0 loss: 0.630332  [   32/   43]
train() client id: f_00003-10-0 loss: 0.638014  [   32/   43]
train() client id: f_00003-11-0 loss: 0.552791  [   32/   43]
train() client id: f_00004-0-0 loss: 0.860218  [   32/  306]
train() client id: f_00004-0-1 loss: 0.965399  [   64/  306]
train() client id: f_00004-0-2 loss: 0.940803  [   96/  306]
train() client id: f_00004-0-3 loss: 0.930178  [  128/  306]
train() client id: f_00004-0-4 loss: 0.899426  [  160/  306]
train() client id: f_00004-0-5 loss: 0.885125  [  192/  306]
train() client id: f_00004-0-6 loss: 0.823557  [  224/  306]
train() client id: f_00004-0-7 loss: 0.886270  [  256/  306]
train() client id: f_00004-0-8 loss: 1.103708  [  288/  306]
train() client id: f_00004-1-0 loss: 1.047106  [   32/  306]
train() client id: f_00004-1-1 loss: 0.887096  [   64/  306]
train() client id: f_00004-1-2 loss: 0.931211  [   96/  306]
train() client id: f_00004-1-3 loss: 0.884464  [  128/  306]
train() client id: f_00004-1-4 loss: 0.868930  [  160/  306]
train() client id: f_00004-1-5 loss: 0.980071  [  192/  306]
train() client id: f_00004-1-6 loss: 0.971945  [  224/  306]
train() client id: f_00004-1-7 loss: 0.928150  [  256/  306]
train() client id: f_00004-1-8 loss: 0.884647  [  288/  306]
train() client id: f_00004-2-0 loss: 0.826384  [   32/  306]
train() client id: f_00004-2-1 loss: 0.803737  [   64/  306]
train() client id: f_00004-2-2 loss: 0.876483  [   96/  306]
train() client id: f_00004-2-3 loss: 0.888606  [  128/  306]
train() client id: f_00004-2-4 loss: 1.068793  [  160/  306]
train() client id: f_00004-2-5 loss: 0.852275  [  192/  306]
train() client id: f_00004-2-6 loss: 0.960405  [  224/  306]
train() client id: f_00004-2-7 loss: 1.002359  [  256/  306]
train() client id: f_00004-2-8 loss: 0.946749  [  288/  306]
train() client id: f_00004-3-0 loss: 0.916400  [   32/  306]
train() client id: f_00004-3-1 loss: 0.899815  [   64/  306]
train() client id: f_00004-3-2 loss: 0.815033  [   96/  306]
train() client id: f_00004-3-3 loss: 0.876572  [  128/  306]
train() client id: f_00004-3-4 loss: 0.861331  [  160/  306]
train() client id: f_00004-3-5 loss: 1.038102  [  192/  306]
train() client id: f_00004-3-6 loss: 0.893723  [  224/  306]
train() client id: f_00004-3-7 loss: 0.936843  [  256/  306]
train() client id: f_00004-3-8 loss: 0.939999  [  288/  306]
train() client id: f_00004-4-0 loss: 0.930147  [   32/  306]
train() client id: f_00004-4-1 loss: 0.955414  [   64/  306]
train() client id: f_00004-4-2 loss: 0.912253  [   96/  306]
train() client id: f_00004-4-3 loss: 0.908858  [  128/  306]
train() client id: f_00004-4-4 loss: 0.867337  [  160/  306]
train() client id: f_00004-4-5 loss: 0.871557  [  192/  306]
train() client id: f_00004-4-6 loss: 0.977880  [  224/  306]
train() client id: f_00004-4-7 loss: 0.813025  [  256/  306]
train() client id: f_00004-4-8 loss: 0.940620  [  288/  306]
train() client id: f_00004-5-0 loss: 0.802639  [   32/  306]
train() client id: f_00004-5-1 loss: 0.981731  [   64/  306]
train() client id: f_00004-5-2 loss: 0.939146  [   96/  306]
train() client id: f_00004-5-3 loss: 0.935272  [  128/  306]
train() client id: f_00004-5-4 loss: 0.781407  [  160/  306]
train() client id: f_00004-5-5 loss: 0.972156  [  192/  306]
train() client id: f_00004-5-6 loss: 0.966304  [  224/  306]
train() client id: f_00004-5-7 loss: 0.897736  [  256/  306]
train() client id: f_00004-5-8 loss: 0.914491  [  288/  306]
train() client id: f_00004-6-0 loss: 0.896191  [   32/  306]
train() client id: f_00004-6-1 loss: 0.920506  [   64/  306]
train() client id: f_00004-6-2 loss: 0.911573  [   96/  306]
train() client id: f_00004-6-3 loss: 0.844919  [  128/  306]
train() client id: f_00004-6-4 loss: 0.938979  [  160/  306]
train() client id: f_00004-6-5 loss: 0.874452  [  192/  306]
train() client id: f_00004-6-6 loss: 0.889377  [  224/  306]
train() client id: f_00004-6-7 loss: 0.931913  [  256/  306]
train() client id: f_00004-6-8 loss: 0.965858  [  288/  306]
train() client id: f_00004-7-0 loss: 1.031858  [   32/  306]
train() client id: f_00004-7-1 loss: 0.952622  [   64/  306]
train() client id: f_00004-7-2 loss: 0.908489  [   96/  306]
train() client id: f_00004-7-3 loss: 0.840367  [  128/  306]
train() client id: f_00004-7-4 loss: 0.858747  [  160/  306]
train() client id: f_00004-7-5 loss: 0.851565  [  192/  306]
train() client id: f_00004-7-6 loss: 0.903594  [  224/  306]
train() client id: f_00004-7-7 loss: 0.904307  [  256/  306]
train() client id: f_00004-7-8 loss: 0.962247  [  288/  306]
train() client id: f_00004-8-0 loss: 0.843225  [   32/  306]
train() client id: f_00004-8-1 loss: 0.948950  [   64/  306]
train() client id: f_00004-8-2 loss: 0.890060  [   96/  306]
train() client id: f_00004-8-3 loss: 0.895986  [  128/  306]
train() client id: f_00004-8-4 loss: 0.930198  [  160/  306]
train() client id: f_00004-8-5 loss: 0.807473  [  192/  306]
train() client id: f_00004-8-6 loss: 1.018727  [  224/  306]
train() client id: f_00004-8-7 loss: 0.926674  [  256/  306]
train() client id: f_00004-8-8 loss: 0.863513  [  288/  306]
train() client id: f_00004-9-0 loss: 1.023606  [   32/  306]
train() client id: f_00004-9-1 loss: 0.963258  [   64/  306]
train() client id: f_00004-9-2 loss: 0.888177  [   96/  306]
train() client id: f_00004-9-3 loss: 0.889384  [  128/  306]
train() client id: f_00004-9-4 loss: 0.841203  [  160/  306]
train() client id: f_00004-9-5 loss: 0.810519  [  192/  306]
train() client id: f_00004-9-6 loss: 0.879025  [  224/  306]
train() client id: f_00004-9-7 loss: 0.954495  [  256/  306]
train() client id: f_00004-9-8 loss: 0.943442  [  288/  306]
train() client id: f_00004-10-0 loss: 1.045685  [   32/  306]
train() client id: f_00004-10-1 loss: 0.859362  [   64/  306]
train() client id: f_00004-10-2 loss: 0.877510  [   96/  306]
train() client id: f_00004-10-3 loss: 0.884289  [  128/  306]
train() client id: f_00004-10-4 loss: 0.790780  [  160/  306]
train() client id: f_00004-10-5 loss: 0.943637  [  192/  306]
train() client id: f_00004-10-6 loss: 0.911140  [  224/  306]
train() client id: f_00004-10-7 loss: 0.801429  [  256/  306]
train() client id: f_00004-10-8 loss: 0.999235  [  288/  306]
train() client id: f_00004-11-0 loss: 0.796279  [   32/  306]
train() client id: f_00004-11-1 loss: 0.852343  [   64/  306]
train() client id: f_00004-11-2 loss: 0.815197  [   96/  306]
train() client id: f_00004-11-3 loss: 0.920722  [  128/  306]
train() client id: f_00004-11-4 loss: 0.941747  [  160/  306]
train() client id: f_00004-11-5 loss: 0.982436  [  192/  306]
train() client id: f_00004-11-6 loss: 0.934154  [  224/  306]
train() client id: f_00004-11-7 loss: 0.828068  [  256/  306]
train() client id: f_00004-11-8 loss: 1.047845  [  288/  306]
train() client id: f_00005-0-0 loss: 0.812916  [   32/  146]
train() client id: f_00005-0-1 loss: 0.460400  [   64/  146]
train() client id: f_00005-0-2 loss: 0.554493  [   96/  146]
train() client id: f_00005-0-3 loss: 0.665287  [  128/  146]
train() client id: f_00005-1-0 loss: 0.417797  [   32/  146]
train() client id: f_00005-1-1 loss: 0.945356  [   64/  146]
train() client id: f_00005-1-2 loss: 0.339444  [   96/  146]
train() client id: f_00005-1-3 loss: 0.846870  [  128/  146]
train() client id: f_00005-2-0 loss: 0.897805  [   32/  146]
train() client id: f_00005-2-1 loss: 0.592617  [   64/  146]
train() client id: f_00005-2-2 loss: 0.543138  [   96/  146]
train() client id: f_00005-2-3 loss: 0.602141  [  128/  146]
train() client id: f_00005-3-0 loss: 0.581111  [   32/  146]
train() client id: f_00005-3-1 loss: 0.885918  [   64/  146]
train() client id: f_00005-3-2 loss: 0.572923  [   96/  146]
train() client id: f_00005-3-3 loss: 0.762798  [  128/  146]
train() client id: f_00005-4-0 loss: 0.589631  [   32/  146]
train() client id: f_00005-4-1 loss: 0.657516  [   64/  146]
train() client id: f_00005-4-2 loss: 0.825373  [   96/  146]
train() client id: f_00005-4-3 loss: 0.561683  [  128/  146]
train() client id: f_00005-5-0 loss: 0.907574  [   32/  146]
train() client id: f_00005-5-1 loss: 0.485698  [   64/  146]
train() client id: f_00005-5-2 loss: 0.517312  [   96/  146]
train() client id: f_00005-5-3 loss: 0.648133  [  128/  146]
train() client id: f_00005-6-0 loss: 0.335452  [   32/  146]
train() client id: f_00005-6-1 loss: 0.751337  [   64/  146]
train() client id: f_00005-6-2 loss: 0.769760  [   96/  146]
train() client id: f_00005-6-3 loss: 0.565417  [  128/  146]
train() client id: f_00005-7-0 loss: 0.332907  [   32/  146]
train() client id: f_00005-7-1 loss: 0.975162  [   64/  146]
train() client id: f_00005-7-2 loss: 0.637714  [   96/  146]
train() client id: f_00005-7-3 loss: 0.665306  [  128/  146]
train() client id: f_00005-8-0 loss: 0.370125  [   32/  146]
train() client id: f_00005-8-1 loss: 0.643296  [   64/  146]
train() client id: f_00005-8-2 loss: 0.736630  [   96/  146]
train() client id: f_00005-8-3 loss: 0.719697  [  128/  146]
train() client id: f_00005-9-0 loss: 0.631762  [   32/  146]
train() client id: f_00005-9-1 loss: 0.571219  [   64/  146]
train() client id: f_00005-9-2 loss: 0.484405  [   96/  146]
train() client id: f_00005-9-3 loss: 0.802105  [  128/  146]
train() client id: f_00005-10-0 loss: 0.832713  [   32/  146]
train() client id: f_00005-10-1 loss: 0.543912  [   64/  146]
train() client id: f_00005-10-2 loss: 0.686099  [   96/  146]
train() client id: f_00005-10-3 loss: 0.471960  [  128/  146]
train() client id: f_00005-11-0 loss: 0.846486  [   32/  146]
train() client id: f_00005-11-1 loss: 0.733115  [   64/  146]
train() client id: f_00005-11-2 loss: 0.458017  [   96/  146]
train() client id: f_00005-11-3 loss: 0.582079  [  128/  146]
train() client id: f_00006-0-0 loss: 0.569688  [   32/   54]
train() client id: f_00006-1-0 loss: 0.524778  [   32/   54]
train() client id: f_00006-2-0 loss: 0.562174  [   32/   54]
train() client id: f_00006-3-0 loss: 0.554965  [   32/   54]
train() client id: f_00006-4-0 loss: 0.520102  [   32/   54]
train() client id: f_00006-5-0 loss: 0.471484  [   32/   54]
train() client id: f_00006-6-0 loss: 0.511423  [   32/   54]
train() client id: f_00006-7-0 loss: 0.577954  [   32/   54]
train() client id: f_00006-8-0 loss: 0.543324  [   32/   54]
train() client id: f_00006-9-0 loss: 0.528713  [   32/   54]
train() client id: f_00006-10-0 loss: 0.528535  [   32/   54]
train() client id: f_00006-11-0 loss: 0.483144  [   32/   54]
train() client id: f_00007-0-0 loss: 0.748606  [   32/  179]
train() client id: f_00007-0-1 loss: 0.638737  [   64/  179]
train() client id: f_00007-0-2 loss: 1.037148  [   96/  179]
train() client id: f_00007-0-3 loss: 0.686330  [  128/  179]
train() client id: f_00007-0-4 loss: 0.646687  [  160/  179]
train() client id: f_00007-1-0 loss: 0.588517  [   32/  179]
train() client id: f_00007-1-1 loss: 0.738847  [   64/  179]
train() client id: f_00007-1-2 loss: 0.867714  [   96/  179]
train() client id: f_00007-1-3 loss: 0.749917  [  128/  179]
train() client id: f_00007-1-4 loss: 0.726977  [  160/  179]
train() client id: f_00007-2-0 loss: 0.598254  [   32/  179]
train() client id: f_00007-2-1 loss: 0.866031  [   64/  179]
train() client id: f_00007-2-2 loss: 0.746876  [   96/  179]
train() client id: f_00007-2-3 loss: 0.731402  [  128/  179]
train() client id: f_00007-2-4 loss: 0.679259  [  160/  179]
train() client id: f_00007-3-0 loss: 0.628007  [   32/  179]
train() client id: f_00007-3-1 loss: 1.030387  [   64/  179]
train() client id: f_00007-3-2 loss: 0.591534  [   96/  179]
train() client id: f_00007-3-3 loss: 0.666840  [  128/  179]
train() client id: f_00007-3-4 loss: 0.683805  [  160/  179]
train() client id: f_00007-4-0 loss: 0.738080  [   32/  179]
train() client id: f_00007-4-1 loss: 0.644480  [   64/  179]
train() client id: f_00007-4-2 loss: 0.621357  [   96/  179]
train() client id: f_00007-4-3 loss: 0.686696  [  128/  179]
train() client id: f_00007-4-4 loss: 0.905842  [  160/  179]
train() client id: f_00007-5-0 loss: 0.783033  [   32/  179]
train() client id: f_00007-5-1 loss: 0.804247  [   64/  179]
train() client id: f_00007-5-2 loss: 0.696268  [   96/  179]
train() client id: f_00007-5-3 loss: 0.568140  [  128/  179]
train() client id: f_00007-5-4 loss: 0.726115  [  160/  179]
train() client id: f_00007-6-0 loss: 0.622835  [   32/  179]
train() client id: f_00007-6-1 loss: 0.638924  [   64/  179]
train() client id: f_00007-6-2 loss: 0.726473  [   96/  179]
train() client id: f_00007-6-3 loss: 0.678444  [  128/  179]
train() client id: f_00007-6-4 loss: 0.556204  [  160/  179]
train() client id: f_00007-7-0 loss: 0.568095  [   32/  179]
train() client id: f_00007-7-1 loss: 0.920756  [   64/  179]
train() client id: f_00007-7-2 loss: 0.809625  [   96/  179]
train() client id: f_00007-7-3 loss: 0.593815  [  128/  179]
train() client id: f_00007-7-4 loss: 0.633890  [  160/  179]
train() client id: f_00007-8-0 loss: 0.897678  [   32/  179]
train() client id: f_00007-8-1 loss: 0.620532  [   64/  179]
train() client id: f_00007-8-2 loss: 0.554734  [   96/  179]
train() client id: f_00007-8-3 loss: 0.599858  [  128/  179]
train() client id: f_00007-8-4 loss: 0.739340  [  160/  179]
train() client id: f_00007-9-0 loss: 0.823976  [   32/  179]
train() client id: f_00007-9-1 loss: 0.543706  [   64/  179]
train() client id: f_00007-9-2 loss: 0.810516  [   96/  179]
train() client id: f_00007-9-3 loss: 0.539952  [  128/  179]
train() client id: f_00007-9-4 loss: 0.707956  [  160/  179]
train() client id: f_00007-10-0 loss: 0.593016  [   32/  179]
train() client id: f_00007-10-1 loss: 0.566844  [   64/  179]
train() client id: f_00007-10-2 loss: 0.633914  [   96/  179]
train() client id: f_00007-10-3 loss: 0.701404  [  128/  179]
train() client id: f_00007-10-4 loss: 0.785710  [  160/  179]
train() client id: f_00007-11-0 loss: 0.541428  [   32/  179]
train() client id: f_00007-11-1 loss: 0.868750  [   64/  179]
train() client id: f_00007-11-2 loss: 0.640640  [   96/  179]
train() client id: f_00007-11-3 loss: 0.727991  [  128/  179]
train() client id: f_00007-11-4 loss: 0.707064  [  160/  179]
train() client id: f_00008-0-0 loss: 0.696031  [   32/  130]
train() client id: f_00008-0-1 loss: 0.689223  [   64/  130]
train() client id: f_00008-0-2 loss: 0.616410  [   96/  130]
train() client id: f_00008-0-3 loss: 0.713982  [  128/  130]
train() client id: f_00008-1-0 loss: 0.848357  [   32/  130]
train() client id: f_00008-1-1 loss: 0.666080  [   64/  130]
train() client id: f_00008-1-2 loss: 0.555323  [   96/  130]
train() client id: f_00008-1-3 loss: 0.631066  [  128/  130]
train() client id: f_00008-2-0 loss: 0.613813  [   32/  130]
train() client id: f_00008-2-1 loss: 0.748628  [   64/  130]
train() client id: f_00008-2-2 loss: 0.724429  [   96/  130]
train() client id: f_00008-2-3 loss: 0.586671  [  128/  130]
train() client id: f_00008-3-0 loss: 0.655553  [   32/  130]
train() client id: f_00008-3-1 loss: 0.768026  [   64/  130]
train() client id: f_00008-3-2 loss: 0.630091  [   96/  130]
train() client id: f_00008-3-3 loss: 0.650676  [  128/  130]
train() client id: f_00008-4-0 loss: 0.698574  [   32/  130]
train() client id: f_00008-4-1 loss: 0.640993  [   64/  130]
train() client id: f_00008-4-2 loss: 0.724396  [   96/  130]
train() client id: f_00008-4-3 loss: 0.612921  [  128/  130]
train() client id: f_00008-5-0 loss: 0.704307  [   32/  130]
train() client id: f_00008-5-1 loss: 0.624461  [   64/  130]
train() client id: f_00008-5-2 loss: 0.645615  [   96/  130]
train() client id: f_00008-5-3 loss: 0.681538  [  128/  130]
train() client id: f_00008-6-0 loss: 0.691442  [   32/  130]
train() client id: f_00008-6-1 loss: 0.754026  [   64/  130]
train() client id: f_00008-6-2 loss: 0.651662  [   96/  130]
train() client id: f_00008-6-3 loss: 0.556643  [  128/  130]
train() client id: f_00008-7-0 loss: 0.710243  [   32/  130]
train() client id: f_00008-7-1 loss: 0.625730  [   64/  130]
train() client id: f_00008-7-2 loss: 0.726558  [   96/  130]
train() client id: f_00008-7-3 loss: 0.588440  [  128/  130]
train() client id: f_00008-8-0 loss: 0.625266  [   32/  130]
train() client id: f_00008-8-1 loss: 0.749896  [   64/  130]
train() client id: f_00008-8-2 loss: 0.626471  [   96/  130]
train() client id: f_00008-8-3 loss: 0.696039  [  128/  130]
train() client id: f_00008-9-0 loss: 0.679540  [   32/  130]
train() client id: f_00008-9-1 loss: 0.708400  [   64/  130]
train() client id: f_00008-9-2 loss: 0.704791  [   96/  130]
train() client id: f_00008-9-3 loss: 0.594283  [  128/  130]
train() client id: f_00008-10-0 loss: 0.734600  [   32/  130]
train() client id: f_00008-10-1 loss: 0.589609  [   64/  130]
train() client id: f_00008-10-2 loss: 0.708699  [   96/  130]
train() client id: f_00008-10-3 loss: 0.658716  [  128/  130]
train() client id: f_00008-11-0 loss: 0.637235  [   32/  130]
train() client id: f_00008-11-1 loss: 0.649351  [   64/  130]
train() client id: f_00008-11-2 loss: 0.578497  [   96/  130]
train() client id: f_00008-11-3 loss: 0.817001  [  128/  130]
train() client id: f_00009-0-0 loss: 1.162515  [   32/  118]
train() client id: f_00009-0-1 loss: 1.024220  [   64/  118]
train() client id: f_00009-0-2 loss: 0.936661  [   96/  118]
train() client id: f_00009-1-0 loss: 0.991742  [   32/  118]
train() client id: f_00009-1-1 loss: 0.974441  [   64/  118]
train() client id: f_00009-1-2 loss: 0.881426  [   96/  118]
train() client id: f_00009-2-0 loss: 0.973260  [   32/  118]
train() client id: f_00009-2-1 loss: 1.002470  [   64/  118]
train() client id: f_00009-2-2 loss: 0.957803  [   96/  118]
train() client id: f_00009-3-0 loss: 0.972504  [   32/  118]
train() client id: f_00009-3-1 loss: 1.107155  [   64/  118]
train() client id: f_00009-3-2 loss: 0.830724  [   96/  118]
train() client id: f_00009-4-0 loss: 0.910886  [   32/  118]
train() client id: f_00009-4-1 loss: 0.945599  [   64/  118]
train() client id: f_00009-4-2 loss: 0.861192  [   96/  118]
train() client id: f_00009-5-0 loss: 0.881854  [   32/  118]
train() client id: f_00009-5-1 loss: 0.734076  [   64/  118]
train() client id: f_00009-5-2 loss: 0.990409  [   96/  118]
train() client id: f_00009-6-0 loss: 0.972605  [   32/  118]
train() client id: f_00009-6-1 loss: 0.806119  [   64/  118]
train() client id: f_00009-6-2 loss: 0.843453  [   96/  118]
train() client id: f_00009-7-0 loss: 0.802424  [   32/  118]
train() client id: f_00009-7-1 loss: 0.842890  [   64/  118]
train() client id: f_00009-7-2 loss: 0.907617  [   96/  118]
train() client id: f_00009-8-0 loss: 0.828686  [   32/  118]
train() client id: f_00009-8-1 loss: 0.903331  [   64/  118]
train() client id: f_00009-8-2 loss: 0.808237  [   96/  118]
train() client id: f_00009-9-0 loss: 0.771055  [   32/  118]
train() client id: f_00009-9-1 loss: 0.700743  [   64/  118]
train() client id: f_00009-9-2 loss: 0.903607  [   96/  118]
train() client id: f_00009-10-0 loss: 0.663333  [   32/  118]
train() client id: f_00009-10-1 loss: 1.020957  [   64/  118]
train() client id: f_00009-10-2 loss: 0.769354  [   96/  118]
train() client id: f_00009-11-0 loss: 0.815466  [   32/  118]
train() client id: f_00009-11-1 loss: 0.742503  [   64/  118]
train() client id: f_00009-11-2 loss: 0.769952  [   96/  118]
At round 38 accuracy: 0.6525198938992043
At round 38 training accuracy: 0.5828303152246814
At round 38 training loss: 0.8415784226570143
update_location
xs = -3.905658 4.200318 210.009024 18.811294 0.979296 3.956410 -172.443192 -151.324852 194.663977 -137.060879 
ys = 202.587959 185.555839 1.320614 -172.455176 164.350187 147.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 225.958261 210.828395 232.605963 200.236491 192.384882 178.506796 199.357832 181.383260 219.551210 169.710625 
dists_bs = 173.304268 179.510946 422.370072 397.854140 176.301251 181.009577 177.643385 175.779314 401.790958 175.156702 
uav_gains = -109.409174 -108.375383 -109.898343 -107.696901 -107.204881 -106.332848 -107.641568 -106.515236 -108.959362 -105.764996 
bs_gains = -102.253718 -102.681605 -113.086467 -112.359328 -102.462210 -102.782703 -102.554432 -102.426157 -112.479064 -102.383008 
Round 39
-------------------------------
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.84371614 12.07295727  5.75849455  2.07999855 13.92335007  6.69969194
  2.5762787   8.21232374  6.06245015  5.43331894]
obj_prev = 68.66258004963703
eta_min = 1.603615121775097e-16	eta_max = 0.9314544727787696
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 15.921783734685466	eta = 0.909090909090909
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 29.685339818895304	eta = 0.4875924930628838
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 22.863992502600777	eta = 0.6330630509114977
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 21.629256357976853	eta = 0.6692023345673616
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 21.563057577188566	eta = 0.6712567917560257
af = 14.474348849714058	bf = 1.3191144648464428	zeta = 21.562850936268624	eta = 0.6712632245380996
eta = 0.6712632245380996
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [0.03289102 0.06917555 0.03236894 0.01122471 0.07987818 0.03811181
 0.01409615 0.04672613 0.03393518 0.0308027 ]
ene_total = [1.94642197 3.43896482 1.93822068 0.92238666 3.91833323 2.03975789
 1.05049566 2.49573328 2.10825374 1.704283  ]
ti_comp = [0.51931345 0.55115743 0.51591145 0.5297508  0.55187418 0.5508223
 0.53005444 0.53584373 0.4938656  0.55212944]
ti_coms = [0.102748   0.07090402 0.10615    0.09231065 0.07018727 0.07123916
 0.09200701 0.08621772 0.12819586 0.06993201]
t_total = [28.04983635 28.04983635 28.04983635 28.04983635 28.04983635 28.04983635
 28.04983635 28.04983635 28.04983635 28.04983635]
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [8.24617860e-06 6.81061808e-05 7.96370126e-06 3.14965047e-07
 1.04588604e-04 1.14034376e-05 6.23075349e-07 2.22066489e-05
 1.00141191e-05 5.99189852e-06]
ene_total = [0.4636805  0.32278962 0.47900796 0.41625902 0.32120273 0.32174399
 0.41490377 0.38977208 0.57850904 0.31560581]
optimize_network iter = 0 obj = 4.023474514862622
eta = 0.6712632245380996
freqs = [31667787.3789384  62754800.17478311 31370631.5489173  10594331.96949695
 72369920.453618   34595378.85521331 13296888.56956221 43600515.27273497
 34356690.70712458 27894452.74815087]
eta_min = 0.6712632245381114	eta_max = 0.6712632245380962
af = 0.008905236727720691	bf = 1.3191144648464428	zeta = 0.009795760400492761	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [1.97179310e-06 1.62852764e-05 1.90424825e-06 7.53131768e-08
 2.50088069e-05 2.72674418e-06 1.48987275e-07 5.30996467e-06
 2.39453594e-06 1.43275870e-06]
ene_total = [1.67569409 1.15879127 1.73115483 1.50519706 1.14852664 1.1620451
 1.50025809 1.40670146 2.09070673 1.14052021]
ti_comp = [0.51931345 0.55115743 0.51591145 0.5297508  0.55187418 0.5508223
 0.53005444 0.53584373 0.4938656  0.55212944]
ti_coms = [0.102748   0.07090402 0.10615    0.09231065 0.07018727 0.07123916
 0.09200701 0.08621772 0.12819586 0.06993201]
t_total = [28.04983635 28.04983635 28.04983635 28.04983635 28.04983635 28.04983635
 28.04983635 28.04983635 28.04983635 28.04983635]
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [8.24617860e-06 6.81061808e-05 7.96370126e-06 3.14965047e-07
 1.04588604e-04 1.14034376e-05 6.23075349e-07 2.22066489e-05
 1.00141191e-05 5.99189852e-06]
ene_total = [0.4636805  0.32278962 0.47900796 0.41625902 0.32120273 0.32174399
 0.41490377 0.38977208 0.57850904 0.31560581]
optimize_network iter = 1 obj = 4.0234745148625795
eta = 0.6712632245380962
freqs = [31667787.37893841 62754800.17478317 31370631.54891731 10594331.96949696
 72369920.4536181  34595378.85521334 13296888.56956222 43600515.27273501
 34356690.70712457 27894452.7481509 ]
Done!
ene_coms = [0.0102748  0.0070904  0.010615   0.00923106 0.00701873 0.00712392
 0.0092007  0.00862177 0.01281959 0.0069932 ]
ene_comp = [8.21333132e-06 6.78348912e-05 7.93197918e-06 3.13710436e-07
 1.04171993e-04 1.13580139e-05 6.20593432e-07 2.21181924e-05
 9.97422956e-06 5.96803079e-06]
ene_total = [0.01028301 0.00715824 0.01062293 0.00923138 0.0071229  0.00713527
 0.00920132 0.00864389 0.01282956 0.00699917]
At round 39 energy consumption: 0.08922767374894407
At round 39 eta: 0.6712632245380962
At round 39 a_n: 14.823314824451002
At round 39 local rounds: 13.051990433476096
At round 39 global rounds: 45.091744918477566
gradient difference: 0.4915880262851715
train() client id: f_00000-0-0 loss: 1.034867  [   32/  126]
train() client id: f_00000-0-1 loss: 1.296220  [   64/  126]
train() client id: f_00000-0-2 loss: 1.018379  [   96/  126]
train() client id: f_00000-1-0 loss: 1.020076  [   32/  126]
train() client id: f_00000-1-1 loss: 0.799479  [   64/  126]
train() client id: f_00000-1-2 loss: 0.896215  [   96/  126]
train() client id: f_00000-2-0 loss: 0.969739  [   32/  126]
train() client id: f_00000-2-1 loss: 0.896070  [   64/  126]
train() client id: f_00000-2-2 loss: 0.968478  [   96/  126]
train() client id: f_00000-3-0 loss: 0.933721  [   32/  126]
train() client id: f_00000-3-1 loss: 0.765085  [   64/  126]
train() client id: f_00000-3-2 loss: 0.816117  [   96/  126]
train() client id: f_00000-4-0 loss: 0.837195  [   32/  126]
train() client id: f_00000-4-1 loss: 0.780268  [   64/  126]
train() client id: f_00000-4-2 loss: 0.880012  [   96/  126]
train() client id: f_00000-5-0 loss: 0.823600  [   32/  126]
train() client id: f_00000-5-1 loss: 0.779043  [   64/  126]
train() client id: f_00000-5-2 loss: 0.803529  [   96/  126]
train() client id: f_00000-6-0 loss: 0.903062  [   32/  126]
train() client id: f_00000-6-1 loss: 0.655059  [   64/  126]
train() client id: f_00000-6-2 loss: 0.854278  [   96/  126]
train() client id: f_00000-7-0 loss: 0.753788  [   32/  126]
train() client id: f_00000-7-1 loss: 0.731299  [   64/  126]
train() client id: f_00000-7-2 loss: 0.712540  [   96/  126]
train() client id: f_00000-8-0 loss: 0.677456  [   32/  126]
train() client id: f_00000-8-1 loss: 0.773140  [   64/  126]
train() client id: f_00000-8-2 loss: 0.821739  [   96/  126]
train() client id: f_00000-9-0 loss: 0.720696  [   32/  126]
train() client id: f_00000-9-1 loss: 0.745213  [   64/  126]
train() client id: f_00000-9-2 loss: 0.849290  [   96/  126]
train() client id: f_00000-10-0 loss: 0.762723  [   32/  126]
train() client id: f_00000-10-1 loss: 0.648485  [   64/  126]
train() client id: f_00000-10-2 loss: 0.833793  [   96/  126]
train() client id: f_00000-11-0 loss: 0.756804  [   32/  126]
train() client id: f_00000-11-1 loss: 0.828462  [   64/  126]
train() client id: f_00000-11-2 loss: 0.798809  [   96/  126]
train() client id: f_00000-12-0 loss: 0.951478  [   32/  126]
train() client id: f_00000-12-1 loss: 0.711042  [   64/  126]
train() client id: f_00000-12-2 loss: 0.769195  [   96/  126]
train() client id: f_00001-0-0 loss: 0.540368  [   32/  265]
train() client id: f_00001-0-1 loss: 0.432690  [   64/  265]
train() client id: f_00001-0-2 loss: 0.359136  [   96/  265]
train() client id: f_00001-0-3 loss: 0.386028  [  128/  265]
train() client id: f_00001-0-4 loss: 0.426963  [  160/  265]
train() client id: f_00001-0-5 loss: 0.663920  [  192/  265]
train() client id: f_00001-0-6 loss: 0.434741  [  224/  265]
train() client id: f_00001-0-7 loss: 0.375677  [  256/  265]
train() client id: f_00001-1-0 loss: 0.494696  [   32/  265]
train() client id: f_00001-1-1 loss: 0.470036  [   64/  265]
train() client id: f_00001-1-2 loss: 0.498327  [   96/  265]
train() client id: f_00001-1-3 loss: 0.369571  [  128/  265]
train() client id: f_00001-1-4 loss: 0.454760  [  160/  265]
train() client id: f_00001-1-5 loss: 0.443267  [  192/  265]
train() client id: f_00001-1-6 loss: 0.361050  [  224/  265]
train() client id: f_00001-1-7 loss: 0.420320  [  256/  265]
train() client id: f_00001-2-0 loss: 0.428356  [   32/  265]
train() client id: f_00001-2-1 loss: 0.429827  [   64/  265]
train() client id: f_00001-2-2 loss: 0.394661  [   96/  265]
train() client id: f_00001-2-3 loss: 0.413256  [  128/  265]
train() client id: f_00001-2-4 loss: 0.428392  [  160/  265]
train() client id: f_00001-2-5 loss: 0.516362  [  192/  265]
train() client id: f_00001-2-6 loss: 0.546866  [  224/  265]
train() client id: f_00001-2-7 loss: 0.334168  [  256/  265]
train() client id: f_00001-3-0 loss: 0.437745  [   32/  265]
train() client id: f_00001-3-1 loss: 0.344314  [   64/  265]
train() client id: f_00001-3-2 loss: 0.363207  [   96/  265]
train() client id: f_00001-3-3 loss: 0.420988  [  128/  265]
train() client id: f_00001-3-4 loss: 0.464046  [  160/  265]
train() client id: f_00001-3-5 loss: 0.504465  [  192/  265]
train() client id: f_00001-3-6 loss: 0.371752  [  224/  265]
train() client id: f_00001-3-7 loss: 0.550180  [  256/  265]
train() client id: f_00001-4-0 loss: 0.547752  [   32/  265]
train() client id: f_00001-4-1 loss: 0.346576  [   64/  265]
train() client id: f_00001-4-2 loss: 0.344304  [   96/  265]
train() client id: f_00001-4-3 loss: 0.401348  [  128/  265]
train() client id: f_00001-4-4 loss: 0.371744  [  160/  265]
train() client id: f_00001-4-5 loss: 0.477162  [  192/  265]
train() client id: f_00001-4-6 loss: 0.343381  [  224/  265]
train() client id: f_00001-4-7 loss: 0.552946  [  256/  265]
train() client id: f_00001-5-0 loss: 0.409343  [   32/  265]
train() client id: f_00001-5-1 loss: 0.465350  [   64/  265]
train() client id: f_00001-5-2 loss: 0.414620  [   96/  265]
train() client id: f_00001-5-3 loss: 0.395543  [  128/  265]
train() client id: f_00001-5-4 loss: 0.548640  [  160/  265]
train() client id: f_00001-5-5 loss: 0.403923  [  192/  265]
train() client id: f_00001-5-6 loss: 0.430412  [  224/  265]
train() client id: f_00001-5-7 loss: 0.329656  [  256/  265]
train() client id: f_00001-6-0 loss: 0.495220  [   32/  265]
train() client id: f_00001-6-1 loss: 0.407916  [   64/  265]
train() client id: f_00001-6-2 loss: 0.355798  [   96/  265]
train() client id: f_00001-6-3 loss: 0.461637  [  128/  265]
train() client id: f_00001-6-4 loss: 0.432624  [  160/  265]
train() client id: f_00001-6-5 loss: 0.334018  [  192/  265]
train() client id: f_00001-6-6 loss: 0.518132  [  224/  265]
train() client id: f_00001-6-7 loss: 0.366418  [  256/  265]
train() client id: f_00001-7-0 loss: 0.343192  [   32/  265]
train() client id: f_00001-7-1 loss: 0.529035  [   64/  265]
train() client id: f_00001-7-2 loss: 0.378130  [   96/  265]
train() client id: f_00001-7-3 loss: 0.360808  [  128/  265]
train() client id: f_00001-7-4 loss: 0.355787  [  160/  265]
train() client id: f_00001-7-5 loss: 0.377276  [  192/  265]
train() client id: f_00001-7-6 loss: 0.468757  [  224/  265]
train() client id: f_00001-7-7 loss: 0.456078  [  256/  265]
train() client id: f_00001-8-0 loss: 0.385480  [   32/  265]
train() client id: f_00001-8-1 loss: 0.326221  [   64/  265]
train() client id: f_00001-8-2 loss: 0.501790  [   96/  265]
train() client id: f_00001-8-3 loss: 0.565645  [  128/  265]
train() client id: f_00001-8-4 loss: 0.414119  [  160/  265]
train() client id: f_00001-8-5 loss: 0.317154  [  192/  265]
train() client id: f_00001-8-6 loss: 0.324441  [  224/  265]
train() client id: f_00001-8-7 loss: 0.465703  [  256/  265]
train() client id: f_00001-9-0 loss: 0.313718  [   32/  265]
train() client id: f_00001-9-1 loss: 0.320781  [   64/  265]
train() client id: f_00001-9-2 loss: 0.340581  [   96/  265]
train() client id: f_00001-9-3 loss: 0.459187  [  128/  265]
train() client id: f_00001-9-4 loss: 0.569489  [  160/  265]
train() client id: f_00001-9-5 loss: 0.330791  [  192/  265]
train() client id: f_00001-9-6 loss: 0.438639  [  224/  265]
train() client id: f_00001-9-7 loss: 0.515206  [  256/  265]
train() client id: f_00001-10-0 loss: 0.500507  [   32/  265]
train() client id: f_00001-10-1 loss: 0.391580  [   64/  265]
train() client id: f_00001-10-2 loss: 0.470241  [   96/  265]
train() client id: f_00001-10-3 loss: 0.504119  [  128/  265]
train() client id: f_00001-10-4 loss: 0.371040  [  160/  265]
train() client id: f_00001-10-5 loss: 0.365187  [  192/  265]
train() client id: f_00001-10-6 loss: 0.330469  [  224/  265]
train() client id: f_00001-10-7 loss: 0.365579  [  256/  265]
train() client id: f_00001-11-0 loss: 0.421847  [   32/  265]
train() client id: f_00001-11-1 loss: 0.326306  [   64/  265]
train() client id: f_00001-11-2 loss: 0.430007  [   96/  265]
train() client id: f_00001-11-3 loss: 0.394634  [  128/  265]
train() client id: f_00001-11-4 loss: 0.404901  [  160/  265]
train() client id: f_00001-11-5 loss: 0.429718  [  192/  265]
train() client id: f_00001-11-6 loss: 0.519784  [  224/  265]
train() client id: f_00001-11-7 loss: 0.417931  [  256/  265]
train() client id: f_00001-12-0 loss: 0.392250  [   32/  265]
train() client id: f_00001-12-1 loss: 0.628983  [   64/  265]
train() client id: f_00001-12-2 loss: 0.429451  [   96/  265]
train() client id: f_00001-12-3 loss: 0.449590  [  128/  265]
train() client id: f_00001-12-4 loss: 0.371678  [  160/  265]
train() client id: f_00001-12-5 loss: 0.322431  [  192/  265]
train() client id: f_00001-12-6 loss: 0.392039  [  224/  265]
train() client id: f_00001-12-7 loss: 0.358067  [  256/  265]
train() client id: f_00002-0-0 loss: 1.259011  [   32/  124]
train() client id: f_00002-0-1 loss: 1.368608  [   64/  124]
train() client id: f_00002-0-2 loss: 1.011920  [   96/  124]
train() client id: f_00002-1-0 loss: 1.221857  [   32/  124]
train() client id: f_00002-1-1 loss: 1.216599  [   64/  124]
train() client id: f_00002-1-2 loss: 1.175192  [   96/  124]
train() client id: f_00002-2-0 loss: 1.044780  [   32/  124]
train() client id: f_00002-2-1 loss: 1.177196  [   64/  124]
train() client id: f_00002-2-2 loss: 1.192596  [   96/  124]
train() client id: f_00002-3-0 loss: 0.892548  [   32/  124]
train() client id: f_00002-3-1 loss: 1.123293  [   64/  124]
train() client id: f_00002-3-2 loss: 1.090390  [   96/  124]
train() client id: f_00002-4-0 loss: 1.097371  [   32/  124]
train() client id: f_00002-4-1 loss: 1.261786  [   64/  124]
train() client id: f_00002-4-2 loss: 1.018759  [   96/  124]
train() client id: f_00002-5-0 loss: 1.149715  [   32/  124]
train() client id: f_00002-5-1 loss: 1.174375  [   64/  124]
train() client id: f_00002-5-2 loss: 0.928450  [   96/  124]
train() client id: f_00002-6-0 loss: 1.066155  [   32/  124]
train() client id: f_00002-6-1 loss: 1.005361  [   64/  124]
train() client id: f_00002-6-2 loss: 1.118465  [   96/  124]
train() client id: f_00002-7-0 loss: 1.010106  [   32/  124]
train() client id: f_00002-7-1 loss: 1.042241  [   64/  124]
train() client id: f_00002-7-2 loss: 1.134081  [   96/  124]
train() client id: f_00002-8-0 loss: 1.037350  [   32/  124]
train() client id: f_00002-8-1 loss: 0.984982  [   64/  124]
train() client id: f_00002-8-2 loss: 0.950278  [   96/  124]
train() client id: f_00002-9-0 loss: 0.937997  [   32/  124]
train() client id: f_00002-9-1 loss: 0.905548  [   64/  124]
train() client id: f_00002-9-2 loss: 1.099917  [   96/  124]
train() client id: f_00002-10-0 loss: 1.142301  [   32/  124]
train() client id: f_00002-10-1 loss: 1.033036  [   64/  124]
train() client id: f_00002-10-2 loss: 0.950746  [   96/  124]
train() client id: f_00002-11-0 loss: 1.118891  [   32/  124]
train() client id: f_00002-11-1 loss: 1.226722  [   64/  124]
train() client id: f_00002-11-2 loss: 0.862024  [   96/  124]
train() client id: f_00002-12-0 loss: 1.066384  [   32/  124]
train() client id: f_00002-12-1 loss: 1.109932  [   64/  124]
train() client id: f_00002-12-2 loss: 0.963502  [   96/  124]
train() client id: f_00003-0-0 loss: 0.822623  [   32/   43]
train() client id: f_00003-1-0 loss: 0.676643  [   32/   43]
train() client id: f_00003-2-0 loss: 0.839666  [   32/   43]
train() client id: f_00003-3-0 loss: 1.012095  [   32/   43]
train() client id: f_00003-4-0 loss: 1.024649  [   32/   43]
train() client id: f_00003-5-0 loss: 0.826983  [   32/   43]
train() client id: f_00003-6-0 loss: 0.893289  [   32/   43]
train() client id: f_00003-7-0 loss: 0.957970  [   32/   43]
train() client id: f_00003-8-0 loss: 0.956795  [   32/   43]
train() client id: f_00003-9-0 loss: 1.025645  [   32/   43]
train() client id: f_00003-10-0 loss: 0.879835  [   32/   43]
train() client id: f_00003-11-0 loss: 0.829120  [   32/   43]
train() client id: f_00003-12-0 loss: 0.719576  [   32/   43]
train() client id: f_00004-0-0 loss: 0.732124  [   32/  306]
train() client id: f_00004-0-1 loss: 0.663816  [   64/  306]
train() client id: f_00004-0-2 loss: 0.697835  [   96/  306]
train() client id: f_00004-0-3 loss: 0.654052  [  128/  306]
train() client id: f_00004-0-4 loss: 0.767044  [  160/  306]
train() client id: f_00004-0-5 loss: 0.654195  [  192/  306]
train() client id: f_00004-0-6 loss: 0.635716  [  224/  306]
train() client id: f_00004-0-7 loss: 0.646307  [  256/  306]
train() client id: f_00004-0-8 loss: 0.720402  [  288/  306]
train() client id: f_00004-1-0 loss: 0.700922  [   32/  306]
train() client id: f_00004-1-1 loss: 0.688102  [   64/  306]
train() client id: f_00004-1-2 loss: 0.734005  [   96/  306]
train() client id: f_00004-1-3 loss: 0.667796  [  128/  306]
train() client id: f_00004-1-4 loss: 0.649514  [  160/  306]
train() client id: f_00004-1-5 loss: 0.878237  [  192/  306]
train() client id: f_00004-1-6 loss: 0.642103  [  224/  306]
train() client id: f_00004-1-7 loss: 0.591832  [  256/  306]
train() client id: f_00004-1-8 loss: 0.798063  [  288/  306]
train() client id: f_00004-2-0 loss: 0.686531  [   32/  306]
train() client id: f_00004-2-1 loss: 0.595153  [   64/  306]
train() client id: f_00004-2-2 loss: 0.697452  [   96/  306]
train() client id: f_00004-2-3 loss: 0.752860  [  128/  306]
train() client id: f_00004-2-4 loss: 0.666100  [  160/  306]
train() client id: f_00004-2-5 loss: 0.751855  [  192/  306]
train() client id: f_00004-2-6 loss: 0.785303  [  224/  306]
train() client id: f_00004-2-7 loss: 0.732663  [  256/  306]
train() client id: f_00004-2-8 loss: 0.656014  [  288/  306]
train() client id: f_00004-3-0 loss: 0.593223  [   32/  306]
train() client id: f_00004-3-1 loss: 0.771122  [   64/  306]
train() client id: f_00004-3-2 loss: 0.803427  [   96/  306]
train() client id: f_00004-3-3 loss: 0.704569  [  128/  306]
train() client id: f_00004-3-4 loss: 0.600111  [  160/  306]
train() client id: f_00004-3-5 loss: 0.910317  [  192/  306]
train() client id: f_00004-3-6 loss: 0.699762  [  224/  306]
train() client id: f_00004-3-7 loss: 0.623930  [  256/  306]
train() client id: f_00004-3-8 loss: 0.724695  [  288/  306]
train() client id: f_00004-4-0 loss: 0.648322  [   32/  306]
train() client id: f_00004-4-1 loss: 0.761422  [   64/  306]
train() client id: f_00004-4-2 loss: 0.811839  [   96/  306]
train() client id: f_00004-4-3 loss: 0.643229  [  128/  306]
train() client id: f_00004-4-4 loss: 0.764407  [  160/  306]
train() client id: f_00004-4-5 loss: 0.749181  [  192/  306]
train() client id: f_00004-4-6 loss: 0.710893  [  224/  306]
train() client id: f_00004-4-7 loss: 0.662543  [  256/  306]
train() client id: f_00004-4-8 loss: 0.731492  [  288/  306]
train() client id: f_00004-5-0 loss: 0.776754  [   32/  306]
train() client id: f_00004-5-1 loss: 0.919738  [   64/  306]
train() client id: f_00004-5-2 loss: 0.689026  [   96/  306]
train() client id: f_00004-5-3 loss: 0.706887  [  128/  306]
train() client id: f_00004-5-4 loss: 0.698709  [  160/  306]
train() client id: f_00004-5-5 loss: 0.680254  [  192/  306]
train() client id: f_00004-5-6 loss: 0.711653  [  224/  306]
train() client id: f_00004-5-7 loss: 0.649124  [  256/  306]
train() client id: f_00004-5-8 loss: 0.683579  [  288/  306]
train() client id: f_00004-6-0 loss: 0.622559  [   32/  306]
train() client id: f_00004-6-1 loss: 0.626132  [   64/  306]
train() client id: f_00004-6-2 loss: 0.654065  [   96/  306]
train() client id: f_00004-6-3 loss: 0.664137  [  128/  306]
train() client id: f_00004-6-4 loss: 0.721722  [  160/  306]
train() client id: f_00004-6-5 loss: 0.679367  [  192/  306]
train() client id: f_00004-6-6 loss: 0.723337  [  224/  306]
train() client id: f_00004-6-7 loss: 0.802171  [  256/  306]
train() client id: f_00004-6-8 loss: 0.822907  [  288/  306]
train() client id: f_00004-7-0 loss: 0.753633  [   32/  306]
train() client id: f_00004-7-1 loss: 0.614987  [   64/  306]
train() client id: f_00004-7-2 loss: 0.806666  [   96/  306]
train() client id: f_00004-7-3 loss: 0.670495  [  128/  306]
train() client id: f_00004-7-4 loss: 0.698238  [  160/  306]
train() client id: f_00004-7-5 loss: 0.677497  [  192/  306]
train() client id: f_00004-7-6 loss: 0.742112  [  224/  306]
train() client id: f_00004-7-7 loss: 0.722801  [  256/  306]
train() client id: f_00004-7-8 loss: 0.717444  [  288/  306]
train() client id: f_00004-8-0 loss: 0.617710  [   32/  306]
train() client id: f_00004-8-1 loss: 0.803899  [   64/  306]
train() client id: f_00004-8-2 loss: 0.705582  [   96/  306]
train() client id: f_00004-8-3 loss: 0.723696  [  128/  306]
train() client id: f_00004-8-4 loss: 0.779473  [  160/  306]
train() client id: f_00004-8-5 loss: 0.658096  [  192/  306]
train() client id: f_00004-8-6 loss: 0.768895  [  224/  306]
train() client id: f_00004-8-7 loss: 0.829375  [  256/  306]
train() client id: f_00004-8-8 loss: 0.636343  [  288/  306]
train() client id: f_00004-9-0 loss: 0.722453  [   32/  306]
train() client id: f_00004-9-1 loss: 0.684913  [   64/  306]
train() client id: f_00004-9-2 loss: 0.811017  [   96/  306]
train() client id: f_00004-9-3 loss: 0.617668  [  128/  306]
train() client id: f_00004-9-4 loss: 0.805320  [  160/  306]
train() client id: f_00004-9-5 loss: 0.687289  [  192/  306]
train() client id: f_00004-9-6 loss: 0.753572  [  224/  306]
train() client id: f_00004-9-7 loss: 0.666062  [  256/  306]
train() client id: f_00004-9-8 loss: 0.773692  [  288/  306]
train() client id: f_00004-10-0 loss: 0.799002  [   32/  306]
train() client id: f_00004-10-1 loss: 0.797942  [   64/  306]
train() client id: f_00004-10-2 loss: 0.771070  [   96/  306]
train() client id: f_00004-10-3 loss: 0.646532  [  128/  306]
train() client id: f_00004-10-4 loss: 0.744319  [  160/  306]
train() client id: f_00004-10-5 loss: 0.654011  [  192/  306]
train() client id: f_00004-10-6 loss: 0.645257  [  224/  306]
train() client id: f_00004-10-7 loss: 0.755306  [  256/  306]
train() client id: f_00004-10-8 loss: 0.709958  [  288/  306]
train() client id: f_00004-11-0 loss: 0.677932  [   32/  306]
train() client id: f_00004-11-1 loss: 0.696383  [   64/  306]
train() client id: f_00004-11-2 loss: 0.619789  [   96/  306]
train() client id: f_00004-11-3 loss: 0.770934  [  128/  306]
train() client id: f_00004-11-4 loss: 0.812164  [  160/  306]
train() client id: f_00004-11-5 loss: 0.728366  [  192/  306]
train() client id: f_00004-11-6 loss: 0.659942  [  224/  306]
train() client id: f_00004-11-7 loss: 0.665262  [  256/  306]
train() client id: f_00004-11-8 loss: 0.871352  [  288/  306]
train() client id: f_00004-12-0 loss: 0.753163  [   32/  306]
train() client id: f_00004-12-1 loss: 0.789830  [   64/  306]
train() client id: f_00004-12-2 loss: 0.847924  [   96/  306]
train() client id: f_00004-12-3 loss: 0.689702  [  128/  306]
train() client id: f_00004-12-4 loss: 0.725574  [  160/  306]
train() client id: f_00004-12-5 loss: 0.686408  [  192/  306]
train() client id: f_00004-12-6 loss: 0.569319  [  224/  306]
train() client id: f_00004-12-7 loss: 0.719037  [  256/  306]
train() client id: f_00004-12-8 loss: 0.772541  [  288/  306]
train() client id: f_00005-0-0 loss: 0.610499  [   32/  146]
train() client id: f_00005-0-1 loss: 0.718370  [   64/  146]
train() client id: f_00005-0-2 loss: 0.536593  [   96/  146]
train() client id: f_00005-0-3 loss: 0.689179  [  128/  146]
train() client id: f_00005-1-0 loss: 0.807298  [   32/  146]
train() client id: f_00005-1-1 loss: 0.850632  [   64/  146]
train() client id: f_00005-1-2 loss: 0.561799  [   96/  146]
train() client id: f_00005-1-3 loss: 0.422549  [  128/  146]
train() client id: f_00005-2-0 loss: 1.058897  [   32/  146]
train() client id: f_00005-2-1 loss: 0.482519  [   64/  146]
train() client id: f_00005-2-2 loss: 0.709886  [   96/  146]
train() client id: f_00005-2-3 loss: 0.492879  [  128/  146]
train() client id: f_00005-3-0 loss: 0.568371  [   32/  146]
train() client id: f_00005-3-1 loss: 0.636653  [   64/  146]
train() client id: f_00005-3-2 loss: 0.747772  [   96/  146]
train() client id: f_00005-3-3 loss: 0.764900  [  128/  146]
train() client id: f_00005-4-0 loss: 0.601230  [   32/  146]
train() client id: f_00005-4-1 loss: 0.575485  [   64/  146]
train() client id: f_00005-4-2 loss: 0.707464  [   96/  146]
train() client id: f_00005-4-3 loss: 0.685009  [  128/  146]
train() client id: f_00005-5-0 loss: 0.692831  [   32/  146]
train() client id: f_00005-5-1 loss: 0.556615  [   64/  146]
train() client id: f_00005-5-2 loss: 0.457687  [   96/  146]
train() client id: f_00005-5-3 loss: 0.970593  [  128/  146]
train() client id: f_00005-6-0 loss: 0.794745  [   32/  146]
train() client id: f_00005-6-1 loss: 0.787185  [   64/  146]
train() client id: f_00005-6-2 loss: 0.490618  [   96/  146]
train() client id: f_00005-6-3 loss: 0.489744  [  128/  146]
train() client id: f_00005-7-0 loss: 0.862261  [   32/  146]
train() client id: f_00005-7-1 loss: 0.560230  [   64/  146]
train() client id: f_00005-7-2 loss: 0.522981  [   96/  146]
train() client id: f_00005-7-3 loss: 0.711112  [  128/  146]
train() client id: f_00005-8-0 loss: 0.393527  [   32/  146]
train() client id: f_00005-8-1 loss: 0.369638  [   64/  146]
train() client id: f_00005-8-2 loss: 0.950500  [   96/  146]
train() client id: f_00005-8-3 loss: 0.932431  [  128/  146]
train() client id: f_00005-9-0 loss: 0.653068  [   32/  146]
train() client id: f_00005-9-1 loss: 0.651473  [   64/  146]
train() client id: f_00005-9-2 loss: 0.630575  [   96/  146]
train() client id: f_00005-9-3 loss: 0.583786  [  128/  146]
train() client id: f_00005-10-0 loss: 0.625635  [   32/  146]
train() client id: f_00005-10-1 loss: 0.632916  [   64/  146]
train() client id: f_00005-10-2 loss: 0.602592  [   96/  146]
train() client id: f_00005-10-3 loss: 0.580774  [  128/  146]
train() client id: f_00005-11-0 loss: 0.797591  [   32/  146]
train() client id: f_00005-11-1 loss: 0.471402  [   64/  146]
train() client id: f_00005-11-2 loss: 0.688942  [   96/  146]
train() client id: f_00005-11-3 loss: 0.698181  [  128/  146]
train() client id: f_00005-12-0 loss: 0.726087  [   32/  146]
train() client id: f_00005-12-1 loss: 0.736924  [   64/  146]
train() client id: f_00005-12-2 loss: 0.524464  [   96/  146]
train() client id: f_00005-12-3 loss: 0.748007  [  128/  146]
train() client id: f_00006-0-0 loss: 0.598578  [   32/   54]
train() client id: f_00006-1-0 loss: 0.566831  [   32/   54]
train() client id: f_00006-2-0 loss: 0.624395  [   32/   54]
train() client id: f_00006-3-0 loss: 0.535831  [   32/   54]
train() client id: f_00006-4-0 loss: 0.573484  [   32/   54]
train() client id: f_00006-5-0 loss: 0.592293  [   32/   54]
train() client id: f_00006-6-0 loss: 0.522522  [   32/   54]
train() client id: f_00006-7-0 loss: 0.634502  [   32/   54]
train() client id: f_00006-8-0 loss: 0.581343  [   32/   54]
train() client id: f_00006-9-0 loss: 0.585029  [   32/   54]
train() client id: f_00006-10-0 loss: 0.562906  [   32/   54]
train() client id: f_00006-11-0 loss: 0.618651  [   32/   54]
train() client id: f_00006-12-0 loss: 0.566800  [   32/   54]
train() client id: f_00007-0-0 loss: 0.594960  [   32/  179]
train() client id: f_00007-0-1 loss: 0.556745  [   64/  179]
train() client id: f_00007-0-2 loss: 0.723693  [   96/  179]
train() client id: f_00007-0-3 loss: 0.573565  [  128/  179]
train() client id: f_00007-0-4 loss: 0.605920  [  160/  179]
train() client id: f_00007-1-0 loss: 0.713015  [   32/  179]
train() client id: f_00007-1-1 loss: 0.645571  [   64/  179]
train() client id: f_00007-1-2 loss: 0.490819  [   96/  179]
train() client id: f_00007-1-3 loss: 0.560714  [  128/  179]
train() client id: f_00007-1-4 loss: 0.478427  [  160/  179]
train() client id: f_00007-2-0 loss: 0.586114  [   32/  179]
train() client id: f_00007-2-1 loss: 0.644491  [   64/  179]
train() client id: f_00007-2-2 loss: 0.535897  [   96/  179]
train() client id: f_00007-2-3 loss: 0.704631  [  128/  179]
train() client id: f_00007-2-4 loss: 0.476068  [  160/  179]
train() client id: f_00007-3-0 loss: 0.554924  [   32/  179]
train() client id: f_00007-3-1 loss: 0.697753  [   64/  179]
train() client id: f_00007-3-2 loss: 0.658127  [   96/  179]
train() client id: f_00007-3-3 loss: 0.468656  [  128/  179]
train() client id: f_00007-3-4 loss: 0.520258  [  160/  179]
train() client id: f_00007-4-0 loss: 0.515237  [   32/  179]
train() client id: f_00007-4-1 loss: 0.498330  [   64/  179]
train() client id: f_00007-4-2 loss: 0.553503  [   96/  179]
train() client id: f_00007-4-3 loss: 0.861899  [  128/  179]
train() client id: f_00007-4-4 loss: 0.412777  [  160/  179]
train() client id: f_00007-5-0 loss: 0.415800  [   32/  179]
train() client id: f_00007-5-1 loss: 0.653029  [   64/  179]
train() client id: f_00007-5-2 loss: 0.485830  [   96/  179]
train() client id: f_00007-5-3 loss: 0.585519  [  128/  179]
train() client id: f_00007-5-4 loss: 0.593014  [  160/  179]
train() client id: f_00007-6-0 loss: 0.384918  [   32/  179]
train() client id: f_00007-6-1 loss: 0.389782  [   64/  179]
train() client id: f_00007-6-2 loss: 0.590116  [   96/  179]
train() client id: f_00007-6-3 loss: 0.578715  [  128/  179]
train() client id: f_00007-6-4 loss: 0.611736  [  160/  179]
train() client id: f_00007-7-0 loss: 0.449373  [   32/  179]
train() client id: f_00007-7-1 loss: 0.566564  [   64/  179]
train() client id: f_00007-7-2 loss: 0.561428  [   96/  179]
train() client id: f_00007-7-3 loss: 0.640078  [  128/  179]
train() client id: f_00007-7-4 loss: 0.587764  [  160/  179]
train() client id: f_00007-8-0 loss: 0.487291  [   32/  179]
train() client id: f_00007-8-1 loss: 0.384775  [   64/  179]
train() client id: f_00007-8-2 loss: 0.628258  [   96/  179]
train() client id: f_00007-8-3 loss: 0.611179  [  128/  179]
train() client id: f_00007-8-4 loss: 0.681027  [  160/  179]
train() client id: f_00007-9-0 loss: 0.774598  [   32/  179]
train() client id: f_00007-9-1 loss: 0.501344  [   64/  179]
train() client id: f_00007-9-2 loss: 0.460494  [   96/  179]
train() client id: f_00007-9-3 loss: 0.456838  [  128/  179]
train() client id: f_00007-9-4 loss: 0.556461  [  160/  179]
train() client id: f_00007-10-0 loss: 0.389238  [   32/  179]
train() client id: f_00007-10-1 loss: 0.406353  [   64/  179]
train() client id: f_00007-10-2 loss: 0.557540  [   96/  179]
train() client id: f_00007-10-3 loss: 0.487118  [  128/  179]
train() client id: f_00007-10-4 loss: 0.821264  [  160/  179]
train() client id: f_00007-11-0 loss: 0.626954  [   32/  179]
train() client id: f_00007-11-1 loss: 0.384014  [   64/  179]
train() client id: f_00007-11-2 loss: 0.619178  [   96/  179]
train() client id: f_00007-11-3 loss: 0.400643  [  128/  179]
train() client id: f_00007-11-4 loss: 0.627696  [  160/  179]
train() client id: f_00007-12-0 loss: 0.523493  [   32/  179]
train() client id: f_00007-12-1 loss: 0.471522  [   64/  179]
train() client id: f_00007-12-2 loss: 0.670192  [   96/  179]
train() client id: f_00007-12-3 loss: 0.729043  [  128/  179]
train() client id: f_00007-12-4 loss: 0.357957  [  160/  179]
train() client id: f_00008-0-0 loss: 0.614713  [   32/  130]
train() client id: f_00008-0-1 loss: 0.440899  [   64/  130]
train() client id: f_00008-0-2 loss: 0.655653  [   96/  130]
train() client id: f_00008-0-3 loss: 0.661892  [  128/  130]
train() client id: f_00008-1-0 loss: 0.527900  [   32/  130]
train() client id: f_00008-1-1 loss: 0.608166  [   64/  130]
train() client id: f_00008-1-2 loss: 0.453655  [   96/  130]
train() client id: f_00008-1-3 loss: 0.800686  [  128/  130]
train() client id: f_00008-2-0 loss: 0.607703  [   32/  130]
train() client id: f_00008-2-1 loss: 0.622489  [   64/  130]
train() client id: f_00008-2-2 loss: 0.635990  [   96/  130]
train() client id: f_00008-2-3 loss: 0.530833  [  128/  130]
train() client id: f_00008-3-0 loss: 0.675231  [   32/  130]
train() client id: f_00008-3-1 loss: 0.612876  [   64/  130]
train() client id: f_00008-3-2 loss: 0.510702  [   96/  130]
train() client id: f_00008-3-3 loss: 0.556525  [  128/  130]
train() client id: f_00008-4-0 loss: 0.638965  [   32/  130]
train() client id: f_00008-4-1 loss: 0.652639  [   64/  130]
train() client id: f_00008-4-2 loss: 0.548089  [   96/  130]
train() client id: f_00008-4-3 loss: 0.556413  [  128/  130]
train() client id: f_00008-5-0 loss: 0.593564  [   32/  130]
train() client id: f_00008-5-1 loss: 0.612378  [   64/  130]
train() client id: f_00008-5-2 loss: 0.578704  [   96/  130]
train() client id: f_00008-5-3 loss: 0.564379  [  128/  130]
train() client id: f_00008-6-0 loss: 0.627602  [   32/  130]
train() client id: f_00008-6-1 loss: 0.596904  [   64/  130]
train() client id: f_00008-6-2 loss: 0.593822  [   96/  130]
train() client id: f_00008-6-3 loss: 0.576181  [  128/  130]
train() client id: f_00008-7-0 loss: 0.624793  [   32/  130]
train() client id: f_00008-7-1 loss: 0.601474  [   64/  130]
train() client id: f_00008-7-2 loss: 0.542876  [   96/  130]
train() client id: f_00008-7-3 loss: 0.622397  [  128/  130]
train() client id: f_00008-8-0 loss: 0.556444  [   32/  130]
train() client id: f_00008-8-1 loss: 0.633624  [   64/  130]
train() client id: f_00008-8-2 loss: 0.562899  [   96/  130]
train() client id: f_00008-8-3 loss: 0.655329  [  128/  130]
train() client id: f_00008-9-0 loss: 0.592422  [   32/  130]
train() client id: f_00008-9-1 loss: 0.510355  [   64/  130]
train() client id: f_00008-9-2 loss: 0.645023  [   96/  130]
train() client id: f_00008-9-3 loss: 0.658820  [  128/  130]
train() client id: f_00008-10-0 loss: 0.598521  [   32/  130]
train() client id: f_00008-10-1 loss: 0.648686  [   64/  130]
train() client id: f_00008-10-2 loss: 0.649815  [   96/  130]
train() client id: f_00008-10-3 loss: 0.513529  [  128/  130]
train() client id: f_00008-11-0 loss: 0.556705  [   32/  130]
train() client id: f_00008-11-1 loss: 0.575322  [   64/  130]
train() client id: f_00008-11-2 loss: 0.653030  [   96/  130]
train() client id: f_00008-11-3 loss: 0.585317  [  128/  130]
train() client id: f_00008-12-0 loss: 0.520480  [   32/  130]
train() client id: f_00008-12-1 loss: 0.542511  [   64/  130]
train() client id: f_00008-12-2 loss: 0.647585  [   96/  130]
train() client id: f_00008-12-3 loss: 0.700424  [  128/  130]
train() client id: f_00009-0-0 loss: 1.096143  [   32/  118]
train() client id: f_00009-0-1 loss: 0.924353  [   64/  118]
train() client id: f_00009-0-2 loss: 0.936010  [   96/  118]
train() client id: f_00009-1-0 loss: 0.946698  [   32/  118]
train() client id: f_00009-1-1 loss: 0.819689  [   64/  118]
train() client id: f_00009-1-2 loss: 1.076713  [   96/  118]
train() client id: f_00009-2-0 loss: 0.936564  [   32/  118]
train() client id: f_00009-2-1 loss: 0.899440  [   64/  118]
train() client id: f_00009-2-2 loss: 0.892861  [   96/  118]
train() client id: f_00009-3-0 loss: 0.775041  [   32/  118]
train() client id: f_00009-3-1 loss: 0.761365  [   64/  118]
train() client id: f_00009-3-2 loss: 0.869121  [   96/  118]
train() client id: f_00009-4-0 loss: 0.928030  [   32/  118]
train() client id: f_00009-4-1 loss: 0.979971  [   64/  118]
train() client id: f_00009-4-2 loss: 0.755007  [   96/  118]
train() client id: f_00009-5-0 loss: 0.872992  [   32/  118]
train() client id: f_00009-5-1 loss: 0.869763  [   64/  118]
train() client id: f_00009-5-2 loss: 0.769089  [   96/  118]
train() client id: f_00009-6-0 loss: 0.857190  [   32/  118]
train() client id: f_00009-6-1 loss: 0.818300  [   64/  118]
train() client id: f_00009-6-2 loss: 0.916384  [   96/  118]
train() client id: f_00009-7-0 loss: 1.014971  [   32/  118]
train() client id: f_00009-7-1 loss: 0.822236  [   64/  118]
train() client id: f_00009-7-2 loss: 0.794030  [   96/  118]
train() client id: f_00009-8-0 loss: 0.754118  [   32/  118]
train() client id: f_00009-8-1 loss: 0.971425  [   64/  118]
train() client id: f_00009-8-2 loss: 0.750832  [   96/  118]
train() client id: f_00009-9-0 loss: 0.851869  [   32/  118]
train() client id: f_00009-9-1 loss: 0.809146  [   64/  118]
train() client id: f_00009-9-2 loss: 0.833114  [   96/  118]
train() client id: f_00009-10-0 loss: 0.923464  [   32/  118]
train() client id: f_00009-10-1 loss: 0.781935  [   64/  118]
train() client id: f_00009-10-2 loss: 0.876238  [   96/  118]
train() client id: f_00009-11-0 loss: 0.870716  [   32/  118]
train() client id: f_00009-11-1 loss: 0.815294  [   64/  118]
train() client id: f_00009-11-2 loss: 0.903888  [   96/  118]
train() client id: f_00009-12-0 loss: 0.940105  [   32/  118]
train() client id: f_00009-12-1 loss: 0.844964  [   64/  118]
train() client id: f_00009-12-2 loss: 0.709447  [   96/  118]
At round 39 accuracy: 0.6525198938992043
At round 39 training accuracy: 0.5868544600938967
At round 39 training loss: 0.8232509064880589
update_location
xs = -3.905658 4.200318 215.009024 18.811294 0.979296 3.956410 -177.443192 -156.324852 199.663977 -142.060879 
ys = 207.587959 190.555839 1.320614 -177.455176 169.350187 152.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 230.451763 215.242120 237.129974 204.558559 196.673447 182.668601 203.698250 185.575148 223.996370 173.773718 
dists_bs = 174.170172 179.874228 426.932744 402.228130 176.069966 180.326394 177.641786 175.175956 406.395883 174.142123 
uav_gains = -109.737166 -108.667265 -110.245428 -107.970810 -107.473055 -106.596344 -107.916023 -106.779046 -109.269306 -106.029467 
bs_gains = -102.314325 -102.706190 -113.217124 -112.492287 -102.446247 -102.736720 -102.554323 -102.384345 -112.617640 -102.312367 
Round 40
-------------------------------
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.71197888 11.79408709  5.62914561  2.03417315 13.60152567  6.544648
  2.51897531  8.02443866  5.92443305  5.30743185]
obj_prev = 67.09083726402451
eta_min = 7.023642441479729e-17	eta_max = 0.9321930357555286
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 15.5538538243213	eta = 0.909090909090909
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 29.16218476187658	eta = 0.4848699515649548
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 22.40002232600407	eta = 0.6312434383873119
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 21.175688042561685	eta = 0.667740622387296
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 21.10970401340027	eta = 0.6698278243997874
af = 14.139867113019362	bf = 1.3034344286740114	zeta = 21.109495669486368	eta = 0.6698344353843775
eta = 0.6698344353843775
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [0.03306684 0.06954534 0.03254197 0.01128472 0.08030519 0.03831555
 0.0141715  0.04697591 0.03411658 0.03096736]
ene_total = [1.91082934 3.36153203 1.90398611 0.90652186 3.82972324 1.99226433
 1.03177315 2.44413259 2.06480829 1.66392474]
ti_comp = [0.53339798 0.56741874 0.52970956 0.54456162 0.56826829 0.56731763
 0.54487186 0.55089405 0.50878776 0.5686981 ]
ti_coms = [0.10500598 0.07098523 0.10869441 0.09384235 0.07013567 0.07108634
 0.09353211 0.08750992 0.12961621 0.06970587]
t_total = [27.99983215 27.99983215 27.99983215 27.99983215 27.99983215 27.99983215
 27.99983215 27.99983215 27.99983215 27.99983215]
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [7.94246766e-06 6.52945351e-05 7.67601831e-06 3.02871121e-07
 1.00231444e-04 1.09232674e-05 5.99154859e-07 2.13486048e-05
 9.58745418e-06 5.73890489e-06]
ene_total = [0.46089529 0.31419869 0.47706073 0.4115975  0.31200491 0.31225746
 0.41024983 0.3847471  0.56890568 0.30597546]
optimize_network iter = 0 obj = 3.9578926480979075
eta = 0.6698344353843775
freqs = [30996407.38459397 61282205.26212957 30716806.61521623 10361284.76037134
 70657812.12672363 33769043.53321395 13004437.39099001 42636067.25492053
 33527323.09434476 27226537.57500104]
eta_min = 0.6698344353843826	eta_max = 0.6698344353843708
af = 0.008298545367209708	bf = 1.3034344286740114	zeta = 0.00912839990393068	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [1.88907249e-06 1.55299480e-05 1.82569896e-06 7.20362393e-08
 2.38395007e-05 2.59803940e-06 1.42505706e-07 5.07764887e-06
 2.28032353e-06 1.36496714e-06]
ene_total = [1.67292505 1.13318653 1.73166738 1.49481175 1.1209777  1.13273715
 1.48988129 1.3947409  2.06499979 1.11055146]
ti_comp = [0.53339798 0.56741874 0.52970956 0.54456162 0.56826829 0.56731763
 0.54487186 0.55089405 0.50878776 0.5686981 ]
ti_coms = [0.10500598 0.07098523 0.10869441 0.09384235 0.07013567 0.07108634
 0.09353211 0.08750992 0.12961621 0.06970587]
t_total = [27.99983215 27.99983215 27.99983215 27.99983215 27.99983215 27.99983215
 27.99983215 27.99983215 27.99983215 27.99983215]
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [7.94246766e-06 6.52945351e-05 7.67601831e-06 3.02871121e-07
 1.00231444e-04 1.09232674e-05 5.99154859e-07 2.13486048e-05
 9.58745418e-06 5.73890489e-06]
ene_total = [0.46089529 0.31419869 0.47706073 0.4115975  0.31200491 0.31225746
 0.41024983 0.3847471  0.56890568 0.30597546]
optimize_network iter = 1 obj = 3.9578926480978276
eta = 0.6698344353843708
freqs = [30996407.384594   61282205.26212971 30716806.61521625 10361284.76037136
 70657812.12672378 33769043.53321403 13004437.39099002 42636067.2549206
 33527323.09434475 27226537.5750011 ]
Done!
ene_coms = [0.0105006  0.00709852 0.01086944 0.00938423 0.00701357 0.00710863
 0.00935321 0.00875099 0.01296162 0.00697059]
ene_comp = [7.86876585e-06 6.46886370e-05 7.60478900e-06 3.00060640e-07
 9.93013504e-05 1.08219054e-05 5.93595025e-07 2.11505013e-05
 9.49848778e-06 5.68565095e-06]
ene_total = [0.01050847 0.00716321 0.01087705 0.00938453 0.00711287 0.00711946
 0.0093538  0.00877214 0.01297112 0.00697627]
At round 40 energy consumption: 0.09023892340678759
At round 40 eta: 0.6698344353843708
At round 40 a_n: 14.480768977481684
At round 40 local rounds: 13.121762881988987
At round 40 global rounds: 43.859113515789716
gradient difference: 0.45146098732948303
train() client id: f_00000-0-0 loss: 1.191442  [   32/  126]
train() client id: f_00000-0-1 loss: 0.960091  [   64/  126]
train() client id: f_00000-0-2 loss: 1.098671  [   96/  126]
train() client id: f_00000-1-0 loss: 1.167061  [   32/  126]
train() client id: f_00000-1-1 loss: 0.973876  [   64/  126]
train() client id: f_00000-1-2 loss: 0.899963  [   96/  126]
train() client id: f_00000-2-0 loss: 1.003396  [   32/  126]
train() client id: f_00000-2-1 loss: 1.108541  [   64/  126]
train() client id: f_00000-2-2 loss: 0.848524  [   96/  126]
train() client id: f_00000-3-0 loss: 0.932531  [   32/  126]
train() client id: f_00000-3-1 loss: 0.771535  [   64/  126]
train() client id: f_00000-3-2 loss: 1.168187  [   96/  126]
train() client id: f_00000-4-0 loss: 0.883815  [   32/  126]
train() client id: f_00000-4-1 loss: 0.845725  [   64/  126]
train() client id: f_00000-4-2 loss: 0.944514  [   96/  126]
train() client id: f_00000-5-0 loss: 0.853110  [   32/  126]
train() client id: f_00000-5-1 loss: 0.914476  [   64/  126]
train() client id: f_00000-5-2 loss: 0.805646  [   96/  126]
train() client id: f_00000-6-0 loss: 0.824669  [   32/  126]
train() client id: f_00000-6-1 loss: 0.762107  [   64/  126]
train() client id: f_00000-6-2 loss: 0.817481  [   96/  126]
train() client id: f_00000-7-0 loss: 0.851635  [   32/  126]
train() client id: f_00000-7-1 loss: 0.838565  [   64/  126]
train() client id: f_00000-7-2 loss: 0.729088  [   96/  126]
train() client id: f_00000-8-0 loss: 0.783711  [   32/  126]
train() client id: f_00000-8-1 loss: 0.920814  [   64/  126]
train() client id: f_00000-8-2 loss: 0.726802  [   96/  126]
train() client id: f_00000-9-0 loss: 0.764354  [   32/  126]
train() client id: f_00000-9-1 loss: 0.789601  [   64/  126]
train() client id: f_00000-9-2 loss: 0.848811  [   96/  126]
train() client id: f_00000-10-0 loss: 0.844406  [   32/  126]
train() client id: f_00000-10-1 loss: 0.842591  [   64/  126]
train() client id: f_00000-10-2 loss: 0.833888  [   96/  126]
train() client id: f_00000-11-0 loss: 0.726553  [   32/  126]
train() client id: f_00000-11-1 loss: 0.899350  [   64/  126]
train() client id: f_00000-11-2 loss: 0.834157  [   96/  126]
train() client id: f_00000-12-0 loss: 0.819185  [   32/  126]
train() client id: f_00000-12-1 loss: 0.702151  [   64/  126]
train() client id: f_00000-12-2 loss: 0.904286  [   96/  126]
train() client id: f_00001-0-0 loss: 0.526543  [   32/  265]
train() client id: f_00001-0-1 loss: 0.568283  [   64/  265]
train() client id: f_00001-0-2 loss: 0.445217  [   96/  265]
train() client id: f_00001-0-3 loss: 0.428347  [  128/  265]
train() client id: f_00001-0-4 loss: 0.474846  [  160/  265]
train() client id: f_00001-0-5 loss: 0.502321  [  192/  265]
train() client id: f_00001-0-6 loss: 0.453348  [  224/  265]
train() client id: f_00001-0-7 loss: 0.404131  [  256/  265]
train() client id: f_00001-1-0 loss: 0.626593  [   32/  265]
train() client id: f_00001-1-1 loss: 0.474963  [   64/  265]
train() client id: f_00001-1-2 loss: 0.467075  [   96/  265]
train() client id: f_00001-1-3 loss: 0.445294  [  128/  265]
train() client id: f_00001-1-4 loss: 0.529020  [  160/  265]
train() client id: f_00001-1-5 loss: 0.464430  [  192/  265]
train() client id: f_00001-1-6 loss: 0.407499  [  224/  265]
train() client id: f_00001-1-7 loss: 0.395342  [  256/  265]
train() client id: f_00001-2-0 loss: 0.498513  [   32/  265]
train() client id: f_00001-2-1 loss: 0.515364  [   64/  265]
train() client id: f_00001-2-2 loss: 0.440182  [   96/  265]
train() client id: f_00001-2-3 loss: 0.362617  [  128/  265]
train() client id: f_00001-2-4 loss: 0.501002  [  160/  265]
train() client id: f_00001-2-5 loss: 0.436211  [  192/  265]
train() client id: f_00001-2-6 loss: 0.390803  [  224/  265]
train() client id: f_00001-2-7 loss: 0.561119  [  256/  265]
train() client id: f_00001-3-0 loss: 0.381456  [   32/  265]
train() client id: f_00001-3-1 loss: 0.380349  [   64/  265]
train() client id: f_00001-3-2 loss: 0.371882  [   96/  265]
train() client id: f_00001-3-3 loss: 0.568501  [  128/  265]
train() client id: f_00001-3-4 loss: 0.510141  [  160/  265]
train() client id: f_00001-3-5 loss: 0.512143  [  192/  265]
train() client id: f_00001-3-6 loss: 0.652265  [  224/  265]
train() client id: f_00001-3-7 loss: 0.363548  [  256/  265]
train() client id: f_00001-4-0 loss: 0.492014  [   32/  265]
train() client id: f_00001-4-1 loss: 0.615288  [   64/  265]
train() client id: f_00001-4-2 loss: 0.371180  [   96/  265]
train() client id: f_00001-4-3 loss: 0.540807  [  128/  265]
train() client id: f_00001-4-4 loss: 0.349419  [  160/  265]
train() client id: f_00001-4-5 loss: 0.422672  [  192/  265]
train() client id: f_00001-4-6 loss: 0.483921  [  224/  265]
train() client id: f_00001-4-7 loss: 0.428486  [  256/  265]
train() client id: f_00001-5-0 loss: 0.509022  [   32/  265]
train() client id: f_00001-5-1 loss: 0.647800  [   64/  265]
train() client id: f_00001-5-2 loss: 0.358290  [   96/  265]
train() client id: f_00001-5-3 loss: 0.358504  [  128/  265]
train() client id: f_00001-5-4 loss: 0.498211  [  160/  265]
train() client id: f_00001-5-5 loss: 0.517055  [  192/  265]
train() client id: f_00001-5-6 loss: 0.390449  [  224/  265]
train() client id: f_00001-5-7 loss: 0.424325  [  256/  265]
train() client id: f_00001-6-0 loss: 0.432984  [   32/  265]
train() client id: f_00001-6-1 loss: 0.397992  [   64/  265]
train() client id: f_00001-6-2 loss: 0.666628  [   96/  265]
train() client id: f_00001-6-3 loss: 0.420688  [  128/  265]
train() client id: f_00001-6-4 loss: 0.443931  [  160/  265]
train() client id: f_00001-6-5 loss: 0.407032  [  192/  265]
train() client id: f_00001-6-6 loss: 0.385212  [  224/  265]
train() client id: f_00001-6-7 loss: 0.512434  [  256/  265]
train() client id: f_00001-7-0 loss: 0.426409  [   32/  265]
train() client id: f_00001-7-1 loss: 0.389670  [   64/  265]
train() client id: f_00001-7-2 loss: 0.621417  [   96/  265]
train() client id: f_00001-7-3 loss: 0.463417  [  128/  265]
train() client id: f_00001-7-4 loss: 0.378089  [  160/  265]
train() client id: f_00001-7-5 loss: 0.432448  [  192/  265]
train() client id: f_00001-7-6 loss: 0.366653  [  224/  265]
train() client id: f_00001-7-7 loss: 0.588430  [  256/  265]
train() client id: f_00001-8-0 loss: 0.418097  [   32/  265]
train() client id: f_00001-8-1 loss: 0.614913  [   64/  265]
train() client id: f_00001-8-2 loss: 0.449271  [   96/  265]
train() client id: f_00001-8-3 loss: 0.404987  [  128/  265]
train() client id: f_00001-8-4 loss: 0.438799  [  160/  265]
train() client id: f_00001-8-5 loss: 0.510681  [  192/  265]
train() client id: f_00001-8-6 loss: 0.399345  [  224/  265]
train() client id: f_00001-8-7 loss: 0.432284  [  256/  265]
train() client id: f_00001-9-0 loss: 0.573038  [   32/  265]
train() client id: f_00001-9-1 loss: 0.349509  [   64/  265]
train() client id: f_00001-9-2 loss: 0.497074  [   96/  265]
train() client id: f_00001-9-3 loss: 0.448343  [  128/  265]
train() client id: f_00001-9-4 loss: 0.433072  [  160/  265]
train() client id: f_00001-9-5 loss: 0.493012  [  192/  265]
train() client id: f_00001-9-6 loss: 0.446610  [  224/  265]
train() client id: f_00001-9-7 loss: 0.415511  [  256/  265]
train() client id: f_00001-10-0 loss: 0.570023  [   32/  265]
train() client id: f_00001-10-1 loss: 0.414026  [   64/  265]
train() client id: f_00001-10-2 loss: 0.541009  [   96/  265]
train() client id: f_00001-10-3 loss: 0.360965  [  128/  265]
train() client id: f_00001-10-4 loss: 0.453445  [  160/  265]
train() client id: f_00001-10-5 loss: 0.503627  [  192/  265]
train() client id: f_00001-10-6 loss: 0.474285  [  224/  265]
train() client id: f_00001-10-7 loss: 0.350533  [  256/  265]
train() client id: f_00001-11-0 loss: 0.545601  [   32/  265]
train() client id: f_00001-11-1 loss: 0.484152  [   64/  265]
train() client id: f_00001-11-2 loss: 0.436151  [   96/  265]
train() client id: f_00001-11-3 loss: 0.444899  [  128/  265]
train() client id: f_00001-11-4 loss: 0.381432  [  160/  265]
train() client id: f_00001-11-5 loss: 0.514001  [  192/  265]
train() client id: f_00001-11-6 loss: 0.423810  [  224/  265]
train() client id: f_00001-11-7 loss: 0.353437  [  256/  265]
train() client id: f_00001-12-0 loss: 0.384579  [   32/  265]
train() client id: f_00001-12-1 loss: 0.405737  [   64/  265]
train() client id: f_00001-12-2 loss: 0.444197  [   96/  265]
train() client id: f_00001-12-3 loss: 0.471849  [  128/  265]
train() client id: f_00001-12-4 loss: 0.486083  [  160/  265]
train() client id: f_00001-12-5 loss: 0.487449  [  192/  265]
train() client id: f_00001-12-6 loss: 0.413538  [  224/  265]
train() client id: f_00001-12-7 loss: 0.409499  [  256/  265]
train() client id: f_00002-0-0 loss: 1.245170  [   32/  124]
train() client id: f_00002-0-1 loss: 1.080899  [   64/  124]
train() client id: f_00002-0-2 loss: 1.288329  [   96/  124]
train() client id: f_00002-1-0 loss: 1.167617  [   32/  124]
train() client id: f_00002-1-1 loss: 0.951617  [   64/  124]
train() client id: f_00002-1-2 loss: 1.290964  [   96/  124]
train() client id: f_00002-2-0 loss: 1.275923  [   32/  124]
train() client id: f_00002-2-1 loss: 0.928490  [   64/  124]
train() client id: f_00002-2-2 loss: 1.189628  [   96/  124]
train() client id: f_00002-3-0 loss: 1.295313  [   32/  124]
train() client id: f_00002-3-1 loss: 1.053261  [   64/  124]
train() client id: f_00002-3-2 loss: 1.040595  [   96/  124]
train() client id: f_00002-4-0 loss: 1.061379  [   32/  124]
train() client id: f_00002-4-1 loss: 0.998278  [   64/  124]
train() client id: f_00002-4-2 loss: 1.201115  [   96/  124]
train() client id: f_00002-5-0 loss: 1.120435  [   32/  124]
train() client id: f_00002-5-1 loss: 1.122673  [   64/  124]
train() client id: f_00002-5-2 loss: 1.080174  [   96/  124]
train() client id: f_00002-6-0 loss: 0.969723  [   32/  124]
train() client id: f_00002-6-1 loss: 0.823176  [   64/  124]
train() client id: f_00002-6-2 loss: 1.202247  [   96/  124]
train() client id: f_00002-7-0 loss: 0.943379  [   32/  124]
train() client id: f_00002-7-1 loss: 1.224445  [   64/  124]
train() client id: f_00002-7-2 loss: 0.908610  [   96/  124]
train() client id: f_00002-8-0 loss: 1.120358  [   32/  124]
train() client id: f_00002-8-1 loss: 0.843573  [   64/  124]
train() client id: f_00002-8-2 loss: 1.054806  [   96/  124]
train() client id: f_00002-9-0 loss: 0.995346  [   32/  124]
train() client id: f_00002-9-1 loss: 1.034982  [   64/  124]
train() client id: f_00002-9-2 loss: 1.110512  [   96/  124]
train() client id: f_00002-10-0 loss: 1.054379  [   32/  124]
train() client id: f_00002-10-1 loss: 1.024157  [   64/  124]
train() client id: f_00002-10-2 loss: 1.033502  [   96/  124]
train() client id: f_00002-11-0 loss: 1.017553  [   32/  124]
train() client id: f_00002-11-1 loss: 1.020016  [   64/  124]
train() client id: f_00002-11-2 loss: 0.900248  [   96/  124]
train() client id: f_00002-12-0 loss: 1.307653  [   32/  124]
train() client id: f_00002-12-1 loss: 0.829663  [   64/  124]
train() client id: f_00002-12-2 loss: 0.842414  [   96/  124]
train() client id: f_00003-0-0 loss: 0.834524  [   32/   43]
train() client id: f_00003-1-0 loss: 0.879158  [   32/   43]
train() client id: f_00003-2-0 loss: 0.862468  [   32/   43]
train() client id: f_00003-3-0 loss: 0.861861  [   32/   43]
train() client id: f_00003-4-0 loss: 0.805969  [   32/   43]
train() client id: f_00003-5-0 loss: 0.844155  [   32/   43]
train() client id: f_00003-6-0 loss: 0.724390  [   32/   43]
train() client id: f_00003-7-0 loss: 0.779198  [   32/   43]
train() client id: f_00003-8-0 loss: 0.807977  [   32/   43]
train() client id: f_00003-9-0 loss: 0.814443  [   32/   43]
train() client id: f_00003-10-0 loss: 0.977278  [   32/   43]
train() client id: f_00003-11-0 loss: 0.782247  [   32/   43]
train() client id: f_00003-12-0 loss: 0.761313  [   32/   43]
train() client id: f_00004-0-0 loss: 0.779752  [   32/  306]
train() client id: f_00004-0-1 loss: 0.839566  [   64/  306]
train() client id: f_00004-0-2 loss: 0.821251  [   96/  306]
train() client id: f_00004-0-3 loss: 0.849609  [  128/  306]
train() client id: f_00004-0-4 loss: 0.852870  [  160/  306]
train() client id: f_00004-0-5 loss: 0.883735  [  192/  306]
train() client id: f_00004-0-6 loss: 0.852840  [  224/  306]
train() client id: f_00004-0-7 loss: 0.701008  [  256/  306]
train() client id: f_00004-0-8 loss: 0.709774  [  288/  306]
train() client id: f_00004-1-0 loss: 0.742221  [   32/  306]
train() client id: f_00004-1-1 loss: 0.878129  [   64/  306]
train() client id: f_00004-1-2 loss: 0.871176  [   96/  306]
train() client id: f_00004-1-3 loss: 0.807361  [  128/  306]
train() client id: f_00004-1-4 loss: 0.923420  [  160/  306]
train() client id: f_00004-1-5 loss: 0.838946  [  192/  306]
train() client id: f_00004-1-6 loss: 0.749095  [  224/  306]
train() client id: f_00004-1-7 loss: 0.847232  [  256/  306]
train() client id: f_00004-1-8 loss: 0.709415  [  288/  306]
train() client id: f_00004-2-0 loss: 0.849277  [   32/  306]
train() client id: f_00004-2-1 loss: 0.898880  [   64/  306]
train() client id: f_00004-2-2 loss: 0.813657  [   96/  306]
train() client id: f_00004-2-3 loss: 0.819202  [  128/  306]
train() client id: f_00004-2-4 loss: 0.737774  [  160/  306]
train() client id: f_00004-2-5 loss: 0.758425  [  192/  306]
train() client id: f_00004-2-6 loss: 0.792116  [  224/  306]
train() client id: f_00004-2-7 loss: 0.686711  [  256/  306]
train() client id: f_00004-2-8 loss: 0.907845  [  288/  306]
train() client id: f_00004-3-0 loss: 0.817512  [   32/  306]
train() client id: f_00004-3-1 loss: 0.838670  [   64/  306]
train() client id: f_00004-3-2 loss: 0.929312  [   96/  306]
train() client id: f_00004-3-3 loss: 0.742496  [  128/  306]
train() client id: f_00004-3-4 loss: 0.799237  [  160/  306]
train() client id: f_00004-3-5 loss: 0.831652  [  192/  306]
train() client id: f_00004-3-6 loss: 0.720469  [  224/  306]
train() client id: f_00004-3-7 loss: 0.880945  [  256/  306]
train() client id: f_00004-3-8 loss: 0.819017  [  288/  306]
train() client id: f_00004-4-0 loss: 0.807084  [   32/  306]
train() client id: f_00004-4-1 loss: 0.831881  [   64/  306]
train() client id: f_00004-4-2 loss: 0.849871  [   96/  306]
train() client id: f_00004-4-3 loss: 0.765979  [  128/  306]
train() client id: f_00004-4-4 loss: 0.814019  [  160/  306]
train() client id: f_00004-4-5 loss: 0.717559  [  192/  306]
train() client id: f_00004-4-6 loss: 0.854690  [  224/  306]
train() client id: f_00004-4-7 loss: 0.828849  [  256/  306]
train() client id: f_00004-4-8 loss: 0.912144  [  288/  306]
train() client id: f_00004-5-0 loss: 0.902115  [   32/  306]
train() client id: f_00004-5-1 loss: 0.639725  [   64/  306]
train() client id: f_00004-5-2 loss: 0.747607  [   96/  306]
train() client id: f_00004-5-3 loss: 0.812829  [  128/  306]
train() client id: f_00004-5-4 loss: 0.825182  [  160/  306]
train() client id: f_00004-5-5 loss: 0.719618  [  192/  306]
train() client id: f_00004-5-6 loss: 1.043065  [  224/  306]
train() client id: f_00004-5-7 loss: 0.817234  [  256/  306]
train() client id: f_00004-5-8 loss: 0.850282  [  288/  306]
train() client id: f_00004-6-0 loss: 0.820130  [   32/  306]
train() client id: f_00004-6-1 loss: 1.019619  [   64/  306]
train() client id: f_00004-6-2 loss: 0.774776  [   96/  306]
train() client id: f_00004-6-3 loss: 0.794641  [  128/  306]
train() client id: f_00004-6-4 loss: 0.805156  [  160/  306]
train() client id: f_00004-6-5 loss: 0.831345  [  192/  306]
train() client id: f_00004-6-6 loss: 0.730275  [  224/  306]
train() client id: f_00004-6-7 loss: 0.716305  [  256/  306]
train() client id: f_00004-6-8 loss: 0.879111  [  288/  306]
train() client id: f_00004-7-0 loss: 0.825340  [   32/  306]
train() client id: f_00004-7-1 loss: 0.813993  [   64/  306]
train() client id: f_00004-7-2 loss: 0.795670  [   96/  306]
train() client id: f_00004-7-3 loss: 0.809377  [  128/  306]
train() client id: f_00004-7-4 loss: 0.889699  [  160/  306]
train() client id: f_00004-7-5 loss: 0.885121  [  192/  306]
train() client id: f_00004-7-6 loss: 0.759945  [  224/  306]
train() client id: f_00004-7-7 loss: 0.846554  [  256/  306]
train() client id: f_00004-7-8 loss: 0.809601  [  288/  306]
train() client id: f_00004-8-0 loss: 0.948331  [   32/  306]
train() client id: f_00004-8-1 loss: 0.815604  [   64/  306]
train() client id: f_00004-8-2 loss: 0.731937  [   96/  306]
train() client id: f_00004-8-3 loss: 0.809532  [  128/  306]
train() client id: f_00004-8-4 loss: 0.832611  [  160/  306]
train() client id: f_00004-8-5 loss: 0.890805  [  192/  306]
train() client id: f_00004-8-6 loss: 0.711311  [  224/  306]
train() client id: f_00004-8-7 loss: 0.695090  [  256/  306]
train() client id: f_00004-8-8 loss: 0.917159  [  288/  306]
train() client id: f_00004-9-0 loss: 0.937208  [   32/  306]
train() client id: f_00004-9-1 loss: 0.772863  [   64/  306]
train() client id: f_00004-9-2 loss: 0.770624  [   96/  306]
train() client id: f_00004-9-3 loss: 0.788141  [  128/  306]
train() client id: f_00004-9-4 loss: 0.803001  [  160/  306]
train() client id: f_00004-9-5 loss: 0.765708  [  192/  306]
train() client id: f_00004-9-6 loss: 0.796952  [  224/  306]
train() client id: f_00004-9-7 loss: 0.984684  [  256/  306]
train() client id: f_00004-9-8 loss: 0.788790  [  288/  306]
train() client id: f_00004-10-0 loss: 0.692519  [   32/  306]
train() client id: f_00004-10-1 loss: 0.894943  [   64/  306]
train() client id: f_00004-10-2 loss: 0.887834  [   96/  306]
train() client id: f_00004-10-3 loss: 0.834405  [  128/  306]
train() client id: f_00004-10-4 loss: 0.740834  [  160/  306]
train() client id: f_00004-10-5 loss: 0.908367  [  192/  306]
train() client id: f_00004-10-6 loss: 0.899387  [  224/  306]
train() client id: f_00004-10-7 loss: 0.822069  [  256/  306]
train() client id: f_00004-10-8 loss: 0.750706  [  288/  306]
train() client id: f_00004-11-0 loss: 0.742399  [   32/  306]
train() client id: f_00004-11-1 loss: 0.835006  [   64/  306]
train() client id: f_00004-11-2 loss: 0.840997  [   96/  306]
train() client id: f_00004-11-3 loss: 0.766372  [  128/  306]
train() client id: f_00004-11-4 loss: 0.827139  [  160/  306]
train() client id: f_00004-11-5 loss: 0.811976  [  192/  306]
train() client id: f_00004-11-6 loss: 0.869347  [  224/  306]
train() client id: f_00004-11-7 loss: 0.826548  [  256/  306]
train() client id: f_00004-11-8 loss: 0.944311  [  288/  306]
train() client id: f_00004-12-0 loss: 0.838696  [   32/  306]
train() client id: f_00004-12-1 loss: 0.821031  [   64/  306]
train() client id: f_00004-12-2 loss: 0.826244  [   96/  306]
train() client id: f_00004-12-3 loss: 0.874985  [  128/  306]
train() client id: f_00004-12-4 loss: 0.843381  [  160/  306]
train() client id: f_00004-12-5 loss: 0.793718  [  192/  306]
train() client id: f_00004-12-6 loss: 0.771417  [  224/  306]
train() client id: f_00004-12-7 loss: 0.806617  [  256/  306]
train() client id: f_00004-12-8 loss: 0.806446  [  288/  306]
train() client id: f_00005-0-0 loss: 0.508088  [   32/  146]
train() client id: f_00005-0-1 loss: 0.464424  [   64/  146]
train() client id: f_00005-0-2 loss: 0.408102  [   96/  146]
train() client id: f_00005-0-3 loss: 0.662366  [  128/  146]
train() client id: f_00005-1-0 loss: 0.400481  [   32/  146]
train() client id: f_00005-1-1 loss: 0.489460  [   64/  146]
train() client id: f_00005-1-2 loss: 0.551915  [   96/  146]
train() client id: f_00005-1-3 loss: 0.552461  [  128/  146]
train() client id: f_00005-2-0 loss: 0.671041  [   32/  146]
train() client id: f_00005-2-1 loss: 0.389140  [   64/  146]
train() client id: f_00005-2-2 loss: 0.425439  [   96/  146]
train() client id: f_00005-2-3 loss: 0.364852  [  128/  146]
train() client id: f_00005-3-0 loss: 0.534295  [   32/  146]
train() client id: f_00005-3-1 loss: 0.642638  [   64/  146]
train() client id: f_00005-3-2 loss: 0.404204  [   96/  146]
train() client id: f_00005-3-3 loss: 0.319411  [  128/  146]
train() client id: f_00005-4-0 loss: 0.482335  [   32/  146]
train() client id: f_00005-4-1 loss: 0.639149  [   64/  146]
train() client id: f_00005-4-2 loss: 0.277680  [   96/  146]
train() client id: f_00005-4-3 loss: 0.367175  [  128/  146]
train() client id: f_00005-5-0 loss: 0.334370  [   32/  146]
train() client id: f_00005-5-1 loss: 0.304185  [   64/  146]
train() client id: f_00005-5-2 loss: 0.642577  [   96/  146]
train() client id: f_00005-5-3 loss: 0.678053  [  128/  146]
train() client id: f_00005-6-0 loss: 0.352835  [   32/  146]
train() client id: f_00005-6-1 loss: 0.382948  [   64/  146]
train() client id: f_00005-6-2 loss: 0.368587  [   96/  146]
train() client id: f_00005-6-3 loss: 0.643355  [  128/  146]
train() client id: f_00005-7-0 loss: 0.391446  [   32/  146]
train() client id: f_00005-7-1 loss: 0.704191  [   64/  146]
train() client id: f_00005-7-2 loss: 0.535157  [   96/  146]
train() client id: f_00005-7-3 loss: 0.112884  [  128/  146]
train() client id: f_00005-8-0 loss: 0.522819  [   32/  146]
train() client id: f_00005-8-1 loss: 0.607247  [   64/  146]
train() client id: f_00005-8-2 loss: 0.328657  [   96/  146]
train() client id: f_00005-8-3 loss: 0.368179  [  128/  146]
train() client id: f_00005-9-0 loss: 0.820542  [   32/  146]
train() client id: f_00005-9-1 loss: 0.266312  [   64/  146]
train() client id: f_00005-9-2 loss: 0.424585  [   96/  146]
train() client id: f_00005-9-3 loss: 0.400759  [  128/  146]
train() client id: f_00005-10-0 loss: 0.719566  [   32/  146]
train() client id: f_00005-10-1 loss: 0.370396  [   64/  146]
train() client id: f_00005-10-2 loss: 0.466337  [   96/  146]
train() client id: f_00005-10-3 loss: 0.248978  [  128/  146]
train() client id: f_00005-11-0 loss: 0.566257  [   32/  146]
train() client id: f_00005-11-1 loss: 0.417975  [   64/  146]
train() client id: f_00005-11-2 loss: 0.272669  [   96/  146]
train() client id: f_00005-11-3 loss: 0.538863  [  128/  146]
train() client id: f_00005-12-0 loss: 0.445116  [   32/  146]
train() client id: f_00005-12-1 loss: 0.581925  [   64/  146]
train() client id: f_00005-12-2 loss: 0.334934  [   96/  146]
train() client id: f_00005-12-3 loss: 0.564106  [  128/  146]
train() client id: f_00006-0-0 loss: 0.592168  [   32/   54]
train() client id: f_00006-1-0 loss: 0.549006  [   32/   54]
train() client id: f_00006-2-0 loss: 0.548502  [   32/   54]
train() client id: f_00006-3-0 loss: 0.584793  [   32/   54]
train() client id: f_00006-4-0 loss: 0.570272  [   32/   54]
train() client id: f_00006-5-0 loss: 0.520888  [   32/   54]
train() client id: f_00006-6-0 loss: 0.593018  [   32/   54]
train() client id: f_00006-7-0 loss: 0.488127  [   32/   54]
train() client id: f_00006-8-0 loss: 0.550973  [   32/   54]
train() client id: f_00006-9-0 loss: 0.532387  [   32/   54]
train() client id: f_00006-10-0 loss: 0.532576  [   32/   54]
train() client id: f_00006-11-0 loss: 0.551303  [   32/   54]
train() client id: f_00006-12-0 loss: 0.536492  [   32/   54]
train() client id: f_00007-0-0 loss: 0.833635  [   32/  179]
train() client id: f_00007-0-1 loss: 0.648779  [   64/  179]
train() client id: f_00007-0-2 loss: 0.833535  [   96/  179]
train() client id: f_00007-0-3 loss: 0.676435  [  128/  179]
train() client id: f_00007-0-4 loss: 0.570855  [  160/  179]
train() client id: f_00007-1-0 loss: 0.681165  [   32/  179]
train() client id: f_00007-1-1 loss: 0.710981  [   64/  179]
train() client id: f_00007-1-2 loss: 0.758846  [   96/  179]
train() client id: f_00007-1-3 loss: 0.579716  [  128/  179]
train() client id: f_00007-1-4 loss: 0.819395  [  160/  179]
train() client id: f_00007-2-0 loss: 0.606747  [   32/  179]
train() client id: f_00007-2-1 loss: 0.723888  [   64/  179]
train() client id: f_00007-2-2 loss: 0.636593  [   96/  179]
train() client id: f_00007-2-3 loss: 0.569044  [  128/  179]
train() client id: f_00007-2-4 loss: 0.881063  [  160/  179]
train() client id: f_00007-3-0 loss: 0.783375  [   32/  179]
train() client id: f_00007-3-1 loss: 0.494907  [   64/  179]
train() client id: f_00007-3-2 loss: 0.552803  [   96/  179]
train() client id: f_00007-3-3 loss: 0.953578  [  128/  179]
train() client id: f_00007-3-4 loss: 0.666290  [  160/  179]
train() client id: f_00007-4-0 loss: 0.614388  [   32/  179]
train() client id: f_00007-4-1 loss: 0.587439  [   64/  179]
train() client id: f_00007-4-2 loss: 0.908276  [   96/  179]
train() client id: f_00007-4-3 loss: 0.598409  [  128/  179]
train() client id: f_00007-4-4 loss: 0.638935  [  160/  179]
train() client id: f_00007-5-0 loss: 0.812971  [   32/  179]
train() client id: f_00007-5-1 loss: 0.537326  [   64/  179]
train() client id: f_00007-5-2 loss: 0.644940  [   96/  179]
train() client id: f_00007-5-3 loss: 0.595390  [  128/  179]
train() client id: f_00007-5-4 loss: 0.837508  [  160/  179]
train() client id: f_00007-6-0 loss: 0.662893  [   32/  179]
train() client id: f_00007-6-1 loss: 0.637483  [   64/  179]
train() client id: f_00007-6-2 loss: 0.554942  [   96/  179]
train() client id: f_00007-6-3 loss: 0.534696  [  128/  179]
train() client id: f_00007-6-4 loss: 0.881038  [  160/  179]
train() client id: f_00007-7-0 loss: 0.627758  [   32/  179]
train() client id: f_00007-7-1 loss: 0.809243  [   64/  179]
train() client id: f_00007-7-2 loss: 0.581929  [   96/  179]
train() client id: f_00007-7-3 loss: 0.668463  [  128/  179]
train() client id: f_00007-7-4 loss: 0.581339  [  160/  179]
train() client id: f_00007-8-0 loss: 0.593553  [   32/  179]
train() client id: f_00007-8-1 loss: 0.811990  [   64/  179]
train() client id: f_00007-8-2 loss: 0.760506  [   96/  179]
train() client id: f_00007-8-3 loss: 0.571292  [  128/  179]
train() client id: f_00007-8-4 loss: 0.614567  [  160/  179]
train() client id: f_00007-9-0 loss: 0.604130  [   32/  179]
train() client id: f_00007-9-1 loss: 0.678508  [   64/  179]
train() client id: f_00007-9-2 loss: 0.911411  [   96/  179]
train() client id: f_00007-9-3 loss: 0.593131  [  128/  179]
train() client id: f_00007-9-4 loss: 0.566646  [  160/  179]
train() client id: f_00007-10-0 loss: 0.507613  [   32/  179]
train() client id: f_00007-10-1 loss: 0.509330  [   64/  179]
train() client id: f_00007-10-2 loss: 0.583057  [   96/  179]
train() client id: f_00007-10-3 loss: 0.763540  [  128/  179]
train() client id: f_00007-10-4 loss: 0.793934  [  160/  179]
train() client id: f_00007-11-0 loss: 0.740204  [   32/  179]
train() client id: f_00007-11-1 loss: 0.526134  [   64/  179]
train() client id: f_00007-11-2 loss: 0.541003  [   96/  179]
train() client id: f_00007-11-3 loss: 0.611103  [  128/  179]
train() client id: f_00007-11-4 loss: 0.730911  [  160/  179]
train() client id: f_00007-12-0 loss: 0.684102  [   32/  179]
train() client id: f_00007-12-1 loss: 0.596426  [   64/  179]
train() client id: f_00007-12-2 loss: 0.610425  [   96/  179]
train() client id: f_00007-12-3 loss: 0.754385  [  128/  179]
train() client id: f_00007-12-4 loss: 0.600322  [  160/  179]
train() client id: f_00008-0-0 loss: 0.605971  [   32/  130]
train() client id: f_00008-0-1 loss: 0.617425  [   64/  130]
train() client id: f_00008-0-2 loss: 0.621688  [   96/  130]
train() client id: f_00008-0-3 loss: 0.615846  [  128/  130]
train() client id: f_00008-1-0 loss: 0.668450  [   32/  130]
train() client id: f_00008-1-1 loss: 0.712732  [   64/  130]
train() client id: f_00008-1-2 loss: 0.539573  [   96/  130]
train() client id: f_00008-1-3 loss: 0.546009  [  128/  130]
train() client id: f_00008-2-0 loss: 0.645680  [   32/  130]
train() client id: f_00008-2-1 loss: 0.659390  [   64/  130]
train() client id: f_00008-2-2 loss: 0.581421  [   96/  130]
train() client id: f_00008-2-3 loss: 0.585313  [  128/  130]
train() client id: f_00008-3-0 loss: 0.628443  [   32/  130]
train() client id: f_00008-3-1 loss: 0.588753  [   64/  130]
train() client id: f_00008-3-2 loss: 0.664644  [   96/  130]
train() client id: f_00008-3-3 loss: 0.581450  [  128/  130]
train() client id: f_00008-4-0 loss: 0.706079  [   32/  130]
train() client id: f_00008-4-1 loss: 0.531853  [   64/  130]
train() client id: f_00008-4-2 loss: 0.621328  [   96/  130]
train() client id: f_00008-4-3 loss: 0.586707  [  128/  130]
train() client id: f_00008-5-0 loss: 0.612526  [   32/  130]
train() client id: f_00008-5-1 loss: 0.623477  [   64/  130]
train() client id: f_00008-5-2 loss: 0.554114  [   96/  130]
train() client id: f_00008-5-3 loss: 0.673570  [  128/  130]
train() client id: f_00008-6-0 loss: 0.569518  [   32/  130]
train() client id: f_00008-6-1 loss: 0.626499  [   64/  130]
train() client id: f_00008-6-2 loss: 0.621868  [   96/  130]
train() client id: f_00008-6-3 loss: 0.615069  [  128/  130]
train() client id: f_00008-7-0 loss: 0.526211  [   32/  130]
train() client id: f_00008-7-1 loss: 0.627293  [   64/  130]
train() client id: f_00008-7-2 loss: 0.608422  [   96/  130]
train() client id: f_00008-7-3 loss: 0.681157  [  128/  130]
train() client id: f_00008-8-0 loss: 0.601105  [   32/  130]
train() client id: f_00008-8-1 loss: 0.607058  [   64/  130]
train() client id: f_00008-8-2 loss: 0.653953  [   96/  130]
train() client id: f_00008-8-3 loss: 0.603062  [  128/  130]
train() client id: f_00008-9-0 loss: 0.636982  [   32/  130]
train() client id: f_00008-9-1 loss: 0.568908  [   64/  130]
train() client id: f_00008-9-2 loss: 0.591117  [   96/  130]
train() client id: f_00008-9-3 loss: 0.643071  [  128/  130]
train() client id: f_00008-10-0 loss: 0.634555  [   32/  130]
train() client id: f_00008-10-1 loss: 0.609982  [   64/  130]
train() client id: f_00008-10-2 loss: 0.555707  [   96/  130]
train() client id: f_00008-10-3 loss: 0.663157  [  128/  130]
train() client id: f_00008-11-0 loss: 0.647737  [   32/  130]
train() client id: f_00008-11-1 loss: 0.483151  [   64/  130]
train() client id: f_00008-11-2 loss: 0.584491  [   96/  130]
train() client id: f_00008-11-3 loss: 0.756109  [  128/  130]
train() client id: f_00008-12-0 loss: 0.644853  [   32/  130]
train() client id: f_00008-12-1 loss: 0.589122  [   64/  130]
train() client id: f_00008-12-2 loss: 0.591446  [   96/  130]
train() client id: f_00008-12-3 loss: 0.646373  [  128/  130]
train() client id: f_00009-0-0 loss: 1.202253  [   32/  118]
train() client id: f_00009-0-1 loss: 1.245074  [   64/  118]
train() client id: f_00009-0-2 loss: 1.093431  [   96/  118]
train() client id: f_00009-1-0 loss: 1.170683  [   32/  118]
train() client id: f_00009-1-1 loss: 1.253427  [   64/  118]
train() client id: f_00009-1-2 loss: 1.012322  [   96/  118]
train() client id: f_00009-2-0 loss: 1.067004  [   32/  118]
train() client id: f_00009-2-1 loss: 0.982040  [   64/  118]
train() client id: f_00009-2-2 loss: 1.115464  [   96/  118]
train() client id: f_00009-3-0 loss: 1.071349  [   32/  118]
train() client id: f_00009-3-1 loss: 0.971086  [   64/  118]
train() client id: f_00009-3-2 loss: 1.142143  [   96/  118]
train() client id: f_00009-4-0 loss: 1.024666  [   32/  118]
train() client id: f_00009-4-1 loss: 1.099794  [   64/  118]
train() client id: f_00009-4-2 loss: 0.978087  [   96/  118]
train() client id: f_00009-5-0 loss: 1.171738  [   32/  118]
train() client id: f_00009-5-1 loss: 0.835184  [   64/  118]
train() client id: f_00009-5-2 loss: 0.865183  [   96/  118]
train() client id: f_00009-6-0 loss: 0.984478  [   32/  118]
train() client id: f_00009-6-1 loss: 0.986090  [   64/  118]
train() client id: f_00009-6-2 loss: 1.052761  [   96/  118]
train() client id: f_00009-7-0 loss: 1.005826  [   32/  118]
train() client id: f_00009-7-1 loss: 0.977893  [   64/  118]
train() client id: f_00009-7-2 loss: 0.937518  [   96/  118]
train() client id: f_00009-8-0 loss: 1.032649  [   32/  118]
train() client id: f_00009-8-1 loss: 0.971603  [   64/  118]
train() client id: f_00009-8-2 loss: 0.923119  [   96/  118]
train() client id: f_00009-9-0 loss: 0.938847  [   32/  118]
train() client id: f_00009-9-1 loss: 0.973171  [   64/  118]
train() client id: f_00009-9-2 loss: 1.052357  [   96/  118]
train() client id: f_00009-10-0 loss: 0.972560  [   32/  118]
train() client id: f_00009-10-1 loss: 1.056356  [   64/  118]
train() client id: f_00009-10-2 loss: 0.878659  [   96/  118]
train() client id: f_00009-11-0 loss: 0.949684  [   32/  118]
train() client id: f_00009-11-1 loss: 0.923978  [   64/  118]
train() client id: f_00009-11-2 loss: 0.961463  [   96/  118]
train() client id: f_00009-12-0 loss: 1.055503  [   32/  118]
train() client id: f_00009-12-1 loss: 0.972339  [   64/  118]
train() client id: f_00009-12-2 loss: 0.938003  [   96/  118]
At round 40 accuracy: 0.6525198938992043
At round 40 training accuracy: 0.5848423876592891
At round 40 training loss: 0.8284157399039007
update_location
xs = -3.905658 4.200318 220.009024 18.811294 0.979296 3.956410 -182.443192 -161.324852 204.663977 -147.060879 
ys = 212.587959 195.555839 1.320614 -182.455176 174.350187 157.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 234.965731 219.680970 241.672743 208.910881 200.994892 186.871505 208.068279 189.806175 228.464469 177.884552 
dists_bs = 175.174566 180.375432 431.505108 406.616552 175.980496 179.779726 177.780866 174.713663 411.010040 173.265945 
uav_gains = -110.077920 -108.968280 -110.605786 -108.250543 -107.744744 -106.860282 -108.196017 -107.043810 -109.590763 -106.293214 
bs_gains = -102.384249 -102.740026 -113.346665 -112.624241 -102.440066 -102.699799 -102.563840 -102.352212 -112.754927 -102.251029 
Round 41
-------------------------------
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.58035445 11.51525536  5.49993513  1.98836081 13.27975002  6.38965832
  2.46168242  7.8365251   5.78633913  5.1816046 ]
obj_prev = 65.51946532847218
eta_min = 2.9576516171485466e-17	eta_max = 0.9329613748543103
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 15.185923913957124	eta = 0.9090909090909091
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 28.643068262013102	eta = 0.4819799767971639
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 21.937413464592197	eta = 0.6293077986886223
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 20.723057688485657	eta = 0.6661847678972268
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 20.65725512436301	eta = 0.668306863289048
af = 13.805385376324658	bf = 1.2881215340611454	zeta = 20.65704485277882	eta = 0.6683136660986402
eta = 0.6683136660986402
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [0.0332544  0.06993981 0.03272655 0.01134872 0.08076069 0.03853288
 0.01425189 0.04724236 0.0343101  0.03114301]
ene_total = [1.87549698 3.28425833 1.87007596 0.89063245 3.74131718 1.94496118
 1.01302102 2.39242243 2.02108855 1.62377077]
ti_comp = [0.54825773 0.58461281 0.54425104 0.56025271 0.5855944  0.58474602
 0.56057165 0.56686532 0.52465853 0.58619944]
ti_coms = [0.10745239 0.0710973  0.11145908 0.09545741 0.07011572 0.0709641
 0.09513846 0.0888448  0.13105159 0.06951068]
t_total = [27.94982796 27.94982796 27.94982796 27.94982796 27.94982796 27.94982796
 27.94982796 27.94982796 27.94982796 27.94982796]
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [7.64641487e-06 6.25628775e-05 7.39575090e-06 2.91040345e-07
 9.60033089e-05 1.04577867e-05 5.75751087e-07 2.05076110e-05
 9.17048565e-06 5.49377638e-06]
ene_total = [0.45834474 0.30572102 0.4754127  0.40690231 0.30296238 0.30293225
 0.40555492 0.37957759 0.55900197 0.29652539]
optimize_network iter = 0 obj = 3.892935261137077
eta = 0.6683136660986402
freqs = [30327342.94858874 59817206.41115162 30065677.93401769 10128218.84644976
 68956163.97393337 32948387.69791197 12711921.71227483 41669829.72547625
 32697548.88052001 26563493.52335148]
eta_min = 0.6683136660986426	eta_max = 0.6683136660986362
af = 0.007721858371378171	bf = 1.2881215340611454	zeta = 0.008494044208515988	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [1.80840054e-06 1.47963121e-05 1.74911774e-06 6.88319331e-08
 2.27050766e-05 2.47329858e-06 1.36166896e-07 4.85011281e-06
 2.16884794e-06 1.29929494e-06]
ene_total = [1.67138562 1.10800963 1.73368868 1.48456874 1.09397392 1.1040216
 1.47961889 1.38247263 2.03845756 1.08123529]
ti_comp = [0.54825773 0.58461281 0.54425104 0.56025271 0.5855944  0.58474602
 0.56057165 0.56686532 0.52465853 0.58619944]
ti_coms = [0.10745239 0.0710973  0.11145908 0.09545741 0.07011572 0.0709641
 0.09513846 0.0888448  0.13105159 0.06951068]
t_total = [27.94982796 27.94982796 27.94982796 27.94982796 27.94982796 27.94982796
 27.94982796 27.94982796 27.94982796 27.94982796]
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [7.64641487e-06 6.25628775e-05 7.39575090e-06 2.91040345e-07
 9.60033089e-05 1.04577867e-05 5.75751087e-07 2.05076110e-05
 9.17048565e-06 5.49377638e-06]
ene_total = [0.45834474 0.30572102 0.4754127  0.40690231 0.30296238 0.30293225
 0.40555492 0.37957759 0.55900197 0.29652539]
optimize_network iter = 1 obj = 3.8929352611370294
eta = 0.6683136660986362
freqs = [30327342.94858875 59817206.41115168 30065677.9340177  10128218.84644976
 68956163.97393346 32948387.697912   12711921.71227484 41669829.72547627
 32697548.88052    26563493.52335151]
Done!
ene_coms = [0.01074524 0.00710973 0.01114591 0.00954574 0.00701157 0.00709641
 0.00951385 0.00888448 0.01310516 0.00695107]
ene_comp = [7.53273391e-06 6.16327412e-05 7.28579662e-06 2.86713384e-07
 9.45760062e-05 1.03023084e-05 5.67191267e-07 2.02027197e-05
 9.03414599e-06 5.41209917e-06]
ene_total = [0.01075277 0.00717136 0.01115319 0.00954603 0.00710615 0.00710671
 0.00951441 0.00890468 0.01311419 0.00695648]
At round 41 energy consumption: 0.09132598543578574
At round 41 eta: 0.6683136660986362
At round 41 a_n: 14.138223130512367
At round 41 local rounds: 13.19619071883817
At round 41 global rounds: 42.625280831488105
gradient difference: 0.43792909383773804
train() client id: f_00000-0-0 loss: 1.377996  [   32/  126]
train() client id: f_00000-0-1 loss: 1.148597  [   64/  126]
train() client id: f_00000-0-2 loss: 1.191939  [   96/  126]
train() client id: f_00000-1-0 loss: 1.084427  [   32/  126]
train() client id: f_00000-1-1 loss: 1.082416  [   64/  126]
train() client id: f_00000-1-2 loss: 1.253979  [   96/  126]
train() client id: f_00000-2-0 loss: 0.936575  [   32/  126]
train() client id: f_00000-2-1 loss: 1.067351  [   64/  126]
train() client id: f_00000-2-2 loss: 0.819359  [   96/  126]
train() client id: f_00000-3-0 loss: 1.186910  [   32/  126]
train() client id: f_00000-3-1 loss: 0.902699  [   64/  126]
train() client id: f_00000-3-2 loss: 0.779262  [   96/  126]
train() client id: f_00000-4-0 loss: 0.967798  [   32/  126]
train() client id: f_00000-4-1 loss: 0.814290  [   64/  126]
train() client id: f_00000-4-2 loss: 0.916389  [   96/  126]
train() client id: f_00000-5-0 loss: 0.816002  [   32/  126]
train() client id: f_00000-5-1 loss: 0.788405  [   64/  126]
train() client id: f_00000-5-2 loss: 0.842552  [   96/  126]
train() client id: f_00000-6-0 loss: 0.864076  [   32/  126]
train() client id: f_00000-6-1 loss: 0.790040  [   64/  126]
train() client id: f_00000-6-2 loss: 0.797349  [   96/  126]
train() client id: f_00000-7-0 loss: 0.754497  [   32/  126]
train() client id: f_00000-7-1 loss: 0.882674  [   64/  126]
train() client id: f_00000-7-2 loss: 0.809654  [   96/  126]
train() client id: f_00000-8-0 loss: 0.774252  [   32/  126]
train() client id: f_00000-8-1 loss: 0.730726  [   64/  126]
train() client id: f_00000-8-2 loss: 0.902778  [   96/  126]
train() client id: f_00000-9-0 loss: 0.849263  [   32/  126]
train() client id: f_00000-9-1 loss: 0.689279  [   64/  126]
train() client id: f_00000-9-2 loss: 0.857374  [   96/  126]
train() client id: f_00000-10-0 loss: 0.815670  [   32/  126]
train() client id: f_00000-10-1 loss: 0.697313  [   64/  126]
train() client id: f_00000-10-2 loss: 0.849886  [   96/  126]
train() client id: f_00000-11-0 loss: 0.696933  [   32/  126]
train() client id: f_00000-11-1 loss: 0.794860  [   64/  126]
train() client id: f_00000-11-2 loss: 0.770865  [   96/  126]
train() client id: f_00000-12-0 loss: 0.823172  [   32/  126]
train() client id: f_00000-12-1 loss: 0.677849  [   64/  126]
train() client id: f_00000-12-2 loss: 0.786833  [   96/  126]
train() client id: f_00001-0-0 loss: 0.477819  [   32/  265]
train() client id: f_00001-0-1 loss: 0.497601  [   64/  265]
train() client id: f_00001-0-2 loss: 0.537415  [   96/  265]
train() client id: f_00001-0-3 loss: 0.534580  [  128/  265]
train() client id: f_00001-0-4 loss: 0.455622  [  160/  265]
train() client id: f_00001-0-5 loss: 0.507727  [  192/  265]
train() client id: f_00001-0-6 loss: 0.512381  [  224/  265]
train() client id: f_00001-0-7 loss: 0.432599  [  256/  265]
train() client id: f_00001-1-0 loss: 0.497309  [   32/  265]
train() client id: f_00001-1-1 loss: 0.550400  [   64/  265]
train() client id: f_00001-1-2 loss: 0.424732  [   96/  265]
train() client id: f_00001-1-3 loss: 0.456198  [  128/  265]
train() client id: f_00001-1-4 loss: 0.561430  [  160/  265]
train() client id: f_00001-1-5 loss: 0.421184  [  192/  265]
train() client id: f_00001-1-6 loss: 0.499240  [  224/  265]
train() client id: f_00001-1-7 loss: 0.493061  [  256/  265]
train() client id: f_00001-2-0 loss: 0.604121  [   32/  265]
train() client id: f_00001-2-1 loss: 0.461087  [   64/  265]
train() client id: f_00001-2-2 loss: 0.397878  [   96/  265]
train() client id: f_00001-2-3 loss: 0.522096  [  128/  265]
train() client id: f_00001-2-4 loss: 0.454842  [  160/  265]
train() client id: f_00001-2-5 loss: 0.446844  [  192/  265]
train() client id: f_00001-2-6 loss: 0.404307  [  224/  265]
train() client id: f_00001-2-7 loss: 0.504716  [  256/  265]
train() client id: f_00001-3-0 loss: 0.480367  [   32/  265]
train() client id: f_00001-3-1 loss: 0.504309  [   64/  265]
train() client id: f_00001-3-2 loss: 0.437827  [   96/  265]
train() client id: f_00001-3-3 loss: 0.480605  [  128/  265]
train() client id: f_00001-3-4 loss: 0.514162  [  160/  265]
train() client id: f_00001-3-5 loss: 0.422825  [  192/  265]
train() client id: f_00001-3-6 loss: 0.409458  [  224/  265]
train() client id: f_00001-3-7 loss: 0.519399  [  256/  265]
train() client id: f_00001-4-0 loss: 0.499142  [   32/  265]
train() client id: f_00001-4-1 loss: 0.506446  [   64/  265]
train() client id: f_00001-4-2 loss: 0.415962  [   96/  265]
train() client id: f_00001-4-3 loss: 0.422532  [  128/  265]
train() client id: f_00001-4-4 loss: 0.501573  [  160/  265]
train() client id: f_00001-4-5 loss: 0.562901  [  192/  265]
train() client id: f_00001-4-6 loss: 0.459919  [  224/  265]
train() client id: f_00001-4-7 loss: 0.374250  [  256/  265]
train() client id: f_00001-5-0 loss: 0.399593  [   32/  265]
train() client id: f_00001-5-1 loss: 0.373195  [   64/  265]
train() client id: f_00001-5-2 loss: 0.536941  [   96/  265]
train() client id: f_00001-5-3 loss: 0.421969  [  128/  265]
train() client id: f_00001-5-4 loss: 0.701373  [  160/  265]
train() client id: f_00001-5-5 loss: 0.444547  [  192/  265]
train() client id: f_00001-5-6 loss: 0.454112  [  224/  265]
train() client id: f_00001-5-7 loss: 0.425835  [  256/  265]
train() client id: f_00001-6-0 loss: 0.469146  [   32/  265]
train() client id: f_00001-6-1 loss: 0.438917  [   64/  265]
train() client id: f_00001-6-2 loss: 0.416976  [   96/  265]
train() client id: f_00001-6-3 loss: 0.499373  [  128/  265]
train() client id: f_00001-6-4 loss: 0.404572  [  160/  265]
train() client id: f_00001-6-5 loss: 0.425895  [  192/  265]
train() client id: f_00001-6-6 loss: 0.465933  [  224/  265]
train() client id: f_00001-6-7 loss: 0.703460  [  256/  265]
train() client id: f_00001-7-0 loss: 0.648832  [   32/  265]
train() client id: f_00001-7-1 loss: 0.402031  [   64/  265]
train() client id: f_00001-7-2 loss: 0.390867  [   96/  265]
train() client id: f_00001-7-3 loss: 0.506134  [  128/  265]
train() client id: f_00001-7-4 loss: 0.394175  [  160/  265]
train() client id: f_00001-7-5 loss: 0.510405  [  192/  265]
train() client id: f_00001-7-6 loss: 0.544752  [  224/  265]
train() client id: f_00001-7-7 loss: 0.415631  [  256/  265]
train() client id: f_00001-8-0 loss: 0.459716  [   32/  265]
train() client id: f_00001-8-1 loss: 0.521484  [   64/  265]
train() client id: f_00001-8-2 loss: 0.455495  [   96/  265]
train() client id: f_00001-8-3 loss: 0.477579  [  128/  265]
train() client id: f_00001-8-4 loss: 0.486192  [  160/  265]
train() client id: f_00001-8-5 loss: 0.456859  [  192/  265]
train() client id: f_00001-8-6 loss: 0.509820  [  224/  265]
train() client id: f_00001-8-7 loss: 0.387565  [  256/  265]
train() client id: f_00001-9-0 loss: 0.416220  [   32/  265]
train() client id: f_00001-9-1 loss: 0.407048  [   64/  265]
train() client id: f_00001-9-2 loss: 0.530834  [   96/  265]
train() client id: f_00001-9-3 loss: 0.381062  [  128/  265]
train() client id: f_00001-9-4 loss: 0.601143  [  160/  265]
train() client id: f_00001-9-5 loss: 0.547776  [  192/  265]
train() client id: f_00001-9-6 loss: 0.381785  [  224/  265]
train() client id: f_00001-9-7 loss: 0.549919  [  256/  265]
train() client id: f_00001-10-0 loss: 0.445971  [   32/  265]
train() client id: f_00001-10-1 loss: 0.442779  [   64/  265]
train() client id: f_00001-10-2 loss: 0.540986  [   96/  265]
train() client id: f_00001-10-3 loss: 0.439884  [  128/  265]
train() client id: f_00001-10-4 loss: 0.466275  [  160/  265]
train() client id: f_00001-10-5 loss: 0.581190  [  192/  265]
train() client id: f_00001-10-6 loss: 0.463966  [  224/  265]
train() client id: f_00001-10-7 loss: 0.439195  [  256/  265]
train() client id: f_00001-11-0 loss: 0.441471  [   32/  265]
train() client id: f_00001-11-1 loss: 0.532610  [   64/  265]
train() client id: f_00001-11-2 loss: 0.391923  [   96/  265]
train() client id: f_00001-11-3 loss: 0.514359  [  128/  265]
train() client id: f_00001-11-4 loss: 0.601987  [  160/  265]
train() client id: f_00001-11-5 loss: 0.369695  [  192/  265]
train() client id: f_00001-11-6 loss: 0.472631  [  224/  265]
train() client id: f_00001-11-7 loss: 0.459644  [  256/  265]
train() client id: f_00001-12-0 loss: 0.505476  [   32/  265]
train() client id: f_00001-12-1 loss: 0.420636  [   64/  265]
train() client id: f_00001-12-2 loss: 0.663603  [   96/  265]
train() client id: f_00001-12-3 loss: 0.460483  [  128/  265]
train() client id: f_00001-12-4 loss: 0.514318  [  160/  265]
train() client id: f_00001-12-5 loss: 0.393171  [  192/  265]
train() client id: f_00001-12-6 loss: 0.384115  [  224/  265]
train() client id: f_00001-12-7 loss: 0.494646  [  256/  265]
train() client id: f_00002-0-0 loss: 1.116788  [   32/  124]
train() client id: f_00002-0-1 loss: 1.213809  [   64/  124]
train() client id: f_00002-0-2 loss: 1.323531  [   96/  124]
train() client id: f_00002-1-0 loss: 1.166360  [   32/  124]
train() client id: f_00002-1-1 loss: 1.360810  [   64/  124]
train() client id: f_00002-1-2 loss: 1.125606  [   96/  124]
train() client id: f_00002-2-0 loss: 1.254067  [   32/  124]
train() client id: f_00002-2-1 loss: 1.094372  [   64/  124]
train() client id: f_00002-2-2 loss: 1.171051  [   96/  124]
train() client id: f_00002-3-0 loss: 1.031548  [   32/  124]
train() client id: f_00002-3-1 loss: 1.187136  [   64/  124]
train() client id: f_00002-3-2 loss: 1.250666  [   96/  124]
train() client id: f_00002-4-0 loss: 1.091944  [   32/  124]
train() client id: f_00002-4-1 loss: 1.291036  [   64/  124]
train() client id: f_00002-4-2 loss: 1.001243  [   96/  124]
train() client id: f_00002-5-0 loss: 1.141696  [   32/  124]
train() client id: f_00002-5-1 loss: 1.031566  [   64/  124]
train() client id: f_00002-5-2 loss: 1.233990  [   96/  124]
train() client id: f_00002-6-0 loss: 1.211771  [   32/  124]
train() client id: f_00002-6-1 loss: 1.208146  [   64/  124]
train() client id: f_00002-6-2 loss: 1.001992  [   96/  124]
train() client id: f_00002-7-0 loss: 1.015649  [   32/  124]
train() client id: f_00002-7-1 loss: 1.133481  [   64/  124]
train() client id: f_00002-7-2 loss: 1.114996  [   96/  124]
train() client id: f_00002-8-0 loss: 1.098091  [   32/  124]
train() client id: f_00002-8-1 loss: 1.041479  [   64/  124]
train() client id: f_00002-8-2 loss: 1.342341  [   96/  124]
train() client id: f_00002-9-0 loss: 1.167831  [   32/  124]
train() client id: f_00002-9-1 loss: 1.119691  [   64/  124]
train() client id: f_00002-9-2 loss: 1.124431  [   96/  124]
train() client id: f_00002-10-0 loss: 1.231679  [   32/  124]
train() client id: f_00002-10-1 loss: 1.079793  [   64/  124]
train() client id: f_00002-10-2 loss: 1.191788  [   96/  124]
train() client id: f_00002-11-0 loss: 1.154672  [   32/  124]
train() client id: f_00002-11-1 loss: 1.326094  [   64/  124]
train() client id: f_00002-11-2 loss: 1.010819  [   96/  124]
train() client id: f_00002-12-0 loss: 1.083645  [   32/  124]
train() client id: f_00002-12-1 loss: 1.069096  [   64/  124]
train() client id: f_00002-12-2 loss: 1.235910  [   96/  124]
train() client id: f_00003-0-0 loss: 0.757200  [   32/   43]
train() client id: f_00003-1-0 loss: 0.730982  [   32/   43]
train() client id: f_00003-2-0 loss: 0.836236  [   32/   43]
train() client id: f_00003-3-0 loss: 0.569736  [   32/   43]
train() client id: f_00003-4-0 loss: 0.745747  [   32/   43]
train() client id: f_00003-5-0 loss: 0.840204  [   32/   43]
train() client id: f_00003-6-0 loss: 0.890772  [   32/   43]
train() client id: f_00003-7-0 loss: 0.699557  [   32/   43]
train() client id: f_00003-8-0 loss: 0.788361  [   32/   43]
train() client id: f_00003-9-0 loss: 0.858763  [   32/   43]
train() client id: f_00003-10-0 loss: 0.679137  [   32/   43]
train() client id: f_00003-11-0 loss: 0.811996  [   32/   43]
train() client id: f_00003-12-0 loss: 0.740967  [   32/   43]
train() client id: f_00004-0-0 loss: 0.872654  [   32/  306]
train() client id: f_00004-0-1 loss: 0.859167  [   64/  306]
train() client id: f_00004-0-2 loss: 0.977245  [   96/  306]
train() client id: f_00004-0-3 loss: 0.847619  [  128/  306]
train() client id: f_00004-0-4 loss: 0.895239  [  160/  306]
train() client id: f_00004-0-5 loss: 0.793581  [  192/  306]
train() client id: f_00004-0-6 loss: 0.934561  [  224/  306]
train() client id: f_00004-0-7 loss: 0.864816  [  256/  306]
train() client id: f_00004-0-8 loss: 1.097771  [  288/  306]
train() client id: f_00004-1-0 loss: 0.909338  [   32/  306]
train() client id: f_00004-1-1 loss: 1.000215  [   64/  306]
train() client id: f_00004-1-2 loss: 1.036952  [   96/  306]
train() client id: f_00004-1-3 loss: 0.947591  [  128/  306]
train() client id: f_00004-1-4 loss: 0.908994  [  160/  306]
train() client id: f_00004-1-5 loss: 0.841655  [  192/  306]
train() client id: f_00004-1-6 loss: 0.776128  [  224/  306]
train() client id: f_00004-1-7 loss: 0.822418  [  256/  306]
train() client id: f_00004-1-8 loss: 0.925550  [  288/  306]
train() client id: f_00004-2-0 loss: 0.902116  [   32/  306]
train() client id: f_00004-2-1 loss: 0.852354  [   64/  306]
train() client id: f_00004-2-2 loss: 0.842825  [   96/  306]
train() client id: f_00004-2-3 loss: 1.013922  [  128/  306]
train() client id: f_00004-2-4 loss: 0.985384  [  160/  306]
train() client id: f_00004-2-5 loss: 0.902920  [  192/  306]
train() client id: f_00004-2-6 loss: 0.888156  [  224/  306]
train() client id: f_00004-2-7 loss: 0.932632  [  256/  306]
train() client id: f_00004-2-8 loss: 0.833969  [  288/  306]
train() client id: f_00004-3-0 loss: 0.947320  [   32/  306]
train() client id: f_00004-3-1 loss: 0.833890  [   64/  306]
train() client id: f_00004-3-2 loss: 0.930418  [   96/  306]
train() client id: f_00004-3-3 loss: 0.964776  [  128/  306]
train() client id: f_00004-3-4 loss: 0.837792  [  160/  306]
train() client id: f_00004-3-5 loss: 0.916123  [  192/  306]
train() client id: f_00004-3-6 loss: 0.928850  [  224/  306]
train() client id: f_00004-3-7 loss: 0.915114  [  256/  306]
train() client id: f_00004-3-8 loss: 0.791655  [  288/  306]
train() client id: f_00004-4-0 loss: 0.860323  [   32/  306]
train() client id: f_00004-4-1 loss: 0.918786  [   64/  306]
train() client id: f_00004-4-2 loss: 0.870982  [   96/  306]
train() client id: f_00004-4-3 loss: 0.837982  [  128/  306]
train() client id: f_00004-4-4 loss: 0.917069  [  160/  306]
train() client id: f_00004-4-5 loss: 0.972323  [  192/  306]
train() client id: f_00004-4-6 loss: 0.866734  [  224/  306]
train() client id: f_00004-4-7 loss: 0.912369  [  256/  306]
train() client id: f_00004-4-8 loss: 0.912057  [  288/  306]
train() client id: f_00004-5-0 loss: 1.047289  [   32/  306]
train() client id: f_00004-5-1 loss: 0.874794  [   64/  306]
train() client id: f_00004-5-2 loss: 0.877824  [   96/  306]
train() client id: f_00004-5-3 loss: 0.883496  [  128/  306]
train() client id: f_00004-5-4 loss: 0.821629  [  160/  306]
train() client id: f_00004-5-5 loss: 0.871586  [  192/  306]
train() client id: f_00004-5-6 loss: 0.881447  [  224/  306]
train() client id: f_00004-5-7 loss: 0.930087  [  256/  306]
train() client id: f_00004-5-8 loss: 0.919191  [  288/  306]
train() client id: f_00004-6-0 loss: 1.002530  [   32/  306]
train() client id: f_00004-6-1 loss: 0.893547  [   64/  306]
train() client id: f_00004-6-2 loss: 0.913301  [   96/  306]
train() client id: f_00004-6-3 loss: 0.857720  [  128/  306]
train() client id: f_00004-6-4 loss: 0.890655  [  160/  306]
train() client id: f_00004-6-5 loss: 0.765709  [  192/  306]
train() client id: f_00004-6-6 loss: 1.004353  [  224/  306]
train() client id: f_00004-6-7 loss: 0.823536  [  256/  306]
train() client id: f_00004-6-8 loss: 0.928940  [  288/  306]
train() client id: f_00004-7-0 loss: 0.999717  [   32/  306]
train() client id: f_00004-7-1 loss: 0.948636  [   64/  306]
train() client id: f_00004-7-2 loss: 0.881569  [   96/  306]
train() client id: f_00004-7-3 loss: 0.912282  [  128/  306]
train() client id: f_00004-7-4 loss: 0.775792  [  160/  306]
train() client id: f_00004-7-5 loss: 0.901394  [  192/  306]
train() client id: f_00004-7-6 loss: 0.890131  [  224/  306]
train() client id: f_00004-7-7 loss: 0.851887  [  256/  306]
train() client id: f_00004-7-8 loss: 0.955210  [  288/  306]
train() client id: f_00004-8-0 loss: 0.844219  [   32/  306]
train() client id: f_00004-8-1 loss: 0.899831  [   64/  306]
train() client id: f_00004-8-2 loss: 0.896128  [   96/  306]
train() client id: f_00004-8-3 loss: 1.013473  [  128/  306]
train() client id: f_00004-8-4 loss: 0.897100  [  160/  306]
train() client id: f_00004-8-5 loss: 0.902585  [  192/  306]
train() client id: f_00004-8-6 loss: 0.849873  [  224/  306]
train() client id: f_00004-8-7 loss: 0.814871  [  256/  306]
train() client id: f_00004-8-8 loss: 0.890342  [  288/  306]
train() client id: f_00004-9-0 loss: 0.917902  [   32/  306]
train() client id: f_00004-9-1 loss: 0.905775  [   64/  306]
train() client id: f_00004-9-2 loss: 0.844132  [   96/  306]
train() client id: f_00004-9-3 loss: 0.898705  [  128/  306]
train() client id: f_00004-9-4 loss: 0.848523  [  160/  306]
train() client id: f_00004-9-5 loss: 0.887237  [  192/  306]
train() client id: f_00004-9-6 loss: 0.859707  [  224/  306]
train() client id: f_00004-9-7 loss: 0.913501  [  256/  306]
train() client id: f_00004-9-8 loss: 0.929049  [  288/  306]
train() client id: f_00004-10-0 loss: 0.925753  [   32/  306]
train() client id: f_00004-10-1 loss: 0.868138  [   64/  306]
train() client id: f_00004-10-2 loss: 0.930220  [   96/  306]
train() client id: f_00004-10-3 loss: 0.805674  [  128/  306]
train() client id: f_00004-10-4 loss: 0.892983  [  160/  306]
train() client id: f_00004-10-5 loss: 0.868303  [  192/  306]
train() client id: f_00004-10-6 loss: 1.047776  [  224/  306]
train() client id: f_00004-10-7 loss: 0.783798  [  256/  306]
train() client id: f_00004-10-8 loss: 0.829881  [  288/  306]
train() client id: f_00004-11-0 loss: 0.904418  [   32/  306]
train() client id: f_00004-11-1 loss: 1.012402  [   64/  306]
train() client id: f_00004-11-2 loss: 0.801143  [   96/  306]
train() client id: f_00004-11-3 loss: 0.869428  [  128/  306]
train() client id: f_00004-11-4 loss: 0.986044  [  160/  306]
train() client id: f_00004-11-5 loss: 0.765025  [  192/  306]
train() client id: f_00004-11-6 loss: 0.941319  [  224/  306]
train() client id: f_00004-11-7 loss: 0.809645  [  256/  306]
train() client id: f_00004-11-8 loss: 0.874766  [  288/  306]
train() client id: f_00004-12-0 loss: 0.819940  [   32/  306]
train() client id: f_00004-12-1 loss: 0.799238  [   64/  306]
train() client id: f_00004-12-2 loss: 0.973253  [   96/  306]
train() client id: f_00004-12-3 loss: 0.911893  [  128/  306]
train() client id: f_00004-12-4 loss: 0.971563  [  160/  306]
train() client id: f_00004-12-5 loss: 0.912217  [  192/  306]
train() client id: f_00004-12-6 loss: 0.879980  [  224/  306]
train() client id: f_00004-12-7 loss: 0.879052  [  256/  306]
train() client id: f_00004-12-8 loss: 0.865574  [  288/  306]
train() client id: f_00005-0-0 loss: 0.653398  [   32/  146]
train() client id: f_00005-0-1 loss: 0.301157  [   64/  146]
train() client id: f_00005-0-2 loss: 0.485331  [   96/  146]
train() client id: f_00005-0-3 loss: 0.594081  [  128/  146]
train() client id: f_00005-1-0 loss: 0.431483  [   32/  146]
train() client id: f_00005-1-1 loss: 0.480868  [   64/  146]
train() client id: f_00005-1-2 loss: 0.378308  [   96/  146]
train() client id: f_00005-1-3 loss: 0.615441  [  128/  146]
train() client id: f_00005-2-0 loss: 0.635936  [   32/  146]
train() client id: f_00005-2-1 loss: 0.590410  [   64/  146]
train() client id: f_00005-2-2 loss: 0.529342  [   96/  146]
train() client id: f_00005-2-3 loss: 0.518470  [  128/  146]
train() client id: f_00005-3-0 loss: 0.271595  [   32/  146]
train() client id: f_00005-3-1 loss: 0.644099  [   64/  146]
train() client id: f_00005-3-2 loss: 0.530014  [   96/  146]
train() client id: f_00005-3-3 loss: 0.632477  [  128/  146]
train() client id: f_00005-4-0 loss: 0.495389  [   32/  146]
train() client id: f_00005-4-1 loss: 0.648542  [   64/  146]
train() client id: f_00005-4-2 loss: 0.410412  [   96/  146]
train() client id: f_00005-4-3 loss: 0.400714  [  128/  146]
train() client id: f_00005-5-0 loss: 0.300290  [   32/  146]
train() client id: f_00005-5-1 loss: 0.558917  [   64/  146]
train() client id: f_00005-5-2 loss: 0.763608  [   96/  146]
train() client id: f_00005-5-3 loss: 0.349003  [  128/  146]
train() client id: f_00005-6-0 loss: 0.525393  [   32/  146]
train() client id: f_00005-6-1 loss: 0.456518  [   64/  146]
train() client id: f_00005-6-2 loss: 0.544720  [   96/  146]
train() client id: f_00005-6-3 loss: 0.396921  [  128/  146]
train() client id: f_00005-7-0 loss: 0.400207  [   32/  146]
train() client id: f_00005-7-1 loss: 0.306196  [   64/  146]
train() client id: f_00005-7-2 loss: 0.550885  [   96/  146]
train() client id: f_00005-7-3 loss: 0.686241  [  128/  146]
train() client id: f_00005-8-0 loss: 0.352081  [   32/  146]
train() client id: f_00005-8-1 loss: 0.610488  [   64/  146]
train() client id: f_00005-8-2 loss: 0.228852  [   96/  146]
train() client id: f_00005-8-3 loss: 0.854658  [  128/  146]
train() client id: f_00005-9-0 loss: 0.589621  [   32/  146]
train() client id: f_00005-9-1 loss: 0.411335  [   64/  146]
train() client id: f_00005-9-2 loss: 0.508461  [   96/  146]
train() client id: f_00005-9-3 loss: 0.643516  [  128/  146]
train() client id: f_00005-10-0 loss: 0.423372  [   32/  146]
train() client id: f_00005-10-1 loss: 0.706133  [   64/  146]
train() client id: f_00005-10-2 loss: 0.571508  [   96/  146]
train() client id: f_00005-10-3 loss: 0.378542  [  128/  146]
train() client id: f_00005-11-0 loss: 0.515815  [   32/  146]
train() client id: f_00005-11-1 loss: 0.390317  [   64/  146]
train() client id: f_00005-11-2 loss: 0.418514  [   96/  146]
train() client id: f_00005-11-3 loss: 0.448762  [  128/  146]
train() client id: f_00005-12-0 loss: 0.599513  [   32/  146]
train() client id: f_00005-12-1 loss: 0.531563  [   64/  146]
train() client id: f_00005-12-2 loss: 0.405288  [   96/  146]
train() client id: f_00005-12-3 loss: 0.690544  [  128/  146]
train() client id: f_00006-0-0 loss: 0.537915  [   32/   54]
train() client id: f_00006-1-0 loss: 0.484787  [   32/   54]
train() client id: f_00006-2-0 loss: 0.512431  [   32/   54]
train() client id: f_00006-3-0 loss: 0.569145  [   32/   54]
train() client id: f_00006-4-0 loss: 0.560896  [   32/   54]
train() client id: f_00006-5-0 loss: 0.588629  [   32/   54]
train() client id: f_00006-6-0 loss: 0.517301  [   32/   54]
train() client id: f_00006-7-0 loss: 0.512446  [   32/   54]
train() client id: f_00006-8-0 loss: 0.540514  [   32/   54]
train() client id: f_00006-9-0 loss: 0.589605  [   32/   54]
train() client id: f_00006-10-0 loss: 0.483395  [   32/   54]
train() client id: f_00006-11-0 loss: 0.524599  [   32/   54]
train() client id: f_00006-12-0 loss: 0.520117  [   32/   54]
train() client id: f_00007-0-0 loss: 0.630631  [   32/  179]
train() client id: f_00007-0-1 loss: 0.721323  [   64/  179]
train() client id: f_00007-0-2 loss: 0.462898  [   96/  179]
train() client id: f_00007-0-3 loss: 0.689941  [  128/  179]
train() client id: f_00007-0-4 loss: 0.588944  [  160/  179]
train() client id: f_00007-1-0 loss: 0.491158  [   32/  179]
train() client id: f_00007-1-1 loss: 0.594978  [   64/  179]
train() client id: f_00007-1-2 loss: 0.794759  [   96/  179]
train() client id: f_00007-1-3 loss: 0.530082  [  128/  179]
train() client id: f_00007-1-4 loss: 0.750868  [  160/  179]
train() client id: f_00007-2-0 loss: 0.858152  [   32/  179]
train() client id: f_00007-2-1 loss: 0.508892  [   64/  179]
train() client id: f_00007-2-2 loss: 0.586342  [   96/  179]
train() client id: f_00007-2-3 loss: 0.648743  [  128/  179]
train() client id: f_00007-2-4 loss: 0.501436  [  160/  179]
train() client id: f_00007-3-0 loss: 0.543165  [   32/  179]
train() client id: f_00007-3-1 loss: 0.504525  [   64/  179]
train() client id: f_00007-3-2 loss: 0.859265  [   96/  179]
train() client id: f_00007-3-3 loss: 0.666763  [  128/  179]
train() client id: f_00007-3-4 loss: 0.489847  [  160/  179]
train() client id: f_00007-4-0 loss: 0.525419  [   32/  179]
train() client id: f_00007-4-1 loss: 0.559232  [   64/  179]
train() client id: f_00007-4-2 loss: 0.467877  [   96/  179]
train() client id: f_00007-4-3 loss: 0.628973  [  128/  179]
train() client id: f_00007-4-4 loss: 0.890932  [  160/  179]
train() client id: f_00007-5-0 loss: 0.737951  [   32/  179]
train() client id: f_00007-5-1 loss: 0.512178  [   64/  179]
train() client id: f_00007-5-2 loss: 0.606328  [   96/  179]
train() client id: f_00007-5-3 loss: 0.467640  [  128/  179]
train() client id: f_00007-5-4 loss: 0.657458  [  160/  179]
train() client id: f_00007-6-0 loss: 0.559986  [   32/  179]
train() client id: f_00007-6-1 loss: 0.657699  [   64/  179]
train() client id: f_00007-6-2 loss: 0.502205  [   96/  179]
train() client id: f_00007-6-3 loss: 0.424916  [  128/  179]
train() client id: f_00007-6-4 loss: 0.763792  [  160/  179]
train() client id: f_00007-7-0 loss: 0.707381  [   32/  179]
train() client id: f_00007-7-1 loss: 0.431371  [   64/  179]
train() client id: f_00007-7-2 loss: 0.486271  [   96/  179]
train() client id: f_00007-7-3 loss: 0.684330  [  128/  179]
train() client id: f_00007-7-4 loss: 0.632088  [  160/  179]
train() client id: f_00007-8-0 loss: 0.466266  [   32/  179]
train() client id: f_00007-8-1 loss: 0.742417  [   64/  179]
train() client id: f_00007-8-2 loss: 0.532275  [   96/  179]
train() client id: f_00007-8-3 loss: 0.654869  [  128/  179]
train() client id: f_00007-8-4 loss: 0.616484  [  160/  179]
train() client id: f_00007-9-0 loss: 0.532256  [   32/  179]
train() client id: f_00007-9-1 loss: 0.891521  [   64/  179]
train() client id: f_00007-9-2 loss: 0.541422  [   96/  179]
train() client id: f_00007-9-3 loss: 0.442663  [  128/  179]
train() client id: f_00007-9-4 loss: 0.578698  [  160/  179]
train() client id: f_00007-10-0 loss: 0.485894  [   32/  179]
train() client id: f_00007-10-1 loss: 0.529259  [   64/  179]
train() client id: f_00007-10-2 loss: 0.729887  [   96/  179]
train() client id: f_00007-10-3 loss: 0.833088  [  128/  179]
train() client id: f_00007-10-4 loss: 0.461965  [  160/  179]
train() client id: f_00007-11-0 loss: 0.555780  [   32/  179]
train() client id: f_00007-11-1 loss: 0.458758  [   64/  179]
train() client id: f_00007-11-2 loss: 0.731797  [   96/  179]
train() client id: f_00007-11-3 loss: 0.819989  [  128/  179]
train() client id: f_00007-11-4 loss: 0.514378  [  160/  179]
train() client id: f_00007-12-0 loss: 0.484963  [   32/  179]
train() client id: f_00007-12-1 loss: 0.766109  [   64/  179]
train() client id: f_00007-12-2 loss: 0.530376  [   96/  179]
train() client id: f_00007-12-3 loss: 0.826343  [  128/  179]
train() client id: f_00007-12-4 loss: 0.524524  [  160/  179]
train() client id: f_00008-0-0 loss: 0.672919  [   32/  130]
train() client id: f_00008-0-1 loss: 0.677712  [   64/  130]
train() client id: f_00008-0-2 loss: 0.596877  [   96/  130]
train() client id: f_00008-0-3 loss: 0.693406  [  128/  130]
train() client id: f_00008-1-0 loss: 0.661477  [   32/  130]
train() client id: f_00008-1-1 loss: 0.650301  [   64/  130]
train() client id: f_00008-1-2 loss: 0.724042  [   96/  130]
train() client id: f_00008-1-3 loss: 0.623295  [  128/  130]
train() client id: f_00008-2-0 loss: 0.584640  [   32/  130]
train() client id: f_00008-2-1 loss: 0.702451  [   64/  130]
train() client id: f_00008-2-2 loss: 0.765728  [   96/  130]
train() client id: f_00008-2-3 loss: 0.620563  [  128/  130]
train() client id: f_00008-3-0 loss: 0.675044  [   32/  130]
train() client id: f_00008-3-1 loss: 0.693536  [   64/  130]
train() client id: f_00008-3-2 loss: 0.753179  [   96/  130]
train() client id: f_00008-3-3 loss: 0.560570  [  128/  130]
train() client id: f_00008-4-0 loss: 0.643959  [   32/  130]
train() client id: f_00008-4-1 loss: 0.594871  [   64/  130]
train() client id: f_00008-4-2 loss: 0.648432  [   96/  130]
train() client id: f_00008-4-3 loss: 0.791226  [  128/  130]
train() client id: f_00008-5-0 loss: 0.714697  [   32/  130]
train() client id: f_00008-5-1 loss: 0.595560  [   64/  130]
train() client id: f_00008-5-2 loss: 0.593455  [   96/  130]
train() client id: f_00008-5-3 loss: 0.768620  [  128/  130]
train() client id: f_00008-6-0 loss: 0.626179  [   32/  130]
train() client id: f_00008-6-1 loss: 0.655924  [   64/  130]
train() client id: f_00008-6-2 loss: 0.584205  [   96/  130]
train() client id: f_00008-6-3 loss: 0.779216  [  128/  130]
train() client id: f_00008-7-0 loss: 0.682770  [   32/  130]
train() client id: f_00008-7-1 loss: 0.740563  [   64/  130]
train() client id: f_00008-7-2 loss: 0.554235  [   96/  130]
train() client id: f_00008-7-3 loss: 0.702965  [  128/  130]
train() client id: f_00008-8-0 loss: 0.721520  [   32/  130]
train() client id: f_00008-8-1 loss: 0.574830  [   64/  130]
train() client id: f_00008-8-2 loss: 0.695943  [   96/  130]
train() client id: f_00008-8-3 loss: 0.628856  [  128/  130]
train() client id: f_00008-9-0 loss: 0.615065  [   32/  130]
train() client id: f_00008-9-1 loss: 0.609258  [   64/  130]
train() client id: f_00008-9-2 loss: 0.644553  [   96/  130]
train() client id: f_00008-9-3 loss: 0.795136  [  128/  130]
train() client id: f_00008-10-0 loss: 0.656653  [   32/  130]
train() client id: f_00008-10-1 loss: 0.727576  [   64/  130]
train() client id: f_00008-10-2 loss: 0.695360  [   96/  130]
train() client id: f_00008-10-3 loss: 0.598415  [  128/  130]
train() client id: f_00008-11-0 loss: 0.709180  [   32/  130]
train() client id: f_00008-11-1 loss: 0.718086  [   64/  130]
train() client id: f_00008-11-2 loss: 0.609273  [   96/  130]
train() client id: f_00008-11-3 loss: 0.580029  [  128/  130]
train() client id: f_00008-12-0 loss: 0.620166  [   32/  130]
train() client id: f_00008-12-1 loss: 0.620162  [   64/  130]
train() client id: f_00008-12-2 loss: 0.751908  [   96/  130]
train() client id: f_00008-12-3 loss: 0.675787  [  128/  130]
train() client id: f_00009-0-0 loss: 1.150522  [   32/  118]
train() client id: f_00009-0-1 loss: 1.124074  [   64/  118]
train() client id: f_00009-0-2 loss: 1.246495  [   96/  118]
train() client id: f_00009-1-0 loss: 1.166288  [   32/  118]
train() client id: f_00009-1-1 loss: 1.072636  [   64/  118]
train() client id: f_00009-1-2 loss: 1.086626  [   96/  118]
train() client id: f_00009-2-0 loss: 0.974391  [   32/  118]
train() client id: f_00009-2-1 loss: 1.090754  [   64/  118]
train() client id: f_00009-2-2 loss: 1.035138  [   96/  118]
train() client id: f_00009-3-0 loss: 0.993523  [   32/  118]
train() client id: f_00009-3-1 loss: 0.925978  [   64/  118]
train() client id: f_00009-3-2 loss: 1.109219  [   96/  118]
train() client id: f_00009-4-0 loss: 1.153646  [   32/  118]
train() client id: f_00009-4-1 loss: 0.877509  [   64/  118]
train() client id: f_00009-4-2 loss: 0.960457  [   96/  118]
train() client id: f_00009-5-0 loss: 1.080549  [   32/  118]
train() client id: f_00009-5-1 loss: 0.856125  [   64/  118]
train() client id: f_00009-5-2 loss: 0.896668  [   96/  118]
train() client id: f_00009-6-0 loss: 1.007057  [   32/  118]
train() client id: f_00009-6-1 loss: 0.816699  [   64/  118]
train() client id: f_00009-6-2 loss: 0.925694  [   96/  118]
train() client id: f_00009-7-0 loss: 0.985639  [   32/  118]
train() client id: f_00009-7-1 loss: 0.931472  [   64/  118]
train() client id: f_00009-7-2 loss: 0.883160  [   96/  118]
train() client id: f_00009-8-0 loss: 0.954307  [   32/  118]
train() client id: f_00009-8-1 loss: 0.907038  [   64/  118]
train() client id: f_00009-8-2 loss: 0.830087  [   96/  118]
train() client id: f_00009-9-0 loss: 0.879366  [   32/  118]
train() client id: f_00009-9-1 loss: 0.972909  [   64/  118]
train() client id: f_00009-9-2 loss: 0.863691  [   96/  118]
train() client id: f_00009-10-0 loss: 0.970990  [   32/  118]
train() client id: f_00009-10-1 loss: 0.749881  [   64/  118]
train() client id: f_00009-10-2 loss: 0.926469  [   96/  118]
train() client id: f_00009-11-0 loss: 0.849029  [   32/  118]
train() client id: f_00009-11-1 loss: 0.880597  [   64/  118]
train() client id: f_00009-11-2 loss: 0.944746  [   96/  118]
train() client id: f_00009-12-0 loss: 0.882729  [   32/  118]
train() client id: f_00009-12-1 loss: 0.741028  [   64/  118]
train() client id: f_00009-12-2 loss: 0.783829  [   96/  118]
At round 41 accuracy: 0.6525198938992043
At round 41 training accuracy: 0.5875251509054326
At round 41 training loss: 0.8314395028325754
update_location
xs = -3.905658 4.200318 225.009024 18.811294 0.979296 3.956410 -187.443192 -166.324852 209.663977 -152.060879 
ys = 217.587959 200.555839 1.320614 -187.455176 179.350187 162.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 239.499007 224.143452 246.233233 213.291603 205.347142 191.112796 212.466093 194.073781 232.954187 182.039893 
dists_bs = 176.315082 181.013411 436.086859 411.018944 176.033056 179.370821 178.060293 174.393557 415.633123 172.530277 
uav_gains = -110.431879 -109.279725 -110.979226 -108.537441 -108.021163 -107.125430 -108.482905 -107.310399 -109.924643 -106.556698 
bs_gains = -102.463164 -102.782960 -113.475103 -112.755191 -102.443698 -102.672109 -102.582938 -102.329911 -112.890943 -102.199288 
Round 42
-------------------------------
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.44885328 11.23645857  5.37086954  1.94257213 12.9580199   6.23472006
  2.4044106   7.64858783  5.64816701  5.05583458]
obj_prev = 63.948493486950056
eta_min = 1.1938832184881957e-17	eta_max = 0.9337601214025776
af = 13.47090363962996	bf = 1.273204732538192	zeta = 14.817994003592956	eta = 0.9090909090909091
af = 13.47090363962996	bf = 1.273204732538192	zeta = 28.128308786138636	eta = 0.47890912112953954
af = 13.47090363962996	bf = 1.273204732538192	zeta = 21.476258560043068	eta = 0.6272462962749386
af = 13.47090363962996	bf = 1.273204732538192	zeta = 20.271427562519737	eta = 0.6645266396796146
af = 13.47090363962996	bf = 1.273204732538192	zeta = 20.2057708129807	eta = 0.666685956418743
af = 13.47090363962996	bf = 1.273204732538192	zeta = 20.20555837007598	eta = 0.6666929660097932
eta = 0.6666929660097932
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [0.03345475 0.07036119 0.03292373 0.0114171  0.08124726 0.03876503
 0.01433775 0.04752699 0.03451681 0.03133064]
ene_total = [1.84044551 3.20713505 1.83649691 0.87474524 3.65310778 1.89784042
 0.99426624 2.34061661 1.97709129 1.58381332]
ti_comp = [0.56395803 0.60282675 0.55960175 0.5768971  0.60393932 0.60319406
 0.57722705 0.58383781 0.54156463 0.6047199 ]
ti_coms = [0.11010873 0.07124001 0.11446502 0.09716967 0.07012744 0.0708727
 0.09683971 0.09022895 0.13250214 0.06934686]
t_total = [27.89982376 27.89982376 27.89982376 27.89982376 27.89982376 27.89982376
 27.89982376 27.89982376 27.89982376 27.89982376]
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [7.35800290e-06 5.99093333e-05 7.12276845e-06 2.79479884e-07
 9.19008568e-05 1.00065955e-05 5.52878649e-07 1.96841446e-05
 8.76337451e-06 5.25629088e-06]
ene_total = [0.45604785 0.2973442  0.47406892 0.4021997  0.29406337 0.29375838
 0.40084532 0.37427503 0.548793   0.28724627]
optimize_network iter = 0 obj = 3.828642052039432
eta = 0.6666929660097932
freqs = [29660676.66717937 58359378.23819214 29417104.30254062  9895264.30741952
 67264422.3037599  32133134.64789411 12419507.6458075  40702220.97084436
 31867673.54934066 25905086.18443851]
eta_min = 0.6666929660097993	eta_max = 0.6666929660097998
af = 0.007174173529547853	bf = 1.273204732538192	zeta = 0.007891590882502639	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [1.72976861e-06 1.40838874e-05 1.67446812e-06 6.57020031e-08
 2.16046690e-05 2.35241749e-06 1.29974416e-07 4.62748057e-06
 2.06015278e-06 1.23568407e-06]
ene_total = [1.67118944 1.08322193 1.73728876 1.47458345 1.06747966 1.07586755
 1.46958603 1.36994863 2.01106501 1.0525431 ]
ti_comp = [0.56395803 0.60282675 0.55960175 0.5768971  0.60393932 0.60319406
 0.57722705 0.58383781 0.54156463 0.6047199 ]
ti_coms = [0.11010873 0.07124001 0.11446502 0.09716967 0.07012744 0.0708727
 0.09683971 0.09022895 0.13250214 0.06934686]
t_total = [27.89982376 27.89982376 27.89982376 27.89982376 27.89982376 27.89982376
 27.89982376 27.89982376 27.89982376 27.89982376]
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [7.35800290e-06 5.99093333e-05 7.12276845e-06 2.79479884e-07
 9.19008568e-05 1.00065955e-05 5.52878649e-07 1.96841446e-05
 8.76337451e-06 5.25629088e-06]
ene_total = [0.45604785 0.2973442  0.47406892 0.4021997  0.29406337 0.29375838
 0.40084532 0.37427503 0.548793   0.28724627]
optimize_network iter = 1 obj = 3.8286420520395077
eta = 0.6666929660097998
freqs = [29660676.66717935 58359378.23819202 29417104.30254061  9895264.30741951
 67264422.30375975 32133134.64789404 12419507.64580748 40702220.97084429
 31867673.54934067 25905086.18443845]
Done!
ene_coms = [0.01101087 0.007124   0.0114465  0.00971697 0.00701274 0.00708727
 0.00968397 0.00902289 0.01325021 0.00693469]
ene_comp = [7.20519951e-06 5.86651983e-05 6.97485017e-06 2.73675935e-07
 8.99923549e-05 9.79878887e-06 5.41397037e-07 1.92753647e-05
 8.58138582e-06 5.14713367e-06]
ene_total = [0.01101808 0.00718267 0.01145348 0.00971724 0.00710274 0.00709707
 0.00968451 0.00904217 0.01325879 0.00693983]
At round 42 energy consumption: 0.09249657683909618
At round 42 eta: 0.6666929660097998
At round 42 a_n: 13.79567728354305
At round 42 local rounds: 13.275695913565364
At round 42 global rounds: 41.390297463535276
gradient difference: 0.37650802731513977
train() client id: f_00000-0-0 loss: 1.328867  [   32/  126]
train() client id: f_00000-0-1 loss: 0.922313  [   64/  126]
train() client id: f_00000-0-2 loss: 1.066362  [   96/  126]
train() client id: f_00000-1-0 loss: 0.985689  [   32/  126]
train() client id: f_00000-1-1 loss: 1.158858  [   64/  126]
train() client id: f_00000-1-2 loss: 0.981122  [   96/  126]
train() client id: f_00000-2-0 loss: 0.917260  [   32/  126]
train() client id: f_00000-2-1 loss: 0.982842  [   64/  126]
train() client id: f_00000-2-2 loss: 0.989505  [   96/  126]
train() client id: f_00000-3-0 loss: 0.859629  [   32/  126]
train() client id: f_00000-3-1 loss: 0.989456  [   64/  126]
train() client id: f_00000-3-2 loss: 0.869387  [   96/  126]
train() client id: f_00000-4-0 loss: 0.836528  [   32/  126]
train() client id: f_00000-4-1 loss: 0.766966  [   64/  126]
train() client id: f_00000-4-2 loss: 0.957976  [   96/  126]
train() client id: f_00000-5-0 loss: 0.790040  [   32/  126]
train() client id: f_00000-5-1 loss: 0.792350  [   64/  126]
train() client id: f_00000-5-2 loss: 0.853204  [   96/  126]
train() client id: f_00000-6-0 loss: 0.775356  [   32/  126]
train() client id: f_00000-6-1 loss: 0.853668  [   64/  126]
train() client id: f_00000-6-2 loss: 0.733059  [   96/  126]
train() client id: f_00000-7-0 loss: 0.866543  [   32/  126]
train() client id: f_00000-7-1 loss: 0.788765  [   64/  126]
train() client id: f_00000-7-2 loss: 0.739608  [   96/  126]
train() client id: f_00000-8-0 loss: 0.773850  [   32/  126]
train() client id: f_00000-8-1 loss: 0.895632  [   64/  126]
train() client id: f_00000-8-2 loss: 0.706814  [   96/  126]
train() client id: f_00000-9-0 loss: 0.698782  [   32/  126]
train() client id: f_00000-9-1 loss: 0.684144  [   64/  126]
train() client id: f_00000-9-2 loss: 0.827646  [   96/  126]
train() client id: f_00000-10-0 loss: 0.631606  [   32/  126]
train() client id: f_00000-10-1 loss: 0.895000  [   64/  126]
train() client id: f_00000-10-2 loss: 0.772119  [   96/  126]
train() client id: f_00000-11-0 loss: 0.777773  [   32/  126]
train() client id: f_00000-11-1 loss: 0.697926  [   64/  126]
train() client id: f_00000-11-2 loss: 0.829559  [   96/  126]
train() client id: f_00000-12-0 loss: 0.752069  [   32/  126]
train() client id: f_00000-12-1 loss: 0.673113  [   64/  126]
train() client id: f_00000-12-2 loss: 0.858903  [   96/  126]
train() client id: f_00001-0-0 loss: 0.438632  [   32/  265]
train() client id: f_00001-0-1 loss: 0.420148  [   64/  265]
train() client id: f_00001-0-2 loss: 0.459549  [   96/  265]
train() client id: f_00001-0-3 loss: 0.466395  [  128/  265]
train() client id: f_00001-0-4 loss: 0.492871  [  160/  265]
train() client id: f_00001-0-5 loss: 0.383686  [  192/  265]
train() client id: f_00001-0-6 loss: 0.495119  [  224/  265]
train() client id: f_00001-0-7 loss: 0.346960  [  256/  265]
train() client id: f_00001-1-0 loss: 0.508066  [   32/  265]
train() client id: f_00001-1-1 loss: 0.380697  [   64/  265]
train() client id: f_00001-1-2 loss: 0.441443  [   96/  265]
train() client id: f_00001-1-3 loss: 0.349199  [  128/  265]
train() client id: f_00001-1-4 loss: 0.358179  [  160/  265]
train() client id: f_00001-1-5 loss: 0.537702  [  192/  265]
train() client id: f_00001-1-6 loss: 0.412062  [  224/  265]
train() client id: f_00001-1-7 loss: 0.358161  [  256/  265]
train() client id: f_00001-2-0 loss: 0.399069  [   32/  265]
train() client id: f_00001-2-1 loss: 0.418967  [   64/  265]
train() client id: f_00001-2-2 loss: 0.463542  [   96/  265]
train() client id: f_00001-2-3 loss: 0.434803  [  128/  265]
train() client id: f_00001-2-4 loss: 0.399058  [  160/  265]
train() client id: f_00001-2-5 loss: 0.394483  [  192/  265]
train() client id: f_00001-2-6 loss: 0.443596  [  224/  265]
train() client id: f_00001-2-7 loss: 0.423536  [  256/  265]
train() client id: f_00001-3-0 loss: 0.391480  [   32/  265]
train() client id: f_00001-3-1 loss: 0.406500  [   64/  265]
train() client id: f_00001-3-2 loss: 0.370118  [   96/  265]
train() client id: f_00001-3-3 loss: 0.374327  [  128/  265]
train() client id: f_00001-3-4 loss: 0.414750  [  160/  265]
train() client id: f_00001-3-5 loss: 0.475853  [  192/  265]
train() client id: f_00001-3-6 loss: 0.466830  [  224/  265]
train() client id: f_00001-3-7 loss: 0.399546  [  256/  265]
train() client id: f_00001-4-0 loss: 0.415139  [   32/  265]
train() client id: f_00001-4-1 loss: 0.387481  [   64/  265]
train() client id: f_00001-4-2 loss: 0.522637  [   96/  265]
train() client id: f_00001-4-3 loss: 0.313481  [  128/  265]
train() client id: f_00001-4-4 loss: 0.415692  [  160/  265]
train() client id: f_00001-4-5 loss: 0.452483  [  192/  265]
train() client id: f_00001-4-6 loss: 0.390388  [  224/  265]
train() client id: f_00001-4-7 loss: 0.342681  [  256/  265]
train() client id: f_00001-5-0 loss: 0.433603  [   32/  265]
train() client id: f_00001-5-1 loss: 0.395670  [   64/  265]
train() client id: f_00001-5-2 loss: 0.383268  [   96/  265]
train() client id: f_00001-5-3 loss: 0.400225  [  128/  265]
train() client id: f_00001-5-4 loss: 0.549372  [  160/  265]
train() client id: f_00001-5-5 loss: 0.359022  [  192/  265]
train() client id: f_00001-5-6 loss: 0.315961  [  224/  265]
train() client id: f_00001-5-7 loss: 0.462622  [  256/  265]
train() client id: f_00001-6-0 loss: 0.400162  [   32/  265]
train() client id: f_00001-6-1 loss: 0.444907  [   64/  265]
train() client id: f_00001-6-2 loss: 0.405333  [   96/  265]
train() client id: f_00001-6-3 loss: 0.475646  [  128/  265]
train() client id: f_00001-6-4 loss: 0.396645  [  160/  265]
train() client id: f_00001-6-5 loss: 0.454361  [  192/  265]
train() client id: f_00001-6-6 loss: 0.306706  [  224/  265]
train() client id: f_00001-6-7 loss: 0.388215  [  256/  265]
train() client id: f_00001-7-0 loss: 0.307247  [   32/  265]
train() client id: f_00001-7-1 loss: 0.329087  [   64/  265]
train() client id: f_00001-7-2 loss: 0.408297  [   96/  265]
train() client id: f_00001-7-3 loss: 0.461961  [  128/  265]
train() client id: f_00001-7-4 loss: 0.367227  [  160/  265]
train() client id: f_00001-7-5 loss: 0.384787  [  192/  265]
train() client id: f_00001-7-6 loss: 0.443937  [  224/  265]
train() client id: f_00001-7-7 loss: 0.524096  [  256/  265]
train() client id: f_00001-8-0 loss: 0.473476  [   32/  265]
train() client id: f_00001-8-1 loss: 0.400626  [   64/  265]
train() client id: f_00001-8-2 loss: 0.303362  [   96/  265]
train() client id: f_00001-8-3 loss: 0.543050  [  128/  265]
train() client id: f_00001-8-4 loss: 0.407716  [  160/  265]
train() client id: f_00001-8-5 loss: 0.391447  [  192/  265]
train() client id: f_00001-8-6 loss: 0.303411  [  224/  265]
train() client id: f_00001-8-7 loss: 0.432277  [  256/  265]
train() client id: f_00001-9-0 loss: 0.368477  [   32/  265]
train() client id: f_00001-9-1 loss: 0.592963  [   64/  265]
train() client id: f_00001-9-2 loss: 0.302744  [   96/  265]
train() client id: f_00001-9-3 loss: 0.352241  [  128/  265]
train() client id: f_00001-9-4 loss: 0.323472  [  160/  265]
train() client id: f_00001-9-5 loss: 0.383510  [  192/  265]
train() client id: f_00001-9-6 loss: 0.499663  [  224/  265]
train() client id: f_00001-9-7 loss: 0.374241  [  256/  265]
train() client id: f_00001-10-0 loss: 0.527225  [   32/  265]
train() client id: f_00001-10-1 loss: 0.306739  [   64/  265]
train() client id: f_00001-10-2 loss: 0.501402  [   96/  265]
train() client id: f_00001-10-3 loss: 0.318034  [  128/  265]
train() client id: f_00001-10-4 loss: 0.356306  [  160/  265]
train() client id: f_00001-10-5 loss: 0.466803  [  192/  265]
train() client id: f_00001-10-6 loss: 0.431913  [  224/  265]
train() client id: f_00001-10-7 loss: 0.336048  [  256/  265]
train() client id: f_00001-11-0 loss: 0.282346  [   32/  265]
train() client id: f_00001-11-1 loss: 0.307022  [   64/  265]
train() client id: f_00001-11-2 loss: 0.588789  [   96/  265]
train() client id: f_00001-11-3 loss: 0.316517  [  128/  265]
train() client id: f_00001-11-4 loss: 0.404236  [  160/  265]
train() client id: f_00001-11-5 loss: 0.304984  [  192/  265]
train() client id: f_00001-11-6 loss: 0.588330  [  224/  265]
train() client id: f_00001-11-7 loss: 0.455887  [  256/  265]
train() client id: f_00001-12-0 loss: 0.544561  [   32/  265]
train() client id: f_00001-12-1 loss: 0.356050  [   64/  265]
train() client id: f_00001-12-2 loss: 0.303494  [   96/  265]
train() client id: f_00001-12-3 loss: 0.396578  [  128/  265]
train() client id: f_00001-12-4 loss: 0.384223  [  160/  265]
train() client id: f_00001-12-5 loss: 0.359248  [  192/  265]
train() client id: f_00001-12-6 loss: 0.414545  [  224/  265]
train() client id: f_00001-12-7 loss: 0.486037  [  256/  265]
train() client id: f_00002-0-0 loss: 1.135519  [   32/  124]
train() client id: f_00002-0-1 loss: 1.372250  [   64/  124]
train() client id: f_00002-0-2 loss: 1.037551  [   96/  124]
train() client id: f_00002-1-0 loss: 1.344993  [   32/  124]
train() client id: f_00002-1-1 loss: 0.936144  [   64/  124]
train() client id: f_00002-1-2 loss: 1.034213  [   96/  124]
train() client id: f_00002-2-0 loss: 1.081781  [   32/  124]
train() client id: f_00002-2-1 loss: 1.158488  [   64/  124]
train() client id: f_00002-2-2 loss: 0.927776  [   96/  124]
train() client id: f_00002-3-0 loss: 1.022157  [   32/  124]
train() client id: f_00002-3-1 loss: 1.053894  [   64/  124]
train() client id: f_00002-3-2 loss: 0.973045  [   96/  124]
train() client id: f_00002-4-0 loss: 0.949349  [   32/  124]
train() client id: f_00002-4-1 loss: 0.931189  [   64/  124]
train() client id: f_00002-4-2 loss: 0.962809  [   96/  124]
train() client id: f_00002-5-0 loss: 0.987124  [   32/  124]
train() client id: f_00002-5-1 loss: 0.948882  [   64/  124]
train() client id: f_00002-5-2 loss: 0.981349  [   96/  124]
train() client id: f_00002-6-0 loss: 0.830448  [   32/  124]
train() client id: f_00002-6-1 loss: 0.930189  [   64/  124]
train() client id: f_00002-6-2 loss: 1.060182  [   96/  124]
train() client id: f_00002-7-0 loss: 1.003518  [   32/  124]
train() client id: f_00002-7-1 loss: 0.876229  [   64/  124]
train() client id: f_00002-7-2 loss: 0.905433  [   96/  124]
train() client id: f_00002-8-0 loss: 0.882344  [   32/  124]
train() client id: f_00002-8-1 loss: 0.932441  [   64/  124]
train() client id: f_00002-8-2 loss: 0.818576  [   96/  124]
train() client id: f_00002-9-0 loss: 0.979189  [   32/  124]
train() client id: f_00002-9-1 loss: 1.006631  [   64/  124]
train() client id: f_00002-9-2 loss: 0.715902  [   96/  124]
train() client id: f_00002-10-0 loss: 0.932989  [   32/  124]
train() client id: f_00002-10-1 loss: 0.715957  [   64/  124]
train() client id: f_00002-10-2 loss: 0.989590  [   96/  124]
train() client id: f_00002-11-0 loss: 1.095202  [   32/  124]
train() client id: f_00002-11-1 loss: 0.767860  [   64/  124]
train() client id: f_00002-11-2 loss: 0.681720  [   96/  124]
train() client id: f_00002-12-0 loss: 0.956655  [   32/  124]
train() client id: f_00002-12-1 loss: 0.751440  [   64/  124]
train() client id: f_00002-12-2 loss: 0.966350  [   96/  124]
train() client id: f_00003-0-0 loss: 0.592795  [   32/   43]
train() client id: f_00003-1-0 loss: 0.629103  [   32/   43]
train() client id: f_00003-2-0 loss: 0.725542  [   32/   43]
train() client id: f_00003-3-0 loss: 0.550627  [   32/   43]
train() client id: f_00003-4-0 loss: 0.835480  [   32/   43]
train() client id: f_00003-5-0 loss: 0.663009  [   32/   43]
train() client id: f_00003-6-0 loss: 0.801590  [   32/   43]
train() client id: f_00003-7-0 loss: 0.403672  [   32/   43]
train() client id: f_00003-8-0 loss: 0.500291  [   32/   43]
train() client id: f_00003-9-0 loss: 0.694732  [   32/   43]
train() client id: f_00003-10-0 loss: 0.801798  [   32/   43]
train() client id: f_00003-11-0 loss: 0.902043  [   32/   43]
train() client id: f_00003-12-0 loss: 0.775122  [   32/   43]
train() client id: f_00004-0-0 loss: 0.929619  [   32/  306]
train() client id: f_00004-0-1 loss: 0.885437  [   64/  306]
train() client id: f_00004-0-2 loss: 0.989090  [   96/  306]
train() client id: f_00004-0-3 loss: 0.913810  [  128/  306]
train() client id: f_00004-0-4 loss: 0.727353  [  160/  306]
train() client id: f_00004-0-5 loss: 0.950151  [  192/  306]
train() client id: f_00004-0-6 loss: 0.801786  [  224/  306]
train() client id: f_00004-0-7 loss: 0.972027  [  256/  306]
train() client id: f_00004-0-8 loss: 0.923858  [  288/  306]
train() client id: f_00004-1-0 loss: 0.906519  [   32/  306]
train() client id: f_00004-1-1 loss: 0.925298  [   64/  306]
train() client id: f_00004-1-2 loss: 0.847870  [   96/  306]
train() client id: f_00004-1-3 loss: 0.961352  [  128/  306]
train() client id: f_00004-1-4 loss: 0.931801  [  160/  306]
train() client id: f_00004-1-5 loss: 0.849530  [  192/  306]
train() client id: f_00004-1-6 loss: 0.863869  [  224/  306]
train() client id: f_00004-1-7 loss: 0.835343  [  256/  306]
train() client id: f_00004-1-8 loss: 0.999123  [  288/  306]
train() client id: f_00004-2-0 loss: 0.951548  [   32/  306]
train() client id: f_00004-2-1 loss: 0.928146  [   64/  306]
train() client id: f_00004-2-2 loss: 0.883613  [   96/  306]
train() client id: f_00004-2-3 loss: 1.007797  [  128/  306]
train() client id: f_00004-2-4 loss: 0.987634  [  160/  306]
train() client id: f_00004-2-5 loss: 0.725145  [  192/  306]
train() client id: f_00004-2-6 loss: 0.812653  [  224/  306]
train() client id: f_00004-2-7 loss: 0.849932  [  256/  306]
train() client id: f_00004-2-8 loss: 0.874234  [  288/  306]
train() client id: f_00004-3-0 loss: 0.801155  [   32/  306]
train() client id: f_00004-3-1 loss: 0.942400  [   64/  306]
train() client id: f_00004-3-2 loss: 0.837711  [   96/  306]
train() client id: f_00004-3-3 loss: 0.747795  [  128/  306]
train() client id: f_00004-3-4 loss: 0.782841  [  160/  306]
train() client id: f_00004-3-5 loss: 0.852147  [  192/  306]
train() client id: f_00004-3-6 loss: 1.026083  [  224/  306]
train() client id: f_00004-3-7 loss: 0.829850  [  256/  306]
train() client id: f_00004-3-8 loss: 1.009115  [  288/  306]
train() client id: f_00004-4-0 loss: 0.853455  [   32/  306]
train() client id: f_00004-4-1 loss: 0.862820  [   64/  306]
train() client id: f_00004-4-2 loss: 0.850153  [   96/  306]
train() client id: f_00004-4-3 loss: 0.875762  [  128/  306]
train() client id: f_00004-4-4 loss: 0.912971  [  160/  306]
train() client id: f_00004-4-5 loss: 0.997242  [  192/  306]
train() client id: f_00004-4-6 loss: 0.905026  [  224/  306]
train() client id: f_00004-4-7 loss: 0.867290  [  256/  306]
train() client id: f_00004-4-8 loss: 0.856126  [  288/  306]
train() client id: f_00004-5-0 loss: 0.910563  [   32/  306]
train() client id: f_00004-5-1 loss: 0.970309  [   64/  306]
train() client id: f_00004-5-2 loss: 0.794857  [   96/  306]
train() client id: f_00004-5-3 loss: 0.943329  [  128/  306]
train() client id: f_00004-5-4 loss: 0.794352  [  160/  306]
train() client id: f_00004-5-5 loss: 0.788813  [  192/  306]
train() client id: f_00004-5-6 loss: 0.978038  [  224/  306]
train() client id: f_00004-5-7 loss: 0.893716  [  256/  306]
train() client id: f_00004-5-8 loss: 0.913330  [  288/  306]
train() client id: f_00004-6-0 loss: 0.860548  [   32/  306]
train() client id: f_00004-6-1 loss: 0.794997  [   64/  306]
train() client id: f_00004-6-2 loss: 0.926383  [   96/  306]
train() client id: f_00004-6-3 loss: 0.900019  [  128/  306]
train() client id: f_00004-6-4 loss: 0.917757  [  160/  306]
train() client id: f_00004-6-5 loss: 0.897364  [  192/  306]
train() client id: f_00004-6-6 loss: 0.825502  [  224/  306]
train() client id: f_00004-6-7 loss: 0.847004  [  256/  306]
train() client id: f_00004-6-8 loss: 0.820672  [  288/  306]
train() client id: f_00004-7-0 loss: 0.825690  [   32/  306]
train() client id: f_00004-7-1 loss: 0.923669  [   64/  306]
train() client id: f_00004-7-2 loss: 0.776967  [   96/  306]
train() client id: f_00004-7-3 loss: 0.902349  [  128/  306]
train() client id: f_00004-7-4 loss: 0.932415  [  160/  306]
train() client id: f_00004-7-5 loss: 0.827524  [  192/  306]
train() client id: f_00004-7-6 loss: 0.978253  [  224/  306]
train() client id: f_00004-7-7 loss: 0.819782  [  256/  306]
train() client id: f_00004-7-8 loss: 0.877648  [  288/  306]
train() client id: f_00004-8-0 loss: 0.924626  [   32/  306]
train() client id: f_00004-8-1 loss: 0.873665  [   64/  306]
train() client id: f_00004-8-2 loss: 0.949621  [   96/  306]
train() client id: f_00004-8-3 loss: 0.861616  [  128/  306]
train() client id: f_00004-8-4 loss: 0.909371  [  160/  306]
train() client id: f_00004-8-5 loss: 0.946775  [  192/  306]
train() client id: f_00004-8-6 loss: 0.791683  [  224/  306]
train() client id: f_00004-8-7 loss: 0.849940  [  256/  306]
train() client id: f_00004-8-8 loss: 0.794784  [  288/  306]
train() client id: f_00004-9-0 loss: 0.837739  [   32/  306]
train() client id: f_00004-9-1 loss: 0.967775  [   64/  306]
train() client id: f_00004-9-2 loss: 0.808962  [   96/  306]
train() client id: f_00004-9-3 loss: 0.867084  [  128/  306]
train() client id: f_00004-9-4 loss: 0.751369  [  160/  306]
train() client id: f_00004-9-5 loss: 0.883810  [  192/  306]
train() client id: f_00004-9-6 loss: 0.940139  [  224/  306]
train() client id: f_00004-9-7 loss: 0.938400  [  256/  306]
train() client id: f_00004-9-8 loss: 0.843462  [  288/  306]
train() client id: f_00004-10-0 loss: 0.859172  [   32/  306]
train() client id: f_00004-10-1 loss: 0.767712  [   64/  306]
train() client id: f_00004-10-2 loss: 0.724557  [   96/  306]
train() client id: f_00004-10-3 loss: 1.000941  [  128/  306]
train() client id: f_00004-10-4 loss: 0.881675  [  160/  306]
train() client id: f_00004-10-5 loss: 0.932502  [  192/  306]
train() client id: f_00004-10-6 loss: 0.762407  [  224/  306]
train() client id: f_00004-10-7 loss: 0.953554  [  256/  306]
train() client id: f_00004-10-8 loss: 0.913731  [  288/  306]
train() client id: f_00004-11-0 loss: 0.739807  [   32/  306]
train() client id: f_00004-11-1 loss: 0.949577  [   64/  306]
train() client id: f_00004-11-2 loss: 0.841231  [   96/  306]
train() client id: f_00004-11-3 loss: 0.826288  [  128/  306]
train() client id: f_00004-11-4 loss: 0.901253  [  160/  306]
train() client id: f_00004-11-5 loss: 0.824501  [  192/  306]
train() client id: f_00004-11-6 loss: 0.872199  [  224/  306]
train() client id: f_00004-11-7 loss: 0.923594  [  256/  306]
train() client id: f_00004-11-8 loss: 0.948446  [  288/  306]
train() client id: f_00004-12-0 loss: 0.782623  [   32/  306]
train() client id: f_00004-12-1 loss: 0.890226  [   64/  306]
train() client id: f_00004-12-2 loss: 0.926081  [   96/  306]
train() client id: f_00004-12-3 loss: 0.966851  [  128/  306]
train() client id: f_00004-12-4 loss: 0.924720  [  160/  306]
train() client id: f_00004-12-5 loss: 0.915416  [  192/  306]
train() client id: f_00004-12-6 loss: 0.769531  [  224/  306]
train() client id: f_00004-12-7 loss: 0.874470  [  256/  306]
train() client id: f_00004-12-8 loss: 0.833200  [  288/  306]
train() client id: f_00005-0-0 loss: 0.444098  [   32/  146]
train() client id: f_00005-0-1 loss: 0.529927  [   64/  146]
train() client id: f_00005-0-2 loss: 0.312934  [   96/  146]
train() client id: f_00005-0-3 loss: 0.513363  [  128/  146]
train() client id: f_00005-1-0 loss: 0.325805  [   32/  146]
train() client id: f_00005-1-1 loss: 0.480043  [   64/  146]
train() client id: f_00005-1-2 loss: 0.416104  [   96/  146]
train() client id: f_00005-1-3 loss: 0.567623  [  128/  146]
train() client id: f_00005-2-0 loss: 0.493993  [   32/  146]
train() client id: f_00005-2-1 loss: 0.429752  [   64/  146]
train() client id: f_00005-2-2 loss: 0.457408  [   96/  146]
train() client id: f_00005-2-3 loss: 0.479505  [  128/  146]
train() client id: f_00005-3-0 loss: 0.354642  [   32/  146]
train() client id: f_00005-3-1 loss: 0.387910  [   64/  146]
train() client id: f_00005-3-2 loss: 0.530939  [   96/  146]
train() client id: f_00005-3-3 loss: 0.402848  [  128/  146]
train() client id: f_00005-4-0 loss: 0.554504  [   32/  146]
train() client id: f_00005-4-1 loss: 0.412208  [   64/  146]
train() client id: f_00005-4-2 loss: 0.285080  [   96/  146]
train() client id: f_00005-4-3 loss: 0.567016  [  128/  146]
train() client id: f_00005-5-0 loss: 0.458296  [   32/  146]
train() client id: f_00005-5-1 loss: 0.618945  [   64/  146]
train() client id: f_00005-5-2 loss: 0.305627  [   96/  146]
train() client id: f_00005-5-3 loss: 0.269365  [  128/  146]
train() client id: f_00005-6-0 loss: 0.382915  [   32/  146]
train() client id: f_00005-6-1 loss: 0.734904  [   64/  146]
train() client id: f_00005-6-2 loss: 0.408812  [   96/  146]
train() client id: f_00005-6-3 loss: 0.423552  [  128/  146]
train() client id: f_00005-7-0 loss: 0.525509  [   32/  146]
train() client id: f_00005-7-1 loss: 0.405466  [   64/  146]
train() client id: f_00005-7-2 loss: 0.551156  [   96/  146]
train() client id: f_00005-7-3 loss: 0.469671  [  128/  146]
train() client id: f_00005-8-0 loss: 0.506208  [   32/  146]
train() client id: f_00005-8-1 loss: 0.430889  [   64/  146]
train() client id: f_00005-8-2 loss: 0.479277  [   96/  146]
train() client id: f_00005-8-3 loss: 0.334565  [  128/  146]
train() client id: f_00005-9-0 loss: 0.760962  [   32/  146]
train() client id: f_00005-9-1 loss: 0.185953  [   64/  146]
train() client id: f_00005-9-2 loss: 0.777867  [   96/  146]
train() client id: f_00005-9-3 loss: 0.205861  [  128/  146]
train() client id: f_00005-10-0 loss: 0.232053  [   32/  146]
train() client id: f_00005-10-1 loss: 0.561093  [   64/  146]
train() client id: f_00005-10-2 loss: 0.317366  [   96/  146]
train() client id: f_00005-10-3 loss: 0.669998  [  128/  146]
train() client id: f_00005-11-0 loss: 0.676181  [   32/  146]
train() client id: f_00005-11-1 loss: 0.342274  [   64/  146]
train() client id: f_00005-11-2 loss: 0.326829  [   96/  146]
train() client id: f_00005-11-3 loss: 0.466995  [  128/  146]
train() client id: f_00005-12-0 loss: 0.302341  [   32/  146]
train() client id: f_00005-12-1 loss: 0.622482  [   64/  146]
train() client id: f_00005-12-2 loss: 0.489320  [   96/  146]
train() client id: f_00005-12-3 loss: 0.313408  [  128/  146]
train() client id: f_00006-0-0 loss: 0.572558  [   32/   54]
train() client id: f_00006-1-0 loss: 0.519248  [   32/   54]
train() client id: f_00006-2-0 loss: 0.532060  [   32/   54]
train() client id: f_00006-3-0 loss: 0.573545  [   32/   54]
train() client id: f_00006-4-0 loss: 0.568973  [   32/   54]
train() client id: f_00006-5-0 loss: 0.584344  [   32/   54]
train() client id: f_00006-6-0 loss: 0.506206  [   32/   54]
train() client id: f_00006-7-0 loss: 0.565580  [   32/   54]
train() client id: f_00006-8-0 loss: 0.595605  [   32/   54]
train() client id: f_00006-9-0 loss: 0.468988  [   32/   54]
train() client id: f_00006-10-0 loss: 0.466964  [   32/   54]
train() client id: f_00006-11-0 loss: 0.495128  [   32/   54]
train() client id: f_00006-12-0 loss: 0.528715  [   32/   54]
train() client id: f_00007-0-0 loss: 0.615970  [   32/  179]
train() client id: f_00007-0-1 loss: 0.518995  [   64/  179]
train() client id: f_00007-0-2 loss: 0.670653  [   96/  179]
train() client id: f_00007-0-3 loss: 0.802937  [  128/  179]
train() client id: f_00007-0-4 loss: 0.652031  [  160/  179]
train() client id: f_00007-1-0 loss: 0.578855  [   32/  179]
train() client id: f_00007-1-1 loss: 0.814824  [   64/  179]
train() client id: f_00007-1-2 loss: 0.497891  [   96/  179]
train() client id: f_00007-1-3 loss: 0.557015  [  128/  179]
train() client id: f_00007-1-4 loss: 0.781824  [  160/  179]
train() client id: f_00007-2-0 loss: 0.695663  [   32/  179]
train() client id: f_00007-2-1 loss: 0.614763  [   64/  179]
train() client id: f_00007-2-2 loss: 0.607694  [   96/  179]
train() client id: f_00007-2-3 loss: 0.516483  [  128/  179]
train() client id: f_00007-2-4 loss: 0.664399  [  160/  179]
train() client id: f_00007-3-0 loss: 0.528238  [   32/  179]
train() client id: f_00007-3-1 loss: 0.563021  [   64/  179]
train() client id: f_00007-3-2 loss: 0.644483  [   96/  179]
train() client id: f_00007-3-3 loss: 0.498189  [  128/  179]
train() client id: f_00007-3-4 loss: 0.825642  [  160/  179]
train() client id: f_00007-4-0 loss: 0.753696  [   32/  179]
train() client id: f_00007-4-1 loss: 0.623497  [   64/  179]
train() client id: f_00007-4-2 loss: 0.448259  [   96/  179]
train() client id: f_00007-4-3 loss: 0.444662  [  128/  179]
train() client id: f_00007-4-4 loss: 0.837763  [  160/  179]
train() client id: f_00007-5-0 loss: 0.640416  [   32/  179]
train() client id: f_00007-5-1 loss: 0.636731  [   64/  179]
train() client id: f_00007-5-2 loss: 0.668804  [   96/  179]
train() client id: f_00007-5-3 loss: 0.559729  [  128/  179]
train() client id: f_00007-5-4 loss: 0.556004  [  160/  179]
train() client id: f_00007-6-0 loss: 0.671955  [   32/  179]
train() client id: f_00007-6-1 loss: 0.726534  [   64/  179]
train() client id: f_00007-6-2 loss: 0.592493  [   96/  179]
train() client id: f_00007-6-3 loss: 0.521422  [  128/  179]
train() client id: f_00007-6-4 loss: 0.562228  [  160/  179]
train() client id: f_00007-7-0 loss: 0.637668  [   32/  179]
train() client id: f_00007-7-1 loss: 0.499695  [   64/  179]
train() client id: f_00007-7-2 loss: 0.573469  [   96/  179]
train() client id: f_00007-7-3 loss: 0.536799  [  128/  179]
train() client id: f_00007-7-4 loss: 0.700102  [  160/  179]
train() client id: f_00007-8-0 loss: 0.653167  [   32/  179]
train() client id: f_00007-8-1 loss: 0.434741  [   64/  179]
train() client id: f_00007-8-2 loss: 0.461250  [   96/  179]
train() client id: f_00007-8-3 loss: 0.798600  [  128/  179]
train() client id: f_00007-8-4 loss: 0.640354  [  160/  179]
train() client id: f_00007-9-0 loss: 0.690552  [   32/  179]
train() client id: f_00007-9-1 loss: 0.637561  [   64/  179]
train() client id: f_00007-9-2 loss: 0.623433  [   96/  179]
train() client id: f_00007-9-3 loss: 0.551468  [  128/  179]
train() client id: f_00007-9-4 loss: 0.522562  [  160/  179]
train() client id: f_00007-10-0 loss: 0.499707  [   32/  179]
train() client id: f_00007-10-1 loss: 0.626853  [   64/  179]
train() client id: f_00007-10-2 loss: 0.715607  [   96/  179]
train() client id: f_00007-10-3 loss: 0.650750  [  128/  179]
train() client id: f_00007-10-4 loss: 0.468427  [  160/  179]
train() client id: f_00007-11-0 loss: 0.660068  [   32/  179]
train() client id: f_00007-11-1 loss: 0.510551  [   64/  179]
train() client id: f_00007-11-2 loss: 0.594706  [   96/  179]
train() client id: f_00007-11-3 loss: 0.530542  [  128/  179]
train() client id: f_00007-11-4 loss: 0.633688  [  160/  179]
train() client id: f_00007-12-0 loss: 0.437400  [   32/  179]
train() client id: f_00007-12-1 loss: 0.618100  [   64/  179]
train() client id: f_00007-12-2 loss: 0.455210  [   96/  179]
train() client id: f_00007-12-3 loss: 0.415805  [  128/  179]
train() client id: f_00007-12-4 loss: 0.732239  [  160/  179]
train() client id: f_00008-0-0 loss: 0.862234  [   32/  130]
train() client id: f_00008-0-1 loss: 0.729211  [   64/  130]
train() client id: f_00008-0-2 loss: 0.631912  [   96/  130]
train() client id: f_00008-0-3 loss: 0.593886  [  128/  130]
train() client id: f_00008-1-0 loss: 0.624001  [   32/  130]
train() client id: f_00008-1-1 loss: 0.780738  [   64/  130]
train() client id: f_00008-1-2 loss: 0.713946  [   96/  130]
train() client id: f_00008-1-3 loss: 0.721977  [  128/  130]
train() client id: f_00008-2-0 loss: 0.716954  [   32/  130]
train() client id: f_00008-2-1 loss: 0.621069  [   64/  130]
train() client id: f_00008-2-2 loss: 0.681026  [   96/  130]
train() client id: f_00008-2-3 loss: 0.818116  [  128/  130]
train() client id: f_00008-3-0 loss: 0.640468  [   32/  130]
train() client id: f_00008-3-1 loss: 0.687119  [   64/  130]
train() client id: f_00008-3-2 loss: 0.829175  [   96/  130]
train() client id: f_00008-3-3 loss: 0.684076  [  128/  130]
train() client id: f_00008-4-0 loss: 0.759806  [   32/  130]
train() client id: f_00008-4-1 loss: 0.674298  [   64/  130]
train() client id: f_00008-4-2 loss: 0.661702  [   96/  130]
train() client id: f_00008-4-3 loss: 0.713785  [  128/  130]
train() client id: f_00008-5-0 loss: 0.697102  [   32/  130]
train() client id: f_00008-5-1 loss: 0.667452  [   64/  130]
train() client id: f_00008-5-2 loss: 0.686880  [   96/  130]
train() client id: f_00008-5-3 loss: 0.783857  [  128/  130]
train() client id: f_00008-6-0 loss: 0.712498  [   32/  130]
train() client id: f_00008-6-1 loss: 0.621320  [   64/  130]
train() client id: f_00008-6-2 loss: 0.748837  [   96/  130]
train() client id: f_00008-6-3 loss: 0.720483  [  128/  130]
train() client id: f_00008-7-0 loss: 0.567753  [   32/  130]
train() client id: f_00008-7-1 loss: 0.777050  [   64/  130]
train() client id: f_00008-7-2 loss: 0.739069  [   96/  130]
train() client id: f_00008-7-3 loss: 0.721519  [  128/  130]
train() client id: f_00008-8-0 loss: 0.775230  [   32/  130]
train() client id: f_00008-8-1 loss: 0.601406  [   64/  130]
train() client id: f_00008-8-2 loss: 0.657290  [   96/  130]
train() client id: f_00008-8-3 loss: 0.794087  [  128/  130]
train() client id: f_00008-9-0 loss: 0.647832  [   32/  130]
train() client id: f_00008-9-1 loss: 0.772071  [   64/  130]
train() client id: f_00008-9-2 loss: 0.588169  [   96/  130]
train() client id: f_00008-9-3 loss: 0.790116  [  128/  130]
train() client id: f_00008-10-0 loss: 0.751049  [   32/  130]
train() client id: f_00008-10-1 loss: 0.755070  [   64/  130]
train() client id: f_00008-10-2 loss: 0.719446  [   96/  130]
train() client id: f_00008-10-3 loss: 0.606075  [  128/  130]
train() client id: f_00008-11-0 loss: 0.704650  [   32/  130]
train() client id: f_00008-11-1 loss: 0.747478  [   64/  130]
train() client id: f_00008-11-2 loss: 0.570576  [   96/  130]
train() client id: f_00008-11-3 loss: 0.809133  [  128/  130]
train() client id: f_00008-12-0 loss: 0.661702  [   32/  130]
train() client id: f_00008-12-1 loss: 0.921571  [   64/  130]
train() client id: f_00008-12-2 loss: 0.585087  [   96/  130]
train() client id: f_00008-12-3 loss: 0.594248  [  128/  130]
train() client id: f_00009-0-0 loss: 1.069538  [   32/  118]
train() client id: f_00009-0-1 loss: 1.190204  [   64/  118]
train() client id: f_00009-0-2 loss: 1.247168  [   96/  118]
train() client id: f_00009-1-0 loss: 1.083484  [   32/  118]
train() client id: f_00009-1-1 loss: 1.049296  [   64/  118]
train() client id: f_00009-1-2 loss: 1.067966  [   96/  118]
train() client id: f_00009-2-0 loss: 0.952415  [   32/  118]
train() client id: f_00009-2-1 loss: 1.008148  [   64/  118]
train() client id: f_00009-2-2 loss: 1.130975  [   96/  118]
train() client id: f_00009-3-0 loss: 1.136774  [   32/  118]
train() client id: f_00009-3-1 loss: 0.977304  [   64/  118]
train() client id: f_00009-3-2 loss: 0.856400  [   96/  118]
train() client id: f_00009-4-0 loss: 1.081049  [   32/  118]
train() client id: f_00009-4-1 loss: 0.840884  [   64/  118]
train() client id: f_00009-4-2 loss: 1.026039  [   96/  118]
train() client id: f_00009-5-0 loss: 1.018803  [   32/  118]
train() client id: f_00009-5-1 loss: 0.908886  [   64/  118]
train() client id: f_00009-5-2 loss: 0.973020  [   96/  118]
train() client id: f_00009-6-0 loss: 1.042872  [   32/  118]
train() client id: f_00009-6-1 loss: 0.906436  [   64/  118]
train() client id: f_00009-6-2 loss: 0.825046  [   96/  118]
train() client id: f_00009-7-0 loss: 0.855511  [   32/  118]
train() client id: f_00009-7-1 loss: 0.922036  [   64/  118]
train() client id: f_00009-7-2 loss: 0.841680  [   96/  118]
train() client id: f_00009-8-0 loss: 0.979903  [   32/  118]
train() client id: f_00009-8-1 loss: 0.937576  [   64/  118]
train() client id: f_00009-8-2 loss: 0.860924  [   96/  118]
train() client id: f_00009-9-0 loss: 0.869801  [   32/  118]
train() client id: f_00009-9-1 loss: 0.854287  [   64/  118]
train() client id: f_00009-9-2 loss: 0.882197  [   96/  118]
train() client id: f_00009-10-0 loss: 1.088353  [   32/  118]
train() client id: f_00009-10-1 loss: 0.825053  [   64/  118]
train() client id: f_00009-10-2 loss: 0.733426  [   96/  118]
train() client id: f_00009-11-0 loss: 0.913829  [   32/  118]
train() client id: f_00009-11-1 loss: 0.915158  [   64/  118]
train() client id: f_00009-11-2 loss: 0.859205  [   96/  118]
train() client id: f_00009-12-0 loss: 0.814320  [   32/  118]
train() client id: f_00009-12-1 loss: 1.033783  [   64/  118]
train() client id: f_00009-12-2 loss: 0.849709  [   96/  118]
At round 42 accuracy: 0.6525198938992043
At round 42 training accuracy: 0.5895372233400402
At round 42 training loss: 0.8335126271954845
update_location
xs = -3.905658 4.200318 230.009024 18.811294 0.979296 3.956410 -192.443192 -171.324852 214.663977 -157.060879 
ys = 222.587959 205.555839 1.320614 -192.455176 184.350187 167.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 244.050515 228.628182 250.810476 217.699011 209.728278 195.389975 216.890001 198.375606 237.464298 186.236762 
dists_bs = 177.589098 181.786725 440.677704 415.434861 176.227520 179.100623 178.479411 174.216421 420.264836 171.936922 
uav_gains = -110.799066 -109.602742 -111.365089 -108.832866 -108.303628 -107.392702 -108.778069 -107.579824 -110.271545 -106.820522 
bs_gains = -102.550716 -102.834800 -113.602449 -112.885141 -102.457124 -102.653778 -102.611527 -102.317554 -113.025705 -102.157395 
Round 43
-------------------------------
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.31748257 10.95769314  5.24195078  1.89681827 12.63633198  6.07983027
  2.34717104  7.46063269  5.5099153   4.93011902]
obj_prev = 62.377945058813665
eta_min = 4.6045000649351174e-18	eta_max = 0.9345899157842099
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 14.450064093228786	eta = 0.9090909090909091
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 27.618160282034836	eta = 0.47564435026761315
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 21.01662000340652	eta = 0.6250492182285267
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 19.820837826510346	eta = 0.6627581547216592
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 19.755289483233774	eta = 0.6649571961010281
af = 13.13642190293526	bf = 1.2587071102671177	zeta = 19.755074608659463	eta = 0.6649644287942615
eta = 0.6649644287942615
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [0.03366898 0.07081173 0.03313455 0.01149021 0.08176751 0.03901326
 0.01442956 0.04783132 0.03473783 0.03153126]
ene_total = [1.80568477 3.13015273 1.80324111 0.85888841 3.56508653 1.85089364
 0.97553732 2.28873162 1.93281417 1.54404431]
ti_comp = [0.58057304 0.62215632 0.57583732 0.59457466 0.62339857 0.62275708
 0.59491807 0.60189896 0.55960141 0.62435462]
ti_coms = [0.11299635 0.07141307 0.11773207 0.09899474 0.07017082 0.07081231
 0.09865133 0.09167044 0.13396798 0.06921478]
t_total = [27.84981956 27.84981956 27.84981956 27.84981956 27.84981956 27.84981956
 27.84981956 27.84981956 27.84981956 27.84981956]
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [7.07711056e-06 5.73319437e-05 6.85683199e-06 2.68194986e-07
 8.79206213e-05 9.56928073e-06 5.30548435e-07 1.88786089e-05
 8.36624401e-06 5.02621941e-06]
ene_total = [0.45401354 0.2890566  0.47302065 0.39751757 0.28529668 0.2847264
 0.39614917 0.36885463 0.53827549 0.27812918]
optimize_network iter = 0 obj = 3.765039909361509
eta = 0.6649644287942615
freqs = [28996330.2200803  56908313.47423983 28770753.14460626  9662542.53169571
 65582048.83208953 31323013.60893005 12127351.29718995 39733680.81777115
 31038012.71809572 25251084.61040274]
eta_min = 0.6649644287942639	eta_max = 0.6649644287943239
af = 0.006654514151181828	bf = 1.2587071102671177	zeta = 0.007319965566300011	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [1.65314891e-06 1.33922227e-05 1.60169383e-06 6.26479190e-08
 2.05374606e-05 2.23529729e-06 1.23931308e-07 4.40987202e-06
 1.95427881e-06 1.17407932e-06]
ene_total = [1.6724149  1.05878427 1.74248852 1.46497738 1.04145823 1.04824288
 1.45990451 1.35723249 1.98280693 1.02444478]
ti_comp = [0.58057304 0.62215632 0.57583732 0.59457466 0.62339857 0.62275708
 0.59491807 0.60189896 0.55960141 0.62435462]
ti_coms = [0.11299635 0.07141307 0.11773207 0.09899474 0.07017082 0.07081231
 0.09865133 0.09167044 0.13396798 0.06921478]
t_total = [27.84981956 27.84981956 27.84981956 27.84981956 27.84981956 27.84981956
 27.84981956 27.84981956 27.84981956 27.84981956]
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [7.07711056e-06 5.73319437e-05 6.85683199e-06 2.68194986e-07
 8.79206213e-05 9.56928073e-06 5.30548435e-07 1.88786089e-05
 8.36624401e-06 5.02621941e-06]
ene_total = [0.45401354 0.2890566  0.47302065 0.39751757 0.28529668 0.2847264
 0.39614917 0.36885463 0.53827549 0.27812918]
optimize_network iter = 1 obj = 3.765039909362207
eta = 0.6649644287943239
freqs = [28996330.22008007 56908313.47423852 28770753.14460608  9662542.53169558
 65582048.832088   31323013.60892932 12127351.29718979 39733680.81777052
 31038012.71809573 25251084.61040214]
Done!
ene_coms = [0.01129964 0.00714131 0.01177321 0.00989947 0.00701708 0.00708123
 0.00986513 0.00916704 0.0133968  0.00692148]
ene_comp = [6.88604689e-06 5.57841296e-05 6.67171527e-06 2.60954415e-07
 8.55469920e-05 9.31093491e-06 5.16225000e-07 1.83689353e-05
 8.14037707e-06 4.89052449e-06]
ene_total = [0.01130652 0.00719709 0.01177988 0.00989973 0.00710263 0.00709054
 0.00986565 0.00918541 0.01340494 0.00692637]
At round 43 energy consumption: 0.09375876535404022
At round 43 eta: 0.6649644287943239
At round 43 a_n: 13.453131436573733
At round 43 local rounds: 13.360704448743727
At round 43 global rounds: 40.15433760708037
gradient difference: 0.45657190680503845
train() client id: f_00000-0-0 loss: 0.802474  [   32/  126]
train() client id: f_00000-0-1 loss: 1.309981  [   64/  126]
train() client id: f_00000-0-2 loss: 0.991428  [   96/  126]
train() client id: f_00000-1-0 loss: 1.291046  [   32/  126]
train() client id: f_00000-1-1 loss: 0.974681  [   64/  126]
train() client id: f_00000-1-2 loss: 0.901942  [   96/  126]
train() client id: f_00000-2-0 loss: 1.064859  [   32/  126]
train() client id: f_00000-2-1 loss: 0.957759  [   64/  126]
train() client id: f_00000-2-2 loss: 0.868392  [   96/  126]
train() client id: f_00000-3-0 loss: 0.821945  [   32/  126]
train() client id: f_00000-3-1 loss: 0.958265  [   64/  126]
train() client id: f_00000-3-2 loss: 0.925484  [   96/  126]
train() client id: f_00000-4-0 loss: 0.934252  [   32/  126]
train() client id: f_00000-4-1 loss: 0.777568  [   64/  126]
train() client id: f_00000-4-2 loss: 0.847830  [   96/  126]
train() client id: f_00000-5-0 loss: 0.871896  [   32/  126]
train() client id: f_00000-5-1 loss: 0.784992  [   64/  126]
train() client id: f_00000-5-2 loss: 0.818021  [   96/  126]
train() client id: f_00000-6-0 loss: 0.641293  [   32/  126]
train() client id: f_00000-6-1 loss: 0.902630  [   64/  126]
train() client id: f_00000-6-2 loss: 0.819748  [   96/  126]
train() client id: f_00000-7-0 loss: 0.864804  [   32/  126]
train() client id: f_00000-7-1 loss: 0.764055  [   64/  126]
train() client id: f_00000-7-2 loss: 0.670178  [   96/  126]
train() client id: f_00000-8-0 loss: 0.714235  [   32/  126]
train() client id: f_00000-8-1 loss: 0.794416  [   64/  126]
train() client id: f_00000-8-2 loss: 0.785784  [   96/  126]
train() client id: f_00000-9-0 loss: 0.771665  [   32/  126]
train() client id: f_00000-9-1 loss: 0.743067  [   64/  126]
train() client id: f_00000-9-2 loss: 0.682951  [   96/  126]
train() client id: f_00000-10-0 loss: 0.729037  [   32/  126]
train() client id: f_00000-10-1 loss: 0.722404  [   64/  126]
train() client id: f_00000-10-2 loss: 0.851020  [   96/  126]
train() client id: f_00000-11-0 loss: 0.674933  [   32/  126]
train() client id: f_00000-11-1 loss: 0.666940  [   64/  126]
train() client id: f_00000-11-2 loss: 0.909422  [   96/  126]
train() client id: f_00000-12-0 loss: 0.792673  [   32/  126]
train() client id: f_00000-12-1 loss: 0.630636  [   64/  126]
train() client id: f_00000-12-2 loss: 0.848062  [   96/  126]
train() client id: f_00001-0-0 loss: 0.328716  [   32/  265]
train() client id: f_00001-0-1 loss: 0.288102  [   64/  265]
train() client id: f_00001-0-2 loss: 0.445627  [   96/  265]
train() client id: f_00001-0-3 loss: 0.283851  [  128/  265]
train() client id: f_00001-0-4 loss: 0.287317  [  160/  265]
train() client id: f_00001-0-5 loss: 0.310182  [  192/  265]
train() client id: f_00001-0-6 loss: 0.401918  [  224/  265]
train() client id: f_00001-0-7 loss: 0.579012  [  256/  265]
train() client id: f_00001-1-0 loss: 0.416978  [   32/  265]
train() client id: f_00001-1-1 loss: 0.355231  [   64/  265]
train() client id: f_00001-1-2 loss: 0.357503  [   96/  265]
train() client id: f_00001-1-3 loss: 0.553533  [  128/  265]
train() client id: f_00001-1-4 loss: 0.280259  [  160/  265]
train() client id: f_00001-1-5 loss: 0.312710  [  192/  265]
train() client id: f_00001-1-6 loss: 0.262938  [  224/  265]
train() client id: f_00001-1-7 loss: 0.338088  [  256/  265]
train() client id: f_00001-2-0 loss: 0.380514  [   32/  265]
train() client id: f_00001-2-1 loss: 0.462693  [   64/  265]
train() client id: f_00001-2-2 loss: 0.416149  [   96/  265]
train() client id: f_00001-2-3 loss: 0.375219  [  128/  265]
train() client id: f_00001-2-4 loss: 0.330541  [  160/  265]
train() client id: f_00001-2-5 loss: 0.277439  [  192/  265]
train() client id: f_00001-2-6 loss: 0.269499  [  224/  265]
train() client id: f_00001-2-7 loss: 0.282834  [  256/  265]
train() client id: f_00001-3-0 loss: 0.448660  [   32/  265]
train() client id: f_00001-3-1 loss: 0.376244  [   64/  265]
train() client id: f_00001-3-2 loss: 0.262246  [   96/  265]
train() client id: f_00001-3-3 loss: 0.380721  [  128/  265]
train() client id: f_00001-3-4 loss: 0.266528  [  160/  265]
train() client id: f_00001-3-5 loss: 0.311697  [  192/  265]
train() client id: f_00001-3-6 loss: 0.308761  [  224/  265]
train() client id: f_00001-3-7 loss: 0.360911  [  256/  265]
train() client id: f_00001-4-0 loss: 0.399544  [   32/  265]
train() client id: f_00001-4-1 loss: 0.304947  [   64/  265]
train() client id: f_00001-4-2 loss: 0.399302  [   96/  265]
train() client id: f_00001-4-3 loss: 0.248935  [  128/  265]
train() client id: f_00001-4-4 loss: 0.292809  [  160/  265]
train() client id: f_00001-4-5 loss: 0.252763  [  192/  265]
train() client id: f_00001-4-6 loss: 0.324277  [  224/  265]
train() client id: f_00001-4-7 loss: 0.444196  [  256/  265]
train() client id: f_00001-5-0 loss: 0.342362  [   32/  265]
train() client id: f_00001-5-1 loss: 0.245990  [   64/  265]
train() client id: f_00001-5-2 loss: 0.390265  [   96/  265]
train() client id: f_00001-5-3 loss: 0.257440  [  128/  265]
train() client id: f_00001-5-4 loss: 0.333066  [  160/  265]
train() client id: f_00001-5-5 loss: 0.426398  [  192/  265]
train() client id: f_00001-5-6 loss: 0.358835  [  224/  265]
train() client id: f_00001-5-7 loss: 0.229661  [  256/  265]
train() client id: f_00001-6-0 loss: 0.309155  [   32/  265]
train() client id: f_00001-6-1 loss: 0.258443  [   64/  265]
train() client id: f_00001-6-2 loss: 0.314820  [   96/  265]
train() client id: f_00001-6-3 loss: 0.226087  [  128/  265]
train() client id: f_00001-6-4 loss: 0.393646  [  160/  265]
train() client id: f_00001-6-5 loss: 0.372913  [  192/  265]
train() client id: f_00001-6-6 loss: 0.414002  [  224/  265]
train() client id: f_00001-6-7 loss: 0.299526  [  256/  265]
train() client id: f_00001-7-0 loss: 0.210760  [   32/  265]
train() client id: f_00001-7-1 loss: 0.237844  [   64/  265]
train() client id: f_00001-7-2 loss: 0.273542  [   96/  265]
train() client id: f_00001-7-3 loss: 0.305630  [  128/  265]
train() client id: f_00001-7-4 loss: 0.450397  [  160/  265]
train() client id: f_00001-7-5 loss: 0.329654  [  192/  265]
train() client id: f_00001-7-6 loss: 0.376236  [  224/  265]
train() client id: f_00001-7-7 loss: 0.322352  [  256/  265]
train() client id: f_00001-8-0 loss: 0.276855  [   32/  265]
train() client id: f_00001-8-1 loss: 0.377268  [   64/  265]
train() client id: f_00001-8-2 loss: 0.230292  [   96/  265]
train() client id: f_00001-8-3 loss: 0.323721  [  128/  265]
train() client id: f_00001-8-4 loss: 0.322396  [  160/  265]
train() client id: f_00001-8-5 loss: 0.302601  [  192/  265]
train() client id: f_00001-8-6 loss: 0.333986  [  224/  265]
train() client id: f_00001-8-7 loss: 0.324757  [  256/  265]
train() client id: f_00001-9-0 loss: 0.217425  [   32/  265]
train() client id: f_00001-9-1 loss: 0.343522  [   64/  265]
train() client id: f_00001-9-2 loss: 0.288625  [   96/  265]
train() client id: f_00001-9-3 loss: 0.330756  [  128/  265]
train() client id: f_00001-9-4 loss: 0.429009  [  160/  265]
train() client id: f_00001-9-5 loss: 0.229499  [  192/  265]
train() client id: f_00001-9-6 loss: 0.329779  [  224/  265]
train() client id: f_00001-9-7 loss: 0.213283  [  256/  265]
train() client id: f_00001-10-0 loss: 0.237879  [   32/  265]
train() client id: f_00001-10-1 loss: 0.458805  [   64/  265]
train() client id: f_00001-10-2 loss: 0.289707  [   96/  265]
train() client id: f_00001-10-3 loss: 0.260791  [  128/  265]
train() client id: f_00001-10-4 loss: 0.263081  [  160/  265]
train() client id: f_00001-10-5 loss: 0.324731  [  192/  265]
train() client id: f_00001-10-6 loss: 0.349889  [  224/  265]
train() client id: f_00001-10-7 loss: 0.286528  [  256/  265]
train() client id: f_00001-11-0 loss: 0.227993  [   32/  265]
train() client id: f_00001-11-1 loss: 0.359488  [   64/  265]
train() client id: f_00001-11-2 loss: 0.302313  [   96/  265]
train() client id: f_00001-11-3 loss: 0.220552  [  128/  265]
train() client id: f_00001-11-4 loss: 0.359786  [  160/  265]
train() client id: f_00001-11-5 loss: 0.406311  [  192/  265]
train() client id: f_00001-11-6 loss: 0.303598  [  224/  265]
train() client id: f_00001-11-7 loss: 0.289792  [  256/  265]
train() client id: f_00001-12-0 loss: 0.357063  [   32/  265]
train() client id: f_00001-12-1 loss: 0.208380  [   64/  265]
train() client id: f_00001-12-2 loss: 0.210629  [   96/  265]
train() client id: f_00001-12-3 loss: 0.352278  [  128/  265]
train() client id: f_00001-12-4 loss: 0.345553  [  160/  265]
train() client id: f_00001-12-5 loss: 0.225777  [  192/  265]
train() client id: f_00001-12-6 loss: 0.308359  [  224/  265]
train() client id: f_00001-12-7 loss: 0.346487  [  256/  265]
train() client id: f_00002-0-0 loss: 0.889702  [   32/  124]
train() client id: f_00002-0-1 loss: 1.283405  [   64/  124]
train() client id: f_00002-0-2 loss: 1.164312  [   96/  124]
train() client id: f_00002-1-0 loss: 1.037637  [   32/  124]
train() client id: f_00002-1-1 loss: 1.017643  [   64/  124]
train() client id: f_00002-1-2 loss: 1.129468  [   96/  124]
train() client id: f_00002-2-0 loss: 0.884891  [   32/  124]
train() client id: f_00002-2-1 loss: 1.151940  [   64/  124]
train() client id: f_00002-2-2 loss: 0.935138  [   96/  124]
train() client id: f_00002-3-0 loss: 1.230578  [   32/  124]
train() client id: f_00002-3-1 loss: 0.997078  [   64/  124]
train() client id: f_00002-3-2 loss: 0.905725  [   96/  124]
train() client id: f_00002-4-0 loss: 1.011555  [   32/  124]
train() client id: f_00002-4-1 loss: 0.806452  [   64/  124]
train() client id: f_00002-4-2 loss: 0.922354  [   96/  124]
train() client id: f_00002-5-0 loss: 0.998384  [   32/  124]
train() client id: f_00002-5-1 loss: 0.910361  [   64/  124]
train() client id: f_00002-5-2 loss: 0.899346  [   96/  124]
train() client id: f_00002-6-0 loss: 0.760367  [   32/  124]
train() client id: f_00002-6-1 loss: 0.865663  [   64/  124]
train() client id: f_00002-6-2 loss: 1.090067  [   96/  124]
train() client id: f_00002-7-0 loss: 0.847777  [   32/  124]
train() client id: f_00002-7-1 loss: 0.935694  [   64/  124]
train() client id: f_00002-7-2 loss: 0.931316  [   96/  124]
train() client id: f_00002-8-0 loss: 0.859202  [   32/  124]
train() client id: f_00002-8-1 loss: 1.034719  [   64/  124]
train() client id: f_00002-8-2 loss: 0.708474  [   96/  124]
train() client id: f_00002-9-0 loss: 1.045151  [   32/  124]
train() client id: f_00002-9-1 loss: 0.796615  [   64/  124]
train() client id: f_00002-9-2 loss: 0.774973  [   96/  124]
train() client id: f_00002-10-0 loss: 0.751378  [   32/  124]
train() client id: f_00002-10-1 loss: 0.957755  [   64/  124]
train() client id: f_00002-10-2 loss: 0.877967  [   96/  124]
train() client id: f_00002-11-0 loss: 1.012104  [   32/  124]
train() client id: f_00002-11-1 loss: 0.875072  [   64/  124]
train() client id: f_00002-11-2 loss: 0.761860  [   96/  124]
train() client id: f_00002-12-0 loss: 0.771010  [   32/  124]
train() client id: f_00002-12-1 loss: 0.866852  [   64/  124]
train() client id: f_00002-12-2 loss: 0.917742  [   96/  124]
train() client id: f_00003-0-0 loss: 0.732661  [   32/   43]
train() client id: f_00003-1-0 loss: 0.805036  [   32/   43]
train() client id: f_00003-2-0 loss: 0.714002  [   32/   43]
train() client id: f_00003-3-0 loss: 0.809659  [   32/   43]
train() client id: f_00003-4-0 loss: 0.785961  [   32/   43]
train() client id: f_00003-5-0 loss: 0.603032  [   32/   43]
train() client id: f_00003-6-0 loss: 0.654732  [   32/   43]
train() client id: f_00003-7-0 loss: 0.814941  [   32/   43]
train() client id: f_00003-8-0 loss: 0.800985  [   32/   43]
train() client id: f_00003-9-0 loss: 0.638032  [   32/   43]
train() client id: f_00003-10-0 loss: 0.713929  [   32/   43]
train() client id: f_00003-11-0 loss: 0.629554  [   32/   43]
train() client id: f_00003-12-0 loss: 0.623482  [   32/   43]
train() client id: f_00004-0-0 loss: 0.781066  [   32/  306]
train() client id: f_00004-0-1 loss: 0.885650  [   64/  306]
train() client id: f_00004-0-2 loss: 0.841219  [   96/  306]
train() client id: f_00004-0-3 loss: 0.899137  [  128/  306]
train() client id: f_00004-0-4 loss: 0.906735  [  160/  306]
train() client id: f_00004-0-5 loss: 0.968126  [  192/  306]
train() client id: f_00004-0-6 loss: 0.944836  [  224/  306]
train() client id: f_00004-0-7 loss: 0.778861  [  256/  306]
train() client id: f_00004-0-8 loss: 0.905360  [  288/  306]
train() client id: f_00004-1-0 loss: 0.889894  [   32/  306]
train() client id: f_00004-1-1 loss: 0.973740  [   64/  306]
train() client id: f_00004-1-2 loss: 0.815996  [   96/  306]
train() client id: f_00004-1-3 loss: 0.800773  [  128/  306]
train() client id: f_00004-1-4 loss: 0.916433  [  160/  306]
train() client id: f_00004-1-5 loss: 0.815229  [  192/  306]
train() client id: f_00004-1-6 loss: 0.828227  [  224/  306]
train() client id: f_00004-1-7 loss: 0.932148  [  256/  306]
train() client id: f_00004-1-8 loss: 0.916962  [  288/  306]
train() client id: f_00004-2-0 loss: 0.959344  [   32/  306]
train() client id: f_00004-2-1 loss: 0.895698  [   64/  306]
train() client id: f_00004-2-2 loss: 0.760242  [   96/  306]
train() client id: f_00004-2-3 loss: 0.812061  [  128/  306]
train() client id: f_00004-2-4 loss: 0.981873  [  160/  306]
train() client id: f_00004-2-5 loss: 0.854760  [  192/  306]
train() client id: f_00004-2-6 loss: 1.047860  [  224/  306]
train() client id: f_00004-2-7 loss: 0.879639  [  256/  306]
train() client id: f_00004-2-8 loss: 0.835728  [  288/  306]
train() client id: f_00004-3-0 loss: 0.884324  [   32/  306]
train() client id: f_00004-3-1 loss: 0.776965  [   64/  306]
train() client id: f_00004-3-2 loss: 0.835434  [   96/  306]
train() client id: f_00004-3-3 loss: 0.806415  [  128/  306]
train() client id: f_00004-3-4 loss: 1.018428  [  160/  306]
train() client id: f_00004-3-5 loss: 0.887754  [  192/  306]
train() client id: f_00004-3-6 loss: 0.833852  [  224/  306]
train() client id: f_00004-3-7 loss: 0.860076  [  256/  306]
train() client id: f_00004-3-8 loss: 0.963863  [  288/  306]
train() client id: f_00004-4-0 loss: 0.784281  [   32/  306]
train() client id: f_00004-4-1 loss: 0.926739  [   64/  306]
train() client id: f_00004-4-2 loss: 0.778102  [   96/  306]
train() client id: f_00004-4-3 loss: 0.944521  [  128/  306]
train() client id: f_00004-4-4 loss: 0.886248  [  160/  306]
train() client id: f_00004-4-5 loss: 0.870704  [  192/  306]
train() client id: f_00004-4-6 loss: 0.794119  [  224/  306]
train() client id: f_00004-4-7 loss: 1.005395  [  256/  306]
train() client id: f_00004-4-8 loss: 0.899304  [  288/  306]
train() client id: f_00004-5-0 loss: 0.806832  [   32/  306]
train() client id: f_00004-5-1 loss: 0.708882  [   64/  306]
train() client id: f_00004-5-2 loss: 0.988566  [   96/  306]
train() client id: f_00004-5-3 loss: 0.924798  [  128/  306]
train() client id: f_00004-5-4 loss: 0.846071  [  160/  306]
train() client id: f_00004-5-5 loss: 0.840472  [  192/  306]
train() client id: f_00004-5-6 loss: 0.920220  [  224/  306]
train() client id: f_00004-5-7 loss: 0.867482  [  256/  306]
train() client id: f_00004-5-8 loss: 0.966257  [  288/  306]
train() client id: f_00004-6-0 loss: 0.944225  [   32/  306]
train() client id: f_00004-6-1 loss: 0.852093  [   64/  306]
train() client id: f_00004-6-2 loss: 1.005120  [   96/  306]
train() client id: f_00004-6-3 loss: 0.849563  [  128/  306]
train() client id: f_00004-6-4 loss: 0.699214  [  160/  306]
train() client id: f_00004-6-5 loss: 0.973213  [  192/  306]
train() client id: f_00004-6-6 loss: 0.824722  [  224/  306]
train() client id: f_00004-6-7 loss: 0.858296  [  256/  306]
train() client id: f_00004-6-8 loss: 0.904746  [  288/  306]
train() client id: f_00004-7-0 loss: 0.833248  [   32/  306]
train() client id: f_00004-7-1 loss: 0.929856  [   64/  306]
train() client id: f_00004-7-2 loss: 0.780330  [   96/  306]
train() client id: f_00004-7-3 loss: 0.843181  [  128/  306]
train() client id: f_00004-7-4 loss: 0.974308  [  160/  306]
train() client id: f_00004-7-5 loss: 0.935817  [  192/  306]
train() client id: f_00004-7-6 loss: 0.902561  [  224/  306]
train() client id: f_00004-7-7 loss: 0.870309  [  256/  306]
train() client id: f_00004-7-8 loss: 0.913134  [  288/  306]
train() client id: f_00004-8-0 loss: 0.851652  [   32/  306]
train() client id: f_00004-8-1 loss: 0.775149  [   64/  306]
train() client id: f_00004-8-2 loss: 0.972772  [   96/  306]
train() client id: f_00004-8-3 loss: 0.881186  [  128/  306]
train() client id: f_00004-8-4 loss: 0.821728  [  160/  306]
train() client id: f_00004-8-5 loss: 0.863386  [  192/  306]
train() client id: f_00004-8-6 loss: 0.963542  [  224/  306]
train() client id: f_00004-8-7 loss: 0.908388  [  256/  306]
train() client id: f_00004-8-8 loss: 0.928897  [  288/  306]
train() client id: f_00004-9-0 loss: 0.998347  [   32/  306]
train() client id: f_00004-9-1 loss: 0.880273  [   64/  306]
train() client id: f_00004-9-2 loss: 0.947158  [   96/  306]
train() client id: f_00004-9-3 loss: 0.718367  [  128/  306]
train() client id: f_00004-9-4 loss: 0.773316  [  160/  306]
train() client id: f_00004-9-5 loss: 0.915166  [  192/  306]
train() client id: f_00004-9-6 loss: 0.954360  [  224/  306]
train() client id: f_00004-9-7 loss: 0.942067  [  256/  306]
train() client id: f_00004-9-8 loss: 0.836383  [  288/  306]
train() client id: f_00004-10-0 loss: 0.799450  [   32/  306]
train() client id: f_00004-10-1 loss: 0.878443  [   64/  306]
train() client id: f_00004-10-2 loss: 0.975419  [   96/  306]
train() client id: f_00004-10-3 loss: 0.916397  [  128/  306]
train() client id: f_00004-10-4 loss: 0.868177  [  160/  306]
train() client id: f_00004-10-5 loss: 0.776251  [  192/  306]
train() client id: f_00004-10-6 loss: 0.951838  [  224/  306]
train() client id: f_00004-10-7 loss: 0.951754  [  256/  306]
train() client id: f_00004-10-8 loss: 0.920030  [  288/  306]
train() client id: f_00004-11-0 loss: 0.857369  [   32/  306]
train() client id: f_00004-11-1 loss: 0.883724  [   64/  306]
train() client id: f_00004-11-2 loss: 0.950020  [   96/  306]
train() client id: f_00004-11-3 loss: 0.986561  [  128/  306]
train() client id: f_00004-11-4 loss: 0.853910  [  160/  306]
train() client id: f_00004-11-5 loss: 0.818699  [  192/  306]
train() client id: f_00004-11-6 loss: 0.897203  [  224/  306]
train() client id: f_00004-11-7 loss: 0.845607  [  256/  306]
train() client id: f_00004-11-8 loss: 0.861338  [  288/  306]
train() client id: f_00004-12-0 loss: 1.022855  [   32/  306]
train() client id: f_00004-12-1 loss: 0.880322  [   64/  306]
train() client id: f_00004-12-2 loss: 0.858777  [   96/  306]
train() client id: f_00004-12-3 loss: 0.908649  [  128/  306]
train() client id: f_00004-12-4 loss: 0.879842  [  160/  306]
train() client id: f_00004-12-5 loss: 0.858805  [  192/  306]
train() client id: f_00004-12-6 loss: 0.908884  [  224/  306]
train() client id: f_00004-12-7 loss: 0.808280  [  256/  306]
train() client id: f_00004-12-8 loss: 0.792171  [  288/  306]
train() client id: f_00005-0-0 loss: 0.656195  [   32/  146]
train() client id: f_00005-0-1 loss: 0.908717  [   64/  146]
train() client id: f_00005-0-2 loss: 0.616340  [   96/  146]
train() client id: f_00005-0-3 loss: 0.786136  [  128/  146]
train() client id: f_00005-1-0 loss: 0.784141  [   32/  146]
train() client id: f_00005-1-1 loss: 0.652685  [   64/  146]
train() client id: f_00005-1-2 loss: 0.681130  [   96/  146]
train() client id: f_00005-1-3 loss: 0.806581  [  128/  146]
train() client id: f_00005-2-0 loss: 0.789172  [   32/  146]
train() client id: f_00005-2-1 loss: 0.627177  [   64/  146]
train() client id: f_00005-2-2 loss: 0.901424  [   96/  146]
train() client id: f_00005-2-3 loss: 0.502280  [  128/  146]
train() client id: f_00005-3-0 loss: 0.707074  [   32/  146]
train() client id: f_00005-3-1 loss: 0.664852  [   64/  146]
train() client id: f_00005-3-2 loss: 0.765874  [   96/  146]
train() client id: f_00005-3-3 loss: 0.920991  [  128/  146]
train() client id: f_00005-4-0 loss: 0.861138  [   32/  146]
train() client id: f_00005-4-1 loss: 0.851246  [   64/  146]
train() client id: f_00005-4-2 loss: 0.505598  [   96/  146]
train() client id: f_00005-4-3 loss: 0.790760  [  128/  146]
train() client id: f_00005-5-0 loss: 0.808435  [   32/  146]
train() client id: f_00005-5-1 loss: 0.760119  [   64/  146]
train() client id: f_00005-5-2 loss: 0.904969  [   96/  146]
train() client id: f_00005-5-3 loss: 0.596136  [  128/  146]
train() client id: f_00005-6-0 loss: 0.716049  [   32/  146]
train() client id: f_00005-6-1 loss: 0.521239  [   64/  146]
train() client id: f_00005-6-2 loss: 0.827961  [   96/  146]
train() client id: f_00005-6-3 loss: 0.885898  [  128/  146]
train() client id: f_00005-7-0 loss: 0.801531  [   32/  146]
train() client id: f_00005-7-1 loss: 0.651945  [   64/  146]
train() client id: f_00005-7-2 loss: 0.737071  [   96/  146]
train() client id: f_00005-7-3 loss: 0.769746  [  128/  146]
train() client id: f_00005-8-0 loss: 0.610007  [   32/  146]
train() client id: f_00005-8-1 loss: 0.729026  [   64/  146]
train() client id: f_00005-8-2 loss: 0.805131  [   96/  146]
train() client id: f_00005-8-3 loss: 0.914015  [  128/  146]
train() client id: f_00005-9-0 loss: 0.451129  [   32/  146]
train() client id: f_00005-9-1 loss: 0.734496  [   64/  146]
train() client id: f_00005-9-2 loss: 0.846373  [   96/  146]
train() client id: f_00005-9-3 loss: 0.778219  [  128/  146]
train() client id: f_00005-10-0 loss: 0.931713  [   32/  146]
train() client id: f_00005-10-1 loss: 0.766356  [   64/  146]
train() client id: f_00005-10-2 loss: 0.651365  [   96/  146]
train() client id: f_00005-10-3 loss: 0.807579  [  128/  146]
train() client id: f_00005-11-0 loss: 0.551111  [   32/  146]
train() client id: f_00005-11-1 loss: 0.761337  [   64/  146]
train() client id: f_00005-11-2 loss: 0.712298  [   96/  146]
train() client id: f_00005-11-3 loss: 0.852213  [  128/  146]
train() client id: f_00005-12-0 loss: 0.674497  [   32/  146]
train() client id: f_00005-12-1 loss: 0.793492  [   64/  146]
train() client id: f_00005-12-2 loss: 0.751990  [   96/  146]
train() client id: f_00005-12-3 loss: 0.637161  [  128/  146]
train() client id: f_00006-0-0 loss: 0.467392  [   32/   54]
train() client id: f_00006-1-0 loss: 0.505652  [   32/   54]
train() client id: f_00006-2-0 loss: 0.527170  [   32/   54]
train() client id: f_00006-3-0 loss: 0.560190  [   32/   54]
train() client id: f_00006-4-0 loss: 0.499543  [   32/   54]
train() client id: f_00006-5-0 loss: 0.505125  [   32/   54]
train() client id: f_00006-6-0 loss: 0.547624  [   32/   54]
train() client id: f_00006-7-0 loss: 0.452445  [   32/   54]
train() client id: f_00006-8-0 loss: 0.516841  [   32/   54]
train() client id: f_00006-9-0 loss: 0.515985  [   32/   54]
train() client id: f_00006-10-0 loss: 0.509175  [   32/   54]
train() client id: f_00006-11-0 loss: 0.514152  [   32/   54]
train() client id: f_00006-12-0 loss: 0.504916  [   32/   54]
train() client id: f_00007-0-0 loss: 0.332354  [   32/  179]
train() client id: f_00007-0-1 loss: 0.308194  [   64/  179]
train() client id: f_00007-0-2 loss: 0.558790  [   96/  179]
train() client id: f_00007-0-3 loss: 0.422759  [  128/  179]
train() client id: f_00007-0-4 loss: 0.803490  [  160/  179]
train() client id: f_00007-1-0 loss: 0.303395  [   32/  179]
train() client id: f_00007-1-1 loss: 0.824020  [   64/  179]
train() client id: f_00007-1-2 loss: 0.435811  [   96/  179]
train() client id: f_00007-1-3 loss: 0.439758  [  128/  179]
train() client id: f_00007-1-4 loss: 0.362479  [  160/  179]
train() client id: f_00007-2-0 loss: 0.428769  [   32/  179]
train() client id: f_00007-2-1 loss: 0.596043  [   64/  179]
train() client id: f_00007-2-2 loss: 0.327404  [   96/  179]
train() client id: f_00007-2-3 loss: 0.562270  [  128/  179]
train() client id: f_00007-2-4 loss: 0.282561  [  160/  179]
train() client id: f_00007-3-0 loss: 0.604630  [   32/  179]
train() client id: f_00007-3-1 loss: 0.394376  [   64/  179]
train() client id: f_00007-3-2 loss: 0.437618  [   96/  179]
train() client id: f_00007-3-3 loss: 0.373378  [  128/  179]
train() client id: f_00007-3-4 loss: 0.392411  [  160/  179]
train() client id: f_00007-4-0 loss: 0.446223  [   32/  179]
train() client id: f_00007-4-1 loss: 0.535133  [   64/  179]
train() client id: f_00007-4-2 loss: 0.277090  [   96/  179]
train() client id: f_00007-4-3 loss: 0.368609  [  128/  179]
train() client id: f_00007-4-4 loss: 0.557432  [  160/  179]
train() client id: f_00007-5-0 loss: 0.511734  [   32/  179]
train() client id: f_00007-5-1 loss: 0.455963  [   64/  179]
train() client id: f_00007-5-2 loss: 0.344893  [   96/  179]
train() client id: f_00007-5-3 loss: 0.361862  [  128/  179]
train() client id: f_00007-5-4 loss: 0.372821  [  160/  179]
train() client id: f_00007-6-0 loss: 0.413005  [   32/  179]
train() client id: f_00007-6-1 loss: 0.356364  [   64/  179]
train() client id: f_00007-6-2 loss: 0.668777  [   96/  179]
train() client id: f_00007-6-3 loss: 0.242616  [  128/  179]
train() client id: f_00007-6-4 loss: 0.382623  [  160/  179]
train() client id: f_00007-7-0 loss: 0.512762  [   32/  179]
train() client id: f_00007-7-1 loss: 0.575988  [   64/  179]
train() client id: f_00007-7-2 loss: 0.274943  [   96/  179]
train() client id: f_00007-7-3 loss: 0.270524  [  128/  179]
train() client id: f_00007-7-4 loss: 0.431598  [  160/  179]
train() client id: f_00007-8-0 loss: 0.359405  [   32/  179]
train() client id: f_00007-8-1 loss: 0.554042  [   64/  179]
train() client id: f_00007-8-2 loss: 0.365416  [   96/  179]
train() client id: f_00007-8-3 loss: 0.432467  [  128/  179]
train() client id: f_00007-8-4 loss: 0.270295  [  160/  179]
train() client id: f_00007-9-0 loss: 0.458235  [   32/  179]
train() client id: f_00007-9-1 loss: 0.343891  [   64/  179]
train() client id: f_00007-9-2 loss: 0.627882  [   96/  179]
train() client id: f_00007-9-3 loss: 0.243541  [  128/  179]
train() client id: f_00007-9-4 loss: 0.329018  [  160/  179]
train() client id: f_00007-10-0 loss: 0.365569  [   32/  179]
train() client id: f_00007-10-1 loss: 0.277701  [   64/  179]
train() client id: f_00007-10-2 loss: 0.412065  [   96/  179]
train() client id: f_00007-10-3 loss: 0.303629  [  128/  179]
train() client id: f_00007-10-4 loss: 0.544852  [  160/  179]
train() client id: f_00007-11-0 loss: 0.291560  [   32/  179]
train() client id: f_00007-11-1 loss: 0.516118  [   64/  179]
train() client id: f_00007-11-2 loss: 0.368398  [   96/  179]
train() client id: f_00007-11-3 loss: 0.325373  [  128/  179]
train() client id: f_00007-11-4 loss: 0.536136  [  160/  179]
train() client id: f_00007-12-0 loss: 0.390055  [   32/  179]
train() client id: f_00007-12-1 loss: 0.263040  [   64/  179]
train() client id: f_00007-12-2 loss: 0.324188  [   96/  179]
train() client id: f_00007-12-3 loss: 0.559068  [  128/  179]
train() client id: f_00007-12-4 loss: 0.379272  [  160/  179]
train() client id: f_00008-0-0 loss: 0.821547  [   32/  130]
train() client id: f_00008-0-1 loss: 0.836524  [   64/  130]
train() client id: f_00008-0-2 loss: 0.917835  [   96/  130]
train() client id: f_00008-0-3 loss: 0.852110  [  128/  130]
train() client id: f_00008-1-0 loss: 0.869388  [   32/  130]
train() client id: f_00008-1-1 loss: 0.849845  [   64/  130]
train() client id: f_00008-1-2 loss: 0.742415  [   96/  130]
train() client id: f_00008-1-3 loss: 0.923107  [  128/  130]
train() client id: f_00008-2-0 loss: 0.970628  [   32/  130]
train() client id: f_00008-2-1 loss: 0.804719  [   64/  130]
train() client id: f_00008-2-2 loss: 0.745329  [   96/  130]
train() client id: f_00008-2-3 loss: 0.878006  [  128/  130]
train() client id: f_00008-3-0 loss: 0.824266  [   32/  130]
train() client id: f_00008-3-1 loss: 0.847749  [   64/  130]
train() client id: f_00008-3-2 loss: 0.808643  [   96/  130]
train() client id: f_00008-3-3 loss: 0.937931  [  128/  130]
train() client id: f_00008-4-0 loss: 0.886274  [   32/  130]
train() client id: f_00008-4-1 loss: 0.878085  [   64/  130]
train() client id: f_00008-4-2 loss: 0.831403  [   96/  130]
train() client id: f_00008-4-3 loss: 0.773503  [  128/  130]
train() client id: f_00008-5-0 loss: 0.844768  [   32/  130]
train() client id: f_00008-5-1 loss: 0.857903  [   64/  130]
train() client id: f_00008-5-2 loss: 0.922411  [   96/  130]
train() client id: f_00008-5-3 loss: 0.745584  [  128/  130]
train() client id: f_00008-6-0 loss: 0.888315  [   32/  130]
train() client id: f_00008-6-1 loss: 0.774618  [   64/  130]
train() client id: f_00008-6-2 loss: 0.942595  [   96/  130]
train() client id: f_00008-6-3 loss: 0.807665  [  128/  130]
train() client id: f_00008-7-0 loss: 0.860554  [   32/  130]
train() client id: f_00008-7-1 loss: 0.805558  [   64/  130]
train() client id: f_00008-7-2 loss: 0.788583  [   96/  130]
train() client id: f_00008-7-3 loss: 0.945887  [  128/  130]
train() client id: f_00008-8-0 loss: 0.724167  [   32/  130]
train() client id: f_00008-8-1 loss: 0.975261  [   64/  130]
train() client id: f_00008-8-2 loss: 0.810326  [   96/  130]
train() client id: f_00008-8-3 loss: 0.900160  [  128/  130]
train() client id: f_00008-9-0 loss: 0.760452  [   32/  130]
train() client id: f_00008-9-1 loss: 0.916905  [   64/  130]
train() client id: f_00008-9-2 loss: 0.794206  [   96/  130]
train() client id: f_00008-9-3 loss: 0.931939  [  128/  130]
train() client id: f_00008-10-0 loss: 0.693082  [   32/  130]
train() client id: f_00008-10-1 loss: 0.924848  [   64/  130]
train() client id: f_00008-10-2 loss: 0.876521  [   96/  130]
train() client id: f_00008-10-3 loss: 0.872251  [  128/  130]
train() client id: f_00008-11-0 loss: 0.966797  [   32/  130]
train() client id: f_00008-11-1 loss: 0.756672  [   64/  130]
train() client id: f_00008-11-2 loss: 0.851251  [   96/  130]
train() client id: f_00008-11-3 loss: 0.782541  [  128/  130]
train() client id: f_00008-12-0 loss: 0.885083  [   32/  130]
train() client id: f_00008-12-1 loss: 0.751095  [   64/  130]
train() client id: f_00008-12-2 loss: 0.848231  [   96/  130]
train() client id: f_00008-12-3 loss: 0.907905  [  128/  130]
train() client id: f_00009-0-0 loss: 1.143229  [   32/  118]
train() client id: f_00009-0-1 loss: 1.099586  [   64/  118]
train() client id: f_00009-0-2 loss: 0.977315  [   96/  118]
train() client id: f_00009-1-0 loss: 1.104482  [   32/  118]
train() client id: f_00009-1-1 loss: 1.035438  [   64/  118]
train() client id: f_00009-1-2 loss: 0.945219  [   96/  118]
train() client id: f_00009-2-0 loss: 0.831575  [   32/  118]
train() client id: f_00009-2-1 loss: 1.053850  [   64/  118]
train() client id: f_00009-2-2 loss: 0.995688  [   96/  118]
train() client id: f_00009-3-0 loss: 0.952195  [   32/  118]
train() client id: f_00009-3-1 loss: 1.035584  [   64/  118]
train() client id: f_00009-3-2 loss: 0.938721  [   96/  118]
train() client id: f_00009-4-0 loss: 0.855543  [   32/  118]
train() client id: f_00009-4-1 loss: 0.935744  [   64/  118]
train() client id: f_00009-4-2 loss: 1.047293  [   96/  118]
train() client id: f_00009-5-0 loss: 0.969075  [   32/  118]
train() client id: f_00009-5-1 loss: 0.978902  [   64/  118]
train() client id: f_00009-5-2 loss: 0.986728  [   96/  118]
train() client id: f_00009-6-0 loss: 0.958528  [   32/  118]
train() client id: f_00009-6-1 loss: 0.946556  [   64/  118]
train() client id: f_00009-6-2 loss: 0.930566  [   96/  118]
train() client id: f_00009-7-0 loss: 0.819123  [   32/  118]
train() client id: f_00009-7-1 loss: 0.976803  [   64/  118]
train() client id: f_00009-7-2 loss: 0.874196  [   96/  118]
train() client id: f_00009-8-0 loss: 0.912524  [   32/  118]
train() client id: f_00009-8-1 loss: 0.989271  [   64/  118]
train() client id: f_00009-8-2 loss: 0.817813  [   96/  118]
train() client id: f_00009-9-0 loss: 0.870326  [   32/  118]
train() client id: f_00009-9-1 loss: 0.866150  [   64/  118]
train() client id: f_00009-9-2 loss: 0.882405  [   96/  118]
train() client id: f_00009-10-0 loss: 0.968747  [   32/  118]
train() client id: f_00009-10-1 loss: 0.889575  [   64/  118]
train() client id: f_00009-10-2 loss: 0.877402  [   96/  118]
train() client id: f_00009-11-0 loss: 0.782846  [   32/  118]
train() client id: f_00009-11-1 loss: 0.817696  [   64/  118]
train() client id: f_00009-11-2 loss: 1.052709  [   96/  118]
train() client id: f_00009-12-0 loss: 0.861415  [   32/  118]
train() client id: f_00009-12-1 loss: 0.886351  [   64/  118]
train() client id: f_00009-12-2 loss: 0.839936  [   96/  118]
At round 43 accuracy: 0.6551724137931034
At round 43 training accuracy: 0.5861837692823608
At round 43 training loss: 0.8353996156332791
update_location
xs = -3.905658 4.200318 235.009024 18.811294 0.979296 3.956410 -197.443192 -176.324852 219.663977 -162.060879 
ys = 227.587959 210.555839 1.320614 -197.455176 189.350187 172.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 248.619254 233.133875 255.403573 222.131518 214.136527 199.700736 221.338439 202.709471 241.993663 190.472413 
dists_bs = 178.993763 182.693655 445.277362 419.863878 176.563418 178.969759 179.037236 174.182691 424.904898 171.487358 
uav_gains = -111.179050 -109.938241 -111.762242 -109.138137 -108.593508 -107.663150 -109.082861 -107.853230 -110.631688 -107.085431 
bs_gains = -102.646520 -102.895316 -113.728716 -113.014097 -102.480280 -102.644889 -102.649473 -102.315199 -113.159228 -102.125558 
Round 44
-------------------------------
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.18624546 10.67895549  5.11317553  1.85111052 12.31468286  5.9249859
  2.28997512  7.27266655  5.37158258  4.80445503]
obj_prev = 60.80783503027712
eta_min = 1.6905775058448652e-18	eta_max = 0.9354514077356543
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 14.082134182864616	eta = 0.9090909090909091
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 27.112785949573404	eta = 0.4721735416659381
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 20.558519671827195	eta = 0.622707294620243
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 19.371299283019138	eta = 0.6608715285021035
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 19.305821021572196	eta = 0.6631129622477986
af = 12.80194016624056	bf = 1.2446435035998953	zeta = 19.305603441902093	eta = 0.6631204357204616
eta = 0.6631204357204616
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [0.03389812 0.07129367 0.03336005 0.01156841 0.08232401 0.03927877
 0.01452777 0.04815685 0.03497425 0.03174586]
ene_total = [1.77121153 3.05330088 1.77028457 0.8430904  3.47724339 1.80411205
 0.95686313 2.2367863  1.88825588 1.50445532]
ti_comp = [0.59818702 0.64270661 0.59304455 0.61337277 0.644077   0.64353969
 0.6137322  0.62114382 0.5788735  0.64520803]
ti_coms = [0.11613574 0.07161615 0.12127821 0.10094999 0.07024576 0.07078307
 0.10059056 0.09317894 0.13544926 0.06911473]
t_total = [27.79981537 27.79981537 27.79981537 27.79981537 27.79981537 27.79981537
 27.79981537 27.79981537 27.79981537 27.79981537]
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [6.80350798e-06 5.48286200e-05 6.59759497e-06 2.57188538e-07
 8.40589437e-05 9.14540875e-06 5.08766715e-07 1.80913087e-05
 7.97917407e-06 4.80332220e-06]
ene_total = [0.45223865 0.28084755 0.47224396 0.39288439 0.2766519  0.27582751
 0.39149537 0.36333529 0.52744819 0.26916572]
optimize_network iter = 0 obj = 3.7021385292782734
eta = 0.6631204357204616
freqs = [28334048.79448884 55463616.087887   28126095.25989743  9430159.49291534
 63908512.8336163  30517755.77420575 11835590.3817379  38764656.20847187
 30208889.06833503 24601258.06337142]
eta_min = 0.6631204357204632	eta_max = 0.6660036163441293
af = 0.006161926359054621	bf = 1.2446435035998953	zeta = 0.006778118994960084	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [1.57849488e-06 1.27208928e-05 1.53072061e-06 5.96708039e-08
 1.95026760e-05 2.12184375e-06 1.18039937e-07 4.19739907e-06
 1.85126341e-06 1.11442796e-06]
ene_total = [1.67509603 1.03465718 1.74925201 1.45587345 1.015872   1.02111426
 1.45069832 1.34439885 1.95366795 0.99690874]
ti_comp = [0.59207349 0.63659309 0.58693102 0.60725925 0.63796347 0.63742617
 0.60761867 0.6150303  0.57275998 0.63909451]
ti_coms = [0.11613574 0.07161615 0.12127821 0.10094999 0.07024576 0.07078307
 0.10059056 0.09317894 0.13544926 0.06911473]
t_total = [27.79981537 27.79981537 27.79981537 27.79981537 27.79981537 27.79981537
 27.79981537 27.79981537 27.79981537 27.79981537]
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [6.72701829e-06 5.41347322e-05 6.52458862e-06 2.54167082e-07
 8.29917378e-05 9.02944383e-06 5.02783784e-07 1.78742687e-05
 7.89490544e-06 4.74218027e-06]
ene_total = [0.45613954 0.28324469 0.47631768 0.39627579 0.27899817 0.278204
 0.39487466 0.36646322 0.53199801 0.27148686]
optimize_network iter = 1 obj = 3.734002631064773
eta = 0.6660036163441293
freqs = [28324287.6160973  55404880.64581718 28118923.39438503  9424501.49550652
 63839529.8841224  30485058.77113441 11828419.4116198  38736518.87654436
 30208889.06833503 24574290.61426852]
eta_min = 0.6660036163441323	eta_max = 0.6660036163441293
af = 0.006150348470560822	bf = 1.2446435035998953	zeta = 0.006765383317616905	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [1.57740747e-06 1.26939645e-05 1.52994007e-06 5.95992217e-08
 1.94605963e-05 2.11729946e-06 1.17896944e-07 4.19130791e-06
 1.85126341e-06 1.11198606e-06]
ene_total = [1.67509587 1.0346533  1.74925189 1.45587344 1.01586593 1.0211136
 1.4506983  1.34439798 1.95366795 0.99690839]
ti_comp = [0.59207349 0.63659309 0.58693102 0.60725925 0.63796347 0.63742617
 0.60761867 0.6150303  0.57275998 0.63909451]
ti_coms = [0.11613574 0.07161615 0.12127821 0.10094999 0.07024576 0.07078307
 0.10059056 0.09317894 0.13544926 0.06911473]
t_total = [27.79981537 27.79981537 27.79981537 27.79981537 27.79981537 27.79981537
 27.79981537 27.79981537 27.79981537 27.79981537]
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [6.72701829e-06 5.41347322e-05 6.52458862e-06 2.54167082e-07
 8.29917378e-05 9.02944383e-06 5.02783784e-07 1.78742687e-05
 7.89490544e-06 4.74218027e-06]
ene_total = [0.45613954 0.28324469 0.47631768 0.39627579 0.27899817 0.278204
 0.39487466 0.36646322 0.53199801 0.27148686]
optimize_network iter = 2 obj = 3.734002631064773
eta = 0.6660036163441293
freqs = [28324287.6160973  55404880.64581718 28118923.39438503  9424501.49550652
 63839529.8841224  30485058.77113441 11828419.4116198  38736518.87654436
 30208889.06833503 24574290.61426852]
Done!
ene_coms = [0.01161357 0.00716161 0.01212782 0.010095   0.00702458 0.00707831
 0.01005906 0.00931789 0.01354493 0.00691147]
ene_comp = [6.57055255e-06 5.28755963e-05 6.37283125e-06 2.48255333e-07
 8.10614081e-05 8.81942529e-06 4.91089385e-07 1.74585257e-05
 7.71127547e-06 4.63188047e-06]
ene_total = [0.01162015 0.00721449 0.01213419 0.01009525 0.00710564 0.00708713
 0.01005955 0.00933535 0.01355264 0.0069161 ]
At round 44 energy consumption: 0.09512048272349369
At round 44 eta: 0.6660036163441293
At round 44 a_n: 13.110585589604415
At round 44 local rounds: 13.309571309784454
At round 44 global rounds: 39.25367528264245
gradient difference: 0.3759942352771759
train() client id: f_00000-0-0 loss: 1.290428  [   32/  126]
train() client id: f_00000-0-1 loss: 1.094786  [   64/  126]
train() client id: f_00000-0-2 loss: 0.917780  [   96/  126]
train() client id: f_00000-1-0 loss: 0.959095  [   32/  126]
train() client id: f_00000-1-1 loss: 1.005967  [   64/  126]
train() client id: f_00000-1-2 loss: 0.970838  [   96/  126]
train() client id: f_00000-2-0 loss: 0.779208  [   32/  126]
train() client id: f_00000-2-1 loss: 1.047112  [   64/  126]
train() client id: f_00000-2-2 loss: 0.810973  [   96/  126]
train() client id: f_00000-3-0 loss: 0.870995  [   32/  126]
train() client id: f_00000-3-1 loss: 0.765390  [   64/  126]
train() client id: f_00000-3-2 loss: 0.914065  [   96/  126]
train() client id: f_00000-4-0 loss: 0.845587  [   32/  126]
train() client id: f_00000-4-1 loss: 0.752602  [   64/  126]
train() client id: f_00000-4-2 loss: 0.705552  [   96/  126]
train() client id: f_00000-5-0 loss: 0.681352  [   32/  126]
train() client id: f_00000-5-1 loss: 0.789543  [   64/  126]
train() client id: f_00000-5-2 loss: 0.710189  [   96/  126]
train() client id: f_00000-6-0 loss: 0.816625  [   32/  126]
train() client id: f_00000-6-1 loss: 0.629812  [   64/  126]
train() client id: f_00000-6-2 loss: 0.766376  [   96/  126]
train() client id: f_00000-7-0 loss: 0.738024  [   32/  126]
train() client id: f_00000-7-1 loss: 0.695605  [   64/  126]
train() client id: f_00000-7-2 loss: 0.697270  [   96/  126]
train() client id: f_00000-8-0 loss: 0.770482  [   32/  126]
train() client id: f_00000-8-1 loss: 0.743693  [   64/  126]
train() client id: f_00000-8-2 loss: 0.711192  [   96/  126]
train() client id: f_00000-9-0 loss: 0.663906  [   32/  126]
train() client id: f_00000-9-1 loss: 0.755533  [   64/  126]
train() client id: f_00000-9-2 loss: 0.691963  [   96/  126]
train() client id: f_00000-10-0 loss: 0.697566  [   32/  126]
train() client id: f_00000-10-1 loss: 0.682413  [   64/  126]
train() client id: f_00000-10-2 loss: 0.726873  [   96/  126]
train() client id: f_00000-11-0 loss: 0.685823  [   32/  126]
train() client id: f_00000-11-1 loss: 0.547460  [   64/  126]
train() client id: f_00000-11-2 loss: 0.719641  [   96/  126]
train() client id: f_00000-12-0 loss: 0.655170  [   32/  126]
train() client id: f_00000-12-1 loss: 0.642600  [   64/  126]
train() client id: f_00000-12-2 loss: 0.679407  [   96/  126]
train() client id: f_00001-0-0 loss: 0.420527  [   32/  265]
train() client id: f_00001-0-1 loss: 0.461907  [   64/  265]
train() client id: f_00001-0-2 loss: 0.413666  [   96/  265]
train() client id: f_00001-0-3 loss: 0.425654  [  128/  265]
train() client id: f_00001-0-4 loss: 0.433917  [  160/  265]
train() client id: f_00001-0-5 loss: 0.397063  [  192/  265]
train() client id: f_00001-0-6 loss: 0.374743  [  224/  265]
train() client id: f_00001-0-7 loss: 0.449374  [  256/  265]
train() client id: f_00001-1-0 loss: 0.461279  [   32/  265]
train() client id: f_00001-1-1 loss: 0.432606  [   64/  265]
train() client id: f_00001-1-2 loss: 0.510603  [   96/  265]
train() client id: f_00001-1-3 loss: 0.482297  [  128/  265]
train() client id: f_00001-1-4 loss: 0.480546  [  160/  265]
train() client id: f_00001-1-5 loss: 0.310095  [  192/  265]
train() client id: f_00001-1-6 loss: 0.344774  [  224/  265]
train() client id: f_00001-1-7 loss: 0.345486  [  256/  265]
train() client id: f_00001-2-0 loss: 0.439269  [   32/  265]
train() client id: f_00001-2-1 loss: 0.433775  [   64/  265]
train() client id: f_00001-2-2 loss: 0.361189  [   96/  265]
train() client id: f_00001-2-3 loss: 0.438115  [  128/  265]
train() client id: f_00001-2-4 loss: 0.425302  [  160/  265]
train() client id: f_00001-2-5 loss: 0.455991  [  192/  265]
train() client id: f_00001-2-6 loss: 0.410220  [  224/  265]
train() client id: f_00001-2-7 loss: 0.329755  [  256/  265]
train() client id: f_00001-3-0 loss: 0.404153  [   32/  265]
train() client id: f_00001-3-1 loss: 0.317529  [   64/  265]
train() client id: f_00001-3-2 loss: 0.479987  [   96/  265]
train() client id: f_00001-3-3 loss: 0.345991  [  128/  265]
train() client id: f_00001-3-4 loss: 0.395214  [  160/  265]
train() client id: f_00001-3-5 loss: 0.326523  [  192/  265]
train() client id: f_00001-3-6 loss: 0.672957  [  224/  265]
train() client id: f_00001-3-7 loss: 0.296858  [  256/  265]
train() client id: f_00001-4-0 loss: 0.451760  [   32/  265]
train() client id: f_00001-4-1 loss: 0.356303  [   64/  265]
train() client id: f_00001-4-2 loss: 0.392819  [   96/  265]
train() client id: f_00001-4-3 loss: 0.417960  [  128/  265]
train() client id: f_00001-4-4 loss: 0.361130  [  160/  265]
train() client id: f_00001-4-5 loss: 0.394719  [  192/  265]
train() client id: f_00001-4-6 loss: 0.470384  [  224/  265]
train() client id: f_00001-4-7 loss: 0.310874  [  256/  265]
train() client id: f_00001-5-0 loss: 0.387581  [   32/  265]
train() client id: f_00001-5-1 loss: 0.329913  [   64/  265]
train() client id: f_00001-5-2 loss: 0.601287  [   96/  265]
train() client id: f_00001-5-3 loss: 0.381880  [  128/  265]
train() client id: f_00001-5-4 loss: 0.414597  [  160/  265]
train() client id: f_00001-5-5 loss: 0.385875  [  192/  265]
train() client id: f_00001-5-6 loss: 0.361341  [  224/  265]
train() client id: f_00001-5-7 loss: 0.304392  [  256/  265]
train() client id: f_00001-6-0 loss: 0.304523  [   32/  265]
train() client id: f_00001-6-1 loss: 0.436908  [   64/  265]
train() client id: f_00001-6-2 loss: 0.427868  [   96/  265]
train() client id: f_00001-6-3 loss: 0.443779  [  128/  265]
train() client id: f_00001-6-4 loss: 0.367093  [  160/  265]
train() client id: f_00001-6-5 loss: 0.410095  [  192/  265]
train() client id: f_00001-6-6 loss: 0.471966  [  224/  265]
train() client id: f_00001-6-7 loss: 0.342972  [  256/  265]
train() client id: f_00001-7-0 loss: 0.325003  [   32/  265]
train() client id: f_00001-7-1 loss: 0.485073  [   64/  265]
train() client id: f_00001-7-2 loss: 0.415227  [   96/  265]
train() client id: f_00001-7-3 loss: 0.386541  [  128/  265]
train() client id: f_00001-7-4 loss: 0.364668  [  160/  265]
train() client id: f_00001-7-5 loss: 0.531881  [  192/  265]
train() client id: f_00001-7-6 loss: 0.301418  [  224/  265]
train() client id: f_00001-7-7 loss: 0.384828  [  256/  265]
train() client id: f_00001-8-0 loss: 0.341442  [   32/  265]
train() client id: f_00001-8-1 loss: 0.329750  [   64/  265]
train() client id: f_00001-8-2 loss: 0.326431  [   96/  265]
train() client id: f_00001-8-3 loss: 0.361024  [  128/  265]
train() client id: f_00001-8-4 loss: 0.375152  [  160/  265]
train() client id: f_00001-8-5 loss: 0.378826  [  192/  265]
train() client id: f_00001-8-6 loss: 0.501166  [  224/  265]
train() client id: f_00001-8-7 loss: 0.423308  [  256/  265]
train() client id: f_00001-9-0 loss: 0.373815  [   32/  265]
train() client id: f_00001-9-1 loss: 0.496336  [   64/  265]
train() client id: f_00001-9-2 loss: 0.345480  [   96/  265]
train() client id: f_00001-9-3 loss: 0.509662  [  128/  265]
train() client id: f_00001-9-4 loss: 0.286838  [  160/  265]
train() client id: f_00001-9-5 loss: 0.476329  [  192/  265]
train() client id: f_00001-9-6 loss: 0.314736  [  224/  265]
train() client id: f_00001-9-7 loss: 0.350107  [  256/  265]
train() client id: f_00001-10-0 loss: 0.443889  [   32/  265]
train() client id: f_00001-10-1 loss: 0.375528  [   64/  265]
train() client id: f_00001-10-2 loss: 0.456918  [   96/  265]
train() client id: f_00001-10-3 loss: 0.397870  [  128/  265]
train() client id: f_00001-10-4 loss: 0.290275  [  160/  265]
train() client id: f_00001-10-5 loss: 0.307675  [  192/  265]
train() client id: f_00001-10-6 loss: 0.442511  [  224/  265]
train() client id: f_00001-10-7 loss: 0.481055  [  256/  265]
train() client id: f_00001-11-0 loss: 0.294781  [   32/  265]
train() client id: f_00001-11-1 loss: 0.356411  [   64/  265]
train() client id: f_00001-11-2 loss: 0.524449  [   96/  265]
train() client id: f_00001-11-3 loss: 0.406352  [  128/  265]
train() client id: f_00001-11-4 loss: 0.458730  [  160/  265]
train() client id: f_00001-11-5 loss: 0.323858  [  192/  265]
train() client id: f_00001-11-6 loss: 0.361496  [  224/  265]
train() client id: f_00001-11-7 loss: 0.454723  [  256/  265]
train() client id: f_00001-12-0 loss: 0.433045  [   32/  265]
train() client id: f_00001-12-1 loss: 0.365154  [   64/  265]
train() client id: f_00001-12-2 loss: 0.399198  [   96/  265]
train() client id: f_00001-12-3 loss: 0.374654  [  128/  265]
train() client id: f_00001-12-4 loss: 0.370270  [  160/  265]
train() client id: f_00001-12-5 loss: 0.502457  [  192/  265]
train() client id: f_00001-12-6 loss: 0.302414  [  224/  265]
train() client id: f_00001-12-7 loss: 0.435112  [  256/  265]
train() client id: f_00002-0-0 loss: 1.250057  [   32/  124]
train() client id: f_00002-0-1 loss: 1.200114  [   64/  124]
train() client id: f_00002-0-2 loss: 1.044304  [   96/  124]
train() client id: f_00002-1-0 loss: 1.104160  [   32/  124]
train() client id: f_00002-1-1 loss: 1.194617  [   64/  124]
train() client id: f_00002-1-2 loss: 1.145834  [   96/  124]
train() client id: f_00002-2-0 loss: 0.954132  [   32/  124]
train() client id: f_00002-2-1 loss: 1.186604  [   64/  124]
train() client id: f_00002-2-2 loss: 1.036702  [   96/  124]
train() client id: f_00002-3-0 loss: 0.972788  [   32/  124]
train() client id: f_00002-3-1 loss: 1.111330  [   64/  124]
train() client id: f_00002-3-2 loss: 1.006606  [   96/  124]
train() client id: f_00002-4-0 loss: 1.082482  [   32/  124]
train() client id: f_00002-4-1 loss: 0.792150  [   64/  124]
train() client id: f_00002-4-2 loss: 1.060830  [   96/  124]
train() client id: f_00002-5-0 loss: 1.010508  [   32/  124]
train() client id: f_00002-5-1 loss: 0.978919  [   64/  124]
train() client id: f_00002-5-2 loss: 0.802669  [   96/  124]
train() client id: f_00002-6-0 loss: 0.984254  [   32/  124]
train() client id: f_00002-6-1 loss: 0.955792  [   64/  124]
train() client id: f_00002-6-2 loss: 0.897823  [   96/  124]
train() client id: f_00002-7-0 loss: 1.065234  [   32/  124]
train() client id: f_00002-7-1 loss: 0.806668  [   64/  124]
train() client id: f_00002-7-2 loss: 0.918812  [   96/  124]
train() client id: f_00002-8-0 loss: 0.813748  [   32/  124]
train() client id: f_00002-8-1 loss: 0.795327  [   64/  124]
train() client id: f_00002-8-2 loss: 0.944864  [   96/  124]
train() client id: f_00002-9-0 loss: 0.808770  [   32/  124]
train() client id: f_00002-9-1 loss: 0.879176  [   64/  124]
train() client id: f_00002-9-2 loss: 1.024692  [   96/  124]
train() client id: f_00002-10-0 loss: 0.837364  [   32/  124]
train() client id: f_00002-10-1 loss: 0.897081  [   64/  124]
train() client id: f_00002-10-2 loss: 0.825244  [   96/  124]
train() client id: f_00002-11-0 loss: 0.856279  [   32/  124]
train() client id: f_00002-11-1 loss: 0.891544  [   64/  124]
train() client id: f_00002-11-2 loss: 0.989718  [   96/  124]
train() client id: f_00002-12-0 loss: 0.782391  [   32/  124]
train() client id: f_00002-12-1 loss: 0.936262  [   64/  124]
train() client id: f_00002-12-2 loss: 0.901630  [   96/  124]
train() client id: f_00003-0-0 loss: 0.737140  [   32/   43]
train() client id: f_00003-1-0 loss: 0.678140  [   32/   43]
train() client id: f_00003-2-0 loss: 0.579363  [   32/   43]
train() client id: f_00003-3-0 loss: 0.717358  [   32/   43]
train() client id: f_00003-4-0 loss: 0.684312  [   32/   43]
train() client id: f_00003-5-0 loss: 0.614442  [   32/   43]
train() client id: f_00003-6-0 loss: 0.706423  [   32/   43]
train() client id: f_00003-7-0 loss: 0.672562  [   32/   43]
train() client id: f_00003-8-0 loss: 0.792803  [   32/   43]
train() client id: f_00003-9-0 loss: 0.865580  [   32/   43]
train() client id: f_00003-10-0 loss: 0.638556  [   32/   43]
train() client id: f_00003-11-0 loss: 0.700955  [   32/   43]
train() client id: f_00003-12-0 loss: 0.528668  [   32/   43]
train() client id: f_00004-0-0 loss: 0.908278  [   32/  306]
train() client id: f_00004-0-1 loss: 0.810663  [   64/  306]
train() client id: f_00004-0-2 loss: 0.861201  [   96/  306]
train() client id: f_00004-0-3 loss: 1.050763  [  128/  306]
train() client id: f_00004-0-4 loss: 0.940722  [  160/  306]
train() client id: f_00004-0-5 loss: 0.989842  [  192/  306]
train() client id: f_00004-0-6 loss: 0.990971  [  224/  306]
train() client id: f_00004-0-7 loss: 0.875628  [  256/  306]
train() client id: f_00004-0-8 loss: 1.029047  [  288/  306]
train() client id: f_00004-1-0 loss: 0.948380  [   32/  306]
train() client id: f_00004-1-1 loss: 0.877057  [   64/  306]
train() client id: f_00004-1-2 loss: 0.948789  [   96/  306]
train() client id: f_00004-1-3 loss: 1.042755  [  128/  306]
train() client id: f_00004-1-4 loss: 0.893285  [  160/  306]
train() client id: f_00004-1-5 loss: 1.023031  [  192/  306]
train() client id: f_00004-1-6 loss: 0.919967  [  224/  306]
train() client id: f_00004-1-7 loss: 0.881899  [  256/  306]
train() client id: f_00004-1-8 loss: 0.883394  [  288/  306]
train() client id: f_00004-2-0 loss: 1.121733  [   32/  306]
train() client id: f_00004-2-1 loss: 0.875682  [   64/  306]
train() client id: f_00004-2-2 loss: 0.959145  [   96/  306]
train() client id: f_00004-2-3 loss: 0.887441  [  128/  306]
train() client id: f_00004-2-4 loss: 0.931786  [  160/  306]
train() client id: f_00004-2-5 loss: 0.814105  [  192/  306]
train() client id: f_00004-2-6 loss: 0.974468  [  224/  306]
train() client id: f_00004-2-7 loss: 0.901309  [  256/  306]
train() client id: f_00004-2-8 loss: 0.939004  [  288/  306]
train() client id: f_00004-3-0 loss: 0.962028  [   32/  306]
train() client id: f_00004-3-1 loss: 1.009339  [   64/  306]
train() client id: f_00004-3-2 loss: 0.930293  [   96/  306]
train() client id: f_00004-3-3 loss: 0.983606  [  128/  306]
train() client id: f_00004-3-4 loss: 0.778884  [  160/  306]
train() client id: f_00004-3-5 loss: 0.943472  [  192/  306]
train() client id: f_00004-3-6 loss: 0.799505  [  224/  306]
train() client id: f_00004-3-7 loss: 0.855545  [  256/  306]
train() client id: f_00004-3-8 loss: 1.160181  [  288/  306]
train() client id: f_00004-4-0 loss: 0.986807  [   32/  306]
train() client id: f_00004-4-1 loss: 0.959396  [   64/  306]
train() client id: f_00004-4-2 loss: 0.974801  [   96/  306]
train() client id: f_00004-4-3 loss: 0.820975  [  128/  306]
train() client id: f_00004-4-4 loss: 1.027759  [  160/  306]
train() client id: f_00004-4-5 loss: 0.825593  [  192/  306]
train() client id: f_00004-4-6 loss: 0.974419  [  224/  306]
train() client id: f_00004-4-7 loss: 1.007270  [  256/  306]
train() client id: f_00004-4-8 loss: 0.821174  [  288/  306]
train() client id: f_00004-5-0 loss: 0.820794  [   32/  306]
train() client id: f_00004-5-1 loss: 0.957860  [   64/  306]
train() client id: f_00004-5-2 loss: 0.860405  [   96/  306]
train() client id: f_00004-5-3 loss: 0.794323  [  128/  306]
train() client id: f_00004-5-4 loss: 0.899497  [  160/  306]
train() client id: f_00004-5-5 loss: 0.931449  [  192/  306]
train() client id: f_00004-5-6 loss: 1.118602  [  224/  306]
train() client id: f_00004-5-7 loss: 0.926083  [  256/  306]
train() client id: f_00004-5-8 loss: 0.928445  [  288/  306]
train() client id: f_00004-6-0 loss: 0.897219  [   32/  306]
train() client id: f_00004-6-1 loss: 0.991771  [   64/  306]
train() client id: f_00004-6-2 loss: 0.838929  [   96/  306]
train() client id: f_00004-6-3 loss: 0.887870  [  128/  306]
train() client id: f_00004-6-4 loss: 0.868570  [  160/  306]
train() client id: f_00004-6-5 loss: 0.954463  [  192/  306]
train() client id: f_00004-6-6 loss: 0.998638  [  224/  306]
train() client id: f_00004-6-7 loss: 0.978582  [  256/  306]
train() client id: f_00004-6-8 loss: 0.967337  [  288/  306]
train() client id: f_00004-7-0 loss: 0.938249  [   32/  306]
train() client id: f_00004-7-1 loss: 0.910886  [   64/  306]
train() client id: f_00004-7-2 loss: 0.958147  [   96/  306]
train() client id: f_00004-7-3 loss: 0.832151  [  128/  306]
train() client id: f_00004-7-4 loss: 0.935076  [  160/  306]
train() client id: f_00004-7-5 loss: 0.810068  [  192/  306]
train() client id: f_00004-7-6 loss: 0.833285  [  224/  306]
train() client id: f_00004-7-7 loss: 1.084872  [  256/  306]
train() client id: f_00004-7-8 loss: 0.974764  [  288/  306]
train() client id: f_00004-8-0 loss: 1.024090  [   32/  306]
train() client id: f_00004-8-1 loss: 0.905085  [   64/  306]
train() client id: f_00004-8-2 loss: 0.852498  [   96/  306]
train() client id: f_00004-8-3 loss: 0.884897  [  128/  306]
train() client id: f_00004-8-4 loss: 0.921860  [  160/  306]
train() client id: f_00004-8-5 loss: 0.937503  [  192/  306]
train() client id: f_00004-8-6 loss: 0.949170  [  224/  306]
train() client id: f_00004-8-7 loss: 0.906609  [  256/  306]
train() client id: f_00004-8-8 loss: 0.845874  [  288/  306]
train() client id: f_00004-9-0 loss: 1.013458  [   32/  306]
train() client id: f_00004-9-1 loss: 0.779854  [   64/  306]
train() client id: f_00004-9-2 loss: 0.935715  [   96/  306]
train() client id: f_00004-9-3 loss: 1.041378  [  128/  306]
train() client id: f_00004-9-4 loss: 0.838474  [  160/  306]
train() client id: f_00004-9-5 loss: 0.931457  [  192/  306]
train() client id: f_00004-9-6 loss: 0.954026  [  224/  306]
train() client id: f_00004-9-7 loss: 0.928054  [  256/  306]
train() client id: f_00004-9-8 loss: 0.824632  [  288/  306]
train() client id: f_00004-10-0 loss: 0.963776  [   32/  306]
train() client id: f_00004-10-1 loss: 0.907430  [   64/  306]
train() client id: f_00004-10-2 loss: 0.933541  [   96/  306]
train() client id: f_00004-10-3 loss: 0.872063  [  128/  306]
train() client id: f_00004-10-4 loss: 0.852179  [  160/  306]
train() client id: f_00004-10-5 loss: 1.049657  [  192/  306]
train() client id: f_00004-10-6 loss: 0.859155  [  224/  306]
train() client id: f_00004-10-7 loss: 0.977764  [  256/  306]
train() client id: f_00004-10-8 loss: 0.883377  [  288/  306]
train() client id: f_00004-11-0 loss: 0.919717  [   32/  306]
train() client id: f_00004-11-1 loss: 0.892204  [   64/  306]
train() client id: f_00004-11-2 loss: 0.876352  [   96/  306]
train() client id: f_00004-11-3 loss: 1.055788  [  128/  306]
train() client id: f_00004-11-4 loss: 0.920160  [  160/  306]
train() client id: f_00004-11-5 loss: 0.877192  [  192/  306]
train() client id: f_00004-11-6 loss: 1.083836  [  224/  306]
train() client id: f_00004-11-7 loss: 0.890485  [  256/  306]
train() client id: f_00004-11-8 loss: 0.806985  [  288/  306]
train() client id: f_00004-12-0 loss: 0.946127  [   32/  306]
train() client id: f_00004-12-1 loss: 0.918081  [   64/  306]
train() client id: f_00004-12-2 loss: 0.994655  [   96/  306]
train() client id: f_00004-12-3 loss: 0.856454  [  128/  306]
train() client id: f_00004-12-4 loss: 1.025671  [  160/  306]
train() client id: f_00004-12-5 loss: 0.980174  [  192/  306]
train() client id: f_00004-12-6 loss: 0.916940  [  224/  306]
train() client id: f_00004-12-7 loss: 0.960330  [  256/  306]
train() client id: f_00004-12-8 loss: 0.821192  [  288/  306]
train() client id: f_00005-0-0 loss: 0.820746  [   32/  146]
train() client id: f_00005-0-1 loss: 0.691968  [   64/  146]
train() client id: f_00005-0-2 loss: 0.823606  [   96/  146]
train() client id: f_00005-0-3 loss: 0.733560  [  128/  146]
train() client id: f_00005-1-0 loss: 0.702123  [   32/  146]
train() client id: f_00005-1-1 loss: 0.640263  [   64/  146]
train() client id: f_00005-1-2 loss: 0.661782  [   96/  146]
train() client id: f_00005-1-3 loss: 0.771894  [  128/  146]
train() client id: f_00005-2-0 loss: 0.698116  [   32/  146]
train() client id: f_00005-2-1 loss: 0.652040  [   64/  146]
train() client id: f_00005-2-2 loss: 0.766656  [   96/  146]
train() client id: f_00005-2-3 loss: 0.673509  [  128/  146]
train() client id: f_00005-3-0 loss: 0.811767  [   32/  146]
train() client id: f_00005-3-1 loss: 0.689737  [   64/  146]
train() client id: f_00005-3-2 loss: 0.700035  [   96/  146]
train() client id: f_00005-3-3 loss: 0.657203  [  128/  146]
train() client id: f_00005-4-0 loss: 0.691509  [   32/  146]
train() client id: f_00005-4-1 loss: 0.701707  [   64/  146]
train() client id: f_00005-4-2 loss: 0.522531  [   96/  146]
train() client id: f_00005-4-3 loss: 0.876474  [  128/  146]
train() client id: f_00005-5-0 loss: 0.855136  [   32/  146]
train() client id: f_00005-5-1 loss: 0.707933  [   64/  146]
train() client id: f_00005-5-2 loss: 0.782792  [   96/  146]
train() client id: f_00005-5-3 loss: 0.432588  [  128/  146]
train() client id: f_00005-6-0 loss: 0.633798  [   32/  146]
train() client id: f_00005-6-1 loss: 0.770367  [   64/  146]
train() client id: f_00005-6-2 loss: 0.859188  [   96/  146]
train() client id: f_00005-6-3 loss: 0.746189  [  128/  146]
train() client id: f_00005-7-0 loss: 0.673584  [   32/  146]
train() client id: f_00005-7-1 loss: 1.119026  [   64/  146]
train() client id: f_00005-7-2 loss: 0.486557  [   96/  146]
train() client id: f_00005-7-3 loss: 0.767355  [  128/  146]
train() client id: f_00005-8-0 loss: 0.815006  [   32/  146]
train() client id: f_00005-8-1 loss: 0.416088  [   64/  146]
train() client id: f_00005-8-2 loss: 0.667774  [   96/  146]
train() client id: f_00005-8-3 loss: 1.083672  [  128/  146]
train() client id: f_00005-9-0 loss: 0.952954  [   32/  146]
train() client id: f_00005-9-1 loss: 0.527475  [   64/  146]
train() client id: f_00005-9-2 loss: 0.853909  [   96/  146]
train() client id: f_00005-9-3 loss: 0.580073  [  128/  146]
train() client id: f_00005-10-0 loss: 0.528375  [   32/  146]
train() client id: f_00005-10-1 loss: 0.420943  [   64/  146]
train() client id: f_00005-10-2 loss: 1.075558  [   96/  146]
train() client id: f_00005-10-3 loss: 0.959041  [  128/  146]
train() client id: f_00005-11-0 loss: 0.716919  [   32/  146]
train() client id: f_00005-11-1 loss: 0.678672  [   64/  146]
train() client id: f_00005-11-2 loss: 0.843118  [   96/  146]
train() client id: f_00005-11-3 loss: 0.653205  [  128/  146]
train() client id: f_00005-12-0 loss: 0.839718  [   32/  146]
train() client id: f_00005-12-1 loss: 0.790072  [   64/  146]
train() client id: f_00005-12-2 loss: 0.591669  [   96/  146]
train() client id: f_00005-12-3 loss: 0.632848  [  128/  146]
train() client id: f_00006-0-0 loss: 0.545919  [   32/   54]
train() client id: f_00006-1-0 loss: 0.552273  [   32/   54]
train() client id: f_00006-2-0 loss: 0.576771  [   32/   54]
train() client id: f_00006-3-0 loss: 0.515654  [   32/   54]
train() client id: f_00006-4-0 loss: 0.571611  [   32/   54]
train() client id: f_00006-5-0 loss: 0.529026  [   32/   54]
train() client id: f_00006-6-0 loss: 0.594194  [   32/   54]
train() client id: f_00006-7-0 loss: 0.536484  [   32/   54]
train() client id: f_00006-8-0 loss: 0.589570  [   32/   54]
train() client id: f_00006-9-0 loss: 0.539795  [   32/   54]
train() client id: f_00006-10-0 loss: 0.520656  [   32/   54]
train() client id: f_00006-11-0 loss: 0.574436  [   32/   54]
train() client id: f_00006-12-0 loss: 0.515199  [   32/   54]
train() client id: f_00007-0-0 loss: 0.672337  [   32/  179]
train() client id: f_00007-0-1 loss: 0.825118  [   64/  179]
train() client id: f_00007-0-2 loss: 0.598266  [   96/  179]
train() client id: f_00007-0-3 loss: 0.496758  [  128/  179]
train() client id: f_00007-0-4 loss: 0.547101  [  160/  179]
train() client id: f_00007-1-0 loss: 0.546289  [   32/  179]
train() client id: f_00007-1-1 loss: 0.594758  [   64/  179]
train() client id: f_00007-1-2 loss: 0.471787  [   96/  179]
train() client id: f_00007-1-3 loss: 0.855135  [  128/  179]
train() client id: f_00007-1-4 loss: 0.543756  [  160/  179]
train() client id: f_00007-2-0 loss: 0.624706  [   32/  179]
train() client id: f_00007-2-1 loss: 0.703771  [   64/  179]
train() client id: f_00007-2-2 loss: 0.545569  [   96/  179]
train() client id: f_00007-2-3 loss: 0.516443  [  128/  179]
train() client id: f_00007-2-4 loss: 0.521671  [  160/  179]
train() client id: f_00007-3-0 loss: 0.598986  [   32/  179]
train() client id: f_00007-3-1 loss: 0.693511  [   64/  179]
train() client id: f_00007-3-2 loss: 0.520896  [   96/  179]
train() client id: f_00007-3-3 loss: 0.547851  [  128/  179]
train() client id: f_00007-3-4 loss: 0.604064  [  160/  179]
train() client id: f_00007-4-0 loss: 0.508464  [   32/  179]
train() client id: f_00007-4-1 loss: 0.668281  [   64/  179]
train() client id: f_00007-4-2 loss: 0.450667  [   96/  179]
train() client id: f_00007-4-3 loss: 0.457383  [  128/  179]
train() client id: f_00007-4-4 loss: 0.609155  [  160/  179]
train() client id: f_00007-5-0 loss: 0.417437  [   32/  179]
train() client id: f_00007-5-1 loss: 0.522566  [   64/  179]
train() client id: f_00007-5-2 loss: 0.506926  [   96/  179]
train() client id: f_00007-5-3 loss: 0.503490  [  128/  179]
train() client id: f_00007-5-4 loss: 0.773257  [  160/  179]
train() client id: f_00007-6-0 loss: 0.600914  [   32/  179]
train() client id: f_00007-6-1 loss: 0.595250  [   64/  179]
train() client id: f_00007-6-2 loss: 0.531674  [   96/  179]
train() client id: f_00007-6-3 loss: 0.755586  [  128/  179]
train() client id: f_00007-6-4 loss: 0.429972  [  160/  179]
train() client id: f_00007-7-0 loss: 0.430025  [   32/  179]
train() client id: f_00007-7-1 loss: 0.529018  [   64/  179]
train() client id: f_00007-7-2 loss: 0.717455  [   96/  179]
train() client id: f_00007-7-3 loss: 0.631176  [  128/  179]
train() client id: f_00007-7-4 loss: 0.590806  [  160/  179]
train() client id: f_00007-8-0 loss: 0.482127  [   32/  179]
train() client id: f_00007-8-1 loss: 0.667897  [   64/  179]
train() client id: f_00007-8-2 loss: 0.418100  [   96/  179]
train() client id: f_00007-8-3 loss: 0.548981  [  128/  179]
train() client id: f_00007-8-4 loss: 0.787116  [  160/  179]
train() client id: f_00007-9-0 loss: 0.604192  [   32/  179]
train() client id: f_00007-9-1 loss: 0.511964  [   64/  179]
train() client id: f_00007-9-2 loss: 0.721454  [   96/  179]
train() client id: f_00007-9-3 loss: 0.601622  [  128/  179]
train() client id: f_00007-9-4 loss: 0.450364  [  160/  179]
train() client id: f_00007-10-0 loss: 0.540635  [   32/  179]
train() client id: f_00007-10-1 loss: 0.761352  [   64/  179]
train() client id: f_00007-10-2 loss: 0.516461  [   96/  179]
train() client id: f_00007-10-3 loss: 0.487984  [  128/  179]
train() client id: f_00007-10-4 loss: 0.592417  [  160/  179]
train() client id: f_00007-11-0 loss: 0.400981  [   32/  179]
train() client id: f_00007-11-1 loss: 0.510003  [   64/  179]
train() client id: f_00007-11-2 loss: 0.409416  [   96/  179]
train() client id: f_00007-11-3 loss: 0.609851  [  128/  179]
train() client id: f_00007-11-4 loss: 0.950617  [  160/  179]
train() client id: f_00007-12-0 loss: 0.502496  [   32/  179]
train() client id: f_00007-12-1 loss: 0.696619  [   64/  179]
train() client id: f_00007-12-2 loss: 0.782208  [   96/  179]
train() client id: f_00007-12-3 loss: 0.511477  [  128/  179]
train() client id: f_00007-12-4 loss: 0.427786  [  160/  179]
train() client id: f_00008-0-0 loss: 0.739318  [   32/  130]
train() client id: f_00008-0-1 loss: 0.676979  [   64/  130]
train() client id: f_00008-0-2 loss: 0.607638  [   96/  130]
train() client id: f_00008-0-3 loss: 0.651330  [  128/  130]
train() client id: f_00008-1-0 loss: 0.687290  [   32/  130]
train() client id: f_00008-1-1 loss: 0.662186  [   64/  130]
train() client id: f_00008-1-2 loss: 0.701263  [   96/  130]
train() client id: f_00008-1-3 loss: 0.701495  [  128/  130]
train() client id: f_00008-2-0 loss: 0.687666  [   32/  130]
train() client id: f_00008-2-1 loss: 0.718188  [   64/  130]
train() client id: f_00008-2-2 loss: 0.651616  [   96/  130]
train() client id: f_00008-2-3 loss: 0.710050  [  128/  130]
train() client id: f_00008-3-0 loss: 0.655446  [   32/  130]
train() client id: f_00008-3-1 loss: 0.626994  [   64/  130]
train() client id: f_00008-3-2 loss: 0.699282  [   96/  130]
train() client id: f_00008-3-3 loss: 0.778241  [  128/  130]
train() client id: f_00008-4-0 loss: 0.645835  [   32/  130]
train() client id: f_00008-4-1 loss: 0.646891  [   64/  130]
train() client id: f_00008-4-2 loss: 0.811450  [   96/  130]
train() client id: f_00008-4-3 loss: 0.623364  [  128/  130]
train() client id: f_00008-5-0 loss: 0.786357  [   32/  130]
train() client id: f_00008-5-1 loss: 0.623665  [   64/  130]
train() client id: f_00008-5-2 loss: 0.681125  [   96/  130]
train() client id: f_00008-5-3 loss: 0.668083  [  128/  130]
train() client id: f_00008-6-0 loss: 0.668004  [   32/  130]
train() client id: f_00008-6-1 loss: 0.679430  [   64/  130]
train() client id: f_00008-6-2 loss: 0.670169  [   96/  130]
train() client id: f_00008-6-3 loss: 0.740753  [  128/  130]
train() client id: f_00008-7-0 loss: 0.721923  [   32/  130]
train() client id: f_00008-7-1 loss: 0.665742  [   64/  130]
train() client id: f_00008-7-2 loss: 0.694806  [   96/  130]
train() client id: f_00008-7-3 loss: 0.640821  [  128/  130]
train() client id: f_00008-8-0 loss: 0.668061  [   32/  130]
train() client id: f_00008-8-1 loss: 0.691784  [   64/  130]
train() client id: f_00008-8-2 loss: 0.609204  [   96/  130]
train() client id: f_00008-8-3 loss: 0.788007  [  128/  130]
train() client id: f_00008-9-0 loss: 0.668770  [   32/  130]
train() client id: f_00008-9-1 loss: 0.681713  [   64/  130]
train() client id: f_00008-9-2 loss: 0.736490  [   96/  130]
train() client id: f_00008-9-3 loss: 0.642009  [  128/  130]
train() client id: f_00008-10-0 loss: 0.778901  [   32/  130]
train() client id: f_00008-10-1 loss: 0.664925  [   64/  130]
train() client id: f_00008-10-2 loss: 0.697204  [   96/  130]
train() client id: f_00008-10-3 loss: 0.610361  [  128/  130]
train() client id: f_00008-11-0 loss: 0.610650  [   32/  130]
train() client id: f_00008-11-1 loss: 0.730946  [   64/  130]
train() client id: f_00008-11-2 loss: 0.736376  [   96/  130]
train() client id: f_00008-11-3 loss: 0.648016  [  128/  130]
train() client id: f_00008-12-0 loss: 0.773757  [   32/  130]
train() client id: f_00008-12-1 loss: 0.629708  [   64/  130]
train() client id: f_00008-12-2 loss: 0.587971  [   96/  130]
train() client id: f_00008-12-3 loss: 0.757932  [  128/  130]
train() client id: f_00009-0-0 loss: 1.232875  [   32/  118]
train() client id: f_00009-0-1 loss: 1.143355  [   64/  118]
train() client id: f_00009-0-2 loss: 1.073317  [   96/  118]
train() client id: f_00009-1-0 loss: 0.994529  [   32/  118]
train() client id: f_00009-1-1 loss: 1.141412  [   64/  118]
train() client id: f_00009-1-2 loss: 1.102196  [   96/  118]
train() client id: f_00009-2-0 loss: 1.025571  [   32/  118]
train() client id: f_00009-2-1 loss: 1.007517  [   64/  118]
train() client id: f_00009-2-2 loss: 1.106532  [   96/  118]
train() client id: f_00009-3-0 loss: 0.969936  [   32/  118]
train() client id: f_00009-3-1 loss: 0.906490  [   64/  118]
train() client id: f_00009-3-2 loss: 1.255254  [   96/  118]
train() client id: f_00009-4-0 loss: 0.869120  [   32/  118]
train() client id: f_00009-4-1 loss: 0.872903  [   64/  118]
train() client id: f_00009-4-2 loss: 1.077768  [   96/  118]
train() client id: f_00009-5-0 loss: 0.957301  [   32/  118]
train() client id: f_00009-5-1 loss: 1.046747  [   64/  118]
train() client id: f_00009-5-2 loss: 0.969663  [   96/  118]
train() client id: f_00009-6-0 loss: 1.022819  [   32/  118]
train() client id: f_00009-6-1 loss: 0.947659  [   64/  118]
train() client id: f_00009-6-2 loss: 0.834171  [   96/  118]
train() client id: f_00009-7-0 loss: 1.042699  [   32/  118]
train() client id: f_00009-7-1 loss: 0.920129  [   64/  118]
train() client id: f_00009-7-2 loss: 0.865791  [   96/  118]
train() client id: f_00009-8-0 loss: 0.773212  [   32/  118]
train() client id: f_00009-8-1 loss: 0.979033  [   64/  118]
train() client id: f_00009-8-2 loss: 0.962584  [   96/  118]
train() client id: f_00009-9-0 loss: 0.985089  [   32/  118]
train() client id: f_00009-9-1 loss: 0.897158  [   64/  118]
train() client id: f_00009-9-2 loss: 0.874329  [   96/  118]
train() client id: f_00009-10-0 loss: 1.028420  [   32/  118]
train() client id: f_00009-10-1 loss: 0.846992  [   64/  118]
train() client id: f_00009-10-2 loss: 0.789101  [   96/  118]
train() client id: f_00009-11-0 loss: 0.994178  [   32/  118]
train() client id: f_00009-11-1 loss: 0.880445  [   64/  118]
train() client id: f_00009-11-2 loss: 0.830750  [   96/  118]
train() client id: f_00009-12-0 loss: 0.770683  [   32/  118]
train() client id: f_00009-12-1 loss: 0.957355  [   64/  118]
train() client id: f_00009-12-2 loss: 0.889911  [   96/  118]
At round 44 accuracy: 0.649867374005305
At round 44 training accuracy: 0.5902079141515761
At round 44 training loss: 0.8225986524054681
update_location
xs = -3.905658 4.200318 240.009024 18.811294 0.979296 3.956410 -202.443192 -181.324852 224.663977 -167.060879 
ys = 232.587959 215.555839 1.320614 -202.455176 194.350187 177.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 253.204291 237.659341 260.011684 226.587650 218.570250 204.042950 225.809957 207.073364 246.541219 194.744317 
dists_bs = 180.526028 183.732224 449.885562 424.305583 177.039946 178.978535 179.732479 174.292450 429.553038 171.182718 
uav_gains = -111.570923 -110.286811 -112.169118 -109.454464 -108.892183 -107.937957 -109.398532 -108.131876 -111.004853 -107.352318 
bs_gains = -102.750174 -102.964248 -113.853917 -113.142064 -102.513055 -102.645486 -102.696603 -102.322859 -113.291529 -102.103937 
Round 45
-------------------------------
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.05514024 10.40024199  4.98453475  1.80545975 11.99306906  5.77018381
  2.2328339   7.08469715  5.23316744  4.67883957]
obj_prev = 59.2381676466099
eta_min = 5.885406011502381e-19	eta_max = 0.9363452566333951
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 13.714204272500444	eta = 0.9090909090909091
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 26.6122320197388	eta = 0.4684860112559707
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 20.101928982645475	eta = 0.6202120423522212
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 18.922786406509566	eta = 0.6588595443458035
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 18.857340066002127	eta = 0.6611461842396013
af = 12.467458429545857	bf = 1.2310181153532935	zeta = 18.857119499819667	eta = 0.6611539174721296
eta = 0.6611539174721296
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [0.03414319 0.0718091  0.03360124 0.01165204 0.08291919 0.03956275
 0.0146328  0.04850501 0.0352271  0.03197537]
ene_total = [1.73700778 2.97656775 1.73758614 0.82737842 3.38956642 1.75748632
 0.93827146 2.18480132 1.84341633 1.46503756]
ti_comp = [0.61689568 0.66459269 0.61132264 0.63338719 0.66608943 0.66565651
 0.63376527 0.64167568 0.59949544 0.66739459]
ti_coms = [0.11954586 0.07184885 0.1251189  0.10305434 0.0703521  0.07078503
 0.10267627 0.09476586 0.1369461  0.06904695]
t_total = [27.74981117 27.74981117 27.74981117 27.74981117 27.74981117 27.74981117
 27.74981117 27.74981117 27.74981117 27.74981117]
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [6.53685718e-06 5.23971089e-05 6.34460979e-06 2.46460673e-07
 8.03119208e-05 8.73451965e-06 4.87534362e-07 1.73224277e-05
 7.60219691e-06 4.58734577e-06]
ene_total = [0.45070637 0.27270761 0.47169887 0.38832777 0.26811959 0.26705379
 0.38691223 0.35773942 0.51631227 0.26034825]
optimize_network iter = 0 obj = 3.639926174690373
eta = 0.6611539174721296
freqs = [27673393.660334   54024893.67528107 27482409.80128485  9198198.6140707
 62243282.60071961 29717090.05574555 11544335.26103987 37795583.8923452
 29380628.13581736 23955372.57683051]
eta_min = 0.6611539174721324	eta_max = 0.6721599416563945
af = 0.005695477061159853	bf = 1.2310181153532935	zeta = 0.006265024767275839	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [1.50574262e-06 1.20694943e-05 1.46145910e-06 5.67713705e-08
 1.84995754e-05 2.01196663e-06 1.12301867e-07 3.99016180e-06
 1.75113998e-06 1.05667936e-06]
ene_total = [1.67921435 1.010801   1.75748063 1.44739012 0.99068256 0.99444728
 1.44208793 1.33153211 1.92363253 0.96990197]
ti_comp = [0.5929754  0.64067241 0.58740236 0.60946691 0.64216915 0.64173622
 0.60984499 0.6177554  0.57557516 0.64347431]
ti_coms = [0.11954586 0.07184885 0.1251189  0.10305434 0.0703521  0.07078503
 0.10267627 0.09476586 0.1369461  0.06904695]
t_total = [27.74981117 27.74981117 27.74981117 27.74981117 27.74981117 27.74981117
 27.74981117 27.74981117 27.74981117 27.74981117]
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [6.26134418e-06 4.98993487e-05 6.08167179e-06 2.35577818e-07
 7.64706350e-05 8.31715417e-06 4.65984701e-07 1.65407536e-05
 7.29886402e-06 4.36729864e-06]
ene_total = [0.46582645 0.28176549 0.48752418 0.40136402 0.27697112 0.27600289
 0.39990055 0.36971876 0.53363374 0.26907992]
optimize_network iter = 1 obj = 3.761787113624325
eta = 0.6721599416563945
freqs = [27640992.43136498 53805865.01782712 27460330.69138759  9177789.36033805
 61985734.40435877 29594844.6264841  11518450.63917791 37692613.12413883
 29380628.13581735 23854513.15446249]
eta_min = 0.6721599416563961	eta_max = 0.6721599416563635
af = 0.0056547282477792775	bf = 1.2310181153532935	zeta = 0.006220201072557206	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [1.50221871e-06 1.19718279e-05 1.45911179e-06 5.65197178e-08
 1.83467983e-05 1.99544766e-06 1.11798827e-07 3.96844972e-06
 1.75113998e-06 1.04780021e-06]
ene_total = [1.67921385 1.01078729 1.7574803  1.44739009 0.9906611  0.99444496
 1.44208786 1.33152906 1.92363253 0.96990072]
ti_comp = [0.5929754  0.64067241 0.58740236 0.60946691 0.64216915 0.64173622
 0.60984499 0.6177554  0.57557516 0.64347431]
ti_coms = [0.11954586 0.07184885 0.1251189  0.10305434 0.0703521  0.07078503
 0.10267627 0.09476586 0.1369461  0.06904695]
t_total = [27.74981117 27.74981117 27.74981117 27.74981117 27.74981117 27.74981117
 27.74981117 27.74981117 27.74981117 27.74981117]
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [6.26134418e-06 4.98993487e-05 6.08167179e-06 2.35577818e-07
 7.64706350e-05 8.31715417e-06 4.65984701e-07 1.65407536e-05
 7.29886402e-06 4.36729864e-06]
ene_total = [0.46582645 0.28176549 0.48752418 0.40136402 0.27697112 0.27600289
 0.39990055 0.36971876 0.53363374 0.26907992]
optimize_network iter = 2 obj = 3.761787113623971
eta = 0.6721599416563635
freqs = [27640992.43136505 53805865.01782771 27460330.69138763  9177789.3603381
 61985734.40435947 29594844.62648444 11518450.63917798 37692613.1241391
 29380628.13581733 23854513.15446277]
Done!
ene_coms = [0.01195459 0.00718488 0.01251189 0.01030543 0.00703521 0.0070785
 0.01026763 0.00947659 0.01369461 0.00690469]
ene_comp = [6.25736035e-06 4.98675999e-05 6.07780228e-06 2.35427930e-07
 7.64219800e-05 8.31186232e-06 4.65688215e-07 1.65302294e-05
 7.29422007e-06 4.36451991e-06]
ene_total = [0.01196084 0.00723475 0.01251797 0.01030567 0.00711163 0.00708682
 0.01026809 0.00949312 0.0137019  0.00690906]
At round 45 energy consumption: 0.09658985266196866
At round 45 eta: 0.6721599416563635
At round 45 a_n: 12.768039742635098
At round 45 local rounds: 13.008276614299735
At round 45 global rounds: 38.94594152753551
gradient difference: 0.48644647002220154
train() client id: f_00000-0-0 loss: 1.052011  [   32/  126]
train() client id: f_00000-0-1 loss: 1.013757  [   64/  126]
train() client id: f_00000-0-2 loss: 0.819310  [   96/  126]
train() client id: f_00000-1-0 loss: 1.090055  [   32/  126]
train() client id: f_00000-1-1 loss: 0.717254  [   64/  126]
train() client id: f_00000-1-2 loss: 0.834181  [   96/  126]
train() client id: f_00000-2-0 loss: 0.720608  [   32/  126]
train() client id: f_00000-2-1 loss: 1.181532  [   64/  126]
train() client id: f_00000-2-2 loss: 0.817543  [   96/  126]
train() client id: f_00000-3-0 loss: 0.801504  [   32/  126]
train() client id: f_00000-3-1 loss: 0.920410  [   64/  126]
train() client id: f_00000-3-2 loss: 0.836597  [   96/  126]
train() client id: f_00000-4-0 loss: 0.750115  [   32/  126]
train() client id: f_00000-4-1 loss: 0.859757  [   64/  126]
train() client id: f_00000-4-2 loss: 0.882303  [   96/  126]
train() client id: f_00000-5-0 loss: 0.708990  [   32/  126]
train() client id: f_00000-5-1 loss: 0.758123  [   64/  126]
train() client id: f_00000-5-2 loss: 0.920726  [   96/  126]
train() client id: f_00000-6-0 loss: 0.837106  [   32/  126]
train() client id: f_00000-6-1 loss: 0.649920  [   64/  126]
train() client id: f_00000-6-2 loss: 0.815963  [   96/  126]
train() client id: f_00000-7-0 loss: 0.820336  [   32/  126]
train() client id: f_00000-7-1 loss: 0.745445  [   64/  126]
train() client id: f_00000-7-2 loss: 0.705278  [   96/  126]
train() client id: f_00000-8-0 loss: 0.816193  [   32/  126]
train() client id: f_00000-8-1 loss: 0.847493  [   64/  126]
train() client id: f_00000-8-2 loss: 0.706555  [   96/  126]
train() client id: f_00000-9-0 loss: 0.629243  [   32/  126]
train() client id: f_00000-9-1 loss: 0.798018  [   64/  126]
train() client id: f_00000-9-2 loss: 0.846889  [   96/  126]
train() client id: f_00000-10-0 loss: 0.717107  [   32/  126]
train() client id: f_00000-10-1 loss: 0.832464  [   64/  126]
train() client id: f_00000-10-2 loss: 0.829417  [   96/  126]
train() client id: f_00000-11-0 loss: 0.786511  [   32/  126]
train() client id: f_00000-11-1 loss: 0.819920  [   64/  126]
train() client id: f_00000-11-2 loss: 0.769310  [   96/  126]
train() client id: f_00000-12-0 loss: 0.719085  [   32/  126]
train() client id: f_00000-12-1 loss: 0.757223  [   64/  126]
train() client id: f_00000-12-2 loss: 0.930373  [   96/  126]
train() client id: f_00001-0-0 loss: 0.370441  [   32/  265]
train() client id: f_00001-0-1 loss: 0.435336  [   64/  265]
train() client id: f_00001-0-2 loss: 0.414407  [   96/  265]
train() client id: f_00001-0-3 loss: 0.374052  [  128/  265]
train() client id: f_00001-0-4 loss: 0.310978  [  160/  265]
train() client id: f_00001-0-5 loss: 0.501954  [  192/  265]
train() client id: f_00001-0-6 loss: 0.340922  [  224/  265]
train() client id: f_00001-0-7 loss: 0.546150  [  256/  265]
train() client id: f_00001-1-0 loss: 0.403286  [   32/  265]
train() client id: f_00001-1-1 loss: 0.426320  [   64/  265]
train() client id: f_00001-1-2 loss: 0.364445  [   96/  265]
train() client id: f_00001-1-3 loss: 0.494224  [  128/  265]
train() client id: f_00001-1-4 loss: 0.329153  [  160/  265]
train() client id: f_00001-1-5 loss: 0.346057  [  192/  265]
train() client id: f_00001-1-6 loss: 0.461029  [  224/  265]
train() client id: f_00001-1-7 loss: 0.419894  [  256/  265]
train() client id: f_00001-2-0 loss: 0.624373  [   32/  265]
train() client id: f_00001-2-1 loss: 0.388041  [   64/  265]
train() client id: f_00001-2-2 loss: 0.414188  [   96/  265]
train() client id: f_00001-2-3 loss: 0.302158  [  128/  265]
train() client id: f_00001-2-4 loss: 0.325857  [  160/  265]
train() client id: f_00001-2-5 loss: 0.462383  [  192/  265]
train() client id: f_00001-2-6 loss: 0.336239  [  224/  265]
train() client id: f_00001-2-7 loss: 0.390181  [  256/  265]
train() client id: f_00001-3-0 loss: 0.370851  [   32/  265]
train() client id: f_00001-3-1 loss: 0.424869  [   64/  265]
train() client id: f_00001-3-2 loss: 0.401750  [   96/  265]
train() client id: f_00001-3-3 loss: 0.327316  [  128/  265]
train() client id: f_00001-3-4 loss: 0.398797  [  160/  265]
train() client id: f_00001-3-5 loss: 0.360184  [  192/  265]
train() client id: f_00001-3-6 loss: 0.435919  [  224/  265]
train() client id: f_00001-3-7 loss: 0.469280  [  256/  265]
train() client id: f_00001-4-0 loss: 0.349499  [   32/  265]
train() client id: f_00001-4-1 loss: 0.301298  [   64/  265]
train() client id: f_00001-4-2 loss: 0.457194  [   96/  265]
train() client id: f_00001-4-3 loss: 0.336167  [  128/  265]
train() client id: f_00001-4-4 loss: 0.524473  [  160/  265]
train() client id: f_00001-4-5 loss: 0.385419  [  192/  265]
train() client id: f_00001-4-6 loss: 0.401846  [  224/  265]
train() client id: f_00001-4-7 loss: 0.388229  [  256/  265]
train() client id: f_00001-5-0 loss: 0.566015  [   32/  265]
train() client id: f_00001-5-1 loss: 0.374403  [   64/  265]
train() client id: f_00001-5-2 loss: 0.319274  [   96/  265]
train() client id: f_00001-5-3 loss: 0.352979  [  128/  265]
train() client id: f_00001-5-4 loss: 0.343479  [  160/  265]
train() client id: f_00001-5-5 loss: 0.504305  [  192/  265]
train() client id: f_00001-5-6 loss: 0.355226  [  224/  265]
train() client id: f_00001-5-7 loss: 0.319478  [  256/  265]
train() client id: f_00001-6-0 loss: 0.364470  [   32/  265]
train() client id: f_00001-6-1 loss: 0.285803  [   64/  265]
train() client id: f_00001-6-2 loss: 0.374550  [   96/  265]
train() client id: f_00001-6-3 loss: 0.374967  [  128/  265]
train() client id: f_00001-6-4 loss: 0.383750  [  160/  265]
train() client id: f_00001-6-5 loss: 0.462449  [  192/  265]
train() client id: f_00001-6-6 loss: 0.577948  [  224/  265]
train() client id: f_00001-6-7 loss: 0.289397  [  256/  265]
train() client id: f_00001-7-0 loss: 0.422617  [   32/  265]
train() client id: f_00001-7-1 loss: 0.331792  [   64/  265]
train() client id: f_00001-7-2 loss: 0.382488  [   96/  265]
train() client id: f_00001-7-3 loss: 0.418826  [  128/  265]
train() client id: f_00001-7-4 loss: 0.433581  [  160/  265]
train() client id: f_00001-7-5 loss: 0.354938  [  192/  265]
train() client id: f_00001-7-6 loss: 0.464759  [  224/  265]
train() client id: f_00001-7-7 loss: 0.295005  [  256/  265]
train() client id: f_00001-8-0 loss: 0.345569  [   32/  265]
train() client id: f_00001-8-1 loss: 0.411556  [   64/  265]
train() client id: f_00001-8-2 loss: 0.344972  [   96/  265]
train() client id: f_00001-8-3 loss: 0.287845  [  128/  265]
train() client id: f_00001-8-4 loss: 0.453222  [  160/  265]
train() client id: f_00001-8-5 loss: 0.346255  [  192/  265]
train() client id: f_00001-8-6 loss: 0.473805  [  224/  265]
train() client id: f_00001-8-7 loss: 0.368645  [  256/  265]
train() client id: f_00001-9-0 loss: 0.281076  [   32/  265]
train() client id: f_00001-9-1 loss: 0.292683  [   64/  265]
train() client id: f_00001-9-2 loss: 0.366014  [   96/  265]
train() client id: f_00001-9-3 loss: 0.363010  [  128/  265]
train() client id: f_00001-9-4 loss: 0.516033  [  160/  265]
train() client id: f_00001-9-5 loss: 0.493794  [  192/  265]
train() client id: f_00001-9-6 loss: 0.485430  [  224/  265]
train() client id: f_00001-9-7 loss: 0.278827  [  256/  265]
train() client id: f_00001-10-0 loss: 0.290520  [   32/  265]
train() client id: f_00001-10-1 loss: 0.471812  [   64/  265]
train() client id: f_00001-10-2 loss: 0.376595  [   96/  265]
train() client id: f_00001-10-3 loss: 0.431746  [  128/  265]
train() client id: f_00001-10-4 loss: 0.375011  [  160/  265]
train() client id: f_00001-10-5 loss: 0.439020  [  192/  265]
train() client id: f_00001-10-6 loss: 0.344631  [  224/  265]
train() client id: f_00001-10-7 loss: 0.351824  [  256/  265]
train() client id: f_00001-11-0 loss: 0.356554  [   32/  265]
train() client id: f_00001-11-1 loss: 0.312874  [   64/  265]
train() client id: f_00001-11-2 loss: 0.563132  [   96/  265]
train() client id: f_00001-11-3 loss: 0.377880  [  128/  265]
train() client id: f_00001-11-4 loss: 0.290413  [  160/  265]
train() client id: f_00001-11-5 loss: 0.509888  [  192/  265]
train() client id: f_00001-11-6 loss: 0.275700  [  224/  265]
train() client id: f_00001-11-7 loss: 0.330845  [  256/  265]
train() client id: f_00001-12-0 loss: 0.399622  [   32/  265]
train() client id: f_00001-12-1 loss: 0.280715  [   64/  265]
train() client id: f_00001-12-2 loss: 0.343780  [   96/  265]
train() client id: f_00001-12-3 loss: 0.462856  [  128/  265]
train() client id: f_00001-12-4 loss: 0.385793  [  160/  265]
train() client id: f_00001-12-5 loss: 0.525263  [  192/  265]
train() client id: f_00001-12-6 loss: 0.297885  [  224/  265]
train() client id: f_00001-12-7 loss: 0.379872  [  256/  265]
train() client id: f_00002-0-0 loss: 1.183998  [   32/  124]
train() client id: f_00002-0-1 loss: 0.981809  [   64/  124]
train() client id: f_00002-0-2 loss: 1.026339  [   96/  124]
train() client id: f_00002-1-0 loss: 0.893921  [   32/  124]
train() client id: f_00002-1-1 loss: 0.998716  [   64/  124]
train() client id: f_00002-1-2 loss: 1.229558  [   96/  124]
train() client id: f_00002-2-0 loss: 0.966646  [   32/  124]
train() client id: f_00002-2-1 loss: 0.958092  [   64/  124]
train() client id: f_00002-2-2 loss: 1.125015  [   96/  124]
train() client id: f_00002-3-0 loss: 1.014666  [   32/  124]
train() client id: f_00002-3-1 loss: 0.968583  [   64/  124]
train() client id: f_00002-3-2 loss: 1.120119  [   96/  124]
train() client id: f_00002-4-0 loss: 1.076304  [   32/  124]
train() client id: f_00002-4-1 loss: 0.910167  [   64/  124]
train() client id: f_00002-4-2 loss: 0.916195  [   96/  124]
train() client id: f_00002-5-0 loss: 1.070805  [   32/  124]
train() client id: f_00002-5-1 loss: 0.830274  [   64/  124]
train() client id: f_00002-5-2 loss: 0.921996  [   96/  124]
train() client id: f_00002-6-0 loss: 0.911065  [   32/  124]
train() client id: f_00002-6-1 loss: 1.051955  [   64/  124]
train() client id: f_00002-6-2 loss: 0.770930  [   96/  124]
train() client id: f_00002-7-0 loss: 0.934257  [   32/  124]
train() client id: f_00002-7-1 loss: 0.920216  [   64/  124]
train() client id: f_00002-7-2 loss: 1.012290  [   96/  124]
train() client id: f_00002-8-0 loss: 0.830016  [   32/  124]
train() client id: f_00002-8-1 loss: 0.729044  [   64/  124]
train() client id: f_00002-8-2 loss: 1.159983  [   96/  124]
train() client id: f_00002-9-0 loss: 1.008166  [   32/  124]
train() client id: f_00002-9-1 loss: 0.833665  [   64/  124]
train() client id: f_00002-9-2 loss: 1.002177  [   96/  124]
train() client id: f_00002-10-0 loss: 0.895334  [   32/  124]
train() client id: f_00002-10-1 loss: 1.167438  [   64/  124]
train() client id: f_00002-10-2 loss: 0.919278  [   96/  124]
train() client id: f_00002-11-0 loss: 0.773930  [   32/  124]
train() client id: f_00002-11-1 loss: 0.969405  [   64/  124]
train() client id: f_00002-11-2 loss: 0.886011  [   96/  124]
train() client id: f_00002-12-0 loss: 0.870317  [   32/  124]
train() client id: f_00002-12-1 loss: 0.997954  [   64/  124]
train() client id: f_00002-12-2 loss: 0.920264  [   96/  124]
train() client id: f_00003-0-0 loss: 0.908027  [   32/   43]
train() client id: f_00003-1-0 loss: 0.903216  [   32/   43]
train() client id: f_00003-2-0 loss: 0.642567  [   32/   43]
train() client id: f_00003-3-0 loss: 0.735106  [   32/   43]
train() client id: f_00003-4-0 loss: 0.981547  [   32/   43]
train() client id: f_00003-5-0 loss: 0.925148  [   32/   43]
train() client id: f_00003-6-0 loss: 0.808879  [   32/   43]
train() client id: f_00003-7-0 loss: 0.684567  [   32/   43]
train() client id: f_00003-8-0 loss: 0.727644  [   32/   43]
train() client id: f_00003-9-0 loss: 0.658449  [   32/   43]
train() client id: f_00003-10-0 loss: 0.950926  [   32/   43]
train() client id: f_00003-11-0 loss: 0.802531  [   32/   43]
train() client id: f_00003-12-0 loss: 0.994727  [   32/   43]
train() client id: f_00004-0-0 loss: 1.042989  [   32/  306]
train() client id: f_00004-0-1 loss: 0.933689  [   64/  306]
train() client id: f_00004-0-2 loss: 0.890481  [   96/  306]
train() client id: f_00004-0-3 loss: 0.941948  [  128/  306]
train() client id: f_00004-0-4 loss: 0.924843  [  160/  306]
train() client id: f_00004-0-5 loss: 0.943069  [  192/  306]
train() client id: f_00004-0-6 loss: 0.955200  [  224/  306]
train() client id: f_00004-0-7 loss: 1.005790  [  256/  306]
train() client id: f_00004-0-8 loss: 1.044244  [  288/  306]
train() client id: f_00004-1-0 loss: 1.022801  [   32/  306]
train() client id: f_00004-1-1 loss: 0.994219  [   64/  306]
train() client id: f_00004-1-2 loss: 0.940944  [   96/  306]
train() client id: f_00004-1-3 loss: 1.065251  [  128/  306]
train() client id: f_00004-1-4 loss: 0.816854  [  160/  306]
train() client id: f_00004-1-5 loss: 0.956976  [  192/  306]
train() client id: f_00004-1-6 loss: 0.854713  [  224/  306]
train() client id: f_00004-1-7 loss: 0.910439  [  256/  306]
train() client id: f_00004-1-8 loss: 1.110906  [  288/  306]
train() client id: f_00004-2-0 loss: 0.848341  [   32/  306]
train() client id: f_00004-2-1 loss: 0.999800  [   64/  306]
train() client id: f_00004-2-2 loss: 0.962233  [   96/  306]
train() client id: f_00004-2-3 loss: 0.895808  [  128/  306]
train() client id: f_00004-2-4 loss: 0.807176  [  160/  306]
train() client id: f_00004-2-5 loss: 0.980409  [  192/  306]
train() client id: f_00004-2-6 loss: 1.074357  [  224/  306]
train() client id: f_00004-2-7 loss: 1.089751  [  256/  306]
train() client id: f_00004-2-8 loss: 0.926306  [  288/  306]
train() client id: f_00004-3-0 loss: 0.859756  [   32/  306]
train() client id: f_00004-3-1 loss: 1.028285  [   64/  306]
train() client id: f_00004-3-2 loss: 0.946414  [   96/  306]
train() client id: f_00004-3-3 loss: 0.921549  [  128/  306]
train() client id: f_00004-3-4 loss: 1.065453  [  160/  306]
train() client id: f_00004-3-5 loss: 0.838680  [  192/  306]
train() client id: f_00004-3-6 loss: 1.045755  [  224/  306]
train() client id: f_00004-3-7 loss: 0.974676  [  256/  306]
train() client id: f_00004-3-8 loss: 0.982015  [  288/  306]
train() client id: f_00004-4-0 loss: 0.844952  [   32/  306]
train() client id: f_00004-4-1 loss: 1.079187  [   64/  306]
train() client id: f_00004-4-2 loss: 0.916637  [   96/  306]
train() client id: f_00004-4-3 loss: 0.859650  [  128/  306]
train() client id: f_00004-4-4 loss: 0.942593  [  160/  306]
train() client id: f_00004-4-5 loss: 0.928518  [  192/  306]
train() client id: f_00004-4-6 loss: 1.093897  [  224/  306]
train() client id: f_00004-4-7 loss: 0.839322  [  256/  306]
train() client id: f_00004-4-8 loss: 1.019473  [  288/  306]
train() client id: f_00004-5-0 loss: 1.131751  [   32/  306]
train() client id: f_00004-5-1 loss: 0.939915  [   64/  306]
train() client id: f_00004-5-2 loss: 0.916628  [   96/  306]
train() client id: f_00004-5-3 loss: 1.039134  [  128/  306]
train() client id: f_00004-5-4 loss: 0.996152  [  160/  306]
train() client id: f_00004-5-5 loss: 0.948553  [  192/  306]
train() client id: f_00004-5-6 loss: 0.896802  [  224/  306]
train() client id: f_00004-5-7 loss: 0.908466  [  256/  306]
train() client id: f_00004-5-8 loss: 0.833935  [  288/  306]
train() client id: f_00004-6-0 loss: 0.888566  [   32/  306]
train() client id: f_00004-6-1 loss: 1.000281  [   64/  306]
train() client id: f_00004-6-2 loss: 0.965627  [   96/  306]
train() client id: f_00004-6-3 loss: 0.861066  [  128/  306]
train() client id: f_00004-6-4 loss: 1.062657  [  160/  306]
train() client id: f_00004-6-5 loss: 1.135927  [  192/  306]
train() client id: f_00004-6-6 loss: 0.942845  [  224/  306]
train() client id: f_00004-6-7 loss: 0.903272  [  256/  306]
train() client id: f_00004-6-8 loss: 0.903740  [  288/  306]
train() client id: f_00004-7-0 loss: 0.872284  [   32/  306]
train() client id: f_00004-7-1 loss: 0.895761  [   64/  306]
train() client id: f_00004-7-2 loss: 0.939458  [   96/  306]
train() client id: f_00004-7-3 loss: 0.897806  [  128/  306]
train() client id: f_00004-7-4 loss: 0.960715  [  160/  306]
train() client id: f_00004-7-5 loss: 0.916378  [  192/  306]
train() client id: f_00004-7-6 loss: 0.982750  [  224/  306]
train() client id: f_00004-7-7 loss: 1.066431  [  256/  306]
train() client id: f_00004-7-8 loss: 0.988814  [  288/  306]
train() client id: f_00004-8-0 loss: 1.096814  [   32/  306]
train() client id: f_00004-8-1 loss: 0.992333  [   64/  306]
train() client id: f_00004-8-2 loss: 0.890655  [   96/  306]
train() client id: f_00004-8-3 loss: 0.936162  [  128/  306]
train() client id: f_00004-8-4 loss: 0.908902  [  160/  306]
train() client id: f_00004-8-5 loss: 0.940079  [  192/  306]
train() client id: f_00004-8-6 loss: 0.906810  [  224/  306]
train() client id: f_00004-8-7 loss: 1.023290  [  256/  306]
train() client id: f_00004-8-8 loss: 0.885953  [  288/  306]
train() client id: f_00004-9-0 loss: 0.798628  [   32/  306]
train() client id: f_00004-9-1 loss: 0.833753  [   64/  306]
train() client id: f_00004-9-2 loss: 0.920824  [   96/  306]
train() client id: f_00004-9-3 loss: 1.008256  [  128/  306]
train() client id: f_00004-9-4 loss: 0.979103  [  160/  306]
train() client id: f_00004-9-5 loss: 1.100144  [  192/  306]
train() client id: f_00004-9-6 loss: 0.981342  [  224/  306]
train() client id: f_00004-9-7 loss: 1.042227  [  256/  306]
train() client id: f_00004-9-8 loss: 0.885556  [  288/  306]
train() client id: f_00004-10-0 loss: 0.989840  [   32/  306]
train() client id: f_00004-10-1 loss: 0.927833  [   64/  306]
train() client id: f_00004-10-2 loss: 0.922911  [   96/  306]
train() client id: f_00004-10-3 loss: 0.829082  [  128/  306]
train() client id: f_00004-10-4 loss: 0.992891  [  160/  306]
train() client id: f_00004-10-5 loss: 0.933592  [  192/  306]
train() client id: f_00004-10-6 loss: 1.053300  [  224/  306]
train() client id: f_00004-10-7 loss: 0.983741  [  256/  306]
train() client id: f_00004-10-8 loss: 0.988368  [  288/  306]
train() client id: f_00004-11-0 loss: 0.960841  [   32/  306]
train() client id: f_00004-11-1 loss: 1.008837  [   64/  306]
train() client id: f_00004-11-2 loss: 0.941382  [   96/  306]
train() client id: f_00004-11-3 loss: 0.955200  [  128/  306]
train() client id: f_00004-11-4 loss: 0.831177  [  160/  306]
train() client id: f_00004-11-5 loss: 1.027798  [  192/  306]
train() client id: f_00004-11-6 loss: 0.923227  [  224/  306]
train() client id: f_00004-11-7 loss: 0.991257  [  256/  306]
train() client id: f_00004-11-8 loss: 0.899007  [  288/  306]
train() client id: f_00004-12-0 loss: 0.917899  [   32/  306]
train() client id: f_00004-12-1 loss: 0.875137  [   64/  306]
train() client id: f_00004-12-2 loss: 0.882012  [   96/  306]
train() client id: f_00004-12-3 loss: 0.983660  [  128/  306]
train() client id: f_00004-12-4 loss: 1.010338  [  160/  306]
train() client id: f_00004-12-5 loss: 0.998094  [  192/  306]
train() client id: f_00004-12-6 loss: 0.938641  [  224/  306]
train() client id: f_00004-12-7 loss: 1.022253  [  256/  306]
train() client id: f_00004-12-8 loss: 0.935537  [  288/  306]
train() client id: f_00005-0-0 loss: 0.649996  [   32/  146]
train() client id: f_00005-0-1 loss: 0.733133  [   64/  146]
train() client id: f_00005-0-2 loss: 0.612426  [   96/  146]
train() client id: f_00005-0-3 loss: 0.838956  [  128/  146]
train() client id: f_00005-1-0 loss: 0.626501  [   32/  146]
train() client id: f_00005-1-1 loss: 0.582653  [   64/  146]
train() client id: f_00005-1-2 loss: 0.696326  [   96/  146]
train() client id: f_00005-1-3 loss: 0.809815  [  128/  146]
train() client id: f_00005-2-0 loss: 0.669643  [   32/  146]
train() client id: f_00005-2-1 loss: 0.730329  [   64/  146]
train() client id: f_00005-2-2 loss: 0.645719  [   96/  146]
train() client id: f_00005-2-3 loss: 0.739187  [  128/  146]
train() client id: f_00005-3-0 loss: 0.935009  [   32/  146]
train() client id: f_00005-3-1 loss: 0.611596  [   64/  146]
train() client id: f_00005-3-2 loss: 0.664635  [   96/  146]
train() client id: f_00005-3-3 loss: 0.589555  [  128/  146]
train() client id: f_00005-4-0 loss: 0.664174  [   32/  146]
train() client id: f_00005-4-1 loss: 0.677166  [   64/  146]
train() client id: f_00005-4-2 loss: 0.801184  [   96/  146]
train() client id: f_00005-4-3 loss: 0.742284  [  128/  146]
train() client id: f_00005-5-0 loss: 0.617958  [   32/  146]
train() client id: f_00005-5-1 loss: 0.956265  [   64/  146]
train() client id: f_00005-5-2 loss: 0.756210  [   96/  146]
train() client id: f_00005-5-3 loss: 0.532636  [  128/  146]
train() client id: f_00005-6-0 loss: 0.683207  [   32/  146]
train() client id: f_00005-6-1 loss: 0.690476  [   64/  146]
train() client id: f_00005-6-2 loss: 0.622133  [   96/  146]
train() client id: f_00005-6-3 loss: 0.540393  [  128/  146]
train() client id: f_00005-7-0 loss: 0.588734  [   32/  146]
train() client id: f_00005-7-1 loss: 0.834444  [   64/  146]
train() client id: f_00005-7-2 loss: 0.568009  [   96/  146]
train() client id: f_00005-7-3 loss: 0.760085  [  128/  146]
train() client id: f_00005-8-0 loss: 0.454748  [   32/  146]
train() client id: f_00005-8-1 loss: 0.749303  [   64/  146]
train() client id: f_00005-8-2 loss: 0.894199  [   96/  146]
train() client id: f_00005-8-3 loss: 0.679090  [  128/  146]
train() client id: f_00005-9-0 loss: 0.704683  [   32/  146]
train() client id: f_00005-9-1 loss: 0.732805  [   64/  146]
train() client id: f_00005-9-2 loss: 0.642034  [   96/  146]
train() client id: f_00005-9-3 loss: 0.795242  [  128/  146]
train() client id: f_00005-10-0 loss: 0.601261  [   32/  146]
train() client id: f_00005-10-1 loss: 0.817107  [   64/  146]
train() client id: f_00005-10-2 loss: 0.587127  [   96/  146]
train() client id: f_00005-10-3 loss: 0.779892  [  128/  146]
train() client id: f_00005-11-0 loss: 0.552114  [   32/  146]
train() client id: f_00005-11-1 loss: 0.652279  [   64/  146]
train() client id: f_00005-11-2 loss: 0.654361  [   96/  146]
train() client id: f_00005-11-3 loss: 0.957264  [  128/  146]
train() client id: f_00005-12-0 loss: 0.500465  [   32/  146]
train() client id: f_00005-12-1 loss: 0.722647  [   64/  146]
train() client id: f_00005-12-2 loss: 0.803269  [   96/  146]
train() client id: f_00005-12-3 loss: 0.689489  [  128/  146]
train() client id: f_00006-0-0 loss: 0.465728  [   32/   54]
train() client id: f_00006-1-0 loss: 0.556401  [   32/   54]
train() client id: f_00006-2-0 loss: 0.521037  [   32/   54]
train() client id: f_00006-3-0 loss: 0.504247  [   32/   54]
train() client id: f_00006-4-0 loss: 0.572466  [   32/   54]
train() client id: f_00006-5-0 loss: 0.576391  [   32/   54]
train() client id: f_00006-6-0 loss: 0.577148  [   32/   54]
train() client id: f_00006-7-0 loss: 0.587781  [   32/   54]
train() client id: f_00006-8-0 loss: 0.569051  [   32/   54]
train() client id: f_00006-9-0 loss: 0.479334  [   32/   54]
train() client id: f_00006-10-0 loss: 0.562737  [   32/   54]
train() client id: f_00006-11-0 loss: 0.572003  [   32/   54]
train() client id: f_00006-12-0 loss: 0.523025  [   32/   54]
train() client id: f_00007-0-0 loss: 0.762678  [   32/  179]
train() client id: f_00007-0-1 loss: 0.752895  [   64/  179]
train() client id: f_00007-0-2 loss: 0.699328  [   96/  179]
train() client id: f_00007-0-3 loss: 0.687398  [  128/  179]
train() client id: f_00007-0-4 loss: 0.712520  [  160/  179]
train() client id: f_00007-1-0 loss: 0.642456  [   32/  179]
train() client id: f_00007-1-1 loss: 0.744346  [   64/  179]
train() client id: f_00007-1-2 loss: 0.722854  [   96/  179]
train() client id: f_00007-1-3 loss: 0.547297  [  128/  179]
train() client id: f_00007-1-4 loss: 0.689463  [  160/  179]
train() client id: f_00007-2-0 loss: 0.750390  [   32/  179]
train() client id: f_00007-2-1 loss: 0.668694  [   64/  179]
train() client id: f_00007-2-2 loss: 0.822513  [   96/  179]
train() client id: f_00007-2-3 loss: 0.520415  [  128/  179]
train() client id: f_00007-2-4 loss: 0.586747  [  160/  179]
train() client id: f_00007-3-0 loss: 0.677049  [   32/  179]
train() client id: f_00007-3-1 loss: 0.767652  [   64/  179]
train() client id: f_00007-3-2 loss: 0.544093  [   96/  179]
train() client id: f_00007-3-3 loss: 0.614869  [  128/  179]
train() client id: f_00007-3-4 loss: 0.689255  [  160/  179]
train() client id: f_00007-4-0 loss: 0.891117  [   32/  179]
train() client id: f_00007-4-1 loss: 0.632675  [   64/  179]
train() client id: f_00007-4-2 loss: 0.665862  [   96/  179]
train() client id: f_00007-4-3 loss: 0.580071  [  128/  179]
train() client id: f_00007-4-4 loss: 0.651090  [  160/  179]
train() client id: f_00007-5-0 loss: 0.729342  [   32/  179]
train() client id: f_00007-5-1 loss: 0.604899  [   64/  179]
train() client id: f_00007-5-2 loss: 0.692880  [   96/  179]
train() client id: f_00007-5-3 loss: 0.701344  [  128/  179]
train() client id: f_00007-5-4 loss: 0.675781  [  160/  179]
train() client id: f_00007-6-0 loss: 0.735225  [   32/  179]
train() client id: f_00007-6-1 loss: 0.695669  [   64/  179]
train() client id: f_00007-6-2 loss: 0.765834  [   96/  179]
train() client id: f_00007-6-3 loss: 0.523943  [  128/  179]
train() client id: f_00007-6-4 loss: 0.670138  [  160/  179]
train() client id: f_00007-7-0 loss: 0.534160  [   32/  179]
train() client id: f_00007-7-1 loss: 0.576063  [   64/  179]
train() client id: f_00007-7-2 loss: 0.770007  [   96/  179]
train() client id: f_00007-7-3 loss: 0.826454  [  128/  179]
train() client id: f_00007-7-4 loss: 0.665588  [  160/  179]
train() client id: f_00007-8-0 loss: 0.492052  [   32/  179]
train() client id: f_00007-8-1 loss: 0.663126  [   64/  179]
train() client id: f_00007-8-2 loss: 0.582634  [   96/  179]
train() client id: f_00007-8-3 loss: 0.719790  [  128/  179]
train() client id: f_00007-8-4 loss: 0.719610  [  160/  179]
train() client id: f_00007-9-0 loss: 0.793356  [   32/  179]
train() client id: f_00007-9-1 loss: 0.493924  [   64/  179]
train() client id: f_00007-9-2 loss: 0.596910  [   96/  179]
train() client id: f_00007-9-3 loss: 0.829829  [  128/  179]
train() client id: f_00007-9-4 loss: 0.567535  [  160/  179]
train() client id: f_00007-10-0 loss: 0.616749  [   32/  179]
train() client id: f_00007-10-1 loss: 0.879398  [   64/  179]
train() client id: f_00007-10-2 loss: 0.763701  [   96/  179]
train() client id: f_00007-10-3 loss: 0.474043  [  128/  179]
train() client id: f_00007-10-4 loss: 0.584406  [  160/  179]
train() client id: f_00007-11-0 loss: 0.790974  [   32/  179]
train() client id: f_00007-11-1 loss: 0.785082  [   64/  179]
train() client id: f_00007-11-2 loss: 0.634987  [   96/  179]
train() client id: f_00007-11-3 loss: 0.481197  [  128/  179]
train() client id: f_00007-11-4 loss: 0.600840  [  160/  179]
train() client id: f_00007-12-0 loss: 0.623700  [   32/  179]
train() client id: f_00007-12-1 loss: 0.939534  [   64/  179]
train() client id: f_00007-12-2 loss: 0.604174  [   96/  179]
train() client id: f_00007-12-3 loss: 0.619478  [  128/  179]
train() client id: f_00007-12-4 loss: 0.609782  [  160/  179]
train() client id: f_00008-0-0 loss: 0.601358  [   32/  130]
train() client id: f_00008-0-1 loss: 0.776970  [   64/  130]
train() client id: f_00008-0-2 loss: 0.625749  [   96/  130]
train() client id: f_00008-0-3 loss: 0.596197  [  128/  130]
train() client id: f_00008-1-0 loss: 0.665557  [   32/  130]
train() client id: f_00008-1-1 loss: 0.659220  [   64/  130]
train() client id: f_00008-1-2 loss: 0.569475  [   96/  130]
train() client id: f_00008-1-3 loss: 0.676128  [  128/  130]
train() client id: f_00008-2-0 loss: 0.728725  [   32/  130]
train() client id: f_00008-2-1 loss: 0.655049  [   64/  130]
train() client id: f_00008-2-2 loss: 0.550678  [   96/  130]
train() client id: f_00008-2-3 loss: 0.620354  [  128/  130]
train() client id: f_00008-3-0 loss: 0.588511  [   32/  130]
train() client id: f_00008-3-1 loss: 0.609108  [   64/  130]
train() client id: f_00008-3-2 loss: 0.800931  [   96/  130]
train() client id: f_00008-3-3 loss: 0.601993  [  128/  130]
train() client id: f_00008-4-0 loss: 0.621946  [   32/  130]
train() client id: f_00008-4-1 loss: 0.804098  [   64/  130]
train() client id: f_00008-4-2 loss: 0.632857  [   96/  130]
train() client id: f_00008-4-3 loss: 0.529340  [  128/  130]
train() client id: f_00008-5-0 loss: 0.605242  [   32/  130]
train() client id: f_00008-5-1 loss: 0.584084  [   64/  130]
train() client id: f_00008-5-2 loss: 0.784854  [   96/  130]
train() client id: f_00008-5-3 loss: 0.588579  [  128/  130]
train() client id: f_00008-6-0 loss: 0.667701  [   32/  130]
train() client id: f_00008-6-1 loss: 0.639046  [   64/  130]
train() client id: f_00008-6-2 loss: 0.625479  [   96/  130]
train() client id: f_00008-6-3 loss: 0.643496  [  128/  130]
train() client id: f_00008-7-0 loss: 0.651475  [   32/  130]
train() client id: f_00008-7-1 loss: 0.590912  [   64/  130]
train() client id: f_00008-7-2 loss: 0.676801  [   96/  130]
train() client id: f_00008-7-3 loss: 0.688205  [  128/  130]
train() client id: f_00008-8-0 loss: 0.621788  [   32/  130]
train() client id: f_00008-8-1 loss: 0.612355  [   64/  130]
train() client id: f_00008-8-2 loss: 0.646233  [   96/  130]
train() client id: f_00008-8-3 loss: 0.693470  [  128/  130]
train() client id: f_00008-9-0 loss: 0.658829  [   32/  130]
train() client id: f_00008-9-1 loss: 0.645100  [   64/  130]
train() client id: f_00008-9-2 loss: 0.621543  [   96/  130]
train() client id: f_00008-9-3 loss: 0.661863  [  128/  130]
train() client id: f_00008-10-0 loss: 0.748665  [   32/  130]
train() client id: f_00008-10-1 loss: 0.608117  [   64/  130]
train() client id: f_00008-10-2 loss: 0.622221  [   96/  130]
train() client id: f_00008-10-3 loss: 0.596334  [  128/  130]
train() client id: f_00008-11-0 loss: 0.703443  [   32/  130]
train() client id: f_00008-11-1 loss: 0.764895  [   64/  130]
train() client id: f_00008-11-2 loss: 0.618189  [   96/  130]
train() client id: f_00008-11-3 loss: 0.526653  [  128/  130]
train() client id: f_00008-12-0 loss: 0.759100  [   32/  130]
train() client id: f_00008-12-1 loss: 0.675851  [   64/  130]
train() client id: f_00008-12-2 loss: 0.545266  [   96/  130]
train() client id: f_00008-12-3 loss: 0.606127  [  128/  130]
train() client id: f_00009-0-0 loss: 1.000288  [   32/  118]
train() client id: f_00009-0-1 loss: 1.084226  [   64/  118]
train() client id: f_00009-0-2 loss: 0.993588  [   96/  118]
train() client id: f_00009-1-0 loss: 1.159844  [   32/  118]
train() client id: f_00009-1-1 loss: 0.972850  [   64/  118]
train() client id: f_00009-1-2 loss: 0.967854  [   96/  118]
train() client id: f_00009-2-0 loss: 0.920131  [   32/  118]
train() client id: f_00009-2-1 loss: 0.983343  [   64/  118]
train() client id: f_00009-2-2 loss: 1.016038  [   96/  118]
train() client id: f_00009-3-0 loss: 1.029411  [   32/  118]
train() client id: f_00009-3-1 loss: 0.934591  [   64/  118]
train() client id: f_00009-3-2 loss: 0.859167  [   96/  118]
train() client id: f_00009-4-0 loss: 0.969213  [   32/  118]
train() client id: f_00009-4-1 loss: 0.983413  [   64/  118]
train() client id: f_00009-4-2 loss: 0.834843  [   96/  118]
train() client id: f_00009-5-0 loss: 1.061647  [   32/  118]
train() client id: f_00009-5-1 loss: 0.710875  [   64/  118]
train() client id: f_00009-5-2 loss: 0.860568  [   96/  118]
train() client id: f_00009-6-0 loss: 0.859510  [   32/  118]
train() client id: f_00009-6-1 loss: 0.851503  [   64/  118]
train() client id: f_00009-6-2 loss: 0.957740  [   96/  118]
train() client id: f_00009-7-0 loss: 0.725435  [   32/  118]
train() client id: f_00009-7-1 loss: 0.915366  [   64/  118]
train() client id: f_00009-7-2 loss: 0.972867  [   96/  118]
train() client id: f_00009-8-0 loss: 0.920912  [   32/  118]
train() client id: f_00009-8-1 loss: 0.911648  [   64/  118]
train() client id: f_00009-8-2 loss: 0.778943  [   96/  118]
train() client id: f_00009-9-0 loss: 0.867616  [   32/  118]
train() client id: f_00009-9-1 loss: 0.923916  [   64/  118]
train() client id: f_00009-9-2 loss: 0.872993  [   96/  118]
train() client id: f_00009-10-0 loss: 0.853408  [   32/  118]
train() client id: f_00009-10-1 loss: 0.856992  [   64/  118]
train() client id: f_00009-10-2 loss: 0.829171  [   96/  118]
train() client id: f_00009-11-0 loss: 0.686226  [   32/  118]
train() client id: f_00009-11-1 loss: 0.830081  [   64/  118]
train() client id: f_00009-11-2 loss: 0.829909  [   96/  118]
train() client id: f_00009-12-0 loss: 0.773418  [   32/  118]
train() client id: f_00009-12-1 loss: 1.038947  [   64/  118]
train() client id: f_00009-12-2 loss: 0.694974  [   96/  118]
At round 45 accuracy: 0.649867374005305
At round 45 training accuracy: 0.5895372233400402
At round 45 training loss: 0.8284560068970408
update_location
xs = -3.905658 4.200318 245.009024 18.811294 0.979296 3.956410 -207.443192 -186.324852 229.663977 -172.060879 
ys = 237.587959 220.555839 1.320614 -207.455176 199.350187 182.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 257.804757 242.203470 264.634022 231.066040 223.027927 208.414651 230.303210 211.465427 251.105978 199.050139 
dists_bs = 182.182673 184.900212 454.502045 428.759582 177.655972 179.126931 180.563550 174.545429 434.208996 171.023775 
uav_gains = -111.973319 -110.648653 -112.583770 -109.782860 -109.200977 -108.218408 -109.726146 -108.417104 -111.390355 -107.622214 
bs_gains = -102.861257 -103.041307 -113.978063 -113.269047 -102.555294 -102.655564 -102.752702 -102.340497 -113.422626 -102.092641 
Round 46
-------------------------------
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.92415982 10.12154902  4.85601333  1.75987582 11.67148702  5.61542073
  2.1757575   6.89673295  5.09466844  4.55326949]
obj_prev = 57.668934107649186
eta_min = 1.934063127362615e-19	eta_max = 0.9372721317732582
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 13.346274362136274	eta = 0.9090909090909091
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 26.11640265875819	eta = 0.46457304443429304
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 19.646759764324543	eta = 0.617556118077178
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 18.475231020287687	eta = 0.6567158310241378
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 18.40977991233988	eta = 0.6590506106332402
af = 12.132976692851157	bf = 1.2178222333661453	zeta = 18.40955607686498	eta = 0.6590586238034545
eta = 0.6590586238034545
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [0.03440512 0.07235997 0.03385901 0.01174143 0.08355529 0.03986625
 0.01474505 0.04887711 0.03549735 0.03222067]
ene_total = [1.70303953 2.89994011 1.70508731 0.81177669 3.30204155 1.7110066
 0.91978726 2.13279846 1.79829675 1.42578181]
ti_comp = [0.63680768 0.68794036 0.63078466 0.65472313 0.68956148 0.68923291
 0.65512248 0.66360668 0.62159244 0.69103952]
ti_coms = [0.12324342 0.07211074 0.12926644 0.10532797 0.07048962 0.07081819
 0.10492862 0.09644442 0.13845866 0.06901158]
t_total = [27.69980698 27.69980698 27.69980698 27.69980698 27.69980698 27.69980698
 27.69980698 27.69980698 27.69980698 27.69980698]
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [6.27671845e-06 5.00349702e-05 6.09733991e-06 2.36008472e-07
 7.66753725e-05 8.33612381e-06 4.66846244e-07 1.65720103e-05
 7.23529432e-06 4.37802116e-06]
ene_total = [0.44938525 0.26462869 0.47132941 0.38387283 0.25969147 0.25839833
 0.38242583 0.35209241 0.50487161 0.25166996]
optimize_network iter = 0 obj = 3.578365787451716
eta = 0.6590586238034545
freqs = [27013744.23254671 52591749.45149608 26838799.91285678  8966713.64201406
 60585816.6007772  28920738.74380525 11253659.91650407 36826870.45355151
 28553553.02519245 23313187.49305714]
eta_min = 0.6590586238034565	eta_max = 0.6785433277513145
af = 0.005254252612989919	bf = 1.2178222333661453	zeta = 0.005779677874288912	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [1.43481355e-06 1.14376411e-05 1.39380888e-06 5.39498714e-08
 1.75274491e-05 1.90557908e-06 1.06717757e-07 3.78824463e-06
 1.65393723e-06 1.00078475e-06]
ene_total = [1.6846925  0.98717606 1.76700979 1.43963457 0.96585086 0.96820657
 1.43418346 1.3187243  1.89268496 0.94339011]
ti_comp = [0.59337097 0.64450365 0.58734795 0.61128643 0.64612477 0.6457962
 0.61168578 0.62016997 0.57815574 0.64760281]
ti_coms = [0.12324342 0.07211074 0.12926644 0.10532797 0.07048962 0.07081819
 0.10492862 0.09644442 0.13845866 0.06901158]
t_total = [27.69980698 27.69980698 27.69980698 27.69980698 27.69980698 27.69980698
 27.69980698 27.69980698 27.69980698 27.69980698]
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [5.81719976e-06 4.58713790e-05 5.65886160e-06 2.17856639e-07
 7.02726667e-05 7.64051325e-06 4.30902936e-07 1.52683660e-05
 6.72969396e-06 4.01128493e-06]
ene_total = [0.47660643 0.28050789 0.49988152 0.40714012 0.27518486 0.27403394
 0.40560472 0.37338368 0.53545424 0.26691045]
optimize_network iter = 1 obj = 3.7947078371699785
eta = 0.6785433277513145
freqs = [26965339.44106933 52213419.54907353 26809447.82144539  8932753.44599161
 60140450.82785589 28709062.73531702 11210552.63537973 36652528.58326961
 28553553.02519245 23138485.42973953]
eta_min = 0.6785433277513172	eta_max = 0.6785433277513145
af = 0.005187942505206725	bf = 1.2178222333661453	zeta = 0.005706736755727398	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [1.42967619e-06 1.12736748e-05 1.39076189e-06 5.35419898e-08
 1.72707080e-05 1.87778663e-06 1.05901756e-07 3.75246172e-06
 1.65393723e-06 9.85841782e-07]
ene_total = [1.6846918  0.98715364 1.76700937 1.43963452 0.96581577 0.96820277
 1.43418335 1.31871941 1.89268496 0.94338807]
ti_comp = [0.59337097 0.64450365 0.58734795 0.61128643 0.64612477 0.6457962
 0.61168578 0.62016997 0.57815574 0.64760281]
ti_coms = [0.12324342 0.07211074 0.12926644 0.10532797 0.07048962 0.07081819
 0.10492862 0.09644442 0.13845866 0.06901158]
t_total = [27.69980698 27.69980698 27.69980698 27.69980698 27.69980698 27.69980698
 27.69980698 27.69980698 27.69980698 27.69980698]
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [5.81719976e-06 4.58713790e-05 5.65886160e-06 2.17856639e-07
 7.02726667e-05 7.64051325e-06 4.30902936e-07 1.52683660e-05
 6.72969396e-06 4.01128493e-06]
ene_total = [0.47660643 0.28050789 0.49988152 0.40714012 0.27518486 0.27403394
 0.40560472 0.37338368 0.53545424 0.26691045]
optimize_network iter = 2 obj = 3.7947078371699785
eta = 0.6785433277513145
freqs = [26965339.44106933 52213419.54907353 26809447.82144539  8932753.44599161
 60140450.82785589 28709062.73531702 11210552.63537973 36652528.58326961
 28553553.02519245 23138485.42973953]
Done!
ene_coms = [0.01232434 0.00721107 0.01292664 0.0105328  0.00704896 0.00708182
 0.01049286 0.00964444 0.01384587 0.00690116]
ene_comp = [5.49709926e-06 4.33472348e-05 5.34747390e-06 2.05868737e-07
 6.64058034e-05 7.22008208e-06 4.07191829e-07 1.44282003e-05
 6.35938204e-06 3.79055772e-06]
ene_total = [0.01232984 0.00725442 0.01293199 0.010533   0.00711537 0.00708904
 0.01049327 0.00965887 0.01385222 0.00690495]
At round 46 energy consumption: 0.0981629748231883
At round 46 eta: 0.6785433277513145
At round 46 a_n: 12.425493895665781
At round 46 local rounds: 12.69876962905909
At round 46 global rounds: 38.653712827752926
gradient difference: 0.4579668641090393
train() client id: f_00000-0-0 loss: 1.212170  [   32/  126]
train() client id: f_00000-0-1 loss: 0.813343  [   64/  126]
train() client id: f_00000-0-2 loss: 0.961407  [   96/  126]
train() client id: f_00000-1-0 loss: 1.046737  [   32/  126]
train() client id: f_00000-1-1 loss: 0.860879  [   64/  126]
train() client id: f_00000-1-2 loss: 0.791379  [   96/  126]
train() client id: f_00000-2-0 loss: 0.952067  [   32/  126]
train() client id: f_00000-2-1 loss: 0.882889  [   64/  126]
train() client id: f_00000-2-2 loss: 0.589381  [   96/  126]
train() client id: f_00000-3-0 loss: 0.698952  [   32/  126]
train() client id: f_00000-3-1 loss: 0.937634  [   64/  126]
train() client id: f_00000-3-2 loss: 0.862822  [   96/  126]
train() client id: f_00000-4-0 loss: 0.869387  [   32/  126]
train() client id: f_00000-4-1 loss: 0.717570  [   64/  126]
train() client id: f_00000-4-2 loss: 0.744400  [   96/  126]
train() client id: f_00000-5-0 loss: 0.767509  [   32/  126]
train() client id: f_00000-5-1 loss: 0.813234  [   64/  126]
train() client id: f_00000-5-2 loss: 0.525973  [   96/  126]
train() client id: f_00000-6-0 loss: 0.670769  [   32/  126]
train() client id: f_00000-6-1 loss: 0.834327  [   64/  126]
train() client id: f_00000-6-2 loss: 0.662922  [   96/  126]
train() client id: f_00000-7-0 loss: 0.659084  [   32/  126]
train() client id: f_00000-7-1 loss: 0.665113  [   64/  126]
train() client id: f_00000-7-2 loss: 0.710592  [   96/  126]
train() client id: f_00000-8-0 loss: 0.640353  [   32/  126]
train() client id: f_00000-8-1 loss: 0.707308  [   64/  126]
train() client id: f_00000-8-2 loss: 0.604800  [   96/  126]
train() client id: f_00000-9-0 loss: 0.609236  [   32/  126]
train() client id: f_00000-9-1 loss: 0.736185  [   64/  126]
train() client id: f_00000-9-2 loss: 0.770101  [   96/  126]
train() client id: f_00000-10-0 loss: 0.616254  [   32/  126]
train() client id: f_00000-10-1 loss: 0.546638  [   64/  126]
train() client id: f_00000-10-2 loss: 0.786611  [   96/  126]
train() client id: f_00000-11-0 loss: 0.639800  [   32/  126]
train() client id: f_00000-11-1 loss: 0.644899  [   64/  126]
train() client id: f_00000-11-2 loss: 0.735637  [   96/  126]
train() client id: f_00001-0-0 loss: 0.465929  [   32/  265]
train() client id: f_00001-0-1 loss: 0.511385  [   64/  265]
train() client id: f_00001-0-2 loss: 0.369933  [   96/  265]
train() client id: f_00001-0-3 loss: 0.501805  [  128/  265]
train() client id: f_00001-0-4 loss: 0.439212  [  160/  265]
train() client id: f_00001-0-5 loss: 0.353442  [  192/  265]
train() client id: f_00001-0-6 loss: 0.411677  [  224/  265]
train() client id: f_00001-0-7 loss: 0.426634  [  256/  265]
train() client id: f_00001-1-0 loss: 0.422013  [   32/  265]
train() client id: f_00001-1-1 loss: 0.408779  [   64/  265]
train() client id: f_00001-1-2 loss: 0.363332  [   96/  265]
train() client id: f_00001-1-3 loss: 0.400623  [  128/  265]
train() client id: f_00001-1-4 loss: 0.468891  [  160/  265]
train() client id: f_00001-1-5 loss: 0.535477  [  192/  265]
train() client id: f_00001-1-6 loss: 0.497570  [  224/  265]
train() client id: f_00001-1-7 loss: 0.329423  [  256/  265]
train() client id: f_00001-2-0 loss: 0.539311  [   32/  265]
train() client id: f_00001-2-1 loss: 0.448479  [   64/  265]
train() client id: f_00001-2-2 loss: 0.487715  [   96/  265]
train() client id: f_00001-2-3 loss: 0.372543  [  128/  265]
train() client id: f_00001-2-4 loss: 0.325677  [  160/  265]
train() client id: f_00001-2-5 loss: 0.544167  [  192/  265]
train() client id: f_00001-2-6 loss: 0.352993  [  224/  265]
train() client id: f_00001-2-7 loss: 0.307808  [  256/  265]
train() client id: f_00001-3-0 loss: 0.379149  [   32/  265]
train() client id: f_00001-3-1 loss: 0.475991  [   64/  265]
train() client id: f_00001-3-2 loss: 0.351202  [   96/  265]
train() client id: f_00001-3-3 loss: 0.400741  [  128/  265]
train() client id: f_00001-3-4 loss: 0.388732  [  160/  265]
train() client id: f_00001-3-5 loss: 0.389531  [  192/  265]
train() client id: f_00001-3-6 loss: 0.521165  [  224/  265]
train() client id: f_00001-3-7 loss: 0.425440  [  256/  265]
train() client id: f_00001-4-0 loss: 0.407948  [   32/  265]
train() client id: f_00001-4-1 loss: 0.634544  [   64/  265]
train() client id: f_00001-4-2 loss: 0.370822  [   96/  265]
train() client id: f_00001-4-3 loss: 0.328243  [  128/  265]
train() client id: f_00001-4-4 loss: 0.601447  [  160/  265]
train() client id: f_00001-4-5 loss: 0.323034  [  192/  265]
train() client id: f_00001-4-6 loss: 0.316089  [  224/  265]
train() client id: f_00001-4-7 loss: 0.316634  [  256/  265]
train() client id: f_00001-5-0 loss: 0.405955  [   32/  265]
train() client id: f_00001-5-1 loss: 0.348645  [   64/  265]
train() client id: f_00001-5-2 loss: 0.389876  [   96/  265]
train() client id: f_00001-5-3 loss: 0.406994  [  128/  265]
train() client id: f_00001-5-4 loss: 0.397381  [  160/  265]
train() client id: f_00001-5-5 loss: 0.369467  [  192/  265]
train() client id: f_00001-5-6 loss: 0.603858  [  224/  265]
train() client id: f_00001-5-7 loss: 0.396743  [  256/  265]
train() client id: f_00001-6-0 loss: 0.393386  [   32/  265]
train() client id: f_00001-6-1 loss: 0.467023  [   64/  265]
train() client id: f_00001-6-2 loss: 0.475288  [   96/  265]
train() client id: f_00001-6-3 loss: 0.361826  [  128/  265]
train() client id: f_00001-6-4 loss: 0.327677  [  160/  265]
train() client id: f_00001-6-5 loss: 0.421100  [  192/  265]
train() client id: f_00001-6-6 loss: 0.401471  [  224/  265]
train() client id: f_00001-6-7 loss: 0.472900  [  256/  265]
train() client id: f_00001-7-0 loss: 0.336461  [   32/  265]
train() client id: f_00001-7-1 loss: 0.406578  [   64/  265]
train() client id: f_00001-7-2 loss: 0.416833  [   96/  265]
train() client id: f_00001-7-3 loss: 0.313393  [  128/  265]
train() client id: f_00001-7-4 loss: 0.397568  [  160/  265]
train() client id: f_00001-7-5 loss: 0.365081  [  192/  265]
train() client id: f_00001-7-6 loss: 0.398620  [  224/  265]
train() client id: f_00001-7-7 loss: 0.678211  [  256/  265]
train() client id: f_00001-8-0 loss: 0.497611  [   32/  265]
train() client id: f_00001-8-1 loss: 0.432860  [   64/  265]
train() client id: f_00001-8-2 loss: 0.395542  [   96/  265]
train() client id: f_00001-8-3 loss: 0.380980  [  128/  265]
train() client id: f_00001-8-4 loss: 0.508676  [  160/  265]
train() client id: f_00001-8-5 loss: 0.373357  [  192/  265]
train() client id: f_00001-8-6 loss: 0.309572  [  224/  265]
train() client id: f_00001-8-7 loss: 0.418660  [  256/  265]
train() client id: f_00001-9-0 loss: 0.527126  [   32/  265]
train() client id: f_00001-9-1 loss: 0.338794  [   64/  265]
train() client id: f_00001-9-2 loss: 0.357342  [   96/  265]
train() client id: f_00001-9-3 loss: 0.385547  [  128/  265]
train() client id: f_00001-9-4 loss: 0.403878  [  160/  265]
train() client id: f_00001-9-5 loss: 0.512134  [  192/  265]
train() client id: f_00001-9-6 loss: 0.410747  [  224/  265]
train() client id: f_00001-9-7 loss: 0.383248  [  256/  265]
train() client id: f_00001-10-0 loss: 0.366659  [   32/  265]
train() client id: f_00001-10-1 loss: 0.388383  [   64/  265]
train() client id: f_00001-10-2 loss: 0.487076  [   96/  265]
train() client id: f_00001-10-3 loss: 0.367082  [  128/  265]
train() client id: f_00001-10-4 loss: 0.396277  [  160/  265]
train() client id: f_00001-10-5 loss: 0.566820  [  192/  265]
train() client id: f_00001-10-6 loss: 0.316964  [  224/  265]
train() client id: f_00001-10-7 loss: 0.444716  [  256/  265]
train() client id: f_00001-11-0 loss: 0.317380  [   32/  265]
train() client id: f_00001-11-1 loss: 0.315733  [   64/  265]
train() client id: f_00001-11-2 loss: 0.409452  [   96/  265]
train() client id: f_00001-11-3 loss: 0.438686  [  128/  265]
train() client id: f_00001-11-4 loss: 0.461386  [  160/  265]
train() client id: f_00001-11-5 loss: 0.335846  [  192/  265]
train() client id: f_00001-11-6 loss: 0.409697  [  224/  265]
train() client id: f_00001-11-7 loss: 0.647547  [  256/  265]
train() client id: f_00002-0-0 loss: 0.906493  [   32/  124]
train() client id: f_00002-0-1 loss: 1.047381  [   64/  124]
train() client id: f_00002-0-2 loss: 1.243361  [   96/  124]
train() client id: f_00002-1-0 loss: 1.175426  [   32/  124]
train() client id: f_00002-1-1 loss: 0.983039  [   64/  124]
train() client id: f_00002-1-2 loss: 1.099019  [   96/  124]
train() client id: f_00002-2-0 loss: 0.990246  [   32/  124]
train() client id: f_00002-2-1 loss: 1.136754  [   64/  124]
train() client id: f_00002-2-2 loss: 0.975177  [   96/  124]
train() client id: f_00002-3-0 loss: 1.120191  [   32/  124]
train() client id: f_00002-3-1 loss: 0.883536  [   64/  124]
train() client id: f_00002-3-2 loss: 1.089650  [   96/  124]
train() client id: f_00002-4-0 loss: 1.026992  [   32/  124]
train() client id: f_00002-4-1 loss: 0.983602  [   64/  124]
train() client id: f_00002-4-2 loss: 1.001495  [   96/  124]
train() client id: f_00002-5-0 loss: 0.965939  [   32/  124]
train() client id: f_00002-5-1 loss: 0.878202  [   64/  124]
train() client id: f_00002-5-2 loss: 0.881856  [   96/  124]
train() client id: f_00002-6-0 loss: 1.009946  [   32/  124]
train() client id: f_00002-6-1 loss: 1.058681  [   64/  124]
train() client id: f_00002-6-2 loss: 0.835831  [   96/  124]
train() client id: f_00002-7-0 loss: 1.000982  [   32/  124]
train() client id: f_00002-7-1 loss: 1.155642  [   64/  124]
train() client id: f_00002-7-2 loss: 0.833447  [   96/  124]
train() client id: f_00002-8-0 loss: 0.882369  [   32/  124]
train() client id: f_00002-8-1 loss: 0.948248  [   64/  124]
train() client id: f_00002-8-2 loss: 1.078978  [   96/  124]
train() client id: f_00002-9-0 loss: 0.995597  [   32/  124]
train() client id: f_00002-9-1 loss: 0.908896  [   64/  124]
train() client id: f_00002-9-2 loss: 1.033197  [   96/  124]
train() client id: f_00002-10-0 loss: 0.996947  [   32/  124]
train() client id: f_00002-10-1 loss: 0.805187  [   64/  124]
train() client id: f_00002-10-2 loss: 0.950468  [   96/  124]
train() client id: f_00002-11-0 loss: 0.853979  [   32/  124]
train() client id: f_00002-11-1 loss: 0.928832  [   64/  124]
train() client id: f_00002-11-2 loss: 1.069854  [   96/  124]
train() client id: f_00003-0-0 loss: 0.386714  [   32/   43]
train() client id: f_00003-1-0 loss: 0.686157  [   32/   43]
train() client id: f_00003-2-0 loss: 0.484930  [   32/   43]
train() client id: f_00003-3-0 loss: 0.534102  [   32/   43]
train() client id: f_00003-4-0 loss: 0.522430  [   32/   43]
train() client id: f_00003-5-0 loss: 0.529682  [   32/   43]
train() client id: f_00003-6-0 loss: 0.478647  [   32/   43]
train() client id: f_00003-7-0 loss: 0.604450  [   32/   43]
train() client id: f_00003-8-0 loss: 0.600377  [   32/   43]
train() client id: f_00003-9-0 loss: 0.477749  [   32/   43]
train() client id: f_00003-10-0 loss: 0.516689  [   32/   43]
train() client id: f_00003-11-0 loss: 0.396500  [   32/   43]
train() client id: f_00004-0-0 loss: 0.748737  [   32/  306]
train() client id: f_00004-0-1 loss: 0.846651  [   64/  306]
train() client id: f_00004-0-2 loss: 0.775121  [   96/  306]
train() client id: f_00004-0-3 loss: 0.801322  [  128/  306]
train() client id: f_00004-0-4 loss: 0.715151  [  160/  306]
train() client id: f_00004-0-5 loss: 0.865987  [  192/  306]
train() client id: f_00004-0-6 loss: 0.812302  [  224/  306]
train() client id: f_00004-0-7 loss: 0.909156  [  256/  306]
train() client id: f_00004-0-8 loss: 0.938727  [  288/  306]
train() client id: f_00004-1-0 loss: 0.795621  [   32/  306]
train() client id: f_00004-1-1 loss: 1.006608  [   64/  306]
train() client id: f_00004-1-2 loss: 0.850156  [   96/  306]
train() client id: f_00004-1-3 loss: 0.837096  [  128/  306]
train() client id: f_00004-1-4 loss: 0.706282  [  160/  306]
train() client id: f_00004-1-5 loss: 0.922674  [  192/  306]
train() client id: f_00004-1-6 loss: 0.709783  [  224/  306]
train() client id: f_00004-1-7 loss: 0.640770  [  256/  306]
train() client id: f_00004-1-8 loss: 0.877400  [  288/  306]
train() client id: f_00004-2-0 loss: 0.801733  [   32/  306]
train() client id: f_00004-2-1 loss: 0.760211  [   64/  306]
train() client id: f_00004-2-2 loss: 0.799152  [   96/  306]
train() client id: f_00004-2-3 loss: 0.913297  [  128/  306]
train() client id: f_00004-2-4 loss: 0.772257  [  160/  306]
train() client id: f_00004-2-5 loss: 0.920055  [  192/  306]
train() client id: f_00004-2-6 loss: 0.709373  [  224/  306]
train() client id: f_00004-2-7 loss: 0.867922  [  256/  306]
train() client id: f_00004-2-8 loss: 0.779772  [  288/  306]
train() client id: f_00004-3-0 loss: 0.791828  [   32/  306]
train() client id: f_00004-3-1 loss: 0.891215  [   64/  306]
train() client id: f_00004-3-2 loss: 0.901355  [   96/  306]
train() client id: f_00004-3-3 loss: 0.717378  [  128/  306]
train() client id: f_00004-3-4 loss: 0.866223  [  160/  306]
train() client id: f_00004-3-5 loss: 0.739366  [  192/  306]
train() client id: f_00004-3-6 loss: 0.699864  [  224/  306]
train() client id: f_00004-3-7 loss: 0.832382  [  256/  306]
train() client id: f_00004-3-8 loss: 0.881726  [  288/  306]
train() client id: f_00004-4-0 loss: 0.788207  [   32/  306]
train() client id: f_00004-4-1 loss: 0.850571  [   64/  306]
train() client id: f_00004-4-2 loss: 0.709320  [   96/  306]
train() client id: f_00004-4-3 loss: 0.696618  [  128/  306]
train() client id: f_00004-4-4 loss: 0.809676  [  160/  306]
train() client id: f_00004-4-5 loss: 0.945176  [  192/  306]
train() client id: f_00004-4-6 loss: 0.793696  [  224/  306]
train() client id: f_00004-4-7 loss: 0.832329  [  256/  306]
train() client id: f_00004-4-8 loss: 0.765617  [  288/  306]
train() client id: f_00004-5-0 loss: 0.734314  [   32/  306]
train() client id: f_00004-5-1 loss: 0.915725  [   64/  306]
train() client id: f_00004-5-2 loss: 0.846434  [   96/  306]
train() client id: f_00004-5-3 loss: 0.784851  [  128/  306]
train() client id: f_00004-5-4 loss: 0.836661  [  160/  306]
train() client id: f_00004-5-5 loss: 0.851478  [  192/  306]
train() client id: f_00004-5-6 loss: 0.892169  [  224/  306]
train() client id: f_00004-5-7 loss: 0.731261  [  256/  306]
train() client id: f_00004-5-8 loss: 0.810707  [  288/  306]
train() client id: f_00004-6-0 loss: 0.786524  [   32/  306]
train() client id: f_00004-6-1 loss: 0.857892  [   64/  306]
train() client id: f_00004-6-2 loss: 0.904827  [   96/  306]
train() client id: f_00004-6-3 loss: 0.774701  [  128/  306]
train() client id: f_00004-6-4 loss: 0.821255  [  160/  306]
train() client id: f_00004-6-5 loss: 0.839679  [  192/  306]
train() client id: f_00004-6-6 loss: 0.757483  [  224/  306]
train() client id: f_00004-6-7 loss: 0.774993  [  256/  306]
train() client id: f_00004-6-8 loss: 0.770844  [  288/  306]
train() client id: f_00004-7-0 loss: 0.858293  [   32/  306]
train() client id: f_00004-7-1 loss: 0.847259  [   64/  306]
train() client id: f_00004-7-2 loss: 0.813057  [   96/  306]
train() client id: f_00004-7-3 loss: 0.779727  [  128/  306]
train() client id: f_00004-7-4 loss: 0.746005  [  160/  306]
train() client id: f_00004-7-5 loss: 0.773757  [  192/  306]
train() client id: f_00004-7-6 loss: 0.803670  [  224/  306]
train() client id: f_00004-7-7 loss: 0.820614  [  256/  306]
train() client id: f_00004-7-8 loss: 0.792257  [  288/  306]
train() client id: f_00004-8-0 loss: 0.828050  [   32/  306]
train() client id: f_00004-8-1 loss: 0.851297  [   64/  306]
train() client id: f_00004-8-2 loss: 0.781659  [   96/  306]
train() client id: f_00004-8-3 loss: 0.909600  [  128/  306]
train() client id: f_00004-8-4 loss: 0.786173  [  160/  306]
train() client id: f_00004-8-5 loss: 0.844106  [  192/  306]
train() client id: f_00004-8-6 loss: 0.871737  [  224/  306]
train() client id: f_00004-8-7 loss: 0.732535  [  256/  306]
train() client id: f_00004-8-8 loss: 0.765394  [  288/  306]
train() client id: f_00004-9-0 loss: 0.811107  [   32/  306]
train() client id: f_00004-9-1 loss: 0.912971  [   64/  306]
train() client id: f_00004-9-2 loss: 0.877185  [   96/  306]
train() client id: f_00004-9-3 loss: 0.756798  [  128/  306]
train() client id: f_00004-9-4 loss: 0.639987  [  160/  306]
train() client id: f_00004-9-5 loss: 0.725675  [  192/  306]
train() client id: f_00004-9-6 loss: 0.853320  [  224/  306]
train() client id: f_00004-9-7 loss: 0.768535  [  256/  306]
train() client id: f_00004-9-8 loss: 0.881175  [  288/  306]
train() client id: f_00004-10-0 loss: 0.844483  [   32/  306]
train() client id: f_00004-10-1 loss: 0.732827  [   64/  306]
train() client id: f_00004-10-2 loss: 0.742737  [   96/  306]
train() client id: f_00004-10-3 loss: 0.800404  [  128/  306]
train() client id: f_00004-10-4 loss: 0.799221  [  160/  306]
train() client id: f_00004-10-5 loss: 0.848906  [  192/  306]
train() client id: f_00004-10-6 loss: 0.836088  [  224/  306]
train() client id: f_00004-10-7 loss: 0.881988  [  256/  306]
train() client id: f_00004-10-8 loss: 0.826313  [  288/  306]
train() client id: f_00004-11-0 loss: 0.696298  [   32/  306]
train() client id: f_00004-11-1 loss: 0.872392  [   64/  306]
train() client id: f_00004-11-2 loss: 0.806366  [   96/  306]
train() client id: f_00004-11-3 loss: 0.805695  [  128/  306]
train() client id: f_00004-11-4 loss: 0.851400  [  160/  306]
train() client id: f_00004-11-5 loss: 0.818628  [  192/  306]
train() client id: f_00004-11-6 loss: 0.759655  [  224/  306]
train() client id: f_00004-11-7 loss: 0.874708  [  256/  306]
train() client id: f_00004-11-8 loss: 0.778464  [  288/  306]
train() client id: f_00005-0-0 loss: 0.644621  [   32/  146]
train() client id: f_00005-0-1 loss: 0.476848  [   64/  146]
train() client id: f_00005-0-2 loss: 0.769841  [   96/  146]
train() client id: f_00005-0-3 loss: 0.684580  [  128/  146]
train() client id: f_00005-1-0 loss: 0.708857  [   32/  146]
train() client id: f_00005-1-1 loss: 0.762530  [   64/  146]
train() client id: f_00005-1-2 loss: 0.452865  [   96/  146]
train() client id: f_00005-1-3 loss: 0.626193  [  128/  146]
train() client id: f_00005-2-0 loss: 0.744125  [   32/  146]
train() client id: f_00005-2-1 loss: 0.730428  [   64/  146]
train() client id: f_00005-2-2 loss: 0.542246  [   96/  146]
train() client id: f_00005-2-3 loss: 0.330546  [  128/  146]
train() client id: f_00005-3-0 loss: 0.654091  [   32/  146]
train() client id: f_00005-3-1 loss: 0.586736  [   64/  146]
train() client id: f_00005-3-2 loss: 0.500980  [   96/  146]
train() client id: f_00005-3-3 loss: 0.590077  [  128/  146]
train() client id: f_00005-4-0 loss: 0.629230  [   32/  146]
train() client id: f_00005-4-1 loss: 0.908686  [   64/  146]
train() client id: f_00005-4-2 loss: 0.461295  [   96/  146]
train() client id: f_00005-4-3 loss: 0.581256  [  128/  146]
train() client id: f_00005-5-0 loss: 0.638598  [   32/  146]
train() client id: f_00005-5-1 loss: 1.047467  [   64/  146]
train() client id: f_00005-5-2 loss: 0.476533  [   96/  146]
train() client id: f_00005-5-3 loss: 0.416723  [  128/  146]
train() client id: f_00005-6-0 loss: 0.538832  [   32/  146]
train() client id: f_00005-6-1 loss: 0.707693  [   64/  146]
train() client id: f_00005-6-2 loss: 0.643184  [   96/  146]
train() client id: f_00005-6-3 loss: 0.662928  [  128/  146]
train() client id: f_00005-7-0 loss: 0.438169  [   32/  146]
train() client id: f_00005-7-1 loss: 0.749279  [   64/  146]
train() client id: f_00005-7-2 loss: 0.703142  [   96/  146]
train() client id: f_00005-7-3 loss: 0.614731  [  128/  146]
train() client id: f_00005-8-0 loss: 0.542192  [   32/  146]
train() client id: f_00005-8-1 loss: 0.410614  [   64/  146]
train() client id: f_00005-8-2 loss: 0.810288  [   96/  146]
train() client id: f_00005-8-3 loss: 0.693880  [  128/  146]
train() client id: f_00005-9-0 loss: 0.605958  [   32/  146]
train() client id: f_00005-9-1 loss: 0.757575  [   64/  146]
train() client id: f_00005-9-2 loss: 0.618164  [   96/  146]
train() client id: f_00005-9-3 loss: 0.568300  [  128/  146]
train() client id: f_00005-10-0 loss: 0.673118  [   32/  146]
train() client id: f_00005-10-1 loss: 0.355930  [   64/  146]
train() client id: f_00005-10-2 loss: 0.434419  [   96/  146]
train() client id: f_00005-10-3 loss: 0.805993  [  128/  146]
train() client id: f_00005-11-0 loss: 0.622343  [   32/  146]
train() client id: f_00005-11-1 loss: 0.665011  [   64/  146]
train() client id: f_00005-11-2 loss: 0.560278  [   96/  146]
train() client id: f_00005-11-3 loss: 0.521551  [  128/  146]
train() client id: f_00006-0-0 loss: 0.582220  [   32/   54]
train() client id: f_00006-1-0 loss: 0.468897  [   32/   54]
train() client id: f_00006-2-0 loss: 0.561819  [   32/   54]
train() client id: f_00006-3-0 loss: 0.554577  [   32/   54]
train() client id: f_00006-4-0 loss: 0.559077  [   32/   54]
train() client id: f_00006-5-0 loss: 0.501989  [   32/   54]
train() client id: f_00006-6-0 loss: 0.564669  [   32/   54]
train() client id: f_00006-7-0 loss: 0.506841  [   32/   54]
train() client id: f_00006-8-0 loss: 0.495457  [   32/   54]
train() client id: f_00006-9-0 loss: 0.487825  [   32/   54]
train() client id: f_00006-10-0 loss: 0.563334  [   32/   54]
train() client id: f_00006-11-0 loss: 0.463424  [   32/   54]
train() client id: f_00007-0-0 loss: 0.548611  [   32/  179]
train() client id: f_00007-0-1 loss: 0.870588  [   64/  179]
train() client id: f_00007-0-2 loss: 0.514036  [   96/  179]
train() client id: f_00007-0-3 loss: 0.623283  [  128/  179]
train() client id: f_00007-0-4 loss: 0.727577  [  160/  179]
train() client id: f_00007-1-0 loss: 0.857915  [   32/  179]
train() client id: f_00007-1-1 loss: 0.500927  [   64/  179]
train() client id: f_00007-1-2 loss: 0.735219  [   96/  179]
train() client id: f_00007-1-3 loss: 0.552162  [  128/  179]
train() client id: f_00007-1-4 loss: 0.637591  [  160/  179]
train() client id: f_00007-2-0 loss: 0.652155  [   32/  179]
train() client id: f_00007-2-1 loss: 0.747883  [   64/  179]
train() client id: f_00007-2-2 loss: 0.504652  [   96/  179]
train() client id: f_00007-2-3 loss: 0.744628  [  128/  179]
train() client id: f_00007-2-4 loss: 0.602299  [  160/  179]
train() client id: f_00007-3-0 loss: 0.730203  [   32/  179]
train() client id: f_00007-3-1 loss: 0.544123  [   64/  179]
train() client id: f_00007-3-2 loss: 0.475281  [   96/  179]
train() client id: f_00007-3-3 loss: 0.763000  [  128/  179]
train() client id: f_00007-3-4 loss: 0.727658  [  160/  179]
train() client id: f_00007-4-0 loss: 0.623303  [   32/  179]
train() client id: f_00007-4-1 loss: 0.711540  [   64/  179]
train() client id: f_00007-4-2 loss: 0.475899  [   96/  179]
train() client id: f_00007-4-3 loss: 0.809921  [  128/  179]
train() client id: f_00007-4-4 loss: 0.549522  [  160/  179]
train() client id: f_00007-5-0 loss: 0.533181  [   32/  179]
train() client id: f_00007-5-1 loss: 0.660620  [   64/  179]
train() client id: f_00007-5-2 loss: 0.588247  [   96/  179]
train() client id: f_00007-5-3 loss: 0.640593  [  128/  179]
train() client id: f_00007-5-4 loss: 0.769240  [  160/  179]
train() client id: f_00007-6-0 loss: 0.528746  [   32/  179]
train() client id: f_00007-6-1 loss: 0.668449  [   64/  179]
train() client id: f_00007-6-2 loss: 0.642820  [   96/  179]
train() client id: f_00007-6-3 loss: 0.619665  [  128/  179]
train() client id: f_00007-6-4 loss: 0.681155  [  160/  179]
train() client id: f_00007-7-0 loss: 0.731598  [   32/  179]
train() client id: f_00007-7-1 loss: 0.567291  [   64/  179]
train() client id: f_00007-7-2 loss: 0.732253  [   96/  179]
train() client id: f_00007-7-3 loss: 0.422761  [  128/  179]
train() client id: f_00007-7-4 loss: 0.645461  [  160/  179]
train() client id: f_00007-8-0 loss: 0.483483  [   32/  179]
train() client id: f_00007-8-1 loss: 0.612846  [   64/  179]
train() client id: f_00007-8-2 loss: 0.647702  [   96/  179]
train() client id: f_00007-8-3 loss: 0.578400  [  128/  179]
train() client id: f_00007-8-4 loss: 0.794414  [  160/  179]
train() client id: f_00007-9-0 loss: 0.462260  [   32/  179]
train() client id: f_00007-9-1 loss: 0.610891  [   64/  179]
train() client id: f_00007-9-2 loss: 0.657638  [   96/  179]
train() client id: f_00007-9-3 loss: 0.569113  [  128/  179]
train() client id: f_00007-9-4 loss: 0.652377  [  160/  179]
train() client id: f_00007-10-0 loss: 0.540972  [   32/  179]
train() client id: f_00007-10-1 loss: 0.816782  [   64/  179]
train() client id: f_00007-10-2 loss: 0.510843  [   96/  179]
train() client id: f_00007-10-3 loss: 0.790405  [  128/  179]
train() client id: f_00007-10-4 loss: 0.446286  [  160/  179]
train() client id: f_00007-11-0 loss: 0.559025  [   32/  179]
train() client id: f_00007-11-1 loss: 0.582974  [   64/  179]
train() client id: f_00007-11-2 loss: 0.764600  [   96/  179]
train() client id: f_00007-11-3 loss: 0.690192  [  128/  179]
train() client id: f_00007-11-4 loss: 0.533150  [  160/  179]
train() client id: f_00008-0-0 loss: 0.779125  [   32/  130]
train() client id: f_00008-0-1 loss: 0.727676  [   64/  130]
train() client id: f_00008-0-2 loss: 0.908804  [   96/  130]
train() client id: f_00008-0-3 loss: 0.787106  [  128/  130]
train() client id: f_00008-1-0 loss: 0.842851  [   32/  130]
train() client id: f_00008-1-1 loss: 0.632633  [   64/  130]
train() client id: f_00008-1-2 loss: 0.835259  [   96/  130]
train() client id: f_00008-1-3 loss: 0.848034  [  128/  130]
train() client id: f_00008-2-0 loss: 0.835510  [   32/  130]
train() client id: f_00008-2-1 loss: 0.785910  [   64/  130]
train() client id: f_00008-2-2 loss: 0.828801  [   96/  130]
train() client id: f_00008-2-3 loss: 0.737641  [  128/  130]
train() client id: f_00008-3-0 loss: 0.767568  [   32/  130]
train() client id: f_00008-3-1 loss: 0.923546  [   64/  130]
train() client id: f_00008-3-2 loss: 0.754294  [   96/  130]
train() client id: f_00008-3-3 loss: 0.753211  [  128/  130]
train() client id: f_00008-4-0 loss: 0.714719  [   32/  130]
train() client id: f_00008-4-1 loss: 0.781587  [   64/  130]
train() client id: f_00008-4-2 loss: 0.830995  [   96/  130]
train() client id: f_00008-4-3 loss: 0.861199  [  128/  130]
train() client id: f_00008-5-0 loss: 0.842745  [   32/  130]
train() client id: f_00008-5-1 loss: 0.753101  [   64/  130]
train() client id: f_00008-5-2 loss: 0.742132  [   96/  130]
train() client id: f_00008-5-3 loss: 0.853048  [  128/  130]
train() client id: f_00008-6-0 loss: 0.871868  [   32/  130]
train() client id: f_00008-6-1 loss: 0.761493  [   64/  130]
train() client id: f_00008-6-2 loss: 0.767264  [   96/  130]
train() client id: f_00008-6-3 loss: 0.765651  [  128/  130]
train() client id: f_00008-7-0 loss: 0.894058  [   32/  130]
train() client id: f_00008-7-1 loss: 0.808312  [   64/  130]
train() client id: f_00008-7-2 loss: 0.689115  [   96/  130]
train() client id: f_00008-7-3 loss: 0.797257  [  128/  130]
train() client id: f_00008-8-0 loss: 0.722579  [   32/  130]
train() client id: f_00008-8-1 loss: 0.690834  [   64/  130]
train() client id: f_00008-8-2 loss: 0.896502  [   96/  130]
train() client id: f_00008-8-3 loss: 0.861428  [  128/  130]
train() client id: f_00008-9-0 loss: 0.657001  [   32/  130]
train() client id: f_00008-9-1 loss: 0.848884  [   64/  130]
train() client id: f_00008-9-2 loss: 0.775385  [   96/  130]
train() client id: f_00008-9-3 loss: 0.891250  [  128/  130]
train() client id: f_00008-10-0 loss: 0.810106  [   32/  130]
train() client id: f_00008-10-1 loss: 0.863006  [   64/  130]
train() client id: f_00008-10-2 loss: 0.806720  [   96/  130]
train() client id: f_00008-10-3 loss: 0.705923  [  128/  130]
train() client id: f_00008-11-0 loss: 0.861212  [   32/  130]
train() client id: f_00008-11-1 loss: 0.692652  [   64/  130]
train() client id: f_00008-11-2 loss: 0.802253  [   96/  130]
train() client id: f_00008-11-3 loss: 0.789302  [  128/  130]
train() client id: f_00009-0-0 loss: 1.220162  [   32/  118]
train() client id: f_00009-0-1 loss: 1.164131  [   64/  118]
train() client id: f_00009-0-2 loss: 1.227176  [   96/  118]
train() client id: f_00009-1-0 loss: 1.168230  [   32/  118]
train() client id: f_00009-1-1 loss: 1.085213  [   64/  118]
train() client id: f_00009-1-2 loss: 1.255571  [   96/  118]
train() client id: f_00009-2-0 loss: 1.108183  [   32/  118]
train() client id: f_00009-2-1 loss: 1.250403  [   64/  118]
train() client id: f_00009-2-2 loss: 1.026578  [   96/  118]
train() client id: f_00009-3-0 loss: 1.105126  [   32/  118]
train() client id: f_00009-3-1 loss: 1.025104  [   64/  118]
train() client id: f_00009-3-2 loss: 1.109766  [   96/  118]
train() client id: f_00009-4-0 loss: 1.281804  [   32/  118]
train() client id: f_00009-4-1 loss: 0.976933  [   64/  118]
train() client id: f_00009-4-2 loss: 0.960315  [   96/  118]
train() client id: f_00009-5-0 loss: 1.012669  [   32/  118]
train() client id: f_00009-5-1 loss: 0.981225  [   64/  118]
train() client id: f_00009-5-2 loss: 1.072379  [   96/  118]
train() client id: f_00009-6-0 loss: 0.928260  [   32/  118]
train() client id: f_00009-6-1 loss: 1.023661  [   64/  118]
train() client id: f_00009-6-2 loss: 1.110591  [   96/  118]
train() client id: f_00009-7-0 loss: 1.009631  [   32/  118]
train() client id: f_00009-7-1 loss: 0.954401  [   64/  118]
train() client id: f_00009-7-2 loss: 1.057496  [   96/  118]
train() client id: f_00009-8-0 loss: 0.942793  [   32/  118]
train() client id: f_00009-8-1 loss: 0.946505  [   64/  118]
train() client id: f_00009-8-2 loss: 0.936149  [   96/  118]
train() client id: f_00009-9-0 loss: 0.935157  [   32/  118]
train() client id: f_00009-9-1 loss: 1.066522  [   64/  118]
train() client id: f_00009-9-2 loss: 0.936232  [   96/  118]
train() client id: f_00009-10-0 loss: 1.060890  [   32/  118]
train() client id: f_00009-10-1 loss: 0.952503  [   64/  118]
train() client id: f_00009-10-2 loss: 0.920280  [   96/  118]
train() client id: f_00009-11-0 loss: 1.148228  [   32/  118]
train() client id: f_00009-11-1 loss: 0.945501  [   64/  118]
train() client id: f_00009-11-2 loss: 0.876182  [   96/  118]
At round 46 accuracy: 0.649867374005305
At round 46 training accuracy: 0.5902079141515761
At round 46 training loss: 0.8321220987377926
update_location
xs = -3.905658 4.200318 250.009024 18.811294 0.979296 3.956410 -212.443192 -191.324852 234.663977 -177.060879 
ys = 242.587959 225.555839 1.320614 -212.455176 204.350187 187.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 262.419840 246.765231 269.269857 235.565419 227.508149 212.814023 234.816951 215.883939 255.687020 203.387725 
dists_bs = 183.960338 186.195185 459.126562 433.225497 178.410051 179.414601 181.528586 174.941005 438.872523 171.010937 
uav_gains = -112.384465 -111.023523 -113.003964 -110.124065 -109.521078 -108.505862 -110.066504 -108.710294 -111.787044 -107.896284 
bs_gains = -102.979337 -103.126176 -114.101167 -113.395051 -102.606800 -102.675077 -102.817520 -102.368024 -113.552534 -102.091728 
Round 47
-------------------------------
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.7932913   9.84287299  4.72759004  1.71436679 11.34993316  5.46069336
  2.11875437  6.70878283  4.95608411  4.42774153]
obj_prev = 56.1001104698108
eta_min = 5.969830954362688e-20	eta_max = 0.9382327126425405
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 12.978344451772108	eta = 0.9090909090909091
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 25.625037120999544	eta = 0.4604284044719558
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 19.192856434060143	eta = 0.6147336638864522
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 18.0285169675076	eta = 0.6544351361468406
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 17.963027394397052	eta = 0.6568210745943969
af = 11.79849495615646	bf = 1.2050321534900845	zeta = 17.962800013672382	eta = 0.6568293889135345
eta = 0.6568293889135345
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [0.0346847  0.07294799 0.03413415 0.01183684 0.08423428 0.04019021
 0.01486487 0.0492743  0.03578581 0.0324825 ]
ene_total = [1.66925633 2.82340307 1.67271292 0.79630461 3.21465228 1.66466235
 0.90143074 2.08079959 1.75289976 1.38667837]
ti_comp = [0.65804615 0.71288711 0.65155892 0.67749658 0.71463044 0.71440598
 0.67791976 0.68705877 0.6453014  0.71627974]
ti_coms = [0.12724232 0.07240136 0.13372954 0.10779188 0.07065803 0.07088248
 0.1073687  0.09822969 0.13998706 0.06900873]
t_total = [27.64980278 27.64980278 27.64980278 27.64980278 27.64980278 27.64980278
 27.64980278 27.64980278 27.64980278 27.64980278]
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [6.02256253e-06 4.77395682e-05 5.85517725e-06 2.25825789e-07
 7.31448314e-05 7.94970091e-06 4.46690869e-07 1.58399461e-05
 6.87839679e-06 4.17506343e-06]
ene_total = [0.44822894 0.2566042  0.4710644  0.37954037 0.25136048 0.24985529
 0.37805813 0.34642191 0.493133   0.24312494]
optimize_network iter = 0 obj = 3.5173916587449052
eta = 0.6568293889135345
freqs = [26354310.22875877 51163774.23769676 26194218.65211249  8735722.05600123
 58935554.78523942 28128413.29923534 10963593.51009882 35858870.20763274
 27727978.21502854 22674452.1605431 ]
eta_min = 0.6568293889135383	eta_max = 0.6851522482532448
af = 0.0048373581457900995	bf = 1.2050321534900845	zeta = 0.00532109396036911	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [1.36561793e-06 1.08249620e-05 1.32766327e-06 5.12060679e-08
 1.65856133e-05 1.80259716e-06 1.01287294e-07 3.59171272e-06
 1.55967862e-06 9.46696933e-07]
ene_total = [1.69139003 0.96374279 1.77760827 1.4326949  0.9413374  0.94235589
 1.42707694 1.30607207 1.86080934 0.91733759]
ti_comp = [0.59323403 0.64807499 0.5867468  0.61268446 0.64981832 0.64959386
 0.61310764 0.62224665 0.58048928 0.65146762]
ti_coms = [0.12724232 0.07240136 0.13372954 0.10779188 0.07065803 0.07088248
 0.1073687  0.09822969 0.13998706 0.06900873]
t_total = [27.64980278 27.64980278 27.64980278 27.64980278 27.64980278 27.64980278
 27.64980278 27.64980278 27.64980278 27.64980278]
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [5.39431995e-06 4.20498333e-05 5.25582343e-06 2.01005902e-07
 6.43958017e-05 6.99925668e-06 3.97543809e-07 1.40575984e-05
 6.18755144e-06 3.67398631e-06]
ene_total = [0.4885263  0.27946926 0.51341709 0.41368185 0.27363643 0.27229513
 0.41206534 0.3775167  0.53746741 0.26497658]
optimize_network iter = 1 obj = 3.8330520913049613
eta = 0.6851522482532448
freqs = [26297444.52410097 50627871.73999517 26166161.04026021  8689617.22720056
 58304024.4554098  27827872.27421467 10905010.46336445 35617179.03212754
 27727978.21502853 22426331.23656411]
eta_min = 0.6851522482532453	eta_max = 0.6851522482532441
af = 0.0047490407946122245	bf = 1.2050321534900845	zeta = 0.005223944874073448	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [1.35973100e-06 1.05993827e-05 1.32482057e-06 5.06669901e-08
 1.62320679e-05 1.76428286e-06 1.00207745e-07 3.54345912e-06
 1.55967862e-06 9.26091353e-07]
ene_total = [1.69138925 0.96371281 1.77760789 1.43269482 0.94129041 0.9423508
 1.4270768  1.30606566 1.86080934 0.91733486]
ti_comp = [0.59323403 0.64807499 0.5867468  0.61268446 0.64981832 0.64959386
 0.61310764 0.62224665 0.58048928 0.65146762]
ti_coms = [0.12724232 0.07240136 0.13372954 0.10779188 0.07065803 0.07088248
 0.1073687  0.09822969 0.13998706 0.06900873]
t_total = [27.64980278 27.64980278 27.64980278 27.64980278 27.64980278 27.64980278
 27.64980278 27.64980278 27.64980278 27.64980278]
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [5.39431995e-06 4.20498333e-05 5.25582343e-06 2.01005902e-07
 6.43958017e-05 6.99925668e-06 3.97543809e-07 1.40575984e-05
 6.18755144e-06 3.67398631e-06]
ene_total = [0.4885263  0.27946926 0.51341709 0.41368185 0.27363643 0.27229513
 0.41206534 0.3775167  0.53746741 0.26497658]
optimize_network iter = 2 obj = 3.8330520913049524
eta = 0.6851522482532441
freqs = [26297444.52410097 50627871.73999519 26166161.04026021  8689617.22720056
 58304024.45540981 27827872.27421467 10905010.46336445 35617179.03212754
 27727978.21502853 22426331.23656412]
Done!
ene_coms = [0.01272423 0.00724014 0.01337295 0.01077919 0.0070658  0.00708825
 0.01073687 0.00982297 0.01399871 0.00690087]
ene_comp = [5.22816025e-06 4.07545842e-05 5.09392980e-06 1.94814375e-07
 6.24122362e-05 6.78366056e-06 3.85298380e-07 1.36245862e-05
 5.99695805e-06 3.56081756e-06]
ene_total = [0.01272946 0.00728089 0.01337805 0.01077938 0.00712821 0.00709503
 0.01073726 0.00983659 0.0140047  0.00690443]
At round 47 energy consumption: 0.09987401390775219
At round 47 eta: 0.6851522482532441
At round 47 a_n: 12.082948048696464
At round 47 local rounds: 12.381380128537735
At round 47 global rounds: 38.37711395955986
gradient difference: 0.40672439336776733
train() client id: f_00000-0-0 loss: 1.240256  [   32/  126]
train() client id: f_00000-0-1 loss: 1.053105  [   64/  126]
train() client id: f_00000-0-2 loss: 1.173472  [   96/  126]
train() client id: f_00000-1-0 loss: 0.932656  [   32/  126]
train() client id: f_00000-1-1 loss: 1.371923  [   64/  126]
train() client id: f_00000-1-2 loss: 1.106390  [   96/  126]
train() client id: f_00000-2-0 loss: 1.322798  [   32/  126]
train() client id: f_00000-2-1 loss: 0.847961  [   64/  126]
train() client id: f_00000-2-2 loss: 0.960317  [   96/  126]
train() client id: f_00000-3-0 loss: 0.957539  [   32/  126]
train() client id: f_00000-3-1 loss: 1.014956  [   64/  126]
train() client id: f_00000-3-2 loss: 0.880083  [   96/  126]
train() client id: f_00000-4-0 loss: 0.777236  [   32/  126]
train() client id: f_00000-4-1 loss: 0.974741  [   64/  126]
train() client id: f_00000-4-2 loss: 0.865409  [   96/  126]
train() client id: f_00000-5-0 loss: 0.920020  [   32/  126]
train() client id: f_00000-5-1 loss: 0.832459  [   64/  126]
train() client id: f_00000-5-2 loss: 0.840828  [   96/  126]
train() client id: f_00000-6-0 loss: 0.926032  [   32/  126]
train() client id: f_00000-6-1 loss: 0.869674  [   64/  126]
train() client id: f_00000-6-2 loss: 0.854997  [   96/  126]
train() client id: f_00000-7-0 loss: 0.971850  [   32/  126]
train() client id: f_00000-7-1 loss: 0.783601  [   64/  126]
train() client id: f_00000-7-2 loss: 0.802682  [   96/  126]
train() client id: f_00000-8-0 loss: 0.802140  [   32/  126]
train() client id: f_00000-8-1 loss: 0.728574  [   64/  126]
train() client id: f_00000-8-2 loss: 0.817251  [   96/  126]
train() client id: f_00000-9-0 loss: 0.801128  [   32/  126]
train() client id: f_00000-9-1 loss: 0.759964  [   64/  126]
train() client id: f_00000-9-2 loss: 0.848259  [   96/  126]
train() client id: f_00000-10-0 loss: 0.798087  [   32/  126]
train() client id: f_00000-10-1 loss: 0.628255  [   64/  126]
train() client id: f_00000-10-2 loss: 0.944480  [   96/  126]
train() client id: f_00000-11-0 loss: 0.717934  [   32/  126]
train() client id: f_00000-11-1 loss: 0.720642  [   64/  126]
train() client id: f_00000-11-2 loss: 0.779452  [   96/  126]
train() client id: f_00001-0-0 loss: 0.245184  [   32/  265]
train() client id: f_00001-0-1 loss: 0.237052  [   64/  265]
train() client id: f_00001-0-2 loss: 0.271730  [   96/  265]
train() client id: f_00001-0-3 loss: 0.303227  [  128/  265]
train() client id: f_00001-0-4 loss: 0.236284  [  160/  265]
train() client id: f_00001-0-5 loss: 0.200035  [  192/  265]
train() client id: f_00001-0-6 loss: 0.165851  [  224/  265]
train() client id: f_00001-0-7 loss: 0.246177  [  256/  265]
train() client id: f_00001-1-0 loss: 0.260805  [   32/  265]
train() client id: f_00001-1-1 loss: 0.227059  [   64/  265]
train() client id: f_00001-1-2 loss: 0.197788  [   96/  265]
train() client id: f_00001-1-3 loss: 0.273973  [  128/  265]
train() client id: f_00001-1-4 loss: 0.210476  [  160/  265]
train() client id: f_00001-1-5 loss: 0.206150  [  192/  265]
train() client id: f_00001-1-6 loss: 0.161860  [  224/  265]
train() client id: f_00001-1-7 loss: 0.261635  [  256/  265]
train() client id: f_00001-2-0 loss: 0.147414  [   32/  265]
train() client id: f_00001-2-1 loss: 0.127928  [   64/  265]
train() client id: f_00001-2-2 loss: 0.259057  [   96/  265]
train() client id: f_00001-2-3 loss: 0.410063  [  128/  265]
train() client id: f_00001-2-4 loss: 0.208225  [  160/  265]
train() client id: f_00001-2-5 loss: 0.156169  [  192/  265]
train() client id: f_00001-2-6 loss: 0.138589  [  224/  265]
train() client id: f_00001-2-7 loss: 0.266326  [  256/  265]
train() client id: f_00001-3-0 loss: 0.226557  [   32/  265]
train() client id: f_00001-3-1 loss: 0.121896  [   64/  265]
train() client id: f_00001-3-2 loss: 0.241530  [   96/  265]
train() client id: f_00001-3-3 loss: 0.107496  [  128/  265]
train() client id: f_00001-3-4 loss: 0.371060  [  160/  265]
train() client id: f_00001-3-5 loss: 0.241005  [  192/  265]
train() client id: f_00001-3-6 loss: 0.159453  [  224/  265]
train() client id: f_00001-3-7 loss: 0.265570  [  256/  265]
train() client id: f_00001-4-0 loss: 0.137172  [   32/  265]
train() client id: f_00001-4-1 loss: 0.330357  [   64/  265]
train() client id: f_00001-4-2 loss: 0.204374  [   96/  265]
train() client id: f_00001-4-3 loss: 0.168541  [  128/  265]
train() client id: f_00001-4-4 loss: 0.203852  [  160/  265]
train() client id: f_00001-4-5 loss: 0.210007  [  192/  265]
train() client id: f_00001-4-6 loss: 0.179177  [  224/  265]
train() client id: f_00001-4-7 loss: 0.131373  [  256/  265]
train() client id: f_00001-5-0 loss: 0.103275  [   32/  265]
train() client id: f_00001-5-1 loss: 0.261672  [   64/  265]
train() client id: f_00001-5-2 loss: 0.227857  [   96/  265]
train() client id: f_00001-5-3 loss: 0.115549  [  128/  265]
train() client id: f_00001-5-4 loss: 0.117894  [  160/  265]
train() client id: f_00001-5-5 loss: 0.512751  [  192/  265]
train() client id: f_00001-5-6 loss: 0.151585  [  224/  265]
train() client id: f_00001-5-7 loss: 0.159413  [  256/  265]
train() client id: f_00001-6-0 loss: 0.167731  [   32/  265]
train() client id: f_00001-6-1 loss: 0.290730  [   64/  265]
train() client id: f_00001-6-2 loss: 0.125099  [   96/  265]
train() client id: f_00001-6-3 loss: 0.209248  [  128/  265]
train() client id: f_00001-6-4 loss: 0.181950  [  160/  265]
train() client id: f_00001-6-5 loss: 0.145617  [  192/  265]
train() client id: f_00001-6-6 loss: 0.185751  [  224/  265]
train() client id: f_00001-6-7 loss: 0.189633  [  256/  265]
train() client id: f_00001-7-0 loss: 0.109732  [   32/  265]
train() client id: f_00001-7-1 loss: 0.446072  [   64/  265]
train() client id: f_00001-7-2 loss: 0.249455  [   96/  265]
train() client id: f_00001-7-3 loss: 0.311268  [  128/  265]
train() client id: f_00001-7-4 loss: 0.090821  [  160/  265]
train() client id: f_00001-7-5 loss: 0.090179  [  192/  265]
train() client id: f_00001-7-6 loss: 0.105769  [  224/  265]
train() client id: f_00001-7-7 loss: 0.186316  [  256/  265]
train() client id: f_00001-8-0 loss: 0.232355  [   32/  265]
train() client id: f_00001-8-1 loss: 0.141031  [   64/  265]
train() client id: f_00001-8-2 loss: 0.112785  [   96/  265]
train() client id: f_00001-8-3 loss: 0.284167  [  128/  265]
train() client id: f_00001-8-4 loss: 0.263360  [  160/  265]
train() client id: f_00001-8-5 loss: 0.271253  [  192/  265]
train() client id: f_00001-8-6 loss: 0.162026  [  224/  265]
train() client id: f_00001-8-7 loss: 0.095348  [  256/  265]
train() client id: f_00001-9-0 loss: 0.153028  [   32/  265]
train() client id: f_00001-9-1 loss: 0.169234  [   64/  265]
train() client id: f_00001-9-2 loss: 0.264337  [   96/  265]
train() client id: f_00001-9-3 loss: 0.089407  [  128/  265]
train() client id: f_00001-9-4 loss: 0.401875  [  160/  265]
train() client id: f_00001-9-5 loss: 0.150394  [  192/  265]
train() client id: f_00001-9-6 loss: 0.094264  [  224/  265]
train() client id: f_00001-9-7 loss: 0.134384  [  256/  265]
train() client id: f_00001-10-0 loss: 0.159283  [   32/  265]
train() client id: f_00001-10-1 loss: 0.163055  [   64/  265]
train() client id: f_00001-10-2 loss: 0.157733  [   96/  265]
train() client id: f_00001-10-3 loss: 0.099527  [  128/  265]
train() client id: f_00001-10-4 loss: 0.352844  [  160/  265]
train() client id: f_00001-10-5 loss: 0.218273  [  192/  265]
train() client id: f_00001-10-6 loss: 0.159398  [  224/  265]
train() client id: f_00001-10-7 loss: 0.188790  [  256/  265]
train() client id: f_00001-11-0 loss: 0.219165  [   32/  265]
train() client id: f_00001-11-1 loss: 0.144261  [   64/  265]
train() client id: f_00001-11-2 loss: 0.116163  [   96/  265]
train() client id: f_00001-11-3 loss: 0.329840  [  128/  265]
train() client id: f_00001-11-4 loss: 0.102861  [  160/  265]
train() client id: f_00001-11-5 loss: 0.200125  [  192/  265]
train() client id: f_00001-11-6 loss: 0.160852  [  224/  265]
train() client id: f_00001-11-7 loss: 0.249685  [  256/  265]
train() client id: f_00002-0-0 loss: 1.099555  [   32/  124]
train() client id: f_00002-0-1 loss: 1.125431  [   64/  124]
train() client id: f_00002-0-2 loss: 1.033358  [   96/  124]
train() client id: f_00002-1-0 loss: 1.264508  [   32/  124]
train() client id: f_00002-1-1 loss: 0.940035  [   64/  124]
train() client id: f_00002-1-2 loss: 0.741097  [   96/  124]
train() client id: f_00002-2-0 loss: 0.880515  [   32/  124]
train() client id: f_00002-2-1 loss: 1.061871  [   64/  124]
train() client id: f_00002-2-2 loss: 0.915176  [   96/  124]
train() client id: f_00002-3-0 loss: 0.999542  [   32/  124]
train() client id: f_00002-3-1 loss: 0.997706  [   64/  124]
train() client id: f_00002-3-2 loss: 0.778513  [   96/  124]
train() client id: f_00002-4-0 loss: 0.808333  [   32/  124]
train() client id: f_00002-4-1 loss: 1.098771  [   64/  124]
train() client id: f_00002-4-2 loss: 0.886787  [   96/  124]
train() client id: f_00002-5-0 loss: 0.804198  [   32/  124]
train() client id: f_00002-5-1 loss: 0.966279  [   64/  124]
train() client id: f_00002-5-2 loss: 1.000260  [   96/  124]
train() client id: f_00002-6-0 loss: 1.003637  [   32/  124]
train() client id: f_00002-6-1 loss: 1.011292  [   64/  124]
train() client id: f_00002-6-2 loss: 0.701316  [   96/  124]
train() client id: f_00002-7-0 loss: 0.881769  [   32/  124]
train() client id: f_00002-7-1 loss: 0.926156  [   64/  124]
train() client id: f_00002-7-2 loss: 0.884458  [   96/  124]
train() client id: f_00002-8-0 loss: 0.859575  [   32/  124]
train() client id: f_00002-8-1 loss: 0.816215  [   64/  124]
train() client id: f_00002-8-2 loss: 0.873361  [   96/  124]
train() client id: f_00002-9-0 loss: 0.881066  [   32/  124]
train() client id: f_00002-9-1 loss: 0.848290  [   64/  124]
train() client id: f_00002-9-2 loss: 0.778496  [   96/  124]
train() client id: f_00002-10-0 loss: 0.623506  [   32/  124]
train() client id: f_00002-10-1 loss: 0.960593  [   64/  124]
train() client id: f_00002-10-2 loss: 0.738099  [   96/  124]
train() client id: f_00002-11-0 loss: 0.773378  [   32/  124]
train() client id: f_00002-11-1 loss: 0.900812  [   64/  124]
train() client id: f_00002-11-2 loss: 0.711068  [   96/  124]
train() client id: f_00003-0-0 loss: 0.819409  [   32/   43]
train() client id: f_00003-1-0 loss: 0.705571  [   32/   43]
train() client id: f_00003-2-0 loss: 0.916573  [   32/   43]
train() client id: f_00003-3-0 loss: 0.757167  [   32/   43]
train() client id: f_00003-4-0 loss: 0.830501  [   32/   43]
train() client id: f_00003-5-0 loss: 0.921418  [   32/   43]
train() client id: f_00003-6-0 loss: 0.783401  [   32/   43]
train() client id: f_00003-7-0 loss: 0.827602  [   32/   43]
train() client id: f_00003-8-0 loss: 0.915355  [   32/   43]
train() client id: f_00003-9-0 loss: 0.892216  [   32/   43]
train() client id: f_00003-10-0 loss: 0.886876  [   32/   43]
train() client id: f_00003-11-0 loss: 0.637932  [   32/   43]
train() client id: f_00004-0-0 loss: 0.741091  [   32/  306]
train() client id: f_00004-0-1 loss: 0.841958  [   64/  306]
train() client id: f_00004-0-2 loss: 0.811785  [   96/  306]
train() client id: f_00004-0-3 loss: 0.894979  [  128/  306]
train() client id: f_00004-0-4 loss: 0.902869  [  160/  306]
train() client id: f_00004-0-5 loss: 0.873762  [  192/  306]
train() client id: f_00004-0-6 loss: 0.751459  [  224/  306]
train() client id: f_00004-0-7 loss: 0.819842  [  256/  306]
train() client id: f_00004-0-8 loss: 0.937055  [  288/  306]
train() client id: f_00004-1-0 loss: 0.820207  [   32/  306]
train() client id: f_00004-1-1 loss: 0.813083  [   64/  306]
train() client id: f_00004-1-2 loss: 0.789733  [   96/  306]
train() client id: f_00004-1-3 loss: 0.791946  [  128/  306]
train() client id: f_00004-1-4 loss: 0.956638  [  160/  306]
train() client id: f_00004-1-5 loss: 0.871880  [  192/  306]
train() client id: f_00004-1-6 loss: 0.883814  [  224/  306]
train() client id: f_00004-1-7 loss: 0.790385  [  256/  306]
train() client id: f_00004-1-8 loss: 0.759373  [  288/  306]
train() client id: f_00004-2-0 loss: 0.686409  [   32/  306]
train() client id: f_00004-2-1 loss: 0.912326  [   64/  306]
train() client id: f_00004-2-2 loss: 0.856712  [   96/  306]
train() client id: f_00004-2-3 loss: 1.016065  [  128/  306]
train() client id: f_00004-2-4 loss: 0.780649  [  160/  306]
train() client id: f_00004-2-5 loss: 0.849632  [  192/  306]
train() client id: f_00004-2-6 loss: 0.841115  [  224/  306]
train() client id: f_00004-2-7 loss: 0.789776  [  256/  306]
train() client id: f_00004-2-8 loss: 0.771824  [  288/  306]
train() client id: f_00004-3-0 loss: 0.807850  [   32/  306]
train() client id: f_00004-3-1 loss: 0.707162  [   64/  306]
train() client id: f_00004-3-2 loss: 0.868778  [   96/  306]
train() client id: f_00004-3-3 loss: 0.843502  [  128/  306]
train() client id: f_00004-3-4 loss: 0.906793  [  160/  306]
train() client id: f_00004-3-5 loss: 0.773994  [  192/  306]
train() client id: f_00004-3-6 loss: 0.831127  [  224/  306]
train() client id: f_00004-3-7 loss: 0.916937  [  256/  306]
train() client id: f_00004-3-8 loss: 0.900698  [  288/  306]
train() client id: f_00004-4-0 loss: 0.960705  [   32/  306]
train() client id: f_00004-4-1 loss: 0.706868  [   64/  306]
train() client id: f_00004-4-2 loss: 0.958516  [   96/  306]
train() client id: f_00004-4-3 loss: 0.808012  [  128/  306]
train() client id: f_00004-4-4 loss: 0.780817  [  160/  306]
train() client id: f_00004-4-5 loss: 0.812544  [  192/  306]
train() client id: f_00004-4-6 loss: 0.793702  [  224/  306]
train() client id: f_00004-4-7 loss: 0.898485  [  256/  306]
train() client id: f_00004-4-8 loss: 0.851288  [  288/  306]
train() client id: f_00004-5-0 loss: 0.856082  [   32/  306]
train() client id: f_00004-5-1 loss: 0.863778  [   64/  306]
train() client id: f_00004-5-2 loss: 0.796430  [   96/  306]
train() client id: f_00004-5-3 loss: 0.795868  [  128/  306]
train() client id: f_00004-5-4 loss: 0.708530  [  160/  306]
train() client id: f_00004-5-5 loss: 0.739104  [  192/  306]
train() client id: f_00004-5-6 loss: 0.937087  [  224/  306]
train() client id: f_00004-5-7 loss: 0.956945  [  256/  306]
train() client id: f_00004-5-8 loss: 0.903376  [  288/  306]
train() client id: f_00004-6-0 loss: 0.699626  [   32/  306]
train() client id: f_00004-6-1 loss: 0.853860  [   64/  306]
train() client id: f_00004-6-2 loss: 0.853763  [   96/  306]
train() client id: f_00004-6-3 loss: 0.874282  [  128/  306]
train() client id: f_00004-6-4 loss: 0.852081  [  160/  306]
train() client id: f_00004-6-5 loss: 0.924620  [  192/  306]
train() client id: f_00004-6-6 loss: 0.943292  [  224/  306]
train() client id: f_00004-6-7 loss: 0.745416  [  256/  306]
train() client id: f_00004-6-8 loss: 0.727392  [  288/  306]
train() client id: f_00004-7-0 loss: 0.719479  [   32/  306]
train() client id: f_00004-7-1 loss: 0.924682  [   64/  306]
train() client id: f_00004-7-2 loss: 0.890297  [   96/  306]
train() client id: f_00004-7-3 loss: 0.744530  [  128/  306]
train() client id: f_00004-7-4 loss: 0.778584  [  160/  306]
train() client id: f_00004-7-5 loss: 0.770878  [  192/  306]
train() client id: f_00004-7-6 loss: 0.949672  [  224/  306]
train() client id: f_00004-7-7 loss: 0.982651  [  256/  306]
train() client id: f_00004-7-8 loss: 0.908486  [  288/  306]
train() client id: f_00004-8-0 loss: 0.824597  [   32/  306]
train() client id: f_00004-8-1 loss: 0.847009  [   64/  306]
train() client id: f_00004-8-2 loss: 0.803164  [   96/  306]
train() client id: f_00004-8-3 loss: 0.829604  [  128/  306]
train() client id: f_00004-8-4 loss: 0.843950  [  160/  306]
train() client id: f_00004-8-5 loss: 0.867269  [  192/  306]
train() client id: f_00004-8-6 loss: 0.831006  [  224/  306]
train() client id: f_00004-8-7 loss: 0.774892  [  256/  306]
train() client id: f_00004-8-8 loss: 0.928490  [  288/  306]
train() client id: f_00004-9-0 loss: 0.920405  [   32/  306]
train() client id: f_00004-9-1 loss: 0.729423  [   64/  306]
train() client id: f_00004-9-2 loss: 0.821094  [   96/  306]
train() client id: f_00004-9-3 loss: 0.792930  [  128/  306]
train() client id: f_00004-9-4 loss: 0.861720  [  160/  306]
train() client id: f_00004-9-5 loss: 0.863201  [  192/  306]
train() client id: f_00004-9-6 loss: 0.901183  [  224/  306]
train() client id: f_00004-9-7 loss: 0.854797  [  256/  306]
train() client id: f_00004-9-8 loss: 0.856234  [  288/  306]
train() client id: f_00004-10-0 loss: 0.845271  [   32/  306]
train() client id: f_00004-10-1 loss: 0.786024  [   64/  306]
train() client id: f_00004-10-2 loss: 0.905447  [   96/  306]
train() client id: f_00004-10-3 loss: 0.817369  [  128/  306]
train() client id: f_00004-10-4 loss: 0.750221  [  160/  306]
train() client id: f_00004-10-5 loss: 0.901282  [  192/  306]
train() client id: f_00004-10-6 loss: 0.930147  [  224/  306]
train() client id: f_00004-10-7 loss: 0.884432  [  256/  306]
train() client id: f_00004-10-8 loss: 0.804102  [  288/  306]
train() client id: f_00004-11-0 loss: 0.745416  [   32/  306]
train() client id: f_00004-11-1 loss: 0.776254  [   64/  306]
train() client id: f_00004-11-2 loss: 0.922810  [   96/  306]
train() client id: f_00004-11-3 loss: 0.860178  [  128/  306]
train() client id: f_00004-11-4 loss: 0.872641  [  160/  306]
train() client id: f_00004-11-5 loss: 0.850398  [  192/  306]
train() client id: f_00004-11-6 loss: 0.860500  [  224/  306]
train() client id: f_00004-11-7 loss: 0.879254  [  256/  306]
train() client id: f_00004-11-8 loss: 0.824844  [  288/  306]
train() client id: f_00005-0-0 loss: 0.843219  [   32/  146]
train() client id: f_00005-0-1 loss: 0.776963  [   64/  146]
train() client id: f_00005-0-2 loss: 0.421983  [   96/  146]
train() client id: f_00005-0-3 loss: 0.594036  [  128/  146]
train() client id: f_00005-1-0 loss: 0.922372  [   32/  146]
train() client id: f_00005-1-1 loss: 0.751239  [   64/  146]
train() client id: f_00005-1-2 loss: 0.546253  [   96/  146]
train() client id: f_00005-1-3 loss: 0.643912  [  128/  146]
train() client id: f_00005-2-0 loss: 0.639120  [   32/  146]
train() client id: f_00005-2-1 loss: 0.790865  [   64/  146]
train() client id: f_00005-2-2 loss: 0.769008  [   96/  146]
train() client id: f_00005-2-3 loss: 0.617220  [  128/  146]
train() client id: f_00005-3-0 loss: 0.804833  [   32/  146]
train() client id: f_00005-3-1 loss: 0.656744  [   64/  146]
train() client id: f_00005-3-2 loss: 0.744086  [   96/  146]
train() client id: f_00005-3-3 loss: 0.689777  [  128/  146]
train() client id: f_00005-4-0 loss: 0.553479  [   32/  146]
train() client id: f_00005-4-1 loss: 0.642468  [   64/  146]
train() client id: f_00005-4-2 loss: 0.916580  [   96/  146]
train() client id: f_00005-4-3 loss: 0.755161  [  128/  146]
train() client id: f_00005-5-0 loss: 0.768043  [   32/  146]
train() client id: f_00005-5-1 loss: 0.566989  [   64/  146]
train() client id: f_00005-5-2 loss: 0.645944  [   96/  146]
train() client id: f_00005-5-3 loss: 0.722525  [  128/  146]
train() client id: f_00005-6-0 loss: 0.600649  [   32/  146]
train() client id: f_00005-6-1 loss: 0.702956  [   64/  146]
train() client id: f_00005-6-2 loss: 0.630769  [   96/  146]
train() client id: f_00005-6-3 loss: 0.907273  [  128/  146]
train() client id: f_00005-7-0 loss: 0.472805  [   32/  146]
train() client id: f_00005-7-1 loss: 0.774615  [   64/  146]
train() client id: f_00005-7-2 loss: 0.520429  [   96/  146]
train() client id: f_00005-7-3 loss: 0.995290  [  128/  146]
train() client id: f_00005-8-0 loss: 0.748792  [   32/  146]
train() client id: f_00005-8-1 loss: 0.944625  [   64/  146]
train() client id: f_00005-8-2 loss: 0.443261  [   96/  146]
train() client id: f_00005-8-3 loss: 0.704024  [  128/  146]
train() client id: f_00005-9-0 loss: 0.748635  [   32/  146]
train() client id: f_00005-9-1 loss: 0.520160  [   64/  146]
train() client id: f_00005-9-2 loss: 0.725196  [   96/  146]
train() client id: f_00005-9-3 loss: 0.609379  [  128/  146]
train() client id: f_00005-10-0 loss: 0.806277  [   32/  146]
train() client id: f_00005-10-1 loss: 0.478502  [   64/  146]
train() client id: f_00005-10-2 loss: 0.735145  [   96/  146]
train() client id: f_00005-10-3 loss: 0.633458  [  128/  146]
train() client id: f_00005-11-0 loss: 0.867619  [   32/  146]
train() client id: f_00005-11-1 loss: 0.516728  [   64/  146]
train() client id: f_00005-11-2 loss: 0.533335  [   96/  146]
train() client id: f_00005-11-3 loss: 0.835437  [  128/  146]
train() client id: f_00006-0-0 loss: 0.538668  [   32/   54]
train() client id: f_00006-1-0 loss: 0.604248  [   32/   54]
train() client id: f_00006-2-0 loss: 0.602771  [   32/   54]
train() client id: f_00006-3-0 loss: 0.546675  [   32/   54]
train() client id: f_00006-4-0 loss: 0.510406  [   32/   54]
train() client id: f_00006-5-0 loss: 0.603493  [   32/   54]
train() client id: f_00006-6-0 loss: 0.553391  [   32/   54]
train() client id: f_00006-7-0 loss: 0.546674  [   32/   54]
train() client id: f_00006-8-0 loss: 0.552620  [   32/   54]
train() client id: f_00006-9-0 loss: 0.543537  [   32/   54]
train() client id: f_00006-10-0 loss: 0.601054  [   32/   54]
train() client id: f_00006-11-0 loss: 0.563986  [   32/   54]
train() client id: f_00007-0-0 loss: 0.677001  [   32/  179]
train() client id: f_00007-0-1 loss: 0.672133  [   64/  179]
train() client id: f_00007-0-2 loss: 0.636378  [   96/  179]
train() client id: f_00007-0-3 loss: 0.570371  [  128/  179]
train() client id: f_00007-0-4 loss: 0.785642  [  160/  179]
train() client id: f_00007-1-0 loss: 0.671394  [   32/  179]
train() client id: f_00007-1-1 loss: 0.697275  [   64/  179]
train() client id: f_00007-1-2 loss: 0.558781  [   96/  179]
train() client id: f_00007-1-3 loss: 0.840529  [  128/  179]
train() client id: f_00007-1-4 loss: 0.510669  [  160/  179]
train() client id: f_00007-2-0 loss: 0.577316  [   32/  179]
train() client id: f_00007-2-1 loss: 0.512253  [   64/  179]
train() client id: f_00007-2-2 loss: 0.840138  [   96/  179]
train() client id: f_00007-2-3 loss: 0.567133  [  128/  179]
train() client id: f_00007-2-4 loss: 0.656434  [  160/  179]
train() client id: f_00007-3-0 loss: 0.718267  [   32/  179]
train() client id: f_00007-3-1 loss: 0.485926  [   64/  179]
train() client id: f_00007-3-2 loss: 0.558915  [   96/  179]
train() client id: f_00007-3-3 loss: 0.576301  [  128/  179]
train() client id: f_00007-3-4 loss: 0.579012  [  160/  179]
train() client id: f_00007-4-0 loss: 0.650609  [   32/  179]
train() client id: f_00007-4-1 loss: 0.685246  [   64/  179]
train() client id: f_00007-4-2 loss: 0.729802  [   96/  179]
train() client id: f_00007-4-3 loss: 0.626718  [  128/  179]
train() client id: f_00007-4-4 loss: 0.444920  [  160/  179]
train() client id: f_00007-5-0 loss: 0.838271  [   32/  179]
train() client id: f_00007-5-1 loss: 0.649974  [   64/  179]
train() client id: f_00007-5-2 loss: 0.549308  [   96/  179]
train() client id: f_00007-5-3 loss: 0.536846  [  128/  179]
train() client id: f_00007-5-4 loss: 0.514288  [  160/  179]
train() client id: f_00007-6-0 loss: 0.568907  [   32/  179]
train() client id: f_00007-6-1 loss: 0.663090  [   64/  179]
train() client id: f_00007-6-2 loss: 0.467633  [   96/  179]
train() client id: f_00007-6-3 loss: 0.663722  [  128/  179]
train() client id: f_00007-6-4 loss: 0.686946  [  160/  179]
train() client id: f_00007-7-0 loss: 0.579591  [   32/  179]
train() client id: f_00007-7-1 loss: 0.563648  [   64/  179]
train() client id: f_00007-7-2 loss: 0.530121  [   96/  179]
train() client id: f_00007-7-3 loss: 0.706237  [  128/  179]
train() client id: f_00007-7-4 loss: 0.656280  [  160/  179]
train() client id: f_00007-8-0 loss: 0.536308  [   32/  179]
train() client id: f_00007-8-1 loss: 0.799685  [   64/  179]
train() client id: f_00007-8-2 loss: 0.448637  [   96/  179]
train() client id: f_00007-8-3 loss: 0.560701  [  128/  179]
train() client id: f_00007-8-4 loss: 0.484510  [  160/  179]
train() client id: f_00007-9-0 loss: 0.635637  [   32/  179]
train() client id: f_00007-9-1 loss: 0.572785  [   64/  179]
train() client id: f_00007-9-2 loss: 0.521406  [   96/  179]
train() client id: f_00007-9-3 loss: 0.764099  [  128/  179]
train() client id: f_00007-9-4 loss: 0.614794  [  160/  179]
train() client id: f_00007-10-0 loss: 0.684581  [   32/  179]
train() client id: f_00007-10-1 loss: 0.537400  [   64/  179]
train() client id: f_00007-10-2 loss: 0.814379  [   96/  179]
train() client id: f_00007-10-3 loss: 0.467440  [  128/  179]
train() client id: f_00007-10-4 loss: 0.586462  [  160/  179]
train() client id: f_00007-11-0 loss: 0.437311  [   32/  179]
train() client id: f_00007-11-1 loss: 0.685863  [   64/  179]
train() client id: f_00007-11-2 loss: 0.667573  [   96/  179]
train() client id: f_00007-11-3 loss: 0.592176  [  128/  179]
train() client id: f_00007-11-4 loss: 0.557320  [  160/  179]
train() client id: f_00008-0-0 loss: 0.633562  [   32/  130]
train() client id: f_00008-0-1 loss: 0.911701  [   64/  130]
train() client id: f_00008-0-2 loss: 0.631573  [   96/  130]
train() client id: f_00008-0-3 loss: 0.697381  [  128/  130]
train() client id: f_00008-1-0 loss: 0.786347  [   32/  130]
train() client id: f_00008-1-1 loss: 0.690028  [   64/  130]
train() client id: f_00008-1-2 loss: 0.658130  [   96/  130]
train() client id: f_00008-1-3 loss: 0.741651  [  128/  130]
train() client id: f_00008-2-0 loss: 0.807245  [   32/  130]
train() client id: f_00008-2-1 loss: 0.651903  [   64/  130]
train() client id: f_00008-2-2 loss: 0.676729  [   96/  130]
train() client id: f_00008-2-3 loss: 0.741545  [  128/  130]
train() client id: f_00008-3-0 loss: 0.735979  [   32/  130]
train() client id: f_00008-3-1 loss: 0.785331  [   64/  130]
train() client id: f_00008-3-2 loss: 0.634862  [   96/  130]
train() client id: f_00008-3-3 loss: 0.724349  [  128/  130]
train() client id: f_00008-4-0 loss: 0.783360  [   32/  130]
train() client id: f_00008-4-1 loss: 0.672370  [   64/  130]
train() client id: f_00008-4-2 loss: 0.633953  [   96/  130]
train() client id: f_00008-4-3 loss: 0.792660  [  128/  130]
train() client id: f_00008-5-0 loss: 0.766639  [   32/  130]
train() client id: f_00008-5-1 loss: 0.670650  [   64/  130]
train() client id: f_00008-5-2 loss: 0.731602  [   96/  130]
train() client id: f_00008-5-3 loss: 0.685215  [  128/  130]
train() client id: f_00008-6-0 loss: 0.703814  [   32/  130]
train() client id: f_00008-6-1 loss: 0.738756  [   64/  130]
train() client id: f_00008-6-2 loss: 0.766506  [   96/  130]
train() client id: f_00008-6-3 loss: 0.677016  [  128/  130]
train() client id: f_00008-7-0 loss: 0.642581  [   32/  130]
train() client id: f_00008-7-1 loss: 0.695195  [   64/  130]
train() client id: f_00008-7-2 loss: 0.739575  [   96/  130]
train() client id: f_00008-7-3 loss: 0.756862  [  128/  130]
train() client id: f_00008-8-0 loss: 0.611588  [   32/  130]
train() client id: f_00008-8-1 loss: 0.722958  [   64/  130]
train() client id: f_00008-8-2 loss: 0.832039  [   96/  130]
train() client id: f_00008-8-3 loss: 0.713595  [  128/  130]
train() client id: f_00008-9-0 loss: 0.769640  [   32/  130]
train() client id: f_00008-9-1 loss: 0.734868  [   64/  130]
train() client id: f_00008-9-2 loss: 0.639702  [   96/  130]
train() client id: f_00008-9-3 loss: 0.705042  [  128/  130]
train() client id: f_00008-10-0 loss: 0.693717  [   32/  130]
train() client id: f_00008-10-1 loss: 0.733355  [   64/  130]
train() client id: f_00008-10-2 loss: 0.772711  [   96/  130]
train() client id: f_00008-10-3 loss: 0.645147  [  128/  130]
train() client id: f_00008-11-0 loss: 0.897746  [   32/  130]
train() client id: f_00008-11-1 loss: 0.698258  [   64/  130]
train() client id: f_00008-11-2 loss: 0.673644  [   96/  130]
train() client id: f_00008-11-3 loss: 0.615429  [  128/  130]
train() client id: f_00009-0-0 loss: 0.997665  [   32/  118]
train() client id: f_00009-0-1 loss: 1.099769  [   64/  118]
train() client id: f_00009-0-2 loss: 1.164213  [   96/  118]
train() client id: f_00009-1-0 loss: 0.899743  [   32/  118]
train() client id: f_00009-1-1 loss: 1.190444  [   64/  118]
train() client id: f_00009-1-2 loss: 1.158630  [   96/  118]
train() client id: f_00009-2-0 loss: 0.980170  [   32/  118]
train() client id: f_00009-2-1 loss: 1.125225  [   64/  118]
train() client id: f_00009-2-2 loss: 1.071959  [   96/  118]
train() client id: f_00009-3-0 loss: 1.025900  [   32/  118]
train() client id: f_00009-3-1 loss: 1.097032  [   64/  118]
train() client id: f_00009-3-2 loss: 0.840000  [   96/  118]
train() client id: f_00009-4-0 loss: 0.893163  [   32/  118]
train() client id: f_00009-4-1 loss: 1.051292  [   64/  118]
train() client id: f_00009-4-2 loss: 0.933223  [   96/  118]
train() client id: f_00009-5-0 loss: 0.840680  [   32/  118]
train() client id: f_00009-5-1 loss: 0.968519  [   64/  118]
train() client id: f_00009-5-2 loss: 0.876704  [   96/  118]
train() client id: f_00009-6-0 loss: 0.787805  [   32/  118]
train() client id: f_00009-6-1 loss: 0.871099  [   64/  118]
train() client id: f_00009-6-2 loss: 1.032663  [   96/  118]
train() client id: f_00009-7-0 loss: 0.820157  [   32/  118]
train() client id: f_00009-7-1 loss: 0.893351  [   64/  118]
train() client id: f_00009-7-2 loss: 0.899382  [   96/  118]
train() client id: f_00009-8-0 loss: 0.939381  [   32/  118]
train() client id: f_00009-8-1 loss: 0.857291  [   64/  118]
train() client id: f_00009-8-2 loss: 0.941443  [   96/  118]
train() client id: f_00009-9-0 loss: 0.712331  [   32/  118]
train() client id: f_00009-9-1 loss: 0.985254  [   64/  118]
train() client id: f_00009-9-2 loss: 1.008236  [   96/  118]
train() client id: f_00009-10-0 loss: 0.911226  [   32/  118]
train() client id: f_00009-10-1 loss: 0.744209  [   64/  118]
train() client id: f_00009-10-2 loss: 1.032621  [   96/  118]
train() client id: f_00009-11-0 loss: 0.924585  [   32/  118]
train() client id: f_00009-11-1 loss: 0.821339  [   64/  118]
train() client id: f_00009-11-2 loss: 0.892235  [   96/  118]
At round 47 accuracy: 0.649867374005305
At round 47 training accuracy: 0.5875251509054326
At round 47 training loss: 0.8301363569655801
update_location
xs = -3.905658 4.200318 255.009024 18.811294 0.979296 3.956410 -217.443192 -196.324852 239.663977 -182.060879 
ys = 247.587959 230.555839 1.320614 -217.455176 209.350187 192.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 267.048782 251.343664 273.918503 240.084607 232.009611 217.239384 239.350020 220.327310 260.283483 207.755085 
dists_bs = 185.855550 187.614512 463.758870 437.702962 179.300441 179.840876 182.625463 175.478215 443.543382 171.144235 
uav_gains = -112.802252 -111.410707 -113.427282 -110.478464 -109.853462 -108.801701 -110.420058 -109.012813 -112.193340 -108.175798 
bs_gains = -103.103974 -103.218519 -114.223242 -113.520085 -102.667337 -102.703935 -102.890776 -102.405309 -113.681270 -102.101203 
Round 48
-------------------------------
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.66251584  9.56421031  4.59923771  1.66893827 11.02840386  5.30599829
  2.06183055  6.52085571  4.81741298  4.30225233]
obj_prev = 54.53165585566928
eta_min = 1.7212465954185627e-20	eta_max = 0.9392276891852349
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 12.610414541407936	eta = 0.909090909090909
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 25.13769025130001	eta = 0.4560487898791294
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 18.739989943316214	eta = 0.6117406281506839
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 17.582476098023903	eta = 0.6520135819067145
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 17.516919047777254	eta = 0.6544537420190021
af = 11.464013219461759	bf = 1.1926074070739463	zeta = 17.516687862363067	eta = 0.6544623795057578
eta = 0.6544623795057578
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [0.03498261 0.07357454 0.03442733 0.01193851 0.08495777 0.0405354
 0.01499255 0.04969752 0.03609317 0.03276149]
ene_total = [1.63559168 2.74693988 1.64037262 0.78097472 3.12737952 1.61844234
 0.88321539 2.02882541 1.7072293  1.347717  ]
ti_comp = [0.68075038 0.73958322 0.67379048 0.70183595 0.74144644 0.74132564
 0.70228536 0.71216487 0.67077194 0.74326503]
ti_coms = [0.13155303 0.07272019 0.13851293 0.11046746 0.07085697 0.07097777
 0.11001805 0.10013853 0.14153147 0.06903838]
t_total = [27.59979858 27.59979858 27.59979858 27.59979858 27.59979858 27.59979858
 27.59979858 27.59979858 27.59979858 27.59979858]
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [5.77378821e-06 4.55080801e-05 5.61746411e-06 2.15903231e-07
 6.97155574e-05 7.57470152e-06 4.27050309e-07 1.51259614e-05
 6.53138483e-06 3.97817259e-06]
ene_total = [0.4471766  0.2486291  0.47081906 0.37534488 0.24312091 0.24142
 0.37382507 0.34075668 0.48110627 0.23470829]
optimize_network iter = 0 obj = 3.4569068484449996
eta = 0.6544623795057578
freqs = [25694153.73552614 49740538.88381484 25547504.06148466  8505199.59149158
 57291910.54818947 27339810.52250793 10674113.26419133 34891861.85896511
 26904202.71882671 22038902.98735746]
eta_min = 0.6544623795057585	eta_max = 0.691983191142256
af = 0.004443917503062845	bf = 1.1926074070739463	zeta = 0.00488830925336913	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [1.29805932e-06 1.02310970e-05 1.26291463e-06 4.85392243e-08
 1.56734064e-05 1.70293948e-06 9.60091734e-08 3.40060882e-06
 1.46838170e-06 8.94370180e-07]
ene_total = [1.6991017  0.94046194 1.78898032 1.42663193 0.91710237 0.91685829
 1.42083414 1.2936725  1.8279896  0.89170773]
ti_comp = [0.59254498 0.65137783 0.58558508 0.61363055 0.65324104 0.65312024
 0.61407996 0.62395948 0.58256655 0.65505963]
ti_coms = [0.13155303 0.07272019 0.13851293 0.11046746 0.07085697 0.07097777
 0.11001805 0.10013853 0.14153147 0.06903838]
t_total = [27.59979858 27.59979858 27.59979858 27.59979858 27.59979858 27.59979858
 27.59979858 27.59979858 27.59979858 27.59979858]
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [4.99236055e-06 3.84333847e-05 4.87216508e-06 1.85024130e-07
 5.88375125e-05 6.39306671e-06 3.65905026e-07 1.29087227e-05
 5.67251856e-06 3.35522234e-06]
ene_total = [0.50161926 0.27864598 0.52814309 0.42106603 0.27232185 0.27078333
 0.41935993 0.38218116 0.53967909 0.26327534]
optimize_network iter = 1 obj = 3.877075061802886
eta = 0.691983191142256
freqs = [25637255.917499   49049571.69376329 25530186.95114224  8448581.49608057
 56476825.65917366 26951443.47001215 10602081.72934842 34587483.18659778
 26904202.71882672 21718183.03696004]
eta_min = 0.6919831911422635	eta_max = 0.6919831911422551
af = 0.004337069151020109	bf = 1.1926074070739463	zeta = 0.004770776066122121	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [1.29231677e-06 9.94882221e-06 1.26120311e-06 4.78951357e-08
 1.52306115e-05 1.65490197e-06 9.47177586e-08 3.34153728e-06
 1.46838170e-06 8.68529037e-07]
ene_total = [1.69910096 0.94042549 1.7889801  1.42663185 0.91704519 0.91685209
 1.42083397 1.29366487 1.8279896  0.8917044 ]
ti_comp = [0.59254498 0.65137783 0.58558508 0.61363055 0.65324104 0.65312024
 0.61407996 0.62395948 0.58256655 0.65505963]
ti_coms = [0.13155303 0.07272019 0.13851293 0.11046746 0.07085697 0.07097777
 0.11001805 0.10013853 0.14153147 0.06903838]
t_total = [27.59979858 27.59979858 27.59979858 27.59979858 27.59979858 27.59979858
 27.59979858 27.59979858 27.59979858 27.59979858]
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [4.99236055e-06 3.84333847e-05 4.87216508e-06 1.85024130e-07
 5.88375125e-05 6.39306671e-06 3.65905026e-07 1.29087227e-05
 5.67251856e-06 3.35522234e-06]
ene_total = [0.50161926 0.27864598 0.52814309 0.42106603 0.27232185 0.27078333
 0.41935993 0.38218116 0.53967909 0.26327534]
optimize_network iter = 2 obj = 3.8770750618028753
eta = 0.6919831911422551
freqs = [25637255.91749901 49049571.69376332 25530186.95114225  8448581.49608057
 56476825.6591737  26951443.47001216 10602081.72934843 34587483.18659779
 26904202.71882672 21718183.03696005]
Done!
ene_coms = [0.0131553  0.00727202 0.01385129 0.01104675 0.0070857  0.00709778
 0.0110018  0.01001385 0.01415315 0.00690384]
ene_comp = [4.96895282e-06 3.82531817e-05 4.84932092e-06 1.84156606e-07
 5.85616405e-05 6.36309147e-06 3.64189404e-07 1.28481975e-05
 5.64592177e-06 3.33949068e-06]
ene_total = [0.01316027 0.00731027 0.01385614 0.01104693 0.00714426 0.00710414
 0.01100217 0.0100267  0.01415879 0.00690718]
At round 48 energy consumption: 0.10171685606737808
At round 48 eta: 0.6919831911422551
At round 48 a_n: 11.740402201727147
At round 48 local rounds: 12.056529567513225
At round 48 global rounds: 38.11610880998822
gradient difference: 0.335304319858551
train() client id: f_00000-0-0 loss: 1.492616  [   32/  126]
train() client id: f_00000-0-1 loss: 1.391618  [   64/  126]
train() client id: f_00000-0-2 loss: 1.195401  [   96/  126]
train() client id: f_00000-1-0 loss: 1.208254  [   32/  126]
train() client id: f_00000-1-1 loss: 1.140336  [   64/  126]
train() client id: f_00000-1-2 loss: 1.340187  [   96/  126]
train() client id: f_00000-2-0 loss: 1.198817  [   32/  126]
train() client id: f_00000-2-1 loss: 1.042750  [   64/  126]
train() client id: f_00000-2-2 loss: 1.156200  [   96/  126]
train() client id: f_00000-3-0 loss: 1.086576  [   32/  126]
train() client id: f_00000-3-1 loss: 1.135422  [   64/  126]
train() client id: f_00000-3-2 loss: 1.258200  [   96/  126]
train() client id: f_00000-4-0 loss: 1.024766  [   32/  126]
train() client id: f_00000-4-1 loss: 1.121156  [   64/  126]
train() client id: f_00000-4-2 loss: 1.033423  [   96/  126]
train() client id: f_00000-5-0 loss: 1.144774  [   32/  126]
train() client id: f_00000-5-1 loss: 0.852309  [   64/  126]
train() client id: f_00000-5-2 loss: 1.069859  [   96/  126]
train() client id: f_00000-6-0 loss: 0.954005  [   32/  126]
train() client id: f_00000-6-1 loss: 0.801008  [   64/  126]
train() client id: f_00000-6-2 loss: 0.979657  [   96/  126]
train() client id: f_00000-7-0 loss: 0.932743  [   32/  126]
train() client id: f_00000-7-1 loss: 0.944616  [   64/  126]
train() client id: f_00000-7-2 loss: 0.911608  [   96/  126]
train() client id: f_00000-8-0 loss: 0.944269  [   32/  126]
train() client id: f_00000-8-1 loss: 0.861255  [   64/  126]
train() client id: f_00000-8-2 loss: 0.897303  [   96/  126]
train() client id: f_00000-9-0 loss: 0.978509  [   32/  126]
train() client id: f_00000-9-1 loss: 0.776312  [   64/  126]
train() client id: f_00000-9-2 loss: 0.918576  [   96/  126]
train() client id: f_00000-10-0 loss: 0.966605  [   32/  126]
train() client id: f_00000-10-1 loss: 0.846324  [   64/  126]
train() client id: f_00000-10-2 loss: 0.804647  [   96/  126]
train() client id: f_00000-11-0 loss: 0.900497  [   32/  126]
train() client id: f_00000-11-1 loss: 0.772854  [   64/  126]
train() client id: f_00000-11-2 loss: 0.864114  [   96/  126]
train() client id: f_00001-0-0 loss: 0.310708  [   32/  265]
train() client id: f_00001-0-1 loss: 0.326092  [   64/  265]
train() client id: f_00001-0-2 loss: 0.458203  [   96/  265]
train() client id: f_00001-0-3 loss: 0.446826  [  128/  265]
train() client id: f_00001-0-4 loss: 0.299669  [  160/  265]
train() client id: f_00001-0-5 loss: 0.284859  [  192/  265]
train() client id: f_00001-0-6 loss: 0.333433  [  224/  265]
train() client id: f_00001-0-7 loss: 0.402167  [  256/  265]
train() client id: f_00001-1-0 loss: 0.301471  [   32/  265]
train() client id: f_00001-1-1 loss: 0.381920  [   64/  265]
train() client id: f_00001-1-2 loss: 0.336161  [   96/  265]
train() client id: f_00001-1-3 loss: 0.316009  [  128/  265]
train() client id: f_00001-1-4 loss: 0.416485  [  160/  265]
train() client id: f_00001-1-5 loss: 0.365163  [  192/  265]
train() client id: f_00001-1-6 loss: 0.290604  [  224/  265]
train() client id: f_00001-1-7 loss: 0.442698  [  256/  265]
train() client id: f_00001-2-0 loss: 0.372695  [   32/  265]
train() client id: f_00001-2-1 loss: 0.266248  [   64/  265]
train() client id: f_00001-2-2 loss: 0.301218  [   96/  265]
train() client id: f_00001-2-3 loss: 0.362344  [  128/  265]
train() client id: f_00001-2-4 loss: 0.326097  [  160/  265]
train() client id: f_00001-2-5 loss: 0.338662  [  192/  265]
train() client id: f_00001-2-6 loss: 0.446193  [  224/  265]
train() client id: f_00001-2-7 loss: 0.321638  [  256/  265]
train() client id: f_00001-3-0 loss: 0.317663  [   32/  265]
train() client id: f_00001-3-1 loss: 0.308283  [   64/  265]
train() client id: f_00001-3-2 loss: 0.274757  [   96/  265]
train() client id: f_00001-3-3 loss: 0.353377  [  128/  265]
train() client id: f_00001-3-4 loss: 0.533754  [  160/  265]
train() client id: f_00001-3-5 loss: 0.271447  [  192/  265]
train() client id: f_00001-3-6 loss: 0.324879  [  224/  265]
train() client id: f_00001-3-7 loss: 0.340966  [  256/  265]
train() client id: f_00001-4-0 loss: 0.262780  [   32/  265]
train() client id: f_00001-4-1 loss: 0.316065  [   64/  265]
train() client id: f_00001-4-2 loss: 0.429673  [   96/  265]
train() client id: f_00001-4-3 loss: 0.387202  [  128/  265]
train() client id: f_00001-4-4 loss: 0.268983  [  160/  265]
train() client id: f_00001-4-5 loss: 0.323320  [  192/  265]
train() client id: f_00001-4-6 loss: 0.327238  [  224/  265]
train() client id: f_00001-4-7 loss: 0.309097  [  256/  265]
train() client id: f_00001-5-0 loss: 0.273304  [   32/  265]
train() client id: f_00001-5-1 loss: 0.296720  [   64/  265]
train() client id: f_00001-5-2 loss: 0.240458  [   96/  265]
train() client id: f_00001-5-3 loss: 0.343337  [  128/  265]
train() client id: f_00001-5-4 loss: 0.432707  [  160/  265]
train() client id: f_00001-5-5 loss: 0.295799  [  192/  265]
train() client id: f_00001-5-6 loss: 0.307733  [  224/  265]
train() client id: f_00001-5-7 loss: 0.468607  [  256/  265]
train() client id: f_00001-6-0 loss: 0.224913  [   32/  265]
train() client id: f_00001-6-1 loss: 0.271748  [   64/  265]
train() client id: f_00001-6-2 loss: 0.309616  [   96/  265]
train() client id: f_00001-6-3 loss: 0.507365  [  128/  265]
train() client id: f_00001-6-4 loss: 0.282377  [  160/  265]
train() client id: f_00001-6-5 loss: 0.278494  [  192/  265]
train() client id: f_00001-6-6 loss: 0.392395  [  224/  265]
train() client id: f_00001-6-7 loss: 0.300008  [  256/  265]
train() client id: f_00001-7-0 loss: 0.266184  [   32/  265]
train() client id: f_00001-7-1 loss: 0.320476  [   64/  265]
train() client id: f_00001-7-2 loss: 0.273017  [   96/  265]
train() client id: f_00001-7-3 loss: 0.373686  [  128/  265]
train() client id: f_00001-7-4 loss: 0.299944  [  160/  265]
train() client id: f_00001-7-5 loss: 0.368565  [  192/  265]
train() client id: f_00001-7-6 loss: 0.262704  [  224/  265]
train() client id: f_00001-7-7 loss: 0.448418  [  256/  265]
train() client id: f_00001-8-0 loss: 0.307452  [   32/  265]
train() client id: f_00001-8-1 loss: 0.262343  [   64/  265]
train() client id: f_00001-8-2 loss: 0.222224  [   96/  265]
train() client id: f_00001-8-3 loss: 0.636445  [  128/  265]
train() client id: f_00001-8-4 loss: 0.308848  [  160/  265]
train() client id: f_00001-8-5 loss: 0.248646  [  192/  265]
train() client id: f_00001-8-6 loss: 0.386820  [  224/  265]
train() client id: f_00001-8-7 loss: 0.225234  [  256/  265]
train() client id: f_00001-9-0 loss: 0.341497  [   32/  265]
train() client id: f_00001-9-1 loss: 0.367791  [   64/  265]
train() client id: f_00001-9-2 loss: 0.240092  [   96/  265]
train() client id: f_00001-9-3 loss: 0.267852  [  128/  265]
train() client id: f_00001-9-4 loss: 0.290182  [  160/  265]
train() client id: f_00001-9-5 loss: 0.232794  [  192/  265]
train() client id: f_00001-9-6 loss: 0.371720  [  224/  265]
train() client id: f_00001-9-7 loss: 0.381617  [  256/  265]
train() client id: f_00001-10-0 loss: 0.247922  [   32/  265]
train() client id: f_00001-10-1 loss: 0.318624  [   64/  265]
train() client id: f_00001-10-2 loss: 0.351264  [   96/  265]
train() client id: f_00001-10-3 loss: 0.214269  [  128/  265]
train() client id: f_00001-10-4 loss: 0.352425  [  160/  265]
train() client id: f_00001-10-5 loss: 0.309654  [  192/  265]
train() client id: f_00001-10-6 loss: 0.303387  [  224/  265]
train() client id: f_00001-10-7 loss: 0.310155  [  256/  265]
train() client id: f_00001-11-0 loss: 0.229685  [   32/  265]
train() client id: f_00001-11-1 loss: 0.323170  [   64/  265]
train() client id: f_00001-11-2 loss: 0.324204  [   96/  265]
train() client id: f_00001-11-3 loss: 0.226312  [  128/  265]
train() client id: f_00001-11-4 loss: 0.512354  [  160/  265]
train() client id: f_00001-11-5 loss: 0.281088  [  192/  265]
train() client id: f_00001-11-6 loss: 0.322435  [  224/  265]
train() client id: f_00001-11-7 loss: 0.300158  [  256/  265]
train() client id: f_00002-0-0 loss: 1.533618  [   32/  124]
train() client id: f_00002-0-1 loss: 1.293225  [   64/  124]
train() client id: f_00002-0-2 loss: 1.151846  [   96/  124]
train() client id: f_00002-1-0 loss: 1.338475  [   32/  124]
train() client id: f_00002-1-1 loss: 1.085090  [   64/  124]
train() client id: f_00002-1-2 loss: 1.294531  [   96/  124]
train() client id: f_00002-2-0 loss: 1.235265  [   32/  124]
train() client id: f_00002-2-1 loss: 1.228831  [   64/  124]
train() client id: f_00002-2-2 loss: 1.210116  [   96/  124]
train() client id: f_00002-3-0 loss: 1.262011  [   32/  124]
train() client id: f_00002-3-1 loss: 1.276049  [   64/  124]
train() client id: f_00002-3-2 loss: 1.099086  [   96/  124]
train() client id: f_00002-4-0 loss: 1.308893  [   32/  124]
train() client id: f_00002-4-1 loss: 1.018054  [   64/  124]
train() client id: f_00002-4-2 loss: 1.143826  [   96/  124]
train() client id: f_00002-5-0 loss: 1.030889  [   32/  124]
train() client id: f_00002-5-1 loss: 1.224394  [   64/  124]
train() client id: f_00002-5-2 loss: 1.228483  [   96/  124]
train() client id: f_00002-6-0 loss: 1.114652  [   32/  124]
train() client id: f_00002-6-1 loss: 1.234127  [   64/  124]
train() client id: f_00002-6-2 loss: 1.211886  [   96/  124]
train() client id: f_00002-7-0 loss: 1.092092  [   32/  124]
train() client id: f_00002-7-1 loss: 1.021625  [   64/  124]
train() client id: f_00002-7-2 loss: 1.247713  [   96/  124]
train() client id: f_00002-8-0 loss: 1.172968  [   32/  124]
train() client id: f_00002-8-1 loss: 0.950157  [   64/  124]
train() client id: f_00002-8-2 loss: 1.250808  [   96/  124]
train() client id: f_00002-9-0 loss: 1.152334  [   32/  124]
train() client id: f_00002-9-1 loss: 1.009152  [   64/  124]
train() client id: f_00002-9-2 loss: 1.212884  [   96/  124]
train() client id: f_00002-10-0 loss: 1.169508  [   32/  124]
train() client id: f_00002-10-1 loss: 1.025256  [   64/  124]
train() client id: f_00002-10-2 loss: 0.962521  [   96/  124]
train() client id: f_00002-11-0 loss: 1.031338  [   32/  124]
train() client id: f_00002-11-1 loss: 1.059026  [   64/  124]
train() client id: f_00002-11-2 loss: 1.174080  [   96/  124]
train() client id: f_00003-0-0 loss: 0.867069  [   32/   43]
train() client id: f_00003-1-0 loss: 0.904231  [   32/   43]
train() client id: f_00003-2-0 loss: 0.947845  [   32/   43]
train() client id: f_00003-3-0 loss: 0.801636  [   32/   43]
train() client id: f_00003-4-0 loss: 0.800543  [   32/   43]
train() client id: f_00003-5-0 loss: 0.812302  [   32/   43]
train() client id: f_00003-6-0 loss: 0.719957  [   32/   43]
train() client id: f_00003-7-0 loss: 0.780251  [   32/   43]
train() client id: f_00003-8-0 loss: 0.746354  [   32/   43]
train() client id: f_00003-9-0 loss: 0.988539  [   32/   43]
train() client id: f_00003-10-0 loss: 0.883372  [   32/   43]
train() client id: f_00003-11-0 loss: 0.856951  [   32/   43]
train() client id: f_00004-0-0 loss: 0.929043  [   32/  306]
train() client id: f_00004-0-1 loss: 0.841091  [   64/  306]
train() client id: f_00004-0-2 loss: 0.900727  [   96/  306]
train() client id: f_00004-0-3 loss: 0.874145  [  128/  306]
train() client id: f_00004-0-4 loss: 1.099186  [  160/  306]
train() client id: f_00004-0-5 loss: 0.880747  [  192/  306]
train() client id: f_00004-0-6 loss: 0.741450  [  224/  306]
train() client id: f_00004-0-7 loss: 0.876714  [  256/  306]
train() client id: f_00004-0-8 loss: 0.777863  [  288/  306]
train() client id: f_00004-1-0 loss: 0.916315  [   32/  306]
train() client id: f_00004-1-1 loss: 0.895854  [   64/  306]
train() client id: f_00004-1-2 loss: 0.862900  [   96/  306]
train() client id: f_00004-1-3 loss: 0.731547  [  128/  306]
train() client id: f_00004-1-4 loss: 0.894018  [  160/  306]
train() client id: f_00004-1-5 loss: 0.785527  [  192/  306]
train() client id: f_00004-1-6 loss: 0.974276  [  224/  306]
train() client id: f_00004-1-7 loss: 0.874216  [  256/  306]
train() client id: f_00004-1-8 loss: 1.059655  [  288/  306]
train() client id: f_00004-2-0 loss: 0.876875  [   32/  306]
train() client id: f_00004-2-1 loss: 1.004833  [   64/  306]
train() client id: f_00004-2-2 loss: 0.951848  [   96/  306]
train() client id: f_00004-2-3 loss: 0.898742  [  128/  306]
train() client id: f_00004-2-4 loss: 0.864793  [  160/  306]
train() client id: f_00004-2-5 loss: 0.737849  [  192/  306]
train() client id: f_00004-2-6 loss: 0.861117  [  224/  306]
train() client id: f_00004-2-7 loss: 0.834303  [  256/  306]
train() client id: f_00004-2-8 loss: 0.906416  [  288/  306]
train() client id: f_00004-3-0 loss: 0.792083  [   32/  306]
train() client id: f_00004-3-1 loss: 0.755666  [   64/  306]
train() client id: f_00004-3-2 loss: 0.962202  [   96/  306]
train() client id: f_00004-3-3 loss: 0.804796  [  128/  306]
train() client id: f_00004-3-4 loss: 0.824164  [  160/  306]
train() client id: f_00004-3-5 loss: 0.901383  [  192/  306]
train() client id: f_00004-3-6 loss: 0.958724  [  224/  306]
train() client id: f_00004-3-7 loss: 0.965190  [  256/  306]
train() client id: f_00004-3-8 loss: 0.849087  [  288/  306]
train() client id: f_00004-4-0 loss: 0.949719  [   32/  306]
train() client id: f_00004-4-1 loss: 0.864178  [   64/  306]
train() client id: f_00004-4-2 loss: 0.888462  [   96/  306]
train() client id: f_00004-4-3 loss: 0.877180  [  128/  306]
train() client id: f_00004-4-4 loss: 0.735083  [  160/  306]
train() client id: f_00004-4-5 loss: 0.929148  [  192/  306]
train() client id: f_00004-4-6 loss: 0.836300  [  224/  306]
train() client id: f_00004-4-7 loss: 1.013891  [  256/  306]
train() client id: f_00004-4-8 loss: 0.802680  [  288/  306]
train() client id: f_00004-5-0 loss: 0.841170  [   32/  306]
train() client id: f_00004-5-1 loss: 1.058573  [   64/  306]
train() client id: f_00004-5-2 loss: 0.756236  [   96/  306]
train() client id: f_00004-5-3 loss: 0.887129  [  128/  306]
train() client id: f_00004-5-4 loss: 0.913258  [  160/  306]
train() client id: f_00004-5-5 loss: 0.868221  [  192/  306]
train() client id: f_00004-5-6 loss: 0.749437  [  224/  306]
train() client id: f_00004-5-7 loss: 0.861617  [  256/  306]
train() client id: f_00004-5-8 loss: 1.035054  [  288/  306]
train() client id: f_00004-6-0 loss: 0.822581  [   32/  306]
train() client id: f_00004-6-1 loss: 1.022850  [   64/  306]
train() client id: f_00004-6-2 loss: 0.839180  [   96/  306]
train() client id: f_00004-6-3 loss: 0.753530  [  128/  306]
train() client id: f_00004-6-4 loss: 0.908973  [  160/  306]
train() client id: f_00004-6-5 loss: 0.876350  [  192/  306]
train() client id: f_00004-6-6 loss: 0.816166  [  224/  306]
train() client id: f_00004-6-7 loss: 0.809689  [  256/  306]
train() client id: f_00004-6-8 loss: 0.916134  [  288/  306]
train() client id: f_00004-7-0 loss: 0.909819  [   32/  306]
train() client id: f_00004-7-1 loss: 0.816517  [   64/  306]
train() client id: f_00004-7-2 loss: 0.926363  [   96/  306]
train() client id: f_00004-7-3 loss: 0.811912  [  128/  306]
train() client id: f_00004-7-4 loss: 0.850063  [  160/  306]
train() client id: f_00004-7-5 loss: 0.861665  [  192/  306]
train() client id: f_00004-7-6 loss: 0.839406  [  224/  306]
train() client id: f_00004-7-7 loss: 1.007015  [  256/  306]
train() client id: f_00004-7-8 loss: 0.848820  [  288/  306]
train() client id: f_00004-8-0 loss: 0.794948  [   32/  306]
train() client id: f_00004-8-1 loss: 0.891881  [   64/  306]
train() client id: f_00004-8-2 loss: 0.893141  [   96/  306]
train() client id: f_00004-8-3 loss: 0.864838  [  128/  306]
train() client id: f_00004-8-4 loss: 0.903845  [  160/  306]
train() client id: f_00004-8-5 loss: 0.860624  [  192/  306]
train() client id: f_00004-8-6 loss: 0.817558  [  224/  306]
train() client id: f_00004-8-7 loss: 0.872928  [  256/  306]
train() client id: f_00004-8-8 loss: 0.939111  [  288/  306]
train() client id: f_00004-9-0 loss: 0.974533  [   32/  306]
train() client id: f_00004-9-1 loss: 0.753093  [   64/  306]
train() client id: f_00004-9-2 loss: 0.940053  [   96/  306]
train() client id: f_00004-9-3 loss: 0.889804  [  128/  306]
train() client id: f_00004-9-4 loss: 0.954826  [  160/  306]
train() client id: f_00004-9-5 loss: 0.814776  [  192/  306]
train() client id: f_00004-9-6 loss: 0.882307  [  224/  306]
train() client id: f_00004-9-7 loss: 0.840692  [  256/  306]
train() client id: f_00004-9-8 loss: 0.834321  [  288/  306]
train() client id: f_00004-10-0 loss: 0.926314  [   32/  306]
train() client id: f_00004-10-1 loss: 0.818732  [   64/  306]
train() client id: f_00004-10-2 loss: 0.890159  [   96/  306]
train() client id: f_00004-10-3 loss: 0.838432  [  128/  306]
train() client id: f_00004-10-4 loss: 0.842093  [  160/  306]
train() client id: f_00004-10-5 loss: 0.785427  [  192/  306]
train() client id: f_00004-10-6 loss: 0.993806  [  224/  306]
train() client id: f_00004-10-7 loss: 0.861409  [  256/  306]
train() client id: f_00004-10-8 loss: 0.861612  [  288/  306]
train() client id: f_00004-11-0 loss: 0.989328  [   32/  306]
train() client id: f_00004-11-1 loss: 0.876282  [   64/  306]
train() client id: f_00004-11-2 loss: 0.818379  [   96/  306]
train() client id: f_00004-11-3 loss: 0.883113  [  128/  306]
train() client id: f_00004-11-4 loss: 0.754046  [  160/  306]
train() client id: f_00004-11-5 loss: 0.912415  [  192/  306]
train() client id: f_00004-11-6 loss: 0.936495  [  224/  306]
train() client id: f_00004-11-7 loss: 0.903133  [  256/  306]
train() client id: f_00004-11-8 loss: 0.861692  [  288/  306]
train() client id: f_00005-0-0 loss: 0.694522  [   32/  146]
train() client id: f_00005-0-1 loss: 0.450296  [   64/  146]
train() client id: f_00005-0-2 loss: 0.597516  [   96/  146]
train() client id: f_00005-0-3 loss: 0.504296  [  128/  146]
train() client id: f_00005-1-0 loss: 0.574267  [   32/  146]
train() client id: f_00005-1-1 loss: 0.627683  [   64/  146]
train() client id: f_00005-1-2 loss: 0.469556  [   96/  146]
train() client id: f_00005-1-3 loss: 0.283356  [  128/  146]
train() client id: f_00005-2-0 loss: 0.679421  [   32/  146]
train() client id: f_00005-2-1 loss: 0.477630  [   64/  146]
train() client id: f_00005-2-2 loss: 0.612593  [   96/  146]
train() client id: f_00005-2-3 loss: 0.376157  [  128/  146]
train() client id: f_00005-3-0 loss: 0.676488  [   32/  146]
train() client id: f_00005-3-1 loss: 0.588922  [   64/  146]
train() client id: f_00005-3-2 loss: 0.233618  [   96/  146]
train() client id: f_00005-3-3 loss: 0.428151  [  128/  146]
train() client id: f_00005-4-0 loss: 0.448119  [   32/  146]
train() client id: f_00005-4-1 loss: 0.339858  [   64/  146]
train() client id: f_00005-4-2 loss: 0.698676  [   96/  146]
train() client id: f_00005-4-3 loss: 0.580677  [  128/  146]
train() client id: f_00005-5-0 loss: 0.334404  [   32/  146]
train() client id: f_00005-5-1 loss: 0.440770  [   64/  146]
train() client id: f_00005-5-2 loss: 0.563336  [   96/  146]
train() client id: f_00005-5-3 loss: 0.693800  [  128/  146]
train() client id: f_00005-6-0 loss: 0.385287  [   32/  146]
train() client id: f_00005-6-1 loss: 0.649222  [   64/  146]
train() client id: f_00005-6-2 loss: 0.478036  [   96/  146]
train() client id: f_00005-6-3 loss: 0.507698  [  128/  146]
train() client id: f_00005-7-0 loss: 0.213211  [   32/  146]
train() client id: f_00005-7-1 loss: 0.257073  [   64/  146]
train() client id: f_00005-7-2 loss: 0.364386  [   96/  146]
train() client id: f_00005-7-3 loss: 1.027815  [  128/  146]
train() client id: f_00005-8-0 loss: 0.460102  [   32/  146]
train() client id: f_00005-8-1 loss: 0.378315  [   64/  146]
train() client id: f_00005-8-2 loss: 0.442002  [   96/  146]
train() client id: f_00005-8-3 loss: 0.525496  [  128/  146]
train() client id: f_00005-9-0 loss: 0.344985  [   32/  146]
train() client id: f_00005-9-1 loss: 0.515121  [   64/  146]
train() client id: f_00005-9-2 loss: 0.524418  [   96/  146]
train() client id: f_00005-9-3 loss: 0.575375  [  128/  146]
train() client id: f_00005-10-0 loss: 0.492077  [   32/  146]
train() client id: f_00005-10-1 loss: 0.511376  [   64/  146]
train() client id: f_00005-10-2 loss: 0.395285  [   96/  146]
train() client id: f_00005-10-3 loss: 0.565718  [  128/  146]
train() client id: f_00005-11-0 loss: 0.393954  [   32/  146]
train() client id: f_00005-11-1 loss: 0.613297  [   64/  146]
train() client id: f_00005-11-2 loss: 0.333527  [   96/  146]
train() client id: f_00005-11-3 loss: 0.710250  [  128/  146]
train() client id: f_00006-0-0 loss: 0.509138  [   32/   54]
train() client id: f_00006-1-0 loss: 0.544199  [   32/   54]
train() client id: f_00006-2-0 loss: 0.489253  [   32/   54]
train() client id: f_00006-3-0 loss: 0.560847  [   32/   54]
train() client id: f_00006-4-0 loss: 0.480907  [   32/   54]
train() client id: f_00006-5-0 loss: 0.544272  [   32/   54]
train() client id: f_00006-6-0 loss: 0.487157  [   32/   54]
train() client id: f_00006-7-0 loss: 0.482011  [   32/   54]
train() client id: f_00006-8-0 loss: 0.443341  [   32/   54]
train() client id: f_00006-9-0 loss: 0.464472  [   32/   54]
train() client id: f_00006-10-0 loss: 0.506129  [   32/   54]
train() client id: f_00006-11-0 loss: 0.553148  [   32/   54]
train() client id: f_00007-0-0 loss: 0.630095  [   32/  179]
train() client id: f_00007-0-1 loss: 0.657573  [   64/  179]
train() client id: f_00007-0-2 loss: 0.781239  [   96/  179]
train() client id: f_00007-0-3 loss: 0.588184  [  128/  179]
train() client id: f_00007-0-4 loss: 0.485301  [  160/  179]
train() client id: f_00007-1-0 loss: 0.707986  [   32/  179]
train() client id: f_00007-1-1 loss: 0.600832  [   64/  179]
train() client id: f_00007-1-2 loss: 0.592972  [   96/  179]
train() client id: f_00007-1-3 loss: 0.731393  [  128/  179]
train() client id: f_00007-1-4 loss: 0.400115  [  160/  179]
train() client id: f_00007-2-0 loss: 0.784208  [   32/  179]
train() client id: f_00007-2-1 loss: 0.649810  [   64/  179]
train() client id: f_00007-2-2 loss: 0.432894  [   96/  179]
train() client id: f_00007-2-3 loss: 0.650327  [  128/  179]
train() client id: f_00007-2-4 loss: 0.510068  [  160/  179]
train() client id: f_00007-3-0 loss: 0.616928  [   32/  179]
train() client id: f_00007-3-1 loss: 0.678228  [   64/  179]
train() client id: f_00007-3-2 loss: 0.556479  [   96/  179]
train() client id: f_00007-3-3 loss: 0.593036  [  128/  179]
train() client id: f_00007-3-4 loss: 0.417386  [  160/  179]
train() client id: f_00007-4-0 loss: 0.728721  [   32/  179]
train() client id: f_00007-4-1 loss: 0.516520  [   64/  179]
train() client id: f_00007-4-2 loss: 0.612668  [   96/  179]
train() client id: f_00007-4-3 loss: 0.645481  [  128/  179]
train() client id: f_00007-4-4 loss: 0.497038  [  160/  179]
train() client id: f_00007-5-0 loss: 0.520166  [   32/  179]
train() client id: f_00007-5-1 loss: 0.625885  [   64/  179]
train() client id: f_00007-5-2 loss: 0.647923  [   96/  179]
train() client id: f_00007-5-3 loss: 0.567181  [  128/  179]
train() client id: f_00007-5-4 loss: 0.611490  [  160/  179]
train() client id: f_00007-6-0 loss: 0.816997  [   32/  179]
train() client id: f_00007-6-1 loss: 0.606897  [   64/  179]
train() client id: f_00007-6-2 loss: 0.407651  [   96/  179]
train() client id: f_00007-6-3 loss: 0.593552  [  128/  179]
train() client id: f_00007-6-4 loss: 0.423720  [  160/  179]
train() client id: f_00007-7-0 loss: 0.554707  [   32/  179]
train() client id: f_00007-7-1 loss: 0.514433  [   64/  179]
train() client id: f_00007-7-2 loss: 0.523308  [   96/  179]
train() client id: f_00007-7-3 loss: 0.585493  [  128/  179]
train() client id: f_00007-7-4 loss: 0.486059  [  160/  179]
train() client id: f_00007-8-0 loss: 0.607798  [   32/  179]
train() client id: f_00007-8-1 loss: 0.580353  [   64/  179]
train() client id: f_00007-8-2 loss: 0.525497  [   96/  179]
train() client id: f_00007-8-3 loss: 0.514429  [  128/  179]
train() client id: f_00007-8-4 loss: 0.540201  [  160/  179]
train() client id: f_00007-9-0 loss: 0.484223  [   32/  179]
train() client id: f_00007-9-1 loss: 0.632303  [   64/  179]
train() client id: f_00007-9-2 loss: 0.717730  [   96/  179]
train() client id: f_00007-9-3 loss: 0.495059  [  128/  179]
train() client id: f_00007-9-4 loss: 0.576664  [  160/  179]
train() client id: f_00007-10-0 loss: 0.493781  [   32/  179]
train() client id: f_00007-10-1 loss: 0.620301  [   64/  179]
train() client id: f_00007-10-2 loss: 0.420819  [   96/  179]
train() client id: f_00007-10-3 loss: 0.636808  [  128/  179]
train() client id: f_00007-10-4 loss: 0.683489  [  160/  179]
train() client id: f_00007-11-0 loss: 0.525638  [   32/  179]
train() client id: f_00007-11-1 loss: 0.517408  [   64/  179]
train() client id: f_00007-11-2 loss: 0.498392  [   96/  179]
train() client id: f_00007-11-3 loss: 0.422649  [  128/  179]
train() client id: f_00007-11-4 loss: 0.705961  [  160/  179]
train() client id: f_00008-0-0 loss: 0.871267  [   32/  130]
train() client id: f_00008-0-1 loss: 0.738474  [   64/  130]
train() client id: f_00008-0-2 loss: 0.666713  [   96/  130]
train() client id: f_00008-0-3 loss: 0.858350  [  128/  130]
train() client id: f_00008-1-0 loss: 0.720743  [   32/  130]
train() client id: f_00008-1-1 loss: 0.797332  [   64/  130]
train() client id: f_00008-1-2 loss: 0.849345  [   96/  130]
train() client id: f_00008-1-3 loss: 0.723632  [  128/  130]
train() client id: f_00008-2-0 loss: 0.911129  [   32/  130]
train() client id: f_00008-2-1 loss: 0.661652  [   64/  130]
train() client id: f_00008-2-2 loss: 0.854661  [   96/  130]
train() client id: f_00008-2-3 loss: 0.705912  [  128/  130]
train() client id: f_00008-3-0 loss: 0.848859  [   32/  130]
train() client id: f_00008-3-1 loss: 0.747188  [   64/  130]
train() client id: f_00008-3-2 loss: 0.869203  [   96/  130]
train() client id: f_00008-3-3 loss: 0.658500  [  128/  130]
train() client id: f_00008-4-0 loss: 0.949545  [   32/  130]
train() client id: f_00008-4-1 loss: 0.757123  [   64/  130]
train() client id: f_00008-4-2 loss: 0.675323  [   96/  130]
train() client id: f_00008-4-3 loss: 0.750677  [  128/  130]
train() client id: f_00008-5-0 loss: 0.811680  [   32/  130]
train() client id: f_00008-5-1 loss: 0.797149  [   64/  130]
train() client id: f_00008-5-2 loss: 0.811312  [   96/  130]
train() client id: f_00008-5-3 loss: 0.665214  [  128/  130]
train() client id: f_00008-6-0 loss: 0.934630  [   32/  130]
train() client id: f_00008-6-1 loss: 0.633761  [   64/  130]
train() client id: f_00008-6-2 loss: 0.793554  [   96/  130]
train() client id: f_00008-6-3 loss: 0.734473  [  128/  130]
train() client id: f_00008-7-0 loss: 0.737272  [   32/  130]
train() client id: f_00008-7-1 loss: 0.833766  [   64/  130]
train() client id: f_00008-7-2 loss: 0.715283  [   96/  130]
train() client id: f_00008-7-3 loss: 0.812914  [  128/  130]
train() client id: f_00008-8-0 loss: 0.769254  [   32/  130]
train() client id: f_00008-8-1 loss: 0.835111  [   64/  130]
train() client id: f_00008-8-2 loss: 0.789980  [   96/  130]
train() client id: f_00008-8-3 loss: 0.727189  [  128/  130]
train() client id: f_00008-9-0 loss: 0.734957  [   32/  130]
train() client id: f_00008-9-1 loss: 0.739757  [   64/  130]
train() client id: f_00008-9-2 loss: 0.846362  [   96/  130]
train() client id: f_00008-9-3 loss: 0.758932  [  128/  130]
train() client id: f_00008-10-0 loss: 0.764700  [   32/  130]
train() client id: f_00008-10-1 loss: 0.812722  [   64/  130]
train() client id: f_00008-10-2 loss: 0.798123  [   96/  130]
train() client id: f_00008-10-3 loss: 0.742530  [  128/  130]
train() client id: f_00008-11-0 loss: 0.737931  [   32/  130]
train() client id: f_00008-11-1 loss: 0.865826  [   64/  130]
train() client id: f_00008-11-2 loss: 0.711033  [   96/  130]
train() client id: f_00008-11-3 loss: 0.793213  [  128/  130]
train() client id: f_00009-0-0 loss: 1.268063  [   32/  118]
train() client id: f_00009-0-1 loss: 1.014017  [   64/  118]
train() client id: f_00009-0-2 loss: 1.232367  [   96/  118]
train() client id: f_00009-1-0 loss: 1.080248  [   32/  118]
train() client id: f_00009-1-1 loss: 1.122358  [   64/  118]
train() client id: f_00009-1-2 loss: 1.085369  [   96/  118]
train() client id: f_00009-2-0 loss: 0.955658  [   32/  118]
train() client id: f_00009-2-1 loss: 1.058596  [   64/  118]
train() client id: f_00009-2-2 loss: 1.102080  [   96/  118]
train() client id: f_00009-3-0 loss: 1.052633  [   32/  118]
train() client id: f_00009-3-1 loss: 0.917051  [   64/  118]
train() client id: f_00009-3-2 loss: 1.064621  [   96/  118]
train() client id: f_00009-4-0 loss: 1.049499  [   32/  118]
train() client id: f_00009-4-1 loss: 0.913165  [   64/  118]
train() client id: f_00009-4-2 loss: 0.959939  [   96/  118]
train() client id: f_00009-5-0 loss: 1.011972  [   32/  118]
train() client id: f_00009-5-1 loss: 0.908838  [   64/  118]
train() client id: f_00009-5-2 loss: 0.904922  [   96/  118]
train() client id: f_00009-6-0 loss: 0.857057  [   32/  118]
train() client id: f_00009-6-1 loss: 0.888614  [   64/  118]
train() client id: f_00009-6-2 loss: 0.902884  [   96/  118]
train() client id: f_00009-7-0 loss: 0.866229  [   32/  118]
train() client id: f_00009-7-1 loss: 0.906249  [   64/  118]
train() client id: f_00009-7-2 loss: 0.923107  [   96/  118]
train() client id: f_00009-8-0 loss: 0.906262  [   32/  118]
train() client id: f_00009-8-1 loss: 0.700453  [   64/  118]
train() client id: f_00009-8-2 loss: 0.849021  [   96/  118]
train() client id: f_00009-9-0 loss: 0.810194  [   32/  118]
train() client id: f_00009-9-1 loss: 0.768140  [   64/  118]
train() client id: f_00009-9-2 loss: 0.886388  [   96/  118]
train() client id: f_00009-10-0 loss: 0.724306  [   32/  118]
train() client id: f_00009-10-1 loss: 1.000252  [   64/  118]
train() client id: f_00009-10-2 loss: 0.736873  [   96/  118]
train() client id: f_00009-11-0 loss: 0.716862  [   32/  118]
train() client id: f_00009-11-1 loss: 1.028265  [   64/  118]
train() client id: f_00009-11-2 loss: 0.613734  [   96/  118]
At round 48 accuracy: 0.649867374005305
At round 48 training accuracy: 0.5888665325285044
At round 48 training loss: 0.8347792588651561
update_location
xs = -3.905658 4.200318 260.009024 18.811294 0.979296 3.956410 -222.443192 -201.324852 244.663977 -187.060879 
ys = 252.587959 235.555839 1.320614 -222.455176 214.350187 197.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 271.690875 255.937875 278.579318 244.622505 236.531101 221.689177 243.901341 224.794066 264.894567 212.150381 
dists_bs = 187.864752 189.155395 468.398740 442.191627 180.325123 180.404773 183.851819 176.155761 448.221342 171.423330 
uav_gains = -113.224338 -111.809021 -113.851229 -110.846028 -110.198807 -109.107271 -110.786848 -109.325940 -112.607292 -108.462109 
bs_gains = -103.234728 -103.317984 -114.344299 -113.644154 -102.736634 -102.742004 -102.972161 -102.452171 -113.808850 -102.121017 
Round 49
-------------------------------
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.53180875  9.28555743  4.47092374  1.6235926  10.7068955   5.1513321
  2.0049889   6.33296008  4.67865356  4.17679842]
obj_prev = 52.96351106757935
eta_min = 4.606985121778528e-21	eta_max = 0.9400638118405741
af = 11.129531482767057	bf = 1.180489388448874	zeta = 12.242484631043764	eta = 0.909090909090909
af = 11.129531482767057	bf = 1.180489388448874	zeta = 24.65371738730223	eta = 0.45143421204703454
af = 11.129531482767057	bf = 1.180489388448874	zeta = 18.287853905358034	eta = 0.608575043324591
af = 11.129531482767057	bf = 1.180489388448874	zeta = 17.136885852504452	eta = 0.6494488892881634
af = 11.129531482767057	bf = 1.180489388448874	zeta = 17.071238826404898	eta = 0.6519463289068677
af = 11.129531482767057	bf = 1.180489388448874	zeta = 17.07100360444631	eta = 0.6519553120982448
eta = 0.6519553120982448
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [0.03529932 0.07424063 0.03473901 0.01204659 0.08572692 0.04090239
 0.01512828 0.05014745 0.03641993 0.03305809]
ene_total = [1.60196419 2.67053184 1.60796312 0.76579092 3.04020143 1.57233453
 0.86514606 1.976894   1.66129063 1.30888688]
ti_comp = [0.70507767 0.76819322 0.69764277 0.72788409 0.77017386 0.77015604
 0.7283619  0.73907045 0.6981679  0.77215943]
ti_coms = [0.13618224 0.07306669 0.14361714 0.11337582 0.07108605 0.07110387
 0.11289801 0.10218946 0.14309201 0.06910048]
t_total = [27.54979439 27.54979439 27.54979439 27.54979439 27.54979439 27.54979439
 27.54979439 27.54979439 27.54979439 27.54979439]
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [5.52974468e-06 4.33375200e-05 5.38351868e-06 2.06228308e-07
 6.63825764e-05 7.21055148e-06 4.07900477e-07 1.44296163e-05
 6.19409234e-06 3.78703601e-06]
ene_total = [0.44615416 0.2406999  0.47049736 0.37129277 0.23496834 0.23308889
 0.36973465 0.33512519 0.46880422 0.22641604]
optimize_network iter = 0 obj = 3.396781522582919
eta = 0.6519553120982448
freqs = [25032220.23451398 48321587.59240132 24897421.60469212  8275076.45541563
 55654263.85367705 26554609.35010472 10385139.39706882 33926025.18152012
 26082502.95796476 21406261.0487775 ]
eta_min = 0.6519553120982468	eta_max = 0.6979914102198819
af = 0.004073073698070901	bf = 1.180489388448874	zeta = 0.004480381067877992	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [1.23203954e-06 9.65569680e-06 1.19946005e-06 4.59481302e-08
 1.47901871e-05 1.60652707e-06 9.08811424e-08 3.21495093e-06
 1.38005769e-06 8.43760128e-07]
ene_total = [1.70755859 0.91729463 1.80077061 1.42147114 0.89310582 0.89167624
 1.41548626 1.28161791 1.79420944 0.86646288]
ti_comp = [0.59380367 0.65691922 0.58636878 0.6166101  0.65889986 0.65888204
 0.6170879  0.62779645 0.5868939  0.66088543]
ti_coms = [0.13618224 0.07306669 0.14361714 0.11337582 0.07108605 0.07110387
 0.11289801 0.10218946 0.14309201 0.06910048]
t_total = [27.54979439 27.54979439 27.54979439 27.54979439 27.54979439 27.54979439
 27.54979439 27.54979439 27.54979439 27.54979439]
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [4.62920539e-06 3.51879741e-05 4.52484764e-06 1.70633650e-07
 5.38525197e-05 5.84956806e-06 3.37417651e-07 1.18741342e-05
 5.20464897e-06 3.06954669e-06]
ene_total = [0.51412882 0.27708296 0.54218432 0.42788873 0.2703124  0.26856798
 0.42609178 0.38611291 0.54022815 0.26090226]
optimize_network iter = 1 obj = 3.9135003002771818
eta = 0.6979914102198819
freqs = [24982229.19460944 47493855.941672   24897421.60469214  8210343.90468077
 54677108.13463603 26088475.84559726 10302681.03588401 33568945.36426646
 26078780.65421337 21021289.55092173]
eta_min = 0.697991410219886	eta_max = 0.6979914102198779
af = 0.003953064498406836	bf = 1.180489388448874	zeta = 0.004348370948247519	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [1.22712352e-06 9.32773270e-06 1.19946005e-06 4.52320749e-08
 1.42753859e-05 1.55062088e-06 8.94436732e-08 3.14763077e-06
 1.37966381e-06 8.13684556e-07]
ene_total = [1.70755797 0.91725351 1.80077061 1.42147105 0.89304128 0.89166923
 1.41548608 1.28160947 1.79420939 0.86645911]
ti_comp = [0.59380367 0.65691922 0.58636878 0.6166101  0.65889986 0.65888204
 0.6170879  0.62779645 0.5868939  0.66088543]
ti_coms = [0.13618224 0.07306669 0.14361714 0.11337582 0.07108605 0.07110387
 0.11289801 0.10218946 0.14309201 0.06910048]
t_total = [27.54979439 27.54979439 27.54979439 27.54979439 27.54979439 27.54979439
 27.54979439 27.54979439 27.54979439 27.54979439]
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [4.62920539e-06 3.51879741e-05 4.52484764e-06 1.70633650e-07
 5.38525197e-05 5.84956806e-06 3.37417651e-07 1.18741342e-05
 5.20464897e-06 3.06954669e-06]
ene_total = [0.51412882 0.27708296 0.54218432 0.42788873 0.2703124  0.26856798
 0.42609178 0.38611291 0.54022815 0.26090226]
optimize_network iter = 2 obj = 3.9135003002771303
eta = 0.6979914102198779
freqs = [24982229.19460944 47493855.94167206 24897421.60469212  8210343.90468077
 54677108.13463611 26088475.8455973  10302681.03588402 33568945.36426648
 26078780.65421337 21021289.55092177]
Done!
ene_coms = [0.01361822 0.00730667 0.01436171 0.01133758 0.00710861 0.00711039
 0.0112898  0.01021895 0.0143092  0.00691005]
ene_comp = [4.32509460e-06 3.28763371e-05 4.22759253e-06 1.59424052e-07
 5.03147350e-05 5.46528683e-06 3.15251352e-07 1.10940754e-05
 4.86273502e-06 2.86789605e-06]
ene_total = [0.01362255 0.00733955 0.01436594 0.01133774 0.00715892 0.00711585
 0.01129012 0.01023004 0.01431406 0.00691292]
At round 49 energy consumption: 0.10368768459529902
At round 49 eta: 0.6979914102198779
At round 49 a_n: 11.397856354757828
At round 49 local rounds: 11.773444044723618
At round 49 global rounds: 37.74017276480797
gradient difference: 0.4351896643638611
train() client id: f_00000-0-0 loss: 1.068430  [   32/  126]
train() client id: f_00000-0-1 loss: 0.927698  [   64/  126]
train() client id: f_00000-0-2 loss: 1.021642  [   96/  126]
train() client id: f_00000-1-0 loss: 1.147162  [   32/  126]
train() client id: f_00000-1-1 loss: 0.786161  [   64/  126]
train() client id: f_00000-1-2 loss: 0.914904  [   96/  126]
train() client id: f_00000-2-0 loss: 0.852847  [   32/  126]
train() client id: f_00000-2-1 loss: 1.185452  [   64/  126]
train() client id: f_00000-2-2 loss: 0.830283  [   96/  126]
train() client id: f_00000-3-0 loss: 0.791370  [   32/  126]
train() client id: f_00000-3-1 loss: 0.922279  [   64/  126]
train() client id: f_00000-3-2 loss: 0.926622  [   96/  126]
train() client id: f_00000-4-0 loss: 0.828641  [   32/  126]
train() client id: f_00000-4-1 loss: 0.777597  [   64/  126]
train() client id: f_00000-4-2 loss: 0.840278  [   96/  126]
train() client id: f_00000-5-0 loss: 0.841375  [   32/  126]
train() client id: f_00000-5-1 loss: 0.758003  [   64/  126]
train() client id: f_00000-5-2 loss: 0.790409  [   96/  126]
train() client id: f_00000-6-0 loss: 0.766966  [   32/  126]
train() client id: f_00000-6-1 loss: 0.880614  [   64/  126]
train() client id: f_00000-6-2 loss: 0.781717  [   96/  126]
train() client id: f_00000-7-0 loss: 0.845743  [   32/  126]
train() client id: f_00000-7-1 loss: 0.809093  [   64/  126]
train() client id: f_00000-7-2 loss: 0.789062  [   96/  126]
train() client id: f_00000-8-0 loss: 0.662898  [   32/  126]
train() client id: f_00000-8-1 loss: 0.727006  [   64/  126]
train() client id: f_00000-8-2 loss: 0.900873  [   96/  126]
train() client id: f_00000-9-0 loss: 0.835911  [   32/  126]
train() client id: f_00000-9-1 loss: 0.793405  [   64/  126]
train() client id: f_00000-9-2 loss: 0.797545  [   96/  126]
train() client id: f_00000-10-0 loss: 0.798757  [   32/  126]
train() client id: f_00000-10-1 loss: 0.764281  [   64/  126]
train() client id: f_00000-10-2 loss: 0.910731  [   96/  126]
train() client id: f_00001-0-0 loss: 0.371635  [   32/  265]
train() client id: f_00001-0-1 loss: 0.363000  [   64/  265]
train() client id: f_00001-0-2 loss: 0.432084  [   96/  265]
train() client id: f_00001-0-3 loss: 0.459237  [  128/  265]
train() client id: f_00001-0-4 loss: 0.465569  [  160/  265]
train() client id: f_00001-0-5 loss: 0.370899  [  192/  265]
train() client id: f_00001-0-6 loss: 0.492326  [  224/  265]
train() client id: f_00001-0-7 loss: 0.574834  [  256/  265]
train() client id: f_00001-1-0 loss: 0.389452  [   32/  265]
train() client id: f_00001-1-1 loss: 0.382161  [   64/  265]
train() client id: f_00001-1-2 loss: 0.523564  [   96/  265]
train() client id: f_00001-1-3 loss: 0.535375  [  128/  265]
train() client id: f_00001-1-4 loss: 0.410074  [  160/  265]
train() client id: f_00001-1-5 loss: 0.389925  [  192/  265]
train() client id: f_00001-1-6 loss: 0.442818  [  224/  265]
train() client id: f_00001-1-7 loss: 0.377930  [  256/  265]
train() client id: f_00001-2-0 loss: 0.457936  [   32/  265]
train() client id: f_00001-2-1 loss: 0.361080  [   64/  265]
train() client id: f_00001-2-2 loss: 0.467101  [   96/  265]
train() client id: f_00001-2-3 loss: 0.544284  [  128/  265]
train() client id: f_00001-2-4 loss: 0.422908  [  160/  265]
train() client id: f_00001-2-5 loss: 0.453528  [  192/  265]
train() client id: f_00001-2-6 loss: 0.328902  [  224/  265]
train() client id: f_00001-2-7 loss: 0.391530  [  256/  265]
train() client id: f_00001-3-0 loss: 0.432162  [   32/  265]
train() client id: f_00001-3-1 loss: 0.418850  [   64/  265]
train() client id: f_00001-3-2 loss: 0.428331  [   96/  265]
train() client id: f_00001-3-3 loss: 0.376216  [  128/  265]
train() client id: f_00001-3-4 loss: 0.455342  [  160/  265]
train() client id: f_00001-3-5 loss: 0.416266  [  192/  265]
train() client id: f_00001-3-6 loss: 0.449376  [  224/  265]
train() client id: f_00001-3-7 loss: 0.417897  [  256/  265]
train() client id: f_00001-4-0 loss: 0.371792  [   32/  265]
train() client id: f_00001-4-1 loss: 0.453201  [   64/  265]
train() client id: f_00001-4-2 loss: 0.352313  [   96/  265]
train() client id: f_00001-4-3 loss: 0.409917  [  128/  265]
train() client id: f_00001-4-4 loss: 0.426613  [  160/  265]
train() client id: f_00001-4-5 loss: 0.501031  [  192/  265]
train() client id: f_00001-4-6 loss: 0.408458  [  224/  265]
train() client id: f_00001-4-7 loss: 0.377028  [  256/  265]
train() client id: f_00001-5-0 loss: 0.392487  [   32/  265]
train() client id: f_00001-5-1 loss: 0.417727  [   64/  265]
train() client id: f_00001-5-2 loss: 0.528210  [   96/  265]
train() client id: f_00001-5-3 loss: 0.408431  [  128/  265]
train() client id: f_00001-5-4 loss: 0.436348  [  160/  265]
train() client id: f_00001-5-5 loss: 0.502814  [  192/  265]
train() client id: f_00001-5-6 loss: 0.325219  [  224/  265]
train() client id: f_00001-5-7 loss: 0.341906  [  256/  265]
train() client id: f_00001-6-0 loss: 0.335969  [   32/  265]
train() client id: f_00001-6-1 loss: 0.524797  [   64/  265]
train() client id: f_00001-6-2 loss: 0.388362  [   96/  265]
train() client id: f_00001-6-3 loss: 0.546207  [  128/  265]
train() client id: f_00001-6-4 loss: 0.386506  [  160/  265]
train() client id: f_00001-6-5 loss: 0.331523  [  192/  265]
train() client id: f_00001-6-6 loss: 0.359104  [  224/  265]
train() client id: f_00001-6-7 loss: 0.473777  [  256/  265]
train() client id: f_00001-7-0 loss: 0.421218  [   32/  265]
train() client id: f_00001-7-1 loss: 0.543740  [   64/  265]
train() client id: f_00001-7-2 loss: 0.402276  [   96/  265]
train() client id: f_00001-7-3 loss: 0.312383  [  128/  265]
train() client id: f_00001-7-4 loss: 0.485249  [  160/  265]
train() client id: f_00001-7-5 loss: 0.378135  [  192/  265]
train() client id: f_00001-7-6 loss: 0.398909  [  224/  265]
train() client id: f_00001-7-7 loss: 0.414623  [  256/  265]
train() client id: f_00001-8-0 loss: 0.402575  [   32/  265]
train() client id: f_00001-8-1 loss: 0.458310  [   64/  265]
train() client id: f_00001-8-2 loss: 0.414319  [   96/  265]
train() client id: f_00001-8-3 loss: 0.399715  [  128/  265]
train() client id: f_00001-8-4 loss: 0.346926  [  160/  265]
train() client id: f_00001-8-5 loss: 0.493665  [  192/  265]
train() client id: f_00001-8-6 loss: 0.443058  [  224/  265]
train() client id: f_00001-8-7 loss: 0.381601  [  256/  265]
train() client id: f_00001-9-0 loss: 0.329409  [   32/  265]
train() client id: f_00001-9-1 loss: 0.426920  [   64/  265]
train() client id: f_00001-9-2 loss: 0.435485  [   96/  265]
train() client id: f_00001-9-3 loss: 0.496133  [  128/  265]
train() client id: f_00001-9-4 loss: 0.566158  [  160/  265]
train() client id: f_00001-9-5 loss: 0.360640  [  192/  265]
train() client id: f_00001-9-6 loss: 0.388140  [  224/  265]
train() client id: f_00001-9-7 loss: 0.335596  [  256/  265]
train() client id: f_00001-10-0 loss: 0.302124  [   32/  265]
train() client id: f_00001-10-1 loss: 0.480810  [   64/  265]
train() client id: f_00001-10-2 loss: 0.312381  [   96/  265]
train() client id: f_00001-10-3 loss: 0.427868  [  128/  265]
train() client id: f_00001-10-4 loss: 0.432552  [  160/  265]
train() client id: f_00001-10-5 loss: 0.430115  [  192/  265]
train() client id: f_00001-10-6 loss: 0.566234  [  224/  265]
train() client id: f_00001-10-7 loss: 0.406256  [  256/  265]
train() client id: f_00002-0-0 loss: 1.125681  [   32/  124]
train() client id: f_00002-0-1 loss: 1.113155  [   64/  124]
train() client id: f_00002-0-2 loss: 1.141415  [   96/  124]
train() client id: f_00002-1-0 loss: 1.034003  [   32/  124]
train() client id: f_00002-1-1 loss: 1.066414  [   64/  124]
train() client id: f_00002-1-2 loss: 0.902802  [   96/  124]
train() client id: f_00002-2-0 loss: 1.091349  [   32/  124]
train() client id: f_00002-2-1 loss: 1.103307  [   64/  124]
train() client id: f_00002-2-2 loss: 0.819846  [   96/  124]
train() client id: f_00002-3-0 loss: 0.991777  [   32/  124]
train() client id: f_00002-3-1 loss: 1.005865  [   64/  124]
train() client id: f_00002-3-2 loss: 0.834807  [   96/  124]
train() client id: f_00002-4-0 loss: 0.900320  [   32/  124]
train() client id: f_00002-4-1 loss: 0.953897  [   64/  124]
train() client id: f_00002-4-2 loss: 0.959851  [   96/  124]
train() client id: f_00002-5-0 loss: 1.050898  [   32/  124]
train() client id: f_00002-5-1 loss: 0.747450  [   64/  124]
train() client id: f_00002-5-2 loss: 0.930478  [   96/  124]
train() client id: f_00002-6-0 loss: 0.856956  [   32/  124]
train() client id: f_00002-6-1 loss: 0.973565  [   64/  124]
train() client id: f_00002-6-2 loss: 0.884065  [   96/  124]
train() client id: f_00002-7-0 loss: 0.867686  [   32/  124]
train() client id: f_00002-7-1 loss: 0.856309  [   64/  124]
train() client id: f_00002-7-2 loss: 1.012153  [   96/  124]
train() client id: f_00002-8-0 loss: 0.749338  [   32/  124]
train() client id: f_00002-8-1 loss: 0.793693  [   64/  124]
train() client id: f_00002-8-2 loss: 1.169549  [   96/  124]
train() client id: f_00002-9-0 loss: 1.035987  [   32/  124]
train() client id: f_00002-9-1 loss: 0.878584  [   64/  124]
train() client id: f_00002-9-2 loss: 0.738077  [   96/  124]
train() client id: f_00002-10-0 loss: 0.846990  [   32/  124]
train() client id: f_00002-10-1 loss: 1.012182  [   64/  124]
train() client id: f_00002-10-2 loss: 0.780536  [   96/  124]
train() client id: f_00003-0-0 loss: 0.736735  [   32/   43]
train() client id: f_00003-1-0 loss: 0.542607  [   32/   43]
train() client id: f_00003-2-0 loss: 0.784872  [   32/   43]
train() client id: f_00003-3-0 loss: 0.807628  [   32/   43]
train() client id: f_00003-4-0 loss: 0.733091  [   32/   43]
train() client id: f_00003-5-0 loss: 0.733807  [   32/   43]
train() client id: f_00003-6-0 loss: 0.585746  [   32/   43]
train() client id: f_00003-7-0 loss: 0.520497  [   32/   43]
train() client id: f_00003-8-0 loss: 0.619120  [   32/   43]
train() client id: f_00003-9-0 loss: 0.649925  [   32/   43]
train() client id: f_00003-10-0 loss: 0.606078  [   32/   43]
train() client id: f_00004-0-0 loss: 0.883126  [   32/  306]
train() client id: f_00004-0-1 loss: 0.840256  [   64/  306]
train() client id: f_00004-0-2 loss: 0.735131  [   96/  306]
train() client id: f_00004-0-3 loss: 0.872778  [  128/  306]
train() client id: f_00004-0-4 loss: 0.920065  [  160/  306]
train() client id: f_00004-0-5 loss: 0.897426  [  192/  306]
train() client id: f_00004-0-6 loss: 0.760196  [  224/  306]
train() client id: f_00004-0-7 loss: 0.857499  [  256/  306]
train() client id: f_00004-0-8 loss: 0.884167  [  288/  306]
train() client id: f_00004-1-0 loss: 0.996864  [   32/  306]
train() client id: f_00004-1-1 loss: 1.063059  [   64/  306]
train() client id: f_00004-1-2 loss: 0.718045  [   96/  306]
train() client id: f_00004-1-3 loss: 0.796112  [  128/  306]
train() client id: f_00004-1-4 loss: 0.795607  [  160/  306]
train() client id: f_00004-1-5 loss: 0.881814  [  192/  306]
train() client id: f_00004-1-6 loss: 0.815056  [  224/  306]
train() client id: f_00004-1-7 loss: 0.740272  [  256/  306]
train() client id: f_00004-1-8 loss: 0.802652  [  288/  306]
train() client id: f_00004-2-0 loss: 0.965233  [   32/  306]
train() client id: f_00004-2-1 loss: 0.918543  [   64/  306]
train() client id: f_00004-2-2 loss: 0.845137  [   96/  306]
train() client id: f_00004-2-3 loss: 0.989356  [  128/  306]
train() client id: f_00004-2-4 loss: 0.846413  [  160/  306]
train() client id: f_00004-2-5 loss: 0.790763  [  192/  306]
train() client id: f_00004-2-6 loss: 0.776035  [  224/  306]
train() client id: f_00004-2-7 loss: 0.634004  [  256/  306]
train() client id: f_00004-2-8 loss: 0.857260  [  288/  306]
train() client id: f_00004-3-0 loss: 0.916692  [   32/  306]
train() client id: f_00004-3-1 loss: 0.774601  [   64/  306]
train() client id: f_00004-3-2 loss: 0.754509  [   96/  306]
train() client id: f_00004-3-3 loss: 0.989331  [  128/  306]
train() client id: f_00004-3-4 loss: 0.880247  [  160/  306]
train() client id: f_00004-3-5 loss: 0.837473  [  192/  306]
train() client id: f_00004-3-6 loss: 0.815766  [  224/  306]
train() client id: f_00004-3-7 loss: 0.873538  [  256/  306]
train() client id: f_00004-3-8 loss: 0.869181  [  288/  306]
train() client id: f_00004-4-0 loss: 0.858681  [   32/  306]
train() client id: f_00004-4-1 loss: 0.906320  [   64/  306]
train() client id: f_00004-4-2 loss: 0.722058  [   96/  306]
train() client id: f_00004-4-3 loss: 0.893358  [  128/  306]
train() client id: f_00004-4-4 loss: 0.881172  [  160/  306]
train() client id: f_00004-4-5 loss: 0.772048  [  192/  306]
train() client id: f_00004-4-6 loss: 0.941489  [  224/  306]
train() client id: f_00004-4-7 loss: 0.840981  [  256/  306]
train() client id: f_00004-4-8 loss: 0.873076  [  288/  306]
train() client id: f_00004-5-0 loss: 0.753866  [   32/  306]
train() client id: f_00004-5-1 loss: 0.820328  [   64/  306]
train() client id: f_00004-5-2 loss: 0.792912  [   96/  306]
train() client id: f_00004-5-3 loss: 0.898431  [  128/  306]
train() client id: f_00004-5-4 loss: 0.948120  [  160/  306]
train() client id: f_00004-5-5 loss: 0.748020  [  192/  306]
train() client id: f_00004-5-6 loss: 0.844887  [  224/  306]
train() client id: f_00004-5-7 loss: 0.981757  [  256/  306]
train() client id: f_00004-5-8 loss: 0.825093  [  288/  306]
train() client id: f_00004-6-0 loss: 0.840128  [   32/  306]
train() client id: f_00004-6-1 loss: 0.839194  [   64/  306]
train() client id: f_00004-6-2 loss: 0.942335  [   96/  306]
train() client id: f_00004-6-3 loss: 0.965155  [  128/  306]
train() client id: f_00004-6-4 loss: 0.803136  [  160/  306]
train() client id: f_00004-6-5 loss: 0.840871  [  192/  306]
train() client id: f_00004-6-6 loss: 0.825183  [  224/  306]
train() client id: f_00004-6-7 loss: 0.767769  [  256/  306]
train() client id: f_00004-6-8 loss: 0.768611  [  288/  306]
train() client id: f_00004-7-0 loss: 0.790993  [   32/  306]
train() client id: f_00004-7-1 loss: 0.887851  [   64/  306]
train() client id: f_00004-7-2 loss: 0.857043  [   96/  306]
train() client id: f_00004-7-3 loss: 0.957559  [  128/  306]
train() client id: f_00004-7-4 loss: 0.905625  [  160/  306]
train() client id: f_00004-7-5 loss: 0.729026  [  192/  306]
train() client id: f_00004-7-6 loss: 0.969373  [  224/  306]
train() client id: f_00004-7-7 loss: 0.829749  [  256/  306]
train() client id: f_00004-7-8 loss: 0.803140  [  288/  306]
train() client id: f_00004-8-0 loss: 0.723830  [   32/  306]
train() client id: f_00004-8-1 loss: 0.781768  [   64/  306]
train() client id: f_00004-8-2 loss: 0.772044  [   96/  306]
train() client id: f_00004-8-3 loss: 0.899455  [  128/  306]
train() client id: f_00004-8-4 loss: 0.781843  [  160/  306]
train() client id: f_00004-8-5 loss: 0.934353  [  192/  306]
train() client id: f_00004-8-6 loss: 0.975780  [  224/  306]
train() client id: f_00004-8-7 loss: 0.818658  [  256/  306]
train() client id: f_00004-8-8 loss: 0.911403  [  288/  306]
train() client id: f_00004-9-0 loss: 0.822134  [   32/  306]
train() client id: f_00004-9-1 loss: 0.712339  [   64/  306]
train() client id: f_00004-9-2 loss: 0.812163  [   96/  306]
train() client id: f_00004-9-3 loss: 0.829404  [  128/  306]
train() client id: f_00004-9-4 loss: 0.886167  [  160/  306]
train() client id: f_00004-9-5 loss: 0.830490  [  192/  306]
train() client id: f_00004-9-6 loss: 0.988506  [  224/  306]
train() client id: f_00004-9-7 loss: 0.893040  [  256/  306]
train() client id: f_00004-9-8 loss: 0.797035  [  288/  306]
train() client id: f_00004-10-0 loss: 0.905213  [   32/  306]
train() client id: f_00004-10-1 loss: 0.810443  [   64/  306]
train() client id: f_00004-10-2 loss: 0.849396  [   96/  306]
train() client id: f_00004-10-3 loss: 0.848129  [  128/  306]
train() client id: f_00004-10-4 loss: 0.919217  [  160/  306]
train() client id: f_00004-10-5 loss: 0.916274  [  192/  306]
train() client id: f_00004-10-6 loss: 0.698209  [  224/  306]
train() client id: f_00004-10-7 loss: 0.797210  [  256/  306]
train() client id: f_00004-10-8 loss: 0.924808  [  288/  306]
train() client id: f_00005-0-0 loss: 0.484207  [   32/  146]
train() client id: f_00005-0-1 loss: 0.572311  [   64/  146]
train() client id: f_00005-0-2 loss: 0.306372  [   96/  146]
train() client id: f_00005-0-3 loss: 0.550251  [  128/  146]
train() client id: f_00005-1-0 loss: 0.276440  [   32/  146]
train() client id: f_00005-1-1 loss: 0.400984  [   64/  146]
train() client id: f_00005-1-2 loss: 0.533045  [   96/  146]
train() client id: f_00005-1-3 loss: 0.699517  [  128/  146]
train() client id: f_00005-2-0 loss: 0.317334  [   32/  146]
train() client id: f_00005-2-1 loss: 0.581913  [   64/  146]
train() client id: f_00005-2-2 loss: 0.377031  [   96/  146]
train() client id: f_00005-2-3 loss: 0.583293  [  128/  146]
train() client id: f_00005-3-0 loss: 0.496900  [   32/  146]
train() client id: f_00005-3-1 loss: 0.668942  [   64/  146]
train() client id: f_00005-3-2 loss: 0.342942  [   96/  146]
train() client id: f_00005-3-3 loss: 0.345390  [  128/  146]
train() client id: f_00005-4-0 loss: 0.544046  [   32/  146]
train() client id: f_00005-4-1 loss: 0.667656  [   64/  146]
train() client id: f_00005-4-2 loss: 0.379215  [   96/  146]
train() client id: f_00005-4-3 loss: 0.167305  [  128/  146]
train() client id: f_00005-5-0 loss: 0.277805  [   32/  146]
train() client id: f_00005-5-1 loss: 0.281099  [   64/  146]
train() client id: f_00005-5-2 loss: 0.482847  [   96/  146]
train() client id: f_00005-5-3 loss: 0.579779  [  128/  146]
train() client id: f_00005-6-0 loss: 0.602934  [   32/  146]
train() client id: f_00005-6-1 loss: 0.190227  [   64/  146]
train() client id: f_00005-6-2 loss: 0.436970  [   96/  146]
train() client id: f_00005-6-3 loss: 0.693078  [  128/  146]
train() client id: f_00005-7-0 loss: 0.378277  [   32/  146]
train() client id: f_00005-7-1 loss: 0.366504  [   64/  146]
train() client id: f_00005-7-2 loss: 0.676613  [   96/  146]
train() client id: f_00005-7-3 loss: 0.286013  [  128/  146]
train() client id: f_00005-8-0 loss: 0.230254  [   32/  146]
train() client id: f_00005-8-1 loss: 0.668698  [   64/  146]
train() client id: f_00005-8-2 loss: 0.587001  [   96/  146]
train() client id: f_00005-8-3 loss: 0.119568  [  128/  146]
train() client id: f_00005-9-0 loss: 0.590057  [   32/  146]
train() client id: f_00005-9-1 loss: 0.496254  [   64/  146]
train() client id: f_00005-9-2 loss: 0.397239  [   96/  146]
train() client id: f_00005-9-3 loss: 0.422937  [  128/  146]
train() client id: f_00005-10-0 loss: 0.365550  [   32/  146]
train() client id: f_00005-10-1 loss: 0.451703  [   64/  146]
train() client id: f_00005-10-2 loss: 0.323307  [   96/  146]
train() client id: f_00005-10-3 loss: 0.384429  [  128/  146]
train() client id: f_00006-0-0 loss: 0.532335  [   32/   54]
train() client id: f_00006-1-0 loss: 0.460640  [   32/   54]
train() client id: f_00006-2-0 loss: 0.508627  [   32/   54]
train() client id: f_00006-3-0 loss: 0.421490  [   32/   54]
train() client id: f_00006-4-0 loss: 0.536570  [   32/   54]
train() client id: f_00006-5-0 loss: 0.482683  [   32/   54]
train() client id: f_00006-6-0 loss: 0.535975  [   32/   54]
train() client id: f_00006-7-0 loss: 0.521537  [   32/   54]
train() client id: f_00006-8-0 loss: 0.519172  [   32/   54]
train() client id: f_00006-9-0 loss: 0.425836  [   32/   54]
train() client id: f_00006-10-0 loss: 0.488562  [   32/   54]
train() client id: f_00007-0-0 loss: 0.448084  [   32/  179]
train() client id: f_00007-0-1 loss: 0.750533  [   64/  179]
train() client id: f_00007-0-2 loss: 0.499544  [   96/  179]
train() client id: f_00007-0-3 loss: 0.409839  [  128/  179]
train() client id: f_00007-0-4 loss: 0.291314  [  160/  179]
train() client id: f_00007-1-0 loss: 0.393008  [   32/  179]
train() client id: f_00007-1-1 loss: 0.344743  [   64/  179]
train() client id: f_00007-1-2 loss: 0.323666  [   96/  179]
train() client id: f_00007-1-3 loss: 0.542762  [  128/  179]
train() client id: f_00007-1-4 loss: 0.634769  [  160/  179]
train() client id: f_00007-2-0 loss: 0.517084  [   32/  179]
train() client id: f_00007-2-1 loss: 0.550334  [   64/  179]
train() client id: f_00007-2-2 loss: 0.476489  [   96/  179]
train() client id: f_00007-2-3 loss: 0.235981  [  128/  179]
train() client id: f_00007-2-4 loss: 0.426771  [  160/  179]
train() client id: f_00007-3-0 loss: 0.251479  [   32/  179]
train() client id: f_00007-3-1 loss: 0.289161  [   64/  179]
train() client id: f_00007-3-2 loss: 0.639606  [   96/  179]
train() client id: f_00007-3-3 loss: 0.376984  [  128/  179]
train() client id: f_00007-3-4 loss: 0.385522  [  160/  179]
train() client id: f_00007-4-0 loss: 0.476413  [   32/  179]
train() client id: f_00007-4-1 loss: 0.274450  [   64/  179]
train() client id: f_00007-4-2 loss: 0.347969  [   96/  179]
train() client id: f_00007-4-3 loss: 0.532191  [  128/  179]
train() client id: f_00007-4-4 loss: 0.341395  [  160/  179]
train() client id: f_00007-5-0 loss: 0.490150  [   32/  179]
train() client id: f_00007-5-1 loss: 0.363608  [   64/  179]
train() client id: f_00007-5-2 loss: 0.328992  [   96/  179]
train() client id: f_00007-5-3 loss: 0.375722  [  128/  179]
train() client id: f_00007-5-4 loss: 0.614101  [  160/  179]
train() client id: f_00007-6-0 loss: 0.346685  [   32/  179]
train() client id: f_00007-6-1 loss: 0.555003  [   64/  179]
train() client id: f_00007-6-2 loss: 0.343122  [   96/  179]
train() client id: f_00007-6-3 loss: 0.435273  [  128/  179]
train() client id: f_00007-6-4 loss: 0.407527  [  160/  179]
train() client id: f_00007-7-0 loss: 0.410674  [   32/  179]
train() client id: f_00007-7-1 loss: 0.341016  [   64/  179]
train() client id: f_00007-7-2 loss: 0.355554  [   96/  179]
train() client id: f_00007-7-3 loss: 0.453577  [  128/  179]
train() client id: f_00007-7-4 loss: 0.436561  [  160/  179]
train() client id: f_00007-8-0 loss: 0.250366  [   32/  179]
train() client id: f_00007-8-1 loss: 0.445778  [   64/  179]
train() client id: f_00007-8-2 loss: 0.471978  [   96/  179]
train() client id: f_00007-8-3 loss: 0.237422  [  128/  179]
train() client id: f_00007-8-4 loss: 0.420572  [  160/  179]
train() client id: f_00007-9-0 loss: 0.429032  [   32/  179]
train() client id: f_00007-9-1 loss: 0.329956  [   64/  179]
train() client id: f_00007-9-2 loss: 0.336407  [   96/  179]
train() client id: f_00007-9-3 loss: 0.492038  [  128/  179]
train() client id: f_00007-9-4 loss: 0.389424  [  160/  179]
train() client id: f_00007-10-0 loss: 0.401505  [   32/  179]
train() client id: f_00007-10-1 loss: 0.247296  [   64/  179]
train() client id: f_00007-10-2 loss: 0.304234  [   96/  179]
train() client id: f_00007-10-3 loss: 0.461890  [  128/  179]
train() client id: f_00007-10-4 loss: 0.461928  [  160/  179]
train() client id: f_00008-0-0 loss: 0.633139  [   32/  130]
train() client id: f_00008-0-1 loss: 0.796600  [   64/  130]
train() client id: f_00008-0-2 loss: 0.586578  [   96/  130]
train() client id: f_00008-0-3 loss: 0.610761  [  128/  130]
train() client id: f_00008-1-0 loss: 0.603238  [   32/  130]
train() client id: f_00008-1-1 loss: 0.715429  [   64/  130]
train() client id: f_00008-1-2 loss: 0.653214  [   96/  130]
train() client id: f_00008-1-3 loss: 0.651287  [  128/  130]
train() client id: f_00008-2-0 loss: 0.640409  [   32/  130]
train() client id: f_00008-2-1 loss: 0.582326  [   64/  130]
train() client id: f_00008-2-2 loss: 0.711456  [   96/  130]
train() client id: f_00008-2-3 loss: 0.701108  [  128/  130]
train() client id: f_00008-3-0 loss: 0.684041  [   32/  130]
train() client id: f_00008-3-1 loss: 0.717349  [   64/  130]
train() client id: f_00008-3-2 loss: 0.624794  [   96/  130]
train() client id: f_00008-3-3 loss: 0.605273  [  128/  130]
train() client id: f_00008-4-0 loss: 0.577368  [   32/  130]
train() client id: f_00008-4-1 loss: 0.686276  [   64/  130]
train() client id: f_00008-4-2 loss: 0.624011  [   96/  130]
train() client id: f_00008-4-3 loss: 0.738149  [  128/  130]
train() client id: f_00008-5-0 loss: 0.695795  [   32/  130]
train() client id: f_00008-5-1 loss: 0.727334  [   64/  130]
train() client id: f_00008-5-2 loss: 0.621106  [   96/  130]
train() client id: f_00008-5-3 loss: 0.584806  [  128/  130]
train() client id: f_00008-6-0 loss: 0.610176  [   32/  130]
train() client id: f_00008-6-1 loss: 0.729006  [   64/  130]
train() client id: f_00008-6-2 loss: 0.661604  [   96/  130]
train() client id: f_00008-6-3 loss: 0.630142  [  128/  130]
train() client id: f_00008-7-0 loss: 0.729574  [   32/  130]
train() client id: f_00008-7-1 loss: 0.632256  [   64/  130]
train() client id: f_00008-7-2 loss: 0.641805  [   96/  130]
train() client id: f_00008-7-3 loss: 0.628906  [  128/  130]
train() client id: f_00008-8-0 loss: 0.637071  [   32/  130]
train() client id: f_00008-8-1 loss: 0.731391  [   64/  130]
train() client id: f_00008-8-2 loss: 0.528140  [   96/  130]
train() client id: f_00008-8-3 loss: 0.692777  [  128/  130]
train() client id: f_00008-9-0 loss: 0.707050  [   32/  130]
train() client id: f_00008-9-1 loss: 0.581826  [   64/  130]
train() client id: f_00008-9-2 loss: 0.620318  [   96/  130]
train() client id: f_00008-9-3 loss: 0.687511  [  128/  130]
train() client id: f_00008-10-0 loss: 0.686118  [   32/  130]
train() client id: f_00008-10-1 loss: 0.682932  [   64/  130]
train() client id: f_00008-10-2 loss: 0.657683  [   96/  130]
train() client id: f_00008-10-3 loss: 0.608915  [  128/  130]
train() client id: f_00009-0-0 loss: 1.322747  [   32/  118]
train() client id: f_00009-0-1 loss: 1.061638  [   64/  118]
train() client id: f_00009-0-2 loss: 1.286978  [   96/  118]
train() client id: f_00009-1-0 loss: 1.103876  [   32/  118]
train() client id: f_00009-1-1 loss: 1.197874  [   64/  118]
train() client id: f_00009-1-2 loss: 1.166333  [   96/  118]
train() client id: f_00009-2-0 loss: 1.080055  [   32/  118]
train() client id: f_00009-2-1 loss: 1.155495  [   64/  118]
train() client id: f_00009-2-2 loss: 1.090561  [   96/  118]
train() client id: f_00009-3-0 loss: 1.097923  [   32/  118]
train() client id: f_00009-3-1 loss: 1.154702  [   64/  118]
train() client id: f_00009-3-2 loss: 1.051869  [   96/  118]
train() client id: f_00009-4-0 loss: 0.969587  [   32/  118]
train() client id: f_00009-4-1 loss: 1.156663  [   64/  118]
train() client id: f_00009-4-2 loss: 1.141292  [   96/  118]
train() client id: f_00009-5-0 loss: 1.170543  [   32/  118]
train() client id: f_00009-5-1 loss: 0.826507  [   64/  118]
train() client id: f_00009-5-2 loss: 1.076332  [   96/  118]
train() client id: f_00009-6-0 loss: 0.971565  [   32/  118]
train() client id: f_00009-6-1 loss: 1.039149  [   64/  118]
train() client id: f_00009-6-2 loss: 1.012957  [   96/  118]
train() client id: f_00009-7-0 loss: 0.958956  [   32/  118]
train() client id: f_00009-7-1 loss: 1.112528  [   64/  118]
train() client id: f_00009-7-2 loss: 0.892714  [   96/  118]
train() client id: f_00009-8-0 loss: 0.828982  [   32/  118]
train() client id: f_00009-8-1 loss: 1.130610  [   64/  118]
train() client id: f_00009-8-2 loss: 0.952296  [   96/  118]
train() client id: f_00009-9-0 loss: 0.953962  [   32/  118]
train() client id: f_00009-9-1 loss: 0.871085  [   64/  118]
train() client id: f_00009-9-2 loss: 0.926122  [   96/  118]
train() client id: f_00009-10-0 loss: 0.901873  [   32/  118]
train() client id: f_00009-10-1 loss: 1.015307  [   64/  118]
train() client id: f_00009-10-2 loss: 0.963758  [   96/  118]
At round 49 accuracy: 0.649867374005305
At round 49 training accuracy: 0.590878604963112
At round 49 training loss: 0.8303627751880902
update_location
xs = -3.905658 4.200318 265.009024 18.811294 0.979296 3.956410 -227.443192 -206.324852 249.663977 -192.060879 
ys = 257.587959 240.555839 1.320614 -227.455176 219.350187 202.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 276.345456 260.547029 283.251702 249.178093 241.071490 226.161962 248.469910 229.282840 269.519519 216.571912 
dists_bs = 189.984328 190.814889 473.045949 446.691153 181.481822 181.105007 185.205085 176.972034 452.906184 171.847510 
uav_gains = -113.648252 -112.216848 -114.273340 -111.226272 -110.557415 -109.423809 -111.166457 -109.650790 -113.026671 -108.756597 
bs_gains = -103.371157 -103.424202 -114.464352 -113.767265 -102.814387 -102.789112 -103.061341 -102.508389 -113.935290 -102.151070 
Round 50
-------------------------------
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.40113982  9.00691085  4.34261068  1.57832816 10.38540444  4.99669129
  1.94822841  6.14510341  4.53980435  4.05137628]
obj_prev = 51.395597693307295
eta_min = 1.1367158248091635e-21	eta_max = 0.9395757044516898
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 11.874554720679596	eta = 0.909090909090909
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 24.172264619780965	eta = 0.44658826617504477
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 17.836063247717608	eta = 0.6052372430028105
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 16.691468667850707	eta = 0.6467405571604739
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 16.62571758399257	eta = 0.6492982748886553
af = 10.795049746072358	bf = 1.1686004695034842	zeta = 16.62547813342434	eta = 0.6493076264898318
eta = 0.6493076264898318
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [0.03563512 0.07494687 0.03506948 0.01216119 0.08654243 0.04129149
 0.01527219 0.05062449 0.03676639 0.03337257]
ene_total = [1.56827942 2.59415826 1.57537116 0.75074681 2.95309342 1.52632602
 0.84721729 1.92501916 1.61509008 1.2701765 ]
ti_comp = [0.73120545 0.79889761 0.72329946 0.75580078 0.8009931  0.80107742
 0.7563088  0.76793556 0.72766908 0.80314305]
ti_coms = [0.14113249 0.07344032 0.14903847 0.11653715 0.07134483 0.07126051
 0.11602913 0.10440237 0.14466885 0.06919488]
t_total = [27.49979019 27.49979019 27.49979019 27.49979019 27.49979019 27.49979019
 27.49979019 27.49979019 27.49979019 27.49979019]
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [5.28975756e-06 4.12247795e-05 5.15266296e-06 1.96785757e-07
 6.31407441e-05 6.85665898e-06 3.89211744e-07 1.37503103e-05
 5.86631230e-06 3.60133211e-06]
ene_total = [0.44507619 0.2328146  0.46999487 0.36738071 0.22689962 0.22485949
 0.36578528 0.32955399 0.45624248 0.21824512]
optimize_network iter = 0 obj = 3.3368523467960083
eta = 0.6493076264898318
freqs = [24367376.98591928 46906432.60757383 24242711.66907599  8045235.73109414
 54021956.04098655 25772468.52031963 10096532.76307652 32961419.31716986
 25263125.77228831 20776230.43734453]
eta_min = 0.6493076264898333	eta_max = 0.6979381775060964
af = 0.0037239897789525765	bf = 1.1686004695034842	zeta = 0.004096388756847835	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [1.16746392e-06 9.09842125e-06 1.13720677e-06 4.34311531e-08
 1.39353344e-05 1.51328333e-06 8.59000931e-08 3.03473097e-06
 1.29471112e-06 7.94823816e-07]
ene_total = [1.71643177 0.89420252 1.81257148 1.41719511 0.86930783 0.86677177
 1.41102229 1.26998969 1.7594524  0.84156459]
ti_comp = [0.61023825 0.67793041 0.60233226 0.63483358 0.6800259  0.68011022
 0.6353416  0.64696837 0.60670188 0.68217585]
ti_coms = [0.14113249 0.07344032 0.14903847 0.11653715 0.07134483 0.07126051
 0.11602913 0.10440237 0.14466885 0.06919488]
t_total = [27.49979019 27.49979019 27.49979019 27.49979019 27.49979019 27.49979019
 27.49979019 27.49979019 27.49979019 27.49979019]
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [4.38601091e-06 3.30616333e-05 4.29091311e-06 1.61080327e-07
 5.05906534e-05 5.49359804e-06 3.18510394e-07 1.11879346e-05
 4.87344749e-06 2.88277172e-06]
ene_total = [0.51669831 0.2699979  0.54563033 0.42652599 0.26297008 0.26101093
 0.42467242 0.38251687 0.52965907 0.25335527]
optimize_network iter = 1 obj = 3.8730371713241745
eta = 0.6979381775060964
freqs = [24314579.32952386 46031634.30360291 24242711.66907599  7976349.99245549
 52989718.17503818 25279541.36753655 10008802.12815238 32581091.06223162
 25232695.58126215 20369543.75424322]
eta_min = 0.6979381775061025	eta_max = 0.697938177506091
af = 0.0036043043465234475	bf = 1.1686004695034842	zeta = 0.003964734781175793	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [1.16241023e-06 8.76221731e-06 1.13720677e-06 4.26905959e-08
 1.34078766e-05 1.45595045e-06 8.44137753e-08 2.96510198e-06
 1.29159396e-06 7.64011626e-07]
ene_total = [1.71643115 0.89416163 1.81257148 1.41719502 0.86924368 0.8667648
 1.41102211 1.26998122 1.75945202 0.84156084]
ti_comp = [0.61023825 0.67793041 0.60233226 0.63483358 0.6800259  0.68011022
 0.6353416  0.64696837 0.60670188 0.68217585]
ti_coms = [0.14113249 0.07344032 0.14903847 0.11653715 0.07134483 0.07126051
 0.11602913 0.10440237 0.14466885 0.06919488]
t_total = [27.49979019 27.49979019 27.49979019 27.49979019 27.49979019 27.49979019
 27.49979019 27.49979019 27.49979019 27.49979019]
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [4.38601091e-06 3.30616333e-05 4.29091311e-06 1.61080327e-07
 5.05906534e-05 5.49359804e-06 3.18510394e-07 1.11879346e-05
 4.87344749e-06 2.88277172e-06]
ene_total = [0.51669831 0.2699979  0.54563033 0.42652599 0.26297008 0.26101093
 0.42467242 0.38251687 0.52965907 0.25335527]
optimize_network iter = 2 obj = 3.873037171324106
eta = 0.697938177506091
freqs = [24314579.32952385 46031634.30360299 24242711.66907597  7976349.99245549
 52989718.17503828 25279541.3675366  10008802.12815239 32581091.06223165
 25232695.58126213 20369543.75424326]
Done!
ene_coms = [0.01411325 0.00734403 0.01490385 0.01165372 0.00713448 0.00712605
 0.01160291 0.01044024 0.01446688 0.00691949]
ene_comp = [4.09700746e-06 3.08831330e-05 4.00817585e-06 1.50466407e-07
 4.72571292e-05 5.13161335e-06 2.97523077e-07 1.04507382e-05
 4.55232582e-06 2.69281985e-06]
ene_total = [0.01411735 0.00737491 0.01490785 0.01165387 0.00718174 0.00713118
 0.01160321 0.01045069 0.01447144 0.00692218]
At round 50 energy consumption: 0.10581441892494492
At round 50 eta: 0.697938177506091
At round 50 a_n: 11.055310507788512
At round 50 local rounds: 11.775941462244061
At round 50 global rounds: 36.59949614457299
gradient difference: 0.4786756634712219
train() client id: f_00000-0-0 loss: 1.153141  [   32/  126]
train() client id: f_00000-0-1 loss: 0.997055  [   64/  126]
train() client id: f_00000-0-2 loss: 1.103011  [   96/  126]
train() client id: f_00000-1-0 loss: 0.968142  [   32/  126]
train() client id: f_00000-1-1 loss: 0.966902  [   64/  126]
train() client id: f_00000-1-2 loss: 1.088898  [   96/  126]
train() client id: f_00000-2-0 loss: 0.988047  [   32/  126]
train() client id: f_00000-2-1 loss: 0.808991  [   64/  126]
train() client id: f_00000-2-2 loss: 0.960515  [   96/  126]
train() client id: f_00000-3-0 loss: 0.963922  [   32/  126]
train() client id: f_00000-3-1 loss: 0.940923  [   64/  126]
train() client id: f_00000-3-2 loss: 0.759342  [   96/  126]
train() client id: f_00000-4-0 loss: 0.799470  [   32/  126]
train() client id: f_00000-4-1 loss: 0.967987  [   64/  126]
train() client id: f_00000-4-2 loss: 0.738314  [   96/  126]
train() client id: f_00000-5-0 loss: 0.732911  [   32/  126]
train() client id: f_00000-5-1 loss: 0.755732  [   64/  126]
train() client id: f_00000-5-2 loss: 0.755093  [   96/  126]
train() client id: f_00000-6-0 loss: 0.841288  [   32/  126]
train() client id: f_00000-6-1 loss: 0.690981  [   64/  126]
train() client id: f_00000-6-2 loss: 0.665076  [   96/  126]
train() client id: f_00000-7-0 loss: 0.884367  [   32/  126]
train() client id: f_00000-7-1 loss: 0.778019  [   64/  126]
train() client id: f_00000-7-2 loss: 0.699196  [   96/  126]
train() client id: f_00000-8-0 loss: 0.854992  [   32/  126]
train() client id: f_00000-8-1 loss: 0.572896  [   64/  126]
train() client id: f_00000-8-2 loss: 0.829078  [   96/  126]
train() client id: f_00000-9-0 loss: 0.662081  [   32/  126]
train() client id: f_00000-9-1 loss: 0.805051  [   64/  126]
train() client id: f_00000-9-2 loss: 0.764003  [   96/  126]
train() client id: f_00000-10-0 loss: 0.741074  [   32/  126]
train() client id: f_00000-10-1 loss: 0.664024  [   64/  126]
train() client id: f_00000-10-2 loss: 0.761140  [   96/  126]
train() client id: f_00001-0-0 loss: 0.453299  [   32/  265]
train() client id: f_00001-0-1 loss: 0.411897  [   64/  265]
train() client id: f_00001-0-2 loss: 0.478948  [   96/  265]
train() client id: f_00001-0-3 loss: 0.465537  [  128/  265]
train() client id: f_00001-0-4 loss: 0.407763  [  160/  265]
train() client id: f_00001-0-5 loss: 0.432413  [  192/  265]
train() client id: f_00001-0-6 loss: 0.434018  [  224/  265]
train() client id: f_00001-0-7 loss: 0.514463  [  256/  265]
train() client id: f_00001-1-0 loss: 0.419178  [   32/  265]
train() client id: f_00001-1-1 loss: 0.395488  [   64/  265]
train() client id: f_00001-1-2 loss: 0.436504  [   96/  265]
train() client id: f_00001-1-3 loss: 0.379826  [  128/  265]
train() client id: f_00001-1-4 loss: 0.526079  [  160/  265]
train() client id: f_00001-1-5 loss: 0.601576  [  192/  265]
train() client id: f_00001-1-6 loss: 0.391080  [  224/  265]
train() client id: f_00001-1-7 loss: 0.413540  [  256/  265]
train() client id: f_00001-2-0 loss: 0.582479  [   32/  265]
train() client id: f_00001-2-1 loss: 0.378999  [   64/  265]
train() client id: f_00001-2-2 loss: 0.411244  [   96/  265]
train() client id: f_00001-2-3 loss: 0.405564  [  128/  265]
train() client id: f_00001-2-4 loss: 0.379338  [  160/  265]
train() client id: f_00001-2-5 loss: 0.353244  [  192/  265]
train() client id: f_00001-2-6 loss: 0.442145  [  224/  265]
train() client id: f_00001-2-7 loss: 0.492878  [  256/  265]
train() client id: f_00001-3-0 loss: 0.331599  [   32/  265]
train() client id: f_00001-3-1 loss: 0.404414  [   64/  265]
train() client id: f_00001-3-2 loss: 0.484512  [   96/  265]
train() client id: f_00001-3-3 loss: 0.386723  [  128/  265]
train() client id: f_00001-3-4 loss: 0.506471  [  160/  265]
train() client id: f_00001-3-5 loss: 0.386557  [  192/  265]
train() client id: f_00001-3-6 loss: 0.400110  [  224/  265]
train() client id: f_00001-3-7 loss: 0.416995  [  256/  265]
train() client id: f_00001-4-0 loss: 0.410489  [   32/  265]
train() client id: f_00001-4-1 loss: 0.345226  [   64/  265]
train() client id: f_00001-4-2 loss: 0.445866  [   96/  265]
train() client id: f_00001-4-3 loss: 0.369061  [  128/  265]
train() client id: f_00001-4-4 loss: 0.372499  [  160/  265]
train() client id: f_00001-4-5 loss: 0.577297  [  192/  265]
train() client id: f_00001-4-6 loss: 0.571499  [  224/  265]
train() client id: f_00001-4-7 loss: 0.343404  [  256/  265]
train() client id: f_00001-5-0 loss: 0.452051  [   32/  265]
train() client id: f_00001-5-1 loss: 0.354828  [   64/  265]
train() client id: f_00001-5-2 loss: 0.417163  [   96/  265]
train() client id: f_00001-5-3 loss: 0.331182  [  128/  265]
train() client id: f_00001-5-4 loss: 0.454580  [  160/  265]
train() client id: f_00001-5-5 loss: 0.546447  [  192/  265]
train() client id: f_00001-5-6 loss: 0.431171  [  224/  265]
train() client id: f_00001-5-7 loss: 0.428859  [  256/  265]
train() client id: f_00001-6-0 loss: 0.437693  [   32/  265]
train() client id: f_00001-6-1 loss: 0.488968  [   64/  265]
train() client id: f_00001-6-2 loss: 0.459468  [   96/  265]
train() client id: f_00001-6-3 loss: 0.511970  [  128/  265]
train() client id: f_00001-6-4 loss: 0.478776  [  160/  265]
train() client id: f_00001-6-5 loss: 0.371374  [  192/  265]
train() client id: f_00001-6-6 loss: 0.329628  [  224/  265]
train() client id: f_00001-6-7 loss: 0.334845  [  256/  265]
train() client id: f_00001-7-0 loss: 0.417745  [   32/  265]
train() client id: f_00001-7-1 loss: 0.464907  [   64/  265]
train() client id: f_00001-7-2 loss: 0.518360  [   96/  265]
train() client id: f_00001-7-3 loss: 0.440405  [  128/  265]
train() client id: f_00001-7-4 loss: 0.347377  [  160/  265]
train() client id: f_00001-7-5 loss: 0.384572  [  192/  265]
train() client id: f_00001-7-6 loss: 0.408164  [  224/  265]
train() client id: f_00001-7-7 loss: 0.422055  [  256/  265]
train() client id: f_00001-8-0 loss: 0.395436  [   32/  265]
train() client id: f_00001-8-1 loss: 0.335365  [   64/  265]
train() client id: f_00001-8-2 loss: 0.462873  [   96/  265]
train() client id: f_00001-8-3 loss: 0.374736  [  128/  265]
train() client id: f_00001-8-4 loss: 0.463093  [  160/  265]
train() client id: f_00001-8-5 loss: 0.412455  [  192/  265]
train() client id: f_00001-8-6 loss: 0.380081  [  224/  265]
train() client id: f_00001-8-7 loss: 0.419773  [  256/  265]
train() client id: f_00001-9-0 loss: 0.373799  [   32/  265]
train() client id: f_00001-9-1 loss: 0.400929  [   64/  265]
train() client id: f_00001-9-2 loss: 0.512703  [   96/  265]
train() client id: f_00001-9-3 loss: 0.367914  [  128/  265]
train() client id: f_00001-9-4 loss: 0.480441  [  160/  265]
train() client id: f_00001-9-5 loss: 0.311674  [  192/  265]
train() client id: f_00001-9-6 loss: 0.466419  [  224/  265]
train() client id: f_00001-9-7 loss: 0.417773  [  256/  265]
train() client id: f_00001-10-0 loss: 0.464693  [   32/  265]
train() client id: f_00001-10-1 loss: 0.408866  [   64/  265]
train() client id: f_00001-10-2 loss: 0.402630  [   96/  265]
train() client id: f_00001-10-3 loss: 0.371359  [  128/  265]
train() client id: f_00001-10-4 loss: 0.442587  [  160/  265]
train() client id: f_00001-10-5 loss: 0.376344  [  192/  265]
train() client id: f_00001-10-6 loss: 0.342758  [  224/  265]
train() client id: f_00001-10-7 loss: 0.576972  [  256/  265]
train() client id: f_00002-0-0 loss: 1.362577  [   32/  124]
train() client id: f_00002-0-1 loss: 1.240971  [   64/  124]
train() client id: f_00002-0-2 loss: 1.159365  [   96/  124]
train() client id: f_00002-1-0 loss: 1.205708  [   32/  124]
train() client id: f_00002-1-1 loss: 1.068858  [   64/  124]
train() client id: f_00002-1-2 loss: 1.329624  [   96/  124]
train() client id: f_00002-2-0 loss: 1.049143  [   32/  124]
train() client id: f_00002-2-1 loss: 1.228918  [   64/  124]
train() client id: f_00002-2-2 loss: 1.198938  [   96/  124]
train() client id: f_00002-3-0 loss: 1.289173  [   32/  124]
train() client id: f_00002-3-1 loss: 1.069770  [   64/  124]
train() client id: f_00002-3-2 loss: 1.055170  [   96/  124]
train() client id: f_00002-4-0 loss: 1.174350  [   32/  124]
train() client id: f_00002-4-1 loss: 1.167773  [   64/  124]
train() client id: f_00002-4-2 loss: 1.073994  [   96/  124]
train() client id: f_00002-5-0 loss: 1.101818  [   32/  124]
train() client id: f_00002-5-1 loss: 0.907059  [   64/  124]
train() client id: f_00002-5-2 loss: 1.282198  [   96/  124]
train() client id: f_00002-6-0 loss: 1.090065  [   32/  124]
train() client id: f_00002-6-1 loss: 1.106639  [   64/  124]
train() client id: f_00002-6-2 loss: 1.096629  [   96/  124]
train() client id: f_00002-7-0 loss: 0.989103  [   32/  124]
train() client id: f_00002-7-1 loss: 1.217885  [   64/  124]
train() client id: f_00002-7-2 loss: 0.926070  [   96/  124]
train() client id: f_00002-8-0 loss: 0.954645  [   32/  124]
train() client id: f_00002-8-1 loss: 1.177855  [   64/  124]
train() client id: f_00002-8-2 loss: 0.930766  [   96/  124]
train() client id: f_00002-9-0 loss: 1.039271  [   32/  124]
train() client id: f_00002-9-1 loss: 1.117835  [   64/  124]
train() client id: f_00002-9-2 loss: 1.026547  [   96/  124]
train() client id: f_00002-10-0 loss: 1.196281  [   32/  124]
train() client id: f_00002-10-1 loss: 1.003451  [   64/  124]
train() client id: f_00002-10-2 loss: 1.052423  [   96/  124]
train() client id: f_00003-0-0 loss: 0.747460  [   32/   43]
train() client id: f_00003-1-0 loss: 0.604663  [   32/   43]
train() client id: f_00003-2-0 loss: 0.825589  [   32/   43]
train() client id: f_00003-3-0 loss: 0.677446  [   32/   43]
train() client id: f_00003-4-0 loss: 0.655706  [   32/   43]
train() client id: f_00003-5-0 loss: 0.647982  [   32/   43]
train() client id: f_00003-6-0 loss: 0.572435  [   32/   43]
train() client id: f_00003-7-0 loss: 0.728185  [   32/   43]
train() client id: f_00003-8-0 loss: 0.635608  [   32/   43]
train() client id: f_00003-9-0 loss: 0.741482  [   32/   43]
train() client id: f_00003-10-0 loss: 0.609398  [   32/   43]
train() client id: f_00004-0-0 loss: 0.766709  [   32/  306]
train() client id: f_00004-0-1 loss: 0.743177  [   64/  306]
train() client id: f_00004-0-2 loss: 0.922155  [   96/  306]
train() client id: f_00004-0-3 loss: 0.824613  [  128/  306]
train() client id: f_00004-0-4 loss: 0.742209  [  160/  306]
train() client id: f_00004-0-5 loss: 0.713144  [  192/  306]
train() client id: f_00004-0-6 loss: 0.660244  [  224/  306]
train() client id: f_00004-0-7 loss: 0.667822  [  256/  306]
train() client id: f_00004-0-8 loss: 0.636702  [  288/  306]
train() client id: f_00004-1-0 loss: 0.723244  [   32/  306]
train() client id: f_00004-1-1 loss: 0.697366  [   64/  306]
train() client id: f_00004-1-2 loss: 0.594133  [   96/  306]
train() client id: f_00004-1-3 loss: 0.803399  [  128/  306]
train() client id: f_00004-1-4 loss: 0.688716  [  160/  306]
train() client id: f_00004-1-5 loss: 0.773520  [  192/  306]
train() client id: f_00004-1-6 loss: 0.712988  [  224/  306]
train() client id: f_00004-1-7 loss: 0.675078  [  256/  306]
train() client id: f_00004-1-8 loss: 0.908655  [  288/  306]
train() client id: f_00004-2-0 loss: 0.716925  [   32/  306]
train() client id: f_00004-2-1 loss: 0.752467  [   64/  306]
train() client id: f_00004-2-2 loss: 0.782652  [   96/  306]
train() client id: f_00004-2-3 loss: 0.713159  [  128/  306]
train() client id: f_00004-2-4 loss: 0.687738  [  160/  306]
train() client id: f_00004-2-5 loss: 0.830793  [  192/  306]
train() client id: f_00004-2-6 loss: 0.753060  [  224/  306]
train() client id: f_00004-2-7 loss: 0.764661  [  256/  306]
train() client id: f_00004-2-8 loss: 0.613569  [  288/  306]
train() client id: f_00004-3-0 loss: 0.701901  [   32/  306]
train() client id: f_00004-3-1 loss: 0.589550  [   64/  306]
train() client id: f_00004-3-2 loss: 0.766518  [   96/  306]
train() client id: f_00004-3-3 loss: 0.775137  [  128/  306]
train() client id: f_00004-3-4 loss: 0.711997  [  160/  306]
train() client id: f_00004-3-5 loss: 0.721129  [  192/  306]
train() client id: f_00004-3-6 loss: 0.807804  [  224/  306]
train() client id: f_00004-3-7 loss: 0.799131  [  256/  306]
train() client id: f_00004-3-8 loss: 0.818160  [  288/  306]
train() client id: f_00004-4-0 loss: 0.657236  [   32/  306]
train() client id: f_00004-4-1 loss: 0.799567  [   64/  306]
train() client id: f_00004-4-2 loss: 0.793449  [   96/  306]
train() client id: f_00004-4-3 loss: 0.785203  [  128/  306]
train() client id: f_00004-4-4 loss: 0.774288  [  160/  306]
train() client id: f_00004-4-5 loss: 0.745253  [  192/  306]
train() client id: f_00004-4-6 loss: 0.921063  [  224/  306]
train() client id: f_00004-4-7 loss: 0.621212  [  256/  306]
train() client id: f_00004-4-8 loss: 0.707951  [  288/  306]
train() client id: f_00004-5-0 loss: 0.816012  [   32/  306]
train() client id: f_00004-5-1 loss: 0.607997  [   64/  306]
train() client id: f_00004-5-2 loss: 0.668897  [   96/  306]
train() client id: f_00004-5-3 loss: 0.660385  [  128/  306]
train() client id: f_00004-5-4 loss: 0.791086  [  160/  306]
train() client id: f_00004-5-5 loss: 0.805775  [  192/  306]
train() client id: f_00004-5-6 loss: 0.657058  [  224/  306]
train() client id: f_00004-5-7 loss: 0.788522  [  256/  306]
train() client id: f_00004-5-8 loss: 0.781660  [  288/  306]
train() client id: f_00004-6-0 loss: 0.684624  [   32/  306]
train() client id: f_00004-6-1 loss: 0.675815  [   64/  306]
train() client id: f_00004-6-2 loss: 0.796692  [   96/  306]
train() client id: f_00004-6-3 loss: 0.791698  [  128/  306]
train() client id: f_00004-6-4 loss: 0.815162  [  160/  306]
train() client id: f_00004-6-5 loss: 0.893464  [  192/  306]
train() client id: f_00004-6-6 loss: 0.693570  [  224/  306]
train() client id: f_00004-6-7 loss: 0.779373  [  256/  306]
train() client id: f_00004-6-8 loss: 0.732623  [  288/  306]
train() client id: f_00004-7-0 loss: 0.690413  [   32/  306]
train() client id: f_00004-7-1 loss: 0.716215  [   64/  306]
train() client id: f_00004-7-2 loss: 0.811751  [   96/  306]
train() client id: f_00004-7-3 loss: 0.828623  [  128/  306]
train() client id: f_00004-7-4 loss: 0.768310  [  160/  306]
train() client id: f_00004-7-5 loss: 0.853713  [  192/  306]
train() client id: f_00004-7-6 loss: 0.783328  [  224/  306]
train() client id: f_00004-7-7 loss: 0.654581  [  256/  306]
train() client id: f_00004-7-8 loss: 0.765078  [  288/  306]
train() client id: f_00004-8-0 loss: 0.844725  [   32/  306]
train() client id: f_00004-8-1 loss: 0.813235  [   64/  306]
train() client id: f_00004-8-2 loss: 0.590799  [   96/  306]
train() client id: f_00004-8-3 loss: 0.681438  [  128/  306]
train() client id: f_00004-8-4 loss: 1.032773  [  160/  306]
train() client id: f_00004-8-5 loss: 0.698181  [  192/  306]
train() client id: f_00004-8-6 loss: 0.745901  [  224/  306]
train() client id: f_00004-8-7 loss: 0.767281  [  256/  306]
train() client id: f_00004-8-8 loss: 0.723265  [  288/  306]
train() client id: f_00004-9-0 loss: 0.718797  [   32/  306]
train() client id: f_00004-9-1 loss: 0.768259  [   64/  306]
train() client id: f_00004-9-2 loss: 0.780128  [   96/  306]
train() client id: f_00004-9-3 loss: 0.797434  [  128/  306]
train() client id: f_00004-9-4 loss: 0.740768  [  160/  306]
train() client id: f_00004-9-5 loss: 0.772024  [  192/  306]
train() client id: f_00004-9-6 loss: 0.694823  [  224/  306]
train() client id: f_00004-9-7 loss: 0.886589  [  256/  306]
train() client id: f_00004-9-8 loss: 0.765491  [  288/  306]
train() client id: f_00004-10-0 loss: 0.751555  [   32/  306]
train() client id: f_00004-10-1 loss: 0.657877  [   64/  306]
train() client id: f_00004-10-2 loss: 0.786579  [   96/  306]
train() client id: f_00004-10-3 loss: 0.860767  [  128/  306]
train() client id: f_00004-10-4 loss: 0.728469  [  160/  306]
train() client id: f_00004-10-5 loss: 0.862357  [  192/  306]
train() client id: f_00004-10-6 loss: 0.724471  [  224/  306]
train() client id: f_00004-10-7 loss: 0.711695  [  256/  306]
train() client id: f_00004-10-8 loss: 0.776632  [  288/  306]
train() client id: f_00005-0-0 loss: 0.592177  [   32/  146]
train() client id: f_00005-0-1 loss: 0.776462  [   64/  146]
train() client id: f_00005-0-2 loss: 0.626979  [   96/  146]
train() client id: f_00005-0-3 loss: 0.884731  [  128/  146]
train() client id: f_00005-1-0 loss: 0.720487  [   32/  146]
train() client id: f_00005-1-1 loss: 0.716131  [   64/  146]
train() client id: f_00005-1-2 loss: 0.799973  [   96/  146]
train() client id: f_00005-1-3 loss: 0.641688  [  128/  146]
train() client id: f_00005-2-0 loss: 0.498063  [   32/  146]
train() client id: f_00005-2-1 loss: 0.780967  [   64/  146]
train() client id: f_00005-2-2 loss: 0.590293  [   96/  146]
train() client id: f_00005-2-3 loss: 0.979262  [  128/  146]
train() client id: f_00005-3-0 loss: 0.786922  [   32/  146]
train() client id: f_00005-3-1 loss: 0.950927  [   64/  146]
train() client id: f_00005-3-2 loss: 0.500186  [   96/  146]
train() client id: f_00005-3-3 loss: 0.577892  [  128/  146]
train() client id: f_00005-4-0 loss: 0.687477  [   32/  146]
train() client id: f_00005-4-1 loss: 0.795201  [   64/  146]
train() client id: f_00005-4-2 loss: 0.710915  [   96/  146]
train() client id: f_00005-4-3 loss: 0.388628  [  128/  146]
train() client id: f_00005-5-0 loss: 0.543017  [   32/  146]
train() client id: f_00005-5-1 loss: 0.763322  [   64/  146]
train() client id: f_00005-5-2 loss: 0.853763  [   96/  146]
train() client id: f_00005-5-3 loss: 0.665752  [  128/  146]
train() client id: f_00005-6-0 loss: 0.710246  [   32/  146]
train() client id: f_00005-6-1 loss: 0.536673  [   64/  146]
train() client id: f_00005-6-2 loss: 0.814531  [   96/  146]
train() client id: f_00005-6-3 loss: 0.679844  [  128/  146]
train() client id: f_00005-7-0 loss: 0.607224  [   32/  146]
train() client id: f_00005-7-1 loss: 0.747659  [   64/  146]
train() client id: f_00005-7-2 loss: 0.809399  [   96/  146]
train() client id: f_00005-7-3 loss: 0.530429  [  128/  146]
train() client id: f_00005-8-0 loss: 0.784147  [   32/  146]
train() client id: f_00005-8-1 loss: 0.398481  [   64/  146]
train() client id: f_00005-8-2 loss: 0.824836  [   96/  146]
train() client id: f_00005-8-3 loss: 0.852059  [  128/  146]
train() client id: f_00005-9-0 loss: 0.746051  [   32/  146]
train() client id: f_00005-9-1 loss: 0.562004  [   64/  146]
train() client id: f_00005-9-2 loss: 0.595006  [   96/  146]
train() client id: f_00005-9-3 loss: 0.793263  [  128/  146]
train() client id: f_00005-10-0 loss: 0.706848  [   32/  146]
train() client id: f_00005-10-1 loss: 0.735147  [   64/  146]
train() client id: f_00005-10-2 loss: 0.751538  [   96/  146]
train() client id: f_00005-10-3 loss: 0.659589  [  128/  146]
train() client id: f_00006-0-0 loss: 0.439501  [   32/   54]
train() client id: f_00006-1-0 loss: 0.538132  [   32/   54]
train() client id: f_00006-2-0 loss: 0.472346  [   32/   54]
train() client id: f_00006-3-0 loss: 0.540091  [   32/   54]
train() client id: f_00006-4-0 loss: 0.486713  [   32/   54]
train() client id: f_00006-5-0 loss: 0.432516  [   32/   54]
train() client id: f_00006-6-0 loss: 0.510822  [   32/   54]
train() client id: f_00006-7-0 loss: 0.560043  [   32/   54]
train() client id: f_00006-8-0 loss: 0.535025  [   32/   54]
train() client id: f_00006-9-0 loss: 0.519291  [   32/   54]
train() client id: f_00006-10-0 loss: 0.440842  [   32/   54]
train() client id: f_00007-0-0 loss: 0.528332  [   32/  179]
train() client id: f_00007-0-1 loss: 0.629404  [   64/  179]
train() client id: f_00007-0-2 loss: 0.636425  [   96/  179]
train() client id: f_00007-0-3 loss: 0.436562  [  128/  179]
train() client id: f_00007-0-4 loss: 0.468270  [  160/  179]
train() client id: f_00007-1-0 loss: 0.448115  [   32/  179]
train() client id: f_00007-1-1 loss: 0.547162  [   64/  179]
train() client id: f_00007-1-2 loss: 0.465884  [   96/  179]
train() client id: f_00007-1-3 loss: 0.662152  [  128/  179]
train() client id: f_00007-1-4 loss: 0.555900  [  160/  179]
train() client id: f_00007-2-0 loss: 0.410387  [   32/  179]
train() client id: f_00007-2-1 loss: 0.495499  [   64/  179]
train() client id: f_00007-2-2 loss: 0.376996  [   96/  179]
train() client id: f_00007-2-3 loss: 0.692744  [  128/  179]
train() client id: f_00007-2-4 loss: 0.688096  [  160/  179]
train() client id: f_00007-3-0 loss: 0.454854  [   32/  179]
train() client id: f_00007-3-1 loss: 0.630295  [   64/  179]
train() client id: f_00007-3-2 loss: 0.434417  [   96/  179]
train() client id: f_00007-3-3 loss: 0.652939  [  128/  179]
train() client id: f_00007-3-4 loss: 0.400208  [  160/  179]
train() client id: f_00007-4-0 loss: 0.568241  [   32/  179]
train() client id: f_00007-4-1 loss: 0.426098  [   64/  179]
train() client id: f_00007-4-2 loss: 0.532791  [   96/  179]
train() client id: f_00007-4-3 loss: 0.339233  [  128/  179]
train() client id: f_00007-4-4 loss: 0.511607  [  160/  179]
train() client id: f_00007-5-0 loss: 0.363959  [   32/  179]
train() client id: f_00007-5-1 loss: 0.511723  [   64/  179]
train() client id: f_00007-5-2 loss: 0.416978  [   96/  179]
train() client id: f_00007-5-3 loss: 0.700205  [  128/  179]
train() client id: f_00007-5-4 loss: 0.430275  [  160/  179]
train() client id: f_00007-6-0 loss: 0.429524  [   32/  179]
train() client id: f_00007-6-1 loss: 0.509265  [   64/  179]
train() client id: f_00007-6-2 loss: 0.404361  [   96/  179]
train() client id: f_00007-6-3 loss: 0.413917  [  128/  179]
train() client id: f_00007-6-4 loss: 0.610263  [  160/  179]
train() client id: f_00007-7-0 loss: 0.564760  [   32/  179]
train() client id: f_00007-7-1 loss: 0.462391  [   64/  179]
train() client id: f_00007-7-2 loss: 0.338976  [   96/  179]
train() client id: f_00007-7-3 loss: 0.585838  [  128/  179]
train() client id: f_00007-7-4 loss: 0.488606  [  160/  179]
train() client id: f_00007-8-0 loss: 0.532649  [   32/  179]
train() client id: f_00007-8-1 loss: 0.530471  [   64/  179]
train() client id: f_00007-8-2 loss: 0.724059  [   96/  179]
train() client id: f_00007-8-3 loss: 0.327589  [  128/  179]
train() client id: f_00007-8-4 loss: 0.418241  [  160/  179]
train() client id: f_00007-9-0 loss: 0.506740  [   32/  179]
train() client id: f_00007-9-1 loss: 0.328040  [   64/  179]
train() client id: f_00007-9-2 loss: 0.473862  [   96/  179]
train() client id: f_00007-9-3 loss: 0.579525  [  128/  179]
train() client id: f_00007-9-4 loss: 0.560516  [  160/  179]
train() client id: f_00007-10-0 loss: 0.425973  [   32/  179]
train() client id: f_00007-10-1 loss: 0.319653  [   64/  179]
train() client id: f_00007-10-2 loss: 0.606090  [   96/  179]
train() client id: f_00007-10-3 loss: 0.527271  [  128/  179]
train() client id: f_00007-10-4 loss: 0.637392  [  160/  179]
train() client id: f_00008-0-0 loss: 0.625047  [   32/  130]
train() client id: f_00008-0-1 loss: 0.740946  [   64/  130]
train() client id: f_00008-0-2 loss: 0.807240  [   96/  130]
train() client id: f_00008-0-3 loss: 0.794864  [  128/  130]
train() client id: f_00008-1-0 loss: 0.802306  [   32/  130]
train() client id: f_00008-1-1 loss: 0.807200  [   64/  130]
train() client id: f_00008-1-2 loss: 0.660452  [   96/  130]
train() client id: f_00008-1-3 loss: 0.683314  [  128/  130]
train() client id: f_00008-2-0 loss: 0.742171  [   32/  130]
train() client id: f_00008-2-1 loss: 0.727191  [   64/  130]
train() client id: f_00008-2-2 loss: 0.735364  [   96/  130]
train() client id: f_00008-2-3 loss: 0.792849  [  128/  130]
train() client id: f_00008-3-0 loss: 0.759636  [   32/  130]
train() client id: f_00008-3-1 loss: 0.740182  [   64/  130]
train() client id: f_00008-3-2 loss: 0.720747  [   96/  130]
train() client id: f_00008-3-3 loss: 0.764408  [  128/  130]
train() client id: f_00008-4-0 loss: 0.781278  [   32/  130]
train() client id: f_00008-4-1 loss: 0.671573  [   64/  130]
train() client id: f_00008-4-2 loss: 0.748467  [   96/  130]
train() client id: f_00008-4-3 loss: 0.785145  [  128/  130]
train() client id: f_00008-5-0 loss: 0.810873  [   32/  130]
train() client id: f_00008-5-1 loss: 0.622846  [   64/  130]
train() client id: f_00008-5-2 loss: 0.774977  [   96/  130]
train() client id: f_00008-5-3 loss: 0.750788  [  128/  130]
train() client id: f_00008-6-0 loss: 0.727843  [   32/  130]
train() client id: f_00008-6-1 loss: 0.720571  [   64/  130]
train() client id: f_00008-6-2 loss: 0.765054  [   96/  130]
train() client id: f_00008-6-3 loss: 0.770858  [  128/  130]
train() client id: f_00008-7-0 loss: 0.812896  [   32/  130]
train() client id: f_00008-7-1 loss: 0.854140  [   64/  130]
train() client id: f_00008-7-2 loss: 0.704487  [   96/  130]
train() client id: f_00008-7-3 loss: 0.614020  [  128/  130]
train() client id: f_00008-8-0 loss: 0.769765  [   32/  130]
train() client id: f_00008-8-1 loss: 0.744317  [   64/  130]
train() client id: f_00008-8-2 loss: 0.618073  [   96/  130]
train() client id: f_00008-8-3 loss: 0.843109  [  128/  130]
train() client id: f_00008-9-0 loss: 0.752712  [   32/  130]
train() client id: f_00008-9-1 loss: 0.760725  [   64/  130]
train() client id: f_00008-9-2 loss: 0.671208  [   96/  130]
train() client id: f_00008-9-3 loss: 0.772102  [  128/  130]
train() client id: f_00008-10-0 loss: 0.659667  [   32/  130]
train() client id: f_00008-10-1 loss: 0.807737  [   64/  130]
train() client id: f_00008-10-2 loss: 0.794556  [   96/  130]
train() client id: f_00008-10-3 loss: 0.681698  [  128/  130]
train() client id: f_00009-0-0 loss: 1.020157  [   32/  118]
train() client id: f_00009-0-1 loss: 1.076377  [   64/  118]
train() client id: f_00009-0-2 loss: 1.136446  [   96/  118]
train() client id: f_00009-1-0 loss: 0.977163  [   32/  118]
train() client id: f_00009-1-1 loss: 1.238200  [   64/  118]
train() client id: f_00009-1-2 loss: 0.989085  [   96/  118]
train() client id: f_00009-2-0 loss: 0.932547  [   32/  118]
train() client id: f_00009-2-1 loss: 0.859808  [   64/  118]
train() client id: f_00009-2-2 loss: 1.089961  [   96/  118]
train() client id: f_00009-3-0 loss: 0.932598  [   32/  118]
train() client id: f_00009-3-1 loss: 0.868502  [   64/  118]
train() client id: f_00009-3-2 loss: 0.816482  [   96/  118]
train() client id: f_00009-4-0 loss: 1.037031  [   32/  118]
train() client id: f_00009-4-1 loss: 0.906920  [   64/  118]
train() client id: f_00009-4-2 loss: 0.816803  [   96/  118]
train() client id: f_00009-5-0 loss: 0.985649  [   32/  118]
train() client id: f_00009-5-1 loss: 0.835448  [   64/  118]
train() client id: f_00009-5-2 loss: 0.950942  [   96/  118]
train() client id: f_00009-6-0 loss: 0.874159  [   32/  118]
train() client id: f_00009-6-1 loss: 0.882276  [   64/  118]
train() client id: f_00009-6-2 loss: 0.892422  [   96/  118]
train() client id: f_00009-7-0 loss: 0.915632  [   32/  118]
train() client id: f_00009-7-1 loss: 0.773289  [   64/  118]
train() client id: f_00009-7-2 loss: 0.816195  [   96/  118]
train() client id: f_00009-8-0 loss: 0.835142  [   32/  118]
train() client id: f_00009-8-1 loss: 0.930215  [   64/  118]
train() client id: f_00009-8-2 loss: 0.918341  [   96/  118]
train() client id: f_00009-9-0 loss: 0.756366  [   32/  118]
train() client id: f_00009-9-1 loss: 0.796903  [   64/  118]
train() client id: f_00009-9-2 loss: 1.037800  [   96/  118]
train() client id: f_00009-10-0 loss: 0.901717  [   32/  118]
train() client id: f_00009-10-1 loss: 0.895360  [   64/  118]
train() client id: f_00009-10-2 loss: 0.786566  [   96/  118]
At round 50 accuracy: 0.649867374005305
At round 50 training accuracy: 0.5922199865861838
At round 50 training loss: 0.8232762368249762
update_location
xs = -3.905658 4.200318 270.009024 18.811294 0.979296 3.956410 -232.443192 -211.324852 254.663977 -197.060879 
ys = 262.587959 245.555839 1.320614 -232.455176 224.350187 207.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 281.011905 265.170346 287.935092 253.750416 245.629732 230.656399 253.054793 233.792364 274.157639 221.018103 
dists_bs = 192.210625 192.589928 477.700283 451.201217 182.768032 181.940004 186.682499 177.925123 457.597695 172.415706 
uav_gains = -114.071502 -112.632205 -114.691274 -111.618241 -110.929157 -109.752365 -111.557993 -109.988230 -113.449067 -109.060618 
bs_gains = -103.512826 -103.536799 -114.583413 -113.889426 -102.900266 -102.845049 -103.157960 -102.573703 -114.060606 -102.191210 
Round 51
-------------------------------
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.27047392  8.72826714  4.21425716  1.53313884 10.06392708  4.84207237
  1.89154359  5.95729153  4.4008638   3.92598233]
obj_prev = 49.827817776908454
eta_min = 2.5652490042097954e-22	eta_max = 0.9391007800776031
af = 10.460568009377656	bf = 1.156843673852587	zeta = 11.506624810315422	eta = 0.9090909090909091
af = 10.460568009377656	bf = 1.156843673852587	zeta = 23.692265208499137	eta = 0.44151827262279386
af = 10.460568009377656	bf = 1.156843673852587	zeta = 17.38415562977596	eta = 0.6017300024316722
af = 10.460568009377656	bf = 1.156843673852587	zeta = 16.245893349082294	eta = 0.6438899840474798
af = 10.460568009377656	bf = 1.156843673852587	zeta = 16.180034455220028	eta = 0.6465108611683364
af = 10.460568009377656	bf = 1.156843673852587	zeta = 16.179790636766835	eta = 0.6465206036478086
eta = 0.6465206036478086
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [0.03599007 0.0756934  0.0354188  0.01228233 0.08740446 0.04170278
 0.01542432 0.05112875 0.03713261 0.03370499]
ene_total = [1.53443253 2.51779645 1.54247681 0.73582445 2.86602817 1.48040303
 0.82941199 1.87320864 1.56863488 1.23157367]
ti_comp = [0.75933363 0.83189525 0.75096662 0.78576566 0.83410293 0.83428835
 0.78630533 0.79893754 0.75947361 0.83641438]
ti_coms = [0.14640211 0.07384048 0.15476912 0.11997008 0.07163281 0.07144739
 0.11943041 0.10679819 0.14626213 0.06932135]
t_total = [27.449786 27.449786 27.449786 27.449786 27.449786 27.449786 27.449786
 27.449786 27.449786 27.449786]
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [5.05315743e-06 3.91666828e-05 4.92425175e-06 1.87558063e-07
 5.99848321e-05 6.51242403e-06 3.70949938e-07 1.30872950e-05
 5.54780444e-06 3.42073551e-06]
ene_total = [0.44384841 0.22497254 0.46920207 0.3635944  0.21891275 0.21673023
 0.36196441 0.32406582 0.44343917 0.21019324]
optimize_network iter = 0 obj = 3.2769230299910435
eta = 0.6465206036478086
freqs = [23698456.65782105 45494550.69374827 23582139.4675787   7815514.32290755
 52394286.76461308 24993025.32151997  9808095.66767819 31997964.51374336
 24446282.03194968 20148497.51725332]
eta_min = 0.6465206036478099	eta_max = 0.6980699554597802
af = 0.0033958499648260683	bf = 1.156843673852587	zeta = 0.003735434961308675	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [1.10424650e-06 8.55894019e-06 1.07607725e-06 4.09863212e-08
 1.31082480e-05 1.42313425e-06 8.10622219e-08 2.85991478e-06
 1.21233975e-06 7.47519793e-07]
ene_total = [1.7253386  0.8711479  1.82393243 1.41373717 0.84566865 0.84210666
 1.4073824  1.2588513  1.7237018  0.81697375]
ti_comp = [0.62724647 0.69980809 0.61887946 0.6536785  0.70201577 0.70220119
 0.65421817 0.66685038 0.62738645 0.70432722]
ti_coms = [0.14640211 0.07384048 0.15476912 0.11997008 0.07163281 0.07144739
 0.11943041 0.10679819 0.14626213 0.06932135]
t_total = [27.449786 27.449786 27.449786 27.449786 27.449786 27.449786 27.449786
 27.449786 27.449786 27.449786]
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [4.14484175e-06 3.09779353e-05 4.05812745e-06 1.51687479e-07
 4.73961769e-05 5.14527051e-06 2.99922360e-07 1.05141713e-05
 4.55022169e-06 2.70003775e-06]
ene_total = [0.51959565 0.26309218 0.54927951 0.42567061 0.25584167 0.25368468
 0.42376107 0.37930318 0.51911337 0.24605454]
optimize_network iter = 1 obj = 3.8353964400634717
eta = 0.6980699554597802
freqs = [23642854.46224663 44569165.40272366 23582139.46757871  7742332.9461439
 51302927.41865983 24471403.78525164  9714909.2256883  31593097.86188618
 24387978.69085684 19718556.51146085]
eta_min = 0.698069955459783	eta_max = 0.6980699554597762
af = 0.0032766452472554383	bf = 1.156843673852587	zeta = 0.003604309771980982	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [1.09907093e-06 8.21429384e-06 1.07607725e-06 4.02223555e-08
 1.25678526e-05 1.36435058e-06 7.95291995e-08 2.78800028e-06
 1.20656389e-06 7.15958094e-07]
ene_total = [1.72533799 0.87110728 1.82393243 1.41373708 0.84560497 0.84209973
 1.40738222 1.25884283 1.72370112 0.81697003]
ti_comp = [0.62724647 0.69980809 0.61887946 0.6536785  0.70201577 0.70220119
 0.65421817 0.66685038 0.62738645 0.70432722]
ti_coms = [0.14640211 0.07384048 0.15476912 0.11997008 0.07163281 0.07144739
 0.11943041 0.10679819 0.14626213 0.06932135]
t_total = [27.449786 27.449786 27.449786 27.449786 27.449786 27.449786 27.449786
 27.449786 27.449786 27.449786]
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [4.14484175e-06 3.09779353e-05 4.05812745e-06 1.51687479e-07
 4.73961769e-05 5.14527051e-06 2.99922360e-07 1.05141713e-05
 4.55022169e-06 2.70003775e-06]
ene_total = [0.51959565 0.26309218 0.54927951 0.42567061 0.25584167 0.25368468
 0.42376107 0.37930318 0.51911337 0.24605454]
optimize_network iter = 2 obj = 3.8353964400634206
eta = 0.6980699554597762
freqs = [23642854.46224662 44569165.40272372 23582139.46757869  7742332.9461439
 51302927.41865991 24471403.78525168  9714909.2256883  31593097.8618862
 24387978.69085682 19718556.51146088]
Done!
ene_coms = [0.01464021 0.00738405 0.01547691 0.01199701 0.00716328 0.00714474
 0.01194304 0.01067982 0.01462621 0.00693214]
ene_comp = [3.87376305e-06 2.89519331e-05 3.79272000e-06 1.41766896e-07
 4.42963978e-05 4.80876231e-06 2.80307000e-07 9.82652913e-06
 4.25263056e-06 2.52345135e-06]
ene_total = [0.01464408 0.007413   0.0154807  0.01199715 0.00720758 0.00714955
 0.01194332 0.01068965 0.01463047 0.00693466]
At round 51 energy consumption: 0.10809015491855846
At round 51 eta: 0.6980699554597762
At round 51 a_n: 10.712764660819193
At round 51 local rounds: 11.76975943561131
At round 51 global rounds: 35.48094949322612
gradient difference: 0.45575016736984253
train() client id: f_00000-0-0 loss: 1.387230  [   32/  126]
train() client id: f_00000-0-1 loss: 1.033302  [   64/  126]
train() client id: f_00000-0-2 loss: 1.236456  [   96/  126]
train() client id: f_00000-1-0 loss: 1.210066  [   32/  126]
train() client id: f_00000-1-1 loss: 1.161962  [   64/  126]
train() client id: f_00000-1-2 loss: 0.956856  [   96/  126]
train() client id: f_00000-2-0 loss: 0.940891  [   32/  126]
train() client id: f_00000-2-1 loss: 0.784122  [   64/  126]
train() client id: f_00000-2-2 loss: 1.099013  [   96/  126]
train() client id: f_00000-3-0 loss: 0.978983  [   32/  126]
train() client id: f_00000-3-1 loss: 0.912199  [   64/  126]
train() client id: f_00000-3-2 loss: 0.956483  [   96/  126]
train() client id: f_00000-4-0 loss: 0.822874  [   32/  126]
train() client id: f_00000-4-1 loss: 1.000627  [   64/  126]
train() client id: f_00000-4-2 loss: 0.820127  [   96/  126]
train() client id: f_00000-5-0 loss: 0.887869  [   32/  126]
train() client id: f_00000-5-1 loss: 0.859694  [   64/  126]
train() client id: f_00000-5-2 loss: 0.861747  [   96/  126]
train() client id: f_00000-6-0 loss: 0.784645  [   32/  126]
train() client id: f_00000-6-1 loss: 0.986965  [   64/  126]
train() client id: f_00000-6-2 loss: 0.880542  [   96/  126]
train() client id: f_00000-7-0 loss: 0.894612  [   32/  126]
train() client id: f_00000-7-1 loss: 0.777855  [   64/  126]
train() client id: f_00000-7-2 loss: 0.838076  [   96/  126]
train() client id: f_00000-8-0 loss: 0.739336  [   32/  126]
train() client id: f_00000-8-1 loss: 0.747652  [   64/  126]
train() client id: f_00000-8-2 loss: 0.834802  [   96/  126]
train() client id: f_00000-9-0 loss: 0.973352  [   32/  126]
train() client id: f_00000-9-1 loss: 0.682582  [   64/  126]
train() client id: f_00000-9-2 loss: 0.757058  [   96/  126]
train() client id: f_00000-10-0 loss: 0.809345  [   32/  126]
train() client id: f_00000-10-1 loss: 0.856690  [   64/  126]
train() client id: f_00000-10-2 loss: 0.669048  [   96/  126]
train() client id: f_00001-0-0 loss: 0.336557  [   32/  265]
train() client id: f_00001-0-1 loss: 0.288065  [   64/  265]
train() client id: f_00001-0-2 loss: 0.449173  [   96/  265]
train() client id: f_00001-0-3 loss: 0.280779  [  128/  265]
train() client id: f_00001-0-4 loss: 0.276704  [  160/  265]
train() client id: f_00001-0-5 loss: 0.414283  [  192/  265]
train() client id: f_00001-0-6 loss: 0.276954  [  224/  265]
train() client id: f_00001-0-7 loss: 0.278029  [  256/  265]
train() client id: f_00001-1-0 loss: 0.364260  [   32/  265]
train() client id: f_00001-1-1 loss: 0.282037  [   64/  265]
train() client id: f_00001-1-2 loss: 0.240893  [   96/  265]
train() client id: f_00001-1-3 loss: 0.363388  [  128/  265]
train() client id: f_00001-1-4 loss: 0.383200  [  160/  265]
train() client id: f_00001-1-5 loss: 0.346461  [  192/  265]
train() client id: f_00001-1-6 loss: 0.297746  [  224/  265]
train() client id: f_00001-1-7 loss: 0.255237  [  256/  265]
train() client id: f_00001-2-0 loss: 0.220125  [   32/  265]
train() client id: f_00001-2-1 loss: 0.236522  [   64/  265]
train() client id: f_00001-2-2 loss: 0.276206  [   96/  265]
train() client id: f_00001-2-3 loss: 0.333875  [  128/  265]
train() client id: f_00001-2-4 loss: 0.332980  [  160/  265]
train() client id: f_00001-2-5 loss: 0.306297  [  192/  265]
train() client id: f_00001-2-6 loss: 0.349688  [  224/  265]
train() client id: f_00001-2-7 loss: 0.276282  [  256/  265]
train() client id: f_00001-3-0 loss: 0.249837  [   32/  265]
train() client id: f_00001-3-1 loss: 0.384395  [   64/  265]
train() client id: f_00001-3-2 loss: 0.184816  [   96/  265]
train() client id: f_00001-3-3 loss: 0.262066  [  128/  265]
train() client id: f_00001-3-4 loss: 0.301462  [  160/  265]
train() client id: f_00001-3-5 loss: 0.337905  [  192/  265]
train() client id: f_00001-3-6 loss: 0.352835  [  224/  265]
train() client id: f_00001-3-7 loss: 0.253697  [  256/  265]
train() client id: f_00001-4-0 loss: 0.189447  [   32/  265]
train() client id: f_00001-4-1 loss: 0.247983  [   64/  265]
train() client id: f_00001-4-2 loss: 0.324239  [   96/  265]
train() client id: f_00001-4-3 loss: 0.391142  [  128/  265]
train() client id: f_00001-4-4 loss: 0.335237  [  160/  265]
train() client id: f_00001-4-5 loss: 0.189793  [  192/  265]
train() client id: f_00001-4-6 loss: 0.259368  [  224/  265]
train() client id: f_00001-4-7 loss: 0.349854  [  256/  265]
train() client id: f_00001-5-0 loss: 0.381287  [   32/  265]
train() client id: f_00001-5-1 loss: 0.354930  [   64/  265]
train() client id: f_00001-5-2 loss: 0.255358  [   96/  265]
train() client id: f_00001-5-3 loss: 0.199040  [  128/  265]
train() client id: f_00001-5-4 loss: 0.423178  [  160/  265]
train() client id: f_00001-5-5 loss: 0.208350  [  192/  265]
train() client id: f_00001-5-6 loss: 0.253531  [  224/  265]
train() client id: f_00001-5-7 loss: 0.242664  [  256/  265]
train() client id: f_00001-6-0 loss: 0.217427  [   32/  265]
train() client id: f_00001-6-1 loss: 0.278476  [   64/  265]
train() client id: f_00001-6-2 loss: 0.371139  [   96/  265]
train() client id: f_00001-6-3 loss: 0.250892  [  128/  265]
train() client id: f_00001-6-4 loss: 0.290006  [  160/  265]
train() client id: f_00001-6-5 loss: 0.204253  [  192/  265]
train() client id: f_00001-6-6 loss: 0.218207  [  224/  265]
train() client id: f_00001-6-7 loss: 0.251849  [  256/  265]
train() client id: f_00001-7-0 loss: 0.320854  [   32/  265]
train() client id: f_00001-7-1 loss: 0.353149  [   64/  265]
train() client id: f_00001-7-2 loss: 0.255159  [   96/  265]
train() client id: f_00001-7-3 loss: 0.233646  [  128/  265]
train() client id: f_00001-7-4 loss: 0.194508  [  160/  265]
train() client id: f_00001-7-5 loss: 0.338263  [  192/  265]
train() client id: f_00001-7-6 loss: 0.277882  [  224/  265]
train() client id: f_00001-7-7 loss: 0.236444  [  256/  265]
train() client id: f_00001-8-0 loss: 0.320363  [   32/  265]
train() client id: f_00001-8-1 loss: 0.342899  [   64/  265]
train() client id: f_00001-8-2 loss: 0.346582  [   96/  265]
train() client id: f_00001-8-3 loss: 0.284640  [  128/  265]
train() client id: f_00001-8-4 loss: 0.249923  [  160/  265]
train() client id: f_00001-8-5 loss: 0.199737  [  192/  265]
train() client id: f_00001-8-6 loss: 0.221292  [  224/  265]
train() client id: f_00001-8-7 loss: 0.280732  [  256/  265]
train() client id: f_00001-9-0 loss: 0.265821  [   32/  265]
train() client id: f_00001-9-1 loss: 0.227938  [   64/  265]
train() client id: f_00001-9-2 loss: 0.274445  [   96/  265]
train() client id: f_00001-9-3 loss: 0.231341  [  128/  265]
train() client id: f_00001-9-4 loss: 0.222122  [  160/  265]
train() client id: f_00001-9-5 loss: 0.248441  [  192/  265]
train() client id: f_00001-9-6 loss: 0.277452  [  224/  265]
train() client id: f_00001-9-7 loss: 0.309431  [  256/  265]
train() client id: f_00001-10-0 loss: 0.394594  [   32/  265]
train() client id: f_00001-10-1 loss: 0.371624  [   64/  265]
train() client id: f_00001-10-2 loss: 0.257850  [   96/  265]
train() client id: f_00001-10-3 loss: 0.179219  [  128/  265]
train() client id: f_00001-10-4 loss: 0.206820  [  160/  265]
train() client id: f_00001-10-5 loss: 0.390442  [  192/  265]
train() client id: f_00001-10-6 loss: 0.203759  [  224/  265]
train() client id: f_00001-10-7 loss: 0.210873  [  256/  265]
train() client id: f_00002-0-0 loss: 0.898673  [   32/  124]
train() client id: f_00002-0-1 loss: 1.091724  [   64/  124]
train() client id: f_00002-0-2 loss: 1.082363  [   96/  124]
train() client id: f_00002-1-0 loss: 0.986865  [   32/  124]
train() client id: f_00002-1-1 loss: 1.002191  [   64/  124]
train() client id: f_00002-1-2 loss: 1.178657  [   96/  124]
train() client id: f_00002-2-0 loss: 1.049689  [   32/  124]
train() client id: f_00002-2-1 loss: 0.906876  [   64/  124]
train() client id: f_00002-2-2 loss: 0.956355  [   96/  124]
train() client id: f_00002-3-0 loss: 1.006644  [   32/  124]
train() client id: f_00002-3-1 loss: 1.062537  [   64/  124]
train() client id: f_00002-3-2 loss: 0.950888  [   96/  124]
train() client id: f_00002-4-0 loss: 0.927727  [   32/  124]
train() client id: f_00002-4-1 loss: 1.224183  [   64/  124]
train() client id: f_00002-4-2 loss: 0.886944  [   96/  124]
train() client id: f_00002-5-0 loss: 1.007572  [   32/  124]
train() client id: f_00002-5-1 loss: 0.886122  [   64/  124]
train() client id: f_00002-5-2 loss: 1.058048  [   96/  124]
train() client id: f_00002-6-0 loss: 0.994945  [   32/  124]
train() client id: f_00002-6-1 loss: 1.018873  [   64/  124]
train() client id: f_00002-6-2 loss: 0.827614  [   96/  124]
train() client id: f_00002-7-0 loss: 1.099144  [   32/  124]
train() client id: f_00002-7-1 loss: 0.920380  [   64/  124]
train() client id: f_00002-7-2 loss: 0.824829  [   96/  124]
train() client id: f_00002-8-0 loss: 1.063126  [   32/  124]
train() client id: f_00002-8-1 loss: 0.925812  [   64/  124]
train() client id: f_00002-8-2 loss: 1.034521  [   96/  124]
train() client id: f_00002-9-0 loss: 0.956530  [   32/  124]
train() client id: f_00002-9-1 loss: 0.855101  [   64/  124]
train() client id: f_00002-9-2 loss: 0.884141  [   96/  124]
train() client id: f_00002-10-0 loss: 1.052318  [   32/  124]
train() client id: f_00002-10-1 loss: 0.881547  [   64/  124]
train() client id: f_00002-10-2 loss: 1.012585  [   96/  124]
train() client id: f_00003-0-0 loss: 0.579966  [   32/   43]
train() client id: f_00003-1-0 loss: 0.895304  [   32/   43]
train() client id: f_00003-2-0 loss: 0.772922  [   32/   43]
train() client id: f_00003-3-0 loss: 0.705687  [   32/   43]
train() client id: f_00003-4-0 loss: 0.791754  [   32/   43]
train() client id: f_00003-5-0 loss: 0.880770  [   32/   43]
train() client id: f_00003-6-0 loss: 0.881138  [   32/   43]
train() client id: f_00003-7-0 loss: 0.867161  [   32/   43]
train() client id: f_00003-8-0 loss: 0.559228  [   32/   43]
train() client id: f_00003-9-0 loss: 0.713693  [   32/   43]
train() client id: f_00003-10-0 loss: 0.900107  [   32/   43]
train() client id: f_00004-0-0 loss: 1.031602  [   32/  306]
train() client id: f_00004-0-1 loss: 0.705491  [   64/  306]
train() client id: f_00004-0-2 loss: 0.836779  [   96/  306]
train() client id: f_00004-0-3 loss: 0.740991  [  128/  306]
train() client id: f_00004-0-4 loss: 0.857081  [  160/  306]
train() client id: f_00004-0-5 loss: 0.854614  [  192/  306]
train() client id: f_00004-0-6 loss: 0.844394  [  224/  306]
train() client id: f_00004-0-7 loss: 0.964864  [  256/  306]
train() client id: f_00004-0-8 loss: 0.879435  [  288/  306]
train() client id: f_00004-1-0 loss: 0.817781  [   32/  306]
train() client id: f_00004-1-1 loss: 0.873383  [   64/  306]
train() client id: f_00004-1-2 loss: 0.960264  [   96/  306]
train() client id: f_00004-1-3 loss: 0.800530  [  128/  306]
train() client id: f_00004-1-4 loss: 0.908690  [  160/  306]
train() client id: f_00004-1-5 loss: 0.843312  [  192/  306]
train() client id: f_00004-1-6 loss: 0.728382  [  224/  306]
train() client id: f_00004-1-7 loss: 0.829649  [  256/  306]
train() client id: f_00004-1-8 loss: 0.906383  [  288/  306]
train() client id: f_00004-2-0 loss: 0.939353  [   32/  306]
train() client id: f_00004-2-1 loss: 0.877774  [   64/  306]
train() client id: f_00004-2-2 loss: 0.961424  [   96/  306]
train() client id: f_00004-2-3 loss: 0.958290  [  128/  306]
train() client id: f_00004-2-4 loss: 0.775098  [  160/  306]
train() client id: f_00004-2-5 loss: 0.710837  [  192/  306]
train() client id: f_00004-2-6 loss: 0.903225  [  224/  306]
train() client id: f_00004-2-7 loss: 0.929609  [  256/  306]
train() client id: f_00004-2-8 loss: 0.814841  [  288/  306]
train() client id: f_00004-3-0 loss: 0.850086  [   32/  306]
train() client id: f_00004-3-1 loss: 0.913329  [   64/  306]
train() client id: f_00004-3-2 loss: 0.933676  [   96/  306]
train() client id: f_00004-3-3 loss: 0.926154  [  128/  306]
train() client id: f_00004-3-4 loss: 0.820144  [  160/  306]
train() client id: f_00004-3-5 loss: 0.768251  [  192/  306]
train() client id: f_00004-3-6 loss: 0.788462  [  224/  306]
train() client id: f_00004-3-7 loss: 0.799593  [  256/  306]
train() client id: f_00004-3-8 loss: 0.879018  [  288/  306]
train() client id: f_00004-4-0 loss: 0.780157  [   32/  306]
train() client id: f_00004-4-1 loss: 0.758869  [   64/  306]
train() client id: f_00004-4-2 loss: 0.833847  [   96/  306]
train() client id: f_00004-4-3 loss: 0.804594  [  128/  306]
train() client id: f_00004-4-4 loss: 0.923101  [  160/  306]
train() client id: f_00004-4-5 loss: 0.859949  [  192/  306]
train() client id: f_00004-4-6 loss: 0.937992  [  224/  306]
train() client id: f_00004-4-7 loss: 0.864300  [  256/  306]
train() client id: f_00004-4-8 loss: 0.975559  [  288/  306]
train() client id: f_00004-5-0 loss: 0.868495  [   32/  306]
train() client id: f_00004-5-1 loss: 0.827969  [   64/  306]
train() client id: f_00004-5-2 loss: 0.790488  [   96/  306]
train() client id: f_00004-5-3 loss: 0.877706  [  128/  306]
train() client id: f_00004-5-4 loss: 0.860127  [  160/  306]
train() client id: f_00004-5-5 loss: 0.993825  [  192/  306]
train() client id: f_00004-5-6 loss: 0.856846  [  224/  306]
train() client id: f_00004-5-7 loss: 0.832066  [  256/  306]
train() client id: f_00004-5-8 loss: 0.792660  [  288/  306]
train() client id: f_00004-6-0 loss: 0.943443  [   32/  306]
train() client id: f_00004-6-1 loss: 0.826693  [   64/  306]
train() client id: f_00004-6-2 loss: 0.880033  [   96/  306]
train() client id: f_00004-6-3 loss: 0.826789  [  128/  306]
train() client id: f_00004-6-4 loss: 0.830597  [  160/  306]
train() client id: f_00004-6-5 loss: 0.729930  [  192/  306]
train() client id: f_00004-6-6 loss: 0.758367  [  224/  306]
train() client id: f_00004-6-7 loss: 0.921818  [  256/  306]
train() client id: f_00004-6-8 loss: 0.945447  [  288/  306]
train() client id: f_00004-7-0 loss: 0.933805  [   32/  306]
train() client id: f_00004-7-1 loss: 0.824946  [   64/  306]
train() client id: f_00004-7-2 loss: 0.692219  [   96/  306]
train() client id: f_00004-7-3 loss: 0.783565  [  128/  306]
train() client id: f_00004-7-4 loss: 1.042153  [  160/  306]
train() client id: f_00004-7-5 loss: 1.039308  [  192/  306]
train() client id: f_00004-7-6 loss: 0.751336  [  224/  306]
train() client id: f_00004-7-7 loss: 0.873678  [  256/  306]
train() client id: f_00004-7-8 loss: 0.767950  [  288/  306]
train() client id: f_00004-8-0 loss: 0.875445  [   32/  306]
train() client id: f_00004-8-1 loss: 0.796279  [   64/  306]
train() client id: f_00004-8-2 loss: 0.808556  [   96/  306]
train() client id: f_00004-8-3 loss: 0.773799  [  128/  306]
train() client id: f_00004-8-4 loss: 0.922967  [  160/  306]
train() client id: f_00004-8-5 loss: 0.979739  [  192/  306]
train() client id: f_00004-8-6 loss: 0.878618  [  224/  306]
train() client id: f_00004-8-7 loss: 0.784190  [  256/  306]
train() client id: f_00004-8-8 loss: 0.891848  [  288/  306]
train() client id: f_00004-9-0 loss: 0.829647  [   32/  306]
train() client id: f_00004-9-1 loss: 0.848963  [   64/  306]
train() client id: f_00004-9-2 loss: 0.980308  [   96/  306]
train() client id: f_00004-9-3 loss: 0.855712  [  128/  306]
train() client id: f_00004-9-4 loss: 0.798813  [  160/  306]
train() client id: f_00004-9-5 loss: 0.839846  [  192/  306]
train() client id: f_00004-9-6 loss: 0.903929  [  224/  306]
train() client id: f_00004-9-7 loss: 0.813398  [  256/  306]
train() client id: f_00004-9-8 loss: 0.743778  [  288/  306]
train() client id: f_00004-10-0 loss: 0.760266  [   32/  306]
train() client id: f_00004-10-1 loss: 0.912912  [   64/  306]
train() client id: f_00004-10-2 loss: 0.838429  [   96/  306]
train() client id: f_00004-10-3 loss: 0.825641  [  128/  306]
train() client id: f_00004-10-4 loss: 0.860086  [  160/  306]
train() client id: f_00004-10-5 loss: 0.992396  [  192/  306]
train() client id: f_00004-10-6 loss: 0.802984  [  224/  306]
train() client id: f_00004-10-7 loss: 0.892600  [  256/  306]
train() client id: f_00004-10-8 loss: 0.787919  [  288/  306]
train() client id: f_00005-0-0 loss: 0.622648  [   32/  146]
train() client id: f_00005-0-1 loss: 0.550141  [   64/  146]
train() client id: f_00005-0-2 loss: 0.437411  [   96/  146]
train() client id: f_00005-0-3 loss: 0.870053  [  128/  146]
train() client id: f_00005-1-0 loss: 0.680112  [   32/  146]
train() client id: f_00005-1-1 loss: 0.621181  [   64/  146]
train() client id: f_00005-1-2 loss: 0.401895  [   96/  146]
train() client id: f_00005-1-3 loss: 0.690538  [  128/  146]
train() client id: f_00005-2-0 loss: 0.622091  [   32/  146]
train() client id: f_00005-2-1 loss: 0.539912  [   64/  146]
train() client id: f_00005-2-2 loss: 0.766202  [   96/  146]
train() client id: f_00005-2-3 loss: 0.491998  [  128/  146]
train() client id: f_00005-3-0 loss: 0.428400  [   32/  146]
train() client id: f_00005-3-1 loss: 0.579924  [   64/  146]
train() client id: f_00005-3-2 loss: 0.569719  [   96/  146]
train() client id: f_00005-3-3 loss: 0.811563  [  128/  146]
train() client id: f_00005-4-0 loss: 0.641374  [   32/  146]
train() client id: f_00005-4-1 loss: 0.441034  [   64/  146]
train() client id: f_00005-4-2 loss: 0.674166  [   96/  146]
train() client id: f_00005-4-3 loss: 0.671884  [  128/  146]
train() client id: f_00005-5-0 loss: 0.446710  [   32/  146]
train() client id: f_00005-5-1 loss: 0.937492  [   64/  146]
train() client id: f_00005-5-2 loss: 0.768810  [   96/  146]
train() client id: f_00005-5-3 loss: 0.447444  [  128/  146]
train() client id: f_00005-6-0 loss: 0.487197  [   32/  146]
train() client id: f_00005-6-1 loss: 0.780369  [   64/  146]
train() client id: f_00005-6-2 loss: 0.421902  [   96/  146]
train() client id: f_00005-6-3 loss: 0.714172  [  128/  146]
train() client id: f_00005-7-0 loss: 0.840642  [   32/  146]
train() client id: f_00005-7-1 loss: 0.569758  [   64/  146]
train() client id: f_00005-7-2 loss: 0.560585  [   96/  146]
train() client id: f_00005-7-3 loss: 0.470981  [  128/  146]
train() client id: f_00005-8-0 loss: 0.505868  [   32/  146]
train() client id: f_00005-8-1 loss: 0.680387  [   64/  146]
train() client id: f_00005-8-2 loss: 0.518913  [   96/  146]
train() client id: f_00005-8-3 loss: 0.657481  [  128/  146]
train() client id: f_00005-9-0 loss: 0.532369  [   32/  146]
train() client id: f_00005-9-1 loss: 0.727777  [   64/  146]
train() client id: f_00005-9-2 loss: 0.762995  [   96/  146]
train() client id: f_00005-9-3 loss: 0.402027  [  128/  146]
train() client id: f_00005-10-0 loss: 0.779917  [   32/  146]
train() client id: f_00005-10-1 loss: 0.389764  [   64/  146]
train() client id: f_00005-10-2 loss: 0.674225  [   96/  146]
train() client id: f_00005-10-3 loss: 0.439143  [  128/  146]
train() client id: f_00006-0-0 loss: 0.507016  [   32/   54]
train() client id: f_00006-1-0 loss: 0.541315  [   32/   54]
train() client id: f_00006-2-0 loss: 0.553188  [   32/   54]
train() client id: f_00006-3-0 loss: 0.587756  [   32/   54]
train() client id: f_00006-4-0 loss: 0.595895  [   32/   54]
train() client id: f_00006-5-0 loss: 0.549137  [   32/   54]
train() client id: f_00006-6-0 loss: 0.601056  [   32/   54]
train() client id: f_00006-7-0 loss: 0.598612  [   32/   54]
train() client id: f_00006-8-0 loss: 0.553889  [   32/   54]
train() client id: f_00006-9-0 loss: 0.609516  [   32/   54]
train() client id: f_00006-10-0 loss: 0.500895  [   32/   54]
train() client id: f_00007-0-0 loss: 0.392402  [   32/  179]
train() client id: f_00007-0-1 loss: 0.482311  [   64/  179]
train() client id: f_00007-0-2 loss: 0.423371  [   96/  179]
train() client id: f_00007-0-3 loss: 0.652488  [  128/  179]
train() client id: f_00007-0-4 loss: 0.334084  [  160/  179]
train() client id: f_00007-1-0 loss: 0.540688  [   32/  179]
train() client id: f_00007-1-1 loss: 0.640840  [   64/  179]
train() client id: f_00007-1-2 loss: 0.313264  [   96/  179]
train() client id: f_00007-1-3 loss: 0.271925  [  128/  179]
train() client id: f_00007-1-4 loss: 0.388474  [  160/  179]
train() client id: f_00007-2-0 loss: 0.430723  [   32/  179]
train() client id: f_00007-2-1 loss: 0.469021  [   64/  179]
train() client id: f_00007-2-2 loss: 0.302813  [   96/  179]
train() client id: f_00007-2-3 loss: 0.317559  [  128/  179]
train() client id: f_00007-2-4 loss: 0.394859  [  160/  179]
train() client id: f_00007-3-0 loss: 0.444303  [   32/  179]
train() client id: f_00007-3-1 loss: 0.359542  [   64/  179]
train() client id: f_00007-3-2 loss: 0.459180  [   96/  179]
train() client id: f_00007-3-3 loss: 0.486394  [  128/  179]
train() client id: f_00007-3-4 loss: 0.296263  [  160/  179]
train() client id: f_00007-4-0 loss: 0.240726  [   32/  179]
train() client id: f_00007-4-1 loss: 0.426912  [   64/  179]
train() client id: f_00007-4-2 loss: 0.441292  [   96/  179]
train() client id: f_00007-4-3 loss: 0.245168  [  128/  179]
train() client id: f_00007-4-4 loss: 0.371356  [  160/  179]
train() client id: f_00007-5-0 loss: 0.626334  [   32/  179]
train() client id: f_00007-5-1 loss: 0.293751  [   64/  179]
train() client id: f_00007-5-2 loss: 0.483111  [   96/  179]
train() client id: f_00007-5-3 loss: 0.397739  [  128/  179]
train() client id: f_00007-5-4 loss: 0.275914  [  160/  179]
train() client id: f_00007-6-0 loss: 0.406221  [   32/  179]
train() client id: f_00007-6-1 loss: 0.391574  [   64/  179]
train() client id: f_00007-6-2 loss: 0.344754  [   96/  179]
train() client id: f_00007-6-3 loss: 0.442998  [  128/  179]
train() client id: f_00007-6-4 loss: 0.431989  [  160/  179]
train() client id: f_00007-7-0 loss: 0.504922  [   32/  179]
train() client id: f_00007-7-1 loss: 0.236656  [   64/  179]
train() client id: f_00007-7-2 loss: 0.233088  [   96/  179]
train() client id: f_00007-7-3 loss: 0.528500  [  128/  179]
train() client id: f_00007-7-4 loss: 0.378733  [  160/  179]
train() client id: f_00007-8-0 loss: 0.413869  [   32/  179]
train() client id: f_00007-8-1 loss: 0.252296  [   64/  179]
train() client id: f_00007-8-2 loss: 0.476188  [   96/  179]
train() client id: f_00007-8-3 loss: 0.306373  [  128/  179]
train() client id: f_00007-8-4 loss: 0.280915  [  160/  179]
train() client id: f_00007-9-0 loss: 0.319837  [   32/  179]
train() client id: f_00007-9-1 loss: 0.579521  [   64/  179]
train() client id: f_00007-9-2 loss: 0.340000  [   96/  179]
train() client id: f_00007-9-3 loss: 0.328030  [  128/  179]
train() client id: f_00007-9-4 loss: 0.336088  [  160/  179]
train() client id: f_00007-10-0 loss: 0.622338  [   32/  179]
train() client id: f_00007-10-1 loss: 0.398156  [   64/  179]
train() client id: f_00007-10-2 loss: 0.423142  [   96/  179]
train() client id: f_00007-10-3 loss: 0.243211  [  128/  179]
train() client id: f_00007-10-4 loss: 0.330610  [  160/  179]
train() client id: f_00008-0-0 loss: 0.743293  [   32/  130]
train() client id: f_00008-0-1 loss: 0.717385  [   64/  130]
train() client id: f_00008-0-2 loss: 0.864042  [   96/  130]
train() client id: f_00008-0-3 loss: 0.680192  [  128/  130]
train() client id: f_00008-1-0 loss: 0.753240  [   32/  130]
train() client id: f_00008-1-1 loss: 0.720736  [   64/  130]
train() client id: f_00008-1-2 loss: 0.789857  [   96/  130]
train() client id: f_00008-1-3 loss: 0.742019  [  128/  130]
train() client id: f_00008-2-0 loss: 0.828937  [   32/  130]
train() client id: f_00008-2-1 loss: 0.808977  [   64/  130]
train() client id: f_00008-2-2 loss: 0.687224  [   96/  130]
train() client id: f_00008-2-3 loss: 0.696595  [  128/  130]
train() client id: f_00008-3-0 loss: 0.737605  [   32/  130]
train() client id: f_00008-3-1 loss: 0.721801  [   64/  130]
train() client id: f_00008-3-2 loss: 0.800212  [   96/  130]
train() client id: f_00008-3-3 loss: 0.743501  [  128/  130]
train() client id: f_00008-4-0 loss: 0.738068  [   32/  130]
train() client id: f_00008-4-1 loss: 0.876881  [   64/  130]
train() client id: f_00008-4-2 loss: 0.763369  [   96/  130]
train() client id: f_00008-4-3 loss: 0.623312  [  128/  130]
train() client id: f_00008-5-0 loss: 0.917236  [   32/  130]
train() client id: f_00008-5-1 loss: 0.727304  [   64/  130]
train() client id: f_00008-5-2 loss: 0.714861  [   96/  130]
train() client id: f_00008-5-3 loss: 0.656990  [  128/  130]
train() client id: f_00008-6-0 loss: 0.767191  [   32/  130]
train() client id: f_00008-6-1 loss: 0.737638  [   64/  130]
train() client id: f_00008-6-2 loss: 0.717432  [   96/  130]
train() client id: f_00008-6-3 loss: 0.795602  [  128/  130]
train() client id: f_00008-7-0 loss: 0.874883  [   32/  130]
train() client id: f_00008-7-1 loss: 0.667381  [   64/  130]
train() client id: f_00008-7-2 loss: 0.768248  [   96/  130]
train() client id: f_00008-7-3 loss: 0.716396  [  128/  130]
train() client id: f_00008-8-0 loss: 0.795694  [   32/  130]
train() client id: f_00008-8-1 loss: 0.710253  [   64/  130]
train() client id: f_00008-8-2 loss: 0.771946  [   96/  130]
train() client id: f_00008-8-3 loss: 0.725862  [  128/  130]
train() client id: f_00008-9-0 loss: 0.740979  [   32/  130]
train() client id: f_00008-9-1 loss: 0.598724  [   64/  130]
train() client id: f_00008-9-2 loss: 0.813427  [   96/  130]
train() client id: f_00008-9-3 loss: 0.805308  [  128/  130]
train() client id: f_00008-10-0 loss: 0.700753  [   32/  130]
train() client id: f_00008-10-1 loss: 0.717255  [   64/  130]
train() client id: f_00008-10-2 loss: 0.803547  [   96/  130]
train() client id: f_00008-10-3 loss: 0.772917  [  128/  130]
train() client id: f_00009-0-0 loss: 1.130639  [   32/  118]
train() client id: f_00009-0-1 loss: 1.184277  [   64/  118]
train() client id: f_00009-0-2 loss: 1.156936  [   96/  118]
train() client id: f_00009-1-0 loss: 1.209652  [   32/  118]
train() client id: f_00009-1-1 loss: 1.119821  [   64/  118]
train() client id: f_00009-1-2 loss: 1.067904  [   96/  118]
train() client id: f_00009-2-0 loss: 1.206704  [   32/  118]
train() client id: f_00009-2-1 loss: 0.978024  [   64/  118]
train() client id: f_00009-2-2 loss: 1.125432  [   96/  118]
train() client id: f_00009-3-0 loss: 1.075337  [   32/  118]
train() client id: f_00009-3-1 loss: 1.000951  [   64/  118]
train() client id: f_00009-3-2 loss: 1.082966  [   96/  118]
train() client id: f_00009-4-0 loss: 0.940398  [   32/  118]
train() client id: f_00009-4-1 loss: 1.024141  [   64/  118]
train() client id: f_00009-4-2 loss: 1.080332  [   96/  118]
train() client id: f_00009-5-0 loss: 1.019351  [   32/  118]
train() client id: f_00009-5-1 loss: 0.962108  [   64/  118]
train() client id: f_00009-5-2 loss: 0.845525  [   96/  118]
train() client id: f_00009-6-0 loss: 0.968518  [   32/  118]
train() client id: f_00009-6-1 loss: 0.851110  [   64/  118]
train() client id: f_00009-6-2 loss: 1.054473  [   96/  118]
train() client id: f_00009-7-0 loss: 1.059825  [   32/  118]
train() client id: f_00009-7-1 loss: 0.941913  [   64/  118]
train() client id: f_00009-7-2 loss: 0.869016  [   96/  118]
train() client id: f_00009-8-0 loss: 1.014110  [   32/  118]
train() client id: f_00009-8-1 loss: 0.843873  [   64/  118]
train() client id: f_00009-8-2 loss: 0.891914  [   96/  118]
train() client id: f_00009-9-0 loss: 0.891066  [   32/  118]
train() client id: f_00009-9-1 loss: 0.921364  [   64/  118]
train() client id: f_00009-9-2 loss: 0.977365  [   96/  118]
train() client id: f_00009-10-0 loss: 0.913178  [   32/  118]
train() client id: f_00009-10-1 loss: 0.741170  [   64/  118]
train() client id: f_00009-10-2 loss: 1.079710  [   96/  118]
At round 51 accuracy: 0.649867374005305
At round 51 training accuracy: 0.5902079141515761
At round 51 training loss: 0.8317766087764751
update_location
xs = -3.905658 4.200318 275.009024 18.811294 0.979296 3.956410 -237.443192 -216.324852 259.663977 -202.060879 
ys = 267.587959 250.555839 1.320614 -237.455176 229.350187 212.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 285.689640 269.807100 292.628958 258.338587 250.204851 235.171248 257.655118 238.321459 278.808269 225.487495 
dists_bs = 194.539980 194.477347 482.361535 455.721505 184.181040 182.907918 188.281139 179.012843 462.295674 173.126498 
uav_gains = -114.491677 -113.052833 -115.102889 -112.020532 -111.313436 -110.093711 -111.960103 -110.338798 -113.872000 -109.375433 
bs_gains = -103.659307 -103.655392 -114.701494 -114.010646 -102.993917 -102.909569 -103.261650 -102.647816 -114.184814 -102.241238 
Round 52
-------------------------------
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [4.13977177 8.44962292 4.08581891 1.48801359 9.74245982 4.68747181
 1.834924   5.76952797 4.26183039 3.80061293]
obj_prev = 48.26005410532363
eta_min = 5.2478248998997527e-23	eta_max = 0.9386552346452276
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 11.138694899951254	eta = 0.9090909090909091
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 23.212442703241933	eta = 0.43623527270004686
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 16.931595727685714	eta = 0.598058590314986
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 15.799778450901695	eta = 0.6409005230136667
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 15.733820172197483	eta = 0.6435872637324471
af = 10.126086272682958	bf = 1.1451029605675662	zeta = 15.733571912722388	eta = 0.6435974188731335
eta = 0.6435974188731335
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [0.03636401 0.07647987 0.03578681 0.01240994 0.0883126  0.04213608
 0.01558458 0.05165999 0.03751843 0.03405519]
ene_total = [1.50031121 2.44142183 1.50915709 0.72099371 2.77897583 1.43455086
 0.8117007  1.8214624  1.52193281 1.19306548]
ti_comp = [0.78968755 0.8674062  0.78087534 0.81798185 0.86972334 0.87000865
 0.81855413 0.83227438 0.79380079 0.87219318]
ti_coms = [0.15198525 0.0742666  0.16079746 0.12369095 0.07194946 0.07166414
 0.12311867 0.10939841 0.147872   0.06947962]
t_total = [27.3997818 27.3997818 27.3997818 27.3997818 27.3997818 27.3997818
 27.3997818 27.3997818 27.3997818 27.3997818]
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [4.81930945e-06 3.71600544e-05 4.69770128e-06 1.78526131e-07
 5.69096341e-05 6.17725006e-06 3.53077655e-07 1.24396969e-05
 5.23830472e-06 3.24492313e-06]
ene_total = [0.44237055 0.21717421 0.46800782 0.35990781 0.2110067  0.20870035
 0.35824773 0.31867769 0.43041446 0.20225874]
optimize_network iter = 0 obj = 3.2167660550334363
eta = 0.6435974188731335
freqs = [23024303.73895735 44085381.74515043 22914544.46834145  7585706.59027375
 50770513.42376783 24215895.58340987  9519576.07846752 31035429.18235327
 23632141.3100814  19522731.20042242]
eta_min = 0.6435974188731354	eta_max = 0.698454705717541
af = 0.003087860896766924	bf = 1.1451029605675662	zeta = 0.003396646986443617	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [1.04231482e-06 8.03693475e-06 1.01601355e-06 3.86114307e-08
 1.23083516e-05 1.33600869e-06 7.63632379e-08 2.69044364e-06
 1.13293465e-06 7.01808322e-07]
ene_total = [1.7338512  0.84809379 1.83437118 1.41097682 0.82214886 0.81764254
 1.40445301 1.24824095 1.68694076 0.79265082]
ti_comp = [0.6447458  0.72246444 0.63593359 0.6730401  0.72478158 0.7250669
 0.67361237 0.68733263 0.64885904 0.72725142]
ti_coms = [0.15198525 0.0742666  0.16079746 0.12369095 0.07194946 0.07166414
 0.12311867 0.10939841 0.147872   0.06947962]
t_total = [27.3997818 27.3997818 27.3997818 27.3997818 27.3997818 27.3997818
 27.3997818 27.3997818 27.3997818 27.3997818]
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [3.90488746e-06 2.89320090e-05 3.82574001e-06 1.42428669e-07
 4.42612736e-05 4.80370504e-06 2.81601583e-07 9.85142590e-06
 4.23451096e-06 2.52087196e-06]
ene_total = [0.52281539 0.2563997  0.55311808 0.42538119 0.24895817 0.24662
 0.4234179  0.3765627  0.50868118 0.23902886]
optimize_network iter = 1 obj = 3.8009831688814426
eta = 0.698454705717541
freqs = [22965892.7521965  43105303.11724808 22914544.46834147  7508078.8429111
 49615295.72405244 23663342.1290412   9420739.8305015  30604647.12122058
 23544761.66323882 19067718.62692055]
eta_min = 0.698454705717542	eta_max = 0.6984547057175405
af = 0.002969273754740996	bf = 1.1451029605675662	zeta = 0.003266201130215096	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [1.03703298e-06 7.68356267e-06 1.01601355e-06 3.78252199e-08
 1.17546026e-05 1.27573473e-06 7.47857989e-08 2.61627349e-06
 1.12457210e-06 6.69475724e-07]
ene_total = [1.73385059 0.84805348 1.83437118 1.41097673 0.8220857  0.81763566
 1.40445283 1.24823249 1.68693981 0.79264713]
ti_comp = [0.6447458  0.72246444 0.63593359 0.6730401  0.72478158 0.7250669
 0.67361237 0.68733263 0.64885904 0.72725142]
ti_coms = [0.15198525 0.0742666  0.16079746 0.12369095 0.07194946 0.07166414
 0.12311867 0.10939841 0.147872   0.06947962]
t_total = [27.3997818 27.3997818 27.3997818 27.3997818 27.3997818 27.3997818
 27.3997818 27.3997818 27.3997818 27.3997818]
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [3.90488746e-06 2.89320090e-05 3.82574001e-06 1.42428669e-07
 4.42612736e-05 4.80370504e-06 2.81601583e-07 9.85142590e-06
 4.23451096e-06 2.52087196e-06]
ene_total = [0.52281539 0.2563997  0.55311808 0.42538119 0.24895817 0.24662
 0.4234179  0.3765627  0.50868118 0.23902886]
optimize_network iter = 2 obj = 3.800983168881437
eta = 0.6984547057175405
freqs = [22965892.7521965  43105303.11724809 22914544.46834146  7508078.8429111
 49615295.72405244 23663342.12904121  9420739.8305015  30604647.12122059
 23544761.66323882 19067718.62692055]
Done!
ene_coms = [0.01519852 0.00742666 0.01607975 0.01236909 0.00719495 0.00716641
 0.01231187 0.01093984 0.0147872  0.00694796]
ene_comp = [3.65510535e-06 2.70813288e-05 3.58102069e-06 1.33318001e-07
 4.14300335e-05 4.49642870e-06 2.63588507e-07 9.22126437e-06
 3.96364398e-06 2.35962053e-06]
ene_total = [0.01520218 0.00745374 0.01608333 0.01236923 0.00723638 0.00717091
 0.01231213 0.01094906 0.01479116 0.00695032]
At round 52 energy consumption: 0.11051844173128197
At round 52 eta: 0.6984547057175405
At round 52 a_n: 10.370218813849878
At round 52 local rounds: 11.751716534008594
At round 52 global rounds: 34.39025251090811
gradient difference: 0.5410770773887634
train() client id: f_00000-0-0 loss: 1.294831  [   32/  126]
train() client id: f_00000-0-1 loss: 1.153372  [   64/  126]
train() client id: f_00000-0-2 loss: 0.954354  [   96/  126]
train() client id: f_00000-1-0 loss: 1.220526  [   32/  126]
train() client id: f_00000-1-1 loss: 1.102041  [   64/  126]
train() client id: f_00000-1-2 loss: 1.018191  [   96/  126]
train() client id: f_00000-2-0 loss: 0.956072  [   32/  126]
train() client id: f_00000-2-1 loss: 1.040661  [   64/  126]
train() client id: f_00000-2-2 loss: 1.030124  [   96/  126]
train() client id: f_00000-3-0 loss: 0.875159  [   32/  126]
train() client id: f_00000-3-1 loss: 1.123349  [   64/  126]
train() client id: f_00000-3-2 loss: 1.125351  [   96/  126]
train() client id: f_00000-4-0 loss: 0.950810  [   32/  126]
train() client id: f_00000-4-1 loss: 0.934304  [   64/  126]
train() client id: f_00000-4-2 loss: 0.956607  [   96/  126]
train() client id: f_00000-5-0 loss: 0.943569  [   32/  126]
train() client id: f_00000-5-1 loss: 0.975384  [   64/  126]
train() client id: f_00000-5-2 loss: 1.008326  [   96/  126]
train() client id: f_00000-6-0 loss: 0.993761  [   32/  126]
train() client id: f_00000-6-1 loss: 0.924720  [   64/  126]
train() client id: f_00000-6-2 loss: 0.906972  [   96/  126]
train() client id: f_00000-7-0 loss: 1.002986  [   32/  126]
train() client id: f_00000-7-1 loss: 0.885556  [   64/  126]
train() client id: f_00000-7-2 loss: 0.976096  [   96/  126]
train() client id: f_00000-8-0 loss: 0.948175  [   32/  126]
train() client id: f_00000-8-1 loss: 1.103285  [   64/  126]
train() client id: f_00000-8-2 loss: 0.980834  [   96/  126]
train() client id: f_00000-9-0 loss: 1.056952  [   32/  126]
train() client id: f_00000-9-1 loss: 1.051675  [   64/  126]
train() client id: f_00000-9-2 loss: 0.908076  [   96/  126]
train() client id: f_00000-10-0 loss: 0.962323  [   32/  126]
train() client id: f_00000-10-1 loss: 0.955834  [   64/  126]
train() client id: f_00000-10-2 loss: 0.971834  [   96/  126]
train() client id: f_00001-0-0 loss: 0.546332  [   32/  265]
train() client id: f_00001-0-1 loss: 0.463439  [   64/  265]
train() client id: f_00001-0-2 loss: 0.485594  [   96/  265]
train() client id: f_00001-0-3 loss: 0.435424  [  128/  265]
train() client id: f_00001-0-4 loss: 0.484317  [  160/  265]
train() client id: f_00001-0-5 loss: 0.521770  [  192/  265]
train() client id: f_00001-0-6 loss: 0.538805  [  224/  265]
train() client id: f_00001-0-7 loss: 0.517826  [  256/  265]
train() client id: f_00001-1-0 loss: 0.617941  [   32/  265]
train() client id: f_00001-1-1 loss: 0.397806  [   64/  265]
train() client id: f_00001-1-2 loss: 0.468930  [   96/  265]
train() client id: f_00001-1-3 loss: 0.497541  [  128/  265]
train() client id: f_00001-1-4 loss: 0.519861  [  160/  265]
train() client id: f_00001-1-5 loss: 0.538453  [  192/  265]
train() client id: f_00001-1-6 loss: 0.423972  [  224/  265]
train() client id: f_00001-1-7 loss: 0.477078  [  256/  265]
train() client id: f_00001-2-0 loss: 0.418801  [   32/  265]
train() client id: f_00001-2-1 loss: 0.469125  [   64/  265]
train() client id: f_00001-2-2 loss: 0.389850  [   96/  265]
train() client id: f_00001-2-3 loss: 0.550177  [  128/  265]
train() client id: f_00001-2-4 loss: 0.473972  [  160/  265]
train() client id: f_00001-2-5 loss: 0.543978  [  192/  265]
train() client id: f_00001-2-6 loss: 0.557861  [  224/  265]
train() client id: f_00001-2-7 loss: 0.504372  [  256/  265]
train() client id: f_00001-3-0 loss: 0.471350  [   32/  265]
train() client id: f_00001-3-1 loss: 0.446194  [   64/  265]
train() client id: f_00001-3-2 loss: 0.401588  [   96/  265]
train() client id: f_00001-3-3 loss: 0.594152  [  128/  265]
train() client id: f_00001-3-4 loss: 0.467940  [  160/  265]
train() client id: f_00001-3-5 loss: 0.388182  [  192/  265]
train() client id: f_00001-3-6 loss: 0.582543  [  224/  265]
train() client id: f_00001-3-7 loss: 0.539787  [  256/  265]
train() client id: f_00001-4-0 loss: 0.404388  [   32/  265]
train() client id: f_00001-4-1 loss: 0.538224  [   64/  265]
train() client id: f_00001-4-2 loss: 0.508461  [   96/  265]
train() client id: f_00001-4-3 loss: 0.464346  [  128/  265]
train() client id: f_00001-4-4 loss: 0.432568  [  160/  265]
train() client id: f_00001-4-5 loss: 0.545194  [  192/  265]
train() client id: f_00001-4-6 loss: 0.457070  [  224/  265]
train() client id: f_00001-4-7 loss: 0.516289  [  256/  265]
train() client id: f_00001-5-0 loss: 0.395842  [   32/  265]
train() client id: f_00001-5-1 loss: 0.417488  [   64/  265]
train() client id: f_00001-5-2 loss: 0.559498  [   96/  265]
train() client id: f_00001-5-3 loss: 0.444317  [  128/  265]
train() client id: f_00001-5-4 loss: 0.400331  [  160/  265]
train() client id: f_00001-5-5 loss: 0.504510  [  192/  265]
train() client id: f_00001-5-6 loss: 0.520914  [  224/  265]
train() client id: f_00001-5-7 loss: 0.554222  [  256/  265]
train() client id: f_00001-6-0 loss: 0.561305  [   32/  265]
train() client id: f_00001-6-1 loss: 0.461943  [   64/  265]
train() client id: f_00001-6-2 loss: 0.447126  [   96/  265]
train() client id: f_00001-6-3 loss: 0.481400  [  128/  265]
train() client id: f_00001-6-4 loss: 0.419000  [  160/  265]
train() client id: f_00001-6-5 loss: 0.449058  [  192/  265]
train() client id: f_00001-6-6 loss: 0.421965  [  224/  265]
train() client id: f_00001-6-7 loss: 0.516045  [  256/  265]
train() client id: f_00001-7-0 loss: 0.436691  [   32/  265]
train() client id: f_00001-7-1 loss: 0.499145  [   64/  265]
train() client id: f_00001-7-2 loss: 0.507020  [   96/  265]
train() client id: f_00001-7-3 loss: 0.584165  [  128/  265]
train() client id: f_00001-7-4 loss: 0.440341  [  160/  265]
train() client id: f_00001-7-5 loss: 0.393063  [  192/  265]
train() client id: f_00001-7-6 loss: 0.581870  [  224/  265]
train() client id: f_00001-7-7 loss: 0.374299  [  256/  265]
train() client id: f_00001-8-0 loss: 0.508206  [   32/  265]
train() client id: f_00001-8-1 loss: 0.368519  [   64/  265]
train() client id: f_00001-8-2 loss: 0.513873  [   96/  265]
train() client id: f_00001-8-3 loss: 0.531113  [  128/  265]
train() client id: f_00001-8-4 loss: 0.371822  [  160/  265]
train() client id: f_00001-8-5 loss: 0.486617  [  192/  265]
train() client id: f_00001-8-6 loss: 0.476846  [  224/  265]
train() client id: f_00001-8-7 loss: 0.486548  [  256/  265]
train() client id: f_00001-9-0 loss: 0.457198  [   32/  265]
train() client id: f_00001-9-1 loss: 0.378100  [   64/  265]
train() client id: f_00001-9-2 loss: 0.445667  [   96/  265]
train() client id: f_00001-9-3 loss: 0.369347  [  128/  265]
train() client id: f_00001-9-4 loss: 0.486082  [  160/  265]
train() client id: f_00001-9-5 loss: 0.589797  [  192/  265]
train() client id: f_00001-9-6 loss: 0.456109  [  224/  265]
train() client id: f_00001-9-7 loss: 0.529843  [  256/  265]
train() client id: f_00001-10-0 loss: 0.478434  [   32/  265]
train() client id: f_00001-10-1 loss: 0.450569  [   64/  265]
train() client id: f_00001-10-2 loss: 0.441116  [   96/  265]
train() client id: f_00001-10-3 loss: 0.410391  [  128/  265]
train() client id: f_00001-10-4 loss: 0.431046  [  160/  265]
train() client id: f_00001-10-5 loss: 0.690346  [  192/  265]
train() client id: f_00001-10-6 loss: 0.469727  [  224/  265]
train() client id: f_00001-10-7 loss: 0.439343  [  256/  265]
train() client id: f_00002-0-0 loss: 1.126961  [   32/  124]
train() client id: f_00002-0-1 loss: 1.015260  [   64/  124]
train() client id: f_00002-0-2 loss: 1.029866  [   96/  124]
train() client id: f_00002-1-0 loss: 1.016083  [   32/  124]
train() client id: f_00002-1-1 loss: 0.871706  [   64/  124]
train() client id: f_00002-1-2 loss: 1.039049  [   96/  124]
train() client id: f_00002-2-0 loss: 1.111835  [   32/  124]
train() client id: f_00002-2-1 loss: 0.896891  [   64/  124]
train() client id: f_00002-2-2 loss: 0.914845  [   96/  124]
train() client id: f_00002-3-0 loss: 0.817264  [   32/  124]
train() client id: f_00002-3-1 loss: 0.976660  [   64/  124]
train() client id: f_00002-3-2 loss: 1.058446  [   96/  124]
train() client id: f_00002-4-0 loss: 0.841895  [   32/  124]
train() client id: f_00002-4-1 loss: 0.944034  [   64/  124]
train() client id: f_00002-4-2 loss: 0.964635  [   96/  124]
train() client id: f_00002-5-0 loss: 0.753454  [   32/  124]
train() client id: f_00002-5-1 loss: 0.880446  [   64/  124]
train() client id: f_00002-5-2 loss: 1.128821  [   96/  124]
train() client id: f_00002-6-0 loss: 0.965604  [   32/  124]
train() client id: f_00002-6-1 loss: 0.880017  [   64/  124]
train() client id: f_00002-6-2 loss: 0.824366  [   96/  124]
train() client id: f_00002-7-0 loss: 0.798413  [   32/  124]
train() client id: f_00002-7-1 loss: 0.956474  [   64/  124]
train() client id: f_00002-7-2 loss: 1.003478  [   96/  124]
train() client id: f_00002-8-0 loss: 1.180344  [   32/  124]
train() client id: f_00002-8-1 loss: 0.762176  [   64/  124]
train() client id: f_00002-8-2 loss: 0.888632  [   96/  124]
train() client id: f_00002-9-0 loss: 1.083511  [   32/  124]
train() client id: f_00002-9-1 loss: 0.868861  [   64/  124]
train() client id: f_00002-9-2 loss: 0.840811  [   96/  124]
train() client id: f_00002-10-0 loss: 0.935318  [   32/  124]
train() client id: f_00002-10-1 loss: 0.776521  [   64/  124]
train() client id: f_00002-10-2 loss: 0.816512  [   96/  124]
train() client id: f_00003-0-0 loss: 0.521213  [   32/   43]
train() client id: f_00003-1-0 loss: 0.611075  [   32/   43]
train() client id: f_00003-2-0 loss: 0.544371  [   32/   43]
train() client id: f_00003-3-0 loss: 0.545612  [   32/   43]
train() client id: f_00003-4-0 loss: 0.630666  [   32/   43]
train() client id: f_00003-5-0 loss: 0.600743  [   32/   43]
train() client id: f_00003-6-0 loss: 0.738776  [   32/   43]
train() client id: f_00003-7-0 loss: 0.455276  [   32/   43]
train() client id: f_00003-8-0 loss: 0.511045  [   32/   43]
train() client id: f_00003-9-0 loss: 0.656931  [   32/   43]
train() client id: f_00003-10-0 loss: 0.518560  [   32/   43]
train() client id: f_00004-0-0 loss: 0.995738  [   32/  306]
train() client id: f_00004-0-1 loss: 0.830439  [   64/  306]
train() client id: f_00004-0-2 loss: 0.989871  [   96/  306]
train() client id: f_00004-0-3 loss: 0.874419  [  128/  306]
train() client id: f_00004-0-4 loss: 1.178127  [  160/  306]
train() client id: f_00004-0-5 loss: 0.975410  [  192/  306]
train() client id: f_00004-0-6 loss: 0.809459  [  224/  306]
train() client id: f_00004-0-7 loss: 0.926547  [  256/  306]
train() client id: f_00004-0-8 loss: 0.879295  [  288/  306]
train() client id: f_00004-1-0 loss: 0.839213  [   32/  306]
train() client id: f_00004-1-1 loss: 0.741548  [   64/  306]
train() client id: f_00004-1-2 loss: 0.895143  [   96/  306]
train() client id: f_00004-1-3 loss: 1.045637  [  128/  306]
train() client id: f_00004-1-4 loss: 0.984735  [  160/  306]
train() client id: f_00004-1-5 loss: 0.906899  [  192/  306]
train() client id: f_00004-1-6 loss: 0.872874  [  224/  306]
train() client id: f_00004-1-7 loss: 1.017618  [  256/  306]
train() client id: f_00004-1-8 loss: 1.052447  [  288/  306]
train() client id: f_00004-2-0 loss: 0.961672  [   32/  306]
train() client id: f_00004-2-1 loss: 0.914526  [   64/  306]
train() client id: f_00004-2-2 loss: 0.802424  [   96/  306]
train() client id: f_00004-2-3 loss: 0.955293  [  128/  306]
train() client id: f_00004-2-4 loss: 1.045577  [  160/  306]
train() client id: f_00004-2-5 loss: 0.941606  [  192/  306]
train() client id: f_00004-2-6 loss: 1.015679  [  224/  306]
train() client id: f_00004-2-7 loss: 0.819362  [  256/  306]
train() client id: f_00004-2-8 loss: 0.899491  [  288/  306]
train() client id: f_00004-3-0 loss: 0.818798  [   32/  306]
train() client id: f_00004-3-1 loss: 0.877656  [   64/  306]
train() client id: f_00004-3-2 loss: 1.111401  [   96/  306]
train() client id: f_00004-3-3 loss: 0.917995  [  128/  306]
train() client id: f_00004-3-4 loss: 0.970685  [  160/  306]
train() client id: f_00004-3-5 loss: 0.844624  [  192/  306]
train() client id: f_00004-3-6 loss: 0.798501  [  224/  306]
train() client id: f_00004-3-7 loss: 0.933029  [  256/  306]
train() client id: f_00004-3-8 loss: 0.936548  [  288/  306]
train() client id: f_00004-4-0 loss: 0.969280  [   32/  306]
train() client id: f_00004-4-1 loss: 0.943500  [   64/  306]
train() client id: f_00004-4-2 loss: 0.881860  [   96/  306]
train() client id: f_00004-4-3 loss: 0.831953  [  128/  306]
train() client id: f_00004-4-4 loss: 0.968052  [  160/  306]
train() client id: f_00004-4-5 loss: 0.836768  [  192/  306]
train() client id: f_00004-4-6 loss: 0.970111  [  224/  306]
train() client id: f_00004-4-7 loss: 0.999584  [  256/  306]
train() client id: f_00004-4-8 loss: 0.890300  [  288/  306]
train() client id: f_00004-5-0 loss: 0.769835  [   32/  306]
train() client id: f_00004-5-1 loss: 0.930233  [   64/  306]
train() client id: f_00004-5-2 loss: 0.774687  [   96/  306]
train() client id: f_00004-5-3 loss: 0.934055  [  128/  306]
train() client id: f_00004-5-4 loss: 1.081022  [  160/  306]
train() client id: f_00004-5-5 loss: 0.948157  [  192/  306]
train() client id: f_00004-5-6 loss: 1.050691  [  224/  306]
train() client id: f_00004-5-7 loss: 0.838497  [  256/  306]
train() client id: f_00004-5-8 loss: 0.894382  [  288/  306]
train() client id: f_00004-6-0 loss: 0.938245  [   32/  306]
train() client id: f_00004-6-1 loss: 1.020960  [   64/  306]
train() client id: f_00004-6-2 loss: 0.893111  [   96/  306]
train() client id: f_00004-6-3 loss: 0.980295  [  128/  306]
train() client id: f_00004-6-4 loss: 0.862537  [  160/  306]
train() client id: f_00004-6-5 loss: 0.925952  [  192/  306]
train() client id: f_00004-6-6 loss: 0.887001  [  224/  306]
train() client id: f_00004-6-7 loss: 0.885815  [  256/  306]
train() client id: f_00004-6-8 loss: 0.918481  [  288/  306]
train() client id: f_00004-7-0 loss: 0.889732  [   32/  306]
train() client id: f_00004-7-1 loss: 0.926962  [   64/  306]
train() client id: f_00004-7-2 loss: 0.900135  [   96/  306]
train() client id: f_00004-7-3 loss: 1.005383  [  128/  306]
train() client id: f_00004-7-4 loss: 0.911230  [  160/  306]
train() client id: f_00004-7-5 loss: 0.838077  [  192/  306]
train() client id: f_00004-7-6 loss: 0.917292  [  224/  306]
train() client id: f_00004-7-7 loss: 0.993554  [  256/  306]
train() client id: f_00004-7-8 loss: 0.880008  [  288/  306]
train() client id: f_00004-8-0 loss: 0.890780  [   32/  306]
train() client id: f_00004-8-1 loss: 0.931846  [   64/  306]
train() client id: f_00004-8-2 loss: 0.917419  [   96/  306]
train() client id: f_00004-8-3 loss: 0.871183  [  128/  306]
train() client id: f_00004-8-4 loss: 0.878986  [  160/  306]
train() client id: f_00004-8-5 loss: 0.901782  [  192/  306]
train() client id: f_00004-8-6 loss: 0.872507  [  224/  306]
train() client id: f_00004-8-7 loss: 0.983947  [  256/  306]
train() client id: f_00004-8-8 loss: 0.940901  [  288/  306]
train() client id: f_00004-9-0 loss: 0.799957  [   32/  306]
train() client id: f_00004-9-1 loss: 0.871641  [   64/  306]
train() client id: f_00004-9-2 loss: 1.023641  [   96/  306]
train() client id: f_00004-9-3 loss: 0.962971  [  128/  306]
train() client id: f_00004-9-4 loss: 0.982957  [  160/  306]
train() client id: f_00004-9-5 loss: 0.921578  [  192/  306]
train() client id: f_00004-9-6 loss: 0.985308  [  224/  306]
train() client id: f_00004-9-7 loss: 0.876245  [  256/  306]
train() client id: f_00004-9-8 loss: 0.845257  [  288/  306]
train() client id: f_00004-10-0 loss: 0.753453  [   32/  306]
train() client id: f_00004-10-1 loss: 0.993566  [   64/  306]
train() client id: f_00004-10-2 loss: 0.841426  [   96/  306]
train() client id: f_00004-10-3 loss: 0.940005  [  128/  306]
train() client id: f_00004-10-4 loss: 0.871918  [  160/  306]
train() client id: f_00004-10-5 loss: 0.881375  [  192/  306]
train() client id: f_00004-10-6 loss: 0.998207  [  224/  306]
train() client id: f_00004-10-7 loss: 0.933019  [  256/  306]
train() client id: f_00004-10-8 loss: 1.040141  [  288/  306]
train() client id: f_00005-0-0 loss: 1.066134  [   32/  146]
train() client id: f_00005-0-1 loss: 0.611971  [   64/  146]
train() client id: f_00005-0-2 loss: 0.637086  [   96/  146]
train() client id: f_00005-0-3 loss: 0.795084  [  128/  146]
train() client id: f_00005-1-0 loss: 0.714529  [   32/  146]
train() client id: f_00005-1-1 loss: 0.813892  [   64/  146]
train() client id: f_00005-1-2 loss: 0.629293  [   96/  146]
train() client id: f_00005-1-3 loss: 0.904382  [  128/  146]
train() client id: f_00005-2-0 loss: 0.643101  [   32/  146]
train() client id: f_00005-2-1 loss: 0.873700  [   64/  146]
train() client id: f_00005-2-2 loss: 0.728413  [   96/  146]
train() client id: f_00005-2-3 loss: 0.663640  [  128/  146]
train() client id: f_00005-3-0 loss: 0.647634  [   32/  146]
train() client id: f_00005-3-1 loss: 0.616014  [   64/  146]
train() client id: f_00005-3-2 loss: 0.626304  [   96/  146]
train() client id: f_00005-3-3 loss: 1.001466  [  128/  146]
train() client id: f_00005-4-0 loss: 0.822819  [   32/  146]
train() client id: f_00005-4-1 loss: 0.709942  [   64/  146]
train() client id: f_00005-4-2 loss: 0.716262  [   96/  146]
train() client id: f_00005-4-3 loss: 0.770421  [  128/  146]
train() client id: f_00005-5-0 loss: 0.555649  [   32/  146]
train() client id: f_00005-5-1 loss: 0.975154  [   64/  146]
train() client id: f_00005-5-2 loss: 0.542237  [   96/  146]
train() client id: f_00005-5-3 loss: 0.783742  [  128/  146]
train() client id: f_00005-6-0 loss: 0.954533  [   32/  146]
train() client id: f_00005-6-1 loss: 0.841566  [   64/  146]
train() client id: f_00005-6-2 loss: 0.937904  [   96/  146]
train() client id: f_00005-6-3 loss: 0.533412  [  128/  146]
train() client id: f_00005-7-0 loss: 0.814678  [   32/  146]
train() client id: f_00005-7-1 loss: 0.761844  [   64/  146]
train() client id: f_00005-7-2 loss: 0.823777  [   96/  146]
train() client id: f_00005-7-3 loss: 0.562811  [  128/  146]
train() client id: f_00005-8-0 loss: 0.738262  [   32/  146]
train() client id: f_00005-8-1 loss: 0.758489  [   64/  146]
train() client id: f_00005-8-2 loss: 0.735097  [   96/  146]
train() client id: f_00005-8-3 loss: 0.887712  [  128/  146]
train() client id: f_00005-9-0 loss: 0.728664  [   32/  146]
train() client id: f_00005-9-1 loss: 0.745987  [   64/  146]
train() client id: f_00005-9-2 loss: 0.696092  [   96/  146]
train() client id: f_00005-9-3 loss: 0.921851  [  128/  146]
train() client id: f_00005-10-0 loss: 0.715940  [   32/  146]
train() client id: f_00005-10-1 loss: 0.874728  [   64/  146]
train() client id: f_00005-10-2 loss: 0.882300  [   96/  146]
train() client id: f_00005-10-3 loss: 0.425139  [  128/  146]
train() client id: f_00006-0-0 loss: 0.521448  [   32/   54]
train() client id: f_00006-1-0 loss: 0.487455  [   32/   54]
train() client id: f_00006-2-0 loss: 0.554827  [   32/   54]
train() client id: f_00006-3-0 loss: 0.607874  [   32/   54]
train() client id: f_00006-4-0 loss: 0.532299  [   32/   54]
train() client id: f_00006-5-0 loss: 0.546936  [   32/   54]
train() client id: f_00006-6-0 loss: 0.584933  [   32/   54]
train() client id: f_00006-7-0 loss: 0.608579  [   32/   54]
train() client id: f_00006-8-0 loss: 0.542979  [   32/   54]
train() client id: f_00006-9-0 loss: 0.564258  [   32/   54]
train() client id: f_00006-10-0 loss: 0.557811  [   32/   54]
train() client id: f_00007-0-0 loss: 0.917368  [   32/  179]
train() client id: f_00007-0-1 loss: 0.638929  [   64/  179]
train() client id: f_00007-0-2 loss: 0.566172  [   96/  179]
train() client id: f_00007-0-3 loss: 0.459652  [  128/  179]
train() client id: f_00007-0-4 loss: 0.531746  [  160/  179]
train() client id: f_00007-1-0 loss: 0.597254  [   32/  179]
train() client id: f_00007-1-1 loss: 0.511908  [   64/  179]
train() client id: f_00007-1-2 loss: 0.528606  [   96/  179]
train() client id: f_00007-1-3 loss: 0.638019  [  128/  179]
train() client id: f_00007-1-4 loss: 0.824917  [  160/  179]
train() client id: f_00007-2-0 loss: 0.510152  [   32/  179]
train() client id: f_00007-2-1 loss: 0.656942  [   64/  179]
train() client id: f_00007-2-2 loss: 0.657172  [   96/  179]
train() client id: f_00007-2-3 loss: 0.657880  [  128/  179]
train() client id: f_00007-2-4 loss: 0.644416  [  160/  179]
train() client id: f_00007-3-0 loss: 0.470969  [   32/  179]
train() client id: f_00007-3-1 loss: 0.646048  [   64/  179]
train() client id: f_00007-3-2 loss: 0.474960  [   96/  179]
train() client id: f_00007-3-3 loss: 0.847127  [  128/  179]
train() client id: f_00007-3-4 loss: 0.503063  [  160/  179]
train() client id: f_00007-4-0 loss: 0.722923  [   32/  179]
train() client id: f_00007-4-1 loss: 0.439511  [   64/  179]
train() client id: f_00007-4-2 loss: 0.625873  [   96/  179]
train() client id: f_00007-4-3 loss: 0.522523  [  128/  179]
train() client id: f_00007-4-4 loss: 0.650777  [  160/  179]
train() client id: f_00007-5-0 loss: 0.704695  [   32/  179]
train() client id: f_00007-5-1 loss: 0.540445  [   64/  179]
train() client id: f_00007-5-2 loss: 0.581573  [   96/  179]
train() client id: f_00007-5-3 loss: 0.432991  [  128/  179]
train() client id: f_00007-5-4 loss: 0.580274  [  160/  179]
train() client id: f_00007-6-0 loss: 0.593978  [   32/  179]
train() client id: f_00007-6-1 loss: 0.529493  [   64/  179]
train() client id: f_00007-6-2 loss: 0.707230  [   96/  179]
train() client id: f_00007-6-3 loss: 0.525180  [  128/  179]
train() client id: f_00007-6-4 loss: 0.571558  [  160/  179]
train() client id: f_00007-7-0 loss: 0.463568  [   32/  179]
train() client id: f_00007-7-1 loss: 0.617693  [   64/  179]
train() client id: f_00007-7-2 loss: 0.788041  [   96/  179]
train() client id: f_00007-7-3 loss: 0.475562  [  128/  179]
train() client id: f_00007-7-4 loss: 0.601805  [  160/  179]
train() client id: f_00007-8-0 loss: 0.465127  [   32/  179]
train() client id: f_00007-8-1 loss: 0.527796  [   64/  179]
train() client id: f_00007-8-2 loss: 0.873796  [   96/  179]
train() client id: f_00007-8-3 loss: 0.641360  [  128/  179]
train() client id: f_00007-8-4 loss: 0.534049  [  160/  179]
train() client id: f_00007-9-0 loss: 0.600457  [   32/  179]
train() client id: f_00007-9-1 loss: 0.547322  [   64/  179]
train() client id: f_00007-9-2 loss: 0.522414  [   96/  179]
train() client id: f_00007-9-3 loss: 0.597668  [  128/  179]
train() client id: f_00007-9-4 loss: 0.652444  [  160/  179]
train() client id: f_00007-10-0 loss: 0.669620  [   32/  179]
train() client id: f_00007-10-1 loss: 0.505838  [   64/  179]
train() client id: f_00007-10-2 loss: 0.517873  [   96/  179]
train() client id: f_00007-10-3 loss: 0.540200  [  128/  179]
train() client id: f_00007-10-4 loss: 0.577089  [  160/  179]
train() client id: f_00008-0-0 loss: 0.864033  [   32/  130]
train() client id: f_00008-0-1 loss: 0.731539  [   64/  130]
train() client id: f_00008-0-2 loss: 0.582787  [   96/  130]
train() client id: f_00008-0-3 loss: 0.703010  [  128/  130]
train() client id: f_00008-1-0 loss: 0.656624  [   32/  130]
train() client id: f_00008-1-1 loss: 0.556593  [   64/  130]
train() client id: f_00008-1-2 loss: 0.882340  [   96/  130]
train() client id: f_00008-1-3 loss: 0.779161  [  128/  130]
train() client id: f_00008-2-0 loss: 0.800031  [   32/  130]
train() client id: f_00008-2-1 loss: 0.749274  [   64/  130]
train() client id: f_00008-2-2 loss: 0.659262  [   96/  130]
train() client id: f_00008-2-3 loss: 0.672254  [  128/  130]
train() client id: f_00008-3-0 loss: 0.756207  [   32/  130]
train() client id: f_00008-3-1 loss: 0.705698  [   64/  130]
train() client id: f_00008-3-2 loss: 0.700858  [   96/  130]
train() client id: f_00008-3-3 loss: 0.682840  [  128/  130]
train() client id: f_00008-4-0 loss: 0.821975  [   32/  130]
train() client id: f_00008-4-1 loss: 0.608980  [   64/  130]
train() client id: f_00008-4-2 loss: 0.628339  [   96/  130]
train() client id: f_00008-4-3 loss: 0.760795  [  128/  130]
train() client id: f_00008-5-0 loss: 0.582155  [   32/  130]
train() client id: f_00008-5-1 loss: 0.816361  [   64/  130]
train() client id: f_00008-5-2 loss: 0.834888  [   96/  130]
train() client id: f_00008-5-3 loss: 0.629776  [  128/  130]
train() client id: f_00008-6-0 loss: 0.711968  [   32/  130]
train() client id: f_00008-6-1 loss: 0.694361  [   64/  130]
train() client id: f_00008-6-2 loss: 0.731508  [   96/  130]
train() client id: f_00008-6-3 loss: 0.721551  [  128/  130]
train() client id: f_00008-7-0 loss: 0.861831  [   32/  130]
train() client id: f_00008-7-1 loss: 0.677000  [   64/  130]
train() client id: f_00008-7-2 loss: 0.656277  [   96/  130]
train() client id: f_00008-7-3 loss: 0.643174  [  128/  130]
train() client id: f_00008-8-0 loss: 0.700471  [   32/  130]
train() client id: f_00008-8-1 loss: 0.697334  [   64/  130]
train() client id: f_00008-8-2 loss: 0.816682  [   96/  130]
train() client id: f_00008-8-3 loss: 0.645514  [  128/  130]
train() client id: f_00008-9-0 loss: 0.806081  [   32/  130]
train() client id: f_00008-9-1 loss: 0.651494  [   64/  130]
train() client id: f_00008-9-2 loss: 0.640089  [   96/  130]
train() client id: f_00008-9-3 loss: 0.754769  [  128/  130]
train() client id: f_00008-10-0 loss: 0.673552  [   32/  130]
train() client id: f_00008-10-1 loss: 0.690687  [   64/  130]
train() client id: f_00008-10-2 loss: 0.862792  [   96/  130]
train() client id: f_00008-10-3 loss: 0.624118  [  128/  130]
train() client id: f_00009-0-0 loss: 0.901646  [   32/  118]
train() client id: f_00009-0-1 loss: 0.885954  [   64/  118]
train() client id: f_00009-0-2 loss: 0.653447  [   96/  118]
train() client id: f_00009-1-0 loss: 0.957763  [   32/  118]
train() client id: f_00009-1-1 loss: 0.751718  [   64/  118]
train() client id: f_00009-1-2 loss: 0.759600  [   96/  118]
train() client id: f_00009-2-0 loss: 0.575301  [   32/  118]
train() client id: f_00009-2-1 loss: 0.787928  [   64/  118]
train() client id: f_00009-2-2 loss: 0.894464  [   96/  118]
train() client id: f_00009-3-0 loss: 0.686500  [   32/  118]
train() client id: f_00009-3-1 loss: 0.553739  [   64/  118]
train() client id: f_00009-3-2 loss: 0.900404  [   96/  118]
train() client id: f_00009-4-0 loss: 0.675842  [   32/  118]
train() client id: f_00009-4-1 loss: 0.739909  [   64/  118]
train() client id: f_00009-4-2 loss: 0.790098  [   96/  118]
train() client id: f_00009-5-0 loss: 0.771417  [   32/  118]
train() client id: f_00009-5-1 loss: 0.678845  [   64/  118]
train() client id: f_00009-5-2 loss: 0.656816  [   96/  118]
train() client id: f_00009-6-0 loss: 0.655751  [   32/  118]
train() client id: f_00009-6-1 loss: 0.759274  [   64/  118]
train() client id: f_00009-6-2 loss: 0.643692  [   96/  118]
train() client id: f_00009-7-0 loss: 0.744461  [   32/  118]
train() client id: f_00009-7-1 loss: 0.633125  [   64/  118]
train() client id: f_00009-7-2 loss: 0.614411  [   96/  118]
train() client id: f_00009-8-0 loss: 0.691386  [   32/  118]
train() client id: f_00009-8-1 loss: 0.655301  [   64/  118]
train() client id: f_00009-8-2 loss: 0.638205  [   96/  118]
train() client id: f_00009-9-0 loss: 0.570622  [   32/  118]
train() client id: f_00009-9-1 loss: 0.666548  [   64/  118]
train() client id: f_00009-9-2 loss: 0.666656  [   96/  118]
train() client id: f_00009-10-0 loss: 0.665316  [   32/  118]
train() client id: f_00009-10-1 loss: 0.549897  [   64/  118]
train() client id: f_00009-10-2 loss: 0.746902  [   96/  118]
At round 52 accuracy: 0.649867374005305
At round 52 training accuracy: 0.5888665325285044
At round 52 training loss: 0.8336067096536561
update_location
xs = -3.905658 4.200318 280.009024 18.811294 0.979296 3.956410 -242.443192 -221.324852 264.663977 -207.060879 
ys = 272.587959 255.555839 1.320614 -242.455176 234.350187 217.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 290.378115 274.456607 297.332806 262.941775 254.795936 239.705355 262.270074 242.869031 283.470793 229.978737 
dists_bs = 196.968738 196.473909 487.029507 460.251716 185.717951 184.006652 189.997945 180.232756 466.999925 173.978140 
uav_gains = -114.906533 -113.476299 -115.506304 -112.431343 -111.709182 -110.448272 -112.371026 -110.702634 -114.293024 -109.702121 
bs_gains = -103.810184 -103.779596 -114.818607 -114.130931 -103.094968 -102.982398 -103.372028 -102.730403 -114.307930 -102.300910 
Round 53
-------------------------------
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [4.00899084 8.17097489 3.95724982 1.44293614 9.42099912 4.53288609
 1.77835404 5.58181324 4.12270253 3.67526442]
obj_prev = 46.692171130454035
eta_min = 9.633908147429921e-24	eta_max = 0.9382560279743308
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 10.770764989587082	eta = 0.9090909090909091
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 22.73132098521351	eta = 0.4307538722609915
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 16.477782321424517	eta = 0.5942307250446651
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 15.352697592199675	eta = 0.6377774640049659
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 15.286662235887265	eta = 0.6405325364618376
af = 9.791604535988256	bf = 1.1332441370306174	zeta = 15.286409541549396	eta = 0.6405431248831895
eta = 0.6405431248831895
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [0.03675654 0.07730543 0.03617311 0.0125439  0.08926589 0.04259092
 0.0157528  0.05221763 0.03792342 0.0344228 ]
ene_total = [1.46579906 2.36500809 1.47528965 0.70621205 2.69190422 1.38875393
 0.79404133 1.76977105 1.47499189 1.15463827]
ti_comp = [0.82252146 0.90567545 0.81328504 0.85268029 0.90809929 0.90848315
 0.85328562 0.86816903 0.8308949  0.91072419]
ti_coms = [0.15787206 0.07471807 0.16710848 0.12771323 0.07229423 0.07191036
 0.1271079  0.11222449 0.14949862 0.06966933]
t_total = [27.3497776 27.3497776 27.3497776 27.3497776 27.3497776 27.3497776
 27.3497776 27.3497776 27.3497776 27.3497776]
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [4.58764259e-06 3.52017978e-05 4.47251598e-06 1.69670107e-07
 5.39100853e-05 5.85055712e-06 3.35555868e-07 1.18065477e-05
 4.93753617e-06 3.07358121e-06]
ene_total = [0.44053948 0.20942103 0.46630285 0.35628294 0.2031812  0.20076965
 0.35459888 0.31339904 0.41719008 0.19444042]
optimize_network iter = 0 obj = 3.1561255646288915
eta = 0.6405431248831895
freqs = [22343821.0822694  42678329.73940749 22238886.46292163  7355570.59611931
 49149853.28596357 23440674.99681275  9230675.16871088 30073423.9891545
 22820828.02431027 18898584.29975834]
eta_min = 0.6405431248831939	eta_max = 0.6991595698398908
af = 0.002799252834533405	bf = 1.1332441370306174	zeta = 0.0030791781179867455	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [9.81614110e-07 7.53209970e-06 9.56980564e-07 3.63041731e-08
 1.15350966e-05 1.25183889e-06 7.17986128e-08 2.52623730e-06
 1.05648055e-06 6.57651642e-07]
ene_total = [1.74150662 0.82500401 1.84338565 1.40873715 0.79870952 0.79334107
 1.40206398 1.23816416 1.64915218 0.76855593]
ti_comp = [0.6626492  0.74580319 0.65341278 0.69280803 0.74822703 0.7486109
 0.69341336 0.70829677 0.67102264 0.75085193]
ti_coms = [0.15787206 0.07471807 0.16710848 0.12771323 0.07229423 0.07191036
 0.1271079  0.11222449 0.14949862 0.06966933]
t_total = [27.3497776 27.3497776 27.3497776 27.3497776 27.3497776 27.3497776
 27.3497776 27.3497776 27.3497776 27.3497776]
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [3.66565189e-06 2.69213035e-05 3.59332702e-06 1.33286597e-07
 4.11817149e-05 4.46841209e-06 2.63514067e-07 9.19889168e-06
 3.92611172e-06 2.34500816e-06]
ene_total = [0.52634448 0.2499491  0.55712907 0.42570072 0.24234523 0.239842
 0.42368735 0.37437551 0.49844265 0.23230136]
optimize_network iter = 1 obj = 3.770117456851034
eta = 0.6991595698398908
freqs = [22282599.06518497 41639030.99946182 22238886.46292161  7273350.56555898
 47925539.38367743 22854708.69293805  9126001.3904321  29615341.56378339
 22703099.87437893 18416482.47146711]
eta_min = 0.6991595698399091	eta_max = 0.6991595698398846
af = 0.002681430697799304	bf = 1.1332441370306174	zeta = 0.0029495737675792346	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [9.76242239e-07 7.16972436e-06 9.56980564e-07 3.54970983e-08
 1.09675798e-05 1.19003461e-06 7.01794852e-08 2.44986346e-06
 1.04560832e-06 6.24526300e-07]
ene_total = [1.74150603 0.82496404 1.84338565 1.40873707 0.79864692 0.79333425
 1.4020638  1.23815573 1.64915098 0.76855228]
ti_comp = [0.6626492  0.74580319 0.65341278 0.69280803 0.74822703 0.7486109
 0.69341336 0.70829677 0.67102264 0.75085193]
ti_coms = [0.15787206 0.07471807 0.16710848 0.12771323 0.07229423 0.07191036
 0.1271079  0.11222449 0.14949862 0.06966933]
t_total = [27.3497776 27.3497776 27.3497776 27.3497776 27.3497776 27.3497776
 27.3497776 27.3497776 27.3497776 27.3497776]
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [3.66565189e-06 2.69213035e-05 3.59332702e-06 1.33286597e-07
 4.11817149e-05 4.46841209e-06 2.63514067e-07 9.19889168e-06
 3.92611172e-06 2.34500816e-06]
ene_total = [0.52634448 0.2499491  0.55712907 0.42570072 0.24234523 0.239842
 0.42368735 0.37437551 0.49844265 0.23230136]
optimize_network iter = 2 obj = 3.770117456850956
eta = 0.6991595698398846
freqs = [22282599.06518495 41639030.99946193 22238886.46292159  7273350.56555898
 47925539.38367755 22854708.6929381   9126001.39043211 29615341.56378343
 22703099.87437893 18416482.47146716]
Done!
ene_coms = [0.01578721 0.00747181 0.01671085 0.01277132 0.00722942 0.00719104
 0.01271079 0.01122245 0.01494986 0.00696693]
ene_comp = [3.44084355e-06 2.52702648e-05 3.37295424e-06 1.25112351e-07
 3.86561088e-05 4.19437181e-06 2.47353187e-07 8.63473895e-06
 3.68532982e-06 2.20119270e-06]
ene_total = [0.01579065 0.00749708 0.01671422 0.01277145 0.00726808 0.00719523
 0.01271104 0.01123108 0.01495355 0.00696913]
At round 53 energy consumption: 0.11310150513411767
At round 53 eta: 0.6991595698398846
At round 53 a_n: 10.027672966880559
At round 53 local rounds: 11.718687629159392
At round 53 global rounds: 33.33219860622975
gradient difference: 0.5288693904876709
train() client id: f_00000-0-0 loss: 1.219954  [   32/  126]
train() client id: f_00000-0-1 loss: 0.787294  [   64/  126]
train() client id: f_00000-0-2 loss: 1.007495  [   96/  126]
train() client id: f_00000-1-0 loss: 1.214512  [   32/  126]
train() client id: f_00000-1-1 loss: 0.796152  [   64/  126]
train() client id: f_00000-1-2 loss: 0.948845  [   96/  126]
train() client id: f_00000-2-0 loss: 1.030669  [   32/  126]
train() client id: f_00000-2-1 loss: 0.876485  [   64/  126]
train() client id: f_00000-2-2 loss: 0.885462  [   96/  126]
train() client id: f_00000-3-0 loss: 0.995344  [   32/  126]
train() client id: f_00000-3-1 loss: 0.888615  [   64/  126]
train() client id: f_00000-3-2 loss: 0.707341  [   96/  126]
train() client id: f_00000-4-0 loss: 0.830728  [   32/  126]
train() client id: f_00000-4-1 loss: 0.807035  [   64/  126]
train() client id: f_00000-4-2 loss: 0.846457  [   96/  126]
train() client id: f_00000-5-0 loss: 0.726282  [   32/  126]
train() client id: f_00000-5-1 loss: 0.900478  [   64/  126]
train() client id: f_00000-5-2 loss: 0.854049  [   96/  126]
train() client id: f_00000-6-0 loss: 0.749527  [   32/  126]
train() client id: f_00000-6-1 loss: 0.812569  [   64/  126]
train() client id: f_00000-6-2 loss: 0.823784  [   96/  126]
train() client id: f_00000-7-0 loss: 0.724065  [   32/  126]
train() client id: f_00000-7-1 loss: 0.827394  [   64/  126]
train() client id: f_00000-7-2 loss: 0.760692  [   96/  126]
train() client id: f_00000-8-0 loss: 0.810001  [   32/  126]
train() client id: f_00000-8-1 loss: 0.711581  [   64/  126]
train() client id: f_00000-8-2 loss: 0.860220  [   96/  126]
train() client id: f_00000-9-0 loss: 0.769833  [   32/  126]
train() client id: f_00000-9-1 loss: 0.946928  [   64/  126]
train() client id: f_00000-9-2 loss: 0.835075  [   96/  126]
train() client id: f_00000-10-0 loss: 0.618481  [   32/  126]
train() client id: f_00000-10-1 loss: 0.823069  [   64/  126]
train() client id: f_00000-10-2 loss: 0.892675  [   96/  126]
train() client id: f_00001-0-0 loss: 0.379682  [   32/  265]
train() client id: f_00001-0-1 loss: 0.401088  [   64/  265]
train() client id: f_00001-0-2 loss: 0.698887  [   96/  265]
train() client id: f_00001-0-3 loss: 0.517067  [  128/  265]
train() client id: f_00001-0-4 loss: 0.468562  [  160/  265]
train() client id: f_00001-0-5 loss: 0.366180  [  192/  265]
train() client id: f_00001-0-6 loss: 0.426285  [  224/  265]
train() client id: f_00001-0-7 loss: 0.423605  [  256/  265]
train() client id: f_00001-1-0 loss: 0.422207  [   32/  265]
train() client id: f_00001-1-1 loss: 0.612540  [   64/  265]
train() client id: f_00001-1-2 loss: 0.399442  [   96/  265]
train() client id: f_00001-1-3 loss: 0.410366  [  128/  265]
train() client id: f_00001-1-4 loss: 0.394204  [  160/  265]
train() client id: f_00001-1-5 loss: 0.566656  [  192/  265]
train() client id: f_00001-1-6 loss: 0.365878  [  224/  265]
train() client id: f_00001-1-7 loss: 0.475223  [  256/  265]
train() client id: f_00001-2-0 loss: 0.428130  [   32/  265]
train() client id: f_00001-2-1 loss: 0.564258  [   64/  265]
train() client id: f_00001-2-2 loss: 0.359462  [   96/  265]
train() client id: f_00001-2-3 loss: 0.503459  [  128/  265]
train() client id: f_00001-2-4 loss: 0.385038  [  160/  265]
train() client id: f_00001-2-5 loss: 0.398869  [  192/  265]
train() client id: f_00001-2-6 loss: 0.533663  [  224/  265]
train() client id: f_00001-2-7 loss: 0.439061  [  256/  265]
train() client id: f_00001-3-0 loss: 0.447054  [   32/  265]
train() client id: f_00001-3-1 loss: 0.425463  [   64/  265]
train() client id: f_00001-3-2 loss: 0.357984  [   96/  265]
train() client id: f_00001-3-3 loss: 0.357621  [  128/  265]
train() client id: f_00001-3-4 loss: 0.460242  [  160/  265]
train() client id: f_00001-3-5 loss: 0.439455  [  192/  265]
train() client id: f_00001-3-6 loss: 0.391174  [  224/  265]
train() client id: f_00001-3-7 loss: 0.676212  [  256/  265]
train() client id: f_00001-4-0 loss: 0.444855  [   32/  265]
train() client id: f_00001-4-1 loss: 0.350396  [   64/  265]
train() client id: f_00001-4-2 loss: 0.380071  [   96/  265]
train() client id: f_00001-4-3 loss: 0.466598  [  128/  265]
train() client id: f_00001-4-4 loss: 0.426296  [  160/  265]
train() client id: f_00001-4-5 loss: 0.454711  [  192/  265]
train() client id: f_00001-4-6 loss: 0.553909  [  224/  265]
train() client id: f_00001-4-7 loss: 0.493766  [  256/  265]
train() client id: f_00001-5-0 loss: 0.522124  [   32/  265]
train() client id: f_00001-5-1 loss: 0.428898  [   64/  265]
train() client id: f_00001-5-2 loss: 0.334650  [   96/  265]
train() client id: f_00001-5-3 loss: 0.378619  [  128/  265]
train() client id: f_00001-5-4 loss: 0.356052  [  160/  265]
train() client id: f_00001-5-5 loss: 0.510624  [  192/  265]
train() client id: f_00001-5-6 loss: 0.507314  [  224/  265]
train() client id: f_00001-5-7 loss: 0.457294  [  256/  265]
train() client id: f_00001-6-0 loss: 0.404681  [   32/  265]
train() client id: f_00001-6-1 loss: 0.390132  [   64/  265]
train() client id: f_00001-6-2 loss: 0.433528  [   96/  265]
train() client id: f_00001-6-3 loss: 0.407401  [  128/  265]
train() client id: f_00001-6-4 loss: 0.335625  [  160/  265]
train() client id: f_00001-6-5 loss: 0.390964  [  192/  265]
train() client id: f_00001-6-6 loss: 0.449088  [  224/  265]
train() client id: f_00001-6-7 loss: 0.544256  [  256/  265]
train() client id: f_00001-7-0 loss: 0.596365  [   32/  265]
train() client id: f_00001-7-1 loss: 0.464490  [   64/  265]
train() client id: f_00001-7-2 loss: 0.325537  [   96/  265]
train() client id: f_00001-7-3 loss: 0.453927  [  128/  265]
train() client id: f_00001-7-4 loss: 0.407300  [  160/  265]
train() client id: f_00001-7-5 loss: 0.402317  [  192/  265]
train() client id: f_00001-7-6 loss: 0.445648  [  224/  265]
train() client id: f_00001-7-7 loss: 0.433146  [  256/  265]
train() client id: f_00001-8-0 loss: 0.331396  [   32/  265]
train() client id: f_00001-8-1 loss: 0.387653  [   64/  265]
train() client id: f_00001-8-2 loss: 0.426381  [   96/  265]
train() client id: f_00001-8-3 loss: 0.603054  [  128/  265]
train() client id: f_00001-8-4 loss: 0.463281  [  160/  265]
train() client id: f_00001-8-5 loss: 0.349387  [  192/  265]
train() client id: f_00001-8-6 loss: 0.447605  [  224/  265]
train() client id: f_00001-8-7 loss: 0.526606  [  256/  265]
train() client id: f_00001-9-0 loss: 0.409576  [   32/  265]
train() client id: f_00001-9-1 loss: 0.470258  [   64/  265]
train() client id: f_00001-9-2 loss: 0.336864  [   96/  265]
train() client id: f_00001-9-3 loss: 0.453555  [  128/  265]
train() client id: f_00001-9-4 loss: 0.407768  [  160/  265]
train() client id: f_00001-9-5 loss: 0.349713  [  192/  265]
train() client id: f_00001-9-6 loss: 0.683989  [  224/  265]
train() client id: f_00001-9-7 loss: 0.420759  [  256/  265]
train() client id: f_00001-10-0 loss: 0.393515  [   32/  265]
train() client id: f_00001-10-1 loss: 0.517134  [   64/  265]
train() client id: f_00001-10-2 loss: 0.452491  [   96/  265]
train() client id: f_00001-10-3 loss: 0.350763  [  128/  265]
train() client id: f_00001-10-4 loss: 0.325468  [  160/  265]
train() client id: f_00001-10-5 loss: 0.366760  [  192/  265]
train() client id: f_00001-10-6 loss: 0.471464  [  224/  265]
train() client id: f_00001-10-7 loss: 0.560498  [  256/  265]
train() client id: f_00002-0-0 loss: 1.272232  [   32/  124]
train() client id: f_00002-0-1 loss: 1.173831  [   64/  124]
train() client id: f_00002-0-2 loss: 1.218051  [   96/  124]
train() client id: f_00002-1-0 loss: 1.079220  [   32/  124]
train() client id: f_00002-1-1 loss: 1.220645  [   64/  124]
train() client id: f_00002-1-2 loss: 1.161497  [   96/  124]
train() client id: f_00002-2-0 loss: 1.121237  [   32/  124]
train() client id: f_00002-2-1 loss: 1.259475  [   64/  124]
train() client id: f_00002-2-2 loss: 1.184560  [   96/  124]
train() client id: f_00002-3-0 loss: 1.069262  [   32/  124]
train() client id: f_00002-3-1 loss: 1.111569  [   64/  124]
train() client id: f_00002-3-2 loss: 1.135486  [   96/  124]
train() client id: f_00002-4-0 loss: 0.868873  [   32/  124]
train() client id: f_00002-4-1 loss: 1.337377  [   64/  124]
train() client id: f_00002-4-2 loss: 1.243598  [   96/  124]
train() client id: f_00002-5-0 loss: 0.945955  [   32/  124]
train() client id: f_00002-5-1 loss: 1.104390  [   64/  124]
train() client id: f_00002-5-2 loss: 1.170703  [   96/  124]
train() client id: f_00002-6-0 loss: 1.235872  [   32/  124]
train() client id: f_00002-6-1 loss: 0.867751  [   64/  124]
train() client id: f_00002-6-2 loss: 1.116465  [   96/  124]
train() client id: f_00002-7-0 loss: 0.940848  [   32/  124]
train() client id: f_00002-7-1 loss: 1.122941  [   64/  124]
train() client id: f_00002-7-2 loss: 1.115933  [   96/  124]
train() client id: f_00002-8-0 loss: 1.060626  [   32/  124]
train() client id: f_00002-8-1 loss: 1.197842  [   64/  124]
train() client id: f_00002-8-2 loss: 0.911936  [   96/  124]
train() client id: f_00002-9-0 loss: 1.017576  [   32/  124]
train() client id: f_00002-9-1 loss: 1.133042  [   64/  124]
train() client id: f_00002-9-2 loss: 1.062554  [   96/  124]
train() client id: f_00002-10-0 loss: 1.160471  [   32/  124]
train() client id: f_00002-10-1 loss: 0.849506  [   64/  124]
train() client id: f_00002-10-2 loss: 1.040665  [   96/  124]
train() client id: f_00003-0-0 loss: 0.694084  [   32/   43]
train() client id: f_00003-1-0 loss: 0.834876  [   32/   43]
train() client id: f_00003-2-0 loss: 0.711858  [   32/   43]
train() client id: f_00003-3-0 loss: 0.696838  [   32/   43]
train() client id: f_00003-4-0 loss: 0.826464  [   32/   43]
train() client id: f_00003-5-0 loss: 1.016454  [   32/   43]
train() client id: f_00003-6-0 loss: 0.678088  [   32/   43]
train() client id: f_00003-7-0 loss: 0.681822  [   32/   43]
train() client id: f_00003-8-0 loss: 0.817908  [   32/   43]
train() client id: f_00003-9-0 loss: 0.752742  [   32/   43]
train() client id: f_00003-10-0 loss: 0.727659  [   32/   43]
train() client id: f_00004-0-0 loss: 0.855478  [   32/  306]
train() client id: f_00004-0-1 loss: 0.647013  [   64/  306]
train() client id: f_00004-0-2 loss: 0.659310  [   96/  306]
train() client id: f_00004-0-3 loss: 0.775949  [  128/  306]
train() client id: f_00004-0-4 loss: 0.718387  [  160/  306]
train() client id: f_00004-0-5 loss: 0.639651  [  192/  306]
train() client id: f_00004-0-6 loss: 0.766493  [  224/  306]
train() client id: f_00004-0-7 loss: 0.538800  [  256/  306]
train() client id: f_00004-0-8 loss: 0.671872  [  288/  306]
train() client id: f_00004-1-0 loss: 0.610295  [   32/  306]
train() client id: f_00004-1-1 loss: 0.603972  [   64/  306]
train() client id: f_00004-1-2 loss: 0.690845  [   96/  306]
train() client id: f_00004-1-3 loss: 0.748541  [  128/  306]
train() client id: f_00004-1-4 loss: 0.615823  [  160/  306]
train() client id: f_00004-1-5 loss: 0.735284  [  192/  306]
train() client id: f_00004-1-6 loss: 0.807441  [  224/  306]
train() client id: f_00004-1-7 loss: 0.837854  [  256/  306]
train() client id: f_00004-1-8 loss: 0.788385  [  288/  306]
train() client id: f_00004-2-0 loss: 0.659873  [   32/  306]
train() client id: f_00004-2-1 loss: 0.828775  [   64/  306]
train() client id: f_00004-2-2 loss: 0.870470  [   96/  306]
train() client id: f_00004-2-3 loss: 0.595430  [  128/  306]
train() client id: f_00004-2-4 loss: 0.577787  [  160/  306]
train() client id: f_00004-2-5 loss: 0.836684  [  192/  306]
train() client id: f_00004-2-6 loss: 0.655927  [  224/  306]
train() client id: f_00004-2-7 loss: 0.819513  [  256/  306]
train() client id: f_00004-2-8 loss: 0.579879  [  288/  306]
train() client id: f_00004-3-0 loss: 0.736634  [   32/  306]
train() client id: f_00004-3-1 loss: 0.656699  [   64/  306]
train() client id: f_00004-3-2 loss: 0.558424  [   96/  306]
train() client id: f_00004-3-3 loss: 0.734805  [  128/  306]
train() client id: f_00004-3-4 loss: 0.627818  [  160/  306]
train() client id: f_00004-3-5 loss: 0.690545  [  192/  306]
train() client id: f_00004-3-6 loss: 0.746575  [  224/  306]
train() client id: f_00004-3-7 loss: 0.863604  [  256/  306]
train() client id: f_00004-3-8 loss: 0.863147  [  288/  306]
train() client id: f_00004-4-0 loss: 0.635416  [   32/  306]
train() client id: f_00004-4-1 loss: 0.834580  [   64/  306]
train() client id: f_00004-4-2 loss: 0.726320  [   96/  306]
train() client id: f_00004-4-3 loss: 0.698649  [  128/  306]
train() client id: f_00004-4-4 loss: 0.693468  [  160/  306]
train() client id: f_00004-4-5 loss: 0.817439  [  192/  306]
train() client id: f_00004-4-6 loss: 0.682965  [  224/  306]
train() client id: f_00004-4-7 loss: 0.745555  [  256/  306]
train() client id: f_00004-4-8 loss: 0.709574  [  288/  306]
train() client id: f_00004-5-0 loss: 0.677813  [   32/  306]
train() client id: f_00004-5-1 loss: 0.613512  [   64/  306]
train() client id: f_00004-5-2 loss: 0.682114  [   96/  306]
train() client id: f_00004-5-3 loss: 0.798931  [  128/  306]
train() client id: f_00004-5-4 loss: 0.798923  [  160/  306]
train() client id: f_00004-5-5 loss: 0.722883  [  192/  306]
train() client id: f_00004-5-6 loss: 0.624999  [  224/  306]
train() client id: f_00004-5-7 loss: 0.758518  [  256/  306]
train() client id: f_00004-5-8 loss: 0.747253  [  288/  306]
train() client id: f_00004-6-0 loss: 0.754519  [   32/  306]
train() client id: f_00004-6-1 loss: 0.725964  [   64/  306]
train() client id: f_00004-6-2 loss: 0.859302  [   96/  306]
train() client id: f_00004-6-3 loss: 0.662741  [  128/  306]
train() client id: f_00004-6-4 loss: 0.621038  [  160/  306]
train() client id: f_00004-6-5 loss: 0.670326  [  192/  306]
train() client id: f_00004-6-6 loss: 0.716307  [  224/  306]
train() client id: f_00004-6-7 loss: 0.753800  [  256/  306]
train() client id: f_00004-6-8 loss: 0.685353  [  288/  306]
train() client id: f_00004-7-0 loss: 0.700472  [   32/  306]
train() client id: f_00004-7-1 loss: 0.755492  [   64/  306]
train() client id: f_00004-7-2 loss: 0.697816  [   96/  306]
train() client id: f_00004-7-3 loss: 0.751412  [  128/  306]
train() client id: f_00004-7-4 loss: 0.732588  [  160/  306]
train() client id: f_00004-7-5 loss: 0.673497  [  192/  306]
train() client id: f_00004-7-6 loss: 0.651389  [  224/  306]
train() client id: f_00004-7-7 loss: 0.849200  [  256/  306]
train() client id: f_00004-7-8 loss: 0.688803  [  288/  306]
train() client id: f_00004-8-0 loss: 0.793650  [   32/  306]
train() client id: f_00004-8-1 loss: 0.805869  [   64/  306]
train() client id: f_00004-8-2 loss: 0.698022  [   96/  306]
train() client id: f_00004-8-3 loss: 0.653831  [  128/  306]
train() client id: f_00004-8-4 loss: 0.766792  [  160/  306]
train() client id: f_00004-8-5 loss: 0.657182  [  192/  306]
train() client id: f_00004-8-6 loss: 0.694124  [  224/  306]
train() client id: f_00004-8-7 loss: 0.863321  [  256/  306]
train() client id: f_00004-8-8 loss: 0.716145  [  288/  306]
train() client id: f_00004-9-0 loss: 0.780833  [   32/  306]
train() client id: f_00004-9-1 loss: 0.705823  [   64/  306]
train() client id: f_00004-9-2 loss: 0.789921  [   96/  306]
train() client id: f_00004-9-3 loss: 0.742619  [  128/  306]
train() client id: f_00004-9-4 loss: 0.758294  [  160/  306]
train() client id: f_00004-9-5 loss: 0.748247  [  192/  306]
train() client id: f_00004-9-6 loss: 0.684435  [  224/  306]
train() client id: f_00004-9-7 loss: 0.726865  [  256/  306]
train() client id: f_00004-9-8 loss: 0.708863  [  288/  306]
train() client id: f_00004-10-0 loss: 0.817777  [   32/  306]
train() client id: f_00004-10-1 loss: 0.698399  [   64/  306]
train() client id: f_00004-10-2 loss: 0.727476  [   96/  306]
train() client id: f_00004-10-3 loss: 0.769882  [  128/  306]
train() client id: f_00004-10-4 loss: 0.873368  [  160/  306]
train() client id: f_00004-10-5 loss: 0.680234  [  192/  306]
train() client id: f_00004-10-6 loss: 0.596019  [  224/  306]
train() client id: f_00004-10-7 loss: 0.793044  [  256/  306]
train() client id: f_00004-10-8 loss: 0.733910  [  288/  306]
train() client id: f_00005-0-0 loss: 0.902270  [   32/  146]
train() client id: f_00005-0-1 loss: 0.921970  [   64/  146]
train() client id: f_00005-0-2 loss: 1.023671  [   96/  146]
train() client id: f_00005-0-3 loss: 0.705304  [  128/  146]
train() client id: f_00005-1-0 loss: 0.807828  [   32/  146]
train() client id: f_00005-1-1 loss: 0.914695  [   64/  146]
train() client id: f_00005-1-2 loss: 0.935994  [   96/  146]
train() client id: f_00005-1-3 loss: 1.084598  [  128/  146]
train() client id: f_00005-2-0 loss: 1.182921  [   32/  146]
train() client id: f_00005-2-1 loss: 0.746197  [   64/  146]
train() client id: f_00005-2-2 loss: 0.838850  [   96/  146]
train() client id: f_00005-2-3 loss: 0.807259  [  128/  146]
train() client id: f_00005-3-0 loss: 1.067840  [   32/  146]
train() client id: f_00005-3-1 loss: 0.776063  [   64/  146]
train() client id: f_00005-3-2 loss: 1.021802  [   96/  146]
train() client id: f_00005-3-3 loss: 0.832832  [  128/  146]
train() client id: f_00005-4-0 loss: 0.716836  [   32/  146]
train() client id: f_00005-4-1 loss: 0.845704  [   64/  146]
train() client id: f_00005-4-2 loss: 1.120029  [   96/  146]
train() client id: f_00005-4-3 loss: 1.028500  [  128/  146]
train() client id: f_00005-5-0 loss: 0.952328  [   32/  146]
train() client id: f_00005-5-1 loss: 0.836416  [   64/  146]
train() client id: f_00005-5-2 loss: 0.898966  [   96/  146]
train() client id: f_00005-5-3 loss: 0.996454  [  128/  146]
train() client id: f_00005-6-0 loss: 0.714974  [   32/  146]
train() client id: f_00005-6-1 loss: 0.888925  [   64/  146]
train() client id: f_00005-6-2 loss: 1.184424  [   96/  146]
train() client id: f_00005-6-3 loss: 0.938237  [  128/  146]
train() client id: f_00005-7-0 loss: 0.793783  [   32/  146]
train() client id: f_00005-7-1 loss: 0.844295  [   64/  146]
train() client id: f_00005-7-2 loss: 1.125693  [   96/  146]
train() client id: f_00005-7-3 loss: 0.895809  [  128/  146]
train() client id: f_00005-8-0 loss: 1.083448  [   32/  146]
train() client id: f_00005-8-1 loss: 0.964382  [   64/  146]
train() client id: f_00005-8-2 loss: 0.797375  [   96/  146]
train() client id: f_00005-8-3 loss: 0.749745  [  128/  146]
train() client id: f_00005-9-0 loss: 0.985766  [   32/  146]
train() client id: f_00005-9-1 loss: 0.867719  [   64/  146]
train() client id: f_00005-9-2 loss: 0.861512  [   96/  146]
train() client id: f_00005-9-3 loss: 1.060262  [  128/  146]
train() client id: f_00005-10-0 loss: 1.012340  [   32/  146]
train() client id: f_00005-10-1 loss: 1.030357  [   64/  146]
train() client id: f_00005-10-2 loss: 1.015779  [   96/  146]
train() client id: f_00005-10-3 loss: 0.710186  [  128/  146]
train() client id: f_00006-0-0 loss: 0.543934  [   32/   54]
train() client id: f_00006-1-0 loss: 0.524069  [   32/   54]
train() client id: f_00006-2-0 loss: 0.506181  [   32/   54]
train() client id: f_00006-3-0 loss: 0.539819  [   32/   54]
train() client id: f_00006-4-0 loss: 0.561872  [   32/   54]
train() client id: f_00006-5-0 loss: 0.504538  [   32/   54]
train() client id: f_00006-6-0 loss: 0.596609  [   32/   54]
train() client id: f_00006-7-0 loss: 0.551655  [   32/   54]
train() client id: f_00006-8-0 loss: 0.511796  [   32/   54]
train() client id: f_00006-9-0 loss: 0.514551  [   32/   54]
train() client id: f_00006-10-0 loss: 0.527534  [   32/   54]
train() client id: f_00007-0-0 loss: 0.634723  [   32/  179]
train() client id: f_00007-0-1 loss: 0.626039  [   64/  179]
train() client id: f_00007-0-2 loss: 0.689085  [   96/  179]
train() client id: f_00007-0-3 loss: 0.487778  [  128/  179]
train() client id: f_00007-0-4 loss: 0.417711  [  160/  179]
train() client id: f_00007-1-0 loss: 0.605125  [   32/  179]
train() client id: f_00007-1-1 loss: 0.487933  [   64/  179]
train() client id: f_00007-1-2 loss: 0.669886  [   96/  179]
train() client id: f_00007-1-3 loss: 0.522507  [  128/  179]
train() client id: f_00007-1-4 loss: 0.523216  [  160/  179]
train() client id: f_00007-2-0 loss: 0.459876  [   32/  179]
train() client id: f_00007-2-1 loss: 0.704843  [   64/  179]
train() client id: f_00007-2-2 loss: 0.451380  [   96/  179]
train() client id: f_00007-2-3 loss: 0.649055  [  128/  179]
train() client id: f_00007-2-4 loss: 0.543067  [  160/  179]
train() client id: f_00007-3-0 loss: 0.399391  [   32/  179]
train() client id: f_00007-3-1 loss: 0.496033  [   64/  179]
train() client id: f_00007-3-2 loss: 0.542229  [   96/  179]
train() client id: f_00007-3-3 loss: 0.667491  [  128/  179]
train() client id: f_00007-3-4 loss: 0.582828  [  160/  179]
train() client id: f_00007-4-0 loss: 0.564534  [   32/  179]
train() client id: f_00007-4-1 loss: 0.524870  [   64/  179]
train() client id: f_00007-4-2 loss: 0.613977  [   96/  179]
train() client id: f_00007-4-3 loss: 0.372911  [  128/  179]
train() client id: f_00007-4-4 loss: 0.625188  [  160/  179]
train() client id: f_00007-5-0 loss: 0.393987  [   32/  179]
train() client id: f_00007-5-1 loss: 0.465984  [   64/  179]
train() client id: f_00007-5-2 loss: 0.659379  [   96/  179]
train() client id: f_00007-5-3 loss: 0.594906  [  128/  179]
train() client id: f_00007-5-4 loss: 0.483822  [  160/  179]
train() client id: f_00007-6-0 loss: 0.553124  [   32/  179]
train() client id: f_00007-6-1 loss: 0.653027  [   64/  179]
train() client id: f_00007-6-2 loss: 0.537861  [   96/  179]
train() client id: f_00007-6-3 loss: 0.412248  [  128/  179]
train() client id: f_00007-6-4 loss: 0.483176  [  160/  179]
train() client id: f_00007-7-0 loss: 0.356841  [   32/  179]
train() client id: f_00007-7-1 loss: 0.565218  [   64/  179]
train() client id: f_00007-7-2 loss: 0.694400  [   96/  179]
train() client id: f_00007-7-3 loss: 0.606947  [  128/  179]
train() client id: f_00007-7-4 loss: 0.346515  [  160/  179]
train() client id: f_00007-8-0 loss: 0.438993  [   32/  179]
train() client id: f_00007-8-1 loss: 0.757423  [   64/  179]
train() client id: f_00007-8-2 loss: 0.426391  [   96/  179]
train() client id: f_00007-8-3 loss: 0.373372  [  128/  179]
train() client id: f_00007-8-4 loss: 0.589163  [  160/  179]
train() client id: f_00007-9-0 loss: 0.387975  [   32/  179]
train() client id: f_00007-9-1 loss: 0.637206  [   64/  179]
train() client id: f_00007-9-2 loss: 0.497628  [   96/  179]
train() client id: f_00007-9-3 loss: 0.615304  [  128/  179]
train() client id: f_00007-9-4 loss: 0.537926  [  160/  179]
train() client id: f_00007-10-0 loss: 0.618576  [   32/  179]
train() client id: f_00007-10-1 loss: 0.429759  [   64/  179]
train() client id: f_00007-10-2 loss: 0.400898  [   96/  179]
train() client id: f_00007-10-3 loss: 0.562085  [  128/  179]
train() client id: f_00007-10-4 loss: 0.507519  [  160/  179]
train() client id: f_00008-0-0 loss: 0.909278  [   32/  130]
train() client id: f_00008-0-1 loss: 0.764066  [   64/  130]
train() client id: f_00008-0-2 loss: 0.761515  [   96/  130]
train() client id: f_00008-0-3 loss: 0.842887  [  128/  130]
train() client id: f_00008-1-0 loss: 0.901814  [   32/  130]
train() client id: f_00008-1-1 loss: 0.723318  [   64/  130]
train() client id: f_00008-1-2 loss: 0.674370  [   96/  130]
train() client id: f_00008-1-3 loss: 0.946049  [  128/  130]
train() client id: f_00008-2-0 loss: 0.813153  [   32/  130]
train() client id: f_00008-2-1 loss: 0.814727  [   64/  130]
train() client id: f_00008-2-2 loss: 0.878925  [   96/  130]
train() client id: f_00008-2-3 loss: 0.764123  [  128/  130]
train() client id: f_00008-3-0 loss: 0.804434  [   32/  130]
train() client id: f_00008-3-1 loss: 0.752273  [   64/  130]
train() client id: f_00008-3-2 loss: 0.824785  [   96/  130]
train() client id: f_00008-3-3 loss: 0.894011  [  128/  130]
train() client id: f_00008-4-0 loss: 0.758413  [   32/  130]
train() client id: f_00008-4-1 loss: 0.734024  [   64/  130]
train() client id: f_00008-4-2 loss: 0.895689  [   96/  130]
train() client id: f_00008-4-3 loss: 0.890331  [  128/  130]
train() client id: f_00008-5-0 loss: 0.787693  [   32/  130]
train() client id: f_00008-5-1 loss: 0.784663  [   64/  130]
train() client id: f_00008-5-2 loss: 0.796324  [   96/  130]
train() client id: f_00008-5-3 loss: 0.856447  [  128/  130]
train() client id: f_00008-6-0 loss: 0.780082  [   32/  130]
train() client id: f_00008-6-1 loss: 0.800286  [   64/  130]
train() client id: f_00008-6-2 loss: 0.907523  [   96/  130]
train() client id: f_00008-6-3 loss: 0.788542  [  128/  130]
train() client id: f_00008-7-0 loss: 0.761987  [   32/  130]
train() client id: f_00008-7-1 loss: 0.854098  [   64/  130]
train() client id: f_00008-7-2 loss: 0.835562  [   96/  130]
train() client id: f_00008-7-3 loss: 0.828631  [  128/  130]
train() client id: f_00008-8-0 loss: 0.940268  [   32/  130]
train() client id: f_00008-8-1 loss: 0.776592  [   64/  130]
train() client id: f_00008-8-2 loss: 0.809206  [   96/  130]
train() client id: f_00008-8-3 loss: 0.744431  [  128/  130]
train() client id: f_00008-9-0 loss: 0.739579  [   32/  130]
train() client id: f_00008-9-1 loss: 0.801541  [   64/  130]
train() client id: f_00008-9-2 loss: 0.836450  [   96/  130]
train() client id: f_00008-9-3 loss: 0.892085  [  128/  130]
train() client id: f_00008-10-0 loss: 0.778132  [   32/  130]
train() client id: f_00008-10-1 loss: 0.808844  [   64/  130]
train() client id: f_00008-10-2 loss: 0.760518  [   96/  130]
train() client id: f_00008-10-3 loss: 0.900872  [  128/  130]
train() client id: f_00009-0-0 loss: 0.936514  [   32/  118]
train() client id: f_00009-0-1 loss: 0.886035  [   64/  118]
train() client id: f_00009-0-2 loss: 0.875267  [   96/  118]
train() client id: f_00009-1-0 loss: 0.749006  [   32/  118]
train() client id: f_00009-1-1 loss: 0.932915  [   64/  118]
train() client id: f_00009-1-2 loss: 0.965158  [   96/  118]
train() client id: f_00009-2-0 loss: 0.743702  [   32/  118]
train() client id: f_00009-2-1 loss: 0.805024  [   64/  118]
train() client id: f_00009-2-2 loss: 0.857384  [   96/  118]
train() client id: f_00009-3-0 loss: 0.622111  [   32/  118]
train() client id: f_00009-3-1 loss: 0.881164  [   64/  118]
train() client id: f_00009-3-2 loss: 0.909909  [   96/  118]
train() client id: f_00009-4-0 loss: 0.866492  [   32/  118]
train() client id: f_00009-4-1 loss: 0.720949  [   64/  118]
train() client id: f_00009-4-2 loss: 0.806267  [   96/  118]
train() client id: f_00009-5-0 loss: 0.818164  [   32/  118]
train() client id: f_00009-5-1 loss: 0.680430  [   64/  118]
train() client id: f_00009-5-2 loss: 0.810369  [   96/  118]
train() client id: f_00009-6-0 loss: 0.809452  [   32/  118]
train() client id: f_00009-6-1 loss: 0.639592  [   64/  118]
train() client id: f_00009-6-2 loss: 0.786824  [   96/  118]
train() client id: f_00009-7-0 loss: 0.845599  [   32/  118]
train() client id: f_00009-7-1 loss: 0.666551  [   64/  118]
train() client id: f_00009-7-2 loss: 0.724561  [   96/  118]
train() client id: f_00009-8-0 loss: 0.650908  [   32/  118]
train() client id: f_00009-8-1 loss: 0.691048  [   64/  118]
train() client id: f_00009-8-2 loss: 0.858690  [   96/  118]
train() client id: f_00009-9-0 loss: 0.866892  [   32/  118]
train() client id: f_00009-9-1 loss: 0.707760  [   64/  118]
train() client id: f_00009-9-2 loss: 0.669088  [   96/  118]
train() client id: f_00009-10-0 loss: 0.706395  [   32/  118]
train() client id: f_00009-10-1 loss: 0.628731  [   64/  118]
train() client id: f_00009-10-2 loss: 0.894614  [   96/  118]
At round 53 accuracy: 0.649867374005305
At round 53 training accuracy: 0.5935613682092555
At round 53 training loss: 0.8165471144546913
update_location
xs = -3.905658 4.200318 285.009024 18.811294 0.979296 3.956410 -247.443192 -226.324852 269.663977 -212.060879 
ys = 277.587959 260.555839 1.320614 -247.455176 239.350187 222.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 295.076820 279.118233 302.046168 267.559206 259.402141 244.257649 266.898902 247.434062 288.144634 234.490571 
dists_bs = 199.493266 198.576322 491.704008 464.791559 187.375717 185.233878 191.829745 181.582199 471.710261 174.968574 
uav_gains = -115.314060 -113.900108 -115.899930 -112.848554 -112.114880 -110.816052 -112.788663 -111.079425 -114.709818 -110.041505 
bs_gains = -103.965050 -103.909028 -114.934764 -114.250290 -103.203032 -103.063231 -103.488706 -102.821111 -114.429968 -102.369941 
Round 54
-------------------------------
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.87808648 7.89231983 3.82850312 1.39788506 9.0995415  4.37831172
 1.72181282 5.39414421 3.98347864 3.54993313]
obj_prev = 45.124016510283404
eta_min = 1.5688388831448793e-24	eta_max = 0.9379204733020944
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 10.402835079222914	eta = 0.9090909090909091
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 22.247241049856704	eta = 0.4250919373822522
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 16.022057938092566	eta = 0.5902564349620266
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 14.904186500929747	eta = 0.6345279427832983
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 14.838111741409746	eta = 0.6373535234204303
af = 9.457122799293558	bf = 1.1211163846456331	zeta = 14.83785471002873	eta = 0.6373645640903601
eta = 0.6373645640903601
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [0.03716704 0.07816878 0.03657709 0.01268399 0.09026281 0.04306657
 0.01592873 0.0528008  0.03834695 0.03480723]
ene_total = [1.43077897 2.28852743 1.44075621 0.69142509 2.60477925 1.34299583
 0.77637967 1.71811452 1.42782004 1.11627769]
ti_comp = [0.85812284 0.94697766 0.84848758 0.89012496 0.94950542 0.94998635
 0.89076325 0.90687475 0.8710298  0.95228188]
ti_coms = [0.16404911 0.07519428 0.17368437 0.13204699 0.07266652 0.0721856
 0.13140869 0.11529719 0.15114215 0.06989007]
t_total = [27.29977341 27.29977341 27.29977341 27.29977341 27.29977341 27.29977341
 27.29977341 27.29977341 27.29977341 27.29977341]
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [4.35767693e-06 3.32889784e-05 4.24831188e-06 1.60970289e-07
 5.09813926e-05 5.53179598e-06 3.18345717e-07 1.11868234e-05
 4.64522066e-06 2.90641279e-06]
ene_total = [0.4382524  0.20171506 0.46398296 0.35267029 0.19543653 0.19293825
 0.35096977 0.30823006 0.40378868 0.18673731]
optimize_network iter = 0 obj = 3.094721297443309
eta = 0.6373645640903601
freqs = [21656013.90690634 41272766.08711902 21554285.58755618  7124836.67893045
 47531488.32308537 22666941.75621849  8941057.85172046 29111404.29436724
 22012419.35744325 18275695.9412735 ]
eta_min = 0.6373645640903616	eta_max = 0.700248974656232
af = 0.0025292806273427074	bf = 1.1211163846456331	zeta = 0.002782208690076978	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [9.22110454e-07 7.04414656e-06 8.98968156e-07 3.40622741e-08
 1.07879670e-05 1.17056105e-06 6.73638541e-08 2.36719862e-06
 9.82956424e-07 6.15014298e-07]
ene_total = [1.74781835 0.80184327 1.85046646 1.40678466 0.77531226 0.76916407
 1.39998808 1.22858694 1.61031875 0.74464912]
ti_comp = [0.68086857 0.76972339 0.67123331 0.71287069 0.77225115 0.77273208
 0.71350898 0.72962048 0.69377552 0.77502761]
ti_coms = [0.16404911 0.07519428 0.17368437 0.13204699 0.07266652 0.0721856
 0.13140869 0.11529719 0.15114215 0.06989007]
t_total = [27.29977341 27.29977341 27.29977341 27.29977341 27.29977341 27.29977341
 27.29977341 27.29977341 27.29977341 27.29977341]
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [3.42697873e-06 2.49456369e-05 3.36081341e-06 1.24253947e-07
 3.81569359e-05 4.13930121e-06 2.45645457e-07 8.55640634e-06
 3.62508715e-06 2.17239112e-06]
ene_total = [0.53016278 0.24376304 0.56129271 0.42665538 0.23602256 0.23336954
 0.42459695 0.37280828 0.48846606 0.22588899]
optimize_network iter = 1 obj = 3.7430263059334705
eta = 0.700248974656232
freqs = [21591991.81873228 40169519.43771622 21554285.58755618  7037900.85535273
 46232598.93038776 22044962.3818897   8830387.52189105 28624734.01565526
 21863003.51871486 17764389.24875431]
eta_min = 0.7002489746562424	eta_max = 0.7002489746562317
af = 0.0024124103758265034	bf = 1.1211163846456331	zeta = 0.002653651413409154	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [9.16666407e-07 6.67259097e-06 8.98968156e-07 3.32361034e-08
 1.02064191e-05 1.10720219e-06 6.57065467e-08 2.28871285e-06
 9.69657495e-07 5.81082673e-07]
ene_total = [1.74781777 0.80180369 1.85046646 1.40678457 0.7752503  0.76915732
 1.3999879  1.22857858 1.61031734 0.7446455 ]
ti_comp = [0.68086857 0.76972339 0.67123331 0.71287069 0.77225115 0.77273208
 0.71350898 0.72962048 0.69377552 0.77502761]
ti_coms = [0.16404911 0.07519428 0.17368437 0.13204699 0.07266652 0.0721856
 0.13140869 0.11529719 0.15114215 0.06989007]
t_total = [27.29977341 27.29977341 27.29977341 27.29977341 27.29977341 27.29977341
 27.29977341 27.29977341 27.29977341 27.29977341]
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [3.42697873e-06 2.49456369e-05 3.36081341e-06 1.24253947e-07
 3.81569359e-05 4.13930121e-06 2.45645457e-07 8.55640634e-06
 3.62508715e-06 2.17239112e-06]
ene_total = [0.53016278 0.24376304 0.56129271 0.42665538 0.23602256 0.23336954
 0.42459695 0.37280828 0.48846606 0.22588899]
optimize_network iter = 2 obj = 3.743026305933466
eta = 0.7002489746562317
freqs = [21591991.81873227 40169519.4377162  21554285.58755618  7037900.85535273
 46232598.93038776 22044962.38188971  8830387.52189105 28624734.01565526
 21863003.51871486 17764389.24875431]
Done!
ene_coms = [0.01640491 0.00751943 0.01736844 0.0132047  0.00726665 0.00721856
 0.01314087 0.01152972 0.01511422 0.00698901]
ene_comp = [3.23086379e-06 2.35180785e-05 3.16848489e-06 1.17143295e-07
 3.59733374e-05 3.90242234e-06 2.31587959e-07 8.06675079e-06
 3.41763510e-06 2.04807218e-06]
ene_total = [0.01640814 0.00754295 0.01737161 0.01320482 0.00730263 0.00722246
 0.0131411  0.01153779 0.01511763 0.00699105]
At round 54 energy consumption: 0.11584017098855656
At round 54 eta: 0.7002489746562317
At round 54 a_n: 9.685127119911243
At round 54 local rounds: 11.66770515392171
At round 54 global rounds: 32.310572111651304
gradient difference: 0.4415777325630188
train() client id: f_00000-0-0 loss: 1.246735  [   32/  126]
train() client id: f_00000-0-1 loss: 0.968940  [   64/  126]
train() client id: f_00000-0-2 loss: 0.922896  [   96/  126]
train() client id: f_00000-1-0 loss: 1.089395  [   32/  126]
train() client id: f_00000-1-1 loss: 1.025859  [   64/  126]
train() client id: f_00000-1-2 loss: 0.892187  [   96/  126]
train() client id: f_00000-2-0 loss: 1.060304  [   32/  126]
train() client id: f_00000-2-1 loss: 0.870230  [   64/  126]
train() client id: f_00000-2-2 loss: 0.814900  [   96/  126]
train() client id: f_00000-3-0 loss: 0.879053  [   32/  126]
train() client id: f_00000-3-1 loss: 0.842739  [   64/  126]
train() client id: f_00000-3-2 loss: 0.850467  [   96/  126]
train() client id: f_00000-4-0 loss: 0.824881  [   32/  126]
train() client id: f_00000-4-1 loss: 0.919525  [   64/  126]
train() client id: f_00000-4-2 loss: 0.826715  [   96/  126]
train() client id: f_00000-5-0 loss: 0.833128  [   32/  126]
train() client id: f_00000-5-1 loss: 0.728525  [   64/  126]
train() client id: f_00000-5-2 loss: 0.788286  [   96/  126]
train() client id: f_00000-6-0 loss: 0.731280  [   32/  126]
train() client id: f_00000-6-1 loss: 0.771902  [   64/  126]
train() client id: f_00000-6-2 loss: 0.656454  [   96/  126]
train() client id: f_00000-7-0 loss: 0.723858  [   32/  126]
train() client id: f_00000-7-1 loss: 0.732207  [   64/  126]
train() client id: f_00000-7-2 loss: 0.791626  [   96/  126]
train() client id: f_00000-8-0 loss: 0.777906  [   32/  126]
train() client id: f_00000-8-1 loss: 0.752404  [   64/  126]
train() client id: f_00000-8-2 loss: 0.687686  [   96/  126]
train() client id: f_00000-9-0 loss: 0.667469  [   32/  126]
train() client id: f_00000-9-1 loss: 0.738936  [   64/  126]
train() client id: f_00000-9-2 loss: 0.693603  [   96/  126]
train() client id: f_00000-10-0 loss: 0.623135  [   32/  126]
train() client id: f_00000-10-1 loss: 0.727183  [   64/  126]
train() client id: f_00000-10-2 loss: 0.648803  [   96/  126]
train() client id: f_00001-0-0 loss: 0.470603  [   32/  265]
train() client id: f_00001-0-1 loss: 0.390911  [   64/  265]
train() client id: f_00001-0-2 loss: 0.448572  [   96/  265]
train() client id: f_00001-0-3 loss: 0.345423  [  128/  265]
train() client id: f_00001-0-4 loss: 0.306043  [  160/  265]
train() client id: f_00001-0-5 loss: 0.421527  [  192/  265]
train() client id: f_00001-0-6 loss: 0.341620  [  224/  265]
train() client id: f_00001-0-7 loss: 0.423716  [  256/  265]
train() client id: f_00001-1-0 loss: 0.375366  [   32/  265]
train() client id: f_00001-1-1 loss: 0.596833  [   64/  265]
train() client id: f_00001-1-2 loss: 0.327607  [   96/  265]
train() client id: f_00001-1-3 loss: 0.309070  [  128/  265]
train() client id: f_00001-1-4 loss: 0.363795  [  160/  265]
train() client id: f_00001-1-5 loss: 0.336021  [  192/  265]
train() client id: f_00001-1-6 loss: 0.323646  [  224/  265]
train() client id: f_00001-1-7 loss: 0.352063  [  256/  265]
train() client id: f_00001-2-0 loss: 0.323870  [   32/  265]
train() client id: f_00001-2-1 loss: 0.383651  [   64/  265]
train() client id: f_00001-2-2 loss: 0.346234  [   96/  265]
train() client id: f_00001-2-3 loss: 0.544267  [  128/  265]
train() client id: f_00001-2-4 loss: 0.397330  [  160/  265]
train() client id: f_00001-2-5 loss: 0.366901  [  192/  265]
train() client id: f_00001-2-6 loss: 0.305105  [  224/  265]
train() client id: f_00001-2-7 loss: 0.262156  [  256/  265]
train() client id: f_00001-3-0 loss: 0.398096  [   32/  265]
train() client id: f_00001-3-1 loss: 0.365584  [   64/  265]
train() client id: f_00001-3-2 loss: 0.357189  [   96/  265]
train() client id: f_00001-3-3 loss: 0.327687  [  128/  265]
train() client id: f_00001-3-4 loss: 0.320859  [  160/  265]
train() client id: f_00001-3-5 loss: 0.481542  [  192/  265]
train() client id: f_00001-3-6 loss: 0.403005  [  224/  265]
train() client id: f_00001-3-7 loss: 0.295202  [  256/  265]
train() client id: f_00001-4-0 loss: 0.472853  [   32/  265]
train() client id: f_00001-4-1 loss: 0.401900  [   64/  265]
train() client id: f_00001-4-2 loss: 0.271637  [   96/  265]
train() client id: f_00001-4-3 loss: 0.329214  [  128/  265]
train() client id: f_00001-4-4 loss: 0.335118  [  160/  265]
train() client id: f_00001-4-5 loss: 0.326861  [  192/  265]
train() client id: f_00001-4-6 loss: 0.305591  [  224/  265]
train() client id: f_00001-4-7 loss: 0.471116  [  256/  265]
train() client id: f_00001-5-0 loss: 0.331005  [   32/  265]
train() client id: f_00001-5-1 loss: 0.409076  [   64/  265]
train() client id: f_00001-5-2 loss: 0.403653  [   96/  265]
train() client id: f_00001-5-3 loss: 0.285268  [  128/  265]
train() client id: f_00001-5-4 loss: 0.450443  [  160/  265]
train() client id: f_00001-5-5 loss: 0.415837  [  192/  265]
train() client id: f_00001-5-6 loss: 0.308137  [  224/  265]
train() client id: f_00001-5-7 loss: 0.274336  [  256/  265]
train() client id: f_00001-6-0 loss: 0.322403  [   32/  265]
train() client id: f_00001-6-1 loss: 0.344847  [   64/  265]
train() client id: f_00001-6-2 loss: 0.327918  [   96/  265]
train() client id: f_00001-6-3 loss: 0.354163  [  128/  265]
train() client id: f_00001-6-4 loss: 0.389431  [  160/  265]
train() client id: f_00001-6-5 loss: 0.307066  [  192/  265]
train() client id: f_00001-6-6 loss: 0.464643  [  224/  265]
train() client id: f_00001-6-7 loss: 0.341573  [  256/  265]
train() client id: f_00001-7-0 loss: 0.299297  [   32/  265]
train() client id: f_00001-7-1 loss: 0.385941  [   64/  265]
train() client id: f_00001-7-2 loss: 0.273178  [   96/  265]
train() client id: f_00001-7-3 loss: 0.463407  [  128/  265]
train() client id: f_00001-7-4 loss: 0.248285  [  160/  265]
train() client id: f_00001-7-5 loss: 0.372877  [  192/  265]
train() client id: f_00001-7-6 loss: 0.353178  [  224/  265]
train() client id: f_00001-7-7 loss: 0.430303  [  256/  265]
train() client id: f_00001-8-0 loss: 0.338214  [   32/  265]
train() client id: f_00001-8-1 loss: 0.321153  [   64/  265]
train() client id: f_00001-8-2 loss: 0.287278  [   96/  265]
train() client id: f_00001-8-3 loss: 0.357314  [  128/  265]
train() client id: f_00001-8-4 loss: 0.384496  [  160/  265]
train() client id: f_00001-8-5 loss: 0.273312  [  192/  265]
train() client id: f_00001-8-6 loss: 0.434535  [  224/  265]
train() client id: f_00001-8-7 loss: 0.420313  [  256/  265]
train() client id: f_00001-9-0 loss: 0.383043  [   32/  265]
train() client id: f_00001-9-1 loss: 0.268198  [   64/  265]
train() client id: f_00001-9-2 loss: 0.237165  [   96/  265]
train() client id: f_00001-9-3 loss: 0.255503  [  128/  265]
train() client id: f_00001-9-4 loss: 0.458025  [  160/  265]
train() client id: f_00001-9-5 loss: 0.326550  [  192/  265]
train() client id: f_00001-9-6 loss: 0.445140  [  224/  265]
train() client id: f_00001-9-7 loss: 0.417223  [  256/  265]
train() client id: f_00001-10-0 loss: 0.337966  [   32/  265]
train() client id: f_00001-10-1 loss: 0.313537  [   64/  265]
train() client id: f_00001-10-2 loss: 0.472402  [   96/  265]
train() client id: f_00001-10-3 loss: 0.317127  [  128/  265]
train() client id: f_00001-10-4 loss: 0.239279  [  160/  265]
train() client id: f_00001-10-5 loss: 0.406177  [  192/  265]
train() client id: f_00001-10-6 loss: 0.355059  [  224/  265]
train() client id: f_00001-10-7 loss: 0.348287  [  256/  265]
train() client id: f_00002-0-0 loss: 1.227051  [   32/  124]
train() client id: f_00002-0-1 loss: 1.095656  [   64/  124]
train() client id: f_00002-0-2 loss: 1.091742  [   96/  124]
train() client id: f_00002-1-0 loss: 1.062115  [   32/  124]
train() client id: f_00002-1-1 loss: 1.168139  [   64/  124]
train() client id: f_00002-1-2 loss: 1.088958  [   96/  124]
train() client id: f_00002-2-0 loss: 1.137442  [   32/  124]
train() client id: f_00002-2-1 loss: 1.121794  [   64/  124]
train() client id: f_00002-2-2 loss: 0.867417  [   96/  124]
train() client id: f_00002-3-0 loss: 0.972062  [   32/  124]
train() client id: f_00002-3-1 loss: 0.986188  [   64/  124]
train() client id: f_00002-3-2 loss: 1.030258  [   96/  124]
train() client id: f_00002-4-0 loss: 0.929811  [   32/  124]
train() client id: f_00002-4-1 loss: 0.978878  [   64/  124]
train() client id: f_00002-4-2 loss: 1.139006  [   96/  124]
train() client id: f_00002-5-0 loss: 1.048784  [   32/  124]
train() client id: f_00002-5-1 loss: 1.007188  [   64/  124]
train() client id: f_00002-5-2 loss: 0.888556  [   96/  124]
train() client id: f_00002-6-0 loss: 0.884524  [   32/  124]
train() client id: f_00002-6-1 loss: 1.022617  [   64/  124]
train() client id: f_00002-6-2 loss: 0.893365  [   96/  124]
train() client id: f_00002-7-0 loss: 0.862987  [   32/  124]
train() client id: f_00002-7-1 loss: 0.932418  [   64/  124]
train() client id: f_00002-7-2 loss: 1.050041  [   96/  124]
train() client id: f_00002-8-0 loss: 1.022087  [   32/  124]
train() client id: f_00002-8-1 loss: 0.763656  [   64/  124]
train() client id: f_00002-8-2 loss: 1.016531  [   96/  124]
train() client id: f_00002-9-0 loss: 1.151080  [   32/  124]
train() client id: f_00002-9-1 loss: 0.979276  [   64/  124]
train() client id: f_00002-9-2 loss: 0.805856  [   96/  124]
train() client id: f_00002-10-0 loss: 0.855373  [   32/  124]
train() client id: f_00002-10-1 loss: 1.067202  [   64/  124]
train() client id: f_00002-10-2 loss: 0.870295  [   96/  124]
train() client id: f_00003-0-0 loss: 0.638237  [   32/   43]
train() client id: f_00003-1-0 loss: 0.839092  [   32/   43]
train() client id: f_00003-2-0 loss: 0.534116  [   32/   43]
train() client id: f_00003-3-0 loss: 0.662197  [   32/   43]
train() client id: f_00003-4-0 loss: 0.785684  [   32/   43]
train() client id: f_00003-5-0 loss: 0.584944  [   32/   43]
train() client id: f_00003-6-0 loss: 0.862257  [   32/   43]
train() client id: f_00003-7-0 loss: 0.561461  [   32/   43]
train() client id: f_00003-8-0 loss: 0.688026  [   32/   43]
train() client id: f_00003-9-0 loss: 0.531649  [   32/   43]
train() client id: f_00003-10-0 loss: 0.735800  [   32/   43]
train() client id: f_00004-0-0 loss: 0.923113  [   32/  306]
train() client id: f_00004-0-1 loss: 0.767687  [   64/  306]
train() client id: f_00004-0-2 loss: 0.772439  [   96/  306]
train() client id: f_00004-0-3 loss: 0.715732  [  128/  306]
train() client id: f_00004-0-4 loss: 0.841527  [  160/  306]
train() client id: f_00004-0-5 loss: 0.827896  [  192/  306]
train() client id: f_00004-0-6 loss: 0.841753  [  224/  306]
train() client id: f_00004-0-7 loss: 1.031199  [  256/  306]
train() client id: f_00004-0-8 loss: 0.815055  [  288/  306]
train() client id: f_00004-1-0 loss: 0.815009  [   32/  306]
train() client id: f_00004-1-1 loss: 0.861738  [   64/  306]
train() client id: f_00004-1-2 loss: 0.820343  [   96/  306]
train() client id: f_00004-1-3 loss: 0.986051  [  128/  306]
train() client id: f_00004-1-4 loss: 0.813486  [  160/  306]
train() client id: f_00004-1-5 loss: 0.653100  [  192/  306]
train() client id: f_00004-1-6 loss: 0.920645  [  224/  306]
train() client id: f_00004-1-7 loss: 0.837523  [  256/  306]
train() client id: f_00004-1-8 loss: 0.898107  [  288/  306]
train() client id: f_00004-2-0 loss: 0.768409  [   32/  306]
train() client id: f_00004-2-1 loss: 0.751644  [   64/  306]
train() client id: f_00004-2-2 loss: 0.774057  [   96/  306]
train() client id: f_00004-2-3 loss: 0.985238  [  128/  306]
train() client id: f_00004-2-4 loss: 0.761534  [  160/  306]
train() client id: f_00004-2-5 loss: 0.930858  [  192/  306]
train() client id: f_00004-2-6 loss: 0.831833  [  224/  306]
train() client id: f_00004-2-7 loss: 0.860128  [  256/  306]
train() client id: f_00004-2-8 loss: 0.927835  [  288/  306]
train() client id: f_00004-3-0 loss: 0.922999  [   32/  306]
train() client id: f_00004-3-1 loss: 0.702079  [   64/  306]
train() client id: f_00004-3-2 loss: 0.800164  [   96/  306]
train() client id: f_00004-3-3 loss: 0.778863  [  128/  306]
train() client id: f_00004-3-4 loss: 0.808791  [  160/  306]
train() client id: f_00004-3-5 loss: 1.034404  [  192/  306]
train() client id: f_00004-3-6 loss: 0.873818  [  224/  306]
train() client id: f_00004-3-7 loss: 0.767972  [  256/  306]
train() client id: f_00004-3-8 loss: 0.835569  [  288/  306]
train() client id: f_00004-4-0 loss: 0.934378  [   32/  306]
train() client id: f_00004-4-1 loss: 0.911873  [   64/  306]
train() client id: f_00004-4-2 loss: 0.728615  [   96/  306]
train() client id: f_00004-4-3 loss: 0.779958  [  128/  306]
train() client id: f_00004-4-4 loss: 0.887908  [  160/  306]
train() client id: f_00004-4-5 loss: 0.810516  [  192/  306]
train() client id: f_00004-4-6 loss: 0.786906  [  224/  306]
train() client id: f_00004-4-7 loss: 0.761152  [  256/  306]
train() client id: f_00004-4-8 loss: 0.880288  [  288/  306]
train() client id: f_00004-5-0 loss: 0.932120  [   32/  306]
train() client id: f_00004-5-1 loss: 0.824521  [   64/  306]
train() client id: f_00004-5-2 loss: 0.855587  [   96/  306]
train() client id: f_00004-5-3 loss: 0.707220  [  128/  306]
train() client id: f_00004-5-4 loss: 0.800417  [  160/  306]
train() client id: f_00004-5-5 loss: 0.762994  [  192/  306]
train() client id: f_00004-5-6 loss: 0.864573  [  224/  306]
train() client id: f_00004-5-7 loss: 0.889119  [  256/  306]
train() client id: f_00004-5-8 loss: 0.903526  [  288/  306]
train() client id: f_00004-6-0 loss: 0.840361  [   32/  306]
train() client id: f_00004-6-1 loss: 0.793743  [   64/  306]
train() client id: f_00004-6-2 loss: 0.836659  [   96/  306]
train() client id: f_00004-6-3 loss: 0.875554  [  128/  306]
train() client id: f_00004-6-4 loss: 0.887202  [  160/  306]
train() client id: f_00004-6-5 loss: 0.821009  [  192/  306]
train() client id: f_00004-6-6 loss: 0.836841  [  224/  306]
train() client id: f_00004-6-7 loss: 0.801497  [  256/  306]
train() client id: f_00004-6-8 loss: 0.785879  [  288/  306]
train() client id: f_00004-7-0 loss: 0.823375  [   32/  306]
train() client id: f_00004-7-1 loss: 0.870325  [   64/  306]
train() client id: f_00004-7-2 loss: 0.751202  [   96/  306]
train() client id: f_00004-7-3 loss: 0.898085  [  128/  306]
train() client id: f_00004-7-4 loss: 0.824778  [  160/  306]
train() client id: f_00004-7-5 loss: 0.839402  [  192/  306]
train() client id: f_00004-7-6 loss: 0.844345  [  224/  306]
train() client id: f_00004-7-7 loss: 0.926157  [  256/  306]
train() client id: f_00004-7-8 loss: 0.768890  [  288/  306]
train() client id: f_00004-8-0 loss: 0.741835  [   32/  306]
train() client id: f_00004-8-1 loss: 0.856759  [   64/  306]
train() client id: f_00004-8-2 loss: 0.806957  [   96/  306]
train() client id: f_00004-8-3 loss: 0.749846  [  128/  306]
train() client id: f_00004-8-4 loss: 0.811204  [  160/  306]
train() client id: f_00004-8-5 loss: 0.953437  [  192/  306]
train() client id: f_00004-8-6 loss: 0.796394  [  224/  306]
train() client id: f_00004-8-7 loss: 0.931599  [  256/  306]
train() client id: f_00004-8-8 loss: 0.874921  [  288/  306]
train() client id: f_00004-9-0 loss: 0.816648  [   32/  306]
train() client id: f_00004-9-1 loss: 0.806227  [   64/  306]
train() client id: f_00004-9-2 loss: 0.829915  [   96/  306]
train() client id: f_00004-9-3 loss: 0.937029  [  128/  306]
train() client id: f_00004-9-4 loss: 0.788011  [  160/  306]
train() client id: f_00004-9-5 loss: 0.923781  [  192/  306]
train() client id: f_00004-9-6 loss: 0.765739  [  224/  306]
train() client id: f_00004-9-7 loss: 0.770628  [  256/  306]
train() client id: f_00004-9-8 loss: 0.875952  [  288/  306]
train() client id: f_00004-10-0 loss: 0.833109  [   32/  306]
train() client id: f_00004-10-1 loss: 0.742786  [   64/  306]
train() client id: f_00004-10-2 loss: 0.887392  [   96/  306]
train() client id: f_00004-10-3 loss: 0.903919  [  128/  306]
train() client id: f_00004-10-4 loss: 0.771740  [  160/  306]
train() client id: f_00004-10-5 loss: 0.811549  [  192/  306]
train() client id: f_00004-10-6 loss: 0.893697  [  224/  306]
train() client id: f_00004-10-7 loss: 0.861337  [  256/  306]
train() client id: f_00004-10-8 loss: 0.782880  [  288/  306]
train() client id: f_00005-0-0 loss: 0.642752  [   32/  146]
train() client id: f_00005-0-1 loss: 0.938244  [   64/  146]
train() client id: f_00005-0-2 loss: 0.772231  [   96/  146]
train() client id: f_00005-0-3 loss: 0.967207  [  128/  146]
train() client id: f_00005-1-0 loss: 0.739831  [   32/  146]
train() client id: f_00005-1-1 loss: 0.725972  [   64/  146]
train() client id: f_00005-1-2 loss: 0.940517  [   96/  146]
train() client id: f_00005-1-3 loss: 0.814769  [  128/  146]
train() client id: f_00005-2-0 loss: 0.915471  [   32/  146]
train() client id: f_00005-2-1 loss: 0.600066  [   64/  146]
train() client id: f_00005-2-2 loss: 0.768475  [   96/  146]
train() client id: f_00005-2-3 loss: 1.064140  [  128/  146]
train() client id: f_00005-3-0 loss: 0.829830  [   32/  146]
train() client id: f_00005-3-1 loss: 1.129088  [   64/  146]
train() client id: f_00005-3-2 loss: 0.581341  [   96/  146]
train() client id: f_00005-3-3 loss: 0.734713  [  128/  146]
train() client id: f_00005-4-0 loss: 0.635728  [   32/  146]
train() client id: f_00005-4-1 loss: 1.032668  [   64/  146]
train() client id: f_00005-4-2 loss: 0.769717  [   96/  146]
train() client id: f_00005-4-3 loss: 0.906230  [  128/  146]
train() client id: f_00005-5-0 loss: 0.685605  [   32/  146]
train() client id: f_00005-5-1 loss: 0.772580  [   64/  146]
train() client id: f_00005-5-2 loss: 0.729577  [   96/  146]
train() client id: f_00005-5-3 loss: 0.916422  [  128/  146]
train() client id: f_00005-6-0 loss: 0.711334  [   32/  146]
train() client id: f_00005-6-1 loss: 0.606656  [   64/  146]
train() client id: f_00005-6-2 loss: 0.870956  [   96/  146]
train() client id: f_00005-6-3 loss: 1.176912  [  128/  146]
train() client id: f_00005-7-0 loss: 0.756886  [   32/  146]
train() client id: f_00005-7-1 loss: 1.160032  [   64/  146]
train() client id: f_00005-7-2 loss: 0.691699  [   96/  146]
train() client id: f_00005-7-3 loss: 0.576294  [  128/  146]
train() client id: f_00005-8-0 loss: 0.809534  [   32/  146]
train() client id: f_00005-8-1 loss: 0.774626  [   64/  146]
train() client id: f_00005-8-2 loss: 0.934308  [   96/  146]
train() client id: f_00005-8-3 loss: 0.903224  [  128/  146]
train() client id: f_00005-9-0 loss: 1.007681  [   32/  146]
train() client id: f_00005-9-1 loss: 0.531661  [   64/  146]
train() client id: f_00005-9-2 loss: 0.666829  [   96/  146]
train() client id: f_00005-9-3 loss: 0.905784  [  128/  146]
train() client id: f_00005-10-0 loss: 0.675606  [   32/  146]
train() client id: f_00005-10-1 loss: 0.772206  [   64/  146]
train() client id: f_00005-10-2 loss: 0.875090  [   96/  146]
train() client id: f_00005-10-3 loss: 0.854343  [  128/  146]
train() client id: f_00006-0-0 loss: 0.487190  [   32/   54]
train() client id: f_00006-1-0 loss: 0.539317  [   32/   54]
train() client id: f_00006-2-0 loss: 0.567843  [   32/   54]
train() client id: f_00006-3-0 loss: 0.519975  [   32/   54]
train() client id: f_00006-4-0 loss: 0.556707  [   32/   54]
train() client id: f_00006-5-0 loss: 0.548491  [   32/   54]
train() client id: f_00006-6-0 loss: 0.514198  [   32/   54]
train() client id: f_00006-7-0 loss: 0.503229  [   32/   54]
train() client id: f_00006-8-0 loss: 0.515406  [   32/   54]
train() client id: f_00006-9-0 loss: 0.451506  [   32/   54]
train() client id: f_00006-10-0 loss: 0.516736  [   32/   54]
train() client id: f_00007-0-0 loss: 0.608301  [   32/  179]
train() client id: f_00007-0-1 loss: 0.419968  [   64/  179]
train() client id: f_00007-0-2 loss: 0.603237  [   96/  179]
train() client id: f_00007-0-3 loss: 0.493684  [  128/  179]
train() client id: f_00007-0-4 loss: 0.706811  [  160/  179]
train() client id: f_00007-1-0 loss: 0.448138  [   32/  179]
train() client id: f_00007-1-1 loss: 0.521058  [   64/  179]
train() client id: f_00007-1-2 loss: 0.651019  [   96/  179]
train() client id: f_00007-1-3 loss: 0.440537  [  128/  179]
train() client id: f_00007-1-4 loss: 0.702061  [  160/  179]
train() client id: f_00007-2-0 loss: 0.460825  [   32/  179]
train() client id: f_00007-2-1 loss: 0.392983  [   64/  179]
train() client id: f_00007-2-2 loss: 0.709854  [   96/  179]
train() client id: f_00007-2-3 loss: 0.585872  [  128/  179]
train() client id: f_00007-2-4 loss: 0.643302  [  160/  179]
train() client id: f_00007-3-0 loss: 0.379326  [   32/  179]
train() client id: f_00007-3-1 loss: 0.436886  [   64/  179]
train() client id: f_00007-3-2 loss: 0.623696  [   96/  179]
train() client id: f_00007-3-3 loss: 0.697281  [  128/  179]
train() client id: f_00007-3-4 loss: 0.567039  [  160/  179]
train() client id: f_00007-4-0 loss: 0.419075  [   32/  179]
train() client id: f_00007-4-1 loss: 0.567584  [   64/  179]
train() client id: f_00007-4-2 loss: 0.663349  [   96/  179]
train() client id: f_00007-4-3 loss: 0.326811  [  128/  179]
train() client id: f_00007-4-4 loss: 0.649187  [  160/  179]
train() client id: f_00007-5-0 loss: 0.453336  [   32/  179]
train() client id: f_00007-5-1 loss: 0.624303  [   64/  179]
train() client id: f_00007-5-2 loss: 0.551076  [   96/  179]
train() client id: f_00007-5-3 loss: 0.555682  [  128/  179]
train() client id: f_00007-5-4 loss: 0.486664  [  160/  179]
train() client id: f_00007-6-0 loss: 0.381990  [   32/  179]
train() client id: f_00007-6-1 loss: 0.548288  [   64/  179]
train() client id: f_00007-6-2 loss: 0.669664  [   96/  179]
train() client id: f_00007-6-3 loss: 0.382017  [  128/  179]
train() client id: f_00007-6-4 loss: 0.519366  [  160/  179]
train() client id: f_00007-7-0 loss: 0.451988  [   32/  179]
train() client id: f_00007-7-1 loss: 0.723575  [   64/  179]
train() client id: f_00007-7-2 loss: 0.489703  [   96/  179]
train() client id: f_00007-7-3 loss: 0.467709  [  128/  179]
train() client id: f_00007-7-4 loss: 0.465420  [  160/  179]
train() client id: f_00007-8-0 loss: 0.565715  [   32/  179]
train() client id: f_00007-8-1 loss: 0.845998  [   64/  179]
train() client id: f_00007-8-2 loss: 0.337275  [   96/  179]
train() client id: f_00007-8-3 loss: 0.385501  [  128/  179]
train() client id: f_00007-8-4 loss: 0.538116  [  160/  179]
train() client id: f_00007-9-0 loss: 0.532426  [   32/  179]
train() client id: f_00007-9-1 loss: 0.567568  [   64/  179]
train() client id: f_00007-9-2 loss: 0.717193  [   96/  179]
train() client id: f_00007-9-3 loss: 0.360972  [  128/  179]
train() client id: f_00007-9-4 loss: 0.368181  [  160/  179]
train() client id: f_00007-10-0 loss: 0.463511  [   32/  179]
train() client id: f_00007-10-1 loss: 0.564638  [   64/  179]
train() client id: f_00007-10-2 loss: 0.392527  [   96/  179]
train() client id: f_00007-10-3 loss: 0.724488  [  128/  179]
train() client id: f_00007-10-4 loss: 0.360784  [  160/  179]
train() client id: f_00008-0-0 loss: 0.630374  [   32/  130]
train() client id: f_00008-0-1 loss: 0.627968  [   64/  130]
train() client id: f_00008-0-2 loss: 0.718798  [   96/  130]
train() client id: f_00008-0-3 loss: 0.542871  [  128/  130]
train() client id: f_00008-1-0 loss: 0.529974  [   32/  130]
train() client id: f_00008-1-1 loss: 0.650181  [   64/  130]
train() client id: f_00008-1-2 loss: 0.624462  [   96/  130]
train() client id: f_00008-1-3 loss: 0.736749  [  128/  130]
train() client id: f_00008-2-0 loss: 0.673556  [   32/  130]
train() client id: f_00008-2-1 loss: 0.622897  [   64/  130]
train() client id: f_00008-2-2 loss: 0.736098  [   96/  130]
train() client id: f_00008-2-3 loss: 0.526142  [  128/  130]
train() client id: f_00008-3-0 loss: 0.670906  [   32/  130]
train() client id: f_00008-3-1 loss: 0.653548  [   64/  130]
train() client id: f_00008-3-2 loss: 0.559894  [   96/  130]
train() client id: f_00008-3-3 loss: 0.663909  [  128/  130]
train() client id: f_00008-4-0 loss: 0.601302  [   32/  130]
train() client id: f_00008-4-1 loss: 0.670779  [   64/  130]
train() client id: f_00008-4-2 loss: 0.499432  [   96/  130]
train() client id: f_00008-4-3 loss: 0.752457  [  128/  130]
train() client id: f_00008-5-0 loss: 0.568756  [   32/  130]
train() client id: f_00008-5-1 loss: 0.652876  [   64/  130]
train() client id: f_00008-5-2 loss: 0.707565  [   96/  130]
train() client id: f_00008-5-3 loss: 0.595044  [  128/  130]
train() client id: f_00008-6-0 loss: 0.703579  [   32/  130]
train() client id: f_00008-6-1 loss: 0.540484  [   64/  130]
train() client id: f_00008-6-2 loss: 0.680964  [   96/  130]
train() client id: f_00008-6-3 loss: 0.578098  [  128/  130]
train() client id: f_00008-7-0 loss: 0.551404  [   32/  130]
train() client id: f_00008-7-1 loss: 0.694897  [   64/  130]
train() client id: f_00008-7-2 loss: 0.713944  [   96/  130]
train() client id: f_00008-7-3 loss: 0.600526  [  128/  130]
train() client id: f_00008-8-0 loss: 0.565846  [   32/  130]
train() client id: f_00008-8-1 loss: 0.691813  [   64/  130]
train() client id: f_00008-8-2 loss: 0.616639  [   96/  130]
train() client id: f_00008-8-3 loss: 0.668267  [  128/  130]
train() client id: f_00008-9-0 loss: 0.541298  [   32/  130]
train() client id: f_00008-9-1 loss: 0.564779  [   64/  130]
train() client id: f_00008-9-2 loss: 0.820948  [   96/  130]
train() client id: f_00008-9-3 loss: 0.634302  [  128/  130]
train() client id: f_00008-10-0 loss: 0.503266  [   32/  130]
train() client id: f_00008-10-1 loss: 0.651392  [   64/  130]
train() client id: f_00008-10-2 loss: 0.778838  [   96/  130]
train() client id: f_00008-10-3 loss: 0.631555  [  128/  130]
train() client id: f_00009-0-0 loss: 1.036431  [   32/  118]
train() client id: f_00009-0-1 loss: 0.778338  [   64/  118]
train() client id: f_00009-0-2 loss: 0.900009  [   96/  118]
train() client id: f_00009-1-0 loss: 0.835041  [   32/  118]
train() client id: f_00009-1-1 loss: 0.813051  [   64/  118]
train() client id: f_00009-1-2 loss: 0.979731  [   96/  118]
train() client id: f_00009-2-0 loss: 0.816851  [   32/  118]
train() client id: f_00009-2-1 loss: 0.821907  [   64/  118]
train() client id: f_00009-2-2 loss: 0.838504  [   96/  118]
train() client id: f_00009-3-0 loss: 0.764724  [   32/  118]
train() client id: f_00009-3-1 loss: 0.796339  [   64/  118]
train() client id: f_00009-3-2 loss: 0.756387  [   96/  118]
train() client id: f_00009-4-0 loss: 0.614121  [   32/  118]
train() client id: f_00009-4-1 loss: 0.649176  [   64/  118]
train() client id: f_00009-4-2 loss: 0.820959  [   96/  118]
train() client id: f_00009-5-0 loss: 0.627689  [   32/  118]
train() client id: f_00009-5-1 loss: 0.763910  [   64/  118]
train() client id: f_00009-5-2 loss: 0.772958  [   96/  118]
train() client id: f_00009-6-0 loss: 0.763573  [   32/  118]
train() client id: f_00009-6-1 loss: 0.621777  [   64/  118]
train() client id: f_00009-6-2 loss: 0.721961  [   96/  118]
train() client id: f_00009-7-0 loss: 0.688805  [   32/  118]
train() client id: f_00009-7-1 loss: 0.645836  [   64/  118]
train() client id: f_00009-7-2 loss: 0.716050  [   96/  118]
train() client id: f_00009-8-0 loss: 0.691351  [   32/  118]
train() client id: f_00009-8-1 loss: 0.560876  [   64/  118]
train() client id: f_00009-8-2 loss: 0.814444  [   96/  118]
train() client id: f_00009-9-0 loss: 0.828546  [   32/  118]
train() client id: f_00009-9-1 loss: 0.606674  [   64/  118]
train() client id: f_00009-9-2 loss: 0.645388  [   96/  118]
train() client id: f_00009-10-0 loss: 0.752152  [   32/  118]
train() client id: f_00009-10-1 loss: 0.641790  [   64/  118]
train() client id: f_00009-10-2 loss: 0.715233  [   96/  118]
At round 54 accuracy: 0.649867374005305
At round 54 training accuracy: 0.5895372233400402
At round 54 training loss: 0.8300876798558371
update_location
xs = -3.905658 4.200318 290.009024 18.811294 0.979296 3.956410 -252.443192 -231.324852 274.663977 -217.060879 
ys = 282.587959 265.555839 1.320614 -252.455176 244.350187 227.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 299.785272 283.791378 306.768607 272.190155 264.022675 248.827130 271.540891 252.015601 292.829251 239.021833 
dists_bs = 202.109977 200.781259 496.384852 469.340756 189.151159 186.587061 193.773277 183.058306 476.426500 176.095459 
uav_gains = -115.712523 -114.321804 -116.282487 -113.269820 -112.528631 -111.196598 -113.210677 -111.468386 -115.120264 -110.394064 
bs_gains = -104.123516 -104.043308 -115.049978 -114.368731 -103.317711 -103.151742 -103.611288 -102.919563 -114.550945 -102.448008 
Round 55
-------------------------------
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.74701293 7.6136546  3.69953246 1.35283391 8.77808353 4.22374522
 1.66527442 5.20651362 3.84415712 3.42461541]
obj_prev = 43.55542321522954
eta_min = 2.2364048538474024e-25	eta_max = 0.9376658335407816
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 10.034905168858742	eta = 0.909090909090909
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 21.758383945021716	eta = 0.4192701574551497
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 15.56372063459429	eta = 0.5861478290943805
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 14.453751471416155	eta = 0.6311607806900414
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 14.387691546337155	eta = 0.6340587044987988
af = 9.122641062598856	bf = 1.1085543441262717	zeta = 14.387430378338086	eta = 0.6340702142568856
eta = 0.6340702142568856
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [0.03759466 0.07906812 0.03699791 0.01282992 0.09130131 0.04356206
 0.016112   0.05340828 0.03878814 0.03520769]
ene_total = [1.39513647 2.21195089 1.40544566 0.67656761 2.51756531 1.29725943
 0.75865034 1.66646122 1.38042471 1.07796874]
ti_comp = [0.89681784 0.99162304 0.88681254 0.93061922 0.99425194 0.99482833
 0.93128983 0.94868174 0.91451495 0.99717632]
ti_coms = [0.17049984 0.07569464 0.18050515 0.13669846 0.07306574 0.07248935
 0.13602786 0.11863595 0.15280274 0.07014136]
t_total = [27.24976921 27.24976921 27.24976921 27.24976921 27.24976921 27.24976921
 27.24976921 27.24976921 27.24976921 27.24976921]
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [4.12904738e-06 3.14189100e-05 4.02483533e-06 1.52408097e-07
 4.81191659e-05 5.22046259e-06 3.01410426e-07 1.05794898e-05
 4.36109102e-06 2.74314532e-06]
ene_total = [0.43540987 0.19405873 0.46095183 0.34900978 0.18777323 0.1852064
 0.34730146 0.30316045 0.39023318 0.17914848]
optimize_network iter = 0 obj = 3.032253401302107
eta = 0.6340702142568856
freqs = [20960028.75390409 39868035.25301737 20860054.05324307  6893217.87674232
 45914572.57827595 21894260.42256957  8650365.72831919 28148681.6418485
 21206945.13364052 17653694.93957705]
eta_min = 0.6340702142568873	eta_max = 0.7017829950267004
af = 0.0022772242984259367	bf = 1.1085543441262717	zeta = 0.0025049467282685306	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [8.63792940e-07 6.57280727e-06 8.41991876e-07 3.18836346e-08
 1.00664856e-05 1.09211601e-06 6.30547859e-08 2.21321959e-06
 9.12336258e-07 5.73863496e-07]
ene_total = [1.75228827 0.77857718 1.85510894 1.40483142 0.75191939 0.74507368
 1.39794292 1.21943002 1.57042295 0.72089048]
ti_comp = [0.69931807 0.79412327 0.68931277 0.73311945 0.79675218 0.79732856
 0.73379006 0.75118197 0.71701518 0.79967655]
ti_coms = [0.17049984 0.07569464 0.18050515 0.13669846 0.07306574 0.07248935
 0.13602786 0.11863595 0.15280274 0.07014136]
t_total = [27.24976921 27.24976921 27.24976921 27.24976921 27.24976921 27.24976921
 27.24976921 27.24976921 27.24976921 27.24976921]
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [3.18905741e-06 2.30071054e-05 3.12847447e-06 1.15333663e-07
 3.51898948e-05 3.81666577e-06 2.28001571e-07 7.92444745e-06
 3.33175889e-06 2.00316859e-06]
ene_total = [0.53424404 0.23785794 0.56558689 0.42825434 0.23000374 0.22721516
 0.42615699 0.3719125  0.47880679 0.21980253]
optimize_network iter = 1 obj = 3.71984092060831
eta = 0.7017829950267004
freqs = [20893243.25277815 38696173.21796749 20860054.05324307  6801485.54855279
 44535695.14880739 21233696.28865842  8533594.53110815 27632357.80406622
 21024470.67520079 17111091.4088647 ]
eta_min = 0.7017829950267082	eta_max = 0.7017829950267004
af = 0.0021615562993590274	bf = 1.1085543441262717	zeta = 0.0023777119292949303	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [8.58297057e-07 6.19209011e-06 8.41991876e-07 3.10406904e-08
 9.47094369e-06 1.02721042e-06 6.13639241e-08 2.13277124e-06
 8.96703470e-07 5.39129116e-07]
ene_total = [1.75228771 0.77853806 1.85510894 1.40483134 0.75185818 0.74506701
 1.39794275 1.21942176 1.57042134 0.72088691]
ti_comp = [0.69931807 0.79412327 0.68931277 0.73311945 0.79675218 0.79732856
 0.73379006 0.75118197 0.71701518 0.79967655]
ti_coms = [0.17049984 0.07569464 0.18050515 0.13669846 0.07306574 0.07248935
 0.13602786 0.11863595 0.15280274 0.07014136]
t_total = [27.24976921 27.24976921 27.24976921 27.24976921 27.24976921 27.24976921
 27.24976921 27.24976921 27.24976921 27.24976921]
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [3.18905741e-06 2.30071054e-05 3.12847447e-06 1.15333663e-07
 3.51898948e-05 3.81666577e-06 2.28001571e-07 7.92444745e-06
 3.33175889e-06 2.00316859e-06]
ene_total = [0.53424404 0.23785794 0.56558689 0.42825434 0.23000374 0.22721516
 0.42615699 0.3719125  0.47880679 0.21980253]
optimize_network iter = 2 obj = 3.71984092060831
eta = 0.7017829950267004
freqs = [20893243.25277815 38696173.21796749 20860054.05324307  6801485.54855279
 44535695.14880739 21233696.28865842  8533594.53110815 27632357.80406622
 21024470.67520079 17111091.4088647 ]
Done!
ene_coms = [0.01704998 0.00756946 0.01805051 0.01366985 0.00730657 0.00724894
 0.01360279 0.01186359 0.01528027 0.00701414]
ene_comp = [3.02513636e-06 2.18245150e-05 2.96766745e-06 1.09405386e-07
 3.33810956e-05 3.62048496e-06 2.16282040e-07 7.51712216e-06
 3.16050283e-06 1.90020353e-06]
ene_total = [0.01705301 0.00759129 0.01805348 0.01366996 0.00733996 0.00725256
 0.013603   0.01187111 0.01528343 0.00701604]
At round 55 energy consumption: 0.11873383165749558
At round 55 eta: 0.7017829950267004
At round 55 a_n: 9.342581272941924
At round 55 local rounds: 11.596049668428781
At round 55 global rounds: 31.328130579872187
gradient difference: 0.5656172037124634
train() client id: f_00000-0-0 loss: 0.858664  [   32/  126]
train() client id: f_00000-0-1 loss: 1.394908  [   64/  126]
train() client id: f_00000-0-2 loss: 0.837622  [   96/  126]
train() client id: f_00000-1-0 loss: 0.867984  [   32/  126]
train() client id: f_00000-1-1 loss: 1.220421  [   64/  126]
train() client id: f_00000-1-2 loss: 1.018792  [   96/  126]
train() client id: f_00000-2-0 loss: 0.799868  [   32/  126]
train() client id: f_00000-2-1 loss: 1.063466  [   64/  126]
train() client id: f_00000-2-2 loss: 0.859723  [   96/  126]
train() client id: f_00000-3-0 loss: 1.030679  [   32/  126]
train() client id: f_00000-3-1 loss: 0.880652  [   64/  126]
train() client id: f_00000-3-2 loss: 0.893654  [   96/  126]
train() client id: f_00000-4-0 loss: 0.863393  [   32/  126]
train() client id: f_00000-4-1 loss: 0.954985  [   64/  126]
train() client id: f_00000-4-2 loss: 0.958365  [   96/  126]
train() client id: f_00000-5-0 loss: 0.877529  [   32/  126]
train() client id: f_00000-5-1 loss: 0.924089  [   64/  126]
train() client id: f_00000-5-2 loss: 0.906160  [   96/  126]
train() client id: f_00000-6-0 loss: 0.956466  [   32/  126]
train() client id: f_00000-6-1 loss: 0.807641  [   64/  126]
train() client id: f_00000-6-2 loss: 0.875626  [   96/  126]
train() client id: f_00000-7-0 loss: 0.822967  [   32/  126]
train() client id: f_00000-7-1 loss: 0.999205  [   64/  126]
train() client id: f_00000-7-2 loss: 0.770199  [   96/  126]
train() client id: f_00000-8-0 loss: 0.879811  [   32/  126]
train() client id: f_00000-8-1 loss: 0.853453  [   64/  126]
train() client id: f_00000-8-2 loss: 0.963522  [   96/  126]
train() client id: f_00000-9-0 loss: 0.770510  [   32/  126]
train() client id: f_00000-9-1 loss: 0.955485  [   64/  126]
train() client id: f_00000-9-2 loss: 0.884466  [   96/  126]
train() client id: f_00000-10-0 loss: 0.868715  [   32/  126]
train() client id: f_00000-10-1 loss: 0.881921  [   64/  126]
train() client id: f_00000-10-2 loss: 1.007716  [   96/  126]
train() client id: f_00001-0-0 loss: 0.507726  [   32/  265]
train() client id: f_00001-0-1 loss: 0.516801  [   64/  265]
train() client id: f_00001-0-2 loss: 0.446557  [   96/  265]
train() client id: f_00001-0-3 loss: 0.468996  [  128/  265]
train() client id: f_00001-0-4 loss: 0.451378  [  160/  265]
train() client id: f_00001-0-5 loss: 0.453802  [  192/  265]
train() client id: f_00001-0-6 loss: 0.513591  [  224/  265]
train() client id: f_00001-0-7 loss: 0.354334  [  256/  265]
train() client id: f_00001-1-0 loss: 0.519095  [   32/  265]
train() client id: f_00001-1-1 loss: 0.438129  [   64/  265]
train() client id: f_00001-1-2 loss: 0.490655  [   96/  265]
train() client id: f_00001-1-3 loss: 0.381101  [  128/  265]
train() client id: f_00001-1-4 loss: 0.493305  [  160/  265]
train() client id: f_00001-1-5 loss: 0.415530  [  192/  265]
train() client id: f_00001-1-6 loss: 0.557995  [  224/  265]
train() client id: f_00001-1-7 loss: 0.355880  [  256/  265]
train() client id: f_00001-2-0 loss: 0.387344  [   32/  265]
train() client id: f_00001-2-1 loss: 0.404715  [   64/  265]
train() client id: f_00001-2-2 loss: 0.404923  [   96/  265]
train() client id: f_00001-2-3 loss: 0.718022  [  128/  265]
train() client id: f_00001-2-4 loss: 0.426372  [  160/  265]
train() client id: f_00001-2-5 loss: 0.367727  [  192/  265]
train() client id: f_00001-2-6 loss: 0.434159  [  224/  265]
train() client id: f_00001-2-7 loss: 0.462056  [  256/  265]
train() client id: f_00001-3-0 loss: 0.534578  [   32/  265]
train() client id: f_00001-3-1 loss: 0.485426  [   64/  265]
train() client id: f_00001-3-2 loss: 0.465225  [   96/  265]
train() client id: f_00001-3-3 loss: 0.465077  [  128/  265]
train() client id: f_00001-3-4 loss: 0.382459  [  160/  265]
train() client id: f_00001-3-5 loss: 0.366274  [  192/  265]
train() client id: f_00001-3-6 loss: 0.493742  [  224/  265]
train() client id: f_00001-3-7 loss: 0.372532  [  256/  265]
train() client id: f_00001-4-0 loss: 0.633469  [   32/  265]
train() client id: f_00001-4-1 loss: 0.386436  [   64/  265]
train() client id: f_00001-4-2 loss: 0.405736  [   96/  265]
train() client id: f_00001-4-3 loss: 0.373741  [  128/  265]
train() client id: f_00001-4-4 loss: 0.344383  [  160/  265]
train() client id: f_00001-4-5 loss: 0.553517  [  192/  265]
train() client id: f_00001-4-6 loss: 0.349201  [  224/  265]
train() client id: f_00001-4-7 loss: 0.482417  [  256/  265]
train() client id: f_00001-5-0 loss: 0.455355  [   32/  265]
train() client id: f_00001-5-1 loss: 0.523697  [   64/  265]
train() client id: f_00001-5-2 loss: 0.413741  [   96/  265]
train() client id: f_00001-5-3 loss: 0.338451  [  128/  265]
train() client id: f_00001-5-4 loss: 0.422622  [  160/  265]
train() client id: f_00001-5-5 loss: 0.437714  [  192/  265]
train() client id: f_00001-5-6 loss: 0.520731  [  224/  265]
train() client id: f_00001-5-7 loss: 0.382065  [  256/  265]
train() client id: f_00001-6-0 loss: 0.437665  [   32/  265]
train() client id: f_00001-6-1 loss: 0.502308  [   64/  265]
train() client id: f_00001-6-2 loss: 0.465338  [   96/  265]
train() client id: f_00001-6-3 loss: 0.417220  [  128/  265]
train() client id: f_00001-6-4 loss: 0.427808  [  160/  265]
train() client id: f_00001-6-5 loss: 0.343387  [  192/  265]
train() client id: f_00001-6-6 loss: 0.451019  [  224/  265]
train() client id: f_00001-6-7 loss: 0.428270  [  256/  265]
train() client id: f_00001-7-0 loss: 0.337521  [   32/  265]
train() client id: f_00001-7-1 loss: 0.406851  [   64/  265]
train() client id: f_00001-7-2 loss: 0.357376  [   96/  265]
train() client id: f_00001-7-3 loss: 0.437491  [  128/  265]
train() client id: f_00001-7-4 loss: 0.425934  [  160/  265]
train() client id: f_00001-7-5 loss: 0.630168  [  192/  265]
train() client id: f_00001-7-6 loss: 0.380068  [  224/  265]
train() client id: f_00001-7-7 loss: 0.376178  [  256/  265]
train() client id: f_00001-8-0 loss: 0.328551  [   32/  265]
train() client id: f_00001-8-1 loss: 0.599701  [   64/  265]
train() client id: f_00001-8-2 loss: 0.463369  [   96/  265]
train() client id: f_00001-8-3 loss: 0.451795  [  128/  265]
train() client id: f_00001-8-4 loss: 0.398596  [  160/  265]
train() client id: f_00001-8-5 loss: 0.458949  [  192/  265]
train() client id: f_00001-8-6 loss: 0.437458  [  224/  265]
train() client id: f_00001-8-7 loss: 0.344282  [  256/  265]
train() client id: f_00001-9-0 loss: 0.473788  [   32/  265]
train() client id: f_00001-9-1 loss: 0.468136  [   64/  265]
train() client id: f_00001-9-2 loss: 0.312803  [   96/  265]
train() client id: f_00001-9-3 loss: 0.484379  [  128/  265]
train() client id: f_00001-9-4 loss: 0.364453  [  160/  265]
train() client id: f_00001-9-5 loss: 0.373315  [  192/  265]
train() client id: f_00001-9-6 loss: 0.361392  [  224/  265]
train() client id: f_00001-9-7 loss: 0.597715  [  256/  265]
train() client id: f_00001-10-0 loss: 0.373971  [   32/  265]
train() client id: f_00001-10-1 loss: 0.513560  [   64/  265]
train() client id: f_00001-10-2 loss: 0.472446  [   96/  265]
train() client id: f_00001-10-3 loss: 0.378073  [  128/  265]
train() client id: f_00001-10-4 loss: 0.595115  [  160/  265]
train() client id: f_00001-10-5 loss: 0.384203  [  192/  265]
train() client id: f_00001-10-6 loss: 0.341614  [  224/  265]
train() client id: f_00001-10-7 loss: 0.339828  [  256/  265]
train() client id: f_00002-0-0 loss: 1.323666  [   32/  124]
train() client id: f_00002-0-1 loss: 1.143111  [   64/  124]
train() client id: f_00002-0-2 loss: 1.164044  [   96/  124]
train() client id: f_00002-1-0 loss: 1.205892  [   32/  124]
train() client id: f_00002-1-1 loss: 1.132035  [   64/  124]
train() client id: f_00002-1-2 loss: 1.401389  [   96/  124]
train() client id: f_00002-2-0 loss: 1.128848  [   32/  124]
train() client id: f_00002-2-1 loss: 1.181367  [   64/  124]
train() client id: f_00002-2-2 loss: 1.155296  [   96/  124]
train() client id: f_00002-3-0 loss: 1.076813  [   32/  124]
train() client id: f_00002-3-1 loss: 1.171182  [   64/  124]
train() client id: f_00002-3-2 loss: 1.109928  [   96/  124]
train() client id: f_00002-4-0 loss: 1.186506  [   32/  124]
train() client id: f_00002-4-1 loss: 1.121868  [   64/  124]
train() client id: f_00002-4-2 loss: 1.141754  [   96/  124]
train() client id: f_00002-5-0 loss: 1.136408  [   32/  124]
train() client id: f_00002-5-1 loss: 0.900308  [   64/  124]
train() client id: f_00002-5-2 loss: 1.289768  [   96/  124]
train() client id: f_00002-6-0 loss: 1.058952  [   32/  124]
train() client id: f_00002-6-1 loss: 1.122576  [   64/  124]
train() client id: f_00002-6-2 loss: 1.128720  [   96/  124]
train() client id: f_00002-7-0 loss: 1.043959  [   32/  124]
train() client id: f_00002-7-1 loss: 0.968016  [   64/  124]
train() client id: f_00002-7-2 loss: 1.198595  [   96/  124]
train() client id: f_00002-8-0 loss: 0.907041  [   32/  124]
train() client id: f_00002-8-1 loss: 1.124095  [   64/  124]
train() client id: f_00002-8-2 loss: 1.134101  [   96/  124]
train() client id: f_00002-9-0 loss: 0.988884  [   32/  124]
train() client id: f_00002-9-1 loss: 0.947419  [   64/  124]
train() client id: f_00002-9-2 loss: 1.105067  [   96/  124]
train() client id: f_00002-10-0 loss: 1.139067  [   32/  124]
train() client id: f_00002-10-1 loss: 0.981325  [   64/  124]
train() client id: f_00002-10-2 loss: 1.154899  [   96/  124]
train() client id: f_00003-0-0 loss: 0.659349  [   32/   43]
train() client id: f_00003-1-0 loss: 0.532504  [   32/   43]
train() client id: f_00003-2-0 loss: 0.647990  [   32/   43]
train() client id: f_00003-3-0 loss: 0.452856  [   32/   43]
train() client id: f_00003-4-0 loss: 0.605684  [   32/   43]
train() client id: f_00003-5-0 loss: 0.658369  [   32/   43]
train() client id: f_00003-6-0 loss: 0.369506  [   32/   43]
train() client id: f_00003-7-0 loss: 0.531795  [   32/   43]
train() client id: f_00003-8-0 loss: 0.468047  [   32/   43]
train() client id: f_00003-9-0 loss: 0.699281  [   32/   43]
train() client id: f_00003-10-0 loss: 0.587250  [   32/   43]
train() client id: f_00004-0-0 loss: 0.720309  [   32/  306]
train() client id: f_00004-0-1 loss: 0.866362  [   64/  306]
train() client id: f_00004-0-2 loss: 0.828050  [   96/  306]
train() client id: f_00004-0-3 loss: 0.911637  [  128/  306]
train() client id: f_00004-0-4 loss: 0.945298  [  160/  306]
train() client id: f_00004-0-5 loss: 0.894868  [  192/  306]
train() client id: f_00004-0-6 loss: 0.850251  [  224/  306]
train() client id: f_00004-0-7 loss: 1.012092  [  256/  306]
train() client id: f_00004-0-8 loss: 0.818906  [  288/  306]
train() client id: f_00004-1-0 loss: 0.640728  [   32/  306]
train() client id: f_00004-1-1 loss: 0.851674  [   64/  306]
train() client id: f_00004-1-2 loss: 0.826547  [   96/  306]
train() client id: f_00004-1-3 loss: 0.948931  [  128/  306]
train() client id: f_00004-1-4 loss: 0.881486  [  160/  306]
train() client id: f_00004-1-5 loss: 1.059199  [  192/  306]
train() client id: f_00004-1-6 loss: 0.853520  [  224/  306]
train() client id: f_00004-1-7 loss: 0.923830  [  256/  306]
train() client id: f_00004-1-8 loss: 0.911032  [  288/  306]
train() client id: f_00004-2-0 loss: 0.765502  [   32/  306]
train() client id: f_00004-2-1 loss: 0.926180  [   64/  306]
train() client id: f_00004-2-2 loss: 0.865556  [   96/  306]
train() client id: f_00004-2-3 loss: 0.980776  [  128/  306]
train() client id: f_00004-2-4 loss: 0.790210  [  160/  306]
train() client id: f_00004-2-5 loss: 0.853750  [  192/  306]
train() client id: f_00004-2-6 loss: 0.793832  [  224/  306]
train() client id: f_00004-2-7 loss: 0.916341  [  256/  306]
train() client id: f_00004-2-8 loss: 0.852321  [  288/  306]
train() client id: f_00004-3-0 loss: 0.959792  [   32/  306]
train() client id: f_00004-3-1 loss: 0.704191  [   64/  306]
train() client id: f_00004-3-2 loss: 1.015368  [   96/  306]
train() client id: f_00004-3-3 loss: 0.835797  [  128/  306]
train() client id: f_00004-3-4 loss: 0.817393  [  160/  306]
train() client id: f_00004-3-5 loss: 0.754584  [  192/  306]
train() client id: f_00004-3-6 loss: 0.958088  [  224/  306]
train() client id: f_00004-3-7 loss: 0.933600  [  256/  306]
train() client id: f_00004-3-8 loss: 0.871167  [  288/  306]
train() client id: f_00004-4-0 loss: 1.006912  [   32/  306]
train() client id: f_00004-4-1 loss: 0.879799  [   64/  306]
train() client id: f_00004-4-2 loss: 0.791043  [   96/  306]
train() client id: f_00004-4-3 loss: 0.886310  [  128/  306]
train() client id: f_00004-4-4 loss: 0.875570  [  160/  306]
train() client id: f_00004-4-5 loss: 0.775608  [  192/  306]
train() client id: f_00004-4-6 loss: 0.775043  [  224/  306]
train() client id: f_00004-4-7 loss: 0.908030  [  256/  306]
train() client id: f_00004-4-8 loss: 1.030888  [  288/  306]
train() client id: f_00004-5-0 loss: 0.883771  [   32/  306]
train() client id: f_00004-5-1 loss: 0.947740  [   64/  306]
train() client id: f_00004-5-2 loss: 0.823547  [   96/  306]
train() client id: f_00004-5-3 loss: 0.759337  [  128/  306]
train() client id: f_00004-5-4 loss: 0.929807  [  160/  306]
train() client id: f_00004-5-5 loss: 0.832697  [  192/  306]
train() client id: f_00004-5-6 loss: 0.848878  [  224/  306]
train() client id: f_00004-5-7 loss: 0.888786  [  256/  306]
train() client id: f_00004-5-8 loss: 0.953974  [  288/  306]
train() client id: f_00004-6-0 loss: 0.917461  [   32/  306]
train() client id: f_00004-6-1 loss: 0.792424  [   64/  306]
train() client id: f_00004-6-2 loss: 0.873133  [   96/  306]
train() client id: f_00004-6-3 loss: 0.820286  [  128/  306]
train() client id: f_00004-6-4 loss: 1.006186  [  160/  306]
train() client id: f_00004-6-5 loss: 0.858547  [  192/  306]
train() client id: f_00004-6-6 loss: 0.896079  [  224/  306]
train() client id: f_00004-6-7 loss: 0.783473  [  256/  306]
train() client id: f_00004-6-8 loss: 0.866172  [  288/  306]
train() client id: f_00004-7-0 loss: 0.729566  [   32/  306]
train() client id: f_00004-7-1 loss: 0.860495  [   64/  306]
train() client id: f_00004-7-2 loss: 0.827355  [   96/  306]
train() client id: f_00004-7-3 loss: 0.985663  [  128/  306]
train() client id: f_00004-7-4 loss: 0.870130  [  160/  306]
train() client id: f_00004-7-5 loss: 0.960363  [  192/  306]
train() client id: f_00004-7-6 loss: 0.854447  [  224/  306]
train() client id: f_00004-7-7 loss: 0.967724  [  256/  306]
train() client id: f_00004-7-8 loss: 0.873373  [  288/  306]
train() client id: f_00004-8-0 loss: 0.867428  [   32/  306]
train() client id: f_00004-8-1 loss: 0.837353  [   64/  306]
train() client id: f_00004-8-2 loss: 0.835212  [   96/  306]
train() client id: f_00004-8-3 loss: 0.833117  [  128/  306]
train() client id: f_00004-8-4 loss: 0.805958  [  160/  306]
train() client id: f_00004-8-5 loss: 0.916840  [  192/  306]
train() client id: f_00004-8-6 loss: 0.867137  [  224/  306]
train() client id: f_00004-8-7 loss: 0.934436  [  256/  306]
train() client id: f_00004-8-8 loss: 1.020710  [  288/  306]
train() client id: f_00004-9-0 loss: 0.945159  [   32/  306]
train() client id: f_00004-9-1 loss: 0.930434  [   64/  306]
train() client id: f_00004-9-2 loss: 0.950882  [   96/  306]
train() client id: f_00004-9-3 loss: 0.759207  [  128/  306]
train() client id: f_00004-9-4 loss: 0.874474  [  160/  306]
train() client id: f_00004-9-5 loss: 0.812583  [  192/  306]
train() client id: f_00004-9-6 loss: 0.743251  [  224/  306]
train() client id: f_00004-9-7 loss: 0.998924  [  256/  306]
train() client id: f_00004-9-8 loss: 0.836077  [  288/  306]
train() client id: f_00004-10-0 loss: 0.848049  [   32/  306]
train() client id: f_00004-10-1 loss: 0.829427  [   64/  306]
train() client id: f_00004-10-2 loss: 0.947354  [   96/  306]
train() client id: f_00004-10-3 loss: 0.839970  [  128/  306]
train() client id: f_00004-10-4 loss: 0.861274  [  160/  306]
train() client id: f_00004-10-5 loss: 0.914752  [  192/  306]
train() client id: f_00004-10-6 loss: 0.794820  [  224/  306]
train() client id: f_00004-10-7 loss: 0.919857  [  256/  306]
train() client id: f_00004-10-8 loss: 0.907331  [  288/  306]
train() client id: f_00005-0-0 loss: 0.639785  [   32/  146]
train() client id: f_00005-0-1 loss: 0.448692  [   64/  146]
train() client id: f_00005-0-2 loss: 0.719801  [   96/  146]
train() client id: f_00005-0-3 loss: 0.670471  [  128/  146]
train() client id: f_00005-1-0 loss: 0.403265  [   32/  146]
train() client id: f_00005-1-1 loss: 0.878641  [   64/  146]
train() client id: f_00005-1-2 loss: 0.456763  [   96/  146]
train() client id: f_00005-1-3 loss: 0.723279  [  128/  146]
train() client id: f_00005-2-0 loss: 0.686894  [   32/  146]
train() client id: f_00005-2-1 loss: 0.658008  [   64/  146]
train() client id: f_00005-2-2 loss: 0.662498  [   96/  146]
train() client id: f_00005-2-3 loss: 0.539225  [  128/  146]
train() client id: f_00005-3-0 loss: 0.441071  [   32/  146]
train() client id: f_00005-3-1 loss: 1.027585  [   64/  146]
train() client id: f_00005-3-2 loss: 0.702530  [   96/  146]
train() client id: f_00005-3-3 loss: 0.491483  [  128/  146]
train() client id: f_00005-4-0 loss: 0.524684  [   32/  146]
train() client id: f_00005-4-1 loss: 0.963772  [   64/  146]
train() client id: f_00005-4-2 loss: 0.551945  [   96/  146]
train() client id: f_00005-4-3 loss: 0.511955  [  128/  146]
train() client id: f_00005-5-0 loss: 0.648501  [   32/  146]
train() client id: f_00005-5-1 loss: 0.651461  [   64/  146]
train() client id: f_00005-5-2 loss: 0.535063  [   96/  146]
train() client id: f_00005-5-3 loss: 0.624826  [  128/  146]
train() client id: f_00005-6-0 loss: 0.670335  [   32/  146]
train() client id: f_00005-6-1 loss: 0.470738  [   64/  146]
train() client id: f_00005-6-2 loss: 0.716920  [   96/  146]
train() client id: f_00005-6-3 loss: 0.711666  [  128/  146]
train() client id: f_00005-7-0 loss: 0.550777  [   32/  146]
train() client id: f_00005-7-1 loss: 0.674017  [   64/  146]
train() client id: f_00005-7-2 loss: 0.560311  [   96/  146]
train() client id: f_00005-7-3 loss: 0.537116  [  128/  146]
train() client id: f_00005-8-0 loss: 0.715589  [   32/  146]
train() client id: f_00005-8-1 loss: 0.598868  [   64/  146]
train() client id: f_00005-8-2 loss: 0.472530  [   96/  146]
train() client id: f_00005-8-3 loss: 0.706215  [  128/  146]
train() client id: f_00005-9-0 loss: 0.714355  [   32/  146]
train() client id: f_00005-9-1 loss: 0.770478  [   64/  146]
train() client id: f_00005-9-2 loss: 0.400009  [   96/  146]
train() client id: f_00005-9-3 loss: 0.705551  [  128/  146]
train() client id: f_00005-10-0 loss: 0.453819  [   32/  146]
train() client id: f_00005-10-1 loss: 0.724115  [   64/  146]
train() client id: f_00005-10-2 loss: 0.761069  [   96/  146]
train() client id: f_00005-10-3 loss: 0.451177  [  128/  146]
train() client id: f_00006-0-0 loss: 0.493081  [   32/   54]
train() client id: f_00006-1-0 loss: 0.526872  [   32/   54]
train() client id: f_00006-2-0 loss: 0.426778  [   32/   54]
train() client id: f_00006-3-0 loss: 0.497309  [   32/   54]
train() client id: f_00006-4-0 loss: 0.477006  [   32/   54]
train() client id: f_00006-5-0 loss: 0.505215  [   32/   54]
train() client id: f_00006-6-0 loss: 0.489003  [   32/   54]
train() client id: f_00006-7-0 loss: 0.457592  [   32/   54]
train() client id: f_00006-8-0 loss: 0.429806  [   32/   54]
train() client id: f_00006-9-0 loss: 0.551690  [   32/   54]
train() client id: f_00006-10-0 loss: 0.553782  [   32/   54]
train() client id: f_00007-0-0 loss: 0.877338  [   32/  179]
train() client id: f_00007-0-1 loss: 0.819816  [   64/  179]
train() client id: f_00007-0-2 loss: 0.568880  [   96/  179]
train() client id: f_00007-0-3 loss: 0.725812  [  128/  179]
train() client id: f_00007-0-4 loss: 0.644275  [  160/  179]
train() client id: f_00007-1-0 loss: 0.620676  [   32/  179]
train() client id: f_00007-1-1 loss: 0.594536  [   64/  179]
train() client id: f_00007-1-2 loss: 0.694358  [   96/  179]
train() client id: f_00007-1-3 loss: 0.775044  [  128/  179]
train() client id: f_00007-1-4 loss: 0.750637  [  160/  179]
train() client id: f_00007-2-0 loss: 0.681320  [   32/  179]
train() client id: f_00007-2-1 loss: 0.746366  [   64/  179]
train() client id: f_00007-2-2 loss: 0.721649  [   96/  179]
train() client id: f_00007-2-3 loss: 0.591780  [  128/  179]
train() client id: f_00007-2-4 loss: 0.811669  [  160/  179]
train() client id: f_00007-3-0 loss: 0.752764  [   32/  179]
train() client id: f_00007-3-1 loss: 0.645205  [   64/  179]
train() client id: f_00007-3-2 loss: 0.587901  [   96/  179]
train() client id: f_00007-3-3 loss: 0.626688  [  128/  179]
train() client id: f_00007-3-4 loss: 0.943281  [  160/  179]
train() client id: f_00007-4-0 loss: 0.709890  [   32/  179]
train() client id: f_00007-4-1 loss: 0.679195  [   64/  179]
train() client id: f_00007-4-2 loss: 0.702123  [   96/  179]
train() client id: f_00007-4-3 loss: 0.925882  [  128/  179]
train() client id: f_00007-4-4 loss: 0.593796  [  160/  179]
train() client id: f_00007-5-0 loss: 0.827350  [   32/  179]
train() client id: f_00007-5-1 loss: 0.733121  [   64/  179]
train() client id: f_00007-5-2 loss: 0.725274  [   96/  179]
train() client id: f_00007-5-3 loss: 0.628712  [  128/  179]
train() client id: f_00007-5-4 loss: 0.622522  [  160/  179]
train() client id: f_00007-6-0 loss: 0.645210  [   32/  179]
train() client id: f_00007-6-1 loss: 0.729430  [   64/  179]
train() client id: f_00007-6-2 loss: 0.738929  [   96/  179]
train() client id: f_00007-6-3 loss: 0.926580  [  128/  179]
train() client id: f_00007-6-4 loss: 0.516760  [  160/  179]
train() client id: f_00007-7-0 loss: 0.709426  [   32/  179]
train() client id: f_00007-7-1 loss: 0.613971  [   64/  179]
train() client id: f_00007-7-2 loss: 0.508291  [   96/  179]
train() client id: f_00007-7-3 loss: 0.798618  [  128/  179]
train() client id: f_00007-7-4 loss: 0.753824  [  160/  179]
train() client id: f_00007-8-0 loss: 0.512712  [   32/  179]
train() client id: f_00007-8-1 loss: 0.677341  [   64/  179]
train() client id: f_00007-8-2 loss: 0.809204  [   96/  179]
train() client id: f_00007-8-3 loss: 0.530275  [  128/  179]
train() client id: f_00007-8-4 loss: 0.831584  [  160/  179]
train() client id: f_00007-9-0 loss: 0.641770  [   32/  179]
train() client id: f_00007-9-1 loss: 0.606141  [   64/  179]
train() client id: f_00007-9-2 loss: 0.646676  [   96/  179]
train() client id: f_00007-9-3 loss: 0.566905  [  128/  179]
train() client id: f_00007-9-4 loss: 0.918472  [  160/  179]
train() client id: f_00007-10-0 loss: 0.796578  [   32/  179]
train() client id: f_00007-10-1 loss: 0.674223  [   64/  179]
train() client id: f_00007-10-2 loss: 0.546957  [   96/  179]
train() client id: f_00007-10-3 loss: 0.689181  [  128/  179]
train() client id: f_00007-10-4 loss: 0.826661  [  160/  179]
train() client id: f_00008-0-0 loss: 0.762162  [   32/  130]
train() client id: f_00008-0-1 loss: 0.555658  [   64/  130]
train() client id: f_00008-0-2 loss: 0.728812  [   96/  130]
train() client id: f_00008-0-3 loss: 0.722890  [  128/  130]
train() client id: f_00008-1-0 loss: 0.590751  [   32/  130]
train() client id: f_00008-1-1 loss: 0.673129  [   64/  130]
train() client id: f_00008-1-2 loss: 0.625637  [   96/  130]
train() client id: f_00008-1-3 loss: 0.825026  [  128/  130]
train() client id: f_00008-2-0 loss: 0.649836  [   32/  130]
train() client id: f_00008-2-1 loss: 0.692260  [   64/  130]
train() client id: f_00008-2-2 loss: 0.689022  [   96/  130]
train() client id: f_00008-2-3 loss: 0.734211  [  128/  130]
train() client id: f_00008-3-0 loss: 0.688702  [   32/  130]
train() client id: f_00008-3-1 loss: 0.662627  [   64/  130]
train() client id: f_00008-3-2 loss: 0.676684  [   96/  130]
train() client id: f_00008-3-3 loss: 0.667526  [  128/  130]
train() client id: f_00008-4-0 loss: 0.827336  [   32/  130]
train() client id: f_00008-4-1 loss: 0.718817  [   64/  130]
train() client id: f_00008-4-2 loss: 0.611405  [   96/  130]
train() client id: f_00008-4-3 loss: 0.584121  [  128/  130]
train() client id: f_00008-5-0 loss: 0.774062  [   32/  130]
train() client id: f_00008-5-1 loss: 0.704124  [   64/  130]
train() client id: f_00008-5-2 loss: 0.645288  [   96/  130]
train() client id: f_00008-5-3 loss: 0.641146  [  128/  130]
train() client id: f_00008-6-0 loss: 0.709328  [   32/  130]
train() client id: f_00008-6-1 loss: 0.702461  [   64/  130]
train() client id: f_00008-6-2 loss: 0.774913  [   96/  130]
train() client id: f_00008-6-3 loss: 0.583350  [  128/  130]
train() client id: f_00008-7-0 loss: 0.649053  [   32/  130]
train() client id: f_00008-7-1 loss: 0.608359  [   64/  130]
train() client id: f_00008-7-2 loss: 0.598424  [   96/  130]
train() client id: f_00008-7-3 loss: 0.896648  [  128/  130]
train() client id: f_00008-8-0 loss: 0.642743  [   32/  130]
train() client id: f_00008-8-1 loss: 0.768948  [   64/  130]
train() client id: f_00008-8-2 loss: 0.683518  [   96/  130]
train() client id: f_00008-8-3 loss: 0.669625  [  128/  130]
train() client id: f_00008-9-0 loss: 0.626809  [   32/  130]
train() client id: f_00008-9-1 loss: 0.778965  [   64/  130]
train() client id: f_00008-9-2 loss: 0.659572  [   96/  130]
train() client id: f_00008-9-3 loss: 0.687873  [  128/  130]
train() client id: f_00008-10-0 loss: 0.568340  [   32/  130]
train() client id: f_00008-10-1 loss: 0.686097  [   64/  130]
train() client id: f_00008-10-2 loss: 0.741558  [   96/  130]
train() client id: f_00008-10-3 loss: 0.766788  [  128/  130]
train() client id: f_00009-0-0 loss: 0.888134  [   32/  118]
train() client id: f_00009-0-1 loss: 0.864199  [   64/  118]
train() client id: f_00009-0-2 loss: 0.884401  [   96/  118]
train() client id: f_00009-1-0 loss: 0.878866  [   32/  118]
train() client id: f_00009-1-1 loss: 0.937606  [   64/  118]
train() client id: f_00009-1-2 loss: 0.857926  [   96/  118]
train() client id: f_00009-2-0 loss: 0.824544  [   32/  118]
train() client id: f_00009-2-1 loss: 0.816494  [   64/  118]
train() client id: f_00009-2-2 loss: 0.910439  [   96/  118]
train() client id: f_00009-3-0 loss: 0.922042  [   32/  118]
train() client id: f_00009-3-1 loss: 0.891629  [   64/  118]
train() client id: f_00009-3-2 loss: 0.663619  [   96/  118]
train() client id: f_00009-4-0 loss: 0.889783  [   32/  118]
train() client id: f_00009-4-1 loss: 0.766534  [   64/  118]
train() client id: f_00009-4-2 loss: 0.821161  [   96/  118]
train() client id: f_00009-5-0 loss: 0.683354  [   32/  118]
train() client id: f_00009-5-1 loss: 0.903874  [   64/  118]
train() client id: f_00009-5-2 loss: 0.737048  [   96/  118]
train() client id: f_00009-6-0 loss: 0.707459  [   32/  118]
train() client id: f_00009-6-1 loss: 0.772412  [   64/  118]
train() client id: f_00009-6-2 loss: 0.963337  [   96/  118]
train() client id: f_00009-7-0 loss: 0.712187  [   32/  118]
train() client id: f_00009-7-1 loss: 0.911521  [   64/  118]
train() client id: f_00009-7-2 loss: 0.770370  [   96/  118]
train() client id: f_00009-8-0 loss: 0.887621  [   32/  118]
train() client id: f_00009-8-1 loss: 0.744148  [   64/  118]
train() client id: f_00009-8-2 loss: 0.772964  [   96/  118]
train() client id: f_00009-9-0 loss: 0.734849  [   32/  118]
train() client id: f_00009-9-1 loss: 0.694630  [   64/  118]
train() client id: f_00009-9-2 loss: 0.867714  [   96/  118]
train() client id: f_00009-10-0 loss: 0.757187  [   32/  118]
train() client id: f_00009-10-1 loss: 0.826638  [   64/  118]
train() client id: f_00009-10-2 loss: 0.752419  [   96/  118]
At round 55 accuracy: 0.649867374005305
At round 55 training accuracy: 0.5888665325285044
At round 55 training loss: 0.8197974631813099
update_location
xs = -3.905658 4.200318 295.009024 18.811294 0.979296 3.956410 -257.443192 -236.324852 279.663977 -222.060879 
ys = 287.587959 270.555839 1.320614 -257.455176 249.350187 232.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 304.503019 288.475483 311.499708 276.833944 268.656797 253.412869 276.195379 256.612766 297.524133 243.571438 
dists_bs = 204.815337 203.085382 501.071863 473.899037 191.040998 188.063480 195.825215 184.658040 481.148470 177.356194 
uav_gains = -116.100494 -114.739068 -116.653005 -113.692681 -112.948232 -111.588980 -113.634598 -111.868266 -115.522498 -110.759871 
bs_gains = -104.285208 -104.182062 -115.164260 -114.486262 -103.438603 -103.247584 -103.739380 -103.025369 -114.670874 -102.534757 
Round 56
-------------------------------
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.61572451 7.33497615 3.57029297 1.30775167 8.45662183 4.06918315
 1.60870824 5.01890964 3.70473634 3.29930761]
obj_prev = 41.986212112103765
eta_min = 2.748442212366637e-26	eta_max = 0.9375089458427088
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 9.666975258494574	eta = 0.909090909090909
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 21.262798910283387	eta = 0.41331150066296846
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 15.102037364580543	eta = 0.5819187910708925
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 14.000878826985186	eta = 0.6276862641626415
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 13.93490538719279	eta = 0.6306579830804683
af = 8.788159325904157	bf = 1.0953806736156944	zeta = 13.934640394529897	eta = 0.6306699761950073
eta = 0.6306699761950073
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [0.03803835 0.0800013  0.03743457 0.01298134 0.09237885 0.04407619
 0.01630215 0.05403861 0.03924592 0.03562322]
ene_total = [1.3587627  2.13524871 1.36925661 0.66156516 2.4302258  1.25152696
 0.7407784  1.61476761 1.33281256 1.03969587]
ti_comp = [0.93897812 1.03996482 0.92863396 0.97451355 1.04269211 1.04336226
 0.97521529 0.99392524 0.96170282 1.04576067]
ti_coms = [0.17720525 0.07621855 0.18754941 0.14166982 0.07349126 0.07282111
 0.14096808 0.12225813 0.15448054 0.0704227 ]
t_total = [27.19976501 27.19976501 27.19976501 27.19976501 27.19976501 27.19976501
 27.19976501 27.19976501 27.19976501 27.19976501]
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [3.90152269e-06 2.95892377e-05 3.80197612e-06 1.43967031e-07
 4.53195465e-05 4.91611198e-06 2.84717213e-07 9.98355239e-06
 4.08490276e-06 2.58353803e-06]
ene_total = [0.43191848 0.18645457 0.45712329 0.34523224 0.18019189 0.17757424
 0.34352564 0.29816856 0.37654616 0.17167282]
optimize_network iter = 0 obj = 2.96840789621886
eta = 0.6306699761950073
freqs = [20255185.25779217 38463462.35602392 20155718.00296714  6660421.60236042
 44298241.69414774 21122186.81412296  8358231.69852659 27184444.26259059
 20404389.67271167 17032203.97092095]
eta_min = 0.6306699761950091	eta_max = 0.7038160577019408
af = 0.0020423891099591697	bf = 1.0953806736156944	zeta = 0.0022466280209550867	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [8.06674524e-07 6.11783813e-06 7.86092385e-07 2.97664645e-08
 9.37021941e-06 1.01644989e-06 5.88678167e-08 2.06418827e-06
 8.44589985e-07 5.34169471e-07]
ene_total = [1.75441871 0.75517231 1.85682433 1.40253966 0.72849401 0.72103243
 1.3955953  1.21056458 1.52944703 0.69724031]
ti_comp = [0.71791719 0.81890388 0.70757302 0.75345262 0.82163117 0.82230133
 0.75415436 0.77286431 0.74064189 0.82469974]
ti_coms = [0.17720525 0.07621855 0.18754941 0.14166982 0.07349126 0.07282111
 0.14096808 0.12225813 0.15448054 0.0704227 ]
t_total = [27.19976501 27.19976501 27.19976501 27.19976501 27.19976501 27.19976501
 27.19976501 27.19976501 27.19976501 27.19976501]
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [2.95240886e-06 2.11098588e-05 2.89691726e-06 1.06538614e-07
 3.22867276e-05 3.50114543e-06 2.10607755e-07 7.30409071e-06
 3.04668167e-06 1.83767148e-06]
ene_total = [0.53855695 0.23224405 0.56998772 0.43049024 0.22429635 0.22138528
 0.42836106 0.37172334 0.46950708 0.21404676]
optimize_network iter = 1 obj = 3.70059882926673
eta = 0.7038160577019408
freqs = [20185710.85684811 37218665.59464449 20155718.00296712  6563875.93252434
 42834368.86978544 20420657.36097779  8235336.89300485 26637755.85065275
 20187518.35748624 16456368.51125106]
eta_min = 0.7038160577019519	eta_max = 0.703816057701941
af = 0.0019282538903956608	bf = 1.0953806736156944	zeta = 0.0021210792794352272	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [8.01150298e-07 5.72826139e-06 7.86092385e-07 2.89097636e-08
 8.76115832e-06 9.50052597e-07 5.71494239e-08 1.98200003e-06
 8.26731677e-07 4.98660964e-07]
ene_total = [1.75441816 0.75513375 1.85682433 1.40253957 0.72843371 0.72102585
 1.39559513 1.21055645 1.52944526 0.6972368 ]
ti_comp = [0.71791719 0.81890388 0.70757302 0.75345262 0.82163117 0.82230133
 0.75415436 0.77286431 0.74064189 0.82469974]
ti_coms = [0.17720525 0.07621855 0.18754941 0.14166982 0.07349126 0.07282111
 0.14096808 0.12225813 0.15448054 0.0704227 ]
t_total = [27.19976501 27.19976501 27.19976501 27.19976501 27.19976501 27.19976501
 27.19976501 27.19976501 27.19976501 27.19976501]
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [2.95240886e-06 2.11098588e-05 2.89691726e-06 1.06538614e-07
 3.22867276e-05 3.50114543e-06 2.10607755e-07 7.30409071e-06
 3.04668167e-06 1.83767148e-06]
ene_total = [0.53855695 0.23224405 0.56998772 0.43049024 0.22429635 0.22138528
 0.42836106 0.37172334 0.46950708 0.21404676]
optimize_network iter = 2 obj = 3.7005988292667333
eta = 0.703816057701941
freqs = [20185710.85684812 37218665.5946445  20155718.00296713  6563875.93252435
 42834368.86978545 20420657.36097779  8235336.89300485 26637755.85065275
 20187518.35748625 16456368.51125106]
Done!
ene_coms = [0.01772052 0.00762186 0.01875494 0.01416698 0.00734913 0.00728211
 0.01409681 0.01222581 0.01544805 0.00704227]
ene_comp = [2.82371805e-06 2.01897137e-05 2.77064524e-06 1.01894765e-07
 3.08794005e-05 3.34853607e-06 2.01427698e-07 6.98571701e-06
 2.91388167e-06 1.75757030e-06]
ene_total = [0.01772335 0.00764205 0.01875771 0.01416708 0.00738001 0.00728546
 0.01409701 0.0122328  0.01545097 0.00704403]
At round 56 energy consumption: 0.12178045716881733
At round 56 eta: 0.703816057701941
At round 56 a_n: 9.000035425972609
At round 56 local rounds: 11.50132443724633
At round 56 global rounds: 30.386642017599986
gradient difference: 0.48186343908309937
train() client id: f_00000-0-0 loss: 1.646228  [   32/  126]
train() client id: f_00000-0-1 loss: 1.189956  [   64/  126]
train() client id: f_00000-0-2 loss: 1.218971  [   96/  126]
train() client id: f_00000-1-0 loss: 1.113221  [   32/  126]
train() client id: f_00000-1-1 loss: 1.288903  [   64/  126]
train() client id: f_00000-1-2 loss: 1.034125  [   96/  126]
train() client id: f_00000-2-0 loss: 1.310102  [   32/  126]
train() client id: f_00000-2-1 loss: 0.852951  [   64/  126]
train() client id: f_00000-2-2 loss: 1.001923  [   96/  126]
train() client id: f_00000-3-0 loss: 0.965229  [   32/  126]
train() client id: f_00000-3-1 loss: 1.122372  [   64/  126]
train() client id: f_00000-3-2 loss: 1.024695  [   96/  126]
train() client id: f_00000-4-0 loss: 0.974009  [   32/  126]
train() client id: f_00000-4-1 loss: 0.821193  [   64/  126]
train() client id: f_00000-4-2 loss: 0.942210  [   96/  126]
train() client id: f_00000-5-0 loss: 0.904485  [   32/  126]
train() client id: f_00000-5-1 loss: 0.798864  [   64/  126]
train() client id: f_00000-5-2 loss: 0.972656  [   96/  126]
train() client id: f_00000-6-0 loss: 0.858943  [   32/  126]
train() client id: f_00000-6-1 loss: 0.742791  [   64/  126]
train() client id: f_00000-6-2 loss: 0.895230  [   96/  126]
train() client id: f_00000-7-0 loss: 0.699596  [   32/  126]
train() client id: f_00000-7-1 loss: 0.699917  [   64/  126]
train() client id: f_00000-7-2 loss: 0.888064  [   96/  126]
train() client id: f_00000-8-0 loss: 0.793197  [   32/  126]
train() client id: f_00000-8-1 loss: 0.788424  [   64/  126]
train() client id: f_00000-8-2 loss: 0.804941  [   96/  126]
train() client id: f_00000-9-0 loss: 0.676386  [   32/  126]
train() client id: f_00000-9-1 loss: 0.804946  [   64/  126]
train() client id: f_00000-9-2 loss: 0.729800  [   96/  126]
train() client id: f_00000-10-0 loss: 0.603315  [   32/  126]
train() client id: f_00000-10-1 loss: 0.784032  [   64/  126]
train() client id: f_00000-10-2 loss: 0.745580  [   96/  126]
train() client id: f_00001-0-0 loss: 0.552055  [   32/  265]
train() client id: f_00001-0-1 loss: 0.558377  [   64/  265]
train() client id: f_00001-0-2 loss: 0.453376  [   96/  265]
train() client id: f_00001-0-3 loss: 0.520143  [  128/  265]
train() client id: f_00001-0-4 loss: 0.529688  [  160/  265]
train() client id: f_00001-0-5 loss: 0.532349  [  192/  265]
train() client id: f_00001-0-6 loss: 0.472582  [  224/  265]
train() client id: f_00001-0-7 loss: 0.539153  [  256/  265]
train() client id: f_00001-1-0 loss: 0.596529  [   32/  265]
train() client id: f_00001-1-1 loss: 0.460943  [   64/  265]
train() client id: f_00001-1-2 loss: 0.553402  [   96/  265]
train() client id: f_00001-1-3 loss: 0.490124  [  128/  265]
train() client id: f_00001-1-4 loss: 0.596901  [  160/  265]
train() client id: f_00001-1-5 loss: 0.438531  [  192/  265]
train() client id: f_00001-1-6 loss: 0.468487  [  224/  265]
train() client id: f_00001-1-7 loss: 0.531584  [  256/  265]
train() client id: f_00001-2-0 loss: 0.525451  [   32/  265]
train() client id: f_00001-2-1 loss: 0.448389  [   64/  265]
train() client id: f_00001-2-2 loss: 0.513987  [   96/  265]
train() client id: f_00001-2-3 loss: 0.477528  [  128/  265]
train() client id: f_00001-2-4 loss: 0.485680  [  160/  265]
train() client id: f_00001-2-5 loss: 0.593798  [  192/  265]
train() client id: f_00001-2-6 loss: 0.463601  [  224/  265]
train() client id: f_00001-2-7 loss: 0.608072  [  256/  265]
train() client id: f_00001-3-0 loss: 0.461349  [   32/  265]
train() client id: f_00001-3-1 loss: 0.514476  [   64/  265]
train() client id: f_00001-3-2 loss: 0.491687  [   96/  265]
train() client id: f_00001-3-3 loss: 0.398160  [  128/  265]
train() client id: f_00001-3-4 loss: 0.474625  [  160/  265]
train() client id: f_00001-3-5 loss: 0.550260  [  192/  265]
train() client id: f_00001-3-6 loss: 0.661123  [  224/  265]
train() client id: f_00001-3-7 loss: 0.533230  [  256/  265]
train() client id: f_00001-4-0 loss: 0.419617  [   32/  265]
train() client id: f_00001-4-1 loss: 0.511670  [   64/  265]
train() client id: f_00001-4-2 loss: 0.467129  [   96/  265]
train() client id: f_00001-4-3 loss: 0.448579  [  128/  265]
train() client id: f_00001-4-4 loss: 0.500729  [  160/  265]
train() client id: f_00001-4-5 loss: 0.596403  [  192/  265]
train() client id: f_00001-4-6 loss: 0.496291  [  224/  265]
train() client id: f_00001-4-7 loss: 0.633431  [  256/  265]
train() client id: f_00001-5-0 loss: 0.498967  [   32/  265]
train() client id: f_00001-5-1 loss: 0.562459  [   64/  265]
train() client id: f_00001-5-2 loss: 0.692897  [   96/  265]
train() client id: f_00001-5-3 loss: 0.447015  [  128/  265]
train() client id: f_00001-5-4 loss: 0.408980  [  160/  265]
train() client id: f_00001-5-5 loss: 0.453825  [  192/  265]
train() client id: f_00001-5-6 loss: 0.414606  [  224/  265]
train() client id: f_00001-5-7 loss: 0.577656  [  256/  265]
train() client id: f_00001-6-0 loss: 0.478545  [   32/  265]
train() client id: f_00001-6-1 loss: 0.654717  [   64/  265]
train() client id: f_00001-6-2 loss: 0.424718  [   96/  265]
train() client id: f_00001-6-3 loss: 0.444339  [  128/  265]
train() client id: f_00001-6-4 loss: 0.531612  [  160/  265]
train() client id: f_00001-6-5 loss: 0.428258  [  192/  265]
train() client id: f_00001-6-6 loss: 0.567316  [  224/  265]
train() client id: f_00001-6-7 loss: 0.447723  [  256/  265]
train() client id: f_00001-7-0 loss: 0.533788  [   32/  265]
train() client id: f_00001-7-1 loss: 0.527644  [   64/  265]
train() client id: f_00001-7-2 loss: 0.412741  [   96/  265]
train() client id: f_00001-7-3 loss: 0.623487  [  128/  265]
train() client id: f_00001-7-4 loss: 0.460696  [  160/  265]
train() client id: f_00001-7-5 loss: 0.559856  [  192/  265]
train() client id: f_00001-7-6 loss: 0.404465  [  224/  265]
train() client id: f_00001-7-7 loss: 0.529974  [  256/  265]
train() client id: f_00001-8-0 loss: 0.462967  [   32/  265]
train() client id: f_00001-8-1 loss: 0.424364  [   64/  265]
train() client id: f_00001-8-2 loss: 0.413681  [   96/  265]
train() client id: f_00001-8-3 loss: 0.434816  [  128/  265]
train() client id: f_00001-8-4 loss: 0.588747  [  160/  265]
train() client id: f_00001-8-5 loss: 0.429443  [  192/  265]
train() client id: f_00001-8-6 loss: 0.542155  [  224/  265]
train() client id: f_00001-8-7 loss: 0.525442  [  256/  265]
train() client id: f_00001-9-0 loss: 0.432857  [   32/  265]
train() client id: f_00001-9-1 loss: 0.563926  [   64/  265]
train() client id: f_00001-9-2 loss: 0.527923  [   96/  265]
train() client id: f_00001-9-3 loss: 0.477555  [  128/  265]
train() client id: f_00001-9-4 loss: 0.396825  [  160/  265]
train() client id: f_00001-9-5 loss: 0.489293  [  192/  265]
train() client id: f_00001-9-6 loss: 0.653895  [  224/  265]
train() client id: f_00001-9-7 loss: 0.511826  [  256/  265]
train() client id: f_00001-10-0 loss: 0.414695  [   32/  265]
train() client id: f_00001-10-1 loss: 0.412163  [   64/  265]
train() client id: f_00001-10-2 loss: 0.500033  [   96/  265]
train() client id: f_00001-10-3 loss: 0.576761  [  128/  265]
train() client id: f_00001-10-4 loss: 0.520254  [  160/  265]
train() client id: f_00001-10-5 loss: 0.587923  [  192/  265]
train() client id: f_00001-10-6 loss: 0.503518  [  224/  265]
train() client id: f_00001-10-7 loss: 0.464424  [  256/  265]
train() client id: f_00002-0-0 loss: 0.977767  [   32/  124]
train() client id: f_00002-0-1 loss: 1.014940  [   64/  124]
train() client id: f_00002-0-2 loss: 0.917185  [   96/  124]
train() client id: f_00002-1-0 loss: 0.963503  [   32/  124]
train() client id: f_00002-1-1 loss: 1.018131  [   64/  124]
train() client id: f_00002-1-2 loss: 0.933522  [   96/  124]
train() client id: f_00002-2-0 loss: 0.831157  [   32/  124]
train() client id: f_00002-2-1 loss: 1.192290  [   64/  124]
train() client id: f_00002-2-2 loss: 0.879621  [   96/  124]
train() client id: f_00002-3-0 loss: 0.907325  [   32/  124]
train() client id: f_00002-3-1 loss: 0.891064  [   64/  124]
train() client id: f_00002-3-2 loss: 0.854821  [   96/  124]
train() client id: f_00002-4-0 loss: 0.984947  [   32/  124]
train() client id: f_00002-4-1 loss: 0.916718  [   64/  124]
train() client id: f_00002-4-2 loss: 0.803268  [   96/  124]
train() client id: f_00002-5-0 loss: 0.946483  [   32/  124]
train() client id: f_00002-5-1 loss: 0.591263  [   64/  124]
train() client id: f_00002-5-2 loss: 0.935463  [   96/  124]
train() client id: f_00002-6-0 loss: 0.731596  [   32/  124]
train() client id: f_00002-6-1 loss: 0.863512  [   64/  124]
train() client id: f_00002-6-2 loss: 1.040085  [   96/  124]
train() client id: f_00002-7-0 loss: 0.796646  [   32/  124]
train() client id: f_00002-7-1 loss: 0.725557  [   64/  124]
train() client id: f_00002-7-2 loss: 0.944104  [   96/  124]
train() client id: f_00002-8-0 loss: 0.765060  [   32/  124]
train() client id: f_00002-8-1 loss: 0.817913  [   64/  124]
train() client id: f_00002-8-2 loss: 0.828467  [   96/  124]
train() client id: f_00002-9-0 loss: 0.864935  [   32/  124]
train() client id: f_00002-9-1 loss: 0.776738  [   64/  124]
train() client id: f_00002-9-2 loss: 0.725111  [   96/  124]
train() client id: f_00002-10-0 loss: 0.975393  [   32/  124]
train() client id: f_00002-10-1 loss: 0.587263  [   64/  124]
train() client id: f_00002-10-2 loss: 1.014053  [   96/  124]
train() client id: f_00003-0-0 loss: 0.683563  [   32/   43]
train() client id: f_00003-1-0 loss: 0.631903  [   32/   43]
train() client id: f_00003-2-0 loss: 0.479374  [   32/   43]
train() client id: f_00003-3-0 loss: 0.530617  [   32/   43]
train() client id: f_00003-4-0 loss: 0.517571  [   32/   43]
train() client id: f_00003-5-0 loss: 0.635320  [   32/   43]
train() client id: f_00003-6-0 loss: 0.504448  [   32/   43]
train() client id: f_00003-7-0 loss: 0.651351  [   32/   43]
train() client id: f_00003-8-0 loss: 0.631964  [   32/   43]
train() client id: f_00003-9-0 loss: 0.617465  [   32/   43]
train() client id: f_00003-10-0 loss: 0.502835  [   32/   43]
train() client id: f_00004-0-0 loss: 0.756506  [   32/  306]
train() client id: f_00004-0-1 loss: 0.808237  [   64/  306]
train() client id: f_00004-0-2 loss: 0.725846  [   96/  306]
train() client id: f_00004-0-3 loss: 0.757289  [  128/  306]
train() client id: f_00004-0-4 loss: 0.881173  [  160/  306]
train() client id: f_00004-0-5 loss: 0.835925  [  192/  306]
train() client id: f_00004-0-6 loss: 0.837159  [  224/  306]
train() client id: f_00004-0-7 loss: 0.765913  [  256/  306]
train() client id: f_00004-0-8 loss: 0.753915  [  288/  306]
train() client id: f_00004-1-0 loss: 0.564273  [   32/  306]
train() client id: f_00004-1-1 loss: 0.824669  [   64/  306]
train() client id: f_00004-1-2 loss: 0.921411  [   96/  306]
train() client id: f_00004-1-3 loss: 0.771646  [  128/  306]
train() client id: f_00004-1-4 loss: 0.690300  [  160/  306]
train() client id: f_00004-1-5 loss: 0.715188  [  192/  306]
train() client id: f_00004-1-6 loss: 0.829688  [  224/  306]
train() client id: f_00004-1-7 loss: 0.732610  [  256/  306]
train() client id: f_00004-1-8 loss: 0.881006  [  288/  306]
train() client id: f_00004-2-0 loss: 0.621575  [   32/  306]
train() client id: f_00004-2-1 loss: 0.842977  [   64/  306]
train() client id: f_00004-2-2 loss: 0.863127  [   96/  306]
train() client id: f_00004-2-3 loss: 0.712381  [  128/  306]
train() client id: f_00004-2-4 loss: 0.727935  [  160/  306]
train() client id: f_00004-2-5 loss: 0.805516  [  192/  306]
train() client id: f_00004-2-6 loss: 0.771938  [  224/  306]
train() client id: f_00004-2-7 loss: 0.703149  [  256/  306]
train() client id: f_00004-2-8 loss: 0.948552  [  288/  306]
train() client id: f_00004-3-0 loss: 0.892590  [   32/  306]
train() client id: f_00004-3-1 loss: 0.807561  [   64/  306]
train() client id: f_00004-3-2 loss: 0.751016  [   96/  306]
train() client id: f_00004-3-3 loss: 0.797814  [  128/  306]
train() client id: f_00004-3-4 loss: 0.668776  [  160/  306]
train() client id: f_00004-3-5 loss: 0.806698  [  192/  306]
train() client id: f_00004-3-6 loss: 0.725388  [  224/  306]
train() client id: f_00004-3-7 loss: 0.862149  [  256/  306]
train() client id: f_00004-3-8 loss: 0.719971  [  288/  306]
train() client id: f_00004-4-0 loss: 0.786347  [   32/  306]
train() client id: f_00004-4-1 loss: 0.635615  [   64/  306]
train() client id: f_00004-4-2 loss: 0.825969  [   96/  306]
train() client id: f_00004-4-3 loss: 0.626816  [  128/  306]
train() client id: f_00004-4-4 loss: 0.916126  [  160/  306]
train() client id: f_00004-4-5 loss: 0.769367  [  192/  306]
train() client id: f_00004-4-6 loss: 0.776747  [  224/  306]
train() client id: f_00004-4-7 loss: 0.815917  [  256/  306]
train() client id: f_00004-4-8 loss: 0.922127  [  288/  306]
train() client id: f_00004-5-0 loss: 0.772214  [   32/  306]
train() client id: f_00004-5-1 loss: 0.711484  [   64/  306]
train() client id: f_00004-5-2 loss: 0.800110  [   96/  306]
train() client id: f_00004-5-3 loss: 0.811907  [  128/  306]
train() client id: f_00004-5-4 loss: 0.760290  [  160/  306]
train() client id: f_00004-5-5 loss: 0.740842  [  192/  306]
train() client id: f_00004-5-6 loss: 0.715634  [  224/  306]
train() client id: f_00004-5-7 loss: 0.830131  [  256/  306]
train() client id: f_00004-5-8 loss: 0.778884  [  288/  306]
train() client id: f_00004-6-0 loss: 0.860731  [   32/  306]
train() client id: f_00004-6-1 loss: 0.871148  [   64/  306]
train() client id: f_00004-6-2 loss: 0.780971  [   96/  306]
train() client id: f_00004-6-3 loss: 0.580475  [  128/  306]
train() client id: f_00004-6-4 loss: 0.803813  [  160/  306]
train() client id: f_00004-6-5 loss: 0.651035  [  192/  306]
train() client id: f_00004-6-6 loss: 0.857190  [  224/  306]
train() client id: f_00004-6-7 loss: 0.910935  [  256/  306]
train() client id: f_00004-6-8 loss: 0.772966  [  288/  306]
train() client id: f_00004-7-0 loss: 0.774538  [   32/  306]
train() client id: f_00004-7-1 loss: 0.880349  [   64/  306]
train() client id: f_00004-7-2 loss: 0.669187  [   96/  306]
train() client id: f_00004-7-3 loss: 0.934151  [  128/  306]
train() client id: f_00004-7-4 loss: 0.703022  [  160/  306]
train() client id: f_00004-7-5 loss: 0.720636  [  192/  306]
train() client id: f_00004-7-6 loss: 0.746223  [  224/  306]
train() client id: f_00004-7-7 loss: 0.741670  [  256/  306]
train() client id: f_00004-7-8 loss: 0.745574  [  288/  306]
train() client id: f_00004-8-0 loss: 1.012272  [   32/  306]
train() client id: f_00004-8-1 loss: 0.892045  [   64/  306]
train() client id: f_00004-8-2 loss: 0.779676  [   96/  306]
train() client id: f_00004-8-3 loss: 0.766238  [  128/  306]
train() client id: f_00004-8-4 loss: 0.660960  [  160/  306]
train() client id: f_00004-8-5 loss: 0.765252  [  192/  306]
train() client id: f_00004-8-6 loss: 0.674475  [  224/  306]
train() client id: f_00004-8-7 loss: 0.677866  [  256/  306]
train() client id: f_00004-8-8 loss: 0.754626  [  288/  306]
train() client id: f_00004-9-0 loss: 0.790794  [   32/  306]
train() client id: f_00004-9-1 loss: 0.731786  [   64/  306]
train() client id: f_00004-9-2 loss: 0.725649  [   96/  306]
train() client id: f_00004-9-3 loss: 0.797126  [  128/  306]
train() client id: f_00004-9-4 loss: 0.780867  [  160/  306]
train() client id: f_00004-9-5 loss: 0.731953  [  192/  306]
train() client id: f_00004-9-6 loss: 0.754168  [  224/  306]
train() client id: f_00004-9-7 loss: 0.826558  [  256/  306]
train() client id: f_00004-9-8 loss: 0.786267  [  288/  306]
train() client id: f_00004-10-0 loss: 0.753848  [   32/  306]
train() client id: f_00004-10-1 loss: 0.799351  [   64/  306]
train() client id: f_00004-10-2 loss: 0.907370  [   96/  306]
train() client id: f_00004-10-3 loss: 0.754518  [  128/  306]
train() client id: f_00004-10-4 loss: 0.799149  [  160/  306]
train() client id: f_00004-10-5 loss: 0.690009  [  192/  306]
train() client id: f_00004-10-6 loss: 0.767210  [  224/  306]
train() client id: f_00004-10-7 loss: 0.617798  [  256/  306]
train() client id: f_00004-10-8 loss: 0.818191  [  288/  306]
train() client id: f_00005-0-0 loss: 1.108675  [   32/  146]
train() client id: f_00005-0-1 loss: 0.579028  [   64/  146]
train() client id: f_00005-0-2 loss: 0.710801  [   96/  146]
train() client id: f_00005-0-3 loss: 0.709196  [  128/  146]
train() client id: f_00005-1-0 loss: 0.843571  [   32/  146]
train() client id: f_00005-1-1 loss: 0.620572  [   64/  146]
train() client id: f_00005-1-2 loss: 0.556721  [   96/  146]
train() client id: f_00005-1-3 loss: 1.016221  [  128/  146]
train() client id: f_00005-2-0 loss: 0.871025  [   32/  146]
train() client id: f_00005-2-1 loss: 0.660424  [   64/  146]
train() client id: f_00005-2-2 loss: 0.514888  [   96/  146]
train() client id: f_00005-2-3 loss: 0.888517  [  128/  146]
train() client id: f_00005-3-0 loss: 0.777303  [   32/  146]
train() client id: f_00005-3-1 loss: 0.687586  [   64/  146]
train() client id: f_00005-3-2 loss: 0.742464  [   96/  146]
train() client id: f_00005-3-3 loss: 0.799722  [  128/  146]
train() client id: f_00005-4-0 loss: 0.662588  [   32/  146]
train() client id: f_00005-4-1 loss: 0.601192  [   64/  146]
train() client id: f_00005-4-2 loss: 1.070130  [   96/  146]
train() client id: f_00005-4-3 loss: 0.735019  [  128/  146]
train() client id: f_00005-5-0 loss: 0.787571  [   32/  146]
train() client id: f_00005-5-1 loss: 0.885004  [   64/  146]
train() client id: f_00005-5-2 loss: 0.676232  [   96/  146]
train() client id: f_00005-5-3 loss: 0.792234  [  128/  146]
train() client id: f_00005-6-0 loss: 0.563176  [   32/  146]
train() client id: f_00005-6-1 loss: 0.843301  [   64/  146]
train() client id: f_00005-6-2 loss: 0.838692  [   96/  146]
train() client id: f_00005-6-3 loss: 0.796458  [  128/  146]
train() client id: f_00005-7-0 loss: 1.092184  [   32/  146]
train() client id: f_00005-7-1 loss: 0.892677  [   64/  146]
train() client id: f_00005-7-2 loss: 0.486734  [   96/  146]
train() client id: f_00005-7-3 loss: 0.589300  [  128/  146]
train() client id: f_00005-8-0 loss: 0.638992  [   32/  146]
train() client id: f_00005-8-1 loss: 0.839244  [   64/  146]
train() client id: f_00005-8-2 loss: 0.791649  [   96/  146]
train() client id: f_00005-8-3 loss: 0.874417  [  128/  146]
train() client id: f_00005-9-0 loss: 0.923154  [   32/  146]
train() client id: f_00005-9-1 loss: 0.692168  [   64/  146]
train() client id: f_00005-9-2 loss: 0.678746  [   96/  146]
train() client id: f_00005-9-3 loss: 0.659362  [  128/  146]
train() client id: f_00005-10-0 loss: 0.643147  [   32/  146]
train() client id: f_00005-10-1 loss: 1.072062  [   64/  146]
train() client id: f_00005-10-2 loss: 0.805351  [   96/  146]
train() client id: f_00005-10-3 loss: 0.568577  [  128/  146]
train() client id: f_00006-0-0 loss: 0.494804  [   32/   54]
train() client id: f_00006-1-0 loss: 0.450781  [   32/   54]
train() client id: f_00006-2-0 loss: 0.429423  [   32/   54]
train() client id: f_00006-3-0 loss: 0.493868  [   32/   54]
train() client id: f_00006-4-0 loss: 0.431896  [   32/   54]
train() client id: f_00006-5-0 loss: 0.520999  [   32/   54]
train() client id: f_00006-6-0 loss: 0.464846  [   32/   54]
train() client id: f_00006-7-0 loss: 0.411806  [   32/   54]
train() client id: f_00006-8-0 loss: 0.510314  [   32/   54]
train() client id: f_00006-9-0 loss: 0.504722  [   32/   54]
train() client id: f_00006-10-0 loss: 0.399987  [   32/   54]
train() client id: f_00007-0-0 loss: 0.414412  [   32/  179]
train() client id: f_00007-0-1 loss: 0.326753  [   64/  179]
train() client id: f_00007-0-2 loss: 0.487244  [   96/  179]
train() client id: f_00007-0-3 loss: 0.441627  [  128/  179]
train() client id: f_00007-0-4 loss: 0.646959  [  160/  179]
train() client id: f_00007-1-0 loss: 0.284002  [   32/  179]
train() client id: f_00007-1-1 loss: 0.579818  [   64/  179]
train() client id: f_00007-1-2 loss: 0.373205  [   96/  179]
train() client id: f_00007-1-3 loss: 0.379798  [  128/  179]
train() client id: f_00007-1-4 loss: 0.647885  [  160/  179]
train() client id: f_00007-2-0 loss: 0.290886  [   32/  179]
train() client id: f_00007-2-1 loss: 0.263941  [   64/  179]
train() client id: f_00007-2-2 loss: 0.524449  [   96/  179]
train() client id: f_00007-2-3 loss: 0.719936  [  128/  179]
train() client id: f_00007-2-4 loss: 0.451435  [  160/  179]
train() client id: f_00007-3-0 loss: 0.466896  [   32/  179]
train() client id: f_00007-3-1 loss: 0.408603  [   64/  179]
train() client id: f_00007-3-2 loss: 0.481033  [   96/  179]
train() client id: f_00007-3-3 loss: 0.496901  [  128/  179]
train() client id: f_00007-3-4 loss: 0.354136  [  160/  179]
train() client id: f_00007-4-0 loss: 0.438116  [   32/  179]
train() client id: f_00007-4-1 loss: 0.357002  [   64/  179]
train() client id: f_00007-4-2 loss: 0.442101  [   96/  179]
train() client id: f_00007-4-3 loss: 0.425651  [  128/  179]
train() client id: f_00007-4-4 loss: 0.477743  [  160/  179]
train() client id: f_00007-5-0 loss: 0.424352  [   32/  179]
train() client id: f_00007-5-1 loss: 0.347428  [   64/  179]
train() client id: f_00007-5-2 loss: 0.392845  [   96/  179]
train() client id: f_00007-5-3 loss: 0.460650  [  128/  179]
train() client id: f_00007-5-4 loss: 0.463630  [  160/  179]
train() client id: f_00007-6-0 loss: 0.299267  [   32/  179]
train() client id: f_00007-6-1 loss: 0.353390  [   64/  179]
train() client id: f_00007-6-2 loss: 0.340026  [   96/  179]
train() client id: f_00007-6-3 loss: 0.585276  [  128/  179]
train() client id: f_00007-6-4 loss: 0.329423  [  160/  179]
train() client id: f_00007-7-0 loss: 0.246443  [   32/  179]
train() client id: f_00007-7-1 loss: 0.285175  [   64/  179]
train() client id: f_00007-7-2 loss: 0.509209  [   96/  179]
train() client id: f_00007-7-3 loss: 0.365115  [  128/  179]
train() client id: f_00007-7-4 loss: 0.535858  [  160/  179]
train() client id: f_00007-8-0 loss: 0.254551  [   32/  179]
train() client id: f_00007-8-1 loss: 0.522879  [   64/  179]
train() client id: f_00007-8-2 loss: 0.341320  [   96/  179]
train() client id: f_00007-8-3 loss: 0.457946  [  128/  179]
train() client id: f_00007-8-4 loss: 0.532488  [  160/  179]
train() client id: f_00007-9-0 loss: 0.275326  [   32/  179]
train() client id: f_00007-9-1 loss: 0.324778  [   64/  179]
train() client id: f_00007-9-2 loss: 0.668005  [   96/  179]
train() client id: f_00007-9-3 loss: 0.420574  [  128/  179]
train() client id: f_00007-9-4 loss: 0.437393  [  160/  179]
train() client id: f_00007-10-0 loss: 0.523957  [   32/  179]
train() client id: f_00007-10-1 loss: 0.229771  [   64/  179]
train() client id: f_00007-10-2 loss: 0.557170  [   96/  179]
train() client id: f_00007-10-3 loss: 0.409424  [  128/  179]
train() client id: f_00007-10-4 loss: 0.365076  [  160/  179]
train() client id: f_00008-0-0 loss: 0.589196  [   32/  130]
train() client id: f_00008-0-1 loss: 0.677773  [   64/  130]
train() client id: f_00008-0-2 loss: 0.670358  [   96/  130]
train() client id: f_00008-0-3 loss: 0.724944  [  128/  130]
train() client id: f_00008-1-0 loss: 0.643547  [   32/  130]
train() client id: f_00008-1-1 loss: 0.734271  [   64/  130]
train() client id: f_00008-1-2 loss: 0.769888  [   96/  130]
train() client id: f_00008-1-3 loss: 0.512809  [  128/  130]
train() client id: f_00008-2-0 loss: 0.619424  [   32/  130]
train() client id: f_00008-2-1 loss: 0.845032  [   64/  130]
train() client id: f_00008-2-2 loss: 0.646225  [   96/  130]
train() client id: f_00008-2-3 loss: 0.514497  [  128/  130]
train() client id: f_00008-3-0 loss: 0.661077  [   32/  130]
train() client id: f_00008-3-1 loss: 0.688499  [   64/  130]
train() client id: f_00008-3-2 loss: 0.751711  [   96/  130]
train() client id: f_00008-3-3 loss: 0.548195  [  128/  130]
train() client id: f_00008-4-0 loss: 0.624256  [   32/  130]
train() client id: f_00008-4-1 loss: 0.726676  [   64/  130]
train() client id: f_00008-4-2 loss: 0.643912  [   96/  130]
train() client id: f_00008-4-3 loss: 0.664774  [  128/  130]
train() client id: f_00008-5-0 loss: 0.735702  [   32/  130]
train() client id: f_00008-5-1 loss: 0.668353  [   64/  130]
train() client id: f_00008-5-2 loss: 0.580067  [   96/  130]
train() client id: f_00008-5-3 loss: 0.637393  [  128/  130]
train() client id: f_00008-6-0 loss: 0.705608  [   32/  130]
train() client id: f_00008-6-1 loss: 0.720276  [   64/  130]
train() client id: f_00008-6-2 loss: 0.634157  [   96/  130]
train() client id: f_00008-6-3 loss: 0.598047  [  128/  130]
train() client id: f_00008-7-0 loss: 0.576199  [   32/  130]
train() client id: f_00008-7-1 loss: 0.686933  [   64/  130]
train() client id: f_00008-7-2 loss: 0.651804  [   96/  130]
train() client id: f_00008-7-3 loss: 0.735375  [  128/  130]
train() client id: f_00008-8-0 loss: 0.631068  [   32/  130]
train() client id: f_00008-8-1 loss: 0.695179  [   64/  130]
train() client id: f_00008-8-2 loss: 0.674839  [   96/  130]
train() client id: f_00008-8-3 loss: 0.572386  [  128/  130]
train() client id: f_00008-9-0 loss: 0.605530  [   32/  130]
train() client id: f_00008-9-1 loss: 0.758806  [   64/  130]
train() client id: f_00008-9-2 loss: 0.612578  [   96/  130]
train() client id: f_00008-9-3 loss: 0.624409  [  128/  130]
train() client id: f_00008-10-0 loss: 0.724031  [   32/  130]
train() client id: f_00008-10-1 loss: 0.703059  [   64/  130]
train() client id: f_00008-10-2 loss: 0.609981  [   96/  130]
train() client id: f_00008-10-3 loss: 0.615343  [  128/  130]
train() client id: f_00009-0-0 loss: 0.968320  [   32/  118]
train() client id: f_00009-0-1 loss: 1.008039  [   64/  118]
train() client id: f_00009-0-2 loss: 0.950334  [   96/  118]
train() client id: f_00009-1-0 loss: 1.100761  [   32/  118]
train() client id: f_00009-1-1 loss: 0.935763  [   64/  118]
train() client id: f_00009-1-2 loss: 1.011894  [   96/  118]
train() client id: f_00009-2-0 loss: 0.876096  [   32/  118]
train() client id: f_00009-2-1 loss: 0.928693  [   64/  118]
train() client id: f_00009-2-2 loss: 1.088111  [   96/  118]
train() client id: f_00009-3-0 loss: 0.941623  [   32/  118]
train() client id: f_00009-3-1 loss: 0.948065  [   64/  118]
train() client id: f_00009-3-2 loss: 0.905337  [   96/  118]
train() client id: f_00009-4-0 loss: 0.927389  [   32/  118]
train() client id: f_00009-4-1 loss: 0.841147  [   64/  118]
train() client id: f_00009-4-2 loss: 0.910458  [   96/  118]
train() client id: f_00009-5-0 loss: 0.862005  [   32/  118]
train() client id: f_00009-5-1 loss: 0.734985  [   64/  118]
train() client id: f_00009-5-2 loss: 0.951400  [   96/  118]
train() client id: f_00009-6-0 loss: 0.796993  [   32/  118]
train() client id: f_00009-6-1 loss: 0.925910  [   64/  118]
train() client id: f_00009-6-2 loss: 0.864503  [   96/  118]
train() client id: f_00009-7-0 loss: 0.740424  [   32/  118]
train() client id: f_00009-7-1 loss: 0.805112  [   64/  118]
train() client id: f_00009-7-2 loss: 1.019661  [   96/  118]
train() client id: f_00009-8-0 loss: 0.750729  [   32/  118]
train() client id: f_00009-8-1 loss: 0.790746  [   64/  118]
train() client id: f_00009-8-2 loss: 0.927014  [   96/  118]
train() client id: f_00009-9-0 loss: 0.871405  [   32/  118]
train() client id: f_00009-9-1 loss: 0.747535  [   64/  118]
train() client id: f_00009-9-2 loss: 0.944328  [   96/  118]
train() client id: f_00009-10-0 loss: 0.930067  [   32/  118]
train() client id: f_00009-10-1 loss: 0.788774  [   64/  118]
train() client id: f_00009-10-2 loss: 0.785237  [   96/  118]
At round 56 accuracy: 0.649867374005305
At round 56 training accuracy: 0.5895372233400402
At round 56 training loss: 0.8244278082214739
update_location
xs = -3.905658 4.200318 300.009024 18.811294 0.979296 3.956410 -262.443192 -241.324852 284.663977 -227.060879 
ys = 292.587959 275.555839 1.320614 -262.455176 254.350187 237.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 309.229637 293.170024 316.239084 281.489936 273.303817 258.013999 280.861745 261.224732 302.228804 248.138378 
dists_bs = 207.605881 205.485354 505.764868 478.466142 193.041873 189.660260 197.982188 186.378219 485.876002 178.747946 
uav_gains = -116.476858 -115.149786 -117.010810 -114.114667 -113.371282 -111.991813 -114.057933 -112.277390 -115.914951 -111.138542 
bs_gains = -104.449769 -104.324924 -115.277622 -114.602893 -103.565302 -103.350397 -103.872591 -103.138124 -114.789772 -102.629809 
Round 57
-------------------------------
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.48417658 7.05628152 3.44074209 1.26260335 8.13515315 3.91462212
 1.55207962 4.8313157  3.56521466 3.17400612]
obj_prev = 40.416194911724524
eta_min = 2.86092118758865e-27	eta_max = 0.9374658932840896
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 9.299045348130402	eta = 0.909090909090909
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 20.758435476455084	eta = 0.4072405937720066
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 14.636258285641127	eta = 0.5775846137877274
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 13.545044928479468	eta = 0.6241158766062833
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 13.479247501397975	eta = 0.6271624278976032
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 13.478979113930322	eta = 0.6271749156783474
af = 8.453677589209455	bf = 1.0814089668242106	zeta = 13.478979109435532	eta = 0.6271749158874893
eta = 0.6271749158874893
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [0.03849692 0.08096575 0.03788586 0.01313784 0.09349252 0.04460754
 0.01649868 0.05469007 0.03971905 0.03605267]
ene_total = [1.32155705 2.05839072 1.33209932 0.6463361  2.34272368 1.20578025
 0.72268136 1.56297838 1.28498919 1.00144307]
ti_comp = [0.9850295  1.0924086  0.97437897 1.022215   1.09523156 1.09599371
 1.02294619 1.0429956  1.01299828 1.09844051]
ti_coms = [0.18414452 0.07676542 0.19479505 0.14695901 0.07394246 0.07318031
 0.14622782 0.12617841 0.15617573 0.07073351]
t_total = [27.14976082 27.14976082 27.14976082 27.14976082 27.14976082 27.14976082
 27.14976082 27.14976082 27.14976082 27.14976082]
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [3.67501858e-06 2.77980148e-05 3.57977419e-06 1.35633586e-07
 4.25793239e-05 4.61837109e-06 2.68239096e-07 9.39810980e-06
 3.81644508e-06 2.42738867e-06]
ene_total = [0.42769316 0.17890492 0.45242289 0.34126134 0.17269288 0.17004157
 0.3395665  0.29322113 0.36274925 0.16430889]
optimize_network iter = 0 obj = 2.90286252803159
eta = 0.6271749158874893
freqs = [19540999.15308214 37058362.31216387 19441028.73626838  6426161.89579366
 42681624.07611285 20350273.65967661  8064295.3890079  26217785.78934446
 19604695.49055661 16410844.32139915]
eta_min = 0.6271749158874926	eta_max = 0.7063960382409014
af = 0.0018241050152146168	bf = 1.0814089668242106	zeta = 0.002006515516736079	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [7.50791650e-07 5.67902365e-06 7.31333600e-07 2.77094010e-08
 8.69878619e-06 9.43514808e-07 5.48001783e-08 1.91999638e-06
 7.79684519e-07 4.95905832e-07]
ene_total = [1.75372365 0.73159623 1.85514936 1.39952837 0.7050001  0.6970034
 1.39256766 1.20180996 1.48737301 0.67365931]
ti_comp = [0.73659322 0.84397231 0.72594269 0.77377872 0.84679527 0.84755742
 0.77450991 0.79455932 0.764562   0.85000422]
ti_coms = [0.18414452 0.07676542 0.19479505 0.14695901 0.07394246 0.07318031
 0.14622782 0.12617841 0.15617573 0.07073351]
t_total = [27.14976082 27.14976082 27.14976082 27.14976082 27.14976082 27.14976082
 27.14976082 27.14976082 27.14976082 27.14976082]
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [2.71785248e-06 1.92597612e-05 2.66704329e-06 9.78907100e-08
 2.94562262e-05 3.19366957e-06 1.93507141e-07 6.69693547e-06
 2.77060309e-06 1.67638409e-06]
ene_total = [0.54306642 0.22692583 0.57447011 0.4333404  0.21890245 0.21588069
 0.43118717 0.37225931 0.46059653 0.20862108]
optimize_network iter = 1 obj = 3.685249982672195
eta = 0.7063960382409014
freqs = [19468958.52365561 35736956.5886699  19441028.73626838  6324869.42089178
 41128502.02279741 19605756.94700696  7935360.67409761 25640506.16051596
 19352208.45479425 15800135.59527822]
eta_min = 0.7063960382409034	eta_max = 0.7063960382409006
af = 0.0017119208968572406	bf = 1.0814089668242106	zeta = 0.0018831129865429649	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [7.45266057e-07 5.28124555e-06 7.31333600e-07 2.68427459e-08
 8.07723219e-06 8.75740514e-07 5.30618587e-08 1.83637587e-06
 7.59730875e-07 4.59683582e-07]
ene_total = [1.75372312 0.73155835 1.85514936 1.39952829 0.7049409  0.69699695
 1.39256749 1.20180199 1.48737111 0.67365586]
ti_comp = [0.73659322 0.84397231 0.72594269 0.77377872 0.84679527 0.84755742
 0.77450991 0.79455932 0.764562   0.85000422]
ti_coms = [0.18414452 0.07676542 0.19479505 0.14695901 0.07394246 0.07318031
 0.14622782 0.12617841 0.15617573 0.07073351]
t_total = [27.14976082 27.14976082 27.14976082 27.14976082 27.14976082 27.14976082
 27.14976082 27.14976082 27.14976082 27.14976082]
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [2.71785248e-06 1.92597612e-05 2.66704329e-06 9.78907100e-08
 2.94562262e-05 3.19366957e-06 1.93507141e-07 6.69693547e-06
 2.77060309e-06 1.67638409e-06]
ene_total = [0.54306642 0.22692583 0.57447011 0.4333404  0.21890245 0.21588069
 0.43118717 0.37225931 0.46059653 0.20862108]
optimize_network iter = 2 obj = 3.6852499826721847
eta = 0.7063960382409006
freqs = [19468958.52365561 35736956.5886699  19441028.73626836  6324869.42089178
 41128502.02279742 19605756.94700696  7935360.67409761 25640506.16051596
 19352208.45479425 15800135.59527823]
Done!
ene_coms = [0.01841445 0.00767654 0.0194795  0.0146959  0.00739425 0.00731803
 0.01462278 0.01261784 0.01561757 0.00707335]
ene_comp = [2.62674960e-06 1.86141707e-05 2.57764354e-06 9.46093966e-08
 2.84688484e-05 3.08661721e-06 1.87020749e-07 6.47245305e-06
 2.67773200e-06 1.62019141e-06]
ene_total = [0.01841708 0.00769516 0.01948208 0.014696   0.00742271 0.00732112
 0.01462297 0.01262431 0.01562025 0.00707497]
At round 57 energy consumption: 0.12497665052505343
At round 57 eta: 0.7063960382409006
At round 57 a_n: 8.65748957900329
At round 57 local rounds: 11.38151016928333
At round 57 global rounds: 29.486964437171718
gradient difference: 0.4450613558292389
train() client id: f_00000-0-0 loss: 1.144006  [   32/  126]
train() client id: f_00000-0-1 loss: 0.854304  [   64/  126]
train() client id: f_00000-0-2 loss: 1.026668  [   96/  126]
train() client id: f_00000-1-0 loss: 0.752336  [   32/  126]
train() client id: f_00000-1-1 loss: 0.998073  [   64/  126]
train() client id: f_00000-1-2 loss: 1.065730  [   96/  126]
train() client id: f_00000-2-0 loss: 1.048712  [   32/  126]
train() client id: f_00000-2-1 loss: 0.873464  [   64/  126]
train() client id: f_00000-2-2 loss: 0.725569  [   96/  126]
train() client id: f_00000-3-0 loss: 0.806066  [   32/  126]
train() client id: f_00000-3-1 loss: 0.799743  [   64/  126]
train() client id: f_00000-3-2 loss: 0.848330  [   96/  126]
train() client id: f_00000-4-0 loss: 0.825415  [   32/  126]
train() client id: f_00000-4-1 loss: 0.803453  [   64/  126]
train() client id: f_00000-4-2 loss: 0.833822  [   96/  126]
train() client id: f_00000-5-0 loss: 0.854477  [   32/  126]
train() client id: f_00000-5-1 loss: 0.774937  [   64/  126]
train() client id: f_00000-5-2 loss: 0.747176  [   96/  126]
train() client id: f_00000-6-0 loss: 0.771632  [   32/  126]
train() client id: f_00000-6-1 loss: 0.861077  [   64/  126]
train() client id: f_00000-6-2 loss: 0.750567  [   96/  126]
train() client id: f_00000-7-0 loss: 0.823906  [   32/  126]
train() client id: f_00000-7-1 loss: 0.795739  [   64/  126]
train() client id: f_00000-7-2 loss: 0.712867  [   96/  126]
train() client id: f_00000-8-0 loss: 0.752668  [   32/  126]
train() client id: f_00000-8-1 loss: 0.716388  [   64/  126]
train() client id: f_00000-8-2 loss: 0.755949  [   96/  126]
train() client id: f_00000-9-0 loss: 0.609115  [   32/  126]
train() client id: f_00000-9-1 loss: 0.897222  [   64/  126]
train() client id: f_00000-9-2 loss: 0.773259  [   96/  126]
train() client id: f_00000-10-0 loss: 0.717402  [   32/  126]
train() client id: f_00000-10-1 loss: 1.015443  [   64/  126]
train() client id: f_00000-10-2 loss: 0.628089  [   96/  126]
train() client id: f_00001-0-0 loss: 0.421365  [   32/  265]
train() client id: f_00001-0-1 loss: 0.665170  [   64/  265]
train() client id: f_00001-0-2 loss: 0.416469  [   96/  265]
train() client id: f_00001-0-3 loss: 0.515943  [  128/  265]
train() client id: f_00001-0-4 loss: 0.383373  [  160/  265]
train() client id: f_00001-0-5 loss: 0.420718  [  192/  265]
train() client id: f_00001-0-6 loss: 0.420848  [  224/  265]
train() client id: f_00001-0-7 loss: 0.443143  [  256/  265]
train() client id: f_00001-1-0 loss: 0.485266  [   32/  265]
train() client id: f_00001-1-1 loss: 0.383047  [   64/  265]
train() client id: f_00001-1-2 loss: 0.540797  [   96/  265]
train() client id: f_00001-1-3 loss: 0.383110  [  128/  265]
train() client id: f_00001-1-4 loss: 0.400752  [  160/  265]
train() client id: f_00001-1-5 loss: 0.414159  [  192/  265]
train() client id: f_00001-1-6 loss: 0.532770  [  224/  265]
train() client id: f_00001-1-7 loss: 0.411665  [  256/  265]
train() client id: f_00001-2-0 loss: 0.362453  [   32/  265]
train() client id: f_00001-2-1 loss: 0.351042  [   64/  265]
train() client id: f_00001-2-2 loss: 0.386868  [   96/  265]
train() client id: f_00001-2-3 loss: 0.388523  [  128/  265]
train() client id: f_00001-2-4 loss: 0.667902  [  160/  265]
train() client id: f_00001-2-5 loss: 0.490927  [  192/  265]
train() client id: f_00001-2-6 loss: 0.474453  [  224/  265]
train() client id: f_00001-2-7 loss: 0.444759  [  256/  265]
train() client id: f_00001-3-0 loss: 0.593307  [   32/  265]
train() client id: f_00001-3-1 loss: 0.498448  [   64/  265]
train() client id: f_00001-3-2 loss: 0.428416  [   96/  265]
train() client id: f_00001-3-3 loss: 0.426003  [  128/  265]
train() client id: f_00001-3-4 loss: 0.356509  [  160/  265]
train() client id: f_00001-3-5 loss: 0.366738  [  192/  265]
train() client id: f_00001-3-6 loss: 0.426214  [  224/  265]
train() client id: f_00001-3-7 loss: 0.347618  [  256/  265]
train() client id: f_00001-4-0 loss: 0.394598  [   32/  265]
train() client id: f_00001-4-1 loss: 0.539549  [   64/  265]
train() client id: f_00001-4-2 loss: 0.437298  [   96/  265]
train() client id: f_00001-4-3 loss: 0.398193  [  128/  265]
train() client id: f_00001-4-4 loss: 0.432712  [  160/  265]
train() client id: f_00001-4-5 loss: 0.378151  [  192/  265]
train() client id: f_00001-4-6 loss: 0.397362  [  224/  265]
train() client id: f_00001-4-7 loss: 0.526770  [  256/  265]
train() client id: f_00001-5-0 loss: 0.414279  [   32/  265]
train() client id: f_00001-5-1 loss: 0.466192  [   64/  265]
train() client id: f_00001-5-2 loss: 0.432259  [   96/  265]
train() client id: f_00001-5-3 loss: 0.464289  [  128/  265]
train() client id: f_00001-5-4 loss: 0.484136  [  160/  265]
train() client id: f_00001-5-5 loss: 0.358167  [  192/  265]
train() client id: f_00001-5-6 loss: 0.486226  [  224/  265]
train() client id: f_00001-5-7 loss: 0.390577  [  256/  265]
train() client id: f_00001-6-0 loss: 0.319498  [   32/  265]
train() client id: f_00001-6-1 loss: 0.466557  [   64/  265]
train() client id: f_00001-6-2 loss: 0.498952  [   96/  265]
train() client id: f_00001-6-3 loss: 0.455134  [  128/  265]
train() client id: f_00001-6-4 loss: 0.351912  [  160/  265]
train() client id: f_00001-6-5 loss: 0.562863  [  192/  265]
train() client id: f_00001-6-6 loss: 0.397600  [  224/  265]
train() client id: f_00001-6-7 loss: 0.410938  [  256/  265]
train() client id: f_00001-7-0 loss: 0.412322  [   32/  265]
train() client id: f_00001-7-1 loss: 0.504750  [   64/  265]
train() client id: f_00001-7-2 loss: 0.338052  [   96/  265]
train() client id: f_00001-7-3 loss: 0.555073  [  128/  265]
train() client id: f_00001-7-4 loss: 0.334443  [  160/  265]
train() client id: f_00001-7-5 loss: 0.450138  [  192/  265]
train() client id: f_00001-7-6 loss: 0.336380  [  224/  265]
train() client id: f_00001-7-7 loss: 0.482389  [  256/  265]
train() client id: f_00001-8-0 loss: 0.454096  [   32/  265]
train() client id: f_00001-8-1 loss: 0.418654  [   64/  265]
train() client id: f_00001-8-2 loss: 0.441878  [   96/  265]
train() client id: f_00001-8-3 loss: 0.339452  [  128/  265]
train() client id: f_00001-8-4 loss: 0.463184  [  160/  265]
train() client id: f_00001-8-5 loss: 0.347665  [  192/  265]
train() client id: f_00001-8-6 loss: 0.319181  [  224/  265]
train() client id: f_00001-8-7 loss: 0.579217  [  256/  265]
train() client id: f_00001-9-0 loss: 0.418132  [   32/  265]
train() client id: f_00001-9-1 loss: 0.427147  [   64/  265]
train() client id: f_00001-9-2 loss: 0.449970  [   96/  265]
train() client id: f_00001-9-3 loss: 0.480487  [  128/  265]
train() client id: f_00001-9-4 loss: 0.448535  [  160/  265]
train() client id: f_00001-9-5 loss: 0.512040  [  192/  265]
train() client id: f_00001-9-6 loss: 0.373046  [  224/  265]
train() client id: f_00001-9-7 loss: 0.342902  [  256/  265]
train() client id: f_00001-10-0 loss: 0.413961  [   32/  265]
train() client id: f_00001-10-1 loss: 0.388993  [   64/  265]
train() client id: f_00001-10-2 loss: 0.652893  [   96/  265]
train() client id: f_00001-10-3 loss: 0.388723  [  128/  265]
train() client id: f_00001-10-4 loss: 0.336401  [  160/  265]
train() client id: f_00001-10-5 loss: 0.477460  [  192/  265]
train() client id: f_00001-10-6 loss: 0.437351  [  224/  265]
train() client id: f_00001-10-7 loss: 0.345053  [  256/  265]
train() client id: f_00002-0-0 loss: 1.083431  [   32/  124]
train() client id: f_00002-0-1 loss: 1.053267  [   64/  124]
train() client id: f_00002-0-2 loss: 0.807235  [   96/  124]
train() client id: f_00002-1-0 loss: 1.114626  [   32/  124]
train() client id: f_00002-1-1 loss: 0.935073  [   64/  124]
train() client id: f_00002-1-2 loss: 0.944268  [   96/  124]
train() client id: f_00002-2-0 loss: 0.949436  [   32/  124]
train() client id: f_00002-2-1 loss: 0.990156  [   64/  124]
train() client id: f_00002-2-2 loss: 0.933199  [   96/  124]
train() client id: f_00002-3-0 loss: 0.961241  [   32/  124]
train() client id: f_00002-3-1 loss: 0.847857  [   64/  124]
train() client id: f_00002-3-2 loss: 0.965534  [   96/  124]
train() client id: f_00002-4-0 loss: 0.975303  [   32/  124]
train() client id: f_00002-4-1 loss: 0.871599  [   64/  124]
train() client id: f_00002-4-2 loss: 0.814993  [   96/  124]
train() client id: f_00002-5-0 loss: 0.859060  [   32/  124]
train() client id: f_00002-5-1 loss: 0.633705  [   64/  124]
train() client id: f_00002-5-2 loss: 1.105031  [   96/  124]
train() client id: f_00002-6-0 loss: 0.884133  [   32/  124]
train() client id: f_00002-6-1 loss: 0.779250  [   64/  124]
train() client id: f_00002-6-2 loss: 0.792807  [   96/  124]
train() client id: f_00002-7-0 loss: 0.853386  [   32/  124]
train() client id: f_00002-7-1 loss: 0.771526  [   64/  124]
train() client id: f_00002-7-2 loss: 0.856016  [   96/  124]
train() client id: f_00002-8-0 loss: 0.791956  [   32/  124]
train() client id: f_00002-8-1 loss: 0.819494  [   64/  124]
train() client id: f_00002-8-2 loss: 0.915428  [   96/  124]
train() client id: f_00002-9-0 loss: 0.733523  [   32/  124]
train() client id: f_00002-9-1 loss: 0.674927  [   64/  124]
train() client id: f_00002-9-2 loss: 0.832655  [   96/  124]
train() client id: f_00002-10-0 loss: 0.601439  [   32/  124]
train() client id: f_00002-10-1 loss: 0.976594  [   64/  124]
train() client id: f_00002-10-2 loss: 0.725718  [   96/  124]
train() client id: f_00003-0-0 loss: 0.621710  [   32/   43]
train() client id: f_00003-1-0 loss: 0.815684  [   32/   43]
train() client id: f_00003-2-0 loss: 0.815690  [   32/   43]
train() client id: f_00003-3-0 loss: 0.658684  [   32/   43]
train() client id: f_00003-4-0 loss: 0.766659  [   32/   43]
train() client id: f_00003-5-0 loss: 0.700677  [   32/   43]
train() client id: f_00003-6-0 loss: 0.723433  [   32/   43]
train() client id: f_00003-7-0 loss: 0.775298  [   32/   43]
train() client id: f_00003-8-0 loss: 0.663630  [   32/   43]
train() client id: f_00003-9-0 loss: 0.795121  [   32/   43]
train() client id: f_00003-10-0 loss: 0.716387  [   32/   43]
train() client id: f_00004-0-0 loss: 0.697648  [   32/  306]
train() client id: f_00004-0-1 loss: 0.752866  [   64/  306]
train() client id: f_00004-0-2 loss: 0.761450  [   96/  306]
train() client id: f_00004-0-3 loss: 0.714344  [  128/  306]
train() client id: f_00004-0-4 loss: 0.688322  [  160/  306]
train() client id: f_00004-0-5 loss: 0.756119  [  192/  306]
train() client id: f_00004-0-6 loss: 0.957409  [  224/  306]
train() client id: f_00004-0-7 loss: 0.883526  [  256/  306]
train() client id: f_00004-0-8 loss: 0.686458  [  288/  306]
train() client id: f_00004-1-0 loss: 0.700983  [   32/  306]
train() client id: f_00004-1-1 loss: 0.733806  [   64/  306]
train() client id: f_00004-1-2 loss: 0.794997  [   96/  306]
train() client id: f_00004-1-3 loss: 0.684161  [  128/  306]
train() client id: f_00004-1-4 loss: 0.645138  [  160/  306]
train() client id: f_00004-1-5 loss: 0.917092  [  192/  306]
train() client id: f_00004-1-6 loss: 0.799627  [  224/  306]
train() client id: f_00004-1-7 loss: 0.803180  [  256/  306]
train() client id: f_00004-1-8 loss: 0.848904  [  288/  306]
train() client id: f_00004-2-0 loss: 0.755060  [   32/  306]
train() client id: f_00004-2-1 loss: 0.781236  [   64/  306]
train() client id: f_00004-2-2 loss: 0.766103  [   96/  306]
train() client id: f_00004-2-3 loss: 0.770789  [  128/  306]
train() client id: f_00004-2-4 loss: 0.884454  [  160/  306]
train() client id: f_00004-2-5 loss: 0.631636  [  192/  306]
train() client id: f_00004-2-6 loss: 0.722976  [  224/  306]
train() client id: f_00004-2-7 loss: 0.782505  [  256/  306]
train() client id: f_00004-2-8 loss: 0.773284  [  288/  306]
train() client id: f_00004-3-0 loss: 0.884277  [   32/  306]
train() client id: f_00004-3-1 loss: 0.745139  [   64/  306]
train() client id: f_00004-3-2 loss: 0.675127  [   96/  306]
train() client id: f_00004-3-3 loss: 0.772094  [  128/  306]
train() client id: f_00004-3-4 loss: 0.713134  [  160/  306]
train() client id: f_00004-3-5 loss: 0.738828  [  192/  306]
train() client id: f_00004-3-6 loss: 0.664343  [  224/  306]
train() client id: f_00004-3-7 loss: 0.807555  [  256/  306]
train() client id: f_00004-3-8 loss: 0.771339  [  288/  306]
train() client id: f_00004-4-0 loss: 0.864053  [   32/  306]
train() client id: f_00004-4-1 loss: 1.044672  [   64/  306]
train() client id: f_00004-4-2 loss: 0.644987  [   96/  306]
train() client id: f_00004-4-3 loss: 0.798501  [  128/  306]
train() client id: f_00004-4-4 loss: 0.780310  [  160/  306]
train() client id: f_00004-4-5 loss: 0.788850  [  192/  306]
train() client id: f_00004-4-6 loss: 0.665271  [  224/  306]
train() client id: f_00004-4-7 loss: 0.670119  [  256/  306]
train() client id: f_00004-4-8 loss: 0.721061  [  288/  306]
train() client id: f_00004-5-0 loss: 0.835929  [   32/  306]
train() client id: f_00004-5-1 loss: 0.808675  [   64/  306]
train() client id: f_00004-5-2 loss: 0.819079  [   96/  306]
train() client id: f_00004-5-3 loss: 0.735923  [  128/  306]
train() client id: f_00004-5-4 loss: 0.754097  [  160/  306]
train() client id: f_00004-5-5 loss: 0.770316  [  192/  306]
train() client id: f_00004-5-6 loss: 0.633116  [  224/  306]
train() client id: f_00004-5-7 loss: 0.748915  [  256/  306]
train() client id: f_00004-5-8 loss: 0.827034  [  288/  306]
train() client id: f_00004-6-0 loss: 0.817648  [   32/  306]
train() client id: f_00004-6-1 loss: 0.807888  [   64/  306]
train() client id: f_00004-6-2 loss: 0.754437  [   96/  306]
train() client id: f_00004-6-3 loss: 0.850184  [  128/  306]
train() client id: f_00004-6-4 loss: 0.804733  [  160/  306]
train() client id: f_00004-6-5 loss: 0.739371  [  192/  306]
train() client id: f_00004-6-6 loss: 0.702388  [  224/  306]
train() client id: f_00004-6-7 loss: 0.684168  [  256/  306]
train() client id: f_00004-6-8 loss: 0.713656  [  288/  306]
train() client id: f_00004-7-0 loss: 0.675200  [   32/  306]
train() client id: f_00004-7-1 loss: 0.749022  [   64/  306]
train() client id: f_00004-7-2 loss: 0.799630  [   96/  306]
train() client id: f_00004-7-3 loss: 0.693482  [  128/  306]
train() client id: f_00004-7-4 loss: 0.669527  [  160/  306]
train() client id: f_00004-7-5 loss: 0.814661  [  192/  306]
train() client id: f_00004-7-6 loss: 0.776985  [  224/  306]
train() client id: f_00004-7-7 loss: 0.770948  [  256/  306]
train() client id: f_00004-7-8 loss: 0.888934  [  288/  306]
train() client id: f_00004-8-0 loss: 0.830048  [   32/  306]
train() client id: f_00004-8-1 loss: 0.681616  [   64/  306]
train() client id: f_00004-8-2 loss: 0.762143  [   96/  306]
train() client id: f_00004-8-3 loss: 0.758830  [  128/  306]
train() client id: f_00004-8-4 loss: 0.820779  [  160/  306]
train() client id: f_00004-8-5 loss: 0.744775  [  192/  306]
train() client id: f_00004-8-6 loss: 0.761380  [  224/  306]
train() client id: f_00004-8-7 loss: 0.711482  [  256/  306]
train() client id: f_00004-8-8 loss: 0.849334  [  288/  306]
train() client id: f_00004-9-0 loss: 0.716136  [   32/  306]
train() client id: f_00004-9-1 loss: 0.698625  [   64/  306]
train() client id: f_00004-9-2 loss: 0.863997  [   96/  306]
train() client id: f_00004-9-3 loss: 0.662727  [  128/  306]
train() client id: f_00004-9-4 loss: 0.793980  [  160/  306]
train() client id: f_00004-9-5 loss: 0.741862  [  192/  306]
train() client id: f_00004-9-6 loss: 0.791610  [  224/  306]
train() client id: f_00004-9-7 loss: 0.788388  [  256/  306]
train() client id: f_00004-9-8 loss: 0.862362  [  288/  306]
train() client id: f_00004-10-0 loss: 0.658872  [   32/  306]
train() client id: f_00004-10-1 loss: 0.765553  [   64/  306]
train() client id: f_00004-10-2 loss: 0.767341  [   96/  306]
train() client id: f_00004-10-3 loss: 0.646314  [  128/  306]
train() client id: f_00004-10-4 loss: 0.835258  [  160/  306]
train() client id: f_00004-10-5 loss: 0.687357  [  192/  306]
train() client id: f_00004-10-6 loss: 0.764137  [  224/  306]
train() client id: f_00004-10-7 loss: 0.933431  [  256/  306]
train() client id: f_00004-10-8 loss: 0.840254  [  288/  306]
train() client id: f_00005-0-0 loss: 0.683706  [   32/  146]
train() client id: f_00005-0-1 loss: 0.763946  [   64/  146]
train() client id: f_00005-0-2 loss: 0.912688  [   96/  146]
train() client id: f_00005-0-3 loss: 0.729394  [  128/  146]
train() client id: f_00005-1-0 loss: 0.694541  [   32/  146]
train() client id: f_00005-1-1 loss: 0.558928  [   64/  146]
train() client id: f_00005-1-2 loss: 0.972487  [   96/  146]
train() client id: f_00005-1-3 loss: 0.674085  [  128/  146]
train() client id: f_00005-2-0 loss: 0.931434  [   32/  146]
train() client id: f_00005-2-1 loss: 0.585831  [   64/  146]
train() client id: f_00005-2-2 loss: 0.647717  [   96/  146]
train() client id: f_00005-2-3 loss: 0.827745  [  128/  146]
train() client id: f_00005-3-0 loss: 0.712392  [   32/  146]
train() client id: f_00005-3-1 loss: 0.763343  [   64/  146]
train() client id: f_00005-3-2 loss: 0.754795  [   96/  146]
train() client id: f_00005-3-3 loss: 0.890006  [  128/  146]
train() client id: f_00005-4-0 loss: 0.679093  [   32/  146]
train() client id: f_00005-4-1 loss: 0.525236  [   64/  146]
train() client id: f_00005-4-2 loss: 0.914381  [   96/  146]
train() client id: f_00005-4-3 loss: 0.949748  [  128/  146]
train() client id: f_00005-5-0 loss: 1.109017  [   32/  146]
train() client id: f_00005-5-1 loss: 0.804677  [   64/  146]
train() client id: f_00005-5-2 loss: 0.689102  [   96/  146]
train() client id: f_00005-5-3 loss: 0.604620  [  128/  146]
train() client id: f_00005-6-0 loss: 0.819835  [   32/  146]
train() client id: f_00005-6-1 loss: 0.761808  [   64/  146]
train() client id: f_00005-6-2 loss: 0.983828  [   96/  146]
train() client id: f_00005-6-3 loss: 0.718049  [  128/  146]
train() client id: f_00005-7-0 loss: 0.774432  [   32/  146]
train() client id: f_00005-7-1 loss: 1.076891  [   64/  146]
train() client id: f_00005-7-2 loss: 0.571081  [   96/  146]
train() client id: f_00005-7-3 loss: 0.767015  [  128/  146]
train() client id: f_00005-8-0 loss: 0.468100  [   32/  146]
train() client id: f_00005-8-1 loss: 0.852120  [   64/  146]
train() client id: f_00005-8-2 loss: 0.745608  [   96/  146]
train() client id: f_00005-8-3 loss: 0.948929  [  128/  146]
train() client id: f_00005-9-0 loss: 0.812472  [   32/  146]
train() client id: f_00005-9-1 loss: 0.874346  [   64/  146]
train() client id: f_00005-9-2 loss: 0.953986  [   96/  146]
train() client id: f_00005-9-3 loss: 0.461018  [  128/  146]
train() client id: f_00005-10-0 loss: 0.735595  [   32/  146]
train() client id: f_00005-10-1 loss: 0.864530  [   64/  146]
train() client id: f_00005-10-2 loss: 0.690350  [   96/  146]
train() client id: f_00005-10-3 loss: 0.795977  [  128/  146]
train() client id: f_00006-0-0 loss: 0.515070  [   32/   54]
train() client id: f_00006-1-0 loss: 0.547666  [   32/   54]
train() client id: f_00006-2-0 loss: 0.504667  [   32/   54]
train() client id: f_00006-3-0 loss: 0.502037  [   32/   54]
train() client id: f_00006-4-0 loss: 0.555451  [   32/   54]
train() client id: f_00006-5-0 loss: 0.544033  [   32/   54]
train() client id: f_00006-6-0 loss: 0.504091  [   32/   54]
train() client id: f_00006-7-0 loss: 0.521843  [   32/   54]
train() client id: f_00006-8-0 loss: 0.559301  [   32/   54]
train() client id: f_00006-9-0 loss: 0.555144  [   32/   54]
train() client id: f_00006-10-0 loss: 0.554840  [   32/   54]
train() client id: f_00007-0-0 loss: 0.488515  [   32/  179]
train() client id: f_00007-0-1 loss: 0.767585  [   64/  179]
train() client id: f_00007-0-2 loss: 0.535754  [   96/  179]
train() client id: f_00007-0-3 loss: 0.872157  [  128/  179]
train() client id: f_00007-0-4 loss: 0.654426  [  160/  179]
train() client id: f_00007-1-0 loss: 0.636690  [   32/  179]
train() client id: f_00007-1-1 loss: 0.525703  [   64/  179]
train() client id: f_00007-1-2 loss: 0.516519  [   96/  179]
train() client id: f_00007-1-3 loss: 0.725335  [  128/  179]
train() client id: f_00007-1-4 loss: 0.766940  [  160/  179]
train() client id: f_00007-2-0 loss: 0.611746  [   32/  179]
train() client id: f_00007-2-1 loss: 0.597903  [   64/  179]
train() client id: f_00007-2-2 loss: 0.528858  [   96/  179]
train() client id: f_00007-2-3 loss: 0.835893  [  128/  179]
train() client id: f_00007-2-4 loss: 0.539059  [  160/  179]
train() client id: f_00007-3-0 loss: 0.656446  [   32/  179]
train() client id: f_00007-3-1 loss: 0.716931  [   64/  179]
train() client id: f_00007-3-2 loss: 0.397630  [   96/  179]
train() client id: f_00007-3-3 loss: 0.825260  [  128/  179]
train() client id: f_00007-3-4 loss: 0.472154  [  160/  179]
train() client id: f_00007-4-0 loss: 0.675584  [   32/  179]
train() client id: f_00007-4-1 loss: 0.534957  [   64/  179]
train() client id: f_00007-4-2 loss: 0.662739  [   96/  179]
train() client id: f_00007-4-3 loss: 0.472231  [  128/  179]
train() client id: f_00007-4-4 loss: 0.698056  [  160/  179]
train() client id: f_00007-5-0 loss: 0.691858  [   32/  179]
train() client id: f_00007-5-1 loss: 0.708411  [   64/  179]
train() client id: f_00007-5-2 loss: 0.543438  [   96/  179]
train() client id: f_00007-5-3 loss: 0.500291  [  128/  179]
train() client id: f_00007-5-4 loss: 0.513899  [  160/  179]
train() client id: f_00007-6-0 loss: 0.582579  [   32/  179]
train() client id: f_00007-6-1 loss: 0.591779  [   64/  179]
train() client id: f_00007-6-2 loss: 0.589710  [   96/  179]
train() client id: f_00007-6-3 loss: 0.637091  [  128/  179]
train() client id: f_00007-6-4 loss: 0.541404  [  160/  179]
train() client id: f_00007-7-0 loss: 0.456351  [   32/  179]
train() client id: f_00007-7-1 loss: 0.648235  [   64/  179]
train() client id: f_00007-7-2 loss: 0.660579  [   96/  179]
train() client id: f_00007-7-3 loss: 0.575930  [  128/  179]
train() client id: f_00007-7-4 loss: 0.782581  [  160/  179]
train() client id: f_00007-8-0 loss: 0.697315  [   32/  179]
train() client id: f_00007-8-1 loss: 0.462169  [   64/  179]
train() client id: f_00007-8-2 loss: 0.538976  [   96/  179]
train() client id: f_00007-8-3 loss: 0.577618  [  128/  179]
train() client id: f_00007-8-4 loss: 0.743367  [  160/  179]
train() client id: f_00007-9-0 loss: 0.582834  [   32/  179]
train() client id: f_00007-9-1 loss: 0.558152  [   64/  179]
train() client id: f_00007-9-2 loss: 0.620418  [   96/  179]
train() client id: f_00007-9-3 loss: 0.706267  [  128/  179]
train() client id: f_00007-9-4 loss: 0.550493  [  160/  179]
train() client id: f_00007-10-0 loss: 0.848628  [   32/  179]
train() client id: f_00007-10-1 loss: 0.436207  [   64/  179]
train() client id: f_00007-10-2 loss: 0.562160  [   96/  179]
train() client id: f_00007-10-3 loss: 0.599486  [  128/  179]
train() client id: f_00007-10-4 loss: 0.487824  [  160/  179]
train() client id: f_00008-0-0 loss: 0.878195  [   32/  130]
train() client id: f_00008-0-1 loss: 0.763434  [   64/  130]
train() client id: f_00008-0-2 loss: 0.710436  [   96/  130]
train() client id: f_00008-0-3 loss: 0.783959  [  128/  130]
train() client id: f_00008-1-0 loss: 0.873983  [   32/  130]
train() client id: f_00008-1-1 loss: 0.769368  [   64/  130]
train() client id: f_00008-1-2 loss: 0.747632  [   96/  130]
train() client id: f_00008-1-3 loss: 0.731337  [  128/  130]
train() client id: f_00008-2-0 loss: 0.878990  [   32/  130]
train() client id: f_00008-2-1 loss: 0.730704  [   64/  130]
train() client id: f_00008-2-2 loss: 0.727697  [   96/  130]
train() client id: f_00008-2-3 loss: 0.763380  [  128/  130]
train() client id: f_00008-3-0 loss: 0.830142  [   32/  130]
train() client id: f_00008-3-1 loss: 0.763916  [   64/  130]
train() client id: f_00008-3-2 loss: 0.762116  [   96/  130]
train() client id: f_00008-3-3 loss: 0.757969  [  128/  130]
train() client id: f_00008-4-0 loss: 0.778228  [   32/  130]
train() client id: f_00008-4-1 loss: 0.722903  [   64/  130]
train() client id: f_00008-4-2 loss: 0.829397  [   96/  130]
train() client id: f_00008-4-3 loss: 0.815896  [  128/  130]
train() client id: f_00008-5-0 loss: 0.648558  [   32/  130]
train() client id: f_00008-5-1 loss: 0.695310  [   64/  130]
train() client id: f_00008-5-2 loss: 0.768590  [   96/  130]
train() client id: f_00008-5-3 loss: 0.982266  [  128/  130]
train() client id: f_00008-6-0 loss: 0.775715  [   32/  130]
train() client id: f_00008-6-1 loss: 0.798079  [   64/  130]
train() client id: f_00008-6-2 loss: 0.759893  [   96/  130]
train() client id: f_00008-6-3 loss: 0.801742  [  128/  130]
train() client id: f_00008-7-0 loss: 0.758226  [   32/  130]
train() client id: f_00008-7-1 loss: 0.870704  [   64/  130]
train() client id: f_00008-7-2 loss: 0.704042  [   96/  130]
train() client id: f_00008-7-3 loss: 0.798359  [  128/  130]
train() client id: f_00008-8-0 loss: 0.713440  [   32/  130]
train() client id: f_00008-8-1 loss: 0.681041  [   64/  130]
train() client id: f_00008-8-2 loss: 1.013437  [   96/  130]
train() client id: f_00008-8-3 loss: 0.707511  [  128/  130]
train() client id: f_00008-9-0 loss: 0.813980  [   32/  130]
train() client id: f_00008-9-1 loss: 0.731393  [   64/  130]
train() client id: f_00008-9-2 loss: 0.818901  [   96/  130]
train() client id: f_00008-9-3 loss: 0.742092  [  128/  130]
train() client id: f_00008-10-0 loss: 0.810991  [   32/  130]
train() client id: f_00008-10-1 loss: 0.752737  [   64/  130]
train() client id: f_00008-10-2 loss: 0.753030  [   96/  130]
train() client id: f_00008-10-3 loss: 0.800180  [  128/  130]
train() client id: f_00009-0-0 loss: 1.065890  [   32/  118]
train() client id: f_00009-0-1 loss: 1.087899  [   64/  118]
train() client id: f_00009-0-2 loss: 1.187668  [   96/  118]
train() client id: f_00009-1-0 loss: 1.024194  [   32/  118]
train() client id: f_00009-1-1 loss: 1.042860  [   64/  118]
train() client id: f_00009-1-2 loss: 0.889495  [   96/  118]
train() client id: f_00009-2-0 loss: 1.062031  [   32/  118]
train() client id: f_00009-2-1 loss: 0.971780  [   64/  118]
train() client id: f_00009-2-2 loss: 0.973042  [   96/  118]
train() client id: f_00009-3-0 loss: 0.913129  [   32/  118]
train() client id: f_00009-3-1 loss: 1.036834  [   64/  118]
train() client id: f_00009-3-2 loss: 1.038947  [   96/  118]
train() client id: f_00009-4-0 loss: 1.096759  [   32/  118]
train() client id: f_00009-4-1 loss: 0.952490  [   64/  118]
train() client id: f_00009-4-2 loss: 0.930980  [   96/  118]
train() client id: f_00009-5-0 loss: 0.918410  [   32/  118]
train() client id: f_00009-5-1 loss: 0.827672  [   64/  118]
train() client id: f_00009-5-2 loss: 1.045430  [   96/  118]
train() client id: f_00009-6-0 loss: 1.019191  [   32/  118]
train() client id: f_00009-6-1 loss: 0.757547  [   64/  118]
train() client id: f_00009-6-2 loss: 0.925323  [   96/  118]
train() client id: f_00009-7-0 loss: 0.951335  [   32/  118]
train() client id: f_00009-7-1 loss: 0.838870  [   64/  118]
train() client id: f_00009-7-2 loss: 0.918440  [   96/  118]
train() client id: f_00009-8-0 loss: 0.969248  [   32/  118]
train() client id: f_00009-8-1 loss: 0.734582  [   64/  118]
train() client id: f_00009-8-2 loss: 1.021256  [   96/  118]
train() client id: f_00009-9-0 loss: 0.824506  [   32/  118]
train() client id: f_00009-9-1 loss: 0.794862  [   64/  118]
train() client id: f_00009-9-2 loss: 1.045399  [   96/  118]
train() client id: f_00009-10-0 loss: 0.886147  [   32/  118]
train() client id: f_00009-10-1 loss: 0.860322  [   64/  118]
train() client id: f_00009-10-2 loss: 0.977884  [   96/  118]
At round 57 accuracy: 0.649867374005305
At round 57 training accuracy: 0.5888665325285044
At round 57 training loss: 0.8261369742796227
update_location
xs = -3.905658 4.200318 305.009024 18.811294 0.979296 3.956410 -267.443192 -246.324852 289.663977 -232.060879 
ys = 297.587959 280.555839 1.320614 -267.455176 259.350187 242.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 313.964724 297.874506 320.986368 286.157537 277.963088 262.629711 285.539404 265.850727 306.942811 252.721711 
dists_bs = 210.478221 207.977857 510.463703 483.041821 195.150370 191.374386 200.240802 188.215539 490.608937 180.267682 
uav_gains = -116.840799 -115.552108 -117.355498 -114.533395 -113.795292 -112.403306 -114.478269 -112.693728 -116.296360 -111.529219 
bs_gains = -104.616860 -104.471538 -115.390075 -114.718632 -103.697402 -103.459806 -104.010531 -103.257412 -114.907652 -102.732760 
Round 58
-------------------------------
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.3523265  6.77756785 3.31084036 1.21735079 7.81367428 3.76005882
 1.49535056 4.64371042 3.4255904  3.04870737]
obj_prev = 38.84517734952248
eta_min = 2.470941254922374e-28	eta_max = 0.9375517380484754
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 8.931115437766236	eta = 0.909090909090909
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 20.2431781023763	eta = 0.40108306173335817
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 14.165631330058979	eta = 0.5731615953668162
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 13.085726257080514	eta = 0.6204619975235665
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 13.020212302687447	eta = 0.623583983407007
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 13.019941071601606	eta = 0.6235969738929089
af = 8.119195852514759	bf = 1.0664469018281375	zeta = 13.01994106692125	eta = 0.623596974117077
eta = 0.623596974117077
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [0.03896902 0.08195865 0.03835046 0.01329895 0.09463905 0.04515458
 0.01670101 0.05536075 0.04020613 0.0364948 ]
ene_total = [1.28342906 1.9813468  1.29389683 0.63079383 2.25502199 1.16000083
 0.7042715  1.51102723 1.23695895 0.96319404]
ti_comp = [1.03546301 1.14942413 1.02453888 1.07419893 1.1523401  1.15319241
 1.07495744 1.09635059 1.06887034 1.15568559]
ti_coms = [0.19129579 0.07733467 0.20221992 0.15255987 0.0744187  0.07356639
 0.15180136 0.13040821 0.15788846 0.07107321]
t_total = [27.09975662 27.09975662 27.09975662 27.09975662 27.09975662 27.09975662
 27.09975662 27.09975662 27.09975662 27.09975662]
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [3.44960441e-06 2.60437692e-05 3.35842002e-06 1.27398049e-07
 3.98960382e-05 4.32694980e-06 2.51956506e-07 8.82240566e-06
 3.55555046e-06 2.27453939e-06]
ene_total = [0.42265881 0.17141173 0.4467888  0.33701571 0.16527621 0.16260767
 0.33534287 0.28827359 0.34886259 0.15705475]
optimize_network iter = 0 obj = 2.83529273119976
eta = 0.623596974117077
freqs = [18817195.6470524  35652050.01875965 18715963.43024579  6190171.56284024
 41063853.09945352 19578076.71969618  7768218.52091367 25247740.71552675
 18807768.58899782 15789240.96805261]
eta_min = 0.6235969741170779	eta_max = 0.7095637697055857
af = 0.0016217254572042322	bf = 1.0664469018281375	zeta = 0.0017838980029246557	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [6.96202700e-07 5.25618019e-06 6.77799772e-07 2.57116048e-08
 8.05185933e-06 8.73269445e-07 5.08501203e-08 1.78054696e-06
 7.17584839e-07 4.59049871e-07]
ene_total = [1.74973877 0.70781749 1.84965402 1.39538178 0.68140253 0.67295033
 1.38844638 1.19293349 1.4441827  0.65010868]
ti_comp = [0.75528321 0.86924434 0.74435908 0.79401913 0.8721603  0.87301261
 0.79477765 0.81617079 0.78869054 0.8755058 ]
ti_coms = [0.19129579 0.07733467 0.20221992 0.15255987 0.0744187  0.07356639
 0.15180136 0.13040821 0.15788846 0.07107321]
t_total = [27.09975662 27.09975662 27.09975662 27.09975662 27.09975662 27.09975662
 27.09975662 27.09975662 27.09975662 27.09975662]
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [2.48645699e-06 1.74639655e-05 2.43999568e-06 8.94195368e-08
 2.67091848e-05 2.89538630e-06 1.76757958e-07 6.10500434e-06
 2.50441200e-06 1.51990678e-06]
ene_total = [0.54773486 0.22190259 0.5790084  0.43676858 0.21381912 0.21069725
 0.43459951 0.37352246 0.45209302 0.2035201 ]
optimize_network iter = 1 obj = 3.6736658995998672
eta = 0.7095637697055857
freqs = [18742766.932176   34251295.00082695 18715963.43024578  6084298.06393133
 39418319.48246514 18789072.00779956  7633454.28294466 24640242.31966294
 18518666.61499257 15142443.926058  ]
eta_min = 0.7095637697056919	eta_max = 0.7095637697055788
af = 0.0015119964848909963	bf = 1.0664469018281375	zeta = 0.001663196133380096	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [6.90706132e-07 4.85126751e-06 6.77799772e-07 2.48396102e-08
 7.41947189e-06 8.04301492e-07 4.91011128e-08 1.69589257e-06
 6.95693805e-07 4.22210775e-07]
ene_total = [1.74973827 0.70778046 1.84965402 1.3953817  0.68134469 0.67294402
 1.38844622 1.19292575 1.44418069 0.65010531]
ti_comp = [0.75528321 0.86924434 0.74435908 0.79401913 0.8721603  0.87301261
 0.79477765 0.81617079 0.78869054 0.8755058 ]
ti_coms = [0.19129579 0.07733467 0.20221992 0.15255987 0.0744187  0.07356639
 0.15180136 0.13040821 0.15788846 0.07107321]
t_total = [27.09975662 27.09975662 27.09975662 27.09975662 27.09975662 27.09975662
 27.09975662 27.09975662 27.09975662 27.09975662]
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [2.48645699e-06 1.74639655e-05 2.43999568e-06 8.94195368e-08
 2.67091848e-05 2.89538630e-06 1.76757958e-07 6.10500434e-06
 2.50441200e-06 1.51990678e-06]
ene_total = [0.54773486 0.22190259 0.5790084  0.43676858 0.21381912 0.21069725
 0.43459951 0.37352246 0.45209302 0.2035201 ]
optimize_network iter = 2 obj = 3.67366589959978
eta = 0.7095637697055788
freqs = [18742766.93217598 34251295.00082704 18715963.43024575  6084298.06393133
 39418319.48246525 18789072.00779961  7633454.28294467 24640242.31966297
 18518666.61499257 15142443.92605804]
Done!
ene_coms = [0.01912958 0.00773347 0.02022199 0.01525599 0.00744187 0.00735664
 0.01518014 0.01304082 0.01578885 0.00710732]
ene_comp = [2.43444879e-06 1.70986789e-05 2.38895930e-06 8.75491851e-08
 2.61505198e-05 2.83482469e-06 1.73060784e-07 5.97730848e-06
 2.45202824e-06 1.48811552e-06]
ene_total = [0.01913201 0.00775057 0.02022438 0.01525607 0.00746802 0.00735947
 0.01518031 0.0130468  0.0157913  0.00710881]
At round 58 energy consumption: 0.12831774437155147
At round 58 eta: 0.7095637697055788
At round 58 a_n: 8.314943732033974
At round 58 local rounds: 11.234997825322132
At round 58 global rounds: 28.629154577598474
gradient difference: 0.45051687955856323
train() client id: f_00000-0-0 loss: 1.100043  [   32/  126]
train() client id: f_00000-0-1 loss: 1.332674  [   64/  126]
train() client id: f_00000-0-2 loss: 1.378783  [   96/  126]
train() client id: f_00000-1-0 loss: 1.327242  [   32/  126]
train() client id: f_00000-1-1 loss: 1.174029  [   64/  126]
train() client id: f_00000-1-2 loss: 0.888324  [   96/  126]
train() client id: f_00000-2-0 loss: 1.280682  [   32/  126]
train() client id: f_00000-2-1 loss: 1.036864  [   64/  126]
train() client id: f_00000-2-2 loss: 1.011576  [   96/  126]
train() client id: f_00000-3-0 loss: 1.183163  [   32/  126]
train() client id: f_00000-3-1 loss: 0.933113  [   64/  126]
train() client id: f_00000-3-2 loss: 1.183454  [   96/  126]
train() client id: f_00000-4-0 loss: 1.064334  [   32/  126]
train() client id: f_00000-4-1 loss: 1.079042  [   64/  126]
train() client id: f_00000-4-2 loss: 0.942029  [   96/  126]
train() client id: f_00000-5-0 loss: 1.120761  [   32/  126]
train() client id: f_00000-5-1 loss: 0.918091  [   64/  126]
train() client id: f_00000-5-2 loss: 1.055669  [   96/  126]
train() client id: f_00000-6-0 loss: 1.049370  [   32/  126]
train() client id: f_00000-6-1 loss: 0.976921  [   64/  126]
train() client id: f_00000-6-2 loss: 0.985075  [   96/  126]
train() client id: f_00000-7-0 loss: 0.938934  [   32/  126]
train() client id: f_00000-7-1 loss: 0.927579  [   64/  126]
train() client id: f_00000-7-2 loss: 1.060341  [   96/  126]
train() client id: f_00000-8-0 loss: 0.946618  [   32/  126]
train() client id: f_00000-8-1 loss: 0.960463  [   64/  126]
train() client id: f_00000-8-2 loss: 0.971979  [   96/  126]
train() client id: f_00000-9-0 loss: 1.033647  [   32/  126]
train() client id: f_00000-9-1 loss: 1.040002  [   64/  126]
train() client id: f_00000-9-2 loss: 0.902938  [   96/  126]
train() client id: f_00000-10-0 loss: 0.899330  [   32/  126]
train() client id: f_00000-10-1 loss: 0.976593  [   64/  126]
train() client id: f_00000-10-2 loss: 1.039527  [   96/  126]
train() client id: f_00001-0-0 loss: 0.412049  [   32/  265]
train() client id: f_00001-0-1 loss: 0.520703  [   64/  265]
train() client id: f_00001-0-2 loss: 0.448947  [   96/  265]
train() client id: f_00001-0-3 loss: 0.795753  [  128/  265]
train() client id: f_00001-0-4 loss: 0.524442  [  160/  265]
train() client id: f_00001-0-5 loss: 0.481469  [  192/  265]
train() client id: f_00001-0-6 loss: 0.508167  [  224/  265]
train() client id: f_00001-0-7 loss: 0.398916  [  256/  265]
train() client id: f_00001-1-0 loss: 0.555335  [   32/  265]
train() client id: f_00001-1-1 loss: 0.525748  [   64/  265]
train() client id: f_00001-1-2 loss: 0.424848  [   96/  265]
train() client id: f_00001-1-3 loss: 0.595326  [  128/  265]
train() client id: f_00001-1-4 loss: 0.502439  [  160/  265]
train() client id: f_00001-1-5 loss: 0.516257  [  192/  265]
train() client id: f_00001-1-6 loss: 0.442638  [  224/  265]
train() client id: f_00001-1-7 loss: 0.532599  [  256/  265]
train() client id: f_00001-2-0 loss: 0.541096  [   32/  265]
train() client id: f_00001-2-1 loss: 0.478161  [   64/  265]
train() client id: f_00001-2-2 loss: 0.501726  [   96/  265]
train() client id: f_00001-2-3 loss: 0.615284  [  128/  265]
train() client id: f_00001-2-4 loss: 0.427967  [  160/  265]
train() client id: f_00001-2-5 loss: 0.388183  [  192/  265]
train() client id: f_00001-2-6 loss: 0.484074  [  224/  265]
train() client id: f_00001-2-7 loss: 0.592911  [  256/  265]
train() client id: f_00001-3-0 loss: 0.572278  [   32/  265]
train() client id: f_00001-3-1 loss: 0.462742  [   64/  265]
train() client id: f_00001-3-2 loss: 0.411232  [   96/  265]
train() client id: f_00001-3-3 loss: 0.562583  [  128/  265]
train() client id: f_00001-3-4 loss: 0.541754  [  160/  265]
train() client id: f_00001-3-5 loss: 0.416342  [  192/  265]
train() client id: f_00001-3-6 loss: 0.554565  [  224/  265]
train() client id: f_00001-3-7 loss: 0.513729  [  256/  265]
train() client id: f_00001-4-0 loss: 0.458613  [   32/  265]
train() client id: f_00001-4-1 loss: 0.664181  [   64/  265]
train() client id: f_00001-4-2 loss: 0.446953  [   96/  265]
train() client id: f_00001-4-3 loss: 0.478324  [  128/  265]
train() client id: f_00001-4-4 loss: 0.432639  [  160/  265]
train() client id: f_00001-4-5 loss: 0.532242  [  192/  265]
train() client id: f_00001-4-6 loss: 0.422747  [  224/  265]
train() client id: f_00001-4-7 loss: 0.588566  [  256/  265]
train() client id: f_00001-5-0 loss: 0.457291  [   32/  265]
train() client id: f_00001-5-1 loss: 0.491165  [   64/  265]
train() client id: f_00001-5-2 loss: 0.525460  [   96/  265]
train() client id: f_00001-5-3 loss: 0.532805  [  128/  265]
train() client id: f_00001-5-4 loss: 0.511522  [  160/  265]
train() client id: f_00001-5-5 loss: 0.461634  [  192/  265]
train() client id: f_00001-5-6 loss: 0.547023  [  224/  265]
train() client id: f_00001-5-7 loss: 0.463271  [  256/  265]
train() client id: f_00001-6-0 loss: 0.546387  [   32/  265]
train() client id: f_00001-6-1 loss: 0.551865  [   64/  265]
train() client id: f_00001-6-2 loss: 0.395616  [   96/  265]
train() client id: f_00001-6-3 loss: 0.413834  [  128/  265]
train() client id: f_00001-6-4 loss: 0.552847  [  160/  265]
train() client id: f_00001-6-5 loss: 0.452192  [  192/  265]
train() client id: f_00001-6-6 loss: 0.556772  [  224/  265]
train() client id: f_00001-6-7 loss: 0.530159  [  256/  265]
train() client id: f_00001-7-0 loss: 0.505254  [   32/  265]
train() client id: f_00001-7-1 loss: 0.519643  [   64/  265]
train() client id: f_00001-7-2 loss: 0.412207  [   96/  265]
train() client id: f_00001-7-3 loss: 0.503210  [  128/  265]
train() client id: f_00001-7-4 loss: 0.619786  [  160/  265]
train() client id: f_00001-7-5 loss: 0.492838  [  192/  265]
train() client id: f_00001-7-6 loss: 0.441679  [  224/  265]
train() client id: f_00001-7-7 loss: 0.492867  [  256/  265]
train() client id: f_00001-8-0 loss: 0.584540  [   32/  265]
train() client id: f_00001-8-1 loss: 0.470189  [   64/  265]
train() client id: f_00001-8-2 loss: 0.517693  [   96/  265]
train() client id: f_00001-8-3 loss: 0.397519  [  128/  265]
train() client id: f_00001-8-4 loss: 0.510513  [  160/  265]
train() client id: f_00001-8-5 loss: 0.590779  [  192/  265]
train() client id: f_00001-8-6 loss: 0.433071  [  224/  265]
train() client id: f_00001-8-7 loss: 0.489031  [  256/  265]
train() client id: f_00001-9-0 loss: 0.454800  [   32/  265]
train() client id: f_00001-9-1 loss: 0.370200  [   64/  265]
train() client id: f_00001-9-2 loss: 0.446682  [   96/  265]
train() client id: f_00001-9-3 loss: 0.405091  [  128/  265]
train() client id: f_00001-9-4 loss: 0.544055  [  160/  265]
train() client id: f_00001-9-5 loss: 0.554029  [  192/  265]
train() client id: f_00001-9-6 loss: 0.640292  [  224/  265]
train() client id: f_00001-9-7 loss: 0.503324  [  256/  265]
train() client id: f_00001-10-0 loss: 0.479548  [   32/  265]
train() client id: f_00001-10-1 loss: 0.406438  [   64/  265]
train() client id: f_00001-10-2 loss: 0.503036  [   96/  265]
train() client id: f_00001-10-3 loss: 0.425844  [  128/  265]
train() client id: f_00001-10-4 loss: 0.557735  [  160/  265]
train() client id: f_00001-10-5 loss: 0.579910  [  192/  265]
train() client id: f_00001-10-6 loss: 0.478828  [  224/  265]
train() client id: f_00001-10-7 loss: 0.482736  [  256/  265]
train() client id: f_00002-0-0 loss: 1.124115  [   32/  124]
train() client id: f_00002-0-1 loss: 1.110425  [   64/  124]
train() client id: f_00002-0-2 loss: 1.158691  [   96/  124]
train() client id: f_00002-1-0 loss: 1.160134  [   32/  124]
train() client id: f_00002-1-1 loss: 1.210735  [   64/  124]
train() client id: f_00002-1-2 loss: 1.129184  [   96/  124]
train() client id: f_00002-2-0 loss: 1.033209  [   32/  124]
train() client id: f_00002-2-1 loss: 1.172056  [   64/  124]
train() client id: f_00002-2-2 loss: 1.187036  [   96/  124]
train() client id: f_00002-3-0 loss: 1.041627  [   32/  124]
train() client id: f_00002-3-1 loss: 1.079405  [   64/  124]
train() client id: f_00002-3-2 loss: 1.069873  [   96/  124]
train() client id: f_00002-4-0 loss: 1.038846  [   32/  124]
train() client id: f_00002-4-1 loss: 1.077652  [   64/  124]
train() client id: f_00002-4-2 loss: 0.915137  [   96/  124]
train() client id: f_00002-5-0 loss: 1.089424  [   32/  124]
train() client id: f_00002-5-1 loss: 1.090449  [   64/  124]
train() client id: f_00002-5-2 loss: 1.089138  [   96/  124]
train() client id: f_00002-6-0 loss: 0.948188  [   32/  124]
train() client id: f_00002-6-1 loss: 0.933045  [   64/  124]
train() client id: f_00002-6-2 loss: 0.978452  [   96/  124]
train() client id: f_00002-7-0 loss: 0.980920  [   32/  124]
train() client id: f_00002-7-1 loss: 0.907452  [   64/  124]
train() client id: f_00002-7-2 loss: 1.027680  [   96/  124]
train() client id: f_00002-8-0 loss: 1.034545  [   32/  124]
train() client id: f_00002-8-1 loss: 0.941991  [   64/  124]
train() client id: f_00002-8-2 loss: 0.864294  [   96/  124]
train() client id: f_00002-9-0 loss: 0.973186  [   32/  124]
train() client id: f_00002-9-1 loss: 0.819111  [   64/  124]
train() client id: f_00002-9-2 loss: 1.018770  [   96/  124]
train() client id: f_00002-10-0 loss: 0.999055  [   32/  124]
train() client id: f_00002-10-1 loss: 0.893341  [   64/  124]
train() client id: f_00002-10-2 loss: 0.930137  [   96/  124]
train() client id: f_00003-0-0 loss: 0.636220  [   32/   43]
train() client id: f_00003-1-0 loss: 0.976568  [   32/   43]
train() client id: f_00003-2-0 loss: 0.701455  [   32/   43]
train() client id: f_00003-3-0 loss: 0.765055  [   32/   43]
train() client id: f_00003-4-0 loss: 0.612854  [   32/   43]
train() client id: f_00003-5-0 loss: 0.594819  [   32/   43]
train() client id: f_00003-6-0 loss: 0.659769  [   32/   43]
train() client id: f_00003-7-0 loss: 0.687118  [   32/   43]
train() client id: f_00003-8-0 loss: 0.692156  [   32/   43]
train() client id: f_00003-9-0 loss: 0.819399  [   32/   43]
train() client id: f_00003-10-0 loss: 0.875605  [   32/   43]
train() client id: f_00004-0-0 loss: 0.600558  [   32/  306]
train() client id: f_00004-0-1 loss: 0.807905  [   64/  306]
train() client id: f_00004-0-2 loss: 0.707186  [   96/  306]
train() client id: f_00004-0-3 loss: 0.627026  [  128/  306]
train() client id: f_00004-0-4 loss: 0.832073  [  160/  306]
train() client id: f_00004-0-5 loss: 0.745517  [  192/  306]
train() client id: f_00004-0-6 loss: 0.839038  [  224/  306]
train() client id: f_00004-0-7 loss: 0.619543  [  256/  306]
train() client id: f_00004-0-8 loss: 0.599843  [  288/  306]
train() client id: f_00004-1-0 loss: 0.563082  [   32/  306]
train() client id: f_00004-1-1 loss: 0.874458  [   64/  306]
train() client id: f_00004-1-2 loss: 0.623322  [   96/  306]
train() client id: f_00004-1-3 loss: 0.908848  [  128/  306]
train() client id: f_00004-1-4 loss: 0.605911  [  160/  306]
train() client id: f_00004-1-5 loss: 0.624687  [  192/  306]
train() client id: f_00004-1-6 loss: 0.656307  [  224/  306]
train() client id: f_00004-1-7 loss: 0.615021  [  256/  306]
train() client id: f_00004-1-8 loss: 0.951346  [  288/  306]
train() client id: f_00004-2-0 loss: 0.724199  [   32/  306]
train() client id: f_00004-2-1 loss: 0.749825  [   64/  306]
train() client id: f_00004-2-2 loss: 0.697712  [   96/  306]
train() client id: f_00004-2-3 loss: 0.740774  [  128/  306]
train() client id: f_00004-2-4 loss: 0.624648  [  160/  306]
train() client id: f_00004-2-5 loss: 0.617839  [  192/  306]
train() client id: f_00004-2-6 loss: 0.763920  [  224/  306]
train() client id: f_00004-2-7 loss: 0.839403  [  256/  306]
train() client id: f_00004-2-8 loss: 0.764022  [  288/  306]
train() client id: f_00004-3-0 loss: 0.702631  [   32/  306]
train() client id: f_00004-3-1 loss: 0.747626  [   64/  306]
train() client id: f_00004-3-2 loss: 0.762876  [   96/  306]
train() client id: f_00004-3-3 loss: 0.839752  [  128/  306]
train() client id: f_00004-3-4 loss: 0.625959  [  160/  306]
train() client id: f_00004-3-5 loss: 0.701636  [  192/  306]
train() client id: f_00004-3-6 loss: 0.744924  [  224/  306]
train() client id: f_00004-3-7 loss: 0.618726  [  256/  306]
train() client id: f_00004-3-8 loss: 0.741711  [  288/  306]
train() client id: f_00004-4-0 loss: 0.849732  [   32/  306]
train() client id: f_00004-4-1 loss: 0.687017  [   64/  306]
train() client id: f_00004-4-2 loss: 0.589719  [   96/  306]
train() client id: f_00004-4-3 loss: 0.665968  [  128/  306]
train() client id: f_00004-4-4 loss: 0.830991  [  160/  306]
train() client id: f_00004-4-5 loss: 0.775250  [  192/  306]
train() client id: f_00004-4-6 loss: 0.693830  [  224/  306]
train() client id: f_00004-4-7 loss: 0.725009  [  256/  306]
train() client id: f_00004-4-8 loss: 0.682425  [  288/  306]
train() client id: f_00004-5-0 loss: 0.793253  [   32/  306]
train() client id: f_00004-5-1 loss: 0.541401  [   64/  306]
train() client id: f_00004-5-2 loss: 0.703245  [   96/  306]
train() client id: f_00004-5-3 loss: 0.747452  [  128/  306]
train() client id: f_00004-5-4 loss: 0.908623  [  160/  306]
train() client id: f_00004-5-5 loss: 0.692554  [  192/  306]
train() client id: f_00004-5-6 loss: 0.560966  [  224/  306]
train() client id: f_00004-5-7 loss: 0.823860  [  256/  306]
train() client id: f_00004-5-8 loss: 0.847421  [  288/  306]
train() client id: f_00004-6-0 loss: 0.862412  [   32/  306]
train() client id: f_00004-6-1 loss: 0.735574  [   64/  306]
train() client id: f_00004-6-2 loss: 0.798732  [   96/  306]
train() client id: f_00004-6-3 loss: 0.633317  [  128/  306]
train() client id: f_00004-6-4 loss: 0.767816  [  160/  306]
train() client id: f_00004-6-5 loss: 0.695398  [  192/  306]
train() client id: f_00004-6-6 loss: 0.752536  [  224/  306]
train() client id: f_00004-6-7 loss: 0.727769  [  256/  306]
train() client id: f_00004-6-8 loss: 0.660186  [  288/  306]
train() client id: f_00004-7-0 loss: 0.662285  [   32/  306]
train() client id: f_00004-7-1 loss: 0.745996  [   64/  306]
train() client id: f_00004-7-2 loss: 0.612228  [   96/  306]
train() client id: f_00004-7-3 loss: 0.795982  [  128/  306]
train() client id: f_00004-7-4 loss: 0.724354  [  160/  306]
train() client id: f_00004-7-5 loss: 0.817571  [  192/  306]
train() client id: f_00004-7-6 loss: 0.925583  [  224/  306]
train() client id: f_00004-7-7 loss: 0.598207  [  256/  306]
train() client id: f_00004-7-8 loss: 0.701036  [  288/  306]
train() client id: f_00004-8-0 loss: 0.841729  [   32/  306]
train() client id: f_00004-8-1 loss: 0.705316  [   64/  306]
train() client id: f_00004-8-2 loss: 0.728063  [   96/  306]
train() client id: f_00004-8-3 loss: 0.803940  [  128/  306]
train() client id: f_00004-8-4 loss: 0.808776  [  160/  306]
train() client id: f_00004-8-5 loss: 0.667093  [  192/  306]
train() client id: f_00004-8-6 loss: 0.772839  [  224/  306]
train() client id: f_00004-8-7 loss: 0.698106  [  256/  306]
train() client id: f_00004-8-8 loss: 0.659406  [  288/  306]
train() client id: f_00004-9-0 loss: 0.771806  [   32/  306]
train() client id: f_00004-9-1 loss: 0.731048  [   64/  306]
train() client id: f_00004-9-2 loss: 0.703995  [   96/  306]
train() client id: f_00004-9-3 loss: 0.825453  [  128/  306]
train() client id: f_00004-9-4 loss: 0.745039  [  160/  306]
train() client id: f_00004-9-5 loss: 0.785661  [  192/  306]
train() client id: f_00004-9-6 loss: 0.618716  [  224/  306]
train() client id: f_00004-9-7 loss: 0.754904  [  256/  306]
train() client id: f_00004-9-8 loss: 0.713790  [  288/  306]
train() client id: f_00004-10-0 loss: 0.880192  [   32/  306]
train() client id: f_00004-10-1 loss: 0.635993  [   64/  306]
train() client id: f_00004-10-2 loss: 0.878216  [   96/  306]
train() client id: f_00004-10-3 loss: 0.739863  [  128/  306]
train() client id: f_00004-10-4 loss: 0.716877  [  160/  306]
train() client id: f_00004-10-5 loss: 0.745921  [  192/  306]
train() client id: f_00004-10-6 loss: 0.810311  [  224/  306]
train() client id: f_00004-10-7 loss: 0.676604  [  256/  306]
train() client id: f_00004-10-8 loss: 0.711177  [  288/  306]
train() client id: f_00005-0-0 loss: 0.320801  [   32/  146]
train() client id: f_00005-0-1 loss: 0.650045  [   64/  146]
train() client id: f_00005-0-2 loss: 0.807303  [   96/  146]
train() client id: f_00005-0-3 loss: 0.598606  [  128/  146]
train() client id: f_00005-1-0 loss: 0.354535  [   32/  146]
train() client id: f_00005-1-1 loss: 0.929950  [   64/  146]
train() client id: f_00005-1-2 loss: 0.541389  [   96/  146]
train() client id: f_00005-1-3 loss: 0.727005  [  128/  146]
train() client id: f_00005-2-0 loss: 0.687385  [   32/  146]
train() client id: f_00005-2-1 loss: 0.718306  [   64/  146]
train() client id: f_00005-2-2 loss: 0.316972  [   96/  146]
train() client id: f_00005-2-3 loss: 0.549969  [  128/  146]
train() client id: f_00005-3-0 loss: 0.919223  [   32/  146]
train() client id: f_00005-3-1 loss: 0.465893  [   64/  146]
train() client id: f_00005-3-2 loss: 0.475826  [   96/  146]
train() client id: f_00005-3-3 loss: 0.578263  [  128/  146]
train() client id: f_00005-4-0 loss: 0.601204  [   32/  146]
train() client id: f_00005-4-1 loss: 0.642644  [   64/  146]
train() client id: f_00005-4-2 loss: 0.652243  [   96/  146]
train() client id: f_00005-4-3 loss: 0.521551  [  128/  146]
train() client id: f_00005-5-0 loss: 0.474371  [   32/  146]
train() client id: f_00005-5-1 loss: 0.783931  [   64/  146]
train() client id: f_00005-5-2 loss: 0.515681  [   96/  146]
train() client id: f_00005-5-3 loss: 0.485466  [  128/  146]
train() client id: f_00005-6-0 loss: 0.410658  [   32/  146]
train() client id: f_00005-6-1 loss: 0.598497  [   64/  146]
train() client id: f_00005-6-2 loss: 0.649986  [   96/  146]
train() client id: f_00005-6-3 loss: 0.442953  [  128/  146]
train() client id: f_00005-7-0 loss: 0.863383  [   32/  146]
train() client id: f_00005-7-1 loss: 0.469157  [   64/  146]
train() client id: f_00005-7-2 loss: 0.543180  [   96/  146]
train() client id: f_00005-7-3 loss: 0.677144  [  128/  146]
train() client id: f_00005-8-0 loss: 0.475147  [   32/  146]
train() client id: f_00005-8-1 loss: 0.419904  [   64/  146]
train() client id: f_00005-8-2 loss: 0.924903  [   96/  146]
train() client id: f_00005-8-3 loss: 0.639739  [  128/  146]
train() client id: f_00005-9-0 loss: 0.447072  [   32/  146]
train() client id: f_00005-9-1 loss: 0.819997  [   64/  146]
train() client id: f_00005-9-2 loss: 0.411882  [   96/  146]
train() client id: f_00005-9-3 loss: 0.490466  [  128/  146]
train() client id: f_00005-10-0 loss: 0.753887  [   32/  146]
train() client id: f_00005-10-1 loss: 0.493049  [   64/  146]
train() client id: f_00005-10-2 loss: 0.620258  [   96/  146]
train() client id: f_00005-10-3 loss: 0.572582  [  128/  146]
train() client id: f_00006-0-0 loss: 0.525900  [   32/   54]
train() client id: f_00006-1-0 loss: 0.525220  [   32/   54]
train() client id: f_00006-2-0 loss: 0.442911  [   32/   54]
train() client id: f_00006-3-0 loss: 0.438030  [   32/   54]
train() client id: f_00006-4-0 loss: 0.474964  [   32/   54]
train() client id: f_00006-5-0 loss: 0.466423  [   32/   54]
train() client id: f_00006-6-0 loss: 0.525717  [   32/   54]
train() client id: f_00006-7-0 loss: 0.534771  [   32/   54]
train() client id: f_00006-8-0 loss: 0.536481  [   32/   54]
train() client id: f_00006-9-0 loss: 0.468851  [   32/   54]
train() client id: f_00006-10-0 loss: 0.466421  [   32/   54]
train() client id: f_00007-0-0 loss: 0.875202  [   32/  179]
train() client id: f_00007-0-1 loss: 0.841465  [   64/  179]
train() client id: f_00007-0-2 loss: 0.581699  [   96/  179]
train() client id: f_00007-0-3 loss: 0.678656  [  128/  179]
train() client id: f_00007-0-4 loss: 0.665801  [  160/  179]
train() client id: f_00007-1-0 loss: 0.789683  [   32/  179]
train() client id: f_00007-1-1 loss: 0.846386  [   64/  179]
train() client id: f_00007-1-2 loss: 0.678229  [   96/  179]
train() client id: f_00007-1-3 loss: 0.736307  [  128/  179]
train() client id: f_00007-1-4 loss: 0.562148  [  160/  179]
train() client id: f_00007-2-0 loss: 0.678771  [   32/  179]
train() client id: f_00007-2-1 loss: 0.928937  [   64/  179]
train() client id: f_00007-2-2 loss: 0.651852  [   96/  179]
train() client id: f_00007-2-3 loss: 0.529539  [  128/  179]
train() client id: f_00007-2-4 loss: 0.705285  [  160/  179]
train() client id: f_00007-3-0 loss: 0.636291  [   32/  179]
train() client id: f_00007-3-1 loss: 0.738111  [   64/  179]
train() client id: f_00007-3-2 loss: 0.663218  [   96/  179]
train() client id: f_00007-3-3 loss: 0.657700  [  128/  179]
train() client id: f_00007-3-4 loss: 0.634775  [  160/  179]
train() client id: f_00007-4-0 loss: 0.600869  [   32/  179]
train() client id: f_00007-4-1 loss: 0.701174  [   64/  179]
train() client id: f_00007-4-2 loss: 0.827917  [   96/  179]
train() client id: f_00007-4-3 loss: 0.531306  [  128/  179]
train() client id: f_00007-4-4 loss: 0.857278  [  160/  179]
train() client id: f_00007-5-0 loss: 0.619233  [   32/  179]
train() client id: f_00007-5-1 loss: 0.719293  [   64/  179]
train() client id: f_00007-5-2 loss: 0.669702  [   96/  179]
train() client id: f_00007-5-3 loss: 0.640852  [  128/  179]
train() client id: f_00007-5-4 loss: 0.940412  [  160/  179]
train() client id: f_00007-6-0 loss: 0.720482  [   32/  179]
train() client id: f_00007-6-1 loss: 0.539697  [   64/  179]
train() client id: f_00007-6-2 loss: 0.664997  [   96/  179]
train() client id: f_00007-6-3 loss: 0.676419  [  128/  179]
train() client id: f_00007-6-4 loss: 0.922000  [  160/  179]
train() client id: f_00007-7-0 loss: 0.621275  [   32/  179]
train() client id: f_00007-7-1 loss: 0.577472  [   64/  179]
train() client id: f_00007-7-2 loss: 0.739788  [   96/  179]
train() client id: f_00007-7-3 loss: 0.503816  [  128/  179]
train() client id: f_00007-7-4 loss: 0.972674  [  160/  179]
train() client id: f_00007-8-0 loss: 0.820385  [   32/  179]
train() client id: f_00007-8-1 loss: 0.652659  [   64/  179]
train() client id: f_00007-8-2 loss: 0.632101  [   96/  179]
train() client id: f_00007-8-3 loss: 0.750241  [  128/  179]
train() client id: f_00007-8-4 loss: 0.762422  [  160/  179]
train() client id: f_00007-9-0 loss: 1.031874  [   32/  179]
train() client id: f_00007-9-1 loss: 0.635923  [   64/  179]
train() client id: f_00007-9-2 loss: 0.558893  [   96/  179]
train() client id: f_00007-9-3 loss: 0.629186  [  128/  179]
train() client id: f_00007-9-4 loss: 0.758605  [  160/  179]
train() client id: f_00007-10-0 loss: 0.686832  [   32/  179]
train() client id: f_00007-10-1 loss: 0.537818  [   64/  179]
train() client id: f_00007-10-2 loss: 0.935805  [   96/  179]
train() client id: f_00007-10-3 loss: 0.577565  [  128/  179]
train() client id: f_00007-10-4 loss: 0.757208  [  160/  179]
train() client id: f_00008-0-0 loss: 0.682664  [   32/  130]
train() client id: f_00008-0-1 loss: 0.620802  [   64/  130]
train() client id: f_00008-0-2 loss: 0.744523  [   96/  130]
train() client id: f_00008-0-3 loss: 0.671560  [  128/  130]
train() client id: f_00008-1-0 loss: 0.675333  [   32/  130]
train() client id: f_00008-1-1 loss: 0.717725  [   64/  130]
train() client id: f_00008-1-2 loss: 0.681990  [   96/  130]
train() client id: f_00008-1-3 loss: 0.661225  [  128/  130]
train() client id: f_00008-2-0 loss: 0.636871  [   32/  130]
train() client id: f_00008-2-1 loss: 0.781440  [   64/  130]
train() client id: f_00008-2-2 loss: 0.707005  [   96/  130]
train() client id: f_00008-2-3 loss: 0.608177  [  128/  130]
train() client id: f_00008-3-0 loss: 0.777349  [   32/  130]
train() client id: f_00008-3-1 loss: 0.588936  [   64/  130]
train() client id: f_00008-3-2 loss: 0.691818  [   96/  130]
train() client id: f_00008-3-3 loss: 0.653408  [  128/  130]
train() client id: f_00008-4-0 loss: 0.677523  [   32/  130]
train() client id: f_00008-4-1 loss: 0.650840  [   64/  130]
train() client id: f_00008-4-2 loss: 0.601921  [   96/  130]
train() client id: f_00008-4-3 loss: 0.774983  [  128/  130]
train() client id: f_00008-5-0 loss: 0.658392  [   32/  130]
train() client id: f_00008-5-1 loss: 0.596792  [   64/  130]
train() client id: f_00008-5-2 loss: 0.738470  [   96/  130]
train() client id: f_00008-5-3 loss: 0.713552  [  128/  130]
train() client id: f_00008-6-0 loss: 0.729403  [   32/  130]
train() client id: f_00008-6-1 loss: 0.673259  [   64/  130]
train() client id: f_00008-6-2 loss: 0.693293  [   96/  130]
train() client id: f_00008-6-3 loss: 0.622482  [  128/  130]
train() client id: f_00008-7-0 loss: 0.624406  [   32/  130]
train() client id: f_00008-7-1 loss: 0.792240  [   64/  130]
train() client id: f_00008-7-2 loss: 0.561605  [   96/  130]
train() client id: f_00008-7-3 loss: 0.712308  [  128/  130]
train() client id: f_00008-8-0 loss: 0.743441  [   32/  130]
train() client id: f_00008-8-1 loss: 0.627874  [   64/  130]
train() client id: f_00008-8-2 loss: 0.671834  [   96/  130]
train() client id: f_00008-8-3 loss: 0.657849  [  128/  130]
train() client id: f_00008-9-0 loss: 0.768723  [   32/  130]
train() client id: f_00008-9-1 loss: 0.642912  [   64/  130]
train() client id: f_00008-9-2 loss: 0.633931  [   96/  130]
train() client id: f_00008-9-3 loss: 0.665919  [  128/  130]
train() client id: f_00008-10-0 loss: 0.631859  [   32/  130]
train() client id: f_00008-10-1 loss: 0.680789  [   64/  130]
train() client id: f_00008-10-2 loss: 0.665644  [   96/  130]
train() client id: f_00008-10-3 loss: 0.699297  [  128/  130]
train() client id: f_00009-0-0 loss: 1.024439  [   32/  118]
train() client id: f_00009-0-1 loss: 0.815416  [   64/  118]
train() client id: f_00009-0-2 loss: 0.873663  [   96/  118]
train() client id: f_00009-1-0 loss: 0.692235  [   32/  118]
train() client id: f_00009-1-1 loss: 0.960220  [   64/  118]
train() client id: f_00009-1-2 loss: 0.877429  [   96/  118]
train() client id: f_00009-2-0 loss: 0.773480  [   32/  118]
train() client id: f_00009-2-1 loss: 0.802562  [   64/  118]
train() client id: f_00009-2-2 loss: 0.818944  [   96/  118]
train() client id: f_00009-3-0 loss: 0.728334  [   32/  118]
train() client id: f_00009-3-1 loss: 0.937544  [   64/  118]
train() client id: f_00009-3-2 loss: 0.660185  [   96/  118]
train() client id: f_00009-4-0 loss: 0.725530  [   32/  118]
train() client id: f_00009-4-1 loss: 0.777030  [   64/  118]
train() client id: f_00009-4-2 loss: 0.634076  [   96/  118]
train() client id: f_00009-5-0 loss: 0.632366  [   32/  118]
train() client id: f_00009-5-1 loss: 0.812929  [   64/  118]
train() client id: f_00009-5-2 loss: 0.678134  [   96/  118]
train() client id: f_00009-6-0 loss: 0.674665  [   32/  118]
train() client id: f_00009-6-1 loss: 0.700992  [   64/  118]
train() client id: f_00009-6-2 loss: 0.874219  [   96/  118]
train() client id: f_00009-7-0 loss: 0.625470  [   32/  118]
train() client id: f_00009-7-1 loss: 0.701456  [   64/  118]
train() client id: f_00009-7-2 loss: 0.742557  [   96/  118]
train() client id: f_00009-8-0 loss: 0.756086  [   32/  118]
train() client id: f_00009-8-1 loss: 0.520907  [   64/  118]
train() client id: f_00009-8-2 loss: 0.783861  [   96/  118]
train() client id: f_00009-9-0 loss: 0.676386  [   32/  118]
train() client id: f_00009-9-1 loss: 0.729216  [   64/  118]
train() client id: f_00009-9-2 loss: 0.664046  [   96/  118]
train() client id: f_00009-10-0 loss: 0.698935  [   32/  118]
train() client id: f_00009-10-1 loss: 0.675408  [   64/  118]
train() client id: f_00009-10-2 loss: 0.582816  [   96/  118]
At round 58 accuracy: 0.649867374005305
At round 58 training accuracy: 0.5942320590207915
At round 58 training loss: 0.823211084050142
update_location
xs = -3.905658 4.200318 310.009024 18.811294 0.979296 3.956410 -272.443192 -251.324852 294.663977 -237.060879 
ys = 302.587959 285.555839 1.320614 -272.455176 264.350187 247.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 318.707903 302.588466 325.741215 290.836187 282.634004 267.259250 290.227813 270.490032 311.665733 257.320563 
dists_bs = 213.429054 210.559605 515.168208 487.625832 197.363038 193.202736 202.597657 190.166605 495.347120 181.912193 
uav_gains = -117.191787 -115.944481 -117.686903 -114.946654 -114.217785 -112.821340 -114.893352 -113.114986 -116.665768 -111.930584 
bs_gains = -104.786158 -104.621561 -115.501633 -114.833487 -103.834502 -103.575431 -104.152823 -103.382818 -115.024529 -102.843190 
Round 59
-------------------------------
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.22013439 6.49883235 3.18055184 1.17195349 7.49218211 3.60548997
 1.43848068 4.45606788 3.28586187 2.92340788]
obj_prev = 37.272962461679306
eta_min = 1.7285175423879787e-29	eta_max = 0.9377803249311158
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 8.56318552740206	eta = 0.909090909090909
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 19.714881852065698	eta = 0.3948648627079844
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 13.689416383079331	eta = 0.5686666179167633
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 12.622409124377576	eta = 0.6167375846489944
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 12.557303683729339	eta = 0.6199351637809642
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 12.557030281762911	eta = 0.6199486615180114
af = 7.784714115820054	bf = 1.0502994844473539	zeta = 12.557030276907934	eta = 0.6199486617577047
eta = 0.6199486617577047
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [0.0394532  0.08297697 0.03882696 0.01346419 0.09581492 0.04571561
 0.01690851 0.0560486  0.04070568 0.03694824]
ene_total = [1.24429988 1.90408729 1.25458552 0.61484934 2.16708445 1.11417022
 0.68545833 1.4588381  1.18872479 0.92493235]
ti_comp = [1.09084903 1.21156017 1.07968336 1.13102359 1.21456653 1.21550713
 1.13180693 1.15453068 1.12986699 1.21804473]
ti_coms = [0.19863687 0.07792573 0.20980254 0.1584623  0.07491936 0.07397877
 0.15767897 0.13495521 0.1596189  0.07144116]
t_total = [27.04975243 27.04975243 27.04975243 27.04975243 27.04975243 27.04975243
 27.04975243 27.04975243 27.04975243 27.04975243]
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [3.22550318e-06 2.43255565e-05 3.13824884e-06 1.19255159e-07
 3.72680608e-05 4.04164983e-06 2.35858602e-07 8.25587810e-06
 3.30210249e-06 2.12488134e-06]
ene_total = [0.4167514  0.16397633 0.44017198 0.33241142 0.15794132 0.15527123
 0.33077065 0.2832709  0.3349044  0.14990784]
optimize_network iter = 0 obj = 2.7653774631169483
eta = 0.6199486617577047
freqs = [18083713.04857494 34243850.94632931 17980716.29606591  5952213.52469169
 39444079.62545728 18805161.01539245  7469699.35866398 24273324.57197437
 18013484.95120145 15167027.70060071]
eta_min = 0.6199486617577048	eta_max = 0.7133529451356779
af = 0.0014346255173217418	bf = 1.0502994844473539	zeta = 0.001578088069053916	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [6.42985407e-07 4.84915903e-06 6.25591760e-07 2.37728263e-08
 7.42917241e-06 8.05679522e-07 4.70170484e-08 1.64576156e-06
 6.58255038e-07 4.23582807e-07]
ene_total = [1.74202974 0.68380564 1.83994709 1.38965906 0.65766716 0.64883763
 1.38279153 1.18365265 1.39985765 0.62655025]
ti_comp = [0.77393525 0.89464639 0.76276958 0.81410981 0.89765275 0.89859335
 0.81489315 0.8376169  0.81295321 0.90113095]
ti_coms = [0.19863687 0.07792573 0.20980254 0.1584623  0.07491936 0.07397877
 0.15767897 0.13495521 0.1596189  0.07144116]
t_total = [27.04975243 27.04975243 27.04975243 27.04975243 27.04975243 27.04975243
 27.04975243 27.04975243 27.04975243 27.04975243]
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [2.25947941e-06 1.57304375e-05 2.21709516e-06 8.11606270e-08
 2.40576692e-05 2.60758319e-06 1.60430136e-07 5.53062528e-06
 2.24908028e-06 1.36891415e-06]
ene_total = [0.55252348 0.21716916 0.58357692 0.44072701 0.20903928 0.20582665
 0.43855056 0.37549926 0.44400411 0.19873447]
optimize_network iter = 1 obj = 3.6656508882711805
eta = 0.7133529451356779
freqs = [18007133.55773868 32762205.35417381 17980716.2960659   5842034.71544074
 37704373.23913116 17970837.74832523  7329456.31607533 23636668.595279
 17687093.31024747 14483474.75860257]
eta_min = 0.7133529451356788	eta_max = 0.7133529451356644
af = 0.0013279300383138625	bf = 1.0502994844473539	zeta = 0.0014607230421452488	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [6.37551209e-07 4.43861511e-06 6.25591760e-07 2.29008752e-08
 6.78828762e-06 7.35774715e-07 4.52681388e-08 1.56056161e-06
 6.34616915e-07 3.86262812e-07]
ene_total = [1.74202926 0.68376964 1.83994709 1.38965899 0.65761096 0.6488315
 1.38279138 1.18364518 1.39985558 0.62654698]
ti_comp = [0.77393525 0.89464639 0.76276958 0.81410981 0.89765275 0.89859335
 0.81489315 0.8376169  0.81295321 0.90113095]
ti_coms = [0.19863687 0.07792573 0.20980254 0.1584623  0.07491936 0.07397877
 0.15767897 0.13495521 0.1596189  0.07144116]
t_total = [27.04975243 27.04975243 27.04975243 27.04975243 27.04975243 27.04975243
 27.04975243 27.04975243 27.04975243 27.04975243]
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [2.25947941e-06 1.57304375e-05 2.21709516e-06 8.11606270e-08
 2.40576692e-05 2.60758319e-06 1.60430136e-07 5.53062528e-06
 2.24908028e-06 1.36891415e-06]
ene_total = [0.55252348 0.21716916 0.58357692 0.44072701 0.20903928 0.20582665
 0.43855056 0.37549926 0.44400411 0.19873447]
optimize_network iter = 2 obj = 3.6656508882710073
eta = 0.7133529451356644
freqs = [18007133.55773862 32762205.35417396 17980716.29606583  5842034.71544073
 37704373.23913135 17970837.74832532  7329456.31607533 23636668.59527903
 17687093.31024747 14483474.75860265]
Done!
ene_coms = [0.01986369 0.00779257 0.02098025 0.01584623 0.00749194 0.00739788
 0.0157679  0.01349552 0.01596189 0.00714412]
ene_comp = [2.24710003e-06 1.56442526e-05 2.20494800e-06 8.07159591e-08
 2.39258606e-05 2.59329661e-06 1.59551162e-07 5.50032375e-06
 2.23675788e-06 1.36141406e-06]
ene_total = [0.01986593 0.00780822 0.02098246 0.01584631 0.00751586 0.00740047
 0.01576806 0.01350102 0.01596413 0.00714548]
At round 59 energy consumption: 0.131797934436514
At round 59 eta: 0.7133529451356644
At round 59 a_n: 7.972397885064655
At round 59 local rounds: 11.060599497015481
At round 59 global rounds: 27.812593047006303
gradient difference: 0.5148689150810242
train() client id: f_00000-0-0 loss: 1.156778  [   32/  126]
train() client id: f_00000-0-1 loss: 1.044562  [   64/  126]
train() client id: f_00000-0-2 loss: 1.045416  [   96/  126]
train() client id: f_00000-1-0 loss: 1.224232  [   32/  126]
train() client id: f_00000-1-1 loss: 0.920604  [   64/  126]
train() client id: f_00000-1-2 loss: 0.834490  [   96/  126]
train() client id: f_00000-2-0 loss: 0.877490  [   32/  126]
train() client id: f_00000-2-1 loss: 0.906941  [   64/  126]
train() client id: f_00000-2-2 loss: 0.898115  [   96/  126]
train() client id: f_00000-3-0 loss: 0.899767  [   32/  126]
train() client id: f_00000-3-1 loss: 0.916706  [   64/  126]
train() client id: f_00000-3-2 loss: 0.816973  [   96/  126]
train() client id: f_00000-4-0 loss: 0.783677  [   32/  126]
train() client id: f_00000-4-1 loss: 0.845187  [   64/  126]
train() client id: f_00000-4-2 loss: 0.933744  [   96/  126]
train() client id: f_00000-5-0 loss: 0.765009  [   32/  126]
train() client id: f_00000-5-1 loss: 0.763326  [   64/  126]
train() client id: f_00000-5-2 loss: 0.883440  [   96/  126]
train() client id: f_00000-6-0 loss: 0.836903  [   32/  126]
train() client id: f_00000-6-1 loss: 0.834426  [   64/  126]
train() client id: f_00000-6-2 loss: 0.815457  [   96/  126]
train() client id: f_00000-7-0 loss: 0.731308  [   32/  126]
train() client id: f_00000-7-1 loss: 0.887379  [   64/  126]
train() client id: f_00000-7-2 loss: 0.832441  [   96/  126]
train() client id: f_00000-8-0 loss: 0.793808  [   32/  126]
train() client id: f_00000-8-1 loss: 0.795547  [   64/  126]
train() client id: f_00000-8-2 loss: 0.824830  [   96/  126]
train() client id: f_00000-9-0 loss: 0.890918  [   32/  126]
train() client id: f_00000-9-1 loss: 0.812826  [   64/  126]
train() client id: f_00000-9-2 loss: 0.678956  [   96/  126]
train() client id: f_00000-10-0 loss: 0.853036  [   32/  126]
train() client id: f_00000-10-1 loss: 0.849534  [   64/  126]
train() client id: f_00000-10-2 loss: 0.763382  [   96/  126]
train() client id: f_00001-0-0 loss: 0.436146  [   32/  265]
train() client id: f_00001-0-1 loss: 0.567793  [   64/  265]
train() client id: f_00001-0-2 loss: 0.513203  [   96/  265]
train() client id: f_00001-0-3 loss: 0.412287  [  128/  265]
train() client id: f_00001-0-4 loss: 0.393749  [  160/  265]
train() client id: f_00001-0-5 loss: 0.366793  [  192/  265]
train() client id: f_00001-0-6 loss: 0.605350  [  224/  265]
train() client id: f_00001-0-7 loss: 0.425538  [  256/  265]
train() client id: f_00001-1-0 loss: 0.481015  [   32/  265]
train() client id: f_00001-1-1 loss: 0.414304  [   64/  265]
train() client id: f_00001-1-2 loss: 0.474249  [   96/  265]
train() client id: f_00001-1-3 loss: 0.427424  [  128/  265]
train() client id: f_00001-1-4 loss: 0.447519  [  160/  265]
train() client id: f_00001-1-5 loss: 0.428089  [  192/  265]
train() client id: f_00001-1-6 loss: 0.545082  [  224/  265]
train() client id: f_00001-1-7 loss: 0.481478  [  256/  265]
train() client id: f_00001-2-0 loss: 0.432815  [   32/  265]
train() client id: f_00001-2-1 loss: 0.463501  [   64/  265]
train() client id: f_00001-2-2 loss: 0.584449  [   96/  265]
train() client id: f_00001-2-3 loss: 0.376378  [  128/  265]
train() client id: f_00001-2-4 loss: 0.587055  [  160/  265]
train() client id: f_00001-2-5 loss: 0.407028  [  192/  265]
train() client id: f_00001-2-6 loss: 0.429393  [  224/  265]
train() client id: f_00001-2-7 loss: 0.470488  [  256/  265]
train() client id: f_00001-3-0 loss: 0.504634  [   32/  265]
train() client id: f_00001-3-1 loss: 0.365969  [   64/  265]
train() client id: f_00001-3-2 loss: 0.419370  [   96/  265]
train() client id: f_00001-3-3 loss: 0.454058  [  128/  265]
train() client id: f_00001-3-4 loss: 0.448604  [  160/  265]
train() client id: f_00001-3-5 loss: 0.359086  [  192/  265]
train() client id: f_00001-3-6 loss: 0.498393  [  224/  265]
train() client id: f_00001-3-7 loss: 0.594295  [  256/  265]
train() client id: f_00001-4-0 loss: 0.466094  [   32/  265]
train() client id: f_00001-4-1 loss: 0.396180  [   64/  265]
train() client id: f_00001-4-2 loss: 0.453792  [   96/  265]
train() client id: f_00001-4-3 loss: 0.372645  [  128/  265]
train() client id: f_00001-4-4 loss: 0.503615  [  160/  265]
train() client id: f_00001-4-5 loss: 0.532918  [  192/  265]
train() client id: f_00001-4-6 loss: 0.355805  [  224/  265]
train() client id: f_00001-4-7 loss: 0.539114  [  256/  265]
train() client id: f_00001-5-0 loss: 0.379478  [   32/  265]
train() client id: f_00001-5-1 loss: 0.530479  [   64/  265]
train() client id: f_00001-5-2 loss: 0.575836  [   96/  265]
train() client id: f_00001-5-3 loss: 0.627465  [  128/  265]
train() client id: f_00001-5-4 loss: 0.372206  [  160/  265]
train() client id: f_00001-5-5 loss: 0.364031  [  192/  265]
train() client id: f_00001-5-6 loss: 0.482291  [  224/  265]
train() client id: f_00001-5-7 loss: 0.368144  [  256/  265]
train() client id: f_00001-6-0 loss: 0.425949  [   32/  265]
train() client id: f_00001-6-1 loss: 0.373939  [   64/  265]
train() client id: f_00001-6-2 loss: 0.479129  [   96/  265]
train() client id: f_00001-6-3 loss: 0.609454  [  128/  265]
train() client id: f_00001-6-4 loss: 0.421987  [  160/  265]
train() client id: f_00001-6-5 loss: 0.381043  [  192/  265]
train() client id: f_00001-6-6 loss: 0.508498  [  224/  265]
train() client id: f_00001-6-7 loss: 0.499968  [  256/  265]
train() client id: f_00001-7-0 loss: 0.364037  [   32/  265]
train() client id: f_00001-7-1 loss: 0.368255  [   64/  265]
train() client id: f_00001-7-2 loss: 0.437873  [   96/  265]
train() client id: f_00001-7-3 loss: 0.589892  [  128/  265]
train() client id: f_00001-7-4 loss: 0.399330  [  160/  265]
train() client id: f_00001-7-5 loss: 0.568704  [  192/  265]
train() client id: f_00001-7-6 loss: 0.553626  [  224/  265]
train() client id: f_00001-7-7 loss: 0.361629  [  256/  265]
train() client id: f_00001-8-0 loss: 0.349616  [   32/  265]
train() client id: f_00001-8-1 loss: 0.363922  [   64/  265]
train() client id: f_00001-8-2 loss: 0.571479  [   96/  265]
train() client id: f_00001-8-3 loss: 0.547329  [  128/  265]
train() client id: f_00001-8-4 loss: 0.530453  [  160/  265]
train() client id: f_00001-8-5 loss: 0.465959  [  192/  265]
train() client id: f_00001-8-6 loss: 0.373760  [  224/  265]
train() client id: f_00001-8-7 loss: 0.455057  [  256/  265]
train() client id: f_00001-9-0 loss: 0.511807  [   32/  265]
train() client id: f_00001-9-1 loss: 0.423889  [   64/  265]
train() client id: f_00001-9-2 loss: 0.428065  [   96/  265]
train() client id: f_00001-9-3 loss: 0.530931  [  128/  265]
train() client id: f_00001-9-4 loss: 0.527536  [  160/  265]
train() client id: f_00001-9-5 loss: 0.370674  [  192/  265]
train() client id: f_00001-9-6 loss: 0.361222  [  224/  265]
train() client id: f_00001-9-7 loss: 0.542765  [  256/  265]
train() client id: f_00001-10-0 loss: 0.378545  [   32/  265]
train() client id: f_00001-10-1 loss: 0.515421  [   64/  265]
train() client id: f_00001-10-2 loss: 0.496207  [   96/  265]
train() client id: f_00001-10-3 loss: 0.392227  [  128/  265]
train() client id: f_00001-10-4 loss: 0.459383  [  160/  265]
train() client id: f_00001-10-5 loss: 0.522777  [  192/  265]
train() client id: f_00001-10-6 loss: 0.561207  [  224/  265]
train() client id: f_00001-10-7 loss: 0.384831  [  256/  265]
train() client id: f_00002-0-0 loss: 1.133117  [   32/  124]
train() client id: f_00002-0-1 loss: 1.180389  [   64/  124]
train() client id: f_00002-0-2 loss: 1.104479  [   96/  124]
train() client id: f_00002-1-0 loss: 1.091616  [   32/  124]
train() client id: f_00002-1-1 loss: 1.101372  [   64/  124]
train() client id: f_00002-1-2 loss: 1.100778  [   96/  124]
train() client id: f_00002-2-0 loss: 0.994371  [   32/  124]
train() client id: f_00002-2-1 loss: 1.183066  [   64/  124]
train() client id: f_00002-2-2 loss: 1.152023  [   96/  124]
train() client id: f_00002-3-0 loss: 0.900905  [   32/  124]
train() client id: f_00002-3-1 loss: 1.073439  [   64/  124]
train() client id: f_00002-3-2 loss: 1.172227  [   96/  124]
train() client id: f_00002-4-0 loss: 1.069140  [   32/  124]
train() client id: f_00002-4-1 loss: 0.997653  [   64/  124]
train() client id: f_00002-4-2 loss: 0.986063  [   96/  124]
train() client id: f_00002-5-0 loss: 1.147555  [   32/  124]
train() client id: f_00002-5-1 loss: 0.856170  [   64/  124]
train() client id: f_00002-5-2 loss: 0.969640  [   96/  124]
train() client id: f_00002-6-0 loss: 0.952580  [   32/  124]
train() client id: f_00002-6-1 loss: 0.989192  [   64/  124]
train() client id: f_00002-6-2 loss: 0.899529  [   96/  124]
train() client id: f_00002-7-0 loss: 0.842105  [   32/  124]
train() client id: f_00002-7-1 loss: 1.128025  [   64/  124]
train() client id: f_00002-7-2 loss: 0.973643  [   96/  124]
train() client id: f_00002-8-0 loss: 0.899479  [   32/  124]
train() client id: f_00002-8-1 loss: 0.995648  [   64/  124]
train() client id: f_00002-8-2 loss: 0.907200  [   96/  124]
train() client id: f_00002-9-0 loss: 0.905137  [   32/  124]
train() client id: f_00002-9-1 loss: 1.012364  [   64/  124]
train() client id: f_00002-9-2 loss: 1.022822  [   96/  124]
train() client id: f_00002-10-0 loss: 0.812667  [   32/  124]
train() client id: f_00002-10-1 loss: 1.217314  [   64/  124]
train() client id: f_00002-10-2 loss: 0.846152  [   96/  124]
train() client id: f_00003-0-0 loss: 0.619502  [   32/   43]
train() client id: f_00003-1-0 loss: 0.648074  [   32/   43]
train() client id: f_00003-2-0 loss: 0.541847  [   32/   43]
train() client id: f_00003-3-0 loss: 0.401596  [   32/   43]
train() client id: f_00003-4-0 loss: 0.441164  [   32/   43]
train() client id: f_00003-5-0 loss: 0.430077  [   32/   43]
train() client id: f_00003-6-0 loss: 0.368134  [   32/   43]
train() client id: f_00003-7-0 loss: 0.435403  [   32/   43]
train() client id: f_00003-8-0 loss: 0.566090  [   32/   43]
train() client id: f_00003-9-0 loss: 0.549573  [   32/   43]
train() client id: f_00003-10-0 loss: 0.617579  [   32/   43]
train() client id: f_00004-0-0 loss: 0.763310  [   32/  306]
train() client id: f_00004-0-1 loss: 0.870469  [   64/  306]
train() client id: f_00004-0-2 loss: 0.789868  [   96/  306]
train() client id: f_00004-0-3 loss: 0.862963  [  128/  306]
train() client id: f_00004-0-4 loss: 0.875197  [  160/  306]
train() client id: f_00004-0-5 loss: 0.892011  [  192/  306]
train() client id: f_00004-0-6 loss: 0.930697  [  224/  306]
train() client id: f_00004-0-7 loss: 0.870000  [  256/  306]
train() client id: f_00004-0-8 loss: 0.976266  [  288/  306]
train() client id: f_00004-1-0 loss: 0.839208  [   32/  306]
train() client id: f_00004-1-1 loss: 0.861015  [   64/  306]
train() client id: f_00004-1-2 loss: 0.803656  [   96/  306]
train() client id: f_00004-1-3 loss: 1.003614  [  128/  306]
train() client id: f_00004-1-4 loss: 0.901693  [  160/  306]
train() client id: f_00004-1-5 loss: 0.760975  [  192/  306]
train() client id: f_00004-1-6 loss: 0.764076  [  224/  306]
train() client id: f_00004-1-7 loss: 0.752217  [  256/  306]
train() client id: f_00004-1-8 loss: 0.905183  [  288/  306]
train() client id: f_00004-2-0 loss: 0.786417  [   32/  306]
train() client id: f_00004-2-1 loss: 0.761253  [   64/  306]
train() client id: f_00004-2-2 loss: 0.809352  [   96/  306]
train() client id: f_00004-2-3 loss: 0.748357  [  128/  306]
train() client id: f_00004-2-4 loss: 0.748593  [  160/  306]
train() client id: f_00004-2-5 loss: 0.906614  [  192/  306]
train() client id: f_00004-2-6 loss: 0.882118  [  224/  306]
train() client id: f_00004-2-7 loss: 0.963184  [  256/  306]
train() client id: f_00004-2-8 loss: 0.982758  [  288/  306]
train() client id: f_00004-3-0 loss: 0.831299  [   32/  306]
train() client id: f_00004-3-1 loss: 0.908312  [   64/  306]
train() client id: f_00004-3-2 loss: 0.753879  [   96/  306]
train() client id: f_00004-3-3 loss: 0.896185  [  128/  306]
train() client id: f_00004-3-4 loss: 0.813334  [  160/  306]
train() client id: f_00004-3-5 loss: 0.881185  [  192/  306]
train() client id: f_00004-3-6 loss: 0.858900  [  224/  306]
train() client id: f_00004-3-7 loss: 0.892015  [  256/  306]
train() client id: f_00004-3-8 loss: 0.824025  [  288/  306]
train() client id: f_00004-4-0 loss: 0.929972  [   32/  306]
train() client id: f_00004-4-1 loss: 0.772575  [   64/  306]
train() client id: f_00004-4-2 loss: 0.957379  [   96/  306]
train() client id: f_00004-4-3 loss: 0.871085  [  128/  306]
train() client id: f_00004-4-4 loss: 0.875209  [  160/  306]
train() client id: f_00004-4-5 loss: 0.789962  [  192/  306]
train() client id: f_00004-4-6 loss: 0.798293  [  224/  306]
train() client id: f_00004-4-7 loss: 0.740925  [  256/  306]
train() client id: f_00004-4-8 loss: 0.881067  [  288/  306]
train() client id: f_00004-5-0 loss: 0.915874  [   32/  306]
train() client id: f_00004-5-1 loss: 0.919697  [   64/  306]
train() client id: f_00004-5-2 loss: 0.799255  [   96/  306]
train() client id: f_00004-5-3 loss: 0.686027  [  128/  306]
train() client id: f_00004-5-4 loss: 0.788413  [  160/  306]
train() client id: f_00004-5-5 loss: 0.924304  [  192/  306]
train() client id: f_00004-5-6 loss: 0.864759  [  224/  306]
train() client id: f_00004-5-7 loss: 0.909488  [  256/  306]
train() client id: f_00004-5-8 loss: 0.857250  [  288/  306]
train() client id: f_00004-6-0 loss: 0.795907  [   32/  306]
train() client id: f_00004-6-1 loss: 0.793618  [   64/  306]
train() client id: f_00004-6-2 loss: 0.777201  [   96/  306]
train() client id: f_00004-6-3 loss: 0.889143  [  128/  306]
train() client id: f_00004-6-4 loss: 0.909439  [  160/  306]
train() client id: f_00004-6-5 loss: 0.859785  [  192/  306]
train() client id: f_00004-6-6 loss: 0.809659  [  224/  306]
train() client id: f_00004-6-7 loss: 0.852793  [  256/  306]
train() client id: f_00004-6-8 loss: 0.866237  [  288/  306]
train() client id: f_00004-7-0 loss: 0.915707  [   32/  306]
train() client id: f_00004-7-1 loss: 0.945012  [   64/  306]
train() client id: f_00004-7-2 loss: 0.787913  [   96/  306]
train() client id: f_00004-7-3 loss: 0.867610  [  128/  306]
train() client id: f_00004-7-4 loss: 0.852030  [  160/  306]
train() client id: f_00004-7-5 loss: 0.828358  [  192/  306]
train() client id: f_00004-7-6 loss: 0.766784  [  224/  306]
train() client id: f_00004-7-7 loss: 0.772498  [  256/  306]
train() client id: f_00004-7-8 loss: 0.877268  [  288/  306]
train() client id: f_00004-8-0 loss: 0.881153  [   32/  306]
train() client id: f_00004-8-1 loss: 0.806276  [   64/  306]
train() client id: f_00004-8-2 loss: 0.781510  [   96/  306]
train() client id: f_00004-8-3 loss: 0.813796  [  128/  306]
train() client id: f_00004-8-4 loss: 0.793122  [  160/  306]
train() client id: f_00004-8-5 loss: 0.848829  [  192/  306]
train() client id: f_00004-8-6 loss: 0.966795  [  224/  306]
train() client id: f_00004-8-7 loss: 0.767997  [  256/  306]
train() client id: f_00004-8-8 loss: 0.840066  [  288/  306]
train() client id: f_00004-9-0 loss: 0.733904  [   32/  306]
train() client id: f_00004-9-1 loss: 0.951296  [   64/  306]
train() client id: f_00004-9-2 loss: 0.791890  [   96/  306]
train() client id: f_00004-9-3 loss: 0.855058  [  128/  306]
train() client id: f_00004-9-4 loss: 0.994852  [  160/  306]
train() client id: f_00004-9-5 loss: 0.768174  [  192/  306]
train() client id: f_00004-9-6 loss: 0.798091  [  224/  306]
train() client id: f_00004-9-7 loss: 0.797182  [  256/  306]
train() client id: f_00004-9-8 loss: 0.830788  [  288/  306]
train() client id: f_00004-10-0 loss: 0.757418  [   32/  306]
train() client id: f_00004-10-1 loss: 0.795897  [   64/  306]
train() client id: f_00004-10-2 loss: 0.746896  [   96/  306]
train() client id: f_00004-10-3 loss: 0.791634  [  128/  306]
train() client id: f_00004-10-4 loss: 0.828515  [  160/  306]
train() client id: f_00004-10-5 loss: 0.881803  [  192/  306]
train() client id: f_00004-10-6 loss: 0.854576  [  224/  306]
train() client id: f_00004-10-7 loss: 0.871637  [  256/  306]
train() client id: f_00004-10-8 loss: 1.006374  [  288/  306]
train() client id: f_00005-0-0 loss: 0.493708  [   32/  146]
train() client id: f_00005-0-1 loss: 0.694390  [   64/  146]
train() client id: f_00005-0-2 loss: 0.658320  [   96/  146]
train() client id: f_00005-0-3 loss: 0.463073  [  128/  146]
train() client id: f_00005-1-0 loss: 0.626889  [   32/  146]
train() client id: f_00005-1-1 loss: 0.844701  [   64/  146]
train() client id: f_00005-1-2 loss: 0.434456  [   96/  146]
train() client id: f_00005-1-3 loss: 0.496093  [  128/  146]
train() client id: f_00005-2-0 loss: 0.488923  [   32/  146]
train() client id: f_00005-2-1 loss: 0.761469  [   64/  146]
train() client id: f_00005-2-2 loss: 0.756696  [   96/  146]
train() client id: f_00005-2-3 loss: 0.519173  [  128/  146]
train() client id: f_00005-3-0 loss: 0.712124  [   32/  146]
train() client id: f_00005-3-1 loss: 0.633831  [   64/  146]
train() client id: f_00005-3-2 loss: 0.540881  [   96/  146]
train() client id: f_00005-3-3 loss: 0.631077  [  128/  146]
train() client id: f_00005-4-0 loss: 0.629606  [   32/  146]
train() client id: f_00005-4-1 loss: 0.386510  [   64/  146]
train() client id: f_00005-4-2 loss: 0.653371  [   96/  146]
train() client id: f_00005-4-3 loss: 0.770369  [  128/  146]
train() client id: f_00005-5-0 loss: 0.583361  [   32/  146]
train() client id: f_00005-5-1 loss: 0.710040  [   64/  146]
train() client id: f_00005-5-2 loss: 0.595986  [   96/  146]
train() client id: f_00005-5-3 loss: 0.575277  [  128/  146]
train() client id: f_00005-6-0 loss: 0.617032  [   32/  146]
train() client id: f_00005-6-1 loss: 0.561943  [   64/  146]
train() client id: f_00005-6-2 loss: 1.006844  [   96/  146]
train() client id: f_00005-6-3 loss: 0.400539  [  128/  146]
train() client id: f_00005-7-0 loss: 0.625320  [   32/  146]
train() client id: f_00005-7-1 loss: 0.894914  [   64/  146]
train() client id: f_00005-7-2 loss: 0.554735  [   96/  146]
train() client id: f_00005-7-3 loss: 0.495052  [  128/  146]
train() client id: f_00005-8-0 loss: 0.741473  [   32/  146]
train() client id: f_00005-8-1 loss: 0.671034  [   64/  146]
train() client id: f_00005-8-2 loss: 0.575483  [   96/  146]
train() client id: f_00005-8-3 loss: 0.694262  [  128/  146]
train() client id: f_00005-9-0 loss: 0.672963  [   32/  146]
train() client id: f_00005-9-1 loss: 0.482127  [   64/  146]
train() client id: f_00005-9-2 loss: 0.690389  [   96/  146]
train() client id: f_00005-9-3 loss: 0.555794  [  128/  146]
train() client id: f_00005-10-0 loss: 0.654991  [   32/  146]
train() client id: f_00005-10-1 loss: 0.524703  [   64/  146]
train() client id: f_00005-10-2 loss: 0.584116  [   96/  146]
train() client id: f_00005-10-3 loss: 0.768093  [  128/  146]
train() client id: f_00006-0-0 loss: 0.475225  [   32/   54]
train() client id: f_00006-1-0 loss: 0.573270  [   32/   54]
train() client id: f_00006-2-0 loss: 0.533177  [   32/   54]
train() client id: f_00006-3-0 loss: 0.527280  [   32/   54]
train() client id: f_00006-4-0 loss: 0.509849  [   32/   54]
train() client id: f_00006-5-0 loss: 0.588835  [   32/   54]
train() client id: f_00006-6-0 loss: 0.581812  [   32/   54]
train() client id: f_00006-7-0 loss: 0.580156  [   32/   54]
train() client id: f_00006-8-0 loss: 0.457630  [   32/   54]
train() client id: f_00006-9-0 loss: 0.564019  [   32/   54]
train() client id: f_00006-10-0 loss: 0.575790  [   32/   54]
train() client id: f_00007-0-0 loss: 0.577513  [   32/  179]
train() client id: f_00007-0-1 loss: 0.513447  [   64/  179]
train() client id: f_00007-0-2 loss: 0.500510  [   96/  179]
train() client id: f_00007-0-3 loss: 0.742643  [  128/  179]
train() client id: f_00007-0-4 loss: 0.467235  [  160/  179]
train() client id: f_00007-1-0 loss: 0.713386  [   32/  179]
train() client id: f_00007-1-1 loss: 0.528476  [   64/  179]
train() client id: f_00007-1-2 loss: 0.365685  [   96/  179]
train() client id: f_00007-1-3 loss: 0.406597  [  128/  179]
train() client id: f_00007-1-4 loss: 0.466431  [  160/  179]
train() client id: f_00007-2-0 loss: 0.511472  [   32/  179]
train() client id: f_00007-2-1 loss: 0.584983  [   64/  179]
train() client id: f_00007-2-2 loss: 0.562282  [   96/  179]
train() client id: f_00007-2-3 loss: 0.501683  [  128/  179]
train() client id: f_00007-2-4 loss: 0.466132  [  160/  179]
train() client id: f_00007-3-0 loss: 0.509371  [   32/  179]
train() client id: f_00007-3-1 loss: 0.551530  [   64/  179]
train() client id: f_00007-3-2 loss: 0.496431  [   96/  179]
train() client id: f_00007-3-3 loss: 0.625829  [  128/  179]
train() client id: f_00007-3-4 loss: 0.543745  [  160/  179]
train() client id: f_00007-4-0 loss: 0.548460  [   32/  179]
train() client id: f_00007-4-1 loss: 0.431324  [   64/  179]
train() client id: f_00007-4-2 loss: 0.644077  [   96/  179]
train() client id: f_00007-4-3 loss: 0.673261  [  128/  179]
train() client id: f_00007-4-4 loss: 0.408065  [  160/  179]
train() client id: f_00007-5-0 loss: 0.478153  [   32/  179]
train() client id: f_00007-5-1 loss: 0.783475  [   64/  179]
train() client id: f_00007-5-2 loss: 0.528727  [   96/  179]
train() client id: f_00007-5-3 loss: 0.459014  [  128/  179]
train() client id: f_00007-5-4 loss: 0.463865  [  160/  179]
train() client id: f_00007-6-0 loss: 0.356501  [   32/  179]
train() client id: f_00007-6-1 loss: 0.550849  [   64/  179]
train() client id: f_00007-6-2 loss: 0.575494  [   96/  179]
train() client id: f_00007-6-3 loss: 0.678670  [  128/  179]
train() client id: f_00007-6-4 loss: 0.530704  [  160/  179]
train() client id: f_00007-7-0 loss: 0.448097  [   32/  179]
train() client id: f_00007-7-1 loss: 0.608467  [   64/  179]
train() client id: f_00007-7-2 loss: 0.455933  [   96/  179]
train() client id: f_00007-7-3 loss: 0.632346  [  128/  179]
train() client id: f_00007-7-4 loss: 0.427016  [  160/  179]
train() client id: f_00007-8-0 loss: 0.536442  [   32/  179]
train() client id: f_00007-8-1 loss: 0.555019  [   64/  179]
train() client id: f_00007-8-2 loss: 0.399494  [   96/  179]
train() client id: f_00007-8-3 loss: 0.509221  [  128/  179]
train() client id: f_00007-8-4 loss: 0.462918  [  160/  179]
train() client id: f_00007-9-0 loss: 0.651935  [   32/  179]
train() client id: f_00007-9-1 loss: 0.686721  [   64/  179]
train() client id: f_00007-9-2 loss: 0.352464  [   96/  179]
train() client id: f_00007-9-3 loss: 0.510813  [  128/  179]
train() client id: f_00007-9-4 loss: 0.453074  [  160/  179]
train() client id: f_00007-10-0 loss: 0.380273  [   32/  179]
train() client id: f_00007-10-1 loss: 0.638036  [   64/  179]
train() client id: f_00007-10-2 loss: 0.454814  [   96/  179]
train() client id: f_00007-10-3 loss: 0.546272  [  128/  179]
train() client id: f_00007-10-4 loss: 0.510092  [  160/  179]
train() client id: f_00008-0-0 loss: 0.690457  [   32/  130]
train() client id: f_00008-0-1 loss: 0.734372  [   64/  130]
train() client id: f_00008-0-2 loss: 0.618871  [   96/  130]
train() client id: f_00008-0-3 loss: 0.796373  [  128/  130]
train() client id: f_00008-1-0 loss: 0.687565  [   32/  130]
train() client id: f_00008-1-1 loss: 0.664074  [   64/  130]
train() client id: f_00008-1-2 loss: 0.723929  [   96/  130]
train() client id: f_00008-1-3 loss: 0.750082  [  128/  130]
train() client id: f_00008-2-0 loss: 0.645106  [   32/  130]
train() client id: f_00008-2-1 loss: 0.757381  [   64/  130]
train() client id: f_00008-2-2 loss: 0.724734  [   96/  130]
train() client id: f_00008-2-3 loss: 0.677931  [  128/  130]
train() client id: f_00008-3-0 loss: 0.737825  [   32/  130]
train() client id: f_00008-3-1 loss: 0.782837  [   64/  130]
train() client id: f_00008-3-2 loss: 0.623545  [   96/  130]
train() client id: f_00008-3-3 loss: 0.698540  [  128/  130]
train() client id: f_00008-4-0 loss: 0.765508  [   32/  130]
train() client id: f_00008-4-1 loss: 0.675197  [   64/  130]
train() client id: f_00008-4-2 loss: 0.641444  [   96/  130]
train() client id: f_00008-4-3 loss: 0.744255  [  128/  130]
train() client id: f_00008-5-0 loss: 0.723385  [   32/  130]
train() client id: f_00008-5-1 loss: 0.720070  [   64/  130]
train() client id: f_00008-5-2 loss: 0.688424  [   96/  130]
train() client id: f_00008-5-3 loss: 0.697957  [  128/  130]
train() client id: f_00008-6-0 loss: 0.689831  [   32/  130]
train() client id: f_00008-6-1 loss: 0.745363  [   64/  130]
train() client id: f_00008-6-2 loss: 0.724673  [   96/  130]
train() client id: f_00008-6-3 loss: 0.651218  [  128/  130]
train() client id: f_00008-7-0 loss: 0.678272  [   32/  130]
train() client id: f_00008-7-1 loss: 0.651053  [   64/  130]
train() client id: f_00008-7-2 loss: 0.823381  [   96/  130]
train() client id: f_00008-7-3 loss: 0.678121  [  128/  130]
train() client id: f_00008-8-0 loss: 0.630264  [   32/  130]
train() client id: f_00008-8-1 loss: 0.641016  [   64/  130]
train() client id: f_00008-8-2 loss: 0.731494  [   96/  130]
train() client id: f_00008-8-3 loss: 0.831906  [  128/  130]
train() client id: f_00008-9-0 loss: 0.717555  [   32/  130]
train() client id: f_00008-9-1 loss: 0.723050  [   64/  130]
train() client id: f_00008-9-2 loss: 0.660114  [   96/  130]
train() client id: f_00008-9-3 loss: 0.740492  [  128/  130]
train() client id: f_00008-10-0 loss: 0.775859  [   32/  130]
train() client id: f_00008-10-1 loss: 0.673755  [   64/  130]
train() client id: f_00008-10-2 loss: 0.710249  [   96/  130]
train() client id: f_00008-10-3 loss: 0.680897  [  128/  130]
train() client id: f_00009-0-0 loss: 1.131583  [   32/  118]
train() client id: f_00009-0-1 loss: 1.279923  [   64/  118]
train() client id: f_00009-0-2 loss: 0.994173  [   96/  118]
train() client id: f_00009-1-0 loss: 0.951672  [   32/  118]
train() client id: f_00009-1-1 loss: 1.220134  [   64/  118]
train() client id: f_00009-1-2 loss: 1.144388  [   96/  118]
train() client id: f_00009-2-0 loss: 1.088266  [   32/  118]
train() client id: f_00009-2-1 loss: 1.107247  [   64/  118]
train() client id: f_00009-2-2 loss: 0.900499  [   96/  118]
train() client id: f_00009-3-0 loss: 1.158473  [   32/  118]
train() client id: f_00009-3-1 loss: 0.989770  [   64/  118]
train() client id: f_00009-3-2 loss: 0.952382  [   96/  118]
train() client id: f_00009-4-0 loss: 1.129573  [   32/  118]
train() client id: f_00009-4-1 loss: 0.961490  [   64/  118]
train() client id: f_00009-4-2 loss: 1.029024  [   96/  118]
train() client id: f_00009-5-0 loss: 0.988833  [   32/  118]
train() client id: f_00009-5-1 loss: 0.936333  [   64/  118]
train() client id: f_00009-5-2 loss: 1.005419  [   96/  118]
train() client id: f_00009-6-0 loss: 0.913941  [   32/  118]
train() client id: f_00009-6-1 loss: 1.055737  [   64/  118]
train() client id: f_00009-6-2 loss: 0.933404  [   96/  118]
train() client id: f_00009-7-0 loss: 0.942196  [   32/  118]
train() client id: f_00009-7-1 loss: 0.976396  [   64/  118]
train() client id: f_00009-7-2 loss: 0.973624  [   96/  118]
train() client id: f_00009-8-0 loss: 1.101530  [   32/  118]
train() client id: f_00009-8-1 loss: 0.796092  [   64/  118]
train() client id: f_00009-8-2 loss: 0.818617  [   96/  118]
train() client id: f_00009-9-0 loss: 0.907590  [   32/  118]
train() client id: f_00009-9-1 loss: 0.866285  [   64/  118]
train() client id: f_00009-9-2 loss: 0.902256  [   96/  118]
train() client id: f_00009-10-0 loss: 1.147229  [   32/  118]
train() client id: f_00009-10-1 loss: 0.916299  [   64/  118]
train() client id: f_00009-10-2 loss: 0.780603  [   96/  118]
At round 59 accuracy: 0.6472148541114059
At round 59 training accuracy: 0.5955734406438632
At round 59 training loss: 0.8142108526453127
update_location
xs = -3.905658 4.200318 315.009024 18.811294 0.979296 3.956410 -277.443192 -256.324852 299.663977 -242.060879 
ys = 307.587959 290.555839 1.320614 -277.455176 269.350187 252.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 323.458818 307.311468 330.503297 295.525362 287.315997 271.901909 294.926457 275.141974 316.397170 261.934115 
dists_bs = 216.455170 213.227357 519.878229 492.217944 199.676414 195.142099 205.049366 192.227953 500.090401 183.678128 
uav_gains = -117.529547 -116.325662 -118.005067 -115.352469 -114.636400 -113.243561 -115.301164 -113.538715 -117.022510 -112.340901 
bs_gains = -104.957362 -104.774661 -115.612305 -114.947468 -103.976209 -103.696886 -104.299095 -103.513922 -115.140418 -102.960668 
Round 60
-------------------------------
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.0875637  6.22007234 3.04984448 1.12636964 7.17067363 3.45091238
 1.38142813 4.26835796 3.14602737 2.7981042 ]
obj_prev = 35.69935382138897
eta_min = 9.518668131305417e-31	eta_max = 0.9381641580347129
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 8.195255617037894	eta = 0.909090909090909
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 19.17140763760524	eta = 0.38861165126506014
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 13.206898476870052	eta = 0.5641167297661407
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 12.154598615650826	eta = 0.6129558543819038
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 12.090043572188366	eta = 0.6162287451356823
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 12.089768790589424	eta = 0.6162427510544748
af = 7.4502323791253575	bf = 1.0327722521438556	zeta = 12.089768785575135	eta = 0.6162427513100645
eta = 0.6162427513100645
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [0.03994795 0.08401752 0.03931386 0.01363303 0.09701646 0.0462889
 0.01712055 0.05675146 0.04121614 0.03741158]
ene_total = [1.20410292 1.8265834  1.21411492 0.59841362 2.07887597 1.06827008
 0.66615118 1.4063269  1.14028819 0.8866416 ]
ti_comp = [1.15185544 1.27946326 1.14047878 1.19334864 1.28255751 1.28358448
 1.19415401 1.21817826 1.19663411 1.28616459]
ti_coms = [0.20614588 0.07853805 0.21752254 0.16465267 0.07544381 0.07441683
 0.16384731 0.13982306 0.16136721 0.07183672]
t_total = [26.99974823 26.99974823 26.99974823 26.99974823 26.99974823 26.99974823
 26.99974823 26.99974823 26.99974823 26.99974823]
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [3.00308521e-06 2.26429994e-05 2.91972948e-06 1.11204574e-07
 3.46946537e-05 3.76237107e-06 2.19944230e-07 7.69820333e-06
 3.05604171e-06 1.97835807e-06]
ene_total = [0.40991842 0.15659935 0.43253582 0.32736424 0.15068698 0.14803017
 0.32576518 0.27814891 0.32089064 0.14286493]
optimize_network iter = 0 obj = 2.6928046428038024
eta = 0.6162427513100645
freqs = [17340697.3279414  32833111.65400368 17235681.79847541  5712090.78528441
 37821484.27558679 18031106.89702021  7168485.47995044 23293576.56497024
 17221697.8440477  14543852.06878811]
eta_min = 0.6162427513100649	eta_max = 0.7177903732762834
af = 0.0012621994794922648	bf = 1.0327722521438556	zeta = 0.0013884194274414914	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [5.91233479e-07 4.45784864e-06 5.74822789e-07 2.18934405e-08
 6.83052241e-06 7.40718154e-07 4.33015993e-08 1.51558655e-06
 6.01659308e-07 3.89489955e-07]
ene_total = [1.73019841 0.65953123 1.82567951 1.3819049  0.63376085 0.62463054
 1.37514743 1.17363935 1.35437921 0.60294663]
ti_comp = [0.79250899 0.92011681 0.78113233 0.8340022  0.92321106 0.92423803
 0.83480756 0.85883181 0.83728766 0.92681814]
ti_coms = [0.20614588 0.07853805 0.21752254 0.16465267 0.07544381 0.07441683
 0.16384731 0.13982306 0.16136721 0.07183672]
t_total = [26.99974823 26.99974823 26.99974823 26.99974823 26.99974823 26.99974823
 26.99974823 26.99974823 26.99974823 26.99974823]
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [2.03829682e-06 1.40674660e-05 1.99976986e-06 7.31535092e-08
 2.15142627e-05 2.33160556e-06 1.44601475e-07 4.97630551e-06
 2.00560251e-06 1.22411200e-06]
ene_total = [0.55739348 0.21271672 0.58815051 0.44515884 0.2045524  0.20125725
 0.44298339 0.37816177 0.43632847 0.19425169]
optimize_network iter = 1 obj = 3.6609545155387906
eta = 0.7177903732762834
freqs = [17262263.50383285 31270462.24446457 17235681.79847541  5597996.92086191
 35987511.86898271 17151433.11384352  7023260.57451295 22629570.05842017
 16857767.37652736 13823527.31845897]
eta_min = 0.7177903732762844	eta_max = 0.7177903732762764
af = 0.001159170611255296	bf = 1.0327722521438556	zeta = 0.0012750876723808255	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [5.85897150e-07 4.04361532e-06 5.74822789e-07 2.10275717e-08
 6.18415585e-06 6.70207126e-07 4.15648943e-08 1.43041150e-06
 5.76499351e-07 3.51864226e-07]
ene_total = [1.73019796 0.65949647 1.82567951 1.38190483 0.6337066  0.62462463
 1.37514729 1.1736322  1.3543771  0.60294347]
ti_comp = [0.79250899 0.92011681 0.78113233 0.8340022  0.92321106 0.92423803
 0.83480756 0.85883181 0.83728766 0.92681814]
ti_coms = [0.20614588 0.07853805 0.21752254 0.16465267 0.07544381 0.07441683
 0.16384731 0.13982306 0.16136721 0.07183672]
t_total = [26.99974823 26.99974823 26.99974823 26.99974823 26.99974823 26.99974823
 26.99974823 26.99974823 26.99974823 26.99974823]
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [2.03829682e-06 1.40674660e-05 1.99976986e-06 7.31535092e-08
 2.15142627e-05 2.33160556e-06 1.44601475e-07 4.97630551e-06
 2.00560251e-06 1.22411200e-06]
ene_total = [0.55739348 0.21271672 0.58815051 0.44515884 0.2045524  0.20125725
 0.44298339 0.37816177 0.43632847 0.19425169]
optimize_network iter = 2 obj = 3.6609545155387
eta = 0.7177903732762764
freqs = [17262263.50383282 31270462.24446465 17235681.79847537  5597996.92086191
 35987511.8689828  17151433.11384356  7023260.57451294 22629570.05842018
 16857767.37652736 13823527.31845901]
Done!
ene_coms = [0.02061459 0.00785381 0.02175225 0.01646527 0.00754438 0.00744168
 0.01638473 0.01398231 0.01613672 0.00718367]
ene_comp = [1.87731017e-06 1.29564040e-05 1.84182611e-06 6.73757745e-08
 1.98150455e-05 2.14745310e-06 1.33180711e-07 4.58327210e-06
 1.84719809e-06 1.12743045e-06]
ene_total = [0.02061646 0.00786676 0.0217541  0.01646533 0.0075642  0.00744383
 0.01638486 0.01398689 0.01613857 0.0071848 ]
At round 60 energy consumption: 0.135405804121047
At round 60 eta: 0.7177903732762764
At round 60 a_n: 7.62985203809534
At round 60 local rounds: 10.857538900724403
At round 60 global rounds: 27.03611541063686
gradient difference: 0.5548101663589478
train() client id: f_00000-0-0 loss: 1.284250  [   32/  126]
train() client id: f_00000-0-1 loss: 1.321139  [   64/  126]
train() client id: f_00000-0-2 loss: 1.193786  [   96/  126]
train() client id: f_00000-1-0 loss: 1.049425  [   32/  126]
train() client id: f_00000-1-1 loss: 1.274662  [   64/  126]
train() client id: f_00000-1-2 loss: 1.101511  [   96/  126]
train() client id: f_00000-2-0 loss: 1.128280  [   32/  126]
train() client id: f_00000-2-1 loss: 1.287519  [   64/  126]
train() client id: f_00000-2-2 loss: 1.000971  [   96/  126]
train() client id: f_00000-3-0 loss: 1.057778  [   32/  126]
train() client id: f_00000-3-1 loss: 1.174222  [   64/  126]
train() client id: f_00000-3-2 loss: 0.822981  [   96/  126]
train() client id: f_00000-4-0 loss: 0.957835  [   32/  126]
train() client id: f_00000-4-1 loss: 0.884304  [   64/  126]
train() client id: f_00000-4-2 loss: 1.108305  [   96/  126]
train() client id: f_00000-5-0 loss: 0.878173  [   32/  126]
train() client id: f_00000-5-1 loss: 1.053091  [   64/  126]
train() client id: f_00000-5-2 loss: 0.860737  [   96/  126]
train() client id: f_00000-6-0 loss: 0.896003  [   32/  126]
train() client id: f_00000-6-1 loss: 0.882005  [   64/  126]
train() client id: f_00000-6-2 loss: 0.954067  [   96/  126]
train() client id: f_00000-7-0 loss: 0.874318  [   32/  126]
train() client id: f_00000-7-1 loss: 0.893502  [   64/  126]
train() client id: f_00000-7-2 loss: 0.829122  [   96/  126]
train() client id: f_00000-8-0 loss: 0.822059  [   32/  126]
train() client id: f_00000-8-1 loss: 0.975664  [   64/  126]
train() client id: f_00000-8-2 loss: 0.726209  [   96/  126]
train() client id: f_00000-9-0 loss: 0.953663  [   32/  126]
train() client id: f_00000-9-1 loss: 0.779908  [   64/  126]
train() client id: f_00000-9-2 loss: 0.818854  [   96/  126]
train() client id: f_00001-0-0 loss: 0.449977  [   32/  265]
train() client id: f_00001-0-1 loss: 0.445957  [   64/  265]
train() client id: f_00001-0-2 loss: 0.622080  [   96/  265]
train() client id: f_00001-0-3 loss: 0.589340  [  128/  265]
train() client id: f_00001-0-4 loss: 0.472930  [  160/  265]
train() client id: f_00001-0-5 loss: 0.537767  [  192/  265]
train() client id: f_00001-0-6 loss: 0.519349  [  224/  265]
train() client id: f_00001-0-7 loss: 0.486253  [  256/  265]
train() client id: f_00001-1-0 loss: 0.498312  [   32/  265]
train() client id: f_00001-1-1 loss: 0.496081  [   64/  265]
train() client id: f_00001-1-2 loss: 0.486254  [   96/  265]
train() client id: f_00001-1-3 loss: 0.598742  [  128/  265]
train() client id: f_00001-1-4 loss: 0.442897  [  160/  265]
train() client id: f_00001-1-5 loss: 0.488239  [  192/  265]
train() client id: f_00001-1-6 loss: 0.506533  [  224/  265]
train() client id: f_00001-1-7 loss: 0.556258  [  256/  265]
train() client id: f_00001-2-0 loss: 0.487736  [   32/  265]
train() client id: f_00001-2-1 loss: 0.513768  [   64/  265]
train() client id: f_00001-2-2 loss: 0.527930  [   96/  265]
train() client id: f_00001-2-3 loss: 0.453803  [  128/  265]
train() client id: f_00001-2-4 loss: 0.528681  [  160/  265]
train() client id: f_00001-2-5 loss: 0.485505  [  192/  265]
train() client id: f_00001-2-6 loss: 0.419451  [  224/  265]
train() client id: f_00001-2-7 loss: 0.617851  [  256/  265]
train() client id: f_00001-3-0 loss: 0.640056  [   32/  265]
train() client id: f_00001-3-1 loss: 0.416426  [   64/  265]
train() client id: f_00001-3-2 loss: 0.477318  [   96/  265]
train() client id: f_00001-3-3 loss: 0.412239  [  128/  265]
train() client id: f_00001-3-4 loss: 0.472044  [  160/  265]
train() client id: f_00001-3-5 loss: 0.462725  [  192/  265]
train() client id: f_00001-3-6 loss: 0.642332  [  224/  265]
train() client id: f_00001-3-7 loss: 0.467675  [  256/  265]
train() client id: f_00001-4-0 loss: 0.438748  [   32/  265]
train() client id: f_00001-4-1 loss: 0.650592  [   64/  265]
train() client id: f_00001-4-2 loss: 0.427401  [   96/  265]
train() client id: f_00001-4-3 loss: 0.510599  [  128/  265]
train() client id: f_00001-4-4 loss: 0.451179  [  160/  265]
train() client id: f_00001-4-5 loss: 0.501385  [  192/  265]
train() client id: f_00001-4-6 loss: 0.433877  [  224/  265]
train() client id: f_00001-4-7 loss: 0.642498  [  256/  265]
train() client id: f_00001-5-0 loss: 0.640791  [   32/  265]
train() client id: f_00001-5-1 loss: 0.462484  [   64/  265]
train() client id: f_00001-5-2 loss: 0.513962  [   96/  265]
train() client id: f_00001-5-3 loss: 0.636723  [  128/  265]
train() client id: f_00001-5-4 loss: 0.398249  [  160/  265]
train() client id: f_00001-5-5 loss: 0.597366  [  192/  265]
train() client id: f_00001-5-6 loss: 0.406509  [  224/  265]
train() client id: f_00001-5-7 loss: 0.415819  [  256/  265]
train() client id: f_00001-6-0 loss: 0.453773  [   32/  265]
train() client id: f_00001-6-1 loss: 0.448616  [   64/  265]
train() client id: f_00001-6-2 loss: 0.517363  [   96/  265]
train() client id: f_00001-6-3 loss: 0.549699  [  128/  265]
train() client id: f_00001-6-4 loss: 0.606873  [  160/  265]
train() client id: f_00001-6-5 loss: 0.481563  [  192/  265]
train() client id: f_00001-6-6 loss: 0.512665  [  224/  265]
train() client id: f_00001-6-7 loss: 0.518584  [  256/  265]
train() client id: f_00001-7-0 loss: 0.476167  [   32/  265]
train() client id: f_00001-7-1 loss: 0.549645  [   64/  265]
train() client id: f_00001-7-2 loss: 0.504037  [   96/  265]
train() client id: f_00001-7-3 loss: 0.539769  [  128/  265]
train() client id: f_00001-7-4 loss: 0.421992  [  160/  265]
train() client id: f_00001-7-5 loss: 0.547620  [  192/  265]
train() client id: f_00001-7-6 loss: 0.551563  [  224/  265]
train() client id: f_00001-7-7 loss: 0.512352  [  256/  265]
train() client id: f_00001-8-0 loss: 0.536975  [   32/  265]
train() client id: f_00001-8-1 loss: 0.555144  [   64/  265]
train() client id: f_00001-8-2 loss: 0.404219  [   96/  265]
train() client id: f_00001-8-3 loss: 0.496251  [  128/  265]
train() client id: f_00001-8-4 loss: 0.473133  [  160/  265]
train() client id: f_00001-8-5 loss: 0.524049  [  192/  265]
train() client id: f_00001-8-6 loss: 0.619801  [  224/  265]
train() client id: f_00001-8-7 loss: 0.511493  [  256/  265]
train() client id: f_00001-9-0 loss: 0.513223  [   32/  265]
train() client id: f_00001-9-1 loss: 0.481200  [   64/  265]
train() client id: f_00001-9-2 loss: 0.467021  [   96/  265]
train() client id: f_00001-9-3 loss: 0.586555  [  128/  265]
train() client id: f_00001-9-4 loss: 0.483611  [  160/  265]
train() client id: f_00001-9-5 loss: 0.589694  [  192/  265]
train() client id: f_00001-9-6 loss: 0.555870  [  224/  265]
train() client id: f_00001-9-7 loss: 0.455891  [  256/  265]
train() client id: f_00002-0-0 loss: 1.180373  [   32/  124]
train() client id: f_00002-0-1 loss: 1.107074  [   64/  124]
train() client id: f_00002-0-2 loss: 1.300313  [   96/  124]
train() client id: f_00002-1-0 loss: 1.303457  [   32/  124]
train() client id: f_00002-1-1 loss: 1.281306  [   64/  124]
train() client id: f_00002-1-2 loss: 1.095471  [   96/  124]
train() client id: f_00002-2-0 loss: 1.343441  [   32/  124]
train() client id: f_00002-2-1 loss: 1.201801  [   64/  124]
train() client id: f_00002-2-2 loss: 0.876780  [   96/  124]
train() client id: f_00002-3-0 loss: 0.992306  [   32/  124]
train() client id: f_00002-3-1 loss: 0.989047  [   64/  124]
train() client id: f_00002-3-2 loss: 1.225661  [   96/  124]
train() client id: f_00002-4-0 loss: 1.110818  [   32/  124]
train() client id: f_00002-4-1 loss: 1.208924  [   64/  124]
train() client id: f_00002-4-2 loss: 1.166451  [   96/  124]
train() client id: f_00002-5-0 loss: 1.249498  [   32/  124]
train() client id: f_00002-5-1 loss: 0.993554  [   64/  124]
train() client id: f_00002-5-2 loss: 0.993804  [   96/  124]
train() client id: f_00002-6-0 loss: 1.085610  [   32/  124]
train() client id: f_00002-6-1 loss: 1.028939  [   64/  124]
train() client id: f_00002-6-2 loss: 1.210078  [   96/  124]
train() client id: f_00002-7-0 loss: 1.037480  [   32/  124]
train() client id: f_00002-7-1 loss: 1.285634  [   64/  124]
train() client id: f_00002-7-2 loss: 0.917943  [   96/  124]
train() client id: f_00002-8-0 loss: 0.905401  [   32/  124]
train() client id: f_00002-8-1 loss: 1.145882  [   64/  124]
train() client id: f_00002-8-2 loss: 1.124554  [   96/  124]
train() client id: f_00002-9-0 loss: 0.991449  [   32/  124]
train() client id: f_00002-9-1 loss: 0.963017  [   64/  124]
train() client id: f_00002-9-2 loss: 1.365332  [   96/  124]
train() client id: f_00003-0-0 loss: 0.516032  [   32/   43]
train() client id: f_00003-1-0 loss: 0.743152  [   32/   43]
train() client id: f_00003-2-0 loss: 0.725515  [   32/   43]
train() client id: f_00003-3-0 loss: 0.505185  [   32/   43]
train() client id: f_00003-4-0 loss: 0.539841  [   32/   43]
train() client id: f_00003-5-0 loss: 0.373265  [   32/   43]
train() client id: f_00003-6-0 loss: 0.379068  [   32/   43]
train() client id: f_00003-7-0 loss: 0.359008  [   32/   43]
train() client id: f_00003-8-0 loss: 0.512753  [   32/   43]
train() client id: f_00003-9-0 loss: 0.535831  [   32/   43]
train() client id: f_00004-0-0 loss: 0.710764  [   32/  306]
train() client id: f_00004-0-1 loss: 0.733424  [   64/  306]
train() client id: f_00004-0-2 loss: 0.642059  [   96/  306]
train() client id: f_00004-0-3 loss: 0.676517  [  128/  306]
train() client id: f_00004-0-4 loss: 0.717579  [  160/  306]
train() client id: f_00004-0-5 loss: 0.594794  [  192/  306]
train() client id: f_00004-0-6 loss: 0.649346  [  224/  306]
train() client id: f_00004-0-7 loss: 0.649594  [  256/  306]
train() client id: f_00004-0-8 loss: 0.765727  [  288/  306]
train() client id: f_00004-1-0 loss: 0.738381  [   32/  306]
train() client id: f_00004-1-1 loss: 0.697891  [   64/  306]
train() client id: f_00004-1-2 loss: 0.656368  [   96/  306]
train() client id: f_00004-1-3 loss: 0.616207  [  128/  306]
train() client id: f_00004-1-4 loss: 0.545738  [  160/  306]
train() client id: f_00004-1-5 loss: 0.607707  [  192/  306]
train() client id: f_00004-1-6 loss: 0.665617  [  224/  306]
train() client id: f_00004-1-7 loss: 0.749440  [  256/  306]
train() client id: f_00004-1-8 loss: 0.836744  [  288/  306]
train() client id: f_00004-2-0 loss: 0.621858  [   32/  306]
train() client id: f_00004-2-1 loss: 0.711667  [   64/  306]
train() client id: f_00004-2-2 loss: 0.695062  [   96/  306]
train() client id: f_00004-2-3 loss: 0.641779  [  128/  306]
train() client id: f_00004-2-4 loss: 0.727575  [  160/  306]
train() client id: f_00004-2-5 loss: 0.622984  [  192/  306]
train() client id: f_00004-2-6 loss: 0.674993  [  224/  306]
train() client id: f_00004-2-7 loss: 0.670398  [  256/  306]
train() client id: f_00004-2-8 loss: 0.597170  [  288/  306]
train() client id: f_00004-3-0 loss: 0.784523  [   32/  306]
train() client id: f_00004-3-1 loss: 0.611389  [   64/  306]
train() client id: f_00004-3-2 loss: 0.869750  [   96/  306]
train() client id: f_00004-3-3 loss: 0.737056  [  128/  306]
train() client id: f_00004-3-4 loss: 0.649621  [  160/  306]
train() client id: f_00004-3-5 loss: 0.686460  [  192/  306]
train() client id: f_00004-3-6 loss: 0.609380  [  224/  306]
train() client id: f_00004-3-7 loss: 0.561494  [  256/  306]
train() client id: f_00004-3-8 loss: 0.536619  [  288/  306]
train() client id: f_00004-4-0 loss: 0.577881  [   32/  306]
train() client id: f_00004-4-1 loss: 0.801318  [   64/  306]
train() client id: f_00004-4-2 loss: 0.614015  [   96/  306]
train() client id: f_00004-4-3 loss: 0.564985  [  128/  306]
train() client id: f_00004-4-4 loss: 0.734607  [  160/  306]
train() client id: f_00004-4-5 loss: 0.646838  [  192/  306]
train() client id: f_00004-4-6 loss: 0.712914  [  224/  306]
train() client id: f_00004-4-7 loss: 0.750564  [  256/  306]
train() client id: f_00004-4-8 loss: 0.687146  [  288/  306]
train() client id: f_00004-5-0 loss: 0.697630  [   32/  306]
train() client id: f_00004-5-1 loss: 0.710787  [   64/  306]
train() client id: f_00004-5-2 loss: 0.744296  [   96/  306]
train() client id: f_00004-5-3 loss: 0.535570  [  128/  306]
train() client id: f_00004-5-4 loss: 0.606196  [  160/  306]
train() client id: f_00004-5-5 loss: 0.738049  [  192/  306]
train() client id: f_00004-5-6 loss: 0.761886  [  224/  306]
train() client id: f_00004-5-7 loss: 0.711478  [  256/  306]
train() client id: f_00004-5-8 loss: 0.690351  [  288/  306]
train() client id: f_00004-6-0 loss: 0.638755  [   32/  306]
train() client id: f_00004-6-1 loss: 0.613147  [   64/  306]
train() client id: f_00004-6-2 loss: 0.597433  [   96/  306]
train() client id: f_00004-6-3 loss: 0.626692  [  128/  306]
train() client id: f_00004-6-4 loss: 0.606230  [  160/  306]
train() client id: f_00004-6-5 loss: 0.774613  [  192/  306]
train() client id: f_00004-6-6 loss: 0.766541  [  224/  306]
train() client id: f_00004-6-7 loss: 0.758612  [  256/  306]
train() client id: f_00004-6-8 loss: 0.659608  [  288/  306]
train() client id: f_00004-7-0 loss: 0.631361  [   32/  306]
train() client id: f_00004-7-1 loss: 0.670004  [   64/  306]
train() client id: f_00004-7-2 loss: 0.851936  [   96/  306]
train() client id: f_00004-7-3 loss: 0.589710  [  128/  306]
train() client id: f_00004-7-4 loss: 0.597313  [  160/  306]
train() client id: f_00004-7-5 loss: 0.556421  [  192/  306]
train() client id: f_00004-7-6 loss: 0.561104  [  224/  306]
train() client id: f_00004-7-7 loss: 0.771814  [  256/  306]
train() client id: f_00004-7-8 loss: 0.811705  [  288/  306]
train() client id: f_00004-8-0 loss: 0.652235  [   32/  306]
train() client id: f_00004-8-1 loss: 0.536201  [   64/  306]
train() client id: f_00004-8-2 loss: 0.607919  [   96/  306]
train() client id: f_00004-8-3 loss: 0.828413  [  128/  306]
train() client id: f_00004-8-4 loss: 0.715205  [  160/  306]
train() client id: f_00004-8-5 loss: 0.583360  [  192/  306]
train() client id: f_00004-8-6 loss: 0.687428  [  224/  306]
train() client id: f_00004-8-7 loss: 0.838965  [  256/  306]
train() client id: f_00004-8-8 loss: 0.775622  [  288/  306]
train() client id: f_00004-9-0 loss: 0.828051  [   32/  306]
train() client id: f_00004-9-1 loss: 0.673768  [   64/  306]
train() client id: f_00004-9-2 loss: 0.588625  [   96/  306]
train() client id: f_00004-9-3 loss: 0.667679  [  128/  306]
train() client id: f_00004-9-4 loss: 0.643671  [  160/  306]
train() client id: f_00004-9-5 loss: 0.800399  [  192/  306]
train() client id: f_00004-9-6 loss: 0.738275  [  224/  306]
train() client id: f_00004-9-7 loss: 0.614982  [  256/  306]
train() client id: f_00004-9-8 loss: 0.652912  [  288/  306]
train() client id: f_00005-0-0 loss: 0.330676  [   32/  146]
train() client id: f_00005-0-1 loss: 0.411760  [   64/  146]
train() client id: f_00005-0-2 loss: 0.350635  [   96/  146]
train() client id: f_00005-0-3 loss: 0.783769  [  128/  146]
train() client id: f_00005-1-0 loss: 0.548523  [   32/  146]
train() client id: f_00005-1-1 loss: 0.498259  [   64/  146]
train() client id: f_00005-1-2 loss: 0.337192  [   96/  146]
train() client id: f_00005-1-3 loss: 0.416540  [  128/  146]
train() client id: f_00005-2-0 loss: 0.482797  [   32/  146]
train() client id: f_00005-2-1 loss: 0.429750  [   64/  146]
train() client id: f_00005-2-2 loss: 0.560446  [   96/  146]
train() client id: f_00005-2-3 loss: 0.484734  [  128/  146]
train() client id: f_00005-3-0 loss: 0.480245  [   32/  146]
train() client id: f_00005-3-1 loss: 0.485990  [   64/  146]
train() client id: f_00005-3-2 loss: 0.453865  [   96/  146]
train() client id: f_00005-3-3 loss: 0.597622  [  128/  146]
train() client id: f_00005-4-0 loss: 0.396056  [   32/  146]
train() client id: f_00005-4-1 loss: 0.536414  [   64/  146]
train() client id: f_00005-4-2 loss: 0.360130  [   96/  146]
train() client id: f_00005-4-3 loss: 0.496690  [  128/  146]
train() client id: f_00005-5-0 loss: 0.487678  [   32/  146]
train() client id: f_00005-5-1 loss: 0.380191  [   64/  146]
train() client id: f_00005-5-2 loss: 0.724306  [   96/  146]
train() client id: f_00005-5-3 loss: 0.455165  [  128/  146]
train() client id: f_00005-6-0 loss: 0.418502  [   32/  146]
train() client id: f_00005-6-1 loss: 0.464529  [   64/  146]
train() client id: f_00005-6-2 loss: 0.220358  [   96/  146]
train() client id: f_00005-6-3 loss: 0.764801  [  128/  146]
train() client id: f_00005-7-0 loss: 0.299177  [   32/  146]
train() client id: f_00005-7-1 loss: 0.496413  [   64/  146]
train() client id: f_00005-7-2 loss: 0.644696  [   96/  146]
train() client id: f_00005-7-3 loss: 0.389681  [  128/  146]
train() client id: f_00005-8-0 loss: 0.336063  [   32/  146]
train() client id: f_00005-8-1 loss: 0.318005  [   64/  146]
train() client id: f_00005-8-2 loss: 0.560580  [   96/  146]
train() client id: f_00005-8-3 loss: 0.645181  [  128/  146]
train() client id: f_00005-9-0 loss: 0.402362  [   32/  146]
train() client id: f_00005-9-1 loss: 0.174777  [   64/  146]
train() client id: f_00005-9-2 loss: 0.752212  [   96/  146]
train() client id: f_00005-9-3 loss: 0.459821  [  128/  146]
train() client id: f_00006-0-0 loss: 0.445150  [   32/   54]
train() client id: f_00006-1-0 loss: 0.468680  [   32/   54]
train() client id: f_00006-2-0 loss: 0.399959  [   32/   54]
train() client id: f_00006-3-0 loss: 0.445720  [   32/   54]
train() client id: f_00006-4-0 loss: 0.497413  [   32/   54]
train() client id: f_00006-5-0 loss: 0.482998  [   32/   54]
train() client id: f_00006-6-0 loss: 0.398075  [   32/   54]
train() client id: f_00006-7-0 loss: 0.449564  [   32/   54]
train() client id: f_00006-8-0 loss: 0.439620  [   32/   54]
train() client id: f_00006-9-0 loss: 0.465948  [   32/   54]
train() client id: f_00007-0-0 loss: 0.525260  [   32/  179]
train() client id: f_00007-0-1 loss: 0.768375  [   64/  179]
train() client id: f_00007-0-2 loss: 0.659758  [   96/  179]
train() client id: f_00007-0-3 loss: 0.517680  [  128/  179]
train() client id: f_00007-0-4 loss: 0.528953  [  160/  179]
train() client id: f_00007-1-0 loss: 0.550743  [   32/  179]
train() client id: f_00007-1-1 loss: 0.757851  [   64/  179]
train() client id: f_00007-1-2 loss: 0.793579  [   96/  179]
train() client id: f_00007-1-3 loss: 0.460090  [  128/  179]
train() client id: f_00007-1-4 loss: 0.541274  [  160/  179]
train() client id: f_00007-2-0 loss: 0.675978  [   32/  179]
train() client id: f_00007-2-1 loss: 0.802222  [   64/  179]
train() client id: f_00007-2-2 loss: 0.568046  [   96/  179]
train() client id: f_00007-2-3 loss: 0.500059  [  128/  179]
train() client id: f_00007-2-4 loss: 0.484719  [  160/  179]
train() client id: f_00007-3-0 loss: 0.667156  [   32/  179]
train() client id: f_00007-3-1 loss: 0.554348  [   64/  179]
train() client id: f_00007-3-2 loss: 0.703655  [   96/  179]
train() client id: f_00007-3-3 loss: 0.577271  [  128/  179]
train() client id: f_00007-3-4 loss: 0.580662  [  160/  179]
train() client id: f_00007-4-0 loss: 0.539761  [   32/  179]
train() client id: f_00007-4-1 loss: 0.542629  [   64/  179]
train() client id: f_00007-4-2 loss: 0.482946  [   96/  179]
train() client id: f_00007-4-3 loss: 0.670627  [  128/  179]
train() client id: f_00007-4-4 loss: 0.719294  [  160/  179]
train() client id: f_00007-5-0 loss: 0.455472  [   32/  179]
train() client id: f_00007-5-1 loss: 0.505895  [   64/  179]
train() client id: f_00007-5-2 loss: 0.461477  [   96/  179]
train() client id: f_00007-5-3 loss: 0.700418  [  128/  179]
train() client id: f_00007-5-4 loss: 0.876092  [  160/  179]
train() client id: f_00007-6-0 loss: 0.470170  [   32/  179]
train() client id: f_00007-6-1 loss: 0.540215  [   64/  179]
train() client id: f_00007-6-2 loss: 0.782411  [   96/  179]
train() client id: f_00007-6-3 loss: 0.702571  [  128/  179]
train() client id: f_00007-6-4 loss: 0.538626  [  160/  179]
train() client id: f_00007-7-0 loss: 0.509015  [   32/  179]
train() client id: f_00007-7-1 loss: 0.458193  [   64/  179]
train() client id: f_00007-7-2 loss: 0.752868  [   96/  179]
train() client id: f_00007-7-3 loss: 0.555203  [  128/  179]
train() client id: f_00007-7-4 loss: 0.528661  [  160/  179]
train() client id: f_00007-8-0 loss: 0.506496  [   32/  179]
train() client id: f_00007-8-1 loss: 0.456270  [   64/  179]
train() client id: f_00007-8-2 loss: 0.868205  [   96/  179]
train() client id: f_00007-8-3 loss: 0.427234  [  128/  179]
train() client id: f_00007-8-4 loss: 0.522150  [  160/  179]
train() client id: f_00007-9-0 loss: 0.507611  [   32/  179]
train() client id: f_00007-9-1 loss: 0.532487  [   64/  179]
train() client id: f_00007-9-2 loss: 0.653173  [   96/  179]
train() client id: f_00007-9-3 loss: 0.526002  [  128/  179]
train() client id: f_00007-9-4 loss: 0.810541  [  160/  179]
train() client id: f_00008-0-0 loss: 0.706235  [   32/  130]
train() client id: f_00008-0-1 loss: 0.715226  [   64/  130]
train() client id: f_00008-0-2 loss: 0.657507  [   96/  130]
train() client id: f_00008-0-3 loss: 0.818820  [  128/  130]
train() client id: f_00008-1-0 loss: 0.757596  [   32/  130]
train() client id: f_00008-1-1 loss: 0.634294  [   64/  130]
train() client id: f_00008-1-2 loss: 0.813132  [   96/  130]
train() client id: f_00008-1-3 loss: 0.712927  [  128/  130]
train() client id: f_00008-2-0 loss: 0.722558  [   32/  130]
train() client id: f_00008-2-1 loss: 0.699338  [   64/  130]
train() client id: f_00008-2-2 loss: 0.794854  [   96/  130]
train() client id: f_00008-2-3 loss: 0.669157  [  128/  130]
train() client id: f_00008-3-0 loss: 0.709859  [   32/  130]
train() client id: f_00008-3-1 loss: 0.677285  [   64/  130]
train() client id: f_00008-3-2 loss: 0.687651  [   96/  130]
train() client id: f_00008-3-3 loss: 0.807131  [  128/  130]
train() client id: f_00008-4-0 loss: 0.848045  [   32/  130]
train() client id: f_00008-4-1 loss: 0.713455  [   64/  130]
train() client id: f_00008-4-2 loss: 0.726939  [   96/  130]
train() client id: f_00008-4-3 loss: 0.635072  [  128/  130]
train() client id: f_00008-5-0 loss: 0.690716  [   32/  130]
train() client id: f_00008-5-1 loss: 0.690094  [   64/  130]
train() client id: f_00008-5-2 loss: 0.775464  [   96/  130]
train() client id: f_00008-5-3 loss: 0.761882  [  128/  130]
train() client id: f_00008-6-0 loss: 0.763515  [   32/  130]
train() client id: f_00008-6-1 loss: 0.744338  [   64/  130]
train() client id: f_00008-6-2 loss: 0.639603  [   96/  130]
train() client id: f_00008-6-3 loss: 0.760741  [  128/  130]
train() client id: f_00008-7-0 loss: 0.896250  [   32/  130]
train() client id: f_00008-7-1 loss: 0.645962  [   64/  130]
train() client id: f_00008-7-2 loss: 0.679246  [   96/  130]
train() client id: f_00008-7-3 loss: 0.691388  [  128/  130]
train() client id: f_00008-8-0 loss: 0.790867  [   32/  130]
train() client id: f_00008-8-1 loss: 0.790293  [   64/  130]
train() client id: f_00008-8-2 loss: 0.611664  [   96/  130]
train() client id: f_00008-8-3 loss: 0.696195  [  128/  130]
train() client id: f_00008-9-0 loss: 0.697685  [   32/  130]
train() client id: f_00008-9-1 loss: 0.580612  [   64/  130]
train() client id: f_00008-9-2 loss: 0.773352  [   96/  130]
train() client id: f_00008-9-3 loss: 0.823717  [  128/  130]
train() client id: f_00009-0-0 loss: 0.995874  [   32/  118]
train() client id: f_00009-0-1 loss: 0.992016  [   64/  118]
train() client id: f_00009-0-2 loss: 1.134657  [   96/  118]
train() client id: f_00009-1-0 loss: 0.959024  [   32/  118]
train() client id: f_00009-1-1 loss: 1.096438  [   64/  118]
train() client id: f_00009-1-2 loss: 1.085225  [   96/  118]
train() client id: f_00009-2-0 loss: 1.035283  [   32/  118]
train() client id: f_00009-2-1 loss: 1.026823  [   64/  118]
train() client id: f_00009-2-2 loss: 0.863898  [   96/  118]
train() client id: f_00009-3-0 loss: 0.957264  [   32/  118]
train() client id: f_00009-3-1 loss: 0.927663  [   64/  118]
train() client id: f_00009-3-2 loss: 1.036083  [   96/  118]
train() client id: f_00009-4-0 loss: 0.847969  [   32/  118]
train() client id: f_00009-4-1 loss: 1.023744  [   64/  118]
train() client id: f_00009-4-2 loss: 1.010492  [   96/  118]
train() client id: f_00009-5-0 loss: 0.855809  [   32/  118]
train() client id: f_00009-5-1 loss: 1.022677  [   64/  118]
train() client id: f_00009-5-2 loss: 0.881239  [   96/  118]
train() client id: f_00009-6-0 loss: 0.977290  [   32/  118]
train() client id: f_00009-6-1 loss: 0.895409  [   64/  118]
train() client id: f_00009-6-2 loss: 0.920377  [   96/  118]
train() client id: f_00009-7-0 loss: 1.073893  [   32/  118]
train() client id: f_00009-7-1 loss: 0.902439  [   64/  118]
train() client id: f_00009-7-2 loss: 0.817440  [   96/  118]
train() client id: f_00009-8-0 loss: 0.998552  [   32/  118]
train() client id: f_00009-8-1 loss: 0.970340  [   64/  118]
train() client id: f_00009-8-2 loss: 0.836440  [   96/  118]
train() client id: f_00009-9-0 loss: 1.018997  [   32/  118]
train() client id: f_00009-9-1 loss: 0.843298  [   64/  118]
train() client id: f_00009-9-2 loss: 0.904543  [   96/  118]
At round 60 accuracy: 0.6472148541114059
At round 60 training accuracy: 0.5895372233400402
At round 60 training loss: 0.8342361615007551
update_location
xs = -3.905658 4.200318 320.009024 18.811294 0.979296 3.956410 -282.443192 -261.324852 304.663977 -247.060879 
ys = 312.587959 295.555839 1.320614 -282.455176 274.350187 257.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 328.217133 312.043100 335.272306 300.224568 292.008534 276.557028 299.634856 279.805923 321.136745 266.561606 
dists_bs = 219.553457 215.977925 524.593617 496.817930 202.087041 197.189199 207.592569 194.396076 504.838636 185.562020 
uav_gains = -117.854026 -116.694716 -118.310199 -115.749144 -115.048966 -113.667497 -115.699961 -113.962418 -117.366191 -112.758094 
bs_gains = -105.130187 -104.930521 -115.722103 -115.060582 -104.122136 -103.823787 -104.448990 -103.650309 -115.255332 -103.084754 
Round 61
-------------------------------
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.95458156 5.94128521 2.91869021 1.0805571  6.84914591 3.29632295
 1.32415061 4.08054691 3.00608516 2.67279298]
obj_prev = 34.12415860899734
eta_min = 3.98916673942854e-32	eta_max = 0.9387143479457988
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 7.8273257066737205	eta = 0.9090909090909091
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 18.61065565296208	eta = 0.3823481974584871
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 12.717399503431304	eta = 0.5595287496088128
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 11.681826444107466	eta = 0.60912997436458
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 11.617979436161402	eta = 0.6124774692131586
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 11.617704178505617	eta = 0.6124919806097139
af = 7.115750642430655	bf = 1.0136743134601105	zeta = 11.617704173351733	eta = 0.6124919808814296
eta = 0.6124919808814296
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [0.04045173 0.08507706 0.03980964 0.01380496 0.09823992 0.04687264
 0.01733646 0.05746714 0.04173591 0.03788337]
ene_total = [1.16278399 1.74880761 1.17244705 0.58140006 1.9903631  1.02228243
 0.6462616  1.35340359 1.09164915 0.84830559]
ti_comp = [1.21927116 1.35390191 1.20771185 1.26195871 1.35708161 1.35819303
 1.2627831  1.28806187 1.26993947 1.36081379]
ti_coms = [0.21380186 0.07917111 0.22536118 0.17111432 0.07599142 0.07487999
 0.17028993 0.14501116 0.16313355 0.07225923]
t_total = [26.94974403 26.94974403 26.94974403 26.94974403 26.94974403 26.94974403
 26.94974403 26.94974403 26.94974403 26.94974403]
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [2.78285633e-06 2.09963099e-05 2.70344885e-06 1.03251135e-07
 3.21760040e-05 3.48911535e-06 2.04222466e-07 7.14933098e-06
 2.81736924e-06 1.83496748e-06]
ene_total = [0.40211875 0.14928058 0.4238552  0.32179204 0.14351122 0.14088165
 0.32024363 0.27283609 0.3068348  0.13592205]
optimize_network iter = 0 obj = 2.617276020137961
eta = 0.6124919808814296
freqs = [16588488.88317586 31419209.64704492 16481431.96089219  5469654.50966602
 36195288.80600585 17255515.63488949  6864384.21294392 22307602.23984509
 16432245.4685148  13919379.90342778]
eta_min = 0.6124919808814304	eta_max = 0.7228965206792303
af = 0.0011038579011319113	bf = 1.0136743134601105	zeta = 0.0012142436912451024	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [5.41052682e-07 4.08217617e-06 5.25614002e-07 2.00744476e-08
 6.25577148e-06 6.78366036e-07 3.97056477e-08 1.38999799e-06
 5.47762803e-07 3.56760810e-07]
ene_total = [1.71388696 0.63496578 1.80654562 1.37166028 0.60965148 0.60029516
 1.36505352 1.16252622 1.30772847 0.57926126]
ti_comp = [0.8109757  0.94560644 0.79941638 0.85366324 0.94878614 0.94989756
 0.85448763 0.8797664  0.861644   0.95251833]
ti_coms = [0.21380186 0.07917111 0.22536118 0.17111432 0.07599142 0.07487999
 0.17028993 0.14501116 0.16313355 0.07225923]
t_total = [26.94974403 26.94974403 26.94974403 26.94974403 26.94974403 26.94974403
 26.94974403 26.94974403 26.94974403 26.94974403]
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [1.82433614e-06 1.24831928e-05 1.78948422e-06 6.54396715e-08
 1.90913448e-05 2.06877816e-06 1.29353631e-07 4.44460588e-06
 1.77493813e-06 1.08619612e-06]
ene_total = [0.56230711 0.20853359 0.59270505 0.45000056 0.20034537 0.19697486
 0.44783425 0.38146927 0.42905758 0.19005691]
optimize_network iter = 1 obj = 3.659284534384905
eta = 0.7228965206792303
freqs = [16508552.89543737 29777055.50547944 16481431.96089219  5352148.75441332
 34268839.33358462 16331361.10328626  6714818.53144301 21618818.75231183
 16031043.09450184 13163002.59895364]
eta_min = 0.7228965206792295	eta_max = 0.7228965206792275
af = 0.0010051578008740583	bf = 1.0136743134601105	zeta = 0.0011056735809614642	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [5.35850837e-07 3.66661011e-06 5.25614002e-07 1.92211852e-08
 5.60758124e-06 6.07649264e-07 3.79942326e-08 1.30548627e-06
 5.21341470e-07 3.19041588e-07]
ene_total = [1.71388654 0.63493247 1.80654562 1.37166021 0.60959952 0.60028949
 1.36505338 1.16251944 1.30772635 0.57925823]
ti_comp = [0.8109757  0.94560644 0.79941638 0.85366324 0.94878614 0.94989756
 0.85448763 0.8797664  0.861644   0.95251833]
ti_coms = [0.21380186 0.07917111 0.22536118 0.17111432 0.07599142 0.07487999
 0.17028993 0.14501116 0.16313355 0.07225923]
t_total = [26.94974403 26.94974403 26.94974403 26.94974403 26.94974403 26.94974403
 26.94974403 26.94974403 26.94974403 26.94974403]
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [1.82433614e-06 1.24831928e-05 1.78948422e-06 6.54396715e-08
 1.90913448e-05 2.06877816e-06 1.29353631e-07 4.44460588e-06
 1.77493813e-06 1.08619612e-06]
ene_total = [0.56230711 0.20853359 0.59270505 0.45000056 0.20034537 0.19697486
 0.44783425 0.38146927 0.42905758 0.19005691]
optimize_network iter = 2 obj = 3.6592845343848683
eta = 0.7228965206792275
freqs = [16508552.89543735 29777055.50547947 16481431.96089218  5352148.75441332
 34268839.33358465 16331361.10328627  6714818.531443   21618818.75231183
 16031043.09450183 13163002.59895365]
Done!
ene_coms = [0.02138019 0.00791711 0.02253612 0.01711143 0.00759914 0.007488
 0.01702899 0.01450112 0.01631336 0.00722592]
ene_comp = [1.71695361e-06 1.17484177e-05 1.68415312e-06 6.15878170e-08
 1.79676062e-05 1.94700750e-06 1.21739727e-07 4.18299125e-06
 1.67046323e-06 1.02226136e-06]
ene_total = [0.0213819  0.00792886 0.0225378  0.01711149 0.00761711 0.00748995
 0.01702911 0.0145053  0.01631503 0.00722695]
At round 61 energy consumption: 0.13914349848735766
At round 61 eta: 0.7228965206792275
At round 61 a_n: 7.287306191126021
At round 61 local rounds: 10.625424748609518
At round 61 global rounds: 26.298140351714242
gradient difference: 0.4412749409675598
train() client id: f_00000-0-0 loss: 1.300971  [   32/  126]
train() client id: f_00000-0-1 loss: 1.187979  [   64/  126]
train() client id: f_00000-0-2 loss: 1.161515  [   96/  126]
train() client id: f_00000-1-0 loss: 1.111758  [   32/  126]
train() client id: f_00000-1-1 loss: 1.007454  [   64/  126]
train() client id: f_00000-1-2 loss: 1.135336  [   96/  126]
train() client id: f_00000-2-0 loss: 1.108270  [   32/  126]
train() client id: f_00000-2-1 loss: 0.959374  [   64/  126]
train() client id: f_00000-2-2 loss: 1.057528  [   96/  126]
train() client id: f_00000-3-0 loss: 0.816723  [   32/  126]
train() client id: f_00000-3-1 loss: 0.966551  [   64/  126]
train() client id: f_00000-3-2 loss: 0.988166  [   96/  126]
train() client id: f_00000-4-0 loss: 0.990008  [   32/  126]
train() client id: f_00000-4-1 loss: 0.942917  [   64/  126]
train() client id: f_00000-4-2 loss: 0.998854  [   96/  126]
train() client id: f_00000-5-0 loss: 0.830110  [   32/  126]
train() client id: f_00000-5-1 loss: 0.899083  [   64/  126]
train() client id: f_00000-5-2 loss: 0.993594  [   96/  126]
train() client id: f_00000-6-0 loss: 0.810497  [   32/  126]
train() client id: f_00000-6-1 loss: 0.898193  [   64/  126]
train() client id: f_00000-6-2 loss: 0.777625  [   96/  126]
train() client id: f_00000-7-0 loss: 0.941047  [   32/  126]
train() client id: f_00000-7-1 loss: 0.858785  [   64/  126]
train() client id: f_00000-7-2 loss: 0.852344  [   96/  126]
train() client id: f_00000-8-0 loss: 0.941165  [   32/  126]
train() client id: f_00000-8-1 loss: 0.824445  [   64/  126]
train() client id: f_00000-8-2 loss: 0.776438  [   96/  126]
train() client id: f_00000-9-0 loss: 0.847148  [   32/  126]
train() client id: f_00000-9-1 loss: 0.914638  [   64/  126]
train() client id: f_00000-9-2 loss: 0.791104  [   96/  126]
train() client id: f_00001-0-0 loss: 0.603012  [   32/  265]
train() client id: f_00001-0-1 loss: 0.428794  [   64/  265]
train() client id: f_00001-0-2 loss: 0.467817  [   96/  265]
train() client id: f_00001-0-3 loss: 0.530200  [  128/  265]
train() client id: f_00001-0-4 loss: 0.515248  [  160/  265]
train() client id: f_00001-0-5 loss: 0.503091  [  192/  265]
train() client id: f_00001-0-6 loss: 0.610085  [  224/  265]
train() client id: f_00001-0-7 loss: 0.437094  [  256/  265]
train() client id: f_00001-1-0 loss: 0.469658  [   32/  265]
train() client id: f_00001-1-1 loss: 0.573748  [   64/  265]
train() client id: f_00001-1-2 loss: 0.462165  [   96/  265]
train() client id: f_00001-1-3 loss: 0.563511  [  128/  265]
train() client id: f_00001-1-4 loss: 0.532150  [  160/  265]
train() client id: f_00001-1-5 loss: 0.403459  [  192/  265]
train() client id: f_00001-1-6 loss: 0.448688  [  224/  265]
train() client id: f_00001-1-7 loss: 0.534529  [  256/  265]
train() client id: f_00001-2-0 loss: 0.471629  [   32/  265]
train() client id: f_00001-2-1 loss: 0.427117  [   64/  265]
train() client id: f_00001-2-2 loss: 0.551006  [   96/  265]
train() client id: f_00001-2-3 loss: 0.477422  [  128/  265]
train() client id: f_00001-2-4 loss: 0.473663  [  160/  265]
train() client id: f_00001-2-5 loss: 0.468337  [  192/  265]
train() client id: f_00001-2-6 loss: 0.539602  [  224/  265]
train() client id: f_00001-2-7 loss: 0.606948  [  256/  265]
train() client id: f_00001-3-0 loss: 0.438620  [   32/  265]
train() client id: f_00001-3-1 loss: 0.537729  [   64/  265]
train() client id: f_00001-3-2 loss: 0.570348  [   96/  265]
train() client id: f_00001-3-3 loss: 0.437453  [  128/  265]
train() client id: f_00001-3-4 loss: 0.569312  [  160/  265]
train() client id: f_00001-3-5 loss: 0.524544  [  192/  265]
train() client id: f_00001-3-6 loss: 0.432619  [  224/  265]
train() client id: f_00001-3-7 loss: 0.478696  [  256/  265]
train() client id: f_00001-4-0 loss: 0.517844  [   32/  265]
train() client id: f_00001-4-1 loss: 0.496254  [   64/  265]
train() client id: f_00001-4-2 loss: 0.564842  [   96/  265]
train() client id: f_00001-4-3 loss: 0.515861  [  128/  265]
train() client id: f_00001-4-4 loss: 0.419731  [  160/  265]
train() client id: f_00001-4-5 loss: 0.498099  [  192/  265]
train() client id: f_00001-4-6 loss: 0.448994  [  224/  265]
train() client id: f_00001-4-7 loss: 0.440649  [  256/  265]
train() client id: f_00001-5-0 loss: 0.418813  [   32/  265]
train() client id: f_00001-5-1 loss: 0.452708  [   64/  265]
train() client id: f_00001-5-2 loss: 0.449296  [   96/  265]
train() client id: f_00001-5-3 loss: 0.467232  [  128/  265]
train() client id: f_00001-5-4 loss: 0.652326  [  160/  265]
train() client id: f_00001-5-5 loss: 0.441348  [  192/  265]
train() client id: f_00001-5-6 loss: 0.517536  [  224/  265]
train() client id: f_00001-5-7 loss: 0.571311  [  256/  265]
train() client id: f_00001-6-0 loss: 0.505402  [   32/  265]
train() client id: f_00001-6-1 loss: 0.598413  [   64/  265]
train() client id: f_00001-6-2 loss: 0.519171  [   96/  265]
train() client id: f_00001-6-3 loss: 0.424055  [  128/  265]
train() client id: f_00001-6-4 loss: 0.494143  [  160/  265]
train() client id: f_00001-6-5 loss: 0.610312  [  192/  265]
train() client id: f_00001-6-6 loss: 0.397916  [  224/  265]
train() client id: f_00001-6-7 loss: 0.420006  [  256/  265]
train() client id: f_00001-7-0 loss: 0.506728  [   32/  265]
train() client id: f_00001-7-1 loss: 0.583425  [   64/  265]
train() client id: f_00001-7-2 loss: 0.472990  [   96/  265]
train() client id: f_00001-7-3 loss: 0.480352  [  128/  265]
train() client id: f_00001-7-4 loss: 0.424956  [  160/  265]
train() client id: f_00001-7-5 loss: 0.449235  [  192/  265]
train() client id: f_00001-7-6 loss: 0.507531  [  224/  265]
train() client id: f_00001-7-7 loss: 0.554338  [  256/  265]
train() client id: f_00001-8-0 loss: 0.567818  [   32/  265]
train() client id: f_00001-8-1 loss: 0.452153  [   64/  265]
train() client id: f_00001-8-2 loss: 0.542224  [   96/  265]
train() client id: f_00001-8-3 loss: 0.528366  [  128/  265]
train() client id: f_00001-8-4 loss: 0.473360  [  160/  265]
train() client id: f_00001-8-5 loss: 0.398759  [  192/  265]
train() client id: f_00001-8-6 loss: 0.564345  [  224/  265]
train() client id: f_00001-8-7 loss: 0.434848  [  256/  265]
train() client id: f_00001-9-0 loss: 0.565418  [   32/  265]
train() client id: f_00001-9-1 loss: 0.633883  [   64/  265]
train() client id: f_00001-9-2 loss: 0.404232  [   96/  265]
train() client id: f_00001-9-3 loss: 0.563857  [  128/  265]
train() client id: f_00001-9-4 loss: 0.586455  [  160/  265]
train() client id: f_00001-9-5 loss: 0.432942  [  192/  265]
train() client id: f_00001-9-6 loss: 0.397290  [  224/  265]
train() client id: f_00001-9-7 loss: 0.395741  [  256/  265]
train() client id: f_00002-0-0 loss: 0.772713  [   32/  124]
train() client id: f_00002-0-1 loss: 1.030435  [   64/  124]
train() client id: f_00002-0-2 loss: 0.925573  [   96/  124]
train() client id: f_00002-1-0 loss: 0.768798  [   32/  124]
train() client id: f_00002-1-1 loss: 0.893328  [   64/  124]
train() client id: f_00002-1-2 loss: 0.901776  [   96/  124]
train() client id: f_00002-2-0 loss: 0.681873  [   32/  124]
train() client id: f_00002-2-1 loss: 0.961299  [   64/  124]
train() client id: f_00002-2-2 loss: 0.694592  [   96/  124]
train() client id: f_00002-3-0 loss: 0.676138  [   32/  124]
train() client id: f_00002-3-1 loss: 0.679534  [   64/  124]
train() client id: f_00002-3-2 loss: 0.949478  [   96/  124]
train() client id: f_00002-4-0 loss: 0.610965  [   32/  124]
train() client id: f_00002-4-1 loss: 0.867465  [   64/  124]
train() client id: f_00002-4-2 loss: 0.621092  [   96/  124]
train() client id: f_00002-5-0 loss: 0.723070  [   32/  124]
train() client id: f_00002-5-1 loss: 0.786371  [   64/  124]
train() client id: f_00002-5-2 loss: 0.684686  [   96/  124]
train() client id: f_00002-6-0 loss: 0.624887  [   32/  124]
train() client id: f_00002-6-1 loss: 0.754645  [   64/  124]
train() client id: f_00002-6-2 loss: 0.746373  [   96/  124]
train() client id: f_00002-7-0 loss: 0.706914  [   32/  124]
train() client id: f_00002-7-1 loss: 0.656308  [   64/  124]
train() client id: f_00002-7-2 loss: 0.510927  [   96/  124]
train() client id: f_00002-8-0 loss: 0.496556  [   32/  124]
train() client id: f_00002-8-1 loss: 0.845067  [   64/  124]
train() client id: f_00002-8-2 loss: 0.791567  [   96/  124]
train() client id: f_00002-9-0 loss: 0.621072  [   32/  124]
train() client id: f_00002-9-1 loss: 0.850702  [   64/  124]
train() client id: f_00002-9-2 loss: 0.567723  [   96/  124]
train() client id: f_00003-0-0 loss: 0.480087  [   32/   43]
train() client id: f_00003-1-0 loss: 0.590572  [   32/   43]
train() client id: f_00003-2-0 loss: 0.618701  [   32/   43]
train() client id: f_00003-3-0 loss: 0.828634  [   32/   43]
train() client id: f_00003-4-0 loss: 0.711770  [   32/   43]
train() client id: f_00003-5-0 loss: 0.586363  [   32/   43]
train() client id: f_00003-6-0 loss: 0.557013  [   32/   43]
train() client id: f_00003-7-0 loss: 0.528158  [   32/   43]
train() client id: f_00003-8-0 loss: 0.489027  [   32/   43]
train() client id: f_00003-9-0 loss: 0.607383  [   32/   43]
train() client id: f_00004-0-0 loss: 1.009039  [   32/  306]
train() client id: f_00004-0-1 loss: 1.098249  [   64/  306]
train() client id: f_00004-0-2 loss: 0.822759  [   96/  306]
train() client id: f_00004-0-3 loss: 0.902971  [  128/  306]
train() client id: f_00004-0-4 loss: 0.806365  [  160/  306]
train() client id: f_00004-0-5 loss: 0.756992  [  192/  306]
train() client id: f_00004-0-6 loss: 0.727802  [  224/  306]
train() client id: f_00004-0-7 loss: 0.858879  [  256/  306]
train() client id: f_00004-0-8 loss: 0.805456  [  288/  306]
train() client id: f_00004-1-0 loss: 0.723607  [   32/  306]
train() client id: f_00004-1-1 loss: 0.819032  [   64/  306]
train() client id: f_00004-1-2 loss: 0.864634  [   96/  306]
train() client id: f_00004-1-3 loss: 0.899047  [  128/  306]
train() client id: f_00004-1-4 loss: 0.860842  [  160/  306]
train() client id: f_00004-1-5 loss: 0.915750  [  192/  306]
train() client id: f_00004-1-6 loss: 0.929433  [  224/  306]
train() client id: f_00004-1-7 loss: 0.819718  [  256/  306]
train() client id: f_00004-1-8 loss: 0.814231  [  288/  306]
train() client id: f_00004-2-0 loss: 0.779933  [   32/  306]
train() client id: f_00004-2-1 loss: 0.932257  [   64/  306]
train() client id: f_00004-2-2 loss: 0.798549  [   96/  306]
train() client id: f_00004-2-3 loss: 0.976998  [  128/  306]
train() client id: f_00004-2-4 loss: 0.815548  [  160/  306]
train() client id: f_00004-2-5 loss: 0.826593  [  192/  306]
train() client id: f_00004-2-6 loss: 0.830413  [  224/  306]
train() client id: f_00004-2-7 loss: 0.785127  [  256/  306]
train() client id: f_00004-2-8 loss: 0.832085  [  288/  306]
train() client id: f_00004-3-0 loss: 0.857185  [   32/  306]
train() client id: f_00004-3-1 loss: 0.705793  [   64/  306]
train() client id: f_00004-3-2 loss: 0.743681  [   96/  306]
train() client id: f_00004-3-3 loss: 0.801599  [  128/  306]
train() client id: f_00004-3-4 loss: 0.900657  [  160/  306]
train() client id: f_00004-3-5 loss: 0.859549  [  192/  306]
train() client id: f_00004-3-6 loss: 0.773640  [  224/  306]
train() client id: f_00004-3-7 loss: 0.984086  [  256/  306]
train() client id: f_00004-3-8 loss: 0.979179  [  288/  306]
train() client id: f_00004-4-0 loss: 0.950008  [   32/  306]
train() client id: f_00004-4-1 loss: 0.802564  [   64/  306]
train() client id: f_00004-4-2 loss: 0.839192  [   96/  306]
train() client id: f_00004-4-3 loss: 0.755123  [  128/  306]
train() client id: f_00004-4-4 loss: 0.844626  [  160/  306]
train() client id: f_00004-4-5 loss: 0.887360  [  192/  306]
train() client id: f_00004-4-6 loss: 0.842504  [  224/  306]
train() client id: f_00004-4-7 loss: 0.874590  [  256/  306]
train() client id: f_00004-4-8 loss: 0.842444  [  288/  306]
train() client id: f_00004-5-0 loss: 0.884469  [   32/  306]
train() client id: f_00004-5-1 loss: 0.847007  [   64/  306]
train() client id: f_00004-5-2 loss: 0.776083  [   96/  306]
train() client id: f_00004-5-3 loss: 0.901035  [  128/  306]
train() client id: f_00004-5-4 loss: 0.865660  [  160/  306]
train() client id: f_00004-5-5 loss: 0.856139  [  192/  306]
train() client id: f_00004-5-6 loss: 0.830003  [  224/  306]
train() client id: f_00004-5-7 loss: 0.874770  [  256/  306]
train() client id: f_00004-5-8 loss: 0.888077  [  288/  306]
train() client id: f_00004-6-0 loss: 0.834352  [   32/  306]
train() client id: f_00004-6-1 loss: 0.838839  [   64/  306]
train() client id: f_00004-6-2 loss: 0.911403  [   96/  306]
train() client id: f_00004-6-3 loss: 0.769792  [  128/  306]
train() client id: f_00004-6-4 loss: 0.835217  [  160/  306]
train() client id: f_00004-6-5 loss: 0.917767  [  192/  306]
train() client id: f_00004-6-6 loss: 0.872882  [  224/  306]
train() client id: f_00004-6-7 loss: 0.754198  [  256/  306]
train() client id: f_00004-6-8 loss: 0.807958  [  288/  306]
train() client id: f_00004-7-0 loss: 0.848554  [   32/  306]
train() client id: f_00004-7-1 loss: 0.895192  [   64/  306]
train() client id: f_00004-7-2 loss: 0.855657  [   96/  306]
train() client id: f_00004-7-3 loss: 0.829236  [  128/  306]
train() client id: f_00004-7-4 loss: 0.858365  [  160/  306]
train() client id: f_00004-7-5 loss: 0.797331  [  192/  306]
train() client id: f_00004-7-6 loss: 0.825718  [  224/  306]
train() client id: f_00004-7-7 loss: 0.878642  [  256/  306]
train() client id: f_00004-7-8 loss: 0.774472  [  288/  306]
train() client id: f_00004-8-0 loss: 0.797777  [   32/  306]
train() client id: f_00004-8-1 loss: 0.944666  [   64/  306]
train() client id: f_00004-8-2 loss: 0.694856  [   96/  306]
train() client id: f_00004-8-3 loss: 0.914086  [  128/  306]
train() client id: f_00004-8-4 loss: 0.885525  [  160/  306]
train() client id: f_00004-8-5 loss: 0.779023  [  192/  306]
train() client id: f_00004-8-6 loss: 0.900589  [  224/  306]
train() client id: f_00004-8-7 loss: 0.921011  [  256/  306]
train() client id: f_00004-8-8 loss: 0.777096  [  288/  306]
train() client id: f_00004-9-0 loss: 0.787258  [   32/  306]
train() client id: f_00004-9-1 loss: 0.848988  [   64/  306]
train() client id: f_00004-9-2 loss: 0.963365  [   96/  306]
train() client id: f_00004-9-3 loss: 0.752746  [  128/  306]
train() client id: f_00004-9-4 loss: 0.844895  [  160/  306]
train() client id: f_00004-9-5 loss: 0.842269  [  192/  306]
train() client id: f_00004-9-6 loss: 0.926934  [  224/  306]
train() client id: f_00004-9-7 loss: 0.887737  [  256/  306]
train() client id: f_00004-9-8 loss: 0.773711  [  288/  306]
train() client id: f_00005-0-0 loss: 0.707915  [   32/  146]
train() client id: f_00005-0-1 loss: 0.820735  [   64/  146]
train() client id: f_00005-0-2 loss: 0.465693  [   96/  146]
train() client id: f_00005-0-3 loss: 0.465366  [  128/  146]
train() client id: f_00005-1-0 loss: 0.620702  [   32/  146]
train() client id: f_00005-1-1 loss: 0.488767  [   64/  146]
train() client id: f_00005-1-2 loss: 0.587090  [   96/  146]
train() client id: f_00005-1-3 loss: 0.727826  [  128/  146]
train() client id: f_00005-2-0 loss: 0.548975  [   32/  146]
train() client id: f_00005-2-1 loss: 0.556192  [   64/  146]
train() client id: f_00005-2-2 loss: 0.642818  [   96/  146]
train() client id: f_00005-2-3 loss: 0.613401  [  128/  146]
train() client id: f_00005-3-0 loss: 0.484512  [   32/  146]
train() client id: f_00005-3-1 loss: 0.814019  [   64/  146]
train() client id: f_00005-3-2 loss: 0.713174  [   96/  146]
train() client id: f_00005-3-3 loss: 0.474864  [  128/  146]
train() client id: f_00005-4-0 loss: 0.422651  [   32/  146]
train() client id: f_00005-4-1 loss: 0.779849  [   64/  146]
train() client id: f_00005-4-2 loss: 0.351222  [   96/  146]
train() client id: f_00005-4-3 loss: 0.751355  [  128/  146]
train() client id: f_00005-5-0 loss: 0.504938  [   32/  146]
train() client id: f_00005-5-1 loss: 0.653822  [   64/  146]
train() client id: f_00005-5-2 loss: 0.514429  [   96/  146]
train() client id: f_00005-5-3 loss: 0.858263  [  128/  146]
train() client id: f_00005-6-0 loss: 0.737320  [   32/  146]
train() client id: f_00005-6-1 loss: 0.501248  [   64/  146]
train() client id: f_00005-6-2 loss: 0.725593  [   96/  146]
train() client id: f_00005-6-3 loss: 0.648656  [  128/  146]
train() client id: f_00005-7-0 loss: 0.613951  [   32/  146]
train() client id: f_00005-7-1 loss: 0.588030  [   64/  146]
train() client id: f_00005-7-2 loss: 0.716790  [   96/  146]
train() client id: f_00005-7-3 loss: 0.545798  [  128/  146]
train() client id: f_00005-8-0 loss: 0.680378  [   32/  146]
train() client id: f_00005-8-1 loss: 0.491602  [   64/  146]
train() client id: f_00005-8-2 loss: 0.530150  [   96/  146]
train() client id: f_00005-8-3 loss: 0.709874  [  128/  146]
train() client id: f_00005-9-0 loss: 0.480110  [   32/  146]
train() client id: f_00005-9-1 loss: 0.407874  [   64/  146]
train() client id: f_00005-9-2 loss: 0.795521  [   96/  146]
train() client id: f_00005-9-3 loss: 0.811013  [  128/  146]
train() client id: f_00006-0-0 loss: 0.462161  [   32/   54]
train() client id: f_00006-1-0 loss: 0.455174  [   32/   54]
train() client id: f_00006-2-0 loss: 0.493260  [   32/   54]
train() client id: f_00006-3-0 loss: 0.493677  [   32/   54]
train() client id: f_00006-4-0 loss: 0.494864  [   32/   54]
train() client id: f_00006-5-0 loss: 0.452353  [   32/   54]
train() client id: f_00006-6-0 loss: 0.458242  [   32/   54]
train() client id: f_00006-7-0 loss: 0.488500  [   32/   54]
train() client id: f_00006-8-0 loss: 0.384428  [   32/   54]
train() client id: f_00006-9-0 loss: 0.482501  [   32/   54]
train() client id: f_00007-0-0 loss: 0.341564  [   32/  179]
train() client id: f_00007-0-1 loss: 0.636661  [   64/  179]
train() client id: f_00007-0-2 loss: 0.308411  [   96/  179]
train() client id: f_00007-0-3 loss: 0.470785  [  128/  179]
train() client id: f_00007-0-4 loss: 0.304319  [  160/  179]
train() client id: f_00007-1-0 loss: 0.512789  [   32/  179]
train() client id: f_00007-1-1 loss: 0.515177  [   64/  179]
train() client id: f_00007-1-2 loss: 0.393613  [   96/  179]
train() client id: f_00007-1-3 loss: 0.289566  [  128/  179]
train() client id: f_00007-1-4 loss: 0.264487  [  160/  179]
train() client id: f_00007-2-0 loss: 0.287921  [   32/  179]
train() client id: f_00007-2-1 loss: 0.277454  [   64/  179]
train() client id: f_00007-2-2 loss: 0.508504  [   96/  179]
train() client id: f_00007-2-3 loss: 0.359970  [  128/  179]
train() client id: f_00007-2-4 loss: 0.487719  [  160/  179]
train() client id: f_00007-3-0 loss: 0.583972  [   32/  179]
train() client id: f_00007-3-1 loss: 0.407942  [   64/  179]
train() client id: f_00007-3-2 loss: 0.331134  [   96/  179]
train() client id: f_00007-3-3 loss: 0.273054  [  128/  179]
train() client id: f_00007-3-4 loss: 0.429043  [  160/  179]
train() client id: f_00007-4-0 loss: 0.543870  [   32/  179]
train() client id: f_00007-4-1 loss: 0.431527  [   64/  179]
train() client id: f_00007-4-2 loss: 0.232415  [   96/  179]
train() client id: f_00007-4-3 loss: 0.321725  [  128/  179]
train() client id: f_00007-4-4 loss: 0.168831  [  160/  179]
train() client id: f_00007-5-0 loss: 0.334023  [   32/  179]
train() client id: f_00007-5-1 loss: 0.313331  [   64/  179]
train() client id: f_00007-5-2 loss: 0.475259  [   96/  179]
train() client id: f_00007-5-3 loss: 0.393892  [  128/  179]
train() client id: f_00007-5-4 loss: 0.372869  [  160/  179]
train() client id: f_00007-6-0 loss: 0.177250  [   32/  179]
train() client id: f_00007-6-1 loss: 0.211301  [   64/  179]
train() client id: f_00007-6-2 loss: 0.370045  [   96/  179]
train() client id: f_00007-6-3 loss: 0.548228  [  128/  179]
train() client id: f_00007-6-4 loss: 0.565977  [  160/  179]
train() client id: f_00007-7-0 loss: 0.213643  [   32/  179]
train() client id: f_00007-7-1 loss: 0.214693  [   64/  179]
train() client id: f_00007-7-2 loss: 0.542038  [   96/  179]
train() client id: f_00007-7-3 loss: 0.405611  [  128/  179]
train() client id: f_00007-7-4 loss: 0.321276  [  160/  179]
train() client id: f_00007-8-0 loss: 0.187559  [   32/  179]
train() client id: f_00007-8-1 loss: 0.311336  [   64/  179]
train() client id: f_00007-8-2 loss: 0.351588  [   96/  179]
train() client id: f_00007-8-3 loss: 0.225007  [  128/  179]
train() client id: f_00007-8-4 loss: 0.684477  [  160/  179]
train() client id: f_00007-9-0 loss: 0.429535  [   32/  179]
train() client id: f_00007-9-1 loss: 0.398468  [   64/  179]
train() client id: f_00007-9-2 loss: 0.353536  [   96/  179]
train() client id: f_00007-9-3 loss: 0.178967  [  128/  179]
train() client id: f_00007-9-4 loss: 0.372537  [  160/  179]
train() client id: f_00008-0-0 loss: 0.673562  [   32/  130]
train() client id: f_00008-0-1 loss: 0.536851  [   64/  130]
train() client id: f_00008-0-2 loss: 0.747240  [   96/  130]
train() client id: f_00008-0-3 loss: 0.480446  [  128/  130]
train() client id: f_00008-1-0 loss: 0.535766  [   32/  130]
train() client id: f_00008-1-1 loss: 0.678485  [   64/  130]
train() client id: f_00008-1-2 loss: 0.593889  [   96/  130]
train() client id: f_00008-1-3 loss: 0.571292  [  128/  130]
train() client id: f_00008-2-0 loss: 0.581641  [   32/  130]
train() client id: f_00008-2-1 loss: 0.647747  [   64/  130]
train() client id: f_00008-2-2 loss: 0.594293  [   96/  130]
train() client id: f_00008-2-3 loss: 0.615830  [  128/  130]
train() client id: f_00008-3-0 loss: 0.660247  [   32/  130]
train() client id: f_00008-3-1 loss: 0.672212  [   64/  130]
train() client id: f_00008-3-2 loss: 0.600139  [   96/  130]
train() client id: f_00008-3-3 loss: 0.522793  [  128/  130]
train() client id: f_00008-4-0 loss: 0.498594  [   32/  130]
train() client id: f_00008-4-1 loss: 0.659080  [   64/  130]
train() client id: f_00008-4-2 loss: 0.683537  [   96/  130]
train() client id: f_00008-4-3 loss: 0.569060  [  128/  130]
train() client id: f_00008-5-0 loss: 0.642513  [   32/  130]
train() client id: f_00008-5-1 loss: 0.616012  [   64/  130]
train() client id: f_00008-5-2 loss: 0.646727  [   96/  130]
train() client id: f_00008-5-3 loss: 0.543576  [  128/  130]
train() client id: f_00008-6-0 loss: 0.555911  [   32/  130]
train() client id: f_00008-6-1 loss: 0.754213  [   64/  130]
train() client id: f_00008-6-2 loss: 0.551169  [   96/  130]
train() client id: f_00008-6-3 loss: 0.593214  [  128/  130]
train() client id: f_00008-7-0 loss: 0.551222  [   32/  130]
train() client id: f_00008-7-1 loss: 0.698047  [   64/  130]
train() client id: f_00008-7-2 loss: 0.535276  [   96/  130]
train() client id: f_00008-7-3 loss: 0.664008  [  128/  130]
train() client id: f_00008-8-0 loss: 0.615000  [   32/  130]
train() client id: f_00008-8-1 loss: 0.680441  [   64/  130]
train() client id: f_00008-8-2 loss: 0.574064  [   96/  130]
train() client id: f_00008-8-3 loss: 0.577885  [  128/  130]
train() client id: f_00008-9-0 loss: 0.575613  [   32/  130]
train() client id: f_00008-9-1 loss: 0.549222  [   64/  130]
train() client id: f_00008-9-2 loss: 0.695281  [   96/  130]
train() client id: f_00008-9-3 loss: 0.630413  [  128/  130]
train() client id: f_00009-0-0 loss: 1.065711  [   32/  118]
train() client id: f_00009-0-1 loss: 0.941144  [   64/  118]
train() client id: f_00009-0-2 loss: 1.042112  [   96/  118]
train() client id: f_00009-1-0 loss: 0.898550  [   32/  118]
train() client id: f_00009-1-1 loss: 0.885401  [   64/  118]
train() client id: f_00009-1-2 loss: 0.995940  [   96/  118]
train() client id: f_00009-2-0 loss: 0.933868  [   32/  118]
train() client id: f_00009-2-1 loss: 1.003925  [   64/  118]
train() client id: f_00009-2-2 loss: 0.882450  [   96/  118]
train() client id: f_00009-3-0 loss: 0.905365  [   32/  118]
train() client id: f_00009-3-1 loss: 0.867184  [   64/  118]
train() client id: f_00009-3-2 loss: 0.967930  [   96/  118]
train() client id: f_00009-4-0 loss: 0.919318  [   32/  118]
train() client id: f_00009-4-1 loss: 0.777159  [   64/  118]
train() client id: f_00009-4-2 loss: 0.932841  [   96/  118]
train() client id: f_00009-5-0 loss: 1.095529  [   32/  118]
train() client id: f_00009-5-1 loss: 0.852883  [   64/  118]
train() client id: f_00009-5-2 loss: 0.743128  [   96/  118]
train() client id: f_00009-6-0 loss: 0.972662  [   32/  118]
train() client id: f_00009-6-1 loss: 0.846413  [   64/  118]
train() client id: f_00009-6-2 loss: 0.786289  [   96/  118]
train() client id: f_00009-7-0 loss: 0.844969  [   32/  118]
train() client id: f_00009-7-1 loss: 0.955048  [   64/  118]
train() client id: f_00009-7-2 loss: 0.805075  [   96/  118]
train() client id: f_00009-8-0 loss: 0.787664  [   32/  118]
train() client id: f_00009-8-1 loss: 0.754804  [   64/  118]
train() client id: f_00009-8-2 loss: 1.014867  [   96/  118]
train() client id: f_00009-9-0 loss: 0.780357  [   32/  118]
train() client id: f_00009-9-1 loss: 1.016350  [   64/  118]
train() client id: f_00009-9-2 loss: 0.970285  [   96/  118]
At round 61 accuracy: 0.6472148541114059
At round 61 training accuracy: 0.5935613682092555
At round 61 training loss: 0.8280861887862632
update_location
xs = -3.905658 4.200318 325.009024 18.811294 0.979296 3.956410 -287.443192 -266.324852 309.663977 -252.060879 
ys = 317.587959 300.555839 1.320614 -287.455176 279.350187 262.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 332.982531 316.782978 340.047952 304.933342 296.711115 281.223987 304.352557 284.481288 325.884103 271.202320 
dists_bs = 222.720901 218.808187 529.314229 501.425575 204.591481 199.340718 210.223944 196.667443 509.591688 187.560316 
uav_gains = -118.165355 -117.051000 -118.602644 -116.135283 -115.453564 -114.090658 -116.088306 -114.383654 -117.696649 -113.179841 
bs_gains = -105.304366 -105.088839 -115.831039 -115.172841 -104.271910 -103.955748 -104.602160 -103.791568 -115.369285 -103.215006 
Round 62
-------------------------------
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.82115899 5.66246847 2.78706489 1.03447434 6.52759612 3.14171865
 1.26660636 3.89259813 2.86603349 2.54747095]
obj_prev = 32.54719040436046
eta_min = 1.2218214947720352e-33	eta_max = 0.9394406220974982
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 7.459395796309552	eta = 0.9090909090909091
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 18.0305957827678	eta = 0.37609788314465736
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 12.220288063290287	eta = 0.5549189078534793
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 11.203657475325056	eta = 0.605272779953424
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 11.140690513111194	eta = 0.6086937697223753
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 11.140415786273087	eta = 0.6087087803393881
af = 6.781268905735956	bf = 0.9928211124535382	zeta = 11.14041578100363	eta = 0.6087087806273096
eta = 0.6087087806273096
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [0.040963   0.08615235 0.0403128  0.01397944 0.09948158 0.04746507
 0.01755557 0.05819347 0.04226342 0.03836218]
ene_total = [1.12030085 1.67073395 1.1295553  0.56372656 1.90151444 0.97618988
 0.62570552 1.29997444 1.04280632 0.80990852]
ti_comp = [1.2940368  1.43579767 1.28232047 1.33779388 1.43906047 1.44025443
 1.33863419 1.36510734 1.35070396 1.44291406]
ti_coms = [0.22158526 0.0798244  0.23330159 0.17782818 0.0765616  0.07536764
 0.17698787 0.15051473 0.1649181  0.07270801]
t_total = [26.89973984 26.89973984 26.89973984 26.89973984 26.89973984 26.89973984
 26.89973984 26.89973984 26.89973984 26.89973984]
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [2.56544140e-06 1.93862966e-05 2.49009332e-06 9.54048993e-08
 2.97132334e-05 3.22198749e-06 1.88712718e-07 6.60950921e-06
 2.58614832e-06 1.69476237e-06]
ene_total = [0.39332206 0.14201894 0.41411524 0.31561678 0.13641131 0.13382205
 0.31412702 0.26725561 0.29274776 0.12907455]
optimize_network iter = 0 obj = 2.5385113123879783
eta = 0.6087087806273096
freqs = [15827603.24540961 30001562.15823117 15718689.94891933  5224809.85098799
 34564766.11963843 16478014.31267369  6557270.26566169 21314613.85120587
 15644958.54527248 13293299.23888385]
eta_min = 0.6087087806273109	eta_max = 0.7286862650795968
af = 0.0009590243179496368	bf = 0.9928211124535382	zeta = 0.0010549267497446007	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [4.92556679e-07 3.72210798e-06 4.78090085e-07 1.83174406e-08
 5.70484736e-06 6.18611464e-07 3.62322484e-08 1.26900498e-06
 4.96532343e-07 3.25389044e-07]
ene_total = [1.6927798  0.61008177 1.78228252 1.35847294 0.58530795 0.57579846
 1.35205494 1.14991453 1.25988629 0.5554585 ]
ti_comp = [0.82931765 0.97107852 0.81760132 0.87307473 0.97434132 0.97553528
 0.87391504 0.90038819 0.88598481 0.97819491]
ti_coms = [0.22158526 0.0798244  0.23330159 0.17782818 0.0765616  0.07536764
 0.17698787 0.15051473 0.1649181  0.07270801]
t_total = [26.89973984 26.89973984 26.89973984 26.89973984 26.89973984 26.89973984
 26.89973984 26.89973984 26.89973984 26.89973984]
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [1.61900639e-06 1.09851926e-05 1.58767144e-06 5.80605679e-08
 1.68004452e-05 1.82033517e-06 1.14768207e-07 3.93802316e-06
 1.55795981e-06 9.55815292e-07]
ene_total = [0.56722855 0.204606   0.59721779 0.45518453 0.19640314 0.19296354
 0.45303505 0.38537017 0.42217726 0.18613361]
optimize_network iter = 1 obj = 3.660319648720293
eta = 0.7286862650795968
freqs = [15746566.89412167 28283149.87566576 15718689.94891934  5104500.91249742
 32549667.43739824 15511225.99197442  6404139.63058966 20604377.17176979
 15207342.34336579 12502384.68143362]
eta_min = 0.7286862650796062	eta_max = 0.7286862650795745
af = 0.0008653145558439159	bf = 0.9928211124535382	zeta = 0.0009518460114283075	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [4.87525871e-07 3.30793358e-06 4.78090085e-07 1.74835808e-08
 5.05906076e-06 5.48151319e-07 3.45597587e-08 1.18584348e-06
 4.69143123e-07 2.87821399e-07]
ene_total = [1.69277942 0.61005013 1.78228252 1.35847287 0.58525862 0.57579307
 1.35205481 1.14990818 1.25988419 0.55545563]
ti_comp = [0.82931765 0.97107852 0.81760132 0.87307473 0.97434132 0.97553528
 0.87391504 0.90038819 0.88598481 0.97819491]
ti_coms = [0.22158526 0.0798244  0.23330159 0.17782818 0.0765616  0.07536764
 0.17698787 0.15051473 0.1649181  0.07270801]
t_total = [26.89973984 26.89973984 26.89973984 26.89973984 26.89973984 26.89973984
 26.89973984 26.89973984 26.89973984 26.89973984]
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [1.61900639e-06 1.09851926e-05 1.58767144e-06 5.80605679e-08
 1.68004452e-05 1.82033517e-06 1.14768207e-07 3.93802316e-06
 1.55795981e-06 9.55815292e-07]
ene_total = [0.56722855 0.204606   0.59721779 0.45518453 0.19640314 0.19296354
 0.45303505 0.38537017 0.42217726 0.18613361]
optimize_network iter = 2 obj = 3.6603196487199923
eta = 0.7286862650795745
freqs = [15746566.89412155 28283149.87566597 15718689.9489192   5104500.91249741
 32549667.4373985  15511225.99197455  6404139.63058965 20604377.17176981
 15207342.34336578 12502384.68143372]
Done!
ene_coms = [0.02215853 0.00798244 0.02333016 0.01778282 0.00765616 0.00753676
 0.01769879 0.01505147 0.01649181 0.0072708 ]
ene_comp = [1.56211252e-06 1.05991595e-05 1.53187873e-06 5.60202486e-08
 1.62100570e-05 1.75636636e-06 1.10735112e-07 3.79963621e-06
 1.50321120e-06 9.22226774e-07]
ene_total = [0.02216009 0.00799304 0.02333169 0.01778287 0.00767237 0.00753852
 0.0176989  0.01505527 0.01649331 0.00727172]
At round 62 energy consumption: 0.1429977884216537
At round 62 eta: 0.7286862650795745
At round 62 a_n: 6.944760344156705
At round 62 local rounds: 10.364211052331338
At round 62 global rounds: 25.59678869996595
gradient difference: 0.4710055887699127
train() client id: f_00000-0-0 loss: 1.158262  [   32/  126]
train() client id: f_00000-0-1 loss: 1.061126  [   64/  126]
train() client id: f_00000-0-2 loss: 1.199346  [   96/  126]
train() client id: f_00000-1-0 loss: 1.108485  [   32/  126]
train() client id: f_00000-1-1 loss: 1.203111  [   64/  126]
train() client id: f_00000-1-2 loss: 1.017801  [   96/  126]
train() client id: f_00000-2-0 loss: 1.189172  [   32/  126]
train() client id: f_00000-2-1 loss: 0.934983  [   64/  126]
train() client id: f_00000-2-2 loss: 0.927724  [   96/  126]
train() client id: f_00000-3-0 loss: 0.950800  [   32/  126]
train() client id: f_00000-3-1 loss: 0.921882  [   64/  126]
train() client id: f_00000-3-2 loss: 0.994630  [   96/  126]
train() client id: f_00000-4-0 loss: 0.975837  [   32/  126]
train() client id: f_00000-4-1 loss: 0.938723  [   64/  126]
train() client id: f_00000-4-2 loss: 0.936077  [   96/  126]
train() client id: f_00000-5-0 loss: 1.042050  [   32/  126]
train() client id: f_00000-5-1 loss: 0.816584  [   64/  126]
train() client id: f_00000-5-2 loss: 0.888875  [   96/  126]
train() client id: f_00000-6-0 loss: 1.020223  [   32/  126]
train() client id: f_00000-6-1 loss: 0.790429  [   64/  126]
train() client id: f_00000-6-2 loss: 0.909114  [   96/  126]
train() client id: f_00000-7-0 loss: 0.951757  [   32/  126]
train() client id: f_00000-7-1 loss: 0.821904  [   64/  126]
train() client id: f_00000-7-2 loss: 0.975361  [   96/  126]
train() client id: f_00000-8-0 loss: 0.989012  [   32/  126]
train() client id: f_00000-8-1 loss: 0.885968  [   64/  126]
train() client id: f_00000-8-2 loss: 0.787878  [   96/  126]
train() client id: f_00000-9-0 loss: 0.886842  [   32/  126]
train() client id: f_00000-9-1 loss: 1.076717  [   64/  126]
train() client id: f_00000-9-2 loss: 0.791782  [   96/  126]
train() client id: f_00001-0-0 loss: 0.458250  [   32/  265]
train() client id: f_00001-0-1 loss: 0.570510  [   64/  265]
train() client id: f_00001-0-2 loss: 0.518961  [   96/  265]
train() client id: f_00001-0-3 loss: 0.572717  [  128/  265]
train() client id: f_00001-0-4 loss: 0.500095  [  160/  265]
train() client id: f_00001-0-5 loss: 0.476915  [  192/  265]
train() client id: f_00001-0-6 loss: 0.442530  [  224/  265]
train() client id: f_00001-0-7 loss: 0.619523  [  256/  265]
train() client id: f_00001-1-0 loss: 0.559408  [   32/  265]
train() client id: f_00001-1-1 loss: 0.444505  [   64/  265]
train() client id: f_00001-1-2 loss: 0.482261  [   96/  265]
train() client id: f_00001-1-3 loss: 0.522830  [  128/  265]
train() client id: f_00001-1-4 loss: 0.571890  [  160/  265]
train() client id: f_00001-1-5 loss: 0.553534  [  192/  265]
train() client id: f_00001-1-6 loss: 0.436989  [  224/  265]
train() client id: f_00001-1-7 loss: 0.483057  [  256/  265]
train() client id: f_00001-2-0 loss: 0.471940  [   32/  265]
train() client id: f_00001-2-1 loss: 0.411608  [   64/  265]
train() client id: f_00001-2-2 loss: 0.566449  [   96/  265]
train() client id: f_00001-2-3 loss: 0.444566  [  128/  265]
train() client id: f_00001-2-4 loss: 0.660042  [  160/  265]
train() client id: f_00001-2-5 loss: 0.548571  [  192/  265]
train() client id: f_00001-2-6 loss: 0.450075  [  224/  265]
train() client id: f_00001-2-7 loss: 0.537625  [  256/  265]
train() client id: f_00001-3-0 loss: 0.408505  [   32/  265]
train() client id: f_00001-3-1 loss: 0.550845  [   64/  265]
train() client id: f_00001-3-2 loss: 0.491085  [   96/  265]
train() client id: f_00001-3-3 loss: 0.509172  [  128/  265]
train() client id: f_00001-3-4 loss: 0.506228  [  160/  265]
train() client id: f_00001-3-5 loss: 0.677971  [  192/  265]
train() client id: f_00001-3-6 loss: 0.510584  [  224/  265]
train() client id: f_00001-3-7 loss: 0.408030  [  256/  265]
train() client id: f_00001-4-0 loss: 0.576975  [   32/  265]
train() client id: f_00001-4-1 loss: 0.543800  [   64/  265]
train() client id: f_00001-4-2 loss: 0.427507  [   96/  265]
train() client id: f_00001-4-3 loss: 0.454971  [  128/  265]
train() client id: f_00001-4-4 loss: 0.455628  [  160/  265]
train() client id: f_00001-4-5 loss: 0.391917  [  192/  265]
train() client id: f_00001-4-6 loss: 0.501416  [  224/  265]
train() client id: f_00001-4-7 loss: 0.603740  [  256/  265]
train() client id: f_00001-5-0 loss: 0.507136  [   32/  265]
train() client id: f_00001-5-1 loss: 0.517501  [   64/  265]
train() client id: f_00001-5-2 loss: 0.454146  [   96/  265]
train() client id: f_00001-5-3 loss: 0.521013  [  128/  265]
train() client id: f_00001-5-4 loss: 0.411025  [  160/  265]
train() client id: f_00001-5-5 loss: 0.488535  [  192/  265]
train() client id: f_00001-5-6 loss: 0.532972  [  224/  265]
train() client id: f_00001-5-7 loss: 0.519393  [  256/  265]
train() client id: f_00001-6-0 loss: 0.467989  [   32/  265]
train() client id: f_00001-6-1 loss: 0.532925  [   64/  265]
train() client id: f_00001-6-2 loss: 0.499696  [   96/  265]
train() client id: f_00001-6-3 loss: 0.604927  [  128/  265]
train() client id: f_00001-6-4 loss: 0.426833  [  160/  265]
train() client id: f_00001-6-5 loss: 0.522631  [  192/  265]
train() client id: f_00001-6-6 loss: 0.498856  [  224/  265]
train() client id: f_00001-6-7 loss: 0.481211  [  256/  265]
train() client id: f_00001-7-0 loss: 0.469268  [   32/  265]
train() client id: f_00001-7-1 loss: 0.576893  [   64/  265]
train() client id: f_00001-7-2 loss: 0.445177  [   96/  265]
train() client id: f_00001-7-3 loss: 0.522293  [  128/  265]
train() client id: f_00001-7-4 loss: 0.485944  [  160/  265]
train() client id: f_00001-7-5 loss: 0.574183  [  192/  265]
train() client id: f_00001-7-6 loss: 0.436661  [  224/  265]
train() client id: f_00001-7-7 loss: 0.430512  [  256/  265]
train() client id: f_00001-8-0 loss: 0.509751  [   32/  265]
train() client id: f_00001-8-1 loss: 0.573036  [   64/  265]
train() client id: f_00001-8-2 loss: 0.493236  [   96/  265]
train() client id: f_00001-8-3 loss: 0.399838  [  128/  265]
train() client id: f_00001-8-4 loss: 0.501657  [  160/  265]
train() client id: f_00001-8-5 loss: 0.573362  [  192/  265]
train() client id: f_00001-8-6 loss: 0.451166  [  224/  265]
train() client id: f_00001-8-7 loss: 0.544561  [  256/  265]
train() client id: f_00001-9-0 loss: 0.491701  [   32/  265]
train() client id: f_00001-9-1 loss: 0.490814  [   64/  265]
train() client id: f_00001-9-2 loss: 0.527533  [   96/  265]
train() client id: f_00001-9-3 loss: 0.518547  [  128/  265]
train() client id: f_00001-9-4 loss: 0.493073  [  160/  265]
train() client id: f_00001-9-5 loss: 0.463782  [  192/  265]
train() client id: f_00001-9-6 loss: 0.549115  [  224/  265]
train() client id: f_00001-9-7 loss: 0.499845  [  256/  265]
train() client id: f_00002-0-0 loss: 1.277576  [   32/  124]
train() client id: f_00002-0-1 loss: 1.421600  [   64/  124]
train() client id: f_00002-0-2 loss: 0.990477  [   96/  124]
train() client id: f_00002-1-0 loss: 1.150521  [   32/  124]
train() client id: f_00002-1-1 loss: 1.219423  [   64/  124]
train() client id: f_00002-1-2 loss: 1.063459  [   96/  124]
train() client id: f_00002-2-0 loss: 1.126869  [   32/  124]
train() client id: f_00002-2-1 loss: 0.996455  [   64/  124]
train() client id: f_00002-2-2 loss: 1.137531  [   96/  124]
train() client id: f_00002-3-0 loss: 1.200608  [   32/  124]
train() client id: f_00002-3-1 loss: 0.995118  [   64/  124]
train() client id: f_00002-3-2 loss: 1.247900  [   96/  124]
train() client id: f_00002-4-0 loss: 1.086281  [   32/  124]
train() client id: f_00002-4-1 loss: 1.046248  [   64/  124]
train() client id: f_00002-4-2 loss: 1.225961  [   96/  124]
train() client id: f_00002-5-0 loss: 1.025048  [   32/  124]
train() client id: f_00002-5-1 loss: 1.104245  [   64/  124]
train() client id: f_00002-5-2 loss: 1.194659  [   96/  124]
train() client id: f_00002-6-0 loss: 1.141959  [   32/  124]
train() client id: f_00002-6-1 loss: 1.208883  [   64/  124]
train() client id: f_00002-6-2 loss: 0.964433  [   96/  124]
train() client id: f_00002-7-0 loss: 0.885030  [   32/  124]
train() client id: f_00002-7-1 loss: 1.165293  [   64/  124]
train() client id: f_00002-7-2 loss: 1.254548  [   96/  124]
train() client id: f_00002-8-0 loss: 1.106492  [   32/  124]
train() client id: f_00002-8-1 loss: 1.095892  [   64/  124]
train() client id: f_00002-8-2 loss: 0.997413  [   96/  124]
train() client id: f_00002-9-0 loss: 0.927921  [   32/  124]
train() client id: f_00002-9-1 loss: 1.217870  [   64/  124]
train() client id: f_00002-9-2 loss: 0.945956  [   96/  124]
train() client id: f_00003-0-0 loss: 0.834914  [   32/   43]
train() client id: f_00003-1-0 loss: 0.813683  [   32/   43]
train() client id: f_00003-2-0 loss: 0.557564  [   32/   43]
train() client id: f_00003-3-0 loss: 0.739231  [   32/   43]
train() client id: f_00003-4-0 loss: 0.638900  [   32/   43]
train() client id: f_00003-5-0 loss: 0.675275  [   32/   43]
train() client id: f_00003-6-0 loss: 0.654100  [   32/   43]
train() client id: f_00003-7-0 loss: 0.770231  [   32/   43]
train() client id: f_00003-8-0 loss: 0.733418  [   32/   43]
train() client id: f_00003-9-0 loss: 0.547803  [   32/   43]
train() client id: f_00004-0-0 loss: 0.827854  [   32/  306]
train() client id: f_00004-0-1 loss: 0.986485  [   64/  306]
train() client id: f_00004-0-2 loss: 0.911115  [   96/  306]
train() client id: f_00004-0-3 loss: 0.898786  [  128/  306]
train() client id: f_00004-0-4 loss: 0.881822  [  160/  306]
train() client id: f_00004-0-5 loss: 0.924103  [  192/  306]
train() client id: f_00004-0-6 loss: 0.870750  [  224/  306]
train() client id: f_00004-0-7 loss: 0.859893  [  256/  306]
train() client id: f_00004-0-8 loss: 0.986396  [  288/  306]
train() client id: f_00004-1-0 loss: 0.952722  [   32/  306]
train() client id: f_00004-1-1 loss: 0.893151  [   64/  306]
train() client id: f_00004-1-2 loss: 0.963673  [   96/  306]
train() client id: f_00004-1-3 loss: 0.878080  [  128/  306]
train() client id: f_00004-1-4 loss: 0.954513  [  160/  306]
train() client id: f_00004-1-5 loss: 0.915959  [  192/  306]
train() client id: f_00004-1-6 loss: 0.918160  [  224/  306]
train() client id: f_00004-1-7 loss: 0.882282  [  256/  306]
train() client id: f_00004-1-8 loss: 0.847871  [  288/  306]
train() client id: f_00004-2-0 loss: 0.912149  [   32/  306]
train() client id: f_00004-2-1 loss: 0.935269  [   64/  306]
train() client id: f_00004-2-2 loss: 0.875372  [   96/  306]
train() client id: f_00004-2-3 loss: 0.874515  [  128/  306]
train() client id: f_00004-2-4 loss: 0.843451  [  160/  306]
train() client id: f_00004-2-5 loss: 0.960793  [  192/  306]
train() client id: f_00004-2-6 loss: 0.874539  [  224/  306]
train() client id: f_00004-2-7 loss: 0.757976  [  256/  306]
train() client id: f_00004-2-8 loss: 0.915070  [  288/  306]
train() client id: f_00004-3-0 loss: 0.946664  [   32/  306]
train() client id: f_00004-3-1 loss: 0.905304  [   64/  306]
train() client id: f_00004-3-2 loss: 0.937231  [   96/  306]
train() client id: f_00004-3-3 loss: 0.789945  [  128/  306]
train() client id: f_00004-3-4 loss: 0.829867  [  160/  306]
train() client id: f_00004-3-5 loss: 0.836953  [  192/  306]
train() client id: f_00004-3-6 loss: 0.947313  [  224/  306]
train() client id: f_00004-3-7 loss: 0.892149  [  256/  306]
train() client id: f_00004-3-8 loss: 1.011000  [  288/  306]
train() client id: f_00004-4-0 loss: 1.015413  [   32/  306]
train() client id: f_00004-4-1 loss: 0.788575  [   64/  306]
train() client id: f_00004-4-2 loss: 0.997305  [   96/  306]
train() client id: f_00004-4-3 loss: 0.846783  [  128/  306]
train() client id: f_00004-4-4 loss: 0.962969  [  160/  306]
train() client id: f_00004-4-5 loss: 0.881951  [  192/  306]
train() client id: f_00004-4-6 loss: 0.945437  [  224/  306]
train() client id: f_00004-4-7 loss: 0.826510  [  256/  306]
train() client id: f_00004-4-8 loss: 0.832305  [  288/  306]
train() client id: f_00004-5-0 loss: 0.904993  [   32/  306]
train() client id: f_00004-5-1 loss: 0.998731  [   64/  306]
train() client id: f_00004-5-2 loss: 0.780039  [   96/  306]
train() client id: f_00004-5-3 loss: 0.966203  [  128/  306]
train() client id: f_00004-5-4 loss: 0.825328  [  160/  306]
train() client id: f_00004-5-5 loss: 0.951651  [  192/  306]
train() client id: f_00004-5-6 loss: 0.890590  [  224/  306]
train() client id: f_00004-5-7 loss: 0.886812  [  256/  306]
train() client id: f_00004-5-8 loss: 0.833162  [  288/  306]
train() client id: f_00004-6-0 loss: 0.945847  [   32/  306]
train() client id: f_00004-6-1 loss: 0.768074  [   64/  306]
train() client id: f_00004-6-2 loss: 1.040436  [   96/  306]
train() client id: f_00004-6-3 loss: 0.970893  [  128/  306]
train() client id: f_00004-6-4 loss: 0.864204  [  160/  306]
train() client id: f_00004-6-5 loss: 0.921648  [  192/  306]
train() client id: f_00004-6-6 loss: 0.737792  [  224/  306]
train() client id: f_00004-6-7 loss: 1.005212  [  256/  306]
train() client id: f_00004-6-8 loss: 0.843985  [  288/  306]
train() client id: f_00004-7-0 loss: 0.913652  [   32/  306]
train() client id: f_00004-7-1 loss: 0.892883  [   64/  306]
train() client id: f_00004-7-2 loss: 0.862579  [   96/  306]
train() client id: f_00004-7-3 loss: 0.768999  [  128/  306]
train() client id: f_00004-7-4 loss: 0.886223  [  160/  306]
train() client id: f_00004-7-5 loss: 0.835698  [  192/  306]
train() client id: f_00004-7-6 loss: 0.940223  [  224/  306]
train() client id: f_00004-7-7 loss: 0.911297  [  256/  306]
train() client id: f_00004-7-8 loss: 0.870290  [  288/  306]
train() client id: f_00004-8-0 loss: 0.858623  [   32/  306]
train() client id: f_00004-8-1 loss: 0.817572  [   64/  306]
train() client id: f_00004-8-2 loss: 0.797092  [   96/  306]
train() client id: f_00004-8-3 loss: 0.851522  [  128/  306]
train() client id: f_00004-8-4 loss: 0.884043  [  160/  306]
train() client id: f_00004-8-5 loss: 1.090015  [  192/  306]
train() client id: f_00004-8-6 loss: 0.955228  [  224/  306]
train() client id: f_00004-8-7 loss: 0.775417  [  256/  306]
train() client id: f_00004-8-8 loss: 0.982622  [  288/  306]
train() client id: f_00004-9-0 loss: 0.869181  [   32/  306]
train() client id: f_00004-9-1 loss: 1.002462  [   64/  306]
train() client id: f_00004-9-2 loss: 0.965760  [   96/  306]
train() client id: f_00004-9-3 loss: 0.934817  [  128/  306]
train() client id: f_00004-9-4 loss: 0.729680  [  160/  306]
train() client id: f_00004-9-5 loss: 0.843903  [  192/  306]
train() client id: f_00004-9-6 loss: 0.826156  [  224/  306]
train() client id: f_00004-9-7 loss: 0.954116  [  256/  306]
train() client id: f_00004-9-8 loss: 0.854967  [  288/  306]
train() client id: f_00005-0-0 loss: 0.559672  [   32/  146]
train() client id: f_00005-0-1 loss: 0.340876  [   64/  146]
train() client id: f_00005-0-2 loss: 0.390058  [   96/  146]
train() client id: f_00005-0-3 loss: 0.414310  [  128/  146]
train() client id: f_00005-1-0 loss: 0.540533  [   32/  146]
train() client id: f_00005-1-1 loss: 0.286547  [   64/  146]
train() client id: f_00005-1-2 loss: 0.396684  [   96/  146]
train() client id: f_00005-1-3 loss: 0.239988  [  128/  146]
train() client id: f_00005-2-0 loss: 0.392416  [   32/  146]
train() client id: f_00005-2-1 loss: 0.439865  [   64/  146]
train() client id: f_00005-2-2 loss: 0.230503  [   96/  146]
train() client id: f_00005-2-3 loss: 0.563575  [  128/  146]
train() client id: f_00005-3-0 loss: 0.591997  [   32/  146]
train() client id: f_00005-3-1 loss: 0.491592  [   64/  146]
train() client id: f_00005-3-2 loss: 0.284522  [   96/  146]
train() client id: f_00005-3-3 loss: 0.475825  [  128/  146]
train() client id: f_00005-4-0 loss: 0.881229  [   32/  146]
train() client id: f_00005-4-1 loss: 0.042997  [   64/  146]
train() client id: f_00005-4-2 loss: 0.344265  [   96/  146]
train() client id: f_00005-4-3 loss: 0.419895  [  128/  146]
train() client id: f_00005-5-0 loss: 0.631791  [   32/  146]
train() client id: f_00005-5-1 loss: 0.135197  [   64/  146]
train() client id: f_00005-5-2 loss: 0.419842  [   96/  146]
train() client id: f_00005-5-3 loss: 0.422586  [  128/  146]
train() client id: f_00005-6-0 loss: 0.356206  [   32/  146]
train() client id: f_00005-6-1 loss: 0.227984  [   64/  146]
train() client id: f_00005-6-2 loss: 0.574742  [   96/  146]
train() client id: f_00005-6-3 loss: 0.524447  [  128/  146]
train() client id: f_00005-7-0 loss: 0.441077  [   32/  146]
train() client id: f_00005-7-1 loss: 0.264637  [   64/  146]
train() client id: f_00005-7-2 loss: 0.444490  [   96/  146]
train() client id: f_00005-7-3 loss: 0.465919  [  128/  146]
train() client id: f_00005-8-0 loss: 0.230158  [   32/  146]
train() client id: f_00005-8-1 loss: 0.422527  [   64/  146]
train() client id: f_00005-8-2 loss: 0.415383  [   96/  146]
train() client id: f_00005-8-3 loss: 0.594183  [  128/  146]
train() client id: f_00005-9-0 loss: 0.618771  [   32/  146]
train() client id: f_00005-9-1 loss: 0.331880  [   64/  146]
train() client id: f_00005-9-2 loss: 0.372142  [   96/  146]
train() client id: f_00005-9-3 loss: 0.280343  [  128/  146]
train() client id: f_00006-0-0 loss: 0.402430  [   32/   54]
train() client id: f_00006-1-0 loss: 0.505190  [   32/   54]
train() client id: f_00006-2-0 loss: 0.447241  [   32/   54]
train() client id: f_00006-3-0 loss: 0.520050  [   32/   54]
train() client id: f_00006-4-0 loss: 0.503551  [   32/   54]
train() client id: f_00006-5-0 loss: 0.502967  [   32/   54]
train() client id: f_00006-6-0 loss: 0.483038  [   32/   54]
train() client id: f_00006-7-0 loss: 0.417159  [   32/   54]
train() client id: f_00006-8-0 loss: 0.455957  [   32/   54]
train() client id: f_00006-9-0 loss: 0.446864  [   32/   54]
train() client id: f_00007-0-0 loss: 0.565459  [   32/  179]
train() client id: f_00007-0-1 loss: 0.618566  [   64/  179]
train() client id: f_00007-0-2 loss: 0.593437  [   96/  179]
train() client id: f_00007-0-3 loss: 0.393061  [  128/  179]
train() client id: f_00007-0-4 loss: 0.555664  [  160/  179]
train() client id: f_00007-1-0 loss: 0.381140  [   32/  179]
train() client id: f_00007-1-1 loss: 0.481649  [   64/  179]
train() client id: f_00007-1-2 loss: 0.602756  [   96/  179]
train() client id: f_00007-1-3 loss: 0.504441  [  128/  179]
train() client id: f_00007-1-4 loss: 0.649390  [  160/  179]
train() client id: f_00007-2-0 loss: 0.763315  [   32/  179]
train() client id: f_00007-2-1 loss: 0.513474  [   64/  179]
train() client id: f_00007-2-2 loss: 0.581274  [   96/  179]
train() client id: f_00007-2-3 loss: 0.378092  [  128/  179]
train() client id: f_00007-2-4 loss: 0.483116  [  160/  179]
train() client id: f_00007-3-0 loss: 0.503149  [   32/  179]
train() client id: f_00007-3-1 loss: 0.383634  [   64/  179]
train() client id: f_00007-3-2 loss: 0.420057  [   96/  179]
train() client id: f_00007-3-3 loss: 0.445025  [  128/  179]
train() client id: f_00007-3-4 loss: 0.803586  [  160/  179]
train() client id: f_00007-4-0 loss: 0.532302  [   32/  179]
train() client id: f_00007-4-1 loss: 0.514785  [   64/  179]
train() client id: f_00007-4-2 loss: 0.381926  [   96/  179]
train() client id: f_00007-4-3 loss: 0.516328  [  128/  179]
train() client id: f_00007-4-4 loss: 0.455927  [  160/  179]
train() client id: f_00007-5-0 loss: 0.640958  [   32/  179]
train() client id: f_00007-5-1 loss: 0.604094  [   64/  179]
train() client id: f_00007-5-2 loss: 0.347109  [   96/  179]
train() client id: f_00007-5-3 loss: 0.479045  [  128/  179]
train() client id: f_00007-5-4 loss: 0.387444  [  160/  179]
train() client id: f_00007-6-0 loss: 0.658823  [   32/  179]
train() client id: f_00007-6-1 loss: 0.486614  [   64/  179]
train() client id: f_00007-6-2 loss: 0.335869  [   96/  179]
train() client id: f_00007-6-3 loss: 0.556180  [  128/  179]
train() client id: f_00007-6-4 loss: 0.618258  [  160/  179]
train() client id: f_00007-7-0 loss: 0.566943  [   32/  179]
train() client id: f_00007-7-1 loss: 0.390236  [   64/  179]
train() client id: f_00007-7-2 loss: 0.697073  [   96/  179]
train() client id: f_00007-7-3 loss: 0.458004  [  128/  179]
train() client id: f_00007-7-4 loss: 0.495055  [  160/  179]
train() client id: f_00007-8-0 loss: 0.599591  [   32/  179]
train() client id: f_00007-8-1 loss: 0.449510  [   64/  179]
train() client id: f_00007-8-2 loss: 0.435135  [   96/  179]
train() client id: f_00007-8-3 loss: 0.376034  [  128/  179]
train() client id: f_00007-8-4 loss: 0.525951  [  160/  179]
train() client id: f_00007-9-0 loss: 0.352900  [   32/  179]
train() client id: f_00007-9-1 loss: 0.467705  [   64/  179]
train() client id: f_00007-9-2 loss: 0.440286  [   96/  179]
train() client id: f_00007-9-3 loss: 0.732685  [  128/  179]
train() client id: f_00007-9-4 loss: 0.506532  [  160/  179]
train() client id: f_00008-0-0 loss: 0.675559  [   32/  130]
train() client id: f_00008-0-1 loss: 0.622382  [   64/  130]
train() client id: f_00008-0-2 loss: 0.691578  [   96/  130]
train() client id: f_00008-0-3 loss: 0.824638  [  128/  130]
train() client id: f_00008-1-0 loss: 0.797783  [   32/  130]
train() client id: f_00008-1-1 loss: 0.695774  [   64/  130]
train() client id: f_00008-1-2 loss: 0.648567  [   96/  130]
train() client id: f_00008-1-3 loss: 0.658445  [  128/  130]
train() client id: f_00008-2-0 loss: 0.662356  [   32/  130]
train() client id: f_00008-2-1 loss: 0.733373  [   64/  130]
train() client id: f_00008-2-2 loss: 0.667521  [   96/  130]
train() client id: f_00008-2-3 loss: 0.751767  [  128/  130]
train() client id: f_00008-3-0 loss: 0.727582  [   32/  130]
train() client id: f_00008-3-1 loss: 0.664320  [   64/  130]
train() client id: f_00008-3-2 loss: 0.699059  [   96/  130]
train() client id: f_00008-3-3 loss: 0.739451  [  128/  130]
train() client id: f_00008-4-0 loss: 0.737464  [   32/  130]
train() client id: f_00008-4-1 loss: 0.669474  [   64/  130]
train() client id: f_00008-4-2 loss: 0.766711  [   96/  130]
train() client id: f_00008-4-3 loss: 0.628233  [  128/  130]
train() client id: f_00008-5-0 loss: 0.695364  [   32/  130]
train() client id: f_00008-5-1 loss: 0.765979  [   64/  130]
train() client id: f_00008-5-2 loss: 0.697418  [   96/  130]
train() client id: f_00008-5-3 loss: 0.662473  [  128/  130]
train() client id: f_00008-6-0 loss: 0.713077  [   32/  130]
train() client id: f_00008-6-1 loss: 0.821561  [   64/  130]
train() client id: f_00008-6-2 loss: 0.648841  [   96/  130]
train() client id: f_00008-6-3 loss: 0.612760  [  128/  130]
train() client id: f_00008-7-0 loss: 0.734080  [   32/  130]
train() client id: f_00008-7-1 loss: 0.652931  [   64/  130]
train() client id: f_00008-7-2 loss: 0.738256  [   96/  130]
train() client id: f_00008-7-3 loss: 0.678932  [  128/  130]
train() client id: f_00008-8-0 loss: 0.830459  [   32/  130]
train() client id: f_00008-8-1 loss: 0.638078  [   64/  130]
train() client id: f_00008-8-2 loss: 0.646891  [   96/  130]
train() client id: f_00008-8-3 loss: 0.713001  [  128/  130]
train() client id: f_00008-9-0 loss: 0.655431  [   32/  130]
train() client id: f_00008-9-1 loss: 0.784399  [   64/  130]
train() client id: f_00008-9-2 loss: 0.696698  [   96/  130]
train() client id: f_00008-9-3 loss: 0.664785  [  128/  130]
train() client id: f_00009-0-0 loss: 0.823149  [   32/  118]
train() client id: f_00009-0-1 loss: 0.941856  [   64/  118]
train() client id: f_00009-0-2 loss: 0.849868  [   96/  118]
train() client id: f_00009-1-0 loss: 1.066139  [   32/  118]
train() client id: f_00009-1-1 loss: 0.870045  [   64/  118]
train() client id: f_00009-1-2 loss: 0.675794  [   96/  118]
train() client id: f_00009-2-0 loss: 0.784622  [   32/  118]
train() client id: f_00009-2-1 loss: 0.851789  [   64/  118]
train() client id: f_00009-2-2 loss: 0.804720  [   96/  118]
train() client id: f_00009-3-0 loss: 0.707561  [   32/  118]
train() client id: f_00009-3-1 loss: 0.970417  [   64/  118]
train() client id: f_00009-3-2 loss: 0.786316  [   96/  118]
train() client id: f_00009-4-0 loss: 0.825525  [   32/  118]
train() client id: f_00009-4-1 loss: 0.797449  [   64/  118]
train() client id: f_00009-4-2 loss: 0.686767  [   96/  118]
train() client id: f_00009-5-0 loss: 0.769786  [   32/  118]
train() client id: f_00009-5-1 loss: 0.740165  [   64/  118]
train() client id: f_00009-5-2 loss: 0.721128  [   96/  118]
train() client id: f_00009-6-0 loss: 0.754313  [   32/  118]
train() client id: f_00009-6-1 loss: 0.786079  [   64/  118]
train() client id: f_00009-6-2 loss: 0.693553  [   96/  118]
train() client id: f_00009-7-0 loss: 0.609077  [   32/  118]
train() client id: f_00009-7-1 loss: 0.655108  [   64/  118]
train() client id: f_00009-7-2 loss: 0.819949  [   96/  118]
train() client id: f_00009-8-0 loss: 0.657169  [   32/  118]
train() client id: f_00009-8-1 loss: 0.648615  [   64/  118]
train() client id: f_00009-8-2 loss: 0.799282  [   96/  118]
train() client id: f_00009-9-0 loss: 0.608100  [   32/  118]
train() client id: f_00009-9-1 loss: 0.869017  [   64/  118]
train() client id: f_00009-9-2 loss: 0.655089  [   96/  118]
At round 62 accuracy: 0.6472148541114059
At round 62 training accuracy: 0.5935613682092555
At round 62 training loss: 0.8301934782523898
update_location
xs = -3.905658 4.200318 330.009024 18.811294 0.979296 3.956410 -292.443192 -271.324852 314.663977 -257.060879 
ys = 322.587959 305.555839 1.320614 -292.455176 284.350187 267.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 337.754712 321.530735 344.829958 309.651247 301.423270 285.902208 309.079134 289.167515 330.638909 275.855591 
dists_bs = 225.954596 221.715090 534.039927 506.040670 207.186332 201.593311 212.940223 199.038518 514.349422 189.669399 
uav_gains = -118.463820 -117.394140 -118.882851 -116.509803 -115.848564 -114.510637 -116.465072 -114.800128 -118.013928 -113.603678 
bs_gains = -105.479652 -105.249326 -115.939124 -115.284251 -104.425170 -104.092390 -104.758275 -103.937299 -115.482290 -103.350983 
Round 63
-------------------------------
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.68727091 5.38361967 2.65494812 0.98808132 6.20602153 2.98709652
 1.20875503 3.70447299 2.72587058 2.42213493]
obj_prev = 30.968271608412685
eta_min = 2.604478585360512e-35	eta_max = 0.9403513878285256
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 7.091465885945381	eta = 0.9090909090909091
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 17.42929397319813	eta = 0.36988229006606876
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 11.714987188776163	eta = 0.5503025368408222
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 10.719694765497792	eta = 0.601396523881516
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 10.657792616593055	eta = 0.6048895302207599
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 10.657519519314839	eta = 0.6049050304207853
af = 6.446787169041255	bf = 0.9700368260492027	zeta = 10.657519513957864	eta = 0.6049050307248393
eta = 0.6049050307248393
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [0.04148026 0.08724024 0.04082185 0.01415596 0.10073778 0.04806443
 0.01777726 0.05892831 0.0427971  0.0388466 ]
ene_total = [1.07662235 1.59233833 1.08542302 0.54531731 1.81230098 0.92997575
 0.60440513 1.2459445  0.99375704 0.77143509]
ti_comp = [1.37728495 1.52626582 1.36543421 1.42198973 1.52960948 1.53088406
 1.42284286 1.45043831 1.44004221 1.53358086]
ti_coms = [0.22947828 0.08049741 0.24132901 0.1847735  0.07715375 0.07587917
 0.18392037 0.15632492 0.16672102 0.07318237]
t_total = [26.84973564 26.84973564 26.84973564 26.84973564 26.84973564 26.84973564
 26.84973564 26.84973564 26.84973564 26.84973564]
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [2.35156460e-06 1.78143569e-05 2.28042807e-06 8.76809665e-08
 2.73083858e-05 2.96119390e-06 1.73444402e-07 6.07929826e-06
 2.36250370e-06 1.55784967e-06]
ene_total = [0.38350781 0.13481247 0.40330974 0.30876628 0.12938372 0.12684698
 0.3073421  0.26132755 0.2786378  0.12231706]
optimize_network iter = 0 obj = 2.4562515021459133
eta = 0.6049050307248393
freqs = [15058707.65852771 28579633.50514324 14948302.02718542  4977519.32111269
 32929248.53672925 15698259.84497274  6247090.26876788 20313966.31628257
 14859667.46433555 12665323.49976712]
eta_min = 0.6049050307248401	eta_max = 0.735169781083726
af = 0.0008271317221307269	bf = 0.9700368260492027	zeta = 0.0009098448943437997	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [4.45862869e-07 3.37764920e-06 4.32375195e-07 1.66245432e-08
 5.17774221e-06 5.61450199e-07 3.28855174e-08 1.15265103e-06
 4.47936952e-07 2.95372418e-07]
ene_total = [1.66660366 0.58485258 1.75266791 1.34190688 0.5607002  0.55110835
 1.3357123  1.1353834  1.21083327 0.53150371]
ti_comp = [0.84752723 0.9965081  0.83567649 0.89223201 0.99985176 1.00112634
 0.89308514 0.92068059 0.91028449 1.00382314]
ti_coms = [0.22947828 0.08049741 0.24132901 0.1847735  0.07715375 0.07587917
 0.18392037 0.15632492 0.16672102 0.07318237]
t_total = [26.84973564 26.84973564 26.84973564 26.84973564 26.84973564 26.84973564
 26.84973564 26.84973564 26.84973564 26.84973564]
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [1.42363747e-06 9.58012233e-06 1.39567334e-06 5.10557738e-08
 1.46517044e-05 1.58736166e-06 1.00923117e-07 3.45888573e-06
 1.35541058e-06 8.33540515e-07]
ene_total = [0.57212459 0.20091879 0.60166777 0.4606414  0.19270948 0.18920625
 0.4585158  0.38980408 0.41566912 0.18246434]
optimize_network iter = 1 obj = 3.6637216068534983
eta = 0.735169781083726
freqs = [14977014.44528698 26790042.71364152 14948302.02718542  4855109.38680831
 30831466.09028835 14691709.46586062  6091289.81349057 19586300.25463695
 14387143.5153923  11842221.20609098]
eta_min = 0.7351697810837274	eta_max = 0.735169781083714
af = 0.0007390421746511915	bf = 0.9700368260492027	zeta = 0.0008129463921163107	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [4.41038396e-07 2.96789166e-06 4.32375195e-07 1.58169176e-08
 4.53905178e-06 4.91759633e-07 3.12656630e-08 1.07155188e-06
 4.19901921e-07 2.58228221e-07]
ene_total = [1.66660331 0.58482282 1.75266791 1.34190682 0.56065381 0.55110329
 1.33571218 1.13537751 1.21083123 0.53150101]
ti_comp = [0.84752723 0.9965081  0.83567649 0.89223201 0.99985176 1.00112634
 0.89308514 0.92068059 0.91028449 1.00382314]
ti_coms = [0.22947828 0.08049741 0.24132901 0.1847735  0.07715375 0.07587917
 0.18392037 0.15632492 0.16672102 0.07318237]
t_total = [26.84973564 26.84973564 26.84973564 26.84973564 26.84973564 26.84973564
 26.84973564 26.84973564 26.84973564 26.84973564]
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [1.42363747e-06 9.58012233e-06 1.39567334e-06 5.10557738e-08
 1.46517044e-05 1.58736166e-06 1.00923117e-07 3.45888573e-06
 1.35541058e-06 8.33540515e-07]
ene_total = [0.57212459 0.20091879 0.60166777 0.4606414  0.19270948 0.18920625
 0.4585158  0.38980408 0.41566912 0.18246434]
optimize_network iter = 2 obj = 3.6637216068533323
eta = 0.735169781083714
freqs = [14977014.44528692 26790042.71364162 14948302.02718534  4855109.38680831
 30831466.09028849 14691709.46586069  6091289.81349056 19586300.25463695
 14387143.51539229 11842221.20609103]
Done!
ene_coms = [0.02294783 0.00804974 0.0241329  0.01847735 0.00771537 0.00758792
 0.01839204 0.01563249 0.0166721  0.00731824]
ene_comp = [1.41315906e-06 9.50960965e-06 1.38540075e-06 5.06799874e-08
 1.45438633e-05 1.57567819e-06 1.00180291e-07 3.43342726e-06
 1.34543434e-06 8.27405398e-07]
ene_total = [0.02294924 0.00805925 0.02413429 0.0184774  0.00772992 0.00758949
 0.01839214 0.01563593 0.01667345 0.00731906]
At round 63 energy consumption: 0.14696016345683638
At round 63 eta: 0.735169781083714
At round 63 a_n: 6.602214497187386
At round 63 local rounds: 10.074148866251232
At round 63 global rounds: 24.929989199134315
gradient difference: 0.5677963495254517
train() client id: f_00000-0-0 loss: 1.211240  [   32/  126]
train() client id: f_00000-0-1 loss: 1.019363  [   64/  126]
train() client id: f_00000-0-2 loss: 1.151977  [   96/  126]
train() client id: f_00000-1-0 loss: 1.142964  [   32/  126]
train() client id: f_00000-1-1 loss: 1.042819  [   64/  126]
train() client id: f_00000-1-2 loss: 1.084570  [   96/  126]
train() client id: f_00000-2-0 loss: 1.139719  [   32/  126]
train() client id: f_00000-2-1 loss: 0.876096  [   64/  126]
train() client id: f_00000-2-2 loss: 1.110183  [   96/  126]
train() client id: f_00000-3-0 loss: 1.035701  [   32/  126]
train() client id: f_00000-3-1 loss: 0.907582  [   64/  126]
train() client id: f_00000-3-2 loss: 0.933145  [   96/  126]
train() client id: f_00000-4-0 loss: 1.004639  [   32/  126]
train() client id: f_00000-4-1 loss: 1.009194  [   64/  126]
train() client id: f_00000-4-2 loss: 0.939256  [   96/  126]
train() client id: f_00000-5-0 loss: 0.998010  [   32/  126]
train() client id: f_00000-5-1 loss: 1.009747  [   64/  126]
train() client id: f_00000-5-2 loss: 0.963829  [   96/  126]
train() client id: f_00000-6-0 loss: 0.941075  [   32/  126]
train() client id: f_00000-6-1 loss: 1.027905  [   64/  126]
train() client id: f_00000-6-2 loss: 1.016888  [   96/  126]
train() client id: f_00000-7-0 loss: 0.877558  [   32/  126]
train() client id: f_00000-7-1 loss: 1.035942  [   64/  126]
train() client id: f_00000-7-2 loss: 0.911253  [   96/  126]
train() client id: f_00000-8-0 loss: 0.955543  [   32/  126]
train() client id: f_00000-8-1 loss: 0.919519  [   64/  126]
train() client id: f_00000-8-2 loss: 0.871312  [   96/  126]
train() client id: f_00000-9-0 loss: 0.974303  [   32/  126]
train() client id: f_00000-9-1 loss: 0.930389  [   64/  126]
train() client id: f_00000-9-2 loss: 0.900768  [   96/  126]
train() client id: f_00001-0-0 loss: 0.519926  [   32/  265]
train() client id: f_00001-0-1 loss: 0.367723  [   64/  265]
train() client id: f_00001-0-2 loss: 0.499458  [   96/  265]
train() client id: f_00001-0-3 loss: 0.490563  [  128/  265]
train() client id: f_00001-0-4 loss: 0.397244  [  160/  265]
train() client id: f_00001-0-5 loss: 0.465241  [  192/  265]
train() client id: f_00001-0-6 loss: 0.533039  [  224/  265]
train() client id: f_00001-0-7 loss: 0.431469  [  256/  265]
train() client id: f_00001-1-0 loss: 0.519477  [   32/  265]
train() client id: f_00001-1-1 loss: 0.375760  [   64/  265]
train() client id: f_00001-1-2 loss: 0.388154  [   96/  265]
train() client id: f_00001-1-3 loss: 0.430363  [  128/  265]
train() client id: f_00001-1-4 loss: 0.544754  [  160/  265]
train() client id: f_00001-1-5 loss: 0.347105  [  192/  265]
train() client id: f_00001-1-6 loss: 0.468061  [  224/  265]
train() client id: f_00001-1-7 loss: 0.462998  [  256/  265]
train() client id: f_00001-2-0 loss: 0.480661  [   32/  265]
train() client id: f_00001-2-1 loss: 0.569957  [   64/  265]
train() client id: f_00001-2-2 loss: 0.319254  [   96/  265]
train() client id: f_00001-2-3 loss: 0.509735  [  128/  265]
train() client id: f_00001-2-4 loss: 0.380462  [  160/  265]
train() client id: f_00001-2-5 loss: 0.378367  [  192/  265]
train() client id: f_00001-2-6 loss: 0.447023  [  224/  265]
train() client id: f_00001-2-7 loss: 0.487769  [  256/  265]
train() client id: f_00001-3-0 loss: 0.348187  [   32/  265]
train() client id: f_00001-3-1 loss: 0.605247  [   64/  265]
train() client id: f_00001-3-2 loss: 0.342892  [   96/  265]
train() client id: f_00001-3-3 loss: 0.566568  [  128/  265]
train() client id: f_00001-3-4 loss: 0.427296  [  160/  265]
train() client id: f_00001-3-5 loss: 0.416908  [  192/  265]
train() client id: f_00001-3-6 loss: 0.331288  [  224/  265]
train() client id: f_00001-3-7 loss: 0.507195  [  256/  265]
train() client id: f_00001-4-0 loss: 0.493459  [   32/  265]
train() client id: f_00001-4-1 loss: 0.467661  [   64/  265]
train() client id: f_00001-4-2 loss: 0.459562  [   96/  265]
train() client id: f_00001-4-3 loss: 0.421298  [  128/  265]
train() client id: f_00001-4-4 loss: 0.384102  [  160/  265]
train() client id: f_00001-4-5 loss: 0.449997  [  192/  265]
train() client id: f_00001-4-6 loss: 0.337530  [  224/  265]
train() client id: f_00001-4-7 loss: 0.401386  [  256/  265]
train() client id: f_00001-5-0 loss: 0.431804  [   32/  265]
train() client id: f_00001-5-1 loss: 0.398323  [   64/  265]
train() client id: f_00001-5-2 loss: 0.453263  [   96/  265]
train() client id: f_00001-5-3 loss: 0.404572  [  128/  265]
train() client id: f_00001-5-4 loss: 0.345985  [  160/  265]
train() client id: f_00001-5-5 loss: 0.445086  [  192/  265]
train() client id: f_00001-5-6 loss: 0.432490  [  224/  265]
train() client id: f_00001-5-7 loss: 0.492774  [  256/  265]
train() client id: f_00001-6-0 loss: 0.454223  [   32/  265]
train() client id: f_00001-6-1 loss: 0.420002  [   64/  265]
train() client id: f_00001-6-2 loss: 0.447888  [   96/  265]
train() client id: f_00001-6-3 loss: 0.490524  [  128/  265]
train() client id: f_00001-6-4 loss: 0.424256  [  160/  265]
train() client id: f_00001-6-5 loss: 0.503066  [  192/  265]
train() client id: f_00001-6-6 loss: 0.347113  [  224/  265]
train() client id: f_00001-6-7 loss: 0.347388  [  256/  265]
train() client id: f_00001-7-0 loss: 0.414279  [   32/  265]
train() client id: f_00001-7-1 loss: 0.400283  [   64/  265]
train() client id: f_00001-7-2 loss: 0.462982  [   96/  265]
train() client id: f_00001-7-3 loss: 0.338450  [  128/  265]
train() client id: f_00001-7-4 loss: 0.497565  [  160/  265]
train() client id: f_00001-7-5 loss: 0.471838  [  192/  265]
train() client id: f_00001-7-6 loss: 0.508618  [  224/  265]
train() client id: f_00001-7-7 loss: 0.364052  [  256/  265]
train() client id: f_00001-8-0 loss: 0.376582  [   32/  265]
train() client id: f_00001-8-1 loss: 0.540650  [   64/  265]
train() client id: f_00001-8-2 loss: 0.405914  [   96/  265]
train() client id: f_00001-8-3 loss: 0.404596  [  128/  265]
train() client id: f_00001-8-4 loss: 0.383671  [  160/  265]
train() client id: f_00001-8-5 loss: 0.422595  [  192/  265]
train() client id: f_00001-8-6 loss: 0.428627  [  224/  265]
train() client id: f_00001-8-7 loss: 0.501387  [  256/  265]
train() client id: f_00001-9-0 loss: 0.462831  [   32/  265]
train() client id: f_00001-9-1 loss: 0.375353  [   64/  265]
train() client id: f_00001-9-2 loss: 0.468020  [   96/  265]
train() client id: f_00001-9-3 loss: 0.450791  [  128/  265]
train() client id: f_00001-9-4 loss: 0.475945  [  160/  265]
train() client id: f_00001-9-5 loss: 0.336737  [  192/  265]
train() client id: f_00001-9-6 loss: 0.469428  [  224/  265]
train() client id: f_00001-9-7 loss: 0.417503  [  256/  265]
train() client id: f_00002-0-0 loss: 1.193563  [   32/  124]
train() client id: f_00002-0-1 loss: 1.192126  [   64/  124]
train() client id: f_00002-0-2 loss: 1.240741  [   96/  124]
train() client id: f_00002-1-0 loss: 1.189368  [   32/  124]
train() client id: f_00002-1-1 loss: 1.042526  [   64/  124]
train() client id: f_00002-1-2 loss: 1.112476  [   96/  124]
train() client id: f_00002-2-0 loss: 1.148481  [   32/  124]
train() client id: f_00002-2-1 loss: 1.075899  [   64/  124]
train() client id: f_00002-2-2 loss: 1.018522  [   96/  124]
train() client id: f_00002-3-0 loss: 1.038347  [   32/  124]
train() client id: f_00002-3-1 loss: 1.216362  [   64/  124]
train() client id: f_00002-3-2 loss: 1.095615  [   96/  124]
train() client id: f_00002-4-0 loss: 1.147638  [   32/  124]
train() client id: f_00002-4-1 loss: 1.033071  [   64/  124]
train() client id: f_00002-4-2 loss: 1.038453  [   96/  124]
train() client id: f_00002-5-0 loss: 1.030373  [   32/  124]
train() client id: f_00002-5-1 loss: 1.251769  [   64/  124]
train() client id: f_00002-5-2 loss: 1.002818  [   96/  124]
train() client id: f_00002-6-0 loss: 1.076946  [   32/  124]
train() client id: f_00002-6-1 loss: 1.128587  [   64/  124]
train() client id: f_00002-6-2 loss: 0.948139  [   96/  124]
train() client id: f_00002-7-0 loss: 0.869006  [   32/  124]
train() client id: f_00002-7-1 loss: 0.986097  [   64/  124]
train() client id: f_00002-7-2 loss: 1.239285  [   96/  124]
train() client id: f_00002-8-0 loss: 1.005583  [   32/  124]
train() client id: f_00002-8-1 loss: 1.158245  [   64/  124]
train() client id: f_00002-8-2 loss: 1.120924  [   96/  124]
train() client id: f_00002-9-0 loss: 0.953552  [   32/  124]
train() client id: f_00002-9-1 loss: 1.232378  [   64/  124]
train() client id: f_00002-9-2 loss: 0.926409  [   96/  124]
train() client id: f_00003-0-0 loss: 0.809288  [   32/   43]
train() client id: f_00003-1-0 loss: 0.762955  [   32/   43]
train() client id: f_00003-2-0 loss: 0.803439  [   32/   43]
train() client id: f_00003-3-0 loss: 0.729159  [   32/   43]
train() client id: f_00003-4-0 loss: 0.792831  [   32/   43]
train() client id: f_00003-5-0 loss: 0.721106  [   32/   43]
train() client id: f_00003-6-0 loss: 0.517752  [   32/   43]
train() client id: f_00003-7-0 loss: 0.488729  [   32/   43]
train() client id: f_00003-8-0 loss: 0.641467  [   32/   43]
train() client id: f_00003-9-0 loss: 0.696237  [   32/   43]
train() client id: f_00004-0-0 loss: 0.980939  [   32/  306]
train() client id: f_00004-0-1 loss: 0.752674  [   64/  306]
train() client id: f_00004-0-2 loss: 0.872374  [   96/  306]
train() client id: f_00004-0-3 loss: 0.869244  [  128/  306]
train() client id: f_00004-0-4 loss: 0.779645  [  160/  306]
train() client id: f_00004-0-5 loss: 0.675645  [  192/  306]
train() client id: f_00004-0-6 loss: 0.836919  [  224/  306]
train() client id: f_00004-0-7 loss: 0.883717  [  256/  306]
train() client id: f_00004-0-8 loss: 0.896170  [  288/  306]
train() client id: f_00004-1-0 loss: 0.693568  [   32/  306]
train() client id: f_00004-1-1 loss: 0.921161  [   64/  306]
train() client id: f_00004-1-2 loss: 0.904763  [   96/  306]
train() client id: f_00004-1-3 loss: 0.860576  [  128/  306]
train() client id: f_00004-1-4 loss: 0.731472  [  160/  306]
train() client id: f_00004-1-5 loss: 0.922672  [  192/  306]
train() client id: f_00004-1-6 loss: 0.901041  [  224/  306]
train() client id: f_00004-1-7 loss: 0.731648  [  256/  306]
train() client id: f_00004-1-8 loss: 0.912318  [  288/  306]
train() client id: f_00004-2-0 loss: 0.812482  [   32/  306]
train() client id: f_00004-2-1 loss: 0.862033  [   64/  306]
train() client id: f_00004-2-2 loss: 0.840734  [   96/  306]
train() client id: f_00004-2-3 loss: 1.008182  [  128/  306]
train() client id: f_00004-2-4 loss: 0.989919  [  160/  306]
train() client id: f_00004-2-5 loss: 0.912736  [  192/  306]
train() client id: f_00004-2-6 loss: 0.776291  [  224/  306]
train() client id: f_00004-2-7 loss: 0.684424  [  256/  306]
train() client id: f_00004-2-8 loss: 0.717828  [  288/  306]
train() client id: f_00004-3-0 loss: 1.006888  [   32/  306]
train() client id: f_00004-3-1 loss: 0.897138  [   64/  306]
train() client id: f_00004-3-2 loss: 0.775599  [   96/  306]
train() client id: f_00004-3-3 loss: 0.792821  [  128/  306]
train() client id: f_00004-3-4 loss: 0.904949  [  160/  306]
train() client id: f_00004-3-5 loss: 0.759935  [  192/  306]
train() client id: f_00004-3-6 loss: 0.874814  [  224/  306]
train() client id: f_00004-3-7 loss: 0.752207  [  256/  306]
train() client id: f_00004-3-8 loss: 0.860609  [  288/  306]
train() client id: f_00004-4-0 loss: 0.863403  [   32/  306]
train() client id: f_00004-4-1 loss: 0.773739  [   64/  306]
train() client id: f_00004-4-2 loss: 0.729746  [   96/  306]
train() client id: f_00004-4-3 loss: 0.819889  [  128/  306]
train() client id: f_00004-4-4 loss: 0.925459  [  160/  306]
train() client id: f_00004-4-5 loss: 0.821624  [  192/  306]
train() client id: f_00004-4-6 loss: 0.701514  [  224/  306]
train() client id: f_00004-4-7 loss: 0.828163  [  256/  306]
train() client id: f_00004-4-8 loss: 0.989478  [  288/  306]
train() client id: f_00004-5-0 loss: 0.802028  [   32/  306]
train() client id: f_00004-5-1 loss: 0.932630  [   64/  306]
train() client id: f_00004-5-2 loss: 0.811570  [   96/  306]
train() client id: f_00004-5-3 loss: 0.844349  [  128/  306]
train() client id: f_00004-5-4 loss: 0.948955  [  160/  306]
train() client id: f_00004-5-5 loss: 0.785105  [  192/  306]
train() client id: f_00004-5-6 loss: 0.868980  [  224/  306]
train() client id: f_00004-5-7 loss: 0.803443  [  256/  306]
train() client id: f_00004-5-8 loss: 0.853496  [  288/  306]
train() client id: f_00004-6-0 loss: 0.903148  [   32/  306]
train() client id: f_00004-6-1 loss: 0.756455  [   64/  306]
train() client id: f_00004-6-2 loss: 0.884954  [   96/  306]
train() client id: f_00004-6-3 loss: 0.918028  [  128/  306]
train() client id: f_00004-6-4 loss: 0.835343  [  160/  306]
train() client id: f_00004-6-5 loss: 0.841192  [  192/  306]
train() client id: f_00004-6-6 loss: 0.867668  [  224/  306]
train() client id: f_00004-6-7 loss: 0.803755  [  256/  306]
train() client id: f_00004-6-8 loss: 0.795051  [  288/  306]
train() client id: f_00004-7-0 loss: 0.895032  [   32/  306]
train() client id: f_00004-7-1 loss: 0.903188  [   64/  306]
train() client id: f_00004-7-2 loss: 0.717681  [   96/  306]
train() client id: f_00004-7-3 loss: 0.860690  [  128/  306]
train() client id: f_00004-7-4 loss: 0.873013  [  160/  306]
train() client id: f_00004-7-5 loss: 0.782172  [  192/  306]
train() client id: f_00004-7-6 loss: 0.753183  [  224/  306]
train() client id: f_00004-7-7 loss: 0.746335  [  256/  306]
train() client id: f_00004-7-8 loss: 1.030322  [  288/  306]
train() client id: f_00004-8-0 loss: 0.842031  [   32/  306]
train() client id: f_00004-8-1 loss: 0.861305  [   64/  306]
train() client id: f_00004-8-2 loss: 0.785604  [   96/  306]
train() client id: f_00004-8-3 loss: 0.778401  [  128/  306]
train() client id: f_00004-8-4 loss: 0.829532  [  160/  306]
train() client id: f_00004-8-5 loss: 0.983103  [  192/  306]
train() client id: f_00004-8-6 loss: 0.864303  [  224/  306]
train() client id: f_00004-8-7 loss: 0.747330  [  256/  306]
train() client id: f_00004-8-8 loss: 0.909499  [  288/  306]
train() client id: f_00004-9-0 loss: 0.827202  [   32/  306]
train() client id: f_00004-9-1 loss: 0.812369  [   64/  306]
train() client id: f_00004-9-2 loss: 0.830387  [   96/  306]
train() client id: f_00004-9-3 loss: 0.890007  [  128/  306]
train() client id: f_00004-9-4 loss: 0.918427  [  160/  306]
train() client id: f_00004-9-5 loss: 0.775758  [  192/  306]
train() client id: f_00004-9-6 loss: 0.905612  [  224/  306]
train() client id: f_00004-9-7 loss: 0.839185  [  256/  306]
train() client id: f_00004-9-8 loss: 0.918061  [  288/  306]
train() client id: f_00005-0-0 loss: 0.753702  [   32/  146]
train() client id: f_00005-0-1 loss: 0.549987  [   64/  146]
train() client id: f_00005-0-2 loss: 0.788993  [   96/  146]
train() client id: f_00005-0-3 loss: 0.660999  [  128/  146]
train() client id: f_00005-1-0 loss: 0.680634  [   32/  146]
train() client id: f_00005-1-1 loss: 0.910028  [   64/  146]
train() client id: f_00005-1-2 loss: 0.484195  [   96/  146]
train() client id: f_00005-1-3 loss: 0.578933  [  128/  146]
train() client id: f_00005-2-0 loss: 0.720375  [   32/  146]
train() client id: f_00005-2-1 loss: 0.546122  [   64/  146]
train() client id: f_00005-2-2 loss: 0.953264  [   96/  146]
train() client id: f_00005-2-3 loss: 0.815300  [  128/  146]
train() client id: f_00005-3-0 loss: 0.756310  [   32/  146]
train() client id: f_00005-3-1 loss: 0.459338  [   64/  146]
train() client id: f_00005-3-2 loss: 0.908774  [   96/  146]
train() client id: f_00005-3-3 loss: 0.907547  [  128/  146]
train() client id: f_00005-4-0 loss: 0.775420  [   32/  146]
train() client id: f_00005-4-1 loss: 0.597093  [   64/  146]
train() client id: f_00005-4-2 loss: 0.804687  [   96/  146]
train() client id: f_00005-4-3 loss: 0.650763  [  128/  146]
train() client id: f_00005-5-0 loss: 0.849118  [   32/  146]
train() client id: f_00005-5-1 loss: 0.623744  [   64/  146]
train() client id: f_00005-5-2 loss: 0.746244  [   96/  146]
train() client id: f_00005-5-3 loss: 0.603752  [  128/  146]
train() client id: f_00005-6-0 loss: 0.690391  [   32/  146]
train() client id: f_00005-6-1 loss: 0.763055  [   64/  146]
train() client id: f_00005-6-2 loss: 0.752348  [   96/  146]
train() client id: f_00005-6-3 loss: 0.661858  [  128/  146]
train() client id: f_00005-7-0 loss: 0.424967  [   32/  146]
train() client id: f_00005-7-1 loss: 0.938280  [   64/  146]
train() client id: f_00005-7-2 loss: 0.680655  [   96/  146]
train() client id: f_00005-7-3 loss: 0.902095  [  128/  146]
train() client id: f_00005-8-0 loss: 0.746476  [   32/  146]
train() client id: f_00005-8-1 loss: 1.008489  [   64/  146]
train() client id: f_00005-8-2 loss: 0.689778  [   96/  146]
train() client id: f_00005-8-3 loss: 0.454071  [  128/  146]
train() client id: f_00005-9-0 loss: 0.555682  [   32/  146]
train() client id: f_00005-9-1 loss: 0.749068  [   64/  146]
train() client id: f_00005-9-2 loss: 0.796552  [   96/  146]
train() client id: f_00005-9-3 loss: 0.834713  [  128/  146]
train() client id: f_00006-0-0 loss: 0.544029  [   32/   54]
train() client id: f_00006-1-0 loss: 0.474820  [   32/   54]
train() client id: f_00006-2-0 loss: 0.469579  [   32/   54]
train() client id: f_00006-3-0 loss: 0.516039  [   32/   54]
train() client id: f_00006-4-0 loss: 0.536906  [   32/   54]
train() client id: f_00006-5-0 loss: 0.544270  [   32/   54]
train() client id: f_00006-6-0 loss: 0.472549  [   32/   54]
train() client id: f_00006-7-0 loss: 0.473553  [   32/   54]
train() client id: f_00006-8-0 loss: 0.537621  [   32/   54]
train() client id: f_00006-9-0 loss: 0.476860  [   32/   54]
train() client id: f_00007-0-0 loss: 0.465866  [   32/  179]
train() client id: f_00007-0-1 loss: 0.482792  [   64/  179]
train() client id: f_00007-0-2 loss: 0.544618  [   96/  179]
train() client id: f_00007-0-3 loss: 0.560837  [  128/  179]
train() client id: f_00007-0-4 loss: 0.514305  [  160/  179]
train() client id: f_00007-1-0 loss: 0.601797  [   32/  179]
train() client id: f_00007-1-1 loss: 0.508431  [   64/  179]
train() client id: f_00007-1-2 loss: 0.546990  [   96/  179]
train() client id: f_00007-1-3 loss: 0.375647  [  128/  179]
train() client id: f_00007-1-4 loss: 0.690721  [  160/  179]
train() client id: f_00007-2-0 loss: 0.496793  [   32/  179]
train() client id: f_00007-2-1 loss: 0.627238  [   64/  179]
train() client id: f_00007-2-2 loss: 0.407948  [   96/  179]
train() client id: f_00007-2-3 loss: 0.469695  [  128/  179]
train() client id: f_00007-2-4 loss: 0.650425  [  160/  179]
train() client id: f_00007-3-0 loss: 0.635868  [   32/  179]
train() client id: f_00007-3-1 loss: 0.502746  [   64/  179]
train() client id: f_00007-3-2 loss: 0.372769  [   96/  179]
train() client id: f_00007-3-3 loss: 0.618013  [  128/  179]
train() client id: f_00007-3-4 loss: 0.580208  [  160/  179]
train() client id: f_00007-4-0 loss: 0.588167  [   32/  179]
train() client id: f_00007-4-1 loss: 0.527462  [   64/  179]
train() client id: f_00007-4-2 loss: 0.495804  [   96/  179]
train() client id: f_00007-4-3 loss: 0.483481  [  128/  179]
train() client id: f_00007-4-4 loss: 0.519197  [  160/  179]
train() client id: f_00007-5-0 loss: 0.530571  [   32/  179]
train() client id: f_00007-5-1 loss: 0.686329  [   64/  179]
train() client id: f_00007-5-2 loss: 0.350362  [   96/  179]
train() client id: f_00007-5-3 loss: 0.628767  [  128/  179]
train() client id: f_00007-5-4 loss: 0.363295  [  160/  179]
train() client id: f_00007-6-0 loss: 0.575320  [   32/  179]
train() client id: f_00007-6-1 loss: 0.457308  [   64/  179]
train() client id: f_00007-6-2 loss: 0.321005  [   96/  179]
train() client id: f_00007-6-3 loss: 0.561960  [  128/  179]
train() client id: f_00007-6-4 loss: 0.709913  [  160/  179]
train() client id: f_00007-7-0 loss: 0.636580  [   32/  179]
train() client id: f_00007-7-1 loss: 0.585372  [   64/  179]
train() client id: f_00007-7-2 loss: 0.501013  [   96/  179]
train() client id: f_00007-7-3 loss: 0.346379  [  128/  179]
train() client id: f_00007-7-4 loss: 0.365628  [  160/  179]
train() client id: f_00007-8-0 loss: 0.644597  [   32/  179]
train() client id: f_00007-8-1 loss: 0.452749  [   64/  179]
train() client id: f_00007-8-2 loss: 0.402774  [   96/  179]
train() client id: f_00007-8-3 loss: 0.472795  [  128/  179]
train() client id: f_00007-8-4 loss: 0.573635  [  160/  179]
train() client id: f_00007-9-0 loss: 0.613602  [   32/  179]
train() client id: f_00007-9-1 loss: 0.512290  [   64/  179]
train() client id: f_00007-9-2 loss: 0.371795  [   96/  179]
train() client id: f_00007-9-3 loss: 0.446138  [  128/  179]
train() client id: f_00007-9-4 loss: 0.546636  [  160/  179]
train() client id: f_00008-0-0 loss: 0.708373  [   32/  130]
train() client id: f_00008-0-1 loss: 0.750054  [   64/  130]
train() client id: f_00008-0-2 loss: 0.829955  [   96/  130]
train() client id: f_00008-0-3 loss: 0.862484  [  128/  130]
train() client id: f_00008-1-0 loss: 0.808245  [   32/  130]
train() client id: f_00008-1-1 loss: 0.783394  [   64/  130]
train() client id: f_00008-1-2 loss: 0.842191  [   96/  130]
train() client id: f_00008-1-3 loss: 0.684710  [  128/  130]
train() client id: f_00008-2-0 loss: 0.695717  [   32/  130]
train() client id: f_00008-2-1 loss: 0.719474  [   64/  130]
train() client id: f_00008-2-2 loss: 0.898706  [   96/  130]
train() client id: f_00008-2-3 loss: 0.810985  [  128/  130]
train() client id: f_00008-3-0 loss: 0.772528  [   32/  130]
train() client id: f_00008-3-1 loss: 0.758561  [   64/  130]
train() client id: f_00008-3-2 loss: 0.820753  [   96/  130]
train() client id: f_00008-3-3 loss: 0.741913  [  128/  130]
train() client id: f_00008-4-0 loss: 0.731668  [   32/  130]
train() client id: f_00008-4-1 loss: 0.738670  [   64/  130]
train() client id: f_00008-4-2 loss: 0.834432  [   96/  130]
train() client id: f_00008-4-3 loss: 0.801562  [  128/  130]
train() client id: f_00008-5-0 loss: 0.772704  [   32/  130]
train() client id: f_00008-5-1 loss: 0.875784  [   64/  130]
train() client id: f_00008-5-2 loss: 0.782646  [   96/  130]
train() client id: f_00008-5-3 loss: 0.681819  [  128/  130]
train() client id: f_00008-6-0 loss: 0.864797  [   32/  130]
train() client id: f_00008-6-1 loss: 0.815678  [   64/  130]
train() client id: f_00008-6-2 loss: 0.656030  [   96/  130]
train() client id: f_00008-6-3 loss: 0.810699  [  128/  130]
train() client id: f_00008-7-0 loss: 0.726039  [   32/  130]
train() client id: f_00008-7-1 loss: 0.795662  [   64/  130]
train() client id: f_00008-7-2 loss: 0.774660  [   96/  130]
train() client id: f_00008-7-3 loss: 0.840935  [  128/  130]
train() client id: f_00008-8-0 loss: 0.765344  [   32/  130]
train() client id: f_00008-8-1 loss: 0.801897  [   64/  130]
train() client id: f_00008-8-2 loss: 0.738367  [   96/  130]
train() client id: f_00008-8-3 loss: 0.824591  [  128/  130]
train() client id: f_00008-9-0 loss: 0.827710  [   32/  130]
train() client id: f_00008-9-1 loss: 0.759245  [   64/  130]
train() client id: f_00008-9-2 loss: 0.720289  [   96/  130]
train() client id: f_00008-9-3 loss: 0.830736  [  128/  130]
train() client id: f_00009-0-0 loss: 1.075203  [   32/  118]
train() client id: f_00009-0-1 loss: 1.022350  [   64/  118]
train() client id: f_00009-0-2 loss: 0.939087  [   96/  118]
train() client id: f_00009-1-0 loss: 0.864678  [   32/  118]
train() client id: f_00009-1-1 loss: 0.997224  [   64/  118]
train() client id: f_00009-1-2 loss: 0.961631  [   96/  118]
train() client id: f_00009-2-0 loss: 1.051377  [   32/  118]
train() client id: f_00009-2-1 loss: 0.893545  [   64/  118]
train() client id: f_00009-2-2 loss: 0.951029  [   96/  118]
train() client id: f_00009-3-0 loss: 0.927837  [   32/  118]
train() client id: f_00009-3-1 loss: 0.923342  [   64/  118]
train() client id: f_00009-3-2 loss: 0.872360  [   96/  118]
train() client id: f_00009-4-0 loss: 0.862477  [   32/  118]
train() client id: f_00009-4-1 loss: 0.838795  [   64/  118]
train() client id: f_00009-4-2 loss: 0.986688  [   96/  118]
train() client id: f_00009-5-0 loss: 0.832567  [   32/  118]
train() client id: f_00009-5-1 loss: 0.830338  [   64/  118]
train() client id: f_00009-5-2 loss: 0.892314  [   96/  118]
train() client id: f_00009-6-0 loss: 0.766158  [   32/  118]
train() client id: f_00009-6-1 loss: 0.902835  [   64/  118]
train() client id: f_00009-6-2 loss: 0.871569  [   96/  118]
train() client id: f_00009-7-0 loss: 0.758373  [   32/  118]
train() client id: f_00009-7-1 loss: 1.045989  [   64/  118]
train() client id: f_00009-7-2 loss: 0.828103  [   96/  118]
train() client id: f_00009-8-0 loss: 0.920157  [   32/  118]
train() client id: f_00009-8-1 loss: 0.782956  [   64/  118]
train() client id: f_00009-8-2 loss: 0.922132  [   96/  118]
train() client id: f_00009-9-0 loss: 0.905490  [   32/  118]
train() client id: f_00009-9-1 loss: 0.872971  [   64/  118]
train() client id: f_00009-9-2 loss: 0.895732  [   96/  118]
At round 63 accuracy: 0.6472148541114059
At round 63 training accuracy: 0.5922199865861838
At round 63 training loss: 0.8252049658267612
update_location
xs = -3.905658 4.200318 335.009024 18.811294 0.979296 3.956410 -297.443192 -276.324852 319.663977 -262.060879 
ys = 327.587959 310.555839 1.320614 -297.455176 289.350187 272.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 342.533393 326.286027 349.618063 314.377872 306.144557 290.591146 313.814185 293.864084 335.400847 280.520794 
dists_bs = 229.251737 224.695659 538.770576 510.663011 209.868239 203.943631 215.738199 201.505782 519.111710 191.885616 
uav_gains = -118.749823 -117.723996 -119.151344 -116.871916 -116.232646 -114.925202 -116.829435 -115.209763 -118.318240 -114.027110 
bs_gains = -105.655813 -105.411710 -116.046367 -115.394822 -104.581567 -104.233343 -104.917016 -104.087109 -115.594362 -103.492247 
Round 64
-------------------------------
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.55289596 5.10473648 2.52232289 0.94134025 5.8844195  2.83245373
 1.15055847 3.5161317  2.58559463 2.29678183]
obj_prev = 29.387235422694697
eta_min = 3.6403482068472025e-37	eta_max = 0.941453835955468
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 6.723535975581212	eta = 0.909090909090909
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 16.80493378903063	eta = 0.36372088751318643
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 11.200979803524296	eta = 0.5456938178232738
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 10.229583039555175	eta = 0.5975126658351408
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 10.168941452928603	eta = 0.601075879986136
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 10.168671162537121	eta = 0.6010918570034193
af = 6.112305432346556	bf = 0.9451563237723394	zeta = 10.168671157124352	eta = 0.6010918573233796
eta = 0.6010918573233796
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [0.04200208 0.08833771 0.04133538 0.01433404 0.10200505 0.04866908
 0.01800089 0.05966962 0.04333548 0.03933528]
ene_total = [1.03172726 1.51359867 1.04004204 0.52610415 1.72269629 0.88362426
 0.58229031 1.19121994 0.94449752 0.7328707 ]
ti_comp = [1.47039382 1.62666926 1.45842809 1.51593046 1.63009164 1.63144497
 1.51679335 1.54542978 1.53931646 1.63417731]
ti_coms = [0.23746513 0.08118968 0.24943086 0.19192848 0.0777673  0.07641398
 0.19106559 0.16242916 0.16854248 0.07368164]
t_total = [26.79973145 26.79973145 26.79973145 26.79973145 26.79973145 26.79973145
 26.79973145 26.79973145 26.79973145 26.79973145]
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [2.14202768e-06 1.62824548e-05 2.07527568e-06 8.00990907e-08
 2.49643934e-05 2.70703886e-06 1.58456196e-07 5.55957146e-06
 2.14661920e-06 1.42438847e-06]
ene_total = [0.37266401 0.1276584  0.3914396  0.30117553 0.12242423 0.11995133
 0.29982271 0.25497116 0.26451064 0.11564362]
optimize_network iter = 0 obj = 2.3702612355707346
eta = 0.6010918573233796
freqs = [14282595.45433634 27152940.78877741 14171209.70092077  4727803.67422734
 31288134.07679844 14915942.00582812  5933864.17259814 19305186.95139418
 14076208.69920771 12035193.86722137]
eta_min = 0.6010918573233806	eta_max = 0.7423534842131878
af = 0.0007076189579367038	bf = 0.9451563237723394	zeta = 0.0007783808537303743	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [4.01088465e-07 3.04884240e-06 3.88589348e-07 1.49983222e-08
 4.67451020e-06 5.06885170e-07 2.96704627e-08 1.04101362e-06
 4.01948215e-07 2.66712607e-07]
ene_total = [1.63512592 0.5592525  1.71751674 1.3215506  0.53579916 0.52619371
 1.31561006 1.11849968 1.16054979 0.50736327]
ti_comp = [0.86560563 1.02188107 0.8536399  0.91114227 1.02530345 1.02665678
 0.91200517 0.9406416  0.93452827 1.02938912]
ti_coms = [0.23746513 0.08118968 0.24943086 0.19192848 0.0777673  0.07641398
 0.19106559 0.16242916 0.16854248 0.07368164]
t_total = [26.79973145 26.79973145 26.79973145 26.79973145 26.79973145 26.79973145
 26.79973145 26.79973145 26.79973145 26.79973145]
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [1.23942807e-06 8.27345194e-06 1.21469007e-06 4.44613668e-08
 1.26534594e-05 1.37074871e-06 8.78894039e-08 3.00926595e-06
 1.16787135e-06 7.19841221e-07]
ene_total = [0.57696514 0.19745596 0.60603597 0.46630224 0.18924751 0.18568541
 0.46420685 0.394704   0.40951193 0.17903122]
optimize_network iter = 1 obj = 3.6691462360745932
eta = 0.7423534842131878
freqs = [14200721.70636506 25299122.79358197 14171209.70092077  4604073.01677737
 29115814.90637482 13873547.37129865  5776388.64214124 18564736.76689159
 13570968.78990086 11183104.37494726]
eta_min = 0.7423534842131893	eta_max = 0.7423534842131851
af = 0.0006257175094953287	bf = 0.9451563237723394	zeta = 0.0006882892604448616	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [3.96503235e-07 2.64674534e-06 3.88589348e-07 1.42235570e-08
 4.04794575e-06 4.38513790e-07 2.81165434e-08 9.62688933e-07
 3.73611652e-07 2.30283129e-07]
ene_total = [1.63512561 0.55922482 1.71751674 1.32155055 0.53575602 0.526189
 1.31560995 1.11849429 1.16054784 0.50736076]
ti_comp = [0.86560563 1.02188107 0.8536399  0.91114227 1.02530345 1.02665678
 0.91200517 0.9406416  0.93452827 1.02938912]
ti_coms = [0.23746513 0.08118968 0.24943086 0.19192848 0.0777673  0.07641398
 0.19106559 0.16242916 0.16854248 0.07368164]
t_total = [26.79973145 26.79973145 26.79973145 26.79973145 26.79973145 26.79973145
 26.79973145 26.79973145 26.79973145 26.79973145]
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [1.23942807e-06 8.27345194e-06 1.21469007e-06 4.44613668e-08
 1.26534594e-05 1.37074871e-06 8.78894039e-08 3.00926595e-06
 1.16787135e-06 7.19841221e-07]
ene_total = [0.57696514 0.19745596 0.60603597 0.46630224 0.18924751 0.18568541
 0.46420685 0.394704   0.40951193 0.17903122]
optimize_network iter = 2 obj = 3.669146236074556
eta = 0.7423534842131851
freqs = [14200721.70636504 25299122.79358199 14171209.70092075  4604073.01677736
 29115814.90637484 13873547.37129866  5776388.64214123 18564736.76689158
 13570968.78990086 11183104.37494727]
Done!
ene_coms = [0.02374651 0.00811897 0.02494309 0.01919285 0.00777673 0.0076414
 0.01910656 0.01624292 0.01685425 0.00736816]
ene_comp = [1.14341502e-06 7.63254395e-06 1.12059337e-06 4.10171399e-08
 1.16732514e-05 1.26456283e-06 8.10809978e-08 2.77615133e-06
 1.07740148e-06 6.64078283e-07]
ene_total = [0.02374766 0.0081266  0.02494421 0.01919289 0.0077884  0.00764266
 0.01910664 0.01624569 0.01685533 0.00736883]
At round 64 energy consumption: 0.15101890456706119
At round 64 eta: 0.7423534842131851
At round 64 a_n: 6.259668650218071
At round 64 local rounds: 9.755733857653984
At round 64 global rounds: 24.295568799375985
gradient difference: 0.5018818974494934
train() client id: f_00000-0-0 loss: 1.181361  [   32/  126]
train() client id: f_00000-0-1 loss: 1.218268  [   64/  126]
train() client id: f_00000-0-2 loss: 0.992748  [   96/  126]
train() client id: f_00000-1-0 loss: 1.073288  [   32/  126]
train() client id: f_00000-1-1 loss: 1.178554  [   64/  126]
train() client id: f_00000-1-2 loss: 1.019447  [   96/  126]
train() client id: f_00000-2-0 loss: 0.913196  [   32/  126]
train() client id: f_00000-2-1 loss: 1.209184  [   64/  126]
train() client id: f_00000-2-2 loss: 0.898545  [   96/  126]
train() client id: f_00000-3-0 loss: 1.218089  [   32/  126]
train() client id: f_00000-3-1 loss: 0.910247  [   64/  126]
train() client id: f_00000-3-2 loss: 0.941384  [   96/  126]
train() client id: f_00000-4-0 loss: 1.008571  [   32/  126]
train() client id: f_00000-4-1 loss: 0.999459  [   64/  126]
train() client id: f_00000-4-2 loss: 0.901450  [   96/  126]
train() client id: f_00000-5-0 loss: 0.923106  [   32/  126]
train() client id: f_00000-5-1 loss: 0.922662  [   64/  126]
train() client id: f_00000-5-2 loss: 1.008234  [   96/  126]
train() client id: f_00000-6-0 loss: 0.943798  [   32/  126]
train() client id: f_00000-6-1 loss: 0.998876  [   64/  126]
train() client id: f_00000-6-2 loss: 0.972636  [   96/  126]
train() client id: f_00000-7-0 loss: 0.917859  [   32/  126]
train() client id: f_00000-7-1 loss: 0.950268  [   64/  126]
train() client id: f_00000-7-2 loss: 0.813616  [   96/  126]
train() client id: f_00000-8-0 loss: 0.902661  [   32/  126]
train() client id: f_00000-8-1 loss: 0.894821  [   64/  126]
train() client id: f_00000-8-2 loss: 1.044207  [   96/  126]
train() client id: f_00001-0-0 loss: 0.403520  [   32/  265]
train() client id: f_00001-0-1 loss: 0.550957  [   64/  265]
train() client id: f_00001-0-2 loss: 0.377314  [   96/  265]
train() client id: f_00001-0-3 loss: 0.598471  [  128/  265]
train() client id: f_00001-0-4 loss: 0.467224  [  160/  265]
train() client id: f_00001-0-5 loss: 0.395252  [  192/  265]
train() client id: f_00001-0-6 loss: 0.419781  [  224/  265]
train() client id: f_00001-0-7 loss: 0.425479  [  256/  265]
train() client id: f_00001-1-0 loss: 0.334652  [   32/  265]
train() client id: f_00001-1-1 loss: 0.410484  [   64/  265]
train() client id: f_00001-1-2 loss: 0.347520  [   96/  265]
train() client id: f_00001-1-3 loss: 0.506372  [  128/  265]
train() client id: f_00001-1-4 loss: 0.673545  [  160/  265]
train() client id: f_00001-1-5 loss: 0.437390  [  192/  265]
train() client id: f_00001-1-6 loss: 0.363998  [  224/  265]
train() client id: f_00001-1-7 loss: 0.496647  [  256/  265]
train() client id: f_00001-2-0 loss: 0.491604  [   32/  265]
train() client id: f_00001-2-1 loss: 0.553385  [   64/  265]
train() client id: f_00001-2-2 loss: 0.394402  [   96/  265]
train() client id: f_00001-2-3 loss: 0.402679  [  128/  265]
train() client id: f_00001-2-4 loss: 0.423845  [  160/  265]
train() client id: f_00001-2-5 loss: 0.356092  [  192/  265]
train() client id: f_00001-2-6 loss: 0.411905  [  224/  265]
train() client id: f_00001-2-7 loss: 0.530853  [  256/  265]
train() client id: f_00001-3-0 loss: 0.433222  [   32/  265]
train() client id: f_00001-3-1 loss: 0.435700  [   64/  265]
train() client id: f_00001-3-2 loss: 0.369915  [   96/  265]
train() client id: f_00001-3-3 loss: 0.610338  [  128/  265]
train() client id: f_00001-3-4 loss: 0.387840  [  160/  265]
train() client id: f_00001-3-5 loss: 0.373452  [  192/  265]
train() client id: f_00001-3-6 loss: 0.411225  [  224/  265]
train() client id: f_00001-3-7 loss: 0.360933  [  256/  265]
train() client id: f_00001-4-0 loss: 0.478888  [   32/  265]
train() client id: f_00001-4-1 loss: 0.491595  [   64/  265]
train() client id: f_00001-4-2 loss: 0.490653  [   96/  265]
train() client id: f_00001-4-3 loss: 0.452193  [  128/  265]
train() client id: f_00001-4-4 loss: 0.357795  [  160/  265]
train() client id: f_00001-4-5 loss: 0.413993  [  192/  265]
train() client id: f_00001-4-6 loss: 0.386886  [  224/  265]
train() client id: f_00001-4-7 loss: 0.435534  [  256/  265]
train() client id: f_00001-5-0 loss: 0.425906  [   32/  265]
train() client id: f_00001-5-1 loss: 0.461097  [   64/  265]
train() client id: f_00001-5-2 loss: 0.521887  [   96/  265]
train() client id: f_00001-5-3 loss: 0.413125  [  128/  265]
train() client id: f_00001-5-4 loss: 0.336301  [  160/  265]
train() client id: f_00001-5-5 loss: 0.347499  [  192/  265]
train() client id: f_00001-5-6 loss: 0.435706  [  224/  265]
train() client id: f_00001-5-7 loss: 0.550231  [  256/  265]
train() client id: f_00001-6-0 loss: 0.371039  [   32/  265]
train() client id: f_00001-6-1 loss: 0.622584  [   64/  265]
train() client id: f_00001-6-2 loss: 0.397037  [   96/  265]
train() client id: f_00001-6-3 loss: 0.398278  [  128/  265]
train() client id: f_00001-6-4 loss: 0.377062  [  160/  265]
train() client id: f_00001-6-5 loss: 0.419401  [  192/  265]
train() client id: f_00001-6-6 loss: 0.429608  [  224/  265]
train() client id: f_00001-6-7 loss: 0.469893  [  256/  265]
train() client id: f_00001-7-0 loss: 0.347571  [   32/  265]
train() client id: f_00001-7-1 loss: 0.436668  [   64/  265]
train() client id: f_00001-7-2 loss: 0.542340  [   96/  265]
train() client id: f_00001-7-3 loss: 0.394074  [  128/  265]
train() client id: f_00001-7-4 loss: 0.480868  [  160/  265]
train() client id: f_00001-7-5 loss: 0.391666  [  192/  265]
train() client id: f_00001-7-6 loss: 0.412416  [  224/  265]
train() client id: f_00001-7-7 loss: 0.468671  [  256/  265]
train() client id: f_00001-8-0 loss: 0.511795  [   32/  265]
train() client id: f_00001-8-1 loss: 0.487808  [   64/  265]
train() client id: f_00001-8-2 loss: 0.483710  [   96/  265]
train() client id: f_00001-8-3 loss: 0.512770  [  128/  265]
train() client id: f_00001-8-4 loss: 0.345619  [  160/  265]
train() client id: f_00001-8-5 loss: 0.336816  [  192/  265]
train() client id: f_00001-8-6 loss: 0.420933  [  224/  265]
train() client id: f_00001-8-7 loss: 0.348465  [  256/  265]
train() client id: f_00002-0-0 loss: 1.182925  [   32/  124]
train() client id: f_00002-0-1 loss: 1.218940  [   64/  124]
train() client id: f_00002-0-2 loss: 1.222414  [   96/  124]
train() client id: f_00002-1-0 loss: 1.275453  [   32/  124]
train() client id: f_00002-1-1 loss: 1.244636  [   64/  124]
train() client id: f_00002-1-2 loss: 1.055311  [   96/  124]
train() client id: f_00002-2-0 loss: 1.231988  [   32/  124]
train() client id: f_00002-2-1 loss: 1.189602  [   64/  124]
train() client id: f_00002-2-2 loss: 1.211190  [   96/  124]
train() client id: f_00002-3-0 loss: 1.385582  [   32/  124]
train() client id: f_00002-3-1 loss: 1.216718  [   64/  124]
train() client id: f_00002-3-2 loss: 1.110067  [   96/  124]
train() client id: f_00002-4-0 loss: 1.144267  [   32/  124]
train() client id: f_00002-4-1 loss: 1.144073  [   64/  124]
train() client id: f_00002-4-2 loss: 1.286875  [   96/  124]
train() client id: f_00002-5-0 loss: 0.925032  [   32/  124]
train() client id: f_00002-5-1 loss: 0.946153  [   64/  124]
train() client id: f_00002-5-2 loss: 1.451273  [   96/  124]
train() client id: f_00002-6-0 loss: 1.189159  [   32/  124]
train() client id: f_00002-6-1 loss: 1.123597  [   64/  124]
train() client id: f_00002-6-2 loss: 1.150313  [   96/  124]
train() client id: f_00002-7-0 loss: 1.203984  [   32/  124]
train() client id: f_00002-7-1 loss: 0.972605  [   64/  124]
train() client id: f_00002-7-2 loss: 0.961003  [   96/  124]
train() client id: f_00002-8-0 loss: 0.930629  [   32/  124]
train() client id: f_00002-8-1 loss: 1.209731  [   64/  124]
train() client id: f_00002-8-2 loss: 1.252950  [   96/  124]
train() client id: f_00003-0-0 loss: 0.764139  [   32/   43]
train() client id: f_00003-1-0 loss: 0.468683  [   32/   43]
train() client id: f_00003-2-0 loss: 0.804579  [   32/   43]
train() client id: f_00003-3-0 loss: 0.687495  [   32/   43]
train() client id: f_00003-4-0 loss: 0.771066  [   32/   43]
train() client id: f_00003-5-0 loss: 0.605001  [   32/   43]
train() client id: f_00003-6-0 loss: 0.703265  [   32/   43]
train() client id: f_00003-7-0 loss: 0.794071  [   32/   43]
train() client id: f_00003-8-0 loss: 0.657734  [   32/   43]
train() client id: f_00004-0-0 loss: 0.940684  [   32/  306]
train() client id: f_00004-0-1 loss: 0.735922  [   64/  306]
train() client id: f_00004-0-2 loss: 0.717513  [   96/  306]
train() client id: f_00004-0-3 loss: 0.696268  [  128/  306]
train() client id: f_00004-0-4 loss: 0.881309  [  160/  306]
train() client id: f_00004-0-5 loss: 0.715835  [  192/  306]
train() client id: f_00004-0-6 loss: 0.577970  [  224/  306]
train() client id: f_00004-0-7 loss: 0.727792  [  256/  306]
train() client id: f_00004-0-8 loss: 0.672689  [  288/  306]
train() client id: f_00004-1-0 loss: 0.761830  [   32/  306]
train() client id: f_00004-1-1 loss: 0.757296  [   64/  306]
train() client id: f_00004-1-2 loss: 0.664901  [   96/  306]
train() client id: f_00004-1-3 loss: 0.808205  [  128/  306]
train() client id: f_00004-1-4 loss: 0.675475  [  160/  306]
train() client id: f_00004-1-5 loss: 0.867839  [  192/  306]
train() client id: f_00004-1-6 loss: 0.640461  [  224/  306]
train() client id: f_00004-1-7 loss: 0.801971  [  256/  306]
train() client id: f_00004-1-8 loss: 0.802497  [  288/  306]
train() client id: f_00004-2-0 loss: 0.649003  [   32/  306]
train() client id: f_00004-2-1 loss: 0.700093  [   64/  306]
train() client id: f_00004-2-2 loss: 0.821918  [   96/  306]
train() client id: f_00004-2-3 loss: 0.639567  [  128/  306]
train() client id: f_00004-2-4 loss: 0.870993  [  160/  306]
train() client id: f_00004-2-5 loss: 0.868297  [  192/  306]
train() client id: f_00004-2-6 loss: 0.693587  [  224/  306]
train() client id: f_00004-2-7 loss: 0.654190  [  256/  306]
train() client id: f_00004-2-8 loss: 0.789705  [  288/  306]
train() client id: f_00004-3-0 loss: 0.690466  [   32/  306]
train() client id: f_00004-3-1 loss: 0.790685  [   64/  306]
train() client id: f_00004-3-2 loss: 0.647879  [   96/  306]
train() client id: f_00004-3-3 loss: 0.770054  [  128/  306]
train() client id: f_00004-3-4 loss: 0.798057  [  160/  306]
train() client id: f_00004-3-5 loss: 0.747908  [  192/  306]
train() client id: f_00004-3-6 loss: 0.723794  [  224/  306]
train() client id: f_00004-3-7 loss: 0.740589  [  256/  306]
train() client id: f_00004-3-8 loss: 0.803088  [  288/  306]
train() client id: f_00004-4-0 loss: 0.855605  [   32/  306]
train() client id: f_00004-4-1 loss: 0.818796  [   64/  306]
train() client id: f_00004-4-2 loss: 0.756707  [   96/  306]
train() client id: f_00004-4-3 loss: 0.721768  [  128/  306]
train() client id: f_00004-4-4 loss: 0.871086  [  160/  306]
train() client id: f_00004-4-5 loss: 0.683781  [  192/  306]
train() client id: f_00004-4-6 loss: 0.666344  [  224/  306]
train() client id: f_00004-4-7 loss: 0.667831  [  256/  306]
train() client id: f_00004-4-8 loss: 0.723073  [  288/  306]
train() client id: f_00004-5-0 loss: 0.787750  [   32/  306]
train() client id: f_00004-5-1 loss: 0.639395  [   64/  306]
train() client id: f_00004-5-2 loss: 0.738763  [   96/  306]
train() client id: f_00004-5-3 loss: 0.778709  [  128/  306]
train() client id: f_00004-5-4 loss: 0.747114  [  160/  306]
train() client id: f_00004-5-5 loss: 0.757647  [  192/  306]
train() client id: f_00004-5-6 loss: 0.829473  [  224/  306]
train() client id: f_00004-5-7 loss: 0.653349  [  256/  306]
train() client id: f_00004-5-8 loss: 0.793632  [  288/  306]
train() client id: f_00004-6-0 loss: 0.782143  [   32/  306]
train() client id: f_00004-6-1 loss: 0.779090  [   64/  306]
train() client id: f_00004-6-2 loss: 0.791144  [   96/  306]
train() client id: f_00004-6-3 loss: 0.764038  [  128/  306]
train() client id: f_00004-6-4 loss: 0.684533  [  160/  306]
train() client id: f_00004-6-5 loss: 0.626439  [  192/  306]
train() client id: f_00004-6-6 loss: 0.772876  [  224/  306]
train() client id: f_00004-6-7 loss: 0.672095  [  256/  306]
train() client id: f_00004-6-8 loss: 0.795292  [  288/  306]
train() client id: f_00004-7-0 loss: 0.775327  [   32/  306]
train() client id: f_00004-7-1 loss: 0.747772  [   64/  306]
train() client id: f_00004-7-2 loss: 0.728046  [   96/  306]
train() client id: f_00004-7-3 loss: 0.780178  [  128/  306]
train() client id: f_00004-7-4 loss: 0.734269  [  160/  306]
train() client id: f_00004-7-5 loss: 0.704305  [  192/  306]
train() client id: f_00004-7-6 loss: 0.782014  [  224/  306]
train() client id: f_00004-7-7 loss: 0.774343  [  256/  306]
train() client id: f_00004-7-8 loss: 0.762677  [  288/  306]
train() client id: f_00004-8-0 loss: 0.745306  [   32/  306]
train() client id: f_00004-8-1 loss: 0.738765  [   64/  306]
train() client id: f_00004-8-2 loss: 0.754817  [   96/  306]
train() client id: f_00004-8-3 loss: 0.673615  [  128/  306]
train() client id: f_00004-8-4 loss: 0.715418  [  160/  306]
train() client id: f_00004-8-5 loss: 0.759820  [  192/  306]
train() client id: f_00004-8-6 loss: 0.806531  [  224/  306]
train() client id: f_00004-8-7 loss: 0.665080  [  256/  306]
train() client id: f_00004-8-8 loss: 0.825955  [  288/  306]
train() client id: f_00005-0-0 loss: 0.365478  [   32/  146]
train() client id: f_00005-0-1 loss: 0.868141  [   64/  146]
train() client id: f_00005-0-2 loss: 0.618190  [   96/  146]
train() client id: f_00005-0-3 loss: 0.414912  [  128/  146]
train() client id: f_00005-1-0 loss: 0.498468  [   32/  146]
train() client id: f_00005-1-1 loss: 0.505912  [   64/  146]
train() client id: f_00005-1-2 loss: 0.591089  [   96/  146]
train() client id: f_00005-1-3 loss: 0.382380  [  128/  146]
train() client id: f_00005-2-0 loss: 0.261048  [   32/  146]
train() client id: f_00005-2-1 loss: 0.429971  [   64/  146]
train() client id: f_00005-2-2 loss: 0.764427  [   96/  146]
train() client id: f_00005-2-3 loss: 0.649758  [  128/  146]
train() client id: f_00005-3-0 loss: 0.752043  [   32/  146]
train() client id: f_00005-3-1 loss: 0.600737  [   64/  146]
train() client id: f_00005-3-2 loss: 0.482322  [   96/  146]
train() client id: f_00005-3-3 loss: 0.266985  [  128/  146]
train() client id: f_00005-4-0 loss: 0.597044  [   32/  146]
train() client id: f_00005-4-1 loss: 0.524384  [   64/  146]
train() client id: f_00005-4-2 loss: 0.218873  [   96/  146]
train() client id: f_00005-4-3 loss: 0.781029  [  128/  146]
train() client id: f_00005-5-0 loss: 0.514774  [   32/  146]
train() client id: f_00005-5-1 loss: 0.624334  [   64/  146]
train() client id: f_00005-5-2 loss: 0.578789  [   96/  146]
train() client id: f_00005-5-3 loss: 0.487120  [  128/  146]
train() client id: f_00005-6-0 loss: 0.582217  [   32/  146]
train() client id: f_00005-6-1 loss: 0.653484  [   64/  146]
train() client id: f_00005-6-2 loss: 0.517308  [   96/  146]
train() client id: f_00005-6-3 loss: 0.458587  [  128/  146]
train() client id: f_00005-7-0 loss: 0.404525  [   32/  146]
train() client id: f_00005-7-1 loss: 0.326844  [   64/  146]
train() client id: f_00005-7-2 loss: 0.476009  [   96/  146]
train() client id: f_00005-7-3 loss: 0.790963  [  128/  146]
train() client id: f_00005-8-0 loss: 0.309141  [   32/  146]
train() client id: f_00005-8-1 loss: 0.796896  [   64/  146]
train() client id: f_00005-8-2 loss: 0.655377  [   96/  146]
train() client id: f_00005-8-3 loss: 0.614502  [  128/  146]
train() client id: f_00006-0-0 loss: 0.472296  [   32/   54]
train() client id: f_00006-1-0 loss: 0.510883  [   32/   54]
train() client id: f_00006-2-0 loss: 0.508815  [   32/   54]
train() client id: f_00006-3-0 loss: 0.505970  [   32/   54]
train() client id: f_00006-4-0 loss: 0.496442  [   32/   54]
train() client id: f_00006-5-0 loss: 0.529096  [   32/   54]
train() client id: f_00006-6-0 loss: 0.574945  [   32/   54]
train() client id: f_00006-7-0 loss: 0.499069  [   32/   54]
train() client id: f_00006-8-0 loss: 0.447736  [   32/   54]
train() client id: f_00007-0-0 loss: 0.614853  [   32/  179]
train() client id: f_00007-0-1 loss: 0.568856  [   64/  179]
train() client id: f_00007-0-2 loss: 0.731348  [   96/  179]
train() client id: f_00007-0-3 loss: 0.545524  [  128/  179]
train() client id: f_00007-0-4 loss: 0.425883  [  160/  179]
train() client id: f_00007-1-0 loss: 0.581172  [   32/  179]
train() client id: f_00007-1-1 loss: 0.584614  [   64/  179]
train() client id: f_00007-1-2 loss: 0.536006  [   96/  179]
train() client id: f_00007-1-3 loss: 0.636569  [  128/  179]
train() client id: f_00007-1-4 loss: 0.497140  [  160/  179]
train() client id: f_00007-2-0 loss: 0.598132  [   32/  179]
train() client id: f_00007-2-1 loss: 0.475558  [   64/  179]
train() client id: f_00007-2-2 loss: 0.553580  [   96/  179]
train() client id: f_00007-2-3 loss: 0.720342  [  128/  179]
train() client id: f_00007-2-4 loss: 0.531639  [  160/  179]
train() client id: f_00007-3-0 loss: 0.511248  [   32/  179]
train() client id: f_00007-3-1 loss: 0.503271  [   64/  179]
train() client id: f_00007-3-2 loss: 0.738313  [   96/  179]
train() client id: f_00007-3-3 loss: 0.444629  [  128/  179]
train() client id: f_00007-3-4 loss: 0.565584  [  160/  179]
train() client id: f_00007-4-0 loss: 0.499087  [   32/  179]
train() client id: f_00007-4-1 loss: 0.562355  [   64/  179]
train() client id: f_00007-4-2 loss: 0.477005  [   96/  179]
train() client id: f_00007-4-3 loss: 0.585451  [  128/  179]
train() client id: f_00007-4-4 loss: 0.532273  [  160/  179]
train() client id: f_00007-5-0 loss: 0.389265  [   32/  179]
train() client id: f_00007-5-1 loss: 0.370797  [   64/  179]
train() client id: f_00007-5-2 loss: 0.854896  [   96/  179]
train() client id: f_00007-5-3 loss: 0.495185  [  128/  179]
train() client id: f_00007-5-4 loss: 0.517549  [  160/  179]
train() client id: f_00007-6-0 loss: 0.373468  [   32/  179]
train() client id: f_00007-6-1 loss: 0.507716  [   64/  179]
train() client id: f_00007-6-2 loss: 0.507766  [   96/  179]
train() client id: f_00007-6-3 loss: 0.612257  [  128/  179]
train() client id: f_00007-6-4 loss: 0.707332  [  160/  179]
train() client id: f_00007-7-0 loss: 0.346828  [   32/  179]
train() client id: f_00007-7-1 loss: 0.529532  [   64/  179]
train() client id: f_00007-7-2 loss: 0.558708  [   96/  179]
train() client id: f_00007-7-3 loss: 0.753484  [  128/  179]
train() client id: f_00007-7-4 loss: 0.486292  [  160/  179]
train() client id: f_00007-8-0 loss: 0.483401  [   32/  179]
train() client id: f_00007-8-1 loss: 0.614897  [   64/  179]
train() client id: f_00007-8-2 loss: 0.532152  [   96/  179]
train() client id: f_00007-8-3 loss: 0.368531  [  128/  179]
train() client id: f_00007-8-4 loss: 0.598286  [  160/  179]
train() client id: f_00008-0-0 loss: 0.623654  [   32/  130]
train() client id: f_00008-0-1 loss: 0.629914  [   64/  130]
train() client id: f_00008-0-2 loss: 0.534081  [   96/  130]
train() client id: f_00008-0-3 loss: 0.834442  [  128/  130]
train() client id: f_00008-1-0 loss: 0.554991  [   32/  130]
train() client id: f_00008-1-1 loss: 0.529953  [   64/  130]
train() client id: f_00008-1-2 loss: 0.838658  [   96/  130]
train() client id: f_00008-1-3 loss: 0.694035  [  128/  130]
train() client id: f_00008-2-0 loss: 0.735519  [   32/  130]
train() client id: f_00008-2-1 loss: 0.541562  [   64/  130]
train() client id: f_00008-2-2 loss: 0.672784  [   96/  130]
train() client id: f_00008-2-3 loss: 0.645738  [  128/  130]
train() client id: f_00008-3-0 loss: 0.694217  [   32/  130]
train() client id: f_00008-3-1 loss: 0.704199  [   64/  130]
train() client id: f_00008-3-2 loss: 0.534394  [   96/  130]
train() client id: f_00008-3-3 loss: 0.657159  [  128/  130]
train() client id: f_00008-4-0 loss: 0.700269  [   32/  130]
train() client id: f_00008-4-1 loss: 0.675438  [   64/  130]
train() client id: f_00008-4-2 loss: 0.674356  [   96/  130]
train() client id: f_00008-4-3 loss: 0.568503  [  128/  130]
train() client id: f_00008-5-0 loss: 0.618637  [   32/  130]
train() client id: f_00008-5-1 loss: 0.728905  [   64/  130]
train() client id: f_00008-5-2 loss: 0.692740  [   96/  130]
train() client id: f_00008-5-3 loss: 0.551139  [  128/  130]
train() client id: f_00008-6-0 loss: 0.580355  [   32/  130]
train() client id: f_00008-6-1 loss: 0.759634  [   64/  130]
train() client id: f_00008-6-2 loss: 0.611860  [   96/  130]
train() client id: f_00008-6-3 loss: 0.672458  [  128/  130]
train() client id: f_00008-7-0 loss: 0.667448  [   32/  130]
train() client id: f_00008-7-1 loss: 0.685825  [   64/  130]
train() client id: f_00008-7-2 loss: 0.652652  [   96/  130]
train() client id: f_00008-7-3 loss: 0.602251  [  128/  130]
train() client id: f_00008-8-0 loss: 0.692388  [   32/  130]
train() client id: f_00008-8-1 loss: 0.667507  [   64/  130]
train() client id: f_00008-8-2 loss: 0.725871  [   96/  130]
train() client id: f_00008-8-3 loss: 0.551010  [  128/  130]
train() client id: f_00009-0-0 loss: 0.971122  [   32/  118]
train() client id: f_00009-0-1 loss: 0.934822  [   64/  118]
train() client id: f_00009-0-2 loss: 0.990119  [   96/  118]
train() client id: f_00009-1-0 loss: 1.069624  [   32/  118]
train() client id: f_00009-1-1 loss: 0.928110  [   64/  118]
train() client id: f_00009-1-2 loss: 0.790086  [   96/  118]
train() client id: f_00009-2-0 loss: 0.962185  [   32/  118]
train() client id: f_00009-2-1 loss: 0.898934  [   64/  118]
train() client id: f_00009-2-2 loss: 0.861146  [   96/  118]
train() client id: f_00009-3-0 loss: 0.901253  [   32/  118]
train() client id: f_00009-3-1 loss: 0.796392  [   64/  118]
train() client id: f_00009-3-2 loss: 1.033359  [   96/  118]
train() client id: f_00009-4-0 loss: 0.810653  [   32/  118]
train() client id: f_00009-4-1 loss: 0.962333  [   64/  118]
train() client id: f_00009-4-2 loss: 0.732827  [   96/  118]
train() client id: f_00009-5-0 loss: 0.695075  [   32/  118]
train() client id: f_00009-5-1 loss: 0.824119  [   64/  118]
train() client id: f_00009-5-2 loss: 0.955590  [   96/  118]
train() client id: f_00009-6-0 loss: 0.706288  [   32/  118]
train() client id: f_00009-6-1 loss: 0.957477  [   64/  118]
train() client id: f_00009-6-2 loss: 0.916168  [   96/  118]
train() client id: f_00009-7-0 loss: 0.738573  [   32/  118]
train() client id: f_00009-7-1 loss: 0.798562  [   64/  118]
train() client id: f_00009-7-2 loss: 0.824805  [   96/  118]
train() client id: f_00009-8-0 loss: 0.834849  [   32/  118]
train() client id: f_00009-8-1 loss: 0.852775  [   64/  118]
train() client id: f_00009-8-2 loss: 0.651392  [   96/  118]
At round 64 accuracy: 0.6472148541114059
At round 64 training accuracy: 0.5955734406438632
At round 64 training loss: 0.8211805267524739
update_location
xs = -3.905658 4.200318 340.009024 18.811294 0.979296 3.956410 -302.443192 -281.324852 324.663977 -267.060879 
ys = 332.587959 315.555839 1.320614 -302.455176 294.350187 277.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 347.318305 331.048531 354.412021 319.112830 310.874559 295.290290 318.557334 298.570508 340.169616 285.197343 
dists_bs = 232.609627 227.747004 543.506048 515.292405 212.633910 206.388338 218.614735 204.065746 523.878428 194.205300 
uav_gains = -119.023854 -118.040629 -119.408694 -117.221114 -116.604801 -115.332351 -117.180856 -115.610749 -118.609927 -114.447716 
bs_gains = -105.832634 -105.575734 -116.152782 -115.504564 -104.740770 -104.378243 -105.078083 -104.240622 -115.705514 -103.638369 
Round 65
-------------------------------
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.41801623 4.82581662 2.38917526 0.89421619 5.56278746 2.67778749
 1.09198131 3.3275343  2.44520381 2.17140866]
obj_prev = 27.80392733965525
eta_min = 3.100306852884529e-39	eta_max = 0.942754072368237
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 6.3556060652170405	eta = 0.9090909090909091
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 16.15583264303265	eta = 0.3576308212219315
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 10.677811898063974	eta = 0.5411055889361986
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 9.733010612728588	eta = 0.5936317061131898
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 9.673834452832171	eta = 0.597263031926322
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 9.673568210715082	eta = 0.5972794701806058
af = 5.777823695651855	bf = 0.9180266431472464	zeta = 9.673568205281397	eta = 0.5972794705161003
eta = 0.5972794705161003
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [0.04252711 0.08944194 0.04185208 0.01451322 0.10328012 0.04927744
 0.0182259  0.0604155  0.04387718 0.03982698]
ene_total = [0.98560292 1.43449508 0.99341101 0.50602748 1.63267669 0.83712064
 0.55929955 1.13571028 0.89502304 0.69420151]
ti_comp = [1.57505947 1.73869087 1.56299493 1.62132055 1.74218994 1.74362015
 1.62219032 1.65177999 1.65020898 1.7463865 ]
ti_coms = [0.24553217 0.08190077 0.25759671 0.19927109 0.0784017  0.07697149
 0.19840132 0.16881166 0.17038266 0.07420514]
t_total = [26.74972725 26.74972725 26.74972725 26.74972725 26.74972725 26.74972725
 26.74972725 26.74972725 26.74972725 26.74972725]
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [1.93768734e-06 1.47930885e-05 1.87549493e-06 7.26831216e-08
 2.26850246e-05 2.45991889e-06 1.43794955e-07 5.05150367e-06
 1.93873362e-06 1.29458708e-06]
ene_total = [0.36078585 0.12055314 0.37851121 0.29278761 0.11552795 0.1131294
 0.29151072 0.24810709 0.25036961 0.10904772]
optimize_network iter = 0 obj = 2.280330304694935
eta = 0.5972794705161003
freqs = [13500159.94622198 25721057.81080511 13388423.45249819  4475740.44373556
 29640890.63331935 14130785.41729783  5617683.65648641 18287997.60708139
 13294430.26761132 11402680.79036755]
eta_min = 0.5972794705161009	eta_max = 0.7502409697515174
af = 0.0005999271736932453	bf = 0.9180266431472464	zeta = 0.0006599198910625699	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [3.58346965e-07 2.73576559e-06 3.46845386e-07 1.34416814e-08
 4.19526387e-06 4.54926060e-07 2.65927760e-08 9.34201802e-07
 3.58540459e-07 2.39414968e-07]
ene_total = [1.59815174 0.53325669 1.67667707 1.29702355 0.51057679 0.50102436
 1.29136322 1.09882803 1.10901597 0.48300462]
ti_comp = [0.8835615  1.0471929  0.87149697 0.92982258 1.05069198 1.05212218
 0.93069235 0.96028202 0.95871102 1.05488853]
ti_coms = [0.24553217 0.08190077 0.25759671 0.19927109 0.0784017  0.07697149
 0.19840132 0.16881166 0.17038266 0.07420514]
t_total = [26.74972725 26.74972725 26.74972725 26.74972725 26.74972725 26.74972725
 26.74972725 26.74972725 26.74972725 26.74972725]
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [1.06740436e-06 7.06927946e-06 1.04574117e-06 3.83085836e-08
 1.08119582e-05 1.17116240e-06 7.57285947e-08 2.59091048e-06
 9.95739029e-07 6.15068927e-07]
ene_total = [0.58172358 0.19420128 0.61030556 0.47210056 0.18600018 0.18238342
 0.47004085 0.39999859 0.40368272 0.1758164 ]
optimize_network iter = 1 obj = 3.676253139763781
eta = 0.7502409697515174
freqs = [13418605.79030925 23811832.48313907 13388423.4524982   4351530.1809025
 27404358.81401067 13057508.37108007  5459605.33203652 17539930.50999551
 12759371.1105045  10525653.53083322]
eta_min = 0.7502409697515208	eta_max = 0.7502409697515149
af = 0.00052469220506583	bf = 0.9180266431472464	zeta = 0.000577161425572413	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [3.54030510e-07 2.34469776e-06 3.46845386e-07 1.27059696e-08
 3.58604780e-06 3.88444377e-07 2.51172226e-08 8.59338214e-07
 3.30260967e-07 2.04002507e-07]
ene_total = [1.59815146 0.53323124 1.67667707 1.2970235  0.51053714 0.50102003
 1.29136313 1.09882316 1.10901413 0.48300231]
ti_comp = [0.8835615  1.0471929  0.87149697 0.92982258 1.05069198 1.05212218
 0.93069235 0.96028202 0.95871102 1.05488853]
ti_coms = [0.24553217 0.08190077 0.25759671 0.19927109 0.0784017  0.07697149
 0.19840132 0.16881166 0.17038266 0.07420514]
t_total = [26.74972725 26.74972725 26.74972725 26.74972725 26.74972725 26.74972725
 26.74972725 26.74972725 26.74972725 26.74972725]
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [1.06740436e-06 7.06927946e-06 1.04574117e-06 3.83085836e-08
 1.08119582e-05 1.17116240e-06 7.57285947e-08 2.59091048e-06
 9.95739029e-07 6.15068927e-07]
ene_total = [0.58172358 0.19420128 0.61030556 0.47210056 0.18600018 0.18238342
 0.47004085 0.39999859 0.40368272 0.1758164 ]
optimize_network iter = 2 obj = 3.6762531397637437
eta = 0.7502409697515149
freqs = [13418605.79030923 23811832.48313908 13388423.45249818  4351530.1809025
 27404358.81401069 13057508.37108008  5459605.33203651 17539930.5099955
 12759371.11050449 10525653.53083322]
Done!
ene_coms = [0.02455322 0.00819008 0.02575967 0.01992711 0.00784017 0.00769715
 0.01984013 0.01688117 0.01703827 0.00742051]
ene_comp = [1.02093442e-06 6.76151514e-06 1.00021434e-06 3.66408019e-08
 1.03412546e-05 1.12017531e-06 7.24317156e-08 2.47811400e-06
 9.52389074e-07 5.88291620e-07]
ene_total = [0.02455424 0.00819684 0.02576067 0.01992715 0.00785051 0.00769827
 0.0198402  0.01688364 0.01703922 0.0074211 ]
At round 65 energy consumption: 0.15517184374296747
At round 65 eta: 0.7502409697515149
At round 65 a_n: 5.917122803248752
At round 65 local rounds: 9.409653576790653
At round 65 global rounds: 23.691326785509258
gradient difference: 0.45931363105773926
train() client id: f_00000-0-0 loss: 1.145903  [   32/  126]
train() client id: f_00000-0-1 loss: 0.942104  [   64/  126]
train() client id: f_00000-0-2 loss: 0.825876  [   96/  126]
train() client id: f_00000-1-0 loss: 1.051885  [   32/  126]
train() client id: f_00000-1-1 loss: 0.935176  [   64/  126]
train() client id: f_00000-1-2 loss: 0.946949  [   96/  126]
train() client id: f_00000-2-0 loss: 0.807352  [   32/  126]
train() client id: f_00000-2-1 loss: 0.836009  [   64/  126]
train() client id: f_00000-2-2 loss: 0.959020  [   96/  126]
train() client id: f_00000-3-0 loss: 0.747798  [   32/  126]
train() client id: f_00000-3-1 loss: 0.749395  [   64/  126]
train() client id: f_00000-3-2 loss: 0.923378  [   96/  126]
train() client id: f_00000-4-0 loss: 0.760041  [   32/  126]
train() client id: f_00000-4-1 loss: 0.687497  [   64/  126]
train() client id: f_00000-4-2 loss: 0.817035  [   96/  126]
train() client id: f_00000-5-0 loss: 0.699033  [   32/  126]
train() client id: f_00000-5-1 loss: 0.850087  [   64/  126]
train() client id: f_00000-5-2 loss: 0.716622  [   96/  126]
train() client id: f_00000-6-0 loss: 0.747150  [   32/  126]
train() client id: f_00000-6-1 loss: 0.722060  [   64/  126]
train() client id: f_00000-6-2 loss: 0.686912  [   96/  126]
train() client id: f_00000-7-0 loss: 0.611359  [   32/  126]
train() client id: f_00000-7-1 loss: 0.805958  [   64/  126]
train() client id: f_00000-7-2 loss: 0.739420  [   96/  126]
train() client id: f_00000-8-0 loss: 0.749446  [   32/  126]
train() client id: f_00000-8-1 loss: 0.822594  [   64/  126]
train() client id: f_00000-8-2 loss: 0.638458  [   96/  126]
train() client id: f_00001-0-0 loss: 0.609276  [   32/  265]
train() client id: f_00001-0-1 loss: 0.565649  [   64/  265]
train() client id: f_00001-0-2 loss: 0.629149  [   96/  265]
train() client id: f_00001-0-3 loss: 0.506510  [  128/  265]
train() client id: f_00001-0-4 loss: 0.470929  [  160/  265]
train() client id: f_00001-0-5 loss: 0.526699  [  192/  265]
train() client id: f_00001-0-6 loss: 0.519139  [  224/  265]
train() client id: f_00001-0-7 loss: 0.596395  [  256/  265]
train() client id: f_00001-1-0 loss: 0.496659  [   32/  265]
train() client id: f_00001-1-1 loss: 0.629872  [   64/  265]
train() client id: f_00001-1-2 loss: 0.523211  [   96/  265]
train() client id: f_00001-1-3 loss: 0.516825  [  128/  265]
train() client id: f_00001-1-4 loss: 0.552392  [  160/  265]
train() client id: f_00001-1-5 loss: 0.518686  [  192/  265]
train() client id: f_00001-1-6 loss: 0.497106  [  224/  265]
train() client id: f_00001-1-7 loss: 0.552014  [  256/  265]
train() client id: f_00001-2-0 loss: 0.501857  [   32/  265]
train() client id: f_00001-2-1 loss: 0.548028  [   64/  265]
train() client id: f_00001-2-2 loss: 0.658180  [   96/  265]
train() client id: f_00001-2-3 loss: 0.550120  [  128/  265]
train() client id: f_00001-2-4 loss: 0.487187  [  160/  265]
train() client id: f_00001-2-5 loss: 0.551715  [  192/  265]
train() client id: f_00001-2-6 loss: 0.524984  [  224/  265]
train() client id: f_00001-2-7 loss: 0.523574  [  256/  265]
train() client id: f_00001-3-0 loss: 0.506627  [   32/  265]
train() client id: f_00001-3-1 loss: 0.600113  [   64/  265]
train() client id: f_00001-3-2 loss: 0.569983  [   96/  265]
train() client id: f_00001-3-3 loss: 0.588687  [  128/  265]
train() client id: f_00001-3-4 loss: 0.466014  [  160/  265]
train() client id: f_00001-3-5 loss: 0.599326  [  192/  265]
train() client id: f_00001-3-6 loss: 0.519256  [  224/  265]
train() client id: f_00001-3-7 loss: 0.481739  [  256/  265]
train() client id: f_00001-4-0 loss: 0.524086  [   32/  265]
train() client id: f_00001-4-1 loss: 0.500865  [   64/  265]
train() client id: f_00001-4-2 loss: 0.531817  [   96/  265]
train() client id: f_00001-4-3 loss: 0.541972  [  128/  265]
train() client id: f_00001-4-4 loss: 0.514879  [  160/  265]
train() client id: f_00001-4-5 loss: 0.492194  [  192/  265]
train() client id: f_00001-4-6 loss: 0.616112  [  224/  265]
train() client id: f_00001-4-7 loss: 0.549786  [  256/  265]
train() client id: f_00001-5-0 loss: 0.525120  [   32/  265]
train() client id: f_00001-5-1 loss: 0.457577  [   64/  265]
train() client id: f_00001-5-2 loss: 0.642449  [   96/  265]
train() client id: f_00001-5-3 loss: 0.466260  [  128/  265]
train() client id: f_00001-5-4 loss: 0.494510  [  160/  265]
train() client id: f_00001-5-5 loss: 0.604896  [  192/  265]
train() client id: f_00001-5-6 loss: 0.627247  [  224/  265]
train() client id: f_00001-5-7 loss: 0.526916  [  256/  265]
train() client id: f_00001-6-0 loss: 0.540973  [   32/  265]
train() client id: f_00001-6-1 loss: 0.462403  [   64/  265]
train() client id: f_00001-6-2 loss: 0.693560  [   96/  265]
train() client id: f_00001-6-3 loss: 0.483749  [  128/  265]
train() client id: f_00001-6-4 loss: 0.621512  [  160/  265]
train() client id: f_00001-6-5 loss: 0.560467  [  192/  265]
train() client id: f_00001-6-6 loss: 0.431972  [  224/  265]
train() client id: f_00001-6-7 loss: 0.530763  [  256/  265]
train() client id: f_00001-7-0 loss: 0.448061  [   32/  265]
train() client id: f_00001-7-1 loss: 0.478939  [   64/  265]
train() client id: f_00001-7-2 loss: 0.535563  [   96/  265]
train() client id: f_00001-7-3 loss: 0.529790  [  128/  265]
train() client id: f_00001-7-4 loss: 0.511073  [  160/  265]
train() client id: f_00001-7-5 loss: 0.534604  [  192/  265]
train() client id: f_00001-7-6 loss: 0.691554  [  224/  265]
train() client id: f_00001-7-7 loss: 0.491160  [  256/  265]
train() client id: f_00001-8-0 loss: 0.545249  [   32/  265]
train() client id: f_00001-8-1 loss: 0.441785  [   64/  265]
train() client id: f_00001-8-2 loss: 0.505735  [   96/  265]
train() client id: f_00001-8-3 loss: 0.478446  [  128/  265]
train() client id: f_00001-8-4 loss: 0.563050  [  160/  265]
train() client id: f_00001-8-5 loss: 0.587179  [  192/  265]
train() client id: f_00001-8-6 loss: 0.562681  [  224/  265]
train() client id: f_00001-8-7 loss: 0.431561  [  256/  265]
train() client id: f_00002-0-0 loss: 0.964415  [   32/  124]
train() client id: f_00002-0-1 loss: 1.021169  [   64/  124]
train() client id: f_00002-0-2 loss: 0.960041  [   96/  124]
train() client id: f_00002-1-0 loss: 0.944360  [   32/  124]
train() client id: f_00002-1-1 loss: 1.051101  [   64/  124]
train() client id: f_00002-1-2 loss: 0.998697  [   96/  124]
train() client id: f_00002-2-0 loss: 1.011149  [   32/  124]
train() client id: f_00002-2-1 loss: 0.920292  [   64/  124]
train() client id: f_00002-2-2 loss: 1.031558  [   96/  124]
train() client id: f_00002-3-0 loss: 0.901113  [   32/  124]
train() client id: f_00002-3-1 loss: 0.924566  [   64/  124]
train() client id: f_00002-3-2 loss: 0.741998  [   96/  124]
train() client id: f_00002-4-0 loss: 0.761994  [   32/  124]
train() client id: f_00002-4-1 loss: 0.915904  [   64/  124]
train() client id: f_00002-4-2 loss: 0.963108  [   96/  124]
train() client id: f_00002-5-0 loss: 0.889392  [   32/  124]
train() client id: f_00002-5-1 loss: 0.968704  [   64/  124]
train() client id: f_00002-5-2 loss: 0.705673  [   96/  124]
train() client id: f_00002-6-0 loss: 0.946204  [   32/  124]
train() client id: f_00002-6-1 loss: 0.933473  [   64/  124]
train() client id: f_00002-6-2 loss: 0.708407  [   96/  124]
train() client id: f_00002-7-0 loss: 0.798402  [   32/  124]
train() client id: f_00002-7-1 loss: 0.793497  [   64/  124]
train() client id: f_00002-7-2 loss: 0.995952  [   96/  124]
train() client id: f_00002-8-0 loss: 0.912914  [   32/  124]
train() client id: f_00002-8-1 loss: 0.932100  [   64/  124]
train() client id: f_00002-8-2 loss: 0.773336  [   96/  124]
train() client id: f_00003-0-0 loss: 0.870724  [   32/   43]
train() client id: f_00003-1-0 loss: 0.639957  [   32/   43]
train() client id: f_00003-2-0 loss: 0.682152  [   32/   43]
train() client id: f_00003-3-0 loss: 0.845744  [   32/   43]
train() client id: f_00003-4-0 loss: 0.604580  [   32/   43]
train() client id: f_00003-5-0 loss: 0.815374  [   32/   43]
train() client id: f_00003-6-0 loss: 0.763813  [   32/   43]
train() client id: f_00003-7-0 loss: 0.464019  [   32/   43]
train() client id: f_00003-8-0 loss: 0.820584  [   32/   43]
train() client id: f_00004-0-0 loss: 0.741001  [   32/  306]
train() client id: f_00004-0-1 loss: 0.683800  [   64/  306]
train() client id: f_00004-0-2 loss: 0.729248  [   96/  306]
train() client id: f_00004-0-3 loss: 0.686325  [  128/  306]
train() client id: f_00004-0-4 loss: 0.797964  [  160/  306]
train() client id: f_00004-0-5 loss: 0.799603  [  192/  306]
train() client id: f_00004-0-6 loss: 0.699476  [  224/  306]
train() client id: f_00004-0-7 loss: 0.841779  [  256/  306]
train() client id: f_00004-0-8 loss: 0.748860  [  288/  306]
train() client id: f_00004-1-0 loss: 0.722906  [   32/  306]
train() client id: f_00004-1-1 loss: 0.741193  [   64/  306]
train() client id: f_00004-1-2 loss: 0.880424  [   96/  306]
train() client id: f_00004-1-3 loss: 0.682806  [  128/  306]
train() client id: f_00004-1-4 loss: 0.668501  [  160/  306]
train() client id: f_00004-1-5 loss: 0.694112  [  192/  306]
train() client id: f_00004-1-6 loss: 0.775562  [  224/  306]
train() client id: f_00004-1-7 loss: 0.756563  [  256/  306]
train() client id: f_00004-1-8 loss: 0.786466  [  288/  306]
train() client id: f_00004-2-0 loss: 0.669456  [   32/  306]
train() client id: f_00004-2-1 loss: 0.683357  [   64/  306]
train() client id: f_00004-2-2 loss: 0.891452  [   96/  306]
train() client id: f_00004-2-3 loss: 0.616021  [  128/  306]
train() client id: f_00004-2-4 loss: 0.982234  [  160/  306]
train() client id: f_00004-2-5 loss: 0.705138  [  192/  306]
train() client id: f_00004-2-6 loss: 0.759023  [  224/  306]
train() client id: f_00004-2-7 loss: 0.642428  [  256/  306]
train() client id: f_00004-2-8 loss: 0.817559  [  288/  306]
train() client id: f_00004-3-0 loss: 0.758346  [   32/  306]
train() client id: f_00004-3-1 loss: 0.767037  [   64/  306]
train() client id: f_00004-3-2 loss: 0.798639  [   96/  306]
train() client id: f_00004-3-3 loss: 0.752261  [  128/  306]
train() client id: f_00004-3-4 loss: 0.676144  [  160/  306]
train() client id: f_00004-3-5 loss: 0.807473  [  192/  306]
train() client id: f_00004-3-6 loss: 0.652036  [  224/  306]
train() client id: f_00004-3-7 loss: 0.818981  [  256/  306]
train() client id: f_00004-3-8 loss: 0.715462  [  288/  306]
train() client id: f_00004-4-0 loss: 0.827582  [   32/  306]
train() client id: f_00004-4-1 loss: 1.025390  [   64/  306]
train() client id: f_00004-4-2 loss: 0.750553  [   96/  306]
train() client id: f_00004-4-3 loss: 0.746653  [  128/  306]
train() client id: f_00004-4-4 loss: 0.730043  [  160/  306]
train() client id: f_00004-4-5 loss: 0.690897  [  192/  306]
train() client id: f_00004-4-6 loss: 0.602396  [  224/  306]
train() client id: f_00004-4-7 loss: 0.674252  [  256/  306]
train() client id: f_00004-4-8 loss: 0.778552  [  288/  306]
train() client id: f_00004-5-0 loss: 0.807064  [   32/  306]
train() client id: f_00004-5-1 loss: 0.751823  [   64/  306]
train() client id: f_00004-5-2 loss: 0.706082  [   96/  306]
train() client id: f_00004-5-3 loss: 0.665039  [  128/  306]
train() client id: f_00004-5-4 loss: 0.819215  [  160/  306]
train() client id: f_00004-5-5 loss: 0.755532  [  192/  306]
train() client id: f_00004-5-6 loss: 0.809031  [  224/  306]
train() client id: f_00004-5-7 loss: 0.723957  [  256/  306]
train() client id: f_00004-5-8 loss: 0.749680  [  288/  306]
train() client id: f_00004-6-0 loss: 0.657205  [   32/  306]
train() client id: f_00004-6-1 loss: 0.628609  [   64/  306]
train() client id: f_00004-6-2 loss: 0.726377  [   96/  306]
train() client id: f_00004-6-3 loss: 0.703225  [  128/  306]
train() client id: f_00004-6-4 loss: 0.826476  [  160/  306]
train() client id: f_00004-6-5 loss: 0.837442  [  192/  306]
train() client id: f_00004-6-6 loss: 0.764154  [  224/  306]
train() client id: f_00004-6-7 loss: 0.907398  [  256/  306]
train() client id: f_00004-6-8 loss: 0.704471  [  288/  306]
train() client id: f_00004-7-0 loss: 0.770992  [   32/  306]
train() client id: f_00004-7-1 loss: 0.782709  [   64/  306]
train() client id: f_00004-7-2 loss: 0.814105  [   96/  306]
train() client id: f_00004-7-3 loss: 0.739910  [  128/  306]
train() client id: f_00004-7-4 loss: 0.788873  [  160/  306]
train() client id: f_00004-7-5 loss: 0.681214  [  192/  306]
train() client id: f_00004-7-6 loss: 0.667042  [  224/  306]
train() client id: f_00004-7-7 loss: 0.666981  [  256/  306]
train() client id: f_00004-7-8 loss: 0.818140  [  288/  306]
train() client id: f_00004-8-0 loss: 0.716955  [   32/  306]
train() client id: f_00004-8-1 loss: 0.713611  [   64/  306]
train() client id: f_00004-8-2 loss: 0.755767  [   96/  306]
train() client id: f_00004-8-3 loss: 0.870395  [  128/  306]
train() client id: f_00004-8-4 loss: 0.718532  [  160/  306]
train() client id: f_00004-8-5 loss: 0.774311  [  192/  306]
train() client id: f_00004-8-6 loss: 0.835752  [  224/  306]
train() client id: f_00004-8-7 loss: 0.610497  [  256/  306]
train() client id: f_00004-8-8 loss: 0.788003  [  288/  306]
train() client id: f_00005-0-0 loss: 0.661920  [   32/  146]
train() client id: f_00005-0-1 loss: 0.651150  [   64/  146]
train() client id: f_00005-0-2 loss: 0.510869  [   96/  146]
train() client id: f_00005-0-3 loss: 0.605115  [  128/  146]
train() client id: f_00005-1-0 loss: 0.616176  [   32/  146]
train() client id: f_00005-1-1 loss: 0.515873  [   64/  146]
train() client id: f_00005-1-2 loss: 0.401363  [   96/  146]
train() client id: f_00005-1-3 loss: 0.752414  [  128/  146]
train() client id: f_00005-2-0 loss: 0.511627  [   32/  146]
train() client id: f_00005-2-1 loss: 0.667440  [   64/  146]
train() client id: f_00005-2-2 loss: 0.401471  [   96/  146]
train() client id: f_00005-2-3 loss: 0.676595  [  128/  146]
train() client id: f_00005-3-0 loss: 0.446177  [   32/  146]
train() client id: f_00005-3-1 loss: 0.872725  [   64/  146]
train() client id: f_00005-3-2 loss: 0.409961  [   96/  146]
train() client id: f_00005-3-3 loss: 0.615037  [  128/  146]
train() client id: f_00005-4-0 loss: 0.794835  [   32/  146]
train() client id: f_00005-4-1 loss: 0.499655  [   64/  146]
train() client id: f_00005-4-2 loss: 0.606528  [   96/  146]
train() client id: f_00005-4-3 loss: 0.409766  [  128/  146]
train() client id: f_00005-5-0 loss: 0.523668  [   32/  146]
train() client id: f_00005-5-1 loss: 0.652271  [   64/  146]
train() client id: f_00005-5-2 loss: 0.529602  [   96/  146]
train() client id: f_00005-5-3 loss: 0.470515  [  128/  146]
train() client id: f_00005-6-0 loss: 0.553372  [   32/  146]
train() client id: f_00005-6-1 loss: 0.781108  [   64/  146]
train() client id: f_00005-6-2 loss: 0.387118  [   96/  146]
train() client id: f_00005-6-3 loss: 0.654017  [  128/  146]
train() client id: f_00005-7-0 loss: 0.391764  [   32/  146]
train() client id: f_00005-7-1 loss: 0.457793  [   64/  146]
train() client id: f_00005-7-2 loss: 0.464491  [   96/  146]
train() client id: f_00005-7-3 loss: 0.686821  [  128/  146]
train() client id: f_00005-8-0 loss: 0.471101  [   32/  146]
train() client id: f_00005-8-1 loss: 0.238543  [   64/  146]
train() client id: f_00005-8-2 loss: 0.641614  [   96/  146]
train() client id: f_00005-8-3 loss: 0.826962  [  128/  146]
train() client id: f_00006-0-0 loss: 0.532072  [   32/   54]
train() client id: f_00006-1-0 loss: 0.507366  [   32/   54]
train() client id: f_00006-2-0 loss: 0.537746  [   32/   54]
train() client id: f_00006-3-0 loss: 0.493843  [   32/   54]
train() client id: f_00006-4-0 loss: 0.594059  [   32/   54]
train() client id: f_00006-5-0 loss: 0.582682  [   32/   54]
train() client id: f_00006-6-0 loss: 0.526705  [   32/   54]
train() client id: f_00006-7-0 loss: 0.588178  [   32/   54]
train() client id: f_00006-8-0 loss: 0.613237  [   32/   54]
train() client id: f_00007-0-0 loss: 0.862116  [   32/  179]
train() client id: f_00007-0-1 loss: 0.398295  [   64/  179]
train() client id: f_00007-0-2 loss: 0.419077  [   96/  179]
train() client id: f_00007-0-3 loss: 0.650811  [  128/  179]
train() client id: f_00007-0-4 loss: 0.414103  [  160/  179]
train() client id: f_00007-1-0 loss: 0.673303  [   32/  179]
train() client id: f_00007-1-1 loss: 0.636581  [   64/  179]
train() client id: f_00007-1-2 loss: 0.494761  [   96/  179]
train() client id: f_00007-1-3 loss: 0.500102  [  128/  179]
train() client id: f_00007-1-4 loss: 0.479078  [  160/  179]
train() client id: f_00007-2-0 loss: 0.514407  [   32/  179]
train() client id: f_00007-2-1 loss: 0.681559  [   64/  179]
train() client id: f_00007-2-2 loss: 0.447866  [   96/  179]
train() client id: f_00007-2-3 loss: 0.568682  [  128/  179]
train() client id: f_00007-2-4 loss: 0.404082  [  160/  179]
train() client id: f_00007-3-0 loss: 0.620965  [   32/  179]
train() client id: f_00007-3-1 loss: 0.462289  [   64/  179]
train() client id: f_00007-3-2 loss: 0.439686  [   96/  179]
train() client id: f_00007-3-3 loss: 0.648111  [  128/  179]
train() client id: f_00007-3-4 loss: 0.576760  [  160/  179]
train() client id: f_00007-4-0 loss: 0.506999  [   32/  179]
train() client id: f_00007-4-1 loss: 0.326937  [   64/  179]
train() client id: f_00007-4-2 loss: 0.774195  [   96/  179]
train() client id: f_00007-4-3 loss: 0.493101  [  128/  179]
train() client id: f_00007-4-4 loss: 0.589770  [  160/  179]
train() client id: f_00007-5-0 loss: 0.446857  [   32/  179]
train() client id: f_00007-5-1 loss: 0.579482  [   64/  179]
train() client id: f_00007-5-2 loss: 0.366690  [   96/  179]
train() client id: f_00007-5-3 loss: 0.824588  [  128/  179]
train() client id: f_00007-5-4 loss: 0.486353  [  160/  179]
train() client id: f_00007-6-0 loss: 0.508839  [   32/  179]
train() client id: f_00007-6-1 loss: 0.606791  [   64/  179]
train() client id: f_00007-6-2 loss: 0.464042  [   96/  179]
train() client id: f_00007-6-3 loss: 0.450066  [  128/  179]
train() client id: f_00007-6-4 loss: 0.579159  [  160/  179]
train() client id: f_00007-7-0 loss: 0.401478  [   32/  179]
train() client id: f_00007-7-1 loss: 0.791095  [   64/  179]
train() client id: f_00007-7-2 loss: 0.579227  [   96/  179]
train() client id: f_00007-7-3 loss: 0.484048  [  128/  179]
train() client id: f_00007-7-4 loss: 0.408845  [  160/  179]
train() client id: f_00007-8-0 loss: 0.395903  [   32/  179]
train() client id: f_00007-8-1 loss: 0.439561  [   64/  179]
train() client id: f_00007-8-2 loss: 0.639398  [   96/  179]
train() client id: f_00007-8-3 loss: 0.426571  [  128/  179]
train() client id: f_00007-8-4 loss: 0.596668  [  160/  179]
train() client id: f_00008-0-0 loss: 0.665545  [   32/  130]
train() client id: f_00008-0-1 loss: 0.908756  [   64/  130]
train() client id: f_00008-0-2 loss: 0.828494  [   96/  130]
train() client id: f_00008-0-3 loss: 0.796134  [  128/  130]
train() client id: f_00008-1-0 loss: 0.762268  [   32/  130]
train() client id: f_00008-1-1 loss: 0.957263  [   64/  130]
train() client id: f_00008-1-2 loss: 0.759259  [   96/  130]
train() client id: f_00008-1-3 loss: 0.742950  [  128/  130]
train() client id: f_00008-2-0 loss: 0.896537  [   32/  130]
train() client id: f_00008-2-1 loss: 0.787640  [   64/  130]
train() client id: f_00008-2-2 loss: 0.783184  [   96/  130]
train() client id: f_00008-2-3 loss: 0.742887  [  128/  130]
train() client id: f_00008-3-0 loss: 0.795465  [   32/  130]
train() client id: f_00008-3-1 loss: 0.885328  [   64/  130]
train() client id: f_00008-3-2 loss: 0.836281  [   96/  130]
train() client id: f_00008-3-3 loss: 0.688676  [  128/  130]
train() client id: f_00008-4-0 loss: 0.711842  [   32/  130]
train() client id: f_00008-4-1 loss: 0.845564  [   64/  130]
train() client id: f_00008-4-2 loss: 0.788778  [   96/  130]
train() client id: f_00008-4-3 loss: 0.813096  [  128/  130]
train() client id: f_00008-5-0 loss: 0.895125  [   32/  130]
train() client id: f_00008-5-1 loss: 0.760821  [   64/  130]
train() client id: f_00008-5-2 loss: 0.760239  [   96/  130]
train() client id: f_00008-5-3 loss: 0.761730  [  128/  130]
train() client id: f_00008-6-0 loss: 0.755640  [   32/  130]
train() client id: f_00008-6-1 loss: 0.758098  [   64/  130]
train() client id: f_00008-6-2 loss: 0.762565  [   96/  130]
train() client id: f_00008-6-3 loss: 0.895812  [  128/  130]
train() client id: f_00008-7-0 loss: 0.877701  [   32/  130]
train() client id: f_00008-7-1 loss: 0.751204  [   64/  130]
train() client id: f_00008-7-2 loss: 0.771480  [   96/  130]
train() client id: f_00008-7-3 loss: 0.756259  [  128/  130]
train() client id: f_00008-8-0 loss: 0.806353  [   32/  130]
train() client id: f_00008-8-1 loss: 0.717617  [   64/  130]
train() client id: f_00008-8-2 loss: 0.826583  [   96/  130]
train() client id: f_00008-8-3 loss: 0.802579  [  128/  130]
train() client id: f_00009-0-0 loss: 1.352264  [   32/  118]
train() client id: f_00009-0-1 loss: 0.963634  [   64/  118]
train() client id: f_00009-0-2 loss: 1.119353  [   96/  118]
train() client id: f_00009-1-0 loss: 1.034900  [   32/  118]
train() client id: f_00009-1-1 loss: 1.212451  [   64/  118]
train() client id: f_00009-1-2 loss: 1.103974  [   96/  118]
train() client id: f_00009-2-0 loss: 1.169053  [   32/  118]
train() client id: f_00009-2-1 loss: 1.006365  [   64/  118]
train() client id: f_00009-2-2 loss: 1.046053  [   96/  118]
train() client id: f_00009-3-0 loss: 0.990441  [   32/  118]
train() client id: f_00009-3-1 loss: 1.156087  [   64/  118]
train() client id: f_00009-3-2 loss: 0.977457  [   96/  118]
train() client id: f_00009-4-0 loss: 1.055003  [   32/  118]
train() client id: f_00009-4-1 loss: 1.010944  [   64/  118]
train() client id: f_00009-4-2 loss: 0.963198  [   96/  118]
train() client id: f_00009-5-0 loss: 1.098623  [   32/  118]
train() client id: f_00009-5-1 loss: 0.988393  [   64/  118]
train() client id: f_00009-5-2 loss: 1.002753  [   96/  118]
train() client id: f_00009-6-0 loss: 1.003778  [   32/  118]
train() client id: f_00009-6-1 loss: 0.975575  [   64/  118]
train() client id: f_00009-6-2 loss: 0.977239  [   96/  118]
train() client id: f_00009-7-0 loss: 0.933176  [   32/  118]
train() client id: f_00009-7-1 loss: 1.029652  [   64/  118]
train() client id: f_00009-7-2 loss: 0.955091  [   96/  118]
train() client id: f_00009-8-0 loss: 1.107112  [   32/  118]
train() client id: f_00009-8-1 loss: 0.908897  [   64/  118]
train() client id: f_00009-8-2 loss: 0.902269  [   96/  118]
At round 65 accuracy: 0.6472148541114059
At round 65 training accuracy: 0.590878604963112
At round 65 training loss: 0.8249325864063649
update_location
xs = -3.905658 4.200318 345.009024 18.811294 0.979296 3.956410 -307.443192 -286.324852 329.663977 -272.060879 
ys = 337.587959 320.555839 1.320614 -307.455176 299.350187 282.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 352.109194 335.817939 359.211596 323.855755 315.612885 299.999162 323.308223 303.286328 344.944934 289.884690 
dists_bs = 236.025673 230.866317 548.246217 519.928663 215.480119 208.924120 221.566771 206.714966 528.649456 196.624788 
uav_gains = -119.286465 -118.344266 -119.655502 -117.557142 -116.964315 -115.730366 -117.519053 -116.001580 -118.889438 -114.863229 
bs_gains = -106.009918 -105.741155 -116.258377 -115.613484 -104.902461 -104.526739 -105.241189 -104.397473 -115.815757 -103.788931 
Round 66
-------------------------------
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.28261697 4.54685792 2.2554939  0.84667744 5.24112296 2.52309512
 1.03299146 3.13864151 2.30469629 2.04601254]
obj_prev = 26.218206122007363
eta_min = 1.468265776943342e-41	eta_max = 0.944257265961695
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 5.987676154852869	eta = 0.9090909090909091
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 15.480452461148737	eta = 0.3516267998379439
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 10.14509351030873	eta = 0.5365492149900848
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 9.229709829328627	eta = 0.5897630651031099
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 9.172211188857027	eta = 0.5934601642807859
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 9.17195028532138	eta = 0.5934770457345999
af = 5.443341958957153	bf = 0.8885079592597985	zeta = 9.17195027990423	eta = 0.5934770460851201
eta = 0.5934770460851201
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [0.04305412 0.09055032 0.04237072 0.01469307 0.10456    0.0498881
 0.01845176 0.06116418 0.04442091 0.04032052]
ene_total = [0.9382438  1.35500986 0.94553393 0.48503672 1.54222126 0.79045116
 0.53538055 1.0793304  0.84532808 0.65541452]
ti_comp = [1.69339463 1.86443235 1.68124436 1.74028295 1.86800619 1.86951147
 1.74115692 1.77160865 1.77482088 1.87231038]
ti_coms = [0.25366796 0.08263025 0.26581824 0.20677964 0.07905641 0.07755113
 0.20590568 0.17545395 0.17224172 0.07475222]
t_total = [26.69972305 26.69972305 26.69972305 26.69972305 26.69972305 26.69972305
 26.69972305 26.69972305 26.69972305 26.69972305]
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [1.73943290e-06 1.33492469e-05 1.68196045e-06 6.54603068e-08
 2.04748185e-05 2.22031557e-06 1.29514340e-07 4.55654761e-06
 1.73913543e-06 1.16869917e-06]
ene_total = [0.34787422 0.11349245 0.36453489 0.28355414 0.10868941 0.10637493
 0.28235657 0.24065936 0.23621585 0.10252241]
optimize_network iter = 0 obj = 2.186274239551174
eta = 0.5934770460851201
freqs = [12712369.24332196 24283617.19415748 12600999.04046834  4221460.42224169
 27987058.04437446 13342550.5059045   5298707.90102926 17262328.31372014
 12514196.1057277  10767584.65528894]
eta_min = 0.5934770460851205	eta_max = 0.7588338965075888
af = 0.0005034964568992424	bf = 0.8885079592597985	zeta = 0.0005538461025891666	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [3.17745128e-07 2.43852935e-06 3.07246539e-07 1.19577441e-08
 3.74016947e-06 4.05588772e-07 2.36586018e-08 8.32352201e-07
 3.17690788e-07 2.13488239e-07]
ene_total = [1.55552023 0.5068411  1.6300255  1.26798066 0.48500598 0.47557109
 1.26262219 1.07594073 1.05621167 0.45839627]
ti_comp = [0.90140949 1.07244721 0.88925922 0.94829781 1.07602105 1.07752633
 0.94917178 0.97962351 0.98283574 1.08032524]
ti_coms = [0.25366796 0.08263025 0.26581824 0.20677964 0.07905641 0.07755113
 0.20590568 0.17545395 0.17224172 0.07475222]
t_total = [26.69972305 26.69972305 26.69972305 26.69972305 26.69972305 26.69972305
 26.69972305 26.69972305 26.69972305 26.69972305]
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [9.08390094e-07 5.97022622e-06 8.89637979e-07 3.26227732e-08
 9.13119788e-06 9.89026201e-07 6.44906348e-08 2.20518843e-06
 8.39214901e-07 5.19447888e-07]
ene_total = [0.58637686 0.1911386  0.6144619  0.47797382 0.18295069 0.17928302
 0.47595438 0.40561441 0.39815774 0.17280245]
optimize_network iter = 1 obj = 3.6847138773635306
eta = 0.7588338965075888
freqs = [12631650.06371988 22329634.8077465  12600999.04046835  4097654.84679903
 25698769.41757561 12244375.3475877   5141153.96101874 16512221.31849963
 11952921.87158274  9870499.99002256]
eta_min = 0.7588338965075906	eta_max = 0.7588338965075868
af = 0.00043529367837054833	bf = 0.8885079592597985	zeta = 0.0004788230462076032	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [3.13722794e-07 2.06188515e-06 3.07246539e-07 1.12666437e-08
 3.15356247e-06 3.41571386e-07 2.22725702e-08 7.61586768e-07
 2.89832359e-07 1.79397204e-07]
ene_total = [1.55551998 0.506818   1.6300255  1.26798062 0.48497001 0.47556716
 1.2626221  1.0759364  1.05620996 0.45839418]
ti_comp = [0.90140949 1.07244721 0.88925922 0.94829781 1.07602105 1.07752633
 0.94917178 0.97962351 0.98283574 1.08032524]
ti_coms = [0.25366796 0.08263025 0.26581824 0.20677964 0.07905641 0.07755113
 0.20590568 0.17545395 0.17224172 0.07475222]
t_total = [26.69972305 26.69972305 26.69972305 26.69972305 26.69972305 26.69972305
 26.69972305 26.69972305 26.69972305 26.69972305]
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [9.08390094e-07 5.97022622e-06 8.89637979e-07 3.26227732e-08
 9.13119788e-06 9.89026201e-07 6.44906348e-08 2.20518843e-06
 8.39214901e-07 5.19447888e-07]
ene_total = [0.58637686 0.1911386  0.6144619  0.47797382 0.18295069 0.17928302
 0.47595438 0.40561441 0.39815774 0.17280245]
optimize_network iter = 2 obj = 3.6847138773635004
eta = 0.7588338965075868
freqs = [12631650.06371987 22329634.80774652 12600999.04046834  4097654.84679903
 25698769.41757564 12244375.34758771  5141153.96101873 16512221.31849963
 11952921.87158274  9870499.99002257]
Done!
ene_coms = [0.0253668  0.00826303 0.02658182 0.02067796 0.00790564 0.00775511
 0.02059057 0.01754539 0.01722417 0.00747522]
ene_comp = [9.04697167e-07 5.94595514e-06 8.86021287e-07 3.24901501e-08
 9.09407634e-06 9.85005461e-07 6.42284576e-08 2.19622356e-06
 8.35803196e-07 5.17336149e-07]
ene_total = [0.0253677  0.00826897 0.02658271 0.020678   0.00791473 0.0077561
 0.02059063 0.01754759 0.01722501 0.00747574]
At round 66 energy consumption: 0.15940718136832308
At round 66 eta: 0.7588338965075868
At round 66 a_n: 5.574576956279433
At round 66 local rounds: 9.036737525084137
At round 66 global rounds: 23.115093189100694
gradient difference: 0.6318172216415405
train() client id: f_00000-0-0 loss: 1.165244  [   32/  126]
train() client id: f_00000-0-1 loss: 1.347638  [   64/  126]
train() client id: f_00000-0-2 loss: 1.213815  [   96/  126]
train() client id: f_00000-1-0 loss: 1.263751  [   32/  126]
train() client id: f_00000-1-1 loss: 1.142466  [   64/  126]
train() client id: f_00000-1-2 loss: 1.125774  [   96/  126]
train() client id: f_00000-2-0 loss: 0.955524  [   32/  126]
train() client id: f_00000-2-1 loss: 1.118578  [   64/  126]
train() client id: f_00000-2-2 loss: 1.008156  [   96/  126]
train() client id: f_00000-3-0 loss: 0.921183  [   32/  126]
train() client id: f_00000-3-1 loss: 1.122780  [   64/  126]
train() client id: f_00000-3-2 loss: 0.935060  [   96/  126]
train() client id: f_00000-4-0 loss: 1.007721  [   32/  126]
train() client id: f_00000-4-1 loss: 0.948277  [   64/  126]
train() client id: f_00000-4-2 loss: 0.997475  [   96/  126]
train() client id: f_00000-5-0 loss: 0.902305  [   32/  126]
train() client id: f_00000-5-1 loss: 0.962988  [   64/  126]
train() client id: f_00000-5-2 loss: 1.044486  [   96/  126]
train() client id: f_00000-6-0 loss: 0.952967  [   32/  126]
train() client id: f_00000-6-1 loss: 0.891871  [   64/  126]
train() client id: f_00000-6-2 loss: 1.078451  [   96/  126]
train() client id: f_00000-7-0 loss: 0.945252  [   32/  126]
train() client id: f_00000-7-1 loss: 0.976426  [   64/  126]
train() client id: f_00000-7-2 loss: 0.769949  [   96/  126]
train() client id: f_00000-8-0 loss: 0.941567  [   32/  126]
train() client id: f_00000-8-1 loss: 0.847645  [   64/  126]
train() client id: f_00000-8-2 loss: 0.990548  [   96/  126]
train() client id: f_00001-0-0 loss: 0.581857  [   32/  265]
train() client id: f_00001-0-1 loss: 0.427592  [   64/  265]
train() client id: f_00001-0-2 loss: 0.405490  [   96/  265]
train() client id: f_00001-0-3 loss: 0.537676  [  128/  265]
train() client id: f_00001-0-4 loss: 0.461375  [  160/  265]
train() client id: f_00001-0-5 loss: 0.582844  [  192/  265]
train() client id: f_00001-0-6 loss: 0.692929  [  224/  265]
train() client id: f_00001-0-7 loss: 0.443769  [  256/  265]
train() client id: f_00001-1-0 loss: 0.461494  [   32/  265]
train() client id: f_00001-1-1 loss: 0.422489  [   64/  265]
train() client id: f_00001-1-2 loss: 0.482234  [   96/  265]
train() client id: f_00001-1-3 loss: 0.441770  [  128/  265]
train() client id: f_00001-1-4 loss: 0.552764  [  160/  265]
train() client id: f_00001-1-5 loss: 0.482438  [  192/  265]
train() client id: f_00001-1-6 loss: 0.673028  [  224/  265]
train() client id: f_00001-1-7 loss: 0.575123  [  256/  265]
train() client id: f_00001-2-0 loss: 0.509668  [   32/  265]
train() client id: f_00001-2-1 loss: 0.471826  [   64/  265]
train() client id: f_00001-2-2 loss: 0.505673  [   96/  265]
train() client id: f_00001-2-3 loss: 0.516395  [  128/  265]
train() client id: f_00001-2-4 loss: 0.571929  [  160/  265]
train() client id: f_00001-2-5 loss: 0.405981  [  192/  265]
train() client id: f_00001-2-6 loss: 0.552009  [  224/  265]
train() client id: f_00001-2-7 loss: 0.427077  [  256/  265]
train() client id: f_00001-3-0 loss: 0.448916  [   32/  265]
train() client id: f_00001-3-1 loss: 0.646353  [   64/  265]
train() client id: f_00001-3-2 loss: 0.489374  [   96/  265]
train() client id: f_00001-3-3 loss: 0.565880  [  128/  265]
train() client id: f_00001-3-4 loss: 0.447560  [  160/  265]
train() client id: f_00001-3-5 loss: 0.496782  [  192/  265]
train() client id: f_00001-3-6 loss: 0.425064  [  224/  265]
train() client id: f_00001-3-7 loss: 0.442464  [  256/  265]
train() client id: f_00001-4-0 loss: 0.497874  [   32/  265]
train() client id: f_00001-4-1 loss: 0.413285  [   64/  265]
train() client id: f_00001-4-2 loss: 0.642802  [   96/  265]
train() client id: f_00001-4-3 loss: 0.436511  [  128/  265]
train() client id: f_00001-4-4 loss: 0.515720  [  160/  265]
train() client id: f_00001-4-5 loss: 0.548814  [  192/  265]
train() client id: f_00001-4-6 loss: 0.393903  [  224/  265]
train() client id: f_00001-4-7 loss: 0.544811  [  256/  265]
train() client id: f_00001-5-0 loss: 0.545296  [   32/  265]
train() client id: f_00001-5-1 loss: 0.572302  [   64/  265]
train() client id: f_00001-5-2 loss: 0.423682  [   96/  265]
train() client id: f_00001-5-3 loss: 0.512892  [  128/  265]
train() client id: f_00001-5-4 loss: 0.541670  [  160/  265]
train() client id: f_00001-5-5 loss: 0.445876  [  192/  265]
train() client id: f_00001-5-6 loss: 0.426688  [  224/  265]
train() client id: f_00001-5-7 loss: 0.419569  [  256/  265]
train() client id: f_00001-6-0 loss: 0.406712  [   32/  265]
train() client id: f_00001-6-1 loss: 0.502042  [   64/  265]
train() client id: f_00001-6-2 loss: 0.491608  [   96/  265]
train() client id: f_00001-6-3 loss: 0.495628  [  128/  265]
train() client id: f_00001-6-4 loss: 0.543017  [  160/  265]
train() client id: f_00001-6-5 loss: 0.534405  [  192/  265]
train() client id: f_00001-6-6 loss: 0.508484  [  224/  265]
train() client id: f_00001-6-7 loss: 0.494048  [  256/  265]
train() client id: f_00001-7-0 loss: 0.460993  [   32/  265]
train() client id: f_00001-7-1 loss: 0.643244  [   64/  265]
train() client id: f_00001-7-2 loss: 0.565494  [   96/  265]
train() client id: f_00001-7-3 loss: 0.411435  [  128/  265]
train() client id: f_00001-7-4 loss: 0.471055  [  160/  265]
train() client id: f_00001-7-5 loss: 0.540710  [  192/  265]
train() client id: f_00001-7-6 loss: 0.418722  [  224/  265]
train() client id: f_00001-7-7 loss: 0.384975  [  256/  265]
train() client id: f_00001-8-0 loss: 0.398877  [   32/  265]
train() client id: f_00001-8-1 loss: 0.469911  [   64/  265]
train() client id: f_00001-8-2 loss: 0.460825  [   96/  265]
train() client id: f_00001-8-3 loss: 0.467606  [  128/  265]
train() client id: f_00001-8-4 loss: 0.396660  [  160/  265]
train() client id: f_00001-8-5 loss: 0.536910  [  192/  265]
train() client id: f_00001-8-6 loss: 0.649842  [  224/  265]
train() client id: f_00001-8-7 loss: 0.598549  [  256/  265]
train() client id: f_00002-0-0 loss: 1.039285  [   32/  124]
train() client id: f_00002-0-1 loss: 0.805364  [   64/  124]
train() client id: f_00002-0-2 loss: 1.080339  [   96/  124]
train() client id: f_00002-1-0 loss: 1.025679  [   32/  124]
train() client id: f_00002-1-1 loss: 0.984189  [   64/  124]
train() client id: f_00002-1-2 loss: 0.997701  [   96/  124]
train() client id: f_00002-2-0 loss: 1.022648  [   32/  124]
train() client id: f_00002-2-1 loss: 1.049326  [   64/  124]
train() client id: f_00002-2-2 loss: 0.991336  [   96/  124]
train() client id: f_00002-3-0 loss: 0.972272  [   32/  124]
train() client id: f_00002-3-1 loss: 0.941916  [   64/  124]
train() client id: f_00002-3-2 loss: 1.167636  [   96/  124]
train() client id: f_00002-4-0 loss: 0.942287  [   32/  124]
train() client id: f_00002-4-1 loss: 1.046386  [   64/  124]
train() client id: f_00002-4-2 loss: 0.879170  [   96/  124]
train() client id: f_00002-5-0 loss: 0.875551  [   32/  124]
train() client id: f_00002-5-1 loss: 0.956199  [   64/  124]
train() client id: f_00002-5-2 loss: 0.985449  [   96/  124]
train() client id: f_00002-6-0 loss: 1.049293  [   32/  124]
train() client id: f_00002-6-1 loss: 0.990379  [   64/  124]
train() client id: f_00002-6-2 loss: 1.029268  [   96/  124]
train() client id: f_00002-7-0 loss: 1.022076  [   32/  124]
train() client id: f_00002-7-1 loss: 1.109789  [   64/  124]
train() client id: f_00002-7-2 loss: 0.887509  [   96/  124]
train() client id: f_00002-8-0 loss: 0.977825  [   32/  124]
train() client id: f_00002-8-1 loss: 1.000300  [   64/  124]
train() client id: f_00002-8-2 loss: 1.072110  [   96/  124]
train() client id: f_00003-0-0 loss: 0.593852  [   32/   43]
train() client id: f_00003-1-0 loss: 0.549654  [   32/   43]
train() client id: f_00003-2-0 loss: 0.489979  [   32/   43]
train() client id: f_00003-3-0 loss: 0.512823  [   32/   43]
train() client id: f_00003-4-0 loss: 0.429878  [   32/   43]
train() client id: f_00003-5-0 loss: 0.597629  [   32/   43]
train() client id: f_00003-6-0 loss: 0.530378  [   32/   43]
train() client id: f_00003-7-0 loss: 0.462539  [   32/   43]
train() client id: f_00003-8-0 loss: 0.592349  [   32/   43]
train() client id: f_00004-0-0 loss: 0.845177  [   32/  306]
train() client id: f_00004-0-1 loss: 0.800776  [   64/  306]
train() client id: f_00004-0-2 loss: 0.789867  [   96/  306]
train() client id: f_00004-0-3 loss: 0.661951  [  128/  306]
train() client id: f_00004-0-4 loss: 0.865910  [  160/  306]
train() client id: f_00004-0-5 loss: 0.979706  [  192/  306]
train() client id: f_00004-0-6 loss: 0.803459  [  224/  306]
train() client id: f_00004-0-7 loss: 0.911967  [  256/  306]
train() client id: f_00004-0-8 loss: 0.780475  [  288/  306]
train() client id: f_00004-1-0 loss: 0.867937  [   32/  306]
train() client id: f_00004-1-1 loss: 0.750331  [   64/  306]
train() client id: f_00004-1-2 loss: 0.955594  [   96/  306]
train() client id: f_00004-1-3 loss: 0.859895  [  128/  306]
train() client id: f_00004-1-4 loss: 0.711981  [  160/  306]
train() client id: f_00004-1-5 loss: 0.787343  [  192/  306]
train() client id: f_00004-1-6 loss: 0.815930  [  224/  306]
train() client id: f_00004-1-7 loss: 0.897163  [  256/  306]
train() client id: f_00004-1-8 loss: 0.720714  [  288/  306]
train() client id: f_00004-2-0 loss: 0.833039  [   32/  306]
train() client id: f_00004-2-1 loss: 0.966472  [   64/  306]
train() client id: f_00004-2-2 loss: 0.677242  [   96/  306]
train() client id: f_00004-2-3 loss: 0.782941  [  128/  306]
train() client id: f_00004-2-4 loss: 0.845917  [  160/  306]
train() client id: f_00004-2-5 loss: 0.836918  [  192/  306]
train() client id: f_00004-2-6 loss: 0.808129  [  224/  306]
train() client id: f_00004-2-7 loss: 0.817683  [  256/  306]
train() client id: f_00004-2-8 loss: 0.800040  [  288/  306]
train() client id: f_00004-3-0 loss: 0.821916  [   32/  306]
train() client id: f_00004-3-1 loss: 0.827779  [   64/  306]
train() client id: f_00004-3-2 loss: 0.956554  [   96/  306]
train() client id: f_00004-3-3 loss: 0.685433  [  128/  306]
train() client id: f_00004-3-4 loss: 0.762510  [  160/  306]
train() client id: f_00004-3-5 loss: 0.930042  [  192/  306]
train() client id: f_00004-3-6 loss: 0.801899  [  224/  306]
train() client id: f_00004-3-7 loss: 0.817494  [  256/  306]
train() client id: f_00004-3-8 loss: 0.812345  [  288/  306]
train() client id: f_00004-4-0 loss: 0.846531  [   32/  306]
train() client id: f_00004-4-1 loss: 0.691946  [   64/  306]
train() client id: f_00004-4-2 loss: 0.750149  [   96/  306]
train() client id: f_00004-4-3 loss: 0.773870  [  128/  306]
train() client id: f_00004-4-4 loss: 0.826314  [  160/  306]
train() client id: f_00004-4-5 loss: 0.929822  [  192/  306]
train() client id: f_00004-4-6 loss: 0.818340  [  224/  306]
train() client id: f_00004-4-7 loss: 0.924705  [  256/  306]
train() client id: f_00004-4-8 loss: 0.864214  [  288/  306]
train() client id: f_00004-5-0 loss: 0.794561  [   32/  306]
train() client id: f_00004-5-1 loss: 0.715177  [   64/  306]
train() client id: f_00004-5-2 loss: 0.895757  [   96/  306]
train() client id: f_00004-5-3 loss: 0.744953  [  128/  306]
train() client id: f_00004-5-4 loss: 0.884938  [  160/  306]
train() client id: f_00004-5-5 loss: 0.899485  [  192/  306]
train() client id: f_00004-5-6 loss: 0.826945  [  224/  306]
train() client id: f_00004-5-7 loss: 0.765072  [  256/  306]
train() client id: f_00004-5-8 loss: 0.910142  [  288/  306]
train() client id: f_00004-6-0 loss: 0.818071  [   32/  306]
train() client id: f_00004-6-1 loss: 0.718640  [   64/  306]
train() client id: f_00004-6-2 loss: 0.769139  [   96/  306]
train() client id: f_00004-6-3 loss: 0.883863  [  128/  306]
train() client id: f_00004-6-4 loss: 0.895711  [  160/  306]
train() client id: f_00004-6-5 loss: 0.879356  [  192/  306]
train() client id: f_00004-6-6 loss: 0.783603  [  224/  306]
train() client id: f_00004-6-7 loss: 0.824263  [  256/  306]
train() client id: f_00004-6-8 loss: 0.940147  [  288/  306]
train() client id: f_00004-7-0 loss: 0.672527  [   32/  306]
train() client id: f_00004-7-1 loss: 0.855642  [   64/  306]
train() client id: f_00004-7-2 loss: 0.751956  [   96/  306]
train() client id: f_00004-7-3 loss: 0.863268  [  128/  306]
train() client id: f_00004-7-4 loss: 1.026941  [  160/  306]
train() client id: f_00004-7-5 loss: 0.917889  [  192/  306]
train() client id: f_00004-7-6 loss: 0.789435  [  224/  306]
train() client id: f_00004-7-7 loss: 0.858568  [  256/  306]
train() client id: f_00004-7-8 loss: 0.787758  [  288/  306]
train() client id: f_00004-8-0 loss: 0.764585  [   32/  306]
train() client id: f_00004-8-1 loss: 0.870681  [   64/  306]
train() client id: f_00004-8-2 loss: 0.802339  [   96/  306]
train() client id: f_00004-8-3 loss: 0.863493  [  128/  306]
train() client id: f_00004-8-4 loss: 0.993009  [  160/  306]
train() client id: f_00004-8-5 loss: 0.764378  [  192/  306]
train() client id: f_00004-8-6 loss: 0.834176  [  224/  306]
train() client id: f_00004-8-7 loss: 0.823856  [  256/  306]
train() client id: f_00004-8-8 loss: 0.873444  [  288/  306]
train() client id: f_00005-0-0 loss: 0.398253  [   32/  146]
train() client id: f_00005-0-1 loss: 0.307371  [   64/  146]
train() client id: f_00005-0-2 loss: 0.580839  [   96/  146]
train() client id: f_00005-0-3 loss: 0.208104  [  128/  146]
train() client id: f_00005-1-0 loss: 0.432321  [   32/  146]
train() client id: f_00005-1-1 loss: 0.457776  [   64/  146]
train() client id: f_00005-1-2 loss: 0.349451  [   96/  146]
train() client id: f_00005-1-3 loss: 0.162523  [  128/  146]
train() client id: f_00005-2-0 loss: 0.698559  [   32/  146]
train() client id: f_00005-2-1 loss: 0.207800  [   64/  146]
train() client id: f_00005-2-2 loss: 0.251013  [   96/  146]
train() client id: f_00005-2-3 loss: 0.418190  [  128/  146]
train() client id: f_00005-3-0 loss: 0.295025  [   32/  146]
train() client id: f_00005-3-1 loss: 0.470506  [   64/  146]
train() client id: f_00005-3-2 loss: 0.339222  [   96/  146]
train() client id: f_00005-3-3 loss: 0.302765  [  128/  146]
train() client id: f_00005-4-0 loss: 0.398861  [   32/  146]
train() client id: f_00005-4-1 loss: 0.281644  [   64/  146]
train() client id: f_00005-4-2 loss: 0.190343  [   96/  146]
train() client id: f_00005-4-3 loss: 0.357372  [  128/  146]
train() client id: f_00005-5-0 loss: 0.395222  [   32/  146]
train() client id: f_00005-5-1 loss: 0.043606  [   64/  146]
train() client id: f_00005-5-2 loss: 0.486099  [   96/  146]
train() client id: f_00005-5-3 loss: 0.603438  [  128/  146]
train() client id: f_00005-6-0 loss: 0.350483  [   32/  146]
train() client id: f_00005-6-1 loss: 0.534057  [   64/  146]
train() client id: f_00005-6-2 loss: 0.169872  [   96/  146]
train() client id: f_00005-6-3 loss: 0.091889  [  128/  146]
train() client id: f_00005-7-0 loss: 0.472361  [   32/  146]
train() client id: f_00005-7-1 loss: 0.265640  [   64/  146]
train() client id: f_00005-7-2 loss: 0.387684  [   96/  146]
train() client id: f_00005-7-3 loss: 0.156215  [  128/  146]
train() client id: f_00005-8-0 loss: 0.330308  [   32/  146]
train() client id: f_00005-8-1 loss: 0.196055  [   64/  146]
train() client id: f_00005-8-2 loss: 0.508592  [   96/  146]
train() client id: f_00005-8-3 loss: 0.359200  [  128/  146]
train() client id: f_00006-0-0 loss: 0.492667  [   32/   54]
train() client id: f_00006-1-0 loss: 0.422720  [   32/   54]
train() client id: f_00006-2-0 loss: 0.441536  [   32/   54]
train() client id: f_00006-3-0 loss: 0.494375  [   32/   54]
train() client id: f_00006-4-0 loss: 0.453084  [   32/   54]
train() client id: f_00006-5-0 loss: 0.461969  [   32/   54]
train() client id: f_00006-6-0 loss: 0.397667  [   32/   54]
train() client id: f_00006-7-0 loss: 0.493908  [   32/   54]
train() client id: f_00006-8-0 loss: 0.479096  [   32/   54]
train() client id: f_00007-0-0 loss: 0.543938  [   32/  179]
train() client id: f_00007-0-1 loss: 0.786778  [   64/  179]
train() client id: f_00007-0-2 loss: 0.531901  [   96/  179]
train() client id: f_00007-0-3 loss: 0.409138  [  128/  179]
train() client id: f_00007-0-4 loss: 0.428458  [  160/  179]
train() client id: f_00007-1-0 loss: 0.414689  [   32/  179]
train() client id: f_00007-1-1 loss: 0.556184  [   64/  179]
train() client id: f_00007-1-2 loss: 0.496811  [   96/  179]
train() client id: f_00007-1-3 loss: 0.399603  [  128/  179]
train() client id: f_00007-1-4 loss: 0.772790  [  160/  179]
train() client id: f_00007-2-0 loss: 0.501084  [   32/  179]
train() client id: f_00007-2-1 loss: 0.356847  [   64/  179]
train() client id: f_00007-2-2 loss: 0.369356  [   96/  179]
train() client id: f_00007-2-3 loss: 0.543504  [  128/  179]
train() client id: f_00007-2-4 loss: 0.656420  [  160/  179]
train() client id: f_00007-3-0 loss: 0.595361  [   32/  179]
train() client id: f_00007-3-1 loss: 0.524449  [   64/  179]
train() client id: f_00007-3-2 loss: 0.451332  [   96/  179]
train() client id: f_00007-3-3 loss: 0.710415  [  128/  179]
train() client id: f_00007-3-4 loss: 0.379986  [  160/  179]
train() client id: f_00007-4-0 loss: 0.520579  [   32/  179]
train() client id: f_00007-4-1 loss: 0.385258  [   64/  179]
train() client id: f_00007-4-2 loss: 0.592940  [   96/  179]
train() client id: f_00007-4-3 loss: 0.477213  [  128/  179]
train() client id: f_00007-4-4 loss: 0.469561  [  160/  179]
train() client id: f_00007-5-0 loss: 0.537956  [   32/  179]
train() client id: f_00007-5-1 loss: 0.499660  [   64/  179]
train() client id: f_00007-5-2 loss: 0.397295  [   96/  179]
train() client id: f_00007-5-3 loss: 0.581500  [  128/  179]
train() client id: f_00007-5-4 loss: 0.483780  [  160/  179]
train() client id: f_00007-6-0 loss: 0.474039  [   32/  179]
train() client id: f_00007-6-1 loss: 0.471508  [   64/  179]
train() client id: f_00007-6-2 loss: 0.648928  [   96/  179]
train() client id: f_00007-6-3 loss: 0.585354  [  128/  179]
train() client id: f_00007-6-4 loss: 0.464364  [  160/  179]
train() client id: f_00007-7-0 loss: 0.686644  [   32/  179]
train() client id: f_00007-7-1 loss: 0.480052  [   64/  179]
train() client id: f_00007-7-2 loss: 0.522762  [   96/  179]
train() client id: f_00007-7-3 loss: 0.501343  [  128/  179]
train() client id: f_00007-7-4 loss: 0.459000  [  160/  179]
train() client id: f_00007-8-0 loss: 0.482335  [   32/  179]
train() client id: f_00007-8-1 loss: 0.505473  [   64/  179]
train() client id: f_00007-8-2 loss: 0.342516  [   96/  179]
train() client id: f_00007-8-3 loss: 0.521710  [  128/  179]
train() client id: f_00007-8-4 loss: 0.709897  [  160/  179]
train() client id: f_00008-0-0 loss: 0.710829  [   32/  130]
train() client id: f_00008-0-1 loss: 0.618966  [   64/  130]
train() client id: f_00008-0-2 loss: 0.749147  [   96/  130]
train() client id: f_00008-0-3 loss: 0.740368  [  128/  130]
train() client id: f_00008-1-0 loss: 0.645256  [   32/  130]
train() client id: f_00008-1-1 loss: 0.744463  [   64/  130]
train() client id: f_00008-1-2 loss: 0.772257  [   96/  130]
train() client id: f_00008-1-3 loss: 0.696323  [  128/  130]
train() client id: f_00008-2-0 loss: 0.743104  [   32/  130]
train() client id: f_00008-2-1 loss: 0.665493  [   64/  130]
train() client id: f_00008-2-2 loss: 0.739865  [   96/  130]
train() client id: f_00008-2-3 loss: 0.714783  [  128/  130]
train() client id: f_00008-3-0 loss: 0.725215  [   32/  130]
train() client id: f_00008-3-1 loss: 0.645478  [   64/  130]
train() client id: f_00008-3-2 loss: 0.613015  [   96/  130]
train() client id: f_00008-3-3 loss: 0.865354  [  128/  130]
train() client id: f_00008-4-0 loss: 0.675146  [   32/  130]
train() client id: f_00008-4-1 loss: 0.738763  [   64/  130]
train() client id: f_00008-4-2 loss: 0.679339  [   96/  130]
train() client id: f_00008-4-3 loss: 0.762448  [  128/  130]
train() client id: f_00008-5-0 loss: 0.700559  [   32/  130]
train() client id: f_00008-5-1 loss: 0.731822  [   64/  130]
train() client id: f_00008-5-2 loss: 0.617451  [   96/  130]
train() client id: f_00008-5-3 loss: 0.797332  [  128/  130]
train() client id: f_00008-6-0 loss: 0.719638  [   32/  130]
train() client id: f_00008-6-1 loss: 0.710390  [   64/  130]
train() client id: f_00008-6-2 loss: 0.612279  [   96/  130]
train() client id: f_00008-6-3 loss: 0.813828  [  128/  130]
train() client id: f_00008-7-0 loss: 0.698315  [   32/  130]
train() client id: f_00008-7-1 loss: 0.753148  [   64/  130]
train() client id: f_00008-7-2 loss: 0.664871  [   96/  130]
train() client id: f_00008-7-3 loss: 0.683130  [  128/  130]
train() client id: f_00008-8-0 loss: 0.623500  [   32/  130]
train() client id: f_00008-8-1 loss: 0.748328  [   64/  130]
train() client id: f_00008-8-2 loss: 0.752407  [   96/  130]
train() client id: f_00008-8-3 loss: 0.689049  [  128/  130]
train() client id: f_00009-0-0 loss: 1.024886  [   32/  118]
train() client id: f_00009-0-1 loss: 1.114634  [   64/  118]
train() client id: f_00009-0-2 loss: 0.933547  [   96/  118]
train() client id: f_00009-1-0 loss: 0.931144  [   32/  118]
train() client id: f_00009-1-1 loss: 0.873487  [   64/  118]
train() client id: f_00009-1-2 loss: 1.040642  [   96/  118]
train() client id: f_00009-2-0 loss: 0.956531  [   32/  118]
train() client id: f_00009-2-1 loss: 0.865857  [   64/  118]
train() client id: f_00009-2-2 loss: 0.949439  [   96/  118]
train() client id: f_00009-3-0 loss: 0.979883  [   32/  118]
train() client id: f_00009-3-1 loss: 1.057295  [   64/  118]
train() client id: f_00009-3-2 loss: 0.868373  [   96/  118]
train() client id: f_00009-4-0 loss: 0.963301  [   32/  118]
train() client id: f_00009-4-1 loss: 0.910226  [   64/  118]
train() client id: f_00009-4-2 loss: 0.835519  [   96/  118]
train() client id: f_00009-5-0 loss: 0.994050  [   32/  118]
train() client id: f_00009-5-1 loss: 0.701647  [   64/  118]
train() client id: f_00009-5-2 loss: 0.898604  [   96/  118]
train() client id: f_00009-6-0 loss: 0.724877  [   32/  118]
train() client id: f_00009-6-1 loss: 0.996613  [   64/  118]
train() client id: f_00009-6-2 loss: 0.880776  [   96/  118]
train() client id: f_00009-7-0 loss: 0.823664  [   32/  118]
train() client id: f_00009-7-1 loss: 0.786319  [   64/  118]
train() client id: f_00009-7-2 loss: 0.921011  [   96/  118]
train() client id: f_00009-8-0 loss: 0.953476  [   32/  118]
train() client id: f_00009-8-1 loss: 0.717317  [   64/  118]
train() client id: f_00009-8-2 loss: 0.771690  [   96/  118]
At round 66 accuracy: 0.6472148541114059
At round 66 training accuracy: 0.5861837692823608
At round 66 training loss: 0.8296295269565868
update_location
xs = -3.905658 4.200318 350.009024 18.811294 0.979296 3.956410 -312.443192 -291.324852 334.663977 -277.060879 
ys = 342.587959 325.555839 1.320614 -312.455176 304.350187 287.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 356.905820 340.593962 364.016567 328.606302 320.359166 304.717309 328.066516 308.011113 349.726532 294.582318 
dists_bs = 239.497386 234.050880 552.990962 524.571603 218.403717 211.547700 224.591331 209.450055 533.424678 199.140443 
uav_gains = -119.538245 -118.635267 -119.892382 -117.879958 -117.310750 -116.117831 -117.843965 -116.381057 -119.157291 -115.271610 
bs_gains = -106.187481 -105.907747 -116.363164 -115.721593 -105.066340 -104.678492 -105.406063 -104.557313 -115.925106 -103.943524 
Round 67
-------------------------------
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.14668611 4.26785824 2.12126966 0.79869584 4.91942362 2.36837405
 0.97356032 2.94941557 2.16407019 1.92059068]
obj_prev = 24.629944275071566
eta_min = 3.445448803028318e-44	eta_max = 0.9459678027745403
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 5.6197462444887005	eta = 0.9090909090909091
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 14.777404826319295	eta = 0.34572107093955495
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 9.602497695143875	eta = 0.5320345168992939
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 8.719456153128892	eta = 0.585915008062652
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 8.663852506476184	eta = 0.5896753457475884
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 8.663598265580141	eta = 0.5896926502882287
af = 5.108860222262455	bf = 0.8564740523773019	zeta = 8.66359826021894	eta = 0.5896926506531418
eta = 0.5896926506531418
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [0.04358199 0.09166053 0.04289021 0.01487322 0.10584197 0.05049976
 0.01867799 0.06191409 0.04496554 0.04081488]
ene_total = [0.88965002 1.27512756 0.89641868 0.46309031 1.45131187 0.74360323
 0.51049018 1.02200221 0.79540655 0.61649765]
ti_comp = [1.82806619 2.00655166 1.81584028 1.87549598 2.01019846 2.01177704
 1.87637173 1.90759375 1.91580954 2.01460717]
ti_coms = [0.26186319 0.08337772 0.27408909 0.2144334  0.07973092 0.07815233
 0.21355765 0.18233562 0.17411984 0.07532221]
t_total = [26.64971886 26.64971886 26.64971886 26.64971886 26.64971886 26.64971886
 26.64971886 26.64971886 26.64971886 26.64971886]
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [1.54816501e-06 1.19543613e-05 1.49554405e-06 5.84605015e-08
 1.83390085e-05 1.98878720e-06 1.15673257e-07 4.07639963e-06
 1.54815642e-06 1.04701937e-06]
ene_total = [0.33393439 0.10647147 0.34952357 0.27343533 0.10190267 0.09968125
 0.27231935 0.23255711 0.22204852 0.09606041]
optimize_network iter = 0 obj = 2.0879340736516
eta = 0.5896926506531418
freqs = [11920243.00940963 22840310.78845544 11810016.9072595   3965142.48672103
 26326248.17150466 12551033.48678927  4977157.22105609 16228322.10506473
 11735389.30408466 10129735.66511689]
eta_min = 0.589692650653142	eta_max = 0.7681327803953306
af = 0.00041776276143721885	bf = 0.8564740523773019	zeta = 0.00045953903758094077	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [2.79380519e-07 2.15727370e-06 2.69884586e-07 1.05497316e-08
 3.30944163e-06 3.58894819e-07 2.08742959e-08 7.35623554e-07
 2.79378970e-07 1.88944211e-07]
ene_total = [1.50710005 0.47998245 1.57746246 1.23411502 0.45906057 0.44980564
 1.22907549 1.04942654 1.00211652 0.43350782]
ti_comp = [0.91916882 1.09765429 0.90694292 0.96659861 1.10130109 1.10287968
 0.96747436 0.99869639 1.00691217 1.1057098 ]
ti_coms = [0.26186319 0.08337772 0.27408909 0.2144334  0.07973092 0.07815233
 0.21355765 0.18233562 0.17411984 0.07532221]
t_total = [26.64971886 26.64971886 26.64971886 26.64971886 26.64971886 26.64971886
 26.64971886 26.64971886 26.64971886 26.64971886]
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [7.62987513e-07 4.97740325e-06 7.46967171e-07 2.74226484e-08
 7.61287159e-06 8.24515199e-07 5.42124102e-08 1.85305581e-06
 6.98301926e-07 4.33071962e-07]
ene_total = [0.59090555 0.18825224 0.61849267 0.4838647  0.18008278 0.17636757
 0.4818892  0.411478   0.39291321 0.16997262]
optimize_network iter = 1 obj = 3.694218537284266
eta = 0.7681327803953306
freqs = [11840881.82931954 20853986.15252445 11810016.9072595   3842652.168879
 24000712.97449805 11434929.97175277  4821288.08222465 15482045.45566182
 11152199.96866208  9218274.46340382]
eta_min = 0.7681327803953335	eta_max = 0.7681327803953281
af = 0.0003568274848523184	bf = 0.8564740523773019	zeta = 0.00039251023333755023	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [2.75672850e-07 1.79837142e-06 2.69884586e-07 9.90799918e-09
 2.75058500e-06 2.97903243e-07 1.95873319e-08 6.69522327e-07
 2.52301484e-07 1.56472000e-07]
ene_total = [1.50709984 0.47996179 1.57746246 1.23411499 0.4590284  0.44980213
 1.22907542 1.04942273 1.00211496 0.43350595]
ti_comp = [0.91916882 1.09765429 0.90694292 0.96659861 1.10130109 1.10287968
 0.96747436 0.99869639 1.00691217 1.1057098 ]
ti_coms = [0.26186319 0.08337772 0.27408909 0.2144334  0.07973092 0.07815233
 0.21355765 0.18233562 0.17411984 0.07532221]
t_total = [26.64971886 26.64971886 26.64971886 26.64971886 26.64971886 26.64971886
 26.64971886 26.64971886 26.64971886 26.64971886]
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [7.62987513e-07 4.97740325e-06 7.46967171e-07 2.74226484e-08
 7.61287159e-06 8.24515199e-07 5.42124102e-08 1.85305581e-06
 6.98301926e-07 4.33071962e-07]
ene_total = [0.59090555 0.18825224 0.61849267 0.4838647  0.18008278 0.17636757
 0.4818892  0.411478   0.39291321 0.16997262]
optimize_network iter = 2 obj = 3.6942185372842253
eta = 0.7681327803953281
freqs = [11840881.82931953 20853986.15252446 11810016.90725948  3842652.168879
 24000712.97449807 11434929.97175278  4821288.08222465 15482045.45566182
 11152199.96866208  9218274.46340384]
Done!
ene_coms = [0.02618632 0.00833777 0.02740891 0.02144334 0.00797309 0.00781523
 0.02135576 0.01823356 0.01741198 0.00753222]
ene_comp = [7.06640672e-07 4.60982063e-06 6.91803437e-07 2.53974782e-08
 7.05065889e-06 7.63624521e-07 5.02088085e-08 1.71620712e-06
 6.46732134e-07 4.01089477e-07]
ene_total = [0.02618703 0.00834238 0.0274096  0.02144336 0.00798014 0.007816
 0.02135582 0.01823528 0.01741263 0.00753262]
At round 67 energy consumption: 0.16371485787111706
At round 67 eta: 0.7681327803953281
At round 67 a_n: 5.2320311093101175
At round 67 local rounds: 8.637912231455728
At round 67 global rounds: 22.564772710133866
gradient difference: 0.5381776094436646
train() client id: f_00000-0-0 loss: 1.057749  [   32/  126]
train() client id: f_00000-0-1 loss: 0.879049  [   64/  126]
train() client id: f_00000-0-2 loss: 0.830719  [   96/  126]
train() client id: f_00000-1-0 loss: 0.777224  [   32/  126]
train() client id: f_00000-1-1 loss: 0.909503  [   64/  126]
train() client id: f_00000-1-2 loss: 1.039982  [   96/  126]
train() client id: f_00000-2-0 loss: 1.015802  [   32/  126]
train() client id: f_00000-2-1 loss: 0.667992  [   64/  126]
train() client id: f_00000-2-2 loss: 0.893421  [   96/  126]
train() client id: f_00000-3-0 loss: 0.847942  [   32/  126]
train() client id: f_00000-3-1 loss: 0.751540  [   64/  126]
train() client id: f_00000-3-2 loss: 0.863082  [   96/  126]
train() client id: f_00000-4-0 loss: 0.718985  [   32/  126]
train() client id: f_00000-4-1 loss: 0.766744  [   64/  126]
train() client id: f_00000-4-2 loss: 0.949410  [   96/  126]
train() client id: f_00000-5-0 loss: 0.728184  [   32/  126]
train() client id: f_00000-5-1 loss: 0.924564  [   64/  126]
train() client id: f_00000-5-2 loss: 0.718667  [   96/  126]
train() client id: f_00000-6-0 loss: 0.953983  [   32/  126]
train() client id: f_00000-6-1 loss: 0.702076  [   64/  126]
train() client id: f_00000-6-2 loss: 0.802674  [   96/  126]
train() client id: f_00000-7-0 loss: 0.811437  [   32/  126]
train() client id: f_00000-7-1 loss: 0.752534  [   64/  126]
train() client id: f_00000-7-2 loss: 0.907496  [   96/  126]
train() client id: f_00001-0-0 loss: 0.489875  [   32/  265]
train() client id: f_00001-0-1 loss: 0.370298  [   64/  265]
train() client id: f_00001-0-2 loss: 0.439578  [   96/  265]
train() client id: f_00001-0-3 loss: 0.327699  [  128/  265]
train() client id: f_00001-0-4 loss: 0.387699  [  160/  265]
train() client id: f_00001-0-5 loss: 0.386915  [  192/  265]
train() client id: f_00001-0-6 loss: 0.419580  [  224/  265]
train() client id: f_00001-0-7 loss: 0.399260  [  256/  265]
train() client id: f_00001-1-0 loss: 0.431396  [   32/  265]
train() client id: f_00001-1-1 loss: 0.473904  [   64/  265]
train() client id: f_00001-1-2 loss: 0.450013  [   96/  265]
train() client id: f_00001-1-3 loss: 0.314336  [  128/  265]
train() client id: f_00001-1-4 loss: 0.413059  [  160/  265]
train() client id: f_00001-1-5 loss: 0.321138  [  192/  265]
train() client id: f_00001-1-6 loss: 0.357868  [  224/  265]
train() client id: f_00001-1-7 loss: 0.334985  [  256/  265]
train() client id: f_00001-2-0 loss: 0.331419  [   32/  265]
train() client id: f_00001-2-1 loss: 0.571922  [   64/  265]
train() client id: f_00001-2-2 loss: 0.494622  [   96/  265]
train() client id: f_00001-2-3 loss: 0.317528  [  128/  265]
train() client id: f_00001-2-4 loss: 0.386266  [  160/  265]
train() client id: f_00001-2-5 loss: 0.295039  [  192/  265]
train() client id: f_00001-2-6 loss: 0.313121  [  224/  265]
train() client id: f_00001-2-7 loss: 0.367786  [  256/  265]
train() client id: f_00001-3-0 loss: 0.381675  [   32/  265]
train() client id: f_00001-3-1 loss: 0.273006  [   64/  265]
train() client id: f_00001-3-2 loss: 0.317863  [   96/  265]
train() client id: f_00001-3-3 loss: 0.404164  [  128/  265]
train() client id: f_00001-3-4 loss: 0.283852  [  160/  265]
train() client id: f_00001-3-5 loss: 0.553637  [  192/  265]
train() client id: f_00001-3-6 loss: 0.353946  [  224/  265]
train() client id: f_00001-3-7 loss: 0.454098  [  256/  265]
train() client id: f_00001-4-0 loss: 0.333394  [   32/  265]
train() client id: f_00001-4-1 loss: 0.445961  [   64/  265]
train() client id: f_00001-4-2 loss: 0.369547  [   96/  265]
train() client id: f_00001-4-3 loss: 0.365464  [  128/  265]
train() client id: f_00001-4-4 loss: 0.444466  [  160/  265]
train() client id: f_00001-4-5 loss: 0.300255  [  192/  265]
train() client id: f_00001-4-6 loss: 0.275906  [  224/  265]
train() client id: f_00001-4-7 loss: 0.481400  [  256/  265]
train() client id: f_00001-5-0 loss: 0.300501  [   32/  265]
train() client id: f_00001-5-1 loss: 0.504351  [   64/  265]
train() client id: f_00001-5-2 loss: 0.420753  [   96/  265]
train() client id: f_00001-5-3 loss: 0.302160  [  128/  265]
train() client id: f_00001-5-4 loss: 0.444186  [  160/  265]
train() client id: f_00001-5-5 loss: 0.252991  [  192/  265]
train() client id: f_00001-5-6 loss: 0.428299  [  224/  265]
train() client id: f_00001-5-7 loss: 0.302134  [  256/  265]
train() client id: f_00001-6-0 loss: 0.409430  [   32/  265]
train() client id: f_00001-6-1 loss: 0.366811  [   64/  265]
train() client id: f_00001-6-2 loss: 0.269641  [   96/  265]
train() client id: f_00001-6-3 loss: 0.478255  [  128/  265]
train() client id: f_00001-6-4 loss: 0.392830  [  160/  265]
train() client id: f_00001-6-5 loss: 0.351690  [  192/  265]
train() client id: f_00001-6-6 loss: 0.275917  [  224/  265]
train() client id: f_00001-6-7 loss: 0.420146  [  256/  265]
train() client id: f_00001-7-0 loss: 0.411444  [   32/  265]
train() client id: f_00001-7-1 loss: 0.400025  [   64/  265]
train() client id: f_00001-7-2 loss: 0.283662  [   96/  265]
train() client id: f_00001-7-3 loss: 0.395470  [  128/  265]
train() client id: f_00001-7-4 loss: 0.271687  [  160/  265]
train() client id: f_00001-7-5 loss: 0.355590  [  192/  265]
train() client id: f_00001-7-6 loss: 0.497222  [  224/  265]
train() client id: f_00001-7-7 loss: 0.336490  [  256/  265]
train() client id: f_00002-0-0 loss: 1.180456  [   32/  124]
train() client id: f_00002-0-1 loss: 0.980798  [   64/  124]
train() client id: f_00002-0-2 loss: 1.015567  [   96/  124]
train() client id: f_00002-1-0 loss: 0.903626  [   32/  124]
train() client id: f_00002-1-1 loss: 1.039564  [   64/  124]
train() client id: f_00002-1-2 loss: 1.140743  [   96/  124]
train() client id: f_00002-2-0 loss: 0.998124  [   32/  124]
train() client id: f_00002-2-1 loss: 0.991019  [   64/  124]
train() client id: f_00002-2-2 loss: 0.762783  [   96/  124]
train() client id: f_00002-3-0 loss: 1.031413  [   32/  124]
train() client id: f_00002-3-1 loss: 1.043433  [   64/  124]
train() client id: f_00002-3-2 loss: 1.003229  [   96/  124]
train() client id: f_00002-4-0 loss: 1.026516  [   32/  124]
train() client id: f_00002-4-1 loss: 0.995828  [   64/  124]
train() client id: f_00002-4-2 loss: 0.872804  [   96/  124]
train() client id: f_00002-5-0 loss: 0.818097  [   32/  124]
train() client id: f_00002-5-1 loss: 0.897086  [   64/  124]
train() client id: f_00002-5-2 loss: 1.175973  [   96/  124]
train() client id: f_00002-6-0 loss: 0.997872  [   32/  124]
train() client id: f_00002-6-1 loss: 0.755726  [   64/  124]
train() client id: f_00002-6-2 loss: 0.892278  [   96/  124]
train() client id: f_00002-7-0 loss: 1.009055  [   32/  124]
train() client id: f_00002-7-1 loss: 0.826567  [   64/  124]
train() client id: f_00002-7-2 loss: 0.845979  [   96/  124]
train() client id: f_00003-0-0 loss: 0.466030  [   32/   43]
train() client id: f_00003-1-0 loss: 0.540287  [   32/   43]
train() client id: f_00003-2-0 loss: 0.708725  [   32/   43]
train() client id: f_00003-3-0 loss: 0.671315  [   32/   43]
train() client id: f_00003-4-0 loss: 0.640499  [   32/   43]
train() client id: f_00003-5-0 loss: 0.504809  [   32/   43]
train() client id: f_00003-6-0 loss: 0.672090  [   32/   43]
train() client id: f_00003-7-0 loss: 0.817795  [   32/   43]
train() client id: f_00004-0-0 loss: 0.619983  [   32/  306]
train() client id: f_00004-0-1 loss: 0.936201  [   64/  306]
train() client id: f_00004-0-2 loss: 0.789739  [   96/  306]
train() client id: f_00004-0-3 loss: 0.810807  [  128/  306]
train() client id: f_00004-0-4 loss: 0.661305  [  160/  306]
train() client id: f_00004-0-5 loss: 0.652405  [  192/  306]
train() client id: f_00004-0-6 loss: 0.662047  [  224/  306]
train() client id: f_00004-0-7 loss: 0.757806  [  256/  306]
train() client id: f_00004-0-8 loss: 0.719593  [  288/  306]
train() client id: f_00004-1-0 loss: 0.927812  [   32/  306]
train() client id: f_00004-1-1 loss: 0.767624  [   64/  306]
train() client id: f_00004-1-2 loss: 0.780014  [   96/  306]
train() client id: f_00004-1-3 loss: 0.587189  [  128/  306]
train() client id: f_00004-1-4 loss: 0.831898  [  160/  306]
train() client id: f_00004-1-5 loss: 0.672385  [  192/  306]
train() client id: f_00004-1-6 loss: 0.590451  [  224/  306]
train() client id: f_00004-1-7 loss: 0.747438  [  256/  306]
train() client id: f_00004-1-8 loss: 0.644345  [  288/  306]
train() client id: f_00004-2-0 loss: 0.708956  [   32/  306]
train() client id: f_00004-2-1 loss: 0.910398  [   64/  306]
train() client id: f_00004-2-2 loss: 0.726004  [   96/  306]
train() client id: f_00004-2-3 loss: 0.783945  [  128/  306]
train() client id: f_00004-2-4 loss: 0.611430  [  160/  306]
train() client id: f_00004-2-5 loss: 0.714290  [  192/  306]
train() client id: f_00004-2-6 loss: 0.714955  [  224/  306]
train() client id: f_00004-2-7 loss: 0.654686  [  256/  306]
train() client id: f_00004-2-8 loss: 0.676584  [  288/  306]
train() client id: f_00004-3-0 loss: 0.905900  [   32/  306]
train() client id: f_00004-3-1 loss: 0.693676  [   64/  306]
train() client id: f_00004-3-2 loss: 0.730095  [   96/  306]
train() client id: f_00004-3-3 loss: 0.747499  [  128/  306]
train() client id: f_00004-3-4 loss: 0.644854  [  160/  306]
train() client id: f_00004-3-5 loss: 0.593804  [  192/  306]
train() client id: f_00004-3-6 loss: 0.624417  [  224/  306]
train() client id: f_00004-3-7 loss: 0.735540  [  256/  306]
train() client id: f_00004-3-8 loss: 0.752636  [  288/  306]
train() client id: f_00004-4-0 loss: 0.838632  [   32/  306]
train() client id: f_00004-4-1 loss: 0.701858  [   64/  306]
train() client id: f_00004-4-2 loss: 0.584505  [   96/  306]
train() client id: f_00004-4-3 loss: 0.670475  [  128/  306]
train() client id: f_00004-4-4 loss: 0.841355  [  160/  306]
train() client id: f_00004-4-5 loss: 0.656547  [  192/  306]
train() client id: f_00004-4-6 loss: 0.662221  [  224/  306]
train() client id: f_00004-4-7 loss: 0.735220  [  256/  306]
train() client id: f_00004-4-8 loss: 0.871900  [  288/  306]
train() client id: f_00004-5-0 loss: 0.769156  [   32/  306]
train() client id: f_00004-5-1 loss: 0.624598  [   64/  306]
train() client id: f_00004-5-2 loss: 0.730422  [   96/  306]
train() client id: f_00004-5-3 loss: 0.752527  [  128/  306]
train() client id: f_00004-5-4 loss: 0.751896  [  160/  306]
train() client id: f_00004-5-5 loss: 0.582765  [  192/  306]
train() client id: f_00004-5-6 loss: 0.677712  [  224/  306]
train() client id: f_00004-5-7 loss: 0.674105  [  256/  306]
train() client id: f_00004-5-8 loss: 0.892607  [  288/  306]
train() client id: f_00004-6-0 loss: 0.829379  [   32/  306]
train() client id: f_00004-6-1 loss: 0.689114  [   64/  306]
train() client id: f_00004-6-2 loss: 0.783637  [   96/  306]
train() client id: f_00004-6-3 loss: 0.695479  [  128/  306]
train() client id: f_00004-6-4 loss: 0.626865  [  160/  306]
train() client id: f_00004-6-5 loss: 0.748018  [  192/  306]
train() client id: f_00004-6-6 loss: 0.708709  [  224/  306]
train() client id: f_00004-6-7 loss: 0.775541  [  256/  306]
train() client id: f_00004-6-8 loss: 0.786428  [  288/  306]
train() client id: f_00004-7-0 loss: 0.798346  [   32/  306]
train() client id: f_00004-7-1 loss: 0.718139  [   64/  306]
train() client id: f_00004-7-2 loss: 0.767616  [   96/  306]
train() client id: f_00004-7-3 loss: 0.666203  [  128/  306]
train() client id: f_00004-7-4 loss: 0.625923  [  160/  306]
train() client id: f_00004-7-5 loss: 0.727564  [  192/  306]
train() client id: f_00004-7-6 loss: 0.812078  [  224/  306]
train() client id: f_00004-7-7 loss: 0.690286  [  256/  306]
train() client id: f_00004-7-8 loss: 0.775231  [  288/  306]
train() client id: f_00005-0-0 loss: 0.658921  [   32/  146]
train() client id: f_00005-0-1 loss: 0.636888  [   64/  146]
train() client id: f_00005-0-2 loss: 0.584802  [   96/  146]
train() client id: f_00005-0-3 loss: 0.518815  [  128/  146]
train() client id: f_00005-1-0 loss: 0.595192  [   32/  146]
train() client id: f_00005-1-1 loss: 0.641186  [   64/  146]
train() client id: f_00005-1-2 loss: 0.787661  [   96/  146]
train() client id: f_00005-1-3 loss: 0.412263  [  128/  146]
train() client id: f_00005-2-0 loss: 0.507973  [   32/  146]
train() client id: f_00005-2-1 loss: 0.520288  [   64/  146]
train() client id: f_00005-2-2 loss: 0.798111  [   96/  146]
train() client id: f_00005-2-3 loss: 0.594053  [  128/  146]
train() client id: f_00005-3-0 loss: 0.865511  [   32/  146]
train() client id: f_00005-3-1 loss: 0.557266  [   64/  146]
train() client id: f_00005-3-2 loss: 0.306766  [   96/  146]
train() client id: f_00005-3-3 loss: 0.770986  [  128/  146]
train() client id: f_00005-4-0 loss: 0.516975  [   32/  146]
train() client id: f_00005-4-1 loss: 0.483486  [   64/  146]
train() client id: f_00005-4-2 loss: 0.686106  [   96/  146]
train() client id: f_00005-4-3 loss: 0.491150  [  128/  146]
train() client id: f_00005-5-0 loss: 0.691643  [   32/  146]
train() client id: f_00005-5-1 loss: 0.677395  [   64/  146]
train() client id: f_00005-5-2 loss: 0.529304  [   96/  146]
train() client id: f_00005-5-3 loss: 0.640135  [  128/  146]
train() client id: f_00005-6-0 loss: 0.577244  [   32/  146]
train() client id: f_00005-6-1 loss: 0.603560  [   64/  146]
train() client id: f_00005-6-2 loss: 0.465707  [   96/  146]
train() client id: f_00005-6-3 loss: 0.677517  [  128/  146]
train() client id: f_00005-7-0 loss: 0.727465  [   32/  146]
train() client id: f_00005-7-1 loss: 0.570410  [   64/  146]
train() client id: f_00005-7-2 loss: 0.487125  [   96/  146]
train() client id: f_00005-7-3 loss: 0.470504  [  128/  146]
train() client id: f_00006-0-0 loss: 0.516330  [   32/   54]
train() client id: f_00006-1-0 loss: 0.503365  [   32/   54]
train() client id: f_00006-2-0 loss: 0.389308  [   32/   54]
train() client id: f_00006-3-0 loss: 0.496238  [   32/   54]
train() client id: f_00006-4-0 loss: 0.510788  [   32/   54]
train() client id: f_00006-5-0 loss: 0.452725  [   32/   54]
train() client id: f_00006-6-0 loss: 0.402556  [   32/   54]
train() client id: f_00006-7-0 loss: 0.480599  [   32/   54]
train() client id: f_00007-0-0 loss: 0.415700  [   32/  179]
train() client id: f_00007-0-1 loss: 0.280034  [   64/  179]
train() client id: f_00007-0-2 loss: 0.478583  [   96/  179]
train() client id: f_00007-0-3 loss: 0.422725  [  128/  179]
train() client id: f_00007-0-4 loss: 0.574800  [  160/  179]
train() client id: f_00007-1-0 loss: 0.414835  [   32/  179]
train() client id: f_00007-1-1 loss: 0.369363  [   64/  179]
train() client id: f_00007-1-2 loss: 0.261813  [   96/  179]
train() client id: f_00007-1-3 loss: 0.470597  [  128/  179]
train() client id: f_00007-1-4 loss: 0.475555  [  160/  179]
train() client id: f_00007-2-0 loss: 0.480292  [   32/  179]
train() client id: f_00007-2-1 loss: 0.227539  [   64/  179]
train() client id: f_00007-2-2 loss: 0.420488  [   96/  179]
train() client id: f_00007-2-3 loss: 0.625838  [  128/  179]
train() client id: f_00007-2-4 loss: 0.267100  [  160/  179]
train() client id: f_00007-3-0 loss: 0.396607  [   32/  179]
train() client id: f_00007-3-1 loss: 0.615271  [   64/  179]
train() client id: f_00007-3-2 loss: 0.352032  [   96/  179]
train() client id: f_00007-3-3 loss: 0.205442  [  128/  179]
train() client id: f_00007-3-4 loss: 0.340916  [  160/  179]
train() client id: f_00007-4-0 loss: 0.296105  [   32/  179]
train() client id: f_00007-4-1 loss: 0.322590  [   64/  179]
train() client id: f_00007-4-2 loss: 0.276339  [   96/  179]
train() client id: f_00007-4-3 loss: 0.482787  [  128/  179]
train() client id: f_00007-4-4 loss: 0.289989  [  160/  179]
train() client id: f_00007-5-0 loss: 0.203331  [   32/  179]
train() client id: f_00007-5-1 loss: 0.536483  [   64/  179]
train() client id: f_00007-5-2 loss: 0.417714  [   96/  179]
train() client id: f_00007-5-3 loss: 0.265547  [  128/  179]
train() client id: f_00007-5-4 loss: 0.352148  [  160/  179]
train() client id: f_00007-6-0 loss: 0.463123  [   32/  179]
train() client id: f_00007-6-1 loss: 0.343842  [   64/  179]
train() client id: f_00007-6-2 loss: 0.200456  [   96/  179]
train() client id: f_00007-6-3 loss: 0.387996  [  128/  179]
train() client id: f_00007-6-4 loss: 0.184042  [  160/  179]
train() client id: f_00007-7-0 loss: 0.225316  [   32/  179]
train() client id: f_00007-7-1 loss: 0.302521  [   64/  179]
train() client id: f_00007-7-2 loss: 0.449187  [   96/  179]
train() client id: f_00007-7-3 loss: 0.293115  [  128/  179]
train() client id: f_00007-7-4 loss: 0.585537  [  160/  179]
train() client id: f_00008-0-0 loss: 0.802056  [   32/  130]
train() client id: f_00008-0-1 loss: 0.920903  [   64/  130]
train() client id: f_00008-0-2 loss: 0.639031  [   96/  130]
train() client id: f_00008-0-3 loss: 0.712960  [  128/  130]
train() client id: f_00008-1-0 loss: 0.800751  [   32/  130]
train() client id: f_00008-1-1 loss: 0.738965  [   64/  130]
train() client id: f_00008-1-2 loss: 0.839241  [   96/  130]
train() client id: f_00008-1-3 loss: 0.679354  [  128/  130]
train() client id: f_00008-2-0 loss: 0.746926  [   32/  130]
train() client id: f_00008-2-1 loss: 0.681118  [   64/  130]
train() client id: f_00008-2-2 loss: 0.825602  [   96/  130]
train() client id: f_00008-2-3 loss: 0.781064  [  128/  130]
train() client id: f_00008-3-0 loss: 0.664745  [   32/  130]
train() client id: f_00008-3-1 loss: 0.770926  [   64/  130]
train() client id: f_00008-3-2 loss: 0.884844  [   96/  130]
train() client id: f_00008-3-3 loss: 0.733340  [  128/  130]
train() client id: f_00008-4-0 loss: 0.720909  [   32/  130]
train() client id: f_00008-4-1 loss: 0.793681  [   64/  130]
train() client id: f_00008-4-2 loss: 0.658302  [   96/  130]
train() client id: f_00008-4-3 loss: 0.841246  [  128/  130]
train() client id: f_00008-5-0 loss: 0.625703  [   32/  130]
train() client id: f_00008-5-1 loss: 0.875929  [   64/  130]
train() client id: f_00008-5-2 loss: 0.814939  [   96/  130]
train() client id: f_00008-5-3 loss: 0.748225  [  128/  130]
train() client id: f_00008-6-0 loss: 0.667349  [   32/  130]
train() client id: f_00008-6-1 loss: 0.698045  [   64/  130]
train() client id: f_00008-6-2 loss: 0.802490  [   96/  130]
train() client id: f_00008-6-3 loss: 0.870602  [  128/  130]
train() client id: f_00008-7-0 loss: 0.695087  [   32/  130]
train() client id: f_00008-7-1 loss: 0.705337  [   64/  130]
train() client id: f_00008-7-2 loss: 0.896372  [   96/  130]
train() client id: f_00008-7-3 loss: 0.739338  [  128/  130]
train() client id: f_00009-0-0 loss: 1.068214  [   32/  118]
train() client id: f_00009-0-1 loss: 1.034648  [   64/  118]
train() client id: f_00009-0-2 loss: 1.013570  [   96/  118]
train() client id: f_00009-1-0 loss: 0.901272  [   32/  118]
train() client id: f_00009-1-1 loss: 0.975410  [   64/  118]
train() client id: f_00009-1-2 loss: 1.017038  [   96/  118]
train() client id: f_00009-2-0 loss: 1.102260  [   32/  118]
train() client id: f_00009-2-1 loss: 0.957729  [   64/  118]
train() client id: f_00009-2-2 loss: 0.930153  [   96/  118]
train() client id: f_00009-3-0 loss: 1.070304  [   32/  118]
train() client id: f_00009-3-1 loss: 0.978290  [   64/  118]
train() client id: f_00009-3-2 loss: 1.062790  [   96/  118]
train() client id: f_00009-4-0 loss: 0.897681  [   32/  118]
train() client id: f_00009-4-1 loss: 0.909807  [   64/  118]
train() client id: f_00009-4-2 loss: 1.024218  [   96/  118]
train() client id: f_00009-5-0 loss: 1.064873  [   32/  118]
train() client id: f_00009-5-1 loss: 0.877705  [   64/  118]
train() client id: f_00009-5-2 loss: 0.785898  [   96/  118]
train() client id: f_00009-6-0 loss: 0.864482  [   32/  118]
train() client id: f_00009-6-1 loss: 1.044004  [   64/  118]
train() client id: f_00009-6-2 loss: 0.942447  [   96/  118]
train() client id: f_00009-7-0 loss: 0.989713  [   32/  118]
train() client id: f_00009-7-1 loss: 0.811009  [   64/  118]
train() client id: f_00009-7-2 loss: 0.998375  [   96/  118]
At round 67 accuracy: 0.6472148541114059
At round 67 training accuracy: 0.5922199865861838
At round 67 training loss: 0.8160454928919111
update_location
xs = -3.905658 4.200318 355.009024 18.811294 0.979296 3.956410 -317.443192 -296.324852 339.663977 -282.060879 
ys = 347.587959 330.555839 1.320614 -317.455176 309.350187 292.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 361.707954 345.376324 368.826722 333.364145 325.113053 309.444308 332.831895 312.744455 354.514156 299.289744 
dists_bs = 243.022381 237.298068 557.740168 529.221048 221.401638 214.255853 227.685524 212.267695 538.203983 201.748668 
uav_gains = -119.779803 -118.914088 -120.119944 -118.189706 -117.643912 -116.493645 -118.155719 -116.748291 -119.414056 -115.671097 
bs_gains = -106.365155 -106.075297 -116.467153 -115.828898 -105.232122 -104.833175 -105.572451 -104.719808 -116.033573 -104.101758 
Round 68
-------------------------------
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.01021389 3.98881553 1.98649518 0.75024676 4.59768714 2.21362175
 0.91366287 2.75982094 2.02332361 1.79514037]
obj_prev = 23.039028040351884
eta_min = nan	eta_max = nan
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 5.251816334124527	eta = 0.9090909090909091
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 14.045450908523552	eta = 0.3399234753417846
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 9.049757744866326	eta = 0.5275697560275713
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 8.202066092537866	eta = 0.5820946127112314
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 8.148578542585753	eta = 0.5859155017793716
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 8.148332307464996	eta = 0.5859332076078638
af = 4.774378485567752	bf = 0.8218123015887785	zeta = 8.148332302200442	eta = 0.5859332079864292
eta = 0.5859332079864292
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [0.04410974 0.09277049 0.04340959 0.01505332 0.10712366 0.05111129
 0.01890417 0.06266384 0.04551005 0.04130912]
ene_total = [0.83982608 1.19483484 0.84607581 0.44015542 1.35993302 0.69656538
 0.48459427 0.96365584 0.74525191 0.57743974]
ti_comp = [1.98249026 2.16845803 1.97019612 2.03038779 2.17217608 2.17382626
 2.03126319 2.06316582 2.07658364 2.17668634]
ti_coms = [0.27011056 0.08414279 0.2824047  0.22221303 0.08042474 0.07877456
 0.22133763 0.18943501 0.17601719 0.07591448]
t_total = [26.59971466 26.59971466 26.59971466 26.59971466 26.59971466 26.59971466
 26.59971466 26.59971466 26.59971466 26.59971466]
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [1.36477616e-06 1.06122506e-05 1.31709789e-06 5.17153298e-08
 1.62834394e-05 1.76595985e-06 1.02334201e-07 3.61295660e-06
 1.36616489e-06 9.29878567e-07]
ene_total = [0.31897471 0.09948487 0.33349161 0.26239966 0.0951614  0.09304137
 0.26136655 0.22373596 0.20786507 0.08965418]
optimize_network iter = 0 obj = 1.9851753783046104
eta = 0.5859332079864292
freqs = [11124831.81396149 21390888.52316821 11016564.88463251  3707007.23591558
 24658143.18962516 11756065.47830876  4653305.14511856 15186331.26526243
 10957914.22550441  9488993.01846851]
eta_min = 0.5859332079864303	eta_max = 0.7781376768963073
af = 0.0003421552139497174	bf = 0.8218123015887785	zeta = 0.00037637073534468917	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [2.43339611e-07 1.89216446e-06 2.34838575e-07 9.22084412e-09
 2.90333750e-06 3.14870668e-07 1.82461896e-08 6.44190219e-07
 2.43587222e-07 1.65797363e-07]
ene_total = [1.45278485 0.4526582  1.5189076  1.19515856 0.43271528 0.42370069
 1.19045077 1.01889828 0.94670988 0.40830993]
ti_comp = [0.93686199 1.12282976 0.92456785 0.98475952 1.12654781 1.12819799
 0.98563492 1.01753754 1.03095536 1.13105807]
ti_coms = [0.27011056 0.08414279 0.2824047  0.22221303 0.08042474 0.07877456
 0.22133763 0.18943501 0.17601719 0.07591448]
t_total = [26.59971466 26.59971466 26.59971466 26.59971466 26.59971466 26.59971466
 26.59971466 26.59971466 26.59971466 26.59971466]
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [6.31568265e-07 4.09043688e-06 6.18083830e-07 2.27198204e-08
 6.25640601e-06 6.77560105e-07 4.49168218e-08 1.53503439e-06
 5.72809530e-07 3.55906665e-07]
ene_total = [0.59529372 0.18552719 0.62238771 0.48972193 0.17738094 0.17362127
 0.48779318 0.41751781 0.38792591 0.16731103]
optimize_network iter = 1 obj = 3.7044806936267416
eta = 0.7781376768963073
freqs = [11047352.84864209 19386314.72442644 11016564.8846325   3586753.80166813
 22311825.119887   10629940.50483284  4500294.94608312 14449934.82166095
 10357782.55225607  8569597.11605592]
eta_min = 0.7781376768963095	eta_max = 0.7781376768963063
af = 0.0002885807068502459	bf = 0.8218123015887785	zeta = 0.0003174387775352705	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [2.39961934e-07 1.55414576e-06 2.34838575e-07 8.63230839e-09
 2.37709741e-06 2.57436357e-07 1.70659737e-08 5.83230415e-07
 2.17636778e-07 1.35225369e-07]
ene_total = [1.45278467 0.45264002 1.5189076  1.19515853 0.43268697 0.4236976
 1.19045071 1.018895   0.94670848 0.40830829]
ti_comp = [0.93686199 1.12282976 0.92456785 0.98475952 1.12654781 1.12819799
 0.98563492 1.01753754 1.03095536 1.13105807]
ti_coms = [0.27011056 0.08414279 0.2824047  0.22221303 0.08042474 0.07877456
 0.22133763 0.18943501 0.17601719 0.07591448]
t_total = [26.59971466 26.59971466 26.59971466 26.59971466 26.59971466 26.59971466
 26.59971466 26.59971466 26.59971466 26.59971466]
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [6.31568265e-07 4.09043688e-06 6.18083830e-07 2.27198204e-08
 6.25640601e-06 6.77560105e-07 4.49168218e-08 1.53503439e-06
 5.72809530e-07 3.55906665e-07]
ene_total = [0.59529372 0.18552719 0.62238771 0.48972193 0.17738094 0.17362127
 0.48779318 0.41751781 0.38792591 0.16731103]
optimize_network iter = 2 obj = 3.7044806936267247
eta = 0.7781376768963063
freqs = [11047352.84864208 19386314.72442644 11016564.88463249  3586753.80166813
 22311825.119887   10629940.50483284  4500294.94608312 14449934.82166094
 10357782.55225607  8569597.11605592]
Done!
ene_coms = [0.02701106 0.00841428 0.02824047 0.0222213  0.00804247 0.00787746
 0.02213376 0.0189435  0.01760172 0.00759145]
ene_comp = [6.15101785e-07 3.98378951e-06 6.01968921e-07 2.21274609e-08
 6.09328669e-06 6.59894509e-07 4.37457339e-08 1.49501241e-06
 5.57875029e-07 3.46627335e-07]
ene_total = [0.02701167 0.00841826 0.02824107 0.02222133 0.00804857 0.00787812
 0.02213381 0.018945   0.01760228 0.00759179]
At round 68 energy consumption: 0.16809188719852586
At round 68 eta: 0.7781376768963063
At round 68 a_n: 4.8894852623407985
At round 68 local rounds: 8.214162669625793
At round 68 global rounds: 22.038375844715002
gradient difference: 0.6175466775894165
train() client id: f_00000-0-0 loss: 1.045298  [   32/  126]
train() client id: f_00000-0-1 loss: 0.895357  [   64/  126]
train() client id: f_00000-0-2 loss: 0.658071  [   96/  126]
train() client id: f_00000-1-0 loss: 0.790209  [   32/  126]
train() client id: f_00000-1-1 loss: 0.884382  [   64/  126]
train() client id: f_00000-1-2 loss: 0.853575  [   96/  126]
train() client id: f_00000-2-0 loss: 0.647213  [   32/  126]
train() client id: f_00000-2-1 loss: 0.754035  [   64/  126]
train() client id: f_00000-2-2 loss: 0.769236  [   96/  126]
train() client id: f_00000-3-0 loss: 0.688287  [   32/  126]
train() client id: f_00000-3-1 loss: 0.867062  [   64/  126]
train() client id: f_00000-3-2 loss: 0.827981  [   96/  126]
train() client id: f_00000-4-0 loss: 0.611991  [   32/  126]
train() client id: f_00000-4-1 loss: 0.841685  [   64/  126]
train() client id: f_00000-4-2 loss: 0.739538  [   96/  126]
train() client id: f_00000-5-0 loss: 0.793750  [   32/  126]
train() client id: f_00000-5-1 loss: 0.800016  [   64/  126]
train() client id: f_00000-5-2 loss: 0.752839  [   96/  126]
train() client id: f_00000-6-0 loss: 0.677330  [   32/  126]
train() client id: f_00000-6-1 loss: 0.679574  [   64/  126]
train() client id: f_00000-6-2 loss: 0.962331  [   96/  126]
train() client id: f_00000-7-0 loss: 0.819779  [   32/  126]
train() client id: f_00000-7-1 loss: 0.655533  [   64/  126]
train() client id: f_00000-7-2 loss: 0.735871  [   96/  126]
train() client id: f_00001-0-0 loss: 0.438221  [   32/  265]
train() client id: f_00001-0-1 loss: 0.597598  [   64/  265]
train() client id: f_00001-0-2 loss: 0.456471  [   96/  265]
train() client id: f_00001-0-3 loss: 0.428799  [  128/  265]
train() client id: f_00001-0-4 loss: 0.551513  [  160/  265]
train() client id: f_00001-0-5 loss: 0.563917  [  192/  265]
train() client id: f_00001-0-6 loss: 0.442827  [  224/  265]
train() client id: f_00001-0-7 loss: 0.424424  [  256/  265]
train() client id: f_00001-1-0 loss: 0.532393  [   32/  265]
train() client id: f_00001-1-1 loss: 0.518546  [   64/  265]
train() client id: f_00001-1-2 loss: 0.466469  [   96/  265]
train() client id: f_00001-1-3 loss: 0.442578  [  128/  265]
train() client id: f_00001-1-4 loss: 0.405064  [  160/  265]
train() client id: f_00001-1-5 loss: 0.461581  [  192/  265]
train() client id: f_00001-1-6 loss: 0.522897  [  224/  265]
train() client id: f_00001-1-7 loss: 0.437460  [  256/  265]
train() client id: f_00001-2-0 loss: 0.480171  [   32/  265]
train() client id: f_00001-2-1 loss: 0.575647  [   64/  265]
train() client id: f_00001-2-2 loss: 0.393311  [   96/  265]
train() client id: f_00001-2-3 loss: 0.359635  [  128/  265]
train() client id: f_00001-2-4 loss: 0.387067  [  160/  265]
train() client id: f_00001-2-5 loss: 0.662491  [  192/  265]
train() client id: f_00001-2-6 loss: 0.516190  [  224/  265]
train() client id: f_00001-2-7 loss: 0.468764  [  256/  265]
train() client id: f_00001-3-0 loss: 0.528477  [   32/  265]
train() client id: f_00001-3-1 loss: 0.518196  [   64/  265]
train() client id: f_00001-3-2 loss: 0.447723  [   96/  265]
train() client id: f_00001-3-3 loss: 0.376826  [  128/  265]
train() client id: f_00001-3-4 loss: 0.434831  [  160/  265]
train() client id: f_00001-3-5 loss: 0.591375  [  192/  265]
train() client id: f_00001-3-6 loss: 0.355806  [  224/  265]
train() client id: f_00001-3-7 loss: 0.518618  [  256/  265]
train() client id: f_00001-4-0 loss: 0.562522  [   32/  265]
train() client id: f_00001-4-1 loss: 0.517025  [   64/  265]
train() client id: f_00001-4-2 loss: 0.494377  [   96/  265]
train() client id: f_00001-4-3 loss: 0.373821  [  128/  265]
train() client id: f_00001-4-4 loss: 0.484208  [  160/  265]
train() client id: f_00001-4-5 loss: 0.479811  [  192/  265]
train() client id: f_00001-4-6 loss: 0.457337  [  224/  265]
train() client id: f_00001-4-7 loss: 0.424493  [  256/  265]
train() client id: f_00001-5-0 loss: 0.485978  [   32/  265]
train() client id: f_00001-5-1 loss: 0.370150  [   64/  265]
train() client id: f_00001-5-2 loss: 0.364766  [   96/  265]
train() client id: f_00001-5-3 loss: 0.563370  [  128/  265]
train() client id: f_00001-5-4 loss: 0.532146  [  160/  265]
train() client id: f_00001-5-5 loss: 0.546003  [  192/  265]
train() client id: f_00001-5-6 loss: 0.421926  [  224/  265]
train() client id: f_00001-5-7 loss: 0.515295  [  256/  265]
train() client id: f_00001-6-0 loss: 0.645739  [   32/  265]
train() client id: f_00001-6-1 loss: 0.377582  [   64/  265]
train() client id: f_00001-6-2 loss: 0.438186  [   96/  265]
train() client id: f_00001-6-3 loss: 0.414007  [  128/  265]
train() client id: f_00001-6-4 loss: 0.496815  [  160/  265]
train() client id: f_00001-6-5 loss: 0.471279  [  192/  265]
train() client id: f_00001-6-6 loss: 0.442166  [  224/  265]
train() client id: f_00001-6-7 loss: 0.483938  [  256/  265]
train() client id: f_00001-7-0 loss: 0.425320  [   32/  265]
train() client id: f_00001-7-1 loss: 0.471186  [   64/  265]
train() client id: f_00001-7-2 loss: 0.516094  [   96/  265]
train() client id: f_00001-7-3 loss: 0.402245  [  128/  265]
train() client id: f_00001-7-4 loss: 0.383458  [  160/  265]
train() client id: f_00001-7-5 loss: 0.510079  [  192/  265]
train() client id: f_00001-7-6 loss: 0.487463  [  224/  265]
train() client id: f_00001-7-7 loss: 0.595532  [  256/  265]
train() client id: f_00002-0-0 loss: 1.202690  [   32/  124]
train() client id: f_00002-0-1 loss: 1.057245  [   64/  124]
train() client id: f_00002-0-2 loss: 1.011840  [   96/  124]
train() client id: f_00002-1-0 loss: 1.220068  [   32/  124]
train() client id: f_00002-1-1 loss: 1.004414  [   64/  124]
train() client id: f_00002-1-2 loss: 1.145625  [   96/  124]
train() client id: f_00002-2-0 loss: 1.123714  [   32/  124]
train() client id: f_00002-2-1 loss: 1.053678  [   64/  124]
train() client id: f_00002-2-2 loss: 0.937247  [   96/  124]
train() client id: f_00002-3-0 loss: 1.373109  [   32/  124]
train() client id: f_00002-3-1 loss: 0.884145  [   64/  124]
train() client id: f_00002-3-2 loss: 1.091887  [   96/  124]
train() client id: f_00002-4-0 loss: 1.036191  [   32/  124]
train() client id: f_00002-4-1 loss: 1.028279  [   64/  124]
train() client id: f_00002-4-2 loss: 1.055070  [   96/  124]
train() client id: f_00002-5-0 loss: 1.156932  [   32/  124]
train() client id: f_00002-5-1 loss: 0.823756  [   64/  124]
train() client id: f_00002-5-2 loss: 1.044027  [   96/  124]
train() client id: f_00002-6-0 loss: 1.051829  [   32/  124]
train() client id: f_00002-6-1 loss: 0.886577  [   64/  124]
train() client id: f_00002-6-2 loss: 0.966253  [   96/  124]
train() client id: f_00002-7-0 loss: 1.131422  [   32/  124]
train() client id: f_00002-7-1 loss: 0.823233  [   64/  124]
train() client id: f_00002-7-2 loss: 1.002140  [   96/  124]
train() client id: f_00003-0-0 loss: 0.292893  [   32/   43]
train() client id: f_00003-1-0 loss: 0.197806  [   32/   43]
train() client id: f_00003-2-0 loss: 0.379257  [   32/   43]
train() client id: f_00003-3-0 loss: 0.362939  [   32/   43]
train() client id: f_00003-4-0 loss: 0.382271  [   32/   43]
train() client id: f_00003-5-0 loss: 0.388101  [   32/   43]
train() client id: f_00003-6-0 loss: 0.500444  [   32/   43]
train() client id: f_00003-7-0 loss: 0.289305  [   32/   43]
train() client id: f_00004-0-0 loss: 0.666904  [   32/  306]
train() client id: f_00004-0-1 loss: 0.772257  [   64/  306]
train() client id: f_00004-0-2 loss: 0.877007  [   96/  306]
train() client id: f_00004-0-3 loss: 0.712963  [  128/  306]
train() client id: f_00004-0-4 loss: 0.689430  [  160/  306]
train() client id: f_00004-0-5 loss: 0.875022  [  192/  306]
train() client id: f_00004-0-6 loss: 0.687624  [  224/  306]
train() client id: f_00004-0-7 loss: 0.822506  [  256/  306]
train() client id: f_00004-0-8 loss: 0.701037  [  288/  306]
train() client id: f_00004-1-0 loss: 0.720141  [   32/  306]
train() client id: f_00004-1-1 loss: 0.769434  [   64/  306]
train() client id: f_00004-1-2 loss: 0.595380  [   96/  306]
train() client id: f_00004-1-3 loss: 0.847676  [  128/  306]
train() client id: f_00004-1-4 loss: 0.801643  [  160/  306]
train() client id: f_00004-1-5 loss: 0.786310  [  192/  306]
train() client id: f_00004-1-6 loss: 0.836132  [  224/  306]
train() client id: f_00004-1-7 loss: 0.895236  [  256/  306]
train() client id: f_00004-1-8 loss: 0.731366  [  288/  306]
train() client id: f_00004-2-0 loss: 0.716987  [   32/  306]
train() client id: f_00004-2-1 loss: 0.811816  [   64/  306]
train() client id: f_00004-2-2 loss: 0.948734  [   96/  306]
train() client id: f_00004-2-3 loss: 0.761281  [  128/  306]
train() client id: f_00004-2-4 loss: 0.604897  [  160/  306]
train() client id: f_00004-2-5 loss: 0.738768  [  192/  306]
train() client id: f_00004-2-6 loss: 0.720454  [  224/  306]
train() client id: f_00004-2-7 loss: 0.763203  [  256/  306]
train() client id: f_00004-2-8 loss: 0.814300  [  288/  306]
train() client id: f_00004-3-0 loss: 0.571941  [   32/  306]
train() client id: f_00004-3-1 loss: 0.875481  [   64/  306]
train() client id: f_00004-3-2 loss: 0.635972  [   96/  306]
train() client id: f_00004-3-3 loss: 0.796037  [  128/  306]
train() client id: f_00004-3-4 loss: 0.883322  [  160/  306]
train() client id: f_00004-3-5 loss: 0.705970  [  192/  306]
train() client id: f_00004-3-6 loss: 0.796867  [  224/  306]
train() client id: f_00004-3-7 loss: 0.686326  [  256/  306]
train() client id: f_00004-3-8 loss: 0.844136  [  288/  306]
train() client id: f_00004-4-0 loss: 0.693632  [   32/  306]
train() client id: f_00004-4-1 loss: 0.744401  [   64/  306]
train() client id: f_00004-4-2 loss: 0.670758  [   96/  306]
train() client id: f_00004-4-3 loss: 0.706442  [  128/  306]
train() client id: f_00004-4-4 loss: 0.767307  [  160/  306]
train() client id: f_00004-4-5 loss: 0.822913  [  192/  306]
train() client id: f_00004-4-6 loss: 0.898959  [  224/  306]
train() client id: f_00004-4-7 loss: 0.781924  [  256/  306]
train() client id: f_00004-4-8 loss: 0.786119  [  288/  306]
train() client id: f_00004-5-0 loss: 0.802444  [   32/  306]
train() client id: f_00004-5-1 loss: 0.908411  [   64/  306]
train() client id: f_00004-5-2 loss: 0.726785  [   96/  306]
train() client id: f_00004-5-3 loss: 0.589633  [  128/  306]
train() client id: f_00004-5-4 loss: 0.829995  [  160/  306]
train() client id: f_00004-5-5 loss: 0.761035  [  192/  306]
train() client id: f_00004-5-6 loss: 0.818975  [  224/  306]
train() client id: f_00004-5-7 loss: 0.675526  [  256/  306]
train() client id: f_00004-5-8 loss: 0.819445  [  288/  306]
train() client id: f_00004-6-0 loss: 0.773986  [   32/  306]
train() client id: f_00004-6-1 loss: 0.737280  [   64/  306]
train() client id: f_00004-6-2 loss: 0.720371  [   96/  306]
train() client id: f_00004-6-3 loss: 0.634865  [  128/  306]
train() client id: f_00004-6-4 loss: 0.860426  [  160/  306]
train() client id: f_00004-6-5 loss: 0.800744  [  192/  306]
train() client id: f_00004-6-6 loss: 0.819576  [  224/  306]
train() client id: f_00004-6-7 loss: 0.768846  [  256/  306]
train() client id: f_00004-6-8 loss: 0.729686  [  288/  306]
train() client id: f_00004-7-0 loss: 0.813497  [   32/  306]
train() client id: f_00004-7-1 loss: 0.716924  [   64/  306]
train() client id: f_00004-7-2 loss: 0.743424  [   96/  306]
train() client id: f_00004-7-3 loss: 0.748753  [  128/  306]
train() client id: f_00004-7-4 loss: 0.824723  [  160/  306]
train() client id: f_00004-7-5 loss: 0.735989  [  192/  306]
train() client id: f_00004-7-6 loss: 0.843083  [  224/  306]
train() client id: f_00004-7-7 loss: 0.725174  [  256/  306]
train() client id: f_00004-7-8 loss: 0.703954  [  288/  306]
train() client id: f_00005-0-0 loss: 0.721785  [   32/  146]
train() client id: f_00005-0-1 loss: 0.708892  [   64/  146]
train() client id: f_00005-0-2 loss: 0.658575  [   96/  146]
train() client id: f_00005-0-3 loss: 0.703806  [  128/  146]
train() client id: f_00005-1-0 loss: 0.616626  [   32/  146]
train() client id: f_00005-1-1 loss: 0.810294  [   64/  146]
train() client id: f_00005-1-2 loss: 0.550611  [   96/  146]
train() client id: f_00005-1-3 loss: 0.978168  [  128/  146]
train() client id: f_00005-2-0 loss: 0.824843  [   32/  146]
train() client id: f_00005-2-1 loss: 0.778175  [   64/  146]
train() client id: f_00005-2-2 loss: 0.477291  [   96/  146]
train() client id: f_00005-2-3 loss: 0.699664  [  128/  146]
train() client id: f_00005-3-0 loss: 0.607570  [   32/  146]
train() client id: f_00005-3-1 loss: 0.668070  [   64/  146]
train() client id: f_00005-3-2 loss: 0.770957  [   96/  146]
train() client id: f_00005-3-3 loss: 0.746020  [  128/  146]
train() client id: f_00005-4-0 loss: 0.591977  [   32/  146]
train() client id: f_00005-4-1 loss: 0.585733  [   64/  146]
train() client id: f_00005-4-2 loss: 0.817281  [   96/  146]
train() client id: f_00005-4-3 loss: 0.715403  [  128/  146]
train() client id: f_00005-5-0 loss: 0.720881  [   32/  146]
train() client id: f_00005-5-1 loss: 0.502628  [   64/  146]
train() client id: f_00005-5-2 loss: 0.733825  [   96/  146]
train() client id: f_00005-5-3 loss: 1.127349  [  128/  146]
train() client id: f_00005-6-0 loss: 0.746856  [   32/  146]
train() client id: f_00005-6-1 loss: 0.460325  [   64/  146]
train() client id: f_00005-6-2 loss: 0.888973  [   96/  146]
train() client id: f_00005-6-3 loss: 0.625010  [  128/  146]
train() client id: f_00005-7-0 loss: 0.753493  [   32/  146]
train() client id: f_00005-7-1 loss: 0.583397  [   64/  146]
train() client id: f_00005-7-2 loss: 0.586323  [   96/  146]
train() client id: f_00005-7-3 loss: 1.001027  [  128/  146]
train() client id: f_00006-0-0 loss: 0.514157  [   32/   54]
train() client id: f_00006-1-0 loss: 0.431108  [   32/   54]
train() client id: f_00006-2-0 loss: 0.481857  [   32/   54]
train() client id: f_00006-3-0 loss: 0.423802  [   32/   54]
train() client id: f_00006-4-0 loss: 0.488987  [   32/   54]
train() client id: f_00006-5-0 loss: 0.506844  [   32/   54]
train() client id: f_00006-6-0 loss: 0.487074  [   32/   54]
train() client id: f_00006-7-0 loss: 0.448213  [   32/   54]
train() client id: f_00007-0-0 loss: 0.352859  [   32/  179]
train() client id: f_00007-0-1 loss: 0.456981  [   64/  179]
train() client id: f_00007-0-2 loss: 0.340648  [   96/  179]
train() client id: f_00007-0-3 loss: 0.391248  [  128/  179]
train() client id: f_00007-0-4 loss: 0.517358  [  160/  179]
train() client id: f_00007-1-0 loss: 0.497760  [   32/  179]
train() client id: f_00007-1-1 loss: 0.325149  [   64/  179]
train() client id: f_00007-1-2 loss: 0.450134  [   96/  179]
train() client id: f_00007-1-3 loss: 0.206790  [  128/  179]
train() client id: f_00007-1-4 loss: 0.458695  [  160/  179]
train() client id: f_00007-2-0 loss: 0.658949  [   32/  179]
train() client id: f_00007-2-1 loss: 0.227407  [   64/  179]
train() client id: f_00007-2-2 loss: 0.499889  [   96/  179]
train() client id: f_00007-2-3 loss: 0.193182  [  128/  179]
train() client id: f_00007-2-4 loss: 0.318262  [  160/  179]
train() client id: f_00007-3-0 loss: 0.484394  [   32/  179]
train() client id: f_00007-3-1 loss: 0.300404  [   64/  179]
train() client id: f_00007-3-2 loss: 0.515031  [   96/  179]
train() client id: f_00007-3-3 loss: 0.305643  [  128/  179]
train() client id: f_00007-3-4 loss: 0.378727  [  160/  179]
train() client id: f_00007-4-0 loss: 0.533000  [   32/  179]
train() client id: f_00007-4-1 loss: 0.292689  [   64/  179]
train() client id: f_00007-4-2 loss: 0.377058  [   96/  179]
train() client id: f_00007-4-3 loss: 0.211040  [  128/  179]
train() client id: f_00007-4-4 loss: 0.427063  [  160/  179]
train() client id: f_00007-5-0 loss: 0.229545  [   32/  179]
train() client id: f_00007-5-1 loss: 0.262139  [   64/  179]
train() client id: f_00007-5-2 loss: 0.661455  [   96/  179]
train() client id: f_00007-5-3 loss: 0.201843  [  128/  179]
train() client id: f_00007-5-4 loss: 0.528422  [  160/  179]
train() client id: f_00007-6-0 loss: 0.701187  [   32/  179]
train() client id: f_00007-6-1 loss: 0.421115  [   64/  179]
train() client id: f_00007-6-2 loss: 0.279612  [   96/  179]
train() client id: f_00007-6-3 loss: 0.194498  [  128/  179]
train() client id: f_00007-6-4 loss: 0.295687  [  160/  179]
train() client id: f_00007-7-0 loss: 0.386313  [   32/  179]
train() client id: f_00007-7-1 loss: 0.449331  [   64/  179]
train() client id: f_00007-7-2 loss: 0.197278  [   96/  179]
train() client id: f_00007-7-3 loss: 0.367903  [  128/  179]
train() client id: f_00007-7-4 loss: 0.484350  [  160/  179]
train() client id: f_00008-0-0 loss: 0.760865  [   32/  130]
train() client id: f_00008-0-1 loss: 0.733681  [   64/  130]
train() client id: f_00008-0-2 loss: 0.733343  [   96/  130]
train() client id: f_00008-0-3 loss: 0.753403  [  128/  130]
train() client id: f_00008-1-0 loss: 0.797980  [   32/  130]
train() client id: f_00008-1-1 loss: 0.718260  [   64/  130]
train() client id: f_00008-1-2 loss: 0.837503  [   96/  130]
train() client id: f_00008-1-3 loss: 0.623457  [  128/  130]
train() client id: f_00008-2-0 loss: 0.815656  [   32/  130]
train() client id: f_00008-2-1 loss: 0.717027  [   64/  130]
train() client id: f_00008-2-2 loss: 0.728299  [   96/  130]
train() client id: f_00008-2-3 loss: 0.721606  [  128/  130]
train() client id: f_00008-3-0 loss: 0.696988  [   32/  130]
train() client id: f_00008-3-1 loss: 0.941307  [   64/  130]
train() client id: f_00008-3-2 loss: 0.664541  [   96/  130]
train() client id: f_00008-3-3 loss: 0.677853  [  128/  130]
train() client id: f_00008-4-0 loss: 0.729734  [   32/  130]
train() client id: f_00008-4-1 loss: 0.663455  [   64/  130]
train() client id: f_00008-4-2 loss: 0.821878  [   96/  130]
train() client id: f_00008-4-3 loss: 0.759858  [  128/  130]
train() client id: f_00008-5-0 loss: 0.710688  [   32/  130]
train() client id: f_00008-5-1 loss: 0.749178  [   64/  130]
train() client id: f_00008-5-2 loss: 0.712717  [   96/  130]
train() client id: f_00008-5-3 loss: 0.808418  [  128/  130]
train() client id: f_00008-6-0 loss: 0.717903  [   32/  130]
train() client id: f_00008-6-1 loss: 0.672148  [   64/  130]
train() client id: f_00008-6-2 loss: 0.740914  [   96/  130]
train() client id: f_00008-6-3 loss: 0.833233  [  128/  130]
train() client id: f_00008-7-0 loss: 0.727620  [   32/  130]
train() client id: f_00008-7-1 loss: 0.661820  [   64/  130]
train() client id: f_00008-7-2 loss: 0.720666  [   96/  130]
train() client id: f_00008-7-3 loss: 0.843193  [  128/  130]
train() client id: f_00009-0-0 loss: 1.008096  [   32/  118]
train() client id: f_00009-0-1 loss: 0.879855  [   64/  118]
train() client id: f_00009-0-2 loss: 0.905424  [   96/  118]
train() client id: f_00009-1-0 loss: 0.867681  [   32/  118]
train() client id: f_00009-1-1 loss: 0.868540  [   64/  118]
train() client id: f_00009-1-2 loss: 0.875949  [   96/  118]
train() client id: f_00009-2-0 loss: 0.796322  [   32/  118]
train() client id: f_00009-2-1 loss: 0.968027  [   64/  118]
train() client id: f_00009-2-2 loss: 0.742588  [   96/  118]
train() client id: f_00009-3-0 loss: 0.837461  [   32/  118]
train() client id: f_00009-3-1 loss: 0.902603  [   64/  118]
train() client id: f_00009-3-2 loss: 0.708291  [   96/  118]
train() client id: f_00009-4-0 loss: 0.626353  [   32/  118]
train() client id: f_00009-4-1 loss: 0.850492  [   64/  118]
train() client id: f_00009-4-2 loss: 0.928368  [   96/  118]
train() client id: f_00009-5-0 loss: 0.703131  [   32/  118]
train() client id: f_00009-5-1 loss: 0.789750  [   64/  118]
train() client id: f_00009-5-2 loss: 0.665356  [   96/  118]
train() client id: f_00009-6-0 loss: 0.641718  [   32/  118]
train() client id: f_00009-6-1 loss: 0.861387  [   64/  118]
train() client id: f_00009-6-2 loss: 0.812748  [   96/  118]
train() client id: f_00009-7-0 loss: 0.886350  [   32/  118]
train() client id: f_00009-7-1 loss: 0.680945  [   64/  118]
train() client id: f_00009-7-2 loss: 0.683658  [   96/  118]
At round 68 accuracy: 0.6472148541114059
At round 68 training accuracy: 0.5949027498323273
At round 68 training loss: 0.8162903134126226
update_location
xs = -3.905658 4.200318 360.009024 18.811294 0.979296 3.956410 -322.443192 -301.324852 344.663977 -287.060879 
ys = 352.587959 335.555839 1.320614 -322.455176 314.350187 297.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 366.515379 350.164766 373.641862 338.128977 329.874217 314.179760 337.604062 317.485972 359.307566 304.006513 
dists_bs = 246.598372 240.605343 562.493720 533.876830 224.470905 217.045415 230.846551 215.164641 542.987262 204.445919 
uav_gains = -120.011752 -119.181260 -120.338787 -118.486677 -117.963816 -116.857007 -118.454597 -117.102679 -119.660331 -116.060228 
bs_gains = -106.542785 -106.243607 -116.570354 -115.935409 -105.399541 -104.990476 -105.740114 -104.884644 -116.141169 -104.263256 
Round 69
-------------------------------
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.8731924  3.7097278  1.85116443 0.70130907 4.2759113  2.0588358
 0.85327763 2.56982483 1.88245466 1.66965903]
obj_prev = 21.445356959883735
eta_min = 1.2228941532952692e-50	eta_max = 0.9500254409851863
af = 4.439896748873053	bf = 0.784423253509249	zeta = 4.883886423760359	eta = 0.9090909090909091
af = 4.439896748873053	bf = 0.784423253509249	zeta = 13.283496720526747	eta = 0.3342415662294824
af = 4.439896748873053	bf = 0.784423253509249	zeta = 8.486662978303045	eta = 0.5231616667498248
af = 4.439896748873053	bf = 0.784423253509249	zeta = 7.677394178566005	eta = 0.5783077754778437
af = 4.439896748873053	bf = 0.784423253509249	zeta = 7.626245838238497	eta = 0.5821864181995182
af = 4.439896748873053	bf = 0.784423253509249	zeta = 7.626008957353865	eta = 0.5822045022110287
eta = 0.5822045022110287
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [0.04463654 0.09387843 0.04392802 0.0152331  0.10840302 0.0517217
 0.01912994 0.06341222 0.04605357 0.04180247]
ene_total = [0.78877956 1.11412037 0.79451743 0.41620728 1.26807169 0.64932725
 0.45766691 0.90423052 0.69485736 0.53823058]
ti_comp = [2.16111529 2.35459485 2.14875789 2.20941898 2.35838257 2.36010268
 2.21029221 2.24279005 2.26158603 2.36299155]
ti_coms = [0.27840468 0.08492511 0.29076208 0.23010099 0.0811374  0.07941728
 0.22922776 0.19672992 0.17793394 0.07652842]
t_total = [26.54971046 26.54971046 26.54971046 26.54971046 26.54971046 26.54971046
 26.54971046 26.54971046 26.54971046 26.54971046]
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [1.19013336e-06 9.32706721e-06 1.14743992e-06 4.52573450e-08
 1.43144819e-05 1.55251797e-06 8.95615827e-08 3.16826659e-06
 1.19355847e-06 8.17638912e-07]
ene_total = [0.30300549 0.09252695 0.3164538  0.25042331 0.088459   0.08644808
 0.24947344 0.214139   0.19366152 0.08329608]
optimize_network iter = 0 obj = 1.8778866839980972
eta = 0.5822045022110287
freqs = [10327199.39136077 19935155.93888518 10221724.11669465  3447309.92299876
 22982492.36545908 10957510.88478662  4327469.55320673 14136905.79686903
 10181697.58740329  8845243.50076053]
eta_min = 0.5822045022110292	eta_max = 0.7888487440198617
af = 0.000276093861788328	bf = 0.784423253509249	zeta = 0.00030370324796716086	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [2.09696425e-07 1.64338949e-06 2.02174023e-07 7.97415130e-09
 2.52215070e-06 2.73547049e-07 1.57803692e-08 5.58235069e-07
 2.10299915e-07 1.44064491e-07]
ene_total = [1.3924886  0.42484648 1.45429544 1.1508812  0.40594569 0.39722982
 1.14651401 0.98399884 0.88997085 0.38277432]
ti_comp = [0.9545136  1.14799316 0.9421562  1.00281729 1.15178087 1.15350099
 1.00369052 1.03618836 1.05498434 1.15638985]
ti_coms = [0.27840468 0.08492511 0.29076208 0.23010099 0.0811374  0.07941728
 0.22922776 0.19672992 0.17793394 0.07652842]
t_total = [26.54971046 26.54971046 26.54971046 26.54971046 26.54971046 26.54971046
 26.54971046 26.54971046 26.54971046 26.54971046]
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [5.14272772e-07 3.30754040e-06 5.03112813e-07 1.85185886e-08
 5.05907005e-06 5.47858951e-07 3.66123645e-08 1.25120311e-06
 4.62364171e-07 2.87795246e-07]
ene_total = [0.59952879 0.1829493  0.62613904 0.49550078 0.17483054 0.17102929
 0.49362075 0.42366592 0.3831736  0.1648028 ]
optimize_network iter = 1 obj = 3.715240812383447
eta = 0.7888487440198617
freqs = [10252122.85547994 17928004.42999087 10221724.11669466  3330213.08308708
 20633691.92866442  9830152.63900685  4178489.52262742 13416514.38117568
  9570237.57402436  7925070.10389261]
eta_min = 0.7888487440198628	eta_max = 0.7888487440198605
af = 0.00022982602808494754	bf = 0.784423253509249	zeta = 0.0002528086308934423	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [2.06658611e-07 1.32912288e-06 2.02174023e-07 7.44162632e-09
 2.03296858e-06 2.20155092e-07 1.47125432e-08 5.02791340e-07
 1.85799332e-07 1.15649455e-07]
ene_total = [1.39248845 0.42483077 1.45429544 1.15088117 0.40592122 0.39722715
 1.14651395 0.98399607 0.88996963 0.38277289]
ti_comp = [0.9545136  1.14799316 0.9421562  1.00281729 1.15178087 1.15350099
 1.00369052 1.03618836 1.05498434 1.15638985]
ti_coms = [0.27840468 0.08492511 0.29076208 0.23010099 0.0811374  0.07941728
 0.22922776 0.19672992 0.17793394 0.07652842]
t_total = [26.54971046 26.54971046 26.54971046 26.54971046 26.54971046 26.54971046
 26.54971046 26.54971046 26.54971046 26.54971046]
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [5.14272772e-07 3.30754040e-06 5.03112813e-07 1.85185886e-08
 5.05907005e-06 5.47858951e-07 3.66123645e-08 1.25120311e-06
 4.62364171e-07 2.87795246e-07]
ene_total = [0.59952879 0.1829493  0.62613904 0.49550078 0.17483054 0.17102929
 0.49362075 0.42366592 0.3831736  0.1648028 ]
optimize_network iter = 2 obj = 3.715240812383425
eta = 0.7888487440198605
freqs = [10252122.85547993 17928004.42999088 10221724.11669465  3330213.08308708
 20633691.92866442  9830152.63900685  4178489.52262742 13416514.38117567
  9570237.57402436  7925070.10389262]
Done!
ene_coms = [0.02784047 0.00849251 0.02907621 0.0230101  0.00811374 0.00794173
 0.02292278 0.01967299 0.01779339 0.00765284]
ene_comp = [4.63517562e-07 2.98110875e-06 4.53459015e-07 1.66909304e-08
 4.55977439e-06 4.93789014e-07 3.29989742e-08 1.12771791e-06
 4.16731985e-07 2.59391820e-07]
ene_total = [0.02784093 0.00849549 0.02907666 0.02301012 0.0081183  0.00794222
 0.02292281 0.01967412 0.01779381 0.0076531 ]
At round 69 energy consumption: 0.17252756466419225
At round 69 eta: 0.7888487440198605
At round 69 a_n: 4.546939415371483
At round 69 local rounds: 7.766500565656417
At round 69 global rounds: 21.53403916195109
gradient difference: 0.6168093085289001
train() client id: f_00000-0-0 loss: 1.233021  [   32/  126]
train() client id: f_00000-0-1 loss: 1.194362  [   64/  126]
train() client id: f_00000-0-2 loss: 1.031082  [   96/  126]
train() client id: f_00000-1-0 loss: 1.119645  [   32/  126]
train() client id: f_00000-1-1 loss: 1.227584  [   64/  126]
train() client id: f_00000-1-2 loss: 1.148691  [   96/  126]
train() client id: f_00000-2-0 loss: 0.877970  [   32/  126]
train() client id: f_00000-2-1 loss: 1.165205  [   64/  126]
train() client id: f_00000-2-2 loss: 0.981148  [   96/  126]
train() client id: f_00000-3-0 loss: 0.988235  [   32/  126]
train() client id: f_00000-3-1 loss: 1.071035  [   64/  126]
train() client id: f_00000-3-2 loss: 0.880352  [   96/  126]
train() client id: f_00000-4-0 loss: 0.944766  [   32/  126]
train() client id: f_00000-4-1 loss: 0.985221  [   64/  126]
train() client id: f_00000-4-2 loss: 0.855783  [   96/  126]
train() client id: f_00000-5-0 loss: 0.995875  [   32/  126]
train() client id: f_00000-5-1 loss: 0.979864  [   64/  126]
train() client id: f_00000-5-2 loss: 0.893267  [   96/  126]
train() client id: f_00000-6-0 loss: 0.867458  [   32/  126]
train() client id: f_00000-6-1 loss: 0.887755  [   64/  126]
train() client id: f_00000-6-2 loss: 0.913589  [   96/  126]
train() client id: f_00001-0-0 loss: 0.486710  [   32/  265]
train() client id: f_00001-0-1 loss: 0.454159  [   64/  265]
train() client id: f_00001-0-2 loss: 0.351410  [   96/  265]
train() client id: f_00001-0-3 loss: 0.310298  [  128/  265]
train() client id: f_00001-0-4 loss: 0.361678  [  160/  265]
train() client id: f_00001-0-5 loss: 0.449170  [  192/  265]
train() client id: f_00001-0-6 loss: 0.345352  [  224/  265]
train() client id: f_00001-0-7 loss: 0.363418  [  256/  265]
train() client id: f_00001-1-0 loss: 0.418578  [   32/  265]
train() client id: f_00001-1-1 loss: 0.286345  [   64/  265]
train() client id: f_00001-1-2 loss: 0.346305  [   96/  265]
train() client id: f_00001-1-3 loss: 0.386674  [  128/  265]
train() client id: f_00001-1-4 loss: 0.303095  [  160/  265]
train() client id: f_00001-1-5 loss: 0.549807  [  192/  265]
train() client id: f_00001-1-6 loss: 0.343858  [  224/  265]
train() client id: f_00001-1-7 loss: 0.481466  [  256/  265]
train() client id: f_00001-2-0 loss: 0.422272  [   32/  265]
train() client id: f_00001-2-1 loss: 0.388043  [   64/  265]
train() client id: f_00001-2-2 loss: 0.366891  [   96/  265]
train() client id: f_00001-2-3 loss: 0.283376  [  128/  265]
train() client id: f_00001-2-4 loss: 0.333682  [  160/  265]
train() client id: f_00001-2-5 loss: 0.320141  [  192/  265]
train() client id: f_00001-2-6 loss: 0.427087  [  224/  265]
train() client id: f_00001-2-7 loss: 0.382250  [  256/  265]
train() client id: f_00001-3-0 loss: 0.299949  [   32/  265]
train() client id: f_00001-3-1 loss: 0.302719  [   64/  265]
train() client id: f_00001-3-2 loss: 0.353923  [   96/  265]
train() client id: f_00001-3-3 loss: 0.419019  [  128/  265]
train() client id: f_00001-3-4 loss: 0.424821  [  160/  265]
train() client id: f_00001-3-5 loss: 0.386639  [  192/  265]
train() client id: f_00001-3-6 loss: 0.304603  [  224/  265]
train() client id: f_00001-3-7 loss: 0.503284  [  256/  265]
train() client id: f_00001-4-0 loss: 0.350840  [   32/  265]
train() client id: f_00001-4-1 loss: 0.447909  [   64/  265]
train() client id: f_00001-4-2 loss: 0.373913  [   96/  265]
train() client id: f_00001-4-3 loss: 0.321537  [  128/  265]
train() client id: f_00001-4-4 loss: 0.284064  [  160/  265]
train() client id: f_00001-4-5 loss: 0.362162  [  192/  265]
train() client id: f_00001-4-6 loss: 0.354166  [  224/  265]
train() client id: f_00001-4-7 loss: 0.409779  [  256/  265]
train() client id: f_00001-5-0 loss: 0.478340  [   32/  265]
train() client id: f_00001-5-1 loss: 0.318024  [   64/  265]
train() client id: f_00001-5-2 loss: 0.381804  [   96/  265]
train() client id: f_00001-5-3 loss: 0.273889  [  128/  265]
train() client id: f_00001-5-4 loss: 0.314621  [  160/  265]
train() client id: f_00001-5-5 loss: 0.341186  [  192/  265]
train() client id: f_00001-5-6 loss: 0.314830  [  224/  265]
train() client id: f_00001-5-7 loss: 0.506098  [  256/  265]
train() client id: f_00001-6-0 loss: 0.410621  [   32/  265]
train() client id: f_00001-6-1 loss: 0.314452  [   64/  265]
train() client id: f_00001-6-2 loss: 0.307039  [   96/  265]
train() client id: f_00001-6-3 loss: 0.466634  [  128/  265]
train() client id: f_00001-6-4 loss: 0.263003  [  160/  265]
train() client id: f_00001-6-5 loss: 0.306046  [  192/  265]
train() client id: f_00001-6-6 loss: 0.447601  [  224/  265]
train() client id: f_00001-6-7 loss: 0.386544  [  256/  265]
train() client id: f_00002-0-0 loss: 1.085293  [   32/  124]
train() client id: f_00002-0-1 loss: 1.069201  [   64/  124]
train() client id: f_00002-0-2 loss: 1.079877  [   96/  124]
train() client id: f_00002-1-0 loss: 0.958096  [   32/  124]
train() client id: f_00002-1-1 loss: 1.022889  [   64/  124]
train() client id: f_00002-1-2 loss: 1.069325  [   96/  124]
train() client id: f_00002-2-0 loss: 0.904885  [   32/  124]
train() client id: f_00002-2-1 loss: 1.005051  [   64/  124]
train() client id: f_00002-2-2 loss: 0.872110  [   96/  124]
train() client id: f_00002-3-0 loss: 0.974202  [   32/  124]
train() client id: f_00002-3-1 loss: 1.039213  [   64/  124]
train() client id: f_00002-3-2 loss: 1.020968  [   96/  124]
train() client id: f_00002-4-0 loss: 0.974873  [   32/  124]
train() client id: f_00002-4-1 loss: 0.844706  [   64/  124]
train() client id: f_00002-4-2 loss: 1.006704  [   96/  124]
train() client id: f_00002-5-0 loss: 1.041200  [   32/  124]
train() client id: f_00002-5-1 loss: 1.112973  [   64/  124]
train() client id: f_00002-5-2 loss: 0.700430  [   96/  124]
train() client id: f_00002-6-0 loss: 0.901431  [   32/  124]
train() client id: f_00002-6-1 loss: 0.848485  [   64/  124]
train() client id: f_00002-6-2 loss: 0.963892  [   96/  124]
train() client id: f_00003-0-0 loss: 0.658555  [   32/   43]
train() client id: f_00003-1-0 loss: 0.863708  [   32/   43]
train() client id: f_00003-2-0 loss: 0.769635  [   32/   43]
train() client id: f_00003-3-0 loss: 0.809245  [   32/   43]
train() client id: f_00003-4-0 loss: 0.910479  [   32/   43]
train() client id: f_00003-5-0 loss: 0.751675  [   32/   43]
train() client id: f_00003-6-0 loss: 0.788697  [   32/   43]
train() client id: f_00004-0-0 loss: 0.986949  [   32/  306]
train() client id: f_00004-0-1 loss: 0.881567  [   64/  306]
train() client id: f_00004-0-2 loss: 0.938875  [   96/  306]
train() client id: f_00004-0-3 loss: 0.679454  [  128/  306]
train() client id: f_00004-0-4 loss: 0.787727  [  160/  306]
train() client id: f_00004-0-5 loss: 0.704305  [  192/  306]
train() client id: f_00004-0-6 loss: 0.809230  [  224/  306]
train() client id: f_00004-0-7 loss: 0.802683  [  256/  306]
train() client id: f_00004-0-8 loss: 1.024127  [  288/  306]
train() client id: f_00004-1-0 loss: 0.867679  [   32/  306]
train() client id: f_00004-1-1 loss: 0.998146  [   64/  306]
train() client id: f_00004-1-2 loss: 0.808787  [   96/  306]
train() client id: f_00004-1-3 loss: 0.893388  [  128/  306]
train() client id: f_00004-1-4 loss: 0.764822  [  160/  306]
train() client id: f_00004-1-5 loss: 0.849110  [  192/  306]
train() client id: f_00004-1-6 loss: 0.726397  [  224/  306]
train() client id: f_00004-1-7 loss: 0.852927  [  256/  306]
train() client id: f_00004-1-8 loss: 0.931076  [  288/  306]
train() client id: f_00004-2-0 loss: 0.847703  [   32/  306]
train() client id: f_00004-2-1 loss: 0.722742  [   64/  306]
train() client id: f_00004-2-2 loss: 0.863268  [   96/  306]
train() client id: f_00004-2-3 loss: 0.795171  [  128/  306]
train() client id: f_00004-2-4 loss: 0.881954  [  160/  306]
train() client id: f_00004-2-5 loss: 0.835091  [  192/  306]
train() client id: f_00004-2-6 loss: 0.837091  [  224/  306]
train() client id: f_00004-2-7 loss: 0.798604  [  256/  306]
train() client id: f_00004-2-8 loss: 0.968320  [  288/  306]
train() client id: f_00004-3-0 loss: 0.809062  [   32/  306]
train() client id: f_00004-3-1 loss: 0.905222  [   64/  306]
train() client id: f_00004-3-2 loss: 0.871076  [   96/  306]
train() client id: f_00004-3-3 loss: 0.825700  [  128/  306]
train() client id: f_00004-3-4 loss: 0.743427  [  160/  306]
train() client id: f_00004-3-5 loss: 0.865489  [  192/  306]
train() client id: f_00004-3-6 loss: 0.851497  [  224/  306]
train() client id: f_00004-3-7 loss: 0.910260  [  256/  306]
train() client id: f_00004-3-8 loss: 0.803292  [  288/  306]
train() client id: f_00004-4-0 loss: 0.855191  [   32/  306]
train() client id: f_00004-4-1 loss: 0.853160  [   64/  306]
train() client id: f_00004-4-2 loss: 0.955963  [   96/  306]
train() client id: f_00004-4-3 loss: 0.957521  [  128/  306]
train() client id: f_00004-4-4 loss: 0.947957  [  160/  306]
train() client id: f_00004-4-5 loss: 0.724570  [  192/  306]
train() client id: f_00004-4-6 loss: 0.686951  [  224/  306]
train() client id: f_00004-4-7 loss: 0.796847  [  256/  306]
train() client id: f_00004-4-8 loss: 0.798340  [  288/  306]
train() client id: f_00004-5-0 loss: 0.862499  [   32/  306]
train() client id: f_00004-5-1 loss: 0.760456  [   64/  306]
train() client id: f_00004-5-2 loss: 0.848869  [   96/  306]
train() client id: f_00004-5-3 loss: 0.891162  [  128/  306]
train() client id: f_00004-5-4 loss: 0.846222  [  160/  306]
train() client id: f_00004-5-5 loss: 0.797826  [  192/  306]
train() client id: f_00004-5-6 loss: 0.858941  [  224/  306]
train() client id: f_00004-5-7 loss: 0.778411  [  256/  306]
train() client id: f_00004-5-8 loss: 0.926428  [  288/  306]
train() client id: f_00004-6-0 loss: 0.860183  [   32/  306]
train() client id: f_00004-6-1 loss: 0.916332  [   64/  306]
train() client id: f_00004-6-2 loss: 0.784254  [   96/  306]
train() client id: f_00004-6-3 loss: 0.848065  [  128/  306]
train() client id: f_00004-6-4 loss: 0.960278  [  160/  306]
train() client id: f_00004-6-5 loss: 0.890578  [  192/  306]
train() client id: f_00004-6-6 loss: 0.891587  [  224/  306]
train() client id: f_00004-6-7 loss: 0.822926  [  256/  306]
train() client id: f_00004-6-8 loss: 0.769671  [  288/  306]
train() client id: f_00005-0-0 loss: 0.225762  [   32/  146]
train() client id: f_00005-0-1 loss: 0.459069  [   64/  146]
train() client id: f_00005-0-2 loss: 0.403146  [   96/  146]
train() client id: f_00005-0-3 loss: 0.087556  [  128/  146]
train() client id: f_00005-1-0 loss: 0.304507  [   32/  146]
train() client id: f_00005-1-1 loss: 0.536762  [   64/  146]
train() client id: f_00005-1-2 loss: 0.264954  [   96/  146]
train() client id: f_00005-1-3 loss: 0.349410  [  128/  146]
train() client id: f_00005-2-0 loss: 0.143010  [   32/  146]
train() client id: f_00005-2-1 loss: 0.408597  [   64/  146]
train() client id: f_00005-2-2 loss: 0.332131  [   96/  146]
train() client id: f_00005-2-3 loss: 0.500831  [  128/  146]
train() client id: f_00005-3-0 loss: 0.532280  [   32/  146]
train() client id: f_00005-3-1 loss: 0.231277  [   64/  146]
train() client id: f_00005-3-2 loss: 0.455939  [   96/  146]
train() client id: f_00005-3-3 loss: 0.068410  [  128/  146]
train() client id: f_00005-4-0 loss: 0.363126  [   32/  146]
train() client id: f_00005-4-1 loss: 0.577566  [   64/  146]
train() client id: f_00005-4-2 loss: 0.367001  [   96/  146]
train() client id: f_00005-4-3 loss: 0.217706  [  128/  146]
train() client id: f_00005-5-0 loss: 0.447630  [   32/  146]
train() client id: f_00005-5-1 loss: 0.305178  [   64/  146]
train() client id: f_00005-5-2 loss: 0.557087  [   96/  146]
train() client id: f_00005-5-3 loss: 0.291860  [  128/  146]
train() client id: f_00005-6-0 loss: 0.637445  [   32/  146]
train() client id: f_00005-6-1 loss: 0.126554  [   64/  146]
train() client id: f_00005-6-2 loss: 0.315738  [   96/  146]
train() client id: f_00005-6-3 loss: 0.437523  [  128/  146]
train() client id: f_00006-0-0 loss: 0.630400  [   32/   54]
train() client id: f_00006-1-0 loss: 0.554649  [   32/   54]
train() client id: f_00006-2-0 loss: 0.557901  [   32/   54]
train() client id: f_00006-3-0 loss: 0.523918  [   32/   54]
train() client id: f_00006-4-0 loss: 0.564011  [   32/   54]
train() client id: f_00006-5-0 loss: 0.595148  [   32/   54]
train() client id: f_00006-6-0 loss: 0.529605  [   32/   54]
train() client id: f_00007-0-0 loss: 0.406094  [   32/  179]
train() client id: f_00007-0-1 loss: 0.474639  [   64/  179]
train() client id: f_00007-0-2 loss: 0.654845  [   96/  179]
train() client id: f_00007-0-3 loss: 0.472730  [  128/  179]
train() client id: f_00007-0-4 loss: 0.568482  [  160/  179]
train() client id: f_00007-1-0 loss: 0.378471  [   32/  179]
train() client id: f_00007-1-1 loss: 0.411896  [   64/  179]
train() client id: f_00007-1-2 loss: 0.671624  [   96/  179]
train() client id: f_00007-1-3 loss: 0.565804  [  128/  179]
train() client id: f_00007-1-4 loss: 0.430092  [  160/  179]
train() client id: f_00007-2-0 loss: 0.352639  [   32/  179]
train() client id: f_00007-2-1 loss: 0.509340  [   64/  179]
train() client id: f_00007-2-2 loss: 0.461144  [   96/  179]
train() client id: f_00007-2-3 loss: 0.651846  [  128/  179]
train() client id: f_00007-2-4 loss: 0.457436  [  160/  179]
train() client id: f_00007-3-0 loss: 0.379958  [   32/  179]
train() client id: f_00007-3-1 loss: 0.489430  [   64/  179]
train() client id: f_00007-3-2 loss: 0.349775  [   96/  179]
train() client id: f_00007-3-3 loss: 0.535861  [  128/  179]
train() client id: f_00007-3-4 loss: 0.844472  [  160/  179]
train() client id: f_00007-4-0 loss: 0.493305  [   32/  179]
train() client id: f_00007-4-1 loss: 0.673120  [   64/  179]
train() client id: f_00007-4-2 loss: 0.457581  [   96/  179]
train() client id: f_00007-4-3 loss: 0.340429  [  128/  179]
train() client id: f_00007-4-4 loss: 0.527861  [  160/  179]
train() client id: f_00007-5-0 loss: 0.421541  [   32/  179]
train() client id: f_00007-5-1 loss: 0.534359  [   64/  179]
train() client id: f_00007-5-2 loss: 0.437107  [   96/  179]
train() client id: f_00007-5-3 loss: 0.478066  [  128/  179]
train() client id: f_00007-5-4 loss: 0.486529  [  160/  179]
train() client id: f_00007-6-0 loss: 0.330835  [   32/  179]
train() client id: f_00007-6-1 loss: 0.610021  [   64/  179]
train() client id: f_00007-6-2 loss: 0.626321  [   96/  179]
train() client id: f_00007-6-3 loss: 0.425900  [  128/  179]
train() client id: f_00007-6-4 loss: 0.474490  [  160/  179]
train() client id: f_00008-0-0 loss: 0.671819  [   32/  130]
train() client id: f_00008-0-1 loss: 0.499787  [   64/  130]
train() client id: f_00008-0-2 loss: 0.716114  [   96/  130]
train() client id: f_00008-0-3 loss: 0.517104  [  128/  130]
train() client id: f_00008-1-0 loss: 0.598387  [   32/  130]
train() client id: f_00008-1-1 loss: 0.700804  [   64/  130]
train() client id: f_00008-1-2 loss: 0.564299  [   96/  130]
train() client id: f_00008-1-3 loss: 0.535990  [  128/  130]
train() client id: f_00008-2-0 loss: 0.562608  [   32/  130]
train() client id: f_00008-2-1 loss: 0.635186  [   64/  130]
train() client id: f_00008-2-2 loss: 0.614789  [   96/  130]
train() client id: f_00008-2-3 loss: 0.539619  [  128/  130]
train() client id: f_00008-3-0 loss: 0.587179  [   32/  130]
train() client id: f_00008-3-1 loss: 0.785655  [   64/  130]
train() client id: f_00008-3-2 loss: 0.516199  [   96/  130]
train() client id: f_00008-3-3 loss: 0.522698  [  128/  130]
train() client id: f_00008-4-0 loss: 0.688157  [   32/  130]
train() client id: f_00008-4-1 loss: 0.498244  [   64/  130]
train() client id: f_00008-4-2 loss: 0.625341  [   96/  130]
train() client id: f_00008-4-3 loss: 0.602182  [  128/  130]
train() client id: f_00008-5-0 loss: 0.601530  [   32/  130]
train() client id: f_00008-5-1 loss: 0.526395  [   64/  130]
train() client id: f_00008-5-2 loss: 0.589133  [   96/  130]
train() client id: f_00008-5-3 loss: 0.662053  [  128/  130]
train() client id: f_00008-6-0 loss: 0.606250  [   32/  130]
train() client id: f_00008-6-1 loss: 0.614404  [   64/  130]
train() client id: f_00008-6-2 loss: 0.537250  [   96/  130]
train() client id: f_00008-6-3 loss: 0.645357  [  128/  130]
train() client id: f_00009-0-0 loss: 0.810462  [   32/  118]
train() client id: f_00009-0-1 loss: 0.915080  [   64/  118]
train() client id: f_00009-0-2 loss: 0.796504  [   96/  118]
train() client id: f_00009-1-0 loss: 0.837327  [   32/  118]
train() client id: f_00009-1-1 loss: 0.899725  [   64/  118]
train() client id: f_00009-1-2 loss: 0.934789  [   96/  118]
train() client id: f_00009-2-0 loss: 0.851075  [   32/  118]
train() client id: f_00009-2-1 loss: 0.865808  [   64/  118]
train() client id: f_00009-2-2 loss: 0.847932  [   96/  118]
train() client id: f_00009-3-0 loss: 0.838024  [   32/  118]
train() client id: f_00009-3-1 loss: 0.777032  [   64/  118]
train() client id: f_00009-3-2 loss: 0.906509  [   96/  118]
train() client id: f_00009-4-0 loss: 0.819489  [   32/  118]
train() client id: f_00009-4-1 loss: 0.990771  [   64/  118]
train() client id: f_00009-4-2 loss: 0.788739  [   96/  118]
train() client id: f_00009-5-0 loss: 0.881020  [   32/  118]
train() client id: f_00009-5-1 loss: 0.786862  [   64/  118]
train() client id: f_00009-5-2 loss: 0.853137  [   96/  118]
train() client id: f_00009-6-0 loss: 0.791925  [   32/  118]
train() client id: f_00009-6-1 loss: 0.790857  [   64/  118]
train() client id: f_00009-6-2 loss: 0.889139  [   96/  118]
At round 69 accuracy: 0.6472148541114059
At round 69 training accuracy: 0.5942320590207915
At round 69 training loss: 0.8139308739152065
update_location
xs = -3.905658 4.200318 365.009024 18.811294 0.979296 3.956410 -327.443192 -306.324852 349.663977 -292.060879 
ys = 357.587959 340.555839 1.320614 -327.455176 319.350187 302.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 371.327891 354.959043 378.461797 342.900506 334.642348 318.923287 342.382731 322.235304 364.106532 308.732196 
dists_bs = 250.223174 243.970264 567.251510 538.538784 227.608632 219.913287 234.071702 218.137734 547.774412 207.228720 
uav_gains = -120.234698 -119.437359 -120.549487 -118.771277 -118.270653 -117.207399 -118.740997 -117.443885 -119.896724 -116.437853 
bs_gains = -106.720230 -106.412492 -116.672778 -116.041135 -105.568344 -105.150100 -105.908829 -105.051521 -116.247908 -104.427658 
Round 70
-------------------------------
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.73561523 3.43059311 1.71527241 0.65186493 3.95409396 1.90401388
 0.79238643 2.37939759 1.74146138 1.54414415]
obj_prev = 19.848843077614966
eta_min = 1.1829730818327539e-54	eta_max = 0.9523787254678187
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 4.515956513396188	eta = 0.9090909090909091
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 12.490584420921488	eta = 0.32868077856324013
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 7.913053448586266	eta = 0.518815529157309
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 7.145329232998529	eta = 0.5745592509885677
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 7.096743771047384	eta = 0.5784927770574487
af = 4.105415012178352	bf = 0.7442198316471329	zeta = 7.096517585671171	eta = 0.5785112152005008
eta = 0.5785112152005008
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [0.04516167 0.09498287 0.04444481 0.01541231 0.10967833 0.05233018
 0.019355   0.06415824 0.04659537 0.04229426]
ene_total = [0.7365201  1.0329747  0.74175636 0.39122836 1.17571715 0.60187953
 0.42968967 0.84367489 0.64421599 0.49886084]
ti_comp = [2.36984321 2.57086067 2.35742536 2.41850329 2.57471655 2.57650502
 2.41937284 2.45238669 2.47671475 2.57942159]
ti_coms = [0.28674182 0.08572436 0.29915967 0.23808174 0.08186847 0.08008
 0.23721219 0.20419834 0.17987028 0.07716343]
t_total = [26.49970627 26.49970627 26.49970627 26.49970627 26.49970627 26.49970627
 26.49970627 26.49970627 26.49970627 26.49970627]
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [1.02506317e-06 8.10324115e-06 9.87341466e-07 3.91192234e-08
 1.24389480e-05 1.34919525e-06 7.74201308e-08 2.74447586e-06
 1.03075701e-06 7.10688982e-07]
ene_total = [0.28603811 0.08559175 0.29842467 0.23748935 0.08178872 0.07989408
 0.23662235 0.2037173  0.17943272 0.0769784 ]
optimize_network iter = 0 obj = 1.7659774351917215
eta = 0.5785112152005008
freqs = [ 9528407.84474552 18472970.62942295  9426557.92578857  3186333.13230265
 21299107.60121082 10155265.18397391  4000003.44569053 13080775.34726803
  9406688.61857887  8198399.6009876 ]
eta_min = 0.578511215200502	eta_max = 0.8002666857305597
af = 0.00021898789766085596	bf = 0.7442198316471329	zeta = 0.00024088668742694157	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [1.78511656e-07 1.41115497e-06 1.71942534e-07 6.81249461e-09
 2.16620522e-06 2.34958278e-07 1.34824820e-08 4.77942182e-07
 1.79503221e-07 1.23764340e-07]
ene_total = [1.32614126 0.39652609 1.38357144 1.10108858 0.3787282  0.37036751
 1.09706736 0.9444053  0.83187829 0.35687372]
ti_comp = [0.9721494  1.17316687 0.95973155 1.02080948 1.17702275 1.17881122
 1.02167903 1.05469288 1.07902094 1.18172779]
ti_coms = [0.28674182 0.08572436 0.29915967 0.23808174 0.08186847 0.08008
 0.23721219 0.20419834 0.17987028 0.07716343]
t_total = [26.49970627 26.49970627 26.49970627 26.49970627 26.49970627 26.49970627
 26.49970627 26.49970627 26.49970627 26.49970627]
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [4.11016540e-07 2.62561918e-06 4.01956633e-07 1.48159555e-08
 4.01613458e-06 4.34894323e-07 2.92931455e-08 1.00119980e-06
 3.66424074e-07 2.28467704e-07]
ene_total = [0.60360123 0.18050535 0.62974066 0.50116324 0.17241798 0.16857786
 0.49933314 0.42985932 0.37863531 0.16243412]
optimize_network iter = 1 obj = 3.726268206640733
eta = 0.8002666857305597
freqs = [ 9456245.97160665 16480383.53188894  9426557.92578857  3073300.22792621
 18967836.56712048  9036283.02009842  3856208.50116259 12382497.36052094
  8790118.03381017  7285272.29487025]
eta_min = 0.800266685730563	eta_max = 0.8002666857305585
af = 0.00017982620690873804	bf = 0.7442198316471329	zeta = 0.00019780882759961186	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [1.75818035e-07 1.12314508e-06 1.71942534e-07 6.33773081e-09
 1.71795736e-06 1.86032089e-07 1.25305500e-08 4.28277123e-07
 1.56742989e-07 9.77302347e-08]
ene_total = [1.32614114 0.39651277 1.38357144 1.10108855 0.37870747 0.37036524
 1.09706732 0.944403   0.83187723 0.35687251]
ti_comp = [0.9721494  1.17316687 0.95973155 1.02080948 1.17702275 1.17881122
 1.02167903 1.05469288 1.07902094 1.18172779]
ti_coms = [0.28674182 0.08572436 0.29915967 0.23808174 0.08186847 0.08008
 0.23721219 0.20419834 0.17987028 0.07716343]
t_total = [26.49970627 26.49970627 26.49970627 26.49970627 26.49970627 26.49970627
 26.49970627 26.49970627 26.49970627 26.49970627]
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [4.11016540e-07 2.62561918e-06 4.01956633e-07 1.48159555e-08
 4.01613458e-06 4.34894323e-07 2.92931455e-08 1.00119980e-06
 3.66424074e-07 2.28467704e-07]
ene_total = [0.60360123 0.18050535 0.62974066 0.50116324 0.17241798 0.16857786
 0.49933314 0.42985932 0.37863531 0.16243412]
optimize_network iter = 2 obj = 3.7262682066407105
eta = 0.8002666857305585
freqs = [ 9456245.97160664 16480383.53188894  9426557.92578855  3073300.22792621
 18967836.56712048  9036283.02009842  3856208.50116259 12382497.36052093
  8790118.03381016  7285272.29487025]
Done!
ene_coms = [0.02867418 0.00857244 0.02991597 0.02380817 0.00818685 0.008008
 0.02372122 0.02041983 0.01798703 0.00771634]
ene_comp = [3.94344793e-07 2.51911821e-06 3.85652375e-07 1.42149873e-08
 3.85323121e-06 4.17254039e-07 2.81049502e-08 9.60588899e-07
 3.51561096e-07 2.19200545e-07]
ene_total = [0.02867458 0.00857495 0.02991635 0.02380819 0.0081907  0.00800842
 0.02372125 0.02042079 0.01798738 0.00771656]
At round 70 energy consumption: 0.1770191732645484
At round 70 eta: 0.8002666857305585
At round 70 a_n: 4.204393568402164
At round 70 local rounds: 7.295939587071601
At round 70 global rounds: 21.05003656390746
gradient difference: 0.6778061985969543
train() client id: f_00000-0-0 loss: 0.969960  [   32/  126]
train() client id: f_00000-0-1 loss: 1.074565  [   64/  126]
train() client id: f_00000-0-2 loss: 0.954257  [   96/  126]
train() client id: f_00000-1-0 loss: 0.946648  [   32/  126]
train() client id: f_00000-1-1 loss: 0.869898  [   64/  126]
train() client id: f_00000-1-2 loss: 1.019045  [   96/  126]
train() client id: f_00000-2-0 loss: 0.849231  [   32/  126]
train() client id: f_00000-2-1 loss: 0.976480  [   64/  126]
train() client id: f_00000-2-2 loss: 0.936028  [   96/  126]
train() client id: f_00000-3-0 loss: 0.986993  [   32/  126]
train() client id: f_00000-3-1 loss: 0.843711  [   64/  126]
train() client id: f_00000-3-2 loss: 0.839059  [   96/  126]
train() client id: f_00000-4-0 loss: 0.935950  [   32/  126]
train() client id: f_00000-4-1 loss: 0.940938  [   64/  126]
train() client id: f_00000-4-2 loss: 0.874566  [   96/  126]
train() client id: f_00000-5-0 loss: 0.769123  [   32/  126]
train() client id: f_00000-5-1 loss: 0.936162  [   64/  126]
train() client id: f_00000-5-2 loss: 1.034061  [   96/  126]
train() client id: f_00000-6-0 loss: 0.853594  [   32/  126]
train() client id: f_00000-6-1 loss: 0.970813  [   64/  126]
train() client id: f_00000-6-2 loss: 0.853865  [   96/  126]
train() client id: f_00001-0-0 loss: 0.451918  [   32/  265]
train() client id: f_00001-0-1 loss: 0.374336  [   64/  265]
train() client id: f_00001-0-2 loss: 0.503088  [   96/  265]
train() client id: f_00001-0-3 loss: 0.397442  [  128/  265]
train() client id: f_00001-0-4 loss: 0.430500  [  160/  265]
train() client id: f_00001-0-5 loss: 0.452804  [  192/  265]
train() client id: f_00001-0-6 loss: 0.387340  [  224/  265]
train() client id: f_00001-0-7 loss: 0.472710  [  256/  265]
train() client id: f_00001-1-0 loss: 0.491282  [   32/  265]
train() client id: f_00001-1-1 loss: 0.467067  [   64/  265]
train() client id: f_00001-1-2 loss: 0.406037  [   96/  265]
train() client id: f_00001-1-3 loss: 0.429738  [  128/  265]
train() client id: f_00001-1-4 loss: 0.372581  [  160/  265]
train() client id: f_00001-1-5 loss: 0.489958  [  192/  265]
train() client id: f_00001-1-6 loss: 0.405977  [  224/  265]
train() client id: f_00001-1-7 loss: 0.522477  [  256/  265]
train() client id: f_00001-2-0 loss: 0.438035  [   32/  265]
train() client id: f_00001-2-1 loss: 0.497985  [   64/  265]
train() client id: f_00001-2-2 loss: 0.425388  [   96/  265]
train() client id: f_00001-2-3 loss: 0.387204  [  128/  265]
train() client id: f_00001-2-4 loss: 0.548091  [  160/  265]
train() client id: f_00001-2-5 loss: 0.343663  [  192/  265]
train() client id: f_00001-2-6 loss: 0.417805  [  224/  265]
train() client id: f_00001-2-7 loss: 0.413810  [  256/  265]
train() client id: f_00001-3-0 loss: 0.537964  [   32/  265]
train() client id: f_00001-3-1 loss: 0.400601  [   64/  265]
train() client id: f_00001-3-2 loss: 0.382402  [   96/  265]
train() client id: f_00001-3-3 loss: 0.584482  [  128/  265]
train() client id: f_00001-3-4 loss: 0.411421  [  160/  265]
train() client id: f_00001-3-5 loss: 0.393094  [  192/  265]
train() client id: f_00001-3-6 loss: 0.342655  [  224/  265]
train() client id: f_00001-3-7 loss: 0.414219  [  256/  265]
train() client id: f_00001-4-0 loss: 0.489719  [   32/  265]
train() client id: f_00001-4-1 loss: 0.341583  [   64/  265]
train() client id: f_00001-4-2 loss: 0.472809  [   96/  265]
train() client id: f_00001-4-3 loss: 0.415619  [  128/  265]
train() client id: f_00001-4-4 loss: 0.427449  [  160/  265]
train() client id: f_00001-4-5 loss: 0.413439  [  192/  265]
train() client id: f_00001-4-6 loss: 0.513403  [  224/  265]
train() client id: f_00001-4-7 loss: 0.413837  [  256/  265]
train() client id: f_00001-5-0 loss: 0.446932  [   32/  265]
train() client id: f_00001-5-1 loss: 0.385987  [   64/  265]
train() client id: f_00001-5-2 loss: 0.352588  [   96/  265]
train() client id: f_00001-5-3 loss: 0.337169  [  128/  265]
train() client id: f_00001-5-4 loss: 0.423599  [  160/  265]
train() client id: f_00001-5-5 loss: 0.632118  [  192/  265]
train() client id: f_00001-5-6 loss: 0.495177  [  224/  265]
train() client id: f_00001-5-7 loss: 0.413653  [  256/  265]
train() client id: f_00001-6-0 loss: 0.391324  [   32/  265]
train() client id: f_00001-6-1 loss: 0.428021  [   64/  265]
train() client id: f_00001-6-2 loss: 0.410077  [   96/  265]
train() client id: f_00001-6-3 loss: 0.368924  [  128/  265]
train() client id: f_00001-6-4 loss: 0.408211  [  160/  265]
train() client id: f_00001-6-5 loss: 0.370606  [  192/  265]
train() client id: f_00001-6-6 loss: 0.541580  [  224/  265]
train() client id: f_00001-6-7 loss: 0.531314  [  256/  265]
train() client id: f_00002-0-0 loss: 0.999445  [   32/  124]
train() client id: f_00002-0-1 loss: 1.016222  [   64/  124]
train() client id: f_00002-0-2 loss: 1.003592  [   96/  124]
train() client id: f_00002-1-0 loss: 1.135167  [   32/  124]
train() client id: f_00002-1-1 loss: 0.975428  [   64/  124]
train() client id: f_00002-1-2 loss: 0.868670  [   96/  124]
train() client id: f_00002-2-0 loss: 1.002367  [   32/  124]
train() client id: f_00002-2-1 loss: 0.886589  [   64/  124]
train() client id: f_00002-2-2 loss: 0.830801  [   96/  124]
train() client id: f_00002-3-0 loss: 0.789551  [   32/  124]
train() client id: f_00002-3-1 loss: 1.029222  [   64/  124]
train() client id: f_00002-3-2 loss: 1.000182  [   96/  124]
train() client id: f_00002-4-0 loss: 0.982275  [   32/  124]
train() client id: f_00002-4-1 loss: 0.739109  [   64/  124]
train() client id: f_00002-4-2 loss: 0.904881  [   96/  124]
train() client id: f_00002-5-0 loss: 0.888833  [   32/  124]
train() client id: f_00002-5-1 loss: 1.019316  [   64/  124]
train() client id: f_00002-5-2 loss: 0.714992  [   96/  124]
train() client id: f_00002-6-0 loss: 0.968173  [   32/  124]
train() client id: f_00002-6-1 loss: 0.954802  [   64/  124]
train() client id: f_00002-6-2 loss: 0.987233  [   96/  124]
train() client id: f_00003-0-0 loss: 0.782472  [   32/   43]
train() client id: f_00003-1-0 loss: 0.635445  [   32/   43]
train() client id: f_00003-2-0 loss: 0.340441  [   32/   43]
train() client id: f_00003-3-0 loss: 0.554107  [   32/   43]
train() client id: f_00003-4-0 loss: 0.612203  [   32/   43]
train() client id: f_00003-5-0 loss: 0.672614  [   32/   43]
train() client id: f_00003-6-0 loss: 0.549986  [   32/   43]
train() client id: f_00004-0-0 loss: 0.920954  [   32/  306]
train() client id: f_00004-0-1 loss: 0.859574  [   64/  306]
train() client id: f_00004-0-2 loss: 0.909777  [   96/  306]
train() client id: f_00004-0-3 loss: 0.923015  [  128/  306]
train() client id: f_00004-0-4 loss: 0.869664  [  160/  306]
train() client id: f_00004-0-5 loss: 0.745818  [  192/  306]
train() client id: f_00004-0-6 loss: 0.936967  [  224/  306]
train() client id: f_00004-0-7 loss: 0.998631  [  256/  306]
train() client id: f_00004-0-8 loss: 1.002807  [  288/  306]
train() client id: f_00004-1-0 loss: 0.847966  [   32/  306]
train() client id: f_00004-1-1 loss: 0.958907  [   64/  306]
train() client id: f_00004-1-2 loss: 0.752130  [   96/  306]
train() client id: f_00004-1-3 loss: 0.870056  [  128/  306]
train() client id: f_00004-1-4 loss: 0.949192  [  160/  306]
train() client id: f_00004-1-5 loss: 0.872419  [  192/  306]
train() client id: f_00004-1-6 loss: 0.786681  [  224/  306]
train() client id: f_00004-1-7 loss: 0.876209  [  256/  306]
train() client id: f_00004-1-8 loss: 1.127565  [  288/  306]
train() client id: f_00004-2-0 loss: 0.856457  [   32/  306]
train() client id: f_00004-2-1 loss: 0.980898  [   64/  306]
train() client id: f_00004-2-2 loss: 0.897543  [   96/  306]
train() client id: f_00004-2-3 loss: 0.905319  [  128/  306]
train() client id: f_00004-2-4 loss: 0.873353  [  160/  306]
train() client id: f_00004-2-5 loss: 0.873547  [  192/  306]
train() client id: f_00004-2-6 loss: 0.874200  [  224/  306]
train() client id: f_00004-2-7 loss: 0.708466  [  256/  306]
train() client id: f_00004-2-8 loss: 1.085766  [  288/  306]
train() client id: f_00004-3-0 loss: 0.963776  [   32/  306]
train() client id: f_00004-3-1 loss: 0.827876  [   64/  306]
train() client id: f_00004-3-2 loss: 0.839978  [   96/  306]
train() client id: f_00004-3-3 loss: 0.894462  [  128/  306]
train() client id: f_00004-3-4 loss: 0.857717  [  160/  306]
train() client id: f_00004-3-5 loss: 0.866996  [  192/  306]
train() client id: f_00004-3-6 loss: 1.053527  [  224/  306]
train() client id: f_00004-3-7 loss: 0.892787  [  256/  306]
train() client id: f_00004-3-8 loss: 0.827286  [  288/  306]
train() client id: f_00004-4-0 loss: 0.963209  [   32/  306]
train() client id: f_00004-4-1 loss: 0.910221  [   64/  306]
train() client id: f_00004-4-2 loss: 0.800575  [   96/  306]
train() client id: f_00004-4-3 loss: 0.918895  [  128/  306]
train() client id: f_00004-4-4 loss: 0.920640  [  160/  306]
train() client id: f_00004-4-5 loss: 0.740683  [  192/  306]
train() client id: f_00004-4-6 loss: 0.903034  [  224/  306]
train() client id: f_00004-4-7 loss: 0.922054  [  256/  306]
train() client id: f_00004-4-8 loss: 0.990655  [  288/  306]
train() client id: f_00004-5-0 loss: 0.854351  [   32/  306]
train() client id: f_00004-5-1 loss: 0.903718  [   64/  306]
train() client id: f_00004-5-2 loss: 0.895178  [   96/  306]
train() client id: f_00004-5-3 loss: 0.959606  [  128/  306]
train() client id: f_00004-5-4 loss: 0.887847  [  160/  306]
train() client id: f_00004-5-5 loss: 0.902897  [  192/  306]
train() client id: f_00004-5-6 loss: 0.931710  [  224/  306]
train() client id: f_00004-5-7 loss: 0.764374  [  256/  306]
train() client id: f_00004-5-8 loss: 0.970739  [  288/  306]
train() client id: f_00004-6-0 loss: 0.900747  [   32/  306]
train() client id: f_00004-6-1 loss: 0.950072  [   64/  306]
train() client id: f_00004-6-2 loss: 0.787097  [   96/  306]
train() client id: f_00004-6-3 loss: 0.885230  [  128/  306]
train() client id: f_00004-6-4 loss: 0.905230  [  160/  306]
train() client id: f_00004-6-5 loss: 0.808847  [  192/  306]
train() client id: f_00004-6-6 loss: 0.961750  [  224/  306]
train() client id: f_00004-6-7 loss: 0.847553  [  256/  306]
train() client id: f_00004-6-8 loss: 0.914344  [  288/  306]
train() client id: f_00005-0-0 loss: 0.854890  [   32/  146]
train() client id: f_00005-0-1 loss: 0.512673  [   64/  146]
train() client id: f_00005-0-2 loss: 0.590216  [   96/  146]
train() client id: f_00005-0-3 loss: 0.375305  [  128/  146]
train() client id: f_00005-1-0 loss: 0.483389  [   32/  146]
train() client id: f_00005-1-1 loss: 0.661027  [   64/  146]
train() client id: f_00005-1-2 loss: 0.641500  [   96/  146]
train() client id: f_00005-1-3 loss: 0.555855  [  128/  146]
train() client id: f_00005-2-0 loss: 0.617896  [   32/  146]
train() client id: f_00005-2-1 loss: 0.298758  [   64/  146]
train() client id: f_00005-2-2 loss: 0.716068  [   96/  146]
train() client id: f_00005-2-3 loss: 0.650308  [  128/  146]
train() client id: f_00005-3-0 loss: 0.680090  [   32/  146]
train() client id: f_00005-3-1 loss: 0.482173  [   64/  146]
train() client id: f_00005-3-2 loss: 0.495825  [   96/  146]
train() client id: f_00005-3-3 loss: 0.687491  [  128/  146]
train() client id: f_00005-4-0 loss: 0.767088  [   32/  146]
train() client id: f_00005-4-1 loss: 0.561918  [   64/  146]
train() client id: f_00005-4-2 loss: 0.502458  [   96/  146]
train() client id: f_00005-4-3 loss: 0.490679  [  128/  146]
train() client id: f_00005-5-0 loss: 0.412699  [   32/  146]
train() client id: f_00005-5-1 loss: 0.392658  [   64/  146]
train() client id: f_00005-5-2 loss: 0.706712  [   96/  146]
train() client id: f_00005-5-3 loss: 0.481756  [  128/  146]
train() client id: f_00005-6-0 loss: 0.510753  [   32/  146]
train() client id: f_00005-6-1 loss: 0.574979  [   64/  146]
train() client id: f_00005-6-2 loss: 0.552110  [   96/  146]
train() client id: f_00005-6-3 loss: 0.662492  [  128/  146]
train() client id: f_00006-0-0 loss: 0.556858  [   32/   54]
train() client id: f_00006-1-0 loss: 0.515204  [   32/   54]
train() client id: f_00006-2-0 loss: 0.527026  [   32/   54]
train() client id: f_00006-3-0 loss: 0.554779  [   32/   54]
train() client id: f_00006-4-0 loss: 0.515070  [   32/   54]
train() client id: f_00006-5-0 loss: 0.527725  [   32/   54]
train() client id: f_00006-6-0 loss: 0.462687  [   32/   54]
train() client id: f_00007-0-0 loss: 0.619565  [   32/  179]
train() client id: f_00007-0-1 loss: 0.730920  [   64/  179]
train() client id: f_00007-0-2 loss: 0.658835  [   96/  179]
train() client id: f_00007-0-3 loss: 0.508173  [  128/  179]
train() client id: f_00007-0-4 loss: 0.520215  [  160/  179]
train() client id: f_00007-1-0 loss: 0.672191  [   32/  179]
train() client id: f_00007-1-1 loss: 0.691590  [   64/  179]
train() client id: f_00007-1-2 loss: 0.553896  [   96/  179]
train() client id: f_00007-1-3 loss: 0.599974  [  128/  179]
train() client id: f_00007-1-4 loss: 0.582291  [  160/  179]
train() client id: f_00007-2-0 loss: 0.498929  [   32/  179]
train() client id: f_00007-2-1 loss: 0.637024  [   64/  179]
train() client id: f_00007-2-2 loss: 0.658199  [   96/  179]
train() client id: f_00007-2-3 loss: 0.663046  [  128/  179]
train() client id: f_00007-2-4 loss: 0.579839  [  160/  179]
train() client id: f_00007-3-0 loss: 0.630288  [   32/  179]
train() client id: f_00007-3-1 loss: 0.614628  [   64/  179]
train() client id: f_00007-3-2 loss: 0.630154  [   96/  179]
train() client id: f_00007-3-3 loss: 0.720267  [  128/  179]
train() client id: f_00007-3-4 loss: 0.483714  [  160/  179]
train() client id: f_00007-4-0 loss: 0.585502  [   32/  179]
train() client id: f_00007-4-1 loss: 0.816326  [   64/  179]
train() client id: f_00007-4-2 loss: 0.542179  [   96/  179]
train() client id: f_00007-4-3 loss: 0.554908  [  128/  179]
train() client id: f_00007-4-4 loss: 0.625532  [  160/  179]
train() client id: f_00007-5-0 loss: 0.498199  [   32/  179]
train() client id: f_00007-5-1 loss: 0.520207  [   64/  179]
train() client id: f_00007-5-2 loss: 0.817993  [   96/  179]
train() client id: f_00007-5-3 loss: 0.433522  [  128/  179]
train() client id: f_00007-5-4 loss: 0.655550  [  160/  179]
train() client id: f_00007-6-0 loss: 0.620165  [   32/  179]
train() client id: f_00007-6-1 loss: 0.706483  [   64/  179]
train() client id: f_00007-6-2 loss: 0.582799  [   96/  179]
train() client id: f_00007-6-3 loss: 0.674099  [  128/  179]
train() client id: f_00007-6-4 loss: 0.462496  [  160/  179]
train() client id: f_00008-0-0 loss: 0.766319  [   32/  130]
train() client id: f_00008-0-1 loss: 0.708409  [   64/  130]
train() client id: f_00008-0-2 loss: 0.537227  [   96/  130]
train() client id: f_00008-0-3 loss: 0.659654  [  128/  130]
train() client id: f_00008-1-0 loss: 0.663369  [   32/  130]
train() client id: f_00008-1-1 loss: 0.695097  [   64/  130]
train() client id: f_00008-1-2 loss: 0.650417  [   96/  130]
train() client id: f_00008-1-3 loss: 0.703859  [  128/  130]
train() client id: f_00008-2-0 loss: 0.695680  [   32/  130]
train() client id: f_00008-2-1 loss: 0.688696  [   64/  130]
train() client id: f_00008-2-2 loss: 0.628560  [   96/  130]
train() client id: f_00008-2-3 loss: 0.697184  [  128/  130]
train() client id: f_00008-3-0 loss: 0.681795  [   32/  130]
train() client id: f_00008-3-1 loss: 0.610215  [   64/  130]
train() client id: f_00008-3-2 loss: 0.789929  [   96/  130]
train() client id: f_00008-3-3 loss: 0.629227  [  128/  130]
train() client id: f_00008-4-0 loss: 0.822666  [   32/  130]
train() client id: f_00008-4-1 loss: 0.648543  [   64/  130]
train() client id: f_00008-4-2 loss: 0.613400  [   96/  130]
train() client id: f_00008-4-3 loss: 0.620871  [  128/  130]
train() client id: f_00008-5-0 loss: 0.736339  [   32/  130]
train() client id: f_00008-5-1 loss: 0.702429  [   64/  130]
train() client id: f_00008-5-2 loss: 0.617287  [   96/  130]
train() client id: f_00008-5-3 loss: 0.635669  [  128/  130]
train() client id: f_00008-6-0 loss: 0.700680  [   32/  130]
train() client id: f_00008-6-1 loss: 0.686877  [   64/  130]
train() client id: f_00008-6-2 loss: 0.657443  [   96/  130]
train() client id: f_00008-6-3 loss: 0.647044  [  128/  130]
train() client id: f_00009-0-0 loss: 0.788021  [   32/  118]
train() client id: f_00009-0-1 loss: 0.966791  [   64/  118]
train() client id: f_00009-0-2 loss: 0.877384  [   96/  118]
train() client id: f_00009-1-0 loss: 0.862651  [   32/  118]
train() client id: f_00009-1-1 loss: 0.926897  [   64/  118]
train() client id: f_00009-1-2 loss: 0.789450  [   96/  118]
train() client id: f_00009-2-0 loss: 0.888081  [   32/  118]
train() client id: f_00009-2-1 loss: 0.843917  [   64/  118]
train() client id: f_00009-2-2 loss: 0.754408  [   96/  118]
train() client id: f_00009-3-0 loss: 0.641510  [   32/  118]
train() client id: f_00009-3-1 loss: 0.725803  [   64/  118]
train() client id: f_00009-3-2 loss: 1.073822  [   96/  118]
train() client id: f_00009-4-0 loss: 0.903479  [   32/  118]
train() client id: f_00009-4-1 loss: 0.714964  [   64/  118]
train() client id: f_00009-4-2 loss: 0.759248  [   96/  118]
train() client id: f_00009-5-0 loss: 0.674137  [   32/  118]
train() client id: f_00009-5-1 loss: 0.845071  [   64/  118]
train() client id: f_00009-5-2 loss: 0.770146  [   96/  118]
train() client id: f_00009-6-0 loss: 0.645300  [   32/  118]
train() client id: f_00009-6-1 loss: 0.821413  [   64/  118]
train() client id: f_00009-6-2 loss: 0.905360  [   96/  118]
At round 70 accuracy: 0.649867374005305
At round 70 training accuracy: 0.5875251509054326
At round 70 training loss: 0.8347852975623955
update_location
xs = -3.905658 4.200318 370.009024 18.811294 0.979296 3.956410 -332.443192 -311.324852 354.663977 -297.060879 
ys = 362.587959 345.555839 1.320614 -332.455176 324.350187 307.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 376.145294 359.758920 383.286344 347.678456 339.417151 323.674535 347.167634 326.992109 368.910838 313.466390 
dists_bs = 253.894695 247.390477 572.013431 543.206750 230.812025 222.856445 237.358366 221.183905 552.565332 210.093673 
uav_gains = -120.449229 -119.682988 -120.752598 -119.043997 -118.564750 -117.544555 -119.015407 -117.771801 -120.123842 -116.803129 
bs_gains = -106.897361 -106.581783 -116.774433 -116.146084 -105.738295 -105.311765 -106.078386 -105.220158 -116.353801 -104.594622 
Round 71
-------------------------------
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.59747704 3.1514096  1.57881481 0.60189948 3.63223305 1.7491537
 0.73097414 2.18851291 1.6003418  1.41859333]
obj_prev = 18.24940985510058
eta_min = 2.2153155362897003e-59	eta_max = 0.954951968660827
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 4.14802660303202	eta = 0.909090909090909
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 11.665880506041221	eta = 0.323244634087488
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 7.328813927254386	eta = 0.5145352730897301
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 6.605790167817666	eta = 0.5708527185521312
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 6.559990536518958	eta = 0.5748382188192438
af = 3.7709332754836535	bf = 0.7011262629418353	zeta = 6.55977636966569	eta = 0.5748569864243457
eta = 0.5748569864243457
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [0.04568455 0.09608258 0.0449594  0.01559076 0.11094819 0.05293607
 0.01957909 0.06490107 0.04713485 0.04278394]
ene_total = [0.68305847 0.95139002 0.68780539 0.3652074  1.08286069 0.55421391
 0.40065058 0.78194702 0.59332082 0.45932206]
ti_comp = [2.6166745  2.82525406 2.60419719 2.66565241 2.82917673 2.83103203
 2.66651706 2.69997525 2.72996789 2.83397532]
ti_coms = [0.29511977 0.0865402  0.30759708 0.24614186 0.08261753 0.08076224
 0.24527721 0.21181901 0.18182637 0.07781895]
t_total = [26.44970207 26.44970207 26.44970207 26.44970207 26.44970207 26.44970207
 26.44970207 26.44970207 26.44970207 26.44970207]
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [8.70339434e-07 6.94542833e-06 8.37517236e-07 3.33330274e-08
 1.06640102e-05 1.15676577e-06 6.59734165e-08 2.34377520e-06
 8.78195905e-07 6.09439114e-07]
ene_total = [0.26808419 0.07867313 0.27941784 0.2235868  0.07514369 0.07337205
 0.22280168 0.19243014 0.16517258 0.0706935 ]
optimize_network iter = 0 obj = 1.6493755938248962
eta = 0.5748569864243457
freqs = [ 8729505.68014144 17004237.96072038  8632103.28335727  2924379.60099508
 19607858.17168588  9349252.32733673  3671285.85880158 12018826.22316111
  8632858.47902665  7548397.32371458]
eta_min = 0.5748569864243458	eta_max = 0.8123930873707683
af = 0.00017023437323749214	bf = 0.7011262629418353	zeta = 0.00018725781056124136	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [1.49832217e-07 1.19568169e-06 1.44181752e-07 5.73840643e-09
 1.83584959e-06 1.99141592e-07 1.13575726e-08 4.03489744e-07
 1.51184738e-07 1.04917243e-07]
ene_total = [1.25368474 0.36767638 1.30668851 1.04561886 0.35103995 0.34308906
 1.04194602 0.8998312  0.77241077 0.33058187]
ti_comp = [0.98979546 1.19837503 0.97731815 1.03877337 1.2022977  1.20415299
 1.03963802 1.07309621 1.10308885 1.20709628]
ti_coms = [0.29511977 0.0865402  0.30759708 0.24614186 0.08261753 0.08076224
 0.24527721 0.21181901 0.18182637 0.07781895]
t_total = [26.44970207 26.44970207 26.44970207 26.44970207 26.44970207 26.44970207
 26.44970207 26.44970207 26.44970207 26.44970207]
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [3.21501828e-07 2.04039780e-06 3.14308302e-07 1.16018294e-08
 3.12106616e-06 3.37954282e-07 2.29392754e-08 7.84231878e-07
 2.84296680e-07 1.77551736e-07]
ene_total = [0.60750428 0.17818314 0.63318841 0.50667793 0.17013065 0.16625427
 0.5048983  0.436041   0.37429151 0.16019228]
optimize_network iter = 1 obj = 3.7373617634764136
eta = 0.8123930873707683
freqs = [ 8660759.82156696 15044717.2754607   8632103.28335727  2816297.66515272
 17315710.58589685  8249014.9990354   3533804.43880649 11348678.03144383
  8017957.73562188  6650755.80408876]
eta_min = 0.8123930873707704	eta_max = 0.8123930873707671
af = 0.0001378387179910577	bf = 0.7011262629418353	zeta = 0.0001516225897901635	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [1.47481618e-07 9.35985870e-07 1.44181752e-07 5.32207417e-09
 1.43171779e-06 1.55028805e-07 1.05228684e-08 3.59748455e-07
 1.30414606e-07 8.14478020e-08]
ene_total = [1.25368464 0.36766535 1.30668851 1.04561884 0.35102278 0.34308719
 1.04194599 0.89982934 0.77240989 0.33058088]
ti_comp = [0.98979546 1.19837503 0.97731815 1.03877337 1.2022977  1.20415299
 1.03963802 1.07309621 1.10308885 1.20709628]
ti_coms = [0.29511977 0.0865402  0.30759708 0.24614186 0.08261753 0.08076224
 0.24527721 0.21181901 0.18182637 0.07781895]
t_total = [26.44970207 26.44970207 26.44970207 26.44970207 26.44970207 26.44970207
 26.44970207 26.44970207 26.44970207 26.44970207]
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [3.21501828e-07 2.04039780e-06 3.14308302e-07 1.16018294e-08
 3.12106616e-06 3.37954282e-07 2.29392754e-08 7.84231878e-07
 2.84296680e-07 1.77551736e-07]
ene_total = [0.60750428 0.17818314 0.63318841 0.50667793 0.17013065 0.16625427
 0.5048983  0.436041   0.37429151 0.16019228]
optimize_network iter = 2 obj = 3.7373617634763896
eta = 0.8123930873707671
freqs = [ 8660759.82156695 15044717.2754607   8632103.28335726  2816297.66515272
 17315710.58589685  8249014.9990354   3533804.43880649 11348678.03144382
  8017957.73562187  6650755.80408876]
Done!
ene_coms = [0.02951198 0.00865402 0.03075971 0.02461419 0.00826175 0.00807622
 0.02452772 0.0211819  0.01818264 0.00778189]
ene_comp = [2.83533115e-07 1.79943097e-06 2.77189130e-07 1.02316770e-08
 2.75247459e-06 2.98042568e-07 2.02301936e-08 6.91615688e-07
 2.50721820e-07 1.56583237e-07]
ene_total = [0.02951226 0.00865582 0.03075999 0.0246142  0.00826451 0.00807652
 0.02452774 0.02118259 0.01818289 0.00778205]
At round 71 energy consumption: 0.18155856217199728
At round 71 eta: 0.8123930873707671
At round 71 a_n: 3.8618477214328486
At round 71 local rounds: 6.803476774877765
At round 71 global rounds: 20.584783723108377
gradient difference: 0.655133068561554
train() client id: f_00000-0-0 loss: 0.917738  [   32/  126]
train() client id: f_00000-0-1 loss: 0.851413  [   64/  126]
train() client id: f_00000-0-2 loss: 1.016550  [   96/  126]
train() client id: f_00000-1-0 loss: 1.057230  [   32/  126]
train() client id: f_00000-1-1 loss: 0.937337  [   64/  126]
train() client id: f_00000-1-2 loss: 0.927564  [   96/  126]
train() client id: f_00000-2-0 loss: 1.063434  [   32/  126]
train() client id: f_00000-2-1 loss: 0.852697  [   64/  126]
train() client id: f_00000-2-2 loss: 0.851297  [   96/  126]
train() client id: f_00000-3-0 loss: 0.764697  [   32/  126]
train() client id: f_00000-3-1 loss: 0.846871  [   64/  126]
train() client id: f_00000-3-2 loss: 1.032950  [   96/  126]
train() client id: f_00000-4-0 loss: 1.095805  [   32/  126]
train() client id: f_00000-4-1 loss: 0.847642  [   64/  126]
train() client id: f_00000-4-2 loss: 0.761712  [   96/  126]
train() client id: f_00000-5-0 loss: 0.894456  [   32/  126]
train() client id: f_00000-5-1 loss: 0.871929  [   64/  126]
train() client id: f_00000-5-2 loss: 0.905282  [   96/  126]
train() client id: f_00001-0-0 loss: 0.311785  [   32/  265]
train() client id: f_00001-0-1 loss: 0.468030  [   64/  265]
train() client id: f_00001-0-2 loss: 0.383159  [   96/  265]
train() client id: f_00001-0-3 loss: 0.533245  [  128/  265]
train() client id: f_00001-0-4 loss: 0.383073  [  160/  265]
train() client id: f_00001-0-5 loss: 0.342008  [  192/  265]
train() client id: f_00001-0-6 loss: 0.423201  [  224/  265]
train() client id: f_00001-0-7 loss: 0.297421  [  256/  265]
train() client id: f_00001-1-0 loss: 0.310474  [   32/  265]
train() client id: f_00001-1-1 loss: 0.454201  [   64/  265]
train() client id: f_00001-1-2 loss: 0.342591  [   96/  265]
train() client id: f_00001-1-3 loss: 0.309138  [  128/  265]
train() client id: f_00001-1-4 loss: 0.345684  [  160/  265]
train() client id: f_00001-1-5 loss: 0.331438  [  192/  265]
train() client id: f_00001-1-6 loss: 0.458958  [  224/  265]
train() client id: f_00001-1-7 loss: 0.422574  [  256/  265]
train() client id: f_00001-2-0 loss: 0.438644  [   32/  265]
train() client id: f_00001-2-1 loss: 0.448886  [   64/  265]
train() client id: f_00001-2-2 loss: 0.313097  [   96/  265]
train() client id: f_00001-2-3 loss: 0.298289  [  128/  265]
train() client id: f_00001-2-4 loss: 0.423489  [  160/  265]
train() client id: f_00001-2-5 loss: 0.384245  [  192/  265]
train() client id: f_00001-2-6 loss: 0.341724  [  224/  265]
train() client id: f_00001-2-7 loss: 0.387105  [  256/  265]
train() client id: f_00001-3-0 loss: 0.340535  [   32/  265]
train() client id: f_00001-3-1 loss: 0.424098  [   64/  265]
train() client id: f_00001-3-2 loss: 0.318164  [   96/  265]
train() client id: f_00001-3-3 loss: 0.507850  [  128/  265]
train() client id: f_00001-3-4 loss: 0.370951  [  160/  265]
train() client id: f_00001-3-5 loss: 0.286155  [  192/  265]
train() client id: f_00001-3-6 loss: 0.316479  [  224/  265]
train() client id: f_00001-3-7 loss: 0.349765  [  256/  265]
train() client id: f_00001-4-0 loss: 0.339182  [   32/  265]
train() client id: f_00001-4-1 loss: 0.456758  [   64/  265]
train() client id: f_00001-4-2 loss: 0.292491  [   96/  265]
train() client id: f_00001-4-3 loss: 0.528169  [  128/  265]
train() client id: f_00001-4-4 loss: 0.346608  [  160/  265]
train() client id: f_00001-4-5 loss: 0.335667  [  192/  265]
train() client id: f_00001-4-6 loss: 0.329599  [  224/  265]
train() client id: f_00001-4-7 loss: 0.342401  [  256/  265]
train() client id: f_00001-5-0 loss: 0.347879  [   32/  265]
train() client id: f_00001-5-1 loss: 0.356797  [   64/  265]
train() client id: f_00001-5-2 loss: 0.276577  [   96/  265]
train() client id: f_00001-5-3 loss: 0.545031  [  128/  265]
train() client id: f_00001-5-4 loss: 0.308205  [  160/  265]
train() client id: f_00001-5-5 loss: 0.465337  [  192/  265]
train() client id: f_00001-5-6 loss: 0.363243  [  224/  265]
train() client id: f_00001-5-7 loss: 0.294570  [  256/  265]
train() client id: f_00002-0-0 loss: 0.859671  [   32/  124]
train() client id: f_00002-0-1 loss: 0.945205  [   64/  124]
train() client id: f_00002-0-2 loss: 0.900718  [   96/  124]
train() client id: f_00002-1-0 loss: 0.729071  [   32/  124]
train() client id: f_00002-1-1 loss: 0.934624  [   64/  124]
train() client id: f_00002-1-2 loss: 0.930142  [   96/  124]
train() client id: f_00002-2-0 loss: 0.842671  [   32/  124]
train() client id: f_00002-2-1 loss: 0.858878  [   64/  124]
train() client id: f_00002-2-2 loss: 0.927098  [   96/  124]
train() client id: f_00002-3-0 loss: 0.957495  [   32/  124]
train() client id: f_00002-3-1 loss: 0.834191  [   64/  124]
train() client id: f_00002-3-2 loss: 0.899562  [   96/  124]
train() client id: f_00002-4-0 loss: 0.801479  [   32/  124]
train() client id: f_00002-4-1 loss: 0.685861  [   64/  124]
train() client id: f_00002-4-2 loss: 0.954091  [   96/  124]
train() client id: f_00002-5-0 loss: 0.760728  [   32/  124]
train() client id: f_00002-5-1 loss: 0.805349  [   64/  124]
train() client id: f_00002-5-2 loss: 0.736945  [   96/  124]
train() client id: f_00003-0-0 loss: 0.780809  [   32/   43]
train() client id: f_00003-1-0 loss: 0.678052  [   32/   43]
train() client id: f_00003-2-0 loss: 0.506787  [   32/   43]
train() client id: f_00003-3-0 loss: 0.915051  [   32/   43]
train() client id: f_00003-4-0 loss: 0.751232  [   32/   43]
train() client id: f_00003-5-0 loss: 0.629086  [   32/   43]
train() client id: f_00004-0-0 loss: 0.800581  [   32/  306]
train() client id: f_00004-0-1 loss: 0.760316  [   64/  306]
train() client id: f_00004-0-2 loss: 0.832423  [   96/  306]
train() client id: f_00004-0-3 loss: 0.808998  [  128/  306]
train() client id: f_00004-0-4 loss: 0.890185  [  160/  306]
train() client id: f_00004-0-5 loss: 0.802758  [  192/  306]
train() client id: f_00004-0-6 loss: 0.757746  [  224/  306]
train() client id: f_00004-0-7 loss: 0.851712  [  256/  306]
train() client id: f_00004-0-8 loss: 0.609363  [  288/  306]
train() client id: f_00004-1-0 loss: 0.883197  [   32/  306]
train() client id: f_00004-1-1 loss: 0.776387  [   64/  306]
train() client id: f_00004-1-2 loss: 0.754100  [   96/  306]
train() client id: f_00004-1-3 loss: 0.636273  [  128/  306]
train() client id: f_00004-1-4 loss: 0.910847  [  160/  306]
train() client id: f_00004-1-5 loss: 0.758544  [  192/  306]
train() client id: f_00004-1-6 loss: 0.695868  [  224/  306]
train() client id: f_00004-1-7 loss: 0.906809  [  256/  306]
train() client id: f_00004-1-8 loss: 0.825957  [  288/  306]
train() client id: f_00004-2-0 loss: 0.768356  [   32/  306]
train() client id: f_00004-2-1 loss: 0.792899  [   64/  306]
train() client id: f_00004-2-2 loss: 0.788718  [   96/  306]
train() client id: f_00004-2-3 loss: 0.822365  [  128/  306]
train() client id: f_00004-2-4 loss: 0.741461  [  160/  306]
train() client id: f_00004-2-5 loss: 0.786987  [  192/  306]
train() client id: f_00004-2-6 loss: 0.756168  [  224/  306]
train() client id: f_00004-2-7 loss: 0.815901  [  256/  306]
train() client id: f_00004-2-8 loss: 0.742590  [  288/  306]
train() client id: f_00004-3-0 loss: 0.758949  [   32/  306]
train() client id: f_00004-3-1 loss: 0.730479  [   64/  306]
train() client id: f_00004-3-2 loss: 0.821701  [   96/  306]
train() client id: f_00004-3-3 loss: 0.759683  [  128/  306]
train() client id: f_00004-3-4 loss: 0.770433  [  160/  306]
train() client id: f_00004-3-5 loss: 0.676595  [  192/  306]
train() client id: f_00004-3-6 loss: 0.937413  [  224/  306]
train() client id: f_00004-3-7 loss: 0.748923  [  256/  306]
train() client id: f_00004-3-8 loss: 0.788974  [  288/  306]
train() client id: f_00004-4-0 loss: 0.861972  [   32/  306]
train() client id: f_00004-4-1 loss: 0.749774  [   64/  306]
train() client id: f_00004-4-2 loss: 0.756162  [   96/  306]
train() client id: f_00004-4-3 loss: 0.697157  [  128/  306]
train() client id: f_00004-4-4 loss: 0.810717  [  160/  306]
train() client id: f_00004-4-5 loss: 0.817410  [  192/  306]
train() client id: f_00004-4-6 loss: 0.714963  [  224/  306]
train() client id: f_00004-4-7 loss: 0.857797  [  256/  306]
train() client id: f_00004-4-8 loss: 0.690306  [  288/  306]
train() client id: f_00004-5-0 loss: 0.842898  [   32/  306]
train() client id: f_00004-5-1 loss: 0.901982  [   64/  306]
train() client id: f_00004-5-2 loss: 0.851008  [   96/  306]
train() client id: f_00004-5-3 loss: 0.698542  [  128/  306]
train() client id: f_00004-5-4 loss: 0.810261  [  160/  306]
train() client id: f_00004-5-5 loss: 0.700402  [  192/  306]
train() client id: f_00004-5-6 loss: 0.709328  [  224/  306]
train() client id: f_00004-5-7 loss: 0.798034  [  256/  306]
train() client id: f_00004-5-8 loss: 0.760177  [  288/  306]
train() client id: f_00005-0-0 loss: 0.526496  [   32/  146]
train() client id: f_00005-0-1 loss: 0.604171  [   64/  146]
train() client id: f_00005-0-2 loss: 0.600088  [   96/  146]
train() client id: f_00005-0-3 loss: 0.605330  [  128/  146]
train() client id: f_00005-1-0 loss: 0.308945  [   32/  146]
train() client id: f_00005-1-1 loss: 0.551100  [   64/  146]
train() client id: f_00005-1-2 loss: 0.488492  [   96/  146]
train() client id: f_00005-1-3 loss: 0.413514  [  128/  146]
train() client id: f_00005-2-0 loss: 0.483632  [   32/  146]
train() client id: f_00005-2-1 loss: 0.593053  [   64/  146]
train() client id: f_00005-2-2 loss: 0.606394  [   96/  146]
train() client id: f_00005-2-3 loss: 0.564464  [  128/  146]
train() client id: f_00005-3-0 loss: 0.409717  [   32/  146]
train() client id: f_00005-3-1 loss: 0.435897  [   64/  146]
train() client id: f_00005-3-2 loss: 0.701945  [   96/  146]
train() client id: f_00005-3-3 loss: 0.554284  [  128/  146]
train() client id: f_00005-4-0 loss: 0.343373  [   32/  146]
train() client id: f_00005-4-1 loss: 0.594485  [   64/  146]
train() client id: f_00005-4-2 loss: 0.842872  [   96/  146]
train() client id: f_00005-4-3 loss: 0.275092  [  128/  146]
train() client id: f_00005-5-0 loss: 0.630237  [   32/  146]
train() client id: f_00005-5-1 loss: 0.530909  [   64/  146]
train() client id: f_00005-5-2 loss: 0.653490  [   96/  146]
train() client id: f_00005-5-3 loss: 0.377500  [  128/  146]
train() client id: f_00006-0-0 loss: 0.552772  [   32/   54]
train() client id: f_00006-1-0 loss: 0.554586  [   32/   54]
train() client id: f_00006-2-0 loss: 0.611456  [   32/   54]
train() client id: f_00006-3-0 loss: 0.617598  [   32/   54]
train() client id: f_00006-4-0 loss: 0.594233  [   32/   54]
train() client id: f_00006-5-0 loss: 0.560645  [   32/   54]
train() client id: f_00007-0-0 loss: 0.697004  [   32/  179]
train() client id: f_00007-0-1 loss: 0.936645  [   64/  179]
train() client id: f_00007-0-2 loss: 0.673135  [   96/  179]
train() client id: f_00007-0-3 loss: 0.774639  [  128/  179]
train() client id: f_00007-0-4 loss: 0.790081  [  160/  179]
train() client id: f_00007-1-0 loss: 0.667062  [   32/  179]
train() client id: f_00007-1-1 loss: 0.576010  [   64/  179]
train() client id: f_00007-1-2 loss: 0.727316  [   96/  179]
train() client id: f_00007-1-3 loss: 0.982384  [  128/  179]
train() client id: f_00007-1-4 loss: 0.785284  [  160/  179]
train() client id: f_00007-2-0 loss: 0.754810  [   32/  179]
train() client id: f_00007-2-1 loss: 0.666590  [   64/  179]
train() client id: f_00007-2-2 loss: 0.851549  [   96/  179]
train() client id: f_00007-2-3 loss: 0.810803  [  128/  179]
train() client id: f_00007-2-4 loss: 0.773116  [  160/  179]
train() client id: f_00007-3-0 loss: 0.876558  [   32/  179]
train() client id: f_00007-3-1 loss: 0.632652  [   64/  179]
train() client id: f_00007-3-2 loss: 0.730116  [   96/  179]
train() client id: f_00007-3-3 loss: 0.589104  [  128/  179]
train() client id: f_00007-3-4 loss: 1.032818  [  160/  179]
train() client id: f_00007-4-0 loss: 0.626890  [   32/  179]
train() client id: f_00007-4-1 loss: 0.696863  [   64/  179]
train() client id: f_00007-4-2 loss: 0.670222  [   96/  179]
train() client id: f_00007-4-3 loss: 0.956863  [  128/  179]
train() client id: f_00007-4-4 loss: 0.773809  [  160/  179]
train() client id: f_00007-5-0 loss: 0.910730  [   32/  179]
train() client id: f_00007-5-1 loss: 0.819843  [   64/  179]
train() client id: f_00007-5-2 loss: 0.695124  [   96/  179]
train() client id: f_00007-5-3 loss: 0.705683  [  128/  179]
train() client id: f_00007-5-4 loss: 0.689226  [  160/  179]
train() client id: f_00008-0-0 loss: 0.788648  [   32/  130]
train() client id: f_00008-0-1 loss: 0.587181  [   64/  130]
train() client id: f_00008-0-2 loss: 0.687854  [   96/  130]
train() client id: f_00008-0-3 loss: 0.878966  [  128/  130]
train() client id: f_00008-1-0 loss: 0.808067  [   32/  130]
train() client id: f_00008-1-1 loss: 0.713446  [   64/  130]
train() client id: f_00008-1-2 loss: 0.739510  [   96/  130]
train() client id: f_00008-1-3 loss: 0.645236  [  128/  130]
train() client id: f_00008-2-0 loss: 0.897334  [   32/  130]
train() client id: f_00008-2-1 loss: 0.669041  [   64/  130]
train() client id: f_00008-2-2 loss: 0.698490  [   96/  130]
train() client id: f_00008-2-3 loss: 0.665913  [  128/  130]
train() client id: f_00008-3-0 loss: 0.677655  [   32/  130]
train() client id: f_00008-3-1 loss: 0.748437  [   64/  130]
train() client id: f_00008-3-2 loss: 0.773098  [   96/  130]
train() client id: f_00008-3-3 loss: 0.747267  [  128/  130]
train() client id: f_00008-4-0 loss: 0.829065  [   32/  130]
train() client id: f_00008-4-1 loss: 0.738238  [   64/  130]
train() client id: f_00008-4-2 loss: 0.720388  [   96/  130]
train() client id: f_00008-4-3 loss: 0.655204  [  128/  130]
train() client id: f_00008-5-0 loss: 0.717199  [   32/  130]
train() client id: f_00008-5-1 loss: 0.809318  [   64/  130]
train() client id: f_00008-5-2 loss: 0.710057  [   96/  130]
train() client id: f_00008-5-3 loss: 0.719532  [  128/  130]
train() client id: f_00009-0-0 loss: 0.748369  [   32/  118]
train() client id: f_00009-0-1 loss: 1.006821  [   64/  118]
train() client id: f_00009-0-2 loss: 0.664787  [   96/  118]
train() client id: f_00009-1-0 loss: 0.932813  [   32/  118]
train() client id: f_00009-1-1 loss: 0.750527  [   64/  118]
train() client id: f_00009-1-2 loss: 0.692257  [   96/  118]
train() client id: f_00009-2-0 loss: 0.812954  [   32/  118]
train() client id: f_00009-2-1 loss: 0.614724  [   64/  118]
train() client id: f_00009-2-2 loss: 0.690293  [   96/  118]
train() client id: f_00009-3-0 loss: 0.666835  [   32/  118]
train() client id: f_00009-3-1 loss: 0.720200  [   64/  118]
train() client id: f_00009-3-2 loss: 0.747790  [   96/  118]
train() client id: f_00009-4-0 loss: 0.655320  [   32/  118]
train() client id: f_00009-4-1 loss: 0.609179  [   64/  118]
train() client id: f_00009-4-2 loss: 0.789364  [   96/  118]
train() client id: f_00009-5-0 loss: 0.675399  [   32/  118]
train() client id: f_00009-5-1 loss: 0.695446  [   64/  118]
train() client id: f_00009-5-2 loss: 0.704999  [   96/  118]
At round 71 accuracy: 0.649867374005305
At round 71 training accuracy: 0.5888665325285044
At round 71 training loss: 0.8313321145066086
update_location
xs = -3.905658 4.200318 375.009024 18.811294 0.979296 3.956410 -337.443192 -316.324852 359.663977 -302.060879 
ys = 367.587959 350.555839 1.320614 -337.455176 329.350187 312.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 380.967403 364.564176 388.115333 352.462566 344.198351 328.433168 351.958518 331.756067 373.720278 318.208715 
dists_bs = 257.610939 250.863721 576.779382 547.880576 234.078390 225.871948 240.704021 224.300174 557.359925 213.037462 
uav_gains = -120.655909 -119.918760 -120.948643 -119.305386 -118.846546 -117.868432 -119.278378 -118.086514 -120.342280 -117.155495 
bs_gains = -107.074060 -106.751319 -116.875332 -116.250264 -105.909176 -105.475203 -106.248593 -105.390288 -116.458860 -104.763827 
Round 72
-------------------------------
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.45877326 2.87217542 1.44178771 0.55140047 3.31032656 1.59425308
 0.66902829 1.99714791 1.45909395 1.29300424]
obj_prev = 16.646990883599326
eta_min = 4.971520837874105e-65	eta_max = 0.9577477121135591
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 3.7800966926678474	eta = 0.9090909090909091
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 10.808661784694127	eta = 0.3179349680138224
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 6.733867507222867	eta = 0.5103236045412168
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 6.058721545799979	eta = 0.5671908690326205
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 6.015928897442357	eta = 0.5712254245973457
af = 3.436451538788952	bf = 0.6550768027395515	zeta = 6.015728044345185	eta = 0.5712444966689002
eta = 0.5712444966689002
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [0.04620474 0.09717663 0.04547133 0.01576828 0.11221151 0.05353882
 0.01980203 0.06564007 0.04767156 0.04327111]
ene_total = [0.62840586 0.86936    0.63267678 0.33813836 0.98949538 0.50632297
 0.37054313 0.71901402 0.54216497 0.41960659]
ti_comp = [2.91272976 3.128895   2.90019242 2.96199725 3.13288317 3.13480384
 2.96285607 2.99669538 3.03246495 3.13777295]
ti_coms = [0.3035376  0.08737236 0.31607494 0.25427011 0.08338419 0.08146352
 0.25341129 0.21957198 0.18380241 0.07849441]
t_total = [26.39969788 26.39969788 26.39969788 26.39969788 26.39969788 26.39969788
 26.39969788 26.39969788 26.39969788 26.39969788]
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [7.26673296e-07 5.85846172e-06 6.98617374e-07 2.79295552e-08
 8.99712661e-06 9.76035873e-07 5.52825571e-08 1.96834785e-06
 7.36319715e-07 5.14317079e-07]
ene_total = [0.24915505 0.07176489 0.2594457  0.20870969 0.06851708 0.06687473
 0.20800497 0.18024477 0.15087435 0.06443384]
optimize_network iter = 0 obj = 1.52802506642978
eta = 0.5712444966689002
freqs = [ 7931518.36924844 15528906.23802998  7839364.44612375  2661765.47781446
 17908664.85509679  8539421.8514933   3341713.30591816 10952075.17012927
  7860199.06137994  6895193.77630764]
eta_min = 0.5712444966689012	eta_max = 0.8252306516812821
af = 0.0001292173845627228	bf = 0.6550768027395515	zeta = 0.00014213912301899509	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [1.23691134e-07 9.97201600e-07 1.18915578e-07 4.75404610e-09
 1.53145134e-06 1.66136535e-07 9.40995383e-09 3.35043518e-07
 1.25333105e-07 8.75447921e-08]
ene_total = [1.17506927 0.33827729 1.22360405 0.98433881 0.32285882 0.31537062
 0.98101428 0.85002707 0.71154662 0.30387348]
ti_comp = [1.00747753 1.22364276 0.99494019 1.05674501 1.22763094 1.22955161
 1.05760384 1.09144314 1.12721272 1.23252071]
ti_coms = [0.3035376  0.08737236 0.31607494 0.25427011 0.08338419 0.08146352
 0.25341129 0.21957198 0.18380241 0.07849441]
t_total = [26.39969788 26.39969788 26.39969788 26.39969788 26.39969788 26.39969788
 26.39969788 26.39969788 26.39969788 26.39969788]
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [2.45233236e-07 1.54655995e-06 2.39667719e-07 8.85938112e-09
 2.36574025e-06 2.56155376e-07 1.75175625e-08 5.99094442e-07
 2.15157618e-07 1.34584796e-07]
ene_total = [0.61123365 0.17597144 0.63647978 0.51201976 0.16795702 0.16404692
 0.51029054 0.44216059 0.37012424 0.15806563]
optimize_network iter = 1 obj = 3.7483495686155184
eta = 0.8252306516812821
freqs = [ 7866677.00292003 13622203.68026247  7839364.44612375  2559495.62396928
 15678688.91426082  7468996.16234366  3211640.19547591 10315922.15655571
  7254268.29694454  6022043.97636097]
eta_min = 0.8252306516812807	eta_max = 0.8252306516812821
af = 0.00010312038831994186	bf = 0.6550768027395515	zeta = 0.00011343242715193606	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [1.21677013e-07 7.67354369e-07 1.18915578e-07 4.39574606e-09
 1.17380585e-06 1.27096235e-07 8.69166314e-09 2.97251805e-07
 1.06754438e-07 6.67767395e-08]
ene_total = [1.17506919 0.33826839 1.22360405 0.98433879 0.32284498 0.31536911
 0.98101425 0.85002561 0.7115459  0.30387267]
ti_comp = [1.00747753 1.22364276 0.99494019 1.05674501 1.22763094 1.22955161
 1.05760384 1.09144314 1.12721272 1.23252071]
ti_coms = [0.3035376  0.08737236 0.31607494 0.25427011 0.08338419 0.08146352
 0.25341129 0.21957198 0.18380241 0.07849441]
t_total = [26.39969788 26.39969788 26.39969788 26.39969788 26.39969788 26.39969788
 26.39969788 26.39969788 26.39969788 26.39969788]
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [2.45233236e-07 1.54655995e-06 2.39667719e-07 8.85938112e-09
 2.36574025e-06 2.56155376e-07 1.75175625e-08 5.99094442e-07
 2.15157618e-07 1.34584796e-07]
ene_total = [0.61123365 0.17597144 0.63647978 0.51201976 0.16795702 0.16404692
 0.51029054 0.44216059 0.37012424 0.15806563]
optimize_network iter = 2 obj = 3.7483495686155184
eta = 0.8252306516812821
freqs = [ 7866677.00292003 13622203.68026247  7839364.44612375  2559495.62396928
 15678688.91426082  7468996.16234366  3211640.19547591 10315922.15655571
  7254268.29694454  6022043.97636097]
Done!
ene_coms = [0.03035376 0.00873724 0.03160749 0.02542701 0.00833842 0.00814635
 0.02534113 0.0219572  0.01838024 0.00784944]
ene_comp = [2.33923815e-07 1.47523724e-06 2.28614962e-07 8.45081303e-09
 2.25663941e-06 2.44342258e-07 1.67097050e-08 5.71466002e-07
 2.05235193e-07 1.28378148e-07]
ene_total = [0.03035399 0.00873871 0.03160772 0.02542702 0.00834068 0.0081466
 0.02534115 0.02195777 0.01838045 0.00784957]
At round 72 energy consumption: 0.18614365042570813
At round 72 eta: 0.8252306516812821
At round 72 a_n: 3.5193018744635296
At round 72 local rounds: 6.290079613751617
At round 72 global rounds: 20.136836970093626
gradient difference: 0.8853836059570312
train() client id: f_00000-0-0 loss: 1.011949  [   32/  126]
train() client id: f_00000-0-1 loss: 0.809976  [   64/  126]
train() client id: f_00000-0-2 loss: 0.750059  [   96/  126]
train() client id: f_00000-1-0 loss: 0.883052  [   32/  126]
train() client id: f_00000-1-1 loss: 0.795137  [   64/  126]
train() client id: f_00000-1-2 loss: 0.982723  [   96/  126]
train() client id: f_00000-2-0 loss: 0.972042  [   32/  126]
train() client id: f_00000-2-1 loss: 0.783264  [   64/  126]
train() client id: f_00000-2-2 loss: 0.919550  [   96/  126]
train() client id: f_00000-3-0 loss: 0.892828  [   32/  126]
train() client id: f_00000-3-1 loss: 1.059948  [   64/  126]
train() client id: f_00000-3-2 loss: 0.756212  [   96/  126]
train() client id: f_00000-4-0 loss: 0.904497  [   32/  126]
train() client id: f_00000-4-1 loss: 0.826311  [   64/  126]
train() client id: f_00000-4-2 loss: 0.905960  [   96/  126]
train() client id: f_00000-5-0 loss: 0.919365  [   32/  126]
train() client id: f_00000-5-1 loss: 0.812747  [   64/  126]
train() client id: f_00000-5-2 loss: 1.010178  [   96/  126]
train() client id: f_00001-0-0 loss: 0.678214  [   32/  265]
train() client id: f_00001-0-1 loss: 0.732711  [   64/  265]
train() client id: f_00001-0-2 loss: 0.518131  [   96/  265]
train() client id: f_00001-0-3 loss: 0.591985  [  128/  265]
train() client id: f_00001-0-4 loss: 0.530620  [  160/  265]
train() client id: f_00001-0-5 loss: 0.588577  [  192/  265]
train() client id: f_00001-0-6 loss: 0.496002  [  224/  265]
train() client id: f_00001-0-7 loss: 0.522965  [  256/  265]
train() client id: f_00001-1-0 loss: 0.665353  [   32/  265]
train() client id: f_00001-1-1 loss: 0.552127  [   64/  265]
train() client id: f_00001-1-2 loss: 0.637991  [   96/  265]
train() client id: f_00001-1-3 loss: 0.544423  [  128/  265]
train() client id: f_00001-1-4 loss: 0.671147  [  160/  265]
train() client id: f_00001-1-5 loss: 0.543550  [  192/  265]
train() client id: f_00001-1-6 loss: 0.495439  [  224/  265]
train() client id: f_00001-1-7 loss: 0.476692  [  256/  265]
train() client id: f_00001-2-0 loss: 0.629041  [   32/  265]
train() client id: f_00001-2-1 loss: 0.630427  [   64/  265]
train() client id: f_00001-2-2 loss: 0.486681  [   96/  265]
train() client id: f_00001-2-3 loss: 0.721030  [  128/  265]
train() client id: f_00001-2-4 loss: 0.520455  [  160/  265]
train() client id: f_00001-2-5 loss: 0.470326  [  192/  265]
train() client id: f_00001-2-6 loss: 0.511513  [  224/  265]
train() client id: f_00001-2-7 loss: 0.621665  [  256/  265]
train() client id: f_00001-3-0 loss: 0.509762  [   32/  265]
train() client id: f_00001-3-1 loss: 0.632741  [   64/  265]
train() client id: f_00001-3-2 loss: 0.536881  [   96/  265]
train() client id: f_00001-3-3 loss: 0.549013  [  128/  265]
train() client id: f_00001-3-4 loss: 0.631917  [  160/  265]
train() client id: f_00001-3-5 loss: 0.665466  [  192/  265]
train() client id: f_00001-3-6 loss: 0.553390  [  224/  265]
train() client id: f_00001-3-7 loss: 0.576340  [  256/  265]
train() client id: f_00001-4-0 loss: 0.561336  [   32/  265]
train() client id: f_00001-4-1 loss: 0.571419  [   64/  265]
train() client id: f_00001-4-2 loss: 0.476071  [   96/  265]
train() client id: f_00001-4-3 loss: 0.670163  [  128/  265]
train() client id: f_00001-4-4 loss: 0.622099  [  160/  265]
train() client id: f_00001-4-5 loss: 0.743290  [  192/  265]
train() client id: f_00001-4-6 loss: 0.477803  [  224/  265]
train() client id: f_00001-4-7 loss: 0.532323  [  256/  265]
train() client id: f_00001-5-0 loss: 0.665899  [   32/  265]
train() client id: f_00001-5-1 loss: 0.503047  [   64/  265]
train() client id: f_00001-5-2 loss: 0.724302  [   96/  265]
train() client id: f_00001-5-3 loss: 0.537310  [  128/  265]
train() client id: f_00001-5-4 loss: 0.509958  [  160/  265]
train() client id: f_00001-5-5 loss: 0.526026  [  192/  265]
train() client id: f_00001-5-6 loss: 0.622519  [  224/  265]
train() client id: f_00001-5-7 loss: 0.585059  [  256/  265]
train() client id: f_00002-0-0 loss: 1.008097  [   32/  124]
train() client id: f_00002-0-1 loss: 1.207503  [   64/  124]
train() client id: f_00002-0-2 loss: 1.254772  [   96/  124]
train() client id: f_00002-1-0 loss: 1.094936  [   32/  124]
train() client id: f_00002-1-1 loss: 1.054836  [   64/  124]
train() client id: f_00002-1-2 loss: 1.022345  [   96/  124]
train() client id: f_00002-2-0 loss: 1.069585  [   32/  124]
train() client id: f_00002-2-1 loss: 0.957817  [   64/  124]
train() client id: f_00002-2-2 loss: 0.980393  [   96/  124]
train() client id: f_00002-3-0 loss: 0.981410  [   32/  124]
train() client id: f_00002-3-1 loss: 1.163587  [   64/  124]
train() client id: f_00002-3-2 loss: 1.168408  [   96/  124]
train() client id: f_00002-4-0 loss: 1.053629  [   32/  124]
train() client id: f_00002-4-1 loss: 1.097198  [   64/  124]
train() client id: f_00002-4-2 loss: 0.892791  [   96/  124]
train() client id: f_00002-5-0 loss: 1.022746  [   32/  124]
train() client id: f_00002-5-1 loss: 1.161415  [   64/  124]
train() client id: f_00002-5-2 loss: 0.919891  [   96/  124]
train() client id: f_00003-0-0 loss: 0.758559  [   32/   43]
train() client id: f_00003-1-0 loss: 0.614833  [   32/   43]
train() client id: f_00003-2-0 loss: 0.699386  [   32/   43]
train() client id: f_00003-3-0 loss: 0.750471  [   32/   43]
train() client id: f_00003-4-0 loss: 0.449169  [   32/   43]
train() client id: f_00003-5-0 loss: 0.777369  [   32/   43]
train() client id: f_00004-0-0 loss: 0.846873  [   32/  306]
train() client id: f_00004-0-1 loss: 0.681213  [   64/  306]
train() client id: f_00004-0-2 loss: 0.826088  [   96/  306]
train() client id: f_00004-0-3 loss: 0.929326  [  128/  306]
train() client id: f_00004-0-4 loss: 0.801820  [  160/  306]
train() client id: f_00004-0-5 loss: 0.787706  [  192/  306]
train() client id: f_00004-0-6 loss: 0.788755  [  224/  306]
train() client id: f_00004-0-7 loss: 0.707566  [  256/  306]
train() client id: f_00004-0-8 loss: 0.874232  [  288/  306]
train() client id: f_00004-1-0 loss: 0.864031  [   32/  306]
train() client id: f_00004-1-1 loss: 0.839064  [   64/  306]
train() client id: f_00004-1-2 loss: 0.871167  [   96/  306]
train() client id: f_00004-1-3 loss: 0.809619  [  128/  306]
train() client id: f_00004-1-4 loss: 0.763599  [  160/  306]
train() client id: f_00004-1-5 loss: 0.903622  [  192/  306]
train() client id: f_00004-1-6 loss: 0.885089  [  224/  306]
train() client id: f_00004-1-7 loss: 0.712909  [  256/  306]
train() client id: f_00004-1-8 loss: 0.765279  [  288/  306]
train() client id: f_00004-2-0 loss: 0.779091  [   32/  306]
train() client id: f_00004-2-1 loss: 0.728591  [   64/  306]
train() client id: f_00004-2-2 loss: 0.838048  [   96/  306]
train() client id: f_00004-2-3 loss: 0.879044  [  128/  306]
train() client id: f_00004-2-4 loss: 0.731051  [  160/  306]
train() client id: f_00004-2-5 loss: 0.825252  [  192/  306]
train() client id: f_00004-2-6 loss: 0.822293  [  224/  306]
train() client id: f_00004-2-7 loss: 0.855236  [  256/  306]
train() client id: f_00004-2-8 loss: 0.913570  [  288/  306]
train() client id: f_00004-3-0 loss: 0.731939  [   32/  306]
train() client id: f_00004-3-1 loss: 0.721423  [   64/  306]
train() client id: f_00004-3-2 loss: 0.973346  [   96/  306]
train() client id: f_00004-3-3 loss: 0.865661  [  128/  306]
train() client id: f_00004-3-4 loss: 0.716127  [  160/  306]
train() client id: f_00004-3-5 loss: 0.811357  [  192/  306]
train() client id: f_00004-3-6 loss: 0.768338  [  224/  306]
train() client id: f_00004-3-7 loss: 0.960054  [  256/  306]
train() client id: f_00004-3-8 loss: 0.792688  [  288/  306]
train() client id: f_00004-4-0 loss: 0.901520  [   32/  306]
train() client id: f_00004-4-1 loss: 0.767314  [   64/  306]
train() client id: f_00004-4-2 loss: 0.852796  [   96/  306]
train() client id: f_00004-4-3 loss: 0.913284  [  128/  306]
train() client id: f_00004-4-4 loss: 0.768422  [  160/  306]
train() client id: f_00004-4-5 loss: 0.768725  [  192/  306]
train() client id: f_00004-4-6 loss: 0.797251  [  224/  306]
train() client id: f_00004-4-7 loss: 0.832081  [  256/  306]
train() client id: f_00004-4-8 loss: 0.694878  [  288/  306]
train() client id: f_00004-5-0 loss: 0.825128  [   32/  306]
train() client id: f_00004-5-1 loss: 0.874511  [   64/  306]
train() client id: f_00004-5-2 loss: 0.808968  [   96/  306]
train() client id: f_00004-5-3 loss: 0.706509  [  128/  306]
train() client id: f_00004-5-4 loss: 0.856924  [  160/  306]
train() client id: f_00004-5-5 loss: 0.757369  [  192/  306]
train() client id: f_00004-5-6 loss: 0.950943  [  224/  306]
train() client id: f_00004-5-7 loss: 0.830647  [  256/  306]
train() client id: f_00004-5-8 loss: 0.849358  [  288/  306]
train() client id: f_00005-0-0 loss: 0.813232  [   32/  146]
train() client id: f_00005-0-1 loss: 0.886516  [   64/  146]
train() client id: f_00005-0-2 loss: 0.831118  [   96/  146]
train() client id: f_00005-0-3 loss: 0.400198  [  128/  146]
train() client id: f_00005-1-0 loss: 0.703331  [   32/  146]
train() client id: f_00005-1-1 loss: 0.946527  [   64/  146]
train() client id: f_00005-1-2 loss: 0.726083  [   96/  146]
train() client id: f_00005-1-3 loss: 0.830575  [  128/  146]
train() client id: f_00005-2-0 loss: 1.038936  [   32/  146]
train() client id: f_00005-2-1 loss: 0.854107  [   64/  146]
train() client id: f_00005-2-2 loss: 0.634207  [   96/  146]
train() client id: f_00005-2-3 loss: 0.648225  [  128/  146]
train() client id: f_00005-3-0 loss: 0.657112  [   32/  146]
train() client id: f_00005-3-1 loss: 0.801821  [   64/  146]
train() client id: f_00005-3-2 loss: 1.087054  [   96/  146]
train() client id: f_00005-3-3 loss: 0.682021  [  128/  146]
train() client id: f_00005-4-0 loss: 0.506181  [   32/  146]
train() client id: f_00005-4-1 loss: 0.880041  [   64/  146]
train() client id: f_00005-4-2 loss: 0.941210  [   96/  146]
train() client id: f_00005-4-3 loss: 0.897228  [  128/  146]
train() client id: f_00005-5-0 loss: 0.609281  [   32/  146]
train() client id: f_00005-5-1 loss: 0.973944  [   64/  146]
train() client id: f_00005-5-2 loss: 0.569457  [   96/  146]
train() client id: f_00005-5-3 loss: 1.016121  [  128/  146]
train() client id: f_00006-0-0 loss: 0.497759  [   32/   54]
train() client id: f_00006-1-0 loss: 0.503365  [   32/   54]
train() client id: f_00006-2-0 loss: 0.515702  [   32/   54]
train() client id: f_00006-3-0 loss: 0.441518  [   32/   54]
train() client id: f_00006-4-0 loss: 0.504746  [   32/   54]
train() client id: f_00006-5-0 loss: 0.589291  [   32/   54]
train() client id: f_00007-0-0 loss: 0.591476  [   32/  179]
train() client id: f_00007-0-1 loss: 0.639156  [   64/  179]
train() client id: f_00007-0-2 loss: 0.745927  [   96/  179]
train() client id: f_00007-0-3 loss: 0.696132  [  128/  179]
train() client id: f_00007-0-4 loss: 0.718302  [  160/  179]
train() client id: f_00007-1-0 loss: 0.623320  [   32/  179]
train() client id: f_00007-1-1 loss: 0.786885  [   64/  179]
train() client id: f_00007-1-2 loss: 0.676485  [   96/  179]
train() client id: f_00007-1-3 loss: 0.596931  [  128/  179]
train() client id: f_00007-1-4 loss: 0.706471  [  160/  179]
train() client id: f_00007-2-0 loss: 0.806969  [   32/  179]
train() client id: f_00007-2-1 loss: 0.587992  [   64/  179]
train() client id: f_00007-2-2 loss: 0.642888  [   96/  179]
train() client id: f_00007-2-3 loss: 0.616257  [  128/  179]
train() client id: f_00007-2-4 loss: 0.695886  [  160/  179]
train() client id: f_00007-3-0 loss: 0.578920  [   32/  179]
train() client id: f_00007-3-1 loss: 0.702989  [   64/  179]
train() client id: f_00007-3-2 loss: 0.655992  [   96/  179]
train() client id: f_00007-3-3 loss: 0.755503  [  128/  179]
train() client id: f_00007-3-4 loss: 0.667166  [  160/  179]
train() client id: f_00007-4-0 loss: 0.626226  [   32/  179]
train() client id: f_00007-4-1 loss: 0.491490  [   64/  179]
train() client id: f_00007-4-2 loss: 0.773090  [   96/  179]
train() client id: f_00007-4-3 loss: 0.501470  [  128/  179]
train() client id: f_00007-4-4 loss: 0.705318  [  160/  179]
train() client id: f_00007-5-0 loss: 0.621218  [   32/  179]
train() client id: f_00007-5-1 loss: 0.685467  [   64/  179]
train() client id: f_00007-5-2 loss: 0.507289  [   96/  179]
train() client id: f_00007-5-3 loss: 0.970968  [  128/  179]
train() client id: f_00007-5-4 loss: 0.528392  [  160/  179]
train() client id: f_00008-0-0 loss: 0.651984  [   32/  130]
train() client id: f_00008-0-1 loss: 0.793887  [   64/  130]
train() client id: f_00008-0-2 loss: 0.713416  [   96/  130]
train() client id: f_00008-0-3 loss: 0.710173  [  128/  130]
train() client id: f_00008-1-0 loss: 0.683195  [   32/  130]
train() client id: f_00008-1-1 loss: 0.772546  [   64/  130]
train() client id: f_00008-1-2 loss: 0.764389  [   96/  130]
train() client id: f_00008-1-3 loss: 0.661445  [  128/  130]
train() client id: f_00008-2-0 loss: 0.849641  [   32/  130]
train() client id: f_00008-2-1 loss: 0.732361  [   64/  130]
train() client id: f_00008-2-2 loss: 0.648724  [   96/  130]
train() client id: f_00008-2-3 loss: 0.659450  [  128/  130]
train() client id: f_00008-3-0 loss: 0.789811  [   32/  130]
train() client id: f_00008-3-1 loss: 0.672096  [   64/  130]
train() client id: f_00008-3-2 loss: 0.720917  [   96/  130]
train() client id: f_00008-3-3 loss: 0.735487  [  128/  130]
train() client id: f_00008-4-0 loss: 0.635638  [   32/  130]
train() client id: f_00008-4-1 loss: 0.762254  [   64/  130]
train() client id: f_00008-4-2 loss: 0.894354  [   96/  130]
train() client id: f_00008-4-3 loss: 0.623586  [  128/  130]
train() client id: f_00008-5-0 loss: 0.639776  [   32/  130]
train() client id: f_00008-5-1 loss: 0.632186  [   64/  130]
train() client id: f_00008-5-2 loss: 0.849108  [   96/  130]
train() client id: f_00008-5-3 loss: 0.750217  [  128/  130]
train() client id: f_00009-0-0 loss: 0.783084  [   32/  118]
train() client id: f_00009-0-1 loss: 0.663377  [   64/  118]
train() client id: f_00009-0-2 loss: 0.825619  [   96/  118]
train() client id: f_00009-1-0 loss: 0.653360  [   32/  118]
train() client id: f_00009-1-1 loss: 0.773734  [   64/  118]
train() client id: f_00009-1-2 loss: 0.804963  [   96/  118]
train() client id: f_00009-2-0 loss: 0.650190  [   32/  118]
train() client id: f_00009-2-1 loss: 0.890635  [   64/  118]
train() client id: f_00009-2-2 loss: 0.770525  [   96/  118]
train() client id: f_00009-3-0 loss: 0.743369  [   32/  118]
train() client id: f_00009-3-1 loss: 0.632805  [   64/  118]
train() client id: f_00009-3-2 loss: 0.802023  [   96/  118]
train() client id: f_00009-4-0 loss: 0.812902  [   32/  118]
train() client id: f_00009-4-1 loss: 0.842520  [   64/  118]
train() client id: f_00009-4-2 loss: 0.684436  [   96/  118]
train() client id: f_00009-5-0 loss: 0.618834  [   32/  118]
train() client id: f_00009-5-1 loss: 0.794136  [   64/  118]
train() client id: f_00009-5-2 loss: 0.749306  [   96/  118]
At round 72 accuracy: 0.649867374005305
At round 72 training accuracy: 0.5881958417169685
At round 72 training loss: 0.820602278231745
update_location
xs = -3.905658 4.200318 380.009024 18.811294 0.979296 3.956410 -342.443192 -321.324852 364.663977 -307.060879 
ys = 372.587959 355.555839 1.320614 -342.455176 334.350187 317.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 385.794041 369.374603 392.948600 357.252589 348.985682 333.198871 356.755140 336.526874 378.534656 322.958813 
dists_bs = 261.369997 254.387824 581.549263 552.560112 237.405128 228.956935 244.106242 227.483663 562.158097 216.056865 
uav_gains = -120.855277 -120.145285 -121.138113 -119.556032 -119.116553 -118.179170 -119.530498 -118.388275 -120.552611 -117.494651 
bs_gains = -107.250220 -106.920956 -116.975481 -116.353686 -106.080782 -105.640166 -106.419268 -105.561665 -116.563096 -104.934965 
Round 73
-------------------------------
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.3194998  2.59288881 1.30418738 0.50035792 2.98837254 1.43930991
 0.6065387  1.80528297 1.31771582 1.16737463]
obj_prev = 15.041528473465261
eta_min = 6.730086322824428e-72	eta_max = 0.960768447983313
af = 3.101969802094253	bf = 0.606014338290774	zeta = 3.4121667823036788	eta = 0.9090909090909091
af = 3.101969802094253	bf = 0.606014338290774	zeta = 9.918300016635595	eta = 0.3127521648761819
af = 3.101969802094253	bf = 0.606014338290774	zeta = 6.128169132936568	eta = 0.5061821458912337
af = 3.101969802094253	bf = 0.606014338290774	zeta = 5.504089108696306	eta = 0.5635755055624424
af = 3.101969802094253	bf = 0.606014338290774	zeta = 5.464521898505789	eta = 0.5676562121459247
af = 3.101969802094253	bf = 0.606014338290774	zeta = 5.464335618649047	eta = 0.5676755636142928
eta = 0.5676755636142928
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [0.0467219  0.09826431 0.04598028 0.01594477 0.11346746 0.05413807
 0.02002367 0.06637476 0.04820513 0.04375543]
ene_total = [0.57257327 0.78687954 0.57638183 0.31001941 0.89561579 0.45820012
 0.33936518 0.65485131 0.49074166 0.3797075 ]
ti_comp = [3.27393193 3.49770683 3.26133269 3.32347001 3.50175933 3.50374398
 3.32432233 3.35848843 3.40012884 3.50673809]
ti_coms = [0.31199547 0.08822057 0.32459471 0.26245738 0.08416807 0.08218342
 0.26160506 0.22743896 0.18579855 0.07918931]
t_total = [26.34969368 26.34969368 26.34969368 26.34969368 26.34969368 26.34969368
 26.34969368 26.34969368 26.34969368 26.34969368]
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [5.94705746e-07 4.84730784e-06 5.71221622e-07 2.29377929e-08
 7.44597339e-06 8.07836791e-07 4.54051187e-08 1.62032140e-06
 6.05576618e-07 4.25764209e-07]
ene_total = [0.22926124 0.06486083 0.2385191  0.19285604 0.06190211 0.060395
 0.19222992 0.16713595 0.13653078 0.05819209]
optimize_network iter = 0 obj = 1.401883055865988
eta = 0.5676755636142928
freqs = [ 7135441.14428652 14046961.66554712  7049308.38877608  2398814.24536511
 16201493.85610097  7725745.89318307  3011692.04068683  9881641.69424418
  7088721.37557428  6238764.68717198]
eta_min = 0.5676755636142934	eta_max = 0.8387833537635655
af = 9.530769728321942e-05	bf = 0.606014338290774	zeta = 0.00010483846701154136	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [1.00107717e-07 8.15954657e-07 9.61545994e-08 3.86115336e-09
 1.25339196e-06 1.35984388e-07 7.64311227e-09 2.72751152e-07
 1.01937628e-07 7.16695331e-08]
ene_total = [1.09025033 0.30830925 1.13427738 0.91713951 0.29416335 0.28718908
 0.91416128 0.79477943 0.64926391 0.27672413]
ti_comp = [1.02522061 1.24899552 1.01262137 1.0747587  1.25304801 1.25503266
 1.07561102 1.10977712 1.15141753 1.25802677]
ti_coms = [0.31199547 0.08822057 0.32459471 0.26245738 0.08416807 0.08218342
 0.26160506 0.22743896 0.18579855 0.07918931]
t_total = [26.34969368 26.34969368 26.34969368 26.34969368 26.34969368 26.34969368
 26.34969368 26.34969368 26.34969368 26.34969368]
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [1.81535984e-07 1.13789349e-06 1.77360386e-07 6.56552073e-09
 1.74066261e-06 1.88466540e-07 1.29824466e-08 4.44194635e-07
 1.58070248e-07 9.90266306e-08]
ene_total = [0.61478717 0.17386006 0.63961375 0.51716947 0.16588653 0.16194522
 0.51549012 0.44817469 0.36611709 0.1560436 ]
optimize_network iter = 1 obj = 3.7590876976936305
eta = 0.8387833537635655
freqs = [ 7074978.59491438 12213971.70314103  7049308.38877607  2303188.06336389
 14058067.6277585   6696837.19701407  2890083.78088009  9285155.45638792
  6499537.15984087  5399630.45377395]
eta_min = 0.8387833537635758	eta_max = 0.8387833537635625
af = 7.4931901688053e-05	bf = 0.606014338290774	zeta = 8.242509185685831e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [9.84183688e-08 6.16900397e-07 9.61545994e-08 3.55944769e-09
 9.43687147e-07 1.02175717e-07 7.03833579e-09 2.40816781e-07
 8.56965960e-08 5.36865431e-08]
ene_total = [1.09025027 0.30830229 1.13427738 0.9171395  0.29415253 0.2871879
 0.91416126 0.79477832 0.64926334 0.2767235 ]
ti_comp = [1.02522061 1.24899552 1.01262137 1.0747587  1.25304801 1.25503266
 1.07561102 1.10977712 1.15141753 1.25802677]
ti_coms = [0.31199547 0.08822057 0.32459471 0.26245738 0.08416807 0.08218342
 0.26160506 0.22743896 0.18579855 0.07918931]
t_total = [26.34969368 26.34969368 26.34969368 26.34969368 26.34969368 26.34969368
 26.34969368 26.34969368 26.34969368 26.34969368]
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [1.81535984e-07 1.13789349e-06 1.77360386e-07 6.56552073e-09
 1.74066261e-06 1.88466540e-07 1.29824466e-08 4.44194635e-07
 1.58070248e-07 9.90266306e-08]
ene_total = [0.61478717 0.17386006 0.63961375 0.51716947 0.16588653 0.16194522
 0.51549012 0.44817469 0.36611709 0.1560436 ]
optimize_network iter = 2 obj = 3.759087697693561
eta = 0.8387833537635625
freqs = [ 7074978.59491436 12213971.70314105  7049308.38877605  2303188.06336388
 14058067.62775852  6696837.19701408  2890083.78088009  9285155.45638791
  6499537.15984087  5399630.45377396]
Done!
ene_coms = [0.03119955 0.00882206 0.03245947 0.02624574 0.00841681 0.00821834
 0.02616051 0.0227439  0.01857986 0.00791893]
ene_comp = [1.57674265e-07 9.88324819e-07 1.54047521e-07 5.70252590e-09
 1.51186388e-06 1.63693844e-07 1.12759888e-08 3.85808151e-07
 1.37292946e-07 8.60102267e-08]
ene_total = [0.0311997  0.00882304 0.03245962 0.02624574 0.00841832 0.00821851
 0.02616052 0.02274428 0.01857999 0.00791902]
At round 73 energy consumption: 0.19076875112517003
At round 73 eta: 0.8387833537635625
At round 73 a_n: 3.176756027494214
At round 73 local rounds: 5.756677690695127
At round 73 global rounds: 19.70488843214887
gradient difference: 0.689144492149353
train() client id: f_00000-0-0 loss: 1.007211  [   32/  126]
train() client id: f_00000-0-1 loss: 0.929928  [   64/  126]
train() client id: f_00000-0-2 loss: 0.921522  [   96/  126]
train() client id: f_00000-1-0 loss: 0.936897  [   32/  126]
train() client id: f_00000-1-1 loss: 0.901681  [   64/  126]
train() client id: f_00000-1-2 loss: 0.828294  [   96/  126]
train() client id: f_00000-2-0 loss: 0.750636  [   32/  126]
train() client id: f_00000-2-1 loss: 0.932207  [   64/  126]
train() client id: f_00000-2-2 loss: 0.825957  [   96/  126]
train() client id: f_00000-3-0 loss: 0.739061  [   32/  126]
train() client id: f_00000-3-1 loss: 0.820494  [   64/  126]
train() client id: f_00000-3-2 loss: 0.840057  [   96/  126]
train() client id: f_00000-4-0 loss: 0.899995  [   32/  126]
train() client id: f_00000-4-1 loss: 0.723287  [   64/  126]
train() client id: f_00000-4-2 loss: 0.736579  [   96/  126]
train() client id: f_00001-0-0 loss: 0.520071  [   32/  265]
train() client id: f_00001-0-1 loss: 0.540520  [   64/  265]
train() client id: f_00001-0-2 loss: 0.495634  [   96/  265]
train() client id: f_00001-0-3 loss: 0.487143  [  128/  265]
train() client id: f_00001-0-4 loss: 0.461604  [  160/  265]
train() client id: f_00001-0-5 loss: 0.498044  [  192/  265]
train() client id: f_00001-0-6 loss: 0.463662  [  224/  265]
train() client id: f_00001-0-7 loss: 0.567463  [  256/  265]
train() client id: f_00001-1-0 loss: 0.452279  [   32/  265]
train() client id: f_00001-1-1 loss: 0.564441  [   64/  265]
train() client id: f_00001-1-2 loss: 0.449687  [   96/  265]
train() client id: f_00001-1-3 loss: 0.448316  [  128/  265]
train() client id: f_00001-1-4 loss: 0.435765  [  160/  265]
train() client id: f_00001-1-5 loss: 0.542976  [  192/  265]
train() client id: f_00001-1-6 loss: 0.644219  [  224/  265]
train() client id: f_00001-1-7 loss: 0.512285  [  256/  265]
train() client id: f_00001-2-0 loss: 0.418308  [   32/  265]
train() client id: f_00001-2-1 loss: 0.612124  [   64/  265]
train() client id: f_00001-2-2 loss: 0.420517  [   96/  265]
train() client id: f_00001-2-3 loss: 0.481303  [  128/  265]
train() client id: f_00001-2-4 loss: 0.465914  [  160/  265]
train() client id: f_00001-2-5 loss: 0.564676  [  192/  265]
train() client id: f_00001-2-6 loss: 0.546195  [  224/  265]
train() client id: f_00001-2-7 loss: 0.505604  [  256/  265]
train() client id: f_00001-3-0 loss: 0.617877  [   32/  265]
train() client id: f_00001-3-1 loss: 0.459055  [   64/  265]
train() client id: f_00001-3-2 loss: 0.431707  [   96/  265]
train() client id: f_00001-3-3 loss: 0.405012  [  128/  265]
train() client id: f_00001-3-4 loss: 0.617014  [  160/  265]
train() client id: f_00001-3-5 loss: 0.477102  [  192/  265]
train() client id: f_00001-3-6 loss: 0.499348  [  224/  265]
train() client id: f_00001-3-7 loss: 0.411199  [  256/  265]
train() client id: f_00001-4-0 loss: 0.592749  [   32/  265]
train() client id: f_00001-4-1 loss: 0.387974  [   64/  265]
train() client id: f_00001-4-2 loss: 0.464960  [   96/  265]
train() client id: f_00001-4-3 loss: 0.452943  [  128/  265]
train() client id: f_00001-4-4 loss: 0.509731  [  160/  265]
train() client id: f_00001-4-5 loss: 0.757786  [  192/  265]
train() client id: f_00001-4-6 loss: 0.398547  [  224/  265]
train() client id: f_00001-4-7 loss: 0.419914  [  256/  265]
train() client id: f_00002-0-0 loss: 1.228617  [   32/  124]
train() client id: f_00002-0-1 loss: 1.195300  [   64/  124]
train() client id: f_00002-0-2 loss: 1.104900  [   96/  124]
train() client id: f_00002-1-0 loss: 1.355761  [   32/  124]
train() client id: f_00002-1-1 loss: 1.008578  [   64/  124]
train() client id: f_00002-1-2 loss: 1.011854  [   96/  124]
train() client id: f_00002-2-0 loss: 1.001727  [   32/  124]
train() client id: f_00002-2-1 loss: 0.979966  [   64/  124]
train() client id: f_00002-2-2 loss: 1.398097  [   96/  124]
train() client id: f_00002-3-0 loss: 1.096528  [   32/  124]
train() client id: f_00002-3-1 loss: 1.087701  [   64/  124]
train() client id: f_00002-3-2 loss: 1.034738  [   96/  124]
train() client id: f_00002-4-0 loss: 1.199224  [   32/  124]
train() client id: f_00002-4-1 loss: 1.003775  [   64/  124]
train() client id: f_00002-4-2 loss: 0.979699  [   96/  124]
train() client id: f_00003-0-0 loss: 0.411024  [   32/   43]
train() client id: f_00003-1-0 loss: 0.447496  [   32/   43]
train() client id: f_00003-2-0 loss: 0.270227  [   32/   43]
train() client id: f_00003-3-0 loss: 0.481178  [   32/   43]
train() client id: f_00003-4-0 loss: 0.344343  [   32/   43]
train() client id: f_00004-0-0 loss: 0.848755  [   32/  306]
train() client id: f_00004-0-1 loss: 0.736017  [   64/  306]
train() client id: f_00004-0-2 loss: 0.506007  [   96/  306]
train() client id: f_00004-0-3 loss: 0.899043  [  128/  306]
train() client id: f_00004-0-4 loss: 0.713377  [  160/  306]
train() client id: f_00004-0-5 loss: 0.881264  [  192/  306]
train() client id: f_00004-0-6 loss: 0.940745  [  224/  306]
train() client id: f_00004-0-7 loss: 0.701942  [  256/  306]
train() client id: f_00004-0-8 loss: 0.859761  [  288/  306]
train() client id: f_00004-1-0 loss: 0.910434  [   32/  306]
train() client id: f_00004-1-1 loss: 0.770308  [   64/  306]
train() client id: f_00004-1-2 loss: 0.798728  [   96/  306]
train() client id: f_00004-1-3 loss: 0.741385  [  128/  306]
train() client id: f_00004-1-4 loss: 0.756359  [  160/  306]
train() client id: f_00004-1-5 loss: 0.668420  [  192/  306]
train() client id: f_00004-1-6 loss: 0.701797  [  224/  306]
train() client id: f_00004-1-7 loss: 0.849161  [  256/  306]
train() client id: f_00004-1-8 loss: 0.950903  [  288/  306]
train() client id: f_00004-2-0 loss: 0.672348  [   32/  306]
train() client id: f_00004-2-1 loss: 0.753570  [   64/  306]
train() client id: f_00004-2-2 loss: 0.798136  [   96/  306]
train() client id: f_00004-2-3 loss: 0.602194  [  128/  306]
train() client id: f_00004-2-4 loss: 0.911920  [  160/  306]
train() client id: f_00004-2-5 loss: 0.746857  [  192/  306]
train() client id: f_00004-2-6 loss: 0.894668  [  224/  306]
train() client id: f_00004-2-7 loss: 0.737563  [  256/  306]
train() client id: f_00004-2-8 loss: 0.836183  [  288/  306]
train() client id: f_00004-3-0 loss: 0.727447  [   32/  306]
train() client id: f_00004-3-1 loss: 0.734438  [   64/  306]
train() client id: f_00004-3-2 loss: 0.907835  [   96/  306]
train() client id: f_00004-3-3 loss: 0.718049  [  128/  306]
train() client id: f_00004-3-4 loss: 0.744765  [  160/  306]
train() client id: f_00004-3-5 loss: 0.951523  [  192/  306]
train() client id: f_00004-3-6 loss: 0.636213  [  224/  306]
train() client id: f_00004-3-7 loss: 0.808078  [  256/  306]
train() client id: f_00004-3-8 loss: 0.766036  [  288/  306]
train() client id: f_00004-4-0 loss: 0.826480  [   32/  306]
train() client id: f_00004-4-1 loss: 0.727661  [   64/  306]
train() client id: f_00004-4-2 loss: 0.765174  [   96/  306]
train() client id: f_00004-4-3 loss: 0.779470  [  128/  306]
train() client id: f_00004-4-4 loss: 0.749311  [  160/  306]
train() client id: f_00004-4-5 loss: 0.780526  [  192/  306]
train() client id: f_00004-4-6 loss: 0.776308  [  224/  306]
train() client id: f_00004-4-7 loss: 0.779848  [  256/  306]
train() client id: f_00004-4-8 loss: 0.786910  [  288/  306]
train() client id: f_00005-0-0 loss: 0.303315  [   32/  146]
train() client id: f_00005-0-1 loss: 0.447229  [   64/  146]
train() client id: f_00005-0-2 loss: 0.420180  [   96/  146]
train() client id: f_00005-0-3 loss: 0.342041  [  128/  146]
train() client id: f_00005-1-0 loss: 0.509330  [   32/  146]
train() client id: f_00005-1-1 loss: 0.416350  [   64/  146]
train() client id: f_00005-1-2 loss: 0.169225  [   96/  146]
train() client id: f_00005-1-3 loss: 0.508345  [  128/  146]
train() client id: f_00005-2-0 loss: 0.441067  [   32/  146]
train() client id: f_00005-2-1 loss: 0.292733  [   64/  146]
train() client id: f_00005-2-2 loss: 0.504697  [   96/  146]
train() client id: f_00005-2-3 loss: 0.182898  [  128/  146]
train() client id: f_00005-3-0 loss: 0.506371  [   32/  146]
train() client id: f_00005-3-1 loss: 0.345782  [   64/  146]
train() client id: f_00005-3-2 loss: 0.448417  [   96/  146]
train() client id: f_00005-3-3 loss: 0.167162  [  128/  146]
train() client id: f_00005-4-0 loss: 0.587553  [   32/  146]
train() client id: f_00005-4-1 loss: 0.288446  [   64/  146]
train() client id: f_00005-4-2 loss: 0.342241  [   96/  146]
train() client id: f_00005-4-3 loss: 0.268162  [  128/  146]
train() client id: f_00006-0-0 loss: 0.410366  [   32/   54]
train() client id: f_00006-1-0 loss: 0.527944  [   32/   54]
train() client id: f_00006-2-0 loss: 0.507556  [   32/   54]
train() client id: f_00006-3-0 loss: 0.453540  [   32/   54]
train() client id: f_00006-4-0 loss: 0.459210  [   32/   54]
train() client id: f_00007-0-0 loss: 0.764847  [   32/  179]
train() client id: f_00007-0-1 loss: 0.780167  [   64/  179]
train() client id: f_00007-0-2 loss: 0.761870  [   96/  179]
train() client id: f_00007-0-3 loss: 0.801661  [  128/  179]
train() client id: f_00007-0-4 loss: 0.783562  [  160/  179]
train() client id: f_00007-1-0 loss: 0.948044  [   32/  179]
train() client id: f_00007-1-1 loss: 0.755027  [   64/  179]
train() client id: f_00007-1-2 loss: 0.784011  [   96/  179]
train() client id: f_00007-1-3 loss: 0.730805  [  128/  179]
train() client id: f_00007-1-4 loss: 0.581257  [  160/  179]
train() client id: f_00007-2-0 loss: 0.657640  [   32/  179]
train() client id: f_00007-2-1 loss: 0.884476  [   64/  179]
train() client id: f_00007-2-2 loss: 0.761455  [   96/  179]
train() client id: f_00007-2-3 loss: 0.871648  [  128/  179]
train() client id: f_00007-2-4 loss: 0.635904  [  160/  179]
train() client id: f_00007-3-0 loss: 0.825128  [   32/  179]
train() client id: f_00007-3-1 loss: 0.705720  [   64/  179]
train() client id: f_00007-3-2 loss: 0.678686  [   96/  179]
train() client id: f_00007-3-3 loss: 1.148768  [  128/  179]
train() client id: f_00007-3-4 loss: 0.596275  [  160/  179]
train() client id: f_00007-4-0 loss: 0.857286  [   32/  179]
train() client id: f_00007-4-1 loss: 0.589182  [   64/  179]
train() client id: f_00007-4-2 loss: 0.953707  [   96/  179]
train() client id: f_00007-4-3 loss: 0.944762  [  128/  179]
train() client id: f_00007-4-4 loss: 0.610377  [  160/  179]
train() client id: f_00008-0-0 loss: 0.749000  [   32/  130]
train() client id: f_00008-0-1 loss: 0.717140  [   64/  130]
train() client id: f_00008-0-2 loss: 0.626722  [   96/  130]
train() client id: f_00008-0-3 loss: 0.688284  [  128/  130]
train() client id: f_00008-1-0 loss: 0.780261  [   32/  130]
train() client id: f_00008-1-1 loss: 0.628198  [   64/  130]
train() client id: f_00008-1-2 loss: 0.580115  [   96/  130]
train() client id: f_00008-1-3 loss: 0.783282  [  128/  130]
train() client id: f_00008-2-0 loss: 0.751242  [   32/  130]
train() client id: f_00008-2-1 loss: 0.690658  [   64/  130]
train() client id: f_00008-2-2 loss: 0.613662  [   96/  130]
train() client id: f_00008-2-3 loss: 0.726984  [  128/  130]
train() client id: f_00008-3-0 loss: 0.739921  [   32/  130]
train() client id: f_00008-3-1 loss: 0.633698  [   64/  130]
train() client id: f_00008-3-2 loss: 0.682479  [   96/  130]
train() client id: f_00008-3-3 loss: 0.726701  [  128/  130]
train() client id: f_00008-4-0 loss: 0.777456  [   32/  130]
train() client id: f_00008-4-1 loss: 0.607217  [   64/  130]
train() client id: f_00008-4-2 loss: 0.712920  [   96/  130]
train() client id: f_00008-4-3 loss: 0.700634  [  128/  130]
train() client id: f_00009-0-0 loss: 0.745835  [   32/  118]
train() client id: f_00009-0-1 loss: 1.058792  [   64/  118]
train() client id: f_00009-0-2 loss: 0.823363  [   96/  118]
train() client id: f_00009-1-0 loss: 0.946756  [   32/  118]
train() client id: f_00009-1-1 loss: 0.896652  [   64/  118]
train() client id: f_00009-1-2 loss: 0.697949  [   96/  118]
train() client id: f_00009-2-0 loss: 0.920024  [   32/  118]
train() client id: f_00009-2-1 loss: 0.887719  [   64/  118]
train() client id: f_00009-2-2 loss: 0.708916  [   96/  118]
train() client id: f_00009-3-0 loss: 0.982816  [   32/  118]
train() client id: f_00009-3-1 loss: 0.629991  [   64/  118]
train() client id: f_00009-3-2 loss: 0.881479  [   96/  118]
train() client id: f_00009-4-0 loss: 0.814427  [   32/  118]
train() client id: f_00009-4-1 loss: 0.744512  [   64/  118]
train() client id: f_00009-4-2 loss: 0.785964  [   96/  118]
At round 73 accuracy: 0.649867374005305
At round 73 training accuracy: 0.5949027498323273
At round 73 training loss: 0.8199688726353607
update_location
xs = -3.905658 4.200318 385.009024 18.811294 0.979296 3.956410 -347.443192 -326.324852 369.663977 -312.060879 
ys = 377.587959 360.555839 1.320614 -347.455176 339.350187 322.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 390.625039 374.189999 397.785988 362.048290 353.778898 337.971344 361.557274 341.304241 383.353787 327.716347 
dists_bs = 265.170049 257.960701 586.322979 557.245214 240.789735 232.108638 247.562698 230.731587 566.959757 219.148757 
uav_gains = -121.047838 -120.363158 -121.321468 -119.796541 -119.375341 -118.477060 -119.772377 -118.677459 -120.755385 -117.820522 
bs_gains = -107.425744 -107.090558 -117.074893 -116.456357 -106.252922 -105.806416 -106.590245 -105.734057 -116.666522 -105.107751 
Round 74
-------------------------------
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.17965282 2.31354803 1.16601007 0.44876366 2.6663691  1.28432213
 0.54349708 1.6129016  1.17620535 1.04170236]
obj_prev = 13.43297219536657
eta_min = 1.985153234470665e-80	eta_max = 0.9640166902686983
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 3.044236871939507	eta = 0.9090909090909091
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 8.994246026442418	eta = 0.3076953929504865
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 5.5116993178884695	eta = 0.5021115822515108
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 4.941875446838901	eta = 0.5600076519876258
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 4.905748712654075	eta = 0.5641316397354368
af = 2.7674880653995517	bf = 0.5538889445570291	zeta = 4.905578223300182	eta = 0.5641512456685993
eta = 0.5641512456685993
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [0.04723579 0.09934512 0.04648602 0.01612015 0.11471549 0.05473354
 0.02024391 0.06710481 0.04873534 0.04423669]
ene_total = [0.51557111 0.70394459 0.51893062 0.28085198 0.80121777 0.40983951
 0.30711799 0.58944182 0.4390443  0.33961853]
ti_comp = [3.7239081  3.95531799 3.71124405 3.77370601 3.95943373 3.96148105
 3.77455137 3.80899893 3.85658756 3.96449942]
ti_coms = [0.32049446 0.08908456 0.3331585  0.27069654 0.08496883 0.08292151
 0.26985118 0.23540363 0.187815   0.07990314]
t_total = [26.29968948 26.29968948 26.29968948 26.29968948 26.29968948 26.29968948
 26.29968948 26.29968948 26.29968948 26.29968948]
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [4.75002260e-07 3.91702877e-06 4.55835322e-07 1.83844731e-08
 6.01838646e-06 6.53018300e-07 3.63942310e-08 1.30172529e-06
 4.86413493e-07 3.44232011e-07]
ene_total = [0.20841223 0.05795483 0.2166472  0.17602698 0.05529214 0.05392593
 0.17547739 0.15308527 0.12213433 0.05196115]
optimize_network iter = 0 obj = 1.2709174591621142
eta = 0.5641512456685993
freqs = [ 6342233.67091708 12558423.2934157   6262861.66135149  2135851.43012001
 14486350.74522172  6908216.2161498   2681631.30754287  8808720.47508642
  6318453.65727492  5579101.94228267]
eta_min = 0.5641512456686001	eta_max = 0.8530565293483942
af = 6.786276083478417e-05	bf = 0.5538889445570291	zeta = 7.464903691826259e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [7.90879612e-08 6.52185990e-07 7.58966627e-08 3.06101806e-09
 1.00206242e-06 1.08727664e-07 6.05964597e-09 2.16737493e-07
 8.09879336e-08 5.73146914e-08]
ene_total = [0.99918599 0.27775315 1.03866771 0.84393208 0.2649327  0.25852208
 0.84129665 0.73390845 0.58554042 0.2491103 ]
ti_comp = [1.04304861 1.27445851 1.03038457 1.09284653 1.27857424 1.28062156
 1.09369189 1.12813944 1.17572807 1.28363993]
ti_coms = [0.32049446 0.08908456 0.3331585  0.27069654 0.08496883 0.08292151
 0.26985118 0.23540363 0.187815   0.07990314]
t_total = [26.29968948 26.29968948 26.29968948 26.29968948 26.29968948 26.29968948
 26.29968948 26.29968948 26.29968948 26.29968948]
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [1.29575823e-07 8.07435167e-07 1.26557476e-07 4.69146470e-09
 1.23519014e-06 1.33732939e-07 9.27711067e-09 3.17581333e-07
 1.12005080e-07 7.02717911e-08]
ene_total = [0.61816447 0.17183973 0.64259052 0.52211305 0.16390964 0.15993959
 0.52048262 0.45404696 0.36225519 0.1541166 ]
optimize_network iter = 1 obj = 3.769458367807797
eta = 0.8530565293483942
freqs = [ 6286609.35895016 10821081.10112869  6262861.66135149  2047669.00897199
 12455063.70509777  5933111.71394756  2569503.70041527  8257350.63294458
  5754226.36383451  4783979.0224429 ]
eta_min = 0.8530565293483974	eta_max = 0.8530565293483936
af = 5.2542085376783516e-05	bf = 0.5538889445570291	zeta = 5.7796293914461874e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [7.77067690e-08 4.84219790e-07 7.58966627e-08 2.81347673e-09
 7.40744934e-07 8.01997961e-08 5.56349384e-09 1.90453887e-07
 6.71695741e-08 4.21420732e-08]
ene_total = [0.99918595 0.27774791 1.03866771 0.84393207 0.26492456 0.25852119
 0.84129664 0.73390763 0.58553999 0.24910983]
ti_comp = [1.04304861 1.27445851 1.03038457 1.09284653 1.27857424 1.28062156
 1.09369189 1.12813944 1.17572807 1.28363993]
ti_coms = [0.32049446 0.08908456 0.3331585  0.27069654 0.08496883 0.08292151
 0.26985118 0.23540363 0.187815   0.07990314]
t_total = [26.29968948 26.29968948 26.29968948 26.29968948 26.29968948 26.29968948
 26.29968948 26.29968948 26.29968948 26.29968948]
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [1.29575823e-07 8.07435167e-07 1.26557476e-07 4.69146470e-09
 1.23519014e-06 1.33732939e-07 9.27711067e-09 3.17581333e-07
 1.12005080e-07 7.02717911e-08]
ene_total = [0.61816447 0.17183973 0.64259052 0.52211305 0.16390964 0.15993959
 0.52048262 0.45404696 0.36225519 0.1541166 ]
optimize_network iter = 2 obj = 3.7694583678077827
eta = 0.8530565293483936
freqs = [ 6286609.35895015 10821081.10112868  6262861.66135148  2047669.00897199
 12455063.70509776  5933111.71394756  2569503.70041527  8257350.63294457
  5754226.3638345   4783979.0224429 ]
Done!
ene_coms = [0.03204945 0.00890846 0.03331585 0.02706965 0.00849688 0.00829215
 0.02698512 0.02354036 0.0187815  0.00799031]
ene_comp = [1.24492590e-07 7.75759650e-07 1.21592652e-07 4.50741950e-09
 1.18673388e-06 1.28486623e-07 8.91317151e-09 3.05122682e-07
 1.07611143e-07 6.75150431e-08]
ene_total = [0.03204957 0.00890923 0.03331597 0.02706966 0.00849807 0.00829228
 0.02698513 0.02354067 0.01878161 0.00799038]
At round 74 energy consumption: 0.1954325657499382
At round 74 eta: 0.8530565293483936
At round 74 a_n: 2.834210180524895
At round 74 local rounds: 5.2041580597357475
At round 74 global rounds: 19.287758537054202
gradient difference: 0.7315664291381836
train() client id: f_00000-0-0 loss: 1.067615  [   32/  126]
train() client id: f_00000-0-1 loss: 1.047796  [   64/  126]
train() client id: f_00000-0-2 loss: 1.111992  [   96/  126]
train() client id: f_00000-1-0 loss: 0.949881  [   32/  126]
train() client id: f_00000-1-1 loss: 0.930972  [   64/  126]
train() client id: f_00000-1-2 loss: 1.077122  [   96/  126]
train() client id: f_00000-2-0 loss: 0.830446  [   32/  126]
train() client id: f_00000-2-1 loss: 1.214345  [   64/  126]
train() client id: f_00000-2-2 loss: 1.087355  [   96/  126]
train() client id: f_00000-3-0 loss: 0.987504  [   32/  126]
train() client id: f_00000-3-1 loss: 1.034032  [   64/  126]
train() client id: f_00000-3-2 loss: 0.814894  [   96/  126]
train() client id: f_00000-4-0 loss: 1.114197  [   32/  126]
train() client id: f_00000-4-1 loss: 0.940614  [   64/  126]
train() client id: f_00000-4-2 loss: 1.004684  [   96/  126]
train() client id: f_00001-0-0 loss: 0.565098  [   32/  265]
train() client id: f_00001-0-1 loss: 0.725850  [   64/  265]
train() client id: f_00001-0-2 loss: 0.502671  [   96/  265]
train() client id: f_00001-0-3 loss: 0.435574  [  128/  265]
train() client id: f_00001-0-4 loss: 0.515761  [  160/  265]
train() client id: f_00001-0-5 loss: 0.469209  [  192/  265]
train() client id: f_00001-0-6 loss: 0.526567  [  224/  265]
train() client id: f_00001-0-7 loss: 0.541438  [  256/  265]
train() client id: f_00001-1-0 loss: 0.527719  [   32/  265]
train() client id: f_00001-1-1 loss: 0.560216  [   64/  265]
train() client id: f_00001-1-2 loss: 0.504297  [   96/  265]
train() client id: f_00001-1-3 loss: 0.530776  [  128/  265]
train() client id: f_00001-1-4 loss: 0.512892  [  160/  265]
train() client id: f_00001-1-5 loss: 0.501046  [  192/  265]
train() client id: f_00001-1-6 loss: 0.456511  [  224/  265]
train() client id: f_00001-1-7 loss: 0.595210  [  256/  265]
train() client id: f_00001-2-0 loss: 0.554781  [   32/  265]
train() client id: f_00001-2-1 loss: 0.425899  [   64/  265]
train() client id: f_00001-2-2 loss: 0.675677  [   96/  265]
train() client id: f_00001-2-3 loss: 0.455126  [  128/  265]
train() client id: f_00001-2-4 loss: 0.522106  [  160/  265]
train() client id: f_00001-2-5 loss: 0.527409  [  192/  265]
train() client id: f_00001-2-6 loss: 0.451124  [  224/  265]
train() client id: f_00001-2-7 loss: 0.620629  [  256/  265]
train() client id: f_00001-3-0 loss: 0.429380  [   32/  265]
train() client id: f_00001-3-1 loss: 0.477574  [   64/  265]
train() client id: f_00001-3-2 loss: 0.528985  [   96/  265]
train() client id: f_00001-3-3 loss: 0.445133  [  128/  265]
train() client id: f_00001-3-4 loss: 0.427283  [  160/  265]
train() client id: f_00001-3-5 loss: 0.739644  [  192/  265]
train() client id: f_00001-3-6 loss: 0.430449  [  224/  265]
train() client id: f_00001-3-7 loss: 0.690129  [  256/  265]
train() client id: f_00001-4-0 loss: 0.557678  [   32/  265]
train() client id: f_00001-4-1 loss: 0.497987  [   64/  265]
train() client id: f_00001-4-2 loss: 0.647528  [   96/  265]
train() client id: f_00001-4-3 loss: 0.556168  [  128/  265]
train() client id: f_00001-4-4 loss: 0.524763  [  160/  265]
train() client id: f_00001-4-5 loss: 0.413714  [  192/  265]
train() client id: f_00001-4-6 loss: 0.436600  [  224/  265]
train() client id: f_00001-4-7 loss: 0.588710  [  256/  265]
train() client id: f_00002-0-0 loss: 1.053591  [   32/  124]
train() client id: f_00002-0-1 loss: 0.984672  [   64/  124]
train() client id: f_00002-0-2 loss: 1.079738  [   96/  124]
train() client id: f_00002-1-0 loss: 0.851920  [   32/  124]
train() client id: f_00002-1-1 loss: 1.158404  [   64/  124]
train() client id: f_00002-1-2 loss: 1.152216  [   96/  124]
train() client id: f_00002-2-0 loss: 0.974560  [   32/  124]
train() client id: f_00002-2-1 loss: 0.977620  [   64/  124]
train() client id: f_00002-2-2 loss: 1.187275  [   96/  124]
train() client id: f_00002-3-0 loss: 1.064213  [   32/  124]
train() client id: f_00002-3-1 loss: 0.793136  [   64/  124]
train() client id: f_00002-3-2 loss: 1.194685  [   96/  124]
train() client id: f_00002-4-0 loss: 1.048347  [   32/  124]
train() client id: f_00002-4-1 loss: 0.963231  [   64/  124]
train() client id: f_00002-4-2 loss: 1.035489  [   96/  124]
train() client id: f_00003-0-0 loss: 0.748213  [   32/   43]
train() client id: f_00003-1-0 loss: 0.711295  [   32/   43]
train() client id: f_00003-2-0 loss: 0.806084  [   32/   43]
train() client id: f_00003-3-0 loss: 0.946005  [   32/   43]
train() client id: f_00003-4-0 loss: 0.941224  [   32/   43]
train() client id: f_00004-0-0 loss: 0.726745  [   32/  306]
train() client id: f_00004-0-1 loss: 0.828797  [   64/  306]
train() client id: f_00004-0-2 loss: 0.852696  [   96/  306]
train() client id: f_00004-0-3 loss: 0.902726  [  128/  306]
train() client id: f_00004-0-4 loss: 0.927754  [  160/  306]
train() client id: f_00004-0-5 loss: 0.991711  [  192/  306]
train() client id: f_00004-0-6 loss: 0.817244  [  224/  306]
train() client id: f_00004-0-7 loss: 0.805218  [  256/  306]
train() client id: f_00004-0-8 loss: 0.782691  [  288/  306]
train() client id: f_00004-1-0 loss: 0.824993  [   32/  306]
train() client id: f_00004-1-1 loss: 0.789850  [   64/  306]
train() client id: f_00004-1-2 loss: 0.831980  [   96/  306]
train() client id: f_00004-1-3 loss: 0.805316  [  128/  306]
train() client id: f_00004-1-4 loss: 0.832183  [  160/  306]
train() client id: f_00004-1-5 loss: 0.954360  [  192/  306]
train() client id: f_00004-1-6 loss: 0.836729  [  224/  306]
train() client id: f_00004-1-7 loss: 0.904056  [  256/  306]
train() client id: f_00004-1-8 loss: 0.875995  [  288/  306]
train() client id: f_00004-2-0 loss: 1.031706  [   32/  306]
train() client id: f_00004-2-1 loss: 0.884146  [   64/  306]
train() client id: f_00004-2-2 loss: 0.713863  [   96/  306]
train() client id: f_00004-2-3 loss: 0.746162  [  128/  306]
train() client id: f_00004-2-4 loss: 0.964504  [  160/  306]
train() client id: f_00004-2-5 loss: 0.705284  [  192/  306]
train() client id: f_00004-2-6 loss: 0.772235  [  224/  306]
train() client id: f_00004-2-7 loss: 0.875523  [  256/  306]
train() client id: f_00004-2-8 loss: 0.778201  [  288/  306]
train() client id: f_00004-3-0 loss: 0.681452  [   32/  306]
train() client id: f_00004-3-1 loss: 0.790174  [   64/  306]
train() client id: f_00004-3-2 loss: 0.910289  [   96/  306]
train() client id: f_00004-3-3 loss: 0.890827  [  128/  306]
train() client id: f_00004-3-4 loss: 0.765056  [  160/  306]
train() client id: f_00004-3-5 loss: 0.905487  [  192/  306]
train() client id: f_00004-3-6 loss: 0.710536  [  224/  306]
train() client id: f_00004-3-7 loss: 0.917017  [  256/  306]
train() client id: f_00004-3-8 loss: 0.975276  [  288/  306]
train() client id: f_00004-4-0 loss: 0.785768  [   32/  306]
train() client id: f_00004-4-1 loss: 0.886769  [   64/  306]
train() client id: f_00004-4-2 loss: 0.868356  [   96/  306]
train() client id: f_00004-4-3 loss: 0.733593  [  128/  306]
train() client id: f_00004-4-4 loss: 0.875513  [  160/  306]
train() client id: f_00004-4-5 loss: 0.946469  [  192/  306]
train() client id: f_00004-4-6 loss: 0.845675  [  224/  306]
train() client id: f_00004-4-7 loss: 0.742466  [  256/  306]
train() client id: f_00004-4-8 loss: 0.903864  [  288/  306]
train() client id: f_00005-0-0 loss: 0.700156  [   32/  146]
train() client id: f_00005-0-1 loss: 0.470711  [   64/  146]
train() client id: f_00005-0-2 loss: 0.860694  [   96/  146]
train() client id: f_00005-0-3 loss: 0.839372  [  128/  146]
train() client id: f_00005-1-0 loss: 0.671628  [   32/  146]
train() client id: f_00005-1-1 loss: 0.711242  [   64/  146]
train() client id: f_00005-1-2 loss: 0.766845  [   96/  146]
train() client id: f_00005-1-3 loss: 0.592221  [  128/  146]
train() client id: f_00005-2-0 loss: 0.639574  [   32/  146]
train() client id: f_00005-2-1 loss: 0.442065  [   64/  146]
train() client id: f_00005-2-2 loss: 0.704969  [   96/  146]
train() client id: f_00005-2-3 loss: 0.645414  [  128/  146]
train() client id: f_00005-3-0 loss: 0.594286  [   32/  146]
train() client id: f_00005-3-1 loss: 0.873451  [   64/  146]
train() client id: f_00005-3-2 loss: 0.762082  [   96/  146]
train() client id: f_00005-3-3 loss: 0.738455  [  128/  146]
train() client id: f_00005-4-0 loss: 0.641837  [   32/  146]
train() client id: f_00005-4-1 loss: 0.634210  [   64/  146]
train() client id: f_00005-4-2 loss: 0.686399  [   96/  146]
train() client id: f_00005-4-3 loss: 0.704131  [  128/  146]
train() client id: f_00006-0-0 loss: 0.473497  [   32/   54]
train() client id: f_00006-1-0 loss: 0.536922  [   32/   54]
train() client id: f_00006-2-0 loss: 0.465730  [   32/   54]
train() client id: f_00006-3-0 loss: 0.500318  [   32/   54]
train() client id: f_00006-4-0 loss: 0.533701  [   32/   54]
train() client id: f_00007-0-0 loss: 0.873342  [   32/  179]
train() client id: f_00007-0-1 loss: 0.687375  [   64/  179]
train() client id: f_00007-0-2 loss: 0.699037  [   96/  179]
train() client id: f_00007-0-3 loss: 0.847896  [  128/  179]
train() client id: f_00007-0-4 loss: 0.884266  [  160/  179]
train() client id: f_00007-1-0 loss: 0.936624  [   32/  179]
train() client id: f_00007-1-1 loss: 0.738456  [   64/  179]
train() client id: f_00007-1-2 loss: 0.786410  [   96/  179]
train() client id: f_00007-1-3 loss: 0.857148  [  128/  179]
train() client id: f_00007-1-4 loss: 0.683941  [  160/  179]
train() client id: f_00007-2-0 loss: 0.839388  [   32/  179]
train() client id: f_00007-2-1 loss: 0.796629  [   64/  179]
train() client id: f_00007-2-2 loss: 0.586130  [   96/  179]
train() client id: f_00007-2-3 loss: 0.601708  [  128/  179]
train() client id: f_00007-2-4 loss: 0.855073  [  160/  179]
train() client id: f_00007-3-0 loss: 0.820909  [   32/  179]
train() client id: f_00007-3-1 loss: 0.623408  [   64/  179]
train() client id: f_00007-3-2 loss: 0.785550  [   96/  179]
train() client id: f_00007-3-3 loss: 0.773414  [  128/  179]
train() client id: f_00007-3-4 loss: 0.712077  [  160/  179]
train() client id: f_00007-4-0 loss: 0.659349  [   32/  179]
train() client id: f_00007-4-1 loss: 0.662267  [   64/  179]
train() client id: f_00007-4-2 loss: 1.071020  [   96/  179]
train() client id: f_00007-4-3 loss: 0.741992  [  128/  179]
train() client id: f_00007-4-4 loss: 0.695513  [  160/  179]
train() client id: f_00008-0-0 loss: 0.723943  [   32/  130]
train() client id: f_00008-0-1 loss: 0.622645  [   64/  130]
train() client id: f_00008-0-2 loss: 0.778737  [   96/  130]
train() client id: f_00008-0-3 loss: 0.754494  [  128/  130]
train() client id: f_00008-1-0 loss: 0.721844  [   32/  130]
train() client id: f_00008-1-1 loss: 0.669286  [   64/  130]
train() client id: f_00008-1-2 loss: 0.709631  [   96/  130]
train() client id: f_00008-1-3 loss: 0.780907  [  128/  130]
train() client id: f_00008-2-0 loss: 0.757343  [   32/  130]
train() client id: f_00008-2-1 loss: 0.761403  [   64/  130]
train() client id: f_00008-2-2 loss: 0.621066  [   96/  130]
train() client id: f_00008-2-3 loss: 0.744801  [  128/  130]
train() client id: f_00008-3-0 loss: 0.735981  [   32/  130]
train() client id: f_00008-3-1 loss: 0.684716  [   64/  130]
train() client id: f_00008-3-2 loss: 0.841574  [   96/  130]
train() client id: f_00008-3-3 loss: 0.639960  [  128/  130]
train() client id: f_00008-4-0 loss: 0.701896  [   32/  130]
train() client id: f_00008-4-1 loss: 0.825256  [   64/  130]
train() client id: f_00008-4-2 loss: 0.651268  [   96/  130]
train() client id: f_00008-4-3 loss: 0.737198  [  128/  130]
train() client id: f_00009-0-0 loss: 0.813888  [   32/  118]
train() client id: f_00009-0-1 loss: 0.878514  [   64/  118]
train() client id: f_00009-0-2 loss: 0.971226  [   96/  118]
train() client id: f_00009-1-0 loss: 0.880842  [   32/  118]
train() client id: f_00009-1-1 loss: 0.850215  [   64/  118]
train() client id: f_00009-1-2 loss: 1.016409  [   96/  118]
train() client id: f_00009-2-0 loss: 0.873779  [   32/  118]
train() client id: f_00009-2-1 loss: 0.845341  [   64/  118]
train() client id: f_00009-2-2 loss: 0.746717  [   96/  118]
train() client id: f_00009-3-0 loss: 0.876676  [   32/  118]
train() client id: f_00009-3-1 loss: 0.629983  [   64/  118]
train() client id: f_00009-3-2 loss: 0.814163  [   96/  118]
train() client id: f_00009-4-0 loss: 0.891506  [   32/  118]
train() client id: f_00009-4-1 loss: 0.771865  [   64/  118]
train() client id: f_00009-4-2 loss: 0.754275  [   96/  118]
At round 74 accuracy: 0.6445623342175066
At round 74 training accuracy: 0.5902079141515761
At round 74 training loss: 0.8169949005678774
update_location
xs = -3.905658 4.200318 390.009024 18.811294 0.979296 3.956410 -352.443192 -331.324852 374.663977 -317.060879 
ys = 382.587959 365.555839 1.320614 -352.455176 344.350187 327.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 395.460239 379.010176 402.627350 366.849446 358.577760 342.750304 366.364701 346.087899 388.177492 332.480996 
dists_bs = 269.009358 261.580354 591.100437 561.935744 244.229807 235.324375 251.071148 234.041265 571.764816 222.310113 
uav_gains = -121.234069 -120.572956 -121.499136 -120.027520 -119.623507 -118.762508 -120.004626 -118.954535 -120.951122 -118.133222 
bs_gains = -107.600546 -107.260003 -117.173575 -116.558286 -106.425422 -105.973733 -106.761370 -105.907247 -116.769147 -105.281917 
Round 75
-------------------------------
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.03922851 2.03415137 1.02725189 0.39661097 2.34431442 1.12928775
 0.47989662 1.41999007 1.0345605  0.91598534]
obj_prev = 11.821277437641127
eta_min = 2.632614097881737e-91	eta_max = 0.9674950324086027
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 2.6763069615753383	eta = 0.9090909090909091
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 8.03601399311375	eta = 0.3027628287842397
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 4.8844582536009025	eta = 0.4981118073659859
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 4.372075946270912	eta = 0.5564876636646818
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 4.3396007499684774	eta = 0.5606521126909028
af = 2.433006328704853	bf = 0.4986564559927847	zeta = 4.3394472210647415	eta = 0.5606719484671788
eta = 0.5606719484671788
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [0.04774628 0.10041876 0.0469884  0.01629436 0.11595525 0.05532505
 0.02046269 0.06783003 0.04926203 0.04471477]
ene_total = [0.45740881 0.62055196 0.46033181 0.25063986 0.70629813 0.3612359
 0.2738053  0.5227749  0.38706651 0.29933403]
ti_comp = [4.2992849  4.53835717 4.28655232 4.34933896 4.54253515 4.54464388
 4.35017711 4.38486951 4.43846938 4.54768586]
ti_coms = [0.32903639 0.08996413 0.34176898 0.27898233 0.08578614 0.08367741
 0.27814418 0.24345178 0.18985191 0.08063543]
t_total = [26.24968529 26.24968529 26.24968529 26.24968529 26.24968529 26.24968529
 26.24968529 26.24968529 26.24968529 26.24968529]
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [3.68049423e-07 3.07275006e-06 3.52887078e-07 1.42937379e-08
 4.72231164e-06 5.12443289e-07 2.82979099e-08 1.01445480e-06
 3.79271804e-07 2.70179295e-07]
ene_total = [0.18661622 0.05104089 0.19383747 0.15822588 0.04868069 0.04746084
 0.1577506  0.13808028 0.10767732 0.0457342 ]
optimize_network iter = 0 obj = 1.1351043989055445
eta = 0.5606719484671788
freqs = [ 5552816.27091243 11063338.14405196  5480908.36118572  1873200.15342277
 12763274.63448044  6086841.3557449   2351937.65383736  7734555.17253756
  5549439.33843933  4916211.22498621]
eta_min = 0.5606719484671795	eta_max = 0.8680569114774389
af = 4.622705191075305e-05	bf = 0.4986564559927847	zeta = 5.084975710182836e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [6.06251059e-08 5.06143430e-07 5.81275641e-08 2.35446469e-09
 7.77859235e-07 8.44096653e-08 4.66123208e-09 1.67101009e-07
 6.24736566e-08 4.45039370e-08]
ene_total = [0.90183473 0.24659032 0.93673252 0.76464342 0.23514662 0.22934794
 0.76234626 0.66726472 0.52035367 0.22100929]
ti_comp = [1.06098413 1.30005639 1.04825154 1.11103819 1.30423438 1.30634311
 1.11187634 1.14656874 1.20016861 1.30938509]
ti_coms = [0.32903639 0.08996413 0.34176898 0.27898233 0.08578614 0.08367741
 0.27814418 0.24345178 0.18985191 0.08063543]
t_total = [26.24968529 26.24968529 26.24968529 26.24968529 26.24968529 26.24968529
 26.24968529 26.24968529 26.24968529 26.24968529]
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [8.83797899e-08 5.47611225e-07 8.62964911e-08 3.20336552e-09
 8.37745486e-07 9.06991369e-08 6.33471974e-09 2.16979151e-07
 7.58585669e-08 4.76617893e-08]
ene_total = [0.62136672 0.16990208 0.64541139 0.52684109 0.16201769 0.15802138
 0.52525835 0.45974786 0.35852513 0.15227597]
optimize_network iter = 1 obj = 3.7793676563367
eta = 0.8680569114774389
freqs = [ 5502474.32221998  9444523.43544286  5480908.36118572  1793229.33817215
 10870816.12174629  5178356.71698614  2250264.85743831  7233513.5913764
  5018771.87691086  4175523.98380345]
eta_min = 0.8680569114774714	eta_max = 0.8680569114774374
af = 3.523192231617816e-05	bf = 0.4986564559927847	zeta = 3.8755114547795976e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [5.95308319e-08 3.68859801e-07 5.81275641e-08 2.15772197e-09
 5.64288349e-07 6.10930969e-08 4.26693859e-09 1.46152750e-07
 5.10967904e-08 3.21040135e-08]
ene_total = [0.9018347  0.24658655 0.93673252 0.76464342 0.23514077 0.2293473
 0.76234625 0.66726414 0.52035336 0.22100895]
ti_comp = [1.06098413 1.30005639 1.04825154 1.11103819 1.30423438 1.30634311
 1.11187634 1.14656874 1.20016861 1.30938509]
ti_coms = [0.32903639 0.08996413 0.34176898 0.27898233 0.08578614 0.08367741
 0.27814418 0.24345178 0.18985191 0.08063543]
t_total = [26.24968529 26.24968529 26.24968529 26.24968529 26.24968529 26.24968529
 26.24968529 26.24968529 26.24968529 26.24968529]
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [8.83797899e-08 5.47611225e-07 8.62964911e-08 3.20336552e-09
 8.37745486e-07 9.06991369e-08 6.33471974e-09 2.16979151e-07
 7.58585669e-08 4.76617893e-08]
ene_total = [0.62136672 0.16990208 0.64541139 0.52684109 0.16201769 0.15802138
 0.52525835 0.45974786 0.35852513 0.15227597]
optimize_network iter = 2 obj = 3.779367656336656
eta = 0.8680569114774374
freqs = [ 5502474.32221997  9444523.43544286  5480908.3611857   1793229.33817215
 10870816.12174629  5178356.71698613  2250264.85743831  7233513.59137638
  5018771.87691085  4175523.98380344]
Done!
ene_coms = [0.03290364 0.00899641 0.0341769  0.02789823 0.00857861 0.00836774
 0.02781442 0.02434518 0.01898519 0.00806354]
ene_comp = [7.62986036e-08 4.72754821e-07 7.45000840e-08 2.76547745e-09
 7.23228816e-07 7.83009046e-08 5.46878728e-09 1.87318914e-07
 6.54889850e-08 4.11466013e-08]
ene_total = [0.03290372 0.00899689 0.03417697 0.02789824 0.00857934 0.00836782
 0.02781442 0.02434537 0.01898526 0.00806358]
At round 75 energy consumption: 0.20013159600162664
At round 75 eta: 0.8680569114774374
At round 75 a_n: 2.4916643335555797
At round 75 local rounds: 4.633363424333758
At round 75 global rounds: 18.884386908447258
gradient difference: 0.9072305560112
train() client id: f_00000-0-0 loss: 0.813994  [   32/  126]
train() client id: f_00000-0-1 loss: 0.875338  [   64/  126]
train() client id: f_00000-0-2 loss: 1.005442  [   96/  126]
train() client id: f_00000-1-0 loss: 1.095174  [   32/  126]
train() client id: f_00000-1-1 loss: 0.886344  [   64/  126]
train() client id: f_00000-1-2 loss: 0.691728  [   96/  126]
train() client id: f_00000-2-0 loss: 0.972527  [   32/  126]
train() client id: f_00000-2-1 loss: 0.912073  [   64/  126]
train() client id: f_00000-2-2 loss: 0.817544  [   96/  126]
train() client id: f_00000-3-0 loss: 0.779489  [   32/  126]
train() client id: f_00000-3-1 loss: 0.981972  [   64/  126]
train() client id: f_00000-3-2 loss: 0.731155  [   96/  126]
train() client id: f_00001-0-0 loss: 0.449257  [   32/  265]
train() client id: f_00001-0-1 loss: 0.395209  [   64/  265]
train() client id: f_00001-0-2 loss: 0.569737  [   96/  265]
train() client id: f_00001-0-3 loss: 0.386011  [  128/  265]
train() client id: f_00001-0-4 loss: 0.478876  [  160/  265]
train() client id: f_00001-0-5 loss: 0.347074  [  192/  265]
train() client id: f_00001-0-6 loss: 0.571675  [  224/  265]
train() client id: f_00001-0-7 loss: 0.457780  [  256/  265]
train() client id: f_00001-1-0 loss: 0.415538  [   32/  265]
train() client id: f_00001-1-1 loss: 0.569000  [   64/  265]
train() client id: f_00001-1-2 loss: 0.441139  [   96/  265]
train() client id: f_00001-1-3 loss: 0.583873  [  128/  265]
train() client id: f_00001-1-4 loss: 0.535006  [  160/  265]
train() client id: f_00001-1-5 loss: 0.438212  [  192/  265]
train() client id: f_00001-1-6 loss: 0.406310  [  224/  265]
train() client id: f_00001-1-7 loss: 0.380040  [  256/  265]
train() client id: f_00001-2-0 loss: 0.520354  [   32/  265]
train() client id: f_00001-2-1 loss: 0.374397  [   64/  265]
train() client id: f_00001-2-2 loss: 0.495368  [   96/  265]
train() client id: f_00001-2-3 loss: 0.369345  [  128/  265]
train() client id: f_00001-2-4 loss: 0.476782  [  160/  265]
train() client id: f_00001-2-5 loss: 0.478347  [  192/  265]
train() client id: f_00001-2-6 loss: 0.491330  [  224/  265]
train() client id: f_00001-2-7 loss: 0.541028  [  256/  265]
train() client id: f_00001-3-0 loss: 0.642004  [   32/  265]
train() client id: f_00001-3-1 loss: 0.536243  [   64/  265]
train() client id: f_00001-3-2 loss: 0.496299  [   96/  265]
train() client id: f_00001-3-3 loss: 0.389675  [  128/  265]
train() client id: f_00001-3-4 loss: 0.361409  [  160/  265]
train() client id: f_00001-3-5 loss: 0.471348  [  192/  265]
train() client id: f_00001-3-6 loss: 0.431690  [  224/  265]
train() client id: f_00001-3-7 loss: 0.397478  [  256/  265]
train() client id: f_00002-0-0 loss: 1.003258  [   32/  124]
train() client id: f_00002-0-1 loss: 0.918664  [   64/  124]
train() client id: f_00002-0-2 loss: 1.066280  [   96/  124]
train() client id: f_00002-1-0 loss: 1.026398  [   32/  124]
train() client id: f_00002-1-1 loss: 0.835952  [   64/  124]
train() client id: f_00002-1-2 loss: 0.850802  [   96/  124]
train() client id: f_00002-2-0 loss: 0.963592  [   32/  124]
train() client id: f_00002-2-1 loss: 1.109329  [   64/  124]
train() client id: f_00002-2-2 loss: 0.846187  [   96/  124]
train() client id: f_00002-3-0 loss: 0.988015  [   32/  124]
train() client id: f_00002-3-1 loss: 1.070531  [   64/  124]
train() client id: f_00002-3-2 loss: 0.888284  [   96/  124]
train() client id: f_00003-0-0 loss: 0.610344  [   32/   43]
train() client id: f_00003-1-0 loss: 0.405845  [   32/   43]
train() client id: f_00003-2-0 loss: 0.410387  [   32/   43]
train() client id: f_00003-3-0 loss: 0.546695  [   32/   43]
train() client id: f_00004-0-0 loss: 0.811329  [   32/  306]
train() client id: f_00004-0-1 loss: 0.670632  [   64/  306]
train() client id: f_00004-0-2 loss: 0.752166  [   96/  306]
train() client id: f_00004-0-3 loss: 0.695978  [  128/  306]
train() client id: f_00004-0-4 loss: 0.868570  [  160/  306]
train() client id: f_00004-0-5 loss: 0.837702  [  192/  306]
train() client id: f_00004-0-6 loss: 0.737670  [  224/  306]
train() client id: f_00004-0-7 loss: 0.746539  [  256/  306]
train() client id: f_00004-0-8 loss: 0.662156  [  288/  306]
train() client id: f_00004-1-0 loss: 0.707600  [   32/  306]
train() client id: f_00004-1-1 loss: 0.722371  [   64/  306]
train() client id: f_00004-1-2 loss: 0.797579  [   96/  306]
train() client id: f_00004-1-3 loss: 0.597568  [  128/  306]
train() client id: f_00004-1-4 loss: 0.769860  [  160/  306]
train() client id: f_00004-1-5 loss: 0.870893  [  192/  306]
train() client id: f_00004-1-6 loss: 0.879559  [  224/  306]
train() client id: f_00004-1-7 loss: 0.740670  [  256/  306]
train() client id: f_00004-1-8 loss: 0.709994  [  288/  306]
train() client id: f_00004-2-0 loss: 0.627991  [   32/  306]
train() client id: f_00004-2-1 loss: 0.791858  [   64/  306]
train() client id: f_00004-2-2 loss: 0.758145  [   96/  306]
train() client id: f_00004-2-3 loss: 0.906892  [  128/  306]
train() client id: f_00004-2-4 loss: 0.833005  [  160/  306]
train() client id: f_00004-2-5 loss: 0.832920  [  192/  306]
train() client id: f_00004-2-6 loss: 0.756112  [  224/  306]
train() client id: f_00004-2-7 loss: 0.642649  [  256/  306]
train() client id: f_00004-2-8 loss: 0.747037  [  288/  306]
train() client id: f_00004-3-0 loss: 0.863846  [   32/  306]
train() client id: f_00004-3-1 loss: 0.692359  [   64/  306]
train() client id: f_00004-3-2 loss: 0.765145  [   96/  306]
train() client id: f_00004-3-3 loss: 0.859352  [  128/  306]
train() client id: f_00004-3-4 loss: 0.836319  [  160/  306]
train() client id: f_00004-3-5 loss: 0.714404  [  192/  306]
train() client id: f_00004-3-6 loss: 0.802865  [  224/  306]
train() client id: f_00004-3-7 loss: 0.684167  [  256/  306]
train() client id: f_00004-3-8 loss: 0.701974  [  288/  306]
train() client id: f_00005-0-0 loss: 0.727050  [   32/  146]
train() client id: f_00005-0-1 loss: 0.539196  [   64/  146]
train() client id: f_00005-0-2 loss: 0.535036  [   96/  146]
train() client id: f_00005-0-3 loss: 0.600587  [  128/  146]
train() client id: f_00005-1-0 loss: 0.577870  [   32/  146]
train() client id: f_00005-1-1 loss: 0.625994  [   64/  146]
train() client id: f_00005-1-2 loss: 0.573317  [   96/  146]
train() client id: f_00005-1-3 loss: 0.528703  [  128/  146]
train() client id: f_00005-2-0 loss: 0.545792  [   32/  146]
train() client id: f_00005-2-1 loss: 0.571593  [   64/  146]
train() client id: f_00005-2-2 loss: 0.647915  [   96/  146]
train() client id: f_00005-2-3 loss: 0.562678  [  128/  146]
train() client id: f_00005-3-0 loss: 0.515474  [   32/  146]
train() client id: f_00005-3-1 loss: 0.694272  [   64/  146]
train() client id: f_00005-3-2 loss: 0.537076  [   96/  146]
train() client id: f_00005-3-3 loss: 0.610336  [  128/  146]
train() client id: f_00006-0-0 loss: 0.469660  [   32/   54]
train() client id: f_00006-1-0 loss: 0.493876  [   32/   54]
train() client id: f_00006-2-0 loss: 0.466650  [   32/   54]
train() client id: f_00006-3-0 loss: 0.417900  [   32/   54]
train() client id: f_00007-0-0 loss: 0.680869  [   32/  179]
train() client id: f_00007-0-1 loss: 0.718062  [   64/  179]
train() client id: f_00007-0-2 loss: 0.672397  [   96/  179]
train() client id: f_00007-0-3 loss: 0.824616  [  128/  179]
train() client id: f_00007-0-4 loss: 0.681591  [  160/  179]
train() client id: f_00007-1-0 loss: 0.558332  [   32/  179]
train() client id: f_00007-1-1 loss: 0.847865  [   64/  179]
train() client id: f_00007-1-2 loss: 0.690873  [   96/  179]
train() client id: f_00007-1-3 loss: 0.820742  [  128/  179]
train() client id: f_00007-1-4 loss: 0.682791  [  160/  179]
train() client id: f_00007-2-0 loss: 0.741256  [   32/  179]
train() client id: f_00007-2-1 loss: 0.592650  [   64/  179]
train() client id: f_00007-2-2 loss: 0.707937  [   96/  179]
train() client id: f_00007-2-3 loss: 0.805274  [  128/  179]
train() client id: f_00007-2-4 loss: 0.761978  [  160/  179]
train() client id: f_00007-3-0 loss: 0.624494  [   32/  179]
train() client id: f_00007-3-1 loss: 0.780666  [   64/  179]
train() client id: f_00007-3-2 loss: 0.570538  [   96/  179]
train() client id: f_00007-3-3 loss: 0.535250  [  128/  179]
train() client id: f_00007-3-4 loss: 0.889649  [  160/  179]
train() client id: f_00008-0-0 loss: 0.771919  [   32/  130]
train() client id: f_00008-0-1 loss: 0.691516  [   64/  130]
train() client id: f_00008-0-2 loss: 0.764859  [   96/  130]
train() client id: f_00008-0-3 loss: 0.680159  [  128/  130]
train() client id: f_00008-1-0 loss: 0.733565  [   32/  130]
train() client id: f_00008-1-1 loss: 0.707159  [   64/  130]
train() client id: f_00008-1-2 loss: 0.720490  [   96/  130]
train() client id: f_00008-1-3 loss: 0.739426  [  128/  130]
train() client id: f_00008-2-0 loss: 0.692555  [   32/  130]
train() client id: f_00008-2-1 loss: 0.690369  [   64/  130]
train() client id: f_00008-2-2 loss: 0.785398  [   96/  130]
train() client id: f_00008-2-3 loss: 0.705788  [  128/  130]
train() client id: f_00008-3-0 loss: 0.739246  [   32/  130]
train() client id: f_00008-3-1 loss: 0.684385  [   64/  130]
train() client id: f_00008-3-2 loss: 0.787488  [   96/  130]
train() client id: f_00008-3-3 loss: 0.696211  [  128/  130]
train() client id: f_00009-0-0 loss: 1.078414  [   32/  118]
train() client id: f_00009-0-1 loss: 0.919942  [   64/  118]
train() client id: f_00009-0-2 loss: 0.695412  [   96/  118]
train() client id: f_00009-1-0 loss: 0.971345  [   32/  118]
train() client id: f_00009-1-1 loss: 0.845276  [   64/  118]
train() client id: f_00009-1-2 loss: 0.779846  [   96/  118]
train() client id: f_00009-2-0 loss: 0.830881  [   32/  118]
train() client id: f_00009-2-1 loss: 0.875177  [   64/  118]
train() client id: f_00009-2-2 loss: 0.905971  [   96/  118]
train() client id: f_00009-3-0 loss: 0.850507  [   32/  118]
train() client id: f_00009-3-1 loss: 0.873242  [   64/  118]
train() client id: f_00009-3-2 loss: 0.787572  [   96/  118]
At round 75 accuracy: 0.6445623342175066
At round 75 training accuracy: 0.5881958417169685
At round 75 training loss: 0.8206930630486197
update_location
xs = -3.905658 4.200318 395.009024 18.811294 0.979296 3.956410 -357.443192 -336.324852 379.663977 -322.060879 
ys = 387.587959 370.555839 1.320614 -357.455176 349.350187 332.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 400.299489 383.834954 407.472543 371.655846 363.382047 347.535483 371.177216 350.877589 393.005605 337.252459 
dists_bs = 272.886266 265.244868 595.881546 566.631566 247.723032 238.601557 254.629443 237.410114 576.573191 225.538013 
uav_gains = -121.414410 -120.775226 -121.671511 -120.249570 -119.861663 -119.036008 -120.227852 -119.220043 -121.140310 -118.433021 
bs_gains = -107.774546 -107.429175 -117.271537 -116.659481 -106.598118 -106.141911 -106.932501 -106.081037 -116.870984 -105.457212 
Round 76
-------------------------------
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.89822293 1.75469717 0.88790864 0.34389424 2.02220671 0.97420485
 0.41573163 1.22653712 0.89277918 0.79022156]
obj_prev = 10.206404031521155
eta_min = 1.1840901563596855e-105	eta_max = 0.9712061927430241
af = 2.098524592010151	bf = 0.440277105517973	zeta = 2.308377051211166	eta = 0.9090909090909091
af = 2.098524592010151	bf = 0.440277105517973	zeta = 7.043166478768837	eta = 0.29795186559000353
af = 2.098524592010151	bf = 0.440277105517973	zeta = 4.246460455912228	eta = 0.49418206381468455
af = 2.098524592010151	bf = 0.440277105517973	zeta = 3.794695110359663	eta = 0.5530153361415251
af = 2.098524592010151	bf = 0.440277105517973	zeta = 3.766078122653466	eta = 0.557217488237231
af = 2.098524592010151	bf = 0.440277105517973	zeta = 3.765942672970741	eta = 0.5572375296819754
eta = 0.5572375296819754
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [0.0482533  0.10148511 0.04748737 0.0164674  0.11718658 0.05591255
 0.02067998 0.06855032 0.04978515 0.0451896 ]
ene_total = [0.39809468 0.53669908 0.4005926  0.21938841 0.6108545  0.31238457
 0.23943255 0.45484532 0.3348021  0.25884885]
ti_comp = [5.06004778 5.30681242 5.04724229 5.11036031 5.31105174 5.31322071
 5.11119117 5.14610005 5.20576198 5.31628572]
ti_coms = [0.33762368 0.09085904 0.35042917 0.28731115 0.08661972 0.08445075
 0.2864803  0.25157141 0.19190948 0.08138574]
t_total = [26.19968109 26.19968109 26.19968109 26.19968109 26.19968109 26.19968109
 26.19968109 26.19968109 26.19968109 26.19968109]
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [2.74253247e-07 2.31963433e-06 2.62727860e-07 1.06868990e-08
 3.56576398e-06 3.86983308e-07 2.11585747e-08 7.60242285e-07
 2.84584225e-07 2.04069817e-07]
ene_total = [0.16388001 0.0441132  0.1700956  0.13945761 0.04206153 0.04099331
 0.13905437 0.12211358 0.09315205 0.0395047 ]
optimize_network iter = 0 obj = 0.9944259543834252
eta = 0.5572375296819754
freqs = [ 4768067.39444     9561776.65897993  4704288.96039803  1611177.51564226
 11032332.74977597  5261643.95832219  2023010.29856296  6660414.59689484
  4781734.99264716  4250109.82064523]
eta_min = 0.5572375296819757	eta_max = 0.8837926289249317
af = 2.9732682841315033e-05	bf = 0.440277105517973	zeta = 3.2705951125446536e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [4.47003242e-08 3.78075401e-07 4.28218103e-08 1.74184938e-09
 5.81181107e-07 6.30741094e-08 3.44861970e-09 1.23911301e-07
 4.63841622e-08 3.32611812e-08]
ene_total = [0.79815367 0.21480247 0.82842621 0.67921232 0.20478538 0.19964562
 0.67724819 0.59472538 0.45368094 0.19239915]
ti_comp = [1.07904836 1.325813   1.06624286 1.12936088 1.33005232 1.33222129
 1.13019174 1.16510062 1.22476255 1.33528629]
ti_coms = [0.33762368 0.09085904 0.35042917 0.28731115 0.08661972 0.08445075
 0.2864803  0.25157141 0.19190948 0.08138574]
t_total = [26.19968109 26.19968109 26.19968109 26.19968109 26.19968109 26.19968109
 26.19968109 26.19968109 26.19968109 26.19968109]
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [5.68571582e-08 3.50371208e-07 5.55019335e-08 2.06298094e-09
 5.36021441e-07 5.80311479e-08 4.07973874e-09 1.39825878e-07
 4.84709564e-08 3.04966807e-08]
ene_total = [0.62439628 0.16803958 0.64807852 0.53134818 0.16020288 0.15618279
 0.52981165 0.46525429 0.35491489 0.15051391]
optimize_network iter = 1 obj = 3.7887429688072136
eta = 0.8837926289249317
freqs = [4723436.47190019 8085223.78253195 4704288.96039802 1540154.03017325
 9306387.77331266 4433073.47420022 1932725.03801711 6214669.51457857
 4293583.32001691 3574670.85257318]
eta_min = 0.8837926289249345	eta_max = 0.8837926289249313
af = 2.229825334963228e-05	bf = 0.440277105517973	zeta = 2.452807868459551e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [4.38674166e-08 2.70324445e-07 4.28218103e-08 1.59166668e-09
 4.13560520e-07 4.47731935e-08 3.14767049e-09 1.07880876e-07
 3.73971494e-08 2.35293258e-08]
ene_total = [0.79815365 0.21479992 0.82842621 0.67921232 0.20478142 0.19964519
 0.67724818 0.594725   0.45368072 0.19239892]
ti_comp = [1.07904836 1.325813   1.06624286 1.12936088 1.33005232 1.33222129
 1.13019174 1.16510062 1.22476255 1.33528629]
ti_coms = [0.33762368 0.09085904 0.35042917 0.28731115 0.08661972 0.08445075
 0.2864803  0.25157141 0.19190948 0.08138574]
t_total = [26.19968109 26.19968109 26.19968109 26.19968109 26.19968109 26.19968109
 26.19968109 26.19968109 26.19968109 26.19968109]
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [5.68571582e-08 3.50371208e-07 5.55019335e-08 2.06298094e-09
 5.36021441e-07 5.80311479e-08 4.07973874e-09 1.39825878e-07
 4.84709564e-08 3.04966807e-08]
ene_total = [0.62439628 0.16803958 0.64807852 0.53134818 0.16020288 0.15618279
 0.52981165 0.46525429 0.35491489 0.15051391]
optimize_network iter = 2 obj = 3.788742968807199
eta = 0.8837926289249313
freqs = [4723436.47190018 8085223.78253194 4704288.96039801 1540154.03017325
 9306387.77331265 4433073.47420021 1932725.03801711 6214669.51457855
 4293583.3200169  3574670.85257317]
Done!
ene_coms = [0.03376237 0.0090859  0.03504292 0.02873112 0.00866197 0.00844507
 0.02864803 0.02515714 0.01919095 0.00813857]
ene_comp = [5.62233473e-08 3.46465471e-07 5.48832299e-08 2.03998402e-09
 5.30046183e-07 5.73842500e-08 4.03426016e-09 1.38267179e-07
 4.79306301e-08 3.01567212e-08]
ene_total = [0.03376242 0.00908625 0.03504297 0.02873112 0.0086625  0.00844513
 0.02864803 0.02515728 0.019191   0.0081386 ]
At round 76 energy consumption: 0.20486531204581118
At round 76 eta: 0.8837926289249313
At round 76 a_n: 2.1491184865862607
At round 76 local rounds: 4.045092362532633
At round 76 global rounds: 18.49382243745926
gradient difference: 0.7914801239967346
train() client id: f_00000-0-0 loss: 1.118358  [   32/  126]
train() client id: f_00000-0-1 loss: 1.341701  [   64/  126]
train() client id: f_00000-0-2 loss: 1.131823  [   96/  126]
train() client id: f_00000-1-0 loss: 1.172459  [   32/  126]
train() client id: f_00000-1-1 loss: 1.133660  [   64/  126]
train() client id: f_00000-1-2 loss: 1.084802  [   96/  126]
train() client id: f_00000-2-0 loss: 1.073586  [   32/  126]
train() client id: f_00000-2-1 loss: 1.274722  [   64/  126]
train() client id: f_00000-2-2 loss: 1.077114  [   96/  126]
train() client id: f_00000-3-0 loss: 1.026902  [   32/  126]
train() client id: f_00000-3-1 loss: 1.003300  [   64/  126]
train() client id: f_00000-3-2 loss: 0.979411  [   96/  126]
train() client id: f_00001-0-0 loss: 0.524477  [   32/  265]
train() client id: f_00001-0-1 loss: 0.583869  [   64/  265]
train() client id: f_00001-0-2 loss: 0.674114  [   96/  265]
train() client id: f_00001-0-3 loss: 0.602068  [  128/  265]
train() client id: f_00001-0-4 loss: 0.602820  [  160/  265]
train() client id: f_00001-0-5 loss: 0.582938  [  192/  265]
train() client id: f_00001-0-6 loss: 0.524875  [  224/  265]
train() client id: f_00001-0-7 loss: 0.688017  [  256/  265]
train() client id: f_00001-1-0 loss: 0.713961  [   32/  265]
train() client id: f_00001-1-1 loss: 0.611747  [   64/  265]
train() client id: f_00001-1-2 loss: 0.656649  [   96/  265]
train() client id: f_00001-1-3 loss: 0.602471  [  128/  265]
train() client id: f_00001-1-4 loss: 0.634265  [  160/  265]
train() client id: f_00001-1-5 loss: 0.502165  [  192/  265]
train() client id: f_00001-1-6 loss: 0.507178  [  224/  265]
train() client id: f_00001-1-7 loss: 0.507105  [  256/  265]
train() client id: f_00001-2-0 loss: 0.506465  [   32/  265]
train() client id: f_00001-2-1 loss: 0.540577  [   64/  265]
train() client id: f_00001-2-2 loss: 0.682419  [   96/  265]
train() client id: f_00001-2-3 loss: 0.775926  [  128/  265]
train() client id: f_00001-2-4 loss: 0.560903  [  160/  265]
train() client id: f_00001-2-5 loss: 0.563462  [  192/  265]
train() client id: f_00001-2-6 loss: 0.603562  [  224/  265]
train() client id: f_00001-2-7 loss: 0.587898  [  256/  265]
train() client id: f_00001-3-0 loss: 0.695985  [   32/  265]
train() client id: f_00001-3-1 loss: 0.558285  [   64/  265]
train() client id: f_00001-3-2 loss: 0.534603  [   96/  265]
train() client id: f_00001-3-3 loss: 0.515102  [  128/  265]
train() client id: f_00001-3-4 loss: 0.605714  [  160/  265]
train() client id: f_00001-3-5 loss: 0.531914  [  192/  265]
train() client id: f_00001-3-6 loss: 0.695808  [  224/  265]
train() client id: f_00001-3-7 loss: 0.610636  [  256/  265]
train() client id: f_00002-0-0 loss: 0.943705  [   32/  124]
train() client id: f_00002-0-1 loss: 1.031626  [   64/  124]
train() client id: f_00002-0-2 loss: 1.028901  [   96/  124]
train() client id: f_00002-1-0 loss: 0.945692  [   32/  124]
train() client id: f_00002-1-1 loss: 0.867103  [   64/  124]
train() client id: f_00002-1-2 loss: 1.150719  [   96/  124]
train() client id: f_00002-2-0 loss: 0.909094  [   32/  124]
train() client id: f_00002-2-1 loss: 0.873662  [   64/  124]
train() client id: f_00002-2-2 loss: 1.089768  [   96/  124]
train() client id: f_00002-3-0 loss: 0.931909  [   32/  124]
train() client id: f_00002-3-1 loss: 1.038757  [   64/  124]
train() client id: f_00002-3-2 loss: 1.094313  [   96/  124]
train() client id: f_00003-0-0 loss: 0.444927  [   32/   43]
train() client id: f_00003-1-0 loss: 0.574732  [   32/   43]
train() client id: f_00003-2-0 loss: 0.471501  [   32/   43]
train() client id: f_00003-3-0 loss: 0.584341  [   32/   43]
train() client id: f_00004-0-0 loss: 1.027136  [   32/  306]
train() client id: f_00004-0-1 loss: 0.825868  [   64/  306]
train() client id: f_00004-0-2 loss: 0.906242  [   96/  306]
train() client id: f_00004-0-3 loss: 1.002340  [  128/  306]
train() client id: f_00004-0-4 loss: 0.799461  [  160/  306]
train() client id: f_00004-0-5 loss: 0.855695  [  192/  306]
train() client id: f_00004-0-6 loss: 0.797141  [  224/  306]
train() client id: f_00004-0-7 loss: 1.062189  [  256/  306]
train() client id: f_00004-0-8 loss: 0.776116  [  288/  306]
train() client id: f_00004-1-0 loss: 0.821840  [   32/  306]
train() client id: f_00004-1-1 loss: 0.782958  [   64/  306]
train() client id: f_00004-1-2 loss: 0.941794  [   96/  306]
train() client id: f_00004-1-3 loss: 0.918838  [  128/  306]
train() client id: f_00004-1-4 loss: 0.871157  [  160/  306]
train() client id: f_00004-1-5 loss: 0.994261  [  192/  306]
train() client id: f_00004-1-6 loss: 0.823186  [  224/  306]
train() client id: f_00004-1-7 loss: 0.860600  [  256/  306]
train() client id: f_00004-1-8 loss: 0.910292  [  288/  306]
train() client id: f_00004-2-0 loss: 0.747489  [   32/  306]
train() client id: f_00004-2-1 loss: 0.806678  [   64/  306]
train() client id: f_00004-2-2 loss: 0.961537  [   96/  306]
train() client id: f_00004-2-3 loss: 1.102627  [  128/  306]
train() client id: f_00004-2-4 loss: 0.921668  [  160/  306]
train() client id: f_00004-2-5 loss: 0.853149  [  192/  306]
train() client id: f_00004-2-6 loss: 0.784852  [  224/  306]
train() client id: f_00004-2-7 loss: 0.830772  [  256/  306]
train() client id: f_00004-2-8 loss: 0.982404  [  288/  306]
train() client id: f_00004-3-0 loss: 0.939140  [   32/  306]
train() client id: f_00004-3-1 loss: 0.994289  [   64/  306]
train() client id: f_00004-3-2 loss: 0.747630  [   96/  306]
train() client id: f_00004-3-3 loss: 1.002764  [  128/  306]
train() client id: f_00004-3-4 loss: 0.887600  [  160/  306]
train() client id: f_00004-3-5 loss: 0.804001  [  192/  306]
train() client id: f_00004-3-6 loss: 0.771586  [  224/  306]
train() client id: f_00004-3-7 loss: 0.894380  [  256/  306]
train() client id: f_00004-3-8 loss: 0.946612  [  288/  306]
train() client id: f_00005-0-0 loss: 0.831387  [   32/  146]
train() client id: f_00005-0-1 loss: 0.596645  [   64/  146]
train() client id: f_00005-0-2 loss: 0.390432  [   96/  146]
train() client id: f_00005-0-3 loss: 0.697258  [  128/  146]
train() client id: f_00005-1-0 loss: 0.896126  [   32/  146]
train() client id: f_00005-1-1 loss: 0.544748  [   64/  146]
train() client id: f_00005-1-2 loss: 0.521730  [   96/  146]
train() client id: f_00005-1-3 loss: 0.593596  [  128/  146]
train() client id: f_00005-2-0 loss: 0.578068  [   32/  146]
train() client id: f_00005-2-1 loss: 0.683729  [   64/  146]
train() client id: f_00005-2-2 loss: 0.361746  [   96/  146]
train() client id: f_00005-2-3 loss: 0.844433  [  128/  146]
train() client id: f_00005-3-0 loss: 0.844856  [   32/  146]
train() client id: f_00005-3-1 loss: 0.667451  [   64/  146]
train() client id: f_00005-3-2 loss: 0.414699  [   96/  146]
train() client id: f_00005-3-3 loss: 0.567522  [  128/  146]
train() client id: f_00006-0-0 loss: 0.559717  [   32/   54]
train() client id: f_00006-1-0 loss: 0.553106  [   32/   54]
train() client id: f_00006-2-0 loss: 0.442889  [   32/   54]
train() client id: f_00006-3-0 loss: 0.581937  [   32/   54]
train() client id: f_00007-0-0 loss: 0.338585  [   32/  179]
train() client id: f_00007-0-1 loss: 0.459101  [   64/  179]
train() client id: f_00007-0-2 loss: 0.401291  [   96/  179]
train() client id: f_00007-0-3 loss: 0.319786  [  128/  179]
train() client id: f_00007-0-4 loss: 0.710404  [  160/  179]
train() client id: f_00007-1-0 loss: 0.598572  [   32/  179]
train() client id: f_00007-1-1 loss: 0.508449  [   64/  179]
train() client id: f_00007-1-2 loss: 0.375298  [   96/  179]
train() client id: f_00007-1-3 loss: 0.275672  [  128/  179]
train() client id: f_00007-1-4 loss: 0.438502  [  160/  179]
train() client id: f_00007-2-0 loss: 0.336280  [   32/  179]
train() client id: f_00007-2-1 loss: 0.568632  [   64/  179]
train() client id: f_00007-2-2 loss: 0.256077  [   96/  179]
train() client id: f_00007-2-3 loss: 0.480591  [  128/  179]
train() client id: f_00007-2-4 loss: 0.595593  [  160/  179]
train() client id: f_00007-3-0 loss: 0.510989  [   32/  179]
train() client id: f_00007-3-1 loss: 0.316969  [   64/  179]
train() client id: f_00007-3-2 loss: 0.630626  [   96/  179]
train() client id: f_00007-3-3 loss: 0.372777  [  128/  179]
train() client id: f_00007-3-4 loss: 0.384128  [  160/  179]
train() client id: f_00008-0-0 loss: 0.817299  [   32/  130]
train() client id: f_00008-0-1 loss: 0.728804  [   64/  130]
train() client id: f_00008-0-2 loss: 0.800074  [   96/  130]
train() client id: f_00008-0-3 loss: 0.713135  [  128/  130]
train() client id: f_00008-1-0 loss: 0.797004  [   32/  130]
train() client id: f_00008-1-1 loss: 0.867079  [   64/  130]
train() client id: f_00008-1-2 loss: 0.657372  [   96/  130]
train() client id: f_00008-1-3 loss: 0.745789  [  128/  130]
train() client id: f_00008-2-0 loss: 0.689155  [   32/  130]
train() client id: f_00008-2-1 loss: 0.767509  [   64/  130]
train() client id: f_00008-2-2 loss: 0.719025  [   96/  130]
train() client id: f_00008-2-3 loss: 0.889787  [  128/  130]
train() client id: f_00008-3-0 loss: 0.748555  [   32/  130]
train() client id: f_00008-3-1 loss: 0.842627  [   64/  130]
train() client id: f_00008-3-2 loss: 0.771584  [   96/  130]
train() client id: f_00008-3-3 loss: 0.681272  [  128/  130]
train() client id: f_00009-0-0 loss: 0.849932  [   32/  118]
train() client id: f_00009-0-1 loss: 0.919856  [   64/  118]
train() client id: f_00009-0-2 loss: 0.885116  [   96/  118]
train() client id: f_00009-1-0 loss: 0.779417  [   32/  118]
train() client id: f_00009-1-1 loss: 1.012877  [   64/  118]
train() client id: f_00009-1-2 loss: 0.788623  [   96/  118]
train() client id: f_00009-2-0 loss: 0.894205  [   32/  118]
train() client id: f_00009-2-1 loss: 0.840657  [   64/  118]
train() client id: f_00009-2-2 loss: 0.913647  [   96/  118]
train() client id: f_00009-3-0 loss: 0.865057  [   32/  118]
train() client id: f_00009-3-1 loss: 0.765267  [   64/  118]
train() client id: f_00009-3-2 loss: 0.927832  [   96/  118]
At round 76 accuracy: 0.6445623342175066
At round 76 training accuracy: 0.5895372233400402
At round 76 training loss: 0.8290828353560679
update_location
xs = -3.905658 4.200318 400.009024 18.811294 0.979296 3.956410 -362.443192 -341.324852 384.663977 -327.060879 
ys = 392.587959 375.555839 1.320614 -362.455176 354.350187 337.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 405.142642 388.664162 412.321432 376.467288 368.191545 352.326629 375.994625 355.673067 397.837963 342.030452 
dists_bs = 276.799194 268.952409 600.666219 571.332551 251.267193 241.937687 258.235523 240.835651 581.384799 228.829640 
uav_gains = -121.589275 -120.970489 -121.838960 -120.463275 -120.090421 -119.298114 -120.442644 -119.474564 -121.323406 -118.720312 
bs_gains = -107.947674 -107.597971 -117.368789 -116.759950 -106.770861 -106.310758 -107.103507 -106.255241 -116.972042 -105.633403 
Round 77
-------------------------------
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.7566319  1.47518382 0.74797579 0.2906086  1.70004424 0.81907155
 0.35099723 1.03253353 0.75085927 0.66440907]
obj_prev = 8.588314982517188
eta_min = nan	eta_max = nan
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 1.9404471408469977	eta = 0.9090909090909091
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 6.015300613417023	eta = 0.2932593013524188
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 3.5977300389560636	eta = 0.49032107362544536
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 3.2097433156161213	eta = 0.5495900082517465
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 3.1851865241926465	eta = 0.5538271752429276
af = 1.7640428553154524	bf = 0.3787142685879881	zeta = 3.1850702185308974	eta = 0.5538473987330524
eta = 0.5538473987330524
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [0.04875685 0.10254417 0.04798293 0.01663924 0.1184095  0.05649604
 0.02089579 0.06926569 0.05030469 0.04566118]
ene_total = [0.33763567 0.45238391 0.33971862 0.18710392 0.51488509 0.26328129
 0.20400615 0.38565213 0.28224513 0.2181583 ]
ti_comp = [6.11168561 6.36617567 6.09880235 6.16226391 6.37047552 6.37270362
 6.16308753 6.19819213 6.2639569  6.37579113]
ti_coms = [0.34625918 0.09176912 0.35914244 0.29568088 0.08746927 0.08524118
 0.29485726 0.25975266 0.19398789 0.08215366]
t_total = [26.1496769 26.1496769 26.1496769 26.1496769 26.1496769 26.1496769
 26.1496769 26.1496769 26.1496769 26.1496769]
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [1.93938935e-07 1.66286030e-06 1.85631348e-07 7.58228597e-09
 2.55679530e-06 2.77515029e-07 1.50127409e-08 5.40635308e-07
 2.02771968e-07 1.46370394e-07]
ene_total = [0.14020895 0.03716612 0.14542564 0.1197279  0.03542864 0.0345172
 0.11939442 0.10518191 0.0785509  0.03326647]
optimize_network iter = 0 obj = 0.848868136074443
eta = 0.5538473987330524
freqs = [3988822.08036756 8053828.5629115  3933799.78027688 1350091.76188237
 9293615.50887199 4432658.36750978 1695237.49554636 5587571.86903341
 4015408.34280281 3580824.62742287]
eta_min = 0.5538473987330529	eta_max = 0.9002731777032497
af = 1.770021082250513e-05	bf = 0.3787142685879881	zeta = 1.9470231904755644e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [3.12834926e-08 2.68229162e-07 2.99434299e-08 1.22306739e-09
 4.12426142e-07 4.47648089e-08 2.42164354e-09 8.72076598e-08
 3.27083129e-08 2.36104067e-08]
ene_total = [0.68809723 0.18237166 0.71369919 0.58758591 0.17382975 0.1693947
 0.58594921 0.51618999 0.38549919 0.16325868]
ti_comp = [1.09726106 1.35175113 1.08437781 1.14783937 1.35605097 1.35827907
 1.14866299 1.18376759 1.24953236 1.36136658]
ti_coms = [0.34625918 0.09176912 0.35914244 0.29568088 0.08746927 0.08524118
 0.29485726 0.25975266 0.19398789 0.08215366]
t_total = [26.1496769 26.1496769 26.1496769 26.1496769 26.1496769 26.1496769
 26.1496769 26.1496769 26.1496769 26.1496769]
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [3.38201293e-08 2.07313558e-07 3.30055808e-08 1.22836293e-09
 3.17172712e-07 3.43371396e-08 2.42929128e-09 8.33124237e-08
 2.86430070e-08 1.80459360e-08]
ene_total = [0.6272565  0.16624547 0.65059478 0.53563224 0.1584582  0.15441683
 0.53414025 0.47054899 0.35141372 0.14882344]
optimize_network iter = 1 obj = 3.7975304096863765
eta = 0.9002731777032497
freqs = [3950315.32823697 6744042.83153091 3933799.78027688 1288719.87693548
 7762767.85413468 3697728.61147668 1617231.97677077 5201849.38696053
 3579043.95899813 2981797.2351148 ]
eta_min = 0.8902239180004755	eta_max = 0.9002731777032479
af = 1.3057149041253039e-05	bf = 0.3787142685879881	zeta = 1.4362863945378344e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [3.06824073e-08 1.88079677e-07 2.99434299e-08 1.11439939e-09
 2.87746454e-07 3.11514510e-08 2.20390950e-09 7.55829668e-08
 2.59856015e-08 1.63716925e-08]
ene_total = [0.68809721 0.18237007 0.71369919 0.58758591 0.17382727 0.16939443
 0.58594921 0.51618975 0.38549905 0.16325854]
ti_comp = [1.09726106 1.35175113 1.08437781 1.14783937 1.35605097 1.35827907
 1.14866299 1.18376759 1.24953236 1.36136658]
ti_coms = [0.34625918 0.09176912 0.35914244 0.29568088 0.08746927 0.08524118
 0.29485726 0.25975266 0.19398789 0.08215366]
t_total = [26.1496769 26.1496769 26.1496769 26.1496769 26.1496769 26.1496769
 26.1496769 26.1496769 26.1496769 26.1496769]
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [3.38201293e-08 2.07313558e-07 3.30055808e-08 1.22836293e-09
 3.17172712e-07 3.43371396e-08 2.42929128e-09 8.33124237e-08
 2.86430070e-08 1.80459360e-08]
ene_total = [0.6272565  0.16624547 0.65059478 0.53563224 0.1584582  0.15441683
 0.53414025 0.47054899 0.35141372 0.14882344]
optimize_network iter = 2 obj = 3.7975304096863085
eta = 0.9002731777032479
freqs = [3950315.32823696 6744042.83153091 3933799.78027686 1288719.87693548
 7762767.85413468 3697728.61147668 1617231.97677076 5201849.38696052
 3579043.95899813 2981797.2351148 ]
Done!
ene_coms = [0.03462592 0.00917691 0.03591424 0.02956809 0.00874693 0.00852412
 0.02948573 0.02597527 0.01939879 0.00821537]
ene_comp = [2.94934334e-08 1.80791402e-07 2.87830921e-08 1.07121530e-09
 2.76595992e-07 2.99443012e-08 2.11850581e-09 7.26540515e-08
 2.49786335e-08 1.57372731e-08]
ene_total = [0.03462595 0.00917709 0.03591427 0.02956809 0.0087472  0.00852415
 0.02948573 0.02597534 0.01939881 0.00821538]
At round 77 energy consumption: 0.20963201619321536
At round 77 eta: 0.9002731777032479
At round 77 a_n: 1.8065726396169453
At round 77 local rounds: 3.4401009464594003
At round 77 global rounds: 18.115213119307235
gradient difference: 0.9729766845703125
train() client id: f_00000-0-0 loss: 0.882029  [   32/  126]
train() client id: f_00000-0-1 loss: 0.977346  [   64/  126]
train() client id: f_00000-0-2 loss: 1.046768  [   96/  126]
train() client id: f_00000-1-0 loss: 0.777899  [   32/  126]
train() client id: f_00000-1-1 loss: 1.054366  [   64/  126]
train() client id: f_00000-1-2 loss: 1.125256  [   96/  126]
train() client id: f_00000-2-0 loss: 0.800411  [   32/  126]
train() client id: f_00000-2-1 loss: 1.075471  [   64/  126]
train() client id: f_00000-2-2 loss: 0.896427  [   96/  126]
train() client id: f_00001-0-0 loss: 0.460516  [   32/  265]
train() client id: f_00001-0-1 loss: 0.479394  [   64/  265]
train() client id: f_00001-0-2 loss: 0.483353  [   96/  265]
train() client id: f_00001-0-3 loss: 0.447471  [  128/  265]
train() client id: f_00001-0-4 loss: 0.563762  [  160/  265]
train() client id: f_00001-0-5 loss: 0.589252  [  192/  265]
train() client id: f_00001-0-6 loss: 0.737890  [  224/  265]
train() client id: f_00001-0-7 loss: 0.597054  [  256/  265]
train() client id: f_00001-1-0 loss: 0.476704  [   32/  265]
train() client id: f_00001-1-1 loss: 0.535110  [   64/  265]
train() client id: f_00001-1-2 loss: 0.659850  [   96/  265]
train() client id: f_00001-1-3 loss: 0.577826  [  128/  265]
train() client id: f_00001-1-4 loss: 0.594675  [  160/  265]
train() client id: f_00001-1-5 loss: 0.460715  [  192/  265]
train() client id: f_00001-1-6 loss: 0.438588  [  224/  265]
train() client id: f_00001-1-7 loss: 0.588539  [  256/  265]
train() client id: f_00001-2-0 loss: 0.495887  [   32/  265]
train() client id: f_00001-2-1 loss: 0.623725  [   64/  265]
train() client id: f_00001-2-2 loss: 0.429658  [   96/  265]
train() client id: f_00001-2-3 loss: 0.518454  [  128/  265]
train() client id: f_00001-2-4 loss: 0.548474  [  160/  265]
train() client id: f_00001-2-5 loss: 0.529885  [  192/  265]
train() client id: f_00001-2-6 loss: 0.591334  [  224/  265]
train() client id: f_00001-2-7 loss: 0.525735  [  256/  265]
train() client id: f_00002-0-0 loss: 1.006438  [   32/  124]
train() client id: f_00002-0-1 loss: 0.682258  [   64/  124]
train() client id: f_00002-0-2 loss: 0.871274  [   96/  124]
train() client id: f_00002-1-0 loss: 0.894787  [   32/  124]
train() client id: f_00002-1-1 loss: 0.651895  [   64/  124]
train() client id: f_00002-1-2 loss: 0.892774  [   96/  124]
train() client id: f_00002-2-0 loss: 0.723689  [   32/  124]
train() client id: f_00002-2-1 loss: 0.652793  [   64/  124]
train() client id: f_00002-2-2 loss: 0.862410  [   96/  124]
train() client id: f_00003-0-0 loss: 0.588419  [   32/   43]
train() client id: f_00003-1-0 loss: 0.639318  [   32/   43]
train() client id: f_00003-2-0 loss: 0.454333  [   32/   43]
train() client id: f_00004-0-0 loss: 0.904976  [   32/  306]
train() client id: f_00004-0-1 loss: 0.648784  [   64/  306]
train() client id: f_00004-0-2 loss: 0.771984  [   96/  306]
train() client id: f_00004-0-3 loss: 0.885032  [  128/  306]
train() client id: f_00004-0-4 loss: 0.805096  [  160/  306]
train() client id: f_00004-0-5 loss: 0.743884  [  192/  306]
train() client id: f_00004-0-6 loss: 0.718771  [  224/  306]
train() client id: f_00004-0-7 loss: 0.861244  [  256/  306]
train() client id: f_00004-0-8 loss: 0.866308  [  288/  306]
train() client id: f_00004-1-0 loss: 0.635985  [   32/  306]
train() client id: f_00004-1-1 loss: 0.838700  [   64/  306]
train() client id: f_00004-1-2 loss: 0.902116  [   96/  306]
train() client id: f_00004-1-3 loss: 0.884192  [  128/  306]
train() client id: f_00004-1-4 loss: 0.816655  [  160/  306]
train() client id: f_00004-1-5 loss: 0.831373  [  192/  306]
train() client id: f_00004-1-6 loss: 0.810009  [  224/  306]
train() client id: f_00004-1-7 loss: 0.742841  [  256/  306]
train() client id: f_00004-1-8 loss: 0.732195  [  288/  306]
train() client id: f_00004-2-0 loss: 0.934555  [   32/  306]
train() client id: f_00004-2-1 loss: 0.803853  [   64/  306]
train() client id: f_00004-2-2 loss: 0.844103  [   96/  306]
train() client id: f_00004-2-3 loss: 0.635814  [  128/  306]
train() client id: f_00004-2-4 loss: 0.816942  [  160/  306]
train() client id: f_00004-2-5 loss: 0.767106  [  192/  306]
train() client id: f_00004-2-6 loss: 0.618370  [  224/  306]
train() client id: f_00004-2-7 loss: 0.739351  [  256/  306]
train() client id: f_00004-2-8 loss: 0.965499  [  288/  306]
train() client id: f_00005-0-0 loss: 0.594522  [   32/  146]
train() client id: f_00005-0-1 loss: 0.713171  [   64/  146]
train() client id: f_00005-0-2 loss: 0.775427  [   96/  146]
train() client id: f_00005-0-3 loss: 0.474992  [  128/  146]
train() client id: f_00005-1-0 loss: 0.597457  [   32/  146]
train() client id: f_00005-1-1 loss: 0.410584  [   64/  146]
train() client id: f_00005-1-2 loss: 0.752385  [   96/  146]
train() client id: f_00005-1-3 loss: 0.808186  [  128/  146]
train() client id: f_00005-2-0 loss: 0.483015  [   32/  146]
train() client id: f_00005-2-1 loss: 0.539005  [   64/  146]
train() client id: f_00005-2-2 loss: 0.621605  [   96/  146]
train() client id: f_00005-2-3 loss: 1.027899  [  128/  146]
train() client id: f_00006-0-0 loss: 0.453865  [   32/   54]
train() client id: f_00006-1-0 loss: 0.471277  [   32/   54]
train() client id: f_00006-2-0 loss: 0.434130  [   32/   54]
train() client id: f_00007-0-0 loss: 0.715041  [   32/  179]
train() client id: f_00007-0-1 loss: 0.708076  [   64/  179]
train() client id: f_00007-0-2 loss: 0.634883  [   96/  179]
train() client id: f_00007-0-3 loss: 0.728687  [  128/  179]
train() client id: f_00007-0-4 loss: 0.888584  [  160/  179]
train() client id: f_00007-1-0 loss: 0.773988  [   32/  179]
train() client id: f_00007-1-1 loss: 0.724217  [   64/  179]
train() client id: f_00007-1-2 loss: 0.611804  [   96/  179]
train() client id: f_00007-1-3 loss: 0.774392  [  128/  179]
train() client id: f_00007-1-4 loss: 0.745083  [  160/  179]
train() client id: f_00007-2-0 loss: 0.818195  [   32/  179]
train() client id: f_00007-2-1 loss: 0.662800  [   64/  179]
train() client id: f_00007-2-2 loss: 0.830744  [   96/  179]
train() client id: f_00007-2-3 loss: 0.666386  [  128/  179]
train() client id: f_00007-2-4 loss: 0.726796  [  160/  179]
train() client id: f_00008-0-0 loss: 0.601199  [   32/  130]
train() client id: f_00008-0-1 loss: 0.825334  [   64/  130]
train() client id: f_00008-0-2 loss: 0.826946  [   96/  130]
train() client id: f_00008-0-3 loss: 0.842000  [  128/  130]
train() client id: f_00008-1-0 loss: 0.753654  [   32/  130]
train() client id: f_00008-1-1 loss: 0.759662  [   64/  130]
train() client id: f_00008-1-2 loss: 0.716989  [   96/  130]
train() client id: f_00008-1-3 loss: 0.875977  [  128/  130]
train() client id: f_00008-2-0 loss: 0.706276  [   32/  130]
train() client id: f_00008-2-1 loss: 0.828229  [   64/  130]
train() client id: f_00008-2-2 loss: 0.733046  [   96/  130]
train() client id: f_00008-2-3 loss: 0.844467  [  128/  130]
train() client id: f_00009-0-0 loss: 0.505970  [   32/  118]
train() client id: f_00009-0-1 loss: 0.885396  [   64/  118]
train() client id: f_00009-0-2 loss: 0.764144  [   96/  118]
train() client id: f_00009-1-0 loss: 0.691529  [   32/  118]
train() client id: f_00009-1-1 loss: 0.781326  [   64/  118]
train() client id: f_00009-1-2 loss: 0.723469  [   96/  118]
train() client id: f_00009-2-0 loss: 0.612304  [   32/  118]
train() client id: f_00009-2-1 loss: 0.678873  [   64/  118]
train() client id: f_00009-2-2 loss: 0.870769  [   96/  118]
At round 77 accuracy: 0.6445623342175066
At round 77 training accuracy: 0.5902079141515761
At round 77 training loss: 0.8263875847362964
update_location
xs = -3.905658 4.200318 405.009024 18.811294 0.979296 3.956410 -367.443192 -346.324852 389.663977 -332.060879 
ys = 397.587959 380.555839 1.320614 -367.455176 359.350187 342.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 409.989560 393.497635 417.173889 381.283584 373.006053 357.123501 380.816740 360.474103 402.674415 346.814704 
dists_bs = 280.746636 272.701223 605.454372 576.038571 254.860166 245.330361 261.887413 244.315491 586.199560 232.182284 
uav_gains = -121.759041 -121.159231 -122.001820 -120.669193 -120.310381 -119.549417 -120.649568 -119.718705 -121.500836 -118.995576 
bs_gains = -108.119867 -107.766298 -117.465339 -116.859703 -106.943514 -106.480095 -107.274269 -106.429687 -117.072333 -105.810273 
Round 78
-------------------------------
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.61445089 1.1956097  0.60744834 0.2367497  1.37782529 0.66388603
 0.28568902 0.83797174 0.60879864 0.53854598]
obj_prev = 6.96697533321086
eta_min = nan	eta_max = nan
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 1.572517230482826	eta = 0.909090909090909
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 4.952035710772682	eta = 0.2886815043580717
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 2.9382966582345253	eta = 0.48652715668257346
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 2.6172340285407683	eta = 0.5462106571408896
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 2.596934549245399	eta = 0.5504802264023727
af = 1.4295611186207509	bf = 0.3139333373586834	zeta = 2.5968383965379944	eta = 0.5505006089430082
eta = 0.5505006089430082
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [0.049257   0.10359608 0.04847515 0.01680993 0.11962415 0.05707558
 0.02111014 0.06997622 0.05082072 0.04612957]
ene_total = [0.27603738 0.36760475 0.27771398 0.15379303 0.4183885  0.21392216
 0.16753295 0.31519772 0.22938986 0.17725807]
ti_comp = [7.65842292 7.92067487 7.64545672 7.70927839 7.9250345  7.92732068
 7.71009494 7.74538131 7.81728173 7.93043026]
ti_coms = [0.35494613 0.09269418 0.36791233 0.30409066 0.08833455 0.08604837
 0.30327411 0.26798773 0.19608732 0.08293879]
t_total = [26.0996727 26.0996727 26.0996727 26.0996727 26.0996727 26.0996727
 26.0996727 26.0996727 26.0996727 26.0996727]
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.27351857e-07 1.10760673e-06 1.21795299e-07 4.99516971e-09
 1.70346924e-06 1.84917516e-07 9.89086254e-09 3.56981594e-07
 1.34242758e-07 9.75494579e-08]
ene_total = [0.11560694 0.03019425 0.11983004 0.09904284 0.02877625 0.02802669
 0.0987769  0.0872852  0.06386639 0.02701362]
optimize_network iter = 0 obj = 0.6984191265921221
eta = 0.5505006089430082
freqs = [3215871.18363491 6539599.20133967 3170192.94839821 1090240.14955988
 7547232.16704463 3599928.48687781 1368993.7919406  4517286.87955715
 3250536.39445521 2908390.39625094]
eta_min = 0.5505006089430086	eta_max = 0.917509374892285
af = 9.439588067529733e-06	bf = 0.3139333373586834	zeta = 1.0383546874282708e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [2.03340174e-08 1.76849361e-07 1.94468128e-08 7.97568798e-10
 2.71989541e-07 2.95254116e-08 1.57925432e-09 5.69985401e-08
 2.14342739e-08 1.55755277e-08]
ene_total = [0.57161603 0.14928029 0.59249717 0.48971657 0.14226095 0.1385753
 0.48840159 0.43157626 0.31578514 0.13356732]
ti_comp = [1.26571924 1.52797119 1.25275304 1.31657471 1.53233081 1.534617
 1.31739126 1.35267763 1.42457805 1.53772658]
ti_coms = [0.35494613 0.09269418 0.36791233 0.30409066 0.08833455 0.08604837
 0.30327411 0.26798773 0.19608732 0.08293879]
t_total = [26.0996727 26.0996727 26.0996727 26.0996727 26.0996727 26.0996727
 26.0996727 26.0996727 26.0996727 26.0996727]
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.89785459e-08 1.21152772e-07 1.84654624e-08 6.97172764e-10
 1.85474954e-07 2.00855912e-08 1.37904294e-09 4.76427188e-08
 1.64544595e-08 1.05612140e-08]
ene_total = [0.57161601 0.14927939 0.59249715 0.48971657 0.14225955 0.13857515
 0.48840158 0.43157611 0.31578506 0.13356724]
optimize_network iter = 1 obj = 3.4532738111991463
eta = 0.909090909090909
freqs = [3106837.34109527 5412726.23484139 3089168.32072196 1019314.72951421
 6232384.57566306 2969191.46464435 1279276.24484534 4129943.16102444
 2848016.20722417 2394904.72079761]
eta_min = 0.9009430532547689	eta_max = 0.9090909090909057
af = 6.772388892494084e-06	bf = 0.3139333373586834	zeta = 7.449627781743493e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.89785459e-08 1.21152772e-07 1.84654624e-08 6.97172764e-10
 1.85474954e-07 2.00855912e-08 1.37904294e-09 4.76427188e-08
 1.64544595e-08 1.05612140e-08]
ene_total = [0.57161601 0.14927939 0.59249715 0.48971657 0.14225955 0.13857515
 0.48840158 0.43157611 0.31578506 0.13356724]
ti_comp = [1.26571924 1.52797119 1.25275304 1.31657471 1.53233081 1.534617
 1.31739126 1.35267763 1.42457805 1.53772658]
ti_coms = [0.35494613 0.09269418 0.36791233 0.30409066 0.08833455 0.08604837
 0.30327411 0.26798773 0.19608732 0.08293879]
t_total = [26.0996727 26.0996727 26.0996727 26.0996727 26.0996727 26.0996727
 26.0996727 26.0996727 26.0996727 26.0996727]
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.89785459e-08 1.21152772e-07 1.84654624e-08 6.97172764e-10
 1.85474954e-07 2.00855912e-08 1.37904294e-09 4.76427188e-08
 1.64544595e-08 1.05612140e-08]
ene_total = [0.57161601 0.14927939 0.59249715 0.48971657 0.14225955 0.13857515
 0.48840158 0.43157611 0.31578506 0.13356724]
optimize_network iter = 2 obj = 3.4532738111990233
eta = 0.9090909090909057
freqs = [3106837.34109525 5412726.23484139 3089168.32072194 1019314.7295142
 6232384.57566307 2969191.46464435 1279276.24484533 4129943.16102442
 2848016.20722416 2394904.72079761]
Done!
ene_coms = [0.03549461 0.00926942 0.03679123 0.03040907 0.00883346 0.00860484
 0.03032741 0.02679877 0.01960873 0.00829388]
ene_comp = [1.82431083e-08 1.16457981e-07 1.77499073e-08 6.70156624e-10
 1.78287614e-07 1.93072545e-08 1.32560365e-09 4.57965159e-08
 1.58168328e-08 1.01519565e-08]
ene_total = [0.03549463 0.00926953 0.03679125 0.03040907 0.00883363 0.00860486
 0.03032741 0.02679882 0.01960875 0.00829389]
At round 78 energy consumption: 0.21443183922587233
At round 78 eta: 0.9090909090909057
At round 78 a_n: 1.4640267926476263
At round 78 local rounds: 3.120939520577876
At round 78 global rounds: 16.104294719123292
gradient difference: 1.0045371055603027
train() client id: f_00000-0-0 loss: 1.091957  [   32/  126]
train() client id: f_00000-0-1 loss: 0.875511  [   64/  126]
train() client id: f_00000-0-2 loss: 1.003819  [   96/  126]
train() client id: f_00000-1-0 loss: 0.943925  [   32/  126]
train() client id: f_00000-1-1 loss: 1.051479  [   64/  126]
train() client id: f_00000-1-2 loss: 0.985209  [   96/  126]
train() client id: f_00000-2-0 loss: 0.985794  [   32/  126]
train() client id: f_00000-2-1 loss: 1.017924  [   64/  126]
train() client id: f_00000-2-2 loss: 0.954039  [   96/  126]
train() client id: f_00001-0-0 loss: 0.691151  [   32/  265]
train() client id: f_00001-0-1 loss: 0.595514  [   64/  265]
train() client id: f_00001-0-2 loss: 0.460298  [   96/  265]
train() client id: f_00001-0-3 loss: 0.541266  [  128/  265]
train() client id: f_00001-0-4 loss: 0.510052  [  160/  265]
train() client id: f_00001-0-5 loss: 0.559303  [  192/  265]
train() client id: f_00001-0-6 loss: 0.482797  [  224/  265]
train() client id: f_00001-0-7 loss: 0.466229  [  256/  265]
train() client id: f_00001-1-0 loss: 0.532732  [   32/  265]
train() client id: f_00001-1-1 loss: 0.479371  [   64/  265]
train() client id: f_00001-1-2 loss: 0.486771  [   96/  265]
train() client id: f_00001-1-3 loss: 0.506830  [  128/  265]
train() client id: f_00001-1-4 loss: 0.533903  [  160/  265]
train() client id: f_00001-1-5 loss: 0.537401  [  192/  265]
train() client id: f_00001-1-6 loss: 0.529431  [  224/  265]
train() client id: f_00001-1-7 loss: 0.599836  [  256/  265]
train() client id: f_00001-2-0 loss: 0.574838  [   32/  265]
train() client id: f_00001-2-1 loss: 0.466769  [   64/  265]
train() client id: f_00001-2-2 loss: 0.429144  [   96/  265]
train() client id: f_00001-2-3 loss: 0.621520  [  128/  265]
train() client id: f_00001-2-4 loss: 0.617167  [  160/  265]
train() client id: f_00001-2-5 loss: 0.462496  [  192/  265]
train() client id: f_00001-2-6 loss: 0.494039  [  224/  265]
train() client id: f_00001-2-7 loss: 0.555914  [  256/  265]
train() client id: f_00002-0-0 loss: 0.837132  [   32/  124]
train() client id: f_00002-0-1 loss: 0.751516  [   64/  124]
train() client id: f_00002-0-2 loss: 0.851398  [   96/  124]
train() client id: f_00002-1-0 loss: 0.889595  [   32/  124]
train() client id: f_00002-1-1 loss: 0.657044  [   64/  124]
train() client id: f_00002-1-2 loss: 0.891669  [   96/  124]
train() client id: f_00002-2-0 loss: 0.744527  [   32/  124]
train() client id: f_00002-2-1 loss: 0.891196  [   64/  124]
train() client id: f_00002-2-2 loss: 0.755008  [   96/  124]
train() client id: f_00003-0-0 loss: 0.834869  [   32/   43]
train() client id: f_00003-1-0 loss: 0.784686  [   32/   43]
train() client id: f_00003-2-0 loss: 0.521130  [   32/   43]
train() client id: f_00004-0-0 loss: 0.573686  [   32/  306]
train() client id: f_00004-0-1 loss: 0.655355  [   64/  306]
train() client id: f_00004-0-2 loss: 0.761499  [   96/  306]
train() client id: f_00004-0-3 loss: 0.570676  [  128/  306]
train() client id: f_00004-0-4 loss: 0.672504  [  160/  306]
train() client id: f_00004-0-5 loss: 0.627554  [  192/  306]
train() client id: f_00004-0-6 loss: 0.739802  [  224/  306]
train() client id: f_00004-0-7 loss: 0.658435  [  256/  306]
train() client id: f_00004-0-8 loss: 0.680057  [  288/  306]
train() client id: f_00004-1-0 loss: 0.629613  [   32/  306]
train() client id: f_00004-1-1 loss: 0.591346  [   64/  306]
train() client id: f_00004-1-2 loss: 0.731153  [   96/  306]
train() client id: f_00004-1-3 loss: 0.487429  [  128/  306]
train() client id: f_00004-1-4 loss: 0.600503  [  160/  306]
train() client id: f_00004-1-5 loss: 0.636786  [  192/  306]
train() client id: f_00004-1-6 loss: 0.712760  [  224/  306]
train() client id: f_00004-1-7 loss: 0.817080  [  256/  306]
train() client id: f_00004-1-8 loss: 0.718084  [  288/  306]
train() client id: f_00004-2-0 loss: 0.535171  [   32/  306]
train() client id: f_00004-2-1 loss: 0.611275  [   64/  306]
train() client id: f_00004-2-2 loss: 0.614045  [   96/  306]
train() client id: f_00004-2-3 loss: 0.695725  [  128/  306]
train() client id: f_00004-2-4 loss: 0.694029  [  160/  306]
train() client id: f_00004-2-5 loss: 0.837143  [  192/  306]
train() client id: f_00004-2-6 loss: 0.722939  [  224/  306]
train() client id: f_00004-2-7 loss: 0.612745  [  256/  306]
train() client id: f_00004-2-8 loss: 0.571047  [  288/  306]
train() client id: f_00005-0-0 loss: 0.112881  [   32/  146]
train() client id: f_00005-0-1 loss: 0.465667  [   64/  146]
train() client id: f_00005-0-2 loss: 0.068205  [   96/  146]
train() client id: f_00005-0-3 loss: 0.535558  [  128/  146]
train() client id: f_00005-1-0 loss: 0.157871  [   32/  146]
train() client id: f_00005-1-1 loss: 0.244119  [   64/  146]
train() client id: f_00005-1-2 loss: 0.267262  [   96/  146]
train() client id: f_00005-1-3 loss: 0.344481  [  128/  146]
train() client id: f_00005-2-0 loss: 0.409479  [   32/  146]
train() client id: f_00005-2-1 loss: -0.011262  [   64/  146]
train() client id: f_00005-2-2 loss: 0.265016  [   96/  146]
train() client id: f_00005-2-3 loss: 0.252097  [  128/  146]
train() client id: f_00006-0-0 loss: 0.575466  [   32/   54]
train() client id: f_00006-1-0 loss: 0.514488  [   32/   54]
train() client id: f_00006-2-0 loss: 0.576347  [   32/   54]
train() client id: f_00007-0-0 loss: 0.867793  [   32/  179]
train() client id: f_00007-0-1 loss: 0.703447  [   64/  179]
train() client id: f_00007-0-2 loss: 0.606735  [   96/  179]
train() client id: f_00007-0-3 loss: 0.583703  [  128/  179]
train() client id: f_00007-0-4 loss: 0.684035  [  160/  179]
train() client id: f_00007-1-0 loss: 0.620500  [   32/  179]
train() client id: f_00007-1-1 loss: 0.724836  [   64/  179]
train() client id: f_00007-1-2 loss: 0.677723  [   96/  179]
train() client id: f_00007-1-3 loss: 0.499125  [  128/  179]
train() client id: f_00007-1-4 loss: 0.686866  [  160/  179]
train() client id: f_00007-2-0 loss: 0.584875  [   32/  179]
train() client id: f_00007-2-1 loss: 0.625549  [   64/  179]
train() client id: f_00007-2-2 loss: 0.503527  [   96/  179]
train() client id: f_00007-2-3 loss: 0.697915  [  128/  179]
train() client id: f_00007-2-4 loss: 0.898978  [  160/  179]
train() client id: f_00008-0-0 loss: 0.622492  [   32/  130]
train() client id: f_00008-0-1 loss: 0.735953  [   64/  130]
train() client id: f_00008-0-2 loss: 0.638178  [   96/  130]
train() client id: f_00008-0-3 loss: 0.698434  [  128/  130]
train() client id: f_00008-1-0 loss: 0.668590  [   32/  130]
train() client id: f_00008-1-1 loss: 0.693507  [   64/  130]
train() client id: f_00008-1-2 loss: 0.716471  [   96/  130]
train() client id: f_00008-1-3 loss: 0.604645  [  128/  130]
train() client id: f_00008-2-0 loss: 0.551034  [   32/  130]
train() client id: f_00008-2-1 loss: 0.636455  [   64/  130]
train() client id: f_00008-2-2 loss: 0.716089  [   96/  130]
train() client id: f_00008-2-3 loss: 0.753529  [  128/  130]
train() client id: f_00009-0-0 loss: 0.507847  [   32/  118]
train() client id: f_00009-0-1 loss: 0.658529  [   64/  118]
train() client id: f_00009-0-2 loss: 0.676814  [   96/  118]
train() client id: f_00009-1-0 loss: 0.456832  [   32/  118]
train() client id: f_00009-1-1 loss: 0.670232  [   64/  118]
train() client id: f_00009-1-2 loss: 0.644791  [   96/  118]
train() client id: f_00009-2-0 loss: 0.434299  [   32/  118]
train() client id: f_00009-2-1 loss: 0.607077  [   64/  118]
train() client id: f_00009-2-2 loss: 0.616553  [   96/  118]
At round 78 accuracy: 0.6445623342175066
At round 78 training accuracy: 0.5875251509054326
At round 78 training loss: 0.8206349509143767
update_location
xs = -3.905658 4.200318 410.009024 18.811294 0.979296 3.956410 -372.443192 -351.324852 394.663977 -337.060879 
ys = 402.587959 385.555839 1.320614 -372.455176 364.350187 347.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 414.840113 398.335220 422.029790 386.104549 377.825380 361.925872 385.643387 365.280478 407.514815 351.604960 
dists_bs = 284.727155 276.489629 610.245924 580.749505 258.499915 248.777264 265.583224 247.847348 591.017398 235.593339 
uav_gains = -121.924061 -121.341910 -122.160400 -120.867858 -120.522125 -119.790526 -120.849162 -119.953078 -121.672992 -119.259360 
bs_gains = -108.291068 -107.934067 -117.561196 -116.958747 -107.115950 -106.649758 -107.444677 -106.604219 -117.171867 -105.987623 
Round 79
-------------------------------
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.47167495 0.91597325 0.46632083 0.18231339 1.05554823 0.50864652
 0.2198029  0.64284551 0.46659513 0.41263047]
obj_prev = 5.342351171013073
eta_min = 2.6352711032412936e-201	eta_max = 0.9959613407486563
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 1.2045873201186579	eta = 0.909090909090909
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 3.853002462740199	eta = 0.28421455540603213
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 2.268192123899988	eta = 0.48279833546161194
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 2.0171814832442236	eta = 0.5428759836545998
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 2.0013314548574064	eta = 0.5471754212767701
af = 1.0950793819260525	bf = 0.24590073836681936	zeta = 2.0012564070127876	eta = 0.5471959405544855
eta = 0.5471959405544855
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [0.04975385 0.10464104 0.04896411 0.01697949 0.12083078 0.05765129
 0.02132308 0.07068206 0.05133334 0.04659488]
ene_total = [0.21330398 0.28236012 0.21458127 0.11946231 0.32136361 0.16430363
 0.13001973 0.24348682 0.17623076 0.13614417]
ti_comp = [10.15400802 10.42406192 10.14095348 10.20515528 10.42848067 10.43082397
 10.20596501 10.24142523 10.31948805 10.43395522]
ti_coms = [0.36368797 0.09363407 0.37674251 0.31254071 0.08921532 0.08687202
 0.31173098 0.27627076 0.19820794 0.08374077]
t_total = [26.0496685 26.0496685 26.0496685 26.0496685 26.0496685 26.0496685
 26.0496685 26.0496685 26.0496685 26.0496685]
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [7.46595073e-08 6.59040563e-07 7.13437549e-08 2.93774855e-09
 1.01384295e-06 1.10070217e-07 5.81730210e-09 2.10419888e-07
 7.93893490e-08 5.80759835e-08]
ene_total = [0.09007648 0.02319242 0.09330975 0.07740842 0.02209888 0.02151627
 0.07720788 0.06842579 0.04909128 0.02074061]
optimize_network iter = 0 obj = 0.5430677938747402
eta = 0.5471959405544855
freqs = [2449961.18756652 5019206.37355413 2414176.71155869  831907.42340227
 5793307.05286581 2763505.9294842  1044638.0630291  3450792.08881117
 2487203.73595765 2232848.20767479]
eta_min = 0.5471959405544858	eta_max = 0.9355133020148048
af = 4.251199729612188e-06	bf = 0.24590073836681936	zeta = 4.6763197025734075e-06	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [1.18016929e-08 1.04176878e-07 1.12775602e-08 4.64380326e-10
 1.60261749e-07 1.73991893e-08 9.19561561e-10 3.32618174e-08
 1.25493558e-08 9.18027656e-09]
ene_total = [0.44865618 0.11551099 0.46476064 0.3855593  0.11006058 0.10716806
 0.3845604  0.34081604 0.24451523 0.10330516]
ti_comp = [1.74794103 2.01799493 1.73488648 1.79908829 2.02241368 2.02475698
 1.79989802 1.83535824 1.91342105 2.02788823]
ti_coms = [0.36368797 0.09363407 0.37674251 0.31254071 0.08921532 0.08687202
 0.31173098 0.27627076 0.19820794 0.08374077]
t_total = [26.0496685 26.0496685 26.0496685 26.0496685 26.0496685 26.0496685
 26.0496685 26.0496685 26.0496685 26.0496685]
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [9.95139913e-09 6.94582032e-08 9.62828620e-09 3.73358873e-10
 1.06475663e-07 1.15382202e-08 7.38773900e-10 2.58787602e-08
 9.12084391e-09 6.07273117e-09]
ene_total = [0.44865616 0.11551056 0.46476062 0.3855593  0.11005992 0.10716799
 0.38456039 0.34081595 0.24451519 0.10330512]
optimize_network iter = 1 obj = 2.7049111966856447
eta = 0.909090909090909
freqs = [2249723.37868762 4098369.93259797 2230673.32764145  745935.59638761
 4722117.45608519 2250428.94068191  936334.90519755 3043809.99590222
 2120401.76543593 1816031.36876833]
eta_min = 0.9090909090910358	eta_max = 0.909090909090905
af = 2.9326740829202082e-06	bf = 0.24590073836681936	zeta = 3.2259414912122293e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [9.95139913e-09 6.94582032e-08 9.62828620e-09 3.73358873e-10
 1.06475663e-07 1.15382202e-08 7.38773900e-10 2.58787602e-08
 9.12084391e-09 6.07273117e-09]
ene_total = [0.44865616 0.11551056 0.46476062 0.3855593  0.11005992 0.10716799
 0.38456039 0.34081595 0.24451519 0.10330512]
ti_comp = [1.74794103 2.01799493 1.73488648 1.79908829 2.02241368 2.02475698
 1.79989802 1.83535824 1.91342105 2.02788823]
ti_coms = [0.36368797 0.09363407 0.37674251 0.31254071 0.08921532 0.08687202
 0.31173098 0.27627076 0.19820794 0.08374077]
t_total = [26.0496685 26.0496685 26.0496685 26.0496685 26.0496685 26.0496685
 26.0496685 26.0496685 26.0496685 26.0496685]
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [9.95139913e-09 6.94582032e-08 9.62828620e-09 3.73358873e-10
 1.06475663e-07 1.15382202e-08 7.38773900e-10 2.58787602e-08
 9.12084391e-09 6.07273117e-09]
ene_total = [0.44865616 0.11551056 0.46476062 0.3855593  0.11005992 0.10716799
 0.38456039 0.34081595 0.24451519 0.10330512]
optimize_network iter = 2 obj = 2.7049111966855257
eta = 0.909090909090905
freqs = [2249723.37868761 4098369.93259797 2230673.32764143  745935.59638761
 4722117.4560852  2250428.94068191  936334.90519755 3043809.99590221
 2120401.76543593 1816031.36876833]
Done!
ene_coms = [0.0363688  0.00936341 0.03767425 0.03125407 0.00892153 0.0086872
 0.0311731  0.02762708 0.01982079 0.00837408]
ene_comp = [9.56577248e-09 6.67666285e-08 9.25518050e-09 3.58890845e-10
 1.02349625e-07 1.10911026e-08 7.10145674e-10 2.48759324e-08
 8.76740211e-09 5.83740678e-09]
ene_total = [0.03636881 0.00936347 0.03767426 0.03125407 0.00892163 0.00868721
 0.0311731  0.0276271  0.0198208  0.00837408]
At round 79 energy consumption: 0.2192645430104472
At round 79 eta: 0.909090909090905
At round 79 a_n: 1.1214809456783108
At round 79 local rounds: 3.120939520577902
At round 79 global rounds: 12.336290402460858
gradient difference: 0.9050496816635132
train() client id: f_00000-0-0 loss: 1.138886  [   32/  126]
train() client id: f_00000-0-1 loss: 0.923957  [   64/  126]
train() client id: f_00000-0-2 loss: 0.904896  [   96/  126]
train() client id: f_00000-1-0 loss: 0.882583  [   32/  126]
train() client id: f_00000-1-1 loss: 0.994285  [   64/  126]
train() client id: f_00000-1-2 loss: 0.979869  [   96/  126]
train() client id: f_00000-2-0 loss: 1.026500  [   32/  126]
train() client id: f_00000-2-1 loss: 0.867645  [   64/  126]
train() client id: f_00000-2-2 loss: 0.878581  [   96/  126]
train() client id: f_00001-0-0 loss: 0.436352  [   32/  265]
train() client id: f_00001-0-1 loss: 0.482238  [   64/  265]
train() client id: f_00001-0-2 loss: 0.528369  [   96/  265]
train() client id: f_00001-0-3 loss: 0.518457  [  128/  265]
train() client id: f_00001-0-4 loss: 0.458919  [  160/  265]
train() client id: f_00001-0-5 loss: 0.436918  [  192/  265]
train() client id: f_00001-0-6 loss: 0.477591  [  224/  265]
train() client id: f_00001-0-7 loss: 0.614266  [  256/  265]
train() client id: f_00001-1-0 loss: 0.472386  [   32/  265]
train() client id: f_00001-1-1 loss: 0.414250  [   64/  265]
train() client id: f_00001-1-2 loss: 0.405342  [   96/  265]
train() client id: f_00001-1-3 loss: 0.600179  [  128/  265]
train() client id: f_00001-1-4 loss: 0.481887  [  160/  265]
train() client id: f_00001-1-5 loss: 0.538309  [  192/  265]
train() client id: f_00001-1-6 loss: 0.379261  [  224/  265]
train() client id: f_00001-1-7 loss: 0.642859  [  256/  265]
train() client id: f_00001-2-0 loss: 0.509804  [   32/  265]
train() client id: f_00001-2-1 loss: 0.510523  [   64/  265]
train() client id: f_00001-2-2 loss: 0.385547  [   96/  265]
train() client id: f_00001-2-3 loss: 0.467602  [  128/  265]
train() client id: f_00001-2-4 loss: 0.636536  [  160/  265]
train() client id: f_00001-2-5 loss: 0.409872  [  192/  265]
train() client id: f_00001-2-6 loss: 0.584476  [  224/  265]
train() client id: f_00001-2-7 loss: 0.390081  [  256/  265]
train() client id: f_00002-0-0 loss: 0.970201  [   32/  124]
train() client id: f_00002-0-1 loss: 0.875829  [   64/  124]
train() client id: f_00002-0-2 loss: 1.066621  [   96/  124]
train() client id: f_00002-1-0 loss: 0.890306  [   32/  124]
train() client id: f_00002-1-1 loss: 0.821334  [   64/  124]
train() client id: f_00002-1-2 loss: 1.190210  [   96/  124]
train() client id: f_00002-2-0 loss: 0.887379  [   32/  124]
train() client id: f_00002-2-1 loss: 0.948347  [   64/  124]
train() client id: f_00002-2-2 loss: 0.933301  [   96/  124]
train() client id: f_00003-0-0 loss: 0.948938  [   32/   43]
train() client id: f_00003-1-0 loss: 0.726643  [   32/   43]
train() client id: f_00003-2-0 loss: 0.647043  [   32/   43]
train() client id: f_00004-0-0 loss: 0.799868  [   32/  306]
train() client id: f_00004-0-1 loss: 0.961081  [   64/  306]
train() client id: f_00004-0-2 loss: 0.931896  [   96/  306]
train() client id: f_00004-0-3 loss: 0.881151  [  128/  306]
train() client id: f_00004-0-4 loss: 0.851716  [  160/  306]
train() client id: f_00004-0-5 loss: 0.965970  [  192/  306]
train() client id: f_00004-0-6 loss: 0.969849  [  224/  306]
train() client id: f_00004-0-7 loss: 0.975564  [  256/  306]
train() client id: f_00004-0-8 loss: 0.908133  [  288/  306]
train() client id: f_00004-1-0 loss: 0.892726  [   32/  306]
train() client id: f_00004-1-1 loss: 0.825451  [   64/  306]
train() client id: f_00004-1-2 loss: 0.906846  [   96/  306]
train() client id: f_00004-1-3 loss: 0.996540  [  128/  306]
train() client id: f_00004-1-4 loss: 0.934966  [  160/  306]
train() client id: f_00004-1-5 loss: 1.095360  [  192/  306]
train() client id: f_00004-1-6 loss: 0.837563  [  224/  306]
train() client id: f_00004-1-7 loss: 0.970085  [  256/  306]
train() client id: f_00004-1-8 loss: 0.790317  [  288/  306]
train() client id: f_00004-2-0 loss: 1.034570  [   32/  306]
train() client id: f_00004-2-1 loss: 0.756388  [   64/  306]
train() client id: f_00004-2-2 loss: 0.863379  [   96/  306]
train() client id: f_00004-2-3 loss: 0.898970  [  128/  306]
train() client id: f_00004-2-4 loss: 0.929012  [  160/  306]
train() client id: f_00004-2-5 loss: 0.937741  [  192/  306]
train() client id: f_00004-2-6 loss: 0.854131  [  224/  306]
train() client id: f_00004-2-7 loss: 1.122088  [  256/  306]
train() client id: f_00004-2-8 loss: 0.958907  [  288/  306]
train() client id: f_00005-0-0 loss: 0.637444  [   32/  146]
train() client id: f_00005-0-1 loss: 0.540596  [   64/  146]
train() client id: f_00005-0-2 loss: 0.683007  [   96/  146]
train() client id: f_00005-0-3 loss: 0.656363  [  128/  146]
train() client id: f_00005-1-0 loss: 0.749271  [   32/  146]
train() client id: f_00005-1-1 loss: 0.754584  [   64/  146]
train() client id: f_00005-1-2 loss: 0.695153  [   96/  146]
train() client id: f_00005-1-3 loss: 0.482271  [  128/  146]
train() client id: f_00005-2-0 loss: 0.673190  [   32/  146]
train() client id: f_00005-2-1 loss: 0.788419  [   64/  146]
train() client id: f_00005-2-2 loss: 0.399886  [   96/  146]
train() client id: f_00005-2-3 loss: 0.682028  [  128/  146]
train() client id: f_00006-0-0 loss: 0.508636  [   32/   54]
train() client id: f_00006-1-0 loss: 0.508931  [   32/   54]
train() client id: f_00006-2-0 loss: 0.491141  [   32/   54]
train() client id: f_00007-0-0 loss: 0.312781  [   32/  179]
train() client id: f_00007-0-1 loss: 0.456215  [   64/  179]
train() client id: f_00007-0-2 loss: 0.724563  [   96/  179]
train() client id: f_00007-0-3 loss: 0.361885  [  128/  179]
train() client id: f_00007-0-4 loss: 0.442701  [  160/  179]
train() client id: f_00007-1-0 loss: 0.385789  [   32/  179]
train() client id: f_00007-1-1 loss: 0.614031  [   64/  179]
train() client id: f_00007-1-2 loss: 0.630672  [   96/  179]
train() client id: f_00007-1-3 loss: 0.440954  [  128/  179]
train() client id: f_00007-1-4 loss: 0.380216  [  160/  179]
train() client id: f_00007-2-0 loss: 0.306358  [   32/  179]
train() client id: f_00007-2-1 loss: 0.545872  [   64/  179]
train() client id: f_00007-2-2 loss: 0.517976  [   96/  179]
train() client id: f_00007-2-3 loss: 0.533341  [  128/  179]
train() client id: f_00007-2-4 loss: 0.558867  [  160/  179]
train() client id: f_00008-0-0 loss: 0.601060  [   32/  130]
train() client id: f_00008-0-1 loss: 0.706643  [   64/  130]
train() client id: f_00008-0-2 loss: 0.649359  [   96/  130]
train() client id: f_00008-0-3 loss: 0.675903  [  128/  130]
train() client id: f_00008-1-0 loss: 0.630636  [   32/  130]
train() client id: f_00008-1-1 loss: 0.734130  [   64/  130]
train() client id: f_00008-1-2 loss: 0.628324  [   96/  130]
train() client id: f_00008-1-3 loss: 0.668913  [  128/  130]
train() client id: f_00008-2-0 loss: 0.625393  [   32/  130]
train() client id: f_00008-2-1 loss: 0.619570  [   64/  130]
train() client id: f_00008-2-2 loss: 0.635802  [   96/  130]
train() client id: f_00008-2-3 loss: 0.757818  [  128/  130]
train() client id: f_00009-0-0 loss: 0.984802  [   32/  118]
train() client id: f_00009-0-1 loss: 0.676365  [   64/  118]
train() client id: f_00009-0-2 loss: 0.712393  [   96/  118]
train() client id: f_00009-1-0 loss: 0.683369  [   32/  118]
train() client id: f_00009-1-1 loss: 0.782284  [   64/  118]
train() client id: f_00009-1-2 loss: 0.899631  [   96/  118]
train() client id: f_00009-2-0 loss: 0.877244  [   32/  118]
train() client id: f_00009-2-1 loss: 0.671664  [   64/  118]
train() client id: f_00009-2-2 loss: 0.772623  [   96/  118]
At round 79 accuracy: 0.6445623342175066
At round 79 training accuracy: 0.590878604963112
At round 79 training loss: 0.8237731757613136
update_location
xs = -3.905658 4.200318 415.009024 18.811294 0.979296 3.956410 -377.443192 -356.324852 399.663977 -342.060879 
ys = 407.587959 390.555839 1.320614 -377.455176 369.350187 352.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 419.694173 403.176767 426.889018 390.930012 382.649343 366.733525 390.474396 370.091984 412.359024 356.400977 
dists_bs = 288.739385 280.316024 615.040793 585.465233 262.184492 252.276176 269.321148 251.429028 595.838237 239.060307 
uav_gains = -122.084655 -121.518948 -122.314984 -121.059772 -120.726208 -120.022054 -121.041934 -120.178291 -121.840240 -119.512250 
bs_gains = -108.461228 -108.101201 -117.656369 -117.057090 -107.288055 -106.819594 -107.614632 -106.778691 -117.270653 -106.165268 
Round 80
-------------------------------
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.32829867 0.63627291 0.3245873  0.12729558 0.73321141 0.35335129
 0.15333479 0.44714953 0.32424655 0.28666076]
obj_prev = 3.7144087846743608
eta_min = 2.74115185868049e-289	eta_max = 0.9972962516128097
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 0.8366574097544858	eta = 0.9090909090909091
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 2.7178337538132036	eta = 0.2798543671643672
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 1.5874476546409413	eta = 0.4791324255686323
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 1.4095987989406396	eta = 0.539584487304448
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 1.398385343651186	eta = 0.5439113393776205
af = 0.7605976452313506	bf = 0.17458309747545425	zeta = 1.39833229498431	eta = 0.5439319737944585
eta = 0.5439319737944585
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [0.05024753 0.10567934 0.04944995 0.01714797 0.12202973 0.05822333
 0.02153466 0.0713834  0.05184269 0.04705721]
ene_total = [0.14943821 0.1966487  0.15032166 0.08411788 0.22380943 0.11442236
 0.09147288 0.17052575 0.12276251 0.0948129 ]
ti_comp = [14.85036606 15.12826574 14.83721764 14.90182223 15.13274303 15.13514254
 14.90262547 14.93825677 15.02250443 15.13829514]
ti_coms = [0.37248833 0.09458864 0.38563675 0.32103215 0.09011135 0.08771185
 0.32022891 0.28459761 0.20034996 0.08455925]
t_total = [25.99966431 25.99966431 25.99966431 25.99966431 25.99966431 25.99966431
 25.99966431 25.99966431 25.99966431 25.99966431]
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [3.59542268e-08 3.22308656e-07 3.43299174e-08 1.41918577e-09
 4.95954233e-07 5.38515457e-08 2.81040133e-09 1.01876001e-07
 3.85885102e-08 2.84187337e-08]
ene_total = [0.06361869 0.01615569 0.06586436 0.05483025 0.01539129 0.01498072
 0.05469306 0.04860763 0.03421856 0.01444223]
optimize_network iter = 0 obj = 0.38280246883118135
eta = 0.5439319737944585
freqs = [1691794.45393543 3492777.65574166 1666416.00768811  575364.79907189
 4031976.3853505  1923448.44978197  722512.19806255 2389281.50597988
 1725501.02801081 1554244.18120876]
eta_min = 0.5439319737944591	eta_max = 0.9542982433752891
af = 1.4269447035111872e-06	bf = 0.17458309747545425	zeta = 1.569639173862306e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [5.62757246e-09 5.04479022e-08 5.37333478e-09 2.22131623e-10
 7.76269894e-08 8.42886922e-09 4.39885336e-10 1.59456795e-08
 6.03989174e-09 4.44811354e-09]
ene_total = [0.3191587  0.08104669 0.33042464 0.27506954 0.07721065 0.07515409
 0.2743813  0.24385151 0.17166562 0.07245282]
ti_comp = [2.66191833 2.93981801 2.64876991 2.7133745  2.9442953  2.9466948
 2.71417774 2.74980904 2.8340567  2.9498474 ]
ti_coms = [0.37248833 0.09458864 0.38563675 0.32103215 0.09011135 0.08771185
 0.32022891 0.28459761 0.20034996 0.08455925]
t_total = [25.99966431 25.99966431 25.99966431 25.99966431 25.99966431 25.99966431
 25.99966431 25.99966431 25.99966431 25.99966431]
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [4.29089854e-09 3.27282506e-08 4.13050023e-09 1.64139063e-10
 5.02374527e-08 5.44771420e-09 3.24885833e-10 1.15286932e-08
 4.15756503e-09 2.86994110e-09]
ene_total = [0.31915869 0.08104654 0.33042463 0.27506954 0.07721041 0.07515407
 0.2743813  0.24385147 0.17166561 0.07245281]
optimize_network iter = 1 obj = 1.9204150651233343
eta = 0.9090909090909091
freqs = [1477274.39950027 2813265.88968522 1461042.34829177  494588.56272364
 3243585.97051685 1546333.09614372  620927.40517717 2031588.97688467
 1431594.99214584 1248440.38657298]
eta_min = 0.9090909090909186	eta_max = 0.9090909090909067
af = 9.470450164325744e-07	bf = 0.17458309747545425	zeta = 1.0417495180758319e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [4.29089854e-09 3.27282506e-08 4.13050023e-09 1.64139063e-10
 5.02374527e-08 5.44771420e-09 3.24885833e-10 1.15286932e-08
 4.15756503e-09 2.86994110e-09]
ene_total = [0.31915869 0.08104654 0.33042463 0.27506954 0.07721041 0.07515407
 0.2743813  0.24385147 0.17166561 0.07245281]
ti_comp = [2.66191833 2.93981801 2.64876991 2.7133745  2.9442953  2.9466948
 2.71417774 2.74980904 2.8340567  2.9498474 ]
ti_coms = [0.37248833 0.09458864 0.38563675 0.32103215 0.09011135 0.08771185
 0.32022891 0.28459761 0.20034996 0.08455925]
t_total = [25.99966431 25.99966431 25.99966431 25.99966431 25.99966431 25.99966431
 25.99966431 25.99966431 25.99966431 25.99966431]
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [4.29089854e-09 3.27282506e-08 4.13050023e-09 1.64139063e-10
 5.02374527e-08 5.44771420e-09 3.24885833e-10 1.15286932e-08
 4.15756503e-09 2.86994110e-09]
ene_total = [0.31915869 0.08104654 0.33042463 0.27506954 0.07721041 0.07515407
 0.2743813  0.24385147 0.17166561 0.07245281]
optimize_network iter = 2 obj = 1.9204150651232854
eta = 0.9090909090909067
freqs = [1477274.39950027 2813265.88968522 1461042.34829176  494588.56272364
 3243585.97051684 1546333.09614371  620927.40517717 2031588.97688466
 1431594.99214584 1248440.38657297]
Done!
ene_coms = [0.03724883 0.00945886 0.03856367 0.03210322 0.00901114 0.00877118
 0.03202289 0.02845976 0.020035   0.00845592]
ene_comp = [4.12462194e-09 3.14599982e-08 3.97043922e-09 1.57778511e-10
 4.82907013e-08 5.23660984e-09 3.12296182e-10 1.10819449e-08
 3.99645523e-09 2.75872802e-09]
ene_total = [0.03724884 0.0094589  0.03856368 0.03210322 0.00901118 0.00877119
 0.03202289 0.02845977 0.020035   0.00845593]
At round 80 energy consumption: 0.22413059127783166
At round 80 eta: 0.9090909090909067
At round 80 a_n: 0.7789350987089918
At round 80 local rounds: 3.120939520577836
At round 80 global rounds: 8.568286085798688
gradient difference: 0.9441202282905579
train() client id: f_00000-0-0 loss: 0.847699  [   32/  126]
train() client id: f_00000-0-1 loss: 1.018496  [   64/  126]
train() client id: f_00000-0-2 loss: 0.968636  [   96/  126]
train() client id: f_00000-1-0 loss: 0.771978  [   32/  126]
train() client id: f_00000-1-1 loss: 1.036954  [   64/  126]
train() client id: f_00000-1-2 loss: 0.949768  [   96/  126]
train() client id: f_00000-2-0 loss: 1.031511  [   32/  126]
train() client id: f_00000-2-1 loss: 0.905763  [   64/  126]
train() client id: f_00000-2-2 loss: 1.052096  [   96/  126]
train() client id: f_00001-0-0 loss: 0.648306  [   32/  265]
train() client id: f_00001-0-1 loss: 0.588630  [   64/  265]
train() client id: f_00001-0-2 loss: 0.559484  [   96/  265]
train() client id: f_00001-0-3 loss: 0.527956  [  128/  265]
train() client id: f_00001-0-4 loss: 0.641199  [  160/  265]
train() client id: f_00001-0-5 loss: 0.677426  [  192/  265]
train() client id: f_00001-0-6 loss: 0.590066  [  224/  265]
train() client id: f_00001-0-7 loss: 0.667167  [  256/  265]
train() client id: f_00001-1-0 loss: 0.610375  [   32/  265]
train() client id: f_00001-1-1 loss: 0.559368  [   64/  265]
train() client id: f_00001-1-2 loss: 0.600098  [   96/  265]
train() client id: f_00001-1-3 loss: 0.659468  [  128/  265]
train() client id: f_00001-1-4 loss: 0.569185  [  160/  265]
train() client id: f_00001-1-5 loss: 0.692827  [  192/  265]
train() client id: f_00001-1-6 loss: 0.698698  [  224/  265]
train() client id: f_00001-1-7 loss: 0.530529  [  256/  265]
train() client id: f_00001-2-0 loss: 0.693723  [   32/  265]
train() client id: f_00001-2-1 loss: 0.537882  [   64/  265]
train() client id: f_00001-2-2 loss: 0.582698  [   96/  265]
train() client id: f_00001-2-3 loss: 0.677689  [  128/  265]
train() client id: f_00001-2-4 loss: 0.614422  [  160/  265]
train() client id: f_00001-2-5 loss: 0.639571  [  192/  265]
train() client id: f_00001-2-6 loss: 0.566244  [  224/  265]
train() client id: f_00001-2-7 loss: 0.581909  [  256/  265]
train() client id: f_00002-0-0 loss: 0.980565  [   32/  124]
train() client id: f_00002-0-1 loss: 0.975580  [   64/  124]
train() client id: f_00002-0-2 loss: 1.068798  [   96/  124]
train() client id: f_00002-1-0 loss: 1.064238  [   32/  124]
train() client id: f_00002-1-1 loss: 0.994096  [   64/  124]
train() client id: f_00002-1-2 loss: 1.021857  [   96/  124]
train() client id: f_00002-2-0 loss: 1.098833  [   32/  124]
train() client id: f_00002-2-1 loss: 1.064406  [   64/  124]
train() client id: f_00002-2-2 loss: 1.033109  [   96/  124]
train() client id: f_00003-0-0 loss: 0.662446  [   32/   43]
train() client id: f_00003-1-0 loss: 1.146339  [   32/   43]
train() client id: f_00003-2-0 loss: 0.729164  [   32/   43]
train() client id: f_00004-0-0 loss: 0.816974  [   32/  306]
train() client id: f_00004-0-1 loss: 0.849332  [   64/  306]
train() client id: f_00004-0-2 loss: 0.717109  [   96/  306]
train() client id: f_00004-0-3 loss: 0.698289  [  128/  306]
train() client id: f_00004-0-4 loss: 0.787773  [  160/  306]
train() client id: f_00004-0-5 loss: 0.749508  [  192/  306]
train() client id: f_00004-0-6 loss: 0.803412  [  224/  306]
train() client id: f_00004-0-7 loss: 0.780802  [  256/  306]
train() client id: f_00004-0-8 loss: 0.685431  [  288/  306]
train() client id: f_00004-1-0 loss: 0.847718  [   32/  306]
train() client id: f_00004-1-1 loss: 0.846461  [   64/  306]
train() client id: f_00004-1-2 loss: 0.699617  [   96/  306]
train() client id: f_00004-1-3 loss: 0.787899  [  128/  306]
train() client id: f_00004-1-4 loss: 0.627118  [  160/  306]
train() client id: f_00004-1-5 loss: 0.872049  [  192/  306]
train() client id: f_00004-1-6 loss: 0.673508  [  224/  306]
train() client id: f_00004-1-7 loss: 0.902232  [  256/  306]
train() client id: f_00004-1-8 loss: 0.842571  [  288/  306]
train() client id: f_00004-2-0 loss: 0.820232  [   32/  306]
train() client id: f_00004-2-1 loss: 0.858291  [   64/  306]
train() client id: f_00004-2-2 loss: 0.909351  [   96/  306]
train() client id: f_00004-2-3 loss: 0.590878  [  128/  306]
train() client id: f_00004-2-4 loss: 0.753994  [  160/  306]
train() client id: f_00004-2-5 loss: 0.687189  [  192/  306]
train() client id: f_00004-2-6 loss: 0.877897  [  224/  306]
train() client id: f_00004-2-7 loss: 0.730907  [  256/  306]
train() client id: f_00004-2-8 loss: 0.841111  [  288/  306]
train() client id: f_00005-0-0 loss: 0.791881  [   32/  146]
train() client id: f_00005-0-1 loss: 0.707040  [   64/  146]
train() client id: f_00005-0-2 loss: 0.552141  [   96/  146]
train() client id: f_00005-0-3 loss: 0.688896  [  128/  146]
train() client id: f_00005-1-0 loss: 0.491658  [   32/  146]
train() client id: f_00005-1-1 loss: 0.895140  [   64/  146]
train() client id: f_00005-1-2 loss: 0.715710  [   96/  146]
train() client id: f_00005-1-3 loss: 0.676008  [  128/  146]
train() client id: f_00005-2-0 loss: 0.453545  [   32/  146]
train() client id: f_00005-2-1 loss: 1.198702  [   64/  146]
train() client id: f_00005-2-2 loss: 0.527860  [   96/  146]
train() client id: f_00005-2-3 loss: 0.436379  [  128/  146]
train() client id: f_00006-0-0 loss: 0.457764  [   32/   54]
train() client id: f_00006-1-0 loss: 0.454363  [   32/   54]
train() client id: f_00006-2-0 loss: 0.385754  [   32/   54]
train() client id: f_00007-0-0 loss: 0.809505  [   32/  179]
train() client id: f_00007-0-1 loss: 0.557555  [   64/  179]
train() client id: f_00007-0-2 loss: 0.508638  [   96/  179]
train() client id: f_00007-0-3 loss: 0.553865  [  128/  179]
train() client id: f_00007-0-4 loss: 0.613004  [  160/  179]
train() client id: f_00007-1-0 loss: 0.611260  [   32/  179]
train() client id: f_00007-1-1 loss: 0.538080  [   64/  179]
train() client id: f_00007-1-2 loss: 0.709084  [   96/  179]
train() client id: f_00007-1-3 loss: 0.464829  [  128/  179]
train() client id: f_00007-1-4 loss: 0.560932  [  160/  179]
train() client id: f_00007-2-0 loss: 0.460247  [   32/  179]
train() client id: f_00007-2-1 loss: 0.494673  [   64/  179]
train() client id: f_00007-2-2 loss: 0.718725  [   96/  179]
train() client id: f_00007-2-3 loss: 0.564085  [  128/  179]
train() client id: f_00007-2-4 loss: 0.557190  [  160/  179]
train() client id: f_00008-0-0 loss: 0.665655  [   32/  130]
train() client id: f_00008-0-1 loss: 0.747605  [   64/  130]
train() client id: f_00008-0-2 loss: 0.634223  [   96/  130]
train() client id: f_00008-0-3 loss: 0.628132  [  128/  130]
train() client id: f_00008-1-0 loss: 0.652618  [   32/  130]
train() client id: f_00008-1-1 loss: 0.699751  [   64/  130]
train() client id: f_00008-1-2 loss: 0.677520  [   96/  130]
train() client id: f_00008-1-3 loss: 0.685564  [  128/  130]
train() client id: f_00008-2-0 loss: 0.700257  [   32/  130]
train() client id: f_00008-2-1 loss: 0.650472  [   64/  130]
train() client id: f_00008-2-2 loss: 0.761520  [   96/  130]
train() client id: f_00008-2-3 loss: 0.616293  [  128/  130]
train() client id: f_00009-0-0 loss: 0.748854  [   32/  118]
train() client id: f_00009-0-1 loss: 0.724607  [   64/  118]
train() client id: f_00009-0-2 loss: 0.755386  [   96/  118]
train() client id: f_00009-1-0 loss: 0.814139  [   32/  118]
train() client id: f_00009-1-1 loss: 0.697301  [   64/  118]
train() client id: f_00009-1-2 loss: 0.719649  [   96/  118]
train() client id: f_00009-2-0 loss: 0.569443  [   32/  118]
train() client id: f_00009-2-1 loss: 0.805781  [   64/  118]
train() client id: f_00009-2-2 loss: 0.751116  [   96/  118]
At round 80 accuracy: 0.6445623342175066
At round 80 training accuracy: 0.5888665325285044
At round 80 training loss: 0.82557827690914
update_location
xs = -3.905658 4.200318 420.009024 18.811294 0.979296 3.956410 -382.443192 -361.324852 404.663977 -347.060879 
ys = 412.587959 395.555839 1.320614 -382.455176 374.350187 357.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 424.551621 408.022137 431.751461 395.759809 387.477769 371.546255 395.309607 374.908422 417.206908 361.202527 
dists_bs = 292.782022 284.178873 619.838905 590.185641 265.912034 255.824960 273.099456 255.058434 600.662005 242.580789 
uav_gains = -122.241120 -121.690738 -122.465833 -121.245403 -120.923157 -120.244605 -121.228360 -120.394939 -122.002913 -119.754854 
bs_gains = -108.630302 -108.267629 -117.750866 -117.154741 -107.459723 -106.989461 -107.784043 -106.952970 -117.368704 -106.343038 
Round 81
-------------------------------
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.18431614 0.35650715 0.18224127 0.07169206 0.41081325 0.19799863
 0.08628049 0.25087914 0.1817507  0.16063512]
obj_prev = 2.0831139657376503
eta_min = 0.0	eta_max = inf
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.4687274993903175	eta = 0.909090909090909
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 1.5461570538938256	eta = 0.2755967820109389
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.8960917211906734	eta = 0.47552711230325256
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.7944965020417399	eta = 0.5363345306638816
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.7881017357421507	eta = 0.5406864230991465
af = 0.4261159085366522	bf = 0.0999465483120546	zeta = 0.7880715228657817	eta = 0.54070715179138
eta = 0.54070715179138
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [0.05073821 0.10671132 0.04993285 0.01731542 0.12322138 0.0587919
 0.02174495 0.07208048 0.05234895 0.04751674]
ene_total = [0.08444145 0.1104692  0.08493497 0.04776518 0.12572503 0.06427522
 0.05189808 0.09632167 0.06897994 0.05326078]
ti_comp = [26.93025964 27.2160528  26.91701177 26.98204373 27.22058811 27.22304297
 26.98284085 27.01864488 27.10909704 27.22621665]
ti_coms = [0.38135093 0.09555778 0.3945988  0.32956684 0.09102246 0.0885676
 0.32876972 0.2929657  0.20251354 0.08539392]
t_total = [25.94966011 25.94966011 25.94966011 25.94966011 25.94966011 25.94966011
 25.94966011 25.94966011 25.94966011 25.94966011]
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.12565281e-08 1.02532430e-07 1.07395299e-08 4.45686613e-10
 1.57813146e-07 1.71379604e-08 8.82632352e-10 3.20631268e-08
 1.22003941e-08 9.04576573e-09]
ene_total = [0.03623342 0.00907936 0.03749214 0.03131323 0.00864849 0.00841512
 0.03123749 0.02783567 0.01924149 0.00811357]
optimize_network iter = 0 obj = 0.21760996983692812
eta = 0.54070715179138
freqs = [ 942029.79379861 1960448.1887988   927533.22358696  320869.35975109
 2263385.64084244 1079818.64242409  402940.3153214  1333902.54699206
  965523.69021721  872628.40406785]
eta_min = 0.5407071517913803	eta_max = 0.9738786232505544
af = 2.5132315156122307e-07	bf = 0.0999465483120546	zeta = 2.764554667173454e-07	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.74483827e-09 1.58932227e-08 1.66470005e-09 6.90844506e-11
 2.44621090e-08 2.65650021e-09 1.36814006e-10 4.97000232e-09
 1.89114391e-09 1.40215510e-09]
ene_total = [0.1830592  0.0458705  0.18941855 0.15820137 0.04369346 0.04251496
 0.15781873 0.14063181 0.09721221 0.04099149]
ti_comp = [5.02451092 5.31030408 5.01126305 5.07629501 5.3148394  5.31729426
 5.07709213 5.11289616 5.20334832 5.32046794]
ti_coms = [0.38135093 0.09555778 0.3945988  0.32956684 0.09102246 0.0885676
 0.32876972 0.2929657  0.20251354 0.08539392]
t_total = [25.94966011 25.94966011 25.94966011 25.94966011 25.94966011 25.94966011
 25.94966011 25.94966011 25.94966011 25.94966011]
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.20434277e-09 1.00305536e-08 1.15397584e-09 4.68962074e-11
 1.54173613e-08 1.67302812e-09 9.28490047e-11 3.33465732e-09
 1.23336131e-09 8.82210910e-10]
ene_total = [0.1830592  0.04587047 0.18941855 0.15820137 0.04369342 0.04251495
 0.15781873 0.14063181 0.09721221 0.04099149]
optimize_network iter = 1 obj = 1.0994121997748156
eta = 0.909090909090909
freqs = [ 782640.11306705 1557441.83511423  772253.41470056  264366.82466173
 1796869.89992426  856934.27544998  331943.42290144 1092625.69941991
  779732.80435494  692177.58267536]
eta_min = 0.9090909090909166	eta_max = 0.9090909090908771
af = 1.605687647826223e-07	bf = 0.0999465483120546	zeta = 1.7662564126088454e-07	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.20434277e-09 1.00305536e-08 1.15397584e-09 4.68962074e-11
 1.54173613e-08 1.67302812e-09 9.28490047e-11 3.33465732e-09
 1.23336131e-09 8.82210910e-10]
ene_total = [0.1830592  0.04587047 0.18941855 0.15820137 0.04369342 0.04251495
 0.15781873 0.14063181 0.09721221 0.04099149]
ti_comp = [5.02451092 5.31030408 5.01126305 5.07629501 5.3148394  5.31729426
 5.07709213 5.11289616 5.20334832 5.32046794]
ti_coms = [0.38135093 0.09555778 0.3945988  0.32956684 0.09102246 0.0885676
 0.32876972 0.2929657  0.20251354 0.08539392]
t_total = [25.94966011 25.94966011 25.94966011 25.94966011 25.94966011 25.94966011
 25.94966011 25.94966011 25.94966011 25.94966011]
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.20434277e-09 1.00305536e-08 1.15397584e-09 4.68962074e-11
 1.54173613e-08 1.67302812e-09 9.28490047e-11 3.33465732e-09
 1.23336131e-09 8.82210910e-10]
ene_total = [0.1830592  0.04587047 0.18941855 0.15820137 0.04369342 0.04251495
 0.15781873 0.14063181 0.09721221 0.04099149]
optimize_network iter = 2 obj = 1.09941219977443
eta = 0.9090909090908771
freqs = [ 782640.11306704 1557441.83511425  772253.41470055  264366.82466173
 1796869.89992429  856934.27544999  331943.42290144 1092625.69941991
  779732.80435495  692177.58267537]
Done!
ene_coms = [0.03813509 0.00955578 0.03945988 0.03295668 0.00910225 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
ene_comp = [1.15767328e-09 9.64185965e-09 1.10925813e-09 4.50789326e-11
 1.48199232e-08 1.60819661e-09 8.92510132e-11 3.20543602e-09
 1.18556733e-09 8.48024357e-10]
ene_total = [0.03813509 0.00955579 0.03945988 0.03295668 0.00910226 0.00885676
 0.03287697 0.02929657 0.02025135 0.00853939]
At round 81 energy consumption: 0.22903076238567763
At round 81 eta: 0.9090909090908771
At round 81 a_n: 0.43638925173967635
At round 81 local rounds: 3.120939520578907
At round 81 global rounds: 4.80028176913475
gradient difference: 1.1722935438156128
train() client id: f_00000-0-0 loss: 0.671803  [   32/  126]
train() client id: f_00000-0-1 loss: 1.090282  [   64/  126]
train() client id: f_00000-0-2 loss: 0.768199  [   96/  126]
train() client id: f_00000-1-0 loss: 0.893951  [   32/  126]
train() client id: f_00000-1-1 loss: 0.746605  [   64/  126]
train() client id: f_00000-1-2 loss: 0.825049  [   96/  126]
train() client id: f_00000-2-0 loss: 0.877696  [   32/  126]
train() client id: f_00000-2-1 loss: 0.788183  [   64/  126]
train() client id: f_00000-2-2 loss: 0.775064  [   96/  126]
train() client id: f_00001-0-0 loss: 0.532745  [   32/  265]
train() client id: f_00001-0-1 loss: 0.472277  [   64/  265]
train() client id: f_00001-0-2 loss: 0.498087  [   96/  265]
train() client id: f_00001-0-3 loss: 0.488125  [  128/  265]
train() client id: f_00001-0-4 loss: 0.558341  [  160/  265]
train() client id: f_00001-0-5 loss: 0.702963  [  192/  265]
train() client id: f_00001-0-6 loss: 0.595093  [  224/  265]
train() client id: f_00001-0-7 loss: 0.643414  [  256/  265]
train() client id: f_00001-1-0 loss: 0.516354  [   32/  265]
train() client id: f_00001-1-1 loss: 0.455817  [   64/  265]
train() client id: f_00001-1-2 loss: 0.555492  [   96/  265]
train() client id: f_00001-1-3 loss: 0.526465  [  128/  265]
train() client id: f_00001-1-4 loss: 0.570090  [  160/  265]
train() client id: f_00001-1-5 loss: 0.662275  [  192/  265]
train() client id: f_00001-1-6 loss: 0.571384  [  224/  265]
train() client id: f_00001-1-7 loss: 0.647887  [  256/  265]
train() client id: f_00001-2-0 loss: 0.579885  [   32/  265]
train() client id: f_00001-2-1 loss: 0.478909  [   64/  265]
train() client id: f_00001-2-2 loss: 0.475867  [   96/  265]
train() client id: f_00001-2-3 loss: 0.528273  [  128/  265]
train() client id: f_00001-2-4 loss: 0.616743  [  160/  265]
train() client id: f_00001-2-5 loss: 0.621667  [  192/  265]
train() client id: f_00001-2-6 loss: 0.466072  [  224/  265]
train() client id: f_00001-2-7 loss: 0.683071  [  256/  265]
train() client id: f_00002-0-0 loss: 0.852697  [   32/  124]
train() client id: f_00002-0-1 loss: 1.050129  [   64/  124]
train() client id: f_00002-0-2 loss: 0.879129  [   96/  124]
train() client id: f_00002-1-0 loss: 0.845489  [   32/  124]
train() client id: f_00002-1-1 loss: 0.979027  [   64/  124]
train() client id: f_00002-1-2 loss: 0.714959  [   96/  124]
train() client id: f_00002-2-0 loss: 0.792543  [   32/  124]
train() client id: f_00002-2-1 loss: 0.801992  [   64/  124]
train() client id: f_00002-2-2 loss: 0.850730  [   96/  124]
train() client id: f_00003-0-0 loss: 0.670967  [   32/   43]
train() client id: f_00003-1-0 loss: 0.438119  [   32/   43]
train() client id: f_00003-2-0 loss: 0.597744  [   32/   43]
train() client id: f_00004-0-0 loss: 0.895645  [   32/  306]
train() client id: f_00004-0-1 loss: 0.849355  [   64/  306]
train() client id: f_00004-0-2 loss: 0.693651  [   96/  306]
train() client id: f_00004-0-3 loss: 0.772273  [  128/  306]
train() client id: f_00004-0-4 loss: 0.741958  [  160/  306]
train() client id: f_00004-0-5 loss: 0.635342  [  192/  306]
train() client id: f_00004-0-6 loss: 0.725121  [  224/  306]
train() client id: f_00004-0-7 loss: 0.692987  [  256/  306]
train() client id: f_00004-0-8 loss: 0.661821  [  288/  306]
train() client id: f_00004-1-0 loss: 0.798744  [   32/  306]
train() client id: f_00004-1-1 loss: 0.728365  [   64/  306]
train() client id: f_00004-1-2 loss: 0.754679  [   96/  306]
train() client id: f_00004-1-3 loss: 0.806052  [  128/  306]
train() client id: f_00004-1-4 loss: 0.670740  [  160/  306]
train() client id: f_00004-1-5 loss: 0.778628  [  192/  306]
train() client id: f_00004-1-6 loss: 0.692502  [  224/  306]
train() client id: f_00004-1-7 loss: 0.773317  [  256/  306]
train() client id: f_00004-1-8 loss: 0.651433  [  288/  306]
train() client id: f_00004-2-0 loss: 0.681198  [   32/  306]
train() client id: f_00004-2-1 loss: 0.693578  [   64/  306]
train() client id: f_00004-2-2 loss: 0.855183  [   96/  306]
train() client id: f_00004-2-3 loss: 0.679228  [  128/  306]
train() client id: f_00004-2-4 loss: 0.722499  [  160/  306]
train() client id: f_00004-2-5 loss: 0.794896  [  192/  306]
train() client id: f_00004-2-6 loss: 0.727026  [  224/  306]
train() client id: f_00004-2-7 loss: 0.792954  [  256/  306]
train() client id: f_00004-2-8 loss: 0.815799  [  288/  306]
train() client id: f_00005-0-0 loss: 0.769391  [   32/  146]
train() client id: f_00005-0-1 loss: 0.965749  [   64/  146]
train() client id: f_00005-0-2 loss: 0.877137  [   96/  146]
train() client id: f_00005-0-3 loss: 0.709892  [  128/  146]
train() client id: f_00005-1-0 loss: 0.775442  [   32/  146]
train() client id: f_00005-1-1 loss: 0.698312  [   64/  146]
train() client id: f_00005-1-2 loss: 1.088447  [   96/  146]
train() client id: f_00005-1-3 loss: 0.904291  [  128/  146]
train() client id: f_00005-2-0 loss: 0.936925  [   32/  146]
train() client id: f_00005-2-1 loss: 0.865273  [   64/  146]
train() client id: f_00005-2-2 loss: 0.694061  [   96/  146]
train() client id: f_00005-2-3 loss: 0.948811  [  128/  146]
train() client id: f_00006-0-0 loss: 0.488354  [   32/   54]
train() client id: f_00006-1-0 loss: 0.605276  [   32/   54]
train() client id: f_00006-2-0 loss: 0.497171  [   32/   54]
train() client id: f_00007-0-0 loss: 0.661838  [   32/  179]
train() client id: f_00007-0-1 loss: 0.907735  [   64/  179]
train() client id: f_00007-0-2 loss: 0.700539  [   96/  179]
train() client id: f_00007-0-3 loss: 0.869432  [  128/  179]
train() client id: f_00007-0-4 loss: 0.579164  [  160/  179]
train() client id: f_00007-1-0 loss: 0.608882  [   32/  179]
train() client id: f_00007-1-1 loss: 0.784107  [   64/  179]
train() client id: f_00007-1-2 loss: 0.670646  [   96/  179]
train() client id: f_00007-1-3 loss: 0.621040  [  128/  179]
train() client id: f_00007-1-4 loss: 0.854834  [  160/  179]
train() client id: f_00007-2-0 loss: 0.693909  [   32/  179]
train() client id: f_00007-2-1 loss: 0.869799  [   64/  179]
train() client id: f_00007-2-2 loss: 0.641892  [   96/  179]
train() client id: f_00007-2-3 loss: 0.551257  [  128/  179]
train() client id: f_00007-2-4 loss: 0.776607  [  160/  179]
train() client id: f_00008-0-0 loss: 0.654151  [   32/  130]
train() client id: f_00008-0-1 loss: 0.708563  [   64/  130]
train() client id: f_00008-0-2 loss: 0.938276  [   96/  130]
train() client id: f_00008-0-3 loss: 0.792459  [  128/  130]
train() client id: f_00008-1-0 loss: 0.729369  [   32/  130]
train() client id: f_00008-1-1 loss: 0.853033  [   64/  130]
train() client id: f_00008-1-2 loss: 0.698621  [   96/  130]
train() client id: f_00008-1-3 loss: 0.806198  [  128/  130]
train() client id: f_00008-2-0 loss: 0.820890  [   32/  130]
train() client id: f_00008-2-1 loss: 0.706097  [   64/  130]
train() client id: f_00008-2-2 loss: 0.811529  [   96/  130]
train() client id: f_00008-2-3 loss: 0.752409  [  128/  130]
train() client id: f_00009-0-0 loss: 0.704092  [   32/  118]
train() client id: f_00009-0-1 loss: 0.810800  [   64/  118]
train() client id: f_00009-0-2 loss: 0.723425  [   96/  118]
train() client id: f_00009-1-0 loss: 0.673125  [   32/  118]
train() client id: f_00009-1-1 loss: 0.727300  [   64/  118]
train() client id: f_00009-1-2 loss: 0.794452  [   96/  118]
train() client id: f_00009-2-0 loss: 0.712870  [   32/  118]
train() client id: f_00009-2-1 loss: 0.750495  [   64/  118]
train() client id: f_00009-2-2 loss: 0.710066  [   96/  118]
At round 81 accuracy: 0.6445623342175066
At round 81 training accuracy: 0.5902079141515761
At round 81 training loss: 0.8279655474447347
update_location
xs = -3.905658 4.200318 425.009024 18.811294 0.979296 3.956410 -387.443192 -366.324852 409.663977 -352.060879 
ys = 417.587959 400.555839 1.320614 -387.455176 379.350187 362.814151 -2.624984 0.822348 17.569006 4.001482 
dists_uav = 429.412340 412.871194 436.617011 400.593782 392.310493 376.363868 400.148869 379.729606 422.058342 366.009391 
dists_bs = 296.853822 288.076709 624.640183 594.910617 269.680759 259.421572 276.916494 258.733557 605.488632 246.152489 
uav_gains = -122.393729 -121.857642 -122.613185 -121.425192 -121.113468 -120.458767 -121.408884 -120.603593 -122.161320 -119.987785 
bs_gains = -108.798253 -108.433287 -117.844697 -117.251707 -107.630858 -107.159230 -107.952826 -107.126936 -117.466027 -106.520778 
Round 82
-------------------------------
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.03972093 0.07667447 0.03927574 0.01549837 0.08835219 0.04258691
 0.01863559 0.05403005 0.03910533 0.03455187]
obj_prev = 0.44843144567355053
eta_min = 0.0	eta_max = inf
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.10079758902614538	eta = 0.9090909090909091
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.3375882886973583	eta = 0.2714376502678347
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.19414841544868844	eta = 0.47198001400206313
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.17188140883090305	eta = 0.5331243935293785
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.17048248758463935	eta = 0.5374990307814271
af = 0.09163417184195034	bf = 0.021956175032555886	zeta = 0.17047588964991173	eta = 0.5375198336265012
eta = 0.5375198336265012
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [0.05122607 0.10773737 0.05041296 0.01748191 0.12440617 0.05935719
 0.02195403 0.07277355 0.0528523  0.04797362]
ene_total = [0.01831375 0.02382031 0.0184197  0.01040877 0.02710943 0.01385921
 0.01130017 0.020882   0.01487805 0.0114845 ]
ti_comp = [127.24870803 127.54244623 127.23535515 127.30084038 127.54703912
 127.54954855 127.30163179 127.33761381 127.43428871 127.5527431 ]
ti_coms = [0.39027955 0.09654136 0.40363243 0.3381472  0.09194846 0.08943903
 0.3373558  0.30137378 0.20469887 0.08624449]
t_total = [25.89965591 25.89965591 25.89965591 25.89965591 25.89965591 25.89965591
 25.89965591 25.89965591 25.89965591 25.89965591]
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [5.18855480e-10 4.80473758e-09 4.94641987e-10 2.06055571e-11
 7.39716068e-09 8.03419283e-10 4.08088986e-11 1.48554980e-09
 5.68197076e-10 4.24138750e-10]
ene_total = [0.00791929 0.00195895 0.00819024 0.00686146 0.00186576 0.00181484
 0.0068454  0.00611528 0.00415361 0.00175002]
optimize_network iter = 0 obj = 0.04747485012945179
eta = 0.5375198336265012
freqs = [201283.26753271 422358.89209453 198109.08366522  68663.77799187
 487687.42285783 232682.88526082  86228.39443656 285750.39151138
 207370.78109251 188054.06037245]
eta_min = 0.537519833626502	eta_max = 0.9942699445819028
af = 2.5030539272411267e-09	bf = 0.021956175032555886	zeta = 2.7533593199652398e-09	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [7.96601739e-11 7.37674065e-10 7.59426628e-11 3.16358278e-12
 1.13569024e-09 1.23349415e-10 6.26541318e-12 2.28077296e-10
 8.72356169e-11 6.51182610e-11]
ene_total = [0.04028768 0.00996575 0.04166607 0.03490617 0.00949163 0.00923259
 0.03482448 0.03111014 0.0211306  0.00890282]
ti_comp = [24.69954088 24.99327907 24.686188   24.75167323 24.99787197 25.0003814
 24.75246463 24.78844665 24.88512156 25.00357594]
ti_coms = [0.39027955 0.09654136 0.40363243 0.3381472  0.09194846 0.08943903
 0.3373558  0.30137378 0.20469887 0.08624449]
t_total = [25.89965591 25.89965591 25.89965591 25.89965591 25.89965591 25.89965591
 25.89965591 25.89965591 25.89965591 25.89965591]
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [4.98379232e-11 4.52811249e-10 4.75535621e-11 1.97252403e-12
 6.96922143e-10 7.56817841e-11 3.90634363e-12 1.41868681e-10
 5.39233154e-11 3.99454976e-11]
ene_total = [0.04028768 0.00996575 0.04166607 0.03490617 0.00949163 0.00923259
 0.03482448 0.03111014 0.0211306  0.00890282]
optimize_network iter = 1 obj = 0.24151792697303298
eta = 0.909090909090909
freqs = [159208.78105798 330908.549687   156766.40742107  54218.71812527
 382035.51668375 182260.08745128  68086.44578208 225366.34992481
 163038.03730914 147287.27775164]
eta_min = 0.909090909090918	eta_max = 0.9684934797724077
af = 1.5403473503468153e-09	bf = 0.021956175032555886	zeta = 1.694382085381497e-09	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [4.98379232e-11 4.52811249e-10 4.75535621e-11 1.97252403e-12
 6.96922143e-10 7.56817841e-11 3.90634363e-12 1.41868681e-10
 5.39233154e-11 3.99454976e-11]
ene_total = [0.04028768 0.00996575 0.04166607 0.03490617 0.00949163 0.00923259
 0.03482448 0.03111014 0.0211306  0.00890282]
ti_comp = [24.69954088 24.99327907 24.686188   24.75167323 24.99787197 25.0003814
 24.75246463 24.78844665 24.88512156 25.00357594]
ti_coms = [0.39027955 0.09654136 0.40363243 0.3381472  0.09194846 0.08943903
 0.3373558  0.30137378 0.20469887 0.08624449]
t_total = [25.89965591 25.89965591 25.89965591 25.89965591 25.89965591 25.89965591
 25.89965591 25.89965591 25.89965591 25.89965591]
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [4.98379232e-11 4.52811249e-10 4.75535621e-11 1.97252403e-12
 6.96922143e-10 7.56817841e-11 3.90634363e-12 1.41868681e-10
 5.39233154e-11 3.99454976e-11]
ene_total = [0.04028768 0.00996575 0.04166607 0.03490617 0.00949163 0.00923259
 0.03482448 0.03111014 0.0211306  0.00890282]
optimize_network iter = 2 obj = 0.24151792697305713
eta = 0.909090909090918
freqs = [159208.78105798 330908.549687   156766.40742107  54218.71812527
 382035.51668375 182260.08745128  68086.44578208 225366.34992481
 163038.03730914 147287.27775163]
Done!
ene_coms = [0.03902795 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
ene_comp = [4.79066540e-11 4.35264361e-10 4.57108141e-11 1.89608676e-12
 6.69915714e-10 7.27490395e-11 3.75496892e-12 1.36371128e-10
 5.18337331e-11 3.83975697e-11]
ene_total = [0.03902796 0.00965414 0.04036324 0.03381472 0.00919485 0.0089439
 0.03373558 0.03013738 0.02046989 0.00862445]
At round 82 energy consumption: 0.23396609732358067
At round 82 eta: 0.909090909090918
At round 82 a_n: 0.09384340477035735
At round 82 local rounds: 3.1209395205774326
At round 82 global rounds: 1.0322774524740326
gradient difference: 0.8966941833496094
train() client id: f_00000-0-0 loss: 0.908660  [   32/  126]
train() client id: f_00000-0-1 loss: 1.103846  [   64/  126]
train() client id: f_00000-0-2 loss: 0.964462  [   96/  126]
train() client id: f_00000-1-0 loss: 0.889724  [   32/  126]
train() client id: f_00000-1-1 loss: 1.154601  [   64/  126]
train() client id: f_00000-1-2 loss: 1.012610  [   96/  126]
train() client id: f_00000-2-0 loss: 1.049376  [   32/  126]
train() client id: f_00000-2-1 loss: 1.045388  [   64/  126]
train() client id: f_00000-2-2 loss: 0.902614  [   96/  126]
train() client id: f_00001-0-0 loss: 0.405338  [   32/  265]
train() client id: f_00001-0-1 loss: 0.491843  [   64/  265]
train() client id: f_00001-0-2 loss: 0.420695  [   96/  265]
train() client id: f_00001-0-3 loss: 0.396203  [  128/  265]
train() client id: f_00001-0-4 loss: 0.406718  [  160/  265]
train() client id: f_00001-0-5 loss: 0.396771  [  192/  265]
train() client id: f_00001-0-6 loss: 0.375331  [  224/  265]
train() client id: f_00001-0-7 loss: 0.464294  [  256/  265]
train() client id: f_00001-1-0 loss: 0.463428  [   32/  265]
train() client id: f_00001-1-1 loss: 0.324342  [   64/  265]
train() client id: f_00001-1-2 loss: 0.359655  [   96/  265]
train() client id: f_00001-1-3 loss: 0.468472  [  128/  265]
train() client id: f_00001-1-4 loss: 0.362302  [  160/  265]
train() client id: f_00001-1-5 loss: 0.436385  [  192/  265]
train() client id: f_00001-1-6 loss: 0.508696  [  224/  265]
train() client id: f_00001-1-7 loss: 0.395405  [  256/  265]
train() client id: f_00001-2-0 loss: 0.406785  [   32/  265]
train() client id: f_00001-2-1 loss: 0.442897  [   64/  265]
train() client id: f_00001-2-2 loss: 0.337642  [   96/  265]
train() client id: f_00001-2-3 loss: 0.379169  [  128/  265]
train() client id: f_00001-2-4 loss: 0.388351  [  160/  265]
train() client id: f_00001-2-5 loss: 0.487105  [  192/  265]
train() client id: f_00001-2-6 loss: 0.461769  [  224/  265]
train() client id: f_00001-2-7 loss: 0.367277  [  256/  265]
train() client id: f_00002-0-0 loss: 0.859993  [   32/  124]
train() client id: f_00002-0-1 loss: 1.068360  [   64/  124]
train() client id: f_00002-0-2 loss: 0.889706  [   96/  124]
train() client id: f_00002-1-0 loss: 0.904216  [   32/  124]
train() client id: f_00002-1-1 loss: 1.008591  [   64/  124]
train() client id: f_00002-1-2 loss: 0.988087  [   96/  124]
train() client id: f_00002-2-0 loss: 0.710027  [   32/  124]
train() client id: f_00002-2-1 loss: 0.954396  [   64/  124]
train() client id: f_00002-2-2 loss: 0.934568  [   96/  124]
train() client id: f_00003-0-0 loss: 0.597287  [   32/   43]
train() client id: f_00003-1-0 loss: 0.504503  [   32/   43]
train() client id: f_00003-2-0 loss: 0.732586  [   32/   43]
train() client id: f_00004-0-0 loss: 0.719089  [   32/  306]
train() client id: f_00004-0-1 loss: 0.861654  [   64/  306]
train() client id: f_00004-0-2 loss: 0.683268  [   96/  306]
train() client id: f_00004-0-3 loss: 0.827092  [  128/  306]
train() client id: f_00004-0-4 loss: 0.812338  [  160/  306]
train() client id: f_00004-0-5 loss: 0.829534  [  192/  306]
train() client id: f_00004-0-6 loss: 0.740728  [  224/  306]
train() client id: f_00004-0-7 loss: 0.782740  [  256/  306]
train() client id: f_00004-0-8 loss: 0.827559  [  288/  306]
train() client id: f_00004-1-0 loss: 0.805483  [   32/  306]
train() client id: f_00004-1-1 loss: 0.885086  [   64/  306]
train() client id: f_00004-1-2 loss: 0.712325  [   96/  306]
train() client id: f_00004-1-3 loss: 0.770210  [  128/  306]
train() client id: f_00004-1-4 loss: 0.745802  [  160/  306]
train() client id: f_00004-1-5 loss: 0.806845  [  192/  306]
train() client id: f_00004-1-6 loss: 0.824898  [  224/  306]
train() client id: f_00004-1-7 loss: 0.825535  [  256/  306]
train() client id: f_00004-1-8 loss: 0.784513  [  288/  306]
train() client id: f_00004-2-0 loss: 0.772237  [   32/  306]
train() client id: f_00004-2-1 loss: 0.752592  [   64/  306]
train() client id: f_00004-2-2 loss: 0.861572  [   96/  306]
train() client id: f_00004-2-3 loss: 0.742620  [  128/  306]
train() client id: f_00004-2-4 loss: 0.897250  [  160/  306]
train() client id: f_00004-2-5 loss: 0.810101  [  192/  306]
train() client id: f_00004-2-6 loss: 0.705095  [  224/  306]
train() client id: f_00004-2-7 loss: 0.664796  [  256/  306]
train() client id: f_00004-2-8 loss: 0.924697  [  288/  306]
train() client id: f_00005-0-0 loss: 0.456159  [   32/  146]
train() client id: f_00005-0-1 loss: 0.590559  [   64/  146]
train() client id: f_00005-0-2 loss: 0.588697  [   96/  146]
train() client id: f_00005-0-3 loss: 0.442790  [  128/  146]
train() client id: f_00005-1-0 loss: 0.607556  [   32/  146]
train() client id: f_00005-1-1 loss: 0.558751  [   64/  146]
train() client id: f_00005-1-2 loss: 0.574550  [   96/  146]
train() client id: f_00005-1-3 loss: 0.349379  [  128/  146]
train() client id: f_00005-2-0 loss: 0.339354  [   32/  146]
train() client id: f_00005-2-1 loss: 0.458403  [   64/  146]
train() client id: f_00005-2-2 loss: 0.618539  [   96/  146]
train() client id: f_00005-2-3 loss: 0.679715  [  128/  146]
train() client id: f_00006-0-0 loss: 0.497590  [   32/   54]
train() client id: f_00006-1-0 loss: 0.585048  [   32/   54]
train() client id: f_00006-2-0 loss: 0.516747  [   32/   54]
train() client id: f_00007-0-0 loss: 0.640365  [   32/  179]
train() client id: f_00007-0-1 loss: 0.831569  [   64/  179]
train() client id: f_00007-0-2 loss: 0.581578  [   96/  179]
train() client id: f_00007-0-3 loss: 0.550209  [  128/  179]
train() client id: f_00007-0-4 loss: 0.683766  [  160/  179]
train() client id: f_00007-1-0 loss: 0.588320  [   32/  179]
train() client id: f_00007-1-1 loss: 0.731718  [   64/  179]
train() client id: f_00007-1-2 loss: 0.747146  [   96/  179]
train() client id: f_00007-1-3 loss: 0.721784  [  128/  179]
train() client id: f_00007-1-4 loss: 0.472654  [  160/  179]
train() client id: f_00007-2-0 loss: 0.457143  [   32/  179]
train() client id: f_00007-2-1 loss: 0.694946  [   64/  179]
train() client id: f_00007-2-2 loss: 0.571226  [   96/  179]
train() client id: f_00007-2-3 loss: 0.908408  [  128/  179]
train() client id: f_00007-2-4 loss: 0.615300  [  160/  179]
train() client id: f_00008-0-0 loss: 0.833186  [   32/  130]
train() client id: f_00008-0-1 loss: 0.792004  [   64/  130]
train() client id: f_00008-0-2 loss: 0.757524  [   96/  130]
train() client id: f_00008-0-3 loss: 0.681513  [  128/  130]
train() client id: f_00008-1-0 loss: 0.748403  [   32/  130]
train() client id: f_00008-1-1 loss: 0.803484  [   64/  130]
train() client id: f_00008-1-2 loss: 0.881175  [   96/  130]
train() client id: f_00008-1-3 loss: 0.610586  [  128/  130]
train() client id: f_00008-2-0 loss: 0.669719  [   32/  130]
train() client id: f_00008-2-1 loss: 0.831953  [   64/  130]
train() client id: f_00008-2-2 loss: 0.854853  [   96/  130]
train() client id: f_00008-2-3 loss: 0.672730  [  128/  130]
train() client id: f_00009-0-0 loss: 0.837567  [   32/  118]
train() client id: f_00009-0-1 loss: 0.712474  [   64/  118]
train() client id: f_00009-0-2 loss: 0.898162  [   96/  118]
train() client id: f_00009-1-0 loss: 0.893536  [   32/  118]
train() client id: f_00009-1-1 loss: 0.794552  [   64/  118]
train() client id: f_00009-1-2 loss: 0.699519  [   96/  118]
train() client id: f_00009-2-0 loss: 0.783458  [   32/  118]
train() client id: f_00009-2-1 loss: 0.747928  [   64/  118]
train() client id: f_00009-2-2 loss: 0.761644  [   96/  118]
At round 82 accuracy: 0.6445623342175066
At round 82 training accuracy: 0.5895372233400402
At round 82 training loss: 0.8131053164821196
Done!
