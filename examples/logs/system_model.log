v = 22.697160301531774
a_0 = 27.840057009285058	a_alpha = -0.3425458469693173
['f_00000', 'f_00001', 'f_00002', 'f_00003', 'f_00004', 'f_00005', 'f_00006', 'f_00007', 'f_00008', 'f_00009']
10
dict_keys(['x', 'y'])
id = f_00000, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 126
id = f_00001, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 265
id = f_00002, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 124
id = f_00003, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 43
id = f_00004, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 306
id = f_00005, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 146
id = f_00006, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 54
id = f_00007, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 179
id = f_00008, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 130
id = f_00009, model = CustomLogisticRegression(
  (linear): Linear(in_features=5, out_features=3, bias=True)
), num_samples = 118
BaseFederated generated!
num_samples = [126 265 124  43 306 146  54 179 130 118]
msize = 502400
xs = 30.471708 -103.998411 75.045120 94.056472 -195.103519 -130.217951 12.784040 -31.624259 -1.680116 -85.304393 
ys = 17.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 36.875078 -19.177652 87.845030 -0.998518 
xs mean: -33.557130840587135
ys mean: 15.371751218646875
dists_uav = 106.008779 145.112555 125.034051 139.121849 219.437483 165.082004 107.346183 106.620242 133.114883 131.445184 
uav_gains = -100.633583 -104.045555 -102.426089 -103.586543 -108.951552 -105.458224 -100.769709 -100.696032 -103.106573 -102.969380 
uav_gains_db_mean: -103.26432393077694
dists_bs = 258.838122 174.538438 304.445547 309.247418 166.865251 197.334317 233.112290 241.374323 193.999410 197.536782 
bs_gains = -107.131850 -102.340010 -109.105329 -109.295629 -101.793305 -103.832732 -105.858884 -106.282409 -103.625471 -103.845202 
bs_gains_db_mean: -105.31108204686706
SystemModel __init__!
t_co_uav = [0.0647258  0.07570307 0.07007063 0.07401864 0.09974635 0.08139011
 0.06510331 0.06489845 0.07233403 0.07186623]
t_co_bs = [0.08929738 0.06979419 0.10073636 0.10198748 0.06808736 0.07491286
 0.08315713 0.08510747 0.07415864 0.07495871]
difference = [-0.02457158  0.00590888 -0.03066574 -0.02796884  0.03165899  0.00647726
 -0.01805382 -0.02020902 -0.00182461 -0.00309248]
decs_opt = [1 0 1 1 0 0 1 1 0 0]
af = 6.796163711028166	bf = 19.508323258209305	zeta = 7.475780082130983	eta = 0.9090909090909091
af = 6.796163711028166	bf = 19.508323258209305	zeta = 221.71673527835725	eta = 0.030652461585706788
af = 6.796163711028166	bf = 19.508323258209305	zeta = 44.559087020146116	eta = 0.1525202638904041
af = 6.796163711028166	bf = 19.508323258209305	zeta = 38.09910849390317	eta = 0.1783811742501987
af = 6.796163711028166	bf = 19.508323258209305	zeta = 38.002746250530386	eta = 0.1788334891964108
af = 6.796163711028166	bf = 19.508323258209305	zeta = 38.00271973380747	eta = 0.17883361397900832
eta_opt = 0.17883361397900832
initialize_feasible_solution eta = 0.17883361397900832, tau = 10.000839233398438
ti_comp = [0.03550938 0.07468242 0.03494574 0.01211828 0.08623706 0.04114578
 0.0152183  0.05044586 0.03663666 0.03325481]
ti_coms = [0.0647258  0.06979419 0.07007063 0.07401864 0.06808736 0.07491286
 0.06510331 0.06489845 0.07415864 0.07495871]
t_total = [3.39827961 4.8981997  3.56037645 2.92030544 5.23207052 3.9347436
 2.72314879 3.91052542 3.75630006 3.66877014]
system_model train() tau = 30	t0 = 0.050004196166992185	t_min = 10.000839233398438
Round 0
-------------------------------
ene_coms = [0.00647258 0.00697942 0.00700706 0.00740186 0.00680874 0.00749129
 0.00651033 0.00648984 0.00741586 0.00749587]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.86831354 22.67141725 10.71372012  3.85506047 26.14390434 12.59319836
  4.76292007 15.36382847 11.23408738 10.21859478]
obj_prev = 128.42504477808617
eta_min = 1.7746825162769225e-09	eta_max = 0.9288432929941559
af = 27.184654844112664	bf = 1.9508323258209304	zeta = 29.903120328523933	eta = 0.9090909090909091
af = 27.184654844112664	bf = 1.9508323258209304	zeta = 49.95987333625001	eta = 0.5441297791359281
af = 27.184654844112664	bf = 1.9508323258209304	zeta = 40.56972534552157	eta = 0.6700724397956404
af = 27.184654844112664	bf = 1.9508323258209304	zeta = 38.90168410648359	eta = 0.6988040612766659
af = 27.184654844112664	bf = 1.9508323258209304	zeta = 38.82323872924005	eta = 0.7002160493024069
af = 27.184654844112664	bf = 1.9508323258209304	zeta = 38.82305411394295	eta = 0.7002193790402889
eta = 0.7002193790402889
ene_coms = [0.00647258 0.00697942 0.00700706 0.00740186 0.00680874 0.00749129
 0.00651033 0.00648984 0.00741586 0.00749587]
ene_comp = [0.0294061  0.06184617 0.02893934 0.01003542 0.07141483 0.03407374
 0.01260262 0.04177534 0.03033963 0.02753905]
ene_total = [3.3319853  6.39170196 3.33827424 1.61936712 7.26447359 3.86006503
 1.7749831  4.48229595 3.50628118 3.25362665]
ti_comp = [0.258313   0.25324461 0.25296818 0.24902016 0.25495144 0.24812594
 0.25793549 0.25814036 0.24888016 0.24808009]
ti_coms = [0.0647258  0.06979419 0.07007063 0.07401864 0.06808736 0.07491286
 0.06510331 0.06489845 0.07415864 0.07495871]
t_total = [30. 30. 30. 30. 30. 30. 30. 30. 30. 30.]
ene_coms = [0.00647258 0.00697942 0.00700706 0.00740186 0.00680874 0.00749129
 0.00651033 0.00648984 0.00741586 0.00749587]
ene_comp = [2.38177087e-05 2.30535636e-04 2.36708665e-05 1.01863177e-06
 3.50211339e-04 4.01601281e-05 1.88035540e-06 6.83798529e-05
 2.81793109e-05 2.12101180e-05]
ene_total = [0.60330811 0.66957478 0.65293086 0.68749163 0.66483781 0.69943107
 0.60477668 0.60904985 0.69131415 0.69809708]
optimize_network iter = 0 obj = 6.580812011011883
eta = 0.7002193790402889
freqs = [5.69195210e+07 1.22107578e+08 5.71995687e+07 2.01498074e+07
 1.40055738e+08 6.86621876e+07 2.44297830e+07 8.09159402e+07
 6.09522901e+07 5.55043547e+07]
eta_min = 0.6458594450554286	eta_max = 0.7002193790402862
af = 0.06164408736519128	bf = 1.9508323258209304	zeta = 0.06780849610171041	eta = 0.9090909090909091
af = 0.06164408736519128	bf = 1.9508323258209304	zeta = 21.523783983587375	eta = 0.002863998607874759
af = 0.06164408736519128	bf = 1.9508323258209304	zeta = 2.3184315150123527	eta = 0.026588703166788533
af = 0.06164408736519128	bf = 1.9508323258209304	zeta = 2.23382655467249	eta = 0.02759573577288285
af = 0.06164408736519128	bf = 1.9508323258209304	zeta = 2.2337832885943225	eta = 0.027596270273819952
eta = 0.027596270273819952
ene_coms = [0.00647258 0.00697942 0.00700706 0.00740186 0.00680874 0.00749129
 0.00651033 0.00648984 0.00741586 0.00749587]
ene_comp = [2.39945475e-04 2.32247289e-03 2.38466151e-04 1.02619478e-05
 3.52811545e-03 4.04583041e-04 1.89431644e-05 6.88875514e-04
 2.83885332e-04 2.13675963e-04]
ene_total = [0.19218056 0.26631448 0.20744052 0.21221021 0.29594552 0.22605984
 0.18693404 0.20552778 0.22044491 0.22072543]
ti_comp = [0.3168904  0.31182201 0.31154557 0.30759756 0.31352884 0.30670334
 0.31651289 0.31671775 0.30745756 0.30665748]
ti_coms = [0.0647258  0.06979419 0.07007063 0.07401864 0.06808736 0.07491286
 0.06510331 0.06489845 0.07415864 0.07495871]
t_total = [30. 30. 30. 30. 30. 30. 30. 30. 30. 30.]
ene_coms = [0.00647258 0.00697942 0.00700706 0.00740186 0.00680874 0.00749129
 0.00651033 0.00648984 0.00741586 0.00749587]
ene_comp = [2.92188280e-05 2.80732962e-04 2.88131810e-05 1.23256239e-06
 4.27541821e-04 4.85278299e-05 2.30551416e-06 8.38655497e-05
 3.40901587e-05 2.56276156e-05]
ene_total = [0.51112601 0.57074243 0.55311141 0.58197973 0.5688656  0.59272747
 0.51197798 0.51677918 0.58566335 0.59128771]
optimize_network iter = 1 obj = 5.584260849865536
eta = 0.6458594450554286
freqs = [5.69195210e+07 1.21657498e+08 5.69770387e+07 2.00117626e+07
 1.39715214e+08 6.81450195e+07 2.44231755e+07 8.09059378e+07
 6.05282264e+07 5.50843476e+07]
eta_min = 0.6458594450554351	eta_max = 0.6560705338873148
af = 0.06125924396244557	bf = 1.9508323258209304	zeta = 0.06738516835869013	eta = 0.9090909090909091
af = 0.06125924396244557	bf = 1.9508323258209304	zeta = 21.52338050915434	eta = 0.002846172047016069
af = 0.06125924396244557	bf = 1.9508323258209304	zeta = 2.3165137537370644	eta = 0.026444584610654895
af = 0.06125924396244557	bf = 1.9508323258209304	zeta = 2.2324040160757788	eta = 0.027440930728179685
af = 0.06125924396244557	bf = 1.9508323258209304	zeta = 2.2323614637336595	eta = 0.02744145379574351
eta = 0.02744145379574351
ene_coms = [0.00647258 0.00697942 0.00700706 0.00740186 0.00680874 0.00749129
 0.00651033 0.00648984 0.00741586 0.00749587]
ene_comp = [2.40321483e-04 2.30899617e-03 2.36985084e-04 1.01376832e-05
 3.51648207e-03 3.99135792e-04 1.89625876e-05 6.89784454e-04
 2.80387616e-04 2.10784177e-04]
ene_total = [0.19216073 0.26588631 0.2073651  0.21217287 0.29556539 0.22586793
 0.18690484 0.20552108 0.2203097  0.22060752]
ti_comp = [0.3168904  0.31182201 0.31154557 0.30759756 0.31352884 0.30670334
 0.31651289 0.31671775 0.30745756 0.30665748]
ti_coms = [0.0647258  0.06979419 0.07007063 0.07401864 0.06808736 0.07491286
 0.06510331 0.06489845 0.07415864 0.07495871]
t_total = [30. 30. 30. 30. 30. 30. 30. 30. 30. 30.]
ene_coms = [0.00647258 0.00697942 0.00700706 0.00740186 0.00680874 0.00749129
 0.00651033 0.00648984 0.00741586 0.00749587]
ene_comp = [2.92188280e-05 2.80732962e-04 2.88131810e-05 1.23256239e-06
 4.27541821e-04 4.85278299e-05 2.30551416e-06 8.38655497e-05
 3.40901587e-05 2.56276156e-05]
ene_total = [0.51112601 0.57074243 0.55311141 0.58197973 0.5688656  0.59272747
 0.51197798 0.51677918 0.58566335 0.59128771]
optimize_network iter = 2 obj = 5.584260849865636
eta = 0.6458594450554351
freqs = [5.69195210e+07 1.21657498e+08 5.69770387e+07 2.00117626e+07
 1.39715214e+08 6.81450195e+07 2.44231755e+07 8.09059378e+07
 6.05282264e+07 5.50843476e+07]
Done!
ene_coms = [0.00647258 0.00697942 0.00700706 0.00740186 0.00680874 0.00749129
 0.00651033 0.00648984 0.00741586 0.00749587]
ene_comp = [2.85753171e-05 2.74550143e-04 2.81786039e-05 1.20541663e-06
 4.18125707e-04 4.74590605e-05 2.25473788e-06 8.20185079e-05
 3.33393623e-05 2.50631970e-05]
ene_total = [0.00650116 0.00725397 0.00703524 0.00740307 0.00722686 0.00753874
 0.00651259 0.00657186 0.0074492  0.00752093]
At round 0 energy consumption: 0.07101362677024886
At round 0 eta: 0.6458594450554351
At round 0 a_n: 27.840057009285058
At round 0 local rounds: 14.315277443458161
At round 0 global rounds: 78.61301570966076
gradient difference: 0.739060640335083
train() client id: f_00000-0-0 loss: 0.776318  [   32/  126]
train() client id: f_00000-0-1 loss: 0.734197  [   64/  126]
train() client id: f_00000-0-2 loss: 1.078415  [   96/  126]
train() client id: f_00000-1-0 loss: 0.961806  [   32/  126]
train() client id: f_00000-1-1 loss: 0.806883  [   64/  126]
train() client id: f_00000-1-2 loss: 0.885127  [   96/  126]
train() client id: f_00000-2-0 loss: 0.731882  [   32/  126]
train() client id: f_00000-2-1 loss: 1.010984  [   64/  126]
train() client id: f_00000-2-2 loss: 0.869800  [   96/  126]
train() client id: f_00000-3-0 loss: 1.084046  [   32/  126]
train() client id: f_00000-3-1 loss: 0.887008  [   64/  126]
train() client id: f_00000-3-2 loss: 0.799992  [   96/  126]
train() client id: f_00000-4-0 loss: 0.755446  [   32/  126]
train() client id: f_00000-4-1 loss: 1.051537  [   64/  126]
train() client id: f_00000-4-2 loss: 0.996704  [   96/  126]
train() client id: f_00000-5-0 loss: 0.909340  [   32/  126]
train() client id: f_00000-5-1 loss: 0.907289  [   64/  126]
train() client id: f_00000-5-2 loss: 1.186484  [   96/  126]
train() client id: f_00000-6-0 loss: 1.052341  [   32/  126]
train() client id: f_00000-6-1 loss: 0.770762  [   64/  126]
train() client id: f_00000-6-2 loss: 0.988505  [   96/  126]
train() client id: f_00000-7-0 loss: 1.021166  [   32/  126]
train() client id: f_00000-7-1 loss: 0.932684  [   64/  126]
train() client id: f_00000-7-2 loss: 0.884740  [   96/  126]
train() client id: f_00000-8-0 loss: 1.110381  [   32/  126]
train() client id: f_00000-8-1 loss: 1.003030  [   64/  126]
train() client id: f_00000-8-2 loss: 0.753906  [   96/  126]
train() client id: f_00000-9-0 loss: 0.989008  [   32/  126]
train() client id: f_00000-9-1 loss: 0.986610  [   64/  126]
train() client id: f_00000-9-2 loss: 0.902543  [   96/  126]
train() client id: f_00000-10-0 loss: 1.015372  [   32/  126]
train() client id: f_00000-10-1 loss: 0.856315  [   64/  126]
train() client id: f_00000-10-2 loss: 1.057272  [   96/  126]
train() client id: f_00000-11-0 loss: 1.083636  [   32/  126]
train() client id: f_00000-11-1 loss: 0.819736  [   64/  126]
train() client id: f_00000-11-2 loss: 1.074669  [   96/  126]
train() client id: f_00000-12-0 loss: 0.938212  [   32/  126]
train() client id: f_00000-12-1 loss: 0.938826  [   64/  126]
train() client id: f_00000-12-2 loss: 1.103680  [   96/  126]
train() client id: f_00000-13-0 loss: 0.968587  [   32/  126]
train() client id: f_00000-13-1 loss: 0.940256  [   64/  126]
train() client id: f_00000-13-2 loss: 0.983129  [   96/  126]
train() client id: f_00001-0-0 loss: 0.891398  [   32/  265]
train() client id: f_00001-0-1 loss: 0.892120  [   64/  265]
train() client id: f_00001-0-2 loss: 0.841503  [   96/  265]
train() client id: f_00001-0-3 loss: 0.865313  [  128/  265]
train() client id: f_00001-0-4 loss: 0.856796  [  160/  265]
train() client id: f_00001-0-5 loss: 0.842981  [  192/  265]
train() client id: f_00001-0-6 loss: 0.814738  [  224/  265]
train() client id: f_00001-0-7 loss: 0.909207  [  256/  265]
train() client id: f_00001-1-0 loss: 0.829845  [   32/  265]
train() client id: f_00001-1-1 loss: 0.962319  [   64/  265]
train() client id: f_00001-1-2 loss: 0.841002  [   96/  265]
train() client id: f_00001-1-3 loss: 0.799056  [  128/  265]
train() client id: f_00001-1-4 loss: 0.900118  [  160/  265]
train() client id: f_00001-1-5 loss: 0.826584  [  192/  265]
train() client id: f_00001-1-6 loss: 0.864491  [  224/  265]
train() client id: f_00001-1-7 loss: 0.846106  [  256/  265]
train() client id: f_00001-2-0 loss: 0.888831  [   32/  265]
train() client id: f_00001-2-1 loss: 0.856962  [   64/  265]
train() client id: f_00001-2-2 loss: 0.951987  [   96/  265]
train() client id: f_00001-2-3 loss: 0.806742  [  128/  265]
train() client id: f_00001-2-4 loss: 0.911733  [  160/  265]
train() client id: f_00001-2-5 loss: 0.866802  [  192/  265]
train() client id: f_00001-2-6 loss: 0.936516  [  224/  265]
train() client id: f_00001-2-7 loss: 0.762760  [  256/  265]
train() client id: f_00001-3-0 loss: 0.905729  [   32/  265]
train() client id: f_00001-3-1 loss: 0.841509  [   64/  265]
train() client id: f_00001-3-2 loss: 0.900221  [   96/  265]
train() client id: f_00001-3-3 loss: 0.889328  [  128/  265]
train() client id: f_00001-3-4 loss: 0.919516  [  160/  265]
train() client id: f_00001-3-5 loss: 0.873417  [  192/  265]
train() client id: f_00001-3-6 loss: 0.870467  [  224/  265]
train() client id: f_00001-3-7 loss: 0.843330  [  256/  265]
train() client id: f_00001-4-0 loss: 0.825032  [   32/  265]
train() client id: f_00001-4-1 loss: 0.942103  [   64/  265]
train() client id: f_00001-4-2 loss: 0.816580  [   96/  265]
train() client id: f_00001-4-3 loss: 0.928159  [  128/  265]
train() client id: f_00001-4-4 loss: 0.879117  [  160/  265]
train() client id: f_00001-4-5 loss: 0.892336  [  192/  265]
train() client id: f_00001-4-6 loss: 0.876086  [  224/  265]
train() client id: f_00001-4-7 loss: 0.950818  [  256/  265]
train() client id: f_00001-5-0 loss: 0.922413  [   32/  265]
train() client id: f_00001-5-1 loss: 0.908459  [   64/  265]
train() client id: f_00001-5-2 loss: 0.934395  [   96/  265]
train() client id: f_00001-5-3 loss: 0.945499  [  128/  265]
train() client id: f_00001-5-4 loss: 0.824616  [  160/  265]
train() client id: f_00001-5-5 loss: 0.870790  [  192/  265]
train() client id: f_00001-5-6 loss: 0.881961  [  224/  265]
train() client id: f_00001-5-7 loss: 0.860202  [  256/  265]
train() client id: f_00001-6-0 loss: 0.976092  [   32/  265]
train() client id: f_00001-6-1 loss: 0.855989  [   64/  265]
train() client id: f_00001-6-2 loss: 0.907639  [   96/  265]
train() client id: f_00001-6-3 loss: 0.895863  [  128/  265]
train() client id: f_00001-6-4 loss: 0.878366  [  160/  265]
train() client id: f_00001-6-5 loss: 0.981883  [  192/  265]
train() client id: f_00001-6-6 loss: 0.898497  [  224/  265]
train() client id: f_00001-6-7 loss: 0.842982  [  256/  265]
train() client id: f_00001-7-0 loss: 0.835445  [   32/  265]
train() client id: f_00001-7-1 loss: 0.871672  [   64/  265]
train() client id: f_00001-7-2 loss: 0.892147  [   96/  265]
train() client id: f_00001-7-3 loss: 0.848861  [  128/  265]
train() client id: f_00001-7-4 loss: 1.032947  [  160/  265]
train() client id: f_00001-7-5 loss: 1.003963  [  192/  265]
train() client id: f_00001-7-6 loss: 0.909252  [  224/  265]
train() client id: f_00001-7-7 loss: 0.920502  [  256/  265]
train() client id: f_00001-8-0 loss: 0.924798  [   32/  265]
train() client id: f_00001-8-1 loss: 1.038825  [   64/  265]
train() client id: f_00001-8-2 loss: 0.929829  [   96/  265]
train() client id: f_00001-8-3 loss: 0.913688  [  128/  265]
train() client id: f_00001-8-4 loss: 0.856595  [  160/  265]
train() client id: f_00001-8-5 loss: 0.934964  [  192/  265]
train() client id: f_00001-8-6 loss: 0.923832  [  224/  265]
train() client id: f_00001-8-7 loss: 0.864808  [  256/  265]
train() client id: f_00001-9-0 loss: 0.995904  [   32/  265]
train() client id: f_00001-9-1 loss: 0.843723  [   64/  265]
train() client id: f_00001-9-2 loss: 1.022415  [   96/  265]
train() client id: f_00001-9-3 loss: 0.981085  [  128/  265]
train() client id: f_00001-9-4 loss: 0.875659  [  160/  265]
train() client id: f_00001-9-5 loss: 0.950532  [  192/  265]
train() client id: f_00001-9-6 loss: 0.912891  [  224/  265]
train() client id: f_00001-9-7 loss: 0.881926  [  256/  265]
train() client id: f_00001-10-0 loss: 0.862979  [   32/  265]
train() client id: f_00001-10-1 loss: 1.013350  [   64/  265]
train() client id: f_00001-10-2 loss: 0.941569  [   96/  265]
train() client id: f_00001-10-3 loss: 0.954353  [  128/  265]
train() client id: f_00001-10-4 loss: 0.882056  [  160/  265]
train() client id: f_00001-10-5 loss: 0.945997  [  192/  265]
train() client id: f_00001-10-6 loss: 0.957992  [  224/  265]
train() client id: f_00001-10-7 loss: 0.935853  [  256/  265]
train() client id: f_00001-11-0 loss: 1.068319  [   32/  265]
train() client id: f_00001-11-1 loss: 0.924106  [   64/  265]
train() client id: f_00001-11-2 loss: 0.910882  [   96/  265]
train() client id: f_00001-11-3 loss: 0.982141  [  128/  265]
train() client id: f_00001-11-4 loss: 0.972404  [  160/  265]
train() client id: f_00001-11-5 loss: 0.969762  [  192/  265]
train() client id: f_00001-11-6 loss: 0.916120  [  224/  265]
train() client id: f_00001-11-7 loss: 0.918189  [  256/  265]
train() client id: f_00001-12-0 loss: 0.916706  [   32/  265]
train() client id: f_00001-12-1 loss: 0.990646  [   64/  265]
train() client id: f_00001-12-2 loss: 0.996246  [   96/  265]
train() client id: f_00001-12-3 loss: 1.040164  [  128/  265]
train() client id: f_00001-12-4 loss: 0.901719  [  160/  265]
train() client id: f_00001-12-5 loss: 0.913607  [  192/  265]
train() client id: f_00001-12-6 loss: 1.039376  [  224/  265]
train() client id: f_00001-12-7 loss: 0.901451  [  256/  265]
train() client id: f_00001-13-0 loss: 0.896464  [   32/  265]
train() client id: f_00001-13-1 loss: 0.983961  [   64/  265]
train() client id: f_00001-13-2 loss: 0.904180  [   96/  265]
train() client id: f_00001-13-3 loss: 0.921513  [  128/  265]
train() client id: f_00001-13-4 loss: 0.992997  [  160/  265]
train() client id: f_00001-13-5 loss: 0.932185  [  192/  265]
train() client id: f_00001-13-6 loss: 1.005626  [  224/  265]
train() client id: f_00001-13-7 loss: 1.100457  [  256/  265]
train() client id: f_00002-0-0 loss: 1.006616  [   32/  124]
train() client id: f_00002-0-1 loss: 0.852213  [   64/  124]
train() client id: f_00002-0-2 loss: 0.823535  [   96/  124]
train() client id: f_00002-1-0 loss: 0.796248  [   32/  124]
train() client id: f_00002-1-1 loss: 0.829493  [   64/  124]
train() client id: f_00002-1-2 loss: 0.896984  [   96/  124]
train() client id: f_00002-2-0 loss: 0.882700  [   32/  124]
train() client id: f_00002-2-1 loss: 0.939761  [   64/  124]
train() client id: f_00002-2-2 loss: 0.843694  [   96/  124]
train() client id: f_00002-3-0 loss: 0.941880  [   32/  124]
train() client id: f_00002-3-1 loss: 0.778344  [   64/  124]
train() client id: f_00002-3-2 loss: 0.925550  [   96/  124]
train() client id: f_00002-4-0 loss: 0.910937  [   32/  124]
train() client id: f_00002-4-1 loss: 0.835544  [   64/  124]
train() client id: f_00002-4-2 loss: 0.853339  [   96/  124]
train() client id: f_00002-5-0 loss: 0.720053  [   32/  124]
train() client id: f_00002-5-1 loss: 0.870815  [   64/  124]
train() client id: f_00002-5-2 loss: 1.118204  [   96/  124]
train() client id: f_00002-6-0 loss: 0.903274  [   32/  124]
train() client id: f_00002-6-1 loss: 0.731969  [   64/  124]
train() client id: f_00002-6-2 loss: 0.908601  [   96/  124]
train() client id: f_00002-7-0 loss: 1.210378  [   32/  124]
train() client id: f_00002-7-1 loss: 0.775135  [   64/  124]
train() client id: f_00002-7-2 loss: 0.786357  [   96/  124]
train() client id: f_00002-8-0 loss: 0.977481  [   32/  124]
train() client id: f_00002-8-1 loss: 0.821844  [   64/  124]
train() client id: f_00002-8-2 loss: 0.943633  [   96/  124]
train() client id: f_00002-9-0 loss: 0.746250  [   32/  124]
train() client id: f_00002-9-1 loss: 1.063908  [   64/  124]
train() client id: f_00002-9-2 loss: 0.946531  [   96/  124]
train() client id: f_00002-10-0 loss: 1.022944  [   32/  124]
train() client id: f_00002-10-1 loss: 0.800151  [   64/  124]
train() client id: f_00002-10-2 loss: 0.779321  [   96/  124]
train() client id: f_00002-11-0 loss: 0.886061  [   32/  124]
train() client id: f_00002-11-1 loss: 1.043156  [   64/  124]
train() client id: f_00002-11-2 loss: 0.877620  [   96/  124]
train() client id: f_00002-12-0 loss: 0.812325  [   32/  124]
train() client id: f_00002-12-1 loss: 1.030599  [   64/  124]
train() client id: f_00002-12-2 loss: 0.911280  [   96/  124]
train() client id: f_00002-13-0 loss: 0.867659  [   32/  124]
train() client id: f_00002-13-1 loss: 0.725460  [   64/  124]
train() client id: f_00002-13-2 loss: 1.144729  [   96/  124]
train() client id: f_00003-0-0 loss: 1.082486  [   32/   43]
train() client id: f_00003-1-0 loss: 1.106404  [   32/   43]
train() client id: f_00003-2-0 loss: 1.054917  [   32/   43]
train() client id: f_00003-3-0 loss: 1.071481  [   32/   43]
train() client id: f_00003-4-0 loss: 1.011170  [   32/   43]
train() client id: f_00003-5-0 loss: 1.011104  [   32/   43]
train() client id: f_00003-6-0 loss: 0.990387  [   32/   43]
train() client id: f_00003-7-0 loss: 1.093522  [   32/   43]
train() client id: f_00003-8-0 loss: 1.011957  [   32/   43]
train() client id: f_00003-9-0 loss: 1.031807  [   32/   43]
train() client id: f_00003-10-0 loss: 0.979324  [   32/   43]
train() client id: f_00003-11-0 loss: 1.074570  [   32/   43]
train() client id: f_00003-12-0 loss: 1.044662  [   32/   43]
train() client id: f_00003-13-0 loss: 1.060082  [   32/   43]
train() client id: f_00004-0-0 loss: 0.996955  [   32/  306]
train() client id: f_00004-0-1 loss: 0.976700  [   64/  306]
train() client id: f_00004-0-2 loss: 0.913842  [   96/  306]
train() client id: f_00004-0-3 loss: 0.933904  [  128/  306]
train() client id: f_00004-0-4 loss: 0.986416  [  160/  306]
train() client id: f_00004-0-5 loss: 0.933853  [  192/  306]
train() client id: f_00004-0-6 loss: 1.028947  [  224/  306]
train() client id: f_00004-0-7 loss: 0.876375  [  256/  306]
train() client id: f_00004-0-8 loss: 0.896032  [  288/  306]
train() client id: f_00004-1-0 loss: 0.819643  [   32/  306]
train() client id: f_00004-1-1 loss: 1.082145  [   64/  306]
train() client id: f_00004-1-2 loss: 0.987693  [   96/  306]
train() client id: f_00004-1-3 loss: 0.895808  [  128/  306]
train() client id: f_00004-1-4 loss: 0.863457  [  160/  306]
train() client id: f_00004-1-5 loss: 0.877279  [  192/  306]
train() client id: f_00004-1-6 loss: 0.980493  [  224/  306]
train() client id: f_00004-1-7 loss: 0.978757  [  256/  306]
train() client id: f_00004-1-8 loss: 0.935716  [  288/  306]
train() client id: f_00004-2-0 loss: 0.815655  [   32/  306]
train() client id: f_00004-2-1 loss: 1.084084  [   64/  306]
train() client id: f_00004-2-2 loss: 0.779962  [   96/  306]
train() client id: f_00004-2-3 loss: 1.026923  [  128/  306]
train() client id: f_00004-2-4 loss: 0.991551  [  160/  306]
train() client id: f_00004-2-5 loss: 0.940790  [  192/  306]
train() client id: f_00004-2-6 loss: 0.948873  [  224/  306]
train() client id: f_00004-2-7 loss: 0.846603  [  256/  306]
train() client id: f_00004-2-8 loss: 1.029876  [  288/  306]
train() client id: f_00004-3-0 loss: 1.103601  [   32/  306]
train() client id: f_00004-3-1 loss: 0.860272  [   64/  306]
train() client id: f_00004-3-2 loss: 1.013643  [   96/  306]
train() client id: f_00004-3-3 loss: 0.894604  [  128/  306]
train() client id: f_00004-3-4 loss: 0.936115  [  160/  306]
train() client id: f_00004-3-5 loss: 0.831489  [  192/  306]
train() client id: f_00004-3-6 loss: 0.917903  [  224/  306]
train() client id: f_00004-3-7 loss: 0.851842  [  256/  306]
train() client id: f_00004-3-8 loss: 0.972979  [  288/  306]
train() client id: f_00004-4-0 loss: 0.896410  [   32/  306]
train() client id: f_00004-4-1 loss: 0.831269  [   64/  306]
train() client id: f_00004-4-2 loss: 0.789270  [   96/  306]
train() client id: f_00004-4-3 loss: 1.060032  [  128/  306]
train() client id: f_00004-4-4 loss: 0.839483  [  160/  306]
train() client id: f_00004-4-5 loss: 0.969340  [  192/  306]
train() client id: f_00004-4-6 loss: 0.831181  [  224/  306]
train() client id: f_00004-4-7 loss: 1.094827  [  256/  306]
train() client id: f_00004-4-8 loss: 0.991126  [  288/  306]
train() client id: f_00004-5-0 loss: 0.969862  [   32/  306]
train() client id: f_00004-5-1 loss: 0.890231  [   64/  306]
train() client id: f_00004-5-2 loss: 0.875308  [   96/  306]
train() client id: f_00004-5-3 loss: 0.869858  [  128/  306]
train() client id: f_00004-5-4 loss: 1.123889  [  160/  306]
train() client id: f_00004-5-5 loss: 0.829964  [  192/  306]
train() client id: f_00004-5-6 loss: 0.910262  [  224/  306]
train() client id: f_00004-5-7 loss: 0.882639  [  256/  306]
train() client id: f_00004-5-8 loss: 0.924508  [  288/  306]
train() client id: f_00004-6-0 loss: 0.946811  [   32/  306]
train() client id: f_00004-6-1 loss: 0.933134  [   64/  306]
train() client id: f_00004-6-2 loss: 0.999253  [   96/  306]
train() client id: f_00004-6-3 loss: 0.958084  [  128/  306]
train() client id: f_00004-6-4 loss: 0.995268  [  160/  306]
train() client id: f_00004-6-5 loss: 0.741747  [  192/  306]
train() client id: f_00004-6-6 loss: 0.996035  [  224/  306]
train() client id: f_00004-6-7 loss: 1.019980  [  256/  306]
train() client id: f_00004-6-8 loss: 0.870764  [  288/  306]
train() client id: f_00004-7-0 loss: 1.081818  [   32/  306]
train() client id: f_00004-7-1 loss: 0.924973  [   64/  306]
train() client id: f_00004-7-2 loss: 0.959480  [   96/  306]
train() client id: f_00004-7-3 loss: 0.800223  [  128/  306]
train() client id: f_00004-7-4 loss: 0.952740  [  160/  306]
train() client id: f_00004-7-5 loss: 1.016917  [  192/  306]
train() client id: f_00004-7-6 loss: 0.856755  [  224/  306]
train() client id: f_00004-7-7 loss: 0.915894  [  256/  306]
train() client id: f_00004-7-8 loss: 0.953376  [  288/  306]
train() client id: f_00004-8-0 loss: 0.910354  [   32/  306]
train() client id: f_00004-8-1 loss: 0.913246  [   64/  306]
train() client id: f_00004-8-2 loss: 1.051458  [   96/  306]
train() client id: f_00004-8-3 loss: 1.073624  [  128/  306]
train() client id: f_00004-8-4 loss: 0.857149  [  160/  306]
train() client id: f_00004-8-5 loss: 0.727743  [  192/  306]
train() client id: f_00004-8-6 loss: 0.914788  [  224/  306]
train() client id: f_00004-8-7 loss: 1.013822  [  256/  306]
train() client id: f_00004-8-8 loss: 0.908494  [  288/  306]
train() client id: f_00004-9-0 loss: 0.962943  [   32/  306]
train() client id: f_00004-9-1 loss: 0.910268  [   64/  306]
train() client id: f_00004-9-2 loss: 0.935088  [   96/  306]
train() client id: f_00004-9-3 loss: 1.075336  [  128/  306]
train() client id: f_00004-9-4 loss: 0.907227  [  160/  306]
train() client id: f_00004-9-5 loss: 0.801259  [  192/  306]
train() client id: f_00004-9-6 loss: 0.934171  [  224/  306]
train() client id: f_00004-9-7 loss: 0.881466  [  256/  306]
train() client id: f_00004-9-8 loss: 1.024253  [  288/  306]
train() client id: f_00004-10-0 loss: 1.016760  [   32/  306]
train() client id: f_00004-10-1 loss: 0.901075  [   64/  306]
train() client id: f_00004-10-2 loss: 0.913826  [   96/  306]
train() client id: f_00004-10-3 loss: 0.824607  [  128/  306]
train() client id: f_00004-10-4 loss: 0.940088  [  160/  306]
train() client id: f_00004-10-5 loss: 0.982587  [  192/  306]
train() client id: f_00004-10-6 loss: 0.913708  [  224/  306]
train() client id: f_00004-10-7 loss: 0.914290  [  256/  306]
train() client id: f_00004-10-8 loss: 0.955638  [  288/  306]
train() client id: f_00004-11-0 loss: 0.983667  [   32/  306]
train() client id: f_00004-11-1 loss: 0.922490  [   64/  306]
train() client id: f_00004-11-2 loss: 0.947928  [   96/  306]
train() client id: f_00004-11-3 loss: 0.893500  [  128/  306]
train() client id: f_00004-11-4 loss: 0.994087  [  160/  306]
train() client id: f_00004-11-5 loss: 0.975830  [  192/  306]
train() client id: f_00004-11-6 loss: 0.879389  [  224/  306]
train() client id: f_00004-11-7 loss: 0.976779  [  256/  306]
train() client id: f_00004-11-8 loss: 0.865819  [  288/  306]
train() client id: f_00004-12-0 loss: 1.062522  [   32/  306]
train() client id: f_00004-12-1 loss: 1.040819  [   64/  306]
train() client id: f_00004-12-2 loss: 0.860294  [   96/  306]
train() client id: f_00004-12-3 loss: 0.950216  [  128/  306]
train() client id: f_00004-12-4 loss: 0.875621  [  160/  306]
train() client id: f_00004-12-5 loss: 0.838825  [  192/  306]
train() client id: f_00004-12-6 loss: 1.018323  [  224/  306]
train() client id: f_00004-12-7 loss: 0.796970  [  256/  306]
train() client id: f_00004-12-8 loss: 0.936608  [  288/  306]
train() client id: f_00004-13-0 loss: 0.929090  [   32/  306]
train() client id: f_00004-13-1 loss: 0.907378  [   64/  306]
train() client id: f_00004-13-2 loss: 0.945333  [   96/  306]
train() client id: f_00004-13-3 loss: 0.883721  [  128/  306]
train() client id: f_00004-13-4 loss: 1.029130  [  160/  306]
train() client id: f_00004-13-5 loss: 0.989784  [  192/  306]
train() client id: f_00004-13-6 loss: 0.937503  [  224/  306]
train() client id: f_00004-13-7 loss: 0.881514  [  256/  306]
train() client id: f_00004-13-8 loss: 0.902649  [  288/  306]
train() client id: f_00005-0-0 loss: 0.645072  [   32/  146]
train() client id: f_00005-0-1 loss: 0.786156  [   64/  146]
train() client id: f_00005-0-2 loss: 1.032845  [   96/  146]
train() client id: f_00005-0-3 loss: 0.947832  [  128/  146]
train() client id: f_00005-1-0 loss: 0.818875  [   32/  146]
train() client id: f_00005-1-1 loss: 0.795899  [   64/  146]
train() client id: f_00005-1-2 loss: 0.759441  [   96/  146]
train() client id: f_00005-1-3 loss: 1.004031  [  128/  146]
train() client id: f_00005-2-0 loss: 0.754976  [   32/  146]
train() client id: f_00005-2-1 loss: 0.906983  [   64/  146]
train() client id: f_00005-2-2 loss: 0.850594  [   96/  146]
train() client id: f_00005-2-3 loss: 0.926943  [  128/  146]
train() client id: f_00005-3-0 loss: 0.873689  [   32/  146]
train() client id: f_00005-3-1 loss: 0.906979  [   64/  146]
train() client id: f_00005-3-2 loss: 0.895391  [   96/  146]
train() client id: f_00005-3-3 loss: 0.827970  [  128/  146]
train() client id: f_00005-4-0 loss: 0.807426  [   32/  146]
train() client id: f_00005-4-1 loss: 0.968222  [   64/  146]
train() client id: f_00005-4-2 loss: 0.771214  [   96/  146]
train() client id: f_00005-4-3 loss: 0.995524  [  128/  146]
train() client id: f_00005-5-0 loss: 0.920895  [   32/  146]
train() client id: f_00005-5-1 loss: 0.885948  [   64/  146]
train() client id: f_00005-5-2 loss: 0.705469  [   96/  146]
train() client id: f_00005-5-3 loss: 1.020902  [  128/  146]
train() client id: f_00005-6-0 loss: 1.009391  [   32/  146]
train() client id: f_00005-6-1 loss: 0.792944  [   64/  146]
train() client id: f_00005-6-2 loss: 0.747522  [   96/  146]
train() client id: f_00005-6-3 loss: 0.888460  [  128/  146]
train() client id: f_00005-7-0 loss: 0.972440  [   32/  146]
train() client id: f_00005-7-1 loss: 0.983326  [   64/  146]
train() client id: f_00005-7-2 loss: 0.873923  [   96/  146]
train() client id: f_00005-7-3 loss: 0.712249  [  128/  146]
train() client id: f_00005-8-0 loss: 0.874523  [   32/  146]
train() client id: f_00005-8-1 loss: 0.851109  [   64/  146]
train() client id: f_00005-8-2 loss: 0.836216  [   96/  146]
train() client id: f_00005-8-3 loss: 1.046372  [  128/  146]
train() client id: f_00005-9-0 loss: 1.039261  [   32/  146]
train() client id: f_00005-9-1 loss: 0.812071  [   64/  146]
train() client id: f_00005-9-2 loss: 0.858705  [   96/  146]
train() client id: f_00005-9-3 loss: 1.029896  [  128/  146]
train() client id: f_00005-10-0 loss: 0.835701  [   32/  146]
train() client id: f_00005-10-1 loss: 0.858544  [   64/  146]
train() client id: f_00005-10-2 loss: 1.089054  [   96/  146]
train() client id: f_00005-10-3 loss: 0.918659  [  128/  146]
train() client id: f_00005-11-0 loss: 0.910173  [   32/  146]
train() client id: f_00005-11-1 loss: 0.753183  [   64/  146]
train() client id: f_00005-11-2 loss: 1.101590  [   96/  146]
train() client id: f_00005-11-3 loss: 0.983806  [  128/  146]
train() client id: f_00005-12-0 loss: 0.835271  [   32/  146]
train() client id: f_00005-12-1 loss: 1.041250  [   64/  146]
train() client id: f_00005-12-2 loss: 0.810452  [   96/  146]
train() client id: f_00005-12-3 loss: 1.001704  [  128/  146]
train() client id: f_00005-13-0 loss: 0.915140  [   32/  146]
train() client id: f_00005-13-1 loss: 0.958753  [   64/  146]
train() client id: f_00005-13-2 loss: 0.746688  [   96/  146]
train() client id: f_00005-13-3 loss: 1.049494  [  128/  146]
train() client id: f_00006-0-0 loss: 1.009583  [   32/   54]
train() client id: f_00006-1-0 loss: 0.891664  [   32/   54]
train() client id: f_00006-2-0 loss: 1.004126  [   32/   54]
train() client id: f_00006-3-0 loss: 0.947903  [   32/   54]
train() client id: f_00006-4-0 loss: 1.016636  [   32/   54]
train() client id: f_00006-5-0 loss: 1.057317  [   32/   54]
train() client id: f_00006-6-0 loss: 1.003701  [   32/   54]
train() client id: f_00006-7-0 loss: 1.020721  [   32/   54]
train() client id: f_00006-8-0 loss: 0.940008  [   32/   54]
train() client id: f_00006-9-0 loss: 1.067802  [   32/   54]
train() client id: f_00006-10-0 loss: 1.039554  [   32/   54]
train() client id: f_00006-11-0 loss: 0.945354  [   32/   54]
train() client id: f_00006-12-0 loss: 1.017848  [   32/   54]
train() client id: f_00006-13-0 loss: 1.029320  [   32/   54]
train() client id: f_00007-0-0 loss: 0.875214  [   32/  179]
train() client id: f_00007-0-1 loss: 0.880448  [   64/  179]
train() client id: f_00007-0-2 loss: 0.745299  [   96/  179]
train() client id: f_00007-0-3 loss: 0.617715  [  128/  179]
train() client id: f_00007-0-4 loss: 0.691589  [  160/  179]
train() client id: f_00007-1-0 loss: 0.764468  [   32/  179]
train() client id: f_00007-1-1 loss: 0.870817  [   64/  179]
train() client id: f_00007-1-2 loss: 0.740478  [   96/  179]
train() client id: f_00007-1-3 loss: 0.617338  [  128/  179]
train() client id: f_00007-1-4 loss: 0.689503  [  160/  179]
train() client id: f_00007-2-0 loss: 0.728405  [   32/  179]
train() client id: f_00007-2-1 loss: 1.087487  [   64/  179]
train() client id: f_00007-2-2 loss: 0.636428  [   96/  179]
train() client id: f_00007-2-3 loss: 0.690981  [  128/  179]
train() client id: f_00007-2-4 loss: 0.665441  [  160/  179]
train() client id: f_00007-3-0 loss: 0.694523  [   32/  179]
train() client id: f_00007-3-1 loss: 0.791659  [   64/  179]
train() client id: f_00007-3-2 loss: 0.979846  [   96/  179]
train() client id: f_00007-3-3 loss: 0.746950  [  128/  179]
train() client id: f_00007-3-4 loss: 0.642250  [  160/  179]
train() client id: f_00007-4-0 loss: 0.643876  [   32/  179]
train() client id: f_00007-4-1 loss: 0.746058  [   64/  179]
train() client id: f_00007-4-2 loss: 0.782133  [   96/  179]
train() client id: f_00007-4-3 loss: 0.875367  [  128/  179]
train() client id: f_00007-4-4 loss: 0.769774  [  160/  179]
train() client id: f_00007-5-0 loss: 0.822554  [   32/  179]
train() client id: f_00007-5-1 loss: 0.723554  [   64/  179]
train() client id: f_00007-5-2 loss: 0.753736  [   96/  179]
train() client id: f_00007-5-3 loss: 0.894437  [  128/  179]
train() client id: f_00007-5-4 loss: 0.635190  [  160/  179]
train() client id: f_00007-6-0 loss: 0.825858  [   32/  179]
train() client id: f_00007-6-1 loss: 0.700785  [   64/  179]
train() client id: f_00007-6-2 loss: 0.745345  [   96/  179]
train() client id: f_00007-6-3 loss: 0.775737  [  128/  179]
train() client id: f_00007-6-4 loss: 0.817741  [  160/  179]
train() client id: f_00007-7-0 loss: 0.728248  [   32/  179]
train() client id: f_00007-7-1 loss: 0.898074  [   64/  179]
train() client id: f_00007-7-2 loss: 0.691218  [   96/  179]
train() client id: f_00007-7-3 loss: 0.744349  [  128/  179]
train() client id: f_00007-7-4 loss: 0.689510  [  160/  179]
train() client id: f_00007-8-0 loss: 0.748793  [   32/  179]
train() client id: f_00007-8-1 loss: 0.825544  [   64/  179]
train() client id: f_00007-8-2 loss: 0.925331  [   96/  179]
train() client id: f_00007-8-3 loss: 0.655856  [  128/  179]
train() client id: f_00007-8-4 loss: 0.844898  [  160/  179]
train() client id: f_00007-9-0 loss: 0.734507  [   32/  179]
train() client id: f_00007-9-1 loss: 0.675185  [   64/  179]
train() client id: f_00007-9-2 loss: 0.868479  [   96/  179]
train() client id: f_00007-9-3 loss: 0.750871  [  128/  179]
train() client id: f_00007-9-4 loss: 0.955287  [  160/  179]
train() client id: f_00007-10-0 loss: 0.675390  [   32/  179]
train() client id: f_00007-10-1 loss: 0.840901  [   64/  179]
train() client id: f_00007-10-2 loss: 0.706718  [   96/  179]
train() client id: f_00007-10-3 loss: 0.898458  [  128/  179]
train() client id: f_00007-10-4 loss: 0.769801  [  160/  179]
train() client id: f_00007-11-0 loss: 0.832197  [   32/  179]
train() client id: f_00007-11-1 loss: 0.694447  [   64/  179]
train() client id: f_00007-11-2 loss: 0.913662  [   96/  179]
train() client id: f_00007-11-3 loss: 0.706507  [  128/  179]
train() client id: f_00007-11-4 loss: 0.864641  [  160/  179]
train() client id: f_00007-12-0 loss: 0.756089  [   32/  179]
train() client id: f_00007-12-1 loss: 0.775612  [   64/  179]
train() client id: f_00007-12-2 loss: 0.882381  [   96/  179]
train() client id: f_00007-12-3 loss: 0.851592  [  128/  179]
train() client id: f_00007-12-4 loss: 0.699420  [  160/  179]
train() client id: f_00007-13-0 loss: 0.845828  [   32/  179]
train() client id: f_00007-13-1 loss: 0.771811  [   64/  179]
train() client id: f_00007-13-2 loss: 0.762085  [   96/  179]
train() client id: f_00007-13-3 loss: 0.858141  [  128/  179]
train() client id: f_00007-13-4 loss: 0.885005  [  160/  179]
train() client id: f_00008-0-0 loss: 0.916322  [   32/  130]
train() client id: f_00008-0-1 loss: 1.018651  [   64/  130]
train() client id: f_00008-0-2 loss: 0.996046  [   96/  130]
train() client id: f_00008-0-3 loss: 0.951006  [  128/  130]
train() client id: f_00008-1-0 loss: 0.998497  [   32/  130]
train() client id: f_00008-1-1 loss: 0.885488  [   64/  130]
train() client id: f_00008-1-2 loss: 1.113646  [   96/  130]
train() client id: f_00008-1-3 loss: 0.881665  [  128/  130]
train() client id: f_00008-2-0 loss: 1.091408  [   32/  130]
train() client id: f_00008-2-1 loss: 0.748256  [   64/  130]
train() client id: f_00008-2-2 loss: 1.027949  [   96/  130]
train() client id: f_00008-2-3 loss: 0.994674  [  128/  130]
train() client id: f_00008-3-0 loss: 1.047199  [   32/  130]
train() client id: f_00008-3-1 loss: 0.815078  [   64/  130]
train() client id: f_00008-3-2 loss: 0.907086  [   96/  130]
train() client id: f_00008-3-3 loss: 1.114473  [  128/  130]
train() client id: f_00008-4-0 loss: 1.108149  [   32/  130]
train() client id: f_00008-4-1 loss: 0.941823  [   64/  130]
train() client id: f_00008-4-2 loss: 0.860732  [   96/  130]
train() client id: f_00008-4-3 loss: 0.944156  [  128/  130]
train() client id: f_00008-5-0 loss: 0.925185  [   32/  130]
train() client id: f_00008-5-1 loss: 0.957037  [   64/  130]
train() client id: f_00008-5-2 loss: 1.089349  [   96/  130]
train() client id: f_00008-5-3 loss: 0.914002  [  128/  130]
train() client id: f_00008-6-0 loss: 0.987261  [   32/  130]
train() client id: f_00008-6-1 loss: 0.841475  [   64/  130]
train() client id: f_00008-6-2 loss: 1.056866  [   96/  130]
train() client id: f_00008-6-3 loss: 1.014556  [  128/  130]
train() client id: f_00008-7-0 loss: 0.854794  [   32/  130]
train() client id: f_00008-7-1 loss: 0.989766  [   64/  130]
train() client id: f_00008-7-2 loss: 1.056594  [   96/  130]
train() client id: f_00008-7-3 loss: 0.995322  [  128/  130]
train() client id: f_00008-8-0 loss: 0.984906  [   32/  130]
train() client id: f_00008-8-1 loss: 1.019324  [   64/  130]
train() client id: f_00008-8-2 loss: 0.947768  [   96/  130]
train() client id: f_00008-8-3 loss: 0.948635  [  128/  130]
train() client id: f_00008-9-0 loss: 1.001306  [   32/  130]
train() client id: f_00008-9-1 loss: 0.984875  [   64/  130]
train() client id: f_00008-9-2 loss: 0.907335  [   96/  130]
train() client id: f_00008-9-3 loss: 1.004561  [  128/  130]
train() client id: f_00008-10-0 loss: 0.968447  [   32/  130]
train() client id: f_00008-10-1 loss: 0.906760  [   64/  130]
train() client id: f_00008-10-2 loss: 1.070275  [   96/  130]
train() client id: f_00008-10-3 loss: 0.961917  [  128/  130]
train() client id: f_00008-11-0 loss: 0.946462  [   32/  130]
train() client id: f_00008-11-1 loss: 1.128560  [   64/  130]
train() client id: f_00008-11-2 loss: 0.963767  [   96/  130]
train() client id: f_00008-11-3 loss: 0.880251  [  128/  130]
train() client id: f_00008-12-0 loss: 0.957023  [   32/  130]
train() client id: f_00008-12-1 loss: 1.046824  [   64/  130]
train() client id: f_00008-12-2 loss: 0.870423  [   96/  130]
train() client id: f_00008-12-3 loss: 1.062617  [  128/  130]
train() client id: f_00008-13-0 loss: 1.123581  [   32/  130]
train() client id: f_00008-13-1 loss: 1.054414  [   64/  130]
train() client id: f_00008-13-2 loss: 0.883416  [   96/  130]
train() client id: f_00008-13-3 loss: 0.873265  [  128/  130]
train() client id: f_00009-0-0 loss: 1.095562  [   32/  118]
train() client id: f_00009-0-1 loss: 1.174297  [   64/  118]
train() client id: f_00009-0-2 loss: 1.228928  [   96/  118]
train() client id: f_00009-1-0 loss: 1.150162  [   32/  118]
train() client id: f_00009-1-1 loss: 1.186868  [   64/  118]
train() client id: f_00009-1-2 loss: 1.101695  [   96/  118]
train() client id: f_00009-2-0 loss: 1.158379  [   32/  118]
train() client id: f_00009-2-1 loss: 1.013634  [   64/  118]
train() client id: f_00009-2-2 loss: 1.302686  [   96/  118]
train() client id: f_00009-3-0 loss: 1.143164  [   32/  118]
train() client id: f_00009-3-1 loss: 1.133122  [   64/  118]
train() client id: f_00009-3-2 loss: 1.193799  [   96/  118]
train() client id: f_00009-4-0 loss: 1.281278  [   32/  118]
train() client id: f_00009-4-1 loss: 0.988906  [   64/  118]
train() client id: f_00009-4-2 loss: 1.166073  [   96/  118]
train() client id: f_00009-5-0 loss: 1.095018  [   32/  118]
train() client id: f_00009-5-1 loss: 1.251955  [   64/  118]
train() client id: f_00009-5-2 loss: 1.206211  [   96/  118]
train() client id: f_00009-6-0 loss: 1.106947  [   32/  118]
train() client id: f_00009-6-1 loss: 1.206982  [   64/  118]
train() client id: f_00009-6-2 loss: 1.213711  [   96/  118]
train() client id: f_00009-7-0 loss: 1.270227  [   32/  118]
train() client id: f_00009-7-1 loss: 1.060007  [   64/  118]
train() client id: f_00009-7-2 loss: 1.244065  [   96/  118]
train() client id: f_00009-8-0 loss: 1.171955  [   32/  118]
train() client id: f_00009-8-1 loss: 1.352304  [   64/  118]
train() client id: f_00009-8-2 loss: 1.137720  [   96/  118]
train() client id: f_00009-9-0 loss: 1.251801  [   32/  118]
train() client id: f_00009-9-1 loss: 1.203176  [   64/  118]
train() client id: f_00009-9-2 loss: 1.180750  [   96/  118]
train() client id: f_00009-10-0 loss: 1.198448  [   32/  118]
train() client id: f_00009-10-1 loss: 1.156714  [   64/  118]
train() client id: f_00009-10-2 loss: 1.306325  [   96/  118]
train() client id: f_00009-11-0 loss: 1.285972  [   32/  118]
train() client id: f_00009-11-1 loss: 1.246350  [   64/  118]
train() client id: f_00009-11-2 loss: 1.188266  [   96/  118]
train() client id: f_00009-12-0 loss: 1.336905  [   32/  118]
train() client id: f_00009-12-1 loss: 1.237903  [   64/  118]
train() client id: f_00009-12-2 loss: 1.219051  [   96/  118]
train() client id: f_00009-13-0 loss: 1.340968  [   32/  118]
train() client id: f_00009-13-1 loss: 1.195680  [   64/  118]
train() client id: f_00009-13-2 loss: 1.197831  [   96/  118]
At round 0 accuracy: 0.5888594164456233
At round 0 training accuracy: 0.5439302481556003
At round 0 training loss: 0.9573362021439032
update_location
xs = 25.471708 -98.998411 70.045120 89.056472 -190.103519 -125.217951 12.784040 -26.624259 -1.680116 -80.304393 
ys = 17.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 31.875078 -19.177652 82.845030 -0.998518 
xs mean: -32.557130840587135
ys mean: 14.371751218646875
dists_uav = 104.681155 141.572135 122.098578 135.791473 215.004125 161.167269 105.732929 105.245587 129.869634 128.256745 
uav_gains = -100.496743 -103.776547 -102.168038 -103.323006 -108.651349 -105.193509 -100.605292 -100.555130 -102.838336 -102.702548 
uav_gains_db_mean: -103.03104993722953
dists_bs = 254.887144 176.631487 300.352526 304.907200 166.336938 198.528721 236.109273 244.377415 196.296512 199.856790 
bs_gains = -106.944801 -102.484967 -108.940735 -109.123754 -101.754743 -103.906113 -106.014224 -106.432769 -103.768611 -103.987189 
bs_gains_db_mean: -105.335790633955
Round 1
-------------------------------
ene_coms = [0.00643506 0.0070261  0.00692483 0.00730843 0.00679701 0.00751835
 0.00646479 0.00645102 0.00746779 0.00754847]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.86725856 22.67272985 10.71140759  3.85243292 26.14357449 12.59395942
  4.76163936 15.36273665 11.23554768 10.22007404]
obj_prev = 128.42136057378684
eta_min = 1.8351590361533117e-09	eta_max = 0.9284638413004053
af = 27.184654844112664	bf = 1.9471849635645784	zeta = 29.903120328523933	eta = 0.9090909090909091
af = 27.184654844112664	bf = 1.9471849635645784	zeta = 49.91975235143014	eta = 0.5445671014698825
af = 27.184654844112664	bf = 1.9471849635645784	zeta = 40.552719309141395	eta = 0.6703534388625992
af = 27.184654844112664	bf = 1.9471849635645784	zeta = 38.88920501190462	eta = 0.6990282993903063
af = 27.184654844112664	bf = 1.9471849635645784	zeta = 38.81106631104125	eta = 0.7004356599287502
af = 27.184654844112664	bf = 1.9471849635645784	zeta = 38.81088288534124	eta = 0.700438970286353
eta = 0.700438970286353
ene_coms = [0.00643506 0.0070261  0.00692483 0.00730843 0.00679701 0.00751835
 0.00646479 0.00645102 0.00746779 0.00754847]
ene_comp = [0.02938023 0.06179176 0.02891388 0.01002659 0.07135199 0.03404376
 0.01259153 0.04173858 0.03031294 0.02751482]
ene_total = [3.32853666 6.39566796 3.33071239 1.61104994 7.26286891 3.86262352
 1.77102117 4.47855732 3.51119654 3.25864846]
ti_comp = [0.25791348 0.25200317 0.25301584 0.24917985 0.25429406 0.24708063
 0.25761624 0.25775393 0.2475862  0.24677938]
ti_coms = [0.06435065 0.07026095 0.06924829 0.07308427 0.06797006 0.07518349
 0.06464789 0.06451019 0.07467792 0.07548474]
t_total = [29.9499958 29.9499958 29.9499958 29.9499958 29.9499958 29.9499958
 29.9499958 29.9499958 29.9499958 29.9499958]
ene_coms = [0.00643506 0.0070261  0.00692483 0.00730843 0.00679701 0.00751835
 0.00646479 0.00645102 0.00746779 0.00754847]
ene_comp = [2.38285468e-05 2.32198601e-04 2.35995452e-05 1.01464360e-06
 3.51095965e-04 4.03938414e-05 1.88004742e-06 6.84041559e-05
 2.83995322e-05 2.13777634e-05]
ene_total = [0.60026485 0.67455807 0.64576035 0.6793116  0.6643173  0.70248069
 0.60098746 0.60589028 0.69666743 0.70351309]
optimize_network iter = 0 obj = 6.573751110907766
eta = 0.700438970286353
freqs = [5.69575332e+07 1.22601146e+08 5.71384744e+07 2.01191764e+07
 1.40294250e+08 6.88920037e+07 2.44385361e+07 8.09659461e+07
 6.12169352e+07 5.57478080e+07]
eta_min = 0.6469623707036117	eta_max = 0.7004389702863497
af = 0.06194301278990222	bf = 1.9471849635645784	zeta = 0.06813731406889245	eta = 0.909090909090909
af = 0.06194301278990222	bf = 1.9471849635645784	zeta = 21.48397639576323	eta = 0.002883219179207335
af = 0.06194301278990222	bf = 1.9471849635645784	zeta = 2.316158211810372	eta = 0.026743860792430876
af = 0.06194301278990222	bf = 1.9471849635645784	zeta = 2.2311788444054046	eta = 0.027762459717302335
af = 0.06194301278990222	bf = 1.9471849635645784	zeta = 2.2311348832289224	eta = 0.02776300673505567
eta = 0.02776300673505567
ene_coms = [0.00643506 0.0070261  0.00692483 0.00730843 0.00679701 0.00751835
 0.00646479 0.00645102 0.00746779 0.00754847]
ene_comp = [2.39862921e-04 2.33735759e-03 2.37557746e-04 1.02136056e-05
 3.53420226e-03 4.06612493e-04 1.89249336e-05 6.88569925e-04
 2.85875375e-04 2.15192845e-04]
ene_total = [0.1911369  0.26812296 0.20509531 0.20956967 0.29583469 0.22693169
 0.18566147 0.20444251 0.22202668 0.22231301]
ti_comp = [0.31544295 0.30953265 0.31054532 0.30670933 0.31182354 0.30461011
 0.31514572 0.31528341 0.30511568 0.30430886]
ti_coms = [0.06435065 0.07026095 0.06924829 0.07308427 0.06797006 0.07518349
 0.06464789 0.06451019 0.07467792 0.07548474]
t_total = [29.9499958 29.9499958 29.9499958 29.9499958 29.9499958 29.9499958
 29.9499958 29.9499958 29.9499958 29.9499958]
ene_coms = [0.00643506 0.0070261  0.00692483 0.00730843 0.00679701 0.00751835
 0.00646479 0.00645102 0.00746779 0.00754847]
ene_comp = [2.91436775e-05 2.81578227e-04 2.86608767e-05 1.22525301e-06
 4.27189794e-04 4.86232880e-05 2.29843827e-06 8.36433149e-05
 3.42117582e-05 2.57211987e-05]
ene_total = [0.50975849 0.57627297 0.54834252 0.57642905 0.56969007 0.59672094
 0.50998549 0.51531441 0.59159763 0.59729051]
optimize_network iter = 1 obj = 5.591402078447414
eta = 0.6469623707036117
freqs = [5.69575332e+07 1.22078973e+08 5.69374694e+07 1.99913872e+07
 1.39931017e+08 6.83455122e+07 2.44333947e+07 8.09568070e+07
 6.07547572e+07 5.52928362e+07]
eta_min = 0.6469623707036255	eta_max = 0.652204420802997
af = 0.06152099167554693	bf = 1.9471849635645784	zeta = 0.06767309084310162	eta = 0.9090909090909091
af = 0.06152099167554693	bf = 1.9471849635645784	zeta = 21.48353394377206	eta = 0.0028636346253164495
af = 0.06152099167554693	bf = 1.9471849635645784	zeta = 2.3140578120291244	eta = 0.02658576261826454
af = 0.06152099167554693	bf = 1.9471849635645784	zeta = 2.229621122173459	eta = 0.02759257663274
af = 0.06152099167554693	bf = 1.9471849635645784	zeta = 2.229577951603547	eta = 0.027593110898544757
eta = 0.027593110898544757
ene_coms = [0.00643506 0.0070261  0.00692483 0.00730843 0.00679701 0.00751835
 0.00646479 0.00645102 0.00746779 0.00754847]
ene_comp = [2.40273728e-04 2.32145893e-03 2.36293299e-04 1.01015429e-05
 3.52194689e-03 4.00872494e-04 1.89493701e-05 6.89593518e-04
 2.82057290e-04 2.12057257e-04]
ene_total = [0.19111527 0.26762093 0.20502327 0.20952984 0.29543214 0.22672771
 0.18562973 0.20443609 0.22187858 0.22218439]
ti_comp = [0.31544295 0.30953265 0.31054532 0.30670933 0.31182354 0.30461011
 0.31514572 0.31528341 0.30511568 0.30430886]
ti_coms = [0.06435065 0.07026095 0.06924829 0.07308427 0.06797006 0.07518349
 0.06464789 0.06451019 0.07467792 0.07548474]
t_total = [29.9499958 29.9499958 29.9499958 29.9499958 29.9499958 29.9499958
 29.9499958 29.9499958 29.9499958 29.9499958]
ene_coms = [0.00643506 0.0070261  0.00692483 0.00730843 0.00679701 0.00751835
 0.00646479 0.00645102 0.00746779 0.00754847]
ene_comp = [2.91436775e-05 2.81578227e-04 2.86608767e-05 1.22525301e-06
 4.27189794e-04 4.86232880e-05 2.29843827e-06 8.36433149e-05
 3.42117582e-05 2.57211987e-05]
ene_total = [0.50975849 0.57627297 0.54834252 0.57642905 0.56969007 0.59672094
 0.50998549 0.51531441 0.59159763 0.59729051]
optimize_network iter = 2 obj = 5.591402078447629
eta = 0.6469623707036255
freqs = [5.69575332e+07 1.22078973e+08 5.69374694e+07 1.99913872e+07
 1.39931017e+08 6.83455122e+07 2.44333947e+07 8.09568070e+07
 6.07547572e+07 5.52928362e+07]
Done!
ene_coms = [0.00643506 0.0070261  0.00692483 0.00730843 0.00679701 0.00751835
 0.00646479 0.00645102 0.00746779 0.00754847]
ene_comp = [2.86134964e-05 2.76455762e-04 2.81394787e-05 1.20296324e-06
 4.19418367e-04 4.77387343e-05 2.25662513e-06 8.21216777e-05
 3.35893787e-05 2.52532793e-05]
ene_total = [0.00646368 0.00730255 0.00695297 0.00730963 0.00721642 0.00756609
 0.00646705 0.00653314 0.00750138 0.00757373]
At round 1 energy consumption: 0.0708866351734602
At round 1 eta: 0.6469623707036255
At round 1 a_n: 27.840057009285058
At round 1 local rounds: 14.259406792797005
At round 1 global rounds: 78.85861080806595
gradient difference: 0.2793499529361725
train() client id: f_00000-0-0 loss: 1.213485  [   32/  126]
train() client id: f_00000-0-1 loss: 1.185938  [   64/  126]
train() client id: f_00000-0-2 loss: 1.182506  [   96/  126]
train() client id: f_00000-1-0 loss: 1.005183  [   32/  126]
train() client id: f_00000-1-1 loss: 1.031246  [   64/  126]
train() client id: f_00000-1-2 loss: 1.125122  [   96/  126]
train() client id: f_00000-2-0 loss: 0.874100  [   32/  126]
train() client id: f_00000-2-1 loss: 0.952641  [   64/  126]
train() client id: f_00000-2-2 loss: 0.964833  [   96/  126]
train() client id: f_00000-3-0 loss: 0.870197  [   32/  126]
train() client id: f_00000-3-1 loss: 1.018652  [   64/  126]
train() client id: f_00000-3-2 loss: 0.891847  [   96/  126]
train() client id: f_00000-4-0 loss: 0.827229  [   32/  126]
train() client id: f_00000-4-1 loss: 0.959081  [   64/  126]
train() client id: f_00000-4-2 loss: 0.869845  [   96/  126]
train() client id: f_00000-5-0 loss: 0.963808  [   32/  126]
train() client id: f_00000-5-1 loss: 0.758347  [   64/  126]
train() client id: f_00000-5-2 loss: 0.904094  [   96/  126]
train() client id: f_00000-6-0 loss: 0.891767  [   32/  126]
train() client id: f_00000-6-1 loss: 0.819680  [   64/  126]
train() client id: f_00000-6-2 loss: 0.716250  [   96/  126]
train() client id: f_00000-7-0 loss: 0.822543  [   32/  126]
train() client id: f_00000-7-1 loss: 0.757772  [   64/  126]
train() client id: f_00000-7-2 loss: 0.890274  [   96/  126]
train() client id: f_00000-8-0 loss: 0.832549  [   32/  126]
train() client id: f_00000-8-1 loss: 0.796809  [   64/  126]
train() client id: f_00000-8-2 loss: 0.771008  [   96/  126]
train() client id: f_00000-9-0 loss: 0.972810  [   32/  126]
train() client id: f_00000-9-1 loss: 0.861383  [   64/  126]
train() client id: f_00000-9-2 loss: 0.647334  [   96/  126]
train() client id: f_00000-10-0 loss: 0.820933  [   32/  126]
train() client id: f_00000-10-1 loss: 0.774166  [   64/  126]
train() client id: f_00000-10-2 loss: 0.827434  [   96/  126]
train() client id: f_00000-11-0 loss: 0.703053  [   32/  126]
train() client id: f_00000-11-1 loss: 0.848109  [   64/  126]
train() client id: f_00000-11-2 loss: 0.762757  [   96/  126]
train() client id: f_00000-12-0 loss: 0.721483  [   32/  126]
train() client id: f_00000-12-1 loss: 0.825917  [   64/  126]
train() client id: f_00000-12-2 loss: 0.705813  [   96/  126]
train() client id: f_00000-13-0 loss: 0.820383  [   32/  126]
train() client id: f_00000-13-1 loss: 0.722708  [   64/  126]
train() client id: f_00000-13-2 loss: 0.676492  [   96/  126]
train() client id: f_00001-0-0 loss: 0.682073  [   32/  265]
train() client id: f_00001-0-1 loss: 0.647647  [   64/  265]
train() client id: f_00001-0-2 loss: 0.748071  [   96/  265]
train() client id: f_00001-0-3 loss: 0.670616  [  128/  265]
train() client id: f_00001-0-4 loss: 0.659146  [  160/  265]
train() client id: f_00001-0-5 loss: 0.673766  [  192/  265]
train() client id: f_00001-0-6 loss: 0.759850  [  224/  265]
train() client id: f_00001-0-7 loss: 0.738347  [  256/  265]
train() client id: f_00001-1-0 loss: 0.662866  [   32/  265]
train() client id: f_00001-1-1 loss: 0.640902  [   64/  265]
train() client id: f_00001-1-2 loss: 0.734503  [   96/  265]
train() client id: f_00001-1-3 loss: 0.549476  [  128/  265]
train() client id: f_00001-1-4 loss: 0.636722  [  160/  265]
train() client id: f_00001-1-5 loss: 0.583382  [  192/  265]
train() client id: f_00001-1-6 loss: 0.612782  [  224/  265]
train() client id: f_00001-1-7 loss: 0.610484  [  256/  265]
train() client id: f_00001-2-0 loss: 0.641335  [   32/  265]
train() client id: f_00001-2-1 loss: 0.582008  [   64/  265]
train() client id: f_00001-2-2 loss: 0.582457  [   96/  265]
train() client id: f_00001-2-3 loss: 0.559017  [  128/  265]
train() client id: f_00001-2-4 loss: 0.561684  [  160/  265]
train() client id: f_00001-2-5 loss: 0.470862  [  192/  265]
train() client id: f_00001-2-6 loss: 0.610790  [  224/  265]
train() client id: f_00001-2-7 loss: 0.526981  [  256/  265]
train() client id: f_00001-3-0 loss: 0.522649  [   32/  265]
train() client id: f_00001-3-1 loss: 0.452104  [   64/  265]
train() client id: f_00001-3-2 loss: 0.481955  [   96/  265]
train() client id: f_00001-3-3 loss: 0.466488  [  128/  265]
train() client id: f_00001-3-4 loss: 0.594284  [  160/  265]
train() client id: f_00001-3-5 loss: 0.607313  [  192/  265]
train() client id: f_00001-3-6 loss: 0.486809  [  224/  265]
train() client id: f_00001-3-7 loss: 0.535843  [  256/  265]
train() client id: f_00001-4-0 loss: 0.500394  [   32/  265]
train() client id: f_00001-4-1 loss: 0.434619  [   64/  265]
train() client id: f_00001-4-2 loss: 0.492365  [   96/  265]
train() client id: f_00001-4-3 loss: 0.454683  [  128/  265]
train() client id: f_00001-4-4 loss: 0.536088  [  160/  265]
train() client id: f_00001-4-5 loss: 0.544818  [  192/  265]
train() client id: f_00001-4-6 loss: 0.462098  [  224/  265]
train() client id: f_00001-4-7 loss: 0.501495  [  256/  265]
train() client id: f_00001-5-0 loss: 0.450166  [   32/  265]
train() client id: f_00001-5-1 loss: 0.507740  [   64/  265]
train() client id: f_00001-5-2 loss: 0.409811  [   96/  265]
train() client id: f_00001-5-3 loss: 0.385964  [  128/  265]
train() client id: f_00001-5-4 loss: 0.393926  [  160/  265]
train() client id: f_00001-5-5 loss: 0.469606  [  192/  265]
train() client id: f_00001-5-6 loss: 0.450608  [  224/  265]
train() client id: f_00001-5-7 loss: 0.539750  [  256/  265]
train() client id: f_00001-6-0 loss: 0.488869  [   32/  265]
train() client id: f_00001-6-1 loss: 0.446722  [   64/  265]
train() client id: f_00001-6-2 loss: 0.437778  [   96/  265]
train() client id: f_00001-6-3 loss: 0.410914  [  128/  265]
train() client id: f_00001-6-4 loss: 0.408567  [  160/  265]
train() client id: f_00001-6-5 loss: 0.422307  [  192/  265]
train() client id: f_00001-6-6 loss: 0.458962  [  224/  265]
train() client id: f_00001-6-7 loss: 0.451590  [  256/  265]
train() client id: f_00001-7-0 loss: 0.420734  [   32/  265]
train() client id: f_00001-7-1 loss: 0.545976  [   64/  265]
train() client id: f_00001-7-2 loss: 0.434123  [   96/  265]
train() client id: f_00001-7-3 loss: 0.423377  [  128/  265]
train() client id: f_00001-7-4 loss: 0.423002  [  160/  265]
train() client id: f_00001-7-5 loss: 0.337840  [  192/  265]
train() client id: f_00001-7-6 loss: 0.338221  [  224/  265]
train() client id: f_00001-7-7 loss: 0.443497  [  256/  265]
train() client id: f_00001-8-0 loss: 0.390755  [   32/  265]
train() client id: f_00001-8-1 loss: 0.410067  [   64/  265]
train() client id: f_00001-8-2 loss: 0.418546  [   96/  265]
train() client id: f_00001-8-3 loss: 0.468507  [  128/  265]
train() client id: f_00001-8-4 loss: 0.350086  [  160/  265]
train() client id: f_00001-8-5 loss: 0.394152  [  192/  265]
train() client id: f_00001-8-6 loss: 0.370343  [  224/  265]
train() client id: f_00001-8-7 loss: 0.449550  [  256/  265]
train() client id: f_00001-9-0 loss: 0.345058  [   32/  265]
train() client id: f_00001-9-1 loss: 0.297335  [   64/  265]
train() client id: f_00001-9-2 loss: 0.429408  [   96/  265]
train() client id: f_00001-9-3 loss: 0.451971  [  128/  265]
train() client id: f_00001-9-4 loss: 0.354661  [  160/  265]
train() client id: f_00001-9-5 loss: 0.491128  [  192/  265]
train() client id: f_00001-9-6 loss: 0.433370  [  224/  265]
train() client id: f_00001-9-7 loss: 0.351635  [  256/  265]
train() client id: f_00001-10-0 loss: 0.436898  [   32/  265]
train() client id: f_00001-10-1 loss: 0.426018  [   64/  265]
train() client id: f_00001-10-2 loss: 0.320558  [   96/  265]
train() client id: f_00001-10-3 loss: 0.401391  [  128/  265]
train() client id: f_00001-10-4 loss: 0.483010  [  160/  265]
train() client id: f_00001-10-5 loss: 0.336689  [  192/  265]
train() client id: f_00001-10-6 loss: 0.325634  [  224/  265]
train() client id: f_00001-10-7 loss: 0.420559  [  256/  265]
train() client id: f_00001-11-0 loss: 0.401111  [   32/  265]
train() client id: f_00001-11-1 loss: 0.435316  [   64/  265]
train() client id: f_00001-11-2 loss: 0.306213  [   96/  265]
train() client id: f_00001-11-3 loss: 0.424670  [  128/  265]
train() client id: f_00001-11-4 loss: 0.372739  [  160/  265]
train() client id: f_00001-11-5 loss: 0.352577  [  192/  265]
train() client id: f_00001-11-6 loss: 0.362673  [  224/  265]
train() client id: f_00001-11-7 loss: 0.414208  [  256/  265]
train() client id: f_00001-12-0 loss: 0.329977  [   32/  265]
train() client id: f_00001-12-1 loss: 0.363900  [   64/  265]
train() client id: f_00001-12-2 loss: 0.326848  [   96/  265]
train() client id: f_00001-12-3 loss: 0.449287  [  128/  265]
train() client id: f_00001-12-4 loss: 0.443934  [  160/  265]
train() client id: f_00001-12-5 loss: 0.274260  [  192/  265]
train() client id: f_00001-12-6 loss: 0.426933  [  224/  265]
train() client id: f_00001-12-7 loss: 0.377953  [  256/  265]
train() client id: f_00001-13-0 loss: 0.390637  [   32/  265]
train() client id: f_00001-13-1 loss: 0.352238  [   64/  265]
train() client id: f_00001-13-2 loss: 0.358649  [   96/  265]
train() client id: f_00001-13-3 loss: 0.308863  [  128/  265]
train() client id: f_00001-13-4 loss: 0.467327  [  160/  265]
train() client id: f_00001-13-5 loss: 0.373734  [  192/  265]
train() client id: f_00001-13-6 loss: 0.335711  [  224/  265]
train() client id: f_00001-13-7 loss: 0.350943  [  256/  265]
train() client id: f_00002-0-0 loss: 0.854021  [   32/  124]
train() client id: f_00002-0-1 loss: 1.072508  [   64/  124]
train() client id: f_00002-0-2 loss: 0.665961  [   96/  124]
train() client id: f_00002-1-0 loss: 0.753392  [   32/  124]
train() client id: f_00002-1-1 loss: 0.683643  [   64/  124]
train() client id: f_00002-1-2 loss: 0.828876  [   96/  124]
train() client id: f_00002-2-0 loss: 0.822995  [   32/  124]
train() client id: f_00002-2-1 loss: 0.732296  [   64/  124]
train() client id: f_00002-2-2 loss: 0.679064  [   96/  124]
train() client id: f_00002-3-0 loss: 0.887091  [   32/  124]
train() client id: f_00002-3-1 loss: 0.685135  [   64/  124]
train() client id: f_00002-3-2 loss: 0.601269  [   96/  124]
train() client id: f_00002-4-0 loss: 0.952114  [   32/  124]
train() client id: f_00002-4-1 loss: 0.562037  [   64/  124]
train() client id: f_00002-4-2 loss: 0.652794  [   96/  124]
train() client id: f_00002-5-0 loss: 0.643524  [   32/  124]
train() client id: f_00002-5-1 loss: 0.723535  [   64/  124]
train() client id: f_00002-5-2 loss: 0.793770  [   96/  124]
train() client id: f_00002-6-0 loss: 0.695655  [   32/  124]
train() client id: f_00002-6-1 loss: 0.656567  [   64/  124]
train() client id: f_00002-6-2 loss: 0.694529  [   96/  124]
train() client id: f_00002-7-0 loss: 0.761547  [   32/  124]
train() client id: f_00002-7-1 loss: 0.734148  [   64/  124]
train() client id: f_00002-7-2 loss: 0.537229  [   96/  124]
train() client id: f_00002-8-0 loss: 0.569801  [   32/  124]
train() client id: f_00002-8-1 loss: 0.811888  [   64/  124]
train() client id: f_00002-8-2 loss: 0.500729  [   96/  124]
train() client id: f_00002-9-0 loss: 0.493850  [   32/  124]
train() client id: f_00002-9-1 loss: 0.577894  [   64/  124]
train() client id: f_00002-9-2 loss: 0.805399  [   96/  124]
train() client id: f_00002-10-0 loss: 0.597353  [   32/  124]
train() client id: f_00002-10-1 loss: 0.507359  [   64/  124]
train() client id: f_00002-10-2 loss: 0.529031  [   96/  124]
train() client id: f_00002-11-0 loss: 0.742894  [   32/  124]
train() client id: f_00002-11-1 loss: 0.710861  [   64/  124]
train() client id: f_00002-11-2 loss: 0.571242  [   96/  124]
train() client id: f_00002-12-0 loss: 0.579632  [   32/  124]
train() client id: f_00002-12-1 loss: 0.585939  [   64/  124]
train() client id: f_00002-12-2 loss: 0.684913  [   96/  124]
train() client id: f_00002-13-0 loss: 0.688823  [   32/  124]
train() client id: f_00002-13-1 loss: 0.552700  [   64/  124]
train() client id: f_00002-13-2 loss: 0.599002  [   96/  124]
train() client id: f_00003-0-0 loss: 1.002861  [   32/   43]
train() client id: f_00003-1-0 loss: 1.016985  [   32/   43]
train() client id: f_00003-2-0 loss: 1.069427  [   32/   43]
train() client id: f_00003-3-0 loss: 1.052573  [   32/   43]
train() client id: f_00003-4-0 loss: 1.076658  [   32/   43]
train() client id: f_00003-5-0 loss: 1.003439  [   32/   43]
train() client id: f_00003-6-0 loss: 1.067790  [   32/   43]
train() client id: f_00003-7-0 loss: 0.949240  [   32/   43]
train() client id: f_00003-8-0 loss: 1.033018  [   32/   43]
train() client id: f_00003-9-0 loss: 1.031837  [   32/   43]
train() client id: f_00003-10-0 loss: 1.090278  [   32/   43]
train() client id: f_00003-11-0 loss: 1.057679  [   32/   43]
train() client id: f_00003-12-0 loss: 1.026114  [   32/   43]
train() client id: f_00003-13-0 loss: 1.026013  [   32/   43]
train() client id: f_00004-0-0 loss: 1.048213  [   32/  306]
train() client id: f_00004-0-1 loss: 0.993556  [   64/  306]
train() client id: f_00004-0-2 loss: 0.921491  [   96/  306]
train() client id: f_00004-0-3 loss: 0.882030  [  128/  306]
train() client id: f_00004-0-4 loss: 0.920943  [  160/  306]
train() client id: f_00004-0-5 loss: 0.892274  [  192/  306]
train() client id: f_00004-0-6 loss: 0.928318  [  224/  306]
train() client id: f_00004-0-7 loss: 1.159703  [  256/  306]
train() client id: f_00004-0-8 loss: 0.813885  [  288/  306]
train() client id: f_00004-1-0 loss: 0.783011  [   32/  306]
train() client id: f_00004-1-1 loss: 0.845219  [   64/  306]
train() client id: f_00004-1-2 loss: 0.985531  [   96/  306]
train() client id: f_00004-1-3 loss: 0.927362  [  128/  306]
train() client id: f_00004-1-4 loss: 1.138782  [  160/  306]
train() client id: f_00004-1-5 loss: 0.962992  [  192/  306]
train() client id: f_00004-1-6 loss: 0.912115  [  224/  306]
train() client id: f_00004-1-7 loss: 0.916554  [  256/  306]
train() client id: f_00004-1-8 loss: 0.975395  [  288/  306]
train() client id: f_00004-2-0 loss: 0.806868  [   32/  306]
train() client id: f_00004-2-1 loss: 0.871124  [   64/  306]
train() client id: f_00004-2-2 loss: 0.993358  [   96/  306]
train() client id: f_00004-2-3 loss: 1.036145  [  128/  306]
train() client id: f_00004-2-4 loss: 0.995715  [  160/  306]
train() client id: f_00004-2-5 loss: 1.179086  [  192/  306]
train() client id: f_00004-2-6 loss: 1.037691  [  224/  306]
train() client id: f_00004-2-7 loss: 0.926416  [  256/  306]
train() client id: f_00004-2-8 loss: 0.759029  [  288/  306]
train() client id: f_00004-3-0 loss: 1.062566  [   32/  306]
train() client id: f_00004-3-1 loss: 0.969249  [   64/  306]
train() client id: f_00004-3-2 loss: 1.042025  [   96/  306]
train() client id: f_00004-3-3 loss: 0.861415  [  128/  306]
train() client id: f_00004-3-4 loss: 0.926986  [  160/  306]
train() client id: f_00004-3-5 loss: 0.892015  [  192/  306]
train() client id: f_00004-3-6 loss: 0.885807  [  224/  306]
train() client id: f_00004-3-7 loss: 1.004681  [  256/  306]
train() client id: f_00004-3-8 loss: 0.862764  [  288/  306]
train() client id: f_00004-4-0 loss: 0.937321  [   32/  306]
train() client id: f_00004-4-1 loss: 0.860138  [   64/  306]
train() client id: f_00004-4-2 loss: 1.126322  [   96/  306]
train() client id: f_00004-4-3 loss: 1.053292  [  128/  306]
train() client id: f_00004-4-4 loss: 0.769921  [  160/  306]
train() client id: f_00004-4-5 loss: 0.926349  [  192/  306]
train() client id: f_00004-4-6 loss: 0.831959  [  224/  306]
train() client id: f_00004-4-7 loss: 1.003341  [  256/  306]
train() client id: f_00004-4-8 loss: 0.947059  [  288/  306]
train() client id: f_00004-5-0 loss: 0.954378  [   32/  306]
train() client id: f_00004-5-1 loss: 1.043156  [   64/  306]
train() client id: f_00004-5-2 loss: 1.005571  [   96/  306]
train() client id: f_00004-5-3 loss: 0.996361  [  128/  306]
train() client id: f_00004-5-4 loss: 0.865794  [  160/  306]
train() client id: f_00004-5-5 loss: 0.820728  [  192/  306]
train() client id: f_00004-5-6 loss: 0.860061  [  224/  306]
train() client id: f_00004-5-7 loss: 0.930835  [  256/  306]
train() client id: f_00004-5-8 loss: 0.927578  [  288/  306]
train() client id: f_00004-6-0 loss: 0.839293  [   32/  306]
train() client id: f_00004-6-1 loss: 0.913515  [   64/  306]
train() client id: f_00004-6-2 loss: 1.038463  [   96/  306]
train() client id: f_00004-6-3 loss: 0.948107  [  128/  306]
train() client id: f_00004-6-4 loss: 1.035758  [  160/  306]
train() client id: f_00004-6-5 loss: 0.973751  [  192/  306]
train() client id: f_00004-6-6 loss: 0.893184  [  224/  306]
train() client id: f_00004-6-7 loss: 0.928738  [  256/  306]
train() client id: f_00004-6-8 loss: 1.007633  [  288/  306]
train() client id: f_00004-7-0 loss: 0.999867  [   32/  306]
train() client id: f_00004-7-1 loss: 0.991901  [   64/  306]
train() client id: f_00004-7-2 loss: 1.065019  [   96/  306]
train() client id: f_00004-7-3 loss: 0.792553  [  128/  306]
train() client id: f_00004-7-4 loss: 0.912838  [  160/  306]
train() client id: f_00004-7-5 loss: 0.968835  [  192/  306]
train() client id: f_00004-7-6 loss: 0.889084  [  224/  306]
train() client id: f_00004-7-7 loss: 0.954786  [  256/  306]
train() client id: f_00004-7-8 loss: 0.899996  [  288/  306]
train() client id: f_00004-8-0 loss: 0.991828  [   32/  306]
train() client id: f_00004-8-1 loss: 0.940810  [   64/  306]
train() client id: f_00004-8-2 loss: 1.061306  [   96/  306]
train() client id: f_00004-8-3 loss: 0.805369  [  128/  306]
train() client id: f_00004-8-4 loss: 0.989649  [  160/  306]
train() client id: f_00004-8-5 loss: 0.956744  [  192/  306]
train() client id: f_00004-8-6 loss: 0.996489  [  224/  306]
train() client id: f_00004-8-7 loss: 0.862448  [  256/  306]
train() client id: f_00004-8-8 loss: 0.906482  [  288/  306]
train() client id: f_00004-9-0 loss: 0.946017  [   32/  306]
train() client id: f_00004-9-1 loss: 1.036189  [   64/  306]
train() client id: f_00004-9-2 loss: 1.053645  [   96/  306]
train() client id: f_00004-9-3 loss: 0.810295  [  128/  306]
train() client id: f_00004-9-4 loss: 0.875968  [  160/  306]
train() client id: f_00004-9-5 loss: 0.872986  [  192/  306]
train() client id: f_00004-9-6 loss: 0.887957  [  224/  306]
train() client id: f_00004-9-7 loss: 0.927225  [  256/  306]
train() client id: f_00004-9-8 loss: 1.060456  [  288/  306]
train() client id: f_00004-10-0 loss: 0.925705  [   32/  306]
train() client id: f_00004-10-1 loss: 0.852263  [   64/  306]
train() client id: f_00004-10-2 loss: 0.921088  [   96/  306]
train() client id: f_00004-10-3 loss: 0.874365  [  128/  306]
train() client id: f_00004-10-4 loss: 0.999512  [  160/  306]
train() client id: f_00004-10-5 loss: 0.888986  [  192/  306]
train() client id: f_00004-10-6 loss: 0.952199  [  224/  306]
train() client id: f_00004-10-7 loss: 1.118076  [  256/  306]
train() client id: f_00004-10-8 loss: 0.939703  [  288/  306]
train() client id: f_00004-11-0 loss: 0.940497  [   32/  306]
train() client id: f_00004-11-1 loss: 0.867056  [   64/  306]
train() client id: f_00004-11-2 loss: 1.071209  [   96/  306]
train() client id: f_00004-11-3 loss: 0.833979  [  128/  306]
train() client id: f_00004-11-4 loss: 0.926320  [  160/  306]
train() client id: f_00004-11-5 loss: 0.958446  [  192/  306]
train() client id: f_00004-11-6 loss: 0.907954  [  224/  306]
train() client id: f_00004-11-7 loss: 0.832688  [  256/  306]
train() client id: f_00004-11-8 loss: 0.997272  [  288/  306]
train() client id: f_00004-12-0 loss: 0.899798  [   32/  306]
train() client id: f_00004-12-1 loss: 0.974640  [   64/  306]
train() client id: f_00004-12-2 loss: 0.807859  [   96/  306]
train() client id: f_00004-12-3 loss: 0.946881  [  128/  306]
train() client id: f_00004-12-4 loss: 1.019560  [  160/  306]
train() client id: f_00004-12-5 loss: 0.892409  [  192/  306]
train() client id: f_00004-12-6 loss: 0.866430  [  224/  306]
train() client id: f_00004-12-7 loss: 0.898667  [  256/  306]
train() client id: f_00004-12-8 loss: 1.019257  [  288/  306]
train() client id: f_00004-13-0 loss: 0.818272  [   32/  306]
train() client id: f_00004-13-1 loss: 0.975510  [   64/  306]
train() client id: f_00004-13-2 loss: 0.901235  [   96/  306]
train() client id: f_00004-13-3 loss: 1.021045  [  128/  306]
train() client id: f_00004-13-4 loss: 0.953513  [  160/  306]
train() client id: f_00004-13-5 loss: 0.963934  [  192/  306]
train() client id: f_00004-13-6 loss: 0.778858  [  224/  306]
train() client id: f_00004-13-7 loss: 0.919980  [  256/  306]
train() client id: f_00004-13-8 loss: 1.015106  [  288/  306]
train() client id: f_00005-0-0 loss: 0.788438  [   32/  146]
train() client id: f_00005-0-1 loss: 0.945342  [   64/  146]
train() client id: f_00005-0-2 loss: 0.764748  [   96/  146]
train() client id: f_00005-0-3 loss: 0.794581  [  128/  146]
train() client id: f_00005-1-0 loss: 0.912634  [   32/  146]
train() client id: f_00005-1-1 loss: 0.871319  [   64/  146]
train() client id: f_00005-1-2 loss: 0.733636  [   96/  146]
train() client id: f_00005-1-3 loss: 0.759448  [  128/  146]
train() client id: f_00005-2-0 loss: 0.753802  [   32/  146]
train() client id: f_00005-2-1 loss: 0.722948  [   64/  146]
train() client id: f_00005-2-2 loss: 0.651011  [   96/  146]
train() client id: f_00005-2-3 loss: 0.945063  [  128/  146]
train() client id: f_00005-3-0 loss: 0.739916  [   32/  146]
train() client id: f_00005-3-1 loss: 0.544423  [   64/  146]
train() client id: f_00005-3-2 loss: 0.925541  [   96/  146]
train() client id: f_00005-3-3 loss: 0.771107  [  128/  146]
train() client id: f_00005-4-0 loss: 0.594585  [   32/  146]
train() client id: f_00005-4-1 loss: 0.918331  [   64/  146]
train() client id: f_00005-4-2 loss: 0.656821  [   96/  146]
train() client id: f_00005-4-3 loss: 0.867333  [  128/  146]
train() client id: f_00005-5-0 loss: 0.632990  [   32/  146]
train() client id: f_00005-5-1 loss: 0.692759  [   64/  146]
train() client id: f_00005-5-2 loss: 0.640194  [   96/  146]
train() client id: f_00005-5-3 loss: 0.763661  [  128/  146]
train() client id: f_00005-6-0 loss: 0.538921  [   32/  146]
train() client id: f_00005-6-1 loss: 0.666911  [   64/  146]
train() client id: f_00005-6-2 loss: 0.665236  [   96/  146]
train() client id: f_00005-6-3 loss: 0.748854  [  128/  146]
train() client id: f_00005-7-0 loss: 0.587327  [   32/  146]
train() client id: f_00005-7-1 loss: 0.633913  [   64/  146]
train() client id: f_00005-7-2 loss: 0.745932  [   96/  146]
train() client id: f_00005-7-3 loss: 0.656202  [  128/  146]
train() client id: f_00005-8-0 loss: 0.602771  [   32/  146]
train() client id: f_00005-8-1 loss: 0.875505  [   64/  146]
train() client id: f_00005-8-2 loss: 0.581457  [   96/  146]
train() client id: f_00005-8-3 loss: 0.624285  [  128/  146]
train() client id: f_00005-9-0 loss: 0.712964  [   32/  146]
train() client id: f_00005-9-1 loss: 0.590687  [   64/  146]
train() client id: f_00005-9-2 loss: 0.760315  [   96/  146]
train() client id: f_00005-9-3 loss: 0.591862  [  128/  146]
train() client id: f_00005-10-0 loss: 0.515710  [   32/  146]
train() client id: f_00005-10-1 loss: 0.536697  [   64/  146]
train() client id: f_00005-10-2 loss: 0.642565  [   96/  146]
train() client id: f_00005-10-3 loss: 0.947656  [  128/  146]
train() client id: f_00005-11-0 loss: 0.718131  [   32/  146]
train() client id: f_00005-11-1 loss: 0.621924  [   64/  146]
train() client id: f_00005-11-2 loss: 0.738119  [   96/  146]
train() client id: f_00005-11-3 loss: 0.487549  [  128/  146]
train() client id: f_00005-12-0 loss: 0.612719  [   32/  146]
train() client id: f_00005-12-1 loss: 0.576287  [   64/  146]
train() client id: f_00005-12-2 loss: 0.826311  [   96/  146]
train() client id: f_00005-12-3 loss: 0.648079  [  128/  146]
train() client id: f_00005-13-0 loss: 0.499041  [   32/  146]
train() client id: f_00005-13-1 loss: 0.656111  [   64/  146]
train() client id: f_00005-13-2 loss: 0.655880  [   96/  146]
train() client id: f_00005-13-3 loss: 0.751256  [  128/  146]
train() client id: f_00006-0-0 loss: 0.919686  [   32/   54]
train() client id: f_00006-1-0 loss: 0.906677  [   32/   54]
train() client id: f_00006-2-0 loss: 0.933796  [   32/   54]
train() client id: f_00006-3-0 loss: 0.932425  [   32/   54]
train() client id: f_00006-4-0 loss: 0.949808  [   32/   54]
train() client id: f_00006-5-0 loss: 0.886115  [   32/   54]
train() client id: f_00006-6-0 loss: 0.926917  [   32/   54]
train() client id: f_00006-7-0 loss: 0.960003  [   32/   54]
train() client id: f_00006-8-0 loss: 0.928495  [   32/   54]
train() client id: f_00006-9-0 loss: 0.948468  [   32/   54]
train() client id: f_00006-10-0 loss: 0.950225  [   32/   54]
train() client id: f_00006-11-0 loss: 0.933429  [   32/   54]
train() client id: f_00006-12-0 loss: 0.973618  [   32/   54]
train() client id: f_00006-13-0 loss: 0.960094  [   32/   54]
train() client id: f_00007-0-0 loss: 0.521528  [   32/  179]
train() client id: f_00007-0-1 loss: 0.577709  [   64/  179]
train() client id: f_00007-0-2 loss: 0.628022  [   96/  179]
train() client id: f_00007-0-3 loss: 0.723867  [  128/  179]
train() client id: f_00007-0-4 loss: 0.483812  [  160/  179]
train() client id: f_00007-1-0 loss: 0.516450  [   32/  179]
train() client id: f_00007-1-1 loss: 0.512463  [   64/  179]
train() client id: f_00007-1-2 loss: 0.628433  [   96/  179]
train() client id: f_00007-1-3 loss: 0.622481  [  128/  179]
train() client id: f_00007-1-4 loss: 0.447120  [  160/  179]
train() client id: f_00007-2-0 loss: 0.623931  [   32/  179]
train() client id: f_00007-2-1 loss: 0.457866  [   64/  179]
train() client id: f_00007-2-2 loss: 0.389200  [   96/  179]
train() client id: f_00007-2-3 loss: 0.467487  [  128/  179]
train() client id: f_00007-2-4 loss: 0.649451  [  160/  179]
train() client id: f_00007-3-0 loss: 0.454824  [   32/  179]
train() client id: f_00007-3-1 loss: 0.570165  [   64/  179]
train() client id: f_00007-3-2 loss: 0.504780  [   96/  179]
train() client id: f_00007-3-3 loss: 0.471865  [  128/  179]
train() client id: f_00007-3-4 loss: 0.460931  [  160/  179]
train() client id: f_00007-4-0 loss: 0.457377  [   32/  179]
train() client id: f_00007-4-1 loss: 0.416067  [   64/  179]
train() client id: f_00007-4-2 loss: 0.606768  [   96/  179]
train() client id: f_00007-4-3 loss: 0.448678  [  128/  179]
train() client id: f_00007-4-4 loss: 0.597945  [  160/  179]
train() client id: f_00007-5-0 loss: 0.440792  [   32/  179]
train() client id: f_00007-5-1 loss: 0.481405  [   64/  179]
train() client id: f_00007-5-2 loss: 0.364254  [   96/  179]
train() client id: f_00007-5-3 loss: 0.553888  [  128/  179]
train() client id: f_00007-5-4 loss: 0.540075  [  160/  179]
train() client id: f_00007-6-0 loss: 0.390468  [   32/  179]
train() client id: f_00007-6-1 loss: 0.534809  [   64/  179]
train() client id: f_00007-6-2 loss: 0.540972  [   96/  179]
train() client id: f_00007-6-3 loss: 0.437822  [  128/  179]
train() client id: f_00007-6-4 loss: 0.434213  [  160/  179]
train() client id: f_00007-7-0 loss: 0.413464  [   32/  179]
train() client id: f_00007-7-1 loss: 0.483581  [   64/  179]
train() client id: f_00007-7-2 loss: 0.533296  [   96/  179]
train() client id: f_00007-7-3 loss: 0.337078  [  128/  179]
train() client id: f_00007-7-4 loss: 0.421170  [  160/  179]
train() client id: f_00007-8-0 loss: 0.454645  [   32/  179]
train() client id: f_00007-8-1 loss: 0.416617  [   64/  179]
train() client id: f_00007-8-2 loss: 0.359365  [   96/  179]
train() client id: f_00007-8-3 loss: 0.490491  [  128/  179]
train() client id: f_00007-8-4 loss: 0.592711  [  160/  179]
train() client id: f_00007-9-0 loss: 0.422135  [   32/  179]
train() client id: f_00007-9-1 loss: 0.389323  [   64/  179]
train() client id: f_00007-9-2 loss: 0.560905  [   96/  179]
train() client id: f_00007-9-3 loss: 0.437496  [  128/  179]
train() client id: f_00007-9-4 loss: 0.400214  [  160/  179]
train() client id: f_00007-10-0 loss: 0.330977  [   32/  179]
train() client id: f_00007-10-1 loss: 0.443753  [   64/  179]
train() client id: f_00007-10-2 loss: 0.475524  [   96/  179]
train() client id: f_00007-10-3 loss: 0.432634  [  128/  179]
train() client id: f_00007-10-4 loss: 0.482050  [  160/  179]
train() client id: f_00007-11-0 loss: 0.652461  [   32/  179]
train() client id: f_00007-11-1 loss: 0.334557  [   64/  179]
train() client id: f_00007-11-2 loss: 0.537293  [   96/  179]
train() client id: f_00007-11-3 loss: 0.328991  [  128/  179]
train() client id: f_00007-11-4 loss: 0.424312  [  160/  179]
train() client id: f_00007-12-0 loss: 0.509141  [   32/  179]
train() client id: f_00007-12-1 loss: 0.451702  [   64/  179]
train() client id: f_00007-12-2 loss: 0.312903  [   96/  179]
train() client id: f_00007-12-3 loss: 0.477226  [  128/  179]
train() client id: f_00007-12-4 loss: 0.398375  [  160/  179]
train() client id: f_00007-13-0 loss: 0.656077  [   32/  179]
train() client id: f_00007-13-1 loss: 0.349580  [   64/  179]
train() client id: f_00007-13-2 loss: 0.440137  [   96/  179]
train() client id: f_00007-13-3 loss: 0.387011  [  128/  179]
train() client id: f_00007-13-4 loss: 0.315087  [  160/  179]
train() client id: f_00008-0-0 loss: 1.024532  [   32/  130]
train() client id: f_00008-0-1 loss: 1.038873  [   64/  130]
train() client id: f_00008-0-2 loss: 0.908412  [   96/  130]
train() client id: f_00008-0-3 loss: 0.906747  [  128/  130]
train() client id: f_00008-1-0 loss: 1.072886  [   32/  130]
train() client id: f_00008-1-1 loss: 0.853200  [   64/  130]
train() client id: f_00008-1-2 loss: 0.972416  [   96/  130]
train() client id: f_00008-1-3 loss: 0.990035  [  128/  130]
train() client id: f_00008-2-0 loss: 0.858593  [   32/  130]
train() client id: f_00008-2-1 loss: 0.977935  [   64/  130]
train() client id: f_00008-2-2 loss: 1.003433  [   96/  130]
train() client id: f_00008-2-3 loss: 1.004797  [  128/  130]
train() client id: f_00008-3-0 loss: 1.010170  [   32/  130]
train() client id: f_00008-3-1 loss: 0.968818  [   64/  130]
train() client id: f_00008-3-2 loss: 0.972080  [   96/  130]
train() client id: f_00008-3-3 loss: 0.946944  [  128/  130]
train() client id: f_00008-4-0 loss: 1.034754  [   32/  130]
train() client id: f_00008-4-1 loss: 0.956025  [   64/  130]
train() client id: f_00008-4-2 loss: 1.013883  [   96/  130]
train() client id: f_00008-4-3 loss: 0.904655  [  128/  130]
train() client id: f_00008-5-0 loss: 0.925472  [   32/  130]
train() client id: f_00008-5-1 loss: 1.003657  [   64/  130]
train() client id: f_00008-5-2 loss: 0.942755  [   96/  130]
train() client id: f_00008-5-3 loss: 1.024092  [  128/  130]
train() client id: f_00008-6-0 loss: 0.963709  [   32/  130]
train() client id: f_00008-6-1 loss: 0.945482  [   64/  130]
train() client id: f_00008-6-2 loss: 1.078451  [   96/  130]
train() client id: f_00008-6-3 loss: 0.891589  [  128/  130]
train() client id: f_00008-7-0 loss: 0.880623  [   32/  130]
train() client id: f_00008-7-1 loss: 1.005795  [   64/  130]
train() client id: f_00008-7-2 loss: 0.959270  [   96/  130]
train() client id: f_00008-7-3 loss: 1.059609  [  128/  130]
train() client id: f_00008-8-0 loss: 1.020806  [   32/  130]
train() client id: f_00008-8-1 loss: 1.060999  [   64/  130]
train() client id: f_00008-8-2 loss: 0.825253  [   96/  130]
train() client id: f_00008-8-3 loss: 0.971395  [  128/  130]
train() client id: f_00008-9-0 loss: 0.969284  [   32/  130]
train() client id: f_00008-9-1 loss: 1.086467  [   64/  130]
train() client id: f_00008-9-2 loss: 0.880303  [   96/  130]
train() client id: f_00008-9-3 loss: 0.989771  [  128/  130]
train() client id: f_00008-10-0 loss: 0.973277  [   32/  130]
train() client id: f_00008-10-1 loss: 0.980749  [   64/  130]
train() client id: f_00008-10-2 loss: 0.995646  [   96/  130]
train() client id: f_00008-10-3 loss: 0.991991  [  128/  130]
train() client id: f_00008-11-0 loss: 1.024133  [   32/  130]
train() client id: f_00008-11-1 loss: 0.902979  [   64/  130]
train() client id: f_00008-11-2 loss: 0.945449  [   96/  130]
train() client id: f_00008-11-3 loss: 1.085564  [  128/  130]
train() client id: f_00008-12-0 loss: 0.962349  [   32/  130]
train() client id: f_00008-12-1 loss: 0.944814  [   64/  130]
train() client id: f_00008-12-2 loss: 1.127662  [   96/  130]
train() client id: f_00008-12-3 loss: 0.876783  [  128/  130]
train() client id: f_00008-13-0 loss: 0.812310  [   32/  130]
train() client id: f_00008-13-1 loss: 0.978500  [   64/  130]
train() client id: f_00008-13-2 loss: 1.022156  [   96/  130]
train() client id: f_00008-13-3 loss: 1.115952  [  128/  130]
train() client id: f_00009-0-0 loss: 1.292585  [   32/  118]
train() client id: f_00009-0-1 loss: 1.318896  [   64/  118]
train() client id: f_00009-0-2 loss: 1.239842  [   96/  118]
train() client id: f_00009-1-0 loss: 1.277139  [   32/  118]
train() client id: f_00009-1-1 loss: 1.167122  [   64/  118]
train() client id: f_00009-1-2 loss: 1.164618  [   96/  118]
train() client id: f_00009-2-0 loss: 1.210729  [   32/  118]
train() client id: f_00009-2-1 loss: 1.123378  [   64/  118]
train() client id: f_00009-2-2 loss: 1.180720  [   96/  118]
train() client id: f_00009-3-0 loss: 1.028561  [   32/  118]
train() client id: f_00009-3-1 loss: 1.223632  [   64/  118]
train() client id: f_00009-3-2 loss: 1.207580  [   96/  118]
train() client id: f_00009-4-0 loss: 1.164611  [   32/  118]
train() client id: f_00009-4-1 loss: 1.131644  [   64/  118]
train() client id: f_00009-4-2 loss: 1.080947  [   96/  118]
train() client id: f_00009-5-0 loss: 1.079604  [   32/  118]
train() client id: f_00009-5-1 loss: 1.062746  [   64/  118]
train() client id: f_00009-5-2 loss: 1.124275  [   96/  118]
train() client id: f_00009-6-0 loss: 1.173863  [   32/  118]
train() client id: f_00009-6-1 loss: 0.996798  [   64/  118]
train() client id: f_00009-6-2 loss: 1.086904  [   96/  118]
train() client id: f_00009-7-0 loss: 0.935578  [   32/  118]
train() client id: f_00009-7-1 loss: 0.987513  [   64/  118]
train() client id: f_00009-7-2 loss: 1.122669  [   96/  118]
train() client id: f_00009-8-0 loss: 1.029294  [   32/  118]
train() client id: f_00009-8-1 loss: 1.032120  [   64/  118]
train() client id: f_00009-8-2 loss: 1.091026  [   96/  118]
train() client id: f_00009-9-0 loss: 1.086053  [   32/  118]
train() client id: f_00009-9-1 loss: 0.945783  [   64/  118]
train() client id: f_00009-9-2 loss: 1.088031  [   96/  118]
train() client id: f_00009-10-0 loss: 0.954703  [   32/  118]
train() client id: f_00009-10-1 loss: 1.009349  [   64/  118]
train() client id: f_00009-10-2 loss: 0.944879  [   96/  118]
train() client id: f_00009-11-0 loss: 1.112080  [   32/  118]
train() client id: f_00009-11-1 loss: 0.831330  [   64/  118]
train() client id: f_00009-11-2 loss: 1.079656  [   96/  118]
train() client id: f_00009-12-0 loss: 0.943212  [   32/  118]
train() client id: f_00009-12-1 loss: 1.079212  [   64/  118]
train() client id: f_00009-12-2 loss: 0.845159  [   96/  118]
train() client id: f_00009-13-0 loss: 0.959942  [   32/  118]
train() client id: f_00009-13-1 loss: 0.820247  [   64/  118]
train() client id: f_00009-13-2 loss: 1.244753  [   96/  118]
At round 1 accuracy: 0.6021220159151194
At round 1 training accuracy: 0.5606975184439973
At round 1 training loss: 0.9126822436952549
update_location
xs = 20.471708 -93.998411 65.045120 84.056472 -185.103519 -120.217951 12.784040 -21.624259 -1.680116 -75.304393 
ys = 17.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 26.875078 -19.177652 77.845030 -0.998518 
xs mean: -31.55713084058713
ys mean: 13.371751218646875
dists_uav = 103.578121 138.121994 119.300510 132.566057 210.596151 157.314046 104.334565 104.093184 126.738595 125.186855 
uav_gains = -100.381727 -103.508076 -101.916255 -103.061666 -108.360204 -104.927672 -100.460735 -100.435586 -102.573182 -102.439356 
uav_gains_db_mean: -102.8064458752069
dists_bs = 250.973583 178.839867 296.287341 300.587485 165.957650 199.841121 239.173239 247.445102 198.692905 202.273806 
bs_gains = -106.756643 -102.636061 -108.775026 -108.950244 -101.726983 -103.986235 -106.171012 -106.584467 -103.916165 -104.133369 
bs_gains_db_mean: -105.36362053754523
Round 2
-------------------------------
ene_coms = [0.00640386 0.0070754  0.0068464  0.00721803 0.00678859 0.00754812
 0.00642526 0.00641844 0.00752207 0.00760339]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.73268048 22.39513271 10.5774354   3.8025214  25.82166843 12.43982938
  4.70195404 15.17280756 11.0988124  10.09585088]
obj_prev = 126.83869269644141
eta_min = 1.4539061136271103e-09	eta_max = 0.9287624284518677
af = 26.850173107417966	bf = 1.9206890865765618	zeta = 29.535190418159765	eta = 0.9090909090909091
af = 26.850173107417966	bf = 1.9206890865765618	zeta = 49.277623045439974	eta = 0.5448755732933553
af = 26.850173107417966	bf = 1.9206890865765618	zeta = 40.04191929600779	eta = 0.6705516014087504
af = 26.850173107417966	bf = 1.9206890865765618	zeta = 38.4020224739835	eta = 0.6991864328397899
af = 26.850173107417966	bf = 1.9206890865765618	zeta = 38.325058423670015	eta = 0.7005905329771129
af = 26.850173107417966	bf = 1.9206890865765618	zeta = 38.324878080395166	eta = 0.7005938297075234
eta = 0.7005938297075234
ene_coms = [0.00640386 0.0070754  0.0068464  0.00721803 0.00678859 0.00754812
 0.00642526 0.00641844 0.00752207 0.00760339]
ene_comp = [0.02936199 0.06175339 0.02889593 0.01002036 0.07130769 0.03402262
 0.01258371 0.04171267 0.03029412 0.02749774]
ene_total = [3.28474166 6.32124764 3.2825811  1.58317621 7.17237454 3.81786356
 1.74578721 4.42036835 3.47304477 3.22369304]
ti_comp = [0.26152691 0.25481149 0.25710153 0.25338529 0.25767967 0.25008435
 0.2613129  0.26138117 0.25034482 0.24953164]
ti_coms = [0.06403863 0.07075405 0.06846401 0.07218025 0.06788587 0.07548118
 0.06425264 0.06418437 0.07522072 0.07603389]
t_total = [29.89999161 29.89999161 29.89999161 29.89999161 29.89999161 29.89999161
 29.89999161 29.89999161 29.89999161 29.89999161]
ene_coms = [0.00640386 0.0070754  0.0068464  0.00721803 0.00678859 0.00754812
 0.00642526 0.00641844 0.00752207 0.00760339]
ene_comp = [2.31314942e-05 2.26685850e-04 2.28129029e-05 9.79416437e-07
 3.41294104e-04 3.93559438e-05 1.82383016e-06 6.63949879e-05
 2.77253883e-05 2.08698620e-05]
ene_total = [0.59025622 0.67062518 0.63086973 0.66299455 0.65480944 0.69683487
 0.59026478 0.59556798 0.69337458 0.70021321]
optimize_network iter = 0 obj = 6.485810544740149
eta = 0.7005938297075234
freqs = [5.61356948e+07 1.21174657e+08 5.61955550e+07 1.97729741e+07
 1.38364988e+08 6.80222932e+07 2.40778580e+07 7.97927924e+07
 6.05047783e+07 5.50986954e+07]
eta_min = 0.6518461648860863	eta_max = 0.7005938297075197
af = 0.059586853532093496	bf = 1.9206890865765618	zeta = 0.06554553888530285	eta = 0.9090909090909091
af = 0.059586853532093496	bf = 1.9206890865765618	zeta = 21.190051523307467	eta = 0.0028120202287640703
af = 0.059586853532093496	bf = 1.9206890865765618	zeta = 2.277096693969931	eta = 0.026167906567115828
af = 0.059586853532093496	bf = 1.9206890865765618	zeta = 2.1952215483850526	eta = 0.02714389059087428
af = 0.059586853532093496	bf = 1.9206890865765618	zeta = 2.1951809804622857	eta = 0.027144392222068648
eta = 0.027144392222068648
ene_coms = [0.00640386 0.0070754  0.0068464  0.00721803 0.00678859 0.00754812
 0.00642526 0.00641844 0.00752207 0.00760339]
ene_comp = [2.34455792e-04 2.29763845e-03 2.31226620e-04 9.92715191e-06
 3.45928278e-03 3.98903283e-04 1.84859457e-05 6.72965151e-04
 2.81018503e-04 2.11532380e-04]
ene_total = [0.18763036 0.26492663 0.20004731 0.20429619 0.28965338 0.2246205
 0.18213092 0.20043663 0.22055231 0.22088674]
ti_comp = [0.31453369 0.30781828 0.31010831 0.30639207 0.31068645 0.30309114
 0.31431969 0.31438796 0.30335161 0.30253843]
ti_coms = [0.06403863 0.07075405 0.06846401 0.07218025 0.06788587 0.07548118
 0.06425264 0.06418437 0.07522072 0.07603389]
t_total = [29.89999161 29.89999161 29.89999161 29.89999161 29.89999161 29.89999161
 29.89999161 29.89999161 29.89999161 29.89999161]
ene_coms = [0.00640386 0.0070754  0.0068464  0.00721803 0.00678859 0.00754812
 0.00642526 0.00641844 0.00752207 0.00760339]
ene_comp = [2.78198310e-05 2.70225306e-04 2.72781831e-05 1.16527256e-06
 4.08410451e-04 4.66110929e-05 2.19288230e-06 7.98370510e-05
 3.28484328e-05 2.46980194e-05]
ene_total = [0.50798025 0.58016464 0.54288954 0.57017832 0.56842551 0.59983874
 0.50764647 0.51323964 0.59669455 0.60247339]
optimize_network iter = 1 obj = 5.5895310556384
eta = 0.6518461648860863
freqs = [5.61356948e+07 1.20638854e+08 5.60330172e+07 1.96664813e+07
 1.38017718e+08 6.75017988e+07 2.40745352e+07 7.97852957e+07
 6.00527337e+07 5.46559178e+07]
eta_min = 0.6518461648860905	eta_max = 0.6518461648860684
af = 0.05918397693800023	bf = 1.9206890865765618	zeta = 0.06510237463180026	eta = 0.9090909090909091
af = 0.05918397693800023	bf = 1.9206890865765618	zeta = 21.18962914266063	eta = 0.00279306336791173
af = 0.05918397693800023	bf = 1.9206890865765618	zeta = 2.2750817939207466	eta = 0.026013999626802838
af = 0.05918397693800023	bf = 1.9206890865765618	zeta = 2.193726137382963	eta = 0.02697874448840939
af = 0.05918397693800023	bf = 1.9206890865765618	zeta = 2.1936862950875486	eta = 0.026979234483314414
eta = 0.026979234483314414
ene_coms = [0.00640386 0.0070754  0.0068464  0.00721803 0.00678859 0.00754812
 0.00642526 0.00641844 0.00752207 0.00760339]
ene_comp = [2.34852534e-04 2.28121795e-03 2.30279991e-04 9.83712708e-06
 3.44776463e-03 3.93486692e-04 1.85121169e-05 6.73977269e-04
 2.77303542e-04 2.08498479e-04]
ene_total = [0.18760972 0.26441762 0.1999866  0.20425897 0.28927871 0.22442931
 0.18210074 0.20043121 0.22040989 0.22076351]
ti_comp = [0.31453369 0.30781828 0.31010831 0.30639207 0.31068645 0.30309114
 0.31431969 0.31438796 0.30335161 0.30253843]
ti_coms = [0.06403863 0.07075405 0.06846401 0.07218025 0.06788587 0.07548118
 0.06425264 0.06418437 0.07522072 0.07603389]
t_total = [29.89999161 29.89999161 29.89999161 29.89999161 29.89999161 29.89999161
 29.89999161 29.89999161 29.89999161 29.89999161]
ene_coms = [0.00640386 0.0070754  0.0068464  0.00721803 0.00678859 0.00754812
 0.00642526 0.00641844 0.00752207 0.00760339]
ene_comp = [2.78198310e-05 2.70225306e-04 2.72781831e-05 1.16527256e-06
 4.08410451e-04 4.66110929e-05 2.19288230e-06 7.98370510e-05
 3.28484328e-05 2.46980194e-05]
ene_total = [0.50798025 0.58016464 0.54288954 0.57017832 0.56842551 0.59983874
 0.50764647 0.51323964 0.59669455 0.60247339]
optimize_network iter = 2 obj = 5.589531055638465
eta = 0.6518461648860905
freqs = [5.61356948e+07 1.20638854e+08 5.60330172e+07 1.96664813e+07
 1.38017718e+08 6.75017988e+07 2.40745352e+07 7.97852957e+07
 6.00527337e+07 5.46559178e+07]
Done!
ene_coms = [0.00640386 0.0070754  0.0068464  0.00721803 0.00678859 0.00754812
 0.00642526 0.00641844 0.00752207 0.00760339]
ene_comp = [2.77937272e-05 2.69971749e-04 2.72525875e-05 1.16417916e-06
 4.08027232e-04 4.65673568e-05 2.19082468e-06 7.97621383e-05
 3.28176105e-05 2.46748448e-05]
ene_total = [0.00643166 0.00734538 0.00687365 0.00721919 0.00719661 0.00759469
 0.00642745 0.0064982  0.00755489 0.00762806]
At round 2 energy consumption: 0.07076978336921294
At round 2 eta: 0.6518461648860905
At round 2 a_n: 27.49751116231574
At round 2 local rounds: 14.013148805538286
At round 2 global rounds: 78.98092276742854
gradient difference: 0.3097192049026489
train() client id: f_00000-0-0 loss: 1.201671  [   32/  126]
train() client id: f_00000-0-1 loss: 1.139739  [   64/  126]
train() client id: f_00000-0-2 loss: 1.148785  [   96/  126]
train() client id: f_00000-1-0 loss: 1.084949  [   32/  126]
train() client id: f_00000-1-1 loss: 1.199647  [   64/  126]
train() client id: f_00000-1-2 loss: 1.068716  [   96/  126]
train() client id: f_00000-2-0 loss: 1.093872  [   32/  126]
train() client id: f_00000-2-1 loss: 1.055256  [   64/  126]
train() client id: f_00000-2-2 loss: 0.939937  [   96/  126]
train() client id: f_00000-3-0 loss: 1.055595  [   32/  126]
train() client id: f_00000-3-1 loss: 1.059420  [   64/  126]
train() client id: f_00000-3-2 loss: 1.027650  [   96/  126]
train() client id: f_00000-4-0 loss: 1.040573  [   32/  126]
train() client id: f_00000-4-1 loss: 0.940012  [   64/  126]
train() client id: f_00000-4-2 loss: 0.917614  [   96/  126]
train() client id: f_00000-5-0 loss: 0.945748  [   32/  126]
train() client id: f_00000-5-1 loss: 0.989641  [   64/  126]
train() client id: f_00000-5-2 loss: 0.926588  [   96/  126]
train() client id: f_00000-6-0 loss: 0.879354  [   32/  126]
train() client id: f_00000-6-1 loss: 1.053524  [   64/  126]
train() client id: f_00000-6-2 loss: 0.934816  [   96/  126]
train() client id: f_00000-7-0 loss: 0.958212  [   32/  126]
train() client id: f_00000-7-1 loss: 0.918756  [   64/  126]
train() client id: f_00000-7-2 loss: 0.929394  [   96/  126]
train() client id: f_00000-8-0 loss: 1.003980  [   32/  126]
train() client id: f_00000-8-1 loss: 0.870239  [   64/  126]
train() client id: f_00000-8-2 loss: 0.926647  [   96/  126]
train() client id: f_00000-9-0 loss: 0.883554  [   32/  126]
train() client id: f_00000-9-1 loss: 1.078774  [   64/  126]
train() client id: f_00000-9-2 loss: 0.945401  [   96/  126]
train() client id: f_00000-10-0 loss: 0.852644  [   32/  126]
train() client id: f_00000-10-1 loss: 0.796573  [   64/  126]
train() client id: f_00000-10-2 loss: 1.037007  [   96/  126]
train() client id: f_00000-11-0 loss: 0.888133  [   32/  126]
train() client id: f_00000-11-1 loss: 0.939120  [   64/  126]
train() client id: f_00000-11-2 loss: 0.949841  [   96/  126]
train() client id: f_00000-12-0 loss: 1.148682  [   32/  126]
train() client id: f_00000-12-1 loss: 0.951764  [   64/  126]
train() client id: f_00000-12-2 loss: 0.866755  [   96/  126]
train() client id: f_00000-13-0 loss: 0.917746  [   32/  126]
train() client id: f_00000-13-1 loss: 1.004918  [   64/  126]
train() client id: f_00000-13-2 loss: 0.913820  [   96/  126]
train() client id: f_00001-0-0 loss: 0.818860  [   32/  265]
train() client id: f_00001-0-1 loss: 0.750309  [   64/  265]
train() client id: f_00001-0-2 loss: 0.648878  [   96/  265]
train() client id: f_00001-0-3 loss: 0.716264  [  128/  265]
train() client id: f_00001-0-4 loss: 0.723088  [  160/  265]
train() client id: f_00001-0-5 loss: 0.654094  [  192/  265]
train() client id: f_00001-0-6 loss: 0.633608  [  224/  265]
train() client id: f_00001-0-7 loss: 0.679971  [  256/  265]
train() client id: f_00001-1-0 loss: 0.698397  [   32/  265]
train() client id: f_00001-1-1 loss: 0.771469  [   64/  265]
train() client id: f_00001-1-2 loss: 0.667099  [   96/  265]
train() client id: f_00001-1-3 loss: 0.657213  [  128/  265]
train() client id: f_00001-1-4 loss: 0.645506  [  160/  265]
train() client id: f_00001-1-5 loss: 0.574770  [  192/  265]
train() client id: f_00001-1-6 loss: 0.612179  [  224/  265]
train() client id: f_00001-1-7 loss: 0.678915  [  256/  265]
train() client id: f_00001-2-0 loss: 0.627804  [   32/  265]
train() client id: f_00001-2-1 loss: 0.674380  [   64/  265]
train() client id: f_00001-2-2 loss: 0.648772  [   96/  265]
train() client id: f_00001-2-3 loss: 0.620620  [  128/  265]
train() client id: f_00001-2-4 loss: 0.608973  [  160/  265]
train() client id: f_00001-2-5 loss: 0.527264  [  192/  265]
train() client id: f_00001-2-6 loss: 0.633148  [  224/  265]
train() client id: f_00001-2-7 loss: 0.692481  [  256/  265]
train() client id: f_00001-3-0 loss: 0.583472  [   32/  265]
train() client id: f_00001-3-1 loss: 0.556215  [   64/  265]
train() client id: f_00001-3-2 loss: 0.655625  [   96/  265]
train() client id: f_00001-3-3 loss: 0.633633  [  128/  265]
train() client id: f_00001-3-4 loss: 0.652347  [  160/  265]
train() client id: f_00001-3-5 loss: 0.627970  [  192/  265]
train() client id: f_00001-3-6 loss: 0.546301  [  224/  265]
train() client id: f_00001-3-7 loss: 0.595417  [  256/  265]
train() client id: f_00001-4-0 loss: 0.560644  [   32/  265]
train() client id: f_00001-4-1 loss: 0.572886  [   64/  265]
train() client id: f_00001-4-2 loss: 0.576304  [   96/  265]
train() client id: f_00001-4-3 loss: 0.542346  [  128/  265]
train() client id: f_00001-4-4 loss: 0.573106  [  160/  265]
train() client id: f_00001-4-5 loss: 0.597505  [  192/  265]
train() client id: f_00001-4-6 loss: 0.703711  [  224/  265]
train() client id: f_00001-4-7 loss: 0.586142  [  256/  265]
train() client id: f_00001-5-0 loss: 0.505174  [   32/  265]
train() client id: f_00001-5-1 loss: 0.519299  [   64/  265]
train() client id: f_00001-5-2 loss: 0.608134  [   96/  265]
train() client id: f_00001-5-3 loss: 0.520593  [  128/  265]
train() client id: f_00001-5-4 loss: 0.564424  [  160/  265]
train() client id: f_00001-5-5 loss: 0.622996  [  192/  265]
train() client id: f_00001-5-6 loss: 0.619554  [  224/  265]
train() client id: f_00001-5-7 loss: 0.588214  [  256/  265]
train() client id: f_00001-6-0 loss: 0.653621  [   32/  265]
train() client id: f_00001-6-1 loss: 0.563948  [   64/  265]
train() client id: f_00001-6-2 loss: 0.591461  [   96/  265]
train() client id: f_00001-6-3 loss: 0.558812  [  128/  265]
train() client id: f_00001-6-4 loss: 0.543741  [  160/  265]
train() client id: f_00001-6-5 loss: 0.568656  [  192/  265]
train() client id: f_00001-6-6 loss: 0.569510  [  224/  265]
train() client id: f_00001-6-7 loss: 0.533782  [  256/  265]
train() client id: f_00001-7-0 loss: 0.636461  [   32/  265]
train() client id: f_00001-7-1 loss: 0.578780  [   64/  265]
train() client id: f_00001-7-2 loss: 0.640943  [   96/  265]
train() client id: f_00001-7-3 loss: 0.498818  [  128/  265]
train() client id: f_00001-7-4 loss: 0.538412  [  160/  265]
train() client id: f_00001-7-5 loss: 0.519240  [  192/  265]
train() client id: f_00001-7-6 loss: 0.631694  [  224/  265]
train() client id: f_00001-7-7 loss: 0.478245  [  256/  265]
train() client id: f_00001-8-0 loss: 0.546631  [   32/  265]
train() client id: f_00001-8-1 loss: 0.494132  [   64/  265]
train() client id: f_00001-8-2 loss: 0.621141  [   96/  265]
train() client id: f_00001-8-3 loss: 0.593443  [  128/  265]
train() client id: f_00001-8-4 loss: 0.504614  [  160/  265]
train() client id: f_00001-8-5 loss: 0.573284  [  192/  265]
train() client id: f_00001-8-6 loss: 0.536245  [  224/  265]
train() client id: f_00001-8-7 loss: 0.621759  [  256/  265]
train() client id: f_00001-9-0 loss: 0.665549  [   32/  265]
train() client id: f_00001-9-1 loss: 0.568558  [   64/  265]
train() client id: f_00001-9-2 loss: 0.569450  [   96/  265]
train() client id: f_00001-9-3 loss: 0.547597  [  128/  265]
train() client id: f_00001-9-4 loss: 0.514065  [  160/  265]
train() client id: f_00001-9-5 loss: 0.532397  [  192/  265]
train() client id: f_00001-9-6 loss: 0.560043  [  224/  265]
train() client id: f_00001-9-7 loss: 0.502375  [  256/  265]
train() client id: f_00001-10-0 loss: 0.476099  [   32/  265]
train() client id: f_00001-10-1 loss: 0.524807  [   64/  265]
train() client id: f_00001-10-2 loss: 0.546466  [   96/  265]
train() client id: f_00001-10-3 loss: 0.631839  [  128/  265]
train() client id: f_00001-10-4 loss: 0.500522  [  160/  265]
train() client id: f_00001-10-5 loss: 0.546705  [  192/  265]
train() client id: f_00001-10-6 loss: 0.612126  [  224/  265]
train() client id: f_00001-10-7 loss: 0.576151  [  256/  265]
train() client id: f_00001-11-0 loss: 0.577616  [   32/  265]
train() client id: f_00001-11-1 loss: 0.552634  [   64/  265]
train() client id: f_00001-11-2 loss: 0.521552  [   96/  265]
train() client id: f_00001-11-3 loss: 0.533134  [  128/  265]
train() client id: f_00001-11-4 loss: 0.488123  [  160/  265]
train() client id: f_00001-11-5 loss: 0.523852  [  192/  265]
train() client id: f_00001-11-6 loss: 0.544940  [  224/  265]
train() client id: f_00001-11-7 loss: 0.646436  [  256/  265]
train() client id: f_00001-12-0 loss: 0.602967  [   32/  265]
train() client id: f_00001-12-1 loss: 0.616453  [   64/  265]
train() client id: f_00001-12-2 loss: 0.546102  [   96/  265]
train() client id: f_00001-12-3 loss: 0.570947  [  128/  265]
train() client id: f_00001-12-4 loss: 0.588083  [  160/  265]
train() client id: f_00001-12-5 loss: 0.506203  [  192/  265]
train() client id: f_00001-12-6 loss: 0.514048  [  224/  265]
train() client id: f_00001-12-7 loss: 0.523144  [  256/  265]
train() client id: f_00001-13-0 loss: 0.542031  [   32/  265]
train() client id: f_00001-13-1 loss: 0.567385  [   64/  265]
train() client id: f_00001-13-2 loss: 0.654258  [   96/  265]
train() client id: f_00001-13-3 loss: 0.592745  [  128/  265]
train() client id: f_00001-13-4 loss: 0.582024  [  160/  265]
train() client id: f_00001-13-5 loss: 0.540481  [  192/  265]
train() client id: f_00001-13-6 loss: 0.478826  [  224/  265]
train() client id: f_00001-13-7 loss: 0.511077  [  256/  265]
train() client id: f_00002-0-0 loss: 0.974797  [   32/  124]
train() client id: f_00002-0-1 loss: 0.914238  [   64/  124]
train() client id: f_00002-0-2 loss: 1.051354  [   96/  124]
train() client id: f_00002-1-0 loss: 0.939626  [   32/  124]
train() client id: f_00002-1-1 loss: 0.835099  [   64/  124]
train() client id: f_00002-1-2 loss: 0.945899  [   96/  124]
train() client id: f_00002-2-0 loss: 0.836790  [   32/  124]
train() client id: f_00002-2-1 loss: 1.193367  [   64/  124]
train() client id: f_00002-2-2 loss: 0.864998  [   96/  124]
train() client id: f_00002-3-0 loss: 0.887678  [   32/  124]
train() client id: f_00002-3-1 loss: 0.908025  [   64/  124]
train() client id: f_00002-3-2 loss: 0.907744  [   96/  124]
train() client id: f_00002-4-0 loss: 1.011715  [   32/  124]
train() client id: f_00002-4-1 loss: 0.797488  [   64/  124]
train() client id: f_00002-4-2 loss: 0.863466  [   96/  124]
train() client id: f_00002-5-0 loss: 0.989926  [   32/  124]
train() client id: f_00002-5-1 loss: 0.771304  [   64/  124]
train() client id: f_00002-5-2 loss: 0.738411  [   96/  124]
train() client id: f_00002-6-0 loss: 0.907500  [   32/  124]
train() client id: f_00002-6-1 loss: 0.672711  [   64/  124]
train() client id: f_00002-6-2 loss: 0.955357  [   96/  124]
train() client id: f_00002-7-0 loss: 0.789044  [   32/  124]
train() client id: f_00002-7-1 loss: 0.812086  [   64/  124]
train() client id: f_00002-7-2 loss: 0.855920  [   96/  124]
train() client id: f_00002-8-0 loss: 0.822407  [   32/  124]
train() client id: f_00002-8-1 loss: 0.867204  [   64/  124]
train() client id: f_00002-8-2 loss: 0.926088  [   96/  124]
train() client id: f_00002-9-0 loss: 0.777117  [   32/  124]
train() client id: f_00002-9-1 loss: 0.937221  [   64/  124]
train() client id: f_00002-9-2 loss: 0.844296  [   96/  124]
train() client id: f_00002-10-0 loss: 0.969802  [   32/  124]
train() client id: f_00002-10-1 loss: 0.800575  [   64/  124]
train() client id: f_00002-10-2 loss: 0.683926  [   96/  124]
train() client id: f_00002-11-0 loss: 0.852771  [   32/  124]
train() client id: f_00002-11-1 loss: 0.803712  [   64/  124]
train() client id: f_00002-11-2 loss: 0.678292  [   96/  124]
train() client id: f_00002-12-0 loss: 0.685986  [   32/  124]
train() client id: f_00002-12-1 loss: 0.743840  [   64/  124]
train() client id: f_00002-12-2 loss: 0.729534  [   96/  124]
train() client id: f_00002-13-0 loss: 0.714304  [   32/  124]
train() client id: f_00002-13-1 loss: 0.876369  [   64/  124]
train() client id: f_00002-13-2 loss: 0.957529  [   96/  124]
train() client id: f_00003-0-0 loss: 1.034494  [   32/   43]
train() client id: f_00003-1-0 loss: 1.039386  [   32/   43]
train() client id: f_00003-2-0 loss: 1.039864  [   32/   43]
train() client id: f_00003-3-0 loss: 1.063706  [   32/   43]
train() client id: f_00003-4-0 loss: 1.025840  [   32/   43]
train() client id: f_00003-5-0 loss: 1.020113  [   32/   43]
train() client id: f_00003-6-0 loss: 1.049742  [   32/   43]
train() client id: f_00003-7-0 loss: 1.054047  [   32/   43]
train() client id: f_00003-8-0 loss: 1.012522  [   32/   43]
train() client id: f_00003-9-0 loss: 1.028942  [   32/   43]
train() client id: f_00003-10-0 loss: 1.052646  [   32/   43]
train() client id: f_00003-11-0 loss: 1.097954  [   32/   43]
train() client id: f_00003-12-0 loss: 1.049404  [   32/   43]
train() client id: f_00003-13-0 loss: 1.040154  [   32/   43]
train() client id: f_00004-0-0 loss: 0.783034  [   32/  306]
train() client id: f_00004-0-1 loss: 0.738742  [   64/  306]
train() client id: f_00004-0-2 loss: 0.881707  [   96/  306]
train() client id: f_00004-0-3 loss: 0.857040  [  128/  306]
train() client id: f_00004-0-4 loss: 0.872316  [  160/  306]
train() client id: f_00004-0-5 loss: 0.750607  [  192/  306]
train() client id: f_00004-0-6 loss: 0.879944  [  224/  306]
train() client id: f_00004-0-7 loss: 0.751797  [  256/  306]
train() client id: f_00004-0-8 loss: 0.786997  [  288/  306]
train() client id: f_00004-1-0 loss: 0.808681  [   32/  306]
train() client id: f_00004-1-1 loss: 0.985958  [   64/  306]
train() client id: f_00004-1-2 loss: 0.696285  [   96/  306]
train() client id: f_00004-1-3 loss: 0.742664  [  128/  306]
train() client id: f_00004-1-4 loss: 0.662506  [  160/  306]
train() client id: f_00004-1-5 loss: 0.789970  [  192/  306]
train() client id: f_00004-1-6 loss: 1.038016  [  224/  306]
train() client id: f_00004-1-7 loss: 0.863467  [  256/  306]
train() client id: f_00004-1-8 loss: 0.750374  [  288/  306]
train() client id: f_00004-2-0 loss: 0.847079  [   32/  306]
train() client id: f_00004-2-1 loss: 0.773971  [   64/  306]
train() client id: f_00004-2-2 loss: 0.707671  [   96/  306]
train() client id: f_00004-2-3 loss: 0.850856  [  128/  306]
train() client id: f_00004-2-4 loss: 0.656164  [  160/  306]
train() client id: f_00004-2-5 loss: 0.983005  [  192/  306]
train() client id: f_00004-2-6 loss: 0.881757  [  224/  306]
train() client id: f_00004-2-7 loss: 0.802711  [  256/  306]
train() client id: f_00004-2-8 loss: 0.816235  [  288/  306]
train() client id: f_00004-3-0 loss: 1.003377  [   32/  306]
train() client id: f_00004-3-1 loss: 0.792389  [   64/  306]
train() client id: f_00004-3-2 loss: 0.741466  [   96/  306]
train() client id: f_00004-3-3 loss: 0.709364  [  128/  306]
train() client id: f_00004-3-4 loss: 0.720755  [  160/  306]
train() client id: f_00004-3-5 loss: 0.919978  [  192/  306]
train() client id: f_00004-3-6 loss: 0.782866  [  224/  306]
train() client id: f_00004-3-7 loss: 0.795641  [  256/  306]
train() client id: f_00004-3-8 loss: 0.970185  [  288/  306]
train() client id: f_00004-4-0 loss: 0.838206  [   32/  306]
train() client id: f_00004-4-1 loss: 0.895294  [   64/  306]
train() client id: f_00004-4-2 loss: 0.911777  [   96/  306]
train() client id: f_00004-4-3 loss: 0.705230  [  128/  306]
train() client id: f_00004-4-4 loss: 0.816448  [  160/  306]
train() client id: f_00004-4-5 loss: 0.778995  [  192/  306]
train() client id: f_00004-4-6 loss: 0.878208  [  224/  306]
train() client id: f_00004-4-7 loss: 0.838611  [  256/  306]
train() client id: f_00004-4-8 loss: 0.740899  [  288/  306]
train() client id: f_00004-5-0 loss: 0.904486  [   32/  306]
train() client id: f_00004-5-1 loss: 0.781269  [   64/  306]
train() client id: f_00004-5-2 loss: 0.873952  [   96/  306]
train() client id: f_00004-5-3 loss: 0.930326  [  128/  306]
train() client id: f_00004-5-4 loss: 0.779323  [  160/  306]
train() client id: f_00004-5-5 loss: 0.762637  [  192/  306]
train() client id: f_00004-5-6 loss: 0.687378  [  224/  306]
train() client id: f_00004-5-7 loss: 0.885054  [  256/  306]
train() client id: f_00004-5-8 loss: 0.770394  [  288/  306]
train() client id: f_00004-6-0 loss: 0.891864  [   32/  306]
train() client id: f_00004-6-1 loss: 0.780171  [   64/  306]
train() client id: f_00004-6-2 loss: 0.831369  [   96/  306]
train() client id: f_00004-6-3 loss: 0.856039  [  128/  306]
train() client id: f_00004-6-4 loss: 0.756925  [  160/  306]
train() client id: f_00004-6-5 loss: 0.881701  [  192/  306]
train() client id: f_00004-6-6 loss: 0.811383  [  224/  306]
train() client id: f_00004-6-7 loss: 0.862806  [  256/  306]
train() client id: f_00004-6-8 loss: 0.733244  [  288/  306]
train() client id: f_00004-7-0 loss: 0.870715  [   32/  306]
train() client id: f_00004-7-1 loss: 0.710971  [   64/  306]
train() client id: f_00004-7-2 loss: 0.734393  [   96/  306]
train() client id: f_00004-7-3 loss: 0.775408  [  128/  306]
train() client id: f_00004-7-4 loss: 0.869493  [  160/  306]
train() client id: f_00004-7-5 loss: 0.885422  [  192/  306]
train() client id: f_00004-7-6 loss: 0.866380  [  224/  306]
train() client id: f_00004-7-7 loss: 0.757705  [  256/  306]
train() client id: f_00004-7-8 loss: 0.846066  [  288/  306]
train() client id: f_00004-8-0 loss: 0.873705  [   32/  306]
train() client id: f_00004-8-1 loss: 0.643114  [   64/  306]
train() client id: f_00004-8-2 loss: 0.849071  [   96/  306]
train() client id: f_00004-8-3 loss: 0.908226  [  128/  306]
train() client id: f_00004-8-4 loss: 0.825876  [  160/  306]
train() client id: f_00004-8-5 loss: 0.801627  [  192/  306]
train() client id: f_00004-8-6 loss: 0.807466  [  224/  306]
train() client id: f_00004-8-7 loss: 0.817887  [  256/  306]
train() client id: f_00004-8-8 loss: 0.896548  [  288/  306]
train() client id: f_00004-9-0 loss: 0.730505  [   32/  306]
train() client id: f_00004-9-1 loss: 0.819163  [   64/  306]
train() client id: f_00004-9-2 loss: 0.851743  [   96/  306]
train() client id: f_00004-9-3 loss: 0.817767  [  128/  306]
train() client id: f_00004-9-4 loss: 0.880082  [  160/  306]
train() client id: f_00004-9-5 loss: 0.736802  [  192/  306]
train() client id: f_00004-9-6 loss: 0.844063  [  224/  306]
train() client id: f_00004-9-7 loss: 0.761222  [  256/  306]
train() client id: f_00004-9-8 loss: 0.866882  [  288/  306]
train() client id: f_00004-10-0 loss: 0.834925  [   32/  306]
train() client id: f_00004-10-1 loss: 0.872998  [   64/  306]
train() client id: f_00004-10-2 loss: 0.806472  [   96/  306]
train() client id: f_00004-10-3 loss: 0.849506  [  128/  306]
train() client id: f_00004-10-4 loss: 0.829761  [  160/  306]
train() client id: f_00004-10-5 loss: 0.837619  [  192/  306]
train() client id: f_00004-10-6 loss: 0.654150  [  224/  306]
train() client id: f_00004-10-7 loss: 0.853899  [  256/  306]
train() client id: f_00004-10-8 loss: 0.809145  [  288/  306]
train() client id: f_00004-11-0 loss: 0.919990  [   32/  306]
train() client id: f_00004-11-1 loss: 0.793706  [   64/  306]
train() client id: f_00004-11-2 loss: 0.861433  [   96/  306]
train() client id: f_00004-11-3 loss: 0.662178  [  128/  306]
train() client id: f_00004-11-4 loss: 0.693965  [  160/  306]
train() client id: f_00004-11-5 loss: 0.816112  [  192/  306]
train() client id: f_00004-11-6 loss: 0.805390  [  224/  306]
train() client id: f_00004-11-7 loss: 1.009404  [  256/  306]
train() client id: f_00004-11-8 loss: 0.763136  [  288/  306]
train() client id: f_00004-12-0 loss: 0.881301  [   32/  306]
train() client id: f_00004-12-1 loss: 0.868439  [   64/  306]
train() client id: f_00004-12-2 loss: 0.819788  [   96/  306]
train() client id: f_00004-12-3 loss: 0.842860  [  128/  306]
train() client id: f_00004-12-4 loss: 0.628876  [  160/  306]
train() client id: f_00004-12-5 loss: 0.879483  [  192/  306]
train() client id: f_00004-12-6 loss: 0.889193  [  224/  306]
train() client id: f_00004-12-7 loss: 0.835799  [  256/  306]
train() client id: f_00004-12-8 loss: 0.870286  [  288/  306]
train() client id: f_00004-13-0 loss: 0.830538  [   32/  306]
train() client id: f_00004-13-1 loss: 0.777495  [   64/  306]
train() client id: f_00004-13-2 loss: 0.763085  [   96/  306]
train() client id: f_00004-13-3 loss: 0.857258  [  128/  306]
train() client id: f_00004-13-4 loss: 0.813823  [  160/  306]
train() client id: f_00004-13-5 loss: 0.937363  [  192/  306]
train() client id: f_00004-13-6 loss: 0.844682  [  224/  306]
train() client id: f_00004-13-7 loss: 0.824242  [  256/  306]
train() client id: f_00004-13-8 loss: 0.765530  [  288/  306]
train() client id: f_00005-0-0 loss: 0.818691  [   32/  146]
train() client id: f_00005-0-1 loss: 0.841087  [   64/  146]
train() client id: f_00005-0-2 loss: 0.698226  [   96/  146]
train() client id: f_00005-0-3 loss: 0.837493  [  128/  146]
train() client id: f_00005-1-0 loss: 0.730106  [   32/  146]
train() client id: f_00005-1-1 loss: 0.884680  [   64/  146]
train() client id: f_00005-1-2 loss: 0.816978  [   96/  146]
train() client id: f_00005-1-3 loss: 0.721300  [  128/  146]
train() client id: f_00005-2-0 loss: 0.701875  [   32/  146]
train() client id: f_00005-2-1 loss: 0.873144  [   64/  146]
train() client id: f_00005-2-2 loss: 0.651968  [   96/  146]
train() client id: f_00005-2-3 loss: 0.763902  [  128/  146]
train() client id: f_00005-3-0 loss: 0.664341  [   32/  146]
train() client id: f_00005-3-1 loss: 0.594837  [   64/  146]
train() client id: f_00005-3-2 loss: 0.849503  [   96/  146]
train() client id: f_00005-3-3 loss: 0.669881  [  128/  146]
train() client id: f_00005-4-0 loss: 0.672848  [   32/  146]
train() client id: f_00005-4-1 loss: 0.640077  [   64/  146]
train() client id: f_00005-4-2 loss: 0.904519  [   96/  146]
train() client id: f_00005-4-3 loss: 0.610535  [  128/  146]
train() client id: f_00005-5-0 loss: 0.652768  [   32/  146]
train() client id: f_00005-5-1 loss: 0.740852  [   64/  146]
train() client id: f_00005-5-2 loss: 0.736225  [   96/  146]
train() client id: f_00005-5-3 loss: 0.566375  [  128/  146]
train() client id: f_00005-6-0 loss: 0.672061  [   32/  146]
train() client id: f_00005-6-1 loss: 0.807509  [   64/  146]
train() client id: f_00005-6-2 loss: 0.672612  [   96/  146]
train() client id: f_00005-6-3 loss: 0.637237  [  128/  146]
train() client id: f_00005-7-0 loss: 0.635919  [   32/  146]
train() client id: f_00005-7-1 loss: 0.820198  [   64/  146]
train() client id: f_00005-7-2 loss: 0.731499  [   96/  146]
train() client id: f_00005-7-3 loss: 0.590272  [  128/  146]
train() client id: f_00005-8-0 loss: 0.682690  [   32/  146]
train() client id: f_00005-8-1 loss: 0.524553  [   64/  146]
train() client id: f_00005-8-2 loss: 0.705434  [   96/  146]
train() client id: f_00005-8-3 loss: 0.803431  [  128/  146]
train() client id: f_00005-9-0 loss: 0.532072  [   32/  146]
train() client id: f_00005-9-1 loss: 0.758521  [   64/  146]
train() client id: f_00005-9-2 loss: 0.587295  [   96/  146]
train() client id: f_00005-9-3 loss: 0.725836  [  128/  146]
train() client id: f_00005-10-0 loss: 0.507957  [   32/  146]
train() client id: f_00005-10-1 loss: 0.750624  [   64/  146]
train() client id: f_00005-10-2 loss: 0.586791  [   96/  146]
train() client id: f_00005-10-3 loss: 0.734134  [  128/  146]
train() client id: f_00005-11-0 loss: 0.696900  [   32/  146]
train() client id: f_00005-11-1 loss: 0.671825  [   64/  146]
train() client id: f_00005-11-2 loss: 0.641527  [   96/  146]
train() client id: f_00005-11-3 loss: 0.489237  [  128/  146]
train() client id: f_00005-12-0 loss: 0.656016  [   32/  146]
train() client id: f_00005-12-1 loss: 0.762863  [   64/  146]
train() client id: f_00005-12-2 loss: 0.630528  [   96/  146]
train() client id: f_00005-12-3 loss: 0.439510  [  128/  146]
train() client id: f_00005-13-0 loss: 0.589085  [   32/  146]
train() client id: f_00005-13-1 loss: 0.537800  [   64/  146]
train() client id: f_00005-13-2 loss: 0.593570  [   96/  146]
train() client id: f_00005-13-3 loss: 0.837806  [  128/  146]
train() client id: f_00006-0-0 loss: 0.869601  [   32/   54]
train() client id: f_00006-1-0 loss: 0.882323  [   32/   54]
train() client id: f_00006-2-0 loss: 0.877850  [   32/   54]
train() client id: f_00006-3-0 loss: 0.847295  [   32/   54]
train() client id: f_00006-4-0 loss: 0.787013  [   32/   54]
train() client id: f_00006-5-0 loss: 0.876370  [   32/   54]
train() client id: f_00006-6-0 loss: 0.798637  [   32/   54]
train() client id: f_00006-7-0 loss: 0.848877  [   32/   54]
train() client id: f_00006-8-0 loss: 0.812894  [   32/   54]
train() client id: f_00006-9-0 loss: 0.828168  [   32/   54]
train() client id: f_00006-10-0 loss: 0.840314  [   32/   54]
train() client id: f_00006-11-0 loss: 0.817650  [   32/   54]
train() client id: f_00006-12-0 loss: 0.866600  [   32/   54]
train() client id: f_00006-13-0 loss: 0.830214  [   32/   54]
train() client id: f_00007-0-0 loss: 0.894312  [   32/  179]
train() client id: f_00007-0-1 loss: 0.660978  [   64/  179]
train() client id: f_00007-0-2 loss: 0.707718  [   96/  179]
train() client id: f_00007-0-3 loss: 0.705786  [  128/  179]
train() client id: f_00007-0-4 loss: 0.744608  [  160/  179]
train() client id: f_00007-1-0 loss: 0.793448  [   32/  179]
train() client id: f_00007-1-1 loss: 0.780008  [   64/  179]
train() client id: f_00007-1-2 loss: 0.700518  [   96/  179]
train() client id: f_00007-1-3 loss: 0.689705  [  128/  179]
train() client id: f_00007-1-4 loss: 0.709400  [  160/  179]
train() client id: f_00007-2-0 loss: 0.853293  [   32/  179]
train() client id: f_00007-2-1 loss: 0.792576  [   64/  179]
train() client id: f_00007-2-2 loss: 0.601006  [   96/  179]
train() client id: f_00007-2-3 loss: 0.568872  [  128/  179]
train() client id: f_00007-2-4 loss: 0.670095  [  160/  179]
train() client id: f_00007-3-0 loss: 0.738899  [   32/  179]
train() client id: f_00007-3-1 loss: 0.774344  [   64/  179]
train() client id: f_00007-3-2 loss: 0.570055  [   96/  179]
train() client id: f_00007-3-3 loss: 0.790119  [  128/  179]
train() client id: f_00007-3-4 loss: 0.668155  [  160/  179]
train() client id: f_00007-4-0 loss: 0.690553  [   32/  179]
train() client id: f_00007-4-1 loss: 0.725952  [   64/  179]
train() client id: f_00007-4-2 loss: 0.795396  [   96/  179]
train() client id: f_00007-4-3 loss: 0.688925  [  128/  179]
train() client id: f_00007-4-4 loss: 0.607521  [  160/  179]
train() client id: f_00007-5-0 loss: 0.648976  [   32/  179]
train() client id: f_00007-5-1 loss: 0.838637  [   64/  179]
train() client id: f_00007-5-2 loss: 0.623136  [   96/  179]
train() client id: f_00007-5-3 loss: 0.747653  [  128/  179]
train() client id: f_00007-5-4 loss: 0.637016  [  160/  179]
train() client id: f_00007-6-0 loss: 0.736409  [   32/  179]
train() client id: f_00007-6-1 loss: 0.582323  [   64/  179]
train() client id: f_00007-6-2 loss: 0.614323  [   96/  179]
train() client id: f_00007-6-3 loss: 0.865042  [  128/  179]
train() client id: f_00007-6-4 loss: 0.666048  [  160/  179]
train() client id: f_00007-7-0 loss: 0.730838  [   32/  179]
train() client id: f_00007-7-1 loss: 0.639113  [   64/  179]
train() client id: f_00007-7-2 loss: 0.825732  [   96/  179]
train() client id: f_00007-7-3 loss: 0.573927  [  128/  179]
train() client id: f_00007-7-4 loss: 0.610304  [  160/  179]
train() client id: f_00007-8-0 loss: 0.663821  [   32/  179]
train() client id: f_00007-8-1 loss: 0.621420  [   64/  179]
train() client id: f_00007-8-2 loss: 0.783216  [   96/  179]
train() client id: f_00007-8-3 loss: 0.621128  [  128/  179]
train() client id: f_00007-8-4 loss: 0.744605  [  160/  179]
train() client id: f_00007-9-0 loss: 0.625303  [   32/  179]
train() client id: f_00007-9-1 loss: 0.655255  [   64/  179]
train() client id: f_00007-9-2 loss: 0.798578  [   96/  179]
train() client id: f_00007-9-3 loss: 0.687689  [  128/  179]
train() client id: f_00007-9-4 loss: 0.646767  [  160/  179]
train() client id: f_00007-10-0 loss: 0.737994  [   32/  179]
train() client id: f_00007-10-1 loss: 0.563668  [   64/  179]
train() client id: f_00007-10-2 loss: 0.621759  [   96/  179]
train() client id: f_00007-10-3 loss: 0.767955  [  128/  179]
train() client id: f_00007-10-4 loss: 0.581805  [  160/  179]
train() client id: f_00007-11-0 loss: 0.696480  [   32/  179]
train() client id: f_00007-11-1 loss: 0.654835  [   64/  179]
train() client id: f_00007-11-2 loss: 0.579889  [   96/  179]
train() client id: f_00007-11-3 loss: 0.646941  [  128/  179]
train() client id: f_00007-11-4 loss: 0.871009  [  160/  179]
train() client id: f_00007-12-0 loss: 0.652940  [   32/  179]
train() client id: f_00007-12-1 loss: 0.633163  [   64/  179]
train() client id: f_00007-12-2 loss: 0.732449  [   96/  179]
train() client id: f_00007-12-3 loss: 0.848524  [  128/  179]
train() client id: f_00007-12-4 loss: 0.575542  [  160/  179]
train() client id: f_00007-13-0 loss: 0.554401  [   32/  179]
train() client id: f_00007-13-1 loss: 0.737882  [   64/  179]
train() client id: f_00007-13-2 loss: 0.701144  [   96/  179]
train() client id: f_00007-13-3 loss: 0.748449  [  128/  179]
train() client id: f_00007-13-4 loss: 0.688349  [  160/  179]
train() client id: f_00008-0-0 loss: 0.802833  [   32/  130]
train() client id: f_00008-0-1 loss: 0.896850  [   64/  130]
train() client id: f_00008-0-2 loss: 0.591334  [   96/  130]
train() client id: f_00008-0-3 loss: 0.878418  [  128/  130]
train() client id: f_00008-1-0 loss: 0.727255  [   32/  130]
train() client id: f_00008-1-1 loss: 0.864919  [   64/  130]
train() client id: f_00008-1-2 loss: 0.796514  [   96/  130]
train() client id: f_00008-1-3 loss: 0.756916  [  128/  130]
train() client id: f_00008-2-0 loss: 0.654153  [   32/  130]
train() client id: f_00008-2-1 loss: 0.863053  [   64/  130]
train() client id: f_00008-2-2 loss: 0.775305  [   96/  130]
train() client id: f_00008-2-3 loss: 0.783092  [  128/  130]
train() client id: f_00008-3-0 loss: 0.824689  [   32/  130]
train() client id: f_00008-3-1 loss: 0.844672  [   64/  130]
train() client id: f_00008-3-2 loss: 0.781347  [   96/  130]
train() client id: f_00008-3-3 loss: 0.644527  [  128/  130]
train() client id: f_00008-4-0 loss: 0.779705  [   32/  130]
train() client id: f_00008-4-1 loss: 0.916726  [   64/  130]
train() client id: f_00008-4-2 loss: 0.751019  [   96/  130]
train() client id: f_00008-4-3 loss: 0.633458  [  128/  130]
train() client id: f_00008-5-0 loss: 0.682235  [   32/  130]
train() client id: f_00008-5-1 loss: 0.730507  [   64/  130]
train() client id: f_00008-5-2 loss: 0.832381  [   96/  130]
train() client id: f_00008-5-3 loss: 0.843491  [  128/  130]
train() client id: f_00008-6-0 loss: 0.801892  [   32/  130]
train() client id: f_00008-6-1 loss: 0.716591  [   64/  130]
train() client id: f_00008-6-2 loss: 0.739511  [   96/  130]
train() client id: f_00008-6-3 loss: 0.839257  [  128/  130]
train() client id: f_00008-7-0 loss: 0.776355  [   32/  130]
train() client id: f_00008-7-1 loss: 0.796559  [   64/  130]
train() client id: f_00008-7-2 loss: 0.749848  [   96/  130]
train() client id: f_00008-7-3 loss: 0.773747  [  128/  130]
train() client id: f_00008-8-0 loss: 0.804568  [   32/  130]
train() client id: f_00008-8-1 loss: 0.915723  [   64/  130]
train() client id: f_00008-8-2 loss: 0.622183  [   96/  130]
train() client id: f_00008-8-3 loss: 0.746873  [  128/  130]
train() client id: f_00008-9-0 loss: 0.820450  [   32/  130]
train() client id: f_00008-9-1 loss: 0.741170  [   64/  130]
train() client id: f_00008-9-2 loss: 0.776887  [   96/  130]
train() client id: f_00008-9-3 loss: 0.755499  [  128/  130]
train() client id: f_00008-10-0 loss: 0.802962  [   32/  130]
train() client id: f_00008-10-1 loss: 0.754475  [   64/  130]
train() client id: f_00008-10-2 loss: 0.826525  [   96/  130]
train() client id: f_00008-10-3 loss: 0.701372  [  128/  130]
train() client id: f_00008-11-0 loss: 0.788123  [   32/  130]
train() client id: f_00008-11-1 loss: 0.753269  [   64/  130]
train() client id: f_00008-11-2 loss: 0.744260  [   96/  130]
train() client id: f_00008-11-3 loss: 0.806264  [  128/  130]
train() client id: f_00008-12-0 loss: 0.758772  [   32/  130]
train() client id: f_00008-12-1 loss: 0.784211  [   64/  130]
train() client id: f_00008-12-2 loss: 0.739858  [   96/  130]
train() client id: f_00008-12-3 loss: 0.797782  [  128/  130]
train() client id: f_00008-13-0 loss: 0.881326  [   32/  130]
train() client id: f_00008-13-1 loss: 0.770744  [   64/  130]
train() client id: f_00008-13-2 loss: 0.681592  [   96/  130]
train() client id: f_00008-13-3 loss: 0.770704  [  128/  130]
train() client id: f_00009-0-0 loss: 1.073660  [   32/  118]
train() client id: f_00009-0-1 loss: 1.083442  [   64/  118]
train() client id: f_00009-0-2 loss: 1.051705  [   96/  118]
train() client id: f_00009-1-0 loss: 0.987454  [   32/  118]
train() client id: f_00009-1-1 loss: 0.984256  [   64/  118]
train() client id: f_00009-1-2 loss: 0.965698  [   96/  118]
train() client id: f_00009-2-0 loss: 1.052650  [   32/  118]
train() client id: f_00009-2-1 loss: 0.847853  [   64/  118]
train() client id: f_00009-2-2 loss: 0.933907  [   96/  118]
train() client id: f_00009-3-0 loss: 1.021186  [   32/  118]
train() client id: f_00009-3-1 loss: 0.823456  [   64/  118]
train() client id: f_00009-3-2 loss: 0.849885  [   96/  118]
train() client id: f_00009-4-0 loss: 0.831639  [   32/  118]
train() client id: f_00009-4-1 loss: 0.795193  [   64/  118]
train() client id: f_00009-4-2 loss: 0.795817  [   96/  118]
train() client id: f_00009-5-0 loss: 0.869221  [   32/  118]
train() client id: f_00009-5-1 loss: 0.840866  [   64/  118]
train() client id: f_00009-5-2 loss: 0.761110  [   96/  118]
train() client id: f_00009-6-0 loss: 0.817041  [   32/  118]
train() client id: f_00009-6-1 loss: 0.721295  [   64/  118]
train() client id: f_00009-6-2 loss: 0.787750  [   96/  118]
train() client id: f_00009-7-0 loss: 0.865706  [   32/  118]
train() client id: f_00009-7-1 loss: 0.654965  [   64/  118]
train() client id: f_00009-7-2 loss: 0.728723  [   96/  118]
train() client id: f_00009-8-0 loss: 0.753346  [   32/  118]
train() client id: f_00009-8-1 loss: 0.765596  [   64/  118]
train() client id: f_00009-8-2 loss: 0.672551  [   96/  118]
train() client id: f_00009-9-0 loss: 0.729942  [   32/  118]
train() client id: f_00009-9-1 loss: 0.778474  [   64/  118]
train() client id: f_00009-9-2 loss: 0.583589  [   96/  118]
train() client id: f_00009-10-0 loss: 0.666935  [   32/  118]
train() client id: f_00009-10-1 loss: 0.604538  [   64/  118]
train() client id: f_00009-10-2 loss: 0.826121  [   96/  118]
train() client id: f_00009-11-0 loss: 0.629842  [   32/  118]
train() client id: f_00009-11-1 loss: 0.662917  [   64/  118]
train() client id: f_00009-11-2 loss: 0.759540  [   96/  118]
train() client id: f_00009-12-0 loss: 0.647018  [   32/  118]
train() client id: f_00009-12-1 loss: 0.578527  [   64/  118]
train() client id: f_00009-12-2 loss: 0.835604  [   96/  118]
train() client id: f_00009-13-0 loss: 0.732054  [   32/  118]
train() client id: f_00009-13-1 loss: 0.731893  [   64/  118]
train() client id: f_00009-13-2 loss: 0.575959  [   96/  118]
At round 2 accuracy: 0.610079575596817
At round 2 training accuracy: 0.5653923541247485
At round 2 training loss: 0.8993259326026607
update_location
xs = 15.471708 -88.998411 60.045120 79.056472 -180.103519 -115.217951 12.784040 -16.624259 -1.680116 -70.304393 
ys = 17.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 21.875078 -19.177652 72.845030 -0.998518 
xs mean: -30.55713084058713
ys mean: 12.371751218646876
dists_uav = 102.706913 134.769066 116.649734 129.453446 206.215187 153.526967 103.159831 103.170482 123.730438 122.244446 
uav_gains = -100.290015 -103.240831 -101.672237 -102.803458 -108.076749 -104.660795 -100.337790 -100.338911 -102.312243 -102.181006 
uav_gains_db_mean: -102.59140370747465
dists_bs = 247.099215 181.159361 292.251155 296.289169 165.728412 201.269208 242.301645 250.575010 201.185040 204.784395 
bs_gains = -106.567457 -102.792761 -108.608234 -108.775101 -101.710174 -104.072825 -106.329037 -106.737316 -104.067738 -104.283371 
bs_gains_db_mean: -105.39440149859718
Round 3
-------------------------------
ene_coms = [0.0063792  0.00712727 0.00677204 0.00713083 0.0067835  0.00758055
 0.00639202 0.00639232 0.00757864 0.00766056]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.59830326 22.11757147 10.44362912  3.75276052 25.49985957 12.28575178
  4.64246842 14.9830785  10.96210229  9.9716515 ]
obj_prev = 125.25717644179377
eta_min = 1.145920846530282e-09	eta_max = 0.929016758771599
af = 26.51569137072327	bf = 1.895333316059338	zeta = 29.1672605077956	eta = 0.9090909090909091
af = 26.51569137072327	bf = 1.895333316059338	zeta = 48.648034910628546	eta = 0.5450516432870378
af = 26.51569137072327	bf = 1.895333316059338	zeta = 39.53643552029552	eta = 0.6706646924989427
af = 26.51569137072327	bf = 1.895333316059338	zeta = 37.918741127666465	eta = 0.6992766790819634
af = 26.51569137072327	bf = 1.895333316059338	zeta = 37.8428558770825	eta = 0.7006789196050365
af = 26.51569137072327	bf = 1.895333316059338	zeta = 37.84267824455833	eta = 0.7006822085732305
eta = 0.7006822085732305
ene_coms = [0.0063792  0.00712727 0.00677204 0.00713083 0.0067835  0.00758055
 0.00639202 0.00639232 0.00757864 0.00766056]
ene_comp = [0.02935158 0.0617315  0.02888568 0.01001681 0.07128241 0.03401056
 0.01257925 0.04169788 0.03028338 0.02748799]
ene_total = [3.24159821 6.24706394 3.23497078 1.55568324 7.0823623  3.7732645
 1.72112795 4.36288075 3.43495013 3.18877645]
ti_comp = [0.26523192 0.25775122 0.26130345 0.25771554 0.2611889  0.25321839
 0.26510366 0.26510065 0.25323751 0.25241832]
ti_coms = [0.06379197 0.07127267 0.06772044 0.07130835 0.06783499 0.0758055
 0.06392023 0.06392324 0.07578638 0.07660557]
t_total = [29.84998741 29.84998741 29.84998741 29.84998741 29.84998741 29.84998741
 29.84998741 29.84998741 29.84998741 29.84998741]
ene_coms = [0.0063792  0.00712727 0.00677204 0.00713083 0.0067835  0.00758055
 0.00639202 0.00639232 0.00757864 0.00766056]
ene_comp = [2.24658527e-05 2.21308968e-04 2.20616305e-05 9.45773173e-07
 3.31831574e-04 3.83469605e-05 1.77016062e-06 6.44763387e-05
 2.70667949e-05 2.03735700e-05]
ene_total = [0.58077714 0.66668381 0.61638064 0.64701549 0.64552314 0.69120814
 0.58006316 0.5857794  0.69001125 0.69683599]
optimize_network iter = 0 obj = 6.400278169498233
eta = 0.7006822085732305
freqs = [5.53319155e+07 1.19750157e+08 5.52722933e+07 1.94338475e+07
 1.36457580e+08 6.71565796e+07 2.37251508e+07 7.86453758e+07
 5.97924382e+07 5.44492733e+07]
eta_min = 0.6564735087650883	eta_max = 0.7006822085732294
af = 0.05730605091158697	bf = 1.895333316059338	zeta = 0.06303665600274568	eta = 0.909090909090909
af = 0.05730605091158697	bf = 1.895333316059338	zeta = 20.908746826831536	eta = 0.0027407692764277927
af = 0.05730605091158697	bf = 1.895333316059338	zeta = 2.2395494051973035	eta = 0.025588205725043306
af = 0.05730605091158697	bf = 1.895333316059338	zeta = 2.1606837576552764	eta = 0.026522183410021168
af = 0.05730605091158697	bf = 1.895333316059338	zeta = 2.160646376951672	eta = 0.026522642262468088
eta = 0.026522642262468088
ene_coms = [0.0063792  0.00712727 0.00677204 0.00713083 0.0067835  0.00758055
 0.00639202 0.00639232 0.00757864 0.00766056]
ene_comp = [2.29253256e-04 2.25835191e-03 2.25128362e-04 9.65116179e-06
 3.38618212e-03 3.91312347e-04 1.80636405e-05 6.57950125e-04
 2.76203665e-04 2.07902514e-04]
ene_total = [0.18434146 0.26181005 0.19518478 0.19918249 0.28368132 0.22237359
 0.17880814 0.19666606 0.21910931 0.21948918]
ti_comp = [0.31382816 0.30634746 0.30989969 0.30631178 0.30978513 0.30181462
 0.3136999  0.31369688 0.30183375 0.30101455]
ti_coms = [0.06379197 0.07127267 0.06772044 0.07130835 0.06783499 0.0758055
 0.06392023 0.06392324 0.07578638 0.07660557]
t_total = [29.84998741 29.84998741 29.84998741 29.84998741 29.84998741 29.84998741
 29.84998741 29.84998741 29.84998741 29.84998741]
ene_coms = [0.0063792  0.00712727 0.00677204 0.00713083 0.0067835  0.00758055
 0.00639202 0.00639232 0.00757864 0.00766056]
ene_comp = [2.65820839e-05 2.59519661e-04 2.59826816e-05 1.10901947e-06
 3.90754812e-04 4.47135565e-05 2.09417863e-06 7.62781033e-05
 3.15613567e-05 2.37318849e-05]
ene_total = [0.50636185 0.5839082  0.53736807 0.56376346 0.567108   0.60275927
 0.50543998 0.51132788 0.60156844 0.60742507]
optimize_network iter = 1 obj = 5.58703020957846
eta = 0.6564735087650883
freqs = [5.53319155e+07 1.19214384e+08 5.51439166e+07 1.93464739e+07
 1.36131277e+08 6.66668054e+07 2.37233735e+07 7.86393459e+07
 5.93570926e+07 5.40246023e+07]
eta_min = 0.656473508765099	eta_max = 0.6564735087650821
af = 0.056928269987321464	bf = 1.895333316059338	zeta = 0.06262109698605361	eta = 0.9090909090909091
af = 0.056928269987321464	bf = 1.895333316059338	zeta = 20.90835075678556	eta = 0.0027227527723030025
af = 0.056928269987321464	bf = 1.895333316059338	zeta = 2.2376506161222367	eta = 0.025441089675552652
af = 0.056928269987321464	bf = 1.895333316059338	zeta = 2.1592734500778197	eta = 0.026364548679681948
af = 0.056928269987321464	bf = 1.895333316059338	zeta = 2.1592367228150824	eta = 0.026364997124123483
eta = 0.026364997124123483
ene_coms = [0.0063792  0.00712727 0.00677204 0.00713083 0.0067835  0.00758055
 0.00639202 0.00639232 0.00757864 0.00766056]
ene_comp = [2.29629783e-04 2.24186499e-03 2.24451836e-04 9.58028346e-06
 3.37554207e-03 3.86258816e-04 1.80905976e-05 6.58929689e-04
 2.72643315e-04 2.05008290e-04]
ene_total = [0.18432211 0.26130783 0.19513431 0.19914826 0.28333864 0.22219664
 0.17877994 0.19666154 0.21897453 0.21937292]
ti_comp = [0.31382816 0.30634746 0.30989969 0.30631178 0.30978513 0.30181462
 0.3136999  0.31369688 0.30183375 0.30101455]
ti_coms = [0.06379197 0.07127267 0.06772044 0.07130835 0.06783499 0.0758055
 0.06392023 0.06392324 0.07578638 0.07660557]
t_total = [29.84998741 29.84998741 29.84998741 29.84998741 29.84998741 29.84998741
 29.84998741 29.84998741 29.84998741 29.84998741]
ene_coms = [0.0063792  0.00712727 0.00677204 0.00713083 0.0067835  0.00758055
 0.00639202 0.00639232 0.00757864 0.00766056]
ene_comp = [2.65820839e-05 2.59519661e-04 2.59826816e-05 1.10901947e-06
 3.90754812e-04 4.47135565e-05 2.09417863e-06 7.62781033e-05
 3.15613567e-05 2.37318849e-05]
ene_total = [0.50636185 0.5839082  0.53736807 0.56376346 0.567108   0.60275927
 0.50543998 0.51132788 0.60156844 0.60742507]
optimize_network iter = 2 obj = 5.587030209578633
eta = 0.656473508765099
freqs = [5.53319155e+07 1.19214384e+08 5.51439166e+07 1.93464739e+07
 1.36131277e+08 6.66668054e+07 2.37233735e+07 7.86393459e+07
 5.93570926e+07 5.40246023e+07]
Done!
ene_coms = [0.0063792  0.00712727 0.00677204 0.00713083 0.0067835  0.00758055
 0.00639202 0.00639232 0.00757864 0.00766056]
ene_comp = [2.50746749e-05 2.44802896e-04 2.45092634e-05 1.04612952e-06
 3.68596003e-04 4.21779533e-05 1.97542256e-06 7.19525471e-05
 2.97715846e-05 2.23861042e-05]
ene_total = [0.00640427 0.00737207 0.00679655 0.00713188 0.0071521  0.00762273
 0.006394   0.00646428 0.00760841 0.00768294]
At round 3 energy consumption: 0.0706292263639348
At round 3 eta: 0.656473508765099
At round 3 a_n: 27.154965315346423
At round 3 local rounds: 13.781518290152922
At round 3 global rounds: 79.04766010251609
gradient difference: 0.3133561313152313
train() client id: f_00000-0-0 loss: 1.120338  [   32/  126]
train() client id: f_00000-0-1 loss: 1.160467  [   64/  126]
train() client id: f_00000-0-2 loss: 1.065045  [   96/  126]
train() client id: f_00000-1-0 loss: 1.031033  [   32/  126]
train() client id: f_00000-1-1 loss: 1.019864  [   64/  126]
train() client id: f_00000-1-2 loss: 1.270786  [   96/  126]
train() client id: f_00000-2-0 loss: 0.981513  [   32/  126]
train() client id: f_00000-2-1 loss: 1.020881  [   64/  126]
train() client id: f_00000-2-2 loss: 1.006285  [   96/  126]
train() client id: f_00000-3-0 loss: 0.902727  [   32/  126]
train() client id: f_00000-3-1 loss: 0.971271  [   64/  126]
train() client id: f_00000-3-2 loss: 1.027636  [   96/  126]
train() client id: f_00000-4-0 loss: 0.876065  [   32/  126]
train() client id: f_00000-4-1 loss: 1.002827  [   64/  126]
train() client id: f_00000-4-2 loss: 0.900504  [   96/  126]
train() client id: f_00000-5-0 loss: 0.822606  [   32/  126]
train() client id: f_00000-5-1 loss: 0.923635  [   64/  126]
train() client id: f_00000-5-2 loss: 0.927097  [   96/  126]
train() client id: f_00000-6-0 loss: 0.822116  [   32/  126]
train() client id: f_00000-6-1 loss: 0.891067  [   64/  126]
train() client id: f_00000-6-2 loss: 0.900017  [   96/  126]
train() client id: f_00000-7-0 loss: 0.901683  [   32/  126]
train() client id: f_00000-7-1 loss: 0.843446  [   64/  126]
train() client id: f_00000-7-2 loss: 0.890716  [   96/  126]
train() client id: f_00000-8-0 loss: 0.918028  [   32/  126]
train() client id: f_00000-8-1 loss: 0.839698  [   64/  126]
train() client id: f_00000-8-2 loss: 0.824382  [   96/  126]
train() client id: f_00000-9-0 loss: 0.870799  [   32/  126]
train() client id: f_00000-9-1 loss: 0.764945  [   64/  126]
train() client id: f_00000-9-2 loss: 0.881546  [   96/  126]
train() client id: f_00000-10-0 loss: 0.771812  [   32/  126]
train() client id: f_00000-10-1 loss: 0.795127  [   64/  126]
train() client id: f_00000-10-2 loss: 0.957185  [   96/  126]
train() client id: f_00000-11-0 loss: 0.766743  [   32/  126]
train() client id: f_00000-11-1 loss: 0.781527  [   64/  126]
train() client id: f_00000-11-2 loss: 0.941943  [   96/  126]
train() client id: f_00000-12-0 loss: 0.819589  [   32/  126]
train() client id: f_00000-12-1 loss: 0.870880  [   64/  126]
train() client id: f_00000-12-2 loss: 1.007166  [   96/  126]
train() client id: f_00001-0-0 loss: 0.747726  [   32/  265]
train() client id: f_00001-0-1 loss: 0.612807  [   64/  265]
train() client id: f_00001-0-2 loss: 0.669021  [   96/  265]
train() client id: f_00001-0-3 loss: 0.714396  [  128/  265]
train() client id: f_00001-0-4 loss: 0.615173  [  160/  265]
train() client id: f_00001-0-5 loss: 0.745144  [  192/  265]
train() client id: f_00001-0-6 loss: 0.684197  [  224/  265]
train() client id: f_00001-0-7 loss: 0.680481  [  256/  265]
train() client id: f_00001-1-0 loss: 0.643488  [   32/  265]
train() client id: f_00001-1-1 loss: 0.657198  [   64/  265]
train() client id: f_00001-1-2 loss: 0.679904  [   96/  265]
train() client id: f_00001-1-3 loss: 0.601866  [  128/  265]
train() client id: f_00001-1-4 loss: 0.615319  [  160/  265]
train() client id: f_00001-1-5 loss: 0.642192  [  192/  265]
train() client id: f_00001-1-6 loss: 0.602088  [  224/  265]
train() client id: f_00001-1-7 loss: 0.725052  [  256/  265]
train() client id: f_00001-2-0 loss: 0.623168  [   32/  265]
train() client id: f_00001-2-1 loss: 0.617273  [   64/  265]
train() client id: f_00001-2-2 loss: 0.653504  [   96/  265]
train() client id: f_00001-2-3 loss: 0.694932  [  128/  265]
train() client id: f_00001-2-4 loss: 0.594495  [  160/  265]
train() client id: f_00001-2-5 loss: 0.632434  [  192/  265]
train() client id: f_00001-2-6 loss: 0.561868  [  224/  265]
train() client id: f_00001-2-7 loss: 0.592945  [  256/  265]
train() client id: f_00001-3-0 loss: 0.548122  [   32/  265]
train() client id: f_00001-3-1 loss: 0.617364  [   64/  265]
train() client id: f_00001-3-2 loss: 0.607084  [   96/  265]
train() client id: f_00001-3-3 loss: 0.603526  [  128/  265]
train() client id: f_00001-3-4 loss: 0.569163  [  160/  265]
train() client id: f_00001-3-5 loss: 0.554687  [  192/  265]
train() client id: f_00001-3-6 loss: 0.613749  [  224/  265]
train() client id: f_00001-3-7 loss: 0.739580  [  256/  265]
train() client id: f_00001-4-0 loss: 0.537915  [   32/  265]
train() client id: f_00001-4-1 loss: 0.611648  [   64/  265]
train() client id: f_00001-4-2 loss: 0.625978  [   96/  265]
train() client id: f_00001-4-3 loss: 0.601345  [  128/  265]
train() client id: f_00001-4-4 loss: 0.521459  [  160/  265]
train() client id: f_00001-4-5 loss: 0.629087  [  192/  265]
train() client id: f_00001-4-6 loss: 0.556232  [  224/  265]
train() client id: f_00001-4-7 loss: 0.586894  [  256/  265]
train() client id: f_00001-5-0 loss: 0.516891  [   32/  265]
train() client id: f_00001-5-1 loss: 0.524161  [   64/  265]
train() client id: f_00001-5-2 loss: 0.559154  [   96/  265]
train() client id: f_00001-5-3 loss: 0.607292  [  128/  265]
train() client id: f_00001-5-4 loss: 0.593723  [  160/  265]
train() client id: f_00001-5-5 loss: 0.516238  [  192/  265]
train() client id: f_00001-5-6 loss: 0.579275  [  224/  265]
train() client id: f_00001-5-7 loss: 0.757403  [  256/  265]
train() client id: f_00001-6-0 loss: 0.510308  [   32/  265]
train() client id: f_00001-6-1 loss: 0.537281  [   64/  265]
train() client id: f_00001-6-2 loss: 0.580966  [   96/  265]
train() client id: f_00001-6-3 loss: 0.655648  [  128/  265]
train() client id: f_00001-6-4 loss: 0.527454  [  160/  265]
train() client id: f_00001-6-5 loss: 0.586929  [  192/  265]
train() client id: f_00001-6-6 loss: 0.529189  [  224/  265]
train() client id: f_00001-6-7 loss: 0.673697  [  256/  265]
train() client id: f_00001-7-0 loss: 0.639014  [   32/  265]
train() client id: f_00001-7-1 loss: 0.504624  [   64/  265]
train() client id: f_00001-7-2 loss: 0.645396  [   96/  265]
train() client id: f_00001-7-3 loss: 0.557311  [  128/  265]
train() client id: f_00001-7-4 loss: 0.500976  [  160/  265]
train() client id: f_00001-7-5 loss: 0.542091  [  192/  265]
train() client id: f_00001-7-6 loss: 0.511527  [  224/  265]
train() client id: f_00001-7-7 loss: 0.533612  [  256/  265]
train() client id: f_00001-8-0 loss: 0.583278  [   32/  265]
train() client id: f_00001-8-1 loss: 0.600642  [   64/  265]
train() client id: f_00001-8-2 loss: 0.605784  [   96/  265]
train() client id: f_00001-8-3 loss: 0.554369  [  128/  265]
train() client id: f_00001-8-4 loss: 0.494134  [  160/  265]
train() client id: f_00001-8-5 loss: 0.614604  [  192/  265]
train() client id: f_00001-8-6 loss: 0.546570  [  224/  265]
train() client id: f_00001-8-7 loss: 0.531064  [  256/  265]
train() client id: f_00001-9-0 loss: 0.484152  [   32/  265]
train() client id: f_00001-9-1 loss: 0.622766  [   64/  265]
train() client id: f_00001-9-2 loss: 0.495115  [   96/  265]
train() client id: f_00001-9-3 loss: 0.638304  [  128/  265]
train() client id: f_00001-9-4 loss: 0.550171  [  160/  265]
train() client id: f_00001-9-5 loss: 0.570921  [  192/  265]
train() client id: f_00001-9-6 loss: 0.568299  [  224/  265]
train() client id: f_00001-9-7 loss: 0.579587  [  256/  265]
train() client id: f_00001-10-0 loss: 0.526112  [   32/  265]
train() client id: f_00001-10-1 loss: 0.513646  [   64/  265]
train() client id: f_00001-10-2 loss: 0.591198  [   96/  265]
train() client id: f_00001-10-3 loss: 0.508349  [  128/  265]
train() client id: f_00001-10-4 loss: 0.547385  [  160/  265]
train() client id: f_00001-10-5 loss: 0.640484  [  192/  265]
train() client id: f_00001-10-6 loss: 0.646654  [  224/  265]
train() client id: f_00001-10-7 loss: 0.514156  [  256/  265]
train() client id: f_00001-11-0 loss: 0.536549  [   32/  265]
train() client id: f_00001-11-1 loss: 0.628035  [   64/  265]
train() client id: f_00001-11-2 loss: 0.551071  [   96/  265]
train() client id: f_00001-11-3 loss: 0.479352  [  128/  265]
train() client id: f_00001-11-4 loss: 0.581875  [  160/  265]
train() client id: f_00001-11-5 loss: 0.498279  [  192/  265]
train() client id: f_00001-11-6 loss: 0.520423  [  224/  265]
train() client id: f_00001-11-7 loss: 0.677320  [  256/  265]
train() client id: f_00001-12-0 loss: 0.538371  [   32/  265]
train() client id: f_00001-12-1 loss: 0.634033  [   64/  265]
train() client id: f_00001-12-2 loss: 0.566226  [   96/  265]
train() client id: f_00001-12-3 loss: 0.520805  [  128/  265]
train() client id: f_00001-12-4 loss: 0.545278  [  160/  265]
train() client id: f_00001-12-5 loss: 0.466122  [  192/  265]
train() client id: f_00001-12-6 loss: 0.583157  [  224/  265]
train() client id: f_00001-12-7 loss: 0.601119  [  256/  265]
train() client id: f_00002-0-0 loss: 1.013207  [   32/  124]
train() client id: f_00002-0-1 loss: 0.944778  [   64/  124]
train() client id: f_00002-0-2 loss: 0.901532  [   96/  124]
train() client id: f_00002-1-0 loss: 0.758492  [   32/  124]
train() client id: f_00002-1-1 loss: 1.011801  [   64/  124]
train() client id: f_00002-1-2 loss: 0.907459  [   96/  124]
train() client id: f_00002-2-0 loss: 0.697981  [   32/  124]
train() client id: f_00002-2-1 loss: 0.947763  [   64/  124]
train() client id: f_00002-2-2 loss: 1.071096  [   96/  124]
train() client id: f_00002-3-0 loss: 1.081734  [   32/  124]
train() client id: f_00002-3-1 loss: 0.790245  [   64/  124]
train() client id: f_00002-3-2 loss: 0.847256  [   96/  124]
train() client id: f_00002-4-0 loss: 0.765120  [   32/  124]
train() client id: f_00002-4-1 loss: 0.790808  [   64/  124]
train() client id: f_00002-4-2 loss: 0.930853  [   96/  124]
train() client id: f_00002-5-0 loss: 0.890137  [   32/  124]
train() client id: f_00002-5-1 loss: 0.861731  [   64/  124]
train() client id: f_00002-5-2 loss: 0.785962  [   96/  124]
train() client id: f_00002-6-0 loss: 0.793137  [   32/  124]
train() client id: f_00002-6-1 loss: 0.860213  [   64/  124]
train() client id: f_00002-6-2 loss: 0.777541  [   96/  124]
train() client id: f_00002-7-0 loss: 1.050172  [   32/  124]
train() client id: f_00002-7-1 loss: 0.532405  [   64/  124]
train() client id: f_00002-7-2 loss: 0.779316  [   96/  124]
train() client id: f_00002-8-0 loss: 0.616736  [   32/  124]
train() client id: f_00002-8-1 loss: 1.001317  [   64/  124]
train() client id: f_00002-8-2 loss: 0.684717  [   96/  124]
train() client id: f_00002-9-0 loss: 0.788927  [   32/  124]
train() client id: f_00002-9-1 loss: 0.911896  [   64/  124]
train() client id: f_00002-9-2 loss: 0.580238  [   96/  124]
train() client id: f_00002-10-0 loss: 0.591337  [   32/  124]
train() client id: f_00002-10-1 loss: 0.730373  [   64/  124]
train() client id: f_00002-10-2 loss: 0.774709  [   96/  124]
train() client id: f_00002-11-0 loss: 0.626111  [   32/  124]
train() client id: f_00002-11-1 loss: 0.813033  [   64/  124]
train() client id: f_00002-11-2 loss: 0.708386  [   96/  124]
train() client id: f_00002-12-0 loss: 0.731587  [   32/  124]
train() client id: f_00002-12-1 loss: 0.600502  [   64/  124]
train() client id: f_00002-12-2 loss: 0.840098  [   96/  124]
train() client id: f_00003-0-0 loss: 1.074848  [   32/   43]
train() client id: f_00003-1-0 loss: 1.068846  [   32/   43]
train() client id: f_00003-2-0 loss: 1.053642  [   32/   43]
train() client id: f_00003-3-0 loss: 1.049265  [   32/   43]
train() client id: f_00003-4-0 loss: 1.054658  [   32/   43]
train() client id: f_00003-5-0 loss: 1.014516  [   32/   43]
train() client id: f_00003-6-0 loss: 1.084474  [   32/   43]
train() client id: f_00003-7-0 loss: 1.088181  [   32/   43]
train() client id: f_00003-8-0 loss: 1.055004  [   32/   43]
train() client id: f_00003-9-0 loss: 1.014252  [   32/   43]
train() client id: f_00003-10-0 loss: 1.095697  [   32/   43]
train() client id: f_00003-11-0 loss: 1.041114  [   32/   43]
train() client id: f_00003-12-0 loss: 1.007996  [   32/   43]
train() client id: f_00004-0-0 loss: 1.107329  [   32/  306]
train() client id: f_00004-0-1 loss: 0.936680  [   64/  306]
train() client id: f_00004-0-2 loss: 0.906248  [   96/  306]
train() client id: f_00004-0-3 loss: 1.050584  [  128/  306]
train() client id: f_00004-0-4 loss: 0.944251  [  160/  306]
train() client id: f_00004-0-5 loss: 1.229503  [  192/  306]
train() client id: f_00004-0-6 loss: 0.877186  [  224/  306]
train() client id: f_00004-0-7 loss: 0.901193  [  256/  306]
train() client id: f_00004-0-8 loss: 0.923290  [  288/  306]
train() client id: f_00004-1-0 loss: 1.049160  [   32/  306]
train() client id: f_00004-1-1 loss: 0.905209  [   64/  306]
train() client id: f_00004-1-2 loss: 0.892128  [   96/  306]
train() client id: f_00004-1-3 loss: 1.083040  [  128/  306]
train() client id: f_00004-1-4 loss: 0.925794  [  160/  306]
train() client id: f_00004-1-5 loss: 1.012248  [  192/  306]
train() client id: f_00004-1-6 loss: 0.868079  [  224/  306]
train() client id: f_00004-1-7 loss: 0.955176  [  256/  306]
train() client id: f_00004-1-8 loss: 1.013837  [  288/  306]
train() client id: f_00004-2-0 loss: 1.056526  [   32/  306]
train() client id: f_00004-2-1 loss: 0.977271  [   64/  306]
train() client id: f_00004-2-2 loss: 0.887345  [   96/  306]
train() client id: f_00004-2-3 loss: 0.872877  [  128/  306]
train() client id: f_00004-2-4 loss: 1.034603  [  160/  306]
train() client id: f_00004-2-5 loss: 1.015211  [  192/  306]
train() client id: f_00004-2-6 loss: 0.797045  [  224/  306]
train() client id: f_00004-2-7 loss: 1.001382  [  256/  306]
train() client id: f_00004-2-8 loss: 1.071246  [  288/  306]
train() client id: f_00004-3-0 loss: 0.979975  [   32/  306]
train() client id: f_00004-3-1 loss: 0.980651  [   64/  306]
train() client id: f_00004-3-2 loss: 1.152355  [   96/  306]
train() client id: f_00004-3-3 loss: 0.916599  [  128/  306]
train() client id: f_00004-3-4 loss: 1.021109  [  160/  306]
train() client id: f_00004-3-5 loss: 0.892195  [  192/  306]
train() client id: f_00004-3-6 loss: 0.849155  [  224/  306]
train() client id: f_00004-3-7 loss: 0.951781  [  256/  306]
train() client id: f_00004-3-8 loss: 0.919086  [  288/  306]
train() client id: f_00004-4-0 loss: 0.882681  [   32/  306]
train() client id: f_00004-4-1 loss: 0.987110  [   64/  306]
train() client id: f_00004-4-2 loss: 1.018726  [   96/  306]
train() client id: f_00004-4-3 loss: 0.966992  [  128/  306]
train() client id: f_00004-4-4 loss: 1.032268  [  160/  306]
train() client id: f_00004-4-5 loss: 1.060088  [  192/  306]
train() client id: f_00004-4-6 loss: 0.931865  [  224/  306]
train() client id: f_00004-4-7 loss: 1.006554  [  256/  306]
train() client id: f_00004-4-8 loss: 0.874901  [  288/  306]
train() client id: f_00004-5-0 loss: 1.004838  [   32/  306]
train() client id: f_00004-5-1 loss: 0.941465  [   64/  306]
train() client id: f_00004-5-2 loss: 0.966461  [   96/  306]
train() client id: f_00004-5-3 loss: 0.928686  [  128/  306]
train() client id: f_00004-5-4 loss: 1.046413  [  160/  306]
train() client id: f_00004-5-5 loss: 1.064795  [  192/  306]
train() client id: f_00004-5-6 loss: 1.013219  [  224/  306]
train() client id: f_00004-5-7 loss: 0.900601  [  256/  306]
train() client id: f_00004-5-8 loss: 0.908047  [  288/  306]
train() client id: f_00004-6-0 loss: 0.823596  [   32/  306]
train() client id: f_00004-6-1 loss: 1.050655  [   64/  306]
train() client id: f_00004-6-2 loss: 1.100645  [   96/  306]
train() client id: f_00004-6-3 loss: 0.836354  [  128/  306]
train() client id: f_00004-6-4 loss: 1.066309  [  160/  306]
train() client id: f_00004-6-5 loss: 0.959106  [  192/  306]
train() client id: f_00004-6-6 loss: 1.010457  [  224/  306]
train() client id: f_00004-6-7 loss: 0.821800  [  256/  306]
train() client id: f_00004-6-8 loss: 0.911614  [  288/  306]
train() client id: f_00004-7-0 loss: 1.026903  [   32/  306]
train() client id: f_00004-7-1 loss: 1.044409  [   64/  306]
train() client id: f_00004-7-2 loss: 0.941981  [   96/  306]
train() client id: f_00004-7-3 loss: 0.973250  [  128/  306]
train() client id: f_00004-7-4 loss: 0.945707  [  160/  306]
train() client id: f_00004-7-5 loss: 0.962035  [  192/  306]
train() client id: f_00004-7-6 loss: 0.965464  [  224/  306]
train() client id: f_00004-7-7 loss: 0.845058  [  256/  306]
train() client id: f_00004-7-8 loss: 0.902648  [  288/  306]
train() client id: f_00004-8-0 loss: 1.013336  [   32/  306]
train() client id: f_00004-8-1 loss: 1.005861  [   64/  306]
train() client id: f_00004-8-2 loss: 0.946541  [   96/  306]
train() client id: f_00004-8-3 loss: 0.915720  [  128/  306]
train() client id: f_00004-8-4 loss: 1.079137  [  160/  306]
train() client id: f_00004-8-5 loss: 1.033990  [  192/  306]
train() client id: f_00004-8-6 loss: 0.908734  [  224/  306]
train() client id: f_00004-8-7 loss: 0.902875  [  256/  306]
train() client id: f_00004-8-8 loss: 0.830465  [  288/  306]
train() client id: f_00004-9-0 loss: 0.916197  [   32/  306]
train() client id: f_00004-9-1 loss: 1.010556  [   64/  306]
train() client id: f_00004-9-2 loss: 0.939638  [   96/  306]
train() client id: f_00004-9-3 loss: 0.971397  [  128/  306]
train() client id: f_00004-9-4 loss: 1.059750  [  160/  306]
train() client id: f_00004-9-5 loss: 0.959732  [  192/  306]
train() client id: f_00004-9-6 loss: 0.885217  [  224/  306]
train() client id: f_00004-9-7 loss: 0.947033  [  256/  306]
train() client id: f_00004-9-8 loss: 0.858239  [  288/  306]
train() client id: f_00004-10-0 loss: 0.837268  [   32/  306]
train() client id: f_00004-10-1 loss: 0.906522  [   64/  306]
train() client id: f_00004-10-2 loss: 0.976362  [   96/  306]
train() client id: f_00004-10-3 loss: 0.916458  [  128/  306]
train() client id: f_00004-10-4 loss: 1.006883  [  160/  306]
train() client id: f_00004-10-5 loss: 1.045356  [  192/  306]
train() client id: f_00004-10-6 loss: 0.924425  [  224/  306]
train() client id: f_00004-10-7 loss: 1.009618  [  256/  306]
train() client id: f_00004-10-8 loss: 0.923194  [  288/  306]
train() client id: f_00004-11-0 loss: 0.958535  [   32/  306]
train() client id: f_00004-11-1 loss: 0.986284  [   64/  306]
train() client id: f_00004-11-2 loss: 0.907451  [   96/  306]
train() client id: f_00004-11-3 loss: 0.905368  [  128/  306]
train() client id: f_00004-11-4 loss: 0.915525  [  160/  306]
train() client id: f_00004-11-5 loss: 0.886244  [  192/  306]
train() client id: f_00004-11-6 loss: 0.970489  [  224/  306]
train() client id: f_00004-11-7 loss: 0.944483  [  256/  306]
train() client id: f_00004-11-8 loss: 1.065376  [  288/  306]
train() client id: f_00004-12-0 loss: 0.821566  [   32/  306]
train() client id: f_00004-12-1 loss: 0.974694  [   64/  306]
train() client id: f_00004-12-2 loss: 1.096914  [   96/  306]
train() client id: f_00004-12-3 loss: 0.879488  [  128/  306]
train() client id: f_00004-12-4 loss: 0.952880  [  160/  306]
train() client id: f_00004-12-5 loss: 0.853668  [  192/  306]
train() client id: f_00004-12-6 loss: 0.906778  [  224/  306]
train() client id: f_00004-12-7 loss: 0.973590  [  256/  306]
train() client id: f_00004-12-8 loss: 1.064216  [  288/  306]
train() client id: f_00005-0-0 loss: 0.709108  [   32/  146]
train() client id: f_00005-0-1 loss: 0.600836  [   64/  146]
train() client id: f_00005-0-2 loss: 0.789935  [   96/  146]
train() client id: f_00005-0-3 loss: 0.561323  [  128/  146]
train() client id: f_00005-1-0 loss: 0.639863  [   32/  146]
train() client id: f_00005-1-1 loss: 0.683118  [   64/  146]
train() client id: f_00005-1-2 loss: 0.667104  [   96/  146]
train() client id: f_00005-1-3 loss: 0.585271  [  128/  146]
train() client id: f_00005-2-0 loss: 0.746387  [   32/  146]
train() client id: f_00005-2-1 loss: 0.693103  [   64/  146]
train() client id: f_00005-2-2 loss: 0.586506  [   96/  146]
train() client id: f_00005-2-3 loss: 0.554119  [  128/  146]
train() client id: f_00005-3-0 loss: 0.833734  [   32/  146]
train() client id: f_00005-3-1 loss: 0.816943  [   64/  146]
train() client id: f_00005-3-2 loss: 0.571993  [   96/  146]
train() client id: f_00005-3-3 loss: 0.435399  [  128/  146]
train() client id: f_00005-4-0 loss: 0.497368  [   32/  146]
train() client id: f_00005-4-1 loss: 0.876252  [   64/  146]
train() client id: f_00005-4-2 loss: 0.585720  [   96/  146]
train() client id: f_00005-4-3 loss: 0.657274  [  128/  146]
train() client id: f_00005-5-0 loss: 0.738661  [   32/  146]
train() client id: f_00005-5-1 loss: 0.767104  [   64/  146]
train() client id: f_00005-5-2 loss: 0.539945  [   96/  146]
train() client id: f_00005-5-3 loss: 0.426434  [  128/  146]
train() client id: f_00005-6-0 loss: 0.626210  [   32/  146]
train() client id: f_00005-6-1 loss: 0.620516  [   64/  146]
train() client id: f_00005-6-2 loss: 0.611745  [   96/  146]
train() client id: f_00005-6-3 loss: 0.536554  [  128/  146]
train() client id: f_00005-7-0 loss: 0.609820  [   32/  146]
train() client id: f_00005-7-1 loss: 0.577740  [   64/  146]
train() client id: f_00005-7-2 loss: 0.509030  [   96/  146]
train() client id: f_00005-7-3 loss: 0.674592  [  128/  146]
train() client id: f_00005-8-0 loss: 0.497000  [   32/  146]
train() client id: f_00005-8-1 loss: 0.658962  [   64/  146]
train() client id: f_00005-8-2 loss: 0.409794  [   96/  146]
train() client id: f_00005-8-3 loss: 0.647082  [  128/  146]
train() client id: f_00005-9-0 loss: 0.582491  [   32/  146]
train() client id: f_00005-9-1 loss: 0.529135  [   64/  146]
train() client id: f_00005-9-2 loss: 0.563572  [   96/  146]
train() client id: f_00005-9-3 loss: 0.774007  [  128/  146]
train() client id: f_00005-10-0 loss: 0.529512  [   32/  146]
train() client id: f_00005-10-1 loss: 0.466417  [   64/  146]
train() client id: f_00005-10-2 loss: 0.562297  [   96/  146]
train() client id: f_00005-10-3 loss: 0.836876  [  128/  146]
train() client id: f_00005-11-0 loss: 0.658592  [   32/  146]
train() client id: f_00005-11-1 loss: 0.609378  [   64/  146]
train() client id: f_00005-11-2 loss: 0.532658  [   96/  146]
train() client id: f_00005-11-3 loss: 0.567120  [  128/  146]
train() client id: f_00005-12-0 loss: 0.297923  [   32/  146]
train() client id: f_00005-12-1 loss: 0.512170  [   64/  146]
train() client id: f_00005-12-2 loss: 0.670002  [   96/  146]
train() client id: f_00005-12-3 loss: 0.710801  [  128/  146]
train() client id: f_00006-0-0 loss: 0.886748  [   32/   54]
train() client id: f_00006-1-0 loss: 0.840662  [   32/   54]
train() client id: f_00006-2-0 loss: 0.822696  [   32/   54]
train() client id: f_00006-3-0 loss: 0.841183  [   32/   54]
train() client id: f_00006-4-0 loss: 0.789967  [   32/   54]
train() client id: f_00006-5-0 loss: 0.821168  [   32/   54]
train() client id: f_00006-6-0 loss: 0.831133  [   32/   54]
train() client id: f_00006-7-0 loss: 0.824336  [   32/   54]
train() client id: f_00006-8-0 loss: 0.879378  [   32/   54]
train() client id: f_00006-9-0 loss: 0.890450  [   32/   54]
train() client id: f_00006-10-0 loss: 0.782558  [   32/   54]
train() client id: f_00006-11-0 loss: 0.839005  [   32/   54]
train() client id: f_00006-12-0 loss: 0.809065  [   32/   54]
train() client id: f_00007-0-0 loss: 0.587577  [   32/  179]
train() client id: f_00007-0-1 loss: 0.649457  [   64/  179]
train() client id: f_00007-0-2 loss: 0.527375  [   96/  179]
train() client id: f_00007-0-3 loss: 0.763796  [  128/  179]
train() client id: f_00007-0-4 loss: 0.651032  [  160/  179]
train() client id: f_00007-1-0 loss: 0.574219  [   32/  179]
train() client id: f_00007-1-1 loss: 0.642319  [   64/  179]
train() client id: f_00007-1-2 loss: 0.471802  [   96/  179]
train() client id: f_00007-1-3 loss: 0.599131  [  128/  179]
train() client id: f_00007-1-4 loss: 0.625895  [  160/  179]
train() client id: f_00007-2-0 loss: 0.579484  [   32/  179]
train() client id: f_00007-2-1 loss: 0.574860  [   64/  179]
train() client id: f_00007-2-2 loss: 0.670324  [   96/  179]
train() client id: f_00007-2-3 loss: 0.629084  [  128/  179]
train() client id: f_00007-2-4 loss: 0.582356  [  160/  179]
train() client id: f_00007-3-0 loss: 0.530490  [   32/  179]
train() client id: f_00007-3-1 loss: 0.646045  [   64/  179]
train() client id: f_00007-3-2 loss: 0.552341  [   96/  179]
train() client id: f_00007-3-3 loss: 0.621659  [  128/  179]
train() client id: f_00007-3-4 loss: 0.537805  [  160/  179]
train() client id: f_00007-4-0 loss: 0.567938  [   32/  179]
train() client id: f_00007-4-1 loss: 0.526482  [   64/  179]
train() client id: f_00007-4-2 loss: 0.697919  [   96/  179]
train() client id: f_00007-4-3 loss: 0.625662  [  128/  179]
train() client id: f_00007-4-4 loss: 0.435530  [  160/  179]
train() client id: f_00007-5-0 loss: 0.749019  [   32/  179]
train() client id: f_00007-5-1 loss: 0.619473  [   64/  179]
train() client id: f_00007-5-2 loss: 0.477563  [   96/  179]
train() client id: f_00007-5-3 loss: 0.590385  [  128/  179]
train() client id: f_00007-5-4 loss: 0.454867  [  160/  179]
train() client id: f_00007-6-0 loss: 0.585330  [   32/  179]
train() client id: f_00007-6-1 loss: 0.459992  [   64/  179]
train() client id: f_00007-6-2 loss: 0.529536  [   96/  179]
train() client id: f_00007-6-3 loss: 0.561518  [  128/  179]
train() client id: f_00007-6-4 loss: 0.636981  [  160/  179]
train() client id: f_00007-7-0 loss: 0.450372  [   32/  179]
train() client id: f_00007-7-1 loss: 0.493744  [   64/  179]
train() client id: f_00007-7-2 loss: 0.480600  [   96/  179]
train() client id: f_00007-7-3 loss: 0.716805  [  128/  179]
train() client id: f_00007-7-4 loss: 0.644771  [  160/  179]
train() client id: f_00007-8-0 loss: 0.518647  [   32/  179]
train() client id: f_00007-8-1 loss: 0.798783  [   64/  179]
train() client id: f_00007-8-2 loss: 0.428113  [   96/  179]
train() client id: f_00007-8-3 loss: 0.438922  [  128/  179]
train() client id: f_00007-8-4 loss: 0.501521  [  160/  179]
train() client id: f_00007-9-0 loss: 0.562944  [   32/  179]
train() client id: f_00007-9-1 loss: 0.505394  [   64/  179]
train() client id: f_00007-9-2 loss: 0.571955  [   96/  179]
train() client id: f_00007-9-3 loss: 0.623089  [  128/  179]
train() client id: f_00007-9-4 loss: 0.427538  [  160/  179]
train() client id: f_00007-10-0 loss: 0.501369  [   32/  179]
train() client id: f_00007-10-1 loss: 0.503295  [   64/  179]
train() client id: f_00007-10-2 loss: 0.684963  [   96/  179]
train() client id: f_00007-10-3 loss: 0.515802  [  128/  179]
train() client id: f_00007-10-4 loss: 0.585716  [  160/  179]
train() client id: f_00007-11-0 loss: 0.590047  [   32/  179]
train() client id: f_00007-11-1 loss: 0.565757  [   64/  179]
train() client id: f_00007-11-2 loss: 0.688918  [   96/  179]
train() client id: f_00007-11-3 loss: 0.497167  [  128/  179]
train() client id: f_00007-11-4 loss: 0.459639  [  160/  179]
train() client id: f_00007-12-0 loss: 0.590994  [   32/  179]
train() client id: f_00007-12-1 loss: 0.782091  [   64/  179]
train() client id: f_00007-12-2 loss: 0.517878  [   96/  179]
train() client id: f_00007-12-3 loss: 0.485274  [  128/  179]
train() client id: f_00007-12-4 loss: 0.404621  [  160/  179]
train() client id: f_00008-0-0 loss: 0.806429  [   32/  130]
train() client id: f_00008-0-1 loss: 0.877718  [   64/  130]
train() client id: f_00008-0-2 loss: 0.871745  [   96/  130]
train() client id: f_00008-0-3 loss: 0.890637  [  128/  130]
train() client id: f_00008-1-0 loss: 0.808492  [   32/  130]
train() client id: f_00008-1-1 loss: 0.933139  [   64/  130]
train() client id: f_00008-1-2 loss: 0.861442  [   96/  130]
train() client id: f_00008-1-3 loss: 0.833138  [  128/  130]
train() client id: f_00008-2-0 loss: 0.798592  [   32/  130]
train() client id: f_00008-2-1 loss: 0.858630  [   64/  130]
train() client id: f_00008-2-2 loss: 0.901818  [   96/  130]
train() client id: f_00008-2-3 loss: 0.896338  [  128/  130]
train() client id: f_00008-3-0 loss: 0.829040  [   32/  130]
train() client id: f_00008-3-1 loss: 0.853528  [   64/  130]
train() client id: f_00008-3-2 loss: 0.878983  [   96/  130]
train() client id: f_00008-3-3 loss: 0.837232  [  128/  130]
train() client id: f_00008-4-0 loss: 0.745547  [   32/  130]
train() client id: f_00008-4-1 loss: 0.952246  [   64/  130]
train() client id: f_00008-4-2 loss: 0.915422  [   96/  130]
train() client id: f_00008-4-3 loss: 0.814415  [  128/  130]
train() client id: f_00008-5-0 loss: 0.816050  [   32/  130]
train() client id: f_00008-5-1 loss: 0.953191  [   64/  130]
train() client id: f_00008-5-2 loss: 0.835678  [   96/  130]
train() client id: f_00008-5-3 loss: 0.815261  [  128/  130]
train() client id: f_00008-6-0 loss: 0.923230  [   32/  130]
train() client id: f_00008-6-1 loss: 0.843924  [   64/  130]
train() client id: f_00008-6-2 loss: 0.784597  [   96/  130]
train() client id: f_00008-6-3 loss: 0.898747  [  128/  130]
train() client id: f_00008-7-0 loss: 0.936087  [   32/  130]
train() client id: f_00008-7-1 loss: 0.927197  [   64/  130]
train() client id: f_00008-7-2 loss: 0.813419  [   96/  130]
train() client id: f_00008-7-3 loss: 0.776376  [  128/  130]
train() client id: f_00008-8-0 loss: 0.805880  [   32/  130]
train() client id: f_00008-8-1 loss: 0.982224  [   64/  130]
train() client id: f_00008-8-2 loss: 0.737758  [   96/  130]
train() client id: f_00008-8-3 loss: 0.893678  [  128/  130]
train() client id: f_00008-9-0 loss: 0.814608  [   32/  130]
train() client id: f_00008-9-1 loss: 0.822195  [   64/  130]
train() client id: f_00008-9-2 loss: 0.923105  [   96/  130]
train() client id: f_00008-9-3 loss: 0.884356  [  128/  130]
train() client id: f_00008-10-0 loss: 0.761540  [   32/  130]
train() client id: f_00008-10-1 loss: 0.869530  [   64/  130]
train() client id: f_00008-10-2 loss: 0.805726  [   96/  130]
train() client id: f_00008-10-3 loss: 1.015541  [  128/  130]
train() client id: f_00008-11-0 loss: 0.821731  [   32/  130]
train() client id: f_00008-11-1 loss: 0.928280  [   64/  130]
train() client id: f_00008-11-2 loss: 0.851116  [   96/  130]
train() client id: f_00008-11-3 loss: 0.815671  [  128/  130]
train() client id: f_00008-12-0 loss: 0.861439  [   32/  130]
train() client id: f_00008-12-1 loss: 0.784609  [   64/  130]
train() client id: f_00008-12-2 loss: 0.926559  [   96/  130]
train() client id: f_00008-12-3 loss: 0.843861  [  128/  130]
train() client id: f_00009-0-0 loss: 1.226704  [   32/  118]
train() client id: f_00009-0-1 loss: 1.296363  [   64/  118]
train() client id: f_00009-0-2 loss: 1.216814  [   96/  118]
train() client id: f_00009-1-0 loss: 1.243295  [   32/  118]
train() client id: f_00009-1-1 loss: 1.199570  [   64/  118]
train() client id: f_00009-1-2 loss: 1.141188  [   96/  118]
train() client id: f_00009-2-0 loss: 1.242757  [   32/  118]
train() client id: f_00009-2-1 loss: 1.183209  [   64/  118]
train() client id: f_00009-2-2 loss: 1.091579  [   96/  118]
train() client id: f_00009-3-0 loss: 1.128538  [   32/  118]
train() client id: f_00009-3-1 loss: 1.188884  [   64/  118]
train() client id: f_00009-3-2 loss: 1.087061  [   96/  118]
train() client id: f_00009-4-0 loss: 1.135090  [   32/  118]
train() client id: f_00009-4-1 loss: 1.103619  [   64/  118]
train() client id: f_00009-4-2 loss: 1.127017  [   96/  118]
train() client id: f_00009-5-0 loss: 1.010081  [   32/  118]
train() client id: f_00009-5-1 loss: 1.009776  [   64/  118]
train() client id: f_00009-5-2 loss: 1.194849  [   96/  118]
train() client id: f_00009-6-0 loss: 1.030698  [   32/  118]
train() client id: f_00009-6-1 loss: 1.135864  [   64/  118]
train() client id: f_00009-6-2 loss: 0.939375  [   96/  118]
train() client id: f_00009-7-0 loss: 1.106428  [   32/  118]
train() client id: f_00009-7-1 loss: 1.030014  [   64/  118]
train() client id: f_00009-7-2 loss: 1.004558  [   96/  118]
train() client id: f_00009-8-0 loss: 1.053851  [   32/  118]
train() client id: f_00009-8-1 loss: 0.946039  [   64/  118]
train() client id: f_00009-8-2 loss: 1.064057  [   96/  118]
train() client id: f_00009-9-0 loss: 1.069640  [   32/  118]
train() client id: f_00009-9-1 loss: 0.961056  [   64/  118]
train() client id: f_00009-9-2 loss: 1.002464  [   96/  118]
train() client id: f_00009-10-0 loss: 0.991873  [   32/  118]
train() client id: f_00009-10-1 loss: 1.071400  [   64/  118]
train() client id: f_00009-10-2 loss: 0.908572  [   96/  118]
train() client id: f_00009-11-0 loss: 1.005511  [   32/  118]
train() client id: f_00009-11-1 loss: 1.054020  [   64/  118]
train() client id: f_00009-11-2 loss: 1.005632  [   96/  118]
train() client id: f_00009-12-0 loss: 0.921519  [   32/  118]
train() client id: f_00009-12-1 loss: 1.096503  [   64/  118]
train() client id: f_00009-12-2 loss: 1.040893  [   96/  118]
At round 3 accuracy: 0.6180371352785146
At round 3 training accuracy: 0.5694164989939637
At round 3 training loss: 0.879257087182964
update_location
xs = 10.471708 -83.998411 55.045120 74.056472 -175.103519 -110.217951 12.784040 -11.624259 -1.680116 -65.304393 
ys = 17.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 16.875078 -19.177652 67.845030 -0.998518 
xs mean: -29.55713084058713
ys mean: 11.371751218646876
dists_uav = 102.073469 131.520786 114.156512 126.461971 201.862994 149.811048 102.216437 102.483685 120.854338 119.438942 
uav_gains = -100.222843 -102.975629 -101.437623 -102.549445 -107.799612 -104.393072 -100.238040 -100.266391 -102.056793 -101.928849 
uav_gains_db_mean: -102.38682967264094
dists_bs = 243.265914 183.585756 288.245184 292.013196 165.649846 202.810539 245.492030 253.764838 203.769404 207.385160 
bs_gains = -106.377334 -102.954551 -108.440397 -108.598329 -101.704408 -104.165594 -106.488106 -106.891140 -104.222950 -104.436834 
bs_gains_db_mean: -105.4279642657619
Round 4
-------------------------------
ene_coms = [0.00636125 0.0071816  0.00670204 0.00704705 0.00678176 0.0076156
 0.0063653  0.00637287 0.00763743 0.00771992]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.46412507 21.84004132 10.30999211  3.70315236 25.17814481 12.13172264
  4.58318238 14.79354792 10.82541331  9.84747197]
obj_prev = 123.6767938848963
eta_min = 8.983229722189704e-10	eta_max = 0.929265116357253
af = 26.181209634028566	bf = 1.8710999063148002	zeta = 28.799330597431425	eta = 0.9090909090909091
af = 26.181209634028566	bf = 1.8710999063148002	zeta = 48.030792744316635	eta = 0.5450921822881327
af = 26.181209634028566	bf = 1.8710999063148002	zeta = 39.03618834964385	eta = 0.6706907293182848
af = 26.181209634028566	bf = 1.8710999063148002	zeta = 37.43930339941195	eta = 0.6992974563314067
af = 26.181209634028566	bf = 1.8710999063148002	zeta = 37.36440267410051	eta = 0.7006992688304454
af = 26.181209634028566	bf = 1.8710999063148002	zeta = 37.36422738771903	eta = 0.7007025560130776
eta = 0.7007025560130776
ene_coms = [0.00636125 0.0071816  0.00670204 0.00704705 0.00678176 0.0076156
 0.0063653  0.00637287 0.00763743 0.00771992]
ene_comp = [0.02934918 0.06172646 0.02888332 0.01001599 0.07127659 0.03400778
 0.01257822 0.04169448 0.0302809  0.02748574]
ene_total = [3.19910226 6.17309593 3.18789761 1.5285849  6.99281984 3.72881119
 1.69704652 4.30609068 3.39689593 3.15388253]
ti_comp = [0.26903405 0.26083053 0.26562619 0.262176   0.26482899 0.25649055
 0.26899354 0.26891781 0.25627225 0.25544737]
ti_coms = [0.0636125  0.07181602 0.06702036 0.07047055 0.06781756 0.076156
 0.06365301 0.06372874 0.07637429 0.07719918]
t_total = [29.79998322 29.79998322 29.79998322 29.79998322 29.79998322 29.79998322
 29.79998322 29.79998322 29.79998322 29.79998322]
ene_coms = [0.00636125 0.0071816  0.00670204 0.00704705 0.00678176 0.0076156
 0.0063653  0.00637287 0.00763743 0.00771992]
ene_comp = [2.18299936e-05 2.16061439e-04 2.13441940e-05 9.13641837e-07
 3.22693116e-04 3.73656311e-05 1.71891370e-06 6.26435597e-05
 2.64230750e-05 1.98883906e-05]
ene_total = [0.57182516 0.66271619 0.60231085 0.63138891 0.63644867 0.68558732
 0.57038649 0.57652274 0.68656259 0.6933669 ]
optimize_network iter = 0 obj = 6.317115825237073
eta = 0.7007025560130776
freqs = [5.45454820e+07 1.18326753e+08 5.43683671e+07 1.91016557e+07
 1.34570972e+08 6.62944215e+07 2.33801562e+07 7.75227108e+07
 5.90795603e+07 5.37992311e+07]
eta_min = 0.6608422164896055	eta_max = 0.7007025560130751
af = 0.05509774398391034	bf = 1.8710999063148002	zeta = 0.06060751838230138	eta = 0.9090909090909091
af = 0.05509774398391034	bf = 1.8710999063148002	zeta = 20.639864104207906	eta = 0.002669481916437493
af = 0.05509774398391034	bf = 1.8710999063148002	zeta = 2.203484183251747	eta = 0.0250048284451949
af = 0.05509774398391034	bf = 1.8710999063148002	zeta = 2.1275368967491253	eta = 0.025897432880294415
af = 0.05509774398391034	bf = 1.8710999063148002	zeta = 2.1275025069767906	eta = 0.025897851496403154
eta = 0.025897851496403154
ene_coms = [0.00636125 0.0071816  0.00670204 0.00704705 0.00678176 0.0076156
 0.0063653  0.00637287 0.00763743 0.00771992]
ene_comp = [2.24245954e-04 2.21946486e-03 2.19255637e-04 9.38527466e-06
 3.31482579e-03 3.83833901e-04 1.76573319e-05 6.43498349e-04
 2.71427824e-04 2.04301074e-04]
ene_total = [0.18126751 0.25876686 0.19051039 0.19423038 0.27791108 0.22018654
 0.17569263 0.19312749 0.21769339 0.21811623]
ti_comp = [0.31333582 0.30513229 0.30992795 0.30647776 0.30913076 0.30079231
 0.3132953  0.31321958 0.30057402 0.29974913]
ti_coms = [0.0636125  0.07181602 0.06702036 0.07047055 0.06781756 0.076156
 0.06365301 0.06372874 0.07637429 0.07719918]
t_total = [29.79998322 29.79998322 29.79998322 29.79998322 29.79998322 29.79998322
 29.79998322 29.79998322 29.79998322 29.79998322]
ene_coms = [0.00636125 0.0071816  0.00670204 0.00704705 0.00678176 0.0076156
 0.0063653  0.00637287 0.00763743 0.00771992]
ene_comp = [2.54247326e-05 2.49417041e-04 2.47689962e-05 1.05626348e-06
 3.74149082e-04 4.29229881e-05 2.00188147e-06 7.29501405e-05
 3.03453409e-05 2.28189153e-05]
ene_total = [0.50490421 0.58746582 0.53179356 0.55719468 0.56571639 0.60545135
 0.5033728  0.50958031 0.60618274 0.61210896]
optimize_network iter = 1 obj = 5.583770821973434
eta = 0.6608422164896055
freqs = [5.45454820e+07 1.17802900e+08 5.42699248e+07 1.90312697e+07
 1.34269536e+08 6.58391788e+07 2.33796583e+07 7.75179734e+07
 5.86665019e+07 5.33976758e+07]
eta_min = 0.6608422164896307	eta_max = 0.6608422164896031
af = 0.05474946863067864	bf = 1.8710999063148002	zeta = 0.060224415493746505	eta = 0.9090909090909091
af = 0.05474946863067864	bf = 1.8710999063148002	zeta = 20.63949896815599	eta = 0.0026526549270963315
af = 0.05474946863067864	bf = 1.8710999063148002	zeta = 2.201724807183894	eta = 0.024866626588408974
af = 0.05474946863067864	bf = 1.8710999063148002	zeta = 2.126229108396108	eta = 0.025749562177698786
af = 0.05474946863067864	bf = 1.8710999063148002	zeta = 2.1261952964444153	eta = 0.0257499716616977
eta = 0.0257499716616977
ene_coms = [0.00636125 0.0071816  0.00670204 0.00704705 0.00678176 0.0076156
 0.0063653  0.00637287 0.00763743 0.00771992]
ene_comp = [2.24597427e-04 2.20330442e-03 2.18804772e-04 9.33083797e-06
 3.30516441e-03 3.79173809e-04 1.76842540e-05 6.44428170e-04
 2.68065179e-04 2.01578116e-04]
ene_total = [0.18124967 0.25828283 0.19046907 0.1941994  0.277603   0.22002487
 0.1756667  0.19312377 0.2175678  0.21800819]
ti_comp = [0.31333582 0.30513229 0.30992795 0.30647776 0.30913076 0.30079231
 0.3132953  0.31321958 0.30057402 0.29974913]
ti_coms = [0.0636125  0.07181602 0.06702036 0.07047055 0.06781756 0.076156
 0.06365301 0.06372874 0.07637429 0.07719918]
t_total = [29.79998322 29.79998322 29.79998322 29.79998322 29.79998322 29.79998322
 29.79998322 29.79998322 29.79998322 29.79998322]
ene_coms = [0.00636125 0.0071816  0.00670204 0.00704705 0.00678176 0.0076156
 0.0063653  0.00637287 0.00763743 0.00771992]
ene_comp = [2.54247326e-05 2.49417041e-04 2.47689962e-05 1.05626348e-06
 3.74149082e-04 4.29229881e-05 2.00188147e-06 7.29501405e-05
 3.03453409e-05 2.28189153e-05]
ene_total = [0.50490421 0.58746582 0.53179356 0.55719468 0.56571639 0.60545135
 0.5033728  0.50958031 0.60618274 0.61210896]
optimize_network iter = 2 obj = 5.583770821973843
eta = 0.6608422164896307
freqs = [5.45454820e+07 1.17802900e+08 5.42699248e+07 1.90312697e+07
 1.34269536e+08 6.58391788e+07 2.33796583e+07 7.75179734e+07
 5.86665019e+07 5.33976758e+07]
Done!
ene_coms = [0.00636125 0.0071816  0.00670204 0.00704705 0.00678176 0.0076156
 0.0063653  0.00637287 0.00763743 0.00771992]
ene_comp = [2.43669666e-05 2.39040340e-04 2.37385114e-05 1.01231889e-06
 3.58583052e-04 4.11372279e-05 1.91859556e-06 6.99151362e-05
 2.90828589e-05 2.18695613e-05]
ene_total = [0.00638562 0.00742064 0.00672577 0.00704807 0.00714034 0.00765674
 0.00636722 0.00644279 0.00766651 0.00774179]
At round 4 energy consumption: 0.07059548605841273
At round 4 eta: 0.6608422164896307
At round 4 a_n: 26.812419468377104
At round 4 local rounds: 13.564327819699978
At round 4 global rounds: 79.05588717694091
gradient difference: 0.3378887474536896
train() client id: f_00000-0-0 loss: 1.227895  [   32/  126]
train() client id: f_00000-0-1 loss: 0.976403  [   64/  126]
train() client id: f_00000-0-2 loss: 0.972316  [   96/  126]
train() client id: f_00000-1-0 loss: 0.969105  [   32/  126]
train() client id: f_00000-1-1 loss: 1.013909  [   64/  126]
train() client id: f_00000-1-2 loss: 0.908861  [   96/  126]
train() client id: f_00000-2-0 loss: 0.994613  [   32/  126]
train() client id: f_00000-2-1 loss: 0.860873  [   64/  126]
train() client id: f_00000-2-2 loss: 0.881845  [   96/  126]
train() client id: f_00000-3-0 loss: 0.936979  [   32/  126]
train() client id: f_00000-3-1 loss: 0.789494  [   64/  126]
train() client id: f_00000-3-2 loss: 0.756610  [   96/  126]
train() client id: f_00000-4-0 loss: 0.841483  [   32/  126]
train() client id: f_00000-4-1 loss: 0.739303  [   64/  126]
train() client id: f_00000-4-2 loss: 0.754466  [   96/  126]
train() client id: f_00000-5-0 loss: 0.695466  [   32/  126]
train() client id: f_00000-5-1 loss: 0.911657  [   64/  126]
train() client id: f_00000-5-2 loss: 0.774951  [   96/  126]
train() client id: f_00000-6-0 loss: 0.859823  [   32/  126]
train() client id: f_00000-6-1 loss: 0.737125  [   64/  126]
train() client id: f_00000-6-2 loss: 0.593668  [   96/  126]
train() client id: f_00000-7-0 loss: 0.758836  [   32/  126]
train() client id: f_00000-7-1 loss: 0.714514  [   64/  126]
train() client id: f_00000-7-2 loss: 0.681382  [   96/  126]
train() client id: f_00000-8-0 loss: 0.750764  [   32/  126]
train() client id: f_00000-8-1 loss: 0.760128  [   64/  126]
train() client id: f_00000-8-2 loss: 0.645477  [   96/  126]
train() client id: f_00000-9-0 loss: 0.792127  [   32/  126]
train() client id: f_00000-9-1 loss: 0.567903  [   64/  126]
train() client id: f_00000-9-2 loss: 0.654633  [   96/  126]
train() client id: f_00000-10-0 loss: 0.735428  [   32/  126]
train() client id: f_00000-10-1 loss: 0.711336  [   64/  126]
train() client id: f_00000-10-2 loss: 0.684205  [   96/  126]
train() client id: f_00000-11-0 loss: 0.700932  [   32/  126]
train() client id: f_00000-11-1 loss: 0.841010  [   64/  126]
train() client id: f_00000-11-2 loss: 0.557948  [   96/  126]
train() client id: f_00000-12-0 loss: 0.887380  [   32/  126]
train() client id: f_00000-12-1 loss: 0.604259  [   64/  126]
train() client id: f_00000-12-2 loss: 0.541485  [   96/  126]
train() client id: f_00001-0-0 loss: 0.615235  [   32/  265]
train() client id: f_00001-0-1 loss: 0.667741  [   64/  265]
train() client id: f_00001-0-2 loss: 0.626125  [   96/  265]
train() client id: f_00001-0-3 loss: 0.600895  [  128/  265]
train() client id: f_00001-0-4 loss: 0.644931  [  160/  265]
train() client id: f_00001-0-5 loss: 0.534500  [  192/  265]
train() client id: f_00001-0-6 loss: 0.582287  [  224/  265]
train() client id: f_00001-0-7 loss: 0.674971  [  256/  265]
train() client id: f_00001-1-0 loss: 0.726954  [   32/  265]
train() client id: f_00001-1-1 loss: 0.542647  [   64/  265]
train() client id: f_00001-1-2 loss: 0.611001  [   96/  265]
train() client id: f_00001-1-3 loss: 0.571844  [  128/  265]
train() client id: f_00001-1-4 loss: 0.560374  [  160/  265]
train() client id: f_00001-1-5 loss: 0.589922  [  192/  265]
train() client id: f_00001-1-6 loss: 0.562012  [  224/  265]
train() client id: f_00001-1-7 loss: 0.549401  [  256/  265]
train() client id: f_00001-2-0 loss: 0.563299  [   32/  265]
train() client id: f_00001-2-1 loss: 0.541538  [   64/  265]
train() client id: f_00001-2-2 loss: 0.531463  [   96/  265]
train() client id: f_00001-2-3 loss: 0.576639  [  128/  265]
train() client id: f_00001-2-4 loss: 0.573217  [  160/  265]
train() client id: f_00001-2-5 loss: 0.576569  [  192/  265]
train() client id: f_00001-2-6 loss: 0.698468  [  224/  265]
train() client id: f_00001-2-7 loss: 0.556507  [  256/  265]
train() client id: f_00001-3-0 loss: 0.525244  [   32/  265]
train() client id: f_00001-3-1 loss: 0.567909  [   64/  265]
train() client id: f_00001-3-2 loss: 0.527238  [   96/  265]
train() client id: f_00001-3-3 loss: 0.616875  [  128/  265]
train() client id: f_00001-3-4 loss: 0.614841  [  160/  265]
train() client id: f_00001-3-5 loss: 0.567542  [  192/  265]
train() client id: f_00001-3-6 loss: 0.562982  [  224/  265]
train() client id: f_00001-3-7 loss: 0.531898  [  256/  265]
train() client id: f_00001-4-0 loss: 0.506570  [   32/  265]
train() client id: f_00001-4-1 loss: 0.538860  [   64/  265]
train() client id: f_00001-4-2 loss: 0.516084  [   96/  265]
train() client id: f_00001-4-3 loss: 0.643312  [  128/  265]
train() client id: f_00001-4-4 loss: 0.477876  [  160/  265]
train() client id: f_00001-4-5 loss: 0.634766  [  192/  265]
train() client id: f_00001-4-6 loss: 0.589695  [  224/  265]
train() client id: f_00001-4-7 loss: 0.494549  [  256/  265]
train() client id: f_00001-5-0 loss: 0.535882  [   32/  265]
train() client id: f_00001-5-1 loss: 0.617270  [   64/  265]
train() client id: f_00001-5-2 loss: 0.582307  [   96/  265]
train() client id: f_00001-5-3 loss: 0.479467  [  128/  265]
train() client id: f_00001-5-4 loss: 0.498974  [  160/  265]
train() client id: f_00001-5-5 loss: 0.620967  [  192/  265]
train() client id: f_00001-5-6 loss: 0.542682  [  224/  265]
train() client id: f_00001-5-7 loss: 0.485461  [  256/  265]
train() client id: f_00001-6-0 loss: 0.516669  [   32/  265]
train() client id: f_00001-6-1 loss: 0.574849  [   64/  265]
train() client id: f_00001-6-2 loss: 0.542300  [   96/  265]
train() client id: f_00001-6-3 loss: 0.561514  [  128/  265]
train() client id: f_00001-6-4 loss: 0.569266  [  160/  265]
train() client id: f_00001-6-5 loss: 0.481389  [  192/  265]
train() client id: f_00001-6-6 loss: 0.610370  [  224/  265]
train() client id: f_00001-6-7 loss: 0.474585  [  256/  265]
train() client id: f_00001-7-0 loss: 0.579239  [   32/  265]
train() client id: f_00001-7-1 loss: 0.455534  [   64/  265]
train() client id: f_00001-7-2 loss: 0.522869  [   96/  265]
train() client id: f_00001-7-3 loss: 0.550334  [  128/  265]
train() client id: f_00001-7-4 loss: 0.597194  [  160/  265]
train() client id: f_00001-7-5 loss: 0.477721  [  192/  265]
train() client id: f_00001-7-6 loss: 0.514999  [  224/  265]
train() client id: f_00001-7-7 loss: 0.615668  [  256/  265]
train() client id: f_00001-8-0 loss: 0.538292  [   32/  265]
train() client id: f_00001-8-1 loss: 0.543645  [   64/  265]
train() client id: f_00001-8-2 loss: 0.457889  [   96/  265]
train() client id: f_00001-8-3 loss: 0.535773  [  128/  265]
train() client id: f_00001-8-4 loss: 0.605300  [  160/  265]
train() client id: f_00001-8-5 loss: 0.555846  [  192/  265]
train() client id: f_00001-8-6 loss: 0.565712  [  224/  265]
train() client id: f_00001-8-7 loss: 0.460320  [  256/  265]
train() client id: f_00001-9-0 loss: 0.519532  [   32/  265]
train() client id: f_00001-9-1 loss: 0.609706  [   64/  265]
train() client id: f_00001-9-2 loss: 0.473397  [   96/  265]
train() client id: f_00001-9-3 loss: 0.470087  [  128/  265]
train() client id: f_00001-9-4 loss: 0.460969  [  160/  265]
train() client id: f_00001-9-5 loss: 0.692983  [  192/  265]
train() client id: f_00001-9-6 loss: 0.483311  [  224/  265]
train() client id: f_00001-9-7 loss: 0.503788  [  256/  265]
train() client id: f_00001-10-0 loss: 0.595284  [   32/  265]
train() client id: f_00001-10-1 loss: 0.517069  [   64/  265]
train() client id: f_00001-10-2 loss: 0.510607  [   96/  265]
train() client id: f_00001-10-3 loss: 0.502725  [  128/  265]
train() client id: f_00001-10-4 loss: 0.591807  [  160/  265]
train() client id: f_00001-10-5 loss: 0.455733  [  192/  265]
train() client id: f_00001-10-6 loss: 0.602456  [  224/  265]
train() client id: f_00001-10-7 loss: 0.510724  [  256/  265]
train() client id: f_00001-11-0 loss: 0.518422  [   32/  265]
train() client id: f_00001-11-1 loss: 0.564311  [   64/  265]
train() client id: f_00001-11-2 loss: 0.530693  [   96/  265]
train() client id: f_00001-11-3 loss: 0.420256  [  128/  265]
train() client id: f_00001-11-4 loss: 0.536119  [  160/  265]
train() client id: f_00001-11-5 loss: 0.616983  [  192/  265]
train() client id: f_00001-11-6 loss: 0.631535  [  224/  265]
train() client id: f_00001-11-7 loss: 0.463491  [  256/  265]
train() client id: f_00001-12-0 loss: 0.468568  [   32/  265]
train() client id: f_00001-12-1 loss: 0.562179  [   64/  265]
train() client id: f_00001-12-2 loss: 0.471487  [   96/  265]
train() client id: f_00001-12-3 loss: 0.503700  [  128/  265]
train() client id: f_00001-12-4 loss: 0.677639  [  160/  265]
train() client id: f_00001-12-5 loss: 0.446626  [  192/  265]
train() client id: f_00001-12-6 loss: 0.568085  [  224/  265]
train() client id: f_00001-12-7 loss: 0.592944  [  256/  265]
train() client id: f_00002-0-0 loss: 0.986251  [   32/  124]
train() client id: f_00002-0-1 loss: 0.959299  [   64/  124]
train() client id: f_00002-0-2 loss: 1.102923  [   96/  124]
train() client id: f_00002-1-0 loss: 0.917301  [   32/  124]
train() client id: f_00002-1-1 loss: 0.973945  [   64/  124]
train() client id: f_00002-1-2 loss: 1.112591  [   96/  124]
train() client id: f_00002-2-0 loss: 1.042433  [   32/  124]
train() client id: f_00002-2-1 loss: 0.844123  [   64/  124]
train() client id: f_00002-2-2 loss: 0.917947  [   96/  124]
train() client id: f_00002-3-0 loss: 0.807840  [   32/  124]
train() client id: f_00002-3-1 loss: 0.840772  [   64/  124]
train() client id: f_00002-3-2 loss: 1.079236  [   96/  124]
train() client id: f_00002-4-0 loss: 0.875501  [   32/  124]
train() client id: f_00002-4-1 loss: 1.121340  [   64/  124]
train() client id: f_00002-4-2 loss: 0.814977  [   96/  124]
train() client id: f_00002-5-0 loss: 0.956234  [   32/  124]
train() client id: f_00002-5-1 loss: 0.976823  [   64/  124]
train() client id: f_00002-5-2 loss: 0.662850  [   96/  124]
train() client id: f_00002-6-0 loss: 0.784366  [   32/  124]
train() client id: f_00002-6-1 loss: 0.842155  [   64/  124]
train() client id: f_00002-6-2 loss: 0.885549  [   96/  124]
train() client id: f_00002-7-0 loss: 0.865148  [   32/  124]
train() client id: f_00002-7-1 loss: 0.757112  [   64/  124]
train() client id: f_00002-7-2 loss: 0.946920  [   96/  124]
train() client id: f_00002-8-0 loss: 0.739107  [   32/  124]
train() client id: f_00002-8-1 loss: 0.756036  [   64/  124]
train() client id: f_00002-8-2 loss: 1.016330  [   96/  124]
train() client id: f_00002-9-0 loss: 0.826599  [   32/  124]
train() client id: f_00002-9-1 loss: 0.795083  [   64/  124]
train() client id: f_00002-9-2 loss: 0.866021  [   96/  124]
train() client id: f_00002-10-0 loss: 0.893507  [   32/  124]
train() client id: f_00002-10-1 loss: 0.826457  [   64/  124]
train() client id: f_00002-10-2 loss: 0.770048  [   96/  124]
train() client id: f_00002-11-0 loss: 0.983987  [   32/  124]
train() client id: f_00002-11-1 loss: 0.596389  [   64/  124]
train() client id: f_00002-11-2 loss: 0.787327  [   96/  124]
train() client id: f_00002-12-0 loss: 0.603398  [   32/  124]
train() client id: f_00002-12-1 loss: 0.970488  [   64/  124]
train() client id: f_00002-12-2 loss: 0.726349  [   96/  124]
train() client id: f_00003-0-0 loss: 1.117914  [   32/   43]
train() client id: f_00003-1-0 loss: 1.124949  [   32/   43]
train() client id: f_00003-2-0 loss: 1.037502  [   32/   43]
train() client id: f_00003-3-0 loss: 1.121322  [   32/   43]
train() client id: f_00003-4-0 loss: 1.104211  [   32/   43]
train() client id: f_00003-5-0 loss: 0.995264  [   32/   43]
train() client id: f_00003-6-0 loss: 1.027037  [   32/   43]
train() client id: f_00003-7-0 loss: 1.009873  [   32/   43]
train() client id: f_00003-8-0 loss: 1.103610  [   32/   43]
train() client id: f_00003-9-0 loss: 1.087154  [   32/   43]
train() client id: f_00003-10-0 loss: 1.071002  [   32/   43]
train() client id: f_00003-11-0 loss: 1.081926  [   32/   43]
train() client id: f_00003-12-0 loss: 1.025537  [   32/   43]
train() client id: f_00004-0-0 loss: 0.736793  [   32/  306]
train() client id: f_00004-0-1 loss: 0.885550  [   64/  306]
train() client id: f_00004-0-2 loss: 0.678824  [   96/  306]
train() client id: f_00004-0-3 loss: 0.853811  [  128/  306]
train() client id: f_00004-0-4 loss: 0.914839  [  160/  306]
train() client id: f_00004-0-5 loss: 0.689576  [  192/  306]
train() client id: f_00004-0-6 loss: 0.820811  [  224/  306]
train() client id: f_00004-0-7 loss: 0.718658  [  256/  306]
train() client id: f_00004-0-8 loss: 0.726036  [  288/  306]
train() client id: f_00004-1-0 loss: 0.767082  [   32/  306]
train() client id: f_00004-1-1 loss: 0.735958  [   64/  306]
train() client id: f_00004-1-2 loss: 0.831508  [   96/  306]
train() client id: f_00004-1-3 loss: 0.870359  [  128/  306]
train() client id: f_00004-1-4 loss: 0.776114  [  160/  306]
train() client id: f_00004-1-5 loss: 0.687067  [  192/  306]
train() client id: f_00004-1-6 loss: 0.917573  [  224/  306]
train() client id: f_00004-1-7 loss: 0.651849  [  256/  306]
train() client id: f_00004-1-8 loss: 0.725914  [  288/  306]
train() client id: f_00004-2-0 loss: 0.756485  [   32/  306]
train() client id: f_00004-2-1 loss: 0.775588  [   64/  306]
train() client id: f_00004-2-2 loss: 0.777106  [   96/  306]
train() client id: f_00004-2-3 loss: 0.841502  [  128/  306]
train() client id: f_00004-2-4 loss: 0.981970  [  160/  306]
train() client id: f_00004-2-5 loss: 0.640360  [  192/  306]
train() client id: f_00004-2-6 loss: 0.859674  [  224/  306]
train() client id: f_00004-2-7 loss: 0.718086  [  256/  306]
train() client id: f_00004-2-8 loss: 0.668149  [  288/  306]
train() client id: f_00004-3-0 loss: 0.822711  [   32/  306]
train() client id: f_00004-3-1 loss: 0.766452  [   64/  306]
train() client id: f_00004-3-2 loss: 0.788059  [   96/  306]
train() client id: f_00004-3-3 loss: 0.853393  [  128/  306]
train() client id: f_00004-3-4 loss: 0.844431  [  160/  306]
train() client id: f_00004-3-5 loss: 0.713905  [  192/  306]
train() client id: f_00004-3-6 loss: 0.736888  [  224/  306]
train() client id: f_00004-3-7 loss: 0.763948  [  256/  306]
train() client id: f_00004-3-8 loss: 0.691869  [  288/  306]
train() client id: f_00004-4-0 loss: 0.833181  [   32/  306]
train() client id: f_00004-4-1 loss: 0.739312  [   64/  306]
train() client id: f_00004-4-2 loss: 0.711618  [   96/  306]
train() client id: f_00004-4-3 loss: 0.764731  [  128/  306]
train() client id: f_00004-4-4 loss: 0.926932  [  160/  306]
train() client id: f_00004-4-5 loss: 0.644649  [  192/  306]
train() client id: f_00004-4-6 loss: 0.807751  [  224/  306]
train() client id: f_00004-4-7 loss: 0.818101  [  256/  306]
train() client id: f_00004-4-8 loss: 0.758668  [  288/  306]
train() client id: f_00004-5-0 loss: 0.819409  [   32/  306]
train() client id: f_00004-5-1 loss: 0.779783  [   64/  306]
train() client id: f_00004-5-2 loss: 0.829694  [   96/  306]
train() client id: f_00004-5-3 loss: 0.758118  [  128/  306]
train() client id: f_00004-5-4 loss: 0.673904  [  160/  306]
train() client id: f_00004-5-5 loss: 0.841999  [  192/  306]
train() client id: f_00004-5-6 loss: 0.728692  [  224/  306]
train() client id: f_00004-5-7 loss: 0.916794  [  256/  306]
train() client id: f_00004-5-8 loss: 0.819463  [  288/  306]
train() client id: f_00004-6-0 loss: 0.814142  [   32/  306]
train() client id: f_00004-6-1 loss: 0.733922  [   64/  306]
train() client id: f_00004-6-2 loss: 0.887028  [   96/  306]
train() client id: f_00004-6-3 loss: 0.849438  [  128/  306]
train() client id: f_00004-6-4 loss: 0.762313  [  160/  306]
train() client id: f_00004-6-5 loss: 0.822520  [  192/  306]
train() client id: f_00004-6-6 loss: 0.737730  [  224/  306]
train() client id: f_00004-6-7 loss: 0.797642  [  256/  306]
train() client id: f_00004-6-8 loss: 0.633598  [  288/  306]
train() client id: f_00004-7-0 loss: 0.685955  [   32/  306]
train() client id: f_00004-7-1 loss: 0.699927  [   64/  306]
train() client id: f_00004-7-2 loss: 0.759741  [   96/  306]
train() client id: f_00004-7-3 loss: 0.872181  [  128/  306]
train() client id: f_00004-7-4 loss: 0.745178  [  160/  306]
train() client id: f_00004-7-5 loss: 0.894097  [  192/  306]
train() client id: f_00004-7-6 loss: 0.877583  [  224/  306]
train() client id: f_00004-7-7 loss: 0.707720  [  256/  306]
train() client id: f_00004-7-8 loss: 0.761043  [  288/  306]
train() client id: f_00004-8-0 loss: 0.748121  [   32/  306]
train() client id: f_00004-8-1 loss: 0.767820  [   64/  306]
train() client id: f_00004-8-2 loss: 0.838225  [   96/  306]
train() client id: f_00004-8-3 loss: 0.716922  [  128/  306]
train() client id: f_00004-8-4 loss: 0.726534  [  160/  306]
train() client id: f_00004-8-5 loss: 0.813786  [  192/  306]
train() client id: f_00004-8-6 loss: 0.700969  [  224/  306]
train() client id: f_00004-8-7 loss: 0.889104  [  256/  306]
train() client id: f_00004-8-8 loss: 0.873842  [  288/  306]
train() client id: f_00004-9-0 loss: 0.785254  [   32/  306]
train() client id: f_00004-9-1 loss: 0.797914  [   64/  306]
train() client id: f_00004-9-2 loss: 0.856728  [   96/  306]
train() client id: f_00004-9-3 loss: 0.836019  [  128/  306]
train() client id: f_00004-9-4 loss: 0.759415  [  160/  306]
train() client id: f_00004-9-5 loss: 0.759444  [  192/  306]
train() client id: f_00004-9-6 loss: 0.764198  [  224/  306]
train() client id: f_00004-9-7 loss: 0.781546  [  256/  306]
train() client id: f_00004-9-8 loss: 0.785243  [  288/  306]
train() client id: f_00004-10-0 loss: 0.736454  [   32/  306]
train() client id: f_00004-10-1 loss: 0.927614  [   64/  306]
train() client id: f_00004-10-2 loss: 0.834598  [   96/  306]
train() client id: f_00004-10-3 loss: 0.795677  [  128/  306]
train() client id: f_00004-10-4 loss: 0.847019  [  160/  306]
train() client id: f_00004-10-5 loss: 0.715039  [  192/  306]
train() client id: f_00004-10-6 loss: 0.773965  [  224/  306]
train() client id: f_00004-10-7 loss: 0.748465  [  256/  306]
train() client id: f_00004-10-8 loss: 0.788385  [  288/  306]
train() client id: f_00004-11-0 loss: 0.805339  [   32/  306]
train() client id: f_00004-11-1 loss: 0.891457  [   64/  306]
train() client id: f_00004-11-2 loss: 0.827576  [   96/  306]
train() client id: f_00004-11-3 loss: 0.708493  [  128/  306]
train() client id: f_00004-11-4 loss: 0.760936  [  160/  306]
train() client id: f_00004-11-5 loss: 0.771578  [  192/  306]
train() client id: f_00004-11-6 loss: 0.800118  [  224/  306]
train() client id: f_00004-11-7 loss: 0.798735  [  256/  306]
train() client id: f_00004-11-8 loss: 0.767952  [  288/  306]
train() client id: f_00004-12-0 loss: 0.878216  [   32/  306]
train() client id: f_00004-12-1 loss: 0.744629  [   64/  306]
train() client id: f_00004-12-2 loss: 0.809462  [   96/  306]
train() client id: f_00004-12-3 loss: 0.765007  [  128/  306]
train() client id: f_00004-12-4 loss: 0.837321  [  160/  306]
train() client id: f_00004-12-5 loss: 0.670000  [  192/  306]
train() client id: f_00004-12-6 loss: 0.742105  [  224/  306]
train() client id: f_00004-12-7 loss: 0.848600  [  256/  306]
train() client id: f_00004-12-8 loss: 0.882998  [  288/  306]
train() client id: f_00005-0-0 loss: 0.770876  [   32/  146]
train() client id: f_00005-0-1 loss: 0.513534  [   64/  146]
train() client id: f_00005-0-2 loss: 0.776740  [   96/  146]
train() client id: f_00005-0-3 loss: 0.682915  [  128/  146]
train() client id: f_00005-1-0 loss: 0.648517  [   32/  146]
train() client id: f_00005-1-1 loss: 0.559538  [   64/  146]
train() client id: f_00005-1-2 loss: 0.615877  [   96/  146]
train() client id: f_00005-1-3 loss: 0.653249  [  128/  146]
train() client id: f_00005-2-0 loss: 0.745639  [   32/  146]
train() client id: f_00005-2-1 loss: 0.758945  [   64/  146]
train() client id: f_00005-2-2 loss: 0.625992  [   96/  146]
train() client id: f_00005-2-3 loss: 0.391123  [  128/  146]
train() client id: f_00005-3-0 loss: 0.563768  [   32/  146]
train() client id: f_00005-3-1 loss: 0.602027  [   64/  146]
train() client id: f_00005-3-2 loss: 0.677604  [   96/  146]
train() client id: f_00005-3-3 loss: 0.535552  [  128/  146]
train() client id: f_00005-4-0 loss: 0.476770  [   32/  146]
train() client id: f_00005-4-1 loss: 0.539109  [   64/  146]
train() client id: f_00005-4-2 loss: 0.860283  [   96/  146]
train() client id: f_00005-4-3 loss: 0.702157  [  128/  146]
train() client id: f_00005-5-0 loss: 0.550670  [   32/  146]
train() client id: f_00005-5-1 loss: 0.516883  [   64/  146]
train() client id: f_00005-5-2 loss: 0.601422  [   96/  146]
train() client id: f_00005-5-3 loss: 0.746249  [  128/  146]
train() client id: f_00005-6-0 loss: 0.611078  [   32/  146]
train() client id: f_00005-6-1 loss: 0.713645  [   64/  146]
train() client id: f_00005-6-2 loss: 0.316112  [   96/  146]
train() client id: f_00005-6-3 loss: 0.675035  [  128/  146]
train() client id: f_00005-7-0 loss: 0.578957  [   32/  146]
train() client id: f_00005-7-1 loss: 0.788877  [   64/  146]
train() client id: f_00005-7-2 loss: 0.571031  [   96/  146]
train() client id: f_00005-7-3 loss: 0.661627  [  128/  146]
train() client id: f_00005-8-0 loss: 0.695985  [   32/  146]
train() client id: f_00005-8-1 loss: 0.599981  [   64/  146]
train() client id: f_00005-8-2 loss: 0.719266  [   96/  146]
train() client id: f_00005-8-3 loss: 0.529963  [  128/  146]
train() client id: f_00005-9-0 loss: 0.689479  [   32/  146]
train() client id: f_00005-9-1 loss: 0.604596  [   64/  146]
train() client id: f_00005-9-2 loss: 0.539455  [   96/  146]
train() client id: f_00005-9-3 loss: 0.564758  [  128/  146]
train() client id: f_00005-10-0 loss: 0.789882  [   32/  146]
train() client id: f_00005-10-1 loss: 0.749422  [   64/  146]
train() client id: f_00005-10-2 loss: 0.434031  [   96/  146]
train() client id: f_00005-10-3 loss: 0.436445  [  128/  146]
train() client id: f_00005-11-0 loss: 0.772938  [   32/  146]
train() client id: f_00005-11-1 loss: 0.552207  [   64/  146]
train() client id: f_00005-11-2 loss: 0.612241  [   96/  146]
train() client id: f_00005-11-3 loss: 0.623979  [  128/  146]
train() client id: f_00005-12-0 loss: 0.678750  [   32/  146]
train() client id: f_00005-12-1 loss: 0.546094  [   64/  146]
train() client id: f_00005-12-2 loss: 0.458992  [   96/  146]
train() client id: f_00005-12-3 loss: 0.917679  [  128/  146]
train() client id: f_00006-0-0 loss: 0.861633  [   32/   54]
train() client id: f_00006-1-0 loss: 0.762713  [   32/   54]
train() client id: f_00006-2-0 loss: 0.836215  [   32/   54]
train() client id: f_00006-3-0 loss: 0.783292  [   32/   54]
train() client id: f_00006-4-0 loss: 0.819590  [   32/   54]
train() client id: f_00006-5-0 loss: 0.830577  [   32/   54]
train() client id: f_00006-6-0 loss: 0.827917  [   32/   54]
train() client id: f_00006-7-0 loss: 0.854525  [   32/   54]
train() client id: f_00006-8-0 loss: 0.830712  [   32/   54]
train() client id: f_00006-9-0 loss: 0.866100  [   32/   54]
train() client id: f_00006-10-0 loss: 0.853339  [   32/   54]
train() client id: f_00006-11-0 loss: 0.892976  [   32/   54]
train() client id: f_00006-12-0 loss: 0.786872  [   32/   54]
train() client id: f_00007-0-0 loss: 0.730044  [   32/  179]
train() client id: f_00007-0-1 loss: 0.678175  [   64/  179]
train() client id: f_00007-0-2 loss: 0.708827  [   96/  179]
train() client id: f_00007-0-3 loss: 0.598296  [  128/  179]
train() client id: f_00007-0-4 loss: 0.675022  [  160/  179]
train() client id: f_00007-1-0 loss: 0.638643  [   32/  179]
train() client id: f_00007-1-1 loss: 0.703611  [   64/  179]
train() client id: f_00007-1-2 loss: 0.965788  [   96/  179]
train() client id: f_00007-1-3 loss: 0.611345  [  128/  179]
train() client id: f_00007-1-4 loss: 0.633163  [  160/  179]
train() client id: f_00007-2-0 loss: 0.728087  [   32/  179]
train() client id: f_00007-2-1 loss: 0.618787  [   64/  179]
train() client id: f_00007-2-2 loss: 0.654657  [   96/  179]
train() client id: f_00007-2-3 loss: 0.596595  [  128/  179]
train() client id: f_00007-2-4 loss: 0.925576  [  160/  179]
train() client id: f_00007-3-0 loss: 0.612586  [   32/  179]
train() client id: f_00007-3-1 loss: 0.652028  [   64/  179]
train() client id: f_00007-3-2 loss: 0.656035  [   96/  179]
train() client id: f_00007-3-3 loss: 0.642154  [  128/  179]
train() client id: f_00007-3-4 loss: 0.677699  [  160/  179]
train() client id: f_00007-4-0 loss: 0.696177  [   32/  179]
train() client id: f_00007-4-1 loss: 0.752845  [   64/  179]
train() client id: f_00007-4-2 loss: 0.820258  [   96/  179]
train() client id: f_00007-4-3 loss: 0.650124  [  128/  179]
train() client id: f_00007-4-4 loss: 0.556986  [  160/  179]
train() client id: f_00007-5-0 loss: 0.707881  [   32/  179]
train() client id: f_00007-5-1 loss: 0.700181  [   64/  179]
train() client id: f_00007-5-2 loss: 0.554979  [   96/  179]
train() client id: f_00007-5-3 loss: 0.761455  [  128/  179]
train() client id: f_00007-5-4 loss: 0.634503  [  160/  179]
train() client id: f_00007-6-0 loss: 0.735385  [   32/  179]
train() client id: f_00007-6-1 loss: 0.583657  [   64/  179]
train() client id: f_00007-6-2 loss: 0.841567  [   96/  179]
train() client id: f_00007-6-3 loss: 0.547185  [  128/  179]
train() client id: f_00007-6-4 loss: 0.632679  [  160/  179]
train() client id: f_00007-7-0 loss: 0.630954  [   32/  179]
train() client id: f_00007-7-1 loss: 0.781025  [   64/  179]
train() client id: f_00007-7-2 loss: 0.606251  [   96/  179]
train() client id: f_00007-7-3 loss: 0.775808  [  128/  179]
train() client id: f_00007-7-4 loss: 0.637654  [  160/  179]
train() client id: f_00007-8-0 loss: 0.760767  [   32/  179]
train() client id: f_00007-8-1 loss: 0.530410  [   64/  179]
train() client id: f_00007-8-2 loss: 0.725690  [   96/  179]
train() client id: f_00007-8-3 loss: 0.743035  [  128/  179]
train() client id: f_00007-8-4 loss: 0.603062  [  160/  179]
train() client id: f_00007-9-0 loss: 0.917892  [   32/  179]
train() client id: f_00007-9-1 loss: 0.643403  [   64/  179]
train() client id: f_00007-9-2 loss: 0.554369  [   96/  179]
train() client id: f_00007-9-3 loss: 0.625880  [  128/  179]
train() client id: f_00007-9-4 loss: 0.628892  [  160/  179]
train() client id: f_00007-10-0 loss: 0.725256  [   32/  179]
train() client id: f_00007-10-1 loss: 0.602135  [   64/  179]
train() client id: f_00007-10-2 loss: 0.852978  [   96/  179]
train() client id: f_00007-10-3 loss: 0.550155  [  128/  179]
train() client id: f_00007-10-4 loss: 0.550673  [  160/  179]
train() client id: f_00007-11-0 loss: 0.648659  [   32/  179]
train() client id: f_00007-11-1 loss: 0.696229  [   64/  179]
train() client id: f_00007-11-2 loss: 0.666613  [   96/  179]
train() client id: f_00007-11-3 loss: 0.784513  [  128/  179]
train() client id: f_00007-11-4 loss: 0.636205  [  160/  179]
train() client id: f_00007-12-0 loss: 0.654007  [   32/  179]
train() client id: f_00007-12-1 loss: 0.663334  [   64/  179]
train() client id: f_00007-12-2 loss: 0.684032  [   96/  179]
train() client id: f_00007-12-3 loss: 0.813129  [  128/  179]
train() client id: f_00007-12-4 loss: 0.558737  [  160/  179]
train() client id: f_00008-0-0 loss: 0.803642  [   32/  130]
train() client id: f_00008-0-1 loss: 0.907267  [   64/  130]
train() client id: f_00008-0-2 loss: 0.819467  [   96/  130]
train() client id: f_00008-0-3 loss: 0.839075  [  128/  130]
train() client id: f_00008-1-0 loss: 0.877848  [   32/  130]
train() client id: f_00008-1-1 loss: 0.838131  [   64/  130]
train() client id: f_00008-1-2 loss: 0.872492  [   96/  130]
train() client id: f_00008-1-3 loss: 0.822716  [  128/  130]
train() client id: f_00008-2-0 loss: 0.800269  [   32/  130]
train() client id: f_00008-2-1 loss: 0.868088  [   64/  130]
train() client id: f_00008-2-2 loss: 0.910730  [   96/  130]
train() client id: f_00008-2-3 loss: 0.817213  [  128/  130]
train() client id: f_00008-3-0 loss: 0.787807  [   32/  130]
train() client id: f_00008-3-1 loss: 0.942204  [   64/  130]
train() client id: f_00008-3-2 loss: 0.798671  [   96/  130]
train() client id: f_00008-3-3 loss: 0.845949  [  128/  130]
train() client id: f_00008-4-0 loss: 0.911661  [   32/  130]
train() client id: f_00008-4-1 loss: 0.652393  [   64/  130]
train() client id: f_00008-4-2 loss: 0.800202  [   96/  130]
train() client id: f_00008-4-3 loss: 1.004894  [  128/  130]
train() client id: f_00008-5-0 loss: 0.859346  [   32/  130]
train() client id: f_00008-5-1 loss: 0.732944  [   64/  130]
train() client id: f_00008-5-2 loss: 0.905286  [   96/  130]
train() client id: f_00008-5-3 loss: 0.892839  [  128/  130]
train() client id: f_00008-6-0 loss: 0.769100  [   32/  130]
train() client id: f_00008-6-1 loss: 1.013904  [   64/  130]
train() client id: f_00008-6-2 loss: 0.830052  [   96/  130]
train() client id: f_00008-6-3 loss: 0.775251  [  128/  130]
train() client id: f_00008-7-0 loss: 0.859559  [   32/  130]
train() client id: f_00008-7-1 loss: 0.835765  [   64/  130]
train() client id: f_00008-7-2 loss: 0.746740  [   96/  130]
train() client id: f_00008-7-3 loss: 0.925137  [  128/  130]
train() client id: f_00008-8-0 loss: 0.851539  [   32/  130]
train() client id: f_00008-8-1 loss: 0.885704  [   64/  130]
train() client id: f_00008-8-2 loss: 0.938105  [   96/  130]
train() client id: f_00008-8-3 loss: 0.711023  [  128/  130]
train() client id: f_00008-9-0 loss: 0.906588  [   32/  130]
train() client id: f_00008-9-1 loss: 0.801632  [   64/  130]
train() client id: f_00008-9-2 loss: 0.808503  [   96/  130]
train() client id: f_00008-9-3 loss: 0.850388  [  128/  130]
train() client id: f_00008-10-0 loss: 0.887560  [   32/  130]
train() client id: f_00008-10-1 loss: 0.939452  [   64/  130]
train() client id: f_00008-10-2 loss: 0.719055  [   96/  130]
train() client id: f_00008-10-3 loss: 0.760657  [  128/  130]
train() client id: f_00008-11-0 loss: 0.856009  [   32/  130]
train() client id: f_00008-11-1 loss: 0.798059  [   64/  130]
train() client id: f_00008-11-2 loss: 0.759251  [   96/  130]
train() client id: f_00008-11-3 loss: 0.921541  [  128/  130]
train() client id: f_00008-12-0 loss: 0.877219  [   32/  130]
train() client id: f_00008-12-1 loss: 0.852234  [   64/  130]
train() client id: f_00008-12-2 loss: 0.781356  [   96/  130]
train() client id: f_00008-12-3 loss: 0.854049  [  128/  130]
train() client id: f_00009-0-0 loss: 1.214282  [   32/  118]
train() client id: f_00009-0-1 loss: 1.251649  [   64/  118]
train() client id: f_00009-0-2 loss: 1.131702  [   96/  118]
train() client id: f_00009-1-0 loss: 1.248829  [   32/  118]
train() client id: f_00009-1-1 loss: 1.128750  [   64/  118]
train() client id: f_00009-1-2 loss: 1.060956  [   96/  118]
train() client id: f_00009-2-0 loss: 1.112231  [   32/  118]
train() client id: f_00009-2-1 loss: 1.040030  [   64/  118]
train() client id: f_00009-2-2 loss: 1.008745  [   96/  118]
train() client id: f_00009-3-0 loss: 1.111721  [   32/  118]
train() client id: f_00009-3-1 loss: 1.025356  [   64/  118]
train() client id: f_00009-3-2 loss: 1.099760  [   96/  118]
train() client id: f_00009-4-0 loss: 1.076294  [   32/  118]
train() client id: f_00009-4-1 loss: 0.997212  [   64/  118]
train() client id: f_00009-4-2 loss: 0.924551  [   96/  118]
train() client id: f_00009-5-0 loss: 1.008191  [   32/  118]
train() client id: f_00009-5-1 loss: 0.894887  [   64/  118]
train() client id: f_00009-5-2 loss: 1.038024  [   96/  118]
train() client id: f_00009-6-0 loss: 0.971445  [   32/  118]
train() client id: f_00009-6-1 loss: 0.933501  [   64/  118]
train() client id: f_00009-6-2 loss: 1.087912  [   96/  118]
train() client id: f_00009-7-0 loss: 0.942942  [   32/  118]
train() client id: f_00009-7-1 loss: 0.882198  [   64/  118]
train() client id: f_00009-7-2 loss: 1.008015  [   96/  118]
train() client id: f_00009-8-0 loss: 0.883287  [   32/  118]
train() client id: f_00009-8-1 loss: 0.997580  [   64/  118]
train() client id: f_00009-8-2 loss: 0.936121  [   96/  118]
train() client id: f_00009-9-0 loss: 0.908797  [   32/  118]
train() client id: f_00009-9-1 loss: 0.908140  [   64/  118]
train() client id: f_00009-9-2 loss: 0.871720  [   96/  118]
train() client id: f_00009-10-0 loss: 0.878724  [   32/  118]
train() client id: f_00009-10-1 loss: 0.866254  [   64/  118]
train() client id: f_00009-10-2 loss: 0.991002  [   96/  118]
train() client id: f_00009-11-0 loss: 0.875144  [   32/  118]
train() client id: f_00009-11-1 loss: 0.823156  [   64/  118]
train() client id: f_00009-11-2 loss: 0.976152  [   96/  118]
train() client id: f_00009-12-0 loss: 0.914216  [   32/  118]
train() client id: f_00009-12-1 loss: 0.948700  [   64/  118]
train() client id: f_00009-12-2 loss: 0.948249  [   96/  118]
At round 4 accuracy: 0.623342175066313
At round 4 training accuracy: 0.5707578806170356
At round 4 training loss: 0.8751278692779392
update_location
xs = 5.471708 -78.998411 50.045120 69.056472 -170.103519 -105.217951 12.784040 -6.624259 -1.680116 -60.304393 
ys = 17.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 11.875078 -19.177652 62.845030 -0.998518 
xs mean: -28.55713084058713
ys mean: 10.371751218646876
dists_uav = 101.682230 128.385096 111.831382 123.600426 197.541472 146.171716 101.510833 102.037558 118.119942 116.780207 
uav_gains = -100.181147 -102.713416 -101.214171 -102.300823 -107.527469 -104.124812 -100.162830 -100.219023 -101.808252 -101.684377 
uav_gains_db_mean: -102.19363198496464
dists_bs = 239.475652 186.114872 284.270707 287.760564 165.722165 204.462552 248.742007 257.012355 206.442533 210.072751 
bs_gains = -106.186377 -103.120929 -108.271558 -108.419935 -101.709716 -104.264245 -106.648035 -107.045771 -104.381436 -104.593411 
bs_gains_db_mean: -105.46414142415105
Round 5
-------------------------------
ene_coms = [0.00635016 0.00723833 0.00663667 0.00696691 0.00678336 0.00765322
 0.0063453  0.00636023 0.00769839 0.00778142]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.33014264 21.56253761 10.17652774  3.65369924 24.85652078 11.97773799
  4.52409448 14.60421286 10.68874154  9.72330846]
obj_prev = 122.09752332819757
eta_min = 7.002744627777035e-10	eta_max = 0.9295087656923436
af = 25.846727897333864	bf = 1.8479673767670175	zeta = 28.431400687067253	eta = 0.909090909090909
af = 25.846727897333864	bf = 1.8479673767670175	zeta = 47.42566026016899	eta = 0.5449945821638155
af = 25.846727897333864	bf = 1.8479673767670175	zeta = 38.54107827225303	eta = 0.6706280430130508
af = 25.846727897333864	bf = 1.8479673767670175	zeta = 36.96363644542004	eta = 0.6992474329601948
af = 25.846727897333864	bf = 1.8479673767670175	zeta = 36.88962779452702	eta = 0.7006502760423218
af = 25.846727897333864	bf = 1.8479673767670175	zeta = 36.889454496950705	eta = 0.7006535675248428
eta = 0.7006535675248428
ene_coms = [0.00635016 0.00723833 0.00663667 0.00696691 0.00678336 0.00765322
 0.0063453  0.00636023 0.00769839 0.00778142]
ene_comp = [0.02935495 0.06173859 0.028889   0.01001796 0.0712906  0.03401447
 0.01258069 0.04170267 0.03028686 0.02749115]
ene_total = [3.1572443  6.09932269 3.14137746 1.50189613 6.90373318 3.6844886
 1.67354143 4.24998881 3.35886611 3.11899579]
ti_comp = [0.27293938 0.26405765 0.27007424 0.26677192 0.26860737 0.25990876
 0.27298798 0.27283866 0.25945711 0.25862682]
ti_coms = [0.06350159 0.07238333 0.06636674 0.06966905 0.06783361 0.07653221
 0.06345299 0.06360232 0.07698387 0.07781415]
t_total = [29.74997902 29.74997902 29.74997902 29.74997902 29.74997902 29.74997902
 29.74997902 29.74997902 29.74997902 29.74997902]
ene_coms = [0.00635016 0.00723833 0.00663667 0.00696691 0.00678336 0.00765322
 0.0063453  0.00636023 0.00769839 0.00778142]
ene_comp = [2.12222679e-05 2.10936961e-04 2.06590953e-05 8.82953232e-07
 3.13863636e-04 3.64107222e-05 1.66996277e-06 6.08919464e-05
 2.57935711e-05 1.94138409e-05]
ene_total = [0.56339292 0.65870578 0.58867831 0.61613053 0.62757598 0.67995996
 0.56123425 0.56779142 0.68301489 0.68979263]
optimize_network iter = 0 obj = 6.236276679122891
eta = 0.7006535675248428
freqs = [5.37755912e+07 1.16903624e+08 5.34834455e+07 1.87762644e+07
 1.32704105e+08 6.54354042e+07 2.30425790e+07 7.64236865e+07
 5.83658254e+07 5.31482905e+07]
eta_min = 0.6651918544164559	eta_max = 0.7006535675248382
af = 0.05295920731205967	bf = 1.8479673767670175	zeta = 0.058255128043265644	eta = 0.909090909090909
af = 0.05295920731205967	bf = 1.8479673767670175	zeta = 20.383164211720437	eta = 0.002598183812972856
af = 0.05295920731205967	bf = 1.8479673767670175	zeta = 2.1688656101678783	eta = 0.024417929383812965
af = 0.05295920731205967	bf = 1.8479673767670175	zeta = 2.0957489801648834	eta = 0.025269823730460838
af = 0.05295920731205967	bf = 1.8479673767670175	zeta = 2.0957173941642453	eta = 0.025270204589383276
eta = 0.025270204589383276
ene_coms = [0.00635016 0.00723833 0.00663667 0.00696691 0.00678336 0.00765322
 0.0063453  0.00636023 0.00769839 0.00778142]
ene_comp = [2.19423930e-04 2.18094585e-03 2.13601105e-04 9.12914065e-06
 3.24513822e-03 3.76462299e-04 1.72662882e-05 6.29581639e-04
 2.66688121e-04 2.00726015e-04]
ene_total = [0.17840435 0.255791   0.18602684 0.18944199 0.27233506 0.218055
 0.17278256 0.18981617 0.21630048 0.21676394]
ti_comp = [0.31279546 0.30391372 0.30993031 0.30662799 0.30846344 0.29976484
 0.31284406 0.31269473 0.29931318 0.29848289]
ti_coms = [0.06350159 0.07238333 0.06636674 0.06966905 0.06783361 0.07653221
 0.06345299 0.06360232 0.07698387 0.07781415]
t_total = [29.74997902 29.74997902 29.74997902 29.74997902 29.74997902 29.74997902
 29.74997902 29.74997902 29.74997902 29.74997902]
ene_coms = [0.00635016 0.00723833 0.00663667 0.00696691 0.00678336 0.00765322
 0.0063453  0.00636023 0.00769839 0.00778142]
ene_comp = [2.43196022e-05 2.39663946e-04 2.36103621e-05 1.00588299e-06
 3.58197526e-04 4.11967446e-05 1.91377627e-06 6.97724018e-05
 2.91705132e-05 2.19367775e-05]
ene_total = [0.50396518 0.5912091  0.52656088 0.55088185 0.56461034 0.60831935
 0.50180955 0.50835502 0.61093932 0.61693166]
optimize_network iter = 1 obj = 5.583582254326441
eta = 0.6651918544164559
freqs = [5.37743714e+07 1.16402095e+08 5.34100351e+07 1.87206915e+07
 1.32428957e+08 6.50185685e+07 2.30425790e+07 7.64183582e+07
 5.79806047e+07 5.27749454e+07]
eta_min = 0.6651918544164617	eta_max = 0.6651918544164503
af = 0.05264289455886872	bf = 1.8479673767670175	zeta = 0.05790718401475559	eta = 0.9090909090909091
af = 0.05264289455886872	bf = 1.8479673767670175	zeta = 20.38283258564127	eta = 0.0025827074984638354
af = 0.05264289455886872	bf = 1.8479673767670175	zeta = 2.1672594138695516	eta = 0.02429007539290233
af = 0.05264289455886872	bf = 1.8479673767670175	zeta = 2.0945540807952856	eta = 0.025133222885742167
af = 0.05264289455886872	bf = 1.8479673767670175	zeta = 2.094522997527627	eta = 0.025133595869326016
eta = 0.025133595869326016
ene_coms = [0.00635016 0.00723833 0.00663667 0.00696691 0.00678336 0.00765322
 0.0063453  0.00636023 0.00769839 0.00778142]
ene_comp = [2.19737333e-04 2.16545961e-03 2.13329066e-04 9.08855515e-06
 3.23645791e-03 3.72229065e-04 1.72917341e-05 6.30421558e-04
 2.63567255e-04 1.98207559e-04]
ene_total = [0.17838786 0.25533467 0.18599339 0.18941434 0.27206121 0.21790951
 0.17275903 0.18981238 0.21618543 0.21666519]
ti_comp = [0.31279546 0.30391372 0.30993031 0.30662799 0.30846344 0.29976484
 0.31284406 0.31269473 0.29931318 0.29848289]
ti_coms = [0.06350159 0.07238333 0.06636674 0.06966905 0.06783361 0.07653221
 0.06345299 0.06360232 0.07698387 0.07781415]
t_total = [29.74997902 29.74997902 29.74997902 29.74997902 29.74997902 29.74997902
 29.74997902 29.74997902 29.74997902 29.74997902]
ene_coms = [0.00635016 0.00723833 0.00663667 0.00696691 0.00678336 0.00765322
 0.0063453  0.00636023 0.00769839 0.00778142]
ene_comp = [2.43196022e-05 2.39663946e-04 2.36103621e-05 1.00588299e-06
 3.58197526e-04 4.11967446e-05 1.91377627e-06 6.97724018e-05
 2.91705132e-05 2.19367775e-05]
ene_total = [0.50396518 0.5912091  0.52656088 0.55088185 0.56461034 0.60831935
 0.50180955 0.50835502 0.61093932 0.61693166]
optimize_network iter = 2 obj = 5.583582254326536
eta = 0.6651918544164617
freqs = [5.37743714e+07 1.16402095e+08 5.34100351e+07 1.87206915e+07
 1.32428957e+08 6.50185685e+07 2.30425790e+07 7.64183582e+07
 5.79806047e+07 5.27749454e+07]
Done!
ene_coms = [0.00635016 0.00723833 0.00663667 0.00696691 0.00678336 0.00765322
 0.0063453  0.00636023 0.00769839 0.00778142]
ene_comp = [2.36828839e-05 2.33389237e-04 2.29922127e-05 9.79547688e-07
 3.48819457e-04 4.01181612e-05 1.86367117e-06 6.79456712e-05
 2.84067919e-05 2.13624446e-05]
ene_total = [0.00637384 0.00747172 0.00665967 0.00696788 0.00713218 0.00769334
 0.00634716 0.00642818 0.00772679 0.00780278]
At round 5 energy consumption: 0.07060354571345553
At round 5 eta: 0.6651918544164617
At round 5 a_n: 26.46987362140779
At round 5 local rounds: 13.34950716238561
At round 5 global rounds: 79.05982566604928
gradient difference: 0.3609941899776459
train() client id: f_00000-0-0 loss: 1.352738  [   32/  126]
train() client id: f_00000-0-1 loss: 1.138422  [   64/  126]
train() client id: f_00000-0-2 loss: 1.182650  [   96/  126]
train() client id: f_00000-1-0 loss: 1.412124  [   32/  126]
train() client id: f_00000-1-1 loss: 1.110857  [   64/  126]
train() client id: f_00000-1-2 loss: 1.072333  [   96/  126]
train() client id: f_00000-2-0 loss: 1.175376  [   32/  126]
train() client id: f_00000-2-1 loss: 1.077892  [   64/  126]
train() client id: f_00000-2-2 loss: 1.141905  [   96/  126]
train() client id: f_00000-3-0 loss: 1.119473  [   32/  126]
train() client id: f_00000-3-1 loss: 1.193168  [   64/  126]
train() client id: f_00000-3-2 loss: 1.001873  [   96/  126]
train() client id: f_00000-4-0 loss: 1.028066  [   32/  126]
train() client id: f_00000-4-1 loss: 1.059770  [   64/  126]
train() client id: f_00000-4-2 loss: 1.096721  [   96/  126]
train() client id: f_00000-5-0 loss: 1.062219  [   32/  126]
train() client id: f_00000-5-1 loss: 1.070605  [   64/  126]
train() client id: f_00000-5-2 loss: 0.997258  [   96/  126]
train() client id: f_00000-6-0 loss: 1.006062  [   32/  126]
train() client id: f_00000-6-1 loss: 1.102971  [   64/  126]
train() client id: f_00000-6-2 loss: 1.049701  [   96/  126]
train() client id: f_00000-7-0 loss: 0.960453  [   32/  126]
train() client id: f_00000-7-1 loss: 1.098649  [   64/  126]
train() client id: f_00000-7-2 loss: 0.992543  [   96/  126]
train() client id: f_00000-8-0 loss: 1.071356  [   32/  126]
train() client id: f_00000-8-1 loss: 0.879929  [   64/  126]
train() client id: f_00000-8-2 loss: 1.096014  [   96/  126]
train() client id: f_00000-9-0 loss: 1.040627  [   32/  126]
train() client id: f_00000-9-1 loss: 1.056507  [   64/  126]
train() client id: f_00000-9-2 loss: 0.996718  [   96/  126]
train() client id: f_00000-10-0 loss: 0.981475  [   32/  126]
train() client id: f_00000-10-1 loss: 0.980945  [   64/  126]
train() client id: f_00000-10-2 loss: 1.102428  [   96/  126]
train() client id: f_00000-11-0 loss: 1.115507  [   32/  126]
train() client id: f_00000-11-1 loss: 1.030609  [   64/  126]
train() client id: f_00000-11-2 loss: 0.940780  [   96/  126]
train() client id: f_00000-12-0 loss: 0.988447  [   32/  126]
train() client id: f_00000-12-1 loss: 0.984609  [   64/  126]
train() client id: f_00000-12-2 loss: 1.115221  [   96/  126]
train() client id: f_00001-0-0 loss: 0.610250  [   32/  265]
train() client id: f_00001-0-1 loss: 0.618298  [   64/  265]
train() client id: f_00001-0-2 loss: 0.583354  [   96/  265]
train() client id: f_00001-0-3 loss: 0.575003  [  128/  265]
train() client id: f_00001-0-4 loss: 0.556144  [  160/  265]
train() client id: f_00001-0-5 loss: 0.470433  [  192/  265]
train() client id: f_00001-0-6 loss: 0.565241  [  224/  265]
train() client id: f_00001-0-7 loss: 0.492275  [  256/  265]
train() client id: f_00001-1-0 loss: 0.509858  [   32/  265]
train() client id: f_00001-1-1 loss: 0.514200  [   64/  265]
train() client id: f_00001-1-2 loss: 0.543971  [   96/  265]
train() client id: f_00001-1-3 loss: 0.525639  [  128/  265]
train() client id: f_00001-1-4 loss: 0.574435  [  160/  265]
train() client id: f_00001-1-5 loss: 0.628791  [  192/  265]
train() client id: f_00001-1-6 loss: 0.448459  [  224/  265]
train() client id: f_00001-1-7 loss: 0.548135  [  256/  265]
train() client id: f_00001-2-0 loss: 0.472256  [   32/  265]
train() client id: f_00001-2-1 loss: 0.491330  [   64/  265]
train() client id: f_00001-2-2 loss: 0.514377  [   96/  265]
train() client id: f_00001-2-3 loss: 0.458988  [  128/  265]
train() client id: f_00001-2-4 loss: 0.505006  [  160/  265]
train() client id: f_00001-2-5 loss: 0.538661  [  192/  265]
train() client id: f_00001-2-6 loss: 0.581447  [  224/  265]
train() client id: f_00001-2-7 loss: 0.511279  [  256/  265]
train() client id: f_00001-3-0 loss: 0.512789  [   32/  265]
train() client id: f_00001-3-1 loss: 0.469561  [   64/  265]
train() client id: f_00001-3-2 loss: 0.504275  [   96/  265]
train() client id: f_00001-3-3 loss: 0.613647  [  128/  265]
train() client id: f_00001-3-4 loss: 0.429873  [  160/  265]
train() client id: f_00001-3-5 loss: 0.503306  [  192/  265]
train() client id: f_00001-3-6 loss: 0.466757  [  224/  265]
train() client id: f_00001-3-7 loss: 0.526040  [  256/  265]
train() client id: f_00001-4-0 loss: 0.470037  [   32/  265]
train() client id: f_00001-4-1 loss: 0.480651  [   64/  265]
train() client id: f_00001-4-2 loss: 0.429979  [   96/  265]
train() client id: f_00001-4-3 loss: 0.637596  [  128/  265]
train() client id: f_00001-4-4 loss: 0.435204  [  160/  265]
train() client id: f_00001-4-5 loss: 0.437669  [  192/  265]
train() client id: f_00001-4-6 loss: 0.504423  [  224/  265]
train() client id: f_00001-4-7 loss: 0.539326  [  256/  265]
train() client id: f_00001-5-0 loss: 0.518234  [   32/  265]
train() client id: f_00001-5-1 loss: 0.410262  [   64/  265]
train() client id: f_00001-5-2 loss: 0.461560  [   96/  265]
train() client id: f_00001-5-3 loss: 0.503557  [  128/  265]
train() client id: f_00001-5-4 loss: 0.446884  [  160/  265]
train() client id: f_00001-5-5 loss: 0.500587  [  192/  265]
train() client id: f_00001-5-6 loss: 0.500331  [  224/  265]
train() client id: f_00001-5-7 loss: 0.459670  [  256/  265]
train() client id: f_00001-6-0 loss: 0.415790  [   32/  265]
train() client id: f_00001-6-1 loss: 0.479868  [   64/  265]
train() client id: f_00001-6-2 loss: 0.461731  [   96/  265]
train() client id: f_00001-6-3 loss: 0.515457  [  128/  265]
train() client id: f_00001-6-4 loss: 0.394240  [  160/  265]
train() client id: f_00001-6-5 loss: 0.595368  [  192/  265]
train() client id: f_00001-6-6 loss: 0.477608  [  224/  265]
train() client id: f_00001-6-7 loss: 0.424441  [  256/  265]
train() client id: f_00001-7-0 loss: 0.467968  [   32/  265]
train() client id: f_00001-7-1 loss: 0.595981  [   64/  265]
train() client id: f_00001-7-2 loss: 0.414141  [   96/  265]
train() client id: f_00001-7-3 loss: 0.399898  [  128/  265]
train() client id: f_00001-7-4 loss: 0.405660  [  160/  265]
train() client id: f_00001-7-5 loss: 0.430436  [  192/  265]
train() client id: f_00001-7-6 loss: 0.549833  [  224/  265]
train() client id: f_00001-7-7 loss: 0.516222  [  256/  265]
train() client id: f_00001-8-0 loss: 0.457829  [   32/  265]
train() client id: f_00001-8-1 loss: 0.468993  [   64/  265]
train() client id: f_00001-8-2 loss: 0.428337  [   96/  265]
train() client id: f_00001-8-3 loss: 0.385490  [  128/  265]
train() client id: f_00001-8-4 loss: 0.479669  [  160/  265]
train() client id: f_00001-8-5 loss: 0.498915  [  192/  265]
train() client id: f_00001-8-6 loss: 0.585848  [  224/  265]
train() client id: f_00001-8-7 loss: 0.482542  [  256/  265]
train() client id: f_00001-9-0 loss: 0.456006  [   32/  265]
train() client id: f_00001-9-1 loss: 0.538102  [   64/  265]
train() client id: f_00001-9-2 loss: 0.439879  [   96/  265]
train() client id: f_00001-9-3 loss: 0.493521  [  128/  265]
train() client id: f_00001-9-4 loss: 0.400173  [  160/  265]
train() client id: f_00001-9-5 loss: 0.526002  [  192/  265]
train() client id: f_00001-9-6 loss: 0.417176  [  224/  265]
train() client id: f_00001-9-7 loss: 0.424514  [  256/  265]
train() client id: f_00001-10-0 loss: 0.396043  [   32/  265]
train() client id: f_00001-10-1 loss: 0.512644  [   64/  265]
train() client id: f_00001-10-2 loss: 0.428233  [   96/  265]
train() client id: f_00001-10-3 loss: 0.459803  [  128/  265]
train() client id: f_00001-10-4 loss: 0.467972  [  160/  265]
train() client id: f_00001-10-5 loss: 0.429098  [  192/  265]
train() client id: f_00001-10-6 loss: 0.503572  [  224/  265]
train() client id: f_00001-10-7 loss: 0.549471  [  256/  265]
train() client id: f_00001-11-0 loss: 0.477185  [   32/  265]
train() client id: f_00001-11-1 loss: 0.514939  [   64/  265]
train() client id: f_00001-11-2 loss: 0.400211  [   96/  265]
train() client id: f_00001-11-3 loss: 0.407699  [  128/  265]
train() client id: f_00001-11-4 loss: 0.452448  [  160/  265]
train() client id: f_00001-11-5 loss: 0.474641  [  192/  265]
train() client id: f_00001-11-6 loss: 0.479551  [  224/  265]
train() client id: f_00001-11-7 loss: 0.450947  [  256/  265]
train() client id: f_00001-12-0 loss: 0.374577  [   32/  265]
train() client id: f_00001-12-1 loss: 0.469977  [   64/  265]
train() client id: f_00001-12-2 loss: 0.394500  [   96/  265]
train() client id: f_00001-12-3 loss: 0.475146  [  128/  265]
train() client id: f_00001-12-4 loss: 0.606155  [  160/  265]
train() client id: f_00001-12-5 loss: 0.494725  [  192/  265]
train() client id: f_00001-12-6 loss: 0.510519  [  224/  265]
train() client id: f_00001-12-7 loss: 0.409252  [  256/  265]
train() client id: f_00002-0-0 loss: 1.276340  [   32/  124]
train() client id: f_00002-0-1 loss: 1.118297  [   64/  124]
train() client id: f_00002-0-2 loss: 1.156697  [   96/  124]
train() client id: f_00002-1-0 loss: 1.155709  [   32/  124]
train() client id: f_00002-1-1 loss: 1.268580  [   64/  124]
train() client id: f_00002-1-2 loss: 1.077993  [   96/  124]
train() client id: f_00002-2-0 loss: 1.093725  [   32/  124]
train() client id: f_00002-2-1 loss: 1.094038  [   64/  124]
train() client id: f_00002-2-2 loss: 1.040701  [   96/  124]
train() client id: f_00002-3-0 loss: 1.108109  [   32/  124]
train() client id: f_00002-3-1 loss: 1.147418  [   64/  124]
train() client id: f_00002-3-2 loss: 1.092286  [   96/  124]
train() client id: f_00002-4-0 loss: 1.204279  [   32/  124]
train() client id: f_00002-4-1 loss: 1.066136  [   64/  124]
train() client id: f_00002-4-2 loss: 1.000595  [   96/  124]
train() client id: f_00002-5-0 loss: 0.997508  [   32/  124]
train() client id: f_00002-5-1 loss: 0.947147  [   64/  124]
train() client id: f_00002-5-2 loss: 1.107213  [   96/  124]
train() client id: f_00002-6-0 loss: 0.925043  [   32/  124]
train() client id: f_00002-6-1 loss: 1.080635  [   64/  124]
train() client id: f_00002-6-2 loss: 1.185343  [   96/  124]
train() client id: f_00002-7-0 loss: 0.964858  [   32/  124]
train() client id: f_00002-7-1 loss: 1.144632  [   64/  124]
train() client id: f_00002-7-2 loss: 1.006009  [   96/  124]
train() client id: f_00002-8-0 loss: 0.951273  [   32/  124]
train() client id: f_00002-8-1 loss: 0.949265  [   64/  124]
train() client id: f_00002-8-2 loss: 1.125524  [   96/  124]
train() client id: f_00002-9-0 loss: 1.101693  [   32/  124]
train() client id: f_00002-9-1 loss: 0.837542  [   64/  124]
train() client id: f_00002-9-2 loss: 1.002547  [   96/  124]
train() client id: f_00002-10-0 loss: 0.871197  [   32/  124]
train() client id: f_00002-10-1 loss: 1.150142  [   64/  124]
train() client id: f_00002-10-2 loss: 0.880584  [   96/  124]
train() client id: f_00002-11-0 loss: 0.998006  [   32/  124]
train() client id: f_00002-11-1 loss: 1.163655  [   64/  124]
train() client id: f_00002-11-2 loss: 0.947819  [   96/  124]
train() client id: f_00002-12-0 loss: 1.165924  [   32/  124]
train() client id: f_00002-12-1 loss: 1.046115  [   64/  124]
train() client id: f_00002-12-2 loss: 0.946320  [   96/  124]
train() client id: f_00003-0-0 loss: 1.169166  [   32/   43]
train() client id: f_00003-1-0 loss: 1.110339  [   32/   43]
train() client id: f_00003-2-0 loss: 0.985852  [   32/   43]
train() client id: f_00003-3-0 loss: 1.095465  [   32/   43]
train() client id: f_00003-4-0 loss: 1.060823  [   32/   43]
train() client id: f_00003-5-0 loss: 1.142951  [   32/   43]
train() client id: f_00003-6-0 loss: 1.077453  [   32/   43]
train() client id: f_00003-7-0 loss: 1.028279  [   32/   43]
train() client id: f_00003-8-0 loss: 1.102269  [   32/   43]
train() client id: f_00003-9-0 loss: 1.103836  [   32/   43]
train() client id: f_00003-10-0 loss: 1.024534  [   32/   43]
train() client id: f_00003-11-0 loss: 1.061152  [   32/   43]
train() client id: f_00003-12-0 loss: 1.107714  [   32/   43]
train() client id: f_00004-0-0 loss: 0.668653  [   32/  306]
train() client id: f_00004-0-1 loss: 0.907594  [   64/  306]
train() client id: f_00004-0-2 loss: 0.788667  [   96/  306]
train() client id: f_00004-0-3 loss: 0.751663  [  128/  306]
train() client id: f_00004-0-4 loss: 0.859965  [  160/  306]
train() client id: f_00004-0-5 loss: 0.827106  [  192/  306]
train() client id: f_00004-0-6 loss: 0.809065  [  224/  306]
train() client id: f_00004-0-7 loss: 0.854825  [  256/  306]
train() client id: f_00004-0-8 loss: 0.902368  [  288/  306]
train() client id: f_00004-1-0 loss: 0.998784  [   32/  306]
train() client id: f_00004-1-1 loss: 0.648591  [   64/  306]
train() client id: f_00004-1-2 loss: 0.941123  [   96/  306]
train() client id: f_00004-1-3 loss: 0.737442  [  128/  306]
train() client id: f_00004-1-4 loss: 0.833967  [  160/  306]
train() client id: f_00004-1-5 loss: 0.826132  [  192/  306]
train() client id: f_00004-1-6 loss: 0.791902  [  224/  306]
train() client id: f_00004-1-7 loss: 0.953209  [  256/  306]
train() client id: f_00004-1-8 loss: 0.713057  [  288/  306]
train() client id: f_00004-2-0 loss: 0.854839  [   32/  306]
train() client id: f_00004-2-1 loss: 0.634615  [   64/  306]
train() client id: f_00004-2-2 loss: 0.987749  [   96/  306]
train() client id: f_00004-2-3 loss: 0.860237  [  128/  306]
train() client id: f_00004-2-4 loss: 0.847905  [  160/  306]
train() client id: f_00004-2-5 loss: 0.817787  [  192/  306]
train() client id: f_00004-2-6 loss: 0.846178  [  224/  306]
train() client id: f_00004-2-7 loss: 0.835960  [  256/  306]
train() client id: f_00004-2-8 loss: 0.784491  [  288/  306]
train() client id: f_00004-3-0 loss: 0.822756  [   32/  306]
train() client id: f_00004-3-1 loss: 0.918317  [   64/  306]
train() client id: f_00004-3-2 loss: 0.804446  [   96/  306]
train() client id: f_00004-3-3 loss: 0.787857  [  128/  306]
train() client id: f_00004-3-4 loss: 0.788209  [  160/  306]
train() client id: f_00004-3-5 loss: 0.924735  [  192/  306]
train() client id: f_00004-3-6 loss: 0.737775  [  224/  306]
train() client id: f_00004-3-7 loss: 0.887432  [  256/  306]
train() client id: f_00004-3-8 loss: 0.780976  [  288/  306]
train() client id: f_00004-4-0 loss: 0.909587  [   32/  306]
train() client id: f_00004-4-1 loss: 0.688293  [   64/  306]
train() client id: f_00004-4-2 loss: 0.765305  [   96/  306]
train() client id: f_00004-4-3 loss: 0.781691  [  128/  306]
train() client id: f_00004-4-4 loss: 0.845793  [  160/  306]
train() client id: f_00004-4-5 loss: 1.005904  [  192/  306]
train() client id: f_00004-4-6 loss: 0.857834  [  224/  306]
train() client id: f_00004-4-7 loss: 0.837793  [  256/  306]
train() client id: f_00004-4-8 loss: 0.807909  [  288/  306]
train() client id: f_00004-5-0 loss: 0.870092  [   32/  306]
train() client id: f_00004-5-1 loss: 0.930454  [   64/  306]
train() client id: f_00004-5-2 loss: 0.803372  [   96/  306]
train() client id: f_00004-5-3 loss: 0.867965  [  128/  306]
train() client id: f_00004-5-4 loss: 0.738488  [  160/  306]
train() client id: f_00004-5-5 loss: 0.749906  [  192/  306]
train() client id: f_00004-5-6 loss: 0.834658  [  224/  306]
train() client id: f_00004-5-7 loss: 0.813692  [  256/  306]
train() client id: f_00004-5-8 loss: 0.746367  [  288/  306]
train() client id: f_00004-6-0 loss: 0.849931  [   32/  306]
train() client id: f_00004-6-1 loss: 0.919678  [   64/  306]
train() client id: f_00004-6-2 loss: 0.836242  [   96/  306]
train() client id: f_00004-6-3 loss: 0.705310  [  128/  306]
train() client id: f_00004-6-4 loss: 0.840787  [  160/  306]
train() client id: f_00004-6-5 loss: 0.836440  [  192/  306]
train() client id: f_00004-6-6 loss: 0.866724  [  224/  306]
train() client id: f_00004-6-7 loss: 0.771633  [  256/  306]
train() client id: f_00004-6-8 loss: 0.686035  [  288/  306]
train() client id: f_00004-7-0 loss: 0.719739  [   32/  306]
train() client id: f_00004-7-1 loss: 0.695640  [   64/  306]
train() client id: f_00004-7-2 loss: 0.910336  [   96/  306]
train() client id: f_00004-7-3 loss: 0.798735  [  128/  306]
train() client id: f_00004-7-4 loss: 0.815775  [  160/  306]
train() client id: f_00004-7-5 loss: 0.896528  [  192/  306]
train() client id: f_00004-7-6 loss: 0.801882  [  224/  306]
train() client id: f_00004-7-7 loss: 0.867739  [  256/  306]
train() client id: f_00004-7-8 loss: 0.888389  [  288/  306]
train() client id: f_00004-8-0 loss: 0.811202  [   32/  306]
train() client id: f_00004-8-1 loss: 0.912521  [   64/  306]
train() client id: f_00004-8-2 loss: 0.797188  [   96/  306]
train() client id: f_00004-8-3 loss: 0.727075  [  128/  306]
train() client id: f_00004-8-4 loss: 0.851606  [  160/  306]
train() client id: f_00004-8-5 loss: 0.761712  [  192/  306]
train() client id: f_00004-8-6 loss: 0.879610  [  224/  306]
train() client id: f_00004-8-7 loss: 0.822754  [  256/  306]
train() client id: f_00004-8-8 loss: 0.795964  [  288/  306]
train() client id: f_00004-9-0 loss: 0.835087  [   32/  306]
train() client id: f_00004-9-1 loss: 0.892911  [   64/  306]
train() client id: f_00004-9-2 loss: 0.721225  [   96/  306]
train() client id: f_00004-9-3 loss: 0.804011  [  128/  306]
train() client id: f_00004-9-4 loss: 0.878438  [  160/  306]
train() client id: f_00004-9-5 loss: 0.803560  [  192/  306]
train() client id: f_00004-9-6 loss: 0.866050  [  224/  306]
train() client id: f_00004-9-7 loss: 0.739608  [  256/  306]
train() client id: f_00004-9-8 loss: 0.946530  [  288/  306]
train() client id: f_00004-10-0 loss: 0.730720  [   32/  306]
train() client id: f_00004-10-1 loss: 0.867792  [   64/  306]
train() client id: f_00004-10-2 loss: 0.831534  [   96/  306]
train() client id: f_00004-10-3 loss: 0.738352  [  128/  306]
train() client id: f_00004-10-4 loss: 0.777955  [  160/  306]
train() client id: f_00004-10-5 loss: 0.926170  [  192/  306]
train() client id: f_00004-10-6 loss: 0.830409  [  224/  306]
train() client id: f_00004-10-7 loss: 0.804031  [  256/  306]
train() client id: f_00004-10-8 loss: 0.882555  [  288/  306]
train() client id: f_00004-11-0 loss: 0.791363  [   32/  306]
train() client id: f_00004-11-1 loss: 0.896352  [   64/  306]
train() client id: f_00004-11-2 loss: 0.807883  [   96/  306]
train() client id: f_00004-11-3 loss: 0.794288  [  128/  306]
train() client id: f_00004-11-4 loss: 0.859718  [  160/  306]
train() client id: f_00004-11-5 loss: 0.767998  [  192/  306]
train() client id: f_00004-11-6 loss: 0.948299  [  224/  306]
train() client id: f_00004-11-7 loss: 0.797226  [  256/  306]
train() client id: f_00004-11-8 loss: 0.803280  [  288/  306]
train() client id: f_00004-12-0 loss: 0.863287  [   32/  306]
train() client id: f_00004-12-1 loss: 0.763270  [   64/  306]
train() client id: f_00004-12-2 loss: 0.881507  [   96/  306]
train() client id: f_00004-12-3 loss: 0.756325  [  128/  306]
train() client id: f_00004-12-4 loss: 0.853298  [  160/  306]
train() client id: f_00004-12-5 loss: 0.808426  [  192/  306]
train() client id: f_00004-12-6 loss: 0.930398  [  224/  306]
train() client id: f_00004-12-7 loss: 0.779400  [  256/  306]
train() client id: f_00004-12-8 loss: 0.841480  [  288/  306]
train() client id: f_00005-0-0 loss: 0.467746  [   32/  146]
train() client id: f_00005-0-1 loss: 0.484857  [   64/  146]
train() client id: f_00005-0-2 loss: 0.566676  [   96/  146]
train() client id: f_00005-0-3 loss: 0.557371  [  128/  146]
train() client id: f_00005-1-0 loss: 0.681254  [   32/  146]
train() client id: f_00005-1-1 loss: 0.589877  [   64/  146]
train() client id: f_00005-1-2 loss: 0.324720  [   96/  146]
train() client id: f_00005-1-3 loss: 0.322790  [  128/  146]
train() client id: f_00005-2-0 loss: 0.605081  [   32/  146]
train() client id: f_00005-2-1 loss: 0.259599  [   64/  146]
train() client id: f_00005-2-2 loss: 0.446728  [   96/  146]
train() client id: f_00005-2-3 loss: 0.479069  [  128/  146]
train() client id: f_00005-3-0 loss: 0.430959  [   32/  146]
train() client id: f_00005-3-1 loss: 0.258524  [   64/  146]
train() client id: f_00005-3-2 loss: 0.612693  [   96/  146]
train() client id: f_00005-3-3 loss: 0.481308  [  128/  146]
train() client id: f_00005-4-0 loss: 0.330257  [   32/  146]
train() client id: f_00005-4-1 loss: 0.525381  [   64/  146]
train() client id: f_00005-4-2 loss: 0.552720  [   96/  146]
train() client id: f_00005-4-3 loss: 0.356321  [  128/  146]
train() client id: f_00005-5-0 loss: 0.276237  [   32/  146]
train() client id: f_00005-5-1 loss: 0.363642  [   64/  146]
train() client id: f_00005-5-2 loss: 0.309364  [   96/  146]
train() client id: f_00005-5-3 loss: 0.617973  [  128/  146]
train() client id: f_00005-6-0 loss: 0.433029  [   32/  146]
train() client id: f_00005-6-1 loss: 0.183284  [   64/  146]
train() client id: f_00005-6-2 loss: 0.578628  [   96/  146]
train() client id: f_00005-6-3 loss: 0.703430  [  128/  146]
train() client id: f_00005-7-0 loss: 0.300259  [   32/  146]
train() client id: f_00005-7-1 loss: 0.263404  [   64/  146]
train() client id: f_00005-7-2 loss: 0.737759  [   96/  146]
train() client id: f_00005-7-3 loss: 0.477968  [  128/  146]
train() client id: f_00005-8-0 loss: 0.462527  [   32/  146]
train() client id: f_00005-8-1 loss: 0.512355  [   64/  146]
train() client id: f_00005-8-2 loss: 0.210986  [   96/  146]
train() client id: f_00005-8-3 loss: 0.412071  [  128/  146]
train() client id: f_00005-9-0 loss: 0.555507  [   32/  146]
train() client id: f_00005-9-1 loss: 0.488086  [   64/  146]
train() client id: f_00005-9-2 loss: 0.420056  [   96/  146]
train() client id: f_00005-9-3 loss: 0.208191  [  128/  146]
train() client id: f_00005-10-0 loss: 0.451969  [   32/  146]
train() client id: f_00005-10-1 loss: 0.571978  [   64/  146]
train() client id: f_00005-10-2 loss: 0.405535  [   96/  146]
train() client id: f_00005-10-3 loss: 0.185295  [  128/  146]
train() client id: f_00005-11-0 loss: 0.367973  [   32/  146]
train() client id: f_00005-11-1 loss: 0.552636  [   64/  146]
train() client id: f_00005-11-2 loss: 0.354626  [   96/  146]
train() client id: f_00005-11-3 loss: 0.503102  [  128/  146]
train() client id: f_00005-12-0 loss: 0.331644  [   32/  146]
train() client id: f_00005-12-1 loss: 0.440589  [   64/  146]
train() client id: f_00005-12-2 loss: 0.315544  [   96/  146]
train() client id: f_00005-12-3 loss: 0.580248  [  128/  146]
train() client id: f_00006-0-0 loss: 0.821505  [   32/   54]
train() client id: f_00006-1-0 loss: 0.737152  [   32/   54]
train() client id: f_00006-2-0 loss: 0.773250  [   32/   54]
train() client id: f_00006-3-0 loss: 0.784342  [   32/   54]
train() client id: f_00006-4-0 loss: 0.774986  [   32/   54]
train() client id: f_00006-5-0 loss: 0.818560  [   32/   54]
train() client id: f_00006-6-0 loss: 0.824770  [   32/   54]
train() client id: f_00006-7-0 loss: 0.800459  [   32/   54]
train() client id: f_00006-8-0 loss: 0.736006  [   32/   54]
train() client id: f_00006-9-0 loss: 0.841541  [   32/   54]
train() client id: f_00006-10-0 loss: 0.773153  [   32/   54]
train() client id: f_00006-11-0 loss: 0.791681  [   32/   54]
train() client id: f_00006-12-0 loss: 0.819842  [   32/   54]
train() client id: f_00007-0-0 loss: 0.898551  [   32/  179]
train() client id: f_00007-0-1 loss: 0.742266  [   64/  179]
train() client id: f_00007-0-2 loss: 0.617295  [   96/  179]
train() client id: f_00007-0-3 loss: 0.740236  [  128/  179]
train() client id: f_00007-0-4 loss: 0.628217  [  160/  179]
train() client id: f_00007-1-0 loss: 0.828830  [   32/  179]
train() client id: f_00007-1-1 loss: 0.607803  [   64/  179]
train() client id: f_00007-1-2 loss: 0.853750  [   96/  179]
train() client id: f_00007-1-3 loss: 0.707173  [  128/  179]
train() client id: f_00007-1-4 loss: 0.632026  [  160/  179]
train() client id: f_00007-2-0 loss: 0.779113  [   32/  179]
train() client id: f_00007-2-1 loss: 0.711293  [   64/  179]
train() client id: f_00007-2-2 loss: 0.592131  [   96/  179]
train() client id: f_00007-2-3 loss: 0.708030  [  128/  179]
train() client id: f_00007-2-4 loss: 0.729752  [  160/  179]
train() client id: f_00007-3-0 loss: 0.858550  [   32/  179]
train() client id: f_00007-3-1 loss: 0.816634  [   64/  179]
train() client id: f_00007-3-2 loss: 0.665574  [   96/  179]
train() client id: f_00007-3-3 loss: 0.641728  [  128/  179]
train() client id: f_00007-3-4 loss: 0.588814  [  160/  179]
train() client id: f_00007-4-0 loss: 0.729746  [   32/  179]
train() client id: f_00007-4-1 loss: 0.820099  [   64/  179]
train() client id: f_00007-4-2 loss: 0.689421  [   96/  179]
train() client id: f_00007-4-3 loss: 0.609228  [  128/  179]
train() client id: f_00007-4-4 loss: 0.691878  [  160/  179]
train() client id: f_00007-5-0 loss: 0.694975  [   32/  179]
train() client id: f_00007-5-1 loss: 0.652062  [   64/  179]
train() client id: f_00007-5-2 loss: 0.734785  [   96/  179]
train() client id: f_00007-5-3 loss: 0.659835  [  128/  179]
train() client id: f_00007-5-4 loss: 0.771650  [  160/  179]
train() client id: f_00007-6-0 loss: 0.760753  [   32/  179]
train() client id: f_00007-6-1 loss: 0.828925  [   64/  179]
train() client id: f_00007-6-2 loss: 0.559602  [   96/  179]
train() client id: f_00007-6-3 loss: 0.709548  [  128/  179]
train() client id: f_00007-6-4 loss: 0.572646  [  160/  179]
train() client id: f_00007-7-0 loss: 0.624772  [   32/  179]
train() client id: f_00007-7-1 loss: 0.652528  [   64/  179]
train() client id: f_00007-7-2 loss: 0.750335  [   96/  179]
train() client id: f_00007-7-3 loss: 0.731694  [  128/  179]
train() client id: f_00007-7-4 loss: 0.716896  [  160/  179]
train() client id: f_00007-8-0 loss: 0.797187  [   32/  179]
train() client id: f_00007-8-1 loss: 0.641748  [   64/  179]
train() client id: f_00007-8-2 loss: 0.722654  [   96/  179]
train() client id: f_00007-8-3 loss: 0.736232  [  128/  179]
train() client id: f_00007-8-4 loss: 0.570901  [  160/  179]
train() client id: f_00007-9-0 loss: 0.615830  [   32/  179]
train() client id: f_00007-9-1 loss: 0.813140  [   64/  179]
train() client id: f_00007-9-2 loss: 0.677871  [   96/  179]
train() client id: f_00007-9-3 loss: 0.650528  [  128/  179]
train() client id: f_00007-9-4 loss: 0.626880  [  160/  179]
train() client id: f_00007-10-0 loss: 0.744259  [   32/  179]
train() client id: f_00007-10-1 loss: 0.655265  [   64/  179]
train() client id: f_00007-10-2 loss: 0.774184  [   96/  179]
train() client id: f_00007-10-3 loss: 0.555973  [  128/  179]
train() client id: f_00007-10-4 loss: 0.659600  [  160/  179]
train() client id: f_00007-11-0 loss: 0.553666  [   32/  179]
train() client id: f_00007-11-1 loss: 0.634150  [   64/  179]
train() client id: f_00007-11-2 loss: 0.843588  [   96/  179]
train() client id: f_00007-11-3 loss: 0.730392  [  128/  179]
train() client id: f_00007-11-4 loss: 0.546573  [  160/  179]
train() client id: f_00007-12-0 loss: 0.806217  [   32/  179]
train() client id: f_00007-12-1 loss: 0.560748  [   64/  179]
train() client id: f_00007-12-2 loss: 0.539991  [   96/  179]
train() client id: f_00007-12-3 loss: 0.850743  [  128/  179]
train() client id: f_00007-12-4 loss: 0.636013  [  160/  179]
train() client id: f_00008-0-0 loss: 0.729393  [   32/  130]
train() client id: f_00008-0-1 loss: 0.928320  [   64/  130]
train() client id: f_00008-0-2 loss: 0.815441  [   96/  130]
train() client id: f_00008-0-3 loss: 0.904153  [  128/  130]
train() client id: f_00008-1-0 loss: 0.889859  [   32/  130]
train() client id: f_00008-1-1 loss: 0.734311  [   64/  130]
train() client id: f_00008-1-2 loss: 0.924656  [   96/  130]
train() client id: f_00008-1-3 loss: 0.797126  [  128/  130]
train() client id: f_00008-2-0 loss: 0.753294  [   32/  130]
train() client id: f_00008-2-1 loss: 0.815107  [   64/  130]
train() client id: f_00008-2-2 loss: 0.798394  [   96/  130]
train() client id: f_00008-2-3 loss: 0.961172  [  128/  130]
train() client id: f_00008-3-0 loss: 0.888328  [   32/  130]
train() client id: f_00008-3-1 loss: 0.869729  [   64/  130]
train() client id: f_00008-3-2 loss: 0.778853  [   96/  130]
train() client id: f_00008-3-3 loss: 0.797540  [  128/  130]
train() client id: f_00008-4-0 loss: 0.898858  [   32/  130]
train() client id: f_00008-4-1 loss: 0.922813  [   64/  130]
train() client id: f_00008-4-2 loss: 0.768750  [   96/  130]
train() client id: f_00008-4-3 loss: 0.768897  [  128/  130]
train() client id: f_00008-5-0 loss: 0.863108  [   32/  130]
train() client id: f_00008-5-1 loss: 0.877593  [   64/  130]
train() client id: f_00008-5-2 loss: 0.746708  [   96/  130]
train() client id: f_00008-5-3 loss: 0.861161  [  128/  130]
train() client id: f_00008-6-0 loss: 0.886187  [   32/  130]
train() client id: f_00008-6-1 loss: 0.793035  [   64/  130]
train() client id: f_00008-6-2 loss: 0.809188  [   96/  130]
train() client id: f_00008-6-3 loss: 0.855811  [  128/  130]
train() client id: f_00008-7-0 loss: 0.822234  [   32/  130]
train() client id: f_00008-7-1 loss: 0.748117  [   64/  130]
train() client id: f_00008-7-2 loss: 0.946896  [   96/  130]
train() client id: f_00008-7-3 loss: 0.799296  [  128/  130]
train() client id: f_00008-8-0 loss: 0.907895  [   32/  130]
train() client id: f_00008-8-1 loss: 0.888063  [   64/  130]
train() client id: f_00008-8-2 loss: 0.783695  [   96/  130]
train() client id: f_00008-8-3 loss: 0.767074  [  128/  130]
train() client id: f_00008-9-0 loss: 0.862036  [   32/  130]
train() client id: f_00008-9-1 loss: 0.916430  [   64/  130]
train() client id: f_00008-9-2 loss: 0.719365  [   96/  130]
train() client id: f_00008-9-3 loss: 0.840007  [  128/  130]
train() client id: f_00008-10-0 loss: 0.789452  [   32/  130]
train() client id: f_00008-10-1 loss: 0.947571  [   64/  130]
train() client id: f_00008-10-2 loss: 0.757147  [   96/  130]
train() client id: f_00008-10-3 loss: 0.846848  [  128/  130]
train() client id: f_00008-11-0 loss: 0.993284  [   32/  130]
train() client id: f_00008-11-1 loss: 0.782455  [   64/  130]
train() client id: f_00008-11-2 loss: 0.855954  [   96/  130]
train() client id: f_00008-11-3 loss: 0.700329  [  128/  130]
train() client id: f_00008-12-0 loss: 0.938884  [   32/  130]
train() client id: f_00008-12-1 loss: 0.798165  [   64/  130]
train() client id: f_00008-12-2 loss: 0.867612  [   96/  130]
train() client id: f_00008-12-3 loss: 0.727373  [  128/  130]
train() client id: f_00009-0-0 loss: 1.266638  [   32/  118]
train() client id: f_00009-0-1 loss: 1.198340  [   64/  118]
train() client id: f_00009-0-2 loss: 1.020803  [   96/  118]
train() client id: f_00009-1-0 loss: 1.192515  [   32/  118]
train() client id: f_00009-1-1 loss: 1.077774  [   64/  118]
train() client id: f_00009-1-2 loss: 1.060028  [   96/  118]
train() client id: f_00009-2-0 loss: 1.090703  [   32/  118]
train() client id: f_00009-2-1 loss: 1.166423  [   64/  118]
train() client id: f_00009-2-2 loss: 0.927191  [   96/  118]
train() client id: f_00009-3-0 loss: 1.077036  [   32/  118]
train() client id: f_00009-3-1 loss: 1.001557  [   64/  118]
train() client id: f_00009-3-2 loss: 0.968407  [   96/  118]
train() client id: f_00009-4-0 loss: 1.054384  [   32/  118]
train() client id: f_00009-4-1 loss: 0.978912  [   64/  118]
train() client id: f_00009-4-2 loss: 0.988116  [   96/  118]
train() client id: f_00009-5-0 loss: 0.934980  [   32/  118]
train() client id: f_00009-5-1 loss: 0.891865  [   64/  118]
train() client id: f_00009-5-2 loss: 0.916088  [   96/  118]
train() client id: f_00009-6-0 loss: 0.935773  [   32/  118]
train() client id: f_00009-6-1 loss: 0.986236  [   64/  118]
train() client id: f_00009-6-2 loss: 0.962551  [   96/  118]
train() client id: f_00009-7-0 loss: 0.880160  [   32/  118]
train() client id: f_00009-7-1 loss: 1.046154  [   64/  118]
train() client id: f_00009-7-2 loss: 0.889044  [   96/  118]
train() client id: f_00009-8-0 loss: 0.894865  [   32/  118]
train() client id: f_00009-8-1 loss: 1.033803  [   64/  118]
train() client id: f_00009-8-2 loss: 0.807980  [   96/  118]
train() client id: f_00009-9-0 loss: 0.932773  [   32/  118]
train() client id: f_00009-9-1 loss: 0.869462  [   64/  118]
train() client id: f_00009-9-2 loss: 0.848588  [   96/  118]
train() client id: f_00009-10-0 loss: 0.949121  [   32/  118]
train() client id: f_00009-10-1 loss: 0.852602  [   64/  118]
train() client id: f_00009-10-2 loss: 0.855432  [   96/  118]
train() client id: f_00009-11-0 loss: 0.887654  [   32/  118]
train() client id: f_00009-11-1 loss: 0.848980  [   64/  118]
train() client id: f_00009-11-2 loss: 0.961787  [   96/  118]
train() client id: f_00009-12-0 loss: 0.794030  [   32/  118]
train() client id: f_00009-12-1 loss: 0.822752  [   64/  118]
train() client id: f_00009-12-2 loss: 1.084153  [   96/  118]
At round 5 accuracy: 0.623342175066313
At round 5 training accuracy: 0.568075117370892
At round 5 training loss: 0.8754328292545298
update_location
xs = 0.471708 -73.998411 45.045120 64.056472 -165.103519 -100.217951 12.784040 -1.624259 -1.680116 -55.304393 
ys = 17.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 6.875078 -19.177652 57.845030 -0.998518 
xs mean: -27.55713084058713
ys mean: 9.371751218646876
dists_uav = 101.535998 125.370447 109.685035 120.878040 193.252679 142.614835 101.048000 101.835262 115.537311 114.278488 
uav_gains = -100.165521 -102.455275 -101.003744 -102.058922 -107.259090 -103.856438 -100.113212 -100.197475 -101.568182 -101.449219 
uav_gains_db_mean: -102.01270785283263
dists_bs = 235.730505 188.742580 280.329063 283.532321 165.945174 206.222587 252.049271 260.315401 209.201026 212.843879 
bs_gains = -105.994701 -103.291416 -108.101767 -108.239931 -101.726069 -104.368474 -106.808652 -107.201055 -104.542846 -104.752772 
bs_gains_db_mean: -105.50276819497782
Round 6
-------------------------------
ene_coms = [0.00634601 0.00729738 0.00657626 0.00689062 0.00678831 0.00769337
 0.00633217 0.0063545  0.00776145 0.00784499]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.19635114 21.28505584 10.0432392   3.60440368 24.53498391 11.82379389
  4.46520185 14.41506884 10.5520832   9.59915732]
obj_prev = 120.51933887434711
eta_min = 5.426929083360556e-10	eta_max = 0.9297489345075307
af = 25.512246160639165	bf = 1.8259100890390092	zeta = 28.063470776703085	eta = 0.909090909090909
af = 25.512246160639165	bf = 1.8259100890390092	zeta = 46.832355436038924	eta = 0.5447568443462639
af = 25.512246160639165	bf = 1.8259100890390092	zeta = 38.05098392178735	eta = 0.6704753341747698
af = 25.512246160639165	bf = 1.8259100890390092	zeta = 36.49165070030884	eta = 0.6991255717687567
af = 25.512246160639165	bf = 1.8259100890390092	zeta = 36.4184437791056	eta = 0.7005309264553566
af = 25.512246160639165	bf = 1.8259100890390092	zeta = 36.418272120607526	eta = 0.700534228426584
eta = 0.700534228426584
ene_coms = [0.00634601 0.00729738 0.00657626 0.00689062 0.00678831 0.00769337
 0.00633217 0.0063545  0.00776145 0.00784499]
ene_comp = [0.02936901 0.06176815 0.02890283 0.01002276 0.07132474 0.03403076
 0.01258672 0.04172264 0.03030136 0.02750431]
ene_total = [3.11600918 6.02572345 3.09542533 1.47563263 6.81508668 3.64028186
 1.65060611 4.19455999 3.32084535 3.08410153]
ti_comp = [0.27695455 0.26744086 0.27465208 0.27150843 0.27253158 0.26348103
 0.27709296 0.2768697  0.26280017 0.26196474]
ti_coms = [0.06346013 0.07297382 0.0657626  0.06890624 0.0678831  0.07693365
 0.06332171 0.06354498 0.07761451 0.07844993]
t_total = [29.69997482 29.69997482 29.69997482 29.69997482 29.69997482 29.69997482
 29.69997482 29.69997482 29.69997482 29.69997482]
ene_coms = [0.00634601 0.00729738 0.00657626 0.00689062 0.00678831 0.00769337
 0.00633217 0.0063545  0.00776145 0.00784499]
ene_comp = [2.06410081e-05 2.05929418e-04 2.00048617e-05 8.53640466e-07
 3.05328188e-04 3.54810248e-05 1.62317953e-06 5.92167417e-05
 2.51776426e-05 1.89494497e-05]
ene_total = [0.55546799 0.65463736 0.57550074 0.60125705 0.61889482 0.67431446
 0.55260114 0.55957387 0.67935578 0.68610115]
optimize_network iter = 0 obj = 6.157704360124754
eta = 0.700534228426584
freqs = [5.30213521e+07 1.15480027e+08 5.26171781e+07 1.84575430e+07
 1.30855914e+08 6.45791417e+07 2.27120857e+07 7.53470680e+07
 5.76509508e+07 5.24962060e+07]
eta_min = 0.6694832070795439	eta_max = 0.7005342284265734
af = 0.05088784773635443	bf = 1.8259100890390092	zeta = 0.055976632509989875	eta = 0.9090909090909091
af = 0.05088784773635443	bf = 1.8259100890390092	zeta = 20.138362408522774	eta = 0.0025269109128167313
af = 0.05088784773635443	bf = 1.8259100890390092	zeta = 2.1356545853734668	eta = 0.02382775196179749
af = 0.05088784773635443	bf = 1.8259100890390092	zeta = 2.0652841868633356	eta = 0.02463963461301696
af = 0.05088784773635443	bf = 1.8259100890390092	zeta = 2.06525522605568	eta = 0.024639980131435084
eta = 0.024639980131435084
ene_coms = [0.00634601 0.00729738 0.00657626 0.00689062 0.00678831 0.00769337
 0.00633217 0.0063545  0.00776145 0.00784499]
ene_comp = [2.14776652e-04 2.14276505e-03 2.08157334e-04 8.88241698e-06
 3.17704279e-03 3.69192031e-04 1.68897306e-05 6.16170171e-04
 2.61981863e-04 1.97175416e-04]
ene_total = [0.17574628 0.2528767  0.18173668 0.18481963 0.26694558 0.21597469
 0.17007464 0.18672585 0.21492665 0.21542853]
ti_comp = [0.31225148 0.30273779 0.30994901 0.30680537 0.30782851 0.29877796
 0.3123899  0.31216663 0.2980971  0.29726168]
ti_coms = [0.06346013 0.07297382 0.0657626  0.06890624 0.0678831  0.07693365
 0.06332171 0.06354498 0.07761451 0.07844993]
t_total = [29.69997482 29.69997482 29.69997482 29.69997482 29.69997482 29.69997482
 29.69997482 29.69997482 29.69997482 29.69997482]
ene_coms = [0.00634601 0.00729738 0.00657626 0.00689062 0.00678831 0.00769337
 0.00633217 0.0063545  0.00776145 0.00784499]
ene_comp = [2.32676938e-05 2.30279392e-04 2.25079151e-05 9.57922081e-07
 3.42923560e-04 3.95377708e-05 1.82994502e-06 6.67477842e-05
 2.80391782e-05 2.10871989e-05]
ene_total = [0.50349115 0.59506109 0.52163211 0.54477907 0.56372349 0.61128541
 0.50070233 0.50759898 0.61575863 0.62181308]
optimize_network iter = 1 obj = 5.585845325709744
eta = 0.6694832070795439
freqs = [5.30183582e+07 1.15011026e+08 5.25643949e+07 1.84147464e+07
 1.30608916e+08 6.42043627e+07 2.27120857e+07 7.53402035e+07
 5.72988416e+07 5.21558853e+07]
eta_min = 0.6694832070795489	eta_max = 0.6694832070795447
af = 0.050605942439086025	bf = 1.8259100890390092	zeta = 0.055666536682994636	eta = 0.909090909090909
af = 0.050605942439086025	bf = 1.8259100890390092	zeta = 20.138066855632474	eta = 0.0025129493710530563
af = 0.050605942439086025	bf = 1.8259100890390092	zeta = 2.1342155250850863	eta = 0.023711730068624856
af = 0.050605942439086025	bf = 1.8259100890390092	zeta = 2.0642127211512107	eta = 0.02451585629743775
af = 0.050605942439086025	bf = 1.8259100890390092	zeta = 2.0641841889907853	eta = 0.024516195167558246
eta = 0.024516195167558246
ene_coms = [0.00634601 0.00729738 0.00657626 0.00689062 0.00678831 0.00769337
 0.00633217 0.0063545  0.00776145 0.00784499]
ene_comp = [2.15044449e-04 2.12828592e-03 2.08022430e-04 8.85329797e-06
 3.16936473e-03 3.65415594e-04 1.69126997e-05 6.16895709e-04
 2.59143415e-04 1.94891901e-04]
ene_total = [0.17573115 0.2524568  0.18171001 0.1847954  0.26670605 0.21584614
 0.17005367 0.18672158 0.21482336 0.21534003]
ti_comp = [0.31225148 0.30273779 0.30994901 0.30680537 0.30782851 0.29877796
 0.3123899  0.31216663 0.2980971  0.29726168]
ti_coms = [0.06346013 0.07297382 0.0657626  0.06890624 0.0678831  0.07693365
 0.06332171 0.06354498 0.07761451 0.07844993]
t_total = [29.69997482 29.69997482 29.69997482 29.69997482 29.69997482 29.69997482
 29.69997482 29.69997482 29.69997482 29.69997482]
ene_coms = [0.00634601 0.00729738 0.00657626 0.00689062 0.00678831 0.00769337
 0.00633217 0.0063545  0.00776145 0.00784499]
ene_comp = [2.32676938e-05 2.30279392e-04 2.25079151e-05 9.57922081e-07
 3.42923560e-04 3.95377708e-05 1.82994502e-06 6.67477842e-05
 2.80391782e-05 2.10871989e-05]
ene_total = [0.50349115 0.59506109 0.52163211 0.54477907 0.56372349 0.61128541
 0.50070233 0.50759898 0.61575863 0.62181308]
optimize_network iter = 2 obj = 5.585845325709827
eta = 0.6694832070795489
freqs = [5.30183582e+07 1.15011026e+08 5.25643949e+07 1.84147464e+07
 1.30608916e+08 6.42043627e+07 2.27120857e+07 7.53402035e+07
 5.72988416e+07 5.21558853e+07]
Done!
ene_coms = [0.00634601 0.00729738 0.00657626 0.00689062 0.00678831 0.00769337
 0.00633217 0.0063545  0.00776145 0.00784499]
ene_comp = [2.30216502e-05 2.27844310e-04 2.22699058e-05 9.47792563e-07
 3.39297325e-04 3.91196798e-05 1.81059432e-06 6.60419617e-05
 2.77426787e-05 2.08642130e-05]
ene_total = [0.00636903 0.00752523 0.00659853 0.00689157 0.00712761 0.00773248
 0.00633398 0.00642054 0.00778919 0.00786586]
At round 6 energy consumption: 0.07065402852706373
At round 6 eta: 0.6694832070795489
At round 6 a_n: 26.12732777443847
At round 6 local rounds: 13.138937298352452
At round 6 global rounds: 79.04992525062653
gradient difference: 0.337081640958786
train() client id: f_00000-0-0 loss: 1.446605  [   32/  126]
train() client id: f_00000-0-1 loss: 1.286829  [   64/  126]
train() client id: f_00000-0-2 loss: 1.226222  [   96/  126]
train() client id: f_00000-1-0 loss: 1.101528  [   32/  126]
train() client id: f_00000-1-1 loss: 1.152767  [   64/  126]
train() client id: f_00000-1-2 loss: 1.129800  [   96/  126]
train() client id: f_00000-2-0 loss: 1.100945  [   32/  126]
train() client id: f_00000-2-1 loss: 1.137996  [   64/  126]
train() client id: f_00000-2-2 loss: 1.074701  [   96/  126]
train() client id: f_00000-3-0 loss: 1.032766  [   32/  126]
train() client id: f_00000-3-1 loss: 1.078637  [   64/  126]
train() client id: f_00000-3-2 loss: 0.910421  [   96/  126]
train() client id: f_00000-4-0 loss: 1.023342  [   32/  126]
train() client id: f_00000-4-1 loss: 0.918470  [   64/  126]
train() client id: f_00000-4-2 loss: 0.998646  [   96/  126]
train() client id: f_00000-5-0 loss: 0.855582  [   32/  126]
train() client id: f_00000-5-1 loss: 0.937064  [   64/  126]
train() client id: f_00000-5-2 loss: 0.907188  [   96/  126]
train() client id: f_00000-6-0 loss: 0.956668  [   32/  126]
train() client id: f_00000-6-1 loss: 0.816261  [   64/  126]
train() client id: f_00000-6-2 loss: 0.895985  [   96/  126]
train() client id: f_00000-7-0 loss: 0.827264  [   32/  126]
train() client id: f_00000-7-1 loss: 0.832102  [   64/  126]
train() client id: f_00000-7-2 loss: 0.875815  [   96/  126]
train() client id: f_00000-8-0 loss: 0.835554  [   32/  126]
train() client id: f_00000-8-1 loss: 0.840548  [   64/  126]
train() client id: f_00000-8-2 loss: 0.786368  [   96/  126]
train() client id: f_00000-9-0 loss: 0.903990  [   32/  126]
train() client id: f_00000-9-1 loss: 0.661060  [   64/  126]
train() client id: f_00000-9-2 loss: 0.774074  [   96/  126]
train() client id: f_00000-10-0 loss: 0.852157  [   32/  126]
train() client id: f_00000-10-1 loss: 0.746195  [   64/  126]
train() client id: f_00000-10-2 loss: 0.680294  [   96/  126]
train() client id: f_00000-11-0 loss: 0.736516  [   32/  126]
train() client id: f_00000-11-1 loss: 0.882382  [   64/  126]
train() client id: f_00000-11-2 loss: 0.771463  [   96/  126]
train() client id: f_00000-12-0 loss: 0.755026  [   32/  126]
train() client id: f_00000-12-1 loss: 0.758670  [   64/  126]
train() client id: f_00000-12-2 loss: 0.854935  [   96/  126]
train() client id: f_00001-0-0 loss: 0.604756  [   32/  265]
train() client id: f_00001-0-1 loss: 0.544474  [   64/  265]
train() client id: f_00001-0-2 loss: 0.613513  [   96/  265]
train() client id: f_00001-0-3 loss: 0.661777  [  128/  265]
train() client id: f_00001-0-4 loss: 0.607773  [  160/  265]
train() client id: f_00001-0-5 loss: 0.585858  [  192/  265]
train() client id: f_00001-0-6 loss: 0.650195  [  224/  265]
train() client id: f_00001-0-7 loss: 0.604111  [  256/  265]
train() client id: f_00001-1-0 loss: 0.624252  [   32/  265]
train() client id: f_00001-1-1 loss: 0.543300  [   64/  265]
train() client id: f_00001-1-2 loss: 0.635133  [   96/  265]
train() client id: f_00001-1-3 loss: 0.670057  [  128/  265]
train() client id: f_00001-1-4 loss: 0.543056  [  160/  265]
train() client id: f_00001-1-5 loss: 0.522840  [  192/  265]
train() client id: f_00001-1-6 loss: 0.546900  [  224/  265]
train() client id: f_00001-1-7 loss: 0.576907  [  256/  265]
train() client id: f_00001-2-0 loss: 0.714829  [   32/  265]
train() client id: f_00001-2-1 loss: 0.591674  [   64/  265]
train() client id: f_00001-2-2 loss: 0.529225  [   96/  265]
train() client id: f_00001-2-3 loss: 0.598787  [  128/  265]
train() client id: f_00001-2-4 loss: 0.500179  [  160/  265]
train() client id: f_00001-2-5 loss: 0.520559  [  192/  265]
train() client id: f_00001-2-6 loss: 0.509668  [  224/  265]
train() client id: f_00001-2-7 loss: 0.531516  [  256/  265]
train() client id: f_00001-3-0 loss: 0.597864  [   32/  265]
train() client id: f_00001-3-1 loss: 0.620535  [   64/  265]
train() client id: f_00001-3-2 loss: 0.522623  [   96/  265]
train() client id: f_00001-3-3 loss: 0.478279  [  128/  265]
train() client id: f_00001-3-4 loss: 0.485004  [  160/  265]
train() client id: f_00001-3-5 loss: 0.579413  [  192/  265]
train() client id: f_00001-3-6 loss: 0.622908  [  224/  265]
train() client id: f_00001-3-7 loss: 0.579162  [  256/  265]
train() client id: f_00001-4-0 loss: 0.546800  [   32/  265]
train() client id: f_00001-4-1 loss: 0.626994  [   64/  265]
train() client id: f_00001-4-2 loss: 0.532733  [   96/  265]
train() client id: f_00001-4-3 loss: 0.676672  [  128/  265]
train() client id: f_00001-4-4 loss: 0.464159  [  160/  265]
train() client id: f_00001-4-5 loss: 0.574327  [  192/  265]
train() client id: f_00001-4-6 loss: 0.499950  [  224/  265]
train() client id: f_00001-4-7 loss: 0.532242  [  256/  265]
train() client id: f_00001-5-0 loss: 0.488565  [   32/  265]
train() client id: f_00001-5-1 loss: 0.457850  [   64/  265]
train() client id: f_00001-5-2 loss: 0.656303  [   96/  265]
train() client id: f_00001-5-3 loss: 0.550893  [  128/  265]
train() client id: f_00001-5-4 loss: 0.501064  [  160/  265]
train() client id: f_00001-5-5 loss: 0.524805  [  192/  265]
train() client id: f_00001-5-6 loss: 0.620930  [  224/  265]
train() client id: f_00001-5-7 loss: 0.612690  [  256/  265]
train() client id: f_00001-6-0 loss: 0.558406  [   32/  265]
train() client id: f_00001-6-1 loss: 0.648589  [   64/  265]
train() client id: f_00001-6-2 loss: 0.464008  [   96/  265]
train() client id: f_00001-6-3 loss: 0.515491  [  128/  265]
train() client id: f_00001-6-4 loss: 0.506182  [  160/  265]
train() client id: f_00001-6-5 loss: 0.574720  [  192/  265]
train() client id: f_00001-6-6 loss: 0.461314  [  224/  265]
train() client id: f_00001-6-7 loss: 0.615503  [  256/  265]
train() client id: f_00001-7-0 loss: 0.493692  [   32/  265]
train() client id: f_00001-7-1 loss: 0.485173  [   64/  265]
train() client id: f_00001-7-2 loss: 0.550945  [   96/  265]
train() client id: f_00001-7-3 loss: 0.544655  [  128/  265]
train() client id: f_00001-7-4 loss: 0.627210  [  160/  265]
train() client id: f_00001-7-5 loss: 0.526808  [  192/  265]
train() client id: f_00001-7-6 loss: 0.571087  [  224/  265]
train() client id: f_00001-7-7 loss: 0.504475  [  256/  265]
train() client id: f_00001-8-0 loss: 0.567484  [   32/  265]
train() client id: f_00001-8-1 loss: 0.499413  [   64/  265]
train() client id: f_00001-8-2 loss: 0.518363  [   96/  265]
train() client id: f_00001-8-3 loss: 0.503663  [  128/  265]
train() client id: f_00001-8-4 loss: 0.554898  [  160/  265]
train() client id: f_00001-8-5 loss: 0.556358  [  192/  265]
train() client id: f_00001-8-6 loss: 0.530846  [  224/  265]
train() client id: f_00001-8-7 loss: 0.526583  [  256/  265]
train() client id: f_00001-9-0 loss: 0.442857  [   32/  265]
train() client id: f_00001-9-1 loss: 0.566222  [   64/  265]
train() client id: f_00001-9-2 loss: 0.514394  [   96/  265]
train() client id: f_00001-9-3 loss: 0.610484  [  128/  265]
train() client id: f_00001-9-4 loss: 0.516636  [  160/  265]
train() client id: f_00001-9-5 loss: 0.600624  [  192/  265]
train() client id: f_00001-9-6 loss: 0.577087  [  224/  265]
train() client id: f_00001-9-7 loss: 0.457181  [  256/  265]
train() client id: f_00001-10-0 loss: 0.537295  [   32/  265]
train() client id: f_00001-10-1 loss: 0.452086  [   64/  265]
train() client id: f_00001-10-2 loss: 0.627221  [   96/  265]
train() client id: f_00001-10-3 loss: 0.514806  [  128/  265]
train() client id: f_00001-10-4 loss: 0.578893  [  160/  265]
train() client id: f_00001-10-5 loss: 0.522615  [  192/  265]
train() client id: f_00001-10-6 loss: 0.603625  [  224/  265]
train() client id: f_00001-10-7 loss: 0.474201  [  256/  265]
train() client id: f_00001-11-0 loss: 0.598308  [   32/  265]
train() client id: f_00001-11-1 loss: 0.486778  [   64/  265]
train() client id: f_00001-11-2 loss: 0.588616  [   96/  265]
train() client id: f_00001-11-3 loss: 0.535096  [  128/  265]
train() client id: f_00001-11-4 loss: 0.470935  [  160/  265]
train() client id: f_00001-11-5 loss: 0.512827  [  192/  265]
train() client id: f_00001-11-6 loss: 0.546400  [  224/  265]
train() client id: f_00001-11-7 loss: 0.578970  [  256/  265]
train() client id: f_00001-12-0 loss: 0.465275  [   32/  265]
train() client id: f_00001-12-1 loss: 0.589858  [   64/  265]
train() client id: f_00001-12-2 loss: 0.538975  [   96/  265]
train() client id: f_00001-12-3 loss: 0.489988  [  128/  265]
train() client id: f_00001-12-4 loss: 0.552117  [  160/  265]
train() client id: f_00001-12-5 loss: 0.580562  [  192/  265]
train() client id: f_00001-12-6 loss: 0.567973  [  224/  265]
train() client id: f_00001-12-7 loss: 0.531858  [  256/  265]
train() client id: f_00002-0-0 loss: 1.232537  [   32/  124]
train() client id: f_00002-0-1 loss: 1.134848  [   64/  124]
train() client id: f_00002-0-2 loss: 1.283811  [   96/  124]
train() client id: f_00002-1-0 loss: 1.127004  [   32/  124]
train() client id: f_00002-1-1 loss: 1.221538  [   64/  124]
train() client id: f_00002-1-2 loss: 1.140443  [   96/  124]
train() client id: f_00002-2-0 loss: 1.138632  [   32/  124]
train() client id: f_00002-2-1 loss: 1.146793  [   64/  124]
train() client id: f_00002-2-2 loss: 1.083452  [   96/  124]
train() client id: f_00002-3-0 loss: 1.196980  [   32/  124]
train() client id: f_00002-3-1 loss: 1.027640  [   64/  124]
train() client id: f_00002-3-2 loss: 1.128013  [   96/  124]
train() client id: f_00002-4-0 loss: 1.009439  [   32/  124]
train() client id: f_00002-4-1 loss: 1.143439  [   64/  124]
train() client id: f_00002-4-2 loss: 1.006779  [   96/  124]
train() client id: f_00002-5-0 loss: 0.983130  [   32/  124]
train() client id: f_00002-5-1 loss: 1.148998  [   64/  124]
train() client id: f_00002-5-2 loss: 1.080268  [   96/  124]
train() client id: f_00002-6-0 loss: 0.939180  [   32/  124]
train() client id: f_00002-6-1 loss: 1.163890  [   64/  124]
train() client id: f_00002-6-2 loss: 1.034832  [   96/  124]
train() client id: f_00002-7-0 loss: 1.069528  [   32/  124]
train() client id: f_00002-7-1 loss: 1.132940  [   64/  124]
train() client id: f_00002-7-2 loss: 0.964788  [   96/  124]
train() client id: f_00002-8-0 loss: 1.048304  [   32/  124]
train() client id: f_00002-8-1 loss: 0.960939  [   64/  124]
train() client id: f_00002-8-2 loss: 1.022996  [   96/  124]
train() client id: f_00002-9-0 loss: 1.092811  [   32/  124]
train() client id: f_00002-9-1 loss: 1.072473  [   64/  124]
train() client id: f_00002-9-2 loss: 1.006871  [   96/  124]
train() client id: f_00002-10-0 loss: 1.128598  [   32/  124]
train() client id: f_00002-10-1 loss: 0.912669  [   64/  124]
train() client id: f_00002-10-2 loss: 1.073529  [   96/  124]
train() client id: f_00002-11-0 loss: 0.995349  [   32/  124]
train() client id: f_00002-11-1 loss: 0.994504  [   64/  124]
train() client id: f_00002-11-2 loss: 0.837571  [   96/  124]
train() client id: f_00002-12-0 loss: 0.961358  [   32/  124]
train() client id: f_00002-12-1 loss: 1.010625  [   64/  124]
train() client id: f_00002-12-2 loss: 1.139259  [   96/  124]
train() client id: f_00003-0-0 loss: 0.899686  [   32/   43]
train() client id: f_00003-1-0 loss: 0.965613  [   32/   43]
train() client id: f_00003-2-0 loss: 0.931538  [   32/   43]
train() client id: f_00003-3-0 loss: 0.953037  [   32/   43]
train() client id: f_00003-4-0 loss: 0.905814  [   32/   43]
train() client id: f_00003-5-0 loss: 0.865987  [   32/   43]
train() client id: f_00003-6-0 loss: 0.895099  [   32/   43]
train() client id: f_00003-7-0 loss: 0.971283  [   32/   43]
train() client id: f_00003-8-0 loss: 0.937904  [   32/   43]
train() client id: f_00003-9-0 loss: 1.001793  [   32/   43]
train() client id: f_00003-10-0 loss: 0.933965  [   32/   43]
train() client id: f_00003-11-0 loss: 0.943793  [   32/   43]
train() client id: f_00003-12-0 loss: 0.901478  [   32/   43]
train() client id: f_00004-0-0 loss: 0.805680  [   32/  306]
train() client id: f_00004-0-1 loss: 0.782238  [   64/  306]
train() client id: f_00004-0-2 loss: 0.893794  [   96/  306]
train() client id: f_00004-0-3 loss: 0.965122  [  128/  306]
train() client id: f_00004-0-4 loss: 0.904685  [  160/  306]
train() client id: f_00004-0-5 loss: 0.710371  [  192/  306]
train() client id: f_00004-0-6 loss: 0.923665  [  224/  306]
train() client id: f_00004-0-7 loss: 0.935082  [  256/  306]
train() client id: f_00004-0-8 loss: 0.965064  [  288/  306]
train() client id: f_00004-1-0 loss: 0.817810  [   32/  306]
train() client id: f_00004-1-1 loss: 0.796378  [   64/  306]
train() client id: f_00004-1-2 loss: 0.981069  [   96/  306]
train() client id: f_00004-1-3 loss: 0.896323  [  128/  306]
train() client id: f_00004-1-4 loss: 1.039357  [  160/  306]
train() client id: f_00004-1-5 loss: 0.957098  [  192/  306]
train() client id: f_00004-1-6 loss: 0.781510  [  224/  306]
train() client id: f_00004-1-7 loss: 0.827595  [  256/  306]
train() client id: f_00004-1-8 loss: 0.816095  [  288/  306]
train() client id: f_00004-2-0 loss: 0.846332  [   32/  306]
train() client id: f_00004-2-1 loss: 0.937373  [   64/  306]
train() client id: f_00004-2-2 loss: 0.839647  [   96/  306]
train() client id: f_00004-2-3 loss: 0.699296  [  128/  306]
train() client id: f_00004-2-4 loss: 0.847735  [  160/  306]
train() client id: f_00004-2-5 loss: 0.912124  [  192/  306]
train() client id: f_00004-2-6 loss: 0.921602  [  224/  306]
train() client id: f_00004-2-7 loss: 0.806104  [  256/  306]
train() client id: f_00004-2-8 loss: 0.919049  [  288/  306]
train() client id: f_00004-3-0 loss: 0.990371  [   32/  306]
train() client id: f_00004-3-1 loss: 0.840095  [   64/  306]
train() client id: f_00004-3-2 loss: 0.921328  [   96/  306]
train() client id: f_00004-3-3 loss: 0.948671  [  128/  306]
train() client id: f_00004-3-4 loss: 0.786878  [  160/  306]
train() client id: f_00004-3-5 loss: 0.793270  [  192/  306]
train() client id: f_00004-3-6 loss: 0.830106  [  224/  306]
train() client id: f_00004-3-7 loss: 0.893733  [  256/  306]
train() client id: f_00004-3-8 loss: 0.857576  [  288/  306]
train() client id: f_00004-4-0 loss: 0.907943  [   32/  306]
train() client id: f_00004-4-1 loss: 0.880866  [   64/  306]
train() client id: f_00004-4-2 loss: 0.857371  [   96/  306]
train() client id: f_00004-4-3 loss: 0.725610  [  128/  306]
train() client id: f_00004-4-4 loss: 0.936785  [  160/  306]
train() client id: f_00004-4-5 loss: 0.840444  [  192/  306]
train() client id: f_00004-4-6 loss: 0.958511  [  224/  306]
train() client id: f_00004-4-7 loss: 1.025250  [  256/  306]
train() client id: f_00004-4-8 loss: 0.764143  [  288/  306]
train() client id: f_00004-5-0 loss: 0.975794  [   32/  306]
train() client id: f_00004-5-1 loss: 0.863570  [   64/  306]
train() client id: f_00004-5-2 loss: 0.763444  [   96/  306]
train() client id: f_00004-5-3 loss: 0.808114  [  128/  306]
train() client id: f_00004-5-4 loss: 0.894229  [  160/  306]
train() client id: f_00004-5-5 loss: 0.807393  [  192/  306]
train() client id: f_00004-5-6 loss: 0.918032  [  224/  306]
train() client id: f_00004-5-7 loss: 1.057820  [  256/  306]
train() client id: f_00004-5-8 loss: 0.775476  [  288/  306]
train() client id: f_00004-6-0 loss: 0.957801  [   32/  306]
train() client id: f_00004-6-1 loss: 0.868057  [   64/  306]
train() client id: f_00004-6-2 loss: 0.894446  [   96/  306]
train() client id: f_00004-6-3 loss: 0.740204  [  128/  306]
train() client id: f_00004-6-4 loss: 0.908292  [  160/  306]
train() client id: f_00004-6-5 loss: 0.786882  [  192/  306]
train() client id: f_00004-6-6 loss: 0.867077  [  224/  306]
train() client id: f_00004-6-7 loss: 1.047987  [  256/  306]
train() client id: f_00004-6-8 loss: 0.915713  [  288/  306]
train() client id: f_00004-7-0 loss: 0.794948  [   32/  306]
train() client id: f_00004-7-1 loss: 0.907104  [   64/  306]
train() client id: f_00004-7-2 loss: 0.896924  [   96/  306]
train() client id: f_00004-7-3 loss: 0.815383  [  128/  306]
train() client id: f_00004-7-4 loss: 0.997106  [  160/  306]
train() client id: f_00004-7-5 loss: 0.933752  [  192/  306]
train() client id: f_00004-7-6 loss: 0.910185  [  224/  306]
train() client id: f_00004-7-7 loss: 0.823124  [  256/  306]
train() client id: f_00004-7-8 loss: 0.842676  [  288/  306]
train() client id: f_00004-8-0 loss: 0.819069  [   32/  306]
train() client id: f_00004-8-1 loss: 0.803359  [   64/  306]
train() client id: f_00004-8-2 loss: 0.885888  [   96/  306]
train() client id: f_00004-8-3 loss: 0.910216  [  128/  306]
train() client id: f_00004-8-4 loss: 0.834478  [  160/  306]
train() client id: f_00004-8-5 loss: 0.837036  [  192/  306]
train() client id: f_00004-8-6 loss: 0.867182  [  224/  306]
train() client id: f_00004-8-7 loss: 0.819909  [  256/  306]
train() client id: f_00004-8-8 loss: 1.039374  [  288/  306]
train() client id: f_00004-9-0 loss: 0.913972  [   32/  306]
train() client id: f_00004-9-1 loss: 0.867337  [   64/  306]
train() client id: f_00004-9-2 loss: 0.900281  [   96/  306]
train() client id: f_00004-9-3 loss: 0.771353  [  128/  306]
train() client id: f_00004-9-4 loss: 0.878313  [  160/  306]
train() client id: f_00004-9-5 loss: 0.962124  [  192/  306]
train() client id: f_00004-9-6 loss: 0.955391  [  224/  306]
train() client id: f_00004-9-7 loss: 0.899470  [  256/  306]
train() client id: f_00004-9-8 loss: 0.746108  [  288/  306]
train() client id: f_00004-10-0 loss: 0.883504  [   32/  306]
train() client id: f_00004-10-1 loss: 0.845444  [   64/  306]
train() client id: f_00004-10-2 loss: 0.975437  [   96/  306]
train() client id: f_00004-10-3 loss: 0.893352  [  128/  306]
train() client id: f_00004-10-4 loss: 0.793083  [  160/  306]
train() client id: f_00004-10-5 loss: 0.842650  [  192/  306]
train() client id: f_00004-10-6 loss: 0.952629  [  224/  306]
train() client id: f_00004-10-7 loss: 0.855056  [  256/  306]
train() client id: f_00004-10-8 loss: 0.878293  [  288/  306]
train() client id: f_00004-11-0 loss: 0.842968  [   32/  306]
train() client id: f_00004-11-1 loss: 0.828161  [   64/  306]
train() client id: f_00004-11-2 loss: 0.885805  [   96/  306]
train() client id: f_00004-11-3 loss: 0.955902  [  128/  306]
train() client id: f_00004-11-4 loss: 0.831519  [  160/  306]
train() client id: f_00004-11-5 loss: 0.825034  [  192/  306]
train() client id: f_00004-11-6 loss: 0.877040  [  224/  306]
train() client id: f_00004-11-7 loss: 0.984962  [  256/  306]
train() client id: f_00004-11-8 loss: 0.858236  [  288/  306]
train() client id: f_00004-12-0 loss: 0.836836  [   32/  306]
train() client id: f_00004-12-1 loss: 0.818068  [   64/  306]
train() client id: f_00004-12-2 loss: 0.818523  [   96/  306]
train() client id: f_00004-12-3 loss: 0.931657  [  128/  306]
train() client id: f_00004-12-4 loss: 0.949279  [  160/  306]
train() client id: f_00004-12-5 loss: 0.808336  [  192/  306]
train() client id: f_00004-12-6 loss: 0.908123  [  224/  306]
train() client id: f_00004-12-7 loss: 0.920933  [  256/  306]
train() client id: f_00004-12-8 loss: 0.879636  [  288/  306]
train() client id: f_00005-0-0 loss: 0.527200  [   32/  146]
train() client id: f_00005-0-1 loss: 0.666223  [   64/  146]
train() client id: f_00005-0-2 loss: 0.619814  [   96/  146]
train() client id: f_00005-0-3 loss: 0.666359  [  128/  146]
train() client id: f_00005-1-0 loss: 0.566393  [   32/  146]
train() client id: f_00005-1-1 loss: 0.702678  [   64/  146]
train() client id: f_00005-1-2 loss: 0.627700  [   96/  146]
train() client id: f_00005-1-3 loss: 0.664813  [  128/  146]
train() client id: f_00005-2-0 loss: 0.698506  [   32/  146]
train() client id: f_00005-2-1 loss: 0.585985  [   64/  146]
train() client id: f_00005-2-2 loss: 0.728966  [   96/  146]
train() client id: f_00005-2-3 loss: 0.420303  [  128/  146]
train() client id: f_00005-3-0 loss: 0.620924  [   32/  146]
train() client id: f_00005-3-1 loss: 0.763058  [   64/  146]
train() client id: f_00005-3-2 loss: 0.645228  [   96/  146]
train() client id: f_00005-3-3 loss: 0.534449  [  128/  146]
train() client id: f_00005-4-0 loss: 0.725692  [   32/  146]
train() client id: f_00005-4-1 loss: 0.666740  [   64/  146]
train() client id: f_00005-4-2 loss: 0.680233  [   96/  146]
train() client id: f_00005-4-3 loss: 0.448216  [  128/  146]
train() client id: f_00005-5-0 loss: 0.547948  [   32/  146]
train() client id: f_00005-5-1 loss: 0.671679  [   64/  146]
train() client id: f_00005-5-2 loss: 0.717870  [   96/  146]
train() client id: f_00005-5-3 loss: 0.331913  [  128/  146]
train() client id: f_00005-6-0 loss: 0.596104  [   32/  146]
train() client id: f_00005-6-1 loss: 0.532375  [   64/  146]
train() client id: f_00005-6-2 loss: 0.565436  [   96/  146]
train() client id: f_00005-6-3 loss: 0.639035  [  128/  146]
train() client id: f_00005-7-0 loss: 0.456124  [   32/  146]
train() client id: f_00005-7-1 loss: 0.713002  [   64/  146]
train() client id: f_00005-7-2 loss: 0.577240  [   96/  146]
train() client id: f_00005-7-3 loss: 0.687725  [  128/  146]
train() client id: f_00005-8-0 loss: 0.598911  [   32/  146]
train() client id: f_00005-8-1 loss: 0.523932  [   64/  146]
train() client id: f_00005-8-2 loss: 0.446277  [   96/  146]
train() client id: f_00005-8-3 loss: 0.776119  [  128/  146]
train() client id: f_00005-9-0 loss: 0.782801  [   32/  146]
train() client id: f_00005-9-1 loss: 0.664900  [   64/  146]
train() client id: f_00005-9-2 loss: 0.468359  [   96/  146]
train() client id: f_00005-9-3 loss: 0.533511  [  128/  146]
train() client id: f_00005-10-0 loss: 0.649094  [   32/  146]
train() client id: f_00005-10-1 loss: 0.452094  [   64/  146]
train() client id: f_00005-10-2 loss: 0.700559  [   96/  146]
train() client id: f_00005-10-3 loss: 0.517835  [  128/  146]
train() client id: f_00005-11-0 loss: 0.559938  [   32/  146]
train() client id: f_00005-11-1 loss: 0.332866  [   64/  146]
train() client id: f_00005-11-2 loss: 0.612681  [   96/  146]
train() client id: f_00005-11-3 loss: 0.580312  [  128/  146]
train() client id: f_00005-12-0 loss: 0.470683  [   32/  146]
train() client id: f_00005-12-1 loss: 0.628379  [   64/  146]
train() client id: f_00005-12-2 loss: 0.592338  [   96/  146]
train() client id: f_00005-12-3 loss: 0.680432  [  128/  146]
train() client id: f_00006-0-0 loss: 0.667464  [   32/   54]
train() client id: f_00006-1-0 loss: 0.616963  [   32/   54]
train() client id: f_00006-2-0 loss: 0.702470  [   32/   54]
train() client id: f_00006-3-0 loss: 0.686413  [   32/   54]
train() client id: f_00006-4-0 loss: 0.615502  [   32/   54]
train() client id: f_00006-5-0 loss: 0.627807  [   32/   54]
train() client id: f_00006-6-0 loss: 0.718370  [   32/   54]
train() client id: f_00006-7-0 loss: 0.714172  [   32/   54]
train() client id: f_00006-8-0 loss: 0.609362  [   32/   54]
train() client id: f_00006-9-0 loss: 0.657583  [   32/   54]
train() client id: f_00006-10-0 loss: 0.716245  [   32/   54]
train() client id: f_00006-11-0 loss: 0.710858  [   32/   54]
train() client id: f_00006-12-0 loss: 0.693406  [   32/   54]
train() client id: f_00007-0-0 loss: 0.638614  [   32/  179]
train() client id: f_00007-0-1 loss: 0.612370  [   64/  179]
train() client id: f_00007-0-2 loss: 0.510649  [   96/  179]
train() client id: f_00007-0-3 loss: 0.628599  [  128/  179]
train() client id: f_00007-0-4 loss: 0.758667  [  160/  179]
train() client id: f_00007-1-0 loss: 0.600086  [   32/  179]
train() client id: f_00007-1-1 loss: 0.727469  [   64/  179]
train() client id: f_00007-1-2 loss: 0.559843  [   96/  179]
train() client id: f_00007-1-3 loss: 0.599380  [  128/  179]
train() client id: f_00007-1-4 loss: 0.616926  [  160/  179]
train() client id: f_00007-2-0 loss: 0.608939  [   32/  179]
train() client id: f_00007-2-1 loss: 0.734517  [   64/  179]
train() client id: f_00007-2-2 loss: 0.603502  [   96/  179]
train() client id: f_00007-2-3 loss: 0.581707  [  128/  179]
train() client id: f_00007-2-4 loss: 0.667387  [  160/  179]
train() client id: f_00007-3-0 loss: 0.476942  [   32/  179]
train() client id: f_00007-3-1 loss: 0.779798  [   64/  179]
train() client id: f_00007-3-2 loss: 0.655731  [   96/  179]
train() client id: f_00007-3-3 loss: 0.530853  [  128/  179]
train() client id: f_00007-3-4 loss: 0.712495  [  160/  179]
train() client id: f_00007-4-0 loss: 0.574830  [   32/  179]
train() client id: f_00007-4-1 loss: 0.754365  [   64/  179]
train() client id: f_00007-4-2 loss: 0.524168  [   96/  179]
train() client id: f_00007-4-3 loss: 0.600045  [  128/  179]
train() client id: f_00007-4-4 loss: 0.657957  [  160/  179]
train() client id: f_00007-5-0 loss: 0.575117  [   32/  179]
train() client id: f_00007-5-1 loss: 0.569840  [   64/  179]
train() client id: f_00007-5-2 loss: 0.574516  [   96/  179]
train() client id: f_00007-5-3 loss: 0.487265  [  128/  179]
train() client id: f_00007-5-4 loss: 0.823400  [  160/  179]
train() client id: f_00007-6-0 loss: 0.475394  [   32/  179]
train() client id: f_00007-6-1 loss: 0.767218  [   64/  179]
train() client id: f_00007-6-2 loss: 0.577777  [   96/  179]
train() client id: f_00007-6-3 loss: 0.681849  [  128/  179]
train() client id: f_00007-6-4 loss: 0.557484  [  160/  179]
train() client id: f_00007-7-0 loss: 0.773286  [   32/  179]
train() client id: f_00007-7-1 loss: 0.565180  [   64/  179]
train() client id: f_00007-7-2 loss: 0.626489  [   96/  179]
train() client id: f_00007-7-3 loss: 0.528484  [  128/  179]
train() client id: f_00007-7-4 loss: 0.566434  [  160/  179]
train() client id: f_00007-8-0 loss: 0.536120  [   32/  179]
train() client id: f_00007-8-1 loss: 0.482397  [   64/  179]
train() client id: f_00007-8-2 loss: 0.478804  [   96/  179]
train() client id: f_00007-8-3 loss: 0.649153  [  128/  179]
train() client id: f_00007-8-4 loss: 0.697271  [  160/  179]
train() client id: f_00007-9-0 loss: 0.552201  [   32/  179]
train() client id: f_00007-9-1 loss: 0.722067  [   64/  179]
train() client id: f_00007-9-2 loss: 0.558704  [   96/  179]
train() client id: f_00007-9-3 loss: 0.552881  [  128/  179]
train() client id: f_00007-9-4 loss: 0.465587  [  160/  179]
train() client id: f_00007-10-0 loss: 0.578678  [   32/  179]
train() client id: f_00007-10-1 loss: 0.615004  [   64/  179]
train() client id: f_00007-10-2 loss: 0.595358  [   96/  179]
train() client id: f_00007-10-3 loss: 0.631408  [  128/  179]
train() client id: f_00007-10-4 loss: 0.528413  [  160/  179]
train() client id: f_00007-11-0 loss: 0.637869  [   32/  179]
train() client id: f_00007-11-1 loss: 0.466056  [   64/  179]
train() client id: f_00007-11-2 loss: 0.642186  [   96/  179]
train() client id: f_00007-11-3 loss: 0.579544  [  128/  179]
train() client id: f_00007-11-4 loss: 0.552937  [  160/  179]
train() client id: f_00007-12-0 loss: 0.464153  [   32/  179]
train() client id: f_00007-12-1 loss: 0.524962  [   64/  179]
train() client id: f_00007-12-2 loss: 0.651129  [   96/  179]
train() client id: f_00007-12-3 loss: 0.707121  [  128/  179]
train() client id: f_00007-12-4 loss: 0.691319  [  160/  179]
train() client id: f_00008-0-0 loss: 0.948973  [   32/  130]
train() client id: f_00008-0-1 loss: 0.853289  [   64/  130]
train() client id: f_00008-0-2 loss: 0.937101  [   96/  130]
train() client id: f_00008-0-3 loss: 1.030102  [  128/  130]
train() client id: f_00008-1-0 loss: 0.999113  [   32/  130]
train() client id: f_00008-1-1 loss: 0.945718  [   64/  130]
train() client id: f_00008-1-2 loss: 0.983857  [   96/  130]
train() client id: f_00008-1-3 loss: 0.799918  [  128/  130]
train() client id: f_00008-2-0 loss: 0.936978  [   32/  130]
train() client id: f_00008-2-1 loss: 0.911806  [   64/  130]
train() client id: f_00008-2-2 loss: 1.024662  [   96/  130]
train() client id: f_00008-2-3 loss: 0.889368  [  128/  130]
train() client id: f_00008-3-0 loss: 1.009978  [   32/  130]
train() client id: f_00008-3-1 loss: 0.896307  [   64/  130]
train() client id: f_00008-3-2 loss: 0.923891  [   96/  130]
train() client id: f_00008-3-3 loss: 0.896377  [  128/  130]
train() client id: f_00008-4-0 loss: 1.032885  [   32/  130]
train() client id: f_00008-4-1 loss: 0.941776  [   64/  130]
train() client id: f_00008-4-2 loss: 0.916695  [   96/  130]
train() client id: f_00008-4-3 loss: 0.871026  [  128/  130]
train() client id: f_00008-5-0 loss: 0.902054  [   32/  130]
train() client id: f_00008-5-1 loss: 0.928787  [   64/  130]
train() client id: f_00008-5-2 loss: 0.898053  [   96/  130]
train() client id: f_00008-5-3 loss: 1.015620  [  128/  130]
train() client id: f_00008-6-0 loss: 0.994578  [   32/  130]
train() client id: f_00008-6-1 loss: 1.041366  [   64/  130]
train() client id: f_00008-6-2 loss: 0.801995  [   96/  130]
train() client id: f_00008-6-3 loss: 0.899366  [  128/  130]
train() client id: f_00008-7-0 loss: 0.980368  [   32/  130]
train() client id: f_00008-7-1 loss: 0.974225  [   64/  130]
train() client id: f_00008-7-2 loss: 0.881937  [   96/  130]
train() client id: f_00008-7-3 loss: 0.904033  [  128/  130]
train() client id: f_00008-8-0 loss: 0.902626  [   32/  130]
train() client id: f_00008-8-1 loss: 0.997495  [   64/  130]
train() client id: f_00008-8-2 loss: 0.892721  [   96/  130]
train() client id: f_00008-8-3 loss: 0.981611  [  128/  130]
train() client id: f_00008-9-0 loss: 0.798843  [   32/  130]
train() client id: f_00008-9-1 loss: 1.068621  [   64/  130]
train() client id: f_00008-9-2 loss: 0.917695  [   96/  130]
train() client id: f_00008-9-3 loss: 0.955177  [  128/  130]
train() client id: f_00008-10-0 loss: 0.872023  [   32/  130]
train() client id: f_00008-10-1 loss: 0.958238  [   64/  130]
train() client id: f_00008-10-2 loss: 1.032385  [   96/  130]
train() client id: f_00008-10-3 loss: 0.919461  [  128/  130]
train() client id: f_00008-11-0 loss: 0.873708  [   32/  130]
train() client id: f_00008-11-1 loss: 0.955706  [   64/  130]
train() client id: f_00008-11-2 loss: 0.940244  [   96/  130]
train() client id: f_00008-11-3 loss: 1.006904  [  128/  130]
train() client id: f_00008-12-0 loss: 0.945115  [   32/  130]
train() client id: f_00008-12-1 loss: 0.947612  [   64/  130]
train() client id: f_00008-12-2 loss: 0.824632  [   96/  130]
train() client id: f_00008-12-3 loss: 1.035017  [  128/  130]
train() client id: f_00009-0-0 loss: 1.125609  [   32/  118]
train() client id: f_00009-0-1 loss: 1.203385  [   64/  118]
train() client id: f_00009-0-2 loss: 1.014390  [   96/  118]
train() client id: f_00009-1-0 loss: 1.030089  [   32/  118]
train() client id: f_00009-1-1 loss: 1.183099  [   64/  118]
train() client id: f_00009-1-2 loss: 0.896663  [   96/  118]
train() client id: f_00009-2-0 loss: 0.966543  [   32/  118]
train() client id: f_00009-2-1 loss: 1.094473  [   64/  118]
train() client id: f_00009-2-2 loss: 0.929772  [   96/  118]
train() client id: f_00009-3-0 loss: 0.972856  [   32/  118]
train() client id: f_00009-3-1 loss: 0.879351  [   64/  118]
train() client id: f_00009-3-2 loss: 1.009161  [   96/  118]
train() client id: f_00009-4-0 loss: 0.856405  [   32/  118]
train() client id: f_00009-4-1 loss: 1.032687  [   64/  118]
train() client id: f_00009-4-2 loss: 0.812872  [   96/  118]
train() client id: f_00009-5-0 loss: 0.935315  [   32/  118]
train() client id: f_00009-5-1 loss: 0.899458  [   64/  118]
train() client id: f_00009-5-2 loss: 0.817319  [   96/  118]
train() client id: f_00009-6-0 loss: 1.048584  [   32/  118]
train() client id: f_00009-6-1 loss: 0.750588  [   64/  118]
train() client id: f_00009-6-2 loss: 0.721421  [   96/  118]
train() client id: f_00009-7-0 loss: 0.717379  [   32/  118]
train() client id: f_00009-7-1 loss: 0.816694  [   64/  118]
train() client id: f_00009-7-2 loss: 0.879433  [   96/  118]
train() client id: f_00009-8-0 loss: 0.819931  [   32/  118]
train() client id: f_00009-8-1 loss: 0.850583  [   64/  118]
train() client id: f_00009-8-2 loss: 0.743405  [   96/  118]
train() client id: f_00009-9-0 loss: 0.838166  [   32/  118]
train() client id: f_00009-9-1 loss: 0.826560  [   64/  118]
train() client id: f_00009-9-2 loss: 0.639890  [   96/  118]
train() client id: f_00009-10-0 loss: 0.761640  [   32/  118]
train() client id: f_00009-10-1 loss: 0.677467  [   64/  118]
train() client id: f_00009-10-2 loss: 0.774554  [   96/  118]
train() client id: f_00009-11-0 loss: 0.758409  [   32/  118]
train() client id: f_00009-11-1 loss: 0.809351  [   64/  118]
train() client id: f_00009-11-2 loss: 0.693244  [   96/  118]
train() client id: f_00009-12-0 loss: 0.728075  [   32/  118]
train() client id: f_00009-12-1 loss: 0.882387  [   64/  118]
train() client id: f_00009-12-2 loss: 0.645567  [   96/  118]
At round 6 accuracy: 0.623342175066313
At round 6 training accuracy: 0.5720992622401073
At round 6 training loss: 0.8625868230678346
update_location
xs = -4.528292 -68.998411 40.045120 59.056472 -160.103519 -95.217951 12.784040 3.375741 -1.680116 -50.304393 
ys = 17.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 1.875078 -19.177652 52.845030 -0.998518 
xs mean: -26.55713084058713
ys mean: 8.371751218646876
dists_uav = 101.635829 122.485774 107.728156 118.304421 188.998843 139.146727 100.831283 101.878251 113.116842 111.944312 
uav_gains = -100.176191 -102.202427 -100.808277 -101.825199 -106.993359 -103.588489 -100.089900 -100.202058 -101.338275 -101.225131 
uav_gains_db_mean: -101.84493054355679
dists_bs = 232.032657 191.464821 276.421657 279.329577 166.318266 208.087905 255.411598 263.671890 212.041550 215.695324 
bs_gains = -105.802434 -103.465551 -107.931077 -108.058333 -101.753378 -104.477971 -106.969796 -107.356846 -104.706846 -104.914600 
bs_gains_db_mean: -105.54368321627479
Round 7
-------------------------------
ene_coms = [0.00634884 0.00735868 0.00652111 0.00681847 0.00679659 0.00773598
 0.00632602 0.00635572 0.00782657 0.0079106 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [10.06274424 21.0075917   9.91012946  3.55526831 24.21353039 11.66988644
  4.40650007 14.22610988 10.41543467  9.47501499]
obj_prev = 118.94221013583174
eta_min = 4.179995472076858e-10	eta_max = 0.9299868132452994
af = 25.177764423944467	bf = 1.8048979594927486	zeta = 27.695540866338916	eta = 0.909090909090909
af = 25.177764423944467	bf = 1.8048979594927486	zeta = 46.250547351908075	eta = 0.5443776531415634
af = 25.177764423944467	bf = 1.8048979594927486	zeta = 37.5657607848704	eta = 0.6702317189349938
af = 25.177764423944467	bf = 1.8048979594927486	zeta = 36.02323894469377	eta = 0.6989311667004656
af = 25.177764423944467	bf = 1.8048979594927486	zeta = 35.95074582304551	eta = 0.7003405311220209
af = 25.177764423944467	bf = 1.8048979594927486	zeta = 35.95057546196409	eta = 0.7003438498664001
eta = 0.7003438498664001
ene_coms = [0.00634884 0.00735868 0.00652111 0.00681847 0.00679659 0.00773598
 0.00632602 0.00635572 0.00782657 0.0079106 ]
ene_comp = [0.02939144 0.06181533 0.02892491 0.01003041 0.07137921 0.03405675
 0.01259633 0.0417545  0.0303245  0.02752531]
ene_total = [3.07537608 5.95227755 3.05005488 1.44981059 6.72686306 3.59617643
 1.62822867 4.13978323 3.2828191  3.04918587]
ti_comp = [0.28108675 0.27098841 0.27936413 0.27639051 0.27660927 0.26721536
 0.28131497 0.28101802 0.26630951 0.2654692 ]
ti_coms = [0.06348844 0.07358678 0.06521106 0.06818467 0.06796592 0.07735983
 0.06326022 0.06355716 0.07826568 0.07910599]
t_total = [29.64997063 29.64997063 29.64997063 29.64997063 29.64997063 29.64997063
 29.64997063 29.64997063 29.64997063 29.64997063]
ene_coms = [0.00634884 0.00735868 0.00652111 0.00681847 0.00679659 0.00773598
 0.00632602 0.00635572 0.00782657 0.0079106 ]
ene_comp = [2.00845337e-05 2.01032861e-04 1.93800386e-05 8.25638566e-07
 2.97071980e-04 3.45753521e-05 1.57843418e-06 5.76131489e-05
 2.45746630e-05 1.84947563e-05]
ene_total = [0.54803288 0.65049719 0.56279504 0.58678583 0.61039486 0.66864022
 0.54447673 0.55185352 0.67557428 0.68228184]
optimize_network iter = 0 obj = 6.081332397941541
eta = 0.7003438498664001
freqs = [5.22817911e+07 1.14055293e+08 5.17691852e+07 1.81453609e+07
 1.29025332e+08 6.37252769e+07 2.23883049e+07 7.42915030e+07
 5.69346903e+07 5.18427651e+07]
eta_min = 0.6735178145390945	eta_max = 0.7003438498663961
af = 0.048881201478251035	bf = 1.8048979594927486	zeta = 0.053769321626076144	eta = 0.909090909090909
af = 0.048881201478251035	bf = 1.8048979594927486	zeta = 19.905125191541583	eta = 0.002455709321487837
af = 0.048881201478251035	bf = 1.8048979594927486	zeta = 2.1038080417237746	eta = 0.02323463001795533
af = 0.048881201478251035	bf = 1.8048979594927486	zeta = 2.0361025768287564	eta = 0.024007239141352023
af = 0.048881201478251035	bf = 1.8048979594927486	zeta = 2.0360760707403065	eta = 0.024007551672898985
eta = 0.024007551672898985
ene_coms = [0.00634884 0.00735868 0.00652111 0.00681847 0.00679659 0.00773598
 0.00632602 0.00635572 0.00782657 0.0079106 ]
ene_comp = [2.10293071e-04 2.10489416e-03 2.02916727e-04 8.64476479e-06
 3.11046201e-03 3.62017713e-04 1.65268348e-05 6.03232627e-04
 2.57306514e-04 1.93647469e-04]
ene_total = [0.17328608 0.25001846 0.1776422  0.18036574 0.26173483 0.21394139
 0.16756405 0.18384874 0.21356818 0.21410639]
ti_comp = [0.31193406 0.30183572 0.31021144 0.30723782 0.30745658 0.29806267
 0.31216228 0.31186533 0.29715682 0.29631651]
ti_coms = [0.06348844 0.07358678 0.06521106 0.06818467 0.06796592 0.07735983
 0.06326022 0.06355716 0.07826568 0.07910599]
t_total = [29.64997063 29.64997063 29.64997063 29.64997063 29.64997063 29.64997063
 29.64997063 29.64997063 29.64997063 29.64997063]
ene_coms = [0.00634884 0.00735868 0.00652111 0.00681847 0.00679659 0.00773598
 0.00632602 0.00635572 0.00782657 0.0079106 ]
ene_comp = [2.22833073e-05 2.21406294e-04 2.14754883e-05 9.12955524e-07
 3.28541557e-04 3.79696869e-05 1.75151580e-06 6.39173120e-05
 2.69682187e-05 2.02828015e-05]
ene_total = [0.50317636 0.59865689 0.5167174  0.53857834 0.56272598 0.61396819
 0.49975243 0.50700731 0.62025346 0.62636206]
optimize_network iter = 1 obj = 5.587198417752405
eta = 0.6735178145390945
freqs = [5.22775969e+07 1.13627410e+08 5.17334860e+07 1.81134696e+07
 1.28808791e+08 6.33947280e+07 2.23883049e+07 7.42837465e+07
 5.66194332e+07 5.15387676e+07]
eta_min = 0.6735178145390943	eta_max = 0.6735178145390959
af = 0.04863563919486084	bf = 1.8048979594927486	zeta = 0.05349920311434692	eta = 0.9090909090909091
af = 0.04863563919486084	bf = 1.8048979594927486	zeta = 19.904867741102393	eta = 0.0024434042882099175
af = 0.04863563919486084	bf = 1.8048979594927486	zeta = 2.1025477230529144	eta = 0.02313176469746976
af = 0.04863563919486084	bf = 1.8048979594927486	zeta = 2.0351633741712196	eta = 0.02389765844457905
af = 0.04863563919486084	bf = 1.8048979594927486	zeta = 2.0351372248112645	eta = 0.02389796550420389
eta = 0.02389796550420389
ene_coms = [0.00634884 0.00735868 0.00652111 0.00681847 0.00679659 0.00773598
 0.00632602 0.00635572 0.00782657 0.0079106 ]
ene_comp = [2.10517271e-04 2.09169349e-03 2.02885556e-04 8.62497218e-06
 3.10383334e-03 3.58711333e-04 1.65471095e-05 6.03846544e-04
 2.54777073e-04 1.91617876e-04]
ene_total = [0.17327255 0.24964168 0.17762144 0.18034496 0.26153035 0.21383003
 0.16754577 0.18384432 0.21347738 0.21402874]
ti_comp = [0.31193406 0.30183572 0.31021144 0.30723782 0.30745658 0.29806267
 0.31216228 0.31186533 0.29715682 0.29631651]
ti_coms = [0.06348844 0.07358678 0.06521106 0.06818467 0.06796592 0.07735983
 0.06326022 0.06355716 0.07826568 0.07910599]
t_total = [29.64997063 29.64997063 29.64997063 29.64997063 29.64997063 29.64997063
 29.64997063 29.64997063 29.64997063 29.64997063]
ene_coms = [0.00634884 0.00735868 0.00652111 0.00681847 0.00679659 0.00773598
 0.00632602 0.00635572 0.00782657 0.0079106 ]
ene_comp = [2.22833073e-05 2.21406294e-04 2.14754883e-05 9.12955524e-07
 3.28541557e-04 3.79696869e-05 1.75151580e-06 6.39173120e-05
 2.69682187e-05 2.02828015e-05]
ene_total = [0.50317636 0.59865689 0.5167174  0.53857834 0.56272598 0.61396819
 0.49975243 0.50700731 0.62025346 0.62636206]
optimize_network iter = 2 obj = 5.587198417752402
eta = 0.6735178145390943
freqs = [5.22775969e+07 1.13627410e+08 5.17334860e+07 1.81134696e+07
 1.28808791e+08 6.33947280e+07 2.23883049e+07 7.42837465e+07
 5.66194332e+07 5.15387676e+07]
Done!
ene_coms = [0.00634884 0.00735868 0.00652111 0.00681847 0.00679659 0.00773598
 0.00632602 0.00635572 0.00782657 0.0079106 ]
ene_comp = [2.06610803e-05 2.05287894e-04 1.99120706e-05 8.46492271e-07
 3.04623699e-04 3.52054899e-05 1.62400527e-06 5.92641254e-05
 2.50049297e-05 1.88062115e-05]
ene_total = [0.0063695  0.00756397 0.00654102 0.00681931 0.00710122 0.00777119
 0.00632765 0.00641498 0.00785157 0.00792941]
At round 7 energy consumption: 0.07068980975288193
At round 7 eta: 0.6735178145390943
At round 7 a_n: 25.784781927469155
At round 7 local rounds: 12.942192929868527
At round 7 global rounds: 78.97760758697423
gradient difference: 0.35013866424560547
train() client id: f_00000-0-0 loss: 1.306570  [   32/  126]
train() client id: f_00000-0-1 loss: 1.192194  [   64/  126]
train() client id: f_00000-0-2 loss: 1.401384  [   96/  126]
train() client id: f_00000-1-0 loss: 1.295074  [   32/  126]
train() client id: f_00000-1-1 loss: 1.296535  [   64/  126]
train() client id: f_00000-1-2 loss: 1.051183  [   96/  126]
train() client id: f_00000-2-0 loss: 1.102257  [   32/  126]
train() client id: f_00000-2-1 loss: 0.990119  [   64/  126]
train() client id: f_00000-2-2 loss: 1.149817  [   96/  126]
train() client id: f_00000-3-0 loss: 1.013619  [   32/  126]
train() client id: f_00000-3-1 loss: 0.955116  [   64/  126]
train() client id: f_00000-3-2 loss: 1.064626  [   96/  126]
train() client id: f_00000-4-0 loss: 0.994224  [   32/  126]
train() client id: f_00000-4-1 loss: 1.010688  [   64/  126]
train() client id: f_00000-4-2 loss: 0.951935  [   96/  126]
train() client id: f_00000-5-0 loss: 0.940162  [   32/  126]
train() client id: f_00000-5-1 loss: 0.897086  [   64/  126]
train() client id: f_00000-5-2 loss: 0.937772  [   96/  126]
train() client id: f_00000-6-0 loss: 0.839724  [   32/  126]
train() client id: f_00000-6-1 loss: 0.972289  [   64/  126]
train() client id: f_00000-6-2 loss: 0.810590  [   96/  126]
train() client id: f_00000-7-0 loss: 0.777579  [   32/  126]
train() client id: f_00000-7-1 loss: 0.806364  [   64/  126]
train() client id: f_00000-7-2 loss: 0.916213  [   96/  126]
train() client id: f_00000-8-0 loss: 0.869422  [   32/  126]
train() client id: f_00000-8-1 loss: 0.849510  [   64/  126]
train() client id: f_00000-8-2 loss: 0.852399  [   96/  126]
train() client id: f_00000-9-0 loss: 0.751031  [   32/  126]
train() client id: f_00000-9-1 loss: 0.742759  [   64/  126]
train() client id: f_00000-9-2 loss: 0.828129  [   96/  126]
train() client id: f_00000-10-0 loss: 0.733285  [   32/  126]
train() client id: f_00000-10-1 loss: 0.823190  [   64/  126]
train() client id: f_00000-10-2 loss: 0.798538  [   96/  126]
train() client id: f_00000-11-0 loss: 0.734232  [   32/  126]
train() client id: f_00000-11-1 loss: 0.845332  [   64/  126]
train() client id: f_00000-11-2 loss: 0.740557  [   96/  126]
train() client id: f_00001-0-0 loss: 0.557729  [   32/  265]
train() client id: f_00001-0-1 loss: 0.536495  [   64/  265]
train() client id: f_00001-0-2 loss: 0.550851  [   96/  265]
train() client id: f_00001-0-3 loss: 0.549559  [  128/  265]
train() client id: f_00001-0-4 loss: 0.569927  [  160/  265]
train() client id: f_00001-0-5 loss: 0.708935  [  192/  265]
train() client id: f_00001-0-6 loss: 0.490333  [  224/  265]
train() client id: f_00001-0-7 loss: 0.501616  [  256/  265]
train() client id: f_00001-1-0 loss: 0.536547  [   32/  265]
train() client id: f_00001-1-1 loss: 0.697640  [   64/  265]
train() client id: f_00001-1-2 loss: 0.541780  [   96/  265]
train() client id: f_00001-1-3 loss: 0.511341  [  128/  265]
train() client id: f_00001-1-4 loss: 0.544602  [  160/  265]
train() client id: f_00001-1-5 loss: 0.491807  [  192/  265]
train() client id: f_00001-1-6 loss: 0.477501  [  224/  265]
train() client id: f_00001-1-7 loss: 0.528557  [  256/  265]
train() client id: f_00001-2-0 loss: 0.531201  [   32/  265]
train() client id: f_00001-2-1 loss: 0.468301  [   64/  265]
train() client id: f_00001-2-2 loss: 0.514324  [   96/  265]
train() client id: f_00001-2-3 loss: 0.530486  [  128/  265]
train() client id: f_00001-2-4 loss: 0.511163  [  160/  265]
train() client id: f_00001-2-5 loss: 0.727924  [  192/  265]
train() client id: f_00001-2-6 loss: 0.486906  [  224/  265]
train() client id: f_00001-2-7 loss: 0.443190  [  256/  265]
train() client id: f_00001-3-0 loss: 0.516331  [   32/  265]
train() client id: f_00001-3-1 loss: 0.502705  [   64/  265]
train() client id: f_00001-3-2 loss: 0.473425  [   96/  265]
train() client id: f_00001-3-3 loss: 0.487750  [  128/  265]
train() client id: f_00001-3-4 loss: 0.497636  [  160/  265]
train() client id: f_00001-3-5 loss: 0.551824  [  192/  265]
train() client id: f_00001-3-6 loss: 0.598729  [  224/  265]
train() client id: f_00001-3-7 loss: 0.511340  [  256/  265]
train() client id: f_00001-4-0 loss: 0.516465  [   32/  265]
train() client id: f_00001-4-1 loss: 0.515590  [   64/  265]
train() client id: f_00001-4-2 loss: 0.499389  [   96/  265]
train() client id: f_00001-4-3 loss: 0.464961  [  128/  265]
train() client id: f_00001-4-4 loss: 0.464602  [  160/  265]
train() client id: f_00001-4-5 loss: 0.471362  [  192/  265]
train() client id: f_00001-4-6 loss: 0.545604  [  224/  265]
train() client id: f_00001-4-7 loss: 0.536697  [  256/  265]
train() client id: f_00001-5-0 loss: 0.452094  [   32/  265]
train() client id: f_00001-5-1 loss: 0.555549  [   64/  265]
train() client id: f_00001-5-2 loss: 0.593740  [   96/  265]
train() client id: f_00001-5-3 loss: 0.473725  [  128/  265]
train() client id: f_00001-5-4 loss: 0.518464  [  160/  265]
train() client id: f_00001-5-5 loss: 0.453262  [  192/  265]
train() client id: f_00001-5-6 loss: 0.502101  [  224/  265]
train() client id: f_00001-5-7 loss: 0.488301  [  256/  265]
train() client id: f_00001-6-0 loss: 0.451366  [   32/  265]
train() client id: f_00001-6-1 loss: 0.538425  [   64/  265]
train() client id: f_00001-6-2 loss: 0.485811  [   96/  265]
train() client id: f_00001-6-3 loss: 0.505232  [  128/  265]
train() client id: f_00001-6-4 loss: 0.619499  [  160/  265]
train() client id: f_00001-6-5 loss: 0.500173  [  192/  265]
train() client id: f_00001-6-6 loss: 0.421659  [  224/  265]
train() client id: f_00001-6-7 loss: 0.432166  [  256/  265]
train() client id: f_00001-7-0 loss: 0.562912  [   32/  265]
train() client id: f_00001-7-1 loss: 0.608940  [   64/  265]
train() client id: f_00001-7-2 loss: 0.520656  [   96/  265]
train() client id: f_00001-7-3 loss: 0.477396  [  128/  265]
train() client id: f_00001-7-4 loss: 0.449566  [  160/  265]
train() client id: f_00001-7-5 loss: 0.381943  [  192/  265]
train() client id: f_00001-7-6 loss: 0.490126  [  224/  265]
train() client id: f_00001-7-7 loss: 0.471422  [  256/  265]
train() client id: f_00001-8-0 loss: 0.547304  [   32/  265]
train() client id: f_00001-8-1 loss: 0.657565  [   64/  265]
train() client id: f_00001-8-2 loss: 0.470452  [   96/  265]
train() client id: f_00001-8-3 loss: 0.415245  [  128/  265]
train() client id: f_00001-8-4 loss: 0.538784  [  160/  265]
train() client id: f_00001-8-5 loss: 0.445841  [  192/  265]
train() client id: f_00001-8-6 loss: 0.446528  [  224/  265]
train() client id: f_00001-8-7 loss: 0.420983  [  256/  265]
train() client id: f_00001-9-0 loss: 0.686634  [   32/  265]
train() client id: f_00001-9-1 loss: 0.430950  [   64/  265]
train() client id: f_00001-9-2 loss: 0.465635  [   96/  265]
train() client id: f_00001-9-3 loss: 0.542678  [  128/  265]
train() client id: f_00001-9-4 loss: 0.452765  [  160/  265]
train() client id: f_00001-9-5 loss: 0.490025  [  192/  265]
train() client id: f_00001-9-6 loss: 0.423755  [  224/  265]
train() client id: f_00001-9-7 loss: 0.445716  [  256/  265]
train() client id: f_00001-10-0 loss: 0.450334  [   32/  265]
train() client id: f_00001-10-1 loss: 0.484805  [   64/  265]
train() client id: f_00001-10-2 loss: 0.503744  [   96/  265]
train() client id: f_00001-10-3 loss: 0.642505  [  128/  265]
train() client id: f_00001-10-4 loss: 0.418524  [  160/  265]
train() client id: f_00001-10-5 loss: 0.467536  [  192/  265]
train() client id: f_00001-10-6 loss: 0.476401  [  224/  265]
train() client id: f_00001-10-7 loss: 0.478678  [  256/  265]
train() client id: f_00001-11-0 loss: 0.413724  [   32/  265]
train() client id: f_00001-11-1 loss: 0.423996  [   64/  265]
train() client id: f_00001-11-2 loss: 0.481094  [   96/  265]
train() client id: f_00001-11-3 loss: 0.488176  [  128/  265]
train() client id: f_00001-11-4 loss: 0.411519  [  160/  265]
train() client id: f_00001-11-5 loss: 0.462649  [  192/  265]
train() client id: f_00001-11-6 loss: 0.480085  [  224/  265]
train() client id: f_00001-11-7 loss: 0.664305  [  256/  265]
train() client id: f_00002-0-0 loss: 1.099277  [   32/  124]
train() client id: f_00002-0-1 loss: 1.060104  [   64/  124]
train() client id: f_00002-0-2 loss: 1.011486  [   96/  124]
train() client id: f_00002-1-0 loss: 0.944796  [   32/  124]
train() client id: f_00002-1-1 loss: 1.079788  [   64/  124]
train() client id: f_00002-1-2 loss: 0.977025  [   96/  124]
train() client id: f_00002-2-0 loss: 1.160615  [   32/  124]
train() client id: f_00002-2-1 loss: 0.789111  [   64/  124]
train() client id: f_00002-2-2 loss: 0.922040  [   96/  124]
train() client id: f_00002-3-0 loss: 0.971860  [   32/  124]
train() client id: f_00002-3-1 loss: 0.834449  [   64/  124]
train() client id: f_00002-3-2 loss: 0.971951  [   96/  124]
train() client id: f_00002-4-0 loss: 0.870097  [   32/  124]
train() client id: f_00002-4-1 loss: 0.938230  [   64/  124]
train() client id: f_00002-4-2 loss: 0.909883  [   96/  124]
train() client id: f_00002-5-0 loss: 0.698115  [   32/  124]
train() client id: f_00002-5-1 loss: 0.986520  [   64/  124]
train() client id: f_00002-5-2 loss: 0.948228  [   96/  124]
train() client id: f_00002-6-0 loss: 0.860093  [   32/  124]
train() client id: f_00002-6-1 loss: 0.909128  [   64/  124]
train() client id: f_00002-6-2 loss: 0.710046  [   96/  124]
train() client id: f_00002-7-0 loss: 0.894178  [   32/  124]
train() client id: f_00002-7-1 loss: 0.817279  [   64/  124]
train() client id: f_00002-7-2 loss: 0.899991  [   96/  124]
train() client id: f_00002-8-0 loss: 0.804518  [   32/  124]
train() client id: f_00002-8-1 loss: 0.836295  [   64/  124]
train() client id: f_00002-8-2 loss: 0.808858  [   96/  124]
train() client id: f_00002-9-0 loss: 0.739033  [   32/  124]
train() client id: f_00002-9-1 loss: 0.873028  [   64/  124]
train() client id: f_00002-9-2 loss: 0.903575  [   96/  124]
train() client id: f_00002-10-0 loss: 0.686322  [   32/  124]
train() client id: f_00002-10-1 loss: 0.824599  [   64/  124]
train() client id: f_00002-10-2 loss: 0.864027  [   96/  124]
train() client id: f_00002-11-0 loss: 0.663708  [   32/  124]
train() client id: f_00002-11-1 loss: 0.847585  [   64/  124]
train() client id: f_00002-11-2 loss: 0.845811  [   96/  124]
train() client id: f_00003-0-0 loss: 1.022726  [   32/   43]
train() client id: f_00003-1-0 loss: 1.005522  [   32/   43]
train() client id: f_00003-2-0 loss: 1.054096  [   32/   43]
train() client id: f_00003-3-0 loss: 1.065504  [   32/   43]
train() client id: f_00003-4-0 loss: 1.047779  [   32/   43]
train() client id: f_00003-5-0 loss: 0.979672  [   32/   43]
train() client id: f_00003-6-0 loss: 1.072222  [   32/   43]
train() client id: f_00003-7-0 loss: 1.022278  [   32/   43]
train() client id: f_00003-8-0 loss: 0.942841  [   32/   43]
train() client id: f_00003-9-0 loss: 1.015970  [   32/   43]
train() client id: f_00003-10-0 loss: 0.971384  [   32/   43]
train() client id: f_00003-11-0 loss: 1.024731  [   32/   43]
train() client id: f_00004-0-0 loss: 0.714452  [   32/  306]
train() client id: f_00004-0-1 loss: 0.862482  [   64/  306]
train() client id: f_00004-0-2 loss: 0.832858  [   96/  306]
train() client id: f_00004-0-3 loss: 1.005367  [  128/  306]
train() client id: f_00004-0-4 loss: 0.926628  [  160/  306]
train() client id: f_00004-0-5 loss: 0.963091  [  192/  306]
train() client id: f_00004-0-6 loss: 0.828795  [  224/  306]
train() client id: f_00004-0-7 loss: 0.795550  [  256/  306]
train() client id: f_00004-0-8 loss: 0.887681  [  288/  306]
train() client id: f_00004-1-0 loss: 0.849511  [   32/  306]
train() client id: f_00004-1-1 loss: 0.845452  [   64/  306]
train() client id: f_00004-1-2 loss: 0.837451  [   96/  306]
train() client id: f_00004-1-3 loss: 0.791410  [  128/  306]
train() client id: f_00004-1-4 loss: 0.865384  [  160/  306]
train() client id: f_00004-1-5 loss: 0.899434  [  192/  306]
train() client id: f_00004-1-6 loss: 0.972048  [  224/  306]
train() client id: f_00004-1-7 loss: 0.871211  [  256/  306]
train() client id: f_00004-1-8 loss: 0.868256  [  288/  306]
train() client id: f_00004-2-0 loss: 0.788968  [   32/  306]
train() client id: f_00004-2-1 loss: 0.921763  [   64/  306]
train() client id: f_00004-2-2 loss: 0.929188  [   96/  306]
train() client id: f_00004-2-3 loss: 0.934647  [  128/  306]
train() client id: f_00004-2-4 loss: 0.885853  [  160/  306]
train() client id: f_00004-2-5 loss: 0.876416  [  192/  306]
train() client id: f_00004-2-6 loss: 0.837710  [  224/  306]
train() client id: f_00004-2-7 loss: 0.836459  [  256/  306]
train() client id: f_00004-2-8 loss: 0.748203  [  288/  306]
train() client id: f_00004-3-0 loss: 0.894043  [   32/  306]
train() client id: f_00004-3-1 loss: 0.908637  [   64/  306]
train() client id: f_00004-3-2 loss: 0.864725  [   96/  306]
train() client id: f_00004-3-3 loss: 0.975635  [  128/  306]
train() client id: f_00004-3-4 loss: 0.822622  [  160/  306]
train() client id: f_00004-3-5 loss: 0.874154  [  192/  306]
train() client id: f_00004-3-6 loss: 0.751585  [  224/  306]
train() client id: f_00004-3-7 loss: 0.895909  [  256/  306]
train() client id: f_00004-3-8 loss: 0.801974  [  288/  306]
train() client id: f_00004-4-0 loss: 1.001607  [   32/  306]
train() client id: f_00004-4-1 loss: 0.891516  [   64/  306]
train() client id: f_00004-4-2 loss: 0.847667  [   96/  306]
train() client id: f_00004-4-3 loss: 0.870337  [  128/  306]
train() client id: f_00004-4-4 loss: 0.853907  [  160/  306]
train() client id: f_00004-4-5 loss: 0.864385  [  192/  306]
train() client id: f_00004-4-6 loss: 0.866515  [  224/  306]
train() client id: f_00004-4-7 loss: 0.899187  [  256/  306]
train() client id: f_00004-4-8 loss: 0.747581  [  288/  306]
train() client id: f_00004-5-0 loss: 0.765045  [   32/  306]
train() client id: f_00004-5-1 loss: 0.784444  [   64/  306]
train() client id: f_00004-5-2 loss: 0.814361  [   96/  306]
train() client id: f_00004-5-3 loss: 1.068007  [  128/  306]
train() client id: f_00004-5-4 loss: 0.878255  [  160/  306]
train() client id: f_00004-5-5 loss: 0.902447  [  192/  306]
train() client id: f_00004-5-6 loss: 0.756853  [  224/  306]
train() client id: f_00004-5-7 loss: 0.932555  [  256/  306]
train() client id: f_00004-5-8 loss: 0.883163  [  288/  306]
train() client id: f_00004-6-0 loss: 0.813066  [   32/  306]
train() client id: f_00004-6-1 loss: 0.871693  [   64/  306]
train() client id: f_00004-6-2 loss: 0.776234  [   96/  306]
train() client id: f_00004-6-3 loss: 0.915314  [  128/  306]
train() client id: f_00004-6-4 loss: 0.904060  [  160/  306]
train() client id: f_00004-6-5 loss: 0.896489  [  192/  306]
train() client id: f_00004-6-6 loss: 0.854449  [  224/  306]
train() client id: f_00004-6-7 loss: 0.799917  [  256/  306]
train() client id: f_00004-6-8 loss: 0.914409  [  288/  306]
train() client id: f_00004-7-0 loss: 0.872210  [   32/  306]
train() client id: f_00004-7-1 loss: 0.894964  [   64/  306]
train() client id: f_00004-7-2 loss: 0.830211  [   96/  306]
train() client id: f_00004-7-3 loss: 0.838828  [  128/  306]
train() client id: f_00004-7-4 loss: 0.857565  [  160/  306]
train() client id: f_00004-7-5 loss: 0.884752  [  192/  306]
train() client id: f_00004-7-6 loss: 0.854134  [  224/  306]
train() client id: f_00004-7-7 loss: 0.784686  [  256/  306]
train() client id: f_00004-7-8 loss: 0.877390  [  288/  306]
train() client id: f_00004-8-0 loss: 0.886762  [   32/  306]
train() client id: f_00004-8-1 loss: 0.845982  [   64/  306]
train() client id: f_00004-8-2 loss: 0.839925  [   96/  306]
train() client id: f_00004-8-3 loss: 0.902218  [  128/  306]
train() client id: f_00004-8-4 loss: 0.896984  [  160/  306]
train() client id: f_00004-8-5 loss: 0.790839  [  192/  306]
train() client id: f_00004-8-6 loss: 0.903821  [  224/  306]
train() client id: f_00004-8-7 loss: 0.866706  [  256/  306]
train() client id: f_00004-8-8 loss: 0.866611  [  288/  306]
train() client id: f_00004-9-0 loss: 0.833910  [   32/  306]
train() client id: f_00004-9-1 loss: 0.908706  [   64/  306]
train() client id: f_00004-9-2 loss: 0.946723  [   96/  306]
train() client id: f_00004-9-3 loss: 0.750184  [  128/  306]
train() client id: f_00004-9-4 loss: 0.885401  [  160/  306]
train() client id: f_00004-9-5 loss: 0.842266  [  192/  306]
train() client id: f_00004-9-6 loss: 0.863391  [  224/  306]
train() client id: f_00004-9-7 loss: 0.994043  [  256/  306]
train() client id: f_00004-9-8 loss: 0.808696  [  288/  306]
train() client id: f_00004-10-0 loss: 0.907763  [   32/  306]
train() client id: f_00004-10-1 loss: 0.908256  [   64/  306]
train() client id: f_00004-10-2 loss: 0.793366  [   96/  306]
train() client id: f_00004-10-3 loss: 0.845800  [  128/  306]
train() client id: f_00004-10-4 loss: 0.801166  [  160/  306]
train() client id: f_00004-10-5 loss: 0.793219  [  192/  306]
train() client id: f_00004-10-6 loss: 0.823440  [  224/  306]
train() client id: f_00004-10-7 loss: 0.939071  [  256/  306]
train() client id: f_00004-10-8 loss: 0.935392  [  288/  306]
train() client id: f_00004-11-0 loss: 0.881464  [   32/  306]
train() client id: f_00004-11-1 loss: 0.834010  [   64/  306]
train() client id: f_00004-11-2 loss: 0.856443  [   96/  306]
train() client id: f_00004-11-3 loss: 0.836306  [  128/  306]
train() client id: f_00004-11-4 loss: 1.015958  [  160/  306]
train() client id: f_00004-11-5 loss: 0.958667  [  192/  306]
train() client id: f_00004-11-6 loss: 0.811073  [  224/  306]
train() client id: f_00004-11-7 loss: 0.819668  [  256/  306]
train() client id: f_00004-11-8 loss: 0.861180  [  288/  306]
train() client id: f_00005-0-0 loss: 0.581678  [   32/  146]
train() client id: f_00005-0-1 loss: 0.779344  [   64/  146]
train() client id: f_00005-0-2 loss: 0.618417  [   96/  146]
train() client id: f_00005-0-3 loss: 0.673633  [  128/  146]
train() client id: f_00005-1-0 loss: 0.733550  [   32/  146]
train() client id: f_00005-1-1 loss: 0.558063  [   64/  146]
train() client id: f_00005-1-2 loss: 0.626146  [   96/  146]
train() client id: f_00005-1-3 loss: 0.661579  [  128/  146]
train() client id: f_00005-2-0 loss: 0.753939  [   32/  146]
train() client id: f_00005-2-1 loss: 0.701896  [   64/  146]
train() client id: f_00005-2-2 loss: 0.486030  [   96/  146]
train() client id: f_00005-2-3 loss: 0.592550  [  128/  146]
train() client id: f_00005-3-0 loss: 0.673542  [   32/  146]
train() client id: f_00005-3-1 loss: 0.758475  [   64/  146]
train() client id: f_00005-3-2 loss: 0.670629  [   96/  146]
train() client id: f_00005-3-3 loss: 0.579037  [  128/  146]
train() client id: f_00005-4-0 loss: 0.539986  [   32/  146]
train() client id: f_00005-4-1 loss: 0.638466  [   64/  146]
train() client id: f_00005-4-2 loss: 0.539727  [   96/  146]
train() client id: f_00005-4-3 loss: 0.739156  [  128/  146]
train() client id: f_00005-5-0 loss: 0.415958  [   32/  146]
train() client id: f_00005-5-1 loss: 0.636315  [   64/  146]
train() client id: f_00005-5-2 loss: 0.656882  [   96/  146]
train() client id: f_00005-5-3 loss: 0.609452  [  128/  146]
train() client id: f_00005-6-0 loss: 0.649530  [   32/  146]
train() client id: f_00005-6-1 loss: 0.472454  [   64/  146]
train() client id: f_00005-6-2 loss: 0.620068  [   96/  146]
train() client id: f_00005-6-3 loss: 0.641783  [  128/  146]
train() client id: f_00005-7-0 loss: 0.831697  [   32/  146]
train() client id: f_00005-7-1 loss: 0.420461  [   64/  146]
train() client id: f_00005-7-2 loss: 0.483428  [   96/  146]
train() client id: f_00005-7-3 loss: 0.685550  [  128/  146]
train() client id: f_00005-8-0 loss: 0.572825  [   32/  146]
train() client id: f_00005-8-1 loss: 0.494431  [   64/  146]
train() client id: f_00005-8-2 loss: 0.733284  [   96/  146]
train() client id: f_00005-8-3 loss: 0.753987  [  128/  146]
train() client id: f_00005-9-0 loss: 0.689050  [   32/  146]
train() client id: f_00005-9-1 loss: 0.473725  [   64/  146]
train() client id: f_00005-9-2 loss: 1.015433  [   96/  146]
train() client id: f_00005-9-3 loss: 0.464771  [  128/  146]
train() client id: f_00005-10-0 loss: 0.624001  [   32/  146]
train() client id: f_00005-10-1 loss: 0.747909  [   64/  146]
train() client id: f_00005-10-2 loss: 0.651966  [   96/  146]
train() client id: f_00005-10-3 loss: 0.613945  [  128/  146]
train() client id: f_00005-11-0 loss: 0.617677  [   32/  146]
train() client id: f_00005-11-1 loss: 0.750098  [   64/  146]
train() client id: f_00005-11-2 loss: 0.495238  [   96/  146]
train() client id: f_00005-11-3 loss: 0.697914  [  128/  146]
train() client id: f_00006-0-0 loss: 0.590761  [   32/   54]
train() client id: f_00006-1-0 loss: 0.628238  [   32/   54]
train() client id: f_00006-2-0 loss: 0.681947  [   32/   54]
train() client id: f_00006-3-0 loss: 0.671928  [   32/   54]
train() client id: f_00006-4-0 loss: 0.628688  [   32/   54]
train() client id: f_00006-5-0 loss: 0.621434  [   32/   54]
train() client id: f_00006-6-0 loss: 0.672705  [   32/   54]
train() client id: f_00006-7-0 loss: 0.632809  [   32/   54]
train() client id: f_00006-8-0 loss: 0.623893  [   32/   54]
train() client id: f_00006-9-0 loss: 0.592095  [   32/   54]
train() client id: f_00006-10-0 loss: 0.623583  [   32/   54]
train() client id: f_00006-11-0 loss: 0.610252  [   32/   54]
train() client id: f_00007-0-0 loss: 0.423208  [   32/  179]
train() client id: f_00007-0-1 loss: 0.562703  [   64/  179]
train() client id: f_00007-0-2 loss: 0.574172  [   96/  179]
train() client id: f_00007-0-3 loss: 0.535237  [  128/  179]
train() client id: f_00007-0-4 loss: 0.598712  [  160/  179]
train() client id: f_00007-1-0 loss: 0.587089  [   32/  179]
train() client id: f_00007-1-1 loss: 0.495223  [   64/  179]
train() client id: f_00007-1-2 loss: 0.476068  [   96/  179]
train() client id: f_00007-1-3 loss: 0.541041  [  128/  179]
train() client id: f_00007-1-4 loss: 0.515075  [  160/  179]
train() client id: f_00007-2-0 loss: 0.542113  [   32/  179]
train() client id: f_00007-2-1 loss: 0.373820  [   64/  179]
train() client id: f_00007-2-2 loss: 0.503465  [   96/  179]
train() client id: f_00007-2-3 loss: 0.678274  [  128/  179]
train() client id: f_00007-2-4 loss: 0.388622  [  160/  179]
train() client id: f_00007-3-0 loss: 0.365822  [   32/  179]
train() client id: f_00007-3-1 loss: 0.474896  [   64/  179]
train() client id: f_00007-3-2 loss: 0.544151  [   96/  179]
train() client id: f_00007-3-3 loss: 0.487429  [  128/  179]
train() client id: f_00007-3-4 loss: 0.501216  [  160/  179]
train() client id: f_00007-4-0 loss: 0.470078  [   32/  179]
train() client id: f_00007-4-1 loss: 0.555969  [   64/  179]
train() client id: f_00007-4-2 loss: 0.423223  [   96/  179]
train() client id: f_00007-4-3 loss: 0.499375  [  128/  179]
train() client id: f_00007-4-4 loss: 0.439627  [  160/  179]
train() client id: f_00007-5-0 loss: 0.494139  [   32/  179]
train() client id: f_00007-5-1 loss: 0.359074  [   64/  179]
train() client id: f_00007-5-2 loss: 0.489204  [   96/  179]
train() client id: f_00007-5-3 loss: 0.608994  [  128/  179]
train() client id: f_00007-5-4 loss: 0.519013  [  160/  179]
train() client id: f_00007-6-0 loss: 0.348152  [   32/  179]
train() client id: f_00007-6-1 loss: 0.530752  [   64/  179]
train() client id: f_00007-6-2 loss: 0.364731  [   96/  179]
train() client id: f_00007-6-3 loss: 0.424225  [  128/  179]
train() client id: f_00007-6-4 loss: 0.420482  [  160/  179]
train() client id: f_00007-7-0 loss: 0.638246  [   32/  179]
train() client id: f_00007-7-1 loss: 0.589306  [   64/  179]
train() client id: f_00007-7-2 loss: 0.350220  [   96/  179]
train() client id: f_00007-7-3 loss: 0.352532  [  128/  179]
train() client id: f_00007-7-4 loss: 0.431240  [  160/  179]
train() client id: f_00007-8-0 loss: 0.420960  [   32/  179]
train() client id: f_00007-8-1 loss: 0.461276  [   64/  179]
train() client id: f_00007-8-2 loss: 0.587084  [   96/  179]
train() client id: f_00007-8-3 loss: 0.441946  [  128/  179]
train() client id: f_00007-8-4 loss: 0.431675  [  160/  179]
train() client id: f_00007-9-0 loss: 0.410730  [   32/  179]
train() client id: f_00007-9-1 loss: 0.402700  [   64/  179]
train() client id: f_00007-9-2 loss: 0.430067  [   96/  179]
train() client id: f_00007-9-3 loss: 0.498685  [  128/  179]
train() client id: f_00007-9-4 loss: 0.441553  [  160/  179]
train() client id: f_00007-10-0 loss: 0.513782  [   32/  179]
train() client id: f_00007-10-1 loss: 0.338654  [   64/  179]
train() client id: f_00007-10-2 loss: 0.536503  [   96/  179]
train() client id: f_00007-10-3 loss: 0.470869  [  128/  179]
train() client id: f_00007-10-4 loss: 0.476505  [  160/  179]
train() client id: f_00007-11-0 loss: 0.406702  [   32/  179]
train() client id: f_00007-11-1 loss: 0.554454  [   64/  179]
train() client id: f_00007-11-2 loss: 0.337744  [   96/  179]
train() client id: f_00007-11-3 loss: 0.417065  [  128/  179]
train() client id: f_00007-11-4 loss: 0.546212  [  160/  179]
train() client id: f_00008-0-0 loss: 0.718505  [   32/  130]
train() client id: f_00008-0-1 loss: 0.745981  [   64/  130]
train() client id: f_00008-0-2 loss: 0.780453  [   96/  130]
train() client id: f_00008-0-3 loss: 0.732580  [  128/  130]
train() client id: f_00008-1-0 loss: 0.926264  [   32/  130]
train() client id: f_00008-1-1 loss: 0.711798  [   64/  130]
train() client id: f_00008-1-2 loss: 0.692517  [   96/  130]
train() client id: f_00008-1-3 loss: 0.649190  [  128/  130]
train() client id: f_00008-2-0 loss: 0.817317  [   32/  130]
train() client id: f_00008-2-1 loss: 0.654118  [   64/  130]
train() client id: f_00008-2-2 loss: 0.746924  [   96/  130]
train() client id: f_00008-2-3 loss: 0.752921  [  128/  130]
train() client id: f_00008-3-0 loss: 0.669794  [   32/  130]
train() client id: f_00008-3-1 loss: 0.804490  [   64/  130]
train() client id: f_00008-3-2 loss: 0.718467  [   96/  130]
train() client id: f_00008-3-3 loss: 0.776122  [  128/  130]
train() client id: f_00008-4-0 loss: 0.757937  [   32/  130]
train() client id: f_00008-4-1 loss: 0.690993  [   64/  130]
train() client id: f_00008-4-2 loss: 0.764100  [   96/  130]
train() client id: f_00008-4-3 loss: 0.747629  [  128/  130]
train() client id: f_00008-5-0 loss: 0.657622  [   32/  130]
train() client id: f_00008-5-1 loss: 0.749494  [   64/  130]
train() client id: f_00008-5-2 loss: 0.773846  [   96/  130]
train() client id: f_00008-5-3 loss: 0.751882  [  128/  130]
train() client id: f_00008-6-0 loss: 0.718315  [   32/  130]
train() client id: f_00008-6-1 loss: 0.763472  [   64/  130]
train() client id: f_00008-6-2 loss: 0.762574  [   96/  130]
train() client id: f_00008-6-3 loss: 0.724333  [  128/  130]
train() client id: f_00008-7-0 loss: 0.737530  [   32/  130]
train() client id: f_00008-7-1 loss: 0.784705  [   64/  130]
train() client id: f_00008-7-2 loss: 0.710050  [   96/  130]
train() client id: f_00008-7-3 loss: 0.729827  [  128/  130]
train() client id: f_00008-8-0 loss: 0.765955  [   32/  130]
train() client id: f_00008-8-1 loss: 0.760892  [   64/  130]
train() client id: f_00008-8-2 loss: 0.727562  [   96/  130]
train() client id: f_00008-8-3 loss: 0.706086  [  128/  130]
train() client id: f_00008-9-0 loss: 0.695727  [   32/  130]
train() client id: f_00008-9-1 loss: 0.694688  [   64/  130]
train() client id: f_00008-9-2 loss: 0.695383  [   96/  130]
train() client id: f_00008-9-3 loss: 0.841819  [  128/  130]
train() client id: f_00008-10-0 loss: 0.753664  [   32/  130]
train() client id: f_00008-10-1 loss: 0.847842  [   64/  130]
train() client id: f_00008-10-2 loss: 0.700349  [   96/  130]
train() client id: f_00008-10-3 loss: 0.637863  [  128/  130]
train() client id: f_00008-11-0 loss: 0.659709  [   32/  130]
train() client id: f_00008-11-1 loss: 0.850599  [   64/  130]
train() client id: f_00008-11-2 loss: 0.725066  [   96/  130]
train() client id: f_00008-11-3 loss: 0.727220  [  128/  130]
train() client id: f_00009-0-0 loss: 1.206908  [   32/  118]
train() client id: f_00009-0-1 loss: 1.354836  [   64/  118]
train() client id: f_00009-0-2 loss: 1.315431  [   96/  118]
train() client id: f_00009-1-0 loss: 1.146599  [   32/  118]
train() client id: f_00009-1-1 loss: 1.340004  [   64/  118]
train() client id: f_00009-1-2 loss: 1.179898  [   96/  118]
train() client id: f_00009-2-0 loss: 1.152871  [   32/  118]
train() client id: f_00009-2-1 loss: 1.183212  [   64/  118]
train() client id: f_00009-2-2 loss: 1.176059  [   96/  118]
train() client id: f_00009-3-0 loss: 1.063733  [   32/  118]
train() client id: f_00009-3-1 loss: 1.013077  [   64/  118]
train() client id: f_00009-3-2 loss: 1.310531  [   96/  118]
train() client id: f_00009-4-0 loss: 1.140350  [   32/  118]
train() client id: f_00009-4-1 loss: 1.139725  [   64/  118]
train() client id: f_00009-4-2 loss: 1.052894  [   96/  118]
train() client id: f_00009-5-0 loss: 1.117305  [   32/  118]
train() client id: f_00009-5-1 loss: 1.069105  [   64/  118]
train() client id: f_00009-5-2 loss: 1.020214  [   96/  118]
train() client id: f_00009-6-0 loss: 1.074972  [   32/  118]
train() client id: f_00009-6-1 loss: 0.945473  [   64/  118]
train() client id: f_00009-6-2 loss: 1.168276  [   96/  118]
train() client id: f_00009-7-0 loss: 1.046125  [   32/  118]
train() client id: f_00009-7-1 loss: 1.072784  [   64/  118]
train() client id: f_00009-7-2 loss: 1.076035  [   96/  118]
train() client id: f_00009-8-0 loss: 1.064963  [   32/  118]
train() client id: f_00009-8-1 loss: 1.003797  [   64/  118]
train() client id: f_00009-8-2 loss: 0.993074  [   96/  118]
train() client id: f_00009-9-0 loss: 1.022737  [   32/  118]
train() client id: f_00009-9-1 loss: 1.012132  [   64/  118]
train() client id: f_00009-9-2 loss: 1.098893  [   96/  118]
train() client id: f_00009-10-0 loss: 1.080117  [   32/  118]
train() client id: f_00009-10-1 loss: 1.000838  [   64/  118]
train() client id: f_00009-10-2 loss: 0.939021  [   96/  118]
train() client id: f_00009-11-0 loss: 1.037908  [   32/  118]
train() client id: f_00009-11-1 loss: 1.109031  [   64/  118]
train() client id: f_00009-11-2 loss: 0.957045  [   96/  118]
At round 7 accuracy: 0.6259946949602122
At round 7 training accuracy: 0.5720992622401073
At round 7 training loss: 0.8596132112599087
update_location
xs = -4.528292 -63.998411 35.045120 54.056472 -155.103519 -90.217951 12.784040 8.375741 -1.680116 -45.304393 
ys = 22.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 -3.124922 -19.177652 47.845030 -0.998518 
xs mean: -25.05713084058713
ys mean: 7.871751218646876
dists_uav = 102.619303 119.740472 105.971243 115.889478 184.782379 135.774195 100.862267 102.166214 110.869156 109.788365 
uav_gains = -100.280750 -101.956232 -100.629737 -101.601231 -106.729299 -103.321622 -100.093236 -100.232704 -101.120338 -101.013969 
uav_gains_db_mean: -101.69791173077515
dists_bs = 228.670141 194.277619 272.549961 275.153499 166.840434 210.055699 258.826841 267.079806 214.960854 218.623944 
bs_gains = -105.624924 -103.642897 -107.759551 -107.875160 -101.791496 -104.592424 -107.131320 -107.513009 -104.873122 -105.078595 
bs_gains_db_mean: -105.58824974954061
Round 8
-------------------------------
ene_coms = [0.00637672 0.00742215 0.00647152 0.0067507  0.00680818 0.00778102
 0.0063269  0.00636388 0.00789368 0.00797818]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.92977891 20.73014104  9.77720099  3.5062958  23.89215625 11.51601178
  4.34798316 14.03732851 10.27879242  9.35087805]
obj_prev = 117.36656690531666
eta_min = 3.198985020174161e-10	eta_max = 0.930223554940174
af = 24.843282687249765	bf = 1.7853564828667754	zeta = 27.327610955974745	eta = 0.909090909090909
af = 24.843282687249765	bf = 1.7853564828667754	zeta = 45.684916449900385	eta = 0.543796172079985
af = 24.843282687249765	bf = 1.7853564828667754	zeta = 37.087384896787825	eta = 0.6698580327620097
af = 24.843282687249765	bf = 1.7853564828667754	zeta = 35.55984903613363	eta = 0.6986329627554274
af = 24.843282687249765	bf = 1.7853564828667754	zeta = 35.48794579662235	eta = 0.7000484849031269
af = 24.843282687249765	bf = 1.7853564828667754	zeta = 35.48777624929195	eta = 0.7000518294731256
eta = 0.7000518294731256
ene_coms = [0.00637672 0.00742215 0.00647152 0.0067507  0.00680818 0.00778102
 0.0063269  0.00636388 0.00789368 0.00797818]
ene_comp = [0.02942585 0.0618877  0.02895877 0.01004216 0.07146278 0.03409662
 0.01261108 0.04180339 0.03036001 0.02755754]
ene_total = [3.03684923 5.87900777 3.00527228 1.42440579 6.63910848 3.55215027
 1.60635945 4.08564945 3.24475845 3.01421508]
ti_comp = [0.28519808 0.27474375 0.28425003 0.28145819 0.28088338 0.27115498
 0.28569622 0.28532645 0.27002841 0.26918342]
ti_coms = [0.06376715 0.07422148 0.0647152  0.06750704 0.06808185 0.07781025
 0.06326901 0.06363878 0.07893682 0.07978181]
t_total = [29.59996643 29.59996643 29.59996643 29.59996643 29.59996643 29.59996643
 29.59996643 29.59996643 29.59996643 29.59996643]
ene_coms = [0.00637672 0.00742215 0.00647152 0.0067507  0.00680818 0.00778102
 0.0063269  0.00636388 0.00789368 0.00797818]
ene_comp = [1.95782544e-05 1.96262574e-04 1.87853602e-05 7.98974851e-07
 2.89113052e-04 3.36960430e-05 1.53577580e-06 5.60829108e-05
 2.39864875e-05 1.80511520e-05]
ene_total = [0.54254711 0.64620963 0.55052136 0.57267673 0.60200775 0.66286109
 0.53679138 0.54455463 0.67159333 0.67825726]
optimize_network iter = 0 obj = 6.008020276544637
eta = 0.7000518294731256
freqs = [5.15884473e+07 1.12628047e+08 5.09389122e+07 1.78395162e+07
 1.27210770e+08 6.28729401e+07 2.20707847e+07 7.32553763e+07
 5.62163180e+07 5.11872978e+07]
eta_min = 0.6773336200423883	eta_max = 0.7000518294731126
af = 0.0469382083903894	bf = 1.7853564828667754	zeta = 0.05163202922942835	eta = 0.9090909090909091
af = 0.0469382083903894	bf = 1.7853564828667754	zeta = 19.688131891429713	eta = 0.0023840864460493434
af = 0.0469382083903894	bf = 1.7853564828667754	zeta = 2.0737574898799442	eta = 0.022634376786799113
af = 0.0469382083903894	bf = 1.7853564828667754	zeta = 2.008636038829727	eta = 0.02336819985453242
af = 0.0469382083903894	bf = 1.7853564828667754	zeta = 2.008611833495322	eta = 0.02336848145951079
eta = 0.02336848145951079
ene_coms = [0.00637672 0.00742215 0.00647152 0.0067507  0.00680818 0.00778102
 0.0063269  0.00636388 0.00789368 0.00797818]
ene_comp = [2.06233663e-04 2.06739318e-03 1.97881465e-04 8.41625137e-06
 3.04546272e-03 3.54947803e-04 1.61775745e-05 5.90766874e-04
 2.52669165e-04 1.90147452e-04]
ene_total = [0.17149246 0.24721212 0.17374463 0.17608191 0.25669745 0.21195029
 0.1652436  0.18117552 0.21222067 0.21279319]
ti_comp = [0.31162886 0.30117454 0.31068082 0.30788897 0.30731417 0.29758577
 0.312127   0.31175723 0.29645919 0.2956142 ]
ti_coms = [0.06376715 0.07422148 0.0647152  0.06750704 0.06808185 0.07781025
 0.06326901 0.06363878 0.07893682 0.07978181]
t_total = [29.59996643 29.59996643 29.59996643 29.59996643 29.59996643 29.59996643
 29.59996643 29.59996643 29.59996643 29.59996643]
ene_coms = [0.00637672 0.00742215 0.00647152 0.0067507  0.00680818 0.00778102
 0.0063269  0.00636388 0.00789368 0.00797818]
ene_comp = [2.13831832e-05 2.12979199e-04 2.05055889e-05 8.70669168e-07
 3.14945335e-04 3.64813012e-05 1.67785593e-06 6.12578968e-05
 2.59499351e-05 1.95178281e-05]
ene_total = [0.50448991 0.60202958 0.51189604 0.53236151 0.56165862 0.61641015
 0.49900832 0.50662182 0.62446279 0.63061835]
optimize_network iter = 1 obj = 5.589557093749259
eta = 0.6773336200423883
freqs = [5.15808182e+07 1.12249129e+08 5.09169759e+07 1.78167988e+07
 1.27026461e+08 6.25887176e+07 2.20707847e+07 7.32473381e+07
 5.59414582e+07 5.09227750e+07]
eta_min = 0.6773336200423892	eta_max = 0.6773336200423871
af = 0.046730172037312126	bf = 1.7853564828667754	zeta = 0.05140318924104334	eta = 0.9090909090909091
af = 0.046730172037312126	bf = 1.7853564828667754	zeta = 19.68791378362532	eta = 0.002373546153791987
af = 0.046730172037312126	bf = 1.7853564828667754	zeta = 2.0726838215285195	eta = 0.02254573107192516
af = 0.046730172037312126	bf = 1.7853564828667754	zeta = 2.007835199536882	eta = 0.023273908161432117
af = 0.046730172037312126	bf = 1.7853564828667754	zeta = 2.007811282414986	eta = 0.023274185401082764
eta = 0.023274185401082764
ene_coms = [0.00637672 0.00742215 0.00647152 0.0067507  0.00680818 0.00778102
 0.0063269  0.00636388 0.00789368 0.00797818]
ene_comp = [2.06394595e-04 2.05571616e-03 1.97923885e-04 8.40386617e-06
 3.03991291e-03 3.52124532e-04 1.61949880e-05 5.91272994e-04
 2.50473762e-04 1.88389829e-04]
ene_total = [0.1714801  0.24688408 0.17372896 0.17606458 0.25652811 0.21185629
 0.16522811 0.18117121 0.21214299 0.21272687]
ti_comp = [0.31162886 0.30117454 0.31068082 0.30788897 0.30731417 0.29758577
 0.312127   0.31175723 0.29645919 0.2956142 ]
ti_coms = [0.06376715 0.07422148 0.0647152  0.06750704 0.06808185 0.07781025
 0.06326901 0.06363878 0.07893682 0.07978181]
t_total = [29.59996643 29.59996643 29.59996643 29.59996643 29.59996643 29.59996643
 29.59996643 29.59996643 29.59996643 29.59996643]
ene_coms = [0.00637672 0.00742215 0.00647152 0.0067507  0.00680818 0.00778102
 0.0063269  0.00636388 0.00789368 0.00797818]
ene_comp = [2.13831832e-05 2.12979199e-04 2.05055889e-05 8.70669168e-07
 3.14945335e-04 3.64813012e-05 1.67785593e-06 6.12578968e-05
 2.59499351e-05 1.95178281e-05]
ene_total = [0.50448991 0.60202958 0.51189604 0.53236151 0.56165862 0.61641015
 0.49900832 0.50662182 0.62446279 0.63061835]
optimize_network iter = 2 obj = 5.589557093749273
eta = 0.6773336200423892
freqs = [5.15808182e+07 1.12249129e+08 5.09169759e+07 1.78167988e+07
 1.27026461e+08 6.25887176e+07 2.20707847e+07 7.32473381e+07
 5.59414582e+07 5.09227750e+07]
Done!
ene_coms = [0.00637672 0.00742215 0.00647152 0.0067507  0.00680818 0.00778102
 0.0063269  0.00636388 0.00789368 0.00797818]
ene_comp = [2.01139909e-05 2.00337884e-04 1.92884859e-05 8.18990866e-07
 2.96251851e-04 3.43159647e-05 1.57826731e-06 5.76219531e-05
 2.44096846e-05 1.83593534e-05]
ene_total = [0.00639683 0.00762249 0.00649081 0.00675152 0.00710444 0.00781534
 0.00632848 0.0064215  0.00791809 0.00799654]
At round 8 energy consumption: 0.07084603551799638
At round 8 eta: 0.6773336200423892
At round 8 a_n: 25.442236080499836
At round 8 local rounds: 12.757199690898329
At round 8 global rounds: 78.84997527118325
gradient difference: 0.44401898980140686
train() client id: f_00000-0-0 loss: 1.183445  [   32/  126]
train() client id: f_00000-0-1 loss: 1.000575  [   64/  126]
train() client id: f_00000-0-2 loss: 1.236726  [   96/  126]
train() client id: f_00000-1-0 loss: 1.108446  [   32/  126]
train() client id: f_00000-1-1 loss: 1.031888  [   64/  126]
train() client id: f_00000-1-2 loss: 1.083931  [   96/  126]
train() client id: f_00000-2-0 loss: 1.180376  [   32/  126]
train() client id: f_00000-2-1 loss: 0.990456  [   64/  126]
train() client id: f_00000-2-2 loss: 0.921132  [   96/  126]
train() client id: f_00000-3-0 loss: 0.962519  [   32/  126]
train() client id: f_00000-3-1 loss: 1.018811  [   64/  126]
train() client id: f_00000-3-2 loss: 0.932042  [   96/  126]
train() client id: f_00000-4-0 loss: 1.052944  [   32/  126]
train() client id: f_00000-4-1 loss: 0.807554  [   64/  126]
train() client id: f_00000-4-2 loss: 0.973169  [   96/  126]
train() client id: f_00000-5-0 loss: 0.906885  [   32/  126]
train() client id: f_00000-5-1 loss: 0.921511  [   64/  126]
train() client id: f_00000-5-2 loss: 0.946237  [   96/  126]
train() client id: f_00000-6-0 loss: 0.911237  [   32/  126]
train() client id: f_00000-6-1 loss: 0.950589  [   64/  126]
train() client id: f_00000-6-2 loss: 0.959442  [   96/  126]
train() client id: f_00000-7-0 loss: 1.036568  [   32/  126]
train() client id: f_00000-7-1 loss: 0.937719  [   64/  126]
train() client id: f_00000-7-2 loss: 0.862388  [   96/  126]
train() client id: f_00000-8-0 loss: 0.962332  [   32/  126]
train() client id: f_00000-8-1 loss: 0.872497  [   64/  126]
train() client id: f_00000-8-2 loss: 0.864597  [   96/  126]
train() client id: f_00000-9-0 loss: 0.936095  [   32/  126]
train() client id: f_00000-9-1 loss: 0.852851  [   64/  126]
train() client id: f_00000-9-2 loss: 0.964550  [   96/  126]
train() client id: f_00000-10-0 loss: 0.822612  [   32/  126]
train() client id: f_00000-10-1 loss: 0.968521  [   64/  126]
train() client id: f_00000-10-2 loss: 1.014480  [   96/  126]
train() client id: f_00000-11-0 loss: 1.008770  [   32/  126]
train() client id: f_00000-11-1 loss: 0.989102  [   64/  126]
train() client id: f_00000-11-2 loss: 0.954959  [   96/  126]
train() client id: f_00001-0-0 loss: 0.424396  [   32/  265]
train() client id: f_00001-0-1 loss: 0.434865  [   64/  265]
train() client id: f_00001-0-2 loss: 0.495334  [   96/  265]
train() client id: f_00001-0-3 loss: 0.401665  [  128/  265]
train() client id: f_00001-0-4 loss: 0.484936  [  160/  265]
train() client id: f_00001-0-5 loss: 0.511314  [  192/  265]
train() client id: f_00001-0-6 loss: 0.403124  [  224/  265]
train() client id: f_00001-0-7 loss: 0.354256  [  256/  265]
train() client id: f_00001-1-0 loss: 0.404761  [   32/  265]
train() client id: f_00001-1-1 loss: 0.605105  [   64/  265]
train() client id: f_00001-1-2 loss: 0.397301  [   96/  265]
train() client id: f_00001-1-3 loss: 0.392009  [  128/  265]
train() client id: f_00001-1-4 loss: 0.414038  [  160/  265]
train() client id: f_00001-1-5 loss: 0.351963  [  192/  265]
train() client id: f_00001-1-6 loss: 0.321451  [  224/  265]
train() client id: f_00001-1-7 loss: 0.436810  [  256/  265]
train() client id: f_00001-2-0 loss: 0.406670  [   32/  265]
train() client id: f_00001-2-1 loss: 0.446775  [   64/  265]
train() client id: f_00001-2-2 loss: 0.388254  [   96/  265]
train() client id: f_00001-2-3 loss: 0.347601  [  128/  265]
train() client id: f_00001-2-4 loss: 0.514726  [  160/  265]
train() client id: f_00001-2-5 loss: 0.383195  [  192/  265]
train() client id: f_00001-2-6 loss: 0.351678  [  224/  265]
train() client id: f_00001-2-7 loss: 0.383121  [  256/  265]
train() client id: f_00001-3-0 loss: 0.411084  [   32/  265]
train() client id: f_00001-3-1 loss: 0.347990  [   64/  265]
train() client id: f_00001-3-2 loss: 0.353365  [   96/  265]
train() client id: f_00001-3-3 loss: 0.370919  [  128/  265]
train() client id: f_00001-3-4 loss: 0.302559  [  160/  265]
train() client id: f_00001-3-5 loss: 0.387057  [  192/  265]
train() client id: f_00001-3-6 loss: 0.394415  [  224/  265]
train() client id: f_00001-3-7 loss: 0.534071  [  256/  265]
train() client id: f_00001-4-0 loss: 0.419162  [   32/  265]
train() client id: f_00001-4-1 loss: 0.493195  [   64/  265]
train() client id: f_00001-4-2 loss: 0.415844  [   96/  265]
train() client id: f_00001-4-3 loss: 0.299534  [  128/  265]
train() client id: f_00001-4-4 loss: 0.381697  [  160/  265]
train() client id: f_00001-4-5 loss: 0.327104  [  192/  265]
train() client id: f_00001-4-6 loss: 0.345086  [  224/  265]
train() client id: f_00001-4-7 loss: 0.347746  [  256/  265]
train() client id: f_00001-5-0 loss: 0.388615  [   32/  265]
train() client id: f_00001-5-1 loss: 0.426249  [   64/  265]
train() client id: f_00001-5-2 loss: 0.386160  [   96/  265]
train() client id: f_00001-5-3 loss: 0.345698  [  128/  265]
train() client id: f_00001-5-4 loss: 0.294401  [  160/  265]
train() client id: f_00001-5-5 loss: 0.387460  [  192/  265]
train() client id: f_00001-5-6 loss: 0.396872  [  224/  265]
train() client id: f_00001-5-7 loss: 0.298010  [  256/  265]
train() client id: f_00001-6-0 loss: 0.353984  [   32/  265]
train() client id: f_00001-6-1 loss: 0.368639  [   64/  265]
train() client id: f_00001-6-2 loss: 0.359475  [   96/  265]
train() client id: f_00001-6-3 loss: 0.356334  [  128/  265]
train() client id: f_00001-6-4 loss: 0.284655  [  160/  265]
train() client id: f_00001-6-5 loss: 0.460822  [  192/  265]
train() client id: f_00001-6-6 loss: 0.421401  [  224/  265]
train() client id: f_00001-6-7 loss: 0.304064  [  256/  265]
train() client id: f_00001-7-0 loss: 0.346444  [   32/  265]
train() client id: f_00001-7-1 loss: 0.332526  [   64/  265]
train() client id: f_00001-7-2 loss: 0.463880  [   96/  265]
train() client id: f_00001-7-3 loss: 0.333880  [  128/  265]
train() client id: f_00001-7-4 loss: 0.335639  [  160/  265]
train() client id: f_00001-7-5 loss: 0.386817  [  192/  265]
train() client id: f_00001-7-6 loss: 0.401414  [  224/  265]
train() client id: f_00001-7-7 loss: 0.287979  [  256/  265]
train() client id: f_00001-8-0 loss: 0.311320  [   32/  265]
train() client id: f_00001-8-1 loss: 0.407262  [   64/  265]
train() client id: f_00001-8-2 loss: 0.307982  [   96/  265]
train() client id: f_00001-8-3 loss: 0.395729  [  128/  265]
train() client id: f_00001-8-4 loss: 0.385133  [  160/  265]
train() client id: f_00001-8-5 loss: 0.239753  [  192/  265]
train() client id: f_00001-8-6 loss: 0.404990  [  224/  265]
train() client id: f_00001-8-7 loss: 0.340454  [  256/  265]
train() client id: f_00001-9-0 loss: 0.262137  [   32/  265]
train() client id: f_00001-9-1 loss: 0.304238  [   64/  265]
train() client id: f_00001-9-2 loss: 0.430424  [   96/  265]
train() client id: f_00001-9-3 loss: 0.309165  [  128/  265]
train() client id: f_00001-9-4 loss: 0.365595  [  160/  265]
train() client id: f_00001-9-5 loss: 0.290192  [  192/  265]
train() client id: f_00001-9-6 loss: 0.441894  [  224/  265]
train() client id: f_00001-9-7 loss: 0.362275  [  256/  265]
train() client id: f_00001-10-0 loss: 0.261880  [   32/  265]
train() client id: f_00001-10-1 loss: 0.346470  [   64/  265]
train() client id: f_00001-10-2 loss: 0.470603  [   96/  265]
train() client id: f_00001-10-3 loss: 0.358707  [  128/  265]
train() client id: f_00001-10-4 loss: 0.264635  [  160/  265]
train() client id: f_00001-10-5 loss: 0.274716  [  192/  265]
train() client id: f_00001-10-6 loss: 0.457730  [  224/  265]
train() client id: f_00001-10-7 loss: 0.292194  [  256/  265]
train() client id: f_00001-11-0 loss: 0.334882  [   32/  265]
train() client id: f_00001-11-1 loss: 0.268305  [   64/  265]
train() client id: f_00001-11-2 loss: 0.355682  [   96/  265]
train() client id: f_00001-11-3 loss: 0.388048  [  128/  265]
train() client id: f_00001-11-4 loss: 0.471672  [  160/  265]
train() client id: f_00001-11-5 loss: 0.296635  [  192/  265]
train() client id: f_00001-11-6 loss: 0.322217  [  224/  265]
train() client id: f_00001-11-7 loss: 0.295015  [  256/  265]
train() client id: f_00002-0-0 loss: 1.056858  [   32/  124]
train() client id: f_00002-0-1 loss: 1.175138  [   64/  124]
train() client id: f_00002-0-2 loss: 1.085407  [   96/  124]
train() client id: f_00002-1-0 loss: 1.074552  [   32/  124]
train() client id: f_00002-1-1 loss: 1.037414  [   64/  124]
train() client id: f_00002-1-2 loss: 1.120359  [   96/  124]
train() client id: f_00002-2-0 loss: 1.067360  [   32/  124]
train() client id: f_00002-2-1 loss: 1.029643  [   64/  124]
train() client id: f_00002-2-2 loss: 0.956769  [   96/  124]
train() client id: f_00002-3-0 loss: 1.032516  [   32/  124]
train() client id: f_00002-3-1 loss: 1.013384  [   64/  124]
train() client id: f_00002-3-2 loss: 1.010249  [   96/  124]
train() client id: f_00002-4-0 loss: 1.149112  [   32/  124]
train() client id: f_00002-4-1 loss: 0.945271  [   64/  124]
train() client id: f_00002-4-2 loss: 0.884776  [   96/  124]
train() client id: f_00002-5-0 loss: 0.936560  [   32/  124]
train() client id: f_00002-5-1 loss: 1.065003  [   64/  124]
train() client id: f_00002-5-2 loss: 0.954238  [   96/  124]
train() client id: f_00002-6-0 loss: 0.862285  [   32/  124]
train() client id: f_00002-6-1 loss: 0.870191  [   64/  124]
train() client id: f_00002-6-2 loss: 0.970209  [   96/  124]
train() client id: f_00002-7-0 loss: 0.871230  [   32/  124]
train() client id: f_00002-7-1 loss: 0.920141  [   64/  124]
train() client id: f_00002-7-2 loss: 0.835228  [   96/  124]
train() client id: f_00002-8-0 loss: 0.947330  [   32/  124]
train() client id: f_00002-8-1 loss: 0.801989  [   64/  124]
train() client id: f_00002-8-2 loss: 1.003493  [   96/  124]
train() client id: f_00002-9-0 loss: 0.814016  [   32/  124]
train() client id: f_00002-9-1 loss: 0.824685  [   64/  124]
train() client id: f_00002-9-2 loss: 1.073169  [   96/  124]
train() client id: f_00002-10-0 loss: 0.948559  [   32/  124]
train() client id: f_00002-10-1 loss: 0.970805  [   64/  124]
train() client id: f_00002-10-2 loss: 0.802400  [   96/  124]
train() client id: f_00002-11-0 loss: 0.986402  [   32/  124]
train() client id: f_00002-11-1 loss: 0.856172  [   64/  124]
train() client id: f_00002-11-2 loss: 0.810869  [   96/  124]
train() client id: f_00003-0-0 loss: 0.967168  [   32/   43]
train() client id: f_00003-1-0 loss: 0.887673  [   32/   43]
train() client id: f_00003-2-0 loss: 0.940366  [   32/   43]
train() client id: f_00003-3-0 loss: 0.846159  [   32/   43]
train() client id: f_00003-4-0 loss: 1.010920  [   32/   43]
train() client id: f_00003-5-0 loss: 0.977383  [   32/   43]
train() client id: f_00003-6-0 loss: 0.912634  [   32/   43]
train() client id: f_00003-7-0 loss: 0.984328  [   32/   43]
train() client id: f_00003-8-0 loss: 0.873853  [   32/   43]
train() client id: f_00003-9-0 loss: 0.858002  [   32/   43]
train() client id: f_00003-10-0 loss: 0.843444  [   32/   43]
train() client id: f_00003-11-0 loss: 0.889249  [   32/   43]
train() client id: f_00004-0-0 loss: 0.976724  [   32/  306]
train() client id: f_00004-0-1 loss: 0.926369  [   64/  306]
train() client id: f_00004-0-2 loss: 0.978237  [   96/  306]
train() client id: f_00004-0-3 loss: 0.991091  [  128/  306]
train() client id: f_00004-0-4 loss: 0.817009  [  160/  306]
train() client id: f_00004-0-5 loss: 0.892322  [  192/  306]
train() client id: f_00004-0-6 loss: 0.868865  [  224/  306]
train() client id: f_00004-0-7 loss: 1.103803  [  256/  306]
train() client id: f_00004-0-8 loss: 0.833575  [  288/  306]
train() client id: f_00004-1-0 loss: 1.108842  [   32/  306]
train() client id: f_00004-1-1 loss: 0.912434  [   64/  306]
train() client id: f_00004-1-2 loss: 0.911798  [   96/  306]
train() client id: f_00004-1-3 loss: 0.795013  [  128/  306]
train() client id: f_00004-1-4 loss: 0.989201  [  160/  306]
train() client id: f_00004-1-5 loss: 1.110103  [  192/  306]
train() client id: f_00004-1-6 loss: 0.830215  [  224/  306]
train() client id: f_00004-1-7 loss: 0.853171  [  256/  306]
train() client id: f_00004-1-8 loss: 0.914449  [  288/  306]
train() client id: f_00004-2-0 loss: 0.812120  [   32/  306]
train() client id: f_00004-2-1 loss: 1.097981  [   64/  306]
train() client id: f_00004-2-2 loss: 0.952036  [   96/  306]
train() client id: f_00004-2-3 loss: 1.063819  [  128/  306]
train() client id: f_00004-2-4 loss: 0.904874  [  160/  306]
train() client id: f_00004-2-5 loss: 0.773598  [  192/  306]
train() client id: f_00004-2-6 loss: 1.053532  [  224/  306]
train() client id: f_00004-2-7 loss: 0.845550  [  256/  306]
train() client id: f_00004-2-8 loss: 0.849415  [  288/  306]
train() client id: f_00004-3-0 loss: 0.986468  [   32/  306]
train() client id: f_00004-3-1 loss: 0.814596  [   64/  306]
train() client id: f_00004-3-2 loss: 0.954216  [   96/  306]
train() client id: f_00004-3-3 loss: 0.903783  [  128/  306]
train() client id: f_00004-3-4 loss: 0.800934  [  160/  306]
train() client id: f_00004-3-5 loss: 1.056504  [  192/  306]
train() client id: f_00004-3-6 loss: 0.899772  [  224/  306]
train() client id: f_00004-3-7 loss: 0.948115  [  256/  306]
train() client id: f_00004-3-8 loss: 0.905303  [  288/  306]
train() client id: f_00004-4-0 loss: 0.908939  [   32/  306]
train() client id: f_00004-4-1 loss: 0.990328  [   64/  306]
train() client id: f_00004-4-2 loss: 0.973747  [   96/  306]
train() client id: f_00004-4-3 loss: 0.916024  [  128/  306]
train() client id: f_00004-4-4 loss: 0.866756  [  160/  306]
train() client id: f_00004-4-5 loss: 0.847961  [  192/  306]
train() client id: f_00004-4-6 loss: 0.979473  [  224/  306]
train() client id: f_00004-4-7 loss: 0.875432  [  256/  306]
train() client id: f_00004-4-8 loss: 0.965246  [  288/  306]
train() client id: f_00004-5-0 loss: 0.951826  [   32/  306]
train() client id: f_00004-5-1 loss: 0.923453  [   64/  306]
train() client id: f_00004-5-2 loss: 0.998387  [   96/  306]
train() client id: f_00004-5-3 loss: 0.911215  [  128/  306]
train() client id: f_00004-5-4 loss: 0.902930  [  160/  306]
train() client id: f_00004-5-5 loss: 0.870420  [  192/  306]
train() client id: f_00004-5-6 loss: 0.904822  [  224/  306]
train() client id: f_00004-5-7 loss: 0.927843  [  256/  306]
train() client id: f_00004-5-8 loss: 0.898528  [  288/  306]
train() client id: f_00004-6-0 loss: 0.863604  [   32/  306]
train() client id: f_00004-6-1 loss: 0.902052  [   64/  306]
train() client id: f_00004-6-2 loss: 0.759371  [   96/  306]
train() client id: f_00004-6-3 loss: 0.996744  [  128/  306]
train() client id: f_00004-6-4 loss: 1.022538  [  160/  306]
train() client id: f_00004-6-5 loss: 0.990025  [  192/  306]
train() client id: f_00004-6-6 loss: 0.902814  [  224/  306]
train() client id: f_00004-6-7 loss: 0.847998  [  256/  306]
train() client id: f_00004-6-8 loss: 0.871711  [  288/  306]
train() client id: f_00004-7-0 loss: 0.921040  [   32/  306]
train() client id: f_00004-7-1 loss: 0.843205  [   64/  306]
train() client id: f_00004-7-2 loss: 0.838741  [   96/  306]
train() client id: f_00004-7-3 loss: 0.957257  [  128/  306]
train() client id: f_00004-7-4 loss: 0.865717  [  160/  306]
train() client id: f_00004-7-5 loss: 0.986908  [  192/  306]
train() client id: f_00004-7-6 loss: 0.911938  [  224/  306]
train() client id: f_00004-7-7 loss: 0.887770  [  256/  306]
train() client id: f_00004-7-8 loss: 1.010837  [  288/  306]
train() client id: f_00004-8-0 loss: 0.767681  [   32/  306]
train() client id: f_00004-8-1 loss: 1.001717  [   64/  306]
train() client id: f_00004-8-2 loss: 1.033908  [   96/  306]
train() client id: f_00004-8-3 loss: 0.921555  [  128/  306]
train() client id: f_00004-8-4 loss: 0.966423  [  160/  306]
train() client id: f_00004-8-5 loss: 0.910446  [  192/  306]
train() client id: f_00004-8-6 loss: 0.806301  [  224/  306]
train() client id: f_00004-8-7 loss: 0.861184  [  256/  306]
train() client id: f_00004-8-8 loss: 0.948997  [  288/  306]
train() client id: f_00004-9-0 loss: 0.895930  [   32/  306]
train() client id: f_00004-9-1 loss: 0.889939  [   64/  306]
train() client id: f_00004-9-2 loss: 0.982986  [   96/  306]
train() client id: f_00004-9-3 loss: 0.979866  [  128/  306]
train() client id: f_00004-9-4 loss: 0.893505  [  160/  306]
train() client id: f_00004-9-5 loss: 0.942378  [  192/  306]
train() client id: f_00004-9-6 loss: 0.843679  [  224/  306]
train() client id: f_00004-9-7 loss: 0.863007  [  256/  306]
train() client id: f_00004-9-8 loss: 0.924503  [  288/  306]
train() client id: f_00004-10-0 loss: 0.886285  [   32/  306]
train() client id: f_00004-10-1 loss: 0.839753  [   64/  306]
train() client id: f_00004-10-2 loss: 0.938590  [   96/  306]
train() client id: f_00004-10-3 loss: 1.063582  [  128/  306]
train() client id: f_00004-10-4 loss: 0.781413  [  160/  306]
train() client id: f_00004-10-5 loss: 0.960661  [  192/  306]
train() client id: f_00004-10-6 loss: 0.838589  [  224/  306]
train() client id: f_00004-10-7 loss: 0.881456  [  256/  306]
train() client id: f_00004-10-8 loss: 0.939263  [  288/  306]
train() client id: f_00004-11-0 loss: 0.871039  [   32/  306]
train() client id: f_00004-11-1 loss: 0.957464  [   64/  306]
train() client id: f_00004-11-2 loss: 0.947097  [   96/  306]
train() client id: f_00004-11-3 loss: 0.875504  [  128/  306]
train() client id: f_00004-11-4 loss: 0.913629  [  160/  306]
train() client id: f_00004-11-5 loss: 0.841806  [  192/  306]
train() client id: f_00004-11-6 loss: 0.986615  [  224/  306]
train() client id: f_00004-11-7 loss: 0.944096  [  256/  306]
train() client id: f_00004-11-8 loss: 0.822859  [  288/  306]
train() client id: f_00005-0-0 loss: 0.570428  [   32/  146]
train() client id: f_00005-0-1 loss: 0.542680  [   64/  146]
train() client id: f_00005-0-2 loss: 0.786437  [   96/  146]
train() client id: f_00005-0-3 loss: 0.807699  [  128/  146]
train() client id: f_00005-1-0 loss: 0.638997  [   32/  146]
train() client id: f_00005-1-1 loss: 0.652202  [   64/  146]
train() client id: f_00005-1-2 loss: 0.624494  [   96/  146]
train() client id: f_00005-1-3 loss: 0.674658  [  128/  146]
train() client id: f_00005-2-0 loss: 0.926546  [   32/  146]
train() client id: f_00005-2-1 loss: 0.773234  [   64/  146]
train() client id: f_00005-2-2 loss: 0.478414  [   96/  146]
train() client id: f_00005-2-3 loss: 0.504708  [  128/  146]
train() client id: f_00005-3-0 loss: 0.702618  [   32/  146]
train() client id: f_00005-3-1 loss: 0.555348  [   64/  146]
train() client id: f_00005-3-2 loss: 0.692920  [   96/  146]
train() client id: f_00005-3-3 loss: 0.669286  [  128/  146]
train() client id: f_00005-4-0 loss: 0.779846  [   32/  146]
train() client id: f_00005-4-1 loss: 0.725957  [   64/  146]
train() client id: f_00005-4-2 loss: 0.661287  [   96/  146]
train() client id: f_00005-4-3 loss: 0.442770  [  128/  146]
train() client id: f_00005-5-0 loss: 0.651299  [   32/  146]
train() client id: f_00005-5-1 loss: 0.660935  [   64/  146]
train() client id: f_00005-5-2 loss: 0.844558  [   96/  146]
train() client id: f_00005-5-3 loss: 0.478166  [  128/  146]
train() client id: f_00005-6-0 loss: 0.364016  [   32/  146]
train() client id: f_00005-6-1 loss: 0.735245  [   64/  146]
train() client id: f_00005-6-2 loss: 0.733534  [   96/  146]
train() client id: f_00005-6-3 loss: 0.811772  [  128/  146]
train() client id: f_00005-7-0 loss: 0.619765  [   32/  146]
train() client id: f_00005-7-1 loss: 0.609968  [   64/  146]
train() client id: f_00005-7-2 loss: 0.722949  [   96/  146]
train() client id: f_00005-7-3 loss: 0.775493  [  128/  146]
train() client id: f_00005-8-0 loss: 0.644104  [   32/  146]
train() client id: f_00005-8-1 loss: 0.733216  [   64/  146]
train() client id: f_00005-8-2 loss: 0.632716  [   96/  146]
train() client id: f_00005-8-3 loss: 0.554210  [  128/  146]
train() client id: f_00005-9-0 loss: 0.762755  [   32/  146]
train() client id: f_00005-9-1 loss: 0.766121  [   64/  146]
train() client id: f_00005-9-2 loss: 0.585977  [   96/  146]
train() client id: f_00005-9-3 loss: 0.557458  [  128/  146]
train() client id: f_00005-10-0 loss: 0.632464  [   32/  146]
train() client id: f_00005-10-1 loss: 0.420125  [   64/  146]
train() client id: f_00005-10-2 loss: 0.786394  [   96/  146]
train() client id: f_00005-10-3 loss: 0.672965  [  128/  146]
train() client id: f_00005-11-0 loss: 0.649931  [   32/  146]
train() client id: f_00005-11-1 loss: 0.874136  [   64/  146]
train() client id: f_00005-11-2 loss: 0.435236  [   96/  146]
train() client id: f_00005-11-3 loss: 0.649272  [  128/  146]
train() client id: f_00006-0-0 loss: 0.672849  [   32/   54]
train() client id: f_00006-1-0 loss: 0.674299  [   32/   54]
train() client id: f_00006-2-0 loss: 0.627998  [   32/   54]
train() client id: f_00006-3-0 loss: 0.661999  [   32/   54]
train() client id: f_00006-4-0 loss: 0.713678  [   32/   54]
train() client id: f_00006-5-0 loss: 0.676212  [   32/   54]
train() client id: f_00006-6-0 loss: 0.675890  [   32/   54]
train() client id: f_00006-7-0 loss: 0.712067  [   32/   54]
train() client id: f_00006-8-0 loss: 0.612718  [   32/   54]
train() client id: f_00006-9-0 loss: 0.712603  [   32/   54]
train() client id: f_00006-10-0 loss: 0.618599  [   32/   54]
train() client id: f_00006-11-0 loss: 0.677915  [   32/   54]
train() client id: f_00007-0-0 loss: 0.565106  [   32/  179]
train() client id: f_00007-0-1 loss: 0.551676  [   64/  179]
train() client id: f_00007-0-2 loss: 0.538703  [   96/  179]
train() client id: f_00007-0-3 loss: 0.647087  [  128/  179]
train() client id: f_00007-0-4 loss: 0.442502  [  160/  179]
train() client id: f_00007-1-0 loss: 0.552949  [   32/  179]
train() client id: f_00007-1-1 loss: 0.514459  [   64/  179]
train() client id: f_00007-1-2 loss: 0.515953  [   96/  179]
train() client id: f_00007-1-3 loss: 0.438242  [  128/  179]
train() client id: f_00007-1-4 loss: 0.597755  [  160/  179]
train() client id: f_00007-2-0 loss: 0.567248  [   32/  179]
train() client id: f_00007-2-1 loss: 0.473837  [   64/  179]
train() client id: f_00007-2-2 loss: 0.585953  [   96/  179]
train() client id: f_00007-2-3 loss: 0.446803  [  128/  179]
train() client id: f_00007-2-4 loss: 0.621154  [  160/  179]
train() client id: f_00007-3-0 loss: 0.761836  [   32/  179]
train() client id: f_00007-3-1 loss: 0.486853  [   64/  179]
train() client id: f_00007-3-2 loss: 0.526398  [   96/  179]
train() client id: f_00007-3-3 loss: 0.404645  [  128/  179]
train() client id: f_00007-3-4 loss: 0.417801  [  160/  179]
train() client id: f_00007-4-0 loss: 0.582003  [   32/  179]
train() client id: f_00007-4-1 loss: 0.500029  [   64/  179]
train() client id: f_00007-4-2 loss: 0.531230  [   96/  179]
train() client id: f_00007-4-3 loss: 0.660889  [  128/  179]
train() client id: f_00007-4-4 loss: 0.387155  [  160/  179]
train() client id: f_00007-5-0 loss: 0.416135  [   32/  179]
train() client id: f_00007-5-1 loss: 0.611673  [   64/  179]
train() client id: f_00007-5-2 loss: 0.398510  [   96/  179]
train() client id: f_00007-5-3 loss: 0.583584  [  128/  179]
train() client id: f_00007-5-4 loss: 0.609738  [  160/  179]
train() client id: f_00007-6-0 loss: 0.536621  [   32/  179]
train() client id: f_00007-6-1 loss: 0.623137  [   64/  179]
train() client id: f_00007-6-2 loss: 0.510527  [   96/  179]
train() client id: f_00007-6-3 loss: 0.527362  [  128/  179]
train() client id: f_00007-6-4 loss: 0.398532  [  160/  179]
train() client id: f_00007-7-0 loss: 0.458568  [   32/  179]
train() client id: f_00007-7-1 loss: 0.553933  [   64/  179]
train() client id: f_00007-7-2 loss: 0.605135  [   96/  179]
train() client id: f_00007-7-3 loss: 0.564025  [  128/  179]
train() client id: f_00007-7-4 loss: 0.390181  [  160/  179]
train() client id: f_00007-8-0 loss: 0.442021  [   32/  179]
train() client id: f_00007-8-1 loss: 0.499297  [   64/  179]
train() client id: f_00007-8-2 loss: 0.522701  [   96/  179]
train() client id: f_00007-8-3 loss: 0.559810  [  128/  179]
train() client id: f_00007-8-4 loss: 0.465006  [  160/  179]
train() client id: f_00007-9-0 loss: 0.498582  [   32/  179]
train() client id: f_00007-9-1 loss: 0.434988  [   64/  179]
train() client id: f_00007-9-2 loss: 0.623737  [   96/  179]
train() client id: f_00007-9-3 loss: 0.385034  [  128/  179]
train() client id: f_00007-9-4 loss: 0.552035  [  160/  179]
train() client id: f_00007-10-0 loss: 0.610884  [   32/  179]
train() client id: f_00007-10-1 loss: 0.357420  [   64/  179]
train() client id: f_00007-10-2 loss: 0.786701  [   96/  179]
train() client id: f_00007-10-3 loss: 0.363422  [  128/  179]
train() client id: f_00007-10-4 loss: 0.371635  [  160/  179]
train() client id: f_00007-11-0 loss: 0.534580  [   32/  179]
train() client id: f_00007-11-1 loss: 0.453331  [   64/  179]
train() client id: f_00007-11-2 loss: 0.445879  [   96/  179]
train() client id: f_00007-11-3 loss: 0.483077  [  128/  179]
train() client id: f_00007-11-4 loss: 0.639574  [  160/  179]
train() client id: f_00008-0-0 loss: 0.940470  [   32/  130]
train() client id: f_00008-0-1 loss: 0.850266  [   64/  130]
train() client id: f_00008-0-2 loss: 0.825510  [   96/  130]
train() client id: f_00008-0-3 loss: 0.798002  [  128/  130]
train() client id: f_00008-1-0 loss: 0.909579  [   32/  130]
train() client id: f_00008-1-1 loss: 0.884495  [   64/  130]
train() client id: f_00008-1-2 loss: 0.803382  [   96/  130]
train() client id: f_00008-1-3 loss: 0.827951  [  128/  130]
train() client id: f_00008-2-0 loss: 0.915720  [   32/  130]
train() client id: f_00008-2-1 loss: 0.886246  [   64/  130]
train() client id: f_00008-2-2 loss: 0.830484  [   96/  130]
train() client id: f_00008-2-3 loss: 0.789936  [  128/  130]
train() client id: f_00008-3-0 loss: 0.877398  [   32/  130]
train() client id: f_00008-3-1 loss: 0.971665  [   64/  130]
train() client id: f_00008-3-2 loss: 0.751726  [   96/  130]
train() client id: f_00008-3-3 loss: 0.821922  [  128/  130]
train() client id: f_00008-4-0 loss: 0.865021  [   32/  130]
train() client id: f_00008-4-1 loss: 0.871540  [   64/  130]
train() client id: f_00008-4-2 loss: 0.783327  [   96/  130]
train() client id: f_00008-4-3 loss: 0.895868  [  128/  130]
train() client id: f_00008-5-0 loss: 0.997939  [   32/  130]
train() client id: f_00008-5-1 loss: 0.792870  [   64/  130]
train() client id: f_00008-5-2 loss: 0.772203  [   96/  130]
train() client id: f_00008-5-3 loss: 0.851237  [  128/  130]
train() client id: f_00008-6-0 loss: 0.828539  [   32/  130]
train() client id: f_00008-6-1 loss: 1.099716  [   64/  130]
train() client id: f_00008-6-2 loss: 0.704184  [   96/  130]
train() client id: f_00008-6-3 loss: 0.775225  [  128/  130]
train() client id: f_00008-7-0 loss: 0.983609  [   32/  130]
train() client id: f_00008-7-1 loss: 0.794485  [   64/  130]
train() client id: f_00008-7-2 loss: 0.855222  [   96/  130]
train() client id: f_00008-7-3 loss: 0.775541  [  128/  130]
train() client id: f_00008-8-0 loss: 0.789717  [   32/  130]
train() client id: f_00008-8-1 loss: 0.846166  [   64/  130]
train() client id: f_00008-8-2 loss: 0.945877  [   96/  130]
train() client id: f_00008-8-3 loss: 0.808082  [  128/  130]
train() client id: f_00008-9-0 loss: 0.912572  [   32/  130]
train() client id: f_00008-9-1 loss: 0.744077  [   64/  130]
train() client id: f_00008-9-2 loss: 0.858336  [   96/  130]
train() client id: f_00008-9-3 loss: 0.881146  [  128/  130]
train() client id: f_00008-10-0 loss: 0.856893  [   32/  130]
train() client id: f_00008-10-1 loss: 0.794838  [   64/  130]
train() client id: f_00008-10-2 loss: 0.850892  [   96/  130]
train() client id: f_00008-10-3 loss: 0.859050  [  128/  130]
train() client id: f_00008-11-0 loss: 0.773402  [   32/  130]
train() client id: f_00008-11-1 loss: 0.895310  [   64/  130]
train() client id: f_00008-11-2 loss: 0.903164  [   96/  130]
train() client id: f_00008-11-3 loss: 0.797802  [  128/  130]
train() client id: f_00009-0-0 loss: 1.133178  [   32/  118]
train() client id: f_00009-0-1 loss: 1.183383  [   64/  118]
train() client id: f_00009-0-2 loss: 1.073721  [   96/  118]
train() client id: f_00009-1-0 loss: 1.154049  [   32/  118]
train() client id: f_00009-1-1 loss: 1.159770  [   64/  118]
train() client id: f_00009-1-2 loss: 0.974940  [   96/  118]
train() client id: f_00009-2-0 loss: 1.103608  [   32/  118]
train() client id: f_00009-2-1 loss: 1.032229  [   64/  118]
train() client id: f_00009-2-2 loss: 0.974611  [   96/  118]
train() client id: f_00009-3-0 loss: 1.074965  [   32/  118]
train() client id: f_00009-3-1 loss: 1.009132  [   64/  118]
train() client id: f_00009-3-2 loss: 0.956309  [   96/  118]
train() client id: f_00009-4-0 loss: 0.999193  [   32/  118]
train() client id: f_00009-4-1 loss: 1.078519  [   64/  118]
train() client id: f_00009-4-2 loss: 0.980337  [   96/  118]
train() client id: f_00009-5-0 loss: 0.991498  [   32/  118]
train() client id: f_00009-5-1 loss: 0.899154  [   64/  118]
train() client id: f_00009-5-2 loss: 1.015056  [   96/  118]
train() client id: f_00009-6-0 loss: 0.942688  [   32/  118]
train() client id: f_00009-6-1 loss: 1.003999  [   64/  118]
train() client id: f_00009-6-2 loss: 0.990384  [   96/  118]
train() client id: f_00009-7-0 loss: 0.923611  [   32/  118]
train() client id: f_00009-7-1 loss: 1.006049  [   64/  118]
train() client id: f_00009-7-2 loss: 0.908299  [   96/  118]
train() client id: f_00009-8-0 loss: 1.024443  [   32/  118]
train() client id: f_00009-8-1 loss: 1.054147  [   64/  118]
train() client id: f_00009-8-2 loss: 0.928113  [   96/  118]
train() client id: f_00009-9-0 loss: 0.951030  [   32/  118]
train() client id: f_00009-9-1 loss: 1.080960  [   64/  118]
train() client id: f_00009-9-2 loss: 0.889246  [   96/  118]
train() client id: f_00009-10-0 loss: 0.926659  [   32/  118]
train() client id: f_00009-10-1 loss: 0.925584  [   64/  118]
train() client id: f_00009-10-2 loss: 1.045739  [   96/  118]
train() client id: f_00009-11-0 loss: 1.015441  [   32/  118]
train() client id: f_00009-11-1 loss: 0.895394  [   64/  118]
train() client id: f_00009-11-2 loss: 1.004553  [   96/  118]
At round 8 accuracy: 0.6286472148541115
At round 8 training accuracy: 0.5720992622401073
At round 8 training loss: 0.8553634007820116
update_location
xs = -4.528292 -58.998411 30.045120 49.056472 -150.103519 -85.217951 7.784040 13.375741 -1.680116 -40.304393 
ys = 27.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 -3.124922 -19.177652 42.845030 -0.998518 
xs mean: -24.05713084058713
ys mean: 7.871751218646876
dists_uav = 103.834488 117.144341 104.424390 113.643330 180.605903 132.504538 100.351166 102.697092 108.804960 107.821339 
uav_gains = -100.408568 -101.718185 -100.470078 -101.388698 -106.466072 -103.056621 -100.038077 -100.288977 -100.916271 -100.817665 
uav_gains_db_mean: -101.55692124493505
dists_bs = 225.368394 197.177101 268.715519 271.005319 167.510284 212.123118 255.222438 270.537207 217.955772 221.626679 
bs_gains = -105.448063 -103.823041 -107.587256 -107.690438 -101.840220 -104.711523 -106.960787 -107.669415 -105.041374 -105.244476 
bs_gains_db_mean: -105.60165938078765
Round 9
-------------------------------
ene_coms = [0.00641112 0.00748773 0.0064278  0.00668762 0.00682306 0.00782844
 0.00631239 0.00637892 0.00796274 0.00804769]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.79695986 20.45269989  9.64445566  3.45748872 23.5708573  11.36216615
  4.28907555 13.8487159  10.1421531   9.22674322]
obj_prev = 115.79131533477704
eta_min = 2.4318403269017715e-10	eta_max = 0.9304602754620065
af = 24.50880095055506	bf = 1.7662027496164654	zeta = 26.95968104561057	eta = 0.909090909090909
af = 24.50880095055506	bf = 1.7662027496164654	zeta = 45.12355072502499	eta = 0.5431487672569785
af = 24.50880095055506	bf = 1.7662027496164654	zeta = 36.61080016005603	eta = 0.6694418270949244
af = 24.50880095055506	bf = 1.7662027496164654	zeta = 35.09776888788439	eta = 0.6983008244440119
af = 24.50880095055506	bf = 1.7662027496164654	zeta = 35.026422481755695	eta = 0.6997232150477549
af = 24.50880095055506	bf = 1.7662027496164654	zeta = 35.02625361484922	eta = 0.6997265885200086
eta = 0.6997265885200086
ene_coms = [0.00641112 0.00748773 0.0064278  0.00668762 0.00682306 0.00782844
 0.00631239 0.00637892 0.00796274 0.00804769]
ene_comp = [0.0294642  0.06196835 0.02899651 0.01005524 0.07155591 0.03414105
 0.01262751 0.04185787 0.03039957 0.02759346]
ene_total = [2.99879803 5.80579562 2.96109926 1.39952642 6.55165522 3.50820714
 1.58317653 4.03208669 3.20668478 2.97922393]
ti_comp = [0.28940187 0.27863579 0.289235   0.28663688 0.28528245 0.27522865
 0.2903891  0.28972386 0.27388561 0.27303614]
ti_coms = [0.06411118 0.07487725 0.06427804 0.06687617 0.0682306  0.0782844
 0.06312394 0.06378919 0.07962744 0.08047691]
t_total = [29.54996223 29.54996223 29.54996223 29.54996223 29.54996223 29.54996223
 29.54996223 29.54996223 29.54996223 29.54996223]
ene_coms = [0.00641112 0.00748773 0.0064278  0.00668762 0.00682306 0.00782844
 0.00631239 0.00637892 0.00796274 0.00804769]
ene_comp = [1.90880354e-05 1.91564960e-04 1.82144312e-05 7.73380991e-07
 2.81362646e-04 3.28339824e-05 1.49235762e-06 5.46063102e-05
 2.34068993e-05 1.76140011e-05]
ene_total = [0.53749738 0.64190768 0.53881918 0.55907887 0.5938548  0.6571196
 0.52777433 0.53777484 0.66755796 0.67417442]
optimize_network iter = 0 obj = 5.93555907298079
eta = 0.6997265885200086
freqs = [5.09053348e+07 1.11199556e+08 5.01262158e+07 1.75400357e+07
 1.25412395e+08 6.20230770e+07 2.17424024e+07 7.22375246e+07
 5.54968359e+07 5.05307752e+07]
eta_min = 0.6819225236609968	eta_max = 0.6997265885199971
af = 0.045055647771854584	bf = 1.7662027496164654	zeta = 0.04956121254904005	eta = 0.909090909090909
af = 0.045055647771854584	bf = 1.7662027496164654	zeta = 19.475467126574785	eta = 0.002313456590233719
af = 0.045055647771854584	bf = 1.7662027496164654	zeta = 2.0443756139834335	eta = 0.022038830566983907
af = 0.045055647771854584	bf = 1.7662027496164654	zeta = 1.9817635143704595	eta = 0.022735128306248624
af = 0.045055647771854584	bf = 1.7662027496164654	zeta = 1.9817414401866678	eta = 0.02273538154786258
eta = 0.02273538154786258
ene_coms = [0.00641112 0.00748773 0.0064278  0.00668762 0.00682306 0.00782844
 0.00631239 0.00637892 0.00796274 0.00804769]
ene_comp = [2.02276374e-04 2.03001853e-03 1.93018769e-04 8.19553713e-06
 2.98160678e-03 3.47942508e-04 1.58145499e-05 5.78664394e-04
 2.48043480e-04 1.86655997e-04]
ene_total = [0.16985588 0.24445009 0.17004668 0.17197268 0.25181931 0.20999908
 0.16253129 0.1786959  0.21088271 0.21148781]
ti_comp = [0.31036266 0.29959659 0.3101958  0.30759767 0.30624324 0.29618944
 0.3113499  0.31068465 0.2948464  0.29399693]
ti_coms = [0.06411118 0.07487725 0.06427804 0.06687617 0.0682306  0.0782844
 0.06312394 0.06378919 0.07962744 0.08047691]
t_total = [29.54996223 29.54996223 29.54996223 29.54996223 29.54996223 29.54996223
 29.54996223 29.54996223 29.54996223 29.54996223]
ene_coms = [0.00641112 0.00748773 0.0064278  0.00668762 0.00682306 0.00782844
 0.00631239 0.00637892 0.00796274 0.00804769]
ene_comp = [2.04564462e-05 2.04230873e-04 1.95186928e-05 8.27745387e-07
 3.00946121e-04 3.49443463e-05 1.60007861e-06 5.85297928e-05
 2.48940685e-05 1.87248316e-05]
ene_total = [0.50751949 0.60697702 0.50876224 0.52778929 0.56215973 0.62050453
 0.4982412  0.50798305 0.63030943 0.63652585]
optimize_network iter = 1 obj = 5.606771816292369
eta = 0.6819225236609968
freqs = [5.08936468e+07 1.10884670e+08 5.01127541e+07 1.75245916e+07
 1.25261443e+08 6.17939251e+07 2.17424024e+07 7.22263598e+07
 5.52726152e+07 5.03154899e+07]
eta_min = 0.6819225236611215	eta_max = 0.6819225236609872
af = 0.04488850359350927	bf = 1.7662027496164654	zeta = 0.049377353952860206	eta = 0.909090909090909
af = 0.04488850359350927	bf = 1.7662027496164654	zeta = 19.47529189061618	eta = 0.0023048950354956164
af = 0.04488850359350927	bf = 1.7662027496164654	zeta = 2.0435081412721328	eta = 0.02196639332474844
af = 0.04488850359350927	bf = 1.7662027496164654	zeta = 1.9811158704717668	eta = 0.022658191912227673
af = 0.04488850359350927	bf = 1.7662027496164654	zeta = 1.9810940169243398	eta = 0.02265844185587867
eta = 0.02265844185587867
ene_coms = [0.00641112 0.00748773 0.0064278  0.00668762 0.00682306 0.00782844
 0.00631239 0.00637892 0.00796274 0.00804769]
ene_comp = [2.02364631e-04 2.02034629e-03 1.93087940e-04 8.18844034e-06
 2.97709827e-03 3.45685642e-04 1.58287179e-05 5.79003790e-04
 2.46263643e-04 1.85234698e-04]
ene_total = [0.16984478 0.24418245 0.17003507 0.17195896 0.2516837  0.20992459
 0.16251886 0.17869055 0.2108204  0.21143466]
ti_comp = [0.31036266 0.29959659 0.3101958  0.30759767 0.30624324 0.29618944
 0.3113499  0.31068465 0.2948464  0.29399693]
ti_coms = [0.06411118 0.07487725 0.06427804 0.06687617 0.0682306  0.0782844
 0.06312394 0.06378919 0.07962744 0.08047691]
t_total = [29.54996223 29.54996223 29.54996223 29.54996223 29.54996223 29.54996223
 29.54996223 29.54996223 29.54996223 29.54996223]
ene_coms = [0.00641112 0.00748773 0.0064278  0.00668762 0.00682306 0.00782844
 0.00631239 0.00637892 0.00796274 0.00804769]
ene_comp = [2.04564462e-05 2.04230873e-04 1.95186928e-05 8.27745387e-07
 3.00946121e-04 3.49443463e-05 1.60007861e-06 5.85297928e-05
 2.48940685e-05 1.87248316e-05]
ene_total = [0.50751949 0.60697702 0.50876224 0.52778929 0.56215973 0.62050453
 0.4982412  0.50798305 0.63030943 0.63652585]
optimize_network iter = 2 obj = 5.606771816294542
eta = 0.6819225236611215
freqs = [5.08936468e+07 1.10884670e+08 5.01127541e+07 1.75245916e+07
 1.25261443e+08 6.17939251e+07 2.17424024e+07 7.22263598e+07
 5.52726152e+07 5.03154899e+07]
Done!
ene_coms = [0.00641112 0.00748773 0.0064278  0.00668762 0.00682306 0.00782844
 0.00631239 0.00637892 0.00796274 0.00804769]
ene_comp = [1.95816344e-05 1.95497020e-04 1.86839836e-05 7.92347182e-07
 2.88076279e-04 3.34499652e-05 1.53165188e-06 5.60267893e-05
 2.38294835e-05 1.79240716e-05]
ene_total = [0.0064307  0.00768322 0.00644649 0.00668841 0.00711114 0.00786189
 0.00631393 0.00643495 0.00798657 0.00806561]
At round 9 energy consumption: 0.07102290506336384
At round 9 eta: 0.6819225236611215
At round 9 a_n: 25.09969023353052
At round 9 local rounds: 12.536101422571074
At round 9 global rounds: 78.9106180117872
gradient difference: 0.3712840676307678
train() client id: f_00000-0-0 loss: 1.400217  [   32/  126]
train() client id: f_00000-0-1 loss: 1.354942  [   64/  126]
train() client id: f_00000-0-2 loss: 1.380994  [   96/  126]
train() client id: f_00000-1-0 loss: 1.155407  [   32/  126]
train() client id: f_00000-1-1 loss: 1.286740  [   64/  126]
train() client id: f_00000-1-2 loss: 1.168035  [   96/  126]
train() client id: f_00000-2-0 loss: 1.187143  [   32/  126]
train() client id: f_00000-2-1 loss: 1.120279  [   64/  126]
train() client id: f_00000-2-2 loss: 1.153987  [   96/  126]
train() client id: f_00000-3-0 loss: 1.133673  [   32/  126]
train() client id: f_00000-3-1 loss: 1.029927  [   64/  126]
train() client id: f_00000-3-2 loss: 1.017035  [   96/  126]
train() client id: f_00000-4-0 loss: 1.027643  [   32/  126]
train() client id: f_00000-4-1 loss: 1.005847  [   64/  126]
train() client id: f_00000-4-2 loss: 0.950826  [   96/  126]
train() client id: f_00000-5-0 loss: 1.040175  [   32/  126]
train() client id: f_00000-5-1 loss: 0.923246  [   64/  126]
train() client id: f_00000-5-2 loss: 0.924151  [   96/  126]
train() client id: f_00000-6-0 loss: 0.919220  [   32/  126]
train() client id: f_00000-6-1 loss: 0.940956  [   64/  126]
train() client id: f_00000-6-2 loss: 0.904391  [   96/  126]
train() client id: f_00000-7-0 loss: 0.938250  [   32/  126]
train() client id: f_00000-7-1 loss: 0.941606  [   64/  126]
train() client id: f_00000-7-2 loss: 0.832689  [   96/  126]
train() client id: f_00000-8-0 loss: 0.900833  [   32/  126]
train() client id: f_00000-8-1 loss: 0.875060  [   64/  126]
train() client id: f_00000-8-2 loss: 0.898736  [   96/  126]
train() client id: f_00000-9-0 loss: 0.883065  [   32/  126]
train() client id: f_00000-9-1 loss: 0.868585  [   64/  126]
train() client id: f_00000-9-2 loss: 0.913531  [   96/  126]
train() client id: f_00000-10-0 loss: 0.887800  [   32/  126]
train() client id: f_00000-10-1 loss: 0.859745  [   64/  126]
train() client id: f_00000-10-2 loss: 0.857901  [   96/  126]
train() client id: f_00000-11-0 loss: 0.798328  [   32/  126]
train() client id: f_00000-11-1 loss: 0.837624  [   64/  126]
train() client id: f_00000-11-2 loss: 0.847785  [   96/  126]
train() client id: f_00001-0-0 loss: 0.586471  [   32/  265]
train() client id: f_00001-0-1 loss: 0.487117  [   64/  265]
train() client id: f_00001-0-2 loss: 0.523921  [   96/  265]
train() client id: f_00001-0-3 loss: 0.640790  [  128/  265]
train() client id: f_00001-0-4 loss: 0.620018  [  160/  265]
train() client id: f_00001-0-5 loss: 0.598894  [  192/  265]
train() client id: f_00001-0-6 loss: 0.516698  [  224/  265]
train() client id: f_00001-0-7 loss: 0.568159  [  256/  265]
train() client id: f_00001-1-0 loss: 0.544069  [   32/  265]
train() client id: f_00001-1-1 loss: 0.604386  [   64/  265]
train() client id: f_00001-1-2 loss: 0.504566  [   96/  265]
train() client id: f_00001-1-3 loss: 0.565972  [  128/  265]
train() client id: f_00001-1-4 loss: 0.604793  [  160/  265]
train() client id: f_00001-1-5 loss: 0.586385  [  192/  265]
train() client id: f_00001-1-6 loss: 0.553414  [  224/  265]
train() client id: f_00001-1-7 loss: 0.466808  [  256/  265]
train() client id: f_00001-2-0 loss: 0.469885  [   32/  265]
train() client id: f_00001-2-1 loss: 0.491566  [   64/  265]
train() client id: f_00001-2-2 loss: 0.541895  [   96/  265]
train() client id: f_00001-2-3 loss: 0.549484  [  128/  265]
train() client id: f_00001-2-4 loss: 0.506510  [  160/  265]
train() client id: f_00001-2-5 loss: 0.566045  [  192/  265]
train() client id: f_00001-2-6 loss: 0.570881  [  224/  265]
train() client id: f_00001-2-7 loss: 0.612978  [  256/  265]
train() client id: f_00001-3-0 loss: 0.488404  [   32/  265]
train() client id: f_00001-3-1 loss: 0.533083  [   64/  265]
train() client id: f_00001-3-2 loss: 0.691182  [   96/  265]
train() client id: f_00001-3-3 loss: 0.512497  [  128/  265]
train() client id: f_00001-3-4 loss: 0.595134  [  160/  265]
train() client id: f_00001-3-5 loss: 0.422813  [  192/  265]
train() client id: f_00001-3-6 loss: 0.527854  [  224/  265]
train() client id: f_00001-3-7 loss: 0.458062  [  256/  265]
train() client id: f_00001-4-0 loss: 0.467531  [   32/  265]
train() client id: f_00001-4-1 loss: 0.623538  [   64/  265]
train() client id: f_00001-4-2 loss: 0.462862  [   96/  265]
train() client id: f_00001-4-3 loss: 0.446665  [  128/  265]
train() client id: f_00001-4-4 loss: 0.610255  [  160/  265]
train() client id: f_00001-4-5 loss: 0.452043  [  192/  265]
train() client id: f_00001-4-6 loss: 0.658424  [  224/  265]
train() client id: f_00001-4-7 loss: 0.525684  [  256/  265]
train() client id: f_00001-5-0 loss: 0.506675  [   32/  265]
train() client id: f_00001-5-1 loss: 0.497004  [   64/  265]
train() client id: f_00001-5-2 loss: 0.584603  [   96/  265]
train() client id: f_00001-5-3 loss: 0.451258  [  128/  265]
train() client id: f_00001-5-4 loss: 0.453756  [  160/  265]
train() client id: f_00001-5-5 loss: 0.539274  [  192/  265]
train() client id: f_00001-5-6 loss: 0.593287  [  224/  265]
train() client id: f_00001-5-7 loss: 0.629859  [  256/  265]
train() client id: f_00001-6-0 loss: 0.471649  [   32/  265]
train() client id: f_00001-6-1 loss: 0.448664  [   64/  265]
train() client id: f_00001-6-2 loss: 0.501851  [   96/  265]
train() client id: f_00001-6-3 loss: 0.547409  [  128/  265]
train() client id: f_00001-6-4 loss: 0.501482  [  160/  265]
train() client id: f_00001-6-5 loss: 0.600048  [  192/  265]
train() client id: f_00001-6-6 loss: 0.574815  [  224/  265]
train() client id: f_00001-6-7 loss: 0.510187  [  256/  265]
train() client id: f_00001-7-0 loss: 0.520806  [   32/  265]
train() client id: f_00001-7-1 loss: 0.516875  [   64/  265]
train() client id: f_00001-7-2 loss: 0.578105  [   96/  265]
train() client id: f_00001-7-3 loss: 0.445372  [  128/  265]
train() client id: f_00001-7-4 loss: 0.625020  [  160/  265]
train() client id: f_00001-7-5 loss: 0.430967  [  192/  265]
train() client id: f_00001-7-6 loss: 0.585807  [  224/  265]
train() client id: f_00001-7-7 loss: 0.531127  [  256/  265]
train() client id: f_00001-8-0 loss: 0.713152  [   32/  265]
train() client id: f_00001-8-1 loss: 0.482323  [   64/  265]
train() client id: f_00001-8-2 loss: 0.453581  [   96/  265]
train() client id: f_00001-8-3 loss: 0.607100  [  128/  265]
train() client id: f_00001-8-4 loss: 0.542019  [  160/  265]
train() client id: f_00001-8-5 loss: 0.443712  [  192/  265]
train() client id: f_00001-8-6 loss: 0.546339  [  224/  265]
train() client id: f_00001-8-7 loss: 0.438547  [  256/  265]
train() client id: f_00001-9-0 loss: 0.477282  [   32/  265]
train() client id: f_00001-9-1 loss: 0.461694  [   64/  265]
train() client id: f_00001-9-2 loss: 0.471833  [   96/  265]
train() client id: f_00001-9-3 loss: 0.505007  [  128/  265]
train() client id: f_00001-9-4 loss: 0.571684  [  160/  265]
train() client id: f_00001-9-5 loss: 0.549600  [  192/  265]
train() client id: f_00001-9-6 loss: 0.523636  [  224/  265]
train() client id: f_00001-9-7 loss: 0.657260  [  256/  265]
train() client id: f_00001-10-0 loss: 0.576621  [   32/  265]
train() client id: f_00001-10-1 loss: 0.626085  [   64/  265]
train() client id: f_00001-10-2 loss: 0.551512  [   96/  265]
train() client id: f_00001-10-3 loss: 0.441096  [  128/  265]
train() client id: f_00001-10-4 loss: 0.518408  [  160/  265]
train() client id: f_00001-10-5 loss: 0.440759  [  192/  265]
train() client id: f_00001-10-6 loss: 0.462752  [  224/  265]
train() client id: f_00001-10-7 loss: 0.620675  [  256/  265]
train() client id: f_00001-11-0 loss: 0.591131  [   32/  265]
train() client id: f_00001-11-1 loss: 0.488748  [   64/  265]
train() client id: f_00001-11-2 loss: 0.454720  [   96/  265]
train() client id: f_00001-11-3 loss: 0.439944  [  128/  265]
train() client id: f_00001-11-4 loss: 0.506879  [  160/  265]
train() client id: f_00001-11-5 loss: 0.509059  [  192/  265]
train() client id: f_00001-11-6 loss: 0.680384  [  224/  265]
train() client id: f_00001-11-7 loss: 0.563420  [  256/  265]
train() client id: f_00002-0-0 loss: 1.263958  [   32/  124]
train() client id: f_00002-0-1 loss: 1.186337  [   64/  124]
train() client id: f_00002-0-2 loss: 1.296262  [   96/  124]
train() client id: f_00002-1-0 loss: 1.117506  [   32/  124]
train() client id: f_00002-1-1 loss: 1.248264  [   64/  124]
train() client id: f_00002-1-2 loss: 1.278795  [   96/  124]
train() client id: f_00002-2-0 loss: 1.132370  [   32/  124]
train() client id: f_00002-2-1 loss: 1.069429  [   64/  124]
train() client id: f_00002-2-2 loss: 1.162347  [   96/  124]
train() client id: f_00002-3-0 loss: 1.015180  [   32/  124]
train() client id: f_00002-3-1 loss: 1.275714  [   64/  124]
train() client id: f_00002-3-2 loss: 1.100740  [   96/  124]
train() client id: f_00002-4-0 loss: 1.040925  [   32/  124]
train() client id: f_00002-4-1 loss: 1.147719  [   64/  124]
train() client id: f_00002-4-2 loss: 1.241858  [   96/  124]
train() client id: f_00002-5-0 loss: 1.116605  [   32/  124]
train() client id: f_00002-5-1 loss: 1.085436  [   64/  124]
train() client id: f_00002-5-2 loss: 1.069399  [   96/  124]
train() client id: f_00002-6-0 loss: 1.135114  [   32/  124]
train() client id: f_00002-6-1 loss: 0.950918  [   64/  124]
train() client id: f_00002-6-2 loss: 0.991396  [   96/  124]
train() client id: f_00002-7-0 loss: 1.042278  [   32/  124]
train() client id: f_00002-7-1 loss: 1.034945  [   64/  124]
train() client id: f_00002-7-2 loss: 1.079853  [   96/  124]
train() client id: f_00002-8-0 loss: 1.150304  [   32/  124]
train() client id: f_00002-8-1 loss: 0.883778  [   64/  124]
train() client id: f_00002-8-2 loss: 1.299704  [   96/  124]
train() client id: f_00002-9-0 loss: 1.104360  [   32/  124]
train() client id: f_00002-9-1 loss: 0.877873  [   64/  124]
train() client id: f_00002-9-2 loss: 1.174357  [   96/  124]
train() client id: f_00002-10-0 loss: 0.949728  [   32/  124]
train() client id: f_00002-10-1 loss: 1.070426  [   64/  124]
train() client id: f_00002-10-2 loss: 1.144124  [   96/  124]
train() client id: f_00002-11-0 loss: 1.020553  [   32/  124]
train() client id: f_00002-11-1 loss: 1.085234  [   64/  124]
train() client id: f_00002-11-2 loss: 1.157116  [   96/  124]
train() client id: f_00003-0-0 loss: 1.040880  [   32/   43]
train() client id: f_00003-1-0 loss: 1.070693  [   32/   43]
train() client id: f_00003-2-0 loss: 1.005107  [   32/   43]
train() client id: f_00003-3-0 loss: 1.049940  [   32/   43]
train() client id: f_00003-4-0 loss: 1.090655  [   32/   43]
train() client id: f_00003-5-0 loss: 0.995830  [   32/   43]
train() client id: f_00003-6-0 loss: 1.002123  [   32/   43]
train() client id: f_00003-7-0 loss: 1.013415  [   32/   43]
train() client id: f_00003-8-0 loss: 0.944711  [   32/   43]
train() client id: f_00003-9-0 loss: 1.106907  [   32/   43]
train() client id: f_00003-10-0 loss: 1.002632  [   32/   43]
train() client id: f_00003-11-0 loss: 0.985782  [   32/   43]
train() client id: f_00004-0-0 loss: 1.006100  [   32/  306]
train() client id: f_00004-0-1 loss: 0.961482  [   64/  306]
train() client id: f_00004-0-2 loss: 1.016218  [   96/  306]
train() client id: f_00004-0-3 loss: 0.870568  [  128/  306]
train() client id: f_00004-0-4 loss: 0.952300  [  160/  306]
train() client id: f_00004-0-5 loss: 0.978460  [  192/  306]
train() client id: f_00004-0-6 loss: 1.093192  [  224/  306]
train() client id: f_00004-0-7 loss: 0.992644  [  256/  306]
train() client id: f_00004-0-8 loss: 0.970781  [  288/  306]
train() client id: f_00004-1-0 loss: 0.966400  [   32/  306]
train() client id: f_00004-1-1 loss: 1.062537  [   64/  306]
train() client id: f_00004-1-2 loss: 0.963629  [   96/  306]
train() client id: f_00004-1-3 loss: 1.038935  [  128/  306]
train() client id: f_00004-1-4 loss: 0.913853  [  160/  306]
train() client id: f_00004-1-5 loss: 1.092220  [  192/  306]
train() client id: f_00004-1-6 loss: 0.978642  [  224/  306]
train() client id: f_00004-1-7 loss: 0.874009  [  256/  306]
train() client id: f_00004-1-8 loss: 0.943650  [  288/  306]
train() client id: f_00004-2-0 loss: 0.955263  [   32/  306]
train() client id: f_00004-2-1 loss: 1.066150  [   64/  306]
train() client id: f_00004-2-2 loss: 0.861845  [   96/  306]
train() client id: f_00004-2-3 loss: 1.020383  [  128/  306]
train() client id: f_00004-2-4 loss: 0.979958  [  160/  306]
train() client id: f_00004-2-5 loss: 1.022008  [  192/  306]
train() client id: f_00004-2-6 loss: 1.075676  [  224/  306]
train() client id: f_00004-2-7 loss: 0.877983  [  256/  306]
train() client id: f_00004-2-8 loss: 0.968665  [  288/  306]
train() client id: f_00004-3-0 loss: 1.011420  [   32/  306]
train() client id: f_00004-3-1 loss: 1.105583  [   64/  306]
train() client id: f_00004-3-2 loss: 0.922435  [   96/  306]
train() client id: f_00004-3-3 loss: 1.012111  [  128/  306]
train() client id: f_00004-3-4 loss: 0.966723  [  160/  306]
train() client id: f_00004-3-5 loss: 1.081668  [  192/  306]
train() client id: f_00004-3-6 loss: 0.953602  [  224/  306]
train() client id: f_00004-3-7 loss: 0.988611  [  256/  306]
train() client id: f_00004-3-8 loss: 0.779765  [  288/  306]
train() client id: f_00004-4-0 loss: 0.990664  [   32/  306]
train() client id: f_00004-4-1 loss: 1.037476  [   64/  306]
train() client id: f_00004-4-2 loss: 1.065308  [   96/  306]
train() client id: f_00004-4-3 loss: 0.909117  [  128/  306]
train() client id: f_00004-4-4 loss: 0.953501  [  160/  306]
train() client id: f_00004-4-5 loss: 0.923673  [  192/  306]
train() client id: f_00004-4-6 loss: 1.027243  [  224/  306]
train() client id: f_00004-4-7 loss: 0.991275  [  256/  306]
train() client id: f_00004-4-8 loss: 0.912407  [  288/  306]
train() client id: f_00004-5-0 loss: 0.986609  [   32/  306]
train() client id: f_00004-5-1 loss: 1.027625  [   64/  306]
train() client id: f_00004-5-2 loss: 0.928997  [   96/  306]
train() client id: f_00004-5-3 loss: 0.923980  [  128/  306]
train() client id: f_00004-5-4 loss: 0.954163  [  160/  306]
train() client id: f_00004-5-5 loss: 0.993173  [  192/  306]
train() client id: f_00004-5-6 loss: 0.909871  [  224/  306]
train() client id: f_00004-5-7 loss: 1.072190  [  256/  306]
train() client id: f_00004-5-8 loss: 0.872176  [  288/  306]
train() client id: f_00004-6-0 loss: 0.986195  [   32/  306]
train() client id: f_00004-6-1 loss: 0.978394  [   64/  306]
train() client id: f_00004-6-2 loss: 0.929654  [   96/  306]
train() client id: f_00004-6-3 loss: 1.040527  [  128/  306]
train() client id: f_00004-6-4 loss: 0.925903  [  160/  306]
train() client id: f_00004-6-5 loss: 0.945339  [  192/  306]
train() client id: f_00004-6-6 loss: 1.031115  [  224/  306]
train() client id: f_00004-6-7 loss: 1.004629  [  256/  306]
train() client id: f_00004-6-8 loss: 0.966802  [  288/  306]
train() client id: f_00004-7-0 loss: 0.943937  [   32/  306]
train() client id: f_00004-7-1 loss: 0.897409  [   64/  306]
train() client id: f_00004-7-2 loss: 0.972118  [   96/  306]
train() client id: f_00004-7-3 loss: 1.089362  [  128/  306]
train() client id: f_00004-7-4 loss: 1.127772  [  160/  306]
train() client id: f_00004-7-5 loss: 0.899104  [  192/  306]
train() client id: f_00004-7-6 loss: 0.897321  [  224/  306]
train() client id: f_00004-7-7 loss: 0.917956  [  256/  306]
train() client id: f_00004-7-8 loss: 0.967341  [  288/  306]
train() client id: f_00004-8-0 loss: 1.004973  [   32/  306]
train() client id: f_00004-8-1 loss: 0.881237  [   64/  306]
train() client id: f_00004-8-2 loss: 1.056571  [   96/  306]
train() client id: f_00004-8-3 loss: 0.912867  [  128/  306]
train() client id: f_00004-8-4 loss: 0.969843  [  160/  306]
train() client id: f_00004-8-5 loss: 0.950945  [  192/  306]
train() client id: f_00004-8-6 loss: 0.984519  [  224/  306]
train() client id: f_00004-8-7 loss: 0.940456  [  256/  306]
train() client id: f_00004-8-8 loss: 0.956629  [  288/  306]
train() client id: f_00004-9-0 loss: 0.964755  [   32/  306]
train() client id: f_00004-9-1 loss: 0.935331  [   64/  306]
train() client id: f_00004-9-2 loss: 0.913971  [   96/  306]
train() client id: f_00004-9-3 loss: 0.989624  [  128/  306]
train() client id: f_00004-9-4 loss: 0.928769  [  160/  306]
train() client id: f_00004-9-5 loss: 0.922121  [  192/  306]
train() client id: f_00004-9-6 loss: 0.966440  [  224/  306]
train() client id: f_00004-9-7 loss: 0.983042  [  256/  306]
train() client id: f_00004-9-8 loss: 1.048421  [  288/  306]
train() client id: f_00004-10-0 loss: 0.889356  [   32/  306]
train() client id: f_00004-10-1 loss: 0.936370  [   64/  306]
train() client id: f_00004-10-2 loss: 1.023967  [   96/  306]
train() client id: f_00004-10-3 loss: 1.115224  [  128/  306]
train() client id: f_00004-10-4 loss: 0.927876  [  160/  306]
train() client id: f_00004-10-5 loss: 0.968209  [  192/  306]
train() client id: f_00004-10-6 loss: 0.836370  [  224/  306]
train() client id: f_00004-10-7 loss: 0.920352  [  256/  306]
train() client id: f_00004-10-8 loss: 0.987272  [  288/  306]
train() client id: f_00004-11-0 loss: 0.930210  [   32/  306]
train() client id: f_00004-11-1 loss: 0.985330  [   64/  306]
train() client id: f_00004-11-2 loss: 0.957097  [   96/  306]
train() client id: f_00004-11-3 loss: 0.949962  [  128/  306]
train() client id: f_00004-11-4 loss: 0.995940  [  160/  306]
train() client id: f_00004-11-5 loss: 1.030460  [  192/  306]
train() client id: f_00004-11-6 loss: 0.884433  [  224/  306]
train() client id: f_00004-11-7 loss: 1.019378  [  256/  306]
train() client id: f_00004-11-8 loss: 0.908256  [  288/  306]
train() client id: f_00005-0-0 loss: 0.772855  [   32/  146]
train() client id: f_00005-0-1 loss: 0.622807  [   64/  146]
train() client id: f_00005-0-2 loss: 0.566323  [   96/  146]
train() client id: f_00005-0-3 loss: 0.725371  [  128/  146]
train() client id: f_00005-1-0 loss: 0.688500  [   32/  146]
train() client id: f_00005-1-1 loss: 0.715111  [   64/  146]
train() client id: f_00005-1-2 loss: 0.607435  [   96/  146]
train() client id: f_00005-1-3 loss: 0.844779  [  128/  146]
train() client id: f_00005-2-0 loss: 0.816369  [   32/  146]
train() client id: f_00005-2-1 loss: 0.550158  [   64/  146]
train() client id: f_00005-2-2 loss: 0.748495  [   96/  146]
train() client id: f_00005-2-3 loss: 0.590184  [  128/  146]
train() client id: f_00005-3-0 loss: 0.750115  [   32/  146]
train() client id: f_00005-3-1 loss: 0.628609  [   64/  146]
train() client id: f_00005-3-2 loss: 0.728033  [   96/  146]
train() client id: f_00005-3-3 loss: 0.821803  [  128/  146]
train() client id: f_00005-4-0 loss: 0.690034  [   32/  146]
train() client id: f_00005-4-1 loss: 0.662020  [   64/  146]
train() client id: f_00005-4-2 loss: 0.620793  [   96/  146]
train() client id: f_00005-4-3 loss: 0.842078  [  128/  146]
train() client id: f_00005-5-0 loss: 0.762725  [   32/  146]
train() client id: f_00005-5-1 loss: 0.615200  [   64/  146]
train() client id: f_00005-5-2 loss: 0.612156  [   96/  146]
train() client id: f_00005-5-3 loss: 0.737004  [  128/  146]
train() client id: f_00005-6-0 loss: 0.755700  [   32/  146]
train() client id: f_00005-6-1 loss: 0.669080  [   64/  146]
train() client id: f_00005-6-2 loss: 0.690347  [   96/  146]
train() client id: f_00005-6-3 loss: 0.651845  [  128/  146]
train() client id: f_00005-7-0 loss: 0.567076  [   32/  146]
train() client id: f_00005-7-1 loss: 0.850375  [   64/  146]
train() client id: f_00005-7-2 loss: 0.585561  [   96/  146]
train() client id: f_00005-7-3 loss: 0.683345  [  128/  146]
train() client id: f_00005-8-0 loss: 0.585077  [   32/  146]
train() client id: f_00005-8-1 loss: 0.932564  [   64/  146]
train() client id: f_00005-8-2 loss: 0.888856  [   96/  146]
train() client id: f_00005-8-3 loss: 0.469608  [  128/  146]
train() client id: f_00005-9-0 loss: 0.400733  [   32/  146]
train() client id: f_00005-9-1 loss: 0.808311  [   64/  146]
train() client id: f_00005-9-2 loss: 0.648941  [   96/  146]
train() client id: f_00005-9-3 loss: 0.731881  [  128/  146]
train() client id: f_00005-10-0 loss: 0.825023  [   32/  146]
train() client id: f_00005-10-1 loss: 0.547760  [   64/  146]
train() client id: f_00005-10-2 loss: 0.785447  [   96/  146]
train() client id: f_00005-10-3 loss: 0.651112  [  128/  146]
train() client id: f_00005-11-0 loss: 0.560368  [   32/  146]
train() client id: f_00005-11-1 loss: 0.791564  [   64/  146]
train() client id: f_00005-11-2 loss: 0.838671  [   96/  146]
train() client id: f_00005-11-3 loss: 0.486613  [  128/  146]
train() client id: f_00006-0-0 loss: 0.640344  [   32/   54]
train() client id: f_00006-1-0 loss: 0.682289  [   32/   54]
train() client id: f_00006-2-0 loss: 0.636370  [   32/   54]
train() client id: f_00006-3-0 loss: 0.685052  [   32/   54]
train() client id: f_00006-4-0 loss: 0.631947  [   32/   54]
train() client id: f_00006-5-0 loss: 0.640586  [   32/   54]
train() client id: f_00006-6-0 loss: 0.638869  [   32/   54]
train() client id: f_00006-7-0 loss: 0.673773  [   32/   54]
train() client id: f_00006-8-0 loss: 0.678791  [   32/   54]
train() client id: f_00006-9-0 loss: 0.617092  [   32/   54]
train() client id: f_00006-10-0 loss: 0.633031  [   32/   54]
train() client id: f_00006-11-0 loss: 0.674858  [   32/   54]
train() client id: f_00007-0-0 loss: 0.568625  [   32/  179]
train() client id: f_00007-0-1 loss: 0.429811  [   64/  179]
train() client id: f_00007-0-2 loss: 0.443741  [   96/  179]
train() client id: f_00007-0-3 loss: 0.424319  [  128/  179]
train() client id: f_00007-0-4 loss: 0.425546  [  160/  179]
train() client id: f_00007-1-0 loss: 0.614197  [   32/  179]
train() client id: f_00007-1-1 loss: 0.539523  [   64/  179]
train() client id: f_00007-1-2 loss: 0.347881  [   96/  179]
train() client id: f_00007-1-3 loss: 0.543823  [  128/  179]
train() client id: f_00007-1-4 loss: 0.358205  [  160/  179]
train() client id: f_00007-2-0 loss: 0.335022  [   32/  179]
train() client id: f_00007-2-1 loss: 0.435421  [   64/  179]
train() client id: f_00007-2-2 loss: 0.533667  [   96/  179]
train() client id: f_00007-2-3 loss: 0.553923  [  128/  179]
train() client id: f_00007-2-4 loss: 0.397784  [  160/  179]
train() client id: f_00007-3-0 loss: 0.330645  [   32/  179]
train() client id: f_00007-3-1 loss: 0.622750  [   64/  179]
train() client id: f_00007-3-2 loss: 0.397084  [   96/  179]
train() client id: f_00007-3-3 loss: 0.402191  [  128/  179]
train() client id: f_00007-3-4 loss: 0.514166  [  160/  179]
train() client id: f_00007-4-0 loss: 0.415189  [   32/  179]
train() client id: f_00007-4-1 loss: 0.562050  [   64/  179]
train() client id: f_00007-4-2 loss: 0.393236  [   96/  179]
train() client id: f_00007-4-3 loss: 0.377507  [  128/  179]
train() client id: f_00007-4-4 loss: 0.414511  [  160/  179]
train() client id: f_00007-5-0 loss: 0.615672  [   32/  179]
train() client id: f_00007-5-1 loss: 0.372293  [   64/  179]
train() client id: f_00007-5-2 loss: 0.491090  [   96/  179]
train() client id: f_00007-5-3 loss: 0.430803  [  128/  179]
train() client id: f_00007-5-4 loss: 0.312606  [  160/  179]
train() client id: f_00007-6-0 loss: 0.361537  [   32/  179]
train() client id: f_00007-6-1 loss: 0.409656  [   64/  179]
train() client id: f_00007-6-2 loss: 0.522194  [   96/  179]
train() client id: f_00007-6-3 loss: 0.293269  [  128/  179]
train() client id: f_00007-6-4 loss: 0.399410  [  160/  179]
train() client id: f_00007-7-0 loss: 0.464205  [   32/  179]
train() client id: f_00007-7-1 loss: 0.369565  [   64/  179]
train() client id: f_00007-7-2 loss: 0.388509  [   96/  179]
train() client id: f_00007-7-3 loss: 0.485422  [  128/  179]
train() client id: f_00007-7-4 loss: 0.465726  [  160/  179]
train() client id: f_00007-8-0 loss: 0.353496  [   32/  179]
train() client id: f_00007-8-1 loss: 0.514381  [   64/  179]
train() client id: f_00007-8-2 loss: 0.393714  [   96/  179]
train() client id: f_00007-8-3 loss: 0.297010  [  128/  179]
train() client id: f_00007-8-4 loss: 0.614019  [  160/  179]
train() client id: f_00007-9-0 loss: 0.441932  [   32/  179]
train() client id: f_00007-9-1 loss: 0.289616  [   64/  179]
train() client id: f_00007-9-2 loss: 0.352261  [   96/  179]
train() client id: f_00007-9-3 loss: 0.468053  [  128/  179]
train() client id: f_00007-9-4 loss: 0.447668  [  160/  179]
train() client id: f_00007-10-0 loss: 0.362338  [   32/  179]
train() client id: f_00007-10-1 loss: 0.269336  [   64/  179]
train() client id: f_00007-10-2 loss: 0.389736  [   96/  179]
train() client id: f_00007-10-3 loss: 0.479230  [  128/  179]
train() client id: f_00007-10-4 loss: 0.467407  [  160/  179]
train() client id: f_00007-11-0 loss: 0.366512  [   32/  179]
train() client id: f_00007-11-1 loss: 0.287078  [   64/  179]
train() client id: f_00007-11-2 loss: 0.534266  [   96/  179]
train() client id: f_00007-11-3 loss: 0.391837  [  128/  179]
train() client id: f_00007-11-4 loss: 0.270710  [  160/  179]
train() client id: f_00008-0-0 loss: 0.829542  [   32/  130]
train() client id: f_00008-0-1 loss: 0.844188  [   64/  130]
train() client id: f_00008-0-2 loss: 0.859946  [   96/  130]
train() client id: f_00008-0-3 loss: 0.863127  [  128/  130]
train() client id: f_00008-1-0 loss: 0.819615  [   32/  130]
train() client id: f_00008-1-1 loss: 0.824940  [   64/  130]
train() client id: f_00008-1-2 loss: 0.882262  [   96/  130]
train() client id: f_00008-1-3 loss: 0.864372  [  128/  130]
train() client id: f_00008-2-0 loss: 0.785559  [   32/  130]
train() client id: f_00008-2-1 loss: 0.860533  [   64/  130]
train() client id: f_00008-2-2 loss: 0.839151  [   96/  130]
train() client id: f_00008-2-3 loss: 0.878823  [  128/  130]
train() client id: f_00008-3-0 loss: 0.772834  [   32/  130]
train() client id: f_00008-3-1 loss: 0.839625  [   64/  130]
train() client id: f_00008-3-2 loss: 0.865781  [   96/  130]
train() client id: f_00008-3-3 loss: 0.913808  [  128/  130]
train() client id: f_00008-4-0 loss: 0.847145  [   32/  130]
train() client id: f_00008-4-1 loss: 0.945109  [   64/  130]
train() client id: f_00008-4-2 loss: 0.771603  [   96/  130]
train() client id: f_00008-4-3 loss: 0.823705  [  128/  130]
train() client id: f_00008-5-0 loss: 0.946258  [   32/  130]
train() client id: f_00008-5-1 loss: 0.799431  [   64/  130]
train() client id: f_00008-5-2 loss: 0.732289  [   96/  130]
train() client id: f_00008-5-3 loss: 0.867620  [  128/  130]
train() client id: f_00008-6-0 loss: 0.803391  [   32/  130]
train() client id: f_00008-6-1 loss: 0.755192  [   64/  130]
train() client id: f_00008-6-2 loss: 0.886072  [   96/  130]
train() client id: f_00008-6-3 loss: 0.934933  [  128/  130]
train() client id: f_00008-7-0 loss: 0.799595  [   32/  130]
train() client id: f_00008-7-1 loss: 0.798759  [   64/  130]
train() client id: f_00008-7-2 loss: 0.904005  [   96/  130]
train() client id: f_00008-7-3 loss: 0.890433  [  128/  130]
train() client id: f_00008-8-0 loss: 0.732426  [   32/  130]
train() client id: f_00008-8-1 loss: 0.994483  [   64/  130]
train() client id: f_00008-8-2 loss: 0.748122  [   96/  130]
train() client id: f_00008-8-3 loss: 0.871653  [  128/  130]
train() client id: f_00008-9-0 loss: 0.881062  [   32/  130]
train() client id: f_00008-9-1 loss: 0.802484  [   64/  130]
train() client id: f_00008-9-2 loss: 0.819127  [   96/  130]
train() client id: f_00008-9-3 loss: 0.845573  [  128/  130]
train() client id: f_00008-10-0 loss: 0.913380  [   32/  130]
train() client id: f_00008-10-1 loss: 0.735839  [   64/  130]
train() client id: f_00008-10-2 loss: 0.778192  [   96/  130]
train() client id: f_00008-10-3 loss: 0.926550  [  128/  130]
train() client id: f_00008-11-0 loss: 0.853432  [   32/  130]
train() client id: f_00008-11-1 loss: 0.839427  [   64/  130]
train() client id: f_00008-11-2 loss: 0.880684  [   96/  130]
train() client id: f_00008-11-3 loss: 0.811082  [  128/  130]
train() client id: f_00009-0-0 loss: 1.114991  [   32/  118]
train() client id: f_00009-0-1 loss: 1.194719  [   64/  118]
train() client id: f_00009-0-2 loss: 1.167378  [   96/  118]
train() client id: f_00009-1-0 loss: 1.125552  [   32/  118]
train() client id: f_00009-1-1 loss: 1.064058  [   64/  118]
train() client id: f_00009-1-2 loss: 1.083554  [   96/  118]
train() client id: f_00009-2-0 loss: 0.988247  [   32/  118]
train() client id: f_00009-2-1 loss: 1.158234  [   64/  118]
train() client id: f_00009-2-2 loss: 0.994907  [   96/  118]
train() client id: f_00009-3-0 loss: 1.105232  [   32/  118]
train() client id: f_00009-3-1 loss: 0.958185  [   64/  118]
train() client id: f_00009-3-2 loss: 1.006633  [   96/  118]
train() client id: f_00009-4-0 loss: 1.020833  [   32/  118]
train() client id: f_00009-4-1 loss: 1.008797  [   64/  118]
train() client id: f_00009-4-2 loss: 0.945668  [   96/  118]
train() client id: f_00009-5-0 loss: 0.912089  [   32/  118]
train() client id: f_00009-5-1 loss: 0.920304  [   64/  118]
train() client id: f_00009-5-2 loss: 1.140502  [   96/  118]
train() client id: f_00009-6-0 loss: 0.872787  [   32/  118]
train() client id: f_00009-6-1 loss: 0.991266  [   64/  118]
train() client id: f_00009-6-2 loss: 0.909334  [   96/  118]
train() client id: f_00009-7-0 loss: 0.954194  [   32/  118]
train() client id: f_00009-7-1 loss: 0.900701  [   64/  118]
train() client id: f_00009-7-2 loss: 1.019728  [   96/  118]
train() client id: f_00009-8-0 loss: 1.085732  [   32/  118]
train() client id: f_00009-8-1 loss: 0.906449  [   64/  118]
train() client id: f_00009-8-2 loss: 0.848038  [   96/  118]
train() client id: f_00009-9-0 loss: 0.883703  [   32/  118]
train() client id: f_00009-9-1 loss: 0.933348  [   64/  118]
train() client id: f_00009-9-2 loss: 0.896156  [   96/  118]
train() client id: f_00009-10-0 loss: 0.934894  [   32/  118]
train() client id: f_00009-10-1 loss: 0.769691  [   64/  118]
train() client id: f_00009-10-2 loss: 0.982937  [   96/  118]
train() client id: f_00009-11-0 loss: 1.016557  [   32/  118]
train() client id: f_00009-11-1 loss: 0.804247  [   64/  118]
train() client id: f_00009-11-2 loss: 0.941069  [   96/  118]
At round 9 accuracy: 0.6286472148541115
At round 9 training accuracy: 0.5754527162977867
At round 9 training loss: 0.8522337503354807
update_location
xs = -4.528292 -53.998411 25.045120 44.056472 -145.103519 -80.217951 2.784040 18.375741 -1.680116 -35.304393 
ys = 32.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 -3.124922 -19.177652 37.845030 -0.998518 
xs mean: -23.05713084058713
ys mean: 7.871751218646876
dists_uav = 105.273361 114.707508 103.097052 111.576170 176.472256 129.345557 100.087542 103.467145 106.934882 106.053747 
uav_gains = -100.557995 -101.489909 -100.331181 -101.189363 -106.202989 -102.794399 -100.009517 -100.370088 -100.728027 -100.638187 
uav_gains_db_mean: -101.43116543872839
dists_bs = 222.130125 200.159499 264.919948 266.886340 168.326053 214.287278 251.665756 274.042219 221.023230 224.700559 
bs_gains = -105.272068 -104.005593 -107.414270 -107.504197 -101.899297 -104.834958 -106.790135 -107.825948 -105.211321 -105.411975 
bs_gains_db_mean: -105.61697616217305
Round 10
-------------------------------
ene_coms = [0.0064518  0.00755535 0.00639025 0.00662949 0.00684118 0.00787818
 0.00630491 0.00640072 0.0080337  0.00811908]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.66427415 20.17526442  9.51189454  3.40884946 23.24962924 11.2083458
  4.23035356 13.66026199 10.00551345  9.10260735]
obj_prev = 114.2169939675041
eta_min = 1.8357285973362503e-10	eta_max = 0.9306980540715724
af = 24.17431921386036	bf = 1.747969917600228	zeta = 26.5917511352464	eta = 0.909090909090909
af = 24.17431921386036	bf = 1.747969917600228	zeta = 44.5723149137244	eta = 0.5423617611212913
af = 24.17431921386036	bf = 1.747969917600228	zeta = 36.13848212917933	eta = 0.6689356550019921
af = 24.17431921386036	bf = 1.747969917600228	zeta = 34.638812270040795	eta = 0.6978968858804895
af = 24.17431921386036	bf = 1.747969917600228	zeta = 34.567944549978165	eta = 0.6993276438206862
af = 24.17431921386036	bf = 1.747969917600228	zeta = 34.56777605205609	eta = 0.699331052638617
eta = 0.699331052638617
ene_coms = [0.0064518  0.00755535 0.00639025 0.00662949 0.00684118 0.00787818
 0.00630491 0.00640072 0.0080337  0.00811908]
ene_comp = [0.02951086 0.06206648 0.02904243 0.01007117 0.07166922 0.03419512
 0.01264751 0.04192415 0.03044771 0.02763715]
ene_total = [2.96117301 5.73267602 2.91753393 1.3751358  6.46456318 3.46432422
 1.56054611 3.97908038 3.1685676  2.94417579]
ti_comp = [0.29375112 0.28271571 0.29436671 0.29197422 0.28985734 0.27948737
 0.29522008 0.29426194 0.27793212 0.27707834]
ti_coms = [0.06451804 0.07555345 0.06390245 0.06629494 0.06841182 0.07878179
 0.06304909 0.06400722 0.08033704 0.08119082]
t_total = [29.49995804 29.49995804 29.49995804 29.49995804 29.49995804 29.49995804
 29.49995804 29.49995804 29.49995804 29.49995804]
ene_coms = [0.0064518  0.00755535 0.00639025 0.00662949 0.00684118 0.00787818
 0.00630491 0.00640072 0.0080337  0.00811908]
ene_comp = [1.86151434e-05 1.86961231e-04 1.76685735e-05 7.48910929e-07
 2.73847940e-04 3.19924872e-05 1.45078588e-06 5.31869055e-05
 2.28384371e-05 1.71852040e-05]
ene_total = [0.53277568 0.63750315 0.52762897 0.54593561 0.58585304 0.65132518
 0.51926695 0.53141623 0.66337738 0.66994199]
optimize_network iter = 0 obj = 5.865024177832159
eta = 0.699331052638617
freqs = [5.02310532e+07 1.09768367e+08 4.93303576e+07 1.72466686e+07
 1.23628440e+08 6.11747126e+07 2.14204772e+07 7.12361122e+07
 5.47754402e+07 4.98724502e+07]
eta_min = 0.6862525218299714	eta_max = 0.6993310526386036
af = 0.04323094606161005	bf = 1.747969917600228	zeta = 0.04755404066777106	eta = 0.909090909090909
af = 0.04323094606161005	bf = 1.747969917600228	zeta = 19.27299293526715	eta = 0.0022430842063197597
af = 0.04323094606161005	bf = 1.747969917600228	zeta = 2.016197015708236	eta = 0.021441826232653248
af = 0.04323094606161005	bf = 1.747969917600228	zeta = 1.9560216906724948	eta = 0.022101465575643454
af = 0.04323094606161005	bf = 1.747969917600228	zeta = 1.9560016000865066	eta = 0.022101692585373197
eta = 0.022101692585373197
ene_coms = [0.0064518  0.00755535 0.00639025 0.00662949 0.00684118 0.00787818
 0.00630491 0.00640072 0.0080337  0.00811908]
ene_comp = [1.98424635e-04 1.99287822e-03 1.88334850e-04 7.98287578e-06
 2.91903083e-03 3.41017925e-04 1.54643803e-05 5.66935857e-04
 2.43442042e-04 1.83182463e-04]
ene_total = [0.16836175 0.24172938 0.16654785 0.16803892 0.24709624 0.20808283
 0.1600109  0.17639801 0.2095499  0.21018583]
ti_comp = [0.30933515 0.29829974 0.30995074 0.30755825 0.30544137 0.2950714
 0.31080411 0.30984597 0.29351615 0.29266237]
ti_coms = [0.06451804 0.07555345 0.06390245 0.06629494 0.06841182 0.07878179
 0.06304909 0.06400722 0.08033704 0.08119082]
t_total = [29.49995804 29.49995804 29.49995804 29.49995804 29.49995804 29.49995804
 29.49995804 29.49995804 29.49995804 29.49995804]
ene_coms = [0.0064518  0.00755535 0.00639025 0.00662949 0.00684118 0.00787818
 0.00630491 0.00640072 0.0080337  0.00811908]
ene_comp = [1.95879725e-05 1.95960380e-04 1.85958529e-05 7.87566077e-07
 2.87769623e-04 3.34919725e-05 1.52736973e-06 5.59762484e-05
 2.38947470e-05 1.79741622e-05]
ene_total = [0.51064374 0.611639   0.50570798 0.52318139 0.56253035 0.62429308
 0.49762741 0.5094843  0.63580791 0.64207778]
optimize_network iter = 1 obj = 5.622992947687243
eta = 0.6862525218299714
freqs = [5.02184614e+07 1.09525472e+08 4.93231881e+07 1.72370605e+07
 1.23513860e+08 6.10025328e+07 2.14204772e+07 7.12244839e+07
 5.46051337e+07 4.97092553e+07]
eta_min = 0.6862525218299769	eta_max = 0.6862525218299705
af = 0.04310630861602106	bf = 1.747969917600228	zeta = 0.047416939477623175	eta = 0.909090909090909
af = 0.04310630861602106	bf = 1.747969917600228	zeta = 19.27286226387631	eta = 0.0022366324226171888
af = 0.04310630861602106	bf = 1.747969917600228	zeta = 2.0155464414985715	eta = 0.02138690914210404
af = 0.04310630861602106	bf = 1.747969917600228	zeta = 1.9555355114717208	eta = 0.022043224663089646
af = 0.04310630861602106	bf = 1.747969917600228	zeta = 1.9555155774183015	eta = 0.022043449366397072
eta = 0.022043449366397072
ene_coms = [0.0064518  0.00755535 0.00639025 0.00662949 0.00684118 0.00787818
 0.00630491 0.00640072 0.0080337  0.00811908]
ene_comp = [1.98462446e-04 1.98544165e-03 1.88410436e-04 7.97950323e-06
 2.91563934e-03 3.39335721e-04 1.54750847e-05 5.67143085e-04
 2.42098050e-04 1.82111558e-04]
ene_total = [0.16835268 0.24152672 0.16653984 0.16802883 0.24699566 0.20802785
 0.16000164 0.17639275 0.2095034  0.2101462 ]
ti_comp = [0.30933515 0.29829974 0.30995074 0.30755825 0.30544137 0.2950714
 0.31080411 0.30984597 0.29351615 0.29266237]
ti_coms = [0.06451804 0.07555345 0.06390245 0.06629494 0.06841182 0.07878179
 0.06304909 0.06400722 0.08033704 0.08119082]
t_total = [29.49995804 29.49995804 29.49995804 29.49995804 29.49995804 29.49995804
 29.49995804 29.49995804 29.49995804 29.49995804]
ene_coms = [0.0064518  0.00755535 0.00639025 0.00662949 0.00684118 0.00787818
 0.00630491 0.00640072 0.0080337  0.00811908]
ene_comp = [1.95879725e-05 1.95960380e-04 1.85958529e-05 7.87566077e-07
 2.87769623e-04 3.34919725e-05 1.52736973e-06 5.59762484e-05
 2.38947470e-05 1.79741622e-05]
ene_total = [0.51064374 0.611639   0.50570798 0.52318139 0.56253035 0.62429308
 0.49762741 0.5094843  0.63580791 0.64207778]
optimize_network iter = 2 obj = 5.62299294768734
eta = 0.6862525218299769
freqs = [5.02184614e+07 1.09525472e+08 4.93231881e+07 1.72370605e+07
 1.23513860e+08 6.10025328e+07 2.14204772e+07 7.12244839e+07
 5.46051337e+07 4.97092553e+07]
Done!
ene_coms = [0.0064518  0.00755535 0.00639025 0.00662949 0.00684118 0.00787818
 0.00630491 0.00640072 0.0080337  0.00811908]
ene_comp = [1.90655176e-05 1.90733680e-04 1.80998600e-05 7.66559934e-07
 2.80094166e-04 3.25986670e-05 1.48663138e-06 5.44832371e-05
 2.32574209e-05 1.74947512e-05]
ene_total = [0.00647087 0.00774608 0.00640835 0.00663026 0.00712128 0.00791078
 0.0063064  0.00645521 0.00805696 0.00813658]
At round 10 energy consumption: 0.07124274677677302
At round 10 eta: 0.6862525218299769
At round 10 a_n: 24.7571443865612
At round 10 local rounds: 12.328837574393813
At round 10 global rounds: 78.90786734274575
gradient difference: 0.42728954553604126
train() client id: f_00000-0-0 loss: 1.148476  [   32/  126]
train() client id: f_00000-0-1 loss: 1.192305  [   64/  126]
train() client id: f_00000-0-2 loss: 0.860969  [   96/  126]
train() client id: f_00000-1-0 loss: 1.019288  [   32/  126]
train() client id: f_00000-1-1 loss: 1.046859  [   64/  126]
train() client id: f_00000-1-2 loss: 0.988742  [   96/  126]
train() client id: f_00000-2-0 loss: 0.924047  [   32/  126]
train() client id: f_00000-2-1 loss: 0.962734  [   64/  126]
train() client id: f_00000-2-2 loss: 0.910491  [   96/  126]
train() client id: f_00000-3-0 loss: 0.885207  [   32/  126]
train() client id: f_00000-3-1 loss: 0.933089  [   64/  126]
train() client id: f_00000-3-2 loss: 0.843359  [   96/  126]
train() client id: f_00000-4-0 loss: 0.865960  [   32/  126]
train() client id: f_00000-4-1 loss: 0.901185  [   64/  126]
train() client id: f_00000-4-2 loss: 0.832701  [   96/  126]
train() client id: f_00000-5-0 loss: 0.931295  [   32/  126]
train() client id: f_00000-5-1 loss: 0.848313  [   64/  126]
train() client id: f_00000-5-2 loss: 0.877727  [   96/  126]
train() client id: f_00000-6-0 loss: 0.947776  [   32/  126]
train() client id: f_00000-6-1 loss: 0.896311  [   64/  126]
train() client id: f_00000-6-2 loss: 0.799859  [   96/  126]
train() client id: f_00000-7-0 loss: 1.009386  [   32/  126]
train() client id: f_00000-7-1 loss: 0.722321  [   64/  126]
train() client id: f_00000-7-2 loss: 0.858778  [   96/  126]
train() client id: f_00000-8-0 loss: 0.820374  [   32/  126]
train() client id: f_00000-8-1 loss: 0.842897  [   64/  126]
train() client id: f_00000-8-2 loss: 0.892555  [   96/  126]
train() client id: f_00000-9-0 loss: 0.865538  [   32/  126]
train() client id: f_00000-9-1 loss: 0.952063  [   64/  126]
train() client id: f_00000-9-2 loss: 0.824533  [   96/  126]
train() client id: f_00000-10-0 loss: 0.964758  [   32/  126]
train() client id: f_00000-10-1 loss: 0.915788  [   64/  126]
train() client id: f_00000-10-2 loss: 0.741289  [   96/  126]
train() client id: f_00000-11-0 loss: 0.835685  [   32/  126]
train() client id: f_00000-11-1 loss: 0.777537  [   64/  126]
train() client id: f_00000-11-2 loss: 0.915488  [   96/  126]
train() client id: f_00001-0-0 loss: 0.429722  [   32/  265]
train() client id: f_00001-0-1 loss: 0.466581  [   64/  265]
train() client id: f_00001-0-2 loss: 0.505734  [   96/  265]
train() client id: f_00001-0-3 loss: 0.420384  [  128/  265]
train() client id: f_00001-0-4 loss: 0.432209  [  160/  265]
train() client id: f_00001-0-5 loss: 0.467559  [  192/  265]
train() client id: f_00001-0-6 loss: 0.462401  [  224/  265]
train() client id: f_00001-0-7 loss: 0.590484  [  256/  265]
train() client id: f_00001-1-0 loss: 0.389653  [   32/  265]
train() client id: f_00001-1-1 loss: 0.536927  [   64/  265]
train() client id: f_00001-1-2 loss: 0.405557  [   96/  265]
train() client id: f_00001-1-3 loss: 0.479575  [  128/  265]
train() client id: f_00001-1-4 loss: 0.391087  [  160/  265]
train() client id: f_00001-1-5 loss: 0.436053  [  192/  265]
train() client id: f_00001-1-6 loss: 0.506419  [  224/  265]
train() client id: f_00001-1-7 loss: 0.532176  [  256/  265]
train() client id: f_00001-2-0 loss: 0.466168  [   32/  265]
train() client id: f_00001-2-1 loss: 0.524595  [   64/  265]
train() client id: f_00001-2-2 loss: 0.380123  [   96/  265]
train() client id: f_00001-2-3 loss: 0.384195  [  128/  265]
train() client id: f_00001-2-4 loss: 0.404131  [  160/  265]
train() client id: f_00001-2-5 loss: 0.462949  [  192/  265]
train() client id: f_00001-2-6 loss: 0.440658  [  224/  265]
train() client id: f_00001-2-7 loss: 0.529655  [  256/  265]
train() client id: f_00001-3-0 loss: 0.433902  [   32/  265]
train() client id: f_00001-3-1 loss: 0.381845  [   64/  265]
train() client id: f_00001-3-2 loss: 0.442036  [   96/  265]
train() client id: f_00001-3-3 loss: 0.389105  [  128/  265]
train() client id: f_00001-3-4 loss: 0.374622  [  160/  265]
train() client id: f_00001-3-5 loss: 0.460430  [  192/  265]
train() client id: f_00001-3-6 loss: 0.498969  [  224/  265]
train() client id: f_00001-3-7 loss: 0.541901  [  256/  265]
train() client id: f_00001-4-0 loss: 0.407883  [   32/  265]
train() client id: f_00001-4-1 loss: 0.407505  [   64/  265]
train() client id: f_00001-4-2 loss: 0.465960  [   96/  265]
train() client id: f_00001-4-3 loss: 0.437352  [  128/  265]
train() client id: f_00001-4-4 loss: 0.491558  [  160/  265]
train() client id: f_00001-4-5 loss: 0.392802  [  192/  265]
train() client id: f_00001-4-6 loss: 0.376744  [  224/  265]
train() client id: f_00001-4-7 loss: 0.479515  [  256/  265]
train() client id: f_00001-5-0 loss: 0.388135  [   32/  265]
train() client id: f_00001-5-1 loss: 0.406149  [   64/  265]
train() client id: f_00001-5-2 loss: 0.525835  [   96/  265]
train() client id: f_00001-5-3 loss: 0.332817  [  128/  265]
train() client id: f_00001-5-4 loss: 0.431051  [  160/  265]
train() client id: f_00001-5-5 loss: 0.454302  [  192/  265]
train() client id: f_00001-5-6 loss: 0.480541  [  224/  265]
train() client id: f_00001-5-7 loss: 0.412643  [  256/  265]
train() client id: f_00001-6-0 loss: 0.447253  [   32/  265]
train() client id: f_00001-6-1 loss: 0.371340  [   64/  265]
train() client id: f_00001-6-2 loss: 0.448731  [   96/  265]
train() client id: f_00001-6-3 loss: 0.430300  [  128/  265]
train() client id: f_00001-6-4 loss: 0.415328  [  160/  265]
train() client id: f_00001-6-5 loss: 0.356657  [  192/  265]
train() client id: f_00001-6-6 loss: 0.431773  [  224/  265]
train() client id: f_00001-6-7 loss: 0.513116  [  256/  265]
train() client id: f_00001-7-0 loss: 0.429487  [   32/  265]
train() client id: f_00001-7-1 loss: 0.415251  [   64/  265]
train() client id: f_00001-7-2 loss: 0.446480  [   96/  265]
train() client id: f_00001-7-3 loss: 0.414522  [  128/  265]
train() client id: f_00001-7-4 loss: 0.383432  [  160/  265]
train() client id: f_00001-7-5 loss: 0.372108  [  192/  265]
train() client id: f_00001-7-6 loss: 0.429511  [  224/  265]
train() client id: f_00001-7-7 loss: 0.492454  [  256/  265]
train() client id: f_00001-8-0 loss: 0.442535  [   32/  265]
train() client id: f_00001-8-1 loss: 0.394855  [   64/  265]
train() client id: f_00001-8-2 loss: 0.357216  [   96/  265]
train() client id: f_00001-8-3 loss: 0.487737  [  128/  265]
train() client id: f_00001-8-4 loss: 0.424819  [  160/  265]
train() client id: f_00001-8-5 loss: 0.391499  [  192/  265]
train() client id: f_00001-8-6 loss: 0.460230  [  224/  265]
train() client id: f_00001-8-7 loss: 0.336008  [  256/  265]
train() client id: f_00001-9-0 loss: 0.393760  [   32/  265]
train() client id: f_00001-9-1 loss: 0.392741  [   64/  265]
train() client id: f_00001-9-2 loss: 0.427637  [   96/  265]
train() client id: f_00001-9-3 loss: 0.476831  [  128/  265]
train() client id: f_00001-9-4 loss: 0.464771  [  160/  265]
train() client id: f_00001-9-5 loss: 0.321457  [  192/  265]
train() client id: f_00001-9-6 loss: 0.384532  [  224/  265]
train() client id: f_00001-9-7 loss: 0.410884  [  256/  265]
train() client id: f_00001-10-0 loss: 0.453471  [   32/  265]
train() client id: f_00001-10-1 loss: 0.417576  [   64/  265]
train() client id: f_00001-10-2 loss: 0.498640  [   96/  265]
train() client id: f_00001-10-3 loss: 0.467233  [  128/  265]
train() client id: f_00001-10-4 loss: 0.382034  [  160/  265]
train() client id: f_00001-10-5 loss: 0.447380  [  192/  265]
train() client id: f_00001-10-6 loss: 0.334482  [  224/  265]
train() client id: f_00001-10-7 loss: 0.342558  [  256/  265]
train() client id: f_00001-11-0 loss: 0.363538  [   32/  265]
train() client id: f_00001-11-1 loss: 0.324970  [   64/  265]
train() client id: f_00001-11-2 loss: 0.420833  [   96/  265]
train() client id: f_00001-11-3 loss: 0.396335  [  128/  265]
train() client id: f_00001-11-4 loss: 0.460298  [  160/  265]
train() client id: f_00001-11-5 loss: 0.372732  [  192/  265]
train() client id: f_00001-11-6 loss: 0.510131  [  224/  265]
train() client id: f_00001-11-7 loss: 0.493526  [  256/  265]
train() client id: f_00002-0-0 loss: 1.274504  [   32/  124]
train() client id: f_00002-0-1 loss: 1.221982  [   64/  124]
train() client id: f_00002-0-2 loss: 1.004214  [   96/  124]
train() client id: f_00002-1-0 loss: 1.006456  [   32/  124]
train() client id: f_00002-1-1 loss: 1.152350  [   64/  124]
train() client id: f_00002-1-2 loss: 1.226706  [   96/  124]
train() client id: f_00002-2-0 loss: 1.166396  [   32/  124]
train() client id: f_00002-2-1 loss: 1.094333  [   64/  124]
train() client id: f_00002-2-2 loss: 0.996064  [   96/  124]
train() client id: f_00002-3-0 loss: 1.204872  [   32/  124]
train() client id: f_00002-3-1 loss: 0.967678  [   64/  124]
train() client id: f_00002-3-2 loss: 1.039501  [   96/  124]
train() client id: f_00002-4-0 loss: 1.003813  [   32/  124]
train() client id: f_00002-4-1 loss: 1.016860  [   64/  124]
train() client id: f_00002-4-2 loss: 1.037247  [   96/  124]
train() client id: f_00002-5-0 loss: 1.080398  [   32/  124]
train() client id: f_00002-5-1 loss: 0.821931  [   64/  124]
train() client id: f_00002-5-2 loss: 1.002427  [   96/  124]
train() client id: f_00002-6-0 loss: 0.971239  [   32/  124]
train() client id: f_00002-6-1 loss: 0.908889  [   64/  124]
train() client id: f_00002-6-2 loss: 0.911877  [   96/  124]
train() client id: f_00002-7-0 loss: 0.976413  [   32/  124]
train() client id: f_00002-7-1 loss: 0.856583  [   64/  124]
train() client id: f_00002-7-2 loss: 0.824082  [   96/  124]
train() client id: f_00002-8-0 loss: 0.978417  [   32/  124]
train() client id: f_00002-8-1 loss: 0.963603  [   64/  124]
train() client id: f_00002-8-2 loss: 0.847825  [   96/  124]
train() client id: f_00002-9-0 loss: 1.026124  [   32/  124]
train() client id: f_00002-9-1 loss: 0.751067  [   64/  124]
train() client id: f_00002-9-2 loss: 0.851523  [   96/  124]
train() client id: f_00002-10-0 loss: 0.807846  [   32/  124]
train() client id: f_00002-10-1 loss: 0.926822  [   64/  124]
train() client id: f_00002-10-2 loss: 0.920477  [   96/  124]
train() client id: f_00002-11-0 loss: 0.770284  [   32/  124]
train() client id: f_00002-11-1 loss: 0.945918  [   64/  124]
train() client id: f_00002-11-2 loss: 0.871233  [   96/  124]
train() client id: f_00003-0-0 loss: 0.834471  [   32/   43]
train() client id: f_00003-1-0 loss: 0.979831  [   32/   43]
train() client id: f_00003-2-0 loss: 0.959400  [   32/   43]
train() client id: f_00003-3-0 loss: 0.922237  [   32/   43]
train() client id: f_00003-4-0 loss: 0.936110  [   32/   43]
train() client id: f_00003-5-0 loss: 0.973767  [   32/   43]
train() client id: f_00003-6-0 loss: 0.912350  [   32/   43]
train() client id: f_00003-7-0 loss: 0.976878  [   32/   43]
train() client id: f_00003-8-0 loss: 1.041914  [   32/   43]
train() client id: f_00003-9-0 loss: 0.940778  [   32/   43]
train() client id: f_00003-10-0 loss: 0.844741  [   32/   43]
train() client id: f_00003-11-0 loss: 0.848451  [   32/   43]
train() client id: f_00004-0-0 loss: 0.931347  [   32/  306]
train() client id: f_00004-0-1 loss: 0.851164  [   64/  306]
train() client id: f_00004-0-2 loss: 0.947126  [   96/  306]
train() client id: f_00004-0-3 loss: 1.034901  [  128/  306]
train() client id: f_00004-0-4 loss: 1.008116  [  160/  306]
train() client id: f_00004-0-5 loss: 0.942039  [  192/  306]
train() client id: f_00004-0-6 loss: 1.093052  [  224/  306]
train() client id: f_00004-0-7 loss: 0.897673  [  256/  306]
train() client id: f_00004-0-8 loss: 0.970504  [  288/  306]
train() client id: f_00004-1-0 loss: 0.969660  [   32/  306]
train() client id: f_00004-1-1 loss: 0.924481  [   64/  306]
train() client id: f_00004-1-2 loss: 1.111573  [   96/  306]
train() client id: f_00004-1-3 loss: 0.801740  [  128/  306]
train() client id: f_00004-1-4 loss: 0.887415  [  160/  306]
train() client id: f_00004-1-5 loss: 0.959754  [  192/  306]
train() client id: f_00004-1-6 loss: 1.003271  [  224/  306]
train() client id: f_00004-1-7 loss: 0.996734  [  256/  306]
train() client id: f_00004-1-8 loss: 0.930681  [  288/  306]
train() client id: f_00004-2-0 loss: 0.969774  [   32/  306]
train() client id: f_00004-2-1 loss: 0.897787  [   64/  306]
train() client id: f_00004-2-2 loss: 0.988440  [   96/  306]
train() client id: f_00004-2-3 loss: 0.944734  [  128/  306]
train() client id: f_00004-2-4 loss: 0.958299  [  160/  306]
train() client id: f_00004-2-5 loss: 0.798061  [  192/  306]
train() client id: f_00004-2-6 loss: 0.833389  [  224/  306]
train() client id: f_00004-2-7 loss: 1.062805  [  256/  306]
train() client id: f_00004-2-8 loss: 0.998841  [  288/  306]
train() client id: f_00004-3-0 loss: 1.060829  [   32/  306]
train() client id: f_00004-3-1 loss: 0.918918  [   64/  306]
train() client id: f_00004-3-2 loss: 0.919687  [   96/  306]
train() client id: f_00004-3-3 loss: 1.087090  [  128/  306]
train() client id: f_00004-3-4 loss: 0.910993  [  160/  306]
train() client id: f_00004-3-5 loss: 0.916874  [  192/  306]
train() client id: f_00004-3-6 loss: 0.921820  [  224/  306]
train() client id: f_00004-3-7 loss: 0.842019  [  256/  306]
train() client id: f_00004-3-8 loss: 0.947172  [  288/  306]
train() client id: f_00004-4-0 loss: 0.894936  [   32/  306]
train() client id: f_00004-4-1 loss: 1.027798  [   64/  306]
train() client id: f_00004-4-2 loss: 1.077914  [   96/  306]
train() client id: f_00004-4-3 loss: 1.029321  [  128/  306]
train() client id: f_00004-4-4 loss: 0.860610  [  160/  306]
train() client id: f_00004-4-5 loss: 0.953859  [  192/  306]
train() client id: f_00004-4-6 loss: 0.921957  [  224/  306]
train() client id: f_00004-4-7 loss: 0.854687  [  256/  306]
train() client id: f_00004-4-8 loss: 0.938997  [  288/  306]
train() client id: f_00004-5-0 loss: 0.966783  [   32/  306]
train() client id: f_00004-5-1 loss: 1.034724  [   64/  306]
train() client id: f_00004-5-2 loss: 0.962817  [   96/  306]
train() client id: f_00004-5-3 loss: 0.917806  [  128/  306]
train() client id: f_00004-5-4 loss: 0.813148  [  160/  306]
train() client id: f_00004-5-5 loss: 0.917834  [  192/  306]
train() client id: f_00004-5-6 loss: 1.042086  [  224/  306]
train() client id: f_00004-5-7 loss: 0.794242  [  256/  306]
train() client id: f_00004-5-8 loss: 0.976211  [  288/  306]
train() client id: f_00004-6-0 loss: 0.915497  [   32/  306]
train() client id: f_00004-6-1 loss: 0.922072  [   64/  306]
train() client id: f_00004-6-2 loss: 0.979516  [   96/  306]
train() client id: f_00004-6-3 loss: 0.896830  [  128/  306]
train() client id: f_00004-6-4 loss: 0.930245  [  160/  306]
train() client id: f_00004-6-5 loss: 0.944204  [  192/  306]
train() client id: f_00004-6-6 loss: 0.880087  [  224/  306]
train() client id: f_00004-6-7 loss: 1.015652  [  256/  306]
train() client id: f_00004-6-8 loss: 1.035047  [  288/  306]
train() client id: f_00004-7-0 loss: 0.883827  [   32/  306]
train() client id: f_00004-7-1 loss: 0.952876  [   64/  306]
train() client id: f_00004-7-2 loss: 0.994577  [   96/  306]
train() client id: f_00004-7-3 loss: 0.994070  [  128/  306]
train() client id: f_00004-7-4 loss: 0.873460  [  160/  306]
train() client id: f_00004-7-5 loss: 1.007686  [  192/  306]
train() client id: f_00004-7-6 loss: 0.902466  [  224/  306]
train() client id: f_00004-7-7 loss: 0.912832  [  256/  306]
train() client id: f_00004-7-8 loss: 0.895372  [  288/  306]
train() client id: f_00004-8-0 loss: 0.961987  [   32/  306]
train() client id: f_00004-8-1 loss: 0.953079  [   64/  306]
train() client id: f_00004-8-2 loss: 0.888495  [   96/  306]
train() client id: f_00004-8-3 loss: 0.972302  [  128/  306]
train() client id: f_00004-8-4 loss: 0.905728  [  160/  306]
train() client id: f_00004-8-5 loss: 1.042216  [  192/  306]
train() client id: f_00004-8-6 loss: 0.950918  [  224/  306]
train() client id: f_00004-8-7 loss: 0.917862  [  256/  306]
train() client id: f_00004-8-8 loss: 0.886243  [  288/  306]
train() client id: f_00004-9-0 loss: 0.941854  [   32/  306]
train() client id: f_00004-9-1 loss: 0.983510  [   64/  306]
train() client id: f_00004-9-2 loss: 0.995861  [   96/  306]
train() client id: f_00004-9-3 loss: 0.977681  [  128/  306]
train() client id: f_00004-9-4 loss: 0.880415  [  160/  306]
train() client id: f_00004-9-5 loss: 0.926122  [  192/  306]
train() client id: f_00004-9-6 loss: 0.915675  [  224/  306]
train() client id: f_00004-9-7 loss: 0.925155  [  256/  306]
train() client id: f_00004-9-8 loss: 0.925073  [  288/  306]
train() client id: f_00004-10-0 loss: 0.981018  [   32/  306]
train() client id: f_00004-10-1 loss: 0.950275  [   64/  306]
train() client id: f_00004-10-2 loss: 0.894856  [   96/  306]
train() client id: f_00004-10-3 loss: 1.015195  [  128/  306]
train() client id: f_00004-10-4 loss: 0.990418  [  160/  306]
train() client id: f_00004-10-5 loss: 0.850076  [  192/  306]
train() client id: f_00004-10-6 loss: 0.877147  [  224/  306]
train() client id: f_00004-10-7 loss: 0.965521  [  256/  306]
train() client id: f_00004-10-8 loss: 0.903375  [  288/  306]
train() client id: f_00004-11-0 loss: 0.897585  [   32/  306]
train() client id: f_00004-11-1 loss: 0.965503  [   64/  306]
train() client id: f_00004-11-2 loss: 1.102822  [   96/  306]
train() client id: f_00004-11-3 loss: 0.801412  [  128/  306]
train() client id: f_00004-11-4 loss: 0.919647  [  160/  306]
train() client id: f_00004-11-5 loss: 0.937954  [  192/  306]
train() client id: f_00004-11-6 loss: 0.928896  [  224/  306]
train() client id: f_00004-11-7 loss: 0.906495  [  256/  306]
train() client id: f_00004-11-8 loss: 0.994394  [  288/  306]
train() client id: f_00005-0-0 loss: 1.034878  [   32/  146]
train() client id: f_00005-0-1 loss: 0.721587  [   64/  146]
train() client id: f_00005-0-2 loss: 0.848192  [   96/  146]
train() client id: f_00005-0-3 loss: 1.005843  [  128/  146]
train() client id: f_00005-1-0 loss: 0.809243  [   32/  146]
train() client id: f_00005-1-1 loss: 0.807918  [   64/  146]
train() client id: f_00005-1-2 loss: 0.907276  [   96/  146]
train() client id: f_00005-1-3 loss: 0.993840  [  128/  146]
train() client id: f_00005-2-0 loss: 0.913613  [   32/  146]
train() client id: f_00005-2-1 loss: 0.826993  [   64/  146]
train() client id: f_00005-2-2 loss: 0.944243  [   96/  146]
train() client id: f_00005-2-3 loss: 0.809186  [  128/  146]
train() client id: f_00005-3-0 loss: 0.765049  [   32/  146]
train() client id: f_00005-3-1 loss: 0.766332  [   64/  146]
train() client id: f_00005-3-2 loss: 0.974578  [   96/  146]
train() client id: f_00005-3-3 loss: 0.902426  [  128/  146]
train() client id: f_00005-4-0 loss: 0.907533  [   32/  146]
train() client id: f_00005-4-1 loss: 0.779688  [   64/  146]
train() client id: f_00005-4-2 loss: 0.943775  [   96/  146]
train() client id: f_00005-4-3 loss: 1.009409  [  128/  146]
train() client id: f_00005-5-0 loss: 1.072140  [   32/  146]
train() client id: f_00005-5-1 loss: 0.896936  [   64/  146]
train() client id: f_00005-5-2 loss: 0.690483  [   96/  146]
train() client id: f_00005-5-3 loss: 0.916120  [  128/  146]
train() client id: f_00005-6-0 loss: 0.685492  [   32/  146]
train() client id: f_00005-6-1 loss: 0.924362  [   64/  146]
train() client id: f_00005-6-2 loss: 0.771148  [   96/  146]
train() client id: f_00005-6-3 loss: 1.082547  [  128/  146]
train() client id: f_00005-7-0 loss: 0.990418  [   32/  146]
train() client id: f_00005-7-1 loss: 1.086670  [   64/  146]
train() client id: f_00005-7-2 loss: 0.964337  [   96/  146]
train() client id: f_00005-7-3 loss: 0.712775  [  128/  146]
train() client id: f_00005-8-0 loss: 1.017714  [   32/  146]
train() client id: f_00005-8-1 loss: 0.552223  [   64/  146]
train() client id: f_00005-8-2 loss: 0.890102  [   96/  146]
train() client id: f_00005-8-3 loss: 1.050534  [  128/  146]
train() client id: f_00005-9-0 loss: 0.779635  [   32/  146]
train() client id: f_00005-9-1 loss: 0.894933  [   64/  146]
train() client id: f_00005-9-2 loss: 1.070841  [   96/  146]
train() client id: f_00005-9-3 loss: 0.891361  [  128/  146]
train() client id: f_00005-10-0 loss: 0.669998  [   32/  146]
train() client id: f_00005-10-1 loss: 1.207175  [   64/  146]
train() client id: f_00005-10-2 loss: 0.801051  [   96/  146]
train() client id: f_00005-10-3 loss: 0.909781  [  128/  146]
train() client id: f_00005-11-0 loss: 1.064160  [   32/  146]
train() client id: f_00005-11-1 loss: 0.847491  [   64/  146]
train() client id: f_00005-11-2 loss: 0.756004  [   96/  146]
train() client id: f_00005-11-3 loss: 0.878796  [  128/  146]
train() client id: f_00006-0-0 loss: 0.580647  [   32/   54]
train() client id: f_00006-1-0 loss: 0.574303  [   32/   54]
train() client id: f_00006-2-0 loss: 0.628945  [   32/   54]
train() client id: f_00006-3-0 loss: 0.627011  [   32/   54]
train() client id: f_00006-4-0 loss: 0.589734  [   32/   54]
train() client id: f_00006-5-0 loss: 0.585921  [   32/   54]
train() client id: f_00006-6-0 loss: 0.567120  [   32/   54]
train() client id: f_00006-7-0 loss: 0.582778  [   32/   54]
train() client id: f_00006-8-0 loss: 0.530443  [   32/   54]
train() client id: f_00006-9-0 loss: 0.575808  [   32/   54]
train() client id: f_00006-10-0 loss: 0.627956  [   32/   54]
train() client id: f_00006-11-0 loss: 0.583417  [   32/   54]
train() client id: f_00007-0-0 loss: 0.606854  [   32/  179]
train() client id: f_00007-0-1 loss: 0.816153  [   64/  179]
train() client id: f_00007-0-2 loss: 0.777257  [   96/  179]
train() client id: f_00007-0-3 loss: 0.826534  [  128/  179]
train() client id: f_00007-0-4 loss: 0.610071  [  160/  179]
train() client id: f_00007-1-0 loss: 0.677264  [   32/  179]
train() client id: f_00007-1-1 loss: 0.780701  [   64/  179]
train() client id: f_00007-1-2 loss: 0.798815  [   96/  179]
train() client id: f_00007-1-3 loss: 0.569923  [  128/  179]
train() client id: f_00007-1-4 loss: 0.741726  [  160/  179]
train() client id: f_00007-2-0 loss: 0.630642  [   32/  179]
train() client id: f_00007-2-1 loss: 0.698186  [   64/  179]
train() client id: f_00007-2-2 loss: 0.763642  [   96/  179]
train() client id: f_00007-2-3 loss: 0.567691  [  128/  179]
train() client id: f_00007-2-4 loss: 0.574659  [  160/  179]
train() client id: f_00007-3-0 loss: 0.572072  [   32/  179]
train() client id: f_00007-3-1 loss: 0.693320  [   64/  179]
train() client id: f_00007-3-2 loss: 0.846963  [   96/  179]
train() client id: f_00007-3-3 loss: 0.714537  [  128/  179]
train() client id: f_00007-3-4 loss: 0.658339  [  160/  179]
train() client id: f_00007-4-0 loss: 0.682827  [   32/  179]
train() client id: f_00007-4-1 loss: 0.550127  [   64/  179]
train() client id: f_00007-4-2 loss: 0.651636  [   96/  179]
train() client id: f_00007-4-3 loss: 0.752120  [  128/  179]
train() client id: f_00007-4-4 loss: 0.835408  [  160/  179]
train() client id: f_00007-5-0 loss: 0.942107  [   32/  179]
train() client id: f_00007-5-1 loss: 0.615139  [   64/  179]
train() client id: f_00007-5-2 loss: 0.620063  [   96/  179]
train() client id: f_00007-5-3 loss: 0.606647  [  128/  179]
train() client id: f_00007-5-4 loss: 0.552672  [  160/  179]
train() client id: f_00007-6-0 loss: 0.727737  [   32/  179]
train() client id: f_00007-6-1 loss: 0.610441  [   64/  179]
train() client id: f_00007-6-2 loss: 0.694940  [   96/  179]
train() client id: f_00007-6-3 loss: 0.654741  [  128/  179]
train() client id: f_00007-6-4 loss: 0.730891  [  160/  179]
train() client id: f_00007-7-0 loss: 0.691538  [   32/  179]
train() client id: f_00007-7-1 loss: 0.540008  [   64/  179]
train() client id: f_00007-7-2 loss: 0.756340  [   96/  179]
train() client id: f_00007-7-3 loss: 0.648365  [  128/  179]
train() client id: f_00007-7-4 loss: 0.725681  [  160/  179]
train() client id: f_00007-8-0 loss: 0.700139  [   32/  179]
train() client id: f_00007-8-1 loss: 0.750825  [   64/  179]
train() client id: f_00007-8-2 loss: 0.709118  [   96/  179]
train() client id: f_00007-8-3 loss: 0.537559  [  128/  179]
train() client id: f_00007-8-4 loss: 0.633650  [  160/  179]
train() client id: f_00007-9-0 loss: 0.533836  [   32/  179]
train() client id: f_00007-9-1 loss: 0.733614  [   64/  179]
train() client id: f_00007-9-2 loss: 0.664696  [   96/  179]
train() client id: f_00007-9-3 loss: 0.777755  [  128/  179]
train() client id: f_00007-9-4 loss: 0.706215  [  160/  179]
train() client id: f_00007-10-0 loss: 0.547444  [   32/  179]
train() client id: f_00007-10-1 loss: 0.706598  [   64/  179]
train() client id: f_00007-10-2 loss: 0.783474  [   96/  179]
train() client id: f_00007-10-3 loss: 0.762434  [  128/  179]
train() client id: f_00007-10-4 loss: 0.615981  [  160/  179]
train() client id: f_00007-11-0 loss: 0.846119  [   32/  179]
train() client id: f_00007-11-1 loss: 0.610270  [   64/  179]
train() client id: f_00007-11-2 loss: 0.536093  [   96/  179]
train() client id: f_00007-11-3 loss: 0.530601  [  128/  179]
train() client id: f_00007-11-4 loss: 0.738081  [  160/  179]
train() client id: f_00008-0-0 loss: 0.917474  [   32/  130]
train() client id: f_00008-0-1 loss: 0.713383  [   64/  130]
train() client id: f_00008-0-2 loss: 0.839672  [   96/  130]
train() client id: f_00008-0-3 loss: 0.807987  [  128/  130]
train() client id: f_00008-1-0 loss: 0.787532  [   32/  130]
train() client id: f_00008-1-1 loss: 0.756571  [   64/  130]
train() client id: f_00008-1-2 loss: 0.821309  [   96/  130]
train() client id: f_00008-1-3 loss: 0.865654  [  128/  130]
train() client id: f_00008-2-0 loss: 0.850803  [   32/  130]
train() client id: f_00008-2-1 loss: 0.853931  [   64/  130]
train() client id: f_00008-2-2 loss: 0.846195  [   96/  130]
train() client id: f_00008-2-3 loss: 0.722032  [  128/  130]
train() client id: f_00008-3-0 loss: 0.835015  [   32/  130]
train() client id: f_00008-3-1 loss: 0.888903  [   64/  130]
train() client id: f_00008-3-2 loss: 0.719895  [   96/  130]
train() client id: f_00008-3-3 loss: 0.786346  [  128/  130]
train() client id: f_00008-4-0 loss: 0.707555  [   32/  130]
train() client id: f_00008-4-1 loss: 0.826713  [   64/  130]
train() client id: f_00008-4-2 loss: 0.879931  [   96/  130]
train() client id: f_00008-4-3 loss: 0.827262  [  128/  130]
train() client id: f_00008-5-0 loss: 0.715141  [   32/  130]
train() client id: f_00008-5-1 loss: 0.984364  [   64/  130]
train() client id: f_00008-5-2 loss: 0.825624  [   96/  130]
train() client id: f_00008-5-3 loss: 0.732923  [  128/  130]
train() client id: f_00008-6-0 loss: 0.872701  [   32/  130]
train() client id: f_00008-6-1 loss: 0.711270  [   64/  130]
train() client id: f_00008-6-2 loss: 0.848758  [   96/  130]
train() client id: f_00008-6-3 loss: 0.827167  [  128/  130]
train() client id: f_00008-7-0 loss: 0.869183  [   32/  130]
train() client id: f_00008-7-1 loss: 0.697587  [   64/  130]
train() client id: f_00008-7-2 loss: 0.925035  [   96/  130]
train() client id: f_00008-7-3 loss: 0.767770  [  128/  130]
train() client id: f_00008-8-0 loss: 0.804129  [   32/  130]
train() client id: f_00008-8-1 loss: 0.835961  [   64/  130]
train() client id: f_00008-8-2 loss: 0.789891  [   96/  130]
train() client id: f_00008-8-3 loss: 0.835183  [  128/  130]
train() client id: f_00008-9-0 loss: 0.753488  [   32/  130]
train() client id: f_00008-9-1 loss: 0.836569  [   64/  130]
train() client id: f_00008-9-2 loss: 0.783518  [   96/  130]
train() client id: f_00008-9-3 loss: 0.856618  [  128/  130]
train() client id: f_00008-10-0 loss: 0.700319  [   32/  130]
train() client id: f_00008-10-1 loss: 0.905807  [   64/  130]
train() client id: f_00008-10-2 loss: 0.809903  [   96/  130]
train() client id: f_00008-10-3 loss: 0.843041  [  128/  130]
train() client id: f_00008-11-0 loss: 0.790557  [   32/  130]
train() client id: f_00008-11-1 loss: 0.845144  [   64/  130]
train() client id: f_00008-11-2 loss: 0.944705  [   96/  130]
train() client id: f_00008-11-3 loss: 0.676847  [  128/  130]
train() client id: f_00009-0-0 loss: 1.022864  [   32/  118]
train() client id: f_00009-0-1 loss: 0.950349  [   64/  118]
train() client id: f_00009-0-2 loss: 1.308125  [   96/  118]
train() client id: f_00009-1-0 loss: 1.031413  [   32/  118]
train() client id: f_00009-1-1 loss: 1.082723  [   64/  118]
train() client id: f_00009-1-2 loss: 0.930592  [   96/  118]
train() client id: f_00009-2-0 loss: 0.937445  [   32/  118]
train() client id: f_00009-2-1 loss: 0.979399  [   64/  118]
train() client id: f_00009-2-2 loss: 0.941881  [   96/  118]
train() client id: f_00009-3-0 loss: 0.881641  [   32/  118]
train() client id: f_00009-3-1 loss: 1.015169  [   64/  118]
train() client id: f_00009-3-2 loss: 0.935050  [   96/  118]
train() client id: f_00009-4-0 loss: 0.954061  [   32/  118]
train() client id: f_00009-4-1 loss: 0.914855  [   64/  118]
train() client id: f_00009-4-2 loss: 0.926392  [   96/  118]
train() client id: f_00009-5-0 loss: 0.855322  [   32/  118]
train() client id: f_00009-5-1 loss: 0.916834  [   64/  118]
train() client id: f_00009-5-2 loss: 0.878222  [   96/  118]
train() client id: f_00009-6-0 loss: 0.806252  [   32/  118]
train() client id: f_00009-6-1 loss: 1.007260  [   64/  118]
train() client id: f_00009-6-2 loss: 0.946209  [   96/  118]
train() client id: f_00009-7-0 loss: 0.780147  [   32/  118]
train() client id: f_00009-7-1 loss: 0.912428  [   64/  118]
train() client id: f_00009-7-2 loss: 0.951042  [   96/  118]
train() client id: f_00009-8-0 loss: 0.869462  [   32/  118]
train() client id: f_00009-8-1 loss: 0.861796  [   64/  118]
train() client id: f_00009-8-2 loss: 0.859534  [   96/  118]
train() client id: f_00009-9-0 loss: 0.833065  [   32/  118]
train() client id: f_00009-9-1 loss: 0.821942  [   64/  118]
train() client id: f_00009-9-2 loss: 0.892204  [   96/  118]
train() client id: f_00009-10-0 loss: 0.905880  [   32/  118]
train() client id: f_00009-10-1 loss: 0.805831  [   64/  118]
train() client id: f_00009-10-2 loss: 0.879445  [   96/  118]
train() client id: f_00009-11-0 loss: 0.860121  [   32/  118]
train() client id: f_00009-11-1 loss: 0.772758  [   64/  118]
train() client id: f_00009-11-2 loss: 0.892941  [   96/  118]
At round 10 accuracy: 0.6312997347480106
At round 10 training accuracy: 0.5774647887323944
At round 10 training loss: 0.8554571339213859
update_location
xs = -4.528292 -48.998411 20.045120 39.056472 -140.103519 -75.217951 -2.215960 23.375741 -1.680116 -30.304393 
ys = 37.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 -3.124922 -19.177652 32.845030 -0.998518 
xs mean: -22.05713084058713
ys mean: 7.871751218646876
dists_uav = 106.926892 112.440332 101.997798 109.698118 172.384518 126.305556 100.073351 104.471085 105.269268 104.495709 
uav_gains = -100.727215 -101.273138 -100.214791 -101.005039 -105.939503 -102.536000 -100.007977 -100.474932 -100.557573 -100.477491 
uav_gains_db_mean: -101.32136612347558
dists_bs = 218.958151 203.221163 261.164944 262.797933 169.285631 216.545279 248.158845 277.593039 224.160250 227.842703 
bs_gains = -105.097170 -104.190189 -107.240676 -107.316474 -101.968422 -104.962423 -106.619492 -107.982499 -105.382700 -105.580843 
bs_gains_db_mean: -105.63408884948737
Round 11
-------------------------------
ene_coms = [0.0064985  0.00762495 0.0063591  0.00657663 0.00686251 0.00793019
 0.00630451 0.00642912 0.00810652 0.00819231]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.53170848 19.89783102  9.3795177   3.36038006 22.92846764 11.0545471
  4.17181143 13.47195574  9.86887039  8.97846738]
obj_prev = 112.64355694567477
eta_min = 1.3755884463279127e-10	eta_max = 0.9309379342402061
af = 23.839837477165666	bf = 1.7306125875731464	zeta = 26.223821224882236	eta = 0.909090909090909
af = 23.839837477165666	bf = 1.7306125875731464	zeta = 44.03070962430452	eta = 0.5414365946081938
af = 23.839837477165666	bf = 1.7306125875731464	zeta = 35.670207150582414	eta = 0.668340314832638
af = 23.839837477165666	bf = 1.7306125875731464	zeta = 34.1828118508009	eta = 0.6974217797301278
af = 23.839837477165666	bf = 1.7306125875731464	zeta = 34.11234818004288	eta = 0.698862398781241
af = 23.839837477165666	bf = 1.7306125875731464	zeta = 34.11217974959267	eta = 0.6988658494463502
eta = 0.6988658494463502
ene_coms = [0.0064985  0.00762495 0.0063591  0.00657663 0.00686251 0.00793019
 0.00630451 0.00642912 0.00810652 0.00819231]
ene_comp = [0.02956577 0.06218197 0.02909647 0.0100899  0.07180257 0.03425874
 0.01267104 0.04200216 0.03050436 0.02768857]
ene_total = [2.92392799 5.65962974 2.87457794 1.3512473  6.37780997 3.42048866
 1.53845185 3.92659009 3.13039582 2.90906039]
ti_comp = [0.29825615 0.28699169 0.2996501  0.29747486 0.29461608 0.28393923
 0.30019609 0.2989499  0.28217599 0.28131803]
ti_coms = [0.064985   0.07624946 0.06359105 0.06576628 0.06862507 0.07930192
 0.06304505 0.06429125 0.08106516 0.08192311]
t_total = [29.44995384 29.44995384 29.44995384 29.44995384 29.44995384 29.44995384
 29.44995384 29.44995384 29.44995384 29.44995384]
ene_coms = [0.0064985  0.00762495 0.0063591  0.00657663 0.00686251 0.00793019
 0.00630451 0.00642912 0.00810652 0.00819231]
ene_comp = [1.81580249e-05 1.82446181e-04 1.71463624e-05 7.25505336e-07
 2.66555209e-04 3.11704843e-05 1.41093497e-06 5.18200775e-05
 2.22805416e-05 1.67643480e-05]
ene_total = [0.52834121 0.63298815 0.5169577  0.53326219 0.57799218 0.64547132
 0.51125526 0.52544577 0.65904617 0.66555484]
optimize_network iter = 0 obj = 5.796314773263603
eta = 0.6988658494463502
freqs = [4.95643860e+07 1.08334093e+08 4.85507402e+07 1.69592553e+07
 1.21857867e+08 6.03275997e+07 2.11046094e+07 7.02494961e+07
 5.40520158e+07 4.92122282e+07]
eta_min = 0.6903282727470414	eta_max = 0.6988658494463238
af = 0.04146213857888941	bf = 1.7306125875731464	zeta = 0.045608352436778354	eta = 0.9090909090909091
af = 0.04146213857888941	bf = 1.7306125875731464	zeta = 19.080207866017886	eta = 0.00217304438557685
af = 0.04146213857888941	bf = 1.7306125875731464	zeta = 1.9891654656472686	eta = 0.020843986734606693
af = 0.04146213857888941	bf = 1.7306125875731464	zeta = 1.9313568399097905	eta = 0.021467880881518516
af = 0.04146213857888941	bf = 1.7306125875731464	zeta = 1.9313385922148125	eta = 0.02146808371459177
eta = 0.02146808371459177
ene_coms = [0.0064985  0.00762495 0.0063591  0.00657663 0.00686251 0.00793019
 0.00630451 0.00642912 0.00810652 0.00819231]
ene_comp = [1.94666683e-04 1.95595023e-03 1.83820956e-04 7.77792288e-06
 2.85765763e-03 3.34169318e-04 1.51262064e-05 5.55547350e-04
 2.38862935e-04 1.79725495e-04]
ene_total = [0.16699605 0.23904559 0.16324752 0.16428247 0.24252036 0.20619772
 0.15767628 0.17426919 0.20821915 0.20888427]
ti_comp = [0.30855455 0.29729009 0.3099485  0.30777326 0.30491448 0.29423763
 0.31049449 0.3092483  0.29247439 0.29161643]
ti_coms = [0.064985   0.07624946 0.06359105 0.06576628 0.06862507 0.07930192
 0.06304505 0.06429125 0.08106516 0.08192311]
t_total = [29.44995384 29.44995384 29.44995384 29.44995384 29.44995384 29.44995384
 29.44995384 29.44995384 29.44995384 29.44995384]
ene_coms = [0.0064985  0.00762495 0.0063591  0.00657663 0.00686251 0.00793019
 0.00630451 0.00642912 0.00810652 0.00819231]
ene_comp = [1.87728456e-05 1.88130475e-04 1.77324347e-05 7.49939061e-07
 2.75353477e-04 3.21177060e-05 1.45933790e-06 5.35829767e-05
 2.29475733e-05 1.72625290e-05]
ene_total = [0.51382344 0.61598496 0.5027515  0.5185622  0.56275073 0.62775051
 0.49716389 0.51109834 0.64092901 0.64724492]
optimize_network iter = 1 obj = 5.638059500302491
eta = 0.6903282727470414
freqs = [4.95536957e+07 1.08169023e+08 4.85478062e+07 1.69541112e+07
 1.21781365e+08 6.02132542e+07 2.11046094e+07 7.02397846e+07
 5.39377684e+07 4.91029376e+07]
eta_min = 0.6903282727470804	eta_max = 0.6903282727470407
af = 0.04138027867223714	bf = 1.7306125875731464	zeta = 0.04551830653946086	eta = 0.909090909090909
af = 0.04138027867223714	bf = 1.7306125875731464	zeta = 19.080122043111224	eta = 0.0021687638359303507
af = 0.04138027867223714	bf = 1.7306125875731464	zeta = 1.9887356773870977	eta = 0.020807329572628104
af = 0.04138027867223714	bf = 1.7306125875731464	zeta = 1.9310353351543499	eta = 0.021429063424636696
af = 0.04138027867223714	bf = 1.7306125875731464	zeta = 1.9310171851228046	eta = 0.021429264840854084
eta = 0.021429264840854084
ene_coms = [0.0064985  0.00762495 0.0063591  0.00657663 0.00686251 0.00793019
 0.00630451 0.00642912 0.00810652 0.00819231]
ene_comp = [1.94674400e-04 1.95091294e-03 1.83885340e-04 7.77686772e-06
 2.85541543e-03 3.33060597e-04 1.51333334e-05 5.55655444e-04
 2.37966324e-04 1.79012417e-04]
ene_total = [0.16698962 0.23891043 0.16324265 0.16427593 0.2424548  0.20616188
 0.1576702  0.17426498 0.20818852 0.20885819]
ti_comp = [0.30855455 0.29729009 0.3099485  0.30777326 0.30491448 0.29423763
 0.31049449 0.3092483  0.29247439 0.29161643]
ti_coms = [0.064985   0.07624946 0.06359105 0.06576628 0.06862507 0.07930192
 0.06304505 0.06429125 0.08106516 0.08192311]
t_total = [29.44995384 29.44995384 29.44995384 29.44995384 29.44995384 29.44995384
 29.44995384 29.44995384 29.44995384 29.44995384]
ene_coms = [0.0064985  0.00762495 0.0063591  0.00657663 0.00686251 0.00793019
 0.00630451 0.00642912 0.00810652 0.00819231]
ene_comp = [1.87728456e-05 1.88130475e-04 1.77324347e-05 7.49939061e-07
 2.75353477e-04 3.21177060e-05 1.45933790e-06 5.35829767e-05
 2.29475733e-05 1.72625290e-05]
ene_total = [0.51382344 0.61598496 0.5027515  0.5185622  0.56275073 0.62775051
 0.49716389 0.51109834 0.64092901 0.64724492]
optimize_network iter = 2 obj = 5.638059500303194
eta = 0.6903282727470804
freqs = [4.95536957e+07 1.08169023e+08 4.85478062e+07 1.69541112e+07
 1.21781365e+08 6.02132542e+07 2.11046094e+07 7.02397846e+07
 5.39377684e+07 4.91029376e+07]
Done!
ene_coms = [0.0064985  0.00762495 0.0063591  0.00657663 0.00686251 0.00793019
 0.00630451 0.00642912 0.00810652 0.00819231]
ene_comp = [1.85640998e-05 1.86038547e-04 1.75352578e-05 7.41600068e-07
 2.72291667e-04 3.17605712e-05 1.44311070e-06 5.29871576e-05
 2.26924063e-05 1.70705772e-05]
ene_total = [0.00651706 0.00781098 0.00637664 0.00657737 0.0071348  0.00796195
 0.00630595 0.00648211 0.00812921 0.00820938]
At round 11 energy consumption: 0.07150546023207707
At round 11 eta: 0.6903282727470804
At round 11 a_n: 24.414598539591886
At round 11 local rounds: 12.134935141883515
At round 11 global rounds: 78.84025692681864
gradient difference: 0.39574939012527466
train() client id: f_00000-0-0 loss: 1.431273  [   32/  126]
train() client id: f_00000-0-1 loss: 1.454323  [   64/  126]
train() client id: f_00000-0-2 loss: 1.461532  [   96/  126]
train() client id: f_00000-1-0 loss: 1.472652  [   32/  126]
train() client id: f_00000-1-1 loss: 1.281501  [   64/  126]
train() client id: f_00000-1-2 loss: 1.111086  [   96/  126]
train() client id: f_00000-2-0 loss: 1.170487  [   32/  126]
train() client id: f_00000-2-1 loss: 1.093352  [   64/  126]
train() client id: f_00000-2-2 loss: 1.223265  [   96/  126]
train() client id: f_00000-3-0 loss: 1.117157  [   32/  126]
train() client id: f_00000-3-1 loss: 1.142685  [   64/  126]
train() client id: f_00000-3-2 loss: 1.076589  [   96/  126]
train() client id: f_00000-4-0 loss: 1.062771  [   32/  126]
train() client id: f_00000-4-1 loss: 1.099479  [   64/  126]
train() client id: f_00000-4-2 loss: 1.006189  [   96/  126]
train() client id: f_00000-5-0 loss: 1.065309  [   32/  126]
train() client id: f_00000-5-1 loss: 0.823161  [   64/  126]
train() client id: f_00000-5-2 loss: 1.032525  [   96/  126]
train() client id: f_00000-6-0 loss: 0.902139  [   32/  126]
train() client id: f_00000-6-1 loss: 0.837923  [   64/  126]
train() client id: f_00000-6-2 loss: 1.064034  [   96/  126]
train() client id: f_00000-7-0 loss: 0.833045  [   32/  126]
train() client id: f_00000-7-1 loss: 0.918523  [   64/  126]
train() client id: f_00000-7-2 loss: 0.997074  [   96/  126]
train() client id: f_00000-8-0 loss: 0.772629  [   32/  126]
train() client id: f_00000-8-1 loss: 0.894921  [   64/  126]
train() client id: f_00000-8-2 loss: 0.868204  [   96/  126]
train() client id: f_00000-9-0 loss: 0.940174  [   32/  126]
train() client id: f_00000-9-1 loss: 0.879826  [   64/  126]
train() client id: f_00000-9-2 loss: 0.747044  [   96/  126]
train() client id: f_00000-10-0 loss: 0.860551  [   32/  126]
train() client id: f_00000-10-1 loss: 0.851365  [   64/  126]
train() client id: f_00000-10-2 loss: 0.762881  [   96/  126]
train() client id: f_00000-11-0 loss: 0.799929  [   32/  126]
train() client id: f_00000-11-1 loss: 0.964317  [   64/  126]
train() client id: f_00000-11-2 loss: 0.717860  [   96/  126]
train() client id: f_00001-0-0 loss: 0.437086  [   32/  265]
train() client id: f_00001-0-1 loss: 0.390486  [   64/  265]
train() client id: f_00001-0-2 loss: 0.558473  [   96/  265]
train() client id: f_00001-0-3 loss: 0.460576  [  128/  265]
train() client id: f_00001-0-4 loss: 0.547869  [  160/  265]
train() client id: f_00001-0-5 loss: 0.459523  [  192/  265]
train() client id: f_00001-0-6 loss: 0.397585  [  224/  265]
train() client id: f_00001-0-7 loss: 0.386010  [  256/  265]
train() client id: f_00001-1-0 loss: 0.479434  [   32/  265]
train() client id: f_00001-1-1 loss: 0.596038  [   64/  265]
train() client id: f_00001-1-2 loss: 0.391758  [   96/  265]
train() client id: f_00001-1-3 loss: 0.382292  [  128/  265]
train() client id: f_00001-1-4 loss: 0.435050  [  160/  265]
train() client id: f_00001-1-5 loss: 0.389296  [  192/  265]
train() client id: f_00001-1-6 loss: 0.398515  [  224/  265]
train() client id: f_00001-1-7 loss: 0.425178  [  256/  265]
train() client id: f_00001-2-0 loss: 0.361000  [   32/  265]
train() client id: f_00001-2-1 loss: 0.409883  [   64/  265]
train() client id: f_00001-2-2 loss: 0.529449  [   96/  265]
train() client id: f_00001-2-3 loss: 0.377391  [  128/  265]
train() client id: f_00001-2-4 loss: 0.518975  [  160/  265]
train() client id: f_00001-2-5 loss: 0.449828  [  192/  265]
train() client id: f_00001-2-6 loss: 0.395785  [  224/  265]
train() client id: f_00001-2-7 loss: 0.371014  [  256/  265]
train() client id: f_00001-3-0 loss: 0.359457  [   32/  265]
train() client id: f_00001-3-1 loss: 0.456826  [   64/  265]
train() client id: f_00001-3-2 loss: 0.532043  [   96/  265]
train() client id: f_00001-3-3 loss: 0.433046  [  128/  265]
train() client id: f_00001-3-4 loss: 0.396637  [  160/  265]
train() client id: f_00001-3-5 loss: 0.376359  [  192/  265]
train() client id: f_00001-3-6 loss: 0.448624  [  224/  265]
train() client id: f_00001-3-7 loss: 0.320907  [  256/  265]
train() client id: f_00001-4-0 loss: 0.572732  [   32/  265]
train() client id: f_00001-4-1 loss: 0.382480  [   64/  265]
train() client id: f_00001-4-2 loss: 0.445385  [   96/  265]
train() client id: f_00001-4-3 loss: 0.405043  [  128/  265]
train() client id: f_00001-4-4 loss: 0.324615  [  160/  265]
train() client id: f_00001-4-5 loss: 0.365385  [  192/  265]
train() client id: f_00001-4-6 loss: 0.397580  [  224/  265]
train() client id: f_00001-4-7 loss: 0.368042  [  256/  265]
train() client id: f_00001-5-0 loss: 0.370504  [   32/  265]
train() client id: f_00001-5-1 loss: 0.406459  [   64/  265]
train() client id: f_00001-5-2 loss: 0.416816  [   96/  265]
train() client id: f_00001-5-3 loss: 0.463990  [  128/  265]
train() client id: f_00001-5-4 loss: 0.337028  [  160/  265]
train() client id: f_00001-5-5 loss: 0.439736  [  192/  265]
train() client id: f_00001-5-6 loss: 0.451168  [  224/  265]
train() client id: f_00001-5-7 loss: 0.303398  [  256/  265]
train() client id: f_00001-6-0 loss: 0.386211  [   32/  265]
train() client id: f_00001-6-1 loss: 0.456631  [   64/  265]
train() client id: f_00001-6-2 loss: 0.399589  [   96/  265]
train() client id: f_00001-6-3 loss: 0.447449  [  128/  265]
train() client id: f_00001-6-4 loss: 0.482830  [  160/  265]
train() client id: f_00001-6-5 loss: 0.298667  [  192/  265]
train() client id: f_00001-6-6 loss: 0.339813  [  224/  265]
train() client id: f_00001-6-7 loss: 0.307236  [  256/  265]
train() client id: f_00001-7-0 loss: 0.413857  [   32/  265]
train() client id: f_00001-7-1 loss: 0.383965  [   64/  265]
train() client id: f_00001-7-2 loss: 0.358878  [   96/  265]
train() client id: f_00001-7-3 loss: 0.394063  [  128/  265]
train() client id: f_00001-7-4 loss: 0.421493  [  160/  265]
train() client id: f_00001-7-5 loss: 0.388001  [  192/  265]
train() client id: f_00001-7-6 loss: 0.341686  [  224/  265]
train() client id: f_00001-7-7 loss: 0.382188  [  256/  265]
train() client id: f_00001-8-0 loss: 0.372014  [   32/  265]
train() client id: f_00001-8-1 loss: 0.367299  [   64/  265]
train() client id: f_00001-8-2 loss: 0.318308  [   96/  265]
train() client id: f_00001-8-3 loss: 0.506595  [  128/  265]
train() client id: f_00001-8-4 loss: 0.423590  [  160/  265]
train() client id: f_00001-8-5 loss: 0.384251  [  192/  265]
train() client id: f_00001-8-6 loss: 0.310616  [  224/  265]
train() client id: f_00001-8-7 loss: 0.434315  [  256/  265]
train() client id: f_00001-9-0 loss: 0.368656  [   32/  265]
train() client id: f_00001-9-1 loss: 0.401746  [   64/  265]
train() client id: f_00001-9-2 loss: 0.410681  [   96/  265]
train() client id: f_00001-9-3 loss: 0.344911  [  128/  265]
train() client id: f_00001-9-4 loss: 0.367287  [  160/  265]
train() client id: f_00001-9-5 loss: 0.324659  [  192/  265]
train() client id: f_00001-9-6 loss: 0.448563  [  224/  265]
train() client id: f_00001-9-7 loss: 0.342063  [  256/  265]
train() client id: f_00001-10-0 loss: 0.392731  [   32/  265]
train() client id: f_00001-10-1 loss: 0.432325  [   64/  265]
train() client id: f_00001-10-2 loss: 0.402629  [   96/  265]
train() client id: f_00001-10-3 loss: 0.375006  [  128/  265]
train() client id: f_00001-10-4 loss: 0.453816  [  160/  265]
train() client id: f_00001-10-5 loss: 0.300969  [  192/  265]
train() client id: f_00001-10-6 loss: 0.354805  [  224/  265]
train() client id: f_00001-10-7 loss: 0.336828  [  256/  265]
train() client id: f_00001-11-0 loss: 0.290661  [   32/  265]
train() client id: f_00001-11-1 loss: 0.337693  [   64/  265]
train() client id: f_00001-11-2 loss: 0.382138  [   96/  265]
train() client id: f_00001-11-3 loss: 0.375902  [  128/  265]
train() client id: f_00001-11-4 loss: 0.416770  [  160/  265]
train() client id: f_00001-11-5 loss: 0.361084  [  192/  265]
train() client id: f_00001-11-6 loss: 0.527568  [  224/  265]
train() client id: f_00001-11-7 loss: 0.304783  [  256/  265]
train() client id: f_00002-0-0 loss: 1.264035  [   32/  124]
train() client id: f_00002-0-1 loss: 1.241034  [   64/  124]
train() client id: f_00002-0-2 loss: 1.076653  [   96/  124]
train() client id: f_00002-1-0 loss: 0.918343  [   32/  124]
train() client id: f_00002-1-1 loss: 1.149542  [   64/  124]
train() client id: f_00002-1-2 loss: 1.150680  [   96/  124]
train() client id: f_00002-2-0 loss: 1.079378  [   32/  124]
train() client id: f_00002-2-1 loss: 1.077892  [   64/  124]
train() client id: f_00002-2-2 loss: 1.148313  [   96/  124]
train() client id: f_00002-3-0 loss: 1.019040  [   32/  124]
train() client id: f_00002-3-1 loss: 1.141253  [   64/  124]
train() client id: f_00002-3-2 loss: 1.102177  [   96/  124]
train() client id: f_00002-4-0 loss: 0.987018  [   32/  124]
train() client id: f_00002-4-1 loss: 0.997304  [   64/  124]
train() client id: f_00002-4-2 loss: 1.206059  [   96/  124]
train() client id: f_00002-5-0 loss: 1.028854  [   32/  124]
train() client id: f_00002-5-1 loss: 0.916535  [   64/  124]
train() client id: f_00002-5-2 loss: 1.292525  [   96/  124]
train() client id: f_00002-6-0 loss: 0.979643  [   32/  124]
train() client id: f_00002-6-1 loss: 0.931746  [   64/  124]
train() client id: f_00002-6-2 loss: 0.933942  [   96/  124]
train() client id: f_00002-7-0 loss: 0.964819  [   32/  124]
train() client id: f_00002-7-1 loss: 1.021489  [   64/  124]
train() client id: f_00002-7-2 loss: 0.893502  [   96/  124]
train() client id: f_00002-8-0 loss: 0.946401  [   32/  124]
train() client id: f_00002-8-1 loss: 0.962422  [   64/  124]
train() client id: f_00002-8-2 loss: 1.045128  [   96/  124]
train() client id: f_00002-9-0 loss: 1.020538  [   32/  124]
train() client id: f_00002-9-1 loss: 0.919117  [   64/  124]
train() client id: f_00002-9-2 loss: 0.960236  [   96/  124]
train() client id: f_00002-10-0 loss: 0.959902  [   32/  124]
train() client id: f_00002-10-1 loss: 0.888779  [   64/  124]
train() client id: f_00002-10-2 loss: 0.949930  [   96/  124]
train() client id: f_00002-11-0 loss: 0.886032  [   32/  124]
train() client id: f_00002-11-1 loss: 1.142136  [   64/  124]
train() client id: f_00002-11-2 loss: 0.841912  [   96/  124]
train() client id: f_00003-0-0 loss: 1.017484  [   32/   43]
train() client id: f_00003-1-0 loss: 1.044894  [   32/   43]
train() client id: f_00003-2-0 loss: 0.975360  [   32/   43]
train() client id: f_00003-3-0 loss: 1.024799  [   32/   43]
train() client id: f_00003-4-0 loss: 0.996488  [   32/   43]
train() client id: f_00003-5-0 loss: 0.960510  [   32/   43]
train() client id: f_00003-6-0 loss: 1.022129  [   32/   43]
train() client id: f_00003-7-0 loss: 0.992457  [   32/   43]
train() client id: f_00003-8-0 loss: 0.940890  [   32/   43]
train() client id: f_00003-9-0 loss: 0.923479  [   32/   43]
train() client id: f_00003-10-0 loss: 0.922566  [   32/   43]
train() client id: f_00003-11-0 loss: 0.910121  [   32/   43]
train() client id: f_00004-0-0 loss: 0.746975  [   32/  306]
train() client id: f_00004-0-1 loss: 0.858940  [   64/  306]
train() client id: f_00004-0-2 loss: 0.833912  [   96/  306]
train() client id: f_00004-0-3 loss: 0.789694  [  128/  306]
train() client id: f_00004-0-4 loss: 0.809964  [  160/  306]
train() client id: f_00004-0-5 loss: 0.823733  [  192/  306]
train() client id: f_00004-0-6 loss: 0.870283  [  224/  306]
train() client id: f_00004-0-7 loss: 0.819323  [  256/  306]
train() client id: f_00004-0-8 loss: 0.750695  [  288/  306]
train() client id: f_00004-1-0 loss: 0.778456  [   32/  306]
train() client id: f_00004-1-1 loss: 0.931523  [   64/  306]
train() client id: f_00004-1-2 loss: 0.843769  [   96/  306]
train() client id: f_00004-1-3 loss: 0.814952  [  128/  306]
train() client id: f_00004-1-4 loss: 0.746675  [  160/  306]
train() client id: f_00004-1-5 loss: 0.886792  [  192/  306]
train() client id: f_00004-1-6 loss: 0.771374  [  224/  306]
train() client id: f_00004-1-7 loss: 0.911151  [  256/  306]
train() client id: f_00004-1-8 loss: 0.657641  [  288/  306]
train() client id: f_00004-2-0 loss: 0.767934  [   32/  306]
train() client id: f_00004-2-1 loss: 0.723104  [   64/  306]
train() client id: f_00004-2-2 loss: 0.730061  [   96/  306]
train() client id: f_00004-2-3 loss: 0.937544  [  128/  306]
train() client id: f_00004-2-4 loss: 0.823192  [  160/  306]
train() client id: f_00004-2-5 loss: 0.752326  [  192/  306]
train() client id: f_00004-2-6 loss: 0.923049  [  224/  306]
train() client id: f_00004-2-7 loss: 0.801791  [  256/  306]
train() client id: f_00004-2-8 loss: 0.846456  [  288/  306]
train() client id: f_00004-3-0 loss: 0.863214  [   32/  306]
train() client id: f_00004-3-1 loss: 0.864790  [   64/  306]
train() client id: f_00004-3-2 loss: 0.785706  [   96/  306]
train() client id: f_00004-3-3 loss: 0.732933  [  128/  306]
train() client id: f_00004-3-4 loss: 0.933638  [  160/  306]
train() client id: f_00004-3-5 loss: 0.847961  [  192/  306]
train() client id: f_00004-3-6 loss: 0.782281  [  224/  306]
train() client id: f_00004-3-7 loss: 0.852358  [  256/  306]
train() client id: f_00004-3-8 loss: 0.697631  [  288/  306]
train() client id: f_00004-4-0 loss: 0.909231  [   32/  306]
train() client id: f_00004-4-1 loss: 0.807602  [   64/  306]
train() client id: f_00004-4-2 loss: 0.778197  [   96/  306]
train() client id: f_00004-4-3 loss: 0.804218  [  128/  306]
train() client id: f_00004-4-4 loss: 0.822921  [  160/  306]
train() client id: f_00004-4-5 loss: 0.784692  [  192/  306]
train() client id: f_00004-4-6 loss: 0.844002  [  224/  306]
train() client id: f_00004-4-7 loss: 0.752343  [  256/  306]
train() client id: f_00004-4-8 loss: 0.830219  [  288/  306]
train() client id: f_00004-5-0 loss: 0.861240  [   32/  306]
train() client id: f_00004-5-1 loss: 0.860628  [   64/  306]
train() client id: f_00004-5-2 loss: 0.754136  [   96/  306]
train() client id: f_00004-5-3 loss: 0.810180  [  128/  306]
train() client id: f_00004-5-4 loss: 0.755287  [  160/  306]
train() client id: f_00004-5-5 loss: 0.749525  [  192/  306]
train() client id: f_00004-5-6 loss: 0.837935  [  224/  306]
train() client id: f_00004-5-7 loss: 0.851346  [  256/  306]
train() client id: f_00004-5-8 loss: 0.820057  [  288/  306]
train() client id: f_00004-6-0 loss: 0.778553  [   32/  306]
train() client id: f_00004-6-1 loss: 0.884457  [   64/  306]
train() client id: f_00004-6-2 loss: 0.676882  [   96/  306]
train() client id: f_00004-6-3 loss: 0.806882  [  128/  306]
train() client id: f_00004-6-4 loss: 0.797898  [  160/  306]
train() client id: f_00004-6-5 loss: 0.799484  [  192/  306]
train() client id: f_00004-6-6 loss: 0.829688  [  224/  306]
train() client id: f_00004-6-7 loss: 0.921645  [  256/  306]
train() client id: f_00004-6-8 loss: 0.816113  [  288/  306]
train() client id: f_00004-7-0 loss: 0.781533  [   32/  306]
train() client id: f_00004-7-1 loss: 0.828744  [   64/  306]
train() client id: f_00004-7-2 loss: 0.846683  [   96/  306]
train() client id: f_00004-7-3 loss: 0.897618  [  128/  306]
train() client id: f_00004-7-4 loss: 0.811065  [  160/  306]
train() client id: f_00004-7-5 loss: 0.711545  [  192/  306]
train() client id: f_00004-7-6 loss: 0.682008  [  224/  306]
train() client id: f_00004-7-7 loss: 0.856741  [  256/  306]
train() client id: f_00004-7-8 loss: 0.873554  [  288/  306]
train() client id: f_00004-8-0 loss: 0.935596  [   32/  306]
train() client id: f_00004-8-1 loss: 0.725729  [   64/  306]
train() client id: f_00004-8-2 loss: 0.812844  [   96/  306]
train() client id: f_00004-8-3 loss: 0.734394  [  128/  306]
train() client id: f_00004-8-4 loss: 0.812548  [  160/  306]
train() client id: f_00004-8-5 loss: 0.957542  [  192/  306]
train() client id: f_00004-8-6 loss: 0.797141  [  224/  306]
train() client id: f_00004-8-7 loss: 0.836353  [  256/  306]
train() client id: f_00004-8-8 loss: 0.732808  [  288/  306]
train() client id: f_00004-9-0 loss: 0.809640  [   32/  306]
train() client id: f_00004-9-1 loss: 0.770875  [   64/  306]
train() client id: f_00004-9-2 loss: 0.754740  [   96/  306]
train() client id: f_00004-9-3 loss: 0.846641  [  128/  306]
train() client id: f_00004-9-4 loss: 0.798608  [  160/  306]
train() client id: f_00004-9-5 loss: 0.775353  [  192/  306]
train() client id: f_00004-9-6 loss: 0.891584  [  224/  306]
train() client id: f_00004-9-7 loss: 0.820181  [  256/  306]
train() client id: f_00004-9-8 loss: 0.828643  [  288/  306]
train() client id: f_00004-10-0 loss: 0.802183  [   32/  306]
train() client id: f_00004-10-1 loss: 0.816030  [   64/  306]
train() client id: f_00004-10-2 loss: 0.839609  [   96/  306]
train() client id: f_00004-10-3 loss: 0.805161  [  128/  306]
train() client id: f_00004-10-4 loss: 0.744741  [  160/  306]
train() client id: f_00004-10-5 loss: 0.855639  [  192/  306]
train() client id: f_00004-10-6 loss: 0.856184  [  224/  306]
train() client id: f_00004-10-7 loss: 0.759132  [  256/  306]
train() client id: f_00004-10-8 loss: 0.893589  [  288/  306]
train() client id: f_00004-11-0 loss: 0.887761  [   32/  306]
train() client id: f_00004-11-1 loss: 0.691375  [   64/  306]
train() client id: f_00004-11-2 loss: 0.856862  [   96/  306]
train() client id: f_00004-11-3 loss: 0.873359  [  128/  306]
train() client id: f_00004-11-4 loss: 0.709208  [  160/  306]
train() client id: f_00004-11-5 loss: 0.878098  [  192/  306]
train() client id: f_00004-11-6 loss: 0.794881  [  224/  306]
train() client id: f_00004-11-7 loss: 0.909387  [  256/  306]
train() client id: f_00004-11-8 loss: 0.791942  [  288/  306]
train() client id: f_00005-0-0 loss: 0.580778  [   32/  146]
train() client id: f_00005-0-1 loss: 0.736043  [   64/  146]
train() client id: f_00005-0-2 loss: 0.576697  [   96/  146]
train() client id: f_00005-0-3 loss: 0.904050  [  128/  146]
train() client id: f_00005-1-0 loss: 0.607313  [   32/  146]
train() client id: f_00005-1-1 loss: 0.670349  [   64/  146]
train() client id: f_00005-1-2 loss: 0.884356  [   96/  146]
train() client id: f_00005-1-3 loss: 0.718689  [  128/  146]
train() client id: f_00005-2-0 loss: 0.739091  [   32/  146]
train() client id: f_00005-2-1 loss: 0.737924  [   64/  146]
train() client id: f_00005-2-2 loss: 0.871419  [   96/  146]
train() client id: f_00005-2-3 loss: 0.572900  [  128/  146]
train() client id: f_00005-3-0 loss: 0.804807  [   32/  146]
train() client id: f_00005-3-1 loss: 0.565050  [   64/  146]
train() client id: f_00005-3-2 loss: 0.696356  [   96/  146]
train() client id: f_00005-3-3 loss: 0.878660  [  128/  146]
train() client id: f_00005-4-0 loss: 0.787001  [   32/  146]
train() client id: f_00005-4-1 loss: 0.707770  [   64/  146]
train() client id: f_00005-4-2 loss: 0.731780  [   96/  146]
train() client id: f_00005-4-3 loss: 0.587316  [  128/  146]
train() client id: f_00005-5-0 loss: 0.658593  [   32/  146]
train() client id: f_00005-5-1 loss: 0.771355  [   64/  146]
train() client id: f_00005-5-2 loss: 0.752620  [   96/  146]
train() client id: f_00005-5-3 loss: 0.541202  [  128/  146]
train() client id: f_00005-6-0 loss: 0.607486  [   32/  146]
train() client id: f_00005-6-1 loss: 1.029670  [   64/  146]
train() client id: f_00005-6-2 loss: 0.515621  [   96/  146]
train() client id: f_00005-6-3 loss: 0.717975  [  128/  146]
train() client id: f_00005-7-0 loss: 0.576554  [   32/  146]
train() client id: f_00005-7-1 loss: 0.842796  [   64/  146]
train() client id: f_00005-7-2 loss: 0.619404  [   96/  146]
train() client id: f_00005-7-3 loss: 0.810689  [  128/  146]
train() client id: f_00005-8-0 loss: 1.011030  [   32/  146]
train() client id: f_00005-8-1 loss: 0.864048  [   64/  146]
train() client id: f_00005-8-2 loss: 0.388963  [   96/  146]
train() client id: f_00005-8-3 loss: 0.719322  [  128/  146]
train() client id: f_00005-9-0 loss: 0.699256  [   32/  146]
train() client id: f_00005-9-1 loss: 0.745905  [   64/  146]
train() client id: f_00005-9-2 loss: 0.686631  [   96/  146]
train() client id: f_00005-9-3 loss: 0.846519  [  128/  146]
train() client id: f_00005-10-0 loss: 0.731873  [   32/  146]
train() client id: f_00005-10-1 loss: 0.617038  [   64/  146]
train() client id: f_00005-10-2 loss: 0.768894  [   96/  146]
train() client id: f_00005-10-3 loss: 0.703821  [  128/  146]
train() client id: f_00005-11-0 loss: 0.681339  [   32/  146]
train() client id: f_00005-11-1 loss: 0.826624  [   64/  146]
train() client id: f_00005-11-2 loss: 0.663088  [   96/  146]
train() client id: f_00005-11-3 loss: 0.653303  [  128/  146]
train() client id: f_00006-0-0 loss: 0.594690  [   32/   54]
train() client id: f_00006-1-0 loss: 0.690849  [   32/   54]
train() client id: f_00006-2-0 loss: 0.678522  [   32/   54]
train() client id: f_00006-3-0 loss: 0.594775  [   32/   54]
train() client id: f_00006-4-0 loss: 0.630976  [   32/   54]
train() client id: f_00006-5-0 loss: 0.638205  [   32/   54]
train() client id: f_00006-6-0 loss: 0.644063  [   32/   54]
train() client id: f_00006-7-0 loss: 0.596493  [   32/   54]
train() client id: f_00006-8-0 loss: 0.641482  [   32/   54]
train() client id: f_00006-9-0 loss: 0.636480  [   32/   54]
train() client id: f_00006-10-0 loss: 0.676988  [   32/   54]
train() client id: f_00006-11-0 loss: 0.647305  [   32/   54]
train() client id: f_00007-0-0 loss: 0.629101  [   32/  179]
train() client id: f_00007-0-1 loss: 0.585053  [   64/  179]
train() client id: f_00007-0-2 loss: 0.508643  [   96/  179]
train() client id: f_00007-0-3 loss: 0.539440  [  128/  179]
train() client id: f_00007-0-4 loss: 0.529325  [  160/  179]
train() client id: f_00007-1-0 loss: 0.436409  [   32/  179]
train() client id: f_00007-1-1 loss: 0.483054  [   64/  179]
train() client id: f_00007-1-2 loss: 0.424194  [   96/  179]
train() client id: f_00007-1-3 loss: 0.550746  [  128/  179]
train() client id: f_00007-1-4 loss: 0.710453  [  160/  179]
train() client id: f_00007-2-0 loss: 0.575980  [   32/  179]
train() client id: f_00007-2-1 loss: 0.493643  [   64/  179]
train() client id: f_00007-2-2 loss: 0.378722  [   96/  179]
train() client id: f_00007-2-3 loss: 0.563101  [  128/  179]
train() client id: f_00007-2-4 loss: 0.666190  [  160/  179]
train() client id: f_00007-3-0 loss: 0.559652  [   32/  179]
train() client id: f_00007-3-1 loss: 0.500262  [   64/  179]
train() client id: f_00007-3-2 loss: 0.524862  [   96/  179]
train() client id: f_00007-3-3 loss: 0.555641  [  128/  179]
train() client id: f_00007-3-4 loss: 0.524286  [  160/  179]
train() client id: f_00007-4-0 loss: 0.491165  [   32/  179]
train() client id: f_00007-4-1 loss: 0.377114  [   64/  179]
train() client id: f_00007-4-2 loss: 0.386581  [   96/  179]
train() client id: f_00007-4-3 loss: 0.495445  [  128/  179]
train() client id: f_00007-4-4 loss: 0.871936  [  160/  179]
train() client id: f_00007-5-0 loss: 0.745523  [   32/  179]
train() client id: f_00007-5-1 loss: 0.462742  [   64/  179]
train() client id: f_00007-5-2 loss: 0.539265  [   96/  179]
train() client id: f_00007-5-3 loss: 0.421911  [  128/  179]
train() client id: f_00007-5-4 loss: 0.474532  [  160/  179]
train() client id: f_00007-6-0 loss: 0.674048  [   32/  179]
train() client id: f_00007-6-1 loss: 0.371329  [   64/  179]
train() client id: f_00007-6-2 loss: 0.369816  [   96/  179]
train() client id: f_00007-6-3 loss: 0.449812  [  128/  179]
train() client id: f_00007-6-4 loss: 0.727531  [  160/  179]
train() client id: f_00007-7-0 loss: 0.365308  [   32/  179]
train() client id: f_00007-7-1 loss: 0.606852  [   64/  179]
train() client id: f_00007-7-2 loss: 0.467546  [   96/  179]
train() client id: f_00007-7-3 loss: 0.407629  [  128/  179]
train() client id: f_00007-7-4 loss: 0.603312  [  160/  179]
train() client id: f_00007-8-0 loss: 0.664751  [   32/  179]
train() client id: f_00007-8-1 loss: 0.459267  [   64/  179]
train() client id: f_00007-8-2 loss: 0.528666  [   96/  179]
train() client id: f_00007-8-3 loss: 0.371745  [  128/  179]
train() client id: f_00007-8-4 loss: 0.357370  [  160/  179]
train() client id: f_00007-9-0 loss: 0.448877  [   32/  179]
train() client id: f_00007-9-1 loss: 0.652285  [   64/  179]
train() client id: f_00007-9-2 loss: 0.488243  [   96/  179]
train() client id: f_00007-9-3 loss: 0.423774  [  128/  179]
train() client id: f_00007-9-4 loss: 0.383660  [  160/  179]
train() client id: f_00007-10-0 loss: 0.425345  [   32/  179]
train() client id: f_00007-10-1 loss: 0.471603  [   64/  179]
train() client id: f_00007-10-2 loss: 0.547176  [   96/  179]
train() client id: f_00007-10-3 loss: 0.497604  [  128/  179]
train() client id: f_00007-10-4 loss: 0.538645  [  160/  179]
train() client id: f_00007-11-0 loss: 0.500256  [   32/  179]
train() client id: f_00007-11-1 loss: 0.551765  [   64/  179]
train() client id: f_00007-11-2 loss: 0.454990  [   96/  179]
train() client id: f_00007-11-3 loss: 0.458353  [  128/  179]
train() client id: f_00007-11-4 loss: 0.543249  [  160/  179]
train() client id: f_00008-0-0 loss: 0.976413  [   32/  130]
train() client id: f_00008-0-1 loss: 0.961353  [   64/  130]
train() client id: f_00008-0-2 loss: 0.915734  [   96/  130]
train() client id: f_00008-0-3 loss: 0.874662  [  128/  130]
train() client id: f_00008-1-0 loss: 0.922504  [   32/  130]
train() client id: f_00008-1-1 loss: 0.835337  [   64/  130]
train() client id: f_00008-1-2 loss: 1.012173  [   96/  130]
train() client id: f_00008-1-3 loss: 0.916963  [  128/  130]
train() client id: f_00008-2-0 loss: 1.011677  [   32/  130]
train() client id: f_00008-2-1 loss: 0.847146  [   64/  130]
train() client id: f_00008-2-2 loss: 0.947363  [   96/  130]
train() client id: f_00008-2-3 loss: 0.918296  [  128/  130]
train() client id: f_00008-3-0 loss: 0.953080  [   32/  130]
train() client id: f_00008-3-1 loss: 0.937159  [   64/  130]
train() client id: f_00008-3-2 loss: 0.937108  [   96/  130]
train() client id: f_00008-3-3 loss: 0.893285  [  128/  130]
train() client id: f_00008-4-0 loss: 0.921207  [   32/  130]
train() client id: f_00008-4-1 loss: 0.895936  [   64/  130]
train() client id: f_00008-4-2 loss: 0.971489  [   96/  130]
train() client id: f_00008-4-3 loss: 0.909034  [  128/  130]
train() client id: f_00008-5-0 loss: 0.945152  [   32/  130]
train() client id: f_00008-5-1 loss: 1.006735  [   64/  130]
train() client id: f_00008-5-2 loss: 0.889517  [   96/  130]
train() client id: f_00008-5-3 loss: 0.885058  [  128/  130]
train() client id: f_00008-6-0 loss: 0.938472  [   32/  130]
train() client id: f_00008-6-1 loss: 0.918171  [   64/  130]
train() client id: f_00008-6-2 loss: 0.951129  [   96/  130]
train() client id: f_00008-6-3 loss: 0.918805  [  128/  130]
train() client id: f_00008-7-0 loss: 0.858219  [   32/  130]
train() client id: f_00008-7-1 loss: 0.908051  [   64/  130]
train() client id: f_00008-7-2 loss: 0.945487  [   96/  130]
train() client id: f_00008-7-3 loss: 1.017231  [  128/  130]
train() client id: f_00008-8-0 loss: 0.960066  [   32/  130]
train() client id: f_00008-8-1 loss: 0.846549  [   64/  130]
train() client id: f_00008-8-2 loss: 0.905595  [   96/  130]
train() client id: f_00008-8-3 loss: 0.982163  [  128/  130]
train() client id: f_00008-9-0 loss: 0.846617  [   32/  130]
train() client id: f_00008-9-1 loss: 0.944324  [   64/  130]
train() client id: f_00008-9-2 loss: 0.924895  [   96/  130]
train() client id: f_00008-9-3 loss: 0.991614  [  128/  130]
train() client id: f_00008-10-0 loss: 0.962009  [   32/  130]
train() client id: f_00008-10-1 loss: 0.815714  [   64/  130]
train() client id: f_00008-10-2 loss: 0.959273  [   96/  130]
train() client id: f_00008-10-3 loss: 0.938145  [  128/  130]
train() client id: f_00008-11-0 loss: 1.003656  [   32/  130]
train() client id: f_00008-11-1 loss: 0.976561  [   64/  130]
train() client id: f_00008-11-2 loss: 0.880064  [   96/  130]
train() client id: f_00008-11-3 loss: 0.838216  [  128/  130]
train() client id: f_00009-0-0 loss: 1.096454  [   32/  118]
train() client id: f_00009-0-1 loss: 1.059392  [   64/  118]
train() client id: f_00009-0-2 loss: 1.042466  [   96/  118]
train() client id: f_00009-1-0 loss: 0.939823  [   32/  118]
train() client id: f_00009-1-1 loss: 1.021376  [   64/  118]
train() client id: f_00009-1-2 loss: 1.087970  [   96/  118]
train() client id: f_00009-2-0 loss: 1.087026  [   32/  118]
train() client id: f_00009-2-1 loss: 1.010053  [   64/  118]
train() client id: f_00009-2-2 loss: 0.880528  [   96/  118]
train() client id: f_00009-3-0 loss: 0.926641  [   32/  118]
train() client id: f_00009-3-1 loss: 0.987755  [   64/  118]
train() client id: f_00009-3-2 loss: 0.912195  [   96/  118]
train() client id: f_00009-4-0 loss: 0.935259  [   32/  118]
train() client id: f_00009-4-1 loss: 0.914031  [   64/  118]
train() client id: f_00009-4-2 loss: 0.863977  [   96/  118]
train() client id: f_00009-5-0 loss: 0.859267  [   32/  118]
train() client id: f_00009-5-1 loss: 0.845474  [   64/  118]
train() client id: f_00009-5-2 loss: 0.792761  [   96/  118]
train() client id: f_00009-6-0 loss: 0.770851  [   32/  118]
train() client id: f_00009-6-1 loss: 0.954360  [   64/  118]
train() client id: f_00009-6-2 loss: 0.961552  [   96/  118]
train() client id: f_00009-7-0 loss: 0.705653  [   32/  118]
train() client id: f_00009-7-1 loss: 0.931099  [   64/  118]
train() client id: f_00009-7-2 loss: 0.941318  [   96/  118]
train() client id: f_00009-8-0 loss: 0.806691  [   32/  118]
train() client id: f_00009-8-1 loss: 0.838273  [   64/  118]
train() client id: f_00009-8-2 loss: 0.903647  [   96/  118]
train() client id: f_00009-9-0 loss: 0.792763  [   32/  118]
train() client id: f_00009-9-1 loss: 0.886394  [   64/  118]
train() client id: f_00009-9-2 loss: 0.761119  [   96/  118]
train() client id: f_00009-10-0 loss: 0.960761  [   32/  118]
train() client id: f_00009-10-1 loss: 0.781626  [   64/  118]
train() client id: f_00009-10-2 loss: 0.861482  [   96/  118]
train() client id: f_00009-11-0 loss: 1.049762  [   32/  118]
train() client id: f_00009-11-1 loss: 0.838161  [   64/  118]
train() client id: f_00009-11-2 loss: 0.781199  [   96/  118]
At round 11 accuracy: 0.6339522546419099
At round 11 training accuracy: 0.5801475519785378
At round 11 training loss: 0.8386248447735545
update_location
xs = -4.528292 -43.998411 15.045120 34.056472 -135.103519 -70.217951 -7.215960 28.375741 -1.680116 -25.304393 
ys = 42.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 -3.124922 -19.177652 27.845030 -0.998518 
xs mean: -21.05713084058713
ys mean: 7.871751218646876
dists_uav = 108.785292 110.353270 101.134068 108.019037 168.346033 123.393331 100.308700 105.702247 103.817958 103.156722 
uav_gains = -100.914308 -101.069695 -100.122456 -100.837556 -105.675203 -102.282609 -100.033482 -100.602141 -100.406839 -100.337463 
uav_gains_db_mean: -101.2281751553788
dists_bs = 215.855395 206.358564 257.452280 258.741549 170.386589 218.894218 244.703845 281.187931 227.363954 231.050326 
bs_gains = -104.923621 -104.376489 -107.066568 -107.127312 -102.047251 -105.093619 -106.449001 -108.138966 -105.555264 -105.750844 
bs_gains_db_mean: -105.65289347614157
Round 12
-------------------------------
ene_coms = [0.00655091 0.00769647 0.00633461 0.00652931 0.00688699 0.00798443
 0.00631119 0.00646392 0.00818114 0.00826734]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.39924946 19.62039619  9.24732408  3.3120821  22.60736796 10.9007665
  4.11344187 13.28378531  9.73222092  8.85432042]
obj_prev = 111.07095480611264
eta_min = 1.0228753811266265e-10	eta_max = 0.9311809246884403
af = 23.50535574047096	bf = 1.7140817909905783	zeta = 25.855891314518058	eta = 0.9090909090909091
af = 23.50535574047096	bf = 1.7140817909905783	zeta = 43.498196202774345	eta = 0.5403754130607322
af = 23.50535574047096	bf = 1.7140817909905783	zeta = 35.20573353632023	eta = 0.6676570370624859
af = 23.50535574047096	bf = 1.7140817909905783	zeta = 33.729586739779606	eta = 0.6968764818203209
af = 23.50535574047096	bf = 1.7140817909905783	zeta = 33.65945626515251	eta = 0.6983284446221419
af = 23.50535574047096	bf = 1.7140817909905783	zeta = 33.659287611210075	eta = 0.6983319436814999
eta = 0.6983319436814999
ene_coms = [0.00655091 0.00769647 0.00633461 0.00652931 0.00688699 0.00798443
 0.00631119 0.00646392 0.00818114 0.00826734]
ene_comp = [0.02962883 0.0623146  0.02915853 0.01011143 0.07195573 0.03433182
 0.01269807 0.04209175 0.03056943 0.02774763]
ene_total = [2.88701639 5.58663805 2.83222843 1.32787229 6.2913722  3.37668813
 1.51687217 3.87457238 3.0921592  2.87386839]
ti_comp = [0.30292752 0.29147192 0.30509049 0.30314353 0.29956676 0.28859233
 0.30532473 0.3037974  0.28662524 0.28576325]
ti_coms = [0.0655091  0.07696469 0.06334613 0.06529309 0.06886985 0.07984428
 0.06311189 0.06463922 0.08181137 0.08267337]
t_total = [29.39994965 29.39994965 29.39994965 29.39994965 29.39994965 29.39994965
 29.39994965 29.39994965 29.39994965 29.39994965]
ene_coms = [0.00655091 0.00769647 0.00633461 0.00652931 0.00688699 0.00798443
 0.00631119 0.00646392 0.00818114 0.00826734]
ene_comp = [1.77152015e-05 1.78014786e-04 1.66463733e-05 7.03105784e-07
 2.59471063e-04 3.03669304e-05 1.37267972e-06 5.05013393e-05
 2.17326701e-05 1.63510321e-05]
ene_total = [0.52415326 0.62835622 0.50680822 0.52107205 0.57026215 0.63955255
 0.50372024 0.51982811 0.65456025 0.66100921]
optimize_network iter = 0 obj = 5.729322271277824
eta = 0.6983319436814999
freqs = [4.89041584e+07 1.06896409e+08 4.77866941e+07 1.66776218e+07
 1.20099657e+08 5.94815166e+07 2.07943689e+07 6.92760228e+07
 5.33264756e+07 4.85500405e+07]
eta_min = 0.6941575130402796	eta_max = 0.6983319436814843
af = 0.03974737865893113	bf = 1.7140817909905783	zeta = 0.04372211652482425	eta = 0.909090909090909
af = 0.03974737865893113	bf = 1.7140817909905783	zeta = 18.8965713287704	eta = 0.0021034174913210297
af = 0.03974737865893113	bf = 1.7140817909905783	zeta = 1.9632217347143825	eta = 0.020245995628564974
af = 0.03974737865893113	bf = 1.7140817909905783	zeta = 1.90771207921147	eta = 0.02083510352115621
af = 0.03974737865893113	bf = 1.7140817909905783	zeta = 1.9076955403539348	eta = 0.020835284152080576
eta = 0.020835284152080576
ene_coms = [0.00655091 0.00769647 0.00633461 0.00652931 0.00688699 0.00798443
 0.00631119 0.00646392 0.00818114 0.00826734]
ene_comp = [1.90991223e-04 1.91921395e-03 1.79467968e-04 7.58032777e-06
 2.79741078e-03 3.27392109e-04 1.47991419e-05 5.44465305e-04
 2.34304377e-04 1.76283833e-04]
ene_total = [0.16574474 0.23639459 0.16014395 0.16070467 0.23808384 0.20434006
 0.15551981 0.1722961  0.20688751 0.20758027]
ti_comp = [0.30802588 0.29657029 0.31018885 0.30824189 0.30466513 0.2936907
 0.31042309 0.30889576 0.29172361 0.29086161]
ti_coms = [0.0655091  0.07696469 0.06334613 0.06529309 0.06886985 0.07984428
 0.06311189 0.06463922 0.08181137 0.08267337]
t_total = [29.39994965 29.39994965 29.39994965 29.39994965 29.39994965 29.39994965
 29.39994965 29.39994965 29.39994965 29.39994965]
ene_coms = [0.00655091 0.00769647 0.00633461 0.00652931 0.00688699 0.00798443
 0.00631119 0.00646392 0.00818114 0.00826734]
ene_comp = [1.80063319e-05 1.80705087e-04 1.69239104e-05 7.14677437e-07
 2.63637275e-04 3.08152869e-05 1.39560096e-06 5.13361409e-05
 2.20482910e-05 1.65867473e-05]
ene_total = [0.51702203 0.61999156 0.49991266 0.51396087 0.56280658 0.63085861
 0.49684679 0.51279871 0.64565102 0.65200568]
optimize_network iter = 1 obj = 5.651854523077496
eta = 0.6941575130402796
freqs = [4.88978031e+07 1.06813033e+08 4.77860915e+07 1.66756512e+07
 1.20061755e+08 5.94249293e+07 2.07943689e+07 6.92703031e+07
 5.32693975e+07 4.84955182e+07]
eta_min = 0.6941575130402702	eta_max = 0.6941575130401907
af = 0.03970743443610646	bf = 1.7140817909905783	zeta = 0.04367817787971711	eta = 0.9090909090909091
af = 0.03970743443610646	bf = 1.7140817909905783	zeta = 18.896529450768764	eta = 0.0021013083137597552
af = 0.03970743443610646	bf = 1.7140817909905783	zeta = 1.9630107629069247	eta = 0.02022782309013207
af = 0.03970743443610646	bf = 1.7140817909905783	zeta = 1.9075540980898578	eta = 0.020815889036052908
af = 0.03970743443610646	bf = 1.7140817909905783	zeta = 1.9075376044316048	eta = 0.020816069022103614
eta = 0.020816069022103614
ene_coms = [0.00655091 0.00769647 0.00633461 0.00652931 0.00688699 0.00798443
 0.00631119 0.00646392 0.00818114 0.00826734]
ene_comp = [1.90987097e-04 1.91667798e-03 1.79506216e-04 7.58034280e-06
 2.79631176e-03 3.26847366e-04 1.48026692e-05 5.44505152e-04
 2.33858795e-04 1.75930041e-04]
ene_total = [0.16574139 0.2363276  0.16014174 0.16070152 0.23805215 0.20432266
 0.15551684 0.1722937  0.2068725  0.2075675 ]
ti_comp = [0.30802588 0.29657029 0.31018885 0.30824189 0.30466513 0.2936907
 0.31042309 0.30889576 0.29172361 0.29086161]
ti_coms = [0.0655091  0.07696469 0.06334613 0.06529309 0.06886985 0.07984428
 0.06311189 0.06463922 0.08181137 0.08267337]
t_total = [29.39994965 29.39994965 29.39994965 29.39994965 29.39994965 29.39994965
 29.39994965 29.39994965 29.39994965 29.39994965]
ene_coms = [0.00655091 0.00769647 0.00633461 0.00652931 0.00688699 0.00798443
 0.00631119 0.00646392 0.00818114 0.00826734]
ene_comp = [1.80063319e-05 1.80705087e-04 1.69239104e-05 7.14677437e-07
 2.63637275e-04 3.08152869e-05 1.39560096e-06 5.13361409e-05
 2.20482910e-05 1.65867473e-05]
ene_total = [0.51702203 0.61999156 0.49991266 0.51396087 0.56280658 0.63085861
 0.49684679 0.51279871 0.64565102 0.65200568]
optimize_network iter = 2 obj = 5.651854523077324
eta = 0.6941575130402702
freqs = [4.88978031e+07 1.06813033e+08 4.77860915e+07 1.66756512e+07
 1.20061755e+08 5.94249293e+07 2.07943689e+07 6.92703031e+07
 5.32693975e+07 4.84955182e+07]
Done!
ene_coms = [0.00655091 0.00769647 0.00633461 0.00652931 0.00688699 0.00798443
 0.00631119 0.00646392 0.00818114 0.00826734]
ene_comp = [1.65695964e-05 1.66286524e-04 1.55735419e-05 6.57652915e-07
 2.42601506e-04 2.83565175e-05 1.28424516e-06 4.72400008e-05
 2.02890452e-05 1.52632812e-05]
ene_total = [0.00656748 0.00786276 0.00635019 0.00652997 0.00712959 0.00801278
 0.00631247 0.00651116 0.00820143 0.0082826 ]
At round 12 energy consumption: 0.07176042130224533
At round 12 eta: 0.6941575130402702
At round 12 a_n: 24.072052692622567
At round 12 local rounds: 11.953800596345877
At round 12 global rounds: 78.70735335667122
gradient difference: 0.4747985899448395
train() client id: f_00000-0-0 loss: 1.038415  [   32/  126]
train() client id: f_00000-0-1 loss: 1.344639  [   64/  126]
train() client id: f_00000-0-2 loss: 1.215929  [   96/  126]
train() client id: f_00000-1-0 loss: 1.088654  [   32/  126]
train() client id: f_00000-1-1 loss: 1.148054  [   64/  126]
train() client id: f_00000-1-2 loss: 0.970759  [   96/  126]
train() client id: f_00000-2-0 loss: 1.007295  [   32/  126]
train() client id: f_00000-2-1 loss: 1.091833  [   64/  126]
train() client id: f_00000-2-2 loss: 1.065483  [   96/  126]
train() client id: f_00000-3-0 loss: 0.969727  [   32/  126]
train() client id: f_00000-3-1 loss: 1.035251  [   64/  126]
train() client id: f_00000-3-2 loss: 0.970719  [   96/  126]
train() client id: f_00000-4-0 loss: 1.035694  [   32/  126]
train() client id: f_00000-4-1 loss: 0.960262  [   64/  126]
train() client id: f_00000-4-2 loss: 0.994823  [   96/  126]
train() client id: f_00000-5-0 loss: 0.992190  [   32/  126]
train() client id: f_00000-5-1 loss: 1.052615  [   64/  126]
train() client id: f_00000-5-2 loss: 0.836134  [   96/  126]
train() client id: f_00000-6-0 loss: 0.945797  [   32/  126]
train() client id: f_00000-6-1 loss: 0.995232  [   64/  126]
train() client id: f_00000-6-2 loss: 0.920688  [   96/  126]
train() client id: f_00000-7-0 loss: 0.972355  [   32/  126]
train() client id: f_00000-7-1 loss: 0.962591  [   64/  126]
train() client id: f_00000-7-2 loss: 0.854828  [   96/  126]
train() client id: f_00000-8-0 loss: 1.006619  [   32/  126]
train() client id: f_00000-8-1 loss: 0.941392  [   64/  126]
train() client id: f_00000-8-2 loss: 0.888624  [   96/  126]
train() client id: f_00000-9-0 loss: 0.904096  [   32/  126]
train() client id: f_00000-9-1 loss: 0.874806  [   64/  126]
train() client id: f_00000-9-2 loss: 0.949783  [   96/  126]
train() client id: f_00000-10-0 loss: 0.854118  [   32/  126]
train() client id: f_00000-10-1 loss: 1.042579  [   64/  126]
train() client id: f_00000-10-2 loss: 0.975787  [   96/  126]
train() client id: f_00001-0-0 loss: 0.497443  [   32/  265]
train() client id: f_00001-0-1 loss: 0.582515  [   64/  265]
train() client id: f_00001-0-2 loss: 0.521023  [   96/  265]
train() client id: f_00001-0-3 loss: 0.526873  [  128/  265]
train() client id: f_00001-0-4 loss: 0.631367  [  160/  265]
train() client id: f_00001-0-5 loss: 0.583525  [  192/  265]
train() client id: f_00001-0-6 loss: 0.496341  [  224/  265]
train() client id: f_00001-0-7 loss: 0.588082  [  256/  265]
train() client id: f_00001-1-0 loss: 0.592390  [   32/  265]
train() client id: f_00001-1-1 loss: 0.638122  [   64/  265]
train() client id: f_00001-1-2 loss: 0.524161  [   96/  265]
train() client id: f_00001-1-3 loss: 0.578306  [  128/  265]
train() client id: f_00001-1-4 loss: 0.538288  [  160/  265]
train() client id: f_00001-1-5 loss: 0.474286  [  192/  265]
train() client id: f_00001-1-6 loss: 0.520901  [  224/  265]
train() client id: f_00001-1-7 loss: 0.477074  [  256/  265]
train() client id: f_00001-2-0 loss: 0.564017  [   32/  265]
train() client id: f_00001-2-1 loss: 0.532319  [   64/  265]
train() client id: f_00001-2-2 loss: 0.531713  [   96/  265]
train() client id: f_00001-2-3 loss: 0.465378  [  128/  265]
train() client id: f_00001-2-4 loss: 0.483648  [  160/  265]
train() client id: f_00001-2-5 loss: 0.620539  [  192/  265]
train() client id: f_00001-2-6 loss: 0.484629  [  224/  265]
train() client id: f_00001-2-7 loss: 0.632005  [  256/  265]
train() client id: f_00001-3-0 loss: 0.493595  [   32/  265]
train() client id: f_00001-3-1 loss: 0.556419  [   64/  265]
train() client id: f_00001-3-2 loss: 0.513465  [   96/  265]
train() client id: f_00001-3-3 loss: 0.570277  [  128/  265]
train() client id: f_00001-3-4 loss: 0.513802  [  160/  265]
train() client id: f_00001-3-5 loss: 0.552152  [  192/  265]
train() client id: f_00001-3-6 loss: 0.508879  [  224/  265]
train() client id: f_00001-3-7 loss: 0.522499  [  256/  265]
train() client id: f_00001-4-0 loss: 0.519105  [   32/  265]
train() client id: f_00001-4-1 loss: 0.445886  [   64/  265]
train() client id: f_00001-4-2 loss: 0.606817  [   96/  265]
train() client id: f_00001-4-3 loss: 0.603492  [  128/  265]
train() client id: f_00001-4-4 loss: 0.462868  [  160/  265]
train() client id: f_00001-4-5 loss: 0.534669  [  192/  265]
train() client id: f_00001-4-6 loss: 0.540332  [  224/  265]
train() client id: f_00001-4-7 loss: 0.533436  [  256/  265]
train() client id: f_00001-5-0 loss: 0.518285  [   32/  265]
train() client id: f_00001-5-1 loss: 0.421932  [   64/  265]
train() client id: f_00001-5-2 loss: 0.562434  [   96/  265]
train() client id: f_00001-5-3 loss: 0.530970  [  128/  265]
train() client id: f_00001-5-4 loss: 0.535000  [  160/  265]
train() client id: f_00001-5-5 loss: 0.467677  [  192/  265]
train() client id: f_00001-5-6 loss: 0.600782  [  224/  265]
train() client id: f_00001-5-7 loss: 0.615422  [  256/  265]
train() client id: f_00001-6-0 loss: 0.538241  [   32/  265]
train() client id: f_00001-6-1 loss: 0.488854  [   64/  265]
train() client id: f_00001-6-2 loss: 0.519673  [   96/  265]
train() client id: f_00001-6-3 loss: 0.549667  [  128/  265]
train() client id: f_00001-6-4 loss: 0.540980  [  160/  265]
train() client id: f_00001-6-5 loss: 0.566371  [  192/  265]
train() client id: f_00001-6-6 loss: 0.506057  [  224/  265]
train() client id: f_00001-6-7 loss: 0.454734  [  256/  265]
train() client id: f_00001-7-0 loss: 0.546440  [   32/  265]
train() client id: f_00001-7-1 loss: 0.649353  [   64/  265]
train() client id: f_00001-7-2 loss: 0.496821  [   96/  265]
train() client id: f_00001-7-3 loss: 0.474705  [  128/  265]
train() client id: f_00001-7-4 loss: 0.595297  [  160/  265]
train() client id: f_00001-7-5 loss: 0.443058  [  192/  265]
train() client id: f_00001-7-6 loss: 0.566253  [  224/  265]
train() client id: f_00001-7-7 loss: 0.449063  [  256/  265]
train() client id: f_00001-8-0 loss: 0.495413  [   32/  265]
train() client id: f_00001-8-1 loss: 0.498467  [   64/  265]
train() client id: f_00001-8-2 loss: 0.620363  [   96/  265]
train() client id: f_00001-8-3 loss: 0.450061  [  128/  265]
train() client id: f_00001-8-4 loss: 0.584096  [  160/  265]
train() client id: f_00001-8-5 loss: 0.559220  [  192/  265]
train() client id: f_00001-8-6 loss: 0.537908  [  224/  265]
train() client id: f_00001-8-7 loss: 0.442237  [  256/  265]
train() client id: f_00001-9-0 loss: 0.551074  [   32/  265]
train() client id: f_00001-9-1 loss: 0.469140  [   64/  265]
train() client id: f_00001-9-2 loss: 0.474999  [   96/  265]
train() client id: f_00001-9-3 loss: 0.551073  [  128/  265]
train() client id: f_00001-9-4 loss: 0.519088  [  160/  265]
train() client id: f_00001-9-5 loss: 0.524904  [  192/  265]
train() client id: f_00001-9-6 loss: 0.528762  [  224/  265]
train() client id: f_00001-9-7 loss: 0.572470  [  256/  265]
train() client id: f_00001-10-0 loss: 0.513843  [   32/  265]
train() client id: f_00001-10-1 loss: 0.432131  [   64/  265]
train() client id: f_00001-10-2 loss: 0.536080  [   96/  265]
train() client id: f_00001-10-3 loss: 0.499407  [  128/  265]
train() client id: f_00001-10-4 loss: 0.589607  [  160/  265]
train() client id: f_00001-10-5 loss: 0.546030  [  192/  265]
train() client id: f_00001-10-6 loss: 0.460089  [  224/  265]
train() client id: f_00001-10-7 loss: 0.660714  [  256/  265]
train() client id: f_00002-0-0 loss: 1.353485  [   32/  124]
train() client id: f_00002-0-1 loss: 1.131283  [   64/  124]
train() client id: f_00002-0-2 loss: 1.303288  [   96/  124]
train() client id: f_00002-1-0 loss: 1.297490  [   32/  124]
train() client id: f_00002-1-1 loss: 1.239507  [   64/  124]
train() client id: f_00002-1-2 loss: 1.166068  [   96/  124]
train() client id: f_00002-2-0 loss: 1.134888  [   32/  124]
train() client id: f_00002-2-1 loss: 1.154391  [   64/  124]
train() client id: f_00002-2-2 loss: 1.316677  [   96/  124]
train() client id: f_00002-3-0 loss: 1.256987  [   32/  124]
train() client id: f_00002-3-1 loss: 1.219816  [   64/  124]
train() client id: f_00002-3-2 loss: 1.061298  [   96/  124]
train() client id: f_00002-4-0 loss: 1.196895  [   32/  124]
train() client id: f_00002-4-1 loss: 1.197365  [   64/  124]
train() client id: f_00002-4-2 loss: 1.120579  [   96/  124]
train() client id: f_00002-5-0 loss: 1.102441  [   32/  124]
train() client id: f_00002-5-1 loss: 1.084502  [   64/  124]
train() client id: f_00002-5-2 loss: 1.253580  [   96/  124]
train() client id: f_00002-6-0 loss: 1.043680  [   32/  124]
train() client id: f_00002-6-1 loss: 1.281995  [   64/  124]
train() client id: f_00002-6-2 loss: 1.153938  [   96/  124]
train() client id: f_00002-7-0 loss: 1.154510  [   32/  124]
train() client id: f_00002-7-1 loss: 1.098334  [   64/  124]
train() client id: f_00002-7-2 loss: 1.038063  [   96/  124]
train() client id: f_00002-8-0 loss: 1.131463  [   32/  124]
train() client id: f_00002-8-1 loss: 1.172567  [   64/  124]
train() client id: f_00002-8-2 loss: 1.105618  [   96/  124]
train() client id: f_00002-9-0 loss: 1.147170  [   32/  124]
train() client id: f_00002-9-1 loss: 1.054669  [   64/  124]
train() client id: f_00002-9-2 loss: 1.125214  [   96/  124]
train() client id: f_00002-10-0 loss: 1.011318  [   32/  124]
train() client id: f_00002-10-1 loss: 1.292282  [   64/  124]
train() client id: f_00002-10-2 loss: 1.094615  [   96/  124]
train() client id: f_00003-0-0 loss: 1.011049  [   32/   43]
train() client id: f_00003-1-0 loss: 1.052268  [   32/   43]
train() client id: f_00003-2-0 loss: 0.940632  [   32/   43]
train() client id: f_00003-3-0 loss: 0.977786  [   32/   43]
train() client id: f_00003-4-0 loss: 1.047803  [   32/   43]
train() client id: f_00003-5-0 loss: 0.901598  [   32/   43]
train() client id: f_00003-6-0 loss: 0.984529  [   32/   43]
train() client id: f_00003-7-0 loss: 0.892796  [   32/   43]
train() client id: f_00003-8-0 loss: 1.009552  [   32/   43]
train() client id: f_00003-9-0 loss: 1.048120  [   32/   43]
train() client id: f_00003-10-0 loss: 0.922482  [   32/   43]
train() client id: f_00004-0-0 loss: 0.942585  [   32/  306]
train() client id: f_00004-0-1 loss: 0.916983  [   64/  306]
train() client id: f_00004-0-2 loss: 0.971308  [   96/  306]
train() client id: f_00004-0-3 loss: 0.954847  [  128/  306]
train() client id: f_00004-0-4 loss: 0.957977  [  160/  306]
train() client id: f_00004-0-5 loss: 0.933433  [  192/  306]
train() client id: f_00004-0-6 loss: 0.892466  [  224/  306]
train() client id: f_00004-0-7 loss: 0.842313  [  256/  306]
train() client id: f_00004-0-8 loss: 0.994877  [  288/  306]
train() client id: f_00004-1-0 loss: 0.937284  [   32/  306]
train() client id: f_00004-1-1 loss: 0.962149  [   64/  306]
train() client id: f_00004-1-2 loss: 0.834010  [   96/  306]
train() client id: f_00004-1-3 loss: 0.840222  [  128/  306]
train() client id: f_00004-1-4 loss: 1.056963  [  160/  306]
train() client id: f_00004-1-5 loss: 0.970059  [  192/  306]
train() client id: f_00004-1-6 loss: 0.849897  [  224/  306]
train() client id: f_00004-1-7 loss: 0.919401  [  256/  306]
train() client id: f_00004-1-8 loss: 1.110873  [  288/  306]
train() client id: f_00004-2-0 loss: 1.081879  [   32/  306]
train() client id: f_00004-2-1 loss: 0.894273  [   64/  306]
train() client id: f_00004-2-2 loss: 0.901695  [   96/  306]
train() client id: f_00004-2-3 loss: 1.051881  [  128/  306]
train() client id: f_00004-2-4 loss: 0.891683  [  160/  306]
train() client id: f_00004-2-5 loss: 0.816929  [  192/  306]
train() client id: f_00004-2-6 loss: 0.958312  [  224/  306]
train() client id: f_00004-2-7 loss: 0.842927  [  256/  306]
train() client id: f_00004-2-8 loss: 1.037429  [  288/  306]
train() client id: f_00004-3-0 loss: 0.873574  [   32/  306]
train() client id: f_00004-3-1 loss: 0.960531  [   64/  306]
train() client id: f_00004-3-2 loss: 1.079513  [   96/  306]
train() client id: f_00004-3-3 loss: 0.923061  [  128/  306]
train() client id: f_00004-3-4 loss: 0.944063  [  160/  306]
train() client id: f_00004-3-5 loss: 0.951449  [  192/  306]
train() client id: f_00004-3-6 loss: 0.947756  [  224/  306]
train() client id: f_00004-3-7 loss: 0.897150  [  256/  306]
train() client id: f_00004-3-8 loss: 0.926816  [  288/  306]
train() client id: f_00004-4-0 loss: 0.897136  [   32/  306]
train() client id: f_00004-4-1 loss: 0.946601  [   64/  306]
train() client id: f_00004-4-2 loss: 0.906940  [   96/  306]
train() client id: f_00004-4-3 loss: 1.005273  [  128/  306]
train() client id: f_00004-4-4 loss: 0.844348  [  160/  306]
train() client id: f_00004-4-5 loss: 1.036881  [  192/  306]
train() client id: f_00004-4-6 loss: 0.944491  [  224/  306]
train() client id: f_00004-4-7 loss: 0.994996  [  256/  306]
train() client id: f_00004-4-8 loss: 0.866050  [  288/  306]
train() client id: f_00004-5-0 loss: 0.999241  [   32/  306]
train() client id: f_00004-5-1 loss: 0.951190  [   64/  306]
train() client id: f_00004-5-2 loss: 0.904916  [   96/  306]
train() client id: f_00004-5-3 loss: 0.954556  [  128/  306]
train() client id: f_00004-5-4 loss: 0.815794  [  160/  306]
train() client id: f_00004-5-5 loss: 0.999492  [  192/  306]
train() client id: f_00004-5-6 loss: 0.997546  [  224/  306]
train() client id: f_00004-5-7 loss: 0.874199  [  256/  306]
train() client id: f_00004-5-8 loss: 0.924766  [  288/  306]
train() client id: f_00004-6-0 loss: 1.036880  [   32/  306]
train() client id: f_00004-6-1 loss: 0.941951  [   64/  306]
train() client id: f_00004-6-2 loss: 0.959401  [   96/  306]
train() client id: f_00004-6-3 loss: 0.945091  [  128/  306]
train() client id: f_00004-6-4 loss: 0.933471  [  160/  306]
train() client id: f_00004-6-5 loss: 0.816977  [  192/  306]
train() client id: f_00004-6-6 loss: 0.971817  [  224/  306]
train() client id: f_00004-6-7 loss: 0.843655  [  256/  306]
train() client id: f_00004-6-8 loss: 0.937818  [  288/  306]
train() client id: f_00004-7-0 loss: 0.968323  [   32/  306]
train() client id: f_00004-7-1 loss: 0.860818  [   64/  306]
train() client id: f_00004-7-2 loss: 0.876491  [   96/  306]
train() client id: f_00004-7-3 loss: 0.900673  [  128/  306]
train() client id: f_00004-7-4 loss: 0.927381  [  160/  306]
train() client id: f_00004-7-5 loss: 0.893700  [  192/  306]
train() client id: f_00004-7-6 loss: 0.972779  [  224/  306]
train() client id: f_00004-7-7 loss: 0.931430  [  256/  306]
train() client id: f_00004-7-8 loss: 1.008523  [  288/  306]
train() client id: f_00004-8-0 loss: 0.997688  [   32/  306]
train() client id: f_00004-8-1 loss: 0.862512  [   64/  306]
train() client id: f_00004-8-2 loss: 0.942813  [   96/  306]
train() client id: f_00004-8-3 loss: 0.878777  [  128/  306]
train() client id: f_00004-8-4 loss: 0.910622  [  160/  306]
train() client id: f_00004-8-5 loss: 0.898011  [  192/  306]
train() client id: f_00004-8-6 loss: 0.935871  [  224/  306]
train() client id: f_00004-8-7 loss: 0.910634  [  256/  306]
train() client id: f_00004-8-8 loss: 0.940431  [  288/  306]
train() client id: f_00004-9-0 loss: 0.870260  [   32/  306]
train() client id: f_00004-9-1 loss: 0.918049  [   64/  306]
train() client id: f_00004-9-2 loss: 1.001948  [   96/  306]
train() client id: f_00004-9-3 loss: 1.000963  [  128/  306]
train() client id: f_00004-9-4 loss: 0.865676  [  160/  306]
train() client id: f_00004-9-5 loss: 0.922582  [  192/  306]
train() client id: f_00004-9-6 loss: 0.952501  [  224/  306]
train() client id: f_00004-9-7 loss: 0.885324  [  256/  306]
train() client id: f_00004-9-8 loss: 0.914222  [  288/  306]
train() client id: f_00004-10-0 loss: 1.042184  [   32/  306]
train() client id: f_00004-10-1 loss: 0.875491  [   64/  306]
train() client id: f_00004-10-2 loss: 0.749481  [   96/  306]
train() client id: f_00004-10-3 loss: 0.982465  [  128/  306]
train() client id: f_00004-10-4 loss: 0.972388  [  160/  306]
train() client id: f_00004-10-5 loss: 0.869022  [  192/  306]
train() client id: f_00004-10-6 loss: 0.909305  [  224/  306]
train() client id: f_00004-10-7 loss: 0.974423  [  256/  306]
train() client id: f_00004-10-8 loss: 0.950683  [  288/  306]
train() client id: f_00005-0-0 loss: 0.859208  [   32/  146]
train() client id: f_00005-0-1 loss: 0.671595  [   64/  146]
train() client id: f_00005-0-2 loss: 0.628045  [   96/  146]
train() client id: f_00005-0-3 loss: 0.827701  [  128/  146]
train() client id: f_00005-1-0 loss: 0.647907  [   32/  146]
train() client id: f_00005-1-1 loss: 0.642044  [   64/  146]
train() client id: f_00005-1-2 loss: 0.671696  [   96/  146]
train() client id: f_00005-1-3 loss: 0.948557  [  128/  146]
train() client id: f_00005-2-0 loss: 0.586210  [   32/  146]
train() client id: f_00005-2-1 loss: 0.598757  [   64/  146]
train() client id: f_00005-2-2 loss: 0.986384  [   96/  146]
train() client id: f_00005-2-3 loss: 0.768412  [  128/  146]
train() client id: f_00005-3-0 loss: 0.776747  [   32/  146]
train() client id: f_00005-3-1 loss: 0.678932  [   64/  146]
train() client id: f_00005-3-2 loss: 0.636556  [   96/  146]
train() client id: f_00005-3-3 loss: 0.893734  [  128/  146]
train() client id: f_00005-4-0 loss: 0.905907  [   32/  146]
train() client id: f_00005-4-1 loss: 0.822722  [   64/  146]
train() client id: f_00005-4-2 loss: 0.499881  [   96/  146]
train() client id: f_00005-4-3 loss: 0.589869  [  128/  146]
train() client id: f_00005-5-0 loss: 0.610822  [   32/  146]
train() client id: f_00005-5-1 loss: 0.564551  [   64/  146]
train() client id: f_00005-5-2 loss: 0.705971  [   96/  146]
train() client id: f_00005-5-3 loss: 1.029825  [  128/  146]
train() client id: f_00005-6-0 loss: 0.762324  [   32/  146]
train() client id: f_00005-6-1 loss: 0.506630  [   64/  146]
train() client id: f_00005-6-2 loss: 0.622997  [   96/  146]
train() client id: f_00005-6-3 loss: 0.845089  [  128/  146]
train() client id: f_00005-7-0 loss: 0.779787  [   32/  146]
train() client id: f_00005-7-1 loss: 0.697399  [   64/  146]
train() client id: f_00005-7-2 loss: 0.728755  [   96/  146]
train() client id: f_00005-7-3 loss: 0.646017  [  128/  146]
train() client id: f_00005-8-0 loss: 0.507256  [   32/  146]
train() client id: f_00005-8-1 loss: 0.749768  [   64/  146]
train() client id: f_00005-8-2 loss: 0.730394  [   96/  146]
train() client id: f_00005-8-3 loss: 0.872284  [  128/  146]
train() client id: f_00005-9-0 loss: 0.841152  [   32/  146]
train() client id: f_00005-9-1 loss: 0.820957  [   64/  146]
train() client id: f_00005-9-2 loss: 0.708302  [   96/  146]
train() client id: f_00005-9-3 loss: 0.611238  [  128/  146]
train() client id: f_00005-10-0 loss: 0.796863  [   32/  146]
train() client id: f_00005-10-1 loss: 0.754400  [   64/  146]
train() client id: f_00005-10-2 loss: 0.721353  [   96/  146]
train() client id: f_00005-10-3 loss: 0.649118  [  128/  146]
train() client id: f_00006-0-0 loss: 0.642501  [   32/   54]
train() client id: f_00006-1-0 loss: 0.639367  [   32/   54]
train() client id: f_00006-2-0 loss: 0.588543  [   32/   54]
train() client id: f_00006-3-0 loss: 0.657995  [   32/   54]
train() client id: f_00006-4-0 loss: 0.631979  [   32/   54]
train() client id: f_00006-5-0 loss: 0.633712  [   32/   54]
train() client id: f_00006-6-0 loss: 0.590813  [   32/   54]
train() client id: f_00006-7-0 loss: 0.599068  [   32/   54]
train() client id: f_00006-8-0 loss: 0.590558  [   32/   54]
train() client id: f_00006-9-0 loss: 0.595222  [   32/   54]
train() client id: f_00006-10-0 loss: 0.652455  [   32/   54]
train() client id: f_00007-0-0 loss: 0.514014  [   32/  179]
train() client id: f_00007-0-1 loss: 0.599494  [   64/  179]
train() client id: f_00007-0-2 loss: 0.440506  [   96/  179]
train() client id: f_00007-0-3 loss: 0.523927  [  128/  179]
train() client id: f_00007-0-4 loss: 0.582213  [  160/  179]
train() client id: f_00007-1-0 loss: 0.625846  [   32/  179]
train() client id: f_00007-1-1 loss: 0.422384  [   64/  179]
train() client id: f_00007-1-2 loss: 0.626507  [   96/  179]
train() client id: f_00007-1-3 loss: 0.519226  [  128/  179]
train() client id: f_00007-1-4 loss: 0.437077  [  160/  179]
train() client id: f_00007-2-0 loss: 0.560359  [   32/  179]
train() client id: f_00007-2-1 loss: 0.642711  [   64/  179]
train() client id: f_00007-2-2 loss: 0.411272  [   96/  179]
train() client id: f_00007-2-3 loss: 0.476124  [  128/  179]
train() client id: f_00007-2-4 loss: 0.600797  [  160/  179]
train() client id: f_00007-3-0 loss: 0.483675  [   32/  179]
train() client id: f_00007-3-1 loss: 0.636800  [   64/  179]
train() client id: f_00007-3-2 loss: 0.474260  [   96/  179]
train() client id: f_00007-3-3 loss: 0.688376  [  128/  179]
train() client id: f_00007-3-4 loss: 0.424594  [  160/  179]
train() client id: f_00007-4-0 loss: 0.428733  [   32/  179]
train() client id: f_00007-4-1 loss: 0.532629  [   64/  179]
train() client id: f_00007-4-2 loss: 0.724177  [   96/  179]
train() client id: f_00007-4-3 loss: 0.473214  [  128/  179]
train() client id: f_00007-4-4 loss: 0.511481  [  160/  179]
train() client id: f_00007-5-0 loss: 0.396576  [   32/  179]
train() client id: f_00007-5-1 loss: 0.472373  [   64/  179]
train() client id: f_00007-5-2 loss: 0.454668  [   96/  179]
train() client id: f_00007-5-3 loss: 0.599288  [  128/  179]
train() client id: f_00007-5-4 loss: 0.578905  [  160/  179]
train() client id: f_00007-6-0 loss: 0.392228  [   32/  179]
train() client id: f_00007-6-1 loss: 0.462923  [   64/  179]
train() client id: f_00007-6-2 loss: 0.432641  [   96/  179]
train() client id: f_00007-6-3 loss: 0.614320  [  128/  179]
train() client id: f_00007-6-4 loss: 0.539320  [  160/  179]
train() client id: f_00007-7-0 loss: 0.532077  [   32/  179]
train() client id: f_00007-7-1 loss: 0.437419  [   64/  179]
train() client id: f_00007-7-2 loss: 0.551684  [   96/  179]
train() client id: f_00007-7-3 loss: 0.630827  [  128/  179]
train() client id: f_00007-7-4 loss: 0.370774  [  160/  179]
train() client id: f_00007-8-0 loss: 0.457157  [   32/  179]
train() client id: f_00007-8-1 loss: 0.748820  [   64/  179]
train() client id: f_00007-8-2 loss: 0.568143  [   96/  179]
train() client id: f_00007-8-3 loss: 0.438494  [  128/  179]
train() client id: f_00007-8-4 loss: 0.384658  [  160/  179]
train() client id: f_00007-9-0 loss: 0.725894  [   32/  179]
train() client id: f_00007-9-1 loss: 0.359920  [   64/  179]
train() client id: f_00007-9-2 loss: 0.567421  [   96/  179]
train() client id: f_00007-9-3 loss: 0.491569  [  128/  179]
train() client id: f_00007-9-4 loss: 0.370312  [  160/  179]
train() client id: f_00007-10-0 loss: 0.390319  [   32/  179]
train() client id: f_00007-10-1 loss: 0.383438  [   64/  179]
train() client id: f_00007-10-2 loss: 0.665231  [   96/  179]
train() client id: f_00007-10-3 loss: 0.427256  [  128/  179]
train() client id: f_00007-10-4 loss: 0.723246  [  160/  179]
train() client id: f_00008-0-0 loss: 0.857949  [   32/  130]
train() client id: f_00008-0-1 loss: 0.861898  [   64/  130]
train() client id: f_00008-0-2 loss: 0.863732  [   96/  130]
train() client id: f_00008-0-3 loss: 0.852558  [  128/  130]
train() client id: f_00008-1-0 loss: 0.967218  [   32/  130]
train() client id: f_00008-1-1 loss: 0.854816  [   64/  130]
train() client id: f_00008-1-2 loss: 0.782599  [   96/  130]
train() client id: f_00008-1-3 loss: 0.807012  [  128/  130]
train() client id: f_00008-2-0 loss: 0.957362  [   32/  130]
train() client id: f_00008-2-1 loss: 0.810486  [   64/  130]
train() client id: f_00008-2-2 loss: 0.865876  [   96/  130]
train() client id: f_00008-2-3 loss: 0.838448  [  128/  130]
train() client id: f_00008-3-0 loss: 0.789840  [   32/  130]
train() client id: f_00008-3-1 loss: 0.850865  [   64/  130]
train() client id: f_00008-3-2 loss: 0.986318  [   96/  130]
train() client id: f_00008-3-3 loss: 0.850432  [  128/  130]
train() client id: f_00008-4-0 loss: 0.864970  [   32/  130]
train() client id: f_00008-4-1 loss: 0.957418  [   64/  130]
train() client id: f_00008-4-2 loss: 0.849357  [   96/  130]
train() client id: f_00008-4-3 loss: 0.794283  [  128/  130]
train() client id: f_00008-5-0 loss: 0.738882  [   32/  130]
train() client id: f_00008-5-1 loss: 0.873941  [   64/  130]
train() client id: f_00008-5-2 loss: 0.761084  [   96/  130]
train() client id: f_00008-5-3 loss: 1.074770  [  128/  130]
train() client id: f_00008-6-0 loss: 0.909175  [   32/  130]
train() client id: f_00008-6-1 loss: 0.823676  [   64/  130]
train() client id: f_00008-6-2 loss: 0.884700  [   96/  130]
train() client id: f_00008-6-3 loss: 0.833535  [  128/  130]
train() client id: f_00008-7-0 loss: 0.898398  [   32/  130]
train() client id: f_00008-7-1 loss: 0.771625  [   64/  130]
train() client id: f_00008-7-2 loss: 0.858121  [   96/  130]
train() client id: f_00008-7-3 loss: 0.947048  [  128/  130]
train() client id: f_00008-8-0 loss: 0.860396  [   32/  130]
train() client id: f_00008-8-1 loss: 0.892872  [   64/  130]
train() client id: f_00008-8-2 loss: 0.880845  [   96/  130]
train() client id: f_00008-8-3 loss: 0.822173  [  128/  130]
train() client id: f_00008-9-0 loss: 0.929802  [   32/  130]
train() client id: f_00008-9-1 loss: 0.883296  [   64/  130]
train() client id: f_00008-9-2 loss: 0.883746  [   96/  130]
train() client id: f_00008-9-3 loss: 0.775985  [  128/  130]
train() client id: f_00008-10-0 loss: 0.883550  [   32/  130]
train() client id: f_00008-10-1 loss: 0.877836  [   64/  130]
train() client id: f_00008-10-2 loss: 0.850551  [   96/  130]
train() client id: f_00008-10-3 loss: 0.862450  [  128/  130]
train() client id: f_00009-0-0 loss: 1.205326  [   32/  118]
train() client id: f_00009-0-1 loss: 1.127551  [   64/  118]
train() client id: f_00009-0-2 loss: 1.205598  [   96/  118]
train() client id: f_00009-1-0 loss: 1.250049  [   32/  118]
train() client id: f_00009-1-1 loss: 1.005198  [   64/  118]
train() client id: f_00009-1-2 loss: 1.100466  [   96/  118]
train() client id: f_00009-2-0 loss: 1.108932  [   32/  118]
train() client id: f_00009-2-1 loss: 0.946749  [   64/  118]
train() client id: f_00009-2-2 loss: 1.166133  [   96/  118]
train() client id: f_00009-3-0 loss: 1.012842  [   32/  118]
train() client id: f_00009-3-1 loss: 1.104459  [   64/  118]
train() client id: f_00009-3-2 loss: 0.990356  [   96/  118]
train() client id: f_00009-4-0 loss: 1.069294  [   32/  118]
train() client id: f_00009-4-1 loss: 0.967657  [   64/  118]
train() client id: f_00009-4-2 loss: 1.064162  [   96/  118]
train() client id: f_00009-5-0 loss: 1.039168  [   32/  118]
train() client id: f_00009-5-1 loss: 0.943302  [   64/  118]
train() client id: f_00009-5-2 loss: 1.056678  [   96/  118]
train() client id: f_00009-6-0 loss: 1.042681  [   32/  118]
train() client id: f_00009-6-1 loss: 1.052420  [   64/  118]
train() client id: f_00009-6-2 loss: 0.933226  [   96/  118]
train() client id: f_00009-7-0 loss: 0.957213  [   32/  118]
train() client id: f_00009-7-1 loss: 0.884837  [   64/  118]
train() client id: f_00009-7-2 loss: 1.101205  [   96/  118]
train() client id: f_00009-8-0 loss: 0.845119  [   32/  118]
train() client id: f_00009-8-1 loss: 1.130576  [   64/  118]
train() client id: f_00009-8-2 loss: 0.999384  [   96/  118]
train() client id: f_00009-9-0 loss: 0.932433  [   32/  118]
train() client id: f_00009-9-1 loss: 1.015667  [   64/  118]
train() client id: f_00009-9-2 loss: 0.884155  [   96/  118]
train() client id: f_00009-10-0 loss: 0.972896  [   32/  118]
train() client id: f_00009-10-1 loss: 0.872638  [   64/  118]
train() client id: f_00009-10-2 loss: 0.912006  [   96/  118]
At round 12 accuracy: 0.6339522546419099
At round 12 training accuracy: 0.5788061703554661
At round 12 training loss: 0.8541489998945955
update_location
xs = -4.528292 -38.998411 10.045120 29.056472 -130.103519 -65.217951 -12.215960 33.375741 -1.680116 -20.304393 
ys = 47.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 -3.124922 -19.177652 22.845030 -0.998518 
xs mean: -20.05713084058713
ys mean: 7.871751218646876
dists_uav = 110.838257 108.456720 100.511932 106.548335 164.360432 120.618135 100.791839 107.152799 102.590049 102.045409 
uav_gains = -101.117312 -100.881463 -100.055458 -100.688706 -105.409813 -102.035545 -100.085652 -100.750131 -100.277654 -100.219858 
uav_gains_db_mean: -101.15215911668702
dists_bs = 212.824884 209.568301 253.783816 254.718716 171.626206 221.331198 241.302987 284.825227 230.631561 234.320740 
bs_gains = -104.751687 -104.564176 -106.892049 -106.936763 -102.135400 -105.228253 -106.278815 -108.295256 -105.728784 -105.921760 
bs_gains_db_mean: -105.6732941249902
Round 13
-------------------------------
ene_coms = [0.00660873 0.00776986 0.00631696 0.00648781 0.00691456 0.00804084
 0.0063249  0.00650487 0.00825753 0.00834412]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.26688382 19.34295664  9.11531128  3.26395651 22.28632562 10.74700051
  4.05523622 13.09573835  9.59556217  8.73016364]
obj_prev = 109.49913477723024
eta_min = 7.544832105964318e-11	eta_max = 0.9314280006023182
af = 23.170874003776266	bf = 1.6983252839809773	zeta = 25.487961404153893	eta = 0.9090909090909091
af = 23.170874003776266	bf = 1.6983252839809773	zeta = 42.974199966546756	eta = 0.539181044017425
af = 23.170874003776266	bf = 1.6983252839809773	zeta = 34.74480325278577	eta = 0.6668874719248401
af = 23.170874003776266	bf = 1.6983252839809773	zeta = 33.278943831206455	eta = 0.6962623009101746
af = 23.170874003776266	bf = 1.6983252839809773	zeta = 33.20907974295021	eta = 0.697727072930863
af = 23.170874003776266	bf = 1.6983252839809773	zeta = 33.20891058576631	eta = 0.6977306269633412
eta = 0.6977306269633412
ene_coms = [0.00660873 0.00776986 0.00631696 0.00648781 0.00691456 0.00804084
 0.0063249  0.00650487 0.00825753 0.00834412]
ene_comp = [0.02969991 0.06246411 0.02922849 0.01013569 0.07212836 0.03441419
 0.01272853 0.04219274 0.03064277 0.02781421]
ene_total = [2.8503921  5.51368268 2.79047757 1.3050196  6.20522572 3.33291087
 1.49578066 3.82298173 3.05384836 2.8385913 ]
ti_comp = [0.30777591 0.29616461 0.31069363 0.30898506 0.30471758 0.2934548
 0.31061418 0.30881446 0.29128795 0.29042202]
ti_coms = [0.0660873  0.0776986  0.06316958 0.06487815 0.06914563 0.08040841
 0.06324903 0.06504875 0.08257526 0.08344119]
t_total = [29.34994545 29.34994545 29.34994545 29.34994545 29.34994545 29.34994545
 29.34994545 29.34994545 29.34994545 29.34994545]
ene_coms = [0.00660873 0.00776986 0.00631696 0.00648781 0.00691456 0.00804084
 0.0063249  0.00650487 0.00825753 0.00834412]
ene_comp = [1.72852784e-05 1.73662204e-04 1.61671834e-05 6.81654637e-07
 2.52582492e-04 2.95808144e-05 1.33589621e-06 4.92263712e-05
 2.11942972e-05 1.59448679e-05]
ene_total = [0.52017202 0.6236023  0.49717887 0.50937618 0.56265316 0.63356444
 0.49663823 0.51452643 0.64991683 0.65630263]
optimize_network iter = 0 obj = 5.6639310786303145
eta = 0.6977306269633412
freqs = [4.82492517e+07 1.05455050e+08 4.70374755e+07 1.64015780e+07
 1.18352810e+08 5.86362652e+07 2.04893011e+07 6.83140544e+07
 5.25987591e+07 4.78858415e+07]
eta_min = 0.6977306269632638	eta_max = 0.697730626963332
af = 0.038084933764188325	bf = 1.6983252839809773	zeta = 0.04189342714060716	eta = 0.909090909090909
af = 0.038084933764188325	bf = 1.6983252839809773	zeta = 18.721506824524628	eta = 0.0020342878445178446
af = 0.038084933764188325	bf = 1.6983252839809773	zeta = 1.938303898387922	eta = 0.019648587507801733
af = 0.038084933764188325	bf = 1.6983252839809773	zeta = 1.8850276770742185	eta = 0.02020391224350645
af = 0.038084933764188325	bf = 1.6983252839809773	zeta = 1.885012719496423	eta = 0.020204072561570106
eta = 0.020204072561570106
ene_coms = [0.00660873 0.00776986 0.00631696 0.00648781 0.00691456 0.00804084
 0.0063249  0.00650487 0.00825753 0.00834412]
ene_comp = [1.87387533e-04 1.88265016e-03 1.75266405e-04 7.38973235e-06
 2.73821509e-03 3.20681895e-04 1.44822831e-05 5.33656911e-04
 2.29764715e-04 1.72856311e-04]
ene_total = [0.16459398 0.23377247 0.15723406 0.15730622 0.23377894 0.20250626
 0.15353246 0.17046497 0.20555222 0.20627113]
ti_comp = [0.30777591 0.29616461 0.31069363 0.30898506 0.30471758 0.2934548
 0.31061418 0.30881446 0.29128795 0.29042202]
ti_coms = [0.0660873  0.0776986  0.06316958 0.06487815 0.06914563 0.08040841
 0.06324903 0.06504875 0.08257526 0.08344119]
t_total = [29.34994545 29.34994545 29.34994545 29.34994545 29.34994545 29.34994545
 29.34994545 29.34994545 29.34994545 29.34994545]
ene_coms = [0.00660873 0.00776986 0.00631696 0.00648781 0.00691456 0.00804084
 0.0063249  0.00650487 0.00825753 0.00834412]
ene_comp = [1.72852784e-05 1.73662204e-04 1.61671834e-05 6.81654637e-07
 2.52582492e-04 2.95808144e-05 1.33589621e-06 4.92263712e-05
 2.11942972e-05 1.59448679e-05]
ene_total = [0.52017202 0.6236023  0.49717887 0.50937618 0.56265316 0.63356444
 0.49663823 0.51452643 0.64991683 0.65630263]
optimize_network iter = 1 obj = 5.6639310786288775
eta = 0.6977306269632638
freqs = [4.82492517e+07 1.05455050e+08 4.70374755e+07 1.64015780e+07
 1.18352810e+08 5.86362652e+07 2.04893011e+07 6.83140544e+07
 5.25987591e+07 4.78858415e+07]
Done!
ene_coms = [0.00660873 0.00776986 0.00631696 0.00648781 0.00691456 0.00804084
 0.0063249  0.00650487 0.00825753 0.00834412]
ene_comp = [1.61329727e-05 1.62085188e-04 1.50894143e-05 6.36212816e-07
 2.35744335e-04 2.76088391e-05 1.24684003e-06 4.59447447e-05
 1.97814007e-05 1.48819193e-05]
ene_total = [0.00662486 0.00793195 0.00633205 0.00648845 0.00715031 0.00806845
 0.00632615 0.00655082 0.00827731 0.008359  ]
At round 13 energy consumption: 0.07210934062067423
At round 13 eta: 0.6977306269632638
At round 13 a_n: 23.72950684565325
At round 13 local rounds: 11.785680533983305
At round 13 global rounds: 78.50450281236166
gradient difference: 0.3848196864128113
train() client id: f_00000-0-0 loss: 1.345076  [   32/  126]
train() client id: f_00000-0-1 loss: 1.220233  [   64/  126]
train() client id: f_00000-0-2 loss: 1.109717  [   96/  126]
train() client id: f_00000-1-0 loss: 1.157012  [   32/  126]
train() client id: f_00000-1-1 loss: 1.156758  [   64/  126]
train() client id: f_00000-1-2 loss: 1.135008  [   96/  126]
train() client id: f_00000-2-0 loss: 1.079634  [   32/  126]
train() client id: f_00000-2-1 loss: 1.151366  [   64/  126]
train() client id: f_00000-2-2 loss: 0.978064  [   96/  126]
train() client id: f_00000-3-0 loss: 1.044036  [   32/  126]
train() client id: f_00000-3-1 loss: 1.048241  [   64/  126]
train() client id: f_00000-3-2 loss: 1.040993  [   96/  126]
train() client id: f_00000-4-0 loss: 0.988892  [   32/  126]
train() client id: f_00000-4-1 loss: 1.018466  [   64/  126]
train() client id: f_00000-4-2 loss: 1.025537  [   96/  126]
train() client id: f_00000-5-0 loss: 1.045662  [   32/  126]
train() client id: f_00000-5-1 loss: 0.910491  [   64/  126]
train() client id: f_00000-5-2 loss: 0.938689  [   96/  126]
train() client id: f_00000-6-0 loss: 0.903988  [   32/  126]
train() client id: f_00000-6-1 loss: 0.939030  [   64/  126]
train() client id: f_00000-6-2 loss: 0.961485  [   96/  126]
train() client id: f_00000-7-0 loss: 0.898852  [   32/  126]
train() client id: f_00000-7-1 loss: 0.929236  [   64/  126]
train() client id: f_00000-7-2 loss: 0.904482  [   96/  126]
train() client id: f_00000-8-0 loss: 0.981600  [   32/  126]
train() client id: f_00000-8-1 loss: 0.992121  [   64/  126]
train() client id: f_00000-8-2 loss: 0.889748  [   96/  126]
train() client id: f_00000-9-0 loss: 0.839020  [   32/  126]
train() client id: f_00000-9-1 loss: 0.821868  [   64/  126]
train() client id: f_00000-9-2 loss: 0.938509  [   96/  126]
train() client id: f_00000-10-0 loss: 0.891950  [   32/  126]
train() client id: f_00000-10-1 loss: 0.839013  [   64/  126]
train() client id: f_00000-10-2 loss: 0.894127  [   96/  126]
train() client id: f_00001-0-0 loss: 0.492082  [   32/  265]
train() client id: f_00001-0-1 loss: 0.363910  [   64/  265]
train() client id: f_00001-0-2 loss: 0.521397  [   96/  265]
train() client id: f_00001-0-3 loss: 0.356072  [  128/  265]
train() client id: f_00001-0-4 loss: 0.473086  [  160/  265]
train() client id: f_00001-0-5 loss: 0.349256  [  192/  265]
train() client id: f_00001-0-6 loss: 0.470068  [  224/  265]
train() client id: f_00001-0-7 loss: 0.430802  [  256/  265]
train() client id: f_00001-1-0 loss: 0.352319  [   32/  265]
train() client id: f_00001-1-1 loss: 0.518437  [   64/  265]
train() client id: f_00001-1-2 loss: 0.349117  [   96/  265]
train() client id: f_00001-1-3 loss: 0.429031  [  128/  265]
train() client id: f_00001-1-4 loss: 0.352123  [  160/  265]
train() client id: f_00001-1-5 loss: 0.367280  [  192/  265]
train() client id: f_00001-1-6 loss: 0.506808  [  224/  265]
train() client id: f_00001-1-7 loss: 0.477520  [  256/  265]
train() client id: f_00001-2-0 loss: 0.385106  [   32/  265]
train() client id: f_00001-2-1 loss: 0.375274  [   64/  265]
train() client id: f_00001-2-2 loss: 0.344485  [   96/  265]
train() client id: f_00001-2-3 loss: 0.456722  [  128/  265]
train() client id: f_00001-2-4 loss: 0.447019  [  160/  265]
train() client id: f_00001-2-5 loss: 0.352047  [  192/  265]
train() client id: f_00001-2-6 loss: 0.387883  [  224/  265]
train() client id: f_00001-2-7 loss: 0.420673  [  256/  265]
train() client id: f_00001-3-0 loss: 0.314614  [   32/  265]
train() client id: f_00001-3-1 loss: 0.367019  [   64/  265]
train() client id: f_00001-3-2 loss: 0.492126  [   96/  265]
train() client id: f_00001-3-3 loss: 0.372416  [  128/  265]
train() client id: f_00001-3-4 loss: 0.404073  [  160/  265]
train() client id: f_00001-3-5 loss: 0.332806  [  192/  265]
train() client id: f_00001-3-6 loss: 0.420119  [  224/  265]
train() client id: f_00001-3-7 loss: 0.443662  [  256/  265]
train() client id: f_00001-4-0 loss: 0.469570  [   32/  265]
train() client id: f_00001-4-1 loss: 0.361661  [   64/  265]
train() client id: f_00001-4-2 loss: 0.378857  [   96/  265]
train() client id: f_00001-4-3 loss: 0.421110  [  128/  265]
train() client id: f_00001-4-4 loss: 0.416078  [  160/  265]
train() client id: f_00001-4-5 loss: 0.342382  [  192/  265]
train() client id: f_00001-4-6 loss: 0.383340  [  224/  265]
train() client id: f_00001-4-7 loss: 0.328535  [  256/  265]
train() client id: f_00001-5-0 loss: 0.394982  [   32/  265]
train() client id: f_00001-5-1 loss: 0.311320  [   64/  265]
train() client id: f_00001-5-2 loss: 0.492755  [   96/  265]
train() client id: f_00001-5-3 loss: 0.385240  [  128/  265]
train() client id: f_00001-5-4 loss: 0.496864  [  160/  265]
train() client id: f_00001-5-5 loss: 0.302588  [  192/  265]
train() client id: f_00001-5-6 loss: 0.425436  [  224/  265]
train() client id: f_00001-5-7 loss: 0.334312  [  256/  265]
train() client id: f_00001-6-0 loss: 0.439774  [   32/  265]
train() client id: f_00001-6-1 loss: 0.388982  [   64/  265]
train() client id: f_00001-6-2 loss: 0.353386  [   96/  265]
train() client id: f_00001-6-3 loss: 0.373184  [  128/  265]
train() client id: f_00001-6-4 loss: 0.433914  [  160/  265]
train() client id: f_00001-6-5 loss: 0.399523  [  192/  265]
train() client id: f_00001-6-6 loss: 0.408576  [  224/  265]
train() client id: f_00001-6-7 loss: 0.312815  [  256/  265]
train() client id: f_00001-7-0 loss: 0.506557  [   32/  265]
train() client id: f_00001-7-1 loss: 0.310321  [   64/  265]
train() client id: f_00001-7-2 loss: 0.314619  [   96/  265]
train() client id: f_00001-7-3 loss: 0.297465  [  128/  265]
train() client id: f_00001-7-4 loss: 0.304847  [  160/  265]
train() client id: f_00001-7-5 loss: 0.531285  [  192/  265]
train() client id: f_00001-7-6 loss: 0.380791  [  224/  265]
train() client id: f_00001-7-7 loss: 0.457697  [  256/  265]
train() client id: f_00001-8-0 loss: 0.493487  [   32/  265]
train() client id: f_00001-8-1 loss: 0.391543  [   64/  265]
train() client id: f_00001-8-2 loss: 0.432053  [   96/  265]
train() client id: f_00001-8-3 loss: 0.289199  [  128/  265]
train() client id: f_00001-8-4 loss: 0.457894  [  160/  265]
train() client id: f_00001-8-5 loss: 0.338775  [  192/  265]
train() client id: f_00001-8-6 loss: 0.313739  [  224/  265]
train() client id: f_00001-8-7 loss: 0.376021  [  256/  265]
train() client id: f_00001-9-0 loss: 0.478105  [   32/  265]
train() client id: f_00001-9-1 loss: 0.305292  [   64/  265]
train() client id: f_00001-9-2 loss: 0.301661  [   96/  265]
train() client id: f_00001-9-3 loss: 0.554444  [  128/  265]
train() client id: f_00001-9-4 loss: 0.374254  [  160/  265]
train() client id: f_00001-9-5 loss: 0.338193  [  192/  265]
train() client id: f_00001-9-6 loss: 0.351195  [  224/  265]
train() client id: f_00001-9-7 loss: 0.372981  [  256/  265]
train() client id: f_00001-10-0 loss: 0.356814  [   32/  265]
train() client id: f_00001-10-1 loss: 0.594868  [   64/  265]
train() client id: f_00001-10-2 loss: 0.334203  [   96/  265]
train() client id: f_00001-10-3 loss: 0.328236  [  128/  265]
train() client id: f_00001-10-4 loss: 0.382155  [  160/  265]
train() client id: f_00001-10-5 loss: 0.394442  [  192/  265]
train() client id: f_00001-10-6 loss: 0.354640  [  224/  265]
train() client id: f_00001-10-7 loss: 0.333305  [  256/  265]
train() client id: f_00002-0-0 loss: 1.092100  [   32/  124]
train() client id: f_00002-0-1 loss: 1.301706  [   64/  124]
train() client id: f_00002-0-2 loss: 1.224482  [   96/  124]
train() client id: f_00002-1-0 loss: 1.267112  [   32/  124]
train() client id: f_00002-1-1 loss: 1.185115  [   64/  124]
train() client id: f_00002-1-2 loss: 1.119734  [   96/  124]
train() client id: f_00002-2-0 loss: 1.147455  [   32/  124]
train() client id: f_00002-2-1 loss: 1.050337  [   64/  124]
train() client id: f_00002-2-2 loss: 1.171375  [   96/  124]
train() client id: f_00002-3-0 loss: 0.915072  [   32/  124]
train() client id: f_00002-3-1 loss: 0.954545  [   64/  124]
train() client id: f_00002-3-2 loss: 1.231784  [   96/  124]
train() client id: f_00002-4-0 loss: 1.068405  [   32/  124]
train() client id: f_00002-4-1 loss: 1.085089  [   64/  124]
train() client id: f_00002-4-2 loss: 1.049871  [   96/  124]
train() client id: f_00002-5-0 loss: 1.214717  [   32/  124]
train() client id: f_00002-5-1 loss: 0.983541  [   64/  124]
train() client id: f_00002-5-2 loss: 0.946052  [   96/  124]
train() client id: f_00002-6-0 loss: 0.988840  [   32/  124]
train() client id: f_00002-6-1 loss: 1.037996  [   64/  124]
train() client id: f_00002-6-2 loss: 1.059162  [   96/  124]
train() client id: f_00002-7-0 loss: 0.844722  [   32/  124]
train() client id: f_00002-7-1 loss: 1.012691  [   64/  124]
train() client id: f_00002-7-2 loss: 1.039783  [   96/  124]
train() client id: f_00002-8-0 loss: 1.121857  [   32/  124]
train() client id: f_00002-8-1 loss: 0.948998  [   64/  124]
train() client id: f_00002-8-2 loss: 0.905407  [   96/  124]
train() client id: f_00002-9-0 loss: 1.064062  [   32/  124]
train() client id: f_00002-9-1 loss: 0.863001  [   64/  124]
train() client id: f_00002-9-2 loss: 1.001264  [   96/  124]
train() client id: f_00002-10-0 loss: 1.123960  [   32/  124]
train() client id: f_00002-10-1 loss: 1.108286  [   64/  124]
train() client id: f_00002-10-2 loss: 0.884830  [   96/  124]
train() client id: f_00003-0-0 loss: 0.948194  [   32/   43]
train() client id: f_00003-1-0 loss: 0.970884  [   32/   43]
train() client id: f_00003-2-0 loss: 0.882237  [   32/   43]
train() client id: f_00003-3-0 loss: 0.915180  [   32/   43]
train() client id: f_00003-4-0 loss: 0.836710  [   32/   43]
train() client id: f_00003-5-0 loss: 1.012527  [   32/   43]
train() client id: f_00003-6-0 loss: 0.943744  [   32/   43]
train() client id: f_00003-7-0 loss: 0.829321  [   32/   43]
train() client id: f_00003-8-0 loss: 0.946581  [   32/   43]
train() client id: f_00003-9-0 loss: 0.929291  [   32/   43]
train() client id: f_00003-10-0 loss: 0.872730  [   32/   43]
train() client id: f_00004-0-0 loss: 0.894011  [   32/  306]
train() client id: f_00004-0-1 loss: 0.987181  [   64/  306]
train() client id: f_00004-0-2 loss: 0.942518  [   96/  306]
train() client id: f_00004-0-3 loss: 0.782888  [  128/  306]
train() client id: f_00004-0-4 loss: 0.836132  [  160/  306]
train() client id: f_00004-0-5 loss: 0.967891  [  192/  306]
train() client id: f_00004-0-6 loss: 0.934007  [  224/  306]
train() client id: f_00004-0-7 loss: 0.830024  [  256/  306]
train() client id: f_00004-0-8 loss: 0.912885  [  288/  306]
train() client id: f_00004-1-0 loss: 0.928345  [   32/  306]
train() client id: f_00004-1-1 loss: 0.993239  [   64/  306]
train() client id: f_00004-1-2 loss: 0.757770  [   96/  306]
train() client id: f_00004-1-3 loss: 0.949825  [  128/  306]
train() client id: f_00004-1-4 loss: 0.873775  [  160/  306]
train() client id: f_00004-1-5 loss: 0.947531  [  192/  306]
train() client id: f_00004-1-6 loss: 0.769871  [  224/  306]
train() client id: f_00004-1-7 loss: 0.897168  [  256/  306]
train() client id: f_00004-1-8 loss: 0.888870  [  288/  306]
train() client id: f_00004-2-0 loss: 0.979275  [   32/  306]
train() client id: f_00004-2-1 loss: 0.882324  [   64/  306]
train() client id: f_00004-2-2 loss: 0.899172  [   96/  306]
train() client id: f_00004-2-3 loss: 0.866331  [  128/  306]
train() client id: f_00004-2-4 loss: 0.936508  [  160/  306]
train() client id: f_00004-2-5 loss: 0.906272  [  192/  306]
train() client id: f_00004-2-6 loss: 0.879543  [  224/  306]
train() client id: f_00004-2-7 loss: 0.866333  [  256/  306]
train() client id: f_00004-2-8 loss: 0.818822  [  288/  306]
train() client id: f_00004-3-0 loss: 0.848731  [   32/  306]
train() client id: f_00004-3-1 loss: 0.873794  [   64/  306]
train() client id: f_00004-3-2 loss: 0.809822  [   96/  306]
train() client id: f_00004-3-3 loss: 0.841838  [  128/  306]
train() client id: f_00004-3-4 loss: 0.958880  [  160/  306]
train() client id: f_00004-3-5 loss: 0.982546  [  192/  306]
train() client id: f_00004-3-6 loss: 0.795021  [  224/  306]
train() client id: f_00004-3-7 loss: 0.877775  [  256/  306]
train() client id: f_00004-3-8 loss: 0.924985  [  288/  306]
train() client id: f_00004-4-0 loss: 0.821077  [   32/  306]
train() client id: f_00004-4-1 loss: 0.919247  [   64/  306]
train() client id: f_00004-4-2 loss: 0.812654  [   96/  306]
train() client id: f_00004-4-3 loss: 0.942575  [  128/  306]
train() client id: f_00004-4-4 loss: 0.854888  [  160/  306]
train() client id: f_00004-4-5 loss: 0.953235  [  192/  306]
train() client id: f_00004-4-6 loss: 0.955864  [  224/  306]
train() client id: f_00004-4-7 loss: 0.735288  [  256/  306]
train() client id: f_00004-4-8 loss: 0.865905  [  288/  306]
train() client id: f_00004-5-0 loss: 0.805815  [   32/  306]
train() client id: f_00004-5-1 loss: 0.982299  [   64/  306]
train() client id: f_00004-5-2 loss: 0.865347  [   96/  306]
train() client id: f_00004-5-3 loss: 0.949026  [  128/  306]
train() client id: f_00004-5-4 loss: 0.890871  [  160/  306]
train() client id: f_00004-5-5 loss: 0.886068  [  192/  306]
train() client id: f_00004-5-6 loss: 0.910603  [  224/  306]
train() client id: f_00004-5-7 loss: 0.877633  [  256/  306]
train() client id: f_00004-5-8 loss: 0.847451  [  288/  306]
train() client id: f_00004-6-0 loss: 0.884635  [   32/  306]
train() client id: f_00004-6-1 loss: 0.937798  [   64/  306]
train() client id: f_00004-6-2 loss: 0.808244  [   96/  306]
train() client id: f_00004-6-3 loss: 0.949014  [  128/  306]
train() client id: f_00004-6-4 loss: 0.911768  [  160/  306]
train() client id: f_00004-6-5 loss: 0.893883  [  192/  306]
train() client id: f_00004-6-6 loss: 0.783181  [  224/  306]
train() client id: f_00004-6-7 loss: 0.803171  [  256/  306]
train() client id: f_00004-6-8 loss: 0.922225  [  288/  306]
train() client id: f_00004-7-0 loss: 0.829826  [   32/  306]
train() client id: f_00004-7-1 loss: 0.888354  [   64/  306]
train() client id: f_00004-7-2 loss: 0.791431  [   96/  306]
train() client id: f_00004-7-3 loss: 0.944784  [  128/  306]
train() client id: f_00004-7-4 loss: 0.990060  [  160/  306]
train() client id: f_00004-7-5 loss: 0.864325  [  192/  306]
train() client id: f_00004-7-6 loss: 0.839171  [  224/  306]
train() client id: f_00004-7-7 loss: 0.919782  [  256/  306]
train() client id: f_00004-7-8 loss: 0.830241  [  288/  306]
train() client id: f_00004-8-0 loss: 0.859839  [   32/  306]
train() client id: f_00004-8-1 loss: 0.833950  [   64/  306]
train() client id: f_00004-8-2 loss: 0.941610  [   96/  306]
train() client id: f_00004-8-3 loss: 0.798737  [  128/  306]
train() client id: f_00004-8-4 loss: 0.811375  [  160/  306]
train() client id: f_00004-8-5 loss: 0.871045  [  192/  306]
train() client id: f_00004-8-6 loss: 0.897482  [  224/  306]
train() client id: f_00004-8-7 loss: 0.920231  [  256/  306]
train() client id: f_00004-8-8 loss: 0.915681  [  288/  306]
train() client id: f_00004-9-0 loss: 0.871005  [   32/  306]
train() client id: f_00004-9-1 loss: 0.819937  [   64/  306]
train() client id: f_00004-9-2 loss: 0.899213  [   96/  306]
train() client id: f_00004-9-3 loss: 0.937117  [  128/  306]
train() client id: f_00004-9-4 loss: 0.801824  [  160/  306]
train() client id: f_00004-9-5 loss: 0.753186  [  192/  306]
train() client id: f_00004-9-6 loss: 0.873753  [  224/  306]
train() client id: f_00004-9-7 loss: 0.949846  [  256/  306]
train() client id: f_00004-9-8 loss: 0.963298  [  288/  306]
train() client id: f_00004-10-0 loss: 0.846051  [   32/  306]
train() client id: f_00004-10-1 loss: 0.854397  [   64/  306]
train() client id: f_00004-10-2 loss: 0.789378  [   96/  306]
train() client id: f_00004-10-3 loss: 0.821733  [  128/  306]
train() client id: f_00004-10-4 loss: 0.836355  [  160/  306]
train() client id: f_00004-10-5 loss: 0.893878  [  192/  306]
train() client id: f_00004-10-6 loss: 0.942789  [  224/  306]
train() client id: f_00004-10-7 loss: 0.918340  [  256/  306]
train() client id: f_00004-10-8 loss: 0.976768  [  288/  306]
train() client id: f_00005-0-0 loss: 0.793121  [   32/  146]
train() client id: f_00005-0-1 loss: 0.696095  [   64/  146]
train() client id: f_00005-0-2 loss: 0.997723  [   96/  146]
train() client id: f_00005-0-3 loss: 0.697403  [  128/  146]
train() client id: f_00005-1-0 loss: 0.851895  [   32/  146]
train() client id: f_00005-1-1 loss: 0.637299  [   64/  146]
train() client id: f_00005-1-2 loss: 0.810207  [   96/  146]
train() client id: f_00005-1-3 loss: 0.875582  [  128/  146]
train() client id: f_00005-2-0 loss: 0.617347  [   32/  146]
train() client id: f_00005-2-1 loss: 0.736028  [   64/  146]
train() client id: f_00005-2-2 loss: 0.975050  [   96/  146]
train() client id: f_00005-2-3 loss: 0.641625  [  128/  146]
train() client id: f_00005-3-0 loss: 0.658156  [   32/  146]
train() client id: f_00005-3-1 loss: 0.801498  [   64/  146]
train() client id: f_00005-3-2 loss: 0.676433  [   96/  146]
train() client id: f_00005-3-3 loss: 0.879791  [  128/  146]
train() client id: f_00005-4-0 loss: 0.657289  [   32/  146]
train() client id: f_00005-4-1 loss: 0.922723  [   64/  146]
train() client id: f_00005-4-2 loss: 0.636704  [   96/  146]
train() client id: f_00005-4-3 loss: 0.715685  [  128/  146]
train() client id: f_00005-5-0 loss: 0.858289  [   32/  146]
train() client id: f_00005-5-1 loss: 0.578639  [   64/  146]
train() client id: f_00005-5-2 loss: 0.742417  [   96/  146]
train() client id: f_00005-5-3 loss: 0.718846  [  128/  146]
train() client id: f_00005-6-0 loss: 0.699809  [   32/  146]
train() client id: f_00005-6-1 loss: 0.928400  [   64/  146]
train() client id: f_00005-6-2 loss: 0.744253  [   96/  146]
train() client id: f_00005-6-3 loss: 0.710946  [  128/  146]
train() client id: f_00005-7-0 loss: 0.675678  [   32/  146]
train() client id: f_00005-7-1 loss: 0.533403  [   64/  146]
train() client id: f_00005-7-2 loss: 1.031823  [   96/  146]
train() client id: f_00005-7-3 loss: 0.766118  [  128/  146]
train() client id: f_00005-8-0 loss: 0.824907  [   32/  146]
train() client id: f_00005-8-1 loss: 0.754782  [   64/  146]
train() client id: f_00005-8-2 loss: 0.811172  [   96/  146]
train() client id: f_00005-8-3 loss: 0.722444  [  128/  146]
train() client id: f_00005-9-0 loss: 0.704846  [   32/  146]
train() client id: f_00005-9-1 loss: 0.802676  [   64/  146]
train() client id: f_00005-9-2 loss: 0.652282  [   96/  146]
train() client id: f_00005-9-3 loss: 0.678532  [  128/  146]
train() client id: f_00005-10-0 loss: 0.655123  [   32/  146]
train() client id: f_00005-10-1 loss: 0.679717  [   64/  146]
train() client id: f_00005-10-2 loss: 0.993603  [   96/  146]
train() client id: f_00005-10-3 loss: 0.812828  [  128/  146]
train() client id: f_00006-0-0 loss: 0.606168  [   32/   54]
train() client id: f_00006-1-0 loss: 0.558571  [   32/   54]
train() client id: f_00006-2-0 loss: 0.557701  [   32/   54]
train() client id: f_00006-3-0 loss: 0.594610  [   32/   54]
train() client id: f_00006-4-0 loss: 0.573745  [   32/   54]
train() client id: f_00006-5-0 loss: 0.523596  [   32/   54]
train() client id: f_00006-6-0 loss: 0.555531  [   32/   54]
train() client id: f_00006-7-0 loss: 0.563207  [   32/   54]
train() client id: f_00006-8-0 loss: 0.546763  [   32/   54]
train() client id: f_00006-9-0 loss: 0.548445  [   32/   54]
train() client id: f_00006-10-0 loss: 0.613277  [   32/   54]
train() client id: f_00007-0-0 loss: 0.533217  [   32/  179]
train() client id: f_00007-0-1 loss: 0.297641  [   64/  179]
train() client id: f_00007-0-2 loss: 0.449842  [   96/  179]
train() client id: f_00007-0-3 loss: 0.416938  [  128/  179]
train() client id: f_00007-0-4 loss: 0.380627  [  160/  179]
train() client id: f_00007-1-0 loss: 0.374968  [   32/  179]
train() client id: f_00007-1-1 loss: 0.284490  [   64/  179]
train() client id: f_00007-1-2 loss: 0.254117  [   96/  179]
train() client id: f_00007-1-3 loss: 0.357814  [  128/  179]
train() client id: f_00007-1-4 loss: 0.394506  [  160/  179]
train() client id: f_00007-2-0 loss: 0.504992  [   32/  179]
train() client id: f_00007-2-1 loss: 0.336967  [   64/  179]
train() client id: f_00007-2-2 loss: 0.264310  [   96/  179]
train() client id: f_00007-2-3 loss: 0.310987  [  128/  179]
train() client id: f_00007-2-4 loss: 0.456582  [  160/  179]
train() client id: f_00007-3-0 loss: 0.427269  [   32/  179]
train() client id: f_00007-3-1 loss: 0.240480  [   64/  179]
train() client id: f_00007-3-2 loss: 0.467242  [   96/  179]
train() client id: f_00007-3-3 loss: 0.309005  [  128/  179]
train() client id: f_00007-3-4 loss: 0.256460  [  160/  179]
train() client id: f_00007-4-0 loss: 0.319839  [   32/  179]
train() client id: f_00007-4-1 loss: 0.302785  [   64/  179]
train() client id: f_00007-4-2 loss: 0.365509  [   96/  179]
train() client id: f_00007-4-3 loss: 0.511598  [  128/  179]
train() client id: f_00007-4-4 loss: 0.319096  [  160/  179]
train() client id: f_00007-5-0 loss: 0.393440  [   32/  179]
train() client id: f_00007-5-1 loss: 0.200325  [   64/  179]
train() client id: f_00007-5-2 loss: 0.233171  [   96/  179]
train() client id: f_00007-5-3 loss: 0.374805  [  128/  179]
train() client id: f_00007-5-4 loss: 0.503378  [  160/  179]
train() client id: f_00007-6-0 loss: 0.204876  [   32/  179]
train() client id: f_00007-6-1 loss: 0.219724  [   64/  179]
train() client id: f_00007-6-2 loss: 0.371374  [   96/  179]
train() client id: f_00007-6-3 loss: 0.294402  [  128/  179]
train() client id: f_00007-6-4 loss: 0.328505  [  160/  179]
train() client id: f_00007-7-0 loss: 0.365858  [   32/  179]
train() client id: f_00007-7-1 loss: 0.289195  [   64/  179]
train() client id: f_00007-7-2 loss: 0.309605  [   96/  179]
train() client id: f_00007-7-3 loss: 0.243635  [  128/  179]
train() client id: f_00007-7-4 loss: 0.415534  [  160/  179]
train() client id: f_00007-8-0 loss: 0.353292  [   32/  179]
train() client id: f_00007-8-1 loss: 0.223255  [   64/  179]
train() client id: f_00007-8-2 loss: 0.169717  [   96/  179]
train() client id: f_00007-8-3 loss: 0.470826  [  128/  179]
train() client id: f_00007-8-4 loss: 0.376440  [  160/  179]
train() client id: f_00007-9-0 loss: 0.358773  [   32/  179]
train() client id: f_00007-9-1 loss: 0.260308  [   64/  179]
train() client id: f_00007-9-2 loss: 0.364458  [   96/  179]
train() client id: f_00007-9-3 loss: 0.276444  [  128/  179]
train() client id: f_00007-9-4 loss: 0.270523  [  160/  179]
train() client id: f_00007-10-0 loss: 0.283046  [   32/  179]
train() client id: f_00007-10-1 loss: 0.461596  [   64/  179]
train() client id: f_00007-10-2 loss: 0.186732  [   96/  179]
train() client id: f_00007-10-3 loss: 0.338126  [  128/  179]
train() client id: f_00007-10-4 loss: 0.295409  [  160/  179]
train() client id: f_00008-0-0 loss: 0.808987  [   32/  130]
train() client id: f_00008-0-1 loss: 0.742614  [   64/  130]
train() client id: f_00008-0-2 loss: 0.891376  [   96/  130]
train() client id: f_00008-0-3 loss: 0.782233  [  128/  130]
train() client id: f_00008-1-0 loss: 0.809070  [   32/  130]
train() client id: f_00008-1-1 loss: 0.828971  [   64/  130]
train() client id: f_00008-1-2 loss: 0.810730  [   96/  130]
train() client id: f_00008-1-3 loss: 0.782393  [  128/  130]
train() client id: f_00008-2-0 loss: 0.906416  [   32/  130]
train() client id: f_00008-2-1 loss: 0.805928  [   64/  130]
train() client id: f_00008-2-2 loss: 0.793901  [   96/  130]
train() client id: f_00008-2-3 loss: 0.720227  [  128/  130]
train() client id: f_00008-3-0 loss: 0.906048  [   32/  130]
train() client id: f_00008-3-1 loss: 0.783294  [   64/  130]
train() client id: f_00008-3-2 loss: 0.776971  [   96/  130]
train() client id: f_00008-3-3 loss: 0.761129  [  128/  130]
train() client id: f_00008-4-0 loss: 0.900701  [   32/  130]
train() client id: f_00008-4-1 loss: 0.697071  [   64/  130]
train() client id: f_00008-4-2 loss: 0.843553  [   96/  130]
train() client id: f_00008-4-3 loss: 0.768190  [  128/  130]
train() client id: f_00008-5-0 loss: 0.740159  [   32/  130]
train() client id: f_00008-5-1 loss: 0.876085  [   64/  130]
train() client id: f_00008-5-2 loss: 0.782647  [   96/  130]
train() client id: f_00008-5-3 loss: 0.773480  [  128/  130]
train() client id: f_00008-6-0 loss: 0.901627  [   32/  130]
train() client id: f_00008-6-1 loss: 0.782940  [   64/  130]
train() client id: f_00008-6-2 loss: 0.721594  [   96/  130]
train() client id: f_00008-6-3 loss: 0.779490  [  128/  130]
train() client id: f_00008-7-0 loss: 0.791990  [   32/  130]
train() client id: f_00008-7-1 loss: 0.831187  [   64/  130]
train() client id: f_00008-7-2 loss: 0.867954  [   96/  130]
train() client id: f_00008-7-3 loss: 0.725693  [  128/  130]
train() client id: f_00008-8-0 loss: 0.796150  [   32/  130]
train() client id: f_00008-8-1 loss: 0.768849  [   64/  130]
train() client id: f_00008-8-2 loss: 0.765521  [   96/  130]
train() client id: f_00008-8-3 loss: 0.891302  [  128/  130]
train() client id: f_00008-9-0 loss: 0.864816  [   32/  130]
train() client id: f_00008-9-1 loss: 0.786036  [   64/  130]
train() client id: f_00008-9-2 loss: 0.706754  [   96/  130]
train() client id: f_00008-9-3 loss: 0.844666  [  128/  130]
train() client id: f_00008-10-0 loss: 0.979777  [   32/  130]
train() client id: f_00008-10-1 loss: 0.679570  [   64/  130]
train() client id: f_00008-10-2 loss: 0.820857  [   96/  130]
train() client id: f_00008-10-3 loss: 0.734517  [  128/  130]
train() client id: f_00009-0-0 loss: 1.123873  [   32/  118]
train() client id: f_00009-0-1 loss: 1.108843  [   64/  118]
train() client id: f_00009-0-2 loss: 1.170376  [   96/  118]
train() client id: f_00009-1-0 loss: 1.101255  [   32/  118]
train() client id: f_00009-1-1 loss: 1.045557  [   64/  118]
train() client id: f_00009-1-2 loss: 1.022882  [   96/  118]
train() client id: f_00009-2-0 loss: 1.065504  [   32/  118]
train() client id: f_00009-2-1 loss: 1.030981  [   64/  118]
train() client id: f_00009-2-2 loss: 0.908035  [   96/  118]
train() client id: f_00009-3-0 loss: 1.047007  [   32/  118]
train() client id: f_00009-3-1 loss: 0.983615  [   64/  118]
train() client id: f_00009-3-2 loss: 1.035725  [   96/  118]
train() client id: f_00009-4-0 loss: 0.972226  [   32/  118]
train() client id: f_00009-4-1 loss: 1.016202  [   64/  118]
train() client id: f_00009-4-2 loss: 0.945669  [   96/  118]
train() client id: f_00009-5-0 loss: 1.133005  [   32/  118]
train() client id: f_00009-5-1 loss: 0.902298  [   64/  118]
train() client id: f_00009-5-2 loss: 0.799076  [   96/  118]
train() client id: f_00009-6-0 loss: 0.917384  [   32/  118]
train() client id: f_00009-6-1 loss: 0.863144  [   64/  118]
train() client id: f_00009-6-2 loss: 0.921382  [   96/  118]
train() client id: f_00009-7-0 loss: 1.013689  [   32/  118]
train() client id: f_00009-7-1 loss: 0.920981  [   64/  118]
train() client id: f_00009-7-2 loss: 0.806843  [   96/  118]
train() client id: f_00009-8-0 loss: 0.892898  [   32/  118]
train() client id: f_00009-8-1 loss: 1.024001  [   64/  118]
train() client id: f_00009-8-2 loss: 0.844357  [   96/  118]
train() client id: f_00009-9-0 loss: 0.808528  [   32/  118]
train() client id: f_00009-9-1 loss: 0.912536  [   64/  118]
train() client id: f_00009-9-2 loss: 0.934750  [   96/  118]
train() client id: f_00009-10-0 loss: 0.886282  [   32/  118]
train() client id: f_00009-10-1 loss: 0.747866  [   64/  118]
train() client id: f_00009-10-2 loss: 1.057818  [   96/  118]
At round 13 accuracy: 0.6339522546419099
At round 13 training accuracy: 0.574111334674715
At round 13 training loss: 0.8496229445376459
update_location
xs = -4.528292 -33.998411 5.045120 24.056472 -125.103519 -60.217951 -17.215960 38.375741 -1.680116 -15.304393 
ys = 52.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 -3.124922 -19.177652 17.845030 -0.998518 
xs mean: -19.05713084058713
ys mean: 7.871751218646876
dists_uav = 113.075191 106.760836 100.135894 105.294743 160.431657 117.989639 101.519232 108.813969 101.593641 101.169271 
uav_gains = -101.334276 -100.710340 -100.014761 -100.560201 -105.143179 -101.796265 -100.163728 -100.917170 -100.171683 -100.126234 
uav_gains_db_mean: -101.09378365980847
dists_bs = 209.869747 212.847102 250.161496 250.731050 173.001501 223.853344 237.958591 288.503323 233.960396 237.651353 
bs_gains = -104.581655 -104.752956 -106.717232 -106.744887 -102.232456 -105.366040 -106.109098 -108.451282 -105.903045 -106.093387 
bs_gains_db_mean: -105.69520366500035
Round 14
-------------------------------
ene_coms = [0.00667165 0.00784507 0.00630628 0.00645241 0.00694518 0.00809938
 0.00634554 0.00655172 0.00833564 0.00842262]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.13459863 19.0655092   8.98347554  3.21600346 21.96533597 10.59324576
  3.99718456 12.90780222  9.45889142  8.60599437]
obj_prev = 107.92804113361386
eta_min = 5.518206648613655e-11	eta_max = 0.93168010499007
af = 22.83639226708156	bf = 1.6832878983847146	zeta = 25.120031493789718	eta = 0.9090909090909091
af = 22.83639226708156	bf = 1.6832878983847146	zeta = 42.45811406586588	eta = 0.5378569625503181
af = 22.83639226708156	bf = 1.6832878983847146	zeta = 34.28714389995952	eta = 0.6660336694625802
af = 22.83639226708156	bf = 1.6832878983847146	zeta = 32.83067936610738	eta = 0.6955808624129971
af = 22.83639226708156	bf = 1.6832878983847146	zeta = 32.76101914024366	eta = 0.6970598859981532
af = 22.83639226708156	bf = 1.6832878983847146	zeta = 32.76084921198774	eta = 0.697063501599505
eta = 0.697063501599505
ene_coms = [0.00667165 0.00784507 0.00630628 0.00645241 0.00694518 0.00809938
 0.00634554 0.00655172 0.00833564 0.00842262]
ene_comp = [0.02977885 0.06263012 0.02930617 0.01016262 0.07232007 0.03450565
 0.01276236 0.04230488 0.03072421 0.02788813]
ene_total = [2.81401024 5.44074592 2.74931227 1.28269491 6.11934578 3.28914566
 1.47514664 3.77177149 3.01545477 2.80322153]
ti_comp = [0.31281216 0.30107797 0.31646583 0.31500456 0.31007686 0.29853483
 0.31607327 0.31401146 0.2961722  0.29530244]
ti_coms = [0.06671649 0.07845067 0.06306282 0.06452408 0.06945178 0.08099382
 0.06345537 0.06551718 0.08335644 0.08422621]
t_total = [29.29994125 29.29994125 29.29994125 29.29994125 29.29994125 29.29994125
 29.29994125 29.29994125 29.29994125 29.29994125]
ene_coms = [0.00667165 0.00784507 0.00630628 0.00645241 0.00694518 0.00809938
 0.00634554 0.00655172 0.00833564 0.00842262]
ene_comp = [1.68669504e-05 1.69383788e-04 1.57073743e-05 6.61094983e-07
 2.45876909e-04 2.88111595e-05 1.30046276e-06 4.79910525e-05
 2.06649159e-05 1.55454803e-05]
ene_total = [0.51635922 0.6187226  0.48806305 0.49818257 0.55515572 0.62750354
 0.4899814  0.5095033  0.64511431 0.65143378]
optimize_network iter = 0 obj = 5.600019488806516
eta = 0.697063501599505
freqs = [4.75986147e+07 1.04009806e+08 4.63022671e+07 1.61309151e+07
 1.16616354e+08 5.77916697e+07 2.01889337e+07 6.73619923e+07
 5.18688294e+07 4.72196072e+07]
eta_min = 0.6970635015994918	eta_max = 0.6970635015992603
af = 0.036473181004098174	bf = 1.6832878983847146	zeta = 0.040120499104507996	eta = 0.909090909090909
af = 0.036473181004098174	bf = 1.6832878983847146	zeta = 18.554405802066732	eta = 0.0019657423359812206
af = 0.036473181004098174	bf = 1.6832878983847146	zeta = 1.9143476941804183	eta = 0.019052537381258363
af = 0.036473181004098174	bf = 1.6832878983847146	zeta = 1.86324141411954	eta = 0.01957512361399142
af = 0.036473181004098174	bf = 1.6832878983847146	zeta = 1.8632279166296741	eta = 0.019575265418990285
eta = 0.019575265418990285
ene_coms = [0.00667165 0.00784507 0.00630628 0.00645241 0.00694518 0.00809938
 0.00634554 0.00655172 0.00833564 0.00842262]
ene_comp = [1.83845550e-04 1.84624102e-03 1.71206460e-04 7.20577033e-06
 2.67999694e-03 3.14034448e-04 1.41747195e-05 5.23090495e-04
 2.25242426e-04 1.69441855e-04]
ene_total = [0.16353033 0.23117557 0.1545134  0.15408704 0.22959804 0.20069285
 0.15170398 0.16876184 0.20421059 0.20495427]
ti_comp = [0.31281216 0.30107797 0.31646583 0.31500456 0.31007686 0.29853483
 0.31607327 0.31401146 0.2961722  0.29530244]
ti_coms = [0.06671649 0.07845067 0.06306282 0.06452408 0.06945178 0.08099382
 0.06345537 0.06551718 0.08335644 0.08422621]
t_total = [29.29994125 29.29994125 29.29994125 29.29994125 29.29994125 29.29994125
 29.29994125 29.29994125 29.29994125 29.29994125]
ene_coms = [0.00667165 0.00784507 0.00630628 0.00645241 0.00694518 0.00809938
 0.00634554 0.00655172 0.00833564 0.00842262]
ene_comp = [1.68669504e-05 1.69383788e-04 1.57073743e-05 6.61094983e-07
 2.45876909e-04 2.88111595e-05 1.30046276e-06 4.79910525e-05
 2.06649159e-05 1.55454803e-05]
ene_total = [0.51635922 0.6187226  0.48806305 0.49818257 0.55515572 0.62750354
 0.4899814  0.5095033  0.64511431 0.65143378]
optimize_network iter = 1 obj = 5.600019488806274
eta = 0.6970635015994918
freqs = [4.75986147e+07 1.04009806e+08 4.63022671e+07 1.61309151e+07
 1.16616354e+08 5.77916697e+07 2.01889337e+07 6.73619923e+07
 5.18688294e+07 4.72196072e+07]
Done!
ene_coms = [0.00667165 0.00784507 0.00630628 0.00645241 0.00694518 0.00809938
 0.00634554 0.00655172 0.00833564 0.00842262]
ene_comp = [1.57008029e-05 1.57672929e-04 1.46213976e-05 6.15388186e-07
 2.28877467e-04 2.68192130e-05 1.21055134e-06 4.46730462e-05
 1.92361846e-05 1.44706966e-05]
ene_total = [0.00668735 0.00800274 0.0063209  0.00645302 0.00717406 0.0081262
 0.00634675 0.00659639 0.00835488 0.00843709]
At round 14 energy consumption: 0.07249938429332317
At round 14 eta: 0.6970635015994918
At round 14 a_n: 23.386960998683932
At round 14 local rounds: 11.817004261308622
At round 14 global rounds: 77.20086923221893
gradient difference: 0.40694084763526917
train() client id: f_00000-0-0 loss: 1.386477  [   32/  126]
train() client id: f_00000-0-1 loss: 1.189260  [   64/  126]
train() client id: f_00000-0-2 loss: 1.331011  [   96/  126]
train() client id: f_00000-1-0 loss: 1.241555  [   32/  126]
train() client id: f_00000-1-1 loss: 1.141877  [   64/  126]
train() client id: f_00000-1-2 loss: 1.096230  [   96/  126]
train() client id: f_00000-2-0 loss: 1.146019  [   32/  126]
train() client id: f_00000-2-1 loss: 1.038044  [   64/  126]
train() client id: f_00000-2-2 loss: 1.157268  [   96/  126]
train() client id: f_00000-3-0 loss: 1.085736  [   32/  126]
train() client id: f_00000-3-1 loss: 0.977156  [   64/  126]
train() client id: f_00000-3-2 loss: 0.971967  [   96/  126]
train() client id: f_00000-4-0 loss: 1.013166  [   32/  126]
train() client id: f_00000-4-1 loss: 0.923128  [   64/  126]
train() client id: f_00000-4-2 loss: 1.037519  [   96/  126]
train() client id: f_00000-5-0 loss: 0.993380  [   32/  126]
train() client id: f_00000-5-1 loss: 0.909479  [   64/  126]
train() client id: f_00000-5-2 loss: 1.067261  [   96/  126]
train() client id: f_00000-6-0 loss: 0.936520  [   32/  126]
train() client id: f_00000-6-1 loss: 1.002171  [   64/  126]
train() client id: f_00000-6-2 loss: 0.886080  [   96/  126]
train() client id: f_00000-7-0 loss: 0.875329  [   32/  126]
train() client id: f_00000-7-1 loss: 0.863131  [   64/  126]
train() client id: f_00000-7-2 loss: 0.933009  [   96/  126]
train() client id: f_00000-8-0 loss: 0.880906  [   32/  126]
train() client id: f_00000-8-1 loss: 0.906255  [   64/  126]
train() client id: f_00000-8-2 loss: 0.959218  [   96/  126]
train() client id: f_00000-9-0 loss: 0.819734  [   32/  126]
train() client id: f_00000-9-1 loss: 0.923164  [   64/  126]
train() client id: f_00000-9-2 loss: 0.942339  [   96/  126]
train() client id: f_00000-10-0 loss: 0.917826  [   32/  126]
train() client id: f_00000-10-1 loss: 0.863073  [   64/  126]
train() client id: f_00000-10-2 loss: 0.905067  [   96/  126]
train() client id: f_00001-0-0 loss: 0.471120  [   32/  265]
train() client id: f_00001-0-1 loss: 0.572840  [   64/  265]
train() client id: f_00001-0-2 loss: 0.749349  [   96/  265]
train() client id: f_00001-0-3 loss: 0.490758  [  128/  265]
train() client id: f_00001-0-4 loss: 0.502088  [  160/  265]
train() client id: f_00001-0-5 loss: 0.481491  [  192/  265]
train() client id: f_00001-0-6 loss: 0.570022  [  224/  265]
train() client id: f_00001-0-7 loss: 0.528824  [  256/  265]
train() client id: f_00001-1-0 loss: 0.508037  [   32/  265]
train() client id: f_00001-1-1 loss: 0.531902  [   64/  265]
train() client id: f_00001-1-2 loss: 0.483167  [   96/  265]
train() client id: f_00001-1-3 loss: 0.475144  [  128/  265]
train() client id: f_00001-1-4 loss: 0.467834  [  160/  265]
train() client id: f_00001-1-5 loss: 0.537951  [  192/  265]
train() client id: f_00001-1-6 loss: 0.505520  [  224/  265]
train() client id: f_00001-1-7 loss: 0.741478  [  256/  265]
train() client id: f_00001-2-0 loss: 0.444675  [   32/  265]
train() client id: f_00001-2-1 loss: 0.485918  [   64/  265]
train() client id: f_00001-2-2 loss: 0.639006  [   96/  265]
train() client id: f_00001-2-3 loss: 0.469839  [  128/  265]
train() client id: f_00001-2-4 loss: 0.567411  [  160/  265]
train() client id: f_00001-2-5 loss: 0.595613  [  192/  265]
train() client id: f_00001-2-6 loss: 0.481177  [  224/  265]
train() client id: f_00001-2-7 loss: 0.560593  [  256/  265]
train() client id: f_00001-3-0 loss: 0.451914  [   32/  265]
train() client id: f_00001-3-1 loss: 0.505373  [   64/  265]
train() client id: f_00001-3-2 loss: 0.502117  [   96/  265]
train() client id: f_00001-3-3 loss: 0.624016  [  128/  265]
train() client id: f_00001-3-4 loss: 0.675084  [  160/  265]
train() client id: f_00001-3-5 loss: 0.431286  [  192/  265]
train() client id: f_00001-3-6 loss: 0.494737  [  224/  265]
train() client id: f_00001-3-7 loss: 0.526133  [  256/  265]
train() client id: f_00001-4-0 loss: 0.539809  [   32/  265]
train() client id: f_00001-4-1 loss: 0.542986  [   64/  265]
train() client id: f_00001-4-2 loss: 0.430081  [   96/  265]
train() client id: f_00001-4-3 loss: 0.560648  [  128/  265]
train() client id: f_00001-4-4 loss: 0.607743  [  160/  265]
train() client id: f_00001-4-5 loss: 0.430287  [  192/  265]
train() client id: f_00001-4-6 loss: 0.444834  [  224/  265]
train() client id: f_00001-4-7 loss: 0.637781  [  256/  265]
train() client id: f_00001-5-0 loss: 0.455108  [   32/  265]
train() client id: f_00001-5-1 loss: 0.607125  [   64/  265]
train() client id: f_00001-5-2 loss: 0.459904  [   96/  265]
train() client id: f_00001-5-3 loss: 0.638746  [  128/  265]
train() client id: f_00001-5-4 loss: 0.470725  [  160/  265]
train() client id: f_00001-5-5 loss: 0.478836  [  192/  265]
train() client id: f_00001-5-6 loss: 0.509253  [  224/  265]
train() client id: f_00001-5-7 loss: 0.488548  [  256/  265]
train() client id: f_00001-6-0 loss: 0.463640  [   32/  265]
train() client id: f_00001-6-1 loss: 0.524102  [   64/  265]
train() client id: f_00001-6-2 loss: 0.506787  [   96/  265]
train() client id: f_00001-6-3 loss: 0.551360  [  128/  265]
train() client id: f_00001-6-4 loss: 0.444365  [  160/  265]
train() client id: f_00001-6-5 loss: 0.582565  [  192/  265]
train() client id: f_00001-6-6 loss: 0.443789  [  224/  265]
train() client id: f_00001-6-7 loss: 0.602237  [  256/  265]
train() client id: f_00001-7-0 loss: 0.666211  [   32/  265]
train() client id: f_00001-7-1 loss: 0.424925  [   64/  265]
train() client id: f_00001-7-2 loss: 0.583153  [   96/  265]
train() client id: f_00001-7-3 loss: 0.431970  [  128/  265]
train() client id: f_00001-7-4 loss: 0.587781  [  160/  265]
train() client id: f_00001-7-5 loss: 0.531457  [  192/  265]
train() client id: f_00001-7-6 loss: 0.425979  [  224/  265]
train() client id: f_00001-7-7 loss: 0.473738  [  256/  265]
train() client id: f_00001-8-0 loss: 0.556081  [   32/  265]
train() client id: f_00001-8-1 loss: 0.479272  [   64/  265]
train() client id: f_00001-8-2 loss: 0.537522  [   96/  265]
train() client id: f_00001-8-3 loss: 0.473283  [  128/  265]
train() client id: f_00001-8-4 loss: 0.431216  [  160/  265]
train() client id: f_00001-8-5 loss: 0.707905  [  192/  265]
train() client id: f_00001-8-6 loss: 0.486156  [  224/  265]
train() client id: f_00001-8-7 loss: 0.491041  [  256/  265]
train() client id: f_00001-9-0 loss: 0.418189  [   32/  265]
train() client id: f_00001-9-1 loss: 0.590381  [   64/  265]
train() client id: f_00001-9-2 loss: 0.420841  [   96/  265]
train() client id: f_00001-9-3 loss: 0.604321  [  128/  265]
train() client id: f_00001-9-4 loss: 0.427241  [  160/  265]
train() client id: f_00001-9-5 loss: 0.697728  [  192/  265]
train() client id: f_00001-9-6 loss: 0.524276  [  224/  265]
train() client id: f_00001-9-7 loss: 0.474723  [  256/  265]
train() client id: f_00001-10-0 loss: 0.737256  [   32/  265]
train() client id: f_00001-10-1 loss: 0.512650  [   64/  265]
train() client id: f_00001-10-2 loss: 0.492408  [   96/  265]
train() client id: f_00001-10-3 loss: 0.559778  [  128/  265]
train() client id: f_00001-10-4 loss: 0.429714  [  160/  265]
train() client id: f_00001-10-5 loss: 0.489557  [  192/  265]
train() client id: f_00001-10-6 loss: 0.460752  [  224/  265]
train() client id: f_00001-10-7 loss: 0.477889  [  256/  265]
train() client id: f_00002-0-0 loss: 1.370474  [   32/  124]
train() client id: f_00002-0-1 loss: 1.470217  [   64/  124]
train() client id: f_00002-0-2 loss: 1.254142  [   96/  124]
train() client id: f_00002-1-0 loss: 1.205574  [   32/  124]
train() client id: f_00002-1-1 loss: 1.319290  [   64/  124]
train() client id: f_00002-1-2 loss: 1.213685  [   96/  124]
train() client id: f_00002-2-0 loss: 1.242745  [   32/  124]
train() client id: f_00002-2-1 loss: 1.260179  [   64/  124]
train() client id: f_00002-2-2 loss: 1.208814  [   96/  124]
train() client id: f_00002-3-0 loss: 1.248857  [   32/  124]
train() client id: f_00002-3-1 loss: 1.090836  [   64/  124]
train() client id: f_00002-3-2 loss: 1.238584  [   96/  124]
train() client id: f_00002-4-0 loss: 1.284269  [   32/  124]
train() client id: f_00002-4-1 loss: 1.207641  [   64/  124]
train() client id: f_00002-4-2 loss: 1.027428  [   96/  124]
train() client id: f_00002-5-0 loss: 1.101123  [   32/  124]
train() client id: f_00002-5-1 loss: 1.195631  [   64/  124]
train() client id: f_00002-5-2 loss: 1.270757  [   96/  124]
train() client id: f_00002-6-0 loss: 1.003184  [   32/  124]
train() client id: f_00002-6-1 loss: 1.258559  [   64/  124]
train() client id: f_00002-6-2 loss: 1.068522  [   96/  124]
train() client id: f_00002-7-0 loss: 1.138770  [   32/  124]
train() client id: f_00002-7-1 loss: 0.964413  [   64/  124]
train() client id: f_00002-7-2 loss: 1.271534  [   96/  124]
train() client id: f_00002-8-0 loss: 1.079295  [   32/  124]
train() client id: f_00002-8-1 loss: 1.048971  [   64/  124]
train() client id: f_00002-8-2 loss: 1.191723  [   96/  124]
train() client id: f_00002-9-0 loss: 1.110871  [   32/  124]
train() client id: f_00002-9-1 loss: 1.017834  [   64/  124]
train() client id: f_00002-9-2 loss: 1.272177  [   96/  124]
train() client id: f_00002-10-0 loss: 1.089993  [   32/  124]
train() client id: f_00002-10-1 loss: 1.028354  [   64/  124]
train() client id: f_00002-10-2 loss: 1.062639  [   96/  124]
train() client id: f_00003-0-0 loss: 0.881910  [   32/   43]
train() client id: f_00003-1-0 loss: 0.957724  [   32/   43]
train() client id: f_00003-2-0 loss: 0.785115  [   32/   43]
train() client id: f_00003-3-0 loss: 1.012395  [   32/   43]
train() client id: f_00003-4-0 loss: 0.879924  [   32/   43]
train() client id: f_00003-5-0 loss: 1.015851  [   32/   43]
train() client id: f_00003-6-0 loss: 0.877204  [   32/   43]
train() client id: f_00003-7-0 loss: 0.848117  [   32/   43]
train() client id: f_00003-8-0 loss: 0.847414  [   32/   43]
train() client id: f_00003-9-0 loss: 0.861948  [   32/   43]
train() client id: f_00003-10-0 loss: 1.015369  [   32/   43]
train() client id: f_00004-0-0 loss: 0.750465  [   32/  306]
train() client id: f_00004-0-1 loss: 0.748165  [   64/  306]
train() client id: f_00004-0-2 loss: 0.774115  [   96/  306]
train() client id: f_00004-0-3 loss: 0.762022  [  128/  306]
train() client id: f_00004-0-4 loss: 0.775821  [  160/  306]
train() client id: f_00004-0-5 loss: 0.770301  [  192/  306]
train() client id: f_00004-0-6 loss: 0.700301  [  224/  306]
train() client id: f_00004-0-7 loss: 0.724917  [  256/  306]
train() client id: f_00004-0-8 loss: 0.696856  [  288/  306]
train() client id: f_00004-1-0 loss: 0.761585  [   32/  306]
train() client id: f_00004-1-1 loss: 0.769922  [   64/  306]
train() client id: f_00004-1-2 loss: 0.749854  [   96/  306]
train() client id: f_00004-1-3 loss: 0.808449  [  128/  306]
train() client id: f_00004-1-4 loss: 0.669749  [  160/  306]
train() client id: f_00004-1-5 loss: 0.735778  [  192/  306]
train() client id: f_00004-1-6 loss: 0.732072  [  224/  306]
train() client id: f_00004-1-7 loss: 0.734473  [  256/  306]
train() client id: f_00004-1-8 loss: 0.776519  [  288/  306]
train() client id: f_00004-2-0 loss: 0.797176  [   32/  306]
train() client id: f_00004-2-1 loss: 0.644083  [   64/  306]
train() client id: f_00004-2-2 loss: 0.744340  [   96/  306]
train() client id: f_00004-2-3 loss: 0.804833  [  128/  306]
train() client id: f_00004-2-4 loss: 0.813718  [  160/  306]
train() client id: f_00004-2-5 loss: 0.709495  [  192/  306]
train() client id: f_00004-2-6 loss: 0.666990  [  224/  306]
train() client id: f_00004-2-7 loss: 0.889552  [  256/  306]
train() client id: f_00004-2-8 loss: 0.662679  [  288/  306]
train() client id: f_00004-3-0 loss: 0.756943  [   32/  306]
train() client id: f_00004-3-1 loss: 0.790724  [   64/  306]
train() client id: f_00004-3-2 loss: 0.741114  [   96/  306]
train() client id: f_00004-3-3 loss: 0.869850  [  128/  306]
train() client id: f_00004-3-4 loss: 0.689832  [  160/  306]
train() client id: f_00004-3-5 loss: 0.741670  [  192/  306]
train() client id: f_00004-3-6 loss: 0.796271  [  224/  306]
train() client id: f_00004-3-7 loss: 0.707832  [  256/  306]
train() client id: f_00004-3-8 loss: 0.743008  [  288/  306]
train() client id: f_00004-4-0 loss: 0.809621  [   32/  306]
train() client id: f_00004-4-1 loss: 0.863542  [   64/  306]
train() client id: f_00004-4-2 loss: 0.766430  [   96/  306]
train() client id: f_00004-4-3 loss: 0.769704  [  128/  306]
train() client id: f_00004-4-4 loss: 0.714954  [  160/  306]
train() client id: f_00004-4-5 loss: 0.801451  [  192/  306]
train() client id: f_00004-4-6 loss: 0.671365  [  224/  306]
train() client id: f_00004-4-7 loss: 0.721439  [  256/  306]
train() client id: f_00004-4-8 loss: 0.791595  [  288/  306]
train() client id: f_00004-5-0 loss: 0.792897  [   32/  306]
train() client id: f_00004-5-1 loss: 0.690624  [   64/  306]
train() client id: f_00004-5-2 loss: 0.772499  [   96/  306]
train() client id: f_00004-5-3 loss: 0.907961  [  128/  306]
train() client id: f_00004-5-4 loss: 0.868053  [  160/  306]
train() client id: f_00004-5-5 loss: 0.745578  [  192/  306]
train() client id: f_00004-5-6 loss: 0.783493  [  224/  306]
train() client id: f_00004-5-7 loss: 0.704616  [  256/  306]
train() client id: f_00004-5-8 loss: 0.619359  [  288/  306]
train() client id: f_00004-6-0 loss: 0.851276  [   32/  306]
train() client id: f_00004-6-1 loss: 0.726594  [   64/  306]
train() client id: f_00004-6-2 loss: 0.805682  [   96/  306]
train() client id: f_00004-6-3 loss: 0.702479  [  128/  306]
train() client id: f_00004-6-4 loss: 0.769635  [  160/  306]
train() client id: f_00004-6-5 loss: 0.710342  [  192/  306]
train() client id: f_00004-6-6 loss: 0.764476  [  224/  306]
train() client id: f_00004-6-7 loss: 0.863591  [  256/  306]
train() client id: f_00004-6-8 loss: 0.687853  [  288/  306]
train() client id: f_00004-7-0 loss: 0.695529  [   32/  306]
train() client id: f_00004-7-1 loss: 0.718484  [   64/  306]
train() client id: f_00004-7-2 loss: 0.731148  [   96/  306]
train() client id: f_00004-7-3 loss: 0.818277  [  128/  306]
train() client id: f_00004-7-4 loss: 0.890900  [  160/  306]
train() client id: f_00004-7-5 loss: 0.745441  [  192/  306]
train() client id: f_00004-7-6 loss: 0.734226  [  224/  306]
train() client id: f_00004-7-7 loss: 0.871486  [  256/  306]
train() client id: f_00004-7-8 loss: 0.690510  [  288/  306]
train() client id: f_00004-8-0 loss: 0.721479  [   32/  306]
train() client id: f_00004-8-1 loss: 0.728434  [   64/  306]
train() client id: f_00004-8-2 loss: 0.799545  [   96/  306]
train() client id: f_00004-8-3 loss: 0.828102  [  128/  306]
train() client id: f_00004-8-4 loss: 0.733992  [  160/  306]
train() client id: f_00004-8-5 loss: 0.855139  [  192/  306]
train() client id: f_00004-8-6 loss: 0.824937  [  224/  306]
train() client id: f_00004-8-7 loss: 0.816219  [  256/  306]
train() client id: f_00004-8-8 loss: 0.716862  [  288/  306]
train() client id: f_00004-9-0 loss: 0.850540  [   32/  306]
train() client id: f_00004-9-1 loss: 0.689758  [   64/  306]
train() client id: f_00004-9-2 loss: 0.792298  [   96/  306]
train() client id: f_00004-9-3 loss: 0.801203  [  128/  306]
train() client id: f_00004-9-4 loss: 0.848501  [  160/  306]
train() client id: f_00004-9-5 loss: 0.788896  [  192/  306]
train() client id: f_00004-9-6 loss: 0.835007  [  224/  306]
train() client id: f_00004-9-7 loss: 0.630669  [  256/  306]
train() client id: f_00004-9-8 loss: 0.857510  [  288/  306]
train() client id: f_00004-10-0 loss: 0.805636  [   32/  306]
train() client id: f_00004-10-1 loss: 0.703790  [   64/  306]
train() client id: f_00004-10-2 loss: 0.682097  [   96/  306]
train() client id: f_00004-10-3 loss: 0.829861  [  128/  306]
train() client id: f_00004-10-4 loss: 0.759016  [  160/  306]
train() client id: f_00004-10-5 loss: 0.769620  [  192/  306]
train() client id: f_00004-10-6 loss: 0.876995  [  224/  306]
train() client id: f_00004-10-7 loss: 0.770306  [  256/  306]
train() client id: f_00004-10-8 loss: 0.821118  [  288/  306]
train() client id: f_00005-0-0 loss: 0.715261  [   32/  146]
train() client id: f_00005-0-1 loss: 0.557514  [   64/  146]
train() client id: f_00005-0-2 loss: 0.588879  [   96/  146]
train() client id: f_00005-0-3 loss: 0.690473  [  128/  146]
train() client id: f_00005-1-0 loss: 0.694214  [   32/  146]
train() client id: f_00005-1-1 loss: 0.530468  [   64/  146]
train() client id: f_00005-1-2 loss: 0.495933  [   96/  146]
train() client id: f_00005-1-3 loss: 0.833236  [  128/  146]
train() client id: f_00005-2-0 loss: 0.743989  [   32/  146]
train() client id: f_00005-2-1 loss: 0.688850  [   64/  146]
train() client id: f_00005-2-2 loss: 0.393247  [   96/  146]
train() client id: f_00005-2-3 loss: 0.793034  [  128/  146]
train() client id: f_00005-3-0 loss: 0.812783  [   32/  146]
train() client id: f_00005-3-1 loss: 0.711949  [   64/  146]
train() client id: f_00005-3-2 loss: 0.479573  [   96/  146]
train() client id: f_00005-3-3 loss: 0.568372  [  128/  146]
train() client id: f_00005-4-0 loss: 0.851824  [   32/  146]
train() client id: f_00005-4-1 loss: 0.724004  [   64/  146]
train() client id: f_00005-4-2 loss: 0.456618  [   96/  146]
train() client id: f_00005-4-3 loss: 0.695550  [  128/  146]
train() client id: f_00005-5-0 loss: 0.478549  [   32/  146]
train() client id: f_00005-5-1 loss: 0.521373  [   64/  146]
train() client id: f_00005-5-2 loss: 0.744097  [   96/  146]
train() client id: f_00005-5-3 loss: 0.829536  [  128/  146]
train() client id: f_00005-6-0 loss: 0.350528  [   32/  146]
train() client id: f_00005-6-1 loss: 0.499345  [   64/  146]
train() client id: f_00005-6-2 loss: 0.626193  [   96/  146]
train() client id: f_00005-6-3 loss: 0.802040  [  128/  146]
train() client id: f_00005-7-0 loss: 0.879219  [   32/  146]
train() client id: f_00005-7-1 loss: 0.546653  [   64/  146]
train() client id: f_00005-7-2 loss: 0.568759  [   96/  146]
train() client id: f_00005-7-3 loss: 0.533598  [  128/  146]
train() client id: f_00005-8-0 loss: 0.817355  [   32/  146]
train() client id: f_00005-8-1 loss: 0.667193  [   64/  146]
train() client id: f_00005-8-2 loss: 0.416520  [   96/  146]
train() client id: f_00005-8-3 loss: 0.504345  [  128/  146]
train() client id: f_00005-9-0 loss: 0.467255  [   32/  146]
train() client id: f_00005-9-1 loss: 0.564846  [   64/  146]
train() client id: f_00005-9-2 loss: 0.802756  [   96/  146]
train() client id: f_00005-9-3 loss: 0.609636  [  128/  146]
train() client id: f_00005-10-0 loss: 0.753768  [   32/  146]
train() client id: f_00005-10-1 loss: 0.408741  [   64/  146]
train() client id: f_00005-10-2 loss: 0.723252  [   96/  146]
train() client id: f_00005-10-3 loss: 0.660452  [  128/  146]
train() client id: f_00006-0-0 loss: 0.611854  [   32/   54]
train() client id: f_00006-1-0 loss: 0.603098  [   32/   54]
train() client id: f_00006-2-0 loss: 0.543727  [   32/   54]
train() client id: f_00006-3-0 loss: 0.510597  [   32/   54]
train() client id: f_00006-4-0 loss: 0.560848  [   32/   54]
train() client id: f_00006-5-0 loss: 0.509198  [   32/   54]
train() client id: f_00006-6-0 loss: 0.610873  [   32/   54]
train() client id: f_00006-7-0 loss: 0.495595  [   32/   54]
train() client id: f_00006-8-0 loss: 0.503624  [   32/   54]
train() client id: f_00006-9-0 loss: 0.533235  [   32/   54]
train() client id: f_00006-10-0 loss: 0.497609  [   32/   54]
train() client id: f_00007-0-0 loss: 0.608086  [   32/  179]
train() client id: f_00007-0-1 loss: 0.602790  [   64/  179]
train() client id: f_00007-0-2 loss: 0.630538  [   96/  179]
train() client id: f_00007-0-3 loss: 0.503399  [  128/  179]
train() client id: f_00007-0-4 loss: 0.698233  [  160/  179]
train() client id: f_00007-1-0 loss: 0.583645  [   32/  179]
train() client id: f_00007-1-1 loss: 0.606937  [   64/  179]
train() client id: f_00007-1-2 loss: 0.719206  [   96/  179]
train() client id: f_00007-1-3 loss: 0.785471  [  128/  179]
train() client id: f_00007-1-4 loss: 0.511270  [  160/  179]
train() client id: f_00007-2-0 loss: 0.592884  [   32/  179]
train() client id: f_00007-2-1 loss: 0.517281  [   64/  179]
train() client id: f_00007-2-2 loss: 0.481743  [   96/  179]
train() client id: f_00007-2-3 loss: 0.766469  [  128/  179]
train() client id: f_00007-2-4 loss: 0.766743  [  160/  179]
train() client id: f_00007-3-0 loss: 0.856543  [   32/  179]
train() client id: f_00007-3-1 loss: 0.563488  [   64/  179]
train() client id: f_00007-3-2 loss: 0.560666  [   96/  179]
train() client id: f_00007-3-3 loss: 0.520928  [  128/  179]
train() client id: f_00007-3-4 loss: 0.547771  [  160/  179]
train() client id: f_00007-4-0 loss: 0.676578  [   32/  179]
train() client id: f_00007-4-1 loss: 0.591281  [   64/  179]
train() client id: f_00007-4-2 loss: 0.742549  [   96/  179]
train() client id: f_00007-4-3 loss: 0.557154  [  128/  179]
train() client id: f_00007-4-4 loss: 0.538871  [  160/  179]
train() client id: f_00007-5-0 loss: 0.586117  [   32/  179]
train() client id: f_00007-5-1 loss: 0.716805  [   64/  179]
train() client id: f_00007-5-2 loss: 0.577320  [   96/  179]
train() client id: f_00007-5-3 loss: 0.599815  [  128/  179]
train() client id: f_00007-5-4 loss: 0.627253  [  160/  179]
train() client id: f_00007-6-0 loss: 0.482122  [   32/  179]
train() client id: f_00007-6-1 loss: 0.556354  [   64/  179]
train() client id: f_00007-6-2 loss: 0.734476  [   96/  179]
train() client id: f_00007-6-3 loss: 0.482642  [  128/  179]
train() client id: f_00007-6-4 loss: 0.658693  [  160/  179]
train() client id: f_00007-7-0 loss: 0.552115  [   32/  179]
train() client id: f_00007-7-1 loss: 0.718984  [   64/  179]
train() client id: f_00007-7-2 loss: 0.544344  [   96/  179]
train() client id: f_00007-7-3 loss: 0.705112  [  128/  179]
train() client id: f_00007-7-4 loss: 0.548240  [  160/  179]
train() client id: f_00007-8-0 loss: 0.728366  [   32/  179]
train() client id: f_00007-8-1 loss: 0.542357  [   64/  179]
train() client id: f_00007-8-2 loss: 0.468144  [   96/  179]
train() client id: f_00007-8-3 loss: 0.788087  [  128/  179]
train() client id: f_00007-8-4 loss: 0.484386  [  160/  179]
train() client id: f_00007-9-0 loss: 0.547047  [   32/  179]
train() client id: f_00007-9-1 loss: 0.543525  [   64/  179]
train() client id: f_00007-9-2 loss: 0.767444  [   96/  179]
train() client id: f_00007-9-3 loss: 0.563275  [  128/  179]
train() client id: f_00007-9-4 loss: 0.457329  [  160/  179]
train() client id: f_00007-10-0 loss: 0.661474  [   32/  179]
train() client id: f_00007-10-1 loss: 0.450132  [   64/  179]
train() client id: f_00007-10-2 loss: 0.552724  [   96/  179]
train() client id: f_00007-10-3 loss: 0.543532  [  128/  179]
train() client id: f_00007-10-4 loss: 0.647023  [  160/  179]
train() client id: f_00008-0-0 loss: 0.814882  [   32/  130]
train() client id: f_00008-0-1 loss: 0.811095  [   64/  130]
train() client id: f_00008-0-2 loss: 1.006851  [   96/  130]
train() client id: f_00008-0-3 loss: 0.882526  [  128/  130]
train() client id: f_00008-1-0 loss: 0.833751  [   32/  130]
train() client id: f_00008-1-1 loss: 0.830233  [   64/  130]
train() client id: f_00008-1-2 loss: 0.922342  [   96/  130]
train() client id: f_00008-1-3 loss: 0.925651  [  128/  130]
train() client id: f_00008-2-0 loss: 0.862505  [   32/  130]
train() client id: f_00008-2-1 loss: 0.891419  [   64/  130]
train() client id: f_00008-2-2 loss: 0.915049  [   96/  130]
train() client id: f_00008-2-3 loss: 0.808770  [  128/  130]
train() client id: f_00008-3-0 loss: 0.866259  [   32/  130]
train() client id: f_00008-3-1 loss: 0.874665  [   64/  130]
train() client id: f_00008-3-2 loss: 0.812478  [   96/  130]
train() client id: f_00008-3-3 loss: 0.951848  [  128/  130]
train() client id: f_00008-4-0 loss: 0.920677  [   32/  130]
train() client id: f_00008-4-1 loss: 0.785595  [   64/  130]
train() client id: f_00008-4-2 loss: 0.919744  [   96/  130]
train() client id: f_00008-4-3 loss: 0.882801  [  128/  130]
train() client id: f_00008-5-0 loss: 0.839585  [   32/  130]
train() client id: f_00008-5-1 loss: 0.848214  [   64/  130]
train() client id: f_00008-5-2 loss: 0.918501  [   96/  130]
train() client id: f_00008-5-3 loss: 0.902732  [  128/  130]
train() client id: f_00008-6-0 loss: 0.860181  [   32/  130]
train() client id: f_00008-6-1 loss: 0.861391  [   64/  130]
train() client id: f_00008-6-2 loss: 0.885159  [   96/  130]
train() client id: f_00008-6-3 loss: 0.886676  [  128/  130]
train() client id: f_00008-7-0 loss: 0.880604  [   32/  130]
train() client id: f_00008-7-1 loss: 0.907020  [   64/  130]
train() client id: f_00008-7-2 loss: 0.884966  [   96/  130]
train() client id: f_00008-7-3 loss: 0.833099  [  128/  130]
train() client id: f_00008-8-0 loss: 0.792514  [   32/  130]
train() client id: f_00008-8-1 loss: 0.856846  [   64/  130]
train() client id: f_00008-8-2 loss: 0.935862  [   96/  130]
train() client id: f_00008-8-3 loss: 0.900252  [  128/  130]
train() client id: f_00008-9-0 loss: 0.831990  [   32/  130]
train() client id: f_00008-9-1 loss: 0.856242  [   64/  130]
train() client id: f_00008-9-2 loss: 0.963831  [   96/  130]
train() client id: f_00008-9-3 loss: 0.845872  [  128/  130]
train() client id: f_00008-10-0 loss: 0.811052  [   32/  130]
train() client id: f_00008-10-1 loss: 0.885507  [   64/  130]
train() client id: f_00008-10-2 loss: 0.945248  [   96/  130]
train() client id: f_00008-10-3 loss: 0.872684  [  128/  130]
train() client id: f_00009-0-0 loss: 1.051774  [   32/  118]
train() client id: f_00009-0-1 loss: 1.069260  [   64/  118]
train() client id: f_00009-0-2 loss: 0.900571  [   96/  118]
train() client id: f_00009-1-0 loss: 0.905617  [   32/  118]
train() client id: f_00009-1-1 loss: 0.955208  [   64/  118]
train() client id: f_00009-1-2 loss: 0.965214  [   96/  118]
train() client id: f_00009-2-0 loss: 0.989069  [   32/  118]
train() client id: f_00009-2-1 loss: 0.896461  [   64/  118]
train() client id: f_00009-2-2 loss: 0.827924  [   96/  118]
train() client id: f_00009-3-0 loss: 0.906485  [   32/  118]
train() client id: f_00009-3-1 loss: 0.831357  [   64/  118]
train() client id: f_00009-3-2 loss: 0.848961  [   96/  118]
train() client id: f_00009-4-0 loss: 0.796237  [   32/  118]
train() client id: f_00009-4-1 loss: 0.961018  [   64/  118]
train() client id: f_00009-4-2 loss: 0.662082  [   96/  118]
train() client id: f_00009-5-0 loss: 0.823319  [   32/  118]
train() client id: f_00009-5-1 loss: 0.754419  [   64/  118]
train() client id: f_00009-5-2 loss: 0.836574  [   96/  118]
train() client id: f_00009-6-0 loss: 0.620602  [   32/  118]
train() client id: f_00009-6-1 loss: 0.843601  [   64/  118]
train() client id: f_00009-6-2 loss: 0.710680  [   96/  118]
train() client id: f_00009-7-0 loss: 0.748283  [   32/  118]
train() client id: f_00009-7-1 loss: 0.775748  [   64/  118]
train() client id: f_00009-7-2 loss: 0.698411  [   96/  118]
train() client id: f_00009-8-0 loss: 0.707845  [   32/  118]
train() client id: f_00009-8-1 loss: 0.675905  [   64/  118]
train() client id: f_00009-8-2 loss: 0.906797  [   96/  118]
train() client id: f_00009-9-0 loss: 0.617141  [   32/  118]
train() client id: f_00009-9-1 loss: 0.776344  [   64/  118]
train() client id: f_00009-9-2 loss: 0.750238  [   96/  118]
train() client id: f_00009-10-0 loss: 0.865565  [   32/  118]
train() client id: f_00009-10-1 loss: 0.580974  [   64/  118]
train() client id: f_00009-10-2 loss: 0.755097  [   96/  118]
At round 14 accuracy: 0.6339522546419099
At round 14 training accuracy: 0.5727699530516432
At round 14 training loss: 0.8508875773148185
update_location
xs = -4.528292 -28.998411 0.045120 19.056472 -120.103519 -55.217951 -22.215960 43.375741 -1.680116 -10.304393 
ys = 57.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 -3.124922 -19.177652 12.845030 -0.998518 
xs mean: -18.05713084058713
ys mean: 7.871751218646876
dists_uav = 115.485404 105.275315 100.008730 104.266093 156.563984 115.517858 102.485677 110.676272 100.835597 100.534460 
uav_gains = -101.563302 -100.558197 -100.000964 -100.453607 -104.875272 -101.566353 -100.266602 -101.101431 -100.090365 -100.057891 
uav_gains_db_mean: -101.05339828071818
dists_bs = 206.993214 216.191824 246.587354 246.780256 174.509267 226.457811 234.673072 292.220678 237.347881 241.039668 
bs_gains = -104.413830 -104.942559 -106.542242 -106.551751 -102.337977 -105.506704 -105.940030 -108.606966 -106.077849 -106.265538 
bs_gains_db_mean: -105.71854444203323
Round 15
-------------------------------
ene_coms = [0.00673936 0.00792204 0.00630267 0.00642333 0.00697877 0.00816
 0.00637293 0.00660417 0.00841546 0.00850281]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 9.00238145 18.78805086  8.85181165  3.16822214 21.64439437 10.43949896
  3.93927593 12.71996422  9.32220601  8.48181002]
obj_prev = 106.35761560012139
eta_min = 4.000244353528857e-11	eta_max = 0.931938150145669
af = 22.501910530386866	bf = 1.6689119418111065	zeta = 24.752101583425553	eta = 0.9090909090909091
af = 22.501910530386866	bf = 1.6689119418111065	zeta = 41.94930388443421	eta = 0.5364072450970151
af = 22.501910530386866	bf = 1.6689119418111065	zeta = 33.832470933571265	eta = 0.6650980525356391
af = 22.501910530386866	bf = 1.6689119418111065	zeta = 32.384580674631096	eta = 0.6948340865198864
af = 22.501910530386866	bf = 1.6689119418111065	zeta = 32.31506629383931	eta = 0.6963287751223562
af = 22.501910530386866	bf = 1.6689119418111065	zeta = 32.31489533933126	eta = 0.6963324588892985
eta = 0.6963324588892985
ene_coms = [0.00673936 0.00792204 0.00630267 0.00642333 0.00697877 0.00816
 0.00637293 0.00660417 0.00841546 0.00850281]
ene_comp = [0.02986544 0.06281223 0.02939138 0.01019217 0.07253034 0.03460598
 0.01279947 0.04242788 0.03081355 0.02796922]
ene_total = [2.77782772 5.36781062 2.70871409 1.26090026 6.03370718 3.24538186
 1.45493589 3.72089475 2.97697069 2.76775228]
ti_comp = [0.31804714 0.30622032 0.32241404 0.32120747 0.31565305 0.3038407
 0.32171144 0.31939904 0.30128619 0.30041265]
ti_coms = [0.0673936  0.07922042 0.0630267  0.06423327 0.06978769 0.08160004
 0.0637293  0.0660417  0.08415455 0.08502809]
t_total = [29.24993706 29.24993706 29.24993706 29.24993706 29.24993706 29.24993706
 29.24993706 29.24993706 29.24993706 29.24993706]
ene_coms = [0.00673936 0.00792204 0.00630267 0.00642333 0.00697877 0.00816
 0.00637293 0.00660417 0.00841546 0.00850281]
ene_comp = [1.64590059e-05 1.65175096e-04 1.52655386e-05 6.41370629e-07
 2.39342186e-04 2.80570261e-05 1.26626100e-06 4.67914877e-05
 2.01440398e-05 1.51525089e-05]
ene_total = [0.51267874 0.61371453 0.47944911 0.48749564 0.5477607  0.6213673
 0.48371855 0.50472146 0.64015221 0.64640244]
optimize_network iter = 0 obj = 5.537460678930613
eta = 0.6963324588892985
freqs = [4.69512723e+07 1.02560513e+08 4.55801826e+07 1.58654043e+07
 1.14889344e+08 5.69475740e+07 1.98927843e+07 6.64182989e+07
 5.11366703e+07 4.65513315e+07]
eta_min = 0.6963324588893399	eta_max = 0.6963324588892498
af = 0.03491060209971311	bf = 1.6689119418111065	zeta = 0.03840166230968443	eta = 0.909090909090909
af = 0.03491060209971311	bf = 1.6689119418111065	zeta = 18.394632053317352	eta = 0.001897869008660992
af = 0.03491060209971311	bf = 1.6689119418111065	zeta = 1.891286923781838	eta = 0.01845864932535223
af = 0.03491060209971311	bf = 1.6689119418111065	zeta = 1.8422889896509402	eta = 0.01894957973250855
af = 0.03491060209971311	bf = 1.6689119418111065	zeta = 1.8422768373124767	eta = 0.01894970473093549
eta = 0.01894970473093549
ene_coms = [0.00673936 0.00792204 0.00630267 0.00642333 0.00697877 0.00816
 0.00637293 0.00660417 0.00841546 0.00850281]
ene_comp = [1.80355933e-04 1.80997009e-03 1.67278053e-04 7.02806709e-06
 2.62268471e-03 3.07445732e-04 1.38755454e-05 5.12735851e-04
 2.20736118e-04 1.66039486e-04]
ene_total = [0.1625409  0.22860043 0.15197607 0.15104606 0.22553367 0.19889646
 0.15002309 0.16717282 0.20286011 0.20362722]
ti_comp = [0.31804714 0.30622032 0.32241404 0.32120747 0.31565305 0.3038407
 0.32171144 0.31939904 0.30128619 0.30041265]
ti_coms = [0.0673936  0.07922042 0.0630267  0.06423327 0.06978769 0.08160004
 0.0637293  0.0660417  0.08415455 0.08502809]
t_total = [29.24993706 29.24993706 29.24993706 29.24993706 29.24993706 29.24993706
 29.24993706 29.24993706 29.24993706 29.24993706]
ene_coms = [0.00673936 0.00792204 0.00630267 0.00642333 0.00697877 0.00816
 0.00637293 0.00660417 0.00841546 0.00850281]
ene_comp = [1.64590059e-05 1.65175096e-04 1.52655386e-05 6.41370629e-07
 2.39342186e-04 2.80570261e-05 1.26626100e-06 4.67914877e-05
 2.01440398e-05 1.51525089e-05]
ene_total = [0.51267874 0.61371453 0.47944911 0.48749564 0.5477607  0.6213673
 0.48371855 0.50472146 0.64015221 0.64640244]
optimize_network iter = 1 obj = 5.537460678931362
eta = 0.6963324588893399
freqs = [4.69512723e+07 1.02560513e+08 4.55801826e+07 1.58654043e+07
 1.14889344e+08 5.69475740e+07 1.98927843e+07 6.64182989e+07
 5.11366703e+07 4.65513315e+07]
Done!
ene_coms = [0.00673936 0.00792204 0.00630267 0.00642333 0.00697877 0.00816
 0.00637293 0.00660417 0.00841546 0.00850281]
ene_comp = [1.52766443e-05 1.53309452e-04 1.41689118e-05 5.95296641e-07
 2.22148619e-04 2.60415003e-05 1.17529692e-06 4.34301388e-05
 1.86969572e-05 1.40640018e-05]
ene_total = [0.00675464 0.00807535 0.00631684 0.00642392 0.00720092 0.00818605
 0.00637411 0.0066476  0.00843415 0.00851687]
At round 15 energy consumption: 0.0729304428317301
At round 15 eta: 0.6963324588893399
At round 15 a_n: 23.044415151714617
At round 15 local rounds: 11.851363560959117
At round 15 global rounds: 75.88698834070303
gradient difference: 0.6018492579460144
train() client id: f_00000-0-0 loss: 1.005591  [   32/  126]
train() client id: f_00000-0-1 loss: 1.098619  [   64/  126]
train() client id: f_00000-0-2 loss: 0.838773  [   96/  126]
train() client id: f_00000-1-0 loss: 0.882683  [   32/  126]
train() client id: f_00000-1-1 loss: 0.929931  [   64/  126]
train() client id: f_00000-1-2 loss: 1.023480  [   96/  126]
train() client id: f_00000-2-0 loss: 0.915651  [   32/  126]
train() client id: f_00000-2-1 loss: 0.856026  [   64/  126]
train() client id: f_00000-2-2 loss: 0.929867  [   96/  126]
train() client id: f_00000-3-0 loss: 0.841368  [   32/  126]
train() client id: f_00000-3-1 loss: 0.963206  [   64/  126]
train() client id: f_00000-3-2 loss: 0.831121  [   96/  126]
train() client id: f_00000-4-0 loss: 0.808739  [   32/  126]
train() client id: f_00000-4-1 loss: 0.866672  [   64/  126]
train() client id: f_00000-4-2 loss: 0.862928  [   96/  126]
train() client id: f_00000-5-0 loss: 0.810948  [   32/  126]
train() client id: f_00000-5-1 loss: 0.927513  [   64/  126]
train() client id: f_00000-5-2 loss: 0.836013  [   96/  126]
train() client id: f_00000-6-0 loss: 0.902838  [   32/  126]
train() client id: f_00000-6-1 loss: 0.863838  [   64/  126]
train() client id: f_00000-6-2 loss: 0.887485  [   96/  126]
train() client id: f_00000-7-0 loss: 0.910401  [   32/  126]
train() client id: f_00000-7-1 loss: 0.927242  [   64/  126]
train() client id: f_00000-7-2 loss: 0.833035  [   96/  126]
train() client id: f_00000-8-0 loss: 0.846562  [   32/  126]
train() client id: f_00000-8-1 loss: 0.939558  [   64/  126]
train() client id: f_00000-8-2 loss: 0.899490  [   96/  126]
train() client id: f_00000-9-0 loss: 0.944108  [   32/  126]
train() client id: f_00000-9-1 loss: 0.920894  [   64/  126]
train() client id: f_00000-9-2 loss: 0.907727  [   96/  126]
train() client id: f_00000-10-0 loss: 0.967330  [   32/  126]
train() client id: f_00000-10-1 loss: 0.880869  [   64/  126]
train() client id: f_00000-10-2 loss: 0.770423  [   96/  126]
train() client id: f_00001-0-0 loss: 0.671682  [   32/  265]
train() client id: f_00001-0-1 loss: 0.555542  [   64/  265]
train() client id: f_00001-0-2 loss: 0.582121  [   96/  265]
train() client id: f_00001-0-3 loss: 0.542391  [  128/  265]
train() client id: f_00001-0-4 loss: 0.532082  [  160/  265]
train() client id: f_00001-0-5 loss: 0.520063  [  192/  265]
train() client id: f_00001-0-6 loss: 0.524885  [  224/  265]
train() client id: f_00001-0-7 loss: 0.584540  [  256/  265]
train() client id: f_00001-1-0 loss: 0.552247  [   32/  265]
train() client id: f_00001-1-1 loss: 0.594459  [   64/  265]
train() client id: f_00001-1-2 loss: 0.511744  [   96/  265]
train() client id: f_00001-1-3 loss: 0.683567  [  128/  265]
train() client id: f_00001-1-4 loss: 0.471051  [  160/  265]
train() client id: f_00001-1-5 loss: 0.529356  [  192/  265]
train() client id: f_00001-1-6 loss: 0.488687  [  224/  265]
train() client id: f_00001-1-7 loss: 0.548523  [  256/  265]
train() client id: f_00001-2-0 loss: 0.524379  [   32/  265]
train() client id: f_00001-2-1 loss: 0.578948  [   64/  265]
train() client id: f_00001-2-2 loss: 0.474112  [   96/  265]
train() client id: f_00001-2-3 loss: 0.564171  [  128/  265]
train() client id: f_00001-2-4 loss: 0.588612  [  160/  265]
train() client id: f_00001-2-5 loss: 0.499696  [  192/  265]
train() client id: f_00001-2-6 loss: 0.601911  [  224/  265]
train() client id: f_00001-2-7 loss: 0.585587  [  256/  265]
train() client id: f_00001-3-0 loss: 0.570502  [   32/  265]
train() client id: f_00001-3-1 loss: 0.453459  [   64/  265]
train() client id: f_00001-3-2 loss: 0.519134  [   96/  265]
train() client id: f_00001-3-3 loss: 0.478283  [  128/  265]
train() client id: f_00001-3-4 loss: 0.693565  [  160/  265]
train() client id: f_00001-3-5 loss: 0.651865  [  192/  265]
train() client id: f_00001-3-6 loss: 0.514422  [  224/  265]
train() client id: f_00001-3-7 loss: 0.490972  [  256/  265]
train() client id: f_00001-4-0 loss: 0.588774  [   32/  265]
train() client id: f_00001-4-1 loss: 0.457373  [   64/  265]
train() client id: f_00001-4-2 loss: 0.518891  [   96/  265]
train() client id: f_00001-4-3 loss: 0.646825  [  128/  265]
train() client id: f_00001-4-4 loss: 0.575055  [  160/  265]
train() client id: f_00001-4-5 loss: 0.460698  [  192/  265]
train() client id: f_00001-4-6 loss: 0.506782  [  224/  265]
train() client id: f_00001-4-7 loss: 0.545797  [  256/  265]
train() client id: f_00001-5-0 loss: 0.622366  [   32/  265]
train() client id: f_00001-5-1 loss: 0.586982  [   64/  265]
train() client id: f_00001-5-2 loss: 0.453847  [   96/  265]
train() client id: f_00001-5-3 loss: 0.456668  [  128/  265]
train() client id: f_00001-5-4 loss: 0.706469  [  160/  265]
train() client id: f_00001-5-5 loss: 0.516869  [  192/  265]
train() client id: f_00001-5-6 loss: 0.450847  [  224/  265]
train() client id: f_00001-5-7 loss: 0.503713  [  256/  265]
train() client id: f_00001-6-0 loss: 0.523498  [   32/  265]
train() client id: f_00001-6-1 loss: 0.548400  [   64/  265]
train() client id: f_00001-6-2 loss: 0.545548  [   96/  265]
train() client id: f_00001-6-3 loss: 0.591926  [  128/  265]
train() client id: f_00001-6-4 loss: 0.546717  [  160/  265]
train() client id: f_00001-6-5 loss: 0.490853  [  192/  265]
train() client id: f_00001-6-6 loss: 0.461673  [  224/  265]
train() client id: f_00001-6-7 loss: 0.550290  [  256/  265]
train() client id: f_00001-7-0 loss: 0.572647  [   32/  265]
train() client id: f_00001-7-1 loss: 0.485846  [   64/  265]
train() client id: f_00001-7-2 loss: 0.485389  [   96/  265]
train() client id: f_00001-7-3 loss: 0.639605  [  128/  265]
train() client id: f_00001-7-4 loss: 0.548538  [  160/  265]
train() client id: f_00001-7-5 loss: 0.459768  [  192/  265]
train() client id: f_00001-7-6 loss: 0.639132  [  224/  265]
train() client id: f_00001-7-7 loss: 0.488203  [  256/  265]
train() client id: f_00001-8-0 loss: 0.580529  [   32/  265]
train() client id: f_00001-8-1 loss: 0.666358  [   64/  265]
train() client id: f_00001-8-2 loss: 0.507561  [   96/  265]
train() client id: f_00001-8-3 loss: 0.517869  [  128/  265]
train() client id: f_00001-8-4 loss: 0.550108  [  160/  265]
train() client id: f_00001-8-5 loss: 0.512267  [  192/  265]
train() client id: f_00001-8-6 loss: 0.546158  [  224/  265]
train() client id: f_00001-8-7 loss: 0.458611  [  256/  265]
train() client id: f_00001-9-0 loss: 0.467464  [   32/  265]
train() client id: f_00001-9-1 loss: 0.519190  [   64/  265]
train() client id: f_00001-9-2 loss: 0.504370  [   96/  265]
train() client id: f_00001-9-3 loss: 0.577708  [  128/  265]
train() client id: f_00001-9-4 loss: 0.618073  [  160/  265]
train() client id: f_00001-9-5 loss: 0.578544  [  192/  265]
train() client id: f_00001-9-6 loss: 0.518637  [  224/  265]
train() client id: f_00001-9-7 loss: 0.565957  [  256/  265]
train() client id: f_00001-10-0 loss: 0.592862  [   32/  265]
train() client id: f_00001-10-1 loss: 0.474610  [   64/  265]
train() client id: f_00001-10-2 loss: 0.545601  [   96/  265]
train() client id: f_00001-10-3 loss: 0.470749  [  128/  265]
train() client id: f_00001-10-4 loss: 0.690915  [  160/  265]
train() client id: f_00001-10-5 loss: 0.587215  [  192/  265]
train() client id: f_00001-10-6 loss: 0.543958  [  224/  265]
train() client id: f_00001-10-7 loss: 0.444250  [  256/  265]
train() client id: f_00002-0-0 loss: 1.190172  [   32/  124]
train() client id: f_00002-0-1 loss: 1.070891  [   64/  124]
train() client id: f_00002-0-2 loss: 1.170692  [   96/  124]
train() client id: f_00002-1-0 loss: 1.081941  [   32/  124]
train() client id: f_00002-1-1 loss: 0.985003  [   64/  124]
train() client id: f_00002-1-2 loss: 1.079279  [   96/  124]
train() client id: f_00002-2-0 loss: 1.036165  [   32/  124]
train() client id: f_00002-2-1 loss: 1.037529  [   64/  124]
train() client id: f_00002-2-2 loss: 1.043993  [   96/  124]
train() client id: f_00002-3-0 loss: 0.957495  [   32/  124]
train() client id: f_00002-3-1 loss: 1.075853  [   64/  124]
train() client id: f_00002-3-2 loss: 0.928514  [   96/  124]
train() client id: f_00002-4-0 loss: 1.143488  [   32/  124]
train() client id: f_00002-4-1 loss: 0.972049  [   64/  124]
train() client id: f_00002-4-2 loss: 0.926306  [   96/  124]
train() client id: f_00002-5-0 loss: 1.026210  [   32/  124]
train() client id: f_00002-5-1 loss: 1.089187  [   64/  124]
train() client id: f_00002-5-2 loss: 0.903244  [   96/  124]
train() client id: f_00002-6-0 loss: 0.912335  [   32/  124]
train() client id: f_00002-6-1 loss: 0.991633  [   64/  124]
train() client id: f_00002-6-2 loss: 1.055119  [   96/  124]
train() client id: f_00002-7-0 loss: 0.988001  [   32/  124]
train() client id: f_00002-7-1 loss: 0.980612  [   64/  124]
train() client id: f_00002-7-2 loss: 0.994589  [   96/  124]
train() client id: f_00002-8-0 loss: 0.988672  [   32/  124]
train() client id: f_00002-8-1 loss: 0.932225  [   64/  124]
train() client id: f_00002-8-2 loss: 1.088655  [   96/  124]
train() client id: f_00002-9-0 loss: 1.008909  [   32/  124]
train() client id: f_00002-9-1 loss: 0.966008  [   64/  124]
train() client id: f_00002-9-2 loss: 1.062063  [   96/  124]
train() client id: f_00002-10-0 loss: 0.945700  [   32/  124]
train() client id: f_00002-10-1 loss: 1.037652  [   64/  124]
train() client id: f_00002-10-2 loss: 0.982711  [   96/  124]
train() client id: f_00003-0-0 loss: 0.691264  [   32/   43]
train() client id: f_00003-1-0 loss: 0.831216  [   32/   43]
train() client id: f_00003-2-0 loss: 0.697317  [   32/   43]
train() client id: f_00003-3-0 loss: 0.711157  [   32/   43]
train() client id: f_00003-4-0 loss: 0.818074  [   32/   43]
train() client id: f_00003-5-0 loss: 0.718282  [   32/   43]
train() client id: f_00003-6-0 loss: 0.706115  [   32/   43]
train() client id: f_00003-7-0 loss: 0.787069  [   32/   43]
train() client id: f_00003-8-0 loss: 0.695320  [   32/   43]
train() client id: f_00003-9-0 loss: 0.583343  [   32/   43]
train() client id: f_00003-10-0 loss: 0.678531  [   32/   43]
train() client id: f_00004-0-0 loss: 0.879284  [   32/  306]
train() client id: f_00004-0-1 loss: 0.953663  [   64/  306]
train() client id: f_00004-0-2 loss: 0.822203  [   96/  306]
train() client id: f_00004-0-3 loss: 0.760804  [  128/  306]
train() client id: f_00004-0-4 loss: 0.817497  [  160/  306]
train() client id: f_00004-0-5 loss: 0.857458  [  192/  306]
train() client id: f_00004-0-6 loss: 0.931660  [  224/  306]
train() client id: f_00004-0-7 loss: 0.984896  [  256/  306]
train() client id: f_00004-0-8 loss: 0.743609  [  288/  306]
train() client id: f_00004-1-0 loss: 0.928990  [   32/  306]
train() client id: f_00004-1-1 loss: 0.817834  [   64/  306]
train() client id: f_00004-1-2 loss: 0.949227  [   96/  306]
train() client id: f_00004-1-3 loss: 0.747988  [  128/  306]
train() client id: f_00004-1-4 loss: 0.783132  [  160/  306]
train() client id: f_00004-1-5 loss: 0.839685  [  192/  306]
train() client id: f_00004-1-6 loss: 0.916518  [  224/  306]
train() client id: f_00004-1-7 loss: 0.917806  [  256/  306]
train() client id: f_00004-1-8 loss: 0.754360  [  288/  306]
train() client id: f_00004-2-0 loss: 0.999115  [   32/  306]
train() client id: f_00004-2-1 loss: 0.814663  [   64/  306]
train() client id: f_00004-2-2 loss: 0.731142  [   96/  306]
train() client id: f_00004-2-3 loss: 0.810590  [  128/  306]
train() client id: f_00004-2-4 loss: 0.905181  [  160/  306]
train() client id: f_00004-2-5 loss: 0.875111  [  192/  306]
train() client id: f_00004-2-6 loss: 0.908013  [  224/  306]
train() client id: f_00004-2-7 loss: 0.752117  [  256/  306]
train() client id: f_00004-2-8 loss: 0.940693  [  288/  306]
train() client id: f_00004-3-0 loss: 0.846438  [   32/  306]
train() client id: f_00004-3-1 loss: 0.857838  [   64/  306]
train() client id: f_00004-3-2 loss: 0.860036  [   96/  306]
train() client id: f_00004-3-3 loss: 0.921849  [  128/  306]
train() client id: f_00004-3-4 loss: 0.825192  [  160/  306]
train() client id: f_00004-3-5 loss: 0.777076  [  192/  306]
train() client id: f_00004-3-6 loss: 0.855905  [  224/  306]
train() client id: f_00004-3-7 loss: 0.755224  [  256/  306]
train() client id: f_00004-3-8 loss: 0.963696  [  288/  306]
train() client id: f_00004-4-0 loss: 0.821550  [   32/  306]
train() client id: f_00004-4-1 loss: 0.893426  [   64/  306]
train() client id: f_00004-4-2 loss: 0.961913  [   96/  306]
train() client id: f_00004-4-3 loss: 0.823306  [  128/  306]
train() client id: f_00004-4-4 loss: 0.914770  [  160/  306]
train() client id: f_00004-4-5 loss: 0.715466  [  192/  306]
train() client id: f_00004-4-6 loss: 0.896194  [  224/  306]
train() client id: f_00004-4-7 loss: 0.786017  [  256/  306]
train() client id: f_00004-4-8 loss: 0.777751  [  288/  306]
train() client id: f_00004-5-0 loss: 0.922724  [   32/  306]
train() client id: f_00004-5-1 loss: 0.718006  [   64/  306]
train() client id: f_00004-5-2 loss: 0.853163  [   96/  306]
train() client id: f_00004-5-3 loss: 0.866968  [  128/  306]
train() client id: f_00004-5-4 loss: 0.883579  [  160/  306]
train() client id: f_00004-5-5 loss: 0.835952  [  192/  306]
train() client id: f_00004-5-6 loss: 0.846747  [  224/  306]
train() client id: f_00004-5-7 loss: 0.838297  [  256/  306]
train() client id: f_00004-5-8 loss: 0.863459  [  288/  306]
train() client id: f_00004-6-0 loss: 0.901125  [   32/  306]
train() client id: f_00004-6-1 loss: 0.886122  [   64/  306]
train() client id: f_00004-6-2 loss: 0.880529  [   96/  306]
train() client id: f_00004-6-3 loss: 1.006138  [  128/  306]
train() client id: f_00004-6-4 loss: 0.816685  [  160/  306]
train() client id: f_00004-6-5 loss: 0.862921  [  192/  306]
train() client id: f_00004-6-6 loss: 0.761390  [  224/  306]
train() client id: f_00004-6-7 loss: 0.797373  [  256/  306]
train() client id: f_00004-6-8 loss: 0.701683  [  288/  306]
train() client id: f_00004-7-0 loss: 0.847470  [   32/  306]
train() client id: f_00004-7-1 loss: 0.979923  [   64/  306]
train() client id: f_00004-7-2 loss: 0.797139  [   96/  306]
train() client id: f_00004-7-3 loss: 0.755117  [  128/  306]
train() client id: f_00004-7-4 loss: 0.994131  [  160/  306]
train() client id: f_00004-7-5 loss: 0.853257  [  192/  306]
train() client id: f_00004-7-6 loss: 0.830586  [  224/  306]
train() client id: f_00004-7-7 loss: 0.833509  [  256/  306]
train() client id: f_00004-7-8 loss: 0.759939  [  288/  306]
train() client id: f_00004-8-0 loss: 0.866716  [   32/  306]
train() client id: f_00004-8-1 loss: 0.801254  [   64/  306]
train() client id: f_00004-8-2 loss: 0.848135  [   96/  306]
train() client id: f_00004-8-3 loss: 0.906498  [  128/  306]
train() client id: f_00004-8-4 loss: 0.830213  [  160/  306]
train() client id: f_00004-8-5 loss: 0.795200  [  192/  306]
train() client id: f_00004-8-6 loss: 0.733188  [  224/  306]
train() client id: f_00004-8-7 loss: 0.751794  [  256/  306]
train() client id: f_00004-8-8 loss: 1.006662  [  288/  306]
train() client id: f_00004-9-0 loss: 0.837499  [   32/  306]
train() client id: f_00004-9-1 loss: 0.890865  [   64/  306]
train() client id: f_00004-9-2 loss: 0.896273  [   96/  306]
train() client id: f_00004-9-3 loss: 0.889693  [  128/  306]
train() client id: f_00004-9-4 loss: 0.841705  [  160/  306]
train() client id: f_00004-9-5 loss: 0.813220  [  192/  306]
train() client id: f_00004-9-6 loss: 0.893110  [  224/  306]
train() client id: f_00004-9-7 loss: 0.751588  [  256/  306]
train() client id: f_00004-9-8 loss: 0.829077  [  288/  306]
train() client id: f_00004-10-0 loss: 0.834992  [   32/  306]
train() client id: f_00004-10-1 loss: 0.999010  [   64/  306]
train() client id: f_00004-10-2 loss: 0.832607  [   96/  306]
train() client id: f_00004-10-3 loss: 0.851491  [  128/  306]
train() client id: f_00004-10-4 loss: 0.878888  [  160/  306]
train() client id: f_00004-10-5 loss: 0.866669  [  192/  306]
train() client id: f_00004-10-6 loss: 0.902880  [  224/  306]
train() client id: f_00004-10-7 loss: 0.722164  [  256/  306]
train() client id: f_00004-10-8 loss: 0.792343  [  288/  306]
train() client id: f_00005-0-0 loss: 0.597829  [   32/  146]
train() client id: f_00005-0-1 loss: 0.778124  [   64/  146]
train() client id: f_00005-0-2 loss: 0.577981  [   96/  146]
train() client id: f_00005-0-3 loss: 0.609814  [  128/  146]
train() client id: f_00005-1-0 loss: 0.769599  [   32/  146]
train() client id: f_00005-1-1 loss: 0.811971  [   64/  146]
train() client id: f_00005-1-2 loss: 0.534890  [   96/  146]
train() client id: f_00005-1-3 loss: 0.500850  [  128/  146]
train() client id: f_00005-2-0 loss: 0.666733  [   32/  146]
train() client id: f_00005-2-1 loss: 0.546427  [   64/  146]
train() client id: f_00005-2-2 loss: 0.662866  [   96/  146]
train() client id: f_00005-2-3 loss: 0.599885  [  128/  146]
train() client id: f_00005-3-0 loss: 0.500083  [   32/  146]
train() client id: f_00005-3-1 loss: 0.912119  [   64/  146]
train() client id: f_00005-3-2 loss: 0.440528  [   96/  146]
train() client id: f_00005-3-3 loss: 0.661259  [  128/  146]
train() client id: f_00005-4-0 loss: 0.713322  [   32/  146]
train() client id: f_00005-4-1 loss: 0.615047  [   64/  146]
train() client id: f_00005-4-2 loss: 0.548475  [   96/  146]
train() client id: f_00005-4-3 loss: 0.679541  [  128/  146]
train() client id: f_00005-5-0 loss: 0.603325  [   32/  146]
train() client id: f_00005-5-1 loss: 0.498185  [   64/  146]
train() client id: f_00005-5-2 loss: 0.720961  [   96/  146]
train() client id: f_00005-5-3 loss: 0.676477  [  128/  146]
train() client id: f_00005-6-0 loss: 0.618111  [   32/  146]
train() client id: f_00005-6-1 loss: 0.662004  [   64/  146]
train() client id: f_00005-6-2 loss: 0.471640  [   96/  146]
train() client id: f_00005-6-3 loss: 0.603512  [  128/  146]
train() client id: f_00005-7-0 loss: 0.651439  [   32/  146]
train() client id: f_00005-7-1 loss: 0.781558  [   64/  146]
train() client id: f_00005-7-2 loss: 0.527097  [   96/  146]
train() client id: f_00005-7-3 loss: 0.689871  [  128/  146]
train() client id: f_00005-8-0 loss: 0.743346  [   32/  146]
train() client id: f_00005-8-1 loss: 0.658256  [   64/  146]
train() client id: f_00005-8-2 loss: 0.620389  [   96/  146]
train() client id: f_00005-8-3 loss: 0.594009  [  128/  146]
train() client id: f_00005-9-0 loss: 0.524000  [   32/  146]
train() client id: f_00005-9-1 loss: 0.531070  [   64/  146]
train() client id: f_00005-9-2 loss: 0.574499  [   96/  146]
train() client id: f_00005-9-3 loss: 0.800671  [  128/  146]
train() client id: f_00005-10-0 loss: 0.661497  [   32/  146]
train() client id: f_00005-10-1 loss: 0.719844  [   64/  146]
train() client id: f_00005-10-2 loss: 0.573668  [   96/  146]
train() client id: f_00005-10-3 loss: 0.583483  [  128/  146]
train() client id: f_00006-0-0 loss: 0.577856  [   32/   54]
train() client id: f_00006-1-0 loss: 0.609545  [   32/   54]
train() client id: f_00006-2-0 loss: 0.629903  [   32/   54]
train() client id: f_00006-3-0 loss: 0.675637  [   32/   54]
train() client id: f_00006-4-0 loss: 0.605996  [   32/   54]
train() client id: f_00006-5-0 loss: 0.620940  [   32/   54]
train() client id: f_00006-6-0 loss: 0.637188  [   32/   54]
train() client id: f_00006-7-0 loss: 0.638499  [   32/   54]
train() client id: f_00006-8-0 loss: 0.626045  [   32/   54]
train() client id: f_00006-9-0 loss: 0.666258  [   32/   54]
train() client id: f_00006-10-0 loss: 0.610402  [   32/   54]
train() client id: f_00007-0-0 loss: 0.571797  [   32/  179]
train() client id: f_00007-0-1 loss: 0.707123  [   64/  179]
train() client id: f_00007-0-2 loss: 0.675654  [   96/  179]
train() client id: f_00007-0-3 loss: 0.530005  [  128/  179]
train() client id: f_00007-0-4 loss: 0.534886  [  160/  179]
train() client id: f_00007-1-0 loss: 0.604428  [   32/  179]
train() client id: f_00007-1-1 loss: 0.586868  [   64/  179]
train() client id: f_00007-1-2 loss: 0.526367  [   96/  179]
train() client id: f_00007-1-3 loss: 0.689038  [  128/  179]
train() client id: f_00007-1-4 loss: 0.425716  [  160/  179]
train() client id: f_00007-2-0 loss: 0.707709  [   32/  179]
train() client id: f_00007-2-1 loss: 0.501969  [   64/  179]
train() client id: f_00007-2-2 loss: 0.644899  [   96/  179]
train() client id: f_00007-2-3 loss: 0.517746  [  128/  179]
train() client id: f_00007-2-4 loss: 0.422147  [  160/  179]
train() client id: f_00007-3-0 loss: 0.683498  [   32/  179]
train() client id: f_00007-3-1 loss: 0.438885  [   64/  179]
train() client id: f_00007-3-2 loss: 0.484886  [   96/  179]
train() client id: f_00007-3-3 loss: 0.549606  [  128/  179]
train() client id: f_00007-3-4 loss: 0.659071  [  160/  179]
train() client id: f_00007-4-0 loss: 0.477992  [   32/  179]
train() client id: f_00007-4-1 loss: 0.413706  [   64/  179]
train() client id: f_00007-4-2 loss: 0.508464  [   96/  179]
train() client id: f_00007-4-3 loss: 0.671520  [  128/  179]
train() client id: f_00007-4-4 loss: 0.583190  [  160/  179]
train() client id: f_00007-5-0 loss: 0.549954  [   32/  179]
train() client id: f_00007-5-1 loss: 0.594374  [   64/  179]
train() client id: f_00007-5-2 loss: 0.554141  [   96/  179]
train() client id: f_00007-5-3 loss: 0.550526  [  128/  179]
train() client id: f_00007-5-4 loss: 0.492662  [  160/  179]
train() client id: f_00007-6-0 loss: 0.492156  [   32/  179]
train() client id: f_00007-6-1 loss: 0.546603  [   64/  179]
train() client id: f_00007-6-2 loss: 0.581657  [   96/  179]
train() client id: f_00007-6-3 loss: 0.494031  [  128/  179]
train() client id: f_00007-6-4 loss: 0.536018  [  160/  179]
train() client id: f_00007-7-0 loss: 0.486285  [   32/  179]
train() client id: f_00007-7-1 loss: 0.568226  [   64/  179]
train() client id: f_00007-7-2 loss: 0.493683  [   96/  179]
train() client id: f_00007-7-3 loss: 0.493209  [  128/  179]
train() client id: f_00007-7-4 loss: 0.631125  [  160/  179]
train() client id: f_00007-8-0 loss: 0.389778  [   32/  179]
train() client id: f_00007-8-1 loss: 0.375650  [   64/  179]
train() client id: f_00007-8-2 loss: 0.493120  [   96/  179]
train() client id: f_00007-8-3 loss: 0.567194  [  128/  179]
train() client id: f_00007-8-4 loss: 0.716122  [  160/  179]
train() client id: f_00007-9-0 loss: 0.575118  [   32/  179]
train() client id: f_00007-9-1 loss: 0.593908  [   64/  179]
train() client id: f_00007-9-2 loss: 0.388501  [   96/  179]
train() client id: f_00007-9-3 loss: 0.376670  [  128/  179]
train() client id: f_00007-9-4 loss: 0.595003  [  160/  179]
train() client id: f_00007-10-0 loss: 0.397785  [   32/  179]
train() client id: f_00007-10-1 loss: 0.493150  [   64/  179]
train() client id: f_00007-10-2 loss: 0.689561  [   96/  179]
train() client id: f_00007-10-3 loss: 0.475144  [  128/  179]
train() client id: f_00007-10-4 loss: 0.476881  [  160/  179]
train() client id: f_00008-0-0 loss: 0.805264  [   32/  130]
train() client id: f_00008-0-1 loss: 0.781616  [   64/  130]
train() client id: f_00008-0-2 loss: 0.886371  [   96/  130]
train() client id: f_00008-0-3 loss: 0.709266  [  128/  130]
train() client id: f_00008-1-0 loss: 0.831963  [   32/  130]
train() client id: f_00008-1-1 loss: 0.724558  [   64/  130]
train() client id: f_00008-1-2 loss: 0.812643  [   96/  130]
train() client id: f_00008-1-3 loss: 0.820163  [  128/  130]
train() client id: f_00008-2-0 loss: 0.675226  [   32/  130]
train() client id: f_00008-2-1 loss: 0.768207  [   64/  130]
train() client id: f_00008-2-2 loss: 0.907917  [   96/  130]
train() client id: f_00008-2-3 loss: 0.795486  [  128/  130]
train() client id: f_00008-3-0 loss: 0.777444  [   32/  130]
train() client id: f_00008-3-1 loss: 0.763178  [   64/  130]
train() client id: f_00008-3-2 loss: 0.771348  [   96/  130]
train() client id: f_00008-3-3 loss: 0.874398  [  128/  130]
train() client id: f_00008-4-0 loss: 0.897993  [   32/  130]
train() client id: f_00008-4-1 loss: 0.738558  [   64/  130]
train() client id: f_00008-4-2 loss: 0.722816  [   96/  130]
train() client id: f_00008-4-3 loss: 0.775742  [  128/  130]
train() client id: f_00008-5-0 loss: 0.744692  [   32/  130]
train() client id: f_00008-5-1 loss: 0.745827  [   64/  130]
train() client id: f_00008-5-2 loss: 0.717751  [   96/  130]
train() client id: f_00008-5-3 loss: 0.955733  [  128/  130]
train() client id: f_00008-6-0 loss: 0.774204  [   32/  130]
train() client id: f_00008-6-1 loss: 0.780282  [   64/  130]
train() client id: f_00008-6-2 loss: 0.803073  [   96/  130]
train() client id: f_00008-6-3 loss: 0.828570  [  128/  130]
train() client id: f_00008-7-0 loss: 0.855448  [   32/  130]
train() client id: f_00008-7-1 loss: 0.703851  [   64/  130]
train() client id: f_00008-7-2 loss: 0.764351  [   96/  130]
train() client id: f_00008-7-3 loss: 0.861327  [  128/  130]
train() client id: f_00008-8-0 loss: 0.690026  [   32/  130]
train() client id: f_00008-8-1 loss: 0.782778  [   64/  130]
train() client id: f_00008-8-2 loss: 0.963839  [   96/  130]
train() client id: f_00008-8-3 loss: 0.749503  [  128/  130]
train() client id: f_00008-9-0 loss: 0.670509  [   32/  130]
train() client id: f_00008-9-1 loss: 0.784904  [   64/  130]
train() client id: f_00008-9-2 loss: 0.957155  [   96/  130]
train() client id: f_00008-9-3 loss: 0.780836  [  128/  130]
train() client id: f_00008-10-0 loss: 0.879153  [   32/  130]
train() client id: f_00008-10-1 loss: 0.810871  [   64/  130]
train() client id: f_00008-10-2 loss: 0.721928  [   96/  130]
train() client id: f_00008-10-3 loss: 0.776898  [  128/  130]
train() client id: f_00009-0-0 loss: 1.011248  [   32/  118]
train() client id: f_00009-0-1 loss: 1.024378  [   64/  118]
train() client id: f_00009-0-2 loss: 1.010945  [   96/  118]
train() client id: f_00009-1-0 loss: 0.980142  [   32/  118]
train() client id: f_00009-1-1 loss: 0.943700  [   64/  118]
train() client id: f_00009-1-2 loss: 1.003691  [   96/  118]
train() client id: f_00009-2-0 loss: 1.076176  [   32/  118]
train() client id: f_00009-2-1 loss: 0.915050  [   64/  118]
train() client id: f_00009-2-2 loss: 0.818743  [   96/  118]
train() client id: f_00009-3-0 loss: 0.918265  [   32/  118]
train() client id: f_00009-3-1 loss: 0.917979  [   64/  118]
train() client id: f_00009-3-2 loss: 0.848560  [   96/  118]
train() client id: f_00009-4-0 loss: 0.841347  [   32/  118]
train() client id: f_00009-4-1 loss: 0.842004  [   64/  118]
train() client id: f_00009-4-2 loss: 0.968155  [   96/  118]
train() client id: f_00009-5-0 loss: 0.959939  [   32/  118]
train() client id: f_00009-5-1 loss: 0.879899  [   64/  118]
train() client id: f_00009-5-2 loss: 0.809036  [   96/  118]
train() client id: f_00009-6-0 loss: 0.789844  [   32/  118]
train() client id: f_00009-6-1 loss: 0.958035  [   64/  118]
train() client id: f_00009-6-2 loss: 0.909385  [   96/  118]
train() client id: f_00009-7-0 loss: 0.873289  [   32/  118]
train() client id: f_00009-7-1 loss: 0.798286  [   64/  118]
train() client id: f_00009-7-2 loss: 0.732431  [   96/  118]
train() client id: f_00009-8-0 loss: 0.805200  [   32/  118]
train() client id: f_00009-8-1 loss: 0.853687  [   64/  118]
train() client id: f_00009-8-2 loss: 0.793067  [   96/  118]
train() client id: f_00009-9-0 loss: 0.809063  [   32/  118]
train() client id: f_00009-9-1 loss: 0.833474  [   64/  118]
train() client id: f_00009-9-2 loss: 0.876701  [   96/  118]
train() client id: f_00009-10-0 loss: 0.906184  [   32/  118]
train() client id: f_00009-10-1 loss: 0.826862  [   64/  118]
train() client id: f_00009-10-2 loss: 0.816898  [   96/  118]
At round 15 accuracy: 0.6392572944297082
At round 15 training accuracy: 0.5734406438631791
At round 15 training loss: 0.8484082326134752
update_location
xs = -4.528292 -23.998411 -4.954880 14.056472 -115.103519 -50.217951 -27.215960 48.375741 -1.680116 -5.304393 
ys = 62.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 -3.124922 -19.177652 7.845030 -0.998518 
xs mean: -17.05713084058713
ys mean: 7.871751218646876
dists_uav = 118.058283 104.009172 100.131388 103.469094 152.762057 113.213056 103.684491 112.729742 100.321320 100.145562 
uav_gains = -101.802582 -100.426819 -100.014272 -100.370292 -104.606174 -101.347507 -100.392872 -101.301051 -100.034848 -100.015809 
uav_gains_db_mean: -101.03122252883855
dists_bs = 204.198604 219.599455 243.063514 242.868133 176.146101 229.141791 231.448937 295.975813 240.791541 244.483287 
bs_gains = -104.248537 -105.132734 -106.367213 -106.357434 -102.451504 -105.649980 -105.771804 -108.762233 -106.253013 -106.438036 
bs_gains_db_mean: -105.7432489126474
Round 16
-------------------------------
ene_coms = [0.00681156 0.00800074 0.00630615 0.00640078 0.00701527 0.00822266
 0.00640687 0.00666194 0.00849693 0.00858465]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.87022043 18.51057874  8.72031296  3.12061071 21.32349618 10.28575691
  3.88149856 12.53221179  9.18550343  8.35760809]
obj_prev = 104.78779779809503
eta_min = 2.8729165696648184e-11	eta_max = 0.9322030191898866
af = 22.167428793692164	bf = 1.6551376393889736	zeta = 24.38417167306138	eta = 0.9090909090909091
af = 22.167428793692164	bf = 1.6551376393889736	zeta = 41.447111898668766	eta = 0.534836512804265
af = 22.167428793692164	bf = 1.6551376393889736	zeta = 33.380490085215285	eta = 0.6640833833506371
af = 22.167428793692164	bf = 1.6551376393889736	zeta = 31.940428061556712	eta = 0.6940241611969107
af = 22.167428793692164	bf = 1.6551376393889736	zeta = 31.871006210673738	eta = 0.6955358938830176
af = 22.167428793692164	bf = 1.6551376393889736	zeta = 31.87083398813142	eta = 0.6955396523965213
eta = 0.6955396523965213
ene_coms = [0.00681156 0.00800074 0.00630615 0.00640078 0.00701527 0.00822266
 0.00640687 0.00666194 0.00849693 0.00858465]
ene_comp = [0.02995944 0.06300993 0.02948389 0.01022425 0.07275864 0.03471491
 0.01283976 0.04256143 0.03091053 0.02805725]
ene_total = [2.7418037  5.29486021 2.66865939 1.23963359 5.94828451 3.20160936
 1.43511152 3.6703051  2.93838912 2.73217749]
ti_comp = [0.32349184 0.31160009 0.32854594 0.3275997  0.32145482 0.30938084
 0.32753874 0.3249881  0.30663822 0.30576097]
ti_coms = [0.06811563 0.08000738 0.06306154 0.06400777 0.07015266 0.08222663
 0.06406873 0.06661937 0.08496925 0.08584651]
t_total = [29.19993286 29.19993286 29.19993286 29.19993286 29.19993286 29.19993286
 29.19993286 29.19993286 29.19993286 29.19993286]
ene_coms = [0.00681156 0.00800074 0.00630615 0.00640078 0.00701527 0.00822266
 0.00640687 0.00666194 0.00849693 0.00858465]
ene_comp = [1.60603283e-05 1.61031901e-04 1.48402869e-05 6.22426134e-07
 2.32966701e-04 2.73175145e-05 1.23317688e-06 4.56240269e-05
 1.96312037e-05 1.47656094e-05]
ene_total = [0.50909691 0.6085766  0.47132042 0.47731582 0.5404593  0.61515396
 0.4778159  0.50014453 0.63503098 0.64120935]
optimize_network iter = 0 obj = 5.4761237907050555
eta = 0.6955396523965213
freqs = [4.63063299e+07 1.01107052e+08 4.48702749e+07 1.56047964e+07
 1.13170865e+08 5.61038400e+07 1.96003683e+07 6.54815146e+07
 5.04022835e+07 4.58810247e+07]
eta_min = 0.6955396523965347	eta_max = 0.6955396523965189
af = 0.03339577787114905	bf = 1.6551376393889736	zeta = 0.03673535565826396	eta = 0.909090909090909
af = 0.03339577787114905	bf = 1.6551376393889736	zeta = 18.24152656680833	eta = 0.0018307556524306957
af = 0.03339577787114905	bf = 1.6551376393889736	zeta = 1.8690538923598028	eta = 0.017867744749181468
af = 0.03339577787114905	bf = 1.6551376393889736	zeta = 1.8221044664298207	eta = 0.01832813567302416
af = 0.03339577787114905	bf = 1.6551376393889736	zeta = 1.8220935504625322	eta = 0.018328245474921772
eta = 0.018328245474921772
ene_coms = [0.00681156 0.00800074 0.00630615 0.00640078 0.00701527 0.00822266
 0.00640687 0.00666194 0.00849693 0.00858465]
ene_comp = [1.76910095e-04 1.77382232e-03 1.63470915e-04 6.85624007e-06
 2.56620913e-03 3.00911911e-04 1.35838717e-05 5.02564504e-04
 2.16244528e-04 1.62648317e-04]
ene_total = [0.1616135  0.22604379 0.14961475 0.14818116 0.22157854 0.19711384
 0.14847772 0.16568428 0.20149836 0.20228761]
ti_comp = [0.32349184 0.31160009 0.32854594 0.3275997  0.32145482 0.30938084
 0.32753874 0.3249881  0.30663822 0.30576097]
ti_coms = [0.06811563 0.08000738 0.06306154 0.06400777 0.07015266 0.08222663
 0.06406873 0.06661937 0.08496925 0.08584651]
t_total = [29.19993286 29.19993286 29.19993286 29.19993286 29.19993286 29.19993286
 29.19993286 29.19993286 29.19993286 29.19993286]
ene_coms = [0.00681156 0.00800074 0.00630615 0.00640078 0.00701527 0.00822266
 0.00640687 0.00666194 0.00849693 0.00858465]
ene_comp = [1.60603283e-05 1.61031901e-04 1.48402869e-05 6.22426134e-07
 2.32966701e-04 2.73175145e-05 1.23317688e-06 4.56240269e-05
 1.96312037e-05 1.47656094e-05]
ene_total = [0.50909691 0.6085766  0.47132042 0.47731582 0.5404593  0.61515396
 0.4778159  0.50014453 0.63503098 0.64120935]
optimize_network iter = 1 obj = 5.476123790705294
eta = 0.6955396523965347
freqs = [4.63063299e+07 1.01107052e+08 4.48702749e+07 1.56047964e+07
 1.13170865e+08 5.61038400e+07 1.96003683e+07 6.54815146e+07
 5.04022835e+07 4.58810247e+07]
Done!
ene_coms = [0.00681156 0.00800074 0.00630615 0.00640078 0.00701527 0.00822266
 0.00640687 0.00666194 0.00849693 0.00858465]
ene_comp = [1.48598340e-05 1.48994918e-04 1.37309895e-05 5.75900370e-07
 2.15552661e-04 2.52755562e-05 1.14099807e-06 4.22136741e-05
 1.81637898e-05 1.36618941e-05]
ene_total = [0.00682642 0.00814973 0.00631988 0.00640135 0.00723082 0.00824794
 0.00640801 0.00670415 0.00851509 0.00859831]
At round 16 energy consumption: 0.07340171880384805
At round 16 eta: 0.6955396523965347
At round 16 a_n: 22.701869304745298
At round 16 local rounds: 11.888666579119368
At round 16 global rounds: 74.56428885876667
gradient difference: 0.42609167098999023
train() client id: f_00000-0-0 loss: 1.361146  [   32/  126]
train() client id: f_00000-0-1 loss: 1.254934  [   64/  126]
train() client id: f_00000-0-2 loss: 1.147873  [   96/  126]
train() client id: f_00000-1-0 loss: 1.277634  [   32/  126]
train() client id: f_00000-1-1 loss: 1.106342  [   64/  126]
train() client id: f_00000-1-2 loss: 1.076605  [   96/  126]
train() client id: f_00000-2-0 loss: 1.187694  [   32/  126]
train() client id: f_00000-2-1 loss: 1.061815  [   64/  126]
train() client id: f_00000-2-2 loss: 1.188692  [   96/  126]
train() client id: f_00000-3-0 loss: 1.063218  [   32/  126]
train() client id: f_00000-3-1 loss: 1.132287  [   64/  126]
train() client id: f_00000-3-2 loss: 0.999208  [   96/  126]
train() client id: f_00000-4-0 loss: 0.934332  [   32/  126]
train() client id: f_00000-4-1 loss: 0.952498  [   64/  126]
train() client id: f_00000-4-2 loss: 1.196715  [   96/  126]
train() client id: f_00000-5-0 loss: 0.981175  [   32/  126]
train() client id: f_00000-5-1 loss: 0.979821  [   64/  126]
train() client id: f_00000-5-2 loss: 1.084703  [   96/  126]
train() client id: f_00000-6-0 loss: 0.986562  [   32/  126]
train() client id: f_00000-6-1 loss: 0.944970  [   64/  126]
train() client id: f_00000-6-2 loss: 1.121188  [   96/  126]
train() client id: f_00000-7-0 loss: 0.967373  [   32/  126]
train() client id: f_00000-7-1 loss: 0.946007  [   64/  126]
train() client id: f_00000-7-2 loss: 1.024229  [   96/  126]
train() client id: f_00000-8-0 loss: 1.056032  [   32/  126]
train() client id: f_00000-8-1 loss: 1.011595  [   64/  126]
train() client id: f_00000-8-2 loss: 0.880992  [   96/  126]
train() client id: f_00000-9-0 loss: 0.966938  [   32/  126]
train() client id: f_00000-9-1 loss: 1.010390  [   64/  126]
train() client id: f_00000-9-2 loss: 0.932343  [   96/  126]
train() client id: f_00000-10-0 loss: 1.051393  [   32/  126]
train() client id: f_00000-10-1 loss: 0.886633  [   64/  126]
train() client id: f_00000-10-2 loss: 0.923881  [   96/  126]
train() client id: f_00001-0-0 loss: 0.522633  [   32/  265]
train() client id: f_00001-0-1 loss: 0.626770  [   64/  265]
train() client id: f_00001-0-2 loss: 0.531079  [   96/  265]
train() client id: f_00001-0-3 loss: 0.644917  [  128/  265]
train() client id: f_00001-0-4 loss: 0.530166  [  160/  265]
train() client id: f_00001-0-5 loss: 0.528440  [  192/  265]
train() client id: f_00001-0-6 loss: 0.503901  [  224/  265]
train() client id: f_00001-0-7 loss: 0.471273  [  256/  265]
train() client id: f_00001-1-0 loss: 0.515847  [   32/  265]
train() client id: f_00001-1-1 loss: 0.528569  [   64/  265]
train() client id: f_00001-1-2 loss: 0.502368  [   96/  265]
train() client id: f_00001-1-3 loss: 0.504863  [  128/  265]
train() client id: f_00001-1-4 loss: 0.555381  [  160/  265]
train() client id: f_00001-1-5 loss: 0.577267  [  192/  265]
train() client id: f_00001-1-6 loss: 0.559959  [  224/  265]
train() client id: f_00001-1-7 loss: 0.490045  [  256/  265]
train() client id: f_00001-2-0 loss: 0.584420  [   32/  265]
train() client id: f_00001-2-1 loss: 0.510465  [   64/  265]
train() client id: f_00001-2-2 loss: 0.700725  [   96/  265]
train() client id: f_00001-2-3 loss: 0.484720  [  128/  265]
train() client id: f_00001-2-4 loss: 0.471329  [  160/  265]
train() client id: f_00001-2-5 loss: 0.428795  [  192/  265]
train() client id: f_00001-2-6 loss: 0.600126  [  224/  265]
train() client id: f_00001-2-7 loss: 0.460315  [  256/  265]
train() client id: f_00001-3-0 loss: 0.586598  [   32/  265]
train() client id: f_00001-3-1 loss: 0.489012  [   64/  265]
train() client id: f_00001-3-2 loss: 0.493279  [   96/  265]
train() client id: f_00001-3-3 loss: 0.575025  [  128/  265]
train() client id: f_00001-3-4 loss: 0.518752  [  160/  265]
train() client id: f_00001-3-5 loss: 0.493551  [  192/  265]
train() client id: f_00001-3-6 loss: 0.524998  [  224/  265]
train() client id: f_00001-3-7 loss: 0.523796  [  256/  265]
train() client id: f_00001-4-0 loss: 0.556327  [   32/  265]
train() client id: f_00001-4-1 loss: 0.517198  [   64/  265]
train() client id: f_00001-4-2 loss: 0.542954  [   96/  265]
train() client id: f_00001-4-3 loss: 0.482097  [  128/  265]
train() client id: f_00001-4-4 loss: 0.593048  [  160/  265]
train() client id: f_00001-4-5 loss: 0.546932  [  192/  265]
train() client id: f_00001-4-6 loss: 0.430412  [  224/  265]
train() client id: f_00001-4-7 loss: 0.475006  [  256/  265]
train() client id: f_00001-5-0 loss: 0.512959  [   32/  265]
train() client id: f_00001-5-1 loss: 0.492658  [   64/  265]
train() client id: f_00001-5-2 loss: 0.512849  [   96/  265]
train() client id: f_00001-5-3 loss: 0.430163  [  128/  265]
train() client id: f_00001-5-4 loss: 0.484775  [  160/  265]
train() client id: f_00001-5-5 loss: 0.602908  [  192/  265]
train() client id: f_00001-5-6 loss: 0.564070  [  224/  265]
train() client id: f_00001-5-7 loss: 0.540640  [  256/  265]
train() client id: f_00001-6-0 loss: 0.544882  [   32/  265]
train() client id: f_00001-6-1 loss: 0.604599  [   64/  265]
train() client id: f_00001-6-2 loss: 0.502199  [   96/  265]
train() client id: f_00001-6-3 loss: 0.540771  [  128/  265]
train() client id: f_00001-6-4 loss: 0.598704  [  160/  265]
train() client id: f_00001-6-5 loss: 0.511137  [  192/  265]
train() client id: f_00001-6-6 loss: 0.433044  [  224/  265]
train() client id: f_00001-6-7 loss: 0.437887  [  256/  265]
train() client id: f_00001-7-0 loss: 0.467672  [   32/  265]
train() client id: f_00001-7-1 loss: 0.645892  [   64/  265]
train() client id: f_00001-7-2 loss: 0.516496  [   96/  265]
train() client id: f_00001-7-3 loss: 0.505159  [  128/  265]
train() client id: f_00001-7-4 loss: 0.464673  [  160/  265]
train() client id: f_00001-7-5 loss: 0.482659  [  192/  265]
train() client id: f_00001-7-6 loss: 0.634869  [  224/  265]
train() client id: f_00001-7-7 loss: 0.454180  [  256/  265]
train() client id: f_00001-8-0 loss: 0.431562  [   32/  265]
train() client id: f_00001-8-1 loss: 0.464888  [   64/  265]
train() client id: f_00001-8-2 loss: 0.681688  [   96/  265]
train() client id: f_00001-8-3 loss: 0.483535  [  128/  265]
train() client id: f_00001-8-4 loss: 0.560570  [  160/  265]
train() client id: f_00001-8-5 loss: 0.432671  [  192/  265]
train() client id: f_00001-8-6 loss: 0.588490  [  224/  265]
train() client id: f_00001-8-7 loss: 0.509792  [  256/  265]
train() client id: f_00001-9-0 loss: 0.484021  [   32/  265]
train() client id: f_00001-9-1 loss: 0.518480  [   64/  265]
train() client id: f_00001-9-2 loss: 0.517081  [   96/  265]
train() client id: f_00001-9-3 loss: 0.562304  [  128/  265]
train() client id: f_00001-9-4 loss: 0.430170  [  160/  265]
train() client id: f_00001-9-5 loss: 0.468915  [  192/  265]
train() client id: f_00001-9-6 loss: 0.566080  [  224/  265]
train() client id: f_00001-9-7 loss: 0.466214  [  256/  265]
train() client id: f_00001-10-0 loss: 0.603050  [   32/  265]
train() client id: f_00001-10-1 loss: 0.574236  [   64/  265]
train() client id: f_00001-10-2 loss: 0.426539  [   96/  265]
train() client id: f_00001-10-3 loss: 0.556415  [  128/  265]
train() client id: f_00001-10-4 loss: 0.566906  [  160/  265]
train() client id: f_00001-10-5 loss: 0.481416  [  192/  265]
train() client id: f_00001-10-6 loss: 0.426513  [  224/  265]
train() client id: f_00001-10-7 loss: 0.475716  [  256/  265]
train() client id: f_00002-0-0 loss: 1.240293  [   32/  124]
train() client id: f_00002-0-1 loss: 1.171294  [   64/  124]
train() client id: f_00002-0-2 loss: 1.090664  [   96/  124]
train() client id: f_00002-1-0 loss: 1.124068  [   32/  124]
train() client id: f_00002-1-1 loss: 1.015870  [   64/  124]
train() client id: f_00002-1-2 loss: 1.072677  [   96/  124]
train() client id: f_00002-2-0 loss: 1.087085  [   32/  124]
train() client id: f_00002-2-1 loss: 1.159457  [   64/  124]
train() client id: f_00002-2-2 loss: 1.024418  [   96/  124]
train() client id: f_00002-3-0 loss: 1.020397  [   32/  124]
train() client id: f_00002-3-1 loss: 1.013595  [   64/  124]
train() client id: f_00002-3-2 loss: 1.242476  [   96/  124]
train() client id: f_00002-4-0 loss: 1.116505  [   32/  124]
train() client id: f_00002-4-1 loss: 0.895085  [   64/  124]
train() client id: f_00002-4-2 loss: 1.165947  [   96/  124]
train() client id: f_00002-5-0 loss: 1.103097  [   32/  124]
train() client id: f_00002-5-1 loss: 1.090649  [   64/  124]
train() client id: f_00002-5-2 loss: 0.879177  [   96/  124]
train() client id: f_00002-6-0 loss: 1.051971  [   32/  124]
train() client id: f_00002-6-1 loss: 1.023524  [   64/  124]
train() client id: f_00002-6-2 loss: 0.983534  [   96/  124]
train() client id: f_00002-7-0 loss: 0.868810  [   32/  124]
train() client id: f_00002-7-1 loss: 0.926818  [   64/  124]
train() client id: f_00002-7-2 loss: 0.995686  [   96/  124]
train() client id: f_00002-8-0 loss: 0.977271  [   32/  124]
train() client id: f_00002-8-1 loss: 0.911383  [   64/  124]
train() client id: f_00002-8-2 loss: 1.132711  [   96/  124]
train() client id: f_00002-9-0 loss: 1.153600  [   32/  124]
train() client id: f_00002-9-1 loss: 0.834848  [   64/  124]
train() client id: f_00002-9-2 loss: 0.975658  [   96/  124]
train() client id: f_00002-10-0 loss: 1.032874  [   32/  124]
train() client id: f_00002-10-1 loss: 0.799599  [   64/  124]
train() client id: f_00002-10-2 loss: 0.966884  [   96/  124]
train() client id: f_00003-0-0 loss: 1.036153  [   32/   43]
train() client id: f_00003-1-0 loss: 0.911810  [   32/   43]
train() client id: f_00003-2-0 loss: 0.892500  [   32/   43]
train() client id: f_00003-3-0 loss: 0.991674  [   32/   43]
train() client id: f_00003-4-0 loss: 1.022848  [   32/   43]
train() client id: f_00003-5-0 loss: 0.974563  [   32/   43]
train() client id: f_00003-6-0 loss: 0.875313  [   32/   43]
train() client id: f_00003-7-0 loss: 0.904446  [   32/   43]
train() client id: f_00003-8-0 loss: 0.918966  [   32/   43]
train() client id: f_00003-9-0 loss: 1.040977  [   32/   43]
train() client id: f_00003-10-0 loss: 0.884185  [   32/   43]
train() client id: f_00004-0-0 loss: 0.846683  [   32/  306]
train() client id: f_00004-0-1 loss: 0.906686  [   64/  306]
train() client id: f_00004-0-2 loss: 0.892967  [   96/  306]
train() client id: f_00004-0-3 loss: 0.918516  [  128/  306]
train() client id: f_00004-0-4 loss: 0.975610  [  160/  306]
train() client id: f_00004-0-5 loss: 0.992026  [  192/  306]
train() client id: f_00004-0-6 loss: 0.943854  [  224/  306]
train() client id: f_00004-0-7 loss: 0.976145  [  256/  306]
train() client id: f_00004-0-8 loss: 0.839877  [  288/  306]
train() client id: f_00004-1-0 loss: 0.915532  [   32/  306]
train() client id: f_00004-1-1 loss: 0.943523  [   64/  306]
train() client id: f_00004-1-2 loss: 0.931138  [   96/  306]
train() client id: f_00004-1-3 loss: 0.951536  [  128/  306]
train() client id: f_00004-1-4 loss: 0.961218  [  160/  306]
train() client id: f_00004-1-5 loss: 0.887832  [  192/  306]
train() client id: f_00004-1-6 loss: 0.922409  [  224/  306]
train() client id: f_00004-1-7 loss: 0.970688  [  256/  306]
train() client id: f_00004-1-8 loss: 0.881326  [  288/  306]
train() client id: f_00004-2-0 loss: 0.855362  [   32/  306]
train() client id: f_00004-2-1 loss: 0.880201  [   64/  306]
train() client id: f_00004-2-2 loss: 0.898573  [   96/  306]
train() client id: f_00004-2-3 loss: 0.906506  [  128/  306]
train() client id: f_00004-2-4 loss: 0.857875  [  160/  306]
train() client id: f_00004-2-5 loss: 0.904796  [  192/  306]
train() client id: f_00004-2-6 loss: 0.996070  [  224/  306]
train() client id: f_00004-2-7 loss: 1.003476  [  256/  306]
train() client id: f_00004-2-8 loss: 0.987102  [  288/  306]
train() client id: f_00004-3-0 loss: 0.840994  [   32/  306]
train() client id: f_00004-3-1 loss: 0.853319  [   64/  306]
train() client id: f_00004-3-2 loss: 1.051024  [   96/  306]
train() client id: f_00004-3-3 loss: 0.899231  [  128/  306]
train() client id: f_00004-3-4 loss: 0.970572  [  160/  306]
train() client id: f_00004-3-5 loss: 0.991920  [  192/  306]
train() client id: f_00004-3-6 loss: 0.866785  [  224/  306]
train() client id: f_00004-3-7 loss: 1.040069  [  256/  306]
train() client id: f_00004-3-8 loss: 0.846281  [  288/  306]
train() client id: f_00004-4-0 loss: 0.999914  [   32/  306]
train() client id: f_00004-4-1 loss: 0.915426  [   64/  306]
train() client id: f_00004-4-2 loss: 1.003285  [   96/  306]
train() client id: f_00004-4-3 loss: 0.896612  [  128/  306]
train() client id: f_00004-4-4 loss: 0.798260  [  160/  306]
train() client id: f_00004-4-5 loss: 0.715399  [  192/  306]
train() client id: f_00004-4-6 loss: 1.089094  [  224/  306]
train() client id: f_00004-4-7 loss: 0.950933  [  256/  306]
train() client id: f_00004-4-8 loss: 0.956606  [  288/  306]
train() client id: f_00004-5-0 loss: 0.855398  [   32/  306]
train() client id: f_00004-5-1 loss: 0.897890  [   64/  306]
train() client id: f_00004-5-2 loss: 0.966358  [   96/  306]
train() client id: f_00004-5-3 loss: 0.813454  [  128/  306]
train() client id: f_00004-5-4 loss: 0.855671  [  160/  306]
train() client id: f_00004-5-5 loss: 0.898422  [  192/  306]
train() client id: f_00004-5-6 loss: 1.016046  [  224/  306]
train() client id: f_00004-5-7 loss: 0.905939  [  256/  306]
train() client id: f_00004-5-8 loss: 1.012178  [  288/  306]
train() client id: f_00004-6-0 loss: 0.932700  [   32/  306]
train() client id: f_00004-6-1 loss: 0.849664  [   64/  306]
train() client id: f_00004-6-2 loss: 0.903776  [   96/  306]
train() client id: f_00004-6-3 loss: 0.834880  [  128/  306]
train() client id: f_00004-6-4 loss: 0.941846  [  160/  306]
train() client id: f_00004-6-5 loss: 1.011915  [  192/  306]
train() client id: f_00004-6-6 loss: 0.894034  [  224/  306]
train() client id: f_00004-6-7 loss: 0.823348  [  256/  306]
train() client id: f_00004-6-8 loss: 1.126500  [  288/  306]
train() client id: f_00004-7-0 loss: 0.891987  [   32/  306]
train() client id: f_00004-7-1 loss: 0.993075  [   64/  306]
train() client id: f_00004-7-2 loss: 0.916891  [   96/  306]
train() client id: f_00004-7-3 loss: 0.948844  [  128/  306]
train() client id: f_00004-7-4 loss: 0.786887  [  160/  306]
train() client id: f_00004-7-5 loss: 0.872161  [  192/  306]
train() client id: f_00004-7-6 loss: 1.025565  [  224/  306]
train() client id: f_00004-7-7 loss: 0.877247  [  256/  306]
train() client id: f_00004-7-8 loss: 0.963091  [  288/  306]
train() client id: f_00004-8-0 loss: 0.812943  [   32/  306]
train() client id: f_00004-8-1 loss: 0.970506  [   64/  306]
train() client id: f_00004-8-2 loss: 0.947015  [   96/  306]
train() client id: f_00004-8-3 loss: 0.906816  [  128/  306]
train() client id: f_00004-8-4 loss: 0.958293  [  160/  306]
train() client id: f_00004-8-5 loss: 0.882102  [  192/  306]
train() client id: f_00004-8-6 loss: 0.974760  [  224/  306]
train() client id: f_00004-8-7 loss: 0.958747  [  256/  306]
train() client id: f_00004-8-8 loss: 0.860202  [  288/  306]
train() client id: f_00004-9-0 loss: 0.975603  [   32/  306]
train() client id: f_00004-9-1 loss: 0.812322  [   64/  306]
train() client id: f_00004-9-2 loss: 0.902896  [   96/  306]
train() client id: f_00004-9-3 loss: 0.957788  [  128/  306]
train() client id: f_00004-9-4 loss: 0.947679  [  160/  306]
train() client id: f_00004-9-5 loss: 0.933781  [  192/  306]
train() client id: f_00004-9-6 loss: 0.855560  [  224/  306]
train() client id: f_00004-9-7 loss: 0.905506  [  256/  306]
train() client id: f_00004-9-8 loss: 0.966739  [  288/  306]
train() client id: f_00004-10-0 loss: 0.928687  [   32/  306]
train() client id: f_00004-10-1 loss: 0.958308  [   64/  306]
train() client id: f_00004-10-2 loss: 0.914162  [   96/  306]
train() client id: f_00004-10-3 loss: 0.909840  [  128/  306]
train() client id: f_00004-10-4 loss: 0.964436  [  160/  306]
train() client id: f_00004-10-5 loss: 1.046316  [  192/  306]
train() client id: f_00004-10-6 loss: 0.864077  [  224/  306]
train() client id: f_00004-10-7 loss: 0.901529  [  256/  306]
train() client id: f_00004-10-8 loss: 0.864673  [  288/  306]
train() client id: f_00005-0-0 loss: 0.521319  [   32/  146]
train() client id: f_00005-0-1 loss: 1.139730  [   64/  146]
train() client id: f_00005-0-2 loss: 0.725774  [   96/  146]
train() client id: f_00005-0-3 loss: 0.589233  [  128/  146]
train() client id: f_00005-1-0 loss: 0.575925  [   32/  146]
train() client id: f_00005-1-1 loss: 0.961784  [   64/  146]
train() client id: f_00005-1-2 loss: 0.807438  [   96/  146]
train() client id: f_00005-1-3 loss: 0.614129  [  128/  146]
train() client id: f_00005-2-0 loss: 0.679741  [   32/  146]
train() client id: f_00005-2-1 loss: 0.746151  [   64/  146]
train() client id: f_00005-2-2 loss: 0.579972  [   96/  146]
train() client id: f_00005-2-3 loss: 0.936515  [  128/  146]
train() client id: f_00005-3-0 loss: 0.591039  [   32/  146]
train() client id: f_00005-3-1 loss: 0.687695  [   64/  146]
train() client id: f_00005-3-2 loss: 0.759120  [   96/  146]
train() client id: f_00005-3-3 loss: 0.824029  [  128/  146]
train() client id: f_00005-4-0 loss: 0.478403  [   32/  146]
train() client id: f_00005-4-1 loss: 0.725277  [   64/  146]
train() client id: f_00005-4-2 loss: 0.804975  [   96/  146]
train() client id: f_00005-4-3 loss: 0.658168  [  128/  146]
train() client id: f_00005-5-0 loss: 0.939834  [   32/  146]
train() client id: f_00005-5-1 loss: 0.822366  [   64/  146]
train() client id: f_00005-5-2 loss: 0.514830  [   96/  146]
train() client id: f_00005-5-3 loss: 0.704257  [  128/  146]
train() client id: f_00005-6-0 loss: 0.828230  [   32/  146]
train() client id: f_00005-6-1 loss: 0.740599  [   64/  146]
train() client id: f_00005-6-2 loss: 0.665071  [   96/  146]
train() client id: f_00005-6-3 loss: 0.550451  [  128/  146]
train() client id: f_00005-7-0 loss: 1.011034  [   32/  146]
train() client id: f_00005-7-1 loss: 0.673228  [   64/  146]
train() client id: f_00005-7-2 loss: 0.571767  [   96/  146]
train() client id: f_00005-7-3 loss: 0.746248  [  128/  146]
train() client id: f_00005-8-0 loss: 0.966312  [   32/  146]
train() client id: f_00005-8-1 loss: 0.810754  [   64/  146]
train() client id: f_00005-8-2 loss: 0.481816  [   96/  146]
train() client id: f_00005-8-3 loss: 0.692780  [  128/  146]
train() client id: f_00005-9-0 loss: 0.680866  [   32/  146]
train() client id: f_00005-9-1 loss: 0.783626  [   64/  146]
train() client id: f_00005-9-2 loss: 0.783512  [   96/  146]
train() client id: f_00005-9-3 loss: 0.541056  [  128/  146]
train() client id: f_00005-10-0 loss: 0.587702  [   32/  146]
train() client id: f_00005-10-1 loss: 0.722660  [   64/  146]
train() client id: f_00005-10-2 loss: 1.008340  [   96/  146]
train() client id: f_00005-10-3 loss: 0.613887  [  128/  146]
train() client id: f_00006-0-0 loss: 0.570215  [   32/   54]
train() client id: f_00006-1-0 loss: 0.621852  [   32/   54]
train() client id: f_00006-2-0 loss: 0.641057  [   32/   54]
train() client id: f_00006-3-0 loss: 0.567461  [   32/   54]
train() client id: f_00006-4-0 loss: 0.577201  [   32/   54]
train() client id: f_00006-5-0 loss: 0.572762  [   32/   54]
train() client id: f_00006-6-0 loss: 0.515116  [   32/   54]
train() client id: f_00006-7-0 loss: 0.576827  [   32/   54]
train() client id: f_00006-8-0 loss: 0.569373  [   32/   54]
train() client id: f_00006-9-0 loss: 0.584566  [   32/   54]
train() client id: f_00006-10-0 loss: 0.632244  [   32/   54]
train() client id: f_00007-0-0 loss: 0.661948  [   32/  179]
train() client id: f_00007-0-1 loss: 0.631981  [   64/  179]
train() client id: f_00007-0-2 loss: 0.621628  [   96/  179]
train() client id: f_00007-0-3 loss: 0.692733  [  128/  179]
train() client id: f_00007-0-4 loss: 0.608390  [  160/  179]
train() client id: f_00007-1-0 loss: 0.647741  [   32/  179]
train() client id: f_00007-1-1 loss: 0.745865  [   64/  179]
train() client id: f_00007-1-2 loss: 0.498062  [   96/  179]
train() client id: f_00007-1-3 loss: 0.504420  [  128/  179]
train() client id: f_00007-1-4 loss: 0.571208  [  160/  179]
train() client id: f_00007-2-0 loss: 0.486946  [   32/  179]
train() client id: f_00007-2-1 loss: 0.638445  [   64/  179]
train() client id: f_00007-2-2 loss: 0.553195  [   96/  179]
train() client id: f_00007-2-3 loss: 0.759026  [  128/  179]
train() client id: f_00007-2-4 loss: 0.565578  [  160/  179]
train() client id: f_00007-3-0 loss: 0.534839  [   32/  179]
train() client id: f_00007-3-1 loss: 0.564925  [   64/  179]
train() client id: f_00007-3-2 loss: 0.560539  [   96/  179]
train() client id: f_00007-3-3 loss: 0.856156  [  128/  179]
train() client id: f_00007-3-4 loss: 0.518490  [  160/  179]
train() client id: f_00007-4-0 loss: 0.666331  [   32/  179]
train() client id: f_00007-4-1 loss: 0.742848  [   64/  179]
train() client id: f_00007-4-2 loss: 0.542798  [   96/  179]
train() client id: f_00007-4-3 loss: 0.484274  [  128/  179]
train() client id: f_00007-4-4 loss: 0.595820  [  160/  179]
train() client id: f_00007-5-0 loss: 0.487507  [   32/  179]
train() client id: f_00007-5-1 loss: 0.544535  [   64/  179]
train() client id: f_00007-5-2 loss: 0.661151  [   96/  179]
train() client id: f_00007-5-3 loss: 0.663911  [  128/  179]
train() client id: f_00007-5-4 loss: 0.635594  [  160/  179]
train() client id: f_00007-6-0 loss: 0.649244  [   32/  179]
train() client id: f_00007-6-1 loss: 0.556070  [   64/  179]
train() client id: f_00007-6-2 loss: 0.542893  [   96/  179]
train() client id: f_00007-6-3 loss: 0.555091  [  128/  179]
train() client id: f_00007-6-4 loss: 0.607597  [  160/  179]
train() client id: f_00007-7-0 loss: 0.637194  [   32/  179]
train() client id: f_00007-7-1 loss: 0.594024  [   64/  179]
train() client id: f_00007-7-2 loss: 0.618061  [   96/  179]
train() client id: f_00007-7-3 loss: 0.580273  [  128/  179]
train() client id: f_00007-7-4 loss: 0.620838  [  160/  179]
train() client id: f_00007-8-0 loss: 0.674597  [   32/  179]
train() client id: f_00007-8-1 loss: 0.638943  [   64/  179]
train() client id: f_00007-8-2 loss: 0.666535  [   96/  179]
train() client id: f_00007-8-3 loss: 0.469165  [  128/  179]
train() client id: f_00007-8-4 loss: 0.460856  [  160/  179]
train() client id: f_00007-9-0 loss: 0.538867  [   32/  179]
train() client id: f_00007-9-1 loss: 0.620718  [   64/  179]
train() client id: f_00007-9-2 loss: 0.711606  [   96/  179]
train() client id: f_00007-9-3 loss: 0.567420  [  128/  179]
train() client id: f_00007-9-4 loss: 0.524526  [  160/  179]
train() client id: f_00007-10-0 loss: 0.789008  [   32/  179]
train() client id: f_00007-10-1 loss: 0.668039  [   64/  179]
train() client id: f_00007-10-2 loss: 0.461446  [   96/  179]
train() client id: f_00007-10-3 loss: 0.624818  [  128/  179]
train() client id: f_00007-10-4 loss: 0.452010  [  160/  179]
train() client id: f_00008-0-0 loss: 0.846962  [   32/  130]
train() client id: f_00008-0-1 loss: 0.814994  [   64/  130]
train() client id: f_00008-0-2 loss: 0.747065  [   96/  130]
train() client id: f_00008-0-3 loss: 0.763544  [  128/  130]
train() client id: f_00008-1-0 loss: 0.695692  [   32/  130]
train() client id: f_00008-1-1 loss: 0.916951  [   64/  130]
train() client id: f_00008-1-2 loss: 0.763437  [   96/  130]
train() client id: f_00008-1-3 loss: 0.794956  [  128/  130]
train() client id: f_00008-2-0 loss: 0.663805  [   32/  130]
train() client id: f_00008-2-1 loss: 0.812759  [   64/  130]
train() client id: f_00008-2-2 loss: 0.815820  [   96/  130]
train() client id: f_00008-2-3 loss: 0.829568  [  128/  130]
train() client id: f_00008-3-0 loss: 0.856445  [   32/  130]
train() client id: f_00008-3-1 loss: 0.943946  [   64/  130]
train() client id: f_00008-3-2 loss: 0.705327  [   96/  130]
train() client id: f_00008-3-3 loss: 0.665755  [  128/  130]
train() client id: f_00008-4-0 loss: 0.710923  [   32/  130]
train() client id: f_00008-4-1 loss: 0.836990  [   64/  130]
train() client id: f_00008-4-2 loss: 0.775120  [   96/  130]
train() client id: f_00008-4-3 loss: 0.849348  [  128/  130]
train() client id: f_00008-5-0 loss: 0.810948  [   32/  130]
train() client id: f_00008-5-1 loss: 0.771903  [   64/  130]
train() client id: f_00008-5-2 loss: 0.795815  [   96/  130]
train() client id: f_00008-5-3 loss: 0.780388  [  128/  130]
train() client id: f_00008-6-0 loss: 0.761421  [   32/  130]
train() client id: f_00008-6-1 loss: 0.905115  [   64/  130]
train() client id: f_00008-6-2 loss: 0.694477  [   96/  130]
train() client id: f_00008-6-3 loss: 0.772831  [  128/  130]
train() client id: f_00008-7-0 loss: 0.810331  [   32/  130]
train() client id: f_00008-7-1 loss: 0.817021  [   64/  130]
train() client id: f_00008-7-2 loss: 0.742161  [   96/  130]
train() client id: f_00008-7-3 loss: 0.805878  [  128/  130]
train() client id: f_00008-8-0 loss: 0.766230  [   32/  130]
train() client id: f_00008-8-1 loss: 0.742680  [   64/  130]
train() client id: f_00008-8-2 loss: 0.778691  [   96/  130]
train() client id: f_00008-8-3 loss: 0.819191  [  128/  130]
train() client id: f_00008-9-0 loss: 0.750564  [   32/  130]
train() client id: f_00008-9-1 loss: 0.831215  [   64/  130]
train() client id: f_00008-9-2 loss: 0.701610  [   96/  130]
train() client id: f_00008-9-3 loss: 0.895081  [  128/  130]
train() client id: f_00008-10-0 loss: 0.854020  [   32/  130]
train() client id: f_00008-10-1 loss: 0.749864  [   64/  130]
train() client id: f_00008-10-2 loss: 0.747899  [   96/  130]
train() client id: f_00008-10-3 loss: 0.756139  [  128/  130]
train() client id: f_00009-0-0 loss: 1.067498  [   32/  118]
train() client id: f_00009-0-1 loss: 0.946293  [   64/  118]
train() client id: f_00009-0-2 loss: 1.161638  [   96/  118]
train() client id: f_00009-1-0 loss: 1.042542  [   32/  118]
train() client id: f_00009-1-1 loss: 0.843721  [   64/  118]
train() client id: f_00009-1-2 loss: 1.144109  [   96/  118]
train() client id: f_00009-2-0 loss: 0.972833  [   32/  118]
train() client id: f_00009-2-1 loss: 0.868503  [   64/  118]
train() client id: f_00009-2-2 loss: 1.025147  [   96/  118]
train() client id: f_00009-3-0 loss: 0.938970  [   32/  118]
train() client id: f_00009-3-1 loss: 0.946769  [   64/  118]
train() client id: f_00009-3-2 loss: 0.999130  [   96/  118]
train() client id: f_00009-4-0 loss: 0.865274  [   32/  118]
train() client id: f_00009-4-1 loss: 0.854850  [   64/  118]
train() client id: f_00009-4-2 loss: 0.983598  [   96/  118]
train() client id: f_00009-5-0 loss: 0.930099  [   32/  118]
train() client id: f_00009-5-1 loss: 0.853890  [   64/  118]
train() client id: f_00009-5-2 loss: 0.926873  [   96/  118]
train() client id: f_00009-6-0 loss: 0.848914  [   32/  118]
train() client id: f_00009-6-1 loss: 0.955148  [   64/  118]
train() client id: f_00009-6-2 loss: 0.937602  [   96/  118]
train() client id: f_00009-7-0 loss: 0.900592  [   32/  118]
train() client id: f_00009-7-1 loss: 0.921844  [   64/  118]
train() client id: f_00009-7-2 loss: 0.806735  [   96/  118]
train() client id: f_00009-8-0 loss: 0.912169  [   32/  118]
train() client id: f_00009-8-1 loss: 0.747384  [   64/  118]
train() client id: f_00009-8-2 loss: 0.887836  [   96/  118]
train() client id: f_00009-9-0 loss: 0.909078  [   32/  118]
train() client id: f_00009-9-1 loss: 0.849576  [   64/  118]
train() client id: f_00009-9-2 loss: 0.795452  [   96/  118]
train() client id: f_00009-10-0 loss: 0.735867  [   32/  118]
train() client id: f_00009-10-1 loss: 0.882849  [   64/  118]
train() client id: f_00009-10-2 loss: 0.829994  [   96/  118]
At round 16 accuracy: 0.6392572944297082
At round 16 training accuracy: 0.5761234071093226
At round 16 training loss: 0.8428910532964174
update_location
xs = -4.528292 -18.998411 0.045120 9.056472 -110.103519 -45.217951 -32.215960 53.375741 -1.680116 -0.304393 
ys = 67.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 -3.124922 -19.177652 2.845030 -0.998518 
xs mean: -15.057130840587131
ys mean: 7.871751218646876
dists_uav = 120.783433 102.970499 100.008730 102.909129 149.030906 111.085627 105.107722 114.964134 100.054570 100.005448 
uav_gains = -102.050419 -100.317845 -100.000964 -100.311372 -104.336086 -101.141518 -100.540898 -101.514176 -100.005939 -100.000607 
uav_gains_db_mean: -101.02198239657719
dists_bs = 201.489329 223.067112 246.587354 238.996580 177.908442 231.902525 228.288786 299.767309 244.289001 247.979906 
bs_gains = -104.086117 -105.323254 -106.542242 -106.162026 -102.572563 -105.795612 -105.604627 -108.917019 -106.428369 -106.610721 
bs_gains_db_mean: -105.8042549766387
Round 17
-------------------------------
ene_coms = [0.00688797 0.00808112 0.00630267 0.00638492 0.0070546  0.00828732
 0.00644712 0.00672472 0.00858002 0.00866812]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.73810443 18.23309011  8.5886545   3.07316613 21.00263681 10.13201652
  3.82384013 12.34453272  9.04878123  8.23338621]
obj_prev = 103.21820878221968
eta_min = 2.0431654819438683e-11	eta_max = 0.9324755676632003
af = 21.83294705699746	bf = 1.6415898352563174	zeta = 24.01624176269721	eta = 0.9090909090909091
af = 21.83294705699746	bf = 1.6415898352563174	zeta = 40.94741139408757	eta = 0.5331948055732271
af = 21.83294705699746	bf = 1.6415898352563174	zeta = 32.929455832904274	eta = 0.6630217993211176
af = 21.83294705699746	bf = 1.6415898352563174	zeta = 31.49694208423324	eta = 0.6931767216832968
af = 21.83294705699746	bf = 1.6415898352563174	zeta = 31.427591165979305	eta = 0.6947063470977011
af = 21.83294705699746	bf = 1.6415898352563174	zeta = 31.427417557745333	eta = 0.694710184725843
eta = 0.694710184725843
ene_coms = [0.00688797 0.00808112 0.00630267 0.00638492 0.0070546  0.00828732
 0.00644712 0.00672472 0.00858002 0.00866812]
ene_comp = [0.03005791 0.06321702 0.0295808  0.01025786 0.07299777 0.034829
 0.01288196 0.04270131 0.03101212 0.02814947]
ene_total = [2.70590368 5.22185166 2.62809296 1.21891165 5.86300861 3.15782457
 1.41565549 3.61994594 2.89971562 2.69650739]
ti_comp = [0.32912768 0.31719625 0.3349807  0.33415817 0.32746141 0.31513425
 0.33353618 0.33076019 0.31220717 0.31132624]
ti_coms = [0.06887973 0.08081115 0.0630267  0.06384924 0.070546   0.08287316
 0.06447123 0.06724722 0.08580024 0.08668116]
t_total = [29.14992867 29.14992867 29.14992867 29.14992867 29.14992867 29.14992867
 29.14992867 29.14992867 29.14992867 29.14992867]
ene_coms = [0.00688797 0.00808112 0.00630267 0.00638492 0.0070546  0.00828732
 0.00644712 0.00672472 0.00858002 0.00866812]
ene_comp = [1.56684967e-05 1.56937248e-04 1.44168367e-05 6.04151304e-07
 2.26719321e-04 2.65896041e-05 1.20099139e-06 4.44812664e-05
 1.91244443e-05 1.43833174e-05]
ene_total = [0.50562036 0.6033522  0.46266133 0.46767394 0.53328135 0.60890765
 0.47227308 0.49577417 0.62979872 0.63590335]
optimize_network iter = 0 obj = 5.41524613920607
eta = 0.694710184725843
freqs = [4.56629859e+07 9.96496994e+07 4.41529842e+07 1.53488041e+07
 1.11460110e+08 5.52605778e+07 1.93111875e+07 6.45502561e+07
 4.96659388e+07 4.52089523e+07]
eta_min = 0.6947101847258458	eta_max = 0.6947101847257949
af = 0.03192676005917873	bf = 1.6415898352563174	zeta = 0.035119436065096606	eta = 0.9090909090909091
af = 0.03192676005917873	bf = 1.6415898352563174	zeta = 18.090960585479394	eta = 0.0017647907588060614
af = 0.03192676005917873	bf = 1.6415898352563174	zeta = 1.8472566764881324	eta = 0.017283337213253723
af = 0.03192676005917873	bf = 1.6415898352563174	zeta = 1.8022987609843288	eta = 0.01771446596442305
af = 0.03192676005917873	bf = 1.6415898352563174	zeta = 1.8022889759143992	eta = 0.017714562140613743
eta = 0.017714562140613743
ene_coms = [0.00688797 0.00808112 0.00630267 0.00638492 0.0070546  0.00828732
 0.00644712 0.00672472 0.00858002 0.00866812]
ene_comp = [1.73493457e-04 1.73772802e-03 1.59634131e-04 6.68962062e-06
 2.51040796e-03 2.94420227e-04 1.32982859e-05 4.92530257e-04
 2.11760325e-04 1.59262980e-04]
ene_total = [0.16073699 0.22350193 0.14709854 0.14548944 0.21772398 0.19534222
 0.14705567 0.16428308 0.20012344 0.20093368]
ti_comp = [0.32912768 0.31719625 0.3349807  0.33415817 0.32746141 0.31513425
 0.33353618 0.33076019 0.31220717 0.31132624]
ti_coms = [0.06887973 0.08081115 0.0630267  0.06384924 0.070546   0.08287316
 0.06447123 0.06724722 0.08580024 0.08668116]
t_total = [29.14992867 29.14992867 29.14992867 29.14992867 29.14992867 29.14992867
 29.14992867 29.14992867 29.14992867 29.14992867]
ene_coms = [0.00688797 0.00808112 0.00630267 0.00638492 0.0070546  0.00828732
 0.00644712 0.00672472 0.00858002 0.00866812]
ene_comp = [1.56684967e-05 1.56937248e-04 1.44168367e-05 6.04151304e-07
 2.26719321e-04 2.65896041e-05 1.20099139e-06 4.44812664e-05
 1.91244443e-05 1.43833174e-05]
ene_total = [0.50562036 0.6033522  0.46266133 0.46767394 0.53328135 0.60890765
 0.47227308 0.49577417 0.62979872 0.63590335]
optimize_network iter = 1 obj = 5.4152461392061175
eta = 0.6947101847258458
freqs = [4.56629859e+07 9.96496994e+07 4.41529842e+07 1.53488041e+07
 1.11460110e+08 5.52605778e+07 1.93111875e+07 6.45502561e+07
 4.96659388e+07 4.52089523e+07]
Done!
ene_coms = [0.00688797 0.00808112 0.00630267 0.00638492 0.0070546  0.00828732
 0.00644712 0.00672472 0.00858002 0.00866812]
ene_comp = [1.44498004e-05 1.44730662e-04 1.32954946e-05 5.57160390e-07
 2.09085083e-04 2.45214636e-05 1.10757823e-06 4.10215117e-05
 1.76369442e-05 1.32645824e-05]
ene_total = [0.00690242 0.00822585 0.00631597 0.00638548 0.00726368 0.00831184
 0.00644823 0.00676574 0.00859766 0.00868138]
At round 17 energy consumption: 0.0738982528396673
At round 17 eta: 0.6947101847258458
At round 17 a_n: 22.359323457775982
At round 17 local rounds: 11.927740115741464
At round 17 global rounds: 73.2396638836347
gradient difference: 0.46340638399124146
train() client id: f_00000-0-0 loss: 1.182631  [   32/  126]
train() client id: f_00000-0-1 loss: 1.217314  [   64/  126]
train() client id: f_00000-0-2 loss: 1.319697  [   96/  126]
train() client id: f_00000-1-0 loss: 1.180903  [   32/  126]
train() client id: f_00000-1-1 loss: 1.185477  [   64/  126]
train() client id: f_00000-1-2 loss: 1.231150  [   96/  126]
train() client id: f_00000-2-0 loss: 0.995207  [   32/  126]
train() client id: f_00000-2-1 loss: 1.112673  [   64/  126]
train() client id: f_00000-2-2 loss: 1.080993  [   96/  126]
train() client id: f_00000-3-0 loss: 0.974749  [   32/  126]
train() client id: f_00000-3-1 loss: 1.185886  [   64/  126]
train() client id: f_00000-3-2 loss: 0.971992  [   96/  126]
train() client id: f_00000-4-0 loss: 0.959931  [   32/  126]
train() client id: f_00000-4-1 loss: 1.017754  [   64/  126]
train() client id: f_00000-4-2 loss: 0.934345  [   96/  126]
train() client id: f_00000-5-0 loss: 0.952401  [   32/  126]
train() client id: f_00000-5-1 loss: 1.018598  [   64/  126]
train() client id: f_00000-5-2 loss: 1.008613  [   96/  126]
train() client id: f_00000-6-0 loss: 0.894793  [   32/  126]
train() client id: f_00000-6-1 loss: 0.998936  [   64/  126]
train() client id: f_00000-6-2 loss: 0.971120  [   96/  126]
train() client id: f_00000-7-0 loss: 0.905324  [   32/  126]
train() client id: f_00000-7-1 loss: 0.936207  [   64/  126]
train() client id: f_00000-7-2 loss: 0.980001  [   96/  126]
train() client id: f_00000-8-0 loss: 0.871552  [   32/  126]
train() client id: f_00000-8-1 loss: 0.916863  [   64/  126]
train() client id: f_00000-8-2 loss: 0.913718  [   96/  126]
train() client id: f_00000-9-0 loss: 0.934373  [   32/  126]
train() client id: f_00000-9-1 loss: 0.863800  [   64/  126]
train() client id: f_00000-9-2 loss: 0.957952  [   96/  126]
train() client id: f_00000-10-0 loss: 0.827298  [   32/  126]
train() client id: f_00000-10-1 loss: 0.889137  [   64/  126]
train() client id: f_00000-10-2 loss: 1.006938  [   96/  126]
train() client id: f_00001-0-0 loss: 0.577971  [   32/  265]
train() client id: f_00001-0-1 loss: 0.533175  [   64/  265]
train() client id: f_00001-0-2 loss: 0.509031  [   96/  265]
train() client id: f_00001-0-3 loss: 0.446078  [  128/  265]
train() client id: f_00001-0-4 loss: 0.594515  [  160/  265]
train() client id: f_00001-0-5 loss: 0.470655  [  192/  265]
train() client id: f_00001-0-6 loss: 0.448906  [  224/  265]
train() client id: f_00001-0-7 loss: 0.543612  [  256/  265]
train() client id: f_00001-1-0 loss: 0.441294  [   32/  265]
train() client id: f_00001-1-1 loss: 0.656251  [   64/  265]
train() client id: f_00001-1-2 loss: 0.502105  [   96/  265]
train() client id: f_00001-1-3 loss: 0.429264  [  128/  265]
train() client id: f_00001-1-4 loss: 0.608119  [  160/  265]
train() client id: f_00001-1-5 loss: 0.568361  [  192/  265]
train() client id: f_00001-1-6 loss: 0.440556  [  224/  265]
train() client id: f_00001-1-7 loss: 0.435386  [  256/  265]
train() client id: f_00001-2-0 loss: 0.479862  [   32/  265]
train() client id: f_00001-2-1 loss: 0.483682  [   64/  265]
train() client id: f_00001-2-2 loss: 0.491613  [   96/  265]
train() client id: f_00001-2-3 loss: 0.564453  [  128/  265]
train() client id: f_00001-2-4 loss: 0.448629  [  160/  265]
train() client id: f_00001-2-5 loss: 0.477570  [  192/  265]
train() client id: f_00001-2-6 loss: 0.513886  [  224/  265]
train() client id: f_00001-2-7 loss: 0.500591  [  256/  265]
train() client id: f_00001-3-0 loss: 0.459338  [   32/  265]
train() client id: f_00001-3-1 loss: 0.438173  [   64/  265]
train() client id: f_00001-3-2 loss: 0.554945  [   96/  265]
train() client id: f_00001-3-3 loss: 0.425082  [  128/  265]
train() client id: f_00001-3-4 loss: 0.549317  [  160/  265]
train() client id: f_00001-3-5 loss: 0.568366  [  192/  265]
train() client id: f_00001-3-6 loss: 0.489819  [  224/  265]
train() client id: f_00001-3-7 loss: 0.517474  [  256/  265]
train() client id: f_00001-4-0 loss: 0.587514  [   32/  265]
train() client id: f_00001-4-1 loss: 0.372485  [   64/  265]
train() client id: f_00001-4-2 loss: 0.580068  [   96/  265]
train() client id: f_00001-4-3 loss: 0.447968  [  128/  265]
train() client id: f_00001-4-4 loss: 0.511217  [  160/  265]
train() client id: f_00001-4-5 loss: 0.500409  [  192/  265]
train() client id: f_00001-4-6 loss: 0.483669  [  224/  265]
train() client id: f_00001-4-7 loss: 0.435219  [  256/  265]
train() client id: f_00001-5-0 loss: 0.456044  [   32/  265]
train() client id: f_00001-5-1 loss: 0.401165  [   64/  265]
train() client id: f_00001-5-2 loss: 0.453019  [   96/  265]
train() client id: f_00001-5-3 loss: 0.454591  [  128/  265]
train() client id: f_00001-5-4 loss: 0.525119  [  160/  265]
train() client id: f_00001-5-5 loss: 0.546000  [  192/  265]
train() client id: f_00001-5-6 loss: 0.599073  [  224/  265]
train() client id: f_00001-5-7 loss: 0.500536  [  256/  265]
train() client id: f_00001-6-0 loss: 0.665988  [   32/  265]
train() client id: f_00001-6-1 loss: 0.470279  [   64/  265]
train() client id: f_00001-6-2 loss: 0.403874  [   96/  265]
train() client id: f_00001-6-3 loss: 0.471415  [  128/  265]
train() client id: f_00001-6-4 loss: 0.484641  [  160/  265]
train() client id: f_00001-6-5 loss: 0.425743  [  192/  265]
train() client id: f_00001-6-6 loss: 0.410779  [  224/  265]
train() client id: f_00001-6-7 loss: 0.588148  [  256/  265]
train() client id: f_00001-7-0 loss: 0.468437  [   32/  265]
train() client id: f_00001-7-1 loss: 0.482390  [   64/  265]
train() client id: f_00001-7-2 loss: 0.540025  [   96/  265]
train() client id: f_00001-7-3 loss: 0.559146  [  128/  265]
train() client id: f_00001-7-4 loss: 0.420331  [  160/  265]
train() client id: f_00001-7-5 loss: 0.428363  [  192/  265]
train() client id: f_00001-7-6 loss: 0.508617  [  224/  265]
train() client id: f_00001-7-7 loss: 0.392892  [  256/  265]
train() client id: f_00001-8-0 loss: 0.492573  [   32/  265]
train() client id: f_00001-8-1 loss: 0.445285  [   64/  265]
train() client id: f_00001-8-2 loss: 0.513376  [   96/  265]
train() client id: f_00001-8-3 loss: 0.509029  [  128/  265]
train() client id: f_00001-8-4 loss: 0.469669  [  160/  265]
train() client id: f_00001-8-5 loss: 0.517967  [  192/  265]
train() client id: f_00001-8-6 loss: 0.466914  [  224/  265]
train() client id: f_00001-8-7 loss: 0.443812  [  256/  265]
train() client id: f_00001-9-0 loss: 0.408240  [   32/  265]
train() client id: f_00001-9-1 loss: 0.552144  [   64/  265]
train() client id: f_00001-9-2 loss: 0.499248  [   96/  265]
train() client id: f_00001-9-3 loss: 0.601801  [  128/  265]
train() client id: f_00001-9-4 loss: 0.501639  [  160/  265]
train() client id: f_00001-9-5 loss: 0.415703  [  192/  265]
train() client id: f_00001-9-6 loss: 0.405768  [  224/  265]
train() client id: f_00001-9-7 loss: 0.484116  [  256/  265]
train() client id: f_00001-10-0 loss: 0.475030  [   32/  265]
train() client id: f_00001-10-1 loss: 0.471233  [   64/  265]
train() client id: f_00001-10-2 loss: 0.535691  [   96/  265]
train() client id: f_00001-10-3 loss: 0.436781  [  128/  265]
train() client id: f_00001-10-4 loss: 0.441003  [  160/  265]
train() client id: f_00001-10-5 loss: 0.604102  [  192/  265]
train() client id: f_00001-10-6 loss: 0.453457  [  224/  265]
train() client id: f_00001-10-7 loss: 0.412346  [  256/  265]
train() client id: f_00002-0-0 loss: 1.139116  [   32/  124]
train() client id: f_00002-0-1 loss: 1.173987  [   64/  124]
train() client id: f_00002-0-2 loss: 1.112943  [   96/  124]
train() client id: f_00002-1-0 loss: 0.952606  [   32/  124]
train() client id: f_00002-1-1 loss: 1.176206  [   64/  124]
train() client id: f_00002-1-2 loss: 1.094662  [   96/  124]
train() client id: f_00002-2-0 loss: 1.054037  [   32/  124]
train() client id: f_00002-2-1 loss: 1.211015  [   64/  124]
train() client id: f_00002-2-2 loss: 1.046985  [   96/  124]
train() client id: f_00002-3-0 loss: 0.967050  [   32/  124]
train() client id: f_00002-3-1 loss: 1.207223  [   64/  124]
train() client id: f_00002-3-2 loss: 1.030353  [   96/  124]
train() client id: f_00002-4-0 loss: 1.143778  [   32/  124]
train() client id: f_00002-4-1 loss: 0.906973  [   64/  124]
train() client id: f_00002-4-2 loss: 1.041977  [   96/  124]
train() client id: f_00002-5-0 loss: 1.198001  [   32/  124]
train() client id: f_00002-5-1 loss: 0.906456  [   64/  124]
train() client id: f_00002-5-2 loss: 0.839556  [   96/  124]
train() client id: f_00002-6-0 loss: 1.060274  [   32/  124]
train() client id: f_00002-6-1 loss: 0.949613  [   64/  124]
train() client id: f_00002-6-2 loss: 0.857507  [   96/  124]
train() client id: f_00002-7-0 loss: 1.002639  [   32/  124]
train() client id: f_00002-7-1 loss: 0.888044  [   64/  124]
train() client id: f_00002-7-2 loss: 0.964009  [   96/  124]
train() client id: f_00002-8-0 loss: 0.947971  [   32/  124]
train() client id: f_00002-8-1 loss: 0.950333  [   64/  124]
train() client id: f_00002-8-2 loss: 1.050748  [   96/  124]
train() client id: f_00002-9-0 loss: 0.912577  [   32/  124]
train() client id: f_00002-9-1 loss: 0.973831  [   64/  124]
train() client id: f_00002-9-2 loss: 0.986912  [   96/  124]
train() client id: f_00002-10-0 loss: 0.963265  [   32/  124]
train() client id: f_00002-10-1 loss: 0.946755  [   64/  124]
train() client id: f_00002-10-2 loss: 0.853623  [   96/  124]
train() client id: f_00003-0-0 loss: 0.857543  [   32/   43]
train() client id: f_00003-1-0 loss: 0.832562  [   32/   43]
train() client id: f_00003-2-0 loss: 0.961400  [   32/   43]
train() client id: f_00003-3-0 loss: 0.924692  [   32/   43]
train() client id: f_00003-4-0 loss: 0.883750  [   32/   43]
train() client id: f_00003-5-0 loss: 0.742362  [   32/   43]
train() client id: f_00003-6-0 loss: 0.741800  [   32/   43]
train() client id: f_00003-7-0 loss: 1.002052  [   32/   43]
train() client id: f_00003-8-0 loss: 0.917225  [   32/   43]
train() client id: f_00003-9-0 loss: 0.736499  [   32/   43]
train() client id: f_00003-10-0 loss: 0.808525  [   32/   43]
train() client id: f_00004-0-0 loss: 0.965036  [   32/  306]
train() client id: f_00004-0-1 loss: 1.024767  [   64/  306]
train() client id: f_00004-0-2 loss: 0.972090  [   96/  306]
train() client id: f_00004-0-3 loss: 0.860739  [  128/  306]
train() client id: f_00004-0-4 loss: 0.855820  [  160/  306]
train() client id: f_00004-0-5 loss: 0.889110  [  192/  306]
train() client id: f_00004-0-6 loss: 0.987738  [  224/  306]
train() client id: f_00004-0-7 loss: 0.971438  [  256/  306]
train() client id: f_00004-0-8 loss: 1.077822  [  288/  306]
train() client id: f_00004-1-0 loss: 1.035412  [   32/  306]
train() client id: f_00004-1-1 loss: 0.971620  [   64/  306]
train() client id: f_00004-1-2 loss: 0.887598  [   96/  306]
train() client id: f_00004-1-3 loss: 0.911180  [  128/  306]
train() client id: f_00004-1-4 loss: 0.898213  [  160/  306]
train() client id: f_00004-1-5 loss: 0.945138  [  192/  306]
train() client id: f_00004-1-6 loss: 0.894401  [  224/  306]
train() client id: f_00004-1-7 loss: 0.922965  [  256/  306]
train() client id: f_00004-1-8 loss: 0.979520  [  288/  306]
train() client id: f_00004-2-0 loss: 1.011248  [   32/  306]
train() client id: f_00004-2-1 loss: 0.897438  [   64/  306]
train() client id: f_00004-2-2 loss: 0.950686  [   96/  306]
train() client id: f_00004-2-3 loss: 0.961960  [  128/  306]
train() client id: f_00004-2-4 loss: 0.953578  [  160/  306]
train() client id: f_00004-2-5 loss: 0.889529  [  192/  306]
train() client id: f_00004-2-6 loss: 0.952729  [  224/  306]
train() client id: f_00004-2-7 loss: 1.071600  [  256/  306]
train() client id: f_00004-2-8 loss: 0.922462  [  288/  306]
train() client id: f_00004-3-0 loss: 0.987749  [   32/  306]
train() client id: f_00004-3-1 loss: 0.890338  [   64/  306]
train() client id: f_00004-3-2 loss: 0.866487  [   96/  306]
train() client id: f_00004-3-3 loss: 0.920623  [  128/  306]
train() client id: f_00004-3-4 loss: 0.925637  [  160/  306]
train() client id: f_00004-3-5 loss: 1.008206  [  192/  306]
train() client id: f_00004-3-6 loss: 0.967939  [  224/  306]
train() client id: f_00004-3-7 loss: 0.995269  [  256/  306]
train() client id: f_00004-3-8 loss: 1.027856  [  288/  306]
train() client id: f_00004-4-0 loss: 0.985963  [   32/  306]
train() client id: f_00004-4-1 loss: 0.824567  [   64/  306]
train() client id: f_00004-4-2 loss: 0.948034  [   96/  306]
train() client id: f_00004-4-3 loss: 1.028501  [  128/  306]
train() client id: f_00004-4-4 loss: 0.948767  [  160/  306]
train() client id: f_00004-4-5 loss: 1.005118  [  192/  306]
train() client id: f_00004-4-6 loss: 0.940430  [  224/  306]
train() client id: f_00004-4-7 loss: 0.948966  [  256/  306]
train() client id: f_00004-4-8 loss: 0.825766  [  288/  306]
train() client id: f_00004-5-0 loss: 0.802634  [   32/  306]
train() client id: f_00004-5-1 loss: 1.028567  [   64/  306]
train() client id: f_00004-5-2 loss: 1.046971  [   96/  306]
train() client id: f_00004-5-3 loss: 0.867311  [  128/  306]
train() client id: f_00004-5-4 loss: 1.027902  [  160/  306]
train() client id: f_00004-5-5 loss: 0.940881  [  192/  306]
train() client id: f_00004-5-6 loss: 0.974895  [  224/  306]
train() client id: f_00004-5-7 loss: 0.939536  [  256/  306]
train() client id: f_00004-5-8 loss: 0.901054  [  288/  306]
train() client id: f_00004-6-0 loss: 1.040020  [   32/  306]
train() client id: f_00004-6-1 loss: 0.717042  [   64/  306]
train() client id: f_00004-6-2 loss: 0.936081  [   96/  306]
train() client id: f_00004-6-3 loss: 0.968618  [  128/  306]
train() client id: f_00004-6-4 loss: 0.905177  [  160/  306]
train() client id: f_00004-6-5 loss: 0.962816  [  192/  306]
train() client id: f_00004-6-6 loss: 1.086066  [  224/  306]
train() client id: f_00004-6-7 loss: 0.923703  [  256/  306]
train() client id: f_00004-6-8 loss: 0.995988  [  288/  306]
train() client id: f_00004-7-0 loss: 0.945901  [   32/  306]
train() client id: f_00004-7-1 loss: 0.901695  [   64/  306]
train() client id: f_00004-7-2 loss: 0.946526  [   96/  306]
train() client id: f_00004-7-3 loss: 0.977566  [  128/  306]
train() client id: f_00004-7-4 loss: 1.040370  [  160/  306]
train() client id: f_00004-7-5 loss: 0.945232  [  192/  306]
train() client id: f_00004-7-6 loss: 0.872822  [  224/  306]
train() client id: f_00004-7-7 loss: 0.981255  [  256/  306]
train() client id: f_00004-7-8 loss: 0.958603  [  288/  306]
train() client id: f_00004-8-0 loss: 0.918412  [   32/  306]
train() client id: f_00004-8-1 loss: 0.904737  [   64/  306]
train() client id: f_00004-8-2 loss: 0.985336  [   96/  306]
train() client id: f_00004-8-3 loss: 0.887657  [  128/  306]
train() client id: f_00004-8-4 loss: 0.955916  [  160/  306]
train() client id: f_00004-8-5 loss: 0.840847  [  192/  306]
train() client id: f_00004-8-6 loss: 0.960905  [  224/  306]
train() client id: f_00004-8-7 loss: 0.944910  [  256/  306]
train() client id: f_00004-8-8 loss: 1.015277  [  288/  306]
train() client id: f_00004-9-0 loss: 0.991904  [   32/  306]
train() client id: f_00004-9-1 loss: 0.878264  [   64/  306]
train() client id: f_00004-9-2 loss: 0.982028  [   96/  306]
train() client id: f_00004-9-3 loss: 0.950392  [  128/  306]
train() client id: f_00004-9-4 loss: 0.917093  [  160/  306]
train() client id: f_00004-9-5 loss: 0.912611  [  192/  306]
train() client id: f_00004-9-6 loss: 0.859793  [  224/  306]
train() client id: f_00004-9-7 loss: 1.041309  [  256/  306]
train() client id: f_00004-9-8 loss: 0.940694  [  288/  306]
train() client id: f_00004-10-0 loss: 1.019209  [   32/  306]
train() client id: f_00004-10-1 loss: 0.856819  [   64/  306]
train() client id: f_00004-10-2 loss: 0.988647  [   96/  306]
train() client id: f_00004-10-3 loss: 0.891690  [  128/  306]
train() client id: f_00004-10-4 loss: 0.932413  [  160/  306]
train() client id: f_00004-10-5 loss: 0.939164  [  192/  306]
train() client id: f_00004-10-6 loss: 0.951936  [  224/  306]
train() client id: f_00004-10-7 loss: 0.861284  [  256/  306]
train() client id: f_00004-10-8 loss: 1.053553  [  288/  306]
train() client id: f_00005-0-0 loss: 0.631661  [   32/  146]
train() client id: f_00005-0-1 loss: 0.646141  [   64/  146]
train() client id: f_00005-0-2 loss: 0.874722  [   96/  146]
train() client id: f_00005-0-3 loss: 0.909042  [  128/  146]
train() client id: f_00005-1-0 loss: 0.784875  [   32/  146]
train() client id: f_00005-1-1 loss: 0.621730  [   64/  146]
train() client id: f_00005-1-2 loss: 0.793892  [   96/  146]
train() client id: f_00005-1-3 loss: 0.924250  [  128/  146]
train() client id: f_00005-2-0 loss: 1.016079  [   32/  146]
train() client id: f_00005-2-1 loss: 0.714532  [   64/  146]
train() client id: f_00005-2-2 loss: 0.643643  [   96/  146]
train() client id: f_00005-2-3 loss: 0.800874  [  128/  146]
train() client id: f_00005-3-0 loss: 0.965339  [   32/  146]
train() client id: f_00005-3-1 loss: 0.587696  [   64/  146]
train() client id: f_00005-3-2 loss: 0.665910  [   96/  146]
train() client id: f_00005-3-3 loss: 0.661920  [  128/  146]
train() client id: f_00005-4-0 loss: 0.838512  [   32/  146]
train() client id: f_00005-4-1 loss: 0.542980  [   64/  146]
train() client id: f_00005-4-2 loss: 0.731215  [   96/  146]
train() client id: f_00005-4-3 loss: 0.890678  [  128/  146]
train() client id: f_00005-5-0 loss: 0.672130  [   32/  146]
train() client id: f_00005-5-1 loss: 0.649327  [   64/  146]
train() client id: f_00005-5-2 loss: 0.802154  [   96/  146]
train() client id: f_00005-5-3 loss: 0.856497  [  128/  146]
train() client id: f_00005-6-0 loss: 0.870467  [   32/  146]
train() client id: f_00005-6-1 loss: 0.987319  [   64/  146]
train() client id: f_00005-6-2 loss: 0.575013  [   96/  146]
train() client id: f_00005-6-3 loss: 0.597108  [  128/  146]
train() client id: f_00005-7-0 loss: 0.558722  [   32/  146]
train() client id: f_00005-7-1 loss: 0.760843  [   64/  146]
train() client id: f_00005-7-2 loss: 0.651094  [   96/  146]
train() client id: f_00005-7-3 loss: 0.776934  [  128/  146]
train() client id: f_00005-8-0 loss: 0.789858  [   32/  146]
train() client id: f_00005-8-1 loss: 0.803547  [   64/  146]
train() client id: f_00005-8-2 loss: 0.746731  [   96/  146]
train() client id: f_00005-8-3 loss: 0.747097  [  128/  146]
train() client id: f_00005-9-0 loss: 0.793134  [   32/  146]
train() client id: f_00005-9-1 loss: 0.814523  [   64/  146]
train() client id: f_00005-9-2 loss: 0.822437  [   96/  146]
train() client id: f_00005-9-3 loss: 0.788331  [  128/  146]
train() client id: f_00005-10-0 loss: 0.820401  [   32/  146]
train() client id: f_00005-10-1 loss: 0.731347  [   64/  146]
train() client id: f_00005-10-2 loss: 0.672449  [   96/  146]
train() client id: f_00005-10-3 loss: 0.767488  [  128/  146]
train() client id: f_00006-0-0 loss: 0.513817  [   32/   54]
train() client id: f_00006-1-0 loss: 0.515279  [   32/   54]
train() client id: f_00006-2-0 loss: 0.571573  [   32/   54]
train() client id: f_00006-3-0 loss: 0.527737  [   32/   54]
train() client id: f_00006-4-0 loss: 0.522630  [   32/   54]
train() client id: f_00006-5-0 loss: 0.526224  [   32/   54]
train() client id: f_00006-6-0 loss: 0.511363  [   32/   54]
train() client id: f_00006-7-0 loss: 0.565025  [   32/   54]
train() client id: f_00006-8-0 loss: 0.511694  [   32/   54]
train() client id: f_00006-9-0 loss: 0.475009  [   32/   54]
train() client id: f_00006-10-0 loss: 0.474554  [   32/   54]
train() client id: f_00007-0-0 loss: 0.578842  [   32/  179]
train() client id: f_00007-0-1 loss: 0.847883  [   64/  179]
train() client id: f_00007-0-2 loss: 0.751333  [   96/  179]
train() client id: f_00007-0-3 loss: 0.841947  [  128/  179]
train() client id: f_00007-0-4 loss: 0.644198  [  160/  179]
train() client id: f_00007-1-0 loss: 0.746581  [   32/  179]
train() client id: f_00007-1-1 loss: 0.865768  [   64/  179]
train() client id: f_00007-1-2 loss: 0.646756  [   96/  179]
train() client id: f_00007-1-3 loss: 0.749555  [  128/  179]
train() client id: f_00007-1-4 loss: 0.623220  [  160/  179]
train() client id: f_00007-2-0 loss: 0.592979  [   32/  179]
train() client id: f_00007-2-1 loss: 0.817058  [   64/  179]
train() client id: f_00007-2-2 loss: 0.669015  [   96/  179]
train() client id: f_00007-2-3 loss: 0.661459  [  128/  179]
train() client id: f_00007-2-4 loss: 0.774889  [  160/  179]
train() client id: f_00007-3-0 loss: 0.558896  [   32/  179]
train() client id: f_00007-3-1 loss: 0.917054  [   64/  179]
train() client id: f_00007-3-2 loss: 0.776705  [   96/  179]
train() client id: f_00007-3-3 loss: 0.666888  [  128/  179]
train() client id: f_00007-3-4 loss: 0.593501  [  160/  179]
train() client id: f_00007-4-0 loss: 0.688972  [   32/  179]
train() client id: f_00007-4-1 loss: 0.694888  [   64/  179]
train() client id: f_00007-4-2 loss: 0.636041  [   96/  179]
train() client id: f_00007-4-3 loss: 0.702546  [  128/  179]
train() client id: f_00007-4-4 loss: 0.701684  [  160/  179]
train() client id: f_00007-5-0 loss: 0.554538  [   32/  179]
train() client id: f_00007-5-1 loss: 0.873660  [   64/  179]
train() client id: f_00007-5-2 loss: 0.726722  [   96/  179]
train() client id: f_00007-5-3 loss: 0.653646  [  128/  179]
train() client id: f_00007-5-4 loss: 0.678918  [  160/  179]
train() client id: f_00007-6-0 loss: 0.635699  [   32/  179]
train() client id: f_00007-6-1 loss: 0.566772  [   64/  179]
train() client id: f_00007-6-2 loss: 0.743992  [   96/  179]
train() client id: f_00007-6-3 loss: 0.855860  [  128/  179]
train() client id: f_00007-6-4 loss: 0.737233  [  160/  179]
train() client id: f_00007-7-0 loss: 0.773677  [   32/  179]
train() client id: f_00007-7-1 loss: 0.573527  [   64/  179]
train() client id: f_00007-7-2 loss: 0.746137  [   96/  179]
train() client id: f_00007-7-3 loss: 0.726554  [  128/  179]
train() client id: f_00007-7-4 loss: 0.650733  [  160/  179]
train() client id: f_00007-8-0 loss: 0.573777  [   32/  179]
train() client id: f_00007-8-1 loss: 0.770506  [   64/  179]
train() client id: f_00007-8-2 loss: 0.744500  [   96/  179]
train() client id: f_00007-8-3 loss: 0.694158  [  128/  179]
train() client id: f_00007-8-4 loss: 0.776308  [  160/  179]
train() client id: f_00007-9-0 loss: 0.692021  [   32/  179]
train() client id: f_00007-9-1 loss: 0.798069  [   64/  179]
train() client id: f_00007-9-2 loss: 0.594404  [   96/  179]
train() client id: f_00007-9-3 loss: 0.633279  [  128/  179]
train() client id: f_00007-9-4 loss: 0.736196  [  160/  179]
train() client id: f_00007-10-0 loss: 0.555458  [   32/  179]
train() client id: f_00007-10-1 loss: 0.862735  [   64/  179]
train() client id: f_00007-10-2 loss: 0.673220  [   96/  179]
train() client id: f_00007-10-3 loss: 0.743496  [  128/  179]
train() client id: f_00007-10-4 loss: 0.637701  [  160/  179]
train() client id: f_00008-0-0 loss: 0.859453  [   32/  130]
train() client id: f_00008-0-1 loss: 0.761373  [   64/  130]
train() client id: f_00008-0-2 loss: 0.686002  [   96/  130]
train() client id: f_00008-0-3 loss: 0.911268  [  128/  130]
train() client id: f_00008-1-0 loss: 0.849558  [   32/  130]
train() client id: f_00008-1-1 loss: 0.703852  [   64/  130]
train() client id: f_00008-1-2 loss: 0.803565  [   96/  130]
train() client id: f_00008-1-3 loss: 0.860454  [  128/  130]
train() client id: f_00008-2-0 loss: 0.868675  [   32/  130]
train() client id: f_00008-2-1 loss: 0.720082  [   64/  130]
train() client id: f_00008-2-2 loss: 0.722163  [   96/  130]
train() client id: f_00008-2-3 loss: 0.873551  [  128/  130]
train() client id: f_00008-3-0 loss: 0.746833  [   32/  130]
train() client id: f_00008-3-1 loss: 0.835592  [   64/  130]
train() client id: f_00008-3-2 loss: 0.832949  [   96/  130]
train() client id: f_00008-3-3 loss: 0.793150  [  128/  130]
train() client id: f_00008-4-0 loss: 0.764005  [   32/  130]
train() client id: f_00008-4-1 loss: 0.816781  [   64/  130]
train() client id: f_00008-4-2 loss: 0.819514  [   96/  130]
train() client id: f_00008-4-3 loss: 0.809803  [  128/  130]
train() client id: f_00008-5-0 loss: 0.783023  [   32/  130]
train() client id: f_00008-5-1 loss: 0.717688  [   64/  130]
train() client id: f_00008-5-2 loss: 0.785519  [   96/  130]
train() client id: f_00008-5-3 loss: 0.921928  [  128/  130]
train() client id: f_00008-6-0 loss: 0.784475  [   32/  130]
train() client id: f_00008-6-1 loss: 0.744417  [   64/  130]
train() client id: f_00008-6-2 loss: 0.925953  [   96/  130]
train() client id: f_00008-6-3 loss: 0.752815  [  128/  130]
train() client id: f_00008-7-0 loss: 0.859341  [   32/  130]
train() client id: f_00008-7-1 loss: 0.679836  [   64/  130]
train() client id: f_00008-7-2 loss: 0.893661  [   96/  130]
train() client id: f_00008-7-3 loss: 0.758491  [  128/  130]
train() client id: f_00008-8-0 loss: 0.771269  [   32/  130]
train() client id: f_00008-8-1 loss: 0.791397  [   64/  130]
train() client id: f_00008-8-2 loss: 0.826628  [   96/  130]
train() client id: f_00008-8-3 loss: 0.815706  [  128/  130]
train() client id: f_00008-9-0 loss: 0.853575  [   32/  130]
train() client id: f_00008-9-1 loss: 0.760150  [   64/  130]
train() client id: f_00008-9-2 loss: 0.662269  [   96/  130]
train() client id: f_00008-9-3 loss: 0.903530  [  128/  130]
train() client id: f_00008-10-0 loss: 0.831731  [   32/  130]
train() client id: f_00008-10-1 loss: 0.810174  [   64/  130]
train() client id: f_00008-10-2 loss: 0.754754  [   96/  130]
train() client id: f_00008-10-3 loss: 0.791570  [  128/  130]
train() client id: f_00009-0-0 loss: 1.033077  [   32/  118]
train() client id: f_00009-0-1 loss: 1.194912  [   64/  118]
train() client id: f_00009-0-2 loss: 1.041394  [   96/  118]
train() client id: f_00009-1-0 loss: 0.935162  [   32/  118]
train() client id: f_00009-1-1 loss: 1.156939  [   64/  118]
train() client id: f_00009-1-2 loss: 1.129575  [   96/  118]
train() client id: f_00009-2-0 loss: 1.091985  [   32/  118]
train() client id: f_00009-2-1 loss: 1.009708  [   64/  118]
train() client id: f_00009-2-2 loss: 0.911289  [   96/  118]
train() client id: f_00009-3-0 loss: 1.104617  [   32/  118]
train() client id: f_00009-3-1 loss: 0.932689  [   64/  118]
train() client id: f_00009-3-2 loss: 0.966577  [   96/  118]
train() client id: f_00009-4-0 loss: 0.875709  [   32/  118]
train() client id: f_00009-4-1 loss: 0.914437  [   64/  118]
train() client id: f_00009-4-2 loss: 0.978808  [   96/  118]
train() client id: f_00009-5-0 loss: 0.951780  [   32/  118]
train() client id: f_00009-5-1 loss: 0.912942  [   64/  118]
train() client id: f_00009-5-2 loss: 0.809403  [   96/  118]
train() client id: f_00009-6-0 loss: 0.945561  [   32/  118]
train() client id: f_00009-6-1 loss: 0.789060  [   64/  118]
train() client id: f_00009-6-2 loss: 0.882501  [   96/  118]
train() client id: f_00009-7-0 loss: 0.837308  [   32/  118]
train() client id: f_00009-7-1 loss: 0.979150  [   64/  118]
train() client id: f_00009-7-2 loss: 0.915825  [   96/  118]
train() client id: f_00009-8-0 loss: 0.865579  [   32/  118]
train() client id: f_00009-8-1 loss: 0.954496  [   64/  118]
train() client id: f_00009-8-2 loss: 0.896364  [   96/  118]
train() client id: f_00009-9-0 loss: 1.014130  [   32/  118]
train() client id: f_00009-9-1 loss: 0.720204  [   64/  118]
train() client id: f_00009-9-2 loss: 0.975346  [   96/  118]
train() client id: f_00009-10-0 loss: 0.861108  [   32/  118]
train() client id: f_00009-10-1 loss: 0.873928  [   64/  118]
train() client id: f_00009-10-2 loss: 0.967726  [   96/  118]
At round 17 accuracy: 0.6419098143236074
At round 17 training accuracy: 0.5747820254862508
At round 17 training loss: 0.8425220475830063
update_location
xs = -4.528292 -13.998411 0.045120 4.056472 -105.103519 -40.217951 -37.215960 58.375741 -1.680116 4.695607 
ys = 72.587959 15.555839 -3.679386 22.544824 9.350187 -17.185849 -3.124922 -19.177652 -2.154970 -0.998518 
xs mean: -13.557130840587131
ys mean: 7.371751218646876
dists_uav = 123.650788 102.166235 100.067677 102.590078 145.375980 109.145943 106.746395 117.369116 100.037326 100.115163 
uav_gains = -102.305248 -100.232706 -100.007362 -100.277657 -104.065318 -100.950246 -100.708871 -101.739002 -100.004068 -100.012513 
uav_gains_db_mean: -101.03029910547137
dists_bs = 198.868875 226.592039 250.134198 235.167601 179.792599 234.737303 225.195314 303.593803 247.837983 251.527314 
bs_gains = -103.926930 -105.513909 -106.715905 -105.965629 -102.700670 -105.943358 -105.438721 -109.071260 -106.603759 -106.783443 
bs_gains_db_mean: -105.86635855113167
Round 18
-------------------------------
ene_coms = [0.00696832 0.00816313 0.00630434 0.00637589 0.0070967  0.00835392
 0.00649341 0.00679223 0.00866472 0.00875318]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.60602304 17.95558235  8.45711316  3.02588412 20.6818117   9.97827478
  3.76628801 12.15691524  8.9120371   8.10914207]
obj_prev = 101.6490715631533
eta_min = 1.43817500293789e-11	eta_max = 0.9327566251486075
af = 21.498465320302763	bf = 1.6284893099645543	zeta = 23.64831185233304	eta = 0.9090909090909091
af = 21.498465320302763	bf = 1.6284893099645543	zeta = 40.45263095675619	eta = 0.5314478888476895
af = 21.498465320302763	bf = 1.6284893099645543	zeta = 32.48037224869525	eta = 0.6618909769781461
af = 21.498465320302763	bf = 1.6284893099645543	zeta = 31.05485336864997	eta = 0.692273927849409
af = 21.498465320302763	bf = 1.6284893099645543	zeta = 30.985532688207797	eta = 0.6938226796560596
af = 21.498465320302763	bf = 1.6284893099645543	zeta = 30.985357494701887	eta = 0.6938266025808718
eta = 0.6938266025808718
ene_coms = [0.00696832 0.00816313 0.00630434 0.00637589 0.0070967  0.00835392
 0.00649341 0.00679223 0.00866472 0.00875318]
ene_comp = [0.03016292 0.0634379  0.02968415 0.0102937  0.07325281 0.03495069
 0.01292697 0.0428505  0.03112048 0.02824782]
ene_total = [2.6700891  5.14879454 2.58791462 1.19870152 5.77789373 3.11401304
 1.3965094  3.56978435 2.86093392 2.66072329]
ti_comp = [0.33499108 0.32304291 0.3416308  0.34091536 0.33370726 0.32113504
 0.33974019 0.33675194 0.31802703 0.31714246]
ti_coms = [0.06968316 0.08163133 0.06304344 0.06375888 0.07096698 0.0835392
 0.06493405 0.0679223  0.08664721 0.08753178]
t_total = [29.09992447 29.09992447 29.09992447 29.09992447 29.09992447 29.09992447
 29.09992447 29.09992447 29.09992447 29.09992447]
ene_coms = [0.00696832 0.00816313 0.00630434 0.00637589 0.0070967  0.00835392
 0.00649341 0.00679223 0.00866472 0.00875318]
ene_comp = [1.52838878e-05 1.52899438e-04 1.40068255e-05 5.86544417e-07
 2.20608204e-04 2.58744938e-05 1.16970441e-06 4.33638234e-05
 1.86247600e-05 1.40063808e-05]
ene_total = [0.50218723 0.5980018  0.45434951 0.4585291  0.52618387 0.60258687
 0.46702169 0.49154416 0.62441511 0.63044389]
optimize_network iter = 0 obj = 5.355263225786993
eta = 0.6938266025808718
freqs = [4.50204881e+07 9.81880326e+07 4.34447764e+07 1.50971432e+07
 1.09756100e+08 5.44174333e+07 1.90247840e+07 6.36232454e+07
 4.89274088e+07 4.45349031e+07]
eta_min = 0.693826602580872	eta_max = 0.6938266025808716
af = 0.030502908876528925	bf = 1.6284893099645543	zeta = 0.03355319976418182	eta = 0.909090909090909
af = 0.030502908876528925	bf = 1.6284893099645543	zeta = 17.945362024635415	eta = 0.0016997655903878948
af = 0.030502908876528925	bf = 1.6284893099645543	zeta = 1.8261184141108466	eta = 0.01670368615793246
af = 0.030502908876528925	bf = 1.6284893099645543	zeta = 1.7830953166166936	eta = 0.01710671807181132
af = 0.030502908876528925	bf = 1.6284893099645543	zeta = 1.7830865659817114	eta = 0.01710680202435095
eta = 0.01710680202435095
ene_coms = [0.00696832 0.00816313 0.00630434 0.00637589 0.0070967  0.00835392
 0.00649341 0.00679223 0.00866472 0.00875318]
ene_comp = [1.70105264e-04 1.70172665e-03 1.55891928e-04 6.52807020e-06
 2.45530569e-03 2.87975654e-04 1.30184728e-05 4.82626848e-04
 2.07288207e-04 1.55886979e-04]
ene_total = [0.15990042 0.22097256 0.14470909 0.14296591 0.21396459 0.1935782
 0.1457437  0.16295657 0.19873274 0.19956278]
ti_comp = [0.33499108 0.32304291 0.3416308  0.34091536 0.33370726 0.32113504
 0.33974019 0.33675194 0.31802703 0.31714246]
ti_coms = [0.06968316 0.08163133 0.06304344 0.06375888 0.07096698 0.0835392
 0.06493405 0.0679223  0.08664721 0.08753178]
t_total = [29.09992447 29.09992447 29.09992447 29.09992447 29.09992447 29.09992447
 29.09992447 29.09992447 29.09992447 29.09992447]
ene_coms = [0.00696832 0.00816313 0.00630434 0.00637589 0.0070967  0.00835392
 0.00649341 0.00679223 0.00866472 0.00875318]
ene_comp = [1.52838878e-05 1.52899438e-04 1.40068255e-05 5.86544417e-07
 2.20608204e-04 2.58744938e-05 1.16970441e-06 4.33638234e-05
 1.86247600e-05 1.40063808e-05]
ene_total = [0.50218723 0.5980018  0.45434951 0.4585291  0.52618387 0.60258687
 0.46702169 0.49154416 0.62441511 0.63044389]
optimize_network iter = 1 obj = 5.355263225786995
eta = 0.693826602580872
freqs = [4.50204881e+07 9.81880326e+07 4.34447764e+07 1.50971432e+07
 1.09756100e+08 5.44174333e+07 1.90247840e+07 6.36232454e+07
 4.89274088e+07 4.45349031e+07]
Done!
ene_coms = [0.00696832 0.00816313 0.00630434 0.00637589 0.0070967  0.00835392
 0.00649341 0.00679223 0.00866472 0.00875318]
ene_comp = [1.40460313e-05 1.40515968e-04 1.28723994e-05 5.39039630e-07
 2.02740937e-04 2.37788941e-05 1.07496895e-06 3.98517463e-05
 1.71163230e-05 1.28719908e-05]
ene_total = [0.00698236 0.00830365 0.00631722 0.00637643 0.00729944 0.0083777
 0.00649448 0.00683208 0.00868184 0.00876605]
At round 18 energy consumption: 0.07443124193484181
At round 18 eta: 0.693826602580872
At round 18 a_n: 22.016777610806663
At round 18 local rounds: 11.969414161012498
At round 18 global rounds: 71.90950551679504
gradient difference: 0.4788111746311188
train() client id: f_00000-0-0 loss: 1.133738  [   32/  126]
train() client id: f_00000-0-1 loss: 1.161988  [   64/  126]
train() client id: f_00000-0-2 loss: 0.818495  [   96/  126]
train() client id: f_00000-1-0 loss: 1.104963  [   32/  126]
train() client id: f_00000-1-1 loss: 0.937508  [   64/  126]
train() client id: f_00000-1-2 loss: 0.853208  [   96/  126]
train() client id: f_00000-2-0 loss: 0.856243  [   32/  126]
train() client id: f_00000-2-1 loss: 0.847717  [   64/  126]
train() client id: f_00000-2-2 loss: 1.038680  [   96/  126]
train() client id: f_00000-3-0 loss: 0.866343  [   32/  126]
train() client id: f_00000-3-1 loss: 0.808930  [   64/  126]
train() client id: f_00000-3-2 loss: 0.779194  [   96/  126]
train() client id: f_00000-4-0 loss: 0.814969  [   32/  126]
train() client id: f_00000-4-1 loss: 0.900172  [   64/  126]
train() client id: f_00000-4-2 loss: 0.759577  [   96/  126]
train() client id: f_00000-5-0 loss: 0.785350  [   32/  126]
train() client id: f_00000-5-1 loss: 0.854409  [   64/  126]
train() client id: f_00000-5-2 loss: 0.696162  [   96/  126]
train() client id: f_00000-6-0 loss: 0.761773  [   32/  126]
train() client id: f_00000-6-1 loss: 0.762724  [   64/  126]
train() client id: f_00000-6-2 loss: 0.831622  [   96/  126]
train() client id: f_00000-7-0 loss: 0.748993  [   32/  126]
train() client id: f_00000-7-1 loss: 0.865919  [   64/  126]
train() client id: f_00000-7-2 loss: 0.758547  [   96/  126]
train() client id: f_00000-8-0 loss: 0.761794  [   32/  126]
train() client id: f_00000-8-1 loss: 0.894822  [   64/  126]
train() client id: f_00000-8-2 loss: 0.677429  [   96/  126]
train() client id: f_00000-9-0 loss: 0.814380  [   32/  126]
train() client id: f_00000-9-1 loss: 0.952562  [   64/  126]
train() client id: f_00000-9-2 loss: 0.682824  [   96/  126]
train() client id: f_00000-10-0 loss: 0.741027  [   32/  126]
train() client id: f_00000-10-1 loss: 0.901544  [   64/  126]
train() client id: f_00000-10-2 loss: 0.666004  [   96/  126]
train() client id: f_00001-0-0 loss: 0.497777  [   32/  265]
train() client id: f_00001-0-1 loss: 0.520449  [   64/  265]
train() client id: f_00001-0-2 loss: 0.592942  [   96/  265]
train() client id: f_00001-0-3 loss: 0.501478  [  128/  265]
train() client id: f_00001-0-4 loss: 0.666653  [  160/  265]
train() client id: f_00001-0-5 loss: 0.664210  [  192/  265]
train() client id: f_00001-0-6 loss: 0.570398  [  224/  265]
train() client id: f_00001-0-7 loss: 0.534603  [  256/  265]
train() client id: f_00001-1-0 loss: 0.534127  [   32/  265]
train() client id: f_00001-1-1 loss: 0.696189  [   64/  265]
train() client id: f_00001-1-2 loss: 0.584210  [   96/  265]
train() client id: f_00001-1-3 loss: 0.483777  [  128/  265]
train() client id: f_00001-1-4 loss: 0.535916  [  160/  265]
train() client id: f_00001-1-5 loss: 0.499110  [  192/  265]
train() client id: f_00001-1-6 loss: 0.569074  [  224/  265]
train() client id: f_00001-1-7 loss: 0.597439  [  256/  265]
train() client id: f_00001-2-0 loss: 0.655394  [   32/  265]
train() client id: f_00001-2-1 loss: 0.609809  [   64/  265]
train() client id: f_00001-2-2 loss: 0.455687  [   96/  265]
train() client id: f_00001-2-3 loss: 0.485650  [  128/  265]
train() client id: f_00001-2-4 loss: 0.663564  [  160/  265]
train() client id: f_00001-2-5 loss: 0.467886  [  192/  265]
train() client id: f_00001-2-6 loss: 0.555987  [  224/  265]
train() client id: f_00001-2-7 loss: 0.558339  [  256/  265]
train() client id: f_00001-3-0 loss: 0.656930  [   32/  265]
train() client id: f_00001-3-1 loss: 0.552607  [   64/  265]
train() client id: f_00001-3-2 loss: 0.538178  [   96/  265]
train() client id: f_00001-3-3 loss: 0.507660  [  128/  265]
train() client id: f_00001-3-4 loss: 0.541249  [  160/  265]
train() client id: f_00001-3-5 loss: 0.530278  [  192/  265]
train() client id: f_00001-3-6 loss: 0.549897  [  224/  265]
train() client id: f_00001-3-7 loss: 0.547169  [  256/  265]
train() client id: f_00001-4-0 loss: 0.501181  [   32/  265]
train() client id: f_00001-4-1 loss: 0.464333  [   64/  265]
train() client id: f_00001-4-2 loss: 0.478659  [   96/  265]
train() client id: f_00001-4-3 loss: 0.686244  [  128/  265]
train() client id: f_00001-4-4 loss: 0.516409  [  160/  265]
train() client id: f_00001-4-5 loss: 0.529778  [  192/  265]
train() client id: f_00001-4-6 loss: 0.538257  [  224/  265]
train() client id: f_00001-4-7 loss: 0.613933  [  256/  265]
train() client id: f_00001-5-0 loss: 0.541223  [   32/  265]
train() client id: f_00001-5-1 loss: 0.662844  [   64/  265]
train() client id: f_00001-5-2 loss: 0.584706  [   96/  265]
train() client id: f_00001-5-3 loss: 0.612887  [  128/  265]
train() client id: f_00001-5-4 loss: 0.472375  [  160/  265]
train() client id: f_00001-5-5 loss: 0.460038  [  192/  265]
train() client id: f_00001-5-6 loss: 0.464731  [  224/  265]
train() client id: f_00001-5-7 loss: 0.567659  [  256/  265]
train() client id: f_00001-6-0 loss: 0.475784  [   32/  265]
train() client id: f_00001-6-1 loss: 0.575283  [   64/  265]
train() client id: f_00001-6-2 loss: 0.607865  [   96/  265]
train() client id: f_00001-6-3 loss: 0.502471  [  128/  265]
train() client id: f_00001-6-4 loss: 0.623058  [  160/  265]
train() client id: f_00001-6-5 loss: 0.464387  [  192/  265]
train() client id: f_00001-6-6 loss: 0.489394  [  224/  265]
train() client id: f_00001-6-7 loss: 0.658567  [  256/  265]
train() client id: f_00001-7-0 loss: 0.589412  [   32/  265]
train() client id: f_00001-7-1 loss: 0.671532  [   64/  265]
train() client id: f_00001-7-2 loss: 0.486970  [   96/  265]
train() client id: f_00001-7-3 loss: 0.525776  [  128/  265]
train() client id: f_00001-7-4 loss: 0.563474  [  160/  265]
train() client id: f_00001-7-5 loss: 0.468054  [  192/  265]
train() client id: f_00001-7-6 loss: 0.524424  [  224/  265]
train() client id: f_00001-7-7 loss: 0.562952  [  256/  265]
train() client id: f_00001-8-0 loss: 0.478658  [   32/  265]
train() client id: f_00001-8-1 loss: 0.496586  [   64/  265]
train() client id: f_00001-8-2 loss: 0.565628  [   96/  265]
train() client id: f_00001-8-3 loss: 0.559732  [  128/  265]
train() client id: f_00001-8-4 loss: 0.506846  [  160/  265]
train() client id: f_00001-8-5 loss: 0.486903  [  192/  265]
train() client id: f_00001-8-6 loss: 0.513338  [  224/  265]
train() client id: f_00001-8-7 loss: 0.687611  [  256/  265]
train() client id: f_00001-9-0 loss: 0.555994  [   32/  265]
train() client id: f_00001-9-1 loss: 0.500036  [   64/  265]
train() client id: f_00001-9-2 loss: 0.681345  [   96/  265]
train() client id: f_00001-9-3 loss: 0.477286  [  128/  265]
train() client id: f_00001-9-4 loss: 0.453570  [  160/  265]
train() client id: f_00001-9-5 loss: 0.648289  [  192/  265]
train() client id: f_00001-9-6 loss: 0.607637  [  224/  265]
train() client id: f_00001-9-7 loss: 0.468729  [  256/  265]
train() client id: f_00001-10-0 loss: 0.520728  [   32/  265]
train() client id: f_00001-10-1 loss: 0.523950  [   64/  265]
train() client id: f_00001-10-2 loss: 0.466424  [   96/  265]
train() client id: f_00001-10-3 loss: 0.525978  [  128/  265]
train() client id: f_00001-10-4 loss: 0.730808  [  160/  265]
train() client id: f_00001-10-5 loss: 0.514277  [  192/  265]
train() client id: f_00001-10-6 loss: 0.612164  [  224/  265]
train() client id: f_00001-10-7 loss: 0.512943  [  256/  265]
train() client id: f_00002-0-0 loss: 1.134181  [   32/  124]
train() client id: f_00002-0-1 loss: 1.230888  [   64/  124]
train() client id: f_00002-0-2 loss: 1.254117  [   96/  124]
train() client id: f_00002-1-0 loss: 1.365697  [   32/  124]
train() client id: f_00002-1-1 loss: 1.165384  [   64/  124]
train() client id: f_00002-1-2 loss: 1.056833  [   96/  124]
train() client id: f_00002-2-0 loss: 1.229392  [   32/  124]
train() client id: f_00002-2-1 loss: 1.132921  [   64/  124]
train() client id: f_00002-2-2 loss: 1.233985  [   96/  124]
train() client id: f_00002-3-0 loss: 1.174026  [   32/  124]
train() client id: f_00002-3-1 loss: 1.158799  [   64/  124]
train() client id: f_00002-3-2 loss: 1.162982  [   96/  124]
train() client id: f_00002-4-0 loss: 1.211844  [   32/  124]
train() client id: f_00002-4-1 loss: 1.218963  [   64/  124]
train() client id: f_00002-4-2 loss: 1.025715  [   96/  124]
train() client id: f_00002-5-0 loss: 1.208918  [   32/  124]
train() client id: f_00002-5-1 loss: 1.097140  [   64/  124]
train() client id: f_00002-5-2 loss: 1.085420  [   96/  124]
train() client id: f_00002-6-0 loss: 1.004743  [   32/  124]
train() client id: f_00002-6-1 loss: 1.089149  [   64/  124]
train() client id: f_00002-6-2 loss: 1.306727  [   96/  124]
train() client id: f_00002-7-0 loss: 1.103058  [   32/  124]
train() client id: f_00002-7-1 loss: 1.247277  [   64/  124]
train() client id: f_00002-7-2 loss: 0.933187  [   96/  124]
train() client id: f_00002-8-0 loss: 1.084257  [   32/  124]
train() client id: f_00002-8-1 loss: 1.113270  [   64/  124]
train() client id: f_00002-8-2 loss: 1.161276  [   96/  124]
train() client id: f_00002-9-0 loss: 1.042887  [   32/  124]
train() client id: f_00002-9-1 loss: 1.015223  [   64/  124]
train() client id: f_00002-9-2 loss: 1.124761  [   96/  124]
train() client id: f_00002-10-0 loss: 1.022062  [   32/  124]
train() client id: f_00002-10-1 loss: 1.066133  [   64/  124]
train() client id: f_00002-10-2 loss: 1.044703  [   96/  124]
train() client id: f_00003-0-0 loss: 1.035075  [   32/   43]
train() client id: f_00003-1-0 loss: 0.851391  [   32/   43]
train() client id: f_00003-2-0 loss: 0.935888  [   32/   43]
train() client id: f_00003-3-0 loss: 0.765810  [   32/   43]
train() client id: f_00003-4-0 loss: 0.901522  [   32/   43]
train() client id: f_00003-5-0 loss: 0.801741  [   32/   43]
train() client id: f_00003-6-0 loss: 0.964369  [   32/   43]
train() client id: f_00003-7-0 loss: 0.884802  [   32/   43]
train() client id: f_00003-8-0 loss: 0.938974  [   32/   43]
train() client id: f_00003-9-0 loss: 0.899362  [   32/   43]
train() client id: f_00003-10-0 loss: 0.881695  [   32/   43]
train() client id: f_00004-0-0 loss: 0.868389  [   32/  306]
train() client id: f_00004-0-1 loss: 0.664431  [   64/  306]
train() client id: f_00004-0-2 loss: 0.698857  [   96/  306]
train() client id: f_00004-0-3 loss: 0.672958  [  128/  306]
train() client id: f_00004-0-4 loss: 0.759095  [  160/  306]
train() client id: f_00004-0-5 loss: 0.766197  [  192/  306]
train() client id: f_00004-0-6 loss: 0.854709  [  224/  306]
train() client id: f_00004-0-7 loss: 0.605875  [  256/  306]
train() client id: f_00004-0-8 loss: 0.772453  [  288/  306]
train() client id: f_00004-1-0 loss: 0.837683  [   32/  306]
train() client id: f_00004-1-1 loss: 0.755168  [   64/  306]
train() client id: f_00004-1-2 loss: 0.870854  [   96/  306]
train() client id: f_00004-1-3 loss: 0.699098  [  128/  306]
train() client id: f_00004-1-4 loss: 0.854551  [  160/  306]
train() client id: f_00004-1-5 loss: 0.702214  [  192/  306]
train() client id: f_00004-1-6 loss: 0.686845  [  224/  306]
train() client id: f_00004-1-7 loss: 0.618220  [  256/  306]
train() client id: f_00004-1-8 loss: 0.707678  [  288/  306]
train() client id: f_00004-2-0 loss: 0.756616  [   32/  306]
train() client id: f_00004-2-1 loss: 0.715961  [   64/  306]
train() client id: f_00004-2-2 loss: 0.769942  [   96/  306]
train() client id: f_00004-2-3 loss: 0.842507  [  128/  306]
train() client id: f_00004-2-4 loss: 0.764239  [  160/  306]
train() client id: f_00004-2-5 loss: 0.770568  [  192/  306]
train() client id: f_00004-2-6 loss: 0.662401  [  224/  306]
train() client id: f_00004-2-7 loss: 0.697522  [  256/  306]
train() client id: f_00004-2-8 loss: 0.729164  [  288/  306]
train() client id: f_00004-3-0 loss: 0.656501  [   32/  306]
train() client id: f_00004-3-1 loss: 0.700949  [   64/  306]
train() client id: f_00004-3-2 loss: 0.695611  [   96/  306]
train() client id: f_00004-3-3 loss: 0.725503  [  128/  306]
train() client id: f_00004-3-4 loss: 0.900198  [  160/  306]
train() client id: f_00004-3-5 loss: 0.710946  [  192/  306]
train() client id: f_00004-3-6 loss: 0.751389  [  224/  306]
train() client id: f_00004-3-7 loss: 0.816704  [  256/  306]
train() client id: f_00004-3-8 loss: 0.664737  [  288/  306]
train() client id: f_00004-4-0 loss: 0.770027  [   32/  306]
train() client id: f_00004-4-1 loss: 0.783539  [   64/  306]
train() client id: f_00004-4-2 loss: 0.722690  [   96/  306]
train() client id: f_00004-4-3 loss: 0.633840  [  128/  306]
train() client id: f_00004-4-4 loss: 0.706287  [  160/  306]
train() client id: f_00004-4-5 loss: 0.721017  [  192/  306]
train() client id: f_00004-4-6 loss: 0.738166  [  224/  306]
train() client id: f_00004-4-7 loss: 0.696351  [  256/  306]
train() client id: f_00004-4-8 loss: 0.828926  [  288/  306]
train() client id: f_00004-5-0 loss: 0.796326  [   32/  306]
train() client id: f_00004-5-1 loss: 0.776117  [   64/  306]
train() client id: f_00004-5-2 loss: 0.813818  [   96/  306]
train() client id: f_00004-5-3 loss: 0.661057  [  128/  306]
train() client id: f_00004-5-4 loss: 0.692844  [  160/  306]
train() client id: f_00004-5-5 loss: 0.753425  [  192/  306]
train() client id: f_00004-5-6 loss: 0.708912  [  224/  306]
train() client id: f_00004-5-7 loss: 0.711921  [  256/  306]
train() client id: f_00004-5-8 loss: 0.760469  [  288/  306]
train() client id: f_00004-6-0 loss: 0.738654  [   32/  306]
train() client id: f_00004-6-1 loss: 0.852630  [   64/  306]
train() client id: f_00004-6-2 loss: 0.709074  [   96/  306]
train() client id: f_00004-6-3 loss: 0.692267  [  128/  306]
train() client id: f_00004-6-4 loss: 0.803741  [  160/  306]
train() client id: f_00004-6-5 loss: 0.649733  [  192/  306]
train() client id: f_00004-6-6 loss: 0.898888  [  224/  306]
train() client id: f_00004-6-7 loss: 0.638129  [  256/  306]
train() client id: f_00004-6-8 loss: 0.758000  [  288/  306]
train() client id: f_00004-7-0 loss: 0.756645  [   32/  306]
train() client id: f_00004-7-1 loss: 0.763590  [   64/  306]
train() client id: f_00004-7-2 loss: 0.714810  [   96/  306]
train() client id: f_00004-7-3 loss: 0.877794  [  128/  306]
train() client id: f_00004-7-4 loss: 0.760431  [  160/  306]
train() client id: f_00004-7-5 loss: 0.737503  [  192/  306]
train() client id: f_00004-7-6 loss: 0.658628  [  224/  306]
train() client id: f_00004-7-7 loss: 0.704109  [  256/  306]
train() client id: f_00004-7-8 loss: 0.737172  [  288/  306]
train() client id: f_00004-8-0 loss: 0.834065  [   32/  306]
train() client id: f_00004-8-1 loss: 0.692862  [   64/  306]
train() client id: f_00004-8-2 loss: 0.819311  [   96/  306]
train() client id: f_00004-8-3 loss: 0.698936  [  128/  306]
train() client id: f_00004-8-4 loss: 0.736997  [  160/  306]
train() client id: f_00004-8-5 loss: 0.695153  [  192/  306]
train() client id: f_00004-8-6 loss: 0.760020  [  224/  306]
train() client id: f_00004-8-7 loss: 0.869850  [  256/  306]
train() client id: f_00004-8-8 loss: 0.718262  [  288/  306]
train() client id: f_00004-9-0 loss: 0.653120  [   32/  306]
train() client id: f_00004-9-1 loss: 0.694123  [   64/  306]
train() client id: f_00004-9-2 loss: 0.818567  [   96/  306]
train() client id: f_00004-9-3 loss: 0.745951  [  128/  306]
train() client id: f_00004-9-4 loss: 0.764885  [  160/  306]
train() client id: f_00004-9-5 loss: 0.854713  [  192/  306]
train() client id: f_00004-9-6 loss: 0.807598  [  224/  306]
train() client id: f_00004-9-7 loss: 0.660592  [  256/  306]
train() client id: f_00004-9-8 loss: 0.720399  [  288/  306]
train() client id: f_00004-10-0 loss: 0.660587  [   32/  306]
train() client id: f_00004-10-1 loss: 0.722134  [   64/  306]
train() client id: f_00004-10-2 loss: 0.744949  [   96/  306]
train() client id: f_00004-10-3 loss: 0.850780  [  128/  306]
train() client id: f_00004-10-4 loss: 0.834684  [  160/  306]
train() client id: f_00004-10-5 loss: 0.704435  [  192/  306]
train() client id: f_00004-10-6 loss: 0.636355  [  224/  306]
train() client id: f_00004-10-7 loss: 0.825703  [  256/  306]
train() client id: f_00004-10-8 loss: 0.761346  [  288/  306]
train() client id: f_00005-0-0 loss: 0.460301  [   32/  146]
train() client id: f_00005-0-1 loss: 1.010941  [   64/  146]
train() client id: f_00005-0-2 loss: 0.521073  [   96/  146]
train() client id: f_00005-0-3 loss: 0.622739  [  128/  146]
train() client id: f_00005-1-0 loss: 0.740319  [   32/  146]
train() client id: f_00005-1-1 loss: 0.505630  [   64/  146]
train() client id: f_00005-1-2 loss: 0.680609  [   96/  146]
train() client id: f_00005-1-3 loss: 0.723764  [  128/  146]
train() client id: f_00005-2-0 loss: 0.483660  [   32/  146]
train() client id: f_00005-2-1 loss: 0.650411  [   64/  146]
train() client id: f_00005-2-2 loss: 0.894859  [   96/  146]
train() client id: f_00005-2-3 loss: 0.526094  [  128/  146]
train() client id: f_00005-3-0 loss: 0.586259  [   32/  146]
train() client id: f_00005-3-1 loss: 0.577700  [   64/  146]
train() client id: f_00005-3-2 loss: 0.548012  [   96/  146]
train() client id: f_00005-3-3 loss: 0.913565  [  128/  146]
train() client id: f_00005-4-0 loss: 0.711974  [   32/  146]
train() client id: f_00005-4-1 loss: 0.386361  [   64/  146]
train() client id: f_00005-4-2 loss: 0.742803  [   96/  146]
train() client id: f_00005-4-3 loss: 0.590786  [  128/  146]
train() client id: f_00005-5-0 loss: 0.704049  [   32/  146]
train() client id: f_00005-5-1 loss: 0.567014  [   64/  146]
train() client id: f_00005-5-2 loss: 0.681208  [   96/  146]
train() client id: f_00005-5-3 loss: 0.673620  [  128/  146]
train() client id: f_00005-6-0 loss: 0.664604  [   32/  146]
train() client id: f_00005-6-1 loss: 0.667333  [   64/  146]
train() client id: f_00005-6-2 loss: 1.037228  [   96/  146]
train() client id: f_00005-6-3 loss: 0.351300  [  128/  146]
train() client id: f_00005-7-0 loss: 0.599797  [   32/  146]
train() client id: f_00005-7-1 loss: 0.913584  [   64/  146]
train() client id: f_00005-7-2 loss: 0.596145  [   96/  146]
train() client id: f_00005-7-3 loss: 0.562243  [  128/  146]
train() client id: f_00005-8-0 loss: 0.605345  [   32/  146]
train() client id: f_00005-8-1 loss: 0.806859  [   64/  146]
train() client id: f_00005-8-2 loss: 0.561808  [   96/  146]
train() client id: f_00005-8-3 loss: 0.425480  [  128/  146]
train() client id: f_00005-9-0 loss: 0.850172  [   32/  146]
train() client id: f_00005-9-1 loss: 0.579335  [   64/  146]
train() client id: f_00005-9-2 loss: 0.327957  [   96/  146]
train() client id: f_00005-9-3 loss: 0.755012  [  128/  146]
train() client id: f_00005-10-0 loss: 0.575603  [   32/  146]
train() client id: f_00005-10-1 loss: 0.654853  [   64/  146]
train() client id: f_00005-10-2 loss: 0.557058  [   96/  146]
train() client id: f_00005-10-3 loss: 0.655478  [  128/  146]
train() client id: f_00006-0-0 loss: 0.605850  [   32/   54]
train() client id: f_00006-1-0 loss: 0.666636  [   32/   54]
train() client id: f_00006-2-0 loss: 0.608524  [   32/   54]
train() client id: f_00006-3-0 loss: 0.604503  [   32/   54]
train() client id: f_00006-4-0 loss: 0.603211  [   32/   54]
train() client id: f_00006-5-0 loss: 0.659090  [   32/   54]
train() client id: f_00006-6-0 loss: 0.656722  [   32/   54]
train() client id: f_00006-7-0 loss: 0.634616  [   32/   54]
train() client id: f_00006-8-0 loss: 0.560115  [   32/   54]
train() client id: f_00006-9-0 loss: 0.606419  [   32/   54]
train() client id: f_00006-10-0 loss: 0.660221  [   32/   54]
train() client id: f_00007-0-0 loss: 0.772269  [   32/  179]
train() client id: f_00007-0-1 loss: 0.527983  [   64/  179]
train() client id: f_00007-0-2 loss: 0.601439  [   96/  179]
train() client id: f_00007-0-3 loss: 0.740165  [  128/  179]
train() client id: f_00007-0-4 loss: 0.613493  [  160/  179]
train() client id: f_00007-1-0 loss: 0.648329  [   32/  179]
train() client id: f_00007-1-1 loss: 0.563042  [   64/  179]
train() client id: f_00007-1-2 loss: 0.628789  [   96/  179]
train() client id: f_00007-1-3 loss: 0.660697  [  128/  179]
train() client id: f_00007-1-4 loss: 0.763819  [  160/  179]
train() client id: f_00007-2-0 loss: 0.574866  [   32/  179]
train() client id: f_00007-2-1 loss: 0.639267  [   64/  179]
train() client id: f_00007-2-2 loss: 0.766981  [   96/  179]
train() client id: f_00007-2-3 loss: 0.607278  [  128/  179]
train() client id: f_00007-2-4 loss: 0.594299  [  160/  179]
train() client id: f_00007-3-0 loss: 0.707643  [   32/  179]
train() client id: f_00007-3-1 loss: 0.619869  [   64/  179]
train() client id: f_00007-3-2 loss: 0.846011  [   96/  179]
train() client id: f_00007-3-3 loss: 0.495605  [  128/  179]
train() client id: f_00007-3-4 loss: 0.559484  [  160/  179]
train() client id: f_00007-4-0 loss: 0.696801  [   32/  179]
train() client id: f_00007-4-1 loss: 0.491831  [   64/  179]
train() client id: f_00007-4-2 loss: 0.754602  [   96/  179]
train() client id: f_00007-4-3 loss: 0.544896  [  128/  179]
train() client id: f_00007-4-4 loss: 0.722896  [  160/  179]
train() client id: f_00007-5-0 loss: 0.691038  [   32/  179]
train() client id: f_00007-5-1 loss: 0.499692  [   64/  179]
train() client id: f_00007-5-2 loss: 0.637925  [   96/  179]
train() client id: f_00007-5-3 loss: 0.572227  [  128/  179]
train() client id: f_00007-5-4 loss: 0.586986  [  160/  179]
train() client id: f_00007-6-0 loss: 0.682103  [   32/  179]
train() client id: f_00007-6-1 loss: 0.752047  [   64/  179]
train() client id: f_00007-6-2 loss: 0.477817  [   96/  179]
train() client id: f_00007-6-3 loss: 0.664643  [  128/  179]
train() client id: f_00007-6-4 loss: 0.510937  [  160/  179]
train() client id: f_00007-7-0 loss: 0.621289  [   32/  179]
train() client id: f_00007-7-1 loss: 0.583709  [   64/  179]
train() client id: f_00007-7-2 loss: 0.572020  [   96/  179]
train() client id: f_00007-7-3 loss: 0.685069  [  128/  179]
train() client id: f_00007-7-4 loss: 0.690864  [  160/  179]
train() client id: f_00007-8-0 loss: 0.563503  [   32/  179]
train() client id: f_00007-8-1 loss: 0.564837  [   64/  179]
train() client id: f_00007-8-2 loss: 0.563196  [   96/  179]
train() client id: f_00007-8-3 loss: 0.771041  [  128/  179]
train() client id: f_00007-8-4 loss: 0.701711  [  160/  179]
train() client id: f_00007-9-0 loss: 0.553169  [   32/  179]
train() client id: f_00007-9-1 loss: 0.892703  [   64/  179]
train() client id: f_00007-9-2 loss: 0.569522  [   96/  179]
train() client id: f_00007-9-3 loss: 0.594751  [  128/  179]
train() client id: f_00007-9-4 loss: 0.565885  [  160/  179]
train() client id: f_00007-10-0 loss: 0.791908  [   32/  179]
train() client id: f_00007-10-1 loss: 0.652032  [   64/  179]
train() client id: f_00007-10-2 loss: 0.590720  [   96/  179]
train() client id: f_00007-10-3 loss: 0.475827  [  128/  179]
train() client id: f_00007-10-4 loss: 0.644145  [  160/  179]
train() client id: f_00008-0-0 loss: 0.724543  [   32/  130]
train() client id: f_00008-0-1 loss: 0.869185  [   64/  130]
train() client id: f_00008-0-2 loss: 0.789907  [   96/  130]
train() client id: f_00008-0-3 loss: 0.781052  [  128/  130]
train() client id: f_00008-1-0 loss: 0.825557  [   32/  130]
train() client id: f_00008-1-1 loss: 0.841513  [   64/  130]
train() client id: f_00008-1-2 loss: 0.725946  [   96/  130]
train() client id: f_00008-1-3 loss: 0.764736  [  128/  130]
train() client id: f_00008-2-0 loss: 0.745821  [   32/  130]
train() client id: f_00008-2-1 loss: 0.744776  [   64/  130]
train() client id: f_00008-2-2 loss: 0.896848  [   96/  130]
train() client id: f_00008-2-3 loss: 0.725813  [  128/  130]
train() client id: f_00008-3-0 loss: 0.808287  [   32/  130]
train() client id: f_00008-3-1 loss: 0.822679  [   64/  130]
train() client id: f_00008-3-2 loss: 0.738252  [   96/  130]
train() client id: f_00008-3-3 loss: 0.782848  [  128/  130]
train() client id: f_00008-4-0 loss: 0.808137  [   32/  130]
train() client id: f_00008-4-1 loss: 0.871561  [   64/  130]
train() client id: f_00008-4-2 loss: 0.680463  [   96/  130]
train() client id: f_00008-4-3 loss: 0.785692  [  128/  130]
train() client id: f_00008-5-0 loss: 0.778789  [   32/  130]
train() client id: f_00008-5-1 loss: 0.843369  [   64/  130]
train() client id: f_00008-5-2 loss: 0.850323  [   96/  130]
train() client id: f_00008-5-3 loss: 0.681606  [  128/  130]
train() client id: f_00008-6-0 loss: 0.826615  [   32/  130]
train() client id: f_00008-6-1 loss: 0.800685  [   64/  130]
train() client id: f_00008-6-2 loss: 0.863307  [   96/  130]
train() client id: f_00008-6-3 loss: 0.629025  [  128/  130]
train() client id: f_00008-7-0 loss: 0.751831  [   32/  130]
train() client id: f_00008-7-1 loss: 0.863221  [   64/  130]
train() client id: f_00008-7-2 loss: 0.869654  [   96/  130]
train() client id: f_00008-7-3 loss: 0.663229  [  128/  130]
train() client id: f_00008-8-0 loss: 0.804726  [   32/  130]
train() client id: f_00008-8-1 loss: 0.723667  [   64/  130]
train() client id: f_00008-8-2 loss: 0.780333  [   96/  130]
train() client id: f_00008-8-3 loss: 0.827929  [  128/  130]
train() client id: f_00008-9-0 loss: 0.665567  [   32/  130]
train() client id: f_00008-9-1 loss: 0.887007  [   64/  130]
train() client id: f_00008-9-2 loss: 0.928369  [   96/  130]
train() client id: f_00008-9-3 loss: 0.680472  [  128/  130]
train() client id: f_00008-10-0 loss: 0.749808  [   32/  130]
train() client id: f_00008-10-1 loss: 0.827193  [   64/  130]
train() client id: f_00008-10-2 loss: 0.838904  [   96/  130]
train() client id: f_00008-10-3 loss: 0.740878  [  128/  130]
train() client id: f_00009-0-0 loss: 1.078937  [   32/  118]
train() client id: f_00009-0-1 loss: 0.970015  [   64/  118]
train() client id: f_00009-0-2 loss: 1.020802  [   96/  118]
train() client id: f_00009-1-0 loss: 1.076284  [   32/  118]
train() client id: f_00009-1-1 loss: 0.931256  [   64/  118]
train() client id: f_00009-1-2 loss: 0.999802  [   96/  118]
train() client id: f_00009-2-0 loss: 0.959058  [   32/  118]
train() client id: f_00009-2-1 loss: 0.963906  [   64/  118]
train() client id: f_00009-2-2 loss: 0.833625  [   96/  118]
train() client id: f_00009-3-0 loss: 0.904615  [   32/  118]
train() client id: f_00009-3-1 loss: 0.961047  [   64/  118]
train() client id: f_00009-3-2 loss: 0.860253  [   96/  118]
train() client id: f_00009-4-0 loss: 0.974101  [   32/  118]
train() client id: f_00009-4-1 loss: 0.773087  [   64/  118]
train() client id: f_00009-4-2 loss: 0.927916  [   96/  118]
train() client id: f_00009-5-0 loss: 0.923107  [   32/  118]
train() client id: f_00009-5-1 loss: 0.846383  [   64/  118]
train() client id: f_00009-5-2 loss: 0.819439  [   96/  118]
train() client id: f_00009-6-0 loss: 0.821300  [   32/  118]
train() client id: f_00009-6-1 loss: 0.834101  [   64/  118]
train() client id: f_00009-6-2 loss: 0.742314  [   96/  118]
train() client id: f_00009-7-0 loss: 0.777121  [   32/  118]
train() client id: f_00009-7-1 loss: 0.852249  [   64/  118]
train() client id: f_00009-7-2 loss: 0.701907  [   96/  118]
train() client id: f_00009-8-0 loss: 0.726502  [   32/  118]
train() client id: f_00009-8-1 loss: 0.866707  [   64/  118]
train() client id: f_00009-8-2 loss: 0.803797  [   96/  118]
train() client id: f_00009-9-0 loss: 0.785921  [   32/  118]
train() client id: f_00009-9-1 loss: 0.753445  [   64/  118]
train() client id: f_00009-9-2 loss: 0.903698  [   96/  118]
train() client id: f_00009-10-0 loss: 0.828903  [   32/  118]
train() client id: f_00009-10-1 loss: 0.831861  [   64/  118]
train() client id: f_00009-10-2 loss: 0.763774  [   96/  118]
At round 18 accuracy: 0.6419098143236074
At round 18 training accuracy: 0.5767940979208585
At round 18 training loss: 0.8393036105788091
update_location
xs = -4.528292 -8.998411 0.045120 -0.943528 -100.103519 -35.217951 -42.215960 63.375741 -1.680116 4.695607 
ys = 77.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 -3.124922 -19.177652 -7.154970 4.001482 
xs mean: -12.557130840587133
ys mean: 8.371751218646876
dists_uav = 126.650688 101.601946 100.008730 102.514191 141.803175 107.404178 108.590756 119.934427 100.269718 100.190122 
uav_gains = -102.565645 -100.172571 -100.000964 -100.269622 -103.794298 -100.775574 -100.894873 -101.973809 -100.029261 -100.020639 
uav_gains_db_mean: -101.0497256090673
dists_bs = 196.340798 230.171606 246.587354 231.383310 181.794784 237.643477 222.171306 307.453987 251.436305 248.054439 
bs_gains = -103.771355 -105.704508 -106.542242 -105.768356 -102.835339 -106.092984 -105.274322 -109.224903 -106.779043 -106.614375 
bs_gains_db_mean: -105.8607426271239
Round 19
-------------------------------
ene_coms = [0.00705234 0.00824676 0.00630267 0.00637374 0.00714149 0.00842243
 0.00654543 0.00686417 0.00875099 0.0086699 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.47396664 17.67805298  8.32549735  2.97875914 20.36101638  9.82452879
  3.70882951 11.96934817  8.77526877  7.98115355]
obj_prev = 100.07642127584147
eta_min = 1.0014236334004614e-11	eta_max = 0.9336178108644619
af = 21.163983583608065	bf = 1.6119108471098023	zeta = 23.280381941968873	eta = 0.9090909090909091
af = 21.163983583608065	bf = 1.6119108471098023	zeta = 39.91959320623194	eta = 0.5301653119126502
af = 21.163983583608065	bf = 1.6119108471098023	zeta = 32.01522652658217	eta = 0.6610599355289789
af = 21.163983583608065	bf = 1.6119108471098023	zeta = 30.601019646687526	eta = 0.6916104047499936
af = 21.163983583608065	bf = 1.6119108471098023	zeta = 30.532025603458184	eta = 0.6931732554688721
af = 21.163983583608065	bf = 1.6119108471098023	zeta = 30.531850021858034	eta = 0.6931772417477674
eta = 0.6931772417477674
ene_coms = [0.00705234 0.00824676 0.00630267 0.00637374 0.00714149 0.00842243
 0.00654543 0.00686417 0.00875099 0.0086699 ]
ene_comp = [0.03024019 0.0636004  0.02976019 0.01032006 0.07344046 0.03504022
 0.01296008 0.04296027 0.0312002  0.02832018]
ene_total = [2.63437734 5.07534666 2.54751215 1.179265   5.69238021 3.07024042
 1.3778864  3.51964282 2.82218721 2.61301181]
ti_comp = [0.3407104  0.32876624 0.3482071  0.34749642 0.33981892 0.32700945
 0.34577953 0.34259206 0.32372389 0.3245348 ]
ti_coms = [0.0705234  0.08246756 0.0630267  0.06373738 0.07141488 0.08422435
 0.06545427 0.06864174 0.08750991 0.08669899]
t_total = [29.04992027 29.04992027 29.04992027 29.04992027 29.04992027 29.04992027
 29.04992027 29.04992027 29.04992027 29.04992027]
ene_coms = [0.00705234 0.00824676 0.00630267 0.00637374 0.00714149 0.00842243
 0.00654543 0.00686417 0.00875099 0.0086699 ]
ene_comp = [1.48889047e-05 1.48759629e-04 1.35866287e-05 5.68887751e-07
 2.14383353e-04 2.51454753e-05 1.13790136e-06 4.22207944e-05
 1.81135029e-05 1.34786176e-05]
ene_total = [0.49923533 0.59306668 0.44618598 0.45028667 0.51962527 0.59674457
 0.46245516 0.48787386 0.61945731 0.61340152]
optimize_network iter = 0 obj = 5.288332349402115
eta = 0.6931772417477674
freqs = [4.43781424e+07 9.67258653e+07 4.27334571e+07 1.48491666e+07
 1.08058226e+08 5.35767683e+07 1.87403823e+07 6.26988683e+07
 4.81895158e+07 4.36319566e+07]
eta_min = 0.6931772417479515	eta_max = 0.6931772417476824
af = 0.02911529980736142	bf = 1.6119108471098023	zeta = 0.032026829788097566	eta = 0.9090909090909091
af = 0.02911529980736142	bf = 1.6119108471098023	zeta = 17.76154414726448	eta = 0.0016392324657113538
af = 0.02911529980736142	bf = 1.6119108471098023	zeta = 1.8015958474654417	eta = 0.01616083865219939
af = 0.02911529980736142	bf = 1.6119108471098023	zeta = 1.760466864291593	eta = 0.016538396943402475
af = 0.02911529980736142	bf = 1.6119108471098023	zeta = 1.760459030202006	eta = 0.016538470539709492
eta = 0.016538470539709492
ene_coms = [0.00705234 0.00824676 0.00630267 0.00637374 0.00714149 0.00842243
 0.00654543 0.00686417 0.00875099 0.0086699 ]
ene_comp = [1.66658513e-04 1.66513650e-03 1.52081525e-04 6.36782817e-06
 2.39969370e-03 2.81465132e-04 1.27370651e-05 4.72597210e-04
 2.02752957e-04 1.50872506e-04]
ene_total = [0.15909748 0.21844541 0.14225446 0.14060935 0.21027541 0.1918228
 0.14453353 0.16169304 0.19732904 0.19439851]
ti_comp = [0.3407104  0.32876624 0.3482071  0.34749642 0.33981892 0.32700945
 0.34577953 0.34259206 0.32372389 0.3245348 ]
ti_coms = [0.0705234  0.08246756 0.0630267  0.06373738 0.07141488 0.08422435
 0.06545427 0.06864174 0.08750991 0.08669899]
t_total = [29.04992027 29.04992027 29.04992027 29.04992027 29.04992027 29.04992027
 29.04992027 29.04992027 29.04992027 29.04992027]
ene_coms = [0.00705234 0.00824676 0.00630267 0.00637374 0.00714149 0.00842243
 0.00654543 0.00686417 0.00875099 0.0086699 ]
ene_comp = [1.48889047e-05 1.48759629e-04 1.35866287e-05 5.68887751e-07
 2.14383353e-04 2.51454753e-05 1.13790136e-06 4.22207944e-05
 1.81135029e-05 1.34786176e-05]
ene_total = [0.49923533 0.59306668 0.44618598 0.45028667 0.51962527 0.59674457
 0.46245516 0.48787386 0.61945731 0.61340152]
optimize_network iter = 1 obj = 5.288332349405264
eta = 0.6931772417479515
freqs = [4.43781424e+07 9.67258653e+07 4.27334571e+07 1.48491666e+07
 1.08058226e+08 5.35767683e+07 1.87403823e+07 6.26988683e+07
 4.81895158e+07 4.36319566e+07]
Done!
ene_coms = [0.00705234 0.00824676 0.00630267 0.00637374 0.00714149 0.00842243
 0.00654543 0.00686417 0.00875099 0.0086699 ]
ene_comp = [1.48888116e-05 1.48758699e-04 1.35865438e-05 5.68884194e-07
 2.14382012e-04 2.51453181e-05 1.13789424e-06 4.22205304e-05
 1.81133896e-05 1.34785333e-05]
ene_total = [0.00706723 0.00839551 0.00631626 0.00637431 0.00735587 0.00844758
 0.00654657 0.00690639 0.0087691  0.00868338]
At round 19 energy consumption: 0.07486219899100377
At round 19 eta: 0.6931772417479515
At round 19 a_n: 21.674231763837348
At round 19 local rounds: 12.00007503402881
At round 19 global rounds: 70.64088689937536
gradient difference: 0.5594996809959412
train() client id: f_00000-0-0 loss: 0.878255  [   32/  126]
train() client id: f_00000-0-1 loss: 1.289959  [   64/  126]
train() client id: f_00000-0-2 loss: 0.954695  [   96/  126]
train() client id: f_00000-1-0 loss: 1.094241  [   32/  126]
train() client id: f_00000-1-1 loss: 0.824187  [   64/  126]
train() client id: f_00000-1-2 loss: 0.875475  [   96/  126]
train() client id: f_00000-2-0 loss: 0.948679  [   32/  126]
train() client id: f_00000-2-1 loss: 0.969160  [   64/  126]
train() client id: f_00000-2-2 loss: 0.976127  [   96/  126]
train() client id: f_00000-3-0 loss: 0.880671  [   32/  126]
train() client id: f_00000-3-1 loss: 0.901608  [   64/  126]
train() client id: f_00000-3-2 loss: 0.845255  [   96/  126]
train() client id: f_00000-4-0 loss: 0.891247  [   32/  126]
train() client id: f_00000-4-1 loss: 0.969101  [   64/  126]
train() client id: f_00000-4-2 loss: 0.929701  [   96/  126]
train() client id: f_00000-5-0 loss: 0.877848  [   32/  126]
train() client id: f_00000-5-1 loss: 0.939696  [   64/  126]
train() client id: f_00000-5-2 loss: 0.907380  [   96/  126]
train() client id: f_00000-6-0 loss: 1.035059  [   32/  126]
train() client id: f_00000-6-1 loss: 0.867638  [   64/  126]
train() client id: f_00000-6-2 loss: 0.933169  [   96/  126]
train() client id: f_00000-7-0 loss: 0.916470  [   32/  126]
train() client id: f_00000-7-1 loss: 0.915529  [   64/  126]
train() client id: f_00000-7-2 loss: 0.907884  [   96/  126]
train() client id: f_00000-8-0 loss: 1.000721  [   32/  126]
train() client id: f_00000-8-1 loss: 0.910637  [   64/  126]
train() client id: f_00000-8-2 loss: 0.945599  [   96/  126]
train() client id: f_00000-9-0 loss: 0.940547  [   32/  126]
train() client id: f_00000-9-1 loss: 0.875749  [   64/  126]
train() client id: f_00000-9-2 loss: 1.012544  [   96/  126]
train() client id: f_00000-10-0 loss: 0.912437  [   32/  126]
train() client id: f_00000-10-1 loss: 1.013306  [   64/  126]
train() client id: f_00000-10-2 loss: 0.971213  [   96/  126]
train() client id: f_00000-11-0 loss: 0.946757  [   32/  126]
train() client id: f_00000-11-1 loss: 0.949501  [   64/  126]
train() client id: f_00000-11-2 loss: 1.055949  [   96/  126]
train() client id: f_00001-0-0 loss: 0.508685  [   32/  265]
train() client id: f_00001-0-1 loss: 0.624235  [   64/  265]
train() client id: f_00001-0-2 loss: 0.479307  [   96/  265]
train() client id: f_00001-0-3 loss: 0.534299  [  128/  265]
train() client id: f_00001-0-4 loss: 0.482638  [  160/  265]
train() client id: f_00001-0-5 loss: 0.459476  [  192/  265]
train() client id: f_00001-0-6 loss: 0.622171  [  224/  265]
train() client id: f_00001-0-7 loss: 0.488520  [  256/  265]
train() client id: f_00001-1-0 loss: 0.575543  [   32/  265]
train() client id: f_00001-1-1 loss: 0.454620  [   64/  265]
train() client id: f_00001-1-2 loss: 0.496263  [   96/  265]
train() client id: f_00001-1-3 loss: 0.510369  [  128/  265]
train() client id: f_00001-1-4 loss: 0.456708  [  160/  265]
train() client id: f_00001-1-5 loss: 0.476116  [  192/  265]
train() client id: f_00001-1-6 loss: 0.569890  [  224/  265]
train() client id: f_00001-1-7 loss: 0.543417  [  256/  265]
train() client id: f_00001-2-0 loss: 0.604927  [   32/  265]
train() client id: f_00001-2-1 loss: 0.496930  [   64/  265]
train() client id: f_00001-2-2 loss: 0.480545  [   96/  265]
train() client id: f_00001-2-3 loss: 0.582899  [  128/  265]
train() client id: f_00001-2-4 loss: 0.431468  [  160/  265]
train() client id: f_00001-2-5 loss: 0.443438  [  192/  265]
train() client id: f_00001-2-6 loss: 0.429204  [  224/  265]
train() client id: f_00001-2-7 loss: 0.620667  [  256/  265]
train() client id: f_00001-3-0 loss: 0.427167  [   32/  265]
train() client id: f_00001-3-1 loss: 0.533849  [   64/  265]
train() client id: f_00001-3-2 loss: 0.555568  [   96/  265]
train() client id: f_00001-3-3 loss: 0.552346  [  128/  265]
train() client id: f_00001-3-4 loss: 0.420568  [  160/  265]
train() client id: f_00001-3-5 loss: 0.470122  [  192/  265]
train() client id: f_00001-3-6 loss: 0.480255  [  224/  265]
train() client id: f_00001-3-7 loss: 0.556448  [  256/  265]
train() client id: f_00001-4-0 loss: 0.598159  [   32/  265]
train() client id: f_00001-4-1 loss: 0.587425  [   64/  265]
train() client id: f_00001-4-2 loss: 0.412384  [   96/  265]
train() client id: f_00001-4-3 loss: 0.481811  [  128/  265]
train() client id: f_00001-4-4 loss: 0.420524  [  160/  265]
train() client id: f_00001-4-5 loss: 0.462830  [  192/  265]
train() client id: f_00001-4-6 loss: 0.511341  [  224/  265]
train() client id: f_00001-4-7 loss: 0.546280  [  256/  265]
train() client id: f_00001-5-0 loss: 0.496411  [   32/  265]
train() client id: f_00001-5-1 loss: 0.633196  [   64/  265]
train() client id: f_00001-5-2 loss: 0.490544  [   96/  265]
train() client id: f_00001-5-3 loss: 0.416795  [  128/  265]
train() client id: f_00001-5-4 loss: 0.498740  [  160/  265]
train() client id: f_00001-5-5 loss: 0.500726  [  192/  265]
train() client id: f_00001-5-6 loss: 0.511688  [  224/  265]
train() client id: f_00001-5-7 loss: 0.458001  [  256/  265]
train() client id: f_00001-6-0 loss: 0.417400  [   32/  265]
train() client id: f_00001-6-1 loss: 0.485729  [   64/  265]
train() client id: f_00001-6-2 loss: 0.403138  [   96/  265]
train() client id: f_00001-6-3 loss: 0.589193  [  128/  265]
train() client id: f_00001-6-4 loss: 0.491853  [  160/  265]
train() client id: f_00001-6-5 loss: 0.402708  [  192/  265]
train() client id: f_00001-6-6 loss: 0.575590  [  224/  265]
train() client id: f_00001-6-7 loss: 0.536572  [  256/  265]
train() client id: f_00001-7-0 loss: 0.553826  [   32/  265]
train() client id: f_00001-7-1 loss: 0.524901  [   64/  265]
train() client id: f_00001-7-2 loss: 0.397883  [   96/  265]
train() client id: f_00001-7-3 loss: 0.508204  [  128/  265]
train() client id: f_00001-7-4 loss: 0.436820  [  160/  265]
train() client id: f_00001-7-5 loss: 0.518980  [  192/  265]
train() client id: f_00001-7-6 loss: 0.412675  [  224/  265]
train() client id: f_00001-7-7 loss: 0.626426  [  256/  265]
train() client id: f_00001-8-0 loss: 0.504840  [   32/  265]
train() client id: f_00001-8-1 loss: 0.418070  [   64/  265]
train() client id: f_00001-8-2 loss: 0.835212  [   96/  265]
train() client id: f_00001-8-3 loss: 0.428868  [  128/  265]
train() client id: f_00001-8-4 loss: 0.425582  [  160/  265]
train() client id: f_00001-8-5 loss: 0.507988  [  192/  265]
train() client id: f_00001-8-6 loss: 0.401851  [  224/  265]
train() client id: f_00001-8-7 loss: 0.409341  [  256/  265]
train() client id: f_00001-9-0 loss: 0.393946  [   32/  265]
train() client id: f_00001-9-1 loss: 0.445638  [   64/  265]
train() client id: f_00001-9-2 loss: 0.678529  [   96/  265]
train() client id: f_00001-9-3 loss: 0.501538  [  128/  265]
train() client id: f_00001-9-4 loss: 0.419395  [  160/  265]
train() client id: f_00001-9-5 loss: 0.612671  [  192/  265]
train() client id: f_00001-9-6 loss: 0.456684  [  224/  265]
train() client id: f_00001-9-7 loss: 0.465204  [  256/  265]
train() client id: f_00001-10-0 loss: 0.568666  [   32/  265]
train() client id: f_00001-10-1 loss: 0.407647  [   64/  265]
train() client id: f_00001-10-2 loss: 0.407609  [   96/  265]
train() client id: f_00001-10-3 loss: 0.403747  [  128/  265]
train() client id: f_00001-10-4 loss: 0.484847  [  160/  265]
train() client id: f_00001-10-5 loss: 0.580206  [  192/  265]
train() client id: f_00001-10-6 loss: 0.488231  [  224/  265]
train() client id: f_00001-10-7 loss: 0.632029  [  256/  265]
train() client id: f_00001-11-0 loss: 0.479473  [   32/  265]
train() client id: f_00001-11-1 loss: 0.407612  [   64/  265]
train() client id: f_00001-11-2 loss: 0.427894  [   96/  265]
train() client id: f_00001-11-3 loss: 0.630844  [  128/  265]
train() client id: f_00001-11-4 loss: 0.530930  [  160/  265]
train() client id: f_00001-11-5 loss: 0.444436  [  192/  265]
train() client id: f_00001-11-6 loss: 0.434136  [  224/  265]
train() client id: f_00001-11-7 loss: 0.618095  [  256/  265]
train() client id: f_00002-0-0 loss: 1.226485  [   32/  124]
train() client id: f_00002-0-1 loss: 1.071314  [   64/  124]
train() client id: f_00002-0-2 loss: 1.194741  [   96/  124]
train() client id: f_00002-1-0 loss: 1.177073  [   32/  124]
train() client id: f_00002-1-1 loss: 1.276320  [   64/  124]
train() client id: f_00002-1-2 loss: 1.042437  [   96/  124]
train() client id: f_00002-2-0 loss: 1.107096  [   32/  124]
train() client id: f_00002-2-1 loss: 1.188071  [   64/  124]
train() client id: f_00002-2-2 loss: 0.987079  [   96/  124]
train() client id: f_00002-3-0 loss: 1.159238  [   32/  124]
train() client id: f_00002-3-1 loss: 1.029099  [   64/  124]
train() client id: f_00002-3-2 loss: 1.161735  [   96/  124]
train() client id: f_00002-4-0 loss: 0.889008  [   32/  124]
train() client id: f_00002-4-1 loss: 1.162085  [   64/  124]
train() client id: f_00002-4-2 loss: 1.205533  [   96/  124]
train() client id: f_00002-5-0 loss: 1.016134  [   32/  124]
train() client id: f_00002-5-1 loss: 0.994327  [   64/  124]
train() client id: f_00002-5-2 loss: 1.238095  [   96/  124]
train() client id: f_00002-6-0 loss: 1.002064  [   32/  124]
train() client id: f_00002-6-1 loss: 1.112457  [   64/  124]
train() client id: f_00002-6-2 loss: 0.889868  [   96/  124]
train() client id: f_00002-7-0 loss: 1.229050  [   32/  124]
train() client id: f_00002-7-1 loss: 0.962376  [   64/  124]
train() client id: f_00002-7-2 loss: 0.911731  [   96/  124]
train() client id: f_00002-8-0 loss: 0.816159  [   32/  124]
train() client id: f_00002-8-1 loss: 0.948164  [   64/  124]
train() client id: f_00002-8-2 loss: 1.093783  [   96/  124]
train() client id: f_00002-9-0 loss: 0.973786  [   32/  124]
train() client id: f_00002-9-1 loss: 1.009459  [   64/  124]
train() client id: f_00002-9-2 loss: 1.065359  [   96/  124]
train() client id: f_00002-10-0 loss: 1.013221  [   32/  124]
train() client id: f_00002-10-1 loss: 0.858112  [   64/  124]
train() client id: f_00002-10-2 loss: 1.147964  [   96/  124]
train() client id: f_00002-11-0 loss: 0.879454  [   32/  124]
train() client id: f_00002-11-1 loss: 1.139081  [   64/  124]
train() client id: f_00002-11-2 loss: 1.017017  [   96/  124]
train() client id: f_00003-0-0 loss: 0.748927  [   32/   43]
train() client id: f_00003-1-0 loss: 0.721278  [   32/   43]
train() client id: f_00003-2-0 loss: 0.866520  [   32/   43]
train() client id: f_00003-3-0 loss: 0.887179  [   32/   43]
train() client id: f_00003-4-0 loss: 0.805776  [   32/   43]
train() client id: f_00003-5-0 loss: 0.770821  [   32/   43]
train() client id: f_00003-6-0 loss: 0.798936  [   32/   43]
train() client id: f_00003-7-0 loss: 0.886989  [   32/   43]
train() client id: f_00003-8-0 loss: 0.720126  [   32/   43]
train() client id: f_00003-9-0 loss: 0.686386  [   32/   43]
train() client id: f_00003-10-0 loss: 0.779258  [   32/   43]
train() client id: f_00003-11-0 loss: 0.830402  [   32/   43]
train() client id: f_00004-0-0 loss: 0.849930  [   32/  306]
train() client id: f_00004-0-1 loss: 0.819925  [   64/  306]
train() client id: f_00004-0-2 loss: 0.889383  [   96/  306]
train() client id: f_00004-0-3 loss: 0.837312  [  128/  306]
train() client id: f_00004-0-4 loss: 0.914096  [  160/  306]
train() client id: f_00004-0-5 loss: 0.746946  [  192/  306]
train() client id: f_00004-0-6 loss: 0.844555  [  224/  306]
train() client id: f_00004-0-7 loss: 0.829420  [  256/  306]
train() client id: f_00004-0-8 loss: 0.853403  [  288/  306]
train() client id: f_00004-1-0 loss: 0.885102  [   32/  306]
train() client id: f_00004-1-1 loss: 0.795514  [   64/  306]
train() client id: f_00004-1-2 loss: 0.907966  [   96/  306]
train() client id: f_00004-1-3 loss: 0.838673  [  128/  306]
train() client id: f_00004-1-4 loss: 0.820287  [  160/  306]
train() client id: f_00004-1-5 loss: 0.981527  [  192/  306]
train() client id: f_00004-1-6 loss: 0.788181  [  224/  306]
train() client id: f_00004-1-7 loss: 0.747573  [  256/  306]
train() client id: f_00004-1-8 loss: 0.812707  [  288/  306]
train() client id: f_00004-2-0 loss: 0.893789  [   32/  306]
train() client id: f_00004-2-1 loss: 0.737568  [   64/  306]
train() client id: f_00004-2-2 loss: 0.908303  [   96/  306]
train() client id: f_00004-2-3 loss: 0.798258  [  128/  306]
train() client id: f_00004-2-4 loss: 0.855642  [  160/  306]
train() client id: f_00004-2-5 loss: 0.827107  [  192/  306]
train() client id: f_00004-2-6 loss: 0.886526  [  224/  306]
train() client id: f_00004-2-7 loss: 0.798522  [  256/  306]
train() client id: f_00004-2-8 loss: 0.857206  [  288/  306]
train() client id: f_00004-3-0 loss: 0.866807  [   32/  306]
train() client id: f_00004-3-1 loss: 0.875490  [   64/  306]
train() client id: f_00004-3-2 loss: 0.836324  [   96/  306]
train() client id: f_00004-3-3 loss: 0.817918  [  128/  306]
train() client id: f_00004-3-4 loss: 0.833403  [  160/  306]
train() client id: f_00004-3-5 loss: 0.723207  [  192/  306]
train() client id: f_00004-3-6 loss: 0.970725  [  224/  306]
train() client id: f_00004-3-7 loss: 0.801941  [  256/  306]
train() client id: f_00004-3-8 loss: 0.800866  [  288/  306]
train() client id: f_00004-4-0 loss: 0.906296  [   32/  306]
train() client id: f_00004-4-1 loss: 0.871498  [   64/  306]
train() client id: f_00004-4-2 loss: 0.755882  [   96/  306]
train() client id: f_00004-4-3 loss: 0.772705  [  128/  306]
train() client id: f_00004-4-4 loss: 0.907019  [  160/  306]
train() client id: f_00004-4-5 loss: 0.811517  [  192/  306]
train() client id: f_00004-4-6 loss: 0.695604  [  224/  306]
train() client id: f_00004-4-7 loss: 0.986674  [  256/  306]
train() client id: f_00004-4-8 loss: 0.784500  [  288/  306]
train() client id: f_00004-5-0 loss: 0.851108  [   32/  306]
train() client id: f_00004-5-1 loss: 0.869525  [   64/  306]
train() client id: f_00004-5-2 loss: 0.765591  [   96/  306]
train() client id: f_00004-5-3 loss: 0.907817  [  128/  306]
train() client id: f_00004-5-4 loss: 0.766948  [  160/  306]
train() client id: f_00004-5-5 loss: 0.774942  [  192/  306]
train() client id: f_00004-5-6 loss: 0.818179  [  224/  306]
train() client id: f_00004-5-7 loss: 0.967173  [  256/  306]
train() client id: f_00004-5-8 loss: 0.820123  [  288/  306]
train() client id: f_00004-6-0 loss: 0.886505  [   32/  306]
train() client id: f_00004-6-1 loss: 0.777417  [   64/  306]
train() client id: f_00004-6-2 loss: 0.809744  [   96/  306]
train() client id: f_00004-6-3 loss: 0.847961  [  128/  306]
train() client id: f_00004-6-4 loss: 0.761110  [  160/  306]
train() client id: f_00004-6-5 loss: 0.757533  [  192/  306]
train() client id: f_00004-6-6 loss: 0.921031  [  224/  306]
train() client id: f_00004-6-7 loss: 0.862179  [  256/  306]
train() client id: f_00004-6-8 loss: 0.836351  [  288/  306]
train() client id: f_00004-7-0 loss: 0.794218  [   32/  306]
train() client id: f_00004-7-1 loss: 0.740009  [   64/  306]
train() client id: f_00004-7-2 loss: 0.798524  [   96/  306]
train() client id: f_00004-7-3 loss: 0.926767  [  128/  306]
train() client id: f_00004-7-4 loss: 0.866503  [  160/  306]
train() client id: f_00004-7-5 loss: 0.896389  [  192/  306]
train() client id: f_00004-7-6 loss: 0.781352  [  224/  306]
train() client id: f_00004-7-7 loss: 0.751150  [  256/  306]
train() client id: f_00004-7-8 loss: 0.852016  [  288/  306]
train() client id: f_00004-8-0 loss: 0.895303  [   32/  306]
train() client id: f_00004-8-1 loss: 0.860458  [   64/  306]
train() client id: f_00004-8-2 loss: 0.767480  [   96/  306]
train() client id: f_00004-8-3 loss: 0.894034  [  128/  306]
train() client id: f_00004-8-4 loss: 0.883030  [  160/  306]
train() client id: f_00004-8-5 loss: 0.788348  [  192/  306]
train() client id: f_00004-8-6 loss: 0.738685  [  224/  306]
train() client id: f_00004-8-7 loss: 0.908168  [  256/  306]
train() client id: f_00004-8-8 loss: 0.750796  [  288/  306]
train() client id: f_00004-9-0 loss: 0.807204  [   32/  306]
train() client id: f_00004-9-1 loss: 0.737192  [   64/  306]
train() client id: f_00004-9-2 loss: 0.902847  [   96/  306]
train() client id: f_00004-9-3 loss: 0.847445  [  128/  306]
train() client id: f_00004-9-4 loss: 0.754454  [  160/  306]
train() client id: f_00004-9-5 loss: 0.858105  [  192/  306]
train() client id: f_00004-9-6 loss: 0.937535  [  224/  306]
train() client id: f_00004-9-7 loss: 0.926838  [  256/  306]
train() client id: f_00004-9-8 loss: 0.826069  [  288/  306]
train() client id: f_00004-10-0 loss: 0.771774  [   32/  306]
train() client id: f_00004-10-1 loss: 0.849715  [   64/  306]
train() client id: f_00004-10-2 loss: 0.899017  [   96/  306]
train() client id: f_00004-10-3 loss: 0.857223  [  128/  306]
train() client id: f_00004-10-4 loss: 0.807790  [  160/  306]
train() client id: f_00004-10-5 loss: 0.812435  [  192/  306]
train() client id: f_00004-10-6 loss: 0.796318  [  224/  306]
train() client id: f_00004-10-7 loss: 0.882281  [  256/  306]
train() client id: f_00004-10-8 loss: 0.861810  [  288/  306]
train() client id: f_00004-11-0 loss: 0.711685  [   32/  306]
train() client id: f_00004-11-1 loss: 0.939178  [   64/  306]
train() client id: f_00004-11-2 loss: 0.834540  [   96/  306]
train() client id: f_00004-11-3 loss: 0.826409  [  128/  306]
train() client id: f_00004-11-4 loss: 0.840189  [  160/  306]
train() client id: f_00004-11-5 loss: 0.884958  [  192/  306]
train() client id: f_00004-11-6 loss: 0.842604  [  224/  306]
train() client id: f_00004-11-7 loss: 0.792206  [  256/  306]
train() client id: f_00004-11-8 loss: 0.906110  [  288/  306]
train() client id: f_00005-0-0 loss: 0.847265  [   32/  146]
train() client id: f_00005-0-1 loss: 0.479490  [   64/  146]
train() client id: f_00005-0-2 loss: 0.800889  [   96/  146]
train() client id: f_00005-0-3 loss: 0.591044  [  128/  146]
train() client id: f_00005-1-0 loss: 0.866526  [   32/  146]
train() client id: f_00005-1-1 loss: 0.595435  [   64/  146]
train() client id: f_00005-1-2 loss: 0.445100  [   96/  146]
train() client id: f_00005-1-3 loss: 0.751391  [  128/  146]
train() client id: f_00005-2-0 loss: 0.867726  [   32/  146]
train() client id: f_00005-2-1 loss: 0.785617  [   64/  146]
train() client id: f_00005-2-2 loss: 0.532745  [   96/  146]
train() client id: f_00005-2-3 loss: 0.547850  [  128/  146]
train() client id: f_00005-3-0 loss: 0.664350  [   32/  146]
train() client id: f_00005-3-1 loss: 0.867071  [   64/  146]
train() client id: f_00005-3-2 loss: 0.566478  [   96/  146]
train() client id: f_00005-3-3 loss: 0.751346  [  128/  146]
train() client id: f_00005-4-0 loss: 0.547300  [   32/  146]
train() client id: f_00005-4-1 loss: 0.604523  [   64/  146]
train() client id: f_00005-4-2 loss: 0.845960  [   96/  146]
train() client id: f_00005-4-3 loss: 0.622639  [  128/  146]
train() client id: f_00005-5-0 loss: 0.579313  [   32/  146]
train() client id: f_00005-5-1 loss: 0.370728  [   64/  146]
train() client id: f_00005-5-2 loss: 0.664786  [   96/  146]
train() client id: f_00005-5-3 loss: 1.062253  [  128/  146]
train() client id: f_00005-6-0 loss: 0.428315  [   32/  146]
train() client id: f_00005-6-1 loss: 0.809912  [   64/  146]
train() client id: f_00005-6-2 loss: 0.740489  [   96/  146]
train() client id: f_00005-6-3 loss: 0.779664  [  128/  146]
train() client id: f_00005-7-0 loss: 0.776910  [   32/  146]
train() client id: f_00005-7-1 loss: 0.578619  [   64/  146]
train() client id: f_00005-7-2 loss: 0.745962  [   96/  146]
train() client id: f_00005-7-3 loss: 0.423724  [  128/  146]
train() client id: f_00005-8-0 loss: 0.602304  [   32/  146]
train() client id: f_00005-8-1 loss: 0.715055  [   64/  146]
train() client id: f_00005-8-2 loss: 0.818602  [   96/  146]
train() client id: f_00005-8-3 loss: 0.539889  [  128/  146]
train() client id: f_00005-9-0 loss: 0.740208  [   32/  146]
train() client id: f_00005-9-1 loss: 0.605113  [   64/  146]
train() client id: f_00005-9-2 loss: 0.558745  [   96/  146]
train() client id: f_00005-9-3 loss: 0.520728  [  128/  146]
train() client id: f_00005-10-0 loss: 0.490674  [   32/  146]
train() client id: f_00005-10-1 loss: 0.910604  [   64/  146]
train() client id: f_00005-10-2 loss: 0.794670  [   96/  146]
train() client id: f_00005-10-3 loss: 0.578157  [  128/  146]
train() client id: f_00005-11-0 loss: 0.778191  [   32/  146]
train() client id: f_00005-11-1 loss: 0.624729  [   64/  146]
train() client id: f_00005-11-2 loss: 0.617778  [   96/  146]
train() client id: f_00005-11-3 loss: 0.679264  [  128/  146]
train() client id: f_00006-0-0 loss: 0.474321  [   32/   54]
train() client id: f_00006-1-0 loss: 0.477247  [   32/   54]
train() client id: f_00006-2-0 loss: 0.567435  [   32/   54]
train() client id: f_00006-3-0 loss: 0.567087  [   32/   54]
train() client id: f_00006-4-0 loss: 0.514651  [   32/   54]
train() client id: f_00006-5-0 loss: 0.566623  [   32/   54]
train() client id: f_00006-6-0 loss: 0.514246  [   32/   54]
train() client id: f_00006-7-0 loss: 0.566686  [   32/   54]
train() client id: f_00006-8-0 loss: 0.560640  [   32/   54]
train() client id: f_00006-9-0 loss: 0.517911  [   32/   54]
train() client id: f_00006-10-0 loss: 0.558440  [   32/   54]
train() client id: f_00006-11-0 loss: 0.568522  [   32/   54]
train() client id: f_00007-0-0 loss: 0.658204  [   32/  179]
train() client id: f_00007-0-1 loss: 0.728206  [   64/  179]
train() client id: f_00007-0-2 loss: 0.420254  [   96/  179]
train() client id: f_00007-0-3 loss: 0.683363  [  128/  179]
train() client id: f_00007-0-4 loss: 0.442745  [  160/  179]
train() client id: f_00007-1-0 loss: 0.708517  [   32/  179]
train() client id: f_00007-1-1 loss: 0.611460  [   64/  179]
train() client id: f_00007-1-2 loss: 0.436296  [   96/  179]
train() client id: f_00007-1-3 loss: 0.509083  [  128/  179]
train() client id: f_00007-1-4 loss: 0.652228  [  160/  179]
train() client id: f_00007-2-0 loss: 0.580846  [   32/  179]
train() client id: f_00007-2-1 loss: 0.490050  [   64/  179]
train() client id: f_00007-2-2 loss: 0.593239  [   96/  179]
train() client id: f_00007-2-3 loss: 0.632499  [  128/  179]
train() client id: f_00007-2-4 loss: 0.420904  [  160/  179]
train() client id: f_00007-3-0 loss: 0.759677  [   32/  179]
train() client id: f_00007-3-1 loss: 0.427161  [   64/  179]
train() client id: f_00007-3-2 loss: 0.591958  [   96/  179]
train() client id: f_00007-3-3 loss: 0.631901  [  128/  179]
train() client id: f_00007-3-4 loss: 0.402855  [  160/  179]
train() client id: f_00007-4-0 loss: 0.624965  [   32/  179]
train() client id: f_00007-4-1 loss: 0.482904  [   64/  179]
train() client id: f_00007-4-2 loss: 0.516321  [   96/  179]
train() client id: f_00007-4-3 loss: 0.412216  [  128/  179]
train() client id: f_00007-4-4 loss: 0.705831  [  160/  179]
train() client id: f_00007-5-0 loss: 0.393770  [   32/  179]
train() client id: f_00007-5-1 loss: 0.490618  [   64/  179]
train() client id: f_00007-5-2 loss: 0.535179  [   96/  179]
train() client id: f_00007-5-3 loss: 0.771604  [  128/  179]
train() client id: f_00007-5-4 loss: 0.493791  [  160/  179]
train() client id: f_00007-6-0 loss: 0.694039  [   32/  179]
train() client id: f_00007-6-1 loss: 0.378969  [   64/  179]
train() client id: f_00007-6-2 loss: 0.556085  [   96/  179]
train() client id: f_00007-6-3 loss: 0.486877  [  128/  179]
train() client id: f_00007-6-4 loss: 0.617197  [  160/  179]
train() client id: f_00007-7-0 loss: 0.376997  [   32/  179]
train() client id: f_00007-7-1 loss: 0.405991  [   64/  179]
train() client id: f_00007-7-2 loss: 0.683065  [   96/  179]
train() client id: f_00007-7-3 loss: 0.675346  [  128/  179]
train() client id: f_00007-7-4 loss: 0.509321  [  160/  179]
train() client id: f_00007-8-0 loss: 0.542852  [   32/  179]
train() client id: f_00007-8-1 loss: 0.406334  [   64/  179]
train() client id: f_00007-8-2 loss: 0.423856  [   96/  179]
train() client id: f_00007-8-3 loss: 0.394656  [  128/  179]
train() client id: f_00007-8-4 loss: 0.895344  [  160/  179]
train() client id: f_00007-9-0 loss: 0.589398  [   32/  179]
train() client id: f_00007-9-1 loss: 0.603420  [   64/  179]
train() client id: f_00007-9-2 loss: 0.525562  [   96/  179]
train() client id: f_00007-9-3 loss: 0.459387  [  128/  179]
train() client id: f_00007-9-4 loss: 0.539861  [  160/  179]
train() client id: f_00007-10-0 loss: 0.658215  [   32/  179]
train() client id: f_00007-10-1 loss: 0.401084  [   64/  179]
train() client id: f_00007-10-2 loss: 0.487647  [   96/  179]
train() client id: f_00007-10-3 loss: 0.539473  [  128/  179]
train() client id: f_00007-10-4 loss: 0.646562  [  160/  179]
train() client id: f_00007-11-0 loss: 0.386546  [   32/  179]
train() client id: f_00007-11-1 loss: 0.530679  [   64/  179]
train() client id: f_00007-11-2 loss: 0.868159  [   96/  179]
train() client id: f_00007-11-3 loss: 0.570171  [  128/  179]
train() client id: f_00007-11-4 loss: 0.372964  [  160/  179]
train() client id: f_00008-0-0 loss: 0.888562  [   32/  130]
train() client id: f_00008-0-1 loss: 0.652522  [   64/  130]
train() client id: f_00008-0-2 loss: 0.897975  [   96/  130]
train() client id: f_00008-0-3 loss: 0.749980  [  128/  130]
train() client id: f_00008-1-0 loss: 0.702316  [   32/  130]
train() client id: f_00008-1-1 loss: 0.800796  [   64/  130]
train() client id: f_00008-1-2 loss: 0.829201  [   96/  130]
train() client id: f_00008-1-3 loss: 0.862080  [  128/  130]
train() client id: f_00008-2-0 loss: 0.749576  [   32/  130]
train() client id: f_00008-2-1 loss: 0.757071  [   64/  130]
train() client id: f_00008-2-2 loss: 0.781664  [   96/  130]
train() client id: f_00008-2-3 loss: 0.892021  [  128/  130]
train() client id: f_00008-3-0 loss: 0.809149  [   32/  130]
train() client id: f_00008-3-1 loss: 0.891327  [   64/  130]
train() client id: f_00008-3-2 loss: 0.712987  [   96/  130]
train() client id: f_00008-3-3 loss: 0.780060  [  128/  130]
train() client id: f_00008-4-0 loss: 0.709226  [   32/  130]
train() client id: f_00008-4-1 loss: 0.737007  [   64/  130]
train() client id: f_00008-4-2 loss: 0.799795  [   96/  130]
train() client id: f_00008-4-3 loss: 0.917617  [  128/  130]
train() client id: f_00008-5-0 loss: 0.694884  [   32/  130]
train() client id: f_00008-5-1 loss: 0.811943  [   64/  130]
train() client id: f_00008-5-2 loss: 0.837899  [   96/  130]
train() client id: f_00008-5-3 loss: 0.838852  [  128/  130]
train() client id: f_00008-6-0 loss: 0.756112  [   32/  130]
train() client id: f_00008-6-1 loss: 0.831314  [   64/  130]
train() client id: f_00008-6-2 loss: 0.833729  [   96/  130]
train() client id: f_00008-6-3 loss: 0.732441  [  128/  130]
train() client id: f_00008-7-0 loss: 0.740278  [   32/  130]
train() client id: f_00008-7-1 loss: 0.904377  [   64/  130]
train() client id: f_00008-7-2 loss: 0.807577  [   96/  130]
train() client id: f_00008-7-3 loss: 0.726442  [  128/  130]
train() client id: f_00008-8-0 loss: 0.867016  [   32/  130]
train() client id: f_00008-8-1 loss: 0.803714  [   64/  130]
train() client id: f_00008-8-2 loss: 0.791810  [   96/  130]
train() client id: f_00008-8-3 loss: 0.711421  [  128/  130]
train() client id: f_00008-9-0 loss: 0.810294  [   32/  130]
train() client id: f_00008-9-1 loss: 0.790797  [   64/  130]
train() client id: f_00008-9-2 loss: 0.707814  [   96/  130]
train() client id: f_00008-9-3 loss: 0.867170  [  128/  130]
train() client id: f_00008-10-0 loss: 0.716594  [   32/  130]
train() client id: f_00008-10-1 loss: 0.709680  [   64/  130]
train() client id: f_00008-10-2 loss: 1.040625  [   96/  130]
train() client id: f_00008-10-3 loss: 0.711138  [  128/  130]
train() client id: f_00008-11-0 loss: 0.684464  [   32/  130]
train() client id: f_00008-11-1 loss: 0.788528  [   64/  130]
train() client id: f_00008-11-2 loss: 0.895927  [   96/  130]
train() client id: f_00008-11-3 loss: 0.804446  [  128/  130]
train() client id: f_00009-0-0 loss: 1.242259  [   32/  118]
train() client id: f_00009-0-1 loss: 1.157859  [   64/  118]
train() client id: f_00009-0-2 loss: 1.172351  [   96/  118]
train() client id: f_00009-1-0 loss: 1.041132  [   32/  118]
train() client id: f_00009-1-1 loss: 1.149189  [   64/  118]
train() client id: f_00009-1-2 loss: 1.093381  [   96/  118]
train() client id: f_00009-2-0 loss: 1.180181  [   32/  118]
train() client id: f_00009-2-1 loss: 1.165946  [   64/  118]
train() client id: f_00009-2-2 loss: 0.873043  [   96/  118]
train() client id: f_00009-3-0 loss: 1.068120  [   32/  118]
train() client id: f_00009-3-1 loss: 1.039455  [   64/  118]
train() client id: f_00009-3-2 loss: 0.921103  [   96/  118]
train() client id: f_00009-4-0 loss: 1.000002  [   32/  118]
train() client id: f_00009-4-1 loss: 1.002786  [   64/  118]
train() client id: f_00009-4-2 loss: 0.944952  [   96/  118]
train() client id: f_00009-5-0 loss: 0.880991  [   32/  118]
train() client id: f_00009-5-1 loss: 0.978633  [   64/  118]
train() client id: f_00009-5-2 loss: 0.915929  [   96/  118]
train() client id: f_00009-6-0 loss: 0.941749  [   32/  118]
train() client id: f_00009-6-1 loss: 0.833623  [   64/  118]
train() client id: f_00009-6-2 loss: 0.963287  [   96/  118]
train() client id: f_00009-7-0 loss: 0.968993  [   32/  118]
train() client id: f_00009-7-1 loss: 0.963553  [   64/  118]
train() client id: f_00009-7-2 loss: 0.873630  [   96/  118]
train() client id: f_00009-8-0 loss: 0.953062  [   32/  118]
train() client id: f_00009-8-1 loss: 0.947156  [   64/  118]
train() client id: f_00009-8-2 loss: 0.899416  [   96/  118]
train() client id: f_00009-9-0 loss: 0.899242  [   32/  118]
train() client id: f_00009-9-1 loss: 0.781724  [   64/  118]
train() client id: f_00009-9-2 loss: 0.973731  [   96/  118]
train() client id: f_00009-10-0 loss: 0.838360  [   32/  118]
train() client id: f_00009-10-1 loss: 0.841517  [   64/  118]
train() client id: f_00009-10-2 loss: 1.008412  [   96/  118]
train() client id: f_00009-11-0 loss: 0.842527  [   32/  118]
train() client id: f_00009-11-1 loss: 0.967747  [   64/  118]
train() client id: f_00009-11-2 loss: 0.844781  [   96/  118]
At round 19 accuracy: 0.6419098143236074
At round 19 training accuracy: 0.5801475519785378
At round 19 training loss: 0.832119671626963
update_location
xs = -4.528292 -3.998411 5.045120 -5.943528 -95.103519 -30.217951 -47.215960 68.375741 -1.680116 -0.304393 
ys = 82.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 -3.124922 -19.177652 -12.154970 4.001482 
xs mean: -11.557130840587133
ys mean: 8.371751218646876
dists_uav = 129.773944 101.281644 100.135894 102.682007 138.318854 105.870099 110.630520 122.650007 100.750018 100.080490 
uav_gains = -102.830327 -100.138288 -100.014761 -100.287382 -103.523569 -100.619369 -101.096941 -102.216980 -100.081146 -100.008752 
uav_gains_db_mean: -101.0817514992845
dists_bs = 193.908712 233.803302 250.161496 227.645934 183.911142 240.618458 219.219636 311.346610 255.081879 244.456639 
bs_gains = -103.619784 -105.894877 -106.717232 -105.570336 -102.976084 -106.244269 -105.111684 -109.377895 -106.954088 -106.436711 
bs_gains_db_mean: -105.89029611238827
Round 20
-------------------------------
ene_coms = [0.00713981 0.00833195 0.00630628 0.00637849 0.0071889  0.00849282
 0.00660288 0.00694028 0.00883881 0.00858402]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.34192636 17.4004996   8.19399659  2.9317844  20.04024646  9.67077574
  3.65145209 11.78182096  8.63847409  7.85316654]
obj_prev = 98.5041428349532
eta_min = 6.894101786431494e-12	eta_max = 0.9339132714474644
af = 20.82950184691336	bf = 1.595700512214362	zeta = 22.912452031604698	eta = 0.9090909090909091
af = 20.82950184691336	bf = 1.595700512214362	zeta = 39.39060486326011	eta = 0.5287936531876204
af = 20.82950184691336	bf = 1.595700512214362	zeta = 31.551704197717484	eta = 0.6601704211089875
af = 20.82950184691336	bf = 1.595700512214362	zeta = 30.148354014654362	eta = 0.6909001346073043
af = 20.82950184691336	bf = 1.595700512214362	zeta = 30.0796536002328	eta = 0.6924781157304335
af = 20.82950184691336	bf = 1.595700512214362	zeta = 30.07947747371303	eta = 0.6924821704471634
eta = 0.6924821704471634
ene_coms = [0.00713981 0.00833195 0.00630628 0.00637849 0.0071889  0.00849282
 0.00660288 0.00694028 0.00883881 0.00858402]
ene_comp = [0.03032297 0.06377451 0.02984166 0.01034832 0.07364151 0.03513614
 0.01299556 0.04307788 0.03128561 0.02839771]
ene_total = [2.59869276 5.00183138 2.50748539 1.16029374 5.60698796 3.02642423
 1.35949133 3.46962503 2.78332296 2.56532269]
ti_comp = [0.34666501 0.33474362 0.35500031 0.35427821 0.34617417 0.3331349
 0.3520343  0.34866033 0.32967504 0.33222296]
ti_coms = [0.07139811 0.08331951 0.06306282 0.06378492 0.07188895 0.08492822
 0.06602882 0.06940279 0.08838808 0.08584016]
t_total = [28.99991608 28.99991608 28.99991608 28.99991608 28.99991608 28.99991608
 28.99991608 28.99991608 28.99991608 28.99991608]
ene_coms = [0.00713981 0.00833195 0.00630628 0.00637849 0.0071889  0.00849282
 0.00660288 0.00694028 0.00883881 0.00858402]
ene_comp = [1.45002456e-05 1.44676089e-04 1.31792697e-05 5.51823556e-07
 2.08285343e-04 2.44287953e-05 1.10686606e-06 4.10996155e-05
 1.76092826e-05 1.29679245e-05]
ene_total = [0.49627536 0.58800083 0.43836403 0.4424971  0.51312256 0.59081884
 0.45810101 0.48427948 0.61434588 0.59634968]
optimize_network iter = 0 obj = 5.222154777783128
eta = 0.6924821704471634
freqs = [4.37352668e+07 9.52587366e+07 4.20304659e+07 1.46047882e+07
 1.06364821e+08 5.27356096e+07 1.84578040e+07 6.17762779e+07
 4.74491600e+07 4.27389262e+07]
eta_min = 0.6924821704472442	eta_max = 0.6924821704471609
af = 0.027771282594377243	bf = 1.595700512214362	zeta = 0.03054841085381497	eta = 0.909090909090909
af = 0.027771282594377243	bf = 1.595700512214362	zeta = 17.58182137967009	eta = 0.0015795452584047554
af = 0.027771282594377243	bf = 1.595700512214362	zeta = 1.7776503346590216	eta = 0.01562246638324638
af = 0.027771282594377243	bf = 1.595700512214362	zeta = 1.7383600038407956	eta = 0.015975564631617367
af = 0.027771282594377243	bf = 1.595700512214362	zeta = 1.738353007067986	eta = 0.01597562893236398
eta = 0.01597562893236398
ene_coms = [0.00713981 0.00833195 0.00630628 0.00637849 0.0071889  0.00849282
 0.00660288 0.00694028 0.00883881 0.00858402]
ene_comp = [1.63231227e-04 1.62863832e-03 1.48360823e-04 6.21195246e-06
 2.34469630e-03 2.74998255e-04 1.24601410e-05 4.62663934e-04
 1.98230077e-04 1.45981681e-04]
ene_total = [0.15831539 0.2159257  0.13992378 0.13840764 0.20666926 0.19006886
 0.14340744 0.16048104 0.19590496 0.18924894]
ti_comp = [0.34666501 0.33474362 0.35500031 0.35427821 0.34617417 0.3331349
 0.3520343  0.34866033 0.32967504 0.33222296]
ti_coms = [0.07139811 0.08331951 0.06306282 0.06378492 0.07188895 0.08492822
 0.06602882 0.06940279 0.08838808 0.08584016]
t_total = [28.99991608 28.99991608 28.99991608 28.99991608 28.99991608 28.99991608
 28.99991608 28.99991608 28.99991608 28.99991608]
ene_coms = [0.00713981 0.00833195 0.00630628 0.00637849 0.0071889  0.00849282
 0.00660288 0.00694028 0.00883881 0.00858402]
ene_comp = [1.45002456e-05 1.44676089e-04 1.31792697e-05 5.51823556e-07
 2.08285343e-04 2.44287953e-05 1.10686606e-06 4.10996155e-05
 1.76092826e-05 1.29679245e-05]
ene_total = [0.49627536 0.58800083 0.43836403 0.4424971  0.51312256 0.59081884
 0.45810101 0.48427948 0.61434588 0.59634968]
optimize_network iter = 1 obj = 5.222154777784491
eta = 0.6924821704472442
freqs = [4.37352668e+07 9.52587366e+07 4.20304659e+07 1.46047882e+07
 1.06364821e+08 5.27356096e+07 1.84578040e+07 6.17762779e+07
 4.74491600e+07 4.27389262e+07]
Done!
ene_coms = [0.00713981 0.00833195 0.00630628 0.00637849 0.0071889  0.00849282
 0.00660288 0.00694028 0.00883881 0.00858402]
ene_comp = [1.44605681e-05 1.44280208e-04 1.31432069e-05 5.50313584e-07
 2.07715406e-04 2.43619500e-05 1.10383731e-06 4.09871533e-05
 1.75610977e-05 1.29324399e-05]
ene_total = [0.00715427 0.00847623 0.00631942 0.00637904 0.00739661 0.00851718
 0.00660399 0.00698127 0.00885637 0.00859695]
At round 20 energy consumption: 0.07528133427135947
At round 20 eta: 0.6924821704472442
At round 20 a_n: 21.33168591686803
At round 20 local rounds: 12.032926061459534
At round 20 global rounds: 69.36731423960738
gradient difference: 0.40016859769821167
train() client id: f_00000-0-0 loss: 1.198879  [   32/  126]
train() client id: f_00000-0-1 loss: 0.963042  [   64/  126]
train() client id: f_00000-0-2 loss: 1.165044  [   96/  126]
train() client id: f_00000-1-0 loss: 1.150437  [   32/  126]
train() client id: f_00000-1-1 loss: 0.899574  [   64/  126]
train() client id: f_00000-1-2 loss: 0.984743  [   96/  126]
train() client id: f_00000-2-0 loss: 0.952503  [   32/  126]
train() client id: f_00000-2-1 loss: 1.050310  [   64/  126]
train() client id: f_00000-2-2 loss: 0.949925  [   96/  126]
train() client id: f_00000-3-0 loss: 1.070765  [   32/  126]
train() client id: f_00000-3-1 loss: 1.015449  [   64/  126]
train() client id: f_00000-3-2 loss: 0.889313  [   96/  126]
train() client id: f_00000-4-0 loss: 0.931980  [   32/  126]
train() client id: f_00000-4-1 loss: 0.836241  [   64/  126]
train() client id: f_00000-4-2 loss: 0.987601  [   96/  126]
train() client id: f_00000-5-0 loss: 0.837074  [   32/  126]
train() client id: f_00000-5-1 loss: 0.898272  [   64/  126]
train() client id: f_00000-5-2 loss: 0.874867  [   96/  126]
train() client id: f_00000-6-0 loss: 0.852272  [   32/  126]
train() client id: f_00000-6-1 loss: 0.896040  [   64/  126]
train() client id: f_00000-6-2 loss: 0.844468  [   96/  126]
train() client id: f_00000-7-0 loss: 0.775797  [   32/  126]
train() client id: f_00000-7-1 loss: 0.916285  [   64/  126]
train() client id: f_00000-7-2 loss: 0.824094  [   96/  126]
train() client id: f_00000-8-0 loss: 0.774769  [   32/  126]
train() client id: f_00000-8-1 loss: 0.903586  [   64/  126]
train() client id: f_00000-8-2 loss: 0.770064  [   96/  126]
train() client id: f_00000-9-0 loss: 0.840373  [   32/  126]
train() client id: f_00000-9-1 loss: 0.755693  [   64/  126]
train() client id: f_00000-9-2 loss: 0.895071  [   96/  126]
train() client id: f_00000-10-0 loss: 0.861269  [   32/  126]
train() client id: f_00000-10-1 loss: 0.770610  [   64/  126]
train() client id: f_00000-10-2 loss: 0.849217  [   96/  126]
train() client id: f_00000-11-0 loss: 0.764890  [   32/  126]
train() client id: f_00000-11-1 loss: 0.802974  [   64/  126]
train() client id: f_00000-11-2 loss: 0.814578  [   96/  126]
train() client id: f_00001-0-0 loss: 0.339465  [   32/  265]
train() client id: f_00001-0-1 loss: 0.338758  [   64/  265]
train() client id: f_00001-0-2 loss: 0.385642  [   96/  265]
train() client id: f_00001-0-3 loss: 0.366246  [  128/  265]
train() client id: f_00001-0-4 loss: 0.351926  [  160/  265]
train() client id: f_00001-0-5 loss: 0.325895  [  192/  265]
train() client id: f_00001-0-6 loss: 0.347743  [  224/  265]
train() client id: f_00001-0-7 loss: 0.405083  [  256/  265]
train() client id: f_00001-1-0 loss: 0.314797  [   32/  265]
train() client id: f_00001-1-1 loss: 0.355297  [   64/  265]
train() client id: f_00001-1-2 loss: 0.283616  [   96/  265]
train() client id: f_00001-1-3 loss: 0.340803  [  128/  265]
train() client id: f_00001-1-4 loss: 0.338312  [  160/  265]
train() client id: f_00001-1-5 loss: 0.309772  [  192/  265]
train() client id: f_00001-1-6 loss: 0.441252  [  224/  265]
train() client id: f_00001-1-7 loss: 0.415025  [  256/  265]
train() client id: f_00001-2-0 loss: 0.291546  [   32/  265]
train() client id: f_00001-2-1 loss: 0.487719  [   64/  265]
train() client id: f_00001-2-2 loss: 0.240546  [   96/  265]
train() client id: f_00001-2-3 loss: 0.281211  [  128/  265]
train() client id: f_00001-2-4 loss: 0.391740  [  160/  265]
train() client id: f_00001-2-5 loss: 0.423615  [  192/  265]
train() client id: f_00001-2-6 loss: 0.328543  [  224/  265]
train() client id: f_00001-2-7 loss: 0.277339  [  256/  265]
train() client id: f_00001-3-0 loss: 0.323314  [   32/  265]
train() client id: f_00001-3-1 loss: 0.301605  [   64/  265]
train() client id: f_00001-3-2 loss: 0.554421  [   96/  265]
train() client id: f_00001-3-3 loss: 0.250522  [  128/  265]
train() client id: f_00001-3-4 loss: 0.305481  [  160/  265]
train() client id: f_00001-3-5 loss: 0.333963  [  192/  265]
train() client id: f_00001-3-6 loss: 0.234628  [  224/  265]
train() client id: f_00001-3-7 loss: 0.349091  [  256/  265]
train() client id: f_00001-4-0 loss: 0.331406  [   32/  265]
train() client id: f_00001-4-1 loss: 0.389518  [   64/  265]
train() client id: f_00001-4-2 loss: 0.345623  [   96/  265]
train() client id: f_00001-4-3 loss: 0.250520  [  128/  265]
train() client id: f_00001-4-4 loss: 0.378914  [  160/  265]
train() client id: f_00001-4-5 loss: 0.305366  [  192/  265]
train() client id: f_00001-4-6 loss: 0.273125  [  224/  265]
train() client id: f_00001-4-7 loss: 0.238264  [  256/  265]
train() client id: f_00001-5-0 loss: 0.338605  [   32/  265]
train() client id: f_00001-5-1 loss: 0.302402  [   64/  265]
train() client id: f_00001-5-2 loss: 0.242441  [   96/  265]
train() client id: f_00001-5-3 loss: 0.231725  [  128/  265]
train() client id: f_00001-5-4 loss: 0.393115  [  160/  265]
train() client id: f_00001-5-5 loss: 0.317723  [  192/  265]
train() client id: f_00001-5-6 loss: 0.318478  [  224/  265]
train() client id: f_00001-5-7 loss: 0.331352  [  256/  265]
train() client id: f_00001-6-0 loss: 0.234740  [   32/  265]
train() client id: f_00001-6-1 loss: 0.289970  [   64/  265]
train() client id: f_00001-6-2 loss: 0.239492  [   96/  265]
train() client id: f_00001-6-3 loss: 0.505557  [  128/  265]
train() client id: f_00001-6-4 loss: 0.281646  [  160/  265]
train() client id: f_00001-6-5 loss: 0.324905  [  192/  265]
train() client id: f_00001-6-6 loss: 0.278574  [  224/  265]
train() client id: f_00001-6-7 loss: 0.364124  [  256/  265]
train() client id: f_00001-7-0 loss: 0.331477  [   32/  265]
train() client id: f_00001-7-1 loss: 0.210115  [   64/  265]
train() client id: f_00001-7-2 loss: 0.263506  [   96/  265]
train() client id: f_00001-7-3 loss: 0.386313  [  128/  265]
train() client id: f_00001-7-4 loss: 0.279284  [  160/  265]
train() client id: f_00001-7-5 loss: 0.219349  [  192/  265]
train() client id: f_00001-7-6 loss: 0.237370  [  224/  265]
train() client id: f_00001-7-7 loss: 0.533549  [  256/  265]
train() client id: f_00001-8-0 loss: 0.386881  [   32/  265]
train() client id: f_00001-8-1 loss: 0.480450  [   64/  265]
train() client id: f_00001-8-2 loss: 0.272917  [   96/  265]
train() client id: f_00001-8-3 loss: 0.219403  [  128/  265]
train() client id: f_00001-8-4 loss: 0.320202  [  160/  265]
train() client id: f_00001-8-5 loss: 0.292263  [  192/  265]
train() client id: f_00001-8-6 loss: 0.285326  [  224/  265]
train() client id: f_00001-8-7 loss: 0.202108  [  256/  265]
train() client id: f_00001-9-0 loss: 0.215049  [   32/  265]
train() client id: f_00001-9-1 loss: 0.259343  [   64/  265]
train() client id: f_00001-9-2 loss: 0.263760  [   96/  265]
train() client id: f_00001-9-3 loss: 0.526435  [  128/  265]
train() client id: f_00001-9-4 loss: 0.265700  [  160/  265]
train() client id: f_00001-9-5 loss: 0.337364  [  192/  265]
train() client id: f_00001-9-6 loss: 0.213700  [  224/  265]
train() client id: f_00001-9-7 loss: 0.286979  [  256/  265]
train() client id: f_00001-10-0 loss: 0.203535  [   32/  265]
train() client id: f_00001-10-1 loss: 0.403385  [   64/  265]
train() client id: f_00001-10-2 loss: 0.285902  [   96/  265]
train() client id: f_00001-10-3 loss: 0.276713  [  128/  265]
train() client id: f_00001-10-4 loss: 0.375137  [  160/  265]
train() client id: f_00001-10-5 loss: 0.293671  [  192/  265]
train() client id: f_00001-10-6 loss: 0.253936  [  224/  265]
train() client id: f_00001-10-7 loss: 0.324434  [  256/  265]
train() client id: f_00001-11-0 loss: 0.347066  [   32/  265]
train() client id: f_00001-11-1 loss: 0.347804  [   64/  265]
train() client id: f_00001-11-2 loss: 0.404756  [   96/  265]
train() client id: f_00001-11-3 loss: 0.218955  [  128/  265]
train() client id: f_00001-11-4 loss: 0.307204  [  160/  265]
train() client id: f_00001-11-5 loss: 0.328375  [  192/  265]
train() client id: f_00001-11-6 loss: 0.214035  [  224/  265]
train() client id: f_00001-11-7 loss: 0.240555  [  256/  265]
train() client id: f_00002-0-0 loss: 1.060052  [   32/  124]
train() client id: f_00002-0-1 loss: 1.203500  [   64/  124]
train() client id: f_00002-0-2 loss: 1.121412  [   96/  124]
train() client id: f_00002-1-0 loss: 1.092245  [   32/  124]
train() client id: f_00002-1-1 loss: 1.116071  [   64/  124]
train() client id: f_00002-1-2 loss: 1.062240  [   96/  124]
train() client id: f_00002-2-0 loss: 0.922691  [   32/  124]
train() client id: f_00002-2-1 loss: 1.099824  [   64/  124]
train() client id: f_00002-2-2 loss: 1.191954  [   96/  124]
train() client id: f_00002-3-0 loss: 1.028583  [   32/  124]
train() client id: f_00002-3-1 loss: 1.062248  [   64/  124]
train() client id: f_00002-3-2 loss: 1.048597  [   96/  124]
train() client id: f_00002-4-0 loss: 1.079117  [   32/  124]
train() client id: f_00002-4-1 loss: 1.083887  [   64/  124]
train() client id: f_00002-4-2 loss: 0.955826  [   96/  124]
train() client id: f_00002-5-0 loss: 1.129045  [   32/  124]
train() client id: f_00002-5-1 loss: 0.923401  [   64/  124]
train() client id: f_00002-5-2 loss: 1.033794  [   96/  124]
train() client id: f_00002-6-0 loss: 1.111056  [   32/  124]
train() client id: f_00002-6-1 loss: 0.969292  [   64/  124]
train() client id: f_00002-6-2 loss: 0.948142  [   96/  124]
train() client id: f_00002-7-0 loss: 0.948874  [   32/  124]
train() client id: f_00002-7-1 loss: 1.063919  [   64/  124]
train() client id: f_00002-7-2 loss: 0.957561  [   96/  124]
train() client id: f_00002-8-0 loss: 1.019626  [   32/  124]
train() client id: f_00002-8-1 loss: 0.809890  [   64/  124]
train() client id: f_00002-8-2 loss: 1.000414  [   96/  124]
train() client id: f_00002-9-0 loss: 0.934546  [   32/  124]
train() client id: f_00002-9-1 loss: 1.119354  [   64/  124]
train() client id: f_00002-9-2 loss: 0.886252  [   96/  124]
train() client id: f_00002-10-0 loss: 0.941824  [   32/  124]
train() client id: f_00002-10-1 loss: 1.201449  [   64/  124]
train() client id: f_00002-10-2 loss: 0.915272  [   96/  124]
train() client id: f_00002-11-0 loss: 0.931422  [   32/  124]
train() client id: f_00002-11-1 loss: 0.905834  [   64/  124]
train() client id: f_00002-11-2 loss: 1.011674  [   96/  124]
train() client id: f_00003-0-0 loss: 0.809073  [   32/   43]
train() client id: f_00003-1-0 loss: 0.741395  [   32/   43]
train() client id: f_00003-2-0 loss: 0.798684  [   32/   43]
train() client id: f_00003-3-0 loss: 0.807730  [   32/   43]
train() client id: f_00003-4-0 loss: 0.783048  [   32/   43]
train() client id: f_00003-5-0 loss: 0.820470  [   32/   43]
train() client id: f_00003-6-0 loss: 0.728725  [   32/   43]
train() client id: f_00003-7-0 loss: 0.730737  [   32/   43]
train() client id: f_00003-8-0 loss: 0.683939  [   32/   43]
train() client id: f_00003-9-0 loss: 0.843508  [   32/   43]
train() client id: f_00003-10-0 loss: 0.804934  [   32/   43]
train() client id: f_00003-11-0 loss: 0.828146  [   32/   43]
train() client id: f_00004-0-0 loss: 0.804944  [   32/  306]
train() client id: f_00004-0-1 loss: 0.821828  [   64/  306]
train() client id: f_00004-0-2 loss: 0.774492  [   96/  306]
train() client id: f_00004-0-3 loss: 0.852502  [  128/  306]
train() client id: f_00004-0-4 loss: 0.765887  [  160/  306]
train() client id: f_00004-0-5 loss: 0.745162  [  192/  306]
train() client id: f_00004-0-6 loss: 0.776911  [  224/  306]
train() client id: f_00004-0-7 loss: 0.891774  [  256/  306]
train() client id: f_00004-0-8 loss: 0.737576  [  288/  306]
train() client id: f_00004-1-0 loss: 0.774925  [   32/  306]
train() client id: f_00004-1-1 loss: 0.981618  [   64/  306]
train() client id: f_00004-1-2 loss: 0.855065  [   96/  306]
train() client id: f_00004-1-3 loss: 0.763798  [  128/  306]
train() client id: f_00004-1-4 loss: 0.762205  [  160/  306]
train() client id: f_00004-1-5 loss: 0.734312  [  192/  306]
train() client id: f_00004-1-6 loss: 0.775488  [  224/  306]
train() client id: f_00004-1-7 loss: 0.747939  [  256/  306]
train() client id: f_00004-1-8 loss: 0.902605  [  288/  306]
train() client id: f_00004-2-0 loss: 0.831642  [   32/  306]
train() client id: f_00004-2-1 loss: 0.714446  [   64/  306]
train() client id: f_00004-2-2 loss: 0.813874  [   96/  306]
train() client id: f_00004-2-3 loss: 0.843640  [  128/  306]
train() client id: f_00004-2-4 loss: 0.843524  [  160/  306]
train() client id: f_00004-2-5 loss: 0.769863  [  192/  306]
train() client id: f_00004-2-6 loss: 0.806621  [  224/  306]
train() client id: f_00004-2-7 loss: 0.906231  [  256/  306]
train() client id: f_00004-2-8 loss: 0.740843  [  288/  306]
train() client id: f_00004-3-0 loss: 0.741934  [   32/  306]
train() client id: f_00004-3-1 loss: 0.762863  [   64/  306]
train() client id: f_00004-3-2 loss: 0.922936  [   96/  306]
train() client id: f_00004-3-3 loss: 0.878926  [  128/  306]
train() client id: f_00004-3-4 loss: 0.875893  [  160/  306]
train() client id: f_00004-3-5 loss: 0.718777  [  192/  306]
train() client id: f_00004-3-6 loss: 0.677734  [  224/  306]
train() client id: f_00004-3-7 loss: 0.802739  [  256/  306]
train() client id: f_00004-3-8 loss: 0.802706  [  288/  306]
train() client id: f_00004-4-0 loss: 0.784670  [   32/  306]
train() client id: f_00004-4-1 loss: 0.876790  [   64/  306]
train() client id: f_00004-4-2 loss: 0.760378  [   96/  306]
train() client id: f_00004-4-3 loss: 0.976357  [  128/  306]
train() client id: f_00004-4-4 loss: 0.803078  [  160/  306]
train() client id: f_00004-4-5 loss: 0.737249  [  192/  306]
train() client id: f_00004-4-6 loss: 0.864547  [  224/  306]
train() client id: f_00004-4-7 loss: 0.821535  [  256/  306]
train() client id: f_00004-4-8 loss: 0.673338  [  288/  306]
train() client id: f_00004-5-0 loss: 0.840062  [   32/  306]
train() client id: f_00004-5-1 loss: 0.741677  [   64/  306]
train() client id: f_00004-5-2 loss: 0.805299  [   96/  306]
train() client id: f_00004-5-3 loss: 0.863118  [  128/  306]
train() client id: f_00004-5-4 loss: 0.816396  [  160/  306]
train() client id: f_00004-5-5 loss: 0.900272  [  192/  306]
train() client id: f_00004-5-6 loss: 0.760659  [  224/  306]
train() client id: f_00004-5-7 loss: 0.809685  [  256/  306]
train() client id: f_00004-5-8 loss: 0.689743  [  288/  306]
train() client id: f_00004-6-0 loss: 0.905881  [   32/  306]
train() client id: f_00004-6-1 loss: 0.811562  [   64/  306]
train() client id: f_00004-6-2 loss: 0.843199  [   96/  306]
train() client id: f_00004-6-3 loss: 0.739078  [  128/  306]
train() client id: f_00004-6-4 loss: 0.767891  [  160/  306]
train() client id: f_00004-6-5 loss: 0.842821  [  192/  306]
train() client id: f_00004-6-6 loss: 0.764754  [  224/  306]
train() client id: f_00004-6-7 loss: 0.740209  [  256/  306]
train() client id: f_00004-6-8 loss: 0.722706  [  288/  306]
train() client id: f_00004-7-0 loss: 0.699045  [   32/  306]
train() client id: f_00004-7-1 loss: 0.863633  [   64/  306]
train() client id: f_00004-7-2 loss: 0.781849  [   96/  306]
train() client id: f_00004-7-3 loss: 0.803249  [  128/  306]
train() client id: f_00004-7-4 loss: 0.738718  [  160/  306]
train() client id: f_00004-7-5 loss: 0.841651  [  192/  306]
train() client id: f_00004-7-6 loss: 0.891693  [  224/  306]
train() client id: f_00004-7-7 loss: 0.793964  [  256/  306]
train() client id: f_00004-7-8 loss: 0.751742  [  288/  306]
train() client id: f_00004-8-0 loss: 0.872037  [   32/  306]
train() client id: f_00004-8-1 loss: 0.730547  [   64/  306]
train() client id: f_00004-8-2 loss: 0.909677  [   96/  306]
train() client id: f_00004-8-3 loss: 0.726510  [  128/  306]
train() client id: f_00004-8-4 loss: 0.745066  [  160/  306]
train() client id: f_00004-8-5 loss: 0.698143  [  192/  306]
train() client id: f_00004-8-6 loss: 0.818459  [  224/  306]
train() client id: f_00004-8-7 loss: 0.790321  [  256/  306]
train() client id: f_00004-8-8 loss: 0.813458  [  288/  306]
train() client id: f_00004-9-0 loss: 0.835186  [   32/  306]
train() client id: f_00004-9-1 loss: 0.796241  [   64/  306]
train() client id: f_00004-9-2 loss: 0.754128  [   96/  306]
train() client id: f_00004-9-3 loss: 0.799219  [  128/  306]
train() client id: f_00004-9-4 loss: 0.866406  [  160/  306]
train() client id: f_00004-9-5 loss: 0.808658  [  192/  306]
train() client id: f_00004-9-6 loss: 0.843641  [  224/  306]
train() client id: f_00004-9-7 loss: 0.747059  [  256/  306]
train() client id: f_00004-9-8 loss: 0.754695  [  288/  306]
train() client id: f_00004-10-0 loss: 0.753331  [   32/  306]
train() client id: f_00004-10-1 loss: 0.859781  [   64/  306]
train() client id: f_00004-10-2 loss: 0.798672  [   96/  306]
train() client id: f_00004-10-3 loss: 0.726717  [  128/  306]
train() client id: f_00004-10-4 loss: 0.684294  [  160/  306]
train() client id: f_00004-10-5 loss: 0.873406  [  192/  306]
train() client id: f_00004-10-6 loss: 0.838726  [  224/  306]
train() client id: f_00004-10-7 loss: 0.803441  [  256/  306]
train() client id: f_00004-10-8 loss: 0.843604  [  288/  306]
train() client id: f_00004-11-0 loss: 0.720056  [   32/  306]
train() client id: f_00004-11-1 loss: 0.769310  [   64/  306]
train() client id: f_00004-11-2 loss: 0.739505  [   96/  306]
train() client id: f_00004-11-3 loss: 0.816512  [  128/  306]
train() client id: f_00004-11-4 loss: 0.921659  [  160/  306]
train() client id: f_00004-11-5 loss: 0.768588  [  192/  306]
train() client id: f_00004-11-6 loss: 0.883232  [  224/  306]
train() client id: f_00004-11-7 loss: 0.741712  [  256/  306]
train() client id: f_00004-11-8 loss: 0.727129  [  288/  306]
train() client id: f_00005-0-0 loss: 0.640060  [   32/  146]
train() client id: f_00005-0-1 loss: 0.505089  [   64/  146]
train() client id: f_00005-0-2 loss: 0.646926  [   96/  146]
train() client id: f_00005-0-3 loss: 0.693737  [  128/  146]
train() client id: f_00005-1-0 loss: 0.463737  [   32/  146]
train() client id: f_00005-1-1 loss: 0.805784  [   64/  146]
train() client id: f_00005-1-2 loss: 0.591649  [   96/  146]
train() client id: f_00005-1-3 loss: 0.627631  [  128/  146]
train() client id: f_00005-2-0 loss: 0.553465  [   32/  146]
train() client id: f_00005-2-1 loss: 0.721764  [   64/  146]
train() client id: f_00005-2-2 loss: 0.532999  [   96/  146]
train() client id: f_00005-2-3 loss: 0.721819  [  128/  146]
train() client id: f_00005-3-0 loss: 0.695749  [   32/  146]
train() client id: f_00005-3-1 loss: 0.673918  [   64/  146]
train() client id: f_00005-3-2 loss: 0.579829  [   96/  146]
train() client id: f_00005-3-3 loss: 0.550899  [  128/  146]
train() client id: f_00005-4-0 loss: 0.502764  [   32/  146]
train() client id: f_00005-4-1 loss: 0.738127  [   64/  146]
train() client id: f_00005-4-2 loss: 0.655082  [   96/  146]
train() client id: f_00005-4-3 loss: 0.557554  [  128/  146]
train() client id: f_00005-5-0 loss: 0.700642  [   32/  146]
train() client id: f_00005-5-1 loss: 0.633758  [   64/  146]
train() client id: f_00005-5-2 loss: 0.710116  [   96/  146]
train() client id: f_00005-5-3 loss: 0.463684  [  128/  146]
train() client id: f_00005-6-0 loss: 0.698084  [   32/  146]
train() client id: f_00005-6-1 loss: 0.552864  [   64/  146]
train() client id: f_00005-6-2 loss: 0.798790  [   96/  146]
train() client id: f_00005-6-3 loss: 0.404500  [  128/  146]
train() client id: f_00005-7-0 loss: 0.539196  [   32/  146]
train() client id: f_00005-7-1 loss: 0.654336  [   64/  146]
train() client id: f_00005-7-2 loss: 0.817294  [   96/  146]
train() client id: f_00005-7-3 loss: 0.502722  [  128/  146]
train() client id: f_00005-8-0 loss: 0.385519  [   32/  146]
train() client id: f_00005-8-1 loss: 0.831209  [   64/  146]
train() client id: f_00005-8-2 loss: 0.467408  [   96/  146]
train() client id: f_00005-8-3 loss: 0.746343  [  128/  146]
train() client id: f_00005-9-0 loss: 0.590993  [   32/  146]
train() client id: f_00005-9-1 loss: 0.725081  [   64/  146]
train() client id: f_00005-9-2 loss: 0.680251  [   96/  146]
train() client id: f_00005-9-3 loss: 0.506240  [  128/  146]
train() client id: f_00005-10-0 loss: 0.707753  [   32/  146]
train() client id: f_00005-10-1 loss: 0.637650  [   64/  146]
train() client id: f_00005-10-2 loss: 0.449996  [   96/  146]
train() client id: f_00005-10-3 loss: 0.565229  [  128/  146]
train() client id: f_00005-11-0 loss: 0.577135  [   32/  146]
train() client id: f_00005-11-1 loss: 0.603534  [   64/  146]
train() client id: f_00005-11-2 loss: 0.568835  [   96/  146]
train() client id: f_00005-11-3 loss: 0.710284  [  128/  146]
train() client id: f_00006-0-0 loss: 0.498179  [   32/   54]
train() client id: f_00006-1-0 loss: 0.530399  [   32/   54]
train() client id: f_00006-2-0 loss: 0.478849  [   32/   54]
train() client id: f_00006-3-0 loss: 0.560696  [   32/   54]
train() client id: f_00006-4-0 loss: 0.563426  [   32/   54]
train() client id: f_00006-5-0 loss: 0.567269  [   32/   54]
train() client id: f_00006-6-0 loss: 0.521639  [   32/   54]
train() client id: f_00006-7-0 loss: 0.562100  [   32/   54]
train() client id: f_00006-8-0 loss: 0.560271  [   32/   54]
train() client id: f_00006-9-0 loss: 0.507599  [   32/   54]
train() client id: f_00006-10-0 loss: 0.569471  [   32/   54]
train() client id: f_00006-11-0 loss: 0.466082  [   32/   54]
train() client id: f_00007-0-0 loss: 0.795250  [   32/  179]
train() client id: f_00007-0-1 loss: 0.626792  [   64/  179]
train() client id: f_00007-0-2 loss: 0.712569  [   96/  179]
train() client id: f_00007-0-3 loss: 0.785892  [  128/  179]
train() client id: f_00007-0-4 loss: 0.773855  [  160/  179]
train() client id: f_00007-1-0 loss: 0.601881  [   32/  179]
train() client id: f_00007-1-1 loss: 0.675931  [   64/  179]
train() client id: f_00007-1-2 loss: 0.879005  [   96/  179]
train() client id: f_00007-1-3 loss: 0.718961  [  128/  179]
train() client id: f_00007-1-4 loss: 0.736196  [  160/  179]
train() client id: f_00007-2-0 loss: 0.636978  [   32/  179]
train() client id: f_00007-2-1 loss: 0.791303  [   64/  179]
train() client id: f_00007-2-2 loss: 0.755485  [   96/  179]
train() client id: f_00007-2-3 loss: 0.748684  [  128/  179]
train() client id: f_00007-2-4 loss: 0.596979  [  160/  179]
train() client id: f_00007-3-0 loss: 0.639397  [   32/  179]
train() client id: f_00007-3-1 loss: 0.807272  [   64/  179]
train() client id: f_00007-3-2 loss: 0.838811  [   96/  179]
train() client id: f_00007-3-3 loss: 0.651828  [  128/  179]
train() client id: f_00007-3-4 loss: 0.560018  [  160/  179]
train() client id: f_00007-4-0 loss: 0.730652  [   32/  179]
train() client id: f_00007-4-1 loss: 0.549024  [   64/  179]
train() client id: f_00007-4-2 loss: 0.644269  [   96/  179]
train() client id: f_00007-4-3 loss: 0.579270  [  128/  179]
train() client id: f_00007-4-4 loss: 0.803443  [  160/  179]
train() client id: f_00007-5-0 loss: 0.552434  [   32/  179]
train() client id: f_00007-5-1 loss: 0.568777  [   64/  179]
train() client id: f_00007-5-2 loss: 0.564459  [   96/  179]
train() client id: f_00007-5-3 loss: 0.831452  [  128/  179]
train() client id: f_00007-5-4 loss: 0.790986  [  160/  179]
train() client id: f_00007-6-0 loss: 0.707372  [   32/  179]
train() client id: f_00007-6-1 loss: 0.644555  [   64/  179]
train() client id: f_00007-6-2 loss: 0.751648  [   96/  179]
train() client id: f_00007-6-3 loss: 0.622011  [  128/  179]
train() client id: f_00007-6-4 loss: 0.608061  [  160/  179]
train() client id: f_00007-7-0 loss: 0.546916  [   32/  179]
train() client id: f_00007-7-1 loss: 0.732698  [   64/  179]
train() client id: f_00007-7-2 loss: 0.789025  [   96/  179]
train() client id: f_00007-7-3 loss: 0.719493  [  128/  179]
train() client id: f_00007-7-4 loss: 0.740905  [  160/  179]
train() client id: f_00007-8-0 loss: 0.684330  [   32/  179]
train() client id: f_00007-8-1 loss: 0.728913  [   64/  179]
train() client id: f_00007-8-2 loss: 0.771684  [   96/  179]
train() client id: f_00007-8-3 loss: 0.605711  [  128/  179]
train() client id: f_00007-8-4 loss: 0.713315  [  160/  179]
train() client id: f_00007-9-0 loss: 0.557376  [   32/  179]
train() client id: f_00007-9-1 loss: 0.620571  [   64/  179]
train() client id: f_00007-9-2 loss: 0.640744  [   96/  179]
train() client id: f_00007-9-3 loss: 0.959275  [  128/  179]
train() client id: f_00007-9-4 loss: 0.583938  [  160/  179]
train() client id: f_00007-10-0 loss: 0.629702  [   32/  179]
train() client id: f_00007-10-1 loss: 0.635947  [   64/  179]
train() client id: f_00007-10-2 loss: 0.755041  [   96/  179]
train() client id: f_00007-10-3 loss: 0.660621  [  128/  179]
train() client id: f_00007-10-4 loss: 0.773888  [  160/  179]
train() client id: f_00007-11-0 loss: 0.844236  [   32/  179]
train() client id: f_00007-11-1 loss: 0.542259  [   64/  179]
train() client id: f_00007-11-2 loss: 0.628361  [   96/  179]
train() client id: f_00007-11-3 loss: 0.664373  [  128/  179]
train() client id: f_00007-11-4 loss: 0.769305  [  160/  179]
train() client id: f_00008-0-0 loss: 0.741391  [   32/  130]
train() client id: f_00008-0-1 loss: 0.771723  [   64/  130]
train() client id: f_00008-0-2 loss: 0.744405  [   96/  130]
train() client id: f_00008-0-3 loss: 0.646032  [  128/  130]
train() client id: f_00008-1-0 loss: 0.854554  [   32/  130]
train() client id: f_00008-1-1 loss: 0.590780  [   64/  130]
train() client id: f_00008-1-2 loss: 0.845385  [   96/  130]
train() client id: f_00008-1-3 loss: 0.639546  [  128/  130]
train() client id: f_00008-2-0 loss: 0.645476  [   32/  130]
train() client id: f_00008-2-1 loss: 0.660182  [   64/  130]
train() client id: f_00008-2-2 loss: 0.835507  [   96/  130]
train() client id: f_00008-2-3 loss: 0.749651  [  128/  130]
train() client id: f_00008-3-0 loss: 0.690248  [   32/  130]
train() client id: f_00008-3-1 loss: 0.715648  [   64/  130]
train() client id: f_00008-3-2 loss: 0.776649  [   96/  130]
train() client id: f_00008-3-3 loss: 0.741687  [  128/  130]
train() client id: f_00008-4-0 loss: 0.654176  [   32/  130]
train() client id: f_00008-4-1 loss: 0.727709  [   64/  130]
train() client id: f_00008-4-2 loss: 0.789482  [   96/  130]
train() client id: f_00008-4-3 loss: 0.730384  [  128/  130]
train() client id: f_00008-5-0 loss: 0.794094  [   32/  130]
train() client id: f_00008-5-1 loss: 0.715499  [   64/  130]
train() client id: f_00008-5-2 loss: 0.728016  [   96/  130]
train() client id: f_00008-5-3 loss: 0.647054  [  128/  130]
train() client id: f_00008-6-0 loss: 0.706777  [   32/  130]
train() client id: f_00008-6-1 loss: 0.734331  [   64/  130]
train() client id: f_00008-6-2 loss: 0.782103  [   96/  130]
train() client id: f_00008-6-3 loss: 0.689987  [  128/  130]
train() client id: f_00008-7-0 loss: 0.761962  [   32/  130]
train() client id: f_00008-7-1 loss: 0.644707  [   64/  130]
train() client id: f_00008-7-2 loss: 0.838696  [   96/  130]
train() client id: f_00008-7-3 loss: 0.681677  [  128/  130]
train() client id: f_00008-8-0 loss: 0.822799  [   32/  130]
train() client id: f_00008-8-1 loss: 0.637246  [   64/  130]
train() client id: f_00008-8-2 loss: 0.748635  [   96/  130]
train() client id: f_00008-8-3 loss: 0.684641  [  128/  130]
train() client id: f_00008-9-0 loss: 0.679273  [   32/  130]
train() client id: f_00008-9-1 loss: 0.736660  [   64/  130]
train() client id: f_00008-9-2 loss: 0.687329  [   96/  130]
train() client id: f_00008-9-3 loss: 0.819144  [  128/  130]
train() client id: f_00008-10-0 loss: 0.757812  [   32/  130]
train() client id: f_00008-10-1 loss: 0.695045  [   64/  130]
train() client id: f_00008-10-2 loss: 0.736732  [   96/  130]
train() client id: f_00008-10-3 loss: 0.731557  [  128/  130]
train() client id: f_00008-11-0 loss: 0.716192  [   32/  130]
train() client id: f_00008-11-1 loss: 0.778915  [   64/  130]
train() client id: f_00008-11-2 loss: 0.678637  [   96/  130]
train() client id: f_00008-11-3 loss: 0.739991  [  128/  130]
train() client id: f_00009-0-0 loss: 1.211354  [   32/  118]
train() client id: f_00009-0-1 loss: 1.173840  [   64/  118]
train() client id: f_00009-0-2 loss: 1.059937  [   96/  118]
train() client id: f_00009-1-0 loss: 1.199284  [   32/  118]
train() client id: f_00009-1-1 loss: 1.079405  [   64/  118]
train() client id: f_00009-1-2 loss: 1.187418  [   96/  118]
train() client id: f_00009-2-0 loss: 1.145697  [   32/  118]
train() client id: f_00009-2-1 loss: 0.995656  [   64/  118]
train() client id: f_00009-2-2 loss: 1.059062  [   96/  118]
train() client id: f_00009-3-0 loss: 1.095833  [   32/  118]
train() client id: f_00009-3-1 loss: 0.895469  [   64/  118]
train() client id: f_00009-3-2 loss: 1.059076  [   96/  118]
train() client id: f_00009-4-0 loss: 0.875108  [   32/  118]
train() client id: f_00009-4-1 loss: 1.057600  [   64/  118]
train() client id: f_00009-4-2 loss: 1.000866  [   96/  118]
train() client id: f_00009-5-0 loss: 0.836558  [   32/  118]
train() client id: f_00009-5-1 loss: 1.130897  [   64/  118]
train() client id: f_00009-5-2 loss: 0.851355  [   96/  118]
train() client id: f_00009-6-0 loss: 0.899523  [   32/  118]
train() client id: f_00009-6-1 loss: 0.880974  [   64/  118]
train() client id: f_00009-6-2 loss: 0.886168  [   96/  118]
train() client id: f_00009-7-0 loss: 0.827772  [   32/  118]
train() client id: f_00009-7-1 loss: 0.930780  [   64/  118]
train() client id: f_00009-7-2 loss: 0.995310  [   96/  118]
train() client id: f_00009-8-0 loss: 0.931793  [   32/  118]
train() client id: f_00009-8-1 loss: 0.897556  [   64/  118]
train() client id: f_00009-8-2 loss: 0.955547  [   96/  118]
train() client id: f_00009-9-0 loss: 0.793551  [   32/  118]
train() client id: f_00009-9-1 loss: 0.871713  [   64/  118]
train() client id: f_00009-9-2 loss: 1.026834  [   96/  118]
train() client id: f_00009-10-0 loss: 0.909731  [   32/  118]
train() client id: f_00009-10-1 loss: 0.955305  [   64/  118]
train() client id: f_00009-10-2 loss: 0.895835  [   96/  118]
train() client id: f_00009-11-0 loss: 0.778995  [   32/  118]
train() client id: f_00009-11-1 loss: 0.887453  [   64/  118]
train() client id: f_00009-11-2 loss: 0.839956  [   96/  118]
At round 20 accuracy: 0.6419098143236074
At round 20 training accuracy: 0.579476861167002
At round 20 training loss: 0.8353895336628565
update_location
xs = -4.528292 1.001589 10.045120 -10.943528 -90.103519 -25.217951 -52.215960 73.375741 -1.680116 4.695607 
ys = 87.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 -3.124922 -19.177652 -17.154970 4.001482 
xs mean: -9.557130840587133
ys mean: 8.371751218646876
dists_uav = 133.011864 101.207644 100.511932 103.092337 134.929871 104.552850 112.855091 125.506102 101.474705 100.190122 
uav_gains = -103.098158 -100.130352 -100.055458 -100.330684 -103.253796 -100.483427 -101.313119 -102.467023 -100.158965 -100.020639 
uav_gains_db_mean: -101.13116199606795
dists_bs = 191.576272 237.484736 253.783816 223.957823 186.137780 243.659728 216.343265 315.270470 258.772709 248.054439 
bs_gains = -103.472627 -106.084859 -106.892049 -105.371714 -103.122426 -106.397004 -104.951074 -109.530191 -107.128776 -106.614375 
bs_gains_db_mean: -105.95650958445695
Round 21
-------------------------------
ene_coms = [0.00723052 0.00841869 0.00631696 0.00639011 0.00723885 0.00856505
 0.00666546 0.00702028 0.00892815 0.0086699 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.20989413 17.12291994  8.06264312  2.88495194 19.71949768  9.5170129
  3.59414354 11.59432378  8.50165096  7.72888062]
obj_prev = 96.93591860446557
eta_min = 4.689554980602224e-12	eta_max = 0.9342194950194699
af = 20.495020110218665	bf = 1.583503845615576	zeta = 22.544522121240533	eta = 0.9090909090909091
af = 20.495020110218665	bf = 1.583503845615576	zeta = 38.905766871551485	eta = 0.52678617485895
af = 20.495020110218665	bf = 1.583503845615576	zeta = 31.10645228187267	eta = 0.6588671676378269
af = 20.495020110218665	bf = 1.583503845615576	zeta = 29.708982545667105	eta = 0.689859374305895
af = 20.495020110218665	bf = 1.583503845615576	zeta = 29.64022806701963	eta = 0.6914595955158408
af = 20.495020110218665	bf = 1.583503845615576	zeta = 29.640049909797114	eta = 0.6914637516667715
eta = 0.6914637516667715
ene_coms = [0.00723052 0.00841869 0.00631696 0.00639011 0.00723885 0.00856505
 0.00666546 0.00702028 0.00892815 0.0086699 ]
ene_comp = [0.03044442 0.06402993 0.02996118 0.01038976 0.07393645 0.03527687
 0.01304761 0.04325041 0.03141091 0.02851144]
ene_total = [2.56295498 4.92854301 2.46793311 1.14150324 5.5222025  2.98248281
 1.34104303 3.41982039 2.74419028 2.52937657]
ti_comp = [0.35325287 0.34137117 0.36238845 0.36165692 0.35316957 0.33990758
 0.35890342 0.3553552  0.33627653 0.33885904]
ti_coms = [0.07230516 0.08418686 0.06316958 0.06390112 0.07238847 0.08565046
 0.06665461 0.07020284 0.0892815  0.08669899]
t_total = [28.94991188 28.94991188 28.94991188 28.94991188 28.94991188 28.94991188
 28.94991188 28.94991188 28.94991188 28.94991188]
ene_coms = [0.00723052 0.00841869 0.00631696 0.00639011 0.00723885 0.00856505
 0.00666546 0.00702028 0.00892815 0.0086699 ]
ene_comp = [1.41329143e-05 1.40791172e-04 1.27999375e-05 5.35924123e-07
 2.02529937e-04 2.37480724e-05 1.07774886e-06 4.00428743e-05
 1.71288587e-05 1.26153516e-05]
ene_total = [0.49283985 0.58228513 0.43060154 0.43474373 0.50622286 0.58427947
 0.4535121  0.48030074 0.6085305  0.59065513]
optimize_network iter = 0 obj = 5.16397103611423
eta = 0.6914637516667715
freqs = [4.30915392e+07 9.37834473e+07 4.13384795e+07 1.43641140e+07
 1.04675566e+08 5.18918520e+07 1.81770467e+07 6.08551760e+07
 4.67039867e+07 4.20697656e+07]
eta_min = 0.6914637516668112	eta_max = 0.6914637516667637
af = 0.02647661929710446	bf = 1.583503845615576	zeta = 0.029124281226814906	eta = 0.9090909090909091
af = 0.02647661929710446	bf = 1.583503845615576	zeta = 17.44630070657532	eta = 0.001517606496781619
af = 0.02647661929710446	bf = 1.583503845615576	zeta = 1.7580215279774807	eta = 0.015060463638101484
af = 0.02647661929710446	bf = 1.583503845615576	zeta = 1.72050289914891	eta = 0.015388883860760587
af = 0.02647661929710446	bf = 1.583503845615576	zeta = 1.7204966869127272	eta = 0.015388939425750545
eta = 0.015388939425750545
ene_coms = [0.00723052 0.00841869 0.00631696 0.00639011 0.00723885 0.00856505
 0.00666546 0.00702028 0.00892815 0.0086699 ]
ene_comp = [1.59894721e-04 1.59286081e-03 1.44813900e-04 6.06325323e-06
 2.29135104e-03 2.68677170e-04 1.21932639e-05 4.53030712e-04
 1.93789761e-04 1.42725561e-04]
ene_total = [0.15754279 0.21341804 0.13774682 0.13634847 0.20315703 0.18831014
 0.14234882 0.15931006 0.19445412 0.18786039]
ti_comp = [0.35325287 0.34137117 0.36238845 0.36165692 0.35316957 0.33990758
 0.35890342 0.3553552  0.33627653 0.33885904]
ti_coms = [0.07230516 0.08418686 0.06316958 0.06390112 0.07238847 0.08565046
 0.06665461 0.07020284 0.0892815  0.08669899]
t_total = [28.94991188 28.94991188 28.94991188 28.94991188 28.94991188 28.94991188
 28.94991188 28.94991188 28.94991188 28.94991188]
ene_coms = [0.00723052 0.00841869 0.00631696 0.00639011 0.00723885 0.00856505
 0.00666546 0.00702028 0.00892815 0.0086699 ]
ene_comp = [1.41329143e-05 1.40791172e-04 1.27999375e-05 5.35924123e-07
 2.02529937e-04 2.37480724e-05 1.07774886e-06 4.00428743e-05
 1.71288587e-05 1.26153516e-05]
ene_total = [0.49283985 0.58228513 0.43060154 0.43474373 0.50622286 0.58427947
 0.4535121  0.48030074 0.6085305  0.59065513]
optimize_network iter = 1 obj = 5.163971036114889
eta = 0.6914637516668112
freqs = [4.30915392e+07 9.37834473e+07 4.13384795e+07 1.43641140e+07
 1.04675566e+08 5.18918520e+07 1.81770467e+07 6.08551760e+07
 4.67039867e+07 4.20697656e+07]
Done!
ene_coms = [0.00723052 0.00841869 0.00631696 0.00639011 0.00723885 0.00856505
 0.00666546 0.00702028 0.00892815 0.0086699 ]
ene_comp = [1.40380185e-05 1.39845826e-04 1.27139919e-05 5.32325646e-07
 2.01170044e-04 2.35886153e-05 1.07051229e-06 3.97740053e-05
 1.70138465e-05 1.25306454e-05]
ene_total = [0.00724455 0.00855853 0.00632967 0.00639064 0.00744002 0.00858863
 0.00666653 0.00706006 0.00894516 0.00868243]
At round 21 energy consumption: 0.07590623791458216
At round 21 eta: 0.6914637516668112
At round 21 a_n: 20.989140069898713
At round 21 local rounds: 12.081119001004339
At round 21 global rounds: 68.02811722541108
gradient difference: 0.4670237898826599
train() client id: f_00000-0-0 loss: 1.157928  [   32/  126]
train() client id: f_00000-0-1 loss: 1.067969  [   64/  126]
train() client id: f_00000-0-2 loss: 0.971635  [   96/  126]
train() client id: f_00000-1-0 loss: 0.846022  [   32/  126]
train() client id: f_00000-1-1 loss: 1.063241  [   64/  126]
train() client id: f_00000-1-2 loss: 0.997162  [   96/  126]
train() client id: f_00000-2-0 loss: 0.868003  [   32/  126]
train() client id: f_00000-2-1 loss: 0.895732  [   64/  126]
train() client id: f_00000-2-2 loss: 0.889438  [   96/  126]
train() client id: f_00000-3-0 loss: 0.953206  [   32/  126]
train() client id: f_00000-3-1 loss: 0.786796  [   64/  126]
train() client id: f_00000-3-2 loss: 0.770713  [   96/  126]
train() client id: f_00000-4-0 loss: 0.846261  [   32/  126]
train() client id: f_00000-4-1 loss: 0.941532  [   64/  126]
train() client id: f_00000-4-2 loss: 0.930709  [   96/  126]
train() client id: f_00000-5-0 loss: 0.908112  [   32/  126]
train() client id: f_00000-5-1 loss: 0.871491  [   64/  126]
train() client id: f_00000-5-2 loss: 0.821958  [   96/  126]
train() client id: f_00000-6-0 loss: 0.770478  [   32/  126]
train() client id: f_00000-6-1 loss: 0.958388  [   64/  126]
train() client id: f_00000-6-2 loss: 0.807603  [   96/  126]
train() client id: f_00000-7-0 loss: 0.919250  [   32/  126]
train() client id: f_00000-7-1 loss: 0.772084  [   64/  126]
train() client id: f_00000-7-2 loss: 0.739706  [   96/  126]
train() client id: f_00000-8-0 loss: 0.774812  [   32/  126]
train() client id: f_00000-8-1 loss: 0.848777  [   64/  126]
train() client id: f_00000-8-2 loss: 0.820213  [   96/  126]
train() client id: f_00000-9-0 loss: 0.776260  [   32/  126]
train() client id: f_00000-9-1 loss: 0.810632  [   64/  126]
train() client id: f_00000-9-2 loss: 0.884766  [   96/  126]
train() client id: f_00000-10-0 loss: 0.695263  [   32/  126]
train() client id: f_00000-10-1 loss: 0.878574  [   64/  126]
train() client id: f_00000-10-2 loss: 0.864346  [   96/  126]
train() client id: f_00000-11-0 loss: 0.838814  [   32/  126]
train() client id: f_00000-11-1 loss: 0.816390  [   64/  126]
train() client id: f_00000-11-2 loss: 0.910982  [   96/  126]
train() client id: f_00001-0-0 loss: 0.476710  [   32/  265]
train() client id: f_00001-0-1 loss: 0.478543  [   64/  265]
train() client id: f_00001-0-2 loss: 0.497497  [   96/  265]
train() client id: f_00001-0-3 loss: 0.449514  [  128/  265]
train() client id: f_00001-0-4 loss: 0.452984  [  160/  265]
train() client id: f_00001-0-5 loss: 0.508331  [  192/  265]
train() client id: f_00001-0-6 loss: 0.557388  [  224/  265]
train() client id: f_00001-0-7 loss: 0.513288  [  256/  265]
train() client id: f_00001-1-0 loss: 0.533956  [   32/  265]
train() client id: f_00001-1-1 loss: 0.435317  [   64/  265]
train() client id: f_00001-1-2 loss: 0.532946  [   96/  265]
train() client id: f_00001-1-3 loss: 0.512968  [  128/  265]
train() client id: f_00001-1-4 loss: 0.470063  [  160/  265]
train() client id: f_00001-1-5 loss: 0.456861  [  192/  265]
train() client id: f_00001-1-6 loss: 0.457442  [  224/  265]
train() client id: f_00001-1-7 loss: 0.457111  [  256/  265]
train() client id: f_00001-2-0 loss: 0.497074  [   32/  265]
train() client id: f_00001-2-1 loss: 0.388511  [   64/  265]
train() client id: f_00001-2-2 loss: 0.588721  [   96/  265]
train() client id: f_00001-2-3 loss: 0.396009  [  128/  265]
train() client id: f_00001-2-4 loss: 0.513579  [  160/  265]
train() client id: f_00001-2-5 loss: 0.400992  [  192/  265]
train() client id: f_00001-2-6 loss: 0.513618  [  224/  265]
train() client id: f_00001-2-7 loss: 0.492837  [  256/  265]
train() client id: f_00001-3-0 loss: 0.421749  [   32/  265]
train() client id: f_00001-3-1 loss: 0.573570  [   64/  265]
train() client id: f_00001-3-2 loss: 0.527134  [   96/  265]
train() client id: f_00001-3-3 loss: 0.401450  [  128/  265]
train() client id: f_00001-3-4 loss: 0.421172  [  160/  265]
train() client id: f_00001-3-5 loss: 0.495322  [  192/  265]
train() client id: f_00001-3-6 loss: 0.466319  [  224/  265]
train() client id: f_00001-3-7 loss: 0.440892  [  256/  265]
train() client id: f_00001-4-0 loss: 0.400133  [   32/  265]
train() client id: f_00001-4-1 loss: 0.498008  [   64/  265]
train() client id: f_00001-4-2 loss: 0.423022  [   96/  265]
train() client id: f_00001-4-3 loss: 0.514124  [  128/  265]
train() client id: f_00001-4-4 loss: 0.383364  [  160/  265]
train() client id: f_00001-4-5 loss: 0.420559  [  192/  265]
train() client id: f_00001-4-6 loss: 0.557066  [  224/  265]
train() client id: f_00001-4-7 loss: 0.452704  [  256/  265]
train() client id: f_00001-5-0 loss: 0.503381  [   32/  265]
train() client id: f_00001-5-1 loss: 0.547212  [   64/  265]
train() client id: f_00001-5-2 loss: 0.437661  [   96/  265]
train() client id: f_00001-5-3 loss: 0.377490  [  128/  265]
train() client id: f_00001-5-4 loss: 0.523782  [  160/  265]
train() client id: f_00001-5-5 loss: 0.380598  [  192/  265]
train() client id: f_00001-5-6 loss: 0.462793  [  224/  265]
train() client id: f_00001-5-7 loss: 0.411859  [  256/  265]
train() client id: f_00001-6-0 loss: 0.468455  [   32/  265]
train() client id: f_00001-6-1 loss: 0.504536  [   64/  265]
train() client id: f_00001-6-2 loss: 0.414980  [   96/  265]
train() client id: f_00001-6-3 loss: 0.443324  [  128/  265]
train() client id: f_00001-6-4 loss: 0.502331  [  160/  265]
train() client id: f_00001-6-5 loss: 0.366014  [  192/  265]
train() client id: f_00001-6-6 loss: 0.486129  [  224/  265]
train() client id: f_00001-6-7 loss: 0.442924  [  256/  265]
train() client id: f_00001-7-0 loss: 0.404132  [   32/  265]
train() client id: f_00001-7-1 loss: 0.376374  [   64/  265]
train() client id: f_00001-7-2 loss: 0.462071  [   96/  265]
train() client id: f_00001-7-3 loss: 0.443281  [  128/  265]
train() client id: f_00001-7-4 loss: 0.568217  [  160/  265]
train() client id: f_00001-7-5 loss: 0.441695  [  192/  265]
train() client id: f_00001-7-6 loss: 0.564070  [  224/  265]
train() client id: f_00001-7-7 loss: 0.396688  [  256/  265]
train() client id: f_00001-8-0 loss: 0.369260  [   32/  265]
train() client id: f_00001-8-1 loss: 0.565178  [   64/  265]
train() client id: f_00001-8-2 loss: 0.403322  [   96/  265]
train() client id: f_00001-8-3 loss: 0.430004  [  128/  265]
train() client id: f_00001-8-4 loss: 0.466455  [  160/  265]
train() client id: f_00001-8-5 loss: 0.621280  [  192/  265]
train() client id: f_00001-8-6 loss: 0.370299  [  224/  265]
train() client id: f_00001-8-7 loss: 0.428827  [  256/  265]
train() client id: f_00001-9-0 loss: 0.539777  [   32/  265]
train() client id: f_00001-9-1 loss: 0.410087  [   64/  265]
train() client id: f_00001-9-2 loss: 0.360422  [   96/  265]
train() client id: f_00001-9-3 loss: 0.601726  [  128/  265]
train() client id: f_00001-9-4 loss: 0.457041  [  160/  265]
train() client id: f_00001-9-5 loss: 0.452500  [  192/  265]
train() client id: f_00001-9-6 loss: 0.360648  [  224/  265]
train() client id: f_00001-9-7 loss: 0.372223  [  256/  265]
train() client id: f_00001-10-0 loss: 0.398255  [   32/  265]
train() client id: f_00001-10-1 loss: 0.394483  [   64/  265]
train() client id: f_00001-10-2 loss: 0.394998  [   96/  265]
train() client id: f_00001-10-3 loss: 0.580667  [  128/  265]
train() client id: f_00001-10-4 loss: 0.559801  [  160/  265]
train() client id: f_00001-10-5 loss: 0.432802  [  192/  265]
train() client id: f_00001-10-6 loss: 0.427384  [  224/  265]
train() client id: f_00001-10-7 loss: 0.438110  [  256/  265]
train() client id: f_00001-11-0 loss: 0.468950  [   32/  265]
train() client id: f_00001-11-1 loss: 0.519276  [   64/  265]
train() client id: f_00001-11-2 loss: 0.506284  [   96/  265]
train() client id: f_00001-11-3 loss: 0.567484  [  128/  265]
train() client id: f_00001-11-4 loss: 0.356514  [  160/  265]
train() client id: f_00001-11-5 loss: 0.363443  [  192/  265]
train() client id: f_00001-11-6 loss: 0.365945  [  224/  265]
train() client id: f_00001-11-7 loss: 0.443055  [  256/  265]
train() client id: f_00002-0-0 loss: 1.121745  [   32/  124]
train() client id: f_00002-0-1 loss: 1.045902  [   64/  124]
train() client id: f_00002-0-2 loss: 1.035025  [   96/  124]
train() client id: f_00002-1-0 loss: 1.173026  [   32/  124]
train() client id: f_00002-1-1 loss: 0.839692  [   64/  124]
train() client id: f_00002-1-2 loss: 1.079344  [   96/  124]
train() client id: f_00002-2-0 loss: 1.092505  [   32/  124]
train() client id: f_00002-2-1 loss: 0.980242  [   64/  124]
train() client id: f_00002-2-2 loss: 0.865789  [   96/  124]
train() client id: f_00002-3-0 loss: 0.953742  [   32/  124]
train() client id: f_00002-3-1 loss: 0.961290  [   64/  124]
train() client id: f_00002-3-2 loss: 0.885629  [   96/  124]
train() client id: f_00002-4-0 loss: 0.854107  [   32/  124]
train() client id: f_00002-4-1 loss: 0.899312  [   64/  124]
train() client id: f_00002-4-2 loss: 0.975201  [   96/  124]
train() client id: f_00002-5-0 loss: 0.891817  [   32/  124]
train() client id: f_00002-5-1 loss: 0.979131  [   64/  124]
train() client id: f_00002-5-2 loss: 0.893931  [   96/  124]
train() client id: f_00002-6-0 loss: 0.839561  [   32/  124]
train() client id: f_00002-6-1 loss: 1.145002  [   64/  124]
train() client id: f_00002-6-2 loss: 0.851199  [   96/  124]
train() client id: f_00002-7-0 loss: 0.823321  [   32/  124]
train() client id: f_00002-7-1 loss: 1.016006  [   64/  124]
train() client id: f_00002-7-2 loss: 1.104399  [   96/  124]
train() client id: f_00002-8-0 loss: 0.788306  [   32/  124]
train() client id: f_00002-8-1 loss: 0.964667  [   64/  124]
train() client id: f_00002-8-2 loss: 1.010997  [   96/  124]
train() client id: f_00002-9-0 loss: 0.870943  [   32/  124]
train() client id: f_00002-9-1 loss: 1.082550  [   64/  124]
train() client id: f_00002-9-2 loss: 0.828053  [   96/  124]
train() client id: f_00002-10-0 loss: 0.925904  [   32/  124]
train() client id: f_00002-10-1 loss: 0.774251  [   64/  124]
train() client id: f_00002-10-2 loss: 0.988177  [   96/  124]
train() client id: f_00002-11-0 loss: 0.917390  [   32/  124]
train() client id: f_00002-11-1 loss: 1.028390  [   64/  124]
train() client id: f_00002-11-2 loss: 0.721415  [   96/  124]
train() client id: f_00003-0-0 loss: 0.908560  [   32/   43]
train() client id: f_00003-1-0 loss: 0.844487  [   32/   43]
train() client id: f_00003-2-0 loss: 0.730005  [   32/   43]
train() client id: f_00003-3-0 loss: 0.801110  [   32/   43]
train() client id: f_00003-4-0 loss: 0.701651  [   32/   43]
train() client id: f_00003-5-0 loss: 0.703063  [   32/   43]
train() client id: f_00003-6-0 loss: 0.738685  [   32/   43]
train() client id: f_00003-7-0 loss: 0.837636  [   32/   43]
train() client id: f_00003-8-0 loss: 0.770452  [   32/   43]
train() client id: f_00003-9-0 loss: 0.872776  [   32/   43]
train() client id: f_00003-10-0 loss: 0.737326  [   32/   43]
train() client id: f_00003-11-0 loss: 0.785021  [   32/   43]
train() client id: f_00004-0-0 loss: 0.872233  [   32/  306]
train() client id: f_00004-0-1 loss: 0.848907  [   64/  306]
train() client id: f_00004-0-2 loss: 0.917174  [   96/  306]
train() client id: f_00004-0-3 loss: 1.004614  [  128/  306]
train() client id: f_00004-0-4 loss: 1.118165  [  160/  306]
train() client id: f_00004-0-5 loss: 0.815142  [  192/  306]
train() client id: f_00004-0-6 loss: 0.946023  [  224/  306]
train() client id: f_00004-0-7 loss: 0.851200  [  256/  306]
train() client id: f_00004-0-8 loss: 1.075361  [  288/  306]
train() client id: f_00004-1-0 loss: 0.982369  [   32/  306]
train() client id: f_00004-1-1 loss: 0.993541  [   64/  306]
train() client id: f_00004-1-2 loss: 0.882388  [   96/  306]
train() client id: f_00004-1-3 loss: 0.936898  [  128/  306]
train() client id: f_00004-1-4 loss: 0.968725  [  160/  306]
train() client id: f_00004-1-5 loss: 1.005426  [  192/  306]
train() client id: f_00004-1-6 loss: 0.885326  [  224/  306]
train() client id: f_00004-1-7 loss: 0.912377  [  256/  306]
train() client id: f_00004-1-8 loss: 0.943405  [  288/  306]
train() client id: f_00004-2-0 loss: 0.884187  [   32/  306]
train() client id: f_00004-2-1 loss: 0.962806  [   64/  306]
train() client id: f_00004-2-2 loss: 0.882411  [   96/  306]
train() client id: f_00004-2-3 loss: 0.982972  [  128/  306]
train() client id: f_00004-2-4 loss: 0.809727  [  160/  306]
train() client id: f_00004-2-5 loss: 1.072034  [  192/  306]
train() client id: f_00004-2-6 loss: 0.923764  [  224/  306]
train() client id: f_00004-2-7 loss: 0.885049  [  256/  306]
train() client id: f_00004-2-8 loss: 1.025396  [  288/  306]
train() client id: f_00004-3-0 loss: 0.906255  [   32/  306]
train() client id: f_00004-3-1 loss: 1.003567  [   64/  306]
train() client id: f_00004-3-2 loss: 0.984356  [   96/  306]
train() client id: f_00004-3-3 loss: 0.918490  [  128/  306]
train() client id: f_00004-3-4 loss: 0.976915  [  160/  306]
train() client id: f_00004-3-5 loss: 0.898590  [  192/  306]
train() client id: f_00004-3-6 loss: 0.880128  [  224/  306]
train() client id: f_00004-3-7 loss: 0.946214  [  256/  306]
train() client id: f_00004-3-8 loss: 0.985607  [  288/  306]
train() client id: f_00004-4-0 loss: 1.056099  [   32/  306]
train() client id: f_00004-4-1 loss: 0.776834  [   64/  306]
train() client id: f_00004-4-2 loss: 0.963484  [   96/  306]
train() client id: f_00004-4-3 loss: 0.926315  [  128/  306]
train() client id: f_00004-4-4 loss: 0.955336  [  160/  306]
train() client id: f_00004-4-5 loss: 0.954944  [  192/  306]
train() client id: f_00004-4-6 loss: 0.890694  [  224/  306]
train() client id: f_00004-4-7 loss: 1.076342  [  256/  306]
train() client id: f_00004-4-8 loss: 0.867393  [  288/  306]
train() client id: f_00004-5-0 loss: 0.872512  [   32/  306]
train() client id: f_00004-5-1 loss: 0.868755  [   64/  306]
train() client id: f_00004-5-2 loss: 0.925752  [   96/  306]
train() client id: f_00004-5-3 loss: 0.846851  [  128/  306]
train() client id: f_00004-5-4 loss: 0.974249  [  160/  306]
train() client id: f_00004-5-5 loss: 0.974182  [  192/  306]
train() client id: f_00004-5-6 loss: 0.960867  [  224/  306]
train() client id: f_00004-5-7 loss: 0.998083  [  256/  306]
train() client id: f_00004-5-8 loss: 0.947181  [  288/  306]
train() client id: f_00004-6-0 loss: 1.104153  [   32/  306]
train() client id: f_00004-6-1 loss: 0.895159  [   64/  306]
train() client id: f_00004-6-2 loss: 0.872203  [   96/  306]
train() client id: f_00004-6-3 loss: 0.924023  [  128/  306]
train() client id: f_00004-6-4 loss: 0.786339  [  160/  306]
train() client id: f_00004-6-5 loss: 0.903327  [  192/  306]
train() client id: f_00004-6-6 loss: 0.920552  [  224/  306]
train() client id: f_00004-6-7 loss: 1.014199  [  256/  306]
train() client id: f_00004-6-8 loss: 0.980160  [  288/  306]
train() client id: f_00004-7-0 loss: 0.920477  [   32/  306]
train() client id: f_00004-7-1 loss: 0.991723  [   64/  306]
train() client id: f_00004-7-2 loss: 1.011177  [   96/  306]
train() client id: f_00004-7-3 loss: 0.880657  [  128/  306]
train() client id: f_00004-7-4 loss: 0.956019  [  160/  306]
train() client id: f_00004-7-5 loss: 0.783410  [  192/  306]
train() client id: f_00004-7-6 loss: 0.901676  [  224/  306]
train() client id: f_00004-7-7 loss: 1.029725  [  256/  306]
train() client id: f_00004-7-8 loss: 0.946982  [  288/  306]
train() client id: f_00004-8-0 loss: 0.838542  [   32/  306]
train() client id: f_00004-8-1 loss: 0.991168  [   64/  306]
train() client id: f_00004-8-2 loss: 0.900417  [   96/  306]
train() client id: f_00004-8-3 loss: 0.919264  [  128/  306]
train() client id: f_00004-8-4 loss: 0.914173  [  160/  306]
train() client id: f_00004-8-5 loss: 0.894381  [  192/  306]
train() client id: f_00004-8-6 loss: 0.952429  [  224/  306]
train() client id: f_00004-8-7 loss: 1.082719  [  256/  306]
train() client id: f_00004-8-8 loss: 0.860624  [  288/  306]
train() client id: f_00004-9-0 loss: 0.952389  [   32/  306]
train() client id: f_00004-9-1 loss: 0.887302  [   64/  306]
train() client id: f_00004-9-2 loss: 0.984371  [   96/  306]
train() client id: f_00004-9-3 loss: 0.838970  [  128/  306]
train() client id: f_00004-9-4 loss: 0.927871  [  160/  306]
train() client id: f_00004-9-5 loss: 1.026074  [  192/  306]
train() client id: f_00004-9-6 loss: 0.906913  [  224/  306]
train() client id: f_00004-9-7 loss: 0.963658  [  256/  306]
train() client id: f_00004-9-8 loss: 0.890732  [  288/  306]
train() client id: f_00004-10-0 loss: 0.847817  [   32/  306]
train() client id: f_00004-10-1 loss: 0.910120  [   64/  306]
train() client id: f_00004-10-2 loss: 0.962779  [   96/  306]
train() client id: f_00004-10-3 loss: 1.024979  [  128/  306]
train() client id: f_00004-10-4 loss: 0.876167  [  160/  306]
train() client id: f_00004-10-5 loss: 0.841750  [  192/  306]
train() client id: f_00004-10-6 loss: 1.011158  [  224/  306]
train() client id: f_00004-10-7 loss: 0.961765  [  256/  306]
train() client id: f_00004-10-8 loss: 0.898800  [  288/  306]
train() client id: f_00004-11-0 loss: 0.899780  [   32/  306]
train() client id: f_00004-11-1 loss: 0.941513  [   64/  306]
train() client id: f_00004-11-2 loss: 0.931203  [   96/  306]
train() client id: f_00004-11-3 loss: 0.978697  [  128/  306]
train() client id: f_00004-11-4 loss: 0.948918  [  160/  306]
train() client id: f_00004-11-5 loss: 0.892524  [  192/  306]
train() client id: f_00004-11-6 loss: 0.900381  [  224/  306]
train() client id: f_00004-11-7 loss: 0.988217  [  256/  306]
train() client id: f_00004-11-8 loss: 0.829860  [  288/  306]
train() client id: f_00005-0-0 loss: 0.389050  [   32/  146]
train() client id: f_00005-0-1 loss: 0.422697  [   64/  146]
train() client id: f_00005-0-2 loss: 0.871346  [   96/  146]
train() client id: f_00005-0-3 loss: 0.416459  [  128/  146]
train() client id: f_00005-1-0 loss: 0.426622  [   32/  146]
train() client id: f_00005-1-1 loss: 0.683376  [   64/  146]
train() client id: f_00005-1-2 loss: 0.570070  [   96/  146]
train() client id: f_00005-1-3 loss: 0.428118  [  128/  146]
train() client id: f_00005-2-0 loss: 0.496356  [   32/  146]
train() client id: f_00005-2-1 loss: 0.566038  [   64/  146]
train() client id: f_00005-2-2 loss: 0.429282  [   96/  146]
train() client id: f_00005-2-3 loss: 0.791316  [  128/  146]
train() client id: f_00005-3-0 loss: 0.521432  [   32/  146]
train() client id: f_00005-3-1 loss: 0.758826  [   64/  146]
train() client id: f_00005-3-2 loss: 0.440946  [   96/  146]
train() client id: f_00005-3-3 loss: 0.558059  [  128/  146]
train() client id: f_00005-4-0 loss: 0.614058  [   32/  146]
train() client id: f_00005-4-1 loss: 0.310187  [   64/  146]
train() client id: f_00005-4-2 loss: 0.750399  [   96/  146]
train() client id: f_00005-4-3 loss: 0.620297  [  128/  146]
train() client id: f_00005-5-0 loss: 0.582904  [   32/  146]
train() client id: f_00005-5-1 loss: 0.440564  [   64/  146]
train() client id: f_00005-5-2 loss: 0.394400  [   96/  146]
train() client id: f_00005-5-3 loss: 0.599021  [  128/  146]
train() client id: f_00005-6-0 loss: 0.745081  [   32/  146]
train() client id: f_00005-6-1 loss: 0.495712  [   64/  146]
train() client id: f_00005-6-2 loss: 0.451590  [   96/  146]
train() client id: f_00005-6-3 loss: 0.543609  [  128/  146]
train() client id: f_00005-7-0 loss: 0.462820  [   32/  146]
train() client id: f_00005-7-1 loss: 0.835243  [   64/  146]
train() client id: f_00005-7-2 loss: 0.348487  [   96/  146]
train() client id: f_00005-7-3 loss: 0.637212  [  128/  146]
train() client id: f_00005-8-0 loss: 0.789758  [   32/  146]
train() client id: f_00005-8-1 loss: 0.418656  [   64/  146]
train() client id: f_00005-8-2 loss: 0.427896  [   96/  146]
train() client id: f_00005-8-3 loss: 0.544180  [  128/  146]
train() client id: f_00005-9-0 loss: 0.655966  [   32/  146]
train() client id: f_00005-9-1 loss: 0.442603  [   64/  146]
train() client id: f_00005-9-2 loss: 0.515850  [   96/  146]
train() client id: f_00005-9-3 loss: 0.514083  [  128/  146]
train() client id: f_00005-10-0 loss: 0.321237  [   32/  146]
train() client id: f_00005-10-1 loss: 0.342109  [   64/  146]
train() client id: f_00005-10-2 loss: 0.424855  [   96/  146]
train() client id: f_00005-10-3 loss: 0.876122  [  128/  146]
train() client id: f_00005-11-0 loss: 0.723032  [   32/  146]
train() client id: f_00005-11-1 loss: 0.453748  [   64/  146]
train() client id: f_00005-11-2 loss: 0.352567  [   96/  146]
train() client id: f_00005-11-3 loss: 0.444192  [  128/  146]
train() client id: f_00006-0-0 loss: 0.604005  [   32/   54]
train() client id: f_00006-1-0 loss: 0.595945  [   32/   54]
train() client id: f_00006-2-0 loss: 0.594488  [   32/   54]
train() client id: f_00006-3-0 loss: 0.555507  [   32/   54]
train() client id: f_00006-4-0 loss: 0.643401  [   32/   54]
train() client id: f_00006-5-0 loss: 0.635892  [   32/   54]
train() client id: f_00006-6-0 loss: 0.592457  [   32/   54]
train() client id: f_00006-7-0 loss: 0.649878  [   32/   54]
train() client id: f_00006-8-0 loss: 0.550411  [   32/   54]
train() client id: f_00006-9-0 loss: 0.648775  [   32/   54]
train() client id: f_00006-10-0 loss: 0.592293  [   32/   54]
train() client id: f_00006-11-0 loss: 0.596339  [   32/   54]
train() client id: f_00007-0-0 loss: 0.522399  [   32/  179]
train() client id: f_00007-0-1 loss: 0.406344  [   64/  179]
train() client id: f_00007-0-2 loss: 0.739971  [   96/  179]
train() client id: f_00007-0-3 loss: 0.447158  [  128/  179]
train() client id: f_00007-0-4 loss: 0.483473  [  160/  179]
train() client id: f_00007-1-0 loss: 0.408717  [   32/  179]
train() client id: f_00007-1-1 loss: 0.419807  [   64/  179]
train() client id: f_00007-1-2 loss: 0.644353  [   96/  179]
train() client id: f_00007-1-3 loss: 0.360900  [  128/  179]
train() client id: f_00007-1-4 loss: 0.698325  [  160/  179]
train() client id: f_00007-2-0 loss: 0.550905  [   32/  179]
train() client id: f_00007-2-1 loss: 0.464756  [   64/  179]
train() client id: f_00007-2-2 loss: 0.560199  [   96/  179]
train() client id: f_00007-2-3 loss: 0.435257  [  128/  179]
train() client id: f_00007-2-4 loss: 0.434556  [  160/  179]
train() client id: f_00007-3-0 loss: 0.355484  [   32/  179]
train() client id: f_00007-3-1 loss: 0.652045  [   64/  179]
train() client id: f_00007-3-2 loss: 0.397819  [   96/  179]
train() client id: f_00007-3-3 loss: 0.327220  [  128/  179]
train() client id: f_00007-3-4 loss: 0.579509  [  160/  179]
train() client id: f_00007-4-0 loss: 0.456634  [   32/  179]
train() client id: f_00007-4-1 loss: 0.451055  [   64/  179]
train() client id: f_00007-4-2 loss: 0.626883  [   96/  179]
train() client id: f_00007-4-3 loss: 0.412691  [  128/  179]
train() client id: f_00007-4-4 loss: 0.438295  [  160/  179]
train() client id: f_00007-5-0 loss: 0.339374  [   32/  179]
train() client id: f_00007-5-1 loss: 0.391609  [   64/  179]
train() client id: f_00007-5-2 loss: 0.719298  [   96/  179]
train() client id: f_00007-5-3 loss: 0.410903  [  128/  179]
train() client id: f_00007-5-4 loss: 0.399883  [  160/  179]
train() client id: f_00007-6-0 loss: 0.295055  [   32/  179]
train() client id: f_00007-6-1 loss: 0.420484  [   64/  179]
train() client id: f_00007-6-2 loss: 0.398093  [   96/  179]
train() client id: f_00007-6-3 loss: 0.399262  [  128/  179]
train() client id: f_00007-6-4 loss: 0.521350  [  160/  179]
train() client id: f_00007-7-0 loss: 0.533329  [   32/  179]
train() client id: f_00007-7-1 loss: 0.305006  [   64/  179]
train() client id: f_00007-7-2 loss: 0.384775  [   96/  179]
train() client id: f_00007-7-3 loss: 0.621359  [  128/  179]
train() client id: f_00007-7-4 loss: 0.461442  [  160/  179]
train() client id: f_00007-8-0 loss: 0.618589  [   32/  179]
train() client id: f_00007-8-1 loss: 0.324625  [   64/  179]
train() client id: f_00007-8-2 loss: 0.441014  [   96/  179]
train() client id: f_00007-8-3 loss: 0.298280  [  128/  179]
train() client id: f_00007-8-4 loss: 0.509302  [  160/  179]
train() client id: f_00007-9-0 loss: 0.373023  [   32/  179]
train() client id: f_00007-9-1 loss: 0.378848  [   64/  179]
train() client id: f_00007-9-2 loss: 0.385132  [   96/  179]
train() client id: f_00007-9-3 loss: 0.457257  [  128/  179]
train() client id: f_00007-9-4 loss: 0.404700  [  160/  179]
train() client id: f_00007-10-0 loss: 0.597700  [   32/  179]
train() client id: f_00007-10-1 loss: 0.501596  [   64/  179]
train() client id: f_00007-10-2 loss: 0.482888  [   96/  179]
train() client id: f_00007-10-3 loss: 0.350636  [  128/  179]
train() client id: f_00007-10-4 loss: 0.302536  [  160/  179]
train() client id: f_00007-11-0 loss: 0.483605  [   32/  179]
train() client id: f_00007-11-1 loss: 0.387318  [   64/  179]
train() client id: f_00007-11-2 loss: 0.301686  [   96/  179]
train() client id: f_00007-11-3 loss: 0.458324  [  128/  179]
train() client id: f_00007-11-4 loss: 0.607721  [  160/  179]
train() client id: f_00008-0-0 loss: 0.698622  [   32/  130]
train() client id: f_00008-0-1 loss: 0.753591  [   64/  130]
train() client id: f_00008-0-2 loss: 0.748391  [   96/  130]
train() client id: f_00008-0-3 loss: 0.681852  [  128/  130]
train() client id: f_00008-1-0 loss: 0.741542  [   32/  130]
train() client id: f_00008-1-1 loss: 0.650456  [   64/  130]
train() client id: f_00008-1-2 loss: 0.780241  [   96/  130]
train() client id: f_00008-1-3 loss: 0.702863  [  128/  130]
train() client id: f_00008-2-0 loss: 0.716546  [   32/  130]
train() client id: f_00008-2-1 loss: 0.769647  [   64/  130]
train() client id: f_00008-2-2 loss: 0.761571  [   96/  130]
train() client id: f_00008-2-3 loss: 0.652221  [  128/  130]
train() client id: f_00008-3-0 loss: 0.720395  [   32/  130]
train() client id: f_00008-3-1 loss: 0.664095  [   64/  130]
train() client id: f_00008-3-2 loss: 0.699472  [   96/  130]
train() client id: f_00008-3-3 loss: 0.750841  [  128/  130]
train() client id: f_00008-4-0 loss: 0.675546  [   32/  130]
train() client id: f_00008-4-1 loss: 0.675384  [   64/  130]
train() client id: f_00008-4-2 loss: 0.857255  [   96/  130]
train() client id: f_00008-4-3 loss: 0.681251  [  128/  130]
train() client id: f_00008-5-0 loss: 0.694083  [   32/  130]
train() client id: f_00008-5-1 loss: 0.876643  [   64/  130]
train() client id: f_00008-5-2 loss: 0.662467  [   96/  130]
train() client id: f_00008-5-3 loss: 0.659526  [  128/  130]
train() client id: f_00008-6-0 loss: 0.788357  [   32/  130]
train() client id: f_00008-6-1 loss: 0.722104  [   64/  130]
train() client id: f_00008-6-2 loss: 0.655668  [   96/  130]
train() client id: f_00008-6-3 loss: 0.732046  [  128/  130]
train() client id: f_00008-7-0 loss: 0.750421  [   32/  130]
train() client id: f_00008-7-1 loss: 0.613896  [   64/  130]
train() client id: f_00008-7-2 loss: 0.775802  [   96/  130]
train() client id: f_00008-7-3 loss: 0.749985  [  128/  130]
train() client id: f_00008-8-0 loss: 0.774518  [   32/  130]
train() client id: f_00008-8-1 loss: 0.703677  [   64/  130]
train() client id: f_00008-8-2 loss: 0.716292  [   96/  130]
train() client id: f_00008-8-3 loss: 0.695845  [  128/  130]
train() client id: f_00008-9-0 loss: 0.709702  [   32/  130]
train() client id: f_00008-9-1 loss: 0.706510  [   64/  130]
train() client id: f_00008-9-2 loss: 0.687694  [   96/  130]
train() client id: f_00008-9-3 loss: 0.736340  [  128/  130]
train() client id: f_00008-10-0 loss: 0.770714  [   32/  130]
train() client id: f_00008-10-1 loss: 0.691481  [   64/  130]
train() client id: f_00008-10-2 loss: 0.643665  [   96/  130]
train() client id: f_00008-10-3 loss: 0.751162  [  128/  130]
train() client id: f_00008-11-0 loss: 0.728823  [   32/  130]
train() client id: f_00008-11-1 loss: 0.710923  [   64/  130]
train() client id: f_00008-11-2 loss: 0.747338  [   96/  130]
train() client id: f_00008-11-3 loss: 0.704513  [  128/  130]
train() client id: f_00009-0-0 loss: 1.209189  [   32/  118]
train() client id: f_00009-0-1 loss: 1.304650  [   64/  118]
train() client id: f_00009-0-2 loss: 1.160516  [   96/  118]
train() client id: f_00009-1-0 loss: 1.175558  [   32/  118]
train() client id: f_00009-1-1 loss: 1.139183  [   64/  118]
train() client id: f_00009-1-2 loss: 1.104548  [   96/  118]
train() client id: f_00009-2-0 loss: 1.187297  [   32/  118]
train() client id: f_00009-2-1 loss: 1.066117  [   64/  118]
train() client id: f_00009-2-2 loss: 1.075767  [   96/  118]
train() client id: f_00009-3-0 loss: 1.020845  [   32/  118]
train() client id: f_00009-3-1 loss: 1.046406  [   64/  118]
train() client id: f_00009-3-2 loss: 0.999011  [   96/  118]
train() client id: f_00009-4-0 loss: 1.018064  [   32/  118]
train() client id: f_00009-4-1 loss: 1.006996  [   64/  118]
train() client id: f_00009-4-2 loss: 1.094591  [   96/  118]
train() client id: f_00009-5-0 loss: 1.000057  [   32/  118]
train() client id: f_00009-5-1 loss: 0.953224  [   64/  118]
train() client id: f_00009-5-2 loss: 1.052807  [   96/  118]
train() client id: f_00009-6-0 loss: 1.063006  [   32/  118]
train() client id: f_00009-6-1 loss: 0.951048  [   64/  118]
train() client id: f_00009-6-2 loss: 0.890873  [   96/  118]
train() client id: f_00009-7-0 loss: 0.847730  [   32/  118]
train() client id: f_00009-7-1 loss: 0.925526  [   64/  118]
train() client id: f_00009-7-2 loss: 0.958437  [   96/  118]
train() client id: f_00009-8-0 loss: 0.904960  [   32/  118]
train() client id: f_00009-8-1 loss: 0.915528  [   64/  118]
train() client id: f_00009-8-2 loss: 0.951896  [   96/  118]
train() client id: f_00009-9-0 loss: 0.914453  [   32/  118]
train() client id: f_00009-9-1 loss: 0.882289  [   64/  118]
train() client id: f_00009-9-2 loss: 1.010182  [   96/  118]
train() client id: f_00009-10-0 loss: 0.982545  [   32/  118]
train() client id: f_00009-10-1 loss: 0.979417  [   64/  118]
train() client id: f_00009-10-2 loss: 0.815464  [   96/  118]
train() client id: f_00009-11-0 loss: 0.862082  [   32/  118]
train() client id: f_00009-11-1 loss: 1.007451  [   64/  118]
train() client id: f_00009-11-2 loss: 0.815196  [   96/  118]
At round 21 accuracy: 0.6419098143236074
At round 21 training accuracy: 0.5848423876592891
At round 21 training loss: 0.8322469338639761
update_location
xs = -4.528292 6.001589 15.045120 -15.943528 -85.103519 -20.217951 -57.215960 78.375741 -1.680116 -0.304393 
ys = 92.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 -3.124922 -19.177652 -22.154970 4.001482 
xs mean: -8.557130840587133
ys mean: 8.371751218646876
dists_uav = 136.356282 101.380487 101.134068 103.742302 131.643590 103.460712 115.253769 128.493343 102.438594 100.080490 
uav_gains = -103.368142 -100.148879 -100.122456 -100.398924 -102.985772 -100.369413 -101.541499 -102.722573 -100.261613 -100.008752 
uav_gains_db_mean: -101.19280212556896
dists_bs = 189.347162 241.213631 257.452280 220.321451 188.470788 246.764834 213.545236 319.224414 262.506885 244.456639 
bs_gains = -103.330306 -106.274311 -107.066568 -105.172649 -103.273892 -106.550991 -104.792776 -109.681750 -107.302999 -106.436711 
bs_gains_db_mean: -105.98829517096414
Round 22
-------------------------------
ene_coms = [0.00732427 0.00850693 0.00633461 0.00640851 0.00729127 0.00863907
 0.00673286 0.00710395 0.009019   0.00858402]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 8.07786261 16.84531181  7.9314278   2.83825278 19.39876586  9.36323764
  3.5368921  11.40684748  8.36479739  7.60095304]
obj_prev = 95.36434850805193
eta_min = 3.1499469142133388e-12	eta_max = 0.9345372303970825
af = 20.160538373523963	bf = 1.5679947717500116	zeta = 22.17659221087636	eta = 0.909090909090909
af = 20.160538373523963	bf = 1.5679947717500116	zeta = 38.38449239990825	eta = 0.5252261294348172
af = 20.160538373523963	bf = 1.5679947717500116	zeta = 30.645952307798048	eta = 0.6578532189516588
af = 20.160538373523963	bf = 1.5679947717500116	zeta = 29.25847447554224	eta = 0.6890495398308129
af = 20.160538373523963	bf = 1.5679947717500116	zeta = 29.189949444526324	eta = 0.6906671219776453
af = 20.160538373523963	bf = 1.5679947717500116	zeta = 29.18977041695093	eta = 0.6906713579979525
eta = 0.6906713579979525
ene_coms = [0.00732427 0.00850693 0.00633461 0.00640851 0.00729127 0.00863907
 0.00673286 0.00710395 0.009019   0.00858402]
ene_comp = [0.03053904 0.06422893 0.03005429 0.01042205 0.07416623 0.0353865
 0.01308816 0.04338482 0.03150853 0.02860005]
ene_total = [2.52724168 4.85486178 2.42883073 1.12338052 5.43700044 2.93855145
 1.32298267 3.36994685 2.70506924 2.48190506]
ti_comp = [0.35973767 0.34791098 0.3696342  0.36889524 0.36006764 0.34658963
 0.36565178 0.36194088 0.34279036 0.34714017]
ti_coms = [0.07324266 0.08506935 0.06334613 0.06408509 0.07291269 0.0863907
 0.06732856 0.07103945 0.09018997 0.08584016]
t_total = [28.89990768 28.89990768 28.89990768 28.89990768 28.89990768 28.89990768
 28.89990768 28.89990768 28.89990768 28.89990768]
ene_coms = [0.00732427 0.00850693 0.00633461 0.00640851 0.00729127 0.00863907
 0.00673286 0.00710395 0.009019   0.00858402]
ene_comp = [1.37554299e-05 1.36815632e-04 1.24180994e-05 5.19916548e-07
 1.96666537e-04 2.30548237e-05 1.04804584e-06 3.89599322e-05
 1.66382338e-05 1.21330684e-05]
ene_total = [0.489787   0.57693982 0.42364191 0.42778006 0.49979326 0.57816622
 0.44946428 0.47676369 0.60309672 0.57376258]
optimize_network iter = 0 obj = 5.099195544228989
eta = 0.6906713579979525
freqs = [4.24462585e+07 9.23065514e+07 4.06540977e+07 1.41260323e+07
 1.02989305e+08 5.10495688e+07 1.78970258e+07 5.99335750e+07
 4.59588907e+07 4.11938063e+07]
eta_min = 0.6906713579979578	eta_max = 0.690671357997948
af = 0.025216654596732196	bf = 1.5679947717500116	zeta = 0.02773832005640542	eta = 0.909090909090909
af = 0.025216654596732196	bf = 1.5679947717500116	zeta = 17.27437993197056	eta = 0.0014597719105426453
af = 0.025216654596732196	bf = 1.5679947717500116	zeta = 1.7351792555055814	eta = 0.014532593400204515
af = 0.025216654596732196	bf = 1.5679947717500116	zeta = 1.6993921236793743	eta = 0.014838632146967538
af = 0.025216654596732196	bf = 1.5679947717500116	zeta = 1.6993866036862053	eta = 0.014838680346210671
eta = 0.014838680346210671
ene_coms = [0.00732427 0.00850693 0.00633461 0.00640851 0.00729127 0.00863907
 0.00673286 0.00710395 0.009019   0.00858402]
ene_comp = [1.56495180e-04 1.55654801e-03 1.41280405e-04 5.91507751e-06
 2.23747026e-03 2.62294150e-04 1.19235912e-05 4.43246171e-04
 1.89292768e-04 1.38037615e-04]
ene_total = [0.15677863 0.21090622 0.13571904 0.13443079 0.19969929 0.18655102
 0.14135423 0.15817084 0.19298344 0.18279311]
ti_comp = [0.35973767 0.34791098 0.3696342  0.36889524 0.36006764 0.34658963
 0.36565178 0.36194088 0.34279036 0.34714017]
ti_coms = [0.07324266 0.08506935 0.06334613 0.06408509 0.07291269 0.0863907
 0.06732856 0.07103945 0.09018997 0.08584016]
t_total = [28.89990768 28.89990768 28.89990768 28.89990768 28.89990768 28.89990768
 28.89990768 28.89990768 28.89990768 28.89990768]
ene_coms = [0.00732427 0.00850693 0.00633461 0.00640851 0.00729127 0.00863907
 0.00673286 0.00710395 0.009019   0.00858402]
ene_comp = [1.37554299e-05 1.36815632e-04 1.24180994e-05 5.19916548e-07
 1.96666537e-04 2.30548237e-05 1.04804584e-06 3.89599322e-05
 1.66382338e-05 1.21330684e-05]
ene_total = [0.489787   0.57693982 0.42364191 0.42778006 0.49979326 0.57816622
 0.44946428 0.47676369 0.60309672 0.57376258]
optimize_network iter = 1 obj = 5.099195544229075
eta = 0.6906713579979578
freqs = [4.24462585e+07 9.23065514e+07 4.06540977e+07 1.41260323e+07
 1.02989305e+08 5.10495688e+07 1.78970258e+07 5.99335750e+07
 4.59588907e+07 4.11938063e+07]
Done!
ene_coms = [0.00732427 0.00850693 0.00633461 0.00640851 0.00729127 0.00863907
 0.00673286 0.00710395 0.009019   0.00858402]
ene_comp = [1.36207375e-05 1.35475941e-04 1.22965021e-05 5.14825555e-07
 1.94740790e-04 2.28290722e-05 1.03778344e-06 3.85784389e-05
 1.64753132e-05 1.20142621e-05]
ene_total = [0.00733789 0.00864241 0.00634691 0.00640902 0.00748601 0.0086619
 0.00673389 0.00714252 0.00903547 0.00859603]
At round 22 energy consumption: 0.07639205929365399
At round 22 eta: 0.6906713579979578
At round 22 a_n: 20.646594222929394
At round 22 local rounds: 12.118665254221202
At round 22 global rounds: 66.74646773509284
gradient difference: 0.4834775924682617
train() client id: f_00000-0-0 loss: 0.895326  [   32/  126]
train() client id: f_00000-0-1 loss: 1.079662  [   64/  126]
train() client id: f_00000-0-2 loss: 1.001397  [   96/  126]
train() client id: f_00000-1-0 loss: 1.018571  [   32/  126]
train() client id: f_00000-1-1 loss: 0.924923  [   64/  126]
train() client id: f_00000-1-2 loss: 0.957124  [   96/  126]
train() client id: f_00000-2-0 loss: 0.849917  [   32/  126]
train() client id: f_00000-2-1 loss: 1.036958  [   64/  126]
train() client id: f_00000-2-2 loss: 0.797050  [   96/  126]
train() client id: f_00000-3-0 loss: 0.895604  [   32/  126]
train() client id: f_00000-3-1 loss: 0.677626  [   64/  126]
train() client id: f_00000-3-2 loss: 0.913869  [   96/  126]
train() client id: f_00000-4-0 loss: 0.869524  [   32/  126]
train() client id: f_00000-4-1 loss: 0.839214  [   64/  126]
train() client id: f_00000-4-2 loss: 0.820537  [   96/  126]
train() client id: f_00000-5-0 loss: 0.771731  [   32/  126]
train() client id: f_00000-5-1 loss: 0.760098  [   64/  126]
train() client id: f_00000-5-2 loss: 0.820280  [   96/  126]
train() client id: f_00000-6-0 loss: 0.795364  [   32/  126]
train() client id: f_00000-6-1 loss: 0.853377  [   64/  126]
train() client id: f_00000-6-2 loss: 0.791724  [   96/  126]
train() client id: f_00000-7-0 loss: 0.703798  [   32/  126]
train() client id: f_00000-7-1 loss: 0.806097  [   64/  126]
train() client id: f_00000-7-2 loss: 0.888764  [   96/  126]
train() client id: f_00000-8-0 loss: 0.718339  [   32/  126]
train() client id: f_00000-8-1 loss: 0.781821  [   64/  126]
train() client id: f_00000-8-2 loss: 0.789984  [   96/  126]
train() client id: f_00000-9-0 loss: 0.827502  [   32/  126]
train() client id: f_00000-9-1 loss: 0.774503  [   64/  126]
train() client id: f_00000-9-2 loss: 0.855108  [   96/  126]
train() client id: f_00000-10-0 loss: 0.714662  [   32/  126]
train() client id: f_00000-10-1 loss: 0.754229  [   64/  126]
train() client id: f_00000-10-2 loss: 0.846624  [   96/  126]
train() client id: f_00000-11-0 loss: 0.845483  [   32/  126]
train() client id: f_00000-11-1 loss: 0.771228  [   64/  126]
train() client id: f_00000-11-2 loss: 0.853915  [   96/  126]
train() client id: f_00001-0-0 loss: 0.380927  [   32/  265]
train() client id: f_00001-0-1 loss: 0.590843  [   64/  265]
train() client id: f_00001-0-2 loss: 0.368469  [   96/  265]
train() client id: f_00001-0-3 loss: 0.517995  [  128/  265]
train() client id: f_00001-0-4 loss: 0.457385  [  160/  265]
train() client id: f_00001-0-5 loss: 0.414925  [  192/  265]
train() client id: f_00001-0-6 loss: 0.474208  [  224/  265]
train() client id: f_00001-0-7 loss: 0.491557  [  256/  265]
train() client id: f_00001-1-0 loss: 0.544332  [   32/  265]
train() client id: f_00001-1-1 loss: 0.456819  [   64/  265]
train() client id: f_00001-1-2 loss: 0.527301  [   96/  265]
train() client id: f_00001-1-3 loss: 0.420992  [  128/  265]
train() client id: f_00001-1-4 loss: 0.404929  [  160/  265]
train() client id: f_00001-1-5 loss: 0.474590  [  192/  265]
train() client id: f_00001-1-6 loss: 0.437247  [  224/  265]
train() client id: f_00001-1-7 loss: 0.406223  [  256/  265]
train() client id: f_00001-2-0 loss: 0.432226  [   32/  265]
train() client id: f_00001-2-1 loss: 0.436831  [   64/  265]
train() client id: f_00001-2-2 loss: 0.444597  [   96/  265]
train() client id: f_00001-2-3 loss: 0.391593  [  128/  265]
train() client id: f_00001-2-4 loss: 0.558416  [  160/  265]
train() client id: f_00001-2-5 loss: 0.524781  [  192/  265]
train() client id: f_00001-2-6 loss: 0.411187  [  224/  265]
train() client id: f_00001-2-7 loss: 0.381829  [  256/  265]
train() client id: f_00001-3-0 loss: 0.379669  [   32/  265]
train() client id: f_00001-3-1 loss: 0.391778  [   64/  265]
train() client id: f_00001-3-2 loss: 0.609909  [   96/  265]
train() client id: f_00001-3-3 loss: 0.414887  [  128/  265]
train() client id: f_00001-3-4 loss: 0.491960  [  160/  265]
train() client id: f_00001-3-5 loss: 0.404848  [  192/  265]
train() client id: f_00001-3-6 loss: 0.513285  [  224/  265]
train() client id: f_00001-3-7 loss: 0.378583  [  256/  265]
train() client id: f_00001-4-0 loss: 0.420708  [   32/  265]
train() client id: f_00001-4-1 loss: 0.570893  [   64/  265]
train() client id: f_00001-4-2 loss: 0.517979  [   96/  265]
train() client id: f_00001-4-3 loss: 0.428940  [  128/  265]
train() client id: f_00001-4-4 loss: 0.428808  [  160/  265]
train() client id: f_00001-4-5 loss: 0.376700  [  192/  265]
train() client id: f_00001-4-6 loss: 0.351935  [  224/  265]
train() client id: f_00001-4-7 loss: 0.448750  [  256/  265]
train() client id: f_00001-5-0 loss: 0.440152  [   32/  265]
train() client id: f_00001-5-1 loss: 0.509970  [   64/  265]
train() client id: f_00001-5-2 loss: 0.586205  [   96/  265]
train() client id: f_00001-5-3 loss: 0.341637  [  128/  265]
train() client id: f_00001-5-4 loss: 0.424823  [  160/  265]
train() client id: f_00001-5-5 loss: 0.393754  [  192/  265]
train() client id: f_00001-5-6 loss: 0.452222  [  224/  265]
train() client id: f_00001-5-7 loss: 0.394214  [  256/  265]
train() client id: f_00001-6-0 loss: 0.456094  [   32/  265]
train() client id: f_00001-6-1 loss: 0.440235  [   64/  265]
train() client id: f_00001-6-2 loss: 0.405492  [   96/  265]
train() client id: f_00001-6-3 loss: 0.485101  [  128/  265]
train() client id: f_00001-6-4 loss: 0.355442  [  160/  265]
train() client id: f_00001-6-5 loss: 0.544896  [  192/  265]
train() client id: f_00001-6-6 loss: 0.356673  [  224/  265]
train() client id: f_00001-6-7 loss: 0.400457  [  256/  265]
train() client id: f_00001-7-0 loss: 0.384698  [   32/  265]
train() client id: f_00001-7-1 loss: 0.435940  [   64/  265]
train() client id: f_00001-7-2 loss: 0.396197  [   96/  265]
train() client id: f_00001-7-3 loss: 0.574828  [  128/  265]
train() client id: f_00001-7-4 loss: 0.487238  [  160/  265]
train() client id: f_00001-7-5 loss: 0.362818  [  192/  265]
train() client id: f_00001-7-6 loss: 0.452593  [  224/  265]
train() client id: f_00001-7-7 loss: 0.398921  [  256/  265]
train() client id: f_00001-8-0 loss: 0.525072  [   32/  265]
train() client id: f_00001-8-1 loss: 0.412460  [   64/  265]
train() client id: f_00001-8-2 loss: 0.392078  [   96/  265]
train() client id: f_00001-8-3 loss: 0.346285  [  128/  265]
train() client id: f_00001-8-4 loss: 0.394693  [  160/  265]
train() client id: f_00001-8-5 loss: 0.390691  [  192/  265]
train() client id: f_00001-8-6 loss: 0.400959  [  224/  265]
train() client id: f_00001-8-7 loss: 0.595953  [  256/  265]
train() client id: f_00001-9-0 loss: 0.378902  [   32/  265]
train() client id: f_00001-9-1 loss: 0.398804  [   64/  265]
train() client id: f_00001-9-2 loss: 0.399817  [   96/  265]
train() client id: f_00001-9-3 loss: 0.390401  [  128/  265]
train() client id: f_00001-9-4 loss: 0.509340  [  160/  265]
train() client id: f_00001-9-5 loss: 0.451934  [  192/  265]
train() client id: f_00001-9-6 loss: 0.548118  [  224/  265]
train() client id: f_00001-9-7 loss: 0.418691  [  256/  265]
train() client id: f_00001-10-0 loss: 0.494374  [   32/  265]
train() client id: f_00001-10-1 loss: 0.539022  [   64/  265]
train() client id: f_00001-10-2 loss: 0.368625  [   96/  265]
train() client id: f_00001-10-3 loss: 0.595334  [  128/  265]
train() client id: f_00001-10-4 loss: 0.385147  [  160/  265]
train() client id: f_00001-10-5 loss: 0.373765  [  192/  265]
train() client id: f_00001-10-6 loss: 0.391606  [  224/  265]
train() client id: f_00001-10-7 loss: 0.335957  [  256/  265]
train() client id: f_00001-11-0 loss: 0.342826  [   32/  265]
train() client id: f_00001-11-1 loss: 0.512866  [   64/  265]
train() client id: f_00001-11-2 loss: 0.342825  [   96/  265]
train() client id: f_00001-11-3 loss: 0.427965  [  128/  265]
train() client id: f_00001-11-4 loss: 0.442822  [  160/  265]
train() client id: f_00001-11-5 loss: 0.422739  [  192/  265]
train() client id: f_00001-11-6 loss: 0.417049  [  224/  265]
train() client id: f_00001-11-7 loss: 0.454129  [  256/  265]
train() client id: f_00002-0-0 loss: 1.490831  [   32/  124]
train() client id: f_00002-0-1 loss: 1.274270  [   64/  124]
train() client id: f_00002-0-2 loss: 1.420095  [   96/  124]
train() client id: f_00002-1-0 loss: 1.263875  [   32/  124]
train() client id: f_00002-1-1 loss: 1.280396  [   64/  124]
train() client id: f_00002-1-2 loss: 1.346270  [   96/  124]
train() client id: f_00002-2-0 loss: 1.344684  [   32/  124]
train() client id: f_00002-2-1 loss: 1.206779  [   64/  124]
train() client id: f_00002-2-2 loss: 1.154410  [   96/  124]
train() client id: f_00002-3-0 loss: 1.310407  [   32/  124]
train() client id: f_00002-3-1 loss: 1.209275  [   64/  124]
train() client id: f_00002-3-2 loss: 1.145196  [   96/  124]
train() client id: f_00002-4-0 loss: 1.209816  [   32/  124]
train() client id: f_00002-4-1 loss: 1.309321  [   64/  124]
train() client id: f_00002-4-2 loss: 1.330866  [   96/  124]
train() client id: f_00002-5-0 loss: 1.131224  [   32/  124]
train() client id: f_00002-5-1 loss: 1.178138  [   64/  124]
train() client id: f_00002-5-2 loss: 1.151486  [   96/  124]
train() client id: f_00002-6-0 loss: 1.232409  [   32/  124]
train() client id: f_00002-6-1 loss: 1.145292  [   64/  124]
train() client id: f_00002-6-2 loss: 1.079822  [   96/  124]
train() client id: f_00002-7-0 loss: 1.039793  [   32/  124]
train() client id: f_00002-7-1 loss: 1.222415  [   64/  124]
train() client id: f_00002-7-2 loss: 1.242299  [   96/  124]
train() client id: f_00002-8-0 loss: 1.128689  [   32/  124]
train() client id: f_00002-8-1 loss: 1.275727  [   64/  124]
train() client id: f_00002-8-2 loss: 1.170708  [   96/  124]
train() client id: f_00002-9-0 loss: 1.171254  [   32/  124]
train() client id: f_00002-9-1 loss: 1.118215  [   64/  124]
train() client id: f_00002-9-2 loss: 1.149416  [   96/  124]
train() client id: f_00002-10-0 loss: 1.309724  [   32/  124]
train() client id: f_00002-10-1 loss: 0.979529  [   64/  124]
train() client id: f_00002-10-2 loss: 1.238824  [   96/  124]
train() client id: f_00002-11-0 loss: 1.126366  [   32/  124]
train() client id: f_00002-11-1 loss: 1.148913  [   64/  124]
train() client id: f_00002-11-2 loss: 1.333744  [   96/  124]
train() client id: f_00003-0-0 loss: 0.764831  [   32/   43]
train() client id: f_00003-1-0 loss: 0.786032  [   32/   43]
train() client id: f_00003-2-0 loss: 0.891641  [   32/   43]
train() client id: f_00003-3-0 loss: 0.832198  [   32/   43]
train() client id: f_00003-4-0 loss: 0.711094  [   32/   43]
train() client id: f_00003-5-0 loss: 0.743199  [   32/   43]
train() client id: f_00003-6-0 loss: 0.732368  [   32/   43]
train() client id: f_00003-7-0 loss: 0.906115  [   32/   43]
train() client id: f_00003-8-0 loss: 0.762007  [   32/   43]
train() client id: f_00003-9-0 loss: 0.777737  [   32/   43]
train() client id: f_00003-10-0 loss: 0.782389  [   32/   43]
train() client id: f_00003-11-0 loss: 0.834089  [   32/   43]
train() client id: f_00004-0-0 loss: 0.858644  [   32/  306]
train() client id: f_00004-0-1 loss: 0.952064  [   64/  306]
train() client id: f_00004-0-2 loss: 1.120367  [   96/  306]
train() client id: f_00004-0-3 loss: 0.946010  [  128/  306]
train() client id: f_00004-0-4 loss: 0.805657  [  160/  306]
train() client id: f_00004-0-5 loss: 1.027195  [  192/  306]
train() client id: f_00004-0-6 loss: 0.968541  [  224/  306]
train() client id: f_00004-0-7 loss: 0.990542  [  256/  306]
train() client id: f_00004-0-8 loss: 0.834285  [  288/  306]
train() client id: f_00004-1-0 loss: 0.894536  [   32/  306]
train() client id: f_00004-1-1 loss: 0.929350  [   64/  306]
train() client id: f_00004-1-2 loss: 1.125709  [   96/  306]
train() client id: f_00004-1-3 loss: 0.918299  [  128/  306]
train() client id: f_00004-1-4 loss: 0.911319  [  160/  306]
train() client id: f_00004-1-5 loss: 0.933893  [  192/  306]
train() client id: f_00004-1-6 loss: 0.872648  [  224/  306]
train() client id: f_00004-1-7 loss: 1.033195  [  256/  306]
train() client id: f_00004-1-8 loss: 0.862487  [  288/  306]
train() client id: f_00004-2-0 loss: 0.793993  [   32/  306]
train() client id: f_00004-2-1 loss: 0.986714  [   64/  306]
train() client id: f_00004-2-2 loss: 0.954664  [   96/  306]
train() client id: f_00004-2-3 loss: 1.058979  [  128/  306]
train() client id: f_00004-2-4 loss: 0.880669  [  160/  306]
train() client id: f_00004-2-5 loss: 0.870476  [  192/  306]
train() client id: f_00004-2-6 loss: 0.955601  [  224/  306]
train() client id: f_00004-2-7 loss: 0.916861  [  256/  306]
train() client id: f_00004-2-8 loss: 0.928897  [  288/  306]
train() client id: f_00004-3-0 loss: 1.004001  [   32/  306]
train() client id: f_00004-3-1 loss: 0.955970  [   64/  306]
train() client id: f_00004-3-2 loss: 0.927036  [   96/  306]
train() client id: f_00004-3-3 loss: 0.852607  [  128/  306]
train() client id: f_00004-3-4 loss: 0.929145  [  160/  306]
train() client id: f_00004-3-5 loss: 0.753553  [  192/  306]
train() client id: f_00004-3-6 loss: 1.088802  [  224/  306]
train() client id: f_00004-3-7 loss: 0.898864  [  256/  306]
train() client id: f_00004-3-8 loss: 1.027282  [  288/  306]
train() client id: f_00004-4-0 loss: 0.932222  [   32/  306]
train() client id: f_00004-4-1 loss: 1.008854  [   64/  306]
train() client id: f_00004-4-2 loss: 0.990246  [   96/  306]
train() client id: f_00004-4-3 loss: 0.850509  [  128/  306]
train() client id: f_00004-4-4 loss: 0.994091  [  160/  306]
train() client id: f_00004-4-5 loss: 0.800228  [  192/  306]
train() client id: f_00004-4-6 loss: 1.099545  [  224/  306]
train() client id: f_00004-4-7 loss: 0.843402  [  256/  306]
train() client id: f_00004-4-8 loss: 0.983773  [  288/  306]
train() client id: f_00004-5-0 loss: 1.013224  [   32/  306]
train() client id: f_00004-5-1 loss: 1.004995  [   64/  306]
train() client id: f_00004-5-2 loss: 1.039781  [   96/  306]
train() client id: f_00004-5-3 loss: 0.784459  [  128/  306]
train() client id: f_00004-5-4 loss: 1.059748  [  160/  306]
train() client id: f_00004-5-5 loss: 0.950105  [  192/  306]
train() client id: f_00004-5-6 loss: 0.853045  [  224/  306]
train() client id: f_00004-5-7 loss: 0.808500  [  256/  306]
train() client id: f_00004-5-8 loss: 0.861238  [  288/  306]
train() client id: f_00004-6-0 loss: 0.944597  [   32/  306]
train() client id: f_00004-6-1 loss: 1.034651  [   64/  306]
train() client id: f_00004-6-2 loss: 0.966560  [   96/  306]
train() client id: f_00004-6-3 loss: 0.998405  [  128/  306]
train() client id: f_00004-6-4 loss: 0.856673  [  160/  306]
train() client id: f_00004-6-5 loss: 0.921521  [  192/  306]
train() client id: f_00004-6-6 loss: 0.798979  [  224/  306]
train() client id: f_00004-6-7 loss: 0.965204  [  256/  306]
train() client id: f_00004-6-8 loss: 0.874089  [  288/  306]
train() client id: f_00004-7-0 loss: 0.998872  [   32/  306]
train() client id: f_00004-7-1 loss: 1.120009  [   64/  306]
train() client id: f_00004-7-2 loss: 0.824124  [   96/  306]
train() client id: f_00004-7-3 loss: 0.921225  [  128/  306]
train() client id: f_00004-7-4 loss: 0.874083  [  160/  306]
train() client id: f_00004-7-5 loss: 0.878491  [  192/  306]
train() client id: f_00004-7-6 loss: 0.984093  [  224/  306]
train() client id: f_00004-7-7 loss: 0.940922  [  256/  306]
train() client id: f_00004-7-8 loss: 0.888405  [  288/  306]
train() client id: f_00004-8-0 loss: 0.959780  [   32/  306]
train() client id: f_00004-8-1 loss: 1.048572  [   64/  306]
train() client id: f_00004-8-2 loss: 0.914646  [   96/  306]
train() client id: f_00004-8-3 loss: 0.900522  [  128/  306]
train() client id: f_00004-8-4 loss: 0.837721  [  160/  306]
train() client id: f_00004-8-5 loss: 0.816560  [  192/  306]
train() client id: f_00004-8-6 loss: 1.065347  [  224/  306]
train() client id: f_00004-8-7 loss: 0.969278  [  256/  306]
train() client id: f_00004-8-8 loss: 0.914970  [  288/  306]
train() client id: f_00004-9-0 loss: 0.980502  [   32/  306]
train() client id: f_00004-9-1 loss: 0.918700  [   64/  306]
train() client id: f_00004-9-2 loss: 0.971555  [   96/  306]
train() client id: f_00004-9-3 loss: 0.973080  [  128/  306]
train() client id: f_00004-9-4 loss: 0.869750  [  160/  306]
train() client id: f_00004-9-5 loss: 0.902799  [  192/  306]
train() client id: f_00004-9-6 loss: 1.003830  [  224/  306]
train() client id: f_00004-9-7 loss: 0.924211  [  256/  306]
train() client id: f_00004-9-8 loss: 0.883828  [  288/  306]
train() client id: f_00004-10-0 loss: 0.974990  [   32/  306]
train() client id: f_00004-10-1 loss: 0.828650  [   64/  306]
train() client id: f_00004-10-2 loss: 0.946669  [   96/  306]
train() client id: f_00004-10-3 loss: 1.006115  [  128/  306]
train() client id: f_00004-10-4 loss: 1.022152  [  160/  306]
train() client id: f_00004-10-5 loss: 0.865098  [  192/  306]
train() client id: f_00004-10-6 loss: 0.800752  [  224/  306]
train() client id: f_00004-10-7 loss: 0.886778  [  256/  306]
train() client id: f_00004-10-8 loss: 0.990076  [  288/  306]
train() client id: f_00004-11-0 loss: 0.899227  [   32/  306]
train() client id: f_00004-11-1 loss: 0.934767  [   64/  306]
train() client id: f_00004-11-2 loss: 0.897571  [   96/  306]
train() client id: f_00004-11-3 loss: 0.871055  [  128/  306]
train() client id: f_00004-11-4 loss: 0.961487  [  160/  306]
train() client id: f_00004-11-5 loss: 0.887428  [  192/  306]
train() client id: f_00004-11-6 loss: 0.904204  [  224/  306]
train() client id: f_00004-11-7 loss: 1.026381  [  256/  306]
train() client id: f_00004-11-8 loss: 0.990700  [  288/  306]
train() client id: f_00005-0-0 loss: 0.779371  [   32/  146]
train() client id: f_00005-0-1 loss: 0.596760  [   64/  146]
train() client id: f_00005-0-2 loss: 0.776045  [   96/  146]
train() client id: f_00005-0-3 loss: 0.879821  [  128/  146]
train() client id: f_00005-1-0 loss: 0.906027  [   32/  146]
train() client id: f_00005-1-1 loss: 0.692694  [   64/  146]
train() client id: f_00005-1-2 loss: 0.734383  [   96/  146]
train() client id: f_00005-1-3 loss: 0.710428  [  128/  146]
train() client id: f_00005-2-0 loss: 0.895786  [   32/  146]
train() client id: f_00005-2-1 loss: 0.707513  [   64/  146]
train() client id: f_00005-2-2 loss: 0.821477  [   96/  146]
train() client id: f_00005-2-3 loss: 0.466234  [  128/  146]
train() client id: f_00005-3-0 loss: 0.611666  [   32/  146]
train() client id: f_00005-3-1 loss: 0.771211  [   64/  146]
train() client id: f_00005-3-2 loss: 0.545584  [   96/  146]
train() client id: f_00005-3-3 loss: 1.014284  [  128/  146]
train() client id: f_00005-4-0 loss: 0.734773  [   32/  146]
train() client id: f_00005-4-1 loss: 0.467178  [   64/  146]
train() client id: f_00005-4-2 loss: 0.771756  [   96/  146]
train() client id: f_00005-4-3 loss: 0.919796  [  128/  146]
train() client id: f_00005-5-0 loss: 0.456203  [   32/  146]
train() client id: f_00005-5-1 loss: 0.661841  [   64/  146]
train() client id: f_00005-5-2 loss: 0.818885  [   96/  146]
train() client id: f_00005-5-3 loss: 0.684298  [  128/  146]
train() client id: f_00005-6-0 loss: 0.723153  [   32/  146]
train() client id: f_00005-6-1 loss: 0.630151  [   64/  146]
train() client id: f_00005-6-2 loss: 0.757561  [   96/  146]
train() client id: f_00005-6-3 loss: 0.736266  [  128/  146]
train() client id: f_00005-7-0 loss: 0.604262  [   32/  146]
train() client id: f_00005-7-1 loss: 0.813043  [   64/  146]
train() client id: f_00005-7-2 loss: 0.768248  [   96/  146]
train() client id: f_00005-7-3 loss: 0.744837  [  128/  146]
train() client id: f_00005-8-0 loss: 0.964504  [   32/  146]
train() client id: f_00005-8-1 loss: 0.751018  [   64/  146]
train() client id: f_00005-8-2 loss: 0.593537  [   96/  146]
train() client id: f_00005-8-3 loss: 0.592931  [  128/  146]
train() client id: f_00005-9-0 loss: 0.794948  [   32/  146]
train() client id: f_00005-9-1 loss: 0.729123  [   64/  146]
train() client id: f_00005-9-2 loss: 0.628455  [   96/  146]
train() client id: f_00005-9-3 loss: 0.762443  [  128/  146]
train() client id: f_00005-10-0 loss: 0.807508  [   32/  146]
train() client id: f_00005-10-1 loss: 0.722537  [   64/  146]
train() client id: f_00005-10-2 loss: 0.809983  [   96/  146]
train() client id: f_00005-10-3 loss: 0.614074  [  128/  146]
train() client id: f_00005-11-0 loss: 0.913966  [   32/  146]
train() client id: f_00005-11-1 loss: 0.933663  [   64/  146]
train() client id: f_00005-11-2 loss: 0.584972  [   96/  146]
train() client id: f_00005-11-3 loss: 0.612539  [  128/  146]
train() client id: f_00006-0-0 loss: 0.506203  [   32/   54]
train() client id: f_00006-1-0 loss: 0.599270  [   32/   54]
train() client id: f_00006-2-0 loss: 0.613893  [   32/   54]
train() client id: f_00006-3-0 loss: 0.508689  [   32/   54]
train() client id: f_00006-4-0 loss: 0.556428  [   32/   54]
train() client id: f_00006-5-0 loss: 0.598426  [   32/   54]
train() client id: f_00006-6-0 loss: 0.567879  [   32/   54]
train() client id: f_00006-7-0 loss: 0.600839  [   32/   54]
train() client id: f_00006-8-0 loss: 0.555488  [   32/   54]
train() client id: f_00006-9-0 loss: 0.615825  [   32/   54]
train() client id: f_00006-10-0 loss: 0.552813  [   32/   54]
train() client id: f_00006-11-0 loss: 0.508798  [   32/   54]
train() client id: f_00007-0-0 loss: 0.316803  [   32/  179]
train() client id: f_00007-0-1 loss: 0.371069  [   64/  179]
train() client id: f_00007-0-2 loss: 0.383128  [   96/  179]
train() client id: f_00007-0-3 loss: 0.555702  [  128/  179]
train() client id: f_00007-0-4 loss: 0.437456  [  160/  179]
train() client id: f_00007-1-0 loss: 0.655013  [   32/  179]
train() client id: f_00007-1-1 loss: 0.290043  [   64/  179]
train() client id: f_00007-1-2 loss: 0.487240  [   96/  179]
train() client id: f_00007-1-3 loss: 0.282110  [  128/  179]
train() client id: f_00007-1-4 loss: 0.305417  [  160/  179]
train() client id: f_00007-2-0 loss: 0.515004  [   32/  179]
train() client id: f_00007-2-1 loss: 0.380604  [   64/  179]
train() client id: f_00007-2-2 loss: 0.269927  [   96/  179]
train() client id: f_00007-2-3 loss: 0.503632  [  128/  179]
train() client id: f_00007-2-4 loss: 0.395394  [  160/  179]
train() client id: f_00007-3-0 loss: 0.352532  [   32/  179]
train() client id: f_00007-3-1 loss: 0.517971  [   64/  179]
train() client id: f_00007-3-2 loss: 0.263592  [   96/  179]
train() client id: f_00007-3-3 loss: 0.306070  [  128/  179]
train() client id: f_00007-3-4 loss: 0.502017  [  160/  179]
train() client id: f_00007-4-0 loss: 0.265061  [   32/  179]
train() client id: f_00007-4-1 loss: 0.328594  [   64/  179]
train() client id: f_00007-4-2 loss: 0.479177  [   96/  179]
train() client id: f_00007-4-3 loss: 0.460855  [  128/  179]
train() client id: f_00007-4-4 loss: 0.351533  [  160/  179]
train() client id: f_00007-5-0 loss: 0.261009  [   32/  179]
train() client id: f_00007-5-1 loss: 0.488448  [   64/  179]
train() client id: f_00007-5-2 loss: 0.307787  [   96/  179]
train() client id: f_00007-5-3 loss: 0.265149  [  128/  179]
train() client id: f_00007-5-4 loss: 0.374224  [  160/  179]
train() client id: f_00007-6-0 loss: 0.506665  [   32/  179]
train() client id: f_00007-6-1 loss: 0.474742  [   64/  179]
train() client id: f_00007-6-2 loss: 0.221633  [   96/  179]
train() client id: f_00007-6-3 loss: 0.301720  [  128/  179]
train() client id: f_00007-6-4 loss: 0.414285  [  160/  179]
train() client id: f_00007-7-0 loss: 0.564984  [   32/  179]
train() client id: f_00007-7-1 loss: 0.261528  [   64/  179]
train() client id: f_00007-7-2 loss: 0.308535  [   96/  179]
train() client id: f_00007-7-3 loss: 0.438004  [  128/  179]
train() client id: f_00007-7-4 loss: 0.337139  [  160/  179]
train() client id: f_00007-8-0 loss: 0.392334  [   32/  179]
train() client id: f_00007-8-1 loss: 0.334019  [   64/  179]
train() client id: f_00007-8-2 loss: 0.373106  [   96/  179]
train() client id: f_00007-8-3 loss: 0.386597  [  128/  179]
train() client id: f_00007-8-4 loss: 0.336071  [  160/  179]
train() client id: f_00007-9-0 loss: 0.552928  [   32/  179]
train() client id: f_00007-9-1 loss: 0.435135  [   64/  179]
train() client id: f_00007-9-2 loss: 0.319309  [   96/  179]
train() client id: f_00007-9-3 loss: 0.292386  [  128/  179]
train() client id: f_00007-9-4 loss: 0.279101  [  160/  179]
train() client id: f_00007-10-0 loss: 0.460984  [   32/  179]
train() client id: f_00007-10-1 loss: 0.361433  [   64/  179]
train() client id: f_00007-10-2 loss: 0.224532  [   96/  179]
train() client id: f_00007-10-3 loss: 0.341432  [  128/  179]
train() client id: f_00007-10-4 loss: 0.394853  [  160/  179]
train() client id: f_00007-11-0 loss: 0.388260  [   32/  179]
train() client id: f_00007-11-1 loss: 0.674930  [   64/  179]
train() client id: f_00007-11-2 loss: 0.295240  [   96/  179]
train() client id: f_00007-11-3 loss: 0.236370  [  128/  179]
train() client id: f_00007-11-4 loss: 0.210113  [  160/  179]
train() client id: f_00008-0-0 loss: 0.892858  [   32/  130]
train() client id: f_00008-0-1 loss: 0.659514  [   64/  130]
train() client id: f_00008-0-2 loss: 0.743366  [   96/  130]
train() client id: f_00008-0-3 loss: 0.763632  [  128/  130]
train() client id: f_00008-1-0 loss: 0.807716  [   32/  130]
train() client id: f_00008-1-1 loss: 0.727542  [   64/  130]
train() client id: f_00008-1-2 loss: 0.735466  [   96/  130]
train() client id: f_00008-1-3 loss: 0.782577  [  128/  130]
train() client id: f_00008-2-0 loss: 0.801156  [   32/  130]
train() client id: f_00008-2-1 loss: 0.795275  [   64/  130]
train() client id: f_00008-2-2 loss: 0.615722  [   96/  130]
train() client id: f_00008-2-3 loss: 0.844402  [  128/  130]
train() client id: f_00008-3-0 loss: 0.662934  [   32/  130]
train() client id: f_00008-3-1 loss: 0.760823  [   64/  130]
train() client id: f_00008-3-2 loss: 0.871401  [   96/  130]
train() client id: f_00008-3-3 loss: 0.760130  [  128/  130]
train() client id: f_00008-4-0 loss: 0.785090  [   32/  130]
train() client id: f_00008-4-1 loss: 0.720379  [   64/  130]
train() client id: f_00008-4-2 loss: 0.737160  [   96/  130]
train() client id: f_00008-4-3 loss: 0.731011  [  128/  130]
train() client id: f_00008-5-0 loss: 0.793585  [   32/  130]
train() client id: f_00008-5-1 loss: 0.681049  [   64/  130]
train() client id: f_00008-5-2 loss: 0.818378  [   96/  130]
train() client id: f_00008-5-3 loss: 0.765007  [  128/  130]
train() client id: f_00008-6-0 loss: 0.708727  [   32/  130]
train() client id: f_00008-6-1 loss: 0.800835  [   64/  130]
train() client id: f_00008-6-2 loss: 0.744853  [   96/  130]
train() client id: f_00008-6-3 loss: 0.782916  [  128/  130]
train() client id: f_00008-7-0 loss: 0.843305  [   32/  130]
train() client id: f_00008-7-1 loss: 0.718889  [   64/  130]
train() client id: f_00008-7-2 loss: 0.742434  [   96/  130]
train() client id: f_00008-7-3 loss: 0.756074  [  128/  130]
train() client id: f_00008-8-0 loss: 0.793084  [   32/  130]
train() client id: f_00008-8-1 loss: 0.697301  [   64/  130]
train() client id: f_00008-8-2 loss: 0.783368  [   96/  130]
train() client id: f_00008-8-3 loss: 0.755696  [  128/  130]
train() client id: f_00008-9-0 loss: 0.698632  [   32/  130]
train() client id: f_00008-9-1 loss: 0.844127  [   64/  130]
train() client id: f_00008-9-2 loss: 0.755761  [   96/  130]
train() client id: f_00008-9-3 loss: 0.760789  [  128/  130]
train() client id: f_00008-10-0 loss: 0.678269  [   32/  130]
train() client id: f_00008-10-1 loss: 0.755848  [   64/  130]
train() client id: f_00008-10-2 loss: 0.962961  [   96/  130]
train() client id: f_00008-10-3 loss: 0.673164  [  128/  130]
train() client id: f_00008-11-0 loss: 0.639259  [   32/  130]
train() client id: f_00008-11-1 loss: 0.809073  [   64/  130]
train() client id: f_00008-11-2 loss: 0.770958  [   96/  130]
train() client id: f_00008-11-3 loss: 0.844476  [  128/  130]
train() client id: f_00009-0-0 loss: 1.096669  [   32/  118]
train() client id: f_00009-0-1 loss: 1.040837  [   64/  118]
train() client id: f_00009-0-2 loss: 1.188388  [   96/  118]
train() client id: f_00009-1-0 loss: 1.072579  [   32/  118]
train() client id: f_00009-1-1 loss: 0.991300  [   64/  118]
train() client id: f_00009-1-2 loss: 1.181103  [   96/  118]
train() client id: f_00009-2-0 loss: 1.075499  [   32/  118]
train() client id: f_00009-2-1 loss: 0.947299  [   64/  118]
train() client id: f_00009-2-2 loss: 1.051124  [   96/  118]
train() client id: f_00009-3-0 loss: 1.127791  [   32/  118]
train() client id: f_00009-3-1 loss: 0.961224  [   64/  118]
train() client id: f_00009-3-2 loss: 1.021629  [   96/  118]
train() client id: f_00009-4-0 loss: 1.017319  [   32/  118]
train() client id: f_00009-4-1 loss: 0.976477  [   64/  118]
train() client id: f_00009-4-2 loss: 0.995586  [   96/  118]
train() client id: f_00009-5-0 loss: 0.880218  [   32/  118]
train() client id: f_00009-5-1 loss: 1.029081  [   64/  118]
train() client id: f_00009-5-2 loss: 0.968252  [   96/  118]
train() client id: f_00009-6-0 loss: 0.978988  [   32/  118]
train() client id: f_00009-6-1 loss: 0.868785  [   64/  118]
train() client id: f_00009-6-2 loss: 1.094699  [   96/  118]
train() client id: f_00009-7-0 loss: 0.932060  [   32/  118]
train() client id: f_00009-7-1 loss: 0.977156  [   64/  118]
train() client id: f_00009-7-2 loss: 1.038453  [   96/  118]
train() client id: f_00009-8-0 loss: 1.218735  [   32/  118]
train() client id: f_00009-8-1 loss: 0.786253  [   64/  118]
train() client id: f_00009-8-2 loss: 0.876621  [   96/  118]
train() client id: f_00009-9-0 loss: 0.989965  [   32/  118]
train() client id: f_00009-9-1 loss: 1.042835  [   64/  118]
train() client id: f_00009-9-2 loss: 0.871539  [   96/  118]
train() client id: f_00009-10-0 loss: 0.854022  [   32/  118]
train() client id: f_00009-10-1 loss: 0.986534  [   64/  118]
train() client id: f_00009-10-2 loss: 0.939256  [   96/  118]
train() client id: f_00009-11-0 loss: 0.879012  [   32/  118]
train() client id: f_00009-11-1 loss: 0.860036  [   64/  118]
train() client id: f_00009-11-2 loss: 0.883388  [   96/  118]
At round 22 accuracy: 0.6419098143236074
At round 22 training accuracy: 0.5821596244131455
At round 22 training loss: 0.8274295684752211
update_location
xs = -4.528292 11.001589 20.045120 -20.943528 -80.103519 -15.217951 -62.215960 83.375741 -1.680116 4.695607 
ys = 97.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 -3.124922 -19.177652 -27.154970 4.001482 
xs mean: -6.557130840587133
ys mean: 8.371751218646876
dists_uav = 139.799554 101.798915 101.997798 104.627437 128.467894 102.600874 117.815919 131.602798 103.635009 100.190122 
uav_gains = -103.639418 -100.193599 -100.214791 -100.491170 -102.720421 -100.278800 -101.780265 -102.982404 -100.387689 -100.020639 
uav_gains_db_mean: -101.27091948009831
dists_bs = 187.225071 244.987820 261.164944 216.739422 190.906267 249.931398 210.828668 323.207339 266.282583 248.054439 
bs_gains = -103.193251 -106.463105 -107.240676 -104.973321 -103.430024 -106.706042 -104.637090 -109.832533 -107.476656 -106.614375 
bs_gains_db_mean: -106.05670736971724
Round 23
-------------------------------
ene_coms = [0.00742089 0.00859667 0.0063591  0.00643355 0.00734609 0.00871486
 0.00680476 0.00719104 0.00911133 0.0086699 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.94582522 16.56767312  7.80034048  2.79167706 19.07804696  9.20944741
  3.47968663 11.21938368  8.22791142  7.47660769]
obj_prev = 93.79659965306418
eta_min = 2.087843507705813e-12	eta_max = 0.9348672073276055
af = 19.826056636829264	bf = 1.5562687268960833	zeta = 21.808662300512193	eta = 0.909090909090909
af = 19.826056636829264	bf = 1.5562687268960833	zeta = 37.904831247393055	eta = 0.5230482760213534
af = 19.826056636829264	bf = 1.5562687268960833	zeta = 30.2025735286035	eta = 0.6564360026490093
af = 19.826056636829264	bf = 1.5562687268960833	zeta = 28.820401041155314	eta = 0.6879174446086925
af = 19.826056636829264	bf = 1.5562687268960833	zeta = 28.75177592955566	eta = 0.6895593748853921
af = 19.826056636829264	bf = 1.5562687268960833	zeta = 28.751594593578055	eta = 0.6895637239284671
eta = 0.6895637239284671
ene_coms = [0.00742089 0.00859667 0.0063591  0.00643355 0.00734609 0.00871486
 0.00680476 0.00719104 0.00911133 0.0086699 ]
ene_comp = [0.03067148 0.06450747 0.03018463 0.01046725 0.07448787 0.03553997
 0.01314492 0.04357297 0.03164517 0.02872408]
ene_total = [2.49142695 4.78136786 2.39013858 1.10539458 5.35234075 2.89448178
 1.30480671 3.3202141  2.66567434 2.44574894]
ti_comp = [0.36688816 0.35513038 0.37750604 0.37676163 0.36763618 0.35394847
 0.37304944 0.3691867  0.3499838  0.35439809]
ti_coms = [0.07420892 0.08596671 0.06359105 0.06433546 0.0734609  0.08714862
 0.06804765 0.07191039 0.09111329 0.08669899]
t_total = [28.84990349 28.84990349 28.84990349 28.84990349 28.84990349 28.84990349
 28.84990349 28.84990349 28.84990349 28.84990349]
ene_coms = [0.00742089 0.00859667 0.0063591  0.00643355 0.00734609 0.00871486
 0.00680476 0.00719104 0.00911133 0.0086699 ]
ene_comp = [1.33972812e-05 1.33025352e-04 1.20611770e-05 5.04945462e-07
 1.91117397e-04 2.23949929e-05 1.02004877e-06 3.79349427e-05
 1.61698732e-05 1.17933098e-05]
ene_total = [0.48623885 0.57096478 0.41670538 0.42081834 0.4929702  0.57145926
 0.44513156 0.47281017 0.59698299 0.5678251 ]
optimize_network iter = 0 obj = 5.041906628443364
eta = 0.6895637239284671
freqs = [4.17994906e+07 9.08222375e+07 3.99790001e+07 1.38910775e+07
 1.01306503e+08 5.02049989e+07 1.76181991e+07 5.90121077e+07
 4.52094847e+07 4.05251634e+07]
eta_min = 0.6895637239284693	eta_max = 0.6895637239284627
af = 0.02400345218942156	bf = 1.5562687268960833	zeta = 0.026403797408363718	eta = 0.9090909090909091
af = 0.02400345218942156	bf = 1.5562687268960833	zeta = 17.14412150264199	eta = 0.0014000981144307986
af = 0.02400345218942156	bf = 1.5562687268960833	zeta = 1.7164036566051022	eta = 0.013984736106248055
af = 0.02400345218942156	bf = 1.5562687268960833	zeta = 1.682284706425487	eta = 0.014268364978734197
af = 0.02400345218942156	bf = 1.5562687268960833	zeta = 1.6822798306799474	eta = 0.014268406332684733
eta = 0.014268406332684733
ene_coms = [0.00742089 0.00859667 0.0063591  0.00643355 0.00734609 0.00871486
 0.00680476 0.00719104 0.00911133 0.0086699 ]
ene_comp = [1.53174910e-04 1.52091653e-03 1.37898852e-04 5.77318448e-06
 2.18509933e-03 2.56048296e-04 1.16625064e-05 4.33720944e-04
 1.84874741e-04 1.34836251e-04]
ene_total = [0.15601025 0.20840155 0.13382495 0.13263676 0.19632296 0.18478234
 0.14040442 0.1570544  0.19148272 0.18135949]
ti_comp = [0.36688816 0.35513038 0.37750604 0.37676163 0.36763618 0.35394847
 0.37304944 0.3691867  0.3499838  0.35439809]
ti_coms = [0.07420892 0.08596671 0.06359105 0.06433546 0.0734609  0.08714862
 0.06804765 0.07191039 0.09111329 0.08669899]
t_total = [28.84990349 28.84990349 28.84990349 28.84990349 28.84990349 28.84990349
 28.84990349 28.84990349 28.84990349 28.84990349]
ene_coms = [0.00742089 0.00859667 0.0063591  0.00643355 0.00734609 0.00871486
 0.00680476 0.00719104 0.00911133 0.0086699 ]
ene_comp = [1.33972812e-05 1.33025352e-04 1.20611770e-05 5.04945462e-07
 1.91117397e-04 2.23949929e-05 1.02004877e-06 3.79349427e-05
 1.61698732e-05 1.17933098e-05]
ene_total = [0.48623885 0.57096478 0.41670538 0.42081834 0.4929702  0.57145926
 0.44513156 0.47281017 0.59698299 0.5678251 ]
optimize_network iter = 1 obj = 5.0419066284433995
eta = 0.6895637239284693
freqs = [4.17994906e+07 9.08222375e+07 3.99790001e+07 1.38910775e+07
 1.01306503e+08 5.02049989e+07 1.76181991e+07 5.90121077e+07
 4.52094847e+07 4.05251634e+07]
Done!
ene_coms = [0.00742089 0.00859667 0.0063591  0.00643355 0.00734609 0.00871486
 0.00680476 0.00719104 0.00911133 0.0086699 ]
ene_comp = [1.32088124e-05 1.31153993e-04 1.18915041e-05 4.97842048e-07
 1.88428818e-04 2.20799472e-05 1.00569904e-06 3.74012858e-05
 1.59424006e-05 1.16274052e-05]
ene_total = [0.0074341  0.00872783 0.006371   0.00643404 0.00753452 0.00873694
 0.00680577 0.00722844 0.00912727 0.00868153]
At round 23 energy consumption: 0.07708143604139418
At round 23 eta: 0.6895637239284693
At round 23 a_n: 20.30404837596008
At round 23 local rounds: 12.171220902388386
At round 23 global rounds: 65.40488319503493
gradient difference: 0.39109593629837036
train() client id: f_00000-0-0 loss: 1.120193  [   32/  126]
train() client id: f_00000-0-1 loss: 1.036068  [   64/  126]
train() client id: f_00000-0-2 loss: 1.196282  [   96/  126]
train() client id: f_00000-1-0 loss: 0.991353  [   32/  126]
train() client id: f_00000-1-1 loss: 1.032199  [   64/  126]
train() client id: f_00000-1-2 loss: 0.905032  [   96/  126]
train() client id: f_00000-2-0 loss: 0.975951  [   32/  126]
train() client id: f_00000-2-1 loss: 1.070575  [   64/  126]
train() client id: f_00000-2-2 loss: 0.861084  [   96/  126]
train() client id: f_00000-3-0 loss: 0.817114  [   32/  126]
train() client id: f_00000-3-1 loss: 1.049637  [   64/  126]
train() client id: f_00000-3-2 loss: 0.853084  [   96/  126]
train() client id: f_00000-4-0 loss: 0.920201  [   32/  126]
train() client id: f_00000-4-1 loss: 0.773784  [   64/  126]
train() client id: f_00000-4-2 loss: 0.878326  [   96/  126]
train() client id: f_00000-5-0 loss: 0.855275  [   32/  126]
train() client id: f_00000-5-1 loss: 0.813472  [   64/  126]
train() client id: f_00000-5-2 loss: 0.865695  [   96/  126]
train() client id: f_00000-6-0 loss: 0.758917  [   32/  126]
train() client id: f_00000-6-1 loss: 0.737695  [   64/  126]
train() client id: f_00000-6-2 loss: 0.792579  [   96/  126]
train() client id: f_00000-7-0 loss: 0.708493  [   32/  126]
train() client id: f_00000-7-1 loss: 0.890949  [   64/  126]
train() client id: f_00000-7-2 loss: 0.826702  [   96/  126]
train() client id: f_00000-8-0 loss: 0.685623  [   32/  126]
train() client id: f_00000-8-1 loss: 0.874303  [   64/  126]
train() client id: f_00000-8-2 loss: 0.758096  [   96/  126]
train() client id: f_00000-9-0 loss: 0.857676  [   32/  126]
train() client id: f_00000-9-1 loss: 0.696012  [   64/  126]
train() client id: f_00000-9-2 loss: 0.659788  [   96/  126]
train() client id: f_00000-10-0 loss: 0.821649  [   32/  126]
train() client id: f_00000-10-1 loss: 0.826272  [   64/  126]
train() client id: f_00000-10-2 loss: 0.618523  [   96/  126]
train() client id: f_00000-11-0 loss: 0.632303  [   32/  126]
train() client id: f_00000-11-1 loss: 0.777439  [   64/  126]
train() client id: f_00000-11-2 loss: 0.852757  [   96/  126]
train() client id: f_00001-0-0 loss: 0.530140  [   32/  265]
train() client id: f_00001-0-1 loss: 0.382765  [   64/  265]
train() client id: f_00001-0-2 loss: 0.430997  [   96/  265]
train() client id: f_00001-0-3 loss: 0.441108  [  128/  265]
train() client id: f_00001-0-4 loss: 0.396878  [  160/  265]
train() client id: f_00001-0-5 loss: 0.408220  [  192/  265]
train() client id: f_00001-0-6 loss: 0.373910  [  224/  265]
train() client id: f_00001-0-7 loss: 0.575897  [  256/  265]
train() client id: f_00001-1-0 loss: 0.418263  [   32/  265]
train() client id: f_00001-1-1 loss: 0.350629  [   64/  265]
train() client id: f_00001-1-2 loss: 0.511713  [   96/  265]
train() client id: f_00001-1-3 loss: 0.429473  [  128/  265]
train() client id: f_00001-1-4 loss: 0.471349  [  160/  265]
train() client id: f_00001-1-5 loss: 0.439192  [  192/  265]
train() client id: f_00001-1-6 loss: 0.520356  [  224/  265]
train() client id: f_00001-1-7 loss: 0.336586  [  256/  265]
train() client id: f_00001-2-0 loss: 0.506178  [   32/  265]
train() client id: f_00001-2-1 loss: 0.464743  [   64/  265]
train() client id: f_00001-2-2 loss: 0.426153  [   96/  265]
train() client id: f_00001-2-3 loss: 0.430492  [  128/  265]
train() client id: f_00001-2-4 loss: 0.420869  [  160/  265]
train() client id: f_00001-2-5 loss: 0.426666  [  192/  265]
train() client id: f_00001-2-6 loss: 0.381090  [  224/  265]
train() client id: f_00001-2-7 loss: 0.362107  [  256/  265]
train() client id: f_00001-3-0 loss: 0.443839  [   32/  265]
train() client id: f_00001-3-1 loss: 0.430825  [   64/  265]
train() client id: f_00001-3-2 loss: 0.413133  [   96/  265]
train() client id: f_00001-3-3 loss: 0.398479  [  128/  265]
train() client id: f_00001-3-4 loss: 0.426715  [  160/  265]
train() client id: f_00001-3-5 loss: 0.474595  [  192/  265]
train() client id: f_00001-3-6 loss: 0.320946  [  224/  265]
train() client id: f_00001-3-7 loss: 0.464356  [  256/  265]
train() client id: f_00001-4-0 loss: 0.347961  [   32/  265]
train() client id: f_00001-4-1 loss: 0.412922  [   64/  265]
train() client id: f_00001-4-2 loss: 0.494905  [   96/  265]
train() client id: f_00001-4-3 loss: 0.438955  [  128/  265]
train() client id: f_00001-4-4 loss: 0.362239  [  160/  265]
train() client id: f_00001-4-5 loss: 0.465426  [  192/  265]
train() client id: f_00001-4-6 loss: 0.377126  [  224/  265]
train() client id: f_00001-4-7 loss: 0.412694  [  256/  265]
train() client id: f_00001-5-0 loss: 0.421090  [   32/  265]
train() client id: f_00001-5-1 loss: 0.322111  [   64/  265]
train() client id: f_00001-5-2 loss: 0.661299  [   96/  265]
train() client id: f_00001-5-3 loss: 0.362779  [  128/  265]
train() client id: f_00001-5-4 loss: 0.341660  [  160/  265]
train() client id: f_00001-5-5 loss: 0.371336  [  192/  265]
train() client id: f_00001-5-6 loss: 0.432465  [  224/  265]
train() client id: f_00001-5-7 loss: 0.374544  [  256/  265]
train() client id: f_00001-6-0 loss: 0.460048  [   32/  265]
train() client id: f_00001-6-1 loss: 0.317850  [   64/  265]
train() client id: f_00001-6-2 loss: 0.399585  [   96/  265]
train() client id: f_00001-6-3 loss: 0.473804  [  128/  265]
train() client id: f_00001-6-4 loss: 0.362646  [  160/  265]
train() client id: f_00001-6-5 loss: 0.512969  [  192/  265]
train() client id: f_00001-6-6 loss: 0.359876  [  224/  265]
train() client id: f_00001-6-7 loss: 0.413244  [  256/  265]
train() client id: f_00001-7-0 loss: 0.566565  [   32/  265]
train() client id: f_00001-7-1 loss: 0.403310  [   64/  265]
train() client id: f_00001-7-2 loss: 0.303107  [   96/  265]
train() client id: f_00001-7-3 loss: 0.407344  [  128/  265]
train() client id: f_00001-7-4 loss: 0.385789  [  160/  265]
train() client id: f_00001-7-5 loss: 0.490729  [  192/  265]
train() client id: f_00001-7-6 loss: 0.368436  [  224/  265]
train() client id: f_00001-7-7 loss: 0.356945  [  256/  265]
train() client id: f_00001-8-0 loss: 0.383998  [   32/  265]
train() client id: f_00001-8-1 loss: 0.500990  [   64/  265]
train() client id: f_00001-8-2 loss: 0.341611  [   96/  265]
train() client id: f_00001-8-3 loss: 0.435695  [  128/  265]
train() client id: f_00001-8-4 loss: 0.331345  [  160/  265]
train() client id: f_00001-8-5 loss: 0.363162  [  192/  265]
train() client id: f_00001-8-6 loss: 0.371410  [  224/  265]
train() client id: f_00001-8-7 loss: 0.517500  [  256/  265]
train() client id: f_00001-9-0 loss: 0.337510  [   32/  265]
train() client id: f_00001-9-1 loss: 0.464153  [   64/  265]
train() client id: f_00001-9-2 loss: 0.384362  [   96/  265]
train() client id: f_00001-9-3 loss: 0.321195  [  128/  265]
train() client id: f_00001-9-4 loss: 0.435128  [  160/  265]
train() client id: f_00001-9-5 loss: 0.473248  [  192/  265]
train() client id: f_00001-9-6 loss: 0.412591  [  224/  265]
train() client id: f_00001-9-7 loss: 0.403482  [  256/  265]
train() client id: f_00001-10-0 loss: 0.412663  [   32/  265]
train() client id: f_00001-10-1 loss: 0.393693  [   64/  265]
train() client id: f_00001-10-2 loss: 0.406610  [   96/  265]
train() client id: f_00001-10-3 loss: 0.422273  [  128/  265]
train() client id: f_00001-10-4 loss: 0.435123  [  160/  265]
train() client id: f_00001-10-5 loss: 0.442395  [  192/  265]
train() client id: f_00001-10-6 loss: 0.324685  [  224/  265]
train() client id: f_00001-10-7 loss: 0.431765  [  256/  265]
train() client id: f_00001-11-0 loss: 0.534155  [   32/  265]
train() client id: f_00001-11-1 loss: 0.374602  [   64/  265]
train() client id: f_00001-11-2 loss: 0.367973  [   96/  265]
train() client id: f_00001-11-3 loss: 0.368904  [  128/  265]
train() client id: f_00001-11-4 loss: 0.465092  [  160/  265]
train() client id: f_00001-11-5 loss: 0.316291  [  192/  265]
train() client id: f_00001-11-6 loss: 0.426389  [  224/  265]
train() client id: f_00001-11-7 loss: 0.420609  [  256/  265]
train() client id: f_00002-0-0 loss: 1.048679  [   32/  124]
train() client id: f_00002-0-1 loss: 0.875288  [   64/  124]
train() client id: f_00002-0-2 loss: 1.125018  [   96/  124]
train() client id: f_00002-1-0 loss: 1.056674  [   32/  124]
train() client id: f_00002-1-1 loss: 1.037975  [   64/  124]
train() client id: f_00002-1-2 loss: 0.946311  [   96/  124]
train() client id: f_00002-2-0 loss: 0.849177  [   32/  124]
train() client id: f_00002-2-1 loss: 1.185168  [   64/  124]
train() client id: f_00002-2-2 loss: 0.881716  [   96/  124]
train() client id: f_00002-3-0 loss: 0.762883  [   32/  124]
train() client id: f_00002-3-1 loss: 0.852452  [   64/  124]
train() client id: f_00002-3-2 loss: 1.028823  [   96/  124]
train() client id: f_00002-4-0 loss: 0.928978  [   32/  124]
train() client id: f_00002-4-1 loss: 0.831530  [   64/  124]
train() client id: f_00002-4-2 loss: 0.955183  [   96/  124]
train() client id: f_00002-5-0 loss: 0.858061  [   32/  124]
train() client id: f_00002-5-1 loss: 0.711615  [   64/  124]
train() client id: f_00002-5-2 loss: 0.858038  [   96/  124]
train() client id: f_00002-6-0 loss: 1.040920  [   32/  124]
train() client id: f_00002-6-1 loss: 0.618127  [   64/  124]
train() client id: f_00002-6-2 loss: 0.823185  [   96/  124]
train() client id: f_00002-7-0 loss: 0.821574  [   32/  124]
train() client id: f_00002-7-1 loss: 0.685246  [   64/  124]
train() client id: f_00002-7-2 loss: 0.919213  [   96/  124]
train() client id: f_00002-8-0 loss: 0.829240  [   32/  124]
train() client id: f_00002-8-1 loss: 0.781356  [   64/  124]
train() client id: f_00002-8-2 loss: 0.709166  [   96/  124]
train() client id: f_00002-9-0 loss: 0.747135  [   32/  124]
train() client id: f_00002-9-1 loss: 0.663752  [   64/  124]
train() client id: f_00002-9-2 loss: 0.928379  [   96/  124]
train() client id: f_00002-10-0 loss: 0.923814  [   32/  124]
train() client id: f_00002-10-1 loss: 0.744140  [   64/  124]
train() client id: f_00002-10-2 loss: 0.707535  [   96/  124]
train() client id: f_00002-11-0 loss: 0.820189  [   32/  124]
train() client id: f_00002-11-1 loss: 0.722940  [   64/  124]
train() client id: f_00002-11-2 loss: 0.760739  [   96/  124]
train() client id: f_00003-0-0 loss: 0.844667  [   32/   43]
train() client id: f_00003-1-0 loss: 0.805124  [   32/   43]
train() client id: f_00003-2-0 loss: 0.771625  [   32/   43]
train() client id: f_00003-3-0 loss: 0.787339  [   32/   43]
train() client id: f_00003-4-0 loss: 0.729304  [   32/   43]
train() client id: f_00003-5-0 loss: 0.653683  [   32/   43]
train() client id: f_00003-6-0 loss: 0.784277  [   32/   43]
train() client id: f_00003-7-0 loss: 0.781147  [   32/   43]
train() client id: f_00003-8-0 loss: 0.716572  [   32/   43]
train() client id: f_00003-9-0 loss: 0.732313  [   32/   43]
train() client id: f_00003-10-0 loss: 0.750065  [   32/   43]
train() client id: f_00003-11-0 loss: 0.782856  [   32/   43]
train() client id: f_00004-0-0 loss: 0.871489  [   32/  306]
train() client id: f_00004-0-1 loss: 0.856238  [   64/  306]
train() client id: f_00004-0-2 loss: 0.705808  [   96/  306]
train() client id: f_00004-0-3 loss: 0.668057  [  128/  306]
train() client id: f_00004-0-4 loss: 0.698072  [  160/  306]
train() client id: f_00004-0-5 loss: 0.847090  [  192/  306]
train() client id: f_00004-0-6 loss: 0.667044  [  224/  306]
train() client id: f_00004-0-7 loss: 0.870720  [  256/  306]
train() client id: f_00004-0-8 loss: 0.771978  [  288/  306]
train() client id: f_00004-1-0 loss: 0.837011  [   32/  306]
train() client id: f_00004-1-1 loss: 0.678687  [   64/  306]
train() client id: f_00004-1-2 loss: 0.810599  [   96/  306]
train() client id: f_00004-1-3 loss: 0.837566  [  128/  306]
train() client id: f_00004-1-4 loss: 0.859119  [  160/  306]
train() client id: f_00004-1-5 loss: 0.749762  [  192/  306]
train() client id: f_00004-1-6 loss: 0.754506  [  224/  306]
train() client id: f_00004-1-7 loss: 0.739325  [  256/  306]
train() client id: f_00004-1-8 loss: 0.818296  [  288/  306]
train() client id: f_00004-2-0 loss: 0.824790  [   32/  306]
train() client id: f_00004-2-1 loss: 0.757342  [   64/  306]
train() client id: f_00004-2-2 loss: 0.900285  [   96/  306]
train() client id: f_00004-2-3 loss: 0.775046  [  128/  306]
train() client id: f_00004-2-4 loss: 0.836863  [  160/  306]
train() client id: f_00004-2-5 loss: 0.720098  [  192/  306]
train() client id: f_00004-2-6 loss: 0.729957  [  224/  306]
train() client id: f_00004-2-7 loss: 0.747923  [  256/  306]
train() client id: f_00004-2-8 loss: 0.730074  [  288/  306]
train() client id: f_00004-3-0 loss: 0.786434  [   32/  306]
train() client id: f_00004-3-1 loss: 0.760375  [   64/  306]
train() client id: f_00004-3-2 loss: 0.831886  [   96/  306]
train() client id: f_00004-3-3 loss: 0.871833  [  128/  306]
train() client id: f_00004-3-4 loss: 0.738985  [  160/  306]
train() client id: f_00004-3-5 loss: 0.787317  [  192/  306]
train() client id: f_00004-3-6 loss: 0.846697  [  224/  306]
train() client id: f_00004-3-7 loss: 0.754989  [  256/  306]
train() client id: f_00004-3-8 loss: 0.704531  [  288/  306]
train() client id: f_00004-4-0 loss: 0.747336  [   32/  306]
train() client id: f_00004-4-1 loss: 0.798242  [   64/  306]
train() client id: f_00004-4-2 loss: 0.752358  [   96/  306]
train() client id: f_00004-4-3 loss: 0.748627  [  128/  306]
train() client id: f_00004-4-4 loss: 0.894313  [  160/  306]
train() client id: f_00004-4-5 loss: 0.752281  [  192/  306]
train() client id: f_00004-4-6 loss: 0.785212  [  224/  306]
train() client id: f_00004-4-7 loss: 0.712707  [  256/  306]
train() client id: f_00004-4-8 loss: 0.827090  [  288/  306]
train() client id: f_00004-5-0 loss: 0.786395  [   32/  306]
train() client id: f_00004-5-1 loss: 0.725064  [   64/  306]
train() client id: f_00004-5-2 loss: 0.755998  [   96/  306]
train() client id: f_00004-5-3 loss: 0.755234  [  128/  306]
train() client id: f_00004-5-4 loss: 0.834376  [  160/  306]
train() client id: f_00004-5-5 loss: 0.883955  [  192/  306]
train() client id: f_00004-5-6 loss: 0.769337  [  224/  306]
train() client id: f_00004-5-7 loss: 0.754128  [  256/  306]
train() client id: f_00004-5-8 loss: 0.761407  [  288/  306]
train() client id: f_00004-6-0 loss: 0.844990  [   32/  306]
train() client id: f_00004-6-1 loss: 0.906013  [   64/  306]
train() client id: f_00004-6-2 loss: 0.777394  [   96/  306]
train() client id: f_00004-6-3 loss: 0.804269  [  128/  306]
train() client id: f_00004-6-4 loss: 0.762824  [  160/  306]
train() client id: f_00004-6-5 loss: 0.718387  [  192/  306]
train() client id: f_00004-6-6 loss: 0.723256  [  224/  306]
train() client id: f_00004-6-7 loss: 0.806566  [  256/  306]
train() client id: f_00004-6-8 loss: 0.769561  [  288/  306]
train() client id: f_00004-7-0 loss: 0.756910  [   32/  306]
train() client id: f_00004-7-1 loss: 0.788482  [   64/  306]
train() client id: f_00004-7-2 loss: 0.874391  [   96/  306]
train() client id: f_00004-7-3 loss: 0.730079  [  128/  306]
train() client id: f_00004-7-4 loss: 0.980487  [  160/  306]
train() client id: f_00004-7-5 loss: 0.735541  [  192/  306]
train() client id: f_00004-7-6 loss: 0.618923  [  224/  306]
train() client id: f_00004-7-7 loss: 0.773635  [  256/  306]
train() client id: f_00004-7-8 loss: 0.802141  [  288/  306]
train() client id: f_00004-8-0 loss: 0.783925  [   32/  306]
train() client id: f_00004-8-1 loss: 0.755043  [   64/  306]
train() client id: f_00004-8-2 loss: 0.778451  [   96/  306]
train() client id: f_00004-8-3 loss: 0.679316  [  128/  306]
train() client id: f_00004-8-4 loss: 0.890893  [  160/  306]
train() client id: f_00004-8-5 loss: 0.713758  [  192/  306]
train() client id: f_00004-8-6 loss: 0.776543  [  224/  306]
train() client id: f_00004-8-7 loss: 0.866353  [  256/  306]
train() client id: f_00004-8-8 loss: 0.819898  [  288/  306]
train() client id: f_00004-9-0 loss: 0.869397  [   32/  306]
train() client id: f_00004-9-1 loss: 0.750283  [   64/  306]
train() client id: f_00004-9-2 loss: 0.891625  [   96/  306]
train() client id: f_00004-9-3 loss: 0.701400  [  128/  306]
train() client id: f_00004-9-4 loss: 0.687000  [  160/  306]
train() client id: f_00004-9-5 loss: 0.782803  [  192/  306]
train() client id: f_00004-9-6 loss: 0.910212  [  224/  306]
train() client id: f_00004-9-7 loss: 0.753272  [  256/  306]
train() client id: f_00004-9-8 loss: 0.724057  [  288/  306]
train() client id: f_00004-10-0 loss: 0.895749  [   32/  306]
train() client id: f_00004-10-1 loss: 0.846262  [   64/  306]
train() client id: f_00004-10-2 loss: 0.663740  [   96/  306]
train() client id: f_00004-10-3 loss: 0.759668  [  128/  306]
train() client id: f_00004-10-4 loss: 0.884518  [  160/  306]
train() client id: f_00004-10-5 loss: 0.864181  [  192/  306]
train() client id: f_00004-10-6 loss: 0.690869  [  224/  306]
train() client id: f_00004-10-7 loss: 0.668694  [  256/  306]
train() client id: f_00004-10-8 loss: 0.864951  [  288/  306]
train() client id: f_00004-11-0 loss: 0.817389  [   32/  306]
train() client id: f_00004-11-1 loss: 0.907161  [   64/  306]
train() client id: f_00004-11-2 loss: 0.823092  [   96/  306]
train() client id: f_00004-11-3 loss: 0.760063  [  128/  306]
train() client id: f_00004-11-4 loss: 0.729039  [  160/  306]
train() client id: f_00004-11-5 loss: 0.794527  [  192/  306]
train() client id: f_00004-11-6 loss: 0.756505  [  224/  306]
train() client id: f_00004-11-7 loss: 0.780154  [  256/  306]
train() client id: f_00004-11-8 loss: 0.789777  [  288/  306]
train() client id: f_00005-0-0 loss: 0.726157  [   32/  146]
train() client id: f_00005-0-1 loss: 0.795641  [   64/  146]
train() client id: f_00005-0-2 loss: 0.631705  [   96/  146]
train() client id: f_00005-0-3 loss: 0.802495  [  128/  146]
train() client id: f_00005-1-0 loss: 0.860940  [   32/  146]
train() client id: f_00005-1-1 loss: 0.892328  [   64/  146]
train() client id: f_00005-1-2 loss: 0.685498  [   96/  146]
train() client id: f_00005-1-3 loss: 0.597373  [  128/  146]
train() client id: f_00005-2-0 loss: 0.653845  [   32/  146]
train() client id: f_00005-2-1 loss: 0.766424  [   64/  146]
train() client id: f_00005-2-2 loss: 0.653627  [   96/  146]
train() client id: f_00005-2-3 loss: 0.924975  [  128/  146]
train() client id: f_00005-3-0 loss: 0.764490  [   32/  146]
train() client id: f_00005-3-1 loss: 0.675322  [   64/  146]
train() client id: f_00005-3-2 loss: 0.961290  [   96/  146]
train() client id: f_00005-3-3 loss: 0.717833  [  128/  146]
train() client id: f_00005-4-0 loss: 0.766795  [   32/  146]
train() client id: f_00005-4-1 loss: 0.671476  [   64/  146]
train() client id: f_00005-4-2 loss: 0.956403  [   96/  146]
train() client id: f_00005-4-3 loss: 0.577385  [  128/  146]
train() client id: f_00005-5-0 loss: 0.505068  [   32/  146]
train() client id: f_00005-5-1 loss: 0.876079  [   64/  146]
train() client id: f_00005-5-2 loss: 0.979694  [   96/  146]
train() client id: f_00005-5-3 loss: 0.711076  [  128/  146]
train() client id: f_00005-6-0 loss: 0.858569  [   32/  146]
train() client id: f_00005-6-1 loss: 0.705635  [   64/  146]
train() client id: f_00005-6-2 loss: 0.632702  [   96/  146]
train() client id: f_00005-6-3 loss: 0.915053  [  128/  146]
train() client id: f_00005-7-0 loss: 0.913675  [   32/  146]
train() client id: f_00005-7-1 loss: 0.644735  [   64/  146]
train() client id: f_00005-7-2 loss: 0.943023  [   96/  146]
train() client id: f_00005-7-3 loss: 0.580018  [  128/  146]
train() client id: f_00005-8-0 loss: 0.833026  [   32/  146]
train() client id: f_00005-8-1 loss: 0.772167  [   64/  146]
train() client id: f_00005-8-2 loss: 0.797087  [   96/  146]
train() client id: f_00005-8-3 loss: 0.700513  [  128/  146]
train() client id: f_00005-9-0 loss: 0.844711  [   32/  146]
train() client id: f_00005-9-1 loss: 0.741822  [   64/  146]
train() client id: f_00005-9-2 loss: 0.794914  [   96/  146]
train() client id: f_00005-9-3 loss: 0.633596  [  128/  146]
train() client id: f_00005-10-0 loss: 0.741537  [   32/  146]
train() client id: f_00005-10-1 loss: 0.659826  [   64/  146]
train() client id: f_00005-10-2 loss: 0.885506  [   96/  146]
train() client id: f_00005-10-3 loss: 0.827935  [  128/  146]
train() client id: f_00005-11-0 loss: 0.993543  [   32/  146]
train() client id: f_00005-11-1 loss: 0.804555  [   64/  146]
train() client id: f_00005-11-2 loss: 0.721687  [   96/  146]
train() client id: f_00005-11-3 loss: 0.750268  [  128/  146]
train() client id: f_00006-0-0 loss: 0.550825  [   32/   54]
train() client id: f_00006-1-0 loss: 0.649020  [   32/   54]
train() client id: f_00006-2-0 loss: 0.631183  [   32/   54]
train() client id: f_00006-3-0 loss: 0.540814  [   32/   54]
train() client id: f_00006-4-0 loss: 0.525920  [   32/   54]
train() client id: f_00006-5-0 loss: 0.538013  [   32/   54]
train() client id: f_00006-6-0 loss: 0.546238  [   32/   54]
train() client id: f_00006-7-0 loss: 0.646370  [   32/   54]
train() client id: f_00006-8-0 loss: 0.589264  [   32/   54]
train() client id: f_00006-9-0 loss: 0.644969  [   32/   54]
train() client id: f_00006-10-0 loss: 0.598701  [   32/   54]
train() client id: f_00006-11-0 loss: 0.641311  [   32/   54]
train() client id: f_00007-0-0 loss: 0.731067  [   32/  179]
train() client id: f_00007-0-1 loss: 0.606058  [   64/  179]
train() client id: f_00007-0-2 loss: 0.796272  [   96/  179]
train() client id: f_00007-0-3 loss: 0.719696  [  128/  179]
train() client id: f_00007-0-4 loss: 0.845467  [  160/  179]
train() client id: f_00007-1-0 loss: 0.684561  [   32/  179]
train() client id: f_00007-1-1 loss: 0.549695  [   64/  179]
train() client id: f_00007-1-2 loss: 0.603733  [   96/  179]
train() client id: f_00007-1-3 loss: 0.874385  [  128/  179]
train() client id: f_00007-1-4 loss: 0.780403  [  160/  179]
train() client id: f_00007-2-0 loss: 0.664857  [   32/  179]
train() client id: f_00007-2-1 loss: 0.813922  [   64/  179]
train() client id: f_00007-2-2 loss: 0.863936  [   96/  179]
train() client id: f_00007-2-3 loss: 0.562337  [  128/  179]
train() client id: f_00007-2-4 loss: 0.732018  [  160/  179]
train() client id: f_00007-3-0 loss: 0.621895  [   32/  179]
train() client id: f_00007-3-1 loss: 0.742219  [   64/  179]
train() client id: f_00007-3-2 loss: 0.568094  [   96/  179]
train() client id: f_00007-3-3 loss: 0.660450  [  128/  179]
train() client id: f_00007-3-4 loss: 0.759221  [  160/  179]
train() client id: f_00007-4-0 loss: 0.650331  [   32/  179]
train() client id: f_00007-4-1 loss: 0.672192  [   64/  179]
train() client id: f_00007-4-2 loss: 0.692865  [   96/  179]
train() client id: f_00007-4-3 loss: 0.625367  [  128/  179]
train() client id: f_00007-4-4 loss: 0.939288  [  160/  179]
train() client id: f_00007-5-0 loss: 0.646076  [   32/  179]
train() client id: f_00007-5-1 loss: 0.838449  [   64/  179]
train() client id: f_00007-5-2 loss: 0.652124  [   96/  179]
train() client id: f_00007-5-3 loss: 0.558743  [  128/  179]
train() client id: f_00007-5-4 loss: 0.703565  [  160/  179]
train() client id: f_00007-6-0 loss: 0.810721  [   32/  179]
train() client id: f_00007-6-1 loss: 0.832260  [   64/  179]
train() client id: f_00007-6-2 loss: 0.549450  [   96/  179]
train() client id: f_00007-6-3 loss: 0.684521  [  128/  179]
train() client id: f_00007-6-4 loss: 0.588911  [  160/  179]
train() client id: f_00007-7-0 loss: 0.610042  [   32/  179]
train() client id: f_00007-7-1 loss: 0.588374  [   64/  179]
train() client id: f_00007-7-2 loss: 0.541482  [   96/  179]
train() client id: f_00007-7-3 loss: 0.760998  [  128/  179]
train() client id: f_00007-7-4 loss: 0.828634  [  160/  179]
train() client id: f_00007-8-0 loss: 0.746601  [   32/  179]
train() client id: f_00007-8-1 loss: 0.699543  [   64/  179]
train() client id: f_00007-8-2 loss: 0.720479  [   96/  179]
train() client id: f_00007-8-3 loss: 0.676006  [  128/  179]
train() client id: f_00007-8-4 loss: 0.705531  [  160/  179]
train() client id: f_00007-9-0 loss: 0.718079  [   32/  179]
train() client id: f_00007-9-1 loss: 0.720232  [   64/  179]
train() client id: f_00007-9-2 loss: 0.561076  [   96/  179]
train() client id: f_00007-9-3 loss: 0.651127  [  128/  179]
train() client id: f_00007-9-4 loss: 0.718923  [  160/  179]
train() client id: f_00007-10-0 loss: 0.726646  [   32/  179]
train() client id: f_00007-10-1 loss: 0.674640  [   64/  179]
train() client id: f_00007-10-2 loss: 0.554438  [   96/  179]
train() client id: f_00007-10-3 loss: 0.637837  [  128/  179]
train() client id: f_00007-10-4 loss: 0.873537  [  160/  179]
train() client id: f_00007-11-0 loss: 0.766393  [   32/  179]
train() client id: f_00007-11-1 loss: 0.616132  [   64/  179]
train() client id: f_00007-11-2 loss: 0.738571  [   96/  179]
train() client id: f_00007-11-3 loss: 0.634765  [  128/  179]
train() client id: f_00007-11-4 loss: 0.775769  [  160/  179]
train() client id: f_00008-0-0 loss: 0.664522  [   32/  130]
train() client id: f_00008-0-1 loss: 0.657195  [   64/  130]
train() client id: f_00008-0-2 loss: 0.719473  [   96/  130]
train() client id: f_00008-0-3 loss: 0.653316  [  128/  130]
train() client id: f_00008-1-0 loss: 0.685182  [   32/  130]
train() client id: f_00008-1-1 loss: 0.634827  [   64/  130]
train() client id: f_00008-1-2 loss: 0.668284  [   96/  130]
train() client id: f_00008-1-3 loss: 0.706440  [  128/  130]
train() client id: f_00008-2-0 loss: 0.587753  [   32/  130]
train() client id: f_00008-2-1 loss: 0.728815  [   64/  130]
train() client id: f_00008-2-2 loss: 0.734095  [   96/  130]
train() client id: f_00008-2-3 loss: 0.599855  [  128/  130]
train() client id: f_00008-3-0 loss: 0.675418  [   32/  130]
train() client id: f_00008-3-1 loss: 0.645160  [   64/  130]
train() client id: f_00008-3-2 loss: 0.628367  [   96/  130]
train() client id: f_00008-3-3 loss: 0.739372  [  128/  130]
train() client id: f_00008-4-0 loss: 0.723312  [   32/  130]
train() client id: f_00008-4-1 loss: 0.606080  [   64/  130]
train() client id: f_00008-4-2 loss: 0.793061  [   96/  130]
train() client id: f_00008-4-3 loss: 0.527052  [  128/  130]
train() client id: f_00008-5-0 loss: 0.681086  [   32/  130]
train() client id: f_00008-5-1 loss: 0.709395  [   64/  130]
train() client id: f_00008-5-2 loss: 0.638536  [   96/  130]
train() client id: f_00008-5-3 loss: 0.661068  [  128/  130]
train() client id: f_00008-6-0 loss: 0.630668  [   32/  130]
train() client id: f_00008-6-1 loss: 0.712447  [   64/  130]
train() client id: f_00008-6-2 loss: 0.670331  [   96/  130]
train() client id: f_00008-6-3 loss: 0.671650  [  128/  130]
train() client id: f_00008-7-0 loss: 0.729835  [   32/  130]
train() client id: f_00008-7-1 loss: 0.518693  [   64/  130]
train() client id: f_00008-7-2 loss: 0.754279  [   96/  130]
train() client id: f_00008-7-3 loss: 0.690870  [  128/  130]
train() client id: f_00008-8-0 loss: 0.634480  [   32/  130]
train() client id: f_00008-8-1 loss: 0.673604  [   64/  130]
train() client id: f_00008-8-2 loss: 0.657607  [   96/  130]
train() client id: f_00008-8-3 loss: 0.720607  [  128/  130]
train() client id: f_00008-9-0 loss: 0.623109  [   32/  130]
train() client id: f_00008-9-1 loss: 0.661022  [   64/  130]
train() client id: f_00008-9-2 loss: 0.655175  [   96/  130]
train() client id: f_00008-9-3 loss: 0.757701  [  128/  130]
train() client id: f_00008-10-0 loss: 0.630904  [   32/  130]
train() client id: f_00008-10-1 loss: 0.701037  [   64/  130]
train() client id: f_00008-10-2 loss: 0.692617  [   96/  130]
train() client id: f_00008-10-3 loss: 0.673419  [  128/  130]
train() client id: f_00008-11-0 loss: 0.632957  [   32/  130]
train() client id: f_00008-11-1 loss: 0.642276  [   64/  130]
train() client id: f_00008-11-2 loss: 0.774338  [   96/  130]
train() client id: f_00008-11-3 loss: 0.618684  [  128/  130]
train() client id: f_00009-0-0 loss: 1.224796  [   32/  118]
train() client id: f_00009-0-1 loss: 1.252004  [   64/  118]
train() client id: f_00009-0-2 loss: 1.120569  [   96/  118]
train() client id: f_00009-1-0 loss: 1.326943  [   32/  118]
train() client id: f_00009-1-1 loss: 1.206694  [   64/  118]
train() client id: f_00009-1-2 loss: 1.052827  [   96/  118]
train() client id: f_00009-2-0 loss: 1.213065  [   32/  118]
train() client id: f_00009-2-1 loss: 1.126215  [   64/  118]
train() client id: f_00009-2-2 loss: 1.001436  [   96/  118]
train() client id: f_00009-3-0 loss: 1.059843  [   32/  118]
train() client id: f_00009-3-1 loss: 1.036510  [   64/  118]
train() client id: f_00009-3-2 loss: 1.064526  [   96/  118]
train() client id: f_00009-4-0 loss: 1.119962  [   32/  118]
train() client id: f_00009-4-1 loss: 1.063174  [   64/  118]
train() client id: f_00009-4-2 loss: 0.892696  [   96/  118]
train() client id: f_00009-5-0 loss: 0.885430  [   32/  118]
train() client id: f_00009-5-1 loss: 1.058523  [   64/  118]
train() client id: f_00009-5-2 loss: 1.058451  [   96/  118]
train() client id: f_00009-6-0 loss: 1.026803  [   32/  118]
train() client id: f_00009-6-1 loss: 0.938165  [   64/  118]
train() client id: f_00009-6-2 loss: 0.883865  [   96/  118]
train() client id: f_00009-7-0 loss: 0.899203  [   32/  118]
train() client id: f_00009-7-1 loss: 0.980076  [   64/  118]
train() client id: f_00009-7-2 loss: 0.966205  [   96/  118]
train() client id: f_00009-8-0 loss: 0.911621  [   32/  118]
train() client id: f_00009-8-1 loss: 1.037612  [   64/  118]
train() client id: f_00009-8-2 loss: 0.864187  [   96/  118]
train() client id: f_00009-9-0 loss: 0.943835  [   32/  118]
train() client id: f_00009-9-1 loss: 0.969659  [   64/  118]
train() client id: f_00009-9-2 loss: 0.994083  [   96/  118]
train() client id: f_00009-10-0 loss: 0.953103  [   32/  118]
train() client id: f_00009-10-1 loss: 0.969688  [   64/  118]
train() client id: f_00009-10-2 loss: 0.917746  [   96/  118]
train() client id: f_00009-11-0 loss: 0.974194  [   32/  118]
train() client id: f_00009-11-1 loss: 0.995359  [   64/  118]
train() client id: f_00009-11-2 loss: 0.769098  [   96/  118]
At round 23 accuracy: 0.6419098143236074
At round 23 training accuracy: 0.5828303152246814
At round 23 training loss: 0.8325902224524813
update_location
xs = -4.528292 16.001589 25.045120 -25.943528 -75.103519 -10.217951 -67.215960 88.375741 -1.680116 9.695607 
ys = 102.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 -3.124922 -19.177652 -32.154970 4.001482 
xs mean: -4.557130840587133
ys mean: 8.371751218646876
dists_uav = 143.334556 102.459919 103.097052 105.741835 125.411182 101.979213 120.531118 134.826013 105.056008 100.548579 
uav_gains = -103.911256 -100.263873 -100.331181 -100.606207 -102.458804 -100.212813 -102.027707 -103.245424 -100.535555 -100.059416 
uav_gains_db_mean: -101.3652234796063
dists_bs = 185.213679 248.805241 264.919948 213.214475 193.440346 253.157114 208.196751 327.218186 270.098063 251.700140 
bs_gains = -103.061905 -106.651126 -107.414270 -104.773926 -103.590377 -106.861983 -104.484330 -109.982507 -107.649660 -106.791796 
bs_gains_db_mean: -106.12618796190603
Round 24
-------------------------------
ene_coms = [0.00752025 0.00868787 0.00639025 0.00646504 0.00740324 0.00879239
 0.0068809  0.00728136 0.00920513 0.00875733]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.8137761  16.29000186  7.66937026  2.74521421 18.75733707  9.05563974
  3.42251662 11.03192469  8.09099116  7.35223414]
obj_prev = 92.22900585217832
eta_min = 1.3645879815164432e-12	eta_max = 0.9352101380760387
af = 19.491574900134562	bf = 1.5446961856030461	zeta = 21.44073239014802	eta = 0.909090909090909
af = 19.491574900134562	bf = 1.5446961856030461	zeta = 37.42685863404766	eta = 0.5207911006028928
af = 19.491574900134562	bf = 1.5446961856030461	zeta = 29.75971857122751	eta = 0.6549650277600252
af = 19.491574900134562	bf = 1.5446961856030461	zeta = 28.38266763193575	eta = 0.6867421749392907
af = 19.491574900134562	bf = 1.5446961856030461	zeta = 28.313925936510543	eta = 0.6884094753882349
af = 19.491574900134562	bf = 1.5446961856030461	zeta = 28.313742169019303	eta = 0.688413943440585
eta = 0.688413943440585
ene_coms = [0.00752025 0.00868787 0.00639025 0.00646504 0.00740324 0.00879239
 0.0068809  0.00728136 0.00920513 0.00875733]
ene_comp = [0.03080918 0.06479709 0.03032015 0.01051424 0.0748223  0.03569953
 0.01320393 0.0437686  0.03178725 0.02885304]
ene_total = [2.45554349 4.70775307 2.35182086 1.0877638  5.26771098 2.85033787
 1.28671835 3.27047333 2.6261429  2.40947753]
ti_comp = [0.37434514 0.36266896 0.38564522 0.38489727 0.37551527 0.36162377
 0.38073867 0.37673408 0.35749638 0.36197435]
ti_coms = [0.07520253 0.08687872 0.06390245 0.0646504  0.0740324  0.0879239
 0.06880901 0.0728136  0.0920513  0.08757332]
t_total = [28.79989929 28.79989929 28.79989929 28.79989929 28.79989929 28.79989929
 28.79989929 28.79989929 28.79989929 28.79989929]
ene_coms = [0.00752025 0.00868787 0.00639025 0.00646504 0.00740324 0.00879239
 0.0068809  0.00728136 0.00920513 0.00875733]
ene_comp = [1.30429581e-05 1.29278326e-04 1.17138056e-05 4.90370758e-07
 1.85659824e-04 2.17447036e-05 9.92512928e-07 3.69230995e-05
 1.57070865e-05 1.14577486e-05]
ene_total = [0.48261438 0.56486341 0.4101362  0.41420885 0.48617659 0.56467026
 0.44088278 0.46883973 0.59072527 0.56176526]
optimize_network iter = 0 obj = 4.984882740461089
eta = 0.688413943440585
freqs = [41150769.8961035  89333656.63056733 39310932.97022042 13658507.37645978
 99626171.57593328 49360040.32196761 17339892.21612528 58089514.5326417
 44458144.06244946 39855093.08493502]
eta_min = 0.6884139434406015	eta_max = 0.688413943440584
af = 0.022829592547671928	bf = 1.5446961856030461	zeta = 0.025112551802439122	eta = 0.9090909090909091
af = 0.022829592547671928	bf = 1.5446961856030461	zeta = 17.015592859909862	eta = 0.0013416865774603908
af = 0.022829592547671928	bf = 1.5446961856030461	zeta = 1.6979653171102258	eta = 0.013445264351173972
af = 0.022829592547671928	bf = 1.5446961856030461	zeta = 1.6654644392146851	eta = 0.013707643351686779
af = 0.022829592547671928	bf = 1.5446961856030461	zeta = 1.6654601440317118	eta = 0.013707678703380147
eta = 0.013707678703380147
ene_coms = [0.00752025 0.00868787 0.00639025 0.00646504 0.00740324 0.00879239
 0.0068809  0.00728136 0.00920513 0.00875733]
ene_comp = [1.49857850e-04 1.48535109e-03 1.34586473e-04 5.63414425e-06
 2.13314969e-03 2.49837077e-04 1.14035369e-05 4.24230167e-04
 1.80467513e-04 1.31644490e-04]
ene_total = [0.15523485 0.20589516 0.13205562 0.13095953 0.19300634 0.18300502
 0.13949287 0.15595291 0.18995446 0.17990339]
ti_comp = [0.37434514 0.36266896 0.38564522 0.38489727 0.37551527 0.36162377
 0.38073867 0.37673408 0.35749638 0.36197435]
ti_coms = [0.07520253 0.08687872 0.06390245 0.0646504  0.0740324  0.0879239
 0.06880901 0.0728136  0.0920513  0.08757332]
t_total = [28.79989929 28.79989929 28.79989929 28.79989929 28.79989929 28.79989929
 28.79989929 28.79989929 28.79989929 28.79989929]
ene_coms = [0.00752025 0.00868787 0.00639025 0.00646504 0.00740324 0.00879239
 0.0068809  0.00728136 0.00920513 0.00875733]
ene_comp = [1.30429581e-05 1.29278326e-04 1.17138056e-05 4.90370758e-07
 1.85659824e-04 2.17447036e-05 9.92512928e-07 3.69230995e-05
 1.57070865e-05 1.14577486e-05]
ene_total = [0.48261438 0.56486341 0.4101362  0.41420885 0.48617659 0.56467026
 0.44088278 0.46883973 0.59072527 0.56176526]
optimize_network iter = 1 obj = 4.984882740461349
eta = 0.6884139434406015
freqs = [41150769.89610345 89333656.6305674  39310932.9702203  13658507.37645974
 99626171.57593314 49360040.32196766 17339892.21612523 58089514.5326416
 44458144.06244954 39855093.08493506]
Done!
ene_coms = [0.00752025 0.00868787 0.00639025 0.00646504 0.00740324 0.00879239
 0.0068809  0.00728136 0.00920513 0.00875733]
ene_comp = [1.28019971e-05 1.26889985e-04 1.14973999e-05 4.81311445e-07
 1.82229868e-04 2.13429830e-05 9.74176833e-07 3.62409668e-05
 1.54169073e-05 1.12460734e-05]
ene_total = [0.00753306 0.00881476 0.00640174 0.00646552 0.00758547 0.00881373
 0.00688187 0.0073176  0.00922055 0.00876858]
At round 24 energy consumption: 0.0778028849079223
At round 24 eta: 0.6884139434406015
At round 24 a_n: 19.96150252899076
At round 24 local rounds: 12.225865711788757
At round 24 global rounds: 64.06417138626176
gradient difference: 0.48033031821250916
train() client id: f_00000-0-0 loss: 1.329439  [   32/  126]
train() client id: f_00000-0-1 loss: 1.255348  [   64/  126]
train() client id: f_00000-0-2 loss: 1.328758  [   96/  126]
train() client id: f_00000-1-0 loss: 1.482336  [   32/  126]
train() client id: f_00000-1-1 loss: 0.987914  [   64/  126]
train() client id: f_00000-1-2 loss: 1.131310  [   96/  126]
train() client id: f_00000-2-0 loss: 1.220768  [   32/  126]
train() client id: f_00000-2-1 loss: 1.029330  [   64/  126]
train() client id: f_00000-2-2 loss: 0.975698  [   96/  126]
train() client id: f_00000-3-0 loss: 0.948673  [   32/  126]
train() client id: f_00000-3-1 loss: 1.077180  [   64/  126]
train() client id: f_00000-3-2 loss: 1.061602  [   96/  126]
train() client id: f_00000-4-0 loss: 0.975327  [   32/  126]
train() client id: f_00000-4-1 loss: 0.871512  [   64/  126]
train() client id: f_00000-4-2 loss: 0.874179  [   96/  126]
train() client id: f_00000-5-0 loss: 0.933145  [   32/  126]
train() client id: f_00000-5-1 loss: 0.900128  [   64/  126]
train() client id: f_00000-5-2 loss: 0.901706  [   96/  126]
train() client id: f_00000-6-0 loss: 0.833349  [   32/  126]
train() client id: f_00000-6-1 loss: 0.916463  [   64/  126]
train() client id: f_00000-6-2 loss: 0.693761  [   96/  126]
train() client id: f_00000-7-0 loss: 0.874856  [   32/  126]
train() client id: f_00000-7-1 loss: 0.836575  [   64/  126]
train() client id: f_00000-7-2 loss: 0.777054  [   96/  126]
train() client id: f_00000-8-0 loss: 0.764977  [   32/  126]
train() client id: f_00000-8-1 loss: 0.876370  [   64/  126]
train() client id: f_00000-8-2 loss: 0.687324  [   96/  126]
train() client id: f_00000-9-0 loss: 0.731698  [   32/  126]
train() client id: f_00000-9-1 loss: 0.867358  [   64/  126]
train() client id: f_00000-9-2 loss: 0.657065  [   96/  126]
train() client id: f_00000-10-0 loss: 0.759120  [   32/  126]
train() client id: f_00000-10-1 loss: 0.753583  [   64/  126]
train() client id: f_00000-10-2 loss: 0.735822  [   96/  126]
train() client id: f_00000-11-0 loss: 0.770716  [   32/  126]
train() client id: f_00000-11-1 loss: 0.682695  [   64/  126]
train() client id: f_00000-11-2 loss: 0.699227  [   96/  126]
train() client id: f_00001-0-0 loss: 0.477828  [   32/  265]
train() client id: f_00001-0-1 loss: 0.372962  [   64/  265]
train() client id: f_00001-0-2 loss: 0.412527  [   96/  265]
train() client id: f_00001-0-3 loss: 0.484039  [  128/  265]
train() client id: f_00001-0-4 loss: 0.536462  [  160/  265]
train() client id: f_00001-0-5 loss: 0.391365  [  192/  265]
train() client id: f_00001-0-6 loss: 0.383623  [  224/  265]
train() client id: f_00001-0-7 loss: 0.464765  [  256/  265]
train() client id: f_00001-1-0 loss: 0.452939  [   32/  265]
train() client id: f_00001-1-1 loss: 0.447154  [   64/  265]
train() client id: f_00001-1-2 loss: 0.359264  [   96/  265]
train() client id: f_00001-1-3 loss: 0.556120  [  128/  265]
train() client id: f_00001-1-4 loss: 0.417415  [  160/  265]
train() client id: f_00001-1-5 loss: 0.354973  [  192/  265]
train() client id: f_00001-1-6 loss: 0.425948  [  224/  265]
train() client id: f_00001-1-7 loss: 0.362893  [  256/  265]
train() client id: f_00001-2-0 loss: 0.493530  [   32/  265]
train() client id: f_00001-2-1 loss: 0.350291  [   64/  265]
train() client id: f_00001-2-2 loss: 0.421892  [   96/  265]
train() client id: f_00001-2-3 loss: 0.411031  [  128/  265]
train() client id: f_00001-2-4 loss: 0.496425  [  160/  265]
train() client id: f_00001-2-5 loss: 0.390721  [  192/  265]
train() client id: f_00001-2-6 loss: 0.398595  [  224/  265]
train() client id: f_00001-2-7 loss: 0.425984  [  256/  265]
train() client id: f_00001-3-0 loss: 0.409949  [   32/  265]
train() client id: f_00001-3-1 loss: 0.485104  [   64/  265]
train() client id: f_00001-3-2 loss: 0.419105  [   96/  265]
train() client id: f_00001-3-3 loss: 0.366375  [  128/  265]
train() client id: f_00001-3-4 loss: 0.380656  [  160/  265]
train() client id: f_00001-3-5 loss: 0.408765  [  192/  265]
train() client id: f_00001-3-6 loss: 0.470555  [  224/  265]
train() client id: f_00001-3-7 loss: 0.399085  [  256/  265]
train() client id: f_00001-4-0 loss: 0.551627  [   32/  265]
train() client id: f_00001-4-1 loss: 0.354573  [   64/  265]
train() client id: f_00001-4-2 loss: 0.426854  [   96/  265]
train() client id: f_00001-4-3 loss: 0.459811  [  128/  265]
train() client id: f_00001-4-4 loss: 0.406687  [  160/  265]
train() client id: f_00001-4-5 loss: 0.380830  [  192/  265]
train() client id: f_00001-4-6 loss: 0.366865  [  224/  265]
train() client id: f_00001-4-7 loss: 0.320014  [  256/  265]
train() client id: f_00001-5-0 loss: 0.358744  [   32/  265]
train() client id: f_00001-5-1 loss: 0.323797  [   64/  265]
train() client id: f_00001-5-2 loss: 0.683428  [   96/  265]
train() client id: f_00001-5-3 loss: 0.404902  [  128/  265]
train() client id: f_00001-5-4 loss: 0.323552  [  160/  265]
train() client id: f_00001-5-5 loss: 0.385398  [  192/  265]
train() client id: f_00001-5-6 loss: 0.412851  [  224/  265]
train() client id: f_00001-5-7 loss: 0.381684  [  256/  265]
train() client id: f_00001-6-0 loss: 0.338842  [   32/  265]
train() client id: f_00001-6-1 loss: 0.541484  [   64/  265]
train() client id: f_00001-6-2 loss: 0.460591  [   96/  265]
train() client id: f_00001-6-3 loss: 0.492349  [  128/  265]
train() client id: f_00001-6-4 loss: 0.357577  [  160/  265]
train() client id: f_00001-6-5 loss: 0.363855  [  192/  265]
train() client id: f_00001-6-6 loss: 0.344263  [  224/  265]
train() client id: f_00001-6-7 loss: 0.356335  [  256/  265]
train() client id: f_00001-7-0 loss: 0.354933  [   32/  265]
train() client id: f_00001-7-1 loss: 0.393915  [   64/  265]
train() client id: f_00001-7-2 loss: 0.299062  [   96/  265]
train() client id: f_00001-7-3 loss: 0.434842  [  128/  265]
train() client id: f_00001-7-4 loss: 0.446028  [  160/  265]
train() client id: f_00001-7-5 loss: 0.384346  [  192/  265]
train() client id: f_00001-7-6 loss: 0.480313  [  224/  265]
train() client id: f_00001-7-7 loss: 0.437674  [  256/  265]
train() client id: f_00001-8-0 loss: 0.307630  [   32/  265]
train() client id: f_00001-8-1 loss: 0.310944  [   64/  265]
train() client id: f_00001-8-2 loss: 0.461938  [   96/  265]
train() client id: f_00001-8-3 loss: 0.455674  [  128/  265]
train() client id: f_00001-8-4 loss: 0.420135  [  160/  265]
train() client id: f_00001-8-5 loss: 0.390201  [  192/  265]
train() client id: f_00001-8-6 loss: 0.528326  [  224/  265]
train() client id: f_00001-8-7 loss: 0.358814  [  256/  265]
train() client id: f_00001-9-0 loss: 0.410291  [   32/  265]
train() client id: f_00001-9-1 loss: 0.421984  [   64/  265]
train() client id: f_00001-9-2 loss: 0.380522  [   96/  265]
train() client id: f_00001-9-3 loss: 0.380091  [  128/  265]
train() client id: f_00001-9-4 loss: 0.497272  [  160/  265]
train() client id: f_00001-9-5 loss: 0.379852  [  192/  265]
train() client id: f_00001-9-6 loss: 0.394775  [  224/  265]
train() client id: f_00001-9-7 loss: 0.357555  [  256/  265]
train() client id: f_00001-10-0 loss: 0.360150  [   32/  265]
train() client id: f_00001-10-1 loss: 0.308997  [   64/  265]
train() client id: f_00001-10-2 loss: 0.400541  [   96/  265]
train() client id: f_00001-10-3 loss: 0.584577  [  128/  265]
train() client id: f_00001-10-4 loss: 0.389730  [  160/  265]
train() client id: f_00001-10-5 loss: 0.356107  [  192/  265]
train() client id: f_00001-10-6 loss: 0.362944  [  224/  265]
train() client id: f_00001-10-7 loss: 0.441950  [  256/  265]
train() client id: f_00001-11-0 loss: 0.317699  [   32/  265]
train() client id: f_00001-11-1 loss: 0.327152  [   64/  265]
train() client id: f_00001-11-2 loss: 0.370493  [   96/  265]
train() client id: f_00001-11-3 loss: 0.419827  [  128/  265]
train() client id: f_00001-11-4 loss: 0.513648  [  160/  265]
train() client id: f_00001-11-5 loss: 0.303135  [  192/  265]
train() client id: f_00001-11-6 loss: 0.506334  [  224/  265]
train() client id: f_00001-11-7 loss: 0.398502  [  256/  265]
train() client id: f_00002-0-0 loss: 1.171206  [   32/  124]
train() client id: f_00002-0-1 loss: 0.981255  [   64/  124]
train() client id: f_00002-0-2 loss: 0.982774  [   96/  124]
train() client id: f_00002-1-0 loss: 1.016619  [   32/  124]
train() client id: f_00002-1-1 loss: 1.041457  [   64/  124]
train() client id: f_00002-1-2 loss: 1.028373  [   96/  124]
train() client id: f_00002-2-0 loss: 1.225356  [   32/  124]
train() client id: f_00002-2-1 loss: 0.822514  [   64/  124]
train() client id: f_00002-2-2 loss: 1.026942  [   96/  124]
train() client id: f_00002-3-0 loss: 1.043925  [   32/  124]
train() client id: f_00002-3-1 loss: 1.108444  [   64/  124]
train() client id: f_00002-3-2 loss: 0.873574  [   96/  124]
train() client id: f_00002-4-0 loss: 1.022775  [   32/  124]
train() client id: f_00002-4-1 loss: 1.030134  [   64/  124]
train() client id: f_00002-4-2 loss: 0.920649  [   96/  124]
train() client id: f_00002-5-0 loss: 0.961308  [   32/  124]
train() client id: f_00002-5-1 loss: 0.915087  [   64/  124]
train() client id: f_00002-5-2 loss: 1.197997  [   96/  124]
train() client id: f_00002-6-0 loss: 0.929561  [   32/  124]
train() client id: f_00002-6-1 loss: 0.876172  [   64/  124]
train() client id: f_00002-6-2 loss: 1.106841  [   96/  124]
train() client id: f_00002-7-0 loss: 1.132906  [   32/  124]
train() client id: f_00002-7-1 loss: 0.893517  [   64/  124]
train() client id: f_00002-7-2 loss: 1.024405  [   96/  124]
train() client id: f_00002-8-0 loss: 0.759011  [   32/  124]
train() client id: f_00002-8-1 loss: 1.030853  [   64/  124]
train() client id: f_00002-8-2 loss: 1.013902  [   96/  124]
train() client id: f_00002-9-0 loss: 0.946825  [   32/  124]
train() client id: f_00002-9-1 loss: 1.068542  [   64/  124]
train() client id: f_00002-9-2 loss: 0.843044  [   96/  124]
train() client id: f_00002-10-0 loss: 1.042828  [   32/  124]
train() client id: f_00002-10-1 loss: 0.994965  [   64/  124]
train() client id: f_00002-10-2 loss: 0.896820  [   96/  124]
train() client id: f_00002-11-0 loss: 0.969920  [   32/  124]
train() client id: f_00002-11-1 loss: 0.992328  [   64/  124]
train() client id: f_00002-11-2 loss: 0.848695  [   96/  124]
train() client id: f_00003-0-0 loss: 0.619613  [   32/   43]
train() client id: f_00003-1-0 loss: 0.788640  [   32/   43]
train() client id: f_00003-2-0 loss: 0.883747  [   32/   43]
train() client id: f_00003-3-0 loss: 0.685103  [   32/   43]
train() client id: f_00003-4-0 loss: 0.692532  [   32/   43]
train() client id: f_00003-5-0 loss: 0.855138  [   32/   43]
train() client id: f_00003-6-0 loss: 0.716348  [   32/   43]
train() client id: f_00003-7-0 loss: 0.679072  [   32/   43]
train() client id: f_00003-8-0 loss: 0.722722  [   32/   43]
train() client id: f_00003-9-0 loss: 0.721515  [   32/   43]
train() client id: f_00003-10-0 loss: 0.782707  [   32/   43]
train() client id: f_00003-11-0 loss: 0.738157  [   32/   43]
train() client id: f_00004-0-0 loss: 0.832135  [   32/  306]
train() client id: f_00004-0-1 loss: 0.810105  [   64/  306]
train() client id: f_00004-0-2 loss: 0.965294  [   96/  306]
train() client id: f_00004-0-3 loss: 0.967949  [  128/  306]
train() client id: f_00004-0-4 loss: 0.865067  [  160/  306]
train() client id: f_00004-0-5 loss: 0.984380  [  192/  306]
train() client id: f_00004-0-6 loss: 0.839456  [  224/  306]
train() client id: f_00004-0-7 loss: 0.778067  [  256/  306]
train() client id: f_00004-0-8 loss: 0.961001  [  288/  306]
train() client id: f_00004-1-0 loss: 0.890857  [   32/  306]
train() client id: f_00004-1-1 loss: 0.843811  [   64/  306]
train() client id: f_00004-1-2 loss: 0.889629  [   96/  306]
train() client id: f_00004-1-3 loss: 0.822648  [  128/  306]
train() client id: f_00004-1-4 loss: 0.854454  [  160/  306]
train() client id: f_00004-1-5 loss: 0.947253  [  192/  306]
train() client id: f_00004-1-6 loss: 1.027727  [  224/  306]
train() client id: f_00004-1-7 loss: 0.832056  [  256/  306]
train() client id: f_00004-1-8 loss: 0.894183  [  288/  306]
train() client id: f_00004-2-0 loss: 0.875311  [   32/  306]
train() client id: f_00004-2-1 loss: 0.818522  [   64/  306]
train() client id: f_00004-2-2 loss: 0.915007  [   96/  306]
train() client id: f_00004-2-3 loss: 0.777240  [  128/  306]
train() client id: f_00004-2-4 loss: 0.911602  [  160/  306]
train() client id: f_00004-2-5 loss: 0.955253  [  192/  306]
train() client id: f_00004-2-6 loss: 1.027277  [  224/  306]
train() client id: f_00004-2-7 loss: 0.925793  [  256/  306]
train() client id: f_00004-2-8 loss: 0.805696  [  288/  306]
train() client id: f_00004-3-0 loss: 0.886081  [   32/  306]
train() client id: f_00004-3-1 loss: 0.903308  [   64/  306]
train() client id: f_00004-3-2 loss: 0.864304  [   96/  306]
train() client id: f_00004-3-3 loss: 0.886276  [  128/  306]
train() client id: f_00004-3-4 loss: 0.886117  [  160/  306]
train() client id: f_00004-3-5 loss: 1.074154  [  192/  306]
train() client id: f_00004-3-6 loss: 0.994460  [  224/  306]
train() client id: f_00004-3-7 loss: 0.804054  [  256/  306]
train() client id: f_00004-3-8 loss: 0.724881  [  288/  306]
train() client id: f_00004-4-0 loss: 0.784529  [   32/  306]
train() client id: f_00004-4-1 loss: 0.905734  [   64/  306]
train() client id: f_00004-4-2 loss: 0.950921  [   96/  306]
train() client id: f_00004-4-3 loss: 0.997031  [  128/  306]
train() client id: f_00004-4-4 loss: 0.858139  [  160/  306]
train() client id: f_00004-4-5 loss: 0.788429  [  192/  306]
train() client id: f_00004-4-6 loss: 0.860605  [  224/  306]
train() client id: f_00004-4-7 loss: 0.778558  [  256/  306]
train() client id: f_00004-4-8 loss: 0.921198  [  288/  306]
train() client id: f_00004-5-0 loss: 0.861943  [   32/  306]
train() client id: f_00004-5-1 loss: 0.772046  [   64/  306]
train() client id: f_00004-5-2 loss: 0.893358  [   96/  306]
train() client id: f_00004-5-3 loss: 0.862474  [  128/  306]
train() client id: f_00004-5-4 loss: 0.859447  [  160/  306]
train() client id: f_00004-5-5 loss: 0.848854  [  192/  306]
train() client id: f_00004-5-6 loss: 1.052244  [  224/  306]
train() client id: f_00004-5-7 loss: 0.905456  [  256/  306]
train() client id: f_00004-5-8 loss: 0.933306  [  288/  306]
train() client id: f_00004-6-0 loss: 0.850550  [   32/  306]
train() client id: f_00004-6-1 loss: 0.791621  [   64/  306]
train() client id: f_00004-6-2 loss: 0.908961  [   96/  306]
train() client id: f_00004-6-3 loss: 0.925259  [  128/  306]
train() client id: f_00004-6-4 loss: 0.860330  [  160/  306]
train() client id: f_00004-6-5 loss: 0.970303  [  192/  306]
train() client id: f_00004-6-6 loss: 0.856334  [  224/  306]
train() client id: f_00004-6-7 loss: 0.862201  [  256/  306]
train() client id: f_00004-6-8 loss: 0.930980  [  288/  306]
train() client id: f_00004-7-0 loss: 0.859488  [   32/  306]
train() client id: f_00004-7-1 loss: 0.904620  [   64/  306]
train() client id: f_00004-7-2 loss: 0.860201  [   96/  306]
train() client id: f_00004-7-3 loss: 0.914271  [  128/  306]
train() client id: f_00004-7-4 loss: 0.830426  [  160/  306]
train() client id: f_00004-7-5 loss: 0.903431  [  192/  306]
train() client id: f_00004-7-6 loss: 0.824091  [  224/  306]
train() client id: f_00004-7-7 loss: 1.042938  [  256/  306]
train() client id: f_00004-7-8 loss: 0.802111  [  288/  306]
train() client id: f_00004-8-0 loss: 0.836086  [   32/  306]
train() client id: f_00004-8-1 loss: 0.829659  [   64/  306]
train() client id: f_00004-8-2 loss: 0.852772  [   96/  306]
train() client id: f_00004-8-3 loss: 0.960431  [  128/  306]
train() client id: f_00004-8-4 loss: 0.753326  [  160/  306]
train() client id: f_00004-8-5 loss: 0.783647  [  192/  306]
train() client id: f_00004-8-6 loss: 1.116341  [  224/  306]
train() client id: f_00004-8-7 loss: 0.870713  [  256/  306]
train() client id: f_00004-8-8 loss: 0.891743  [  288/  306]
train() client id: f_00004-9-0 loss: 0.921945  [   32/  306]
train() client id: f_00004-9-1 loss: 0.769365  [   64/  306]
train() client id: f_00004-9-2 loss: 0.870638  [   96/  306]
train() client id: f_00004-9-3 loss: 0.874254  [  128/  306]
train() client id: f_00004-9-4 loss: 0.804606  [  160/  306]
train() client id: f_00004-9-5 loss: 0.816323  [  192/  306]
train() client id: f_00004-9-6 loss: 0.949905  [  224/  306]
train() client id: f_00004-9-7 loss: 0.913996  [  256/  306]
train() client id: f_00004-9-8 loss: 0.907386  [  288/  306]
train() client id: f_00004-10-0 loss: 0.822667  [   32/  306]
train() client id: f_00004-10-1 loss: 0.821597  [   64/  306]
train() client id: f_00004-10-2 loss: 0.991345  [   96/  306]
train() client id: f_00004-10-3 loss: 0.811316  [  128/  306]
train() client id: f_00004-10-4 loss: 0.805102  [  160/  306]
train() client id: f_00004-10-5 loss: 0.928040  [  192/  306]
train() client id: f_00004-10-6 loss: 0.879247  [  224/  306]
train() client id: f_00004-10-7 loss: 0.879975  [  256/  306]
train() client id: f_00004-10-8 loss: 0.919932  [  288/  306]
train() client id: f_00004-11-0 loss: 0.931986  [   32/  306]
train() client id: f_00004-11-1 loss: 0.923617  [   64/  306]
train() client id: f_00004-11-2 loss: 0.863067  [   96/  306]
train() client id: f_00004-11-3 loss: 0.871211  [  128/  306]
train() client id: f_00004-11-4 loss: 0.825721  [  160/  306]
train() client id: f_00004-11-5 loss: 0.812559  [  192/  306]
train() client id: f_00004-11-6 loss: 0.898053  [  224/  306]
train() client id: f_00004-11-7 loss: 0.896618  [  256/  306]
train() client id: f_00004-11-8 loss: 0.780881  [  288/  306]
train() client id: f_00005-0-0 loss: 0.574801  [   32/  146]
train() client id: f_00005-0-1 loss: 0.796750  [   64/  146]
train() client id: f_00005-0-2 loss: 0.722871  [   96/  146]
train() client id: f_00005-0-3 loss: 0.534459  [  128/  146]
train() client id: f_00005-1-0 loss: 0.850978  [   32/  146]
train() client id: f_00005-1-1 loss: 0.744843  [   64/  146]
train() client id: f_00005-1-2 loss: 0.568634  [   96/  146]
train() client id: f_00005-1-3 loss: 0.478450  [  128/  146]
train() client id: f_00005-2-0 loss: 0.721354  [   32/  146]
train() client id: f_00005-2-1 loss: 0.521130  [   64/  146]
train() client id: f_00005-2-2 loss: 0.668922  [   96/  146]
train() client id: f_00005-2-3 loss: 0.670759  [  128/  146]
train() client id: f_00005-3-0 loss: 0.700401  [   32/  146]
train() client id: f_00005-3-1 loss: 0.530105  [   64/  146]
train() client id: f_00005-3-2 loss: 0.555759  [   96/  146]
train() client id: f_00005-3-3 loss: 0.859704  [  128/  146]
train() client id: f_00005-4-0 loss: 0.467087  [   32/  146]
train() client id: f_00005-4-1 loss: 0.954994  [   64/  146]
train() client id: f_00005-4-2 loss: 0.595419  [   96/  146]
train() client id: f_00005-4-3 loss: 0.560298  [  128/  146]
train() client id: f_00005-5-0 loss: 0.676149  [   32/  146]
train() client id: f_00005-5-1 loss: 0.737244  [   64/  146]
train() client id: f_00005-5-2 loss: 0.512082  [   96/  146]
train() client id: f_00005-5-3 loss: 0.652139  [  128/  146]
train() client id: f_00005-6-0 loss: 0.609829  [   32/  146]
train() client id: f_00005-6-1 loss: 0.578245  [   64/  146]
train() client id: f_00005-6-2 loss: 0.682456  [   96/  146]
train() client id: f_00005-6-3 loss: 0.444957  [  128/  146]
train() client id: f_00005-7-0 loss: 0.680072  [   32/  146]
train() client id: f_00005-7-1 loss: 0.356306  [   64/  146]
train() client id: f_00005-7-2 loss: 0.724733  [   96/  146]
train() client id: f_00005-7-3 loss: 0.673783  [  128/  146]
train() client id: f_00005-8-0 loss: 0.639803  [   32/  146]
train() client id: f_00005-8-1 loss: 0.521132  [   64/  146]
train() client id: f_00005-8-2 loss: 0.429310  [   96/  146]
train() client id: f_00005-8-3 loss: 0.800218  [  128/  146]
train() client id: f_00005-9-0 loss: 0.483551  [   32/  146]
train() client id: f_00005-9-1 loss: 0.721058  [   64/  146]
train() client id: f_00005-9-2 loss: 0.618292  [   96/  146]
train() client id: f_00005-9-3 loss: 0.417101  [  128/  146]
train() client id: f_00005-10-0 loss: 0.775240  [   32/  146]
train() client id: f_00005-10-1 loss: 0.698564  [   64/  146]
train() client id: f_00005-10-2 loss: 0.452045  [   96/  146]
train() client id: f_00005-10-3 loss: 0.657562  [  128/  146]
train() client id: f_00005-11-0 loss: 0.512731  [   32/  146]
train() client id: f_00005-11-1 loss: 0.699667  [   64/  146]
train() client id: f_00005-11-2 loss: 0.696484  [   96/  146]
train() client id: f_00005-11-3 loss: 0.591062  [  128/  146]
train() client id: f_00006-0-0 loss: 0.497046  [   32/   54]
train() client id: f_00006-1-0 loss: 0.534224  [   32/   54]
train() client id: f_00006-2-0 loss: 0.529195  [   32/   54]
train() client id: f_00006-3-0 loss: 0.483607  [   32/   54]
train() client id: f_00006-4-0 loss: 0.489626  [   32/   54]
train() client id: f_00006-5-0 loss: 0.485540  [   32/   54]
train() client id: f_00006-6-0 loss: 0.438403  [   32/   54]
train() client id: f_00006-7-0 loss: 0.485161  [   32/   54]
train() client id: f_00006-8-0 loss: 0.542041  [   32/   54]
train() client id: f_00006-9-0 loss: 0.527117  [   32/   54]
train() client id: f_00006-10-0 loss: 0.523493  [   32/   54]
train() client id: f_00006-11-0 loss: 0.531134  [   32/   54]
train() client id: f_00007-0-0 loss: 0.572979  [   32/  179]
train() client id: f_00007-0-1 loss: 0.411465  [   64/  179]
train() client id: f_00007-0-2 loss: 0.574331  [   96/  179]
train() client id: f_00007-0-3 loss: 0.680364  [  128/  179]
train() client id: f_00007-0-4 loss: 0.437494  [  160/  179]
train() client id: f_00007-1-0 loss: 0.641929  [   32/  179]
train() client id: f_00007-1-1 loss: 0.370615  [   64/  179]
train() client id: f_00007-1-2 loss: 0.777108  [   96/  179]
train() client id: f_00007-1-3 loss: 0.457958  [  128/  179]
train() client id: f_00007-1-4 loss: 0.415672  [  160/  179]
train() client id: f_00007-2-0 loss: 0.626449  [   32/  179]
train() client id: f_00007-2-1 loss: 0.585698  [   64/  179]
train() client id: f_00007-2-2 loss: 0.461307  [   96/  179]
train() client id: f_00007-2-3 loss: 0.444139  [  128/  179]
train() client id: f_00007-2-4 loss: 0.501751  [  160/  179]
train() client id: f_00007-3-0 loss: 0.541396  [   32/  179]
train() client id: f_00007-3-1 loss: 0.456410  [   64/  179]
train() client id: f_00007-3-2 loss: 0.349976  [   96/  179]
train() client id: f_00007-3-3 loss: 0.568433  [  128/  179]
train() client id: f_00007-3-4 loss: 0.674907  [  160/  179]
train() client id: f_00007-4-0 loss: 0.519842  [   32/  179]
train() client id: f_00007-4-1 loss: 0.640053  [   64/  179]
train() client id: f_00007-4-2 loss: 0.594371  [   96/  179]
train() client id: f_00007-4-3 loss: 0.386935  [  128/  179]
train() client id: f_00007-4-4 loss: 0.441518  [  160/  179]
train() client id: f_00007-5-0 loss: 0.458482  [   32/  179]
train() client id: f_00007-5-1 loss: 0.557712  [   64/  179]
train() client id: f_00007-5-2 loss: 0.447579  [   96/  179]
train() client id: f_00007-5-3 loss: 0.427208  [  128/  179]
train() client id: f_00007-5-4 loss: 0.693019  [  160/  179]
train() client id: f_00007-6-0 loss: 0.367965  [   32/  179]
train() client id: f_00007-6-1 loss: 0.557812  [   64/  179]
train() client id: f_00007-6-2 loss: 0.638698  [   96/  179]
train() client id: f_00007-6-3 loss: 0.371651  [  128/  179]
train() client id: f_00007-6-4 loss: 0.426877  [  160/  179]
train() client id: f_00007-7-0 loss: 0.547827  [   32/  179]
train() client id: f_00007-7-1 loss: 0.349614  [   64/  179]
train() client id: f_00007-7-2 loss: 0.662232  [   96/  179]
train() client id: f_00007-7-3 loss: 0.557537  [  128/  179]
train() client id: f_00007-7-4 loss: 0.326448  [  160/  179]
train() client id: f_00007-8-0 loss: 0.476006  [   32/  179]
train() client id: f_00007-8-1 loss: 0.623339  [   64/  179]
train() client id: f_00007-8-2 loss: 0.423453  [   96/  179]
train() client id: f_00007-8-3 loss: 0.495317  [  128/  179]
train() client id: f_00007-8-4 loss: 0.497786  [  160/  179]
train() client id: f_00007-9-0 loss: 0.603687  [   32/  179]
train() client id: f_00007-9-1 loss: 0.456215  [   64/  179]
train() client id: f_00007-9-2 loss: 0.410966  [   96/  179]
train() client id: f_00007-9-3 loss: 0.516664  [  128/  179]
train() client id: f_00007-9-4 loss: 0.548336  [  160/  179]
train() client id: f_00007-10-0 loss: 0.431291  [   32/  179]
train() client id: f_00007-10-1 loss: 0.458163  [   64/  179]
train() client id: f_00007-10-2 loss: 0.588917  [   96/  179]
train() client id: f_00007-10-3 loss: 0.496553  [  128/  179]
train() client id: f_00007-10-4 loss: 0.570144  [  160/  179]
train() client id: f_00007-11-0 loss: 0.562148  [   32/  179]
train() client id: f_00007-11-1 loss: 0.416642  [   64/  179]
train() client id: f_00007-11-2 loss: 0.615726  [   96/  179]
train() client id: f_00007-11-3 loss: 0.485099  [  128/  179]
train() client id: f_00007-11-4 loss: 0.371343  [  160/  179]
train() client id: f_00008-0-0 loss: 0.910696  [   32/  130]
train() client id: f_00008-0-1 loss: 0.725001  [   64/  130]
train() client id: f_00008-0-2 loss: 0.774643  [   96/  130]
train() client id: f_00008-0-3 loss: 0.753345  [  128/  130]
train() client id: f_00008-1-0 loss: 0.764074  [   32/  130]
train() client id: f_00008-1-1 loss: 0.769808  [   64/  130]
train() client id: f_00008-1-2 loss: 0.809146  [   96/  130]
train() client id: f_00008-1-3 loss: 0.863083  [  128/  130]
train() client id: f_00008-2-0 loss: 0.703726  [   32/  130]
train() client id: f_00008-2-1 loss: 0.848266  [   64/  130]
train() client id: f_00008-2-2 loss: 0.822145  [   96/  130]
train() client id: f_00008-2-3 loss: 0.828963  [  128/  130]
train() client id: f_00008-3-0 loss: 0.746125  [   32/  130]
train() client id: f_00008-3-1 loss: 0.827686  [   64/  130]
train() client id: f_00008-3-2 loss: 0.735480  [   96/  130]
train() client id: f_00008-3-3 loss: 0.884766  [  128/  130]
train() client id: f_00008-4-0 loss: 0.757712  [   32/  130]
train() client id: f_00008-4-1 loss: 0.831091  [   64/  130]
train() client id: f_00008-4-2 loss: 0.833038  [   96/  130]
train() client id: f_00008-4-3 loss: 0.770661  [  128/  130]
train() client id: f_00008-5-0 loss: 0.841912  [   32/  130]
train() client id: f_00008-5-1 loss: 0.756546  [   64/  130]
train() client id: f_00008-5-2 loss: 0.774759  [   96/  130]
train() client id: f_00008-5-3 loss: 0.795679  [  128/  130]
train() client id: f_00008-6-0 loss: 0.796559  [   32/  130]
train() client id: f_00008-6-1 loss: 0.716190  [   64/  130]
train() client id: f_00008-6-2 loss: 0.917318  [   96/  130]
train() client id: f_00008-6-3 loss: 0.760756  [  128/  130]
train() client id: f_00008-7-0 loss: 0.729035  [   32/  130]
train() client id: f_00008-7-1 loss: 0.820049  [   64/  130]
train() client id: f_00008-7-2 loss: 0.867460  [   96/  130]
train() client id: f_00008-7-3 loss: 0.776489  [  128/  130]
train() client id: f_00008-8-0 loss: 0.845564  [   32/  130]
train() client id: f_00008-8-1 loss: 0.800445  [   64/  130]
train() client id: f_00008-8-2 loss: 0.715017  [   96/  130]
train() client id: f_00008-8-3 loss: 0.807662  [  128/  130]
train() client id: f_00008-9-0 loss: 0.846481  [   32/  130]
train() client id: f_00008-9-1 loss: 0.835894  [   64/  130]
train() client id: f_00008-9-2 loss: 0.652244  [   96/  130]
train() client id: f_00008-9-3 loss: 0.831859  [  128/  130]
train() client id: f_00008-10-0 loss: 0.886332  [   32/  130]
train() client id: f_00008-10-1 loss: 0.817150  [   64/  130]
train() client id: f_00008-10-2 loss: 0.677290  [   96/  130]
train() client id: f_00008-10-3 loss: 0.796001  [  128/  130]
train() client id: f_00008-11-0 loss: 0.739309  [   32/  130]
train() client id: f_00008-11-1 loss: 0.735434  [   64/  130]
train() client id: f_00008-11-2 loss: 0.868403  [   96/  130]
train() client id: f_00008-11-3 loss: 0.824059  [  128/  130]
train() client id: f_00009-0-0 loss: 1.181136  [   32/  118]
train() client id: f_00009-0-1 loss: 1.259389  [   64/  118]
train() client id: f_00009-0-2 loss: 1.182858  [   96/  118]
train() client id: f_00009-1-0 loss: 1.217934  [   32/  118]
train() client id: f_00009-1-1 loss: 1.124537  [   64/  118]
train() client id: f_00009-1-2 loss: 1.105077  [   96/  118]
train() client id: f_00009-2-0 loss: 1.252522  [   32/  118]
train() client id: f_00009-2-1 loss: 1.025440  [   64/  118]
train() client id: f_00009-2-2 loss: 0.981604  [   96/  118]
train() client id: f_00009-3-0 loss: 1.032965  [   32/  118]
train() client id: f_00009-3-1 loss: 1.098764  [   64/  118]
train() client id: f_00009-3-2 loss: 1.201479  [   96/  118]
train() client id: f_00009-4-0 loss: 1.159537  [   32/  118]
train() client id: f_00009-4-1 loss: 0.984642  [   64/  118]
train() client id: f_00009-4-2 loss: 0.956070  [   96/  118]
train() client id: f_00009-5-0 loss: 1.116265  [   32/  118]
train() client id: f_00009-5-1 loss: 1.065787  [   64/  118]
train() client id: f_00009-5-2 loss: 0.929236  [   96/  118]
train() client id: f_00009-6-0 loss: 0.884409  [   32/  118]
train() client id: f_00009-6-1 loss: 1.046860  [   64/  118]
train() client id: f_00009-6-2 loss: 1.090978  [   96/  118]
train() client id: f_00009-7-0 loss: 1.062867  [   32/  118]
train() client id: f_00009-7-1 loss: 0.887646  [   64/  118]
train() client id: f_00009-7-2 loss: 1.013327  [   96/  118]
train() client id: f_00009-8-0 loss: 0.948110  [   32/  118]
train() client id: f_00009-8-1 loss: 0.977369  [   64/  118]
train() client id: f_00009-8-2 loss: 0.939078  [   96/  118]
train() client id: f_00009-9-0 loss: 1.020854  [   32/  118]
train() client id: f_00009-9-1 loss: 1.050503  [   64/  118]
train() client id: f_00009-9-2 loss: 1.001893  [   96/  118]
train() client id: f_00009-10-0 loss: 0.788958  [   32/  118]
train() client id: f_00009-10-1 loss: 1.007873  [   64/  118]
train() client id: f_00009-10-2 loss: 1.116884  [   96/  118]
train() client id: f_00009-11-0 loss: 0.974605  [   32/  118]
train() client id: f_00009-11-1 loss: 1.093291  [   64/  118]
train() client id: f_00009-11-2 loss: 0.979477  [   96/  118]
At round 24 accuracy: 0.6445623342175066
At round 24 training accuracy: 0.5828303152246814
At round 24 training loss: 0.834527655938345
update_location
xs = -4.528292 21.001589 30.045120 -30.943528 -70.103519 -5.217951 -72.215960 93.375741 -1.680116 14.695607 
ys = 107.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 -3.124922 -19.177652 -37.154970 4.001482 
xs mean: -2.5571308405871336
ys mean: 8.371751218646876
dists_uav = 146.954668 103.358845 104.424390 107.078341 122.482363 101.600101 123.389262 138.155026 106.692617 101.153214 
uav_gains = -104.183052 -100.358717 -100.470078 -100.742583 -102.202124 -100.172373 -102.282251 -103.510677 -100.703400 -100.124511 
uav_gains_db_mean: -101.47497662326627
dists_bs = 183.316629 252.663934 268.715519 209.749488 196.069203 256.439749 205.652733 331.255941 273.951663 255.391693 
bs_gains = -102.936711 -106.838270 -107.587256 -104.574685 -103.754522 -107.018649 -104.334825 -110.131642 -107.821929 -106.968849 
bs_gains_db_mean: -106.19673378411315
Round 25
-------------------------------
ene_coms = [0.00762223 0.00878052 0.0064278  0.00650277 0.00746265 0.00887163
 0.00696099 0.00737473 0.00930038 0.00884629]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.68171009 16.0122961   7.53850569  2.69885322 18.43663242  8.90181225
  3.36537226 10.84446355  7.9540348   7.22783032]
obj_prev = 90.66151070513638
eta_min = 8.787765460958707e-13	eta_max = 0.9355667189601727
af = 19.15709316343986	bf = 1.5332213116154825	zeta = 21.07280247978385	eta = 0.909090909090909
af = 19.15709316343986	bf = 1.5332213116154825	zeta = 36.94996036106248	eta = 0.5184604523588996
af = 19.15709316343986	bf = 1.5332213116154825	zeta = 29.317121655688823	eta = 0.6534438608410432
af = 19.15709316343986	bf = 1.5332213116154825	zeta = 27.945078997703806	eta = 0.6855265345649573
af = 19.15709316343986	bf = 1.5332213116154825	zeta = 27.876209008102023	eta = 0.6872201725088224
af = 19.15709316343986	bf = 1.5332213116154825	zeta = 27.876022703104766	eta = 0.6872247654363615
eta = 0.6872247654363615
ene_coms = [0.00762223 0.00878052 0.0064278  0.00650277 0.00746265 0.00887163
 0.00696099 0.00737473 0.00930038 0.00884629]
ene_comp = [0.03095185 0.06509714 0.03046055 0.01056293 0.07516877 0.03586484
 0.01326508 0.04397127 0.03193445 0.02898665]
ene_total = [2.41957514 4.63400662 2.31383724 1.07045347 5.18309027 2.80611336
 1.26868848 3.22069931 2.58647184 2.37308697]
ti_comp = [0.38212291 0.37054007 0.39406719 0.39331749 0.38371873 0.36962897
 0.38873533 0.38459796 0.36534139 0.36988232]
ti_coms = [0.07622232 0.08780516 0.06427804 0.06502774 0.0746265  0.08871626
 0.0696099  0.07374727 0.09300384 0.08846291]
t_total = [28.7498951 28.7498951 28.7498951 28.7498951 28.7498951 28.7498951
 28.7498951 28.7498951 28.7498951 28.7498951]
ene_coms = [0.00762223 0.00878052 0.0064278  0.00650277 0.00746265 0.00887163
 0.00696099 0.00737473 0.00930038 0.00884629]
ene_comp = [1.26921027e-05 1.25572737e-04 1.13750316e-05 4.76153455e-07
 1.80287815e-04 2.11035086e-05 9.65386791e-07 3.59230528e-05
 1.52496688e-05 1.11262027e-05]
ene_total = [0.47890377 0.55863811 0.4039002  0.40791904 0.47940642 0.55780016
 0.43669154 0.46483609 0.58432701 0.55558515]
optimize_network iter = 0 obj = 4.928007490680106
eta = 0.6872247654363615
freqs = [40499857.80197837 87840889.61106814 38648927.28975732 13427996.65608273
 97947748.17219956 48514648.26620392 17061836.59427158 57165247.10755022
 43704938.26766153 39183611.61968298]
eta_min = 0.6872247654363628	eta_max = 0.6872247654363592
af = 0.021694268680991797	bf = 1.5332213116154825	zeta = 0.02386369554909098	eta = 0.9090909090909091
af = 0.021694268680991797	bf = 1.5332213116154825	zeta = 16.888178958906096	eta = 0.0012845830645080401
af = 0.021694268680991797	bf = 1.5332213116154825	zeta = 1.679804872640167	eta = 0.012914755180400616
af = 0.021694268680991797	bf = 1.5332213116154825	zeta = 1.648872823089195	eta = 0.013157029685495796
af = 0.021694268680991797	bf = 1.5332213116154825	zeta = 1.6488690496316492	eta = 0.013157059795523611
eta = 0.013157059795523611
ene_coms = [0.00762223 0.00878052 0.0064278  0.00650277 0.00746265 0.00887163
 0.00696099 0.00737473 0.00930038 0.00884629]
ene_comp = [1.46541765e-04 1.44985042e-03 1.31334991e-04 5.49762078e-06
 2.08158531e-03 2.43659027e-04 1.11462606e-05 4.14764021e-04
 1.76071170e-04 1.28462038e-04]
ene_total = [0.1544473  0.20338506 0.13039914 0.12938785 0.18974441 0.18121666
 0.13860973 0.15485917 0.18839692 0.1784228 ]
ti_comp = [0.38212291 0.37054007 0.39406719 0.39331749 0.38371873 0.36962897
 0.38873533 0.38459796 0.36534139 0.36988232]
ti_coms = [0.07622232 0.08780516 0.06427804 0.06502774 0.0746265  0.08871626
 0.0696099  0.07374727 0.09300384 0.08846291]
t_total = [28.7498951 28.7498951 28.7498951 28.7498951 28.7498951 28.7498951
 28.7498951 28.7498951 28.7498951 28.7498951]
ene_coms = [0.00762223 0.00878052 0.0064278  0.00650277 0.00746265 0.00887163
 0.00696099 0.00737473 0.00930038 0.00884629]
ene_comp = [1.26921027e-05 1.25572737e-04 1.13750316e-05 4.76153455e-07
 1.80287815e-04 2.11035086e-05 9.65386791e-07 3.59230528e-05
 1.52496688e-05 1.11262027e-05]
ene_total = [0.47890377 0.55863811 0.4039002  0.40791904 0.47940642 0.55780016
 0.43669154 0.46483609 0.58432701 0.55558515]
optimize_network iter = 1 obj = 4.928007490680127
eta = 0.6872247654363628
freqs = [40499857.80197836 87840889.61106813 38648927.28975729 13427996.65608272
 97947748.17219953 48514648.26620391 17061836.59427158 57165247.10755022
 43704938.26766152 39183611.61968299]
Done!
ene_coms = [0.00762223 0.00878052 0.0064278  0.00650277 0.00746265 0.00887163
 0.00696099 0.00737473 0.00930038 0.00884629]
ene_comp = [1.24002029e-05 1.22684748e-04 1.11134225e-05 4.65202623e-07
 1.76141459e-04 2.06181588e-05 9.43184308e-07 3.50968752e-05
 1.48989487e-05 1.08703164e-05]
ene_total = [0.00763463 0.0089032  0.00643892 0.00650324 0.00763879 0.00889224
 0.00696193 0.00740982 0.00931528 0.00885716]
At round 25 energy consumption: 0.07855522471535487
At round 25 eta: 0.6872247654363628
At round 25 a_n: 19.61895668202144
At round 25 local rounds: 12.282479034830587
At round 25 global rounds: 62.72541593452077
gradient difference: 0.3924884796142578
train() client id: f_00000-0-0 loss: 1.267974  [   32/  126]
train() client id: f_00000-0-1 loss: 1.028800  [   64/  126]
train() client id: f_00000-0-2 loss: 1.268172  [   96/  126]
train() client id: f_00000-1-0 loss: 0.976639  [   32/  126]
train() client id: f_00000-1-1 loss: 1.170909  [   64/  126]
train() client id: f_00000-1-2 loss: 1.335071  [   96/  126]
train() client id: f_00000-2-0 loss: 0.977374  [   32/  126]
train() client id: f_00000-2-1 loss: 1.086874  [   64/  126]
train() client id: f_00000-2-2 loss: 1.152301  [   96/  126]
train() client id: f_00000-3-0 loss: 0.938656  [   32/  126]
train() client id: f_00000-3-1 loss: 0.997321  [   64/  126]
train() client id: f_00000-3-2 loss: 1.060147  [   96/  126]
train() client id: f_00000-4-0 loss: 0.997703  [   32/  126]
train() client id: f_00000-4-1 loss: 0.983871  [   64/  126]
train() client id: f_00000-4-2 loss: 0.937897  [   96/  126]
train() client id: f_00000-5-0 loss: 1.049128  [   32/  126]
train() client id: f_00000-5-1 loss: 0.863251  [   64/  126]
train() client id: f_00000-5-2 loss: 0.786407  [   96/  126]
train() client id: f_00000-6-0 loss: 0.945161  [   32/  126]
train() client id: f_00000-6-1 loss: 0.905473  [   64/  126]
train() client id: f_00000-6-2 loss: 0.900842  [   96/  126]
train() client id: f_00000-7-0 loss: 0.773635  [   32/  126]
train() client id: f_00000-7-1 loss: 1.061516  [   64/  126]
train() client id: f_00000-7-2 loss: 0.889473  [   96/  126]
train() client id: f_00000-8-0 loss: 0.887809  [   32/  126]
train() client id: f_00000-8-1 loss: 0.846402  [   64/  126]
train() client id: f_00000-8-2 loss: 0.951173  [   96/  126]
train() client id: f_00000-9-0 loss: 0.696609  [   32/  126]
train() client id: f_00000-9-1 loss: 1.077281  [   64/  126]
train() client id: f_00000-9-2 loss: 1.002857  [   96/  126]
train() client id: f_00000-10-0 loss: 0.838917  [   32/  126]
train() client id: f_00000-10-1 loss: 0.763289  [   64/  126]
train() client id: f_00000-10-2 loss: 0.909140  [   96/  126]
train() client id: f_00000-11-0 loss: 0.996554  [   32/  126]
train() client id: f_00000-11-1 loss: 0.871268  [   64/  126]
train() client id: f_00000-11-2 loss: 0.866856  [   96/  126]
train() client id: f_00001-0-0 loss: 0.367640  [   32/  265]
train() client id: f_00001-0-1 loss: 0.498709  [   64/  265]
train() client id: f_00001-0-2 loss: 0.408416  [   96/  265]
train() client id: f_00001-0-3 loss: 0.464915  [  128/  265]
train() client id: f_00001-0-4 loss: 0.454182  [  160/  265]
train() client id: f_00001-0-5 loss: 0.470394  [  192/  265]
train() client id: f_00001-0-6 loss: 0.360160  [  224/  265]
train() client id: f_00001-0-7 loss: 0.496400  [  256/  265]
train() client id: f_00001-1-0 loss: 0.339549  [   32/  265]
train() client id: f_00001-1-1 loss: 0.502622  [   64/  265]
train() client id: f_00001-1-2 loss: 0.342044  [   96/  265]
train() client id: f_00001-1-3 loss: 0.400248  [  128/  265]
train() client id: f_00001-1-4 loss: 0.558911  [  160/  265]
train() client id: f_00001-1-5 loss: 0.399433  [  192/  265]
train() client id: f_00001-1-6 loss: 0.342099  [  224/  265]
train() client id: f_00001-1-7 loss: 0.572805  [  256/  265]
train() client id: f_00001-2-0 loss: 0.457400  [   32/  265]
train() client id: f_00001-2-1 loss: 0.452186  [   64/  265]
train() client id: f_00001-2-2 loss: 0.382088  [   96/  265]
train() client id: f_00001-2-3 loss: 0.440851  [  128/  265]
train() client id: f_00001-2-4 loss: 0.596195  [  160/  265]
train() client id: f_00001-2-5 loss: 0.325352  [  192/  265]
train() client id: f_00001-2-6 loss: 0.344399  [  224/  265]
train() client id: f_00001-2-7 loss: 0.334777  [  256/  265]
train() client id: f_00001-3-0 loss: 0.479636  [   32/  265]
train() client id: f_00001-3-1 loss: 0.498119  [   64/  265]
train() client id: f_00001-3-2 loss: 0.471288  [   96/  265]
train() client id: f_00001-3-3 loss: 0.368287  [  128/  265]
train() client id: f_00001-3-4 loss: 0.353309  [  160/  265]
train() client id: f_00001-3-5 loss: 0.338767  [  192/  265]
train() client id: f_00001-3-6 loss: 0.379113  [  224/  265]
train() client id: f_00001-3-7 loss: 0.384947  [  256/  265]
train() client id: f_00001-4-0 loss: 0.312189  [   32/  265]
train() client id: f_00001-4-1 loss: 0.459017  [   64/  265]
train() client id: f_00001-4-2 loss: 0.428640  [   96/  265]
train() client id: f_00001-4-3 loss: 0.528241  [  128/  265]
train() client id: f_00001-4-4 loss: 0.332217  [  160/  265]
train() client id: f_00001-4-5 loss: 0.426612  [  192/  265]
train() client id: f_00001-4-6 loss: 0.367236  [  224/  265]
train() client id: f_00001-4-7 loss: 0.485111  [  256/  265]
train() client id: f_00001-5-0 loss: 0.362063  [   32/  265]
train() client id: f_00001-5-1 loss: 0.518001  [   64/  265]
train() client id: f_00001-5-2 loss: 0.377567  [   96/  265]
train() client id: f_00001-5-3 loss: 0.468168  [  128/  265]
train() client id: f_00001-5-4 loss: 0.472216  [  160/  265]
train() client id: f_00001-5-5 loss: 0.379326  [  192/  265]
train() client id: f_00001-5-6 loss: 0.397626  [  224/  265]
train() client id: f_00001-5-7 loss: 0.361068  [  256/  265]
train() client id: f_00001-6-0 loss: 0.424583  [   32/  265]
train() client id: f_00001-6-1 loss: 0.421623  [   64/  265]
train() client id: f_00001-6-2 loss: 0.445896  [   96/  265]
train() client id: f_00001-6-3 loss: 0.442180  [  128/  265]
train() client id: f_00001-6-4 loss: 0.317073  [  160/  265]
train() client id: f_00001-6-5 loss: 0.389436  [  192/  265]
train() client id: f_00001-6-6 loss: 0.371214  [  224/  265]
train() client id: f_00001-6-7 loss: 0.374376  [  256/  265]
train() client id: f_00001-7-0 loss: 0.365506  [   32/  265]
train() client id: f_00001-7-1 loss: 0.427846  [   64/  265]
train() client id: f_00001-7-2 loss: 0.394624  [   96/  265]
train() client id: f_00001-7-3 loss: 0.439244  [  128/  265]
train() client id: f_00001-7-4 loss: 0.457770  [  160/  265]
train() client id: f_00001-7-5 loss: 0.432839  [  192/  265]
train() client id: f_00001-7-6 loss: 0.405109  [  224/  265]
train() client id: f_00001-7-7 loss: 0.398866  [  256/  265]
train() client id: f_00001-8-0 loss: 0.385000  [   32/  265]
train() client id: f_00001-8-1 loss: 0.334310  [   64/  265]
train() client id: f_00001-8-2 loss: 0.462275  [   96/  265]
train() client id: f_00001-8-3 loss: 0.453486  [  128/  265]
train() client id: f_00001-8-4 loss: 0.379465  [  160/  265]
train() client id: f_00001-8-5 loss: 0.514912  [  192/  265]
train() client id: f_00001-8-6 loss: 0.460313  [  224/  265]
train() client id: f_00001-8-7 loss: 0.315366  [  256/  265]
train() client id: f_00001-9-0 loss: 0.481552  [   32/  265]
train() client id: f_00001-9-1 loss: 0.407898  [   64/  265]
train() client id: f_00001-9-2 loss: 0.380781  [   96/  265]
train() client id: f_00001-9-3 loss: 0.414401  [  128/  265]
train() client id: f_00001-9-4 loss: 0.335366  [  160/  265]
train() client id: f_00001-9-5 loss: 0.357009  [  192/  265]
train() client id: f_00001-9-6 loss: 0.391340  [  224/  265]
train() client id: f_00001-9-7 loss: 0.518829  [  256/  265]
train() client id: f_00001-10-0 loss: 0.491432  [   32/  265]
train() client id: f_00001-10-1 loss: 0.323953  [   64/  265]
train() client id: f_00001-10-2 loss: 0.616637  [   96/  265]
train() client id: f_00001-10-3 loss: 0.319680  [  128/  265]
train() client id: f_00001-10-4 loss: 0.362378  [  160/  265]
train() client id: f_00001-10-5 loss: 0.393435  [  192/  265]
train() client id: f_00001-10-6 loss: 0.381525  [  224/  265]
train() client id: f_00001-10-7 loss: 0.421075  [  256/  265]
train() client id: f_00001-11-0 loss: 0.587843  [   32/  265]
train() client id: f_00001-11-1 loss: 0.397701  [   64/  265]
train() client id: f_00001-11-2 loss: 0.362889  [   96/  265]
train() client id: f_00001-11-3 loss: 0.450065  [  128/  265]
train() client id: f_00001-11-4 loss: 0.317454  [  160/  265]
train() client id: f_00001-11-5 loss: 0.360507  [  192/  265]
train() client id: f_00001-11-6 loss: 0.379384  [  224/  265]
train() client id: f_00001-11-7 loss: 0.459542  [  256/  265]
train() client id: f_00002-0-0 loss: 1.347586  [   32/  124]
train() client id: f_00002-0-1 loss: 1.185647  [   64/  124]
train() client id: f_00002-0-2 loss: 1.214294  [   96/  124]
train() client id: f_00002-1-0 loss: 1.195027  [   32/  124]
train() client id: f_00002-1-1 loss: 1.279352  [   64/  124]
train() client id: f_00002-1-2 loss: 1.116463  [   96/  124]
train() client id: f_00002-2-0 loss: 1.192593  [   32/  124]
train() client id: f_00002-2-1 loss: 1.277700  [   64/  124]
train() client id: f_00002-2-2 loss: 1.154169  [   96/  124]
train() client id: f_00002-3-0 loss: 1.130011  [   32/  124]
train() client id: f_00002-3-1 loss: 1.171520  [   64/  124]
train() client id: f_00002-3-2 loss: 1.220787  [   96/  124]
train() client id: f_00002-4-0 loss: 1.099163  [   32/  124]
train() client id: f_00002-4-1 loss: 1.087133  [   64/  124]
train() client id: f_00002-4-2 loss: 1.144655  [   96/  124]
train() client id: f_00002-5-0 loss: 1.097146  [   32/  124]
train() client id: f_00002-5-1 loss: 1.272652  [   64/  124]
train() client id: f_00002-5-2 loss: 1.153096  [   96/  124]
train() client id: f_00002-6-0 loss: 1.066475  [   32/  124]
train() client id: f_00002-6-1 loss: 1.123205  [   64/  124]
train() client id: f_00002-6-2 loss: 1.213545  [   96/  124]
train() client id: f_00002-7-0 loss: 1.209347  [   32/  124]
train() client id: f_00002-7-1 loss: 1.086737  [   64/  124]
train() client id: f_00002-7-2 loss: 1.081991  [   96/  124]
train() client id: f_00002-8-0 loss: 1.031071  [   32/  124]
train() client id: f_00002-8-1 loss: 1.198010  [   64/  124]
train() client id: f_00002-8-2 loss: 1.153338  [   96/  124]
train() client id: f_00002-9-0 loss: 1.113671  [   32/  124]
train() client id: f_00002-9-1 loss: 1.110180  [   64/  124]
train() client id: f_00002-9-2 loss: 0.915200  [   96/  124]
train() client id: f_00002-10-0 loss: 0.976078  [   32/  124]
train() client id: f_00002-10-1 loss: 1.106381  [   64/  124]
train() client id: f_00002-10-2 loss: 0.986747  [   96/  124]
train() client id: f_00002-11-0 loss: 1.225798  [   32/  124]
train() client id: f_00002-11-1 loss: 1.055999  [   64/  124]
train() client id: f_00002-11-2 loss: 0.913243  [   96/  124]
train() client id: f_00003-0-0 loss: 0.754985  [   32/   43]
train() client id: f_00003-1-0 loss: 0.875620  [   32/   43]
train() client id: f_00003-2-0 loss: 0.938597  [   32/   43]
train() client id: f_00003-3-0 loss: 0.743919  [   32/   43]
train() client id: f_00003-4-0 loss: 0.591967  [   32/   43]
train() client id: f_00003-5-0 loss: 0.666765  [   32/   43]
train() client id: f_00003-6-0 loss: 0.802303  [   32/   43]
train() client id: f_00003-7-0 loss: 0.734734  [   32/   43]
train() client id: f_00003-8-0 loss: 0.706495  [   32/   43]
train() client id: f_00003-9-0 loss: 0.729264  [   32/   43]
train() client id: f_00003-10-0 loss: 0.796377  [   32/   43]
train() client id: f_00003-11-0 loss: 0.529061  [   32/   43]
train() client id: f_00004-0-0 loss: 1.070824  [   32/  306]
train() client id: f_00004-0-1 loss: 0.897657  [   64/  306]
train() client id: f_00004-0-2 loss: 1.019266  [   96/  306]
train() client id: f_00004-0-3 loss: 0.937572  [  128/  306]
train() client id: f_00004-0-4 loss: 1.005555  [  160/  306]
train() client id: f_00004-0-5 loss: 1.061945  [  192/  306]
train() client id: f_00004-0-6 loss: 1.042275  [  224/  306]
train() client id: f_00004-0-7 loss: 0.805052  [  256/  306]
train() client id: f_00004-0-8 loss: 0.990575  [  288/  306]
train() client id: f_00004-1-0 loss: 1.027092  [   32/  306]
train() client id: f_00004-1-1 loss: 0.907236  [   64/  306]
train() client id: f_00004-1-2 loss: 1.014628  [   96/  306]
train() client id: f_00004-1-3 loss: 0.954439  [  128/  306]
train() client id: f_00004-1-4 loss: 1.014092  [  160/  306]
train() client id: f_00004-1-5 loss: 0.980852  [  192/  306]
train() client id: f_00004-1-6 loss: 1.071053  [  224/  306]
train() client id: f_00004-1-7 loss: 0.912216  [  256/  306]
train() client id: f_00004-1-8 loss: 0.876802  [  288/  306]
train() client id: f_00004-2-0 loss: 1.014717  [   32/  306]
train() client id: f_00004-2-1 loss: 1.022666  [   64/  306]
train() client id: f_00004-2-2 loss: 0.957889  [   96/  306]
train() client id: f_00004-2-3 loss: 0.937951  [  128/  306]
train() client id: f_00004-2-4 loss: 1.057391  [  160/  306]
train() client id: f_00004-2-5 loss: 0.973742  [  192/  306]
train() client id: f_00004-2-6 loss: 0.888101  [  224/  306]
train() client id: f_00004-2-7 loss: 0.899769  [  256/  306]
train() client id: f_00004-2-8 loss: 0.922818  [  288/  306]
train() client id: f_00004-3-0 loss: 0.955008  [   32/  306]
train() client id: f_00004-3-1 loss: 0.883762  [   64/  306]
train() client id: f_00004-3-2 loss: 0.944958  [   96/  306]
train() client id: f_00004-3-3 loss: 1.016901  [  128/  306]
train() client id: f_00004-3-4 loss: 0.953607  [  160/  306]
train() client id: f_00004-3-5 loss: 0.973179  [  192/  306]
train() client id: f_00004-3-6 loss: 1.041437  [  224/  306]
train() client id: f_00004-3-7 loss: 0.860331  [  256/  306]
train() client id: f_00004-3-8 loss: 1.086906  [  288/  306]
train() client id: f_00004-4-0 loss: 0.875657  [   32/  306]
train() client id: f_00004-4-1 loss: 0.873528  [   64/  306]
train() client id: f_00004-4-2 loss: 0.915382  [   96/  306]
train() client id: f_00004-4-3 loss: 0.983670  [  128/  306]
train() client id: f_00004-4-4 loss: 0.920462  [  160/  306]
train() client id: f_00004-4-5 loss: 1.042140  [  192/  306]
train() client id: f_00004-4-6 loss: 1.035887  [  224/  306]
train() client id: f_00004-4-7 loss: 0.981622  [  256/  306]
train() client id: f_00004-4-8 loss: 0.987466  [  288/  306]
train() client id: f_00004-5-0 loss: 0.953279  [   32/  306]
train() client id: f_00004-5-1 loss: 1.055748  [   64/  306]
train() client id: f_00004-5-2 loss: 0.897813  [   96/  306]
train() client id: f_00004-5-3 loss: 0.944593  [  128/  306]
train() client id: f_00004-5-4 loss: 0.952888  [  160/  306]
train() client id: f_00004-5-5 loss: 1.016351  [  192/  306]
train() client id: f_00004-5-6 loss: 0.852392  [  224/  306]
train() client id: f_00004-5-7 loss: 0.993438  [  256/  306]
train() client id: f_00004-5-8 loss: 0.974849  [  288/  306]
train() client id: f_00004-6-0 loss: 0.992833  [   32/  306]
train() client id: f_00004-6-1 loss: 1.062605  [   64/  306]
train() client id: f_00004-6-2 loss: 0.914879  [   96/  306]
train() client id: f_00004-6-3 loss: 0.875385  [  128/  306]
train() client id: f_00004-6-4 loss: 1.011095  [  160/  306]
train() client id: f_00004-6-5 loss: 0.852709  [  192/  306]
train() client id: f_00004-6-6 loss: 0.974135  [  224/  306]
train() client id: f_00004-6-7 loss: 0.950985  [  256/  306]
train() client id: f_00004-6-8 loss: 0.951293  [  288/  306]
train() client id: f_00004-7-0 loss: 0.894638  [   32/  306]
train() client id: f_00004-7-1 loss: 0.945358  [   64/  306]
train() client id: f_00004-7-2 loss: 0.944317  [   96/  306]
train() client id: f_00004-7-3 loss: 0.957627  [  128/  306]
train() client id: f_00004-7-4 loss: 0.885959  [  160/  306]
train() client id: f_00004-7-5 loss: 0.832264  [  192/  306]
train() client id: f_00004-7-6 loss: 1.032357  [  224/  306]
train() client id: f_00004-7-7 loss: 1.034801  [  256/  306]
train() client id: f_00004-7-8 loss: 0.994542  [  288/  306]
train() client id: f_00004-8-0 loss: 0.846887  [   32/  306]
train() client id: f_00004-8-1 loss: 0.862679  [   64/  306]
train() client id: f_00004-8-2 loss: 1.041400  [   96/  306]
train() client id: f_00004-8-3 loss: 1.044699  [  128/  306]
train() client id: f_00004-8-4 loss: 0.980502  [  160/  306]
train() client id: f_00004-8-5 loss: 0.945522  [  192/  306]
train() client id: f_00004-8-6 loss: 0.928860  [  224/  306]
train() client id: f_00004-8-7 loss: 1.019580  [  256/  306]
train() client id: f_00004-8-8 loss: 0.893729  [  288/  306]
train() client id: f_00004-9-0 loss: 0.866391  [   32/  306]
train() client id: f_00004-9-1 loss: 0.936544  [   64/  306]
train() client id: f_00004-9-2 loss: 0.996520  [   96/  306]
train() client id: f_00004-9-3 loss: 0.996419  [  128/  306]
train() client id: f_00004-9-4 loss: 0.955681  [  160/  306]
train() client id: f_00004-9-5 loss: 1.078123  [  192/  306]
train() client id: f_00004-9-6 loss: 0.928704  [  224/  306]
train() client id: f_00004-9-7 loss: 0.837471  [  256/  306]
train() client id: f_00004-9-8 loss: 0.974596  [  288/  306]
train() client id: f_00004-10-0 loss: 0.932418  [   32/  306]
train() client id: f_00004-10-1 loss: 1.015276  [   64/  306]
train() client id: f_00004-10-2 loss: 0.883304  [   96/  306]
train() client id: f_00004-10-3 loss: 0.871181  [  128/  306]
train() client id: f_00004-10-4 loss: 0.913116  [  160/  306]
train() client id: f_00004-10-5 loss: 1.013889  [  192/  306]
train() client id: f_00004-10-6 loss: 0.988260  [  224/  306]
train() client id: f_00004-10-7 loss: 0.964065  [  256/  306]
train() client id: f_00004-10-8 loss: 0.816095  [  288/  306]
train() client id: f_00004-11-0 loss: 1.031912  [   32/  306]
train() client id: f_00004-11-1 loss: 0.958217  [   64/  306]
train() client id: f_00004-11-2 loss: 0.843288  [   96/  306]
train() client id: f_00004-11-3 loss: 1.005963  [  128/  306]
train() client id: f_00004-11-4 loss: 0.895250  [  160/  306]
train() client id: f_00004-11-5 loss: 0.900155  [  192/  306]
train() client id: f_00004-11-6 loss: 1.016222  [  224/  306]
train() client id: f_00004-11-7 loss: 0.997825  [  256/  306]
train() client id: f_00004-11-8 loss: 0.897142  [  288/  306]
train() client id: f_00005-0-0 loss: 0.715810  [   32/  146]
train() client id: f_00005-0-1 loss: 0.580230  [   64/  146]
train() client id: f_00005-0-2 loss: 0.888436  [   96/  146]
train() client id: f_00005-0-3 loss: 0.507255  [  128/  146]
train() client id: f_00005-1-0 loss: 0.763247  [   32/  146]
train() client id: f_00005-1-1 loss: 0.547176  [   64/  146]
train() client id: f_00005-1-2 loss: 0.484893  [   96/  146]
train() client id: f_00005-1-3 loss: 0.739059  [  128/  146]
train() client id: f_00005-2-0 loss: 0.811064  [   32/  146]
train() client id: f_00005-2-1 loss: 0.675741  [   64/  146]
train() client id: f_00005-2-2 loss: 0.569038  [   96/  146]
train() client id: f_00005-2-3 loss: 0.529477  [  128/  146]
train() client id: f_00005-3-0 loss: 0.554209  [   32/  146]
train() client id: f_00005-3-1 loss: 0.655787  [   64/  146]
train() client id: f_00005-3-2 loss: 0.747165  [   96/  146]
train() client id: f_00005-3-3 loss: 0.706077  [  128/  146]
train() client id: f_00005-4-0 loss: 0.559406  [   32/  146]
train() client id: f_00005-4-1 loss: 0.833426  [   64/  146]
train() client id: f_00005-4-2 loss: 0.577589  [   96/  146]
train() client id: f_00005-4-3 loss: 0.474387  [  128/  146]
train() client id: f_00005-5-0 loss: 0.899771  [   32/  146]
train() client id: f_00005-5-1 loss: 0.602145  [   64/  146]
train() client id: f_00005-5-2 loss: 0.570314  [   96/  146]
train() client id: f_00005-5-3 loss: 0.428135  [  128/  146]
train() client id: f_00005-6-0 loss: 0.748513  [   32/  146]
train() client id: f_00005-6-1 loss: 0.660477  [   64/  146]
train() client id: f_00005-6-2 loss: 0.513011  [   96/  146]
train() client id: f_00005-6-3 loss: 0.571981  [  128/  146]
train() client id: f_00005-7-0 loss: 0.450737  [   32/  146]
train() client id: f_00005-7-1 loss: 0.528954  [   64/  146]
train() client id: f_00005-7-2 loss: 0.767130  [   96/  146]
train() client id: f_00005-7-3 loss: 0.709653  [  128/  146]
train() client id: f_00005-8-0 loss: 0.671888  [   32/  146]
train() client id: f_00005-8-1 loss: 0.902179  [   64/  146]
train() client id: f_00005-8-2 loss: 0.478084  [   96/  146]
train() client id: f_00005-8-3 loss: 0.625147  [  128/  146]
train() client id: f_00005-9-0 loss: 0.573148  [   32/  146]
train() client id: f_00005-9-1 loss: 0.699567  [   64/  146]
train() client id: f_00005-9-2 loss: 0.624985  [   96/  146]
train() client id: f_00005-9-3 loss: 0.689811  [  128/  146]
train() client id: f_00005-10-0 loss: 0.451340  [   32/  146]
train() client id: f_00005-10-1 loss: 0.700047  [   64/  146]
train() client id: f_00005-10-2 loss: 0.936258  [   96/  146]
train() client id: f_00005-10-3 loss: 0.555844  [  128/  146]
train() client id: f_00005-11-0 loss: 0.775955  [   32/  146]
train() client id: f_00005-11-1 loss: 0.616330  [   64/  146]
train() client id: f_00005-11-2 loss: 0.636691  [   96/  146]
train() client id: f_00005-11-3 loss: 0.465383  [  128/  146]
train() client id: f_00006-0-0 loss: 0.530092  [   32/   54]
train() client id: f_00006-1-0 loss: 0.548079  [   32/   54]
train() client id: f_00006-2-0 loss: 0.495099  [   32/   54]
train() client id: f_00006-3-0 loss: 0.440000  [   32/   54]
train() client id: f_00006-4-0 loss: 0.493289  [   32/   54]
train() client id: f_00006-5-0 loss: 0.535405  [   32/   54]
train() client id: f_00006-6-0 loss: 0.435413  [   32/   54]
train() client id: f_00006-7-0 loss: 0.440811  [   32/   54]
train() client id: f_00006-8-0 loss: 0.487620  [   32/   54]
train() client id: f_00006-9-0 loss: 0.525922  [   32/   54]
train() client id: f_00006-10-0 loss: 0.437305  [   32/   54]
train() client id: f_00006-11-0 loss: 0.506503  [   32/   54]
train() client id: f_00007-0-0 loss: 0.814605  [   32/  179]
train() client id: f_00007-0-1 loss: 0.810816  [   64/  179]
train() client id: f_00007-0-2 loss: 0.637504  [   96/  179]
train() client id: f_00007-0-3 loss: 0.739077  [  128/  179]
train() client id: f_00007-0-4 loss: 0.686671  [  160/  179]
train() client id: f_00007-1-0 loss: 0.692772  [   32/  179]
train() client id: f_00007-1-1 loss: 0.738770  [   64/  179]
train() client id: f_00007-1-2 loss: 0.752392  [   96/  179]
train() client id: f_00007-1-3 loss: 0.657380  [  128/  179]
train() client id: f_00007-1-4 loss: 0.614303  [  160/  179]
train() client id: f_00007-2-0 loss: 0.559744  [   32/  179]
train() client id: f_00007-2-1 loss: 0.759672  [   64/  179]
train() client id: f_00007-2-2 loss: 0.864623  [   96/  179]
train() client id: f_00007-2-3 loss: 0.614164  [  128/  179]
train() client id: f_00007-2-4 loss: 0.779356  [  160/  179]
train() client id: f_00007-3-0 loss: 0.668611  [   32/  179]
train() client id: f_00007-3-1 loss: 0.876596  [   64/  179]
train() client id: f_00007-3-2 loss: 0.632071  [   96/  179]
train() client id: f_00007-3-3 loss: 0.613434  [  128/  179]
train() client id: f_00007-3-4 loss: 0.668478  [  160/  179]
train() client id: f_00007-4-0 loss: 0.561860  [   32/  179]
train() client id: f_00007-4-1 loss: 0.749309  [   64/  179]
train() client id: f_00007-4-2 loss: 0.801036  [   96/  179]
train() client id: f_00007-4-3 loss: 0.594933  [  128/  179]
train() client id: f_00007-4-4 loss: 0.650102  [  160/  179]
train() client id: f_00007-5-0 loss: 1.063381  [   32/  179]
train() client id: f_00007-5-1 loss: 0.564142  [   64/  179]
train() client id: f_00007-5-2 loss: 0.646867  [   96/  179]
train() client id: f_00007-5-3 loss: 0.654601  [  128/  179]
train() client id: f_00007-5-4 loss: 0.657939  [  160/  179]
train() client id: f_00007-6-0 loss: 0.790200  [   32/  179]
train() client id: f_00007-6-1 loss: 0.597149  [   64/  179]
train() client id: f_00007-6-2 loss: 0.669404  [   96/  179]
train() client id: f_00007-6-3 loss: 0.788319  [  128/  179]
train() client id: f_00007-6-4 loss: 0.677668  [  160/  179]
train() client id: f_00007-7-0 loss: 0.873323  [   32/  179]
train() client id: f_00007-7-1 loss: 0.805443  [   64/  179]
train() client id: f_00007-7-2 loss: 0.543860  [   96/  179]
train() client id: f_00007-7-3 loss: 0.753155  [  128/  179]
train() client id: f_00007-7-4 loss: 0.552665  [  160/  179]
train() client id: f_00007-8-0 loss: 0.766450  [   32/  179]
train() client id: f_00007-8-1 loss: 0.631300  [   64/  179]
train() client id: f_00007-8-2 loss: 0.582711  [   96/  179]
train() client id: f_00007-8-3 loss: 0.804212  [  128/  179]
train() client id: f_00007-8-4 loss: 0.703254  [  160/  179]
train() client id: f_00007-9-0 loss: 0.832872  [   32/  179]
train() client id: f_00007-9-1 loss: 0.564026  [   64/  179]
train() client id: f_00007-9-2 loss: 0.566713  [   96/  179]
train() client id: f_00007-9-3 loss: 0.805999  [  128/  179]
train() client id: f_00007-9-4 loss: 0.824170  [  160/  179]
train() client id: f_00007-10-0 loss: 0.649690  [   32/  179]
train() client id: f_00007-10-1 loss: 0.766046  [   64/  179]
train() client id: f_00007-10-2 loss: 0.647272  [   96/  179]
train() client id: f_00007-10-3 loss: 0.666023  [  128/  179]
train() client id: f_00007-10-4 loss: 0.646351  [  160/  179]
train() client id: f_00007-11-0 loss: 0.648694  [   32/  179]
train() client id: f_00007-11-1 loss: 0.628446  [   64/  179]
train() client id: f_00007-11-2 loss: 0.672693  [   96/  179]
train() client id: f_00007-11-3 loss: 0.809736  [  128/  179]
train() client id: f_00007-11-4 loss: 0.844536  [  160/  179]
train() client id: f_00008-0-0 loss: 0.708238  [   32/  130]
train() client id: f_00008-0-1 loss: 0.733464  [   64/  130]
train() client id: f_00008-0-2 loss: 0.833617  [   96/  130]
train() client id: f_00008-0-3 loss: 0.603696  [  128/  130]
train() client id: f_00008-1-0 loss: 0.686398  [   32/  130]
train() client id: f_00008-1-1 loss: 0.754582  [   64/  130]
train() client id: f_00008-1-2 loss: 0.691468  [   96/  130]
train() client id: f_00008-1-3 loss: 0.744764  [  128/  130]
train() client id: f_00008-2-0 loss: 0.682589  [   32/  130]
train() client id: f_00008-2-1 loss: 0.803865  [   64/  130]
train() client id: f_00008-2-2 loss: 0.609518  [   96/  130]
train() client id: f_00008-2-3 loss: 0.753788  [  128/  130]
train() client id: f_00008-3-0 loss: 0.715354  [   32/  130]
train() client id: f_00008-3-1 loss: 0.826463  [   64/  130]
train() client id: f_00008-3-2 loss: 0.564788  [   96/  130]
train() client id: f_00008-3-3 loss: 0.764158  [  128/  130]
train() client id: f_00008-4-0 loss: 0.784828  [   32/  130]
train() client id: f_00008-4-1 loss: 0.605604  [   64/  130]
train() client id: f_00008-4-2 loss: 0.702084  [   96/  130]
train() client id: f_00008-4-3 loss: 0.782370  [  128/  130]
train() client id: f_00008-5-0 loss: 0.771900  [   32/  130]
train() client id: f_00008-5-1 loss: 0.620461  [   64/  130]
train() client id: f_00008-5-2 loss: 0.743809  [   96/  130]
train() client id: f_00008-5-3 loss: 0.693640  [  128/  130]
train() client id: f_00008-6-0 loss: 0.832783  [   32/  130]
train() client id: f_00008-6-1 loss: 0.767634  [   64/  130]
train() client id: f_00008-6-2 loss: 0.666011  [   96/  130]
train() client id: f_00008-6-3 loss: 0.563708  [  128/  130]
train() client id: f_00008-7-0 loss: 0.592592  [   32/  130]
train() client id: f_00008-7-1 loss: 0.749466  [   64/  130]
train() client id: f_00008-7-2 loss: 0.734783  [   96/  130]
train() client id: f_00008-7-3 loss: 0.798131  [  128/  130]
train() client id: f_00008-8-0 loss: 0.779745  [   32/  130]
train() client id: f_00008-8-1 loss: 0.692662  [   64/  130]
train() client id: f_00008-8-2 loss: 0.696250  [   96/  130]
train() client id: f_00008-8-3 loss: 0.703696  [  128/  130]
train() client id: f_00008-9-0 loss: 0.730226  [   32/  130]
train() client id: f_00008-9-1 loss: 0.770665  [   64/  130]
train() client id: f_00008-9-2 loss: 0.659501  [   96/  130]
train() client id: f_00008-9-3 loss: 0.702499  [  128/  130]
train() client id: f_00008-10-0 loss: 0.762624  [   32/  130]
train() client id: f_00008-10-1 loss: 0.610329  [   64/  130]
train() client id: f_00008-10-2 loss: 0.778686  [   96/  130]
train() client id: f_00008-10-3 loss: 0.672924  [  128/  130]
train() client id: f_00008-11-0 loss: 0.785028  [   32/  130]
train() client id: f_00008-11-1 loss: 0.654695  [   64/  130]
train() client id: f_00008-11-2 loss: 0.743624  [   96/  130]
train() client id: f_00008-11-3 loss: 0.683315  [  128/  130]
train() client id: f_00009-0-0 loss: 0.951577  [   32/  118]
train() client id: f_00009-0-1 loss: 1.117108  [   64/  118]
train() client id: f_00009-0-2 loss: 1.132751  [   96/  118]
train() client id: f_00009-1-0 loss: 1.070851  [   32/  118]
train() client id: f_00009-1-1 loss: 1.052062  [   64/  118]
train() client id: f_00009-1-2 loss: 0.881618  [   96/  118]
train() client id: f_00009-2-0 loss: 0.998618  [   32/  118]
train() client id: f_00009-2-1 loss: 0.968946  [   64/  118]
train() client id: f_00009-2-2 loss: 1.029917  [   96/  118]
train() client id: f_00009-3-0 loss: 0.986660  [   32/  118]
train() client id: f_00009-3-1 loss: 1.002109  [   64/  118]
train() client id: f_00009-3-2 loss: 0.917731  [   96/  118]
train() client id: f_00009-4-0 loss: 0.896124  [   32/  118]
train() client id: f_00009-4-1 loss: 0.873477  [   64/  118]
train() client id: f_00009-4-2 loss: 0.882708  [   96/  118]
train() client id: f_00009-5-0 loss: 0.796392  [   32/  118]
train() client id: f_00009-5-1 loss: 1.014464  [   64/  118]
train() client id: f_00009-5-2 loss: 0.830402  [   96/  118]
train() client id: f_00009-6-0 loss: 0.832481  [   32/  118]
train() client id: f_00009-6-1 loss: 1.037966  [   64/  118]
train() client id: f_00009-6-2 loss: 0.835613  [   96/  118]
train() client id: f_00009-7-0 loss: 0.774981  [   32/  118]
train() client id: f_00009-7-1 loss: 0.971240  [   64/  118]
train() client id: f_00009-7-2 loss: 0.909632  [   96/  118]
train() client id: f_00009-8-0 loss: 0.916103  [   32/  118]
train() client id: f_00009-8-1 loss: 0.794261  [   64/  118]
train() client id: f_00009-8-2 loss: 0.713937  [   96/  118]
train() client id: f_00009-9-0 loss: 0.965362  [   32/  118]
train() client id: f_00009-9-1 loss: 0.762956  [   64/  118]
train() client id: f_00009-9-2 loss: 0.807770  [   96/  118]
train() client id: f_00009-10-0 loss: 0.826864  [   32/  118]
train() client id: f_00009-10-1 loss: 0.778809  [   64/  118]
train() client id: f_00009-10-2 loss: 0.811631  [   96/  118]
train() client id: f_00009-11-0 loss: 0.740956  [   32/  118]
train() client id: f_00009-11-1 loss: 0.938322  [   64/  118]
train() client id: f_00009-11-2 loss: 0.885197  [   96/  118]
At round 25 accuracy: 0.6472148541114059
At round 25 training accuracy: 0.5801475519785378
At round 25 training loss: 0.8306191254546251
update_location
xs = -4.528292 26.001589 35.045120 -35.943528 -65.103519 -0.217951 -77.215960 98.375741 -1.680116 19.695607 
ys = 112.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 -3.124922 -19.177652 -42.154970 4.001482 
xs mean: -0.5571308405871335
ys mean: 8.371751218646876
dists_uav = 150.653756 104.489553 105.971243 108.628755 119.690827 101.466255 126.380653 141.582374 108.535083 101.999651 
uav_gains = -104.454319 -100.476852 -100.629737 -100.898672 -101.951728 -100.158060 -102.542457 -103.777334 -100.889305 -100.214988 
uav_gains_db_mean: -101.59934537671685
dists_bs = 181.537506 256.562038 272.549961 206.347481 198.789077 259.777146 203.199917 335.319633 277.841795 259.127136 
bs_gains = -102.818117 -107.024446 -107.759551 -104.375836 -103.922050 -107.175885 -104.188918 -110.279911 -107.993391 -107.145420 
bs_gains_db_mean: -106.26835253582912
Round 26
-------------------------------
ene_coms = [0.00772674 0.00887458 0.00647152 0.0065465  0.00752425 0.00895254
 0.00704478 0.00747098 0.00939708 0.00893675]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.54962275 15.73455396  7.40773501  2.65258276 18.11592936  8.74796264
  3.3082445  10.656994    7.81704057  7.10339423]
obj_prev = 89.09405976356342
eta_min = 5.571470993640699e-13	eta_max = 0.9359376318302453
af = 18.822611426745162	bf = 1.5217902010421955	zeta = 20.70487256941968	eta = 0.909090909090909
af = 18.822611426745162	bf = 1.5217902010421955	zeta = 36.47354348563434	eta = 0.5160620446477523
af = 18.822611426745162	bf = 1.5217902010421955	zeta = 28.874527804646075	eta = 0.6518759909804135
af = 18.822611426745162	bf = 1.5217902010421955	zeta = 27.50744833584538	eta = 0.6842732628972032
af = 18.822611426745162	bf = 1.5217902010421955	zeta = 27.438443028245974	eta = 0.685994150884166
af = 18.822611426745162	bf = 1.5217902010421955	zeta = 27.438254097082403	eta = 0.6859988744235236
eta = 0.6859988744235236
ene_coms = [0.00772674 0.00887458 0.00647152 0.0065465  0.00752425 0.00895254
 0.00704478 0.00747098 0.00939708 0.00893675]
ene_comp = [0.03109918 0.065407   0.03060554 0.01061321 0.07552657 0.03603555
 0.01332822 0.04418058 0.03208645 0.02912463]
ene_total = [2.38350827 4.5601183  2.27614662 1.05342807 5.09845869 2.76180216
 1.25069053 3.17086951 2.54665827 2.33657367]
ti_comp = [0.39023656 0.3787581  0.40278874 0.40203896 0.39226142 0.37797853
 0.39705617 0.39279414 0.37353316 0.37813641]
ti_coms = [0.07726739 0.08874584 0.0647152  0.06546498 0.07524252 0.08952541
 0.07044777 0.0747098  0.09397079 0.08936753]
t_total = [28.6998909 28.6998909 28.6998909 28.6998909 28.6998909 28.6998909
 28.6998909 28.6998909 28.6998909 28.6998909]
ene_coms = [0.00772674 0.00887458 0.00647152 0.0065465  0.00752425 0.00895254
 0.00704478 0.00747098 0.00939708 0.00893675]
ene_comp = [1.23444220e-05 1.21907042e-04 1.10439753e-05 4.62257651e-07
 1.74995995e-04 2.04710127e-05 9.38625632e-07 3.49336616e-05
 1.47974460e-05 1.07985153e-05]
ene_total = [0.47509939 0.55229123 0.39796215 0.40191544 0.47265395 0.55084984
 0.43253386 0.46078521 0.57779152 0.54928682]
optimize_network iter = 0 obj = 4.871169404392186
eta = 0.6859988744235236
freqs = [39846569.26000275 86344024.59212281 37992047.47429627 13199231.75341679
 96270708.96076392 47668782.91894776 16783794.49456897 56238841.70139346
 42949937.32636417 38510738.68426921]
eta_min = 0.685998874423532	eta_max = 0.6859988744235204
af = 0.020596713522345232	bf = 1.5217902010421955	zeta = 0.022656384874579757	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00772674 0.00887458 0.00647152 0.0065465  0.00752425 0.00895254
 0.00704478 0.00747098 0.00939708 0.00893675]
ene_comp = [3.12181938e-06 3.08294521e-05 2.79294537e-06 1.16901779e-07
 4.42552831e-05 5.17697825e-06 2.37371964e-07 8.83448266e-06
 3.74217229e-06 2.73087023e-06]
ene_total = [1.63904363 1.8883086  1.37281664 1.38814766 1.60483026 1.89939921
 1.4938286  1.58602373 1.99335491 1.89553282]
ti_comp = [0.39023656 0.3787581  0.40278874 0.40203896 0.39226142 0.37797853
 0.39705617 0.39279414 0.37353316 0.37813641]
ti_coms = [0.07726739 0.08874584 0.0647152  0.06546498 0.07524252 0.08952541
 0.07044777 0.0747098  0.09397079 0.08936753]
t_total = [28.6998909 28.6998909 28.6998909 28.6998909 28.6998909 28.6998909
 28.6998909 28.6998909 28.6998909 28.6998909]
ene_coms = [0.00772674 0.00887458 0.00647152 0.0065465  0.00752425 0.00895254
 0.00704478 0.00747098 0.00939708 0.00893675]
ene_comp = [1.23444220e-05 1.21907042e-04 1.10439753e-05 4.62257651e-07
 1.74995995e-04 2.04710127e-05 9.38625632e-07 3.49336616e-05
 1.47974460e-05 1.07985153e-05]
ene_total = [0.47509939 0.55229123 0.39796215 0.40191544 0.47265395 0.55084984
 0.43253386 0.46078521 0.57779152 0.54928682]
optimize_network iter = 1 obj = 4.871169404392136
eta = 0.6859988744235204
freqs = [39846569.26000275 86344024.59212278 37992047.47429628 13199231.7534168
 96270708.96076392 47668782.91894775 16783794.49456897 56238841.70139346
 42949937.32636414 38510738.6842692 ]
Done!
ene_coms = [0.00772674 0.00887458 0.00647152 0.0065465  0.00752425 0.00895254
 0.00704478 0.00747098 0.00939708 0.00893675]
ene_comp = [1.20033831e-05 1.18539120e-04 1.07388638e-05 4.49486875e-07
 1.70161387e-04 1.99054607e-05 9.12694255e-07 3.39685506e-05
 1.43886375e-05 1.05001851e-05]
ene_total = [0.00773874 0.00899312 0.00648226 0.00654695 0.00769441 0.00897245
 0.00704569 0.00750495 0.00941147 0.00894725]
At round 26 energy consumption: 0.07933729133071103
At round 26 eta: 0.6859988744235204
At round 26 a_n: 19.276410835052125
At round 26 local rounds: 12.340942797806987
At round 26 global rounds: 61.3896233641273
gradient difference: 0.44544655084609985
train() client id: f_00000-0-0 loss: 1.258143  [   32/  126]
train() client id: f_00000-0-1 loss: 1.137150  [   64/  126]
train() client id: f_00000-0-2 loss: 1.120970  [   96/  126]
train() client id: f_00000-1-0 loss: 0.857922  [   32/  126]
train() client id: f_00000-1-1 loss: 1.177720  [   64/  126]
train() client id: f_00000-1-2 loss: 0.984557  [   96/  126]
train() client id: f_00000-2-0 loss: 1.005625  [   32/  126]
train() client id: f_00000-2-1 loss: 1.117373  [   64/  126]
train() client id: f_00000-2-2 loss: 0.905776  [   96/  126]
train() client id: f_00000-3-0 loss: 0.986175  [   32/  126]
train() client id: f_00000-3-1 loss: 1.074296  [   64/  126]
train() client id: f_00000-3-2 loss: 0.915473  [   96/  126]
train() client id: f_00000-4-0 loss: 1.003057  [   32/  126]
train() client id: f_00000-4-1 loss: 0.896393  [   64/  126]
train() client id: f_00000-4-2 loss: 0.869910  [   96/  126]
train() client id: f_00000-5-0 loss: 0.872336  [   32/  126]
train() client id: f_00000-5-1 loss: 1.017796  [   64/  126]
train() client id: f_00000-5-2 loss: 0.792036  [   96/  126]
train() client id: f_00000-6-0 loss: 0.786891  [   32/  126]
train() client id: f_00000-6-1 loss: 0.849885  [   64/  126]
train() client id: f_00000-6-2 loss: 0.877449  [   96/  126]
train() client id: f_00000-7-0 loss: 0.988978  [   32/  126]
train() client id: f_00000-7-1 loss: 0.919578  [   64/  126]
train() client id: f_00000-7-2 loss: 0.826541  [   96/  126]
train() client id: f_00000-8-0 loss: 0.857296  [   32/  126]
train() client id: f_00000-8-1 loss: 0.786212  [   64/  126]
train() client id: f_00000-8-2 loss: 0.872362  [   96/  126]
train() client id: f_00000-9-0 loss: 0.866196  [   32/  126]
train() client id: f_00000-9-1 loss: 0.939142  [   64/  126]
train() client id: f_00000-9-2 loss: 0.884867  [   96/  126]
train() client id: f_00000-10-0 loss: 0.815704  [   32/  126]
train() client id: f_00000-10-1 loss: 0.862962  [   64/  126]
train() client id: f_00000-10-2 loss: 0.862436  [   96/  126]
train() client id: f_00000-11-0 loss: 0.945352  [   32/  126]
train() client id: f_00000-11-1 loss: 0.772971  [   64/  126]
train() client id: f_00000-11-2 loss: 0.778352  [   96/  126]
train() client id: f_00001-0-0 loss: 0.431965  [   32/  265]
train() client id: f_00001-0-1 loss: 0.470174  [   64/  265]
train() client id: f_00001-0-2 loss: 0.387794  [   96/  265]
train() client id: f_00001-0-3 loss: 0.329493  [  128/  265]
train() client id: f_00001-0-4 loss: 0.456446  [  160/  265]
train() client id: f_00001-0-5 loss: 0.460746  [  192/  265]
train() client id: f_00001-0-6 loss: 0.450894  [  224/  265]
train() client id: f_00001-0-7 loss: 0.364286  [  256/  265]
train() client id: f_00001-1-0 loss: 0.441885  [   32/  265]
train() client id: f_00001-1-1 loss: 0.330637  [   64/  265]
train() client id: f_00001-1-2 loss: 0.525865  [   96/  265]
train() client id: f_00001-1-3 loss: 0.568340  [  128/  265]
train() client id: f_00001-1-4 loss: 0.318645  [  160/  265]
train() client id: f_00001-1-5 loss: 0.437196  [  192/  265]
train() client id: f_00001-1-6 loss: 0.325266  [  224/  265]
train() client id: f_00001-1-7 loss: 0.329479  [  256/  265]
train() client id: f_00001-2-0 loss: 0.315546  [   32/  265]
train() client id: f_00001-2-1 loss: 0.330769  [   64/  265]
train() client id: f_00001-2-2 loss: 0.337568  [   96/  265]
train() client id: f_00001-2-3 loss: 0.441787  [  128/  265]
train() client id: f_00001-2-4 loss: 0.386552  [  160/  265]
train() client id: f_00001-2-5 loss: 0.589903  [  192/  265]
train() client id: f_00001-2-6 loss: 0.441461  [  224/  265]
train() client id: f_00001-2-7 loss: 0.357911  [  256/  265]
train() client id: f_00001-3-0 loss: 0.427179  [   32/  265]
train() client id: f_00001-3-1 loss: 0.295246  [   64/  265]
train() client id: f_00001-3-2 loss: 0.436758  [   96/  265]
train() client id: f_00001-3-3 loss: 0.439257  [  128/  265]
train() client id: f_00001-3-4 loss: 0.316841  [  160/  265]
train() client id: f_00001-3-5 loss: 0.552219  [  192/  265]
train() client id: f_00001-3-6 loss: 0.395295  [  224/  265]
train() client id: f_00001-3-7 loss: 0.289070  [  256/  265]
train() client id: f_00001-4-0 loss: 0.352476  [   32/  265]
train() client id: f_00001-4-1 loss: 0.329310  [   64/  265]
train() client id: f_00001-4-2 loss: 0.378947  [   96/  265]
train() client id: f_00001-4-3 loss: 0.318968  [  128/  265]
train() client id: f_00001-4-4 loss: 0.444759  [  160/  265]
train() client id: f_00001-4-5 loss: 0.326480  [  192/  265]
train() client id: f_00001-4-6 loss: 0.364271  [  224/  265]
train() client id: f_00001-4-7 loss: 0.516888  [  256/  265]
train() client id: f_00001-5-0 loss: 0.393548  [   32/  265]
train() client id: f_00001-5-1 loss: 0.327155  [   64/  265]
train() client id: f_00001-5-2 loss: 0.332336  [   96/  265]
train() client id: f_00001-5-3 loss: 0.383461  [  128/  265]
train() client id: f_00001-5-4 loss: 0.507078  [  160/  265]
train() client id: f_00001-5-5 loss: 0.350586  [  192/  265]
train() client id: f_00001-5-6 loss: 0.398041  [  224/  265]
train() client id: f_00001-5-7 loss: 0.404427  [  256/  265]
train() client id: f_00001-6-0 loss: 0.381486  [   32/  265]
train() client id: f_00001-6-1 loss: 0.486212  [   64/  265]
train() client id: f_00001-6-2 loss: 0.419575  [   96/  265]
train() client id: f_00001-6-3 loss: 0.367313  [  128/  265]
train() client id: f_00001-6-4 loss: 0.312183  [  160/  265]
train() client id: f_00001-6-5 loss: 0.477814  [  192/  265]
train() client id: f_00001-6-6 loss: 0.307516  [  224/  265]
train() client id: f_00001-6-7 loss: 0.323326  [  256/  265]
train() client id: f_00001-7-0 loss: 0.390591  [   32/  265]
train() client id: f_00001-7-1 loss: 0.274573  [   64/  265]
train() client id: f_00001-7-2 loss: 0.368842  [   96/  265]
train() client id: f_00001-7-3 loss: 0.307535  [  128/  265]
train() client id: f_00001-7-4 loss: 0.444622  [  160/  265]
train() client id: f_00001-7-5 loss: 0.325012  [  192/  265]
train() client id: f_00001-7-6 loss: 0.366522  [  224/  265]
train() client id: f_00001-7-7 loss: 0.495390  [  256/  265]
train() client id: f_00001-8-0 loss: 0.356999  [   32/  265]
train() client id: f_00001-8-1 loss: 0.436329  [   64/  265]
train() client id: f_00001-8-2 loss: 0.409452  [   96/  265]
train() client id: f_00001-8-3 loss: 0.288434  [  128/  265]
train() client id: f_00001-8-4 loss: 0.369675  [  160/  265]
train() client id: f_00001-8-5 loss: 0.320583  [  192/  265]
train() client id: f_00001-8-6 loss: 0.459062  [  224/  265]
train() client id: f_00001-8-7 loss: 0.398731  [  256/  265]
train() client id: f_00001-9-0 loss: 0.288295  [   32/  265]
train() client id: f_00001-9-1 loss: 0.402561  [   64/  265]
train() client id: f_00001-9-2 loss: 0.373514  [   96/  265]
train() client id: f_00001-9-3 loss: 0.488330  [  128/  265]
train() client id: f_00001-9-4 loss: 0.321491  [  160/  265]
train() client id: f_00001-9-5 loss: 0.279712  [  192/  265]
train() client id: f_00001-9-6 loss: 0.470041  [  224/  265]
train() client id: f_00001-9-7 loss: 0.409691  [  256/  265]
train() client id: f_00001-10-0 loss: 0.305460  [   32/  265]
train() client id: f_00001-10-1 loss: 0.323680  [   64/  265]
train() client id: f_00001-10-2 loss: 0.376309  [   96/  265]
train() client id: f_00001-10-3 loss: 0.281795  [  128/  265]
train() client id: f_00001-10-4 loss: 0.435222  [  160/  265]
train() client id: f_00001-10-5 loss: 0.372414  [  192/  265]
train() client id: f_00001-10-6 loss: 0.411586  [  224/  265]
train() client id: f_00001-10-7 loss: 0.485453  [  256/  265]
train() client id: f_00001-11-0 loss: 0.359824  [   32/  265]
train() client id: f_00001-11-1 loss: 0.291048  [   64/  265]
train() client id: f_00001-11-2 loss: 0.427952  [   96/  265]
train() client id: f_00001-11-3 loss: 0.417816  [  128/  265]
train() client id: f_00001-11-4 loss: 0.393502  [  160/  265]
train() client id: f_00001-11-5 loss: 0.346196  [  192/  265]
train() client id: f_00001-11-6 loss: 0.448681  [  224/  265]
train() client id: f_00001-11-7 loss: 0.272478  [  256/  265]
train() client id: f_00002-0-0 loss: 1.294131  [   32/  124]
train() client id: f_00002-0-1 loss: 1.353852  [   64/  124]
train() client id: f_00002-0-2 loss: 1.300489  [   96/  124]
train() client id: f_00002-1-0 loss: 1.320943  [   32/  124]
train() client id: f_00002-1-1 loss: 1.188826  [   64/  124]
train() client id: f_00002-1-2 loss: 1.318806  [   96/  124]
train() client id: f_00002-2-0 loss: 1.213233  [   32/  124]
train() client id: f_00002-2-1 loss: 1.421685  [   64/  124]
train() client id: f_00002-2-2 loss: 1.070478  [   96/  124]
train() client id: f_00002-3-0 loss: 1.371353  [   32/  124]
train() client id: f_00002-3-1 loss: 1.163016  [   64/  124]
train() client id: f_00002-3-2 loss: 1.253519  [   96/  124]
train() client id: f_00002-4-0 loss: 1.242121  [   32/  124]
train() client id: f_00002-4-1 loss: 1.133636  [   64/  124]
train() client id: f_00002-4-2 loss: 1.352479  [   96/  124]
train() client id: f_00002-5-0 loss: 1.211031  [   32/  124]
train() client id: f_00002-5-1 loss: 1.124781  [   64/  124]
train() client id: f_00002-5-2 loss: 1.092786  [   96/  124]
train() client id: f_00002-6-0 loss: 1.248858  [   32/  124]
train() client id: f_00002-6-1 loss: 1.381209  [   64/  124]
train() client id: f_00002-6-2 loss: 0.998661  [   96/  124]
train() client id: f_00002-7-0 loss: 1.132155  [   32/  124]
train() client id: f_00002-7-1 loss: 1.154917  [   64/  124]
train() client id: f_00002-7-2 loss: 1.104216  [   96/  124]
train() client id: f_00002-8-0 loss: 1.159707  [   32/  124]
train() client id: f_00002-8-1 loss: 1.192154  [   64/  124]
train() client id: f_00002-8-2 loss: 1.208123  [   96/  124]
train() client id: f_00002-9-0 loss: 1.125842  [   32/  124]
train() client id: f_00002-9-1 loss: 1.118278  [   64/  124]
train() client id: f_00002-9-2 loss: 1.143409  [   96/  124]
train() client id: f_00002-10-0 loss: 1.089366  [   32/  124]
train() client id: f_00002-10-1 loss: 0.993479  [   64/  124]
train() client id: f_00002-10-2 loss: 1.168126  [   96/  124]
train() client id: f_00002-11-0 loss: 1.111897  [   32/  124]
train() client id: f_00002-11-1 loss: 1.096511  [   64/  124]
train() client id: f_00002-11-2 loss: 0.999758  [   96/  124]
train() client id: f_00003-0-0 loss: 0.887594  [   32/   43]
train() client id: f_00003-1-0 loss: 1.057146  [   32/   43]
train() client id: f_00003-2-0 loss: 1.086135  [   32/   43]
train() client id: f_00003-3-0 loss: 0.971384  [   32/   43]
train() client id: f_00003-4-0 loss: 1.127309  [   32/   43]
train() client id: f_00003-5-0 loss: 1.119177  [   32/   43]
train() client id: f_00003-6-0 loss: 0.984882  [   32/   43]
train() client id: f_00003-7-0 loss: 0.961121  [   32/   43]
train() client id: f_00003-8-0 loss: 0.844391  [   32/   43]
train() client id: f_00003-9-0 loss: 0.940040  [   32/   43]
train() client id: f_00003-10-0 loss: 1.040620  [   32/   43]
train() client id: f_00003-11-0 loss: 1.031125  [   32/   43]
train() client id: f_00004-0-0 loss: 0.909251  [   32/  306]
train() client id: f_00004-0-1 loss: 0.999983  [   64/  306]
train() client id: f_00004-0-2 loss: 0.917701  [   96/  306]
train() client id: f_00004-0-3 loss: 0.903374  [  128/  306]
train() client id: f_00004-0-4 loss: 0.830660  [  160/  306]
train() client id: f_00004-0-5 loss: 0.904853  [  192/  306]
train() client id: f_00004-0-6 loss: 0.844262  [  224/  306]
train() client id: f_00004-0-7 loss: 0.932866  [  256/  306]
train() client id: f_00004-0-8 loss: 0.907987  [  288/  306]
train() client id: f_00004-1-0 loss: 0.805945  [   32/  306]
train() client id: f_00004-1-1 loss: 0.983287  [   64/  306]
train() client id: f_00004-1-2 loss: 0.943983  [   96/  306]
train() client id: f_00004-1-3 loss: 0.935024  [  128/  306]
train() client id: f_00004-1-4 loss: 0.920155  [  160/  306]
train() client id: f_00004-1-5 loss: 0.954720  [  192/  306]
train() client id: f_00004-1-6 loss: 0.931241  [  224/  306]
train() client id: f_00004-1-7 loss: 0.900253  [  256/  306]
train() client id: f_00004-1-8 loss: 0.860053  [  288/  306]
train() client id: f_00004-2-0 loss: 0.980615  [   32/  306]
train() client id: f_00004-2-1 loss: 0.851407  [   64/  306]
train() client id: f_00004-2-2 loss: 0.943475  [   96/  306]
train() client id: f_00004-2-3 loss: 0.816170  [  128/  306]
train() client id: f_00004-2-4 loss: 0.851705  [  160/  306]
train() client id: f_00004-2-5 loss: 0.936506  [  192/  306]
train() client id: f_00004-2-6 loss: 0.877136  [  224/  306]
train() client id: f_00004-2-7 loss: 1.002862  [  256/  306]
train() client id: f_00004-2-8 loss: 0.931009  [  288/  306]
train() client id: f_00004-3-0 loss: 0.837068  [   32/  306]
train() client id: f_00004-3-1 loss: 0.980611  [   64/  306]
train() client id: f_00004-3-2 loss: 0.907326  [   96/  306]
train() client id: f_00004-3-3 loss: 0.857382  [  128/  306]
train() client id: f_00004-3-4 loss: 0.954096  [  160/  306]
train() client id: f_00004-3-5 loss: 0.953960  [  192/  306]
train() client id: f_00004-3-6 loss: 0.860922  [  224/  306]
train() client id: f_00004-3-7 loss: 0.909328  [  256/  306]
train() client id: f_00004-3-8 loss: 0.930784  [  288/  306]
train() client id: f_00004-4-0 loss: 0.903587  [   32/  306]
train() client id: f_00004-4-1 loss: 0.850974  [   64/  306]
train() client id: f_00004-4-2 loss: 0.960862  [   96/  306]
train() client id: f_00004-4-3 loss: 0.878443  [  128/  306]
train() client id: f_00004-4-4 loss: 1.043832  [  160/  306]
train() client id: f_00004-4-5 loss: 0.926617  [  192/  306]
train() client id: f_00004-4-6 loss: 0.920083  [  224/  306]
train() client id: f_00004-4-7 loss: 0.756745  [  256/  306]
train() client id: f_00004-4-8 loss: 0.829433  [  288/  306]
train() client id: f_00004-5-0 loss: 0.903039  [   32/  306]
train() client id: f_00004-5-1 loss: 0.902123  [   64/  306]
train() client id: f_00004-5-2 loss: 0.894683  [   96/  306]
train() client id: f_00004-5-3 loss: 0.931197  [  128/  306]
train() client id: f_00004-5-4 loss: 0.952700  [  160/  306]
train() client id: f_00004-5-5 loss: 0.832935  [  192/  306]
train() client id: f_00004-5-6 loss: 0.907735  [  224/  306]
train() client id: f_00004-5-7 loss: 0.827349  [  256/  306]
train() client id: f_00004-5-8 loss: 0.876654  [  288/  306]
train() client id: f_00004-6-0 loss: 0.980651  [   32/  306]
train() client id: f_00004-6-1 loss: 0.955347  [   64/  306]
train() client id: f_00004-6-2 loss: 0.872222  [   96/  306]
train() client id: f_00004-6-3 loss: 0.963004  [  128/  306]
train() client id: f_00004-6-4 loss: 0.851154  [  160/  306]
train() client id: f_00004-6-5 loss: 0.885530  [  192/  306]
train() client id: f_00004-6-6 loss: 0.710502  [  224/  306]
train() client id: f_00004-6-7 loss: 0.841388  [  256/  306]
train() client id: f_00004-6-8 loss: 1.034409  [  288/  306]
train() client id: f_00004-7-0 loss: 0.776943  [   32/  306]
train() client id: f_00004-7-1 loss: 0.807069  [   64/  306]
train() client id: f_00004-7-2 loss: 0.943812  [   96/  306]
train() client id: f_00004-7-3 loss: 1.029296  [  128/  306]
train() client id: f_00004-7-4 loss: 0.904051  [  160/  306]
train() client id: f_00004-7-5 loss: 0.886665  [  192/  306]
train() client id: f_00004-7-6 loss: 0.860936  [  224/  306]
train() client id: f_00004-7-7 loss: 0.881511  [  256/  306]
train() client id: f_00004-7-8 loss: 0.914480  [  288/  306]
train() client id: f_00004-8-0 loss: 0.870395  [   32/  306]
train() client id: f_00004-8-1 loss: 0.930551  [   64/  306]
train() client id: f_00004-8-2 loss: 1.067139  [   96/  306]
train() client id: f_00004-8-3 loss: 0.949170  [  128/  306]
train() client id: f_00004-8-4 loss: 0.961028  [  160/  306]
train() client id: f_00004-8-5 loss: 0.861682  [  192/  306]
train() client id: f_00004-8-6 loss: 0.839682  [  224/  306]
train() client id: f_00004-8-7 loss: 0.847583  [  256/  306]
train() client id: f_00004-8-8 loss: 0.780972  [  288/  306]
train() client id: f_00004-9-0 loss: 0.854813  [   32/  306]
train() client id: f_00004-9-1 loss: 0.993101  [   64/  306]
train() client id: f_00004-9-2 loss: 0.777789  [   96/  306]
train() client id: f_00004-9-3 loss: 0.966949  [  128/  306]
train() client id: f_00004-9-4 loss: 0.957620  [  160/  306]
train() client id: f_00004-9-5 loss: 0.745677  [  192/  306]
train() client id: f_00004-9-6 loss: 0.915288  [  224/  306]
train() client id: f_00004-9-7 loss: 0.837736  [  256/  306]
train() client id: f_00004-9-8 loss: 0.962330  [  288/  306]
train() client id: f_00004-10-0 loss: 0.907360  [   32/  306]
train() client id: f_00004-10-1 loss: 0.951270  [   64/  306]
train() client id: f_00004-10-2 loss: 0.905448  [   96/  306]
train() client id: f_00004-10-3 loss: 1.051507  [  128/  306]
train() client id: f_00004-10-4 loss: 0.857558  [  160/  306]
train() client id: f_00004-10-5 loss: 0.786818  [  192/  306]
train() client id: f_00004-10-6 loss: 0.884521  [  224/  306]
train() client id: f_00004-10-7 loss: 0.865106  [  256/  306]
train() client id: f_00004-10-8 loss: 0.812633  [  288/  306]
train() client id: f_00004-11-0 loss: 0.971662  [   32/  306]
train() client id: f_00004-11-1 loss: 0.815323  [   64/  306]
train() client id: f_00004-11-2 loss: 0.841349  [   96/  306]
train() client id: f_00004-11-3 loss: 0.950274  [  128/  306]
train() client id: f_00004-11-4 loss: 0.914222  [  160/  306]
train() client id: f_00004-11-5 loss: 0.899177  [  192/  306]
train() client id: f_00004-11-6 loss: 0.902313  [  224/  306]
train() client id: f_00004-11-7 loss: 0.845420  [  256/  306]
train() client id: f_00004-11-8 loss: 0.927733  [  288/  306]
train() client id: f_00005-0-0 loss: 0.797362  [   32/  146]
train() client id: f_00005-0-1 loss: 0.703854  [   64/  146]
train() client id: f_00005-0-2 loss: 0.997453  [   96/  146]
train() client id: f_00005-0-3 loss: 0.646231  [  128/  146]
train() client id: f_00005-1-0 loss: 0.830139  [   32/  146]
train() client id: f_00005-1-1 loss: 0.732104  [   64/  146]
train() client id: f_00005-1-2 loss: 0.775713  [   96/  146]
train() client id: f_00005-1-3 loss: 0.919052  [  128/  146]
train() client id: f_00005-2-0 loss: 0.805873  [   32/  146]
train() client id: f_00005-2-1 loss: 0.609930  [   64/  146]
train() client id: f_00005-2-2 loss: 0.863360  [   96/  146]
train() client id: f_00005-2-3 loss: 0.825024  [  128/  146]
train() client id: f_00005-3-0 loss: 0.759832  [   32/  146]
train() client id: f_00005-3-1 loss: 0.848930  [   64/  146]
train() client id: f_00005-3-2 loss: 0.932770  [   96/  146]
train() client id: f_00005-3-3 loss: 0.678866  [  128/  146]
train() client id: f_00005-4-0 loss: 0.887012  [   32/  146]
train() client id: f_00005-4-1 loss: 0.919473  [   64/  146]
train() client id: f_00005-4-2 loss: 0.714718  [   96/  146]
train() client id: f_00005-4-3 loss: 0.753802  [  128/  146]
train() client id: f_00005-5-0 loss: 1.117082  [   32/  146]
train() client id: f_00005-5-1 loss: 0.662744  [   64/  146]
train() client id: f_00005-5-2 loss: 0.780900  [   96/  146]
train() client id: f_00005-5-3 loss: 0.769017  [  128/  146]
train() client id: f_00005-6-0 loss: 1.065370  [   32/  146]
train() client id: f_00005-6-1 loss: 0.516771  [   64/  146]
train() client id: f_00005-6-2 loss: 0.955825  [   96/  146]
train() client id: f_00005-6-3 loss: 0.637951  [  128/  146]
train() client id: f_00005-7-0 loss: 0.828365  [   32/  146]
train() client id: f_00005-7-1 loss: 0.704230  [   64/  146]
train() client id: f_00005-7-2 loss: 0.937276  [   96/  146]
train() client id: f_00005-7-3 loss: 0.644301  [  128/  146]
train() client id: f_00005-8-0 loss: 0.972388  [   32/  146]
train() client id: f_00005-8-1 loss: 0.818347  [   64/  146]
train() client id: f_00005-8-2 loss: 0.812221  [   96/  146]
train() client id: f_00005-8-3 loss: 0.640780  [  128/  146]
train() client id: f_00005-9-0 loss: 0.882037  [   32/  146]
train() client id: f_00005-9-1 loss: 0.754773  [   64/  146]
train() client id: f_00005-9-2 loss: 0.914419  [   96/  146]
train() client id: f_00005-9-3 loss: 0.653853  [  128/  146]
train() client id: f_00005-10-0 loss: 0.875313  [   32/  146]
train() client id: f_00005-10-1 loss: 0.866266  [   64/  146]
train() client id: f_00005-10-2 loss: 0.914730  [   96/  146]
train() client id: f_00005-10-3 loss: 0.679941  [  128/  146]
train() client id: f_00005-11-0 loss: 0.793102  [   32/  146]
train() client id: f_00005-11-1 loss: 0.843577  [   64/  146]
train() client id: f_00005-11-2 loss: 0.743698  [   96/  146]
train() client id: f_00005-11-3 loss: 0.856056  [  128/  146]
train() client id: f_00006-0-0 loss: 0.522669  [   32/   54]
train() client id: f_00006-1-0 loss: 0.520325  [   32/   54]
train() client id: f_00006-2-0 loss: 0.534551  [   32/   54]
train() client id: f_00006-3-0 loss: 0.579285  [   32/   54]
train() client id: f_00006-4-0 loss: 0.523550  [   32/   54]
train() client id: f_00006-5-0 loss: 0.549373  [   32/   54]
train() client id: f_00006-6-0 loss: 0.539300  [   32/   54]
train() client id: f_00006-7-0 loss: 0.514249  [   32/   54]
train() client id: f_00006-8-0 loss: 0.550931  [   32/   54]
train() client id: f_00006-9-0 loss: 0.549166  [   32/   54]
train() client id: f_00006-10-0 loss: 0.580182  [   32/   54]
train() client id: f_00006-11-0 loss: 0.573746  [   32/   54]
train() client id: f_00007-0-0 loss: 0.384064  [   32/  179]
train() client id: f_00007-0-1 loss: 0.736286  [   64/  179]
train() client id: f_00007-0-2 loss: 0.350257  [   96/  179]
train() client id: f_00007-0-3 loss: 0.405900  [  128/  179]
train() client id: f_00007-0-4 loss: 0.528249  [  160/  179]
train() client id: f_00007-1-0 loss: 0.703041  [   32/  179]
train() client id: f_00007-1-1 loss: 0.373869  [   64/  179]
train() client id: f_00007-1-2 loss: 0.552618  [   96/  179]
train() client id: f_00007-1-3 loss: 0.423200  [  128/  179]
train() client id: f_00007-1-4 loss: 0.309371  [  160/  179]
train() client id: f_00007-2-0 loss: 0.490112  [   32/  179]
train() client id: f_00007-2-1 loss: 0.453151  [   64/  179]
train() client id: f_00007-2-2 loss: 0.363847  [   96/  179]
train() client id: f_00007-2-3 loss: 0.436907  [  128/  179]
train() client id: f_00007-2-4 loss: 0.421887  [  160/  179]
train() client id: f_00007-3-0 loss: 0.320295  [   32/  179]
train() client id: f_00007-3-1 loss: 0.382248  [   64/  179]
train() client id: f_00007-3-2 loss: 0.613128  [   96/  179]
train() client id: f_00007-3-3 loss: 0.295276  [  128/  179]
train() client id: f_00007-3-4 loss: 0.397461  [  160/  179]
train() client id: f_00007-4-0 loss: 0.385804  [   32/  179]
train() client id: f_00007-4-1 loss: 0.467949  [   64/  179]
train() client id: f_00007-4-2 loss: 0.493524  [   96/  179]
train() client id: f_00007-4-3 loss: 0.451910  [  128/  179]
train() client id: f_00007-4-4 loss: 0.463913  [  160/  179]
train() client id: f_00007-5-0 loss: 0.636959  [   32/  179]
train() client id: f_00007-5-1 loss: 0.399895  [   64/  179]
train() client id: f_00007-5-2 loss: 0.308871  [   96/  179]
train() client id: f_00007-5-3 loss: 0.386853  [  128/  179]
train() client id: f_00007-5-4 loss: 0.505706  [  160/  179]
train() client id: f_00007-6-0 loss: 0.382864  [   32/  179]
train() client id: f_00007-6-1 loss: 0.558322  [   64/  179]
train() client id: f_00007-6-2 loss: 0.328873  [   96/  179]
train() client id: f_00007-6-3 loss: 0.420103  [  128/  179]
train() client id: f_00007-6-4 loss: 0.461585  [  160/  179]
train() client id: f_00007-7-0 loss: 0.384376  [   32/  179]
train() client id: f_00007-7-1 loss: 0.624299  [   64/  179]
train() client id: f_00007-7-2 loss: 0.351952  [   96/  179]
train() client id: f_00007-7-3 loss: 0.350686  [  128/  179]
train() client id: f_00007-7-4 loss: 0.514874  [  160/  179]
train() client id: f_00007-8-0 loss: 0.284477  [   32/  179]
train() client id: f_00007-8-1 loss: 0.608974  [   64/  179]
train() client id: f_00007-8-2 loss: 0.385514  [   96/  179]
train() client id: f_00007-8-3 loss: 0.546744  [  128/  179]
train() client id: f_00007-8-4 loss: 0.383984  [  160/  179]
train() client id: f_00007-9-0 loss: 0.354200  [   32/  179]
train() client id: f_00007-9-1 loss: 0.515500  [   64/  179]
train() client id: f_00007-9-2 loss: 0.374263  [   96/  179]
train() client id: f_00007-9-3 loss: 0.482258  [  128/  179]
train() client id: f_00007-9-4 loss: 0.289438  [  160/  179]
train() client id: f_00007-10-0 loss: 0.279379  [   32/  179]
train() client id: f_00007-10-1 loss: 0.370151  [   64/  179]
train() client id: f_00007-10-2 loss: 0.435101  [   96/  179]
train() client id: f_00007-10-3 loss: 0.381949  [  128/  179]
train() client id: f_00007-10-4 loss: 0.718796  [  160/  179]
train() client id: f_00007-11-0 loss: 0.619576  [   32/  179]
train() client id: f_00007-11-1 loss: 0.277993  [   64/  179]
train() client id: f_00007-11-2 loss: 0.281896  [   96/  179]
train() client id: f_00007-11-3 loss: 0.494820  [  128/  179]
train() client id: f_00007-11-4 loss: 0.354751  [  160/  179]
train() client id: f_00008-0-0 loss: 0.768422  [   32/  130]
train() client id: f_00008-0-1 loss: 0.781753  [   64/  130]
train() client id: f_00008-0-2 loss: 0.748646  [   96/  130]
train() client id: f_00008-0-3 loss: 0.658647  [  128/  130]
train() client id: f_00008-1-0 loss: 0.714368  [   32/  130]
train() client id: f_00008-1-1 loss: 0.707434  [   64/  130]
train() client id: f_00008-1-2 loss: 0.755259  [   96/  130]
train() client id: f_00008-1-3 loss: 0.777125  [  128/  130]
train() client id: f_00008-2-0 loss: 0.718175  [   32/  130]
train() client id: f_00008-2-1 loss: 0.725386  [   64/  130]
train() client id: f_00008-2-2 loss: 0.746956  [   96/  130]
train() client id: f_00008-2-3 loss: 0.719293  [  128/  130]
train() client id: f_00008-3-0 loss: 0.869766  [   32/  130]
train() client id: f_00008-3-1 loss: 0.685824  [   64/  130]
train() client id: f_00008-3-2 loss: 0.714569  [   96/  130]
train() client id: f_00008-3-3 loss: 0.676682  [  128/  130]
train() client id: f_00008-4-0 loss: 0.626601  [   32/  130]
train() client id: f_00008-4-1 loss: 0.680095  [   64/  130]
train() client id: f_00008-4-2 loss: 0.768273  [   96/  130]
train() client id: f_00008-4-3 loss: 0.874351  [  128/  130]
train() client id: f_00008-5-0 loss: 0.739843  [   32/  130]
train() client id: f_00008-5-1 loss: 0.738215  [   64/  130]
train() client id: f_00008-5-2 loss: 0.803999  [   96/  130]
train() client id: f_00008-5-3 loss: 0.660534  [  128/  130]
train() client id: f_00008-6-0 loss: 0.820644  [   32/  130]
train() client id: f_00008-6-1 loss: 0.748314  [   64/  130]
train() client id: f_00008-6-2 loss: 0.584173  [   96/  130]
train() client id: f_00008-6-3 loss: 0.744424  [  128/  130]
train() client id: f_00008-7-0 loss: 0.623458  [   32/  130]
train() client id: f_00008-7-1 loss: 0.743465  [   64/  130]
train() client id: f_00008-7-2 loss: 0.810605  [   96/  130]
train() client id: f_00008-7-3 loss: 0.749839  [  128/  130]
train() client id: f_00008-8-0 loss: 0.696655  [   32/  130]
train() client id: f_00008-8-1 loss: 0.745672  [   64/  130]
train() client id: f_00008-8-2 loss: 0.807395  [   96/  130]
train() client id: f_00008-8-3 loss: 0.695708  [  128/  130]
train() client id: f_00008-9-0 loss: 0.708882  [   32/  130]
train() client id: f_00008-9-1 loss: 0.829968  [   64/  130]
train() client id: f_00008-9-2 loss: 0.697750  [   96/  130]
train() client id: f_00008-9-3 loss: 0.695206  [  128/  130]
train() client id: f_00008-10-0 loss: 0.746845  [   32/  130]
train() client id: f_00008-10-1 loss: 0.698690  [   64/  130]
train() client id: f_00008-10-2 loss: 0.704158  [   96/  130]
train() client id: f_00008-10-3 loss: 0.757709  [  128/  130]
train() client id: f_00008-11-0 loss: 0.675481  [   32/  130]
train() client id: f_00008-11-1 loss: 0.686363  [   64/  130]
train() client id: f_00008-11-2 loss: 0.763100  [   96/  130]
train() client id: f_00008-11-3 loss: 0.815823  [  128/  130]
train() client id: f_00009-0-0 loss: 0.926450  [   32/  118]
train() client id: f_00009-0-1 loss: 1.032319  [   64/  118]
train() client id: f_00009-0-2 loss: 0.880646  [   96/  118]
train() client id: f_00009-1-0 loss: 0.971500  [   32/  118]
train() client id: f_00009-1-1 loss: 0.774509  [   64/  118]
train() client id: f_00009-1-2 loss: 0.957156  [   96/  118]
train() client id: f_00009-2-0 loss: 0.830124  [   32/  118]
train() client id: f_00009-2-1 loss: 0.941551  [   64/  118]
train() client id: f_00009-2-2 loss: 0.789240  [   96/  118]
train() client id: f_00009-3-0 loss: 0.799328  [   32/  118]
train() client id: f_00009-3-1 loss: 0.823481  [   64/  118]
train() client id: f_00009-3-2 loss: 0.787938  [   96/  118]
train() client id: f_00009-4-0 loss: 0.755340  [   32/  118]
train() client id: f_00009-4-1 loss: 0.800203  [   64/  118]
train() client id: f_00009-4-2 loss: 0.863908  [   96/  118]
train() client id: f_00009-5-0 loss: 0.919136  [   32/  118]
train() client id: f_00009-5-1 loss: 0.649659  [   64/  118]
train() client id: f_00009-5-2 loss: 0.727692  [   96/  118]
train() client id: f_00009-6-0 loss: 0.736411  [   32/  118]
train() client id: f_00009-6-1 loss: 0.815854  [   64/  118]
train() client id: f_00009-6-2 loss: 0.814306  [   96/  118]
train() client id: f_00009-7-0 loss: 0.774865  [   32/  118]
train() client id: f_00009-7-1 loss: 0.824663  [   64/  118]
train() client id: f_00009-7-2 loss: 0.674542  [   96/  118]
train() client id: f_00009-8-0 loss: 0.742629  [   32/  118]
train() client id: f_00009-8-1 loss: 0.819067  [   64/  118]
train() client id: f_00009-8-2 loss: 0.749676  [   96/  118]
train() client id: f_00009-9-0 loss: 0.683534  [   32/  118]
train() client id: f_00009-9-1 loss: 0.811166  [   64/  118]
train() client id: f_00009-9-2 loss: 0.680993  [   96/  118]
train() client id: f_00009-10-0 loss: 0.649326  [   32/  118]
train() client id: f_00009-10-1 loss: 0.759519  [   64/  118]
train() client id: f_00009-10-2 loss: 0.838638  [   96/  118]
train() client id: f_00009-11-0 loss: 0.843996  [   32/  118]
train() client id: f_00009-11-1 loss: 0.764444  [   64/  118]
train() client id: f_00009-11-2 loss: 0.742319  [   96/  118]
At round 26 accuracy: 0.6472148541114059
At round 26 training accuracy: 0.5841716968477532
At round 26 training loss: 0.8257482493450757
update_location
xs = -4.528292 31.001589 40.045120 -40.943528 -60.103519 4.782049 -82.215960 103.375741 -1.680116 24.695607 
ys = 117.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 -3.124922 -19.177652 -47.154970 4.001482 
xs mean: 1.4428691594128664
ys mean: 8.371751218646876
dists_uav = 154.426143 105.844616 107.728156 110.384064 117.046397 101.578646 129.496058 145.101089 110.573116 103.081933 
uav_gains = -104.724689 -100.616755 -100.808277 -101.072725 -101.709102 -100.170080 -102.807034 -104.044694 -101.091306 -100.329588 
uav_gains_db_mean: -101.73742503818183
dists_bs = 179.879809 260.497784 276.421657 203.011621 201.596285 263.167220 200.841644 339.408329 281.766948 262.904600 
bs_gains = -102.706567 -107.209572 -107.931077 -104.177644 -104.092570 -107.333549 -104.046965 -110.427289 -108.163980 -107.321408 
bs_gains_db_mean: -106.34106218519779
Round 27
-------------------------------
ene_coms = [0.00783372 0.00897006 0.00652111 0.00659594 0.00758798 0.00903511
 0.00713203 0.00756998 0.0094952  0.0090287 ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.41751033 15.45677365  7.27704635  2.60639148 17.79522439  8.59408865
  3.25112498 10.46951046  7.68000672  6.97892392]
obj_prev = 87.52660093030741
eta_min = 3.4744945406592995e-13	eta_max = 0.9363235454906789
af = 18.488129690050464	bf = 1.510351277702705	zeta = 20.336942659055513	eta = 0.909090909090909
af = 18.488129690050464	bf = 1.510351277702705	zeta = 35.99704066977797	eta = 0.5136013779480639
af = 18.488129690050464	bf = 1.510351277702705	zeta = 28.431694660211672	eta = 0.6502647805909161
af = 18.488129690050464	bf = 1.510351277702705	zeta = 27.069598597056356	eta = 0.6829849960191477
af = 18.488129690050464	bf = 1.510351277702705	zeta = 27.000455490294694	eta = 0.6847339926060142
af = 18.488129690050464	bf = 1.510351277702705	zeta = 27.00026386170231	eta = 0.6847388523589348
eta = 0.6847388523589348
ene_coms = [0.00783372 0.00897006 0.00652111 0.00659594 0.00758798 0.00903511
 0.00713203 0.00756998 0.0094952  0.0090287 ]
ene_comp = [0.03125088 0.06572606 0.03075484 0.01066498 0.075895   0.03621134
 0.01339323 0.04439609 0.03224297 0.0292667 ]
ene_total = [2.3473317  4.48607851 2.23870794 1.03665192 5.0137973  2.71739843
 1.23270044 3.12096398 2.50669944 2.29993421]
ti_comp = [0.39870206 0.38733861 0.41182816 0.41107977 0.40115938 0.38668812
 0.40571894 0.40133938 0.38208721 0.38675222]
ti_coms = [0.07833716 0.08970061 0.06521106 0.06595944 0.07587984 0.0903511
 0.07132028 0.07569984 0.09495201 0.090287  ]
t_total = [28.6498867 28.6498867 28.6498867 28.6498867 28.6498867 28.6498867
 28.6498867 28.6498867 28.6498867 28.6498867]
ene_coms = [0.00783372 0.00897006 0.00652111 0.00659594 0.00758798 0.00903511
 0.00713203 0.00756998 0.0094952  0.0090287 ]
ene_comp = [1.19996854e-05 1.18279976e-04 1.07198362e-05 4.48650678e-07
 1.69779622e-04 1.98468731e-05 9.12191244e-07 3.39539831e-05
 1.43502755e-05 1.04745548e-05]
ene_total = [0.47119578 0.54582502 0.39228653 0.39616431 0.46591358 0.54382005
 0.42838811 0.45667517 0.57112194 0.54287219]
optimize_network iter = 0 obj = 4.814262693238813
eta = 0.6847388523589348
freqs = [39190769.72874343 84843155.38341507 37339402.84350059 12971914.31758976
 94594568.97574641 46822408.9260356  16505557.76627725 55309914.06061858
 42193211.09846129 37836496.70621367]
eta_min = 0.6847388523589386	eta_max = 0.6847388523589252
af = 0.01953619548301387	bf = 1.510351277702705	zeta = 0.021489815031315262	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00783372 0.00897006 0.00652111 0.00659594 0.00758798 0.00903511
 0.00713203 0.00756998 0.0094952  0.0090287 ]
ene_comp = [3.01990644e-06 2.97669854e-05 2.69781260e-06 1.12909882e-07
 4.27276678e-05 4.99477259e-06 2.29567036e-07 8.54504502e-06
 3.61146879e-06 2.63608373e-06]
ene_total = [1.63217672 1.87441683 1.35872895 1.37377745 1.58926753 1.88280537
 1.48545323 1.57839933 1.97834156 1.88097907]
ti_comp = [0.39870206 0.38733861 0.41182816 0.41107977 0.40115938 0.38668812
 0.40571894 0.40133938 0.38208721 0.38675222]
ti_coms = [0.07833716 0.08970061 0.06521106 0.06595944 0.07587984 0.0903511
 0.07132028 0.07569984 0.09495201 0.090287  ]
t_total = [28.6498867 28.6498867 28.6498867 28.6498867 28.6498867 28.6498867
 28.6498867 28.6498867 28.6498867 28.6498867]
ene_coms = [0.00783372 0.00897006 0.00652111 0.00659594 0.00758798 0.00903511
 0.00713203 0.00756998 0.0094952  0.0090287 ]
ene_comp = [1.19996854e-05 1.18279976e-04 1.07198362e-05 4.48650678e-07
 1.69779622e-04 1.98468731e-05 9.12191244e-07 3.39539831e-05
 1.43502755e-05 1.04745548e-05]
ene_total = [0.47119578 0.54582502 0.39228653 0.39616431 0.46591358 0.54382005
 0.42838811 0.45667517 0.57112194 0.54287219]
optimize_network iter = 1 obj = 4.814262693238668
eta = 0.6847388523589252
freqs = [39190769.72874345 84843155.38341506 37339402.84350066 12971914.31758978
 94594568.9757465  46822408.92603558 16505557.76627727 55309914.06061863
 42193211.09846126 37836496.70621365]
Done!
ene_coms = [0.00783372 0.00897006 0.00652111 0.00659594 0.00758798 0.00903511
 0.00713203 0.00756998 0.0094952  0.0090287 ]
ene_comp = [1.16115282e-05 1.14453940e-04 1.03730787e-05 4.34138048e-07
 1.64287712e-04 1.92048807e-05 8.82684336e-07 3.28556640e-05
 1.38860831e-05 1.01357314e-05]
ene_total = [0.00784533 0.00908452 0.00653148 0.00659638 0.00775227 0.00905431
 0.00713291 0.00760284 0.00950909 0.00903884]
At round 27 energy consumption: 0.08014796000212354
At round 27 eta: 0.6847388523589252
At round 27 a_n: 18.93386498808281
At round 27 local rounds: 12.401143279474164
At round 27 global rounds: 60.057717640611514
gradient difference: 0.4881797730922699
train() client id: f_00000-0-0 loss: 1.117962  [   32/  126]
train() client id: f_00000-0-1 loss: 1.066422  [   64/  126]
train() client id: f_00000-0-2 loss: 1.178397  [   96/  126]
train() client id: f_00000-1-0 loss: 1.018177  [   32/  126]
train() client id: f_00000-1-1 loss: 1.137086  [   64/  126]
train() client id: f_00000-1-2 loss: 1.005354  [   96/  126]
train() client id: f_00000-2-0 loss: 0.965452  [   32/  126]
train() client id: f_00000-2-1 loss: 1.086517  [   64/  126]
train() client id: f_00000-2-2 loss: 0.942172  [   96/  126]
train() client id: f_00000-3-0 loss: 0.991497  [   32/  126]
train() client id: f_00000-3-1 loss: 0.992986  [   64/  126]
train() client id: f_00000-3-2 loss: 0.954404  [   96/  126]
train() client id: f_00000-4-0 loss: 1.097407  [   32/  126]
train() client id: f_00000-4-1 loss: 0.942177  [   64/  126]
train() client id: f_00000-4-2 loss: 0.882992  [   96/  126]
train() client id: f_00000-5-0 loss: 1.056301  [   32/  126]
train() client id: f_00000-5-1 loss: 0.862363  [   64/  126]
train() client id: f_00000-5-2 loss: 0.944016  [   96/  126]
train() client id: f_00000-6-0 loss: 0.959754  [   32/  126]
train() client id: f_00000-6-1 loss: 0.954077  [   64/  126]
train() client id: f_00000-6-2 loss: 0.937404  [   96/  126]
train() client id: f_00000-7-0 loss: 0.974171  [   32/  126]
train() client id: f_00000-7-1 loss: 0.902351  [   64/  126]
train() client id: f_00000-7-2 loss: 0.921424  [   96/  126]
train() client id: f_00000-8-0 loss: 0.909445  [   32/  126]
train() client id: f_00000-8-1 loss: 0.889221  [   64/  126]
train() client id: f_00000-8-2 loss: 0.959191  [   96/  126]
train() client id: f_00000-9-0 loss: 0.878788  [   32/  126]
train() client id: f_00000-9-1 loss: 0.958679  [   64/  126]
train() client id: f_00000-9-2 loss: 0.856084  [   96/  126]
train() client id: f_00000-10-0 loss: 0.971058  [   32/  126]
train() client id: f_00000-10-1 loss: 0.860126  [   64/  126]
train() client id: f_00000-10-2 loss: 0.910505  [   96/  126]
train() client id: f_00000-11-0 loss: 0.872706  [   32/  126]
train() client id: f_00000-11-1 loss: 1.000071  [   64/  126]
train() client id: f_00000-11-2 loss: 0.828926  [   96/  126]
train() client id: f_00001-0-0 loss: 0.554644  [   32/  265]
train() client id: f_00001-0-1 loss: 0.423686  [   64/  265]
train() client id: f_00001-0-2 loss: 0.586789  [   96/  265]
train() client id: f_00001-0-3 loss: 0.536852  [  128/  265]
train() client id: f_00001-0-4 loss: 0.461646  [  160/  265]
train() client id: f_00001-0-5 loss: 0.544381  [  192/  265]
train() client id: f_00001-0-6 loss: 0.583293  [  224/  265]
train() client id: f_00001-0-7 loss: 0.416086  [  256/  265]
train() client id: f_00001-1-0 loss: 0.484770  [   32/  265]
train() client id: f_00001-1-1 loss: 0.489802  [   64/  265]
train() client id: f_00001-1-2 loss: 0.576683  [   96/  265]
train() client id: f_00001-1-3 loss: 0.413837  [  128/  265]
train() client id: f_00001-1-4 loss: 0.573140  [  160/  265]
train() client id: f_00001-1-5 loss: 0.626358  [  192/  265]
train() client id: f_00001-1-6 loss: 0.434645  [  224/  265]
train() client id: f_00001-1-7 loss: 0.468258  [  256/  265]
train() client id: f_00001-2-0 loss: 0.482252  [   32/  265]
train() client id: f_00001-2-1 loss: 0.405933  [   64/  265]
train() client id: f_00001-2-2 loss: 0.504869  [   96/  265]
train() client id: f_00001-2-3 loss: 0.482612  [  128/  265]
train() client id: f_00001-2-4 loss: 0.499196  [  160/  265]
train() client id: f_00001-2-5 loss: 0.419787  [  192/  265]
train() client id: f_00001-2-6 loss: 0.581971  [  224/  265]
train() client id: f_00001-2-7 loss: 0.538768  [  256/  265]
train() client id: f_00001-3-0 loss: 0.555811  [   32/  265]
train() client id: f_00001-3-1 loss: 0.489020  [   64/  265]
train() client id: f_00001-3-2 loss: 0.552191  [   96/  265]
train() client id: f_00001-3-3 loss: 0.544686  [  128/  265]
train() client id: f_00001-3-4 loss: 0.401424  [  160/  265]
train() client id: f_00001-3-5 loss: 0.546295  [  192/  265]
train() client id: f_00001-3-6 loss: 0.410252  [  224/  265]
train() client id: f_00001-3-7 loss: 0.501502  [  256/  265]
train() client id: f_00001-4-0 loss: 0.505243  [   32/  265]
train() client id: f_00001-4-1 loss: 0.488904  [   64/  265]
train() client id: f_00001-4-2 loss: 0.510912  [   96/  265]
train() client id: f_00001-4-3 loss: 0.483834  [  128/  265]
train() client id: f_00001-4-4 loss: 0.468110  [  160/  265]
train() client id: f_00001-4-5 loss: 0.545218  [  192/  265]
train() client id: f_00001-4-6 loss: 0.381530  [  224/  265]
train() client id: f_00001-4-7 loss: 0.571281  [  256/  265]
train() client id: f_00001-5-0 loss: 0.515806  [   32/  265]
train() client id: f_00001-5-1 loss: 0.481542  [   64/  265]
train() client id: f_00001-5-2 loss: 0.523365  [   96/  265]
train() client id: f_00001-5-3 loss: 0.546004  [  128/  265]
train() client id: f_00001-5-4 loss: 0.475894  [  160/  265]
train() client id: f_00001-5-5 loss: 0.541478  [  192/  265]
train() client id: f_00001-5-6 loss: 0.398051  [  224/  265]
train() client id: f_00001-5-7 loss: 0.452473  [  256/  265]
train() client id: f_00001-6-0 loss: 0.483342  [   32/  265]
train() client id: f_00001-6-1 loss: 0.501296  [   64/  265]
train() client id: f_00001-6-2 loss: 0.469382  [   96/  265]
train() client id: f_00001-6-3 loss: 0.507657  [  128/  265]
train() client id: f_00001-6-4 loss: 0.569629  [  160/  265]
train() client id: f_00001-6-5 loss: 0.459997  [  192/  265]
train() client id: f_00001-6-6 loss: 0.500025  [  224/  265]
train() client id: f_00001-6-7 loss: 0.447433  [  256/  265]
train() client id: f_00001-7-0 loss: 0.475339  [   32/  265]
train() client id: f_00001-7-1 loss: 0.546510  [   64/  265]
train() client id: f_00001-7-2 loss: 0.511202  [   96/  265]
train() client id: f_00001-7-3 loss: 0.397063  [  128/  265]
train() client id: f_00001-7-4 loss: 0.477687  [  160/  265]
train() client id: f_00001-7-5 loss: 0.516360  [  192/  265]
train() client id: f_00001-7-6 loss: 0.501961  [  224/  265]
train() client id: f_00001-7-7 loss: 0.507751  [  256/  265]
train() client id: f_00001-8-0 loss: 0.410265  [   32/  265]
train() client id: f_00001-8-1 loss: 0.571766  [   64/  265]
train() client id: f_00001-8-2 loss: 0.450536  [   96/  265]
train() client id: f_00001-8-3 loss: 0.397930  [  128/  265]
train() client id: f_00001-8-4 loss: 0.549115  [  160/  265]
train() client id: f_00001-8-5 loss: 0.519676  [  192/  265]
train() client id: f_00001-8-6 loss: 0.641681  [  224/  265]
train() client id: f_00001-8-7 loss: 0.396824  [  256/  265]
train() client id: f_00001-9-0 loss: 0.500960  [   32/  265]
train() client id: f_00001-9-1 loss: 0.405577  [   64/  265]
train() client id: f_00001-9-2 loss: 0.534496  [   96/  265]
train() client id: f_00001-9-3 loss: 0.535206  [  128/  265]
train() client id: f_00001-9-4 loss: 0.464024  [  160/  265]
train() client id: f_00001-9-5 loss: 0.632589  [  192/  265]
train() client id: f_00001-9-6 loss: 0.426542  [  224/  265]
train() client id: f_00001-9-7 loss: 0.440599  [  256/  265]
train() client id: f_00001-10-0 loss: 0.458136  [   32/  265]
train() client id: f_00001-10-1 loss: 0.490324  [   64/  265]
train() client id: f_00001-10-2 loss: 0.385978  [   96/  265]
train() client id: f_00001-10-3 loss: 0.446663  [  128/  265]
train() client id: f_00001-10-4 loss: 0.500898  [  160/  265]
train() client id: f_00001-10-5 loss: 0.542325  [  192/  265]
train() client id: f_00001-10-6 loss: 0.472197  [  224/  265]
train() client id: f_00001-10-7 loss: 0.568141  [  256/  265]
train() client id: f_00001-11-0 loss: 0.545521  [   32/  265]
train() client id: f_00001-11-1 loss: 0.447542  [   64/  265]
train() client id: f_00001-11-2 loss: 0.535396  [   96/  265]
train() client id: f_00001-11-3 loss: 0.483280  [  128/  265]
train() client id: f_00001-11-4 loss: 0.458777  [  160/  265]
train() client id: f_00001-11-5 loss: 0.455270  [  192/  265]
train() client id: f_00001-11-6 loss: 0.407772  [  224/  265]
train() client id: f_00001-11-7 loss: 0.522881  [  256/  265]
train() client id: f_00002-0-0 loss: 1.298938  [   32/  124]
train() client id: f_00002-0-1 loss: 1.257657  [   64/  124]
train() client id: f_00002-0-2 loss: 1.233514  [   96/  124]
train() client id: f_00002-1-0 loss: 1.152779  [   32/  124]
train() client id: f_00002-1-1 loss: 1.175426  [   64/  124]
train() client id: f_00002-1-2 loss: 1.129565  [   96/  124]
train() client id: f_00002-2-0 loss: 1.059892  [   32/  124]
train() client id: f_00002-2-1 loss: 1.146851  [   64/  124]
train() client id: f_00002-2-2 loss: 1.210653  [   96/  124]
train() client id: f_00002-3-0 loss: 0.976676  [   32/  124]
train() client id: f_00002-3-1 loss: 1.053706  [   64/  124]
train() client id: f_00002-3-2 loss: 1.155575  [   96/  124]
train() client id: f_00002-4-0 loss: 1.264701  [   32/  124]
train() client id: f_00002-4-1 loss: 1.180153  [   64/  124]
train() client id: f_00002-4-2 loss: 1.043526  [   96/  124]
train() client id: f_00002-5-0 loss: 0.972066  [   32/  124]
train() client id: f_00002-5-1 loss: 1.071723  [   64/  124]
train() client id: f_00002-5-2 loss: 1.224641  [   96/  124]
train() client id: f_00002-6-0 loss: 0.981355  [   32/  124]
train() client id: f_00002-6-1 loss: 1.082173  [   64/  124]
train() client id: f_00002-6-2 loss: 1.307912  [   96/  124]
train() client id: f_00002-7-0 loss: 0.986823  [   32/  124]
train() client id: f_00002-7-1 loss: 1.021999  [   64/  124]
train() client id: f_00002-7-2 loss: 1.308381  [   96/  124]
train() client id: f_00002-8-0 loss: 0.989253  [   32/  124]
train() client id: f_00002-8-1 loss: 1.174236  [   64/  124]
train() client id: f_00002-8-2 loss: 1.148432  [   96/  124]
train() client id: f_00002-9-0 loss: 1.151194  [   32/  124]
train() client id: f_00002-9-1 loss: 1.087121  [   64/  124]
train() client id: f_00002-9-2 loss: 1.025759  [   96/  124]
train() client id: f_00002-10-0 loss: 1.072019  [   32/  124]
train() client id: f_00002-10-1 loss: 1.073932  [   64/  124]
train() client id: f_00002-10-2 loss: 1.140305  [   96/  124]
train() client id: f_00002-11-0 loss: 1.112335  [   32/  124]
train() client id: f_00002-11-1 loss: 1.004895  [   64/  124]
train() client id: f_00002-11-2 loss: 1.228090  [   96/  124]
train() client id: f_00003-0-0 loss: 0.733188  [   32/   43]
train() client id: f_00003-1-0 loss: 0.805073  [   32/   43]
train() client id: f_00003-2-0 loss: 0.740929  [   32/   43]
train() client id: f_00003-3-0 loss: 0.746384  [   32/   43]
train() client id: f_00003-4-0 loss: 0.601560  [   32/   43]
train() client id: f_00003-5-0 loss: 0.684575  [   32/   43]
train() client id: f_00003-6-0 loss: 0.634284  [   32/   43]
train() client id: f_00003-7-0 loss: 0.711173  [   32/   43]
train() client id: f_00003-8-0 loss: 0.614093  [   32/   43]
train() client id: f_00003-9-0 loss: 0.725946  [   32/   43]
train() client id: f_00003-10-0 loss: 0.781476  [   32/   43]
train() client id: f_00003-11-0 loss: 0.705393  [   32/   43]
train() client id: f_00004-0-0 loss: 0.717414  [   32/  306]
train() client id: f_00004-0-1 loss: 0.885627  [   64/  306]
train() client id: f_00004-0-2 loss: 0.849677  [   96/  306]
train() client id: f_00004-0-3 loss: 0.751146  [  128/  306]
train() client id: f_00004-0-4 loss: 0.801763  [  160/  306]
train() client id: f_00004-0-5 loss: 0.802427  [  192/  306]
train() client id: f_00004-0-6 loss: 0.911363  [  224/  306]
train() client id: f_00004-0-7 loss: 0.931028  [  256/  306]
train() client id: f_00004-0-8 loss: 0.808331  [  288/  306]
train() client id: f_00004-1-0 loss: 0.753259  [   32/  306]
train() client id: f_00004-1-1 loss: 0.775606  [   64/  306]
train() client id: f_00004-1-2 loss: 0.804302  [   96/  306]
train() client id: f_00004-1-3 loss: 0.866108  [  128/  306]
train() client id: f_00004-1-4 loss: 0.838875  [  160/  306]
train() client id: f_00004-1-5 loss: 0.943749  [  192/  306]
train() client id: f_00004-1-6 loss: 0.726955  [  224/  306]
train() client id: f_00004-1-7 loss: 0.942299  [  256/  306]
train() client id: f_00004-1-8 loss: 0.833697  [  288/  306]
train() client id: f_00004-2-0 loss: 0.917975  [   32/  306]
train() client id: f_00004-2-1 loss: 0.921662  [   64/  306]
train() client id: f_00004-2-2 loss: 0.862257  [   96/  306]
train() client id: f_00004-2-3 loss: 0.688075  [  128/  306]
train() client id: f_00004-2-4 loss: 0.741564  [  160/  306]
train() client id: f_00004-2-5 loss: 0.978897  [  192/  306]
train() client id: f_00004-2-6 loss: 0.888241  [  224/  306]
train() client id: f_00004-2-7 loss: 0.786356  [  256/  306]
train() client id: f_00004-2-8 loss: 0.790431  [  288/  306]
train() client id: f_00004-3-0 loss: 0.794628  [   32/  306]
train() client id: f_00004-3-1 loss: 0.877936  [   64/  306]
train() client id: f_00004-3-2 loss: 0.802641  [   96/  306]
train() client id: f_00004-3-3 loss: 0.771157  [  128/  306]
train() client id: f_00004-3-4 loss: 0.924071  [  160/  306]
train() client id: f_00004-3-5 loss: 0.850281  [  192/  306]
train() client id: f_00004-3-6 loss: 0.776747  [  224/  306]
train() client id: f_00004-3-7 loss: 0.870915  [  256/  306]
train() client id: f_00004-3-8 loss: 0.866049  [  288/  306]
train() client id: f_00004-4-0 loss: 0.798062  [   32/  306]
train() client id: f_00004-4-1 loss: 0.789538  [   64/  306]
train() client id: f_00004-4-2 loss: 0.911243  [   96/  306]
train() client id: f_00004-4-3 loss: 0.823723  [  128/  306]
train() client id: f_00004-4-4 loss: 0.789799  [  160/  306]
train() client id: f_00004-4-5 loss: 0.762009  [  192/  306]
train() client id: f_00004-4-6 loss: 0.898136  [  224/  306]
train() client id: f_00004-4-7 loss: 0.821540  [  256/  306]
train() client id: f_00004-4-8 loss: 0.838184  [  288/  306]
train() client id: f_00004-5-0 loss: 0.818932  [   32/  306]
train() client id: f_00004-5-1 loss: 0.767834  [   64/  306]
train() client id: f_00004-5-2 loss: 0.809577  [   96/  306]
train() client id: f_00004-5-3 loss: 0.933844  [  128/  306]
train() client id: f_00004-5-4 loss: 0.868179  [  160/  306]
train() client id: f_00004-5-5 loss: 0.866750  [  192/  306]
train() client id: f_00004-5-6 loss: 0.843164  [  224/  306]
train() client id: f_00004-5-7 loss: 0.838484  [  256/  306]
train() client id: f_00004-5-8 loss: 0.757354  [  288/  306]
train() client id: f_00004-6-0 loss: 0.928217  [   32/  306]
train() client id: f_00004-6-1 loss: 0.762668  [   64/  306]
train() client id: f_00004-6-2 loss: 0.839550  [   96/  306]
train() client id: f_00004-6-3 loss: 0.877508  [  128/  306]
train() client id: f_00004-6-4 loss: 0.805271  [  160/  306]
train() client id: f_00004-6-5 loss: 0.828599  [  192/  306]
train() client id: f_00004-6-6 loss: 0.818196  [  224/  306]
train() client id: f_00004-6-7 loss: 0.794466  [  256/  306]
train() client id: f_00004-6-8 loss: 0.841762  [  288/  306]
train() client id: f_00004-7-0 loss: 0.842481  [   32/  306]
train() client id: f_00004-7-1 loss: 0.837022  [   64/  306]
train() client id: f_00004-7-2 loss: 0.846658  [   96/  306]
train() client id: f_00004-7-3 loss: 0.798046  [  128/  306]
train() client id: f_00004-7-4 loss: 0.971951  [  160/  306]
train() client id: f_00004-7-5 loss: 0.834091  [  192/  306]
train() client id: f_00004-7-6 loss: 0.803989  [  224/  306]
train() client id: f_00004-7-7 loss: 0.849033  [  256/  306]
train() client id: f_00004-7-8 loss: 0.735089  [  288/  306]
train() client id: f_00004-8-0 loss: 0.851845  [   32/  306]
train() client id: f_00004-8-1 loss: 0.821050  [   64/  306]
train() client id: f_00004-8-2 loss: 0.823757  [   96/  306]
train() client id: f_00004-8-3 loss: 0.740412  [  128/  306]
train() client id: f_00004-8-4 loss: 0.910432  [  160/  306]
train() client id: f_00004-8-5 loss: 0.795738  [  192/  306]
train() client id: f_00004-8-6 loss: 0.878467  [  224/  306]
train() client id: f_00004-8-7 loss: 0.847367  [  256/  306]
train() client id: f_00004-8-8 loss: 0.813598  [  288/  306]
train() client id: f_00004-9-0 loss: 0.880755  [   32/  306]
train() client id: f_00004-9-1 loss: 0.810883  [   64/  306]
train() client id: f_00004-9-2 loss: 0.795562  [   96/  306]
train() client id: f_00004-9-3 loss: 0.787055  [  128/  306]
train() client id: f_00004-9-4 loss: 0.910198  [  160/  306]
train() client id: f_00004-9-5 loss: 0.862810  [  192/  306]
train() client id: f_00004-9-6 loss: 0.817910  [  224/  306]
train() client id: f_00004-9-7 loss: 0.795759  [  256/  306]
train() client id: f_00004-9-8 loss: 0.780189  [  288/  306]
train() client id: f_00004-10-0 loss: 0.905943  [   32/  306]
train() client id: f_00004-10-1 loss: 0.838737  [   64/  306]
train() client id: f_00004-10-2 loss: 0.890082  [   96/  306]
train() client id: f_00004-10-3 loss: 0.766117  [  128/  306]
train() client id: f_00004-10-4 loss: 0.882284  [  160/  306]
train() client id: f_00004-10-5 loss: 0.806065  [  192/  306]
train() client id: f_00004-10-6 loss: 0.896310  [  224/  306]
train() client id: f_00004-10-7 loss: 0.821816  [  256/  306]
train() client id: f_00004-10-8 loss: 0.754567  [  288/  306]
train() client id: f_00004-11-0 loss: 0.877705  [   32/  306]
train() client id: f_00004-11-1 loss: 0.809326  [   64/  306]
train() client id: f_00004-11-2 loss: 0.912917  [   96/  306]
train() client id: f_00004-11-3 loss: 0.772475  [  128/  306]
train() client id: f_00004-11-4 loss: 0.809312  [  160/  306]
train() client id: f_00004-11-5 loss: 0.819474  [  192/  306]
train() client id: f_00004-11-6 loss: 0.996430  [  224/  306]
train() client id: f_00004-11-7 loss: 0.802636  [  256/  306]
train() client id: f_00004-11-8 loss: 0.780261  [  288/  306]
train() client id: f_00005-0-0 loss: 0.304541  [   32/  146]
train() client id: f_00005-0-1 loss: 0.324669  [   64/  146]
train() client id: f_00005-0-2 loss: 0.962662  [   96/  146]
train() client id: f_00005-0-3 loss: 0.761605  [  128/  146]
train() client id: f_00005-1-0 loss: 0.800691  [   32/  146]
train() client id: f_00005-1-1 loss: 0.553629  [   64/  146]
train() client id: f_00005-1-2 loss: 0.569514  [   96/  146]
train() client id: f_00005-1-3 loss: 0.436078  [  128/  146]
train() client id: f_00005-2-0 loss: 0.598609  [   32/  146]
train() client id: f_00005-2-1 loss: 0.651960  [   64/  146]
train() client id: f_00005-2-2 loss: 0.674311  [   96/  146]
train() client id: f_00005-2-3 loss: 0.474150  [  128/  146]
train() client id: f_00005-3-0 loss: 0.596708  [   32/  146]
train() client id: f_00005-3-1 loss: 0.658419  [   64/  146]
train() client id: f_00005-3-2 loss: 0.480310  [   96/  146]
train() client id: f_00005-3-3 loss: 0.696872  [  128/  146]
train() client id: f_00005-4-0 loss: 0.788954  [   32/  146]
train() client id: f_00005-4-1 loss: 0.595170  [   64/  146]
train() client id: f_00005-4-2 loss: 0.482923  [   96/  146]
train() client id: f_00005-4-3 loss: 0.565865  [  128/  146]
train() client id: f_00005-5-0 loss: 0.771276  [   32/  146]
train() client id: f_00005-5-1 loss: 0.366540  [   64/  146]
train() client id: f_00005-5-2 loss: 0.569114  [   96/  146]
train() client id: f_00005-5-3 loss: 0.373253  [  128/  146]
train() client id: f_00005-6-0 loss: 0.898966  [   32/  146]
train() client id: f_00005-6-1 loss: 0.687266  [   64/  146]
train() client id: f_00005-6-2 loss: 0.470224  [   96/  146]
train() client id: f_00005-6-3 loss: 0.468197  [  128/  146]
train() client id: f_00005-7-0 loss: 0.382052  [   32/  146]
train() client id: f_00005-7-1 loss: 0.696369  [   64/  146]
train() client id: f_00005-7-2 loss: 0.820980  [   96/  146]
train() client id: f_00005-7-3 loss: 0.606907  [  128/  146]
train() client id: f_00005-8-0 loss: 0.815285  [   32/  146]
train() client id: f_00005-8-1 loss: 0.669396  [   64/  146]
train() client id: f_00005-8-2 loss: 0.504896  [   96/  146]
train() client id: f_00005-8-3 loss: 0.258703  [  128/  146]
train() client id: f_00005-9-0 loss: 0.439879  [   32/  146]
train() client id: f_00005-9-1 loss: 0.850306  [   64/  146]
train() client id: f_00005-9-2 loss: 0.375693  [   96/  146]
train() client id: f_00005-9-3 loss: 0.663146  [  128/  146]
train() client id: f_00005-10-0 loss: 0.392659  [   32/  146]
train() client id: f_00005-10-1 loss: 0.567737  [   64/  146]
train() client id: f_00005-10-2 loss: 0.583145  [   96/  146]
train() client id: f_00005-10-3 loss: 0.664382  [  128/  146]
train() client id: f_00005-11-0 loss: 0.566160  [   32/  146]
train() client id: f_00005-11-1 loss: 0.456674  [   64/  146]
train() client id: f_00005-11-2 loss: 0.477326  [   96/  146]
train() client id: f_00005-11-3 loss: 0.572248  [  128/  146]
train() client id: f_00006-0-0 loss: 0.483160  [   32/   54]
train() client id: f_00006-1-0 loss: 0.533727  [   32/   54]
train() client id: f_00006-2-0 loss: 0.522230  [   32/   54]
train() client id: f_00006-3-0 loss: 0.516036  [   32/   54]
train() client id: f_00006-4-0 loss: 0.516406  [   32/   54]
train() client id: f_00006-5-0 loss: 0.533575  [   32/   54]
train() client id: f_00006-6-0 loss: 0.524699  [   32/   54]
train() client id: f_00006-7-0 loss: 0.533216  [   32/   54]
train() client id: f_00006-8-0 loss: 0.497575  [   32/   54]
train() client id: f_00006-9-0 loss: 0.478540  [   32/   54]
train() client id: f_00006-10-0 loss: 0.459460  [   32/   54]
train() client id: f_00006-11-0 loss: 0.472407  [   32/   54]
train() client id: f_00007-0-0 loss: 0.614322  [   32/  179]
train() client id: f_00007-0-1 loss: 0.701074  [   64/  179]
train() client id: f_00007-0-2 loss: 0.775842  [   96/  179]
train() client id: f_00007-0-3 loss: 0.736380  [  128/  179]
train() client id: f_00007-0-4 loss: 0.502624  [  160/  179]
train() client id: f_00007-1-0 loss: 0.890250  [   32/  179]
train() client id: f_00007-1-1 loss: 0.685938  [   64/  179]
train() client id: f_00007-1-2 loss: 0.477442  [   96/  179]
train() client id: f_00007-1-3 loss: 0.527047  [  128/  179]
train() client id: f_00007-1-4 loss: 0.658544  [  160/  179]
train() client id: f_00007-2-0 loss: 0.578315  [   32/  179]
train() client id: f_00007-2-1 loss: 0.491202  [   64/  179]
train() client id: f_00007-2-2 loss: 0.678824  [   96/  179]
train() client id: f_00007-2-3 loss: 0.848136  [  128/  179]
train() client id: f_00007-2-4 loss: 0.554760  [  160/  179]
train() client id: f_00007-3-0 loss: 0.565194  [   32/  179]
train() client id: f_00007-3-1 loss: 1.043257  [   64/  179]
train() client id: f_00007-3-2 loss: 0.490712  [   96/  179]
train() client id: f_00007-3-3 loss: 0.495424  [  128/  179]
train() client id: f_00007-3-4 loss: 0.505888  [  160/  179]
train() client id: f_00007-4-0 loss: 0.704704  [   32/  179]
train() client id: f_00007-4-1 loss: 0.511460  [   64/  179]
train() client id: f_00007-4-2 loss: 0.704300  [   96/  179]
train() client id: f_00007-4-3 loss: 0.565909  [  128/  179]
train() client id: f_00007-4-4 loss: 0.594342  [  160/  179]
train() client id: f_00007-5-0 loss: 0.718375  [   32/  179]
train() client id: f_00007-5-1 loss: 0.696517  [   64/  179]
train() client id: f_00007-5-2 loss: 0.558239  [   96/  179]
train() client id: f_00007-5-3 loss: 0.533997  [  128/  179]
train() client id: f_00007-5-4 loss: 0.473463  [  160/  179]
train() client id: f_00007-6-0 loss: 0.702555  [   32/  179]
train() client id: f_00007-6-1 loss: 0.720572  [   64/  179]
train() client id: f_00007-6-2 loss: 0.628566  [   96/  179]
train() client id: f_00007-6-3 loss: 0.459473  [  128/  179]
train() client id: f_00007-6-4 loss: 0.503747  [  160/  179]
train() client id: f_00007-7-0 loss: 0.456946  [   32/  179]
train() client id: f_00007-7-1 loss: 0.708473  [   64/  179]
train() client id: f_00007-7-2 loss: 0.499869  [   96/  179]
train() client id: f_00007-7-3 loss: 0.628299  [  128/  179]
train() client id: f_00007-7-4 loss: 0.788230  [  160/  179]
train() client id: f_00007-8-0 loss: 0.645271  [   32/  179]
train() client id: f_00007-8-1 loss: 0.711469  [   64/  179]
train() client id: f_00007-8-2 loss: 0.516623  [   96/  179]
train() client id: f_00007-8-3 loss: 0.584907  [  128/  179]
train() client id: f_00007-8-4 loss: 0.491475  [  160/  179]
train() client id: f_00007-9-0 loss: 0.564235  [   32/  179]
train() client id: f_00007-9-1 loss: 0.515871  [   64/  179]
train() client id: f_00007-9-2 loss: 0.669337  [   96/  179]
train() client id: f_00007-9-3 loss: 0.479537  [  128/  179]
train() client id: f_00007-9-4 loss: 0.822126  [  160/  179]
train() client id: f_00007-10-0 loss: 0.594101  [   32/  179]
train() client id: f_00007-10-1 loss: 0.725193  [   64/  179]
train() client id: f_00007-10-2 loss: 0.529186  [   96/  179]
train() client id: f_00007-10-3 loss: 0.577660  [  128/  179]
train() client id: f_00007-10-4 loss: 0.636145  [  160/  179]
train() client id: f_00007-11-0 loss: 0.538506  [   32/  179]
train() client id: f_00007-11-1 loss: 0.484304  [   64/  179]
train() client id: f_00007-11-2 loss: 0.735262  [   96/  179]
train() client id: f_00007-11-3 loss: 0.683531  [  128/  179]
train() client id: f_00007-11-4 loss: 0.525771  [  160/  179]
train() client id: f_00008-0-0 loss: 0.763239  [   32/  130]
train() client id: f_00008-0-1 loss: 0.722620  [   64/  130]
train() client id: f_00008-0-2 loss: 0.656370  [   96/  130]
train() client id: f_00008-0-3 loss: 0.578421  [  128/  130]
train() client id: f_00008-1-0 loss: 0.681774  [   32/  130]
train() client id: f_00008-1-1 loss: 0.682917  [   64/  130]
train() client id: f_00008-1-2 loss: 0.746038  [   96/  130]
train() client id: f_00008-1-3 loss: 0.612187  [  128/  130]
train() client id: f_00008-2-0 loss: 0.622200  [   32/  130]
train() client id: f_00008-2-1 loss: 0.584745  [   64/  130]
train() client id: f_00008-2-2 loss: 0.802122  [   96/  130]
train() client id: f_00008-2-3 loss: 0.703451  [  128/  130]
train() client id: f_00008-3-0 loss: 0.635748  [   32/  130]
train() client id: f_00008-3-1 loss: 0.710773  [   64/  130]
train() client id: f_00008-3-2 loss: 0.727534  [   96/  130]
train() client id: f_00008-3-3 loss: 0.642451  [  128/  130]
train() client id: f_00008-4-0 loss: 0.581425  [   32/  130]
train() client id: f_00008-4-1 loss: 0.645814  [   64/  130]
train() client id: f_00008-4-2 loss: 0.731359  [   96/  130]
train() client id: f_00008-4-3 loss: 0.714846  [  128/  130]
train() client id: f_00008-5-0 loss: 0.599844  [   32/  130]
train() client id: f_00008-5-1 loss: 0.655719  [   64/  130]
train() client id: f_00008-5-2 loss: 0.695227  [   96/  130]
train() client id: f_00008-5-3 loss: 0.769021  [  128/  130]
train() client id: f_00008-6-0 loss: 0.837190  [   32/  130]
train() client id: f_00008-6-1 loss: 0.701067  [   64/  130]
train() client id: f_00008-6-2 loss: 0.617539  [   96/  130]
train() client id: f_00008-6-3 loss: 0.556700  [  128/  130]
train() client id: f_00008-7-0 loss: 0.714594  [   32/  130]
train() client id: f_00008-7-1 loss: 0.629631  [   64/  130]
train() client id: f_00008-7-2 loss: 0.742173  [   96/  130]
train() client id: f_00008-7-3 loss: 0.623400  [  128/  130]
train() client id: f_00008-8-0 loss: 0.542368  [   32/  130]
train() client id: f_00008-8-1 loss: 0.653153  [   64/  130]
train() client id: f_00008-8-2 loss: 0.693899  [   96/  130]
train() client id: f_00008-8-3 loss: 0.808382  [  128/  130]
train() client id: f_00008-9-0 loss: 0.703711  [   32/  130]
train() client id: f_00008-9-1 loss: 0.707181  [   64/  130]
train() client id: f_00008-9-2 loss: 0.644188  [   96/  130]
train() client id: f_00008-9-3 loss: 0.645656  [  128/  130]
train() client id: f_00008-10-0 loss: 0.674455  [   32/  130]
train() client id: f_00008-10-1 loss: 0.837150  [   64/  130]
train() client id: f_00008-10-2 loss: 0.587202  [   96/  130]
train() client id: f_00008-10-3 loss: 0.576465  [  128/  130]
train() client id: f_00008-11-0 loss: 0.587206  [   32/  130]
train() client id: f_00008-11-1 loss: 0.717627  [   64/  130]
train() client id: f_00008-11-2 loss: 0.759427  [   96/  130]
train() client id: f_00008-11-3 loss: 0.593229  [  128/  130]
train() client id: f_00009-0-0 loss: 1.010161  [   32/  118]
train() client id: f_00009-0-1 loss: 1.089961  [   64/  118]
train() client id: f_00009-0-2 loss: 1.095680  [   96/  118]
train() client id: f_00009-1-0 loss: 1.010031  [   32/  118]
train() client id: f_00009-1-1 loss: 0.961257  [   64/  118]
train() client id: f_00009-1-2 loss: 1.053563  [   96/  118]
train() client id: f_00009-2-0 loss: 0.966056  [   32/  118]
train() client id: f_00009-2-1 loss: 0.866301  [   64/  118]
train() client id: f_00009-2-2 loss: 1.096563  [   96/  118]
train() client id: f_00009-3-0 loss: 0.946142  [   32/  118]
train() client id: f_00009-3-1 loss: 0.912343  [   64/  118]
train() client id: f_00009-3-2 loss: 1.039332  [   96/  118]
train() client id: f_00009-4-0 loss: 0.866014  [   32/  118]
train() client id: f_00009-4-1 loss: 0.868945  [   64/  118]
train() client id: f_00009-4-2 loss: 0.996073  [   96/  118]
train() client id: f_00009-5-0 loss: 1.090931  [   32/  118]
train() client id: f_00009-5-1 loss: 0.820678  [   64/  118]
train() client id: f_00009-5-2 loss: 0.903778  [   96/  118]
train() client id: f_00009-6-0 loss: 0.811824  [   32/  118]
train() client id: f_00009-6-1 loss: 1.072003  [   64/  118]
train() client id: f_00009-6-2 loss: 0.792984  [   96/  118]
train() client id: f_00009-7-0 loss: 0.903996  [   32/  118]
train() client id: f_00009-7-1 loss: 0.767274  [   64/  118]
train() client id: f_00009-7-2 loss: 0.847869  [   96/  118]
train() client id: f_00009-8-0 loss: 0.831239  [   32/  118]
train() client id: f_00009-8-1 loss: 0.779592  [   64/  118]
train() client id: f_00009-8-2 loss: 1.035130  [   96/  118]
train() client id: f_00009-9-0 loss: 0.811931  [   32/  118]
train() client id: f_00009-9-1 loss: 0.999439  [   64/  118]
train() client id: f_00009-9-2 loss: 0.690377  [   96/  118]
train() client id: f_00009-10-0 loss: 0.742675  [   32/  118]
train() client id: f_00009-10-1 loss: 0.894056  [   64/  118]
train() client id: f_00009-10-2 loss: 0.819593  [   96/  118]
train() client id: f_00009-11-0 loss: 0.925464  [   32/  118]
train() client id: f_00009-11-1 loss: 0.738476  [   64/  118]
train() client id: f_00009-11-2 loss: 0.913243  [   96/  118]
At round 27 accuracy: 0.6472148541114059
At round 27 training accuracy: 0.5835010060362174
At round 27 training loss: 0.8322825584118394
update_location
xs = -4.528292 36.001589 45.045120 -45.943528 -55.103519 -0.217951 -87.215960 108.375741 -1.680116 29.695607 
ys = 122.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 -3.124922 -19.177652 -52.154970 4.001482 
xs mean: 2.4428691594128664
ys mean: 8.371751218646876
dists_uav = 158.266589 107.415541 109.685035 112.334665 114.559259 101.466255 132.726745 148.704686 112.796115 104.392725 
uav_gains = -104.993905 -100.776722 -101.003744 -101.262929 -101.475866 -100.158060 -103.074833 -104.312174 -101.307443 -100.466785 
uav_gains_db_mean: -101.8832462156524
dists_bs = 178.346924 264.469490 280.329063 199.745222 204.487229 259.777146 198.581282 343.521136 285.725677 266.722299 
bs_gains = -102.602497 -107.393575 -108.101767 -103.980398 -104.265712 -107.175885 -103.909332 -110.573756 -108.333639 -107.496720 
bs_gains_db_mean: -106.38332817837274
Round 28
-------------------------------
ene_coms = [0.00794314 0.00906693 0.00657626 0.00665083 0.00765378 0.00895254
 0.00722253 0.00767163 0.00959474 0.00912211]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.28536984 15.17895343  7.14642794  2.56026809 17.47451414  8.43705639
  3.19400609 10.28200802  7.5429316   6.85441753]
obj_prev = 85.9559530793796
eta_min = 2.1292757116646409e-13	eta_max = 0.9367251170626595
af = 18.15364795335576	bf = 1.4957552268681122	zeta = 19.96901274869134	eta = 0.909090909090909
af = 18.15364795335576	bf = 1.4957552268681122	zeta = 35.48580945147546	eta = 0.5115748586256627
af = 18.15364795335576	bf = 1.4957552268681122	zeta = 27.97448925291288	eta = 0.6489358139564779
af = 18.15364795335576	bf = 1.4957552268681122	zeta = 26.621292285817805	eta = 0.6819221155175443
af = 18.15364795335576	bf = 1.4957552268681122	zeta = 26.55228513188854	eta = 0.6836943736926713
af = 18.15364795335576	bf = 1.4957552268681122	zeta = 26.552091975851834	eta = 0.6836993473006138
eta = 0.6836993473006138
ene_coms = [0.00794314 0.00906693 0.00657626 0.00665083 0.00765378 0.00895254
 0.00722253 0.00767163 0.00959474 0.00912211]
ene_comp = [0.03137625 0.06598972 0.03087821 0.01070777 0.07619946 0.0363566
 0.01344696 0.04457419 0.03237232 0.0293841 ]
ene_total = [2.31109001 4.4116325  2.20147513 1.02029249 4.92867254 2.66315215
 1.21489818 3.07087179 2.46671317 2.26329402]
ti_comp = [0.40714846 0.39591056 0.42081726 0.42007158 0.41004203 0.39705445
 0.41435459 0.40986356 0.39063245 0.39535873]
ti_coms = [0.07943141 0.09066931 0.0657626  0.06650828 0.07653784 0.08952541
 0.07222527 0.0767163  0.09594741 0.09122113]
t_total = [28.59988251 28.59988251 28.59988251 28.59988251 28.59988251 28.59988251
 28.59988251 28.59988251 28.59988251 28.59988251]
ene_coms = [0.00794314 0.00906693 0.00657626 0.00665083 0.00765378 0.00895254
 0.00722253 0.00767163 0.00959474 0.00912211]
ene_comp = [1.16460165e-05 1.14581581e-04 1.03908078e-05 4.34840634e-07
 1.64467045e-04 1.90515266e-05 8.85132577e-07 3.29497279e-05
 1.38951989e-05 1.01445949e-05]
ene_total = [0.46756141 0.5396651  0.38714598 0.39094373 0.45953618 0.52732659
 0.42457316 0.45285493 0.56477032 0.53677007]
optimize_network iter = 0 obj = 4.7511474705655665
eta = 0.6836993473006138
freqs = [38531702.83891229 83339183.64970113 36688384.34586613 12745169.21199405
 92916641.83490519 45782893.45528691 16226395.45789896 54376866.08843827
 41435776.39414379 37161319.42987317]
eta_min = 0.6836993473006244	eta_max = 0.683699347300604
af = 0.01850377259828312	bf = 1.4957552268681122	zeta = 0.02035414985811143	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00794314 0.00906693 0.00657626 0.00665083 0.00765378 0.00895254
 0.00722253 0.00767163 0.00959474 0.00912211]
ene_comp = [2.91918962e-06 2.87210105e-05 2.60455910e-06 1.08997120e-07
 4.12252971e-05 4.77545422e-06 2.21867265e-07 8.25917630e-06
 3.48296950e-06 2.54284340e-06]
ene_total = [1.62500508 1.8601018  1.34540747 1.3601467  1.57366405 1.83181166
 1.4770848  1.57057207 1.96288009 1.86603337]
ti_comp = [0.40714846 0.39591056 0.42081726 0.42007158 0.41004203 0.39705445
 0.41435459 0.40986356 0.39063245 0.39535873]
ti_coms = [0.07943141 0.09066931 0.0657626  0.06650828 0.07653784 0.08952541
 0.07222527 0.0767163  0.09594741 0.09122113]
t_total = [28.59988251 28.59988251 28.59988251 28.59988251 28.59988251 28.59988251
 28.59988251 28.59988251 28.59988251 28.59988251]
ene_coms = [0.00794314 0.00906693 0.00657626 0.00665083 0.00765378 0.00895254
 0.00722253 0.00767163 0.00959474 0.00912211]
ene_comp = [1.16460165e-05 1.14581581e-04 1.03908078e-05 4.34840634e-07
 1.64467045e-04 1.90515266e-05 8.85132577e-07 3.29497279e-05
 1.38951989e-05 1.01445949e-05]
ene_total = [0.46756141 0.5396651  0.38714598 0.39094373 0.45953618 0.52732659
 0.42457316 0.45285493 0.56477032 0.53677007]
optimize_network iter = 1 obj = 4.751147470565421
eta = 0.683699347300604
freqs = [38531702.83891232 83339183.6497011  36688384.3458662  12745169.21199407
 92916641.8349053  45782893.4552869  16226395.45789898 54376866.08843833
 41435776.39414377 37161319.42987315]
Done!
ene_coms = [0.00794314 0.00906693 0.00657626 0.00665083 0.00765378 0.00895254
 0.00722253 0.00767163 0.00959474 0.00912211]
ene_comp = [1.12242725e-05 1.10432171e-04 1.00145193e-05 4.19093493e-07
 1.58511103e-04 1.83616024e-05 8.53078747e-07 3.17564999e-05
 1.33920038e-05 9.77722273e-06]
ene_total = [0.00795436 0.00917736 0.00658627 0.00665125 0.00781229 0.0089709
 0.00722338 0.00770339 0.00960813 0.00913189]
At round 28 energy consumption: 0.08081923839424714
At round 28 eta: 0.683699347300604
At round 28 a_n: 18.59131914111349
At round 28 local rounds: 12.450891501182872
At round 28 global rounds: 58.77736571976727
gradient difference: 0.4256163239479065
train() client id: f_00000-0-0 loss: 1.336785  [   32/  126]
train() client id: f_00000-0-1 loss: 1.405983  [   64/  126]
train() client id: f_00000-0-2 loss: 1.127829  [   96/  126]
train() client id: f_00000-1-0 loss: 1.196857  [   32/  126]
train() client id: f_00000-1-1 loss: 1.202346  [   64/  126]
train() client id: f_00000-1-2 loss: 1.229628  [   96/  126]
train() client id: f_00000-2-0 loss: 1.131239  [   32/  126]
train() client id: f_00000-2-1 loss: 1.131515  [   64/  126]
train() client id: f_00000-2-2 loss: 1.062999  [   96/  126]
train() client id: f_00000-3-0 loss: 0.946931  [   32/  126]
train() client id: f_00000-3-1 loss: 1.132386  [   64/  126]
train() client id: f_00000-3-2 loss: 1.083286  [   96/  126]
train() client id: f_00000-4-0 loss: 1.076570  [   32/  126]
train() client id: f_00000-4-1 loss: 0.984834  [   64/  126]
train() client id: f_00000-4-2 loss: 0.945598  [   96/  126]
train() client id: f_00000-5-0 loss: 1.042804  [   32/  126]
train() client id: f_00000-5-1 loss: 0.938069  [   64/  126]
train() client id: f_00000-5-2 loss: 1.006172  [   96/  126]
train() client id: f_00000-6-0 loss: 0.944181  [   32/  126]
train() client id: f_00000-6-1 loss: 1.053017  [   64/  126]
train() client id: f_00000-6-2 loss: 0.975888  [   96/  126]
train() client id: f_00000-7-0 loss: 0.837291  [   32/  126]
train() client id: f_00000-7-1 loss: 1.051712  [   64/  126]
train() client id: f_00000-7-2 loss: 0.985992  [   96/  126]
train() client id: f_00000-8-0 loss: 0.959069  [   32/  126]
train() client id: f_00000-8-1 loss: 0.942282  [   64/  126]
train() client id: f_00000-8-2 loss: 0.952823  [   96/  126]
train() client id: f_00000-9-0 loss: 0.894576  [   32/  126]
train() client id: f_00000-9-1 loss: 0.927872  [   64/  126]
train() client id: f_00000-9-2 loss: 1.026190  [   96/  126]
train() client id: f_00000-10-0 loss: 0.947975  [   32/  126]
train() client id: f_00000-10-1 loss: 0.857547  [   64/  126]
train() client id: f_00000-10-2 loss: 1.051763  [   96/  126]
train() client id: f_00000-11-0 loss: 1.008837  [   32/  126]
train() client id: f_00000-11-1 loss: 0.960070  [   64/  126]
train() client id: f_00000-11-2 loss: 0.918081  [   96/  126]
train() client id: f_00001-0-0 loss: 0.386232  [   32/  265]
train() client id: f_00001-0-1 loss: 0.409559  [   64/  265]
train() client id: f_00001-0-2 loss: 0.423402  [   96/  265]
train() client id: f_00001-0-3 loss: 0.375612  [  128/  265]
train() client id: f_00001-0-4 loss: 0.513187  [  160/  265]
train() client id: f_00001-0-5 loss: 0.433034  [  192/  265]
train() client id: f_00001-0-6 loss: 0.562167  [  224/  265]
train() client id: f_00001-0-7 loss: 0.459478  [  256/  265]
train() client id: f_00001-1-0 loss: 0.383757  [   32/  265]
train() client id: f_00001-1-1 loss: 0.412949  [   64/  265]
train() client id: f_00001-1-2 loss: 0.497550  [   96/  265]
train() client id: f_00001-1-3 loss: 0.415175  [  128/  265]
train() client id: f_00001-1-4 loss: 0.342126  [  160/  265]
train() client id: f_00001-1-5 loss: 0.507405  [  192/  265]
train() client id: f_00001-1-6 loss: 0.466270  [  224/  265]
train() client id: f_00001-1-7 loss: 0.448200  [  256/  265]
train() client id: f_00001-2-0 loss: 0.422077  [   32/  265]
train() client id: f_00001-2-1 loss: 0.434080  [   64/  265]
train() client id: f_00001-2-2 loss: 0.429412  [   96/  265]
train() client id: f_00001-2-3 loss: 0.510380  [  128/  265]
train() client id: f_00001-2-4 loss: 0.328349  [  160/  265]
train() client id: f_00001-2-5 loss: 0.420439  [  192/  265]
train() client id: f_00001-2-6 loss: 0.436787  [  224/  265]
train() client id: f_00001-2-7 loss: 0.369787  [  256/  265]
train() client id: f_00001-3-0 loss: 0.394168  [   32/  265]
train() client id: f_00001-3-1 loss: 0.437522  [   64/  265]
train() client id: f_00001-3-2 loss: 0.532934  [   96/  265]
train() client id: f_00001-3-3 loss: 0.334715  [  128/  265]
train() client id: f_00001-3-4 loss: 0.430858  [  160/  265]
train() client id: f_00001-3-5 loss: 0.471863  [  192/  265]
train() client id: f_00001-3-6 loss: 0.317938  [  224/  265]
train() client id: f_00001-3-7 loss: 0.431400  [  256/  265]
train() client id: f_00001-4-0 loss: 0.462740  [   32/  265]
train() client id: f_00001-4-1 loss: 0.347092  [   64/  265]
train() client id: f_00001-4-2 loss: 0.550239  [   96/  265]
train() client id: f_00001-4-3 loss: 0.400728  [  128/  265]
train() client id: f_00001-4-4 loss: 0.416430  [  160/  265]
train() client id: f_00001-4-5 loss: 0.370159  [  192/  265]
train() client id: f_00001-4-6 loss: 0.385381  [  224/  265]
train() client id: f_00001-4-7 loss: 0.374587  [  256/  265]
train() client id: f_00001-5-0 loss: 0.553533  [   32/  265]
train() client id: f_00001-5-1 loss: 0.328157  [   64/  265]
train() client id: f_00001-5-2 loss: 0.398870  [   96/  265]
train() client id: f_00001-5-3 loss: 0.446269  [  128/  265]
train() client id: f_00001-5-4 loss: 0.461853  [  160/  265]
train() client id: f_00001-5-5 loss: 0.326824  [  192/  265]
train() client id: f_00001-5-6 loss: 0.377107  [  224/  265]
train() client id: f_00001-5-7 loss: 0.318138  [  256/  265]
train() client id: f_00001-6-0 loss: 0.314718  [   32/  265]
train() client id: f_00001-6-1 loss: 0.371414  [   64/  265]
train() client id: f_00001-6-2 loss: 0.330378  [   96/  265]
train() client id: f_00001-6-3 loss: 0.456094  [  128/  265]
train() client id: f_00001-6-4 loss: 0.303400  [  160/  265]
train() client id: f_00001-6-5 loss: 0.674723  [  192/  265]
train() client id: f_00001-6-6 loss: 0.448488  [  224/  265]
train() client id: f_00001-6-7 loss: 0.348795  [  256/  265]
train() client id: f_00001-7-0 loss: 0.444492  [   32/  265]
train() client id: f_00001-7-1 loss: 0.443118  [   64/  265]
train() client id: f_00001-7-2 loss: 0.414192  [   96/  265]
train() client id: f_00001-7-3 loss: 0.371164  [  128/  265]
train() client id: f_00001-7-4 loss: 0.513021  [  160/  265]
train() client id: f_00001-7-5 loss: 0.351438  [  192/  265]
train() client id: f_00001-7-6 loss: 0.370374  [  224/  265]
train() client id: f_00001-7-7 loss: 0.315671  [  256/  265]
train() client id: f_00001-8-0 loss: 0.401888  [   32/  265]
train() client id: f_00001-8-1 loss: 0.377094  [   64/  265]
train() client id: f_00001-8-2 loss: 0.374080  [   96/  265]
train() client id: f_00001-8-3 loss: 0.371709  [  128/  265]
train() client id: f_00001-8-4 loss: 0.316365  [  160/  265]
train() client id: f_00001-8-5 loss: 0.511883  [  192/  265]
train() client id: f_00001-8-6 loss: 0.359897  [  224/  265]
train() client id: f_00001-8-7 loss: 0.503761  [  256/  265]
train() client id: f_00001-9-0 loss: 0.294847  [   32/  265]
train() client id: f_00001-9-1 loss: 0.292132  [   64/  265]
train() client id: f_00001-9-2 loss: 0.354974  [   96/  265]
train() client id: f_00001-9-3 loss: 0.450041  [  128/  265]
train() client id: f_00001-9-4 loss: 0.393202  [  160/  265]
train() client id: f_00001-9-5 loss: 0.441753  [  192/  265]
train() client id: f_00001-9-6 loss: 0.430886  [  224/  265]
train() client id: f_00001-9-7 loss: 0.531458  [  256/  265]
train() client id: f_00001-10-0 loss: 0.302453  [   32/  265]
train() client id: f_00001-10-1 loss: 0.401648  [   64/  265]
train() client id: f_00001-10-2 loss: 0.464300  [   96/  265]
train() client id: f_00001-10-3 loss: 0.281507  [  128/  265]
train() client id: f_00001-10-4 loss: 0.417320  [  160/  265]
train() client id: f_00001-10-5 loss: 0.300428  [  192/  265]
train() client id: f_00001-10-6 loss: 0.432123  [  224/  265]
train() client id: f_00001-10-7 loss: 0.433595  [  256/  265]
train() client id: f_00001-11-0 loss: 0.475654  [   32/  265]
train() client id: f_00001-11-1 loss: 0.369459  [   64/  265]
train() client id: f_00001-11-2 loss: 0.422354  [   96/  265]
train() client id: f_00001-11-3 loss: 0.328158  [  128/  265]
train() client id: f_00001-11-4 loss: 0.401596  [  160/  265]
train() client id: f_00001-11-5 loss: 0.469101  [  192/  265]
train() client id: f_00001-11-6 loss: 0.311331  [  224/  265]
train() client id: f_00001-11-7 loss: 0.400827  [  256/  265]
train() client id: f_00002-0-0 loss: 1.130650  [   32/  124]
train() client id: f_00002-0-1 loss: 1.024878  [   64/  124]
train() client id: f_00002-0-2 loss: 1.057679  [   96/  124]
train() client id: f_00002-1-0 loss: 0.995582  [   32/  124]
train() client id: f_00002-1-1 loss: 1.108862  [   64/  124]
train() client id: f_00002-1-2 loss: 1.026586  [   96/  124]
train() client id: f_00002-2-0 loss: 1.028263  [   32/  124]
train() client id: f_00002-2-1 loss: 1.018895  [   64/  124]
train() client id: f_00002-2-2 loss: 0.878431  [   96/  124]
train() client id: f_00002-3-0 loss: 0.896150  [   32/  124]
train() client id: f_00002-3-1 loss: 0.973598  [   64/  124]
train() client id: f_00002-3-2 loss: 0.953058  [   96/  124]
train() client id: f_00002-4-0 loss: 1.053988  [   32/  124]
train() client id: f_00002-4-1 loss: 0.941359  [   64/  124]
train() client id: f_00002-4-2 loss: 0.962439  [   96/  124]
train() client id: f_00002-5-0 loss: 0.844051  [   32/  124]
train() client id: f_00002-5-1 loss: 0.957657  [   64/  124]
train() client id: f_00002-5-2 loss: 0.817390  [   96/  124]
train() client id: f_00002-6-0 loss: 0.798005  [   32/  124]
train() client id: f_00002-6-1 loss: 0.813578  [   64/  124]
train() client id: f_00002-6-2 loss: 1.059791  [   96/  124]
train() client id: f_00002-7-0 loss: 0.949821  [   32/  124]
train() client id: f_00002-7-1 loss: 0.913570  [   64/  124]
train() client id: f_00002-7-2 loss: 0.810702  [   96/  124]
train() client id: f_00002-8-0 loss: 0.904433  [   32/  124]
train() client id: f_00002-8-1 loss: 0.650376  [   64/  124]
train() client id: f_00002-8-2 loss: 0.872566  [   96/  124]
train() client id: f_00002-9-0 loss: 0.813803  [   32/  124]
train() client id: f_00002-9-1 loss: 0.924566  [   64/  124]
train() client id: f_00002-9-2 loss: 0.767585  [   96/  124]
train() client id: f_00002-10-0 loss: 0.783333  [   32/  124]
train() client id: f_00002-10-1 loss: 1.023378  [   64/  124]
train() client id: f_00002-10-2 loss: 0.762560  [   96/  124]
train() client id: f_00002-11-0 loss: 0.915161  [   32/  124]
train() client id: f_00002-11-1 loss: 0.852187  [   64/  124]
train() client id: f_00002-11-2 loss: 0.686450  [   96/  124]
train() client id: f_00003-0-0 loss: 0.846586  [   32/   43]
train() client id: f_00003-1-0 loss: 0.803054  [   32/   43]
train() client id: f_00003-2-0 loss: 0.719622  [   32/   43]
train() client id: f_00003-3-0 loss: 0.720129  [   32/   43]
train() client id: f_00003-4-0 loss: 0.758145  [   32/   43]
train() client id: f_00003-5-0 loss: 0.727157  [   32/   43]
train() client id: f_00003-6-0 loss: 0.680140  [   32/   43]
train() client id: f_00003-7-0 loss: 0.780768  [   32/   43]
train() client id: f_00003-8-0 loss: 0.893229  [   32/   43]
train() client id: f_00003-9-0 loss: 0.776871  [   32/   43]
train() client id: f_00003-10-0 loss: 0.847426  [   32/   43]
train() client id: f_00003-11-0 loss: 0.753453  [   32/   43]
train() client id: f_00004-0-0 loss: 0.959520  [   32/  306]
train() client id: f_00004-0-1 loss: 0.998147  [   64/  306]
train() client id: f_00004-0-2 loss: 0.918009  [   96/  306]
train() client id: f_00004-0-3 loss: 1.072932  [  128/  306]
train() client id: f_00004-0-4 loss: 0.899892  [  160/  306]
train() client id: f_00004-0-5 loss: 1.047411  [  192/  306]
train() client id: f_00004-0-6 loss: 0.859614  [  224/  306]
train() client id: f_00004-0-7 loss: 0.807703  [  256/  306]
train() client id: f_00004-0-8 loss: 0.874990  [  288/  306]
train() client id: f_00004-1-0 loss: 0.799769  [   32/  306]
train() client id: f_00004-1-1 loss: 0.857502  [   64/  306]
train() client id: f_00004-1-2 loss: 0.870856  [   96/  306]
train() client id: f_00004-1-3 loss: 1.063288  [  128/  306]
train() client id: f_00004-1-4 loss: 1.045277  [  160/  306]
train() client id: f_00004-1-5 loss: 0.890413  [  192/  306]
train() client id: f_00004-1-6 loss: 0.974083  [  224/  306]
train() client id: f_00004-1-7 loss: 0.855812  [  256/  306]
train() client id: f_00004-1-8 loss: 0.981585  [  288/  306]
train() client id: f_00004-2-0 loss: 1.083324  [   32/  306]
train() client id: f_00004-2-1 loss: 0.768748  [   64/  306]
train() client id: f_00004-2-2 loss: 0.904405  [   96/  306]
train() client id: f_00004-2-3 loss: 0.969315  [  128/  306]
train() client id: f_00004-2-4 loss: 0.846845  [  160/  306]
train() client id: f_00004-2-5 loss: 0.973337  [  192/  306]
train() client id: f_00004-2-6 loss: 0.985640  [  224/  306]
train() client id: f_00004-2-7 loss: 0.890219  [  256/  306]
train() client id: f_00004-2-8 loss: 1.014492  [  288/  306]
train() client id: f_00004-3-0 loss: 0.938099  [   32/  306]
train() client id: f_00004-3-1 loss: 0.916165  [   64/  306]
train() client id: f_00004-3-2 loss: 0.908317  [   96/  306]
train() client id: f_00004-3-3 loss: 0.909338  [  128/  306]
train() client id: f_00004-3-4 loss: 0.796882  [  160/  306]
train() client id: f_00004-3-5 loss: 0.893078  [  192/  306]
train() client id: f_00004-3-6 loss: 1.028237  [  224/  306]
train() client id: f_00004-3-7 loss: 1.071749  [  256/  306]
train() client id: f_00004-3-8 loss: 0.883280  [  288/  306]
train() client id: f_00004-4-0 loss: 0.768475  [   32/  306]
train() client id: f_00004-4-1 loss: 1.059929  [   64/  306]
train() client id: f_00004-4-2 loss: 0.868531  [   96/  306]
train() client id: f_00004-4-3 loss: 0.805663  [  128/  306]
train() client id: f_00004-4-4 loss: 0.863921  [  160/  306]
train() client id: f_00004-4-5 loss: 0.998556  [  192/  306]
train() client id: f_00004-4-6 loss: 0.906730  [  224/  306]
train() client id: f_00004-4-7 loss: 0.979440  [  256/  306]
train() client id: f_00004-4-8 loss: 0.985557  [  288/  306]
train() client id: f_00004-5-0 loss: 0.921093  [   32/  306]
train() client id: f_00004-5-1 loss: 1.042467  [   64/  306]
train() client id: f_00004-5-2 loss: 0.878711  [   96/  306]
train() client id: f_00004-5-3 loss: 0.918409  [  128/  306]
train() client id: f_00004-5-4 loss: 1.021471  [  160/  306]
train() client id: f_00004-5-5 loss: 0.932829  [  192/  306]
train() client id: f_00004-5-6 loss: 0.852693  [  224/  306]
train() client id: f_00004-5-7 loss: 0.824686  [  256/  306]
train() client id: f_00004-5-8 loss: 0.744960  [  288/  306]
train() client id: f_00004-6-0 loss: 0.877073  [   32/  306]
train() client id: f_00004-6-1 loss: 0.910371  [   64/  306]
train() client id: f_00004-6-2 loss: 1.009181  [   96/  306]
train() client id: f_00004-6-3 loss: 0.937845  [  128/  306]
train() client id: f_00004-6-4 loss: 0.765904  [  160/  306]
train() client id: f_00004-6-5 loss: 1.045829  [  192/  306]
train() client id: f_00004-6-6 loss: 0.783775  [  224/  306]
train() client id: f_00004-6-7 loss: 0.976894  [  256/  306]
train() client id: f_00004-6-8 loss: 0.879771  [  288/  306]
train() client id: f_00004-7-0 loss: 0.802934  [   32/  306]
train() client id: f_00004-7-1 loss: 0.902681  [   64/  306]
train() client id: f_00004-7-2 loss: 0.949528  [   96/  306]
train() client id: f_00004-7-3 loss: 0.898001  [  128/  306]
train() client id: f_00004-7-4 loss: 0.985859  [  160/  306]
train() client id: f_00004-7-5 loss: 0.907132  [  192/  306]
train() client id: f_00004-7-6 loss: 0.890917  [  224/  306]
train() client id: f_00004-7-7 loss: 0.960443  [  256/  306]
train() client id: f_00004-7-8 loss: 0.947692  [  288/  306]
train() client id: f_00004-8-0 loss: 0.856293  [   32/  306]
train() client id: f_00004-8-1 loss: 0.868051  [   64/  306]
train() client id: f_00004-8-2 loss: 0.857916  [   96/  306]
train() client id: f_00004-8-3 loss: 0.943683  [  128/  306]
train() client id: f_00004-8-4 loss: 0.926944  [  160/  306]
train() client id: f_00004-8-5 loss: 0.836578  [  192/  306]
train() client id: f_00004-8-6 loss: 0.850681  [  224/  306]
train() client id: f_00004-8-7 loss: 0.948490  [  256/  306]
train() client id: f_00004-8-8 loss: 1.035763  [  288/  306]
train() client id: f_00004-9-0 loss: 0.803097  [   32/  306]
train() client id: f_00004-9-1 loss: 0.830063  [   64/  306]
train() client id: f_00004-9-2 loss: 0.850520  [   96/  306]
train() client id: f_00004-9-3 loss: 0.993845  [  128/  306]
train() client id: f_00004-9-4 loss: 0.880173  [  160/  306]
train() client id: f_00004-9-5 loss: 0.954926  [  192/  306]
train() client id: f_00004-9-6 loss: 0.836311  [  224/  306]
train() client id: f_00004-9-7 loss: 0.931767  [  256/  306]
train() client id: f_00004-9-8 loss: 1.055795  [  288/  306]
train() client id: f_00004-10-0 loss: 0.971052  [   32/  306]
train() client id: f_00004-10-1 loss: 0.905390  [   64/  306]
train() client id: f_00004-10-2 loss: 0.874023  [   96/  306]
train() client id: f_00004-10-3 loss: 1.000707  [  128/  306]
train() client id: f_00004-10-4 loss: 0.850950  [  160/  306]
train() client id: f_00004-10-5 loss: 0.893487  [  192/  306]
train() client id: f_00004-10-6 loss: 0.946436  [  224/  306]
train() client id: f_00004-10-7 loss: 0.834368  [  256/  306]
train() client id: f_00004-10-8 loss: 0.858932  [  288/  306]
train() client id: f_00004-11-0 loss: 0.884214  [   32/  306]
train() client id: f_00004-11-1 loss: 0.966801  [   64/  306]
train() client id: f_00004-11-2 loss: 0.933739  [   96/  306]
train() client id: f_00004-11-3 loss: 0.872625  [  128/  306]
train() client id: f_00004-11-4 loss: 0.989065  [  160/  306]
train() client id: f_00004-11-5 loss: 0.877393  [  192/  306]
train() client id: f_00004-11-6 loss: 0.908343  [  224/  306]
train() client id: f_00004-11-7 loss: 0.923616  [  256/  306]
train() client id: f_00004-11-8 loss: 0.865311  [  288/  306]
train() client id: f_00005-0-0 loss: 0.613104  [   32/  146]
train() client id: f_00005-0-1 loss: 0.715433  [   64/  146]
train() client id: f_00005-0-2 loss: 0.805629  [   96/  146]
train() client id: f_00005-0-3 loss: 0.524082  [  128/  146]
train() client id: f_00005-1-0 loss: 0.578635  [   32/  146]
train() client id: f_00005-1-1 loss: 0.434613  [   64/  146]
train() client id: f_00005-1-2 loss: 0.857007  [   96/  146]
train() client id: f_00005-1-3 loss: 0.817683  [  128/  146]
train() client id: f_00005-2-0 loss: 0.866883  [   32/  146]
train() client id: f_00005-2-1 loss: 0.724220  [   64/  146]
train() client id: f_00005-2-2 loss: 0.683677  [   96/  146]
train() client id: f_00005-2-3 loss: 0.415948  [  128/  146]
train() client id: f_00005-3-0 loss: 0.559813  [   32/  146]
train() client id: f_00005-3-1 loss: 0.563607  [   64/  146]
train() client id: f_00005-3-2 loss: 0.558239  [   96/  146]
train() client id: f_00005-3-3 loss: 0.752175  [  128/  146]
train() client id: f_00005-4-0 loss: 0.603650  [   32/  146]
train() client id: f_00005-4-1 loss: 0.796919  [   64/  146]
train() client id: f_00005-4-2 loss: 0.503285  [   96/  146]
train() client id: f_00005-4-3 loss: 0.645660  [  128/  146]
train() client id: f_00005-5-0 loss: 0.618162  [   32/  146]
train() client id: f_00005-5-1 loss: 0.476329  [   64/  146]
train() client id: f_00005-5-2 loss: 0.541403  [   96/  146]
train() client id: f_00005-5-3 loss: 0.945979  [  128/  146]
train() client id: f_00005-6-0 loss: 0.819581  [   32/  146]
train() client id: f_00005-6-1 loss: 0.524865  [   64/  146]
train() client id: f_00005-6-2 loss: 0.510548  [   96/  146]
train() client id: f_00005-6-3 loss: 0.553593  [  128/  146]
train() client id: f_00005-7-0 loss: 0.485549  [   32/  146]
train() client id: f_00005-7-1 loss: 0.616768  [   64/  146]
train() client id: f_00005-7-2 loss: 0.851429  [   96/  146]
train() client id: f_00005-7-3 loss: 0.617324  [  128/  146]
train() client id: f_00005-8-0 loss: 0.732874  [   32/  146]
train() client id: f_00005-8-1 loss: 0.603044  [   64/  146]
train() client id: f_00005-8-2 loss: 0.680004  [   96/  146]
train() client id: f_00005-8-3 loss: 0.581028  [  128/  146]
train() client id: f_00005-9-0 loss: 0.557468  [   32/  146]
train() client id: f_00005-9-1 loss: 0.724136  [   64/  146]
train() client id: f_00005-9-2 loss: 0.646771  [   96/  146]
train() client id: f_00005-9-3 loss: 0.827922  [  128/  146]
train() client id: f_00005-10-0 loss: 0.482430  [   32/  146]
train() client id: f_00005-10-1 loss: 0.731585  [   64/  146]
train() client id: f_00005-10-2 loss: 0.762897  [   96/  146]
train() client id: f_00005-10-3 loss: 0.676647  [  128/  146]
train() client id: f_00005-11-0 loss: 0.687134  [   32/  146]
train() client id: f_00005-11-1 loss: 0.720906  [   64/  146]
train() client id: f_00005-11-2 loss: 0.604116  [   96/  146]
train() client id: f_00005-11-3 loss: 0.713401  [  128/  146]
train() client id: f_00006-0-0 loss: 0.560535  [   32/   54]
train() client id: f_00006-1-0 loss: 0.573503  [   32/   54]
train() client id: f_00006-2-0 loss: 0.535234  [   32/   54]
train() client id: f_00006-3-0 loss: 0.591540  [   32/   54]
train() client id: f_00006-4-0 loss: 0.553391  [   32/   54]
train() client id: f_00006-5-0 loss: 0.531451  [   32/   54]
train() client id: f_00006-6-0 loss: 0.506235  [   32/   54]
train() client id: f_00006-7-0 loss: 0.495140  [   32/   54]
train() client id: f_00006-8-0 loss: 0.548584  [   32/   54]
train() client id: f_00006-9-0 loss: 0.565321  [   32/   54]
train() client id: f_00006-10-0 loss: 0.496807  [   32/   54]
train() client id: f_00006-11-0 loss: 0.608017  [   32/   54]
train() client id: f_00007-0-0 loss: 0.702095  [   32/  179]
train() client id: f_00007-0-1 loss: 0.764802  [   64/  179]
train() client id: f_00007-0-2 loss: 0.745826  [   96/  179]
train() client id: f_00007-0-3 loss: 0.633853  [  128/  179]
train() client id: f_00007-0-4 loss: 0.450031  [  160/  179]
train() client id: f_00007-1-0 loss: 0.502665  [   32/  179]
train() client id: f_00007-1-1 loss: 0.709768  [   64/  179]
train() client id: f_00007-1-2 loss: 0.528846  [   96/  179]
train() client id: f_00007-1-3 loss: 0.594564  [  128/  179]
train() client id: f_00007-1-4 loss: 0.548623  [  160/  179]
train() client id: f_00007-2-0 loss: 0.546110  [   32/  179]
train() client id: f_00007-2-1 loss: 0.474271  [   64/  179]
train() client id: f_00007-2-2 loss: 0.871884  [   96/  179]
train() client id: f_00007-2-3 loss: 0.730864  [  128/  179]
train() client id: f_00007-2-4 loss: 0.480897  [  160/  179]
train() client id: f_00007-3-0 loss: 0.750754  [   32/  179]
train() client id: f_00007-3-1 loss: 0.718784  [   64/  179]
train() client id: f_00007-3-2 loss: 0.645878  [   96/  179]
train() client id: f_00007-3-3 loss: 0.577901  [  128/  179]
train() client id: f_00007-3-4 loss: 0.445634  [  160/  179]
train() client id: f_00007-4-0 loss: 0.625912  [   32/  179]
train() client id: f_00007-4-1 loss: 0.490100  [   64/  179]
train() client id: f_00007-4-2 loss: 0.633890  [   96/  179]
train() client id: f_00007-4-3 loss: 0.529208  [  128/  179]
train() client id: f_00007-4-4 loss: 0.644895  [  160/  179]
train() client id: f_00007-5-0 loss: 0.610685  [   32/  179]
train() client id: f_00007-5-1 loss: 0.470175  [   64/  179]
train() client id: f_00007-5-2 loss: 0.593750  [   96/  179]
train() client id: f_00007-5-3 loss: 0.638437  [  128/  179]
train() client id: f_00007-5-4 loss: 0.787518  [  160/  179]
train() client id: f_00007-6-0 loss: 0.651872  [   32/  179]
train() client id: f_00007-6-1 loss: 0.700273  [   64/  179]
train() client id: f_00007-6-2 loss: 0.519954  [   96/  179]
train() client id: f_00007-6-3 loss: 0.598821  [  128/  179]
train() client id: f_00007-6-4 loss: 0.637620  [  160/  179]
train() client id: f_00007-7-0 loss: 0.453007  [   32/  179]
train() client id: f_00007-7-1 loss: 0.552854  [   64/  179]
train() client id: f_00007-7-2 loss: 0.552356  [   96/  179]
train() client id: f_00007-7-3 loss: 0.618493  [  128/  179]
train() client id: f_00007-7-4 loss: 0.799764  [  160/  179]
train() client id: f_00007-8-0 loss: 0.660431  [   32/  179]
train() client id: f_00007-8-1 loss: 0.685336  [   64/  179]
train() client id: f_00007-8-2 loss: 0.551501  [   96/  179]
train() client id: f_00007-8-3 loss: 0.697446  [  128/  179]
train() client id: f_00007-8-4 loss: 0.461290  [  160/  179]
train() client id: f_00007-9-0 loss: 0.456134  [   32/  179]
train() client id: f_00007-9-1 loss: 0.728374  [   64/  179]
train() client id: f_00007-9-2 loss: 0.551269  [   96/  179]
train() client id: f_00007-9-3 loss: 0.624663  [  128/  179]
train() client id: f_00007-9-4 loss: 0.538674  [  160/  179]
train() client id: f_00007-10-0 loss: 0.554197  [   32/  179]
train() client id: f_00007-10-1 loss: 0.528574  [   64/  179]
train() client id: f_00007-10-2 loss: 0.558491  [   96/  179]
train() client id: f_00007-10-3 loss: 0.883918  [  128/  179]
train() client id: f_00007-10-4 loss: 0.463463  [  160/  179]
train() client id: f_00007-11-0 loss: 0.712663  [   32/  179]
train() client id: f_00007-11-1 loss: 0.565947  [   64/  179]
train() client id: f_00007-11-2 loss: 0.701173  [   96/  179]
train() client id: f_00007-11-3 loss: 0.558197  [  128/  179]
train() client id: f_00007-11-4 loss: 0.547982  [  160/  179]
train() client id: f_00008-0-0 loss: 0.767088  [   32/  130]
train() client id: f_00008-0-1 loss: 0.848402  [   64/  130]
train() client id: f_00008-0-2 loss: 0.804168  [   96/  130]
train() client id: f_00008-0-3 loss: 0.895040  [  128/  130]
train() client id: f_00008-1-0 loss: 0.927090  [   32/  130]
train() client id: f_00008-1-1 loss: 0.792550  [   64/  130]
train() client id: f_00008-1-2 loss: 0.768028  [   96/  130]
train() client id: f_00008-1-3 loss: 0.780240  [  128/  130]
train() client id: f_00008-2-0 loss: 0.875427  [   32/  130]
train() client id: f_00008-2-1 loss: 0.755393  [   64/  130]
train() client id: f_00008-2-2 loss: 0.758289  [   96/  130]
train() client id: f_00008-2-3 loss: 0.880749  [  128/  130]
train() client id: f_00008-3-0 loss: 0.809865  [   32/  130]
train() client id: f_00008-3-1 loss: 0.908936  [   64/  130]
train() client id: f_00008-3-2 loss: 0.787828  [   96/  130]
train() client id: f_00008-3-3 loss: 0.810185  [  128/  130]
train() client id: f_00008-4-0 loss: 0.867268  [   32/  130]
train() client id: f_00008-4-1 loss: 0.793849  [   64/  130]
train() client id: f_00008-4-2 loss: 0.844829  [   96/  130]
train() client id: f_00008-4-3 loss: 0.800795  [  128/  130]
train() client id: f_00008-5-0 loss: 0.712871  [   32/  130]
train() client id: f_00008-5-1 loss: 0.764014  [   64/  130]
train() client id: f_00008-5-2 loss: 0.972049  [   96/  130]
train() client id: f_00008-5-3 loss: 0.860629  [  128/  130]
train() client id: f_00008-6-0 loss: 0.785184  [   32/  130]
train() client id: f_00008-6-1 loss: 0.836713  [   64/  130]
train() client id: f_00008-6-2 loss: 0.784148  [   96/  130]
train() client id: f_00008-6-3 loss: 0.904193  [  128/  130]
train() client id: f_00008-7-0 loss: 0.865647  [   32/  130]
train() client id: f_00008-7-1 loss: 0.728078  [   64/  130]
train() client id: f_00008-7-2 loss: 0.985194  [   96/  130]
train() client id: f_00008-7-3 loss: 0.732713  [  128/  130]
train() client id: f_00008-8-0 loss: 0.761659  [   32/  130]
train() client id: f_00008-8-1 loss: 0.782352  [   64/  130]
train() client id: f_00008-8-2 loss: 0.949889  [   96/  130]
train() client id: f_00008-8-3 loss: 0.814466  [  128/  130]
train() client id: f_00008-9-0 loss: 0.874137  [   32/  130]
train() client id: f_00008-9-1 loss: 0.878341  [   64/  130]
train() client id: f_00008-9-2 loss: 0.769900  [   96/  130]
train() client id: f_00008-9-3 loss: 0.785980  [  128/  130]
train() client id: f_00008-10-0 loss: 0.935592  [   32/  130]
train() client id: f_00008-10-1 loss: 0.855323  [   64/  130]
train() client id: f_00008-10-2 loss: 0.783280  [   96/  130]
train() client id: f_00008-10-3 loss: 0.737852  [  128/  130]
train() client id: f_00008-11-0 loss: 0.832306  [   32/  130]
train() client id: f_00008-11-1 loss: 0.698908  [   64/  130]
train() client id: f_00008-11-2 loss: 0.841752  [   96/  130]
train() client id: f_00008-11-3 loss: 0.905456  [  128/  130]
train() client id: f_00009-0-0 loss: 0.949531  [   32/  118]
train() client id: f_00009-0-1 loss: 0.936561  [   64/  118]
train() client id: f_00009-0-2 loss: 0.875956  [   96/  118]
train() client id: f_00009-1-0 loss: 0.897248  [   32/  118]
train() client id: f_00009-1-1 loss: 0.988857  [   64/  118]
train() client id: f_00009-1-2 loss: 1.033267  [   96/  118]
train() client id: f_00009-2-0 loss: 1.074817  [   32/  118]
train() client id: f_00009-2-1 loss: 0.821149  [   64/  118]
train() client id: f_00009-2-2 loss: 0.835804  [   96/  118]
train() client id: f_00009-3-0 loss: 0.915638  [   32/  118]
train() client id: f_00009-3-1 loss: 0.806827  [   64/  118]
train() client id: f_00009-3-2 loss: 0.822701  [   96/  118]
train() client id: f_00009-4-0 loss: 0.774115  [   32/  118]
train() client id: f_00009-4-1 loss: 0.954193  [   64/  118]
train() client id: f_00009-4-2 loss: 0.807799  [   96/  118]
train() client id: f_00009-5-0 loss: 0.830696  [   32/  118]
train() client id: f_00009-5-1 loss: 0.727625  [   64/  118]
train() client id: f_00009-5-2 loss: 0.846239  [   96/  118]
train() client id: f_00009-6-0 loss: 0.771215  [   32/  118]
train() client id: f_00009-6-1 loss: 0.890374  [   64/  118]
train() client id: f_00009-6-2 loss: 0.658007  [   96/  118]
train() client id: f_00009-7-0 loss: 0.807068  [   32/  118]
train() client id: f_00009-7-1 loss: 0.785306  [   64/  118]
train() client id: f_00009-7-2 loss: 0.758235  [   96/  118]
train() client id: f_00009-8-0 loss: 0.672642  [   32/  118]
train() client id: f_00009-8-1 loss: 0.969182  [   64/  118]
train() client id: f_00009-8-2 loss: 0.777118  [   96/  118]
train() client id: f_00009-9-0 loss: 0.660391  [   32/  118]
train() client id: f_00009-9-1 loss: 0.659080  [   64/  118]
train() client id: f_00009-9-2 loss: 0.868522  [   96/  118]
train() client id: f_00009-10-0 loss: 0.759463  [   32/  118]
train() client id: f_00009-10-1 loss: 0.641508  [   64/  118]
train() client id: f_00009-10-2 loss: 0.848013  [   96/  118]
train() client id: f_00009-11-0 loss: 0.881575  [   32/  118]
train() client id: f_00009-11-1 loss: 0.590129  [   64/  118]
train() client id: f_00009-11-2 loss: 0.794526  [   96/  118]
At round 28 accuracy: 0.649867374005305
At round 28 training accuracy: 0.5828303152246814
At round 28 training loss: 0.8305658056238521
update_location
xs = -4.528292 41.001589 50.045120 -50.943528 -50.103519 4.782049 -92.215960 113.375741 -1.680116 34.695607 
ys = 127.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 -3.124922 -19.177652 -57.154970 4.001482 
xs mean: 4.442869159412867
ys mean: 8.371751218646876
dists_uav = 162.170259 109.193015 111.831382 114.470573 112.239871 101.578646 136.064501 152.387142 115.193374 105.923543 
uav_gains = -105.261825 -100.954928 -101.214171 -101.467456 -101.253762 -100.170080 -103.344847 -104.579311 -101.535808 -100.624849 
uav_gains_db_mean: -102.0407037338166
dists_bs = 176.942094 268.475562 284.270707 196.551746 207.458409 263.167220 196.422211 347.657200 289.716607 270.578530 
bs_gains = -102.506332 -107.576392 -108.271558 -103.784413 -104.441128 -107.333549 -103.776396 -110.719294 -108.502314 -107.671272 
bs_gains_db_mean: -106.45826485041565
Round 29
-------------------------------
ene_coms = [0.00805503 0.00916518 0.00663667 0.00671086 0.00772159 0.00903511
 0.00731608 0.00777584 0.00969569 0.00921698]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.15319903 14.90109161  7.01586831  2.5142016  17.1537954   8.28312527
  3.13688092 10.09448246  7.40581356  6.72987324]
obj_prev = 84.38833139020676
eta_min = 1.2809982938785055e-13	eta_max = 0.9371429932870347
af = 17.81916621666106	bf = 1.4841550761709479	zeta = 19.601082838327166	eta = 0.9090909090909091
af = 17.81916621666106	bf = 1.4841550761709479	zeta = 35.007533134684714	eta = 0.5090094794199084
af = 17.81916621666106	bf = 1.4841550761709479	zeta = 27.530542876044414	eta = 0.6472508114675186
af = 17.81916621666106	bf = 1.4841550761709479	zeta = 26.18255159491219	eta = 0.6805740896591489
af = 17.81916621666106	bf = 1.4841550761709479	zeta = 26.113415286317753	eta = 0.6823759367085735
af = 17.81916621666106	bf = 1.4841550761709479	zeta = 26.11321935774061	eta = 0.68238105660377
eta = 0.68238105660377
ene_coms = [0.00805503 0.00916518 0.00663667 0.00671086 0.00772159 0.00903511
 0.00731608 0.00777584 0.00969569 0.00921698]
ene_comp = [0.03153551 0.06632468 0.03103494 0.01076212 0.07658623 0.03654114
 0.01351522 0.04480044 0.03253664 0.02953325]
ene_total = [2.27467143 4.33726445 2.16442009 1.00390863 4.84390006 2.6185804
 1.19686089 3.02076651 2.42645517 2.22639173]
ti_comp = [0.41635885 0.40525733 0.4305424  0.42980055 0.41969321 0.40655804
 0.4237483  0.41915075 0.39995224 0.40473936]
ti_coms = [0.08055029 0.0916518  0.06636674 0.06710859 0.07721592 0.0903511
 0.07316083 0.07775838 0.09695689 0.09216978]
t_total = [28.54987831 28.54987831 28.54987831 28.54987831 28.54987831 28.54987831
 28.54987831 28.54987831 28.54987831 28.54987831]
ene_coms = [0.00805503 0.00916518 0.00663667 0.00671086 0.00772159 0.00903511
 0.00731608 0.00777584 0.00969569 0.00921698]
ene_comp = [1.13069098e-05 1.11030871e-04 1.00786204e-05 4.21734698e-07
 1.59392645e-04 1.84493623e-05 8.59277078e-07 3.19879555e-05
 1.34580358e-05 9.82795211e-06]
ene_total = [0.46345072 0.53296403 0.38188866 0.38559615 0.45280141 0.5201716
 0.4203944  0.44859809 0.55783836 0.53012544]
optimize_network iter = 0 obj = 4.693828882306191
eta = 0.68238105660377
freqs = [37870587.35899329 81830326.86824135 36041682.14759794 12519897.95223852
 91240734.96145466 44939641.35659925 15947223.58507791 53441922.90468411
 40675651.03723782 36484286.87177813]
eta_min = 0.6823810566037709	eta_max = 0.6823810566037626
af = 0.0175156231544447	bf = 1.4841550761709479	zeta = 0.019267185469889172	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00805503 0.00916518 0.00663667 0.00671086 0.00772159 0.00903511
 0.00731608 0.00777584 0.00969569 0.00921698]
ene_comp = [2.81987582e-06 2.76904366e-05 2.51354779e-06 1.05178117e-07
 3.97515743e-05 4.60116086e-06 2.14298575e-07 7.97760521e-06
 3.35635380e-06 2.45103260e-06]
ene_total = [1.61750437 1.84534473 1.33272722 1.34713552 1.55798504 1.81460005
 1.46864802 1.56249602 1.94695257 1.85067589]
ti_comp = [0.41635885 0.40525733 0.4305424  0.42980055 0.41969321 0.40655804
 0.4237483  0.41915075 0.39995224 0.40473936]
ti_coms = [0.08055029 0.0916518  0.06636674 0.06710859 0.07721592 0.0903511
 0.07316083 0.07775838 0.09695689 0.09216978]
t_total = [28.54987831 28.54987831 28.54987831 28.54987831 28.54987831 28.54987831
 28.54987831 28.54987831 28.54987831 28.54987831]
ene_coms = [0.00805503 0.00916518 0.00663667 0.00671086 0.00772159 0.00903511
 0.00731608 0.00777584 0.00969569 0.00921698]
ene_comp = [1.13069098e-05 1.11030871e-04 1.00786204e-05 4.21734698e-07
 1.59392645e-04 1.84493623e-05 8.59277078e-07 3.19879555e-05
 1.34580358e-05 9.82795211e-06]
ene_total = [0.46345072 0.53296403 0.38188866 0.38559615 0.45280141 0.5201716
 0.4203944  0.44859809 0.55783836 0.53012544]
optimize_network iter = 1 obj = 4.6938288823060805
eta = 0.6823810566037626
freqs = [37870587.35899331 81830326.8682413  36041682.14759798 12519897.95223854
 91240734.96145469 44939641.35659924 15947223.58507792 53441922.90468413
 40675651.03723779 36484286.87177811]
Done!
ene_coms = [0.00805503 0.00916518 0.00663667 0.00671086 0.00772159 0.00903511
 0.00731608 0.00777584 0.00969569 0.00921698]
ene_comp = [1.08424113e-05 1.06469618e-04 9.66458122e-06 4.04409439e-07
 1.52844645e-04 1.76914452e-05 8.23977166e-07 3.06738602e-05
 1.29051670e-05 9.42421057e-06]
ene_total = [0.00806587 0.00927165 0.00664634 0.00671126 0.00787444 0.0090528
 0.00731691 0.00780651 0.00970859 0.0092264 ]
At round 29 energy consumption: 0.08168077681639271
At round 29 eta: 0.6823810566037626
At round 29 a_n: 18.248773294144172
At round 29 local rounds: 12.51409063413464
At round 29 global rounds: 57.45492727547544
gradient difference: 0.47343817353248596
train() client id: f_00000-0-0 loss: 1.269063  [   32/  126]
train() client id: f_00000-0-1 loss: 1.190477  [   64/  126]
train() client id: f_00000-0-2 loss: 1.080550  [   96/  126]
train() client id: f_00000-1-0 loss: 1.147162  [   32/  126]
train() client id: f_00000-1-1 loss: 0.914551  [   64/  126]
train() client id: f_00000-1-2 loss: 1.195246  [   96/  126]
train() client id: f_00000-2-0 loss: 0.998741  [   32/  126]
train() client id: f_00000-2-1 loss: 0.971300  [   64/  126]
train() client id: f_00000-2-2 loss: 1.006928  [   96/  126]
train() client id: f_00000-3-0 loss: 0.961489  [   32/  126]
train() client id: f_00000-3-1 loss: 0.905781  [   64/  126]
train() client id: f_00000-3-2 loss: 0.914282  [   96/  126]
train() client id: f_00000-4-0 loss: 0.891515  [   32/  126]
train() client id: f_00000-4-1 loss: 1.070876  [   64/  126]
train() client id: f_00000-4-2 loss: 0.933372  [   96/  126]
train() client id: f_00000-5-0 loss: 1.144898  [   32/  126]
train() client id: f_00000-5-1 loss: 0.749845  [   64/  126]
train() client id: f_00000-5-2 loss: 0.901928  [   96/  126]
train() client id: f_00000-6-0 loss: 0.812636  [   32/  126]
train() client id: f_00000-6-1 loss: 0.921029  [   64/  126]
train() client id: f_00000-6-2 loss: 0.857526  [   96/  126]
train() client id: f_00000-7-0 loss: 0.798167  [   32/  126]
train() client id: f_00000-7-1 loss: 0.932708  [   64/  126]
train() client id: f_00000-7-2 loss: 0.958101  [   96/  126]
train() client id: f_00000-8-0 loss: 0.773610  [   32/  126]
train() client id: f_00000-8-1 loss: 1.019351  [   64/  126]
train() client id: f_00000-8-2 loss: 0.802618  [   96/  126]
train() client id: f_00000-9-0 loss: 0.919319  [   32/  126]
train() client id: f_00000-9-1 loss: 0.949859  [   64/  126]
train() client id: f_00000-9-2 loss: 0.888044  [   96/  126]
train() client id: f_00000-10-0 loss: 0.946925  [   32/  126]
train() client id: f_00000-10-1 loss: 0.962776  [   64/  126]
train() client id: f_00000-10-2 loss: 0.892020  [   96/  126]
train() client id: f_00000-11-0 loss: 0.890984  [   32/  126]
train() client id: f_00000-11-1 loss: 0.948053  [   64/  126]
train() client id: f_00000-11-2 loss: 0.948469  [   96/  126]
train() client id: f_00001-0-0 loss: 0.668084  [   32/  265]
train() client id: f_00001-0-1 loss: 0.425027  [   64/  265]
train() client id: f_00001-0-2 loss: 0.557129  [   96/  265]
train() client id: f_00001-0-3 loss: 0.370162  [  128/  265]
train() client id: f_00001-0-4 loss: 0.373630  [  160/  265]
train() client id: f_00001-0-5 loss: 0.505393  [  192/  265]
train() client id: f_00001-0-6 loss: 0.431265  [  224/  265]
train() client id: f_00001-0-7 loss: 0.382150  [  256/  265]
train() client id: f_00001-1-0 loss: 0.519326  [   32/  265]
train() client id: f_00001-1-1 loss: 0.527526  [   64/  265]
train() client id: f_00001-1-2 loss: 0.442086  [   96/  265]
train() client id: f_00001-1-3 loss: 0.428695  [  128/  265]
train() client id: f_00001-1-4 loss: 0.428916  [  160/  265]
train() client id: f_00001-1-5 loss: 0.462571  [  192/  265]
train() client id: f_00001-1-6 loss: 0.373146  [  224/  265]
train() client id: f_00001-1-7 loss: 0.434505  [  256/  265]
train() client id: f_00001-2-0 loss: 0.436423  [   32/  265]
train() client id: f_00001-2-1 loss: 0.577458  [   64/  265]
train() client id: f_00001-2-2 loss: 0.569790  [   96/  265]
train() client id: f_00001-2-3 loss: 0.428089  [  128/  265]
train() client id: f_00001-2-4 loss: 0.386714  [  160/  265]
train() client id: f_00001-2-5 loss: 0.414521  [  192/  265]
train() client id: f_00001-2-6 loss: 0.357417  [  224/  265]
train() client id: f_00001-2-7 loss: 0.408507  [  256/  265]
train() client id: f_00001-3-0 loss: 0.467818  [   32/  265]
train() client id: f_00001-3-1 loss: 0.334912  [   64/  265]
train() client id: f_00001-3-2 loss: 0.595949  [   96/  265]
train() client id: f_00001-3-3 loss: 0.427063  [  128/  265]
train() client id: f_00001-3-4 loss: 0.381544  [  160/  265]
train() client id: f_00001-3-5 loss: 0.360954  [  192/  265]
train() client id: f_00001-3-6 loss: 0.453087  [  224/  265]
train() client id: f_00001-3-7 loss: 0.502855  [  256/  265]
train() client id: f_00001-4-0 loss: 0.368136  [   32/  265]
train() client id: f_00001-4-1 loss: 0.489596  [   64/  265]
train() client id: f_00001-4-2 loss: 0.426350  [   96/  265]
train() client id: f_00001-4-3 loss: 0.523390  [  128/  265]
train() client id: f_00001-4-4 loss: 0.370767  [  160/  265]
train() client id: f_00001-4-5 loss: 0.488627  [  192/  265]
train() client id: f_00001-4-6 loss: 0.319677  [  224/  265]
train() client id: f_00001-4-7 loss: 0.414654  [  256/  265]
train() client id: f_00001-5-0 loss: 0.346194  [   32/  265]
train() client id: f_00001-5-1 loss: 0.503130  [   64/  265]
train() client id: f_00001-5-2 loss: 0.417507  [   96/  265]
train() client id: f_00001-5-3 loss: 0.340968  [  128/  265]
train() client id: f_00001-5-4 loss: 0.378487  [  160/  265]
train() client id: f_00001-5-5 loss: 0.535888  [  192/  265]
train() client id: f_00001-5-6 loss: 0.485731  [  224/  265]
train() client id: f_00001-5-7 loss: 0.443878  [  256/  265]
train() client id: f_00001-6-0 loss: 0.421767  [   32/  265]
train() client id: f_00001-6-1 loss: 0.493862  [   64/  265]
train() client id: f_00001-6-2 loss: 0.383173  [   96/  265]
train() client id: f_00001-6-3 loss: 0.347035  [  128/  265]
train() client id: f_00001-6-4 loss: 0.514769  [  160/  265]
train() client id: f_00001-6-5 loss: 0.377321  [  192/  265]
train() client id: f_00001-6-6 loss: 0.457643  [  224/  265]
train() client id: f_00001-6-7 loss: 0.389581  [  256/  265]
train() client id: f_00001-7-0 loss: 0.347165  [   32/  265]
train() client id: f_00001-7-1 loss: 0.386556  [   64/  265]
train() client id: f_00001-7-2 loss: 0.405901  [   96/  265]
train() client id: f_00001-7-3 loss: 0.414102  [  128/  265]
train() client id: f_00001-7-4 loss: 0.362578  [  160/  265]
train() client id: f_00001-7-5 loss: 0.389927  [  192/  265]
train() client id: f_00001-7-6 loss: 0.570306  [  224/  265]
train() client id: f_00001-7-7 loss: 0.539556  [  256/  265]
train() client id: f_00001-8-0 loss: 0.443374  [   32/  265]
train() client id: f_00001-8-1 loss: 0.402701  [   64/  265]
train() client id: f_00001-8-2 loss: 0.375105  [   96/  265]
train() client id: f_00001-8-3 loss: 0.473601  [  128/  265]
train() client id: f_00001-8-4 loss: 0.310373  [  160/  265]
train() client id: f_00001-8-5 loss: 0.383859  [  192/  265]
train() client id: f_00001-8-6 loss: 0.409000  [  224/  265]
train() client id: f_00001-8-7 loss: 0.423674  [  256/  265]
train() client id: f_00001-9-0 loss: 0.319831  [   32/  265]
train() client id: f_00001-9-1 loss: 0.371604  [   64/  265]
train() client id: f_00001-9-2 loss: 0.513961  [   96/  265]
train() client id: f_00001-9-3 loss: 0.382940  [  128/  265]
train() client id: f_00001-9-4 loss: 0.437853  [  160/  265]
train() client id: f_00001-9-5 loss: 0.326820  [  192/  265]
train() client id: f_00001-9-6 loss: 0.534428  [  224/  265]
train() client id: f_00001-9-7 loss: 0.506111  [  256/  265]
train() client id: f_00001-10-0 loss: 0.443554  [   32/  265]
train() client id: f_00001-10-1 loss: 0.479713  [   64/  265]
train() client id: f_00001-10-2 loss: 0.447280  [   96/  265]
train() client id: f_00001-10-3 loss: 0.426477  [  128/  265]
train() client id: f_00001-10-4 loss: 0.372364  [  160/  265]
train() client id: f_00001-10-5 loss: 0.336747  [  192/  265]
train() client id: f_00001-10-6 loss: 0.326443  [  224/  265]
train() client id: f_00001-10-7 loss: 0.548466  [  256/  265]
train() client id: f_00001-11-0 loss: 0.405275  [   32/  265]
train() client id: f_00001-11-1 loss: 0.402275  [   64/  265]
train() client id: f_00001-11-2 loss: 0.465577  [   96/  265]
train() client id: f_00001-11-3 loss: 0.451775  [  128/  265]
train() client id: f_00001-11-4 loss: 0.470462  [  160/  265]
train() client id: f_00001-11-5 loss: 0.381610  [  192/  265]
train() client id: f_00001-11-6 loss: 0.387146  [  224/  265]
train() client id: f_00001-11-7 loss: 0.329881  [  256/  265]
train() client id: f_00002-0-0 loss: 1.230541  [   32/  124]
train() client id: f_00002-0-1 loss: 1.223050  [   64/  124]
train() client id: f_00002-0-2 loss: 1.244389  [   96/  124]
train() client id: f_00002-1-0 loss: 1.403200  [   32/  124]
train() client id: f_00002-1-1 loss: 1.197718  [   64/  124]
train() client id: f_00002-1-2 loss: 1.021786  [   96/  124]
train() client id: f_00002-2-0 loss: 1.044111  [   32/  124]
train() client id: f_00002-2-1 loss: 1.004052  [   64/  124]
train() client id: f_00002-2-2 loss: 1.237891  [   96/  124]
train() client id: f_00002-3-0 loss: 1.169824  [   32/  124]
train() client id: f_00002-3-1 loss: 1.095636  [   64/  124]
train() client id: f_00002-3-2 loss: 1.213232  [   96/  124]
train() client id: f_00002-4-0 loss: 1.199069  [   32/  124]
train() client id: f_00002-4-1 loss: 1.127603  [   64/  124]
train() client id: f_00002-4-2 loss: 1.141625  [   96/  124]
train() client id: f_00002-5-0 loss: 1.086604  [   32/  124]
train() client id: f_00002-5-1 loss: 1.373299  [   64/  124]
train() client id: f_00002-5-2 loss: 0.960032  [   96/  124]
train() client id: f_00002-6-0 loss: 1.154688  [   32/  124]
train() client id: f_00002-6-1 loss: 1.066882  [   64/  124]
train() client id: f_00002-6-2 loss: 1.219885  [   96/  124]
train() client id: f_00002-7-0 loss: 0.989576  [   32/  124]
train() client id: f_00002-7-1 loss: 1.196157  [   64/  124]
train() client id: f_00002-7-2 loss: 1.208231  [   96/  124]
train() client id: f_00002-8-0 loss: 0.840039  [   32/  124]
train() client id: f_00002-8-1 loss: 1.063266  [   64/  124]
train() client id: f_00002-8-2 loss: 1.409187  [   96/  124]
train() client id: f_00002-9-0 loss: 1.195779  [   32/  124]
train() client id: f_00002-9-1 loss: 1.029151  [   64/  124]
train() client id: f_00002-9-2 loss: 1.205471  [   96/  124]
train() client id: f_00002-10-0 loss: 1.089356  [   32/  124]
train() client id: f_00002-10-1 loss: 1.197718  [   64/  124]
train() client id: f_00002-10-2 loss: 1.092171  [   96/  124]
train() client id: f_00002-11-0 loss: 1.180874  [   32/  124]
train() client id: f_00002-11-1 loss: 1.092252  [   64/  124]
train() client id: f_00002-11-2 loss: 1.076238  [   96/  124]
train() client id: f_00003-0-0 loss: 0.578152  [   32/   43]
train() client id: f_00003-1-0 loss: 0.736744  [   32/   43]
train() client id: f_00003-2-0 loss: 0.790136  [   32/   43]
train() client id: f_00003-3-0 loss: 0.690399  [   32/   43]
train() client id: f_00003-4-0 loss: 0.779942  [   32/   43]
train() client id: f_00003-5-0 loss: 0.756881  [   32/   43]
train() client id: f_00003-6-0 loss: 0.726268  [   32/   43]
train() client id: f_00003-7-0 loss: 0.836796  [   32/   43]
train() client id: f_00003-8-0 loss: 0.929447  [   32/   43]
train() client id: f_00003-9-0 loss: 0.659940  [   32/   43]
train() client id: f_00003-10-0 loss: 0.753535  [   32/   43]
train() client id: f_00003-11-0 loss: 0.753216  [   32/   43]
train() client id: f_00004-0-0 loss: 0.861419  [   32/  306]
train() client id: f_00004-0-1 loss: 1.011807  [   64/  306]
train() client id: f_00004-0-2 loss: 0.940628  [   96/  306]
train() client id: f_00004-0-3 loss: 0.863443  [  128/  306]
train() client id: f_00004-0-4 loss: 0.949127  [  160/  306]
train() client id: f_00004-0-5 loss: 0.942556  [  192/  306]
train() client id: f_00004-0-6 loss: 1.067237  [  224/  306]
train() client id: f_00004-0-7 loss: 0.826210  [  256/  306]
train() client id: f_00004-0-8 loss: 0.694741  [  288/  306]
train() client id: f_00004-1-0 loss: 1.034046  [   32/  306]
train() client id: f_00004-1-1 loss: 0.920734  [   64/  306]
train() client id: f_00004-1-2 loss: 1.042109  [   96/  306]
train() client id: f_00004-1-3 loss: 0.854236  [  128/  306]
train() client id: f_00004-1-4 loss: 0.959165  [  160/  306]
train() client id: f_00004-1-5 loss: 0.843645  [  192/  306]
train() client id: f_00004-1-6 loss: 0.834337  [  224/  306]
train() client id: f_00004-1-7 loss: 0.838896  [  256/  306]
train() client id: f_00004-1-8 loss: 0.728860  [  288/  306]
train() client id: f_00004-2-0 loss: 0.937764  [   32/  306]
train() client id: f_00004-2-1 loss: 0.872452  [   64/  306]
train() client id: f_00004-2-2 loss: 0.982646  [   96/  306]
train() client id: f_00004-2-3 loss: 0.875948  [  128/  306]
train() client id: f_00004-2-4 loss: 0.824902  [  160/  306]
train() client id: f_00004-2-5 loss: 0.967261  [  192/  306]
train() client id: f_00004-2-6 loss: 0.835447  [  224/  306]
train() client id: f_00004-2-7 loss: 0.857224  [  256/  306]
train() client id: f_00004-2-8 loss: 0.871669  [  288/  306]
train() client id: f_00004-3-0 loss: 0.861196  [   32/  306]
train() client id: f_00004-3-1 loss: 1.000454  [   64/  306]
train() client id: f_00004-3-2 loss: 0.962585  [   96/  306]
train() client id: f_00004-3-3 loss: 0.941134  [  128/  306]
train() client id: f_00004-3-4 loss: 0.869543  [  160/  306]
train() client id: f_00004-3-5 loss: 0.704558  [  192/  306]
train() client id: f_00004-3-6 loss: 1.041546  [  224/  306]
train() client id: f_00004-3-7 loss: 0.861397  [  256/  306]
train() client id: f_00004-3-8 loss: 0.794530  [  288/  306]
train() client id: f_00004-4-0 loss: 0.791130  [   32/  306]
train() client id: f_00004-4-1 loss: 0.833223  [   64/  306]
train() client id: f_00004-4-2 loss: 0.901370  [   96/  306]
train() client id: f_00004-4-3 loss: 0.897387  [  128/  306]
train() client id: f_00004-4-4 loss: 0.799804  [  160/  306]
train() client id: f_00004-4-5 loss: 0.939658  [  192/  306]
train() client id: f_00004-4-6 loss: 0.881488  [  224/  306]
train() client id: f_00004-4-7 loss: 0.967001  [  256/  306]
train() client id: f_00004-4-8 loss: 0.991609  [  288/  306]
train() client id: f_00004-5-0 loss: 0.998219  [   32/  306]
train() client id: f_00004-5-1 loss: 1.031723  [   64/  306]
train() client id: f_00004-5-2 loss: 0.880073  [   96/  306]
train() client id: f_00004-5-3 loss: 0.778506  [  128/  306]
train() client id: f_00004-5-4 loss: 0.934618  [  160/  306]
train() client id: f_00004-5-5 loss: 0.832271  [  192/  306]
train() client id: f_00004-5-6 loss: 0.925916  [  224/  306]
train() client id: f_00004-5-7 loss: 0.889412  [  256/  306]
train() client id: f_00004-5-8 loss: 0.876234  [  288/  306]
train() client id: f_00004-6-0 loss: 0.835854  [   32/  306]
train() client id: f_00004-6-1 loss: 0.790468  [   64/  306]
train() client id: f_00004-6-2 loss: 1.033217  [   96/  306]
train() client id: f_00004-6-3 loss: 0.828291  [  128/  306]
train() client id: f_00004-6-4 loss: 0.915581  [  160/  306]
train() client id: f_00004-6-5 loss: 0.779835  [  192/  306]
train() client id: f_00004-6-6 loss: 0.840047  [  224/  306]
train() client id: f_00004-6-7 loss: 1.009400  [  256/  306]
train() client id: f_00004-6-8 loss: 0.985389  [  288/  306]
train() client id: f_00004-7-0 loss: 0.918063  [   32/  306]
train() client id: f_00004-7-1 loss: 0.877533  [   64/  306]
train() client id: f_00004-7-2 loss: 0.809058  [   96/  306]
train() client id: f_00004-7-3 loss: 0.952919  [  128/  306]
train() client id: f_00004-7-4 loss: 0.960481  [  160/  306]
train() client id: f_00004-7-5 loss: 0.937747  [  192/  306]
train() client id: f_00004-7-6 loss: 0.821871  [  224/  306]
train() client id: f_00004-7-7 loss: 0.913828  [  256/  306]
train() client id: f_00004-7-8 loss: 0.866245  [  288/  306]
train() client id: f_00004-8-0 loss: 0.947050  [   32/  306]
train() client id: f_00004-8-1 loss: 0.858028  [   64/  306]
train() client id: f_00004-8-2 loss: 0.966297  [   96/  306]
train() client id: f_00004-8-3 loss: 0.822596  [  128/  306]
train() client id: f_00004-8-4 loss: 0.874752  [  160/  306]
train() client id: f_00004-8-5 loss: 0.872292  [  192/  306]
train() client id: f_00004-8-6 loss: 0.911675  [  224/  306]
train() client id: f_00004-8-7 loss: 0.915524  [  256/  306]
train() client id: f_00004-8-8 loss: 0.815329  [  288/  306]
train() client id: f_00004-9-0 loss: 0.936595  [   32/  306]
train() client id: f_00004-9-1 loss: 0.882166  [   64/  306]
train() client id: f_00004-9-2 loss: 0.905837  [   96/  306]
train() client id: f_00004-9-3 loss: 0.933397  [  128/  306]
train() client id: f_00004-9-4 loss: 0.887126  [  160/  306]
train() client id: f_00004-9-5 loss: 0.809653  [  192/  306]
train() client id: f_00004-9-6 loss: 0.804380  [  224/  306]
train() client id: f_00004-9-7 loss: 0.929508  [  256/  306]
train() client id: f_00004-9-8 loss: 0.928941  [  288/  306]
train() client id: f_00004-10-0 loss: 0.936839  [   32/  306]
train() client id: f_00004-10-1 loss: 0.860344  [   64/  306]
train() client id: f_00004-10-2 loss: 0.885189  [   96/  306]
train() client id: f_00004-10-3 loss: 0.858133  [  128/  306]
train() client id: f_00004-10-4 loss: 0.853296  [  160/  306]
train() client id: f_00004-10-5 loss: 0.853583  [  192/  306]
train() client id: f_00004-10-6 loss: 0.904056  [  224/  306]
train() client id: f_00004-10-7 loss: 0.856852  [  256/  306]
train() client id: f_00004-10-8 loss: 0.958637  [  288/  306]
train() client id: f_00004-11-0 loss: 0.886305  [   32/  306]
train() client id: f_00004-11-1 loss: 0.785506  [   64/  306]
train() client id: f_00004-11-2 loss: 0.884974  [   96/  306]
train() client id: f_00004-11-3 loss: 0.888120  [  128/  306]
train() client id: f_00004-11-4 loss: 0.938544  [  160/  306]
train() client id: f_00004-11-5 loss: 0.933315  [  192/  306]
train() client id: f_00004-11-6 loss: 0.917549  [  224/  306]
train() client id: f_00004-11-7 loss: 0.934622  [  256/  306]
train() client id: f_00004-11-8 loss: 0.801720  [  288/  306]
train() client id: f_00005-0-0 loss: 0.511711  [   32/  146]
train() client id: f_00005-0-1 loss: 0.556120  [   64/  146]
train() client id: f_00005-0-2 loss: 0.743761  [   96/  146]
train() client id: f_00005-0-3 loss: 0.762555  [  128/  146]
train() client id: f_00005-1-0 loss: 0.648201  [   32/  146]
train() client id: f_00005-1-1 loss: 0.397693  [   64/  146]
train() client id: f_00005-1-2 loss: 0.553239  [   96/  146]
train() client id: f_00005-1-3 loss: 0.747710  [  128/  146]
train() client id: f_00005-2-0 loss: 0.804481  [   32/  146]
train() client id: f_00005-2-1 loss: 0.359938  [   64/  146]
train() client id: f_00005-2-2 loss: 0.619546  [   96/  146]
train() client id: f_00005-2-3 loss: 0.709983  [  128/  146]
train() client id: f_00005-3-0 loss: 0.758287  [   32/  146]
train() client id: f_00005-3-1 loss: 0.580452  [   64/  146]
train() client id: f_00005-3-2 loss: 0.595669  [   96/  146]
train() client id: f_00005-3-3 loss: 0.565587  [  128/  146]
train() client id: f_00005-4-0 loss: 0.808640  [   32/  146]
train() client id: f_00005-4-1 loss: 0.298021  [   64/  146]
train() client id: f_00005-4-2 loss: 0.703147  [   96/  146]
train() client id: f_00005-4-3 loss: 0.490645  [  128/  146]
train() client id: f_00005-5-0 loss: 0.606993  [   32/  146]
train() client id: f_00005-5-1 loss: 0.567833  [   64/  146]
train() client id: f_00005-5-2 loss: 0.550997  [   96/  146]
train() client id: f_00005-5-3 loss: 0.560625  [  128/  146]
train() client id: f_00005-6-0 loss: 0.477124  [   32/  146]
train() client id: f_00005-6-1 loss: 0.707799  [   64/  146]
train() client id: f_00005-6-2 loss: 0.681471  [   96/  146]
train() client id: f_00005-6-3 loss: 0.560257  [  128/  146]
train() client id: f_00005-7-0 loss: 0.535015  [   32/  146]
train() client id: f_00005-7-1 loss: 0.868869  [   64/  146]
train() client id: f_00005-7-2 loss: 0.671411  [   96/  146]
train() client id: f_00005-7-3 loss: 0.389322  [  128/  146]
train() client id: f_00005-8-0 loss: 0.745273  [   32/  146]
train() client id: f_00005-8-1 loss: 0.654159  [   64/  146]
train() client id: f_00005-8-2 loss: 0.452367  [   96/  146]
train() client id: f_00005-8-3 loss: 0.552966  [  128/  146]
train() client id: f_00005-9-0 loss: 0.608336  [   32/  146]
train() client id: f_00005-9-1 loss: 0.512423  [   64/  146]
train() client id: f_00005-9-2 loss: 0.586761  [   96/  146]
train() client id: f_00005-9-3 loss: 0.724280  [  128/  146]
train() client id: f_00005-10-0 loss: 0.384445  [   32/  146]
train() client id: f_00005-10-1 loss: 0.614755  [   64/  146]
train() client id: f_00005-10-2 loss: 0.878796  [   96/  146]
train() client id: f_00005-10-3 loss: 0.520791  [  128/  146]
train() client id: f_00005-11-0 loss: 0.589994  [   32/  146]
train() client id: f_00005-11-1 loss: 0.581478  [   64/  146]
train() client id: f_00005-11-2 loss: 0.418540  [   96/  146]
train() client id: f_00005-11-3 loss: 0.617074  [  128/  146]
train() client id: f_00006-0-0 loss: 0.487243  [   32/   54]
train() client id: f_00006-1-0 loss: 0.484905  [   32/   54]
train() client id: f_00006-2-0 loss: 0.472370  [   32/   54]
train() client id: f_00006-3-0 loss: 0.434810  [   32/   54]
train() client id: f_00006-4-0 loss: 0.463102  [   32/   54]
train() client id: f_00006-5-0 loss: 0.530856  [   32/   54]
train() client id: f_00006-6-0 loss: 0.489004  [   32/   54]
train() client id: f_00006-7-0 loss: 0.493058  [   32/   54]
train() client id: f_00006-8-0 loss: 0.538198  [   32/   54]
train() client id: f_00006-9-0 loss: 0.476000  [   32/   54]
train() client id: f_00006-10-0 loss: 0.442121  [   32/   54]
train() client id: f_00006-11-0 loss: 0.543081  [   32/   54]
train() client id: f_00007-0-0 loss: 0.666907  [   32/  179]
train() client id: f_00007-0-1 loss: 0.724262  [   64/  179]
train() client id: f_00007-0-2 loss: 0.786732  [   96/  179]
train() client id: f_00007-0-3 loss: 0.550279  [  128/  179]
train() client id: f_00007-0-4 loss: 0.548124  [  160/  179]
train() client id: f_00007-1-0 loss: 0.540415  [   32/  179]
train() client id: f_00007-1-1 loss: 0.681687  [   64/  179]
train() client id: f_00007-1-2 loss: 0.534673  [   96/  179]
train() client id: f_00007-1-3 loss: 0.765960  [  128/  179]
train() client id: f_00007-1-4 loss: 0.534047  [  160/  179]
train() client id: f_00007-2-0 loss: 0.491964  [   32/  179]
train() client id: f_00007-2-1 loss: 0.483808  [   64/  179]
train() client id: f_00007-2-2 loss: 0.981044  [   96/  179]
train() client id: f_00007-2-3 loss: 0.555306  [  128/  179]
train() client id: f_00007-2-4 loss: 0.594539  [  160/  179]
train() client id: f_00007-3-0 loss: 0.699826  [   32/  179]
train() client id: f_00007-3-1 loss: 0.645126  [   64/  179]
train() client id: f_00007-3-2 loss: 0.575563  [   96/  179]
train() client id: f_00007-3-3 loss: 0.557605  [  128/  179]
train() client id: f_00007-3-4 loss: 0.597588  [  160/  179]
train() client id: f_00007-4-0 loss: 0.546874  [   32/  179]
train() client id: f_00007-4-1 loss: 0.519269  [   64/  179]
train() client id: f_00007-4-2 loss: 0.509447  [   96/  179]
train() client id: f_00007-4-3 loss: 0.614053  [  128/  179]
train() client id: f_00007-4-4 loss: 0.825080  [  160/  179]
train() client id: f_00007-5-0 loss: 0.441531  [   32/  179]
train() client id: f_00007-5-1 loss: 0.715070  [   64/  179]
train() client id: f_00007-5-2 loss: 0.812910  [   96/  179]
train() client id: f_00007-5-3 loss: 0.534947  [  128/  179]
train() client id: f_00007-5-4 loss: 0.488138  [  160/  179]
train() client id: f_00007-6-0 loss: 0.555927  [   32/  179]
train() client id: f_00007-6-1 loss: 0.599144  [   64/  179]
train() client id: f_00007-6-2 loss: 0.566053  [   96/  179]
train() client id: f_00007-6-3 loss: 0.558006  [  128/  179]
train() client id: f_00007-6-4 loss: 0.445635  [  160/  179]
train() client id: f_00007-7-0 loss: 0.782477  [   32/  179]
train() client id: f_00007-7-1 loss: 0.613628  [   64/  179]
train() client id: f_00007-7-2 loss: 0.573657  [   96/  179]
train() client id: f_00007-7-3 loss: 0.432835  [  128/  179]
train() client id: f_00007-7-4 loss: 0.656116  [  160/  179]
train() client id: f_00007-8-0 loss: 0.629617  [   32/  179]
train() client id: f_00007-8-1 loss: 0.690675  [   64/  179]
train() client id: f_00007-8-2 loss: 0.406780  [   96/  179]
train() client id: f_00007-8-3 loss: 0.703360  [  128/  179]
train() client id: f_00007-8-4 loss: 0.597150  [  160/  179]
train() client id: f_00007-9-0 loss: 0.518438  [   32/  179]
train() client id: f_00007-9-1 loss: 0.679334  [   64/  179]
train() client id: f_00007-9-2 loss: 0.606627  [   96/  179]
train() client id: f_00007-9-3 loss: 0.698103  [  128/  179]
train() client id: f_00007-9-4 loss: 0.528756  [  160/  179]
train() client id: f_00007-10-0 loss: 0.566117  [   32/  179]
train() client id: f_00007-10-1 loss: 0.848152  [   64/  179]
train() client id: f_00007-10-2 loss: 0.453335  [   96/  179]
train() client id: f_00007-10-3 loss: 0.636847  [  128/  179]
train() client id: f_00007-10-4 loss: 0.426173  [  160/  179]
train() client id: f_00007-11-0 loss: 0.530244  [   32/  179]
train() client id: f_00007-11-1 loss: 0.516491  [   64/  179]
train() client id: f_00007-11-2 loss: 0.557823  [   96/  179]
train() client id: f_00007-11-3 loss: 0.792879  [  128/  179]
train() client id: f_00007-11-4 loss: 0.627898  [  160/  179]
train() client id: f_00008-0-0 loss: 0.688689  [   32/  130]
train() client id: f_00008-0-1 loss: 0.645515  [   64/  130]
train() client id: f_00008-0-2 loss: 0.637648  [   96/  130]
train() client id: f_00008-0-3 loss: 0.650937  [  128/  130]
train() client id: f_00008-1-0 loss: 0.696309  [   32/  130]
train() client id: f_00008-1-1 loss: 0.627791  [   64/  130]
train() client id: f_00008-1-2 loss: 0.629044  [   96/  130]
train() client id: f_00008-1-3 loss: 0.668222  [  128/  130]
train() client id: f_00008-2-0 loss: 0.560772  [   32/  130]
train() client id: f_00008-2-1 loss: 0.753300  [   64/  130]
train() client id: f_00008-2-2 loss: 0.580457  [   96/  130]
train() client id: f_00008-2-3 loss: 0.718788  [  128/  130]
train() client id: f_00008-3-0 loss: 0.529306  [   32/  130]
train() client id: f_00008-3-1 loss: 0.780771  [   64/  130]
train() client id: f_00008-3-2 loss: 0.687586  [   96/  130]
train() client id: f_00008-3-3 loss: 0.632545  [  128/  130]
train() client id: f_00008-4-0 loss: 0.638873  [   32/  130]
train() client id: f_00008-4-1 loss: 0.681000  [   64/  130]
train() client id: f_00008-4-2 loss: 0.538849  [   96/  130]
train() client id: f_00008-4-3 loss: 0.694711  [  128/  130]
train() client id: f_00008-5-0 loss: 0.560924  [   32/  130]
train() client id: f_00008-5-1 loss: 0.732509  [   64/  130]
train() client id: f_00008-5-2 loss: 0.649337  [   96/  130]
train() client id: f_00008-5-3 loss: 0.664502  [  128/  130]
train() client id: f_00008-6-0 loss: 0.658338  [   32/  130]
train() client id: f_00008-6-1 loss: 0.485203  [   64/  130]
train() client id: f_00008-6-2 loss: 0.731776  [   96/  130]
train() client id: f_00008-6-3 loss: 0.744067  [  128/  130]
train() client id: f_00008-7-0 loss: 0.680727  [   32/  130]
train() client id: f_00008-7-1 loss: 0.671811  [   64/  130]
train() client id: f_00008-7-2 loss: 0.623848  [   96/  130]
train() client id: f_00008-7-3 loss: 0.654173  [  128/  130]
train() client id: f_00008-8-0 loss: 0.915328  [   32/  130]
train() client id: f_00008-8-1 loss: 0.485399  [   64/  130]
train() client id: f_00008-8-2 loss: 0.663566  [   96/  130]
train() client id: f_00008-8-3 loss: 0.569455  [  128/  130]
train() client id: f_00008-9-0 loss: 0.732716  [   32/  130]
train() client id: f_00008-9-1 loss: 0.607708  [   64/  130]
train() client id: f_00008-9-2 loss: 0.574952  [   96/  130]
train() client id: f_00008-9-3 loss: 0.676832  [  128/  130]
train() client id: f_00008-10-0 loss: 0.667992  [   32/  130]
train() client id: f_00008-10-1 loss: 0.659548  [   64/  130]
train() client id: f_00008-10-2 loss: 0.605813  [   96/  130]
train() client id: f_00008-10-3 loss: 0.696724  [  128/  130]
train() client id: f_00008-11-0 loss: 0.638130  [   32/  130]
train() client id: f_00008-11-1 loss: 0.708646  [   64/  130]
train() client id: f_00008-11-2 loss: 0.595133  [   96/  130]
train() client id: f_00008-11-3 loss: 0.671070  [  128/  130]
train() client id: f_00009-0-0 loss: 1.082559  [   32/  118]
train() client id: f_00009-0-1 loss: 1.107466  [   64/  118]
train() client id: f_00009-0-2 loss: 1.008326  [   96/  118]
train() client id: f_00009-1-0 loss: 0.962229  [   32/  118]
train() client id: f_00009-1-1 loss: 1.096484  [   64/  118]
train() client id: f_00009-1-2 loss: 1.003285  [   96/  118]
train() client id: f_00009-2-0 loss: 0.898862  [   32/  118]
train() client id: f_00009-2-1 loss: 1.134332  [   64/  118]
train() client id: f_00009-2-2 loss: 0.912770  [   96/  118]
train() client id: f_00009-3-0 loss: 1.094634  [   32/  118]
train() client id: f_00009-3-1 loss: 0.782721  [   64/  118]
train() client id: f_00009-3-2 loss: 1.028332  [   96/  118]
train() client id: f_00009-4-0 loss: 0.819057  [   32/  118]
train() client id: f_00009-4-1 loss: 0.892851  [   64/  118]
train() client id: f_00009-4-2 loss: 0.946959  [   96/  118]
train() client id: f_00009-5-0 loss: 0.810136  [   32/  118]
train() client id: f_00009-5-1 loss: 0.688369  [   64/  118]
train() client id: f_00009-5-2 loss: 1.062295  [   96/  118]
train() client id: f_00009-6-0 loss: 0.759783  [   32/  118]
train() client id: f_00009-6-1 loss: 0.937337  [   64/  118]
train() client id: f_00009-6-2 loss: 0.858430  [   96/  118]
train() client id: f_00009-7-0 loss: 0.718278  [   32/  118]
train() client id: f_00009-7-1 loss: 0.760225  [   64/  118]
train() client id: f_00009-7-2 loss: 1.071733  [   96/  118]
train() client id: f_00009-8-0 loss: 0.781919  [   32/  118]
train() client id: f_00009-8-1 loss: 0.860733  [   64/  118]
train() client id: f_00009-8-2 loss: 0.980678  [   96/  118]
train() client id: f_00009-9-0 loss: 0.662013  [   32/  118]
train() client id: f_00009-9-1 loss: 0.788337  [   64/  118]
train() client id: f_00009-9-2 loss: 0.875467  [   96/  118]
train() client id: f_00009-10-0 loss: 0.731242  [   32/  118]
train() client id: f_00009-10-1 loss: 0.951116  [   64/  118]
train() client id: f_00009-10-2 loss: 0.809394  [   96/  118]
train() client id: f_00009-11-0 loss: 0.888318  [   32/  118]
train() client id: f_00009-11-1 loss: 0.900486  [   64/  118]
train() client id: f_00009-11-2 loss: 0.666387  [   96/  118]
At round 29 accuracy: 0.649867374005305
At round 29 training accuracy: 0.5828303152246814
At round 29 training loss: 0.8258561972012336
update_location
xs = -4.528292 46.001589 55.045120 -55.943528 -45.103519 -0.217951 -97.215960 118.375741 -1.680116 39.695607 
ys = 132.587959 15.555839 1.320614 22.544824 9.350187 -17.185849 -3.124922 -19.177652 -62.154970 4.001482 
xs mean: 5.442869159412867
ys mean: 8.371751218646876
dists_uav = 166.132695 111.167128 114.156512 116.781623 110.098835 101.466255 139.501641 156.142878 117.754249 107.665004 
uav_gains = -105.528422 -101.149482 -101.437623 -101.684508 -101.044631 -100.158060 -103.616205 -104.845755 -101.774579 -100.801910 
uav_gains_db_mean: -102.20411761240051
dists_bs = 175.668393 272.514482 288.245184 193.434806 210.506428 259.777146 194.367808 351.815699 293.738424 274.471669 
bs_gains = -102.418481 -107.757968 -108.440397 -103.590029 -104.618489 -107.175885 -103.648541 -110.863885 -108.669960 -107.844990 
bs_gains_db_mean: -106.50286244795475
Round 30
-------------------------------
ene_coms = [0.00816944 0.0092648  0.00670204 0.00677574 0.00779135 0.00895254
 0.00741253 0.00788256 0.00979804 0.00931328]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 7.02099648 14.62318655  6.88535639  2.46818137 16.83306505  8.12615014
  3.07974323  9.90693024  7.268651    6.60528928]
obj_prev = 82.81754974047797
eta_min = 7.557293140626813e-14	eta_max = 0.9375778117679979
af = 17.48468447996636	bf = 1.4694265645234144	zeta = 19.233152927962998	eta = 0.9090909090909091
af = 17.48468447996636	bf = 1.4694265645234144	zeta = 34.494844847439865	eta = 0.5068781888220042
af = 17.48468447996636	bf = 1.4694265645234144	zeta = 27.072417919509878	eta = 0.6458486468386679
af = 17.48468447996636	bf = 1.4694265645234144	zeta = 25.73351015103234	eta = 0.6794519821566175
af = 17.48468447996636	bf = 1.4694265645234144	zeta = 25.664517132900134	eta = 0.6812785290065796
af = 17.48468447996636	bf = 1.4694265645234144	zeta = 25.664319615288658	eta = 0.6812837722590723
eta = 0.6812837722590723
ene_coms = [0.00816944 0.0092648  0.00670204 0.00677574 0.00779135 0.00895254
 0.00741253 0.00788256 0.00979804 0.00931328]
ene_comp = [0.03166831 0.06660398 0.03116563 0.01080744 0.07690874 0.03669502
 0.01357213 0.0449891  0.03267365 0.02965762]
ene_total = [2.2381784  4.26248619 2.12749479 0.98786455 4.75865062 2.5645874
 1.17896744 2.97045422 2.3861593  2.1894767 ]
ti_comp = [0.42558006 0.41462651 0.44025413 0.43951704 0.42936095 0.41774908
 0.43314922 0.4284489  0.4092941  0.41414171]
ti_coms = [0.08169443 0.09264798 0.06702036 0.06775745 0.07791354 0.08952541
 0.07412527 0.07882559 0.09798039 0.09313278]
t_total = [28.49987411 28.49987411 28.49987411 28.49987411 28.49987411 28.49987411
 28.49987411 28.49987411 28.49987411 28.49987411]
ene_coms = [0.00816944 0.0092648  0.00670204 0.00677574 0.00779135 0.00895254
 0.00741253 0.00788256 0.00979804 0.00931328]
ene_comp = [1.09595303e-05 1.07415351e-04 9.76115123e-06 4.08410396e-07
 1.54227581e-04 1.76958091e-05 8.32816003e-07 3.10030121e-05
 1.30137345e-05 9.50584960e-06]
ene_total = [0.45959423 0.52655301 0.37708457 0.38070023 0.44640145 0.50396902
 0.41649999 0.44460254 0.55120801 0.52377594]
optimize_network iter = 0 obj = 4.630388984944314
eta = 0.6812837722590723
freqs = [37206049.03574967 80318038.75249498 35395050.32028963 12294674.13227516
 89561872.72468919 43919930.15006008 15666807.49216766 52502294.7197521
 39914633.99313886 35806124.86011227]
eta_min = 0.681283772259077	eta_max = 0.6812837722590203
af = 0.016555333753817802	bf = 1.4694265645234144	zeta = 0.018210867129199584	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00816944 0.0092648  0.00670204 0.00677574 0.00779135 0.00895254
 0.00741253 0.00788256 0.00979804 0.00931328]
ene_comp = [2.72177995e-06 2.66764123e-05 2.42416463e-06 1.01427999e-07
 3.83021467e-05 4.39472288e-06 2.06828380e-07 7.69954318e-06
 3.23193793e-06 2.36076092e-06]
ene_total = [1.60965896 1.83012784 1.32056753 1.3346283  1.5421956  1.76423425
 1.46007505 1.55413244 1.93054162 1.83488744]
ti_comp = [0.42558006 0.41462651 0.44025413 0.43951704 0.42936095 0.41774908
 0.43314922 0.4284489  0.4092941  0.41414171]
ti_coms = [0.08169443 0.09264798 0.06702036 0.06775745 0.07791354 0.08952541
 0.07412527 0.07882559 0.09798039 0.09313278]
t_total = [28.49987411 28.49987411 28.49987411 28.49987411 28.49987411 28.49987411
 28.49987411 28.49987411 28.49987411 28.49987411]
ene_coms = [0.00816944 0.0092648  0.00670204 0.00677574 0.00779135 0.00895254
 0.00741253 0.00788256 0.00979804 0.00931328]
ene_comp = [1.09595303e-05 1.07415351e-04 9.76115123e-06 4.08410396e-07
 1.54227581e-04 1.76958091e-05 8.32816003e-07 3.10030121e-05
 1.30137345e-05 9.50584960e-06]
ene_total = [0.45959423 0.52655301 0.37708457 0.38070023 0.44640145 0.50396902
 0.41649999 0.44460254 0.55120801 0.52377594]
optimize_network iter = 1 obj = 4.630388984943563
eta = 0.6812837722590203
freqs = [37206049.03574985 80318038.75249496 35395050.32029004 12294674.13227529
 89561872.7246898  43919930.15006013 15666807.49216779 52502294.71975243
 39914633.99313875 35806124.86011226]
Done!
ene_coms = [0.00816944 0.0092648  0.00670204 0.00677574 0.00779135 0.00895254
 0.00741253 0.00788256 0.00979804 0.00931328]
ene_comp = [1.04652330e-05 1.02570699e-04 9.32090333e-06 3.89990251e-07
 1.47271601e-04 1.68976919e-05 7.95254297e-07 2.96047128e-05
 1.24267885e-05 9.07711633e-06]
ene_total = [0.00817991 0.00936737 0.00671136 0.00677613 0.00793863 0.00896944
 0.00741332 0.00791216 0.00981047 0.00932236]
At round 30 energy consumption: 0.08240113986549671
At round 30 eta: 0.6812837722590203
At round 30 a_n: 17.906227447174857
At round 30 local rounds: 12.56678785314937
At round 30 global rounds: 56.18235247728655
gradient difference: 0.44075626134872437
train() client id: f_00000-0-0 loss: 1.267421  [   32/  126]
train() client id: f_00000-0-1 loss: 1.129884  [   64/  126]
train() client id: f_00000-0-2 loss: 1.189570  [   96/  126]
train() client id: f_00000-1-0 loss: 1.015392  [   32/  126]
train() client id: f_00000-1-1 loss: 1.194821  [   64/  126]
train() client id: f_00000-1-2 loss: 1.391866  [   96/  126]
train() client id: f_00000-2-0 loss: 1.111014  [   32/  126]
train() client id: f_00000-2-1 loss: 1.000763  [   64/  126]
train() client id: f_00000-2-2 loss: 1.039700  [   96/  126]
train() client id: f_00000-3-0 loss: 1.001709  [   32/  126]
train() client id: f_00000-3-1 loss: 1.021282  [   64/  126]
train() client id: f_00000-3-2 loss: 0.960468  [   96/  126]
train() client id: f_00000-4-0 loss: 0.938570  [   32/  126]
train() client id: f_00000-4-1 loss: 0.883323  [   64/  126]
train() client id: f_00000-4-2 loss: 0.983151  [   96/  126]
train() client id: f_00000-5-0 loss: 0.846559  [   32/  126]
train() client id: f_00000-5-1 loss: 0.957530  [   64/  126]
train() client id: f_00000-5-2 loss: 0.969330  [   96/  126]
train() client id: f_00000-6-0 loss: 0.884990  [   32/  126]
train() client id: f_00000-6-1 loss: 0.866806  [   64/  126]
train() client id: f_00000-6-2 loss: 0.781228  [   96/  126]
train() client id: f_00000-7-0 loss: 0.929776  [   32/  126]
train() client id: f_00000-7-1 loss: 0.757212  [   64/  126]
train() client id: f_00000-7-2 loss: 0.895856  [   96/  126]
train() client id: f_00000-8-0 loss: 0.737572  [   32/  126]
train() client id: f_00000-8-1 loss: 0.794395  [   64/  126]
train() client id: f_00000-8-2 loss: 0.824806  [   96/  126]
train() client id: f_00000-9-0 loss: 0.836614  [   32/  126]
train() client id: f_00000-9-1 loss: 0.791840  [   64/  126]
train() client id: f_00000-9-2 loss: 0.820682  [   96/  126]
train() client id: f_00000-10-0 loss: 0.892138  [   32/  126]
train() client id: f_00000-10-1 loss: 0.711957  [   64/  126]
train() client id: f_00000-10-2 loss: 0.899902  [   96/  126]
train() client id: f_00000-11-0 loss: 0.862941  [   32/  126]
train() client id: f_00000-11-1 loss: 0.779982  [   64/  126]
train() client id: f_00000-11-2 loss: 0.741176  [   96/  126]
train() client id: f_00001-0-0 loss: 0.454923  [   32/  265]
train() client id: f_00001-0-1 loss: 0.449516  [   64/  265]
train() client id: f_00001-0-2 loss: 0.617429  [   96/  265]
train() client id: f_00001-0-3 loss: 0.443015  [  128/  265]
train() client id: f_00001-0-4 loss: 0.529858  [  160/  265]
train() client id: f_00001-0-5 loss: 0.521233  [  192/  265]
train() client id: f_00001-0-6 loss: 0.566417  [  224/  265]
train() client id: f_00001-0-7 loss: 0.624537  [  256/  265]
train() client id: f_00001-1-0 loss: 0.547745  [   32/  265]
train() client id: f_00001-1-1 loss: 0.477375  [   64/  265]
train() client id: f_00001-1-2 loss: 0.488739  [   96/  265]
train() client id: f_00001-1-3 loss: 0.623161  [  128/  265]
train() client id: f_00001-1-4 loss: 0.494763  [  160/  265]
train() client id: f_00001-1-5 loss: 0.417062  [  192/  265]
train() client id: f_00001-1-6 loss: 0.605462  [  224/  265]
train() client id: f_00001-1-7 loss: 0.513699  [  256/  265]
train() client id: f_00001-2-0 loss: 0.430588  [   32/  265]
train() client id: f_00001-2-1 loss: 0.555648  [   64/  265]
train() client id: f_00001-2-2 loss: 0.441384  [   96/  265]
train() client id: f_00001-2-3 loss: 0.429921  [  128/  265]
train() client id: f_00001-2-4 loss: 0.628304  [  160/  265]
train() client id: f_00001-2-5 loss: 0.534511  [  192/  265]
train() client id: f_00001-2-6 loss: 0.561311  [  224/  265]
train() client id: f_00001-2-7 loss: 0.525580  [  256/  265]
train() client id: f_00001-3-0 loss: 0.578623  [   32/  265]
train() client id: f_00001-3-1 loss: 0.501876  [   64/  265]
train() client id: f_00001-3-2 loss: 0.502845  [   96/  265]
train() client id: f_00001-3-3 loss: 0.572515  [  128/  265]
train() client id: f_00001-3-4 loss: 0.436807  [  160/  265]
train() client id: f_00001-3-5 loss: 0.483428  [  192/  265]
train() client id: f_00001-3-6 loss: 0.445152  [  224/  265]
train() client id: f_00001-3-7 loss: 0.517672  [  256/  265]
train() client id: f_00001-4-0 loss: 0.481260  [   32/  265]
train() client id: f_00001-4-1 loss: 0.541454  [   64/  265]
train() client id: f_00001-4-2 loss: 0.558533  [   96/  265]
train() client id: f_00001-4-3 loss: 0.462450  [  128/  265]
train() client id: f_00001-4-4 loss: 0.475489  [  160/  265]
train() client id: f_00001-4-5 loss: 0.517753  [  192/  265]
train() client id: f_00001-4-6 loss: 0.613758  [  224/  265]
train() client id: f_00001-4-7 loss: 0.413249  [  256/  265]
train() client id: f_00001-5-0 loss: 0.401306  [   32/  265]
train() client id: f_00001-5-1 loss: 0.487582  [   64/  265]
train() client id: f_00001-5-2 loss: 0.467949  [   96/  265]
train() client id: f_00001-5-3 loss: 0.450780  [  128/  265]
train() client id: f_00001-5-4 loss: 0.717706  [  160/  265]
train() client id: f_00001-5-5 loss: 0.556434  [  192/  265]
train() client id: f_00001-5-6 loss: 0.496571  [  224/  265]
train() client id: f_00001-5-7 loss: 0.459245  [  256/  265]
train() client id: f_00001-6-0 loss: 0.595726  [   32/  265]
train() client id: f_00001-6-1 loss: 0.527334  [   64/  265]
train() client id: f_00001-6-2 loss: 0.573996  [   96/  265]
train() client id: f_00001-6-3 loss: 0.424662  [  128/  265]
train() client id: f_00001-6-4 loss: 0.409261  [  160/  265]
train() client id: f_00001-6-5 loss: 0.574711  [  192/  265]
train() client id: f_00001-6-6 loss: 0.443522  [  224/  265]
train() client id: f_00001-6-7 loss: 0.410289  [  256/  265]
train() client id: f_00001-7-0 loss: 0.428139  [   32/  265]
train() client id: f_00001-7-1 loss: 0.559919  [   64/  265]
train() client id: f_00001-7-2 loss: 0.491554  [   96/  265]
train() client id: f_00001-7-3 loss: 0.583020  [  128/  265]
train() client id: f_00001-7-4 loss: 0.423623  [  160/  265]
train() client id: f_00001-7-5 loss: 0.536353  [  192/  265]
train() client id: f_00001-7-6 loss: 0.580357  [  224/  265]
train() client id: f_00001-7-7 loss: 0.416738  [  256/  265]
train() client id: f_00001-8-0 loss: 0.455134  [   32/  265]
train() client id: f_00001-8-1 loss: 0.494311  [   64/  265]
train() client id: f_00001-8-2 loss: 0.403504  [   96/  265]
train() client id: f_00001-8-3 loss: 0.464478  [  128/  265]
train() client id: f_00001-8-4 loss: 0.526118  [  160/  265]
train() client id: f_00001-8-5 loss: 0.492646  [  192/  265]
train() client id: f_00001-8-6 loss: 0.651948  [  224/  265]
train() client id: f_00001-8-7 loss: 0.523249  [  256/  265]
train() client id: f_00001-9-0 loss: 0.538784  [   32/  265]
train() client id: f_00001-9-1 loss: 0.456248  [   64/  265]
train() client id: f_00001-9-2 loss: 0.471109  [   96/  265]
train() client id: f_00001-9-3 loss: 0.482140  [  128/  265]
train() client id: f_00001-9-4 loss: 0.495167  [  160/  265]
train() client id: f_00001-9-5 loss: 0.416413  [  192/  265]
train() client id: f_00001-9-6 loss: 0.631059  [  224/  265]
train() client id: f_00001-9-7 loss: 0.526433  [  256/  265]
train() client id: f_00001-10-0 loss: 0.553205  [   32/  265]
train() client id: f_00001-10-1 loss: 0.478732  [   64/  265]
train() client id: f_00001-10-2 loss: 0.602222  [   96/  265]
train() client id: f_00001-10-3 loss: 0.419537  [  128/  265]
train() client id: f_00001-10-4 loss: 0.584543  [  160/  265]
train() client id: f_00001-10-5 loss: 0.441774  [  192/  265]
train() client id: f_00001-10-6 loss: 0.508760  [  224/  265]
train() client id: f_00001-10-7 loss: 0.427713  [  256/  265]
train() client id: f_00001-11-0 loss: 0.531938  [   32/  265]
train() client id: f_00001-11-1 loss: 0.449118  [   64/  265]
train() client id: f_00001-11-2 loss: 0.556720  [   96/  265]
train() client id: f_00001-11-3 loss: 0.488230  [  128/  265]
train() client id: f_00001-11-4 loss: 0.581266  [  160/  265]
train() client id: f_00001-11-5 loss: 0.491365  [  192/  265]
train() client id: f_00001-11-6 loss: 0.499480  [  224/  265]
train() client id: f_00001-11-7 loss: 0.420076  [  256/  265]
train() client id: f_00002-0-0 loss: 1.433587  [   32/  124]
train() client id: f_00002-0-1 loss: 1.118579  [   64/  124]
train() client id: f_00002-0-2 loss: 1.222984  [   96/  124]
train() client id: f_00002-1-0 loss: 1.324126  [   32/  124]
train() client id: f_00002-1-1 loss: 1.158662  [   64/  124]
train() client id: f_00002-1-2 loss: 1.140578  [   96/  124]
train() client id: f_00002-2-0 loss: 1.149241  [   32/  124]
train() client id: f_00002-2-1 loss: 1.390543  [   64/  124]
train() client id: f_00002-2-2 loss: 1.091338  [   96/  124]
train() client id: f_00002-3-0 loss: 1.158468  [   32/  124]
train() client id: f_00002-3-1 loss: 1.026271  [   64/  124]
train() client id: f_00002-3-2 loss: 1.136188  [   96/  124]
train() client id: f_00002-4-0 loss: 1.237941  [   32/  124]
train() client id: f_00002-4-1 loss: 1.076023  [   64/  124]
train() client id: f_00002-4-2 loss: 1.044227  [   96/  124]
train() client id: f_00002-5-0 loss: 1.094124  [   32/  124]
train() client id: f_00002-5-1 loss: 1.186955  [   64/  124]
train() client id: f_00002-5-2 loss: 1.061121  [   96/  124]
train() client id: f_00002-6-0 loss: 0.869417  [   32/  124]
train() client id: f_00002-6-1 loss: 0.928410  [   64/  124]
train() client id: f_00002-6-2 loss: 1.201837  [   96/  124]
train() client id: f_00002-7-0 loss: 0.856836  [   32/  124]
train() client id: f_00002-7-1 loss: 0.926045  [   64/  124]
train() client id: f_00002-7-2 loss: 1.236036  [   96/  124]
train() client id: f_00002-8-0 loss: 0.959432  [   32/  124]
train() client id: f_00002-8-1 loss: 1.031760  [   64/  124]
train() client id: f_00002-8-2 loss: 1.068097  [   96/  124]
train() client id: f_00002-9-0 loss: 0.826981  [   32/  124]
train() client id: f_00002-9-1 loss: 1.012878  [   64/  124]
train() client id: f_00002-9-2 loss: 0.908791  [   96/  124]
train() client id: f_00002-10-0 loss: 0.777189  [   32/  124]
train() client id: f_00002-10-1 loss: 0.907817  [   64/  124]
train() client id: f_00002-10-2 loss: 1.156632  [   96/  124]
train() client id: f_00002-11-0 loss: 1.041556  [   32/  124]
train() client id: f_00002-11-1 loss: 1.079909  [   64/  124]
train() client id: f_00002-11-2 loss: 1.039700  [   96/  124]
train() client id: f_00003-0-0 loss: 0.751413  [   32/   43]
train() client id: f_00003-1-0 loss: 0.686627  [   32/   43]
train() client id: f_00003-2-0 loss: 0.672723  [   32/   43]
train() client id: f_00003-3-0 loss: 0.647249  [   32/   43]
train() client id: f_00003-4-0 loss: 0.774091  [   32/   43]
train() client id: f_00003-5-0 loss: 0.683685  [   32/   43]
train() client id: f_00003-6-0 loss: 0.827967  [   32/   43]
train() client id: f_00003-7-0 loss: 0.632201  [   32/   43]
train() client id: f_00003-8-0 loss: 0.660455  [   32/   43]
train() client id: f_00003-9-0 loss: 0.601679  [   32/   43]
train() client id: f_00003-10-0 loss: 0.595764  [   32/   43]
train() client id: f_00003-11-0 loss: 0.884432  [   32/   43]
train() client id: f_00004-0-0 loss: 0.898554  [   32/  306]
train() client id: f_00004-0-1 loss: 0.877615  [   64/  306]
train() client id: f_00004-0-2 loss: 1.043179  [   96/  306]
train() client id: f_00004-0-3 loss: 0.842757  [  128/  306]
train() client id: f_00004-0-4 loss: 1.058675  [  160/  306]
train() client id: f_00004-0-5 loss: 0.935451  [  192/  306]
train() client id: f_00004-0-6 loss: 0.901468  [  224/  306]
train() client id: f_00004-0-7 loss: 0.931665  [  256/  306]
train() client id: f_00004-0-8 loss: 0.981773  [  288/  306]
train() client id: f_00004-1-0 loss: 0.846462  [   32/  306]
train() client id: f_00004-1-1 loss: 0.984884  [   64/  306]
train() client id: f_00004-1-2 loss: 1.016001  [   96/  306]
train() client id: f_00004-1-3 loss: 0.914023  [  128/  306]
train() client id: f_00004-1-4 loss: 0.879601  [  160/  306]
train() client id: f_00004-1-5 loss: 0.927781  [  192/  306]
train() client id: f_00004-1-6 loss: 0.961505  [  224/  306]
train() client id: f_00004-1-7 loss: 0.914350  [  256/  306]
train() client id: f_00004-1-8 loss: 0.994629  [  288/  306]
train() client id: f_00004-2-0 loss: 0.872374  [   32/  306]
train() client id: f_00004-2-1 loss: 0.960003  [   64/  306]
train() client id: f_00004-2-2 loss: 0.914851  [   96/  306]
train() client id: f_00004-2-3 loss: 1.015070  [  128/  306]
train() client id: f_00004-2-4 loss: 0.905866  [  160/  306]
train() client id: f_00004-2-5 loss: 0.899673  [  192/  306]
train() client id: f_00004-2-6 loss: 0.998351  [  224/  306]
train() client id: f_00004-2-7 loss: 0.864107  [  256/  306]
train() client id: f_00004-2-8 loss: 1.036950  [  288/  306]
train() client id: f_00004-3-0 loss: 0.903379  [   32/  306]
train() client id: f_00004-3-1 loss: 0.904385  [   64/  306]
train() client id: f_00004-3-2 loss: 0.881519  [   96/  306]
train() client id: f_00004-3-3 loss: 0.984297  [  128/  306]
train() client id: f_00004-3-4 loss: 1.008627  [  160/  306]
train() client id: f_00004-3-5 loss: 0.961015  [  192/  306]
train() client id: f_00004-3-6 loss: 0.918822  [  224/  306]
train() client id: f_00004-3-7 loss: 1.090725  [  256/  306]
train() client id: f_00004-3-8 loss: 0.833188  [  288/  306]
train() client id: f_00004-4-0 loss: 0.911911  [   32/  306]
train() client id: f_00004-4-1 loss: 0.894531  [   64/  306]
train() client id: f_00004-4-2 loss: 0.949408  [   96/  306]
train() client id: f_00004-4-3 loss: 0.996841  [  128/  306]
train() client id: f_00004-4-4 loss: 1.090618  [  160/  306]
train() client id: f_00004-4-5 loss: 0.884232  [  192/  306]
train() client id: f_00004-4-6 loss: 0.977037  [  224/  306]
train() client id: f_00004-4-7 loss: 0.779040  [  256/  306]
train() client id: f_00004-4-8 loss: 0.973305  [  288/  306]
train() client id: f_00004-5-0 loss: 1.021257  [   32/  306]
train() client id: f_00004-5-1 loss: 0.750344  [   64/  306]
train() client id: f_00004-5-2 loss: 1.025087  [   96/  306]
train() client id: f_00004-5-3 loss: 0.871346  [  128/  306]
train() client id: f_00004-5-4 loss: 1.040403  [  160/  306]
train() client id: f_00004-5-5 loss: 0.983549  [  192/  306]
train() client id: f_00004-5-6 loss: 0.931016  [  224/  306]
train() client id: f_00004-5-7 loss: 0.844016  [  256/  306]
train() client id: f_00004-5-8 loss: 1.004363  [  288/  306]
train() client id: f_00004-6-0 loss: 0.889135  [   32/  306]
train() client id: f_00004-6-1 loss: 0.944490  [   64/  306]
train() client id: f_00004-6-2 loss: 0.856992  [   96/  306]
train() client id: f_00004-6-3 loss: 0.828671  [  128/  306]
train() client id: f_00004-6-4 loss: 0.913151  [  160/  306]
train() client id: f_00004-6-5 loss: 0.959280  [  192/  306]
train() client id: f_00004-6-6 loss: 0.943918  [  224/  306]
train() client id: f_00004-6-7 loss: 0.978226  [  256/  306]
train() client id: f_00004-6-8 loss: 1.015312  [  288/  306]
train() client id: f_00004-7-0 loss: 0.906623  [   32/  306]
train() client id: f_00004-7-1 loss: 0.951443  [   64/  306]
train() client id: f_00004-7-2 loss: 0.918910  [   96/  306]
train() client id: f_00004-7-3 loss: 0.935948  [  128/  306]
train() client id: f_00004-7-4 loss: 1.081102  [  160/  306]
train() client id: f_00004-7-5 loss: 0.869158  [  192/  306]
train() client id: f_00004-7-6 loss: 0.850544  [  224/  306]
train() client id: f_00004-7-7 loss: 0.945498  [  256/  306]
train() client id: f_00004-7-8 loss: 0.987348  [  288/  306]
train() client id: f_00004-8-0 loss: 0.938617  [   32/  306]
train() client id: f_00004-8-1 loss: 0.825004  [   64/  306]
train() client id: f_00004-8-2 loss: 0.866668  [   96/  306]
train() client id: f_00004-8-3 loss: 0.846485  [  128/  306]
train() client id: f_00004-8-4 loss: 0.994493  [  160/  306]
train() client id: f_00004-8-5 loss: 0.991659  [  192/  306]
train() client id: f_00004-8-6 loss: 1.095847  [  224/  306]
train() client id: f_00004-8-7 loss: 0.836737  [  256/  306]
train() client id: f_00004-8-8 loss: 0.941538  [  288/  306]
train() client id: f_00004-9-0 loss: 0.932964  [   32/  306]
train() client id: f_00004-9-1 loss: 0.922413  [   64/  306]
train() client id: f_00004-9-2 loss: 0.851729  [   96/  306]
train() client id: f_00004-9-3 loss: 0.913860  [  128/  306]
train() client id: f_00004-9-4 loss: 0.913324  [  160/  306]
train() client id: f_00004-9-5 loss: 0.948200  [  192/  306]
train() client id: f_00004-9-6 loss: 0.953232  [  224/  306]
train() client id: f_00004-9-7 loss: 0.958341  [  256/  306]
train() client id: f_00004-9-8 loss: 0.910967  [  288/  306]
train() client id: f_00004-10-0 loss: 0.929340  [   32/  306]
train() client id: f_00004-10-1 loss: 0.915849  [   64/  306]
train() client id: f_00004-10-2 loss: 0.923063  [   96/  306]
train() client id: f_00004-10-3 loss: 0.871071  [  128/  306]
train() client id: f_00004-10-4 loss: 0.894386  [  160/  306]
train() client id: f_00004-10-5 loss: 0.945603  [  192/  306]
train() client id: f_00004-10-6 loss: 0.912144  [  224/  306]
train() client id: f_00004-10-7 loss: 0.888652  [  256/  306]
train() client id: f_00004-10-8 loss: 1.081559  [  288/  306]
train() client id: f_00004-11-0 loss: 0.972164  [   32/  306]
train() client id: f_00004-11-1 loss: 0.905911  [   64/  306]
train() client id: f_00004-11-2 loss: 0.977464  [   96/  306]
train() client id: f_00004-11-3 loss: 0.901724  [  128/  306]
train() client id: f_00004-11-4 loss: 1.047956  [  160/  306]
train() client id: f_00004-11-5 loss: 0.926530  [  192/  306]
train() client id: f_00004-11-6 loss: 1.021318  [  224/  306]
train() client id: f_00004-11-7 loss: 0.825599  [  256/  306]
train() client id: f_00004-11-8 loss: 0.850185  [  288/  306]
train() client id: f_00005-0-0 loss: 0.637760  [   32/  146]
train() client id: f_00005-0-1 loss: 0.528026  [   64/  146]
train() client id: f_00005-0-2 loss: 0.718517  [   96/  146]
train() client id: f_00005-0-3 loss: 0.562126  [  128/  146]
train() client id: f_00005-1-0 loss: 0.941415  [   32/  146]
train() client id: f_00005-1-1 loss: 0.545704  [   64/  146]
train() client id: f_00005-1-2 loss: 0.411761  [   96/  146]
train() client id: f_00005-1-3 loss: 0.735682  [  128/  146]
train() client id: f_00005-2-0 loss: 0.559186  [   32/  146]
train() client id: f_00005-2-1 loss: 0.540252  [   64/  146]
train() client id: f_00005-2-2 loss: 0.689637  [   96/  146]
train() client id: f_00005-2-3 loss: 0.621796  [  128/  146]
train() client id: f_00005-3-0 loss: 0.694381  [   32/  146]
train() client id: f_00005-3-1 loss: 0.532768  [   64/  146]
train() client id: f_00005-3-2 loss: 0.710350  [   96/  146]
train() client id: f_00005-3-3 loss: 0.438801  [  128/  146]
train() client id: f_00005-4-0 loss: 0.694070  [   32/  146]
train() client id: f_00005-4-1 loss: 0.290982  [   64/  146]
train() client id: f_00005-4-2 loss: 0.785424  [   96/  146]
train() client id: f_00005-4-3 loss: 0.804222  [  128/  146]
train() client id: f_00005-5-0 loss: 0.418085  [   32/  146]
train() client id: f_00005-5-1 loss: 0.644000  [   64/  146]
train() client id: f_00005-5-2 loss: 0.787874  [   96/  146]
train() client id: f_00005-5-3 loss: 0.695764  [  128/  146]
train() client id: f_00005-6-0 loss: 0.496594  [   32/  146]
train() client id: f_00005-6-1 loss: 0.799667  [   64/  146]
train() client id: f_00005-6-2 loss: 0.682513  [   96/  146]
train() client id: f_00005-6-3 loss: 0.579458  [  128/  146]
train() client id: f_00005-7-0 loss: 0.369739  [   32/  146]
train() client id: f_00005-7-1 loss: 0.620736  [   64/  146]
train() client id: f_00005-7-2 loss: 0.671124  [   96/  146]
train() client id: f_00005-7-3 loss: 0.855468  [  128/  146]
train() client id: f_00005-8-0 loss: 0.750913  [   32/  146]
train() client id: f_00005-8-1 loss: 0.297870  [   64/  146]
train() client id: f_00005-8-2 loss: 0.690006  [   96/  146]
train() client id: f_00005-8-3 loss: 0.778648  [  128/  146]
train() client id: f_00005-9-0 loss: 0.695871  [   32/  146]
train() client id: f_00005-9-1 loss: 0.465792  [   64/  146]
train() client id: f_00005-9-2 loss: 0.587832  [   96/  146]
train() client id: f_00005-9-3 loss: 0.673670  [  128/  146]
train() client id: f_00005-10-0 loss: 0.738298  [   32/  146]
train() client id: f_00005-10-1 loss: 0.625155  [   64/  146]
train() client id: f_00005-10-2 loss: 0.592866  [   96/  146]
train() client id: f_00005-10-3 loss: 0.569471  [  128/  146]
train() client id: f_00005-11-0 loss: 0.498186  [   32/  146]
train() client id: f_00005-11-1 loss: 0.732150  [   64/  146]
train() client id: f_00005-11-2 loss: 0.624972  [   96/  146]
train() client id: f_00005-11-3 loss: 0.693777  [  128/  146]
train() client id: f_00006-0-0 loss: 0.468268  [   32/   54]
train() client id: f_00006-1-0 loss: 0.513349  [   32/   54]
train() client id: f_00006-2-0 loss: 0.473350  [   32/   54]
train() client id: f_00006-3-0 loss: 0.482477  [   32/   54]
train() client id: f_00006-4-0 loss: 0.513988  [   32/   54]
train() client id: f_00006-5-0 loss: 0.451773  [   32/   54]
train() client id: f_00006-6-0 loss: 0.450374  [   32/   54]
train() client id: f_00006-7-0 loss: 0.506545  [   32/   54]
train() client id: f_00006-8-0 loss: 0.510828  [   32/   54]
train() client id: f_00006-9-0 loss: 0.440710  [   32/   54]
train() client id: f_00006-10-0 loss: 0.454132  [   32/   54]
train() client id: f_00006-11-0 loss: 0.438646  [   32/   54]
train() client id: f_00007-0-0 loss: 0.613840  [   32/  179]
train() client id: f_00007-0-1 loss: 0.524572  [   64/  179]
train() client id: f_00007-0-2 loss: 0.570225  [   96/  179]
train() client id: f_00007-0-3 loss: 0.569245  [  128/  179]
train() client id: f_00007-0-4 loss: 0.539567  [  160/  179]
train() client id: f_00007-1-0 loss: 0.694876  [   32/  179]
train() client id: f_00007-1-1 loss: 0.391733  [   64/  179]
train() client id: f_00007-1-2 loss: 0.492521  [   96/  179]
train() client id: f_00007-1-3 loss: 0.553236  [  128/  179]
train() client id: f_00007-1-4 loss: 0.525096  [  160/  179]
train() client id: f_00007-2-0 loss: 0.506631  [   32/  179]
train() client id: f_00007-2-1 loss: 0.627845  [   64/  179]
train() client id: f_00007-2-2 loss: 0.548436  [   96/  179]
train() client id: f_00007-2-3 loss: 0.572326  [  128/  179]
train() client id: f_00007-2-4 loss: 0.396048  [  160/  179]
train() client id: f_00007-3-0 loss: 0.568654  [   32/  179]
train() client id: f_00007-3-1 loss: 0.487251  [   64/  179]
train() client id: f_00007-3-2 loss: 0.419913  [   96/  179]
train() client id: f_00007-3-3 loss: 0.706879  [  128/  179]
train() client id: f_00007-3-4 loss: 0.428633  [  160/  179]
train() client id: f_00007-4-0 loss: 0.400767  [   32/  179]
train() client id: f_00007-4-1 loss: 0.388383  [   64/  179]
train() client id: f_00007-4-2 loss: 0.536869  [   96/  179]
train() client id: f_00007-4-3 loss: 0.859889  [  128/  179]
train() client id: f_00007-4-4 loss: 0.390432  [  160/  179]
train() client id: f_00007-5-0 loss: 0.389525  [   32/  179]
train() client id: f_00007-5-1 loss: 0.530703  [   64/  179]
train() client id: f_00007-5-2 loss: 0.700943  [   96/  179]
train() client id: f_00007-5-3 loss: 0.375928  [  128/  179]
train() client id: f_00007-5-4 loss: 0.620116  [  160/  179]
train() client id: f_00007-6-0 loss: 0.530285  [   32/  179]
train() client id: f_00007-6-1 loss: 0.562915  [   64/  179]
train() client id: f_00007-6-2 loss: 0.641787  [   96/  179]
train() client id: f_00007-6-3 loss: 0.468884  [  128/  179]
train() client id: f_00007-6-4 loss: 0.502150  [  160/  179]
train() client id: f_00007-7-0 loss: 0.487383  [   32/  179]
train() client id: f_00007-7-1 loss: 0.373874  [   64/  179]
train() client id: f_00007-7-2 loss: 0.542173  [   96/  179]
train() client id: f_00007-7-3 loss: 0.742823  [  128/  179]
train() client id: f_00007-7-4 loss: 0.370525  [  160/  179]
train() client id: f_00007-8-0 loss: 0.540740  [   32/  179]
train() client id: f_00007-8-1 loss: 0.453688  [   64/  179]
train() client id: f_00007-8-2 loss: 0.461666  [   96/  179]
train() client id: f_00007-8-3 loss: 0.600793  [  128/  179]
train() client id: f_00007-8-4 loss: 0.593195  [  160/  179]
train() client id: f_00007-9-0 loss: 0.480249  [   32/  179]
train() client id: f_00007-9-1 loss: 0.605211  [   64/  179]
train() client id: f_00007-9-2 loss: 0.577999  [   96/  179]
train() client id: f_00007-9-3 loss: 0.384170  [  128/  179]
train() client id: f_00007-9-4 loss: 0.491933  [  160/  179]
train() client id: f_00007-10-0 loss: 0.336359  [   32/  179]
train() client id: f_00007-10-1 loss: 0.371387  [   64/  179]
train() client id: f_00007-10-2 loss: 0.710506  [   96/  179]
train() client id: f_00007-10-3 loss: 0.667779  [  128/  179]
train() client id: f_00007-10-4 loss: 0.563322  [  160/  179]
train() client id: f_00007-11-0 loss: 0.374471  [   32/  179]
train() client id: f_00007-11-1 loss: 0.691479  [   64/  179]
train() client id: f_00007-11-2 loss: 0.483582  [   96/  179]
train() client id: f_00007-11-3 loss: 0.486330  [  128/  179]
train() client id: f_00007-11-4 loss: 0.432217  [  160/  179]
train() client id: f_00008-0-0 loss: 0.685500  [   32/  130]
train() client id: f_00008-0-1 loss: 0.671291  [   64/  130]
train() client id: f_00008-0-2 loss: 0.801645  [   96/  130]
train() client id: f_00008-0-3 loss: 0.743290  [  128/  130]
train() client id: f_00008-1-0 loss: 0.698399  [   32/  130]
train() client id: f_00008-1-1 loss: 0.618377  [   64/  130]
train() client id: f_00008-1-2 loss: 0.747008  [   96/  130]
train() client id: f_00008-1-3 loss: 0.824119  [  128/  130]
train() client id: f_00008-2-0 loss: 0.727299  [   32/  130]
train() client id: f_00008-2-1 loss: 0.728341  [   64/  130]
train() client id: f_00008-2-2 loss: 0.728288  [   96/  130]
train() client id: f_00008-2-3 loss: 0.715621  [  128/  130]
train() client id: f_00008-3-0 loss: 0.711309  [   32/  130]
train() client id: f_00008-3-1 loss: 0.785492  [   64/  130]
train() client id: f_00008-3-2 loss: 0.631904  [   96/  130]
train() client id: f_00008-3-3 loss: 0.761397  [  128/  130]
train() client id: f_00008-4-0 loss: 0.675784  [   32/  130]
train() client id: f_00008-4-1 loss: 0.634652  [   64/  130]
train() client id: f_00008-4-2 loss: 0.817236  [   96/  130]
train() client id: f_00008-4-3 loss: 0.762570  [  128/  130]
train() client id: f_00008-5-0 loss: 0.781272  [   32/  130]
train() client id: f_00008-5-1 loss: 0.674240  [   64/  130]
train() client id: f_00008-5-2 loss: 0.728905  [   96/  130]
train() client id: f_00008-5-3 loss: 0.700212  [  128/  130]
train() client id: f_00008-6-0 loss: 0.763681  [   32/  130]
train() client id: f_00008-6-1 loss: 0.715639  [   64/  130]
train() client id: f_00008-6-2 loss: 0.738949  [   96/  130]
train() client id: f_00008-6-3 loss: 0.612987  [  128/  130]
train() client id: f_00008-7-0 loss: 0.809536  [   32/  130]
train() client id: f_00008-7-1 loss: 0.727371  [   64/  130]
train() client id: f_00008-7-2 loss: 0.755970  [   96/  130]
train() client id: f_00008-7-3 loss: 0.599517  [  128/  130]
train() client id: f_00008-8-0 loss: 0.650701  [   32/  130]
train() client id: f_00008-8-1 loss: 0.702675  [   64/  130]
train() client id: f_00008-8-2 loss: 0.832955  [   96/  130]
train() client id: f_00008-8-3 loss: 0.701379  [  128/  130]
train() client id: f_00008-9-0 loss: 0.714053  [   32/  130]
train() client id: f_00008-9-1 loss: 0.728724  [   64/  130]
train() client id: f_00008-9-2 loss: 0.757258  [   96/  130]
train() client id: f_00008-9-3 loss: 0.678740  [  128/  130]
train() client id: f_00008-10-0 loss: 0.789915  [   32/  130]
train() client id: f_00008-10-1 loss: 0.680629  [   64/  130]
train() client id: f_00008-10-2 loss: 0.729311  [   96/  130]
train() client id: f_00008-10-3 loss: 0.693606  [  128/  130]
train() client id: f_00008-11-0 loss: 0.677636  [   32/  130]
train() client id: f_00008-11-1 loss: 0.770817  [   64/  130]
train() client id: f_00008-11-2 loss: 0.779661  [   96/  130]
train() client id: f_00008-11-3 loss: 0.658134  [  128/  130]
train() client id: f_00009-0-0 loss: 1.271560  [   32/  118]
train() client id: f_00009-0-1 loss: 0.985826  [   64/  118]
train() client id: f_00009-0-2 loss: 1.174558  [   96/  118]
train() client id: f_00009-1-0 loss: 1.184800  [   32/  118]
train() client id: f_00009-1-1 loss: 1.149248  [   64/  118]
train() client id: f_00009-1-2 loss: 1.098902  [   96/  118]
train() client id: f_00009-2-0 loss: 1.151579  [   32/  118]
train() client id: f_00009-2-1 loss: 0.981799  [   64/  118]
train() client id: f_00009-2-2 loss: 1.011470  [   96/  118]
train() client id: f_00009-3-0 loss: 1.048391  [   32/  118]
train() client id: f_00009-3-1 loss: 1.002049  [   64/  118]
train() client id: f_00009-3-2 loss: 1.069825  [   96/  118]
train() client id: f_00009-4-0 loss: 0.915888  [   32/  118]
train() client id: f_00009-4-1 loss: 1.005549  [   64/  118]
train() client id: f_00009-4-2 loss: 1.071904  [   96/  118]
train() client id: f_00009-5-0 loss: 1.035011  [   32/  118]
train() client id: f_00009-5-1 loss: 0.978285  [   64/  118]
train() client id: f_00009-5-2 loss: 1.068234  [   96/  118]
train() client id: f_00009-6-0 loss: 1.001630  [   32/  118]
train() client id: f_00009-6-1 loss: 0.968340  [   64/  118]
train() client id: f_00009-6-2 loss: 1.021244  [   96/  118]
train() client id: f_00009-7-0 loss: 0.967938  [   32/  118]
train() client id: f_00009-7-1 loss: 1.072413  [   64/  118]
train() client id: f_00009-7-2 loss: 0.894573  [   96/  118]
train() client id: f_00009-8-0 loss: 0.942073  [   32/  118]
train() client id: f_00009-8-1 loss: 1.063854  [   64/  118]
train() client id: f_00009-8-2 loss: 0.965693  [   96/  118]
train() client id: f_00009-9-0 loss: 0.971740  [   32/  118]
train() client id: f_00009-9-1 loss: 0.920030  [   64/  118]
train() client id: f_00009-9-2 loss: 0.847189  [   96/  118]
train() client id: f_00009-10-0 loss: 0.979055  [   32/  118]
train() client id: f_00009-10-1 loss: 1.093136  [   64/  118]
train() client id: f_00009-10-2 loss: 0.813934  [   96/  118]
train() client id: f_00009-11-0 loss: 0.922539  [   32/  118]
train() client id: f_00009-11-1 loss: 1.057834  [   64/  118]
train() client id: f_00009-11-2 loss: 0.956578  [   96/  118]
At round 30 accuracy: 0.649867374005305
At round 30 training accuracy: 0.5868544600938967
At round 30 training loss: 0.8206985539991785
update_location
xs = -4.528292 51.001589 60.045120 -60.943528 -40.103519 -0.217951 -102.215960 123.375741 -1.680116 44.695607 
ys = 137.587959 15.555839 1.320614 22.544824 9.350187 -12.185849 -3.124922 -19.177652 -67.154970 4.001482 
xs mean: 6.942869159412867
ys mean: 8.871751218646876
dists_uav = 170.149793 113.327606 116.649734 119.257632 108.146744 100.739974 143.031002 159.966733 120.468306 109.607067 
uav_gains = -105.793785 -101.358488 -101.672237 -101.912351 -100.850385 -100.080064 -103.888168 -105.111270 -102.022046 -100.996023 
uav_gains_db_mean: -102.36848175402068
dists_bs = 174.528691 276.584813 292.251155 190.398160 213.627997 256.100189 192.421423 355.995847 297.789878 278.400167 
bs_gains = -102.339330 -107.938252 -108.608234 -103.397616 -104.797488 -107.002536 -103.526155 -111.007517 -108.836537 -108.017805 
bs_gains_db_mean: -106.54714712552061
Round 31
-------------------------------
ene_coms = [0.0082865  0.00936577 0.00677204 0.0068452  0.00786302 0.00886341
 0.00751171 0.00799178 0.00990178 0.009411  ]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.88876164 14.34523666  6.75488164  2.42219729 16.51232014  7.9691158
  3.02258747  9.71934852  7.13144235  6.48066394]
obj_prev = 81.24655544473394
eta_min = 4.36690034173214e-14	eta_max = 0.9380302021582339
af = 17.150202743271663	bf = 1.4544875333207623	zeta = 18.86522301759883	eta = 0.9090909090909091
af = 17.150202743271663	bf = 1.4544875333207623	zeta = 33.97984084508871	eta = 0.5047169826797607
af = 17.150202743271663	bf = 1.4544875333207623	zeta = 26.61320042603408	eta = 0.644424664028557
af = 17.150202743271663	bf = 1.4544875333207623	zeta = 25.283646895192255	eta = 0.6783120652793492
af = 17.150202743271663	bf = 1.4544875333207623	zeta = 25.21481388638612	eta = 0.6801637648625013
af = 17.150202743271663	bf = 1.4544875333207623	zeta = 25.214614812271428	eta = 0.6801691348830369
eta = 0.6801691348830369
ene_coms = [0.0082865  0.00936577 0.00677204 0.0068452  0.00786302 0.00886341
 0.00751171 0.00799178 0.00990178 0.009411  ]
ene_comp = [0.03180342 0.06688815 0.03129861 0.01085355 0.07723688 0.03685158
 0.01363004 0.04518105 0.03281305 0.02978416]
ene_total = [2.20155925 4.18752467 2.09066989 0.97193611 4.67330595 2.51046334
 1.16101048 2.92001425 2.34570791 2.15242296]
ti_comp = [0.43520103 0.42440826 0.45034558 0.44961402 0.43943586 0.42943188
 0.44294888 0.43814821 0.41904819 0.42395599]
ti_coms = [0.08286499 0.09365775 0.06772044 0.06845199 0.07863015 0.08863414
 0.07511713 0.0799178  0.09901783 0.09411003]
t_total = [28.44986992 28.44986992 28.44986992 28.44986992 28.44986992 28.44986992
 28.44986992 28.44986992 28.44986992 28.44986992]
ene_coms = [0.0082865  0.00936577 0.00677204 0.0068452  0.00786302 0.00886341
 0.00751171 0.00799178 0.00990178 0.009411  ]
ene_comp = [1.06150425e-05 1.03838853e-04 9.44850670e-06 3.95289785e-07
 1.49129380e-04 1.69613312e-05 8.06610719e-07 3.00266534e-05
 1.25745383e-05 9.18743752e-06]
ene_total = [0.45564041 0.52002887 0.37240924 0.37592943 0.43999117 0.48767051
 0.41255402 0.44052178 0.54445218 0.51731473]
optimize_network iter = 0 obj = 4.566512329241354
eta = 0.6801691348830369
freqs = [36538771.50229015 78801656.8858988  34749542.36628039 12069851.10001355
 87881859.50457968 42907368.85535058 15385565.34464635 51559096.03178159
 39151886.43204936 35126472.60228232]
eta_min = 0.680169134883038	eta_max = 0.6801691348830324
af = 0.015630098762217635	bf = 1.4544875333207623	zeta = 0.0171931086384394	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.0082865  0.00936577 0.00677204 0.0068452  0.00786302 0.00886341
 0.00751171 0.00799178 0.00990178 0.009411  ]
ene_comp = [2.62502706e-06 2.56786347e-05 2.33655077e-06 9.77524470e-08
 3.68786707e-05 4.19442063e-06 1.99469287e-07 7.42538501e-06
 3.10959689e-06 2.27199015e-06]
ene_total = [1.60146284 1.81443351 1.30881362 1.32251471 1.52626344 1.7132263
 1.4513053  1.5454505  1.9136301  1.81864932]
ti_comp = [0.43520103 0.42440826 0.45034558 0.44961402 0.43943586 0.42943188
 0.44294888 0.43814821 0.41904819 0.42395599]
ti_coms = [0.08286499 0.09365775 0.06772044 0.06845199 0.07863015 0.08863414
 0.07511713 0.0799178  0.09901783 0.09411003]
t_total = [28.44986992 28.44986992 28.44986992 28.44986992 28.44986992 28.44986992
 28.44986992 28.44986992 28.44986992 28.44986992]
ene_coms = [0.0082865  0.00936577 0.00677204 0.0068452  0.00786302 0.00886341
 0.00751171 0.00799178 0.00990178 0.009411  ]
ene_comp = [1.06150425e-05 1.03838853e-04 9.44850670e-06 3.95289785e-07
 1.49129380e-04 1.69613312e-05 8.06610719e-07 3.00266534e-05
 1.25745383e-05 9.18743752e-06]
ene_total = [0.45564041 0.52002887 0.37240924 0.37592943 0.43999117 0.48767051
 0.41255402 0.44052178 0.54445218 0.51731473]
optimize_network iter = 1 obj = 4.56651232924129
eta = 0.6801691348830324
freqs = [36538771.50229015 78801656.88589875 34749542.36628042 12069851.10001356
 87881859.50457971 42907368.85535058 15385565.34464636 51559096.03178161
 39151886.43204935 35126472.60228231]
Done!
ene_coms = [0.0082865  0.00936577 0.00677204 0.0068452  0.00786302 0.00886341
 0.00751171 0.00799178 0.00990178 0.009411  ]
ene_comp = [1.00932186e-05 9.87342479e-05 8.98402837e-06 3.75857768e-07
 1.41798342e-04 1.61275306e-05 7.66958612e-07 2.85505757e-05
 1.19563876e-05 8.73579307e-06]
ene_total = [0.00829659 0.00946451 0.00678103 0.00684557 0.00800481 0.00887954
 0.00751248 0.00802033 0.00991374 0.00941974]
At round 31 energy consumption: 0.08313834684780537
At round 31 eta: 0.6801691348830324
At round 31 a_n: 17.56368160020554
At round 31 local rounds: 12.620405424040444
At round 31 global rounds: 54.915530412557914
gradient difference: 0.4888460636138916
train() client id: f_00000-0-0 loss: 0.989457  [   32/  126]
train() client id: f_00000-0-1 loss: 1.144749  [   64/  126]
train() client id: f_00000-0-2 loss: 0.805415  [   96/  126]
train() client id: f_00000-1-0 loss: 0.888956  [   32/  126]
train() client id: f_00000-1-1 loss: 1.017573  [   64/  126]
train() client id: f_00000-1-2 loss: 0.798471  [   96/  126]
train() client id: f_00000-2-0 loss: 0.999998  [   32/  126]
train() client id: f_00000-2-1 loss: 0.865443  [   64/  126]
train() client id: f_00000-2-2 loss: 0.795830  [   96/  126]
train() client id: f_00000-3-0 loss: 0.858211  [   32/  126]
train() client id: f_00000-3-1 loss: 0.798155  [   64/  126]
train() client id: f_00000-3-2 loss: 0.881293  [   96/  126]
train() client id: f_00000-4-0 loss: 0.871115  [   32/  126]
train() client id: f_00000-4-1 loss: 0.801181  [   64/  126]
train() client id: f_00000-4-2 loss: 0.906633  [   96/  126]
train() client id: f_00000-5-0 loss: 0.753701  [   32/  126]
train() client id: f_00000-5-1 loss: 0.822746  [   64/  126]
train() client id: f_00000-5-2 loss: 0.921159  [   96/  126]
train() client id: f_00000-6-0 loss: 0.896254  [   32/  126]
train() client id: f_00000-6-1 loss: 0.772390  [   64/  126]
train() client id: f_00000-6-2 loss: 0.756621  [   96/  126]
train() client id: f_00000-7-0 loss: 0.905560  [   32/  126]
train() client id: f_00000-7-1 loss: 0.762646  [   64/  126]
train() client id: f_00000-7-2 loss: 0.849365  [   96/  126]
train() client id: f_00000-8-0 loss: 0.753437  [   32/  126]
train() client id: f_00000-8-1 loss: 0.870498  [   64/  126]
train() client id: f_00000-8-2 loss: 0.894808  [   96/  126]
train() client id: f_00000-9-0 loss: 0.999175  [   32/  126]
train() client id: f_00000-9-1 loss: 0.783152  [   64/  126]
train() client id: f_00000-9-2 loss: 0.722009  [   96/  126]
train() client id: f_00000-10-0 loss: 0.879002  [   32/  126]
train() client id: f_00000-10-1 loss: 0.857417  [   64/  126]
train() client id: f_00000-10-2 loss: 0.818950  [   96/  126]
train() client id: f_00000-11-0 loss: 0.792317  [   32/  126]
train() client id: f_00000-11-1 loss: 0.855949  [   64/  126]
train() client id: f_00000-11-2 loss: 0.911482  [   96/  126]
train() client id: f_00001-0-0 loss: 0.598499  [   32/  265]
train() client id: f_00001-0-1 loss: 0.621387  [   64/  265]
train() client id: f_00001-0-2 loss: 0.443184  [   96/  265]
train() client id: f_00001-0-3 loss: 0.466003  [  128/  265]
train() client id: f_00001-0-4 loss: 0.665482  [  160/  265]
train() client id: f_00001-0-5 loss: 0.463358  [  192/  265]
train() client id: f_00001-0-6 loss: 0.449515  [  224/  265]
train() client id: f_00001-0-7 loss: 0.443292  [  256/  265]
train() client id: f_00001-1-0 loss: 0.614896  [   32/  265]
train() client id: f_00001-1-1 loss: 0.471127  [   64/  265]
train() client id: f_00001-1-2 loss: 0.548690  [   96/  265]
train() client id: f_00001-1-3 loss: 0.437243  [  128/  265]
train() client id: f_00001-1-4 loss: 0.481885  [  160/  265]
train() client id: f_00001-1-5 loss: 0.579524  [  192/  265]
train() client id: f_00001-1-6 loss: 0.533168  [  224/  265]
train() client id: f_00001-1-7 loss: 0.439694  [  256/  265]
train() client id: f_00001-2-0 loss: 0.444786  [   32/  265]
train() client id: f_00001-2-1 loss: 0.631375  [   64/  265]
train() client id: f_00001-2-2 loss: 0.492625  [   96/  265]
train() client id: f_00001-2-3 loss: 0.544695  [  128/  265]
train() client id: f_00001-2-4 loss: 0.500926  [  160/  265]
train() client id: f_00001-2-5 loss: 0.469504  [  192/  265]
train() client id: f_00001-2-6 loss: 0.489768  [  224/  265]
train() client id: f_00001-2-7 loss: 0.519972  [  256/  265]
train() client id: f_00001-3-0 loss: 0.509957  [   32/  265]
train() client id: f_00001-3-1 loss: 0.501738  [   64/  265]
train() client id: f_00001-3-2 loss: 0.412575  [   96/  265]
train() client id: f_00001-3-3 loss: 0.428370  [  128/  265]
train() client id: f_00001-3-4 loss: 0.468291  [  160/  265]
train() client id: f_00001-3-5 loss: 0.623720  [  192/  265]
train() client id: f_00001-3-6 loss: 0.474418  [  224/  265]
train() client id: f_00001-3-7 loss: 0.496620  [  256/  265]
train() client id: f_00001-4-0 loss: 0.421366  [   32/  265]
train() client id: f_00001-4-1 loss: 0.458868  [   64/  265]
train() client id: f_00001-4-2 loss: 0.468611  [   96/  265]
train() client id: f_00001-4-3 loss: 0.535991  [  128/  265]
train() client id: f_00001-4-4 loss: 0.463202  [  160/  265]
train() client id: f_00001-4-5 loss: 0.498519  [  192/  265]
train() client id: f_00001-4-6 loss: 0.645165  [  224/  265]
train() client id: f_00001-4-7 loss: 0.538520  [  256/  265]
train() client id: f_00001-5-0 loss: 0.730081  [   32/  265]
train() client id: f_00001-5-1 loss: 0.475005  [   64/  265]
train() client id: f_00001-5-2 loss: 0.413153  [   96/  265]
train() client id: f_00001-5-3 loss: 0.442625  [  128/  265]
train() client id: f_00001-5-4 loss: 0.468526  [  160/  265]
train() client id: f_00001-5-5 loss: 0.544188  [  192/  265]
train() client id: f_00001-5-6 loss: 0.421936  [  224/  265]
train() client id: f_00001-5-7 loss: 0.521048  [  256/  265]
train() client id: f_00001-6-0 loss: 0.453698  [   32/  265]
train() client id: f_00001-6-1 loss: 0.520290  [   64/  265]
train() client id: f_00001-6-2 loss: 0.564358  [   96/  265]
train() client id: f_00001-6-3 loss: 0.429309  [  128/  265]
train() client id: f_00001-6-4 loss: 0.478032  [  160/  265]
train() client id: f_00001-6-5 loss: 0.572680  [  192/  265]
train() client id: f_00001-6-6 loss: 0.512550  [  224/  265]
train() client id: f_00001-6-7 loss: 0.485181  [  256/  265]
train() client id: f_00001-7-0 loss: 0.402315  [   32/  265]
train() client id: f_00001-7-1 loss: 0.461715  [   64/  265]
train() client id: f_00001-7-2 loss: 0.420779  [   96/  265]
train() client id: f_00001-7-3 loss: 0.610991  [  128/  265]
train() client id: f_00001-7-4 loss: 0.450618  [  160/  265]
train() client id: f_00001-7-5 loss: 0.660810  [  192/  265]
train() client id: f_00001-7-6 loss: 0.519411  [  224/  265]
train() client id: f_00001-7-7 loss: 0.486729  [  256/  265]
train() client id: f_00001-8-0 loss: 0.454674  [   32/  265]
train() client id: f_00001-8-1 loss: 0.582855  [   64/  265]
train() client id: f_00001-8-2 loss: 0.696343  [   96/  265]
train() client id: f_00001-8-3 loss: 0.409742  [  128/  265]
train() client id: f_00001-8-4 loss: 0.438600  [  160/  265]
train() client id: f_00001-8-5 loss: 0.439141  [  192/  265]
train() client id: f_00001-8-6 loss: 0.466371  [  224/  265]
train() client id: f_00001-8-7 loss: 0.529008  [  256/  265]
train() client id: f_00001-9-0 loss: 0.597056  [   32/  265]
train() client id: f_00001-9-1 loss: 0.419997  [   64/  265]
train() client id: f_00001-9-2 loss: 0.406590  [   96/  265]
train() client id: f_00001-9-3 loss: 0.492763  [  128/  265]
train() client id: f_00001-9-4 loss: 0.477616  [  160/  265]
train() client id: f_00001-9-5 loss: 0.557641  [  192/  265]
train() client id: f_00001-9-6 loss: 0.581815  [  224/  265]
train() client id: f_00001-9-7 loss: 0.483467  [  256/  265]
train() client id: f_00001-10-0 loss: 0.561898  [   32/  265]
train() client id: f_00001-10-1 loss: 0.491197  [   64/  265]
train() client id: f_00001-10-2 loss: 0.432611  [   96/  265]
train() client id: f_00001-10-3 loss: 0.410271  [  128/  265]
train() client id: f_00001-10-4 loss: 0.577431  [  160/  265]
train() client id: f_00001-10-5 loss: 0.647910  [  192/  265]
train() client id: f_00001-10-6 loss: 0.398447  [  224/  265]
train() client id: f_00001-10-7 loss: 0.505450  [  256/  265]
train() client id: f_00001-11-0 loss: 0.621202  [   32/  265]
train() client id: f_00001-11-1 loss: 0.417348  [   64/  265]
train() client id: f_00001-11-2 loss: 0.476025  [   96/  265]
train() client id: f_00001-11-3 loss: 0.406643  [  128/  265]
train() client id: f_00001-11-4 loss: 0.582622  [  160/  265]
train() client id: f_00001-11-5 loss: 0.638178  [  192/  265]
train() client id: f_00001-11-6 loss: 0.404433  [  224/  265]
train() client id: f_00001-11-7 loss: 0.490219  [  256/  265]
train() client id: f_00002-0-0 loss: 1.383388  [   32/  124]
train() client id: f_00002-0-1 loss: 1.062901  [   64/  124]
train() client id: f_00002-0-2 loss: 1.163735  [   96/  124]
train() client id: f_00002-1-0 loss: 1.062864  [   32/  124]
train() client id: f_00002-1-1 loss: 1.235536  [   64/  124]
train() client id: f_00002-1-2 loss: 1.072316  [   96/  124]
train() client id: f_00002-2-0 loss: 1.110889  [   32/  124]
train() client id: f_00002-2-1 loss: 1.103760  [   64/  124]
train() client id: f_00002-2-2 loss: 1.150803  [   96/  124]
train() client id: f_00002-3-0 loss: 1.030104  [   32/  124]
train() client id: f_00002-3-1 loss: 1.298391  [   64/  124]
train() client id: f_00002-3-2 loss: 0.945860  [   96/  124]
train() client id: f_00002-4-0 loss: 1.072949  [   32/  124]
train() client id: f_00002-4-1 loss: 1.051704  [   64/  124]
train() client id: f_00002-4-2 loss: 1.157341  [   96/  124]
train() client id: f_00002-5-0 loss: 0.994378  [   32/  124]
train() client id: f_00002-5-1 loss: 1.057770  [   64/  124]
train() client id: f_00002-5-2 loss: 1.003579  [   96/  124]
train() client id: f_00002-6-0 loss: 0.877979  [   32/  124]
train() client id: f_00002-6-1 loss: 1.158868  [   64/  124]
train() client id: f_00002-6-2 loss: 0.986499  [   96/  124]
train() client id: f_00002-7-0 loss: 1.168207  [   32/  124]
train() client id: f_00002-7-1 loss: 0.991664  [   64/  124]
train() client id: f_00002-7-2 loss: 0.785445  [   96/  124]
train() client id: f_00002-8-0 loss: 1.115086  [   32/  124]
train() client id: f_00002-8-1 loss: 0.982068  [   64/  124]
train() client id: f_00002-8-2 loss: 0.831114  [   96/  124]
train() client id: f_00002-9-0 loss: 0.941565  [   32/  124]
train() client id: f_00002-9-1 loss: 0.898582  [   64/  124]
train() client id: f_00002-9-2 loss: 1.071897  [   96/  124]
train() client id: f_00002-10-0 loss: 0.871903  [   32/  124]
train() client id: f_00002-10-1 loss: 0.976651  [   64/  124]
train() client id: f_00002-10-2 loss: 0.886413  [   96/  124]
train() client id: f_00002-11-0 loss: 1.123272  [   32/  124]
train() client id: f_00002-11-1 loss: 0.917579  [   64/  124]
train() client id: f_00002-11-2 loss: 0.966240  [   96/  124]
train() client id: f_00003-0-0 loss: 0.434398  [   32/   43]
train() client id: f_00003-1-0 loss: 0.563989  [   32/   43]
train() client id: f_00003-2-0 loss: 0.623209  [   32/   43]
train() client id: f_00003-3-0 loss: 0.592145  [   32/   43]
train() client id: f_00003-4-0 loss: 0.368732  [   32/   43]
train() client id: f_00003-5-0 loss: 0.507496  [   32/   43]
train() client id: f_00003-6-0 loss: 0.541417  [   32/   43]
train() client id: f_00003-7-0 loss: 0.613037  [   32/   43]
train() client id: f_00003-8-0 loss: 0.484302  [   32/   43]
train() client id: f_00003-9-0 loss: 0.560569  [   32/   43]
train() client id: f_00003-10-0 loss: 0.496659  [   32/   43]
train() client id: f_00003-11-0 loss: 0.641321  [   32/   43]
train() client id: f_00004-0-0 loss: 0.879018  [   32/  306]
train() client id: f_00004-0-1 loss: 0.879293  [   64/  306]
train() client id: f_00004-0-2 loss: 0.989840  [   96/  306]
train() client id: f_00004-0-3 loss: 0.927550  [  128/  306]
train() client id: f_00004-0-4 loss: 0.912383  [  160/  306]
train() client id: f_00004-0-5 loss: 0.947679  [  192/  306]
train() client id: f_00004-0-6 loss: 0.936059  [  224/  306]
train() client id: f_00004-0-7 loss: 1.006220  [  256/  306]
train() client id: f_00004-0-8 loss: 0.920041  [  288/  306]
train() client id: f_00004-1-0 loss: 0.858364  [   32/  306]
train() client id: f_00004-1-1 loss: 0.900615  [   64/  306]
train() client id: f_00004-1-2 loss: 0.917812  [   96/  306]
train() client id: f_00004-1-3 loss: 0.955861  [  128/  306]
train() client id: f_00004-1-4 loss: 1.006328  [  160/  306]
train() client id: f_00004-1-5 loss: 1.045516  [  192/  306]
train() client id: f_00004-1-6 loss: 0.894205  [  224/  306]
train() client id: f_00004-1-7 loss: 0.901357  [  256/  306]
train() client id: f_00004-1-8 loss: 0.979560  [  288/  306]
train() client id: f_00004-2-0 loss: 0.947680  [   32/  306]
train() client id: f_00004-2-1 loss: 0.971017  [   64/  306]
train() client id: f_00004-2-2 loss: 0.831967  [   96/  306]
train() client id: f_00004-2-3 loss: 0.890292  [  128/  306]
train() client id: f_00004-2-4 loss: 0.984586  [  160/  306]
train() client id: f_00004-2-5 loss: 0.949462  [  192/  306]
train() client id: f_00004-2-6 loss: 0.829246  [  224/  306]
train() client id: f_00004-2-7 loss: 0.978420  [  256/  306]
train() client id: f_00004-2-8 loss: 0.931131  [  288/  306]
train() client id: f_00004-3-0 loss: 0.858161  [   32/  306]
train() client id: f_00004-3-1 loss: 0.983049  [   64/  306]
train() client id: f_00004-3-2 loss: 0.973652  [   96/  306]
train() client id: f_00004-3-3 loss: 0.980301  [  128/  306]
train() client id: f_00004-3-4 loss: 0.806264  [  160/  306]
train() client id: f_00004-3-5 loss: 0.840421  [  192/  306]
train() client id: f_00004-3-6 loss: 0.905155  [  224/  306]
train() client id: f_00004-3-7 loss: 0.943954  [  256/  306]
train() client id: f_00004-3-8 loss: 1.085515  [  288/  306]
train() client id: f_00004-4-0 loss: 0.806087  [   32/  306]
train() client id: f_00004-4-1 loss: 0.915995  [   64/  306]
train() client id: f_00004-4-2 loss: 0.931523  [   96/  306]
train() client id: f_00004-4-3 loss: 0.856242  [  128/  306]
train() client id: f_00004-4-4 loss: 0.847760  [  160/  306]
train() client id: f_00004-4-5 loss: 0.958812  [  192/  306]
train() client id: f_00004-4-6 loss: 1.043532  [  224/  306]
train() client id: f_00004-4-7 loss: 0.991919  [  256/  306]
train() client id: f_00004-4-8 loss: 0.917568  [  288/  306]
train() client id: f_00004-5-0 loss: 0.995966  [   32/  306]
train() client id: f_00004-5-1 loss: 0.966448  [   64/  306]
train() client id: f_00004-5-2 loss: 0.819543  [   96/  306]
train() client id: f_00004-5-3 loss: 0.924443  [  128/  306]
train() client id: f_00004-5-4 loss: 0.830675  [  160/  306]
train() client id: f_00004-5-5 loss: 0.906827  [  192/  306]
train() client id: f_00004-5-6 loss: 0.940066  [  224/  306]
train() client id: f_00004-5-7 loss: 0.856625  [  256/  306]
train() client id: f_00004-5-8 loss: 1.037758  [  288/  306]
train() client id: f_00004-6-0 loss: 1.007305  [   32/  306]
train() client id: f_00004-6-1 loss: 1.020683  [   64/  306]
train() client id: f_00004-6-2 loss: 0.884960  [   96/  306]
train() client id: f_00004-6-3 loss: 0.933999  [  128/  306]
train() client id: f_00004-6-4 loss: 0.889070  [  160/  306]
train() client id: f_00004-6-5 loss: 0.869986  [  192/  306]
train() client id: f_00004-6-6 loss: 0.902018  [  224/  306]
train() client id: f_00004-6-7 loss: 0.875613  [  256/  306]
train() client id: f_00004-6-8 loss: 0.851601  [  288/  306]
train() client id: f_00004-7-0 loss: 1.016947  [   32/  306]
train() client id: f_00004-7-1 loss: 0.892026  [   64/  306]
train() client id: f_00004-7-2 loss: 0.973708  [   96/  306]
train() client id: f_00004-7-3 loss: 0.912519  [  128/  306]
train() client id: f_00004-7-4 loss: 0.879330  [  160/  306]
train() client id: f_00004-7-5 loss: 0.873840  [  192/  306]
train() client id: f_00004-7-6 loss: 0.864493  [  224/  306]
train() client id: f_00004-7-7 loss: 0.815273  [  256/  306]
train() client id: f_00004-7-8 loss: 0.986510  [  288/  306]
train() client id: f_00004-8-0 loss: 0.821109  [   32/  306]
train() client id: f_00004-8-1 loss: 0.910313  [   64/  306]
train() client id: f_00004-8-2 loss: 1.071445  [   96/  306]
train() client id: f_00004-8-3 loss: 0.979358  [  128/  306]
train() client id: f_00004-8-4 loss: 1.021496  [  160/  306]
train() client id: f_00004-8-5 loss: 0.785512  [  192/  306]
train() client id: f_00004-8-6 loss: 0.825159  [  224/  306]
train() client id: f_00004-8-7 loss: 0.839176  [  256/  306]
train() client id: f_00004-8-8 loss: 0.934519  [  288/  306]
train() client id: f_00004-9-0 loss: 0.989955  [   32/  306]
train() client id: f_00004-9-1 loss: 0.860210  [   64/  306]
train() client id: f_00004-9-2 loss: 0.937192  [   96/  306]
train() client id: f_00004-9-3 loss: 0.877423  [  128/  306]
train() client id: f_00004-9-4 loss: 0.828963  [  160/  306]
train() client id: f_00004-9-5 loss: 0.936176  [  192/  306]
train() client id: f_00004-9-6 loss: 0.865445  [  224/  306]
train() client id: f_00004-9-7 loss: 0.981692  [  256/  306]
train() client id: f_00004-9-8 loss: 0.980765  [  288/  306]
train() client id: f_00004-10-0 loss: 0.803992  [   32/  306]
train() client id: f_00004-10-1 loss: 0.863232  [   64/  306]
train() client id: f_00004-10-2 loss: 0.872658  [   96/  306]
train() client id: f_00004-10-3 loss: 1.042883  [  128/  306]
train() client id: f_00004-10-4 loss: 0.791067  [  160/  306]
train() client id: f_00004-10-5 loss: 0.884690  [  192/  306]
train() client id: f_00004-10-6 loss: 0.995258  [  224/  306]
train() client id: f_00004-10-7 loss: 0.949068  [  256/  306]
train() client id: f_00004-10-8 loss: 0.959713  [  288/  306]
train() client id: f_00004-11-0 loss: 1.000354  [   32/  306]
train() client id: f_00004-11-1 loss: 0.859116  [   64/  306]
train() client id: f_00004-11-2 loss: 0.864373  [   96/  306]
train() client id: f_00004-11-3 loss: 0.892183  [  128/  306]
train() client id: f_00004-11-4 loss: 0.888371  [  160/  306]
train() client id: f_00004-11-5 loss: 0.908151  [  192/  306]
train() client id: f_00004-11-6 loss: 1.003122  [  224/  306]
train() client id: f_00004-11-7 loss: 0.843131  [  256/  306]
train() client id: f_00004-11-8 loss: 0.935247  [  288/  306]
train() client id: f_00005-0-0 loss: 0.596637  [   32/  146]
train() client id: f_00005-0-1 loss: 0.577237  [   64/  146]
train() client id: f_00005-0-2 loss: 0.540727  [   96/  146]
train() client id: f_00005-0-3 loss: 0.473258  [  128/  146]
train() client id: f_00005-1-0 loss: 0.352216  [   32/  146]
train() client id: f_00005-1-1 loss: 0.590477  [   64/  146]
train() client id: f_00005-1-2 loss: 0.932708  [   96/  146]
train() client id: f_00005-1-3 loss: 0.520169  [  128/  146]
train() client id: f_00005-2-0 loss: 0.680141  [   32/  146]
train() client id: f_00005-2-1 loss: 0.680715  [   64/  146]
train() client id: f_00005-2-2 loss: 0.526631  [   96/  146]
train() client id: f_00005-2-3 loss: 0.664604  [  128/  146]
train() client id: f_00005-3-0 loss: 0.651009  [   32/  146]
train() client id: f_00005-3-1 loss: 0.417516  [   64/  146]
train() client id: f_00005-3-2 loss: 0.641115  [   96/  146]
train() client id: f_00005-3-3 loss: 0.627765  [  128/  146]
train() client id: f_00005-4-0 loss: 0.859924  [   32/  146]
train() client id: f_00005-4-1 loss: 0.532322  [   64/  146]
train() client id: f_00005-4-2 loss: 0.516801  [   96/  146]
train() client id: f_00005-4-3 loss: 0.405686  [  128/  146]
train() client id: f_00005-5-0 loss: 0.717231  [   32/  146]
train() client id: f_00005-5-1 loss: 0.415767  [   64/  146]
train() client id: f_00005-5-2 loss: 0.643242  [   96/  146]
train() client id: f_00005-5-3 loss: 0.478023  [  128/  146]
train() client id: f_00005-6-0 loss: 0.660740  [   32/  146]
train() client id: f_00005-6-1 loss: 0.571261  [   64/  146]
train() client id: f_00005-6-2 loss: 0.491190  [   96/  146]
train() client id: f_00005-6-3 loss: 0.570256  [  128/  146]
train() client id: f_00005-7-0 loss: 0.816748  [   32/  146]
train() client id: f_00005-7-1 loss: 0.337123  [   64/  146]
train() client id: f_00005-7-2 loss: 0.654908  [   96/  146]
train() client id: f_00005-7-3 loss: 0.665865  [  128/  146]
train() client id: f_00005-8-0 loss: 0.333785  [   32/  146]
train() client id: f_00005-8-1 loss: 0.631499  [   64/  146]
train() client id: f_00005-8-2 loss: 0.706857  [   96/  146]
train() client id: f_00005-8-3 loss: 0.622439  [  128/  146]
train() client id: f_00005-9-0 loss: 0.369554  [   32/  146]
train() client id: f_00005-9-1 loss: 0.601301  [   64/  146]
train() client id: f_00005-9-2 loss: 0.795264  [   96/  146]
train() client id: f_00005-9-3 loss: 0.797529  [  128/  146]
train() client id: f_00005-10-0 loss: 0.357692  [   32/  146]
train() client id: f_00005-10-1 loss: 0.800544  [   64/  146]
train() client id: f_00005-10-2 loss: 0.212197  [   96/  146]
train() client id: f_00005-10-3 loss: 0.825378  [  128/  146]
train() client id: f_00005-11-0 loss: 0.443292  [   32/  146]
train() client id: f_00005-11-1 loss: 0.562482  [   64/  146]
train() client id: f_00005-11-2 loss: 0.469511  [   96/  146]
train() client id: f_00005-11-3 loss: 0.709137  [  128/  146]
train() client id: f_00006-0-0 loss: 0.517129  [   32/   54]
train() client id: f_00006-1-0 loss: 0.526980  [   32/   54]
train() client id: f_00006-2-0 loss: 0.488001  [   32/   54]
train() client id: f_00006-3-0 loss: 0.572286  [   32/   54]
train() client id: f_00006-4-0 loss: 0.572827  [   32/   54]
train() client id: f_00006-5-0 loss: 0.500341  [   32/   54]
train() client id: f_00006-6-0 loss: 0.470585  [   32/   54]
train() client id: f_00006-7-0 loss: 0.523918  [   32/   54]
train() client id: f_00006-8-0 loss: 0.521751  [   32/   54]
train() client id: f_00006-9-0 loss: 0.472904  [   32/   54]
train() client id: f_00006-10-0 loss: 0.482579  [   32/   54]
train() client id: f_00006-11-0 loss: 0.526058  [   32/   54]
train() client id: f_00007-0-0 loss: 0.677799  [   32/  179]
train() client id: f_00007-0-1 loss: 0.742200  [   64/  179]
train() client id: f_00007-0-2 loss: 0.614182  [   96/  179]
train() client id: f_00007-0-3 loss: 0.810122  [  128/  179]
train() client id: f_00007-0-4 loss: 0.560478  [  160/  179]
train() client id: f_00007-1-0 loss: 0.772691  [   32/  179]
train() client id: f_00007-1-1 loss: 0.493238  [   64/  179]
train() client id: f_00007-1-2 loss: 0.782410  [   96/  179]
train() client id: f_00007-1-3 loss: 0.617048  [  128/  179]
train() client id: f_00007-1-4 loss: 0.651680  [  160/  179]
train() client id: f_00007-2-0 loss: 0.690880  [   32/  179]
train() client id: f_00007-2-1 loss: 0.518637  [   64/  179]
train() client id: f_00007-2-2 loss: 0.673443  [   96/  179]
train() client id: f_00007-2-3 loss: 0.774405  [  128/  179]
train() client id: f_00007-2-4 loss: 0.500699  [  160/  179]
train() client id: f_00007-3-0 loss: 0.633950  [   32/  179]
train() client id: f_00007-3-1 loss: 0.522796  [   64/  179]
train() client id: f_00007-3-2 loss: 0.939561  [   96/  179]
train() client id: f_00007-3-3 loss: 0.663710  [  128/  179]
train() client id: f_00007-3-4 loss: 0.523789  [  160/  179]
train() client id: f_00007-4-0 loss: 0.814892  [   32/  179]
train() client id: f_00007-4-1 loss: 0.552489  [   64/  179]
train() client id: f_00007-4-2 loss: 0.786694  [   96/  179]
train() client id: f_00007-4-3 loss: 0.509374  [  128/  179]
train() client id: f_00007-4-4 loss: 0.497691  [  160/  179]
train() client id: f_00007-5-0 loss: 0.604719  [   32/  179]
train() client id: f_00007-5-1 loss: 0.554910  [   64/  179]
train() client id: f_00007-5-2 loss: 0.745264  [   96/  179]
train() client id: f_00007-5-3 loss: 0.660027  [  128/  179]
train() client id: f_00007-5-4 loss: 0.576350  [  160/  179]
train() client id: f_00007-6-0 loss: 0.635813  [   32/  179]
train() client id: f_00007-6-1 loss: 0.467324  [   64/  179]
train() client id: f_00007-6-2 loss: 0.521192  [   96/  179]
train() client id: f_00007-6-3 loss: 0.771998  [  128/  179]
train() client id: f_00007-6-4 loss: 0.474307  [  160/  179]
train() client id: f_00007-7-0 loss: 0.597439  [   32/  179]
train() client id: f_00007-7-1 loss: 0.818362  [   64/  179]
train() client id: f_00007-7-2 loss: 0.456846  [   96/  179]
train() client id: f_00007-7-3 loss: 0.550750  [  128/  179]
train() client id: f_00007-7-4 loss: 0.566997  [  160/  179]
train() client id: f_00007-8-0 loss: 0.843660  [   32/  179]
train() client id: f_00007-8-1 loss: 0.546856  [   64/  179]
train() client id: f_00007-8-2 loss: 0.663926  [   96/  179]
train() client id: f_00007-8-3 loss: 0.546162  [  128/  179]
train() client id: f_00007-8-4 loss: 0.571973  [  160/  179]
train() client id: f_00007-9-0 loss: 0.545382  [   32/  179]
train() client id: f_00007-9-1 loss: 0.750037  [   64/  179]
train() client id: f_00007-9-2 loss: 0.606366  [   96/  179]
train() client id: f_00007-9-3 loss: 0.765827  [  128/  179]
train() client id: f_00007-9-4 loss: 0.493300  [  160/  179]
train() client id: f_00007-10-0 loss: 0.655615  [   32/  179]
train() client id: f_00007-10-1 loss: 0.646869  [   64/  179]
train() client id: f_00007-10-2 loss: 0.664593  [   96/  179]
train() client id: f_00007-10-3 loss: 0.547512  [  128/  179]
train() client id: f_00007-10-4 loss: 0.626150  [  160/  179]
train() client id: f_00007-11-0 loss: 0.480925  [   32/  179]
train() client id: f_00007-11-1 loss: 0.543844  [   64/  179]
train() client id: f_00007-11-2 loss: 0.591650  [   96/  179]
train() client id: f_00007-11-3 loss: 0.572998  [  128/  179]
train() client id: f_00007-11-4 loss: 0.891104  [  160/  179]
train() client id: f_00008-0-0 loss: 0.779425  [   32/  130]
train() client id: f_00008-0-1 loss: 0.932261  [   64/  130]
train() client id: f_00008-0-2 loss: 0.731906  [   96/  130]
train() client id: f_00008-0-3 loss: 0.764084  [  128/  130]
train() client id: f_00008-1-0 loss: 0.822772  [   32/  130]
train() client id: f_00008-1-1 loss: 0.825370  [   64/  130]
train() client id: f_00008-1-2 loss: 0.881054  [   96/  130]
train() client id: f_00008-1-3 loss: 0.689200  [  128/  130]
train() client id: f_00008-2-0 loss: 0.796668  [   32/  130]
train() client id: f_00008-2-1 loss: 0.789334  [   64/  130]
train() client id: f_00008-2-2 loss: 0.881968  [   96/  130]
train() client id: f_00008-2-3 loss: 0.724470  [  128/  130]
train() client id: f_00008-3-0 loss: 0.790128  [   32/  130]
train() client id: f_00008-3-1 loss: 0.918815  [   64/  130]
train() client id: f_00008-3-2 loss: 0.864342  [   96/  130]
train() client id: f_00008-3-3 loss: 0.640363  [  128/  130]
train() client id: f_00008-4-0 loss: 0.765410  [   32/  130]
train() client id: f_00008-4-1 loss: 0.814500  [   64/  130]
train() client id: f_00008-4-2 loss: 0.731761  [   96/  130]
train() client id: f_00008-4-3 loss: 0.852143  [  128/  130]
train() client id: f_00008-5-0 loss: 0.729328  [   32/  130]
train() client id: f_00008-5-1 loss: 0.870326  [   64/  130]
train() client id: f_00008-5-2 loss: 0.789842  [   96/  130]
train() client id: f_00008-5-3 loss: 0.834536  [  128/  130]
train() client id: f_00008-6-0 loss: 0.839977  [   32/  130]
train() client id: f_00008-6-1 loss: 0.820257  [   64/  130]
train() client id: f_00008-6-2 loss: 0.779553  [   96/  130]
train() client id: f_00008-6-3 loss: 0.767268  [  128/  130]
train() client id: f_00008-7-0 loss: 0.766534  [   32/  130]
train() client id: f_00008-7-1 loss: 0.885083  [   64/  130]
train() client id: f_00008-7-2 loss: 0.758019  [   96/  130]
train() client id: f_00008-7-3 loss: 0.796186  [  128/  130]
train() client id: f_00008-8-0 loss: 0.914217  [   32/  130]
train() client id: f_00008-8-1 loss: 0.774774  [   64/  130]
train() client id: f_00008-8-2 loss: 0.696106  [   96/  130]
train() client id: f_00008-8-3 loss: 0.809763  [  128/  130]
train() client id: f_00008-9-0 loss: 0.861008  [   32/  130]
train() client id: f_00008-9-1 loss: 0.743814  [   64/  130]
train() client id: f_00008-9-2 loss: 0.829194  [   96/  130]
train() client id: f_00008-9-3 loss: 0.755115  [  128/  130]
train() client id: f_00008-10-0 loss: 0.878956  [   32/  130]
train() client id: f_00008-10-1 loss: 0.802806  [   64/  130]
train() client id: f_00008-10-2 loss: 0.795850  [   96/  130]
train() client id: f_00008-10-3 loss: 0.657945  [  128/  130]
train() client id: f_00008-11-0 loss: 0.762084  [   32/  130]
train() client id: f_00008-11-1 loss: 0.733321  [   64/  130]
train() client id: f_00008-11-2 loss: 0.823742  [   96/  130]
train() client id: f_00008-11-3 loss: 0.876244  [  128/  130]
train() client id: f_00009-0-0 loss: 1.234914  [   32/  118]
train() client id: f_00009-0-1 loss: 1.017549  [   64/  118]
train() client id: f_00009-0-2 loss: 0.965057  [   96/  118]
train() client id: f_00009-1-0 loss: 1.045779  [   32/  118]
train() client id: f_00009-1-1 loss: 1.136864  [   64/  118]
train() client id: f_00009-1-2 loss: 0.998431  [   96/  118]
train() client id: f_00009-2-0 loss: 0.944688  [   32/  118]
train() client id: f_00009-2-1 loss: 0.994792  [   64/  118]
train() client id: f_00009-2-2 loss: 1.150672  [   96/  118]
train() client id: f_00009-3-0 loss: 0.862935  [   32/  118]
train() client id: f_00009-3-1 loss: 0.984407  [   64/  118]
train() client id: f_00009-3-2 loss: 1.121626  [   96/  118]
train() client id: f_00009-4-0 loss: 1.133675  [   32/  118]
train() client id: f_00009-4-1 loss: 0.895587  [   64/  118]
train() client id: f_00009-4-2 loss: 0.810293  [   96/  118]
train() client id: f_00009-5-0 loss: 0.950858  [   32/  118]
train() client id: f_00009-5-1 loss: 0.888737  [   64/  118]
train() client id: f_00009-5-2 loss: 0.841810  [   96/  118]
train() client id: f_00009-6-0 loss: 0.848880  [   32/  118]
train() client id: f_00009-6-1 loss: 0.728423  [   64/  118]
train() client id: f_00009-6-2 loss: 0.889767  [   96/  118]
train() client id: f_00009-7-0 loss: 1.012361  [   32/  118]
train() client id: f_00009-7-1 loss: 0.782096  [   64/  118]
train() client id: f_00009-7-2 loss: 0.792413  [   96/  118]
train() client id: f_00009-8-0 loss: 0.901866  [   32/  118]
train() client id: f_00009-8-1 loss: 0.754475  [   64/  118]
train() client id: f_00009-8-2 loss: 0.886496  [   96/  118]
train() client id: f_00009-9-0 loss: 0.812688  [   32/  118]
train() client id: f_00009-9-1 loss: 0.850217  [   64/  118]
train() client id: f_00009-9-2 loss: 0.835082  [   96/  118]
train() client id: f_00009-10-0 loss: 0.790222  [   32/  118]
train() client id: f_00009-10-1 loss: 0.762980  [   64/  118]
train() client id: f_00009-10-2 loss: 0.821078  [   96/  118]
train() client id: f_00009-11-0 loss: 0.977921  [   32/  118]
train() client id: f_00009-11-1 loss: 0.601436  [   64/  118]
train() client id: f_00009-11-2 loss: 0.875842  [   96/  118]
At round 31 accuracy: 0.6525198938992043
At round 31 training accuracy: 0.5821596244131455
At round 31 training loss: 0.8296252476333364
update_location
xs = -4.528292 56.001589 65.045120 -65.943528 -35.103519 -0.217951 -107.215960 128.375741 -1.680116 49.695607 
ys = 142.587959 15.555839 1.320614 22.544824 9.350187 -7.185849 -3.124922 -19.177652 -72.154970 4.001482 
xs mean: 8.442869159412867
ys mean: 9.371751218646876
dists_uav = 174.217771 115.664005 119.300510 121.888548 106.393999 100.258087 146.645924 163.853938 123.325433 111.739273 
uav_gains = -106.058129 -101.580083 -101.916255 -102.149339 -100.672967 -100.028002 -104.160121 -105.375731 -102.276631 -101.205224 
uav_gains_db_mean: -102.54224816375688
dists_bs = 173.525628 280.685188 296.287341 187.445711 216.819939 252.468708 190.586368 360.196891 301.869776 282.362549 
bs_gains = -102.269241 -108.117205 -108.775026 -103.207573 -104.977837 -106.828871 -103.409631 -111.150178 -109.002009 -108.189658 
bs_gains_db_mean: -106.592722808469
Round 32
-------------------------------
ene_coms = [0.00840638 0.0094681  0.0068464  0.00691894 0.00793653 0.00877582
 0.00761352 0.00810353 0.01000692 0.00951014]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.75649488 14.06724039  6.62443411  2.37623978 16.19155781  7.81216977
  2.96540872  9.53173522  6.99418607  6.35599554]
obj_prev = 79.67546229194683
eta_min = 2.468412393008129e-14	eta_max = 0.938500787286958
af = 16.815721006576958	bf = 1.439450633645488	zeta = 18.497293107234654	eta = 0.9090909090909091
af = 16.815721006576958	bf = 1.439450633645488	zeta = 33.463760289538705	eta = 0.5025054226148582
af = 16.815721006576958	bf = 1.439450633645488	zeta = 26.153389928797118	eta = 0.64296525430768
af = 16.815721006576958	bf = 1.439450633645488	zeta = 24.833323131291895	eta = 0.6771434059659884
af = 16.815721006576958	bf = 1.439450633645488	zeta = 24.7646570419883	eta = 0.6790209522411726
af = 16.815721006576958	bf = 1.439450633645488	zeta = 24.764456398412914	eta = 0.6790264537223853
eta = 0.6790264537223853
ene_coms = [0.00840638 0.0094681  0.0068464  0.00691894 0.00793653 0.00877582
 0.00761352 0.00810353 0.01000692 0.00951014]
ene_comp = [0.03194217 0.06717996 0.03143515 0.0109009  0.07757384 0.03701235
 0.0136895  0.04537816 0.0329562  0.02991409]
ene_total = [2.1648132  4.1123844  2.0539131  0.95608475 4.58787201 2.45666443
 1.14296732 2.8694434  2.30509253 2.11522126]
ti_comp = [0.44526285 0.4346456  0.46086261 0.46013719 0.44996136 0.44156845
 0.45319138 0.44829132 0.42925747 0.43422523]
ti_coms = [0.08406378 0.09468102 0.06846401 0.06918943 0.07936527 0.08775817
 0.07613524 0.0810353  0.10006916 0.0951014 ]
t_total = [28.39986572 28.39986572 28.39986572 28.39986572 28.39986572 28.39986572
 28.39986572 28.39986572 28.39986572 28.39986572]
ene_coms = [0.00840638 0.0094681  0.0068464  0.00691894 0.00793653 0.00877582
 0.00761352 0.00810353 0.01000692 0.00951014]
ene_comp = [1.02740162e-05 1.00306384e-04 9.14078792e-06 3.82377409e-07
 1.44103780e-04 1.62526442e-05 7.80691627e-07 2.90602914e-05
 1.21410394e-05 8.87314241e-06]
ene_total = [0.45157709 0.5133721  0.36781915 0.37124133 0.4335486  0.47171934
 0.40852891 0.43633642 0.53755064 0.51072188]
optimize_network iter = 0 obj = 4.502415459666557
eta = 0.6790264537223853
freqs = [35868889.23939817 77281301.61952248 34104685.64947879 11845269.93771866
 86200552.68461472 41910096.70758054 15103442.89605517 50612354.79222963
 38387456.38801759 34445365.6606557 ]
eta_min = 0.6790264537224637	eta_max = 0.6790264537223574
af = 0.014739586984077952	bf = 1.439450633645488	zeta = 0.016213545682485748	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00840638 0.0094681  0.0068464  0.00691894 0.00793653 0.00877582
 0.00761352 0.00810353 0.01000692 0.00951014]
ene_comp = [2.52965770e-06 2.46973347e-05 2.25063539e-06 9.41485719e-08
 3.54810844e-05 4.00170932e-06 1.92221089e-07 7.15519503e-06
 2.98935423e-06 2.18473599e-06]
ene_total = [1.59292025 1.79824469 1.29735718 1.31069052 1.51015728 1.66318301
 1.44228523 1.53642738 1.89620125 1.80194333]
ti_comp = [0.44526285 0.4346456  0.46086261 0.46013719 0.44996136 0.44156845
 0.45319138 0.44829132 0.42925747 0.43422523]
ti_coms = [0.08406378 0.09468102 0.06846401 0.06918943 0.07936527 0.08775817
 0.07613524 0.0810353  0.10006916 0.0951014 ]
t_total = [28.39986572 28.39986572 28.39986572 28.39986572 28.39986572 28.39986572
 28.39986572 28.39986572 28.39986572 28.39986572]
ene_coms = [0.00840638 0.0094681  0.0068464  0.00691894 0.00793653 0.00877582
 0.00761352 0.00810353 0.01000692 0.00951014]
ene_comp = [1.02740162e-05 1.00306384e-04 9.14078792e-06 3.82377409e-07
 1.44103780e-04 1.62526442e-05 7.80691627e-07 2.90602914e-05
 1.21410394e-05 8.87314241e-06]
ene_total = [0.45157709 0.5133721  0.36781915 0.37124133 0.4335486  0.47171934
 0.40852891 0.43633642 0.53755064 0.51072188]
optimize_network iter = 1 obj = 4.502415459666167
eta = 0.6790264537223574
freqs = [35868889.23939829 77281301.61952254 34104685.64947902 11845269.93771873
 86200552.6846151  41910096.70758065 15103442.89605525 50612354.79222983
 38387456.38801758 34445365.66065571]
Done!
ene_coms = [0.00840638 0.0094681  0.0068464  0.00691894 0.00793653 0.00877582
 0.00761352 0.00810353 0.01000692 0.00951014]
ene_comp = [9.72652375e-06 9.49611533e-05 8.65368410e-06 3.62000883e-07
 1.36424628e-04 1.53865564e-05 7.39089319e-07 2.75116963e-05
 1.14940551e-05 8.40030117e-06]
ene_total = [0.0084161  0.00956306 0.00685505 0.00691931 0.00807295 0.0087912
 0.00761426 0.00813104 0.01001841 0.00951854]
At round 32 energy consumption: 0.08389993728743161
At round 32 eta: 0.6790264537223574
At round 32 a_n: 17.221135753236222
At round 32 local rounds: 12.675463277150001
At round 32 global rounds: 53.65281953279699
gradient difference: 0.5141579508781433
train() client id: f_00000-0-0 loss: 0.929473  [   32/  126]
train() client id: f_00000-0-1 loss: 1.042865  [   64/  126]
train() client id: f_00000-0-2 loss: 0.809323  [   96/  126]
train() client id: f_00000-1-0 loss: 0.977418  [   32/  126]
train() client id: f_00000-1-1 loss: 0.811075  [   64/  126]
train() client id: f_00000-1-2 loss: 1.029627  [   96/  126]
train() client id: f_00000-2-0 loss: 0.762243  [   32/  126]
train() client id: f_00000-2-1 loss: 0.930321  [   64/  126]
train() client id: f_00000-2-2 loss: 0.921190  [   96/  126]
train() client id: f_00000-3-0 loss: 0.844674  [   32/  126]
train() client id: f_00000-3-1 loss: 0.857600  [   64/  126]
train() client id: f_00000-3-2 loss: 0.829643  [   96/  126]
train() client id: f_00000-4-0 loss: 0.737210  [   32/  126]
train() client id: f_00000-4-1 loss: 0.873539  [   64/  126]
train() client id: f_00000-4-2 loss: 0.902161  [   96/  126]
train() client id: f_00000-5-0 loss: 0.682745  [   32/  126]
train() client id: f_00000-5-1 loss: 0.849559  [   64/  126]
train() client id: f_00000-5-2 loss: 0.877207  [   96/  126]
train() client id: f_00000-6-0 loss: 0.760515  [   32/  126]
train() client id: f_00000-6-1 loss: 0.781747  [   64/  126]
train() client id: f_00000-6-2 loss: 0.788888  [   96/  126]
train() client id: f_00000-7-0 loss: 0.777457  [   32/  126]
train() client id: f_00000-7-1 loss: 0.978076  [   64/  126]
train() client id: f_00000-7-2 loss: 0.748352  [   96/  126]
train() client id: f_00000-8-0 loss: 0.824346  [   32/  126]
train() client id: f_00000-8-1 loss: 0.765341  [   64/  126]
train() client id: f_00000-8-2 loss: 0.872558  [   96/  126]
train() client id: f_00000-9-0 loss: 0.829138  [   32/  126]
train() client id: f_00000-9-1 loss: 0.904471  [   64/  126]
train() client id: f_00000-9-2 loss: 0.769754  [   96/  126]
train() client id: f_00000-10-0 loss: 0.723989  [   32/  126]
train() client id: f_00000-10-1 loss: 0.875757  [   64/  126]
train() client id: f_00000-10-2 loss: 0.940932  [   96/  126]
train() client id: f_00000-11-0 loss: 0.799857  [   32/  126]
train() client id: f_00000-11-1 loss: 0.874557  [   64/  126]
train() client id: f_00000-11-2 loss: 0.860991  [   96/  126]
train() client id: f_00001-0-0 loss: 0.478011  [   32/  265]
train() client id: f_00001-0-1 loss: 0.564399  [   64/  265]
train() client id: f_00001-0-2 loss: 0.613755  [   96/  265]
train() client id: f_00001-0-3 loss: 0.491604  [  128/  265]
train() client id: f_00001-0-4 loss: 0.424809  [  160/  265]
train() client id: f_00001-0-5 loss: 0.483167  [  192/  265]
train() client id: f_00001-0-6 loss: 0.470556  [  224/  265]
train() client id: f_00001-0-7 loss: 0.449670  [  256/  265]
train() client id: f_00001-1-0 loss: 0.598856  [   32/  265]
train() client id: f_00001-1-1 loss: 0.603757  [   64/  265]
train() client id: f_00001-1-2 loss: 0.401976  [   96/  265]
train() client id: f_00001-1-3 loss: 0.410017  [  128/  265]
train() client id: f_00001-1-4 loss: 0.461087  [  160/  265]
train() client id: f_00001-1-5 loss: 0.448192  [  192/  265]
train() client id: f_00001-1-6 loss: 0.468926  [  224/  265]
train() client id: f_00001-1-7 loss: 0.526644  [  256/  265]
train() client id: f_00001-2-0 loss: 0.461613  [   32/  265]
train() client id: f_00001-2-1 loss: 0.418573  [   64/  265]
train() client id: f_00001-2-2 loss: 0.488878  [   96/  265]
train() client id: f_00001-2-3 loss: 0.607169  [  128/  265]
train() client id: f_00001-2-4 loss: 0.463138  [  160/  265]
train() client id: f_00001-2-5 loss: 0.550015  [  192/  265]
train() client id: f_00001-2-6 loss: 0.409601  [  224/  265]
train() client id: f_00001-2-7 loss: 0.478162  [  256/  265]
train() client id: f_00001-3-0 loss: 0.499615  [   32/  265]
train() client id: f_00001-3-1 loss: 0.445515  [   64/  265]
train() client id: f_00001-3-2 loss: 0.592363  [   96/  265]
train() client id: f_00001-3-3 loss: 0.478480  [  128/  265]
train() client id: f_00001-3-4 loss: 0.534593  [  160/  265]
train() client id: f_00001-3-5 loss: 0.416246  [  192/  265]
train() client id: f_00001-3-6 loss: 0.445380  [  224/  265]
train() client id: f_00001-3-7 loss: 0.487510  [  256/  265]
train() client id: f_00001-4-0 loss: 0.701555  [   32/  265]
train() client id: f_00001-4-1 loss: 0.415000  [   64/  265]
train() client id: f_00001-4-2 loss: 0.556617  [   96/  265]
train() client id: f_00001-4-3 loss: 0.398854  [  128/  265]
train() client id: f_00001-4-4 loss: 0.434703  [  160/  265]
train() client id: f_00001-4-5 loss: 0.549439  [  192/  265]
train() client id: f_00001-4-6 loss: 0.425662  [  224/  265]
train() client id: f_00001-4-7 loss: 0.409100  [  256/  265]
train() client id: f_00001-5-0 loss: 0.477676  [   32/  265]
train() client id: f_00001-5-1 loss: 0.434741  [   64/  265]
train() client id: f_00001-5-2 loss: 0.520174  [   96/  265]
train() client id: f_00001-5-3 loss: 0.398607  [  128/  265]
train() client id: f_00001-5-4 loss: 0.529161  [  160/  265]
train() client id: f_00001-5-5 loss: 0.600034  [  192/  265]
train() client id: f_00001-5-6 loss: 0.484685  [  224/  265]
train() client id: f_00001-5-7 loss: 0.426755  [  256/  265]
train() client id: f_00001-6-0 loss: 0.457062  [   32/  265]
train() client id: f_00001-6-1 loss: 0.390172  [   64/  265]
train() client id: f_00001-6-2 loss: 0.435653  [   96/  265]
train() client id: f_00001-6-3 loss: 0.451883  [  128/  265]
train() client id: f_00001-6-4 loss: 0.435192  [  160/  265]
train() client id: f_00001-6-5 loss: 0.563262  [  192/  265]
train() client id: f_00001-6-6 loss: 0.472949  [  224/  265]
train() client id: f_00001-6-7 loss: 0.534817  [  256/  265]
train() client id: f_00001-7-0 loss: 0.479342  [   32/  265]
train() client id: f_00001-7-1 loss: 0.418856  [   64/  265]
train() client id: f_00001-7-2 loss: 0.459928  [   96/  265]
train() client id: f_00001-7-3 loss: 0.542208  [  128/  265]
train() client id: f_00001-7-4 loss: 0.512825  [  160/  265]
train() client id: f_00001-7-5 loss: 0.435708  [  192/  265]
train() client id: f_00001-7-6 loss: 0.522866  [  224/  265]
train() client id: f_00001-7-7 loss: 0.500212  [  256/  265]
train() client id: f_00001-8-0 loss: 0.442021  [   32/  265]
train() client id: f_00001-8-1 loss: 0.456950  [   64/  265]
train() client id: f_00001-8-2 loss: 0.447643  [   96/  265]
train() client id: f_00001-8-3 loss: 0.553672  [  128/  265]
train() client id: f_00001-8-4 loss: 0.483353  [  160/  265]
train() client id: f_00001-8-5 loss: 0.519195  [  192/  265]
train() client id: f_00001-8-6 loss: 0.596918  [  224/  265]
train() client id: f_00001-8-7 loss: 0.378494  [  256/  265]
train() client id: f_00001-9-0 loss: 0.535089  [   32/  265]
train() client id: f_00001-9-1 loss: 0.409428  [   64/  265]
train() client id: f_00001-9-2 loss: 0.419444  [   96/  265]
train() client id: f_00001-9-3 loss: 0.530596  [  128/  265]
train() client id: f_00001-9-4 loss: 0.479752  [  160/  265]
train() client id: f_00001-9-5 loss: 0.469532  [  192/  265]
train() client id: f_00001-9-6 loss: 0.544412  [  224/  265]
train() client id: f_00001-9-7 loss: 0.494750  [  256/  265]
train() client id: f_00001-10-0 loss: 0.393196  [   32/  265]
train() client id: f_00001-10-1 loss: 0.531376  [   64/  265]
train() client id: f_00001-10-2 loss: 0.491480  [   96/  265]
train() client id: f_00001-10-3 loss: 0.494232  [  128/  265]
train() client id: f_00001-10-4 loss: 0.384004  [  160/  265]
train() client id: f_00001-10-5 loss: 0.653069  [  192/  265]
train() client id: f_00001-10-6 loss: 0.402608  [  224/  265]
train() client id: f_00001-10-7 loss: 0.524987  [  256/  265]
train() client id: f_00001-11-0 loss: 0.463132  [   32/  265]
train() client id: f_00001-11-1 loss: 0.614754  [   64/  265]
train() client id: f_00001-11-2 loss: 0.444928  [   96/  265]
train() client id: f_00001-11-3 loss: 0.498376  [  128/  265]
train() client id: f_00001-11-4 loss: 0.439782  [  160/  265]
train() client id: f_00001-11-5 loss: 0.477230  [  192/  265]
train() client id: f_00001-11-6 loss: 0.497821  [  224/  265]
train() client id: f_00001-11-7 loss: 0.412689  [  256/  265]
train() client id: f_00002-0-0 loss: 1.270893  [   32/  124]
train() client id: f_00002-0-1 loss: 0.890010  [   64/  124]
train() client id: f_00002-0-2 loss: 1.096352  [   96/  124]
train() client id: f_00002-1-0 loss: 1.080834  [   32/  124]
train() client id: f_00002-1-1 loss: 1.019614  [   64/  124]
train() client id: f_00002-1-2 loss: 1.077781  [   96/  124]
train() client id: f_00002-2-0 loss: 1.200202  [   32/  124]
train() client id: f_00002-2-1 loss: 0.965726  [   64/  124]
train() client id: f_00002-2-2 loss: 0.951906  [   96/  124]
train() client id: f_00002-3-0 loss: 0.984223  [   32/  124]
train() client id: f_00002-3-1 loss: 0.971380  [   64/  124]
train() client id: f_00002-3-2 loss: 1.122992  [   96/  124]
train() client id: f_00002-4-0 loss: 1.090823  [   32/  124]
train() client id: f_00002-4-1 loss: 0.865110  [   64/  124]
train() client id: f_00002-4-2 loss: 1.053071  [   96/  124]
train() client id: f_00002-5-0 loss: 0.976214  [   32/  124]
train() client id: f_00002-5-1 loss: 1.032537  [   64/  124]
train() client id: f_00002-5-2 loss: 0.953693  [   96/  124]
train() client id: f_00002-6-0 loss: 1.005439  [   32/  124]
train() client id: f_00002-6-1 loss: 0.985762  [   64/  124]
train() client id: f_00002-6-2 loss: 0.928910  [   96/  124]
train() client id: f_00002-7-0 loss: 1.078430  [   32/  124]
train() client id: f_00002-7-1 loss: 0.960102  [   64/  124]
train() client id: f_00002-7-2 loss: 0.893132  [   96/  124]
train() client id: f_00002-8-0 loss: 1.014752  [   32/  124]
train() client id: f_00002-8-1 loss: 0.856339  [   64/  124]
train() client id: f_00002-8-2 loss: 0.855931  [   96/  124]
train() client id: f_00002-9-0 loss: 0.901910  [   32/  124]
train() client id: f_00002-9-1 loss: 0.896431  [   64/  124]
train() client id: f_00002-9-2 loss: 0.920772  [   96/  124]
train() client id: f_00002-10-0 loss: 0.854986  [   32/  124]
train() client id: f_00002-10-1 loss: 1.070864  [   64/  124]
train() client id: f_00002-10-2 loss: 0.921756  [   96/  124]
train() client id: f_00002-11-0 loss: 0.846512  [   32/  124]
train() client id: f_00002-11-1 loss: 1.048482  [   64/  124]
train() client id: f_00002-11-2 loss: 0.765099  [   96/  124]
train() client id: f_00003-0-0 loss: 0.470647  [   32/   43]
train() client id: f_00003-1-0 loss: 0.720522  [   32/   43]
train() client id: f_00003-2-0 loss: 0.570355  [   32/   43]
train() client id: f_00003-3-0 loss: 0.437149  [   32/   43]
train() client id: f_00003-4-0 loss: 0.582944  [   32/   43]
train() client id: f_00003-5-0 loss: 0.569984  [   32/   43]
train() client id: f_00003-6-0 loss: 0.642605  [   32/   43]
train() client id: f_00003-7-0 loss: 0.384786  [   32/   43]
train() client id: f_00003-8-0 loss: 0.797532  [   32/   43]
train() client id: f_00003-9-0 loss: 0.484137  [   32/   43]
train() client id: f_00003-10-0 loss: 0.727938  [   32/   43]
train() client id: f_00003-11-0 loss: 0.592653  [   32/   43]
train() client id: f_00004-0-0 loss: 0.817661  [   32/  306]
train() client id: f_00004-0-1 loss: 0.847770  [   64/  306]
train() client id: f_00004-0-2 loss: 0.791097  [   96/  306]
train() client id: f_00004-0-3 loss: 0.947243  [  128/  306]
train() client id: f_00004-0-4 loss: 0.811106  [  160/  306]
train() client id: f_00004-0-5 loss: 0.818054  [  192/  306]
train() client id: f_00004-0-6 loss: 0.745680  [  224/  306]
train() client id: f_00004-0-7 loss: 0.863398  [  256/  306]
train() client id: f_00004-0-8 loss: 1.026927  [  288/  306]
train() client id: f_00004-1-0 loss: 0.912257  [   32/  306]
train() client id: f_00004-1-1 loss: 0.751258  [   64/  306]
train() client id: f_00004-1-2 loss: 0.968907  [   96/  306]
train() client id: f_00004-1-3 loss: 0.975357  [  128/  306]
train() client id: f_00004-1-4 loss: 0.852427  [  160/  306]
train() client id: f_00004-1-5 loss: 0.826446  [  192/  306]
train() client id: f_00004-1-6 loss: 0.906347  [  224/  306]
train() client id: f_00004-1-7 loss: 0.723282  [  256/  306]
train() client id: f_00004-1-8 loss: 0.764706  [  288/  306]
train() client id: f_00004-2-0 loss: 0.807077  [   32/  306]
train() client id: f_00004-2-1 loss: 0.784228  [   64/  306]
train() client id: f_00004-2-2 loss: 0.900734  [   96/  306]
train() client id: f_00004-2-3 loss: 0.870214  [  128/  306]
train() client id: f_00004-2-4 loss: 1.018664  [  160/  306]
train() client id: f_00004-2-5 loss: 0.797608  [  192/  306]
train() client id: f_00004-2-6 loss: 0.975209  [  224/  306]
train() client id: f_00004-2-7 loss: 0.825337  [  256/  306]
train() client id: f_00004-2-8 loss: 0.785599  [  288/  306]
train() client id: f_00004-3-0 loss: 0.782808  [   32/  306]
train() client id: f_00004-3-1 loss: 0.790015  [   64/  306]
train() client id: f_00004-3-2 loss: 1.014523  [   96/  306]
train() client id: f_00004-3-3 loss: 0.835511  [  128/  306]
train() client id: f_00004-3-4 loss: 0.835315  [  160/  306]
train() client id: f_00004-3-5 loss: 0.974927  [  192/  306]
train() client id: f_00004-3-6 loss: 0.858198  [  224/  306]
train() client id: f_00004-3-7 loss: 0.809973  [  256/  306]
train() client id: f_00004-3-8 loss: 0.794007  [  288/  306]
train() client id: f_00004-4-0 loss: 0.886398  [   32/  306]
train() client id: f_00004-4-1 loss: 0.837719  [   64/  306]
train() client id: f_00004-4-2 loss: 0.893747  [   96/  306]
train() client id: f_00004-4-3 loss: 0.892527  [  128/  306]
train() client id: f_00004-4-4 loss: 0.864136  [  160/  306]
train() client id: f_00004-4-5 loss: 0.905414  [  192/  306]
train() client id: f_00004-4-6 loss: 0.746122  [  224/  306]
train() client id: f_00004-4-7 loss: 0.893139  [  256/  306]
train() client id: f_00004-4-8 loss: 0.796819  [  288/  306]
train() client id: f_00004-5-0 loss: 0.928303  [   32/  306]
train() client id: f_00004-5-1 loss: 0.866706  [   64/  306]
train() client id: f_00004-5-2 loss: 0.976411  [   96/  306]
train() client id: f_00004-5-3 loss: 0.762202  [  128/  306]
train() client id: f_00004-5-4 loss: 0.864637  [  160/  306]
train() client id: f_00004-5-5 loss: 0.758771  [  192/  306]
train() client id: f_00004-5-6 loss: 0.888404  [  224/  306]
train() client id: f_00004-5-7 loss: 0.772567  [  256/  306]
train() client id: f_00004-5-8 loss: 0.830021  [  288/  306]
train() client id: f_00004-6-0 loss: 0.770341  [   32/  306]
train() client id: f_00004-6-1 loss: 0.847968  [   64/  306]
train() client id: f_00004-6-2 loss: 0.836735  [   96/  306]
train() client id: f_00004-6-3 loss: 0.851463  [  128/  306]
train() client id: f_00004-6-4 loss: 0.847946  [  160/  306]
train() client id: f_00004-6-5 loss: 0.813025  [  192/  306]
train() client id: f_00004-6-6 loss: 0.872761  [  224/  306]
train() client id: f_00004-6-7 loss: 0.953889  [  256/  306]
train() client id: f_00004-6-8 loss: 0.842631  [  288/  306]
train() client id: f_00004-7-0 loss: 0.981314  [   32/  306]
train() client id: f_00004-7-1 loss: 0.747559  [   64/  306]
train() client id: f_00004-7-2 loss: 0.899903  [   96/  306]
train() client id: f_00004-7-3 loss: 0.826231  [  128/  306]
train() client id: f_00004-7-4 loss: 0.816075  [  160/  306]
train() client id: f_00004-7-5 loss: 0.741910  [  192/  306]
train() client id: f_00004-7-6 loss: 0.866288  [  224/  306]
train() client id: f_00004-7-7 loss: 0.916215  [  256/  306]
train() client id: f_00004-7-8 loss: 0.901891  [  288/  306]
train() client id: f_00004-8-0 loss: 0.716981  [   32/  306]
train() client id: f_00004-8-1 loss: 0.909468  [   64/  306]
train() client id: f_00004-8-2 loss: 0.902496  [   96/  306]
train() client id: f_00004-8-3 loss: 0.836607  [  128/  306]
train() client id: f_00004-8-4 loss: 0.805853  [  160/  306]
train() client id: f_00004-8-5 loss: 0.930871  [  192/  306]
train() client id: f_00004-8-6 loss: 0.783307  [  224/  306]
train() client id: f_00004-8-7 loss: 0.913859  [  256/  306]
train() client id: f_00004-8-8 loss: 0.789412  [  288/  306]
train() client id: f_00004-9-0 loss: 0.870127  [   32/  306]
train() client id: f_00004-9-1 loss: 0.847575  [   64/  306]
train() client id: f_00004-9-2 loss: 0.833076  [   96/  306]
train() client id: f_00004-9-3 loss: 0.842950  [  128/  306]
train() client id: f_00004-9-4 loss: 0.858422  [  160/  306]
train() client id: f_00004-9-5 loss: 0.737295  [  192/  306]
train() client id: f_00004-9-6 loss: 0.851824  [  224/  306]
train() client id: f_00004-9-7 loss: 0.833885  [  256/  306]
train() client id: f_00004-9-8 loss: 0.899142  [  288/  306]
train() client id: f_00004-10-0 loss: 0.796201  [   32/  306]
train() client id: f_00004-10-1 loss: 0.833852  [   64/  306]
train() client id: f_00004-10-2 loss: 0.808540  [   96/  306]
train() client id: f_00004-10-3 loss: 0.931208  [  128/  306]
train() client id: f_00004-10-4 loss: 0.862825  [  160/  306]
train() client id: f_00004-10-5 loss: 0.845048  [  192/  306]
train() client id: f_00004-10-6 loss: 0.816423  [  224/  306]
train() client id: f_00004-10-7 loss: 0.832929  [  256/  306]
train() client id: f_00004-10-8 loss: 0.808556  [  288/  306]
train() client id: f_00004-11-0 loss: 0.877001  [   32/  306]
train() client id: f_00004-11-1 loss: 0.940931  [   64/  306]
train() client id: f_00004-11-2 loss: 0.799016  [   96/  306]
train() client id: f_00004-11-3 loss: 0.838093  [  128/  306]
train() client id: f_00004-11-4 loss: 0.805800  [  160/  306]
train() client id: f_00004-11-5 loss: 0.853346  [  192/  306]
train() client id: f_00004-11-6 loss: 0.842101  [  224/  306]
train() client id: f_00004-11-7 loss: 0.886795  [  256/  306]
train() client id: f_00004-11-8 loss: 0.733439  [  288/  306]
train() client id: f_00005-0-0 loss: 0.564632  [   32/  146]
train() client id: f_00005-0-1 loss: 0.686346  [   64/  146]
train() client id: f_00005-0-2 loss: 0.501395  [   96/  146]
train() client id: f_00005-0-3 loss: 0.515949  [  128/  146]
train() client id: f_00005-1-0 loss: 0.580312  [   32/  146]
train() client id: f_00005-1-1 loss: 0.484442  [   64/  146]
train() client id: f_00005-1-2 loss: 0.585533  [   96/  146]
train() client id: f_00005-1-3 loss: 0.411343  [  128/  146]
train() client id: f_00005-2-0 loss: 0.605087  [   32/  146]
train() client id: f_00005-2-1 loss: 0.556840  [   64/  146]
train() client id: f_00005-2-2 loss: 0.533857  [   96/  146]
train() client id: f_00005-2-3 loss: 0.524250  [  128/  146]
train() client id: f_00005-3-0 loss: 0.591455  [   32/  146]
train() client id: f_00005-3-1 loss: 0.710857  [   64/  146]
train() client id: f_00005-3-2 loss: 0.444410  [   96/  146]
train() client id: f_00005-3-3 loss: 0.362416  [  128/  146]
train() client id: f_00005-4-0 loss: 0.761935  [   32/  146]
train() client id: f_00005-4-1 loss: 0.524296  [   64/  146]
train() client id: f_00005-4-2 loss: 0.519657  [   96/  146]
train() client id: f_00005-4-3 loss: 0.219082  [  128/  146]
train() client id: f_00005-5-0 loss: 0.463323  [   32/  146]
train() client id: f_00005-5-1 loss: 0.433781  [   64/  146]
train() client id: f_00005-5-2 loss: 0.567906  [   96/  146]
train() client id: f_00005-5-3 loss: 0.639554  [  128/  146]
train() client id: f_00005-6-0 loss: 0.325448  [   32/  146]
train() client id: f_00005-6-1 loss: 0.407827  [   64/  146]
train() client id: f_00005-6-2 loss: 0.639720  [   96/  146]
train() client id: f_00005-6-3 loss: 0.899991  [  128/  146]
train() client id: f_00005-7-0 loss: 0.428556  [   32/  146]
train() client id: f_00005-7-1 loss: 0.643883  [   64/  146]
train() client id: f_00005-7-2 loss: 0.306549  [   96/  146]
train() client id: f_00005-7-3 loss: 0.595868  [  128/  146]
train() client id: f_00005-8-0 loss: 0.428897  [   32/  146]
train() client id: f_00005-8-1 loss: 0.385617  [   64/  146]
train() client id: f_00005-8-2 loss: 0.838782  [   96/  146]
train() client id: f_00005-8-3 loss: 0.392951  [  128/  146]
train() client id: f_00005-9-0 loss: 0.478754  [   32/  146]
train() client id: f_00005-9-1 loss: 0.563059  [   64/  146]
train() client id: f_00005-9-2 loss: 0.514236  [   96/  146]
train() client id: f_00005-9-3 loss: 0.489224  [  128/  146]
train() client id: f_00005-10-0 loss: 0.623433  [   32/  146]
train() client id: f_00005-10-1 loss: 0.527318  [   64/  146]
train() client id: f_00005-10-2 loss: 0.154828  [   96/  146]
train() client id: f_00005-10-3 loss: 0.558091  [  128/  146]
train() client id: f_00005-11-0 loss: 0.461617  [   32/  146]
train() client id: f_00005-11-1 loss: 0.426957  [   64/  146]
train() client id: f_00005-11-2 loss: 0.585315  [   96/  146]
train() client id: f_00005-11-3 loss: 0.533302  [  128/  146]
train() client id: f_00006-0-0 loss: 0.537591  [   32/   54]
train() client id: f_00006-1-0 loss: 0.606901  [   32/   54]
train() client id: f_00006-2-0 loss: 0.499164  [   32/   54]
train() client id: f_00006-3-0 loss: 0.603198  [   32/   54]
train() client id: f_00006-4-0 loss: 0.602994  [   32/   54]
train() client id: f_00006-5-0 loss: 0.595269  [   32/   54]
train() client id: f_00006-6-0 loss: 0.548131  [   32/   54]
train() client id: f_00006-7-0 loss: 0.497675  [   32/   54]
train() client id: f_00006-8-0 loss: 0.557835  [   32/   54]
train() client id: f_00006-9-0 loss: 0.540652  [   32/   54]
train() client id: f_00006-10-0 loss: 0.546578  [   32/   54]
train() client id: f_00006-11-0 loss: 0.555226  [   32/   54]
train() client id: f_00007-0-0 loss: 0.276682  [   32/  179]
train() client id: f_00007-0-1 loss: 0.604195  [   64/  179]
train() client id: f_00007-0-2 loss: 0.230219  [   96/  179]
train() client id: f_00007-0-3 loss: 0.298362  [  128/  179]
train() client id: f_00007-0-4 loss: 0.170883  [  160/  179]
train() client id: f_00007-1-0 loss: 0.239160  [   32/  179]
train() client id: f_00007-1-1 loss: 0.443886  [   64/  179]
train() client id: f_00007-1-2 loss: 0.280061  [   96/  179]
train() client id: f_00007-1-3 loss: 0.342857  [  128/  179]
train() client id: f_00007-1-4 loss: 0.172622  [  160/  179]
train() client id: f_00007-2-0 loss: 0.447936  [   32/  179]
train() client id: f_00007-2-1 loss: 0.254414  [   64/  179]
train() client id: f_00007-2-2 loss: 0.331850  [   96/  179]
train() client id: f_00007-2-3 loss: 0.221624  [  128/  179]
train() client id: f_00007-2-4 loss: 0.213197  [  160/  179]
train() client id: f_00007-3-0 loss: 0.329336  [   32/  179]
train() client id: f_00007-3-1 loss: 0.444407  [   64/  179]
train() client id: f_00007-3-2 loss: 0.242209  [   96/  179]
train() client id: f_00007-3-3 loss: 0.155513  [  128/  179]
train() client id: f_00007-3-4 loss: 0.214789  [  160/  179]
train() client id: f_00007-4-0 loss: 0.292556  [   32/  179]
train() client id: f_00007-4-1 loss: 0.368186  [   64/  179]
train() client id: f_00007-4-2 loss: 0.127890  [   96/  179]
train() client id: f_00007-4-3 loss: 0.433887  [  128/  179]
train() client id: f_00007-4-4 loss: 0.108352  [  160/  179]
train() client id: f_00007-5-0 loss: 0.315955  [   32/  179]
train() client id: f_00007-5-1 loss: 0.131376  [   64/  179]
train() client id: f_00007-5-2 loss: 0.112855  [   96/  179]
train() client id: f_00007-5-3 loss: 0.220927  [  128/  179]
train() client id: f_00007-5-4 loss: 0.556818  [  160/  179]
train() client id: f_00007-6-0 loss: 0.357789  [   32/  179]
train() client id: f_00007-6-1 loss: 0.251525  [   64/  179]
train() client id: f_00007-6-2 loss: 0.395450  [   96/  179]
train() client id: f_00007-6-3 loss: 0.082935  [  128/  179]
train() client id: f_00007-6-4 loss: 0.191489  [  160/  179]
train() client id: f_00007-7-0 loss: 0.195498  [   32/  179]
train() client id: f_00007-7-1 loss: 0.232965  [   64/  179]
train() client id: f_00007-7-2 loss: 0.415480  [   96/  179]
train() client id: f_00007-7-3 loss: 0.192148  [  128/  179]
train() client id: f_00007-7-4 loss: 0.321854  [  160/  179]
train() client id: f_00007-8-0 loss: 0.084931  [   32/  179]
train() client id: f_00007-8-1 loss: 0.256131  [   64/  179]
train() client id: f_00007-8-2 loss: 0.188136  [   96/  179]
train() client id: f_00007-8-3 loss: 0.184166  [  128/  179]
train() client id: f_00007-8-4 loss: 0.442107  [  160/  179]
train() client id: f_00007-9-0 loss: 0.170247  [   32/  179]
train() client id: f_00007-9-1 loss: 0.090686  [   64/  179]
train() client id: f_00007-9-2 loss: 0.375246  [   96/  179]
train() client id: f_00007-9-3 loss: 0.200878  [  128/  179]
train() client id: f_00007-9-4 loss: 0.253964  [  160/  179]
train() client id: f_00007-10-0 loss: 0.237440  [   32/  179]
train() client id: f_00007-10-1 loss: 0.340747  [   64/  179]
train() client id: f_00007-10-2 loss: 0.160731  [   96/  179]
train() client id: f_00007-10-3 loss: 0.163736  [  128/  179]
train() client id: f_00007-10-4 loss: 0.303910  [  160/  179]
train() client id: f_00007-11-0 loss: 0.156198  [   32/  179]
train() client id: f_00007-11-1 loss: 0.270737  [   64/  179]
train() client id: f_00007-11-2 loss: 0.171462  [   96/  179]
train() client id: f_00007-11-3 loss: 0.292523  [  128/  179]
train() client id: f_00007-11-4 loss: 0.112919  [  160/  179]
train() client id: f_00008-0-0 loss: 0.495202  [   32/  130]
train() client id: f_00008-0-1 loss: 0.713872  [   64/  130]
train() client id: f_00008-0-2 loss: 0.754472  [   96/  130]
train() client id: f_00008-0-3 loss: 0.678856  [  128/  130]
train() client id: f_00008-1-0 loss: 0.598640  [   32/  130]
train() client id: f_00008-1-1 loss: 0.649597  [   64/  130]
train() client id: f_00008-1-2 loss: 0.757285  [   96/  130]
train() client id: f_00008-1-3 loss: 0.645488  [  128/  130]
train() client id: f_00008-2-0 loss: 0.556497  [   32/  130]
train() client id: f_00008-2-1 loss: 0.609586  [   64/  130]
train() client id: f_00008-2-2 loss: 0.767133  [   96/  130]
train() client id: f_00008-2-3 loss: 0.712098  [  128/  130]
train() client id: f_00008-3-0 loss: 0.675997  [   32/  130]
train() client id: f_00008-3-1 loss: 0.667188  [   64/  130]
train() client id: f_00008-3-2 loss: 0.632943  [   96/  130]
train() client id: f_00008-3-3 loss: 0.689344  [  128/  130]
train() client id: f_00008-4-0 loss: 0.728097  [   32/  130]
train() client id: f_00008-4-1 loss: 0.555074  [   64/  130]
train() client id: f_00008-4-2 loss: 0.640553  [   96/  130]
train() client id: f_00008-4-3 loss: 0.727369  [  128/  130]
train() client id: f_00008-5-0 loss: 0.669356  [   32/  130]
train() client id: f_00008-5-1 loss: 0.705954  [   64/  130]
train() client id: f_00008-5-2 loss: 0.652017  [   96/  130]
train() client id: f_00008-5-3 loss: 0.601088  [  128/  130]
train() client id: f_00008-6-0 loss: 0.689703  [   32/  130]
train() client id: f_00008-6-1 loss: 0.672039  [   64/  130]
train() client id: f_00008-6-2 loss: 0.570001  [   96/  130]
train() client id: f_00008-6-3 loss: 0.727417  [  128/  130]
train() client id: f_00008-7-0 loss: 0.763131  [   32/  130]
train() client id: f_00008-7-1 loss: 0.546266  [   64/  130]
train() client id: f_00008-7-2 loss: 0.706111  [   96/  130]
train() client id: f_00008-7-3 loss: 0.607622  [  128/  130]
train() client id: f_00008-8-0 loss: 0.600605  [   32/  130]
train() client id: f_00008-8-1 loss: 0.652204  [   64/  130]
train() client id: f_00008-8-2 loss: 0.629775  [   96/  130]
train() client id: f_00008-8-3 loss: 0.764028  [  128/  130]
train() client id: f_00008-9-0 loss: 0.644414  [   32/  130]
train() client id: f_00008-9-1 loss: 0.619406  [   64/  130]
train() client id: f_00008-9-2 loss: 0.734211  [   96/  130]
train() client id: f_00008-9-3 loss: 0.663372  [  128/  130]
train() client id: f_00008-10-0 loss: 0.716198  [   32/  130]
train() client id: f_00008-10-1 loss: 0.574207  [   64/  130]
train() client id: f_00008-10-2 loss: 0.595380  [   96/  130]
train() client id: f_00008-10-3 loss: 0.747097  [  128/  130]
train() client id: f_00008-11-0 loss: 0.677274  [   32/  130]
train() client id: f_00008-11-1 loss: 0.585747  [   64/  130]
train() client id: f_00008-11-2 loss: 0.683033  [   96/  130]
train() client id: f_00008-11-3 loss: 0.716174  [  128/  130]
train() client id: f_00009-0-0 loss: 0.966175  [   32/  118]
train() client id: f_00009-0-1 loss: 1.000986  [   64/  118]
train() client id: f_00009-0-2 loss: 1.374581  [   96/  118]
train() client id: f_00009-1-0 loss: 0.980161  [   32/  118]
train() client id: f_00009-1-1 loss: 1.142359  [   64/  118]
train() client id: f_00009-1-2 loss: 1.042841  [   96/  118]
train() client id: f_00009-2-0 loss: 1.028723  [   32/  118]
train() client id: f_00009-2-1 loss: 0.967084  [   64/  118]
train() client id: f_00009-2-2 loss: 0.953660  [   96/  118]
train() client id: f_00009-3-0 loss: 0.921831  [   32/  118]
train() client id: f_00009-3-1 loss: 0.948185  [   64/  118]
train() client id: f_00009-3-2 loss: 0.876739  [   96/  118]
train() client id: f_00009-4-0 loss: 0.885229  [   32/  118]
train() client id: f_00009-4-1 loss: 0.960324  [   64/  118]
train() client id: f_00009-4-2 loss: 0.863407  [   96/  118]
train() client id: f_00009-5-0 loss: 0.981982  [   32/  118]
train() client id: f_00009-5-1 loss: 0.726942  [   64/  118]
train() client id: f_00009-5-2 loss: 0.886787  [   96/  118]
train() client id: f_00009-6-0 loss: 0.749962  [   32/  118]
train() client id: f_00009-6-1 loss: 1.015106  [   64/  118]
train() client id: f_00009-6-2 loss: 0.854837  [   96/  118]
train() client id: f_00009-7-0 loss: 0.780124  [   32/  118]
train() client id: f_00009-7-1 loss: 0.849891  [   64/  118]
train() client id: f_00009-7-2 loss: 0.845346  [   96/  118]
train() client id: f_00009-8-0 loss: 0.678514  [   32/  118]
train() client id: f_00009-8-1 loss: 0.917611  [   64/  118]
train() client id: f_00009-8-2 loss: 0.842288  [   96/  118]
train() client id: f_00009-9-0 loss: 0.752472  [   32/  118]
train() client id: f_00009-9-1 loss: 1.019593  [   64/  118]
train() client id: f_00009-9-2 loss: 0.711703  [   96/  118]
train() client id: f_00009-10-0 loss: 0.723662  [   32/  118]
train() client id: f_00009-10-1 loss: 0.696175  [   64/  118]
train() client id: f_00009-10-2 loss: 0.895097  [   96/  118]
train() client id: f_00009-11-0 loss: 0.885530  [   32/  118]
train() client id: f_00009-11-1 loss: 0.697277  [   64/  118]
train() client id: f_00009-11-2 loss: 0.692598  [   96/  118]
At round 32 accuracy: 0.6525198938992043
At round 32 training accuracy: 0.5814889336016097
At round 32 training loss: 0.8306494340253922
update_location
xs = -4.528292 61.001589 70.045120 -70.943528 -30.103519 -0.217951 -112.215960 133.375741 -1.680116 54.695607 
ys = 147.587959 15.555839 1.320614 22.544824 9.350187 -2.185849 -3.124922 -19.177652 -77.154970 4.001482 
xs mean: 9.942869159412867
ys mean: 9.871751218646876
dists_uav = 178.333147 118.165892 122.098578 124.664563 104.850598 100.024124 150.340237 167.800091 126.315922 114.050959 
uav_gains = -106.321794 -101.812476 -102.168038 -102.393941 -100.514304 -100.002635 -104.431570 -105.639132 -102.536892 -101.427578 
uav_gains_db_mean: -102.7248360354601
dists_bs = 172.661586 284.814309 300.352526 184.581499 220.079192 248.884692 188.865887 364.418109 305.976978 286.357408 
bs_gains = -102.208539 -108.294790 -108.940735 -103.020328 -105.159271 -106.655008 -103.299358 -111.291858 -109.166344 -108.360495 
bs_gains_db_mean: -106.63967259980555
Round 33
-------------------------------
ene_coms = [0.00852933 0.00957177 0.00692483 0.00699671 0.00801184 0.00868977
 0.00771787 0.00821789 0.01011343 0.00961068]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.62419764 13.78919621  6.49400452  2.33029987 15.87077535  7.65531084
  2.90820269  9.34408901  6.85688066  6.23128247]
obj_prev = 78.104239260727
eta_min = 1.3630240393990404e-14	eta_max = 0.9389901842325441
af = 16.48123926988226	bf = 1.42428515432181	zeta = 18.129363196870486	eta = 0.9090909090909091
af = 16.48123926988226	bf = 1.42428515432181	zeta = 32.94626535785627	eta = 0.5002460549281101
af = 16.48123926988226	bf = 1.42428515432181	zeta = 25.692845904865905	eta = 0.6414719229978691
af = 16.48123926988226	bf = 1.42428515432181	zeta = 24.38243722258025	eta = 0.6759471630924246
af = 16.48123926988226	bf = 1.42428515432181	zeta = 24.313947772922024	eta = 0.6778512244826445
af = 16.48123926988226	bf = 1.42428515432181	zeta = 24.3137455591456	eta = 0.6778568620696472
eta = 0.6778568620696472
ene_coms = [0.00852933 0.00957177 0.00692483 0.00699671 0.00801184 0.00868977
 0.00771787 0.00821789 0.01011343 0.00961068]
ene_comp = [0.03208442 0.06747914 0.03157515 0.01094945 0.07791931 0.03717719
 0.01375047 0.04558025 0.03310298 0.03004732]
ene_total = [2.12794519 4.03705888 2.01719425 0.94028346 4.50233613 2.40318531
 1.12482672 2.81873678 2.26431039 2.07786844]
ti_comp = [0.45578938 0.44536499 0.47183444 0.47111559 0.46096432 0.45418497
 0.46390405 0.45890385 0.43994838 0.44497592]
ti_coms = [0.08529334 0.09571773 0.06924829 0.06996713 0.0801184  0.08689775
 0.07717867 0.08217887 0.10113434 0.0961068 ]
t_total = [28.34986153 28.34986153 28.34986153 28.34986153 28.34986153 28.34986153
 28.34986153 28.34986153 28.34986153 28.34986153]
ene_coms = [0.00852933 0.00957177 0.00692483 0.00699671 0.00801184 0.00868977
 0.00771787 0.00821789 0.01011343 0.00961068]
ene_comp = [9.93651974e-06 9.68180979e-05 8.83765129e-06 3.69659178e-07
 1.39149207e-04 1.55684048e-05 7.55050548e-07 2.81039107e-05
 1.17132571e-05 8.56296309e-06]
ene_total = [0.44741243 0.50658284 0.36328729 0.36661    0.42706858 0.45611375
 0.40441482 0.43204613 0.53050394 0.50399725]
optimize_network iter = 0 obj = 4.438037032346958
eta = 0.6778568620696472
freqs = [35196544.71585989 75757125.79810438 33459984.95647496 11620763.75367332
 84517726.8258018  40927364.77606785 14820378.21033025 49662092.68581295
 37621431.43505009 33762856.70392315]
eta_min = 0.6778568620696548	eta_max = 0.6778568620696529
af = 0.013883117100203703	bf = 1.42428515432181	zeta = 0.015271428810224075	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00852933 0.00957177 0.00692483 0.00699671 0.00801184 0.00868977
 0.00771787 0.00821789 0.01011343 0.00961068]
ene_comp = [2.43571215e-06 2.37327579e-05 2.16634951e-06 9.06135525e-08
 3.41092680e-05 3.81624086e-06 1.85083494e-07 6.88903546e-06
 2.87123898e-06 2.09901593e-06]
ene_total = [1.58404664 1.78154469 1.28609697 1.29905805 1.49384713 1.6140911
 1.43296825 1.5270488  1.8782386  1.78475169]
ti_comp = [0.45578938 0.44536499 0.47183444 0.47111559 0.46096432 0.45418497
 0.46390405 0.45890385 0.43994838 0.44497592]
ti_coms = [0.08529334 0.09571773 0.06924829 0.06996713 0.0801184  0.08689775
 0.07717867 0.08217887 0.10113434 0.0961068 ]
t_total = [28.34986153 28.34986153 28.34986153 28.34986153 28.34986153 28.34986153
 28.34986153 28.34986153 28.34986153 28.34986153]
ene_coms = [0.00852933 0.00957177 0.00692483 0.00699671 0.00801184 0.00868977
 0.00771787 0.00821789 0.01011343 0.00961068]
ene_comp = [9.93651974e-06 9.68180979e-05 8.83765129e-06 3.69659178e-07
 1.39149207e-04 1.55684048e-05 7.55050548e-07 2.81039107e-05
 1.17132571e-05 8.56296309e-06]
ene_total = [0.44741243 0.50658284 0.36328729 0.36661    0.42706858 0.45611375
 0.40441482 0.43204613 0.53050394 0.50399725]
optimize_network iter = 1 obj = 4.438037032347036
eta = 0.6778568620696529
freqs = [35196544.71585986 75757125.79810435 33459984.95647491 11620763.7536733
 84517726.82580173 40927364.77606782 14820378.21033023 49662092.6858129
 37621431.43505009 33762856.70392315]
Done!
ene_coms = [0.00852933 0.00957177 0.00692483 0.00699671 0.00801184 0.00868977
 0.00771787 0.00821789 0.01011343 0.00961068]
ene_comp = [9.36530351e-06 9.12523595e-05 8.32960521e-06 3.48408748e-07
 1.31149999e-04 1.46734309e-05 7.11645297e-07 2.64883139e-05
 1.10399024e-05 8.07070789e-06]
ene_total = [0.0085387  0.00966303 0.00693316 0.00699706 0.00814299 0.00870445
 0.00771858 0.00824438 0.01012447 0.00961875]
At round 33 energy consumption: 0.08468556142169519
At round 33 eta: 0.6778568620696529
At round 33 a_n: 16.878589906266903
At round 33 local rounds: 12.7319137942831
At round 33 global rounds: 52.39469018246276
gradient difference: 0.46589598059654236
train() client id: f_00000-0-0 loss: 1.194547  [   32/  126]
train() client id: f_00000-0-1 loss: 1.186537  [   64/  126]
train() client id: f_00000-0-2 loss: 1.159406  [   96/  126]
train() client id: f_00000-1-0 loss: 1.282738  [   32/  126]
train() client id: f_00000-1-1 loss: 1.051208  [   64/  126]
train() client id: f_00000-1-2 loss: 0.861968  [   96/  126]
train() client id: f_00000-2-0 loss: 1.062627  [   32/  126]
train() client id: f_00000-2-1 loss: 0.989523  [   64/  126]
train() client id: f_00000-2-2 loss: 0.884236  [   96/  126]
train() client id: f_00000-3-0 loss: 0.882862  [   32/  126]
train() client id: f_00000-3-1 loss: 0.983050  [   64/  126]
train() client id: f_00000-3-2 loss: 0.834308  [   96/  126]
train() client id: f_00000-4-0 loss: 1.005484  [   32/  126]
train() client id: f_00000-4-1 loss: 0.831300  [   64/  126]
train() client id: f_00000-4-2 loss: 0.931688  [   96/  126]
train() client id: f_00000-5-0 loss: 0.784993  [   32/  126]
train() client id: f_00000-5-1 loss: 0.892676  [   64/  126]
train() client id: f_00000-5-2 loss: 0.787695  [   96/  126]
train() client id: f_00000-6-0 loss: 0.807586  [   32/  126]
train() client id: f_00000-6-1 loss: 0.888091  [   64/  126]
train() client id: f_00000-6-2 loss: 0.778656  [   96/  126]
train() client id: f_00000-7-0 loss: 0.817533  [   32/  126]
train() client id: f_00000-7-1 loss: 0.783935  [   64/  126]
train() client id: f_00000-7-2 loss: 0.911314  [   96/  126]
train() client id: f_00000-8-0 loss: 0.773025  [   32/  126]
train() client id: f_00000-8-1 loss: 0.761739  [   64/  126]
train() client id: f_00000-8-2 loss: 0.742825  [   96/  126]
train() client id: f_00000-9-0 loss: 0.772489  [   32/  126]
train() client id: f_00000-9-1 loss: 0.740289  [   64/  126]
train() client id: f_00000-9-2 loss: 0.780353  [   96/  126]
train() client id: f_00000-10-0 loss: 0.758558  [   32/  126]
train() client id: f_00000-10-1 loss: 0.881731  [   64/  126]
train() client id: f_00000-10-2 loss: 0.755543  [   96/  126]
train() client id: f_00000-11-0 loss: 0.741424  [   32/  126]
train() client id: f_00000-11-1 loss: 0.669751  [   64/  126]
train() client id: f_00000-11-2 loss: 0.807212  [   96/  126]
train() client id: f_00001-0-0 loss: 0.625563  [   32/  265]
train() client id: f_00001-0-1 loss: 0.483946  [   64/  265]
train() client id: f_00001-0-2 loss: 0.536379  [   96/  265]
train() client id: f_00001-0-3 loss: 0.495858  [  128/  265]
train() client id: f_00001-0-4 loss: 0.570794  [  160/  265]
train() client id: f_00001-0-5 loss: 0.533546  [  192/  265]
train() client id: f_00001-0-6 loss: 0.509098  [  224/  265]
train() client id: f_00001-0-7 loss: 0.517180  [  256/  265]
train() client id: f_00001-1-0 loss: 0.590522  [   32/  265]
train() client id: f_00001-1-1 loss: 0.586823  [   64/  265]
train() client id: f_00001-1-2 loss: 0.444213  [   96/  265]
train() client id: f_00001-1-3 loss: 0.572669  [  128/  265]
train() client id: f_00001-1-4 loss: 0.638896  [  160/  265]
train() client id: f_00001-1-5 loss: 0.488244  [  192/  265]
train() client id: f_00001-1-6 loss: 0.433420  [  224/  265]
train() client id: f_00001-1-7 loss: 0.456838  [  256/  265]
train() client id: f_00001-2-0 loss: 0.507663  [   32/  265]
train() client id: f_00001-2-1 loss: 0.496349  [   64/  265]
train() client id: f_00001-2-2 loss: 0.501733  [   96/  265]
train() client id: f_00001-2-3 loss: 0.582460  [  128/  265]
train() client id: f_00001-2-4 loss: 0.526177  [  160/  265]
train() client id: f_00001-2-5 loss: 0.507391  [  192/  265]
train() client id: f_00001-2-6 loss: 0.502126  [  224/  265]
train() client id: f_00001-2-7 loss: 0.554741  [  256/  265]
train() client id: f_00001-3-0 loss: 0.441518  [   32/  265]
train() client id: f_00001-3-1 loss: 0.413374  [   64/  265]
train() client id: f_00001-3-2 loss: 0.593265  [   96/  265]
train() client id: f_00001-3-3 loss: 0.546107  [  128/  265]
train() client id: f_00001-3-4 loss: 0.596649  [  160/  265]
train() client id: f_00001-3-5 loss: 0.511338  [  192/  265]
train() client id: f_00001-3-6 loss: 0.439814  [  224/  265]
train() client id: f_00001-3-7 loss: 0.622066  [  256/  265]
train() client id: f_00001-4-0 loss: 0.455225  [   32/  265]
train() client id: f_00001-4-1 loss: 0.569071  [   64/  265]
train() client id: f_00001-4-2 loss: 0.578567  [   96/  265]
train() client id: f_00001-4-3 loss: 0.600482  [  128/  265]
train() client id: f_00001-4-4 loss: 0.526805  [  160/  265]
train() client id: f_00001-4-5 loss: 0.438345  [  192/  265]
train() client id: f_00001-4-6 loss: 0.537802  [  224/  265]
train() client id: f_00001-4-7 loss: 0.425731  [  256/  265]
train() client id: f_00001-5-0 loss: 0.616452  [   32/  265]
train() client id: f_00001-5-1 loss: 0.539557  [   64/  265]
train() client id: f_00001-5-2 loss: 0.427577  [   96/  265]
train() client id: f_00001-5-3 loss: 0.432958  [  128/  265]
train() client id: f_00001-5-4 loss: 0.526143  [  160/  265]
train() client id: f_00001-5-5 loss: 0.515045  [  192/  265]
train() client id: f_00001-5-6 loss: 0.519316  [  224/  265]
train() client id: f_00001-5-7 loss: 0.461686  [  256/  265]
train() client id: f_00001-6-0 loss: 0.442809  [   32/  265]
train() client id: f_00001-6-1 loss: 0.579577  [   64/  265]
train() client id: f_00001-6-2 loss: 0.628011  [   96/  265]
train() client id: f_00001-6-3 loss: 0.413386  [  128/  265]
train() client id: f_00001-6-4 loss: 0.512916  [  160/  265]
train() client id: f_00001-6-5 loss: 0.489365  [  192/  265]
train() client id: f_00001-6-6 loss: 0.492265  [  224/  265]
train() client id: f_00001-6-7 loss: 0.498307  [  256/  265]
train() client id: f_00001-7-0 loss: 0.470835  [   32/  265]
train() client id: f_00001-7-1 loss: 0.524155  [   64/  265]
train() client id: f_00001-7-2 loss: 0.612511  [   96/  265]
train() client id: f_00001-7-3 loss: 0.491962  [  128/  265]
train() client id: f_00001-7-4 loss: 0.506577  [  160/  265]
train() client id: f_00001-7-5 loss: 0.561768  [  192/  265]
train() client id: f_00001-7-6 loss: 0.420365  [  224/  265]
train() client id: f_00001-7-7 loss: 0.516872  [  256/  265]
train() client id: f_00001-8-0 loss: 0.428293  [   32/  265]
train() client id: f_00001-8-1 loss: 0.506237  [   64/  265]
train() client id: f_00001-8-2 loss: 0.546287  [   96/  265]
train() client id: f_00001-8-3 loss: 0.450685  [  128/  265]
train() client id: f_00001-8-4 loss: 0.761323  [  160/  265]
train() client id: f_00001-8-5 loss: 0.575021  [  192/  265]
train() client id: f_00001-8-6 loss: 0.415179  [  224/  265]
train() client id: f_00001-8-7 loss: 0.434066  [  256/  265]
train() client id: f_00001-9-0 loss: 0.540134  [   32/  265]
train() client id: f_00001-9-1 loss: 0.557097  [   64/  265]
train() client id: f_00001-9-2 loss: 0.437340  [   96/  265]
train() client id: f_00001-9-3 loss: 0.591733  [  128/  265]
train() client id: f_00001-9-4 loss: 0.469511  [  160/  265]
train() client id: f_00001-9-5 loss: 0.469228  [  192/  265]
train() client id: f_00001-9-6 loss: 0.587460  [  224/  265]
train() client id: f_00001-9-7 loss: 0.468484  [  256/  265]
train() client id: f_00001-10-0 loss: 0.536728  [   32/  265]
train() client id: f_00001-10-1 loss: 0.449781  [   64/  265]
train() client id: f_00001-10-2 loss: 0.679852  [   96/  265]
train() client id: f_00001-10-3 loss: 0.419239  [  128/  265]
train() client id: f_00001-10-4 loss: 0.631326  [  160/  265]
train() client id: f_00001-10-5 loss: 0.422180  [  192/  265]
train() client id: f_00001-10-6 loss: 0.412128  [  224/  265]
train() client id: f_00001-10-7 loss: 0.426918  [  256/  265]
train() client id: f_00001-11-0 loss: 0.448309  [   32/  265]
train() client id: f_00001-11-1 loss: 0.442776  [   64/  265]
train() client id: f_00001-11-2 loss: 0.575441  [   96/  265]
train() client id: f_00001-11-3 loss: 0.472962  [  128/  265]
train() client id: f_00001-11-4 loss: 0.493424  [  160/  265]
train() client id: f_00001-11-5 loss: 0.596659  [  192/  265]
train() client id: f_00001-11-6 loss: 0.468619  [  224/  265]
train() client id: f_00001-11-7 loss: 0.633347  [  256/  265]
train() client id: f_00002-0-0 loss: 1.174834  [   32/  124]
train() client id: f_00002-0-1 loss: 1.103409  [   64/  124]
train() client id: f_00002-0-2 loss: 1.404068  [   96/  124]
train() client id: f_00002-1-0 loss: 1.145049  [   32/  124]
train() client id: f_00002-1-1 loss: 1.109545  [   64/  124]
train() client id: f_00002-1-2 loss: 1.512847  [   96/  124]
train() client id: f_00002-2-0 loss: 1.149058  [   32/  124]
train() client id: f_00002-2-1 loss: 1.220538  [   64/  124]
train() client id: f_00002-2-2 loss: 1.354356  [   96/  124]
train() client id: f_00002-3-0 loss: 1.353034  [   32/  124]
train() client id: f_00002-3-1 loss: 1.228591  [   64/  124]
train() client id: f_00002-3-2 loss: 1.103819  [   96/  124]
train() client id: f_00002-4-0 loss: 1.178366  [   32/  124]
train() client id: f_00002-4-1 loss: 1.293934  [   64/  124]
train() client id: f_00002-4-2 loss: 1.245106  [   96/  124]
train() client id: f_00002-5-0 loss: 1.078926  [   32/  124]
train() client id: f_00002-5-1 loss: 1.252187  [   64/  124]
train() client id: f_00002-5-2 loss: 1.281207  [   96/  124]
train() client id: f_00002-6-0 loss: 1.336809  [   32/  124]
train() client id: f_00002-6-1 loss: 1.044303  [   64/  124]
train() client id: f_00002-6-2 loss: 1.040281  [   96/  124]
train() client id: f_00002-7-0 loss: 1.141297  [   32/  124]
train() client id: f_00002-7-1 loss: 0.972391  [   64/  124]
train() client id: f_00002-7-2 loss: 1.256781  [   96/  124]
train() client id: f_00002-8-0 loss: 1.137471  [   32/  124]
train() client id: f_00002-8-1 loss: 1.131394  [   64/  124]
train() client id: f_00002-8-2 loss: 0.993031  [   96/  124]
train() client id: f_00002-9-0 loss: 1.060603  [   32/  124]
train() client id: f_00002-9-1 loss: 1.262779  [   64/  124]
train() client id: f_00002-9-2 loss: 1.138326  [   96/  124]
train() client id: f_00002-10-0 loss: 1.123461  [   32/  124]
train() client id: f_00002-10-1 loss: 1.273462  [   64/  124]
train() client id: f_00002-10-2 loss: 1.077981  [   96/  124]
train() client id: f_00002-11-0 loss: 1.132573  [   32/  124]
train() client id: f_00002-11-1 loss: 1.148331  [   64/  124]
train() client id: f_00002-11-2 loss: 1.084331  [   96/  124]
train() client id: f_00003-0-0 loss: 0.898617  [   32/   43]
train() client id: f_00003-1-0 loss: 0.795214  [   32/   43]
train() client id: f_00003-2-0 loss: 0.705248  [   32/   43]
train() client id: f_00003-3-0 loss: 0.871040  [   32/   43]
train() client id: f_00003-4-0 loss: 0.910979  [   32/   43]
train() client id: f_00003-5-0 loss: 0.798058  [   32/   43]
train() client id: f_00003-6-0 loss: 0.683954  [   32/   43]
train() client id: f_00003-7-0 loss: 0.961141  [   32/   43]
train() client id: f_00003-8-0 loss: 0.747106  [   32/   43]
train() client id: f_00003-9-0 loss: 0.729430  [   32/   43]
train() client id: f_00003-10-0 loss: 0.830054  [   32/   43]
train() client id: f_00003-11-0 loss: 0.850149  [   32/   43]
train() client id: f_00004-0-0 loss: 0.920109  [   32/  306]
train() client id: f_00004-0-1 loss: 0.993907  [   64/  306]
train() client id: f_00004-0-2 loss: 0.936123  [   96/  306]
train() client id: f_00004-0-3 loss: 0.821608  [  128/  306]
train() client id: f_00004-0-4 loss: 1.096860  [  160/  306]
train() client id: f_00004-0-5 loss: 0.917407  [  192/  306]
train() client id: f_00004-0-6 loss: 0.947991  [  224/  306]
train() client id: f_00004-0-7 loss: 0.877290  [  256/  306]
train() client id: f_00004-0-8 loss: 1.004837  [  288/  306]
train() client id: f_00004-1-0 loss: 0.958488  [   32/  306]
train() client id: f_00004-1-1 loss: 0.924968  [   64/  306]
train() client id: f_00004-1-2 loss: 0.840428  [   96/  306]
train() client id: f_00004-1-3 loss: 1.063039  [  128/  306]
train() client id: f_00004-1-4 loss: 0.872147  [  160/  306]
train() client id: f_00004-1-5 loss: 0.867877  [  192/  306]
train() client id: f_00004-1-6 loss: 1.003001  [  224/  306]
train() client id: f_00004-1-7 loss: 0.944494  [  256/  306]
train() client id: f_00004-1-8 loss: 0.992507  [  288/  306]
train() client id: f_00004-2-0 loss: 0.968606  [   32/  306]
train() client id: f_00004-2-1 loss: 1.129827  [   64/  306]
train() client id: f_00004-2-2 loss: 0.948460  [   96/  306]
train() client id: f_00004-2-3 loss: 0.871530  [  128/  306]
train() client id: f_00004-2-4 loss: 0.913929  [  160/  306]
train() client id: f_00004-2-5 loss: 0.926318  [  192/  306]
train() client id: f_00004-2-6 loss: 1.046291  [  224/  306]
train() client id: f_00004-2-7 loss: 0.896846  [  256/  306]
train() client id: f_00004-2-8 loss: 0.791217  [  288/  306]
train() client id: f_00004-3-0 loss: 0.975241  [   32/  306]
train() client id: f_00004-3-1 loss: 0.881358  [   64/  306]
train() client id: f_00004-3-2 loss: 1.020922  [   96/  306]
train() client id: f_00004-3-3 loss: 0.915404  [  128/  306]
train() client id: f_00004-3-4 loss: 0.960760  [  160/  306]
train() client id: f_00004-3-5 loss: 0.943784  [  192/  306]
train() client id: f_00004-3-6 loss: 0.936828  [  224/  306]
train() client id: f_00004-3-7 loss: 0.831207  [  256/  306]
train() client id: f_00004-3-8 loss: 1.037021  [  288/  306]
train() client id: f_00004-4-0 loss: 0.920308  [   32/  306]
train() client id: f_00004-4-1 loss: 0.989166  [   64/  306]
train() client id: f_00004-4-2 loss: 0.879706  [   96/  306]
train() client id: f_00004-4-3 loss: 1.135841  [  128/  306]
train() client id: f_00004-4-4 loss: 0.902035  [  160/  306]
train() client id: f_00004-4-5 loss: 0.850033  [  192/  306]
train() client id: f_00004-4-6 loss: 1.007992  [  224/  306]
train() client id: f_00004-4-7 loss: 0.885926  [  256/  306]
train() client id: f_00004-4-8 loss: 0.891324  [  288/  306]
train() client id: f_00004-5-0 loss: 0.856424  [   32/  306]
train() client id: f_00004-5-1 loss: 0.931730  [   64/  306]
train() client id: f_00004-5-2 loss: 0.988219  [   96/  306]
train() client id: f_00004-5-3 loss: 0.967024  [  128/  306]
train() client id: f_00004-5-4 loss: 0.985980  [  160/  306]
train() client id: f_00004-5-5 loss: 0.873194  [  192/  306]
train() client id: f_00004-5-6 loss: 0.791871  [  224/  306]
train() client id: f_00004-5-7 loss: 0.981352  [  256/  306]
train() client id: f_00004-5-8 loss: 1.061500  [  288/  306]
train() client id: f_00004-6-0 loss: 0.891916  [   32/  306]
train() client id: f_00004-6-1 loss: 0.922487  [   64/  306]
train() client id: f_00004-6-2 loss: 0.997251  [   96/  306]
train() client id: f_00004-6-3 loss: 0.888234  [  128/  306]
train() client id: f_00004-6-4 loss: 0.838451  [  160/  306]
train() client id: f_00004-6-5 loss: 0.935561  [  192/  306]
train() client id: f_00004-6-6 loss: 0.859850  [  224/  306]
train() client id: f_00004-6-7 loss: 0.973518  [  256/  306]
train() client id: f_00004-6-8 loss: 1.060827  [  288/  306]
train() client id: f_00004-7-0 loss: 0.951613  [   32/  306]
train() client id: f_00004-7-1 loss: 0.869342  [   64/  306]
train() client id: f_00004-7-2 loss: 0.792273  [   96/  306]
train() client id: f_00004-7-3 loss: 0.974005  [  128/  306]
train() client id: f_00004-7-4 loss: 1.073379  [  160/  306]
train() client id: f_00004-7-5 loss: 0.833816  [  192/  306]
train() client id: f_00004-7-6 loss: 0.925511  [  224/  306]
train() client id: f_00004-7-7 loss: 0.985318  [  256/  306]
train() client id: f_00004-7-8 loss: 0.912813  [  288/  306]
train() client id: f_00004-8-0 loss: 1.039286  [   32/  306]
train() client id: f_00004-8-1 loss: 0.862900  [   64/  306]
train() client id: f_00004-8-2 loss: 0.900145  [   96/  306]
train() client id: f_00004-8-3 loss: 1.102775  [  128/  306]
train() client id: f_00004-8-4 loss: 0.807847  [  160/  306]
train() client id: f_00004-8-5 loss: 0.882499  [  192/  306]
train() client id: f_00004-8-6 loss: 0.963554  [  224/  306]
train() client id: f_00004-8-7 loss: 0.895785  [  256/  306]
train() client id: f_00004-8-8 loss: 0.889701  [  288/  306]
train() client id: f_00004-9-0 loss: 0.824291  [   32/  306]
train() client id: f_00004-9-1 loss: 0.863666  [   64/  306]
train() client id: f_00004-9-2 loss: 1.158887  [   96/  306]
train() client id: f_00004-9-3 loss: 0.823138  [  128/  306]
train() client id: f_00004-9-4 loss: 0.886452  [  160/  306]
train() client id: f_00004-9-5 loss: 0.941755  [  192/  306]
train() client id: f_00004-9-6 loss: 0.943522  [  224/  306]
train() client id: f_00004-9-7 loss: 0.945544  [  256/  306]
train() client id: f_00004-9-8 loss: 1.045904  [  288/  306]
train() client id: f_00004-10-0 loss: 0.907328  [   32/  306]
train() client id: f_00004-10-1 loss: 0.974929  [   64/  306]
train() client id: f_00004-10-2 loss: 0.953874  [   96/  306]
train() client id: f_00004-10-3 loss: 0.810235  [  128/  306]
train() client id: f_00004-10-4 loss: 1.118711  [  160/  306]
train() client id: f_00004-10-5 loss: 0.980758  [  192/  306]
train() client id: f_00004-10-6 loss: 0.851070  [  224/  306]
train() client id: f_00004-10-7 loss: 0.866366  [  256/  306]
train() client id: f_00004-10-8 loss: 0.841948  [  288/  306]
train() client id: f_00004-11-0 loss: 1.014689  [   32/  306]
train() client id: f_00004-11-1 loss: 1.028679  [   64/  306]
train() client id: f_00004-11-2 loss: 0.890930  [   96/  306]
train() client id: f_00004-11-3 loss: 0.900374  [  128/  306]
train() client id: f_00004-11-4 loss: 0.916340  [  160/  306]
train() client id: f_00004-11-5 loss: 0.847073  [  192/  306]
train() client id: f_00004-11-6 loss: 0.842915  [  224/  306]
train() client id: f_00004-11-7 loss: 0.947622  [  256/  306]
train() client id: f_00004-11-8 loss: 0.996877  [  288/  306]
train() client id: f_00005-0-0 loss: 0.624027  [   32/  146]
train() client id: f_00005-0-1 loss: 0.716266  [   64/  146]
train() client id: f_00005-0-2 loss: 0.609354  [   96/  146]
train() client id: f_00005-0-3 loss: 0.601384  [  128/  146]
train() client id: f_00005-1-0 loss: 0.896487  [   32/  146]
train() client id: f_00005-1-1 loss: 0.521520  [   64/  146]
train() client id: f_00005-1-2 loss: 0.562959  [   96/  146]
train() client id: f_00005-1-3 loss: 0.670054  [  128/  146]
train() client id: f_00005-2-0 loss: 0.590110  [   32/  146]
train() client id: f_00005-2-1 loss: 0.674955  [   64/  146]
train() client id: f_00005-2-2 loss: 1.028277  [   96/  146]
train() client id: f_00005-2-3 loss: 0.321986  [  128/  146]
train() client id: f_00005-3-0 loss: 0.632868  [   32/  146]
train() client id: f_00005-3-1 loss: 0.772531  [   64/  146]
train() client id: f_00005-3-2 loss: 0.576557  [   96/  146]
train() client id: f_00005-3-3 loss: 0.531908  [  128/  146]
train() client id: f_00005-4-0 loss: 0.775223  [   32/  146]
train() client id: f_00005-4-1 loss: 0.734342  [   64/  146]
train() client id: f_00005-4-2 loss: 0.321772  [   96/  146]
train() client id: f_00005-4-3 loss: 0.684979  [  128/  146]
train() client id: f_00005-5-0 loss: 0.523756  [   32/  146]
train() client id: f_00005-5-1 loss: 0.742982  [   64/  146]
train() client id: f_00005-5-2 loss: 0.766536  [   96/  146]
train() client id: f_00005-5-3 loss: 0.482558  [  128/  146]
train() client id: f_00005-6-0 loss: 1.013629  [   32/  146]
train() client id: f_00005-6-1 loss: 0.435696  [   64/  146]
train() client id: f_00005-6-2 loss: 0.425010  [   96/  146]
train() client id: f_00005-6-3 loss: 0.628699  [  128/  146]
train() client id: f_00005-7-0 loss: 0.563216  [   32/  146]
train() client id: f_00005-7-1 loss: 0.529882  [   64/  146]
train() client id: f_00005-7-2 loss: 0.741917  [   96/  146]
train() client id: f_00005-7-3 loss: 0.573094  [  128/  146]
train() client id: f_00005-8-0 loss: 0.817123  [   32/  146]
train() client id: f_00005-8-1 loss: 0.711544  [   64/  146]
train() client id: f_00005-8-2 loss: 0.511391  [   96/  146]
train() client id: f_00005-8-3 loss: 0.650688  [  128/  146]
train() client id: f_00005-9-0 loss: 0.647048  [   32/  146]
train() client id: f_00005-9-1 loss: 0.609672  [   64/  146]
train() client id: f_00005-9-2 loss: 0.583745  [   96/  146]
train() client id: f_00005-9-3 loss: 0.560246  [  128/  146]
train() client id: f_00005-10-0 loss: 0.618350  [   32/  146]
train() client id: f_00005-10-1 loss: 0.598903  [   64/  146]
train() client id: f_00005-10-2 loss: 0.703874  [   96/  146]
train() client id: f_00005-10-3 loss: 0.659059  [  128/  146]
train() client id: f_00005-11-0 loss: 0.559212  [   32/  146]
train() client id: f_00005-11-1 loss: 0.651113  [   64/  146]
train() client id: f_00005-11-2 loss: 0.911269  [   96/  146]
train() client id: f_00005-11-3 loss: 0.556808  [  128/  146]
train() client id: f_00006-0-0 loss: 0.542208  [   32/   54]
train() client id: f_00006-1-0 loss: 0.528918  [   32/   54]
train() client id: f_00006-2-0 loss: 0.533847  [   32/   54]
train() client id: f_00006-3-0 loss: 0.534393  [   32/   54]
train() client id: f_00006-4-0 loss: 0.586105  [   32/   54]
train() client id: f_00006-5-0 loss: 0.527213  [   32/   54]
train() client id: f_00006-6-0 loss: 0.537264  [   32/   54]
train() client id: f_00006-7-0 loss: 0.593819  [   32/   54]
train() client id: f_00006-8-0 loss: 0.589474  [   32/   54]
train() client id: f_00006-9-0 loss: 0.519129  [   32/   54]
train() client id: f_00006-10-0 loss: 0.528827  [   32/   54]
train() client id: f_00006-11-0 loss: 0.541486  [   32/   54]
train() client id: f_00007-0-0 loss: 0.682235  [   32/  179]
train() client id: f_00007-0-1 loss: 0.832029  [   64/  179]
train() client id: f_00007-0-2 loss: 0.659589  [   96/  179]
train() client id: f_00007-0-3 loss: 0.653737  [  128/  179]
train() client id: f_00007-0-4 loss: 0.557758  [  160/  179]
train() client id: f_00007-1-0 loss: 0.537692  [   32/  179]
train() client id: f_00007-1-1 loss: 0.550534  [   64/  179]
train() client id: f_00007-1-2 loss: 0.895541  [   96/  179]
train() client id: f_00007-1-3 loss: 0.956472  [  128/  179]
train() client id: f_00007-1-4 loss: 0.587571  [  160/  179]
train() client id: f_00007-2-0 loss: 0.704617  [   32/  179]
train() client id: f_00007-2-1 loss: 0.496711  [   64/  179]
train() client id: f_00007-2-2 loss: 0.686858  [   96/  179]
train() client id: f_00007-2-3 loss: 0.692599  [  128/  179]
train() client id: f_00007-2-4 loss: 0.723847  [  160/  179]
train() client id: f_00007-3-0 loss: 0.834525  [   32/  179]
train() client id: f_00007-3-1 loss: 0.646354  [   64/  179]
train() client id: f_00007-3-2 loss: 0.530501  [   96/  179]
train() client id: f_00007-3-3 loss: 0.791259  [  128/  179]
train() client id: f_00007-3-4 loss: 0.684512  [  160/  179]
train() client id: f_00007-4-0 loss: 0.948801  [   32/  179]
train() client id: f_00007-4-1 loss: 0.581207  [   64/  179]
train() client id: f_00007-4-2 loss: 0.573610  [   96/  179]
train() client id: f_00007-4-3 loss: 0.631801  [  128/  179]
train() client id: f_00007-4-4 loss: 0.573618  [  160/  179]
train() client id: f_00007-5-0 loss: 0.639017  [   32/  179]
train() client id: f_00007-5-1 loss: 0.784079  [   64/  179]
train() client id: f_00007-5-2 loss: 0.830373  [   96/  179]
train() client id: f_00007-5-3 loss: 0.570515  [  128/  179]
train() client id: f_00007-5-4 loss: 0.680482  [  160/  179]
train() client id: f_00007-6-0 loss: 0.599585  [   32/  179]
train() client id: f_00007-6-1 loss: 0.706560  [   64/  179]
train() client id: f_00007-6-2 loss: 0.794737  [   96/  179]
train() client id: f_00007-6-3 loss: 0.718812  [  128/  179]
train() client id: f_00007-6-4 loss: 0.542453  [  160/  179]
train() client id: f_00007-7-0 loss: 0.897462  [   32/  179]
train() client id: f_00007-7-1 loss: 0.623773  [   64/  179]
train() client id: f_00007-7-2 loss: 0.753358  [   96/  179]
train() client id: f_00007-7-3 loss: 0.517343  [  128/  179]
train() client id: f_00007-7-4 loss: 0.562931  [  160/  179]
train() client id: f_00007-8-0 loss: 0.694838  [   32/  179]
train() client id: f_00007-8-1 loss: 0.733817  [   64/  179]
train() client id: f_00007-8-2 loss: 0.634465  [   96/  179]
train() client id: f_00007-8-3 loss: 0.635602  [  128/  179]
train() client id: f_00007-8-4 loss: 0.766336  [  160/  179]
train() client id: f_00007-9-0 loss: 0.613465  [   32/  179]
train() client id: f_00007-9-1 loss: 0.605798  [   64/  179]
train() client id: f_00007-9-2 loss: 0.553500  [   96/  179]
train() client id: f_00007-9-3 loss: 0.847465  [  128/  179]
train() client id: f_00007-9-4 loss: 0.761254  [  160/  179]
train() client id: f_00007-10-0 loss: 0.899151  [   32/  179]
train() client id: f_00007-10-1 loss: 0.804533  [   64/  179]
train() client id: f_00007-10-2 loss: 0.636526  [   96/  179]
train() client id: f_00007-10-3 loss: 0.533568  [  128/  179]
train() client id: f_00007-10-4 loss: 0.503929  [  160/  179]
train() client id: f_00007-11-0 loss: 0.732282  [   32/  179]
train() client id: f_00007-11-1 loss: 0.580686  [   64/  179]
train() client id: f_00007-11-2 loss: 0.827900  [   96/  179]
train() client id: f_00007-11-3 loss: 0.631570  [  128/  179]
train() client id: f_00007-11-4 loss: 0.722338  [  160/  179]
train() client id: f_00008-0-0 loss: 0.627961  [   32/  130]
train() client id: f_00008-0-1 loss: 0.711396  [   64/  130]
train() client id: f_00008-0-2 loss: 0.748843  [   96/  130]
train() client id: f_00008-0-3 loss: 0.661915  [  128/  130]
train() client id: f_00008-1-0 loss: 0.769020  [   32/  130]
train() client id: f_00008-1-1 loss: 0.698067  [   64/  130]
train() client id: f_00008-1-2 loss: 0.639463  [   96/  130]
train() client id: f_00008-1-3 loss: 0.680036  [  128/  130]
train() client id: f_00008-2-0 loss: 0.637975  [   32/  130]
train() client id: f_00008-2-1 loss: 0.742670  [   64/  130]
train() client id: f_00008-2-2 loss: 0.659293  [   96/  130]
train() client id: f_00008-2-3 loss: 0.712226  [  128/  130]
train() client id: f_00008-3-0 loss: 0.686005  [   32/  130]
train() client id: f_00008-3-1 loss: 0.723576  [   64/  130]
train() client id: f_00008-3-2 loss: 0.645798  [   96/  130]
train() client id: f_00008-3-3 loss: 0.735076  [  128/  130]
train() client id: f_00008-4-0 loss: 0.676372  [   32/  130]
train() client id: f_00008-4-1 loss: 0.780250  [   64/  130]
train() client id: f_00008-4-2 loss: 0.646029  [   96/  130]
train() client id: f_00008-4-3 loss: 0.688854  [  128/  130]
train() client id: f_00008-5-0 loss: 0.684807  [   32/  130]
train() client id: f_00008-5-1 loss: 0.648926  [   64/  130]
train() client id: f_00008-5-2 loss: 0.730370  [   96/  130]
train() client id: f_00008-5-3 loss: 0.726932  [  128/  130]
train() client id: f_00008-6-0 loss: 0.635283  [   32/  130]
train() client id: f_00008-6-1 loss: 0.578942  [   64/  130]
train() client id: f_00008-6-2 loss: 0.753330  [   96/  130]
train() client id: f_00008-6-3 loss: 0.815939  [  128/  130]
train() client id: f_00008-7-0 loss: 0.703458  [   32/  130]
train() client id: f_00008-7-1 loss: 0.734851  [   64/  130]
train() client id: f_00008-7-2 loss: 0.677844  [   96/  130]
train() client id: f_00008-7-3 loss: 0.681724  [  128/  130]
train() client id: f_00008-8-0 loss: 0.604335  [   32/  130]
train() client id: f_00008-8-1 loss: 0.721442  [   64/  130]
train() client id: f_00008-8-2 loss: 0.694352  [   96/  130]
train() client id: f_00008-8-3 loss: 0.727732  [  128/  130]
train() client id: f_00008-9-0 loss: 0.547280  [   32/  130]
train() client id: f_00008-9-1 loss: 0.665986  [   64/  130]
train() client id: f_00008-9-2 loss: 0.836282  [   96/  130]
train() client id: f_00008-9-3 loss: 0.741565  [  128/  130]
train() client id: f_00008-10-0 loss: 0.732225  [   32/  130]
train() client id: f_00008-10-1 loss: 0.667795  [   64/  130]
train() client id: f_00008-10-2 loss: 0.662520  [   96/  130]
train() client id: f_00008-10-3 loss: 0.728282  [  128/  130]
train() client id: f_00008-11-0 loss: 0.764898  [   32/  130]
train() client id: f_00008-11-1 loss: 0.770255  [   64/  130]
train() client id: f_00008-11-2 loss: 0.557752  [   96/  130]
train() client id: f_00008-11-3 loss: 0.715282  [  128/  130]
train() client id: f_00009-0-0 loss: 0.993521  [   32/  118]
train() client id: f_00009-0-1 loss: 1.203274  [   64/  118]
train() client id: f_00009-0-2 loss: 1.025976  [   96/  118]
train() client id: f_00009-1-0 loss: 1.101175  [   32/  118]
train() client id: f_00009-1-1 loss: 1.010338  [   64/  118]
train() client id: f_00009-1-2 loss: 1.021697  [   96/  118]
train() client id: f_00009-2-0 loss: 1.017707  [   32/  118]
train() client id: f_00009-2-1 loss: 0.873610  [   64/  118]
train() client id: f_00009-2-2 loss: 1.006666  [   96/  118]
train() client id: f_00009-3-0 loss: 0.914780  [   32/  118]
train() client id: f_00009-3-1 loss: 0.957009  [   64/  118]
train() client id: f_00009-3-2 loss: 1.005228  [   96/  118]
train() client id: f_00009-4-0 loss: 0.994101  [   32/  118]
train() client id: f_00009-4-1 loss: 0.950831  [   64/  118]
train() client id: f_00009-4-2 loss: 0.890950  [   96/  118]
train() client id: f_00009-5-0 loss: 0.747129  [   32/  118]
train() client id: f_00009-5-1 loss: 0.895082  [   64/  118]
train() client id: f_00009-5-2 loss: 0.901311  [   96/  118]
train() client id: f_00009-6-0 loss: 0.934545  [   32/  118]
train() client id: f_00009-6-1 loss: 0.944328  [   64/  118]
train() client id: f_00009-6-2 loss: 0.802522  [   96/  118]
train() client id: f_00009-7-0 loss: 0.886132  [   32/  118]
train() client id: f_00009-7-1 loss: 0.867148  [   64/  118]
train() client id: f_00009-7-2 loss: 0.860732  [   96/  118]
train() client id: f_00009-8-0 loss: 0.870639  [   32/  118]
train() client id: f_00009-8-1 loss: 0.938322  [   64/  118]
train() client id: f_00009-8-2 loss: 0.787930  [   96/  118]
train() client id: f_00009-9-0 loss: 0.867447  [   32/  118]
train() client id: f_00009-9-1 loss: 0.766936  [   64/  118]
train() client id: f_00009-9-2 loss: 0.911386  [   96/  118]
train() client id: f_00009-10-0 loss: 0.822084  [   32/  118]
train() client id: f_00009-10-1 loss: 1.004256  [   64/  118]
train() client id: f_00009-10-2 loss: 0.745521  [   96/  118]
train() client id: f_00009-11-0 loss: 0.825217  [   32/  118]
train() client id: f_00009-11-1 loss: 0.999512  [   64/  118]
train() client id: f_00009-11-2 loss: 0.766213  [   96/  118]
At round 33 accuracy: 0.6525198938992043
At round 33 training accuracy: 0.5801475519785378
At round 33 training loss: 0.8343143241875176
update_location
xs = -4.528292 66.001589 75.045120 -75.943528 -25.103519 -0.217951 -117.215960 138.375741 -1.680116 59.695607 
ys = 152.587959 15.555839 1.320614 22.544824 9.350187 2.814151 -3.124922 -19.177652 -82.154970 4.001482 
xs mean: 11.442869159412867
ys mean: 10.371751218646876
dists_uav = 182.492714 120.822986 125.034051 127.576207 103.525903 100.039827 154.108229 171.801129 129.430529 116.531444 
uav_gains = -106.585258 -102.053975 -102.426089 -102.644747 -100.376252 -100.004339 -104.702137 -105.901585 -102.801534 -101.661220 
uav_gains_db_mean: -102.91571343029585
dists_bs = 171.938660 288.970944 304.445547 181.809695 223.402810 245.350222 187.263138 368.658806 310.110401 290.383404 
bs_gains = -102.157518 -108.470976 -109.105329 -102.836336 -105.341541 -106.481080 -103.195723 -111.432548 -109.329516 -108.530269 
bs_gains_db_mean: -106.68808374975794
Round 34
-------------------------------
ene_coms = [0.00865571 0.00967678 0.00700706 0.00707826 0.00808891 0.00860531
 0.00782468 0.00833498 0.01022133 0.00971262]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.49187245 13.51110264  6.36358423  2.28436922 15.54997013  7.49853782
  2.85096575  9.15640944  6.71952462  6.10652312]
obj_prev = 76.53285942997307
eta_min = 7.341516983320002e-15	eta_max = 0.9394990053415678
af = 16.14675753318756	bf = 1.4089644434593587	zeta = 17.761433286506318	eta = 0.9090909090909091
af = 16.14675753318756	bf = 1.4089644434593587	zeta = 32.42706287924732	eta = 0.49794079696071286
af = 16.14675753318756	bf = 1.4089644434593587	zeta = 25.23144689379545	eta = 0.6399457629660603
af = 16.14675753318756	bf = 1.4089644434593587	zeta = 23.930901554231124	eta = 0.6747241635086966
af = 16.14675753318756	bf = 1.4089644434593587	zeta = 23.862600937719435	eta = 0.6766553895499506
af = 16.14675753318756	bf = 1.4089644434593587	zeta = 23.862397164009217	eta = 0.6766611678704739
eta = 0.6766611678704739
ene_coms = [0.00865571 0.00967678 0.00700706 0.00707826 0.00808891 0.00860531
 0.00782468 0.00833498 0.01022133 0.00971262]
ene_comp = [0.03223011 0.06778554 0.03171852 0.01099916 0.07827312 0.037346
 0.0138129  0.04578722 0.03325328 0.03018375]
ene_total = [2.09096345 3.96154213 1.98048561 0.92450718 4.4166867  2.35002035
 1.10657934 2.7678923  2.22335873 2.04036138]
ti_comp = [0.46680637 0.45659565 0.48329285 0.48258088 0.47247435 0.46731037
 0.47511665 0.47001364 0.45115013 0.45623732]
ti_coms = [0.08655711 0.09676782 0.07007063 0.0707826  0.08088913 0.0860531
 0.07824683 0.08334983 0.10221334 0.09712615]
t_total = [28.29985733 28.29985733 28.29985733 28.29985733 28.29985733 28.29985733
 28.29985733 28.29985733 28.29985733 28.29985733]
ene_coms = [0.00865571 0.00967678 0.00700706 0.00707826 0.00808891 0.00860531
 0.00782468 0.00833498 0.01022133 0.00971262]
ene_comp = [9.60266339e-06 9.33743713e-05 8.53882143e-06 3.57123780e-07
 1.34264569e-04 1.49073806e-05 7.29683289e-07 2.71576248e-05
 1.12912362e-05 8.25691884e-06]
ene_total = [0.44315742 0.4996608  0.35878862 0.36201134 0.4205459  0.44085115
 0.4002036  0.42765273 0.52331217 0.49714032]
optimize_network iter = 0 obj = 4.373324043741772
eta = 0.6766611678704739
freqs = [34521922.71341529 74229289.58855276 32815008.33384686 11396186.62985964
 82833191.34587295 39958450.32767326 14536328.06465595 48708389.75169367
 36853901.09795559 33079002.12586122]
eta_min = 0.6766611678704826	eta_max = 0.6766611678704767
af = 0.013060026128657174	bf = 1.4089644434593587	zeta = 0.014366028741522893	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00865571 0.00967678 0.00700706 0.00707826 0.00808891 0.00860531
 0.00782468 0.00833498 0.01022133 0.00971262]
ene_comp = [2.34323505e-06 2.27851472e-05 2.08363709e-06 8.71450891e-08
 3.27631441e-05 3.63768833e-06 1.78056794e-07 6.62698417e-06
 2.75527938e-06 2.01484742e-06]
ene_total = [1.57486956 1.76431719 1.27493905 1.28752647 1.47730426 1.56593734
 1.42331468 1.51730962 1.85972594 1.76705705]
ti_comp = [0.46680637 0.45659565 0.48329285 0.48258088 0.47247435 0.46731037
 0.47511665 0.47001364 0.45115013 0.45623732]
ti_coms = [0.08655711 0.09676782 0.07007063 0.0707826  0.08088913 0.0860531
 0.07824683 0.08334983 0.10221334 0.09712615]
t_total = [28.29985733 28.29985733 28.29985733 28.29985733 28.29985733 28.29985733
 28.29985733 28.29985733 28.29985733 28.29985733]
ene_coms = [0.00865571 0.00967678 0.00700706 0.00707826 0.00808891 0.00860531
 0.00782468 0.00833498 0.01022133 0.00971262]
ene_comp = [9.60266339e-06 9.33743713e-05 8.53882143e-06 3.57123780e-07
 1.34264569e-04 1.49073806e-05 7.29683289e-07 2.71576248e-05
 1.12912362e-05 8.25691884e-06]
ene_total = [0.44315742 0.4996608  0.35878862 0.36201134 0.4205459  0.44085115
 0.4002036  0.42765273 0.52331217 0.49714032]
optimize_network iter = 1 obj = 4.37332404374181
eta = 0.6766611678704767
freqs = [34521922.71341528 74229289.58855274 32815008.33384684 11396186.62985963
 82833191.34587291 39958450.32767323 14536328.06465594 48708389.75169366
 36853901.09795559 33079002.12586122]
Done!
ene_coms = [0.00865571 0.00967678 0.00700706 0.00707826 0.00808891 0.00860531
 0.00782468 0.00833498 0.01022133 0.00971262]
ene_comp = [9.00972940e-06 8.76088002e-05 8.01157630e-06 3.35072520e-07
 1.25974158e-04 1.39868971e-05 6.84627661e-07 2.54807277e-05
 1.05940382e-05 7.74708030e-06]
ene_total = [0.00866472 0.00976439 0.00701507 0.0070786  0.00821489 0.0086193
 0.00782537 0.00836046 0.01023193 0.00972036]
At round 34 energy consumption: 0.08549508639386924
At round 34 eta: 0.6766611678704767
At round 34 a_n: 16.536044059297588
At round 34 local rounds: 12.789724930407564
At round 34 global rounds: 51.1415345641923
gradient difference: 0.3817129135131836
train() client id: f_00000-0-0 loss: 1.222223  [   32/  126]
train() client id: f_00000-0-1 loss: 1.181687  [   64/  126]
train() client id: f_00000-0-2 loss: 1.265810  [   96/  126]
train() client id: f_00000-1-0 loss: 1.062873  [   32/  126]
train() client id: f_00000-1-1 loss: 1.244477  [   64/  126]
train() client id: f_00000-1-2 loss: 0.968008  [   96/  126]
train() client id: f_00000-2-0 loss: 1.186229  [   32/  126]
train() client id: f_00000-2-1 loss: 0.979799  [   64/  126]
train() client id: f_00000-2-2 loss: 0.998620  [   96/  126]
train() client id: f_00000-3-0 loss: 0.764400  [   32/  126]
train() client id: f_00000-3-1 loss: 0.974751  [   64/  126]
train() client id: f_00000-3-2 loss: 0.971980  [   96/  126]
train() client id: f_00000-4-0 loss: 0.977212  [   32/  126]
train() client id: f_00000-4-1 loss: 0.897338  [   64/  126]
train() client id: f_00000-4-2 loss: 0.907784  [   96/  126]
train() client id: f_00000-5-0 loss: 0.806680  [   32/  126]
train() client id: f_00000-5-1 loss: 0.729864  [   64/  126]
train() client id: f_00000-5-2 loss: 0.948076  [   96/  126]
train() client id: f_00000-6-0 loss: 0.840824  [   32/  126]
train() client id: f_00000-6-1 loss: 0.934201  [   64/  126]
train() client id: f_00000-6-2 loss: 0.740911  [   96/  126]
train() client id: f_00000-7-0 loss: 0.863480  [   32/  126]
train() client id: f_00000-7-1 loss: 0.815117  [   64/  126]
train() client id: f_00000-7-2 loss: 0.796990  [   96/  126]
train() client id: f_00000-8-0 loss: 0.733758  [   32/  126]
train() client id: f_00000-8-1 loss: 0.845273  [   64/  126]
train() client id: f_00000-8-2 loss: 0.867585  [   96/  126]
train() client id: f_00000-9-0 loss: 0.751707  [   32/  126]
train() client id: f_00000-9-1 loss: 0.683804  [   64/  126]
train() client id: f_00000-9-2 loss: 0.909051  [   96/  126]
train() client id: f_00000-10-0 loss: 0.790383  [   32/  126]
train() client id: f_00000-10-1 loss: 0.821418  [   64/  126]
train() client id: f_00000-10-2 loss: 0.711188  [   96/  126]
train() client id: f_00000-11-0 loss: 0.798789  [   32/  126]
train() client id: f_00000-11-1 loss: 0.792126  [   64/  126]
train() client id: f_00000-11-2 loss: 0.798771  [   96/  126]
train() client id: f_00001-0-0 loss: 0.425631  [   32/  265]
train() client id: f_00001-0-1 loss: 0.362662  [   64/  265]
train() client id: f_00001-0-2 loss: 0.303827  [   96/  265]
train() client id: f_00001-0-3 loss: 0.237153  [  128/  265]
train() client id: f_00001-0-4 loss: 0.292009  [  160/  265]
train() client id: f_00001-0-5 loss: 0.331288  [  192/  265]
train() client id: f_00001-0-6 loss: 0.314156  [  224/  265]
train() client id: f_00001-0-7 loss: 0.398573  [  256/  265]
train() client id: f_00001-1-0 loss: 0.249178  [   32/  265]
train() client id: f_00001-1-1 loss: 0.266297  [   64/  265]
train() client id: f_00001-1-2 loss: 0.259602  [   96/  265]
train() client id: f_00001-1-3 loss: 0.266162  [  128/  265]
train() client id: f_00001-1-4 loss: 0.424376  [  160/  265]
train() client id: f_00001-1-5 loss: 0.375705  [  192/  265]
train() client id: f_00001-1-6 loss: 0.305305  [  224/  265]
train() client id: f_00001-1-7 loss: 0.423694  [  256/  265]
train() client id: f_00001-2-0 loss: 0.474614  [   32/  265]
train() client id: f_00001-2-1 loss: 0.223308  [   64/  265]
train() client id: f_00001-2-2 loss: 0.386599  [   96/  265]
train() client id: f_00001-2-3 loss: 0.378593  [  128/  265]
train() client id: f_00001-2-4 loss: 0.232390  [  160/  265]
train() client id: f_00001-2-5 loss: 0.283101  [  192/  265]
train() client id: f_00001-2-6 loss: 0.234083  [  224/  265]
train() client id: f_00001-2-7 loss: 0.288662  [  256/  265]
train() client id: f_00001-3-0 loss: 0.403855  [   32/  265]
train() client id: f_00001-3-1 loss: 0.294051  [   64/  265]
train() client id: f_00001-3-2 loss: 0.276532  [   96/  265]
train() client id: f_00001-3-3 loss: 0.342324  [  128/  265]
train() client id: f_00001-3-4 loss: 0.206450  [  160/  265]
train() client id: f_00001-3-5 loss: 0.320443  [  192/  265]
train() client id: f_00001-3-6 loss: 0.298954  [  224/  265]
train() client id: f_00001-3-7 loss: 0.284441  [  256/  265]
train() client id: f_00001-4-0 loss: 0.243233  [   32/  265]
train() client id: f_00001-4-1 loss: 0.389362  [   64/  265]
train() client id: f_00001-4-2 loss: 0.349333  [   96/  265]
train() client id: f_00001-4-3 loss: 0.225649  [  128/  265]
train() client id: f_00001-4-4 loss: 0.290423  [  160/  265]
train() client id: f_00001-4-5 loss: 0.292128  [  192/  265]
train() client id: f_00001-4-6 loss: 0.337038  [  224/  265]
train() client id: f_00001-4-7 loss: 0.249978  [  256/  265]
train() client id: f_00001-5-0 loss: 0.210457  [   32/  265]
train() client id: f_00001-5-1 loss: 0.234946  [   64/  265]
train() client id: f_00001-5-2 loss: 0.449295  [   96/  265]
train() client id: f_00001-5-3 loss: 0.281521  [  128/  265]
train() client id: f_00001-5-4 loss: 0.268314  [  160/  265]
train() client id: f_00001-5-5 loss: 0.255902  [  192/  265]
train() client id: f_00001-5-6 loss: 0.209641  [  224/  265]
train() client id: f_00001-5-7 loss: 0.290388  [  256/  265]
train() client id: f_00001-6-0 loss: 0.174625  [   32/  265]
train() client id: f_00001-6-1 loss: 0.337747  [   64/  265]
train() client id: f_00001-6-2 loss: 0.245188  [   96/  265]
train() client id: f_00001-6-3 loss: 0.304590  [  128/  265]
train() client id: f_00001-6-4 loss: 0.298493  [  160/  265]
train() client id: f_00001-6-5 loss: 0.344751  [  192/  265]
train() client id: f_00001-6-6 loss: 0.318592  [  224/  265]
train() client id: f_00001-6-7 loss: 0.277612  [  256/  265]
train() client id: f_00001-7-0 loss: 0.369258  [   32/  265]
train() client id: f_00001-7-1 loss: 0.269976  [   64/  265]
train() client id: f_00001-7-2 loss: 0.275349  [   96/  265]
train() client id: f_00001-7-3 loss: 0.247688  [  128/  265]
train() client id: f_00001-7-4 loss: 0.374454  [  160/  265]
train() client id: f_00001-7-5 loss: 0.233099  [  192/  265]
train() client id: f_00001-7-6 loss: 0.234484  [  224/  265]
train() client id: f_00001-7-7 loss: 0.281617  [  256/  265]
train() client id: f_00001-8-0 loss: 0.267599  [   32/  265]
train() client id: f_00001-8-1 loss: 0.223159  [   64/  265]
train() client id: f_00001-8-2 loss: 0.255801  [   96/  265]
train() client id: f_00001-8-3 loss: 0.333503  [  128/  265]
train() client id: f_00001-8-4 loss: 0.371709  [  160/  265]
train() client id: f_00001-8-5 loss: 0.228139  [  192/  265]
train() client id: f_00001-8-6 loss: 0.189434  [  224/  265]
train() client id: f_00001-8-7 loss: 0.336455  [  256/  265]
train() client id: f_00001-9-0 loss: 0.417120  [   32/  265]
train() client id: f_00001-9-1 loss: 0.258349  [   64/  265]
train() client id: f_00001-9-2 loss: 0.164947  [   96/  265]
train() client id: f_00001-9-3 loss: 0.244676  [  128/  265]
train() client id: f_00001-9-4 loss: 0.202918  [  160/  265]
train() client id: f_00001-9-5 loss: 0.246628  [  192/  265]
train() client id: f_00001-9-6 loss: 0.338473  [  224/  265]
train() client id: f_00001-9-7 loss: 0.356867  [  256/  265]
train() client id: f_00001-10-0 loss: 0.245668  [   32/  265]
train() client id: f_00001-10-1 loss: 0.265390  [   64/  265]
train() client id: f_00001-10-2 loss: 0.299504  [   96/  265]
train() client id: f_00001-10-3 loss: 0.173330  [  128/  265]
train() client id: f_00001-10-4 loss: 0.277477  [  160/  265]
train() client id: f_00001-10-5 loss: 0.440635  [  192/  265]
train() client id: f_00001-10-6 loss: 0.173732  [  224/  265]
train() client id: f_00001-10-7 loss: 0.343306  [  256/  265]
train() client id: f_00001-11-0 loss: 0.400178  [   32/  265]
train() client id: f_00001-11-1 loss: 0.173941  [   64/  265]
train() client id: f_00001-11-2 loss: 0.255868  [   96/  265]
train() client id: f_00001-11-3 loss: 0.466865  [  128/  265]
train() client id: f_00001-11-4 loss: 0.267939  [  160/  265]
train() client id: f_00001-11-5 loss: 0.260915  [  192/  265]
train() client id: f_00001-11-6 loss: 0.197285  [  224/  265]
train() client id: f_00001-11-7 loss: 0.185142  [  256/  265]
train() client id: f_00002-0-0 loss: 1.253582  [   32/  124]
train() client id: f_00002-0-1 loss: 1.547393  [   64/  124]
train() client id: f_00002-0-2 loss: 0.994244  [   96/  124]
train() client id: f_00002-1-0 loss: 1.065159  [   32/  124]
train() client id: f_00002-1-1 loss: 1.264854  [   64/  124]
train() client id: f_00002-1-2 loss: 1.316442  [   96/  124]
train() client id: f_00002-2-0 loss: 1.027745  [   32/  124]
train() client id: f_00002-2-1 loss: 1.150246  [   64/  124]
train() client id: f_00002-2-2 loss: 1.197842  [   96/  124]
train() client id: f_00002-3-0 loss: 1.023569  [   32/  124]
train() client id: f_00002-3-1 loss: 1.152851  [   64/  124]
train() client id: f_00002-3-2 loss: 1.108068  [   96/  124]
train() client id: f_00002-4-0 loss: 1.222034  [   32/  124]
train() client id: f_00002-4-1 loss: 1.036081  [   64/  124]
train() client id: f_00002-4-2 loss: 0.988426  [   96/  124]
train() client id: f_00002-5-0 loss: 1.091930  [   32/  124]
train() client id: f_00002-5-1 loss: 1.365595  [   64/  124]
train() client id: f_00002-5-2 loss: 0.815438  [   96/  124]
train() client id: f_00002-6-0 loss: 1.127362  [   32/  124]
train() client id: f_00002-6-1 loss: 1.063383  [   64/  124]
train() client id: f_00002-6-2 loss: 0.967889  [   96/  124]
train() client id: f_00002-7-0 loss: 1.103909  [   32/  124]
train() client id: f_00002-7-1 loss: 1.153379  [   64/  124]
train() client id: f_00002-7-2 loss: 0.871678  [   96/  124]
train() client id: f_00002-8-0 loss: 1.036658  [   32/  124]
train() client id: f_00002-8-1 loss: 0.917161  [   64/  124]
train() client id: f_00002-8-2 loss: 1.070631  [   96/  124]
train() client id: f_00002-9-0 loss: 0.984372  [   32/  124]
train() client id: f_00002-9-1 loss: 1.118182  [   64/  124]
train() client id: f_00002-9-2 loss: 0.935534  [   96/  124]
train() client id: f_00002-10-0 loss: 0.936688  [   32/  124]
train() client id: f_00002-10-1 loss: 0.934918  [   64/  124]
train() client id: f_00002-10-2 loss: 1.107409  [   96/  124]
train() client id: f_00002-11-0 loss: 1.058345  [   32/  124]
train() client id: f_00002-11-1 loss: 0.949293  [   64/  124]
train() client id: f_00002-11-2 loss: 0.931662  [   96/  124]
train() client id: f_00003-0-0 loss: 0.524824  [   32/   43]
train() client id: f_00003-1-0 loss: 0.634655  [   32/   43]
train() client id: f_00003-2-0 loss: 0.641131  [   32/   43]
train() client id: f_00003-3-0 loss: 0.768061  [   32/   43]
train() client id: f_00003-4-0 loss: 0.831523  [   32/   43]
train() client id: f_00003-5-0 loss: 0.712430  [   32/   43]
train() client id: f_00003-6-0 loss: 0.723408  [   32/   43]
train() client id: f_00003-7-0 loss: 0.671334  [   32/   43]
train() client id: f_00003-8-0 loss: 0.689456  [   32/   43]
train() client id: f_00003-9-0 loss: 0.724358  [   32/   43]
train() client id: f_00003-10-0 loss: 0.663104  [   32/   43]
train() client id: f_00003-11-0 loss: 0.670654  [   32/   43]
train() client id: f_00004-0-0 loss: 0.750919  [   32/  306]
train() client id: f_00004-0-1 loss: 0.831249  [   64/  306]
train() client id: f_00004-0-2 loss: 0.783116  [   96/  306]
train() client id: f_00004-0-3 loss: 0.745155  [  128/  306]
train() client id: f_00004-0-4 loss: 0.741287  [  160/  306]
train() client id: f_00004-0-5 loss: 0.708536  [  192/  306]
train() client id: f_00004-0-6 loss: 0.774148  [  224/  306]
train() client id: f_00004-0-7 loss: 0.748536  [  256/  306]
train() client id: f_00004-0-8 loss: 0.781818  [  288/  306]
train() client id: f_00004-1-0 loss: 0.801908  [   32/  306]
train() client id: f_00004-1-1 loss: 0.732689  [   64/  306]
train() client id: f_00004-1-2 loss: 0.744701  [   96/  306]
train() client id: f_00004-1-3 loss: 0.713860  [  128/  306]
train() client id: f_00004-1-4 loss: 0.796552  [  160/  306]
train() client id: f_00004-1-5 loss: 0.722286  [  192/  306]
train() client id: f_00004-1-6 loss: 0.808718  [  224/  306]
train() client id: f_00004-1-7 loss: 0.750163  [  256/  306]
train() client id: f_00004-1-8 loss: 0.724046  [  288/  306]
train() client id: f_00004-2-0 loss: 0.844180  [   32/  306]
train() client id: f_00004-2-1 loss: 0.798023  [   64/  306]
train() client id: f_00004-2-2 loss: 0.716644  [   96/  306]
train() client id: f_00004-2-3 loss: 0.777423  [  128/  306]
train() client id: f_00004-2-4 loss: 0.662689  [  160/  306]
train() client id: f_00004-2-5 loss: 0.621952  [  192/  306]
train() client id: f_00004-2-6 loss: 0.795446  [  224/  306]
train() client id: f_00004-2-7 loss: 0.714815  [  256/  306]
train() client id: f_00004-2-8 loss: 0.849264  [  288/  306]
train() client id: f_00004-3-0 loss: 0.572926  [   32/  306]
train() client id: f_00004-3-1 loss: 0.745412  [   64/  306]
train() client id: f_00004-3-2 loss: 0.711982  [   96/  306]
train() client id: f_00004-3-3 loss: 0.938889  [  128/  306]
train() client id: f_00004-3-4 loss: 0.703895  [  160/  306]
train() client id: f_00004-3-5 loss: 0.848193  [  192/  306]
train() client id: f_00004-3-6 loss: 0.799452  [  224/  306]
train() client id: f_00004-3-7 loss: 0.761816  [  256/  306]
train() client id: f_00004-3-8 loss: 0.750989  [  288/  306]
train() client id: f_00004-4-0 loss: 0.739103  [   32/  306]
train() client id: f_00004-4-1 loss: 0.853413  [   64/  306]
train() client id: f_00004-4-2 loss: 0.752784  [   96/  306]
train() client id: f_00004-4-3 loss: 0.721484  [  128/  306]
train() client id: f_00004-4-4 loss: 0.859643  [  160/  306]
train() client id: f_00004-4-5 loss: 0.908692  [  192/  306]
train() client id: f_00004-4-6 loss: 0.669875  [  224/  306]
train() client id: f_00004-4-7 loss: 0.717118  [  256/  306]
train() client id: f_00004-4-8 loss: 0.655315  [  288/  306]
train() client id: f_00004-5-0 loss: 0.856762  [   32/  306]
train() client id: f_00004-5-1 loss: 0.744573  [   64/  306]
train() client id: f_00004-5-2 loss: 0.831339  [   96/  306]
train() client id: f_00004-5-3 loss: 0.686352  [  128/  306]
train() client id: f_00004-5-4 loss: 0.758276  [  160/  306]
train() client id: f_00004-5-5 loss: 0.735212  [  192/  306]
train() client id: f_00004-5-6 loss: 0.832675  [  224/  306]
train() client id: f_00004-5-7 loss: 0.670650  [  256/  306]
train() client id: f_00004-5-8 loss: 0.625332  [  288/  306]
train() client id: f_00004-6-0 loss: 0.768088  [   32/  306]
train() client id: f_00004-6-1 loss: 0.789096  [   64/  306]
train() client id: f_00004-6-2 loss: 0.796920  [   96/  306]
train() client id: f_00004-6-3 loss: 0.745781  [  128/  306]
train() client id: f_00004-6-4 loss: 0.728445  [  160/  306]
train() client id: f_00004-6-5 loss: 0.795208  [  192/  306]
train() client id: f_00004-6-6 loss: 0.656912  [  224/  306]
train() client id: f_00004-6-7 loss: 0.738537  [  256/  306]
train() client id: f_00004-6-8 loss: 0.793917  [  288/  306]
train() client id: f_00004-7-0 loss: 0.765895  [   32/  306]
train() client id: f_00004-7-1 loss: 0.828699  [   64/  306]
train() client id: f_00004-7-2 loss: 0.767297  [   96/  306]
train() client id: f_00004-7-3 loss: 0.683043  [  128/  306]
train() client id: f_00004-7-4 loss: 0.876042  [  160/  306]
train() client id: f_00004-7-5 loss: 0.800060  [  192/  306]
train() client id: f_00004-7-6 loss: 0.683917  [  224/  306]
train() client id: f_00004-7-7 loss: 0.650065  [  256/  306]
train() client id: f_00004-7-8 loss: 0.754513  [  288/  306]
train() client id: f_00004-8-0 loss: 0.854629  [   32/  306]
train() client id: f_00004-8-1 loss: 0.886502  [   64/  306]
train() client id: f_00004-8-2 loss: 0.673693  [   96/  306]
train() client id: f_00004-8-3 loss: 0.662115  [  128/  306]
train() client id: f_00004-8-4 loss: 0.797011  [  160/  306]
train() client id: f_00004-8-5 loss: 0.721308  [  192/  306]
train() client id: f_00004-8-6 loss: 0.701155  [  224/  306]
train() client id: f_00004-8-7 loss: 0.805154  [  256/  306]
train() client id: f_00004-8-8 loss: 0.694832  [  288/  306]
train() client id: f_00004-9-0 loss: 0.705632  [   32/  306]
train() client id: f_00004-9-1 loss: 0.773615  [   64/  306]
train() client id: f_00004-9-2 loss: 0.745391  [   96/  306]
train() client id: f_00004-9-3 loss: 0.785182  [  128/  306]
train() client id: f_00004-9-4 loss: 0.867062  [  160/  306]
train() client id: f_00004-9-5 loss: 0.795546  [  192/  306]
train() client id: f_00004-9-6 loss: 0.773322  [  224/  306]
train() client id: f_00004-9-7 loss: 0.695458  [  256/  306]
train() client id: f_00004-9-8 loss: 0.694296  [  288/  306]
train() client id: f_00004-10-0 loss: 0.673999  [   32/  306]
train() client id: f_00004-10-1 loss: 0.881431  [   64/  306]
train() client id: f_00004-10-2 loss: 0.792809  [   96/  306]
train() client id: f_00004-10-3 loss: 0.728634  [  128/  306]
train() client id: f_00004-10-4 loss: 0.740647  [  160/  306]
train() client id: f_00004-10-5 loss: 0.641965  [  192/  306]
train() client id: f_00004-10-6 loss: 0.737374  [  224/  306]
train() client id: f_00004-10-7 loss: 0.874216  [  256/  306]
train() client id: f_00004-10-8 loss: 0.684201  [  288/  306]
train() client id: f_00004-11-0 loss: 0.776971  [   32/  306]
train() client id: f_00004-11-1 loss: 0.629754  [   64/  306]
train() client id: f_00004-11-2 loss: 0.659529  [   96/  306]
train() client id: f_00004-11-3 loss: 0.802570  [  128/  306]
train() client id: f_00004-11-4 loss: 0.909202  [  160/  306]
train() client id: f_00004-11-5 loss: 0.665555  [  192/  306]
train() client id: f_00004-11-6 loss: 0.941238  [  224/  306]
train() client id: f_00004-11-7 loss: 0.696463  [  256/  306]
train() client id: f_00004-11-8 loss: 0.754679  [  288/  306]
train() client id: f_00005-0-0 loss: 0.957488  [   32/  146]
train() client id: f_00005-0-1 loss: 0.625226  [   64/  146]
train() client id: f_00005-0-2 loss: 0.459630  [   96/  146]
train() client id: f_00005-0-3 loss: 0.790591  [  128/  146]
train() client id: f_00005-1-0 loss: 0.764132  [   32/  146]
train() client id: f_00005-1-1 loss: 0.721121  [   64/  146]
train() client id: f_00005-1-2 loss: 0.541504  [   96/  146]
train() client id: f_00005-1-3 loss: 0.587156  [  128/  146]
train() client id: f_00005-2-0 loss: 0.613272  [   32/  146]
train() client id: f_00005-2-1 loss: 0.623242  [   64/  146]
train() client id: f_00005-2-2 loss: 0.863578  [   96/  146]
train() client id: f_00005-2-3 loss: 0.662896  [  128/  146]
train() client id: f_00005-3-0 loss: 0.697782  [   32/  146]
train() client id: f_00005-3-1 loss: 0.597550  [   64/  146]
train() client id: f_00005-3-2 loss: 0.777042  [   96/  146]
train() client id: f_00005-3-3 loss: 0.775916  [  128/  146]
train() client id: f_00005-4-0 loss: 0.345265  [   32/  146]
train() client id: f_00005-4-1 loss: 0.591895  [   64/  146]
train() client id: f_00005-4-2 loss: 0.618006  [   96/  146]
train() client id: f_00005-4-3 loss: 1.104978  [  128/  146]
train() client id: f_00005-5-0 loss: 0.678335  [   32/  146]
train() client id: f_00005-5-1 loss: 0.803008  [   64/  146]
train() client id: f_00005-5-2 loss: 0.540436  [   96/  146]
train() client id: f_00005-5-3 loss: 0.659343  [  128/  146]
train() client id: f_00005-6-0 loss: 0.493318  [   32/  146]
train() client id: f_00005-6-1 loss: 0.833534  [   64/  146]
train() client id: f_00005-6-2 loss: 0.605589  [   96/  146]
train() client id: f_00005-6-3 loss: 0.586342  [  128/  146]
train() client id: f_00005-7-0 loss: 0.444153  [   32/  146]
train() client id: f_00005-7-1 loss: 0.642984  [   64/  146]
train() client id: f_00005-7-2 loss: 0.722381  [   96/  146]
train() client id: f_00005-7-3 loss: 0.907267  [  128/  146]
train() client id: f_00005-8-0 loss: 0.767560  [   32/  146]
train() client id: f_00005-8-1 loss: 0.499689  [   64/  146]
train() client id: f_00005-8-2 loss: 0.824508  [   96/  146]
train() client id: f_00005-8-3 loss: 0.791334  [  128/  146]
train() client id: f_00005-9-0 loss: 0.508972  [   32/  146]
train() client id: f_00005-9-1 loss: 0.552513  [   64/  146]
train() client id: f_00005-9-2 loss: 0.903145  [   96/  146]
train() client id: f_00005-9-3 loss: 0.687227  [  128/  146]
train() client id: f_00005-10-0 loss: 0.738684  [   32/  146]
train() client id: f_00005-10-1 loss: 0.733773  [   64/  146]
train() client id: f_00005-10-2 loss: 0.522892  [   96/  146]
train() client id: f_00005-10-3 loss: 0.548110  [  128/  146]
train() client id: f_00005-11-0 loss: 0.799752  [   32/  146]
train() client id: f_00005-11-1 loss: 0.530340  [   64/  146]
train() client id: f_00005-11-2 loss: 0.671446  [   96/  146]
train() client id: f_00005-11-3 loss: 0.889633  [  128/  146]
train() client id: f_00006-0-0 loss: 0.499932  [   32/   54]
train() client id: f_00006-1-0 loss: 0.487181  [   32/   54]
train() client id: f_00006-2-0 loss: 0.616720  [   32/   54]
train() client id: f_00006-3-0 loss: 0.501391  [   32/   54]
train() client id: f_00006-4-0 loss: 0.479675  [   32/   54]
train() client id: f_00006-5-0 loss: 0.587047  [   32/   54]
train() client id: f_00006-6-0 loss: 0.503650  [   32/   54]
train() client id: f_00006-7-0 loss: 0.549903  [   32/   54]
train() client id: f_00006-8-0 loss: 0.590078  [   32/   54]
train() client id: f_00006-9-0 loss: 0.544100  [   32/   54]
train() client id: f_00006-10-0 loss: 0.610572  [   32/   54]
train() client id: f_00006-11-0 loss: 0.524766  [   32/   54]
train() client id: f_00007-0-0 loss: 0.483692  [   32/  179]
train() client id: f_00007-0-1 loss: 0.597051  [   64/  179]
train() client id: f_00007-0-2 loss: 0.433542  [   96/  179]
train() client id: f_00007-0-3 loss: 0.613954  [  128/  179]
train() client id: f_00007-0-4 loss: 0.654073  [  160/  179]
train() client id: f_00007-1-0 loss: 0.539833  [   32/  179]
train() client id: f_00007-1-1 loss: 0.451034  [   64/  179]
train() client id: f_00007-1-2 loss: 0.555005  [   96/  179]
train() client id: f_00007-1-3 loss: 0.760090  [  128/  179]
train() client id: f_00007-1-4 loss: 0.599773  [  160/  179]
train() client id: f_00007-2-0 loss: 0.522398  [   32/  179]
train() client id: f_00007-2-1 loss: 0.526792  [   64/  179]
train() client id: f_00007-2-2 loss: 0.426980  [   96/  179]
train() client id: f_00007-2-3 loss: 0.559011  [  128/  179]
train() client id: f_00007-2-4 loss: 0.809987  [  160/  179]
train() client id: f_00007-3-0 loss: 0.659479  [   32/  179]
train() client id: f_00007-3-1 loss: 0.476803  [   64/  179]
train() client id: f_00007-3-2 loss: 0.400610  [   96/  179]
train() client id: f_00007-3-3 loss: 0.612882  [  128/  179]
train() client id: f_00007-3-4 loss: 0.571306  [  160/  179]
train() client id: f_00007-4-0 loss: 0.639433  [   32/  179]
train() client id: f_00007-4-1 loss: 0.674811  [   64/  179]
train() client id: f_00007-4-2 loss: 0.527498  [   96/  179]
train() client id: f_00007-4-3 loss: 0.491725  [  128/  179]
train() client id: f_00007-4-4 loss: 0.365383  [  160/  179]
train() client id: f_00007-5-0 loss: 0.692112  [   32/  179]
train() client id: f_00007-5-1 loss: 0.431497  [   64/  179]
train() client id: f_00007-5-2 loss: 0.386515  [   96/  179]
train() client id: f_00007-5-3 loss: 0.509388  [  128/  179]
train() client id: f_00007-5-4 loss: 0.500213  [  160/  179]
train() client id: f_00007-6-0 loss: 0.361800  [   32/  179]
train() client id: f_00007-6-1 loss: 0.579938  [   64/  179]
train() client id: f_00007-6-2 loss: 0.472585  [   96/  179]
train() client id: f_00007-6-3 loss: 0.738395  [  128/  179]
train() client id: f_00007-6-4 loss: 0.556673  [  160/  179]
train() client id: f_00007-7-0 loss: 0.638971  [   32/  179]
train() client id: f_00007-7-1 loss: 0.488331  [   64/  179]
train() client id: f_00007-7-2 loss: 0.481787  [   96/  179]
train() client id: f_00007-7-3 loss: 0.449033  [  128/  179]
train() client id: f_00007-7-4 loss: 0.455145  [  160/  179]
train() client id: f_00007-8-0 loss: 0.489810  [   32/  179]
train() client id: f_00007-8-1 loss: 0.461988  [   64/  179]
train() client id: f_00007-8-2 loss: 0.788144  [   96/  179]
train() client id: f_00007-8-3 loss: 0.464781  [  128/  179]
train() client id: f_00007-8-4 loss: 0.365903  [  160/  179]
train() client id: f_00007-9-0 loss: 0.472924  [   32/  179]
train() client id: f_00007-9-1 loss: 0.532636  [   64/  179]
train() client id: f_00007-9-2 loss: 0.759975  [   96/  179]
train() client id: f_00007-9-3 loss: 0.352468  [  128/  179]
train() client id: f_00007-9-4 loss: 0.554039  [  160/  179]
train() client id: f_00007-10-0 loss: 0.735509  [   32/  179]
train() client id: f_00007-10-1 loss: 0.606082  [   64/  179]
train() client id: f_00007-10-2 loss: 0.374540  [   96/  179]
train() client id: f_00007-10-3 loss: 0.370289  [  128/  179]
train() client id: f_00007-10-4 loss: 0.580836  [  160/  179]
train() client id: f_00007-11-0 loss: 0.645423  [   32/  179]
train() client id: f_00007-11-1 loss: 0.516024  [   64/  179]
train() client id: f_00007-11-2 loss: 0.473407  [   96/  179]
train() client id: f_00007-11-3 loss: 0.535796  [  128/  179]
train() client id: f_00007-11-4 loss: 0.437958  [  160/  179]
train() client id: f_00008-0-0 loss: 0.760349  [   32/  130]
train() client id: f_00008-0-1 loss: 0.788932  [   64/  130]
train() client id: f_00008-0-2 loss: 0.723439  [   96/  130]
train() client id: f_00008-0-3 loss: 0.627640  [  128/  130]
train() client id: f_00008-1-0 loss: 0.664982  [   32/  130]
train() client id: f_00008-1-1 loss: 0.763108  [   64/  130]
train() client id: f_00008-1-2 loss: 0.735102  [   96/  130]
train() client id: f_00008-1-3 loss: 0.737742  [  128/  130]
train() client id: f_00008-2-0 loss: 0.710486  [   32/  130]
train() client id: f_00008-2-1 loss: 0.675294  [   64/  130]
train() client id: f_00008-2-2 loss: 0.821039  [   96/  130]
train() client id: f_00008-2-3 loss: 0.646256  [  128/  130]
train() client id: f_00008-3-0 loss: 0.759721  [   32/  130]
train() client id: f_00008-3-1 loss: 0.742795  [   64/  130]
train() client id: f_00008-3-2 loss: 0.725551  [   96/  130]
train() client id: f_00008-3-3 loss: 0.670128  [  128/  130]
train() client id: f_00008-4-0 loss: 0.678739  [   32/  130]
train() client id: f_00008-4-1 loss: 0.622965  [   64/  130]
train() client id: f_00008-4-2 loss: 0.731683  [   96/  130]
train() client id: f_00008-4-3 loss: 0.860761  [  128/  130]
train() client id: f_00008-5-0 loss: 0.779410  [   32/  130]
train() client id: f_00008-5-1 loss: 0.769950  [   64/  130]
train() client id: f_00008-5-2 loss: 0.654887  [   96/  130]
train() client id: f_00008-5-3 loss: 0.694162  [  128/  130]
train() client id: f_00008-6-0 loss: 0.804233  [   32/  130]
train() client id: f_00008-6-1 loss: 0.699312  [   64/  130]
train() client id: f_00008-6-2 loss: 0.704448  [   96/  130]
train() client id: f_00008-6-3 loss: 0.695923  [  128/  130]
train() client id: f_00008-7-0 loss: 0.837306  [   32/  130]
train() client id: f_00008-7-1 loss: 0.619003  [   64/  130]
train() client id: f_00008-7-2 loss: 0.744439  [   96/  130]
train() client id: f_00008-7-3 loss: 0.690914  [  128/  130]
train() client id: f_00008-8-0 loss: 0.725084  [   32/  130]
train() client id: f_00008-8-1 loss: 0.649389  [   64/  130]
train() client id: f_00008-8-2 loss: 0.703039  [   96/  130]
train() client id: f_00008-8-3 loss: 0.785515  [  128/  130]
train() client id: f_00008-9-0 loss: 0.665766  [   32/  130]
train() client id: f_00008-9-1 loss: 0.745317  [   64/  130]
train() client id: f_00008-9-2 loss: 0.670733  [   96/  130]
train() client id: f_00008-9-3 loss: 0.810714  [  128/  130]
train() client id: f_00008-10-0 loss: 0.676529  [   32/  130]
train() client id: f_00008-10-1 loss: 0.705405  [   64/  130]
train() client id: f_00008-10-2 loss: 0.737504  [   96/  130]
train() client id: f_00008-10-3 loss: 0.776744  [  128/  130]
train() client id: f_00008-11-0 loss: 0.773307  [   32/  130]
train() client id: f_00008-11-1 loss: 0.665916  [   64/  130]
train() client id: f_00008-11-2 loss: 0.787741  [   96/  130]
train() client id: f_00008-11-3 loss: 0.660570  [  128/  130]
train() client id: f_00009-0-0 loss: 1.129742  [   32/  118]
train() client id: f_00009-0-1 loss: 1.255653  [   64/  118]
train() client id: f_00009-0-2 loss: 1.215179  [   96/  118]
train() client id: f_00009-1-0 loss: 1.176161  [   32/  118]
train() client id: f_00009-1-1 loss: 1.245256  [   64/  118]
train() client id: f_00009-1-2 loss: 1.174805  [   96/  118]
train() client id: f_00009-2-0 loss: 0.932779  [   32/  118]
train() client id: f_00009-2-1 loss: 1.147617  [   64/  118]
train() client id: f_00009-2-2 loss: 1.126832  [   96/  118]
train() client id: f_00009-3-0 loss: 0.933538  [   32/  118]
train() client id: f_00009-3-1 loss: 1.092973  [   64/  118]
train() client id: f_00009-3-2 loss: 1.122075  [   96/  118]
train() client id: f_00009-4-0 loss: 1.019727  [   32/  118]
train() client id: f_00009-4-1 loss: 1.023766  [   64/  118]
train() client id: f_00009-4-2 loss: 0.985480  [   96/  118]
train() client id: f_00009-5-0 loss: 0.982918  [   32/  118]
train() client id: f_00009-5-1 loss: 0.915026  [   64/  118]
train() client id: f_00009-5-2 loss: 0.947482  [   96/  118]
train() client id: f_00009-6-0 loss: 1.095983  [   32/  118]
train() client id: f_00009-6-1 loss: 0.950523  [   64/  118]
train() client id: f_00009-6-2 loss: 0.748495  [   96/  118]
train() client id: f_00009-7-0 loss: 0.825407  [   32/  118]
train() client id: f_00009-7-1 loss: 0.984861  [   64/  118]
train() client id: f_00009-7-2 loss: 0.972270  [   96/  118]
train() client id: f_00009-8-0 loss: 0.921224  [   32/  118]
train() client id: f_00009-8-1 loss: 0.907638  [   64/  118]
train() client id: f_00009-8-2 loss: 0.983949  [   96/  118]
train() client id: f_00009-9-0 loss: 0.848193  [   32/  118]
train() client id: f_00009-9-1 loss: 0.975447  [   64/  118]
train() client id: f_00009-9-2 loss: 0.976796  [   96/  118]
train() client id: f_00009-10-0 loss: 0.835515  [   32/  118]
train() client id: f_00009-10-1 loss: 0.841676  [   64/  118]
train() client id: f_00009-10-2 loss: 1.065939  [   96/  118]
train() client id: f_00009-11-0 loss: 0.846759  [   32/  118]
train() client id: f_00009-11-1 loss: 0.936458  [   64/  118]
train() client id: f_00009-11-2 loss: 0.934039  [   96/  118]
At round 34 accuracy: 0.6525198938992043
At round 34 training accuracy: 0.5814889336016097
At round 34 training loss: 0.8386025510595937
update_location
xs = -4.528292 71.001589 80.045120 -80.943528 -20.103519 -5.217951 -122.215960 143.375741 -1.680116 64.695607 
ys = 157.587959 15.555839 1.320614 22.544824 9.350187 2.814151 -3.124922 -19.177652 -87.154970 4.001482 
xs mean: 12.442869159412867
ys mean: 10.371751218646876
dists_uav = 186.693520 123.625280 128.097483 130.614409 102.428402 100.175578 157.944629 175.853306 132.660512 119.170187 
uav_gains = -106.849136 -102.303007 -102.689049 -102.900476 -100.260532 -100.019063 -104.971557 -106.163325 -103.069408 -101.904385 
uav_gains_db_mean: -103.11299384060717
dists_bs = 171.358637 293.153923 308.565296 179.134588 226.787964 241.813794 185.781169 372.918319 314.269010 294.439259 
bs_gains = -102.116427 -108.645739 -109.268777 -102.656084 -105.524419 -106.304529 -103.099106 -111.572243 -109.491503 -108.698939 
bs_gains_db_mean: -106.7377767249751
Round 35
-------------------------------
ene_coms = [0.00878595 0.00978313 0.00709286 0.00716335 0.0081677  0.00852118
 0.00793394 0.00845502 0.01033062 0.00981594]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.35952303 13.23295822  6.23316531  2.23844013 15.22913966  7.34182866
  2.7936949   8.96869695  6.58211649  5.98171593]
obj_prev = 74.96127929024203
eta_min = 3.85093101210205e-15	eta_max = 0.9400278591965108
af = 15.812275796492862	bf = 1.3934454267095853	zeta = 17.39350337614215	eta = 0.9090909090909091
af = 15.812275796492862	bf = 1.3934454267095853	zeta = 31.90567903587784	eta = 0.49559439806035804
af = 15.812275796492862	bf = 1.3934454267095853	zeta = 24.76900016969837	eta = 0.6383897488053277
af = 15.812275796492862	bf = 1.3934454267095853	zeta = 23.478577404833082	eta = 0.6734767411094462
af = 15.812275796492862	bf = 1.3934454267095853	zeta = 23.41048176207774	eta = 0.6754357282004726
af = 15.812275796492862	bf = 1.3934454267095853	zeta = 23.41027645708461	eta = 0.6754416516814615
eta = 0.6754416516814615
ene_coms = [0.00878595 0.00978313 0.00709286 0.00716335 0.0081677  0.00852118
 0.00793394 0.00845502 0.01033062 0.00981594]
ene_comp = [0.03237896 0.0680986  0.03186501 0.01104996 0.07863461 0.03751848
 0.0138767  0.04599868 0.03340686 0.03032315]
ene_total = [2.05388008 3.88582714 1.94376184 0.90873418 4.33091045 2.29710014
 1.08821904 2.71691026 2.18223553 2.00269779]
ti_comp = [0.47833843 0.46836671 0.49526937 0.49456444 0.48452095 0.4809862
 0.48685852 0.48164775 0.46289181 0.46803858]
ti_coms = [0.08785953 0.09783125 0.07092859 0.07163352 0.08167701 0.08521176
 0.07933944 0.08455021 0.10330615 0.09815938]
t_total = [28.24985313 28.24985313 28.24985313 28.24985313 28.24985313 28.24985313
 28.24985313 28.24985313 28.24985313 28.24985313]
ene_coms = [0.00878595 0.00978313 0.00709286 0.00716335 0.0081677  0.00852118
 0.00793394 0.00845502 0.01033062 0.00981594]
ene_comp = [9.27252699e-06 8.99751545e-05 8.24402175e-06 3.44759789e-07
 1.29448250e-04 1.42675813e-05 7.04583960e-07 2.62214704e-05
 1.08749703e-05 7.95499190e-06]
ene_total = [0.43882855 0.49260797 0.35430207 0.35742512 0.41397758 0.42586702
 0.39589098 0.4231627  0.51597779 0.49015286]
optimize_network iter = 0 obj = 4.308192642579049
eta = 0.6754416516814615
freqs = [33845240.86151915 72697954.66667533 32169369.61363224 11171407.76245485
 81146764.37001187 39001613.9805776  14251262.01485988 47751369.74474764
 36084957.03643823 32393859.62855104]
eta_min = 0.6754416516814641	eta_max = 0.6769979972965541
af = 0.012269629623897611	bf = 1.3934454267095853	zeta = 0.013496592586287373	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00878595 0.00978313 0.00709286 0.00716335 0.0081677  0.00852118
 0.00793394 0.00845502 0.01033062 0.00981594]
ene_comp = [2.25227347e-06 2.18547386e-05 2.00245213e-06 8.37412850e-08
 3.14426541e-05 3.46555960e-06 1.71141670e-07 6.36912915e-06
 2.64150292e-06 1.93224751e-06]
ene_total = [1.56542966 1.74654615 1.26379685 1.27601188 1.46050108 1.5184815
 1.41329192 1.5072146  1.84064728 1.7488424 ]
ti_comp = [0.47562336 0.46565164 0.4925543  0.49184937 0.48180587 0.47827112
 0.48414345 0.47893268 0.46017674 0.4653235 ]
ti_coms = [0.08785953 0.09783125 0.07092859 0.07163352 0.08167701 0.08521176
 0.07933944 0.08455021 0.10330615 0.09815938]
t_total = [28.24985313 28.24985313 28.24985313 28.24985313 28.24985313 28.24985313
 28.24985313 28.24985313 28.24985313 28.24985313]
ene_coms = [0.00878595 0.00978313 0.00709286 0.00716335 0.0081677  0.00852118
 0.00793394 0.00845502 0.01033062 0.00981594]
ene_comp = [9.21462776e-06 8.94350721e-05 8.18934815e-06 3.42478755e-07
 1.28621215e-04 1.41776011e-05 7.00044544e-07 2.60556951e-05
 1.08111836e-05 7.90730613e-06]
ene_total = [0.44094009 0.49495447 0.35600649 0.35914722 0.41593081 0.42791449
 0.3977983  0.42519335 0.51846077 0.49251221]
optimize_network iter = 1 obj = 4.328858205511795
eta = 0.6769979972965541
freqs = [33838793.67933559 72692941.1868681  32156966.38793324 11167188.25257666
 81125397.52770592 38992959.23402298 14247124.02229961 47740401.1188276
 36084957.03643823 32391758.05732122]
eta_min = 0.676997997296559	eta_max = 0.6769979972965541
af = 0.01226506786463439	bf = 1.3934454267095853	zeta = 0.01349157465109783	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00878595 0.00978313 0.00709286 0.00716335 0.0081677  0.00852118
 0.00793394 0.00845502 0.01033062 0.00981594]
ene_comp = [2.25141548e-06 2.18517243e-05 2.00090830e-06 8.36780377e-08
 3.14260979e-05 3.46402170e-06 1.71042300e-07 6.36620348e-06
 2.64150292e-06 1.93199681e-06]
ene_total = [1.56542951 1.74654561 1.26379658 1.27601187 1.46049813 1.51848123
 1.4132919  1.50721407 1.84064728 1.74884235]
ti_comp = [0.47562336 0.46565164 0.4925543  0.49184937 0.48180587 0.47827112
 0.48414345 0.47893268 0.46017674 0.4653235 ]
ti_coms = [0.08785953 0.09783125 0.07092859 0.07163352 0.08167701 0.08521176
 0.07933944 0.08455021 0.10330615 0.09815938]
t_total = [28.24985313 28.24985313 28.24985313 28.24985313 28.24985313 28.24985313
 28.24985313 28.24985313 28.24985313 28.24985313]
ene_coms = [0.00878595 0.00978313 0.00709286 0.00716335 0.0081677  0.00852118
 0.00793394 0.00845502 0.01033062 0.00981594]
ene_comp = [9.21462776e-06 8.94350721e-05 8.18934815e-06 3.42478755e-07
 1.28621215e-04 1.41776011e-05 7.00044544e-07 2.60556951e-05
 1.08111836e-05 7.90730613e-06]
ene_total = [0.44094009 0.49495447 0.35600649 0.35914722 0.41593081 0.42791449
 0.3977983  0.42519335 0.51846077 0.49251221]
optimize_network iter = 2 obj = 4.328858205511795
eta = 0.6769979972965541
freqs = [33838793.67933559 72692941.1868681  32156966.38793324 11167188.25257666
 81125397.52770592 38992959.23402298 14247124.02229961 47740401.1188276
 36084957.03643823 32391758.05732122]
Done!
ene_coms = [0.00878595 0.00978313 0.00709286 0.00716335 0.0081677  0.00852118
 0.00793394 0.00845502 0.01033062 0.00981594]
ene_comp = [8.65668352e-06 8.40197928e-05 7.69348443e-06 3.21741721e-07
 1.20833221e-04 1.33191496e-05 6.57656959e-07 2.44780270e-05
 1.01565682e-05 7.42852001e-06]
ene_total = [0.00879461 0.00986714 0.00710055 0.00716367 0.00828853 0.0085345
 0.0079346  0.0084795  0.01034077 0.00982337]
At round 35 energy consumption: 0.08632725024608096
At round 35 eta: 0.6769979972965541
At round 35 a_n: 16.193498212328272
At round 35 local rounds: 12.773429089888872
At round 35 global rounds: 50.13435853893395
gradient difference: 0.46642598509788513
train() client id: f_00000-0-0 loss: 1.222897  [   32/  126]
train() client id: f_00000-0-1 loss: 0.884673  [   64/  126]
train() client id: f_00000-0-2 loss: 0.961362  [   96/  126]
train() client id: f_00000-1-0 loss: 1.080402  [   32/  126]
train() client id: f_00000-1-1 loss: 0.995339  [   64/  126]
train() client id: f_00000-1-2 loss: 0.837770  [   96/  126]
train() client id: f_00000-2-0 loss: 0.935284  [   32/  126]
train() client id: f_00000-2-1 loss: 0.984513  [   64/  126]
train() client id: f_00000-2-2 loss: 0.978854  [   96/  126]
train() client id: f_00000-3-0 loss: 0.949082  [   32/  126]
train() client id: f_00000-3-1 loss: 0.862115  [   64/  126]
train() client id: f_00000-3-2 loss: 0.793976  [   96/  126]
train() client id: f_00000-4-0 loss: 0.691911  [   32/  126]
train() client id: f_00000-4-1 loss: 0.864829  [   64/  126]
train() client id: f_00000-4-2 loss: 0.872884  [   96/  126]
train() client id: f_00000-5-0 loss: 0.791630  [   32/  126]
train() client id: f_00000-5-1 loss: 0.698770  [   64/  126]
train() client id: f_00000-5-2 loss: 0.864012  [   96/  126]
train() client id: f_00000-6-0 loss: 0.759048  [   32/  126]
train() client id: f_00000-6-1 loss: 0.877159  [   64/  126]
train() client id: f_00000-6-2 loss: 0.797581  [   96/  126]
train() client id: f_00000-7-0 loss: 0.866637  [   32/  126]
train() client id: f_00000-7-1 loss: 0.748492  [   64/  126]
train() client id: f_00000-7-2 loss: 0.788203  [   96/  126]
train() client id: f_00000-8-0 loss: 0.922009  [   32/  126]
train() client id: f_00000-8-1 loss: 0.741025  [   64/  126]
train() client id: f_00000-8-2 loss: 0.661014  [   96/  126]
train() client id: f_00000-9-0 loss: 0.722529  [   32/  126]
train() client id: f_00000-9-1 loss: 0.819041  [   64/  126]
train() client id: f_00000-9-2 loss: 0.871831  [   96/  126]
train() client id: f_00000-10-0 loss: 0.756595  [   32/  126]
train() client id: f_00000-10-1 loss: 0.807113  [   64/  126]
train() client id: f_00000-10-2 loss: 0.758048  [   96/  126]
train() client id: f_00000-11-0 loss: 0.694413  [   32/  126]
train() client id: f_00000-11-1 loss: 0.743087  [   64/  126]
train() client id: f_00000-11-2 loss: 0.797082  [   96/  126]
train() client id: f_00001-0-0 loss: 0.445594  [   32/  265]
train() client id: f_00001-0-1 loss: 0.566710  [   64/  265]
train() client id: f_00001-0-2 loss: 0.440252  [   96/  265]
train() client id: f_00001-0-3 loss: 0.464680  [  128/  265]
train() client id: f_00001-0-4 loss: 0.542443  [  160/  265]
train() client id: f_00001-0-5 loss: 0.559528  [  192/  265]
train() client id: f_00001-0-6 loss: 0.602813  [  224/  265]
train() client id: f_00001-0-7 loss: 0.475366  [  256/  265]
train() client id: f_00001-1-0 loss: 0.467070  [   32/  265]
train() client id: f_00001-1-1 loss: 0.508780  [   64/  265]
train() client id: f_00001-1-2 loss: 0.628340  [   96/  265]
train() client id: f_00001-1-3 loss: 0.626473  [  128/  265]
train() client id: f_00001-1-4 loss: 0.410525  [  160/  265]
train() client id: f_00001-1-5 loss: 0.540991  [  192/  265]
train() client id: f_00001-1-6 loss: 0.406037  [  224/  265]
train() client id: f_00001-1-7 loss: 0.462371  [  256/  265]
train() client id: f_00001-2-0 loss: 0.471844  [   32/  265]
train() client id: f_00001-2-1 loss: 0.555262  [   64/  265]
train() client id: f_00001-2-2 loss: 0.517752  [   96/  265]
train() client id: f_00001-2-3 loss: 0.495649  [  128/  265]
train() client id: f_00001-2-4 loss: 0.602422  [  160/  265]
train() client id: f_00001-2-5 loss: 0.389372  [  192/  265]
train() client id: f_00001-2-6 loss: 0.482226  [  224/  265]
train() client id: f_00001-2-7 loss: 0.506660  [  256/  265]
train() client id: f_00001-3-0 loss: 0.484412  [   32/  265]
train() client id: f_00001-3-1 loss: 0.502902  [   64/  265]
train() client id: f_00001-3-2 loss: 0.445774  [   96/  265]
train() client id: f_00001-3-3 loss: 0.533447  [  128/  265]
train() client id: f_00001-3-4 loss: 0.438435  [  160/  265]
train() client id: f_00001-3-5 loss: 0.468508  [  192/  265]
train() client id: f_00001-3-6 loss: 0.561727  [  224/  265]
train() client id: f_00001-3-7 loss: 0.468802  [  256/  265]
train() client id: f_00001-4-0 loss: 0.458292  [   32/  265]
train() client id: f_00001-4-1 loss: 0.697913  [   64/  265]
train() client id: f_00001-4-2 loss: 0.584022  [   96/  265]
train() client id: f_00001-4-3 loss: 0.441584  [  128/  265]
train() client id: f_00001-4-4 loss: 0.409877  [  160/  265]
train() client id: f_00001-4-5 loss: 0.410313  [  192/  265]
train() client id: f_00001-4-6 loss: 0.466373  [  224/  265]
train() client id: f_00001-4-7 loss: 0.479912  [  256/  265]
train() client id: f_00001-5-0 loss: 0.407116  [   32/  265]
train() client id: f_00001-5-1 loss: 0.476906  [   64/  265]
train() client id: f_00001-5-2 loss: 0.569879  [   96/  265]
train() client id: f_00001-5-3 loss: 0.528192  [  128/  265]
train() client id: f_00001-5-4 loss: 0.480320  [  160/  265]
train() client id: f_00001-5-5 loss: 0.514767  [  192/  265]
train() client id: f_00001-5-6 loss: 0.431151  [  224/  265]
train() client id: f_00001-5-7 loss: 0.536032  [  256/  265]
train() client id: f_00001-6-0 loss: 0.384489  [   32/  265]
train() client id: f_00001-6-1 loss: 0.601652  [   64/  265]
train() client id: f_00001-6-2 loss: 0.451873  [   96/  265]
train() client id: f_00001-6-3 loss: 0.479634  [  128/  265]
train() client id: f_00001-6-4 loss: 0.582043  [  160/  265]
train() client id: f_00001-6-5 loss: 0.584284  [  192/  265]
train() client id: f_00001-6-6 loss: 0.462084  [  224/  265]
train() client id: f_00001-6-7 loss: 0.400203  [  256/  265]
train() client id: f_00001-7-0 loss: 0.454683  [   32/  265]
train() client id: f_00001-7-1 loss: 0.417071  [   64/  265]
train() client id: f_00001-7-2 loss: 0.658438  [   96/  265]
train() client id: f_00001-7-3 loss: 0.569069  [  128/  265]
train() client id: f_00001-7-4 loss: 0.389570  [  160/  265]
train() client id: f_00001-7-5 loss: 0.401997  [  192/  265]
train() client id: f_00001-7-6 loss: 0.549439  [  224/  265]
train() client id: f_00001-7-7 loss: 0.509390  [  256/  265]
train() client id: f_00001-8-0 loss: 0.401670  [   32/  265]
train() client id: f_00001-8-1 loss: 0.604869  [   64/  265]
train() client id: f_00001-8-2 loss: 0.438465  [   96/  265]
train() client id: f_00001-8-3 loss: 0.614832  [  128/  265]
train() client id: f_00001-8-4 loss: 0.479593  [  160/  265]
train() client id: f_00001-8-5 loss: 0.566707  [  192/  265]
train() client id: f_00001-8-6 loss: 0.394420  [  224/  265]
train() client id: f_00001-8-7 loss: 0.450476  [  256/  265]
train() client id: f_00001-9-0 loss: 0.396854  [   32/  265]
train() client id: f_00001-9-1 loss: 0.465464  [   64/  265]
train() client id: f_00001-9-2 loss: 0.396536  [   96/  265]
train() client id: f_00001-9-3 loss: 0.618878  [  128/  265]
train() client id: f_00001-9-4 loss: 0.472671  [  160/  265]
train() client id: f_00001-9-5 loss: 0.540010  [  192/  265]
train() client id: f_00001-9-6 loss: 0.458271  [  224/  265]
train() client id: f_00001-9-7 loss: 0.604059  [  256/  265]
train() client id: f_00001-10-0 loss: 0.403890  [   32/  265]
train() client id: f_00001-10-1 loss: 0.554632  [   64/  265]
train() client id: f_00001-10-2 loss: 0.465372  [   96/  265]
train() client id: f_00001-10-3 loss: 0.628588  [  128/  265]
train() client id: f_00001-10-4 loss: 0.433186  [  160/  265]
train() client id: f_00001-10-5 loss: 0.409922  [  192/  265]
train() client id: f_00001-10-6 loss: 0.535557  [  224/  265]
train() client id: f_00001-10-7 loss: 0.520550  [  256/  265]
train() client id: f_00001-11-0 loss: 0.440188  [   32/  265]
train() client id: f_00001-11-1 loss: 0.488382  [   64/  265]
train() client id: f_00001-11-2 loss: 0.487855  [   96/  265]
train() client id: f_00001-11-3 loss: 0.562224  [  128/  265]
train() client id: f_00001-11-4 loss: 0.606377  [  160/  265]
train() client id: f_00001-11-5 loss: 0.403832  [  192/  265]
train() client id: f_00001-11-6 loss: 0.433146  [  224/  265]
train() client id: f_00001-11-7 loss: 0.445139  [  256/  265]
train() client id: f_00002-0-0 loss: 1.152810  [   32/  124]
train() client id: f_00002-0-1 loss: 1.180431  [   64/  124]
train() client id: f_00002-0-2 loss: 1.245991  [   96/  124]
train() client id: f_00002-1-0 loss: 1.139171  [   32/  124]
train() client id: f_00002-1-1 loss: 1.108460  [   64/  124]
train() client id: f_00002-1-2 loss: 1.094828  [   96/  124]
train() client id: f_00002-2-0 loss: 1.187891  [   32/  124]
train() client id: f_00002-2-1 loss: 1.164965  [   64/  124]
train() client id: f_00002-2-2 loss: 0.886057  [   96/  124]
train() client id: f_00002-3-0 loss: 1.086248  [   32/  124]
train() client id: f_00002-3-1 loss: 1.054114  [   64/  124]
train() client id: f_00002-3-2 loss: 1.177239  [   96/  124]
train() client id: f_00002-4-0 loss: 1.074496  [   32/  124]
train() client id: f_00002-4-1 loss: 1.095949  [   64/  124]
train() client id: f_00002-4-2 loss: 1.071564  [   96/  124]
train() client id: f_00002-5-0 loss: 0.951393  [   32/  124]
train() client id: f_00002-5-1 loss: 1.345358  [   64/  124]
train() client id: f_00002-5-2 loss: 0.940193  [   96/  124]
train() client id: f_00002-6-0 loss: 1.080153  [   32/  124]
train() client id: f_00002-6-1 loss: 1.056763  [   64/  124]
train() client id: f_00002-6-2 loss: 0.848996  [   96/  124]
train() client id: f_00002-7-0 loss: 1.017928  [   32/  124]
train() client id: f_00002-7-1 loss: 1.151493  [   64/  124]
train() client id: f_00002-7-2 loss: 0.944230  [   96/  124]
train() client id: f_00002-8-0 loss: 1.187333  [   32/  124]
train() client id: f_00002-8-1 loss: 0.935252  [   64/  124]
train() client id: f_00002-8-2 loss: 0.916282  [   96/  124]
train() client id: f_00002-9-0 loss: 0.889434  [   32/  124]
train() client id: f_00002-9-1 loss: 1.116734  [   64/  124]
train() client id: f_00002-9-2 loss: 1.056085  [   96/  124]
train() client id: f_00002-10-0 loss: 1.023976  [   32/  124]
train() client id: f_00002-10-1 loss: 1.100472  [   64/  124]
train() client id: f_00002-10-2 loss: 0.965046  [   96/  124]
train() client id: f_00002-11-0 loss: 0.894318  [   32/  124]
train() client id: f_00002-11-1 loss: 0.996808  [   64/  124]
train() client id: f_00002-11-2 loss: 1.133708  [   96/  124]
train() client id: f_00003-0-0 loss: 0.769187  [   32/   43]
train() client id: f_00003-1-0 loss: 0.629265  [   32/   43]
train() client id: f_00003-2-0 loss: 0.925914  [   32/   43]
train() client id: f_00003-3-0 loss: 0.690748  [   32/   43]
train() client id: f_00003-4-0 loss: 0.755552  [   32/   43]
train() client id: f_00003-5-0 loss: 0.681584  [   32/   43]
train() client id: f_00003-6-0 loss: 0.878414  [   32/   43]
train() client id: f_00003-7-0 loss: 0.635085  [   32/   43]
train() client id: f_00003-8-0 loss: 0.791705  [   32/   43]
train() client id: f_00003-9-0 loss: 0.580370  [   32/   43]
train() client id: f_00003-10-0 loss: 0.569880  [   32/   43]
train() client id: f_00003-11-0 loss: 0.903133  [   32/   43]
train() client id: f_00004-0-0 loss: 0.819038  [   32/  306]
train() client id: f_00004-0-1 loss: 0.750416  [   64/  306]
train() client id: f_00004-0-2 loss: 0.878336  [   96/  306]
train() client id: f_00004-0-3 loss: 0.914552  [  128/  306]
train() client id: f_00004-0-4 loss: 0.758052  [  160/  306]
train() client id: f_00004-0-5 loss: 1.049758  [  192/  306]
train() client id: f_00004-0-6 loss: 0.926340  [  224/  306]
train() client id: f_00004-0-7 loss: 0.819132  [  256/  306]
train() client id: f_00004-0-8 loss: 0.779220  [  288/  306]
train() client id: f_00004-1-0 loss: 0.953071  [   32/  306]
train() client id: f_00004-1-1 loss: 0.999018  [   64/  306]
train() client id: f_00004-1-2 loss: 0.673076  [   96/  306]
train() client id: f_00004-1-3 loss: 0.790942  [  128/  306]
train() client id: f_00004-1-4 loss: 0.782769  [  160/  306]
train() client id: f_00004-1-5 loss: 0.926912  [  192/  306]
train() client id: f_00004-1-6 loss: 0.784810  [  224/  306]
train() client id: f_00004-1-7 loss: 0.883556  [  256/  306]
train() client id: f_00004-1-8 loss: 0.842796  [  288/  306]
train() client id: f_00004-2-0 loss: 0.831781  [   32/  306]
train() client id: f_00004-2-1 loss: 0.951258  [   64/  306]
train() client id: f_00004-2-2 loss: 0.976759  [   96/  306]
train() client id: f_00004-2-3 loss: 0.770971  [  128/  306]
train() client id: f_00004-2-4 loss: 0.899418  [  160/  306]
train() client id: f_00004-2-5 loss: 0.902572  [  192/  306]
train() client id: f_00004-2-6 loss: 0.773569  [  224/  306]
train() client id: f_00004-2-7 loss: 0.826146  [  256/  306]
train() client id: f_00004-2-8 loss: 0.782484  [  288/  306]
train() client id: f_00004-3-0 loss: 0.931734  [   32/  306]
train() client id: f_00004-3-1 loss: 0.859025  [   64/  306]
train() client id: f_00004-3-2 loss: 0.811227  [   96/  306]
train() client id: f_00004-3-3 loss: 0.859034  [  128/  306]
train() client id: f_00004-3-4 loss: 0.861348  [  160/  306]
train() client id: f_00004-3-5 loss: 0.917093  [  192/  306]
train() client id: f_00004-3-6 loss: 0.858956  [  224/  306]
train() client id: f_00004-3-7 loss: 0.776084  [  256/  306]
train() client id: f_00004-3-8 loss: 0.808324  [  288/  306]
train() client id: f_00004-4-0 loss: 0.988767  [   32/  306]
train() client id: f_00004-4-1 loss: 0.917445  [   64/  306]
train() client id: f_00004-4-2 loss: 0.798120  [   96/  306]
train() client id: f_00004-4-3 loss: 0.961468  [  128/  306]
train() client id: f_00004-4-4 loss: 0.740907  [  160/  306]
train() client id: f_00004-4-5 loss: 0.820433  [  192/  306]
train() client id: f_00004-4-6 loss: 0.918883  [  224/  306]
train() client id: f_00004-4-7 loss: 0.885404  [  256/  306]
train() client id: f_00004-4-8 loss: 0.692706  [  288/  306]
train() client id: f_00004-5-0 loss: 0.847840  [   32/  306]
train() client id: f_00004-5-1 loss: 0.876839  [   64/  306]
train() client id: f_00004-5-2 loss: 0.839563  [   96/  306]
train() client id: f_00004-5-3 loss: 0.696005  [  128/  306]
train() client id: f_00004-5-4 loss: 0.814233  [  160/  306]
train() client id: f_00004-5-5 loss: 0.969970  [  192/  306]
train() client id: f_00004-5-6 loss: 0.817612  [  224/  306]
train() client id: f_00004-5-7 loss: 0.860974  [  256/  306]
train() client id: f_00004-5-8 loss: 0.935083  [  288/  306]
train() client id: f_00004-6-0 loss: 0.783661  [   32/  306]
train() client id: f_00004-6-1 loss: 0.844960  [   64/  306]
train() client id: f_00004-6-2 loss: 0.863924  [   96/  306]
train() client id: f_00004-6-3 loss: 0.882461  [  128/  306]
train() client id: f_00004-6-4 loss: 0.829336  [  160/  306]
train() client id: f_00004-6-5 loss: 0.875199  [  192/  306]
train() client id: f_00004-6-6 loss: 0.888741  [  224/  306]
train() client id: f_00004-6-7 loss: 0.804511  [  256/  306]
train() client id: f_00004-6-8 loss: 0.957671  [  288/  306]
train() client id: f_00004-7-0 loss: 0.836285  [   32/  306]
train() client id: f_00004-7-1 loss: 0.757352  [   64/  306]
train() client id: f_00004-7-2 loss: 0.906363  [   96/  306]
train() client id: f_00004-7-3 loss: 0.948196  [  128/  306]
train() client id: f_00004-7-4 loss: 0.841889  [  160/  306]
train() client id: f_00004-7-5 loss: 0.866996  [  192/  306]
train() client id: f_00004-7-6 loss: 0.762553  [  224/  306]
train() client id: f_00004-7-7 loss: 0.835217  [  256/  306]
train() client id: f_00004-7-8 loss: 0.900820  [  288/  306]
train() client id: f_00004-8-0 loss: 0.985046  [   32/  306]
train() client id: f_00004-8-1 loss: 0.946528  [   64/  306]
train() client id: f_00004-8-2 loss: 0.826783  [   96/  306]
train() client id: f_00004-8-3 loss: 0.930002  [  128/  306]
train() client id: f_00004-8-4 loss: 0.762340  [  160/  306]
train() client id: f_00004-8-5 loss: 0.760573  [  192/  306]
train() client id: f_00004-8-6 loss: 0.740799  [  224/  306]
train() client id: f_00004-8-7 loss: 0.843248  [  256/  306]
train() client id: f_00004-8-8 loss: 0.897515  [  288/  306]
train() client id: f_00004-9-0 loss: 0.859590  [   32/  306]
train() client id: f_00004-9-1 loss: 0.766765  [   64/  306]
train() client id: f_00004-9-2 loss: 0.823913  [   96/  306]
train() client id: f_00004-9-3 loss: 0.801487  [  128/  306]
train() client id: f_00004-9-4 loss: 0.750659  [  160/  306]
train() client id: f_00004-9-5 loss: 0.857333  [  192/  306]
train() client id: f_00004-9-6 loss: 0.955738  [  224/  306]
train() client id: f_00004-9-7 loss: 0.982955  [  256/  306]
train() client id: f_00004-9-8 loss: 0.969265  [  288/  306]
train() client id: f_00004-10-0 loss: 0.994771  [   32/  306]
train() client id: f_00004-10-1 loss: 0.839854  [   64/  306]
train() client id: f_00004-10-2 loss: 0.828306  [   96/  306]
train() client id: f_00004-10-3 loss: 0.819842  [  128/  306]
train() client id: f_00004-10-4 loss: 0.830118  [  160/  306]
train() client id: f_00004-10-5 loss: 0.930665  [  192/  306]
train() client id: f_00004-10-6 loss: 0.920285  [  224/  306]
train() client id: f_00004-10-7 loss: 0.728449  [  256/  306]
train() client id: f_00004-10-8 loss: 0.898380  [  288/  306]
train() client id: f_00004-11-0 loss: 0.806564  [   32/  306]
train() client id: f_00004-11-1 loss: 0.796073  [   64/  306]
train() client id: f_00004-11-2 loss: 0.795802  [   96/  306]
train() client id: f_00004-11-3 loss: 0.903519  [  128/  306]
train() client id: f_00004-11-4 loss: 0.864606  [  160/  306]
train() client id: f_00004-11-5 loss: 0.966913  [  192/  306]
train() client id: f_00004-11-6 loss: 0.793707  [  224/  306]
train() client id: f_00004-11-7 loss: 0.882493  [  256/  306]
train() client id: f_00004-11-8 loss: 0.902401  [  288/  306]
train() client id: f_00005-0-0 loss: 0.396840  [   32/  146]
train() client id: f_00005-0-1 loss: 0.591865  [   64/  146]
train() client id: f_00005-0-2 loss: 0.473047  [   96/  146]
train() client id: f_00005-0-3 loss: 0.338725  [  128/  146]
train() client id: f_00005-1-0 loss: 0.316864  [   32/  146]
train() client id: f_00005-1-1 loss: 0.546859  [   64/  146]
train() client id: f_00005-1-2 loss: 0.607696  [   96/  146]
train() client id: f_00005-1-3 loss: 0.309337  [  128/  146]
train() client id: f_00005-2-0 loss: 0.510375  [   32/  146]
train() client id: f_00005-2-1 loss: 0.486156  [   64/  146]
train() client id: f_00005-2-2 loss: 0.431920  [   96/  146]
train() client id: f_00005-2-3 loss: 0.186336  [  128/  146]
train() client id: f_00005-3-0 loss: 0.278156  [   32/  146]
train() client id: f_00005-3-1 loss: 0.540049  [   64/  146]
train() client id: f_00005-3-2 loss: 0.406261  [   96/  146]
train() client id: f_00005-3-3 loss: 0.364929  [  128/  146]
train() client id: f_00005-4-0 loss: 0.446378  [   32/  146]
train() client id: f_00005-4-1 loss: 0.565386  [   64/  146]
train() client id: f_00005-4-2 loss: 0.305900  [   96/  146]
train() client id: f_00005-4-3 loss: 0.371027  [  128/  146]
train() client id: f_00005-5-0 loss: 0.421023  [   32/  146]
train() client id: f_00005-5-1 loss: 0.350940  [   64/  146]
train() client id: f_00005-5-2 loss: 0.384681  [   96/  146]
train() client id: f_00005-5-3 loss: 0.525040  [  128/  146]
train() client id: f_00005-6-0 loss: 0.478705  [   32/  146]
train() client id: f_00005-6-1 loss: 0.455109  [   64/  146]
train() client id: f_00005-6-2 loss: 0.342475  [   96/  146]
train() client id: f_00005-6-3 loss: 0.415109  [  128/  146]
train() client id: f_00005-7-0 loss: 0.373821  [   32/  146]
train() client id: f_00005-7-1 loss: 0.512836  [   64/  146]
train() client id: f_00005-7-2 loss: 0.355340  [   96/  146]
train() client id: f_00005-7-3 loss: 0.607486  [  128/  146]
train() client id: f_00005-8-0 loss: 0.244429  [   32/  146]
train() client id: f_00005-8-1 loss: 0.445674  [   64/  146]
train() client id: f_00005-8-2 loss: 0.440518  [   96/  146]
train() client id: f_00005-8-3 loss: 0.642774  [  128/  146]
train() client id: f_00005-9-0 loss: 0.128720  [   32/  146]
train() client id: f_00005-9-1 loss: 0.494503  [   64/  146]
train() client id: f_00005-9-2 loss: 0.355236  [   96/  146]
train() client id: f_00005-9-3 loss: 0.662106  [  128/  146]
train() client id: f_00005-10-0 loss: 0.406437  [   32/  146]
train() client id: f_00005-10-1 loss: 0.450651  [   64/  146]
train() client id: f_00005-10-2 loss: 0.443705  [   96/  146]
train() client id: f_00005-10-3 loss: 0.386907  [  128/  146]
train() client id: f_00005-11-0 loss: 0.254208  [   32/  146]
train() client id: f_00005-11-1 loss: 0.601059  [   64/  146]
train() client id: f_00005-11-2 loss: 0.328256  [   96/  146]
train() client id: f_00005-11-3 loss: 0.471075  [  128/  146]
train() client id: f_00006-0-0 loss: 0.520958  [   32/   54]
train() client id: f_00006-1-0 loss: 0.514975  [   32/   54]
train() client id: f_00006-2-0 loss: 0.486862  [   32/   54]
train() client id: f_00006-3-0 loss: 0.530254  [   32/   54]
train() client id: f_00006-4-0 loss: 0.489548  [   32/   54]
train() client id: f_00006-5-0 loss: 0.522468  [   32/   54]
train() client id: f_00006-6-0 loss: 0.581125  [   32/   54]
train() client id: f_00006-7-0 loss: 0.575489  [   32/   54]
train() client id: f_00006-8-0 loss: 0.567687  [   32/   54]
train() client id: f_00006-9-0 loss: 0.561233  [   32/   54]
train() client id: f_00006-10-0 loss: 0.515887  [   32/   54]
train() client id: f_00006-11-0 loss: 0.583086  [   32/   54]
train() client id: f_00007-0-0 loss: 0.625986  [   32/  179]
train() client id: f_00007-0-1 loss: 0.703887  [   64/  179]
train() client id: f_00007-0-2 loss: 0.534752  [   96/  179]
train() client id: f_00007-0-3 loss: 0.591510  [  128/  179]
train() client id: f_00007-0-4 loss: 0.587763  [  160/  179]
train() client id: f_00007-1-0 loss: 0.632977  [   32/  179]
train() client id: f_00007-1-1 loss: 0.828983  [   64/  179]
train() client id: f_00007-1-2 loss: 0.472655  [   96/  179]
train() client id: f_00007-1-3 loss: 0.557659  [  128/  179]
train() client id: f_00007-1-4 loss: 0.652503  [  160/  179]
train() client id: f_00007-2-0 loss: 0.805123  [   32/  179]
train() client id: f_00007-2-1 loss: 0.521411  [   64/  179]
train() client id: f_00007-2-2 loss: 0.618655  [   96/  179]
train() client id: f_00007-2-3 loss: 0.550237  [  128/  179]
train() client id: f_00007-2-4 loss: 0.599094  [  160/  179]
train() client id: f_00007-3-0 loss: 0.691807  [   32/  179]
train() client id: f_00007-3-1 loss: 0.549242  [   64/  179]
train() client id: f_00007-3-2 loss: 0.651826  [   96/  179]
train() client id: f_00007-3-3 loss: 0.550415  [  128/  179]
train() client id: f_00007-3-4 loss: 0.610089  [  160/  179]
train() client id: f_00007-4-0 loss: 0.590378  [   32/  179]
train() client id: f_00007-4-1 loss: 0.687527  [   64/  179]
train() client id: f_00007-4-2 loss: 0.437019  [   96/  179]
train() client id: f_00007-4-3 loss: 0.773841  [  128/  179]
train() client id: f_00007-4-4 loss: 0.530067  [  160/  179]
train() client id: f_00007-5-0 loss: 0.474281  [   32/  179]
train() client id: f_00007-5-1 loss: 0.718657  [   64/  179]
train() client id: f_00007-5-2 loss: 0.571976  [   96/  179]
train() client id: f_00007-5-3 loss: 0.704988  [  128/  179]
train() client id: f_00007-5-4 loss: 0.548661  [  160/  179]
train() client id: f_00007-6-0 loss: 0.439124  [   32/  179]
train() client id: f_00007-6-1 loss: 0.586659  [   64/  179]
train() client id: f_00007-6-2 loss: 0.524770  [   96/  179]
train() client id: f_00007-6-3 loss: 0.734857  [  128/  179]
train() client id: f_00007-6-4 loss: 0.523951  [  160/  179]
train() client id: f_00007-7-0 loss: 0.613508  [   32/  179]
train() client id: f_00007-7-1 loss: 0.493743  [   64/  179]
train() client id: f_00007-7-2 loss: 0.440499  [   96/  179]
train() client id: f_00007-7-3 loss: 0.731270  [  128/  179]
train() client id: f_00007-7-4 loss: 0.527017  [  160/  179]
train() client id: f_00007-8-0 loss: 0.585801  [   32/  179]
train() client id: f_00007-8-1 loss: 0.451375  [   64/  179]
train() client id: f_00007-8-2 loss: 0.534839  [   96/  179]
train() client id: f_00007-8-3 loss: 0.492867  [  128/  179]
train() client id: f_00007-8-4 loss: 0.693288  [  160/  179]
train() client id: f_00007-9-0 loss: 0.588913  [   32/  179]
train() client id: f_00007-9-1 loss: 0.624628  [   64/  179]
train() client id: f_00007-9-2 loss: 0.661638  [   96/  179]
train() client id: f_00007-9-3 loss: 0.533112  [  128/  179]
train() client id: f_00007-9-4 loss: 0.424247  [  160/  179]
train() client id: f_00007-10-0 loss: 0.493183  [   32/  179]
train() client id: f_00007-10-1 loss: 0.712280  [   64/  179]
train() client id: f_00007-10-2 loss: 0.515310  [   96/  179]
train() client id: f_00007-10-3 loss: 0.513962  [  128/  179]
train() client id: f_00007-10-4 loss: 0.530072  [  160/  179]
train() client id: f_00007-11-0 loss: 0.598792  [   32/  179]
train() client id: f_00007-11-1 loss: 0.888215  [   64/  179]
train() client id: f_00007-11-2 loss: 0.458687  [   96/  179]
train() client id: f_00007-11-3 loss: 0.473977  [  128/  179]
train() client id: f_00007-11-4 loss: 0.511776  [  160/  179]
train() client id: f_00008-0-0 loss: 0.730722  [   32/  130]
train() client id: f_00008-0-1 loss: 0.720793  [   64/  130]
train() client id: f_00008-0-2 loss: 0.758715  [   96/  130]
train() client id: f_00008-0-3 loss: 0.912213  [  128/  130]
train() client id: f_00008-1-0 loss: 0.857333  [   32/  130]
train() client id: f_00008-1-1 loss: 0.695164  [   64/  130]
train() client id: f_00008-1-2 loss: 0.813586  [   96/  130]
train() client id: f_00008-1-3 loss: 0.751376  [  128/  130]
train() client id: f_00008-2-0 loss: 0.673610  [   32/  130]
train() client id: f_00008-2-1 loss: 0.927126  [   64/  130]
train() client id: f_00008-2-2 loss: 0.764557  [   96/  130]
train() client id: f_00008-2-3 loss: 0.753012  [  128/  130]
train() client id: f_00008-3-0 loss: 0.882572  [   32/  130]
train() client id: f_00008-3-1 loss: 0.684602  [   64/  130]
train() client id: f_00008-3-2 loss: 0.744269  [   96/  130]
train() client id: f_00008-3-3 loss: 0.776237  [  128/  130]
train() client id: f_00008-4-0 loss: 0.796036  [   32/  130]
train() client id: f_00008-4-1 loss: 0.810372  [   64/  130]
train() client id: f_00008-4-2 loss: 0.762020  [   96/  130]
train() client id: f_00008-4-3 loss: 0.699836  [  128/  130]
train() client id: f_00008-5-0 loss: 0.833395  [   32/  130]
train() client id: f_00008-5-1 loss: 0.623683  [   64/  130]
train() client id: f_00008-5-2 loss: 0.885510  [   96/  130]
train() client id: f_00008-5-3 loss: 0.766532  [  128/  130]
train() client id: f_00008-6-0 loss: 0.823594  [   32/  130]
train() client id: f_00008-6-1 loss: 0.810713  [   64/  130]
train() client id: f_00008-6-2 loss: 0.764471  [   96/  130]
train() client id: f_00008-6-3 loss: 0.714145  [  128/  130]
train() client id: f_00008-7-0 loss: 0.763904  [   32/  130]
train() client id: f_00008-7-1 loss: 0.768167  [   64/  130]
train() client id: f_00008-7-2 loss: 0.833047  [   96/  130]
train() client id: f_00008-7-3 loss: 0.743520  [  128/  130]
train() client id: f_00008-8-0 loss: 0.800258  [   32/  130]
train() client id: f_00008-8-1 loss: 0.865373  [   64/  130]
train() client id: f_00008-8-2 loss: 0.690336  [   96/  130]
train() client id: f_00008-8-3 loss: 0.716313  [  128/  130]
train() client id: f_00008-9-0 loss: 0.754462  [   32/  130]
train() client id: f_00008-9-1 loss: 0.667207  [   64/  130]
train() client id: f_00008-9-2 loss: 0.897285  [   96/  130]
train() client id: f_00008-9-3 loss: 0.785852  [  128/  130]
train() client id: f_00008-10-0 loss: 0.745458  [   32/  130]
train() client id: f_00008-10-1 loss: 0.848498  [   64/  130]
train() client id: f_00008-10-2 loss: 0.691765  [   96/  130]
train() client id: f_00008-10-3 loss: 0.811516  [  128/  130]
train() client id: f_00008-11-0 loss: 0.727166  [   32/  130]
train() client id: f_00008-11-1 loss: 0.766394  [   64/  130]
train() client id: f_00008-11-2 loss: 0.784801  [   96/  130]
train() client id: f_00008-11-3 loss: 0.834241  [  128/  130]
train() client id: f_00009-0-0 loss: 0.864032  [   32/  118]
train() client id: f_00009-0-1 loss: 1.022797  [   64/  118]
train() client id: f_00009-0-2 loss: 0.927291  [   96/  118]
train() client id: f_00009-1-0 loss: 1.020475  [   32/  118]
train() client id: f_00009-1-1 loss: 0.859283  [   64/  118]
train() client id: f_00009-1-2 loss: 0.907923  [   96/  118]
train() client id: f_00009-2-0 loss: 0.950651  [   32/  118]
train() client id: f_00009-2-1 loss: 0.867455  [   64/  118]
train() client id: f_00009-2-2 loss: 0.787677  [   96/  118]
train() client id: f_00009-3-0 loss: 0.751060  [   32/  118]
train() client id: f_00009-3-1 loss: 0.895987  [   64/  118]
train() client id: f_00009-3-2 loss: 0.902494  [   96/  118]
train() client id: f_00009-4-0 loss: 0.889331  [   32/  118]
train() client id: f_00009-4-1 loss: 0.680936  [   64/  118]
train() client id: f_00009-4-2 loss: 0.878339  [   96/  118]
train() client id: f_00009-5-0 loss: 0.763417  [   32/  118]
train() client id: f_00009-5-1 loss: 0.742805  [   64/  118]
train() client id: f_00009-5-2 loss: 0.817380  [   96/  118]
train() client id: f_00009-6-0 loss: 0.848838  [   32/  118]
train() client id: f_00009-6-1 loss: 0.765482  [   64/  118]
train() client id: f_00009-6-2 loss: 0.684016  [   96/  118]
train() client id: f_00009-7-0 loss: 0.639342  [   32/  118]
train() client id: f_00009-7-1 loss: 0.912904  [   64/  118]
train() client id: f_00009-7-2 loss: 0.787609  [   96/  118]
train() client id: f_00009-8-0 loss: 0.870556  [   32/  118]
train() client id: f_00009-8-1 loss: 0.627406  [   64/  118]
train() client id: f_00009-8-2 loss: 0.857560  [   96/  118]
train() client id: f_00009-9-0 loss: 0.751257  [   32/  118]
train() client id: f_00009-9-1 loss: 0.685297  [   64/  118]
train() client id: f_00009-9-2 loss: 0.807437  [   96/  118]
train() client id: f_00009-10-0 loss: 0.831126  [   32/  118]
train() client id: f_00009-10-1 loss: 0.807774  [   64/  118]
train() client id: f_00009-10-2 loss: 0.578719  [   96/  118]
train() client id: f_00009-11-0 loss: 0.956669  [   32/  118]
train() client id: f_00009-11-1 loss: 0.657092  [   64/  118]
train() client id: f_00009-11-2 loss: 0.648180  [   96/  118]
At round 35 accuracy: 0.649867374005305
At round 35 training accuracy: 0.5855130784708249
At round 35 training loss: 0.8244407085368272
update_location
xs = -4.528292 76.001589 85.045120 -85.943528 -15.103519 -10.217951 -127.215960 148.375741 -1.680116 69.695607 
ys = 162.587959 15.555839 1.320614 22.544824 9.350187 2.814151 -3.124922 -19.177652 -92.154970 4.001482 
xs mean: 13.442869159412869
ys mean: 10.371751218646876
dists_uav = 190.932842 126.563129 131.279916 133.770547 101.565458 100.560061 161.844572 179.953168 135.997652 121.956917 
uav_gains = -107.114190 -102.558132 -102.955707 -103.159984 -100.168671 -100.060656 -105.239680 -106.424721 -103.339503 -102.155430 
uav_gains_db_mean: -103.31766734868933
dists_bs = 170.922971 297.362133 312.710718 176.560573 230.231938 238.329793 184.422890 377.196010 318.451818 298.523757 
bs_gains = -102.085471 -108.819058 -109.431057 -102.480084 -105.707695 -106.128053 -103.009874 -111.710938 -109.652284 -108.866468 
bs_gains_db_mean: -106.78909806508426
Round 36
-------------------------------
ene_coms = [0.00892062 0.0098908  0.00718199 0.00725178 0.00824817 0.00843865
 0.00804566 0.00857828 0.01044128 0.00992064]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.22715439 12.95476152  6.10274047  2.19250552 14.90828154  7.18520346
  2.73638781  8.78095295  6.44465483  5.85685938]
obj_prev = 73.38950186892095
eta_min = 1.9637427443500896e-15	eta_max = 0.9405773515343955
af = 15.47779405979816	bf = 1.377731101185679	zeta = 17.025573465777978	eta = 0.9090909090909091
af = 15.47779405979816	bf = 1.377731101185679	zeta = 31.382146795992888	eta = 0.49320380025035393
af = 15.47779405979816	bf = 1.377731101185679	zeta = 24.305517006591355	eta = 0.6368016798655538
af = 15.47779405979816	bf = 1.377731101185679	zeta = 23.02547331199929	eta = 0.672203079175454
af = 15.47779405979816	bf = 1.377731101185679	zeta = 22.957598669675313	eta = 0.6741904622735119
af = 15.47779405979816	bf = 1.377731101185679	zeta = 22.95739186146446	eta = 0.6741965356168654
eta = 0.6741965356168654
ene_coms = [0.00892062 0.0098908  0.00718199 0.00725178 0.00824817 0.00843865
 0.00804566 0.00857828 0.01044128 0.00992064]
ene_comp = [0.03253121 0.06841882 0.03201484 0.01110192 0.07900437 0.0376949
 0.01394195 0.04621498 0.03356395 0.03046574]
ene_total = [2.01670997 3.80991054 1.90700002 0.89294201 4.24500059 2.24448402
 1.0697388  2.66579515 2.14093711 1.96487364]
ti_comp = [0.49041877 0.48071703 0.50780508 0.50710725 0.49714333 0.49523851
 0.49916837 0.49384222 0.47521226 0.48041857]
ti_coms = [0.08920624 0.09890799 0.07181994 0.07251777 0.08248168 0.0843865
 0.08045665 0.08578279 0.10441275 0.09920644]
t_total = [28.19984894 28.19984894 28.19984894 28.19984894 28.19984894 28.19984894
 28.19984894 28.19984894 28.19984894 28.19984894]
ene_coms = [0.00892062 0.0098908  0.00718199 0.00725178 0.00824817 0.00843865
 0.00804566 0.00857828 0.01044128 0.00992064]
ene_comp = [8.94636508e-06 8.66219123e-05 7.95316750e-06 3.32563732e-07
 1.24701076e-04 1.36489393e-05 6.79761119e-07 2.52959974e-05
 1.04646313e-05 7.65729892e-06]
ene_total = [0.43444043 0.48542032 0.34980446 0.35282878 0.40735588 0.41122028
 0.39146982 0.41858014 0.50849683 0.48303058]
optimize_network iter = 0 obj = 4.2426475071592025
eta = 0.6741965356168654
freqs = [33166769.73750473 71163301.19806692 31522769.17165682 10946325.41534601
 79458347.06496707 38057316.58240967 13965176.2177651  46791237.18926334
 35314692.61636025 31707495.29411647]
eta_min = 0.6741965356168852	eta_max = 0.6809183700232875
af = 0.01151133397555619	bf = 1.377731101185679	zeta = 0.01266246737311181	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00892062 0.0098908  0.00718199 0.00725178 0.00824817 0.00843865
 0.00804566 0.00857828 0.01044128 0.00992064]
ene_comp = [2.16287915e-06 2.09417709e-05 1.92276304e-06 8.04008282e-08
 3.01478146e-05 3.29977660e-06 1.64339498e-07 6.11557709e-06
 2.52993621e-06 1.85123366e-06]
ene_total = [1.5557814  1.72821581 1.25259107 1.26443725 1.44341117 1.47194243
 1.40287457 1.49677914 1.82098684 1.73009105]
ti_comp = [0.4784602  0.46875846 0.49584651 0.49514868 0.48518476 0.48327994
 0.4872098  0.48188365 0.46325369 0.46846   ]
ti_coms = [0.08920624 0.09890799 0.07181994 0.07251777 0.08248168 0.0843865
 0.08045665 0.08578279 0.10441275 0.09920644]
t_total = [28.19984894 28.19984894 28.19984894 28.19984894 28.19984894 28.19984894
 28.19984894 28.19984894 28.19984894 28.19984894]
ene_coms = [0.00892062 0.0098908  0.00718199 0.00725178 0.00824817 0.00843865
 0.00804566 0.00857828 0.01044128 0.00992064]
ene_comp = [8.70728786e-06 8.43921980e-05 7.72740088e-06 3.23144692e-07
 1.21286618e-04 1.32777331e-05 6.61016286e-07 2.46114732e-05
 1.02012921e-05 7.46043066e-06]
ene_total = [0.44358056 0.49553551 0.35716229 0.36026107 0.4157677  0.41986469
 0.39971566 0.42736402 0.51919584 0.49319641]
optimize_network iter = 1 obj = 4.331643745474295
eta = 0.6809183700232875
freqs = [33140243.32542806 71142271.29602909 31470626.82359112 10928581.61146151
 79367964.69210051 38017631.19726177 13947896.43749068 46745714.71319178
 35314692.61636023 31698627.59489733]
eta_min = 0.6809183700232898	eta_max = 0.6809183700232673
af = 0.011492838773653266	bf = 1.377731101185679	zeta = 0.012642122651018594	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00892062 0.0098908  0.00718199 0.00725178 0.00824817 0.00843865
 0.00804566 0.00857828 0.01044128 0.00992064]
ene_comp = [2.15942084e-06 2.09293954e-05 1.91640735e-06 8.01403827e-08
 3.00792685e-05 3.29289831e-06 1.63933060e-07 6.10368337e-06
 2.52993621e-06 1.85019833e-06]
ene_total = [1.55578079 1.72821365 1.25258996 1.26443721 1.44339922 1.47194123
 1.4028745  1.49677707 1.82098684 1.73009087]
ti_comp = [0.4784602  0.46875846 0.49584651 0.49514868 0.48518476 0.48327994
 0.4872098  0.48188365 0.46325369 0.46846   ]
ti_coms = [0.08920624 0.09890799 0.07181994 0.07251777 0.08248168 0.0843865
 0.08045665 0.08578279 0.10441275 0.09920644]
t_total = [28.19984894 28.19984894 28.19984894 28.19984894 28.19984894 28.19984894
 28.19984894 28.19984894 28.19984894 28.19984894]
ene_coms = [0.00892062 0.0098908  0.00718199 0.00725178 0.00824817 0.00843865
 0.00804566 0.00857828 0.01044128 0.00992064]
ene_comp = [8.70728786e-06 8.43921980e-05 7.72740088e-06 3.23144692e-07
 1.21286618e-04 1.32777331e-05 6.61016286e-07 2.46114732e-05
 1.02012921e-05 7.46043066e-06]
ene_total = [0.44358056 0.49553551 0.35716229 0.36026107 0.4157677  0.41986469
 0.39971566 0.42736402 0.51919584 0.49319641]
optimize_network iter = 2 obj = 4.331643745474022
eta = 0.6809183700232673
freqs = [33140243.32542812 71142271.29602912 31470626.82359127 10928581.61146155
 79367964.69210076 38017631.19726186 13947896.43749072 46745714.71319189
 35314692.61636022 31698627.59489733]
Done!
ene_coms = [0.00892062 0.0098908  0.00718199 0.00725178 0.00824817 0.00843865
 0.00804566 0.00857828 0.01044128 0.00992064]
ene_comp = [8.30296450e-06 8.04734420e-05 7.36857862e-06 3.08139452e-07
 1.15654667e-04 1.26611809e-05 6.30321961e-07 2.34686382e-05
 9.72759461e-06 7.11400518e-06]
ene_total = [0.00892893 0.00997127 0.00718936 0.00725208 0.00836382 0.00845131
 0.00804629 0.00860175 0.010451   0.00992776]
At round 36 energy consumption: 0.08718358483945514
At round 36 eta: 0.6809183700232673
At round 36 a_n: 15.850952365358953
At round 36 local rounds: 12.584355181198754
At round 36 global rounds: 49.67679388661389
gradient difference: 0.500270426273346
train() client id: f_00000-0-0 loss: 1.414136  [   32/  126]
train() client id: f_00000-0-1 loss: 1.021176  [   64/  126]
train() client id: f_00000-0-2 loss: 0.953356  [   96/  126]
train() client id: f_00000-1-0 loss: 1.046897  [   32/  126]
train() client id: f_00000-1-1 loss: 0.950777  [   64/  126]
train() client id: f_00000-1-2 loss: 0.998730  [   96/  126]
train() client id: f_00000-2-0 loss: 0.840918  [   32/  126]
train() client id: f_00000-2-1 loss: 1.048043  [   64/  126]
train() client id: f_00000-2-2 loss: 0.904592  [   96/  126]
train() client id: f_00000-3-0 loss: 0.859265  [   32/  126]
train() client id: f_00000-3-1 loss: 0.954320  [   64/  126]
train() client id: f_00000-3-2 loss: 0.923685  [   96/  126]
train() client id: f_00000-4-0 loss: 0.901267  [   32/  126]
train() client id: f_00000-4-1 loss: 0.833772  [   64/  126]
train() client id: f_00000-4-2 loss: 0.909434  [   96/  126]
train() client id: f_00000-5-0 loss: 0.889611  [   32/  126]
train() client id: f_00000-5-1 loss: 1.047565  [   64/  126]
train() client id: f_00000-5-2 loss: 0.761527  [   96/  126]
train() client id: f_00000-6-0 loss: 0.811628  [   32/  126]
train() client id: f_00000-6-1 loss: 0.836627  [   64/  126]
train() client id: f_00000-6-2 loss: 0.813161  [   96/  126]
train() client id: f_00000-7-0 loss: 0.799154  [   32/  126]
train() client id: f_00000-7-1 loss: 0.877965  [   64/  126]
train() client id: f_00000-7-2 loss: 0.706303  [   96/  126]
train() client id: f_00000-8-0 loss: 0.887770  [   32/  126]
train() client id: f_00000-8-1 loss: 0.808828  [   64/  126]
train() client id: f_00000-8-2 loss: 0.751793  [   96/  126]
train() client id: f_00000-9-0 loss: 0.896828  [   32/  126]
train() client id: f_00000-9-1 loss: 0.816278  [   64/  126]
train() client id: f_00000-9-2 loss: 0.765139  [   96/  126]
train() client id: f_00000-10-0 loss: 0.743925  [   32/  126]
train() client id: f_00000-10-1 loss: 0.768909  [   64/  126]
train() client id: f_00000-10-2 loss: 0.813659  [   96/  126]
train() client id: f_00000-11-0 loss: 0.673305  [   32/  126]
train() client id: f_00000-11-1 loss: 0.982734  [   64/  126]
train() client id: f_00000-11-2 loss: 0.718353  [   96/  126]
train() client id: f_00001-0-0 loss: 0.459469  [   32/  265]
train() client id: f_00001-0-1 loss: 0.440977  [   64/  265]
train() client id: f_00001-0-2 loss: 0.636106  [   96/  265]
train() client id: f_00001-0-3 loss: 0.402925  [  128/  265]
train() client id: f_00001-0-4 loss: 0.470659  [  160/  265]
train() client id: f_00001-0-5 loss: 0.494463  [  192/  265]
train() client id: f_00001-0-6 loss: 0.482128  [  224/  265]
train() client id: f_00001-0-7 loss: 0.526679  [  256/  265]
train() client id: f_00001-1-0 loss: 0.540787  [   32/  265]
train() client id: f_00001-1-1 loss: 0.402500  [   64/  265]
train() client id: f_00001-1-2 loss: 0.594500  [   96/  265]
train() client id: f_00001-1-3 loss: 0.467897  [  128/  265]
train() client id: f_00001-1-4 loss: 0.379315  [  160/  265]
train() client id: f_00001-1-5 loss: 0.448266  [  192/  265]
train() client id: f_00001-1-6 loss: 0.476677  [  224/  265]
train() client id: f_00001-1-7 loss: 0.573619  [  256/  265]
train() client id: f_00001-2-0 loss: 0.458205  [   32/  265]
train() client id: f_00001-2-1 loss: 0.494359  [   64/  265]
train() client id: f_00001-2-2 loss: 0.488592  [   96/  265]
train() client id: f_00001-2-3 loss: 0.493339  [  128/  265]
train() client id: f_00001-2-4 loss: 0.485816  [  160/  265]
train() client id: f_00001-2-5 loss: 0.503824  [  192/  265]
train() client id: f_00001-2-6 loss: 0.388314  [  224/  265]
train() client id: f_00001-2-7 loss: 0.465338  [  256/  265]
train() client id: f_00001-3-0 loss: 0.434012  [   32/  265]
train() client id: f_00001-3-1 loss: 0.452080  [   64/  265]
train() client id: f_00001-3-2 loss: 0.530769  [   96/  265]
train() client id: f_00001-3-3 loss: 0.593919  [  128/  265]
train() client id: f_00001-3-4 loss: 0.405996  [  160/  265]
train() client id: f_00001-3-5 loss: 0.377075  [  192/  265]
train() client id: f_00001-3-6 loss: 0.592926  [  224/  265]
train() client id: f_00001-3-7 loss: 0.410203  [  256/  265]
train() client id: f_00001-4-0 loss: 0.441977  [   32/  265]
train() client id: f_00001-4-1 loss: 0.675056  [   64/  265]
train() client id: f_00001-4-2 loss: 0.473633  [   96/  265]
train() client id: f_00001-4-3 loss: 0.438460  [  128/  265]
train() client id: f_00001-4-4 loss: 0.445111  [  160/  265]
train() client id: f_00001-4-5 loss: 0.530338  [  192/  265]
train() client id: f_00001-4-6 loss: 0.363376  [  224/  265]
train() client id: f_00001-4-7 loss: 0.395767  [  256/  265]
train() client id: f_00001-5-0 loss: 0.682326  [   32/  265]
train() client id: f_00001-5-1 loss: 0.368824  [   64/  265]
train() client id: f_00001-5-2 loss: 0.370146  [   96/  265]
train() client id: f_00001-5-3 loss: 0.480577  [  128/  265]
train() client id: f_00001-5-4 loss: 0.586916  [  160/  265]
train() client id: f_00001-5-5 loss: 0.417636  [  192/  265]
train() client id: f_00001-5-6 loss: 0.382864  [  224/  265]
train() client id: f_00001-5-7 loss: 0.467017  [  256/  265]
train() client id: f_00001-6-0 loss: 0.454638  [   32/  265]
train() client id: f_00001-6-1 loss: 0.511194  [   64/  265]
train() client id: f_00001-6-2 loss: 0.414252  [   96/  265]
train() client id: f_00001-6-3 loss: 0.510690  [  128/  265]
train() client id: f_00001-6-4 loss: 0.489019  [  160/  265]
train() client id: f_00001-6-5 loss: 0.456749  [  192/  265]
train() client id: f_00001-6-6 loss: 0.444964  [  224/  265]
train() client id: f_00001-6-7 loss: 0.477328  [  256/  265]
train() client id: f_00001-7-0 loss: 0.390269  [   32/  265]
train() client id: f_00001-7-1 loss: 0.501382  [   64/  265]
train() client id: f_00001-7-2 loss: 0.496917  [   96/  265]
train() client id: f_00001-7-3 loss: 0.470490  [  128/  265]
train() client id: f_00001-7-4 loss: 0.431926  [  160/  265]
train() client id: f_00001-7-5 loss: 0.382361  [  192/  265]
train() client id: f_00001-7-6 loss: 0.468411  [  224/  265]
train() client id: f_00001-7-7 loss: 0.550505  [  256/  265]
train() client id: f_00001-8-0 loss: 0.546426  [   32/  265]
train() client id: f_00001-8-1 loss: 0.379054  [   64/  265]
train() client id: f_00001-8-2 loss: 0.433401  [   96/  265]
train() client id: f_00001-8-3 loss: 0.555638  [  128/  265]
train() client id: f_00001-8-4 loss: 0.422661  [  160/  265]
train() client id: f_00001-8-5 loss: 0.363337  [  192/  265]
train() client id: f_00001-8-6 loss: 0.359958  [  224/  265]
train() client id: f_00001-8-7 loss: 0.593251  [  256/  265]
train() client id: f_00001-9-0 loss: 0.439092  [   32/  265]
train() client id: f_00001-9-1 loss: 0.454484  [   64/  265]
train() client id: f_00001-9-2 loss: 0.473011  [   96/  265]
train() client id: f_00001-9-3 loss: 0.450460  [  128/  265]
train() client id: f_00001-9-4 loss: 0.507470  [  160/  265]
train() client id: f_00001-9-5 loss: 0.575569  [  192/  265]
train() client id: f_00001-9-6 loss: 0.365998  [  224/  265]
train() client id: f_00001-9-7 loss: 0.474620  [  256/  265]
train() client id: f_00001-10-0 loss: 0.397694  [   32/  265]
train() client id: f_00001-10-1 loss: 0.578641  [   64/  265]
train() client id: f_00001-10-2 loss: 0.392899  [   96/  265]
train() client id: f_00001-10-3 loss: 0.450967  [  128/  265]
train() client id: f_00001-10-4 loss: 0.447723  [  160/  265]
train() client id: f_00001-10-5 loss: 0.429046  [  192/  265]
train() client id: f_00001-10-6 loss: 0.667680  [  224/  265]
train() client id: f_00001-10-7 loss: 0.374247  [  256/  265]
train() client id: f_00001-11-0 loss: 0.515352  [   32/  265]
train() client id: f_00001-11-1 loss: 0.574615  [   64/  265]
train() client id: f_00001-11-2 loss: 0.414147  [   96/  265]
train() client id: f_00001-11-3 loss: 0.459697  [  128/  265]
train() client id: f_00001-11-4 loss: 0.406418  [  160/  265]
train() client id: f_00001-11-5 loss: 0.471651  [  192/  265]
train() client id: f_00001-11-6 loss: 0.386151  [  224/  265]
train() client id: f_00001-11-7 loss: 0.518000  [  256/  265]
train() client id: f_00002-0-0 loss: 1.054120  [   32/  124]
train() client id: f_00002-0-1 loss: 1.190939  [   64/  124]
train() client id: f_00002-0-2 loss: 0.940648  [   96/  124]
train() client id: f_00002-1-0 loss: 1.008979  [   32/  124]
train() client id: f_00002-1-1 loss: 1.126142  [   64/  124]
train() client id: f_00002-1-2 loss: 0.991742  [   96/  124]
train() client id: f_00002-2-0 loss: 0.865438  [   32/  124]
train() client id: f_00002-2-1 loss: 0.984166  [   64/  124]
train() client id: f_00002-2-2 loss: 1.003700  [   96/  124]
train() client id: f_00002-3-0 loss: 0.932221  [   32/  124]
train() client id: f_00002-3-1 loss: 0.782591  [   64/  124]
train() client id: f_00002-3-2 loss: 0.994729  [   96/  124]
train() client id: f_00002-4-0 loss: 0.739140  [   32/  124]
train() client id: f_00002-4-1 loss: 0.895377  [   64/  124]
train() client id: f_00002-4-2 loss: 1.048233  [   96/  124]
train() client id: f_00002-5-0 loss: 0.980316  [   32/  124]
train() client id: f_00002-5-1 loss: 0.858909  [   64/  124]
train() client id: f_00002-5-2 loss: 0.813187  [   96/  124]
train() client id: f_00002-6-0 loss: 0.776274  [   32/  124]
train() client id: f_00002-6-1 loss: 1.002799  [   64/  124]
train() client id: f_00002-6-2 loss: 0.743453  [   96/  124]
train() client id: f_00002-7-0 loss: 0.690561  [   32/  124]
train() client id: f_00002-7-1 loss: 0.779534  [   64/  124]
train() client id: f_00002-7-2 loss: 1.007740  [   96/  124]
train() client id: f_00002-8-0 loss: 0.843732  [   32/  124]
train() client id: f_00002-8-1 loss: 0.675828  [   64/  124]
train() client id: f_00002-8-2 loss: 0.997212  [   96/  124]
train() client id: f_00002-9-0 loss: 0.871519  [   32/  124]
train() client id: f_00002-9-1 loss: 0.692386  [   64/  124]
train() client id: f_00002-9-2 loss: 1.081501  [   96/  124]
train() client id: f_00002-10-0 loss: 0.820055  [   32/  124]
train() client id: f_00002-10-1 loss: 0.953182  [   64/  124]
train() client id: f_00002-10-2 loss: 0.692458  [   96/  124]
train() client id: f_00002-11-0 loss: 0.736331  [   32/  124]
train() client id: f_00002-11-1 loss: 0.775024  [   64/  124]
train() client id: f_00002-11-2 loss: 0.718399  [   96/  124]
train() client id: f_00003-0-0 loss: 0.490133  [   32/   43]
train() client id: f_00003-1-0 loss: 0.618541  [   32/   43]
train() client id: f_00003-2-0 loss: 0.523680  [   32/   43]
train() client id: f_00003-3-0 loss: 0.453043  [   32/   43]
train() client id: f_00003-4-0 loss: 0.329055  [   32/   43]
train() client id: f_00003-5-0 loss: 0.613711  [   32/   43]
train() client id: f_00003-6-0 loss: 0.411432  [   32/   43]
train() client id: f_00003-7-0 loss: 0.574526  [   32/   43]
train() client id: f_00003-8-0 loss: 0.456954  [   32/   43]
train() client id: f_00003-9-0 loss: 0.414856  [   32/   43]
train() client id: f_00003-10-0 loss: 0.626466  [   32/   43]
train() client id: f_00003-11-0 loss: 0.671693  [   32/   43]
train() client id: f_00004-0-0 loss: 0.769922  [   32/  306]
train() client id: f_00004-0-1 loss: 0.891717  [   64/  306]
train() client id: f_00004-0-2 loss: 0.682613  [   96/  306]
train() client id: f_00004-0-3 loss: 0.715223  [  128/  306]
train() client id: f_00004-0-4 loss: 0.810249  [  160/  306]
train() client id: f_00004-0-5 loss: 0.880297  [  192/  306]
train() client id: f_00004-0-6 loss: 0.818444  [  224/  306]
train() client id: f_00004-0-7 loss: 0.656329  [  256/  306]
train() client id: f_00004-0-8 loss: 0.906817  [  288/  306]
train() client id: f_00004-1-0 loss: 0.656181  [   32/  306]
train() client id: f_00004-1-1 loss: 0.850300  [   64/  306]
train() client id: f_00004-1-2 loss: 0.826490  [   96/  306]
train() client id: f_00004-1-3 loss: 0.670798  [  128/  306]
train() client id: f_00004-1-4 loss: 0.891869  [  160/  306]
train() client id: f_00004-1-5 loss: 0.763929  [  192/  306]
train() client id: f_00004-1-6 loss: 0.883711  [  224/  306]
train() client id: f_00004-1-7 loss: 0.778441  [  256/  306]
train() client id: f_00004-1-8 loss: 0.732271  [  288/  306]
train() client id: f_00004-2-0 loss: 0.744191  [   32/  306]
train() client id: f_00004-2-1 loss: 0.877909  [   64/  306]
train() client id: f_00004-2-2 loss: 0.817279  [   96/  306]
train() client id: f_00004-2-3 loss: 0.771406  [  128/  306]
train() client id: f_00004-2-4 loss: 0.861512  [  160/  306]
train() client id: f_00004-2-5 loss: 0.721932  [  192/  306]
train() client id: f_00004-2-6 loss: 0.782685  [  224/  306]
train() client id: f_00004-2-7 loss: 0.797583  [  256/  306]
train() client id: f_00004-2-8 loss: 0.738166  [  288/  306]
train() client id: f_00004-3-0 loss: 0.835161  [   32/  306]
train() client id: f_00004-3-1 loss: 0.904774  [   64/  306]
train() client id: f_00004-3-2 loss: 0.799837  [   96/  306]
train() client id: f_00004-3-3 loss: 0.847566  [  128/  306]
train() client id: f_00004-3-4 loss: 0.590019  [  160/  306]
train() client id: f_00004-3-5 loss: 0.738787  [  192/  306]
train() client id: f_00004-3-6 loss: 0.732343  [  224/  306]
train() client id: f_00004-3-7 loss: 0.742326  [  256/  306]
train() client id: f_00004-3-8 loss: 0.899387  [  288/  306]
train() client id: f_00004-4-0 loss: 0.837125  [   32/  306]
train() client id: f_00004-4-1 loss: 0.755136  [   64/  306]
train() client id: f_00004-4-2 loss: 0.789104  [   96/  306]
train() client id: f_00004-4-3 loss: 0.897598  [  128/  306]
train() client id: f_00004-4-4 loss: 0.755003  [  160/  306]
train() client id: f_00004-4-5 loss: 0.879277  [  192/  306]
train() client id: f_00004-4-6 loss: 0.824672  [  224/  306]
train() client id: f_00004-4-7 loss: 0.757986  [  256/  306]
train() client id: f_00004-4-8 loss: 0.603187  [  288/  306]
train() client id: f_00004-5-0 loss: 0.858067  [   32/  306]
train() client id: f_00004-5-1 loss: 0.830401  [   64/  306]
train() client id: f_00004-5-2 loss: 0.828555  [   96/  306]
train() client id: f_00004-5-3 loss: 0.830158  [  128/  306]
train() client id: f_00004-5-4 loss: 0.692560  [  160/  306]
train() client id: f_00004-5-5 loss: 0.818447  [  192/  306]
train() client id: f_00004-5-6 loss: 0.835310  [  224/  306]
train() client id: f_00004-5-7 loss: 0.715999  [  256/  306]
train() client id: f_00004-5-8 loss: 0.730876  [  288/  306]
train() client id: f_00004-6-0 loss: 0.795393  [   32/  306]
train() client id: f_00004-6-1 loss: 0.748130  [   64/  306]
train() client id: f_00004-6-2 loss: 0.866053  [   96/  306]
train() client id: f_00004-6-3 loss: 0.819089  [  128/  306]
train() client id: f_00004-6-4 loss: 0.778763  [  160/  306]
train() client id: f_00004-6-5 loss: 0.665162  [  192/  306]
train() client id: f_00004-6-6 loss: 0.764052  [  224/  306]
train() client id: f_00004-6-7 loss: 0.885230  [  256/  306]
train() client id: f_00004-6-8 loss: 0.821570  [  288/  306]
train() client id: f_00004-7-0 loss: 0.848604  [   32/  306]
train() client id: f_00004-7-1 loss: 0.730415  [   64/  306]
train() client id: f_00004-7-2 loss: 0.791047  [   96/  306]
train() client id: f_00004-7-3 loss: 0.846382  [  128/  306]
train() client id: f_00004-7-4 loss: 0.744113  [  160/  306]
train() client id: f_00004-7-5 loss: 0.825887  [  192/  306]
train() client id: f_00004-7-6 loss: 0.834568  [  224/  306]
train() client id: f_00004-7-7 loss: 0.757158  [  256/  306]
train() client id: f_00004-7-8 loss: 0.772446  [  288/  306]
train() client id: f_00004-8-0 loss: 0.821821  [   32/  306]
train() client id: f_00004-8-1 loss: 0.839614  [   64/  306]
train() client id: f_00004-8-2 loss: 0.773974  [   96/  306]
train() client id: f_00004-8-3 loss: 0.737561  [  128/  306]
train() client id: f_00004-8-4 loss: 0.849381  [  160/  306]
train() client id: f_00004-8-5 loss: 0.769000  [  192/  306]
train() client id: f_00004-8-6 loss: 0.882563  [  224/  306]
train() client id: f_00004-8-7 loss: 0.750282  [  256/  306]
train() client id: f_00004-8-8 loss: 0.858822  [  288/  306]
train() client id: f_00004-9-0 loss: 0.744330  [   32/  306]
train() client id: f_00004-9-1 loss: 0.869691  [   64/  306]
train() client id: f_00004-9-2 loss: 0.839114  [   96/  306]
train() client id: f_00004-9-3 loss: 0.991689  [  128/  306]
train() client id: f_00004-9-4 loss: 0.714783  [  160/  306]
train() client id: f_00004-9-5 loss: 0.771555  [  192/  306]
train() client id: f_00004-9-6 loss: 0.751934  [  224/  306]
train() client id: f_00004-9-7 loss: 0.733983  [  256/  306]
train() client id: f_00004-9-8 loss: 0.792039  [  288/  306]
train() client id: f_00004-10-0 loss: 0.761605  [   32/  306]
train() client id: f_00004-10-1 loss: 0.780751  [   64/  306]
train() client id: f_00004-10-2 loss: 0.777637  [   96/  306]
train() client id: f_00004-10-3 loss: 0.749739  [  128/  306]
train() client id: f_00004-10-4 loss: 0.889217  [  160/  306]
train() client id: f_00004-10-5 loss: 0.677880  [  192/  306]
train() client id: f_00004-10-6 loss: 0.870728  [  224/  306]
train() client id: f_00004-10-7 loss: 0.751117  [  256/  306]
train() client id: f_00004-10-8 loss: 0.835174  [  288/  306]
train() client id: f_00004-11-0 loss: 0.738207  [   32/  306]
train() client id: f_00004-11-1 loss: 0.787046  [   64/  306]
train() client id: f_00004-11-2 loss: 0.770577  [   96/  306]
train() client id: f_00004-11-3 loss: 0.838881  [  128/  306]
train() client id: f_00004-11-4 loss: 0.778010  [  160/  306]
train() client id: f_00004-11-5 loss: 0.909886  [  192/  306]
train() client id: f_00004-11-6 loss: 0.833041  [  224/  306]
train() client id: f_00004-11-7 loss: 0.730183  [  256/  306]
train() client id: f_00004-11-8 loss: 0.790570  [  288/  306]
train() client id: f_00005-0-0 loss: 0.568203  [   32/  146]
train() client id: f_00005-0-1 loss: 0.450139  [   64/  146]
train() client id: f_00005-0-2 loss: 0.656629  [   96/  146]
train() client id: f_00005-0-3 loss: 0.642974  [  128/  146]
train() client id: f_00005-1-0 loss: 0.622250  [   32/  146]
train() client id: f_00005-1-1 loss: 0.529408  [   64/  146]
train() client id: f_00005-1-2 loss: 0.392470  [   96/  146]
train() client id: f_00005-1-3 loss: 0.850430  [  128/  146]
train() client id: f_00005-2-0 loss: 0.525014  [   32/  146]
train() client id: f_00005-2-1 loss: 0.447286  [   64/  146]
train() client id: f_00005-2-2 loss: 0.757857  [   96/  146]
train() client id: f_00005-2-3 loss: 0.633002  [  128/  146]
train() client id: f_00005-3-0 loss: 0.775474  [   32/  146]
train() client id: f_00005-3-1 loss: 0.545296  [   64/  146]
train() client id: f_00005-3-2 loss: 0.569776  [   96/  146]
train() client id: f_00005-3-3 loss: 0.467779  [  128/  146]
train() client id: f_00005-4-0 loss: 0.569343  [   32/  146]
train() client id: f_00005-4-1 loss: 0.545984  [   64/  146]
train() client id: f_00005-4-2 loss: 0.461154  [   96/  146]
train() client id: f_00005-4-3 loss: 0.788003  [  128/  146]
train() client id: f_00005-5-0 loss: 0.828587  [   32/  146]
train() client id: f_00005-5-1 loss: 0.690674  [   64/  146]
train() client id: f_00005-5-2 loss: 0.473207  [   96/  146]
train() client id: f_00005-5-3 loss: 0.455224  [  128/  146]
train() client id: f_00005-6-0 loss: 0.643606  [   32/  146]
train() client id: f_00005-6-1 loss: 0.566069  [   64/  146]
train() client id: f_00005-6-2 loss: 0.587120  [   96/  146]
train() client id: f_00005-6-3 loss: 0.371262  [  128/  146]
train() client id: f_00005-7-0 loss: 0.576989  [   32/  146]
train() client id: f_00005-7-1 loss: 0.447184  [   64/  146]
train() client id: f_00005-7-2 loss: 0.647719  [   96/  146]
train() client id: f_00005-7-3 loss: 0.561077  [  128/  146]
train() client id: f_00005-8-0 loss: 0.714729  [   32/  146]
train() client id: f_00005-8-1 loss: 0.512579  [   64/  146]
train() client id: f_00005-8-2 loss: 0.479058  [   96/  146]
train() client id: f_00005-8-3 loss: 0.658220  [  128/  146]
train() client id: f_00005-9-0 loss: 0.488057  [   32/  146]
train() client id: f_00005-9-1 loss: 0.308937  [   64/  146]
train() client id: f_00005-9-2 loss: 1.131835  [   96/  146]
train() client id: f_00005-9-3 loss: 0.569081  [  128/  146]
train() client id: f_00005-10-0 loss: 0.593883  [   32/  146]
train() client id: f_00005-10-1 loss: 0.670242  [   64/  146]
train() client id: f_00005-10-2 loss: 0.569286  [   96/  146]
train() client id: f_00005-10-3 loss: 0.388733  [  128/  146]
train() client id: f_00005-11-0 loss: 0.547975  [   32/  146]
train() client id: f_00005-11-1 loss: 0.600619  [   64/  146]
train() client id: f_00005-11-2 loss: 0.593393  [   96/  146]
train() client id: f_00005-11-3 loss: 0.525933  [  128/  146]
train() client id: f_00006-0-0 loss: 0.523710  [   32/   54]
train() client id: f_00006-1-0 loss: 0.477238  [   32/   54]
train() client id: f_00006-2-0 loss: 0.424966  [   32/   54]
train() client id: f_00006-3-0 loss: 0.484145  [   32/   54]
train() client id: f_00006-4-0 loss: 0.446595  [   32/   54]
train() client id: f_00006-5-0 loss: 0.443524  [   32/   54]
train() client id: f_00006-6-0 loss: 0.482608  [   32/   54]
train() client id: f_00006-7-0 loss: 0.470242  [   32/   54]
train() client id: f_00006-8-0 loss: 0.531090  [   32/   54]
train() client id: f_00006-9-0 loss: 0.490629  [   32/   54]
train() client id: f_00006-10-0 loss: 0.485501  [   32/   54]
train() client id: f_00006-11-0 loss: 0.488097  [   32/   54]
train() client id: f_00007-0-0 loss: 0.477428  [   32/  179]
train() client id: f_00007-0-1 loss: 0.612933  [   64/  179]
train() client id: f_00007-0-2 loss: 0.784482  [   96/  179]
train() client id: f_00007-0-3 loss: 0.568591  [  128/  179]
train() client id: f_00007-0-4 loss: 0.625087  [  160/  179]
train() client id: f_00007-1-0 loss: 0.519291  [   32/  179]
train() client id: f_00007-1-1 loss: 0.946603  [   64/  179]
train() client id: f_00007-1-2 loss: 0.580037  [   96/  179]
train() client id: f_00007-1-3 loss: 0.499953  [  128/  179]
train() client id: f_00007-1-4 loss: 0.685280  [  160/  179]
train() client id: f_00007-2-0 loss: 0.622110  [   32/  179]
train() client id: f_00007-2-1 loss: 0.491787  [   64/  179]
train() client id: f_00007-2-2 loss: 0.666998  [   96/  179]
train() client id: f_00007-2-3 loss: 0.574381  [  128/  179]
train() client id: f_00007-2-4 loss: 0.784406  [  160/  179]
train() client id: f_00007-3-0 loss: 0.703160  [   32/  179]
train() client id: f_00007-3-1 loss: 0.487646  [   64/  179]
train() client id: f_00007-3-2 loss: 0.677423  [   96/  179]
train() client id: f_00007-3-3 loss: 0.607821  [  128/  179]
train() client id: f_00007-3-4 loss: 0.765191  [  160/  179]
train() client id: f_00007-4-0 loss: 0.637976  [   32/  179]
train() client id: f_00007-4-1 loss: 0.763357  [   64/  179]
train() client id: f_00007-4-2 loss: 0.463329  [   96/  179]
train() client id: f_00007-4-3 loss: 0.476113  [  128/  179]
train() client id: f_00007-4-4 loss: 0.864766  [  160/  179]
train() client id: f_00007-5-0 loss: 0.642688  [   32/  179]
train() client id: f_00007-5-1 loss: 0.521946  [   64/  179]
train() client id: f_00007-5-2 loss: 0.605830  [   96/  179]
train() client id: f_00007-5-3 loss: 0.640932  [  128/  179]
train() client id: f_00007-5-4 loss: 0.717527  [  160/  179]
train() client id: f_00007-6-0 loss: 0.707829  [   32/  179]
train() client id: f_00007-6-1 loss: 0.620133  [   64/  179]
train() client id: f_00007-6-2 loss: 0.461007  [   96/  179]
train() client id: f_00007-6-3 loss: 0.486295  [  128/  179]
train() client id: f_00007-6-4 loss: 0.571955  [  160/  179]
train() client id: f_00007-7-0 loss: 0.557342  [   32/  179]
train() client id: f_00007-7-1 loss: 0.653334  [   64/  179]
train() client id: f_00007-7-2 loss: 0.865789  [   96/  179]
train() client id: f_00007-7-3 loss: 0.554450  [  128/  179]
train() client id: f_00007-7-4 loss: 0.551773  [  160/  179]
train() client id: f_00007-8-0 loss: 0.474581  [   32/  179]
train() client id: f_00007-8-1 loss: 0.587211  [   64/  179]
train() client id: f_00007-8-2 loss: 0.641035  [   96/  179]
train() client id: f_00007-8-3 loss: 0.678107  [  128/  179]
train() client id: f_00007-8-4 loss: 0.537574  [  160/  179]
train() client id: f_00007-9-0 loss: 0.572080  [   32/  179]
train() client id: f_00007-9-1 loss: 0.506967  [   64/  179]
train() client id: f_00007-9-2 loss: 1.073753  [   96/  179]
train() client id: f_00007-9-3 loss: 0.550273  [  128/  179]
train() client id: f_00007-9-4 loss: 0.464738  [  160/  179]
train() client id: f_00007-10-0 loss: 0.576610  [   32/  179]
train() client id: f_00007-10-1 loss: 0.662649  [   64/  179]
train() client id: f_00007-10-2 loss: 0.645445  [   96/  179]
train() client id: f_00007-10-3 loss: 0.642589  [  128/  179]
train() client id: f_00007-10-4 loss: 0.532033  [  160/  179]
train() client id: f_00007-11-0 loss: 0.875277  [   32/  179]
train() client id: f_00007-11-1 loss: 0.486606  [   64/  179]
train() client id: f_00007-11-2 loss: 0.739048  [   96/  179]
train() client id: f_00007-11-3 loss: 0.464226  [  128/  179]
train() client id: f_00007-11-4 loss: 0.612255  [  160/  179]
train() client id: f_00008-0-0 loss: 0.610716  [   32/  130]
train() client id: f_00008-0-1 loss: 0.597365  [   64/  130]
train() client id: f_00008-0-2 loss: 0.675500  [   96/  130]
train() client id: f_00008-0-3 loss: 0.694720  [  128/  130]
train() client id: f_00008-1-0 loss: 0.601855  [   32/  130]
train() client id: f_00008-1-1 loss: 0.768006  [   64/  130]
train() client id: f_00008-1-2 loss: 0.635779  [   96/  130]
train() client id: f_00008-1-3 loss: 0.622809  [  128/  130]
train() client id: f_00008-2-0 loss: 0.593332  [   32/  130]
train() client id: f_00008-2-1 loss: 0.667600  [   64/  130]
train() client id: f_00008-2-2 loss: 0.645752  [   96/  130]
train() client id: f_00008-2-3 loss: 0.721791  [  128/  130]
train() client id: f_00008-3-0 loss: 0.599111  [   32/  130]
train() client id: f_00008-3-1 loss: 0.649388  [   64/  130]
train() client id: f_00008-3-2 loss: 0.615808  [   96/  130]
train() client id: f_00008-3-3 loss: 0.750872  [  128/  130]
train() client id: f_00008-4-0 loss: 0.587017  [   32/  130]
train() client id: f_00008-4-1 loss: 0.716990  [   64/  130]
train() client id: f_00008-4-2 loss: 0.738583  [   96/  130]
train() client id: f_00008-4-3 loss: 0.584260  [  128/  130]
train() client id: f_00008-5-0 loss: 0.594990  [   32/  130]
train() client id: f_00008-5-1 loss: 0.567352  [   64/  130]
train() client id: f_00008-5-2 loss: 0.673629  [   96/  130]
train() client id: f_00008-5-3 loss: 0.718874  [  128/  130]
train() client id: f_00008-6-0 loss: 0.589990  [   32/  130]
train() client id: f_00008-6-1 loss: 0.722201  [   64/  130]
train() client id: f_00008-6-2 loss: 0.626770  [   96/  130]
train() client id: f_00008-6-3 loss: 0.656063  [  128/  130]
train() client id: f_00008-7-0 loss: 0.754079  [   32/  130]
train() client id: f_00008-7-1 loss: 0.651326  [   64/  130]
train() client id: f_00008-7-2 loss: 0.694021  [   96/  130]
train() client id: f_00008-7-3 loss: 0.528457  [  128/  130]
train() client id: f_00008-8-0 loss: 0.669445  [   32/  130]
train() client id: f_00008-8-1 loss: 0.643361  [   64/  130]
train() client id: f_00008-8-2 loss: 0.610545  [   96/  130]
train() client id: f_00008-8-3 loss: 0.684970  [  128/  130]
train() client id: f_00008-9-0 loss: 0.681713  [   32/  130]
train() client id: f_00008-9-1 loss: 0.607298  [   64/  130]
train() client id: f_00008-9-2 loss: 0.691949  [   96/  130]
train() client id: f_00008-9-3 loss: 0.637645  [  128/  130]
train() client id: f_00008-10-0 loss: 0.659692  [   32/  130]
train() client id: f_00008-10-1 loss: 0.527591  [   64/  130]
train() client id: f_00008-10-2 loss: 0.677010  [   96/  130]
train() client id: f_00008-10-3 loss: 0.762041  [  128/  130]
train() client id: f_00008-11-0 loss: 0.705391  [   32/  130]
train() client id: f_00008-11-1 loss: 0.689535  [   64/  130]
train() client id: f_00008-11-2 loss: 0.501162  [   96/  130]
train() client id: f_00008-11-3 loss: 0.730245  [  128/  130]
train() client id: f_00009-0-0 loss: 1.165150  [   32/  118]
train() client id: f_00009-0-1 loss: 0.922467  [   64/  118]
train() client id: f_00009-0-2 loss: 1.088422  [   96/  118]
train() client id: f_00009-1-0 loss: 1.022259  [   32/  118]
train() client id: f_00009-1-1 loss: 0.947953  [   64/  118]
train() client id: f_00009-1-2 loss: 1.105162  [   96/  118]
train() client id: f_00009-2-0 loss: 0.973525  [   32/  118]
train() client id: f_00009-2-1 loss: 1.033556  [   64/  118]
train() client id: f_00009-2-2 loss: 1.023978  [   96/  118]
train() client id: f_00009-3-0 loss: 0.977212  [   32/  118]
train() client id: f_00009-3-1 loss: 1.103355  [   64/  118]
train() client id: f_00009-3-2 loss: 0.836354  [   96/  118]
train() client id: f_00009-4-0 loss: 0.823466  [   32/  118]
train() client id: f_00009-4-1 loss: 0.907809  [   64/  118]
train() client id: f_00009-4-2 loss: 0.906348  [   96/  118]
train() client id: f_00009-5-0 loss: 0.799200  [   32/  118]
train() client id: f_00009-5-1 loss: 0.928657  [   64/  118]
train() client id: f_00009-5-2 loss: 0.919550  [   96/  118]
train() client id: f_00009-6-0 loss: 1.113545  [   32/  118]
train() client id: f_00009-6-1 loss: 0.849852  [   64/  118]
train() client id: f_00009-6-2 loss: 0.813954  [   96/  118]
train() client id: f_00009-7-0 loss: 0.927200  [   32/  118]
train() client id: f_00009-7-1 loss: 0.991143  [   64/  118]
train() client id: f_00009-7-2 loss: 0.831000  [   96/  118]
train() client id: f_00009-8-0 loss: 0.873945  [   32/  118]
train() client id: f_00009-8-1 loss: 0.769105  [   64/  118]
train() client id: f_00009-8-2 loss: 0.976994  [   96/  118]
train() client id: f_00009-9-0 loss: 0.919770  [   32/  118]
train() client id: f_00009-9-1 loss: 0.985028  [   64/  118]
train() client id: f_00009-9-2 loss: 0.852306  [   96/  118]
train() client id: f_00009-10-0 loss: 0.916822  [   32/  118]
train() client id: f_00009-10-1 loss: 0.909492  [   64/  118]
train() client id: f_00009-10-2 loss: 1.024174  [   96/  118]
train() client id: f_00009-11-0 loss: 0.927479  [   32/  118]
train() client id: f_00009-11-1 loss: 0.931305  [   64/  118]
train() client id: f_00009-11-2 loss: 0.939404  [   96/  118]
At round 36 accuracy: 0.649867374005305
At round 36 training accuracy: 0.5868544600938967
At round 36 training loss: 0.8324001700473916
update_location
xs = -4.528292 81.001589 90.045120 -90.943528 -10.103519 -15.217951 -132.215960 153.375741 -1.680116 74.695607 
ys = 167.587959 15.555839 1.320614 22.544824 9.350187 2.814151 -3.124922 -19.177652 -97.154970 4.001482 
xs mean: 14.442869159412865
ys mean: 10.371751218646876
dists_uav = 195.208170 129.627318 134.572908 137.036471 100.943088 101.190442 165.803574 184.097529 139.434253 124.881726 
uav_gains = -107.381328 -102.818043 -103.224996 -103.422256 -100.101933 -100.128506 -105.506470 -106.686275 -103.610948 -102.412847 
uav_gains_db_mean: -103.52936011087154
dists_bs = 170.632768 301.594520 316.880805 174.092134 233.732133 234.900553 183.191053 381.491268 322.657884 302.635737 
bs_gains = -102.064807 -108.990916 -109.592145 -102.308875 -105.891175 -105.951812 -102.928378 -111.848628 -109.811843 -109.032825 
bs_gains_db_mean: -106.84214052612032
Round 37
-------------------------------
ene_coms = [0.00906042 0.0099998  0.00727426 0.00734334 0.00833028 0.00835776
 0.0081599  0.00870513 0.01055331 0.01002673]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 6.09477285 12.67651113  5.97230308  2.14655895 14.58739345  7.02866103
  2.67904284  8.5931799   6.30713819  5.73195196]
obj_prev = 71.81751338905976
eta_min = 9.716663380313954e-16	eta_max = 0.9411480861183917
af = 15.14331232310346	bf = 1.361807827707032	zeta = 16.65764355541381	eta = 0.909090909090909
af = 15.14331232310346	bf = 1.361807827707032	zeta = 30.856316128605748	eta = 0.4907686406889206
af = 15.14331232310346	bf = 1.361807827707032	zeta = 23.84093544302711	eta = 0.6351811303416162
af = 15.14331232310346	bf = 1.361807827707032	zeta = 22.57154507819107	eta = 0.6709027791692972
af = 15.14331232310346	bf = 1.361807827707032	zeta = 22.503908831098574	eta = 0.6729191998048194
af = 15.14331232310346	bf = 1.361807827707032	zeta = 22.503700554283558	eta = 0.6729254278235117
eta = 0.6729254278235117
ene_coms = [0.00906042 0.0099998  0.00727426 0.00734334 0.00833028 0.00835776
 0.0081599  0.00870513 0.01055331 0.01002673]
ene_comp = [0.03268694 0.06874633 0.0321681  0.01115507 0.07938256 0.03787534
 0.01400869 0.0464362  0.03372462 0.03061157]
ene_total = [1.97947206 3.73378786 1.8701795  0.87711144 4.15894854 2.19216588
 1.05113489 2.61455412 2.09946053 1.92688572]
ti_comp = [0.50308015 0.49368635 0.52094171 0.52025092 0.51038159 0.51010674
 0.51208534 0.50663308 0.48815121 0.49341708]
ti_coms = [0.09060421 0.09999802 0.07274265 0.07343344 0.08330278 0.08357762
 0.08159903 0.08705128 0.10553315 0.10026728]
t_total = [28.14984474 28.14984474 28.14984474 28.14984474 28.14984474 28.14984474
 28.14984474 28.14984474 28.14984474 28.14984474]
ene_coms = [0.00906042 0.0099998  0.00727426 0.00734334 0.00833028 0.00835776
 0.0081599  0.00870513 0.01055331 0.01002673]
ene_comp = [8.62438781e-06 8.33156141e-05 7.66615860e-06 3.20531455e-07
 1.20023226e-04 1.30505092e-05 6.55220701e-07 2.43816458e-05
 1.00603322e-05 7.36391344e-06]
ene_total = [0.43001338 0.47809611 0.34527645 0.34820356 0.4006753  0.39690633
 0.38693698 0.41391413 0.50086779 0.47577156]
optimize_network iter = 0 obj = 4.176661573384145
eta = 0.6729254278235117
freqs = [32486807.08447486 69625515.93227786 30874947.00100454 10720851.07894379
 77767851.07624051 37124914.62004546 13678078.39723318 45828237.64996305
 34543206.42949407 31019978.84057898]
eta_min = 0.6729254278235521	eta_max = 0.684947986721094
af = 0.010784513090981183	bf = 1.361807827707032	zeta = 0.011862964400079303	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00906042 0.0099998  0.00727426 0.00734334 0.00833028 0.00835776
 0.0081599  0.00870513 0.01055331 0.01002673]
ene_comp = [2.07510445e-06 2.00464781e-05 1.84454597e-06 7.71227202e-08
 2.88786562e-05 3.14006863e-06 1.57651931e-07 5.86644092e-06
 2.42060544e-06 1.77182310e-06]
ene_total = [1.54599359 1.70931064 1.2412495  1.25273231 1.42600913 1.426307
 1.39204484 1.4860301  1.800729   1.7107866 ]
ti_comp = [0.48125759 0.47186378 0.49911915 0.49842836 0.48855902 0.48828418
 0.49026277 0.48481052 0.46632865 0.47159452]
ti_coms = [0.09060421 0.09999802 0.07274265 0.07343344 0.08330278 0.08357762
 0.08159903 0.08705128 0.10553315 0.10026728]
t_total = [28.14984474 28.14984474 28.14984474 28.14984474 28.14984474 28.14984474
 28.14984474 28.14984474 28.14984474 28.14984474]
ene_coms = [0.00906042 0.0099998  0.00727426 0.00734334 0.00833028 0.00835776
 0.0081599  0.00870513 0.01055331 0.01002673]
ene_comp = [8.21600394e-06 7.95075769e-05 7.28049283e-06 3.04441645e-07
 1.14191661e-04 1.24170221e-05 6.23200268e-07 2.32123556e-05
 9.61058994e-06 7.02769193e-06]
ene_total = [0.44640282 0.49615307 0.35843341 0.36149041 0.41567823 0.41202131
 0.40170113 0.42965176 0.51995904 0.49391071]
optimize_network iter = 1 obj = 4.335401892372793
eta = 0.684947986721094
freqs = [32441755.5311707  69589004.30315635 30784269.11647037 10689985.17406873
 77609660.29816927 37050288.98850761 13648226.9729078  45750136.33861712
 34543206.42949404 31004494.45513327]
eta_min = 0.6849479867211025	eta_max = 0.6849479867210945
af = 0.010753524786093942	bf = 1.361807827707032	zeta = 0.011828877264703337	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00906042 0.0099998  0.00727426 0.00734334 0.00833028 0.00835776
 0.0081599  0.00870513 0.01055331 0.01002673]
ene_comp = [2.06935308e-06 2.00254589e-05 1.83372724e-06 7.66792786e-08
 2.87612892e-05 3.12745747e-06 1.56964554e-07 5.84646257e-06
 2.42060544e-06 1.77005464e-06]
ene_total = [1.54599261 1.70930705 1.24124766 1.25273223 1.42598911 1.42630485
 1.39204472 1.48602669 1.800729   1.7107863 ]
ti_comp = [0.48125759 0.47186378 0.49911915 0.49842836 0.48855902 0.48828418
 0.49026277 0.48481052 0.46632865 0.47159452]
ti_coms = [0.09060421 0.09999802 0.07274265 0.07343344 0.08330278 0.08357762
 0.08159903 0.08705128 0.10553315 0.10026728]
t_total = [28.14984474 28.14984474 28.14984474 28.14984474 28.14984474 28.14984474
 28.14984474 28.14984474 28.14984474 28.14984474]
ene_coms = [0.00906042 0.0099998  0.00727426 0.00734334 0.00833028 0.00835776
 0.0081599  0.00870513 0.01055331 0.01002673]
ene_comp = [8.21600394e-06 7.95075769e-05 7.28049283e-06 3.04441645e-07
 1.14191661e-04 1.24170221e-05 6.23200268e-07 2.32123556e-05
 9.61058994e-06 7.02769193e-06]
ene_total = [0.44640282 0.49615307 0.35843341 0.36149041 0.41567823 0.41202131
 0.40170113 0.42965176 0.51995904 0.49391071]
optimize_network iter = 2 obj = 4.335401892372799
eta = 0.6849479867210945
freqs = [32441755.5311707  69589004.30315636 30784269.11647037 10689985.17406873
 77609660.29816927 37050288.98850761 13648226.9729078  45750136.33861712
 34543206.42949405 31004494.45513327]
Done!
ene_coms = [0.00906042 0.0099998  0.00727426 0.00734334 0.00833028 0.00835776
 0.0081599  0.00870513 0.01055331 0.01002673]
ene_comp = [7.95665431e-06 7.69978094e-05 7.05067391e-06 2.94831520e-07
 1.10587042e-04 1.20250615e-05 6.03528082e-07 2.24796252e-05
 9.30721826e-06 6.80585303e-06]
ene_total = [0.00906838 0.0100768  0.00728132 0.00734364 0.00844086 0.00836979
 0.00816051 0.00872761 0.01056262 0.01003353]
At round 37 energy consumption: 0.08806505303180641
At round 37 eta: 0.6849479867210945
At round 37 a_n: 15.508406518389636
At round 37 local rounds: 12.391143735921244
At round 37 global rounds: 49.22490847459063
gradient difference: 0.43302035331726074
train() client id: f_00000-0-0 loss: 1.243815  [   32/  126]
train() client id: f_00000-0-1 loss: 1.142880  [   64/  126]
train() client id: f_00000-0-2 loss: 1.338591  [   96/  126]
train() client id: f_00000-1-0 loss: 1.333044  [   32/  126]
train() client id: f_00000-1-1 loss: 0.948837  [   64/  126]
train() client id: f_00000-1-2 loss: 1.135108  [   96/  126]
train() client id: f_00000-2-0 loss: 1.156344  [   32/  126]
train() client id: f_00000-2-1 loss: 1.084231  [   64/  126]
train() client id: f_00000-2-2 loss: 0.912372  [   96/  126]
train() client id: f_00000-3-0 loss: 1.032940  [   32/  126]
train() client id: f_00000-3-1 loss: 0.901857  [   64/  126]
train() client id: f_00000-3-2 loss: 1.174658  [   96/  126]
train() client id: f_00000-4-0 loss: 0.977223  [   32/  126]
train() client id: f_00000-4-1 loss: 1.014239  [   64/  126]
train() client id: f_00000-4-2 loss: 0.974814  [   96/  126]
train() client id: f_00000-5-0 loss: 0.951796  [   32/  126]
train() client id: f_00000-5-1 loss: 0.963703  [   64/  126]
train() client id: f_00000-5-2 loss: 1.037709  [   96/  126]
train() client id: f_00000-6-0 loss: 1.023088  [   32/  126]
train() client id: f_00000-6-1 loss: 0.834404  [   64/  126]
train() client id: f_00000-6-2 loss: 0.907288  [   96/  126]
train() client id: f_00000-7-0 loss: 0.931720  [   32/  126]
train() client id: f_00000-7-1 loss: 0.939055  [   64/  126]
train() client id: f_00000-7-2 loss: 0.969631  [   96/  126]
train() client id: f_00000-8-0 loss: 0.944429  [   32/  126]
train() client id: f_00000-8-1 loss: 0.843548  [   64/  126]
train() client id: f_00000-8-2 loss: 0.989782  [   96/  126]
train() client id: f_00000-9-0 loss: 0.864453  [   32/  126]
train() client id: f_00000-9-1 loss: 0.944803  [   64/  126]
train() client id: f_00000-9-2 loss: 0.874057  [   96/  126]
train() client id: f_00000-10-0 loss: 0.948159  [   32/  126]
train() client id: f_00000-10-1 loss: 0.951818  [   64/  126]
train() client id: f_00000-10-2 loss: 0.827518  [   96/  126]
train() client id: f_00000-11-0 loss: 0.930230  [   32/  126]
train() client id: f_00000-11-1 loss: 1.030639  [   64/  126]
train() client id: f_00000-11-2 loss: 0.861450  [   96/  126]
train() client id: f_00001-0-0 loss: 0.371265  [   32/  265]
train() client id: f_00001-0-1 loss: 0.450795  [   64/  265]
train() client id: f_00001-0-2 loss: 0.401184  [   96/  265]
train() client id: f_00001-0-3 loss: 0.427724  [  128/  265]
train() client id: f_00001-0-4 loss: 0.367912  [  160/  265]
train() client id: f_00001-0-5 loss: 0.276254  [  192/  265]
train() client id: f_00001-0-6 loss: 0.327300  [  224/  265]
train() client id: f_00001-0-7 loss: 0.385520  [  256/  265]
train() client id: f_00001-1-0 loss: 0.298035  [   32/  265]
train() client id: f_00001-1-1 loss: 0.270510  [   64/  265]
train() client id: f_00001-1-2 loss: 0.424171  [   96/  265]
train() client id: f_00001-1-3 loss: 0.451320  [  128/  265]
train() client id: f_00001-1-4 loss: 0.302573  [  160/  265]
train() client id: f_00001-1-5 loss: 0.473035  [  192/  265]
train() client id: f_00001-1-6 loss: 0.348082  [  224/  265]
train() client id: f_00001-1-7 loss: 0.424307  [  256/  265]
train() client id: f_00001-2-0 loss: 0.350794  [   32/  265]
train() client id: f_00001-2-1 loss: 0.475451  [   64/  265]
train() client id: f_00001-2-2 loss: 0.335939  [   96/  265]
train() client id: f_00001-2-3 loss: 0.386773  [  128/  265]
train() client id: f_00001-2-4 loss: 0.369936  [  160/  265]
train() client id: f_00001-2-5 loss: 0.379253  [  192/  265]
train() client id: f_00001-2-6 loss: 0.364883  [  224/  265]
train() client id: f_00001-2-7 loss: 0.290675  [  256/  265]
train() client id: f_00001-3-0 loss: 0.463971  [   32/  265]
train() client id: f_00001-3-1 loss: 0.289564  [   64/  265]
train() client id: f_00001-3-2 loss: 0.338678  [   96/  265]
train() client id: f_00001-3-3 loss: 0.347346  [  128/  265]
train() client id: f_00001-3-4 loss: 0.279431  [  160/  265]
train() client id: f_00001-3-5 loss: 0.326755  [  192/  265]
train() client id: f_00001-3-6 loss: 0.447344  [  224/  265]
train() client id: f_00001-3-7 loss: 0.337916  [  256/  265]
train() client id: f_00001-4-0 loss: 0.281597  [   32/  265]
train() client id: f_00001-4-1 loss: 0.387230  [   64/  265]
train() client id: f_00001-4-2 loss: 0.409275  [   96/  265]
train() client id: f_00001-4-3 loss: 0.324584  [  128/  265]
train() client id: f_00001-4-4 loss: 0.423189  [  160/  265]
train() client id: f_00001-4-5 loss: 0.278739  [  192/  265]
train() client id: f_00001-4-6 loss: 0.361512  [  224/  265]
train() client id: f_00001-4-7 loss: 0.331155  [  256/  265]
train() client id: f_00001-5-0 loss: 0.325624  [   32/  265]
train() client id: f_00001-5-1 loss: 0.369859  [   64/  265]
train() client id: f_00001-5-2 loss: 0.362355  [   96/  265]
train() client id: f_00001-5-3 loss: 0.360731  [  128/  265]
train() client id: f_00001-5-4 loss: 0.406800  [  160/  265]
train() client id: f_00001-5-5 loss: 0.318645  [  192/  265]
train() client id: f_00001-5-6 loss: 0.307666  [  224/  265]
train() client id: f_00001-5-7 loss: 0.272489  [  256/  265]
train() client id: f_00001-6-0 loss: 0.266318  [   32/  265]
train() client id: f_00001-6-1 loss: 0.380722  [   64/  265]
train() client id: f_00001-6-2 loss: 0.348332  [   96/  265]
train() client id: f_00001-6-3 loss: 0.254386  [  128/  265]
train() client id: f_00001-6-4 loss: 0.285107  [  160/  265]
train() client id: f_00001-6-5 loss: 0.488077  [  192/  265]
train() client id: f_00001-6-6 loss: 0.343046  [  224/  265]
train() client id: f_00001-6-7 loss: 0.450390  [  256/  265]
train() client id: f_00001-7-0 loss: 0.258208  [   32/  265]
train() client id: f_00001-7-1 loss: 0.430714  [   64/  265]
train() client id: f_00001-7-2 loss: 0.334538  [   96/  265]
train() client id: f_00001-7-3 loss: 0.491290  [  128/  265]
train() client id: f_00001-7-4 loss: 0.277201  [  160/  265]
train() client id: f_00001-7-5 loss: 0.369934  [  192/  265]
train() client id: f_00001-7-6 loss: 0.253705  [  224/  265]
train() client id: f_00001-7-7 loss: 0.400024  [  256/  265]
train() client id: f_00001-8-0 loss: 0.243438  [   32/  265]
train() client id: f_00001-8-1 loss: 0.314996  [   64/  265]
train() client id: f_00001-8-2 loss: 0.360482  [   96/  265]
train() client id: f_00001-8-3 loss: 0.506988  [  128/  265]
train() client id: f_00001-8-4 loss: 0.424187  [  160/  265]
train() client id: f_00001-8-5 loss: 0.285435  [  192/  265]
train() client id: f_00001-8-6 loss: 0.384404  [  224/  265]
train() client id: f_00001-8-7 loss: 0.274003  [  256/  265]
train() client id: f_00001-9-0 loss: 0.339596  [   32/  265]
train() client id: f_00001-9-1 loss: 0.300450  [   64/  265]
train() client id: f_00001-9-2 loss: 0.394779  [   96/  265]
train() client id: f_00001-9-3 loss: 0.298161  [  128/  265]
train() client id: f_00001-9-4 loss: 0.414213  [  160/  265]
train() client id: f_00001-9-5 loss: 0.362210  [  192/  265]
train() client id: f_00001-9-6 loss: 0.335028  [  224/  265]
train() client id: f_00001-9-7 loss: 0.278763  [  256/  265]
train() client id: f_00001-10-0 loss: 0.364134  [   32/  265]
train() client id: f_00001-10-1 loss: 0.273933  [   64/  265]
train() client id: f_00001-10-2 loss: 0.406082  [   96/  265]
train() client id: f_00001-10-3 loss: 0.299463  [  128/  265]
train() client id: f_00001-10-4 loss: 0.367113  [  160/  265]
train() client id: f_00001-10-5 loss: 0.472000  [  192/  265]
train() client id: f_00001-10-6 loss: 0.307089  [  224/  265]
train() client id: f_00001-10-7 loss: 0.242684  [  256/  265]
train() client id: f_00001-11-0 loss: 0.261101  [   32/  265]
train() client id: f_00001-11-1 loss: 0.404931  [   64/  265]
train() client id: f_00001-11-2 loss: 0.359715  [   96/  265]
train() client id: f_00001-11-3 loss: 0.429045  [  128/  265]
train() client id: f_00001-11-4 loss: 0.371629  [  160/  265]
train() client id: f_00001-11-5 loss: 0.379643  [  192/  265]
train() client id: f_00001-11-6 loss: 0.276430  [  224/  265]
train() client id: f_00001-11-7 loss: 0.304368  [  256/  265]
train() client id: f_00002-0-0 loss: 1.518455  [   32/  124]
train() client id: f_00002-0-1 loss: 1.256743  [   64/  124]
train() client id: f_00002-0-2 loss: 1.136400  [   96/  124]
train() client id: f_00002-1-0 loss: 1.258171  [   32/  124]
train() client id: f_00002-1-1 loss: 1.183588  [   64/  124]
train() client id: f_00002-1-2 loss: 1.321413  [   96/  124]
train() client id: f_00002-2-0 loss: 1.544498  [   32/  124]
train() client id: f_00002-2-1 loss: 1.202618  [   64/  124]
train() client id: f_00002-2-2 loss: 1.027650  [   96/  124]
train() client id: f_00002-3-0 loss: 1.245388  [   32/  124]
train() client id: f_00002-3-1 loss: 1.201563  [   64/  124]
train() client id: f_00002-3-2 loss: 1.127769  [   96/  124]
train() client id: f_00002-4-0 loss: 1.110760  [   32/  124]
train() client id: f_00002-4-1 loss: 0.931770  [   64/  124]
train() client id: f_00002-4-2 loss: 1.342480  [   96/  124]
train() client id: f_00002-5-0 loss: 1.161968  [   32/  124]
train() client id: f_00002-5-1 loss: 0.993773  [   64/  124]
train() client id: f_00002-5-2 loss: 1.333658  [   96/  124]
train() client id: f_00002-6-0 loss: 1.210202  [   32/  124]
train() client id: f_00002-6-1 loss: 1.095468  [   64/  124]
train() client id: f_00002-6-2 loss: 0.989262  [   96/  124]
train() client id: f_00002-7-0 loss: 1.060494  [   32/  124]
train() client id: f_00002-7-1 loss: 1.008121  [   64/  124]
train() client id: f_00002-7-2 loss: 1.091377  [   96/  124]
train() client id: f_00002-8-0 loss: 1.063028  [   32/  124]
train() client id: f_00002-8-1 loss: 1.026358  [   64/  124]
train() client id: f_00002-8-2 loss: 1.109087  [   96/  124]
train() client id: f_00002-9-0 loss: 0.974017  [   32/  124]
train() client id: f_00002-9-1 loss: 1.232222  [   64/  124]
train() client id: f_00002-9-2 loss: 1.067102  [   96/  124]
train() client id: f_00002-10-0 loss: 1.111179  [   32/  124]
train() client id: f_00002-10-1 loss: 0.971211  [   64/  124]
train() client id: f_00002-10-2 loss: 1.141528  [   96/  124]
train() client id: f_00002-11-0 loss: 1.120554  [   32/  124]
train() client id: f_00002-11-1 loss: 1.096638  [   64/  124]
train() client id: f_00002-11-2 loss: 0.943530  [   96/  124]
train() client id: f_00003-0-0 loss: 0.451422  [   32/   43]
train() client id: f_00003-1-0 loss: 0.374937  [   32/   43]
train() client id: f_00003-2-0 loss: 0.443147  [   32/   43]
train() client id: f_00003-3-0 loss: 0.544252  [   32/   43]
train() client id: f_00003-4-0 loss: 0.423812  [   32/   43]
train() client id: f_00003-5-0 loss: 0.677818  [   32/   43]
train() client id: f_00003-6-0 loss: 0.477966  [   32/   43]
train() client id: f_00003-7-0 loss: 0.626561  [   32/   43]
train() client id: f_00003-8-0 loss: 0.517570  [   32/   43]
train() client id: f_00003-9-0 loss: 0.596111  [   32/   43]
train() client id: f_00003-10-0 loss: 0.631882  [   32/   43]
train() client id: f_00003-11-0 loss: 0.516666  [   32/   43]
train() client id: f_00004-0-0 loss: 0.793966  [   32/  306]
train() client id: f_00004-0-1 loss: 0.891172  [   64/  306]
train() client id: f_00004-0-2 loss: 0.992148  [   96/  306]
train() client id: f_00004-0-3 loss: 0.894913  [  128/  306]
train() client id: f_00004-0-4 loss: 0.869904  [  160/  306]
train() client id: f_00004-0-5 loss: 0.819505  [  192/  306]
train() client id: f_00004-0-6 loss: 0.742249  [  224/  306]
train() client id: f_00004-0-7 loss: 0.955102  [  256/  306]
train() client id: f_00004-0-8 loss: 0.734874  [  288/  306]
train() client id: f_00004-1-0 loss: 0.920919  [   32/  306]
train() client id: f_00004-1-1 loss: 0.884796  [   64/  306]
train() client id: f_00004-1-2 loss: 0.830356  [   96/  306]
train() client id: f_00004-1-3 loss: 0.815549  [  128/  306]
train() client id: f_00004-1-4 loss: 0.867164  [  160/  306]
train() client id: f_00004-1-5 loss: 0.688497  [  192/  306]
train() client id: f_00004-1-6 loss: 0.936027  [  224/  306]
train() client id: f_00004-1-7 loss: 0.965526  [  256/  306]
train() client id: f_00004-1-8 loss: 0.786725  [  288/  306]
train() client id: f_00004-2-0 loss: 0.846041  [   32/  306]
train() client id: f_00004-2-1 loss: 0.904005  [   64/  306]
train() client id: f_00004-2-2 loss: 0.853668  [   96/  306]
train() client id: f_00004-2-3 loss: 0.860040  [  128/  306]
train() client id: f_00004-2-4 loss: 0.870854  [  160/  306]
train() client id: f_00004-2-5 loss: 0.739841  [  192/  306]
train() client id: f_00004-2-6 loss: 0.963071  [  224/  306]
train() client id: f_00004-2-7 loss: 0.839875  [  256/  306]
train() client id: f_00004-2-8 loss: 0.845608  [  288/  306]
train() client id: f_00004-3-0 loss: 0.839732  [   32/  306]
train() client id: f_00004-3-1 loss: 0.886820  [   64/  306]
train() client id: f_00004-3-2 loss: 0.772981  [   96/  306]
train() client id: f_00004-3-3 loss: 0.810112  [  128/  306]
train() client id: f_00004-3-4 loss: 0.978930  [  160/  306]
train() client id: f_00004-3-5 loss: 0.872856  [  192/  306]
train() client id: f_00004-3-6 loss: 0.791062  [  224/  306]
train() client id: f_00004-3-7 loss: 0.819199  [  256/  306]
train() client id: f_00004-3-8 loss: 0.859528  [  288/  306]
train() client id: f_00004-4-0 loss: 0.866516  [   32/  306]
train() client id: f_00004-4-1 loss: 0.858015  [   64/  306]
train() client id: f_00004-4-2 loss: 0.829310  [   96/  306]
train() client id: f_00004-4-3 loss: 0.850201  [  128/  306]
train() client id: f_00004-4-4 loss: 0.857918  [  160/  306]
train() client id: f_00004-4-5 loss: 0.927368  [  192/  306]
train() client id: f_00004-4-6 loss: 0.777680  [  224/  306]
train() client id: f_00004-4-7 loss: 0.768238  [  256/  306]
train() client id: f_00004-4-8 loss: 0.895980  [  288/  306]
train() client id: f_00004-5-0 loss: 0.878262  [   32/  306]
train() client id: f_00004-5-1 loss: 0.946940  [   64/  306]
train() client id: f_00004-5-2 loss: 0.748323  [   96/  306]
train() client id: f_00004-5-3 loss: 0.780060  [  128/  306]
train() client id: f_00004-5-4 loss: 0.871845  [  160/  306]
train() client id: f_00004-5-5 loss: 0.693259  [  192/  306]
train() client id: f_00004-5-6 loss: 0.865118  [  224/  306]
train() client id: f_00004-5-7 loss: 0.922086  [  256/  306]
train() client id: f_00004-5-8 loss: 0.935830  [  288/  306]
train() client id: f_00004-6-0 loss: 0.887643  [   32/  306]
train() client id: f_00004-6-1 loss: 0.812063  [   64/  306]
train() client id: f_00004-6-2 loss: 0.892873  [   96/  306]
train() client id: f_00004-6-3 loss: 0.910602  [  128/  306]
train() client id: f_00004-6-4 loss: 0.916670  [  160/  306]
train() client id: f_00004-6-5 loss: 0.774254  [  192/  306]
train() client id: f_00004-6-6 loss: 0.798942  [  224/  306]
train() client id: f_00004-6-7 loss: 0.845331  [  256/  306]
train() client id: f_00004-6-8 loss: 0.791760  [  288/  306]
train() client id: f_00004-7-0 loss: 0.837684  [   32/  306]
train() client id: f_00004-7-1 loss: 0.755743  [   64/  306]
train() client id: f_00004-7-2 loss: 0.994236  [   96/  306]
train() client id: f_00004-7-3 loss: 0.831673  [  128/  306]
train() client id: f_00004-7-4 loss: 0.899926  [  160/  306]
train() client id: f_00004-7-5 loss: 0.741370  [  192/  306]
train() client id: f_00004-7-6 loss: 0.772978  [  224/  306]
train() client id: f_00004-7-7 loss: 0.911987  [  256/  306]
train() client id: f_00004-7-8 loss: 0.847611  [  288/  306]
train() client id: f_00004-8-0 loss: 0.766790  [   32/  306]
train() client id: f_00004-8-1 loss: 0.821857  [   64/  306]
train() client id: f_00004-8-2 loss: 0.898992  [   96/  306]
train() client id: f_00004-8-3 loss: 0.849629  [  128/  306]
train() client id: f_00004-8-4 loss: 0.796804  [  160/  306]
train() client id: f_00004-8-5 loss: 0.786911  [  192/  306]
train() client id: f_00004-8-6 loss: 0.991236  [  224/  306]
train() client id: f_00004-8-7 loss: 0.777808  [  256/  306]
train() client id: f_00004-8-8 loss: 0.879993  [  288/  306]
train() client id: f_00004-9-0 loss: 0.904047  [   32/  306]
train() client id: f_00004-9-1 loss: 0.850049  [   64/  306]
train() client id: f_00004-9-2 loss: 0.800236  [   96/  306]
train() client id: f_00004-9-3 loss: 0.884289  [  128/  306]
train() client id: f_00004-9-4 loss: 0.766518  [  160/  306]
train() client id: f_00004-9-5 loss: 0.767275  [  192/  306]
train() client id: f_00004-9-6 loss: 0.917314  [  224/  306]
train() client id: f_00004-9-7 loss: 0.830663  [  256/  306]
train() client id: f_00004-9-8 loss: 0.784138  [  288/  306]
train() client id: f_00004-10-0 loss: 0.776134  [   32/  306]
train() client id: f_00004-10-1 loss: 0.860670  [   64/  306]
train() client id: f_00004-10-2 loss: 0.830894  [   96/  306]
train() client id: f_00004-10-3 loss: 0.810036  [  128/  306]
train() client id: f_00004-10-4 loss: 0.864423  [  160/  306]
train() client id: f_00004-10-5 loss: 0.775507  [  192/  306]
train() client id: f_00004-10-6 loss: 0.906968  [  224/  306]
train() client id: f_00004-10-7 loss: 0.986410  [  256/  306]
train() client id: f_00004-10-8 loss: 0.775569  [  288/  306]
train() client id: f_00004-11-0 loss: 0.809016  [   32/  306]
train() client id: f_00004-11-1 loss: 0.856429  [   64/  306]
train() client id: f_00004-11-2 loss: 0.766647  [   96/  306]
train() client id: f_00004-11-3 loss: 0.864577  [  128/  306]
train() client id: f_00004-11-4 loss: 0.823817  [  160/  306]
train() client id: f_00004-11-5 loss: 0.806616  [  192/  306]
train() client id: f_00004-11-6 loss: 0.849467  [  224/  306]
train() client id: f_00004-11-7 loss: 0.892433  [  256/  306]
train() client id: f_00004-11-8 loss: 0.823005  [  288/  306]
train() client id: f_00005-0-0 loss: 0.524243  [   32/  146]
train() client id: f_00005-0-1 loss: 0.519022  [   64/  146]
train() client id: f_00005-0-2 loss: 0.520875  [   96/  146]
train() client id: f_00005-0-3 loss: 0.810208  [  128/  146]
train() client id: f_00005-1-0 loss: 0.559384  [   32/  146]
train() client id: f_00005-1-1 loss: 0.798852  [   64/  146]
train() client id: f_00005-1-2 loss: 0.602957  [   96/  146]
train() client id: f_00005-1-3 loss: 0.571650  [  128/  146]
train() client id: f_00005-2-0 loss: 0.682546  [   32/  146]
train() client id: f_00005-2-1 loss: 0.541574  [   64/  146]
train() client id: f_00005-2-2 loss: 0.800036  [   96/  146]
train() client id: f_00005-2-3 loss: 0.651725  [  128/  146]
train() client id: f_00005-3-0 loss: 0.603676  [   32/  146]
train() client id: f_00005-3-1 loss: 0.874591  [   64/  146]
train() client id: f_00005-3-2 loss: 0.653603  [   96/  146]
train() client id: f_00005-3-3 loss: 0.411056  [  128/  146]
train() client id: f_00005-4-0 loss: 0.672378  [   32/  146]
train() client id: f_00005-4-1 loss: 0.535296  [   64/  146]
train() client id: f_00005-4-2 loss: 0.676646  [   96/  146]
train() client id: f_00005-4-3 loss: 0.731556  [  128/  146]
train() client id: f_00005-5-0 loss: 0.238805  [   32/  146]
train() client id: f_00005-5-1 loss: 0.567982  [   64/  146]
train() client id: f_00005-5-2 loss: 1.095256  [   96/  146]
train() client id: f_00005-5-3 loss: 0.513010  [  128/  146]
train() client id: f_00005-6-0 loss: 0.875881  [   32/  146]
train() client id: f_00005-6-1 loss: 0.504889  [   64/  146]
train() client id: f_00005-6-2 loss: 0.452383  [   96/  146]
train() client id: f_00005-6-3 loss: 0.824703  [  128/  146]
train() client id: f_00005-7-0 loss: 0.517098  [   32/  146]
train() client id: f_00005-7-1 loss: 0.574208  [   64/  146]
train() client id: f_00005-7-2 loss: 0.648956  [   96/  146]
train() client id: f_00005-7-3 loss: 0.846902  [  128/  146]
train() client id: f_00005-8-0 loss: 0.663383  [   32/  146]
train() client id: f_00005-8-1 loss: 0.647612  [   64/  146]
train() client id: f_00005-8-2 loss: 0.587396  [   96/  146]
train() client id: f_00005-8-3 loss: 0.648790  [  128/  146]
train() client id: f_00005-9-0 loss: 0.603924  [   32/  146]
train() client id: f_00005-9-1 loss: 0.579147  [   64/  146]
train() client id: f_00005-9-2 loss: 0.796393  [   96/  146]
train() client id: f_00005-9-3 loss: 0.609314  [  128/  146]
train() client id: f_00005-10-0 loss: 0.736778  [   32/  146]
train() client id: f_00005-10-1 loss: 0.641234  [   64/  146]
train() client id: f_00005-10-2 loss: 0.754523  [   96/  146]
train() client id: f_00005-10-3 loss: 0.542967  [  128/  146]
train() client id: f_00005-11-0 loss: 0.772243  [   32/  146]
train() client id: f_00005-11-1 loss: 0.507771  [   64/  146]
train() client id: f_00005-11-2 loss: 0.567679  [   96/  146]
train() client id: f_00005-11-3 loss: 0.534906  [  128/  146]
train() client id: f_00006-0-0 loss: 0.491828  [   32/   54]
train() client id: f_00006-1-0 loss: 0.497054  [   32/   54]
train() client id: f_00006-2-0 loss: 0.461743  [   32/   54]
train() client id: f_00006-3-0 loss: 0.546812  [   32/   54]
train() client id: f_00006-4-0 loss: 0.549051  [   32/   54]
train() client id: f_00006-5-0 loss: 0.533589  [   32/   54]
train() client id: f_00006-6-0 loss: 0.510141  [   32/   54]
train() client id: f_00006-7-0 loss: 0.447083  [   32/   54]
train() client id: f_00006-8-0 loss: 0.491727  [   32/   54]
train() client id: f_00006-9-0 loss: 0.558495  [   32/   54]
train() client id: f_00006-10-0 loss: 0.557881  [   32/   54]
train() client id: f_00006-11-0 loss: 0.522873  [   32/   54]
train() client id: f_00007-0-0 loss: 0.567164  [   32/  179]
train() client id: f_00007-0-1 loss: 0.653550  [   64/  179]
train() client id: f_00007-0-2 loss: 0.550789  [   96/  179]
train() client id: f_00007-0-3 loss: 0.504347  [  128/  179]
train() client id: f_00007-0-4 loss: 0.507997  [  160/  179]
train() client id: f_00007-1-0 loss: 0.495314  [   32/  179]
train() client id: f_00007-1-1 loss: 0.548465  [   64/  179]
train() client id: f_00007-1-2 loss: 0.708051  [   96/  179]
train() client id: f_00007-1-3 loss: 0.574190  [  128/  179]
train() client id: f_00007-1-4 loss: 0.550526  [  160/  179]
train() client id: f_00007-2-0 loss: 0.563658  [   32/  179]
train() client id: f_00007-2-1 loss: 0.563602  [   64/  179]
train() client id: f_00007-2-2 loss: 0.481117  [   96/  179]
train() client id: f_00007-2-3 loss: 0.761042  [  128/  179]
train() client id: f_00007-2-4 loss: 0.492958  [  160/  179]
train() client id: f_00007-3-0 loss: 0.630441  [   32/  179]
train() client id: f_00007-3-1 loss: 0.686478  [   64/  179]
train() client id: f_00007-3-2 loss: 0.654805  [   96/  179]
train() client id: f_00007-3-3 loss: 0.432597  [  128/  179]
train() client id: f_00007-3-4 loss: 0.491787  [  160/  179]
train() client id: f_00007-4-0 loss: 0.710240  [   32/  179]
train() client id: f_00007-4-1 loss: 0.743685  [   64/  179]
train() client id: f_00007-4-2 loss: 0.446946  [   96/  179]
train() client id: f_00007-4-3 loss: 0.659521  [  128/  179]
train() client id: f_00007-4-4 loss: 0.404482  [  160/  179]
train() client id: f_00007-5-0 loss: 0.625590  [   32/  179]
train() client id: f_00007-5-1 loss: 0.890584  [   64/  179]
train() client id: f_00007-5-2 loss: 0.463713  [   96/  179]
train() client id: f_00007-5-3 loss: 0.408113  [  128/  179]
train() client id: f_00007-5-4 loss: 0.502048  [  160/  179]
train() client id: f_00007-6-0 loss: 0.629257  [   32/  179]
train() client id: f_00007-6-1 loss: 0.612218  [   64/  179]
train() client id: f_00007-6-2 loss: 0.539558  [   96/  179]
train() client id: f_00007-6-3 loss: 0.754182  [  128/  179]
train() client id: f_00007-6-4 loss: 0.406392  [  160/  179]
train() client id: f_00007-7-0 loss: 0.498030  [   32/  179]
train() client id: f_00007-7-1 loss: 0.522113  [   64/  179]
train() client id: f_00007-7-2 loss: 0.528682  [   96/  179]
train() client id: f_00007-7-3 loss: 0.737115  [  128/  179]
train() client id: f_00007-7-4 loss: 0.586912  [  160/  179]
train() client id: f_00007-8-0 loss: 0.485705  [   32/  179]
train() client id: f_00007-8-1 loss: 0.511885  [   64/  179]
train() client id: f_00007-8-2 loss: 0.519268  [   96/  179]
train() client id: f_00007-8-3 loss: 0.697044  [  128/  179]
train() client id: f_00007-8-4 loss: 0.434827  [  160/  179]
train() client id: f_00007-9-0 loss: 0.787928  [   32/  179]
train() client id: f_00007-9-1 loss: 0.619917  [   64/  179]
train() client id: f_00007-9-2 loss: 0.431348  [   96/  179]
train() client id: f_00007-9-3 loss: 0.485505  [  128/  179]
train() client id: f_00007-9-4 loss: 0.622853  [  160/  179]
train() client id: f_00007-10-0 loss: 0.658647  [   32/  179]
train() client id: f_00007-10-1 loss: 0.683202  [   64/  179]
train() client id: f_00007-10-2 loss: 0.502927  [   96/  179]
train() client id: f_00007-10-3 loss: 0.449054  [  128/  179]
train() client id: f_00007-10-4 loss: 0.555476  [  160/  179]
train() client id: f_00007-11-0 loss: 0.588873  [   32/  179]
train() client id: f_00007-11-1 loss: 0.422004  [   64/  179]
train() client id: f_00007-11-2 loss: 0.467531  [   96/  179]
train() client id: f_00007-11-3 loss: 0.954506  [  128/  179]
train() client id: f_00007-11-4 loss: 0.522945  [  160/  179]
train() client id: f_00008-0-0 loss: 0.674059  [   32/  130]
train() client id: f_00008-0-1 loss: 0.775966  [   64/  130]
train() client id: f_00008-0-2 loss: 0.666143  [   96/  130]
train() client id: f_00008-0-3 loss: 0.478067  [  128/  130]
train() client id: f_00008-1-0 loss: 0.539680  [   32/  130]
train() client id: f_00008-1-1 loss: 0.787927  [   64/  130]
train() client id: f_00008-1-2 loss: 0.642801  [   96/  130]
train() client id: f_00008-1-3 loss: 0.615839  [  128/  130]
train() client id: f_00008-2-0 loss: 0.692591  [   32/  130]
train() client id: f_00008-2-1 loss: 0.627700  [   64/  130]
train() client id: f_00008-2-2 loss: 0.662047  [   96/  130]
train() client id: f_00008-2-3 loss: 0.604840  [  128/  130]
train() client id: f_00008-3-0 loss: 0.643418  [   32/  130]
train() client id: f_00008-3-1 loss: 0.740949  [   64/  130]
train() client id: f_00008-3-2 loss: 0.735643  [   96/  130]
train() client id: f_00008-3-3 loss: 0.507091  [  128/  130]
train() client id: f_00008-4-0 loss: 0.719154  [   32/  130]
train() client id: f_00008-4-1 loss: 0.573697  [   64/  130]
train() client id: f_00008-4-2 loss: 0.606380  [   96/  130]
train() client id: f_00008-4-3 loss: 0.732395  [  128/  130]
train() client id: f_00008-5-0 loss: 0.589773  [   32/  130]
train() client id: f_00008-5-1 loss: 0.715303  [   64/  130]
train() client id: f_00008-5-2 loss: 0.715432  [   96/  130]
train() client id: f_00008-5-3 loss: 0.597926  [  128/  130]
train() client id: f_00008-6-0 loss: 0.521088  [   32/  130]
train() client id: f_00008-6-1 loss: 0.684802  [   64/  130]
train() client id: f_00008-6-2 loss: 0.753352  [   96/  130]
train() client id: f_00008-6-3 loss: 0.629295  [  128/  130]
train() client id: f_00008-7-0 loss: 0.610227  [   32/  130]
train() client id: f_00008-7-1 loss: 0.717188  [   64/  130]
train() client id: f_00008-7-2 loss: 0.661756  [   96/  130]
train() client id: f_00008-7-3 loss: 0.630344  [  128/  130]
train() client id: f_00008-8-0 loss: 0.650926  [   32/  130]
train() client id: f_00008-8-1 loss: 0.614696  [   64/  130]
train() client id: f_00008-8-2 loss: 0.699109  [   96/  130]
train() client id: f_00008-8-3 loss: 0.601187  [  128/  130]
train() client id: f_00008-9-0 loss: 0.651720  [   32/  130]
train() client id: f_00008-9-1 loss: 0.597571  [   64/  130]
train() client id: f_00008-9-2 loss: 0.637680  [   96/  130]
train() client id: f_00008-9-3 loss: 0.700307  [  128/  130]
train() client id: f_00008-10-0 loss: 0.598075  [   32/  130]
train() client id: f_00008-10-1 loss: 0.688194  [   64/  130]
train() client id: f_00008-10-2 loss: 0.588387  [   96/  130]
train() client id: f_00008-10-3 loss: 0.736502  [  128/  130]
train() client id: f_00008-11-0 loss: 0.585860  [   32/  130]
train() client id: f_00008-11-1 loss: 0.604628  [   64/  130]
train() client id: f_00008-11-2 loss: 0.714912  [   96/  130]
train() client id: f_00008-11-3 loss: 0.671873  [  128/  130]
train() client id: f_00009-0-0 loss: 1.275599  [   32/  118]
train() client id: f_00009-0-1 loss: 1.009892  [   64/  118]
train() client id: f_00009-0-2 loss: 1.064422  [   96/  118]
train() client id: f_00009-1-0 loss: 1.042011  [   32/  118]
train() client id: f_00009-1-1 loss: 1.102665  [   64/  118]
train() client id: f_00009-1-2 loss: 0.954277  [   96/  118]
train() client id: f_00009-2-0 loss: 0.771467  [   32/  118]
train() client id: f_00009-2-1 loss: 0.918962  [   64/  118]
train() client id: f_00009-2-2 loss: 1.065033  [   96/  118]
train() client id: f_00009-3-0 loss: 1.005404  [   32/  118]
train() client id: f_00009-3-1 loss: 0.796155  [   64/  118]
train() client id: f_00009-3-2 loss: 1.010256  [   96/  118]
train() client id: f_00009-4-0 loss: 0.834437  [   32/  118]
train() client id: f_00009-4-1 loss: 0.927623  [   64/  118]
train() client id: f_00009-4-2 loss: 1.046978  [   96/  118]
train() client id: f_00009-5-0 loss: 0.920272  [   32/  118]
train() client id: f_00009-5-1 loss: 0.907502  [   64/  118]
train() client id: f_00009-5-2 loss: 0.917575  [   96/  118]
train() client id: f_00009-6-0 loss: 0.933434  [   32/  118]
train() client id: f_00009-6-1 loss: 0.785605  [   64/  118]
train() client id: f_00009-6-2 loss: 0.992299  [   96/  118]
train() client id: f_00009-7-0 loss: 0.873719  [   32/  118]
train() client id: f_00009-7-1 loss: 0.773212  [   64/  118]
train() client id: f_00009-7-2 loss: 0.939082  [   96/  118]
train() client id: f_00009-8-0 loss: 0.778834  [   32/  118]
train() client id: f_00009-8-1 loss: 0.799834  [   64/  118]
train() client id: f_00009-8-2 loss: 0.868778  [   96/  118]
train() client id: f_00009-9-0 loss: 0.783232  [   32/  118]
train() client id: f_00009-9-1 loss: 0.818309  [   64/  118]
train() client id: f_00009-9-2 loss: 0.860464  [   96/  118]
train() client id: f_00009-10-0 loss: 0.628654  [   32/  118]
train() client id: f_00009-10-1 loss: 0.944050  [   64/  118]
train() client id: f_00009-10-2 loss: 0.913328  [   96/  118]
train() client id: f_00009-11-0 loss: 0.784257  [   32/  118]
train() client id: f_00009-11-1 loss: 0.849763  [   64/  118]
train() client id: f_00009-11-2 loss: 0.740762  [   96/  118]
At round 37 accuracy: 0.649867374005305
At round 37 training accuracy: 0.5814889336016097
At round 37 training loss: 0.8377708732916915
update_location
xs = -4.528292 86.001589 95.045120 -95.943528 -5.103519 -20.217951 -137.215960 158.375741 -1.680116 79.695607 
ys = 172.587959 15.555839 1.320614 22.544824 9.350187 2.814151 -3.124922 -19.177652 -102.154970 4.001482 
xs mean: 15.442869159412865
ys mean: 10.371751218646876
dists_uav = 199.517190 132.809102 137.968543 140.404522 100.565759 102.062162 169.817504 188.283450 142.963144 127.935146 
uav_gains = -107.651596 -103.081576 -103.495984 -103.686407 -100.061271 -100.221640 -105.772007 -106.948633 -103.883001 -102.675271 
uav_gains_db_mean: -103.74773867665749
dists_bs = 170.488771 305.850078 321.074595 171.733824 237.286061 231.528507 182.088224 385.803506 326.886310 306.774095 
bs_gains = -102.054541 -109.161300 -109.752026 -102.143023 -106.074682 -105.775984 -102.854951 -111.985312 -109.970168 -109.197982 
bs_gains_db_mean: -106.89699680604471
Round 38
-------------------------------
ene_coms = [0.00920619 0.01011013 0.00736949 0.00743788 0.008414   0.00827854
 0.00827677 0.00883605 0.01066673 0.01013419]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.96238606 12.39820565  5.84184714  2.10059455 14.2664732   6.87220017
  2.6216591   8.4053814   6.16956517  5.60699219]
obj_prev = 70.24530462721496
eta_min = 4.655476842164401e-16	eta_max = 0.9417406655652466
af = 14.80883058640876	bf = 1.3456664750646534	zeta = 16.289713645049638	eta = 0.9090909090909091
af = 14.80883058640876	bf = 1.3456664750646534	zeta = 30.32808659041764	eta = 0.4882876650414111
af = 14.80883058640876	bf = 1.3456664750646534	zeta = 23.375213560087854	eta = 0.6335270712432842
af = 14.80883058640876	bf = 1.3456664750646534	zeta = 22.116763021014663	eta = 0.6695749541801334
af = 14.80883058640876	bf = 1.3456664750646534	zeta = 22.049383543412958	eta = 0.6716210708227603
af = 14.80883058640876	bf = 1.3456664750646534	zeta = 22.04917383743029	eta = 0.671627458497767
eta = 0.671627458497767
ene_coms = [0.00920619 0.01011013 0.00736949 0.00743788 0.008414   0.00827854
 0.00827677 0.00883605 0.01066673 0.01013419]
ene_comp = [0.03284625 0.06908141 0.03232488 0.01120944 0.07976947 0.03805994
 0.01407697 0.04666253 0.03388899 0.03076078]
ene_total = [1.94218886 3.65745511 1.8332818  0.86122502 4.0727467  2.14013956
 1.03240556 2.56319767 2.0578028  1.88873076]
ti_comp = [0.51635831 0.50731888 0.53472527 0.53404137 0.52428025 0.52563476
 0.52565251 0.52005975 0.50175286 0.50707834]
ti_coms = [0.0920619  0.10110132 0.07369494 0.07437884 0.08413996 0.08278545
 0.0827677  0.08836046 0.10666735 0.10134187]
t_total = [28.09984055 28.09984055 28.09984055 28.09984055 28.09984055 28.09984055
 28.09984055 28.09984055 28.09984055 28.09984055]
ene_coms = [0.00920619 0.01011013 0.00736949 0.00743788 0.008414   0.00827854
 0.00827677 0.00883605 0.01066673 0.01013419]
ene_comp = [8.30682751e-06 8.00573960e-05 7.38294329e-06 3.08660787e-07
 1.15415245e-04 1.24714316e-05 6.30971424e-07 2.34789408e-05
 9.66220483e-06 7.07492432e-06]
ene_total = [0.42557083 0.47063313 0.34070048 0.34353235 0.39393019 0.38291995
 0.38229112 0.40917647 0.49308858 0.4683734 ]
optimize_network iter = 0 obj = 4.110216494471539
eta = 0.671627458497767
freqs = [31805679.16364105 68084796.39263307 30225693.48576584 10494913.21748239
 76075221.41835228 36203793.32321751 13389991.94149501 44862666.74185053
 33770601.33400602 30331385.28763921]
eta_min = 0.6716274584977686	eta_max = 0.6890890865711947
af = 0.010088547984090354	bf = 1.3456664750646534	zeta = 0.011097402782499391	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.00920619 0.01011013 0.00736949 0.00743788 0.008414   0.00827854
 0.00827677 0.00883605 0.01066673 0.01013419]
ene_comp = [1.98900214e-06 1.91690909e-05 1.76778559e-06 7.39063095e-08
 2.76352396e-05 2.98618264e-06 1.51080964e-07 5.62184101e-06
 2.31353619e-06 1.69403296e-06]
ene_total = [1.53614955 1.68981532 1.22970677 1.24083332 1.40827057 1.381562
 1.38079292 1.47500648 1.77985831 1.69091293]
ti_comp = [0.48400479 0.47496536 0.50237175 0.50168785 0.49192673 0.49328124
 0.49329899 0.48770623 0.46939934 0.47472482]
ti_coms = [0.0920619  0.10110132 0.07369494 0.07437884 0.08413996 0.08278545
 0.0827677  0.08836046 0.10666735 0.10134187]
t_total = [28.09984055 28.09984055 28.09984055 28.09984055 28.09984055 28.09984055
 28.09984055 28.09984055 28.09984055 28.09984055]
ene_coms = [0.00920619 0.01011013 0.00736949 0.00743788 0.008414   0.00827854
 0.00827677 0.00883605 0.01066673 0.01013419]
ene_comp = [7.74098220e-06 7.47820712e-05 6.84854612e-06 2.86366327e-07
 1.07336439e-04 1.15945292e-05 5.86603403e-07 2.18587959e-05
 9.03917633e-06 6.60915437e-06]
ene_total = [0.44944448 0.49680788 0.35980911 0.362825   0.41566034 0.40438303
 0.4037595  0.43207794 0.52075142 0.49465584]
optimize_network iter = 1 obj = 4.340174537770971
eta = 0.6890890865711947
freqs = [31743791.8554347  68033348.80466662 30097774.96003791 10451359.52726642
 75850582.29495808 36090772.0945589  13348161.41773808 44754080.93772856
 33770601.33400604 30309445.08017885]
eta_min = 0.689089086571198	eta_max = 0.6890890865711955
af = 0.010046478100941619	bf = 1.3456664750646534	zeta = 0.011051125911035782	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00920619 0.01011013 0.00736949 0.00743788 0.008414   0.00827854
 0.00827677 0.00883605 0.01066673 0.01013419]
ene_comp = [1.98126929e-06 1.91401320e-05 1.75285432e-06 7.32941627e-08
 2.74722748e-05 2.96756718e-06 1.50138481e-07 5.59465968e-06
 2.31353619e-06 1.69158309e-06]
ene_total = [1.53614826 1.68981048 1.22970428 1.24083322 1.40824338 1.3815589
 1.38079277 1.47500195 1.77985831 1.69091252]
ti_comp = [0.48400479 0.47496536 0.50237175 0.50168785 0.49192673 0.49328124
 0.49329899 0.48770623 0.46939934 0.47472482]
ti_coms = [0.0920619  0.10110132 0.07369494 0.07437884 0.08413996 0.08278545
 0.0827677  0.08836046 0.10666735 0.10134187]
t_total = [28.09984055 28.09984055 28.09984055 28.09984055 28.09984055 28.09984055
 28.09984055 28.09984055 28.09984055 28.09984055]
ene_coms = [0.00920619 0.01011013 0.00736949 0.00743788 0.008414   0.00827854
 0.00827677 0.00883605 0.01066673 0.01013419]
ene_comp = [7.74098220e-06 7.47820712e-05 6.84854612e-06 2.86366327e-07
 1.07336439e-04 1.15945292e-05 5.86603403e-07 2.18587959e-05
 9.03917633e-06 6.60915437e-06]
ene_total = [0.44944448 0.49680788 0.35980911 0.362825   0.41566034 0.40438303
 0.4037595  0.43207794 0.52075142 0.49465584]
optimize_network iter = 2 obj = 4.340174537770982
eta = 0.6890890865711955
freqs = [31743791.85543469 68033348.8046666  30097774.9600379  10451359.52726641
 75850582.29495806 36090772.09455889 13348161.41773808 44754080.93772855
 33770601.33400603 30309445.08017884]
Done!
ene_coms = [0.00920619 0.01011013 0.00736949 0.00743788 0.008414   0.00827854
 0.00827677 0.00883605 0.01066673 0.01013419]
ene_comp = [7.61797251e-06 7.35937311e-05 6.73971787e-06 2.81815763e-07
 1.05630787e-04 1.14102840e-05 5.77281859e-07 2.15114441e-05
 8.89553741e-06 6.50413022e-06]
ene_total = [0.00921381 0.01018373 0.00737623 0.00743817 0.00851963 0.00828996
 0.00827735 0.00885756 0.01067563 0.01014069]
At round 38 energy consumption: 0.08897274013067838
At round 38 eta: 0.6890890865711955
At round 38 a_n: 15.165860671420319
At round 38 local rounds: 12.193767607432306
At round 38 global rounds: 48.77879809417867
gradient difference: 0.4396577775478363
train() client id: f_00000-0-0 loss: 1.152168  [   32/  126]
train() client id: f_00000-0-1 loss: 0.907610  [   64/  126]
train() client id: f_00000-0-2 loss: 1.020427  [   96/  126]
train() client id: f_00000-1-0 loss: 1.013856  [   32/  126]
train() client id: f_00000-1-1 loss: 0.927205  [   64/  126]
train() client id: f_00000-1-2 loss: 1.139112  [   96/  126]
train() client id: f_00000-2-0 loss: 0.984343  [   32/  126]
train() client id: f_00000-2-1 loss: 0.892341  [   64/  126]
train() client id: f_00000-2-2 loss: 0.993657  [   96/  126]
train() client id: f_00000-3-0 loss: 0.887033  [   32/  126]
train() client id: f_00000-3-1 loss: 0.992404  [   64/  126]
train() client id: f_00000-3-2 loss: 0.823767  [   96/  126]
train() client id: f_00000-4-0 loss: 0.917989  [   32/  126]
train() client id: f_00000-4-1 loss: 0.912469  [   64/  126]
train() client id: f_00000-4-2 loss: 0.806455  [   96/  126]
train() client id: f_00000-5-0 loss: 0.975826  [   32/  126]
train() client id: f_00000-5-1 loss: 0.872239  [   64/  126]
train() client id: f_00000-5-2 loss: 0.833769  [   96/  126]
train() client id: f_00000-6-0 loss: 0.781653  [   32/  126]
train() client id: f_00000-6-1 loss: 0.950757  [   64/  126]
train() client id: f_00000-6-2 loss: 0.819217  [   96/  126]
train() client id: f_00000-7-0 loss: 0.900967  [   32/  126]
train() client id: f_00000-7-1 loss: 0.877592  [   64/  126]
train() client id: f_00000-7-2 loss: 0.754841  [   96/  126]
train() client id: f_00000-8-0 loss: 0.898828  [   32/  126]
train() client id: f_00000-8-1 loss: 0.847808  [   64/  126]
train() client id: f_00000-8-2 loss: 0.890406  [   96/  126]
train() client id: f_00000-9-0 loss: 0.808037  [   32/  126]
train() client id: f_00000-9-1 loss: 0.895551  [   64/  126]
train() client id: f_00000-9-2 loss: 0.845162  [   96/  126]
train() client id: f_00000-10-0 loss: 0.953341  [   32/  126]
train() client id: f_00000-10-1 loss: 0.764327  [   64/  126]
train() client id: f_00000-10-2 loss: 0.882199  [   96/  126]
train() client id: f_00000-11-0 loss: 0.847622  [   32/  126]
train() client id: f_00000-11-1 loss: 0.837419  [   64/  126]
train() client id: f_00000-11-2 loss: 0.913388  [   96/  126]
train() client id: f_00001-0-0 loss: 0.546032  [   32/  265]
train() client id: f_00001-0-1 loss: 0.455754  [   64/  265]
train() client id: f_00001-0-2 loss: 0.505102  [   96/  265]
train() client id: f_00001-0-3 loss: 0.425020  [  128/  265]
train() client id: f_00001-0-4 loss: 0.478007  [  160/  265]
train() client id: f_00001-0-5 loss: 0.441193  [  192/  265]
train() client id: f_00001-0-6 loss: 0.367839  [  224/  265]
train() client id: f_00001-0-7 loss: 0.435993  [  256/  265]
train() client id: f_00001-1-0 loss: 0.445370  [   32/  265]
train() client id: f_00001-1-1 loss: 0.521060  [   64/  265]
train() client id: f_00001-1-2 loss: 0.453744  [   96/  265]
train() client id: f_00001-1-3 loss: 0.447214  [  128/  265]
train() client id: f_00001-1-4 loss: 0.495907  [  160/  265]
train() client id: f_00001-1-5 loss: 0.374766  [  192/  265]
train() client id: f_00001-1-6 loss: 0.480519  [  224/  265]
train() client id: f_00001-1-7 loss: 0.390685  [  256/  265]
train() client id: f_00001-2-0 loss: 0.380178  [   32/  265]
train() client id: f_00001-2-1 loss: 0.385561  [   64/  265]
train() client id: f_00001-2-2 loss: 0.552140  [   96/  265]
train() client id: f_00001-2-3 loss: 0.428618  [  128/  265]
train() client id: f_00001-2-4 loss: 0.426699  [  160/  265]
train() client id: f_00001-2-5 loss: 0.388001  [  192/  265]
train() client id: f_00001-2-6 loss: 0.459452  [  224/  265]
train() client id: f_00001-2-7 loss: 0.501920  [  256/  265]
train() client id: f_00001-3-0 loss: 0.405190  [   32/  265]
train() client id: f_00001-3-1 loss: 0.368341  [   64/  265]
train() client id: f_00001-3-2 loss: 0.455109  [   96/  265]
train() client id: f_00001-3-3 loss: 0.402757  [  128/  265]
train() client id: f_00001-3-4 loss: 0.372257  [  160/  265]
train() client id: f_00001-3-5 loss: 0.429734  [  192/  265]
train() client id: f_00001-3-6 loss: 0.460470  [  224/  265]
train() client id: f_00001-3-7 loss: 0.523724  [  256/  265]
train() client id: f_00001-4-0 loss: 0.348537  [   32/  265]
train() client id: f_00001-4-1 loss: 0.468815  [   64/  265]
train() client id: f_00001-4-2 loss: 0.497384  [   96/  265]
train() client id: f_00001-4-3 loss: 0.394521  [  128/  265]
train() client id: f_00001-4-4 loss: 0.507199  [  160/  265]
train() client id: f_00001-4-5 loss: 0.463408  [  192/  265]
train() client id: f_00001-4-6 loss: 0.393590  [  224/  265]
train() client id: f_00001-4-7 loss: 0.363154  [  256/  265]
train() client id: f_00001-5-0 loss: 0.332950  [   32/  265]
train() client id: f_00001-5-1 loss: 0.348804  [   64/  265]
train() client id: f_00001-5-2 loss: 0.388898  [   96/  265]
train() client id: f_00001-5-3 loss: 0.413975  [  128/  265]
train() client id: f_00001-5-4 loss: 0.573417  [  160/  265]
train() client id: f_00001-5-5 loss: 0.340502  [  192/  265]
train() client id: f_00001-5-6 loss: 0.590585  [  224/  265]
train() client id: f_00001-5-7 loss: 0.410529  [  256/  265]
train() client id: f_00001-6-0 loss: 0.372293  [   32/  265]
train() client id: f_00001-6-1 loss: 0.399777  [   64/  265]
train() client id: f_00001-6-2 loss: 0.480099  [   96/  265]
train() client id: f_00001-6-3 loss: 0.437474  [  128/  265]
train() client id: f_00001-6-4 loss: 0.380695  [  160/  265]
train() client id: f_00001-6-5 loss: 0.499373  [  192/  265]
train() client id: f_00001-6-6 loss: 0.413336  [  224/  265]
train() client id: f_00001-6-7 loss: 0.468874  [  256/  265]
train() client id: f_00001-7-0 loss: 0.462137  [   32/  265]
train() client id: f_00001-7-1 loss: 0.407351  [   64/  265]
train() client id: f_00001-7-2 loss: 0.527058  [   96/  265]
train() client id: f_00001-7-3 loss: 0.325310  [  128/  265]
train() client id: f_00001-7-4 loss: 0.432279  [  160/  265]
train() client id: f_00001-7-5 loss: 0.503085  [  192/  265]
train() client id: f_00001-7-6 loss: 0.346779  [  224/  265]
train() client id: f_00001-7-7 loss: 0.427696  [  256/  265]
train() client id: f_00001-8-0 loss: 0.393320  [   32/  265]
train() client id: f_00001-8-1 loss: 0.413231  [   64/  265]
train() client id: f_00001-8-2 loss: 0.311417  [   96/  265]
train() client id: f_00001-8-3 loss: 0.425358  [  128/  265]
train() client id: f_00001-8-4 loss: 0.424297  [  160/  265]
train() client id: f_00001-8-5 loss: 0.344320  [  192/  265]
train() client id: f_00001-8-6 loss: 0.571680  [  224/  265]
train() client id: f_00001-8-7 loss: 0.539609  [  256/  265]
train() client id: f_00001-9-0 loss: 0.423453  [   32/  265]
train() client id: f_00001-9-1 loss: 0.348273  [   64/  265]
train() client id: f_00001-9-2 loss: 0.442343  [   96/  265]
train() client id: f_00001-9-3 loss: 0.373741  [  128/  265]
train() client id: f_00001-9-4 loss: 0.480482  [  160/  265]
train() client id: f_00001-9-5 loss: 0.416196  [  192/  265]
train() client id: f_00001-9-6 loss: 0.465947  [  224/  265]
train() client id: f_00001-9-7 loss: 0.474702  [  256/  265]
train() client id: f_00001-10-0 loss: 0.632586  [   32/  265]
train() client id: f_00001-10-1 loss: 0.336953  [   64/  265]
train() client id: f_00001-10-2 loss: 0.373797  [   96/  265]
train() client id: f_00001-10-3 loss: 0.423531  [  128/  265]
train() client id: f_00001-10-4 loss: 0.516648  [  160/  265]
train() client id: f_00001-10-5 loss: 0.339085  [  192/  265]
train() client id: f_00001-10-6 loss: 0.479680  [  224/  265]
train() client id: f_00001-10-7 loss: 0.318811  [  256/  265]
train() client id: f_00001-11-0 loss: 0.404716  [   32/  265]
train() client id: f_00001-11-1 loss: 0.528252  [   64/  265]
train() client id: f_00001-11-2 loss: 0.448529  [   96/  265]
train() client id: f_00001-11-3 loss: 0.408154  [  128/  265]
train() client id: f_00001-11-4 loss: 0.332332  [  160/  265]
train() client id: f_00001-11-5 loss: 0.453285  [  192/  265]
train() client id: f_00001-11-6 loss: 0.507097  [  224/  265]
train() client id: f_00001-11-7 loss: 0.334034  [  256/  265]
train() client id: f_00002-0-0 loss: 1.382859  [   32/  124]
train() client id: f_00002-0-1 loss: 1.104665  [   64/  124]
train() client id: f_00002-0-2 loss: 1.212989  [   96/  124]
train() client id: f_00002-1-0 loss: 1.188377  [   32/  124]
train() client id: f_00002-1-1 loss: 1.095508  [   64/  124]
train() client id: f_00002-1-2 loss: 1.295139  [   96/  124]
train() client id: f_00002-2-0 loss: 1.096098  [   32/  124]
train() client id: f_00002-2-1 loss: 1.493234  [   64/  124]
train() client id: f_00002-2-2 loss: 1.024340  [   96/  124]
train() client id: f_00002-3-0 loss: 1.016758  [   32/  124]
train() client id: f_00002-3-1 loss: 1.097607  [   64/  124]
train() client id: f_00002-3-2 loss: 1.246969  [   96/  124]
train() client id: f_00002-4-0 loss: 1.150254  [   32/  124]
train() client id: f_00002-4-1 loss: 1.072623  [   64/  124]
train() client id: f_00002-4-2 loss: 0.995204  [   96/  124]
train() client id: f_00002-5-0 loss: 1.107414  [   32/  124]
train() client id: f_00002-5-1 loss: 1.052493  [   64/  124]
train() client id: f_00002-5-2 loss: 1.153245  [   96/  124]
train() client id: f_00002-6-0 loss: 1.111494  [   32/  124]
train() client id: f_00002-6-1 loss: 1.097582  [   64/  124]
train() client id: f_00002-6-2 loss: 1.127272  [   96/  124]
train() client id: f_00002-7-0 loss: 1.177782  [   32/  124]
train() client id: f_00002-7-1 loss: 1.152944  [   64/  124]
train() client id: f_00002-7-2 loss: 1.010260  [   96/  124]
train() client id: f_00002-8-0 loss: 1.184559  [   32/  124]
train() client id: f_00002-8-1 loss: 0.991266  [   64/  124]
train() client id: f_00002-8-2 loss: 0.975073  [   96/  124]
train() client id: f_00002-9-0 loss: 0.887829  [   32/  124]
train() client id: f_00002-9-1 loss: 1.133917  [   64/  124]
train() client id: f_00002-9-2 loss: 1.079511  [   96/  124]
train() client id: f_00002-10-0 loss: 1.078200  [   32/  124]
train() client id: f_00002-10-1 loss: 1.157572  [   64/  124]
train() client id: f_00002-10-2 loss: 1.121460  [   96/  124]
train() client id: f_00002-11-0 loss: 0.990186  [   32/  124]
train() client id: f_00002-11-1 loss: 1.267927  [   64/  124]
train() client id: f_00002-11-2 loss: 1.024005  [   96/  124]
train() client id: f_00003-0-0 loss: 0.558791  [   32/   43]
train() client id: f_00003-1-0 loss: 0.647192  [   32/   43]
train() client id: f_00003-2-0 loss: 0.673984  [   32/   43]
train() client id: f_00003-3-0 loss: 0.814758  [   32/   43]
train() client id: f_00003-4-0 loss: 0.673125  [   32/   43]
train() client id: f_00003-5-0 loss: 0.649693  [   32/   43]
train() client id: f_00003-6-0 loss: 0.783848  [   32/   43]
train() client id: f_00003-7-0 loss: 0.773995  [   32/   43]
train() client id: f_00003-8-0 loss: 0.793793  [   32/   43]
train() client id: f_00003-9-0 loss: 0.671907  [   32/   43]
train() client id: f_00003-10-0 loss: 0.810819  [   32/   43]
train() client id: f_00003-11-0 loss: 0.701097  [   32/   43]
train() client id: f_00004-0-0 loss: 0.915408  [   32/  306]
train() client id: f_00004-0-1 loss: 0.921871  [   64/  306]
train() client id: f_00004-0-2 loss: 0.796812  [   96/  306]
train() client id: f_00004-0-3 loss: 0.845236  [  128/  306]
train() client id: f_00004-0-4 loss: 0.840753  [  160/  306]
train() client id: f_00004-0-5 loss: 0.967125  [  192/  306]
train() client id: f_00004-0-6 loss: 1.000527  [  224/  306]
train() client id: f_00004-0-7 loss: 0.931384  [  256/  306]
train() client id: f_00004-0-8 loss: 0.788536  [  288/  306]
train() client id: f_00004-1-0 loss: 0.931609  [   32/  306]
train() client id: f_00004-1-1 loss: 0.839186  [   64/  306]
train() client id: f_00004-1-2 loss: 0.788997  [   96/  306]
train() client id: f_00004-1-3 loss: 0.877193  [  128/  306]
train() client id: f_00004-1-4 loss: 0.876687  [  160/  306]
train() client id: f_00004-1-5 loss: 0.849327  [  192/  306]
train() client id: f_00004-1-6 loss: 0.827455  [  224/  306]
train() client id: f_00004-1-7 loss: 1.026352  [  256/  306]
train() client id: f_00004-1-8 loss: 0.916528  [  288/  306]
train() client id: f_00004-2-0 loss: 0.812883  [   32/  306]
train() client id: f_00004-2-1 loss: 0.871048  [   64/  306]
train() client id: f_00004-2-2 loss: 0.944684  [   96/  306]
train() client id: f_00004-2-3 loss: 0.747357  [  128/  306]
train() client id: f_00004-2-4 loss: 0.901400  [  160/  306]
train() client id: f_00004-2-5 loss: 0.831775  [  192/  306]
train() client id: f_00004-2-6 loss: 0.963716  [  224/  306]
train() client id: f_00004-2-7 loss: 0.872517  [  256/  306]
train() client id: f_00004-2-8 loss: 0.945061  [  288/  306]
train() client id: f_00004-3-0 loss: 0.899068  [   32/  306]
train() client id: f_00004-3-1 loss: 0.979602  [   64/  306]
train() client id: f_00004-3-2 loss: 0.800373  [   96/  306]
train() client id: f_00004-3-3 loss: 0.821971  [  128/  306]
train() client id: f_00004-3-4 loss: 0.915899  [  160/  306]
train() client id: f_00004-3-5 loss: 0.929567  [  192/  306]
train() client id: f_00004-3-6 loss: 0.809819  [  224/  306]
train() client id: f_00004-3-7 loss: 0.955580  [  256/  306]
train() client id: f_00004-3-8 loss: 0.843440  [  288/  306]
train() client id: f_00004-4-0 loss: 0.855074  [   32/  306]
train() client id: f_00004-4-1 loss: 0.839920  [   64/  306]
train() client id: f_00004-4-2 loss: 1.013429  [   96/  306]
train() client id: f_00004-4-3 loss: 0.798699  [  128/  306]
train() client id: f_00004-4-4 loss: 1.063818  [  160/  306]
train() client id: f_00004-4-5 loss: 0.764056  [  192/  306]
train() client id: f_00004-4-6 loss: 0.821690  [  224/  306]
train() client id: f_00004-4-7 loss: 0.785802  [  256/  306]
train() client id: f_00004-4-8 loss: 1.006461  [  288/  306]
train() client id: f_00004-5-0 loss: 0.851028  [   32/  306]
train() client id: f_00004-5-1 loss: 0.833859  [   64/  306]
train() client id: f_00004-5-2 loss: 0.839769  [   96/  306]
train() client id: f_00004-5-3 loss: 0.883301  [  128/  306]
train() client id: f_00004-5-4 loss: 0.914620  [  160/  306]
train() client id: f_00004-5-5 loss: 0.855965  [  192/  306]
train() client id: f_00004-5-6 loss: 0.955476  [  224/  306]
train() client id: f_00004-5-7 loss: 0.913683  [  256/  306]
train() client id: f_00004-5-8 loss: 0.886567  [  288/  306]
train() client id: f_00004-6-0 loss: 0.967945  [   32/  306]
train() client id: f_00004-6-1 loss: 0.861497  [   64/  306]
train() client id: f_00004-6-2 loss: 0.793379  [   96/  306]
train() client id: f_00004-6-3 loss: 0.814812  [  128/  306]
train() client id: f_00004-6-4 loss: 1.034344  [  160/  306]
train() client id: f_00004-6-5 loss: 0.874296  [  192/  306]
train() client id: f_00004-6-6 loss: 0.775895  [  224/  306]
train() client id: f_00004-6-7 loss: 0.868372  [  256/  306]
train() client id: f_00004-6-8 loss: 0.858103  [  288/  306]
train() client id: f_00004-7-0 loss: 0.815527  [   32/  306]
train() client id: f_00004-7-1 loss: 0.862973  [   64/  306]
train() client id: f_00004-7-2 loss: 0.941247  [   96/  306]
train() client id: f_00004-7-3 loss: 0.828527  [  128/  306]
train() client id: f_00004-7-4 loss: 0.795570  [  160/  306]
train() client id: f_00004-7-5 loss: 0.816464  [  192/  306]
train() client id: f_00004-7-6 loss: 0.890395  [  224/  306]
train() client id: f_00004-7-7 loss: 0.855960  [  256/  306]
train() client id: f_00004-7-8 loss: 1.027974  [  288/  306]
train() client id: f_00004-8-0 loss: 0.929009  [   32/  306]
train() client id: f_00004-8-1 loss: 0.757892  [   64/  306]
train() client id: f_00004-8-2 loss: 0.961703  [   96/  306]
train() client id: f_00004-8-3 loss: 1.045030  [  128/  306]
train() client id: f_00004-8-4 loss: 0.864802  [  160/  306]
train() client id: f_00004-8-5 loss: 0.872731  [  192/  306]
train() client id: f_00004-8-6 loss: 0.884346  [  224/  306]
train() client id: f_00004-8-7 loss: 0.761076  [  256/  306]
train() client id: f_00004-8-8 loss: 0.793176  [  288/  306]
train() client id: f_00004-9-0 loss: 0.768362  [   32/  306]
train() client id: f_00004-9-1 loss: 0.926695  [   64/  306]
train() client id: f_00004-9-2 loss: 0.878898  [   96/  306]
train() client id: f_00004-9-3 loss: 0.753010  [  128/  306]
train() client id: f_00004-9-4 loss: 0.824723  [  160/  306]
train() client id: f_00004-9-5 loss: 1.005803  [  192/  306]
train() client id: f_00004-9-6 loss: 0.857450  [  224/  306]
train() client id: f_00004-9-7 loss: 0.939131  [  256/  306]
train() client id: f_00004-9-8 loss: 0.870536  [  288/  306]
train() client id: f_00004-10-0 loss: 0.943852  [   32/  306]
train() client id: f_00004-10-1 loss: 0.822461  [   64/  306]
train() client id: f_00004-10-2 loss: 0.839588  [   96/  306]
train() client id: f_00004-10-3 loss: 0.899533  [  128/  306]
train() client id: f_00004-10-4 loss: 0.988712  [  160/  306]
train() client id: f_00004-10-5 loss: 0.783309  [  192/  306]
train() client id: f_00004-10-6 loss: 0.974987  [  224/  306]
train() client id: f_00004-10-7 loss: 0.809232  [  256/  306]
train() client id: f_00004-10-8 loss: 0.867974  [  288/  306]
train() client id: f_00004-11-0 loss: 0.857416  [   32/  306]
train() client id: f_00004-11-1 loss: 0.864895  [   64/  306]
train() client id: f_00004-11-2 loss: 0.849136  [   96/  306]
train() client id: f_00004-11-3 loss: 0.921814  [  128/  306]
train() client id: f_00004-11-4 loss: 0.841618  [  160/  306]
train() client id: f_00004-11-5 loss: 0.992507  [  192/  306]
train() client id: f_00004-11-6 loss: 0.799086  [  224/  306]
train() client id: f_00004-11-7 loss: 0.977627  [  256/  306]
train() client id: f_00004-11-8 loss: 0.694100  [  288/  306]
train() client id: f_00005-0-0 loss: 0.371071  [   32/  146]
train() client id: f_00005-0-1 loss: 0.768691  [   64/  146]
train() client id: f_00005-0-2 loss: 0.761967  [   96/  146]
train() client id: f_00005-0-3 loss: 0.713294  [  128/  146]
train() client id: f_00005-1-0 loss: 0.723465  [   32/  146]
train() client id: f_00005-1-1 loss: 0.909560  [   64/  146]
train() client id: f_00005-1-2 loss: 0.622619  [   96/  146]
train() client id: f_00005-1-3 loss: 0.732454  [  128/  146]
train() client id: f_00005-2-0 loss: 0.845339  [   32/  146]
train() client id: f_00005-2-1 loss: 0.824775  [   64/  146]
train() client id: f_00005-2-2 loss: 0.557477  [   96/  146]
train() client id: f_00005-2-3 loss: 0.576133  [  128/  146]
train() client id: f_00005-3-0 loss: 0.729792  [   32/  146]
train() client id: f_00005-3-1 loss: 0.990798  [   64/  146]
train() client id: f_00005-3-2 loss: 0.526567  [   96/  146]
train() client id: f_00005-3-3 loss: 0.582104  [  128/  146]
train() client id: f_00005-4-0 loss: 0.721392  [   32/  146]
train() client id: f_00005-4-1 loss: 0.783287  [   64/  146]
train() client id: f_00005-4-2 loss: 0.559774  [   96/  146]
train() client id: f_00005-4-3 loss: 0.899687  [  128/  146]
train() client id: f_00005-5-0 loss: 0.891399  [   32/  146]
train() client id: f_00005-5-1 loss: 0.690870  [   64/  146]
train() client id: f_00005-5-2 loss: 0.524900  [   96/  146]
train() client id: f_00005-5-3 loss: 0.779484  [  128/  146]
train() client id: f_00005-6-0 loss: 0.482155  [   32/  146]
train() client id: f_00005-6-1 loss: 0.481380  [   64/  146]
train() client id: f_00005-6-2 loss: 1.151257  [   96/  146]
train() client id: f_00005-6-3 loss: 0.853672  [  128/  146]
train() client id: f_00005-7-0 loss: 0.613972  [   32/  146]
train() client id: f_00005-7-1 loss: 0.685164  [   64/  146]
train() client id: f_00005-7-2 loss: 0.785800  [   96/  146]
train() client id: f_00005-7-3 loss: 0.715465  [  128/  146]
train() client id: f_00005-8-0 loss: 0.445814  [   32/  146]
train() client id: f_00005-8-1 loss: 0.835693  [   64/  146]
train() client id: f_00005-8-2 loss: 0.607637  [   96/  146]
train() client id: f_00005-8-3 loss: 1.055170  [  128/  146]
train() client id: f_00005-9-0 loss: 0.824099  [   32/  146]
train() client id: f_00005-9-1 loss: 0.553622  [   64/  146]
train() client id: f_00005-9-2 loss: 0.694529  [   96/  146]
train() client id: f_00005-9-3 loss: 0.690648  [  128/  146]
train() client id: f_00005-10-0 loss: 0.667147  [   32/  146]
train() client id: f_00005-10-1 loss: 0.793721  [   64/  146]
train() client id: f_00005-10-2 loss: 0.584763  [   96/  146]
train() client id: f_00005-10-3 loss: 0.714460  [  128/  146]
train() client id: f_00005-11-0 loss: 0.561676  [   32/  146]
train() client id: f_00005-11-1 loss: 0.711534  [   64/  146]
train() client id: f_00005-11-2 loss: 0.571451  [   96/  146]
train() client id: f_00005-11-3 loss: 0.883389  [  128/  146]
train() client id: f_00006-0-0 loss: 0.511578  [   32/   54]
train() client id: f_00006-1-0 loss: 0.557297  [   32/   54]
train() client id: f_00006-2-0 loss: 0.491688  [   32/   54]
train() client id: f_00006-3-0 loss: 0.549737  [   32/   54]
train() client id: f_00006-4-0 loss: 0.564576  [   32/   54]
train() client id: f_00006-5-0 loss: 0.505115  [   32/   54]
train() client id: f_00006-6-0 loss: 0.546665  [   32/   54]
train() client id: f_00006-7-0 loss: 0.562981  [   32/   54]
train() client id: f_00006-8-0 loss: 0.479469  [   32/   54]
train() client id: f_00006-9-0 loss: 0.534697  [   32/   54]
train() client id: f_00006-10-0 loss: 0.517669  [   32/   54]
train() client id: f_00006-11-0 loss: 0.565504  [   32/   54]
train() client id: f_00007-0-0 loss: 0.598515  [   32/  179]
train() client id: f_00007-0-1 loss: 0.513114  [   64/  179]
train() client id: f_00007-0-2 loss: 0.979439  [   96/  179]
train() client id: f_00007-0-3 loss: 0.456939  [  128/  179]
train() client id: f_00007-0-4 loss: 0.592713  [  160/  179]
train() client id: f_00007-1-0 loss: 0.541846  [   32/  179]
train() client id: f_00007-1-1 loss: 0.770317  [   64/  179]
train() client id: f_00007-1-2 loss: 0.465989  [   96/  179]
train() client id: f_00007-1-3 loss: 0.512068  [  128/  179]
train() client id: f_00007-1-4 loss: 0.546507  [  160/  179]
train() client id: f_00007-2-0 loss: 0.487435  [   32/  179]
train() client id: f_00007-2-1 loss: 0.498742  [   64/  179]
train() client id: f_00007-2-2 loss: 0.710841  [   96/  179]
train() client id: f_00007-2-3 loss: 0.686603  [  128/  179]
train() client id: f_00007-2-4 loss: 0.705529  [  160/  179]
train() client id: f_00007-3-0 loss: 0.730390  [   32/  179]
train() client id: f_00007-3-1 loss: 0.534390  [   64/  179]
train() client id: f_00007-3-2 loss: 0.476331  [   96/  179]
train() client id: f_00007-3-3 loss: 0.632915  [  128/  179]
train() client id: f_00007-3-4 loss: 0.686008  [  160/  179]
train() client id: f_00007-4-0 loss: 0.684667  [   32/  179]
train() client id: f_00007-4-1 loss: 0.566930  [   64/  179]
train() client id: f_00007-4-2 loss: 0.530530  [   96/  179]
train() client id: f_00007-4-3 loss: 0.676418  [  128/  179]
train() client id: f_00007-4-4 loss: 0.576126  [  160/  179]
train() client id: f_00007-5-0 loss: 0.381791  [   32/  179]
train() client id: f_00007-5-1 loss: 0.629592  [   64/  179]
train() client id: f_00007-5-2 loss: 0.526384  [   96/  179]
train() client id: f_00007-5-3 loss: 0.801405  [  128/  179]
train() client id: f_00007-5-4 loss: 0.644298  [  160/  179]
train() client id: f_00007-6-0 loss: 0.515120  [   32/  179]
train() client id: f_00007-6-1 loss: 0.687160  [   64/  179]
train() client id: f_00007-6-2 loss: 0.519456  [   96/  179]
train() client id: f_00007-6-3 loss: 0.630209  [  128/  179]
train() client id: f_00007-6-4 loss: 0.458778  [  160/  179]
train() client id: f_00007-7-0 loss: 0.424969  [   32/  179]
train() client id: f_00007-7-1 loss: 0.523651  [   64/  179]
train() client id: f_00007-7-2 loss: 0.540078  [   96/  179]
train() client id: f_00007-7-3 loss: 0.879328  [  128/  179]
train() client id: f_00007-7-4 loss: 0.543009  [  160/  179]
train() client id: f_00007-8-0 loss: 0.507484  [   32/  179]
train() client id: f_00007-8-1 loss: 0.542914  [   64/  179]
train() client id: f_00007-8-2 loss: 0.518880  [   96/  179]
train() client id: f_00007-8-3 loss: 0.698965  [  128/  179]
train() client id: f_00007-8-4 loss: 0.671223  [  160/  179]
train() client id: f_00007-9-0 loss: 0.434245  [   32/  179]
train() client id: f_00007-9-1 loss: 0.417333  [   64/  179]
train() client id: f_00007-9-2 loss: 0.912494  [   96/  179]
train() client id: f_00007-9-3 loss: 0.685601  [  128/  179]
train() client id: f_00007-9-4 loss: 0.461518  [  160/  179]
train() client id: f_00007-10-0 loss: 0.564186  [   32/  179]
train() client id: f_00007-10-1 loss: 0.599264  [   64/  179]
train() client id: f_00007-10-2 loss: 0.535310  [   96/  179]
train() client id: f_00007-10-3 loss: 0.685555  [  128/  179]
train() client id: f_00007-10-4 loss: 0.519354  [  160/  179]
train() client id: f_00007-11-0 loss: 0.552496  [   32/  179]
train() client id: f_00007-11-1 loss: 0.513148  [   64/  179]
train() client id: f_00007-11-2 loss: 0.685977  [   96/  179]
train() client id: f_00007-11-3 loss: 0.515503  [  128/  179]
train() client id: f_00007-11-4 loss: 0.440269  [  160/  179]
train() client id: f_00008-0-0 loss: 0.771569  [   32/  130]
train() client id: f_00008-0-1 loss: 0.658755  [   64/  130]
train() client id: f_00008-0-2 loss: 0.605900  [   96/  130]
train() client id: f_00008-0-3 loss: 0.727891  [  128/  130]
train() client id: f_00008-1-0 loss: 0.683264  [   32/  130]
train() client id: f_00008-1-1 loss: 0.704151  [   64/  130]
train() client id: f_00008-1-2 loss: 0.779102  [   96/  130]
train() client id: f_00008-1-3 loss: 0.582745  [  128/  130]
train() client id: f_00008-2-0 loss: 0.746467  [   32/  130]
train() client id: f_00008-2-1 loss: 0.645743  [   64/  130]
train() client id: f_00008-2-2 loss: 0.699406  [   96/  130]
train() client id: f_00008-2-3 loss: 0.666420  [  128/  130]
train() client id: f_00008-3-0 loss: 0.622210  [   32/  130]
train() client id: f_00008-3-1 loss: 0.632233  [   64/  130]
train() client id: f_00008-3-2 loss: 0.649082  [   96/  130]
train() client id: f_00008-3-3 loss: 0.856435  [  128/  130]
train() client id: f_00008-4-0 loss: 0.705153  [   32/  130]
train() client id: f_00008-4-1 loss: 0.675584  [   64/  130]
train() client id: f_00008-4-2 loss: 0.596412  [   96/  130]
train() client id: f_00008-4-3 loss: 0.788451  [  128/  130]
train() client id: f_00008-5-0 loss: 0.673671  [   32/  130]
train() client id: f_00008-5-1 loss: 0.657791  [   64/  130]
train() client id: f_00008-5-2 loss: 0.632748  [   96/  130]
train() client id: f_00008-5-3 loss: 0.799476  [  128/  130]
train() client id: f_00008-6-0 loss: 0.524816  [   32/  130]
train() client id: f_00008-6-1 loss: 0.798642  [   64/  130]
train() client id: f_00008-6-2 loss: 0.684674  [   96/  130]
train() client id: f_00008-6-3 loss: 0.724068  [  128/  130]
train() client id: f_00008-7-0 loss: 0.699075  [   32/  130]
train() client id: f_00008-7-1 loss: 0.597698  [   64/  130]
train() client id: f_00008-7-2 loss: 0.739867  [   96/  130]
train() client id: f_00008-7-3 loss: 0.729861  [  128/  130]
train() client id: f_00008-8-0 loss: 0.769030  [   32/  130]
train() client id: f_00008-8-1 loss: 0.616340  [   64/  130]
train() client id: f_00008-8-2 loss: 0.775063  [   96/  130]
train() client id: f_00008-8-3 loss: 0.595698  [  128/  130]
train() client id: f_00008-9-0 loss: 0.726890  [   32/  130]
train() client id: f_00008-9-1 loss: 0.691917  [   64/  130]
train() client id: f_00008-9-2 loss: 0.637506  [   96/  130]
train() client id: f_00008-9-3 loss: 0.696813  [  128/  130]
train() client id: f_00008-10-0 loss: 0.750472  [   32/  130]
train() client id: f_00008-10-1 loss: 0.744211  [   64/  130]
train() client id: f_00008-10-2 loss: 0.745378  [   96/  130]
train() client id: f_00008-10-3 loss: 0.529431  [  128/  130]
train() client id: f_00008-11-0 loss: 0.742106  [   32/  130]
train() client id: f_00008-11-1 loss: 0.602160  [   64/  130]
train() client id: f_00008-11-2 loss: 0.645322  [   96/  130]
train() client id: f_00008-11-3 loss: 0.753793  [  128/  130]
train() client id: f_00009-0-0 loss: 1.139557  [   32/  118]
train() client id: f_00009-0-1 loss: 1.305455  [   64/  118]
train() client id: f_00009-0-2 loss: 1.085307  [   96/  118]
train() client id: f_00009-1-0 loss: 1.121739  [   32/  118]
train() client id: f_00009-1-1 loss: 1.057726  [   64/  118]
train() client id: f_00009-1-2 loss: 1.164931  [   96/  118]
train() client id: f_00009-2-0 loss: 0.951110  [   32/  118]
train() client id: f_00009-2-1 loss: 1.054051  [   64/  118]
train() client id: f_00009-2-2 loss: 1.171080  [   96/  118]
train() client id: f_00009-3-0 loss: 1.049633  [   32/  118]
train() client id: f_00009-3-1 loss: 0.927961  [   64/  118]
train() client id: f_00009-3-2 loss: 1.137156  [   96/  118]
train() client id: f_00009-4-0 loss: 1.043230  [   32/  118]
train() client id: f_00009-4-1 loss: 1.119832  [   64/  118]
train() client id: f_00009-4-2 loss: 0.999009  [   96/  118]
train() client id: f_00009-5-0 loss: 1.017508  [   32/  118]
train() client id: f_00009-5-1 loss: 0.903830  [   64/  118]
train() client id: f_00009-5-2 loss: 0.915499  [   96/  118]
train() client id: f_00009-6-0 loss: 0.875373  [   32/  118]
train() client id: f_00009-6-1 loss: 1.051973  [   64/  118]
train() client id: f_00009-6-2 loss: 0.908309  [   96/  118]
train() client id: f_00009-7-0 loss: 0.951278  [   32/  118]
train() client id: f_00009-7-1 loss: 0.966157  [   64/  118]
train() client id: f_00009-7-2 loss: 0.946110  [   96/  118]
train() client id: f_00009-8-0 loss: 0.900543  [   32/  118]
train() client id: f_00009-8-1 loss: 0.907242  [   64/  118]
train() client id: f_00009-8-2 loss: 1.036198  [   96/  118]
train() client id: f_00009-9-0 loss: 0.880599  [   32/  118]
train() client id: f_00009-9-1 loss: 1.026965  [   64/  118]
train() client id: f_00009-9-2 loss: 0.873102  [   96/  118]
train() client id: f_00009-10-0 loss: 0.904403  [   32/  118]
train() client id: f_00009-10-1 loss: 1.006805  [   64/  118]
train() client id: f_00009-10-2 loss: 0.893467  [   96/  118]
train() client id: f_00009-11-0 loss: 0.769366  [   32/  118]
train() client id: f_00009-11-1 loss: 1.053733  [   64/  118]
train() client id: f_00009-11-2 loss: 0.855752  [   96/  118]
At round 38 accuracy: 0.649867374005305
At round 38 training accuracy: 0.5902079141515761
At round 38 training loss: 0.8153294099680123
update_location
xs = -4.528292 91.001589 100.045120 -100.943528 -0.103519 -25.217951 -142.215960 163.375741 -1.680116 84.695607 
ys = 177.587959 15.555839 1.320614 22.544824 9.350187 2.814151 -3.124922 -19.177652 -107.154970 4.001482 
xs mean: 16.442869159412865
ys mean: 10.371751218646876
dists_uav = 203.857766 136.100233 141.459429 143.867526 100.436232 103.169106 173.882559 192.508221 146.577660 131.108191 
uav_gains = -107.926171 -103.347703 -103.767877 -103.951680 -100.047277 -100.338767 -106.036496 -107.212584 -104.155045 -102.941482 
uav_gains_db_mean: -103.97250821958083
dists_bs = 170.491351 310.127854 325.291173 169.490241 240.891344 228.216189 181.116761 390.132160 331.136240 310.937778 
bs_gains = -102.054725 -109.330201 -109.910683 -101.983111 -106.258053 -105.600760 -102.789901 -112.120988 -110.127247 -109.361917 
bs_gains_db_mean: -106.9537585066715
Round 39
-------------------------------
ene_coms = [0.00935894 0.01022179 0.00746752 0.00753525 0.00849929 0.00820103
 0.00839644 0.00897163 0.01078154 0.01024302]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.83000296 12.11984371  5.71136725  2.05460706 13.94551863  6.71581967
  2.56423648  8.2175622   6.03193435  5.4819786 ]
obj_prev = 68.67287091093348
eta_min = 2.1549824540275892e-16	eta_max = 0.9423556921304909
af = 14.474348849714058	bf = 1.3293024175299408	zeta = 15.921783734685466	eta = 0.909090909090909
af = 14.474348849714058	bf = 1.3293024175299408	zeta = 29.797407298413777	eta = 0.48575866701276993
af = 14.474348849714058	bf = 1.3293024175299408	zeta = 22.90832916458033	eta = 0.631837823951541
af = 14.474348849714058	bf = 1.3293024175299408	zeta = 21.661111690100967	eta = 0.6682181901277381
af = 14.474348849714058	bf = 1.3293024175299408	zeta = 21.594007945696543	eta = 0.6702946894394676
af = 14.474348849714058	bf = 1.3293024175299408	zeta = 21.59379685299687	eta = 0.670301241984003
eta = 0.670301241984003
ene_coms = [0.00935894 0.01022179 0.00746752 0.00753525 0.00849929 0.00820103
 0.00839644 0.00897163 0.01078154 0.01024302]
ene_comp = [0.03300936 0.06942444 0.0324854  0.0112651  0.08016558 0.03824894
 0.01414687 0.04689424 0.03405727 0.03091352]
ene_total = [1.90488624 3.58090871 1.7962905  0.84526704 3.98638837 2.08839889
 1.01355115 2.51173968 2.01596083 1.85040545]
ti_comp = [0.53029237 0.52166388 0.54920655 0.54852928 0.53888887 0.54187146
 0.53991738 0.53416548 0.51606642 0.52145161]
ti_coms = [0.09358942 0.10221791 0.07467524 0.07535251 0.08499292 0.08201033
 0.08396441 0.08971631 0.10781537 0.10243018]
t_total = [28.04983635 28.04983635 28.04983635 28.04983635 28.04983635 28.04983635
 28.04983635 28.04983635 28.04983635 28.04983635]
ene_coms = [0.00935894 0.01022179 0.00746752 0.00753525 0.00849929 0.00820103
 0.00839644 0.00897163 0.01078154 0.01024302]
ene_comp = [7.99393152e-06 7.68485418e-05 7.10351305e-06 2.96951333e-07
 1.10878000e-04 1.19109267e-05 6.07024483e-07 2.25884793e-05
 9.27039824e-06 6.79043404e-06]
ene_total = [0.42113913 0.46302868 0.3360606  0.33879959 0.38711476 0.36925542
 0.37753279 0.40438171 0.48515661 0.46083321]
optimize_network iter = 0 obj = 4.043302497116118
eta = 0.670301241984003
freqs = [31123733.00930754 66541351.95481739 29574845.91996371 10268456.17320272
 74380436.14770317 35293366.74395455 13100955.3236272  43894865.563789
 32996985.13644098 29641795.43458417]
eta_min = 0.6703012419840074	eta_max = 0.6933442891162961
af = 0.009422824290406388	bf = 1.3293024175299408	zeta = 0.010365106719447027	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00935894 0.01022179 0.00746752 0.00753525 0.00849929 0.00820103
 0.00839644 0.00897163 0.01078154 0.01024302]
ene_comp = [1.90462405e-06 1.83098367e-05 1.69247407e-06 7.07512505e-08
 2.64176526e-05 2.83788238e-06 1.44628888e-07 5.38190261e-06
 2.20875340e-06 1.61788026e-06]
ene_total = [1.52634661 1.66971468 1.21790408 1.22868296 1.390172   1.33769417
 1.36911761 1.46375997 1.75835942 1.6704541 ]
ti_comp = [0.48668851 0.47806002 0.50560269 0.50492542 0.49528502 0.4982676
 0.49631352 0.49056162 0.47246257 0.47784775]
ti_coms = [0.09358942 0.10221791 0.07467524 0.07535251 0.08499292 0.08201033
 0.08396441 0.08971631 0.10781537 0.10243018]
t_total = [28.04983635 28.04983635 28.04983635 28.04983635 28.04983635 28.04983635
 28.04983635 28.04983635 28.04983635 28.04983635]
ene_coms = [0.00935894 0.01022179 0.00746752 0.00753525 0.00849929 0.00820103
 0.00839644 0.00897163 0.01078154 0.01024302]
ene_comp = [7.28239769e-06 7.02162631e-05 6.43148477e-06 2.68915501e-07
 1.00720724e-04 1.08093158e-05 5.51231477e-07 2.05511862e-05
 8.48711705e-06 6.20485829e-06]
ene_total = [0.45275041 0.49750148 0.36128073 0.36425667 0.41571276 0.39694915
 0.40589906 0.43466971 0.5215749  0.49543332]
optimize_network iter = 1 obj = 4.346028186902483
eta = 0.6933442891162961
freqs = [31046865.82111443 66475522.67205466 29411056.22956575 10212675.47508408
 74090845.09626882 35138928.71334169 13047760.18529646 43758031.11713481
 32996985.13644097 29613570.35679944]
eta_min = 0.6933442891163022	eta_max = 0.6933442891162898
af = 0.00937104777303224	bf = 1.3293024175299408	zeta = 0.010308152550335464	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00935894 0.01022179 0.00746752 0.00753525 0.00849929 0.00820103
 0.00839644 0.00897163 0.01078154 0.01024302]
ene_comp = [1.89522786e-06 1.82736269e-05 1.67377966e-06 6.99846631e-08
 2.62123453e-05 2.81310049e-06 1.43456770e-07 5.34840066e-06
 2.20875340e-06 1.61480062e-06]
ene_total = [1.52634508 1.66970878 1.21790103 1.22868283 1.39013853 1.33769013
 1.36911742 1.46375451 1.75835942 1.6704536 ]
ti_comp = [0.48668851 0.47806002 0.50560269 0.50492542 0.49528502 0.4982676
 0.49631352 0.49056162 0.47246257 0.47784775]
ti_coms = [0.09358942 0.10221791 0.07467524 0.07535251 0.08499292 0.08201033
 0.08396441 0.08971631 0.10781537 0.10243018]
t_total = [28.04983635 28.04983635 28.04983635 28.04983635 28.04983635 28.04983635
 28.04983635 28.04983635 28.04983635 28.04983635]
ene_coms = [0.00935894 0.01022179 0.00746752 0.00753525 0.00849929 0.00820103
 0.00839644 0.00897163 0.01078154 0.01024302]
ene_comp = [7.28239769e-06 7.02162631e-05 6.43148477e-06 2.68915501e-07
 1.00720724e-04 1.08093158e-05 5.51231477e-07 2.05511862e-05
 8.48711705e-06 6.20485829e-06]
ene_total = [0.45275041 0.49750148 0.36128073 0.36425667 0.41571276 0.39694915
 0.40589906 0.43466971 0.5215749  0.49543332]
optimize_network iter = 2 obj = 4.346028186902394
eta = 0.6933442891162898
freqs = [31046865.82111444 66475522.67205465 29411056.22956579 10212675.47508409
 74090845.09626888 35138928.71334172 13047760.18529647 43758031.11713483
 32996985.13644095 29613570.35679944]
Done!
ene_coms = [0.00935894 0.01022179 0.00746752 0.00753525 0.00849929 0.00820103
 0.00839644 0.00897163 0.01078154 0.01024302]
ene_comp = [6.67988159e-06 6.44068538e-05 5.89936976e-06 2.46666521e-07
 9.23874995e-05 9.91499682e-06 5.05624816e-07 1.88508643e-05
 7.78492735e-06 5.69149343e-06]
ene_total = [0.00936562 0.0102862  0.00747342 0.0075355  0.00859168 0.00821095
 0.00839695 0.00899048 0.01078932 0.01024871]
At round 39 energy consumption: 0.0898888294331278
At round 39 eta: 0.6933442891162898
At round 39 a_n: 14.823314824451002
At round 39 local rounds: 11.992184814588997
At round 39 global rounds: 48.33862308232795
gradient difference: 0.4534333646297455
train() client id: f_00000-0-0 loss: 1.192489  [   32/  126]
train() client id: f_00000-0-1 loss: 1.180744  [   64/  126]
train() client id: f_00000-0-2 loss: 1.166420  [   96/  126]
train() client id: f_00000-1-0 loss: 1.125365  [   32/  126]
train() client id: f_00000-1-1 loss: 1.085194  [   64/  126]
train() client id: f_00000-1-2 loss: 1.083806  [   96/  126]
train() client id: f_00000-2-0 loss: 0.819147  [   32/  126]
train() client id: f_00000-2-1 loss: 1.074570  [   64/  126]
train() client id: f_00000-2-2 loss: 1.054207  [   96/  126]
train() client id: f_00000-3-0 loss: 0.948307  [   32/  126]
train() client id: f_00000-3-1 loss: 0.938330  [   64/  126]
train() client id: f_00000-3-2 loss: 0.995647  [   96/  126]
train() client id: f_00000-4-0 loss: 0.875109  [   32/  126]
train() client id: f_00000-4-1 loss: 0.812920  [   64/  126]
train() client id: f_00000-4-2 loss: 0.898342  [   96/  126]
train() client id: f_00000-5-0 loss: 0.861872  [   32/  126]
train() client id: f_00000-5-1 loss: 0.751567  [   64/  126]
train() client id: f_00000-5-2 loss: 0.927932  [   96/  126]
train() client id: f_00000-6-0 loss: 0.928721  [   32/  126]
train() client id: f_00000-6-1 loss: 0.733902  [   64/  126]
train() client id: f_00000-6-2 loss: 0.801470  [   96/  126]
train() client id: f_00000-7-0 loss: 0.727159  [   32/  126]
train() client id: f_00000-7-1 loss: 0.820472  [   64/  126]
train() client id: f_00000-7-2 loss: 0.808655  [   96/  126]
train() client id: f_00000-8-0 loss: 0.784538  [   32/  126]
train() client id: f_00000-8-1 loss: 0.779975  [   64/  126]
train() client id: f_00000-8-2 loss: 0.757521  [   96/  126]
train() client id: f_00000-9-0 loss: 0.656147  [   32/  126]
train() client id: f_00000-9-1 loss: 0.683810  [   64/  126]
train() client id: f_00000-9-2 loss: 0.877856  [   96/  126]
train() client id: f_00000-10-0 loss: 0.746279  [   32/  126]
train() client id: f_00000-10-1 loss: 0.789468  [   64/  126]
train() client id: f_00000-10-2 loss: 0.618462  [   96/  126]
train() client id: f_00001-0-0 loss: 0.515219  [   32/  265]
train() client id: f_00001-0-1 loss: 0.492467  [   64/  265]
train() client id: f_00001-0-2 loss: 0.525018  [   96/  265]
train() client id: f_00001-0-3 loss: 0.517795  [  128/  265]
train() client id: f_00001-0-4 loss: 0.532954  [  160/  265]
train() client id: f_00001-0-5 loss: 0.584002  [  192/  265]
train() client id: f_00001-0-6 loss: 0.488688  [  224/  265]
train() client id: f_00001-0-7 loss: 0.566126  [  256/  265]
train() client id: f_00001-1-0 loss: 0.496265  [   32/  265]
train() client id: f_00001-1-1 loss: 0.513593  [   64/  265]
train() client id: f_00001-1-2 loss: 0.598343  [   96/  265]
train() client id: f_00001-1-3 loss: 0.446116  [  128/  265]
train() client id: f_00001-1-4 loss: 0.463776  [  160/  265]
train() client id: f_00001-1-5 loss: 0.637483  [  192/  265]
train() client id: f_00001-1-6 loss: 0.487143  [  224/  265]
train() client id: f_00001-1-7 loss: 0.434773  [  256/  265]
train() client id: f_00001-2-0 loss: 0.521108  [   32/  265]
train() client id: f_00001-2-1 loss: 0.583068  [   64/  265]
train() client id: f_00001-2-2 loss: 0.442908  [   96/  265]
train() client id: f_00001-2-3 loss: 0.486316  [  128/  265]
train() client id: f_00001-2-4 loss: 0.545069  [  160/  265]
train() client id: f_00001-2-5 loss: 0.478669  [  192/  265]
train() client id: f_00001-2-6 loss: 0.630710  [  224/  265]
train() client id: f_00001-2-7 loss: 0.399949  [  256/  265]
train() client id: f_00001-3-0 loss: 0.522329  [   32/  265]
train() client id: f_00001-3-1 loss: 0.403700  [   64/  265]
train() client id: f_00001-3-2 loss: 0.443716  [   96/  265]
train() client id: f_00001-3-3 loss: 0.441653  [  128/  265]
train() client id: f_00001-3-4 loss: 0.564234  [  160/  265]
train() client id: f_00001-3-5 loss: 0.627684  [  192/  265]
train() client id: f_00001-3-6 loss: 0.510782  [  224/  265]
train() client id: f_00001-3-7 loss: 0.576404  [  256/  265]
train() client id: f_00001-4-0 loss: 0.491507  [   32/  265]
train() client id: f_00001-4-1 loss: 0.538958  [   64/  265]
train() client id: f_00001-4-2 loss: 0.487884  [   96/  265]
train() client id: f_00001-4-3 loss: 0.455054  [  128/  265]
train() client id: f_00001-4-4 loss: 0.539414  [  160/  265]
train() client id: f_00001-4-5 loss: 0.578573  [  192/  265]
train() client id: f_00001-4-6 loss: 0.476113  [  224/  265]
train() client id: f_00001-4-7 loss: 0.505918  [  256/  265]
train() client id: f_00001-5-0 loss: 0.494375  [   32/  265]
train() client id: f_00001-5-1 loss: 0.495340  [   64/  265]
train() client id: f_00001-5-2 loss: 0.467534  [   96/  265]
train() client id: f_00001-5-3 loss: 0.595680  [  128/  265]
train() client id: f_00001-5-4 loss: 0.593755  [  160/  265]
train() client id: f_00001-5-5 loss: 0.420330  [  192/  265]
train() client id: f_00001-5-6 loss: 0.508626  [  224/  265]
train() client id: f_00001-5-7 loss: 0.414523  [  256/  265]
train() client id: f_00001-6-0 loss: 0.527222  [   32/  265]
train() client id: f_00001-6-1 loss: 0.503695  [   64/  265]
train() client id: f_00001-6-2 loss: 0.453278  [   96/  265]
train() client id: f_00001-6-3 loss: 0.550081  [  128/  265]
train() client id: f_00001-6-4 loss: 0.516086  [  160/  265]
train() client id: f_00001-6-5 loss: 0.498553  [  192/  265]
train() client id: f_00001-6-6 loss: 0.536960  [  224/  265]
train() client id: f_00001-6-7 loss: 0.441643  [  256/  265]
train() client id: f_00001-7-0 loss: 0.550617  [   32/  265]
train() client id: f_00001-7-1 loss: 0.472801  [   64/  265]
train() client id: f_00001-7-2 loss: 0.483456  [   96/  265]
train() client id: f_00001-7-3 loss: 0.414281  [  128/  265]
train() client id: f_00001-7-4 loss: 0.508536  [  160/  265]
train() client id: f_00001-7-5 loss: 0.447742  [  192/  265]
train() client id: f_00001-7-6 loss: 0.604038  [  224/  265]
train() client id: f_00001-7-7 loss: 0.450624  [  256/  265]
train() client id: f_00001-8-0 loss: 0.632940  [   32/  265]
train() client id: f_00001-8-1 loss: 0.505148  [   64/  265]
train() client id: f_00001-8-2 loss: 0.463115  [   96/  265]
train() client id: f_00001-8-3 loss: 0.402072  [  128/  265]
train() client id: f_00001-8-4 loss: 0.446990  [  160/  265]
train() client id: f_00001-8-5 loss: 0.527242  [  192/  265]
train() client id: f_00001-8-6 loss: 0.481965  [  224/  265]
train() client id: f_00001-8-7 loss: 0.406782  [  256/  265]
train() client id: f_00001-9-0 loss: 0.607643  [   32/  265]
train() client id: f_00001-9-1 loss: 0.460255  [   64/  265]
train() client id: f_00001-9-2 loss: 0.507598  [   96/  265]
train() client id: f_00001-9-3 loss: 0.482406  [  128/  265]
train() client id: f_00001-9-4 loss: 0.443317  [  160/  265]
train() client id: f_00001-9-5 loss: 0.484011  [  192/  265]
train() client id: f_00001-9-6 loss: 0.433415  [  224/  265]
train() client id: f_00001-9-7 loss: 0.517680  [  256/  265]
train() client id: f_00001-10-0 loss: 0.491501  [   32/  265]
train() client id: f_00001-10-1 loss: 0.535225  [   64/  265]
train() client id: f_00001-10-2 loss: 0.435465  [   96/  265]
train() client id: f_00001-10-3 loss: 0.527029  [  128/  265]
train() client id: f_00001-10-4 loss: 0.527875  [  160/  265]
train() client id: f_00001-10-5 loss: 0.403479  [  192/  265]
train() client id: f_00001-10-6 loss: 0.544919  [  224/  265]
train() client id: f_00001-10-7 loss: 0.552049  [  256/  265]
train() client id: f_00002-0-0 loss: 1.113618  [   32/  124]
train() client id: f_00002-0-1 loss: 1.190992  [   64/  124]
train() client id: f_00002-0-2 loss: 1.303974  [   96/  124]
train() client id: f_00002-1-0 loss: 1.267949  [   32/  124]
train() client id: f_00002-1-1 loss: 1.132702  [   64/  124]
train() client id: f_00002-1-2 loss: 1.114561  [   96/  124]
train() client id: f_00002-2-0 loss: 1.276528  [   32/  124]
train() client id: f_00002-2-1 loss: 1.155790  [   64/  124]
train() client id: f_00002-2-2 loss: 1.052849  [   96/  124]
train() client id: f_00002-3-0 loss: 1.218382  [   32/  124]
train() client id: f_00002-3-1 loss: 0.901910  [   64/  124]
train() client id: f_00002-3-2 loss: 1.094023  [   96/  124]
train() client id: f_00002-4-0 loss: 1.157148  [   32/  124]
train() client id: f_00002-4-1 loss: 0.855378  [   64/  124]
train() client id: f_00002-4-2 loss: 1.055840  [   96/  124]
train() client id: f_00002-5-0 loss: 1.033937  [   32/  124]
train() client id: f_00002-5-1 loss: 0.927583  [   64/  124]
train() client id: f_00002-5-2 loss: 1.109412  [   96/  124]
train() client id: f_00002-6-0 loss: 0.797082  [   32/  124]
train() client id: f_00002-6-1 loss: 1.182835  [   64/  124]
train() client id: f_00002-6-2 loss: 0.999560  [   96/  124]
train() client id: f_00002-7-0 loss: 1.108592  [   32/  124]
train() client id: f_00002-7-1 loss: 1.085198  [   64/  124]
train() client id: f_00002-7-2 loss: 0.912780  [   96/  124]
train() client id: f_00002-8-0 loss: 0.832753  [   32/  124]
train() client id: f_00002-8-1 loss: 0.886391  [   64/  124]
train() client id: f_00002-8-2 loss: 1.073652  [   96/  124]
train() client id: f_00002-9-0 loss: 0.892420  [   32/  124]
train() client id: f_00002-9-1 loss: 1.079700  [   64/  124]
train() client id: f_00002-9-2 loss: 1.090246  [   96/  124]
train() client id: f_00002-10-0 loss: 0.895490  [   32/  124]
train() client id: f_00002-10-1 loss: 1.017240  [   64/  124]
train() client id: f_00002-10-2 loss: 0.928864  [   96/  124]
train() client id: f_00003-0-0 loss: 0.745030  [   32/   43]
train() client id: f_00003-1-0 loss: 0.659209  [   32/   43]
train() client id: f_00003-2-0 loss: 0.832247  [   32/   43]
train() client id: f_00003-3-0 loss: 0.577662  [   32/   43]
train() client id: f_00003-4-0 loss: 0.769168  [   32/   43]
train() client id: f_00003-5-0 loss: 0.679895  [   32/   43]
train() client id: f_00003-6-0 loss: 0.802044  [   32/   43]
train() client id: f_00003-7-0 loss: 0.616169  [   32/   43]
train() client id: f_00003-8-0 loss: 0.618139  [   32/   43]
train() client id: f_00003-9-0 loss: 0.734824  [   32/   43]
train() client id: f_00003-10-0 loss: 0.527340  [   32/   43]
train() client id: f_00004-0-0 loss: 1.146323  [   32/  306]
train() client id: f_00004-0-1 loss: 1.031461  [   64/  306]
train() client id: f_00004-0-2 loss: 1.013044  [   96/  306]
train() client id: f_00004-0-3 loss: 1.214845  [  128/  306]
train() client id: f_00004-0-4 loss: 1.107638  [  160/  306]
train() client id: f_00004-0-5 loss: 0.898307  [  192/  306]
train() client id: f_00004-0-6 loss: 0.937626  [  224/  306]
train() client id: f_00004-0-7 loss: 1.122676  [  256/  306]
train() client id: f_00004-0-8 loss: 1.004708  [  288/  306]
train() client id: f_00004-1-0 loss: 0.960336  [   32/  306]
train() client id: f_00004-1-1 loss: 1.142629  [   64/  306]
train() client id: f_00004-1-2 loss: 1.077124  [   96/  306]
train() client id: f_00004-1-3 loss: 0.958519  [  128/  306]
train() client id: f_00004-1-4 loss: 1.107163  [  160/  306]
train() client id: f_00004-1-5 loss: 1.064881  [  192/  306]
train() client id: f_00004-1-6 loss: 1.001529  [  224/  306]
train() client id: f_00004-1-7 loss: 1.007390  [  256/  306]
train() client id: f_00004-1-8 loss: 1.088763  [  288/  306]
train() client id: f_00004-2-0 loss: 0.941656  [   32/  306]
train() client id: f_00004-2-1 loss: 1.041691  [   64/  306]
train() client id: f_00004-2-2 loss: 1.045802  [   96/  306]
train() client id: f_00004-2-3 loss: 0.882225  [  128/  306]
train() client id: f_00004-2-4 loss: 1.126545  [  160/  306]
train() client id: f_00004-2-5 loss: 1.067991  [  192/  306]
train() client id: f_00004-2-6 loss: 1.185402  [  224/  306]
train() client id: f_00004-2-7 loss: 1.135067  [  256/  306]
train() client id: f_00004-2-8 loss: 0.958379  [  288/  306]
train() client id: f_00004-3-0 loss: 1.008787  [   32/  306]
train() client id: f_00004-3-1 loss: 1.099484  [   64/  306]
train() client id: f_00004-3-2 loss: 1.139894  [   96/  306]
train() client id: f_00004-3-3 loss: 0.941616  [  128/  306]
train() client id: f_00004-3-4 loss: 1.084116  [  160/  306]
train() client id: f_00004-3-5 loss: 1.080393  [  192/  306]
train() client id: f_00004-3-6 loss: 0.940591  [  224/  306]
train() client id: f_00004-3-7 loss: 1.068215  [  256/  306]
train() client id: f_00004-3-8 loss: 0.949302  [  288/  306]
train() client id: f_00004-4-0 loss: 0.952757  [   32/  306]
train() client id: f_00004-4-1 loss: 1.043622  [   64/  306]
train() client id: f_00004-4-2 loss: 1.087725  [   96/  306]
train() client id: f_00004-4-3 loss: 0.990531  [  128/  306]
train() client id: f_00004-4-4 loss: 1.057756  [  160/  306]
train() client id: f_00004-4-5 loss: 0.991290  [  192/  306]
train() client id: f_00004-4-6 loss: 1.072895  [  224/  306]
train() client id: f_00004-4-7 loss: 1.091669  [  256/  306]
train() client id: f_00004-4-8 loss: 0.955200  [  288/  306]
train() client id: f_00004-5-0 loss: 0.861198  [   32/  306]
train() client id: f_00004-5-1 loss: 1.101963  [   64/  306]
train() client id: f_00004-5-2 loss: 1.050579  [   96/  306]
train() client id: f_00004-5-3 loss: 1.047094  [  128/  306]
train() client id: f_00004-5-4 loss: 0.965310  [  160/  306]
train() client id: f_00004-5-5 loss: 0.968619  [  192/  306]
train() client id: f_00004-5-6 loss: 1.069329  [  224/  306]
train() client id: f_00004-5-7 loss: 1.019978  [  256/  306]
train() client id: f_00004-5-8 loss: 1.122245  [  288/  306]
train() client id: f_00004-6-0 loss: 0.987965  [   32/  306]
train() client id: f_00004-6-1 loss: 1.005591  [   64/  306]
train() client id: f_00004-6-2 loss: 1.044004  [   96/  306]
train() client id: f_00004-6-3 loss: 0.904567  [  128/  306]
train() client id: f_00004-6-4 loss: 1.010955  [  160/  306]
train() client id: f_00004-6-5 loss: 1.035321  [  192/  306]
train() client id: f_00004-6-6 loss: 1.118789  [  224/  306]
train() client id: f_00004-6-7 loss: 1.037285  [  256/  306]
train() client id: f_00004-6-8 loss: 0.994088  [  288/  306]
train() client id: f_00004-7-0 loss: 1.118676  [   32/  306]
train() client id: f_00004-7-1 loss: 1.045368  [   64/  306]
train() client id: f_00004-7-2 loss: 0.998833  [   96/  306]
train() client id: f_00004-7-3 loss: 0.972619  [  128/  306]
train() client id: f_00004-7-4 loss: 1.081769  [  160/  306]
train() client id: f_00004-7-5 loss: 1.047328  [  192/  306]
train() client id: f_00004-7-6 loss: 1.022846  [  224/  306]
train() client id: f_00004-7-7 loss: 0.918187  [  256/  306]
train() client id: f_00004-7-8 loss: 0.987138  [  288/  306]
train() client id: f_00004-8-0 loss: 1.015678  [   32/  306]
train() client id: f_00004-8-1 loss: 1.096956  [   64/  306]
train() client id: f_00004-8-2 loss: 0.905726  [   96/  306]
train() client id: f_00004-8-3 loss: 0.971850  [  128/  306]
train() client id: f_00004-8-4 loss: 0.988330  [  160/  306]
train() client id: f_00004-8-5 loss: 0.995484  [  192/  306]
train() client id: f_00004-8-6 loss: 1.005419  [  224/  306]
train() client id: f_00004-8-7 loss: 1.061162  [  256/  306]
train() client id: f_00004-8-8 loss: 1.058740  [  288/  306]
train() client id: f_00004-9-0 loss: 1.047170  [   32/  306]
train() client id: f_00004-9-1 loss: 1.059204  [   64/  306]
train() client id: f_00004-9-2 loss: 0.980831  [   96/  306]
train() client id: f_00004-9-3 loss: 1.026263  [  128/  306]
train() client id: f_00004-9-4 loss: 1.130214  [  160/  306]
train() client id: f_00004-9-5 loss: 1.003869  [  192/  306]
train() client id: f_00004-9-6 loss: 0.893965  [  224/  306]
train() client id: f_00004-9-7 loss: 0.912780  [  256/  306]
train() client id: f_00004-9-8 loss: 1.061041  [  288/  306]
train() client id: f_00004-10-0 loss: 1.012579  [   32/  306]
train() client id: f_00004-10-1 loss: 1.009296  [   64/  306]
train() client id: f_00004-10-2 loss: 0.966239  [   96/  306]
train() client id: f_00004-10-3 loss: 1.024590  [  128/  306]
train() client id: f_00004-10-4 loss: 0.996088  [  160/  306]
train() client id: f_00004-10-5 loss: 0.902535  [  192/  306]
train() client id: f_00004-10-6 loss: 1.165993  [  224/  306]
train() client id: f_00004-10-7 loss: 1.093657  [  256/  306]
train() client id: f_00004-10-8 loss: 0.952627  [  288/  306]
train() client id: f_00005-0-0 loss: 0.350456  [   32/  146]
train() client id: f_00005-0-1 loss: 0.719314  [   64/  146]
train() client id: f_00005-0-2 loss: 0.433375  [   96/  146]
train() client id: f_00005-0-3 loss: 0.396428  [  128/  146]
train() client id: f_00005-1-0 loss: 0.387402  [   32/  146]
train() client id: f_00005-1-1 loss: 0.440962  [   64/  146]
train() client id: f_00005-1-2 loss: 0.328992  [   96/  146]
train() client id: f_00005-1-3 loss: 0.844384  [  128/  146]
train() client id: f_00005-2-0 loss: 0.557802  [   32/  146]
train() client id: f_00005-2-1 loss: 0.492300  [   64/  146]
train() client id: f_00005-2-2 loss: 0.363357  [   96/  146]
train() client id: f_00005-2-3 loss: 0.420493  [  128/  146]
train() client id: f_00005-3-0 loss: 0.364824  [   32/  146]
train() client id: f_00005-3-1 loss: 0.722379  [   64/  146]
train() client id: f_00005-3-2 loss: 0.598847  [   96/  146]
train() client id: f_00005-3-3 loss: 0.342606  [  128/  146]
train() client id: f_00005-4-0 loss: 0.490134  [   32/  146]
train() client id: f_00005-4-1 loss: 0.414628  [   64/  146]
train() client id: f_00005-4-2 loss: 0.450181  [   96/  146]
train() client id: f_00005-4-3 loss: 0.462908  [  128/  146]
train() client id: f_00005-5-0 loss: 0.584659  [   32/  146]
train() client id: f_00005-5-1 loss: 0.602134  [   64/  146]
train() client id: f_00005-5-2 loss: 0.462806  [   96/  146]
train() client id: f_00005-5-3 loss: 0.280696  [  128/  146]
train() client id: f_00005-6-0 loss: 0.483914  [   32/  146]
train() client id: f_00005-6-1 loss: 0.411965  [   64/  146]
train() client id: f_00005-6-2 loss: 0.617494  [   96/  146]
train() client id: f_00005-6-3 loss: 0.237159  [  128/  146]
train() client id: f_00005-7-0 loss: 0.741628  [   32/  146]
train() client id: f_00005-7-1 loss: 0.249537  [   64/  146]
train() client id: f_00005-7-2 loss: 0.717296  [   96/  146]
train() client id: f_00005-7-3 loss: 0.335563  [  128/  146]
train() client id: f_00005-8-0 loss: 0.229050  [   32/  146]
train() client id: f_00005-8-1 loss: 0.596236  [   64/  146]
train() client id: f_00005-8-2 loss: 0.692350  [   96/  146]
train() client id: f_00005-8-3 loss: 0.517829  [  128/  146]
train() client id: f_00005-9-0 loss: 0.418096  [   32/  146]
train() client id: f_00005-9-1 loss: 0.446339  [   64/  146]
train() client id: f_00005-9-2 loss: 0.693709  [   96/  146]
train() client id: f_00005-9-3 loss: 0.366549  [  128/  146]
train() client id: f_00005-10-0 loss: 0.576608  [   32/  146]
train() client id: f_00005-10-1 loss: 0.222867  [   64/  146]
train() client id: f_00005-10-2 loss: 0.420352  [   96/  146]
train() client id: f_00005-10-3 loss: 0.540519  [  128/  146]
train() client id: f_00006-0-0 loss: 0.452923  [   32/   54]
train() client id: f_00006-1-0 loss: 0.517830  [   32/   54]
train() client id: f_00006-2-0 loss: 0.491593  [   32/   54]
train() client id: f_00006-3-0 loss: 0.475775  [   32/   54]
train() client id: f_00006-4-0 loss: 0.482060  [   32/   54]
train() client id: f_00006-5-0 loss: 0.486123  [   32/   54]
train() client id: f_00006-6-0 loss: 0.526810  [   32/   54]
train() client id: f_00006-7-0 loss: 0.412697  [   32/   54]
train() client id: f_00006-8-0 loss: 0.449532  [   32/   54]
train() client id: f_00006-9-0 loss: 0.476688  [   32/   54]
train() client id: f_00006-10-0 loss: 0.501366  [   32/   54]
train() client id: f_00007-0-0 loss: 0.537027  [   32/  179]
train() client id: f_00007-0-1 loss: 0.723872  [   64/  179]
train() client id: f_00007-0-2 loss: 0.795443  [   96/  179]
train() client id: f_00007-0-3 loss: 0.880884  [  128/  179]
train() client id: f_00007-0-4 loss: 0.660355  [  160/  179]
train() client id: f_00007-1-0 loss: 0.880058  [   32/  179]
train() client id: f_00007-1-1 loss: 0.645140  [   64/  179]
train() client id: f_00007-1-2 loss: 0.533377  [   96/  179]
train() client id: f_00007-1-3 loss: 0.692413  [  128/  179]
train() client id: f_00007-1-4 loss: 0.657530  [  160/  179]
train() client id: f_00007-2-0 loss: 0.564242  [   32/  179]
train() client id: f_00007-2-1 loss: 0.571815  [   64/  179]
train() client id: f_00007-2-2 loss: 0.806877  [   96/  179]
train() client id: f_00007-2-3 loss: 0.641106  [  128/  179]
train() client id: f_00007-2-4 loss: 0.683715  [  160/  179]
train() client id: f_00007-3-0 loss: 0.744427  [   32/  179]
train() client id: f_00007-3-1 loss: 0.677853  [   64/  179]
train() client id: f_00007-3-2 loss: 0.699410  [   96/  179]
train() client id: f_00007-3-3 loss: 0.520250  [  128/  179]
train() client id: f_00007-3-4 loss: 0.530543  [  160/  179]
train() client id: f_00007-4-0 loss: 0.698089  [   32/  179]
train() client id: f_00007-4-1 loss: 0.513764  [   64/  179]
train() client id: f_00007-4-2 loss: 0.641100  [   96/  179]
train() client id: f_00007-4-3 loss: 0.748945  [  128/  179]
train() client id: f_00007-4-4 loss: 0.684398  [  160/  179]
train() client id: f_00007-5-0 loss: 0.503510  [   32/  179]
train() client id: f_00007-5-1 loss: 0.717698  [   64/  179]
train() client id: f_00007-5-2 loss: 0.732396  [   96/  179]
train() client id: f_00007-5-3 loss: 0.866209  [  128/  179]
train() client id: f_00007-5-4 loss: 0.531717  [  160/  179]
train() client id: f_00007-6-0 loss: 0.500886  [   32/  179]
train() client id: f_00007-6-1 loss: 0.574178  [   64/  179]
train() client id: f_00007-6-2 loss: 0.803307  [   96/  179]
train() client id: f_00007-6-3 loss: 0.711330  [  128/  179]
train() client id: f_00007-6-4 loss: 0.730602  [  160/  179]
train() client id: f_00007-7-0 loss: 0.584146  [   32/  179]
train() client id: f_00007-7-1 loss: 0.602733  [   64/  179]
train() client id: f_00007-7-2 loss: 0.600639  [   96/  179]
train() client id: f_00007-7-3 loss: 0.740065  [  128/  179]
train() client id: f_00007-7-4 loss: 0.865795  [  160/  179]
train() client id: f_00007-8-0 loss: 0.785261  [   32/  179]
train() client id: f_00007-8-1 loss: 0.513338  [   64/  179]
train() client id: f_00007-8-2 loss: 0.657421  [   96/  179]
train() client id: f_00007-8-3 loss: 0.690119  [  128/  179]
train() client id: f_00007-8-4 loss: 0.680016  [  160/  179]
train() client id: f_00007-9-0 loss: 0.717918  [   32/  179]
train() client id: f_00007-9-1 loss: 0.661173  [   64/  179]
train() client id: f_00007-9-2 loss: 0.691579  [   96/  179]
train() client id: f_00007-9-3 loss: 0.531652  [  128/  179]
train() client id: f_00007-9-4 loss: 0.830494  [  160/  179]
train() client id: f_00007-10-0 loss: 0.624336  [   32/  179]
train() client id: f_00007-10-1 loss: 0.718506  [   64/  179]
train() client id: f_00007-10-2 loss: 0.604812  [   96/  179]
train() client id: f_00007-10-3 loss: 0.762053  [  128/  179]
train() client id: f_00007-10-4 loss: 0.702240  [  160/  179]
train() client id: f_00008-0-0 loss: 0.720071  [   32/  130]
train() client id: f_00008-0-1 loss: 0.631798  [   64/  130]
train() client id: f_00008-0-2 loss: 0.618695  [   96/  130]
train() client id: f_00008-0-3 loss: 0.629127  [  128/  130]
train() client id: f_00008-1-0 loss: 0.742243  [   32/  130]
train() client id: f_00008-1-1 loss: 0.629571  [   64/  130]
train() client id: f_00008-1-2 loss: 0.675218  [   96/  130]
train() client id: f_00008-1-3 loss: 0.522200  [  128/  130]
train() client id: f_00008-2-0 loss: 0.561616  [   32/  130]
train() client id: f_00008-2-1 loss: 0.727575  [   64/  130]
train() client id: f_00008-2-2 loss: 0.686281  [   96/  130]
train() client id: f_00008-2-3 loss: 0.626218  [  128/  130]
train() client id: f_00008-3-0 loss: 0.711095  [   32/  130]
train() client id: f_00008-3-1 loss: 0.700973  [   64/  130]
train() client id: f_00008-3-2 loss: 0.679423  [   96/  130]
train() client id: f_00008-3-3 loss: 0.515658  [  128/  130]
train() client id: f_00008-4-0 loss: 0.706111  [   32/  130]
train() client id: f_00008-4-1 loss: 0.612690  [   64/  130]
train() client id: f_00008-4-2 loss: 0.663270  [   96/  130]
train() client id: f_00008-4-3 loss: 0.619836  [  128/  130]
train() client id: f_00008-5-0 loss: 0.600774  [   32/  130]
train() client id: f_00008-5-1 loss: 0.653978  [   64/  130]
train() client id: f_00008-5-2 loss: 0.688333  [   96/  130]
train() client id: f_00008-5-3 loss: 0.653035  [  128/  130]
train() client id: f_00008-6-0 loss: 0.764006  [   32/  130]
train() client id: f_00008-6-1 loss: 0.655268  [   64/  130]
train() client id: f_00008-6-2 loss: 0.592366  [   96/  130]
train() client id: f_00008-6-3 loss: 0.554448  [  128/  130]
train() client id: f_00008-7-0 loss: 0.710483  [   32/  130]
train() client id: f_00008-7-1 loss: 0.587373  [   64/  130]
train() client id: f_00008-7-2 loss: 0.665207  [   96/  130]
train() client id: f_00008-7-3 loss: 0.636037  [  128/  130]
train() client id: f_00008-8-0 loss: 0.641491  [   32/  130]
train() client id: f_00008-8-1 loss: 0.614450  [   64/  130]
train() client id: f_00008-8-2 loss: 0.733215  [   96/  130]
train() client id: f_00008-8-3 loss: 0.576424  [  128/  130]
train() client id: f_00008-9-0 loss: 0.622635  [   32/  130]
train() client id: f_00008-9-1 loss: 0.508360  [   64/  130]
train() client id: f_00008-9-2 loss: 0.749995  [   96/  130]
train() client id: f_00008-9-3 loss: 0.718177  [  128/  130]
train() client id: f_00008-10-0 loss: 0.637484  [   32/  130]
train() client id: f_00008-10-1 loss: 0.588463  [   64/  130]
train() client id: f_00008-10-2 loss: 0.630917  [   96/  130]
train() client id: f_00008-10-3 loss: 0.706278  [  128/  130]
train() client id: f_00009-0-0 loss: 1.025324  [   32/  118]
train() client id: f_00009-0-1 loss: 1.062637  [   64/  118]
train() client id: f_00009-0-2 loss: 0.927314  [   96/  118]
train() client id: f_00009-1-0 loss: 0.863910  [   32/  118]
train() client id: f_00009-1-1 loss: 0.922603  [   64/  118]
train() client id: f_00009-1-2 loss: 1.176348  [   96/  118]
train() client id: f_00009-2-0 loss: 0.968049  [   32/  118]
train() client id: f_00009-2-1 loss: 1.071443  [   64/  118]
train() client id: f_00009-2-2 loss: 0.925937  [   96/  118]
train() client id: f_00009-3-0 loss: 0.833748  [   32/  118]
train() client id: f_00009-3-1 loss: 1.068853  [   64/  118]
train() client id: f_00009-3-2 loss: 0.996764  [   96/  118]
train() client id: f_00009-4-0 loss: 0.979635  [   32/  118]
train() client id: f_00009-4-1 loss: 0.980817  [   64/  118]
train() client id: f_00009-4-2 loss: 0.776204  [   96/  118]
train() client id: f_00009-5-0 loss: 0.914808  [   32/  118]
train() client id: f_00009-5-1 loss: 0.953185  [   64/  118]
train() client id: f_00009-5-2 loss: 0.830398  [   96/  118]
train() client id: f_00009-6-0 loss: 0.777790  [   32/  118]
train() client id: f_00009-6-1 loss: 0.798778  [   64/  118]
train() client id: f_00009-6-2 loss: 0.998479  [   96/  118]
train() client id: f_00009-7-0 loss: 0.904913  [   32/  118]
train() client id: f_00009-7-1 loss: 0.793786  [   64/  118]
train() client id: f_00009-7-2 loss: 1.002141  [   96/  118]
train() client id: f_00009-8-0 loss: 0.927099  [   32/  118]
train() client id: f_00009-8-1 loss: 0.779200  [   64/  118]
train() client id: f_00009-8-2 loss: 0.914243  [   96/  118]
train() client id: f_00009-9-0 loss: 0.778936  [   32/  118]
train() client id: f_00009-9-1 loss: 0.821657  [   64/  118]
train() client id: f_00009-9-2 loss: 0.776797  [   96/  118]
train() client id: f_00009-10-0 loss: 0.905765  [   32/  118]
train() client id: f_00009-10-1 loss: 0.910723  [   64/  118]
train() client id: f_00009-10-2 loss: 0.764125  [   96/  118]
At round 39 accuracy: 0.649867374005305
At round 39 training accuracy: 0.5814889336016097
At round 39 training loss: 0.8385879539295601
update_location
xs = -4.528292 96.001589 105.045120 -105.943528 4.896481 -30.217951 -147.215960 168.375741 -1.680116 89.695607 
ys = 182.587959 15.555839 1.320614 22.544824 9.350187 2.814151 -3.124922 -19.177652 -112.154970 4.001482 
xs mean: 17.442869159412865
ys mean: 10.371751218646876
dists_uav = 208.227924 139.492972 145.038689 147.418792 100.555465 104.503799 177.995236 196.769338 150.271621 134.392387 
uav_gains = -108.206333 -103.615529 -104.040007 -104.217437 -100.060159 -100.478332 -106.300269 -107.479063 -104.426586 -103.210402 
uav_gains_db_mean: -104.20341164902862
dists_bs = 170.640500 314.426942 329.529662 167.365997 244.545710 224.966239 180.278786 394.476691 335.406856 315.125781 
bs_gains = -102.065358 -109.497612 -110.068106 -101.829742 -106.441141 -105.426345 -102.733509 -112.255657 -110.283073 -109.524609 
bs_gains_db_mean: -107.012515119848
Round 40
-------------------------------
ene_coms = [0.00951987 0.01033478 0.00756823 0.00763533 0.00858614 0.00812526
 0.00851917 0.00911262 0.01089772 0.01035322]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.69763365 11.84142394  5.5808586   2.00859179 13.62452771  6.5595183
  2.50677569  8.02972822  5.89424435  5.35690977]
obj_prev = 67.1002120261657
eta_min = 9.613499849974312e-17	eta_max = 0.9429937684538493
af = 14.139867113019362	bf = 1.3127154431937862	zeta = 15.5538538243213	eta = 0.909090909090909
af = 14.139867113019362	bf = 1.3127154431937862	zeta = 29.264275921594102	eta = 0.4831784374540276
af = 14.139867113019362	bf = 1.3127154431937862	zeta = 22.440279066327225	eta = 0.6301110191734178
af = 14.139867113019362	bf = 1.3127154431937862	zeta = 21.204589293616195	eta = 0.6668305109439813
af = 14.139867113019362	bf = 1.3127154431937862	zeta = 21.13778045301289	eta = 0.6689381198016902
af = 14.139867113019362	bf = 1.3127154431937862	zeta = 21.137568017211382	eta = 0.6689448427324229
eta = 0.6689448427324229
ene_coms = [0.00951987 0.01033478 0.00756823 0.00763533 0.00858614 0.00812526
 0.00851917 0.00911262 0.01089772 0.01035322]
ene_comp = [0.03317651 0.06977598 0.03264989 0.01132214 0.08057151 0.03844262
 0.0142185  0.0471317  0.03422973 0.03107006]
ene_total = [1.86759294 3.50414552 1.75919117 0.82922343 3.89986775 2.0369377
 0.99457419 2.46019743 1.97393147 1.81190641]
ti_comp = [0.54492542 0.53677628 0.56444181 0.56377082 0.5542627  0.55887143
 0.55493242 0.54899786 0.53114684 0.53659187]
ti_coms = [0.09519865 0.1033478  0.07568226 0.07635326 0.08586137 0.08125264
 0.08519166 0.09112621 0.10897723 0.10353221]
t_total = [27.99983215 27.99983215 27.99983215 27.99983215 27.99983215 27.99983215
 27.99983215 27.99983215 27.99983215 27.99983215]
ene_coms = [0.00951987 0.01033478 0.00756823 0.00763533 0.00858614 0.00812526
 0.00851917 0.00911262 0.01089772 0.01035322]
ene_comp = [7.68595406e-06 7.36904620e-05 6.82789747e-06 2.85404274e-07
 1.06412650e-04 1.13682871e-05 5.83393217e-07 2.17109122e-05
 8.88507587e-06 6.51055684e-06]
ene_total = [0.41674707 0.45527957 0.33134241 0.33399124 0.38022307 0.35590654
 0.37266454 0.39954717 0.47706878 0.45314767]
optimize_network iter = 0 obj = 3.975918064689806
eta = 0.6689448427324229
freqs = [30441326.29245647 64995404.89362554 28922285.15672572 10041439.10494345
 72683505.94245087 34393077.60183965 12811021.45361711 42925214.22426704
 32222471.25816865 28951296.32005942]
eta_min = 0.6689448427324343	eta_max = 0.6977165862275222
af = 0.008786729887861106	bf = 1.3127154431937862	zeta = 0.009665402876647217	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00951987 0.01033478 0.00756823 0.00763533 0.00858614 0.00812526
 0.00851917 0.00911262 0.01089772 0.01035322]
ene_comp = [1.82201959e-06 1.74689394e-05 1.61861011e-06 6.76574664e-08
 2.52260074e-05 2.69494739e-06 1.38298234e-07 5.14675305e-06
 2.10628143e-06 1.54338187e-06]
ene_total = [1.51669489 1.64899372 1.20578895 1.21623005 1.37169081 1.29469009
 1.3570268  1.45235519 1.73621709 1.64939439]
ti_comp = [0.48929274 0.4811436  0.50880913 0.50813814 0.49863002 0.50323875
 0.49929974 0.49336518 0.47551416 0.48095919]
ti_coms = [0.09519865 0.1033478  0.07568226 0.07635326 0.08586137 0.08125264
 0.08519166 0.09112621 0.10897723 0.10353221]
t_total = [27.99983215 27.99983215 27.99983215 27.99983215 27.99983215 27.99983215
 27.99983215 27.99983215 27.99983215 27.99983215]
ene_coms = [0.00951987 0.01033478 0.00756823 0.00763533 0.00858614 0.00812526
 0.00851917 0.00911262 0.01089772 0.01035322]
ene_comp = [6.84039106e-06 6.58104774e-05 6.02923355e-06 2.52085943e-07
 9.43439985e-05 1.00604465e-05 5.17089176e-07 1.92898540e-05
 7.95444700e-06 5.81482183e-06]
ene_total = [0.45637312 0.49823621 0.36284176 0.36577938 0.41583508 0.38971955
 0.40813208 0.43746063 0.52243225 0.49624555]
optimize_network iter = 1 obj = 4.353055602914066
eta = 0.6977165862275222
freqs = [30351539.03226437 64915754.41171952 28724054.08787028  9973913.81573195
 72330580.5596175  34194615.53007852 12747099.28366816 42762538.60106516
 32222471.25816862 28916966.1958212 ]
eta_min = 0.6977165862275304	eta_max = 0.6977165862275105
af = 0.008726578773597858	bf = 1.3127154431937862	zeta = 0.009599236650957644	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00951987 0.01033478 0.00756823 0.00763533 0.00858614 0.00812526
 0.00851917 0.00911262 0.01089772 0.01035322]
ene_comp = [1.81128728e-06 1.74261500e-05 1.59649850e-06 6.67505787e-08
 2.49816250e-05 2.66393524e-06 1.36921565e-07 5.10781720e-06
 2.10628143e-06 1.53972379e-06]
ene_total = [1.51669318 1.6489869  1.20578543 1.21622991 1.37165188 1.29468515
 1.35702659 1.45234899 1.73621709 1.64939381]
ti_comp = [0.48929274 0.4811436  0.50880913 0.50813814 0.49863002 0.50323875
 0.49929974 0.49336518 0.47551416 0.48095919]
ti_coms = [0.09519865 0.1033478  0.07568226 0.07635326 0.08586137 0.08125264
 0.08519166 0.09112621 0.10897723 0.10353221]
t_total = [27.99983215 27.99983215 27.99983215 27.99983215 27.99983215 27.99983215
 27.99983215 27.99983215 27.99983215 27.99983215]
ene_coms = [0.00951987 0.01033478 0.00756823 0.00763533 0.00858614 0.00812526
 0.00851917 0.00911262 0.01089772 0.01035322]
ene_comp = [6.84039106e-06 6.58104774e-05 6.02923355e-06 2.52085943e-07
 9.43439985e-05 1.00604465e-05 5.17089176e-07 1.92898540e-05
 7.95444700e-06 5.81482183e-06]
ene_total = [0.45637312 0.49823621 0.36284176 0.36577938 0.41583508 0.38971955
 0.40813208 0.43746063 0.52243225 0.49624555]
optimize_network iter = 2 obj = 4.353055602913899
eta = 0.6977165862275105
freqs = [30351539.03226439 64915754.4117195  28724054.08787033  9973913.81573197
 72330580.55961758 34194615.53007858 12747099.28366818 42762538.60106518
 32222471.25816859 28916966.19582118]
Done!
ene_coms = [0.00951987 0.01033478 0.00756823 0.00763533 0.00858614 0.00812526
 0.00851917 0.00911262 0.01089772 0.01035322]
ene_comp = [6.38402634e-06 6.14198541e-05 5.62698615e-06 2.35267733e-07
 8.80497278e-05 9.38925200e-06 4.82590964e-07 1.80029087e-05
 7.42375673e-06 5.42687916e-06]
ene_total = [0.00952625 0.0103962  0.00757385 0.00763556 0.00867419 0.00813465
 0.00851965 0.00913062 0.01090515 0.01035865]
At round 40 energy consumption: 0.09085477091779268
At round 40 eta: 0.6977165862275105
At round 40 a_n: 14.480768977481684
At round 40 local rounds: 11.786339484059987
At round 40 global rounds: 47.904609772537796
gradient difference: 0.4665285646915436
train() client id: f_00000-0-0 loss: 1.010176  [   32/  126]
train() client id: f_00000-0-1 loss: 1.224946  [   64/  126]
train() client id: f_00000-0-2 loss: 1.185898  [   96/  126]
train() client id: f_00000-1-0 loss: 1.042453  [   32/  126]
train() client id: f_00000-1-1 loss: 1.173723  [   64/  126]
train() client id: f_00000-1-2 loss: 1.068286  [   96/  126]
train() client id: f_00000-2-0 loss: 1.159819  [   32/  126]
train() client id: f_00000-2-1 loss: 0.954322  [   64/  126]
train() client id: f_00000-2-2 loss: 0.977527  [   96/  126]
train() client id: f_00000-3-0 loss: 1.114035  [   32/  126]
train() client id: f_00000-3-1 loss: 0.893084  [   64/  126]
train() client id: f_00000-3-2 loss: 0.973670  [   96/  126]
train() client id: f_00000-4-0 loss: 1.035645  [   32/  126]
train() client id: f_00000-4-1 loss: 0.944640  [   64/  126]
train() client id: f_00000-4-2 loss: 1.012397  [   96/  126]
train() client id: f_00000-5-0 loss: 0.938330  [   32/  126]
train() client id: f_00000-5-1 loss: 0.982342  [   64/  126]
train() client id: f_00000-5-2 loss: 0.957668  [   96/  126]
train() client id: f_00000-6-0 loss: 0.946038  [   32/  126]
train() client id: f_00000-6-1 loss: 0.941317  [   64/  126]
train() client id: f_00000-6-2 loss: 0.880003  [   96/  126]
train() client id: f_00000-7-0 loss: 1.004457  [   32/  126]
train() client id: f_00000-7-1 loss: 1.010758  [   64/  126]
train() client id: f_00000-7-2 loss: 0.942664  [   96/  126]
train() client id: f_00000-8-0 loss: 0.959192  [   32/  126]
train() client id: f_00000-8-1 loss: 1.034109  [   64/  126]
train() client id: f_00000-8-2 loss: 0.838884  [   96/  126]
train() client id: f_00000-9-0 loss: 0.891416  [   32/  126]
train() client id: f_00000-9-1 loss: 0.976285  [   64/  126]
train() client id: f_00000-9-2 loss: 0.969002  [   96/  126]
train() client id: f_00000-10-0 loss: 0.895690  [   32/  126]
train() client id: f_00000-10-1 loss: 1.084363  [   64/  126]
train() client id: f_00000-10-2 loss: 0.858730  [   96/  126]
train() client id: f_00001-0-0 loss: 0.425014  [   32/  265]
train() client id: f_00001-0-1 loss: 0.355800  [   64/  265]
train() client id: f_00001-0-2 loss: 0.428394  [   96/  265]
train() client id: f_00001-0-3 loss: 0.484198  [  128/  265]
train() client id: f_00001-0-4 loss: 0.433188  [  160/  265]
train() client id: f_00001-0-5 loss: 0.319251  [  192/  265]
train() client id: f_00001-0-6 loss: 0.491885  [  224/  265]
train() client id: f_00001-0-7 loss: 0.413149  [  256/  265]
train() client id: f_00001-1-0 loss: 0.390299  [   32/  265]
train() client id: f_00001-1-1 loss: 0.419317  [   64/  265]
train() client id: f_00001-1-2 loss: 0.456279  [   96/  265]
train() client id: f_00001-1-3 loss: 0.481375  [  128/  265]
train() client id: f_00001-1-4 loss: 0.365854  [  160/  265]
train() client id: f_00001-1-5 loss: 0.395932  [  192/  265]
train() client id: f_00001-1-6 loss: 0.418500  [  224/  265]
train() client id: f_00001-1-7 loss: 0.372221  [  256/  265]
train() client id: f_00001-2-0 loss: 0.443412  [   32/  265]
train() client id: f_00001-2-1 loss: 0.492627  [   64/  265]
train() client id: f_00001-2-2 loss: 0.322890  [   96/  265]
train() client id: f_00001-2-3 loss: 0.413303  [  128/  265]
train() client id: f_00001-2-4 loss: 0.448494  [  160/  265]
train() client id: f_00001-2-5 loss: 0.326470  [  192/  265]
train() client id: f_00001-2-6 loss: 0.374559  [  224/  265]
train() client id: f_00001-2-7 loss: 0.419738  [  256/  265]
train() client id: f_00001-3-0 loss: 0.367739  [   32/  265]
train() client id: f_00001-3-1 loss: 0.479184  [   64/  265]
train() client id: f_00001-3-2 loss: 0.464621  [   96/  265]
train() client id: f_00001-3-3 loss: 0.365018  [  128/  265]
train() client id: f_00001-3-4 loss: 0.338670  [  160/  265]
train() client id: f_00001-3-5 loss: 0.320936  [  192/  265]
train() client id: f_00001-3-6 loss: 0.485354  [  224/  265]
train() client id: f_00001-3-7 loss: 0.318426  [  256/  265]
train() client id: f_00001-4-0 loss: 0.365545  [   32/  265]
train() client id: f_00001-4-1 loss: 0.302528  [   64/  265]
train() client id: f_00001-4-2 loss: 0.400405  [   96/  265]
train() client id: f_00001-4-3 loss: 0.372214  [  128/  265]
train() client id: f_00001-4-4 loss: 0.347845  [  160/  265]
train() client id: f_00001-4-5 loss: 0.462249  [  192/  265]
train() client id: f_00001-4-6 loss: 0.450489  [  224/  265]
train() client id: f_00001-4-7 loss: 0.282201  [  256/  265]
train() client id: f_00001-5-0 loss: 0.411404  [   32/  265]
train() client id: f_00001-5-1 loss: 0.379469  [   64/  265]
train() client id: f_00001-5-2 loss: 0.383561  [   96/  265]
train() client id: f_00001-5-3 loss: 0.368928  [  128/  265]
train() client id: f_00001-5-4 loss: 0.448867  [  160/  265]
train() client id: f_00001-5-5 loss: 0.387682  [  192/  265]
train() client id: f_00001-5-6 loss: 0.347524  [  224/  265]
train() client id: f_00001-5-7 loss: 0.429443  [  256/  265]
train() client id: f_00001-6-0 loss: 0.291219  [   32/  265]
train() client id: f_00001-6-1 loss: 0.373760  [   64/  265]
train() client id: f_00001-6-2 loss: 0.336303  [   96/  265]
train() client id: f_00001-6-3 loss: 0.479145  [  128/  265]
train() client id: f_00001-6-4 loss: 0.405236  [  160/  265]
train() client id: f_00001-6-5 loss: 0.395869  [  192/  265]
train() client id: f_00001-6-6 loss: 0.382277  [  224/  265]
train() client id: f_00001-6-7 loss: 0.418097  [  256/  265]
train() client id: f_00001-7-0 loss: 0.503667  [   32/  265]
train() client id: f_00001-7-1 loss: 0.316940  [   64/  265]
train() client id: f_00001-7-2 loss: 0.443690  [   96/  265]
train() client id: f_00001-7-3 loss: 0.413285  [  128/  265]
train() client id: f_00001-7-4 loss: 0.303248  [  160/  265]
train() client id: f_00001-7-5 loss: 0.452141  [  192/  265]
train() client id: f_00001-7-6 loss: 0.414264  [  224/  265]
train() client id: f_00001-7-7 loss: 0.289152  [  256/  265]
train() client id: f_00001-8-0 loss: 0.484726  [   32/  265]
train() client id: f_00001-8-1 loss: 0.435784  [   64/  265]
train() client id: f_00001-8-2 loss: 0.292404  [   96/  265]
train() client id: f_00001-8-3 loss: 0.295689  [  128/  265]
train() client id: f_00001-8-4 loss: 0.373267  [  160/  265]
train() client id: f_00001-8-5 loss: 0.471864  [  192/  265]
train() client id: f_00001-8-6 loss: 0.341630  [  224/  265]
train() client id: f_00001-8-7 loss: 0.419717  [  256/  265]
train() client id: f_00001-9-0 loss: 0.437746  [   32/  265]
train() client id: f_00001-9-1 loss: 0.292722  [   64/  265]
train() client id: f_00001-9-2 loss: 0.471028  [   96/  265]
train() client id: f_00001-9-3 loss: 0.367763  [  128/  265]
train() client id: f_00001-9-4 loss: 0.409423  [  160/  265]
train() client id: f_00001-9-5 loss: 0.386676  [  192/  265]
train() client id: f_00001-9-6 loss: 0.307294  [  224/  265]
train() client id: f_00001-9-7 loss: 0.410629  [  256/  265]
train() client id: f_00001-10-0 loss: 0.376757  [   32/  265]
train() client id: f_00001-10-1 loss: 0.394203  [   64/  265]
train() client id: f_00001-10-2 loss: 0.333913  [   96/  265]
train() client id: f_00001-10-3 loss: 0.424320  [  128/  265]
train() client id: f_00001-10-4 loss: 0.374616  [  160/  265]
train() client id: f_00001-10-5 loss: 0.434316  [  192/  265]
train() client id: f_00001-10-6 loss: 0.305315  [  224/  265]
train() client id: f_00001-10-7 loss: 0.481204  [  256/  265]
train() client id: f_00002-0-0 loss: 1.412722  [   32/  124]
train() client id: f_00002-0-1 loss: 1.153106  [   64/  124]
train() client id: f_00002-0-2 loss: 1.096803  [   96/  124]
train() client id: f_00002-1-0 loss: 1.143104  [   32/  124]
train() client id: f_00002-1-1 loss: 1.299932  [   64/  124]
train() client id: f_00002-1-2 loss: 1.287531  [   96/  124]
train() client id: f_00002-2-0 loss: 1.134221  [   32/  124]
train() client id: f_00002-2-1 loss: 1.154308  [   64/  124]
train() client id: f_00002-2-2 loss: 1.159528  [   96/  124]
train() client id: f_00002-3-0 loss: 1.052120  [   32/  124]
train() client id: f_00002-3-1 loss: 1.311875  [   64/  124]
train() client id: f_00002-3-2 loss: 1.063243  [   96/  124]
train() client id: f_00002-4-0 loss: 1.254228  [   32/  124]
train() client id: f_00002-4-1 loss: 1.006105  [   64/  124]
train() client id: f_00002-4-2 loss: 1.102688  [   96/  124]
train() client id: f_00002-5-0 loss: 1.141432  [   32/  124]
train() client id: f_00002-5-1 loss: 1.127551  [   64/  124]
train() client id: f_00002-5-2 loss: 1.045877  [   96/  124]
train() client id: f_00002-6-0 loss: 1.016872  [   32/  124]
train() client id: f_00002-6-1 loss: 0.931274  [   64/  124]
train() client id: f_00002-6-2 loss: 0.992309  [   96/  124]
train() client id: f_00002-7-0 loss: 1.227595  [   32/  124]
train() client id: f_00002-7-1 loss: 1.215351  [   64/  124]
train() client id: f_00002-7-2 loss: 0.916724  [   96/  124]
train() client id: f_00002-8-0 loss: 1.076450  [   32/  124]
train() client id: f_00002-8-1 loss: 0.917814  [   64/  124]
train() client id: f_00002-8-2 loss: 1.156621  [   96/  124]
train() client id: f_00002-9-0 loss: 1.201009  [   32/  124]
train() client id: f_00002-9-1 loss: 0.976214  [   64/  124]
train() client id: f_00002-9-2 loss: 1.060329  [   96/  124]
train() client id: f_00002-10-0 loss: 1.009957  [   32/  124]
train() client id: f_00002-10-1 loss: 1.094491  [   64/  124]
train() client id: f_00002-10-2 loss: 1.116445  [   96/  124]
train() client id: f_00003-0-0 loss: 1.000829  [   32/   43]
train() client id: f_00003-1-0 loss: 0.880046  [   32/   43]
train() client id: f_00003-2-0 loss: 0.782730  [   32/   43]
train() client id: f_00003-3-0 loss: 0.975139  [   32/   43]
train() client id: f_00003-4-0 loss: 0.851554  [   32/   43]
train() client id: f_00003-5-0 loss: 1.139151  [   32/   43]
train() client id: f_00003-6-0 loss: 1.029834  [   32/   43]
train() client id: f_00003-7-0 loss: 0.800429  [   32/   43]
train() client id: f_00003-8-0 loss: 0.777834  [   32/   43]
train() client id: f_00003-9-0 loss: 0.676485  [   32/   43]
train() client id: f_00003-10-0 loss: 0.943246  [   32/   43]
train() client id: f_00004-0-0 loss: 0.991715  [   32/  306]
train() client id: f_00004-0-1 loss: 0.975793  [   64/  306]
train() client id: f_00004-0-2 loss: 0.746741  [   96/  306]
train() client id: f_00004-0-3 loss: 0.947726  [  128/  306]
train() client id: f_00004-0-4 loss: 0.902743  [  160/  306]
train() client id: f_00004-0-5 loss: 0.960290  [  192/  306]
train() client id: f_00004-0-6 loss: 0.935339  [  224/  306]
train() client id: f_00004-0-7 loss: 0.747417  [  256/  306]
train() client id: f_00004-0-8 loss: 0.867153  [  288/  306]
train() client id: f_00004-1-0 loss: 0.866146  [   32/  306]
train() client id: f_00004-1-1 loss: 0.987773  [   64/  306]
train() client id: f_00004-1-2 loss: 0.822689  [   96/  306]
train() client id: f_00004-1-3 loss: 0.826196  [  128/  306]
train() client id: f_00004-1-4 loss: 0.910742  [  160/  306]
train() client id: f_00004-1-5 loss: 0.794765  [  192/  306]
train() client id: f_00004-1-6 loss: 1.048440  [  224/  306]
train() client id: f_00004-1-7 loss: 0.878365  [  256/  306]
train() client id: f_00004-1-8 loss: 0.939844  [  288/  306]
train() client id: f_00004-2-0 loss: 0.944777  [   32/  306]
train() client id: f_00004-2-1 loss: 0.899977  [   64/  306]
train() client id: f_00004-2-2 loss: 1.025025  [   96/  306]
train() client id: f_00004-2-3 loss: 0.800811  [  128/  306]
train() client id: f_00004-2-4 loss: 0.907440  [  160/  306]
train() client id: f_00004-2-5 loss: 0.870143  [  192/  306]
train() client id: f_00004-2-6 loss: 0.913312  [  224/  306]
train() client id: f_00004-2-7 loss: 0.943984  [  256/  306]
train() client id: f_00004-2-8 loss: 0.724179  [  288/  306]
train() client id: f_00004-3-0 loss: 0.937016  [   32/  306]
train() client id: f_00004-3-1 loss: 0.894996  [   64/  306]
train() client id: f_00004-3-2 loss: 0.981357  [   96/  306]
train() client id: f_00004-3-3 loss: 0.732782  [  128/  306]
train() client id: f_00004-3-4 loss: 0.789979  [  160/  306]
train() client id: f_00004-3-5 loss: 1.021424  [  192/  306]
train() client id: f_00004-3-6 loss: 0.802963  [  224/  306]
train() client id: f_00004-3-7 loss: 0.818552  [  256/  306]
train() client id: f_00004-3-8 loss: 0.890325  [  288/  306]
train() client id: f_00004-4-0 loss: 0.724288  [   32/  306]
train() client id: f_00004-4-1 loss: 0.882376  [   64/  306]
train() client id: f_00004-4-2 loss: 1.072557  [   96/  306]
train() client id: f_00004-4-3 loss: 0.857440  [  128/  306]
train() client id: f_00004-4-4 loss: 0.855248  [  160/  306]
train() client id: f_00004-4-5 loss: 0.930250  [  192/  306]
train() client id: f_00004-4-6 loss: 0.940028  [  224/  306]
train() client id: f_00004-4-7 loss: 0.873879  [  256/  306]
train() client id: f_00004-4-8 loss: 0.796398  [  288/  306]
train() client id: f_00004-5-0 loss: 1.042221  [   32/  306]
train() client id: f_00004-5-1 loss: 0.806213  [   64/  306]
train() client id: f_00004-5-2 loss: 0.840025  [   96/  306]
train() client id: f_00004-5-3 loss: 0.919768  [  128/  306]
train() client id: f_00004-5-4 loss: 0.975433  [  160/  306]
train() client id: f_00004-5-5 loss: 0.805902  [  192/  306]
train() client id: f_00004-5-6 loss: 0.904047  [  224/  306]
train() client id: f_00004-5-7 loss: 0.816854  [  256/  306]
train() client id: f_00004-5-8 loss: 0.799066  [  288/  306]
train() client id: f_00004-6-0 loss: 0.921867  [   32/  306]
train() client id: f_00004-6-1 loss: 0.850812  [   64/  306]
train() client id: f_00004-6-2 loss: 0.877517  [   96/  306]
train() client id: f_00004-6-3 loss: 0.865146  [  128/  306]
train() client id: f_00004-6-4 loss: 0.831223  [  160/  306]
train() client id: f_00004-6-5 loss: 1.071176  [  192/  306]
train() client id: f_00004-6-6 loss: 0.830303  [  224/  306]
train() client id: f_00004-6-7 loss: 0.734600  [  256/  306]
train() client id: f_00004-6-8 loss: 0.919664  [  288/  306]
train() client id: f_00004-7-0 loss: 0.853758  [   32/  306]
train() client id: f_00004-7-1 loss: 0.915227  [   64/  306]
train() client id: f_00004-7-2 loss: 0.819991  [   96/  306]
train() client id: f_00004-7-3 loss: 0.936628  [  128/  306]
train() client id: f_00004-7-4 loss: 0.910399  [  160/  306]
train() client id: f_00004-7-5 loss: 0.905879  [  192/  306]
train() client id: f_00004-7-6 loss: 0.854751  [  224/  306]
train() client id: f_00004-7-7 loss: 0.826934  [  256/  306]
train() client id: f_00004-7-8 loss: 0.892876  [  288/  306]
train() client id: f_00004-8-0 loss: 0.859639  [   32/  306]
train() client id: f_00004-8-1 loss: 0.789073  [   64/  306]
train() client id: f_00004-8-2 loss: 0.892688  [   96/  306]
train() client id: f_00004-8-3 loss: 0.869985  [  128/  306]
train() client id: f_00004-8-4 loss: 0.945913  [  160/  306]
train() client id: f_00004-8-5 loss: 0.868479  [  192/  306]
train() client id: f_00004-8-6 loss: 0.891156  [  224/  306]
train() client id: f_00004-8-7 loss: 0.953897  [  256/  306]
train() client id: f_00004-8-8 loss: 0.764353  [  288/  306]
train() client id: f_00004-9-0 loss: 0.752292  [   32/  306]
train() client id: f_00004-9-1 loss: 0.886136  [   64/  306]
train() client id: f_00004-9-2 loss: 0.953458  [   96/  306]
train() client id: f_00004-9-3 loss: 0.979762  [  128/  306]
train() client id: f_00004-9-4 loss: 0.855948  [  160/  306]
train() client id: f_00004-9-5 loss: 0.757983  [  192/  306]
train() client id: f_00004-9-6 loss: 0.871448  [  224/  306]
train() client id: f_00004-9-7 loss: 0.907591  [  256/  306]
train() client id: f_00004-9-8 loss: 0.904561  [  288/  306]
train() client id: f_00004-10-0 loss: 0.868135  [   32/  306]
train() client id: f_00004-10-1 loss: 0.927004  [   64/  306]
train() client id: f_00004-10-2 loss: 0.832287  [   96/  306]
train() client id: f_00004-10-3 loss: 0.824214  [  128/  306]
train() client id: f_00004-10-4 loss: 0.822769  [  160/  306]
train() client id: f_00004-10-5 loss: 0.875908  [  192/  306]
train() client id: f_00004-10-6 loss: 0.841585  [  224/  306]
train() client id: f_00004-10-7 loss: 0.916267  [  256/  306]
train() client id: f_00004-10-8 loss: 0.906340  [  288/  306]
train() client id: f_00005-0-0 loss: 0.343982  [   32/  146]
train() client id: f_00005-0-1 loss: 0.856715  [   64/  146]
train() client id: f_00005-0-2 loss: 0.470969  [   96/  146]
train() client id: f_00005-0-3 loss: 0.704393  [  128/  146]
train() client id: f_00005-1-0 loss: 0.479105  [   32/  146]
train() client id: f_00005-1-1 loss: 0.532371  [   64/  146]
train() client id: f_00005-1-2 loss: 0.614277  [   96/  146]
train() client id: f_00005-1-3 loss: 0.580170  [  128/  146]
train() client id: f_00005-2-0 loss: 0.679091  [   32/  146]
train() client id: f_00005-2-1 loss: 0.603721  [   64/  146]
train() client id: f_00005-2-2 loss: 0.505782  [   96/  146]
train() client id: f_00005-2-3 loss: 0.610537  [  128/  146]
train() client id: f_00005-3-0 loss: 0.488894  [   32/  146]
train() client id: f_00005-3-1 loss: 0.506526  [   64/  146]
train() client id: f_00005-3-2 loss: 0.778857  [   96/  146]
train() client id: f_00005-3-3 loss: 0.641842  [  128/  146]
train() client id: f_00005-4-0 loss: 0.385981  [   32/  146]
train() client id: f_00005-4-1 loss: 0.444512  [   64/  146]
train() client id: f_00005-4-2 loss: 0.711244  [   96/  146]
train() client id: f_00005-4-3 loss: 0.642861  [  128/  146]
train() client id: f_00005-5-0 loss: 0.497108  [   32/  146]
train() client id: f_00005-5-1 loss: 0.840632  [   64/  146]
train() client id: f_00005-5-2 loss: 0.856394  [   96/  146]
train() client id: f_00005-5-3 loss: 0.243070  [  128/  146]
train() client id: f_00005-6-0 loss: 0.504995  [   32/  146]
train() client id: f_00005-6-1 loss: 0.880678  [   64/  146]
train() client id: f_00005-6-2 loss: 0.313684  [   96/  146]
train() client id: f_00005-6-3 loss: 0.660736  [  128/  146]
train() client id: f_00005-7-0 loss: 0.551324  [   32/  146]
train() client id: f_00005-7-1 loss: 0.671914  [   64/  146]
train() client id: f_00005-7-2 loss: 0.464763  [   96/  146]
train() client id: f_00005-7-3 loss: 0.555576  [  128/  146]
train() client id: f_00005-8-0 loss: 0.503843  [   32/  146]
train() client id: f_00005-8-1 loss: 0.713489  [   64/  146]
train() client id: f_00005-8-2 loss: 0.612367  [   96/  146]
train() client id: f_00005-8-3 loss: 0.663400  [  128/  146]
train() client id: f_00005-9-0 loss: 0.526152  [   32/  146]
train() client id: f_00005-9-1 loss: 0.390480  [   64/  146]
train() client id: f_00005-9-2 loss: 0.697871  [   96/  146]
train() client id: f_00005-9-3 loss: 0.634411  [  128/  146]
train() client id: f_00005-10-0 loss: 0.643499  [   32/  146]
train() client id: f_00005-10-1 loss: 0.517585  [   64/  146]
train() client id: f_00005-10-2 loss: 0.657439  [   96/  146]
train() client id: f_00005-10-3 loss: 0.525185  [  128/  146]
train() client id: f_00006-0-0 loss: 0.496552  [   32/   54]
train() client id: f_00006-1-0 loss: 0.556085  [   32/   54]
train() client id: f_00006-2-0 loss: 0.450314  [   32/   54]
train() client id: f_00006-3-0 loss: 0.509295  [   32/   54]
train() client id: f_00006-4-0 loss: 0.460962  [   32/   54]
train() client id: f_00006-5-0 loss: 0.562720  [   32/   54]
train() client id: f_00006-6-0 loss: 0.538498  [   32/   54]
train() client id: f_00006-7-0 loss: 0.553928  [   32/   54]
train() client id: f_00006-8-0 loss: 0.482943  [   32/   54]
train() client id: f_00006-9-0 loss: 0.502213  [   32/   54]
train() client id: f_00006-10-0 loss: 0.462043  [   32/   54]
train() client id: f_00007-0-0 loss: 0.524610  [   32/  179]
train() client id: f_00007-0-1 loss: 0.591988  [   64/  179]
train() client id: f_00007-0-2 loss: 0.429739  [   96/  179]
train() client id: f_00007-0-3 loss: 0.471089  [  128/  179]
train() client id: f_00007-0-4 loss: 0.560260  [  160/  179]
train() client id: f_00007-1-0 loss: 0.482133  [   32/  179]
train() client id: f_00007-1-1 loss: 0.513532  [   64/  179]
train() client id: f_00007-1-2 loss: 0.453359  [   96/  179]
train() client id: f_00007-1-3 loss: 0.516692  [  128/  179]
train() client id: f_00007-1-4 loss: 0.410671  [  160/  179]
train() client id: f_00007-2-0 loss: 0.552191  [   32/  179]
train() client id: f_00007-2-1 loss: 0.361521  [   64/  179]
train() client id: f_00007-2-2 loss: 0.413437  [   96/  179]
train() client id: f_00007-2-3 loss: 0.499448  [  128/  179]
train() client id: f_00007-2-4 loss: 0.680782  [  160/  179]
train() client id: f_00007-3-0 loss: 0.579531  [   32/  179]
train() client id: f_00007-3-1 loss: 0.368918  [   64/  179]
train() client id: f_00007-3-2 loss: 0.517974  [   96/  179]
train() client id: f_00007-3-3 loss: 0.496332  [  128/  179]
train() client id: f_00007-3-4 loss: 0.334612  [  160/  179]
train() client id: f_00007-4-0 loss: 0.575662  [   32/  179]
train() client id: f_00007-4-1 loss: 0.361262  [   64/  179]
train() client id: f_00007-4-2 loss: 0.590473  [   96/  179]
train() client id: f_00007-4-3 loss: 0.532419  [  128/  179]
train() client id: f_00007-4-4 loss: 0.417738  [  160/  179]
train() client id: f_00007-5-0 loss: 0.399377  [   32/  179]
train() client id: f_00007-5-1 loss: 0.315741  [   64/  179]
train() client id: f_00007-5-2 loss: 0.686914  [   96/  179]
train() client id: f_00007-5-3 loss: 0.429413  [  128/  179]
train() client id: f_00007-5-4 loss: 0.485945  [  160/  179]
train() client id: f_00007-6-0 loss: 0.527380  [   32/  179]
train() client id: f_00007-6-1 loss: 0.494960  [   64/  179]
train() client id: f_00007-6-2 loss: 0.488451  [   96/  179]
train() client id: f_00007-6-3 loss: 0.431829  [  128/  179]
train() client id: f_00007-6-4 loss: 0.453760  [  160/  179]
train() client id: f_00007-7-0 loss: 0.501885  [   32/  179]
train() client id: f_00007-7-1 loss: 0.570818  [   64/  179]
train() client id: f_00007-7-2 loss: 0.583967  [   96/  179]
train() client id: f_00007-7-3 loss: 0.402716  [  128/  179]
train() client id: f_00007-7-4 loss: 0.332957  [  160/  179]
train() client id: f_00007-8-0 loss: 0.510049  [   32/  179]
train() client id: f_00007-8-1 loss: 0.306110  [   64/  179]
train() client id: f_00007-8-2 loss: 0.385808  [   96/  179]
train() client id: f_00007-8-3 loss: 0.489380  [  128/  179]
train() client id: f_00007-8-4 loss: 0.366621  [  160/  179]
train() client id: f_00007-9-0 loss: 0.457292  [   32/  179]
train() client id: f_00007-9-1 loss: 0.335865  [   64/  179]
train() client id: f_00007-9-2 loss: 0.588158  [   96/  179]
train() client id: f_00007-9-3 loss: 0.304799  [  128/  179]
train() client id: f_00007-9-4 loss: 0.460910  [  160/  179]
train() client id: f_00007-10-0 loss: 0.355887  [   32/  179]
train() client id: f_00007-10-1 loss: 0.422963  [   64/  179]
train() client id: f_00007-10-2 loss: 0.399879  [   96/  179]
train() client id: f_00007-10-3 loss: 0.506394  [  128/  179]
train() client id: f_00007-10-4 loss: 0.383540  [  160/  179]
train() client id: f_00008-0-0 loss: 0.633623  [   32/  130]
train() client id: f_00008-0-1 loss: 0.853327  [   64/  130]
train() client id: f_00008-0-2 loss: 0.694309  [   96/  130]
train() client id: f_00008-0-3 loss: 0.584381  [  128/  130]
train() client id: f_00008-1-0 loss: 0.600547  [   32/  130]
train() client id: f_00008-1-1 loss: 0.753781  [   64/  130]
train() client id: f_00008-1-2 loss: 0.741880  [   96/  130]
train() client id: f_00008-1-3 loss: 0.674216  [  128/  130]
train() client id: f_00008-2-0 loss: 0.697034  [   32/  130]
train() client id: f_00008-2-1 loss: 0.693547  [   64/  130]
train() client id: f_00008-2-2 loss: 0.685327  [   96/  130]
train() client id: f_00008-2-3 loss: 0.716308  [  128/  130]
train() client id: f_00008-3-0 loss: 0.653649  [   32/  130]
train() client id: f_00008-3-1 loss: 0.705493  [   64/  130]
train() client id: f_00008-3-2 loss: 0.759067  [   96/  130]
train() client id: f_00008-3-3 loss: 0.678183  [  128/  130]
train() client id: f_00008-4-0 loss: 0.778386  [   32/  130]
train() client id: f_00008-4-1 loss: 0.590822  [   64/  130]
train() client id: f_00008-4-2 loss: 0.740793  [   96/  130]
train() client id: f_00008-4-3 loss: 0.635760  [  128/  130]
train() client id: f_00008-5-0 loss: 0.763387  [   32/  130]
train() client id: f_00008-5-1 loss: 0.721435  [   64/  130]
train() client id: f_00008-5-2 loss: 0.668024  [   96/  130]
train() client id: f_00008-5-3 loss: 0.642733  [  128/  130]
train() client id: f_00008-6-0 loss: 0.717169  [   32/  130]
train() client id: f_00008-6-1 loss: 0.727477  [   64/  130]
train() client id: f_00008-6-2 loss: 0.628056  [   96/  130]
train() client id: f_00008-6-3 loss: 0.719507  [  128/  130]
train() client id: f_00008-7-0 loss: 0.649876  [   32/  130]
train() client id: f_00008-7-1 loss: 0.666061  [   64/  130]
train() client id: f_00008-7-2 loss: 0.716850  [   96/  130]
train() client id: f_00008-7-3 loss: 0.749161  [  128/  130]
train() client id: f_00008-8-0 loss: 0.699813  [   32/  130]
train() client id: f_00008-8-1 loss: 0.713410  [   64/  130]
train() client id: f_00008-8-2 loss: 0.662128  [   96/  130]
train() client id: f_00008-8-3 loss: 0.680062  [  128/  130]
train() client id: f_00008-9-0 loss: 0.755389  [   32/  130]
train() client id: f_00008-9-1 loss: 0.606428  [   64/  130]
train() client id: f_00008-9-2 loss: 0.743285  [   96/  130]
train() client id: f_00008-9-3 loss: 0.682665  [  128/  130]
train() client id: f_00008-10-0 loss: 0.728051  [   32/  130]
train() client id: f_00008-10-1 loss: 0.656457  [   64/  130]
train() client id: f_00008-10-2 loss: 0.602527  [   96/  130]
train() client id: f_00008-10-3 loss: 0.793563  [  128/  130]
train() client id: f_00009-0-0 loss: 0.823173  [   32/  118]
train() client id: f_00009-0-1 loss: 1.219084  [   64/  118]
train() client id: f_00009-0-2 loss: 1.069824  [   96/  118]
train() client id: f_00009-1-0 loss: 1.077977  [   32/  118]
train() client id: f_00009-1-1 loss: 0.924209  [   64/  118]
train() client id: f_00009-1-2 loss: 0.963054  [   96/  118]
train() client id: f_00009-2-0 loss: 1.020133  [   32/  118]
train() client id: f_00009-2-1 loss: 1.031108  [   64/  118]
train() client id: f_00009-2-2 loss: 0.832685  [   96/  118]
train() client id: f_00009-3-0 loss: 0.810431  [   32/  118]
train() client id: f_00009-3-1 loss: 0.877371  [   64/  118]
train() client id: f_00009-3-2 loss: 1.146895  [   96/  118]
train() client id: f_00009-4-0 loss: 0.937075  [   32/  118]
train() client id: f_00009-4-1 loss: 0.900072  [   64/  118]
train() client id: f_00009-4-2 loss: 0.999555  [   96/  118]
train() client id: f_00009-5-0 loss: 1.002067  [   32/  118]
train() client id: f_00009-5-1 loss: 0.888512  [   64/  118]
train() client id: f_00009-5-2 loss: 0.897700  [   96/  118]
train() client id: f_00009-6-0 loss: 0.709276  [   32/  118]
train() client id: f_00009-6-1 loss: 0.863878  [   64/  118]
train() client id: f_00009-6-2 loss: 1.037442  [   96/  118]
train() client id: f_00009-7-0 loss: 1.022912  [   32/  118]
train() client id: f_00009-7-1 loss: 0.709895  [   64/  118]
train() client id: f_00009-7-2 loss: 0.747259  [   96/  118]
train() client id: f_00009-8-0 loss: 0.839461  [   32/  118]
train() client id: f_00009-8-1 loss: 0.987274  [   64/  118]
train() client id: f_00009-8-2 loss: 0.778290  [   96/  118]
train() client id: f_00009-9-0 loss: 0.729572  [   32/  118]
train() client id: f_00009-9-1 loss: 0.838244  [   64/  118]
train() client id: f_00009-9-2 loss: 1.069678  [   96/  118]
train() client id: f_00009-10-0 loss: 0.859240  [   32/  118]
train() client id: f_00009-10-1 loss: 0.933351  [   64/  118]
train() client id: f_00009-10-2 loss: 0.949110  [   96/  118]
At round 40 accuracy: 0.649867374005305
At round 40 training accuracy: 0.5855130784708249
At round 40 training loss: 0.8259465945512724
update_location
xs = -4.528292 101.001589 110.045120 -110.943528 4.896481 -35.217951 -152.215960 173.375741 -1.680116 94.695607 
ys = 187.587959 15.555839 1.320614 22.544824 4.350187 2.814151 -3.124922 -19.177652 -117.154970 4.001482 
xs mean: 17.942869159412865
ys mean: 9.871751218646876
dists_uav = 212.625840 142.980087 148.699941 151.052096 100.214269 106.057642 182.152309 201.064492 154.039312 137.779788 
uav_gains = -108.493440 -103.884291 -104.311826 -104.483160 -100.023255 -100.638586 -106.563791 -107.749138 -104.697243 -103.481093 
uav_gains_db_mean: -104.4325822579859
dists_bs = 170.935836 318.746479 333.789229 165.365692 247.959881 221.781397 179.576168 398.836580 339.697378 319.337148 
bs_gains = -102.086386 -109.663530 -110.224284 -101.683531 -106.609739 -105.252962 -102.686023 -112.389319 -110.437641 -109.686044 
bs_gains_db_mean: -107.07194583184898
Round 41
-------------------------------
ene_coms = [0.00969033 0.0104491  0.0076715  0.00773801 0.00866764 0.00805128
 0.00864528 0.00925991 0.0110153  0.01046479]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.56528924 11.562945    5.45031694  1.9625446  13.30340034  6.40329482
  2.44927837  7.84188658  5.75649376  5.23178425]
obj_prev = 65.52723389503151
eta_min = 4.1162713601214933e-17	eta_max = 0.9436554982673295
af = 13.805385376324658	bf = 1.295812414954899	zeta = 15.185923913957124	eta = 0.9090909090909091
af = 13.805385376324658	bf = 1.295812414954899	zeta = 28.727667951844392	eta = 0.4805606010020147
af = 13.805385376324658	bf = 1.295812414954899	zeta = 21.970658353966904	eta = 0.6283555619457362
af = 13.805385376324658	bf = 1.295812414954899	zeta = 20.746905901153696	eta = 0.6654189999269706
af = 13.805385376324658	bf = 1.295812414954899	zeta = 20.680419613637774	eta = 0.667558281420008
af = 13.805385376324658	bf = 1.295812414954899	zeta = 20.680205920984783	eta = 0.6675651794316008
eta = 0.6675651794316008
ene_coms = [0.00969033 0.0104491  0.0076715  0.00773801 0.00866764 0.00805128
 0.00864528 0.00925991 0.0110153  0.01046479]
ene_comp = [0.03334687 0.07013429 0.03281755 0.01138028 0.08098525 0.03864002
 0.01429152 0.04737373 0.0344055  0.03122961]
ene_total = [1.83034226 3.42715576 1.72197131 0.81308787 3.81287547 1.98574877
 0.97548599 2.40858927 1.93171514 1.77323409]
ti_comp = [0.56028652 0.55269881 0.58047484 0.57980966 0.57051343 0.57667702
 0.57073702 0.56459074 0.54703682 0.55254186]
ti_coms = [0.09690328 0.10449099 0.07671496 0.07738014 0.08667637 0.08051279
 0.08645279 0.09259906 0.11015298 0.10464794]
t_total = [27.94982796 27.94982796 27.94982796 27.94982796 27.94982796 27.94982796
 27.94982796 27.94982796 27.94982796 27.94982796]
ene_coms = [0.00969033 0.0104491  0.0076715  0.00773801 0.00866764 0.00805128
 0.00864528 0.00925991 0.0110153  0.01046479]
ene_comp = [7.38286399e-06 7.05820292e-05 6.55589267e-06 2.74011033e-07
 1.01992001e-04 1.08424358e-05 5.60070599e-07 2.08461170e-05
 8.50610058e-06 6.23518335e-06]
ene_total = [0.41243696 0.44739477 0.32654226 0.32910405 0.37296627 0.34287643
 0.36770154 0.39470399 0.46883477 0.44532564]
optimize_network iter = 0 obj = 3.907886664671132
eta = 0.6675651794316008
freqs = [29758764.54551297 63447113.51835044 28267852.13001964  9813807.4573661
 70975764.38113679 33502308.78804253 12520228.16463864 41954041.7470303
 31447151.7660712  28259947.24659058]
eta_min = 0.6675651794316073	eta_max = 0.7022007882332124
af = 0.008178762642479813	bf = 1.295812414954899	zeta = 0.008996638906727795	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00969033 0.0104491  0.0076715  0.00773801 0.00866764 0.00805128
 0.00864528 0.00925991 0.0110153  0.01046479]
ene_comp = [1.74122822e-06 1.66465780e-05 1.54618930e-06 6.46247504e-08
 2.40545336e-05 2.55715873e-06 1.32091114e-07 4.91649951e-06
 2.00614049e-06 1.47055089e-06]
ene_total = [1.50731507 1.62763754 1.19331507 1.20342953 1.35173584 1.25253618
 1.34453818 1.44086946 1.71341615 1.62771825]
ti_comp = [0.49181544 0.48422774 0.51200376 0.51133859 0.50204236 0.50820594
 0.50226594 0.49611966 0.47856575 0.48407079]
ti_coms = [0.09690328 0.10449099 0.07671496 0.07738014 0.08667637 0.08051279
 0.08645279 0.09259906 0.11015298 0.10464794]
t_total = [27.94982796 27.94982796 27.94982796 27.94982796 27.94982796 27.94982796
 27.94982796 27.94982796 27.94982796 27.94982796]
ene_coms = [0.00969033 0.0104491  0.0076715  0.00773801 0.00866764 0.00805128
 0.00864528 0.00925991 0.0110153  0.01046479]
ene_comp = [6.41528630e-06 6.15668637e-05 5.64192724e-06 2.35883176e-07
 8.81845074e-05 9.34733529e-06 4.84197508e-07 1.80756924e-05
 7.44141569e-06 5.43922480e-06]
ene_total = [0.4603596  0.49900113 0.36447744 0.36737876 0.41568868 0.38268378
 0.41046355 0.4404786  0.52331216 0.49708156]
optimize_network iter = 1 obj = 4.360925255779727
eta = 0.7022007882332124
freqs = [29658416.42670785 63354255.00122377 28036778.31612519  9735078.66306134
 70560336.48714016 33257736.84805448 12446281.08831986 41768239.07096829
 31447151.76607118 28219720.64346902]
eta_min = 0.7022007882332361	eta_max = 0.7022007882332086
af = 0.008111450206781232	bf = 1.295812414954899	zeta = 0.008922595227459355	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00969033 0.0104491  0.0076715  0.00773801 0.00866764 0.00805128
 0.00864528 0.00925991 0.0110153  0.01046479]
ene_comp = [1.72950499e-06 1.65978872e-05 1.52101416e-06 6.35920379e-08
 2.37737707e-05 2.51995972e-06 1.30535407e-07 4.87304835e-06
 2.00614049e-06 1.46636736e-06]
ene_total = [1.50731325 1.62762997 1.19331116 1.20342937 1.35169218 1.2525304
 1.34453794 1.44086271 1.71341615 1.6277176 ]
ti_comp = [0.49181544 0.48422774 0.51200376 0.51133859 0.50204236 0.50820594
 0.50226594 0.49611966 0.47856575 0.48407079]
ti_coms = [0.09690328 0.10449099 0.07671496 0.07738014 0.08667637 0.08051279
 0.08645279 0.09259906 0.11015298 0.10464794]
t_total = [27.94982796 27.94982796 27.94982796 27.94982796 27.94982796 27.94982796
 27.94982796 27.94982796 27.94982796 27.94982796]
ene_coms = [0.00969033 0.0104491  0.0076715  0.00773801 0.00866764 0.00805128
 0.00864528 0.00925991 0.0110153  0.01046479]
ene_comp = [6.41528630e-06 6.15668637e-05 5.64192724e-06 2.35883176e-07
 8.81845074e-05 9.34733529e-06 4.84197508e-07 1.80756924e-05
 7.44141569e-06 5.43922480e-06]
ene_total = [0.4603596  0.49900113 0.36447744 0.36737876 0.41568868 0.38268378
 0.41046355 0.4404786  0.52331216 0.49708156]
optimize_network iter = 2 obj = 4.360925255779672
eta = 0.7022007882332086
freqs = [29658416.42670785 63354255.00122375 28036778.3161252   9735078.66306135
 70560336.48714018 33257736.84805449 12446281.08831986 41768239.07096829
 31447151.76607117 28219720.64346901]
Done!
ene_coms = [0.00969033 0.0104491  0.0076715  0.00773801 0.00866764 0.00805128
 0.00864528 0.00925991 0.0110153  0.01046479]
ene_comp = [6.09577814e-06 5.85005757e-05 5.36093560e-06 2.24135204e-07
 8.37925491e-05 8.88179879e-06 4.60082441e-07 1.71754472e-05
 7.07080198e-06 5.16832859e-06]
ene_total = [0.00969642 0.0105076  0.00767686 0.00773824 0.00875143 0.00806016
 0.00864574 0.00927708 0.01102237 0.01046996]
At round 41 energy consumption: 0.09184586131008697
At round 41 eta: 0.7022007882332086
At round 41 a_n: 14.138223130512367
At round 41 local rounds: 11.57656130568263
At round 41 global rounds: 47.47569023649434
gradient difference: 0.5015922784805298
train() client id: f_00000-0-0 loss: 1.360109  [   32/  126]
train() client id: f_00000-0-1 loss: 1.058490  [   64/  126]
train() client id: f_00000-0-2 loss: 0.997864  [   96/  126]
train() client id: f_00000-1-0 loss: 0.906583  [   32/  126]
train() client id: f_00000-1-1 loss: 1.110398  [   64/  126]
train() client id: f_00000-1-2 loss: 1.177644  [   96/  126]
train() client id: f_00000-2-0 loss: 1.008445  [   32/  126]
train() client id: f_00000-2-1 loss: 1.066778  [   64/  126]
train() client id: f_00000-2-2 loss: 0.881072  [   96/  126]
train() client id: f_00000-3-0 loss: 0.986548  [   32/  126]
train() client id: f_00000-3-1 loss: 1.100255  [   64/  126]
train() client id: f_00000-3-2 loss: 1.035285  [   96/  126]
train() client id: f_00000-4-0 loss: 1.011082  [   32/  126]
train() client id: f_00000-4-1 loss: 0.821971  [   64/  126]
train() client id: f_00000-4-2 loss: 0.986911  [   96/  126]
train() client id: f_00000-5-0 loss: 0.936310  [   32/  126]
train() client id: f_00000-5-1 loss: 0.954248  [   64/  126]
train() client id: f_00000-5-2 loss: 0.864757  [   96/  126]
train() client id: f_00000-6-0 loss: 0.990241  [   32/  126]
train() client id: f_00000-6-1 loss: 0.972267  [   64/  126]
train() client id: f_00000-6-2 loss: 0.848011  [   96/  126]
train() client id: f_00000-7-0 loss: 0.941095  [   32/  126]
train() client id: f_00000-7-1 loss: 0.859776  [   64/  126]
train() client id: f_00000-7-2 loss: 1.052171  [   96/  126]
train() client id: f_00000-8-0 loss: 0.931959  [   32/  126]
train() client id: f_00000-8-1 loss: 1.004333  [   64/  126]
train() client id: f_00000-8-2 loss: 0.873924  [   96/  126]
train() client id: f_00000-9-0 loss: 0.940839  [   32/  126]
train() client id: f_00000-9-1 loss: 0.810135  [   64/  126]
train() client id: f_00000-9-2 loss: 0.961268  [   96/  126]
train() client id: f_00000-10-0 loss: 0.862799  [   32/  126]
train() client id: f_00000-10-1 loss: 0.890088  [   64/  126]
train() client id: f_00000-10-2 loss: 0.975404  [   96/  126]
train() client id: f_00001-0-0 loss: 0.346409  [   32/  265]
train() client id: f_00001-0-1 loss: 0.475945  [   64/  265]
train() client id: f_00001-0-2 loss: 0.482652  [   96/  265]
train() client id: f_00001-0-3 loss: 0.310582  [  128/  265]
train() client id: f_00001-0-4 loss: 0.567534  [  160/  265]
train() client id: f_00001-0-5 loss: 0.336949  [  192/  265]
train() client id: f_00001-0-6 loss: 0.326214  [  224/  265]
train() client id: f_00001-0-7 loss: 0.367234  [  256/  265]
train() client id: f_00001-1-0 loss: 0.428374  [   32/  265]
train() client id: f_00001-1-1 loss: 0.420081  [   64/  265]
train() client id: f_00001-1-2 loss: 0.444336  [   96/  265]
train() client id: f_00001-1-3 loss: 0.409143  [  128/  265]
train() client id: f_00001-1-4 loss: 0.393926  [  160/  265]
train() client id: f_00001-1-5 loss: 0.302646  [  192/  265]
train() client id: f_00001-1-6 loss: 0.298837  [  224/  265]
train() client id: f_00001-1-7 loss: 0.397714  [  256/  265]
train() client id: f_00001-2-0 loss: 0.303667  [   32/  265]
train() client id: f_00001-2-1 loss: 0.350074  [   64/  265]
train() client id: f_00001-2-2 loss: 0.382881  [   96/  265]
train() client id: f_00001-2-3 loss: 0.293918  [  128/  265]
train() client id: f_00001-2-4 loss: 0.563685  [  160/  265]
train() client id: f_00001-2-5 loss: 0.352327  [  192/  265]
train() client id: f_00001-2-6 loss: 0.405312  [  224/  265]
train() client id: f_00001-2-7 loss: 0.416555  [  256/  265]
train() client id: f_00001-3-0 loss: 0.296746  [   32/  265]
train() client id: f_00001-3-1 loss: 0.614190  [   64/  265]
train() client id: f_00001-3-2 loss: 0.411093  [   96/  265]
train() client id: f_00001-3-3 loss: 0.336348  [  128/  265]
train() client id: f_00001-3-4 loss: 0.349997  [  160/  265]
train() client id: f_00001-3-5 loss: 0.364414  [  192/  265]
train() client id: f_00001-3-6 loss: 0.344305  [  224/  265]
train() client id: f_00001-3-7 loss: 0.311702  [  256/  265]
train() client id: f_00001-4-0 loss: 0.480431  [   32/  265]
train() client id: f_00001-4-1 loss: 0.302796  [   64/  265]
train() client id: f_00001-4-2 loss: 0.464134  [   96/  265]
train() client id: f_00001-4-3 loss: 0.344407  [  128/  265]
train() client id: f_00001-4-4 loss: 0.283702  [  160/  265]
train() client id: f_00001-4-5 loss: 0.383191  [  192/  265]
train() client id: f_00001-4-6 loss: 0.344786  [  224/  265]
train() client id: f_00001-4-7 loss: 0.406172  [  256/  265]
train() client id: f_00001-5-0 loss: 0.291567  [   32/  265]
train() client id: f_00001-5-1 loss: 0.418016  [   64/  265]
train() client id: f_00001-5-2 loss: 0.386179  [   96/  265]
train() client id: f_00001-5-3 loss: 0.403526  [  128/  265]
train() client id: f_00001-5-4 loss: 0.385125  [  160/  265]
train() client id: f_00001-5-5 loss: 0.277230  [  192/  265]
train() client id: f_00001-5-6 loss: 0.429955  [  224/  265]
train() client id: f_00001-5-7 loss: 0.385860  [  256/  265]
train() client id: f_00001-6-0 loss: 0.425616  [   32/  265]
train() client id: f_00001-6-1 loss: 0.343239  [   64/  265]
train() client id: f_00001-6-2 loss: 0.473403  [   96/  265]
train() client id: f_00001-6-3 loss: 0.326725  [  128/  265]
train() client id: f_00001-6-4 loss: 0.371291  [  160/  265]
train() client id: f_00001-6-5 loss: 0.366016  [  192/  265]
train() client id: f_00001-6-6 loss: 0.311038  [  224/  265]
train() client id: f_00001-6-7 loss: 0.337413  [  256/  265]
train() client id: f_00001-7-0 loss: 0.348649  [   32/  265]
train() client id: f_00001-7-1 loss: 0.327591  [   64/  265]
train() client id: f_00001-7-2 loss: 0.344063  [   96/  265]
train() client id: f_00001-7-3 loss: 0.500543  [  128/  265]
train() client id: f_00001-7-4 loss: 0.277423  [  160/  265]
train() client id: f_00001-7-5 loss: 0.374827  [  192/  265]
train() client id: f_00001-7-6 loss: 0.327362  [  224/  265]
train() client id: f_00001-7-7 loss: 0.429416  [  256/  265]
train() client id: f_00001-8-0 loss: 0.500892  [   32/  265]
train() client id: f_00001-8-1 loss: 0.271696  [   64/  265]
train() client id: f_00001-8-2 loss: 0.392325  [   96/  265]
train() client id: f_00001-8-3 loss: 0.260317  [  128/  265]
train() client id: f_00001-8-4 loss: 0.388659  [  160/  265]
train() client id: f_00001-8-5 loss: 0.329318  [  192/  265]
train() client id: f_00001-8-6 loss: 0.423932  [  224/  265]
train() client id: f_00001-8-7 loss: 0.326953  [  256/  265]
train() client id: f_00001-9-0 loss: 0.432845  [   32/  265]
train() client id: f_00001-9-1 loss: 0.270912  [   64/  265]
train() client id: f_00001-9-2 loss: 0.333715  [   96/  265]
train() client id: f_00001-9-3 loss: 0.323808  [  128/  265]
train() client id: f_00001-9-4 loss: 0.346067  [  160/  265]
train() client id: f_00001-9-5 loss: 0.480075  [  192/  265]
train() client id: f_00001-9-6 loss: 0.343415  [  224/  265]
train() client id: f_00001-9-7 loss: 0.390136  [  256/  265]
train() client id: f_00001-10-0 loss: 0.354602  [   32/  265]
train() client id: f_00001-10-1 loss: 0.402546  [   64/  265]
train() client id: f_00001-10-2 loss: 0.349633  [   96/  265]
train() client id: f_00001-10-3 loss: 0.431491  [  128/  265]
train() client id: f_00001-10-4 loss: 0.266816  [  160/  265]
train() client id: f_00001-10-5 loss: 0.312743  [  192/  265]
train() client id: f_00001-10-6 loss: 0.269209  [  224/  265]
train() client id: f_00001-10-7 loss: 0.529125  [  256/  265]
train() client id: f_00002-0-0 loss: 1.045527  [   32/  124]
train() client id: f_00002-0-1 loss: 0.848158  [   64/  124]
train() client id: f_00002-0-2 loss: 1.158212  [   96/  124]
train() client id: f_00002-1-0 loss: 0.888047  [   32/  124]
train() client id: f_00002-1-1 loss: 0.933367  [   64/  124]
train() client id: f_00002-1-2 loss: 1.021500  [   96/  124]
train() client id: f_00002-2-0 loss: 0.858250  [   32/  124]
train() client id: f_00002-2-1 loss: 1.106845  [   64/  124]
train() client id: f_00002-2-2 loss: 1.026815  [   96/  124]
train() client id: f_00002-3-0 loss: 0.731190  [   32/  124]
train() client id: f_00002-3-1 loss: 0.959198  [   64/  124]
train() client id: f_00002-3-2 loss: 0.944214  [   96/  124]
train() client id: f_00002-4-0 loss: 0.936791  [   32/  124]
train() client id: f_00002-4-1 loss: 0.984445  [   64/  124]
train() client id: f_00002-4-2 loss: 0.891905  [   96/  124]
train() client id: f_00002-5-0 loss: 0.912815  [   32/  124]
train() client id: f_00002-5-1 loss: 1.065879  [   64/  124]
train() client id: f_00002-5-2 loss: 0.799063  [   96/  124]
train() client id: f_00002-6-0 loss: 0.824662  [   32/  124]
train() client id: f_00002-6-1 loss: 0.997381  [   64/  124]
train() client id: f_00002-6-2 loss: 0.901605  [   96/  124]
train() client id: f_00002-7-0 loss: 0.903899  [   32/  124]
train() client id: f_00002-7-1 loss: 0.881394  [   64/  124]
train() client id: f_00002-7-2 loss: 1.013591  [   96/  124]
train() client id: f_00002-8-0 loss: 0.992663  [   32/  124]
train() client id: f_00002-8-1 loss: 0.789299  [   64/  124]
train() client id: f_00002-8-2 loss: 0.937873  [   96/  124]
train() client id: f_00002-9-0 loss: 0.745209  [   32/  124]
train() client id: f_00002-9-1 loss: 0.893401  [   64/  124]
train() client id: f_00002-9-2 loss: 0.822609  [   96/  124]
train() client id: f_00002-10-0 loss: 0.796579  [   32/  124]
train() client id: f_00002-10-1 loss: 0.858453  [   64/  124]
train() client id: f_00002-10-2 loss: 0.800894  [   96/  124]
train() client id: f_00003-0-0 loss: 0.715484  [   32/   43]
train() client id: f_00003-1-0 loss: 0.554254  [   32/   43]
train() client id: f_00003-2-0 loss: 0.738701  [   32/   43]
train() client id: f_00003-3-0 loss: 0.644448  [   32/   43]
train() client id: f_00003-4-0 loss: 0.735496  [   32/   43]
train() client id: f_00003-5-0 loss: 0.611007  [   32/   43]
train() client id: f_00003-6-0 loss: 0.641692  [   32/   43]
train() client id: f_00003-7-0 loss: 0.796568  [   32/   43]
train() client id: f_00003-8-0 loss: 0.602513  [   32/   43]
train() client id: f_00003-9-0 loss: 0.633339  [   32/   43]
train() client id: f_00003-10-0 loss: 0.691210  [   32/   43]
train() client id: f_00004-0-0 loss: 0.723579  [   32/  306]
train() client id: f_00004-0-1 loss: 0.783304  [   64/  306]
train() client id: f_00004-0-2 loss: 0.902968  [   96/  306]
train() client id: f_00004-0-3 loss: 0.705147  [  128/  306]
train() client id: f_00004-0-4 loss: 0.756032  [  160/  306]
train() client id: f_00004-0-5 loss: 0.869791  [  192/  306]
train() client id: f_00004-0-6 loss: 0.981954  [  224/  306]
train() client id: f_00004-0-7 loss: 0.805157  [  256/  306]
train() client id: f_00004-0-8 loss: 0.902768  [  288/  306]
train() client id: f_00004-1-0 loss: 0.822362  [   32/  306]
train() client id: f_00004-1-1 loss: 0.771995  [   64/  306]
train() client id: f_00004-1-2 loss: 1.023740  [   96/  306]
train() client id: f_00004-1-3 loss: 0.761798  [  128/  306]
train() client id: f_00004-1-4 loss: 0.779637  [  160/  306]
train() client id: f_00004-1-5 loss: 0.877421  [  192/  306]
train() client id: f_00004-1-6 loss: 0.798684  [  224/  306]
train() client id: f_00004-1-7 loss: 0.794698  [  256/  306]
train() client id: f_00004-1-8 loss: 0.757647  [  288/  306]
train() client id: f_00004-2-0 loss: 0.846399  [   32/  306]
train() client id: f_00004-2-1 loss: 0.795399  [   64/  306]
train() client id: f_00004-2-2 loss: 0.971683  [   96/  306]
train() client id: f_00004-2-3 loss: 0.760348  [  128/  306]
train() client id: f_00004-2-4 loss: 0.902787  [  160/  306]
train() client id: f_00004-2-5 loss: 0.775158  [  192/  306]
train() client id: f_00004-2-6 loss: 0.879277  [  224/  306]
train() client id: f_00004-2-7 loss: 0.704198  [  256/  306]
train() client id: f_00004-2-8 loss: 0.681863  [  288/  306]
train() client id: f_00004-3-0 loss: 0.707393  [   32/  306]
train() client id: f_00004-3-1 loss: 0.812246  [   64/  306]
train() client id: f_00004-3-2 loss: 0.937962  [   96/  306]
train() client id: f_00004-3-3 loss: 0.797690  [  128/  306]
train() client id: f_00004-3-4 loss: 0.808828  [  160/  306]
train() client id: f_00004-3-5 loss: 0.904375  [  192/  306]
train() client id: f_00004-3-6 loss: 0.768358  [  224/  306]
train() client id: f_00004-3-7 loss: 0.734152  [  256/  306]
train() client id: f_00004-3-8 loss: 0.831507  [  288/  306]
train() client id: f_00004-4-0 loss: 0.757472  [   32/  306]
train() client id: f_00004-4-1 loss: 0.899211  [   64/  306]
train() client id: f_00004-4-2 loss: 0.782179  [   96/  306]
train() client id: f_00004-4-3 loss: 0.905170  [  128/  306]
train() client id: f_00004-4-4 loss: 0.729305  [  160/  306]
train() client id: f_00004-4-5 loss: 0.804899  [  192/  306]
train() client id: f_00004-4-6 loss: 0.827734  [  224/  306]
train() client id: f_00004-4-7 loss: 0.869943  [  256/  306]
train() client id: f_00004-4-8 loss: 0.769675  [  288/  306]
train() client id: f_00004-5-0 loss: 0.830933  [   32/  306]
train() client id: f_00004-5-1 loss: 0.748695  [   64/  306]
train() client id: f_00004-5-2 loss: 0.805896  [   96/  306]
train() client id: f_00004-5-3 loss: 0.812889  [  128/  306]
train() client id: f_00004-5-4 loss: 0.883597  [  160/  306]
train() client id: f_00004-5-5 loss: 0.738212  [  192/  306]
train() client id: f_00004-5-6 loss: 0.843350  [  224/  306]
train() client id: f_00004-5-7 loss: 0.866458  [  256/  306]
train() client id: f_00004-5-8 loss: 0.809778  [  288/  306]
train() client id: f_00004-6-0 loss: 0.915818  [   32/  306]
train() client id: f_00004-6-1 loss: 0.789763  [   64/  306]
train() client id: f_00004-6-2 loss: 0.802211  [   96/  306]
train() client id: f_00004-6-3 loss: 0.793256  [  128/  306]
train() client id: f_00004-6-4 loss: 0.899586  [  160/  306]
train() client id: f_00004-6-5 loss: 0.866095  [  192/  306]
train() client id: f_00004-6-6 loss: 0.747943  [  224/  306]
train() client id: f_00004-6-7 loss: 0.735110  [  256/  306]
train() client id: f_00004-6-8 loss: 0.753934  [  288/  306]
train() client id: f_00004-7-0 loss: 0.973266  [   32/  306]
train() client id: f_00004-7-1 loss: 0.680297  [   64/  306]
train() client id: f_00004-7-2 loss: 0.936476  [   96/  306]
train() client id: f_00004-7-3 loss: 0.695081  [  128/  306]
train() client id: f_00004-7-4 loss: 0.751562  [  160/  306]
train() client id: f_00004-7-5 loss: 0.922622  [  192/  306]
train() client id: f_00004-7-6 loss: 0.820799  [  224/  306]
train() client id: f_00004-7-7 loss: 0.775183  [  256/  306]
train() client id: f_00004-7-8 loss: 0.754444  [  288/  306]
train() client id: f_00004-8-0 loss: 0.848805  [   32/  306]
train() client id: f_00004-8-1 loss: 0.713532  [   64/  306]
train() client id: f_00004-8-2 loss: 0.810010  [   96/  306]
train() client id: f_00004-8-3 loss: 0.878805  [  128/  306]
train() client id: f_00004-8-4 loss: 0.728435  [  160/  306]
train() client id: f_00004-8-5 loss: 0.884367  [  192/  306]
train() client id: f_00004-8-6 loss: 0.775888  [  224/  306]
train() client id: f_00004-8-7 loss: 0.801238  [  256/  306]
train() client id: f_00004-8-8 loss: 0.854772  [  288/  306]
train() client id: f_00004-9-0 loss: 0.752175  [   32/  306]
train() client id: f_00004-9-1 loss: 0.727595  [   64/  306]
train() client id: f_00004-9-2 loss: 0.759642  [   96/  306]
train() client id: f_00004-9-3 loss: 0.851117  [  128/  306]
train() client id: f_00004-9-4 loss: 0.908960  [  160/  306]
train() client id: f_00004-9-5 loss: 0.873226  [  192/  306]
train() client id: f_00004-9-6 loss: 0.805775  [  224/  306]
train() client id: f_00004-9-7 loss: 0.799947  [  256/  306]
train() client id: f_00004-9-8 loss: 0.810670  [  288/  306]
train() client id: f_00004-10-0 loss: 0.806001  [   32/  306]
train() client id: f_00004-10-1 loss: 0.793375  [   64/  306]
train() client id: f_00004-10-2 loss: 0.878252  [   96/  306]
train() client id: f_00004-10-3 loss: 0.845105  [  128/  306]
train() client id: f_00004-10-4 loss: 0.673912  [  160/  306]
train() client id: f_00004-10-5 loss: 0.809099  [  192/  306]
train() client id: f_00004-10-6 loss: 0.784920  [  224/  306]
train() client id: f_00004-10-7 loss: 0.752939  [  256/  306]
train() client id: f_00004-10-8 loss: 0.925517  [  288/  306]
train() client id: f_00005-0-0 loss: 0.741659  [   32/  146]
train() client id: f_00005-0-1 loss: 0.762978  [   64/  146]
train() client id: f_00005-0-2 loss: 0.614338  [   96/  146]
train() client id: f_00005-0-3 loss: 0.457650  [  128/  146]
train() client id: f_00005-1-0 loss: 0.655280  [   32/  146]
train() client id: f_00005-1-1 loss: 0.602012  [   64/  146]
train() client id: f_00005-1-2 loss: 0.695329  [   96/  146]
train() client id: f_00005-1-3 loss: 0.625397  [  128/  146]
train() client id: f_00005-2-0 loss: 0.695875  [   32/  146]
train() client id: f_00005-2-1 loss: 0.837395  [   64/  146]
train() client id: f_00005-2-2 loss: 0.478128  [   96/  146]
train() client id: f_00005-2-3 loss: 0.679087  [  128/  146]
train() client id: f_00005-3-0 loss: 0.938817  [   32/  146]
train() client id: f_00005-3-1 loss: 0.532622  [   64/  146]
train() client id: f_00005-3-2 loss: 0.581409  [   96/  146]
train() client id: f_00005-3-3 loss: 0.571538  [  128/  146]
train() client id: f_00005-4-0 loss: 0.617295  [   32/  146]
train() client id: f_00005-4-1 loss: 0.655896  [   64/  146]
train() client id: f_00005-4-2 loss: 0.680768  [   96/  146]
train() client id: f_00005-4-3 loss: 0.678335  [  128/  146]
train() client id: f_00005-5-0 loss: 0.468361  [   32/  146]
train() client id: f_00005-5-1 loss: 0.571590  [   64/  146]
train() client id: f_00005-5-2 loss: 0.954599  [   96/  146]
train() client id: f_00005-5-3 loss: 0.538163  [  128/  146]
train() client id: f_00005-6-0 loss: 0.659216  [   32/  146]
train() client id: f_00005-6-1 loss: 0.747815  [   64/  146]
train() client id: f_00005-6-2 loss: 0.708724  [   96/  146]
train() client id: f_00005-6-3 loss: 0.391363  [  128/  146]
train() client id: f_00005-7-0 loss: 0.688277  [   32/  146]
train() client id: f_00005-7-1 loss: 0.645853  [   64/  146]
train() client id: f_00005-7-2 loss: 0.578708  [   96/  146]
train() client id: f_00005-7-3 loss: 0.609278  [  128/  146]
train() client id: f_00005-8-0 loss: 0.475624  [   32/  146]
train() client id: f_00005-8-1 loss: 0.520853  [   64/  146]
train() client id: f_00005-8-2 loss: 0.708704  [   96/  146]
train() client id: f_00005-8-3 loss: 0.879264  [  128/  146]
train() client id: f_00005-9-0 loss: 0.675802  [   32/  146]
train() client id: f_00005-9-1 loss: 0.867237  [   64/  146]
train() client id: f_00005-9-2 loss: 0.711187  [   96/  146]
train() client id: f_00005-9-3 loss: 0.310828  [  128/  146]
train() client id: f_00005-10-0 loss: 0.626068  [   32/  146]
train() client id: f_00005-10-1 loss: 0.819619  [   64/  146]
train() client id: f_00005-10-2 loss: 0.621226  [   96/  146]
train() client id: f_00005-10-3 loss: 0.647436  [  128/  146]
train() client id: f_00006-0-0 loss: 0.554319  [   32/   54]
train() client id: f_00006-1-0 loss: 0.604600  [   32/   54]
train() client id: f_00006-2-0 loss: 0.492142  [   32/   54]
train() client id: f_00006-3-0 loss: 0.532645  [   32/   54]
train() client id: f_00006-4-0 loss: 0.613349  [   32/   54]
train() client id: f_00006-5-0 loss: 0.527780  [   32/   54]
train() client id: f_00006-6-0 loss: 0.613811  [   32/   54]
train() client id: f_00006-7-0 loss: 0.556859  [   32/   54]
train() client id: f_00006-8-0 loss: 0.564004  [   32/   54]
train() client id: f_00006-9-0 loss: 0.608795  [   32/   54]
train() client id: f_00006-10-0 loss: 0.561840  [   32/   54]
train() client id: f_00007-0-0 loss: 0.493815  [   32/  179]
train() client id: f_00007-0-1 loss: 0.779920  [   64/  179]
train() client id: f_00007-0-2 loss: 0.719092  [   96/  179]
train() client id: f_00007-0-3 loss: 0.506444  [  128/  179]
train() client id: f_00007-0-4 loss: 0.789327  [  160/  179]
train() client id: f_00007-1-0 loss: 0.532405  [   32/  179]
train() client id: f_00007-1-1 loss: 0.839850  [   64/  179]
train() client id: f_00007-1-2 loss: 0.577769  [   96/  179]
train() client id: f_00007-1-3 loss: 0.754329  [  128/  179]
train() client id: f_00007-1-4 loss: 0.493664  [  160/  179]
train() client id: f_00007-2-0 loss: 0.579779  [   32/  179]
train() client id: f_00007-2-1 loss: 0.748180  [   64/  179]
train() client id: f_00007-2-2 loss: 0.531671  [   96/  179]
train() client id: f_00007-2-3 loss: 0.422185  [  128/  179]
train() client id: f_00007-2-4 loss: 0.727902  [  160/  179]
train() client id: f_00007-3-0 loss: 0.611080  [   32/  179]
train() client id: f_00007-3-1 loss: 0.588170  [   64/  179]
train() client id: f_00007-3-2 loss: 0.556427  [   96/  179]
train() client id: f_00007-3-3 loss: 0.757353  [  128/  179]
train() client id: f_00007-3-4 loss: 0.533364  [  160/  179]
train() client id: f_00007-4-0 loss: 0.745039  [   32/  179]
train() client id: f_00007-4-1 loss: 0.471741  [   64/  179]
train() client id: f_00007-4-2 loss: 0.655796  [   96/  179]
train() client id: f_00007-4-3 loss: 0.690656  [  128/  179]
train() client id: f_00007-4-4 loss: 0.605620  [  160/  179]
train() client id: f_00007-5-0 loss: 0.587352  [   32/  179]
train() client id: f_00007-5-1 loss: 0.556833  [   64/  179]
train() client id: f_00007-5-2 loss: 0.838748  [   96/  179]
train() client id: f_00007-5-3 loss: 0.675042  [  128/  179]
train() client id: f_00007-5-4 loss: 0.489534  [  160/  179]
train() client id: f_00007-6-0 loss: 0.750575  [   32/  179]
train() client id: f_00007-6-1 loss: 0.484748  [   64/  179]
train() client id: f_00007-6-2 loss: 0.480795  [   96/  179]
train() client id: f_00007-6-3 loss: 0.578862  [  128/  179]
train() client id: f_00007-6-4 loss: 0.703446  [  160/  179]
train() client id: f_00007-7-0 loss: 0.548048  [   32/  179]
train() client id: f_00007-7-1 loss: 0.638667  [   64/  179]
train() client id: f_00007-7-2 loss: 0.630998  [   96/  179]
train() client id: f_00007-7-3 loss: 0.739829  [  128/  179]
train() client id: f_00007-7-4 loss: 0.462301  [  160/  179]
train() client id: f_00007-8-0 loss: 0.491321  [   32/  179]
train() client id: f_00007-8-1 loss: 0.479816  [   64/  179]
train() client id: f_00007-8-2 loss: 0.778115  [   96/  179]
train() client id: f_00007-8-3 loss: 0.632407  [  128/  179]
train() client id: f_00007-8-4 loss: 0.680343  [  160/  179]
train() client id: f_00007-9-0 loss: 0.623146  [   32/  179]
train() client id: f_00007-9-1 loss: 0.749337  [   64/  179]
train() client id: f_00007-9-2 loss: 0.658233  [   96/  179]
train() client id: f_00007-9-3 loss: 0.682839  [  128/  179]
train() client id: f_00007-9-4 loss: 0.434874  [  160/  179]
train() client id: f_00007-10-0 loss: 0.502340  [   32/  179]
train() client id: f_00007-10-1 loss: 0.663654  [   64/  179]
train() client id: f_00007-10-2 loss: 0.697906  [   96/  179]
train() client id: f_00007-10-3 loss: 0.423209  [  128/  179]
train() client id: f_00007-10-4 loss: 0.728535  [  160/  179]
train() client id: f_00008-0-0 loss: 0.777732  [   32/  130]
train() client id: f_00008-0-1 loss: 0.636414  [   64/  130]
train() client id: f_00008-0-2 loss: 0.639655  [   96/  130]
train() client id: f_00008-0-3 loss: 0.608188  [  128/  130]
train() client id: f_00008-1-0 loss: 0.841179  [   32/  130]
train() client id: f_00008-1-1 loss: 0.617419  [   64/  130]
train() client id: f_00008-1-2 loss: 0.647399  [   96/  130]
train() client id: f_00008-1-3 loss: 0.560306  [  128/  130]
train() client id: f_00008-2-0 loss: 0.672625  [   32/  130]
train() client id: f_00008-2-1 loss: 0.685419  [   64/  130]
train() client id: f_00008-2-2 loss: 0.563397  [   96/  130]
train() client id: f_00008-2-3 loss: 0.749477  [  128/  130]
train() client id: f_00008-3-0 loss: 0.626092  [   32/  130]
train() client id: f_00008-3-1 loss: 0.681148  [   64/  130]
train() client id: f_00008-3-2 loss: 0.682431  [   96/  130]
train() client id: f_00008-3-3 loss: 0.668496  [  128/  130]
train() client id: f_00008-4-0 loss: 0.770661  [   32/  130]
train() client id: f_00008-4-1 loss: 0.644119  [   64/  130]
train() client id: f_00008-4-2 loss: 0.576587  [   96/  130]
train() client id: f_00008-4-3 loss: 0.668529  [  128/  130]
train() client id: f_00008-5-0 loss: 0.508908  [   32/  130]
train() client id: f_00008-5-1 loss: 0.646883  [   64/  130]
train() client id: f_00008-5-2 loss: 0.786089  [   96/  130]
train() client id: f_00008-5-3 loss: 0.693754  [  128/  130]
train() client id: f_00008-6-0 loss: 0.651853  [   32/  130]
train() client id: f_00008-6-1 loss: 0.682953  [   64/  130]
train() client id: f_00008-6-2 loss: 0.622965  [   96/  130]
train() client id: f_00008-6-3 loss: 0.671501  [  128/  130]
train() client id: f_00008-7-0 loss: 0.592561  [   32/  130]
train() client id: f_00008-7-1 loss: 0.628291  [   64/  130]
train() client id: f_00008-7-2 loss: 0.664814  [   96/  130]
train() client id: f_00008-7-3 loss: 0.777698  [  128/  130]
train() client id: f_00008-8-0 loss: 0.714508  [   32/  130]
train() client id: f_00008-8-1 loss: 0.612642  [   64/  130]
train() client id: f_00008-8-2 loss: 0.657178  [   96/  130]
train() client id: f_00008-8-3 loss: 0.672936  [  128/  130]
train() client id: f_00008-9-0 loss: 0.569867  [   32/  130]
train() client id: f_00008-9-1 loss: 0.770411  [   64/  130]
train() client id: f_00008-9-2 loss: 0.651217  [   96/  130]
train() client id: f_00008-9-3 loss: 0.675126  [  128/  130]
train() client id: f_00008-10-0 loss: 0.638376  [   32/  130]
train() client id: f_00008-10-1 loss: 0.647175  [   64/  130]
train() client id: f_00008-10-2 loss: 0.695245  [   96/  130]
train() client id: f_00008-10-3 loss: 0.685117  [  128/  130]
train() client id: f_00009-0-0 loss: 0.928105  [   32/  118]
train() client id: f_00009-0-1 loss: 1.163947  [   64/  118]
train() client id: f_00009-0-2 loss: 1.065000  [   96/  118]
train() client id: f_00009-1-0 loss: 1.062038  [   32/  118]
train() client id: f_00009-1-1 loss: 0.804226  [   64/  118]
train() client id: f_00009-1-2 loss: 1.087636  [   96/  118]
train() client id: f_00009-2-0 loss: 1.081649  [   32/  118]
train() client id: f_00009-2-1 loss: 0.839866  [   64/  118]
train() client id: f_00009-2-2 loss: 0.914098  [   96/  118]
train() client id: f_00009-3-0 loss: 0.890598  [   32/  118]
train() client id: f_00009-3-1 loss: 0.939107  [   64/  118]
train() client id: f_00009-3-2 loss: 1.014538  [   96/  118]
train() client id: f_00009-4-0 loss: 0.951981  [   32/  118]
train() client id: f_00009-4-1 loss: 0.778727  [   64/  118]
train() client id: f_00009-4-2 loss: 0.872975  [   96/  118]
train() client id: f_00009-5-0 loss: 0.937793  [   32/  118]
train() client id: f_00009-5-1 loss: 0.717348  [   64/  118]
train() client id: f_00009-5-2 loss: 0.764327  [   96/  118]
train() client id: f_00009-6-0 loss: 0.810179  [   32/  118]
train() client id: f_00009-6-1 loss: 0.875083  [   64/  118]
train() client id: f_00009-6-2 loss: 0.860610  [   96/  118]
train() client id: f_00009-7-0 loss: 0.689519  [   32/  118]
train() client id: f_00009-7-1 loss: 0.965461  [   64/  118]
train() client id: f_00009-7-2 loss: 0.716161  [   96/  118]
train() client id: f_00009-8-0 loss: 0.880229  [   32/  118]
train() client id: f_00009-8-1 loss: 0.823025  [   64/  118]
train() client id: f_00009-8-2 loss: 0.708783  [   96/  118]
train() client id: f_00009-9-0 loss: 0.805957  [   32/  118]
train() client id: f_00009-9-1 loss: 0.695236  [   64/  118]
train() client id: f_00009-9-2 loss: 0.784527  [   96/  118]
train() client id: f_00009-10-0 loss: 0.687405  [   32/  118]
train() client id: f_00009-10-1 loss: 1.002659  [   64/  118]
train() client id: f_00009-10-2 loss: 0.737671  [   96/  118]
At round 41 accuracy: 0.649867374005305
At round 41 training accuracy: 0.5895372233400402
At round 41 training loss: 0.8270354306513955
update_location
xs = -4.528292 106.001589 115.045120 -115.943528 4.896481 -40.217951 -157.215960 178.375741 -1.680116 99.695607 
ys = 192.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -122.154970 4.001482 
xs mean: 18.442869159412865
ys mean: 9.371751218646876
dists_uav = 217.049827 146.554840 152.437277 154.761658 100.121915 107.821162 186.350807 205.391546 157.875456 141.262967 
uav_gains = -108.788873 -104.153348 -104.582907 -104.748444 -100.013245 -100.817647 -106.827668 -108.024003 -104.966751 -103.752749 
uav_gains_db_mean: -104.66756341335825
dists_bs = 171.376601 323.085645 338.069077 163.493876 251.427128 218.664509 179.010502 403.211328 344.007062 323.570966 
bs_gains = -102.117702 -109.827954 -110.379212 -101.545101 -106.778599 -105.080852 -102.647657 -112.521975 -110.590945 -109.846206 
bs_gains_db_mean: -107.13362029837694
Round 42
-------------------------------
ene_coms = [0.00987189 0.01056475 0.00777726 0.00784325 0.00875077 0.00797912
 0.00877522 0.00941454 0.01113426 0.01057774]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.43298146 11.28440553  5.31973856  1.91646189 12.98223934  6.24714799
  2.39174708  7.65404547  5.6186812   5.10660063]
obj_prev = 63.95404914641084
eta_min = 1.6889766471263014e-17	eta_max = 0.9443414870680567
af = 13.47090363962996	bf = 1.2787048354043702	zeta = 14.817994003592956	eta = 0.9090909090909091
af = 13.47090363962996	bf = 1.2787048354043702	zeta = 28.1888099176666	eta = 0.4778812471677785
af = 13.47090363962996	bf = 1.2787048354043702	zeta = 21.499947890331363	eta = 0.6265551762424454
af = 13.47090363962996	bf = 1.2787048354043702	zeta = 20.288407229603276	eta = 0.6639704875390247
af = 13.47090363962996	bf = 1.2787048354043702	zeta = 20.222261442892158	eta = 0.6661422946030002
af = 13.47090363962996	bf = 1.2787048354043702	zeta = 20.22204653170352	eta = 0.6661493740759957
eta = 0.6661493740759957
ene_coms = [0.00987189 0.01056475 0.00777726 0.00784325 0.00875077 0.00797912
 0.00877522 0.00941454 0.01113426 0.01057774]
ene_comp = [0.03352206 0.07050275 0.03298997 0.01144007 0.08141072 0.03884302
 0.0143666  0.04762261 0.03458625 0.03139368]
ene_total = [1.79316394 3.34994462 1.68462016 0.79684287 3.7257347  1.93482691
 0.95628697 2.35694091 1.88930462 1.73438083]
ti_comp = [0.57644724 0.56951857 0.5973935  0.59673357 0.5876584  0.59537492
 0.58741394 0.58102066 0.56382346 0.5693887 ]
ti_coms = [0.09871886 0.10564753 0.0777726  0.07843253 0.0875077  0.07979118
 0.08775216 0.09414544 0.11134264 0.1057774 ]
t_total = [27.89982376 27.89982376 27.89982376 27.89982376 27.89982376 27.89982376
 27.89982376 27.89982376 27.89982376 27.89982376]
ene_coms = [0.00987189 0.01056475 0.00777726 0.00784325 0.00875077 0.00797912
 0.00877522 0.00941454 0.01113426 0.01057774]
ene_comp = [7.08521564e-06 6.75277704e-05 6.28788569e-06 2.62787681e-07
 9.76506095e-05 1.03332770e-05 5.37097780e-07 1.99956952e-05
 8.13400339e-06 5.96470403e-06]
ene_total = [0.40822776 0.43935671 0.32163879 0.32411687 0.36564245 0.33014735
 0.36263961 0.38986256 0.46043626 0.43734939]
optimize_network iter = 0 obj = 3.8394177496669255
eta = 0.6661493740759957
freqs = [29076435.65346267 61896796.39915188 27611587.71223903  9585575.16616115
 69267046.3923866  32620642.59453281 12228683.4311352  40981857.80400394
 30671174.15605509 27567878.70360248]
eta_min = 0.6661493740759968	eta_max = 0.7068096989129604
af = 0.007599382779264725	bf = 1.2787048354043702	zeta = 0.008359321057191197	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00987189 0.01056475 0.00777726 0.00784325 0.00875077 0.00797912
 0.00877522 0.00941454 0.01113426 0.01057774]
ene_comp = [1.66229553e-06 1.58430056e-05 1.47523023e-06 6.16538451e-08
 2.29102655e-05 2.42433838e-06 1.26011018e-07 4.69128343e-06
 1.90835652e-06 1.39940707e-06]
ene_total = [1.49833509 1.60563134 1.18044211 1.19024228 1.33142753 1.21121868
 1.3316797  1.42939198 1.6899415  1.60541027]
ti_comp = [0.49421743 0.48728876 0.51516369 0.51450376 0.50542859 0.51314511
 0.50518413 0.49879085 0.48159365 0.4871589 ]
ti_coms = [0.09871886 0.10564753 0.0777726  0.07843253 0.0875077  0.07979118
 0.08775216 0.09414544 0.11134264 0.1057774 ]
t_total = [27.89982376 27.89982376 27.89982376 27.89982376 27.89982376 27.89982376
 27.89982376 27.89982376 27.89982376 27.89982376]
ene_coms = [0.00987189 0.01056475 0.00777726 0.00784325 0.00875077 0.00797912
 0.00877522 0.00941454 0.01113426 0.01057774]
ene_comp = [6.00687940e-06 5.74828417e-05 5.26923978e-06 2.20293492e-07
 8.22655632e-05 8.66865234e-06 4.52538243e-07 1.69081282e-05
 6.94771444e-06 5.07782992e-06]
ene_total = [0.46479102 0.49981508 0.36619649 0.36906416 0.41562668 0.37585463
 0.41292735 0.44378434 0.52423483 0.49796031]
optimize_network iter = 1 obj = 4.370254909384719
eta = 0.7068096989129604
freqs = [28968118.08075105 61791292.01593351 27349175.4035231   9496153.01789486
 68790651.51246415 32328121.01492507 12145401.08728695 40775786.63951412
 30671174.15605511 27521947.97073216]
eta_min = 0.7068096989129643	eta_max = 0.7068096989129573
af = 0.007526120588461908	bf = 1.2787048354043702	zeta = 0.0082787326473081	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.00987189 0.01056475 0.00777726 0.00784325 0.00875077 0.00797912
 0.00877522 0.00941454 0.01113426 0.01057774]
ene_comp = [1.64993360e-06 1.57890421e-05 1.44732317e-06 6.05088948e-08
 2.25962114e-05 2.38105342e-06 1.24300490e-07 4.64422321e-06
 1.90835652e-06 1.39474786e-06]
ene_total = [1.49833322 1.60562315 1.18043787 1.1902421  1.33137987 1.21121211
 1.33167944 1.42938484 1.6899415  1.60540956]
ti_comp = [0.49421743 0.48728876 0.51516369 0.51450376 0.50542859 0.51314511
 0.50518413 0.49879085 0.48159365 0.4871589 ]
ti_coms = [0.09871886 0.10564753 0.0777726  0.07843253 0.0875077  0.07979118
 0.08775216 0.09414544 0.11134264 0.1057774 ]
t_total = [27.89982376 27.89982376 27.89982376 27.89982376 27.89982376 27.89982376
 27.89982376 27.89982376 27.89982376 27.89982376]
ene_coms = [0.00987189 0.01056475 0.00777726 0.00784325 0.00875077 0.00797912
 0.00877522 0.00941454 0.01113426 0.01057774]
ene_comp = [6.00687940e-06 5.74828417e-05 5.26923978e-06 2.20293492e-07
 8.22655632e-05 8.66865234e-06 4.52538243e-07 1.69081282e-05
 6.94771444e-06 5.07782992e-06]
ene_total = [0.46479102 0.49981508 0.36619649 0.36906416 0.41562668 0.37585463
 0.41292735 0.44378434 0.52423483 0.49796031]
optimize_network iter = 2 obj = 4.370254909384672
eta = 0.7068096989129573
freqs = [28968118.08075104 61791292.01593347 27349175.40352311  9496153.01789486
 68790651.51246415 32328121.01492507 12145401.08728695 40775786.63951411
 30671174.15605509 27521947.97073215]
Done!
ene_coms = [0.00987189 0.01056475 0.00777726 0.00784325 0.00875077 0.00797912
 0.00877522 0.00941454 0.01113426 0.01057774]
ene_comp = [5.81532243e-06 5.56497369e-05 5.10120584e-06 2.13268421e-07
 7.96421474e-05 8.39221249e-06 4.38106980e-07 1.63689347e-05
 6.72615461e-06 4.91589995e-06]
ene_total = [0.0098777  0.0106204  0.00778236 0.00784347 0.00883041 0.00798751
 0.00877565 0.00943091 0.01114099 0.01058266]
At round 42 energy consumption: 0.09287206754228436
At round 42 eta: 0.7068096989129573
At round 42 a_n: 13.79567728354305
At round 42 local rounds: 11.36234048707482
At round 42 global rounds: 47.053661844862226
gradient difference: 0.436479389667511
train() client id: f_00000-0-0 loss: 0.939765  [   32/  126]
train() client id: f_00000-0-1 loss: 1.430483  [   64/  126]
train() client id: f_00000-0-2 loss: 0.991320  [   96/  126]
train() client id: f_00000-1-0 loss: 0.982517  [   32/  126]
train() client id: f_00000-1-1 loss: 1.239427  [   64/  126]
train() client id: f_00000-1-2 loss: 1.032675  [   96/  126]
train() client id: f_00000-2-0 loss: 1.014636  [   32/  126]
train() client id: f_00000-2-1 loss: 0.734913  [   64/  126]
train() client id: f_00000-2-2 loss: 1.051255  [   96/  126]
train() client id: f_00000-3-0 loss: 1.114710  [   32/  126]
train() client id: f_00000-3-1 loss: 0.820966  [   64/  126]
train() client id: f_00000-3-2 loss: 0.876981  [   96/  126]
train() client id: f_00000-4-0 loss: 0.843997  [   32/  126]
train() client id: f_00000-4-1 loss: 0.865959  [   64/  126]
train() client id: f_00000-4-2 loss: 0.981188  [   96/  126]
train() client id: f_00000-5-0 loss: 0.848605  [   32/  126]
train() client id: f_00000-5-1 loss: 0.887357  [   64/  126]
train() client id: f_00000-5-2 loss: 0.953626  [   96/  126]
train() client id: f_00000-6-0 loss: 0.903278  [   32/  126]
train() client id: f_00000-6-1 loss: 0.923675  [   64/  126]
train() client id: f_00000-6-2 loss: 0.761661  [   96/  126]
train() client id: f_00000-7-0 loss: 0.820096  [   32/  126]
train() client id: f_00000-7-1 loss: 0.778620  [   64/  126]
train() client id: f_00000-7-2 loss: 0.815169  [   96/  126]
train() client id: f_00000-8-0 loss: 0.924432  [   32/  126]
train() client id: f_00000-8-1 loss: 0.799665  [   64/  126]
train() client id: f_00000-8-2 loss: 0.757212  [   96/  126]
train() client id: f_00000-9-0 loss: 0.808317  [   32/  126]
train() client id: f_00000-9-1 loss: 0.889126  [   64/  126]
train() client id: f_00000-9-2 loss: 0.807156  [   96/  126]
train() client id: f_00000-10-0 loss: 0.827830  [   32/  126]
train() client id: f_00000-10-1 loss: 0.915114  [   64/  126]
train() client id: f_00000-10-2 loss: 0.748096  [   96/  126]
train() client id: f_00001-0-0 loss: 0.226341  [   32/  265]
train() client id: f_00001-0-1 loss: 0.332801  [   64/  265]
train() client id: f_00001-0-2 loss: 0.246509  [   96/  265]
train() client id: f_00001-0-3 loss: 0.261514  [  128/  265]
train() client id: f_00001-0-4 loss: 0.399469  [  160/  265]
train() client id: f_00001-0-5 loss: 0.450744  [  192/  265]
train() client id: f_00001-0-6 loss: 0.303501  [  224/  265]
train() client id: f_00001-0-7 loss: 0.327001  [  256/  265]
train() client id: f_00001-1-0 loss: 0.286696  [   32/  265]
train() client id: f_00001-1-1 loss: 0.214484  [   64/  265]
train() client id: f_00001-1-2 loss: 0.270430  [   96/  265]
train() client id: f_00001-1-3 loss: 0.334840  [  128/  265]
train() client id: f_00001-1-4 loss: 0.383931  [  160/  265]
train() client id: f_00001-1-5 loss: 0.338015  [  192/  265]
train() client id: f_00001-1-6 loss: 0.285884  [  224/  265]
train() client id: f_00001-1-7 loss: 0.330263  [  256/  265]
train() client id: f_00001-2-0 loss: 0.281196  [   32/  265]
train() client id: f_00001-2-1 loss: 0.198375  [   64/  265]
train() client id: f_00001-2-2 loss: 0.270179  [   96/  265]
train() client id: f_00001-2-3 loss: 0.280678  [  128/  265]
train() client id: f_00001-2-4 loss: 0.273854  [  160/  265]
train() client id: f_00001-2-5 loss: 0.290373  [  192/  265]
train() client id: f_00001-2-6 loss: 0.454922  [  224/  265]
train() client id: f_00001-2-7 loss: 0.315338  [  256/  265]
train() client id: f_00001-3-0 loss: 0.201923  [   32/  265]
train() client id: f_00001-3-1 loss: 0.407991  [   64/  265]
train() client id: f_00001-3-2 loss: 0.195561  [   96/  265]
train() client id: f_00001-3-3 loss: 0.270692  [  128/  265]
train() client id: f_00001-3-4 loss: 0.269792  [  160/  265]
train() client id: f_00001-3-5 loss: 0.185110  [  192/  265]
train() client id: f_00001-3-6 loss: 0.363590  [  224/  265]
train() client id: f_00001-3-7 loss: 0.359938  [  256/  265]
train() client id: f_00001-4-0 loss: 0.183742  [   32/  265]
train() client id: f_00001-4-1 loss: 0.354136  [   64/  265]
train() client id: f_00001-4-2 loss: 0.217094  [   96/  265]
train() client id: f_00001-4-3 loss: 0.305626  [  128/  265]
train() client id: f_00001-4-4 loss: 0.340793  [  160/  265]
train() client id: f_00001-4-5 loss: 0.321831  [  192/  265]
train() client id: f_00001-4-6 loss: 0.235729  [  224/  265]
train() client id: f_00001-4-7 loss: 0.175001  [  256/  265]
train() client id: f_00001-5-0 loss: 0.270314  [   32/  265]
train() client id: f_00001-5-1 loss: 0.276763  [   64/  265]
train() client id: f_00001-5-2 loss: 0.249091  [   96/  265]
train() client id: f_00001-5-3 loss: 0.433989  [  128/  265]
train() client id: f_00001-5-4 loss: 0.327232  [  160/  265]
train() client id: f_00001-5-5 loss: 0.211619  [  192/  265]
train() client id: f_00001-5-6 loss: 0.195674  [  224/  265]
train() client id: f_00001-5-7 loss: 0.253714  [  256/  265]
train() client id: f_00001-6-0 loss: 0.312779  [   32/  265]
train() client id: f_00001-6-1 loss: 0.294210  [   64/  265]
train() client id: f_00001-6-2 loss: 0.178360  [   96/  265]
train() client id: f_00001-6-3 loss: 0.333043  [  128/  265]
train() client id: f_00001-6-4 loss: 0.220965  [  160/  265]
train() client id: f_00001-6-5 loss: 0.318847  [  192/  265]
train() client id: f_00001-6-6 loss: 0.267274  [  224/  265]
train() client id: f_00001-6-7 loss: 0.248347  [  256/  265]
train() client id: f_00001-7-0 loss: 0.390610  [   32/  265]
train() client id: f_00001-7-1 loss: 0.239856  [   64/  265]
train() client id: f_00001-7-2 loss: 0.186999  [   96/  265]
train() client id: f_00001-7-3 loss: 0.283730  [  128/  265]
train() client id: f_00001-7-4 loss: 0.219982  [  160/  265]
train() client id: f_00001-7-5 loss: 0.161824  [  192/  265]
train() client id: f_00001-7-6 loss: 0.383377  [  224/  265]
train() client id: f_00001-7-7 loss: 0.276373  [  256/  265]
train() client id: f_00001-8-0 loss: 0.209705  [   32/  265]
train() client id: f_00001-8-1 loss: 0.195982  [   64/  265]
train() client id: f_00001-8-2 loss: 0.312602  [   96/  265]
train() client id: f_00001-8-3 loss: 0.166643  [  128/  265]
train() client id: f_00001-8-4 loss: 0.442545  [  160/  265]
train() client id: f_00001-8-5 loss: 0.226487  [  192/  265]
train() client id: f_00001-8-6 loss: 0.184265  [  224/  265]
train() client id: f_00001-8-7 loss: 0.401177  [  256/  265]
train() client id: f_00001-9-0 loss: 0.312177  [   32/  265]
train() client id: f_00001-9-1 loss: 0.362436  [   64/  265]
train() client id: f_00001-9-2 loss: 0.225532  [   96/  265]
train() client id: f_00001-9-3 loss: 0.300756  [  128/  265]
train() client id: f_00001-9-4 loss: 0.169929  [  160/  265]
train() client id: f_00001-9-5 loss: 0.233084  [  192/  265]
train() client id: f_00001-9-6 loss: 0.176126  [  224/  265]
train() client id: f_00001-9-7 loss: 0.237175  [  256/  265]
train() client id: f_00001-10-0 loss: 0.181355  [   32/  265]
train() client id: f_00001-10-1 loss: 0.218649  [   64/  265]
train() client id: f_00001-10-2 loss: 0.397672  [   96/  265]
train() client id: f_00001-10-3 loss: 0.307050  [  128/  265]
train() client id: f_00001-10-4 loss: 0.277970  [  160/  265]
train() client id: f_00001-10-5 loss: 0.279736  [  192/  265]
train() client id: f_00001-10-6 loss: 0.263662  [  224/  265]
train() client id: f_00001-10-7 loss: 0.159209  [  256/  265]
train() client id: f_00002-0-0 loss: 1.216188  [   32/  124]
train() client id: f_00002-0-1 loss: 1.160516  [   64/  124]
train() client id: f_00002-0-2 loss: 1.332800  [   96/  124]
train() client id: f_00002-1-0 loss: 1.347693  [   32/  124]
train() client id: f_00002-1-1 loss: 1.024130  [   64/  124]
train() client id: f_00002-1-2 loss: 1.249344  [   96/  124]
train() client id: f_00002-2-0 loss: 1.212910  [   32/  124]
train() client id: f_00002-2-1 loss: 1.109033  [   64/  124]
train() client id: f_00002-2-2 loss: 1.274848  [   96/  124]
train() client id: f_00002-3-0 loss: 1.164035  [   32/  124]
train() client id: f_00002-3-1 loss: 1.058249  [   64/  124]
train() client id: f_00002-3-2 loss: 1.188310  [   96/  124]
train() client id: f_00002-4-0 loss: 1.122952  [   32/  124]
train() client id: f_00002-4-1 loss: 1.063166  [   64/  124]
train() client id: f_00002-4-2 loss: 1.085891  [   96/  124]
train() client id: f_00002-5-0 loss: 1.170010  [   32/  124]
train() client id: f_00002-5-1 loss: 0.927802  [   64/  124]
train() client id: f_00002-5-2 loss: 1.172478  [   96/  124]
train() client id: f_00002-6-0 loss: 1.115834  [   32/  124]
train() client id: f_00002-6-1 loss: 1.200957  [   64/  124]
train() client id: f_00002-6-2 loss: 0.984725  [   96/  124]
train() client id: f_00002-7-0 loss: 0.940378  [   32/  124]
train() client id: f_00002-7-1 loss: 1.105783  [   64/  124]
train() client id: f_00002-7-2 loss: 1.177633  [   96/  124]
train() client id: f_00002-8-0 loss: 1.191382  [   32/  124]
train() client id: f_00002-8-1 loss: 1.173680  [   64/  124]
train() client id: f_00002-8-2 loss: 0.840915  [   96/  124]
train() client id: f_00002-9-0 loss: 0.980443  [   32/  124]
train() client id: f_00002-9-1 loss: 1.007686  [   64/  124]
train() client id: f_00002-9-2 loss: 1.119678  [   96/  124]
train() client id: f_00002-10-0 loss: 0.961347  [   32/  124]
train() client id: f_00002-10-1 loss: 0.958676  [   64/  124]
train() client id: f_00002-10-2 loss: 1.173632  [   96/  124]
train() client id: f_00003-0-0 loss: 0.629822  [   32/   43]
train() client id: f_00003-1-0 loss: 0.532538  [   32/   43]
train() client id: f_00003-2-0 loss: 0.509456  [   32/   43]
train() client id: f_00003-3-0 loss: 0.650319  [   32/   43]
train() client id: f_00003-4-0 loss: 0.654348  [   32/   43]
train() client id: f_00003-5-0 loss: 0.502942  [   32/   43]
train() client id: f_00003-6-0 loss: 0.762668  [   32/   43]
train() client id: f_00003-7-0 loss: 0.725995  [   32/   43]
train() client id: f_00003-8-0 loss: 0.420968  [   32/   43]
train() client id: f_00003-9-0 loss: 0.543089  [   32/   43]
train() client id: f_00003-10-0 loss: 0.677689  [   32/   43]
train() client id: f_00004-0-0 loss: 0.814611  [   32/  306]
train() client id: f_00004-0-1 loss: 0.632311  [   64/  306]
train() client id: f_00004-0-2 loss: 0.799922  [   96/  306]
train() client id: f_00004-0-3 loss: 0.775451  [  128/  306]
train() client id: f_00004-0-4 loss: 0.737520  [  160/  306]
train() client id: f_00004-0-5 loss: 0.785018  [  192/  306]
train() client id: f_00004-0-6 loss: 0.755291  [  224/  306]
train() client id: f_00004-0-7 loss: 0.726216  [  256/  306]
train() client id: f_00004-0-8 loss: 0.715976  [  288/  306]
train() client id: f_00004-1-0 loss: 0.769598  [   32/  306]
train() client id: f_00004-1-1 loss: 0.700437  [   64/  306]
train() client id: f_00004-1-2 loss: 0.766412  [   96/  306]
train() client id: f_00004-1-3 loss: 0.712153  [  128/  306]
train() client id: f_00004-1-4 loss: 0.702736  [  160/  306]
train() client id: f_00004-1-5 loss: 0.825243  [  192/  306]
train() client id: f_00004-1-6 loss: 0.770799  [  224/  306]
train() client id: f_00004-1-7 loss: 0.792959  [  256/  306]
train() client id: f_00004-1-8 loss: 0.684284  [  288/  306]
train() client id: f_00004-2-0 loss: 0.649501  [   32/  306]
train() client id: f_00004-2-1 loss: 0.831672  [   64/  306]
train() client id: f_00004-2-2 loss: 0.797621  [   96/  306]
train() client id: f_00004-2-3 loss: 0.901338  [  128/  306]
train() client id: f_00004-2-4 loss: 0.822106  [  160/  306]
train() client id: f_00004-2-5 loss: 0.535014  [  192/  306]
train() client id: f_00004-2-6 loss: 0.764744  [  224/  306]
train() client id: f_00004-2-7 loss: 0.667990  [  256/  306]
train() client id: f_00004-2-8 loss: 0.724967  [  288/  306]
train() client id: f_00004-3-0 loss: 0.685690  [   32/  306]
train() client id: f_00004-3-1 loss: 0.724701  [   64/  306]
train() client id: f_00004-3-2 loss: 0.750089  [   96/  306]
train() client id: f_00004-3-3 loss: 0.744222  [  128/  306]
train() client id: f_00004-3-4 loss: 0.783195  [  160/  306]
train() client id: f_00004-3-5 loss: 0.657872  [  192/  306]
train() client id: f_00004-3-6 loss: 0.841041  [  224/  306]
train() client id: f_00004-3-7 loss: 0.702268  [  256/  306]
train() client id: f_00004-3-8 loss: 0.778937  [  288/  306]
train() client id: f_00004-4-0 loss: 0.645549  [   32/  306]
train() client id: f_00004-4-1 loss: 0.752213  [   64/  306]
train() client id: f_00004-4-2 loss: 0.693779  [   96/  306]
train() client id: f_00004-4-3 loss: 0.762029  [  128/  306]
train() client id: f_00004-4-4 loss: 0.761252  [  160/  306]
train() client id: f_00004-4-5 loss: 0.824141  [  192/  306]
train() client id: f_00004-4-6 loss: 0.711747  [  224/  306]
train() client id: f_00004-4-7 loss: 0.740359  [  256/  306]
train() client id: f_00004-4-8 loss: 0.808792  [  288/  306]
train() client id: f_00004-5-0 loss: 0.637409  [   32/  306]
train() client id: f_00004-5-1 loss: 0.717308  [   64/  306]
train() client id: f_00004-5-2 loss: 0.854088  [   96/  306]
train() client id: f_00004-5-3 loss: 0.718334  [  128/  306]
train() client id: f_00004-5-4 loss: 0.751005  [  160/  306]
train() client id: f_00004-5-5 loss: 0.689999  [  192/  306]
train() client id: f_00004-5-6 loss: 0.814723  [  224/  306]
train() client id: f_00004-5-7 loss: 0.873416  [  256/  306]
train() client id: f_00004-5-8 loss: 0.738896  [  288/  306]
train() client id: f_00004-6-0 loss: 0.908554  [   32/  306]
train() client id: f_00004-6-1 loss: 0.672972  [   64/  306]
train() client id: f_00004-6-2 loss: 0.646548  [   96/  306]
train() client id: f_00004-6-3 loss: 0.721579  [  128/  306]
train() client id: f_00004-6-4 loss: 0.736459  [  160/  306]
train() client id: f_00004-6-5 loss: 0.806435  [  192/  306]
train() client id: f_00004-6-6 loss: 0.856254  [  224/  306]
train() client id: f_00004-6-7 loss: 0.817371  [  256/  306]
train() client id: f_00004-6-8 loss: 0.633894  [  288/  306]
train() client id: f_00004-7-0 loss: 0.846062  [   32/  306]
train() client id: f_00004-7-1 loss: 0.783177  [   64/  306]
train() client id: f_00004-7-2 loss: 0.865991  [   96/  306]
train() client id: f_00004-7-3 loss: 0.714820  [  128/  306]
train() client id: f_00004-7-4 loss: 0.704826  [  160/  306]
train() client id: f_00004-7-5 loss: 0.877529  [  192/  306]
train() client id: f_00004-7-6 loss: 0.677912  [  224/  306]
train() client id: f_00004-7-7 loss: 0.658756  [  256/  306]
train() client id: f_00004-7-8 loss: 0.715815  [  288/  306]
train() client id: f_00004-8-0 loss: 0.662827  [   32/  306]
train() client id: f_00004-8-1 loss: 0.752853  [   64/  306]
train() client id: f_00004-8-2 loss: 0.745713  [   96/  306]
train() client id: f_00004-8-3 loss: 0.820144  [  128/  306]
train() client id: f_00004-8-4 loss: 0.917144  [  160/  306]
train() client id: f_00004-8-5 loss: 0.682734  [  192/  306]
train() client id: f_00004-8-6 loss: 0.681782  [  224/  306]
train() client id: f_00004-8-7 loss: 0.886383  [  256/  306]
train() client id: f_00004-8-8 loss: 0.604810  [  288/  306]
train() client id: f_00004-9-0 loss: 0.763571  [   32/  306]
train() client id: f_00004-9-1 loss: 0.746103  [   64/  306]
train() client id: f_00004-9-2 loss: 0.750476  [   96/  306]
train() client id: f_00004-9-3 loss: 0.793878  [  128/  306]
train() client id: f_00004-9-4 loss: 0.733609  [  160/  306]
train() client id: f_00004-9-5 loss: 0.571431  [  192/  306]
train() client id: f_00004-9-6 loss: 0.801506  [  224/  306]
train() client id: f_00004-9-7 loss: 0.967400  [  256/  306]
train() client id: f_00004-9-8 loss: 0.717391  [  288/  306]
train() client id: f_00004-10-0 loss: 0.729170  [   32/  306]
train() client id: f_00004-10-1 loss: 0.769028  [   64/  306]
train() client id: f_00004-10-2 loss: 0.801055  [   96/  306]
train() client id: f_00004-10-3 loss: 0.807872  [  128/  306]
train() client id: f_00004-10-4 loss: 0.695123  [  160/  306]
train() client id: f_00004-10-5 loss: 0.778181  [  192/  306]
train() client id: f_00004-10-6 loss: 0.782775  [  224/  306]
train() client id: f_00004-10-7 loss: 0.693926  [  256/  306]
train() client id: f_00004-10-8 loss: 0.805598  [  288/  306]
train() client id: f_00005-0-0 loss: 0.426821  [   32/  146]
train() client id: f_00005-0-1 loss: 0.515287  [   64/  146]
train() client id: f_00005-0-2 loss: 0.853295  [   96/  146]
train() client id: f_00005-0-3 loss: 0.759496  [  128/  146]
train() client id: f_00005-1-0 loss: 0.587125  [   32/  146]
train() client id: f_00005-1-1 loss: 0.672923  [   64/  146]
train() client id: f_00005-1-2 loss: 0.706105  [   96/  146]
train() client id: f_00005-1-3 loss: 0.510291  [  128/  146]
train() client id: f_00005-2-0 loss: 0.619612  [   32/  146]
train() client id: f_00005-2-1 loss: 0.748652  [   64/  146]
train() client id: f_00005-2-2 loss: 0.580751  [   96/  146]
train() client id: f_00005-2-3 loss: 0.749483  [  128/  146]
train() client id: f_00005-3-0 loss: 0.877567  [   32/  146]
train() client id: f_00005-3-1 loss: 0.546168  [   64/  146]
train() client id: f_00005-3-2 loss: 0.577979  [   96/  146]
train() client id: f_00005-3-3 loss: 0.630105  [  128/  146]
train() client id: f_00005-4-0 loss: 0.560647  [   32/  146]
train() client id: f_00005-4-1 loss: 0.626845  [   64/  146]
train() client id: f_00005-4-2 loss: 0.960281  [   96/  146]
train() client id: f_00005-4-3 loss: 0.485620  [  128/  146]
train() client id: f_00005-5-0 loss: 0.704694  [   32/  146]
train() client id: f_00005-5-1 loss: 0.481311  [   64/  146]
train() client id: f_00005-5-2 loss: 0.823871  [   96/  146]
train() client id: f_00005-5-3 loss: 0.531735  [  128/  146]
train() client id: f_00005-6-0 loss: 0.358220  [   32/  146]
train() client id: f_00005-6-1 loss: 0.550755  [   64/  146]
train() client id: f_00005-6-2 loss: 0.713184  [   96/  146]
train() client id: f_00005-6-3 loss: 0.822408  [  128/  146]
train() client id: f_00005-7-0 loss: 0.465374  [   32/  146]
train() client id: f_00005-7-1 loss: 0.677176  [   64/  146]
train() client id: f_00005-7-2 loss: 0.752180  [   96/  146]
train() client id: f_00005-7-3 loss: 0.775152  [  128/  146]
train() client id: f_00005-8-0 loss: 0.954104  [   32/  146]
train() client id: f_00005-8-1 loss: 0.663992  [   64/  146]
train() client id: f_00005-8-2 loss: 0.611651  [   96/  146]
train() client id: f_00005-8-3 loss: 0.477140  [  128/  146]
train() client id: f_00005-9-0 loss: 0.749800  [   32/  146]
train() client id: f_00005-9-1 loss: 0.609775  [   64/  146]
train() client id: f_00005-9-2 loss: 0.720775  [   96/  146]
train() client id: f_00005-9-3 loss: 0.508293  [  128/  146]
train() client id: f_00005-10-0 loss: 0.564867  [   32/  146]
train() client id: f_00005-10-1 loss: 0.313146  [   64/  146]
train() client id: f_00005-10-2 loss: 0.846925  [   96/  146]
train() client id: f_00005-10-3 loss: 0.926524  [  128/  146]
train() client id: f_00006-0-0 loss: 0.539884  [   32/   54]
train() client id: f_00006-1-0 loss: 0.545253  [   32/   54]
train() client id: f_00006-2-0 loss: 0.554689  [   32/   54]
train() client id: f_00006-3-0 loss: 0.485875  [   32/   54]
train() client id: f_00006-4-0 loss: 0.581589  [   32/   54]
train() client id: f_00006-5-0 loss: 0.524926  [   32/   54]
train() client id: f_00006-6-0 loss: 0.518431  [   32/   54]
train() client id: f_00006-7-0 loss: 0.542890  [   32/   54]
train() client id: f_00006-8-0 loss: 0.544837  [   32/   54]
train() client id: f_00006-9-0 loss: 0.594082  [   32/   54]
train() client id: f_00006-10-0 loss: 0.565411  [   32/   54]
train() client id: f_00007-0-0 loss: 0.523556  [   32/  179]
train() client id: f_00007-0-1 loss: 0.608400  [   64/  179]
train() client id: f_00007-0-2 loss: 0.419244  [   96/  179]
train() client id: f_00007-0-3 loss: 0.427550  [  128/  179]
train() client id: f_00007-0-4 loss: 0.582323  [  160/  179]
train() client id: f_00007-1-0 loss: 0.434558  [   32/  179]
train() client id: f_00007-1-1 loss: 0.431648  [   64/  179]
train() client id: f_00007-1-2 loss: 0.320354  [   96/  179]
train() client id: f_00007-1-3 loss: 0.648181  [  128/  179]
train() client id: f_00007-1-4 loss: 0.473184  [  160/  179]
train() client id: f_00007-2-0 loss: 0.466893  [   32/  179]
train() client id: f_00007-2-1 loss: 0.437975  [   64/  179]
train() client id: f_00007-2-2 loss: 0.358116  [   96/  179]
train() client id: f_00007-2-3 loss: 0.425068  [  128/  179]
train() client id: f_00007-2-4 loss: 0.450354  [  160/  179]
train() client id: f_00007-3-0 loss: 0.310571  [   32/  179]
train() client id: f_00007-3-1 loss: 0.393037  [   64/  179]
train() client id: f_00007-3-2 loss: 0.546783  [   96/  179]
train() client id: f_00007-3-3 loss: 0.643939  [  128/  179]
train() client id: f_00007-3-4 loss: 0.365465  [  160/  179]
train() client id: f_00007-4-0 loss: 0.500157  [   32/  179]
train() client id: f_00007-4-1 loss: 0.615805  [   64/  179]
train() client id: f_00007-4-2 loss: 0.308969  [   96/  179]
train() client id: f_00007-4-3 loss: 0.344501  [  128/  179]
train() client id: f_00007-4-4 loss: 0.320659  [  160/  179]
train() client id: f_00007-5-0 loss: 0.306387  [   32/  179]
train() client id: f_00007-5-1 loss: 0.540958  [   64/  179]
train() client id: f_00007-5-2 loss: 0.253772  [   96/  179]
train() client id: f_00007-5-3 loss: 0.686994  [  128/  179]
train() client id: f_00007-5-4 loss: 0.447605  [  160/  179]
train() client id: f_00007-6-0 loss: 0.587574  [   32/  179]
train() client id: f_00007-6-1 loss: 0.345988  [   64/  179]
train() client id: f_00007-6-2 loss: 0.288803  [   96/  179]
train() client id: f_00007-6-3 loss: 0.685282  [  128/  179]
train() client id: f_00007-6-4 loss: 0.295143  [  160/  179]
train() client id: f_00007-7-0 loss: 0.549714  [   32/  179]
train() client id: f_00007-7-1 loss: 0.558698  [   64/  179]
train() client id: f_00007-7-2 loss: 0.297176  [   96/  179]
train() client id: f_00007-7-3 loss: 0.266205  [  128/  179]
train() client id: f_00007-7-4 loss: 0.487675  [  160/  179]
train() client id: f_00007-8-0 loss: 0.555226  [   32/  179]
train() client id: f_00007-8-1 loss: 0.546818  [   64/  179]
train() client id: f_00007-8-2 loss: 0.458398  [   96/  179]
train() client id: f_00007-8-3 loss: 0.386027  [  128/  179]
train() client id: f_00007-8-4 loss: 0.291307  [  160/  179]
train() client id: f_00007-9-0 loss: 0.367874  [   32/  179]
train() client id: f_00007-9-1 loss: 0.455062  [   64/  179]
train() client id: f_00007-9-2 loss: 0.569594  [   96/  179]
train() client id: f_00007-9-3 loss: 0.371636  [  128/  179]
train() client id: f_00007-9-4 loss: 0.397113  [  160/  179]
train() client id: f_00007-10-0 loss: 0.546202  [   32/  179]
train() client id: f_00007-10-1 loss: 0.304766  [   64/  179]
train() client id: f_00007-10-2 loss: 0.605539  [   96/  179]
train() client id: f_00007-10-3 loss: 0.384236  [  128/  179]
train() client id: f_00007-10-4 loss: 0.418407  [  160/  179]
train() client id: f_00008-0-0 loss: 0.700077  [   32/  130]
train() client id: f_00008-0-1 loss: 0.656706  [   64/  130]
train() client id: f_00008-0-2 loss: 0.719121  [   96/  130]
train() client id: f_00008-0-3 loss: 0.766787  [  128/  130]
train() client id: f_00008-1-0 loss: 0.640718  [   32/  130]
train() client id: f_00008-1-1 loss: 0.676640  [   64/  130]
train() client id: f_00008-1-2 loss: 0.758523  [   96/  130]
train() client id: f_00008-1-3 loss: 0.744476  [  128/  130]
train() client id: f_00008-2-0 loss: 0.688928  [   32/  130]
train() client id: f_00008-2-1 loss: 0.575489  [   64/  130]
train() client id: f_00008-2-2 loss: 0.735938  [   96/  130]
train() client id: f_00008-2-3 loss: 0.849738  [  128/  130]
train() client id: f_00008-3-0 loss: 0.644650  [   32/  130]
train() client id: f_00008-3-1 loss: 0.754368  [   64/  130]
train() client id: f_00008-3-2 loss: 0.738834  [   96/  130]
train() client id: f_00008-3-3 loss: 0.715100  [  128/  130]
train() client id: f_00008-4-0 loss: 0.680298  [   32/  130]
train() client id: f_00008-4-1 loss: 0.770874  [   64/  130]
train() client id: f_00008-4-2 loss: 0.688123  [   96/  130]
train() client id: f_00008-4-3 loss: 0.704445  [  128/  130]
train() client id: f_00008-5-0 loss: 0.680421  [   32/  130]
train() client id: f_00008-5-1 loss: 0.839333  [   64/  130]
train() client id: f_00008-5-2 loss: 0.683379  [   96/  130]
train() client id: f_00008-5-3 loss: 0.651795  [  128/  130]
train() client id: f_00008-6-0 loss: 0.691598  [   32/  130]
train() client id: f_00008-6-1 loss: 0.700123  [   64/  130]
train() client id: f_00008-6-2 loss: 0.630814  [   96/  130]
train() client id: f_00008-6-3 loss: 0.800101  [  128/  130]
train() client id: f_00008-7-0 loss: 0.664878  [   32/  130]
train() client id: f_00008-7-1 loss: 0.737628  [   64/  130]
train() client id: f_00008-7-2 loss: 0.640944  [   96/  130]
train() client id: f_00008-7-3 loss: 0.776535  [  128/  130]
train() client id: f_00008-8-0 loss: 0.615398  [   32/  130]
train() client id: f_00008-8-1 loss: 0.757047  [   64/  130]
train() client id: f_00008-8-2 loss: 0.752575  [   96/  130]
train() client id: f_00008-8-3 loss: 0.704871  [  128/  130]
train() client id: f_00008-9-0 loss: 0.765348  [   32/  130]
train() client id: f_00008-9-1 loss: 0.668407  [   64/  130]
train() client id: f_00008-9-2 loss: 0.683142  [   96/  130]
train() client id: f_00008-9-3 loss: 0.652113  [  128/  130]
train() client id: f_00008-10-0 loss: 0.745520  [   32/  130]
train() client id: f_00008-10-1 loss: 0.638352  [   64/  130]
train() client id: f_00008-10-2 loss: 0.681670  [   96/  130]
train() client id: f_00008-10-3 loss: 0.744839  [  128/  130]
train() client id: f_00009-0-0 loss: 1.272009  [   32/  118]
train() client id: f_00009-0-1 loss: 1.138845  [   64/  118]
train() client id: f_00009-0-2 loss: 0.948170  [   96/  118]
train() client id: f_00009-1-0 loss: 1.108235  [   32/  118]
train() client id: f_00009-1-1 loss: 1.093892  [   64/  118]
train() client id: f_00009-1-2 loss: 1.003535  [   96/  118]
train() client id: f_00009-2-0 loss: 0.880776  [   32/  118]
train() client id: f_00009-2-1 loss: 1.110817  [   64/  118]
train() client id: f_00009-2-2 loss: 1.103496  [   96/  118]
train() client id: f_00009-3-0 loss: 1.051712  [   32/  118]
train() client id: f_00009-3-1 loss: 1.131418  [   64/  118]
train() client id: f_00009-3-2 loss: 0.880622  [   96/  118]
train() client id: f_00009-4-0 loss: 0.757287  [   32/  118]
train() client id: f_00009-4-1 loss: 1.062025  [   64/  118]
train() client id: f_00009-4-2 loss: 1.043811  [   96/  118]
train() client id: f_00009-5-0 loss: 0.906130  [   32/  118]
train() client id: f_00009-5-1 loss: 0.852110  [   64/  118]
train() client id: f_00009-5-2 loss: 0.935360  [   96/  118]
train() client id: f_00009-6-0 loss: 0.984720  [   32/  118]
train() client id: f_00009-6-1 loss: 0.921453  [   64/  118]
train() client id: f_00009-6-2 loss: 0.911342  [   96/  118]
train() client id: f_00009-7-0 loss: 0.887548  [   32/  118]
train() client id: f_00009-7-1 loss: 0.876517  [   64/  118]
train() client id: f_00009-7-2 loss: 0.894372  [   96/  118]
train() client id: f_00009-8-0 loss: 0.929913  [   32/  118]
train() client id: f_00009-8-1 loss: 0.896005  [   64/  118]
train() client id: f_00009-8-2 loss: 0.834696  [   96/  118]
train() client id: f_00009-9-0 loss: 0.855322  [   32/  118]
train() client id: f_00009-9-1 loss: 1.085844  [   64/  118]
train() client id: f_00009-9-2 loss: 0.776666  [   96/  118]
train() client id: f_00009-10-0 loss: 0.844557  [   32/  118]
train() client id: f_00009-10-1 loss: 0.989189  [   64/  118]
train() client id: f_00009-10-2 loss: 0.763501  [   96/  118]
At round 42 accuracy: 0.649867374005305
At round 42 training accuracy: 0.5855130784708249
At round 42 training loss: 0.8294756626348427
update_location
xs = -4.528292 111.001589 120.045120 -120.943528 9.896481 -45.217951 -162.215960 183.375741 -1.680116 104.695607 
ys = 197.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -127.154970 4.001482 
xs mean: 19.442869159412865
ys mean: 9.371751218646876
dists_uav = 221.498323 150.210975 156.245239 158.542127 100.490609 109.784254 190.587992 209.748527 161.775181 144.835017 
uav_gains = -109.093981 -104.422178 -104.852937 -105.012999 -100.053154 -101.013562 -107.092651 -108.304945 -105.234957 -104.024695 
uav_gains_db_mean: -104.91060594588396
dists_bs = 171.961679 327.443659 342.368444 161.755008 255.028558 215.618522 178.583088 407.600456 348.335195 327.826366 
bs_gains = -102.159146 -109.990884 -110.532884 -101.415076 -106.951546 -104.910269 -102.618588 -112.653630 -110.742985 -110.005088 
bs_gains_db_mean: -107.19800948547106
Round 43
-------------------------------
ene_coms = [0.01006627 0.01068174 0.00788547 0.00795101 0.00883752 0.00790883
 0.00890953 0.00957777 0.01125463 0.01069206]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.30072231 11.00580421  5.18912031  1.87034062 12.66106996  6.09107652
  2.33418536  7.46621402  5.48080529  4.98135752]
obj_prev = 62.38069612098825
eta_min = 6.6219856531571245e-18	eta_max = 0.9450523427579234
af = 13.13642190293526	bf = 1.26143066181996	zeta = 14.450064093228786	eta = 0.9090909090909091
af = 13.13642190293526	bf = 1.26143066181996	zeta = 27.648119349116104	eta = 0.47512894953396617
af = 13.13642190293526	bf = 1.26143066181996	zeta = 21.0283061269271	eta = 0.6247018577551451
af = 13.13642190293526	bf = 1.26143066181996	zeta = 19.829207034707583	eta = 0.6624784279039618
af = 13.13642190293526	bf = 1.26143066181996	zeta = 19.763416541740135	eta = 0.6646837542077438
af = 13.13642190293526	bf = 1.26143066181996	zeta = 19.76320043414373	eta = 0.6646910224236874
eta = 0.6646910224236874
ene_coms = [0.01006627 0.01068174 0.00788547 0.00795101 0.00883752 0.00790883
 0.00890953 0.00957777 0.01125463 0.01069206]
ene_comp = [0.03370291 0.0708831  0.03316794 0.01150179 0.08184992 0.03905258
 0.0144441  0.04787953 0.03477284 0.03156304]
ene_total = [1.75608927 3.27251193 1.64712867 0.78047745 3.63852506 1.88416664
 0.93698518 2.30527851 1.84669562 1.69534211]
ti_comp = [0.59347271 0.58731794 0.61528063 0.61462523 0.60576018 0.61504709
 0.60504006 0.59835771 0.58158911 0.5872148 ]
ti_coms = [0.10066267 0.10681744 0.07885475 0.07951015 0.08837521 0.07908829
 0.08909532 0.09577767 0.11254627 0.10692058]
t_total = [27.84981956 27.84981956 27.84981956 27.84981956 27.84981956 27.84981956
 27.84981956 27.84981956 27.84981956 27.84981956]
ene_coms = [0.01006627 0.01068174 0.00788547 0.00795101 0.00883752 0.00790883
 0.00890953 0.00957777 0.01125463 0.01069206]
ene_comp = [6.79329882e-06 6.45300012e-05 6.02405812e-06 2.51741711e-07
 9.33971082e-05 9.84039247e-06 5.14497902e-07 1.91605466e-05
 7.76905356e-06 5.69931452e-06]
ene_total = [0.40414727 0.43115767 0.31661954 0.3190175  0.35832267 0.31770967
 0.3574853  0.38504405 0.45186532 0.42921111]
optimize_network iter = 0 obj = 3.7705801144951843
eta = 0.6646910224236874
freqs = [28394658.78439153 60344746.48202559 26953508.30145651  9356747.91946019
 67559677.68619405 31747632.86555275 11936485.95978845 40009119.9760301
 29894682.74652138 26875210.59591984]
eta_min = 0.6646910224236898	eta_max = 0.71154959174799
af = 0.0070481607083708155	bf = 1.26143066181996	zeta = 0.007752976779207898	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01006627 0.01068174 0.00788547 0.00795101 0.00883752 0.00790883
 0.00890953 0.00957777 0.01125463 0.01069206]
ene_comp = [1.58525529e-06 1.50584463e-05 1.40574855e-06 5.87453738e-08
 2.17947515e-05 2.29631209e-06 1.20061040e-07 4.47122357e-06
 1.81295326e-06 1.32996777e-06]
ene_total = [1.48988559 1.58296039 1.16713567 1.1766352  1.31104087 1.17072354
 1.31848997 1.41802221 1.66577809 1.58245513]
ti_comp = [0.49646907 0.4903143  0.51827699 0.5176216  0.50875654 0.51804345
 0.50803642 0.50135407 0.48458547 0.49021116]
ti_coms = [0.10066267 0.10681744 0.07885475 0.07951015 0.08837521 0.07908829
 0.08909532 0.09577767 0.11254627 0.10692058]
t_total = [27.84981956 27.84981956 27.84981956 27.84981956 27.84981956 27.84981956
 27.84981956 27.84981956 27.84981956 27.84981956]
ene_coms = [0.01006627 0.01068174 0.00788547 0.00795101 0.00883752 0.00790883
 0.00890953 0.00957777 0.01125463 0.01069206]
ene_comp = [5.61513494e-06 5.35576427e-05 4.91104767e-06 2.05311628e-07
 7.65909100e-05 8.02341301e-06 4.22108932e-07 1.57871215e-05
 6.47324788e-06 4.73056037e-06]
ene_total = [0.46974577 0.50068719 0.36800225 0.37083951 0.41574812 0.36923664
 0.41555423 0.44743686 0.5252101  0.49889098]
optimize_network iter = 1 obj = 4.381351645538056
eta = 0.71154959174799
freqs = [28281297.6328493  60227147.69037971 26661264.17426017  9257144.68787046
 67024318.61063506 31405640.30833918 11844586.31046074 39785925.66063512
 29894682.74652138 26823768.77565373]
eta_min = 0.7115495917488212	eta_max = 0.7115495917479834
af = 0.00697013866995728	bf = 1.26143066181996	zeta = 0.007667152536953008	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01006627 0.01068174 0.00788547 0.00795101 0.00883752 0.00790883
 0.00890953 0.00957777 0.01125463 0.01069206]
ene_comp = [1.57262280e-06 1.49998122e-05 1.37543008e-06 5.75013335e-08
 2.14507064e-05 2.24710578e-06 1.18219444e-07 4.42147651e-06
 1.81295326e-06 1.32488126e-06]
ene_total = [1.48988372 1.58295171 1.16713118 1.17663501 1.31098996 1.17071626
 1.3184897  1.41801485 1.66577809 1.58245438]
ti_comp = [0.49646907 0.4903143  0.51827699 0.5176216  0.50875654 0.51804345
 0.50803642 0.50135407 0.48458547 0.49021116]
ti_coms = [0.10066267 0.10681744 0.07885475 0.07951015 0.08837521 0.07908829
 0.08909532 0.09577767 0.11254627 0.10692058]
t_total = [27.84981956 27.84981956 27.84981956 27.84981956 27.84981956 27.84981956
 27.84981956 27.84981956 27.84981956 27.84981956]
ene_coms = [0.01006627 0.01068174 0.00788547 0.00795101 0.00883752 0.00790883
 0.00890953 0.00957777 0.01125463 0.01069206]
ene_comp = [5.61513494e-06 5.35576427e-05 4.91104767e-06 2.05311628e-07
 7.65909100e-05 8.02341301e-06 4.22108932e-07 1.57871215e-05
 6.47324788e-06 4.73056037e-06]
ene_total = [0.46974577 0.50068719 0.36800225 0.37083951 0.41574812 0.36923664
 0.41555423 0.44743686 0.5252101  0.49889098]
optimize_network iter = 2 obj = 4.381351645537955
eta = 0.7115495917479834
freqs = [28281297.63284929 60227147.69037967 26661264.1742602   9257144.68787047
 67024318.61063508 31405640.30833921 11844586.31046075 39785925.66063511
 29894682.74652135 26823768.77565371]
Done!
ene_coms = [0.01006627 0.01068174 0.00788547 0.00795101 0.00883752 0.00790883
 0.00890953 0.00957777 0.01125463 0.01069206]
ene_comp = [5.54283434e-06 5.28680333e-05 4.84781291e-06 2.02668031e-07
 7.56047238e-05 7.92010337e-06 4.16673848e-07 1.55838462e-05
 6.38989820e-06 4.66964957e-06]
ene_total = [0.01007181 0.01073461 0.00789032 0.00795122 0.00891313 0.00791675
 0.00890995 0.00959335 0.01126102 0.01069673]
At round 43 energy consumption: 0.09393888216801516
At round 43 eta: 0.7115495917479834
At round 43 a_n: 13.453131436573733
At round 43 local rounds: 11.143483739335458
At round 43 global rounds: 46.6393218789271
gradient difference: 0.4167964458465576
train() client id: f_00000-0-0 loss: 1.247851  [   32/  126]
train() client id: f_00000-0-1 loss: 1.334115  [   64/  126]
train() client id: f_00000-0-2 loss: 1.213593  [   96/  126]
train() client id: f_00000-1-0 loss: 1.272804  [   32/  126]
train() client id: f_00000-1-1 loss: 0.956922  [   64/  126]
train() client id: f_00000-1-2 loss: 1.070549  [   96/  126]
train() client id: f_00000-2-0 loss: 1.097852  [   32/  126]
train() client id: f_00000-2-1 loss: 1.221055  [   64/  126]
train() client id: f_00000-2-2 loss: 1.053540  [   96/  126]
train() client id: f_00000-3-0 loss: 1.003246  [   32/  126]
train() client id: f_00000-3-1 loss: 1.039031  [   64/  126]
train() client id: f_00000-3-2 loss: 1.042519  [   96/  126]
train() client id: f_00000-4-0 loss: 0.991911  [   32/  126]
train() client id: f_00000-4-1 loss: 0.856224  [   64/  126]
train() client id: f_00000-4-2 loss: 0.964615  [   96/  126]
train() client id: f_00000-5-0 loss: 0.855255  [   32/  126]
train() client id: f_00000-5-1 loss: 1.110631  [   64/  126]
train() client id: f_00000-5-2 loss: 0.905563  [   96/  126]
train() client id: f_00000-6-0 loss: 0.923546  [   32/  126]
train() client id: f_00000-6-1 loss: 0.973389  [   64/  126]
train() client id: f_00000-6-2 loss: 0.893085  [   96/  126]
train() client id: f_00000-7-0 loss: 0.920438  [   32/  126]
train() client id: f_00000-7-1 loss: 0.896585  [   64/  126]
train() client id: f_00000-7-2 loss: 0.779013  [   96/  126]
train() client id: f_00000-8-0 loss: 0.887464  [   32/  126]
train() client id: f_00000-8-1 loss: 0.877320  [   64/  126]
train() client id: f_00000-8-2 loss: 0.825694  [   96/  126]
train() client id: f_00000-9-0 loss: 0.784452  [   32/  126]
train() client id: f_00000-9-1 loss: 0.886467  [   64/  126]
train() client id: f_00000-9-2 loss: 0.878991  [   96/  126]
train() client id: f_00000-10-0 loss: 0.862903  [   32/  126]
train() client id: f_00000-10-1 loss: 0.822135  [   64/  126]
train() client id: f_00000-10-2 loss: 0.988819  [   96/  126]
train() client id: f_00001-0-0 loss: 0.523666  [   32/  265]
train() client id: f_00001-0-1 loss: 0.562161  [   64/  265]
train() client id: f_00001-0-2 loss: 0.504404  [   96/  265]
train() client id: f_00001-0-3 loss: 0.536086  [  128/  265]
train() client id: f_00001-0-4 loss: 0.470752  [  160/  265]
train() client id: f_00001-0-5 loss: 0.495625  [  192/  265]
train() client id: f_00001-0-6 loss: 0.473411  [  224/  265]
train() client id: f_00001-0-7 loss: 0.602027  [  256/  265]
train() client id: f_00001-1-0 loss: 0.481272  [   32/  265]
train() client id: f_00001-1-1 loss: 0.438335  [   64/  265]
train() client id: f_00001-1-2 loss: 0.595455  [   96/  265]
train() client id: f_00001-1-3 loss: 0.510731  [  128/  265]
train() client id: f_00001-1-4 loss: 0.489626  [  160/  265]
train() client id: f_00001-1-5 loss: 0.609172  [  192/  265]
train() client id: f_00001-1-6 loss: 0.505158  [  224/  265]
train() client id: f_00001-1-7 loss: 0.496682  [  256/  265]
train() client id: f_00001-2-0 loss: 0.428980  [   32/  265]
train() client id: f_00001-2-1 loss: 0.533288  [   64/  265]
train() client id: f_00001-2-2 loss: 0.425423  [   96/  265]
train() client id: f_00001-2-3 loss: 0.512458  [  128/  265]
train() client id: f_00001-2-4 loss: 0.523092  [  160/  265]
train() client id: f_00001-2-5 loss: 0.597585  [  192/  265]
train() client id: f_00001-2-6 loss: 0.490431  [  224/  265]
train() client id: f_00001-2-7 loss: 0.603661  [  256/  265]
train() client id: f_00001-3-0 loss: 0.519342  [   32/  265]
train() client id: f_00001-3-1 loss: 0.505045  [   64/  265]
train() client id: f_00001-3-2 loss: 0.546788  [   96/  265]
train() client id: f_00001-3-3 loss: 0.489064  [  128/  265]
train() client id: f_00001-3-4 loss: 0.430488  [  160/  265]
train() client id: f_00001-3-5 loss: 0.487805  [  192/  265]
train() client id: f_00001-3-6 loss: 0.409208  [  224/  265]
train() client id: f_00001-3-7 loss: 0.566816  [  256/  265]
train() client id: f_00001-4-0 loss: 0.467493  [   32/  265]
train() client id: f_00001-4-1 loss: 0.537116  [   64/  265]
train() client id: f_00001-4-2 loss: 0.402618  [   96/  265]
train() client id: f_00001-4-3 loss: 0.480169  [  128/  265]
train() client id: f_00001-4-4 loss: 0.418890  [  160/  265]
train() client id: f_00001-4-5 loss: 0.600462  [  192/  265]
train() client id: f_00001-4-6 loss: 0.439711  [  224/  265]
train() client id: f_00001-4-7 loss: 0.655130  [  256/  265]
train() client id: f_00001-5-0 loss: 0.411409  [   32/  265]
train() client id: f_00001-5-1 loss: 0.415254  [   64/  265]
train() client id: f_00001-5-2 loss: 0.683660  [   96/  265]
train() client id: f_00001-5-3 loss: 0.561383  [  128/  265]
train() client id: f_00001-5-4 loss: 0.424633  [  160/  265]
train() client id: f_00001-5-5 loss: 0.408365  [  192/  265]
train() client id: f_00001-5-6 loss: 0.529079  [  224/  265]
train() client id: f_00001-5-7 loss: 0.547732  [  256/  265]
train() client id: f_00001-6-0 loss: 0.424034  [   32/  265]
train() client id: f_00001-6-1 loss: 0.413131  [   64/  265]
train() client id: f_00001-6-2 loss: 0.525202  [   96/  265]
train() client id: f_00001-6-3 loss: 0.461941  [  128/  265]
train() client id: f_00001-6-4 loss: 0.407128  [  160/  265]
train() client id: f_00001-6-5 loss: 0.460655  [  192/  265]
train() client id: f_00001-6-6 loss: 0.573871  [  224/  265]
train() client id: f_00001-6-7 loss: 0.688597  [  256/  265]
train() client id: f_00001-7-0 loss: 0.532952  [   32/  265]
train() client id: f_00001-7-1 loss: 0.576810  [   64/  265]
train() client id: f_00001-7-2 loss: 0.554001  [   96/  265]
train() client id: f_00001-7-3 loss: 0.430995  [  128/  265]
train() client id: f_00001-7-4 loss: 0.518234  [  160/  265]
train() client id: f_00001-7-5 loss: 0.401159  [  192/  265]
train() client id: f_00001-7-6 loss: 0.524825  [  224/  265]
train() client id: f_00001-7-7 loss: 0.493777  [  256/  265]
train() client id: f_00001-8-0 loss: 0.528498  [   32/  265]
train() client id: f_00001-8-1 loss: 0.501376  [   64/  265]
train() client id: f_00001-8-2 loss: 0.442463  [   96/  265]
train() client id: f_00001-8-3 loss: 0.487185  [  128/  265]
train() client id: f_00001-8-4 loss: 0.451238  [  160/  265]
train() client id: f_00001-8-5 loss: 0.541952  [  192/  265]
train() client id: f_00001-8-6 loss: 0.476910  [  224/  265]
train() client id: f_00001-8-7 loss: 0.600679  [  256/  265]
train() client id: f_00001-9-0 loss: 0.520317  [   32/  265]
train() client id: f_00001-9-1 loss: 0.485495  [   64/  265]
train() client id: f_00001-9-2 loss: 0.519659  [   96/  265]
train() client id: f_00001-9-3 loss: 0.405520  [  128/  265]
train() client id: f_00001-9-4 loss: 0.498969  [  160/  265]
train() client id: f_00001-9-5 loss: 0.490057  [  192/  265]
train() client id: f_00001-9-6 loss: 0.647616  [  224/  265]
train() client id: f_00001-9-7 loss: 0.469664  [  256/  265]
train() client id: f_00001-10-0 loss: 0.464126  [   32/  265]
train() client id: f_00001-10-1 loss: 0.482361  [   64/  265]
train() client id: f_00001-10-2 loss: 0.421467  [   96/  265]
train() client id: f_00001-10-3 loss: 0.391527  [  128/  265]
train() client id: f_00001-10-4 loss: 0.419825  [  160/  265]
train() client id: f_00001-10-5 loss: 0.706872  [  192/  265]
train() client id: f_00001-10-6 loss: 0.486022  [  224/  265]
train() client id: f_00001-10-7 loss: 0.661310  [  256/  265]
train() client id: f_00002-0-0 loss: 1.060791  [   32/  124]
train() client id: f_00002-0-1 loss: 1.155254  [   64/  124]
train() client id: f_00002-0-2 loss: 0.943965  [   96/  124]
train() client id: f_00002-1-0 loss: 1.091710  [   32/  124]
train() client id: f_00002-1-1 loss: 0.906094  [   64/  124]
train() client id: f_00002-1-2 loss: 0.976186  [   96/  124]
train() client id: f_00002-2-0 loss: 0.928809  [   32/  124]
train() client id: f_00002-2-1 loss: 1.031929  [   64/  124]
train() client id: f_00002-2-2 loss: 1.041678  [   96/  124]
train() client id: f_00002-3-0 loss: 1.115136  [   32/  124]
train() client id: f_00002-3-1 loss: 0.916311  [   64/  124]
train() client id: f_00002-3-2 loss: 0.762140  [   96/  124]
train() client id: f_00002-4-0 loss: 0.811029  [   32/  124]
train() client id: f_00002-4-1 loss: 0.997512  [   64/  124]
train() client id: f_00002-4-2 loss: 0.963314  [   96/  124]
train() client id: f_00002-5-0 loss: 0.811599  [   32/  124]
train() client id: f_00002-5-1 loss: 0.851826  [   64/  124]
train() client id: f_00002-5-2 loss: 1.038423  [   96/  124]
train() client id: f_00002-6-0 loss: 0.836209  [   32/  124]
train() client id: f_00002-6-1 loss: 0.933392  [   64/  124]
train() client id: f_00002-6-2 loss: 0.824129  [   96/  124]
train() client id: f_00002-7-0 loss: 0.751958  [   32/  124]
train() client id: f_00002-7-1 loss: 0.800725  [   64/  124]
train() client id: f_00002-7-2 loss: 0.960872  [   96/  124]
train() client id: f_00002-8-0 loss: 0.949727  [   32/  124]
train() client id: f_00002-8-1 loss: 0.710827  [   64/  124]
train() client id: f_00002-8-2 loss: 0.823131  [   96/  124]
train() client id: f_00002-9-0 loss: 0.917305  [   32/  124]
train() client id: f_00002-9-1 loss: 0.879898  [   64/  124]
train() client id: f_00002-9-2 loss: 0.654559  [   96/  124]
train() client id: f_00002-10-0 loss: 0.784857  [   32/  124]
train() client id: f_00002-10-1 loss: 0.824134  [   64/  124]
train() client id: f_00002-10-2 loss: 0.938104  [   96/  124]
train() client id: f_00003-0-0 loss: 0.634958  [   32/   43]
train() client id: f_00003-1-0 loss: 0.516711  [   32/   43]
train() client id: f_00003-2-0 loss: 0.419124  [   32/   43]
train() client id: f_00003-3-0 loss: 0.663487  [   32/   43]
train() client id: f_00003-4-0 loss: 0.477788  [   32/   43]
train() client id: f_00003-5-0 loss: 0.561363  [   32/   43]
train() client id: f_00003-6-0 loss: 0.601545  [   32/   43]
train() client id: f_00003-7-0 loss: 0.745056  [   32/   43]
train() client id: f_00003-8-0 loss: 0.512150  [   32/   43]
train() client id: f_00003-9-0 loss: 0.611706  [   32/   43]
train() client id: f_00003-10-0 loss: 0.669555  [   32/   43]
train() client id: f_00004-0-0 loss: 0.759395  [   32/  306]
train() client id: f_00004-0-1 loss: 0.791010  [   64/  306]
train() client id: f_00004-0-2 loss: 0.808393  [   96/  306]
train() client id: f_00004-0-3 loss: 0.814384  [  128/  306]
train() client id: f_00004-0-4 loss: 0.826205  [  160/  306]
train() client id: f_00004-0-5 loss: 0.815222  [  192/  306]
train() client id: f_00004-0-6 loss: 0.857053  [  224/  306]
train() client id: f_00004-0-7 loss: 0.791342  [  256/  306]
train() client id: f_00004-0-8 loss: 0.773666  [  288/  306]
train() client id: f_00004-1-0 loss: 0.827868  [   32/  306]
train() client id: f_00004-1-1 loss: 0.854913  [   64/  306]
train() client id: f_00004-1-2 loss: 0.749214  [   96/  306]
train() client id: f_00004-1-3 loss: 0.848387  [  128/  306]
train() client id: f_00004-1-4 loss: 0.651095  [  160/  306]
train() client id: f_00004-1-5 loss: 0.814445  [  192/  306]
train() client id: f_00004-1-6 loss: 0.813809  [  224/  306]
train() client id: f_00004-1-7 loss: 0.893739  [  256/  306]
train() client id: f_00004-1-8 loss: 0.753899  [  288/  306]
train() client id: f_00004-2-0 loss: 0.879998  [   32/  306]
train() client id: f_00004-2-1 loss: 0.831195  [   64/  306]
train() client id: f_00004-2-2 loss: 0.738618  [   96/  306]
train() client id: f_00004-2-3 loss: 0.831851  [  128/  306]
train() client id: f_00004-2-4 loss: 0.801228  [  160/  306]
train() client id: f_00004-2-5 loss: 0.706420  [  192/  306]
train() client id: f_00004-2-6 loss: 0.818943  [  224/  306]
train() client id: f_00004-2-7 loss: 0.808244  [  256/  306]
train() client id: f_00004-2-8 loss: 0.766770  [  288/  306]
train() client id: f_00004-3-0 loss: 0.725040  [   32/  306]
train() client id: f_00004-3-1 loss: 0.678145  [   64/  306]
train() client id: f_00004-3-2 loss: 0.908023  [   96/  306]
train() client id: f_00004-3-3 loss: 0.739538  [  128/  306]
train() client id: f_00004-3-4 loss: 0.858208  [  160/  306]
train() client id: f_00004-3-5 loss: 0.874114  [  192/  306]
train() client id: f_00004-3-6 loss: 0.841747  [  224/  306]
train() client id: f_00004-3-7 loss: 0.810555  [  256/  306]
train() client id: f_00004-3-8 loss: 0.732977  [  288/  306]
train() client id: f_00004-4-0 loss: 0.816332  [   32/  306]
train() client id: f_00004-4-1 loss: 0.892840  [   64/  306]
train() client id: f_00004-4-2 loss: 0.731696  [   96/  306]
train() client id: f_00004-4-3 loss: 0.836919  [  128/  306]
train() client id: f_00004-4-4 loss: 0.715417  [  160/  306]
train() client id: f_00004-4-5 loss: 0.706830  [  192/  306]
train() client id: f_00004-4-6 loss: 0.893311  [  224/  306]
train() client id: f_00004-4-7 loss: 0.777425  [  256/  306]
train() client id: f_00004-4-8 loss: 0.675518  [  288/  306]
train() client id: f_00004-5-0 loss: 0.775981  [   32/  306]
train() client id: f_00004-5-1 loss: 0.784711  [   64/  306]
train() client id: f_00004-5-2 loss: 0.819487  [   96/  306]
train() client id: f_00004-5-3 loss: 0.619538  [  128/  306]
train() client id: f_00004-5-4 loss: 0.863052  [  160/  306]
train() client id: f_00004-5-5 loss: 0.846346  [  192/  306]
train() client id: f_00004-5-6 loss: 0.802832  [  224/  306]
train() client id: f_00004-5-7 loss: 0.875057  [  256/  306]
train() client id: f_00004-5-8 loss: 0.763572  [  288/  306]
train() client id: f_00004-6-0 loss: 0.714344  [   32/  306]
train() client id: f_00004-6-1 loss: 0.779395  [   64/  306]
train() client id: f_00004-6-2 loss: 0.883541  [   96/  306]
train() client id: f_00004-6-3 loss: 0.792605  [  128/  306]
train() client id: f_00004-6-4 loss: 0.786475  [  160/  306]
train() client id: f_00004-6-5 loss: 0.777499  [  192/  306]
train() client id: f_00004-6-6 loss: 0.809366  [  224/  306]
train() client id: f_00004-6-7 loss: 0.816368  [  256/  306]
train() client id: f_00004-6-8 loss: 0.738947  [  288/  306]
train() client id: f_00004-7-0 loss: 0.865090  [   32/  306]
train() client id: f_00004-7-1 loss: 0.805496  [   64/  306]
train() client id: f_00004-7-2 loss: 0.797327  [   96/  306]
train() client id: f_00004-7-3 loss: 0.759257  [  128/  306]
train() client id: f_00004-7-4 loss: 0.780482  [  160/  306]
train() client id: f_00004-7-5 loss: 0.777171  [  192/  306]
train() client id: f_00004-7-6 loss: 0.823436  [  224/  306]
train() client id: f_00004-7-7 loss: 0.763752  [  256/  306]
train() client id: f_00004-7-8 loss: 0.815157  [  288/  306]
train() client id: f_00004-8-0 loss: 0.844529  [   32/  306]
train() client id: f_00004-8-1 loss: 1.024288  [   64/  306]
train() client id: f_00004-8-2 loss: 0.750268  [   96/  306]
train() client id: f_00004-8-3 loss: 0.788551  [  128/  306]
train() client id: f_00004-8-4 loss: 0.816402  [  160/  306]
train() client id: f_00004-8-5 loss: 0.792825  [  192/  306]
train() client id: f_00004-8-6 loss: 0.704350  [  224/  306]
train() client id: f_00004-8-7 loss: 0.752623  [  256/  306]
train() client id: f_00004-8-8 loss: 0.735177  [  288/  306]
train() client id: f_00004-9-0 loss: 0.705779  [   32/  306]
train() client id: f_00004-9-1 loss: 0.764306  [   64/  306]
train() client id: f_00004-9-2 loss: 0.854649  [   96/  306]
train() client id: f_00004-9-3 loss: 0.724388  [  128/  306]
train() client id: f_00004-9-4 loss: 0.743072  [  160/  306]
train() client id: f_00004-9-5 loss: 0.780579  [  192/  306]
train() client id: f_00004-9-6 loss: 0.990217  [  224/  306]
train() client id: f_00004-9-7 loss: 0.686250  [  256/  306]
train() client id: f_00004-9-8 loss: 0.882917  [  288/  306]
train() client id: f_00004-10-0 loss: 0.881786  [   32/  306]
train() client id: f_00004-10-1 loss: 0.742439  [   64/  306]
train() client id: f_00004-10-2 loss: 0.716565  [   96/  306]
train() client id: f_00004-10-3 loss: 0.773063  [  128/  306]
train() client id: f_00004-10-4 loss: 0.715561  [  160/  306]
train() client id: f_00004-10-5 loss: 0.943885  [  192/  306]
train() client id: f_00004-10-6 loss: 0.765265  [  224/  306]
train() client id: f_00004-10-7 loss: 0.813708  [  256/  306]
train() client id: f_00004-10-8 loss: 0.767095  [  288/  306]
train() client id: f_00005-0-0 loss: 0.547617  [   32/  146]
train() client id: f_00005-0-1 loss: 0.322274  [   64/  146]
train() client id: f_00005-0-2 loss: 0.380659  [   96/  146]
train() client id: f_00005-0-3 loss: 0.329238  [  128/  146]
train() client id: f_00005-1-0 loss: 0.506693  [   32/  146]
train() client id: f_00005-1-1 loss: 0.281227  [   64/  146]
train() client id: f_00005-1-2 loss: 0.771817  [   96/  146]
train() client id: f_00005-1-3 loss: 0.086315  [  128/  146]
train() client id: f_00005-2-0 loss: 0.318421  [   32/  146]
train() client id: f_00005-2-1 loss: 0.156168  [   64/  146]
train() client id: f_00005-2-2 loss: 0.622246  [   96/  146]
train() client id: f_00005-2-3 loss: 0.527593  [  128/  146]
train() client id: f_00005-3-0 loss: 0.340660  [   32/  146]
train() client id: f_00005-3-1 loss: 0.195130  [   64/  146]
train() client id: f_00005-3-2 loss: 0.400957  [   96/  146]
train() client id: f_00005-3-3 loss: 0.306240  [  128/  146]
train() client id: f_00005-4-0 loss: 0.245292  [   32/  146]
train() client id: f_00005-4-1 loss: 0.378238  [   64/  146]
train() client id: f_00005-4-2 loss: 0.570907  [   96/  146]
train() client id: f_00005-4-3 loss: 0.381119  [  128/  146]
train() client id: f_00005-5-0 loss: 0.243828  [   32/  146]
train() client id: f_00005-5-1 loss: 0.392800  [   64/  146]
train() client id: f_00005-5-2 loss: 0.088429  [   96/  146]
train() client id: f_00005-5-3 loss: 0.853070  [  128/  146]
train() client id: f_00005-6-0 loss: 0.352949  [   32/  146]
train() client id: f_00005-6-1 loss: 0.216400  [   64/  146]
train() client id: f_00005-6-2 loss: 0.451581  [   96/  146]
train() client id: f_00005-6-3 loss: 0.315497  [  128/  146]
train() client id: f_00005-7-0 loss: 0.215867  [   32/  146]
train() client id: f_00005-7-1 loss: 0.189977  [   64/  146]
train() client id: f_00005-7-2 loss: 0.448722  [   96/  146]
train() client id: f_00005-7-3 loss: 0.636957  [  128/  146]
train() client id: f_00005-8-0 loss: 0.286871  [   32/  146]
train() client id: f_00005-8-1 loss: 0.607849  [   64/  146]
train() client id: f_00005-8-2 loss: 0.468841  [   96/  146]
train() client id: f_00005-8-3 loss: 0.233635  [  128/  146]
train() client id: f_00005-9-0 loss: 0.500633  [   32/  146]
train() client id: f_00005-9-1 loss: 0.446156  [   64/  146]
train() client id: f_00005-9-2 loss: 0.083847  [   96/  146]
train() client id: f_00005-9-3 loss: 0.407479  [  128/  146]
train() client id: f_00005-10-0 loss: 0.296195  [   32/  146]
train() client id: f_00005-10-1 loss: 0.346281  [   64/  146]
train() client id: f_00005-10-2 loss: 0.307411  [   96/  146]
train() client id: f_00005-10-3 loss: 0.387600  [  128/  146]
train() client id: f_00006-0-0 loss: 0.469820  [   32/   54]
train() client id: f_00006-1-0 loss: 0.466727  [   32/   54]
train() client id: f_00006-2-0 loss: 0.491879  [   32/   54]
train() client id: f_00006-3-0 loss: 0.454112  [   32/   54]
train() client id: f_00006-4-0 loss: 0.509343  [   32/   54]
train() client id: f_00006-5-0 loss: 0.490971  [   32/   54]
train() client id: f_00006-6-0 loss: 0.447789  [   32/   54]
train() client id: f_00006-7-0 loss: 0.498902  [   32/   54]
train() client id: f_00006-8-0 loss: 0.471676  [   32/   54]
train() client id: f_00006-9-0 loss: 0.502912  [   32/   54]
train() client id: f_00006-10-0 loss: 0.441053  [   32/   54]
train() client id: f_00007-0-0 loss: 0.530579  [   32/  179]
train() client id: f_00007-0-1 loss: 0.680142  [   64/  179]
train() client id: f_00007-0-2 loss: 0.806966  [   96/  179]
train() client id: f_00007-0-3 loss: 0.528995  [  128/  179]
train() client id: f_00007-0-4 loss: 0.570483  [  160/  179]
train() client id: f_00007-1-0 loss: 0.534608  [   32/  179]
train() client id: f_00007-1-1 loss: 0.695675  [   64/  179]
train() client id: f_00007-1-2 loss: 0.456192  [   96/  179]
train() client id: f_00007-1-3 loss: 0.468597  [  128/  179]
train() client id: f_00007-1-4 loss: 0.694601  [  160/  179]
train() client id: f_00007-2-0 loss: 0.539370  [   32/  179]
train() client id: f_00007-2-1 loss: 0.521707  [   64/  179]
train() client id: f_00007-2-2 loss: 0.565760  [   96/  179]
train() client id: f_00007-2-3 loss: 0.443711  [  128/  179]
train() client id: f_00007-2-4 loss: 0.784959  [  160/  179]
train() client id: f_00007-3-0 loss: 0.552613  [   32/  179]
train() client id: f_00007-3-1 loss: 0.560198  [   64/  179]
train() client id: f_00007-3-2 loss: 0.604154  [   96/  179]
train() client id: f_00007-3-3 loss: 0.538621  [  128/  179]
train() client id: f_00007-3-4 loss: 0.640466  [  160/  179]
train() client id: f_00007-4-0 loss: 0.521579  [   32/  179]
train() client id: f_00007-4-1 loss: 0.445838  [   64/  179]
train() client id: f_00007-4-2 loss: 0.777237  [   96/  179]
train() client id: f_00007-4-3 loss: 0.771203  [  128/  179]
train() client id: f_00007-4-4 loss: 0.445102  [  160/  179]
train() client id: f_00007-5-0 loss: 0.607618  [   32/  179]
train() client id: f_00007-5-1 loss: 0.668937  [   64/  179]
train() client id: f_00007-5-2 loss: 0.555992  [   96/  179]
train() client id: f_00007-5-3 loss: 0.418784  [  128/  179]
train() client id: f_00007-5-4 loss: 0.619007  [  160/  179]
train() client id: f_00007-6-0 loss: 0.539326  [   32/  179]
train() client id: f_00007-6-1 loss: 0.492117  [   64/  179]
train() client id: f_00007-6-2 loss: 0.472893  [   96/  179]
train() client id: f_00007-6-3 loss: 0.530661  [  128/  179]
train() client id: f_00007-6-4 loss: 0.707957  [  160/  179]
train() client id: f_00007-7-0 loss: 0.439292  [   32/  179]
train() client id: f_00007-7-1 loss: 0.666105  [   64/  179]
train() client id: f_00007-7-2 loss: 0.629853  [   96/  179]
train() client id: f_00007-7-3 loss: 0.663387  [  128/  179]
train() client id: f_00007-7-4 loss: 0.543188  [  160/  179]
train() client id: f_00007-8-0 loss: 0.416909  [   32/  179]
train() client id: f_00007-8-1 loss: 0.737202  [   64/  179]
train() client id: f_00007-8-2 loss: 0.535604  [   96/  179]
train() client id: f_00007-8-3 loss: 0.678315  [  128/  179]
train() client id: f_00007-8-4 loss: 0.489182  [  160/  179]
train() client id: f_00007-9-0 loss: 0.718769  [   32/  179]
train() client id: f_00007-9-1 loss: 0.506540  [   64/  179]
train() client id: f_00007-9-2 loss: 0.406534  [   96/  179]
train() client id: f_00007-9-3 loss: 0.718206  [  128/  179]
train() client id: f_00007-9-4 loss: 0.591244  [  160/  179]
train() client id: f_00007-10-0 loss: 0.584504  [   32/  179]
train() client id: f_00007-10-1 loss: 0.511355  [   64/  179]
train() client id: f_00007-10-2 loss: 0.416033  [   96/  179]
train() client id: f_00007-10-3 loss: 0.681082  [  128/  179]
train() client id: f_00007-10-4 loss: 0.594661  [  160/  179]
train() client id: f_00008-0-0 loss: 0.701173  [   32/  130]
train() client id: f_00008-0-1 loss: 0.676165  [   64/  130]
train() client id: f_00008-0-2 loss: 0.641621  [   96/  130]
train() client id: f_00008-0-3 loss: 0.629126  [  128/  130]
train() client id: f_00008-1-0 loss: 0.660899  [   32/  130]
train() client id: f_00008-1-1 loss: 0.779965  [   64/  130]
train() client id: f_00008-1-2 loss: 0.667854  [   96/  130]
train() client id: f_00008-1-3 loss: 0.553968  [  128/  130]
train() client id: f_00008-2-0 loss: 0.639947  [   32/  130]
train() client id: f_00008-2-1 loss: 0.732258  [   64/  130]
train() client id: f_00008-2-2 loss: 0.583041  [   96/  130]
train() client id: f_00008-2-3 loss: 0.662398  [  128/  130]
train() client id: f_00008-3-0 loss: 0.669153  [   32/  130]
train() client id: f_00008-3-1 loss: 0.691671  [   64/  130]
train() client id: f_00008-3-2 loss: 0.625873  [   96/  130]
train() client id: f_00008-3-3 loss: 0.670478  [  128/  130]
train() client id: f_00008-4-0 loss: 0.762281  [   32/  130]
train() client id: f_00008-4-1 loss: 0.600085  [   64/  130]
train() client id: f_00008-4-2 loss: 0.613928  [   96/  130]
train() client id: f_00008-4-3 loss: 0.677043  [  128/  130]
train() client id: f_00008-5-0 loss: 0.668449  [   32/  130]
train() client id: f_00008-5-1 loss: 0.714354  [   64/  130]
train() client id: f_00008-5-2 loss: 0.648889  [   96/  130]
train() client id: f_00008-5-3 loss: 0.606377  [  128/  130]
train() client id: f_00008-6-0 loss: 0.709079  [   32/  130]
train() client id: f_00008-6-1 loss: 0.556762  [   64/  130]
train() client id: f_00008-6-2 loss: 0.724590  [   96/  130]
train() client id: f_00008-6-3 loss: 0.676109  [  128/  130]
train() client id: f_00008-7-0 loss: 0.626956  [   32/  130]
train() client id: f_00008-7-1 loss: 0.545927  [   64/  130]
train() client id: f_00008-7-2 loss: 0.594089  [   96/  130]
train() client id: f_00008-7-3 loss: 0.858205  [  128/  130]
train() client id: f_00008-8-0 loss: 0.714207  [   32/  130]
train() client id: f_00008-8-1 loss: 0.722564  [   64/  130]
train() client id: f_00008-8-2 loss: 0.614393  [   96/  130]
train() client id: f_00008-8-3 loss: 0.611047  [  128/  130]
train() client id: f_00008-9-0 loss: 0.661918  [   32/  130]
train() client id: f_00008-9-1 loss: 0.605253  [   64/  130]
train() client id: f_00008-9-2 loss: 0.686476  [   96/  130]
train() client id: f_00008-9-3 loss: 0.680908  [  128/  130]
train() client id: f_00008-10-0 loss: 0.614546  [   32/  130]
train() client id: f_00008-10-1 loss: 0.707546  [   64/  130]
train() client id: f_00008-10-2 loss: 0.711552  [   96/  130]
train() client id: f_00008-10-3 loss: 0.619626  [  128/  130]
train() client id: f_00009-0-0 loss: 1.088459  [   32/  118]
train() client id: f_00009-0-1 loss: 1.288843  [   64/  118]
train() client id: f_00009-0-2 loss: 0.877343  [   96/  118]
train() client id: f_00009-1-0 loss: 1.180669  [   32/  118]
train() client id: f_00009-1-1 loss: 0.965600  [   64/  118]
train() client id: f_00009-1-2 loss: 0.993855  [   96/  118]
train() client id: f_00009-2-0 loss: 0.907673  [   32/  118]
train() client id: f_00009-2-1 loss: 1.159313  [   64/  118]
train() client id: f_00009-2-2 loss: 0.925235  [   96/  118]
train() client id: f_00009-3-0 loss: 1.019361  [   32/  118]
train() client id: f_00009-3-1 loss: 0.940156  [   64/  118]
train() client id: f_00009-3-2 loss: 0.894162  [   96/  118]
train() client id: f_00009-4-0 loss: 1.354260  [   32/  118]
train() client id: f_00009-4-1 loss: 0.745053  [   64/  118]
train() client id: f_00009-4-2 loss: 0.817022  [   96/  118]
train() client id: f_00009-5-0 loss: 0.928900  [   32/  118]
train() client id: f_00009-5-1 loss: 0.982549  [   64/  118]
train() client id: f_00009-5-2 loss: 0.948545  [   96/  118]
train() client id: f_00009-6-0 loss: 0.963448  [   32/  118]
train() client id: f_00009-6-1 loss: 0.993406  [   64/  118]
train() client id: f_00009-6-2 loss: 0.797311  [   96/  118]
train() client id: f_00009-7-0 loss: 0.976186  [   32/  118]
train() client id: f_00009-7-1 loss: 0.949392  [   64/  118]
train() client id: f_00009-7-2 loss: 0.891691  [   96/  118]
train() client id: f_00009-8-0 loss: 1.043894  [   32/  118]
train() client id: f_00009-8-1 loss: 0.884470  [   64/  118]
train() client id: f_00009-8-2 loss: 0.810031  [   96/  118]
train() client id: f_00009-9-0 loss: 1.079324  [   32/  118]
train() client id: f_00009-9-1 loss: 0.810710  [   64/  118]
train() client id: f_00009-9-2 loss: 0.851304  [   96/  118]
train() client id: f_00009-10-0 loss: 0.869357  [   32/  118]
train() client id: f_00009-10-1 loss: 0.878552  [   64/  118]
train() client id: f_00009-10-2 loss: 0.847212  [   96/  118]
At round 43 accuracy: 0.649867374005305
At round 43 training accuracy: 0.5861837692823608
At round 43 training loss: 0.8359874895350954
update_location
xs = -4.528292 116.001589 125.045120 -125.943528 14.896481 -50.217951 -167.215960 188.375741 -1.680116 109.695607 
ys = 202.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -132.154970 4.001482 
xs mean: 20.442869159412865
ys mean: 9.371751218646876
dists_uav = 225.969880 153.942693 160.118787 162.388551 101.105526 111.936420 194.861341 214.133608 165.734000 148.489522 
uav_gains = -109.410008 -104.690378 -105.121714 -105.276647 -100.119391 -101.224365 -107.359636 -108.593314 -105.501825 -104.296376 
uav_gains_db_mean: -105.15936536660247
dists_bs = 172.689602 331.819779 346.686606 160.153420 258.676498 212.646482 178.294921 412.003507 352.681099 332.102517 
bs_gains = -102.210512 -110.152323 -110.685297 -101.294073 -107.124254 -104.741489 -102.598950 -112.784285 -110.893760 -110.162680 
bs_gains_db_mean: -107.26476237901215
Round 44
-------------------------------
ene_coms = [0.01027536 0.01080008 0.00799614 0.00806131 0.00892582 0.00784046
 0.00904891 0.00975099 0.01137639 0.01080775]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.1685235  10.7271397   5.05845957  1.82417832 12.33986101  5.93507909
  2.27659777  7.27840207  5.34286466  4.85605352]
obj_prev = 60.80715920281729
eta_min = 2.4708427816779945e-18	eta_max = 0.9457886762524679
af = 12.80194016624056	bf = 1.2439744344146624	zeta = 14.082134182864616	eta = 0.9090909090909091
af = 12.80194016624056	bf = 1.2439744344146624	zeta = 27.105426188535844	eta = 0.4723017478933831
af = 12.80194016624056	bf = 1.2439744344146624	zeta = 20.555660281813953	eta = 0.6227939161636524
af = 12.80194016624056	bf = 1.2439744344146624	zeta = 19.36925322853428	eta = 0.6609413391003156
af = 12.80194016624056	bf = 1.2439744344146624	zeta = 19.303834436597814	eta = 0.6631812041430265
af = 12.80194016624056	bf = 1.2439744344146624	zeta = 19.303617162469283	eta = 0.6631886686569036
eta = 0.6631886686569036
ene_coms = [0.01027536 0.01080008 0.00799614 0.00806131 0.00892582 0.00784046
 0.00904891 0.00975099 0.01137639 0.01080775]
ene_comp = [0.03388963 0.07127581 0.0333517  0.01156551 0.08230339 0.03926894
 0.01452413 0.04814479 0.03496549 0.03173791]
ene_total = [1.71914916 3.1948537  1.60948963 0.76398576 3.5511521  1.8337618
 0.91759488 2.25362854 1.80388595 1.65611564]
ti_comp = [0.61142445 0.60617731 0.63421671 0.633565   0.62491992 0.63577349
 0.62368895 0.61666816 0.60041416 0.60610055]
ti_coms = [0.10275363 0.10800077 0.07996137 0.08061308 0.08925816 0.07840459
 0.09048913 0.09750992 0.11376392 0.10807753]
t_total = [27.79981537 27.79981537 27.79981537 27.79981537 27.79981537 27.79981537
 27.79981537 27.79981537 27.79981537 27.79981537]
ene_coms = [0.01027536 0.01080008 0.00799614 0.00806131 0.00892582 0.00784046
 0.00904891 0.00975099 0.01137639 0.01080775]
ene_comp = [6.50721102e-06 6.15895645e-05 5.76445897e-06 2.40875080e-07
 8.92245564e-05 9.36316317e-06 4.92282087e-07 1.83410800e-05
 7.41134565e-06 5.43908090e-06]
ene_total = [0.40022804 0.42279698 0.31147887 0.31380069 0.35091592 0.30555908
 0.35225358 0.38027723 0.44312147 0.42091005]
optimize_network iter = 0 obj = 3.7013418974102903
eta = 0.6631886686569036
freqs = [27713669.17790573 58791221.72511058 26293614.37353004  9127326.0345558
 65851146.57104991 30882803.89984836 11643726.40264419 39036223.23338856
 29117810.38841359 26182047.79648772]
eta_min = 0.6631886686569064	eta_max = 0.7164220210504543
af = 0.0065241915340413445	bf = 1.2439744344146624	zeta = 0.00717661068744548	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01027536 0.01080008 0.00799614 0.00806131 0.00892582 0.00784046
 0.00904891 0.00975099 0.01137639 0.01080775]
ene_comp = [1.51012869e-06 1.42930924e-05 1.33775820e-06 5.58998884e-08
 2.07063460e-05 2.17290961e-06 1.14243921e-07 4.25641509e-06
 1.71995125e-06 1.26224770e-06]
ene_total = [1.48209406 1.55961001 1.15336733 1.16258123 1.29023557 1.13103642
 1.30501844 1.40686725 1.64091085 1.55883765]
ti_comp = [0.49854792 0.49330078 0.52134018 0.52068847 0.51204339 0.52289696
 0.51081243 0.50379163 0.48753763 0.49322403]
ti_coms = [0.10275363 0.10800077 0.07996137 0.08061308 0.08925816 0.07840459
 0.09048913 0.09750992 0.11376392 0.10807753]
t_total = [27.79981537 27.79981537 27.79981537 27.79981537 27.79981537 27.79981537
 27.79981537 27.79981537 27.79981537 27.79981537]
ene_coms = [0.01027536 0.01080008 0.00799614 0.00806131 0.00892582 0.00784046
 0.00904891 0.00975099 0.01137639 0.01080775]
ene_comp = [5.24008492e-06 4.97914221e-05 4.56733815e-06 1.90936939e-07
 7.11526786e-05 7.41082975e-06 3.92915248e-07 1.47128806e-05
 6.01802914e-06 4.39742139e-06]
ene_total = [0.4753004  0.50161911 0.36989444 0.37270515 0.41595446 0.36282847
 0.41837414 0.45149526 0.52623996 0.49987527]
optimize_network iter = 1 obj = 4.394286644441915
eta = 0.7164220210504543
freqs = [27598605.37504555 58662095.84178719 25973111.94905409  9018078.19025445
 65258661.28721723 30490197.46001095 11543986.44807133 38799451.52163409
 29117810.38841359 26125299.96226408]
eta_min = 0.7164220210504543	eta_max = 0.7164220210504549
af = 0.00644252524435483	bf = 1.2439744344146624	zeta = 0.007086777768790313	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01027536 0.01080008 0.00799614 0.00806131 0.00892582 0.00784046
 0.00904891 0.00975099 0.01137639 0.01080775]
ene_comp = [1.49761498e-06 1.42303762e-05 1.30534412e-06 5.45697301e-08
 2.03354180e-05 2.11801332e-06 1.12295081e-07 4.20493765e-06
 1.71995125e-06 1.25678195e-06]
ene_total = [1.48209226 1.55960097 1.15336266 1.16258104 1.29018208 1.1310285
 1.30501816 1.40685983 1.64091085 1.55883686]
ti_comp = [0.49854792 0.49330078 0.52134018 0.52068847 0.51204339 0.52289696
 0.51081243 0.50379163 0.48753763 0.49322403]
ti_coms = [0.10275363 0.10800077 0.07996137 0.08061308 0.08925816 0.07840459
 0.09048913 0.09750992 0.11376392 0.10807753]
t_total = [27.79981537 27.79981537 27.79981537 27.79981537 27.79981537 27.79981537
 27.79981537 27.79981537 27.79981537 27.79981537]
ene_coms = [0.01027536 0.01080008 0.00799614 0.00806131 0.00892582 0.00784046
 0.00904891 0.00975099 0.01137639 0.01080775]
ene_comp = [5.24008492e-06 4.97914221e-05 4.56733815e-06 1.90936939e-07
 7.11526786e-05 7.41082975e-06 3.92915248e-07 1.47128806e-05
 6.01802914e-06 4.39742139e-06]
ene_total = [0.4753004  0.50161911 0.36989444 0.37270515 0.41595446 0.36282847
 0.41837414 0.45149526 0.52623996 0.49987527]
optimize_network iter = 2 obj = 4.394286644441926
eta = 0.7164220210504549
freqs = [27598605.37504555 58662095.84178718 25973111.94905408  9018078.19025444
 65258661.28721721 30490197.46001093 11543986.44807133 38799451.52163407
 29117810.38841359 26125299.96226407]
Done!
ene_coms = [0.01027536 0.01080008 0.00799614 0.00806131 0.00892582 0.00784046
 0.00904891 0.00975099 0.01137639 0.01080775]
ene_comp = [4.79860302e-06 4.55964497e-05 4.18253577e-06 1.74850329e-07
 6.51580010e-05 6.78646063e-06 3.59811782e-07 1.34733071e-05
 5.51100473e-06 4.02693466e-06]
ene_total = [0.01028016 0.01084567 0.00800032 0.00806148 0.00899097 0.00784725
 0.00904927 0.00976447 0.0113819  0.01081178]
At round 44 energy consumption: 0.0950332770962645
At round 44 eta: 0.7164220210504549
At round 44 a_n: 13.110585589604415
At round 44 local rounds: 10.920021720854878
At round 44 global rounds: 46.23273513045625
gradient difference: 0.4658181667327881
train() client id: f_00000-0-0 loss: 0.825604  [   32/  126]
train() client id: f_00000-0-1 loss: 1.228757  [   64/  126]
train() client id: f_00000-0-2 loss: 0.760062  [   96/  126]
train() client id: f_00000-1-0 loss: 0.729249  [   32/  126]
train() client id: f_00000-1-1 loss: 0.863358  [   64/  126]
train() client id: f_00000-1-2 loss: 0.844815  [   96/  126]
train() client id: f_00000-2-0 loss: 0.887371  [   32/  126]
train() client id: f_00000-2-1 loss: 0.864915  [   64/  126]
train() client id: f_00000-2-2 loss: 0.781923  [   96/  126]
train() client id: f_00000-3-0 loss: 0.805734  [   32/  126]
train() client id: f_00000-3-1 loss: 0.891339  [   64/  126]
train() client id: f_00000-3-2 loss: 0.782535  [   96/  126]
train() client id: f_00000-4-0 loss: 0.718932  [   32/  126]
train() client id: f_00000-4-1 loss: 0.785846  [   64/  126]
train() client id: f_00000-4-2 loss: 0.733641  [   96/  126]
train() client id: f_00000-5-0 loss: 0.796104  [   32/  126]
train() client id: f_00000-5-1 loss: 0.675015  [   64/  126]
train() client id: f_00000-5-2 loss: 0.672594  [   96/  126]
train() client id: f_00000-6-0 loss: 0.592549  [   32/  126]
train() client id: f_00000-6-1 loss: 0.852619  [   64/  126]
train() client id: f_00000-6-2 loss: 0.671798  [   96/  126]
train() client id: f_00000-7-0 loss: 0.688442  [   32/  126]
train() client id: f_00000-7-1 loss: 0.654021  [   64/  126]
train() client id: f_00000-7-2 loss: 0.753158  [   96/  126]
train() client id: f_00000-8-0 loss: 0.693319  [   32/  126]
train() client id: f_00000-8-1 loss: 0.714201  [   64/  126]
train() client id: f_00000-8-2 loss: 0.739794  [   96/  126]
train() client id: f_00000-9-0 loss: 0.716340  [   32/  126]
train() client id: f_00000-9-1 loss: 0.616698  [   64/  126]
train() client id: f_00000-9-2 loss: 0.633257  [   96/  126]
train() client id: f_00001-0-0 loss: 0.223934  [   32/  265]
train() client id: f_00001-0-1 loss: 0.442020  [   64/  265]
train() client id: f_00001-0-2 loss: 0.215467  [   96/  265]
train() client id: f_00001-0-3 loss: 0.335807  [  128/  265]
train() client id: f_00001-0-4 loss: 0.286801  [  160/  265]
train() client id: f_00001-0-5 loss: 0.248779  [  192/  265]
train() client id: f_00001-0-6 loss: 0.274329  [  224/  265]
train() client id: f_00001-0-7 loss: 0.388553  [  256/  265]
train() client id: f_00001-1-0 loss: 0.321318  [   32/  265]
train() client id: f_00001-1-1 loss: 0.318154  [   64/  265]
train() client id: f_00001-1-2 loss: 0.201142  [   96/  265]
train() client id: f_00001-1-3 loss: 0.313029  [  128/  265]
train() client id: f_00001-1-4 loss: 0.190446  [  160/  265]
train() client id: f_00001-1-5 loss: 0.355295  [  192/  265]
train() client id: f_00001-1-6 loss: 0.443789  [  224/  265]
train() client id: f_00001-1-7 loss: 0.266480  [  256/  265]
train() client id: f_00001-2-0 loss: 0.295681  [   32/  265]
train() client id: f_00001-2-1 loss: 0.313581  [   64/  265]
train() client id: f_00001-2-2 loss: 0.336336  [   96/  265]
train() client id: f_00001-2-3 loss: 0.339052  [  128/  265]
train() client id: f_00001-2-4 loss: 0.234210  [  160/  265]
train() client id: f_00001-2-5 loss: 0.237717  [  192/  265]
train() client id: f_00001-2-6 loss: 0.310674  [  224/  265]
train() client id: f_00001-2-7 loss: 0.267012  [  256/  265]
train() client id: f_00001-3-0 loss: 0.397154  [   32/  265]
train() client id: f_00001-3-1 loss: 0.348990  [   64/  265]
train() client id: f_00001-3-2 loss: 0.202092  [   96/  265]
train() client id: f_00001-3-3 loss: 0.212542  [  128/  265]
train() client id: f_00001-3-4 loss: 0.181066  [  160/  265]
train() client id: f_00001-3-5 loss: 0.206291  [  192/  265]
train() client id: f_00001-3-6 loss: 0.494940  [  224/  265]
train() client id: f_00001-3-7 loss: 0.191768  [  256/  265]
train() client id: f_00001-4-0 loss: 0.294591  [   32/  265]
train() client id: f_00001-4-1 loss: 0.288009  [   64/  265]
train() client id: f_00001-4-2 loss: 0.201560  [   96/  265]
train() client id: f_00001-4-3 loss: 0.208057  [  128/  265]
train() client id: f_00001-4-4 loss: 0.192651  [  160/  265]
train() client id: f_00001-4-5 loss: 0.299757  [  192/  265]
train() client id: f_00001-4-6 loss: 0.337043  [  224/  265]
train() client id: f_00001-4-7 loss: 0.390870  [  256/  265]
train() client id: f_00001-5-0 loss: 0.185132  [   32/  265]
train() client id: f_00001-5-1 loss: 0.364050  [   64/  265]
train() client id: f_00001-5-2 loss: 0.273714  [   96/  265]
train() client id: f_00001-5-3 loss: 0.254732  [  128/  265]
train() client id: f_00001-5-4 loss: 0.173305  [  160/  265]
train() client id: f_00001-5-5 loss: 0.270186  [  192/  265]
train() client id: f_00001-5-6 loss: 0.274965  [  224/  265]
train() client id: f_00001-5-7 loss: 0.335082  [  256/  265]
train() client id: f_00001-6-0 loss: 0.240780  [   32/  265]
train() client id: f_00001-6-1 loss: 0.322630  [   64/  265]
train() client id: f_00001-6-2 loss: 0.176557  [   96/  265]
train() client id: f_00001-6-3 loss: 0.200630  [  128/  265]
train() client id: f_00001-6-4 loss: 0.259062  [  160/  265]
train() client id: f_00001-6-5 loss: 0.366358  [  192/  265]
train() client id: f_00001-6-6 loss: 0.362712  [  224/  265]
train() client id: f_00001-6-7 loss: 0.228598  [  256/  265]
train() client id: f_00001-7-0 loss: 0.253515  [   32/  265]
train() client id: f_00001-7-1 loss: 0.272156  [   64/  265]
train() client id: f_00001-7-2 loss: 0.226740  [   96/  265]
train() client id: f_00001-7-3 loss: 0.248799  [  128/  265]
train() client id: f_00001-7-4 loss: 0.305800  [  160/  265]
train() client id: f_00001-7-5 loss: 0.320499  [  192/  265]
train() client id: f_00001-7-6 loss: 0.168389  [  224/  265]
train() client id: f_00001-7-7 loss: 0.280842  [  256/  265]
train() client id: f_00001-8-0 loss: 0.262363  [   32/  265]
train() client id: f_00001-8-1 loss: 0.223816  [   64/  265]
train() client id: f_00001-8-2 loss: 0.373178  [   96/  265]
train() client id: f_00001-8-3 loss: 0.144222  [  128/  265]
train() client id: f_00001-8-4 loss: 0.206415  [  160/  265]
train() client id: f_00001-8-5 loss: 0.392952  [  192/  265]
train() client id: f_00001-8-6 loss: 0.253953  [  224/  265]
train() client id: f_00001-8-7 loss: 0.236886  [  256/  265]
train() client id: f_00001-9-0 loss: 0.262186  [   32/  265]
train() client id: f_00001-9-1 loss: 0.220216  [   64/  265]
train() client id: f_00001-9-2 loss: 0.292227  [   96/  265]
train() client id: f_00001-9-3 loss: 0.201545  [  128/  265]
train() client id: f_00001-9-4 loss: 0.218476  [  160/  265]
train() client id: f_00001-9-5 loss: 0.277239  [  192/  265]
train() client id: f_00001-9-6 loss: 0.272193  [  224/  265]
train() client id: f_00001-9-7 loss: 0.260900  [  256/  265]
train() client id: f_00002-0-0 loss: 1.222609  [   32/  124]
train() client id: f_00002-0-1 loss: 1.066244  [   64/  124]
train() client id: f_00002-0-2 loss: 0.971357  [   96/  124]
train() client id: f_00002-1-0 loss: 0.887848  [   32/  124]
train() client id: f_00002-1-1 loss: 0.950844  [   64/  124]
train() client id: f_00002-1-2 loss: 1.114018  [   96/  124]
train() client id: f_00002-2-0 loss: 1.029681  [   32/  124]
train() client id: f_00002-2-1 loss: 0.940357  [   64/  124]
train() client id: f_00002-2-2 loss: 1.033181  [   96/  124]
train() client id: f_00002-3-0 loss: 0.901861  [   32/  124]
train() client id: f_00002-3-1 loss: 1.045781  [   64/  124]
train() client id: f_00002-3-2 loss: 1.005236  [   96/  124]
train() client id: f_00002-4-0 loss: 0.889331  [   32/  124]
train() client id: f_00002-4-1 loss: 0.897139  [   64/  124]
train() client id: f_00002-4-2 loss: 0.875071  [   96/  124]
train() client id: f_00002-5-0 loss: 1.115513  [   32/  124]
train() client id: f_00002-5-1 loss: 0.687535  [   64/  124]
train() client id: f_00002-5-2 loss: 0.754795  [   96/  124]
train() client id: f_00002-6-0 loss: 0.862033  [   32/  124]
train() client id: f_00002-6-1 loss: 0.999320  [   64/  124]
train() client id: f_00002-6-2 loss: 0.826198  [   96/  124]
train() client id: f_00002-7-0 loss: 0.810385  [   32/  124]
train() client id: f_00002-7-1 loss: 0.732178  [   64/  124]
train() client id: f_00002-7-2 loss: 1.063192  [   96/  124]
train() client id: f_00002-8-0 loss: 0.894969  [   32/  124]
train() client id: f_00002-8-1 loss: 0.905592  [   64/  124]
train() client id: f_00002-8-2 loss: 0.899154  [   96/  124]
train() client id: f_00002-9-0 loss: 0.763388  [   32/  124]
train() client id: f_00002-9-1 loss: 1.084930  [   64/  124]
train() client id: f_00002-9-2 loss: 0.767240  [   96/  124]
train() client id: f_00003-0-0 loss: 0.520828  [   32/   43]
train() client id: f_00003-1-0 loss: 0.735733  [   32/   43]
train() client id: f_00003-2-0 loss: 0.390442  [   32/   43]
train() client id: f_00003-3-0 loss: 0.717326  [   32/   43]
train() client id: f_00003-4-0 loss: 0.574687  [   32/   43]
train() client id: f_00003-5-0 loss: 0.552195  [   32/   43]
train() client id: f_00003-6-0 loss: 0.658123  [   32/   43]
train() client id: f_00003-7-0 loss: 0.516606  [   32/   43]
train() client id: f_00003-8-0 loss: 0.429497  [   32/   43]
train() client id: f_00003-9-0 loss: 0.494046  [   32/   43]
train() client id: f_00004-0-0 loss: 1.031164  [   32/  306]
train() client id: f_00004-0-1 loss: 0.753662  [   64/  306]
train() client id: f_00004-0-2 loss: 0.962641  [   96/  306]
train() client id: f_00004-0-3 loss: 0.702249  [  128/  306]
train() client id: f_00004-0-4 loss: 0.830900  [  160/  306]
train() client id: f_00004-0-5 loss: 0.901566  [  192/  306]
train() client id: f_00004-0-6 loss: 0.951852  [  224/  306]
train() client id: f_00004-0-7 loss: 0.807368  [  256/  306]
train() client id: f_00004-0-8 loss: 0.873680  [  288/  306]
train() client id: f_00004-1-0 loss: 0.837061  [   32/  306]
train() client id: f_00004-1-1 loss: 0.927858  [   64/  306]
train() client id: f_00004-1-2 loss: 0.888055  [   96/  306]
train() client id: f_00004-1-3 loss: 0.906462  [  128/  306]
train() client id: f_00004-1-4 loss: 0.795055  [  160/  306]
train() client id: f_00004-1-5 loss: 0.839423  [  192/  306]
train() client id: f_00004-1-6 loss: 0.915665  [  224/  306]
train() client id: f_00004-1-7 loss: 0.927419  [  256/  306]
train() client id: f_00004-1-8 loss: 0.757596  [  288/  306]
train() client id: f_00004-2-0 loss: 0.916158  [   32/  306]
train() client id: f_00004-2-1 loss: 0.839780  [   64/  306]
train() client id: f_00004-2-2 loss: 0.917358  [   96/  306]
train() client id: f_00004-2-3 loss: 0.882972  [  128/  306]
train() client id: f_00004-2-4 loss: 0.842942  [  160/  306]
train() client id: f_00004-2-5 loss: 0.817833  [  192/  306]
train() client id: f_00004-2-6 loss: 0.832422  [  224/  306]
train() client id: f_00004-2-7 loss: 0.942013  [  256/  306]
train() client id: f_00004-2-8 loss: 0.798343  [  288/  306]
train() client id: f_00004-3-0 loss: 0.902027  [   32/  306]
train() client id: f_00004-3-1 loss: 0.922783  [   64/  306]
train() client id: f_00004-3-2 loss: 0.918631  [   96/  306]
train() client id: f_00004-3-3 loss: 0.727463  [  128/  306]
train() client id: f_00004-3-4 loss: 0.776182  [  160/  306]
train() client id: f_00004-3-5 loss: 1.005976  [  192/  306]
train() client id: f_00004-3-6 loss: 0.908578  [  224/  306]
train() client id: f_00004-3-7 loss: 0.758426  [  256/  306]
train() client id: f_00004-3-8 loss: 0.854302  [  288/  306]
train() client id: f_00004-4-0 loss: 0.723251  [   32/  306]
train() client id: f_00004-4-1 loss: 0.800106  [   64/  306]
train() client id: f_00004-4-2 loss: 1.007467  [   96/  306]
train() client id: f_00004-4-3 loss: 0.959337  [  128/  306]
train() client id: f_00004-4-4 loss: 0.867133  [  160/  306]
train() client id: f_00004-4-5 loss: 0.895642  [  192/  306]
train() client id: f_00004-4-6 loss: 0.781600  [  224/  306]
train() client id: f_00004-4-7 loss: 1.022455  [  256/  306]
train() client id: f_00004-4-8 loss: 0.840828  [  288/  306]
train() client id: f_00004-5-0 loss: 0.831773  [   32/  306]
train() client id: f_00004-5-1 loss: 0.871298  [   64/  306]
train() client id: f_00004-5-2 loss: 0.878287  [   96/  306]
train() client id: f_00004-5-3 loss: 0.823163  [  128/  306]
train() client id: f_00004-5-4 loss: 0.899146  [  160/  306]
train() client id: f_00004-5-5 loss: 0.747994  [  192/  306]
train() client id: f_00004-5-6 loss: 1.010843  [  224/  306]
train() client id: f_00004-5-7 loss: 0.764298  [  256/  306]
train() client id: f_00004-5-8 loss: 0.880744  [  288/  306]
train() client id: f_00004-6-0 loss: 0.961473  [   32/  306]
train() client id: f_00004-6-1 loss: 0.951214  [   64/  306]
train() client id: f_00004-6-2 loss: 1.024559  [   96/  306]
train() client id: f_00004-6-3 loss: 0.865998  [  128/  306]
train() client id: f_00004-6-4 loss: 0.877214  [  160/  306]
train() client id: f_00004-6-5 loss: 0.846286  [  192/  306]
train() client id: f_00004-6-6 loss: 0.790821  [  224/  306]
train() client id: f_00004-6-7 loss: 0.798322  [  256/  306]
train() client id: f_00004-6-8 loss: 0.774591  [  288/  306]
train() client id: f_00004-7-0 loss: 0.874895  [   32/  306]
train() client id: f_00004-7-1 loss: 0.949327  [   64/  306]
train() client id: f_00004-7-2 loss: 1.003160  [   96/  306]
train() client id: f_00004-7-3 loss: 0.809813  [  128/  306]
train() client id: f_00004-7-4 loss: 0.965165  [  160/  306]
train() client id: f_00004-7-5 loss: 0.713369  [  192/  306]
train() client id: f_00004-7-6 loss: 0.958595  [  224/  306]
train() client id: f_00004-7-7 loss: 0.785601  [  256/  306]
train() client id: f_00004-7-8 loss: 0.830319  [  288/  306]
train() client id: f_00004-8-0 loss: 0.803237  [   32/  306]
train() client id: f_00004-8-1 loss: 0.820606  [   64/  306]
train() client id: f_00004-8-2 loss: 0.876178  [   96/  306]
train() client id: f_00004-8-3 loss: 0.775780  [  128/  306]
train() client id: f_00004-8-4 loss: 0.975813  [  160/  306]
train() client id: f_00004-8-5 loss: 0.887165  [  192/  306]
train() client id: f_00004-8-6 loss: 0.772584  [  224/  306]
train() client id: f_00004-8-7 loss: 1.018267  [  256/  306]
train() client id: f_00004-8-8 loss: 0.883400  [  288/  306]
train() client id: f_00004-9-0 loss: 1.023195  [   32/  306]
train() client id: f_00004-9-1 loss: 0.818366  [   64/  306]
train() client id: f_00004-9-2 loss: 0.782197  [   96/  306]
train() client id: f_00004-9-3 loss: 0.831258  [  128/  306]
train() client id: f_00004-9-4 loss: 0.906592  [  160/  306]
train() client id: f_00004-9-5 loss: 0.897572  [  192/  306]
train() client id: f_00004-9-6 loss: 0.872692  [  224/  306]
train() client id: f_00004-9-7 loss: 1.029862  [  256/  306]
train() client id: f_00004-9-8 loss: 0.801704  [  288/  306]
train() client id: f_00005-0-0 loss: 0.699741  [   32/  146]
train() client id: f_00005-0-1 loss: 0.534845  [   64/  146]
train() client id: f_00005-0-2 loss: 0.667213  [   96/  146]
train() client id: f_00005-0-3 loss: 0.634491  [  128/  146]
train() client id: f_00005-1-0 loss: 0.694691  [   32/  146]
train() client id: f_00005-1-1 loss: 0.638718  [   64/  146]
train() client id: f_00005-1-2 loss: 0.518041  [   96/  146]
train() client id: f_00005-1-3 loss: 0.382287  [  128/  146]
train() client id: f_00005-2-0 loss: 0.600139  [   32/  146]
train() client id: f_00005-2-1 loss: 0.604990  [   64/  146]
train() client id: f_00005-2-2 loss: 0.554173  [   96/  146]
train() client id: f_00005-2-3 loss: 0.532568  [  128/  146]
train() client id: f_00005-3-0 loss: 0.522341  [   32/  146]
train() client id: f_00005-3-1 loss: 0.643596  [   64/  146]
train() client id: f_00005-3-2 loss: 0.397866  [   96/  146]
train() client id: f_00005-3-3 loss: 0.601723  [  128/  146]
train() client id: f_00005-4-0 loss: 0.381513  [   32/  146]
train() client id: f_00005-4-1 loss: 0.582512  [   64/  146]
train() client id: f_00005-4-2 loss: 0.770706  [   96/  146]
train() client id: f_00005-4-3 loss: 0.611454  [  128/  146]
train() client id: f_00005-5-0 loss: 0.521945  [   32/  146]
train() client id: f_00005-5-1 loss: 0.641234  [   64/  146]
train() client id: f_00005-5-2 loss: 0.634020  [   96/  146]
train() client id: f_00005-5-3 loss: 0.566339  [  128/  146]
train() client id: f_00005-6-0 loss: 0.469952  [   32/  146]
train() client id: f_00005-6-1 loss: 0.550381  [   64/  146]
train() client id: f_00005-6-2 loss: 0.665651  [   96/  146]
train() client id: f_00005-6-3 loss: 0.438756  [  128/  146]
train() client id: f_00005-7-0 loss: 0.442708  [   32/  146]
train() client id: f_00005-7-1 loss: 0.507694  [   64/  146]
train() client id: f_00005-7-2 loss: 0.612705  [   96/  146]
train() client id: f_00005-7-3 loss: 0.779011  [  128/  146]
train() client id: f_00005-8-0 loss: 0.494439  [   32/  146]
train() client id: f_00005-8-1 loss: 0.819197  [   64/  146]
train() client id: f_00005-8-2 loss: 0.540037  [   96/  146]
train() client id: f_00005-8-3 loss: 0.576988  [  128/  146]
train() client id: f_00005-9-0 loss: 0.702058  [   32/  146]
train() client id: f_00005-9-1 loss: 0.489209  [   64/  146]
train() client id: f_00005-9-2 loss: 0.589796  [   96/  146]
train() client id: f_00005-9-3 loss: 0.618395  [  128/  146]
train() client id: f_00006-0-0 loss: 0.565422  [   32/   54]
train() client id: f_00006-1-0 loss: 0.553579  [   32/   54]
train() client id: f_00006-2-0 loss: 0.572557  [   32/   54]
train() client id: f_00006-3-0 loss: 0.486833  [   32/   54]
train() client id: f_00006-4-0 loss: 0.570278  [   32/   54]
train() client id: f_00006-5-0 loss: 0.526833  [   32/   54]
train() client id: f_00006-6-0 loss: 0.491087  [   32/   54]
train() client id: f_00006-7-0 loss: 0.515842  [   32/   54]
train() client id: f_00006-8-0 loss: 0.470885  [   32/   54]
train() client id: f_00006-9-0 loss: 0.512101  [   32/   54]
train() client id: f_00007-0-0 loss: 0.537381  [   32/  179]
train() client id: f_00007-0-1 loss: 0.601841  [   64/  179]
train() client id: f_00007-0-2 loss: 0.461756  [   96/  179]
train() client id: f_00007-0-3 loss: 0.581879  [  128/  179]
train() client id: f_00007-0-4 loss: 0.699293  [  160/  179]
train() client id: f_00007-1-0 loss: 0.581892  [   32/  179]
train() client id: f_00007-1-1 loss: 0.496145  [   64/  179]
train() client id: f_00007-1-2 loss: 0.745402  [   96/  179]
train() client id: f_00007-1-3 loss: 0.416153  [  128/  179]
train() client id: f_00007-1-4 loss: 0.559404  [  160/  179]
train() client id: f_00007-2-0 loss: 0.719804  [   32/  179]
train() client id: f_00007-2-1 loss: 0.481672  [   64/  179]
train() client id: f_00007-2-2 loss: 0.672193  [   96/  179]
train() client id: f_00007-2-3 loss: 0.462315  [  128/  179]
train() client id: f_00007-2-4 loss: 0.667117  [  160/  179]
train() client id: f_00007-3-0 loss: 0.562547  [   32/  179]
train() client id: f_00007-3-1 loss: 0.433215  [   64/  179]
train() client id: f_00007-3-2 loss: 0.530903  [   96/  179]
train() client id: f_00007-3-3 loss: 0.875117  [  128/  179]
train() client id: f_00007-3-4 loss: 0.440766  [  160/  179]
train() client id: f_00007-4-0 loss: 0.663909  [   32/  179]
train() client id: f_00007-4-1 loss: 0.555705  [   64/  179]
train() client id: f_00007-4-2 loss: 0.518504  [   96/  179]
train() client id: f_00007-4-3 loss: 0.498181  [  128/  179]
train() client id: f_00007-4-4 loss: 0.575979  [  160/  179]
train() client id: f_00007-5-0 loss: 0.412469  [   32/  179]
train() client id: f_00007-5-1 loss: 0.682543  [   64/  179]
train() client id: f_00007-5-2 loss: 0.609921  [   96/  179]
train() client id: f_00007-5-3 loss: 0.384938  [  128/  179]
train() client id: f_00007-5-4 loss: 0.783930  [  160/  179]
train() client id: f_00007-6-0 loss: 0.567664  [   32/  179]
train() client id: f_00007-6-1 loss: 0.439860  [   64/  179]
train() client id: f_00007-6-2 loss: 0.401779  [   96/  179]
train() client id: f_00007-6-3 loss: 0.589670  [  128/  179]
train() client id: f_00007-6-4 loss: 0.889580  [  160/  179]
train() client id: f_00007-7-0 loss: 0.486319  [   32/  179]
train() client id: f_00007-7-1 loss: 0.440907  [   64/  179]
train() client id: f_00007-7-2 loss: 0.620871  [   96/  179]
train() client id: f_00007-7-3 loss: 0.569830  [  128/  179]
train() client id: f_00007-7-4 loss: 0.507266  [  160/  179]
train() client id: f_00007-8-0 loss: 0.516884  [   32/  179]
train() client id: f_00007-8-1 loss: 0.530739  [   64/  179]
train() client id: f_00007-8-2 loss: 0.520183  [   96/  179]
train() client id: f_00007-8-3 loss: 0.511521  [  128/  179]
train() client id: f_00007-8-4 loss: 0.725777  [  160/  179]
train() client id: f_00007-9-0 loss: 0.542556  [   32/  179]
train() client id: f_00007-9-1 loss: 0.572252  [   64/  179]
train() client id: f_00007-9-2 loss: 0.601862  [   96/  179]
train() client id: f_00007-9-3 loss: 0.489172  [  128/  179]
train() client id: f_00007-9-4 loss: 0.666837  [  160/  179]
train() client id: f_00008-0-0 loss: 0.746310  [   32/  130]
train() client id: f_00008-0-1 loss: 0.741352  [   64/  130]
train() client id: f_00008-0-2 loss: 0.782343  [   96/  130]
train() client id: f_00008-0-3 loss: 0.704212  [  128/  130]
train() client id: f_00008-1-0 loss: 0.781286  [   32/  130]
train() client id: f_00008-1-1 loss: 0.776757  [   64/  130]
train() client id: f_00008-1-2 loss: 0.625762  [   96/  130]
train() client id: f_00008-1-3 loss: 0.863405  [  128/  130]
train() client id: f_00008-2-0 loss: 0.741124  [   32/  130]
train() client id: f_00008-2-1 loss: 0.660891  [   64/  130]
train() client id: f_00008-2-2 loss: 0.869848  [   96/  130]
train() client id: f_00008-2-3 loss: 0.779756  [  128/  130]
train() client id: f_00008-3-0 loss: 0.711114  [   32/  130]
train() client id: f_00008-3-1 loss: 0.778741  [   64/  130]
train() client id: f_00008-3-2 loss: 0.809685  [   96/  130]
train() client id: f_00008-3-3 loss: 0.700394  [  128/  130]
train() client id: f_00008-4-0 loss: 0.734724  [   32/  130]
train() client id: f_00008-4-1 loss: 0.792365  [   64/  130]
train() client id: f_00008-4-2 loss: 0.719452  [   96/  130]
train() client id: f_00008-4-3 loss: 0.754056  [  128/  130]
train() client id: f_00008-5-0 loss: 0.748133  [   32/  130]
train() client id: f_00008-5-1 loss: 0.787777  [   64/  130]
train() client id: f_00008-5-2 loss: 0.716890  [   96/  130]
train() client id: f_00008-5-3 loss: 0.784096  [  128/  130]
train() client id: f_00008-6-0 loss: 0.698519  [   32/  130]
train() client id: f_00008-6-1 loss: 0.811804  [   64/  130]
train() client id: f_00008-6-2 loss: 0.714833  [   96/  130]
train() client id: f_00008-6-3 loss: 0.780291  [  128/  130]
train() client id: f_00008-7-0 loss: 0.754572  [   32/  130]
train() client id: f_00008-7-1 loss: 0.815727  [   64/  130]
train() client id: f_00008-7-2 loss: 0.680193  [   96/  130]
train() client id: f_00008-7-3 loss: 0.785594  [  128/  130]
train() client id: f_00008-8-0 loss: 0.763288  [   32/  130]
train() client id: f_00008-8-1 loss: 0.690630  [   64/  130]
train() client id: f_00008-8-2 loss: 0.785594  [   96/  130]
train() client id: f_00008-8-3 loss: 0.751805  [  128/  130]
train() client id: f_00008-9-0 loss: 0.658588  [   32/  130]
train() client id: f_00008-9-1 loss: 0.837015  [   64/  130]
train() client id: f_00008-9-2 loss: 0.790924  [   96/  130]
train() client id: f_00008-9-3 loss: 0.739104  [  128/  130]
train() client id: f_00009-0-0 loss: 1.188353  [   32/  118]
train() client id: f_00009-0-1 loss: 0.995837  [   64/  118]
train() client id: f_00009-0-2 loss: 1.165692  [   96/  118]
train() client id: f_00009-1-0 loss: 1.020940  [   32/  118]
train() client id: f_00009-1-1 loss: 1.046973  [   64/  118]
train() client id: f_00009-1-2 loss: 1.249517  [   96/  118]
train() client id: f_00009-2-0 loss: 1.068054  [   32/  118]
train() client id: f_00009-2-1 loss: 1.042784  [   64/  118]
train() client id: f_00009-2-2 loss: 0.931865  [   96/  118]
train() client id: f_00009-3-0 loss: 0.962560  [   32/  118]
train() client id: f_00009-3-1 loss: 0.962796  [   64/  118]
train() client id: f_00009-3-2 loss: 1.027731  [   96/  118]
train() client id: f_00009-4-0 loss: 0.950252  [   32/  118]
train() client id: f_00009-4-1 loss: 1.052369  [   64/  118]
train() client id: f_00009-4-2 loss: 0.961903  [   96/  118]
train() client id: f_00009-5-0 loss: 0.880289  [   32/  118]
train() client id: f_00009-5-1 loss: 0.734442  [   64/  118]
train() client id: f_00009-5-2 loss: 1.066207  [   96/  118]
train() client id: f_00009-6-0 loss: 0.844989  [   32/  118]
train() client id: f_00009-6-1 loss: 1.070407  [   64/  118]
train() client id: f_00009-6-2 loss: 0.827408  [   96/  118]
train() client id: f_00009-7-0 loss: 1.002586  [   32/  118]
train() client id: f_00009-7-1 loss: 0.808590  [   64/  118]
train() client id: f_00009-7-2 loss: 0.891084  [   96/  118]
train() client id: f_00009-8-0 loss: 1.092357  [   32/  118]
train() client id: f_00009-8-1 loss: 0.743413  [   64/  118]
train() client id: f_00009-8-2 loss: 0.840780  [   96/  118]
train() client id: f_00009-9-0 loss: 0.799583  [   32/  118]
train() client id: f_00009-9-1 loss: 0.875670  [   64/  118]
train() client id: f_00009-9-2 loss: 0.941445  [   96/  118]
At round 44 accuracy: 0.649867374005305
At round 44 training accuracy: 0.5881958417169685
At round 44 training loss: 0.8287115377437915
update_location
xs = -4.528292 121.001589 130.045120 -130.943528 19.896481 -55.217951 -172.215960 193.375741 -1.680116 114.695607 
ys = 207.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -137.154970 4.001482 
xs mean: 21.442869159412865
ys mean: 9.371751218646876
dists_uav = 230.463156 157.744631 164.053275 166.296352 101.962210 114.266975 199.168526 218.545097 169.747779 152.220545 
uav_gains = -109.738011 -104.957655 -105.389154 -105.539325 -100.211002 -101.448125 -107.629660 -108.890466 -105.767434 -104.567355 
uav_gains_db_mean: -105.413818774988
dists_bs = 173.558573 336.213298 351.022868 158.693268 262.369006 209.751534 178.146677 416.420037 357.044125 336.398630 
bs_gains = -102.271549 -110.312276 -110.836451 -101.182697 -107.296610 -104.574803 -102.588835 -112.913944 -111.043272 -110.318977 
bs_gains_db_mean: -107.33394155174093
Round 45
-------------------------------
ene_coms = [0.01050119 0.01091976 0.00810928 0.00817419 0.00901563 0.00777406
 0.00919419 0.00993582 0.01149956 0.01092483]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 5.03639582 10.44841068  4.92775429  1.77797306 12.0186106   5.77915437
  2.21898979  7.09061984  5.20485792  4.73068723]
obj_prev = 59.23345360645646
eta_min = 8.738775574435608e-19	eta_max = 0.9465511020604689
af = 12.467458429545857	bf = 1.2263512156013905	zeta = 13.714204272500444	eta = 0.9090909090909091
af = 12.467458429545857	bf = 1.2263512156013905	zeta = 26.560896122467874	eta = 0.46939148333175507
af = 12.467458429545857	bf = 1.2263512156013905	zeta = 20.082067746996113	eta = 0.6208254342439785
af = 12.467458429545857	bf = 1.2263512156013905	zeta = 18.908586923300536	eta = 0.6593543177032731
af = 12.467458429545857	bf = 1.2263512156013905	zeta = 18.843555164388516	eta = 0.6616298421811335
af = 12.467458429545857	bf = 1.2263512156013905	zeta = 18.843336747642617	eta = 0.6616375112600793
eta = 0.6616375112600793
ene_coms = [0.01050119 0.01091976 0.00810928 0.00817419 0.00901563 0.00777406
 0.00919419 0.00993582 0.01149956 0.01092483]
ene_comp = [0.03408286 0.0716822  0.03354186 0.01163145 0.08277266 0.03949284
 0.01460694 0.0484193  0.03516486 0.03191887]
ene_total = [1.68237014 3.11696825 1.57169749 0.74736184 3.46361258 1.78360664
 0.89813084 2.20201847 1.7608724  1.61669808]
ti_comp = [0.63037858 0.62619293 0.65429767 0.65364861 0.64523416 0.65764993
 0.64344859 0.63603234 0.62039485 0.62614225]
ti_coms = [0.10501193 0.10919758 0.08109283 0.08174189 0.09015634 0.07774057
 0.09194191 0.09935816 0.11499565 0.10924826]
t_total = [27.74981117 27.74981117 27.74981117 27.74981117 27.74981117 27.74981117
 27.74981117 27.74981117 27.74981117 27.74981117]
ene_coms = [0.01050119 0.01091976 0.00810928 0.00817419 0.00901563 0.00777406
 0.00919419 0.00993582 0.01149956 0.01092483]
ene_comp = [6.22709080e-06 5.87080640e-05 5.50922821e-06 2.30193540e-07
 8.51345859e-05 8.90113647e-06 4.70467932e-07 1.75378823e-05
 7.06106384e-06 5.18413688e-06]
ene_total = [0.39649535 0.41427017 0.30621005 0.30846005 0.34341566 0.29368834
 0.34695869 0.3755878  0.4342002  0.41244169]
optimize_network iter = 0 obj = 3.6317279919557923
eta = 0.6616375112600793
freqs = [27033643.52907766 57236517.12493804 25631958.72170933  8897327.86931374
 64141565.7073678  30025728.84227252 11350510.36148662 38063552.22838626
 28340704.97764475 25488512.00686221]
eta_min = 0.6616375112600837	eta_max = 0.7214311778666516
af = 0.00602682940488629	bf = 1.2263512156013905	zeta = 0.00662951234537492	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01050119 0.01091976 0.00810928 0.00817419 0.00901563 0.00777406
 0.00919419 0.00993582 0.01149956 0.01092483]
ene_comp = [1.43692820e-06 1.35471403e-05 1.27127830e-06 5.31181575e-08
 1.96451748e-05 2.05397583e-06 1.08562515e-07 4.04694237e-06
 1.62937110e-06 1.19626206e-06]
ene_total = [1.47507791 1.53556556 1.13911472 1.14805953 1.26899086 1.09214262
 1.29132505 1.39603826 1.61532477 1.53454269]
ti_comp = [0.50042418 0.49623853 0.52434327 0.52369421 0.51527976 0.52769553
 0.51349419 0.50607794 0.49044045 0.49618785]
ti_coms = [0.10501193 0.10919758 0.08109283 0.08174189 0.09015634 0.07774057
 0.09194191 0.09935816 0.11499565 0.10924826]
t_total = [27.74981117 27.74981117 27.74981117 27.74981117 27.74981117 27.74981117
 27.74981117 27.74981117 27.74981117 27.74981117]
ene_coms = [0.01050119 0.01091976 0.00810928 0.00817419 0.00901563 0.00777406
 0.00919419 0.00993582 0.01149956 0.01092483]
ene_comp = [4.88164077e-06 4.61835606e-05 4.23803374e-06 1.77166076e-07
 6.59490978e-05 6.83002793e-06 3.64956380e-07 1.36853223e-05
 5.58197949e-06 4.07835029e-06]
ene_total = [0.4815398  0.50261752 0.37187853 0.37466731 0.41624908 0.35663242
 0.42142713 0.45602961 0.52733171 0.50091995]
optimize_network iter = 1 obj = 4.4092930677675
eta = 0.7214311778666516
freqs = [26920668.70936292 57096432.87647713 25284802.7203839   8778984.10451052
 63493888.26199911 29581692.56575185 11243766.23438634 37817185.72371631
 28340704.97764473 25426668.77652948]
eta_min = 0.7214311778666566	eta_max = 0.7214311778666429
af = 0.00594257892924082	bf = 1.2263512156013905	zeta = 0.006536836822164903	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01050119 0.01091976 0.00810928 0.00817419 0.00901563 0.00777406
 0.00919419 0.00993582 0.01149956 0.01092483]
ene_comp = [1.42494332e-06 1.34809092e-05 1.23707543e-06 5.17145009e-08
 1.92504387e-05 1.99367449e-06 1.06530198e-07 3.99472420e-06
 1.62937110e-06 1.19046408e-06]
ene_total = [1.47507622 1.53555625 1.13910992 1.14805933 1.26893542 1.09213415
 1.29132476 1.39603093 1.61532477 1.53454187]
ti_comp = [0.50042418 0.49623853 0.52434327 0.52369421 0.51527976 0.52769553
 0.51349419 0.50607794 0.49044045 0.49618785]
ti_coms = [0.10501193 0.10919758 0.08109283 0.08174189 0.09015634 0.07774057
 0.09194191 0.09935816 0.11499565 0.10924826]
t_total = [27.74981117 27.74981117 27.74981117 27.74981117 27.74981117 27.74981117
 27.74981117 27.74981117 27.74981117 27.74981117]
ene_coms = [0.01050119 0.01091976 0.00810928 0.00817419 0.00901563 0.00777406
 0.00919419 0.00993582 0.01149956 0.01092483]
ene_comp = [4.88164077e-06 4.61835606e-05 4.23803374e-06 1.77166076e-07
 6.59490978e-05 6.83002793e-06 3.64956380e-07 1.36853223e-05
 5.58197949e-06 4.07835029e-06]
ene_total = [0.4815398  0.50261752 0.37187853 0.37466731 0.41624908 0.35663242
 0.42142713 0.45602961 0.52733171 0.50091995]
optimize_network iter = 2 obj = 4.409293067767363
eta = 0.7214311778666429
freqs = [26920668.7093629  57096432.87647708 25284802.72038393  8778984.10451053
 63493888.26199914 29581692.56575188 11243766.23438634 37817185.7237163
 28340704.97764469 25426668.77652945]
Done!
ene_coms = [0.01050119 0.01091976 0.00810928 0.00817419 0.00901563 0.00777406
 0.00919419 0.00993582 0.01149956 0.01092483]
ene_comp = [4.56575114e-06 4.31950351e-05 3.96379174e-06 1.65701708e-07
 6.16815499e-05 6.38805871e-06 3.41340154e-07 1.27997488e-05
 5.22077113e-06 3.81444136e-06]
ene_total = [0.01050576 0.01096295 0.00811325 0.00817435 0.00907732 0.00778044
 0.00919453 0.00994862 0.01150479 0.01092864]
At round 45 energy consumption: 0.0961906479676388
At round 45 eta: 0.7214311778666429
At round 45 a_n: 12.768039742635098
At round 45 local rounds: 10.691867821091906
At round 45 global rounds: 45.834417666894375
gradient difference: 0.4496622085571289
train() client id: f_00000-0-0 loss: 1.269596  [   32/  126]
train() client id: f_00000-0-1 loss: 1.401951  [   64/  126]
train() client id: f_00000-0-2 loss: 1.095204  [   96/  126]
train() client id: f_00000-1-0 loss: 0.996659  [   32/  126]
train() client id: f_00000-1-1 loss: 1.084518  [   64/  126]
train() client id: f_00000-1-2 loss: 1.192144  [   96/  126]
train() client id: f_00000-2-0 loss: 1.109783  [   32/  126]
train() client id: f_00000-2-1 loss: 1.041540  [   64/  126]
train() client id: f_00000-2-2 loss: 1.058826  [   96/  126]
train() client id: f_00000-3-0 loss: 0.999097  [   32/  126]
train() client id: f_00000-3-1 loss: 1.012764  [   64/  126]
train() client id: f_00000-3-2 loss: 1.042344  [   96/  126]
train() client id: f_00000-4-0 loss: 1.092778  [   32/  126]
train() client id: f_00000-4-1 loss: 0.818417  [   64/  126]
train() client id: f_00000-4-2 loss: 1.105411  [   96/  126]
train() client id: f_00000-5-0 loss: 0.955042  [   32/  126]
train() client id: f_00000-5-1 loss: 0.903894  [   64/  126]
train() client id: f_00000-5-2 loss: 1.019484  [   96/  126]
train() client id: f_00000-6-0 loss: 0.989554  [   32/  126]
train() client id: f_00000-6-1 loss: 1.039096  [   64/  126]
train() client id: f_00000-6-2 loss: 0.893070  [   96/  126]
train() client id: f_00000-7-0 loss: 1.023699  [   32/  126]
train() client id: f_00000-7-1 loss: 0.968635  [   64/  126]
train() client id: f_00000-7-2 loss: 0.768244  [   96/  126]
train() client id: f_00000-8-0 loss: 0.935004  [   32/  126]
train() client id: f_00000-8-1 loss: 0.950097  [   64/  126]
train() client id: f_00000-8-2 loss: 0.984779  [   96/  126]
train() client id: f_00000-9-0 loss: 0.904720  [   32/  126]
train() client id: f_00000-9-1 loss: 0.987507  [   64/  126]
train() client id: f_00000-9-2 loss: 1.007617  [   96/  126]
train() client id: f_00001-0-0 loss: 0.472228  [   32/  265]
train() client id: f_00001-0-1 loss: 0.486055  [   64/  265]
train() client id: f_00001-0-2 loss: 0.482153  [   96/  265]
train() client id: f_00001-0-3 loss: 0.485859  [  128/  265]
train() client id: f_00001-0-4 loss: 0.465292  [  160/  265]
train() client id: f_00001-0-5 loss: 0.419860  [  192/  265]
train() client id: f_00001-0-6 loss: 0.697718  [  224/  265]
train() client id: f_00001-0-7 loss: 0.367858  [  256/  265]
train() client id: f_00001-1-0 loss: 0.457318  [   32/  265]
train() client id: f_00001-1-1 loss: 0.510741  [   64/  265]
train() client id: f_00001-1-2 loss: 0.430839  [   96/  265]
train() client id: f_00001-1-3 loss: 0.462955  [  128/  265]
train() client id: f_00001-1-4 loss: 0.479959  [  160/  265]
train() client id: f_00001-1-5 loss: 0.614095  [  192/  265]
train() client id: f_00001-1-6 loss: 0.495106  [  224/  265]
train() client id: f_00001-1-7 loss: 0.385255  [  256/  265]
train() client id: f_00001-2-0 loss: 0.384707  [   32/  265]
train() client id: f_00001-2-1 loss: 0.391223  [   64/  265]
train() client id: f_00001-2-2 loss: 0.537340  [   96/  265]
train() client id: f_00001-2-3 loss: 0.543174  [  128/  265]
train() client id: f_00001-2-4 loss: 0.427487  [  160/  265]
train() client id: f_00001-2-5 loss: 0.574340  [  192/  265]
train() client id: f_00001-2-6 loss: 0.506189  [  224/  265]
train() client id: f_00001-2-7 loss: 0.408092  [  256/  265]
train() client id: f_00001-3-0 loss: 0.377033  [   32/  265]
train() client id: f_00001-3-1 loss: 0.449267  [   64/  265]
train() client id: f_00001-3-2 loss: 0.555405  [   96/  265]
train() client id: f_00001-3-3 loss: 0.670396  [  128/  265]
train() client id: f_00001-3-4 loss: 0.510044  [  160/  265]
train() client id: f_00001-3-5 loss: 0.454786  [  192/  265]
train() client id: f_00001-3-6 loss: 0.378740  [  224/  265]
train() client id: f_00001-3-7 loss: 0.383640  [  256/  265]
train() client id: f_00001-4-0 loss: 0.458488  [   32/  265]
train() client id: f_00001-4-1 loss: 0.528198  [   64/  265]
train() client id: f_00001-4-2 loss: 0.373408  [   96/  265]
train() client id: f_00001-4-3 loss: 0.424634  [  128/  265]
train() client id: f_00001-4-4 loss: 0.538899  [  160/  265]
train() client id: f_00001-4-5 loss: 0.465485  [  192/  265]
train() client id: f_00001-4-6 loss: 0.491655  [  224/  265]
train() client id: f_00001-4-7 loss: 0.504603  [  256/  265]
train() client id: f_00001-5-0 loss: 0.511620  [   32/  265]
train() client id: f_00001-5-1 loss: 0.504523  [   64/  265]
train() client id: f_00001-5-2 loss: 0.526265  [   96/  265]
train() client id: f_00001-5-3 loss: 0.413705  [  128/  265]
train() client id: f_00001-5-4 loss: 0.404759  [  160/  265]
train() client id: f_00001-5-5 loss: 0.507231  [  192/  265]
train() client id: f_00001-5-6 loss: 0.477672  [  224/  265]
train() client id: f_00001-5-7 loss: 0.418604  [  256/  265]
train() client id: f_00001-6-0 loss: 0.521882  [   32/  265]
train() client id: f_00001-6-1 loss: 0.471906  [   64/  265]
train() client id: f_00001-6-2 loss: 0.401510  [   96/  265]
train() client id: f_00001-6-3 loss: 0.429789  [  128/  265]
train() client id: f_00001-6-4 loss: 0.449406  [  160/  265]
train() client id: f_00001-6-5 loss: 0.498812  [  192/  265]
train() client id: f_00001-6-6 loss: 0.501908  [  224/  265]
train() client id: f_00001-6-7 loss: 0.426067  [  256/  265]
train() client id: f_00001-7-0 loss: 0.410874  [   32/  265]
train() client id: f_00001-7-1 loss: 0.462931  [   64/  265]
train() client id: f_00001-7-2 loss: 0.456576  [   96/  265]
train() client id: f_00001-7-3 loss: 0.503467  [  128/  265]
train() client id: f_00001-7-4 loss: 0.426732  [  160/  265]
train() client id: f_00001-7-5 loss: 0.365433  [  192/  265]
train() client id: f_00001-7-6 loss: 0.527798  [  224/  265]
train() client id: f_00001-7-7 loss: 0.551853  [  256/  265]
train() client id: f_00001-8-0 loss: 0.525945  [   32/  265]
train() client id: f_00001-8-1 loss: 0.355040  [   64/  265]
train() client id: f_00001-8-2 loss: 0.491709  [   96/  265]
train() client id: f_00001-8-3 loss: 0.542981  [  128/  265]
train() client id: f_00001-8-4 loss: 0.413710  [  160/  265]
train() client id: f_00001-8-5 loss: 0.376022  [  192/  265]
train() client id: f_00001-8-6 loss: 0.585914  [  224/  265]
train() client id: f_00001-8-7 loss: 0.475493  [  256/  265]
train() client id: f_00001-9-0 loss: 0.369942  [   32/  265]
train() client id: f_00001-9-1 loss: 0.371569  [   64/  265]
train() client id: f_00001-9-2 loss: 0.531977  [   96/  265]
train() client id: f_00001-9-3 loss: 0.449199  [  128/  265]
train() client id: f_00001-9-4 loss: 0.488092  [  160/  265]
train() client id: f_00001-9-5 loss: 0.478255  [  192/  265]
train() client id: f_00001-9-6 loss: 0.527740  [  224/  265]
train() client id: f_00001-9-7 loss: 0.557096  [  256/  265]
train() client id: f_00002-0-0 loss: 1.232515  [   32/  124]
train() client id: f_00002-0-1 loss: 1.264751  [   64/  124]
train() client id: f_00002-0-2 loss: 1.409747  [   96/  124]
train() client id: f_00002-1-0 loss: 1.307720  [   32/  124]
train() client id: f_00002-1-1 loss: 1.189117  [   64/  124]
train() client id: f_00002-1-2 loss: 1.166363  [   96/  124]
train() client id: f_00002-2-0 loss: 1.061450  [   32/  124]
train() client id: f_00002-2-1 loss: 1.311659  [   64/  124]
train() client id: f_00002-2-2 loss: 1.257010  [   96/  124]
train() client id: f_00002-3-0 loss: 1.051644  [   32/  124]
train() client id: f_00002-3-1 loss: 1.277070  [   64/  124]
train() client id: f_00002-3-2 loss: 1.039287  [   96/  124]
train() client id: f_00002-4-0 loss: 1.315855  [   32/  124]
train() client id: f_00002-4-1 loss: 1.320088  [   64/  124]
train() client id: f_00002-4-2 loss: 0.910993  [   96/  124]
train() client id: f_00002-5-0 loss: 1.128839  [   32/  124]
train() client id: f_00002-5-1 loss: 1.001374  [   64/  124]
train() client id: f_00002-5-2 loss: 1.190911  [   96/  124]
train() client id: f_00002-6-0 loss: 0.981162  [   32/  124]
train() client id: f_00002-6-1 loss: 1.116328  [   64/  124]
train() client id: f_00002-6-2 loss: 1.169526  [   96/  124]
train() client id: f_00002-7-0 loss: 1.136089  [   32/  124]
train() client id: f_00002-7-1 loss: 0.962147  [   64/  124]
train() client id: f_00002-7-2 loss: 1.047957  [   96/  124]
train() client id: f_00002-8-0 loss: 0.965646  [   32/  124]
train() client id: f_00002-8-1 loss: 1.126021  [   64/  124]
train() client id: f_00002-8-2 loss: 1.105677  [   96/  124]
train() client id: f_00002-9-0 loss: 0.993207  [   32/  124]
train() client id: f_00002-9-1 loss: 1.157676  [   64/  124]
train() client id: f_00002-9-2 loss: 0.922778  [   96/  124]
train() client id: f_00003-0-0 loss: 0.778496  [   32/   43]
train() client id: f_00003-1-0 loss: 0.900644  [   32/   43]
train() client id: f_00003-2-0 loss: 1.020187  [   32/   43]
train() client id: f_00003-3-0 loss: 0.860182  [   32/   43]
train() client id: f_00003-4-0 loss: 1.033833  [   32/   43]
train() client id: f_00003-5-0 loss: 0.735228  [   32/   43]
train() client id: f_00003-6-0 loss: 0.740812  [   32/   43]
train() client id: f_00003-7-0 loss: 0.726654  [   32/   43]
train() client id: f_00003-8-0 loss: 0.934836  [   32/   43]
train() client id: f_00003-9-0 loss: 0.799706  [   32/   43]
train() client id: f_00004-0-0 loss: 0.903379  [   32/  306]
train() client id: f_00004-0-1 loss: 0.887892  [   64/  306]
train() client id: f_00004-0-2 loss: 0.758841  [   96/  306]
train() client id: f_00004-0-3 loss: 0.984609  [  128/  306]
train() client id: f_00004-0-4 loss: 0.996277  [  160/  306]
train() client id: f_00004-0-5 loss: 0.971641  [  192/  306]
train() client id: f_00004-0-6 loss: 0.846259  [  224/  306]
train() client id: f_00004-0-7 loss: 0.966056  [  256/  306]
train() client id: f_00004-0-8 loss: 0.924513  [  288/  306]
train() client id: f_00004-1-0 loss: 0.827456  [   32/  306]
train() client id: f_00004-1-1 loss: 0.924573  [   64/  306]
train() client id: f_00004-1-2 loss: 1.000759  [   96/  306]
train() client id: f_00004-1-3 loss: 0.989865  [  128/  306]
train() client id: f_00004-1-4 loss: 0.819095  [  160/  306]
train() client id: f_00004-1-5 loss: 0.875752  [  192/  306]
train() client id: f_00004-1-6 loss: 0.933617  [  224/  306]
train() client id: f_00004-1-7 loss: 0.884787  [  256/  306]
train() client id: f_00004-1-8 loss: 0.941421  [  288/  306]
train() client id: f_00004-2-0 loss: 1.009591  [   32/  306]
train() client id: f_00004-2-1 loss: 0.933343  [   64/  306]
train() client id: f_00004-2-2 loss: 0.926528  [   96/  306]
train() client id: f_00004-2-3 loss: 0.896907  [  128/  306]
train() client id: f_00004-2-4 loss: 0.758098  [  160/  306]
train() client id: f_00004-2-5 loss: 0.935326  [  192/  306]
train() client id: f_00004-2-6 loss: 0.882725  [  224/  306]
train() client id: f_00004-2-7 loss: 0.732957  [  256/  306]
train() client id: f_00004-2-8 loss: 1.061252  [  288/  306]
train() client id: f_00004-3-0 loss: 0.847032  [   32/  306]
train() client id: f_00004-3-1 loss: 0.871161  [   64/  306]
train() client id: f_00004-3-2 loss: 0.970578  [   96/  306]
train() client id: f_00004-3-3 loss: 0.981101  [  128/  306]
train() client id: f_00004-3-4 loss: 0.928592  [  160/  306]
train() client id: f_00004-3-5 loss: 0.825879  [  192/  306]
train() client id: f_00004-3-6 loss: 0.986217  [  224/  306]
train() client id: f_00004-3-7 loss: 0.801329  [  256/  306]
train() client id: f_00004-3-8 loss: 0.843402  [  288/  306]
train() client id: f_00004-4-0 loss: 0.944057  [   32/  306]
train() client id: f_00004-4-1 loss: 0.928832  [   64/  306]
train() client id: f_00004-4-2 loss: 0.834691  [   96/  306]
train() client id: f_00004-4-3 loss: 0.854621  [  128/  306]
train() client id: f_00004-4-4 loss: 0.928082  [  160/  306]
train() client id: f_00004-4-5 loss: 0.856282  [  192/  306]
train() client id: f_00004-4-6 loss: 0.849064  [  224/  306]
train() client id: f_00004-4-7 loss: 0.919567  [  256/  306]
train() client id: f_00004-4-8 loss: 0.919039  [  288/  306]
train() client id: f_00004-5-0 loss: 0.839827  [   32/  306]
train() client id: f_00004-5-1 loss: 0.877176  [   64/  306]
train() client id: f_00004-5-2 loss: 0.851201  [   96/  306]
train() client id: f_00004-5-3 loss: 1.056883  [  128/  306]
train() client id: f_00004-5-4 loss: 0.904270  [  160/  306]
train() client id: f_00004-5-5 loss: 0.918134  [  192/  306]
train() client id: f_00004-5-6 loss: 1.026283  [  224/  306]
train() client id: f_00004-5-7 loss: 0.859654  [  256/  306]
train() client id: f_00004-5-8 loss: 0.850483  [  288/  306]
train() client id: f_00004-6-0 loss: 0.813292  [   32/  306]
train() client id: f_00004-6-1 loss: 0.876274  [   64/  306]
train() client id: f_00004-6-2 loss: 0.926137  [   96/  306]
train() client id: f_00004-6-3 loss: 0.853493  [  128/  306]
train() client id: f_00004-6-4 loss: 1.110645  [  160/  306]
train() client id: f_00004-6-5 loss: 0.878570  [  192/  306]
train() client id: f_00004-6-6 loss: 0.915770  [  224/  306]
train() client id: f_00004-6-7 loss: 0.741713  [  256/  306]
train() client id: f_00004-6-8 loss: 0.952865  [  288/  306]
train() client id: f_00004-7-0 loss: 0.943099  [   32/  306]
train() client id: f_00004-7-1 loss: 0.967434  [   64/  306]
train() client id: f_00004-7-2 loss: 0.814149  [   96/  306]
train() client id: f_00004-7-3 loss: 1.078152  [  128/  306]
train() client id: f_00004-7-4 loss: 0.851190  [  160/  306]
train() client id: f_00004-7-5 loss: 0.800403  [  192/  306]
train() client id: f_00004-7-6 loss: 0.787661  [  224/  306]
train() client id: f_00004-7-7 loss: 0.784835  [  256/  306]
train() client id: f_00004-7-8 loss: 0.968534  [  288/  306]
train() client id: f_00004-8-0 loss: 0.870064  [   32/  306]
train() client id: f_00004-8-1 loss: 0.791746  [   64/  306]
train() client id: f_00004-8-2 loss: 0.915472  [   96/  306]
train() client id: f_00004-8-3 loss: 0.879823  [  128/  306]
train() client id: f_00004-8-4 loss: 0.865806  [  160/  306]
train() client id: f_00004-8-5 loss: 0.870855  [  192/  306]
train() client id: f_00004-8-6 loss: 0.947188  [  224/  306]
train() client id: f_00004-8-7 loss: 0.954407  [  256/  306]
train() client id: f_00004-8-8 loss: 0.881706  [  288/  306]
train() client id: f_00004-9-0 loss: 0.867441  [   32/  306]
train() client id: f_00004-9-1 loss: 0.974077  [   64/  306]
train() client id: f_00004-9-2 loss: 0.922783  [   96/  306]
train() client id: f_00004-9-3 loss: 0.876354  [  128/  306]
train() client id: f_00004-9-4 loss: 0.914329  [  160/  306]
train() client id: f_00004-9-5 loss: 0.907898  [  192/  306]
train() client id: f_00004-9-6 loss: 0.815287  [  224/  306]
train() client id: f_00004-9-7 loss: 0.850518  [  256/  306]
train() client id: f_00004-9-8 loss: 0.880643  [  288/  306]
train() client id: f_00005-0-0 loss: 0.543185  [   32/  146]
train() client id: f_00005-0-1 loss: 0.755415  [   64/  146]
train() client id: f_00005-0-2 loss: 0.564148  [   96/  146]
train() client id: f_00005-0-3 loss: 0.581154  [  128/  146]
train() client id: f_00005-1-0 loss: 0.543919  [   32/  146]
train() client id: f_00005-1-1 loss: 0.662559  [   64/  146]
train() client id: f_00005-1-2 loss: 0.415004  [   96/  146]
train() client id: f_00005-1-3 loss: 0.705328  [  128/  146]
train() client id: f_00005-2-0 loss: 0.829178  [   32/  146]
train() client id: f_00005-2-1 loss: 0.731734  [   64/  146]
train() client id: f_00005-2-2 loss: 0.490258  [   96/  146]
train() client id: f_00005-2-3 loss: 0.283235  [  128/  146]
train() client id: f_00005-3-0 loss: 0.676192  [   32/  146]
train() client id: f_00005-3-1 loss: 0.647571  [   64/  146]
train() client id: f_00005-3-2 loss: 0.507795  [   96/  146]
train() client id: f_00005-3-3 loss: 0.462825  [  128/  146]
train() client id: f_00005-4-0 loss: 0.625855  [   32/  146]
train() client id: f_00005-4-1 loss: 0.634435  [   64/  146]
train() client id: f_00005-4-2 loss: 0.676479  [   96/  146]
train() client id: f_00005-4-3 loss: 0.547136  [  128/  146]
train() client id: f_00005-5-0 loss: 0.618753  [   32/  146]
train() client id: f_00005-5-1 loss: 0.634431  [   64/  146]
train() client id: f_00005-5-2 loss: 0.496737  [   96/  146]
train() client id: f_00005-5-3 loss: 0.653996  [  128/  146]
train() client id: f_00005-6-0 loss: 0.573017  [   32/  146]
train() client id: f_00005-6-1 loss: 0.674237  [   64/  146]
train() client id: f_00005-6-2 loss: 0.545644  [   96/  146]
train() client id: f_00005-6-3 loss: 0.567144  [  128/  146]
train() client id: f_00005-7-0 loss: 0.520635  [   32/  146]
train() client id: f_00005-7-1 loss: 0.694151  [   64/  146]
train() client id: f_00005-7-2 loss: 0.463484  [   96/  146]
train() client id: f_00005-7-3 loss: 0.565314  [  128/  146]
train() client id: f_00005-8-0 loss: 0.550058  [   32/  146]
train() client id: f_00005-8-1 loss: 0.533024  [   64/  146]
train() client id: f_00005-8-2 loss: 0.684075  [   96/  146]
train() client id: f_00005-8-3 loss: 0.647528  [  128/  146]
train() client id: f_00005-9-0 loss: 0.615209  [   32/  146]
train() client id: f_00005-9-1 loss: 0.578305  [   64/  146]
train() client id: f_00005-9-2 loss: 0.724482  [   96/  146]
train() client id: f_00005-9-3 loss: 0.667269  [  128/  146]
train() client id: f_00006-0-0 loss: 0.502653  [   32/   54]
train() client id: f_00006-1-0 loss: 0.536501  [   32/   54]
train() client id: f_00006-2-0 loss: 0.495333  [   32/   54]
train() client id: f_00006-3-0 loss: 0.518513  [   32/   54]
train() client id: f_00006-4-0 loss: 0.468838  [   32/   54]
train() client id: f_00006-5-0 loss: 0.584203  [   32/   54]
train() client id: f_00006-6-0 loss: 0.520772  [   32/   54]
train() client id: f_00006-7-0 loss: 0.466325  [   32/   54]
train() client id: f_00006-8-0 loss: 0.518744  [   32/   54]
train() client id: f_00006-9-0 loss: 0.469867  [   32/   54]
train() client id: f_00007-0-0 loss: 0.480830  [   32/  179]
train() client id: f_00007-0-1 loss: 0.432040  [   64/  179]
train() client id: f_00007-0-2 loss: 0.382976  [   96/  179]
train() client id: f_00007-0-3 loss: 0.678869  [  128/  179]
train() client id: f_00007-0-4 loss: 0.581006  [  160/  179]
train() client id: f_00007-1-0 loss: 0.682851  [   32/  179]
train() client id: f_00007-1-1 loss: 0.556499  [   64/  179]
train() client id: f_00007-1-2 loss: 0.446126  [   96/  179]
train() client id: f_00007-1-3 loss: 0.494051  [  128/  179]
train() client id: f_00007-1-4 loss: 0.447329  [  160/  179]
train() client id: f_00007-2-0 loss: 0.560482  [   32/  179]
train() client id: f_00007-2-1 loss: 0.452107  [   64/  179]
train() client id: f_00007-2-2 loss: 0.551555  [   96/  179]
train() client id: f_00007-2-3 loss: 0.407851  [  128/  179]
train() client id: f_00007-2-4 loss: 0.437298  [  160/  179]
train() client id: f_00007-3-0 loss: 0.649328  [   32/  179]
train() client id: f_00007-3-1 loss: 0.357409  [   64/  179]
train() client id: f_00007-3-2 loss: 0.453320  [   96/  179]
train() client id: f_00007-3-3 loss: 0.434124  [  128/  179]
train() client id: f_00007-3-4 loss: 0.524187  [  160/  179]
train() client id: f_00007-4-0 loss: 0.462624  [   32/  179]
train() client id: f_00007-4-1 loss: 0.394552  [   64/  179]
train() client id: f_00007-4-2 loss: 0.705502  [   96/  179]
train() client id: f_00007-4-3 loss: 0.346805  [  128/  179]
train() client id: f_00007-4-4 loss: 0.530486  [  160/  179]
train() client id: f_00007-5-0 loss: 0.526885  [   32/  179]
train() client id: f_00007-5-1 loss: 0.649531  [   64/  179]
train() client id: f_00007-5-2 loss: 0.333222  [   96/  179]
train() client id: f_00007-5-3 loss: 0.308533  [  128/  179]
train() client id: f_00007-5-4 loss: 0.510830  [  160/  179]
train() client id: f_00007-6-0 loss: 0.471292  [   32/  179]
train() client id: f_00007-6-1 loss: 0.513049  [   64/  179]
train() client id: f_00007-6-2 loss: 0.489552  [   96/  179]
train() client id: f_00007-6-3 loss: 0.516135  [  128/  179]
train() client id: f_00007-6-4 loss: 0.399314  [  160/  179]
train() client id: f_00007-7-0 loss: 0.601716  [   32/  179]
train() client id: f_00007-7-1 loss: 0.531266  [   64/  179]
train() client id: f_00007-7-2 loss: 0.457768  [   96/  179]
train() client id: f_00007-7-3 loss: 0.288714  [  128/  179]
train() client id: f_00007-7-4 loss: 0.476527  [  160/  179]
train() client id: f_00007-8-0 loss: 0.427224  [   32/  179]
train() client id: f_00007-8-1 loss: 0.475105  [   64/  179]
train() client id: f_00007-8-2 loss: 0.421436  [   96/  179]
train() client id: f_00007-8-3 loss: 0.309876  [  128/  179]
train() client id: f_00007-8-4 loss: 0.630973  [  160/  179]
train() client id: f_00007-9-0 loss: 0.300133  [   32/  179]
train() client id: f_00007-9-1 loss: 0.373201  [   64/  179]
train() client id: f_00007-9-2 loss: 0.522115  [   96/  179]
train() client id: f_00007-9-3 loss: 0.406737  [  128/  179]
train() client id: f_00007-9-4 loss: 0.663242  [  160/  179]
train() client id: f_00008-0-0 loss: 0.801180  [   32/  130]
train() client id: f_00008-0-1 loss: 0.814442  [   64/  130]
train() client id: f_00008-0-2 loss: 0.729736  [   96/  130]
train() client id: f_00008-0-3 loss: 0.716266  [  128/  130]
train() client id: f_00008-1-0 loss: 0.712388  [   32/  130]
train() client id: f_00008-1-1 loss: 0.732476  [   64/  130]
train() client id: f_00008-1-2 loss: 0.899104  [   96/  130]
train() client id: f_00008-1-3 loss: 0.736585  [  128/  130]
train() client id: f_00008-2-0 loss: 0.791688  [   32/  130]
train() client id: f_00008-2-1 loss: 0.770067  [   64/  130]
train() client id: f_00008-2-2 loss: 0.712009  [   96/  130]
train() client id: f_00008-2-3 loss: 0.827210  [  128/  130]
train() client id: f_00008-3-0 loss: 0.912529  [   32/  130]
train() client id: f_00008-3-1 loss: 0.606083  [   64/  130]
train() client id: f_00008-3-2 loss: 0.728829  [   96/  130]
train() client id: f_00008-3-3 loss: 0.864711  [  128/  130]
train() client id: f_00008-4-0 loss: 0.746143  [   32/  130]
train() client id: f_00008-4-1 loss: 0.725618  [   64/  130]
train() client id: f_00008-4-2 loss: 0.853446  [   96/  130]
train() client id: f_00008-4-3 loss: 0.759665  [  128/  130]
train() client id: f_00008-5-0 loss: 0.677750  [   32/  130]
train() client id: f_00008-5-1 loss: 0.760541  [   64/  130]
train() client id: f_00008-5-2 loss: 0.870897  [   96/  130]
train() client id: f_00008-5-3 loss: 0.769594  [  128/  130]
train() client id: f_00008-6-0 loss: 0.775303  [   32/  130]
train() client id: f_00008-6-1 loss: 0.701820  [   64/  130]
train() client id: f_00008-6-2 loss: 0.843429  [   96/  130]
train() client id: f_00008-6-3 loss: 0.794377  [  128/  130]
train() client id: f_00008-7-0 loss: 0.800226  [   32/  130]
train() client id: f_00008-7-1 loss: 0.756367  [   64/  130]
train() client id: f_00008-7-2 loss: 0.781042  [   96/  130]
train() client id: f_00008-7-3 loss: 0.771983  [  128/  130]
train() client id: f_00008-8-0 loss: 0.624446  [   32/  130]
train() client id: f_00008-8-1 loss: 0.816094  [   64/  130]
train() client id: f_00008-8-2 loss: 0.765257  [   96/  130]
train() client id: f_00008-8-3 loss: 0.902857  [  128/  130]
train() client id: f_00008-9-0 loss: 0.817203  [   32/  130]
train() client id: f_00008-9-1 loss: 0.797199  [   64/  130]
train() client id: f_00008-9-2 loss: 0.760944  [   96/  130]
train() client id: f_00008-9-3 loss: 0.712367  [  128/  130]
train() client id: f_00009-0-0 loss: 1.135222  [   32/  118]
train() client id: f_00009-0-1 loss: 0.701235  [   64/  118]
train() client id: f_00009-0-2 loss: 0.882851  [   96/  118]
train() client id: f_00009-1-0 loss: 0.799990  [   32/  118]
train() client id: f_00009-1-1 loss: 0.970546  [   64/  118]
train() client id: f_00009-1-2 loss: 0.789687  [   96/  118]
train() client id: f_00009-2-0 loss: 0.701551  [   32/  118]
train() client id: f_00009-2-1 loss: 0.779674  [   64/  118]
train() client id: f_00009-2-2 loss: 0.897355  [   96/  118]
train() client id: f_00009-3-0 loss: 0.672562  [   32/  118]
train() client id: f_00009-3-1 loss: 0.899544  [   64/  118]
train() client id: f_00009-3-2 loss: 0.826628  [   96/  118]
train() client id: f_00009-4-0 loss: 0.951342  [   32/  118]
train() client id: f_00009-4-1 loss: 0.621356  [   64/  118]
train() client id: f_00009-4-2 loss: 0.830992  [   96/  118]
train() client id: f_00009-5-0 loss: 0.704127  [   32/  118]
train() client id: f_00009-5-1 loss: 0.767004  [   64/  118]
train() client id: f_00009-5-2 loss: 0.733749  [   96/  118]
train() client id: f_00009-6-0 loss: 0.597914  [   32/  118]
train() client id: f_00009-6-1 loss: 0.746595  [   64/  118]
train() client id: f_00009-6-2 loss: 0.846898  [   96/  118]
train() client id: f_00009-7-0 loss: 0.678512  [   32/  118]
train() client id: f_00009-7-1 loss: 0.697624  [   64/  118]
train() client id: f_00009-7-2 loss: 0.896625  [   96/  118]
train() client id: f_00009-8-0 loss: 0.905694  [   32/  118]
train() client id: f_00009-8-1 loss: 0.609611  [   64/  118]
train() client id: f_00009-8-2 loss: 0.714962  [   96/  118]
train() client id: f_00009-9-0 loss: 0.732034  [   32/  118]
train() client id: f_00009-9-1 loss: 0.803939  [   64/  118]
train() client id: f_00009-9-2 loss: 0.671314  [   96/  118]
At round 45 accuracy: 0.649867374005305
At round 45 training accuracy: 0.5949027498323273
At round 45 training loss: 0.817617759211649
update_location
xs = -4.528292 126.001589 135.045120 -135.943528 24.896481 -60.217951 -177.215960 198.375741 -1.680116 119.695607 
ys = 212.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -142.154970 4.001482 
xs mean: 22.442869159412865
ys mean: 9.371751218646876
dists_uav = 234.976905 161.611833 168.044424 170.261305 103.054631 116.765239 203.507399 222.981427 173.812710 156.022595 
uav_gains = -110.078778 -105.223833 -105.655286 -105.801087 -100.326712 -101.682985 -107.903889 -109.197708 -106.031986 -104.837312 
uav_gains_db_mean: -105.67395755337219
dists_bs = 174.566486 340.623542 355.376568 157.378488 266.104228 206.936912 178.138705 420.849622 361.423653 340.713948 
bs_gains = -102.341963 -110.470750 -110.986345 -101.081530 -107.468509 -104.410522 -102.588291 -113.042614 -111.191523 -110.473977 
bs_gains_db_mean: -107.40560234780575
Round 46
-------------------------------
ene_coms = [0.01074587 0.01104079 0.008225   0.00828977 0.00910696 0.00770968
 0.00934637 0.01013401 0.01162415 0.01104328]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.90434839 10.16961581  4.797003    1.73172353 11.69731691  5.62330097
  2.16136784  6.90287747  5.0667837   4.60525729]
obj_prev = 57.65959489925964
eta_min = 2.916547519424219e-19	eta_max = 0.9473402388367176
af = 12.132976692851157	bf = 1.2085764170604951	zeta = 13.346274362136274	eta = 0.9090909090909091
af = 12.132976692851157	bf = 1.2085764170604951	zeta = 26.01469867939604	eta = 0.4663892840880999
af = 12.132976692851157	bf = 1.2085764170604951	zeta = 19.607585590851965	eta = 0.6187899390586808
af = 12.132976692851157	bf = 1.2085764170604951	zeta = 18.447248836430443	eta = 0.657711987323032
af = 12.132976692851157	bf = 1.2085764170604951	zeta = 18.382618366686206	eta = 0.6600244019012588
af = 12.132976692851157	bf = 1.2085764170604951	zeta = 18.38239882505161	eta = 0.6600322845958649
eta = 0.6600322845958649
ene_coms = [0.01074587 0.01104079 0.008225   0.00828977 0.00910696 0.00770968
 0.00934637 0.01013401 0.01162415 0.01104328]
ene_comp = [0.0342833  0.07210377 0.03373912 0.01169986 0.08325945 0.0397251
 0.01469284 0.04870406 0.03537166 0.03210658]
ene_total = [1.64577297 3.03885395 1.53374851 0.73060154 3.37590349 1.73369545
 0.87861008 2.15047475 1.71765189 1.5770862 ]
ti_comp = [0.65042187 0.64747264 0.67563053 0.67498289 0.66681098 0.68078378
 0.66441688 0.65654048 0.64163903 0.64744773]
ti_coms = [0.10745868 0.11040791 0.08225002 0.08289766 0.09106956 0.07709677
 0.09346367 0.10134006 0.11624152 0.11043281]
t_total = [27.69980698 27.69980698 27.69980698 27.69980698 27.69980698 27.69980698
 27.69980698 27.69980698 27.69980698 27.69980698]
ene_coms = [0.01074587 0.01104079 0.008225   0.00828977 0.00910696 0.00770968
 0.00934637 0.01013401 0.01162415 0.01104328]
ene_comp = [5.95302478e-06 5.58870389e-05 5.25851177e-06 2.19703088e-07
 8.11288116e-05 8.45388806e-06 4.49072318e-07 1.67514495e-05
 6.71838423e-06 4.93461088e-06]
ene_total = [0.39296874 0.40557292 0.30080813 0.30299101 0.3358157  0.28209028
 0.34161714 0.37100046 0.42509716 0.40380169]
optimize_network iter = 0 obj = 3.5617632308626725
eta = 0.6600322845958649
freqs = [26354666.04804484 55680939.94554255 24968619.29426727  8666780.50851065
 62431071.53602718 29176000.88528102 11056946.48733735 37091434.19528886
 27563520.58857054 24794730.78181279]
eta_min = 0.6600322845958673	eta_max = 0.7265812278175124
af = 0.005555420676890964	bf = 1.2085764170604951	zeta = 0.00611096274458006	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01074587 0.01104079 0.008225   0.00828977 0.00910696 0.00770968
 0.00934637 0.01013401 0.01162415 0.01104328]
ene_comp = [1.36565480e-06 1.28207770e-05 1.20632991e-06 5.04010296e-08
 1.86113708e-05 1.93936583e-06 1.03019522e-07 3.84286954e-06
 1.54123224e-06 1.13202537e-06]
ene_total = [1.46893654 1.51081238 1.12436171 1.13305565 1.24728656 1.05402705
 1.2774795  1.38564561 1.58900479 1.50955516]
ti_comp = [0.50206616 0.49911694 0.52727482 0.52662719 0.51845528 0.53242807
 0.51606118 0.50818478 0.49328332 0.49909203]
ti_coms = [0.10745868 0.11040791 0.08225002 0.08289766 0.09106956 0.07709677
 0.09346367 0.10134006 0.11624152 0.11043281]
t_total = [27.69980698 27.69980698 27.69980698 27.69980698 27.69980698 27.69980698
 27.69980698 27.69980698 27.69980698 27.69980698]
ene_coms = [0.01074587 0.01104079 0.008225   0.00828977 0.00910696 0.00770968
 0.00934637 0.01013401 0.01162415 0.01104328]
ene_comp = [4.53965695e-06 4.27332884e-05 3.92305407e-06 1.63995573e-07
 6.09782542e-05 6.28014948e-06 3.38230065e-07 1.27042518e-05
 5.16500207e-06 3.77327151e-06]
ene_total = [0.4885514  0.50368984 0.37396283 0.37673518 0.41663605 0.35065104
 0.42476024 0.46111644 0.52849337 0.50203249]
optimize_network iter = 1 obj = 4.426628871639538
eta = 0.7265812278175124
freqs = [26248069.02524563 55530468.00844563 24596449.34254611  8539903.21120951
 61730229.85113925 28680034.96529516 10944107.20306952 36839959.54828674
 27563520.58857054 24728008.34302732]
eta_min = 0.7265812278175666	eta_max = 0.7265812278174688
af = 0.005469588632344422	bf = 1.2085764170604951	zeta = 0.006016547495578864	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01074587 0.01104079 0.008225   0.00828977 0.00910696 0.00770968
 0.00934637 0.01013401 0.01162415 0.01104328]
ene_comp = [1.35462979e-06 1.27515770e-05 1.17063601e-06 4.89361400e-08
 1.81958594e-05 1.87399128e-06 1.00927565e-07 3.79093796e-06
 1.54123224e-06 1.12594102e-06]
ene_total = [1.46893503 1.51080292 1.12435684 1.13305545 1.24722977 1.05401811
 1.27747922 1.38563851 1.58900479 1.50955433]
ti_comp = [0.50206616 0.49911694 0.52727482 0.52662719 0.51845528 0.53242807
 0.51606118 0.50818478 0.49328332 0.49909203]
ti_coms = [0.10745868 0.11040791 0.08225002 0.08289766 0.09106956 0.07709677
 0.09346367 0.10134006 0.11624152 0.11043281]
t_total = [27.69980698 27.69980698 27.69980698 27.69980698 27.69980698 27.69980698
 27.69980698 27.69980698 27.69980698 27.69980698]
ene_coms = [0.01074587 0.01104079 0.008225   0.00828977 0.00910696 0.00770968
 0.00934637 0.01013401 0.01162415 0.01104328]
ene_comp = [4.53965695e-06 4.27332884e-05 3.92305407e-06 1.63995573e-07
 6.09782542e-05 6.28014948e-06 3.38230065e-07 1.27042518e-05
 5.16500207e-06 3.77327151e-06]
ene_total = [0.4885514  0.50368984 0.37396283 0.37673518 0.41663605 0.35065104
 0.42476024 0.46111644 0.52849337 0.50203249]
optimize_network iter = 2 obj = 4.426628871638833
eta = 0.7265812278174688
freqs = [26248069.02524548 55530468.00844526 24596449.34254619  8539903.21120954
 61730229.8511393  28680034.96529532 10944107.20306952 36839959.54828663
 27563520.5885703  24728008.34302716]
Done!
ene_coms = [0.01074587 0.01104079 0.008225   0.00828977 0.00910696 0.00770968
 0.00934637 0.01013401 0.01162415 0.01104328]
ene_comp = [4.34045510e-06 4.08581356e-05 3.75090899e-06 1.56799386e-07
 5.83025055e-05 6.00457416e-06 3.23388403e-07 1.21467844e-05
 4.93835984e-06 3.60769894e-06]
ene_total = [0.01075021 0.01108165 0.00822875 0.00828992 0.00916526 0.00771568
 0.00934669 0.01014615 0.01162909 0.01104689]
At round 46 energy consumption: 0.09740029503265318
At round 46 eta: 0.7265812278174688
At round 46 a_n: 12.425493895665781
At round 46 local rounds: 10.458942314102252
At round 46 global rounds: 45.44491878330383
gradient difference: 0.5539610981941223
train() client id: f_00000-0-0 loss: 1.075164  [   32/  126]
train() client id: f_00000-0-1 loss: 1.123269  [   64/  126]
train() client id: f_00000-0-2 loss: 0.942300  [   96/  126]
train() client id: f_00000-1-0 loss: 0.993426  [   32/  126]
train() client id: f_00000-1-1 loss: 1.011021  [   64/  126]
train() client id: f_00000-1-2 loss: 0.724514  [   96/  126]
train() client id: f_00000-2-0 loss: 0.796816  [   32/  126]
train() client id: f_00000-2-1 loss: 0.870667  [   64/  126]
train() client id: f_00000-2-2 loss: 0.993413  [   96/  126]
train() client id: f_00000-3-0 loss: 0.868003  [   32/  126]
train() client id: f_00000-3-1 loss: 0.881154  [   64/  126]
train() client id: f_00000-3-2 loss: 0.871499  [   96/  126]
train() client id: f_00000-4-0 loss: 0.840841  [   32/  126]
train() client id: f_00000-4-1 loss: 0.999069  [   64/  126]
train() client id: f_00000-4-2 loss: 0.916874  [   96/  126]
train() client id: f_00000-5-0 loss: 0.671887  [   32/  126]
train() client id: f_00000-5-1 loss: 0.832239  [   64/  126]
train() client id: f_00000-5-2 loss: 0.982718  [   96/  126]
train() client id: f_00000-6-0 loss: 0.768429  [   32/  126]
train() client id: f_00000-6-1 loss: 0.765153  [   64/  126]
train() client id: f_00000-6-2 loss: 0.803674  [   96/  126]
train() client id: f_00000-7-0 loss: 0.669154  [   32/  126]
train() client id: f_00000-7-1 loss: 0.886117  [   64/  126]
train() client id: f_00000-7-2 loss: 0.798839  [   96/  126]
train() client id: f_00000-8-0 loss: 0.954267  [   32/  126]
train() client id: f_00000-8-1 loss: 0.690183  [   64/  126]
train() client id: f_00000-8-2 loss: 0.842449  [   96/  126]
train() client id: f_00000-9-0 loss: 0.963491  [   32/  126]
train() client id: f_00000-9-1 loss: 0.832207  [   64/  126]
train() client id: f_00000-9-2 loss: 0.736635  [   96/  126]
train() client id: f_00001-0-0 loss: 0.274617  [   32/  265]
train() client id: f_00001-0-1 loss: 0.342514  [   64/  265]
train() client id: f_00001-0-2 loss: 0.436150  [   96/  265]
train() client id: f_00001-0-3 loss: 0.391544  [  128/  265]
train() client id: f_00001-0-4 loss: 0.463584  [  160/  265]
train() client id: f_00001-0-5 loss: 0.294264  [  192/  265]
train() client id: f_00001-0-6 loss: 0.417440  [  224/  265]
train() client id: f_00001-0-7 loss: 0.336152  [  256/  265]
train() client id: f_00001-1-0 loss: 0.337623  [   32/  265]
train() client id: f_00001-1-1 loss: 0.394744  [   64/  265]
train() client id: f_00001-1-2 loss: 0.342738  [   96/  265]
train() client id: f_00001-1-3 loss: 0.333541  [  128/  265]
train() client id: f_00001-1-4 loss: 0.394705  [  160/  265]
train() client id: f_00001-1-5 loss: 0.360848  [  192/  265]
train() client id: f_00001-1-6 loss: 0.339824  [  224/  265]
train() client id: f_00001-1-7 loss: 0.309503  [  256/  265]
train() client id: f_00001-2-0 loss: 0.398343  [   32/  265]
train() client id: f_00001-2-1 loss: 0.256433  [   64/  265]
train() client id: f_00001-2-2 loss: 0.263829  [   96/  265]
train() client id: f_00001-2-3 loss: 0.257557  [  128/  265]
train() client id: f_00001-2-4 loss: 0.529296  [  160/  265]
train() client id: f_00001-2-5 loss: 0.345782  [  192/  265]
train() client id: f_00001-2-6 loss: 0.394324  [  224/  265]
train() client id: f_00001-2-7 loss: 0.346937  [  256/  265]
train() client id: f_00001-3-0 loss: 0.282858  [   32/  265]
train() client id: f_00001-3-1 loss: 0.334676  [   64/  265]
train() client id: f_00001-3-2 loss: 0.274418  [   96/  265]
train() client id: f_00001-3-3 loss: 0.376173  [  128/  265]
train() client id: f_00001-3-4 loss: 0.386756  [  160/  265]
train() client id: f_00001-3-5 loss: 0.344536  [  192/  265]
train() client id: f_00001-3-6 loss: 0.381229  [  224/  265]
train() client id: f_00001-3-7 loss: 0.424701  [  256/  265]
train() client id: f_00001-4-0 loss: 0.370073  [   32/  265]
train() client id: f_00001-4-1 loss: 0.297143  [   64/  265]
train() client id: f_00001-4-2 loss: 0.278338  [   96/  265]
train() client id: f_00001-4-3 loss: 0.447416  [  128/  265]
train() client id: f_00001-4-4 loss: 0.228204  [  160/  265]
train() client id: f_00001-4-5 loss: 0.320851  [  192/  265]
train() client id: f_00001-4-6 loss: 0.389226  [  224/  265]
train() client id: f_00001-4-7 loss: 0.393438  [  256/  265]
train() client id: f_00001-5-0 loss: 0.261620  [   32/  265]
train() client id: f_00001-5-1 loss: 0.345711  [   64/  265]
train() client id: f_00001-5-2 loss: 0.293301  [   96/  265]
train() client id: f_00001-5-3 loss: 0.410009  [  128/  265]
train() client id: f_00001-5-4 loss: 0.346145  [  160/  265]
train() client id: f_00001-5-5 loss: 0.299453  [  192/  265]
train() client id: f_00001-5-6 loss: 0.334838  [  224/  265]
train() client id: f_00001-5-7 loss: 0.380799  [  256/  265]
train() client id: f_00001-6-0 loss: 0.285324  [   32/  265]
train() client id: f_00001-6-1 loss: 0.257176  [   64/  265]
train() client id: f_00001-6-2 loss: 0.323106  [   96/  265]
train() client id: f_00001-6-3 loss: 0.313855  [  128/  265]
train() client id: f_00001-6-4 loss: 0.258521  [  160/  265]
train() client id: f_00001-6-5 loss: 0.273723  [  192/  265]
train() client id: f_00001-6-6 loss: 0.634906  [  224/  265]
train() client id: f_00001-6-7 loss: 0.373524  [  256/  265]
train() client id: f_00001-7-0 loss: 0.513962  [   32/  265]
train() client id: f_00001-7-1 loss: 0.490066  [   64/  265]
train() client id: f_00001-7-2 loss: 0.294750  [   96/  265]
train() client id: f_00001-7-3 loss: 0.431735  [  128/  265]
train() client id: f_00001-7-4 loss: 0.239485  [  160/  265]
train() client id: f_00001-7-5 loss: 0.278531  [  192/  265]
train() client id: f_00001-7-6 loss: 0.229984  [  224/  265]
train() client id: f_00001-7-7 loss: 0.237095  [  256/  265]
train() client id: f_00001-8-0 loss: 0.276726  [   32/  265]
train() client id: f_00001-8-1 loss: 0.448286  [   64/  265]
train() client id: f_00001-8-2 loss: 0.250665  [   96/  265]
train() client id: f_00001-8-3 loss: 0.275743  [  128/  265]
train() client id: f_00001-8-4 loss: 0.333788  [  160/  265]
train() client id: f_00001-8-5 loss: 0.507351  [  192/  265]
train() client id: f_00001-8-6 loss: 0.258407  [  224/  265]
train() client id: f_00001-8-7 loss: 0.359536  [  256/  265]
train() client id: f_00001-9-0 loss: 0.302805  [   32/  265]
train() client id: f_00001-9-1 loss: 0.279167  [   64/  265]
train() client id: f_00001-9-2 loss: 0.415151  [   96/  265]
train() client id: f_00001-9-3 loss: 0.325737  [  128/  265]
train() client id: f_00001-9-4 loss: 0.384327  [  160/  265]
train() client id: f_00001-9-5 loss: 0.326184  [  192/  265]
train() client id: f_00001-9-6 loss: 0.302560  [  224/  265]
train() client id: f_00001-9-7 loss: 0.360565  [  256/  265]
train() client id: f_00002-0-0 loss: 1.110410  [   32/  124]
train() client id: f_00002-0-1 loss: 1.118618  [   64/  124]
train() client id: f_00002-0-2 loss: 1.332093  [   96/  124]
train() client id: f_00002-1-0 loss: 0.971162  [   32/  124]
train() client id: f_00002-1-1 loss: 1.270304  [   64/  124]
train() client id: f_00002-1-2 loss: 1.091511  [   96/  124]
train() client id: f_00002-2-0 loss: 1.052571  [   32/  124]
train() client id: f_00002-2-1 loss: 0.891796  [   64/  124]
train() client id: f_00002-2-2 loss: 1.306959  [   96/  124]
train() client id: f_00002-3-0 loss: 1.136189  [   32/  124]
train() client id: f_00002-3-1 loss: 1.073927  [   64/  124]
train() client id: f_00002-3-2 loss: 1.021777  [   96/  124]
train() client id: f_00002-4-0 loss: 1.157120  [   32/  124]
train() client id: f_00002-4-1 loss: 1.035323  [   64/  124]
train() client id: f_00002-4-2 loss: 0.974496  [   96/  124]
train() client id: f_00002-5-0 loss: 1.011522  [   32/  124]
train() client id: f_00002-5-1 loss: 1.222399  [   64/  124]
train() client id: f_00002-5-2 loss: 0.971850  [   96/  124]
train() client id: f_00002-6-0 loss: 1.077908  [   32/  124]
train() client id: f_00002-6-1 loss: 0.962712  [   64/  124]
train() client id: f_00002-6-2 loss: 1.067729  [   96/  124]
train() client id: f_00002-7-0 loss: 1.100185  [   32/  124]
train() client id: f_00002-7-1 loss: 0.988685  [   64/  124]
train() client id: f_00002-7-2 loss: 1.171406  [   96/  124]
train() client id: f_00002-8-0 loss: 1.222898  [   32/  124]
train() client id: f_00002-8-1 loss: 1.115949  [   64/  124]
train() client id: f_00002-8-2 loss: 1.122357  [   96/  124]
train() client id: f_00002-9-0 loss: 1.039860  [   32/  124]
train() client id: f_00002-9-1 loss: 1.161483  [   64/  124]
train() client id: f_00002-9-2 loss: 1.158669  [   96/  124]
train() client id: f_00003-0-0 loss: 0.864313  [   32/   43]
train() client id: f_00003-1-0 loss: 0.576064  [   32/   43]
train() client id: f_00003-2-0 loss: 0.786478  [   32/   43]
train() client id: f_00003-3-0 loss: 0.694751  [   32/   43]
train() client id: f_00003-4-0 loss: 0.635412  [   32/   43]
train() client id: f_00003-5-0 loss: 0.612057  [   32/   43]
train() client id: f_00003-6-0 loss: 0.539744  [   32/   43]
train() client id: f_00003-7-0 loss: 0.707226  [   32/   43]
train() client id: f_00003-8-0 loss: 0.574030  [   32/   43]
train() client id: f_00003-9-0 loss: 0.722564  [   32/   43]
train() client id: f_00004-0-0 loss: 0.764108  [   32/  306]
train() client id: f_00004-0-1 loss: 0.864821  [   64/  306]
train() client id: f_00004-0-2 loss: 0.797629  [   96/  306]
train() client id: f_00004-0-3 loss: 0.875186  [  128/  306]
train() client id: f_00004-0-4 loss: 0.732543  [  160/  306]
train() client id: f_00004-0-5 loss: 0.732238  [  192/  306]
train() client id: f_00004-0-6 loss: 0.922750  [  224/  306]
train() client id: f_00004-0-7 loss: 0.811884  [  256/  306]
train() client id: f_00004-0-8 loss: 0.867428  [  288/  306]
train() client id: f_00004-1-0 loss: 0.726817  [   32/  306]
train() client id: f_00004-1-1 loss: 0.850114  [   64/  306]
train() client id: f_00004-1-2 loss: 0.787187  [   96/  306]
train() client id: f_00004-1-3 loss: 0.890383  [  128/  306]
train() client id: f_00004-1-4 loss: 0.807450  [  160/  306]
train() client id: f_00004-1-5 loss: 0.867536  [  192/  306]
train() client id: f_00004-1-6 loss: 0.813775  [  224/  306]
train() client id: f_00004-1-7 loss: 0.761846  [  256/  306]
train() client id: f_00004-1-8 loss: 0.827389  [  288/  306]
train() client id: f_00004-2-0 loss: 0.836328  [   32/  306]
train() client id: f_00004-2-1 loss: 0.803691  [   64/  306]
train() client id: f_00004-2-2 loss: 0.804878  [   96/  306]
train() client id: f_00004-2-3 loss: 0.849948  [  128/  306]
train() client id: f_00004-2-4 loss: 0.862020  [  160/  306]
train() client id: f_00004-2-5 loss: 0.730897  [  192/  306]
train() client id: f_00004-2-6 loss: 0.838225  [  224/  306]
train() client id: f_00004-2-7 loss: 0.721311  [  256/  306]
train() client id: f_00004-2-8 loss: 0.787210  [  288/  306]
train() client id: f_00004-3-0 loss: 0.876326  [   32/  306]
train() client id: f_00004-3-1 loss: 0.755557  [   64/  306]
train() client id: f_00004-3-2 loss: 0.788438  [   96/  306]
train() client id: f_00004-3-3 loss: 0.846796  [  128/  306]
train() client id: f_00004-3-4 loss: 0.733437  [  160/  306]
train() client id: f_00004-3-5 loss: 0.831222  [  192/  306]
train() client id: f_00004-3-6 loss: 0.799145  [  224/  306]
train() client id: f_00004-3-7 loss: 0.784887  [  256/  306]
train() client id: f_00004-3-8 loss: 0.877382  [  288/  306]
train() client id: f_00004-4-0 loss: 0.792993  [   32/  306]
train() client id: f_00004-4-1 loss: 0.839766  [   64/  306]
train() client id: f_00004-4-2 loss: 0.689655  [   96/  306]
train() client id: f_00004-4-3 loss: 0.937025  [  128/  306]
train() client id: f_00004-4-4 loss: 0.620596  [  160/  306]
train() client id: f_00004-4-5 loss: 0.974344  [  192/  306]
train() client id: f_00004-4-6 loss: 0.756951  [  224/  306]
train() client id: f_00004-4-7 loss: 0.712561  [  256/  306]
train() client id: f_00004-4-8 loss: 0.867516  [  288/  306]
train() client id: f_00004-5-0 loss: 0.798433  [   32/  306]
train() client id: f_00004-5-1 loss: 0.770552  [   64/  306]
train() client id: f_00004-5-2 loss: 0.712316  [   96/  306]
train() client id: f_00004-5-3 loss: 0.829031  [  128/  306]
train() client id: f_00004-5-4 loss: 0.789465  [  160/  306]
train() client id: f_00004-5-5 loss: 0.850117  [  192/  306]
train() client id: f_00004-5-6 loss: 0.785028  [  224/  306]
train() client id: f_00004-5-7 loss: 0.735414  [  256/  306]
train() client id: f_00004-5-8 loss: 0.911452  [  288/  306]
train() client id: f_00004-6-0 loss: 0.774773  [   32/  306]
train() client id: f_00004-6-1 loss: 0.834097  [   64/  306]
train() client id: f_00004-6-2 loss: 0.695932  [   96/  306]
train() client id: f_00004-6-3 loss: 0.685978  [  128/  306]
train() client id: f_00004-6-4 loss: 0.829289  [  160/  306]
train() client id: f_00004-6-5 loss: 0.837316  [  192/  306]
train() client id: f_00004-6-6 loss: 0.894823  [  224/  306]
train() client id: f_00004-6-7 loss: 0.911125  [  256/  306]
train() client id: f_00004-6-8 loss: 0.706703  [  288/  306]
train() client id: f_00004-7-0 loss: 0.723097  [   32/  306]
train() client id: f_00004-7-1 loss: 0.816384  [   64/  306]
train() client id: f_00004-7-2 loss: 0.943360  [   96/  306]
train() client id: f_00004-7-3 loss: 0.816232  [  128/  306]
train() client id: f_00004-7-4 loss: 0.716458  [  160/  306]
train() client id: f_00004-7-5 loss: 0.677657  [  192/  306]
train() client id: f_00004-7-6 loss: 0.816780  [  224/  306]
train() client id: f_00004-7-7 loss: 0.883417  [  256/  306]
train() client id: f_00004-7-8 loss: 0.720808  [  288/  306]
train() client id: f_00004-8-0 loss: 0.855561  [   32/  306]
train() client id: f_00004-8-1 loss: 0.816982  [   64/  306]
train() client id: f_00004-8-2 loss: 0.809943  [   96/  306]
train() client id: f_00004-8-3 loss: 0.718067  [  128/  306]
train() client id: f_00004-8-4 loss: 0.675167  [  160/  306]
train() client id: f_00004-8-5 loss: 0.905089  [  192/  306]
train() client id: f_00004-8-6 loss: 0.902227  [  224/  306]
train() client id: f_00004-8-7 loss: 0.828941  [  256/  306]
train() client id: f_00004-8-8 loss: 0.755337  [  288/  306]
train() client id: f_00004-9-0 loss: 0.741159  [   32/  306]
train() client id: f_00004-9-1 loss: 0.773190  [   64/  306]
train() client id: f_00004-9-2 loss: 0.851154  [   96/  306]
train() client id: f_00004-9-3 loss: 0.819541  [  128/  306]
train() client id: f_00004-9-4 loss: 0.820725  [  160/  306]
train() client id: f_00004-9-5 loss: 0.668168  [  192/  306]
train() client id: f_00004-9-6 loss: 0.789164  [  224/  306]
train() client id: f_00004-9-7 loss: 0.825330  [  256/  306]
train() client id: f_00004-9-8 loss: 0.822025  [  288/  306]
train() client id: f_00005-0-0 loss: 0.311477  [   32/  146]
train() client id: f_00005-0-1 loss: 0.608284  [   64/  146]
train() client id: f_00005-0-2 loss: 0.707202  [   96/  146]
train() client id: f_00005-0-3 loss: 0.502506  [  128/  146]
train() client id: f_00005-1-0 loss: 0.626233  [   32/  146]
train() client id: f_00005-1-1 loss: 0.485089  [   64/  146]
train() client id: f_00005-1-2 loss: 0.590022  [   96/  146]
train() client id: f_00005-1-3 loss: 0.542435  [  128/  146]
train() client id: f_00005-2-0 loss: 0.493995  [   32/  146]
train() client id: f_00005-2-1 loss: 0.531065  [   64/  146]
train() client id: f_00005-2-2 loss: 0.443458  [   96/  146]
train() client id: f_00005-2-3 loss: 0.626721  [  128/  146]
train() client id: f_00005-3-0 loss: 0.725170  [   32/  146]
train() client id: f_00005-3-1 loss: 0.395853  [   64/  146]
train() client id: f_00005-3-2 loss: 0.508865  [   96/  146]
train() client id: f_00005-3-3 loss: 0.613416  [  128/  146]
train() client id: f_00005-4-0 loss: 0.638950  [   32/  146]
train() client id: f_00005-4-1 loss: 0.464983  [   64/  146]
train() client id: f_00005-4-2 loss: 0.603168  [   96/  146]
train() client id: f_00005-4-3 loss: 0.555229  [  128/  146]
train() client id: f_00005-5-0 loss: 0.620882  [   32/  146]
train() client id: f_00005-5-1 loss: 0.662481  [   64/  146]
train() client id: f_00005-5-2 loss: 0.654347  [   96/  146]
train() client id: f_00005-5-3 loss: 0.296360  [  128/  146]
train() client id: f_00005-6-0 loss: 0.474467  [   32/  146]
train() client id: f_00005-6-1 loss: 0.525002  [   64/  146]
train() client id: f_00005-6-2 loss: 0.391280  [   96/  146]
train() client id: f_00005-6-3 loss: 0.774690  [  128/  146]
train() client id: f_00005-7-0 loss: 0.389886  [   32/  146]
train() client id: f_00005-7-1 loss: 0.857324  [   64/  146]
train() client id: f_00005-7-2 loss: 0.383209  [   96/  146]
train() client id: f_00005-7-3 loss: 0.484493  [  128/  146]
train() client id: f_00005-8-0 loss: 0.756268  [   32/  146]
train() client id: f_00005-8-1 loss: 0.363692  [   64/  146]
train() client id: f_00005-8-2 loss: 0.570280  [   96/  146]
train() client id: f_00005-8-3 loss: 0.586947  [  128/  146]
train() client id: f_00005-9-0 loss: 0.849895  [   32/  146]
train() client id: f_00005-9-1 loss: 0.426420  [   64/  146]
train() client id: f_00005-9-2 loss: 0.567420  [   96/  146]
train() client id: f_00005-9-3 loss: 0.346532  [  128/  146]
train() client id: f_00006-0-0 loss: 0.600177  [   32/   54]
train() client id: f_00006-1-0 loss: 0.532214  [   32/   54]
train() client id: f_00006-2-0 loss: 0.568757  [   32/   54]
train() client id: f_00006-3-0 loss: 0.621148  [   32/   54]
train() client id: f_00006-4-0 loss: 0.552671  [   32/   54]
train() client id: f_00006-5-0 loss: 0.573263  [   32/   54]
train() client id: f_00006-6-0 loss: 0.500534  [   32/   54]
train() client id: f_00006-7-0 loss: 0.548357  [   32/   54]
train() client id: f_00006-8-0 loss: 0.569287  [   32/   54]
train() client id: f_00006-9-0 loss: 0.538781  [   32/   54]
train() client id: f_00007-0-0 loss: 0.571202  [   32/  179]
train() client id: f_00007-0-1 loss: 0.475788  [   64/  179]
train() client id: f_00007-0-2 loss: 0.409573  [   96/  179]
train() client id: f_00007-0-3 loss: 0.485869  [  128/  179]
train() client id: f_00007-0-4 loss: 0.265656  [  160/  179]
train() client id: f_00007-1-0 loss: 0.367350  [   32/  179]
train() client id: f_00007-1-1 loss: 0.291328  [   64/  179]
train() client id: f_00007-1-2 loss: 0.552637  [   96/  179]
train() client id: f_00007-1-3 loss: 0.240835  [  128/  179]
train() client id: f_00007-1-4 loss: 0.590251  [  160/  179]
train() client id: f_00007-2-0 loss: 0.366416  [   32/  179]
train() client id: f_00007-2-1 loss: 0.386364  [   64/  179]
train() client id: f_00007-2-2 loss: 0.457895  [   96/  179]
train() client id: f_00007-2-3 loss: 0.262683  [  128/  179]
train() client id: f_00007-2-4 loss: 0.433194  [  160/  179]
train() client id: f_00007-3-0 loss: 0.267862  [   32/  179]
train() client id: f_00007-3-1 loss: 0.427145  [   64/  179]
train() client id: f_00007-3-2 loss: 0.331055  [   96/  179]
train() client id: f_00007-3-3 loss: 0.380729  [  128/  179]
train() client id: f_00007-3-4 loss: 0.328767  [  160/  179]
train() client id: f_00007-4-0 loss: 0.399003  [   32/  179]
train() client id: f_00007-4-1 loss: 0.477794  [   64/  179]
train() client id: f_00007-4-2 loss: 0.245405  [   96/  179]
train() client id: f_00007-4-3 loss: 0.427276  [  128/  179]
train() client id: f_00007-4-4 loss: 0.350957  [  160/  179]
train() client id: f_00007-5-0 loss: 0.355022  [   32/  179]
train() client id: f_00007-5-1 loss: 0.274941  [   64/  179]
train() client id: f_00007-5-2 loss: 0.406831  [   96/  179]
train() client id: f_00007-5-3 loss: 0.460399  [  128/  179]
train() client id: f_00007-5-4 loss: 0.460281  [  160/  179]
train() client id: f_00007-6-0 loss: 0.465215  [   32/  179]
train() client id: f_00007-6-1 loss: 0.473668  [   64/  179]
train() client id: f_00007-6-2 loss: 0.398948  [   96/  179]
train() client id: f_00007-6-3 loss: 0.396223  [  128/  179]
train() client id: f_00007-6-4 loss: 0.229124  [  160/  179]
train() client id: f_00007-7-0 loss: 0.362101  [   32/  179]
train() client id: f_00007-7-1 loss: 0.375313  [   64/  179]
train() client id: f_00007-7-2 loss: 0.212964  [   96/  179]
train() client id: f_00007-7-3 loss: 0.589794  [  128/  179]
train() client id: f_00007-7-4 loss: 0.374902  [  160/  179]
train() client id: f_00007-8-0 loss: 0.581003  [   32/  179]
train() client id: f_00007-8-1 loss: 0.486607  [   64/  179]
train() client id: f_00007-8-2 loss: 0.316484  [   96/  179]
train() client id: f_00007-8-3 loss: 0.218019  [  128/  179]
train() client id: f_00007-8-4 loss: 0.321524  [  160/  179]
train() client id: f_00007-9-0 loss: 0.212911  [   32/  179]
train() client id: f_00007-9-1 loss: 0.310661  [   64/  179]
train() client id: f_00007-9-2 loss: 0.346324  [   96/  179]
train() client id: f_00007-9-3 loss: 0.486496  [  128/  179]
train() client id: f_00007-9-4 loss: 0.500955  [  160/  179]
train() client id: f_00008-0-0 loss: 0.715789  [   32/  130]
train() client id: f_00008-0-1 loss: 0.861924  [   64/  130]
train() client id: f_00008-0-2 loss: 0.734956  [   96/  130]
train() client id: f_00008-0-3 loss: 0.767335  [  128/  130]
train() client id: f_00008-1-0 loss: 0.726619  [   32/  130]
train() client id: f_00008-1-1 loss: 0.758143  [   64/  130]
train() client id: f_00008-1-2 loss: 0.778269  [   96/  130]
train() client id: f_00008-1-3 loss: 0.821220  [  128/  130]
train() client id: f_00008-2-0 loss: 0.744493  [   32/  130]
train() client id: f_00008-2-1 loss: 0.807034  [   64/  130]
train() client id: f_00008-2-2 loss: 0.782622  [   96/  130]
train() client id: f_00008-2-3 loss: 0.781143  [  128/  130]
train() client id: f_00008-3-0 loss: 0.802161  [   32/  130]
train() client id: f_00008-3-1 loss: 0.814871  [   64/  130]
train() client id: f_00008-3-2 loss: 0.707572  [   96/  130]
train() client id: f_00008-3-3 loss: 0.784503  [  128/  130]
train() client id: f_00008-4-0 loss: 0.689640  [   32/  130]
train() client id: f_00008-4-1 loss: 0.782226  [   64/  130]
train() client id: f_00008-4-2 loss: 0.757931  [   96/  130]
train() client id: f_00008-4-3 loss: 0.882721  [  128/  130]
train() client id: f_00008-5-0 loss: 0.665769  [   32/  130]
train() client id: f_00008-5-1 loss: 0.838533  [   64/  130]
train() client id: f_00008-5-2 loss: 0.781880  [   96/  130]
train() client id: f_00008-5-3 loss: 0.835842  [  128/  130]
train() client id: f_00008-6-0 loss: 0.764701  [   32/  130]
train() client id: f_00008-6-1 loss: 0.787992  [   64/  130]
train() client id: f_00008-6-2 loss: 0.707104  [   96/  130]
train() client id: f_00008-6-3 loss: 0.809446  [  128/  130]
train() client id: f_00008-7-0 loss: 0.799889  [   32/  130]
train() client id: f_00008-7-1 loss: 0.707962  [   64/  130]
train() client id: f_00008-7-2 loss: 0.742868  [   96/  130]
train() client id: f_00008-7-3 loss: 0.851279  [  128/  130]
train() client id: f_00008-8-0 loss: 0.777174  [   32/  130]
train() client id: f_00008-8-1 loss: 0.889009  [   64/  130]
train() client id: f_00008-8-2 loss: 0.729377  [   96/  130]
train() client id: f_00008-8-3 loss: 0.715428  [  128/  130]
train() client id: f_00008-9-0 loss: 0.675299  [   32/  130]
train() client id: f_00008-9-1 loss: 0.870868  [   64/  130]
train() client id: f_00008-9-2 loss: 0.702078  [   96/  130]
train() client id: f_00008-9-3 loss: 0.869370  [  128/  130]
train() client id: f_00009-0-0 loss: 1.093762  [   32/  118]
train() client id: f_00009-0-1 loss: 1.226341  [   64/  118]
train() client id: f_00009-0-2 loss: 0.948779  [   96/  118]
train() client id: f_00009-1-0 loss: 1.013202  [   32/  118]
train() client id: f_00009-1-1 loss: 1.173112  [   64/  118]
train() client id: f_00009-1-2 loss: 1.051630  [   96/  118]
train() client id: f_00009-2-0 loss: 1.165104  [   32/  118]
train() client id: f_00009-2-1 loss: 0.948235  [   64/  118]
train() client id: f_00009-2-2 loss: 0.989510  [   96/  118]
train() client id: f_00009-3-0 loss: 1.115161  [   32/  118]
train() client id: f_00009-3-1 loss: 0.992793  [   64/  118]
train() client id: f_00009-3-2 loss: 0.955373  [   96/  118]
train() client id: f_00009-4-0 loss: 1.039648  [   32/  118]
train() client id: f_00009-4-1 loss: 1.128377  [   64/  118]
train() client id: f_00009-4-2 loss: 0.986501  [   96/  118]
train() client id: f_00009-5-0 loss: 1.035295  [   32/  118]
train() client id: f_00009-5-1 loss: 0.773072  [   64/  118]
train() client id: f_00009-5-2 loss: 1.016567  [   96/  118]
train() client id: f_00009-6-0 loss: 1.128802  [   32/  118]
train() client id: f_00009-6-1 loss: 0.944135  [   64/  118]
train() client id: f_00009-6-2 loss: 0.984532  [   96/  118]
train() client id: f_00009-7-0 loss: 0.897466  [   32/  118]
train() client id: f_00009-7-1 loss: 0.841931  [   64/  118]
train() client id: f_00009-7-2 loss: 0.874048  [   96/  118]
train() client id: f_00009-8-0 loss: 0.894989  [   32/  118]
train() client id: f_00009-8-1 loss: 0.971126  [   64/  118]
train() client id: f_00009-8-2 loss: 1.051949  [   96/  118]
train() client id: f_00009-9-0 loss: 0.983745  [   32/  118]
train() client id: f_00009-9-1 loss: 0.815074  [   64/  118]
train() client id: f_00009-9-2 loss: 0.893431  [   96/  118]
At round 46 accuracy: 0.649867374005305
At round 46 training accuracy: 0.5902079141515761
At round 46 training loss: 0.8231281069292035
update_location
xs = -4.528292 131.001589 140.045120 -140.943528 29.896481 -65.217951 -182.215960 203.375741 -1.680116 124.695607 
ys = 217.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -147.154970 4.001482 
xs mean: 23.44286915941287
ys mean: 9.371751218646876
dists_uav = 239.509970 165.539725 172.088290 174.279509 104.375389 119.420687 207.875975 227.441145 177.925288 159.890607 
uav_gains = -110.432749 -105.488847 -105.920260 -106.062111 -100.464982 -101.927189 -108.183600 -109.516214 -106.295810 -105.106037 
uav_gains_db_mean: -105.93977992708969
dists_bs = 175.710949 345.049871 359.747073 156.212752 269.880390 204.205938 178.271023 425.291855 365.819090 345.047751 
bs_gains = -102.421426 -110.627751 -111.134983 -100.991121 -107.639856 -104.248973 -102.597320 -113.170297 -111.338517 -110.627677 
bs_gains_db_mean: -107.47979222905192
Round 47
-------------------------------
ene_coms = [0.01101154 0.01116318 0.00834344 0.00840821 0.00919976 0.00764737
 0.00950661 0.01034748 0.01175016 0.01116312]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.77238792  9.89075377  4.66620481  1.68542899 11.37597816  5.46751748
  2.10373906  6.71518448  4.92864061  4.47976229]
obj_prev = 56.08559757623574
eta_min = 9.139991267953814e-20	eta_max = 0.9481567099092483
af = 11.79849495615646	bf = 1.1906643888507737	zeta = 12.978344451772108	eta = 0.9090909090909091
af = 11.79849495615646	bf = 1.1906643888507737	zeta = 25.466991709967125	eta = 0.4632857736211864
af = 11.79849495615646	bf = 1.1906643888507737	zeta = 19.13226438151051	eta = 0.6166805309024774
af = 11.79849495615646	bf = 1.1906643888507737	zeta = 17.985274879440684	eta = 0.6560085978804554
af = 11.79849495615646	bf = 1.1906643888507737	zeta = 17.921059011553265	eta = 0.658359249224628
af = 11.79849495615646	bf = 1.1906643888507737	zeta = 17.92083835704796	eta = 0.6583673554265566
eta = 0.6583673554265566
ene_coms = [0.01101154 0.01116318 0.00834344 0.00840821 0.00919976 0.00764737
 0.00950661 0.01034748 0.01175016 0.01116312]
ene_comp = [0.03449172 0.0725421  0.03394423 0.01177098 0.08376559 0.03996659
 0.01478216 0.04900014 0.03558669 0.03230177]
ene_total = [1.60937046 2.96050918 1.49564068 0.71370263 3.28802188 1.68402251
 0.85905141 2.09902115 1.67422149 1.53727697]
ti_comp = [0.67165367 0.67013726 0.6983347  0.697687   0.68977144 0.70529535
 0.68670297 0.67829429 0.66426748 0.67013785]
ti_coms = [0.11011541 0.11163183 0.08343439 0.08408209 0.09199765 0.07647374
 0.09506611 0.10347479 0.11750161 0.11163124]
t_total = [27.64980278 27.64980278 27.64980278 27.64980278 27.64980278 27.64980278
 27.64980278 27.64980278 27.64980278 27.64980278]
ene_coms = [0.01101154 0.01116318 0.00834344 0.00840821 0.00919976 0.00764737
 0.00950661 0.01034748 0.01175016 0.01116312]
ene_comp = [5.68504116e-06 5.31279256e-05 5.01245609e-06 2.09409739e-07
 7.72087701e-05 8.02101441e-06 4.28110807e-07 1.59821623e-05
 6.38347030e-06 4.69062265e-06]
ene_total = [0.38965998 0.39670125 0.29527009 0.29739102 0.3281103  0.2707579
 0.33624732 0.36653748 0.41580836 0.39498603]
optimize_network iter = 0 obj = 3.491469730946691
eta = 0.6583673554265566
freqs = [25676711.68366667 54124807.27240222 24303695.67934918  8435718.53395539
 60719819.88297606 28333230.25152369 10763142.39432637 36120114.90571564
 26786416.38244415 24100836.41544166]
eta_min = 0.6583673554265584	eta_max = 0.7318761758177215
af = 0.005109302797271965	bf = 1.1906643888507737	zeta = 0.005620233076999162	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01101154 0.01116318 0.00834344 0.00840821 0.00919976 0.00764737
 0.00950661 0.01034748 0.01175016 0.01116312]
ene_comp = [1.29629758e-06 1.21141782e-05 1.14293539e-06 4.77494061e-08
 1.76050690e-05 1.82894394e-06 9.76174118e-08 3.64423718e-06
 1.45555272e-06 1.06955124e-06]
ene_total = [1.46374296 1.48533583 1.10909862 1.11756179 1.22510302 1.01667411
 1.26355975 1.37579298 1.56193584 1.48386004]
ti_comp = [0.50344105 0.50192464 0.53012207 0.52947437 0.52155881 0.53708272
 0.51849035 0.51008167 0.49605486 0.50192523]
ti_coms = [0.11011541 0.11163183 0.08343439 0.08408209 0.09199765 0.07647374
 0.09506611 0.10347479 0.11750161 0.11163124]
t_total = [27.64980278 27.64980278 27.64980278 27.64980278 27.64980278 27.64980278
 27.64980278 27.64980278 27.64980278 27.64980278]
ene_coms = [0.01101154 0.01116318 0.00834344 0.00840821 0.00919976 0.00764737
 0.00950661 0.01034748 0.01175016 0.01116312]
ene_comp = [4.21392979e-06 3.94396913e-05 3.62231479e-06 1.51421794e-07
 5.62380966e-05 5.76035318e-06 3.12732368e-07 1.17693525e-05
 4.76698249e-06 3.48209719e-06]
ene_total = [0.49642286 0.504844   0.37615869 0.37892112 0.41711998 0.344887
 0.42842768 0.46683753 0.52973353 0.50322092]
optimize_network iter = 1 obj = 4.446573292166787
eta = 0.7318761758177215
freqs = [25581316.35000189 53964521.38822971 23908193.67111469  8300886.55488852
 59967936.74358896 27785143.31310865 10645205.51126698 35868588.27194511
 26786416.38244415 24029456.82445259]
eta_min = 0.7318761758177234	eta_max = 0.7318761758176926
af = 0.0050228326123206175	bf = 1.1906643888507737	zeta = 0.0055251158735526796	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01101154 0.01116318 0.00834344 0.00840821 0.00919976 0.00764737
 0.00950661 0.01034748 0.01175016 0.01116312]
ene_comp = [1.28668334e-06 1.20425343e-05 1.10603934e-06 4.62352031e-08
 1.71717674e-05 1.75886900e-06 9.54898513e-08 3.59365975e-06
 1.45555272e-06 1.06322523e-06]
ene_total = [1.46374168 1.48532631 1.10909371 1.11756159 1.22504543 1.01666479
 1.26355946 1.37578625 1.56193584 1.4838592 ]
ti_comp = [0.50344105 0.50192464 0.53012207 0.52947437 0.52155881 0.53708272
 0.51849035 0.51008167 0.49605486 0.50192523]
ti_coms = [0.11011541 0.11163183 0.08343439 0.08408209 0.09199765 0.07647374
 0.09506611 0.10347479 0.11750161 0.11163124]
t_total = [27.64980278 27.64980278 27.64980278 27.64980278 27.64980278 27.64980278
 27.64980278 27.64980278 27.64980278 27.64980278]
ene_coms = [0.01101154 0.01116318 0.00834344 0.00840821 0.00919976 0.00764737
 0.00950661 0.01034748 0.01175016 0.01116312]
ene_comp = [4.21392979e-06 3.94396913e-05 3.62231479e-06 1.51421794e-07
 5.62380966e-05 5.76035318e-06 3.12732368e-07 1.17693525e-05
 4.76698249e-06 3.48209719e-06]
ene_total = [0.49642286 0.504844   0.37615869 0.37892112 0.41711998 0.344887
 0.42842768 0.46683753 0.52973353 0.50322092]
optimize_network iter = 2 obj = 4.44657329216631
eta = 0.7318761758176926
freqs = [25581316.35000177 53964521.38822945 23908193.67111474  8300886.55488854
 59967936.74358895 27785143.31310874 10645205.51126697 35868588.27194501
 26786416.38244397 24029456.82445247]
Done!
ene_coms = [0.01101154 0.01116318 0.00834344 0.00840821 0.00919976 0.00764737
 0.00950661 0.01034748 0.01175016 0.01116312]
ene_comp = [4.12274360e-06 3.85862468e-05 3.54393069e-06 1.48145143e-07
 5.50211476e-05 5.63570358e-06 3.05965081e-07 1.15146728e-05
 4.66382867e-06 3.40674729e-06]
ene_total = [0.01101566 0.01120177 0.00834698 0.00840836 0.00925479 0.00765301
 0.00950692 0.01035899 0.01175482 0.01116653]
At round 47 energy consumption: 0.09866783369455687
At round 47 eta: 0.7318761758176926
At round 47 a_n: 12.082948048696464
At round 47 local rounds: 10.221178413105767
At round 47 global rounds: 45.06480573125354
gradient difference: 0.5040087103843689
train() client id: f_00000-0-0 loss: 1.251905  [   32/  126]
train() client id: f_00000-0-1 loss: 1.184308  [   64/  126]
train() client id: f_00000-0-2 loss: 1.034591  [   96/  126]
train() client id: f_00000-1-0 loss: 1.228130  [   32/  126]
train() client id: f_00000-1-1 loss: 1.106133  [   64/  126]
train() client id: f_00000-1-2 loss: 0.986705  [   96/  126]
train() client id: f_00000-2-0 loss: 1.232030  [   32/  126]
train() client id: f_00000-2-1 loss: 1.071706  [   64/  126]
train() client id: f_00000-2-2 loss: 0.937981  [   96/  126]
train() client id: f_00000-3-0 loss: 0.922528  [   32/  126]
train() client id: f_00000-3-1 loss: 0.941413  [   64/  126]
train() client id: f_00000-3-2 loss: 0.998439  [   96/  126]
train() client id: f_00000-4-0 loss: 1.013387  [   32/  126]
train() client id: f_00000-4-1 loss: 0.876487  [   64/  126]
train() client id: f_00000-4-2 loss: 0.934447  [   96/  126]
train() client id: f_00000-5-0 loss: 1.027428  [   32/  126]
train() client id: f_00000-5-1 loss: 0.943288  [   64/  126]
train() client id: f_00000-5-2 loss: 0.810144  [   96/  126]
train() client id: f_00000-6-0 loss: 0.833767  [   32/  126]
train() client id: f_00000-6-1 loss: 1.001133  [   64/  126]
train() client id: f_00000-6-2 loss: 0.892836  [   96/  126]
train() client id: f_00000-7-0 loss: 0.837323  [   32/  126]
train() client id: f_00000-7-1 loss: 0.864802  [   64/  126]
train() client id: f_00000-7-2 loss: 0.858367  [   96/  126]
train() client id: f_00000-8-0 loss: 0.861852  [   32/  126]
train() client id: f_00000-8-1 loss: 0.863656  [   64/  126]
train() client id: f_00000-8-2 loss: 0.895895  [   96/  126]
train() client id: f_00000-9-0 loss: 0.988156  [   32/  126]
train() client id: f_00000-9-1 loss: 0.820180  [   64/  126]
train() client id: f_00000-9-2 loss: 0.811758  [   96/  126]
train() client id: f_00001-0-0 loss: 0.390933  [   32/  265]
train() client id: f_00001-0-1 loss: 0.466723  [   64/  265]
train() client id: f_00001-0-2 loss: 0.518666  [   96/  265]
train() client id: f_00001-0-3 loss: 0.421083  [  128/  265]
train() client id: f_00001-0-4 loss: 0.458284  [  160/  265]
train() client id: f_00001-0-5 loss: 0.315280  [  192/  265]
train() client id: f_00001-0-6 loss: 0.377728  [  224/  265]
train() client id: f_00001-0-7 loss: 0.319252  [  256/  265]
train() client id: f_00001-1-0 loss: 0.403514  [   32/  265]
train() client id: f_00001-1-1 loss: 0.571533  [   64/  265]
train() client id: f_00001-1-2 loss: 0.347682  [   96/  265]
train() client id: f_00001-1-3 loss: 0.357883  [  128/  265]
train() client id: f_00001-1-4 loss: 0.426398  [  160/  265]
train() client id: f_00001-1-5 loss: 0.331492  [  192/  265]
train() client id: f_00001-1-6 loss: 0.371324  [  224/  265]
train() client id: f_00001-1-7 loss: 0.378172  [  256/  265]
train() client id: f_00001-2-0 loss: 0.403483  [   32/  265]
train() client id: f_00001-2-1 loss: 0.381120  [   64/  265]
train() client id: f_00001-2-2 loss: 0.373997  [   96/  265]
train() client id: f_00001-2-3 loss: 0.345356  [  128/  265]
train() client id: f_00001-2-4 loss: 0.296247  [  160/  265]
train() client id: f_00001-2-5 loss: 0.465916  [  192/  265]
train() client id: f_00001-2-6 loss: 0.412937  [  224/  265]
train() client id: f_00001-2-7 loss: 0.489324  [  256/  265]
train() client id: f_00001-3-0 loss: 0.293795  [   32/  265]
train() client id: f_00001-3-1 loss: 0.378582  [   64/  265]
train() client id: f_00001-3-2 loss: 0.406558  [   96/  265]
train() client id: f_00001-3-3 loss: 0.289294  [  128/  265]
train() client id: f_00001-3-4 loss: 0.656046  [  160/  265]
train() client id: f_00001-3-5 loss: 0.286420  [  192/  265]
train() client id: f_00001-3-6 loss: 0.449756  [  224/  265]
train() client id: f_00001-3-7 loss: 0.374105  [  256/  265]
train() client id: f_00001-4-0 loss: 0.329550  [   32/  265]
train() client id: f_00001-4-1 loss: 0.377938  [   64/  265]
train() client id: f_00001-4-2 loss: 0.349790  [   96/  265]
train() client id: f_00001-4-3 loss: 0.341248  [  128/  265]
train() client id: f_00001-4-4 loss: 0.309062  [  160/  265]
train() client id: f_00001-4-5 loss: 0.457804  [  192/  265]
train() client id: f_00001-4-6 loss: 0.515244  [  224/  265]
train() client id: f_00001-4-7 loss: 0.444853  [  256/  265]
train() client id: f_00001-5-0 loss: 0.284047  [   32/  265]
train() client id: f_00001-5-1 loss: 0.366448  [   64/  265]
train() client id: f_00001-5-2 loss: 0.496522  [   96/  265]
train() client id: f_00001-5-3 loss: 0.358257  [  128/  265]
train() client id: f_00001-5-4 loss: 0.341338  [  160/  265]
train() client id: f_00001-5-5 loss: 0.383168  [  192/  265]
train() client id: f_00001-5-6 loss: 0.431968  [  224/  265]
train() client id: f_00001-5-7 loss: 0.428465  [  256/  265]
train() client id: f_00001-6-0 loss: 0.388137  [   32/  265]
train() client id: f_00001-6-1 loss: 0.381155  [   64/  265]
train() client id: f_00001-6-2 loss: 0.421173  [   96/  265]
train() client id: f_00001-6-3 loss: 0.371131  [  128/  265]
train() client id: f_00001-6-4 loss: 0.310706  [  160/  265]
train() client id: f_00001-6-5 loss: 0.294851  [  192/  265]
train() client id: f_00001-6-6 loss: 0.547377  [  224/  265]
train() client id: f_00001-6-7 loss: 0.372744  [  256/  265]
train() client id: f_00001-7-0 loss: 0.384004  [   32/  265]
train() client id: f_00001-7-1 loss: 0.388456  [   64/  265]
train() client id: f_00001-7-2 loss: 0.312382  [   96/  265]
train() client id: f_00001-7-3 loss: 0.427097  [  128/  265]
train() client id: f_00001-7-4 loss: 0.382347  [  160/  265]
train() client id: f_00001-7-5 loss: 0.392165  [  192/  265]
train() client id: f_00001-7-6 loss: 0.440100  [  224/  265]
train() client id: f_00001-7-7 loss: 0.357195  [  256/  265]
train() client id: f_00001-8-0 loss: 0.551769  [   32/  265]
train() client id: f_00001-8-1 loss: 0.391106  [   64/  265]
train() client id: f_00001-8-2 loss: 0.319473  [   96/  265]
train() client id: f_00001-8-3 loss: 0.422506  [  128/  265]
train() client id: f_00001-8-4 loss: 0.345755  [  160/  265]
train() client id: f_00001-8-5 loss: 0.369307  [  192/  265]
train() client id: f_00001-8-6 loss: 0.305724  [  224/  265]
train() client id: f_00001-8-7 loss: 0.366966  [  256/  265]
train() client id: f_00001-9-0 loss: 0.508725  [   32/  265]
train() client id: f_00001-9-1 loss: 0.343746  [   64/  265]
train() client id: f_00001-9-2 loss: 0.301730  [   96/  265]
train() client id: f_00001-9-3 loss: 0.464628  [  128/  265]
train() client id: f_00001-9-4 loss: 0.406515  [  160/  265]
train() client id: f_00001-9-5 loss: 0.370793  [  192/  265]
train() client id: f_00001-9-6 loss: 0.360924  [  224/  265]
train() client id: f_00001-9-7 loss: 0.309228  [  256/  265]
train() client id: f_00002-0-0 loss: 0.840214  [   32/  124]
train() client id: f_00002-0-1 loss: 1.077605  [   64/  124]
train() client id: f_00002-0-2 loss: 1.206741  [   96/  124]
train() client id: f_00002-1-0 loss: 0.937066  [   32/  124]
train() client id: f_00002-1-1 loss: 1.143035  [   64/  124]
train() client id: f_00002-1-2 loss: 1.075957  [   96/  124]
train() client id: f_00002-2-0 loss: 0.888667  [   32/  124]
train() client id: f_00002-2-1 loss: 0.952484  [   64/  124]
train() client id: f_00002-2-2 loss: 1.063424  [   96/  124]
train() client id: f_00002-3-0 loss: 0.923872  [   32/  124]
train() client id: f_00002-3-1 loss: 1.260335  [   64/  124]
train() client id: f_00002-3-2 loss: 0.852081  [   96/  124]
train() client id: f_00002-4-0 loss: 0.873332  [   32/  124]
train() client id: f_00002-4-1 loss: 1.026198  [   64/  124]
train() client id: f_00002-4-2 loss: 0.920973  [   96/  124]
train() client id: f_00002-5-0 loss: 0.854723  [   32/  124]
train() client id: f_00002-5-1 loss: 1.019416  [   64/  124]
train() client id: f_00002-5-2 loss: 1.011626  [   96/  124]
train() client id: f_00002-6-0 loss: 0.901330  [   32/  124]
train() client id: f_00002-6-1 loss: 0.874137  [   64/  124]
train() client id: f_00002-6-2 loss: 0.912174  [   96/  124]
train() client id: f_00002-7-0 loss: 0.997971  [   32/  124]
train() client id: f_00002-7-1 loss: 0.847400  [   64/  124]
train() client id: f_00002-7-2 loss: 0.998824  [   96/  124]
train() client id: f_00002-8-0 loss: 0.927139  [   32/  124]
train() client id: f_00002-8-1 loss: 0.807250  [   64/  124]
train() client id: f_00002-8-2 loss: 1.072462  [   96/  124]
train() client id: f_00002-9-0 loss: 0.925319  [   32/  124]
train() client id: f_00002-9-1 loss: 0.761820  [   64/  124]
train() client id: f_00002-9-2 loss: 0.820639  [   96/  124]
train() client id: f_00003-0-0 loss: 0.639533  [   32/   43]
train() client id: f_00003-1-0 loss: 0.804895  [   32/   43]
train() client id: f_00003-2-0 loss: 0.593719  [   32/   43]
train() client id: f_00003-3-0 loss: 0.694984  [   32/   43]
train() client id: f_00003-4-0 loss: 0.631658  [   32/   43]
train() client id: f_00003-5-0 loss: 0.649630  [   32/   43]
train() client id: f_00003-6-0 loss: 0.705032  [   32/   43]
train() client id: f_00003-7-0 loss: 0.614236  [   32/   43]
train() client id: f_00003-8-0 loss: 0.743923  [   32/   43]
train() client id: f_00003-9-0 loss: 0.781593  [   32/   43]
train() client id: f_00004-0-0 loss: 0.962279  [   32/  306]
train() client id: f_00004-0-1 loss: 0.888852  [   64/  306]
train() client id: f_00004-0-2 loss: 0.840440  [   96/  306]
train() client id: f_00004-0-3 loss: 0.830563  [  128/  306]
train() client id: f_00004-0-4 loss: 0.794702  [  160/  306]
train() client id: f_00004-0-5 loss: 0.769631  [  192/  306]
train() client id: f_00004-0-6 loss: 0.744190  [  224/  306]
train() client id: f_00004-0-7 loss: 0.791158  [  256/  306]
train() client id: f_00004-0-8 loss: 0.675344  [  288/  306]
train() client id: f_00004-1-0 loss: 0.741943  [   32/  306]
train() client id: f_00004-1-1 loss: 0.780186  [   64/  306]
train() client id: f_00004-1-2 loss: 0.975363  [   96/  306]
train() client id: f_00004-1-3 loss: 0.827732  [  128/  306]
train() client id: f_00004-1-4 loss: 0.832470  [  160/  306]
train() client id: f_00004-1-5 loss: 0.758942  [  192/  306]
train() client id: f_00004-1-6 loss: 0.827879  [  224/  306]
train() client id: f_00004-1-7 loss: 0.845838  [  256/  306]
train() client id: f_00004-1-8 loss: 0.681403  [  288/  306]
train() client id: f_00004-2-0 loss: 0.729913  [   32/  306]
train() client id: f_00004-2-1 loss: 0.732195  [   64/  306]
train() client id: f_00004-2-2 loss: 0.799409  [   96/  306]
train() client id: f_00004-2-3 loss: 0.821942  [  128/  306]
train() client id: f_00004-2-4 loss: 0.803535  [  160/  306]
train() client id: f_00004-2-5 loss: 0.961107  [  192/  306]
train() client id: f_00004-2-6 loss: 0.886053  [  224/  306]
train() client id: f_00004-2-7 loss: 0.683451  [  256/  306]
train() client id: f_00004-2-8 loss: 0.790618  [  288/  306]
train() client id: f_00004-3-0 loss: 0.771877  [   32/  306]
train() client id: f_00004-3-1 loss: 0.875454  [   64/  306]
train() client id: f_00004-3-2 loss: 0.791028  [   96/  306]
train() client id: f_00004-3-3 loss: 0.666388  [  128/  306]
train() client id: f_00004-3-4 loss: 0.950312  [  160/  306]
train() client id: f_00004-3-5 loss: 0.850251  [  192/  306]
train() client id: f_00004-3-6 loss: 0.853359  [  224/  306]
train() client id: f_00004-3-7 loss: 0.718013  [  256/  306]
train() client id: f_00004-3-8 loss: 0.769377  [  288/  306]
train() client id: f_00004-4-0 loss: 0.657384  [   32/  306]
train() client id: f_00004-4-1 loss: 0.857461  [   64/  306]
train() client id: f_00004-4-2 loss: 0.642591  [   96/  306]
train() client id: f_00004-4-3 loss: 0.754427  [  128/  306]
train() client id: f_00004-4-4 loss: 0.882527  [  160/  306]
train() client id: f_00004-4-5 loss: 0.978866  [  192/  306]
train() client id: f_00004-4-6 loss: 0.910901  [  224/  306]
train() client id: f_00004-4-7 loss: 0.798943  [  256/  306]
train() client id: f_00004-4-8 loss: 0.763629  [  288/  306]
train() client id: f_00004-5-0 loss: 0.875955  [   32/  306]
train() client id: f_00004-5-1 loss: 0.804793  [   64/  306]
train() client id: f_00004-5-2 loss: 0.890025  [   96/  306]
train() client id: f_00004-5-3 loss: 0.778574  [  128/  306]
train() client id: f_00004-5-4 loss: 0.754804  [  160/  306]
train() client id: f_00004-5-5 loss: 0.756821  [  192/  306]
train() client id: f_00004-5-6 loss: 0.806488  [  224/  306]
train() client id: f_00004-5-7 loss: 0.766985  [  256/  306]
train() client id: f_00004-5-8 loss: 0.860327  [  288/  306]
train() client id: f_00004-6-0 loss: 0.747752  [   32/  306]
train() client id: f_00004-6-1 loss: 0.888212  [   64/  306]
train() client id: f_00004-6-2 loss: 0.813058  [   96/  306]
train() client id: f_00004-6-3 loss: 0.834467  [  128/  306]
train() client id: f_00004-6-4 loss: 0.848467  [  160/  306]
train() client id: f_00004-6-5 loss: 0.969654  [  192/  306]
train() client id: f_00004-6-6 loss: 0.809496  [  224/  306]
train() client id: f_00004-6-7 loss: 0.767414  [  256/  306]
train() client id: f_00004-6-8 loss: 0.705015  [  288/  306]
train() client id: f_00004-7-0 loss: 0.937487  [   32/  306]
train() client id: f_00004-7-1 loss: 0.929401  [   64/  306]
train() client id: f_00004-7-2 loss: 0.812660  [   96/  306]
train() client id: f_00004-7-3 loss: 0.748590  [  128/  306]
train() client id: f_00004-7-4 loss: 0.617782  [  160/  306]
train() client id: f_00004-7-5 loss: 0.898988  [  192/  306]
train() client id: f_00004-7-6 loss: 0.820714  [  224/  306]
train() client id: f_00004-7-7 loss: 0.767948  [  256/  306]
train() client id: f_00004-7-8 loss: 0.843191  [  288/  306]
train() client id: f_00004-8-0 loss: 0.796531  [   32/  306]
train() client id: f_00004-8-1 loss: 0.893736  [   64/  306]
train() client id: f_00004-8-2 loss: 0.762201  [   96/  306]
train() client id: f_00004-8-3 loss: 0.739887  [  128/  306]
train() client id: f_00004-8-4 loss: 0.856939  [  160/  306]
train() client id: f_00004-8-5 loss: 0.687020  [  192/  306]
train() client id: f_00004-8-6 loss: 0.871190  [  224/  306]
train() client id: f_00004-8-7 loss: 0.854523  [  256/  306]
train() client id: f_00004-8-8 loss: 0.807092  [  288/  306]
train() client id: f_00004-9-0 loss: 0.825766  [   32/  306]
train() client id: f_00004-9-1 loss: 0.818106  [   64/  306]
train() client id: f_00004-9-2 loss: 0.755645  [   96/  306]
train() client id: f_00004-9-3 loss: 0.821946  [  128/  306]
train() client id: f_00004-9-4 loss: 0.882650  [  160/  306]
train() client id: f_00004-9-5 loss: 0.760225  [  192/  306]
train() client id: f_00004-9-6 loss: 0.841747  [  224/  306]
train() client id: f_00004-9-7 loss: 0.776797  [  256/  306]
train() client id: f_00004-9-8 loss: 0.854618  [  288/  306]
train() client id: f_00005-0-0 loss: 0.663455  [   32/  146]
train() client id: f_00005-0-1 loss: 0.999653  [   64/  146]
train() client id: f_00005-0-2 loss: 0.778567  [   96/  146]
train() client id: f_00005-0-3 loss: 0.677713  [  128/  146]
train() client id: f_00005-1-0 loss: 1.076152  [   32/  146]
train() client id: f_00005-1-1 loss: 0.820329  [   64/  146]
train() client id: f_00005-1-2 loss: 0.554023  [   96/  146]
train() client id: f_00005-1-3 loss: 0.596362  [  128/  146]
train() client id: f_00005-2-0 loss: 0.678379  [   32/  146]
train() client id: f_00005-2-1 loss: 0.634865  [   64/  146]
train() client id: f_00005-2-2 loss: 0.568329  [   96/  146]
train() client id: f_00005-2-3 loss: 1.053163  [  128/  146]
train() client id: f_00005-3-0 loss: 0.701604  [   32/  146]
train() client id: f_00005-3-1 loss: 1.031815  [   64/  146]
train() client id: f_00005-3-2 loss: 0.901101  [   96/  146]
train() client id: f_00005-3-3 loss: 0.614970  [  128/  146]
train() client id: f_00005-4-0 loss: 0.649702  [   32/  146]
train() client id: f_00005-4-1 loss: 0.774817  [   64/  146]
train() client id: f_00005-4-2 loss: 0.861076  [   96/  146]
train() client id: f_00005-4-3 loss: 0.872828  [  128/  146]
train() client id: f_00005-5-0 loss: 0.699150  [   32/  146]
train() client id: f_00005-5-1 loss: 0.749963  [   64/  146]
train() client id: f_00005-5-2 loss: 0.803214  [   96/  146]
train() client id: f_00005-5-3 loss: 0.806159  [  128/  146]
train() client id: f_00005-6-0 loss: 0.842874  [   32/  146]
train() client id: f_00005-6-1 loss: 1.001942  [   64/  146]
train() client id: f_00005-6-2 loss: 0.696828  [   96/  146]
train() client id: f_00005-6-3 loss: 0.633528  [  128/  146]
train() client id: f_00005-7-0 loss: 0.789407  [   32/  146]
train() client id: f_00005-7-1 loss: 0.974471  [   64/  146]
train() client id: f_00005-7-2 loss: 0.715291  [   96/  146]
train() client id: f_00005-7-3 loss: 0.887941  [  128/  146]
train() client id: f_00005-8-0 loss: 0.663647  [   32/  146]
train() client id: f_00005-8-1 loss: 0.956280  [   64/  146]
train() client id: f_00005-8-2 loss: 0.698538  [   96/  146]
train() client id: f_00005-8-3 loss: 0.817386  [  128/  146]
train() client id: f_00005-9-0 loss: 0.824678  [   32/  146]
train() client id: f_00005-9-1 loss: 0.880085  [   64/  146]
train() client id: f_00005-9-2 loss: 0.817740  [   96/  146]
train() client id: f_00005-9-3 loss: 0.459092  [  128/  146]
train() client id: f_00006-0-0 loss: 0.525181  [   32/   54]
train() client id: f_00006-1-0 loss: 0.519427  [   32/   54]
train() client id: f_00006-2-0 loss: 0.491829  [   32/   54]
train() client id: f_00006-3-0 loss: 0.539366  [   32/   54]
train() client id: f_00006-4-0 loss: 0.467134  [   32/   54]
train() client id: f_00006-5-0 loss: 0.570786  [   32/   54]
train() client id: f_00006-6-0 loss: 0.559588  [   32/   54]
train() client id: f_00006-7-0 loss: 0.582924  [   32/   54]
train() client id: f_00006-8-0 loss: 0.487065  [   32/   54]
train() client id: f_00006-9-0 loss: 0.491761  [   32/   54]
train() client id: f_00007-0-0 loss: 0.420107  [   32/  179]
train() client id: f_00007-0-1 loss: 0.476074  [   64/  179]
train() client id: f_00007-0-2 loss: 0.496858  [   96/  179]
train() client id: f_00007-0-3 loss: 0.384069  [  128/  179]
train() client id: f_00007-0-4 loss: 0.503862  [  160/  179]
train() client id: f_00007-1-0 loss: 0.357215  [   32/  179]
train() client id: f_00007-1-1 loss: 0.421059  [   64/  179]
train() client id: f_00007-1-2 loss: 0.565450  [   96/  179]
train() client id: f_00007-1-3 loss: 0.374879  [  128/  179]
train() client id: f_00007-1-4 loss: 0.384881  [  160/  179]
train() client id: f_00007-2-0 loss: 0.391802  [   32/  179]
train() client id: f_00007-2-1 loss: 0.472488  [   64/  179]
train() client id: f_00007-2-2 loss: 0.375244  [   96/  179]
train() client id: f_00007-2-3 loss: 0.477525  [  128/  179]
train() client id: f_00007-2-4 loss: 0.412177  [  160/  179]
train() client id: f_00007-3-0 loss: 0.403884  [   32/  179]
train() client id: f_00007-3-1 loss: 0.241106  [   64/  179]
train() client id: f_00007-3-2 loss: 0.717581  [   96/  179]
train() client id: f_00007-3-3 loss: 0.272132  [  128/  179]
train() client id: f_00007-3-4 loss: 0.410192  [  160/  179]
train() client id: f_00007-4-0 loss: 0.506377  [   32/  179]
train() client id: f_00007-4-1 loss: 0.420291  [   64/  179]
train() client id: f_00007-4-2 loss: 0.366921  [   96/  179]
train() client id: f_00007-4-3 loss: 0.289340  [  128/  179]
train() client id: f_00007-4-4 loss: 0.510301  [  160/  179]
train() client id: f_00007-5-0 loss: 0.455873  [   32/  179]
train() client id: f_00007-5-1 loss: 0.311057  [   64/  179]
train() client id: f_00007-5-2 loss: 0.245023  [   96/  179]
train() client id: f_00007-5-3 loss: 0.418179  [  128/  179]
train() client id: f_00007-5-4 loss: 0.620325  [  160/  179]
train() client id: f_00007-6-0 loss: 0.446298  [   32/  179]
train() client id: f_00007-6-1 loss: 0.612462  [   64/  179]
train() client id: f_00007-6-2 loss: 0.260027  [   96/  179]
train() client id: f_00007-6-3 loss: 0.375743  [  128/  179]
train() client id: f_00007-6-4 loss: 0.253558  [  160/  179]
train() client id: f_00007-7-0 loss: 0.396793  [   32/  179]
train() client id: f_00007-7-1 loss: 0.586481  [   64/  179]
train() client id: f_00007-7-2 loss: 0.272174  [   96/  179]
train() client id: f_00007-7-3 loss: 0.357727  [  128/  179]
train() client id: f_00007-7-4 loss: 0.447773  [  160/  179]
train() client id: f_00007-8-0 loss: 0.413141  [   32/  179]
train() client id: f_00007-8-1 loss: 0.342276  [   64/  179]
train() client id: f_00007-8-2 loss: 0.601296  [   96/  179]
train() client id: f_00007-8-3 loss: 0.344681  [  128/  179]
train() client id: f_00007-8-4 loss: 0.276398  [  160/  179]
train() client id: f_00007-9-0 loss: 0.340890  [   32/  179]
train() client id: f_00007-9-1 loss: 0.378250  [   64/  179]
train() client id: f_00007-9-2 loss: 0.509380  [   96/  179]
train() client id: f_00007-9-3 loss: 0.413222  [  128/  179]
train() client id: f_00007-9-4 loss: 0.343186  [  160/  179]
train() client id: f_00008-0-0 loss: 0.630994  [   32/  130]
train() client id: f_00008-0-1 loss: 0.635550  [   64/  130]
train() client id: f_00008-0-2 loss: 0.560379  [   96/  130]
train() client id: f_00008-0-3 loss: 0.668361  [  128/  130]
train() client id: f_00008-1-0 loss: 0.681662  [   32/  130]
train() client id: f_00008-1-1 loss: 0.587876  [   64/  130]
train() client id: f_00008-1-2 loss: 0.608175  [   96/  130]
train() client id: f_00008-1-3 loss: 0.588197  [  128/  130]
train() client id: f_00008-2-0 loss: 0.553404  [   32/  130]
train() client id: f_00008-2-1 loss: 0.582273  [   64/  130]
train() client id: f_00008-2-2 loss: 0.643873  [   96/  130]
train() client id: f_00008-2-3 loss: 0.680727  [  128/  130]
train() client id: f_00008-3-0 loss: 0.672109  [   32/  130]
train() client id: f_00008-3-1 loss: 0.648084  [   64/  130]
train() client id: f_00008-3-2 loss: 0.650366  [   96/  130]
train() client id: f_00008-3-3 loss: 0.496909  [  128/  130]
train() client id: f_00008-4-0 loss: 0.573978  [   32/  130]
train() client id: f_00008-4-1 loss: 0.611400  [   64/  130]
train() client id: f_00008-4-2 loss: 0.724558  [   96/  130]
train() client id: f_00008-4-3 loss: 0.564285  [  128/  130]
train() client id: f_00008-5-0 loss: 0.752566  [   32/  130]
train() client id: f_00008-5-1 loss: 0.586572  [   64/  130]
train() client id: f_00008-5-2 loss: 0.568247  [   96/  130]
train() client id: f_00008-5-3 loss: 0.573551  [  128/  130]
train() client id: f_00008-6-0 loss: 0.590208  [   32/  130]
train() client id: f_00008-6-1 loss: 0.702741  [   64/  130]
train() client id: f_00008-6-2 loss: 0.601715  [   96/  130]
train() client id: f_00008-6-3 loss: 0.601233  [  128/  130]
train() client id: f_00008-7-0 loss: 0.513656  [   32/  130]
train() client id: f_00008-7-1 loss: 0.569191  [   64/  130]
train() client id: f_00008-7-2 loss: 0.659381  [   96/  130]
train() client id: f_00008-7-3 loss: 0.737625  [  128/  130]
train() client id: f_00008-8-0 loss: 0.557188  [   32/  130]
train() client id: f_00008-8-1 loss: 0.615854  [   64/  130]
train() client id: f_00008-8-2 loss: 0.562532  [   96/  130]
train() client id: f_00008-8-3 loss: 0.717075  [  128/  130]
train() client id: f_00008-9-0 loss: 0.694073  [   32/  130]
train() client id: f_00008-9-1 loss: 0.568133  [   64/  130]
train() client id: f_00008-9-2 loss: 0.475786  [   96/  130]
train() client id: f_00008-9-3 loss: 0.715927  [  128/  130]
train() client id: f_00009-0-0 loss: 1.177151  [   32/  118]
train() client id: f_00009-0-1 loss: 0.869323  [   64/  118]
train() client id: f_00009-0-2 loss: 1.065502  [   96/  118]
train() client id: f_00009-1-0 loss: 0.965788  [   32/  118]
train() client id: f_00009-1-1 loss: 0.918888  [   64/  118]
train() client id: f_00009-1-2 loss: 1.027079  [   96/  118]
train() client id: f_00009-2-0 loss: 1.059442  [   32/  118]
train() client id: f_00009-2-1 loss: 0.966225  [   64/  118]
train() client id: f_00009-2-2 loss: 0.861955  [   96/  118]
train() client id: f_00009-3-0 loss: 1.043072  [   32/  118]
train() client id: f_00009-3-1 loss: 0.980659  [   64/  118]
train() client id: f_00009-3-2 loss: 0.946213  [   96/  118]
train() client id: f_00009-4-0 loss: 0.986849  [   32/  118]
train() client id: f_00009-4-1 loss: 0.796990  [   64/  118]
train() client id: f_00009-4-2 loss: 0.806604  [   96/  118]
train() client id: f_00009-5-0 loss: 0.873600  [   32/  118]
train() client id: f_00009-5-1 loss: 0.973398  [   64/  118]
train() client id: f_00009-5-2 loss: 0.820776  [   96/  118]
train() client id: f_00009-6-0 loss: 0.915940  [   32/  118]
train() client id: f_00009-6-1 loss: 0.831556  [   64/  118]
train() client id: f_00009-6-2 loss: 0.851631  [   96/  118]
train() client id: f_00009-7-0 loss: 0.684646  [   32/  118]
train() client id: f_00009-7-1 loss: 1.125659  [   64/  118]
train() client id: f_00009-7-2 loss: 0.846022  [   96/  118]
train() client id: f_00009-8-0 loss: 0.802025  [   32/  118]
train() client id: f_00009-8-1 loss: 0.906011  [   64/  118]
train() client id: f_00009-8-2 loss: 0.856895  [   96/  118]
train() client id: f_00009-9-0 loss: 0.829286  [   32/  118]
train() client id: f_00009-9-1 loss: 0.823151  [   64/  118]
train() client id: f_00009-9-2 loss: 0.943825  [   96/  118]
At round 47 accuracy: 0.649867374005305
At round 47 training accuracy: 0.5949027498323273
At round 47 training loss: 0.8168485083382202
update_location
xs = -4.528292 136.001589 145.045120 -145.943528 34.896481 -70.217951 -187.215960 208.375741 -1.680116 129.695607 
ys = 222.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -152.154970 4.001482 
xs mean: 24.442869159412865
ys mean: 9.371751218646876
dists_uav = 244.061273 169.524088 176.181244 178.347365 105.915941 122.223075 212.272421 231.922900 182.082283 163.819908 
uav_gains = -110.799948 -105.752752 -106.184350 -106.322700 -100.624070 -102.179107 -108.470144 -109.846953 -106.559373 -105.373438 
uav_gains_db_mean: -106.21128345158665
dists_bs = 176.989314 349.491673 364.133777 155.199417 273.695798 201.562012 178.543321 429.746343 370.229870 349.399351 
bs_gains = -102.509576 -110.783290 -111.282366 -100.911982 -107.810567 -104.090502 -102.615880 -113.297000 -111.484260 -110.780078 
bs_gains_db_mean: -107.55655019582483
Round 48
-------------------------------
ene_coms = [0.01130035 0.01128694 0.00846481 0.00852976 0.00929404 0.00758721
 0.00967628 0.01057827 0.0118776  0.01128436]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.64051791  9.61182324  4.53535944  1.63908938 11.05459262  5.31180241
  2.04611117  6.52754914  4.79042727  4.35420088]
obj_prev = 54.51147346843486
eta_min = 2.6746987320566712e-20	eta_max = 0.9490011437830361
af = 11.464013219461759	bf = 1.1726268437118712	zeta = 12.610414541407936	eta = 0.909090909090909
af = 11.464013219461759	bf = 1.1726268437118712	zeta = 24.917904054317184	eta = 0.46007132840996495
af = 11.464013219461759	bf = 1.1726268437118712	zeta = 18.656141430475426	eta = 0.614490046732542
af = 11.464013219461759	bf = 1.1726268437118712	zeta = 17.522691354173396	eta = 0.6542381525616134
af = 11.464013219461759	bf = 1.1726268437118712	zeta = 17.45890273796865	eta = 0.6566285058986244
af = 11.464013219461759	bf = 1.1726268437118712	zeta = 17.45868097819327	eta = 0.656636846379223
eta = 0.656636846379223
ene_coms = [0.01130035 0.01128694 0.00846481 0.00852976 0.00929404 0.00758721
 0.00967628 0.01057827 0.0118776  0.01128436]
ene_comp = [0.0347089  0.07299887 0.03415796 0.0118451  0.08429304 0.04021825
 0.01487524 0.04930867 0.03581077 0.03250516]
ene_total = [1.57316535 2.88193221 1.45737383 0.69666495 3.19996466 1.63458198
 0.8394749  2.04767691 1.63057854 1.49726763]
ti_comp = [0.69418813 0.69432218 0.7225435  0.72189397 0.71425115 0.73131954
 0.71042878 0.70140887 0.68841561 0.694348  ]
ti_coms = [0.11300345 0.11286941 0.08464809 0.08529762 0.09294043 0.07587205
 0.09676281 0.10578272 0.11877598 0.11284359]
t_total = [27.59979858 27.59979858 27.59979858 27.59979858 27.59979858 27.59979858
 27.59979858 27.59979858 27.59979858 27.59979858]
ene_coms = [0.01130035 0.01128694 0.00846481 0.00852976 0.00929404 0.00758721
 0.00967628 0.01057827 0.0118776  0.01128436]
ene_comp = [5.42310609e-06 5.04320216e-05 4.77120297e-06 1.99319311e-07
 7.33758615e-05 7.60212579e-06 4.07597040e-07 1.52302636e-05
 6.05646872e-06 4.45228009e-06]
ene_total = [0.38657109 0.38765171 0.28959506 0.29165965 0.32029432 0.25968451
 0.33086897 0.36221701 0.40633039 0.38599127]
optimize_network iter = 0 obj = 3.4208639883800904
eta = 0.656636846379223
freqs = [24999633.22508174 52568442.64377525 23637305.200351    8204182.64115953
 59007980.6906969  27497040.75065285 10469199.85705707 35149734.94280257
 26009555.03631463 23406964.41564802]
eta_min = 0.6566368463792238	eta_max = 0.7373197174275724
af = 0.004687803603200706	bf = 1.1726268437118712	zeta = 0.005156583963520777	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01130035 0.01128694 0.00846481 0.00852976 0.00929404 0.00758721
 0.00967628 0.01057827 0.0118776  0.01128436]
ene_comp = [1.22883388e-06 1.14275059e-05 1.08111768e-06 4.51642136e-08
 1.66264025e-05 1.72258288e-06 9.23583356e-08 3.45105990e-06
 1.37234895e-06 1.00885222e-06]
ene_total = [1.4595353  1.45912124 1.09332245 1.10157703 1.20242108 0.98006769
 1.24964964 1.3665705  1.5341028  1.45744229]
ti_comp = [0.50451567 0.50464972 0.53287104 0.5322215  0.52457869 0.54164707
 0.52075632 0.51173641 0.49874314 0.50467554]
ti_coms = [0.11300345 0.11286941 0.08464809 0.08529762 0.09294043 0.07587205
 0.09676281 0.10578272 0.11877598 0.11284359]
t_total = [27.59979858 27.59979858 27.59979858 27.59979858 27.59979858 27.59979858
 27.59979858 27.59979858 27.59979858 27.59979858]
ene_coms = [0.01130035 0.01128694 0.00846481 0.00852976 0.00929404 0.00758721
 0.00967628 0.01057827 0.0118776  0.01128436]
ene_comp = [3.90419809e-06 3.63017097e-05 3.33572584e-06 1.39440862e-07
 5.17264315e-05 5.26981408e-06 2.88457420e-07 1.08801777e-05
 4.38778874e-06 3.20472701e-06]
ene_total = [0.50523943 0.5060883  0.37848071 0.38124092 0.41770587 0.33934298
 0.43249084 0.47327839 0.53106115 0.50449365]
optimize_network iter = 1 obj = 4.469422249366028
eta = 0.7373197174275724
freqs = [24920823.73472554 52398921.28579371 23220206.40853826  8061995.38574156
 58207277.86821689 26896945.46377653 10347268.56528505 34903841.14324031
 26009555.03631463 23331156.16431859]
eta_min = 0.7373197174275771	eta_max = 0.7373197174275691
af = 0.004601577763770081	bf = 1.1726268437118712	zeta = 0.00506173554014709	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01130035 0.01128694 0.00846481 0.00852976 0.00929404 0.00758721
 0.00967628 0.01057827 0.0118776  0.01128436]
ene_comp = [1.22109848e-06 1.13539225e-05 1.04329997e-06 4.36122912e-08
 1.61782433e-05 1.64821605e-06 9.02195299e-08 3.40294426e-06
 1.37234895e-06 1.00232805e-06]
ene_total = [1.4595343  1.45911174 1.09331757 1.10157683 1.20236321 0.98005809
 1.24964936 1.36656428 1.5341028  1.45744144]
ti_comp = [0.50451567 0.50464972 0.53287104 0.5322215  0.52457869 0.54164707
 0.52075632 0.51173641 0.49874314 0.50467554]
ti_coms = [0.11300345 0.11286941 0.08464809 0.08529762 0.09294043 0.07587205
 0.09676281 0.10578272 0.11877598 0.11284359]
t_total = [27.59979858 27.59979858 27.59979858 27.59979858 27.59979858 27.59979858
 27.59979858 27.59979858 27.59979858 27.59979858]
ene_coms = [0.01130035 0.01128694 0.00846481 0.00852976 0.00929404 0.00758721
 0.00967628 0.01057827 0.0118776  0.01128436]
ene_comp = [3.90419809e-06 3.63017097e-05 3.33572584e-06 1.39440862e-07
 5.17264315e-05 5.26981408e-06 2.88457420e-07 1.08801777e-05
 4.38778874e-06 3.20472701e-06]
ene_total = [0.50523943 0.5060883  0.37848071 0.38124092 0.41770587 0.33934298
 0.43249084 0.47327839 0.53106115 0.50449365]
optimize_network iter = 2 obj = 4.469422249365971
eta = 0.7373197174275691
freqs = [24920823.73472552 52398921.28579367 23220206.40853826  8061995.38574156
 58207277.86821689 26896945.46377654 10347268.56528505 34903841.1432403
 26009555.03631461 23331156.16431857]
Done!
ene_coms = [0.01130035 0.01128694 0.00846481 0.00852976 0.00929404 0.00758721
 0.00967628 0.01057827 0.0118776  0.01128436]
ene_comp = [3.52133907e-06 3.27418399e-05 3.00861316e-06 1.25766814e-07
 4.66539607e-05 4.75303809e-06 2.60170299e-07 9.81323031e-06
 3.95750718e-06 2.89046052e-06]
ene_total = [0.01130387 0.01131968 0.00846782 0.00852989 0.0093407  0.00759196
 0.00967654 0.01058808 0.01188156 0.01128725]
At round 48 energy consumption: 0.0999873402325588
At round 48 eta: 0.7373197174275691
At round 48 a_n: 11.740402201727147
At round 48 local rounds: 9.97852864080071
At round 48 global rounds: 44.69464585142538
gradient difference: 0.530657172203064
train() client id: f_00000-0-0 loss: 1.296192  [   32/  126]
train() client id: f_00000-0-1 loss: 1.546549  [   64/  126]
train() client id: f_00000-0-2 loss: 1.273247  [   96/  126]
train() client id: f_00000-1-0 loss: 1.175452  [   32/  126]
train() client id: f_00000-1-1 loss: 1.125931  [   64/  126]
train() client id: f_00000-1-2 loss: 1.262781  [   96/  126]
train() client id: f_00000-2-0 loss: 1.070228  [   32/  126]
train() client id: f_00000-2-1 loss: 1.217715  [   64/  126]
train() client id: f_00000-2-2 loss: 0.972989  [   96/  126]
train() client id: f_00000-3-0 loss: 0.841772  [   32/  126]
train() client id: f_00000-3-1 loss: 1.198052  [   64/  126]
train() client id: f_00000-3-2 loss: 1.084428  [   96/  126]
train() client id: f_00000-4-0 loss: 1.032042  [   32/  126]
train() client id: f_00000-4-1 loss: 0.943164  [   64/  126]
train() client id: f_00000-4-2 loss: 1.019649  [   96/  126]
train() client id: f_00000-5-0 loss: 0.984230  [   32/  126]
train() client id: f_00000-5-1 loss: 0.944029  [   64/  126]
train() client id: f_00000-5-2 loss: 0.926154  [   96/  126]
train() client id: f_00000-6-0 loss: 0.826898  [   32/  126]
train() client id: f_00000-6-1 loss: 0.897516  [   64/  126]
train() client id: f_00000-6-2 loss: 1.000344  [   96/  126]
train() client id: f_00000-7-0 loss: 0.977024  [   32/  126]
train() client id: f_00000-7-1 loss: 0.904572  [   64/  126]
train() client id: f_00000-7-2 loss: 0.911051  [   96/  126]
train() client id: f_00000-8-0 loss: 0.854822  [   32/  126]
train() client id: f_00000-8-1 loss: 0.960447  [   64/  126]
train() client id: f_00000-8-2 loss: 0.786055  [   96/  126]
train() client id: f_00001-0-0 loss: 0.485785  [   32/  265]
train() client id: f_00001-0-1 loss: 0.450619  [   64/  265]
train() client id: f_00001-0-2 loss: 0.388245  [   96/  265]
train() client id: f_00001-0-3 loss: 0.432692  [  128/  265]
train() client id: f_00001-0-4 loss: 0.519010  [  160/  265]
train() client id: f_00001-0-5 loss: 0.534954  [  192/  265]
train() client id: f_00001-0-6 loss: 0.456378  [  224/  265]
train() client id: f_00001-0-7 loss: 0.435236  [  256/  265]
train() client id: f_00001-1-0 loss: 0.469783  [   32/  265]
train() client id: f_00001-1-1 loss: 0.454666  [   64/  265]
train() client id: f_00001-1-2 loss: 0.348146  [   96/  265]
train() client id: f_00001-1-3 loss: 0.591369  [  128/  265]
train() client id: f_00001-1-4 loss: 0.471033  [  160/  265]
train() client id: f_00001-1-5 loss: 0.394096  [  192/  265]
train() client id: f_00001-1-6 loss: 0.461073  [  224/  265]
train() client id: f_00001-1-7 loss: 0.443054  [  256/  265]
train() client id: f_00001-2-0 loss: 0.372979  [   32/  265]
train() client id: f_00001-2-1 loss: 0.459450  [   64/  265]
train() client id: f_00001-2-2 loss: 0.469238  [   96/  265]
train() client id: f_00001-2-3 loss: 0.503752  [  128/  265]
train() client id: f_00001-2-4 loss: 0.487116  [  160/  265]
train() client id: f_00001-2-5 loss: 0.428626  [  192/  265]
train() client id: f_00001-2-6 loss: 0.409075  [  224/  265]
train() client id: f_00001-2-7 loss: 0.456432  [  256/  265]
train() client id: f_00001-3-0 loss: 0.393082  [   32/  265]
train() client id: f_00001-3-1 loss: 0.467219  [   64/  265]
train() client id: f_00001-3-2 loss: 0.390840  [   96/  265]
train() client id: f_00001-3-3 loss: 0.346648  [  128/  265]
train() client id: f_00001-3-4 loss: 0.425434  [  160/  265]
train() client id: f_00001-3-5 loss: 0.582260  [  192/  265]
train() client id: f_00001-3-6 loss: 0.528762  [  224/  265]
train() client id: f_00001-3-7 loss: 0.403917  [  256/  265]
train() client id: f_00001-4-0 loss: 0.373088  [   32/  265]
train() client id: f_00001-4-1 loss: 0.409381  [   64/  265]
train() client id: f_00001-4-2 loss: 0.566330  [   96/  265]
train() client id: f_00001-4-3 loss: 0.441963  [  128/  265]
train() client id: f_00001-4-4 loss: 0.381987  [  160/  265]
train() client id: f_00001-4-5 loss: 0.376571  [  192/  265]
train() client id: f_00001-4-6 loss: 0.515610  [  224/  265]
train() client id: f_00001-4-7 loss: 0.455752  [  256/  265]
train() client id: f_00001-5-0 loss: 0.366396  [   32/  265]
train() client id: f_00001-5-1 loss: 0.420781  [   64/  265]
train() client id: f_00001-5-2 loss: 0.397249  [   96/  265]
train() client id: f_00001-5-3 loss: 0.337061  [  128/  265]
train() client id: f_00001-5-4 loss: 0.681700  [  160/  265]
train() client id: f_00001-5-5 loss: 0.492623  [  192/  265]
train() client id: f_00001-5-6 loss: 0.452975  [  224/  265]
train() client id: f_00001-5-7 loss: 0.345269  [  256/  265]
train() client id: f_00001-6-0 loss: 0.351232  [   32/  265]
train() client id: f_00001-6-1 loss: 0.511919  [   64/  265]
train() client id: f_00001-6-2 loss: 0.470842  [   96/  265]
train() client id: f_00001-6-3 loss: 0.600833  [  128/  265]
train() client id: f_00001-6-4 loss: 0.390508  [  160/  265]
train() client id: f_00001-6-5 loss: 0.394597  [  192/  265]
train() client id: f_00001-6-6 loss: 0.344244  [  224/  265]
train() client id: f_00001-6-7 loss: 0.415233  [  256/  265]
train() client id: f_00001-7-0 loss: 0.441728  [   32/  265]
train() client id: f_00001-7-1 loss: 0.544006  [   64/  265]
train() client id: f_00001-7-2 loss: 0.387296  [   96/  265]
train() client id: f_00001-7-3 loss: 0.461386  [  128/  265]
train() client id: f_00001-7-4 loss: 0.421289  [  160/  265]
train() client id: f_00001-7-5 loss: 0.447930  [  192/  265]
train() client id: f_00001-7-6 loss: 0.432690  [  224/  265]
train() client id: f_00001-7-7 loss: 0.338875  [  256/  265]
train() client id: f_00001-8-0 loss: 0.376332  [   32/  265]
train() client id: f_00001-8-1 loss: 0.342632  [   64/  265]
train() client id: f_00001-8-2 loss: 0.600283  [   96/  265]
train() client id: f_00001-8-3 loss: 0.477918  [  128/  265]
train() client id: f_00001-8-4 loss: 0.479314  [  160/  265]
train() client id: f_00001-8-5 loss: 0.410891  [  192/  265]
train() client id: f_00001-8-6 loss: 0.400352  [  224/  265]
train() client id: f_00001-8-7 loss: 0.375933  [  256/  265]
train() client id: f_00002-0-0 loss: 1.221168  [   32/  124]
train() client id: f_00002-0-1 loss: 1.033614  [   64/  124]
train() client id: f_00002-0-2 loss: 1.226130  [   96/  124]
train() client id: f_00002-1-0 loss: 1.214244  [   32/  124]
train() client id: f_00002-1-1 loss: 1.074873  [   64/  124]
train() client id: f_00002-1-2 loss: 1.091229  [   96/  124]
train() client id: f_00002-2-0 loss: 1.056694  [   32/  124]
train() client id: f_00002-2-1 loss: 1.133327  [   64/  124]
train() client id: f_00002-2-2 loss: 0.984474  [   96/  124]
train() client id: f_00002-3-0 loss: 1.126693  [   32/  124]
train() client id: f_00002-3-1 loss: 1.247159  [   64/  124]
train() client id: f_00002-3-2 loss: 0.947199  [   96/  124]
train() client id: f_00002-4-0 loss: 1.091313  [   32/  124]
train() client id: f_00002-4-1 loss: 0.939819  [   64/  124]
train() client id: f_00002-4-2 loss: 1.118486  [   96/  124]
train() client id: f_00002-5-0 loss: 0.977822  [   32/  124]
train() client id: f_00002-5-1 loss: 1.169435  [   64/  124]
train() client id: f_00002-5-2 loss: 1.188242  [   96/  124]
train() client id: f_00002-6-0 loss: 1.034654  [   32/  124]
train() client id: f_00002-6-1 loss: 0.922870  [   64/  124]
train() client id: f_00002-6-2 loss: 1.149971  [   96/  124]
train() client id: f_00002-7-0 loss: 0.986996  [   32/  124]
train() client id: f_00002-7-1 loss: 0.926124  [   64/  124]
train() client id: f_00002-7-2 loss: 1.118599  [   96/  124]
train() client id: f_00002-8-0 loss: 1.270876  [   32/  124]
train() client id: f_00002-8-1 loss: 0.978449  [   64/  124]
train() client id: f_00002-8-2 loss: 0.936078  [   96/  124]
train() client id: f_00003-0-0 loss: 0.483076  [   32/   43]
train() client id: f_00003-1-0 loss: 0.478043  [   32/   43]
train() client id: f_00003-2-0 loss: 0.457716  [   32/   43]
train() client id: f_00003-3-0 loss: 0.476753  [   32/   43]
train() client id: f_00003-4-0 loss: 0.628304  [   32/   43]
train() client id: f_00003-5-0 loss: 0.816950  [   32/   43]
train() client id: f_00003-6-0 loss: 0.452251  [   32/   43]
train() client id: f_00003-7-0 loss: 0.492156  [   32/   43]
train() client id: f_00003-8-0 loss: 0.423617  [   32/   43]
train() client id: f_00004-0-0 loss: 1.001280  [   32/  306]
train() client id: f_00004-0-1 loss: 0.829301  [   64/  306]
train() client id: f_00004-0-2 loss: 0.827119  [   96/  306]
train() client id: f_00004-0-3 loss: 0.741148  [  128/  306]
train() client id: f_00004-0-4 loss: 0.855540  [  160/  306]
train() client id: f_00004-0-5 loss: 0.871747  [  192/  306]
train() client id: f_00004-0-6 loss: 0.908205  [  224/  306]
train() client id: f_00004-0-7 loss: 0.751803  [  256/  306]
train() client id: f_00004-0-8 loss: 0.882500  [  288/  306]
train() client id: f_00004-1-0 loss: 0.859295  [   32/  306]
train() client id: f_00004-1-1 loss: 0.820076  [   64/  306]
train() client id: f_00004-1-2 loss: 0.796400  [   96/  306]
train() client id: f_00004-1-3 loss: 0.834931  [  128/  306]
train() client id: f_00004-1-4 loss: 0.678053  [  160/  306]
train() client id: f_00004-1-5 loss: 0.958143  [  192/  306]
train() client id: f_00004-1-6 loss: 0.798197  [  224/  306]
train() client id: f_00004-1-7 loss: 0.914392  [  256/  306]
train() client id: f_00004-1-8 loss: 0.893236  [  288/  306]
train() client id: f_00004-2-0 loss: 0.820867  [   32/  306]
train() client id: f_00004-2-1 loss: 0.759461  [   64/  306]
train() client id: f_00004-2-2 loss: 0.918185  [   96/  306]
train() client id: f_00004-2-3 loss: 1.013356  [  128/  306]
train() client id: f_00004-2-4 loss: 0.712729  [  160/  306]
train() client id: f_00004-2-5 loss: 0.758600  [  192/  306]
train() client id: f_00004-2-6 loss: 0.835847  [  224/  306]
train() client id: f_00004-2-7 loss: 0.957168  [  256/  306]
train() client id: f_00004-2-8 loss: 0.867847  [  288/  306]
train() client id: f_00004-3-0 loss: 0.843449  [   32/  306]
train() client id: f_00004-3-1 loss: 0.931481  [   64/  306]
train() client id: f_00004-3-2 loss: 0.886867  [   96/  306]
train() client id: f_00004-3-3 loss: 0.785731  [  128/  306]
train() client id: f_00004-3-4 loss: 0.736519  [  160/  306]
train() client id: f_00004-3-5 loss: 0.854011  [  192/  306]
train() client id: f_00004-3-6 loss: 0.841078  [  224/  306]
train() client id: f_00004-3-7 loss: 0.870614  [  256/  306]
train() client id: f_00004-3-8 loss: 0.908604  [  288/  306]
train() client id: f_00004-4-0 loss: 0.788172  [   32/  306]
train() client id: f_00004-4-1 loss: 0.796137  [   64/  306]
train() client id: f_00004-4-2 loss: 0.991467  [   96/  306]
train() client id: f_00004-4-3 loss: 0.788482  [  128/  306]
train() client id: f_00004-4-4 loss: 0.976679  [  160/  306]
train() client id: f_00004-4-5 loss: 0.788986  [  192/  306]
train() client id: f_00004-4-6 loss: 0.800766  [  224/  306]
train() client id: f_00004-4-7 loss: 0.811638  [  256/  306]
train() client id: f_00004-4-8 loss: 0.812791  [  288/  306]
train() client id: f_00004-5-0 loss: 0.810437  [   32/  306]
train() client id: f_00004-5-1 loss: 0.787052  [   64/  306]
train() client id: f_00004-5-2 loss: 0.843714  [   96/  306]
train() client id: f_00004-5-3 loss: 0.811364  [  128/  306]
train() client id: f_00004-5-4 loss: 0.846177  [  160/  306]
train() client id: f_00004-5-5 loss: 0.921878  [  192/  306]
train() client id: f_00004-5-6 loss: 0.898489  [  224/  306]
train() client id: f_00004-5-7 loss: 0.810824  [  256/  306]
train() client id: f_00004-5-8 loss: 0.847602  [  288/  306]
train() client id: f_00004-6-0 loss: 0.682502  [   32/  306]
train() client id: f_00004-6-1 loss: 0.768292  [   64/  306]
train() client id: f_00004-6-2 loss: 0.830194  [   96/  306]
train() client id: f_00004-6-3 loss: 0.794384  [  128/  306]
train() client id: f_00004-6-4 loss: 0.949391  [  160/  306]
train() client id: f_00004-6-5 loss: 1.056275  [  192/  306]
train() client id: f_00004-6-6 loss: 0.845974  [  224/  306]
train() client id: f_00004-6-7 loss: 0.866113  [  256/  306]
train() client id: f_00004-6-8 loss: 0.781174  [  288/  306]
train() client id: f_00004-7-0 loss: 0.696731  [   32/  306]
train() client id: f_00004-7-1 loss: 1.038458  [   64/  306]
train() client id: f_00004-7-2 loss: 0.888029  [   96/  306]
train() client id: f_00004-7-3 loss: 0.888734  [  128/  306]
train() client id: f_00004-7-4 loss: 0.771368  [  160/  306]
train() client id: f_00004-7-5 loss: 0.858726  [  192/  306]
train() client id: f_00004-7-6 loss: 0.776289  [  224/  306]
train() client id: f_00004-7-7 loss: 0.796189  [  256/  306]
train() client id: f_00004-7-8 loss: 0.941738  [  288/  306]
train() client id: f_00004-8-0 loss: 0.823119  [   32/  306]
train() client id: f_00004-8-1 loss: 0.815869  [   64/  306]
train() client id: f_00004-8-2 loss: 0.958451  [   96/  306]
train() client id: f_00004-8-3 loss: 0.880997  [  128/  306]
train() client id: f_00004-8-4 loss: 0.903786  [  160/  306]
train() client id: f_00004-8-5 loss: 0.779619  [  192/  306]
train() client id: f_00004-8-6 loss: 0.900868  [  224/  306]
train() client id: f_00004-8-7 loss: 0.804610  [  256/  306]
train() client id: f_00004-8-8 loss: 0.819463  [  288/  306]
train() client id: f_00005-0-0 loss: 1.041348  [   32/  146]
train() client id: f_00005-0-1 loss: 0.762376  [   64/  146]
train() client id: f_00005-0-2 loss: 1.067703  [   96/  146]
train() client id: f_00005-0-3 loss: 0.817461  [  128/  146]
train() client id: f_00005-1-0 loss: 0.990196  [   32/  146]
train() client id: f_00005-1-1 loss: 0.664364  [   64/  146]
train() client id: f_00005-1-2 loss: 1.053529  [   96/  146]
train() client id: f_00005-1-3 loss: 0.964197  [  128/  146]
train() client id: f_00005-2-0 loss: 1.314395  [   32/  146]
train() client id: f_00005-2-1 loss: 0.702767  [   64/  146]
train() client id: f_00005-2-2 loss: 0.581069  [   96/  146]
train() client id: f_00005-2-3 loss: 0.971730  [  128/  146]
train() client id: f_00005-3-0 loss: 0.976487  [   32/  146]
train() client id: f_00005-3-1 loss: 0.925001  [   64/  146]
train() client id: f_00005-3-2 loss: 0.792648  [   96/  146]
train() client id: f_00005-3-3 loss: 0.920374  [  128/  146]
train() client id: f_00005-4-0 loss: 0.983048  [   32/  146]
train() client id: f_00005-4-1 loss: 0.944389  [   64/  146]
train() client id: f_00005-4-2 loss: 1.031705  [   96/  146]
train() client id: f_00005-4-3 loss: 0.789586  [  128/  146]
train() client id: f_00005-5-0 loss: 0.950192  [   32/  146]
train() client id: f_00005-5-1 loss: 1.257903  [   64/  146]
train() client id: f_00005-5-2 loss: 0.899330  [   96/  146]
train() client id: f_00005-5-3 loss: 0.556205  [  128/  146]
train() client id: f_00005-6-0 loss: 0.959131  [   32/  146]
train() client id: f_00005-6-1 loss: 1.034268  [   64/  146]
train() client id: f_00005-6-2 loss: 0.914152  [   96/  146]
train() client id: f_00005-6-3 loss: 0.785061  [  128/  146]
train() client id: f_00005-7-0 loss: 1.230825  [   32/  146]
train() client id: f_00005-7-1 loss: 0.802702  [   64/  146]
train() client id: f_00005-7-2 loss: 0.820176  [   96/  146]
train() client id: f_00005-7-3 loss: 0.957935  [  128/  146]
train() client id: f_00005-8-0 loss: 0.821152  [   32/  146]
train() client id: f_00005-8-1 loss: 1.109352  [   64/  146]
train() client id: f_00005-8-2 loss: 0.826330  [   96/  146]
train() client id: f_00005-8-3 loss: 1.094522  [  128/  146]
train() client id: f_00006-0-0 loss: 0.541774  [   32/   54]
train() client id: f_00006-1-0 loss: 0.543771  [   32/   54]
train() client id: f_00006-2-0 loss: 0.506050  [   32/   54]
train() client id: f_00006-3-0 loss: 0.626581  [   32/   54]
train() client id: f_00006-4-0 loss: 0.592615  [   32/   54]
train() client id: f_00006-5-0 loss: 0.550555  [   32/   54]
train() client id: f_00006-6-0 loss: 0.559971  [   32/   54]
train() client id: f_00006-7-0 loss: 0.551791  [   32/   54]
train() client id: f_00006-8-0 loss: 0.545125  [   32/   54]
train() client id: f_00007-0-0 loss: 0.616341  [   32/  179]
train() client id: f_00007-0-1 loss: 0.607878  [   64/  179]
train() client id: f_00007-0-2 loss: 0.746191  [   96/  179]
train() client id: f_00007-0-3 loss: 0.686715  [  128/  179]
train() client id: f_00007-0-4 loss: 0.583210  [  160/  179]
train() client id: f_00007-1-0 loss: 0.523915  [   32/  179]
train() client id: f_00007-1-1 loss: 0.930269  [   64/  179]
train() client id: f_00007-1-2 loss: 0.622324  [   96/  179]
train() client id: f_00007-1-3 loss: 0.559997  [  128/  179]
train() client id: f_00007-1-4 loss: 0.661828  [  160/  179]
train() client id: f_00007-2-0 loss: 0.479974  [   32/  179]
train() client id: f_00007-2-1 loss: 0.704046  [   64/  179]
train() client id: f_00007-2-2 loss: 0.557997  [   96/  179]
train() client id: f_00007-2-3 loss: 0.785132  [  128/  179]
train() client id: f_00007-2-4 loss: 0.572693  [  160/  179]
train() client id: f_00007-3-0 loss: 0.502139  [   32/  179]
train() client id: f_00007-3-1 loss: 0.694052  [   64/  179]
train() client id: f_00007-3-2 loss: 0.550736  [   96/  179]
train() client id: f_00007-3-3 loss: 0.462609  [  128/  179]
train() client id: f_00007-3-4 loss: 0.767582  [  160/  179]
train() client id: f_00007-4-0 loss: 0.502066  [   32/  179]
train() client id: f_00007-4-1 loss: 0.730344  [   64/  179]
train() client id: f_00007-4-2 loss: 0.557818  [   96/  179]
train() client id: f_00007-4-3 loss: 0.740283  [  128/  179]
train() client id: f_00007-4-4 loss: 0.507505  [  160/  179]
train() client id: f_00007-5-0 loss: 0.868602  [   32/  179]
train() client id: f_00007-5-1 loss: 0.475680  [   64/  179]
train() client id: f_00007-5-2 loss: 0.566566  [   96/  179]
train() client id: f_00007-5-3 loss: 0.454866  [  128/  179]
train() client id: f_00007-5-4 loss: 0.479438  [  160/  179]
train() client id: f_00007-6-0 loss: 0.732254  [   32/  179]
train() client id: f_00007-6-1 loss: 0.810908  [   64/  179]
train() client id: f_00007-6-2 loss: 0.426497  [   96/  179]
train() client id: f_00007-6-3 loss: 0.654855  [  128/  179]
train() client id: f_00007-6-4 loss: 0.470983  [  160/  179]
train() client id: f_00007-7-0 loss: 0.552325  [   32/  179]
train() client id: f_00007-7-1 loss: 0.854645  [   64/  179]
train() client id: f_00007-7-2 loss: 0.494203  [   96/  179]
train() client id: f_00007-7-3 loss: 0.728327  [  128/  179]
train() client id: f_00007-7-4 loss: 0.424589  [  160/  179]
train() client id: f_00007-8-0 loss: 0.645784  [   32/  179]
train() client id: f_00007-8-1 loss: 0.518186  [   64/  179]
train() client id: f_00007-8-2 loss: 0.593532  [   96/  179]
train() client id: f_00007-8-3 loss: 0.631197  [  128/  179]
train() client id: f_00007-8-4 loss: 0.651336  [  160/  179]
train() client id: f_00008-0-0 loss: 0.487519  [   32/  130]
train() client id: f_00008-0-1 loss: 0.610465  [   64/  130]
train() client id: f_00008-0-2 loss: 0.900020  [   96/  130]
train() client id: f_00008-0-3 loss: 0.537470  [  128/  130]
train() client id: f_00008-1-0 loss: 0.562612  [   32/  130]
train() client id: f_00008-1-1 loss: 0.621121  [   64/  130]
train() client id: f_00008-1-2 loss: 0.681901  [   96/  130]
train() client id: f_00008-1-3 loss: 0.644922  [  128/  130]
train() client id: f_00008-2-0 loss: 0.649773  [   32/  130]
train() client id: f_00008-2-1 loss: 0.696203  [   64/  130]
train() client id: f_00008-2-2 loss: 0.503122  [   96/  130]
train() client id: f_00008-2-3 loss: 0.658475  [  128/  130]
train() client id: f_00008-3-0 loss: 0.570138  [   32/  130]
train() client id: f_00008-3-1 loss: 0.537474  [   64/  130]
train() client id: f_00008-3-2 loss: 0.704693  [   96/  130]
train() client id: f_00008-3-3 loss: 0.673810  [  128/  130]
train() client id: f_00008-4-0 loss: 0.672739  [   32/  130]
train() client id: f_00008-4-1 loss: 0.648112  [   64/  130]
train() client id: f_00008-4-2 loss: 0.578337  [   96/  130]
train() client id: f_00008-4-3 loss: 0.591551  [  128/  130]
train() client id: f_00008-5-0 loss: 0.647646  [   32/  130]
train() client id: f_00008-5-1 loss: 0.562527  [   64/  130]
train() client id: f_00008-5-2 loss: 0.667232  [   96/  130]
train() client id: f_00008-5-3 loss: 0.643801  [  128/  130]
train() client id: f_00008-6-0 loss: 0.580032  [   32/  130]
train() client id: f_00008-6-1 loss: 0.582650  [   64/  130]
train() client id: f_00008-6-2 loss: 0.687534  [   96/  130]
train() client id: f_00008-6-3 loss: 0.640442  [  128/  130]
train() client id: f_00008-7-0 loss: 0.486863  [   32/  130]
train() client id: f_00008-7-1 loss: 0.750671  [   64/  130]
train() client id: f_00008-7-2 loss: 0.666141  [   96/  130]
train() client id: f_00008-7-3 loss: 0.615506  [  128/  130]
train() client id: f_00008-8-0 loss: 0.586870  [   32/  130]
train() client id: f_00008-8-1 loss: 0.609191  [   64/  130]
train() client id: f_00008-8-2 loss: 0.668885  [   96/  130]
train() client id: f_00008-8-3 loss: 0.658893  [  128/  130]
train() client id: f_00009-0-0 loss: 0.887769  [   32/  118]
train() client id: f_00009-0-1 loss: 0.882034  [   64/  118]
train() client id: f_00009-0-2 loss: 1.035064  [   96/  118]
train() client id: f_00009-1-0 loss: 0.979574  [   32/  118]
train() client id: f_00009-1-1 loss: 0.831139  [   64/  118]
train() client id: f_00009-1-2 loss: 1.000416  [   96/  118]
train() client id: f_00009-2-0 loss: 0.874910  [   32/  118]
train() client id: f_00009-2-1 loss: 0.739166  [   64/  118]
train() client id: f_00009-2-2 loss: 0.998487  [   96/  118]
train() client id: f_00009-3-0 loss: 0.917482  [   32/  118]
train() client id: f_00009-3-1 loss: 0.854336  [   64/  118]
train() client id: f_00009-3-2 loss: 0.791509  [   96/  118]
train() client id: f_00009-4-0 loss: 0.832180  [   32/  118]
train() client id: f_00009-4-1 loss: 0.899814  [   64/  118]
train() client id: f_00009-4-2 loss: 0.764424  [   96/  118]
train() client id: f_00009-5-0 loss: 0.830818  [   32/  118]
train() client id: f_00009-5-1 loss: 0.756699  [   64/  118]
train() client id: f_00009-5-2 loss: 0.798188  [   96/  118]
train() client id: f_00009-6-0 loss: 0.763401  [   32/  118]
train() client id: f_00009-6-1 loss: 0.902670  [   64/  118]
train() client id: f_00009-6-2 loss: 0.700878  [   96/  118]
train() client id: f_00009-7-0 loss: 0.883480  [   32/  118]
train() client id: f_00009-7-1 loss: 0.833728  [   64/  118]
train() client id: f_00009-7-2 loss: 0.774065  [   96/  118]
train() client id: f_00009-8-0 loss: 0.656619  [   32/  118]
train() client id: f_00009-8-1 loss: 0.753196  [   64/  118]
train() client id: f_00009-8-2 loss: 0.676956  [   96/  118]
At round 48 accuracy: 0.6472148541114059
At round 48 training accuracy: 0.5881958417169685
At round 48 training loss: 0.8172749093875518
update_location
xs = -4.528292 141.001589 150.045120 -150.943528 39.896481 -75.217951 -192.215960 213.375741 -1.680116 134.695607 
ys = 227.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -157.154970 4.001482 
xs mean: 25.442869159412865
ys mean: 9.371751218646876
dists_uav = 248.629815 173.561033 180.319943 182.461552 107.666854 125.162533 216.695040 236.425441 186.280722 167.806193 
uav_gains = -111.179941 -106.015723 -106.447964 -106.583293 -100.802096 -102.437245 -108.764903 -110.190603 -106.823276 -105.639536 
uav_gains_db_mean: -106.48845813348129
dists_bs = 178.398702 353.948365 368.536103 154.341487 277.548833 199.008603 178.954958 434.212709 374.655450 353.768091 
bs_gains = -102.606026 -110.937377 -111.428500 -100.844574 -107.980562 -103.935471 -102.643884 -113.422730 -111.628757 -110.931182 
bs_gains_db_mean: -107.6359062913406
Round 49
-------------------------------
ene_coms = [0.01161433 0.01141207 0.00858941 0.00865475 0.00938978 0.00752923
 0.00985692 0.0108285  0.01200647 0.01140699]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.50873794  9.33282288  4.40446726  1.59270525 10.7331586   5.15615424
  1.9884921   6.33997782  4.65214228  4.22857167]
obj_prev = 52.937230044491855
eta_min = 7.263760774978712e-21	eta_max = 0.949874174621732
af = 11.129531482767057	bf = 1.1544711755922563	zeta = 12.242484631043764	eta = 0.909090909090909
af = 11.129531482767057	bf = 1.1544711755922563	zeta = 24.367517045879435	eta = 0.4567363782617757
af = 11.129531482767057	bf = 1.1544711755922563	zeta = 18.17923373883381	eta = 0.6122112539315979
af = 11.129531482767057	bf = 1.1544711755922563	zeta = 17.059509960719115	eta = 0.6523945593040886
af = 11.129531482767057	bf = 1.1544711755922563	zeta = 16.996161021375993	eta = 0.6548261968552485
af = 11.129531482767057	bf = 1.1544711755922563	zeta = 16.99593816171764	eta = 0.6548347832799062
eta = 0.6548347832799062
ene_coms = [0.01161433 0.01141207 0.00858941 0.00865475 0.00938978 0.00752923
 0.00985692 0.0108285  0.01200647 0.01140699]
ene_comp = [0.03493567 0.07347581 0.03438113 0.01192249 0.08484377 0.04048101
 0.01497243 0.04963083 0.03604474 0.03271753]
ene_total = [1.53714849 2.80312109 1.41894964 0.67949049 3.11172838 1.58536791
 0.819901   1.9964547  1.5867207  1.45705577]
ti_comp = [0.71815664 0.72017921 0.74840586 0.74775238 0.74040215 0.75900762
 0.73573078 0.72601497 0.7142352  0.72023001]
ti_coms = [0.11614329 0.11412072 0.08589407 0.08654755 0.09389778 0.0752923
 0.09856915 0.10828496 0.12006473 0.11406992]
t_total = [27.54979439 27.54979439 27.54979439 27.54979439 27.54979439 27.54979439
 27.54979439 27.54979439 27.54979439 27.54979439]
ene_coms = [0.01161433 0.01141207 0.00858941 0.00865475 0.00938978 0.00752923
 0.00985692 0.0108285  0.01200647 0.01140699]
ene_comp = [5.16712349e-06 4.78004542e-05 4.53488483e-06 1.89437223e-07
 6.96312984e-05 7.19684005e-06 3.87542194e-07 1.44958438e-05
 5.73750553e-06 4.21967646e-06]
ene_total = [0.38369264 0.37842162 0.28378444 0.28579882 0.31236341 0.24886377
 0.32550245 0.35805132 0.39666062 0.37681476]
optimize_network iter = 0 obj = 3.349953844938813
eta = 0.6548347832799062
freqs = [24323153.83393451 51012171.77053706 22969578.54752532  7972218.07702432
 57295731.95348039 26667066.01035964 10175209.40270066 34180308.14199054
 25233100.69125696 22713251.57271565]
eta_min = 0.6548347832799076	eta_max = 0.7429150838083656
af = 0.004290241086732392	bf = 1.1544711755922563	zeta = 0.004719265195405632	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01161433 0.01141207 0.00858941 0.00865475 0.00938978 0.00752923
 0.00985692 0.0108285  0.01200647 0.01140699]
ene_comp = [1.16323022e-06 1.07609065e-05 1.02089975e-06 4.26463781e-08
 1.56754973e-05 1.62016291e-06 8.72440520e-08 3.26332505e-06
 1.29163543e-06 9.49939588e-07]
ene_total = [1.45630888 1.4321539  1.07703713 1.08510753 1.17922203 0.94419109
 1.2358357  1.35804719 1.50549052 1.43028688]
ti_comp = [0.50525746 0.50728004 0.53550669 0.53485321 0.52750298 0.54610845
 0.5228316  0.5131158  0.50133603 0.50733084]
ti_coms = [0.11614329 0.11412072 0.08589407 0.08654755 0.09389778 0.0752923
 0.09856915 0.10828496 0.12006473 0.11406992]
t_total = [27.54979439 27.54979439 27.54979439 27.54979439 27.54979439 27.54979439
 27.54979439 27.54979439 27.54979439 27.54979439]
ene_coms = [0.01161433 0.01141207 0.00858941 0.00865475 0.00938978 0.00752923
 0.00985692 0.0108285  0.01200647 0.01140699]
ene_comp = [3.61014563e-06 3.33181315e-05 3.06318921e-06 1.28048565e-07
 4.74409090e-05 4.80772181e-06 2.65397102e-07 1.00361439e-05
 4.02727034e-06 2.94104760e-06]
ene_total = [0.5150812  0.50743122 0.38094707 0.38371412 0.41839896 0.33402148
 0.43701798 0.48052617 0.53248536 0.50585922]
optimize_network iter = 1 obj = 4.4954827803236
eta = 0.7429150838083656
freqs = [24266882.80849318 50834000.30652381 22532686.47788142  7823300.91498542
 56448537.26154671 26015378.10118223 10050510.30568127 33946408.19839266
 25233100.69125695 22633250.40353962]
eta_min = 0.7429150838084453	eta_max = 0.742915083808359
af = 0.004205079207629246	bf = 1.1544711755922563	zeta = 0.004625587128392171	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01161433 0.01141207 0.00858941 0.00865475 0.00938978 0.00752923
 0.00985692 0.0108285  0.01200647 0.01140699]
ene_comp = [1.15785423e-06 1.06858680e-05 9.82433109e-07 4.10680312e-08
 1.52153578e-05 1.54194363e-06 8.51187708e-08 3.21881522e-06
 1.29163543e-06 9.43259572e-07]
ene_total = [1.45630821 1.4321445  1.07703231 1.08510733 1.17916434 0.94418129
 1.23583543 1.35804161 1.50549052 1.43028604]
ti_comp = [0.50525746 0.50728004 0.53550669 0.53485321 0.52750298 0.54610845
 0.5228316  0.5131158  0.50133603 0.50733084]
ti_coms = [0.11614329 0.11412072 0.08589407 0.08654755 0.09389778 0.0752923
 0.09856915 0.10828496 0.12006473 0.11406992]
t_total = [27.54979439 27.54979439 27.54979439 27.54979439 27.54979439 27.54979439
 27.54979439 27.54979439 27.54979439 27.54979439]
ene_coms = [0.01161433 0.01141207 0.00858941 0.00865475 0.00938978 0.00752923
 0.00985692 0.0108285  0.01200647 0.01140699]
ene_comp = [3.61014563e-06 3.33181315e-05 3.06318921e-06 1.28048565e-07
 4.74409090e-05 4.80772181e-06 2.65397102e-07 1.00361439e-05
 4.02727034e-06 2.94104760e-06]
ene_total = [0.5150812  0.50743122 0.38094707 0.38371412 0.41839896 0.33402148
 0.43701798 0.48052617 0.53248536 0.50585922]
optimize_network iter = 2 obj = 4.495482780323486
eta = 0.742915083808359
freqs = [24266882.80849315 50834000.30652376 22532686.47788143  7823300.91498542
 56448537.26154671 26015378.10118226 10050510.30568127 33946408.19839264
 25233100.69125691 22633250.40353959]
Done!
ene_coms = [0.01161433 0.01141207 0.00858941 0.00865475 0.00938978 0.00752923
 0.00985692 0.0108285  0.01200647 0.01140699]
ene_comp = [3.33895868e-06 3.08153399e-05 2.83308854e-06 1.18429812e-07
 4.38772424e-05 4.44657533e-06 2.45461000e-07 9.28224876e-06
 3.72474982e-06 2.72012197e-06]
ene_total = [0.01161767 0.01144289 0.00859224 0.00865487 0.00943365 0.00753368
 0.00985716 0.01083778 0.0120102  0.01140971]
At round 49 energy consumption: 0.10138984976804774
At round 49 eta: 0.742915083808359
At round 49 a_n: 11.397856354757828
At round 49 local rounds: 9.730971164513594
At round 49 global rounds: 44.33498675691042
gradient difference: 0.5550568103790283
train() client id: f_00000-0-0 loss: 1.426272  [   32/  126]
train() client id: f_00000-0-1 loss: 1.133445  [   64/  126]
train() client id: f_00000-0-2 loss: 1.009951  [   96/  126]
train() client id: f_00000-1-0 loss: 1.025862  [   32/  126]
train() client id: f_00000-1-1 loss: 1.055344  [   64/  126]
train() client id: f_00000-1-2 loss: 1.000182  [   96/  126]
train() client id: f_00000-2-0 loss: 1.109195  [   32/  126]
train() client id: f_00000-2-1 loss: 0.988661  [   64/  126]
train() client id: f_00000-2-2 loss: 1.087774  [   96/  126]
train() client id: f_00000-3-0 loss: 1.066503  [   32/  126]
train() client id: f_00000-3-1 loss: 1.081045  [   64/  126]
train() client id: f_00000-3-2 loss: 0.881815  [   96/  126]
train() client id: f_00000-4-0 loss: 0.977027  [   32/  126]
train() client id: f_00000-4-1 loss: 1.033310  [   64/  126]
train() client id: f_00000-4-2 loss: 0.957454  [   96/  126]
train() client id: f_00000-5-0 loss: 0.976560  [   32/  126]
train() client id: f_00000-5-1 loss: 1.047918  [   64/  126]
train() client id: f_00000-5-2 loss: 0.922968  [   96/  126]
train() client id: f_00000-6-0 loss: 0.872917  [   32/  126]
train() client id: f_00000-6-1 loss: 1.041209  [   64/  126]
train() client id: f_00000-6-2 loss: 0.908802  [   96/  126]
train() client id: f_00000-7-0 loss: 0.970856  [   32/  126]
train() client id: f_00000-7-1 loss: 0.918349  [   64/  126]
train() client id: f_00000-7-2 loss: 1.110272  [   96/  126]
train() client id: f_00000-8-0 loss: 0.936105  [   32/  126]
train() client id: f_00000-8-1 loss: 0.866032  [   64/  126]
train() client id: f_00000-8-2 loss: 0.972873  [   96/  126]
train() client id: f_00001-0-0 loss: 0.524715  [   32/  265]
train() client id: f_00001-0-1 loss: 0.587148  [   64/  265]
train() client id: f_00001-0-2 loss: 0.545496  [   96/  265]
train() client id: f_00001-0-3 loss: 0.454792  [  128/  265]
train() client id: f_00001-0-4 loss: 0.461571  [  160/  265]
train() client id: f_00001-0-5 loss: 0.462962  [  192/  265]
train() client id: f_00001-0-6 loss: 0.614045  [  224/  265]
train() client id: f_00001-0-7 loss: 0.485582  [  256/  265]
train() client id: f_00001-1-0 loss: 0.602375  [   32/  265]
train() client id: f_00001-1-1 loss: 0.512341  [   64/  265]
train() client id: f_00001-1-2 loss: 0.576980  [   96/  265]
train() client id: f_00001-1-3 loss: 0.586866  [  128/  265]
train() client id: f_00001-1-4 loss: 0.458006  [  160/  265]
train() client id: f_00001-1-5 loss: 0.427965  [  192/  265]
train() client id: f_00001-1-6 loss: 0.408268  [  224/  265]
train() client id: f_00001-1-7 loss: 0.505739  [  256/  265]
train() client id: f_00001-2-0 loss: 0.435624  [   32/  265]
train() client id: f_00001-2-1 loss: 0.436916  [   64/  265]
train() client id: f_00001-2-2 loss: 0.527664  [   96/  265]
train() client id: f_00001-2-3 loss: 0.566766  [  128/  265]
train() client id: f_00001-2-4 loss: 0.482102  [  160/  265]
train() client id: f_00001-2-5 loss: 0.548161  [  192/  265]
train() client id: f_00001-2-6 loss: 0.597878  [  224/  265]
train() client id: f_00001-2-7 loss: 0.479651  [  256/  265]
train() client id: f_00001-3-0 loss: 0.521291  [   32/  265]
train() client id: f_00001-3-1 loss: 0.499403  [   64/  265]
train() client id: f_00001-3-2 loss: 0.544940  [   96/  265]
train() client id: f_00001-3-3 loss: 0.542708  [  128/  265]
train() client id: f_00001-3-4 loss: 0.485978  [  160/  265]
train() client id: f_00001-3-5 loss: 0.383020  [  192/  265]
train() client id: f_00001-3-6 loss: 0.536624  [  224/  265]
train() client id: f_00001-3-7 loss: 0.485815  [  256/  265]
train() client id: f_00001-4-0 loss: 0.404257  [   32/  265]
train() client id: f_00001-4-1 loss: 0.466771  [   64/  265]
train() client id: f_00001-4-2 loss: 0.707861  [   96/  265]
train() client id: f_00001-4-3 loss: 0.408261  [  128/  265]
train() client id: f_00001-4-4 loss: 0.476233  [  160/  265]
train() client id: f_00001-4-5 loss: 0.494407  [  192/  265]
train() client id: f_00001-4-6 loss: 0.499651  [  224/  265]
train() client id: f_00001-4-7 loss: 0.563344  [  256/  265]
train() client id: f_00001-5-0 loss: 0.463816  [   32/  265]
train() client id: f_00001-5-1 loss: 0.481418  [   64/  265]
train() client id: f_00001-5-2 loss: 0.586288  [   96/  265]
train() client id: f_00001-5-3 loss: 0.467021  [  128/  265]
train() client id: f_00001-5-4 loss: 0.390225  [  160/  265]
train() client id: f_00001-5-5 loss: 0.461637  [  192/  265]
train() client id: f_00001-5-6 loss: 0.679502  [  224/  265]
train() client id: f_00001-5-7 loss: 0.479929  [  256/  265]
train() client id: f_00001-6-0 loss: 0.473804  [   32/  265]
train() client id: f_00001-6-1 loss: 0.660991  [   64/  265]
train() client id: f_00001-6-2 loss: 0.577951  [   96/  265]
train() client id: f_00001-6-3 loss: 0.399188  [  128/  265]
train() client id: f_00001-6-4 loss: 0.520210  [  160/  265]
train() client id: f_00001-6-5 loss: 0.440138  [  192/  265]
train() client id: f_00001-6-6 loss: 0.443591  [  224/  265]
train() client id: f_00001-6-7 loss: 0.474360  [  256/  265]
train() client id: f_00001-7-0 loss: 0.571549  [   32/  265]
train() client id: f_00001-7-1 loss: 0.449054  [   64/  265]
train() client id: f_00001-7-2 loss: 0.454312  [   96/  265]
train() client id: f_00001-7-3 loss: 0.415361  [  128/  265]
train() client id: f_00001-7-4 loss: 0.574473  [  160/  265]
train() client id: f_00001-7-5 loss: 0.413144  [  192/  265]
train() client id: f_00001-7-6 loss: 0.507478  [  224/  265]
train() client id: f_00001-7-7 loss: 0.615764  [  256/  265]
train() client id: f_00001-8-0 loss: 0.445493  [   32/  265]
train() client id: f_00001-8-1 loss: 0.638449  [   64/  265]
train() client id: f_00001-8-2 loss: 0.650865  [   96/  265]
train() client id: f_00001-8-3 loss: 0.411652  [  128/  265]
train() client id: f_00001-8-4 loss: 0.448657  [  160/  265]
train() client id: f_00001-8-5 loss: 0.502943  [  192/  265]
train() client id: f_00001-8-6 loss: 0.418929  [  224/  265]
train() client id: f_00001-8-7 loss: 0.489390  [  256/  265]
train() client id: f_00002-0-0 loss: 1.082467  [   32/  124]
train() client id: f_00002-0-1 loss: 1.037133  [   64/  124]
train() client id: f_00002-0-2 loss: 1.320752  [   96/  124]
train() client id: f_00002-1-0 loss: 1.255108  [   32/  124]
train() client id: f_00002-1-1 loss: 1.009371  [   64/  124]
train() client id: f_00002-1-2 loss: 1.104843  [   96/  124]
train() client id: f_00002-2-0 loss: 1.347042  [   32/  124]
train() client id: f_00002-2-1 loss: 0.943466  [   64/  124]
train() client id: f_00002-2-2 loss: 1.096645  [   96/  124]
train() client id: f_00002-3-0 loss: 1.063942  [   32/  124]
train() client id: f_00002-3-1 loss: 1.130582  [   64/  124]
train() client id: f_00002-3-2 loss: 0.981833  [   96/  124]
train() client id: f_00002-4-0 loss: 1.079693  [   32/  124]
train() client id: f_00002-4-1 loss: 1.113909  [   64/  124]
train() client id: f_00002-4-2 loss: 1.127231  [   96/  124]
train() client id: f_00002-5-0 loss: 1.007045  [   32/  124]
train() client id: f_00002-5-1 loss: 1.107576  [   64/  124]
train() client id: f_00002-5-2 loss: 1.101572  [   96/  124]
train() client id: f_00002-6-0 loss: 1.044680  [   32/  124]
train() client id: f_00002-6-1 loss: 0.978122  [   64/  124]
train() client id: f_00002-6-2 loss: 0.903695  [   96/  124]
train() client id: f_00002-7-0 loss: 0.964671  [   32/  124]
train() client id: f_00002-7-1 loss: 1.231016  [   64/  124]
train() client id: f_00002-7-2 loss: 0.974549  [   96/  124]
train() client id: f_00002-8-0 loss: 1.278666  [   32/  124]
train() client id: f_00002-8-1 loss: 1.024534  [   64/  124]
train() client id: f_00002-8-2 loss: 0.987770  [   96/  124]
train() client id: f_00003-0-0 loss: 0.655626  [   32/   43]
train() client id: f_00003-1-0 loss: 0.711692  [   32/   43]
train() client id: f_00003-2-0 loss: 0.574563  [   32/   43]
train() client id: f_00003-3-0 loss: 0.859198  [   32/   43]
train() client id: f_00003-4-0 loss: 0.776885  [   32/   43]
train() client id: f_00003-5-0 loss: 0.803242  [   32/   43]
train() client id: f_00003-6-0 loss: 0.850749  [   32/   43]
train() client id: f_00003-7-0 loss: 0.835202  [   32/   43]
train() client id: f_00003-8-0 loss: 0.619163  [   32/   43]
train() client id: f_00004-0-0 loss: 1.003365  [   32/  306]
train() client id: f_00004-0-1 loss: 0.717670  [   64/  306]
train() client id: f_00004-0-2 loss: 0.900242  [   96/  306]
train() client id: f_00004-0-3 loss: 0.855278  [  128/  306]
train() client id: f_00004-0-4 loss: 0.921975  [  160/  306]
train() client id: f_00004-0-5 loss: 0.812966  [  192/  306]
train() client id: f_00004-0-6 loss: 0.796389  [  224/  306]
train() client id: f_00004-0-7 loss: 0.817106  [  256/  306]
train() client id: f_00004-0-8 loss: 0.867854  [  288/  306]
train() client id: f_00004-1-0 loss: 0.727832  [   32/  306]
train() client id: f_00004-1-1 loss: 0.849641  [   64/  306]
train() client id: f_00004-1-2 loss: 0.852168  [   96/  306]
train() client id: f_00004-1-3 loss: 0.863209  [  128/  306]
train() client id: f_00004-1-4 loss: 0.866510  [  160/  306]
train() client id: f_00004-1-5 loss: 0.853466  [  192/  306]
train() client id: f_00004-1-6 loss: 1.005242  [  224/  306]
train() client id: f_00004-1-7 loss: 0.826536  [  256/  306]
train() client id: f_00004-1-8 loss: 0.784982  [  288/  306]
train() client id: f_00004-2-0 loss: 0.865636  [   32/  306]
train() client id: f_00004-2-1 loss: 0.876074  [   64/  306]
train() client id: f_00004-2-2 loss: 0.824361  [   96/  306]
train() client id: f_00004-2-3 loss: 0.732615  [  128/  306]
train() client id: f_00004-2-4 loss: 0.815863  [  160/  306]
train() client id: f_00004-2-5 loss: 1.048940  [  192/  306]
train() client id: f_00004-2-6 loss: 0.913632  [  224/  306]
train() client id: f_00004-2-7 loss: 0.771864  [  256/  306]
train() client id: f_00004-2-8 loss: 0.721545  [  288/  306]
train() client id: f_00004-3-0 loss: 0.874057  [   32/  306]
train() client id: f_00004-3-1 loss: 0.914075  [   64/  306]
train() client id: f_00004-3-2 loss: 0.734871  [   96/  306]
train() client id: f_00004-3-3 loss: 0.967794  [  128/  306]
train() client id: f_00004-3-4 loss: 0.718537  [  160/  306]
train() client id: f_00004-3-5 loss: 0.763969  [  192/  306]
train() client id: f_00004-3-6 loss: 0.756239  [  224/  306]
train() client id: f_00004-3-7 loss: 0.836424  [  256/  306]
train() client id: f_00004-3-8 loss: 1.011962  [  288/  306]
train() client id: f_00004-4-0 loss: 0.937781  [   32/  306]
train() client id: f_00004-4-1 loss: 0.919772  [   64/  306]
train() client id: f_00004-4-2 loss: 0.795404  [   96/  306]
train() client id: f_00004-4-3 loss: 0.908492  [  128/  306]
train() client id: f_00004-4-4 loss: 0.815058  [  160/  306]
train() client id: f_00004-4-5 loss: 0.881347  [  192/  306]
train() client id: f_00004-4-6 loss: 0.925652  [  224/  306]
train() client id: f_00004-4-7 loss: 0.816640  [  256/  306]
train() client id: f_00004-4-8 loss: 0.697962  [  288/  306]
train() client id: f_00004-5-0 loss: 0.806078  [   32/  306]
train() client id: f_00004-5-1 loss: 0.890909  [   64/  306]
train() client id: f_00004-5-2 loss: 0.809670  [   96/  306]
train() client id: f_00004-5-3 loss: 0.920635  [  128/  306]
train() client id: f_00004-5-4 loss: 0.912542  [  160/  306]
train() client id: f_00004-5-5 loss: 0.696712  [  192/  306]
train() client id: f_00004-5-6 loss: 0.757171  [  224/  306]
train() client id: f_00004-5-7 loss: 0.881671  [  256/  306]
train() client id: f_00004-5-8 loss: 0.898571  [  288/  306]
train() client id: f_00004-6-0 loss: 0.906595  [   32/  306]
train() client id: f_00004-6-1 loss: 0.814555  [   64/  306]
train() client id: f_00004-6-2 loss: 0.796463  [   96/  306]
train() client id: f_00004-6-3 loss: 0.821675  [  128/  306]
train() client id: f_00004-6-4 loss: 0.740096  [  160/  306]
train() client id: f_00004-6-5 loss: 0.994494  [  192/  306]
train() client id: f_00004-6-6 loss: 0.852964  [  224/  306]
train() client id: f_00004-6-7 loss: 0.940993  [  256/  306]
train() client id: f_00004-6-8 loss: 0.825893  [  288/  306]
train() client id: f_00004-7-0 loss: 0.990958  [   32/  306]
train() client id: f_00004-7-1 loss: 0.897207  [   64/  306]
train() client id: f_00004-7-2 loss: 0.754154  [   96/  306]
train() client id: f_00004-7-3 loss: 0.797991  [  128/  306]
train() client id: f_00004-7-4 loss: 0.764688  [  160/  306]
train() client id: f_00004-7-5 loss: 0.799580  [  192/  306]
train() client id: f_00004-7-6 loss: 0.967360  [  224/  306]
train() client id: f_00004-7-7 loss: 0.785358  [  256/  306]
train() client id: f_00004-7-8 loss: 0.749793  [  288/  306]
train() client id: f_00004-8-0 loss: 0.795283  [   32/  306]
train() client id: f_00004-8-1 loss: 0.933710  [   64/  306]
train() client id: f_00004-8-2 loss: 0.810024  [   96/  306]
train() client id: f_00004-8-3 loss: 0.983202  [  128/  306]
train() client id: f_00004-8-4 loss: 0.821451  [  160/  306]
train() client id: f_00004-8-5 loss: 0.819139  [  192/  306]
train() client id: f_00004-8-6 loss: 0.819571  [  224/  306]
train() client id: f_00004-8-7 loss: 0.840834  [  256/  306]
train() client id: f_00004-8-8 loss: 0.855753  [  288/  306]
train() client id: f_00005-0-0 loss: 0.408895  [   32/  146]
train() client id: f_00005-0-1 loss: 0.642453  [   64/  146]
train() client id: f_00005-0-2 loss: 0.428316  [   96/  146]
train() client id: f_00005-0-3 loss: 0.544253  [  128/  146]
train() client id: f_00005-1-0 loss: 0.443153  [   32/  146]
train() client id: f_00005-1-1 loss: 0.582727  [   64/  146]
train() client id: f_00005-1-2 loss: 0.344233  [   96/  146]
train() client id: f_00005-1-3 loss: 0.382965  [  128/  146]
train() client id: f_00005-2-0 loss: 0.431184  [   32/  146]
train() client id: f_00005-2-1 loss: 0.778122  [   64/  146]
train() client id: f_00005-2-2 loss: 0.221039  [   96/  146]
train() client id: f_00005-2-3 loss: 0.347437  [  128/  146]
train() client id: f_00005-3-0 loss: 0.457921  [   32/  146]
train() client id: f_00005-3-1 loss: 0.387389  [   64/  146]
train() client id: f_00005-3-2 loss: 0.344136  [   96/  146]
train() client id: f_00005-3-3 loss: 0.642912  [  128/  146]
train() client id: f_00005-4-0 loss: 0.174536  [   32/  146]
train() client id: f_00005-4-1 loss: 0.502824  [   64/  146]
train() client id: f_00005-4-2 loss: 0.643376  [   96/  146]
train() client id: f_00005-4-3 loss: 0.517220  [  128/  146]
train() client id: f_00005-5-0 loss: 0.336997  [   32/  146]
train() client id: f_00005-5-1 loss: 0.389702  [   64/  146]
train() client id: f_00005-5-2 loss: 0.483478  [   96/  146]
train() client id: f_00005-5-3 loss: 0.473794  [  128/  146]
train() client id: f_00005-6-0 loss: 0.497238  [   32/  146]
train() client id: f_00005-6-1 loss: 0.245248  [   64/  146]
train() client id: f_00005-6-2 loss: 0.531214  [   96/  146]
train() client id: f_00005-6-3 loss: 0.587621  [  128/  146]
train() client id: f_00005-7-0 loss: 0.394825  [   32/  146]
train() client id: f_00005-7-1 loss: 0.270788  [   64/  146]
train() client id: f_00005-7-2 loss: 0.747879  [   96/  146]
train() client id: f_00005-7-3 loss: 0.469411  [  128/  146]
train() client id: f_00005-8-0 loss: 0.626875  [   32/  146]
train() client id: f_00005-8-1 loss: 0.641958  [   64/  146]
train() client id: f_00005-8-2 loss: 0.182395  [   96/  146]
train() client id: f_00005-8-3 loss: 0.286141  [  128/  146]
train() client id: f_00006-0-0 loss: 0.539572  [   32/   54]
train() client id: f_00006-1-0 loss: 0.427330  [   32/   54]
train() client id: f_00006-2-0 loss: 0.478859  [   32/   54]
train() client id: f_00006-3-0 loss: 0.529281  [   32/   54]
train() client id: f_00006-4-0 loss: 0.467836  [   32/   54]
train() client id: f_00006-5-0 loss: 0.501281  [   32/   54]
train() client id: f_00006-6-0 loss: 0.423855  [   32/   54]
train() client id: f_00006-7-0 loss: 0.531907  [   32/   54]
train() client id: f_00006-8-0 loss: 0.521584  [   32/   54]
train() client id: f_00007-0-0 loss: 0.583118  [   32/  179]
train() client id: f_00007-0-1 loss: 0.751770  [   64/  179]
train() client id: f_00007-0-2 loss: 0.445983  [   96/  179]
train() client id: f_00007-0-3 loss: 0.572942  [  128/  179]
train() client id: f_00007-0-4 loss: 0.569771  [  160/  179]
train() client id: f_00007-1-0 loss: 0.717370  [   32/  179]
train() client id: f_00007-1-1 loss: 0.512676  [   64/  179]
train() client id: f_00007-1-2 loss: 0.579595  [   96/  179]
train() client id: f_00007-1-3 loss: 0.560012  [  128/  179]
train() client id: f_00007-1-4 loss: 0.517240  [  160/  179]
train() client id: f_00007-2-0 loss: 0.543787  [   32/  179]
train() client id: f_00007-2-1 loss: 0.486575  [   64/  179]
train() client id: f_00007-2-2 loss: 0.667977  [   96/  179]
train() client id: f_00007-2-3 loss: 0.488117  [  128/  179]
train() client id: f_00007-2-4 loss: 0.611389  [  160/  179]
train() client id: f_00007-3-0 loss: 0.462565  [   32/  179]
train() client id: f_00007-3-1 loss: 0.480474  [   64/  179]
train() client id: f_00007-3-2 loss: 0.750387  [   96/  179]
train() client id: f_00007-3-3 loss: 0.489300  [  128/  179]
train() client id: f_00007-3-4 loss: 0.678370  [  160/  179]
train() client id: f_00007-4-0 loss: 0.523229  [   32/  179]
train() client id: f_00007-4-1 loss: 0.617215  [   64/  179]
train() client id: f_00007-4-2 loss: 0.427731  [   96/  179]
train() client id: f_00007-4-3 loss: 0.407339  [  128/  179]
train() client id: f_00007-4-4 loss: 0.768976  [  160/  179]
train() client id: f_00007-5-0 loss: 0.379595  [   32/  179]
train() client id: f_00007-5-1 loss: 0.655038  [   64/  179]
train() client id: f_00007-5-2 loss: 0.570149  [   96/  179]
train() client id: f_00007-5-3 loss: 0.681084  [  128/  179]
train() client id: f_00007-5-4 loss: 0.512148  [  160/  179]
train() client id: f_00007-6-0 loss: 0.623973  [   32/  179]
train() client id: f_00007-6-1 loss: 0.462515  [   64/  179]
train() client id: f_00007-6-2 loss: 0.409023  [   96/  179]
train() client id: f_00007-6-3 loss: 0.584107  [  128/  179]
train() client id: f_00007-6-4 loss: 0.494030  [  160/  179]
train() client id: f_00007-7-0 loss: 0.347995  [   32/  179]
train() client id: f_00007-7-1 loss: 0.568567  [   64/  179]
train() client id: f_00007-7-2 loss: 0.577117  [   96/  179]
train() client id: f_00007-7-3 loss: 0.536913  [  128/  179]
train() client id: f_00007-7-4 loss: 0.517587  [  160/  179]
train() client id: f_00007-8-0 loss: 0.617005  [   32/  179]
train() client id: f_00007-8-1 loss: 0.648528  [   64/  179]
train() client id: f_00007-8-2 loss: 0.387324  [   96/  179]
train() client id: f_00007-8-3 loss: 0.592474  [  128/  179]
train() client id: f_00007-8-4 loss: 0.359683  [  160/  179]
train() client id: f_00008-0-0 loss: 0.836345  [   32/  130]
train() client id: f_00008-0-1 loss: 0.745355  [   64/  130]
train() client id: f_00008-0-2 loss: 0.741110  [   96/  130]
train() client id: f_00008-0-3 loss: 0.638166  [  128/  130]
train() client id: f_00008-1-0 loss: 0.749669  [   32/  130]
train() client id: f_00008-1-1 loss: 0.710118  [   64/  130]
train() client id: f_00008-1-2 loss: 0.705127  [   96/  130]
train() client id: f_00008-1-3 loss: 0.802251  [  128/  130]
train() client id: f_00008-2-0 loss: 0.771952  [   32/  130]
train() client id: f_00008-2-1 loss: 0.704426  [   64/  130]
train() client id: f_00008-2-2 loss: 0.773362  [   96/  130]
train() client id: f_00008-2-3 loss: 0.713063  [  128/  130]
train() client id: f_00008-3-0 loss: 0.649979  [   32/  130]
train() client id: f_00008-3-1 loss: 0.727486  [   64/  130]
train() client id: f_00008-3-2 loss: 0.754262  [   96/  130]
train() client id: f_00008-3-3 loss: 0.829791  [  128/  130]
train() client id: f_00008-4-0 loss: 0.841572  [   32/  130]
train() client id: f_00008-4-1 loss: 0.669937  [   64/  130]
train() client id: f_00008-4-2 loss: 0.676006  [   96/  130]
train() client id: f_00008-4-3 loss: 0.774181  [  128/  130]
train() client id: f_00008-5-0 loss: 0.678119  [   32/  130]
train() client id: f_00008-5-1 loss: 0.834888  [   64/  130]
train() client id: f_00008-5-2 loss: 0.710211  [   96/  130]
train() client id: f_00008-5-3 loss: 0.736227  [  128/  130]
train() client id: f_00008-6-0 loss: 0.658763  [   32/  130]
train() client id: f_00008-6-1 loss: 0.783570  [   64/  130]
train() client id: f_00008-6-2 loss: 0.802857  [   96/  130]
train() client id: f_00008-6-3 loss: 0.710556  [  128/  130]
train() client id: f_00008-7-0 loss: 0.825648  [   32/  130]
train() client id: f_00008-7-1 loss: 0.664865  [   64/  130]
train() client id: f_00008-7-2 loss: 0.746425  [   96/  130]
train() client id: f_00008-7-3 loss: 0.733935  [  128/  130]
train() client id: f_00008-8-0 loss: 0.693268  [   32/  130]
train() client id: f_00008-8-1 loss: 0.706088  [   64/  130]
train() client id: f_00008-8-2 loss: 0.755653  [   96/  130]
train() client id: f_00008-8-3 loss: 0.770771  [  128/  130]
train() client id: f_00009-0-0 loss: 0.922253  [   32/  118]
train() client id: f_00009-0-1 loss: 1.109522  [   64/  118]
train() client id: f_00009-0-2 loss: 1.044001  [   96/  118]
train() client id: f_00009-1-0 loss: 0.954136  [   32/  118]
train() client id: f_00009-1-1 loss: 0.895044  [   64/  118]
train() client id: f_00009-1-2 loss: 1.049084  [   96/  118]
train() client id: f_00009-2-0 loss: 1.108779  [   32/  118]
train() client id: f_00009-2-1 loss: 1.007805  [   64/  118]
train() client id: f_00009-2-2 loss: 0.878465  [   96/  118]
train() client id: f_00009-3-0 loss: 0.854121  [   32/  118]
train() client id: f_00009-3-1 loss: 0.856616  [   64/  118]
train() client id: f_00009-3-2 loss: 1.126151  [   96/  118]
train() client id: f_00009-4-0 loss: 1.081300  [   32/  118]
train() client id: f_00009-4-1 loss: 0.775537  [   64/  118]
train() client id: f_00009-4-2 loss: 0.913649  [   96/  118]
train() client id: f_00009-5-0 loss: 1.064495  [   32/  118]
train() client id: f_00009-5-1 loss: 0.848194  [   64/  118]
train() client id: f_00009-5-2 loss: 0.736342  [   96/  118]
train() client id: f_00009-6-0 loss: 0.910485  [   32/  118]
train() client id: f_00009-6-1 loss: 1.049478  [   64/  118]
train() client id: f_00009-6-2 loss: 0.855622  [   96/  118]
train() client id: f_00009-7-0 loss: 0.927492  [   32/  118]
train() client id: f_00009-7-1 loss: 0.910985  [   64/  118]
train() client id: f_00009-7-2 loss: 0.879046  [   96/  118]
train() client id: f_00009-8-0 loss: 0.778765  [   32/  118]
train() client id: f_00009-8-1 loss: 0.914777  [   64/  118]
train() client id: f_00009-8-2 loss: 0.894467  [   96/  118]
At round 49 accuracy: 0.6445623342175066
At round 49 training accuracy: 0.5888665325285044
At round 49 training loss: 0.8325706581127256
update_location
xs = -4.528292 146.001589 155.045120 -155.943528 44.896481 -80.217951 -197.215960 218.375741 -1.680116 139.695607 
ys = 232.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -162.154970 4.001482 
xs mean: 26.44286915941286
ys mean: 9.371751218646876
dists_uav = 253.214661 177.646976 184.501309 186.619005 109.618047 128.229634 221.142262 240.947601 190.517865 171.845496 
uav_gains = -111.571820 -106.278062 -106.711647 -106.844469 -100.997111 -102.700252 -109.069234 -110.547473 -107.088270 -105.904471 
uav_gains_db_mean: -106.77128096444497
dists_bs = 179.936034 358.419393 372.953496 153.641563 281.437949 196.549240 179.504976 438.690590 379.095313 358.153345 
bs_gains = -102.710367 -111.090021 -111.573390 -100.789303 -108.149774 -103.784257 -102.681201 -113.547492 -111.772015 -111.080991 
bs_gains_db_mean: -107.71788119932393
Round 50
-------------------------------
ene_coms = [0.01195539 0.01153859 0.00871763 0.00878362 0.00948695 0.00747351
 0.01005023 0.01110029 0.01213679 0.01153103]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.37704303  9.05375135  4.27352927  1.54627783 10.41167445  5.00057138
  1.93088965  6.15247419  4.51378426  4.10287328]
obj_prev = 51.36286868013636
eta_min = 1.8179304642661205e-21	eta_max = 0.9507764427088279
af = 10.795049746072358	bf = 1.1361987464642604	zeta = 11.874554720679596	eta = 0.909090909090909
af = 10.795049746072358	bf = 1.1361987464642604	zeta = 23.815845666349503	eta = 0.45327173753586997
af = 10.795049746072358	bf = 1.1361987464642604	zeta = 17.701530989216348	eta = 0.6098370673501986
af = 10.795049746072358	bf = 1.1361987464642604	zeta = 16.59572286355404	eta = 0.6504718013687386
af = 10.795049746072358	bf = 1.1361987464642604	zeta = 16.53282639953079	eta = 0.6529464161299563
af = 10.795049746072358	bf = 1.1361987464642604	zeta = 16.532602446335503	eta = 0.6529552610433157
eta = 0.6529552610433157
ene_coms = [0.01195539 0.01153859 0.00871763 0.00878362 0.00948695 0.00747351
 0.01005023 0.01110029 0.01213679 0.01153103]
ene_comp = [0.03517285 0.07397465 0.03461455 0.01200343 0.08541979 0.04075585
 0.01507408 0.04996778 0.03628945 0.03293966]
ene_total = [1.50129731 2.72407352 1.38037159 0.6621835  3.02330902 1.53637402
 0.80034946 1.94535867 1.54264599 1.41663935]
ti_comp = [0.74371066 0.74787869 0.77608828 0.77542837 0.76839499 0.78852942
 0.76276222 0.75226166 0.7418966  0.74795424]
ti_coms = [0.11955387 0.11538585 0.08717626 0.08783617 0.09486955 0.07473512
 0.10050232 0.11100288 0.12136793 0.11531029]
t_total = [27.49979019 27.49979019 27.49979019 27.49979019 27.49979019 27.49979019
 27.49979019 27.49979019 27.49979019 27.49979019]
ene_coms = [0.01195539 0.01153859 0.00871763 0.00878362 0.00948695 0.00747351
 0.01005023 0.01110029 0.01213679 0.01153103]
ene_comp = [4.91693874e-06 4.52341555e-05 4.30362064e-06 1.79768326e-07
 6.59760650e-05 6.80477770e-06 3.67954496e-07 1.37788324e-05
 5.42668320e-06 3.99288808e-06]
ene_total = [0.3810024  0.36900925 0.27784209 0.27981291 0.30431413 0.2382898
 0.32016767 0.35404501 0.38679743 0.36745479]
optimize_network iter = 0 obj = 3.278735471636884
eta = 0.6529552610433157
freqs = [23646866.26047672 49456317.42881675 22300654.89805067  7739872.88298246
 55583253.00902981 25842945.52264338  9881244.50774416 33211704.24547745
 24457216.45620771 22019833.66050049]
eta_min = 0.6529552610433184	eta_max = 0.7486648888964381
af = 0.003915923654136672	bf = 1.1361987464642604	zeta = 0.00430751601955034	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01195539 0.01153859 0.00871763 0.00878362 0.00948695 0.00747351
 0.01005023 0.01110029 0.01213679 0.01153103]
ene_comp = [1.09944395e-06 1.01145084e-05 9.62303967e-07 4.01967989e-08
 1.47524688e-05 1.52157105e-06 8.22758559e-08 3.08099300e-06
 1.21342451e-06 8.92823129e-07]
ene_total = [1.45400943 1.40441906 1.06025365 1.06816661 1.15548756 0.90902695
 1.22220281 1.3502631  1.47608377 1.40237877]
ti_comp = [0.50563551 0.50980354 0.53801313 0.53735322 0.53031984 0.55045427
 0.52468707 0.51418651 0.50382146 0.5098791 ]
ti_coms = [0.11955387 0.11538585 0.08717626 0.08783617 0.09486955 0.07473512
 0.10050232 0.11100288 0.12136793 0.11531029]
t_total = [27.49979019 27.49979019 27.49979019 27.49979019 27.49979019 27.49979019
 27.49979019 27.49979019 27.49979019 27.49979019]
ene_coms = [0.01195539 0.01153859 0.00871763 0.00878362 0.00948695 0.00747351
 0.01005023 0.01110029 0.01213679 0.01153103]
ene_comp = [3.33140551e-06 3.04875787e-05 2.80459596e-06 1.17240229e-07
 4.33789996e-05 4.37327793e-06 2.43540707e-07 9.23652776e-06
 3.68525670e-06 2.69093130e-06]
ene_total = [0.52602022 0.50888111 0.38357977 0.38636427 0.41920446 0.32892476
 0.44208359 0.48866727 0.53401516 0.50732608]
optimize_network iter = 1 obj = 4.52506669180829
eta = 0.7486648888964381
freqs = [23619641.87878228 49270090.69236103 21845859.73793619  7584883.8153761
 54692009.91399779 25140386.0530596   9755145.08453626 32996865.42483376
 24457216.45620772 21935883.59377256]
eta_min = 0.7486648888964291	eta_max = 0.748664888896431
af = 0.00383258025984263	bf = 1.1361987464642604	zeta = 0.004215838285826893	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01195539 0.01153859 0.00871763 0.00878362 0.00948695 0.00747351
 0.01005023 0.01110029 0.01213679 0.01153103]
ene_comp = [1.09691385e-06 1.00384799e-05 9.23454122e-07 3.86030552e-08
 1.42831682e-05 1.43996554e-06 8.01893296e-08 3.04126146e-06
 1.21342451e-06 8.86028374e-07]
ene_total = [1.45400912 1.40440981 1.06024893 1.06816642 1.15543049 0.90901703
 1.22220255 1.35025826 1.47608377 1.40237794]
ti_comp = [0.50563551 0.50980354 0.53801313 0.53735322 0.53031984 0.55045427
 0.52468707 0.51418651 0.50382146 0.5098791 ]
ti_coms = [0.11955387 0.11538585 0.08717626 0.08783617 0.09486955 0.07473512
 0.10050232 0.11100288 0.12136793 0.11531029]
t_total = [27.49979019 27.49979019 27.49979019 27.49979019 27.49979019 27.49979019
 27.49979019 27.49979019 27.49979019 27.49979019]
ene_coms = [0.01195539 0.01153859 0.00871763 0.00878362 0.00948695 0.00747351
 0.01005023 0.01110029 0.01213679 0.01153103]
ene_comp = [3.33140551e-06 3.04875787e-05 2.80459596e-06 1.17240229e-07
 4.33789996e-05 4.37327793e-06 2.43540707e-07 9.23652776e-06
 3.68525670e-06 2.69093130e-06]
ene_total = [0.52602022 0.50888111 0.38357977 0.38636427 0.41920446 0.32892476
 0.44208359 0.48866727 0.53401516 0.50732608]
optimize_network iter = 2 obj = 4.525066691808162
eta = 0.748664888896431
freqs = [23619641.87878222 49270090.69236093 21845859.73793619  7584883.81537609
 54692009.91399775 25140386.05305961  9755145.08453625 32996865.4248337
 24457216.45620766 21935883.5937725 ]
Done!
ene_coms = [0.01195539 0.01153859 0.00871763 0.00878362 0.00948695 0.00747351
 0.01005023 0.01110029 0.01213679 0.01153103]
ene_comp = [3.16322203e-06 2.89484364e-05 2.66300806e-06 1.11321445e-07
 4.11890436e-05 4.15249630e-06 2.31245739e-07 8.77022864e-06
 3.49920931e-06 2.55508167e-06]
ene_total = [0.01195855 0.01156753 0.00872029 0.00878373 0.00952814 0.00747766
 0.01005046 0.01110906 0.01214029 0.01153358]
At round 50 energy consumption: 0.10286930720359838
At round 50 eta: 0.748664888896431
At round 50 a_n: 11.055310507788512
At round 50 local rounds: 9.478515691604294
At round 50 global rounds: 43.98633545168662
gradient difference: 0.5382505059242249
train() client id: f_00000-0-0 loss: 0.932927  [   32/  126]
train() client id: f_00000-0-1 loss: 1.007983  [   64/  126]
train() client id: f_00000-0-2 loss: 0.949667  [   96/  126]
train() client id: f_00000-1-0 loss: 0.920636  [   32/  126]
train() client id: f_00000-1-1 loss: 0.972531  [   64/  126]
train() client id: f_00000-1-2 loss: 0.944385  [   96/  126]
train() client id: f_00000-2-0 loss: 0.839620  [   32/  126]
train() client id: f_00000-2-1 loss: 0.803612  [   64/  126]
train() client id: f_00000-2-2 loss: 0.997934  [   96/  126]
train() client id: f_00000-3-0 loss: 0.943308  [   32/  126]
train() client id: f_00000-3-1 loss: 0.819467  [   64/  126]
train() client id: f_00000-3-2 loss: 0.872622  [   96/  126]
train() client id: f_00000-4-0 loss: 0.895905  [   32/  126]
train() client id: f_00000-4-1 loss: 0.682710  [   64/  126]
train() client id: f_00000-4-2 loss: 0.848344  [   96/  126]
train() client id: f_00000-5-0 loss: 0.741714  [   32/  126]
train() client id: f_00000-5-1 loss: 0.920207  [   64/  126]
train() client id: f_00000-5-2 loss: 0.793825  [   96/  126]
train() client id: f_00000-6-0 loss: 0.913571  [   32/  126]
train() client id: f_00000-6-1 loss: 0.697527  [   64/  126]
train() client id: f_00000-6-2 loss: 0.819895  [   96/  126]
train() client id: f_00000-7-0 loss: 0.808049  [   32/  126]
train() client id: f_00000-7-1 loss: 0.789620  [   64/  126]
train() client id: f_00000-7-2 loss: 0.889573  [   96/  126]
train() client id: f_00000-8-0 loss: 0.843358  [   32/  126]
train() client id: f_00000-8-1 loss: 0.710038  [   64/  126]
train() client id: f_00000-8-2 loss: 0.928434  [   96/  126]
train() client id: f_00001-0-0 loss: 0.441585  [   32/  265]
train() client id: f_00001-0-1 loss: 0.560855  [   64/  265]
train() client id: f_00001-0-2 loss: 0.428174  [   96/  265]
train() client id: f_00001-0-3 loss: 0.468107  [  128/  265]
train() client id: f_00001-0-4 loss: 0.561073  [  160/  265]
train() client id: f_00001-0-5 loss: 0.442408  [  192/  265]
train() client id: f_00001-0-6 loss: 0.398256  [  224/  265]
train() client id: f_00001-0-7 loss: 0.443709  [  256/  265]
train() client id: f_00001-1-0 loss: 0.469027  [   32/  265]
train() client id: f_00001-1-1 loss: 0.577032  [   64/  265]
train() client id: f_00001-1-2 loss: 0.365235  [   96/  265]
train() client id: f_00001-1-3 loss: 0.473959  [  128/  265]
train() client id: f_00001-1-4 loss: 0.434276  [  160/  265]
train() client id: f_00001-1-5 loss: 0.467189  [  192/  265]
train() client id: f_00001-1-6 loss: 0.561258  [  224/  265]
train() client id: f_00001-1-7 loss: 0.358372  [  256/  265]
train() client id: f_00001-2-0 loss: 0.426467  [   32/  265]
train() client id: f_00001-2-1 loss: 0.514759  [   64/  265]
train() client id: f_00001-2-2 loss: 0.438065  [   96/  265]
train() client id: f_00001-2-3 loss: 0.457044  [  128/  265]
train() client id: f_00001-2-4 loss: 0.367829  [  160/  265]
train() client id: f_00001-2-5 loss: 0.546035  [  192/  265]
train() client id: f_00001-2-6 loss: 0.545758  [  224/  265]
train() client id: f_00001-2-7 loss: 0.358423  [  256/  265]
train() client id: f_00001-3-0 loss: 0.355904  [   32/  265]
train() client id: f_00001-3-1 loss: 0.439277  [   64/  265]
train() client id: f_00001-3-2 loss: 0.446153  [   96/  265]
train() client id: f_00001-3-3 loss: 0.570717  [  128/  265]
train() client id: f_00001-3-4 loss: 0.475802  [  160/  265]
train() client id: f_00001-3-5 loss: 0.407814  [  192/  265]
train() client id: f_00001-3-6 loss: 0.372263  [  224/  265]
train() client id: f_00001-3-7 loss: 0.549271  [  256/  265]
train() client id: f_00001-4-0 loss: 0.432588  [   32/  265]
train() client id: f_00001-4-1 loss: 0.580952  [   64/  265]
train() client id: f_00001-4-2 loss: 0.422591  [   96/  265]
train() client id: f_00001-4-3 loss: 0.425057  [  128/  265]
train() client id: f_00001-4-4 loss: 0.365617  [  160/  265]
train() client id: f_00001-4-5 loss: 0.396184  [  192/  265]
train() client id: f_00001-4-6 loss: 0.447148  [  224/  265]
train() client id: f_00001-4-7 loss: 0.464049  [  256/  265]
train() client id: f_00001-5-0 loss: 0.429326  [   32/  265]
train() client id: f_00001-5-1 loss: 0.440713  [   64/  265]
train() client id: f_00001-5-2 loss: 0.336352  [   96/  265]
train() client id: f_00001-5-3 loss: 0.367757  [  128/  265]
train() client id: f_00001-5-4 loss: 0.506374  [  160/  265]
train() client id: f_00001-5-5 loss: 0.501141  [  192/  265]
train() client id: f_00001-5-6 loss: 0.449245  [  224/  265]
train() client id: f_00001-5-7 loss: 0.533281  [  256/  265]
train() client id: f_00001-6-0 loss: 0.382652  [   32/  265]
train() client id: f_00001-6-1 loss: 0.416673  [   64/  265]
train() client id: f_00001-6-2 loss: 0.418902  [   96/  265]
train() client id: f_00001-6-3 loss: 0.447041  [  128/  265]
train() client id: f_00001-6-4 loss: 0.473541  [  160/  265]
train() client id: f_00001-6-5 loss: 0.459005  [  192/  265]
train() client id: f_00001-6-6 loss: 0.577333  [  224/  265]
train() client id: f_00001-6-7 loss: 0.368739  [  256/  265]
train() client id: f_00001-7-0 loss: 0.477760  [   32/  265]
train() client id: f_00001-7-1 loss: 0.426897  [   64/  265]
train() client id: f_00001-7-2 loss: 0.400639  [   96/  265]
train() client id: f_00001-7-3 loss: 0.349770  [  128/  265]
train() client id: f_00001-7-4 loss: 0.502091  [  160/  265]
train() client id: f_00001-7-5 loss: 0.530501  [  192/  265]
train() client id: f_00001-7-6 loss: 0.471134  [  224/  265]
train() client id: f_00001-7-7 loss: 0.370694  [  256/  265]
train() client id: f_00001-8-0 loss: 0.342346  [   32/  265]
train() client id: f_00001-8-1 loss: 0.471768  [   64/  265]
train() client id: f_00001-8-2 loss: 0.361698  [   96/  265]
train() client id: f_00001-8-3 loss: 0.510660  [  128/  265]
train() client id: f_00001-8-4 loss: 0.555172  [  160/  265]
train() client id: f_00001-8-5 loss: 0.501398  [  192/  265]
train() client id: f_00001-8-6 loss: 0.325133  [  224/  265]
train() client id: f_00001-8-7 loss: 0.330553  [  256/  265]
train() client id: f_00002-0-0 loss: 0.964262  [   32/  124]
train() client id: f_00002-0-1 loss: 0.994732  [   64/  124]
train() client id: f_00002-0-2 loss: 1.097765  [   96/  124]
train() client id: f_00002-1-0 loss: 0.957886  [   32/  124]
train() client id: f_00002-1-1 loss: 1.183919  [   64/  124]
train() client id: f_00002-1-2 loss: 1.095939  [   96/  124]
train() client id: f_00002-2-0 loss: 1.425047  [   32/  124]
train() client id: f_00002-2-1 loss: 1.198422  [   64/  124]
train() client id: f_00002-2-2 loss: 0.847964  [   96/  124]
train() client id: f_00002-3-0 loss: 1.134516  [   32/  124]
train() client id: f_00002-3-1 loss: 1.009394  [   64/  124]
train() client id: f_00002-3-2 loss: 1.089263  [   96/  124]
train() client id: f_00002-4-0 loss: 1.122724  [   32/  124]
train() client id: f_00002-4-1 loss: 0.868722  [   64/  124]
train() client id: f_00002-4-2 loss: 1.175057  [   96/  124]
train() client id: f_00002-5-0 loss: 1.188573  [   32/  124]
train() client id: f_00002-5-1 loss: 1.043808  [   64/  124]
train() client id: f_00002-5-2 loss: 0.892721  [   96/  124]
train() client id: f_00002-6-0 loss: 1.138725  [   32/  124]
train() client id: f_00002-6-1 loss: 1.124890  [   64/  124]
train() client id: f_00002-6-2 loss: 0.865559  [   96/  124]
train() client id: f_00002-7-0 loss: 0.897874  [   32/  124]
train() client id: f_00002-7-1 loss: 0.806449  [   64/  124]
train() client id: f_00002-7-2 loss: 1.031980  [   96/  124]
train() client id: f_00002-8-0 loss: 1.001198  [   32/  124]
train() client id: f_00002-8-1 loss: 1.034166  [   64/  124]
train() client id: f_00002-8-2 loss: 0.908803  [   96/  124]
train() client id: f_00003-0-0 loss: 1.044239  [   32/   43]
train() client id: f_00003-1-0 loss: 0.817689  [   32/   43]
train() client id: f_00003-2-0 loss: 0.932259  [   32/   43]
train() client id: f_00003-3-0 loss: 0.805058  [   32/   43]
train() client id: f_00003-4-0 loss: 0.929711  [   32/   43]
train() client id: f_00003-5-0 loss: 0.919459  [   32/   43]
train() client id: f_00003-6-0 loss: 0.747973  [   32/   43]
train() client id: f_00003-7-0 loss: 0.768714  [   32/   43]
train() client id: f_00003-8-0 loss: 0.586666  [   32/   43]
train() client id: f_00004-0-0 loss: 0.854190  [   32/  306]
train() client id: f_00004-0-1 loss: 0.730070  [   64/  306]
train() client id: f_00004-0-2 loss: 0.904492  [   96/  306]
train() client id: f_00004-0-3 loss: 0.912677  [  128/  306]
train() client id: f_00004-0-4 loss: 0.895326  [  160/  306]
train() client id: f_00004-0-5 loss: 0.843801  [  192/  306]
train() client id: f_00004-0-6 loss: 1.085876  [  224/  306]
train() client id: f_00004-0-7 loss: 0.910361  [  256/  306]
train() client id: f_00004-0-8 loss: 0.805401  [  288/  306]
train() client id: f_00004-1-0 loss: 0.920452  [   32/  306]
train() client id: f_00004-1-1 loss: 0.990029  [   64/  306]
train() client id: f_00004-1-2 loss: 0.893360  [   96/  306]
train() client id: f_00004-1-3 loss: 0.794461  [  128/  306]
train() client id: f_00004-1-4 loss: 0.878872  [  160/  306]
train() client id: f_00004-1-5 loss: 0.960296  [  192/  306]
train() client id: f_00004-1-6 loss: 0.849255  [  224/  306]
train() client id: f_00004-1-7 loss: 0.936986  [  256/  306]
train() client id: f_00004-1-8 loss: 0.827438  [  288/  306]
train() client id: f_00004-2-0 loss: 0.771124  [   32/  306]
train() client id: f_00004-2-1 loss: 0.899774  [   64/  306]
train() client id: f_00004-2-2 loss: 0.906399  [   96/  306]
train() client id: f_00004-2-3 loss: 0.898661  [  128/  306]
train() client id: f_00004-2-4 loss: 1.148290  [  160/  306]
train() client id: f_00004-2-5 loss: 0.884542  [  192/  306]
train() client id: f_00004-2-6 loss: 0.807975  [  224/  306]
train() client id: f_00004-2-7 loss: 0.740502  [  256/  306]
train() client id: f_00004-2-8 loss: 0.938108  [  288/  306]
train() client id: f_00004-3-0 loss: 0.939424  [   32/  306]
train() client id: f_00004-3-1 loss: 0.839461  [   64/  306]
train() client id: f_00004-3-2 loss: 0.772382  [   96/  306]
train() client id: f_00004-3-3 loss: 0.845553  [  128/  306]
train() client id: f_00004-3-4 loss: 0.907127  [  160/  306]
train() client id: f_00004-3-5 loss: 0.856810  [  192/  306]
train() client id: f_00004-3-6 loss: 0.948793  [  224/  306]
train() client id: f_00004-3-7 loss: 0.979312  [  256/  306]
train() client id: f_00004-3-8 loss: 0.949028  [  288/  306]
train() client id: f_00004-4-0 loss: 0.863211  [   32/  306]
train() client id: f_00004-4-1 loss: 1.012228  [   64/  306]
train() client id: f_00004-4-2 loss: 0.819973  [   96/  306]
train() client id: f_00004-4-3 loss: 0.816313  [  128/  306]
train() client id: f_00004-4-4 loss: 0.880709  [  160/  306]
train() client id: f_00004-4-5 loss: 0.830017  [  192/  306]
train() client id: f_00004-4-6 loss: 1.076570  [  224/  306]
train() client id: f_00004-4-7 loss: 0.838850  [  256/  306]
train() client id: f_00004-4-8 loss: 0.893789  [  288/  306]
train() client id: f_00004-5-0 loss: 0.911969  [   32/  306]
train() client id: f_00004-5-1 loss: 0.871282  [   64/  306]
train() client id: f_00004-5-2 loss: 0.929002  [   96/  306]
train() client id: f_00004-5-3 loss: 0.907213  [  128/  306]
train() client id: f_00004-5-4 loss: 0.850405  [  160/  306]
train() client id: f_00004-5-5 loss: 0.865557  [  192/  306]
train() client id: f_00004-5-6 loss: 0.819169  [  224/  306]
train() client id: f_00004-5-7 loss: 0.901640  [  256/  306]
train() client id: f_00004-5-8 loss: 0.928476  [  288/  306]
train() client id: f_00004-6-0 loss: 0.858479  [   32/  306]
train() client id: f_00004-6-1 loss: 0.878185  [   64/  306]
train() client id: f_00004-6-2 loss: 0.970307  [   96/  306]
train() client id: f_00004-6-3 loss: 0.768560  [  128/  306]
train() client id: f_00004-6-4 loss: 0.822782  [  160/  306]
train() client id: f_00004-6-5 loss: 0.944325  [  192/  306]
train() client id: f_00004-6-6 loss: 0.802451  [  224/  306]
train() client id: f_00004-6-7 loss: 0.939358  [  256/  306]
train() client id: f_00004-6-8 loss: 0.846150  [  288/  306]
train() client id: f_00004-7-0 loss: 0.801365  [   32/  306]
train() client id: f_00004-7-1 loss: 0.934589  [   64/  306]
train() client id: f_00004-7-2 loss: 0.898332  [   96/  306]
train() client id: f_00004-7-3 loss: 0.793327  [  128/  306]
train() client id: f_00004-7-4 loss: 0.836494  [  160/  306]
train() client id: f_00004-7-5 loss: 1.083349  [  192/  306]
train() client id: f_00004-7-6 loss: 0.930285  [  224/  306]
train() client id: f_00004-7-7 loss: 0.924441  [  256/  306]
train() client id: f_00004-7-8 loss: 0.730474  [  288/  306]
train() client id: f_00004-8-0 loss: 0.839047  [   32/  306]
train() client id: f_00004-8-1 loss: 0.830564  [   64/  306]
train() client id: f_00004-8-2 loss: 0.895445  [   96/  306]
train() client id: f_00004-8-3 loss: 0.889235  [  128/  306]
train() client id: f_00004-8-4 loss: 0.937526  [  160/  306]
train() client id: f_00004-8-5 loss: 0.963927  [  192/  306]
train() client id: f_00004-8-6 loss: 0.884815  [  224/  306]
train() client id: f_00004-8-7 loss: 0.878784  [  256/  306]
train() client id: f_00004-8-8 loss: 0.758711  [  288/  306]
train() client id: f_00005-0-0 loss: 0.997088  [   32/  146]
train() client id: f_00005-0-1 loss: 1.149029  [   64/  146]
train() client id: f_00005-0-2 loss: 0.559215  [   96/  146]
train() client id: f_00005-0-3 loss: 0.507963  [  128/  146]
train() client id: f_00005-1-0 loss: 0.933428  [   32/  146]
train() client id: f_00005-1-1 loss: 0.783948  [   64/  146]
train() client id: f_00005-1-2 loss: 0.665843  [   96/  146]
train() client id: f_00005-1-3 loss: 0.744687  [  128/  146]
train() client id: f_00005-2-0 loss: 0.636500  [   32/  146]
train() client id: f_00005-2-1 loss: 0.778310  [   64/  146]
train() client id: f_00005-2-2 loss: 0.792358  [   96/  146]
train() client id: f_00005-2-3 loss: 0.980379  [  128/  146]
train() client id: f_00005-3-0 loss: 0.858329  [   32/  146]
train() client id: f_00005-3-1 loss: 0.778865  [   64/  146]
train() client id: f_00005-3-2 loss: 0.824127  [   96/  146]
train() client id: f_00005-3-3 loss: 0.641130  [  128/  146]
train() client id: f_00005-4-0 loss: 0.695302  [   32/  146]
train() client id: f_00005-4-1 loss: 0.596170  [   64/  146]
train() client id: f_00005-4-2 loss: 1.141030  [   96/  146]
train() client id: f_00005-4-3 loss: 0.662334  [  128/  146]
train() client id: f_00005-5-0 loss: 0.639515  [   32/  146]
train() client id: f_00005-5-1 loss: 0.919338  [   64/  146]
train() client id: f_00005-5-2 loss: 0.734608  [   96/  146]
train() client id: f_00005-5-3 loss: 0.637592  [  128/  146]
train() client id: f_00005-6-0 loss: 0.768403  [   32/  146]
train() client id: f_00005-6-1 loss: 0.645377  [   64/  146]
train() client id: f_00005-6-2 loss: 0.620999  [   96/  146]
train() client id: f_00005-6-3 loss: 0.818262  [  128/  146]
train() client id: f_00005-7-0 loss: 0.929068  [   32/  146]
train() client id: f_00005-7-1 loss: 0.674393  [   64/  146]
train() client id: f_00005-7-2 loss: 0.694616  [   96/  146]
train() client id: f_00005-7-3 loss: 0.782373  [  128/  146]
train() client id: f_00005-8-0 loss: 0.684367  [   32/  146]
train() client id: f_00005-8-1 loss: 0.589773  [   64/  146]
train() client id: f_00005-8-2 loss: 1.148457  [   96/  146]
train() client id: f_00005-8-3 loss: 0.536457  [  128/  146]
train() client id: f_00006-0-0 loss: 0.499108  [   32/   54]
train() client id: f_00006-1-0 loss: 0.405533  [   32/   54]
train() client id: f_00006-2-0 loss: 0.444863  [   32/   54]
train() client id: f_00006-3-0 loss: 0.502417  [   32/   54]
train() client id: f_00006-4-0 loss: 0.501186  [   32/   54]
train() client id: f_00006-5-0 loss: 0.446566  [   32/   54]
train() client id: f_00006-6-0 loss: 0.395238  [   32/   54]
train() client id: f_00006-7-0 loss: 0.438972  [   32/   54]
train() client id: f_00006-8-0 loss: 0.510721  [   32/   54]
train() client id: f_00007-0-0 loss: 0.637999  [   32/  179]
train() client id: f_00007-0-1 loss: 0.750234  [   64/  179]
train() client id: f_00007-0-2 loss: 0.571912  [   96/  179]
train() client id: f_00007-0-3 loss: 0.808776  [  128/  179]
train() client id: f_00007-0-4 loss: 0.612947  [  160/  179]
train() client id: f_00007-1-0 loss: 0.719242  [   32/  179]
train() client id: f_00007-1-1 loss: 0.732738  [   64/  179]
train() client id: f_00007-1-2 loss: 0.690968  [   96/  179]
train() client id: f_00007-1-3 loss: 0.681404  [  128/  179]
train() client id: f_00007-1-4 loss: 0.718402  [  160/  179]
train() client id: f_00007-2-0 loss: 0.757042  [   32/  179]
train() client id: f_00007-2-1 loss: 0.524646  [   64/  179]
train() client id: f_00007-2-2 loss: 0.834321  [   96/  179]
train() client id: f_00007-2-3 loss: 0.774095  [  128/  179]
train() client id: f_00007-2-4 loss: 0.605709  [  160/  179]
train() client id: f_00007-3-0 loss: 0.838851  [   32/  179]
train() client id: f_00007-3-1 loss: 0.748222  [   64/  179]
train() client id: f_00007-3-2 loss: 0.617650  [   96/  179]
train() client id: f_00007-3-3 loss: 0.641548  [  128/  179]
train() client id: f_00007-3-4 loss: 0.667698  [  160/  179]
train() client id: f_00007-4-0 loss: 0.605979  [   32/  179]
train() client id: f_00007-4-1 loss: 0.579597  [   64/  179]
train() client id: f_00007-4-2 loss: 0.693741  [   96/  179]
train() client id: f_00007-4-3 loss: 0.917275  [  128/  179]
train() client id: f_00007-4-4 loss: 0.770868  [  160/  179]
train() client id: f_00007-5-0 loss: 0.567019  [   32/  179]
train() client id: f_00007-5-1 loss: 0.586296  [   64/  179]
train() client id: f_00007-5-2 loss: 0.879941  [   96/  179]
train() client id: f_00007-5-3 loss: 0.925871  [  128/  179]
train() client id: f_00007-5-4 loss: 0.598488  [  160/  179]
train() client id: f_00007-6-0 loss: 0.528656  [   32/  179]
train() client id: f_00007-6-1 loss: 0.781704  [   64/  179]
train() client id: f_00007-6-2 loss: 0.630224  [   96/  179]
train() client id: f_00007-6-3 loss: 0.624254  [  128/  179]
train() client id: f_00007-6-4 loss: 0.717926  [  160/  179]
train() client id: f_00007-7-0 loss: 0.902216  [   32/  179]
train() client id: f_00007-7-1 loss: 0.526735  [   64/  179]
train() client id: f_00007-7-2 loss: 0.708907  [   96/  179]
train() client id: f_00007-7-3 loss: 0.662432  [  128/  179]
train() client id: f_00007-7-4 loss: 0.541528  [  160/  179]
train() client id: f_00007-8-0 loss: 0.546821  [   32/  179]
train() client id: f_00007-8-1 loss: 0.699468  [   64/  179]
train() client id: f_00007-8-2 loss: 0.927505  [   96/  179]
train() client id: f_00007-8-3 loss: 0.516826  [  128/  179]
train() client id: f_00007-8-4 loss: 0.739601  [  160/  179]
train() client id: f_00008-0-0 loss: 0.673238  [   32/  130]
train() client id: f_00008-0-1 loss: 0.611052  [   64/  130]
train() client id: f_00008-0-2 loss: 0.714810  [   96/  130]
train() client id: f_00008-0-3 loss: 0.715380  [  128/  130]
train() client id: f_00008-1-0 loss: 0.534728  [   32/  130]
train() client id: f_00008-1-1 loss: 0.786296  [   64/  130]
train() client id: f_00008-1-2 loss: 0.759262  [   96/  130]
train() client id: f_00008-1-3 loss: 0.634445  [  128/  130]
train() client id: f_00008-2-0 loss: 0.671017  [   32/  130]
train() client id: f_00008-2-1 loss: 0.642037  [   64/  130]
train() client id: f_00008-2-2 loss: 0.722359  [   96/  130]
train() client id: f_00008-2-3 loss: 0.646912  [  128/  130]
train() client id: f_00008-3-0 loss: 0.620846  [   32/  130]
train() client id: f_00008-3-1 loss: 0.596832  [   64/  130]
train() client id: f_00008-3-2 loss: 0.711785  [   96/  130]
train() client id: f_00008-3-3 loss: 0.782916  [  128/  130]
train() client id: f_00008-4-0 loss: 0.690033  [   32/  130]
train() client id: f_00008-4-1 loss: 0.664177  [   64/  130]
train() client id: f_00008-4-2 loss: 0.754127  [   96/  130]
train() client id: f_00008-4-3 loss: 0.612387  [  128/  130]
train() client id: f_00008-5-0 loss: 0.688992  [   32/  130]
train() client id: f_00008-5-1 loss: 0.577582  [   64/  130]
train() client id: f_00008-5-2 loss: 0.701655  [   96/  130]
train() client id: f_00008-5-3 loss: 0.751474  [  128/  130]
train() client id: f_00008-6-0 loss: 0.619380  [   32/  130]
train() client id: f_00008-6-1 loss: 0.816869  [   64/  130]
train() client id: f_00008-6-2 loss: 0.573829  [   96/  130]
train() client id: f_00008-6-3 loss: 0.699414  [  128/  130]
train() client id: f_00008-7-0 loss: 0.682624  [   32/  130]
train() client id: f_00008-7-1 loss: 0.738162  [   64/  130]
train() client id: f_00008-7-2 loss: 0.692131  [   96/  130]
train() client id: f_00008-7-3 loss: 0.613770  [  128/  130]
train() client id: f_00008-8-0 loss: 0.653631  [   32/  130]
train() client id: f_00008-8-1 loss: 0.695548  [   64/  130]
train() client id: f_00008-8-2 loss: 0.680270  [   96/  130]
train() client id: f_00008-8-3 loss: 0.690873  [  128/  130]
train() client id: f_00009-0-0 loss: 1.022968  [   32/  118]
train() client id: f_00009-0-1 loss: 0.750020  [   64/  118]
train() client id: f_00009-0-2 loss: 1.315108  [   96/  118]
train() client id: f_00009-1-0 loss: 0.963090  [   32/  118]
train() client id: f_00009-1-1 loss: 0.931269  [   64/  118]
train() client id: f_00009-1-2 loss: 1.032735  [   96/  118]
train() client id: f_00009-2-0 loss: 0.838274  [   32/  118]
train() client id: f_00009-2-1 loss: 0.955433  [   64/  118]
train() client id: f_00009-2-2 loss: 1.062079  [   96/  118]
train() client id: f_00009-3-0 loss: 0.759088  [   32/  118]
train() client id: f_00009-3-1 loss: 0.935115  [   64/  118]
train() client id: f_00009-3-2 loss: 0.938576  [   96/  118]
train() client id: f_00009-4-0 loss: 0.845191  [   32/  118]
train() client id: f_00009-4-1 loss: 0.929582  [   64/  118]
train() client id: f_00009-4-2 loss: 0.923468  [   96/  118]
train() client id: f_00009-5-0 loss: 0.901753  [   32/  118]
train() client id: f_00009-5-1 loss: 0.821513  [   64/  118]
train() client id: f_00009-5-2 loss: 0.933024  [   96/  118]
train() client id: f_00009-6-0 loss: 0.887977  [   32/  118]
train() client id: f_00009-6-1 loss: 0.813587  [   64/  118]
train() client id: f_00009-6-2 loss: 0.812887  [   96/  118]
train() client id: f_00009-7-0 loss: 0.802469  [   32/  118]
train() client id: f_00009-7-1 loss: 0.860820  [   64/  118]
train() client id: f_00009-7-2 loss: 0.889057  [   96/  118]
train() client id: f_00009-8-0 loss: 0.860143  [   32/  118]
train() client id: f_00009-8-1 loss: 0.867061  [   64/  118]
train() client id: f_00009-8-2 loss: 0.772405  [   96/  118]
At round 50 accuracy: 0.6472148541114059
At round 50 training accuracy: 0.5902079141515761
At round 50 training loss: 0.8187660137016934
update_location
xs = -4.528292 151.001589 160.045120 -160.943528 49.896481 -85.217951 -202.215960 223.375741 -1.680116 144.695607 
ys = 237.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -167.154970 4.001482 
xs mean: 27.44286915941286
ys mean: 9.371751218646875
dists_uav = 257.814941 181.778613 188.722506 190.816898 111.759031 131.415443 225.612631 245.488297 194.791188 175.934166 
uav_gains = -111.974219 -106.540207 -106.976085 -107.106949 -101.207144 -102.966921 -109.384390 -110.917451 -107.355249 -106.168511 
uav_gains_db_mean: -107.0597125442878
dists_bs = 181.598061 362.904225 377.385428 153.101813 285.361672 194.187495 180.192109 443.179638 383.548961 362.554512 
bs_gains = -102.822173 -111.241236 -111.717043 -100.746508 -108.318137 -103.637254 -102.727661 -113.671293 -111.914042 -111.229512 
bs_gains_db_mean: -107.8024859446484
Round 51
-------------------------------
ene_coms = [0.01232519 0.01166649 0.00884996 0.00891689 0.00958556 0.00742011
 0.01025811 0.01139575 0.01226857 0.01165648]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [ 4.24542316  8.77460731  4.14254713  1.49980899 10.09013854  4.84505218
  1.87331098  5.96503857  4.37535179  3.97710433]
obj_prev = 49.78838297919806
eta_min = 4.160126169237913e-22	eta_max = 0.9514984846463778
af = 10.460568009377656	bf = 1.117803224119295	zeta = 11.506624810315422	eta = 0.9090909090909091
af = 10.460568009377656	bf = 1.117803224119295	zeta = 23.26282026143293	eta = 0.44966895207973
af = 10.460568009377656	bf = 1.117803224119295	zeta = 17.22298895746783	eta = 0.607360780129978
af = 10.460568009377656	bf = 1.117803224119295	zeta = 16.131298082690563	eta = 0.6484641196111927
af = 10.460568009377656	bf = 1.117803224119295	zeta = 16.06886801713799	eta = 0.6509835041411198
af = 10.460568009377656	bf = 1.117803224119295	zeta = 16.068642981867065	eta = 0.65099262091902
eta = 0.65099262091902
ene_coms = [0.01232519 0.01166649 0.00884996 0.00891689 0.00958556 0.00742011
 0.01025811 0.01139575 0.01226857 0.01165648]
ene_comp = [0.03542126 0.07449709 0.03485901 0.01208821 0.08602305 0.04104368
 0.01518054 0.05032067 0.03654574 0.03317229]
ene_total = [1.46557493 2.64478678 1.34164496 0.64475049 2.93470183 1.4875937
 0.78083806 1.89438254 1.49835287 1.37601681]
ti_comp = [0.77102497 0.77761201 0.80577725 0.80510797 0.79842125 0.82007577
 0.7916958  0.78031943 0.7715912  0.77771211]
ti_coms = [0.12325192 0.11666488 0.08849964 0.08916892 0.09585564 0.07420112
 0.10258109 0.11395746 0.12268569 0.11656478]
t_total = [27.449786 27.449786 27.449786 27.449786 27.449786 27.449786 27.449786
 27.449786 27.449786 27.449786]
ene_coms = [0.01232519 0.01166649 0.00884996 0.00891689 0.00958556 0.00742011
 0.01025811 0.01139575 0.01226857 0.01165648]
ene_comp = [4.67234624e-06 4.27338451e-05 4.07751264e-06 1.70316759e-07
 6.24108893e-05 6.42555840e-06 3.48838867e-07 1.30789995e-05
 5.12407867e-06 3.77197296e-06]
ene_total = [0.37846459 0.359414   0.27177436 0.2737088  0.29614412 0.22795721
 0.31488287 0.35019335 0.3767404  0.35791082]
optimize_network iter = 0 obj = 3.2071905289770286
eta = 0.65099262091902
freqs = [22970239.52155292 47901193.68542282 21630676.50921788  7507195.94037415
 53870717.40956067 25024320.67850741  9587355.71381697 32243637.78926586
 23682061.54320398 21326842.84243184]
eta_min = 0.6509926209190218	eta_max = 0.7534903404207394
af = 0.003564150876319061	bf = 1.117803224119295	zeta = 0.003920565963950968	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01232519 0.01166649 0.00884996 0.00891689 0.00958556 0.00742011
 0.01025811 0.01139575 0.01226857 0.01165648]
ene_comp = [1.03742557e-06 9.48842006e-06 9.05351545e-07 3.78163244e-08
 1.38574175e-05 1.42670048e-06 7.74545257e-08 2.90399895e-06
 1.13772610e-06 8.37510965e-07]
ene_total = [1.45252796 1.37590187 1.04299003 1.05077468 1.13119974 0.87455717
 1.20882893 1.34322166 1.44586724 1.37370289]
ti_comp = [0.50839057 0.51497761 0.54314286 0.54247357 0.53578686 0.55744138
 0.5290614  0.51768503 0.50895681 0.51507771]
ti_coms = [0.12325192 0.11666488 0.08849964 0.08916892 0.09585564 0.07420112
 0.10258109 0.11395746 0.12268569 0.11656478]
t_total = [27.449786 27.449786 27.449786 27.449786 27.449786 27.449786 27.449786
 27.449786 27.449786 27.449786]
ene_coms = [0.01232519 0.01166649 0.00884996 0.00891689 0.00958556 0.00742011
 0.01025811 0.01139575 0.01226857 0.01165648]
ene_comp = [3.08080397e-06 2.79324050e-05 2.57267139e-06 1.07546332e-07
 3.97308244e-05 3.98664512e-06 2.23932192e-07 8.51873855e-06
 3.37610239e-06 2.46517592e-06]
ene_total = [0.53575948 0.50821365 0.38471165 0.38751308 0.41829402 0.32263491
 0.44580447 0.49560415 0.53331158 0.5066719 ]
optimize_network iter = 1 obj = 4.538518876524752
eta = 0.7534903404207394
freqs = [22970239.5215529  47692489.35418815 21159240.70936229  7346531.36051086
 52932430.8132641  24274266.98018777  9459760.66579523 32046447.47715355
 23673086.87325081 21232528.6047598 ]
eta_min = 0.7534903404207476	eta_max = 0.7534903404207071
af = 0.0034819189093590057	bf = 1.117803224119295	zeta = 0.0038301108002949066	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01232519 0.01166649 0.00884996 0.00891689 0.00958556 0.00742011
 0.01025811 0.01139575 0.01226857 0.01165648]
ene_comp = [1.03742557e-06 9.40591855e-06 8.66317726e-07 3.62149997e-08
 1.33789016e-05 1.34245724e-06 7.54066096e-08 2.86858797e-06
 1.13686394e-06 8.30119852e-07]
ene_total = [1.45252796 1.37589215 1.04298543 1.05077449 1.13114335 0.87454725
 1.20882869 1.34321749 1.44586714 1.37370202]
ti_comp = [0.50839057 0.51497761 0.54314286 0.54247357 0.53578686 0.55744138
 0.5290614  0.51768503 0.50895681 0.51507771]
ti_coms = [0.12325192 0.11666488 0.08849964 0.08916892 0.09585564 0.07420112
 0.10258109 0.11395746 0.12268569 0.11656478]
t_total = [27.449786 27.449786 27.449786 27.449786 27.449786 27.449786 27.449786
 27.449786 27.449786 27.449786]
ene_coms = [0.01232519 0.01166649 0.00884996 0.00891689 0.00958556 0.00742011
 0.01025811 0.01139575 0.01226857 0.01165648]
ene_comp = [3.08080397e-06 2.79324050e-05 2.57267139e-06 1.07546332e-07
 3.97308244e-05 3.98664512e-06 2.23932192e-07 8.51873855e-06
 3.37610239e-06 2.46517592e-06]
ene_total = [0.53575948 0.50821365 0.38471165 0.38751308 0.41829402 0.32263491
 0.44580447 0.49560415 0.53331158 0.5066719 ]
optimize_network iter = 2 obj = 4.538518876524158
eta = 0.7534903404207071
freqs = [22970239.52155264 47692489.3541877  21159240.70936228  7346531.36051085
 52932430.81326395 24274266.98018784  9459760.66579519 32046447.47715328
 23673086.87325055 21232528.6047596 ]
Done!
ene_coms = [0.01232519 0.01166649 0.00884996 0.00891689 0.00958556 0.00742011
 0.01025811 0.01139575 0.01226857 0.01165648]
ene_comp = [2.99167289e-06 2.71242895e-05 2.49824115e-06 1.04434897e-07
 3.85813675e-05 3.87130705e-06 2.17453585e-07 8.27228198e-06
 3.27842800e-06 2.39385564e-06]
ene_total = [0.01232818 0.01169361 0.00885246 0.008917   0.00962415 0.00742398
 0.01025833 0.01140402 0.01227185 0.01165887]
At round 51 energy consumption: 0.10443244731914789
At round 51 eta: 0.7534903404207071
At round 51 a_n: 10.712764660819193
At round 51 local rounds: 9.26813751021529
At round 51 global rounds: 43.45778854711897
gradient difference: 0.4761212468147278
train() client id: f_00000-0-0 loss: 1.074815  [   32/  126]
train() client id: f_00000-0-1 loss: 1.112538  [   64/  126]
train() client id: f_00000-0-2 loss: 1.118698  [   96/  126]
train() client id: f_00000-1-0 loss: 1.005234  [   32/  126]
train() client id: f_00000-1-1 loss: 0.920782  [   64/  126]
train() client id: f_00000-1-2 loss: 1.040952  [   96/  126]
train() client id: f_00000-2-0 loss: 1.053935  [   32/  126]
train() client id: f_00000-2-1 loss: 0.776572  [   64/  126]
train() client id: f_00000-2-2 loss: 0.950203  [   96/  126]
train() client id: f_00000-3-0 loss: 0.873675  [   32/  126]
train() client id: f_00000-3-1 loss: 0.897221  [   64/  126]
train() client id: f_00000-3-2 loss: 0.926211  [   96/  126]
train() client id: f_00000-4-0 loss: 0.973218  [   32/  126]
train() client id: f_00000-4-1 loss: 0.784292  [   64/  126]
train() client id: f_00000-4-2 loss: 0.703955  [   96/  126]
train() client id: f_00000-5-0 loss: 0.835391  [   32/  126]
train() client id: f_00000-5-1 loss: 0.677594  [   64/  126]
train() client id: f_00000-5-2 loss: 0.932657  [   96/  126]
train() client id: f_00000-6-0 loss: 0.844142  [   32/  126]
train() client id: f_00000-6-1 loss: 0.724848  [   64/  126]
train() client id: f_00000-6-2 loss: 0.743333  [   96/  126]
train() client id: f_00000-7-0 loss: 0.849257  [   32/  126]
train() client id: f_00000-7-1 loss: 0.754822  [   64/  126]
train() client id: f_00000-7-2 loss: 0.639744  [   96/  126]
train() client id: f_00000-8-0 loss: 0.641745  [   32/  126]
train() client id: f_00000-8-1 loss: 0.735533  [   64/  126]
train() client id: f_00000-8-2 loss: 0.773198  [   96/  126]
train() client id: f_00001-0-0 loss: 0.661531  [   32/  265]
train() client id: f_00001-0-1 loss: 0.484170  [   64/  265]
train() client id: f_00001-0-2 loss: 0.516848  [   96/  265]
train() client id: f_00001-0-3 loss: 0.449987  [  128/  265]
train() client id: f_00001-0-4 loss: 0.423316  [  160/  265]
train() client id: f_00001-0-5 loss: 0.497152  [  192/  265]
train() client id: f_00001-0-6 loss: 0.547583  [  224/  265]
train() client id: f_00001-0-7 loss: 0.554807  [  256/  265]
train() client id: f_00001-1-0 loss: 0.475840  [   32/  265]
train() client id: f_00001-1-1 loss: 0.559247  [   64/  265]
train() client id: f_00001-1-2 loss: 0.436418  [   96/  265]
train() client id: f_00001-1-3 loss: 0.484713  [  128/  265]
train() client id: f_00001-1-4 loss: 0.499703  [  160/  265]
train() client id: f_00001-1-5 loss: 0.520925  [  192/  265]
train() client id: f_00001-1-6 loss: 0.522795  [  224/  265]
train() client id: f_00001-1-7 loss: 0.480445  [  256/  265]
train() client id: f_00001-2-0 loss: 0.465324  [   32/  265]
train() client id: f_00001-2-1 loss: 0.549588  [   64/  265]
train() client id: f_00001-2-2 loss: 0.520725  [   96/  265]
train() client id: f_00001-2-3 loss: 0.430554  [  128/  265]
train() client id: f_00001-2-4 loss: 0.508403  [  160/  265]
train() client id: f_00001-2-5 loss: 0.476027  [  192/  265]
train() client id: f_00001-2-6 loss: 0.410879  [  224/  265]
train() client id: f_00001-2-7 loss: 0.625100  [  256/  265]
train() client id: f_00001-3-0 loss: 0.525148  [   32/  265]
train() client id: f_00001-3-1 loss: 0.447165  [   64/  265]
train() client id: f_00001-3-2 loss: 0.452985  [   96/  265]
train() client id: f_00001-3-3 loss: 0.535358  [  128/  265]
train() client id: f_00001-3-4 loss: 0.401552  [  160/  265]
train() client id: f_00001-3-5 loss: 0.432258  [  192/  265]
train() client id: f_00001-3-6 loss: 0.485850  [  224/  265]
train() client id: f_00001-3-7 loss: 0.512092  [  256/  265]
train() client id: f_00001-4-0 loss: 0.457090  [   32/  265]
train() client id: f_00001-4-1 loss: 0.540146  [   64/  265]
train() client id: f_00001-4-2 loss: 0.480793  [   96/  265]
train() client id: f_00001-4-3 loss: 0.448090  [  128/  265]
train() client id: f_00001-4-4 loss: 0.649356  [  160/  265]
train() client id: f_00001-4-5 loss: 0.397818  [  192/  265]
train() client id: f_00001-4-6 loss: 0.599605  [  224/  265]
train() client id: f_00001-4-7 loss: 0.418789  [  256/  265]
train() client id: f_00001-5-0 loss: 0.428805  [   32/  265]
train() client id: f_00001-5-1 loss: 0.691554  [   64/  265]
train() client id: f_00001-5-2 loss: 0.517855  [   96/  265]
train() client id: f_00001-5-3 loss: 0.407422  [  128/  265]
train() client id: f_00001-5-4 loss: 0.452232  [  160/  265]
train() client id: f_00001-5-5 loss: 0.526451  [  192/  265]
train() client id: f_00001-5-6 loss: 0.481514  [  224/  265]
train() client id: f_00001-5-7 loss: 0.470133  [  256/  265]
train() client id: f_00001-6-0 loss: 0.455705  [   32/  265]
train() client id: f_00001-6-1 loss: 0.547739  [   64/  265]
train() client id: f_00001-6-2 loss: 0.462697  [   96/  265]
train() client id: f_00001-6-3 loss: 0.442240  [  128/  265]
train() client id: f_00001-6-4 loss: 0.527037  [  160/  265]
train() client id: f_00001-6-5 loss: 0.469547  [  192/  265]
train() client id: f_00001-6-6 loss: 0.635711  [  224/  265]
train() client id: f_00001-6-7 loss: 0.413318  [  256/  265]
train() client id: f_00001-7-0 loss: 0.398333  [   32/  265]
train() client id: f_00001-7-1 loss: 0.624039  [   64/  265]
train() client id: f_00001-7-2 loss: 0.428609  [   96/  265]
train() client id: f_00001-7-3 loss: 0.524278  [  128/  265]
train() client id: f_00001-7-4 loss: 0.394078  [  160/  265]
train() client id: f_00001-7-5 loss: 0.478017  [  192/  265]
train() client id: f_00001-7-6 loss: 0.476626  [  224/  265]
train() client id: f_00001-7-7 loss: 0.578786  [  256/  265]
train() client id: f_00001-8-0 loss: 0.570013  [   32/  265]
train() client id: f_00001-8-1 loss: 0.466204  [   64/  265]
train() client id: f_00001-8-2 loss: 0.391035  [   96/  265]
train() client id: f_00001-8-3 loss: 0.493399  [  128/  265]
train() client id: f_00001-8-4 loss: 0.467044  [  160/  265]
train() client id: f_00001-8-5 loss: 0.424581  [  192/  265]
train() client id: f_00001-8-6 loss: 0.444842  [  224/  265]
train() client id: f_00001-8-7 loss: 0.650120  [  256/  265]
train() client id: f_00002-0-0 loss: 1.289650  [   32/  124]
train() client id: f_00002-0-1 loss: 1.155891  [   64/  124]
train() client id: f_00002-0-2 loss: 1.183151  [   96/  124]
train() client id: f_00002-1-0 loss: 1.181549  [   32/  124]
train() client id: f_00002-1-1 loss: 1.192854  [   64/  124]
train() client id: f_00002-1-2 loss: 1.234833  [   96/  124]
train() client id: f_00002-2-0 loss: 1.059820  [   32/  124]
train() client id: f_00002-2-1 loss: 1.291094  [   64/  124]
train() client id: f_00002-2-2 loss: 1.111864  [   96/  124]
train() client id: f_00002-3-0 loss: 1.083905  [   32/  124]
train() client id: f_00002-3-1 loss: 1.093776  [   64/  124]
train() client id: f_00002-3-2 loss: 1.161803  [   96/  124]
train() client id: f_00002-4-0 loss: 0.973855  [   32/  124]
train() client id: f_00002-4-1 loss: 1.066066  [   64/  124]
train() client id: f_00002-4-2 loss: 1.073734  [   96/  124]
train() client id: f_00002-5-0 loss: 1.132959  [   32/  124]
train() client id: f_00002-5-1 loss: 1.065680  [   64/  124]
train() client id: f_00002-5-2 loss: 1.056437  [   96/  124]
train() client id: f_00002-6-0 loss: 0.923545  [   32/  124]
train() client id: f_00002-6-1 loss: 1.244180  [   64/  124]
train() client id: f_00002-6-2 loss: 1.131665  [   96/  124]
train() client id: f_00002-7-0 loss: 1.107532  [   32/  124]
train() client id: f_00002-7-1 loss: 1.063468  [   64/  124]
train() client id: f_00002-7-2 loss: 0.894697  [   96/  124]
train() client id: f_00002-8-0 loss: 1.030809  [   32/  124]
train() client id: f_00002-8-1 loss: 1.056899  [   64/  124]
train() client id: f_00002-8-2 loss: 1.087526  [   96/  124]
train() client id: f_00003-0-0 loss: 0.874159  [   32/   43]
train() client id: f_00003-1-0 loss: 0.919331  [   32/   43]
train() client id: f_00003-2-0 loss: 0.869668  [   32/   43]
train() client id: f_00003-3-0 loss: 0.998044  [   32/   43]
train() client id: f_00003-4-0 loss: 0.886746  [   32/   43]
train() client id: f_00003-5-0 loss: 0.798973  [   32/   43]
train() client id: f_00003-6-0 loss: 0.981161  [   32/   43]
train() client id: f_00003-7-0 loss: 0.884056  [   32/   43]
train() client id: f_00003-8-0 loss: 0.840167  [   32/   43]
train() client id: f_00004-0-0 loss: 1.037198  [   32/  306]
train() client id: f_00004-0-1 loss: 0.939607  [   64/  306]
train() client id: f_00004-0-2 loss: 0.960404  [   96/  306]
train() client id: f_00004-0-3 loss: 0.958853  [  128/  306]
train() client id: f_00004-0-4 loss: 0.829977  [  160/  306]
train() client id: f_00004-0-5 loss: 0.812461  [  192/  306]
train() client id: f_00004-0-6 loss: 0.922809  [  224/  306]
train() client id: f_00004-0-7 loss: 1.093301  [  256/  306]
train() client id: f_00004-0-8 loss: 0.893134  [  288/  306]
train() client id: f_00004-1-0 loss: 0.873975  [   32/  306]
train() client id: f_00004-1-1 loss: 0.878997  [   64/  306]
train() client id: f_00004-1-2 loss: 0.998779  [   96/  306]
train() client id: f_00004-1-3 loss: 0.918150  [  128/  306]
train() client id: f_00004-1-4 loss: 1.011147  [  160/  306]
train() client id: f_00004-1-5 loss: 0.991451  [  192/  306]
train() client id: f_00004-1-6 loss: 0.859501  [  224/  306]
train() client id: f_00004-1-7 loss: 1.123601  [  256/  306]
train() client id: f_00004-1-8 loss: 0.838758  [  288/  306]
train() client id: f_00004-2-0 loss: 1.007595  [   32/  306]
train() client id: f_00004-2-1 loss: 1.015966  [   64/  306]
train() client id: f_00004-2-2 loss: 0.883509  [   96/  306]
train() client id: f_00004-2-3 loss: 0.960079  [  128/  306]
train() client id: f_00004-2-4 loss: 0.863738  [  160/  306]
train() client id: f_00004-2-5 loss: 1.060021  [  192/  306]
train() client id: f_00004-2-6 loss: 0.858823  [  224/  306]
train() client id: f_00004-2-7 loss: 0.882068  [  256/  306]
train() client id: f_00004-2-8 loss: 0.987601  [  288/  306]
train() client id: f_00004-3-0 loss: 0.853987  [   32/  306]
train() client id: f_00004-3-1 loss: 1.031320  [   64/  306]
train() client id: f_00004-3-2 loss: 0.872707  [   96/  306]
train() client id: f_00004-3-3 loss: 0.936875  [  128/  306]
train() client id: f_00004-3-4 loss: 0.976792  [  160/  306]
train() client id: f_00004-3-5 loss: 0.947197  [  192/  306]
train() client id: f_00004-3-6 loss: 0.959130  [  224/  306]
train() client id: f_00004-3-7 loss: 0.899879  [  256/  306]
train() client id: f_00004-3-8 loss: 0.895374  [  288/  306]
train() client id: f_00004-4-0 loss: 0.803277  [   32/  306]
train() client id: f_00004-4-1 loss: 0.985565  [   64/  306]
train() client id: f_00004-4-2 loss: 0.867834  [   96/  306]
train() client id: f_00004-4-3 loss: 1.102412  [  128/  306]
train() client id: f_00004-4-4 loss: 1.029897  [  160/  306]
train() client id: f_00004-4-5 loss: 0.868692  [  192/  306]
train() client id: f_00004-4-6 loss: 0.911063  [  224/  306]
train() client id: f_00004-4-7 loss: 0.940615  [  256/  306]
train() client id: f_00004-4-8 loss: 0.900581  [  288/  306]
train() client id: f_00004-5-0 loss: 0.903509  [   32/  306]
train() client id: f_00004-5-1 loss: 0.906663  [   64/  306]
train() client id: f_00004-5-2 loss: 1.050992  [   96/  306]
train() client id: f_00004-5-3 loss: 0.794881  [  128/  306]
train() client id: f_00004-5-4 loss: 1.021254  [  160/  306]
train() client id: f_00004-5-5 loss: 0.858907  [  192/  306]
train() client id: f_00004-5-6 loss: 0.971871  [  224/  306]
train() client id: f_00004-5-7 loss: 0.989641  [  256/  306]
train() client id: f_00004-5-8 loss: 0.936478  [  288/  306]
train() client id: f_00004-6-0 loss: 1.000697  [   32/  306]
train() client id: f_00004-6-1 loss: 0.975528  [   64/  306]
train() client id: f_00004-6-2 loss: 0.839021  [   96/  306]
train() client id: f_00004-6-3 loss: 0.925771  [  128/  306]
train() client id: f_00004-6-4 loss: 0.994833  [  160/  306]
train() client id: f_00004-6-5 loss: 0.969074  [  192/  306]
train() client id: f_00004-6-6 loss: 0.967131  [  224/  306]
train() client id: f_00004-6-7 loss: 0.888935  [  256/  306]
train() client id: f_00004-6-8 loss: 0.877390  [  288/  306]
train() client id: f_00004-7-0 loss: 0.879456  [   32/  306]
train() client id: f_00004-7-1 loss: 0.898688  [   64/  306]
train() client id: f_00004-7-2 loss: 1.118630  [   96/  306]
train() client id: f_00004-7-3 loss: 0.835677  [  128/  306]
train() client id: f_00004-7-4 loss: 1.055425  [  160/  306]
train() client id: f_00004-7-5 loss: 0.925323  [  192/  306]
train() client id: f_00004-7-6 loss: 0.877938  [  224/  306]
train() client id: f_00004-7-7 loss: 0.858842  [  256/  306]
train() client id: f_00004-7-8 loss: 0.953704  [  288/  306]
train() client id: f_00004-8-0 loss: 0.884316  [   32/  306]
train() client id: f_00004-8-1 loss: 0.813992  [   64/  306]
train() client id: f_00004-8-2 loss: 1.017846  [   96/  306]
train() client id: f_00004-8-3 loss: 0.988248  [  128/  306]
train() client id: f_00004-8-4 loss: 0.904558  [  160/  306]
train() client id: f_00004-8-5 loss: 0.963382  [  192/  306]
train() client id: f_00004-8-6 loss: 1.042216  [  224/  306]
train() client id: f_00004-8-7 loss: 0.976410  [  256/  306]
train() client id: f_00004-8-8 loss: 0.780106  [  288/  306]
train() client id: f_00005-0-0 loss: 0.777523  [   32/  146]
train() client id: f_00005-0-1 loss: 0.493508  [   64/  146]
train() client id: f_00005-0-2 loss: 1.036226  [   96/  146]
train() client id: f_00005-0-3 loss: 0.966257  [  128/  146]
train() client id: f_00005-1-0 loss: 0.775780  [   32/  146]
train() client id: f_00005-1-1 loss: 1.248552  [   64/  146]
train() client id: f_00005-1-2 loss: 0.623066  [   96/  146]
train() client id: f_00005-1-3 loss: 0.962888  [  128/  146]
train() client id: f_00005-2-0 loss: 1.313761  [   32/  146]
train() client id: f_00005-2-1 loss: 0.799276  [   64/  146]
train() client id: f_00005-2-2 loss: 0.601513  [   96/  146]
train() client id: f_00005-2-3 loss: 0.795980  [  128/  146]
train() client id: f_00005-3-0 loss: 1.093115  [   32/  146]
train() client id: f_00005-3-1 loss: 0.785579  [   64/  146]
train() client id: f_00005-3-2 loss: 0.684066  [   96/  146]
train() client id: f_00005-3-3 loss: 0.888399  [  128/  146]
train() client id: f_00005-4-0 loss: 0.671387  [   32/  146]
train() client id: f_00005-4-1 loss: 0.818869  [   64/  146]
train() client id: f_00005-4-2 loss: 0.791245  [   96/  146]
train() client id: f_00005-4-3 loss: 1.096070  [  128/  146]
train() client id: f_00005-5-0 loss: 0.800913  [   32/  146]
train() client id: f_00005-5-1 loss: 0.779120  [   64/  146]
train() client id: f_00005-5-2 loss: 0.837523  [   96/  146]
train() client id: f_00005-5-3 loss: 1.060613  [  128/  146]
train() client id: f_00005-6-0 loss: 1.152584  [   32/  146]
train() client id: f_00005-6-1 loss: 0.693212  [   64/  146]
train() client id: f_00005-6-2 loss: 0.794677  [   96/  146]
train() client id: f_00005-6-3 loss: 0.862582  [  128/  146]
train() client id: f_00005-7-0 loss: 0.777586  [   32/  146]
train() client id: f_00005-7-1 loss: 1.064543  [   64/  146]
train() client id: f_00005-7-2 loss: 0.693290  [   96/  146]
train() client id: f_00005-7-3 loss: 0.887769  [  128/  146]
train() client id: f_00005-8-0 loss: 0.823661  [   32/  146]
train() client id: f_00005-8-1 loss: 0.621209  [   64/  146]
train() client id: f_00005-8-2 loss: 1.022252  [   96/  146]
train() client id: f_00005-8-3 loss: 0.863921  [  128/  146]
train() client id: f_00006-0-0 loss: 0.413221  [   32/   54]
train() client id: f_00006-1-0 loss: 0.489544  [   32/   54]
train() client id: f_00006-2-0 loss: 0.501945  [   32/   54]
train() client id: f_00006-3-0 loss: 0.439711  [   32/   54]
train() client id: f_00006-4-0 loss: 0.404389  [   32/   54]
train() client id: f_00006-5-0 loss: 0.491521  [   32/   54]
train() client id: f_00006-6-0 loss: 0.517404  [   32/   54]
train() client id: f_00006-7-0 loss: 0.440600  [   32/   54]
train() client id: f_00006-8-0 loss: 0.443602  [   32/   54]
train() client id: f_00007-0-0 loss: 0.673231  [   32/  179]
train() client id: f_00007-0-1 loss: 0.389869  [   64/  179]
train() client id: f_00007-0-2 loss: 0.475889  [   96/  179]
train() client id: f_00007-0-3 loss: 0.485694  [  128/  179]
train() client id: f_00007-0-4 loss: 0.516801  [  160/  179]
train() client id: f_00007-1-0 loss: 0.656311  [   32/  179]
train() client id: f_00007-1-1 loss: 0.398570  [   64/  179]
train() client id: f_00007-1-2 loss: 0.538070  [   96/  179]
train() client id: f_00007-1-3 loss: 0.625771  [  128/  179]
train() client id: f_00007-1-4 loss: 0.501631  [  160/  179]
train() client id: f_00007-2-0 loss: 0.397946  [   32/  179]
train() client id: f_00007-2-1 loss: 0.520200  [   64/  179]
train() client id: f_00007-2-2 loss: 0.667514  [   96/  179]
train() client id: f_00007-2-3 loss: 0.605092  [  128/  179]
train() client id: f_00007-2-4 loss: 0.463755  [  160/  179]
train() client id: f_00007-3-0 loss: 0.662949  [   32/  179]
train() client id: f_00007-3-1 loss: 0.580965  [   64/  179]
train() client id: f_00007-3-2 loss: 0.557710  [   96/  179]
train() client id: f_00007-3-3 loss: 0.377045  [  128/  179]
train() client id: f_00007-3-4 loss: 0.554204  [  160/  179]
train() client id: f_00007-4-0 loss: 0.377839  [   32/  179]
train() client id: f_00007-4-1 loss: 0.534251  [   64/  179]
train() client id: f_00007-4-2 loss: 0.704529  [   96/  179]
train() client id: f_00007-4-3 loss: 0.637776  [  128/  179]
train() client id: f_00007-4-4 loss: 0.425541  [  160/  179]
train() client id: f_00007-5-0 loss: 0.592195  [   32/  179]
train() client id: f_00007-5-1 loss: 0.445464  [   64/  179]
train() client id: f_00007-5-2 loss: 0.658353  [   96/  179]
train() client id: f_00007-5-3 loss: 0.353626  [  128/  179]
train() client id: f_00007-5-4 loss: 0.441011  [  160/  179]
train() client id: f_00007-6-0 loss: 0.731645  [   32/  179]
train() client id: f_00007-6-1 loss: 0.438896  [   64/  179]
train() client id: f_00007-6-2 loss: 0.400622  [   96/  179]
train() client id: f_00007-6-3 loss: 0.459543  [  128/  179]
train() client id: f_00007-6-4 loss: 0.587774  [  160/  179]
train() client id: f_00007-7-0 loss: 0.515781  [   32/  179]
train() client id: f_00007-7-1 loss: 0.730795  [   64/  179]
train() client id: f_00007-7-2 loss: 0.522697  [   96/  179]
train() client id: f_00007-7-3 loss: 0.335873  [  128/  179]
train() client id: f_00007-7-4 loss: 0.527541  [  160/  179]
train() client id: f_00007-8-0 loss: 0.383417  [   32/  179]
train() client id: f_00007-8-1 loss: 0.333701  [   64/  179]
train() client id: f_00007-8-2 loss: 0.545639  [   96/  179]
train() client id: f_00007-8-3 loss: 0.521052  [  128/  179]
train() client id: f_00007-8-4 loss: 0.736871  [  160/  179]
train() client id: f_00008-0-0 loss: 0.659417  [   32/  130]
train() client id: f_00008-0-1 loss: 0.755449  [   64/  130]
train() client id: f_00008-0-2 loss: 0.732387  [   96/  130]
train() client id: f_00008-0-3 loss: 0.648564  [  128/  130]
train() client id: f_00008-1-0 loss: 0.709773  [   32/  130]
train() client id: f_00008-1-1 loss: 0.778894  [   64/  130]
train() client id: f_00008-1-2 loss: 0.654868  [   96/  130]
train() client id: f_00008-1-3 loss: 0.646484  [  128/  130]
train() client id: f_00008-2-0 loss: 0.697076  [   32/  130]
train() client id: f_00008-2-1 loss: 0.687798  [   64/  130]
train() client id: f_00008-2-2 loss: 0.742941  [   96/  130]
train() client id: f_00008-2-3 loss: 0.675609  [  128/  130]
train() client id: f_00008-3-0 loss: 0.762764  [   32/  130]
train() client id: f_00008-3-1 loss: 0.673957  [   64/  130]
train() client id: f_00008-3-2 loss: 0.648170  [   96/  130]
train() client id: f_00008-3-3 loss: 0.693343  [  128/  130]
train() client id: f_00008-4-0 loss: 0.639028  [   32/  130]
train() client id: f_00008-4-1 loss: 0.657571  [   64/  130]
train() client id: f_00008-4-2 loss: 0.785456  [   96/  130]
train() client id: f_00008-4-3 loss: 0.714126  [  128/  130]
train() client id: f_00008-5-0 loss: 0.697227  [   32/  130]
train() client id: f_00008-5-1 loss: 0.598894  [   64/  130]
train() client id: f_00008-5-2 loss: 0.776154  [   96/  130]
train() client id: f_00008-5-3 loss: 0.730145  [  128/  130]
train() client id: f_00008-6-0 loss: 0.804150  [   32/  130]
train() client id: f_00008-6-1 loss: 0.654038  [   64/  130]
train() client id: f_00008-6-2 loss: 0.774003  [   96/  130]
train() client id: f_00008-6-3 loss: 0.571358  [  128/  130]
train() client id: f_00008-7-0 loss: 0.719179  [   32/  130]
train() client id: f_00008-7-1 loss: 0.716796  [   64/  130]
train() client id: f_00008-7-2 loss: 0.680794  [   96/  130]
train() client id: f_00008-7-3 loss: 0.681132  [  128/  130]
train() client id: f_00008-8-0 loss: 0.712538  [   32/  130]
train() client id: f_00008-8-1 loss: 0.662133  [   64/  130]
train() client id: f_00008-8-2 loss: 0.690805  [   96/  130]
train() client id: f_00008-8-3 loss: 0.739834  [  128/  130]
train() client id: f_00009-0-0 loss: 1.022703  [   32/  118]
train() client id: f_00009-0-1 loss: 0.970814  [   64/  118]
train() client id: f_00009-0-2 loss: 0.871512  [   96/  118]
train() client id: f_00009-1-0 loss: 0.966035  [   32/  118]
train() client id: f_00009-1-1 loss: 0.778956  [   64/  118]
train() client id: f_00009-1-2 loss: 1.005513  [   96/  118]
train() client id: f_00009-2-0 loss: 0.972214  [   32/  118]
train() client id: f_00009-2-1 loss: 0.880285  [   64/  118]
train() client id: f_00009-2-2 loss: 1.021591  [   96/  118]
train() client id: f_00009-3-0 loss: 0.862482  [   32/  118]
train() client id: f_00009-3-1 loss: 0.950319  [   64/  118]
train() client id: f_00009-3-2 loss: 0.909844  [   96/  118]
train() client id: f_00009-4-0 loss: 0.955842  [   32/  118]
train() client id: f_00009-4-1 loss: 0.797061  [   64/  118]
train() client id: f_00009-4-2 loss: 0.933251  [   96/  118]
train() client id: f_00009-5-0 loss: 1.030659  [   32/  118]
train() client id: f_00009-5-1 loss: 1.020535  [   64/  118]
train() client id: f_00009-5-2 loss: 0.757709  [   96/  118]
train() client id: f_00009-6-0 loss: 0.879417  [   32/  118]
train() client id: f_00009-6-1 loss: 0.886869  [   64/  118]
train() client id: f_00009-6-2 loss: 0.950996  [   96/  118]
train() client id: f_00009-7-0 loss: 0.881503  [   32/  118]
train() client id: f_00009-7-1 loss: 0.837523  [   64/  118]
train() client id: f_00009-7-2 loss: 0.814595  [   96/  118]
train() client id: f_00009-8-0 loss: 0.923518  [   32/  118]
train() client id: f_00009-8-1 loss: 0.811243  [   64/  118]
train() client id: f_00009-8-2 loss: 0.866752  [   96/  118]
At round 51 accuracy: 0.6472148541114059
At round 51 training accuracy: 0.5928906773977196
At round 51 training loss: 0.8166445618273394
update_location
xs = -4.528292 156.001589 165.045120 -165.943528 54.896481 -90.217951 -207.215960 228.375741 -1.680116 149.695607 
ys = 242.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -172.154970 4.001482 
xs mean: 28.44286915941286
ys mean: 9.371751218646875
dists_uav = 262.429845 185.952897 192.980920 195.052618 114.079121 134.711537 230.104800 250.046518 199.098359 180.068839 
uav_gains = -112.385363 -106.802731 -107.242112 -107.371598 -101.430259 -103.236189 -109.711448 -111.299961 -107.625247 -106.432053 
uav_gains_db_mean: -107.35369618251812
dists_bs = 183.381394 367.402358 381.831393 152.723935 289.318594 191.926972 181.014794 447.679516 388.015922 366.971021 
bs_gains = -102.941007 -111.391033 -111.859465 -100.716458 -108.485597 -103.494867 -102.783053 -113.794141 -112.054846 -111.376749 
bs_gains_db_mean: -107.88972170590898
Round 52
-------------------------------
ene_coms = [0.01272513 0.01179579 0.00898705 0.00905526 0.00968559 0.0073691
 0.01048256 0.01171687 0.01240181 0.01178335]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [4.11386304 8.49538942 4.01152312 1.45330122 9.76854928 4.68959489
 1.81576205 5.77766729 4.23684348 3.85126342]
obj_prev = 48.213757230698306
eta_min = 8.62745350998333e-23	eta_max = 0.9514495782848396
af = 10.126086272682958	bf = 1.0992690546884842	zeta = 11.138694899951254	eta = 0.9090909090909091
af = 10.126086272682958	bf = 1.0992690546884842	zeta = 22.70826973857203	eta = 0.44592064429650896
af = 10.126086272682958	bf = 1.0992690546884842	zeta = 16.743523713892806	eta = 0.6047762971351675
af = 10.126086272682958	bf = 1.0992690546884842	zeta = 15.666175471659441	eta = 0.646366197736093
af = 10.126086272682958	bf = 1.0992690546884842	zeta = 15.604227742522447	eta = 0.6489322278403289
af = 10.126086272682958	bf = 1.0992690546884842	zeta = 15.604001646859645	eta = 0.6489416306054329
eta = 0.6489416306054329
ene_coms = [0.01272513 0.01179579 0.00898705 0.00905526 0.00968559 0.0073691
 0.01048256 0.01171687 0.01240181 0.01178335]
ene_comp = [0.03568164 0.07504473 0.03511527 0.01217707 0.08665542 0.0413454
 0.01529213 0.05069059 0.03681439 0.03341614]
ene_total = [1.42992984 2.56525762 1.30277669 0.62720011 2.8459011  1.43901981
 0.76138104 1.84350815 1.45384024 1.33518704]
ti_comp = [0.80030123 0.80959463 0.83768206 0.83699998 0.83069659 0.85386157
 0.82272695 0.81038387 0.80353445 0.80971907]
ti_coms = [0.12725131 0.1179579  0.08987047 0.09055256 0.09685595 0.07369096
 0.10482558 0.11716867 0.12401808 0.11783346]
t_total = [27.3997818 27.3997818 27.3997818 27.3997818 27.3997818 27.3997818
 27.3997818 27.3997818 27.3997818 27.3997818]
ene_coms = [0.01272513 0.01179579 0.00898705 0.00905526 0.00968559 0.0073691
 0.01048256 0.01171687 0.01240181 0.01178335]
ene_comp = [4.43310072e-06 4.03000245e-05 3.85664401e-06 1.61085853e-07
 5.89362305e-05 6.05879920e-06 3.30196724e-07 1.23959680e-05
 4.82974254e-06 3.55697010e-06]
ene_total = [0.37602967 0.34963657 0.26559014 0.26749583 0.28785218 0.21786113
 0.30966317 0.3464809  0.36649048 0.3481836 ]
optimize_network iter = 0 obj = 3.1352836608307055
eta = 0.6489416306054329
freqs = [22292633.23167621 46347099.68579585 20959782.80710144  7274234.82937536
 52158285.6529427  24210830.98289709  9293565.0733434  31275664.95004752
 22907788.14708702 20634404.88553991]
eta_min = 0.6489416306054343	eta_max = 0.754678520835421
af = 0.00323421469088643	bf = 1.0992690546884842	zeta = 0.0035576361599750734	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01272513 0.01179579 0.00898705 0.00905526 0.00968559 0.0073691
 0.01048256 0.01171687 0.01240181 0.01178335]
ene_comp = [9.77121668e-07 8.88272782e-06 8.50061992e-07 3.55057300e-08
 1.29904262e-05 1.33544991e-06 7.27802939e-08 2.73225665e-06
 1.06454745e-06 7.84009383e-07]
ene_total = [1.45169775 1.34658744 1.02527107 1.03295884 1.10634094 0.84076288
 1.19577896 1.33688284 1.41482553 1.34424412]
ti_comp = [0.5209274  0.5302208  0.55830823 0.55762615 0.55132276 0.57448774
 0.54335312 0.53101004 0.52416062 0.53034524]
ti_coms = [0.12725131 0.1179579  0.08987047 0.09055256 0.09685595 0.07369096
 0.10482558 0.11716867 0.12401808 0.11783346]
t_total = [27.3997818 27.3997818 27.3997818 27.3997818 27.3997818 27.3997818
 27.3997818 27.3997818 27.3997818 27.3997818]
ene_coms = [0.01272513 0.01179579 0.00898705 0.00905526 0.00968559 0.0073691
 0.01048256 0.01171687 0.01240181 0.01178335]
ene_comp = [2.88556802e-06 2.59118353e-05 2.39436895e-06 1.00090575e-07
 3.69000130e-05 3.69123006e-06 2.08781026e-07 7.96210148e-06
 3.13022353e-06 2.28666608e-06]
ene_total = [0.53803815 0.49972647 0.38000128 0.3827876  0.41098887 0.31166215
 0.44312707 0.49563144 0.52438101 0.49820176]
optimize_network iter = 1 obj = 4.4845457892299185
eta = 0.754678520835421
freqs = [22292633.23167621 46063521.4203551  20469897.99002772  7107115.10310987
 51154463.15662132 23422870.33856065  9159665.59597676 31068359.65682642
 22858460.87700202 20506491.2119379 ]
eta_min = 0.7546785208354184	eta_max = 0.7546785208354196
af = 0.003149083119578347	bf = 1.0992690546884842	zeta = 0.003463991431536182	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01272513 0.01179579 0.00898705 0.00905526 0.00968559 0.0073691
 0.01048256 0.01171687 0.01240181 0.01178335]
ene_comp = [9.77121668e-07 8.77436107e-06 8.10790029e-07 3.38930390e-08
 1.24952182e-05 1.24993791e-06 7.06981996e-08 2.69615612e-06
 1.05996782e-06 7.74319287e-07]
ene_total = [1.45169775 1.34657508 1.02526659 1.03295866 1.10628445 0.84075313
 1.19577872 1.33687872 1.41482501 1.34424301]
ti_comp = [0.5209274  0.5302208  0.55830823 0.55762615 0.55132276 0.57448774
 0.54335312 0.53101004 0.52416062 0.53034524]
ti_coms = [0.12725131 0.1179579  0.08987047 0.09055256 0.09685595 0.07369096
 0.10482558 0.11716867 0.12401808 0.11783346]
t_total = [27.3997818 27.3997818 27.3997818 27.3997818 27.3997818 27.3997818
 27.3997818 27.3997818 27.3997818 27.3997818]
ene_coms = [0.01272513 0.01179579 0.00898705 0.00905526 0.00968559 0.0073691
 0.01048256 0.01171687 0.01240181 0.01178335]
ene_comp = [2.88556802e-06 2.59118353e-05 2.39436895e-06 1.00090575e-07
 3.69000130e-05 3.69123006e-06 2.08781026e-07 7.96210148e-06
 3.13022353e-06 2.28666608e-06]
ene_total = [0.53803815 0.49972647 0.38000128 0.3827876  0.41098887 0.31166215
 0.44312707 0.49563144 0.52438101 0.49820176]
optimize_network iter = 2 obj = 4.4845457892298946
eta = 0.7546785208354196
freqs = [22292633.23167619 46063521.42035507 20469897.99002771  7107115.10310987
 51154463.15662129 23422870.33856065  9159665.59597676 31068359.6568264
 22858460.877002   20506491.21193789]
Done!
ene_coms = [0.01272513 0.01179579 0.00898705 0.00905526 0.00968559 0.0073691
 0.01048256 0.01171687 0.01240181 0.01178335]
ene_comp = [2.81777168e-06 2.53030375e-05 2.33811332e-06 9.77389496e-08
 3.60330482e-05 3.60450472e-06 2.03875721e-07 7.77503214e-06
 3.05667902e-06 2.23294093e-06]
ene_total = [0.01272795 0.01182109 0.00898939 0.00905535 0.00972163 0.0073727
 0.01048276 0.01172464 0.01240486 0.01178558]
At round 52 energy consumption: 0.1060859564619962
At round 52 eta: 0.7546785208354196
At round 52 a_n: 10.370218813849878
At round 52 local rounds: 9.216542397085467
At round 52 global rounds: 42.271956166107834
gradient difference: 0.48914432525634766
train() client id: f_00000-0-0 loss: 1.204609  [   32/  126]
train() client id: f_00000-0-1 loss: 1.162785  [   64/  126]
train() client id: f_00000-0-2 loss: 1.037183  [   96/  126]
train() client id: f_00000-1-0 loss: 1.080508  [   32/  126]
train() client id: f_00000-1-1 loss: 1.357525  [   64/  126]
train() client id: f_00000-1-2 loss: 1.011534  [   96/  126]
train() client id: f_00000-2-0 loss: 1.216049  [   32/  126]
train() client id: f_00000-2-1 loss: 0.994590  [   64/  126]
train() client id: f_00000-2-2 loss: 0.992068  [   96/  126]
train() client id: f_00000-3-0 loss: 0.919949  [   32/  126]
train() client id: f_00000-3-1 loss: 1.006184  [   64/  126]
train() client id: f_00000-3-2 loss: 0.857803  [   96/  126]
train() client id: f_00000-4-0 loss: 0.992580  [   32/  126]
train() client id: f_00000-4-1 loss: 0.842048  [   64/  126]
train() client id: f_00000-4-2 loss: 0.917104  [   96/  126]
train() client id: f_00000-5-0 loss: 0.865373  [   32/  126]
train() client id: f_00000-5-1 loss: 0.925228  [   64/  126]
train() client id: f_00000-5-2 loss: 0.988281  [   96/  126]
train() client id: f_00000-6-0 loss: 0.862302  [   32/  126]
train() client id: f_00000-6-1 loss: 0.782873  [   64/  126]
train() client id: f_00000-6-2 loss: 0.776037  [   96/  126]
train() client id: f_00000-7-0 loss: 0.799247  [   32/  126]
train() client id: f_00000-7-1 loss: 0.913665  [   64/  126]
train() client id: f_00000-7-2 loss: 0.730885  [   96/  126]
train() client id: f_00000-8-0 loss: 0.780751  [   32/  126]
train() client id: f_00000-8-1 loss: 0.873995  [   64/  126]
train() client id: f_00000-8-2 loss: 0.869494  [   96/  126]
train() client id: f_00001-0-0 loss: 0.316857  [   32/  265]
train() client id: f_00001-0-1 loss: 0.458821  [   64/  265]
train() client id: f_00001-0-2 loss: 0.379565  [   96/  265]
train() client id: f_00001-0-3 loss: 0.321274  [  128/  265]
train() client id: f_00001-0-4 loss: 0.247164  [  160/  265]
train() client id: f_00001-0-5 loss: 0.330075  [  192/  265]
train() client id: f_00001-0-6 loss: 0.269676  [  224/  265]
train() client id: f_00001-0-7 loss: 0.248684  [  256/  265]
train() client id: f_00001-1-0 loss: 0.245430  [   32/  265]
train() client id: f_00001-1-1 loss: 0.347386  [   64/  265]
train() client id: f_00001-1-2 loss: 0.330174  [   96/  265]
train() client id: f_00001-1-3 loss: 0.336665  [  128/  265]
train() client id: f_00001-1-4 loss: 0.392875  [  160/  265]
train() client id: f_00001-1-5 loss: 0.342042  [  192/  265]
train() client id: f_00001-1-6 loss: 0.310053  [  224/  265]
train() client id: f_00001-1-7 loss: 0.217970  [  256/  265]
train() client id: f_00001-2-0 loss: 0.281843  [   32/  265]
train() client id: f_00001-2-1 loss: 0.314179  [   64/  265]
train() client id: f_00001-2-2 loss: 0.243681  [   96/  265]
train() client id: f_00001-2-3 loss: 0.368496  [  128/  265]
train() client id: f_00001-2-4 loss: 0.299874  [  160/  265]
train() client id: f_00001-2-5 loss: 0.325482  [  192/  265]
train() client id: f_00001-2-6 loss: 0.291042  [  224/  265]
train() client id: f_00001-2-7 loss: 0.314418  [  256/  265]
train() client id: f_00001-3-0 loss: 0.207454  [   32/  265]
train() client id: f_00001-3-1 loss: 0.274080  [   64/  265]
train() client id: f_00001-3-2 loss: 0.203122  [   96/  265]
train() client id: f_00001-3-3 loss: 0.452392  [  128/  265]
train() client id: f_00001-3-4 loss: 0.339419  [  160/  265]
train() client id: f_00001-3-5 loss: 0.294222  [  192/  265]
train() client id: f_00001-3-6 loss: 0.248043  [  224/  265]
train() client id: f_00001-3-7 loss: 0.333953  [  256/  265]
train() client id: f_00001-4-0 loss: 0.218658  [   32/  265]
train() client id: f_00001-4-1 loss: 0.317523  [   64/  265]
train() client id: f_00001-4-2 loss: 0.380245  [   96/  265]
train() client id: f_00001-4-3 loss: 0.270797  [  128/  265]
train() client id: f_00001-4-4 loss: 0.273893  [  160/  265]
train() client id: f_00001-4-5 loss: 0.382008  [  192/  265]
train() client id: f_00001-4-6 loss: 0.288515  [  224/  265]
train() client id: f_00001-4-7 loss: 0.245977  [  256/  265]
train() client id: f_00001-5-0 loss: 0.277849  [   32/  265]
train() client id: f_00001-5-1 loss: 0.287419  [   64/  265]
train() client id: f_00001-5-2 loss: 0.207222  [   96/  265]
train() client id: f_00001-5-3 loss: 0.260681  [  128/  265]
train() client id: f_00001-5-4 loss: 0.291325  [  160/  265]
train() client id: f_00001-5-5 loss: 0.273079  [  192/  265]
train() client id: f_00001-5-6 loss: 0.392009  [  224/  265]
train() client id: f_00001-5-7 loss: 0.370949  [  256/  265]
train() client id: f_00001-6-0 loss: 0.262073  [   32/  265]
train() client id: f_00001-6-1 loss: 0.271012  [   64/  265]
train() client id: f_00001-6-2 loss: 0.275882  [   96/  265]
train() client id: f_00001-6-3 loss: 0.311999  [  128/  265]
train() client id: f_00001-6-4 loss: 0.197942  [  160/  265]
train() client id: f_00001-6-5 loss: 0.309968  [  192/  265]
train() client id: f_00001-6-6 loss: 0.271674  [  224/  265]
train() client id: f_00001-6-7 loss: 0.457957  [  256/  265]
train() client id: f_00001-7-0 loss: 0.181002  [   32/  265]
train() client id: f_00001-7-1 loss: 0.268076  [   64/  265]
train() client id: f_00001-7-2 loss: 0.282143  [   96/  265]
train() client id: f_00001-7-3 loss: 0.275314  [  128/  265]
train() client id: f_00001-7-4 loss: 0.367095  [  160/  265]
train() client id: f_00001-7-5 loss: 0.386700  [  192/  265]
train() client id: f_00001-7-6 loss: 0.285266  [  224/  265]
train() client id: f_00001-7-7 loss: 0.293863  [  256/  265]
train() client id: f_00001-8-0 loss: 0.350951  [   32/  265]
train() client id: f_00001-8-1 loss: 0.247646  [   64/  265]
train() client id: f_00001-8-2 loss: 0.374443  [   96/  265]
train() client id: f_00001-8-3 loss: 0.193692  [  128/  265]
train() client id: f_00001-8-4 loss: 0.291501  [  160/  265]
train() client id: f_00001-8-5 loss: 0.308739  [  192/  265]
train() client id: f_00001-8-6 loss: 0.307518  [  224/  265]
train() client id: f_00001-8-7 loss: 0.253968  [  256/  265]
train() client id: f_00002-0-0 loss: 1.141462  [   32/  124]
train() client id: f_00002-0-1 loss: 0.956724  [   64/  124]
train() client id: f_00002-0-2 loss: 1.027679  [   96/  124]
train() client id: f_00002-1-0 loss: 0.974482  [   32/  124]
train() client id: f_00002-1-1 loss: 1.084703  [   64/  124]
train() client id: f_00002-1-2 loss: 1.010697  [   96/  124]
train() client id: f_00002-2-0 loss: 0.957958  [   32/  124]
train() client id: f_00002-2-1 loss: 1.096427  [   64/  124]
train() client id: f_00002-2-2 loss: 0.993227  [   96/  124]
train() client id: f_00002-3-0 loss: 1.091585  [   32/  124]
train() client id: f_00002-3-1 loss: 0.872194  [   64/  124]
train() client id: f_00002-3-2 loss: 0.938677  [   96/  124]
train() client id: f_00002-4-0 loss: 1.005425  [   32/  124]
train() client id: f_00002-4-1 loss: 0.961045  [   64/  124]
train() client id: f_00002-4-2 loss: 0.974824  [   96/  124]
train() client id: f_00002-5-0 loss: 0.843623  [   32/  124]
train() client id: f_00002-5-1 loss: 1.013827  [   64/  124]
train() client id: f_00002-5-2 loss: 0.931711  [   96/  124]
train() client id: f_00002-6-0 loss: 0.858512  [   32/  124]
train() client id: f_00002-6-1 loss: 0.973556  [   64/  124]
train() client id: f_00002-6-2 loss: 1.017292  [   96/  124]
train() client id: f_00002-7-0 loss: 1.013119  [   32/  124]
train() client id: f_00002-7-1 loss: 0.737787  [   64/  124]
train() client id: f_00002-7-2 loss: 0.957283  [   96/  124]
train() client id: f_00002-8-0 loss: 0.883665  [   32/  124]
train() client id: f_00002-8-1 loss: 0.854901  [   64/  124]
train() client id: f_00002-8-2 loss: 0.969608  [   96/  124]
train() client id: f_00003-0-0 loss: 0.609035  [   32/   43]
train() client id: f_00003-1-0 loss: 0.825889  [   32/   43]
train() client id: f_00003-2-0 loss: 0.866111  [   32/   43]
train() client id: f_00003-3-0 loss: 0.675851  [   32/   43]
train() client id: f_00003-4-0 loss: 0.778357  [   32/   43]
train() client id: f_00003-5-0 loss: 0.859744  [   32/   43]
train() client id: f_00003-6-0 loss: 0.652631  [   32/   43]
train() client id: f_00003-7-0 loss: 0.561185  [   32/   43]
train() client id: f_00003-8-0 loss: 0.748353  [   32/   43]
train() client id: f_00004-0-0 loss: 0.972106  [   32/  306]
train() client id: f_00004-0-1 loss: 0.981514  [   64/  306]
train() client id: f_00004-0-2 loss: 1.068218  [   96/  306]
train() client id: f_00004-0-3 loss: 1.014150  [  128/  306]
train() client id: f_00004-0-4 loss: 1.017981  [  160/  306]
train() client id: f_00004-0-5 loss: 0.837227  [  192/  306]
train() client id: f_00004-0-6 loss: 0.935831  [  224/  306]
train() client id: f_00004-0-7 loss: 1.155198  [  256/  306]
train() client id: f_00004-0-8 loss: 0.992808  [  288/  306]
train() client id: f_00004-1-0 loss: 1.045468  [   32/  306]
train() client id: f_00004-1-1 loss: 0.935058  [   64/  306]
train() client id: f_00004-1-2 loss: 0.975195  [   96/  306]
train() client id: f_00004-1-3 loss: 0.892889  [  128/  306]
train() client id: f_00004-1-4 loss: 1.000320  [  160/  306]
train() client id: f_00004-1-5 loss: 1.095984  [  192/  306]
train() client id: f_00004-1-6 loss: 0.881647  [  224/  306]
train() client id: f_00004-1-7 loss: 1.069211  [  256/  306]
train() client id: f_00004-1-8 loss: 1.128980  [  288/  306]
train() client id: f_00004-2-0 loss: 1.035490  [   32/  306]
train() client id: f_00004-2-1 loss: 0.984353  [   64/  306]
train() client id: f_00004-2-2 loss: 0.954042  [   96/  306]
train() client id: f_00004-2-3 loss: 0.919273  [  128/  306]
train() client id: f_00004-2-4 loss: 0.912626  [  160/  306]
train() client id: f_00004-2-5 loss: 0.973503  [  192/  306]
train() client id: f_00004-2-6 loss: 0.882123  [  224/  306]
train() client id: f_00004-2-7 loss: 1.030017  [  256/  306]
train() client id: f_00004-2-8 loss: 1.090852  [  288/  306]
train() client id: f_00004-3-0 loss: 0.870090  [   32/  306]
train() client id: f_00004-3-1 loss: 0.936793  [   64/  306]
train() client id: f_00004-3-2 loss: 0.869664  [   96/  306]
train() client id: f_00004-3-3 loss: 1.033213  [  128/  306]
train() client id: f_00004-3-4 loss: 1.008662  [  160/  306]
train() client id: f_00004-3-5 loss: 1.181347  [  192/  306]
train() client id: f_00004-3-6 loss: 0.908385  [  224/  306]
train() client id: f_00004-3-7 loss: 0.949925  [  256/  306]
train() client id: f_00004-3-8 loss: 1.059290  [  288/  306]
train() client id: f_00004-4-0 loss: 1.008039  [   32/  306]
train() client id: f_00004-4-1 loss: 1.053978  [   64/  306]
train() client id: f_00004-4-2 loss: 0.959196  [   96/  306]
train() client id: f_00004-4-3 loss: 0.949966  [  128/  306]
train() client id: f_00004-4-4 loss: 0.994420  [  160/  306]
train() client id: f_00004-4-5 loss: 0.970636  [  192/  306]
train() client id: f_00004-4-6 loss: 0.979878  [  224/  306]
train() client id: f_00004-4-7 loss: 0.938206  [  256/  306]
train() client id: f_00004-4-8 loss: 0.919589  [  288/  306]
train() client id: f_00004-5-0 loss: 1.003205  [   32/  306]
train() client id: f_00004-5-1 loss: 1.033847  [   64/  306]
train() client id: f_00004-5-2 loss: 0.988346  [   96/  306]
train() client id: f_00004-5-3 loss: 0.872102  [  128/  306]
train() client id: f_00004-5-4 loss: 0.879150  [  160/  306]
train() client id: f_00004-5-5 loss: 0.983239  [  192/  306]
train() client id: f_00004-5-6 loss: 0.981698  [  224/  306]
train() client id: f_00004-5-7 loss: 0.883466  [  256/  306]
train() client id: f_00004-5-8 loss: 1.136055  [  288/  306]
train() client id: f_00004-6-0 loss: 0.992305  [   32/  306]
train() client id: f_00004-6-1 loss: 0.865198  [   64/  306]
train() client id: f_00004-6-2 loss: 1.024631  [   96/  306]
train() client id: f_00004-6-3 loss: 1.026739  [  128/  306]
train() client id: f_00004-6-4 loss: 1.038521  [  160/  306]
train() client id: f_00004-6-5 loss: 0.905414  [  192/  306]
train() client id: f_00004-6-6 loss: 0.979979  [  224/  306]
train() client id: f_00004-6-7 loss: 0.884055  [  256/  306]
train() client id: f_00004-6-8 loss: 0.994648  [  288/  306]
train() client id: f_00004-7-0 loss: 0.940704  [   32/  306]
train() client id: f_00004-7-1 loss: 1.028235  [   64/  306]
train() client id: f_00004-7-2 loss: 0.958871  [   96/  306]
train() client id: f_00004-7-3 loss: 0.935833  [  128/  306]
train() client id: f_00004-7-4 loss: 0.868446  [  160/  306]
train() client id: f_00004-7-5 loss: 1.005840  [  192/  306]
train() client id: f_00004-7-6 loss: 0.924046  [  224/  306]
train() client id: f_00004-7-7 loss: 1.050369  [  256/  306]
train() client id: f_00004-7-8 loss: 1.053538  [  288/  306]
train() client id: f_00004-8-0 loss: 0.876214  [   32/  306]
train() client id: f_00004-8-1 loss: 0.919043  [   64/  306]
train() client id: f_00004-8-2 loss: 0.972167  [   96/  306]
train() client id: f_00004-8-3 loss: 0.989747  [  128/  306]
train() client id: f_00004-8-4 loss: 1.010898  [  160/  306]
train() client id: f_00004-8-5 loss: 1.023459  [  192/  306]
train() client id: f_00004-8-6 loss: 0.915458  [  224/  306]
train() client id: f_00004-8-7 loss: 1.031098  [  256/  306]
train() client id: f_00004-8-8 loss: 0.930015  [  288/  306]
train() client id: f_00005-0-0 loss: 0.798963  [   32/  146]
train() client id: f_00005-0-1 loss: 0.374683  [   64/  146]
train() client id: f_00005-0-2 loss: 0.677916  [   96/  146]
train() client id: f_00005-0-3 loss: 0.700261  [  128/  146]
train() client id: f_00005-1-0 loss: 0.595176  [   32/  146]
train() client id: f_00005-1-1 loss: 0.647649  [   64/  146]
train() client id: f_00005-1-2 loss: 0.546592  [   96/  146]
train() client id: f_00005-1-3 loss: 0.704467  [  128/  146]
train() client id: f_00005-2-0 loss: 0.628129  [   32/  146]
train() client id: f_00005-2-1 loss: 0.401165  [   64/  146]
train() client id: f_00005-2-2 loss: 0.691233  [   96/  146]
train() client id: f_00005-2-3 loss: 0.648772  [  128/  146]
train() client id: f_00005-3-0 loss: 0.661834  [   32/  146]
train() client id: f_00005-3-1 loss: 0.678290  [   64/  146]
train() client id: f_00005-3-2 loss: 0.573887  [   96/  146]
train() client id: f_00005-3-3 loss: 0.574144  [  128/  146]
train() client id: f_00005-4-0 loss: 0.818060  [   32/  146]
train() client id: f_00005-4-1 loss: 0.882733  [   64/  146]
train() client id: f_00005-4-2 loss: 0.374380  [   96/  146]
train() client id: f_00005-4-3 loss: 0.547408  [  128/  146]
train() client id: f_00005-5-0 loss: 0.732307  [   32/  146]
train() client id: f_00005-5-1 loss: 0.508158  [   64/  146]
train() client id: f_00005-5-2 loss: 0.589318  [   96/  146]
train() client id: f_00005-5-3 loss: 0.661676  [  128/  146]
train() client id: f_00005-6-0 loss: 0.550997  [   32/  146]
train() client id: f_00005-6-1 loss: 0.415220  [   64/  146]
train() client id: f_00005-6-2 loss: 0.697335  [   96/  146]
train() client id: f_00005-6-3 loss: 0.705355  [  128/  146]
train() client id: f_00005-7-0 loss: 0.523095  [   32/  146]
train() client id: f_00005-7-1 loss: 0.630024  [   64/  146]
train() client id: f_00005-7-2 loss: 0.719607  [   96/  146]
train() client id: f_00005-7-3 loss: 0.649601  [  128/  146]
train() client id: f_00005-8-0 loss: 0.890108  [   32/  146]
train() client id: f_00005-8-1 loss: 0.463818  [   64/  146]
train() client id: f_00005-8-2 loss: 0.673752  [   96/  146]
train() client id: f_00005-8-3 loss: 0.539331  [  128/  146]
train() client id: f_00006-0-0 loss: 0.540887  [   32/   54]
train() client id: f_00006-1-0 loss: 0.472061  [   32/   54]
train() client id: f_00006-2-0 loss: 0.527974  [   32/   54]
train() client id: f_00006-3-0 loss: 0.500172  [   32/   54]
train() client id: f_00006-4-0 loss: 0.525982  [   32/   54]
train() client id: f_00006-5-0 loss: 0.500179  [   32/   54]
train() client id: f_00006-6-0 loss: 0.492383  [   32/   54]
train() client id: f_00006-7-0 loss: 0.541581  [   32/   54]
train() client id: f_00006-8-0 loss: 0.540416  [   32/   54]
train() client id: f_00007-0-0 loss: 0.454816  [   32/  179]
train() client id: f_00007-0-1 loss: 0.418689  [   64/  179]
train() client id: f_00007-0-2 loss: 0.249024  [   96/  179]
train() client id: f_00007-0-3 loss: 0.207911  [  128/  179]
train() client id: f_00007-0-4 loss: 0.621192  [  160/  179]
train() client id: f_00007-1-0 loss: 0.558792  [   32/  179]
train() client id: f_00007-1-1 loss: 0.366405  [   64/  179]
train() client id: f_00007-1-2 loss: 0.407104  [   96/  179]
train() client id: f_00007-1-3 loss: 0.339889  [  128/  179]
train() client id: f_00007-1-4 loss: 0.196523  [  160/  179]
train() client id: f_00007-2-0 loss: 0.222664  [   32/  179]
train() client id: f_00007-2-1 loss: 0.361583  [   64/  179]
train() client id: f_00007-2-2 loss: 0.286938  [   96/  179]
train() client id: f_00007-2-3 loss: 0.390807  [  128/  179]
train() client id: f_00007-2-4 loss: 0.276887  [  160/  179]
train() client id: f_00007-3-0 loss: 0.303016  [   32/  179]
train() client id: f_00007-3-1 loss: 0.409023  [   64/  179]
train() client id: f_00007-3-2 loss: 0.271543  [   96/  179]
train() client id: f_00007-3-3 loss: 0.426337  [  128/  179]
train() client id: f_00007-3-4 loss: 0.323178  [  160/  179]
train() client id: f_00007-4-0 loss: 0.461880  [   32/  179]
train() client id: f_00007-4-1 loss: 0.203655  [   64/  179]
train() client id: f_00007-4-2 loss: 0.368077  [   96/  179]
train() client id: f_00007-4-3 loss: 0.157255  [  128/  179]
train() client id: f_00007-4-4 loss: 0.435292  [  160/  179]
train() client id: f_00007-5-0 loss: 0.323648  [   32/  179]
train() client id: f_00007-5-1 loss: 0.352267  [   64/  179]
train() client id: f_00007-5-2 loss: 0.255912  [   96/  179]
train() client id: f_00007-5-3 loss: 0.333574  [  128/  179]
train() client id: f_00007-5-4 loss: 0.167111  [  160/  179]
train() client id: f_00007-6-0 loss: 0.590932  [   32/  179]
train() client id: f_00007-6-1 loss: 0.261255  [   64/  179]
train() client id: f_00007-6-2 loss: 0.285915  [   96/  179]
train() client id: f_00007-6-3 loss: 0.376803  [  128/  179]
train() client id: f_00007-6-4 loss: 0.150714  [  160/  179]
train() client id: f_00007-7-0 loss: 0.168612  [   32/  179]
train() client id: f_00007-7-1 loss: 0.167932  [   64/  179]
train() client id: f_00007-7-2 loss: 0.762662  [   96/  179]
train() client id: f_00007-7-3 loss: 0.173970  [  128/  179]
train() client id: f_00007-7-4 loss: 0.298378  [  160/  179]
train() client id: f_00007-8-0 loss: 0.173518  [   32/  179]
train() client id: f_00007-8-1 loss: 0.243975  [   64/  179]
train() client id: f_00007-8-2 loss: 0.538602  [   96/  179]
train() client id: f_00007-8-3 loss: 0.262136  [  128/  179]
train() client id: f_00007-8-4 loss: 0.308209  [  160/  179]
train() client id: f_00008-0-0 loss: 0.705955  [   32/  130]
train() client id: f_00008-0-1 loss: 0.652805  [   64/  130]
train() client id: f_00008-0-2 loss: 0.716681  [   96/  130]
train() client id: f_00008-0-3 loss: 0.657533  [  128/  130]
train() client id: f_00008-1-0 loss: 0.769119  [   32/  130]
train() client id: f_00008-1-1 loss: 0.622397  [   64/  130]
train() client id: f_00008-1-2 loss: 0.671825  [   96/  130]
train() client id: f_00008-1-3 loss: 0.653728  [  128/  130]
train() client id: f_00008-2-0 loss: 0.660034  [   32/  130]
train() client id: f_00008-2-1 loss: 0.651932  [   64/  130]
train() client id: f_00008-2-2 loss: 0.743361  [   96/  130]
train() client id: f_00008-2-3 loss: 0.680718  [  128/  130]
train() client id: f_00008-3-0 loss: 0.671645  [   32/  130]
train() client id: f_00008-3-1 loss: 0.633420  [   64/  130]
train() client id: f_00008-3-2 loss: 0.655953  [   96/  130]
train() client id: f_00008-3-3 loss: 0.781806  [  128/  130]
train() client id: f_00008-4-0 loss: 0.736379  [   32/  130]
train() client id: f_00008-4-1 loss: 0.636554  [   64/  130]
train() client id: f_00008-4-2 loss: 0.613250  [   96/  130]
train() client id: f_00008-4-3 loss: 0.749963  [  128/  130]
train() client id: f_00008-5-0 loss: 0.770673  [   32/  130]
train() client id: f_00008-5-1 loss: 0.669648  [   64/  130]
train() client id: f_00008-5-2 loss: 0.577181  [   96/  130]
train() client id: f_00008-5-3 loss: 0.707771  [  128/  130]
train() client id: f_00008-6-0 loss: 0.619223  [   32/  130]
train() client id: f_00008-6-1 loss: 0.675663  [   64/  130]
train() client id: f_00008-6-2 loss: 0.708263  [   96/  130]
train() client id: f_00008-6-3 loss: 0.736551  [  128/  130]
train() client id: f_00008-7-0 loss: 0.670021  [   32/  130]
train() client id: f_00008-7-1 loss: 0.697810  [   64/  130]
train() client id: f_00008-7-2 loss: 0.661462  [   96/  130]
train() client id: f_00008-7-3 loss: 0.709831  [  128/  130]
train() client id: f_00008-8-0 loss: 0.680042  [   32/  130]
train() client id: f_00008-8-1 loss: 0.723597  [   64/  130]
train() client id: f_00008-8-2 loss: 0.683087  [   96/  130]
train() client id: f_00008-8-3 loss: 0.641968  [  128/  130]
train() client id: f_00009-0-0 loss: 0.966548  [   32/  118]
train() client id: f_00009-0-1 loss: 0.970299  [   64/  118]
train() client id: f_00009-0-2 loss: 0.983982  [   96/  118]
train() client id: f_00009-1-0 loss: 0.857813  [   32/  118]
train() client id: f_00009-1-1 loss: 0.926510  [   64/  118]
train() client id: f_00009-1-2 loss: 1.020309  [   96/  118]
train() client id: f_00009-2-0 loss: 0.815305  [   32/  118]
train() client id: f_00009-2-1 loss: 0.971261  [   64/  118]
train() client id: f_00009-2-2 loss: 0.915775  [   96/  118]
train() client id: f_00009-3-0 loss: 0.998879  [   32/  118]
train() client id: f_00009-3-1 loss: 0.789159  [   64/  118]
train() client id: f_00009-3-2 loss: 0.789191  [   96/  118]
train() client id: f_00009-4-0 loss: 0.801849  [   32/  118]
train() client id: f_00009-4-1 loss: 0.897525  [   64/  118]
train() client id: f_00009-4-2 loss: 0.858538  [   96/  118]
train() client id: f_00009-5-0 loss: 0.791484  [   32/  118]
train() client id: f_00009-5-1 loss: 1.023930  [   64/  118]
train() client id: f_00009-5-2 loss: 0.846845  [   96/  118]
train() client id: f_00009-6-0 loss: 0.833992  [   32/  118]
train() client id: f_00009-6-1 loss: 0.902424  [   64/  118]
train() client id: f_00009-6-2 loss: 0.929539  [   96/  118]
train() client id: f_00009-7-0 loss: 0.878375  [   32/  118]
train() client id: f_00009-7-1 loss: 1.048677  [   64/  118]
train() client id: f_00009-7-2 loss: 0.657573  [   96/  118]
train() client id: f_00009-8-0 loss: 0.872340  [   32/  118]
train() client id: f_00009-8-1 loss: 0.887750  [   64/  118]
train() client id: f_00009-8-2 loss: 0.799642  [   96/  118]
At round 52 accuracy: 0.6472148541114059
At round 52 training accuracy: 0.5915492957746479
At round 52 training loss: 0.8204544545465068
update_location
xs = -4.528292 161.001589 170.045120 -170.943528 59.896481 -95.217951 -212.215960 233.375741 -1.680116 154.695607 
ys = 247.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -177.154970 4.001482 
xs mean: 29.44286915941286
ys mean: 9.371751218646875
dists_uav = 267.058614 190.167021 197.274141 199.323754 116.567623 138.110020 234.617516 254.621324 203.437229 184.246419 
uav_gains = -112.803143 -107.066354 -107.510704 -107.639424 -101.664591 -103.507133 -110.051221 -111.693962 -107.899429 -106.695633 
uav_gains_db_mean: -107.65315943169207
dists_bs = 185.282528 371.913308 386.290906 152.509133 293.307371 189.771289 181.971194 452.189901 392.495739 371.402325 
bs_gains = -103.066424 -111.539427 -112.000665 -100.699343 -108.652102 -103.357513 -102.847133 -113.916043 -112.194438 -111.522708 
bs_gains_db_mean: -107.97957974704357
Round 53
-------------------------------
ene_coms = [0.01315625 0.0119265  0.00912964 0.00919953 0.00978704 0.00732053
 0.01072569 0.01206547 0.01253652 0.01191164]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.982342   8.21609632 3.8804601  1.40675756 9.44690512 4.53419773
 1.758247   5.59035217 4.09825789 3.72534918]
obj_prev = 46.63896508011116
eta_min = 1.6051326742090796e-23	eta_max = 0.9513862596284618
af = 9.791604535988256	bf = 1.0805701471911853	zeta = 10.770764989587082	eta = 0.9090909090909091
af = 9.791604535988256	bf = 1.0805701471911853	zeta = 22.151907096979755	eta = 0.44202083789540936
af = 9.791604535988256	bf = 1.0805701471911853	zeta = 16.263006945772794	eta = 0.6020783591027958
af = 9.791604535988256	bf = 1.0805701471911853	zeta = 15.200263514660627	eta = 0.6441733412413719
af = 9.791604535988256	bf = 1.0805701471911853	zeta = 15.138817080536572	eta = 0.6467879546927723
af = 9.791604535988256	bf = 1.0805701471911853	zeta = 15.138589962362994	eta = 0.6467976581921951
eta = 0.6467976581921951
ene_coms = [0.01315625 0.0119265  0.00912964 0.00919953 0.00978704 0.00732053
 0.01072569 0.01206547 0.01253652 0.01191164]
ene_comp = [0.03595472 0.07561905 0.03538401 0.01227026 0.0873186  0.04166181
 0.01540916 0.05107853 0.03709614 0.03367188]
ene_total = [1.39429631 2.48548214 1.26377512 0.60954302 2.75690006 1.3906446
 0.74198752 1.79270427 1.40910747 1.29414945]
ti_comp = [0.8317722  0.84406971 0.87203827 0.87133943 0.86546433 0.89012941
 0.85607786 0.84268001 0.8379695  0.8442183 ]
ti_coms = [0.13156251 0.11926501 0.09129644 0.09199529 0.09787039 0.0732053
 0.10725686 0.12065471 0.12536522 0.11911641]
t_total = [27.3497776 27.3497776 27.3497776 27.3497776 27.3497776 27.3497776
 27.3497776 27.3497776 27.3497776 27.3497776]
ene_coms = [0.01315625 0.0119265  0.00912964 0.00919953 0.00978704 0.00732053
 0.01072569 0.01206547 0.01253652 0.01191164]
ene_comp = [4.19893167e-06 3.79329806e-05 3.64107765e-06 1.52078076e-07
 5.55522835e-05 5.70411459e-06 3.12025963e-07 1.17292351e-05
 4.54369962e-06 3.34789998e-06]
ene_total = [0.37363481 0.33967903 0.25930065 0.26118566 0.27943837 0.20799716
 0.30451894 0.34288055 0.35605003 0.33827527]
optimize_network iter = 0 obj = 3.062960462132567
eta = 0.6467976581921951
freqs = [21613319.16109408 44794313.29005264 20288104.03642318  7041033.52963594
 50446098.09200083 23402110.64401511  8999861.40618465 30307189.53780713
 22134538.21343184 19942636.31042207]
eta_min = 0.6467976581921985	eta_max = 0.7558509035600001
af = 0.0029254009805846186	bf = 1.0805701471911853	zeta = 0.0032179410786430805	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01315625 0.0119265  0.00912964 0.00919953 0.00978704 0.00732053
 0.01072569 0.01206547 0.01253652 0.01191164]
ene_comp = [9.18478163e-07 8.29749496e-06 7.96452663e-07 3.32656978e-08
 1.21515574e-05 1.24772326e-06 6.82528452e-08 2.56566363e-06
 9.93893021e-07 7.32322711e-07]
ene_total = [1.45129374 1.31646078 1.00712781 1.01475217 1.08089386 0.80762435
 1.18309793 1.33115756 1.38294317 1.31398728]
ti_comp = [0.53433709 0.5466346  0.57460316 0.57390432 0.56802922 0.5926943
 0.55864275 0.5452449  0.54053439 0.54678319]
ti_coms = [0.13156251 0.11926501 0.09129644 0.09199529 0.09787039 0.0732053
 0.10725686 0.12065471 0.12536522 0.11911641]
t_total = [27.3497776 27.3497776 27.3497776 27.3497776 27.3497776 27.3497776
 27.3497776 27.3497776 27.3497776 27.3497776]
ene_coms = [0.01315625 0.0119265  0.00912964 0.00919953 0.00978704 0.00732053
 0.01072569 0.01206547 0.01253652 0.01191164]
ene_comp = [2.69742718e-06 2.39779544e-05 2.22329849e-06 9.29383847e-08
 3.41894154e-05 3.41087500e-06 1.94259245e-07 7.42753342e-06
 2.89502382e-06 2.11584794e-06]
ene_total = [0.54046333 0.49082914 0.37506336 0.37784615 0.40337674 0.30080835
 0.44053254 0.49585716 0.51501796 0.48932093]
optimize_network iter = 1 obj = 4.4291156554339235
eta = 0.7558509035600001
freqs = [21613319.16109406 44433958.22648147 19779709.91411282  6867445.26500105
 49376124.40577187 22578149.71277461  8859839.25474585 30090379.59103995
 22043790.19938494 19780310.1795105 ]
eta_min = 0.755850903559998	eta_max = 0.7558509035599892
af = 0.0028381796877753177	bf = 1.0805701471911853	zeta = 0.00312199765655285	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01315625 0.0119265  0.00912964 0.00919953 0.00978704 0.00732053
 0.01072569 0.01206547 0.01253652 0.01191164]
ene_comp = [9.18478163e-07 8.16453088e-06 7.57036603e-07 3.16456650e-08
 1.16415493e-05 1.16140826e-06 6.61455759e-08 2.52908672e-06
 9.85760127e-07 7.20449525e-07]
ene_total = [1.45129374 1.31644611 1.00712346 1.01475199 1.08083761 0.80761483
 1.1830977  1.33115352 1.38294227 1.31398597]
ti_comp = [0.53433709 0.5466346  0.57460316 0.57390432 0.56802922 0.5926943
 0.55864275 0.5452449  0.54053439 0.54678319]
ti_coms = [0.13156251 0.11926501 0.09129644 0.09199529 0.09787039 0.0732053
 0.10725686 0.12065471 0.12536522 0.11911641]
t_total = [27.3497776 27.3497776 27.3497776 27.3497776 27.3497776 27.3497776
 27.3497776 27.3497776 27.3497776 27.3497776]
ene_coms = [0.01315625 0.0119265  0.00912964 0.00919953 0.00978704 0.00732053
 0.01072569 0.01206547 0.01253652 0.01191164]
ene_comp = [2.69742718e-06 2.39779544e-05 2.22329849e-06 9.29383847e-08
 3.41894154e-05 3.41087500e-06 1.94259245e-07 7.42753342e-06
 2.89502382e-06 2.11584794e-06]
ene_total = [0.54046333 0.49082914 0.37506336 0.37784615 0.40337674 0.30080835
 0.44053254 0.49585716 0.51501796 0.48932093]
optimize_network iter = 2 obj = 4.429115655433726
eta = 0.7558509035599892
freqs = [21613319.16109397 44433958.22648134 19779709.91411282  6867445.26500105
 49376124.40577184 22578149.71277465  8859839.25474584 30090379.59103987
 22043790.19938486 19780310.17951044]
Done!
ene_coms = [0.01315625 0.0119265  0.00912964 0.00919953 0.00978704 0.00732053
 0.01072569 0.01206547 0.01253652 0.01191164]
ene_comp = [2.64865865e-06 2.35444415e-05 2.18310204e-06 9.12580916e-08
 3.35712829e-05 3.34920759e-06 1.90747106e-07 7.29324625e-06
 2.84268282e-06 2.07759416e-06]
ene_total = [0.0131589  0.01195005 0.00913183 0.00919962 0.00982061 0.00732388
 0.01072588 0.01207276 0.01253936 0.01191372]
At round 53 energy consumption: 0.10783660633078482
At round 53 eta: 0.7558509035599892
At round 53 a_n: 10.027672966880559
At round 53 local rounds: 9.165712835066293
At round 53 global rounds: 41.071923316924625
gradient difference: 0.5464153289794922
train() client id: f_00000-0-0 loss: 0.964288  [   32/  126]
train() client id: f_00000-0-1 loss: 1.153877  [   64/  126]
train() client id: f_00000-0-2 loss: 0.994677  [   96/  126]
train() client id: f_00000-1-0 loss: 0.781352  [   32/  126]
train() client id: f_00000-1-1 loss: 0.993926  [   64/  126]
train() client id: f_00000-1-2 loss: 1.004265  [   96/  126]
train() client id: f_00000-2-0 loss: 0.920755  [   32/  126]
train() client id: f_00000-2-1 loss: 0.808723  [   64/  126]
train() client id: f_00000-2-2 loss: 1.043955  [   96/  126]
train() client id: f_00000-3-0 loss: 1.015420  [   32/  126]
train() client id: f_00000-3-1 loss: 0.845886  [   64/  126]
train() client id: f_00000-3-2 loss: 0.925126  [   96/  126]
train() client id: f_00000-4-0 loss: 0.849229  [   32/  126]
train() client id: f_00000-4-1 loss: 0.859293  [   64/  126]
train() client id: f_00000-4-2 loss: 0.921641  [   96/  126]
train() client id: f_00000-5-0 loss: 0.903131  [   32/  126]
train() client id: f_00000-5-1 loss: 0.825565  [   64/  126]
train() client id: f_00000-5-2 loss: 0.869939  [   96/  126]
train() client id: f_00000-6-0 loss: 0.869365  [   32/  126]
train() client id: f_00000-6-1 loss: 0.832483  [   64/  126]
train() client id: f_00000-6-2 loss: 0.954539  [   96/  126]
train() client id: f_00000-7-0 loss: 0.988723  [   32/  126]
train() client id: f_00000-7-1 loss: 0.811328  [   64/  126]
train() client id: f_00000-7-2 loss: 0.800780  [   96/  126]
train() client id: f_00000-8-0 loss: 0.755490  [   32/  126]
train() client id: f_00000-8-1 loss: 0.897572  [   64/  126]
train() client id: f_00000-8-2 loss: 1.016576  [   96/  126]
train() client id: f_00001-0-0 loss: 0.445283  [   32/  265]
train() client id: f_00001-0-1 loss: 0.447357  [   64/  265]
train() client id: f_00001-0-2 loss: 0.557735  [   96/  265]
train() client id: f_00001-0-3 loss: 0.525730  [  128/  265]
train() client id: f_00001-0-4 loss: 0.512850  [  160/  265]
train() client id: f_00001-0-5 loss: 0.551827  [  192/  265]
train() client id: f_00001-0-6 loss: 0.489966  [  224/  265]
train() client id: f_00001-0-7 loss: 0.512043  [  256/  265]
train() client id: f_00001-1-0 loss: 0.439577  [   32/  265]
train() client id: f_00001-1-1 loss: 0.507745  [   64/  265]
train() client id: f_00001-1-2 loss: 0.422659  [   96/  265]
train() client id: f_00001-1-3 loss: 0.528559  [  128/  265]
train() client id: f_00001-1-4 loss: 0.469680  [  160/  265]
train() client id: f_00001-1-5 loss: 0.579292  [  192/  265]
train() client id: f_00001-1-6 loss: 0.551062  [  224/  265]
train() client id: f_00001-1-7 loss: 0.520045  [  256/  265]
train() client id: f_00001-2-0 loss: 0.421048  [   32/  265]
train() client id: f_00001-2-1 loss: 0.478233  [   64/  265]
train() client id: f_00001-2-2 loss: 0.579828  [   96/  265]
train() client id: f_00001-2-3 loss: 0.547420  [  128/  265]
train() client id: f_00001-2-4 loss: 0.525695  [  160/  265]
train() client id: f_00001-2-5 loss: 0.465700  [  192/  265]
train() client id: f_00001-2-6 loss: 0.411937  [  224/  265]
train() client id: f_00001-2-7 loss: 0.507014  [  256/  265]
train() client id: f_00001-3-0 loss: 0.463729  [   32/  265]
train() client id: f_00001-3-1 loss: 0.528318  [   64/  265]
train() client id: f_00001-3-2 loss: 0.391948  [   96/  265]
train() client id: f_00001-3-3 loss: 0.503331  [  128/  265]
train() client id: f_00001-3-4 loss: 0.490598  [  160/  265]
train() client id: f_00001-3-5 loss: 0.549864  [  192/  265]
train() client id: f_00001-3-6 loss: 0.487997  [  224/  265]
train() client id: f_00001-3-7 loss: 0.484325  [  256/  265]
train() client id: f_00001-4-0 loss: 0.533240  [   32/  265]
train() client id: f_00001-4-1 loss: 0.583407  [   64/  265]
train() client id: f_00001-4-2 loss: 0.380576  [   96/  265]
train() client id: f_00001-4-3 loss: 0.476373  [  128/  265]
train() client id: f_00001-4-4 loss: 0.395447  [  160/  265]
train() client id: f_00001-4-5 loss: 0.574986  [  192/  265]
train() client id: f_00001-4-6 loss: 0.478870  [  224/  265]
train() client id: f_00001-4-7 loss: 0.500223  [  256/  265]
train() client id: f_00001-5-0 loss: 0.459569  [   32/  265]
train() client id: f_00001-5-1 loss: 0.643211  [   64/  265]
train() client id: f_00001-5-2 loss: 0.412863  [   96/  265]
train() client id: f_00001-5-3 loss: 0.493089  [  128/  265]
train() client id: f_00001-5-4 loss: 0.572863  [  160/  265]
train() client id: f_00001-5-5 loss: 0.435373  [  192/  265]
train() client id: f_00001-5-6 loss: 0.441783  [  224/  265]
train() client id: f_00001-5-7 loss: 0.471041  [  256/  265]
train() client id: f_00001-6-0 loss: 0.412119  [   32/  265]
train() client id: f_00001-6-1 loss: 0.583675  [   64/  265]
train() client id: f_00001-6-2 loss: 0.510395  [   96/  265]
train() client id: f_00001-6-3 loss: 0.483603  [  128/  265]
train() client id: f_00001-6-4 loss: 0.400506  [  160/  265]
train() client id: f_00001-6-5 loss: 0.459282  [  192/  265]
train() client id: f_00001-6-6 loss: 0.584824  [  224/  265]
train() client id: f_00001-6-7 loss: 0.486903  [  256/  265]
train() client id: f_00001-7-0 loss: 0.394218  [   32/  265]
train() client id: f_00001-7-1 loss: 0.505106  [   64/  265]
train() client id: f_00001-7-2 loss: 0.532388  [   96/  265]
train() client id: f_00001-7-3 loss: 0.507503  [  128/  265]
train() client id: f_00001-7-4 loss: 0.483178  [  160/  265]
train() client id: f_00001-7-5 loss: 0.484084  [  192/  265]
train() client id: f_00001-7-6 loss: 0.459392  [  224/  265]
train() client id: f_00001-7-7 loss: 0.452551  [  256/  265]
train() client id: f_00001-8-0 loss: 0.569114  [   32/  265]
train() client id: f_00001-8-1 loss: 0.413660  [   64/  265]
train() client id: f_00001-8-2 loss: 0.474400  [   96/  265]
train() client id: f_00001-8-3 loss: 0.484167  [  128/  265]
train() client id: f_00001-8-4 loss: 0.440665  [  160/  265]
train() client id: f_00001-8-5 loss: 0.437341  [  192/  265]
train() client id: f_00001-8-6 loss: 0.477852  [  224/  265]
train() client id: f_00001-8-7 loss: 0.621103  [  256/  265]
train() client id: f_00002-0-0 loss: 1.124289  [   32/  124]
train() client id: f_00002-0-1 loss: 0.960552  [   64/  124]
train() client id: f_00002-0-2 loss: 1.293163  [   96/  124]
train() client id: f_00002-1-0 loss: 0.837579  [   32/  124]
train() client id: f_00002-1-1 loss: 1.206151  [   64/  124]
train() client id: f_00002-1-2 loss: 1.109112  [   96/  124]
train() client id: f_00002-2-0 loss: 1.078408  [   32/  124]
train() client id: f_00002-2-1 loss: 0.909177  [   64/  124]
train() client id: f_00002-2-2 loss: 0.931387  [   96/  124]
train() client id: f_00002-3-0 loss: 1.063501  [   32/  124]
train() client id: f_00002-3-1 loss: 0.904158  [   64/  124]
train() client id: f_00002-3-2 loss: 1.133524  [   96/  124]
train() client id: f_00002-4-0 loss: 0.901557  [   32/  124]
train() client id: f_00002-4-1 loss: 1.037401  [   64/  124]
train() client id: f_00002-4-2 loss: 1.017462  [   96/  124]
train() client id: f_00002-5-0 loss: 0.984518  [   32/  124]
train() client id: f_00002-5-1 loss: 0.992832  [   64/  124]
train() client id: f_00002-5-2 loss: 1.052774  [   96/  124]
train() client id: f_00002-6-0 loss: 0.773838  [   32/  124]
train() client id: f_00002-6-1 loss: 1.016302  [   64/  124]
train() client id: f_00002-6-2 loss: 0.893355  [   96/  124]
train() client id: f_00002-7-0 loss: 0.893428  [   32/  124]
train() client id: f_00002-7-1 loss: 1.078987  [   64/  124]
train() client id: f_00002-7-2 loss: 0.750922  [   96/  124]
train() client id: f_00002-8-0 loss: 1.024010  [   32/  124]
train() client id: f_00002-8-1 loss: 0.651135  [   64/  124]
train() client id: f_00002-8-2 loss: 0.949409  [   96/  124]
train() client id: f_00003-0-0 loss: 0.697634  [   32/   43]
train() client id: f_00003-1-0 loss: 0.734754  [   32/   43]
train() client id: f_00003-2-0 loss: 0.646654  [   32/   43]
train() client id: f_00003-3-0 loss: 0.471578  [   32/   43]
train() client id: f_00003-4-0 loss: 0.616859  [   32/   43]
train() client id: f_00003-5-0 loss: 0.657141  [   32/   43]
train() client id: f_00003-6-0 loss: 0.678286  [   32/   43]
train() client id: f_00003-7-0 loss: 0.751674  [   32/   43]
train() client id: f_00003-8-0 loss: 0.590303  [   32/   43]
train() client id: f_00004-0-0 loss: 0.799732  [   32/  306]
train() client id: f_00004-0-1 loss: 0.912214  [   64/  306]
train() client id: f_00004-0-2 loss: 0.830389  [   96/  306]
train() client id: f_00004-0-3 loss: 0.955091  [  128/  306]
train() client id: f_00004-0-4 loss: 0.915697  [  160/  306]
train() client id: f_00004-0-5 loss: 0.696160  [  192/  306]
train() client id: f_00004-0-6 loss: 0.809817  [  224/  306]
train() client id: f_00004-0-7 loss: 0.867018  [  256/  306]
train() client id: f_00004-0-8 loss: 0.776830  [  288/  306]
train() client id: f_00004-1-0 loss: 0.804962  [   32/  306]
train() client id: f_00004-1-1 loss: 0.803472  [   64/  306]
train() client id: f_00004-1-2 loss: 0.702001  [   96/  306]
train() client id: f_00004-1-3 loss: 0.924685  [  128/  306]
train() client id: f_00004-1-4 loss: 0.829163  [  160/  306]
train() client id: f_00004-1-5 loss: 0.964375  [  192/  306]
train() client id: f_00004-1-6 loss: 0.798717  [  224/  306]
train() client id: f_00004-1-7 loss: 0.919006  [  256/  306]
train() client id: f_00004-1-8 loss: 0.764613  [  288/  306]
train() client id: f_00004-2-0 loss: 0.812642  [   32/  306]
train() client id: f_00004-2-1 loss: 0.787630  [   64/  306]
train() client id: f_00004-2-2 loss: 0.895907  [   96/  306]
train() client id: f_00004-2-3 loss: 0.865202  [  128/  306]
train() client id: f_00004-2-4 loss: 0.897171  [  160/  306]
train() client id: f_00004-2-5 loss: 0.750238  [  192/  306]
train() client id: f_00004-2-6 loss: 0.810595  [  224/  306]
train() client id: f_00004-2-7 loss: 0.914260  [  256/  306]
train() client id: f_00004-2-8 loss: 0.915631  [  288/  306]
train() client id: f_00004-3-0 loss: 0.742494  [   32/  306]
train() client id: f_00004-3-1 loss: 1.124959  [   64/  306]
train() client id: f_00004-3-2 loss: 0.785304  [   96/  306]
train() client id: f_00004-3-3 loss: 0.824096  [  128/  306]
train() client id: f_00004-3-4 loss: 0.799464  [  160/  306]
train() client id: f_00004-3-5 loss: 0.801418  [  192/  306]
train() client id: f_00004-3-6 loss: 0.948196  [  224/  306]
train() client id: f_00004-3-7 loss: 0.704989  [  256/  306]
train() client id: f_00004-3-8 loss: 0.899039  [  288/  306]
train() client id: f_00004-4-0 loss: 0.827953  [   32/  306]
train() client id: f_00004-4-1 loss: 0.877774  [   64/  306]
train() client id: f_00004-4-2 loss: 0.850163  [   96/  306]
train() client id: f_00004-4-3 loss: 0.675713  [  128/  306]
train() client id: f_00004-4-4 loss: 0.885699  [  160/  306]
train() client id: f_00004-4-5 loss: 0.926781  [  192/  306]
train() client id: f_00004-4-6 loss: 0.777968  [  224/  306]
train() client id: f_00004-4-7 loss: 0.816204  [  256/  306]
train() client id: f_00004-4-8 loss: 1.021309  [  288/  306]
train() client id: f_00004-5-0 loss: 0.917607  [   32/  306]
train() client id: f_00004-5-1 loss: 0.838715  [   64/  306]
train() client id: f_00004-5-2 loss: 0.704172  [   96/  306]
train() client id: f_00004-5-3 loss: 0.798152  [  128/  306]
train() client id: f_00004-5-4 loss: 0.627662  [  160/  306]
train() client id: f_00004-5-5 loss: 0.888586  [  192/  306]
train() client id: f_00004-5-6 loss: 1.120283  [  224/  306]
train() client id: f_00004-5-7 loss: 0.857872  [  256/  306]
train() client id: f_00004-5-8 loss: 0.785080  [  288/  306]
train() client id: f_00004-6-0 loss: 0.895619  [   32/  306]
train() client id: f_00004-6-1 loss: 0.819407  [   64/  306]
train() client id: f_00004-6-2 loss: 0.823291  [   96/  306]
train() client id: f_00004-6-3 loss: 0.769315  [  128/  306]
train() client id: f_00004-6-4 loss: 0.814343  [  160/  306]
train() client id: f_00004-6-5 loss: 0.691249  [  192/  306]
train() client id: f_00004-6-6 loss: 0.886478  [  224/  306]
train() client id: f_00004-6-7 loss: 0.935554  [  256/  306]
train() client id: f_00004-6-8 loss: 0.864239  [  288/  306]
train() client id: f_00004-7-0 loss: 0.761702  [   32/  306]
train() client id: f_00004-7-1 loss: 0.754809  [   64/  306]
train() client id: f_00004-7-2 loss: 0.895867  [   96/  306]
train() client id: f_00004-7-3 loss: 0.686966  [  128/  306]
train() client id: f_00004-7-4 loss: 0.791580  [  160/  306]
train() client id: f_00004-7-5 loss: 0.960644  [  192/  306]
train() client id: f_00004-7-6 loss: 0.832466  [  224/  306]
train() client id: f_00004-7-7 loss: 0.975044  [  256/  306]
train() client id: f_00004-7-8 loss: 0.935731  [  288/  306]
train() client id: f_00004-8-0 loss: 0.803921  [   32/  306]
train() client id: f_00004-8-1 loss: 0.807336  [   64/  306]
train() client id: f_00004-8-2 loss: 0.823011  [   96/  306]
train() client id: f_00004-8-3 loss: 0.866203  [  128/  306]
train() client id: f_00004-8-4 loss: 0.749166  [  160/  306]
train() client id: f_00004-8-5 loss: 0.795531  [  192/  306]
train() client id: f_00004-8-6 loss: 0.892045  [  224/  306]
train() client id: f_00004-8-7 loss: 0.875512  [  256/  306]
train() client id: f_00004-8-8 loss: 0.887932  [  288/  306]
train() client id: f_00005-0-0 loss: 0.453368  [   32/  146]
train() client id: f_00005-0-1 loss: 0.989590  [   64/  146]
train() client id: f_00005-0-2 loss: 0.787210  [   96/  146]
train() client id: f_00005-0-3 loss: 0.586052  [  128/  146]
train() client id: f_00005-1-0 loss: 0.524716  [   32/  146]
train() client id: f_00005-1-1 loss: 0.633559  [   64/  146]
train() client id: f_00005-1-2 loss: 0.417613  [   96/  146]
train() client id: f_00005-1-3 loss: 0.935660  [  128/  146]
train() client id: f_00005-2-0 loss: 0.748895  [   32/  146]
train() client id: f_00005-2-1 loss: 0.669059  [   64/  146]
train() client id: f_00005-2-2 loss: 0.583928  [   96/  146]
train() client id: f_00005-2-3 loss: 0.663373  [  128/  146]
train() client id: f_00005-3-0 loss: 0.440349  [   32/  146]
train() client id: f_00005-3-1 loss: 0.694777  [   64/  146]
train() client id: f_00005-3-2 loss: 0.492339  [   96/  146]
train() client id: f_00005-3-3 loss: 0.867437  [  128/  146]
train() client id: f_00005-4-0 loss: 0.685925  [   32/  146]
train() client id: f_00005-4-1 loss: 0.461737  [   64/  146]
train() client id: f_00005-4-2 loss: 0.932607  [   96/  146]
train() client id: f_00005-4-3 loss: 0.680349  [  128/  146]
train() client id: f_00005-5-0 loss: 0.545360  [   32/  146]
train() client id: f_00005-5-1 loss: 0.609306  [   64/  146]
train() client id: f_00005-5-2 loss: 0.741062  [   96/  146]
train() client id: f_00005-5-3 loss: 0.583557  [  128/  146]
train() client id: f_00005-6-0 loss: 0.425156  [   32/  146]
train() client id: f_00005-6-1 loss: 1.001202  [   64/  146]
train() client id: f_00005-6-2 loss: 0.658906  [   96/  146]
train() client id: f_00005-6-3 loss: 0.672381  [  128/  146]
train() client id: f_00005-7-0 loss: 0.458469  [   32/  146]
train() client id: f_00005-7-1 loss: 0.888853  [   64/  146]
train() client id: f_00005-7-2 loss: 0.549097  [   96/  146]
train() client id: f_00005-7-3 loss: 0.626678  [  128/  146]
train() client id: f_00005-8-0 loss: 0.641516  [   32/  146]
train() client id: f_00005-8-1 loss: 0.543940  [   64/  146]
train() client id: f_00005-8-2 loss: 0.677827  [   96/  146]
train() client id: f_00005-8-3 loss: 0.655226  [  128/  146]
train() client id: f_00006-0-0 loss: 0.508668  [   32/   54]
train() client id: f_00006-1-0 loss: 0.486353  [   32/   54]
train() client id: f_00006-2-0 loss: 0.455126  [   32/   54]
train() client id: f_00006-3-0 loss: 0.508610  [   32/   54]
train() client id: f_00006-4-0 loss: 0.509030  [   32/   54]
train() client id: f_00006-5-0 loss: 0.409875  [   32/   54]
train() client id: f_00006-6-0 loss: 0.413909  [   32/   54]
train() client id: f_00006-7-0 loss: 0.460112  [   32/   54]
train() client id: f_00006-8-0 loss: 0.411769  [   32/   54]
train() client id: f_00007-0-0 loss: 0.546282  [   32/  179]
train() client id: f_00007-0-1 loss: 0.557276  [   64/  179]
train() client id: f_00007-0-2 loss: 0.497117  [   96/  179]
train() client id: f_00007-0-3 loss: 0.778957  [  128/  179]
train() client id: f_00007-0-4 loss: 0.657752  [  160/  179]
train() client id: f_00007-1-0 loss: 0.832138  [   32/  179]
train() client id: f_00007-1-1 loss: 0.529848  [   64/  179]
train() client id: f_00007-1-2 loss: 0.511996  [   96/  179]
train() client id: f_00007-1-3 loss: 0.566526  [  128/  179]
train() client id: f_00007-1-4 loss: 0.589321  [  160/  179]
train() client id: f_00007-2-0 loss: 0.511339  [   32/  179]
train() client id: f_00007-2-1 loss: 0.558728  [   64/  179]
train() client id: f_00007-2-2 loss: 0.569483  [   96/  179]
train() client id: f_00007-2-3 loss: 0.663329  [  128/  179]
train() client id: f_00007-2-4 loss: 0.732167  [  160/  179]
train() client id: f_00007-3-0 loss: 0.480510  [   32/  179]
train() client id: f_00007-3-1 loss: 0.812074  [   64/  179]
train() client id: f_00007-3-2 loss: 0.430821  [   96/  179]
train() client id: f_00007-3-3 loss: 0.616282  [  128/  179]
train() client id: f_00007-3-4 loss: 0.483312  [  160/  179]
train() client id: f_00007-4-0 loss: 0.690995  [   32/  179]
train() client id: f_00007-4-1 loss: 0.586568  [   64/  179]
train() client id: f_00007-4-2 loss: 0.505310  [   96/  179]
train() client id: f_00007-4-3 loss: 0.763805  [  128/  179]
train() client id: f_00007-4-4 loss: 0.416240  [  160/  179]
train() client id: f_00007-5-0 loss: 0.442841  [   32/  179]
train() client id: f_00007-5-1 loss: 0.528553  [   64/  179]
train() client id: f_00007-5-2 loss: 0.524244  [   96/  179]
train() client id: f_00007-5-3 loss: 0.547250  [  128/  179]
train() client id: f_00007-5-4 loss: 0.598531  [  160/  179]
train() client id: f_00007-6-0 loss: 0.422391  [   32/  179]
train() client id: f_00007-6-1 loss: 0.376630  [   64/  179]
train() client id: f_00007-6-2 loss: 0.886333  [   96/  179]
train() client id: f_00007-6-3 loss: 0.494948  [  128/  179]
train() client id: f_00007-6-4 loss: 0.535408  [  160/  179]
train() client id: f_00007-7-0 loss: 0.473107  [   32/  179]
train() client id: f_00007-7-1 loss: 0.728449  [   64/  179]
train() client id: f_00007-7-2 loss: 0.575033  [   96/  179]
train() client id: f_00007-7-3 loss: 0.604618  [  128/  179]
train() client id: f_00007-7-4 loss: 0.471653  [  160/  179]
train() client id: f_00007-8-0 loss: 0.503028  [   32/  179]
train() client id: f_00007-8-1 loss: 0.412892  [   64/  179]
train() client id: f_00007-8-2 loss: 0.628607  [   96/  179]
train() client id: f_00007-8-3 loss: 0.686948  [  128/  179]
train() client id: f_00007-8-4 loss: 0.500423  [  160/  179]
train() client id: f_00008-0-0 loss: 0.728112  [   32/  130]
train() client id: f_00008-0-1 loss: 0.769527  [   64/  130]
train() client id: f_00008-0-2 loss: 0.740027  [   96/  130]
train() client id: f_00008-0-3 loss: 0.763722  [  128/  130]
train() client id: f_00008-1-0 loss: 0.823416  [   32/  130]
train() client id: f_00008-1-1 loss: 0.625692  [   64/  130]
train() client id: f_00008-1-2 loss: 0.674202  [   96/  130]
train() client id: f_00008-1-3 loss: 0.859999  [  128/  130]
train() client id: f_00008-2-0 loss: 0.681557  [   32/  130]
train() client id: f_00008-2-1 loss: 0.849713  [   64/  130]
train() client id: f_00008-2-2 loss: 0.736245  [   96/  130]
train() client id: f_00008-2-3 loss: 0.698834  [  128/  130]
train() client id: f_00008-3-0 loss: 0.854235  [   32/  130]
train() client id: f_00008-3-1 loss: 0.668915  [   64/  130]
train() client id: f_00008-3-2 loss: 0.685404  [   96/  130]
train() client id: f_00008-3-3 loss: 0.784378  [  128/  130]
train() client id: f_00008-4-0 loss: 0.734593  [   32/  130]
train() client id: f_00008-4-1 loss: 0.737278  [   64/  130]
train() client id: f_00008-4-2 loss: 0.761922  [   96/  130]
train() client id: f_00008-4-3 loss: 0.762513  [  128/  130]
train() client id: f_00008-5-0 loss: 0.686597  [   32/  130]
train() client id: f_00008-5-1 loss: 0.751190  [   64/  130]
train() client id: f_00008-5-2 loss: 0.756078  [   96/  130]
train() client id: f_00008-5-3 loss: 0.788201  [  128/  130]
train() client id: f_00008-6-0 loss: 0.663449  [   32/  130]
train() client id: f_00008-6-1 loss: 0.728965  [   64/  130]
train() client id: f_00008-6-2 loss: 0.791611  [   96/  130]
train() client id: f_00008-6-3 loss: 0.796909  [  128/  130]
train() client id: f_00008-7-0 loss: 0.681725  [   32/  130]
train() client id: f_00008-7-1 loss: 0.687706  [   64/  130]
train() client id: f_00008-7-2 loss: 0.824885  [   96/  130]
train() client id: f_00008-7-3 loss: 0.785714  [  128/  130]
train() client id: f_00008-8-0 loss: 0.758512  [   32/  130]
train() client id: f_00008-8-1 loss: 0.755330  [   64/  130]
train() client id: f_00008-8-2 loss: 0.721693  [   96/  130]
train() client id: f_00008-8-3 loss: 0.722627  [  128/  130]
train() client id: f_00009-0-0 loss: 1.075069  [   32/  118]
train() client id: f_00009-0-1 loss: 0.789691  [   64/  118]
train() client id: f_00009-0-2 loss: 0.988849  [   96/  118]
train() client id: f_00009-1-0 loss: 0.844921  [   32/  118]
train() client id: f_00009-1-1 loss: 0.935125  [   64/  118]
train() client id: f_00009-1-2 loss: 0.924842  [   96/  118]
train() client id: f_00009-2-0 loss: 0.848923  [   32/  118]
train() client id: f_00009-2-1 loss: 0.771032  [   64/  118]
train() client id: f_00009-2-2 loss: 1.032130  [   96/  118]
train() client id: f_00009-3-0 loss: 1.024686  [   32/  118]
train() client id: f_00009-3-1 loss: 0.659041  [   64/  118]
train() client id: f_00009-3-2 loss: 0.735418  [   96/  118]
train() client id: f_00009-4-0 loss: 1.015411  [   32/  118]
train() client id: f_00009-4-1 loss: 0.716081  [   64/  118]
train() client id: f_00009-4-2 loss: 0.821237  [   96/  118]
train() client id: f_00009-5-0 loss: 0.948971  [   32/  118]
train() client id: f_00009-5-1 loss: 0.882502  [   64/  118]
train() client id: f_00009-5-2 loss: 0.758748  [   96/  118]
train() client id: f_00009-6-0 loss: 0.888600  [   32/  118]
train() client id: f_00009-6-1 loss: 1.011679  [   64/  118]
train() client id: f_00009-6-2 loss: 0.685841  [   96/  118]
train() client id: f_00009-7-0 loss: 0.890884  [   32/  118]
train() client id: f_00009-7-1 loss: 0.800698  [   64/  118]
train() client id: f_00009-7-2 loss: 0.921016  [   96/  118]
train() client id: f_00009-8-0 loss: 0.931401  [   32/  118]
train() client id: f_00009-8-1 loss: 0.764334  [   64/  118]
train() client id: f_00009-8-2 loss: 0.845935  [   96/  118]
At round 53 accuracy: 0.6472148541114059
At round 53 training accuracy: 0.5902079141515761
At round 53 training loss: 0.8272495818118467
update_location
xs = -4.528292 166.001589 175.045120 -175.943528 64.896481 -100.217951 -217.215960 238.375741 -1.680116 159.695607 
ys = 252.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -182.154970 4.001482 
xs mean: 30.44286915941286
ys mean: 9.371751218646875
dists_uav = 271.700539 194.418394 201.599945 203.628078 119.213990 141.603521 239.149615 259.211836 207.805813 188.464052 
uav_gains = -113.225218 -107.331940 -107.782973 -107.911561 -101.908376 -103.778960 -110.404178 -112.097971 -108.179072 -106.959926 
uav_gains_db_mean: -107.95801749527124
dists_bs = 187.297877 376.436614 390.763502 152.458096 297.326720 187.724057 183.059212 456.710481 396.987978 375.847898 
bs_gains = -103.197979 -111.686431 -112.140651 -100.695273 -108.817609 -103.225617 -102.919624 -114.037007 -112.332825 -111.667399 
bs_gains_db_mean: -108.07204147229436
Round 54
-------------------------------
ene_coms = [0.01361922 0.01205863 0.00927868 0.00935069 0.00988989 0.00727448
 0.01098964 0.01244314 0.01267272 0.01204137]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.85083418 7.93672664 3.74936138 1.36018149 9.12520452 4.37885879
 1.70076748 5.4030802  3.9595936  3.59936021]
obj_prev = 45.063968479946304
eta_min = 2.6483489054347357e-24	eta_max = 0.9513179092121365
af = 9.457122799293558	bf = 1.061668834611905	zeta = 10.402835079222914	eta = 0.9090909090909091
af = 9.457122799293558	bf = 1.061668834611905	zeta = 21.593317999485695	eta = 0.43796524459644437
af = 9.457122799293558	bf = 1.061668834611905	zeta = 15.78126266610506	eta = 0.5992627459148457
af = 9.457122799293558	bf = 1.061668834611905	zeta = 14.733437125383404	eta = 0.6418816409784257
af = 9.457122799293558	bf = 1.061668834611905	zeta = 14.672515057154802	eta = 0.6445468116716604
af = 9.457122799293558	bf = 1.061668834611905	zeta = 14.672286977258464	eta = 0.6445568311164966
eta = 0.6445568311164966
ene_coms = [0.01361922 0.01205863 0.00927868 0.00935069 0.00988989 0.00727448
 0.01098964 0.01244314 0.01267272 0.01204137]
ene_comp = [0.03624109 0.07622135 0.03566584 0.01236799 0.08801409 0.04199365
 0.0155319  0.05148536 0.03739161 0.03394007]
ene_total = [1.35859547 2.40545576 1.22464971 0.59179146 2.66769075 1.34245961
 0.72265984 1.74192609 1.36415437 1.25290391]
ti_comp = [0.86570668 0.88131258 0.90911208 0.90839195 0.90299998 0.92915407
 0.89200243 0.87746744 0.87517168 0.88148515]
ti_coms = [0.13619219 0.1205863  0.09278679 0.09350693 0.0988989  0.07274481
 0.10989644 0.12443144 0.1267272  0.12041372]
t_total = [27.29977341 27.29977341 27.29977341 27.29977341 27.29977341 27.29977341
 27.29977341 27.29977341 27.29977341 27.29977341]
ene_coms = [0.01361922 0.01205863 0.00927868 0.00935069 0.00988989 0.00727448
 0.01098964 0.01244314 0.01267272 0.01204137]
ene_comp = [3.96956019e-06 3.56328023e-05 3.43085601e-06 1.43295025e-07
 5.22590000e-05 5.36111836e-06 2.94321166e-07 1.10782043e-05
 4.26595080e-06 3.14476583e-06]
ene_total = [0.3712051  0.32954486 0.25291925 0.25479189 0.270904   0.19836137
 0.2994542  0.33935305 0.34542292 0.32818939]
optimize_network iter = 0 obj = 2.9901460315877615
eta = 0.6445568311164966
freqs = [20931509.02233179 43243084.87810559 19615754.58731639  6807630.0102119
 48734268.35370749 22597785.71671405  8706196.8743755  29337478.59481804
 21362440.25897724 19251641.62058752]
eta_min = 0.6445568311164985	eta_max = 0.7570543686121822
af = 0.002636991422372342	bf = 1.061668834611905	zeta = 0.0029006905646095766	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01361922 0.01205863 0.00927868 0.00935069 0.00988989 0.00727448
 0.01098964 0.01244314 0.01267272 0.01204137]
ene_comp = [8.61443861e-07 7.73276064e-06 7.44538365e-07 3.10968000e-08
 1.13408520e-05 1.16342927e-06 6.38713483e-08 2.40410792e-06
 9.25764306e-07 6.82453242e-07]
ene_total = [1.45103436 1.28550678 0.98859641 0.99619247 1.05484145 0.77512092
 1.17080391 1.32590386 1.35020455 1.28291713]
ti_comp = [0.54860632 0.56421222 0.59201172 0.59129159 0.58589962 0.61205371
 0.57490208 0.56036708 0.55807132 0.5643848 ]
ti_coms = [0.13619219 0.1205863  0.09278679 0.09350693 0.0988989  0.07274481
 0.10989644 0.12443144 0.1267272  0.12041372]
t_total = [27.29977341 27.29977341 27.29977341 27.29977341 27.29977341 27.29977341
 27.29977341 27.29977341 27.29977341 27.29977341]
ene_coms = [0.01361922 0.01205863 0.00927868 0.00935069 0.00988989 0.00727448
 0.01098964 0.01244314 0.01267272 0.01204137]
ene_comp = [2.51554697e-06 2.21256000e-05 2.05895500e-06 8.60685287e-08
 3.15907736e-05 3.14428504e-06 1.80316695e-07 6.91283642e-06
 2.66988812e-06 1.95226075e-06]
ene_total = [0.54303604 0.48160425 0.36998044 0.37277264 0.39552389 0.2901255
 0.43811385 0.49632661 0.5053096  0.48011206]
optimize_network iter = 1 obj = 4.372904890926687
eta = 0.7570543686121822
freqs = [20931509.02233177 42804969.07534508 19088956.19792116  6627619.3671282
 47598033.86137    21739728.87277305  8560333.52535313 29111944.93754781
 21229729.0619233  19054497.70115985]
eta_min = 0.7570543686121888	eta_max = 0.7570543686121642
af = 0.002548521793920347	bf = 1.061668834611905	zeta = 0.002803373973312382	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01361922 0.01205863 0.00927868 0.00935069 0.00988989 0.00727448
 0.01098964 0.01244314 0.01267272 0.01204137]
ene_comp = [8.61443861e-07 7.57686599e-06 7.05084885e-07 2.94739898e-08
 1.08181951e-05 1.07675392e-06 6.17490796e-08 2.36728655e-06
 9.14297669e-07 6.68547661e-07]
ene_total = [1.45103436 1.28549018 0.98859221 0.99619229 1.05478577 0.77511168
 1.17080368 1.32589993 1.35020333 1.28291565]
ti_comp = [0.54860632 0.56421222 0.59201172 0.59129159 0.58589962 0.61205371
 0.57490208 0.56036708 0.55807132 0.5643848 ]
ti_coms = [0.13619219 0.1205863  0.09278679 0.09350693 0.0988989  0.07274481
 0.10989644 0.12443144 0.1267272  0.12041372]
t_total = [27.29977341 27.29977341 27.29977341 27.29977341 27.29977341 27.29977341
 27.29977341 27.29977341 27.29977341 27.29977341]
ene_coms = [0.01361922 0.01205863 0.00927868 0.00935069 0.00988989 0.00727448
 0.01098964 0.01244314 0.01267272 0.01204137]
ene_comp = [2.51554697e-06 2.21256000e-05 2.05895500e-06 8.60685287e-08
 3.15907736e-05 3.14428504e-06 1.80316695e-07 6.91283642e-06
 2.66988812e-06 1.95226075e-06]
ene_total = [0.54303604 0.48160425 0.36998044 0.37277264 0.39552389 0.2901255
 0.43811385 0.49632661 0.5053096  0.48011206]
optimize_network iter = 2 obj = 4.372904890926364
eta = 0.7570543686121642
freqs = [20931509.02233163 42804969.07534491 19088956.19792117  6627619.3671282
 47598033.86136997 21739728.87277312  8560333.52535311 29111944.93754768
 21229729.06192319 19054497.70115977]
Done!
ene_coms = [0.01361922 0.01205863 0.00927868 0.00935069 0.00988989 0.00727448
 0.01098964 0.01244314 0.01267272 0.01204137]
ene_comp = [2.48418616e-06 2.18497646e-05 2.03328643e-06 8.49955300e-08
 3.11969378e-05 3.10508588e-06 1.78068723e-07 6.82665550e-06
 2.63660317e-06 1.92792232e-06]
ene_total = [0.0136217  0.01208048 0.00928071 0.00935078 0.00992109 0.00727759
 0.01098982 0.01244997 0.01267536 0.0120433 ]
At round 54 energy consumption: 0.10969079536176295
At round 54 eta: 0.7570543686121642
At round 54 a_n: 9.685127119911243
At round 54 local rounds: 9.113617612476084
At round 54 global rounds: 39.86540965805642
gradient difference: 0.6083876490592957
train() client id: f_00000-0-0 loss: 1.016922  [   32/  126]
train() client id: f_00000-0-1 loss: 0.821675  [   64/  126]
train() client id: f_00000-0-2 loss: 0.837919  [   96/  126]
train() client id: f_00000-1-0 loss: 0.662434  [   32/  126]
train() client id: f_00000-1-1 loss: 1.034782  [   64/  126]
train() client id: f_00000-1-2 loss: 0.840765  [   96/  126]
train() client id: f_00000-2-0 loss: 0.726570  [   32/  126]
train() client id: f_00000-2-1 loss: 0.787804  [   64/  126]
train() client id: f_00000-2-2 loss: 0.902125  [   96/  126]
train() client id: f_00000-3-0 loss: 0.784023  [   32/  126]
train() client id: f_00000-3-1 loss: 0.663174  [   64/  126]
train() client id: f_00000-3-2 loss: 0.939036  [   96/  126]
train() client id: f_00000-4-0 loss: 0.815939  [   32/  126]
train() client id: f_00000-4-1 loss: 0.627629  [   64/  126]
train() client id: f_00000-4-2 loss: 0.839306  [   96/  126]
train() client id: f_00000-5-0 loss: 0.760343  [   32/  126]
train() client id: f_00000-5-1 loss: 0.887431  [   64/  126]
train() client id: f_00000-5-2 loss: 0.638538  [   96/  126]
train() client id: f_00000-6-0 loss: 0.581730  [   32/  126]
train() client id: f_00000-6-1 loss: 0.848921  [   64/  126]
train() client id: f_00000-6-2 loss: 0.771311  [   96/  126]
train() client id: f_00000-7-0 loss: 0.719626  [   32/  126]
train() client id: f_00000-7-1 loss: 0.800625  [   64/  126]
train() client id: f_00000-7-2 loss: 0.752320  [   96/  126]
train() client id: f_00000-8-0 loss: 0.644564  [   32/  126]
train() client id: f_00000-8-1 loss: 0.848152  [   64/  126]
train() client id: f_00000-8-2 loss: 0.773026  [   96/  126]
train() client id: f_00001-0-0 loss: 0.460333  [   32/  265]
train() client id: f_00001-0-1 loss: 0.364511  [   64/  265]
train() client id: f_00001-0-2 loss: 0.267458  [   96/  265]
train() client id: f_00001-0-3 loss: 0.359995  [  128/  265]
train() client id: f_00001-0-4 loss: 0.276284  [  160/  265]
train() client id: f_00001-0-5 loss: 0.301824  [  192/  265]
train() client id: f_00001-0-6 loss: 0.364100  [  224/  265]
train() client id: f_00001-0-7 loss: 0.346154  [  256/  265]
train() client id: f_00001-1-0 loss: 0.392148  [   32/  265]
train() client id: f_00001-1-1 loss: 0.362757  [   64/  265]
train() client id: f_00001-1-2 loss: 0.383608  [   96/  265]
train() client id: f_00001-1-3 loss: 0.293215  [  128/  265]
train() client id: f_00001-1-4 loss: 0.310092  [  160/  265]
train() client id: f_00001-1-5 loss: 0.347024  [  192/  265]
train() client id: f_00001-1-6 loss: 0.338393  [  224/  265]
train() client id: f_00001-1-7 loss: 0.379280  [  256/  265]
train() client id: f_00001-2-0 loss: 0.376013  [   32/  265]
train() client id: f_00001-2-1 loss: 0.281103  [   64/  265]
train() client id: f_00001-2-2 loss: 0.338499  [   96/  265]
train() client id: f_00001-2-3 loss: 0.346682  [  128/  265]
train() client id: f_00001-2-4 loss: 0.424949  [  160/  265]
train() client id: f_00001-2-5 loss: 0.272532  [  192/  265]
train() client id: f_00001-2-6 loss: 0.373526  [  224/  265]
train() client id: f_00001-2-7 loss: 0.323124  [  256/  265]
train() client id: f_00001-3-0 loss: 0.341536  [   32/  265]
train() client id: f_00001-3-1 loss: 0.302498  [   64/  265]
train() client id: f_00001-3-2 loss: 0.265431  [   96/  265]
train() client id: f_00001-3-3 loss: 0.346004  [  128/  265]
train() client id: f_00001-3-4 loss: 0.454212  [  160/  265]
train() client id: f_00001-3-5 loss: 0.442037  [  192/  265]
train() client id: f_00001-3-6 loss: 0.314994  [  224/  265]
train() client id: f_00001-3-7 loss: 0.247435  [  256/  265]
train() client id: f_00001-4-0 loss: 0.330262  [   32/  265]
train() client id: f_00001-4-1 loss: 0.283496  [   64/  265]
train() client id: f_00001-4-2 loss: 0.249466  [   96/  265]
train() client id: f_00001-4-3 loss: 0.470033  [  128/  265]
train() client id: f_00001-4-4 loss: 0.328146  [  160/  265]
train() client id: f_00001-4-5 loss: 0.332890  [  192/  265]
train() client id: f_00001-4-6 loss: 0.381944  [  224/  265]
train() client id: f_00001-4-7 loss: 0.289425  [  256/  265]
train() client id: f_00001-5-0 loss: 0.414535  [   32/  265]
train() client id: f_00001-5-1 loss: 0.364512  [   64/  265]
train() client id: f_00001-5-2 loss: 0.260073  [   96/  265]
train() client id: f_00001-5-3 loss: 0.355475  [  128/  265]
train() client id: f_00001-5-4 loss: 0.399297  [  160/  265]
train() client id: f_00001-5-5 loss: 0.299435  [  192/  265]
train() client id: f_00001-5-6 loss: 0.299013  [  224/  265]
train() client id: f_00001-5-7 loss: 0.253732  [  256/  265]
train() client id: f_00001-6-0 loss: 0.356420  [   32/  265]
train() client id: f_00001-6-1 loss: 0.282248  [   64/  265]
train() client id: f_00001-6-2 loss: 0.204667  [   96/  265]
train() client id: f_00001-6-3 loss: 0.259634  [  128/  265]
train() client id: f_00001-6-4 loss: 0.277254  [  160/  265]
train() client id: f_00001-6-5 loss: 0.406575  [  192/  265]
train() client id: f_00001-6-6 loss: 0.446960  [  224/  265]
train() client id: f_00001-6-7 loss: 0.356871  [  256/  265]
train() client id: f_00001-7-0 loss: 0.278032  [   32/  265]
train() client id: f_00001-7-1 loss: 0.404846  [   64/  265]
train() client id: f_00001-7-2 loss: 0.222720  [   96/  265]
train() client id: f_00001-7-3 loss: 0.357247  [  128/  265]
train() client id: f_00001-7-4 loss: 0.307281  [  160/  265]
train() client id: f_00001-7-5 loss: 0.385003  [  192/  265]
train() client id: f_00001-7-6 loss: 0.393159  [  224/  265]
train() client id: f_00001-7-7 loss: 0.278145  [  256/  265]
train() client id: f_00001-8-0 loss: 0.388693  [   32/  265]
train() client id: f_00001-8-1 loss: 0.492883  [   64/  265]
train() client id: f_00001-8-2 loss: 0.230408  [   96/  265]
train() client id: f_00001-8-3 loss: 0.268497  [  128/  265]
train() client id: f_00001-8-4 loss: 0.310266  [  160/  265]
train() client id: f_00001-8-5 loss: 0.342282  [  192/  265]
train() client id: f_00001-8-6 loss: 0.296060  [  224/  265]
train() client id: f_00001-8-7 loss: 0.279677  [  256/  265]
train() client id: f_00002-0-0 loss: 1.041247  [   32/  124]
train() client id: f_00002-0-1 loss: 0.916550  [   64/  124]
train() client id: f_00002-0-2 loss: 1.113792  [   96/  124]
train() client id: f_00002-1-0 loss: 0.856790  [   32/  124]
train() client id: f_00002-1-1 loss: 1.025979  [   64/  124]
train() client id: f_00002-1-2 loss: 0.978193  [   96/  124]
train() client id: f_00002-2-0 loss: 0.834544  [   32/  124]
train() client id: f_00002-2-1 loss: 0.920256  [   64/  124]
train() client id: f_00002-2-2 loss: 0.952817  [   96/  124]
train() client id: f_00002-3-0 loss: 0.999979  [   32/  124]
train() client id: f_00002-3-1 loss: 0.951561  [   64/  124]
train() client id: f_00002-3-2 loss: 0.759655  [   96/  124]
train() client id: f_00002-4-0 loss: 1.002232  [   32/  124]
train() client id: f_00002-4-1 loss: 0.765251  [   64/  124]
train() client id: f_00002-4-2 loss: 1.009257  [   96/  124]
train() client id: f_00002-5-0 loss: 0.874799  [   32/  124]
train() client id: f_00002-5-1 loss: 0.770963  [   64/  124]
train() client id: f_00002-5-2 loss: 1.028936  [   96/  124]
train() client id: f_00002-6-0 loss: 0.728204  [   32/  124]
train() client id: f_00002-6-1 loss: 1.022836  [   64/  124]
train() client id: f_00002-6-2 loss: 0.877178  [   96/  124]
train() client id: f_00002-7-0 loss: 0.942890  [   32/  124]
train() client id: f_00002-7-1 loss: 0.825874  [   64/  124]
train() client id: f_00002-7-2 loss: 1.005840  [   96/  124]
train() client id: f_00002-8-0 loss: 0.768378  [   32/  124]
train() client id: f_00002-8-1 loss: 0.973316  [   64/  124]
train() client id: f_00002-8-2 loss: 0.922106  [   96/  124]
train() client id: f_00003-0-0 loss: 0.548978  [   32/   43]
train() client id: f_00003-1-0 loss: 0.882644  [   32/   43]
train() client id: f_00003-2-0 loss: 0.809461  [   32/   43]
train() client id: f_00003-3-0 loss: 0.658443  [   32/   43]
train() client id: f_00003-4-0 loss: 0.761899  [   32/   43]
train() client id: f_00003-5-0 loss: 0.782826  [   32/   43]
train() client id: f_00003-6-0 loss: 0.686123  [   32/   43]
train() client id: f_00003-7-0 loss: 0.536465  [   32/   43]
train() client id: f_00003-8-0 loss: 0.686718  [   32/   43]
train() client id: f_00004-0-0 loss: 0.902835  [   32/  306]
train() client id: f_00004-0-1 loss: 0.750599  [   64/  306]
train() client id: f_00004-0-2 loss: 0.825471  [   96/  306]
train() client id: f_00004-0-3 loss: 0.703877  [  128/  306]
train() client id: f_00004-0-4 loss: 0.745470  [  160/  306]
train() client id: f_00004-0-5 loss: 0.688544  [  192/  306]
train() client id: f_00004-0-6 loss: 0.733777  [  224/  306]
train() client id: f_00004-0-7 loss: 0.865257  [  256/  306]
train() client id: f_00004-0-8 loss: 0.959774  [  288/  306]
train() client id: f_00004-1-0 loss: 0.857639  [   32/  306]
train() client id: f_00004-1-1 loss: 0.693096  [   64/  306]
train() client id: f_00004-1-2 loss: 0.708787  [   96/  306]
train() client id: f_00004-1-3 loss: 0.860789  [  128/  306]
train() client id: f_00004-1-4 loss: 0.714820  [  160/  306]
train() client id: f_00004-1-5 loss: 0.895963  [  192/  306]
train() client id: f_00004-1-6 loss: 0.821550  [  224/  306]
train() client id: f_00004-1-7 loss: 0.769253  [  256/  306]
train() client id: f_00004-1-8 loss: 0.833386  [  288/  306]
train() client id: f_00004-2-0 loss: 0.758904  [   32/  306]
train() client id: f_00004-2-1 loss: 0.792108  [   64/  306]
train() client id: f_00004-2-2 loss: 0.901286  [   96/  306]
train() client id: f_00004-2-3 loss: 0.785400  [  128/  306]
train() client id: f_00004-2-4 loss: 0.738444  [  160/  306]
train() client id: f_00004-2-5 loss: 0.792720  [  192/  306]
train() client id: f_00004-2-6 loss: 0.711457  [  224/  306]
train() client id: f_00004-2-7 loss: 0.775719  [  256/  306]
train() client id: f_00004-2-8 loss: 0.934606  [  288/  306]
train() client id: f_00004-3-0 loss: 0.766906  [   32/  306]
train() client id: f_00004-3-1 loss: 0.855544  [   64/  306]
train() client id: f_00004-3-2 loss: 0.828513  [   96/  306]
train() client id: f_00004-3-3 loss: 0.749841  [  128/  306]
train() client id: f_00004-3-4 loss: 0.797364  [  160/  306]
train() client id: f_00004-3-5 loss: 0.788827  [  192/  306]
train() client id: f_00004-3-6 loss: 0.864391  [  224/  306]
train() client id: f_00004-3-7 loss: 0.683966  [  256/  306]
train() client id: f_00004-3-8 loss: 0.825671  [  288/  306]
train() client id: f_00004-4-0 loss: 0.951653  [   32/  306]
train() client id: f_00004-4-1 loss: 0.836424  [   64/  306]
train() client id: f_00004-4-2 loss: 0.765206  [   96/  306]
train() client id: f_00004-4-3 loss: 0.732800  [  128/  306]
train() client id: f_00004-4-4 loss: 0.720393  [  160/  306]
train() client id: f_00004-4-5 loss: 0.793463  [  192/  306]
train() client id: f_00004-4-6 loss: 0.740856  [  224/  306]
train() client id: f_00004-4-7 loss: 0.733273  [  256/  306]
train() client id: f_00004-4-8 loss: 0.910907  [  288/  306]
train() client id: f_00004-5-0 loss: 0.740790  [   32/  306]
train() client id: f_00004-5-1 loss: 0.908471  [   64/  306]
train() client id: f_00004-5-2 loss: 0.842518  [   96/  306]
train() client id: f_00004-5-3 loss: 0.717878  [  128/  306]
train() client id: f_00004-5-4 loss: 0.770095  [  160/  306]
train() client id: f_00004-5-5 loss: 0.745962  [  192/  306]
train() client id: f_00004-5-6 loss: 0.930049  [  224/  306]
train() client id: f_00004-5-7 loss: 0.747253  [  256/  306]
train() client id: f_00004-5-8 loss: 0.779221  [  288/  306]
train() client id: f_00004-6-0 loss: 0.703162  [   32/  306]
train() client id: f_00004-6-1 loss: 0.791411  [   64/  306]
train() client id: f_00004-6-2 loss: 0.776784  [   96/  306]
train() client id: f_00004-6-3 loss: 0.895828  [  128/  306]
train() client id: f_00004-6-4 loss: 0.651592  [  160/  306]
train() client id: f_00004-6-5 loss: 0.941028  [  192/  306]
train() client id: f_00004-6-6 loss: 0.887519  [  224/  306]
train() client id: f_00004-6-7 loss: 0.820291  [  256/  306]
train() client id: f_00004-6-8 loss: 0.788430  [  288/  306]
train() client id: f_00004-7-0 loss: 0.737898  [   32/  306]
train() client id: f_00004-7-1 loss: 0.749032  [   64/  306]
train() client id: f_00004-7-2 loss: 0.957937  [   96/  306]
train() client id: f_00004-7-3 loss: 0.925640  [  128/  306]
train() client id: f_00004-7-4 loss: 0.735190  [  160/  306]
train() client id: f_00004-7-5 loss: 0.795308  [  192/  306]
train() client id: f_00004-7-6 loss: 0.775533  [  224/  306]
train() client id: f_00004-7-7 loss: 0.925323  [  256/  306]
train() client id: f_00004-7-8 loss: 0.759954  [  288/  306]
train() client id: f_00004-8-0 loss: 0.655847  [   32/  306]
train() client id: f_00004-8-1 loss: 0.838181  [   64/  306]
train() client id: f_00004-8-2 loss: 0.885122  [   96/  306]
train() client id: f_00004-8-3 loss: 0.965981  [  128/  306]
train() client id: f_00004-8-4 loss: 0.753697  [  160/  306]
train() client id: f_00004-8-5 loss: 0.840989  [  192/  306]
train() client id: f_00004-8-6 loss: 0.719245  [  224/  306]
train() client id: f_00004-8-7 loss: 0.814555  [  256/  306]
train() client id: f_00004-8-8 loss: 0.726906  [  288/  306]
train() client id: f_00005-0-0 loss: 0.575557  [   32/  146]
train() client id: f_00005-0-1 loss: 0.605771  [   64/  146]
train() client id: f_00005-0-2 loss: 0.544111  [   96/  146]
train() client id: f_00005-0-3 loss: 0.752252  [  128/  146]
train() client id: f_00005-1-0 loss: 0.583838  [   32/  146]
train() client id: f_00005-1-1 loss: 0.704742  [   64/  146]
train() client id: f_00005-1-2 loss: 0.724915  [   96/  146]
train() client id: f_00005-1-3 loss: 0.678997  [  128/  146]
train() client id: f_00005-2-0 loss: 0.775764  [   32/  146]
train() client id: f_00005-2-1 loss: 0.560849  [   64/  146]
train() client id: f_00005-2-2 loss: 0.458907  [   96/  146]
train() client id: f_00005-2-3 loss: 0.624190  [  128/  146]
train() client id: f_00005-3-0 loss: 0.441804  [   32/  146]
train() client id: f_00005-3-1 loss: 0.808632  [   64/  146]
train() client id: f_00005-3-2 loss: 0.452769  [   96/  146]
train() client id: f_00005-3-3 loss: 0.854214  [  128/  146]
train() client id: f_00005-4-0 loss: 0.633642  [   32/  146]
train() client id: f_00005-4-1 loss: 0.761151  [   64/  146]
train() client id: f_00005-4-2 loss: 0.516779  [   96/  146]
train() client id: f_00005-4-3 loss: 0.598027  [  128/  146]
train() client id: f_00005-5-0 loss: 0.606291  [   32/  146]
train() client id: f_00005-5-1 loss: 0.522535  [   64/  146]
train() client id: f_00005-5-2 loss: 0.456282  [   96/  146]
train() client id: f_00005-5-3 loss: 0.955239  [  128/  146]
train() client id: f_00005-6-0 loss: 0.449499  [   32/  146]
train() client id: f_00005-6-1 loss: 0.776733  [   64/  146]
train() client id: f_00005-6-2 loss: 0.653268  [   96/  146]
train() client id: f_00005-6-3 loss: 0.617790  [  128/  146]
train() client id: f_00005-7-0 loss: 0.469434  [   32/  146]
train() client id: f_00005-7-1 loss: 0.811685  [   64/  146]
train() client id: f_00005-7-2 loss: 0.493405  [   96/  146]
train() client id: f_00005-7-3 loss: 0.758432  [  128/  146]
train() client id: f_00005-8-0 loss: 0.520835  [   32/  146]
train() client id: f_00005-8-1 loss: 0.755744  [   64/  146]
train() client id: f_00005-8-2 loss: 0.833232  [   96/  146]
train() client id: f_00005-8-3 loss: 0.692522  [  128/  146]
train() client id: f_00006-0-0 loss: 0.487507  [   32/   54]
train() client id: f_00006-1-0 loss: 0.593569  [   32/   54]
train() client id: f_00006-2-0 loss: 0.494192  [   32/   54]
train() client id: f_00006-3-0 loss: 0.561349  [   32/   54]
train() client id: f_00006-4-0 loss: 0.538307  [   32/   54]
train() client id: f_00006-5-0 loss: 0.532263  [   32/   54]
train() client id: f_00006-6-0 loss: 0.488046  [   32/   54]
train() client id: f_00006-7-0 loss: 0.598039  [   32/   54]
train() client id: f_00006-8-0 loss: 0.544802  [   32/   54]
train() client id: f_00007-0-0 loss: 0.505979  [   32/  179]
train() client id: f_00007-0-1 loss: 0.965232  [   64/  179]
train() client id: f_00007-0-2 loss: 0.502387  [   96/  179]
train() client id: f_00007-0-3 loss: 0.603018  [  128/  179]
train() client id: f_00007-0-4 loss: 0.668358  [  160/  179]
train() client id: f_00007-1-0 loss: 0.745837  [   32/  179]
train() client id: f_00007-1-1 loss: 0.495731  [   64/  179]
train() client id: f_00007-1-2 loss: 0.810074  [   96/  179]
train() client id: f_00007-1-3 loss: 0.492131  [  128/  179]
train() client id: f_00007-1-4 loss: 0.748768  [  160/  179]
train() client id: f_00007-2-0 loss: 0.846181  [   32/  179]
train() client id: f_00007-2-1 loss: 0.614261  [   64/  179]
train() client id: f_00007-2-2 loss: 0.473730  [   96/  179]
train() client id: f_00007-2-3 loss: 0.555114  [  128/  179]
train() client id: f_00007-2-4 loss: 0.704951  [  160/  179]
train() client id: f_00007-3-0 loss: 0.617489  [   32/  179]
train() client id: f_00007-3-1 loss: 0.625879  [   64/  179]
train() client id: f_00007-3-2 loss: 0.784900  [   96/  179]
train() client id: f_00007-3-3 loss: 0.666344  [  128/  179]
train() client id: f_00007-3-4 loss: 0.541647  [  160/  179]
train() client id: f_00007-4-0 loss: 0.471653  [   32/  179]
train() client id: f_00007-4-1 loss: 0.704077  [   64/  179]
train() client id: f_00007-4-2 loss: 0.467622  [   96/  179]
train() client id: f_00007-4-3 loss: 0.575876  [  128/  179]
train() client id: f_00007-4-4 loss: 1.041770  [  160/  179]
train() client id: f_00007-5-0 loss: 0.523512  [   32/  179]
train() client id: f_00007-5-1 loss: 0.636554  [   64/  179]
train() client id: f_00007-5-2 loss: 0.567289  [   96/  179]
train() client id: f_00007-5-3 loss: 0.832280  [  128/  179]
train() client id: f_00007-5-4 loss: 0.601385  [  160/  179]
train() client id: f_00007-6-0 loss: 0.762045  [   32/  179]
train() client id: f_00007-6-1 loss: 0.665920  [   64/  179]
train() client id: f_00007-6-2 loss: 0.494006  [   96/  179]
train() client id: f_00007-6-3 loss: 0.745965  [  128/  179]
train() client id: f_00007-6-4 loss: 0.540690  [  160/  179]
train() client id: f_00007-7-0 loss: 0.523140  [   32/  179]
train() client id: f_00007-7-1 loss: 0.676468  [   64/  179]
train() client id: f_00007-7-2 loss: 0.826037  [   96/  179]
train() client id: f_00007-7-3 loss: 0.668535  [  128/  179]
train() client id: f_00007-7-4 loss: 0.475665  [  160/  179]
train() client id: f_00007-8-0 loss: 0.539729  [   32/  179]
train() client id: f_00007-8-1 loss: 0.794246  [   64/  179]
train() client id: f_00007-8-2 loss: 0.837884  [   96/  179]
train() client id: f_00007-8-3 loss: 0.606192  [  128/  179]
train() client id: f_00007-8-4 loss: 0.480670  [  160/  179]
train() client id: f_00008-0-0 loss: 0.605168  [   32/  130]
train() client id: f_00008-0-1 loss: 0.800782  [   64/  130]
train() client id: f_00008-0-2 loss: 0.860041  [   96/  130]
train() client id: f_00008-0-3 loss: 0.759774  [  128/  130]
train() client id: f_00008-1-0 loss: 0.825560  [   32/  130]
train() client id: f_00008-1-1 loss: 0.818105  [   64/  130]
train() client id: f_00008-1-2 loss: 0.794290  [   96/  130]
train() client id: f_00008-1-3 loss: 0.596425  [  128/  130]
train() client id: f_00008-2-0 loss: 0.736604  [   32/  130]
train() client id: f_00008-2-1 loss: 0.755655  [   64/  130]
train() client id: f_00008-2-2 loss: 0.751473  [   96/  130]
train() client id: f_00008-2-3 loss: 0.787467  [  128/  130]
train() client id: f_00008-3-0 loss: 0.647950  [   32/  130]
train() client id: f_00008-3-1 loss: 0.773522  [   64/  130]
train() client id: f_00008-3-2 loss: 0.806081  [   96/  130]
train() client id: f_00008-3-3 loss: 0.798481  [  128/  130]
train() client id: f_00008-4-0 loss: 0.761030  [   32/  130]
train() client id: f_00008-4-1 loss: 0.785668  [   64/  130]
train() client id: f_00008-4-2 loss: 0.787372  [   96/  130]
train() client id: f_00008-4-3 loss: 0.694924  [  128/  130]
train() client id: f_00008-5-0 loss: 0.814291  [   32/  130]
train() client id: f_00008-5-1 loss: 0.679709  [   64/  130]
train() client id: f_00008-5-2 loss: 0.786032  [   96/  130]
train() client id: f_00008-5-3 loss: 0.740064  [  128/  130]
train() client id: f_00008-6-0 loss: 0.773924  [   32/  130]
train() client id: f_00008-6-1 loss: 0.650743  [   64/  130]
train() client id: f_00008-6-2 loss: 0.739503  [   96/  130]
train() client id: f_00008-6-3 loss: 0.841069  [  128/  130]
train() client id: f_00008-7-0 loss: 0.720486  [   32/  130]
train() client id: f_00008-7-1 loss: 0.808879  [   64/  130]
train() client id: f_00008-7-2 loss: 0.790276  [   96/  130]
train() client id: f_00008-7-3 loss: 0.688941  [  128/  130]
train() client id: f_00008-8-0 loss: 0.740961  [   32/  130]
train() client id: f_00008-8-1 loss: 0.787543  [   64/  130]
train() client id: f_00008-8-2 loss: 0.751580  [   96/  130]
train() client id: f_00008-8-3 loss: 0.732465  [  128/  130]
train() client id: f_00009-0-0 loss: 0.832074  [   32/  118]
train() client id: f_00009-0-1 loss: 1.033546  [   64/  118]
train() client id: f_00009-0-2 loss: 0.913356  [   96/  118]
train() client id: f_00009-1-0 loss: 0.867930  [   32/  118]
train() client id: f_00009-1-1 loss: 0.869844  [   64/  118]
train() client id: f_00009-1-2 loss: 0.862045  [   96/  118]
train() client id: f_00009-2-0 loss: 0.991924  [   32/  118]
train() client id: f_00009-2-1 loss: 0.926844  [   64/  118]
train() client id: f_00009-2-2 loss: 0.746983  [   96/  118]
train() client id: f_00009-3-0 loss: 0.925711  [   32/  118]
train() client id: f_00009-3-1 loss: 0.843469  [   64/  118]
train() client id: f_00009-3-2 loss: 0.695449  [   96/  118]
train() client id: f_00009-4-0 loss: 0.807522  [   32/  118]
train() client id: f_00009-4-1 loss: 0.846158  [   64/  118]
train() client id: f_00009-4-2 loss: 0.786533  [   96/  118]
train() client id: f_00009-5-0 loss: 0.741979  [   32/  118]
train() client id: f_00009-5-1 loss: 0.830363  [   64/  118]
train() client id: f_00009-5-2 loss: 0.870774  [   96/  118]
train() client id: f_00009-6-0 loss: 0.741439  [   32/  118]
train() client id: f_00009-6-1 loss: 0.870078  [   64/  118]
train() client id: f_00009-6-2 loss: 0.747947  [   96/  118]
train() client id: f_00009-7-0 loss: 0.968164  [   32/  118]
train() client id: f_00009-7-1 loss: 0.660899  [   64/  118]
train() client id: f_00009-7-2 loss: 0.737273  [   96/  118]
train() client id: f_00009-8-0 loss: 0.790184  [   32/  118]
train() client id: f_00009-8-1 loss: 0.744198  [   64/  118]
train() client id: f_00009-8-2 loss: 0.812032  [   96/  118]
At round 54 accuracy: 0.6472148541114059
At round 54 training accuracy: 0.5888665325285044
At round 54 training loss: 0.8178998408229051
update_location
xs = -4.528292 171.001589 180.045120 -180.943528 69.896481 -105.217951 -222.215960 243.375741 -1.680116 164.695607 
ys = 257.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -187.154970 4.001482 
xs mean: 31.44286915941286
ys mean: 9.371751218646875
dists_uav = 276.354957 198.704624 205.956280 207.963529 122.007952 145.185180 243.700016 263.817235 212.202275 192.719109 
uav_gains = -113.649116 -107.600494 -108.060152 -108.189252 -102.159974 -104.051007 -110.770378 -112.510119 -108.465525 -107.225757 
uav_gains_db_mean: -108.26817746251655
dists_bs = 189.423795 380.971837 395.248739 152.570987 301.375419 185.788862 184.276517 461.240958 401.492222 380.307243 
bs_gains = -103.335226 -111.832060 -112.279433 -100.704274 -108.982078 -103.099610 -103.000219 -114.157039 -112.470019 -111.810828 
bs_gains_db_mean: -108.16707860506547
Round 55
-------------------------------
ene_coms = [0.01411429 0.01219219 0.00943525 0.0095099  0.00999414 0.00723101
 0.01127657 0.01285117 0.01281041 0.01217255]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.71930894 7.65727901 3.61823061 1.31357673 8.80344596 4.2235761
 1.64332204 5.21583333 3.82084917 3.47329508]
obj_prev = 43.48871696719543
eta_min = 3.824051359485939e-25	eta_max = 0.951255722172846
af = 9.122641062598856	bf = 1.0425151585725023	zeta = 10.034905168858742	eta = 0.909090909090909
af = 9.122641062598856	bf = 1.0425151585725023	zeta = 21.031952903930257	eta = 0.43375149727033196
af = 9.122641062598856	bf = 1.0425151585725023	zeta = 15.298065486395961	eta = 0.5963264486422355
af = 9.122641062598856	bf = 1.0425151585725023	zeta = 14.265536565910065	eta = 0.6394881132195871
af = 9.122641062598856	bf = 1.0425151585725023	zeta = 14.205167188711439	eta = 0.6422058214033859
af = 9.122641062598856	bf = 1.0425151585725023	zeta = 14.204938238052804	eta = 0.6422161722717477
eta = 0.6422161722717477
ene_coms = [0.01411429 0.01219219 0.00943525 0.0095099  0.00999414 0.00723101
 0.01127657 0.01285117 0.01281041 0.01217255]
ene_comp = [0.0365413  0.07685273 0.03596128 0.01247044 0.08874315 0.0423415
 0.01566056 0.05191184 0.03770134 0.03422121]
ene_total = [1.32273708 2.32517312 1.18541052 0.57395877 2.57826395 1.29445555
 0.70339201 1.69111532 1.31898117 1.21145075]
ti_comp = [0.90241524 0.92163627 0.94920566 0.94845911 0.94361673 0.971248
 0.93079244 0.91504642 0.91545401 0.92183267]
ti_coms = [0.1411429  0.12192187 0.09435248 0.09509903 0.09994141 0.07231014
 0.1127657  0.12851171 0.12810413 0.12172547]
t_total = [27.24976921 27.24976921 27.24976921 27.24976921 27.24976921 27.24976921
 27.24976921 27.24976921 27.24976921 27.24976921]
ene_coms = [0.01411429 0.01219219 0.00943525 0.0095099  0.00999414 0.00723101
 0.01127657 0.01285117 0.01281041 0.01217255]
ene_comp = [3.74471720e-06 3.33994055e-05 3.22600210e-06 1.34737465e-07
 4.90561254e-05 5.02942716e-06 2.77074025e-07 1.04422254e-05
 3.99647611e-06 2.94755598e-06]
ene_total = [0.36865526 0.31923896 0.24646086 0.24832957 0.26225161 0.1889502
 0.29446513 0.33584713 0.33461451 0.31793095]
optimize_network iter = 0 obj = 2.9167441830653944
eta = 0.6422161722717477
freqs = [20246387.01183396 41693631.65542174 18942826.1724381   6574053.77858508
 47022877.58791491 21797471.94998933  8412485.34860386 28365687.25451386
 20591606.41657853 18561510.75902493]
eta_min = 0.6422161722717491	eta_max = 0.7583425790389472
af = 0.0023682654803508026	bf = 1.0425151585725023	zeta = 0.002605092028385883	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01411429 0.01219219 0.00943525 0.0095099  0.00999414 0.00723101
 0.01127657 0.01285117 0.01281041 0.01217255]
ene_comp = [8.05973886e-07 7.18853980e-06 6.94331055e-07 2.89994872e-08
 1.05583289e-05 1.08248146e-06 5.96345242e-08 2.24747572e-06
 8.60159849e-07 6.34401217e-07]
ene_total = [1.45058579 1.25371025 0.96971664 0.97732047 1.02816689 0.74323097
 1.15888112 1.32092522 1.31659396 1.25101834]
ti_comp = [0.56370608 0.58292712 0.61049651 0.60974995 0.60490758 0.63253884
 0.59208329 0.57633727 0.57674486 0.58312351]
ti_coms = [0.1411429  0.12192187 0.09435248 0.09509903 0.09994141 0.07231014
 0.1127657  0.12851171 0.12810413 0.12172547]
t_total = [27.24976921 27.24976921 27.24976921 27.24976921 27.24976921 27.24976921
 27.24976921 27.24976921 27.24976921 27.24976921]
ene_coms = [0.01411429 0.01219219 0.00943525 0.0095099  0.00999414 0.00723101
 0.01127657 0.01285117 0.01281041 0.01217255]
ene_comp = [2.33918908e-06 2.03501794e-05 1.90089367e-06 7.94622666e-08
 2.90968092e-05 2.89030251e-06 1.66907100e-07 6.41600678e-06
 2.45426656e-06 1.79549438e-06]
ene_total = [0.54575506 0.47214202 0.36484429 0.36766008 0.38750275 0.27966591
 0.43596357 0.49707994 0.49535103 0.4706654 ]
optimize_network iter = 1 obj = 4.316630048706649
eta = 0.7583425790389472
freqs = [20246387.01183396 41177628.10237318 18397898.21832189  6387727.62236118
 45820741.44056871 20907175.51435725  8261153.04889369 28132351.45728883
 20416877.89436585 18329523.01214841]
eta_min = 0.7583425790389494	eta_max = 0.758342579038884
af = 0.0022793946214321468	bf = 1.0425151585725023	zeta = 0.0025073340835753614	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01411429 0.01219219 0.00943525 0.0095099  0.00999414 0.00723101
 0.01127657 0.01285117 0.01281041 0.01217255]
ene_comp = [8.05973886e-07 7.01170902e-06 6.54958022e-07 2.73789375e-08
 1.00253838e-05 9.95861501e-07 5.75082899e-08 2.21065238e-06
 8.45624140e-07 6.18642413e-07]
ene_total = [1.45058579 1.25369208 0.9697126  0.9773203  1.02811212 0.74322207
 1.15888091 1.32092144 1.31659247 1.25101672]
ti_comp = [0.56370608 0.58292712 0.61049651 0.60974995 0.60490758 0.63253884
 0.59208329 0.57633727 0.57674486 0.58312351]
ti_coms = [0.1411429  0.12192187 0.09435248 0.09509903 0.09994141 0.07231014
 0.1127657  0.12851171 0.12810413 0.12172547]
t_total = [27.24976921 27.24976921 27.24976921 27.24976921 27.24976921 27.24976921
 27.24976921 27.24976921 27.24976921 27.24976921]
ene_coms = [0.01411429 0.01219219 0.00943525 0.0095099  0.00999414 0.00723101
 0.01127657 0.01285117 0.01281041 0.01217255]
ene_comp = [2.33918908e-06 2.03501794e-05 1.90089367e-06 7.94622666e-08
 2.90968092e-05 2.89030251e-06 1.66907100e-07 6.41600678e-06
 2.45426656e-06 1.79549438e-06]
ene_total = [0.54575506 0.47214202 0.36484429 0.36766008 0.38750275 0.27966591
 0.43596357 0.49707994 0.49535103 0.4706654 ]
optimize_network iter = 2 obj = 4.316630048705521
eta = 0.758342579038884
freqs = [20246387.01183344 41177628.10237257 18397898.21832189  6387727.62236117
 45820741.44056856 20907175.51435746  8261153.04889361 28132351.45728831
 20416877.89436548 18329523.01214814]
Done!
ene_coms = [0.01411429 0.01219219 0.00943525 0.0095099  0.00999414 0.00723101
 0.01127657 0.01285117 0.01281041 0.01217255]
ene_comp = [2.32422478e-06 2.02199949e-05 1.88873324e-06 7.89539292e-08
 2.89106706e-05 2.87181262e-06 1.65839359e-07 6.37496219e-06
 2.43856608e-06 1.78400821e-06]
ene_total = [0.01411661 0.01221241 0.00943714 0.00950998 0.01002305 0.00723389
 0.01127674 0.01285755 0.01281285 0.01217433]
At round 55 energy consumption: 0.11165454393437177
At round 55 eta: 0.758342579038884
At round 55 a_n: 9.342581272941924
At round 55 local rounds: 9.057945645482942
At round 55 global rounds: 38.66043606600104
gradient difference: 0.5478049516677856
train() client id: f_00000-0-0 loss: 0.856566  [   32/  126]
train() client id: f_00000-0-1 loss: 1.163877  [   64/  126]
train() client id: f_00000-0-2 loss: 0.982481  [   96/  126]
train() client id: f_00000-1-0 loss: 0.927595  [   32/  126]
train() client id: f_00000-1-1 loss: 1.019168  [   64/  126]
train() client id: f_00000-1-2 loss: 0.853614  [   96/  126]
train() client id: f_00000-2-0 loss: 1.011651  [   32/  126]
train() client id: f_00000-2-1 loss: 0.917402  [   64/  126]
train() client id: f_00000-2-2 loss: 0.825020  [   96/  126]
train() client id: f_00000-3-0 loss: 0.828377  [   32/  126]
train() client id: f_00000-3-1 loss: 0.815361  [   64/  126]
train() client id: f_00000-3-2 loss: 1.021655  [   96/  126]
train() client id: f_00000-4-0 loss: 0.989433  [   32/  126]
train() client id: f_00000-4-1 loss: 0.758221  [   64/  126]
train() client id: f_00000-4-2 loss: 0.918082  [   96/  126]
train() client id: f_00000-5-0 loss: 0.930595  [   32/  126]
train() client id: f_00000-5-1 loss: 0.788507  [   64/  126]
train() client id: f_00000-5-2 loss: 0.910526  [   96/  126]
train() client id: f_00000-6-0 loss: 0.887844  [   32/  126]
train() client id: f_00000-6-1 loss: 0.838070  [   64/  126]
train() client id: f_00000-6-2 loss: 0.790676  [   96/  126]
train() client id: f_00000-7-0 loss: 0.811662  [   32/  126]
train() client id: f_00000-7-1 loss: 0.888220  [   64/  126]
train() client id: f_00000-7-2 loss: 0.733944  [   96/  126]
train() client id: f_00000-8-0 loss: 0.969338  [   32/  126]
train() client id: f_00000-8-1 loss: 0.806967  [   64/  126]
train() client id: f_00000-8-2 loss: 0.730711  [   96/  126]
train() client id: f_00001-0-0 loss: 0.419672  [   32/  265]
train() client id: f_00001-0-1 loss: 0.550795  [   64/  265]
train() client id: f_00001-0-2 loss: 0.540199  [   96/  265]
train() client id: f_00001-0-3 loss: 0.421564  [  128/  265]
train() client id: f_00001-0-4 loss: 0.599217  [  160/  265]
train() client id: f_00001-0-5 loss: 0.401074  [  192/  265]
train() client id: f_00001-0-6 loss: 0.546982  [  224/  265]
train() client id: f_00001-0-7 loss: 0.478225  [  256/  265]
train() client id: f_00001-1-0 loss: 0.390550  [   32/  265]
train() client id: f_00001-1-1 loss: 0.477531  [   64/  265]
train() client id: f_00001-1-2 loss: 0.483373  [   96/  265]
train() client id: f_00001-1-3 loss: 0.444725  [  128/  265]
train() client id: f_00001-1-4 loss: 0.508801  [  160/  265]
train() client id: f_00001-1-5 loss: 0.476282  [  192/  265]
train() client id: f_00001-1-6 loss: 0.546769  [  224/  265]
train() client id: f_00001-1-7 loss: 0.602678  [  256/  265]
train() client id: f_00001-2-0 loss: 0.509124  [   32/  265]
train() client id: f_00001-2-1 loss: 0.406337  [   64/  265]
train() client id: f_00001-2-2 loss: 0.403509  [   96/  265]
train() client id: f_00001-2-3 loss: 0.494883  [  128/  265]
train() client id: f_00001-2-4 loss: 0.686072  [  160/  265]
train() client id: f_00001-2-5 loss: 0.512416  [  192/  265]
train() client id: f_00001-2-6 loss: 0.451652  [  224/  265]
train() client id: f_00001-2-7 loss: 0.455159  [  256/  265]
train() client id: f_00001-3-0 loss: 0.457129  [   32/  265]
train() client id: f_00001-3-1 loss: 0.456010  [   64/  265]
train() client id: f_00001-3-2 loss: 0.489656  [   96/  265]
train() client id: f_00001-3-3 loss: 0.582406  [  128/  265]
train() client id: f_00001-3-4 loss: 0.482750  [  160/  265]
train() client id: f_00001-3-5 loss: 0.460330  [  192/  265]
train() client id: f_00001-3-6 loss: 0.518560  [  224/  265]
train() client id: f_00001-3-7 loss: 0.461475  [  256/  265]
train() client id: f_00001-4-0 loss: 0.531391  [   32/  265]
train() client id: f_00001-4-1 loss: 0.405301  [   64/  265]
train() client id: f_00001-4-2 loss: 0.557312  [   96/  265]
train() client id: f_00001-4-3 loss: 0.600287  [  128/  265]
train() client id: f_00001-4-4 loss: 0.516432  [  160/  265]
train() client id: f_00001-4-5 loss: 0.460755  [  192/  265]
train() client id: f_00001-4-6 loss: 0.441396  [  224/  265]
train() client id: f_00001-4-7 loss: 0.391964  [  256/  265]
train() client id: f_00001-5-0 loss: 0.578412  [   32/  265]
train() client id: f_00001-5-1 loss: 0.504242  [   64/  265]
train() client id: f_00001-5-2 loss: 0.390016  [   96/  265]
train() client id: f_00001-5-3 loss: 0.407452  [  128/  265]
train() client id: f_00001-5-4 loss: 0.573613  [  160/  265]
train() client id: f_00001-5-5 loss: 0.431834  [  192/  265]
train() client id: f_00001-5-6 loss: 0.517104  [  224/  265]
train() client id: f_00001-5-7 loss: 0.516297  [  256/  265]
train() client id: f_00001-6-0 loss: 0.479399  [   32/  265]
train() client id: f_00001-6-1 loss: 0.449982  [   64/  265]
train() client id: f_00001-6-2 loss: 0.552740  [   96/  265]
train() client id: f_00001-6-3 loss: 0.442764  [  128/  265]
train() client id: f_00001-6-4 loss: 0.414709  [  160/  265]
train() client id: f_00001-6-5 loss: 0.512503  [  192/  265]
train() client id: f_00001-6-6 loss: 0.554924  [  224/  265]
train() client id: f_00001-6-7 loss: 0.524793  [  256/  265]
train() client id: f_00001-7-0 loss: 0.396172  [   32/  265]
train() client id: f_00001-7-1 loss: 0.510258  [   64/  265]
train() client id: f_00001-7-2 loss: 0.439964  [   96/  265]
train() client id: f_00001-7-3 loss: 0.680264  [  128/  265]
train() client id: f_00001-7-4 loss: 0.579794  [  160/  265]
train() client id: f_00001-7-5 loss: 0.439990  [  192/  265]
train() client id: f_00001-7-6 loss: 0.422231  [  224/  265]
train() client id: f_00001-7-7 loss: 0.459185  [  256/  265]
train() client id: f_00001-8-0 loss: 0.486196  [   32/  265]
train() client id: f_00001-8-1 loss: 0.659954  [   64/  265]
train() client id: f_00001-8-2 loss: 0.380392  [   96/  265]
train() client id: f_00001-8-3 loss: 0.451375  [  128/  265]
train() client id: f_00001-8-4 loss: 0.416029  [  160/  265]
train() client id: f_00001-8-5 loss: 0.480627  [  192/  265]
train() client id: f_00001-8-6 loss: 0.535421  [  224/  265]
train() client id: f_00001-8-7 loss: 0.517427  [  256/  265]
train() client id: f_00002-0-0 loss: 1.065209  [   32/  124]
train() client id: f_00002-0-1 loss: 1.338959  [   64/  124]
train() client id: f_00002-0-2 loss: 0.846869  [   96/  124]
train() client id: f_00002-1-0 loss: 1.162892  [   32/  124]
train() client id: f_00002-1-1 loss: 1.013536  [   64/  124]
train() client id: f_00002-1-2 loss: 0.877170  [   96/  124]
train() client id: f_00002-2-0 loss: 1.195565  [   32/  124]
train() client id: f_00002-2-1 loss: 0.916206  [   64/  124]
train() client id: f_00002-2-2 loss: 1.009080  [   96/  124]
train() client id: f_00002-3-0 loss: 1.068959  [   32/  124]
train() client id: f_00002-3-1 loss: 0.957993  [   64/  124]
train() client id: f_00002-3-2 loss: 0.903139  [   96/  124]
train() client id: f_00002-4-0 loss: 0.973967  [   32/  124]
train() client id: f_00002-4-1 loss: 0.930905  [   64/  124]
train() client id: f_00002-4-2 loss: 0.998148  [   96/  124]
train() client id: f_00002-5-0 loss: 0.971329  [   32/  124]
train() client id: f_00002-5-1 loss: 1.043458  [   64/  124]
train() client id: f_00002-5-2 loss: 1.044917  [   96/  124]
train() client id: f_00002-6-0 loss: 0.999164  [   32/  124]
train() client id: f_00002-6-1 loss: 0.906866  [   64/  124]
train() client id: f_00002-6-2 loss: 0.902262  [   96/  124]
train() client id: f_00002-7-0 loss: 0.778465  [   32/  124]
train() client id: f_00002-7-1 loss: 1.009596  [   64/  124]
train() client id: f_00002-7-2 loss: 0.798815  [   96/  124]
train() client id: f_00002-8-0 loss: 0.938895  [   32/  124]
train() client id: f_00002-8-1 loss: 0.920152  [   64/  124]
train() client id: f_00002-8-2 loss: 0.835942  [   96/  124]
train() client id: f_00003-0-0 loss: 0.516128  [   32/   43]
train() client id: f_00003-1-0 loss: 0.792232  [   32/   43]
train() client id: f_00003-2-0 loss: 0.649369  [   32/   43]
train() client id: f_00003-3-0 loss: 0.676164  [   32/   43]
train() client id: f_00003-4-0 loss: 0.599955  [   32/   43]
train() client id: f_00003-5-0 loss: 0.857931  [   32/   43]
train() client id: f_00003-6-0 loss: 0.772790  [   32/   43]
train() client id: f_00003-7-0 loss: 0.734740  [   32/   43]
train() client id: f_00003-8-0 loss: 0.811271  [   32/   43]
train() client id: f_00004-0-0 loss: 0.914294  [   32/  306]
train() client id: f_00004-0-1 loss: 0.986564  [   64/  306]
train() client id: f_00004-0-2 loss: 0.970272  [   96/  306]
train() client id: f_00004-0-3 loss: 0.889187  [  128/  306]
train() client id: f_00004-0-4 loss: 0.950601  [  160/  306]
train() client id: f_00004-0-5 loss: 0.950556  [  192/  306]
train() client id: f_00004-0-6 loss: 0.841709  [  224/  306]
train() client id: f_00004-0-7 loss: 0.978262  [  256/  306]
train() client id: f_00004-0-8 loss: 0.987216  [  288/  306]
train() client id: f_00004-1-0 loss: 0.954351  [   32/  306]
train() client id: f_00004-1-1 loss: 0.922054  [   64/  306]
train() client id: f_00004-1-2 loss: 0.971928  [   96/  306]
train() client id: f_00004-1-3 loss: 0.978241  [  128/  306]
train() client id: f_00004-1-4 loss: 0.957193  [  160/  306]
train() client id: f_00004-1-5 loss: 0.936645  [  192/  306]
train() client id: f_00004-1-6 loss: 0.873618  [  224/  306]
train() client id: f_00004-1-7 loss: 0.995238  [  256/  306]
train() client id: f_00004-1-8 loss: 0.940969  [  288/  306]
train() client id: f_00004-2-0 loss: 1.000805  [   32/  306]
train() client id: f_00004-2-1 loss: 1.027426  [   64/  306]
train() client id: f_00004-2-2 loss: 0.983688  [   96/  306]
train() client id: f_00004-2-3 loss: 0.863064  [  128/  306]
train() client id: f_00004-2-4 loss: 0.913294  [  160/  306]
train() client id: f_00004-2-5 loss: 0.888384  [  192/  306]
train() client id: f_00004-2-6 loss: 0.902402  [  224/  306]
train() client id: f_00004-2-7 loss: 0.883013  [  256/  306]
train() client id: f_00004-2-8 loss: 1.033061  [  288/  306]
train() client id: f_00004-3-0 loss: 1.102489  [   32/  306]
train() client id: f_00004-3-1 loss: 0.932013  [   64/  306]
train() client id: f_00004-3-2 loss: 0.752252  [   96/  306]
train() client id: f_00004-3-3 loss: 0.916439  [  128/  306]
train() client id: f_00004-3-4 loss: 0.859781  [  160/  306]
train() client id: f_00004-3-5 loss: 0.974480  [  192/  306]
train() client id: f_00004-3-6 loss: 0.868630  [  224/  306]
train() client id: f_00004-3-7 loss: 0.917513  [  256/  306]
train() client id: f_00004-3-8 loss: 0.991099  [  288/  306]
train() client id: f_00004-4-0 loss: 0.789038  [   32/  306]
train() client id: f_00004-4-1 loss: 0.920985  [   64/  306]
train() client id: f_00004-4-2 loss: 0.997944  [   96/  306]
train() client id: f_00004-4-3 loss: 0.947925  [  128/  306]
train() client id: f_00004-4-4 loss: 0.943330  [  160/  306]
train() client id: f_00004-4-5 loss: 0.993967  [  192/  306]
train() client id: f_00004-4-6 loss: 0.960955  [  224/  306]
train() client id: f_00004-4-7 loss: 1.007135  [  256/  306]
train() client id: f_00004-4-8 loss: 0.754681  [  288/  306]
train() client id: f_00004-5-0 loss: 0.905875  [   32/  306]
train() client id: f_00004-5-1 loss: 1.030011  [   64/  306]
train() client id: f_00004-5-2 loss: 1.019916  [   96/  306]
train() client id: f_00004-5-3 loss: 0.953697  [  128/  306]
train() client id: f_00004-5-4 loss: 0.889686  [  160/  306]
train() client id: f_00004-5-5 loss: 0.845145  [  192/  306]
train() client id: f_00004-5-6 loss: 0.932452  [  224/  306]
train() client id: f_00004-5-7 loss: 1.037814  [  256/  306]
train() client id: f_00004-5-8 loss: 0.823591  [  288/  306]
train() client id: f_00004-6-0 loss: 1.019079  [   32/  306]
train() client id: f_00004-6-1 loss: 0.905262  [   64/  306]
train() client id: f_00004-6-2 loss: 0.953990  [   96/  306]
train() client id: f_00004-6-3 loss: 1.002807  [  128/  306]
train() client id: f_00004-6-4 loss: 0.901906  [  160/  306]
train() client id: f_00004-6-5 loss: 0.948496  [  192/  306]
train() client id: f_00004-6-6 loss: 0.919601  [  224/  306]
train() client id: f_00004-6-7 loss: 0.801545  [  256/  306]
train() client id: f_00004-6-8 loss: 0.866773  [  288/  306]
train() client id: f_00004-7-0 loss: 0.940682  [   32/  306]
train() client id: f_00004-7-1 loss: 0.835880  [   64/  306]
train() client id: f_00004-7-2 loss: 1.114353  [   96/  306]
train() client id: f_00004-7-3 loss: 0.921921  [  128/  306]
train() client id: f_00004-7-4 loss: 0.952494  [  160/  306]
train() client id: f_00004-7-5 loss: 0.830807  [  192/  306]
train() client id: f_00004-7-6 loss: 0.888342  [  224/  306]
train() client id: f_00004-7-7 loss: 0.975315  [  256/  306]
train() client id: f_00004-7-8 loss: 0.908783  [  288/  306]
train() client id: f_00004-8-0 loss: 0.999042  [   32/  306]
train() client id: f_00004-8-1 loss: 0.924124  [   64/  306]
train() client id: f_00004-8-2 loss: 1.017544  [   96/  306]
train() client id: f_00004-8-3 loss: 0.872768  [  128/  306]
train() client id: f_00004-8-4 loss: 1.036979  [  160/  306]
train() client id: f_00004-8-5 loss: 0.922686  [  192/  306]
train() client id: f_00004-8-6 loss: 0.828855  [  224/  306]
train() client id: f_00004-8-7 loss: 0.809842  [  256/  306]
train() client id: f_00004-8-8 loss: 0.796689  [  288/  306]
train() client id: f_00005-0-0 loss: 0.496878  [   32/  146]
train() client id: f_00005-0-1 loss: 0.494017  [   64/  146]
train() client id: f_00005-0-2 loss: 0.681180  [   96/  146]
train() client id: f_00005-0-3 loss: 0.651149  [  128/  146]
train() client id: f_00005-1-0 loss: 0.539014  [   32/  146]
train() client id: f_00005-1-1 loss: 0.805700  [   64/  146]
train() client id: f_00005-1-2 loss: 0.583213  [   96/  146]
train() client id: f_00005-1-3 loss: 0.346838  [  128/  146]
train() client id: f_00005-2-0 loss: 0.450239  [   32/  146]
train() client id: f_00005-2-1 loss: 0.502617  [   64/  146]
train() client id: f_00005-2-2 loss: 0.575914  [   96/  146]
train() client id: f_00005-2-3 loss: 0.586046  [  128/  146]
train() client id: f_00005-3-0 loss: 0.592089  [   32/  146]
train() client id: f_00005-3-1 loss: 0.463788  [   64/  146]
train() client id: f_00005-3-2 loss: 0.569426  [   96/  146]
train() client id: f_00005-3-3 loss: 0.757451  [  128/  146]
train() client id: f_00005-4-0 loss: 0.703421  [   32/  146]
train() client id: f_00005-4-1 loss: 0.585799  [   64/  146]
train() client id: f_00005-4-2 loss: 0.417248  [   96/  146]
train() client id: f_00005-4-3 loss: 0.496307  [  128/  146]
train() client id: f_00005-5-0 loss: 0.323042  [   32/  146]
train() client id: f_00005-5-1 loss: 0.767530  [   64/  146]
train() client id: f_00005-5-2 loss: 0.634233  [   96/  146]
train() client id: f_00005-5-3 loss: 0.622784  [  128/  146]
train() client id: f_00005-6-0 loss: 0.573615  [   32/  146]
train() client id: f_00005-6-1 loss: 0.628938  [   64/  146]
train() client id: f_00005-6-2 loss: 0.536123  [   96/  146]
train() client id: f_00005-6-3 loss: 0.510768  [  128/  146]
train() client id: f_00005-7-0 loss: 0.844155  [   32/  146]
train() client id: f_00005-7-1 loss: 0.527815  [   64/  146]
train() client id: f_00005-7-2 loss: 0.540128  [   96/  146]
train() client id: f_00005-7-3 loss: 0.526548  [  128/  146]
train() client id: f_00005-8-0 loss: 0.510644  [   32/  146]
train() client id: f_00005-8-1 loss: 0.778974  [   64/  146]
train() client id: f_00005-8-2 loss: 0.547259  [   96/  146]
train() client id: f_00005-8-3 loss: 0.622801  [  128/  146]
train() client id: f_00006-0-0 loss: 0.519357  [   32/   54]
train() client id: f_00006-1-0 loss: 0.558078  [   32/   54]
train() client id: f_00006-2-0 loss: 0.517653  [   32/   54]
train() client id: f_00006-3-0 loss: 0.480294  [   32/   54]
train() client id: f_00006-4-0 loss: 0.587611  [   32/   54]
train() client id: f_00006-5-0 loss: 0.488764  [   32/   54]
train() client id: f_00006-6-0 loss: 0.515595  [   32/   54]
train() client id: f_00006-7-0 loss: 0.518112  [   32/   54]
train() client id: f_00006-8-0 loss: 0.515553  [   32/   54]
train() client id: f_00007-0-0 loss: 0.780394  [   32/  179]
train() client id: f_00007-0-1 loss: 0.697069  [   64/  179]
train() client id: f_00007-0-2 loss: 0.794091  [   96/  179]
train() client id: f_00007-0-3 loss: 0.915779  [  128/  179]
train() client id: f_00007-0-4 loss: 0.686681  [  160/  179]
train() client id: f_00007-1-0 loss: 0.792252  [   32/  179]
train() client id: f_00007-1-1 loss: 0.749139  [   64/  179]
train() client id: f_00007-1-2 loss: 0.683493  [   96/  179]
train() client id: f_00007-1-3 loss: 0.786807  [  128/  179]
train() client id: f_00007-1-4 loss: 0.627468  [  160/  179]
train() client id: f_00007-2-0 loss: 0.801356  [   32/  179]
train() client id: f_00007-2-1 loss: 0.709079  [   64/  179]
train() client id: f_00007-2-2 loss: 0.788082  [   96/  179]
train() client id: f_00007-2-3 loss: 0.649239  [  128/  179]
train() client id: f_00007-2-4 loss: 0.794226  [  160/  179]
train() client id: f_00007-3-0 loss: 0.882261  [   32/  179]
train() client id: f_00007-3-1 loss: 0.810196  [   64/  179]
train() client id: f_00007-3-2 loss: 0.779983  [   96/  179]
train() client id: f_00007-3-3 loss: 0.646358  [  128/  179]
train() client id: f_00007-3-4 loss: 0.644658  [  160/  179]
train() client id: f_00007-4-0 loss: 0.786704  [   32/  179]
train() client id: f_00007-4-1 loss: 0.679554  [   64/  179]
train() client id: f_00007-4-2 loss: 0.695818  [   96/  179]
train() client id: f_00007-4-3 loss: 0.666132  [  128/  179]
train() client id: f_00007-4-4 loss: 0.678992  [  160/  179]
train() client id: f_00007-5-0 loss: 0.643426  [   32/  179]
train() client id: f_00007-5-1 loss: 0.792520  [   64/  179]
train() client id: f_00007-5-2 loss: 0.594058  [   96/  179]
train() client id: f_00007-5-3 loss: 0.652051  [  128/  179]
train() client id: f_00007-5-4 loss: 0.703463  [  160/  179]
train() client id: f_00007-6-0 loss: 0.835472  [   32/  179]
train() client id: f_00007-6-1 loss: 0.907363  [   64/  179]
train() client id: f_00007-6-2 loss: 0.567125  [   96/  179]
train() client id: f_00007-6-3 loss: 0.612520  [  128/  179]
train() client id: f_00007-6-4 loss: 0.772437  [  160/  179]
train() client id: f_00007-7-0 loss: 0.736559  [   32/  179]
train() client id: f_00007-7-1 loss: 0.838057  [   64/  179]
train() client id: f_00007-7-2 loss: 0.746642  [   96/  179]
train() client id: f_00007-7-3 loss: 0.743619  [  128/  179]
train() client id: f_00007-7-4 loss: 0.644648  [  160/  179]
train() client id: f_00007-8-0 loss: 0.683605  [   32/  179]
train() client id: f_00007-8-1 loss: 0.563021  [   64/  179]
train() client id: f_00007-8-2 loss: 0.656965  [   96/  179]
train() client id: f_00007-8-3 loss: 0.969611  [  128/  179]
train() client id: f_00007-8-4 loss: 0.653644  [  160/  179]
train() client id: f_00008-0-0 loss: 0.913615  [   32/  130]
train() client id: f_00008-0-1 loss: 0.712115  [   64/  130]
train() client id: f_00008-0-2 loss: 0.734805  [   96/  130]
train() client id: f_00008-0-3 loss: 0.694066  [  128/  130]
train() client id: f_00008-1-0 loss: 0.807325  [   32/  130]
train() client id: f_00008-1-1 loss: 0.730813  [   64/  130]
train() client id: f_00008-1-2 loss: 0.839180  [   96/  130]
train() client id: f_00008-1-3 loss: 0.701623  [  128/  130]
train() client id: f_00008-2-0 loss: 0.839584  [   32/  130]
train() client id: f_00008-2-1 loss: 0.818107  [   64/  130]
train() client id: f_00008-2-2 loss: 0.700502  [   96/  130]
train() client id: f_00008-2-3 loss: 0.724999  [  128/  130]
train() client id: f_00008-3-0 loss: 0.868314  [   32/  130]
train() client id: f_00008-3-1 loss: 0.775834  [   64/  130]
train() client id: f_00008-3-2 loss: 0.674905  [   96/  130]
train() client id: f_00008-3-3 loss: 0.740562  [  128/  130]
train() client id: f_00008-4-0 loss: 0.740482  [   32/  130]
train() client id: f_00008-4-1 loss: 0.698413  [   64/  130]
train() client id: f_00008-4-2 loss: 0.805139  [   96/  130]
train() client id: f_00008-4-3 loss: 0.795054  [  128/  130]
train() client id: f_00008-5-0 loss: 0.717272  [   32/  130]
train() client id: f_00008-5-1 loss: 0.804710  [   64/  130]
train() client id: f_00008-5-2 loss: 0.808702  [   96/  130]
train() client id: f_00008-5-3 loss: 0.749683  [  128/  130]
train() client id: f_00008-6-0 loss: 0.718228  [   32/  130]
train() client id: f_00008-6-1 loss: 0.829743  [   64/  130]
train() client id: f_00008-6-2 loss: 0.765218  [   96/  130]
train() client id: f_00008-6-3 loss: 0.771747  [  128/  130]
train() client id: f_00008-7-0 loss: 0.735942  [   32/  130]
train() client id: f_00008-7-1 loss: 0.764958  [   64/  130]
train() client id: f_00008-7-2 loss: 0.913307  [   96/  130]
train() client id: f_00008-7-3 loss: 0.665594  [  128/  130]
train() client id: f_00008-8-0 loss: 0.690945  [   32/  130]
train() client id: f_00008-8-1 loss: 0.748829  [   64/  130]
train() client id: f_00008-8-2 loss: 0.776493  [   96/  130]
train() client id: f_00008-8-3 loss: 0.817939  [  128/  130]
train() client id: f_00009-0-0 loss: 0.733448  [   32/  118]
train() client id: f_00009-0-1 loss: 0.928961  [   64/  118]
train() client id: f_00009-0-2 loss: 0.836077  [   96/  118]
train() client id: f_00009-1-0 loss: 0.892679  [   32/  118]
train() client id: f_00009-1-1 loss: 0.758605  [   64/  118]
train() client id: f_00009-1-2 loss: 0.815673  [   96/  118]
train() client id: f_00009-2-0 loss: 0.718144  [   32/  118]
train() client id: f_00009-2-1 loss: 0.777202  [   64/  118]
train() client id: f_00009-2-2 loss: 1.002625  [   96/  118]
train() client id: f_00009-3-0 loss: 0.922250  [   32/  118]
train() client id: f_00009-3-1 loss: 0.795590  [   64/  118]
train() client id: f_00009-3-2 loss: 0.547908  [   96/  118]
train() client id: f_00009-4-0 loss: 0.765795  [   32/  118]
train() client id: f_00009-4-1 loss: 1.034807  [   64/  118]
train() client id: f_00009-4-2 loss: 0.565127  [   96/  118]
train() client id: f_00009-5-0 loss: 0.855666  [   32/  118]
train() client id: f_00009-5-1 loss: 0.705466  [   64/  118]
train() client id: f_00009-5-2 loss: 0.685938  [   96/  118]
train() client id: f_00009-6-0 loss: 0.806264  [   32/  118]
train() client id: f_00009-6-1 loss: 0.675916  [   64/  118]
train() client id: f_00009-6-2 loss: 0.913478  [   96/  118]
train() client id: f_00009-7-0 loss: 0.808936  [   32/  118]
train() client id: f_00009-7-1 loss: 0.824066  [   64/  118]
train() client id: f_00009-7-2 loss: 0.627303  [   96/  118]
train() client id: f_00009-8-0 loss: 0.712826  [   32/  118]
train() client id: f_00009-8-1 loss: 0.655822  [   64/  118]
train() client id: f_00009-8-2 loss: 0.718025  [   96/  118]
At round 55 accuracy: 0.6472148541114059
At round 55 training accuracy: 0.5888665325285044
At round 55 training loss: 0.8363545603605423
update_location
xs = -4.528292 176.001589 185.045120 -185.943528 74.896481 -110.217951 -227.215960 248.375741 -1.680116 169.695607 
ys = 262.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -192.154970 4.001482 
xs mean: 32.442869159412865
ys mean: 9.371751218646875
dists_uav = 281.021248 203.023505 210.341247 212.328201 124.939606 148.848635 248.267713 268.436754 216.624918 197.009165 
uav_gains = -114.072346 -107.873154 -108.343563 -108.473818 -102.417880 -104.322731 -111.149426 -112.928237 -108.760172 -107.494093 
uav_gains_db_mean: -108.58354195000493
dists_bs = 191.656604 385.518555 399.746191 152.847445 305.452301 183.969238 185.620565 465.781041 406.008072 384.779878 
bs_gains = -103.477725 -111.976327 -112.417021 -100.726288 -109.145474 -102.979925 -103.088590 -114.276150 -112.606030 -111.953005 
bs_gains_db_mean: -108.26465348686773
Round 56
-------------------------------
ene_coms = [0.0146413  0.01232718 0.00960063 0.00967849 0.01009979 0.0071902
 0.01158852 0.01329049 0.01294961 0.01230518]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.58773141 7.37775204 3.48707158 1.26694707 8.48162795 4.06834757
 1.58590556 5.02858857 3.68202314 3.3471524 ]
obj_prev = 41.91314729480134
eta_min = 4.759149038067531e-26	eta_max = 0.9512125037863887
af = 8.788159325904157	bf = 1.0230465044863017	zeta = 9.666975258494574	eta = 0.909090909090909
af = 8.788159325904157	bf = 1.0230465044863017	zeta = 20.46712304986007	eta = 0.4293793174788305
af = 8.788159325904157	bf = 1.0230465044863017	zeta = 14.81314053290705	eta = 0.5932678020829859
af = 8.788159325904157	bf = 1.0230465044863017	zeta = 13.796367532491574	eta = 0.6369908097336
af = 8.788159325904157	bf = 1.0230465044863017	zeta = 13.736585579071056	eta = 0.6397630091784761
af = 8.788159325904157	bf = 1.0230465044863017	zeta = 13.73635588638625	eta = 0.6397737069854077
eta = 0.6397737069854077
ene_coms = [0.0146413  0.01232718 0.00960063 0.00967849 0.01009979 0.0071902
 0.01158852 0.01329049 0.01294961 0.01230518]
ene_comp = [0.03685572 0.07751402 0.03627071 0.01257775 0.08950676 0.04270584
 0.01579531 0.05235853 0.03802575 0.03451568]
ene_total = [1.28662182 2.24462806 1.14606755 0.55605867 2.48860914 1.24662221
 0.68416844 1.64020091 1.27358843 1.16979065]
ti_comp = [0.94225725 0.96539837 0.99266396 0.99188528 0.98767234 1.01676824
 0.972785   0.95576531 0.9591741  0.96561845]
ti_coms = [0.14641296 0.12327184 0.09600625 0.09678494 0.10099788 0.07190198
 0.11588522 0.1329049  0.12949612 0.12305177]
t_total = [27.19976501 27.19976501 27.19976501 27.19976501 27.19976501 27.19976501
 27.19976501 27.19976501 27.19976501 27.19976501]
ene_coms = [0.0146413  0.01232718 0.00960063 0.00967849 0.01009979 0.0071902
 0.01158852 0.01329049 0.01294961 0.01230518]
ene_comp = [3.52416212e-06 3.12325697e-05 3.02652152e-06 1.26405421e-07
 4.59432509e-05 4.70866557e-06 2.60273992e-07 9.82063984e-06
 3.73523901e-06 2.75624699e-06]
ene_total = [0.36589196 0.30876755 0.23994142 0.24181445 0.25348494 0.17976037
 0.2895387  0.3323002  0.32363153 0.30750623]
optimize_network iter = 0 obj = 2.842637347252927
eta = 0.6397737069854077
freqs = [19557145.28181084 40146132.77463586 18269381.09348262  6340323.48328349
 45311969.8279321  21000773.44663899  8118602.94098014 27390891.73459735
 19822129.87035489 17872316.93222682]
eta_min = 0.6397737069854086	eta_max = 0.7597746102988421
af = 0.0021185024064982105	bf = 1.0230465044863017	zeta = 0.0023303526471480317	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0146413  0.01232718 0.00960063 0.00967849 0.01009979 0.0071902
 0.01158852 0.01329049 0.01294961 0.01230518]
ene_comp = [7.52032875e-07 6.66482368e-06 6.45839662e-07 2.69740802e-08
 9.80398569e-06 1.00479807e-06 5.55407475e-08 2.09565955e-06
 7.97075286e-07 5.88164867e-07]
ene_total = [1.4495685  1.22105586 0.95052959 0.95817731 1.00085355 0.71193185
 1.14727369 1.31597118 1.28209556 1.2182755 ]
ti_comp = [0.57959242 0.60273354 0.62999913 0.62922044 0.6250075  0.6541034
 0.61012016 0.59310048 0.59650926 0.60295362]
ti_coms = [0.14641296 0.12327184 0.09600625 0.09678494 0.10099788 0.07190198
 0.11588522 0.1329049  0.12949612 0.12305177]
t_total = [27.19976501 27.19976501 27.19976501 27.19976501 27.19976501 27.19976501
 27.19976501 27.19976501 27.19976501 27.19976501]
ene_coms = [0.0146413  0.01232718 0.00960063 0.00967849 0.01009979 0.0071902
 0.01158852 0.01329049 0.01294961 0.01230518]
ene_comp = [2.16774945e-06 1.86478920e-05 1.74875109e-06 7.31042382e-08
 2.67015615e-05 2.64794274e-06 1.53990300e-07 5.93533701e-06
 2.24770089e-06 1.64520891e-06]
ene_total = [0.54861685 0.46253615 0.35975259 0.36260715 0.37938856 0.2694797
 0.43416932 0.4981501  0.48524096 0.46107463]
optimize_network iter = 1 obj = 4.261016002923723
eta = 0.7597746102988421
freqs = [19557145.28181082 39552881.44214477 17706770.11611295  6147849.70852182
 44044715.4776543  20080016.18174965  7962253.40476884 27150783.59223954
 19605764.18585788 17605798.01646692]
eta_min = 0.7597746102988454	eta_max = 0.7597746102988289
af = 0.0020300597855946647	bf = 1.0230465044863017	zeta = 0.0022330657641541313	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0146413  0.01232718 0.00960063 0.00967849 0.01009979 0.0071902
 0.01158852 0.01329049 0.01294961 0.01230518]
ene_comp = [7.52032875e-07 6.46930290e-06 6.06674499e-07 2.53612291e-08
 9.26327167e-06 9.18620916e-07 5.34221186e-08 2.05907955e-06
 7.79769526e-07 5.70753778e-07]
ene_total = [1.4495685  1.22103651 0.95052571 0.95817715 1.00080002 0.71192332
 1.14727348 1.31596756 1.28209385 1.21827378]
ti_comp = [0.57959242 0.60273354 0.62999913 0.62922044 0.6250075  0.6541034
 0.61012016 0.59310048 0.59650926 0.60295362]
ti_coms = [0.14641296 0.12327184 0.09600625 0.09678494 0.10099788 0.07190198
 0.11588522 0.1329049  0.12949612 0.12305177]
t_total = [27.19976501 27.19976501 27.19976501 27.19976501 27.19976501 27.19976501
 27.19976501 27.19976501 27.19976501 27.19976501]
ene_coms = [0.0146413  0.01232718 0.00960063 0.00967849 0.01009979 0.0071902
 0.01158852 0.01329049 0.01294961 0.01230518]
ene_comp = [2.16774945e-06 1.86478920e-05 1.74875109e-06 7.31042382e-08
 2.67015615e-05 2.64794274e-06 1.53990300e-07 5.93533701e-06
 2.24770089e-06 1.64520891e-06]
ene_total = [0.54861685 0.46253615 0.35975259 0.36260715 0.37938856 0.2694797
 0.43416932 0.4981501  0.48524096 0.46107463]
optimize_network iter = 2 obj = 4.26101600292349
eta = 0.7597746102988289
freqs = [19557145.28181072 39552881.44214468 17706770.11611296  6147849.70852182
 44044715.4776543  20080016.18174971  7962253.40476883 27150783.59223944
 19605764.18585783 17605798.01646688]
Done!
ene_coms = [0.0146413  0.01232718 0.00960063 0.00967849 0.01009979 0.0071902
 0.01158852 0.01329049 0.01294961 0.01230518]
ene_comp = [1.92770894e-06 1.65829626e-05 1.55510735e-06 6.50092164e-08
 2.37448284e-05 2.35472917e-06 1.36938555e-07 5.27810176e-06
 1.99880714e-06 1.46303066e-06]
ene_total = [0.01464322 0.01234377 0.00960218 0.00967856 0.01012353 0.00719255
 0.01158866 0.01329577 0.01295161 0.01230664]
At round 56 energy consumption: 0.11372649362008472
At round 56 eta: 0.7597746102988289
At round 56 a_n: 9.000035425972609
At round 56 local rounds: 8.996169132734813
At round 56 global rounds: 37.464963371141664
gradient difference: 0.5756404995918274
train() client id: f_00000-0-0 loss: 1.271367  [   32/  126]
train() client id: f_00000-0-1 loss: 1.187280  [   64/  126]
train() client id: f_00000-0-2 loss: 1.241927  [   96/  126]
train() client id: f_00000-1-0 loss: 1.171553  [   32/  126]
train() client id: f_00000-1-1 loss: 1.253902  [   64/  126]
train() client id: f_00000-1-2 loss: 1.096946  [   96/  126]
train() client id: f_00000-2-0 loss: 1.143879  [   32/  126]
train() client id: f_00000-2-1 loss: 1.118079  [   64/  126]
train() client id: f_00000-2-2 loss: 1.181609  [   96/  126]
train() client id: f_00000-3-0 loss: 1.084952  [   32/  126]
train() client id: f_00000-3-1 loss: 0.991876  [   64/  126]
train() client id: f_00000-3-2 loss: 1.051266  [   96/  126]
train() client id: f_00000-4-0 loss: 1.112496  [   32/  126]
train() client id: f_00000-4-1 loss: 1.024857  [   64/  126]
train() client id: f_00000-4-2 loss: 1.053380  [   96/  126]
train() client id: f_00000-5-0 loss: 0.971574  [   32/  126]
train() client id: f_00000-5-1 loss: 1.038456  [   64/  126]
train() client id: f_00000-5-2 loss: 0.959749  [   96/  126]
train() client id: f_00000-6-0 loss: 1.036431  [   32/  126]
train() client id: f_00000-6-1 loss: 1.015104  [   64/  126]
train() client id: f_00000-6-2 loss: 0.876010  [   96/  126]
train() client id: f_00000-7-0 loss: 1.039809  [   32/  126]
train() client id: f_00000-7-1 loss: 1.023752  [   64/  126]
train() client id: f_00000-7-2 loss: 0.830113  [   96/  126]
train() client id: f_00001-0-0 loss: 0.431786  [   32/  265]
train() client id: f_00001-0-1 loss: 0.358388  [   64/  265]
train() client id: f_00001-0-2 loss: 0.420051  [   96/  265]
train() client id: f_00001-0-3 loss: 0.346968  [  128/  265]
train() client id: f_00001-0-4 loss: 0.451017  [  160/  265]
train() client id: f_00001-0-5 loss: 0.473557  [  192/  265]
train() client id: f_00001-0-6 loss: 0.569477  [  224/  265]
train() client id: f_00001-0-7 loss: 0.421825  [  256/  265]
train() client id: f_00001-1-0 loss: 0.520197  [   32/  265]
train() client id: f_00001-1-1 loss: 0.427250  [   64/  265]
train() client id: f_00001-1-2 loss: 0.476054  [   96/  265]
train() client id: f_00001-1-3 loss: 0.534111  [  128/  265]
train() client id: f_00001-1-4 loss: 0.449607  [  160/  265]
train() client id: f_00001-1-5 loss: 0.354535  [  192/  265]
train() client id: f_00001-1-6 loss: 0.324377  [  224/  265]
train() client id: f_00001-1-7 loss: 0.335175  [  256/  265]
train() client id: f_00001-2-0 loss: 0.374228  [   32/  265]
train() client id: f_00001-2-1 loss: 0.317833  [   64/  265]
train() client id: f_00001-2-2 loss: 0.328654  [   96/  265]
train() client id: f_00001-2-3 loss: 0.405004  [  128/  265]
train() client id: f_00001-2-4 loss: 0.559630  [  160/  265]
train() client id: f_00001-2-5 loss: 0.610979  [  192/  265]
train() client id: f_00001-2-6 loss: 0.408231  [  224/  265]
train() client id: f_00001-2-7 loss: 0.379580  [  256/  265]
train() client id: f_00001-3-0 loss: 0.400697  [   32/  265]
train() client id: f_00001-3-1 loss: 0.366645  [   64/  265]
train() client id: f_00001-3-2 loss: 0.318840  [   96/  265]
train() client id: f_00001-3-3 loss: 0.505358  [  128/  265]
train() client id: f_00001-3-4 loss: 0.511083  [  160/  265]
train() client id: f_00001-3-5 loss: 0.379638  [  192/  265]
train() client id: f_00001-3-6 loss: 0.314740  [  224/  265]
train() client id: f_00001-3-7 loss: 0.384247  [  256/  265]
train() client id: f_00001-4-0 loss: 0.387214  [   32/  265]
train() client id: f_00001-4-1 loss: 0.417314  [   64/  265]
train() client id: f_00001-4-2 loss: 0.546245  [   96/  265]
train() client id: f_00001-4-3 loss: 0.476783  [  128/  265]
train() client id: f_00001-4-4 loss: 0.394894  [  160/  265]
train() client id: f_00001-4-5 loss: 0.357481  [  192/  265]
train() client id: f_00001-4-6 loss: 0.339954  [  224/  265]
train() client id: f_00001-4-7 loss: 0.397083  [  256/  265]
train() client id: f_00001-5-0 loss: 0.459364  [   32/  265]
train() client id: f_00001-5-1 loss: 0.375283  [   64/  265]
train() client id: f_00001-5-2 loss: 0.503667  [   96/  265]
train() client id: f_00001-5-3 loss: 0.397024  [  128/  265]
train() client id: f_00001-5-4 loss: 0.320983  [  160/  265]
train() client id: f_00001-5-5 loss: 0.517946  [  192/  265]
train() client id: f_00001-5-6 loss: 0.377330  [  224/  265]
train() client id: f_00001-5-7 loss: 0.350240  [  256/  265]
train() client id: f_00001-6-0 loss: 0.437429  [   32/  265]
train() client id: f_00001-6-1 loss: 0.406918  [   64/  265]
train() client id: f_00001-6-2 loss: 0.411181  [   96/  265]
train() client id: f_00001-6-3 loss: 0.507104  [  128/  265]
train() client id: f_00001-6-4 loss: 0.381367  [  160/  265]
train() client id: f_00001-6-5 loss: 0.310672  [  192/  265]
train() client id: f_00001-6-6 loss: 0.306798  [  224/  265]
train() client id: f_00001-6-7 loss: 0.441741  [  256/  265]
train() client id: f_00001-7-0 loss: 0.412110  [   32/  265]
train() client id: f_00001-7-1 loss: 0.385035  [   64/  265]
train() client id: f_00001-7-2 loss: 0.323374  [   96/  265]
train() client id: f_00001-7-3 loss: 0.421428  [  128/  265]
train() client id: f_00001-7-4 loss: 0.555459  [  160/  265]
train() client id: f_00001-7-5 loss: 0.305854  [  192/  265]
train() client id: f_00001-7-6 loss: 0.456934  [  224/  265]
train() client id: f_00001-7-7 loss: 0.424458  [  256/  265]
train() client id: f_00002-0-0 loss: 1.102777  [   32/  124]
train() client id: f_00002-0-1 loss: 1.189196  [   64/  124]
train() client id: f_00002-0-2 loss: 1.171072  [   96/  124]
train() client id: f_00002-1-0 loss: 1.263629  [   32/  124]
train() client id: f_00002-1-1 loss: 1.150470  [   64/  124]
train() client id: f_00002-1-2 loss: 1.060677  [   96/  124]
train() client id: f_00002-2-0 loss: 1.234923  [   32/  124]
train() client id: f_00002-2-1 loss: 1.024504  [   64/  124]
train() client id: f_00002-2-2 loss: 1.027430  [   96/  124]
train() client id: f_00002-3-0 loss: 1.062249  [   32/  124]
train() client id: f_00002-3-1 loss: 1.007506  [   64/  124]
train() client id: f_00002-3-2 loss: 1.174755  [   96/  124]
train() client id: f_00002-4-0 loss: 1.180200  [   32/  124]
train() client id: f_00002-4-1 loss: 1.065520  [   64/  124]
train() client id: f_00002-4-2 loss: 0.880522  [   96/  124]
train() client id: f_00002-5-0 loss: 1.018409  [   32/  124]
train() client id: f_00002-5-1 loss: 1.193338  [   64/  124]
train() client id: f_00002-5-2 loss: 1.048680  [   96/  124]
train() client id: f_00002-6-0 loss: 0.973511  [   32/  124]
train() client id: f_00002-6-1 loss: 1.094635  [   64/  124]
train() client id: f_00002-6-2 loss: 0.945349  [   96/  124]
train() client id: f_00002-7-0 loss: 1.059383  [   32/  124]
train() client id: f_00002-7-1 loss: 1.022709  [   64/  124]
train() client id: f_00002-7-2 loss: 0.980780  [   96/  124]
train() client id: f_00003-0-0 loss: 0.366598  [   32/   43]
train() client id: f_00003-1-0 loss: 0.591975  [   32/   43]
train() client id: f_00003-2-0 loss: 0.679128  [   32/   43]
train() client id: f_00003-3-0 loss: 0.639310  [   32/   43]
train() client id: f_00003-4-0 loss: 0.531121  [   32/   43]
train() client id: f_00003-5-0 loss: 0.703591  [   32/   43]
train() client id: f_00003-6-0 loss: 0.537068  [   32/   43]
train() client id: f_00003-7-0 loss: 0.467172  [   32/   43]
train() client id: f_00004-0-0 loss: 0.983282  [   32/  306]
train() client id: f_00004-0-1 loss: 0.782475  [   64/  306]
train() client id: f_00004-0-2 loss: 1.021180  [   96/  306]
train() client id: f_00004-0-3 loss: 0.927488  [  128/  306]
train() client id: f_00004-0-4 loss: 0.840832  [  160/  306]
train() client id: f_00004-0-5 loss: 0.952467  [  192/  306]
train() client id: f_00004-0-6 loss: 1.085650  [  224/  306]
train() client id: f_00004-0-7 loss: 0.977666  [  256/  306]
train() client id: f_00004-0-8 loss: 0.946170  [  288/  306]
train() client id: f_00004-1-0 loss: 0.777764  [   32/  306]
train() client id: f_00004-1-1 loss: 1.086655  [   64/  306]
train() client id: f_00004-1-2 loss: 1.054081  [   96/  306]
train() client id: f_00004-1-3 loss: 0.845736  [  128/  306]
train() client id: f_00004-1-4 loss: 1.029227  [  160/  306]
train() client id: f_00004-1-5 loss: 0.933823  [  192/  306]
train() client id: f_00004-1-6 loss: 1.017569  [  224/  306]
train() client id: f_00004-1-7 loss: 0.938820  [  256/  306]
train() client id: f_00004-1-8 loss: 0.913050  [  288/  306]
train() client id: f_00004-2-0 loss: 0.911477  [   32/  306]
train() client id: f_00004-2-1 loss: 1.052679  [   64/  306]
train() client id: f_00004-2-2 loss: 1.059457  [   96/  306]
train() client id: f_00004-2-3 loss: 0.951973  [  128/  306]
train() client id: f_00004-2-4 loss: 0.960421  [  160/  306]
train() client id: f_00004-2-5 loss: 0.960438  [  192/  306]
train() client id: f_00004-2-6 loss: 0.846481  [  224/  306]
train() client id: f_00004-2-7 loss: 0.866427  [  256/  306]
train() client id: f_00004-2-8 loss: 0.964039  [  288/  306]
train() client id: f_00004-3-0 loss: 1.030316  [   32/  306]
train() client id: f_00004-3-1 loss: 0.847613  [   64/  306]
train() client id: f_00004-3-2 loss: 0.861907  [   96/  306]
train() client id: f_00004-3-3 loss: 0.877237  [  128/  306]
train() client id: f_00004-3-4 loss: 0.920317  [  160/  306]
train() client id: f_00004-3-5 loss: 0.945551  [  192/  306]
train() client id: f_00004-3-6 loss: 1.084149  [  224/  306]
train() client id: f_00004-3-7 loss: 0.934844  [  256/  306]
train() client id: f_00004-3-8 loss: 0.985072  [  288/  306]
train() client id: f_00004-4-0 loss: 0.935423  [   32/  306]
train() client id: f_00004-4-1 loss: 1.062142  [   64/  306]
train() client id: f_00004-4-2 loss: 0.909814  [   96/  306]
train() client id: f_00004-4-3 loss: 0.902694  [  128/  306]
train() client id: f_00004-4-4 loss: 0.968222  [  160/  306]
train() client id: f_00004-4-5 loss: 1.006324  [  192/  306]
train() client id: f_00004-4-6 loss: 0.856725  [  224/  306]
train() client id: f_00004-4-7 loss: 0.878209  [  256/  306]
train() client id: f_00004-4-8 loss: 1.014421  [  288/  306]
train() client id: f_00004-5-0 loss: 1.064583  [   32/  306]
train() client id: f_00004-5-1 loss: 0.956396  [   64/  306]
train() client id: f_00004-5-2 loss: 0.844745  [   96/  306]
train() client id: f_00004-5-3 loss: 1.068925  [  128/  306]
train() client id: f_00004-5-4 loss: 1.000956  [  160/  306]
train() client id: f_00004-5-5 loss: 0.976628  [  192/  306]
train() client id: f_00004-5-6 loss: 0.790734  [  224/  306]
train() client id: f_00004-5-7 loss: 0.915587  [  256/  306]
train() client id: f_00004-5-8 loss: 0.863538  [  288/  306]
train() client id: f_00004-6-0 loss: 0.883037  [   32/  306]
train() client id: f_00004-6-1 loss: 0.940018  [   64/  306]
train() client id: f_00004-6-2 loss: 0.988799  [   96/  306]
train() client id: f_00004-6-3 loss: 0.869907  [  128/  306]
train() client id: f_00004-6-4 loss: 0.995558  [  160/  306]
train() client id: f_00004-6-5 loss: 0.932906  [  192/  306]
train() client id: f_00004-6-6 loss: 1.030086  [  224/  306]
train() client id: f_00004-6-7 loss: 0.892512  [  256/  306]
train() client id: f_00004-6-8 loss: 0.906467  [  288/  306]
train() client id: f_00004-7-0 loss: 0.877607  [   32/  306]
train() client id: f_00004-7-1 loss: 0.972179  [   64/  306]
train() client id: f_00004-7-2 loss: 0.906087  [   96/  306]
train() client id: f_00004-7-3 loss: 0.993458  [  128/  306]
train() client id: f_00004-7-4 loss: 0.805690  [  160/  306]
train() client id: f_00004-7-5 loss: 1.029669  [  192/  306]
train() client id: f_00004-7-6 loss: 0.956248  [  224/  306]
train() client id: f_00004-7-7 loss: 0.871332  [  256/  306]
train() client id: f_00004-7-8 loss: 0.967563  [  288/  306]
train() client id: f_00005-0-0 loss: 0.481995  [   32/  146]
train() client id: f_00005-0-1 loss: 0.417377  [   64/  146]
train() client id: f_00005-0-2 loss: 0.722992  [   96/  146]
train() client id: f_00005-0-3 loss: 0.424401  [  128/  146]
train() client id: f_00005-1-0 loss: 0.699093  [   32/  146]
train() client id: f_00005-1-1 loss: 0.529808  [   64/  146]
train() client id: f_00005-1-2 loss: 0.462152  [   96/  146]
train() client id: f_00005-1-3 loss: 0.817367  [  128/  146]
train() client id: f_00005-2-0 loss: 0.638697  [   32/  146]
train() client id: f_00005-2-1 loss: 0.574753  [   64/  146]
train() client id: f_00005-2-2 loss: 0.517220  [   96/  146]
train() client id: f_00005-2-3 loss: 0.654166  [  128/  146]
train() client id: f_00005-3-0 loss: 0.793430  [   32/  146]
train() client id: f_00005-3-1 loss: 0.583986  [   64/  146]
train() client id: f_00005-3-2 loss: 0.497875  [   96/  146]
train() client id: f_00005-3-3 loss: 0.632035  [  128/  146]
train() client id: f_00005-4-0 loss: 0.573718  [   32/  146]
train() client id: f_00005-4-1 loss: 0.713381  [   64/  146]
train() client id: f_00005-4-2 loss: 0.410529  [   96/  146]
train() client id: f_00005-4-3 loss: 0.537364  [  128/  146]
train() client id: f_00005-5-0 loss: 0.575601  [   32/  146]
train() client id: f_00005-5-1 loss: 0.367636  [   64/  146]
train() client id: f_00005-5-2 loss: 0.747020  [   96/  146]
train() client id: f_00005-5-3 loss: 0.862167  [  128/  146]
train() client id: f_00005-6-0 loss: 0.530482  [   32/  146]
train() client id: f_00005-6-1 loss: 0.578921  [   64/  146]
train() client id: f_00005-6-2 loss: 0.590367  [   96/  146]
train() client id: f_00005-6-3 loss: 0.657013  [  128/  146]
train() client id: f_00005-7-0 loss: 0.406360  [   32/  146]
train() client id: f_00005-7-1 loss: 0.400766  [   64/  146]
train() client id: f_00005-7-2 loss: 0.574540  [   96/  146]
train() client id: f_00005-7-3 loss: 0.888869  [  128/  146]
train() client id: f_00006-0-0 loss: 0.450661  [   32/   54]
train() client id: f_00006-1-0 loss: 0.530801  [   32/   54]
train() client id: f_00006-2-0 loss: 0.491334  [   32/   54]
train() client id: f_00006-3-0 loss: 0.550598  [   32/   54]
train() client id: f_00006-4-0 loss: 0.506394  [   32/   54]
train() client id: f_00006-5-0 loss: 0.510190  [   32/   54]
train() client id: f_00006-6-0 loss: 0.494992  [   32/   54]
train() client id: f_00006-7-0 loss: 0.455868  [   32/   54]
train() client id: f_00007-0-0 loss: 0.797704  [   32/  179]
train() client id: f_00007-0-1 loss: 0.552164  [   64/  179]
train() client id: f_00007-0-2 loss: 0.607498  [   96/  179]
train() client id: f_00007-0-3 loss: 0.923704  [  128/  179]
train() client id: f_00007-0-4 loss: 0.498128  [  160/  179]
train() client id: f_00007-1-0 loss: 0.572633  [   32/  179]
train() client id: f_00007-1-1 loss: 0.542992  [   64/  179]
train() client id: f_00007-1-2 loss: 0.709789  [   96/  179]
train() client id: f_00007-1-3 loss: 0.847752  [  128/  179]
train() client id: f_00007-1-4 loss: 0.653989  [  160/  179]
train() client id: f_00007-2-0 loss: 0.666634  [   32/  179]
train() client id: f_00007-2-1 loss: 0.847927  [   64/  179]
train() client id: f_00007-2-2 loss: 0.506522  [   96/  179]
train() client id: f_00007-2-3 loss: 0.576352  [  128/  179]
train() client id: f_00007-2-4 loss: 0.667722  [  160/  179]
train() client id: f_00007-3-0 loss: 0.638651  [   32/  179]
train() client id: f_00007-3-1 loss: 0.489136  [   64/  179]
train() client id: f_00007-3-2 loss: 0.738919  [   96/  179]
train() client id: f_00007-3-3 loss: 0.485555  [  128/  179]
train() client id: f_00007-3-4 loss: 0.879251  [  160/  179]
train() client id: f_00007-4-0 loss: 0.786574  [   32/  179]
train() client id: f_00007-4-1 loss: 0.679734  [   64/  179]
train() client id: f_00007-4-2 loss: 0.647101  [   96/  179]
train() client id: f_00007-4-3 loss: 0.602349  [  128/  179]
train() client id: f_00007-4-4 loss: 0.478832  [  160/  179]
train() client id: f_00007-5-0 loss: 0.442213  [   32/  179]
train() client id: f_00007-5-1 loss: 0.751012  [   64/  179]
train() client id: f_00007-5-2 loss: 0.594120  [   96/  179]
train() client id: f_00007-5-3 loss: 0.692250  [  128/  179]
train() client id: f_00007-5-4 loss: 0.512441  [  160/  179]
train() client id: f_00007-6-0 loss: 0.826141  [   32/  179]
train() client id: f_00007-6-1 loss: 0.656926  [   64/  179]
train() client id: f_00007-6-2 loss: 0.584121  [   96/  179]
train() client id: f_00007-6-3 loss: 0.597325  [  128/  179]
train() client id: f_00007-6-4 loss: 0.587321  [  160/  179]
train() client id: f_00007-7-0 loss: 0.670025  [   32/  179]
train() client id: f_00007-7-1 loss: 0.611679  [   64/  179]
train() client id: f_00007-7-2 loss: 0.462107  [   96/  179]
train() client id: f_00007-7-3 loss: 0.669663  [  128/  179]
train() client id: f_00007-7-4 loss: 0.728901  [  160/  179]
train() client id: f_00008-0-0 loss: 0.588749  [   32/  130]
train() client id: f_00008-0-1 loss: 0.718427  [   64/  130]
train() client id: f_00008-0-2 loss: 0.629777  [   96/  130]
train() client id: f_00008-0-3 loss: 0.768320  [  128/  130]
train() client id: f_00008-1-0 loss: 0.679272  [   32/  130]
train() client id: f_00008-1-1 loss: 0.678769  [   64/  130]
train() client id: f_00008-1-2 loss: 0.590240  [   96/  130]
train() client id: f_00008-1-3 loss: 0.790149  [  128/  130]
train() client id: f_00008-2-0 loss: 0.637288  [   32/  130]
train() client id: f_00008-2-1 loss: 0.685603  [   64/  130]
train() client id: f_00008-2-2 loss: 0.697242  [   96/  130]
train() client id: f_00008-2-3 loss: 0.695709  [  128/  130]
train() client id: f_00008-3-0 loss: 0.597304  [   32/  130]
train() client id: f_00008-3-1 loss: 0.569256  [   64/  130]
train() client id: f_00008-3-2 loss: 0.765586  [   96/  130]
train() client id: f_00008-3-3 loss: 0.804446  [  128/  130]
train() client id: f_00008-4-0 loss: 0.753320  [   32/  130]
train() client id: f_00008-4-1 loss: 0.588364  [   64/  130]
train() client id: f_00008-4-2 loss: 0.732640  [   96/  130]
train() client id: f_00008-4-3 loss: 0.684852  [  128/  130]
train() client id: f_00008-5-0 loss: 0.653165  [   32/  130]
train() client id: f_00008-5-1 loss: 0.738561  [   64/  130]
train() client id: f_00008-5-2 loss: 0.728153  [   96/  130]
train() client id: f_00008-5-3 loss: 0.632622  [  128/  130]
train() client id: f_00008-6-0 loss: 0.659011  [   32/  130]
train() client id: f_00008-6-1 loss: 0.645302  [   64/  130]
train() client id: f_00008-6-2 loss: 0.787333  [   96/  130]
train() client id: f_00008-6-3 loss: 0.640537  [  128/  130]
train() client id: f_00008-7-0 loss: 0.772502  [   32/  130]
train() client id: f_00008-7-1 loss: 0.657684  [   64/  130]
train() client id: f_00008-7-2 loss: 0.656180  [   96/  130]
train() client id: f_00008-7-3 loss: 0.676056  [  128/  130]
train() client id: f_00009-0-0 loss: 1.027584  [   32/  118]
train() client id: f_00009-0-1 loss: 1.007707  [   64/  118]
train() client id: f_00009-0-2 loss: 1.007904  [   96/  118]
train() client id: f_00009-1-0 loss: 1.014162  [   32/  118]
train() client id: f_00009-1-1 loss: 1.094016  [   64/  118]
train() client id: f_00009-1-2 loss: 0.813431  [   96/  118]
train() client id: f_00009-2-0 loss: 0.726916  [   32/  118]
train() client id: f_00009-2-1 loss: 1.118716  [   64/  118]
train() client id: f_00009-2-2 loss: 0.824640  [   96/  118]
train() client id: f_00009-3-0 loss: 0.936010  [   32/  118]
train() client id: f_00009-3-1 loss: 0.863663  [   64/  118]
train() client id: f_00009-3-2 loss: 0.946097  [   96/  118]
train() client id: f_00009-4-0 loss: 0.963148  [   32/  118]
train() client id: f_00009-4-1 loss: 0.929190  [   64/  118]
train() client id: f_00009-4-2 loss: 0.920305  [   96/  118]
train() client id: f_00009-5-0 loss: 0.907711  [   32/  118]
train() client id: f_00009-5-1 loss: 0.859688  [   64/  118]
train() client id: f_00009-5-2 loss: 0.688931  [   96/  118]
train() client id: f_00009-6-0 loss: 0.850305  [   32/  118]
train() client id: f_00009-6-1 loss: 0.987285  [   64/  118]
train() client id: f_00009-6-2 loss: 0.853650  [   96/  118]
train() client id: f_00009-7-0 loss: 0.982433  [   32/  118]
train() client id: f_00009-7-1 loss: 0.823432  [   64/  118]
train() client id: f_00009-7-2 loss: 0.848179  [   96/  118]
At round 56 accuracy: 0.6472148541114059
At round 56 training accuracy: 0.5861837692823608
At round 56 training loss: 0.8281941669790931
update_location
xs = -4.528292 181.001589 190.045120 -190.943528 79.896481 -115.217951 -232.215960 253.375741 -1.680116 174.695607 
ys = 267.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -197.154970 4.001482 
xs mean: 33.442869159412865
ys mean: 9.371751218646875
dists_uav = 285.698830 207.372996 214.753094 216.720327 127.999492 152.587993 252.851769 273.069677 221.072171 201.331982 
uav_gains = -114.492497 -108.151167 -108.634584 -108.766610 -102.680734 -104.593710 -111.540449 -113.349950 -109.064369 -107.766035 
uav_gains_db_mean: -108.90401055833618
dists_bs = 193.992612 390.076367 404.255450 153.286584 309.556252 182.268648 187.088625 470.330454 410.535143 389.265348 
bs_gains = -103.625044 -112.119249 -112.553424 -100.761175 -109.307767 -102.866994 -103.184386 -114.394346 -112.740869 -112.093940 
bs_gains_db_mean: -108.36471948792112
Round 57
-------------------------------
ene_coms = [0.01519965 0.01246363 0.00977627 0.00985798 0.01020683 0.0071521
 0.01192741 0.01376165 0.01309033 0.01243927]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.45606334 7.09814433 3.35588791 1.22029603 8.159749   3.91317104
 1.52850875 4.84131823 3.54311406 3.22093075]
obj_prev = 40.33718342300374
eta_min = 5.0155214935871174e-27	eta_max = 0.9512023995441812
af = 8.453677589209455	bf = 1.003187592990633	zeta = 9.299045348130402	eta = 0.909090909090909
af = 8.453677589209455	bf = 1.003187592990633	zeta = 19.898000364285732	eta = 0.4248506098322666
af = 8.453677589209455	bf = 1.003187592990633	zeta = 14.326164984964441	eta = 0.5900865722321177
af = 8.453677589209455	bf = 1.003187592990633	zeta = 13.325702382431468	eta = 0.6343888934781207
af = 8.453677589209455	bf = 1.003187592990633	zeta = 13.26655011763831	eta = 0.6372174766045632
af = 8.453677589209455	bf = 1.003187592990633	zeta = 13.266319857130927	eta = 0.6372285366439001
eta = 0.6372285366439001
ene_coms = [0.01519965 0.01246363 0.00977627 0.00985798 0.01020683 0.0071521
 0.01192741 0.01376165 0.01309033 0.01243927]
ene_comp = [0.03718465 0.07820582 0.03659442 0.01269    0.09030559 0.04308698
 0.01593628 0.05282582 0.03836512 0.03482372]
ene_total = [1.25014399 2.16381358 1.10662999 0.53810436 2.39871451 1.19894843
 0.664963   1.58910043 1.22797696 1.12792461]
ti_comp = [0.9856495  1.01300969 1.0398833  1.03906621 1.03557774 1.06612504
 1.01837191 1.00002952 1.00674272 1.01325331]
ti_coms = [0.15199651 0.12463632 0.09776271 0.0985798  0.10206827 0.07152097
 0.11927409 0.13761649 0.13090329 0.12439269]
t_total = [27.14976082 27.14976082 27.14976082 27.14976082 27.14976082 27.14976082
 27.14976082 27.14976082 27.14976082 27.14976082]
ene_coms = [0.01519965 0.01246363 0.00977627 0.00985798 0.01020683 0.0071521
 0.01192741 0.01376165 0.01309033 0.01243927]
ene_comp = [3.30770072e-06 2.91319803e-05 2.83240556e-06 1.18298302e-07
 4.29198757e-05 4.39847236e-06 2.43909120e-07 9.21282959e-06
 3.48219148e-06 2.57080741e-06]
ene_total = [0.36281645 0.29813804 0.23337692 0.23526212 0.24460876 0.17078875
 0.28465176 0.32863966 0.31248195 0.29692276]
optimize_network iter = 0 obj = 2.7676871523659443
eta = 0.6372285366439001
freqs = [18863020.2966219  38600725.54550414 17595445.9076773   6106444.69069896
 43601548.6852721  20207282.19289965  7824390.92004367 26412128.6779453
 19054082.82635181 17184114.92256154]
eta_min = 0.6372285366439012	eta_max = 0.761413335591738
af = 0.0018869831161758177	bf = 1.003187592990633	zeta = 0.0020756814277933995	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01519965 0.01246363 0.00977627 0.00985798 0.01020683 0.0071521
 0.01192741 0.01376165 0.01309033 0.01243927]
ene_comp = [6.99597689e-07 6.16158106e-06 5.99070035e-07 2.50207699e-08
 9.07780008e-06 9.30302152e-07 5.15881790e-08 1.94856634e-06
 7.36503486e-07 5.43740524e-07]
ene_total = [1.44756561 1.18752816 0.93107469 0.93880132 0.972885   0.68119985
 1.13588056 1.31074021 1.24669338 1.18467308]
ti_comp = [0.59620779 0.62356798 0.65044158 0.64962449 0.64613602 0.67668332
 0.6289302  0.61058781 0.617301   0.6238116 ]
ti_coms = [0.15199651 0.12463632 0.09776271 0.0985798  0.10206827 0.07152097
 0.11927409 0.13761649 0.13090329 0.12439269]
t_total = [27.14976082 27.14976082 27.14976082 27.14976082 27.14976082 27.14976082
 27.14976082 27.14976082 27.14976082 27.14976082]
ene_coms = [0.01519965 0.01246363 0.00977627 0.00985798 0.01020683 0.0071521
 0.01192741 0.01376165 0.01309033 0.01243927]
ene_comp = [2.00078925e-06 1.70159035e-05 1.60226173e-06 6.69831522e-08
 2.44006486e-05 2.41642109e-06 1.41534195e-07 5.46950266e-06
 2.04984531e-06 1.50115013e-06]
ene_total = [0.55161565 0.45287978 0.35480541 0.35771462 0.37125607 0.25961269
 0.43280981 0.49956138 0.4750774  0.45143278]
optimize_network iter = 1 obj = 4.206765589643969
eta = 0.761413335591738
freqs = [18863020.29662189 37931533.58961879 17015774.44306582  5908053.16898707
 42270346.16856276 19257757.47293996  7663543.77430642 26166355.39214286
 18796833.73711289 16883671.47526203]
eta_min = 0.7614133355917415	eta_max = 0.7614133355912088
af = 0.0017997612906462794	bf = 1.003187592990633	zeta = 0.0019797374197109076	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01519965 0.01246363 0.00977627 0.00985798 0.01020683 0.0071521
 0.01192741 0.01376165 0.01309033 0.01243927]
ene_comp = [6.99597689e-07 5.94979544e-06 5.60248216e-07 2.34213866e-08
 8.53195178e-06 8.44927876e-07 4.94889685e-08 1.91247100e-06
 7.16750673e-07 5.24893446e-07]
ene_total = [1.44756561 1.18750799 0.93107099 0.93880117 0.97283302 0.68119171
 1.13588036 1.31073677 1.2466915  1.18467129]
ti_comp = [0.59620779 0.62356798 0.65044158 0.64962449 0.64613602 0.67668332
 0.6289302  0.61058781 0.617301   0.6238116 ]
ti_coms = [0.15199651 0.12463632 0.09776271 0.0985798  0.10206827 0.07152097
 0.11927409 0.13761649 0.13090329 0.12439269]
t_total = [27.14976082 27.14976082 27.14976082 27.14976082 27.14976082 27.14976082
 27.14976082 27.14976082 27.14976082 27.14976082]
ene_coms = [0.01519965 0.01246363 0.00977627 0.00985798 0.01020683 0.0071521
 0.01192741 0.01376165 0.01309033 0.01243927]
ene_comp = [2.00078925e-06 1.70159035e-05 1.60226173e-06 6.69831522e-08
 2.44006486e-05 2.41642109e-06 1.41534195e-07 5.46950266e-06
 2.04984531e-06 1.50115013e-06]
ene_total = [0.55161565 0.45287978 0.35480541 0.35771462 0.37125607 0.25961269
 0.43280981 0.49956138 0.4750774  0.45143278]
optimize_network iter = 2 obj = 4.206765589634644
eta = 0.7614133355912088
freqs = [18863020.29661749 37931533.58961457 17015774.4430658   5908053.16898705
 42270346.168562   19257757.47294185  7663543.77430574 26166355.39213847
 18796833.7371103  16883671.47526017]
Done!
ene_coms = [0.01519965 0.01246363 0.00977627 0.00985798 0.01020683 0.0071521
 0.01192741 0.01376165 0.01309033 0.01243927]
ene_comp = [1.79330021e-06 1.52512931e-05 1.43610144e-06 6.00367587e-08
 2.18702137e-05 2.16582954e-06 1.26856591e-07 4.90229559e-06
 1.83726898e-06 1.34547547e-06]
ene_total = [0.01520144 0.01247888 0.00977771 0.00985804 0.0102287  0.00715426
 0.01192754 0.01376655 0.01309217 0.01244061]
At round 57 energy consumption: 0.1159259027950061
At round 57 eta: 0.7614133355912088
At round 57 a_n: 8.65748957900329
At round 57 local rounds: 8.925618728101298
At round 57 global rounds: 36.28656111378322
gradient difference: 0.5475285649299622
train() client id: f_00000-0-0 loss: 0.954367  [   32/  126]
train() client id: f_00000-0-1 loss: 1.033992  [   64/  126]
train() client id: f_00000-0-2 loss: 1.085979  [   96/  126]
train() client id: f_00000-1-0 loss: 1.123785  [   32/  126]
train() client id: f_00000-1-1 loss: 0.968862  [   64/  126]
train() client id: f_00000-1-2 loss: 1.125435  [   96/  126]
train() client id: f_00000-2-0 loss: 0.923427  [   32/  126]
train() client id: f_00000-2-1 loss: 1.161213  [   64/  126]
train() client id: f_00000-2-2 loss: 1.072054  [   96/  126]
train() client id: f_00000-3-0 loss: 0.968197  [   32/  126]
train() client id: f_00000-3-1 loss: 1.012964  [   64/  126]
train() client id: f_00000-3-2 loss: 0.955620  [   96/  126]
train() client id: f_00000-4-0 loss: 0.955796  [   32/  126]
train() client id: f_00000-4-1 loss: 0.999952  [   64/  126]
train() client id: f_00000-4-2 loss: 1.054948  [   96/  126]
train() client id: f_00000-5-0 loss: 0.927426  [   32/  126]
train() client id: f_00000-5-1 loss: 0.982275  [   64/  126]
train() client id: f_00000-5-2 loss: 1.127340  [   96/  126]
train() client id: f_00000-6-0 loss: 0.910157  [   32/  126]
train() client id: f_00000-6-1 loss: 0.999225  [   64/  126]
train() client id: f_00000-6-2 loss: 1.095736  [   96/  126]
train() client id: f_00000-7-0 loss: 1.001642  [   32/  126]
train() client id: f_00000-7-1 loss: 1.091403  [   64/  126]
train() client id: f_00000-7-2 loss: 0.876989  [   96/  126]
train() client id: f_00001-0-0 loss: 0.546306  [   32/  265]
train() client id: f_00001-0-1 loss: 0.381696  [   64/  265]
train() client id: f_00001-0-2 loss: 0.503982  [   96/  265]
train() client id: f_00001-0-3 loss: 0.487697  [  128/  265]
train() client id: f_00001-0-4 loss: 0.384448  [  160/  265]
train() client id: f_00001-0-5 loss: 0.372712  [  192/  265]
train() client id: f_00001-0-6 loss: 0.386609  [  224/  265]
train() client id: f_00001-0-7 loss: 0.424773  [  256/  265]
train() client id: f_00001-1-0 loss: 0.523658  [   32/  265]
train() client id: f_00001-1-1 loss: 0.412127  [   64/  265]
train() client id: f_00001-1-2 loss: 0.439714  [   96/  265]
train() client id: f_00001-1-3 loss: 0.446152  [  128/  265]
train() client id: f_00001-1-4 loss: 0.383196  [  160/  265]
train() client id: f_00001-1-5 loss: 0.461078  [  192/  265]
train() client id: f_00001-1-6 loss: 0.454023  [  224/  265]
train() client id: f_00001-1-7 loss: 0.454180  [  256/  265]
train() client id: f_00001-2-0 loss: 0.476660  [   32/  265]
train() client id: f_00001-2-1 loss: 0.527541  [   64/  265]
train() client id: f_00001-2-2 loss: 0.359458  [   96/  265]
train() client id: f_00001-2-3 loss: 0.412345  [  128/  265]
train() client id: f_00001-2-4 loss: 0.472752  [  160/  265]
train() client id: f_00001-2-5 loss: 0.552559  [  192/  265]
train() client id: f_00001-2-6 loss: 0.370795  [  224/  265]
train() client id: f_00001-2-7 loss: 0.343597  [  256/  265]
train() client id: f_00001-3-0 loss: 0.401199  [   32/  265]
train() client id: f_00001-3-1 loss: 0.421215  [   64/  265]
train() client id: f_00001-3-2 loss: 0.355846  [   96/  265]
train() client id: f_00001-3-3 loss: 0.432064  [  128/  265]
train() client id: f_00001-3-4 loss: 0.520580  [  160/  265]
train() client id: f_00001-3-5 loss: 0.490402  [  192/  265]
train() client id: f_00001-3-6 loss: 0.421241  [  224/  265]
train() client id: f_00001-3-7 loss: 0.388555  [  256/  265]
train() client id: f_00001-4-0 loss: 0.527030  [   32/  265]
train() client id: f_00001-4-1 loss: 0.439515  [   64/  265]
train() client id: f_00001-4-2 loss: 0.358993  [   96/  265]
train() client id: f_00001-4-3 loss: 0.459359  [  128/  265]
train() client id: f_00001-4-4 loss: 0.470472  [  160/  265]
train() client id: f_00001-4-5 loss: 0.405315  [  192/  265]
train() client id: f_00001-4-6 loss: 0.435296  [  224/  265]
train() client id: f_00001-4-7 loss: 0.355760  [  256/  265]
train() client id: f_00001-5-0 loss: 0.409842  [   32/  265]
train() client id: f_00001-5-1 loss: 0.461820  [   64/  265]
train() client id: f_00001-5-2 loss: 0.457994  [   96/  265]
train() client id: f_00001-5-3 loss: 0.434150  [  128/  265]
train() client id: f_00001-5-4 loss: 0.321484  [  160/  265]
train() client id: f_00001-5-5 loss: 0.381639  [  192/  265]
train() client id: f_00001-5-6 loss: 0.470450  [  224/  265]
train() client id: f_00001-5-7 loss: 0.420077  [  256/  265]
train() client id: f_00001-6-0 loss: 0.498942  [   32/  265]
train() client id: f_00001-6-1 loss: 0.432285  [   64/  265]
train() client id: f_00001-6-2 loss: 0.327758  [   96/  265]
train() client id: f_00001-6-3 loss: 0.391071  [  128/  265]
train() client id: f_00001-6-4 loss: 0.464992  [  160/  265]
train() client id: f_00001-6-5 loss: 0.514347  [  192/  265]
train() client id: f_00001-6-6 loss: 0.340530  [  224/  265]
train() client id: f_00001-6-7 loss: 0.418059  [  256/  265]
train() client id: f_00001-7-0 loss: 0.555020  [   32/  265]
train() client id: f_00001-7-1 loss: 0.430742  [   64/  265]
train() client id: f_00001-7-2 loss: 0.402354  [   96/  265]
train() client id: f_00001-7-3 loss: 0.389530  [  128/  265]
train() client id: f_00001-7-4 loss: 0.408163  [  160/  265]
train() client id: f_00001-7-5 loss: 0.450501  [  192/  265]
train() client id: f_00001-7-6 loss: 0.354738  [  224/  265]
train() client id: f_00001-7-7 loss: 0.413229  [  256/  265]
train() client id: f_00002-0-0 loss: 1.224995  [   32/  124]
train() client id: f_00002-0-1 loss: 0.976075  [   64/  124]
train() client id: f_00002-0-2 loss: 1.179169  [   96/  124]
train() client id: f_00002-1-0 loss: 1.125979  [   32/  124]
train() client id: f_00002-1-1 loss: 1.016797  [   64/  124]
train() client id: f_00002-1-2 loss: 1.143326  [   96/  124]
train() client id: f_00002-2-0 loss: 1.052781  [   32/  124]
train() client id: f_00002-2-1 loss: 1.115412  [   64/  124]
train() client id: f_00002-2-2 loss: 1.082101  [   96/  124]
train() client id: f_00002-3-0 loss: 0.972501  [   32/  124]
train() client id: f_00002-3-1 loss: 1.172136  [   64/  124]
train() client id: f_00002-3-2 loss: 1.119465  [   96/  124]
train() client id: f_00002-4-0 loss: 1.268997  [   32/  124]
train() client id: f_00002-4-1 loss: 0.899836  [   64/  124]
train() client id: f_00002-4-2 loss: 1.004318  [   96/  124]
train() client id: f_00002-5-0 loss: 1.002893  [   32/  124]
train() client id: f_00002-5-1 loss: 1.080430  [   64/  124]
train() client id: f_00002-5-2 loss: 1.130278  [   96/  124]
train() client id: f_00002-6-0 loss: 1.280264  [   32/  124]
train() client id: f_00002-6-1 loss: 0.894226  [   64/  124]
train() client id: f_00002-6-2 loss: 0.953506  [   96/  124]
train() client id: f_00002-7-0 loss: 0.955508  [   32/  124]
train() client id: f_00002-7-1 loss: 1.128564  [   64/  124]
train() client id: f_00002-7-2 loss: 0.790714  [   96/  124]
train() client id: f_00003-0-0 loss: 0.685745  [   32/   43]
train() client id: f_00003-1-0 loss: 0.757083  [   32/   43]
train() client id: f_00003-2-0 loss: 0.868577  [   32/   43]
train() client id: f_00003-3-0 loss: 0.680902  [   32/   43]
train() client id: f_00003-4-0 loss: 0.722281  [   32/   43]
train() client id: f_00003-5-0 loss: 0.684679  [   32/   43]
train() client id: f_00003-6-0 loss: 0.600714  [   32/   43]
train() client id: f_00003-7-0 loss: 0.555941  [   32/   43]
train() client id: f_00004-0-0 loss: 0.739717  [   32/  306]
train() client id: f_00004-0-1 loss: 0.702953  [   64/  306]
train() client id: f_00004-0-2 loss: 0.811724  [   96/  306]
train() client id: f_00004-0-3 loss: 0.860723  [  128/  306]
train() client id: f_00004-0-4 loss: 0.895171  [  160/  306]
train() client id: f_00004-0-5 loss: 0.688864  [  192/  306]
train() client id: f_00004-0-6 loss: 0.717858  [  224/  306]
train() client id: f_00004-0-7 loss: 0.912370  [  256/  306]
train() client id: f_00004-0-8 loss: 0.891405  [  288/  306]
train() client id: f_00004-1-0 loss: 0.777145  [   32/  306]
train() client id: f_00004-1-1 loss: 0.826845  [   64/  306]
train() client id: f_00004-1-2 loss: 0.745350  [   96/  306]
train() client id: f_00004-1-3 loss: 0.840598  [  128/  306]
train() client id: f_00004-1-4 loss: 0.671743  [  160/  306]
train() client id: f_00004-1-5 loss: 0.811428  [  192/  306]
train() client id: f_00004-1-6 loss: 0.864422  [  224/  306]
train() client id: f_00004-1-7 loss: 0.821248  [  256/  306]
train() client id: f_00004-1-8 loss: 0.838087  [  288/  306]
train() client id: f_00004-2-0 loss: 0.692168  [   32/  306]
train() client id: f_00004-2-1 loss: 0.840022  [   64/  306]
train() client id: f_00004-2-2 loss: 0.915937  [   96/  306]
train() client id: f_00004-2-3 loss: 0.792481  [  128/  306]
train() client id: f_00004-2-4 loss: 0.696930  [  160/  306]
train() client id: f_00004-2-5 loss: 0.732957  [  192/  306]
train() client id: f_00004-2-6 loss: 0.876853  [  224/  306]
train() client id: f_00004-2-7 loss: 0.819992  [  256/  306]
train() client id: f_00004-2-8 loss: 0.792299  [  288/  306]
train() client id: f_00004-3-0 loss: 0.773585  [   32/  306]
train() client id: f_00004-3-1 loss: 0.804351  [   64/  306]
train() client id: f_00004-3-2 loss: 0.909392  [   96/  306]
train() client id: f_00004-3-3 loss: 0.823903  [  128/  306]
train() client id: f_00004-3-4 loss: 0.753994  [  160/  306]
train() client id: f_00004-3-5 loss: 0.800368  [  192/  306]
train() client id: f_00004-3-6 loss: 0.824761  [  224/  306]
train() client id: f_00004-3-7 loss: 0.709513  [  256/  306]
train() client id: f_00004-3-8 loss: 0.724903  [  288/  306]
train() client id: f_00004-4-0 loss: 0.873420  [   32/  306]
train() client id: f_00004-4-1 loss: 0.788092  [   64/  306]
train() client id: f_00004-4-2 loss: 0.699879  [   96/  306]
train() client id: f_00004-4-3 loss: 0.850121  [  128/  306]
train() client id: f_00004-4-4 loss: 0.857421  [  160/  306]
train() client id: f_00004-4-5 loss: 0.785939  [  192/  306]
train() client id: f_00004-4-6 loss: 0.766157  [  224/  306]
train() client id: f_00004-4-7 loss: 0.812432  [  256/  306]
train() client id: f_00004-4-8 loss: 0.697799  [  288/  306]
train() client id: f_00004-5-0 loss: 0.772968  [   32/  306]
train() client id: f_00004-5-1 loss: 0.785621  [   64/  306]
train() client id: f_00004-5-2 loss: 0.828732  [   96/  306]
train() client id: f_00004-5-3 loss: 0.707706  [  128/  306]
train() client id: f_00004-5-4 loss: 0.848236  [  160/  306]
train() client id: f_00004-5-5 loss: 0.828437  [  192/  306]
train() client id: f_00004-5-6 loss: 0.761170  [  224/  306]
train() client id: f_00004-5-7 loss: 0.777691  [  256/  306]
train() client id: f_00004-5-8 loss: 0.850246  [  288/  306]
train() client id: f_00004-6-0 loss: 0.795677  [   32/  306]
train() client id: f_00004-6-1 loss: 0.863735  [   64/  306]
train() client id: f_00004-6-2 loss: 0.808951  [   96/  306]
train() client id: f_00004-6-3 loss: 0.782460  [  128/  306]
train() client id: f_00004-6-4 loss: 0.931425  [  160/  306]
train() client id: f_00004-6-5 loss: 0.865228  [  192/  306]
train() client id: f_00004-6-6 loss: 0.706973  [  224/  306]
train() client id: f_00004-6-7 loss: 0.717957  [  256/  306]
train() client id: f_00004-6-8 loss: 0.801039  [  288/  306]
train() client id: f_00004-7-0 loss: 0.880283  [   32/  306]
train() client id: f_00004-7-1 loss: 0.671757  [   64/  306]
train() client id: f_00004-7-2 loss: 0.742927  [   96/  306]
train() client id: f_00004-7-3 loss: 0.831343  [  128/  306]
train() client id: f_00004-7-4 loss: 0.795008  [  160/  306]
train() client id: f_00004-7-5 loss: 0.817090  [  192/  306]
train() client id: f_00004-7-6 loss: 0.849274  [  224/  306]
train() client id: f_00004-7-7 loss: 0.870404  [  256/  306]
train() client id: f_00004-7-8 loss: 0.671020  [  288/  306]
train() client id: f_00005-0-0 loss: 0.568941  [   32/  146]
train() client id: f_00005-0-1 loss: 0.439290  [   64/  146]
train() client id: f_00005-0-2 loss: 0.444815  [   96/  146]
train() client id: f_00005-0-3 loss: 0.245521  [  128/  146]
train() client id: f_00005-1-0 loss: 0.648879  [   32/  146]
train() client id: f_00005-1-1 loss: 0.546359  [   64/  146]
train() client id: f_00005-1-2 loss: 0.229657  [   96/  146]
train() client id: f_00005-1-3 loss: 0.217632  [  128/  146]
train() client id: f_00005-2-0 loss: 0.423960  [   32/  146]
train() client id: f_00005-2-1 loss: 0.475842  [   64/  146]
train() client id: f_00005-2-2 loss: 0.281009  [   96/  146]
train() client id: f_00005-2-3 loss: 0.522616  [  128/  146]
train() client id: f_00005-3-0 loss: 0.231771  [   32/  146]
train() client id: f_00005-3-1 loss: 0.386523  [   64/  146]
train() client id: f_00005-3-2 loss: 0.597582  [   96/  146]
train() client id: f_00005-3-3 loss: 0.427836  [  128/  146]
train() client id: f_00005-4-0 loss: 0.194433  [   32/  146]
train() client id: f_00005-4-1 loss: 0.338957  [   64/  146]
train() client id: f_00005-4-2 loss: 0.744798  [   96/  146]
train() client id: f_00005-4-3 loss: 0.320721  [  128/  146]
train() client id: f_00005-5-0 loss: 0.405894  [   32/  146]
train() client id: f_00005-5-1 loss: 0.519328  [   64/  146]
train() client id: f_00005-5-2 loss: 0.465142  [   96/  146]
train() client id: f_00005-5-3 loss: 0.270617  [  128/  146]
train() client id: f_00005-6-0 loss: 0.481330  [   32/  146]
train() client id: f_00005-6-1 loss: 0.296587  [   64/  146]
train() client id: f_00005-6-2 loss: 0.331262  [   96/  146]
train() client id: f_00005-6-3 loss: 0.526602  [  128/  146]
train() client id: f_00005-7-0 loss: 0.488739  [   32/  146]
train() client id: f_00005-7-1 loss: 0.316155  [   64/  146]
train() client id: f_00005-7-2 loss: 0.287621  [   96/  146]
train() client id: f_00005-7-3 loss: 0.708752  [  128/  146]
train() client id: f_00006-0-0 loss: 0.588406  [   32/   54]
train() client id: f_00006-1-0 loss: 0.511553  [   32/   54]
train() client id: f_00006-2-0 loss: 0.566753  [   32/   54]
train() client id: f_00006-3-0 loss: 0.518836  [   32/   54]
train() client id: f_00006-4-0 loss: 0.532929  [   32/   54]
train() client id: f_00006-5-0 loss: 0.588172  [   32/   54]
train() client id: f_00006-6-0 loss: 0.607265  [   32/   54]
train() client id: f_00006-7-0 loss: 0.602131  [   32/   54]
train() client id: f_00007-0-0 loss: 0.438808  [   32/  179]
train() client id: f_00007-0-1 loss: 0.563051  [   64/  179]
train() client id: f_00007-0-2 loss: 0.448321  [   96/  179]
train() client id: f_00007-0-3 loss: 0.521166  [  128/  179]
train() client id: f_00007-0-4 loss: 0.526928  [  160/  179]
train() client id: f_00007-1-0 loss: 0.689050  [   32/  179]
train() client id: f_00007-1-1 loss: 0.472667  [   64/  179]
train() client id: f_00007-1-2 loss: 0.488939  [   96/  179]
train() client id: f_00007-1-3 loss: 0.456214  [  128/  179]
train() client id: f_00007-1-4 loss: 0.491667  [  160/  179]
train() client id: f_00007-2-0 loss: 0.416674  [   32/  179]
train() client id: f_00007-2-1 loss: 0.669443  [   64/  179]
train() client id: f_00007-2-2 loss: 0.564081  [   96/  179]
train() client id: f_00007-2-3 loss: 0.492579  [  128/  179]
train() client id: f_00007-2-4 loss: 0.361048  [  160/  179]
train() client id: f_00007-3-0 loss: 0.389128  [   32/  179]
train() client id: f_00007-3-1 loss: 0.570664  [   64/  179]
train() client id: f_00007-3-2 loss: 0.783964  [   96/  179]
train() client id: f_00007-3-3 loss: 0.385087  [  128/  179]
train() client id: f_00007-3-4 loss: 0.385133  [  160/  179]
train() client id: f_00007-4-0 loss: 0.455275  [   32/  179]
train() client id: f_00007-4-1 loss: 0.355586  [   64/  179]
train() client id: f_00007-4-2 loss: 0.472860  [   96/  179]
train() client id: f_00007-4-3 loss: 0.412982  [  128/  179]
train() client id: f_00007-4-4 loss: 0.509199  [  160/  179]
train() client id: f_00007-5-0 loss: 0.358242  [   32/  179]
train() client id: f_00007-5-1 loss: 0.619670  [   64/  179]
train() client id: f_00007-5-2 loss: 0.448216  [   96/  179]
train() client id: f_00007-5-3 loss: 0.538281  [  128/  179]
train() client id: f_00007-5-4 loss: 0.531132  [  160/  179]
train() client id: f_00007-6-0 loss: 0.321409  [   32/  179]
train() client id: f_00007-6-1 loss: 0.520139  [   64/  179]
train() client id: f_00007-6-2 loss: 0.684727  [   96/  179]
train() client id: f_00007-6-3 loss: 0.361047  [  128/  179]
train() client id: f_00007-6-4 loss: 0.506268  [  160/  179]
train() client id: f_00007-7-0 loss: 0.647176  [   32/  179]
train() client id: f_00007-7-1 loss: 0.300066  [   64/  179]
train() client id: f_00007-7-2 loss: 0.426873  [   96/  179]
train() client id: f_00007-7-3 loss: 0.339131  [  128/  179]
train() client id: f_00007-7-4 loss: 0.654775  [  160/  179]
train() client id: f_00008-0-0 loss: 0.736373  [   32/  130]
train() client id: f_00008-0-1 loss: 0.597478  [   64/  130]
train() client id: f_00008-0-2 loss: 0.734857  [   96/  130]
train() client id: f_00008-0-3 loss: 0.749531  [  128/  130]
train() client id: f_00008-1-0 loss: 0.719222  [   32/  130]
train() client id: f_00008-1-1 loss: 0.676521  [   64/  130]
train() client id: f_00008-1-2 loss: 0.733644  [   96/  130]
train() client id: f_00008-1-3 loss: 0.739865  [  128/  130]
train() client id: f_00008-2-0 loss: 0.718338  [   32/  130]
train() client id: f_00008-2-1 loss: 0.779625  [   64/  130]
train() client id: f_00008-2-2 loss: 0.696525  [   96/  130]
train() client id: f_00008-2-3 loss: 0.677559  [  128/  130]
train() client id: f_00008-3-0 loss: 0.739745  [   32/  130]
train() client id: f_00008-3-1 loss: 0.667736  [   64/  130]
train() client id: f_00008-3-2 loss: 0.636101  [   96/  130]
train() client id: f_00008-3-3 loss: 0.806722  [  128/  130]
train() client id: f_00008-4-0 loss: 0.774866  [   32/  130]
train() client id: f_00008-4-1 loss: 0.733415  [   64/  130]
train() client id: f_00008-4-2 loss: 0.632892  [   96/  130]
train() client id: f_00008-4-3 loss: 0.708552  [  128/  130]
train() client id: f_00008-5-0 loss: 0.650890  [   32/  130]
train() client id: f_00008-5-1 loss: 0.775546  [   64/  130]
train() client id: f_00008-5-2 loss: 0.692595  [   96/  130]
train() client id: f_00008-5-3 loss: 0.754988  [  128/  130]
train() client id: f_00008-6-0 loss: 0.742120  [   32/  130]
train() client id: f_00008-6-1 loss: 0.673313  [   64/  130]
train() client id: f_00008-6-2 loss: 0.783821  [   96/  130]
train() client id: f_00008-6-3 loss: 0.633116  [  128/  130]
train() client id: f_00008-7-0 loss: 0.808993  [   32/  130]
train() client id: f_00008-7-1 loss: 0.680994  [   64/  130]
train() client id: f_00008-7-2 loss: 0.627771  [   96/  130]
train() client id: f_00008-7-3 loss: 0.736257  [  128/  130]
train() client id: f_00009-0-0 loss: 1.078831  [   32/  118]
train() client id: f_00009-0-1 loss: 1.002740  [   64/  118]
train() client id: f_00009-0-2 loss: 1.070613  [   96/  118]
train() client id: f_00009-1-0 loss: 0.943011  [   32/  118]
train() client id: f_00009-1-1 loss: 1.016919  [   64/  118]
train() client id: f_00009-1-2 loss: 1.093441  [   96/  118]
train() client id: f_00009-2-0 loss: 0.991493  [   32/  118]
train() client id: f_00009-2-1 loss: 0.935792  [   64/  118]
train() client id: f_00009-2-2 loss: 0.934302  [   96/  118]
train() client id: f_00009-3-0 loss: 0.916847  [   32/  118]
train() client id: f_00009-3-1 loss: 0.973173  [   64/  118]
train() client id: f_00009-3-2 loss: 0.877710  [   96/  118]
train() client id: f_00009-4-0 loss: 0.943203  [   32/  118]
train() client id: f_00009-4-1 loss: 1.099276  [   64/  118]
train() client id: f_00009-4-2 loss: 0.808104  [   96/  118]
train() client id: f_00009-5-0 loss: 0.913904  [   32/  118]
train() client id: f_00009-5-1 loss: 1.043464  [   64/  118]
train() client id: f_00009-5-2 loss: 0.827483  [   96/  118]
train() client id: f_00009-6-0 loss: 0.937129  [   32/  118]
train() client id: f_00009-6-1 loss: 0.787553  [   64/  118]
train() client id: f_00009-6-2 loss: 0.896163  [   96/  118]
train() client id: f_00009-7-0 loss: 0.781080  [   32/  118]
train() client id: f_00009-7-1 loss: 0.811451  [   64/  118]
train() client id: f_00009-7-2 loss: 0.877598  [   96/  118]
At round 57 accuracy: 0.6472148541114059
At round 57 training accuracy: 0.5922199865861838
At round 57 training loss: 0.8181712387555663
update_location
xs = -4.528292 186.001589 195.045120 -195.943528 84.896481 -120.217951 -237.215960 258.375741 -1.680116 179.695607 
ys = 272.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -202.154970 4.001482 
xs mean: 34.442869159412865
ys mean: 9.371751218646875
dists_uav = 290.387157 211.751211 219.190198 221.138272 131.178637 156.397810 257.451309 277.715332 225.542578 205.685496 
uav_gains = -114.907326 -108.435863 -108.934590 -109.068957 -102.947320 -104.863633 -111.942115 -113.772789 -109.379375 -108.042810 
uav_gains_db_mean: -109.22947764772849
dists_bs = 196.428137 394.644889 408.776124 153.887011 313.686209 180.690452 188.677803 474.888927 415.073070 393.763212 
bs_gains = -103.776763 -112.260841 -112.688654 -100.808714 -109.468931 -102.761245 -103.287242 -114.511636 -112.874547 -112.233643 
bs_gains_db_mean: -108.46722151656007
Round 58
-------------------------------
ene_coms = [0.01578837 0.01260154 0.00996383 0.01005005 0.01031526 0.00711678
 0.01229493 0.01426478 0.01323257 0.01257484]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.3242639  6.81845448 3.22468272 1.17362649 7.83780765 3.75804422
 1.47111787 4.65399041 3.40412044 3.09462867]
obj_prev = 38.760736856032466
eta_min = 4.3847363664514355e-28	eta_max = 0.9512405739949283
af = 8.119195852514759	bf = 0.9828508132730173	zeta = 8.931115437766236	eta = 0.909090909090909
af = 8.119195852514759	bf = 0.9828508132730173	zeta = 19.32362112826998	eta = 0.42016948058646086
af = 8.119195852514759	bf = 0.9828508132730173	zeta = 13.836771119653044	eta = 0.5867839962303537
af = 8.119195852514759	bf = 0.9828508132730173	zeta = 12.853282409295574	eta = 0.6316826779315847
af = 8.119195852514759	bf = 0.9828508132730173	zeta = 12.794810686307779	eta = 0.6345694400311389
af = 8.119195852514759	bf = 0.9828508132730173	zeta = 12.794580085160508	eta = 0.6345808770958897
eta = 0.6345808770958897
ene_coms = [0.01578837 0.01260154 0.00996383 0.01005005 0.01031526 0.00711678
 0.01229493 0.01426478 0.01323257 0.01257484]
ene_comp = [0.03752822 0.07892841 0.03693254 0.01280725 0.09113997 0.04348509
 0.01608352 0.05331391 0.0387196  0.03514548]
ene_total = [1.21319451 2.08272184 1.06710519 0.52010742 2.30856701 1.15142206
 0.64573855 1.53772196 1.18214773 1.08585381]
ti_comp = [1.03307678 1.06494506 1.0913222  1.09045994 1.08780791 1.11979271
 1.06801122 1.04831263 1.05863472 1.06521211]
ti_coms = [0.15788369 0.12601541 0.09963827 0.10050053 0.10315256 0.07116776
 0.12294925 0.14264784 0.13232575 0.12574836]
t_total = [27.09975662 27.09975662 27.09975662 27.09975662 27.09975662 27.09975662
 27.09975662 27.09975662 27.09975662 27.09975662]
ene_coms = [0.01578837 0.01260154 0.00996383 0.01005005 0.01031526 0.00711678
 0.01229493 0.01426478 0.01323257 0.01257484]
ene_comp = [3.09520115e-06 2.70972777e-05 2.64363518e-06 1.10415077e-07
 3.99854787e-05 4.09850760e-06 2.27967058e-07 8.61826616e-06
 3.23727978e-06 2.39120209e-06]
ene_total = [0.35932748 0.28735885 0.22678245 0.22868683 0.23562875 0.16203223
 0.27977054 0.32478469 0.3011748  0.28618902]
optimize_network iter = 0 obj = 2.69173563304271
eta = 0.6345808770958897
freqs = [18163327.94064038 37057502.9402291  16921005.87722573  5872407.9817862
 41891575.52346818 19416578.46213752  7529661.02481795 25428438.56691303
 18287515.13143767 16496939.98047107]
eta_min = 0.6345808770958904	eta_max = 0.7633236623549908
af = 0.0016729918196986533	bf = 0.9828508132730173	zeta = 0.0018402910016685188	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01578837 0.01260154 0.00996383 0.01005005 0.01031526 0.00711678
 0.01229493 0.01426478 0.01323257 0.01257484]
ene_comp = [6.48659452e-07 5.67876026e-06 5.54025041e-07 2.31396216e-08
 8.37973283e-06 8.58921783e-07 4.77749199e-08 1.80612488e-06
 6.78434784e-07 5.01122789e-07]
ene_total = [1.4441327  1.15311156 0.91138595 0.91922399 0.94424494 0.65101011
 1.12455208 1.30488483 1.21037131 1.15019546]
ti_comp = [0.61348307 0.64535134 0.67172848 0.67086622 0.66821419 0.70019899
 0.6484175  0.62871891 0.63904101 0.64561839]
ti_coms = [0.15788369 0.12601541 0.09963827 0.10050053 0.10315256 0.07116776
 0.12294925 0.14264784 0.13232575 0.12574836]
t_total = [27.09975662 27.09975662 27.09975662 27.09975662 27.09975662 27.09975662
 27.09975662 27.09975662 27.09975662 27.09975662]
ene_coms = [0.01578837 0.01260154 0.00996383 0.01005005 0.01031526 0.00711678
 0.01229493 0.01426478 0.01323257 0.01257484]
ene_comp = [1.83805650e-06 1.54524578e-05 1.46126812e-06 6.10922180e-08
 2.21914289e-05 2.19516914e-06 1.29516232e-07 5.01762756e-06
 1.86047991e-06 1.36315932e-06]
ene_total = [0.55474359 0.44326185 0.3501018  0.35308191 0.36317654 0.25010463
 0.43195145 0.50132852 0.46495389 0.44182867]
optimize_network iter = 1 obj = 4.1545328393671195
eta = 0.7633236623549908
freqs = [18163327.94064039 36314253.01261663 16325082.33941737  5668393.44611089
 40497963.8667432  18439911.60897475  7364893.80359871 25178158.55722818
 17990452.39063981 16163431.75548961]
eta_min = 0.7633236623549942	eta_max = 0.7633236623549762
af = 0.0015877320246774668	bf = 0.9828508132730173	zeta = 0.0017465052271452136	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01578837 0.01260154 0.00996383 0.01005005 0.01031526 0.00711678
 0.01229493 0.01426478 0.01323257 0.01257484]
ene_comp = [6.48659452e-07 5.45325067e-06 5.15688923e-07 2.15597533e-08
 7.83146769e-06 7.74686313e-07 4.57069345e-08 1.77074619e-06
 6.56572786e-07 4.81065832e-07]
ene_total = [1.4441327  1.15309094 0.91138244 0.91922384 0.9441948  0.6510024
 1.12455189 1.30488159 1.21036931 1.15019363]
ti_comp = [0.61348307 0.64535134 0.67172848 0.67086622 0.66821419 0.70019899
 0.6484175  0.62871891 0.63904101 0.64561839]
ti_coms = [0.15788369 0.12601541 0.09963827 0.10050053 0.10315256 0.07116776
 0.12294925 0.14264784 0.13232575 0.12574836]
t_total = [27.09975662 27.09975662 27.09975662 27.09975662 27.09975662 27.09975662
 27.09975662 27.09975662 27.09975662 27.09975662]
ene_coms = [0.01578837 0.01260154 0.00996383 0.01005005 0.01031526 0.00711678
 0.01229493 0.01426478 0.01323257 0.01257484]
ene_comp = [1.83805650e-06 1.54524578e-05 1.46126812e-06 6.10922180e-08
 2.21914289e-05 2.19516914e-06 1.29516232e-07 5.01762756e-06
 1.86047991e-06 1.36315932e-06]
ene_total = [0.55474359 0.44326185 0.3501018  0.35308191 0.36317654 0.25010463
 0.43195145 0.50132852 0.46495389 0.44182867]
optimize_network iter = 2 obj = 4.154532839366863
eta = 0.7633236623549762
freqs = [18163327.94064027 36314253.01261654 16325082.33941738  5668393.44611089
 40497963.8667432  18439911.60897482  7364893.8035987  25178158.55722807
 17990452.39063975 16163431.75548957]
Done!
ene_coms = [0.01578837 0.01260154 0.00996383 0.01005005 0.01031526 0.00711678
 0.01229493 0.01426478 0.01323257 0.01257484]
ene_comp = [1.66272867e-06 1.39784847e-05 1.32188123e-06 5.52647769e-08
 2.00746413e-05 1.98577719e-06 1.17161987e-07 4.53900803e-06
 1.68301316e-06 1.23313080e-06]
ene_total = [0.01579003 0.01261552 0.00996515 0.01005011 0.01033533 0.00711876
 0.01229504 0.01426932 0.01323426 0.01257607]
At round 58 energy consumption: 0.11824959328216919
At round 58 eta: 0.7633236623549762
At round 58 a_n: 8.314943732033974
At round 58 local rounds: 8.843566758896356
At round 58 global rounds: 35.13212945057923
gradient difference: 0.794417679309845
train() client id: f_00000-0-0 loss: 0.988810  [   32/  126]
train() client id: f_00000-0-1 loss: 1.001137  [   64/  126]
train() client id: f_00000-0-2 loss: 0.962273  [   96/  126]
train() client id: f_00000-1-0 loss: 0.876444  [   32/  126]
train() client id: f_00000-1-1 loss: 0.996445  [   64/  126]
train() client id: f_00000-1-2 loss: 0.941092  [   96/  126]
train() client id: f_00000-2-0 loss: 1.042925  [   32/  126]
train() client id: f_00000-2-1 loss: 0.904665  [   64/  126]
train() client id: f_00000-2-2 loss: 0.978086  [   96/  126]
train() client id: f_00000-3-0 loss: 1.085932  [   32/  126]
train() client id: f_00000-3-1 loss: 0.930038  [   64/  126]
train() client id: f_00000-3-2 loss: 0.872743  [   96/  126]
train() client id: f_00000-4-0 loss: 0.911511  [   32/  126]
train() client id: f_00000-4-1 loss: 1.025902  [   64/  126]
train() client id: f_00000-4-2 loss: 0.978273  [   96/  126]
train() client id: f_00000-5-0 loss: 0.879064  [   32/  126]
train() client id: f_00000-5-1 loss: 0.840502  [   64/  126]
train() client id: f_00000-5-2 loss: 1.197809  [   96/  126]
train() client id: f_00000-6-0 loss: 1.055082  [   32/  126]
train() client id: f_00000-6-1 loss: 0.922686  [   64/  126]
train() client id: f_00000-6-2 loss: 0.880218  [   96/  126]
train() client id: f_00000-7-0 loss: 0.973108  [   32/  126]
train() client id: f_00000-7-1 loss: 1.019078  [   64/  126]
train() client id: f_00000-7-2 loss: 0.999937  [   96/  126]
train() client id: f_00001-0-0 loss: 0.743068  [   32/  265]
train() client id: f_00001-0-1 loss: 0.391528  [   64/  265]
train() client id: f_00001-0-2 loss: 0.437362  [   96/  265]
train() client id: f_00001-0-3 loss: 0.357128  [  128/  265]
train() client id: f_00001-0-4 loss: 0.539127  [  160/  265]
train() client id: f_00001-0-5 loss: 0.413831  [  192/  265]
train() client id: f_00001-0-6 loss: 0.380555  [  224/  265]
train() client id: f_00001-0-7 loss: 0.475513  [  256/  265]
train() client id: f_00001-1-0 loss: 0.351114  [   32/  265]
train() client id: f_00001-1-1 loss: 0.434867  [   64/  265]
train() client id: f_00001-1-2 loss: 0.614692  [   96/  265]
train() client id: f_00001-1-3 loss: 0.516097  [  128/  265]
train() client id: f_00001-1-4 loss: 0.437254  [  160/  265]
train() client id: f_00001-1-5 loss: 0.406846  [  192/  265]
train() client id: f_00001-1-6 loss: 0.492371  [  224/  265]
train() client id: f_00001-1-7 loss: 0.456017  [  256/  265]
train() client id: f_00001-2-0 loss: 0.408570  [   32/  265]
train() client id: f_00001-2-1 loss: 0.435801  [   64/  265]
train() client id: f_00001-2-2 loss: 0.417697  [   96/  265]
train() client id: f_00001-2-3 loss: 0.409035  [  128/  265]
train() client id: f_00001-2-4 loss: 0.513585  [  160/  265]
train() client id: f_00001-2-5 loss: 0.497145  [  192/  265]
train() client id: f_00001-2-6 loss: 0.544281  [  224/  265]
train() client id: f_00001-2-7 loss: 0.441646  [  256/  265]
train() client id: f_00001-3-0 loss: 0.466442  [   32/  265]
train() client id: f_00001-3-1 loss: 0.429776  [   64/  265]
train() client id: f_00001-3-2 loss: 0.377628  [   96/  265]
train() client id: f_00001-3-3 loss: 0.388013  [  128/  265]
train() client id: f_00001-3-4 loss: 0.482425  [  160/  265]
train() client id: f_00001-3-5 loss: 0.538558  [  192/  265]
train() client id: f_00001-3-6 loss: 0.451933  [  224/  265]
train() client id: f_00001-3-7 loss: 0.503226  [  256/  265]
train() client id: f_00001-4-0 loss: 0.509278  [   32/  265]
train() client id: f_00001-4-1 loss: 0.373964  [   64/  265]
train() client id: f_00001-4-2 loss: 0.424342  [   96/  265]
train() client id: f_00001-4-3 loss: 0.391460  [  128/  265]
train() client id: f_00001-4-4 loss: 0.382404  [  160/  265]
train() client id: f_00001-4-5 loss: 0.385456  [  192/  265]
train() client id: f_00001-4-6 loss: 0.530519  [  224/  265]
train() client id: f_00001-4-7 loss: 0.564483  [  256/  265]
train() client id: f_00001-5-0 loss: 0.406203  [   32/  265]
train() client id: f_00001-5-1 loss: 0.418655  [   64/  265]
train() client id: f_00001-5-2 loss: 0.547776  [   96/  265]
train() client id: f_00001-5-3 loss: 0.406442  [  128/  265]
train() client id: f_00001-5-4 loss: 0.446881  [  160/  265]
train() client id: f_00001-5-5 loss: 0.457988  [  192/  265]
train() client id: f_00001-5-6 loss: 0.525898  [  224/  265]
train() client id: f_00001-5-7 loss: 0.393657  [  256/  265]
train() client id: f_00001-6-0 loss: 0.415863  [   32/  265]
train() client id: f_00001-6-1 loss: 0.468423  [   64/  265]
train() client id: f_00001-6-2 loss: 0.374499  [   96/  265]
train() client id: f_00001-6-3 loss: 0.444515  [  128/  265]
train() client id: f_00001-6-4 loss: 0.453537  [  160/  265]
train() client id: f_00001-6-5 loss: 0.490534  [  192/  265]
train() client id: f_00001-6-6 loss: 0.529518  [  224/  265]
train() client id: f_00001-6-7 loss: 0.380258  [  256/  265]
train() client id: f_00001-7-0 loss: 0.383837  [   32/  265]
train() client id: f_00001-7-1 loss: 0.453027  [   64/  265]
train() client id: f_00001-7-2 loss: 0.423106  [   96/  265]
train() client id: f_00001-7-3 loss: 0.465646  [  128/  265]
train() client id: f_00001-7-4 loss: 0.344880  [  160/  265]
train() client id: f_00001-7-5 loss: 0.447020  [  192/  265]
train() client id: f_00001-7-6 loss: 0.400660  [  224/  265]
train() client id: f_00001-7-7 loss: 0.594561  [  256/  265]
train() client id: f_00002-0-0 loss: 1.281215  [   32/  124]
train() client id: f_00002-0-1 loss: 1.255676  [   64/  124]
train() client id: f_00002-0-2 loss: 1.262308  [   96/  124]
train() client id: f_00002-1-0 loss: 1.260173  [   32/  124]
train() client id: f_00002-1-1 loss: 1.208675  [   64/  124]
train() client id: f_00002-1-2 loss: 1.358424  [   96/  124]
train() client id: f_00002-2-0 loss: 1.268242  [   32/  124]
train() client id: f_00002-2-1 loss: 1.113961  [   64/  124]
train() client id: f_00002-2-2 loss: 1.247126  [   96/  124]
train() client id: f_00002-3-0 loss: 1.048487  [   32/  124]
train() client id: f_00002-3-1 loss: 1.207997  [   64/  124]
train() client id: f_00002-3-2 loss: 1.204721  [   96/  124]
train() client id: f_00002-4-0 loss: 1.100488  [   32/  124]
train() client id: f_00002-4-1 loss: 1.037773  [   64/  124]
train() client id: f_00002-4-2 loss: 1.271108  [   96/  124]
train() client id: f_00002-5-0 loss: 1.171750  [   32/  124]
train() client id: f_00002-5-1 loss: 1.036773  [   64/  124]
train() client id: f_00002-5-2 loss: 1.328562  [   96/  124]
train() client id: f_00002-6-0 loss: 1.290264  [   32/  124]
train() client id: f_00002-6-1 loss: 1.260939  [   64/  124]
train() client id: f_00002-6-2 loss: 1.121433  [   96/  124]
train() client id: f_00002-7-0 loss: 1.221535  [   32/  124]
train() client id: f_00002-7-1 loss: 1.093935  [   64/  124]
train() client id: f_00002-7-2 loss: 1.053926  [   96/  124]
train() client id: f_00003-0-0 loss: 0.692545  [   32/   43]
train() client id: f_00003-1-0 loss: 0.793277  [   32/   43]
train() client id: f_00003-2-0 loss: 0.844293  [   32/   43]
train() client id: f_00003-3-0 loss: 0.774906  [   32/   43]
train() client id: f_00003-4-0 loss: 0.764289  [   32/   43]
train() client id: f_00003-5-0 loss: 0.787783  [   32/   43]
train() client id: f_00003-6-0 loss: 0.867333  [   32/   43]
train() client id: f_00003-7-0 loss: 0.740494  [   32/   43]
train() client id: f_00004-0-0 loss: 1.026747  [   32/  306]
train() client id: f_00004-0-1 loss: 0.983816  [   64/  306]
train() client id: f_00004-0-2 loss: 0.925710  [   96/  306]
train() client id: f_00004-0-3 loss: 0.974538  [  128/  306]
train() client id: f_00004-0-4 loss: 1.034824  [  160/  306]
train() client id: f_00004-0-5 loss: 0.999508  [  192/  306]
train() client id: f_00004-0-6 loss: 1.008931  [  224/  306]
train() client id: f_00004-0-7 loss: 1.031283  [  256/  306]
train() client id: f_00004-0-8 loss: 1.155388  [  288/  306]
train() client id: f_00004-1-0 loss: 1.068194  [   32/  306]
train() client id: f_00004-1-1 loss: 1.098513  [   64/  306]
train() client id: f_00004-1-2 loss: 0.986958  [   96/  306]
train() client id: f_00004-1-3 loss: 0.944509  [  128/  306]
train() client id: f_00004-1-4 loss: 0.989259  [  160/  306]
train() client id: f_00004-1-5 loss: 1.103447  [  192/  306]
train() client id: f_00004-1-6 loss: 0.908901  [  224/  306]
train() client id: f_00004-1-7 loss: 0.991306  [  256/  306]
train() client id: f_00004-1-8 loss: 0.958075  [  288/  306]
train() client id: f_00004-2-0 loss: 0.933368  [   32/  306]
train() client id: f_00004-2-1 loss: 0.907506  [   64/  306]
train() client id: f_00004-2-2 loss: 1.014229  [   96/  306]
train() client id: f_00004-2-3 loss: 1.017334  [  128/  306]
train() client id: f_00004-2-4 loss: 1.016728  [  160/  306]
train() client id: f_00004-2-5 loss: 1.193284  [  192/  306]
train() client id: f_00004-2-6 loss: 0.965400  [  224/  306]
train() client id: f_00004-2-7 loss: 0.855319  [  256/  306]
train() client id: f_00004-2-8 loss: 1.134838  [  288/  306]
train() client id: f_00004-3-0 loss: 1.101078  [   32/  306]
train() client id: f_00004-3-1 loss: 1.042747  [   64/  306]
train() client id: f_00004-3-2 loss: 0.898975  [   96/  306]
train() client id: f_00004-3-3 loss: 1.269753  [  128/  306]
train() client id: f_00004-3-4 loss: 1.070096  [  160/  306]
train() client id: f_00004-3-5 loss: 0.883674  [  192/  306]
train() client id: f_00004-3-6 loss: 0.882722  [  224/  306]
train() client id: f_00004-3-7 loss: 1.016622  [  256/  306]
train() client id: f_00004-3-8 loss: 0.879333  [  288/  306]
train() client id: f_00004-4-0 loss: 0.935350  [   32/  306]
train() client id: f_00004-4-1 loss: 0.925713  [   64/  306]
train() client id: f_00004-4-2 loss: 0.986558  [   96/  306]
train() client id: f_00004-4-3 loss: 1.045230  [  128/  306]
train() client id: f_00004-4-4 loss: 1.009123  [  160/  306]
train() client id: f_00004-4-5 loss: 1.018103  [  192/  306]
train() client id: f_00004-4-6 loss: 1.066304  [  224/  306]
train() client id: f_00004-4-7 loss: 0.997602  [  256/  306]
train() client id: f_00004-4-8 loss: 1.031414  [  288/  306]
train() client id: f_00004-5-0 loss: 1.035174  [   32/  306]
train() client id: f_00004-5-1 loss: 1.132079  [   64/  306]
train() client id: f_00004-5-2 loss: 1.035297  [   96/  306]
train() client id: f_00004-5-3 loss: 0.988658  [  128/  306]
train() client id: f_00004-5-4 loss: 0.999776  [  160/  306]
train() client id: f_00004-5-5 loss: 1.002918  [  192/  306]
train() client id: f_00004-5-6 loss: 0.935522  [  224/  306]
train() client id: f_00004-5-7 loss: 0.872975  [  256/  306]
train() client id: f_00004-5-8 loss: 0.988771  [  288/  306]
train() client id: f_00004-6-0 loss: 1.055405  [   32/  306]
train() client id: f_00004-6-1 loss: 0.978540  [   64/  306]
train() client id: f_00004-6-2 loss: 0.995770  [   96/  306]
train() client id: f_00004-6-3 loss: 0.850712  [  128/  306]
train() client id: f_00004-6-4 loss: 1.014903  [  160/  306]
train() client id: f_00004-6-5 loss: 0.966748  [  192/  306]
train() client id: f_00004-6-6 loss: 0.984805  [  224/  306]
train() client id: f_00004-6-7 loss: 1.044886  [  256/  306]
train() client id: f_00004-6-8 loss: 1.142196  [  288/  306]
train() client id: f_00004-7-0 loss: 0.994394  [   32/  306]
train() client id: f_00004-7-1 loss: 0.921278  [   64/  306]
train() client id: f_00004-7-2 loss: 1.089872  [   96/  306]
train() client id: f_00004-7-3 loss: 1.005183  [  128/  306]
train() client id: f_00004-7-4 loss: 1.023972  [  160/  306]
train() client id: f_00004-7-5 loss: 0.994348  [  192/  306]
train() client id: f_00004-7-6 loss: 1.149140  [  224/  306]
train() client id: f_00004-7-7 loss: 0.895570  [  256/  306]
train() client id: f_00004-7-8 loss: 0.912095  [  288/  306]
train() client id: f_00005-0-0 loss: 0.524901  [   32/  146]
train() client id: f_00005-0-1 loss: 0.567911  [   64/  146]
train() client id: f_00005-0-2 loss: 0.297732  [   96/  146]
train() client id: f_00005-0-3 loss: 0.239372  [  128/  146]
train() client id: f_00005-1-0 loss: 0.483526  [   32/  146]
train() client id: f_00005-1-1 loss: 0.196018  [   64/  146]
train() client id: f_00005-1-2 loss: 0.434871  [   96/  146]
train() client id: f_00005-1-3 loss: 0.607530  [  128/  146]
train() client id: f_00005-2-0 loss: 0.285454  [   32/  146]
train() client id: f_00005-2-1 loss: 0.276184  [   64/  146]
train() client id: f_00005-2-2 loss: 0.694125  [   96/  146]
train() client id: f_00005-2-3 loss: 0.363301  [  128/  146]
train() client id: f_00005-3-0 loss: 0.421855  [   32/  146]
train() client id: f_00005-3-1 loss: 0.374162  [   64/  146]
train() client id: f_00005-3-2 loss: 0.247364  [   96/  146]
train() client id: f_00005-3-3 loss: 0.512782  [  128/  146]
train() client id: f_00005-4-0 loss: 0.315930  [   32/  146]
train() client id: f_00005-4-1 loss: 0.672129  [   64/  146]
train() client id: f_00005-4-2 loss: 0.375959  [   96/  146]
train() client id: f_00005-4-3 loss: 0.253831  [  128/  146]
train() client id: f_00005-5-0 loss: 0.307674  [   32/  146]
train() client id: f_00005-5-1 loss: 0.356763  [   64/  146]
train() client id: f_00005-5-2 loss: 0.514262  [   96/  146]
train() client id: f_00005-5-3 loss: 0.482435  [  128/  146]
train() client id: f_00005-6-0 loss: 0.426503  [   32/  146]
train() client id: f_00005-6-1 loss: 0.177744  [   64/  146]
train() client id: f_00005-6-2 loss: 0.196958  [   96/  146]
train() client id: f_00005-6-3 loss: 0.676467  [  128/  146]
train() client id: f_00005-7-0 loss: 0.485870  [   32/  146]
train() client id: f_00005-7-1 loss: 0.284236  [   64/  146]
train() client id: f_00005-7-2 loss: 0.349366  [   96/  146]
train() client id: f_00005-7-3 loss: 0.557025  [  128/  146]
train() client id: f_00006-0-0 loss: 0.437552  [   32/   54]
train() client id: f_00006-1-0 loss: 0.474327  [   32/   54]
train() client id: f_00006-2-0 loss: 0.441865  [   32/   54]
train() client id: f_00006-3-0 loss: 0.463467  [   32/   54]
train() client id: f_00006-4-0 loss: 0.437646  [   32/   54]
train() client id: f_00006-5-0 loss: 0.523215  [   32/   54]
train() client id: f_00006-6-0 loss: 0.487142  [   32/   54]
train() client id: f_00006-7-0 loss: 0.443246  [   32/   54]
train() client id: f_00007-0-0 loss: 0.437241  [   32/  179]
train() client id: f_00007-0-1 loss: 0.628270  [   64/  179]
train() client id: f_00007-0-2 loss: 0.402822  [   96/  179]
train() client id: f_00007-0-3 loss: 0.378950  [  128/  179]
train() client id: f_00007-0-4 loss: 0.516002  [  160/  179]
train() client id: f_00007-1-0 loss: 0.662872  [   32/  179]
train() client id: f_00007-1-1 loss: 0.416917  [   64/  179]
train() client id: f_00007-1-2 loss: 0.283703  [   96/  179]
train() client id: f_00007-1-3 loss: 0.495914  [  128/  179]
train() client id: f_00007-1-4 loss: 0.421732  [  160/  179]
train() client id: f_00007-2-0 loss: 0.405576  [   32/  179]
train() client id: f_00007-2-1 loss: 0.563647  [   64/  179]
train() client id: f_00007-2-2 loss: 0.371804  [   96/  179]
train() client id: f_00007-2-3 loss: 0.580962  [  128/  179]
train() client id: f_00007-2-4 loss: 0.372646  [  160/  179]
train() client id: f_00007-3-0 loss: 0.411720  [   32/  179]
train() client id: f_00007-3-1 loss: 0.288043  [   64/  179]
train() client id: f_00007-3-2 loss: 0.441049  [   96/  179]
train() client id: f_00007-3-3 loss: 0.559643  [  128/  179]
train() client id: f_00007-3-4 loss: 0.490693  [  160/  179]
train() client id: f_00007-4-0 loss: 0.446645  [   32/  179]
train() client id: f_00007-4-1 loss: 0.254359  [   64/  179]
train() client id: f_00007-4-2 loss: 0.466494  [   96/  179]
train() client id: f_00007-4-3 loss: 0.257689  [  128/  179]
train() client id: f_00007-4-4 loss: 0.249651  [  160/  179]
train() client id: f_00007-5-0 loss: 0.359441  [   32/  179]
train() client id: f_00007-5-1 loss: 0.398835  [   64/  179]
train() client id: f_00007-5-2 loss: 0.639286  [   96/  179]
train() client id: f_00007-5-3 loss: 0.524355  [  128/  179]
train() client id: f_00007-5-4 loss: 0.264200  [  160/  179]
train() client id: f_00007-6-0 loss: 0.459439  [   32/  179]
train() client id: f_00007-6-1 loss: 0.490162  [   64/  179]
train() client id: f_00007-6-2 loss: 0.256359  [   96/  179]
train() client id: f_00007-6-3 loss: 0.488160  [  128/  179]
train() client id: f_00007-6-4 loss: 0.438708  [  160/  179]
train() client id: f_00007-7-0 loss: 0.388070  [   32/  179]
train() client id: f_00007-7-1 loss: 0.257103  [   64/  179]
train() client id: f_00007-7-2 loss: 0.589851  [   96/  179]
train() client id: f_00007-7-3 loss: 0.347880  [  128/  179]
train() client id: f_00007-7-4 loss: 0.447370  [  160/  179]
train() client id: f_00008-0-0 loss: 0.597413  [   32/  130]
train() client id: f_00008-0-1 loss: 0.722718  [   64/  130]
train() client id: f_00008-0-2 loss: 0.815887  [   96/  130]
train() client id: f_00008-0-3 loss: 0.747233  [  128/  130]
train() client id: f_00008-1-0 loss: 0.591862  [   32/  130]
train() client id: f_00008-1-1 loss: 0.749377  [   64/  130]
train() client id: f_00008-1-2 loss: 0.661526  [   96/  130]
train() client id: f_00008-1-3 loss: 0.839532  [  128/  130]
train() client id: f_00008-2-0 loss: 0.656575  [   32/  130]
train() client id: f_00008-2-1 loss: 0.818421  [   64/  130]
train() client id: f_00008-2-2 loss: 0.645139  [   96/  130]
train() client id: f_00008-2-3 loss: 0.761320  [  128/  130]
train() client id: f_00008-3-0 loss: 0.775908  [   32/  130]
train() client id: f_00008-3-1 loss: 0.744825  [   64/  130]
train() client id: f_00008-3-2 loss: 0.591777  [   96/  130]
train() client id: f_00008-3-3 loss: 0.768395  [  128/  130]
train() client id: f_00008-4-0 loss: 0.907772  [   32/  130]
train() client id: f_00008-4-1 loss: 0.580354  [   64/  130]
train() client id: f_00008-4-2 loss: 0.728249  [   96/  130]
train() client id: f_00008-4-3 loss: 0.676626  [  128/  130]
train() client id: f_00008-5-0 loss: 0.654758  [   32/  130]
train() client id: f_00008-5-1 loss: 0.765197  [   64/  130]
train() client id: f_00008-5-2 loss: 0.710400  [   96/  130]
train() client id: f_00008-5-3 loss: 0.759256  [  128/  130]
train() client id: f_00008-6-0 loss: 0.709171  [   32/  130]
train() client id: f_00008-6-1 loss: 0.622154  [   64/  130]
train() client id: f_00008-6-2 loss: 0.767073  [   96/  130]
train() client id: f_00008-6-3 loss: 0.800633  [  128/  130]
train() client id: f_00008-7-0 loss: 0.767030  [   32/  130]
train() client id: f_00008-7-1 loss: 0.704285  [   64/  130]
train() client id: f_00008-7-2 loss: 0.800766  [   96/  130]
train() client id: f_00008-7-3 loss: 0.618313  [  128/  130]
train() client id: f_00009-0-0 loss: 0.867862  [   32/  118]
train() client id: f_00009-0-1 loss: 0.849914  [   64/  118]
train() client id: f_00009-0-2 loss: 0.837116  [   96/  118]
train() client id: f_00009-1-0 loss: 0.857621  [   32/  118]
train() client id: f_00009-1-1 loss: 0.737411  [   64/  118]
train() client id: f_00009-1-2 loss: 0.818358  [   96/  118]
train() client id: f_00009-2-0 loss: 0.794909  [   32/  118]
train() client id: f_00009-2-1 loss: 0.799044  [   64/  118]
train() client id: f_00009-2-2 loss: 0.882974  [   96/  118]
train() client id: f_00009-3-0 loss: 0.721966  [   32/  118]
train() client id: f_00009-3-1 loss: 0.656367  [   64/  118]
train() client id: f_00009-3-2 loss: 0.884215  [   96/  118]
train() client id: f_00009-4-0 loss: 0.821954  [   32/  118]
train() client id: f_00009-4-1 loss: 0.751208  [   64/  118]
train() client id: f_00009-4-2 loss: 0.809733  [   96/  118]
train() client id: f_00009-5-0 loss: 0.801879  [   32/  118]
train() client id: f_00009-5-1 loss: 0.694181  [   64/  118]
train() client id: f_00009-5-2 loss: 0.727954  [   96/  118]
train() client id: f_00009-6-0 loss: 0.840674  [   32/  118]
train() client id: f_00009-6-1 loss: 0.891123  [   64/  118]
train() client id: f_00009-6-2 loss: 0.742241  [   96/  118]
train() client id: f_00009-7-0 loss: 0.831204  [   32/  118]
train() client id: f_00009-7-1 loss: 0.798862  [   64/  118]
train() client id: f_00009-7-2 loss: 0.729194  [   96/  118]
At round 58 accuracy: 0.6472148541114059
At round 58 training accuracy: 0.5875251509054326
At round 58 training loss: 0.8351080660917082
update_location
xs = -4.528292 191.001589 200.045120 -200.943528 89.896481 -125.217951 -242.215960 263.375741 -1.680116 184.695607 
ys = 277.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -207.154970 4.001482 
xs mean: 35.442869159412865
ys mean: 9.371751218646875
dists_uav = 295.085718 216.156405 223.651054 225.580519 134.468582 160.273063 262.065519 282.373092 230.034789 210.067797 
uav_gains = -115.314822 -108.728610 -109.244889 -109.382091 -103.216564 -105.132303 -112.352680 -114.194293 -109.706267 -108.325734 
uav_gains_db_mean: -109.55982533339014
dists_bs = 198.959525 399.223752 413.307840 154.646847 317.841159 179.237884 190.385065 479.456203 419.621500 398.273051 
bs_gains = -103.932472 -112.401118 -112.822721 -100.868609 -109.628943 -102.663094 -103.396780 -114.628030 -113.007076 -112.372125 
bs_gains_db_mean: -108.57209661080921
Round 59
-------------------------------
ene_coms = [0.01640611 0.01274092 0.0101651  0.01025656 0.01042507 0.0070843
 0.01269248 0.01479961 0.01337636 0.01271189]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.19229074 6.53868106 3.09345822 1.1269403  7.51580245 3.60296475
 1.41371458 4.46656964 3.2650408  2.96824474]
obj_prev = 37.1837072920383
eta_min = 3.104090795844973e-29	eta_max = 0.9513428555825635
af = 7.784714115820054	bf = 0.9619368665027545	zeta = 8.56318552740206	eta = 0.909090909090909
af = 7.784714115820054	bf = 0.9619368665027545	zeta = 18.742893054675104	eta = 0.41534218293361524
af = 7.784714115820054	bf = 0.9619368665027545	zeta = 13.344550668203146	eta = 0.5833627755162378
af = 7.784714115820054	bf = 0.9619368665027545	zeta = 12.378821018472475	eta = 0.6288736305503733
af = 7.784714115820054	bf = 0.9619368665027545	zeta = 12.321090230326593	eta = 0.6318202342727025
af = 7.784714115820054	bf = 0.9619368665027545	zeta = 12.32085957575621	eta = 0.6318320623617899
eta = 0.6318320623617899
ene_coms = [0.01640611 0.01274092 0.0101651  0.01025656 0.01042507 0.0070843
 0.01269248 0.01479961 0.01337636 0.01271189]
ene_comp = [0.03788644 0.0796818  0.03728507 0.0129295  0.09200993 0.04390016
 0.01623705 0.0538228  0.03908919 0.03548095]
ene_total = [1.17566403 2.0013442  1.02749759 0.50207665 2.21815247 1.10402993
 0.62644702 1.48596647 1.13610173 1.0435795 ]
ti_comp = [1.08510534 1.12175716 1.14751536 1.14660077 1.14491567 1.17832341
 1.12224159 1.10117027 1.11540278 1.12204752]
ti_coms = [0.16406106 0.12740924 0.10165104 0.10256563 0.10425073 0.07084299
 0.12692481 0.14799613 0.13376362 0.12711888]
t_total = [27.04975243 27.04975243 27.04975243 27.04975243 27.04975243 27.04975243
 27.04975243 27.04975243 27.04975243 27.04975243]
ene_coms = [0.01640611 0.01274092 0.0101651  0.01025656 0.01042507 0.0070843
 0.01269248 0.01479961 0.01337636 0.01271189]
ene_comp = [2.88660731e-06 2.51281084e-05 2.46018583e-06 1.02754484e-07
 3.71395941e-05 3.80846026e-06 2.12436156e-07 8.03655668e-06
 3.00045063e-06 2.21739669e-06]
ene_total = [0.3553243  0.27643919 0.22017089 0.22210033 0.22655128 0.15348759
 0.27485066 0.32064844 0.28971996 0.27531432]
optimize_network iter = 0 obj = 2.6146069837268784
eta = 0.6318320623617899
freqs = [17457495.29787459 35516512.51993179 16246000.64890076  5638187.5346617
 40181969.16888655 18628232.04491844  7234202.98301138 24438910.64146075
 17522453.61179685 15810807.35160415]
eta_min = 0.6318320623617905	eta_max = 0.7655707229189843
af = 0.001475817314939502	bf = 0.9619368665027545	zeta = 0.0016233990464334523	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01640611 0.01274092 0.0101651  0.01025656 0.01042507 0.0070843
 0.01269248 0.01479961 0.01337636 0.01271189]
ene_comp = [5.99224787e-07 5.21629158e-06 5.10704841e-07 2.13305890e-08
 7.70973083e-06 7.90590317e-07 4.40991783e-08 1.66829203e-06
 6.22857283e-07 4.60304752e-07]
ene_total = [1.43880858 1.11779033 0.89148754 0.89946529 0.91491722 0.62133667
 1.11308846 1.29801874 1.17311308 1.11482688]
ti_comp = [0.63134002 0.66799184 0.69375005 0.69283545 0.69115036 0.72455809
 0.66847627 0.64740495 0.66163746 0.6682822 ]
ti_coms = [0.16406106 0.12740924 0.10165104 0.10256563 0.10425073 0.07084299
 0.12692481 0.14799613 0.13376362 0.12711888]
t_total = [27.04975243 27.04975243 27.04975243 27.04975243 27.04975243 27.04975243
 27.04975243 27.04975243 27.04975243 27.04975243]
ene_coms = [0.01640611 0.01274092 0.0101651  0.01025656 0.01042507 0.0070843
 0.01269248 0.01479961 0.01337636 0.01271189]
ene_comp = [1.67949660e-06 1.39569153e-05 1.32572408e-06 5.54292882e-08
 2.00730491e-05 1.98383835e-06 1.17924332e-07 4.57932214e-06
 1.67951550e-06 1.23117665e-06]
ene_total = [0.55799086 0.43376403 0.34573676 0.34880389 0.35521527 0.24098809
 0.43164595 0.50345636 0.45495626 0.4323438 ]
optimize_network iter = 1 obj = 4.1049012695131335
eta = 0.7655707229189843
freqs = [17457495.29787458 34701594.24583305 15634837.47847065  5428915.28985748
 38727869.1404616  17626023.89187681  7066143.85083459 24185313.6972893
 17186916.80476187 15445316.88497728]
eta_min = 0.7655707229189843	eta_max = 0.7655707229189802
af = 0.0013931998307684273	bf = 0.9619368665027545	zeta = 0.00153251981384527	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01640611 0.01274092 0.0101651  0.01025656 0.01042507 0.0070843
 0.01269248 0.01479961 0.01337636 0.01271189]
ene_comp = [5.99224787e-07 4.97966453e-06 4.73002879e-07 1.97765232e-08
 7.16182967e-06 7.07810371e-07 4.20740255e-08 1.63384870e-06
 5.99231533e-07 4.39269460e-07]
ene_total = [1.43880858 1.11776958 0.89148423 0.89946516 0.91486917 0.62132941
 1.11308828 1.29801572 1.17311101 1.11482503]
ti_comp = [0.63134002 0.66799184 0.69375005 0.69283545 0.69115036 0.72455809
 0.66847627 0.64740495 0.66163746 0.6682822 ]
ti_coms = [0.16406106 0.12740924 0.10165104 0.10256563 0.10425073 0.07084299
 0.12692481 0.14799613 0.13376362 0.12711888]
t_total = [27.04975243 27.04975243 27.04975243 27.04975243 27.04975243 27.04975243
 27.04975243 27.04975243 27.04975243 27.04975243]
ene_coms = [0.01640611 0.01274092 0.0101651  0.01025656 0.01042507 0.0070843
 0.01269248 0.01479961 0.01337636 0.01271189]
ene_comp = [1.67949660e-06 1.39569153e-05 1.32572408e-06 5.54292882e-08
 2.00730491e-05 1.98383835e-06 1.17924332e-07 4.57932214e-06
 1.67951550e-06 1.23117665e-06]
ene_total = [0.55799086 0.43376403 0.34573676 0.34880389 0.35521527 0.24098809
 0.43164595 0.50345636 0.45495626 0.4323438 ]
optimize_network iter = 2 obj = 4.104901269513062
eta = 0.7655707229189802
freqs = [17457495.29787456 34701594.24583303 15634837.47847066  5428915.28985748
 38727869.14046162 17626023.89187683  7066143.85083458 24185313.69728928
 17186916.80476186 15445316.88497728]
Done!
ene_coms = [0.01640611 0.01274092 0.0101651  0.01025656 0.01042507 0.0070843
 0.01269248 0.01479961 0.01337636 0.01271189]
ene_comp = [1.53601128e-06 1.27645268e-05 1.21246279e-06 5.06937685e-08
 1.83581377e-05 1.81435203e-06 1.07849640e-07 4.18809449e-06
 1.53602857e-06 1.12599288e-06]
ene_total = [0.01640764 0.01275369 0.01016632 0.01025661 0.01044343 0.00708611
 0.01269259 0.0148038  0.0133779  0.01271301]
At round 59 energy consumption: 0.12070110588134075
At round 59 eta: 0.7655707229189802
At round 59 a_n: 7.972397885064655
At round 59 local rounds: 8.74731389409248
At round 59 global rounds: 34.00768873381527
gradient difference: 0.6433150172233582
train() client id: f_00000-0-0 loss: 1.183761  [   32/  126]
train() client id: f_00000-0-1 loss: 1.050906  [   64/  126]
train() client id: f_00000-0-2 loss: 0.944945  [   96/  126]
train() client id: f_00000-1-0 loss: 1.016676  [   32/  126]
train() client id: f_00000-1-1 loss: 1.037372  [   64/  126]
train() client id: f_00000-1-2 loss: 0.952119  [   96/  126]
train() client id: f_00000-2-0 loss: 0.872846  [   32/  126]
train() client id: f_00000-2-1 loss: 0.891306  [   64/  126]
train() client id: f_00000-2-2 loss: 1.025856  [   96/  126]
train() client id: f_00000-3-0 loss: 0.864942  [   32/  126]
train() client id: f_00000-3-1 loss: 0.761262  [   64/  126]
train() client id: f_00000-3-2 loss: 1.024854  [   96/  126]
train() client id: f_00000-4-0 loss: 1.032924  [   32/  126]
train() client id: f_00000-4-1 loss: 0.896486  [   64/  126]
train() client id: f_00000-4-2 loss: 0.863861  [   96/  126]
train() client id: f_00000-5-0 loss: 0.916081  [   32/  126]
train() client id: f_00000-5-1 loss: 0.886554  [   64/  126]
train() client id: f_00000-5-2 loss: 0.912137  [   96/  126]
train() client id: f_00000-6-0 loss: 0.951070  [   32/  126]
train() client id: f_00000-6-1 loss: 0.946745  [   64/  126]
train() client id: f_00000-6-2 loss: 0.874370  [   96/  126]
train() client id: f_00000-7-0 loss: 0.924180  [   32/  126]
train() client id: f_00000-7-1 loss: 0.934429  [   64/  126]
train() client id: f_00000-7-2 loss: 0.861124  [   96/  126]
train() client id: f_00001-0-0 loss: 0.254548  [   32/  265]
train() client id: f_00001-0-1 loss: 0.413947  [   64/  265]
train() client id: f_00001-0-2 loss: 0.324283  [   96/  265]
train() client id: f_00001-0-3 loss: 0.437156  [  128/  265]
train() client id: f_00001-0-4 loss: 0.321799  [  160/  265]
train() client id: f_00001-0-5 loss: 0.323704  [  192/  265]
train() client id: f_00001-0-6 loss: 0.358817  [  224/  265]
train() client id: f_00001-0-7 loss: 0.278472  [  256/  265]
train() client id: f_00001-1-0 loss: 0.411248  [   32/  265]
train() client id: f_00001-1-1 loss: 0.251715  [   64/  265]
train() client id: f_00001-1-2 loss: 0.369046  [   96/  265]
train() client id: f_00001-1-3 loss: 0.414218  [  128/  265]
train() client id: f_00001-1-4 loss: 0.327279  [  160/  265]
train() client id: f_00001-1-5 loss: 0.361992  [  192/  265]
train() client id: f_00001-1-6 loss: 0.332566  [  224/  265]
train() client id: f_00001-1-7 loss: 0.241915  [  256/  265]
train() client id: f_00001-2-0 loss: 0.351875  [   32/  265]
train() client id: f_00001-2-1 loss: 0.348755  [   64/  265]
train() client id: f_00001-2-2 loss: 0.301396  [   96/  265]
train() client id: f_00001-2-3 loss: 0.301094  [  128/  265]
train() client id: f_00001-2-4 loss: 0.400312  [  160/  265]
train() client id: f_00001-2-5 loss: 0.353684  [  192/  265]
train() client id: f_00001-2-6 loss: 0.325250  [  224/  265]
train() client id: f_00001-2-7 loss: 0.325863  [  256/  265]
train() client id: f_00001-3-0 loss: 0.366202  [   32/  265]
train() client id: f_00001-3-1 loss: 0.330341  [   64/  265]
train() client id: f_00001-3-2 loss: 0.291708  [   96/  265]
train() client id: f_00001-3-3 loss: 0.423617  [  128/  265]
train() client id: f_00001-3-4 loss: 0.338406  [  160/  265]
train() client id: f_00001-3-5 loss: 0.330697  [  192/  265]
train() client id: f_00001-3-6 loss: 0.356242  [  224/  265]
train() client id: f_00001-3-7 loss: 0.246396  [  256/  265]
train() client id: f_00001-4-0 loss: 0.358728  [   32/  265]
train() client id: f_00001-4-1 loss: 0.244824  [   64/  265]
train() client id: f_00001-4-2 loss: 0.473570  [   96/  265]
train() client id: f_00001-4-3 loss: 0.344953  [  128/  265]
train() client id: f_00001-4-4 loss: 0.379995  [  160/  265]
train() client id: f_00001-4-5 loss: 0.270865  [  192/  265]
train() client id: f_00001-4-6 loss: 0.204642  [  224/  265]
train() client id: f_00001-4-7 loss: 0.292102  [  256/  265]
train() client id: f_00001-5-0 loss: 0.247234  [   32/  265]
train() client id: f_00001-5-1 loss: 0.336260  [   64/  265]
train() client id: f_00001-5-2 loss: 0.294073  [   96/  265]
train() client id: f_00001-5-3 loss: 0.235609  [  128/  265]
train() client id: f_00001-5-4 loss: 0.503679  [  160/  265]
train() client id: f_00001-5-5 loss: 0.321054  [  192/  265]
train() client id: f_00001-5-6 loss: 0.383471  [  224/  265]
train() client id: f_00001-5-7 loss: 0.236719  [  256/  265]
train() client id: f_00001-6-0 loss: 0.309954  [   32/  265]
train() client id: f_00001-6-1 loss: 0.457617  [   64/  265]
train() client id: f_00001-6-2 loss: 0.244515  [   96/  265]
train() client id: f_00001-6-3 loss: 0.364373  [  128/  265]
train() client id: f_00001-6-4 loss: 0.259500  [  160/  265]
train() client id: f_00001-6-5 loss: 0.265518  [  192/  265]
train() client id: f_00001-6-6 loss: 0.507015  [  224/  265]
train() client id: f_00001-6-7 loss: 0.217318  [  256/  265]
train() client id: f_00001-7-0 loss: 0.399663  [   32/  265]
train() client id: f_00001-7-1 loss: 0.290508  [   64/  265]
train() client id: f_00001-7-2 loss: 0.321040  [   96/  265]
train() client id: f_00001-7-3 loss: 0.329801  [  128/  265]
train() client id: f_00001-7-4 loss: 0.311960  [  160/  265]
train() client id: f_00001-7-5 loss: 0.307560  [  192/  265]
train() client id: f_00001-7-6 loss: 0.344175  [  224/  265]
train() client id: f_00001-7-7 loss: 0.309747  [  256/  265]
train() client id: f_00002-0-0 loss: 0.835727  [   32/  124]
train() client id: f_00002-0-1 loss: 1.226239  [   64/  124]
train() client id: f_00002-0-2 loss: 1.272820  [   96/  124]
train() client id: f_00002-1-0 loss: 1.209950  [   32/  124]
train() client id: f_00002-1-1 loss: 1.080693  [   64/  124]
train() client id: f_00002-1-2 loss: 1.111702  [   96/  124]
train() client id: f_00002-2-0 loss: 1.088473  [   32/  124]
train() client id: f_00002-2-1 loss: 1.071391  [   64/  124]
train() client id: f_00002-2-2 loss: 1.058967  [   96/  124]
train() client id: f_00002-3-0 loss: 1.215483  [   32/  124]
train() client id: f_00002-3-1 loss: 0.927497  [   64/  124]
train() client id: f_00002-3-2 loss: 1.199727  [   96/  124]
train() client id: f_00002-4-0 loss: 1.177034  [   32/  124]
train() client id: f_00002-4-1 loss: 1.134397  [   64/  124]
train() client id: f_00002-4-2 loss: 0.989831  [   96/  124]
train() client id: f_00002-5-0 loss: 1.047449  [   32/  124]
train() client id: f_00002-5-1 loss: 1.162676  [   64/  124]
train() client id: f_00002-5-2 loss: 1.011961  [   96/  124]
train() client id: f_00002-6-0 loss: 1.056899  [   32/  124]
train() client id: f_00002-6-1 loss: 1.171456  [   64/  124]
train() client id: f_00002-6-2 loss: 1.064274  [   96/  124]
train() client id: f_00002-7-0 loss: 0.995037  [   32/  124]
train() client id: f_00002-7-1 loss: 1.379427  [   64/  124]
train() client id: f_00002-7-2 loss: 0.982725  [   96/  124]
train() client id: f_00003-0-0 loss: 0.597713  [   32/   43]
train() client id: f_00003-1-0 loss: 0.886032  [   32/   43]
train() client id: f_00003-2-0 loss: 0.543425  [   32/   43]
train() client id: f_00003-3-0 loss: 0.676475  [   32/   43]
train() client id: f_00003-4-0 loss: 0.613843  [   32/   43]
train() client id: f_00003-5-0 loss: 0.800178  [   32/   43]
train() client id: f_00003-6-0 loss: 0.493311  [   32/   43]
train() client id: f_00003-7-0 loss: 0.887451  [   32/   43]
train() client id: f_00004-0-0 loss: 0.860728  [   32/  306]
train() client id: f_00004-0-1 loss: 0.996781  [   64/  306]
train() client id: f_00004-0-2 loss: 0.994357  [   96/  306]
train() client id: f_00004-0-3 loss: 0.752486  [  128/  306]
train() client id: f_00004-0-4 loss: 0.776980  [  160/  306]
train() client id: f_00004-0-5 loss: 1.026830  [  192/  306]
train() client id: f_00004-0-6 loss: 0.843250  [  224/  306]
train() client id: f_00004-0-7 loss: 0.855615  [  256/  306]
train() client id: f_00004-0-8 loss: 0.793676  [  288/  306]
train() client id: f_00004-1-0 loss: 0.878304  [   32/  306]
train() client id: f_00004-1-1 loss: 1.015172  [   64/  306]
train() client id: f_00004-1-2 loss: 0.936135  [   96/  306]
train() client id: f_00004-1-3 loss: 0.908052  [  128/  306]
train() client id: f_00004-1-4 loss: 0.960694  [  160/  306]
train() client id: f_00004-1-5 loss: 0.831998  [  192/  306]
train() client id: f_00004-1-6 loss: 0.704089  [  224/  306]
train() client id: f_00004-1-7 loss: 0.874167  [  256/  306]
train() client id: f_00004-1-8 loss: 0.819848  [  288/  306]
train() client id: f_00004-2-0 loss: 0.879622  [   32/  306]
train() client id: f_00004-2-1 loss: 1.053746  [   64/  306]
train() client id: f_00004-2-2 loss: 0.952008  [   96/  306]
train() client id: f_00004-2-3 loss: 0.982252  [  128/  306]
train() client id: f_00004-2-4 loss: 0.845010  [  160/  306]
train() client id: f_00004-2-5 loss: 0.875415  [  192/  306]
train() client id: f_00004-2-6 loss: 0.719833  [  224/  306]
train() client id: f_00004-2-7 loss: 0.820741  [  256/  306]
train() client id: f_00004-2-8 loss: 0.888301  [  288/  306]
train() client id: f_00004-3-0 loss: 0.918071  [   32/  306]
train() client id: f_00004-3-1 loss: 0.742546  [   64/  306]
train() client id: f_00004-3-2 loss: 0.903251  [   96/  306]
train() client id: f_00004-3-3 loss: 0.850196  [  128/  306]
train() client id: f_00004-3-4 loss: 0.913189  [  160/  306]
train() client id: f_00004-3-5 loss: 0.907060  [  192/  306]
train() client id: f_00004-3-6 loss: 0.838848  [  224/  306]
train() client id: f_00004-3-7 loss: 0.989952  [  256/  306]
train() client id: f_00004-3-8 loss: 0.886619  [  288/  306]
train() client id: f_00004-4-0 loss: 0.818914  [   32/  306]
train() client id: f_00004-4-1 loss: 0.821244  [   64/  306]
train() client id: f_00004-4-2 loss: 0.871455  [   96/  306]
train() client id: f_00004-4-3 loss: 0.932844  [  128/  306]
train() client id: f_00004-4-4 loss: 0.916651  [  160/  306]
train() client id: f_00004-4-5 loss: 0.943480  [  192/  306]
train() client id: f_00004-4-6 loss: 0.896150  [  224/  306]
train() client id: f_00004-4-7 loss: 0.803432  [  256/  306]
train() client id: f_00004-4-8 loss: 0.945333  [  288/  306]
train() client id: f_00004-5-0 loss: 0.804072  [   32/  306]
train() client id: f_00004-5-1 loss: 0.888420  [   64/  306]
train() client id: f_00004-5-2 loss: 0.994312  [   96/  306]
train() client id: f_00004-5-3 loss: 0.984978  [  128/  306]
train() client id: f_00004-5-4 loss: 0.792759  [  160/  306]
train() client id: f_00004-5-5 loss: 0.849641  [  192/  306]
train() client id: f_00004-5-6 loss: 0.836852  [  224/  306]
train() client id: f_00004-5-7 loss: 0.765509  [  256/  306]
train() client id: f_00004-5-8 loss: 0.970635  [  288/  306]
train() client id: f_00004-6-0 loss: 0.840852  [   32/  306]
train() client id: f_00004-6-1 loss: 0.763761  [   64/  306]
train() client id: f_00004-6-2 loss: 0.881844  [   96/  306]
train() client id: f_00004-6-3 loss: 0.841628  [  128/  306]
train() client id: f_00004-6-4 loss: 0.886586  [  160/  306]
train() client id: f_00004-6-5 loss: 0.962130  [  192/  306]
train() client id: f_00004-6-6 loss: 0.866146  [  224/  306]
train() client id: f_00004-6-7 loss: 0.912697  [  256/  306]
train() client id: f_00004-6-8 loss: 0.875772  [  288/  306]
train() client id: f_00004-7-0 loss: 0.946711  [   32/  306]
train() client id: f_00004-7-1 loss: 0.845550  [   64/  306]
train() client id: f_00004-7-2 loss: 0.805899  [   96/  306]
train() client id: f_00004-7-3 loss: 0.868439  [  128/  306]
train() client id: f_00004-7-4 loss: 0.807088  [  160/  306]
train() client id: f_00004-7-5 loss: 0.795087  [  192/  306]
train() client id: f_00004-7-6 loss: 0.929139  [  224/  306]
train() client id: f_00004-7-7 loss: 0.896535  [  256/  306]
train() client id: f_00004-7-8 loss: 0.948728  [  288/  306]
train() client id: f_00005-0-0 loss: 0.281941  [   32/  146]
train() client id: f_00005-0-1 loss: 0.455054  [   64/  146]
train() client id: f_00005-0-2 loss: 0.292448  [   96/  146]
train() client id: f_00005-0-3 loss: 0.500342  [  128/  146]
train() client id: f_00005-1-0 loss: 0.468533  [   32/  146]
train() client id: f_00005-1-1 loss: 0.413014  [   64/  146]
train() client id: f_00005-1-2 loss: 0.237341  [   96/  146]
train() client id: f_00005-1-3 loss: 0.492290  [  128/  146]
train() client id: f_00005-2-0 loss: 0.580723  [   32/  146]
train() client id: f_00005-2-1 loss: 0.087704  [   64/  146]
train() client id: f_00005-2-2 loss: 0.209002  [   96/  146]
train() client id: f_00005-2-3 loss: 0.952644  [  128/  146]
train() client id: f_00005-3-0 loss: 0.386159  [   32/  146]
train() client id: f_00005-3-1 loss: 0.292932  [   64/  146]
train() client id: f_00005-3-2 loss: 0.415140  [   96/  146]
train() client id: f_00005-3-3 loss: 0.704305  [  128/  146]
train() client id: f_00005-4-0 loss: 0.540491  [   32/  146]
train() client id: f_00005-4-1 loss: 0.314665  [   64/  146]
train() client id: f_00005-4-2 loss: 0.270878  [   96/  146]
train() client id: f_00005-4-3 loss: 0.286057  [  128/  146]
train() client id: f_00005-5-0 loss: 0.613980  [   32/  146]
train() client id: f_00005-5-1 loss: 0.362402  [   64/  146]
train() client id: f_00005-5-2 loss: 0.497274  [   96/  146]
train() client id: f_00005-5-3 loss: 0.441849  [  128/  146]
train() client id: f_00005-6-0 loss: 0.638747  [   32/  146]
train() client id: f_00005-6-1 loss: 0.391731  [   64/  146]
train() client id: f_00005-6-2 loss: 0.250981  [   96/  146]
train() client id: f_00005-6-3 loss: 0.390103  [  128/  146]
train() client id: f_00005-7-0 loss: 0.321584  [   32/  146]
train() client id: f_00005-7-1 loss: 0.663270  [   64/  146]
train() client id: f_00005-7-2 loss: 0.225353  [   96/  146]
train() client id: f_00005-7-3 loss: 0.358758  [  128/  146]
train() client id: f_00006-0-0 loss: 0.522022  [   32/   54]
train() client id: f_00006-1-0 loss: 0.499577  [   32/   54]
train() client id: f_00006-2-0 loss: 0.454657  [   32/   54]
train() client id: f_00006-3-0 loss: 0.532819  [   32/   54]
train() client id: f_00006-4-0 loss: 0.482199  [   32/   54]
train() client id: f_00006-5-0 loss: 0.508818  [   32/   54]
train() client id: f_00006-6-0 loss: 0.423975  [   32/   54]
train() client id: f_00006-7-0 loss: 0.479273  [   32/   54]
train() client id: f_00007-0-0 loss: 0.519454  [   32/  179]
train() client id: f_00007-0-1 loss: 0.445633  [   64/  179]
train() client id: f_00007-0-2 loss: 0.643872  [   96/  179]
train() client id: f_00007-0-3 loss: 0.614814  [  128/  179]
train() client id: f_00007-0-4 loss: 0.495662  [  160/  179]
train() client id: f_00007-1-0 loss: 0.454995  [   32/  179]
train() client id: f_00007-1-1 loss: 0.495349  [   64/  179]
train() client id: f_00007-1-2 loss: 0.862615  [   96/  179]
train() client id: f_00007-1-3 loss: 0.466222  [  128/  179]
train() client id: f_00007-1-4 loss: 0.405495  [  160/  179]
train() client id: f_00007-2-0 loss: 0.474036  [   32/  179]
train() client id: f_00007-2-1 loss: 0.548018  [   64/  179]
train() client id: f_00007-2-2 loss: 0.437574  [   96/  179]
train() client id: f_00007-2-3 loss: 0.479493  [  128/  179]
train() client id: f_00007-2-4 loss: 0.753228  [  160/  179]
train() client id: f_00007-3-0 loss: 0.532299  [   32/  179]
train() client id: f_00007-3-1 loss: 0.493567  [   64/  179]
train() client id: f_00007-3-2 loss: 0.524306  [   96/  179]
train() client id: f_00007-3-3 loss: 0.629885  [  128/  179]
train() client id: f_00007-3-4 loss: 0.472114  [  160/  179]
train() client id: f_00007-4-0 loss: 0.813997  [   32/  179]
train() client id: f_00007-4-1 loss: 0.510739  [   64/  179]
train() client id: f_00007-4-2 loss: 0.371541  [   96/  179]
train() client id: f_00007-4-3 loss: 0.347579  [  128/  179]
train() client id: f_00007-4-4 loss: 0.451341  [  160/  179]
train() client id: f_00007-5-0 loss: 0.639269  [   32/  179]
train() client id: f_00007-5-1 loss: 0.323239  [   64/  179]
train() client id: f_00007-5-2 loss: 0.508857  [   96/  179]
train() client id: f_00007-5-3 loss: 0.572721  [  128/  179]
train() client id: f_00007-5-4 loss: 0.390461  [  160/  179]
train() client id: f_00007-6-0 loss: 0.447990  [   32/  179]
train() client id: f_00007-6-1 loss: 0.637525  [   64/  179]
train() client id: f_00007-6-2 loss: 0.513731  [   96/  179]
train() client id: f_00007-6-3 loss: 0.499450  [  128/  179]
train() client id: f_00007-6-4 loss: 0.435435  [  160/  179]
train() client id: f_00007-7-0 loss: 0.476442  [   32/  179]
train() client id: f_00007-7-1 loss: 0.530453  [   64/  179]
train() client id: f_00007-7-2 loss: 0.332573  [   96/  179]
train() client id: f_00007-7-3 loss: 0.666177  [  128/  179]
train() client id: f_00007-7-4 loss: 0.521811  [  160/  179]
train() client id: f_00008-0-0 loss: 0.688326  [   32/  130]
train() client id: f_00008-0-1 loss: 0.862523  [   64/  130]
train() client id: f_00008-0-2 loss: 0.880618  [   96/  130]
train() client id: f_00008-0-3 loss: 0.905065  [  128/  130]
train() client id: f_00008-1-0 loss: 0.835153  [   32/  130]
train() client id: f_00008-1-1 loss: 0.845805  [   64/  130]
train() client id: f_00008-1-2 loss: 0.858323  [   96/  130]
train() client id: f_00008-1-3 loss: 0.749496  [  128/  130]
train() client id: f_00008-2-0 loss: 0.661519  [   32/  130]
train() client id: f_00008-2-1 loss: 0.912319  [   64/  130]
train() client id: f_00008-2-2 loss: 0.784354  [   96/  130]
train() client id: f_00008-2-3 loss: 0.913495  [  128/  130]
train() client id: f_00008-3-0 loss: 0.847374  [   32/  130]
train() client id: f_00008-3-1 loss: 0.836015  [   64/  130]
train() client id: f_00008-3-2 loss: 0.814486  [   96/  130]
train() client id: f_00008-3-3 loss: 0.801354  [  128/  130]
train() client id: f_00008-4-0 loss: 0.733538  [   32/  130]
train() client id: f_00008-4-1 loss: 0.816781  [   64/  130]
train() client id: f_00008-4-2 loss: 0.793858  [   96/  130]
train() client id: f_00008-4-3 loss: 0.973409  [  128/  130]
train() client id: f_00008-5-0 loss: 0.788311  [   32/  130]
train() client id: f_00008-5-1 loss: 0.785879  [   64/  130]
train() client id: f_00008-5-2 loss: 0.819797  [   96/  130]
train() client id: f_00008-5-3 loss: 0.918840  [  128/  130]
train() client id: f_00008-6-0 loss: 0.826758  [   32/  130]
train() client id: f_00008-6-1 loss: 0.902276  [   64/  130]
train() client id: f_00008-6-2 loss: 0.870391  [   96/  130]
train() client id: f_00008-6-3 loss: 0.689580  [  128/  130]
train() client id: f_00008-7-0 loss: 0.745650  [   32/  130]
train() client id: f_00008-7-1 loss: 0.830080  [   64/  130]
train() client id: f_00008-7-2 loss: 0.813261  [   96/  130]
train() client id: f_00008-7-3 loss: 0.906662  [  128/  130]
train() client id: f_00009-0-0 loss: 1.150943  [   32/  118]
train() client id: f_00009-0-1 loss: 0.991842  [   64/  118]
train() client id: f_00009-0-2 loss: 1.107126  [   96/  118]
train() client id: f_00009-1-0 loss: 1.054607  [   32/  118]
train() client id: f_00009-1-1 loss: 0.826632  [   64/  118]
train() client id: f_00009-1-2 loss: 0.959896  [   96/  118]
train() client id: f_00009-2-0 loss: 0.955154  [   32/  118]
train() client id: f_00009-2-1 loss: 0.922437  [   64/  118]
train() client id: f_00009-2-2 loss: 0.941249  [   96/  118]
train() client id: f_00009-3-0 loss: 1.006712  [   32/  118]
train() client id: f_00009-3-1 loss: 0.869352  [   64/  118]
train() client id: f_00009-3-2 loss: 0.926832  [   96/  118]
train() client id: f_00009-4-0 loss: 0.878689  [   32/  118]
train() client id: f_00009-4-1 loss: 0.840225  [   64/  118]
train() client id: f_00009-4-2 loss: 1.025207  [   96/  118]
train() client id: f_00009-5-0 loss: 0.832758  [   32/  118]
train() client id: f_00009-5-1 loss: 0.799899  [   64/  118]
train() client id: f_00009-5-2 loss: 0.895071  [   96/  118]
train() client id: f_00009-6-0 loss: 0.847969  [   32/  118]
train() client id: f_00009-6-1 loss: 0.828615  [   64/  118]
train() client id: f_00009-6-2 loss: 0.941870  [   96/  118]
train() client id: f_00009-7-0 loss: 0.923261  [   32/  118]
train() client id: f_00009-7-1 loss: 0.750252  [   64/  118]
train() client id: f_00009-7-2 loss: 0.901558  [   96/  118]
At round 59 accuracy: 0.6472148541114059
At round 59 training accuracy: 0.5895372233400402
At round 59 training loss: 0.8268884098221
update_location
xs = -4.528292 196.001589 205.045120 -205.943528 94.896481 -130.217951 -247.215960 268.375741 -1.680116 189.695607 
ys = 282.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -212.154970 4.001482 
xs mean: 36.442869159412865
ys mean: 9.371751218646875
dists_uav = 299.794030 220.586961 228.134270 230.045661 137.861396 164.209117 266.693637 287.042367 234.547552 214.477121 
uav_gains = -115.713254 -109.030756 -109.566647 -109.707072 -103.487533 -105.399640 -112.770058 -114.612107 -110.045865 -108.616181 
uav_gains_db_mean: -109.89491129552535
dists_bs = 201.583165 403.812605 417.850239 155.563757 322.020135 177.914020 192.207264 484.032032 424.180095 402.794463 
bs_gains = -104.091778 -112.540096 -112.955637 -100.940495 -109.787784 -102.572944 -103.512613 -114.743534 -113.138467 -112.509397 
bs_gains_db_mean: -108.67927459276598
Round 60
-------------------------------
ene_coms = [0.01705121 0.01288179 0.01038206 0.01047949 0.01053628 0.00705472
 0.01312115 0.01536544 0.0135217  0.01285044]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [3.060101   6.25882264 2.96221528 1.08023776 7.19373194 3.44793012
 1.356276   4.27901768 3.12587363 2.8417775 ]
obj_prev = 35.60598354118303
eta_min = 1.7295393204406857e-30	eta_max = 0.9515253667965865
af = 7.4502323791253575	bf = 0.9403356747399739	zeta = 8.195255617037894	eta = 0.909090909090909
af = 7.4502323791253575	bf = 0.9403356747399739	zeta = 18.154605286162543	eta = 0.4103769959021875
af = 7.4502323791253575	bf = 0.9403356747399739	zeta = 12.84906023352826	eta = 0.5798270257683723
af = 7.4502323791253575	bf = 0.9403356747399739	zeta = 11.902007617268087	eta = 0.6259643430505077
af = 7.4502323791253575	bf = 0.9403356747399739	zeta = 11.845088511921924	eta = 0.6289722843038951
af = 7.4502323791253575	bf = 0.9403356747399739	zeta = 11.844858157506431	eta = 0.6289845163239822
eta = 0.6289845163239822
ene_coms = [0.01705121 0.01288179 0.01038206 0.01047949 0.01053628 0.00705472
 0.01312115 0.01536544 0.0135217  0.01285044]
ene_comp = [0.03825917 0.08046572 0.03765189 0.0130567  0.09291514 0.04433206
 0.01639679 0.05435232 0.03947375 0.03583002]
ene_total = [1.13744587 1.91967121 0.98780753 0.48401671 2.12745565 1.05675793
 0.60703002 1.43373047 1.08983988 1.00110289]
ti_comp = [1.14239997 1.18409412 1.20909146 1.20811711 1.20754928 1.2423648
 1.18170051 1.15925762 1.17769502 1.1844077 ]
ti_coms = [0.17051207 0.12881793 0.10382059 0.10479494 0.10536277 0.07054724
 0.13121154 0.15365442 0.13521702 0.12850435]
t_total = [26.99974823 26.99974823 26.99974823 26.99974823 26.99974823 26.99974823
 26.99974823 26.99974823 26.99974823 26.99974823]
ene_coms = [0.01705121 0.01288179 0.01038206 0.01047949 0.01053628 0.00705472
 0.01312115 0.01536544 0.0135217  0.01285044]
ene_comp = [2.68194848e-06 2.32241773e-05 2.28203295e-06 9.53152655e-08
 3.43818878e-05 3.52805585e-06 1.97306594e-07 7.46748460e-06
 2.77165736e-06 2.04936231e-06]
ene_total = [0.35070949 0.26538885 0.21355169 0.21551045 0.21738328 0.14515141
 0.26983771 0.31614052 0.27812784 0.26430852]
optimize_network iter = 0 obj = 2.536109750163864
eta = 0.6289845163239822
freqs = [16745087.22908975 33977756.8206219  15570321.65391675  5403740.36966979
 38472607.12989803 17841804.21352578  6937793.84404409 23442726.64817338
 16758902.15720318 15125712.45497397]
eta_min = 0.6289845163239839	eta_max = 0.7682181231332252
af = 0.0012947538764109111	bf = 0.9403356747399739	zeta = 0.0014242292640520024	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01705121 0.01288179 0.01038206 0.01047949 0.01053628 0.00705472
 0.01312115 0.01536544 0.0135217  0.01285044]
ene_comp = [5.51316170e-07 4.77409040e-06 4.69107321e-07 1.95935334e-08
 7.06773112e-06 7.25246684e-07 4.05594352e-08 1.53505745e-06
 5.69757224e-07 4.21278257e-07]
ene_total = [1.43112635 1.0815486  0.87138865 0.87952851 0.88488576 0.59215238
 1.10124048 1.28972539 1.13490228 1.07855146]
ti_comp = [0.64969418 0.69138832 0.71638566 0.71541131 0.71484348 0.74965901
 0.68899471 0.66655183 0.68498923 0.6917019 ]
ti_coms = [0.17051207 0.12881793 0.10382059 0.10479494 0.10536277 0.07054724
 0.13121154 0.15365442 0.13521702 0.12850435]
t_total = [26.99974823 26.99974823 26.99974823 26.99974823 26.99974823 26.99974823
 26.99974823 26.99974823 26.99974823 26.99974823]
ene_coms = [0.01705121 0.01288179 0.01038206 0.01047949 0.01053628 0.00705472
 0.01312115 0.01536544 0.0135217  0.01285044]
ene_comp = [1.52525066e-06 1.25297117e-05 1.19569060e-06 4.99966948e-08
 1.80463714e-05 1.78229083e-06 1.06757166e-07 4.15469148e-06
 1.50698908e-06 1.10523752e-06]
ene_total = [0.56134599 0.42445844 0.34179848 0.34496817 0.34742978 0.23228779
 0.43192884 0.50594012 0.44516028 0.42305012]
optimize_network iter = 1 obj = 4.058367996810318
eta = 0.7682181231332252
freqs = [16745087.22908974 33094031.6666294  14945162.50155104  5189655.10263478
 36960369.90736971 16815699.37939614  6767117.45001178 23187020.75103569
 16386471.75341484 14729529.76352042]
eta_min = 0.7682181231332245	eta_max = 0.7682181231331137
af = 0.0012153923945231347	bf = 0.9403356747399739	zeta = 0.0013369316339754483	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01705121 0.01288179 0.01038206 0.01047949 0.01053628 0.00705472
 0.01312115 0.01536544 0.0135217  0.01285044]
ene_comp = [5.51316170e-07 4.52898192e-06 4.32193594e-07 1.80717748e-08
 6.52303039e-06 6.44225754e-07 3.85883799e-08 1.50175224e-06
 5.44715354e-07 3.99498478e-07]
ene_total = [1.43112635 1.08152802 0.87138555 0.87952838 0.88484004 0.59214558
 1.10124032 1.2897226  1.13490018 1.07854963]
ti_comp = [0.64969418 0.69138832 0.71638566 0.71541131 0.71484348 0.74965901
 0.68899471 0.66655183 0.68498923 0.6917019 ]
ti_coms = [0.17051207 0.12881793 0.10382059 0.10479494 0.10536277 0.07054724
 0.13121154 0.15365442 0.13521702 0.12850435]
t_total = [26.99974823 26.99974823 26.99974823 26.99974823 26.99974823 26.99974823
 26.99974823 26.99974823 26.99974823 26.99974823]
ene_coms = [0.01705121 0.01288179 0.01038206 0.01047949 0.01053628 0.00705472
 0.01312115 0.01536544 0.0135217  0.01285044]
ene_comp = [1.52525066e-06 1.25297117e-05 1.19569060e-06 4.99966948e-08
 1.80463714e-05 1.78229083e-06 1.06757166e-07 4.15469148e-06
 1.50698908e-06 1.10523752e-06]
ene_total = [0.56134599 0.42445844 0.34179848 0.34496817 0.34742978 0.23228779
 0.43192884 0.50594012 0.44516028 0.42305012]
optimize_network iter = 2 obj = 4.058367996808365
eta = 0.7682181231331137
freqs = [16745087.2290888  33094031.66662874 14945162.50155105  5189655.10263478
 36960369.90736967 16815699.37939655  6767117.45001163 23187020.75103474
 16386471.75341443 14729529.76352013]
Done!
ene_coms = [0.01705121 0.01288179 0.01038206 0.01047949 0.01053628 0.00705472
 0.01312115 0.01536544 0.0135217  0.01285044]
ene_comp = [1.41320565e-06 1.16092783e-05 1.10785510e-06 4.63239345e-08
 1.67206839e-05 1.65136363e-06 9.89147777e-08 3.84948759e-06
 1.39628557e-06 1.02404670e-06]
ene_total = [0.01705262 0.0128934  0.01038317 0.01047954 0.010553   0.00705638
 0.01312125 0.01536929 0.0135231  0.01285146]
At round 60 energy consumption: 0.12328320449614406
At round 60 eta: 0.7682181231331137
At round 60 a_n: 7.62985203809534
At round 60 local rounds: 8.634274319277383
At round 60 global rounds: 32.91824253574928
gradient difference: 0.5970796346664429
train() client id: f_00000-0-0 loss: 1.278084  [   32/  126]
train() client id: f_00000-0-1 loss: 1.177455  [   64/  126]
train() client id: f_00000-0-2 loss: 1.119660  [   96/  126]
train() client id: f_00000-1-0 loss: 0.897537  [   32/  126]
train() client id: f_00000-1-1 loss: 1.107184  [   64/  126]
train() client id: f_00000-1-2 loss: 1.160100  [   96/  126]
train() client id: f_00000-2-0 loss: 1.050576  [   32/  126]
train() client id: f_00000-2-1 loss: 1.006687  [   64/  126]
train() client id: f_00000-2-2 loss: 0.951927  [   96/  126]
train() client id: f_00000-3-0 loss: 0.916464  [   32/  126]
train() client id: f_00000-3-1 loss: 0.963929  [   64/  126]
train() client id: f_00000-3-2 loss: 0.934093  [   96/  126]
train() client id: f_00000-4-0 loss: 0.971509  [   32/  126]
train() client id: f_00000-4-1 loss: 0.944064  [   64/  126]
train() client id: f_00000-4-2 loss: 0.838201  [   96/  126]
train() client id: f_00000-5-0 loss: 0.978789  [   32/  126]
train() client id: f_00000-5-1 loss: 0.849433  [   64/  126]
train() client id: f_00000-5-2 loss: 0.885331  [   96/  126]
train() client id: f_00000-6-0 loss: 0.916633  [   32/  126]
train() client id: f_00000-6-1 loss: 0.905905  [   64/  126]
train() client id: f_00000-6-2 loss: 0.802059  [   96/  126]
train() client id: f_00000-7-0 loss: 0.899751  [   32/  126]
train() client id: f_00000-7-1 loss: 0.976494  [   64/  126]
train() client id: f_00000-7-2 loss: 0.825107  [   96/  126]
train() client id: f_00001-0-0 loss: 0.612048  [   32/  265]
train() client id: f_00001-0-1 loss: 0.492976  [   64/  265]
train() client id: f_00001-0-2 loss: 0.441518  [   96/  265]
train() client id: f_00001-0-3 loss: 0.472029  [  128/  265]
train() client id: f_00001-0-4 loss: 0.390714  [  160/  265]
train() client id: f_00001-0-5 loss: 0.552559  [  192/  265]
train() client id: f_00001-0-6 loss: 0.478016  [  224/  265]
train() client id: f_00001-0-7 loss: 0.476989  [  256/  265]
train() client id: f_00001-1-0 loss: 0.416121  [   32/  265]
train() client id: f_00001-1-1 loss: 0.539606  [   64/  265]
train() client id: f_00001-1-2 loss: 0.478303  [   96/  265]
train() client id: f_00001-1-3 loss: 0.400951  [  128/  265]
train() client id: f_00001-1-4 loss: 0.569351  [  160/  265]
train() client id: f_00001-1-5 loss: 0.472887  [  192/  265]
train() client id: f_00001-1-6 loss: 0.456993  [  224/  265]
train() client id: f_00001-1-7 loss: 0.571860  [  256/  265]
train() client id: f_00001-2-0 loss: 0.461965  [   32/  265]
train() client id: f_00001-2-1 loss: 0.462090  [   64/  265]
train() client id: f_00001-2-2 loss: 0.561400  [   96/  265]
train() client id: f_00001-2-3 loss: 0.523110  [  128/  265]
train() client id: f_00001-2-4 loss: 0.501792  [  160/  265]
train() client id: f_00001-2-5 loss: 0.401033  [  192/  265]
train() client id: f_00001-2-6 loss: 0.401763  [  224/  265]
train() client id: f_00001-2-7 loss: 0.560500  [  256/  265]
train() client id: f_00001-3-0 loss: 0.518412  [   32/  265]
train() client id: f_00001-3-1 loss: 0.516202  [   64/  265]
train() client id: f_00001-3-2 loss: 0.519083  [   96/  265]
train() client id: f_00001-3-3 loss: 0.471517  [  128/  265]
train() client id: f_00001-3-4 loss: 0.365268  [  160/  265]
train() client id: f_00001-3-5 loss: 0.466965  [  192/  265]
train() client id: f_00001-3-6 loss: 0.450547  [  224/  265]
train() client id: f_00001-3-7 loss: 0.543999  [  256/  265]
train() client id: f_00001-4-0 loss: 0.497482  [   32/  265]
train() client id: f_00001-4-1 loss: 0.525694  [   64/  265]
train() client id: f_00001-4-2 loss: 0.454847  [   96/  265]
train() client id: f_00001-4-3 loss: 0.591393  [  128/  265]
train() client id: f_00001-4-4 loss: 0.377412  [  160/  265]
train() client id: f_00001-4-5 loss: 0.524045  [  192/  265]
train() client id: f_00001-4-6 loss: 0.445948  [  224/  265]
train() client id: f_00001-4-7 loss: 0.414052  [  256/  265]
train() client id: f_00001-5-0 loss: 0.425022  [   32/  265]
train() client id: f_00001-5-1 loss: 0.407933  [   64/  265]
train() client id: f_00001-5-2 loss: 0.583099  [   96/  265]
train() client id: f_00001-5-3 loss: 0.365978  [  128/  265]
train() client id: f_00001-5-4 loss: 0.585144  [  160/  265]
train() client id: f_00001-5-5 loss: 0.535283  [  192/  265]
train() client id: f_00001-5-6 loss: 0.417851  [  224/  265]
train() client id: f_00001-5-7 loss: 0.501899  [  256/  265]
train() client id: f_00001-6-0 loss: 0.494668  [   32/  265]
train() client id: f_00001-6-1 loss: 0.448105  [   64/  265]
train() client id: f_00001-6-2 loss: 0.385775  [   96/  265]
train() client id: f_00001-6-3 loss: 0.393185  [  128/  265]
train() client id: f_00001-6-4 loss: 0.502332  [  160/  265]
train() client id: f_00001-6-5 loss: 0.482221  [  192/  265]
train() client id: f_00001-6-6 loss: 0.490327  [  224/  265]
train() client id: f_00001-6-7 loss: 0.620996  [  256/  265]
train() client id: f_00001-7-0 loss: 0.432470  [   32/  265]
train() client id: f_00001-7-1 loss: 0.503979  [   64/  265]
train() client id: f_00001-7-2 loss: 0.453599  [   96/  265]
train() client id: f_00001-7-3 loss: 0.467191  [  128/  265]
train() client id: f_00001-7-4 loss: 0.590615  [  160/  265]
train() client id: f_00001-7-5 loss: 0.518963  [  192/  265]
train() client id: f_00001-7-6 loss: 0.470555  [  224/  265]
train() client id: f_00001-7-7 loss: 0.378750  [  256/  265]
train() client id: f_00002-0-0 loss: 1.150954  [   32/  124]
train() client id: f_00002-0-1 loss: 1.129846  [   64/  124]
train() client id: f_00002-0-2 loss: 1.113822  [   96/  124]
train() client id: f_00002-1-0 loss: 1.115921  [   32/  124]
train() client id: f_00002-1-1 loss: 1.080921  [   64/  124]
train() client id: f_00002-1-2 loss: 1.147565  [   96/  124]
train() client id: f_00002-2-0 loss: 0.969318  [   32/  124]
train() client id: f_00002-2-1 loss: 1.204657  [   64/  124]
train() client id: f_00002-2-2 loss: 1.145712  [   96/  124]
train() client id: f_00002-3-0 loss: 1.101556  [   32/  124]
train() client id: f_00002-3-1 loss: 1.180260  [   64/  124]
train() client id: f_00002-3-2 loss: 1.102516  [   96/  124]
train() client id: f_00002-4-0 loss: 1.041611  [   32/  124]
train() client id: f_00002-4-1 loss: 1.095879  [   64/  124]
train() client id: f_00002-4-2 loss: 1.049001  [   96/  124]
train() client id: f_00002-5-0 loss: 1.066795  [   32/  124]
train() client id: f_00002-5-1 loss: 1.205153  [   64/  124]
train() client id: f_00002-5-2 loss: 0.964570  [   96/  124]
train() client id: f_00002-6-0 loss: 1.124401  [   32/  124]
train() client id: f_00002-6-1 loss: 0.994950  [   64/  124]
train() client id: f_00002-6-2 loss: 1.184050  [   96/  124]
train() client id: f_00002-7-0 loss: 1.117891  [   32/  124]
train() client id: f_00002-7-1 loss: 1.200734  [   64/  124]
train() client id: f_00002-7-2 loss: 1.123329  [   96/  124]
train() client id: f_00003-0-0 loss: 0.363887  [   32/   43]
train() client id: f_00003-1-0 loss: 0.414051  [   32/   43]
train() client id: f_00003-2-0 loss: 0.394267  [   32/   43]
train() client id: f_00003-3-0 loss: 0.427383  [   32/   43]
train() client id: f_00003-4-0 loss: 0.417989  [   32/   43]
train() client id: f_00003-5-0 loss: 0.333207  [   32/   43]
train() client id: f_00003-6-0 loss: 0.407491  [   32/   43]
train() client id: f_00003-7-0 loss: 0.307668  [   32/   43]
train() client id: f_00004-0-0 loss: 0.750636  [   32/  306]
train() client id: f_00004-0-1 loss: 0.765986  [   64/  306]
train() client id: f_00004-0-2 loss: 0.716212  [   96/  306]
train() client id: f_00004-0-3 loss: 0.528641  [  128/  306]
train() client id: f_00004-0-4 loss: 0.782601  [  160/  306]
train() client id: f_00004-0-5 loss: 0.761862  [  192/  306]
train() client id: f_00004-0-6 loss: 0.689396  [  224/  306]
train() client id: f_00004-0-7 loss: 0.774907  [  256/  306]
train() client id: f_00004-0-8 loss: 0.870801  [  288/  306]
train() client id: f_00004-1-0 loss: 0.603038  [   32/  306]
train() client id: f_00004-1-1 loss: 0.851300  [   64/  306]
train() client id: f_00004-1-2 loss: 0.690243  [   96/  306]
train() client id: f_00004-1-3 loss: 0.747418  [  128/  306]
train() client id: f_00004-1-4 loss: 0.779160  [  160/  306]
train() client id: f_00004-1-5 loss: 0.743478  [  192/  306]
train() client id: f_00004-1-6 loss: 0.819305  [  224/  306]
train() client id: f_00004-1-7 loss: 0.633608  [  256/  306]
train() client id: f_00004-1-8 loss: 0.769491  [  288/  306]
train() client id: f_00004-2-0 loss: 0.749749  [   32/  306]
train() client id: f_00004-2-1 loss: 0.670418  [   64/  306]
train() client id: f_00004-2-2 loss: 0.706805  [   96/  306]
train() client id: f_00004-2-3 loss: 0.584793  [  128/  306]
train() client id: f_00004-2-4 loss: 0.852638  [  160/  306]
train() client id: f_00004-2-5 loss: 0.782193  [  192/  306]
train() client id: f_00004-2-6 loss: 0.779275  [  224/  306]
train() client id: f_00004-2-7 loss: 0.584854  [  256/  306]
train() client id: f_00004-2-8 loss: 0.798303  [  288/  306]
train() client id: f_00004-3-0 loss: 0.661064  [   32/  306]
train() client id: f_00004-3-1 loss: 0.754230  [   64/  306]
train() client id: f_00004-3-2 loss: 0.655105  [   96/  306]
train() client id: f_00004-3-3 loss: 0.668601  [  128/  306]
train() client id: f_00004-3-4 loss: 0.769674  [  160/  306]
train() client id: f_00004-3-5 loss: 0.858221  [  192/  306]
train() client id: f_00004-3-6 loss: 0.806166  [  224/  306]
train() client id: f_00004-3-7 loss: 0.715056  [  256/  306]
train() client id: f_00004-3-8 loss: 0.804613  [  288/  306]
train() client id: f_00004-4-0 loss: 0.831891  [   32/  306]
train() client id: f_00004-4-1 loss: 0.794219  [   64/  306]
train() client id: f_00004-4-2 loss: 0.671678  [   96/  306]
train() client id: f_00004-4-3 loss: 0.690924  [  128/  306]
train() client id: f_00004-4-4 loss: 0.785188  [  160/  306]
train() client id: f_00004-4-5 loss: 0.640981  [  192/  306]
train() client id: f_00004-4-6 loss: 0.775583  [  224/  306]
train() client id: f_00004-4-7 loss: 0.800792  [  256/  306]
train() client id: f_00004-4-8 loss: 0.665311  [  288/  306]
train() client id: f_00004-5-0 loss: 0.655619  [   32/  306]
train() client id: f_00004-5-1 loss: 0.719416  [   64/  306]
train() client id: f_00004-5-2 loss: 0.759075  [   96/  306]
train() client id: f_00004-5-3 loss: 0.891938  [  128/  306]
train() client id: f_00004-5-4 loss: 0.855832  [  160/  306]
train() client id: f_00004-5-5 loss: 0.728921  [  192/  306]
train() client id: f_00004-5-6 loss: 0.649566  [  224/  306]
train() client id: f_00004-5-7 loss: 0.672324  [  256/  306]
train() client id: f_00004-5-8 loss: 0.752879  [  288/  306]
train() client id: f_00004-6-0 loss: 0.748284  [   32/  306]
train() client id: f_00004-6-1 loss: 0.807958  [   64/  306]
train() client id: f_00004-6-2 loss: 0.855789  [   96/  306]
train() client id: f_00004-6-3 loss: 0.741993  [  128/  306]
train() client id: f_00004-6-4 loss: 0.703135  [  160/  306]
train() client id: f_00004-6-5 loss: 0.727706  [  192/  306]
train() client id: f_00004-6-6 loss: 0.817782  [  224/  306]
train() client id: f_00004-6-7 loss: 0.651156  [  256/  306]
train() client id: f_00004-6-8 loss: 0.617008  [  288/  306]
train() client id: f_00004-7-0 loss: 0.797122  [   32/  306]
train() client id: f_00004-7-1 loss: 0.739209  [   64/  306]
train() client id: f_00004-7-2 loss: 0.669572  [   96/  306]
train() client id: f_00004-7-3 loss: 0.802292  [  128/  306]
train() client id: f_00004-7-4 loss: 0.868277  [  160/  306]
train() client id: f_00004-7-5 loss: 0.758126  [  192/  306]
train() client id: f_00004-7-6 loss: 0.724943  [  224/  306]
train() client id: f_00004-7-7 loss: 0.685457  [  256/  306]
train() client id: f_00004-7-8 loss: 0.765890  [  288/  306]
train() client id: f_00005-0-0 loss: 0.743109  [   32/  146]
train() client id: f_00005-0-1 loss: 0.557215  [   64/  146]
train() client id: f_00005-0-2 loss: 0.942305  [   96/  146]
train() client id: f_00005-0-3 loss: 0.631156  [  128/  146]
train() client id: f_00005-1-0 loss: 0.615214  [   32/  146]
train() client id: f_00005-1-1 loss: 0.690692  [   64/  146]
train() client id: f_00005-1-2 loss: 0.867956  [   96/  146]
train() client id: f_00005-1-3 loss: 0.648563  [  128/  146]
train() client id: f_00005-2-0 loss: 0.610202  [   32/  146]
train() client id: f_00005-2-1 loss: 0.627092  [   64/  146]
train() client id: f_00005-2-2 loss: 0.876745  [   96/  146]
train() client id: f_00005-2-3 loss: 0.567852  [  128/  146]
train() client id: f_00005-3-0 loss: 0.652667  [   32/  146]
train() client id: f_00005-3-1 loss: 0.846553  [   64/  146]
train() client id: f_00005-3-2 loss: 0.716167  [   96/  146]
train() client id: f_00005-3-3 loss: 0.608543  [  128/  146]
train() client id: f_00005-4-0 loss: 0.615196  [   32/  146]
train() client id: f_00005-4-1 loss: 0.826738  [   64/  146]
train() client id: f_00005-4-2 loss: 0.552374  [   96/  146]
train() client id: f_00005-4-3 loss: 0.816925  [  128/  146]
train() client id: f_00005-5-0 loss: 0.546838  [   32/  146]
train() client id: f_00005-5-1 loss: 0.827255  [   64/  146]
train() client id: f_00005-5-2 loss: 0.792316  [   96/  146]
train() client id: f_00005-5-3 loss: 0.500132  [  128/  146]
train() client id: f_00005-6-0 loss: 0.566609  [   32/  146]
train() client id: f_00005-6-1 loss: 0.634106  [   64/  146]
train() client id: f_00005-6-2 loss: 0.793516  [   96/  146]
train() client id: f_00005-6-3 loss: 0.794146  [  128/  146]
train() client id: f_00005-7-0 loss: 0.816796  [   32/  146]
train() client id: f_00005-7-1 loss: 0.579839  [   64/  146]
train() client id: f_00005-7-2 loss: 0.561414  [   96/  146]
train() client id: f_00005-7-3 loss: 0.927104  [  128/  146]
train() client id: f_00006-0-0 loss: 0.538646  [   32/   54]
train() client id: f_00006-1-0 loss: 0.481169  [   32/   54]
train() client id: f_00006-2-0 loss: 0.508215  [   32/   54]
train() client id: f_00006-3-0 loss: 0.508622  [   32/   54]
train() client id: f_00006-4-0 loss: 0.534209  [   32/   54]
train() client id: f_00006-5-0 loss: 0.524706  [   32/   54]
train() client id: f_00006-6-0 loss: 0.532582  [   32/   54]
train() client id: f_00006-7-0 loss: 0.563896  [   32/   54]
train() client id: f_00007-0-0 loss: 0.726894  [   32/  179]
train() client id: f_00007-0-1 loss: 0.656908  [   64/  179]
train() client id: f_00007-0-2 loss: 0.554835  [   96/  179]
train() client id: f_00007-0-3 loss: 0.634442  [  128/  179]
train() client id: f_00007-0-4 loss: 0.451619  [  160/  179]
train() client id: f_00007-1-0 loss: 0.596215  [   32/  179]
train() client id: f_00007-1-1 loss: 0.659317  [   64/  179]
train() client id: f_00007-1-2 loss: 0.570823  [   96/  179]
train() client id: f_00007-1-3 loss: 0.678995  [  128/  179]
train() client id: f_00007-1-4 loss: 0.543859  [  160/  179]
train() client id: f_00007-2-0 loss: 0.561972  [   32/  179]
train() client id: f_00007-2-1 loss: 0.557724  [   64/  179]
train() client id: f_00007-2-2 loss: 0.593738  [   96/  179]
train() client id: f_00007-2-3 loss: 0.821151  [  128/  179]
train() client id: f_00007-2-4 loss: 0.506254  [  160/  179]
train() client id: f_00007-3-0 loss: 0.714379  [   32/  179]
train() client id: f_00007-3-1 loss: 0.442536  [   64/  179]
train() client id: f_00007-3-2 loss: 0.740348  [   96/  179]
train() client id: f_00007-3-3 loss: 0.688597  [  128/  179]
train() client id: f_00007-3-4 loss: 0.452890  [  160/  179]
train() client id: f_00007-4-0 loss: 0.466919  [   32/  179]
train() client id: f_00007-4-1 loss: 0.696989  [   64/  179]
train() client id: f_00007-4-2 loss: 0.800399  [   96/  179]
train() client id: f_00007-4-3 loss: 0.598297  [  128/  179]
train() client id: f_00007-4-4 loss: 0.411360  [  160/  179]
train() client id: f_00007-5-0 loss: 0.587832  [   32/  179]
train() client id: f_00007-5-1 loss: 0.439549  [   64/  179]
train() client id: f_00007-5-2 loss: 0.640397  [   96/  179]
train() client id: f_00007-5-3 loss: 0.565001  [  128/  179]
train() client id: f_00007-5-4 loss: 0.775584  [  160/  179]
train() client id: f_00007-6-0 loss: 0.532040  [   32/  179]
train() client id: f_00007-6-1 loss: 0.632193  [   64/  179]
train() client id: f_00007-6-2 loss: 0.591377  [   96/  179]
train() client id: f_00007-6-3 loss: 0.728029  [  128/  179]
train() client id: f_00007-6-4 loss: 0.449155  [  160/  179]
train() client id: f_00007-7-0 loss: 0.564375  [   32/  179]
train() client id: f_00007-7-1 loss: 0.612319  [   64/  179]
train() client id: f_00007-7-2 loss: 0.809187  [   96/  179]
train() client id: f_00007-7-3 loss: 0.544772  [  128/  179]
train() client id: f_00007-7-4 loss: 0.446325  [  160/  179]
train() client id: f_00008-0-0 loss: 0.583093  [   32/  130]
train() client id: f_00008-0-1 loss: 0.661518  [   64/  130]
train() client id: f_00008-0-2 loss: 0.771718  [   96/  130]
train() client id: f_00008-0-3 loss: 0.576243  [  128/  130]
train() client id: f_00008-1-0 loss: 0.608018  [   32/  130]
train() client id: f_00008-1-1 loss: 0.579670  [   64/  130]
train() client id: f_00008-1-2 loss: 0.695190  [   96/  130]
train() client id: f_00008-1-3 loss: 0.672402  [  128/  130]
train() client id: f_00008-2-0 loss: 0.681075  [   32/  130]
train() client id: f_00008-2-1 loss: 0.532888  [   64/  130]
train() client id: f_00008-2-2 loss: 0.696939  [   96/  130]
train() client id: f_00008-2-3 loss: 0.667967  [  128/  130]
train() client id: f_00008-3-0 loss: 0.616980  [   32/  130]
train() client id: f_00008-3-1 loss: 0.666409  [   64/  130]
train() client id: f_00008-3-2 loss: 0.644607  [   96/  130]
train() client id: f_00008-3-3 loss: 0.630527  [  128/  130]
train() client id: f_00008-4-0 loss: 0.664488  [   32/  130]
train() client id: f_00008-4-1 loss: 0.667171  [   64/  130]
train() client id: f_00008-4-2 loss: 0.638420  [   96/  130]
train() client id: f_00008-4-3 loss: 0.576371  [  128/  130]
train() client id: f_00008-5-0 loss: 0.699873  [   32/  130]
train() client id: f_00008-5-1 loss: 0.561932  [   64/  130]
train() client id: f_00008-5-2 loss: 0.486499  [   96/  130]
train() client id: f_00008-5-3 loss: 0.773318  [  128/  130]
train() client id: f_00008-6-0 loss: 0.714085  [   32/  130]
train() client id: f_00008-6-1 loss: 0.517267  [   64/  130]
train() client id: f_00008-6-2 loss: 0.711588  [   96/  130]
train() client id: f_00008-6-3 loss: 0.644443  [  128/  130]
train() client id: f_00008-7-0 loss: 0.623039  [   32/  130]
train() client id: f_00008-7-1 loss: 0.670606  [   64/  130]
train() client id: f_00008-7-2 loss: 0.692695  [   96/  130]
train() client id: f_00008-7-3 loss: 0.569318  [  128/  130]
train() client id: f_00009-0-0 loss: 0.904540  [   32/  118]
train() client id: f_00009-0-1 loss: 0.885960  [   64/  118]
train() client id: f_00009-0-2 loss: 0.821619  [   96/  118]
train() client id: f_00009-1-0 loss: 0.923654  [   32/  118]
train() client id: f_00009-1-1 loss: 0.769961  [   64/  118]
train() client id: f_00009-1-2 loss: 0.783217  [   96/  118]
train() client id: f_00009-2-0 loss: 0.729631  [   32/  118]
train() client id: f_00009-2-1 loss: 0.977555  [   64/  118]
train() client id: f_00009-2-2 loss: 0.691865  [   96/  118]
train() client id: f_00009-3-0 loss: 0.923168  [   32/  118]
train() client id: f_00009-3-1 loss: 0.821665  [   64/  118]
train() client id: f_00009-3-2 loss: 0.709534  [   96/  118]
train() client id: f_00009-4-0 loss: 0.736017  [   32/  118]
train() client id: f_00009-4-1 loss: 0.681485  [   64/  118]
train() client id: f_00009-4-2 loss: 0.797200  [   96/  118]
train() client id: f_00009-5-0 loss: 0.645601  [   32/  118]
train() client id: f_00009-5-1 loss: 0.893759  [   64/  118]
train() client id: f_00009-5-2 loss: 0.775733  [   96/  118]
train() client id: f_00009-6-0 loss: 0.777079  [   32/  118]
train() client id: f_00009-6-1 loss: 0.731023  [   64/  118]
train() client id: f_00009-6-2 loss: 0.721361  [   96/  118]
train() client id: f_00009-7-0 loss: 0.790189  [   32/  118]
train() client id: f_00009-7-1 loss: 0.719696  [   64/  118]
train() client id: f_00009-7-2 loss: 0.732443  [   96/  118]
At round 60 accuracy: 0.6472148541114059
At round 60 training accuracy: 0.5888665325285044
At round 60 training loss: 0.8269411252180675
update_location
xs = -4.528292 201.001589 210.045120 -210.943528 99.896481 -135.217951 -252.215960 273.375741 -1.680116 194.695607 
ys = 287.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -217.154970 4.001482 
xs mean: 37.442869159412865
ys mean: 9.371751218646875
dists_uav = 304.511642 225.041381 232.638553 234.532388 141.349670 168.201705 271.334951 291.722604 239.079702 218.911835 
uav_gains = -116.101193 -109.343563 -109.900801 -110.044704 -103.759428 -105.665676 -113.191920 -115.024062 -110.398643 -108.915528 
uav_gains_db_mean: -110.23455192617223
dists_bs = 204.295501 408.411111 422.402975 156.634983 326.222213 176.721753 194.141165 488.616174 428.748531 407.327062 
bs_gains = -104.254306 -112.677791 -113.087414 -101.023945 -109.945438 -102.491179 -103.634353 -114.858159 -113.268733 -112.645471 
bs_gains_db_mean: -108.78867876501737
Round 61
-------------------------------
ene_coms = [0.01772177 0.01302416 0.01061676 0.01072092 0.01064887 0.00702811
 0.01358164 0.01596119 0.01366861 0.01299049]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.92765227 5.97887776 2.83095285 1.03351712 6.87159469 3.29293774
 1.29877494 4.09129441 2.98661741 2.71522546]
obj_prev = 34.027444659747225
eta_min = 7.332457016828517e-32	eta_max = 0.9518041598057914
af = 7.115750642430655	bf = 0.9179275037024878	zeta = 7.8273257066737205	eta = 0.9090909090909091
af = 7.115750642430655	bf = 0.9179275037024878	zeta = 17.557440745628227	eta = 0.405284047118454
af = 7.115750642430655	bf = 0.9179275037024878	zeta = 12.349827490808122	eta = 0.5761821894052246
af = 7.115750642430655	bf = 0.9179275037024878	zeta = 11.422512016653531	eta = 0.622958472887242
af = 7.115750642430655	bf = 0.9179275037024878	zeta = 11.36648634969858	eta = 0.6260290492161945
af = 7.115750642430655	bf = 0.9179275037024878	zeta = 11.366256720722618	eta = 0.6260416966878315
eta = 0.6260416966878315
ene_coms = [0.01772177 0.01302416 0.01061676 0.01072092 0.01064887 0.00702811
 0.01358164 0.01596119 0.01366861 0.01299049]
ene_comp = [0.03864615 0.08127961 0.03803272 0.01318877 0.09385495 0.04478046
 0.01656264 0.05490208 0.03987302 0.03619243]
ene_total = [1.09843886 1.8376927  0.94803003 0.46592692 2.03646045 1.00959101
 0.58741999 1.38090887 1.04336289 0.95842501]
ti_comp = [1.20574607 1.25272219 1.27679616 1.27575454 1.27647509 1.31268269
 1.24714738 1.22335186 1.2462777  1.25305889]
ti_coms = [0.17721771 0.1302416  0.10616763 0.10720924 0.10648869 0.07028109
 0.1358164  0.15961193 0.13668609 0.12990489]
t_total = [26.94974403 26.94974403 26.94974403 26.94974403 26.94974403 26.94974403
 26.94974403 26.94974403 26.94974403 26.94974403]
ene_coms = [0.01772177 0.01302416 0.01061676 0.01072092 0.01064887 0.00702811
 0.01358164 0.01596119 0.01366861 0.01299049]
ene_comp = [2.48134499e-06 2.13852982e-05 2.10915817e-06 8.80964373e-08
 3.17122324e-05 3.25706388e-06 1.82571485e-07 6.91104224e-06
 2.55086595e-06 1.88707987e-06]
ene_total = [0.34539159 0.25421787 0.20692941 0.20891981 0.20813194 0.13701988
 0.26466829 0.31116961 0.26640918 0.25318177]
optimize_network iter = 0 obj = 2.456039342208897
eta = 0.6260416966878315
freqs = [16025826.23525838 32441195.15396541 14893811.72346102  5169006.4285148
 36763328.21966479 17056850.29678025  6640208.5847112  22439200.82953397
 15996842.53374188 14441631.69125531]
eta_min = 0.626041696687832	eta_max = 0.771326346283716
af = 0.0011291017105763853	bf = 0.9179275037024878	zeta = 0.001242011881634024	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01772177 0.01302416 0.01061676 0.01072092 0.01064887 0.00702811
 0.01358164 0.01596119 0.01366861 0.01299049]
ene_comp = [5.04971376e-07 4.35206048e-06 4.29228708e-07 1.79282524e-08
 6.45366513e-06 6.62835696e-07 3.71545975e-08 1.40644631e-06
 5.19119388e-07 3.84034193e-07]
ene_total = [1.42062417 1.04437031 0.85107803 0.85939465 0.85413459 0.56342897
 1.08871228 1.27956781 1.09572232 1.04135318]
ti_comp = [0.66845781 0.71543392 0.73950789 0.73846628 0.73918683 0.77539442
 0.70985911 0.68606359 0.70898943 0.71577063]
ti_coms = [0.17721771 0.1302416  0.10616763 0.10720924 0.10648869 0.07028109
 0.1358164  0.15961193 0.13668609 0.12990489]
t_total = [26.94974403 26.94974403 26.94974403 26.94974403 26.94974403 26.94974403
 26.94974403 26.94974403 26.94974403 26.94974403]
ene_coms = [0.01772177 0.01302416 0.01061676 0.01072092 0.01064887 0.00702811
 0.01358164 0.01596119 0.01366861 0.01299049]
ene_comp = [1.37564157e-06 1.11722392e-05 1.07132463e-06 4.48007916e-08
 1.61137825e-05 1.59057765e-06 9.60237387e-08 3.74431177e-06
 1.34305003e-06 9.85462118e-07]
ene_total = [0.56479614 0.41540621 0.33836609 0.34165275 0.3398686  0.22402045
 0.43281887 0.50876601 0.43563009 0.41400857]
optimize_network iter = 1 obj = 4.015333790982397
eta = 0.771326346283716
freqs = [16025826.23525838 31491999.15801006 14256166.46785084  4950643.70793148
 35195820.28130172 16008626.2759278   6467634.68746326 22182604.81151965
 15589330.89959625 14016255.94407742]
eta_min = 0.7713263462837149	eta_max = 0.7713263462836617
af = 0.0010535404964197774	bf = 0.9179275037024878	zeta = 0.0011588945460617552	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01772177 0.01302416 0.01061676 0.01072092 0.01064887 0.00702811
 0.01358164 0.01596119 0.01366861 0.01299049]
ene_comp = [5.04971376e-07 4.10111260e-06 3.93262520e-07 1.64455028e-08
 5.91505744e-06 5.83870247e-07 3.52484545e-08 1.37446433e-06
 4.93007652e-07 3.61744055e-07]
ene_total = [1.42062417 1.04435019 0.85107515 0.85939453 0.85409141 0.56342264
 1.08871212 1.27956525 1.09572023 1.04135139]
ti_comp = [0.66845781 0.71543392 0.73950789 0.73846628 0.73918683 0.77539442
 0.70985911 0.68606359 0.70898943 0.71577063]
ti_coms = [0.17721771 0.1302416  0.10616763 0.10720924 0.10648869 0.07028109
 0.1358164  0.15961193 0.13668609 0.12990489]
t_total = [26.94974403 26.94974403 26.94974403 26.94974403 26.94974403 26.94974403
 26.94974403 26.94974403 26.94974403 26.94974403]
ene_coms = [0.01772177 0.01302416 0.01061676 0.01072092 0.01064887 0.00702811
 0.01358164 0.01596119 0.01366861 0.01299049]
ene_comp = [1.37564157e-06 1.11722392e-05 1.07132463e-06 4.48007916e-08
 1.61137825e-05 1.59057765e-06 9.60237387e-08 3.74431177e-06
 1.34305003e-06 9.85462118e-07]
ene_total = [0.56479614 0.41540621 0.33836609 0.34165275 0.3398686  0.22402045
 0.43281887 0.50876601 0.43563009 0.41400857]
optimize_network iter = 2 obj = 4.015333790981444
eta = 0.7713263462836617
freqs = [16025826.23525791 31491999.15800976 14256166.46785083  4950643.70793148
 35195820.28130171 16008626.275928    6467634.68746318 22182604.81151918
 15589330.89959606 14016255.94407729]
Done!
ene_coms = [0.01772177 0.01302416 0.01061676 0.01072092 0.01064887 0.00702811
 0.01358164 0.01596119 0.01366861 0.01299049]
ene_comp = [1.29440862e-06 1.05125077e-05 1.00806188e-06 4.21552618e-08
 1.51622482e-05 1.49665251e-06 9.03534447e-08 3.52320657e-06
 1.26374164e-06 9.27269633e-07]
ene_total = [0.01772307 0.01303467 0.01061777 0.01072097 0.01066403 0.00702961
 0.01358173 0.01596472 0.01366987 0.01299142]
At round 61 energy consumption: 0.12599784772146144
At round 61 eta: 0.7713263462836617
At round 61 a_n: 7.287306191126021
At round 61 local rounds: 8.502054486821605
At round 61 global rounds: 31.867712229611158
gradient difference: 0.6562073826789856
train() client id: f_00000-0-0 loss: 0.959628  [   32/  126]
train() client id: f_00000-0-1 loss: 1.109128  [   64/  126]
train() client id: f_00000-0-2 loss: 1.121861  [   96/  126]
train() client id: f_00000-1-0 loss: 0.929870  [   32/  126]
train() client id: f_00000-1-1 loss: 1.066750  [   64/  126]
train() client id: f_00000-1-2 loss: 1.034171  [   96/  126]
train() client id: f_00000-2-0 loss: 0.954776  [   32/  126]
train() client id: f_00000-2-1 loss: 1.079801  [   64/  126]
train() client id: f_00000-2-2 loss: 1.015248  [   96/  126]
train() client id: f_00000-3-0 loss: 1.044897  [   32/  126]
train() client id: f_00000-3-1 loss: 0.983218  [   64/  126]
train() client id: f_00000-3-2 loss: 0.828704  [   96/  126]
train() client id: f_00000-4-0 loss: 0.965123  [   32/  126]
train() client id: f_00000-4-1 loss: 0.888289  [   64/  126]
train() client id: f_00000-4-2 loss: 0.895119  [   96/  126]
train() client id: f_00000-5-0 loss: 0.871480  [   32/  126]
train() client id: f_00000-5-1 loss: 0.942921  [   64/  126]
train() client id: f_00000-5-2 loss: 0.858185  [   96/  126]
train() client id: f_00000-6-0 loss: 0.944928  [   32/  126]
train() client id: f_00000-6-1 loss: 0.937440  [   64/  126]
train() client id: f_00000-6-2 loss: 0.999064  [   96/  126]
train() client id: f_00000-7-0 loss: 0.833131  [   32/  126]
train() client id: f_00000-7-1 loss: 0.965090  [   64/  126]
train() client id: f_00000-7-2 loss: 0.945568  [   96/  126]
train() client id: f_00001-0-0 loss: 0.510845  [   32/  265]
train() client id: f_00001-0-1 loss: 0.619270  [   64/  265]
train() client id: f_00001-0-2 loss: 0.481157  [   96/  265]
train() client id: f_00001-0-3 loss: 0.549410  [  128/  265]
train() client id: f_00001-0-4 loss: 0.460003  [  160/  265]
train() client id: f_00001-0-5 loss: 0.559122  [  192/  265]
train() client id: f_00001-0-6 loss: 0.601134  [  224/  265]
train() client id: f_00001-0-7 loss: 0.488617  [  256/  265]
train() client id: f_00001-1-0 loss: 0.534958  [   32/  265]
train() client id: f_00001-1-1 loss: 0.517640  [   64/  265]
train() client id: f_00001-1-2 loss: 0.505249  [   96/  265]
train() client id: f_00001-1-3 loss: 0.598760  [  128/  265]
train() client id: f_00001-1-4 loss: 0.566602  [  160/  265]
train() client id: f_00001-1-5 loss: 0.508373  [  192/  265]
train() client id: f_00001-1-6 loss: 0.511852  [  224/  265]
train() client id: f_00001-1-7 loss: 0.614996  [  256/  265]
train() client id: f_00001-2-0 loss: 0.630907  [   32/  265]
train() client id: f_00001-2-1 loss: 0.518227  [   64/  265]
train() client id: f_00001-2-2 loss: 0.522279  [   96/  265]
train() client id: f_00001-2-3 loss: 0.582782  [  128/  265]
train() client id: f_00001-2-4 loss: 0.517351  [  160/  265]
train() client id: f_00001-2-5 loss: 0.508015  [  192/  265]
train() client id: f_00001-2-6 loss: 0.482573  [  224/  265]
train() client id: f_00001-2-7 loss: 0.448309  [  256/  265]
train() client id: f_00001-3-0 loss: 0.592259  [   32/  265]
train() client id: f_00001-3-1 loss: 0.547431  [   64/  265]
train() client id: f_00001-3-2 loss: 0.475781  [   96/  265]
train() client id: f_00001-3-3 loss: 0.509659  [  128/  265]
train() client id: f_00001-3-4 loss: 0.462098  [  160/  265]
train() client id: f_00001-3-5 loss: 0.624849  [  192/  265]
train() client id: f_00001-3-6 loss: 0.523867  [  224/  265]
train() client id: f_00001-3-7 loss: 0.524862  [  256/  265]
train() client id: f_00001-4-0 loss: 0.534655  [   32/  265]
train() client id: f_00001-4-1 loss: 0.511684  [   64/  265]
train() client id: f_00001-4-2 loss: 0.494567  [   96/  265]
train() client id: f_00001-4-3 loss: 0.637950  [  128/  265]
train() client id: f_00001-4-4 loss: 0.556110  [  160/  265]
train() client id: f_00001-4-5 loss: 0.476088  [  192/  265]
train() client id: f_00001-4-6 loss: 0.481520  [  224/  265]
train() client id: f_00001-4-7 loss: 0.618794  [  256/  265]
train() client id: f_00001-5-0 loss: 0.551409  [   32/  265]
train() client id: f_00001-5-1 loss: 0.514738  [   64/  265]
train() client id: f_00001-5-2 loss: 0.448367  [   96/  265]
train() client id: f_00001-5-3 loss: 0.528986  [  128/  265]
train() client id: f_00001-5-4 loss: 0.579535  [  160/  265]
train() client id: f_00001-5-5 loss: 0.520837  [  192/  265]
train() client id: f_00001-5-6 loss: 0.475137  [  224/  265]
train() client id: f_00001-5-7 loss: 0.614883  [  256/  265]
train() client id: f_00001-6-0 loss: 0.429351  [   32/  265]
train() client id: f_00001-6-1 loss: 0.482568  [   64/  265]
train() client id: f_00001-6-2 loss: 0.576413  [   96/  265]
train() client id: f_00001-6-3 loss: 0.587818  [  128/  265]
train() client id: f_00001-6-4 loss: 0.599178  [  160/  265]
train() client id: f_00001-6-5 loss: 0.503262  [  192/  265]
train() client id: f_00001-6-6 loss: 0.597931  [  224/  265]
train() client id: f_00001-6-7 loss: 0.535178  [  256/  265]
train() client id: f_00001-7-0 loss: 0.590897  [   32/  265]
train() client id: f_00001-7-1 loss: 0.612960  [   64/  265]
train() client id: f_00001-7-2 loss: 0.442478  [   96/  265]
train() client id: f_00001-7-3 loss: 0.458850  [  128/  265]
train() client id: f_00001-7-4 loss: 0.489685  [  160/  265]
train() client id: f_00001-7-5 loss: 0.498354  [  192/  265]
train() client id: f_00001-7-6 loss: 0.432159  [  224/  265]
train() client id: f_00001-7-7 loss: 0.777538  [  256/  265]
train() client id: f_00002-0-0 loss: 0.653103  [   32/  124]
train() client id: f_00002-0-1 loss: 1.141991  [   64/  124]
train() client id: f_00002-0-2 loss: 0.845860  [   96/  124]
train() client id: f_00002-1-0 loss: 0.868315  [   32/  124]
train() client id: f_00002-1-1 loss: 0.911194  [   64/  124]
train() client id: f_00002-1-2 loss: 0.706912  [   96/  124]
train() client id: f_00002-2-0 loss: 0.948934  [   32/  124]
train() client id: f_00002-2-1 loss: 0.728230  [   64/  124]
train() client id: f_00002-2-2 loss: 0.978303  [   96/  124]
train() client id: f_00002-3-0 loss: 0.801680  [   32/  124]
train() client id: f_00002-3-1 loss: 0.932037  [   64/  124]
train() client id: f_00002-3-2 loss: 0.923850  [   96/  124]
train() client id: f_00002-4-0 loss: 0.821096  [   32/  124]
train() client id: f_00002-4-1 loss: 0.824100  [   64/  124]
train() client id: f_00002-4-2 loss: 0.944109  [   96/  124]
train() client id: f_00002-5-0 loss: 0.904266  [   32/  124]
train() client id: f_00002-5-1 loss: 0.844247  [   64/  124]
train() client id: f_00002-5-2 loss: 1.058216  [   96/  124]
train() client id: f_00002-6-0 loss: 0.817269  [   32/  124]
train() client id: f_00002-6-1 loss: 0.905633  [   64/  124]
train() client id: f_00002-6-2 loss: 0.923405  [   96/  124]
train() client id: f_00002-7-0 loss: 0.795520  [   32/  124]
train() client id: f_00002-7-1 loss: 0.915973  [   64/  124]
train() client id: f_00002-7-2 loss: 0.941046  [   96/  124]
train() client id: f_00003-0-0 loss: 0.628678  [   32/   43]
train() client id: f_00003-1-0 loss: 0.897156  [   32/   43]
train() client id: f_00003-2-0 loss: 0.815874  [   32/   43]
train() client id: f_00003-3-0 loss: 0.946043  [   32/   43]
train() client id: f_00003-4-0 loss: 0.806672  [   32/   43]
train() client id: f_00003-5-0 loss: 0.966484  [   32/   43]
train() client id: f_00003-6-0 loss: 0.790573  [   32/   43]
train() client id: f_00003-7-0 loss: 0.981063  [   32/   43]
train() client id: f_00004-0-0 loss: 0.860865  [   32/  306]
train() client id: f_00004-0-1 loss: 0.724342  [   64/  306]
train() client id: f_00004-0-2 loss: 0.817342  [   96/  306]
train() client id: f_00004-0-3 loss: 0.778799  [  128/  306]
train() client id: f_00004-0-4 loss: 0.839466  [  160/  306]
train() client id: f_00004-0-5 loss: 0.847802  [  192/  306]
train() client id: f_00004-0-6 loss: 0.721413  [  224/  306]
train() client id: f_00004-0-7 loss: 0.796256  [  256/  306]
train() client id: f_00004-0-8 loss: 0.777241  [  288/  306]
train() client id: f_00004-1-0 loss: 0.739171  [   32/  306]
train() client id: f_00004-1-1 loss: 0.863697  [   64/  306]
train() client id: f_00004-1-2 loss: 0.684315  [   96/  306]
train() client id: f_00004-1-3 loss: 0.652750  [  128/  306]
train() client id: f_00004-1-4 loss: 0.919511  [  160/  306]
train() client id: f_00004-1-5 loss: 0.836653  [  192/  306]
train() client id: f_00004-1-6 loss: 0.766277  [  224/  306]
train() client id: f_00004-1-7 loss: 0.855341  [  256/  306]
train() client id: f_00004-1-8 loss: 0.856746  [  288/  306]
train() client id: f_00004-2-0 loss: 0.775485  [   32/  306]
train() client id: f_00004-2-1 loss: 0.804573  [   64/  306]
train() client id: f_00004-2-2 loss: 0.762796  [   96/  306]
train() client id: f_00004-2-3 loss: 0.793141  [  128/  306]
train() client id: f_00004-2-4 loss: 0.816571  [  160/  306]
train() client id: f_00004-2-5 loss: 0.842891  [  192/  306]
train() client id: f_00004-2-6 loss: 0.750751  [  224/  306]
train() client id: f_00004-2-7 loss: 0.645984  [  256/  306]
train() client id: f_00004-2-8 loss: 1.051726  [  288/  306]
train() client id: f_00004-3-0 loss: 0.711092  [   32/  306]
train() client id: f_00004-3-1 loss: 0.909780  [   64/  306]
train() client id: f_00004-3-2 loss: 0.815879  [   96/  306]
train() client id: f_00004-3-3 loss: 0.874525  [  128/  306]
train() client id: f_00004-3-4 loss: 0.844211  [  160/  306]
train() client id: f_00004-3-5 loss: 0.789315  [  192/  306]
train() client id: f_00004-3-6 loss: 0.671831  [  224/  306]
train() client id: f_00004-3-7 loss: 0.819132  [  256/  306]
train() client id: f_00004-3-8 loss: 0.796331  [  288/  306]
train() client id: f_00004-4-0 loss: 0.872696  [   32/  306]
train() client id: f_00004-4-1 loss: 0.704939  [   64/  306]
train() client id: f_00004-4-2 loss: 0.875416  [   96/  306]
train() client id: f_00004-4-3 loss: 0.761677  [  128/  306]
train() client id: f_00004-4-4 loss: 0.728354  [  160/  306]
train() client id: f_00004-4-5 loss: 0.660668  [  192/  306]
train() client id: f_00004-4-6 loss: 0.909735  [  224/  306]
train() client id: f_00004-4-7 loss: 0.929050  [  256/  306]
train() client id: f_00004-4-8 loss: 0.789857  [  288/  306]
train() client id: f_00004-5-0 loss: 0.896740  [   32/  306]
train() client id: f_00004-5-1 loss: 0.850725  [   64/  306]
train() client id: f_00004-5-2 loss: 0.678253  [   96/  306]
train() client id: f_00004-5-3 loss: 0.903375  [  128/  306]
train() client id: f_00004-5-4 loss: 0.715775  [  160/  306]
train() client id: f_00004-5-5 loss: 0.833221  [  192/  306]
train() client id: f_00004-5-6 loss: 0.767484  [  224/  306]
train() client id: f_00004-5-7 loss: 0.739437  [  256/  306]
train() client id: f_00004-5-8 loss: 0.765774  [  288/  306]
train() client id: f_00004-6-0 loss: 0.796174  [   32/  306]
train() client id: f_00004-6-1 loss: 0.770007  [   64/  306]
train() client id: f_00004-6-2 loss: 0.739506  [   96/  306]
train() client id: f_00004-6-3 loss: 0.842828  [  128/  306]
train() client id: f_00004-6-4 loss: 0.763897  [  160/  306]
train() client id: f_00004-6-5 loss: 0.934962  [  192/  306]
train() client id: f_00004-6-6 loss: 0.892786  [  224/  306]
train() client id: f_00004-6-7 loss: 0.644483  [  256/  306]
train() client id: f_00004-6-8 loss: 0.729571  [  288/  306]
train() client id: f_00004-7-0 loss: 0.770613  [   32/  306]
train() client id: f_00004-7-1 loss: 0.806549  [   64/  306]
train() client id: f_00004-7-2 loss: 0.737605  [   96/  306]
train() client id: f_00004-7-3 loss: 0.823572  [  128/  306]
train() client id: f_00004-7-4 loss: 0.919127  [  160/  306]
train() client id: f_00004-7-5 loss: 0.892990  [  192/  306]
train() client id: f_00004-7-6 loss: 0.758139  [  224/  306]
train() client id: f_00004-7-7 loss: 0.722699  [  256/  306]
train() client id: f_00004-7-8 loss: 0.803010  [  288/  306]
train() client id: f_00005-0-0 loss: 0.458284  [   32/  146]
train() client id: f_00005-0-1 loss: 0.310098  [   64/  146]
train() client id: f_00005-0-2 loss: 0.234543  [   96/  146]
train() client id: f_00005-0-3 loss: 0.619594  [  128/  146]
train() client id: f_00005-1-0 loss: 0.392280  [   32/  146]
train() client id: f_00005-1-1 loss: 0.319197  [   64/  146]
train() client id: f_00005-1-2 loss: 0.429597  [   96/  146]
train() client id: f_00005-1-3 loss: 0.349438  [  128/  146]
train() client id: f_00005-2-0 loss: 0.495061  [   32/  146]
train() client id: f_00005-2-1 loss: 0.253351  [   64/  146]
train() client id: f_00005-2-2 loss: 0.273158  [   96/  146]
train() client id: f_00005-2-3 loss: 0.229615  [  128/  146]
train() client id: f_00005-3-0 loss: 0.509086  [   32/  146]
train() client id: f_00005-3-1 loss: 0.408091  [   64/  146]
train() client id: f_00005-3-2 loss: 0.138998  [   96/  146]
train() client id: f_00005-3-3 loss: 0.529553  [  128/  146]
train() client id: f_00005-4-0 loss: 0.550605  [   32/  146]
train() client id: f_00005-4-1 loss: 0.313070  [   64/  146]
train() client id: f_00005-4-2 loss: 0.504457  [   96/  146]
train() client id: f_00005-4-3 loss: 0.374241  [  128/  146]
train() client id: f_00005-5-0 loss: 0.349748  [   32/  146]
train() client id: f_00005-5-1 loss: 0.359081  [   64/  146]
train() client id: f_00005-5-2 loss: 0.440443  [   96/  146]
train() client id: f_00005-5-3 loss: 0.423514  [  128/  146]
train() client id: f_00005-6-0 loss: 0.043001  [   32/  146]
train() client id: f_00005-6-1 loss: 0.412326  [   64/  146]
train() client id: f_00005-6-2 loss: 0.552908  [   96/  146]
train() client id: f_00005-6-3 loss: 0.597712  [  128/  146]
train() client id: f_00005-7-0 loss: 0.390606  [   32/  146]
train() client id: f_00005-7-1 loss: 0.452095  [   64/  146]
train() client id: f_00005-7-2 loss: 0.186908  [   96/  146]
train() client id: f_00005-7-3 loss: 0.520901  [  128/  146]
train() client id: f_00006-0-0 loss: 0.422434  [   32/   54]
train() client id: f_00006-1-0 loss: 0.446723  [   32/   54]
train() client id: f_00006-2-0 loss: 0.445332  [   32/   54]
train() client id: f_00006-3-0 loss: 0.433832  [   32/   54]
train() client id: f_00006-4-0 loss: 0.505059  [   32/   54]
train() client id: f_00006-5-0 loss: 0.442793  [   32/   54]
train() client id: f_00006-6-0 loss: 0.440834  [   32/   54]
train() client id: f_00006-7-0 loss: 0.458983  [   32/   54]
train() client id: f_00007-0-0 loss: 0.821948  [   32/  179]
train() client id: f_00007-0-1 loss: 0.609588  [   64/  179]
train() client id: f_00007-0-2 loss: 0.856956  [   96/  179]
train() client id: f_00007-0-3 loss: 0.697860  [  128/  179]
train() client id: f_00007-0-4 loss: 0.732138  [  160/  179]
train() client id: f_00007-1-0 loss: 0.557478  [   32/  179]
train() client id: f_00007-1-1 loss: 0.701525  [   64/  179]
train() client id: f_00007-1-2 loss: 0.832478  [   96/  179]
train() client id: f_00007-1-3 loss: 0.571595  [  128/  179]
train() client id: f_00007-1-4 loss: 0.930427  [  160/  179]
train() client id: f_00007-2-0 loss: 0.727891  [   32/  179]
train() client id: f_00007-2-1 loss: 0.982276  [   64/  179]
train() client id: f_00007-2-2 loss: 0.855367  [   96/  179]
train() client id: f_00007-2-3 loss: 0.605648  [  128/  179]
train() client id: f_00007-2-4 loss: 0.569761  [  160/  179]
train() client id: f_00007-3-0 loss: 0.707015  [   32/  179]
train() client id: f_00007-3-1 loss: 0.778598  [   64/  179]
train() client id: f_00007-3-2 loss: 0.743186  [   96/  179]
train() client id: f_00007-3-3 loss: 0.638972  [  128/  179]
train() client id: f_00007-3-4 loss: 0.619858  [  160/  179]
train() client id: f_00007-4-0 loss: 0.645088  [   32/  179]
train() client id: f_00007-4-1 loss: 0.782251  [   64/  179]
train() client id: f_00007-4-2 loss: 0.663861  [   96/  179]
train() client id: f_00007-4-3 loss: 0.797080  [  128/  179]
train() client id: f_00007-4-4 loss: 0.674287  [  160/  179]
train() client id: f_00007-5-0 loss: 0.633796  [   32/  179]
train() client id: f_00007-5-1 loss: 0.731297  [   64/  179]
train() client id: f_00007-5-2 loss: 0.677839  [   96/  179]
train() client id: f_00007-5-3 loss: 0.853703  [  128/  179]
train() client id: f_00007-5-4 loss: 0.689831  [  160/  179]
train() client id: f_00007-6-0 loss: 0.731567  [   32/  179]
train() client id: f_00007-6-1 loss: 0.704761  [   64/  179]
train() client id: f_00007-6-2 loss: 0.716857  [   96/  179]
train() client id: f_00007-6-3 loss: 0.576159  [  128/  179]
train() client id: f_00007-6-4 loss: 0.675474  [  160/  179]
train() client id: f_00007-7-0 loss: 0.740585  [   32/  179]
train() client id: f_00007-7-1 loss: 0.804985  [   64/  179]
train() client id: f_00007-7-2 loss: 0.670154  [   96/  179]
train() client id: f_00007-7-3 loss: 0.815927  [  128/  179]
train() client id: f_00007-7-4 loss: 0.684960  [  160/  179]
train() client id: f_00008-0-0 loss: 0.701788  [   32/  130]
train() client id: f_00008-0-1 loss: 0.984162  [   64/  130]
train() client id: f_00008-0-2 loss: 0.748537  [   96/  130]
train() client id: f_00008-0-3 loss: 0.786289  [  128/  130]
train() client id: f_00008-1-0 loss: 0.902543  [   32/  130]
train() client id: f_00008-1-1 loss: 0.725726  [   64/  130]
train() client id: f_00008-1-2 loss: 0.798920  [   96/  130]
train() client id: f_00008-1-3 loss: 0.835667  [  128/  130]
train() client id: f_00008-2-0 loss: 0.730912  [   32/  130]
train() client id: f_00008-2-1 loss: 0.829133  [   64/  130]
train() client id: f_00008-2-2 loss: 0.808604  [   96/  130]
train() client id: f_00008-2-3 loss: 0.879772  [  128/  130]
train() client id: f_00008-3-0 loss: 0.801874  [   32/  130]
train() client id: f_00008-3-1 loss: 0.899751  [   64/  130]
train() client id: f_00008-3-2 loss: 0.810356  [   96/  130]
train() client id: f_00008-3-3 loss: 0.738041  [  128/  130]
train() client id: f_00008-4-0 loss: 0.815626  [   32/  130]
train() client id: f_00008-4-1 loss: 0.768131  [   64/  130]
train() client id: f_00008-4-2 loss: 0.866432  [   96/  130]
train() client id: f_00008-4-3 loss: 0.740878  [  128/  130]
train() client id: f_00008-5-0 loss: 0.912405  [   32/  130]
train() client id: f_00008-5-1 loss: 0.853223  [   64/  130]
train() client id: f_00008-5-2 loss: 0.765357  [   96/  130]
train() client id: f_00008-5-3 loss: 0.712612  [  128/  130]
train() client id: f_00008-6-0 loss: 0.698005  [   32/  130]
train() client id: f_00008-6-1 loss: 0.882006  [   64/  130]
train() client id: f_00008-6-2 loss: 0.851907  [   96/  130]
train() client id: f_00008-6-3 loss: 0.798991  [  128/  130]
train() client id: f_00008-7-0 loss: 0.911062  [   32/  130]
train() client id: f_00008-7-1 loss: 0.816742  [   64/  130]
train() client id: f_00008-7-2 loss: 0.767738  [   96/  130]
train() client id: f_00008-7-3 loss: 0.743506  [  128/  130]
train() client id: f_00009-0-0 loss: 0.981047  [   32/  118]
train() client id: f_00009-0-1 loss: 0.999363  [   64/  118]
train() client id: f_00009-0-2 loss: 0.868705  [   96/  118]
train() client id: f_00009-1-0 loss: 0.968391  [   32/  118]
train() client id: f_00009-1-1 loss: 0.793375  [   64/  118]
train() client id: f_00009-1-2 loss: 0.862841  [   96/  118]
train() client id: f_00009-2-0 loss: 0.913581  [   32/  118]
train() client id: f_00009-2-1 loss: 0.881306  [   64/  118]
train() client id: f_00009-2-2 loss: 0.898910  [   96/  118]
train() client id: f_00009-3-0 loss: 0.808863  [   32/  118]
train() client id: f_00009-3-1 loss: 0.980508  [   64/  118]
train() client id: f_00009-3-2 loss: 0.847979  [   96/  118]
train() client id: f_00009-4-0 loss: 0.938941  [   32/  118]
train() client id: f_00009-4-1 loss: 1.003091  [   64/  118]
train() client id: f_00009-4-2 loss: 0.827218  [   96/  118]
train() client id: f_00009-5-0 loss: 0.966375  [   32/  118]
train() client id: f_00009-5-1 loss: 0.742666  [   64/  118]
train() client id: f_00009-5-2 loss: 0.810262  [   96/  118]
train() client id: f_00009-6-0 loss: 0.999928  [   32/  118]
train() client id: f_00009-6-1 loss: 0.691728  [   64/  118]
train() client id: f_00009-6-2 loss: 0.855114  [   96/  118]
train() client id: f_00009-7-0 loss: 0.911965  [   32/  118]
train() client id: f_00009-7-1 loss: 0.854030  [   64/  118]
train() client id: f_00009-7-2 loss: 0.878876  [   96/  118]
At round 61 accuracy: 0.6472148541114059
At round 61 training accuracy: 0.5928906773977196
At round 61 training loss: 0.8136207144321836
update_location
xs = -4.528292 206.001589 215.045120 -215.943528 104.896481 -140.217951 -257.215960 278.375741 -1.680116 199.695607 
ys = 292.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -222.154970 4.001482 
xs mean: 38.442869159412865
ys mean: 9.371751218646875
dists_uav = 309.238127 229.518276 237.162703 239.039487 144.926512 172.246896 275.988795 296.413285 243.630157 223.370426 
uav_gains = -116.477522 -109.668126 -110.247982 -110.395461 -104.031576 -105.930566 -113.615800 -115.428233 -110.764668 -109.225089 
uav_gains_db_mean: -110.57850236298239
dists_bs = 207.093050 413.018948 426.965718 157.857382 330.446512 175.663763 196.183464 493.208397 433.326496 411.870479 
bs_gains = -104.419694 -112.814218 -113.218063 -101.118476 -110.101892 -102.418160 -103.761606 -114.971912 -113.397886 -112.780358 
bs_gains_db_mean: -108.9002266283593
Round 62
-------------------------------
ene_coms = [0.01841572 0.01316804 0.01087136 0.01098298 0.01076285 0.00700451
 0.01407423 0.01658544 0.01381709 0.01313206]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.79490353 5.69884497 2.69966752 0.98677409 6.54938927 3.13798492
 1.24118042 3.90335877 2.8472706  2.58858717]
obj_prev = 32.44796124722022
eta_min = 2.271501264167692e-33	eta_max = 0.9521948770795574
af = 6.781268905735956	bf = 0.8945842468847052	zeta = 7.459395796309552	eta = 0.9090909090909091
af = 6.781268905735956	bf = 0.8945842468847052	zeta = 16.94999026151064	eta = 0.400075091555338
af = 6.781268905735956	bf = 0.8945842468847052	zeta = 11.846357894002182	eta = 0.5724349176694482
af = 6.781268905735956	bf = 0.8945842468847052	zeta = 10.939989145397123	eta = 0.6198606612502078
af = 6.781268905735956	bf = 0.8945842468847052	zeta = 10.884950151071346	eta = 0.6229949436257651
af = 6.781268905735956	bf = 0.8945842468847052	zeta = 10.884721748663965	eta = 0.6230080164032045
eta = 0.6230080164032045
ene_coms = [0.01841572 0.01316804 0.01087136 0.01098298 0.01076285 0.00700451
 0.01407423 0.01658544 0.01381709 0.01313206]
ene_comp = [0.03904699 0.08212264 0.0384272  0.01332556 0.09482841 0.04524493
 0.01673442 0.05547152 0.04028658 0.03656782]
ene_total = [1.05854965 1.75539784 0.90815363 0.44779993 1.94515004 0.96251329
 0.56754182 1.32739774 0.99667116 0.91554665]
ti_comp = [1.27607843 1.32855522 1.35152204 1.3504058  1.3526071  1.39019052
 1.31949331 1.2943812  1.32206466 1.32891498]
ti_coms = [0.18415717 0.13168038 0.10871356 0.1098298  0.1076285  0.07004507
 0.14074229 0.1658544  0.13817094 0.13132062]
t_total = [26.89973984 26.89973984 26.89973984 26.89973984 26.89973984 26.89973984
 26.89973984 26.89973984 26.89973984 26.89973984]
ene_coms = [0.01841572 0.01316804 0.01087136 0.01098298 0.01076285 0.00700451
 0.01407423 0.01658544 0.01381709 0.01313206]
ene_comp = [2.28500942e-06 1.96114405e-05 1.94155584e-06 8.10975758e-08
 2.91307758e-05 2.99530475e-06 1.68227887e-07 6.36745368e-06
 2.33806059e-06 1.73054431e-06]
ene_total = [0.33928734 0.24293636 0.20030252 0.20232453 0.19880455 0.12908876
 0.2592716  0.30564609 0.25457476 0.24194424]
optimize_network iter = 0 obj = 2.374180752979567
eta = 0.6230080164032045
freqs = [15299604.6032604  30906746.70842928 14216267.36605219  4933909.63335124
 35053936.41971414 16272922.723209    6341231.34771929 21427813.82690096
 15236235.87054931 13758523.82938607]
eta_min = 0.6230080164032057	eta_max = 0.774951395805132
af = 0.000978166981968207	bf = 0.8945842468847052	zeta = 0.0010759836801650278	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01841572 0.01316804 0.01087136 0.01098298 0.01076285 0.00700451
 0.01407423 0.01658544 0.01381709 0.01313206]
ene_comp = [4.60242072e-07 3.95009751e-06 3.91064333e-07 1.63345131e-08
 5.86746318e-06 6.03308351e-07 3.38841279e-08 1.28251991e-06
 4.70927535e-07 3.48562808e-07]
ene_total = [1.40685529 1.00623927 0.83051844 0.83901703 0.82264779 0.53513697
 1.07516621 1.26709892 1.05555646 1.00321587]
ti_comp = [0.68754302 0.74001981 0.76298663 0.76187039 0.76407169 0.80165511
 0.7309579  0.70584579 0.73352925 0.74037957]
ti_coms = [0.18415717 0.13168038 0.10871356 0.1098298  0.1076285  0.07004507
 0.14074229 0.1658544  0.13817094 0.13132062]
t_total = [26.89973984 26.89973984 26.89973984 26.89973984 26.89973984 26.89973984
 26.89973984 26.89973984 26.89973984 26.89973984]
ene_coms = [0.01841572 0.01316804 0.01087136 0.01098298 0.01076285 0.00700451
 0.01407423 0.01658544 0.01381709 0.01313206]
ene_comp = [1.23114868e-06 9.88665893e-06 9.52861598e-07 3.98512365e-08
 1.42788980e-05 1.40890613e-06 8.57423097e-08 3.34917669e-06
 1.18793811e-06 8.72038771e-07]
ene_total = [0.56832741 0.40665671 0.33550787 0.33892429 0.33257073 0.21619512
 0.43431837 0.51191237 0.42641748 0.40526836]
optimize_network iter = 1 obj = 3.976098689982005
eta = 0.774951395805132
freqs = [15299604.60326039 29895929.9034228  13567951.95641503  4711909.06369118
 33434656.00284957 15204594.02478522  6167525.25854117 21171554.26409379
 14795698.0009297  13305681.42193927]
eta_min = 0.7749513958051355	eta_max = 0.7749513958050913
af = 0.0009068795159953842	bf = 0.8945842468847052	zeta = 0.0009975674675949226	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01841572 0.01316804 0.01087136 0.01098298 0.01076285 0.00700451
 0.01407423 0.01658544 0.01381709 0.01313206]
ene_comp = [4.60242072e-07 3.69594385e-06 3.56209614e-07 1.48976447e-08
 5.33790084e-06 5.26693393e-07 3.20531703e-08 1.25202751e-06
 4.44088601e-07 3.25995501e-07]
ene_total = [1.40685529 1.00621985 0.83051577 0.83901692 0.82260733 0.53513111
 1.07516607 1.26709659 1.05555441 1.00321414]
ti_comp = [0.68754302 0.74001981 0.76298663 0.76187039 0.76407169 0.80165511
 0.7309579  0.70584579 0.73352925 0.74037957]
ti_coms = [0.18415717 0.13168038 0.10871356 0.1098298  0.1076285  0.07004507
 0.14074229 0.1658544  0.13817094 0.13132062]
t_total = [26.89973984 26.89973984 26.89973984 26.89973984 26.89973984 26.89973984
 26.89973984 26.89973984 26.89973984 26.89973984]
ene_coms = [0.01841572 0.01316804 0.01087136 0.01098298 0.01076285 0.00700451
 0.01407423 0.01658544 0.01381709 0.01313206]
ene_comp = [1.23114868e-06 9.88665893e-06 9.52861598e-07 3.98512365e-08
 1.42788980e-05 1.40890613e-06 8.57423097e-08 3.34917669e-06
 1.18793811e-06 8.72038771e-07]
ene_total = [0.56832741 0.40665671 0.33550787 0.33892429 0.33257073 0.21619512
 0.43431837 0.51191237 0.42641748 0.40526836]
optimize_network iter = 2 obj = 3.9760986899812876
eta = 0.7749513958050913
freqs = [15299604.60326003 29895929.9034226  13567951.95641502  4711909.06369117
 33434656.00284956 15204594.02478536  6167525.25854111 21171554.26409343
 14795698.00092956 13305681.42193918]
Done!
ene_coms = [0.01841572 0.01316804 0.01087136 0.01098298 0.01076285 0.00700451
 0.01407423 0.01658544 0.01381709 0.01313206]
ene_comp = [1.17975262e-06 9.47392622e-06 9.13083029e-07 3.81875897e-08
 1.36828050e-05 1.35008933e-06 8.21628745e-08 3.20936052e-06
 1.13834593e-06 8.35634266e-07]
ene_total = [0.0184169  0.01317751 0.01087227 0.01098302 0.01077653 0.00700586
 0.01407431 0.01658865 0.01381823 0.0131329 ]
At round 62 energy consumption: 0.12884617519437874
At round 62 eta: 0.7749513958050913
At round 62 a_n: 6.944760344156705
At round 62 local rounds: 8.348520935858948
At round 62 global rounds: 30.858935424198545
gradient difference: 0.5611849427223206
train() client id: f_00000-0-0 loss: 0.979919  [   32/  126]
train() client id: f_00000-0-1 loss: 1.056522  [   64/  126]
train() client id: f_00000-0-2 loss: 1.131425  [   96/  126]
train() client id: f_00000-1-0 loss: 1.039360  [   32/  126]
train() client id: f_00000-1-1 loss: 1.111935  [   64/  126]
train() client id: f_00000-1-2 loss: 1.052193  [   96/  126]
train() client id: f_00000-2-0 loss: 1.095233  [   32/  126]
train() client id: f_00000-2-1 loss: 0.837375  [   64/  126]
train() client id: f_00000-2-2 loss: 1.042092  [   96/  126]
train() client id: f_00000-3-0 loss: 0.937740  [   32/  126]
train() client id: f_00000-3-1 loss: 0.864917  [   64/  126]
train() client id: f_00000-3-2 loss: 0.953901  [   96/  126]
train() client id: f_00000-4-0 loss: 0.828325  [   32/  126]
train() client id: f_00000-4-1 loss: 0.876916  [   64/  126]
train() client id: f_00000-4-2 loss: 1.088760  [   96/  126]
train() client id: f_00000-5-0 loss: 0.693778  [   32/  126]
train() client id: f_00000-5-1 loss: 0.979129  [   64/  126]
train() client id: f_00000-5-2 loss: 0.824136  [   96/  126]
train() client id: f_00000-6-0 loss: 0.921643  [   32/  126]
train() client id: f_00000-6-1 loss: 0.775899  [   64/  126]
train() client id: f_00000-6-2 loss: 0.794315  [   96/  126]
train() client id: f_00000-7-0 loss: 0.867454  [   32/  126]
train() client id: f_00000-7-1 loss: 0.909051  [   64/  126]
train() client id: f_00000-7-2 loss: 0.899357  [   96/  126]
train() client id: f_00001-0-0 loss: 0.382110  [   32/  265]
train() client id: f_00001-0-1 loss: 0.258465  [   64/  265]
train() client id: f_00001-0-2 loss: 0.388391  [   96/  265]
train() client id: f_00001-0-3 loss: 0.213796  [  128/  265]
train() client id: f_00001-0-4 loss: 0.278274  [  160/  265]
train() client id: f_00001-0-5 loss: 0.311449  [  192/  265]
train() client id: f_00001-0-6 loss: 0.163686  [  224/  265]
train() client id: f_00001-0-7 loss: 0.335455  [  256/  265]
train() client id: f_00001-1-0 loss: 0.192431  [   32/  265]
train() client id: f_00001-1-1 loss: 0.417547  [   64/  265]
train() client id: f_00001-1-2 loss: 0.275077  [   96/  265]
train() client id: f_00001-1-3 loss: 0.225809  [  128/  265]
train() client id: f_00001-1-4 loss: 0.380285  [  160/  265]
train() client id: f_00001-1-5 loss: 0.301074  [  192/  265]
train() client id: f_00001-1-6 loss: 0.191749  [  224/  265]
train() client id: f_00001-1-7 loss: 0.208127  [  256/  265]
train() client id: f_00001-2-0 loss: 0.180667  [   32/  265]
train() client id: f_00001-2-1 loss: 0.206335  [   64/  265]
train() client id: f_00001-2-2 loss: 0.182741  [   96/  265]
train() client id: f_00001-2-3 loss: 0.352554  [  128/  265]
train() client id: f_00001-2-4 loss: 0.355368  [  160/  265]
train() client id: f_00001-2-5 loss: 0.256791  [  192/  265]
train() client id: f_00001-2-6 loss: 0.386189  [  224/  265]
train() client id: f_00001-2-7 loss: 0.181459  [  256/  265]
train() client id: f_00001-3-0 loss: 0.257828  [   32/  265]
train() client id: f_00001-3-1 loss: 0.281729  [   64/  265]
train() client id: f_00001-3-2 loss: 0.179939  [   96/  265]
train() client id: f_00001-3-3 loss: 0.471744  [  128/  265]
train() client id: f_00001-3-4 loss: 0.262503  [  160/  265]
train() client id: f_00001-3-5 loss: 0.234754  [  192/  265]
train() client id: f_00001-3-6 loss: 0.296002  [  224/  265]
train() client id: f_00001-3-7 loss: 0.146287  [  256/  265]
train() client id: f_00001-4-0 loss: 0.228505  [   32/  265]
train() client id: f_00001-4-1 loss: 0.269627  [   64/  265]
train() client id: f_00001-4-2 loss: 0.250964  [   96/  265]
train() client id: f_00001-4-3 loss: 0.320271  [  128/  265]
train() client id: f_00001-4-4 loss: 0.213316  [  160/  265]
train() client id: f_00001-4-5 loss: 0.289484  [  192/  265]
train() client id: f_00001-4-6 loss: 0.210754  [  224/  265]
train() client id: f_00001-4-7 loss: 0.317085  [  256/  265]
train() client id: f_00001-5-0 loss: 0.231158  [   32/  265]
train() client id: f_00001-5-1 loss: 0.307627  [   64/  265]
train() client id: f_00001-5-2 loss: 0.311823  [   96/  265]
train() client id: f_00001-5-3 loss: 0.233329  [  128/  265]
train() client id: f_00001-5-4 loss: 0.297510  [  160/  265]
train() client id: f_00001-5-5 loss: 0.165268  [  192/  265]
train() client id: f_00001-5-6 loss: 0.332041  [  224/  265]
train() client id: f_00001-5-7 loss: 0.181579  [  256/  265]
train() client id: f_00001-6-0 loss: 0.185451  [   32/  265]
train() client id: f_00001-6-1 loss: 0.173058  [   64/  265]
train() client id: f_00001-6-2 loss: 0.322290  [   96/  265]
train() client id: f_00001-6-3 loss: 0.148584  [  128/  265]
train() client id: f_00001-6-4 loss: 0.379015  [  160/  265]
train() client id: f_00001-6-5 loss: 0.239171  [  192/  265]
train() client id: f_00001-6-6 loss: 0.247254  [  224/  265]
train() client id: f_00001-6-7 loss: 0.255575  [  256/  265]
train() client id: f_00001-7-0 loss: 0.233817  [   32/  265]
train() client id: f_00001-7-1 loss: 0.148659  [   64/  265]
train() client id: f_00001-7-2 loss: 0.170194  [   96/  265]
train() client id: f_00001-7-3 loss: 0.292389  [  128/  265]
train() client id: f_00001-7-4 loss: 0.259337  [  160/  265]
train() client id: f_00001-7-5 loss: 0.217178  [  192/  265]
train() client id: f_00001-7-6 loss: 0.251661  [  224/  265]
train() client id: f_00001-7-7 loss: 0.431529  [  256/  265]
train() client id: f_00002-0-0 loss: 1.060193  [   32/  124]
train() client id: f_00002-0-1 loss: 0.971705  [   64/  124]
train() client id: f_00002-0-2 loss: 1.008408  [   96/  124]
train() client id: f_00002-1-0 loss: 1.114043  [   32/  124]
train() client id: f_00002-1-1 loss: 0.915155  [   64/  124]
train() client id: f_00002-1-2 loss: 0.937164  [   96/  124]
train() client id: f_00002-2-0 loss: 1.055493  [   32/  124]
train() client id: f_00002-2-1 loss: 1.046635  [   64/  124]
train() client id: f_00002-2-2 loss: 0.856475  [   96/  124]
train() client id: f_00002-3-0 loss: 0.867651  [   32/  124]
train() client id: f_00002-3-1 loss: 1.028000  [   64/  124]
train() client id: f_00002-3-2 loss: 0.972969  [   96/  124]
train() client id: f_00002-4-0 loss: 1.047411  [   32/  124]
train() client id: f_00002-4-1 loss: 0.991566  [   64/  124]
train() client id: f_00002-4-2 loss: 0.970759  [   96/  124]
train() client id: f_00002-5-0 loss: 0.719017  [   32/  124]
train() client id: f_00002-5-1 loss: 1.093971  [   64/  124]
train() client id: f_00002-5-2 loss: 0.945862  [   96/  124]
train() client id: f_00002-6-0 loss: 0.908569  [   32/  124]
train() client id: f_00002-6-1 loss: 0.849035  [   64/  124]
train() client id: f_00002-6-2 loss: 0.928304  [   96/  124]
train() client id: f_00002-7-0 loss: 1.037644  [   32/  124]
train() client id: f_00002-7-1 loss: 0.944838  [   64/  124]
train() client id: f_00002-7-2 loss: 0.684471  [   96/  124]
train() client id: f_00003-0-0 loss: 0.347812  [   32/   43]
train() client id: f_00003-1-0 loss: 0.545862  [   32/   43]
train() client id: f_00003-2-0 loss: 0.439095  [   32/   43]
train() client id: f_00003-3-0 loss: 0.475599  [   32/   43]
train() client id: f_00003-4-0 loss: 0.575222  [   32/   43]
train() client id: f_00003-5-0 loss: 0.516479  [   32/   43]
train() client id: f_00003-6-0 loss: 0.301919  [   32/   43]
train() client id: f_00003-7-0 loss: 0.491001  [   32/   43]
train() client id: f_00004-0-0 loss: 0.719443  [   32/  306]
train() client id: f_00004-0-1 loss: 0.727231  [   64/  306]
train() client id: f_00004-0-2 loss: 0.875453  [   96/  306]
train() client id: f_00004-0-3 loss: 0.740708  [  128/  306]
train() client id: f_00004-0-4 loss: 0.781170  [  160/  306]
train() client id: f_00004-0-5 loss: 0.928737  [  192/  306]
train() client id: f_00004-0-6 loss: 0.897440  [  224/  306]
train() client id: f_00004-0-7 loss: 0.876619  [  256/  306]
train() client id: f_00004-0-8 loss: 0.852373  [  288/  306]
train() client id: f_00004-1-0 loss: 0.786628  [   32/  306]
train() client id: f_00004-1-1 loss: 0.863942  [   64/  306]
train() client id: f_00004-1-2 loss: 1.021106  [   96/  306]
train() client id: f_00004-1-3 loss: 0.791995  [  128/  306]
train() client id: f_00004-1-4 loss: 0.732023  [  160/  306]
train() client id: f_00004-1-5 loss: 0.849667  [  192/  306]
train() client id: f_00004-1-6 loss: 0.751399  [  224/  306]
train() client id: f_00004-1-7 loss: 0.729657  [  256/  306]
train() client id: f_00004-1-8 loss: 1.005635  [  288/  306]
train() client id: f_00004-2-0 loss: 0.765623  [   32/  306]
train() client id: f_00004-2-1 loss: 0.838695  [   64/  306]
train() client id: f_00004-2-2 loss: 0.687914  [   96/  306]
train() client id: f_00004-2-3 loss: 0.862816  [  128/  306]
train() client id: f_00004-2-4 loss: 0.896263  [  160/  306]
train() client id: f_00004-2-5 loss: 0.914853  [  192/  306]
train() client id: f_00004-2-6 loss: 0.950257  [  224/  306]
train() client id: f_00004-2-7 loss: 0.765361  [  256/  306]
train() client id: f_00004-2-8 loss: 0.783772  [  288/  306]
train() client id: f_00004-3-0 loss: 0.853808  [   32/  306]
train() client id: f_00004-3-1 loss: 0.772041  [   64/  306]
train() client id: f_00004-3-2 loss: 0.825004  [   96/  306]
train() client id: f_00004-3-3 loss: 0.780667  [  128/  306]
train() client id: f_00004-3-4 loss: 0.866252  [  160/  306]
train() client id: f_00004-3-5 loss: 0.855269  [  192/  306]
train() client id: f_00004-3-6 loss: 0.852133  [  224/  306]
train() client id: f_00004-3-7 loss: 0.926951  [  256/  306]
train() client id: f_00004-3-8 loss: 0.721094  [  288/  306]
train() client id: f_00004-4-0 loss: 0.639859  [   32/  306]
train() client id: f_00004-4-1 loss: 0.898275  [   64/  306]
train() client id: f_00004-4-2 loss: 0.967176  [   96/  306]
train() client id: f_00004-4-3 loss: 0.955936  [  128/  306]
train() client id: f_00004-4-4 loss: 0.861225  [  160/  306]
train() client id: f_00004-4-5 loss: 0.743250  [  192/  306]
train() client id: f_00004-4-6 loss: 0.926015  [  224/  306]
train() client id: f_00004-4-7 loss: 0.771879  [  256/  306]
train() client id: f_00004-4-8 loss: 0.767604  [  288/  306]
train() client id: f_00004-5-0 loss: 0.809541  [   32/  306]
train() client id: f_00004-5-1 loss: 0.833234  [   64/  306]
train() client id: f_00004-5-2 loss: 0.819031  [   96/  306]
train() client id: f_00004-5-3 loss: 0.890620  [  128/  306]
train() client id: f_00004-5-4 loss: 0.736177  [  160/  306]
train() client id: f_00004-5-5 loss: 0.754003  [  192/  306]
train() client id: f_00004-5-6 loss: 0.997598  [  224/  306]
train() client id: f_00004-5-7 loss: 0.751353  [  256/  306]
train() client id: f_00004-5-8 loss: 0.874884  [  288/  306]
train() client id: f_00004-6-0 loss: 0.907172  [   32/  306]
train() client id: f_00004-6-1 loss: 0.878181  [   64/  306]
train() client id: f_00004-6-2 loss: 0.878992  [   96/  306]
train() client id: f_00004-6-3 loss: 0.810412  [  128/  306]
train() client id: f_00004-6-4 loss: 0.744830  [  160/  306]
train() client id: f_00004-6-5 loss: 0.827817  [  192/  306]
train() client id: f_00004-6-6 loss: 0.771597  [  224/  306]
train() client id: f_00004-6-7 loss: 0.777946  [  256/  306]
train() client id: f_00004-6-8 loss: 0.959931  [  288/  306]
train() client id: f_00004-7-0 loss: 0.842087  [   32/  306]
train() client id: f_00004-7-1 loss: 0.789685  [   64/  306]
train() client id: f_00004-7-2 loss: 0.889239  [   96/  306]
train() client id: f_00004-7-3 loss: 0.911784  [  128/  306]
train() client id: f_00004-7-4 loss: 0.915161  [  160/  306]
train() client id: f_00004-7-5 loss: 0.814470  [  192/  306]
train() client id: f_00004-7-6 loss: 0.800105  [  224/  306]
train() client id: f_00004-7-7 loss: 0.806067  [  256/  306]
train() client id: f_00004-7-8 loss: 0.831618  [  288/  306]
train() client id: f_00005-0-0 loss: 0.622276  [   32/  146]
train() client id: f_00005-0-1 loss: 0.166590  [   64/  146]
train() client id: f_00005-0-2 loss: 0.446667  [   96/  146]
train() client id: f_00005-0-3 loss: 0.508774  [  128/  146]
train() client id: f_00005-1-0 loss: 0.290922  [   32/  146]
train() client id: f_00005-1-1 loss: 0.367850  [   64/  146]
train() client id: f_00005-1-2 loss: 0.524901  [   96/  146]
train() client id: f_00005-1-3 loss: 0.447693  [  128/  146]
train() client id: f_00005-2-0 loss: 0.118628  [   32/  146]
train() client id: f_00005-2-1 loss: 0.462663  [   64/  146]
train() client id: f_00005-2-2 loss: 0.695974  [   96/  146]
train() client id: f_00005-2-3 loss: 0.546821  [  128/  146]
train() client id: f_00005-3-0 loss: 0.456833  [   32/  146]
train() client id: f_00005-3-1 loss: 0.438038  [   64/  146]
train() client id: f_00005-3-2 loss: 0.363691  [   96/  146]
train() client id: f_00005-3-3 loss: 0.337782  [  128/  146]
train() client id: f_00005-4-0 loss: 0.231569  [   32/  146]
train() client id: f_00005-4-1 loss: 0.543328  [   64/  146]
train() client id: f_00005-4-2 loss: 0.369978  [   96/  146]
train() client id: f_00005-4-3 loss: 0.635816  [  128/  146]
train() client id: f_00005-5-0 loss: 0.583608  [   32/  146]
train() client id: f_00005-5-1 loss: 0.335059  [   64/  146]
train() client id: f_00005-5-2 loss: 0.691017  [   96/  146]
train() client id: f_00005-5-3 loss: 0.164947  [  128/  146]
train() client id: f_00005-6-0 loss: 0.431209  [   32/  146]
train() client id: f_00005-6-1 loss: 0.746257  [   64/  146]
train() client id: f_00005-6-2 loss: 0.067952  [   96/  146]
train() client id: f_00005-6-3 loss: 0.408577  [  128/  146]
train() client id: f_00005-7-0 loss: 0.365100  [   32/  146]
train() client id: f_00005-7-1 loss: 0.278544  [   64/  146]
train() client id: f_00005-7-2 loss: 0.576559  [   96/  146]
train() client id: f_00005-7-3 loss: 0.485042  [  128/  146]
train() client id: f_00006-0-0 loss: 0.529568  [   32/   54]
train() client id: f_00006-1-0 loss: 0.584468  [   32/   54]
train() client id: f_00006-2-0 loss: 0.552843  [   32/   54]
train() client id: f_00006-3-0 loss: 0.557587  [   32/   54]
train() client id: f_00006-4-0 loss: 0.519229  [   32/   54]
train() client id: f_00006-5-0 loss: 0.477651  [   32/   54]
train() client id: f_00006-6-0 loss: 0.558298  [   32/   54]
train() client id: f_00006-7-0 loss: 0.513163  [   32/   54]
train() client id: f_00007-0-0 loss: 0.522241  [   32/  179]
train() client id: f_00007-0-1 loss: 0.727914  [   64/  179]
train() client id: f_00007-0-2 loss: 0.558306  [   96/  179]
train() client id: f_00007-0-3 loss: 0.687662  [  128/  179]
train() client id: f_00007-0-4 loss: 0.668444  [  160/  179]
train() client id: f_00007-1-0 loss: 0.741885  [   32/  179]
train() client id: f_00007-1-1 loss: 0.612522  [   64/  179]
train() client id: f_00007-1-2 loss: 0.585319  [   96/  179]
train() client id: f_00007-1-3 loss: 0.880646  [  128/  179]
train() client id: f_00007-1-4 loss: 0.496988  [  160/  179]
train() client id: f_00007-2-0 loss: 0.778338  [   32/  179]
train() client id: f_00007-2-1 loss: 0.654195  [   64/  179]
train() client id: f_00007-2-2 loss: 0.788650  [   96/  179]
train() client id: f_00007-2-3 loss: 0.476657  [  128/  179]
train() client id: f_00007-2-4 loss: 0.522676  [  160/  179]
train() client id: f_00007-3-0 loss: 0.848136  [   32/  179]
train() client id: f_00007-3-1 loss: 0.697544  [   64/  179]
train() client id: f_00007-3-2 loss: 0.518364  [   96/  179]
train() client id: f_00007-3-3 loss: 0.430103  [  128/  179]
train() client id: f_00007-3-4 loss: 0.800162  [  160/  179]
train() client id: f_00007-4-0 loss: 0.551724  [   32/  179]
train() client id: f_00007-4-1 loss: 0.751750  [   64/  179]
train() client id: f_00007-4-2 loss: 0.666495  [   96/  179]
train() client id: f_00007-4-3 loss: 0.580159  [  128/  179]
train() client id: f_00007-4-4 loss: 0.706662  [  160/  179]
train() client id: f_00007-5-0 loss: 0.581200  [   32/  179]
train() client id: f_00007-5-1 loss: 0.602689  [   64/  179]
train() client id: f_00007-5-2 loss: 0.695727  [   96/  179]
train() client id: f_00007-5-3 loss: 0.561684  [  128/  179]
train() client id: f_00007-5-4 loss: 0.622914  [  160/  179]
train() client id: f_00007-6-0 loss: 0.753925  [   32/  179]
train() client id: f_00007-6-1 loss: 0.544186  [   64/  179]
train() client id: f_00007-6-2 loss: 0.568542  [   96/  179]
train() client id: f_00007-6-3 loss: 0.747393  [  128/  179]
train() client id: f_00007-6-4 loss: 0.642548  [  160/  179]
train() client id: f_00007-7-0 loss: 0.543185  [   32/  179]
train() client id: f_00007-7-1 loss: 0.665369  [   64/  179]
train() client id: f_00007-7-2 loss: 0.492254  [   96/  179]
train() client id: f_00007-7-3 loss: 0.537154  [  128/  179]
train() client id: f_00007-7-4 loss: 0.962422  [  160/  179]
train() client id: f_00008-0-0 loss: 0.890200  [   32/  130]
train() client id: f_00008-0-1 loss: 0.841516  [   64/  130]
train() client id: f_00008-0-2 loss: 0.727650  [   96/  130]
train() client id: f_00008-0-3 loss: 0.663740  [  128/  130]
train() client id: f_00008-1-0 loss: 0.742191  [   32/  130]
train() client id: f_00008-1-1 loss: 0.879779  [   64/  130]
train() client id: f_00008-1-2 loss: 0.734787  [   96/  130]
train() client id: f_00008-1-3 loss: 0.760342  [  128/  130]
train() client id: f_00008-2-0 loss: 0.715438  [   32/  130]
train() client id: f_00008-2-1 loss: 0.755228  [   64/  130]
train() client id: f_00008-2-2 loss: 0.791211  [   96/  130]
train() client id: f_00008-2-3 loss: 0.862084  [  128/  130]
train() client id: f_00008-3-0 loss: 0.760377  [   32/  130]
train() client id: f_00008-3-1 loss: 0.890432  [   64/  130]
train() client id: f_00008-3-2 loss: 0.760214  [   96/  130]
train() client id: f_00008-3-3 loss: 0.708928  [  128/  130]
train() client id: f_00008-4-0 loss: 0.794487  [   32/  130]
train() client id: f_00008-4-1 loss: 0.698044  [   64/  130]
train() client id: f_00008-4-2 loss: 0.788380  [   96/  130]
train() client id: f_00008-4-3 loss: 0.845372  [  128/  130]
train() client id: f_00008-5-0 loss: 0.806510  [   32/  130]
train() client id: f_00008-5-1 loss: 0.798023  [   64/  130]
train() client id: f_00008-5-2 loss: 0.716273  [   96/  130]
train() client id: f_00008-5-3 loss: 0.735658  [  128/  130]
train() client id: f_00008-6-0 loss: 0.743623  [   32/  130]
train() client id: f_00008-6-1 loss: 0.847980  [   64/  130]
train() client id: f_00008-6-2 loss: 0.706673  [   96/  130]
train() client id: f_00008-6-3 loss: 0.807034  [  128/  130]
train() client id: f_00008-7-0 loss: 0.844967  [   32/  130]
train() client id: f_00008-7-1 loss: 0.714714  [   64/  130]
train() client id: f_00008-7-2 loss: 0.773966  [   96/  130]
train() client id: f_00008-7-3 loss: 0.778117  [  128/  130]
train() client id: f_00009-0-0 loss: 0.977740  [   32/  118]
train() client id: f_00009-0-1 loss: 0.716924  [   64/  118]
train() client id: f_00009-0-2 loss: 0.883027  [   96/  118]
train() client id: f_00009-1-0 loss: 0.785018  [   32/  118]
train() client id: f_00009-1-1 loss: 0.924652  [   64/  118]
train() client id: f_00009-1-2 loss: 0.859412  [   96/  118]
train() client id: f_00009-2-0 loss: 0.842895  [   32/  118]
train() client id: f_00009-2-1 loss: 0.741097  [   64/  118]
train() client id: f_00009-2-2 loss: 0.769722  [   96/  118]
train() client id: f_00009-3-0 loss: 0.827587  [   32/  118]
train() client id: f_00009-3-1 loss: 0.932282  [   64/  118]
train() client id: f_00009-3-2 loss: 0.627686  [   96/  118]
train() client id: f_00009-4-0 loss: 0.618086  [   32/  118]
train() client id: f_00009-4-1 loss: 0.893679  [   64/  118]
train() client id: f_00009-4-2 loss: 0.716333  [   96/  118]
train() client id: f_00009-5-0 loss: 0.788337  [   32/  118]
train() client id: f_00009-5-1 loss: 0.663398  [   64/  118]
train() client id: f_00009-5-2 loss: 0.882441  [   96/  118]
train() client id: f_00009-6-0 loss: 0.500311  [   32/  118]
train() client id: f_00009-6-1 loss: 0.877032  [   64/  118]
train() client id: f_00009-6-2 loss: 0.861994  [   96/  118]
train() client id: f_00009-7-0 loss: 0.798353  [   32/  118]
train() client id: f_00009-7-1 loss: 0.709294  [   64/  118]
train() client id: f_00009-7-2 loss: 0.728438  [   96/  118]
At round 62 accuracy: 0.6472148541114059
At round 62 training accuracy: 0.5928906773977196
At round 62 training loss: 0.822260297417704
update_location
xs = -4.528292 211.001589 220.045120 -220.943528 109.896481 -145.217951 -262.215960 283.375741 -1.680116 204.695607 
ys = 297.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -227.154970 4.001482 
xs mean: 39.442869159412865
ys mean: 9.371751218646875
dists_uav = 313.973087 234.016356 241.705603 243.565826 148.585527 176.341069 280.654547 301.113920 248.197911 227.851494 
uav_gains = -116.841430 -110.005291 -110.608436 -110.759413 -104.303428 -106.194589 -114.039205 -115.822978 -111.143551 -109.546040 
uav_gains_db_mean: -110.92643589463206
dists_bs = 209.972405 417.635807 431.538151 159.227474 334.692190 174.742488 198.330813 497.808477 437.913692 416.424360 
bs_gains = -104.587601 -112.949396 -113.347597 -101.223563 -110.257135 -102.354218 -103.893985 -115.084803 -113.525938 -112.914071 
bs_gains_db_mean: -109.01383060153395
Round 63
-------------------------------
ene_coms = [0.01913086 0.01331344 0.01114799 0.01126778 0.01087822 0.00698397
 0.01459878 0.01723647 0.01396717 0.01327516]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.66181591 5.41872276 2.56835299 0.94000137 6.22711426 2.98306884
 1.1834582  3.71516975 2.70783166 2.46186111]
obj_prev = 30.867396857542005
eta_min = 4.89659286679812e-35	eta_max = 0.9527124541796904
af = 6.446787169041255	bf = 0.8701708226872302	zeta = 7.091465885945381	eta = 0.9090909090909091
af = 6.446787169041255	bf = 0.8701708226872302	zeta = 16.330767936216436	eta = 0.3947632587898293
af = 6.446787169041255	bf = 0.8701708226872302	zeta = 11.33814163529907	eta = 0.5685929296358808
af = 6.446787169041255	bf = 0.8701708226872302	zeta = 10.45408389743025	eta = 0.6166764330852519
af = 6.446787169041255	bf = 0.8701708226872302	zeta = 10.400136565246356	eta = 0.6198752418871285
af = 6.446787169041255	bf = 0.8701708226872302	zeta = 10.399909969122263	eta = 0.619888747900897
eta = 0.619888747900897
ene_coms = [0.01913086 0.01331344 0.01114799 0.01126778 0.01087822 0.00698397
 0.01459878 0.01723647 0.01396717 0.01327516]
ene_comp = [0.03946118 0.08299374 0.03883481 0.01346691 0.09583429 0.04572486
 0.01691193 0.05605993 0.04071391 0.03695571]
ene_total = [1.01769464 1.67277522 0.86815939 0.42962081 1.85350701 0.91550816
 0.5473148  1.27309705 0.94976469 0.8724682 ]
ti_comp = [1.35451921 1.4126934  1.4343479  1.43315002 1.43704557 1.47598812
 1.39983994 1.37346311 1.40615607 1.41307614]
ti_coms = [0.19130858 0.13313439 0.11147989 0.11267777 0.10878222 0.06983967
 0.14598785 0.17236468 0.13967172 0.13275165]
t_total = [26.84973564 26.84973564 26.84973564 26.84973564 26.84973564 26.84973564
 26.84973564 26.84973564 26.84973564 26.84973564]
ene_coms = [0.01913086 0.01331344 0.01114799 0.01126778 0.01087822 0.00698397
 0.01459878 0.01723647 0.01396717 0.01327516]
ene_comp = [2.09324362e-06 1.79027714e-05 1.77923998e-06 7.43191161e-08
 2.66380038e-05 2.74265588e-06 1.54277642e-07 5.83718654e-06
 2.13324865e-06 1.57976822e-06]
ene_total = [0.33232336 0.23155425 0.19366215 0.19571315 0.1894083  0.1213533
 0.25357136 0.29948442 0.24263514 0.23060595]
optimize_network iter = 0 obj = 2.2903113810433116
eta = 0.619888747900897
freqs = [14566488.43973643 29374294.78508725 13537444.03678362  4698360.02229052
 33344205.784364   15489574.38667754  6040666.63391748 20408238.61108695
 14477024.73830728 13076331.89695712]
eta_min = 0.619888747900898	eta_max = 0.7791437399226808
af = 0.0008412614465849944	bf = 0.8701708226872302	zeta = 0.0009253875912434939	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01913086 0.01331344 0.01114799 0.01126778 0.01087822 0.00698397
 0.01459878 0.01723647 0.01396717 0.01327516]
ene_comp = [4.17191680e-07 3.56809270e-06 3.54609520e-07 1.48120919e-08
 5.30905884e-06 5.46622095e-07 3.07481404e-08 1.16337422e-06
 4.25164840e-07 3.14854014e-07]
ene_total = [1.38939662 0.96713911 0.80964131 0.81831616 0.79040947 0.50724582
 1.06022961 1.25187194 1.01438777 0.96412322]
ti_comp = [0.70686463 0.76503882 0.78669332 0.78549544 0.78939099 0.82833354
 0.75218537 0.72580853 0.75850149 0.76542156]
ti_coms = [0.19130858 0.13313439 0.11147989 0.11267777 0.10878222 0.06983967
 0.14598785 0.17236468 0.13967172 0.13275165]
t_total = [26.84973564 26.84973564 26.84973564 26.84973564 26.84973564 26.84973564
 26.84973564 26.84973564 26.84973564 26.84973564]
ene_coms = [0.01913086 0.01331344 0.01114799 0.01126778 0.01087822 0.00698397
 0.01459878 0.01723647 0.01396717 0.01327516]
ene_comp = [1.09237276e-06 8.67565753e-06 8.40593164e-07 3.51600718e-08
 1.25461846e-05 1.23759856e-06 7.59387152e-08 2.97061714e-06
 1.04195485e-06 7.65202471e-07]
ene_total = [0.57192511 0.39824752 0.33327995 0.33683678 0.32556554 0.20881372
 0.4364142  0.5153509  0.41756182 0.3968669 ]
optimize_network iter = 1 obj = 3.9408624581116536
eta = 0.7791437399226808
freqs = [14566488.43973642 28306291.44254436 12880620.81074022  4473478.55726214
 31677422.31285744 14403504.79003056  5866640.19764572 20153549.04415841
 14005785.95886459 12598008.26823561]
eta_min = 0.7791437399226822	eta_max = 0.7791437399226783
af = 0.0007746493761905924	bf = 0.8701708226872302	zeta = 0.0008521143138096517	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01913086 0.01331344 0.01114799 0.01126778 0.01087822 0.00698397
 0.01459878 0.01723647 0.01396717 0.01327516]
ene_comp = [4.17191680e-07 3.31334897e-06 3.21033706e-07 1.34280989e-08
 4.79155472e-06 4.72655347e-07 2.90020051e-08 1.13451818e-06
 3.97936412e-07 2.92240998e-07]
ene_total = [1.38939662 0.96712061 0.80963887 0.81831606 0.79037189 0.50724045
 1.06022948 1.25186985 1.0143858  0.96412158]
ti_comp = [0.70686463 0.76503882 0.78669332 0.78549544 0.78939099 0.82833354
 0.75218537 0.72580853 0.75850149 0.76542156]
ti_coms = [0.19130858 0.13313439 0.11147989 0.11267777 0.10878222 0.06983967
 0.14598785 0.17236468 0.13967172 0.13275165]
t_total = [26.84973564 26.84973564 26.84973564 26.84973564 26.84973564 26.84973564
 26.84973564 26.84973564 26.84973564 26.84973564]
ene_coms = [0.01913086 0.01331344 0.01114799 0.01126778 0.01087822 0.00698397
 0.01459878 0.01723647 0.01396717 0.01327516]
ene_comp = [1.09237276e-06 8.67565753e-06 8.40593164e-07 3.51600718e-08
 1.25461846e-05 1.23759856e-06 7.59387152e-08 2.97061714e-06
 1.04195485e-06 7.65202471e-07]
ene_total = [0.57192511 0.39824752 0.33327995 0.33683678 0.32556554 0.20881372
 0.4364142  0.5153509  0.41756182 0.3968669 ]
optimize_network iter = 2 obj = 3.9408624581116083
eta = 0.7791437399226783
freqs = [14566488.4397364  28306291.44254434 12880620.81074022  4473478.55726214
 31677422.31285744 14403504.79003057  5866640.19764572 20153549.04415838
 14005785.95886458 12598008.2682356 ]
Done!
ene_coms = [0.01913086 0.01331344 0.01114799 0.01126778 0.01087822 0.00698397
 0.01459878 0.01723647 0.01396717 0.01327516]
ene_comp = [1.06940023e-06 8.49320903e-06 8.22915547e-07 3.44206579e-08
 1.22823392e-05 1.21157195e-06 7.43417292e-08 2.90814526e-06
 1.02004261e-06 7.49110314e-07]
ene_total = [0.01913193 0.01332193 0.01114881 0.01126781 0.0108905  0.00698518
 0.01459886 0.01723938 0.01396819 0.01327591]
At round 63 energy consumption: 0.13182850675553356
At round 63 eta: 0.7791437399226783
At round 63 a_n: 6.602214497187386
At round 63 local rounds: 8.17185352867806
At round 63 global rounds: 29.89371682231671
gradient difference: 0.6672314405441284
train() client id: f_00000-0-0 loss: 0.955174  [   32/  126]
train() client id: f_00000-0-1 loss: 0.890231  [   64/  126]
train() client id: f_00000-0-2 loss: 0.747872  [   96/  126]
train() client id: f_00000-1-0 loss: 0.766365  [   32/  126]
train() client id: f_00000-1-1 loss: 0.800723  [   64/  126]
train() client id: f_00000-1-2 loss: 0.936318  [   96/  126]
train() client id: f_00000-2-0 loss: 0.942796  [   32/  126]
train() client id: f_00000-2-1 loss: 0.832537  [   64/  126]
train() client id: f_00000-2-2 loss: 0.785554  [   96/  126]
train() client id: f_00000-3-0 loss: 0.780838  [   32/  126]
train() client id: f_00000-3-1 loss: 0.777529  [   64/  126]
train() client id: f_00000-3-2 loss: 0.808033  [   96/  126]
train() client id: f_00000-4-0 loss: 0.873642  [   32/  126]
train() client id: f_00000-4-1 loss: 0.722392  [   64/  126]
train() client id: f_00000-4-2 loss: 0.863333  [   96/  126]
train() client id: f_00000-5-0 loss: 0.752582  [   32/  126]
train() client id: f_00000-5-1 loss: 0.808186  [   64/  126]
train() client id: f_00000-5-2 loss: 0.859133  [   96/  126]
train() client id: f_00000-6-0 loss: 0.652104  [   32/  126]
train() client id: f_00000-6-1 loss: 0.822918  [   64/  126]
train() client id: f_00000-6-2 loss: 0.889474  [   96/  126]
train() client id: f_00000-7-0 loss: 0.771868  [   32/  126]
train() client id: f_00000-7-1 loss: 0.826283  [   64/  126]
train() client id: f_00000-7-2 loss: 0.899189  [   96/  126]
train() client id: f_00001-0-0 loss: 0.371072  [   32/  265]
train() client id: f_00001-0-1 loss: 0.315465  [   64/  265]
train() client id: f_00001-0-2 loss: 0.307260  [   96/  265]
train() client id: f_00001-0-3 loss: 0.374667  [  128/  265]
train() client id: f_00001-0-4 loss: 0.323178  [  160/  265]
train() client id: f_00001-0-5 loss: 0.594941  [  192/  265]
train() client id: f_00001-0-6 loss: 0.414773  [  224/  265]
train() client id: f_00001-0-7 loss: 0.276059  [  256/  265]
train() client id: f_00001-1-0 loss: 0.529605  [   32/  265]
train() client id: f_00001-1-1 loss: 0.315285  [   64/  265]
train() client id: f_00001-1-2 loss: 0.338491  [   96/  265]
train() client id: f_00001-1-3 loss: 0.293305  [  128/  265]
train() client id: f_00001-1-4 loss: 0.333613  [  160/  265]
train() client id: f_00001-1-5 loss: 0.376304  [  192/  265]
train() client id: f_00001-1-6 loss: 0.456179  [  224/  265]
train() client id: f_00001-1-7 loss: 0.287682  [  256/  265]
train() client id: f_00001-2-0 loss: 0.430006  [   32/  265]
train() client id: f_00001-2-1 loss: 0.420472  [   64/  265]
train() client id: f_00001-2-2 loss: 0.275971  [   96/  265]
train() client id: f_00001-2-3 loss: 0.377688  [  128/  265]
train() client id: f_00001-2-4 loss: 0.271646  [  160/  265]
train() client id: f_00001-2-5 loss: 0.328239  [  192/  265]
train() client id: f_00001-2-6 loss: 0.456992  [  224/  265]
train() client id: f_00001-2-7 loss: 0.259544  [  256/  265]
train() client id: f_00001-3-0 loss: 0.456804  [   32/  265]
train() client id: f_00001-3-1 loss: 0.331031  [   64/  265]
train() client id: f_00001-3-2 loss: 0.393845  [   96/  265]
train() client id: f_00001-3-3 loss: 0.350250  [  128/  265]
train() client id: f_00001-3-4 loss: 0.325756  [  160/  265]
train() client id: f_00001-3-5 loss: 0.361492  [  192/  265]
train() client id: f_00001-3-6 loss: 0.240785  [  224/  265]
train() client id: f_00001-3-7 loss: 0.404745  [  256/  265]
train() client id: f_00001-4-0 loss: 0.269062  [   32/  265]
train() client id: f_00001-4-1 loss: 0.233051  [   64/  265]
train() client id: f_00001-4-2 loss: 0.297088  [   96/  265]
train() client id: f_00001-4-3 loss: 0.425447  [  128/  265]
train() client id: f_00001-4-4 loss: 0.490775  [  160/  265]
train() client id: f_00001-4-5 loss: 0.490864  [  192/  265]
train() client id: f_00001-4-6 loss: 0.324613  [  224/  265]
train() client id: f_00001-4-7 loss: 0.313715  [  256/  265]
train() client id: f_00001-5-0 loss: 0.317519  [   32/  265]
train() client id: f_00001-5-1 loss: 0.361742  [   64/  265]
train() client id: f_00001-5-2 loss: 0.250541  [   96/  265]
train() client id: f_00001-5-3 loss: 0.283542  [  128/  265]
train() client id: f_00001-5-4 loss: 0.360783  [  160/  265]
train() client id: f_00001-5-5 loss: 0.477179  [  192/  265]
train() client id: f_00001-5-6 loss: 0.403357  [  224/  265]
train() client id: f_00001-5-7 loss: 0.366033  [  256/  265]
train() client id: f_00001-6-0 loss: 0.430608  [   32/  265]
train() client id: f_00001-6-1 loss: 0.454232  [   64/  265]
train() client id: f_00001-6-2 loss: 0.271001  [   96/  265]
train() client id: f_00001-6-3 loss: 0.386658  [  128/  265]
train() client id: f_00001-6-4 loss: 0.270549  [  160/  265]
train() client id: f_00001-6-5 loss: 0.415515  [  192/  265]
train() client id: f_00001-6-6 loss: 0.325167  [  224/  265]
train() client id: f_00001-6-7 loss: 0.264861  [  256/  265]
train() client id: f_00001-7-0 loss: 0.427709  [   32/  265]
train() client id: f_00001-7-1 loss: 0.250568  [   64/  265]
train() client id: f_00001-7-2 loss: 0.352208  [   96/  265]
train() client id: f_00001-7-3 loss: 0.442392  [  128/  265]
train() client id: f_00001-7-4 loss: 0.257954  [  160/  265]
train() client id: f_00001-7-5 loss: 0.270805  [  192/  265]
train() client id: f_00001-7-6 loss: 0.394545  [  224/  265]
train() client id: f_00001-7-7 loss: 0.421881  [  256/  265]
train() client id: f_00002-0-0 loss: 1.205960  [   32/  124]
train() client id: f_00002-0-1 loss: 1.385791  [   64/  124]
train() client id: f_00002-0-2 loss: 1.185439  [   96/  124]
train() client id: f_00002-1-0 loss: 1.406377  [   32/  124]
train() client id: f_00002-1-1 loss: 1.373837  [   64/  124]
train() client id: f_00002-1-2 loss: 1.209104  [   96/  124]
train() client id: f_00002-2-0 loss: 1.109817  [   32/  124]
train() client id: f_00002-2-1 loss: 1.465874  [   64/  124]
train() client id: f_00002-2-2 loss: 1.195057  [   96/  124]
train() client id: f_00002-3-0 loss: 1.180963  [   32/  124]
train() client id: f_00002-3-1 loss: 1.289758  [   64/  124]
train() client id: f_00002-3-2 loss: 1.201254  [   96/  124]
train() client id: f_00002-4-0 loss: 1.056355  [   32/  124]
train() client id: f_00002-4-1 loss: 1.327514  [   64/  124]
train() client id: f_00002-4-2 loss: 1.275253  [   96/  124]
train() client id: f_00002-5-0 loss: 1.215599  [   32/  124]
train() client id: f_00002-5-1 loss: 1.154541  [   64/  124]
train() client id: f_00002-5-2 loss: 1.446364  [   96/  124]
train() client id: f_00002-6-0 loss: 1.356574  [   32/  124]
train() client id: f_00002-6-1 loss: 1.284756  [   64/  124]
train() client id: f_00002-6-2 loss: 1.020608  [   96/  124]
train() client id: f_00002-7-0 loss: 1.227001  [   32/  124]
train() client id: f_00002-7-1 loss: 1.130117  [   64/  124]
train() client id: f_00002-7-2 loss: 1.231909  [   96/  124]
train() client id: f_00003-0-0 loss: 0.391154  [   32/   43]
train() client id: f_00003-1-0 loss: 0.771934  [   32/   43]
train() client id: f_00003-2-0 loss: 0.602926  [   32/   43]
train() client id: f_00003-3-0 loss: 0.542156  [   32/   43]
train() client id: f_00003-4-0 loss: 0.691029  [   32/   43]
train() client id: f_00003-5-0 loss: 0.333071  [   32/   43]
train() client id: f_00003-6-0 loss: 0.694574  [   32/   43]
train() client id: f_00003-7-0 loss: 0.392493  [   32/   43]
train() client id: f_00004-0-0 loss: 0.756121  [   32/  306]
train() client id: f_00004-0-1 loss: 0.903884  [   64/  306]
train() client id: f_00004-0-2 loss: 0.863647  [   96/  306]
train() client id: f_00004-0-3 loss: 0.780044  [  128/  306]
train() client id: f_00004-0-4 loss: 0.722260  [  160/  306]
train() client id: f_00004-0-5 loss: 0.967669  [  192/  306]
train() client id: f_00004-0-6 loss: 0.969256  [  224/  306]
train() client id: f_00004-0-7 loss: 0.787253  [  256/  306]
train() client id: f_00004-0-8 loss: 0.875162  [  288/  306]
train() client id: f_00004-1-0 loss: 0.806069  [   32/  306]
train() client id: f_00004-1-1 loss: 0.705424  [   64/  306]
train() client id: f_00004-1-2 loss: 0.792219  [   96/  306]
train() client id: f_00004-1-3 loss: 0.841582  [  128/  306]
train() client id: f_00004-1-4 loss: 1.044268  [  160/  306]
train() client id: f_00004-1-5 loss: 0.727005  [  192/  306]
train() client id: f_00004-1-6 loss: 0.972509  [  224/  306]
train() client id: f_00004-1-7 loss: 0.746536  [  256/  306]
train() client id: f_00004-1-8 loss: 0.945579  [  288/  306]
train() client id: f_00004-2-0 loss: 0.831778  [   32/  306]
train() client id: f_00004-2-1 loss: 0.941216  [   64/  306]
train() client id: f_00004-2-2 loss: 0.723065  [   96/  306]
train() client id: f_00004-2-3 loss: 0.791229  [  128/  306]
train() client id: f_00004-2-4 loss: 0.734184  [  160/  306]
train() client id: f_00004-2-5 loss: 0.804294  [  192/  306]
train() client id: f_00004-2-6 loss: 0.877024  [  224/  306]
train() client id: f_00004-2-7 loss: 0.856919  [  256/  306]
train() client id: f_00004-2-8 loss: 0.888482  [  288/  306]
train() client id: f_00004-3-0 loss: 0.766980  [   32/  306]
train() client id: f_00004-3-1 loss: 0.756846  [   64/  306]
train() client id: f_00004-3-2 loss: 0.967094  [   96/  306]
train() client id: f_00004-3-3 loss: 0.819078  [  128/  306]
train() client id: f_00004-3-4 loss: 0.737496  [  160/  306]
train() client id: f_00004-3-5 loss: 0.807191  [  192/  306]
train() client id: f_00004-3-6 loss: 0.912355  [  224/  306]
train() client id: f_00004-3-7 loss: 0.954693  [  256/  306]
train() client id: f_00004-3-8 loss: 0.924083  [  288/  306]
train() client id: f_00004-4-0 loss: 0.838431  [   32/  306]
train() client id: f_00004-4-1 loss: 0.939247  [   64/  306]
train() client id: f_00004-4-2 loss: 0.817931  [   96/  306]
train() client id: f_00004-4-3 loss: 0.877288  [  128/  306]
train() client id: f_00004-4-4 loss: 0.903541  [  160/  306]
train() client id: f_00004-4-5 loss: 0.837642  [  192/  306]
train() client id: f_00004-4-6 loss: 0.845748  [  224/  306]
train() client id: f_00004-4-7 loss: 0.716815  [  256/  306]
train() client id: f_00004-4-8 loss: 0.711550  [  288/  306]
train() client id: f_00004-5-0 loss: 0.811495  [   32/  306]
train() client id: f_00004-5-1 loss: 0.805581  [   64/  306]
train() client id: f_00004-5-2 loss: 0.767802  [   96/  306]
train() client id: f_00004-5-3 loss: 0.913756  [  128/  306]
train() client id: f_00004-5-4 loss: 0.888169  [  160/  306]
train() client id: f_00004-5-5 loss: 0.752965  [  192/  306]
train() client id: f_00004-5-6 loss: 0.880070  [  224/  306]
train() client id: f_00004-5-7 loss: 0.817053  [  256/  306]
train() client id: f_00004-5-8 loss: 0.859962  [  288/  306]
train() client id: f_00004-6-0 loss: 0.786524  [   32/  306]
train() client id: f_00004-6-1 loss: 0.844983  [   64/  306]
train() client id: f_00004-6-2 loss: 0.856331  [   96/  306]
train() client id: f_00004-6-3 loss: 0.810511  [  128/  306]
train() client id: f_00004-6-4 loss: 0.884239  [  160/  306]
train() client id: f_00004-6-5 loss: 0.687853  [  192/  306]
train() client id: f_00004-6-6 loss: 0.950786  [  224/  306]
train() client id: f_00004-6-7 loss: 0.894989  [  256/  306]
train() client id: f_00004-6-8 loss: 0.805620  [  288/  306]
train() client id: f_00004-7-0 loss: 0.866861  [   32/  306]
train() client id: f_00004-7-1 loss: 0.802745  [   64/  306]
train() client id: f_00004-7-2 loss: 0.819503  [   96/  306]
train() client id: f_00004-7-3 loss: 0.842057  [  128/  306]
train() client id: f_00004-7-4 loss: 0.795206  [  160/  306]
train() client id: f_00004-7-5 loss: 0.968206  [  192/  306]
train() client id: f_00004-7-6 loss: 0.882714  [  224/  306]
train() client id: f_00004-7-7 loss: 0.819774  [  256/  306]
train() client id: f_00004-7-8 loss: 0.752318  [  288/  306]
train() client id: f_00005-0-0 loss: 0.548178  [   32/  146]
train() client id: f_00005-0-1 loss: 0.371871  [   64/  146]
train() client id: f_00005-0-2 loss: 0.503277  [   96/  146]
train() client id: f_00005-0-3 loss: 0.720141  [  128/  146]
train() client id: f_00005-1-0 loss: 0.697427  [   32/  146]
train() client id: f_00005-1-1 loss: 0.616988  [   64/  146]
train() client id: f_00005-1-2 loss: 0.457584  [   96/  146]
train() client id: f_00005-1-3 loss: 0.411102  [  128/  146]
train() client id: f_00005-2-0 loss: 0.598950  [   32/  146]
train() client id: f_00005-2-1 loss: 0.347371  [   64/  146]
train() client id: f_00005-2-2 loss: 0.562321  [   96/  146]
train() client id: f_00005-2-3 loss: 0.871731  [  128/  146]
train() client id: f_00005-3-0 loss: 0.444810  [   32/  146]
train() client id: f_00005-3-1 loss: 0.636027  [   64/  146]
train() client id: f_00005-3-2 loss: 0.511494  [   96/  146]
train() client id: f_00005-3-3 loss: 0.774080  [  128/  146]
train() client id: f_00005-4-0 loss: 0.520821  [   32/  146]
train() client id: f_00005-4-1 loss: 0.556968  [   64/  146]
train() client id: f_00005-4-2 loss: 0.690752  [   96/  146]
train() client id: f_00005-4-3 loss: 0.490695  [  128/  146]
train() client id: f_00005-5-0 loss: 0.751722  [   32/  146]
train() client id: f_00005-5-1 loss: 0.587426  [   64/  146]
train() client id: f_00005-5-2 loss: 0.478117  [   96/  146]
train() client id: f_00005-5-3 loss: 0.367921  [  128/  146]
train() client id: f_00005-6-0 loss: 0.476054  [   32/  146]
train() client id: f_00005-6-1 loss: 0.513765  [   64/  146]
train() client id: f_00005-6-2 loss: 0.545018  [   96/  146]
train() client id: f_00005-6-3 loss: 0.640160  [  128/  146]
train() client id: f_00005-7-0 loss: 0.368263  [   32/  146]
train() client id: f_00005-7-1 loss: 0.657749  [   64/  146]
train() client id: f_00005-7-2 loss: 0.592156  [   96/  146]
train() client id: f_00005-7-3 loss: 0.696399  [  128/  146]
train() client id: f_00006-0-0 loss: 0.542960  [   32/   54]
train() client id: f_00006-1-0 loss: 0.528685  [   32/   54]
train() client id: f_00006-2-0 loss: 0.581101  [   32/   54]
train() client id: f_00006-3-0 loss: 0.471100  [   32/   54]
train() client id: f_00006-4-0 loss: 0.579753  [   32/   54]
train() client id: f_00006-5-0 loss: 0.549274  [   32/   54]
train() client id: f_00006-6-0 loss: 0.528829  [   32/   54]
train() client id: f_00006-7-0 loss: 0.474973  [   32/   54]
train() client id: f_00007-0-0 loss: 0.290329  [   32/  179]
train() client id: f_00007-0-1 loss: 0.328251  [   64/  179]
train() client id: f_00007-0-2 loss: 0.128305  [   96/  179]
train() client id: f_00007-0-3 loss: 0.197143  [  128/  179]
train() client id: f_00007-0-4 loss: 0.267996  [  160/  179]
train() client id: f_00007-1-0 loss: 0.132240  [   32/  179]
train() client id: f_00007-1-1 loss: 0.344861  [   64/  179]
train() client id: f_00007-1-2 loss: 0.299859  [   96/  179]
train() client id: f_00007-1-3 loss: 0.324583  [  128/  179]
train() client id: f_00007-1-4 loss: 0.078263  [  160/  179]
train() client id: f_00007-2-0 loss: 0.325873  [   32/  179]
train() client id: f_00007-2-1 loss: 0.021758  [   64/  179]
train() client id: f_00007-2-2 loss: 0.218240  [   96/  179]
train() client id: f_00007-2-3 loss: 0.162782  [  128/  179]
train() client id: f_00007-2-4 loss: 0.411629  [  160/  179]
train() client id: f_00007-3-0 loss: 0.064273  [   32/  179]
train() client id: f_00007-3-1 loss: 0.171853  [   64/  179]
train() client id: f_00007-3-2 loss: 0.382539  [   96/  179]
train() client id: f_00007-3-3 loss: 0.164686  [  128/  179]
train() client id: f_00007-3-4 loss: 0.210725  [  160/  179]
train() client id: f_00007-4-0 loss: 0.289200  [   32/  179]
train() client id: f_00007-4-1 loss: 0.290082  [   64/  179]
train() client id: f_00007-4-2 loss: 0.049499  [   96/  179]
train() client id: f_00007-4-3 loss: 0.171782  [  128/  179]
train() client id: f_00007-4-4 loss: 0.118195  [  160/  179]
train() client id: f_00007-5-0 loss: 0.162792  [   32/  179]
train() client id: f_00007-5-1 loss: 0.417612  [   64/  179]
train() client id: f_00007-5-2 loss: 0.150366  [   96/  179]
train() client id: f_00007-5-3 loss: 0.149941  [  128/  179]
train() client id: f_00007-5-4 loss: 0.197622  [  160/  179]
train() client id: f_00007-6-0 loss: 0.057846  [   32/  179]
train() client id: f_00007-6-1 loss: 0.457162  [   64/  179]
train() client id: f_00007-6-2 loss: 0.147731  [   96/  179]
train() client id: f_00007-6-3 loss: 0.348626  [  128/  179]
train() client id: f_00007-6-4 loss: 0.013370  [  160/  179]
train() client id: f_00007-7-0 loss: 0.283086  [   32/  179]
train() client id: f_00007-7-1 loss: 0.052357  [   64/  179]
train() client id: f_00007-7-2 loss: 0.136567  [   96/  179]
train() client id: f_00007-7-3 loss: 0.287066  [  128/  179]
train() client id: f_00007-7-4 loss: 0.203872  [  160/  179]
train() client id: f_00008-0-0 loss: 0.848261  [   32/  130]
train() client id: f_00008-0-1 loss: 0.638330  [   64/  130]
train() client id: f_00008-0-2 loss: 0.800391  [   96/  130]
train() client id: f_00008-0-3 loss: 0.649174  [  128/  130]
train() client id: f_00008-1-0 loss: 0.767334  [   32/  130]
train() client id: f_00008-1-1 loss: 0.802959  [   64/  130]
train() client id: f_00008-1-2 loss: 0.756201  [   96/  130]
train() client id: f_00008-1-3 loss: 0.605412  [  128/  130]
train() client id: f_00008-2-0 loss: 0.610473  [   32/  130]
train() client id: f_00008-2-1 loss: 0.883338  [   64/  130]
train() client id: f_00008-2-2 loss: 0.786331  [   96/  130]
train() client id: f_00008-2-3 loss: 0.668175  [  128/  130]
train() client id: f_00008-3-0 loss: 0.811397  [   32/  130]
train() client id: f_00008-3-1 loss: 0.811544  [   64/  130]
train() client id: f_00008-3-2 loss: 0.642656  [   96/  130]
train() client id: f_00008-3-3 loss: 0.672647  [  128/  130]
train() client id: f_00008-4-0 loss: 0.710041  [   32/  130]
train() client id: f_00008-4-1 loss: 0.831751  [   64/  130]
train() client id: f_00008-4-2 loss: 0.684383  [   96/  130]
train() client id: f_00008-4-3 loss: 0.714593  [  128/  130]
train() client id: f_00008-5-0 loss: 0.656442  [   32/  130]
train() client id: f_00008-5-1 loss: 0.722247  [   64/  130]
train() client id: f_00008-5-2 loss: 0.738973  [   96/  130]
train() client id: f_00008-5-3 loss: 0.829224  [  128/  130]
train() client id: f_00008-6-0 loss: 0.777920  [   32/  130]
train() client id: f_00008-6-1 loss: 0.801346  [   64/  130]
train() client id: f_00008-6-2 loss: 0.750962  [   96/  130]
train() client id: f_00008-6-3 loss: 0.622827  [  128/  130]
train() client id: f_00008-7-0 loss: 0.688055  [   32/  130]
train() client id: f_00008-7-1 loss: 0.851189  [   64/  130]
train() client id: f_00008-7-2 loss: 0.636703  [   96/  130]
train() client id: f_00008-7-3 loss: 0.762900  [  128/  130]
train() client id: f_00009-0-0 loss: 0.889901  [   32/  118]
train() client id: f_00009-0-1 loss: 1.073875  [   64/  118]
train() client id: f_00009-0-2 loss: 0.768587  [   96/  118]
train() client id: f_00009-1-0 loss: 0.986802  [   32/  118]
train() client id: f_00009-1-1 loss: 0.996122  [   64/  118]
train() client id: f_00009-1-2 loss: 0.854235  [   96/  118]
train() client id: f_00009-2-0 loss: 0.841304  [   32/  118]
train() client id: f_00009-2-1 loss: 0.855059  [   64/  118]
train() client id: f_00009-2-2 loss: 0.953133  [   96/  118]
train() client id: f_00009-3-0 loss: 0.823533  [   32/  118]
train() client id: f_00009-3-1 loss: 0.811560  [   64/  118]
train() client id: f_00009-3-2 loss: 0.896192  [   96/  118]
train() client id: f_00009-4-0 loss: 0.800794  [   32/  118]
train() client id: f_00009-4-1 loss: 0.842298  [   64/  118]
train() client id: f_00009-4-2 loss: 0.974565  [   96/  118]
train() client id: f_00009-5-0 loss: 0.858665  [   32/  118]
train() client id: f_00009-5-1 loss: 0.916417  [   64/  118]
train() client id: f_00009-5-2 loss: 0.739270  [   96/  118]
train() client id: f_00009-6-0 loss: 0.866367  [   32/  118]
train() client id: f_00009-6-1 loss: 0.882184  [   64/  118]
train() client id: f_00009-6-2 loss: 0.854768  [   96/  118]
train() client id: f_00009-7-0 loss: 0.978386  [   32/  118]
train() client id: f_00009-7-1 loss: 0.882103  [   64/  118]
train() client id: f_00009-7-2 loss: 0.795663  [   96/  118]
At round 63 accuracy: 0.6472148541114059
At round 63 training accuracy: 0.590878604963112
At round 63 training loss: 0.8242545040655385
update_location
xs = -4.528292 216.001589 225.045120 -225.943528 114.896481 -150.217951 -267.215960 288.375741 -1.680116 209.695607 
ys = 302.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -232.154970 4.001482 
xs mean: 40.442869159412865
ys mean: 9.371751218646875
dists_uav = 318.716141 238.534423 246.266217 248.110353 152.320792 180.480891 285.331622 305.824051 252.782026 232.353738 
uav_gains = -117.192385 -110.355573 -110.981968 -111.136185 -104.574551 -106.458157 -114.459713 -116.206963 -111.534426 -109.879336 
uav_gains_db_mean: -111.27792584473093
dists_bs = 212.930248 422.261392 436.119968 160.741481 338.958444 173.960101 200.579837 502.416200 442.509832 420.988365 
bs_gains = -104.757705 -113.083338 -113.476026 -101.338642 -110.411160 -102.299649 -104.031103 -115.196841 -113.652901 -113.046622 
bs_gains_db_mean: -109.12939872635303
Round 64
-------------------------------
ene_coms = [0.01986498 0.01346038 0.01144876 0.01157736 0.01099499 0.00696653
 0.01515475 0.01791233 0.01411885 0.01341981]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.5283534  5.13850966 2.43699968 0.89318828 5.90476821 2.82818661
 1.1255716  3.52668726 2.56829901 2.33504578]
obj_prev = 29.28560948281358
eta_min = 6.920212406836414e-37	eta_max = 0.9533708780711534
af = 6.112305432346556	bf = 0.8445466432900316	zeta = 6.723535975581212	eta = 0.909090909090909
af = 6.112305432346556	bf = 0.8445466432900316	zeta = 15.698227303725249	eta = 0.38936278052847934
af = 6.112305432346556	bf = 0.8445466432900316	zeta = 10.824660642001787	eta = 0.564664855046783
af = 6.112305432346556	bf = 0.8445466432900316	zeta = 9.96443596273056	eta = 0.6134120842572606
af = 6.112305432346556	bf = 0.8445466432900316	zeta = 9.91169711346713	eta = 0.6166759700557941
af = 6.112305432346556	bf = 0.8445466432900316	zeta = 9.911472983126679	eta = 0.6166899150865026
eta = 0.6166899150865026
ene_coms = [0.01986498 0.01346038 0.01144876 0.01157736 0.01099499 0.00696653
 0.01515475 0.01791233 0.01411885 0.01341981]
ene_comp = [0.0398881  0.08389163 0.03925495 0.0136126  0.09687109 0.04621954
 0.0170949  0.05666642 0.04115439 0.03735552]
ene_total = [0.9758012  1.58981294 0.82802014 0.4113662  1.76151361 0.86855839
 0.52665489 1.21791295 0.90264302 0.82918963]
ti_comp = [1.44242869 1.50647468 1.52659081 1.52530488 1.53112857 1.57141314
 1.48953093 1.46195511 1.49988991 1.50688034]
ti_coms = [0.19864976 0.13460377 0.11448764 0.11577357 0.10994988 0.06966531
 0.15154752 0.17912334 0.14118855 0.13419811]
t_total = [26.79973145 26.79973145 26.79973145 26.79973145 26.79973145 26.79973145
 26.79973145 26.79973145 26.79973145 26.79973145]
ene_coms = [0.01986498 0.01346038 0.01144876 0.01157736 0.01099499 0.00696653
 0.01515475 0.01791233 0.01411885 0.01341981]
ene_comp = [1.90643184e-06 1.62596915e-05 1.62225126e-06 6.77626526e-08
 2.42347927e-05 2.49905698e-06 1.40728021e-07 5.32095236e-06
 1.93646497e-06 1.43478504e-06]
ene_total = [0.32443733 0.22008104 0.18699123 0.18906584 0.17995008 0.11380817
 0.2474879  0.29260536 0.23060044 0.21917647]
optimize_network iter = 0 obj = 2.204203869508145
eta = 0.6166899150865026
freqs = [13826713.8448759  27843691.96989011 12857064.54316399  4462256.985669
 31633886.16790054 14706362.19538179  5738349.78319659 19380357.15265262
 13719135.71912492 12394985.48560833]
eta_min = 0.6166899150865036	eta_max = 0.7839475962371341
af = 0.0007177017548472777	bf = 0.8445466432900316	zeta = 0.0007894719303320056	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.01986498 0.01346038 0.01144876 0.01157736 0.01099499 0.00696653
 0.01515475 0.01791233 0.01411885 0.01341981]
ene_comp = [3.75892666e-07 3.20593618e-06 3.19860558e-07 1.33608156e-08
 4.77839314e-06 4.92741032e-07 2.77474547e-08 1.04913636e-06
 3.81814321e-07 2.82897695e-07]
ene_total = [1.36785572 0.92705328 0.7883422  0.79717553 0.7574038  0.47972387
 1.04350292 1.23345029 0.97219916 0.92405875]
ti_comp = [0.72634272 0.7903887  0.81050483 0.8092189  0.8150426  0.85532716
 0.77344495 0.74586913 0.78380393 0.79079437]
ti_coms = [0.19864976 0.13460377 0.11448764 0.11577357 0.10994988 0.06966531
 0.15154752 0.17912334 0.14118855 0.13419811]
t_total = [26.79973145 26.79973145 26.79973145 26.79973145 26.79973145 26.79973145
 26.79973145 26.79973145 26.79973145 26.79973145]
ene_coms = [0.01986498 0.01346038 0.01144876 0.01157736 0.01099499 0.00696653
 0.01515475 0.01791233 0.01411885 0.01341981]
ene_comp = [9.59993991e-07 7.54216843e-06 7.34841861e-07 3.07406735e-08
 1.09205300e-05 1.07704559e-06 6.66442236e-08 2.61020054e-06
 9.05430740e-07 6.65210262e-07]
ene_total = [0.57557414 0.39020494 0.3317254  0.33543071 0.31887328 0.20187194
 0.43907933 0.51904822 0.40909065 0.38883036]
optimize_network iter = 1 obj = 3.909728983764987
eta = 0.7839475962371341
freqs = [13826713.8448759  26723612.54729014 12194277.99921754  4235380.67990561
 29924791.59213965 13605377.81492111  5564861.58441241 19128477.84127906
 13219831.86044278 11893466.58371018]
eta_min = 0.7839475962371365	eta_max = 0.783947596237132
af = 0.0006560933333756521	bf = 0.8445466432900316	zeta = 0.0007217026667132174	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.01986498 0.01346038 0.01144876 0.01157736 0.01099499 0.00696653
 0.01515475 0.01791233 0.01411885 0.01341981]
ene_comp = [3.75892666e-07 2.95319119e-06 2.87732703e-07 1.20367354e-08
 4.27601335e-06 4.21725077e-07 2.60950330e-08 1.02204310e-06
 3.54528026e-07 2.60467941e-07]
ene_total = [1.36785572 0.92703588 0.78833999 0.79717544 0.75736921 0.47971898
 1.04350281 1.23344843 0.97219728 0.92405721]
ti_comp = [0.72634272 0.7903887  0.81050483 0.8092189  0.8150426  0.85532716
 0.77344495 0.74586913 0.78380393 0.79079437]
ti_coms = [0.19864976 0.13460377 0.11448764 0.11577357 0.10994988 0.06966531
 0.15154752 0.17912334 0.14118855 0.13419811]
t_total = [26.79973145 26.79973145 26.79973145 26.79973145 26.79973145 26.79973145
 26.79973145 26.79973145 26.79973145 26.79973145]
ene_coms = [0.01986498 0.01346038 0.01144876 0.01157736 0.01099499 0.00696653
 0.01515475 0.01791233 0.01411885 0.01341981]
ene_comp = [9.59993991e-07 7.54216843e-06 7.34841861e-07 3.07406735e-08
 1.09205300e-05 1.07704559e-06 6.66442236e-08 2.61020054e-06
 9.05430740e-07 6.65210262e-07]
ene_total = [0.57557414 0.39020494 0.3317254  0.33543071 0.31887328 0.20187194
 0.43907933 0.51904822 0.40909065 0.38883036]
optimize_network iter = 2 obj = 3.9097289837649476
eta = 0.783947596237132
freqs = [13826713.84487588 26723612.54729012 12194277.99921754  4235380.67990561
 29924791.59213964 13605377.81492112  5564861.5844124  19128477.84127903
 13219831.86044277 11893466.58371018]
Done!
ene_coms = [0.01986498 0.01346038 0.01144876 0.01157736 0.01099499 0.00696653
 0.01515475 0.01791233 0.01411885 0.01341981]
ene_comp = [8.43095049e-07 6.62375486e-06 6.45359805e-07 2.69973665e-08
 9.59073166e-06 9.45893221e-07 5.85289236e-08 2.29235513e-06
 7.95175993e-07 5.84207281e-07]
ene_total = [0.01986582 0.013467   0.01144941 0.01157738 0.01100458 0.00696748
 0.01515481 0.01791463 0.01411965 0.0134204 ]
At round 64 energy consumption: 0.13494115187996492
At round 64 eta: 0.783947596237132
At round 64 a_n: 6.259668650218071
At round 64 local rounds: 7.970581659443173
At round 64 global rounds: 28.97291833461143
gradient difference: 0.5015862584114075
train() client id: f_00000-0-0 loss: 1.060694  [   32/  126]
train() client id: f_00000-0-1 loss: 1.391645  [   64/  126]
train() client id: f_00000-0-2 loss: 0.908385  [   96/  126]
train() client id: f_00000-1-0 loss: 0.855742  [   32/  126]
train() client id: f_00000-1-1 loss: 1.193029  [   64/  126]
train() client id: f_00000-1-2 loss: 1.023770  [   96/  126]
train() client id: f_00000-2-0 loss: 0.838693  [   32/  126]
train() client id: f_00000-2-1 loss: 0.966685  [   64/  126]
train() client id: f_00000-2-2 loss: 1.176249  [   96/  126]
train() client id: f_00000-3-0 loss: 0.938184  [   32/  126]
train() client id: f_00000-3-1 loss: 0.945900  [   64/  126]
train() client id: f_00000-3-2 loss: 0.868891  [   96/  126]
train() client id: f_00000-4-0 loss: 0.837544  [   32/  126]
train() client id: f_00000-4-1 loss: 0.866925  [   64/  126]
train() client id: f_00000-4-2 loss: 0.938986  [   96/  126]
train() client id: f_00000-5-0 loss: 0.797726  [   32/  126]
train() client id: f_00000-5-1 loss: 0.883961  [   64/  126]
train() client id: f_00000-5-2 loss: 0.785883  [   96/  126]
train() client id: f_00000-6-0 loss: 0.789945  [   32/  126]
train() client id: f_00000-6-1 loss: 0.761302  [   64/  126]
train() client id: f_00000-6-2 loss: 0.822147  [   96/  126]
train() client id: f_00001-0-0 loss: 0.486476  [   32/  265]
train() client id: f_00001-0-1 loss: 0.349553  [   64/  265]
train() client id: f_00001-0-2 loss: 0.555062  [   96/  265]
train() client id: f_00001-0-3 loss: 0.394418  [  128/  265]
train() client id: f_00001-0-4 loss: 0.487878  [  160/  265]
train() client id: f_00001-0-5 loss: 0.327733  [  192/  265]
train() client id: f_00001-0-6 loss: 0.399115  [  224/  265]
train() client id: f_00001-0-7 loss: 0.370706  [  256/  265]
train() client id: f_00001-1-0 loss: 0.404414  [   32/  265]
train() client id: f_00001-1-1 loss: 0.377543  [   64/  265]
train() client id: f_00001-1-2 loss: 0.402693  [   96/  265]
train() client id: f_00001-1-3 loss: 0.358894  [  128/  265]
train() client id: f_00001-1-4 loss: 0.543521  [  160/  265]
train() client id: f_00001-1-5 loss: 0.358686  [  192/  265]
train() client id: f_00001-1-6 loss: 0.337400  [  224/  265]
train() client id: f_00001-1-7 loss: 0.429889  [  256/  265]
train() client id: f_00001-2-0 loss: 0.446290  [   32/  265]
train() client id: f_00001-2-1 loss: 0.318396  [   64/  265]
train() client id: f_00001-2-2 loss: 0.503692  [   96/  265]
train() client id: f_00001-2-3 loss: 0.362523  [  128/  265]
train() client id: f_00001-2-4 loss: 0.333142  [  160/  265]
train() client id: f_00001-2-5 loss: 0.402525  [  192/  265]
train() client id: f_00001-2-6 loss: 0.489085  [  224/  265]
train() client id: f_00001-2-7 loss: 0.454987  [  256/  265]
train() client id: f_00001-3-0 loss: 0.336402  [   32/  265]
train() client id: f_00001-3-1 loss: 0.523705  [   64/  265]
train() client id: f_00001-3-2 loss: 0.372741  [   96/  265]
train() client id: f_00001-3-3 loss: 0.316667  [  128/  265]
train() client id: f_00001-3-4 loss: 0.389832  [  160/  265]
train() client id: f_00001-3-5 loss: 0.524256  [  192/  265]
train() client id: f_00001-3-6 loss: 0.412591  [  224/  265]
train() client id: f_00001-3-7 loss: 0.406791  [  256/  265]
train() client id: f_00001-4-0 loss: 0.290090  [   32/  265]
train() client id: f_00001-4-1 loss: 0.443470  [   64/  265]
train() client id: f_00001-4-2 loss: 0.504751  [   96/  265]
train() client id: f_00001-4-3 loss: 0.366202  [  128/  265]
train() client id: f_00001-4-4 loss: 0.450774  [  160/  265]
train() client id: f_00001-4-5 loss: 0.372259  [  192/  265]
train() client id: f_00001-4-6 loss: 0.367039  [  224/  265]
train() client id: f_00001-4-7 loss: 0.405480  [  256/  265]
train() client id: f_00001-5-0 loss: 0.427861  [   32/  265]
train() client id: f_00001-5-1 loss: 0.422297  [   64/  265]
train() client id: f_00001-5-2 loss: 0.380217  [   96/  265]
train() client id: f_00001-5-3 loss: 0.403465  [  128/  265]
train() client id: f_00001-5-4 loss: 0.473556  [  160/  265]
train() client id: f_00001-5-5 loss: 0.394736  [  192/  265]
train() client id: f_00001-5-6 loss: 0.319469  [  224/  265]
train() client id: f_00001-5-7 loss: 0.432690  [  256/  265]
train() client id: f_00001-6-0 loss: 0.446177  [   32/  265]
train() client id: f_00001-6-1 loss: 0.440344  [   64/  265]
train() client id: f_00001-6-2 loss: 0.295452  [   96/  265]
train() client id: f_00001-6-3 loss: 0.424062  [  128/  265]
train() client id: f_00001-6-4 loss: 0.448127  [  160/  265]
train() client id: f_00001-6-5 loss: 0.397617  [  192/  265]
train() client id: f_00001-6-6 loss: 0.407312  [  224/  265]
train() client id: f_00001-6-7 loss: 0.389182  [  256/  265]
train() client id: f_00002-0-0 loss: 1.121065  [   32/  124]
train() client id: f_00002-0-1 loss: 0.988642  [   64/  124]
train() client id: f_00002-0-2 loss: 1.144669  [   96/  124]
train() client id: f_00002-1-0 loss: 0.971866  [   32/  124]
train() client id: f_00002-1-1 loss: 0.936830  [   64/  124]
train() client id: f_00002-1-2 loss: 0.977423  [   96/  124]
train() client id: f_00002-2-0 loss: 0.896230  [   32/  124]
train() client id: f_00002-2-1 loss: 1.155056  [   64/  124]
train() client id: f_00002-2-2 loss: 0.944594  [   96/  124]
train() client id: f_00002-3-0 loss: 1.019244  [   32/  124]
train() client id: f_00002-3-1 loss: 0.698470  [   64/  124]
train() client id: f_00002-3-2 loss: 0.950044  [   96/  124]
train() client id: f_00002-4-0 loss: 0.788017  [   32/  124]
train() client id: f_00002-4-1 loss: 1.159600  [   64/  124]
train() client id: f_00002-4-2 loss: 0.780389  [   96/  124]
train() client id: f_00002-5-0 loss: 1.109806  [   32/  124]
train() client id: f_00002-5-1 loss: 0.810430  [   64/  124]
train() client id: f_00002-5-2 loss: 1.008327  [   96/  124]
train() client id: f_00002-6-0 loss: 0.909475  [   32/  124]
train() client id: f_00002-6-1 loss: 0.756064  [   64/  124]
train() client id: f_00002-6-2 loss: 1.067924  [   96/  124]
train() client id: f_00003-0-0 loss: 0.962758  [   32/   43]
train() client id: f_00003-1-0 loss: 0.828173  [   32/   43]
train() client id: f_00003-2-0 loss: 0.802771  [   32/   43]
train() client id: f_00003-3-0 loss: 0.767592  [   32/   43]
train() client id: f_00003-4-0 loss: 0.743837  [   32/   43]
train() client id: f_00003-5-0 loss: 0.896187  [   32/   43]
train() client id: f_00003-6-0 loss: 0.983411  [   32/   43]
train() client id: f_00004-0-0 loss: 0.981377  [   32/  306]
train() client id: f_00004-0-1 loss: 0.856096  [   64/  306]
train() client id: f_00004-0-2 loss: 1.055691  [   96/  306]
train() client id: f_00004-0-3 loss: 0.807356  [  128/  306]
train() client id: f_00004-0-4 loss: 0.967861  [  160/  306]
train() client id: f_00004-0-5 loss: 0.904503  [  192/  306]
train() client id: f_00004-0-6 loss: 0.856985  [  224/  306]
train() client id: f_00004-0-7 loss: 1.043153  [  256/  306]
train() client id: f_00004-0-8 loss: 0.980981  [  288/  306]
train() client id: f_00004-1-0 loss: 0.849056  [   32/  306]
train() client id: f_00004-1-1 loss: 0.929819  [   64/  306]
train() client id: f_00004-1-2 loss: 0.896993  [   96/  306]
train() client id: f_00004-1-3 loss: 1.067903  [  128/  306]
train() client id: f_00004-1-4 loss: 1.090690  [  160/  306]
train() client id: f_00004-1-5 loss: 0.735869  [  192/  306]
train() client id: f_00004-1-6 loss: 0.826393  [  224/  306]
train() client id: f_00004-1-7 loss: 1.026651  [  256/  306]
train() client id: f_00004-1-8 loss: 1.026440  [  288/  306]
train() client id: f_00004-2-0 loss: 0.941353  [   32/  306]
train() client id: f_00004-2-1 loss: 1.029317  [   64/  306]
train() client id: f_00004-2-2 loss: 0.965145  [   96/  306]
train() client id: f_00004-2-3 loss: 0.966844  [  128/  306]
train() client id: f_00004-2-4 loss: 0.796358  [  160/  306]
train() client id: f_00004-2-5 loss: 0.976038  [  192/  306]
train() client id: f_00004-2-6 loss: 0.864776  [  224/  306]
train() client id: f_00004-2-7 loss: 0.995762  [  256/  306]
train() client id: f_00004-2-8 loss: 0.845020  [  288/  306]
train() client id: f_00004-3-0 loss: 0.848870  [   32/  306]
train() client id: f_00004-3-1 loss: 0.904388  [   64/  306]
train() client id: f_00004-3-2 loss: 0.963323  [   96/  306]
train() client id: f_00004-3-3 loss: 0.870742  [  128/  306]
train() client id: f_00004-3-4 loss: 0.998186  [  160/  306]
train() client id: f_00004-3-5 loss: 0.778056  [  192/  306]
train() client id: f_00004-3-6 loss: 0.956658  [  224/  306]
train() client id: f_00004-3-7 loss: 0.989831  [  256/  306]
train() client id: f_00004-3-8 loss: 0.977498  [  288/  306]
train() client id: f_00004-4-0 loss: 0.890994  [   32/  306]
train() client id: f_00004-4-1 loss: 1.082514  [   64/  306]
train() client id: f_00004-4-2 loss: 0.930490  [   96/  306]
train() client id: f_00004-4-3 loss: 0.933331  [  128/  306]
train() client id: f_00004-4-4 loss: 0.869665  [  160/  306]
train() client id: f_00004-4-5 loss: 0.972670  [  192/  306]
train() client id: f_00004-4-6 loss: 0.831188  [  224/  306]
train() client id: f_00004-4-7 loss: 1.047992  [  256/  306]
train() client id: f_00004-4-8 loss: 0.868900  [  288/  306]
train() client id: f_00004-5-0 loss: 0.962138  [   32/  306]
train() client id: f_00004-5-1 loss: 0.885385  [   64/  306]
train() client id: f_00004-5-2 loss: 0.916408  [   96/  306]
train() client id: f_00004-5-3 loss: 0.949100  [  128/  306]
train() client id: f_00004-5-4 loss: 1.015186  [  160/  306]
train() client id: f_00004-5-5 loss: 0.739251  [  192/  306]
train() client id: f_00004-5-6 loss: 0.903339  [  224/  306]
train() client id: f_00004-5-7 loss: 1.026841  [  256/  306]
train() client id: f_00004-5-8 loss: 0.843394  [  288/  306]
train() client id: f_00004-6-0 loss: 0.852629  [   32/  306]
train() client id: f_00004-6-1 loss: 0.830813  [   64/  306]
train() client id: f_00004-6-2 loss: 0.922812  [   96/  306]
train() client id: f_00004-6-3 loss: 0.915901  [  128/  306]
train() client id: f_00004-6-4 loss: 0.917093  [  160/  306]
train() client id: f_00004-6-5 loss: 0.976011  [  192/  306]
train() client id: f_00004-6-6 loss: 0.854538  [  224/  306]
train() client id: f_00004-6-7 loss: 0.911782  [  256/  306]
train() client id: f_00004-6-8 loss: 1.120500  [  288/  306]
train() client id: f_00005-0-0 loss: 0.624822  [   32/  146]
train() client id: f_00005-0-1 loss: 0.737991  [   64/  146]
train() client id: f_00005-0-2 loss: 0.316408  [   96/  146]
train() client id: f_00005-0-3 loss: 0.357257  [  128/  146]
train() client id: f_00005-1-0 loss: 0.344412  [   32/  146]
train() client id: f_00005-1-1 loss: 0.485580  [   64/  146]
train() client id: f_00005-1-2 loss: 0.624206  [   96/  146]
train() client id: f_00005-1-3 loss: 0.307350  [  128/  146]
train() client id: f_00005-2-0 loss: 0.599097  [   32/  146]
train() client id: f_00005-2-1 loss: 0.171017  [   64/  146]
train() client id: f_00005-2-2 loss: 0.377985  [   96/  146]
train() client id: f_00005-2-3 loss: 0.727831  [  128/  146]
train() client id: f_00005-3-0 loss: 0.507737  [   32/  146]
train() client id: f_00005-3-1 loss: 0.532796  [   64/  146]
train() client id: f_00005-3-2 loss: 0.439951  [   96/  146]
train() client id: f_00005-3-3 loss: 0.534501  [  128/  146]
train() client id: f_00005-4-0 loss: 0.474152  [   32/  146]
train() client id: f_00005-4-1 loss: 0.567621  [   64/  146]
train() client id: f_00005-4-2 loss: 0.444383  [   96/  146]
train() client id: f_00005-4-3 loss: 0.281643  [  128/  146]
train() client id: f_00005-5-0 loss: 0.329669  [   32/  146]
train() client id: f_00005-5-1 loss: 0.462275  [   64/  146]
train() client id: f_00005-5-2 loss: 0.302202  [   96/  146]
train() client id: f_00005-5-3 loss: 0.637016  [  128/  146]
train() client id: f_00005-6-0 loss: 0.934850  [   32/  146]
train() client id: f_00005-6-1 loss: 0.372576  [   64/  146]
train() client id: f_00005-6-2 loss: 0.223164  [   96/  146]
train() client id: f_00005-6-3 loss: 0.466100  [  128/  146]
train() client id: f_00006-0-0 loss: 0.484908  [   32/   54]
train() client id: f_00006-1-0 loss: 0.529121  [   32/   54]
train() client id: f_00006-2-0 loss: 0.573891  [   32/   54]
train() client id: f_00006-3-0 loss: 0.596084  [   32/   54]
train() client id: f_00006-4-0 loss: 0.500199  [   32/   54]
train() client id: f_00006-5-0 loss: 0.546483  [   32/   54]
train() client id: f_00006-6-0 loss: 0.594955  [   32/   54]
train() client id: f_00007-0-0 loss: 0.717745  [   32/  179]
train() client id: f_00007-0-1 loss: 0.525400  [   64/  179]
train() client id: f_00007-0-2 loss: 0.480505  [   96/  179]
train() client id: f_00007-0-3 loss: 0.665634  [  128/  179]
train() client id: f_00007-0-4 loss: 0.474588  [  160/  179]
train() client id: f_00007-1-0 loss: 0.606385  [   32/  179]
train() client id: f_00007-1-1 loss: 0.527431  [   64/  179]
train() client id: f_00007-1-2 loss: 0.743019  [   96/  179]
train() client id: f_00007-1-3 loss: 0.695227  [  128/  179]
train() client id: f_00007-1-4 loss: 0.389037  [  160/  179]
train() client id: f_00007-2-0 loss: 0.673632  [   32/  179]
train() client id: f_00007-2-1 loss: 0.405050  [   64/  179]
train() client id: f_00007-2-2 loss: 0.745549  [   96/  179]
train() client id: f_00007-2-3 loss: 0.590437  [  128/  179]
train() client id: f_00007-2-4 loss: 0.507828  [  160/  179]
train() client id: f_00007-3-0 loss: 0.477450  [   32/  179]
train() client id: f_00007-3-1 loss: 0.406900  [   64/  179]
train() client id: f_00007-3-2 loss: 0.737547  [   96/  179]
train() client id: f_00007-3-3 loss: 0.469195  [  128/  179]
train() client id: f_00007-3-4 loss: 0.517603  [  160/  179]
train() client id: f_00007-4-0 loss: 0.581367  [   32/  179]
train() client id: f_00007-4-1 loss: 0.700206  [   64/  179]
train() client id: f_00007-4-2 loss: 0.393099  [   96/  179]
train() client id: f_00007-4-3 loss: 0.493123  [  128/  179]
train() client id: f_00007-4-4 loss: 0.655225  [  160/  179]
train() client id: f_00007-5-0 loss: 0.645643  [   32/  179]
train() client id: f_00007-5-1 loss: 0.468049  [   64/  179]
train() client id: f_00007-5-2 loss: 0.528711  [   96/  179]
train() client id: f_00007-5-3 loss: 0.657904  [  128/  179]
train() client id: f_00007-5-4 loss: 0.416368  [  160/  179]
train() client id: f_00007-6-0 loss: 0.658571  [   32/  179]
train() client id: f_00007-6-1 loss: 0.646083  [   64/  179]
train() client id: f_00007-6-2 loss: 0.479154  [   96/  179]
train() client id: f_00007-6-3 loss: 0.432044  [  128/  179]
train() client id: f_00007-6-4 loss: 0.504303  [  160/  179]
train() client id: f_00008-0-0 loss: 0.712597  [   32/  130]
train() client id: f_00008-0-1 loss: 0.575100  [   64/  130]
train() client id: f_00008-0-2 loss: 0.606621  [   96/  130]
train() client id: f_00008-0-3 loss: 0.755495  [  128/  130]
train() client id: f_00008-1-0 loss: 0.692490  [   32/  130]
train() client id: f_00008-1-1 loss: 0.728212  [   64/  130]
train() client id: f_00008-1-2 loss: 0.676052  [   96/  130]
train() client id: f_00008-1-3 loss: 0.605564  [  128/  130]
train() client id: f_00008-2-0 loss: 0.731113  [   32/  130]
train() client id: f_00008-2-1 loss: 0.630619  [   64/  130]
train() client id: f_00008-2-2 loss: 0.635359  [   96/  130]
train() client id: f_00008-2-3 loss: 0.703031  [  128/  130]
train() client id: f_00008-3-0 loss: 0.691680  [   32/  130]
train() client id: f_00008-3-1 loss: 0.726214  [   64/  130]
train() client id: f_00008-3-2 loss: 0.613165  [   96/  130]
train() client id: f_00008-3-3 loss: 0.655566  [  128/  130]
train() client id: f_00008-4-0 loss: 0.658464  [   32/  130]
train() client id: f_00008-4-1 loss: 0.679538  [   64/  130]
train() client id: f_00008-4-2 loss: 0.588995  [   96/  130]
train() client id: f_00008-4-3 loss: 0.723814  [  128/  130]
train() client id: f_00008-5-0 loss: 0.644509  [   32/  130]
train() client id: f_00008-5-1 loss: 0.595584  [   64/  130]
train() client id: f_00008-5-2 loss: 0.765325  [   96/  130]
train() client id: f_00008-5-3 loss: 0.693754  [  128/  130]
train() client id: f_00008-6-0 loss: 0.701804  [   32/  130]
train() client id: f_00008-6-1 loss: 0.728730  [   64/  130]
train() client id: f_00008-6-2 loss: 0.615122  [   96/  130]
train() client id: f_00008-6-3 loss: 0.648422  [  128/  130]
train() client id: f_00009-0-0 loss: 1.012706  [   32/  118]
train() client id: f_00009-0-1 loss: 1.174679  [   64/  118]
train() client id: f_00009-0-2 loss: 0.826687  [   96/  118]
train() client id: f_00009-1-0 loss: 1.075318  [   32/  118]
train() client id: f_00009-1-1 loss: 1.011004  [   64/  118]
train() client id: f_00009-1-2 loss: 0.918766  [   96/  118]
train() client id: f_00009-2-0 loss: 0.956981  [   32/  118]
train() client id: f_00009-2-1 loss: 1.117175  [   64/  118]
train() client id: f_00009-2-2 loss: 0.853129  [   96/  118]
train() client id: f_00009-3-0 loss: 0.922408  [   32/  118]
train() client id: f_00009-3-1 loss: 0.788084  [   64/  118]
train() client id: f_00009-3-2 loss: 0.934210  [   96/  118]
train() client id: f_00009-4-0 loss: 1.107912  [   32/  118]
train() client id: f_00009-4-1 loss: 0.884900  [   64/  118]
train() client id: f_00009-4-2 loss: 0.790741  [   96/  118]
train() client id: f_00009-5-0 loss: 0.896713  [   32/  118]
train() client id: f_00009-5-1 loss: 0.931865  [   64/  118]
train() client id: f_00009-5-2 loss: 0.951485  [   96/  118]
train() client id: f_00009-6-0 loss: 0.851591  [   32/  118]
train() client id: f_00009-6-1 loss: 0.940584  [   64/  118]
train() client id: f_00009-6-2 loss: 0.788462  [   96/  118]
At round 64 accuracy: 0.649867374005305
At round 64 training accuracy: 0.590878604963112
At round 64 training loss: 0.826671720232349
update_location
xs = -4.528292 221.001589 230.045120 -230.943528 119.896481 -155.217951 -272.215960 293.375741 -1.680116 214.695607 
ys = 307.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -237.154970 4.001482 
xs mean: 41.442869159412865
ys mean: 9.371751218646875
dists_uav = 323.466935 243.071361 250.843579 252.672085 156.126834 184.663293 290.019471 310.543246 257.381628 236.875950 
uav_gains = -117.530113 -110.719093 -111.367917 -111.524936 -104.844630 -106.721821 -114.875066 -116.579160 -111.935969 -110.225628 
uav_gains_db_mean: -111.63243316485281
dists_bs = 215.963354 426.895420 440.710878 162.395379 343.244507 173.318482 202.927156 507.031355 447.114640 425.562169 
bs_gains = -104.929701 -113.216061 -113.603365 -101.463122 -110.563960 -102.254716 -104.172584 -115.308034 -113.778788 -113.178024 
bs_gains_db_mean: -109.24683534510808
Round 65
-------------------------------
ene_coms = [0.02061588 0.01360887 0.01177566 0.01191362 0.01111315 0.00695224
 0.01574118 0.01861094 0.01427216 0.01356601]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.39448321 4.85820414 2.30559437 0.84632043 5.58234971 2.67333523
 1.06748231 3.33787295 2.42867108 2.20813964]
obj_prev = 27.70245307535614
eta_min = 5.958334655839358e-39	eta_max = 0.9541830093779471
af = 5.777823695651855	bf = 0.8175671214911233	zeta = 6.3556060652170405	eta = 0.9090909090909091
af = 5.777823695651855	bf = 0.8175671214911233	zeta = 15.050777904815295	eta = 0.383888708755932
af = 5.777823695651855	bf = 0.8175671214911233	zeta = 10.305395435438193	eta = 0.5606600670346986
af = 5.777823695651855	bf = 0.8175671214911233	zeta = 9.470684522911627	eta = 0.6100745602573979
af = 5.777823695651855	bf = 0.8175671214911233	zeta = 9.419282683738448	eta = 0.6134037898264538
af = 5.777823695651855	bf = 0.8175671214911233	zeta = 9.419061758031905	eta = 0.6134181773173892
eta = 0.6134181773173892
ene_coms = [0.02061588 0.01360887 0.01177566 0.01191362 0.01111315 0.00695224
 0.01574118 0.01861094 0.01427216 0.01356601]
ene_comp = [0.04032705 0.08481482 0.03968693 0.0137624  0.09793711 0.04672816
 0.01728302 0.05729001 0.04160727 0.0377666 ]
ene_total = [0.93280845 1.50649873 0.78770003 0.39300396 1.6691519  0.82164631
 0.50547703 1.16175979 0.85530515 0.7857104 ]
ti_comp = [1.54147404 1.61154421 1.62987622 1.62849667 1.63650136 1.67811049
 1.59022109 1.5615235  1.60491131 1.61197275]
ti_coms = [0.20615883 0.13608866 0.11775665 0.1191362  0.11113151 0.06952238
 0.15741178 0.18610937 0.14272157 0.13566012]
t_total = [26.74972725 26.74972725 26.74972725 26.74972725 26.74972725 26.74972725
 26.74972725 26.74972725 26.74972725 26.74972725]
ene_coms = [0.02061588 0.01360887 0.01177566 0.01191362 0.01111315 0.00695224
 0.01574118 0.01861094 0.01427216 0.01356601]
ene_comp = [1.72503056e-06 1.46828637e-05 1.47066379e-06 6.14312293e-08
 2.19224520e-05 2.26451423e-06 1.27592120e-07 4.81969588e-06
 1.74777521e-06 1.29565156e-06]
ene_total = [0.31557851 0.20852562 0.18026393 0.18235393 0.17043635 0.10644744
 0.24094055 0.28493766 0.21848013 0.20766478]
optimize_network iter = 0 obj = 2.11562889383957
eta = 0.6134181773173892
freqs = [13080676.07887981 26314766.02728051 12174830.48992397  4225493.53876088
 29922709.55185018 13922850.67811357  5434156.1394427  18344267.25754316
 12962482.35761149 11714403.37531967]
eta_min = 0.6134181773173903	eta_max = 0.7894005670789017
af = 0.0006068085037931356	bf = 0.8175671214911233	zeta = 0.0006674893541724492	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02061588 0.01360887 0.01177566 0.01191362 0.01111315 0.00695224
 0.01574118 0.01861094 0.01427216 0.01356601]
ene_comp = [3.36423469e-07 2.86352024e-06 2.86815679e-07 1.19806035e-08
 4.27541836e-06 4.41636077e-07 2.48836075e-08 9.39959466e-07
 3.40859236e-07 2.52683983e-07]
ene_total = [1.34187571 0.88596505 0.76647727 0.77543864 0.72361493 0.45253845
 1.02456893 1.21141639 0.92897332 0.88300582]
ti_comp = [0.7459048  0.81597496 0.83430698 0.83292743 0.84093212 0.88254125
 0.79465185 0.76595425 0.80934206 0.8164035 ]
ti_coms = [0.20615883 0.13608866 0.11775665 0.1191362  0.11113151 0.06952238
 0.15741178 0.18610937 0.14272157 0.13566012]
t_total = [26.74972725 26.74972725 26.74972725 26.74972725 26.74972725 26.74972725
 26.74972725 26.74972725 26.74972725 26.74972725]
ene_coms = [0.02061588 0.01360887 0.01177566 0.01191362 0.01111315 0.00695224
 0.01574118 0.01861094 0.01427216 0.01356601]
ene_comp = [8.34726072e-07 6.48907961e-06 6.35934592e-07 2.66066492e-08
 9.40679206e-06 9.27657562e-07 5.78930768e-08 2.26961699e-06
 7.78690688e-07 5.72315423e-07]
ene_total = [0.57925915 0.38254486 0.33087372 0.33473266 0.31250577 0.19536014
 0.44227482 0.52296737 0.4010206  0.38117457]
optimize_network iter = 1 obj = 3.882713655892865
eta = 0.7894005670789017
freqs = [13080676.07887981 25148500.12150626 11509033.55849563  3997646.04566316
 28177569.72008245 12810346.91625405  5262109.85217892 18096444.0974752
 12438107.01082789 11192321.98197726]
eta_min = 0.7894005670789	eta_max = 0.789400567078864
af = 0.0005504561304064595	bf = 0.8175671214911233	zeta = 0.0006055017434471055	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02061588 0.01360887 0.01177566 0.01191362 0.01111315 0.00695224
 0.01574118 0.01861094 0.01427216 0.01356601]
ene_comp = [3.36423469e-07 2.61532345e-06 2.56303629e-07 1.07233996e-08
 3.79126245e-06 3.73878072e-07 2.33329117e-08 9.14734121e-07
 3.13839272e-07 2.30662904e-07]
ene_total = [1.34187571 0.8859489  0.76647529 0.77543856 0.72358342 0.45253404
 1.02456883 1.21141475 0.92897156 0.88300439]
ti_comp = [0.7459048  0.81597496 0.83430698 0.83292743 0.84093212 0.88254125
 0.79465185 0.76595425 0.80934206 0.8164035 ]
ti_coms = [0.20615883 0.13608866 0.11775665 0.1191362  0.11113151 0.06952238
 0.15741178 0.18610937 0.14272157 0.13566012]
t_total = [26.74972725 26.74972725 26.74972725 26.74972725 26.74972725 26.74972725
 26.74972725 26.74972725 26.74972725 26.74972725]
ene_coms = [0.02061588 0.01360887 0.01177566 0.01191362 0.01111315 0.00695224
 0.01574118 0.01861094 0.01427216 0.01356601]
ene_comp = [8.34726072e-07 6.48907961e-06 6.35934592e-07 2.66066492e-08
 9.40679206e-06 9.27657562e-07 5.78930768e-08 2.26961699e-06
 7.78690688e-07 5.72315423e-07]
ene_total = [0.57925915 0.38254486 0.33087372 0.33473266 0.31250577 0.19536014
 0.44227482 0.52296737 0.4010206  0.38117457]
optimize_network iter = 2 obj = 3.8827136558921698
eta = 0.789400567078864
freqs = [13080676.07887946 25148500.12150609 11509033.55849561  3997646.04566315
 28177569.72008242 12810346.91625417  5262109.85217885 18096444.09747482
 12438107.01082778 11192321.98197719]
Done!
ene_coms = [0.02061588 0.01360887 0.01177566 0.01191362 0.01111315 0.00695224
 0.01574118 0.01861094 0.01427216 0.01356601]
ene_comp = [7.54569022e-07 5.86594647e-06 5.74867084e-07 2.40516667e-08
 8.50347691e-06 8.38576489e-07 5.23337222e-08 2.05167028e-06
 7.03914602e-07 5.17357135e-07]
ene_total = [0.02061664 0.01361473 0.01177624 0.01191364 0.01112165 0.00695308
 0.01574123 0.01861299 0.01427286 0.01356653]
At round 65 energy consumption: 0.13818959333803785
At round 65 eta: 0.789400567078864
At round 65 a_n: 5.917122803248752
At round 65 local rounds: 7.74360242002329
At round 65 global rounds: 28.096575195739295
gradient difference: 0.7534738779067993
train() client id: f_00000-0-0 loss: 1.031642  [   32/  126]
train() client id: f_00000-0-1 loss: 0.997769  [   64/  126]
train() client id: f_00000-0-2 loss: 0.793664  [   96/  126]
train() client id: f_00000-1-0 loss: 0.678081  [   32/  126]
train() client id: f_00000-1-1 loss: 0.845992  [   64/  126]
train() client id: f_00000-1-2 loss: 0.937393  [   96/  126]
train() client id: f_00000-2-0 loss: 0.873183  [   32/  126]
train() client id: f_00000-2-1 loss: 0.729508  [   64/  126]
train() client id: f_00000-2-2 loss: 1.024297  [   96/  126]
train() client id: f_00000-3-0 loss: 0.814357  [   32/  126]
train() client id: f_00000-3-1 loss: 0.787486  [   64/  126]
train() client id: f_00000-3-2 loss: 0.897170  [   96/  126]
train() client id: f_00000-4-0 loss: 0.962503  [   32/  126]
train() client id: f_00000-4-1 loss: 0.838819  [   64/  126]
train() client id: f_00000-4-2 loss: 0.812124  [   96/  126]
train() client id: f_00000-5-0 loss: 0.811055  [   32/  126]
train() client id: f_00000-5-1 loss: 0.801911  [   64/  126]
train() client id: f_00000-5-2 loss: 0.938323  [   96/  126]
train() client id: f_00000-6-0 loss: 0.908842  [   32/  126]
train() client id: f_00000-6-1 loss: 0.840806  [   64/  126]
train() client id: f_00000-6-2 loss: 0.836568  [   96/  126]
train() client id: f_00001-0-0 loss: 0.421080  [   32/  265]
train() client id: f_00001-0-1 loss: 0.628045  [   64/  265]
train() client id: f_00001-0-2 loss: 0.514749  [   96/  265]
train() client id: f_00001-0-3 loss: 0.424344  [  128/  265]
train() client id: f_00001-0-4 loss: 0.507272  [  160/  265]
train() client id: f_00001-0-5 loss: 0.434688  [  192/  265]
train() client id: f_00001-0-6 loss: 0.501763  [  224/  265]
train() client id: f_00001-0-7 loss: 0.670871  [  256/  265]
train() client id: f_00001-1-0 loss: 0.416358  [   32/  265]
train() client id: f_00001-1-1 loss: 0.557933  [   64/  265]
train() client id: f_00001-1-2 loss: 0.567527  [   96/  265]
train() client id: f_00001-1-3 loss: 0.623300  [  128/  265]
train() client id: f_00001-1-4 loss: 0.632358  [  160/  265]
train() client id: f_00001-1-5 loss: 0.427894  [  192/  265]
train() client id: f_00001-1-6 loss: 0.414369  [  224/  265]
train() client id: f_00001-1-7 loss: 0.425231  [  256/  265]
train() client id: f_00001-2-0 loss: 0.564297  [   32/  265]
train() client id: f_00001-2-1 loss: 0.489005  [   64/  265]
train() client id: f_00001-2-2 loss: 0.466609  [   96/  265]
train() client id: f_00001-2-3 loss: 0.502324  [  128/  265]
train() client id: f_00001-2-4 loss: 0.479026  [  160/  265]
train() client id: f_00001-2-5 loss: 0.523752  [  192/  265]
train() client id: f_00001-2-6 loss: 0.475839  [  224/  265]
train() client id: f_00001-2-7 loss: 0.511062  [  256/  265]
train() client id: f_00001-3-0 loss: 0.467913  [   32/  265]
train() client id: f_00001-3-1 loss: 0.593756  [   64/  265]
train() client id: f_00001-3-2 loss: 0.534816  [   96/  265]
train() client id: f_00001-3-3 loss: 0.429593  [  128/  265]
train() client id: f_00001-3-4 loss: 0.418114  [  160/  265]
train() client id: f_00001-3-5 loss: 0.533586  [  192/  265]
train() client id: f_00001-3-6 loss: 0.410351  [  224/  265]
train() client id: f_00001-3-7 loss: 0.575017  [  256/  265]
train() client id: f_00001-4-0 loss: 0.460385  [   32/  265]
train() client id: f_00001-4-1 loss: 0.568226  [   64/  265]
train() client id: f_00001-4-2 loss: 0.529758  [   96/  265]
train() client id: f_00001-4-3 loss: 0.414327  [  128/  265]
train() client id: f_00001-4-4 loss: 0.467344  [  160/  265]
train() client id: f_00001-4-5 loss: 0.632253  [  192/  265]
train() client id: f_00001-4-6 loss: 0.521505  [  224/  265]
train() client id: f_00001-4-7 loss: 0.426820  [  256/  265]
train() client id: f_00001-5-0 loss: 0.537649  [   32/  265]
train() client id: f_00001-5-1 loss: 0.451303  [   64/  265]
train() client id: f_00001-5-2 loss: 0.440713  [   96/  265]
train() client id: f_00001-5-3 loss: 0.450118  [  128/  265]
train() client id: f_00001-5-4 loss: 0.509286  [  160/  265]
train() client id: f_00001-5-5 loss: 0.533061  [  192/  265]
train() client id: f_00001-5-6 loss: 0.704480  [  224/  265]
train() client id: f_00001-5-7 loss: 0.411400  [  256/  265]
train() client id: f_00001-6-0 loss: 0.410436  [   32/  265]
train() client id: f_00001-6-1 loss: 0.439259  [   64/  265]
train() client id: f_00001-6-2 loss: 0.537209  [   96/  265]
train() client id: f_00001-6-3 loss: 0.533595  [  128/  265]
train() client id: f_00001-6-4 loss: 0.430296  [  160/  265]
train() client id: f_00001-6-5 loss: 0.555138  [  192/  265]
train() client id: f_00001-6-6 loss: 0.604935  [  224/  265]
train() client id: f_00001-6-7 loss: 0.524699  [  256/  265]
train() client id: f_00002-0-0 loss: 0.734018  [   32/  124]
train() client id: f_00002-0-1 loss: 0.912493  [   64/  124]
train() client id: f_00002-0-2 loss: 0.941548  [   96/  124]
train() client id: f_00002-1-0 loss: 0.902849  [   32/  124]
train() client id: f_00002-1-1 loss: 0.737550  [   64/  124]
train() client id: f_00002-1-2 loss: 0.875212  [   96/  124]
train() client id: f_00002-2-0 loss: 0.880787  [   32/  124]
train() client id: f_00002-2-1 loss: 0.868196  [   64/  124]
train() client id: f_00002-2-2 loss: 0.798139  [   96/  124]
train() client id: f_00002-3-0 loss: 0.922248  [   32/  124]
train() client id: f_00002-3-1 loss: 0.628709  [   64/  124]
train() client id: f_00002-3-2 loss: 0.779112  [   96/  124]
train() client id: f_00002-4-0 loss: 0.922133  [   32/  124]
train() client id: f_00002-4-1 loss: 0.634142  [   64/  124]
train() client id: f_00002-4-2 loss: 0.927500  [   96/  124]
train() client id: f_00002-5-0 loss: 0.569888  [   32/  124]
train() client id: f_00002-5-1 loss: 1.184759  [   64/  124]
train() client id: f_00002-5-2 loss: 0.725881  [   96/  124]
train() client id: f_00002-6-0 loss: 0.876731  [   32/  124]
train() client id: f_00002-6-1 loss: 0.661419  [   64/  124]
train() client id: f_00002-6-2 loss: 0.856398  [   96/  124]
train() client id: f_00003-0-0 loss: 0.584471  [   32/   43]
train() client id: f_00003-1-0 loss: 0.432043  [   32/   43]
train() client id: f_00003-2-0 loss: 0.393590  [   32/   43]
train() client id: f_00003-3-0 loss: 0.401912  [   32/   43]
train() client id: f_00003-4-0 loss: 0.448880  [   32/   43]
train() client id: f_00003-5-0 loss: 0.561996  [   32/   43]
train() client id: f_00003-6-0 loss: 0.519546  [   32/   43]
train() client id: f_00004-0-0 loss: 0.634019  [   32/  306]
train() client id: f_00004-0-1 loss: 0.700046  [   64/  306]
train() client id: f_00004-0-2 loss: 0.508158  [   96/  306]
train() client id: f_00004-0-3 loss: 0.529124  [  128/  306]
train() client id: f_00004-0-4 loss: 0.698803  [  160/  306]
train() client id: f_00004-0-5 loss: 0.680551  [  192/  306]
train() client id: f_00004-0-6 loss: 0.535640  [  224/  306]
train() client id: f_00004-0-7 loss: 0.566051  [  256/  306]
train() client id: f_00004-0-8 loss: 0.578524  [  288/  306]
train() client id: f_00004-1-0 loss: 0.752842  [   32/  306]
train() client id: f_00004-1-1 loss: 0.600641  [   64/  306]
train() client id: f_00004-1-2 loss: 0.639955  [   96/  306]
train() client id: f_00004-1-3 loss: 0.449674  [  128/  306]
train() client id: f_00004-1-4 loss: 0.595889  [  160/  306]
train() client id: f_00004-1-5 loss: 0.602447  [  192/  306]
train() client id: f_00004-1-6 loss: 0.628250  [  224/  306]
train() client id: f_00004-1-7 loss: 0.481678  [  256/  306]
train() client id: f_00004-1-8 loss: 0.563925  [  288/  306]
train() client id: f_00004-2-0 loss: 0.599933  [   32/  306]
train() client id: f_00004-2-1 loss: 0.651934  [   64/  306]
train() client id: f_00004-2-2 loss: 0.502703  [   96/  306]
train() client id: f_00004-2-3 loss: 0.573193  [  128/  306]
train() client id: f_00004-2-4 loss: 0.661268  [  160/  306]
train() client id: f_00004-2-5 loss: 0.646679  [  192/  306]
train() client id: f_00004-2-6 loss: 0.629887  [  224/  306]
train() client id: f_00004-2-7 loss: 0.559375  [  256/  306]
train() client id: f_00004-2-8 loss: 0.648506  [  288/  306]
train() client id: f_00004-3-0 loss: 0.599400  [   32/  306]
train() client id: f_00004-3-1 loss: 0.541595  [   64/  306]
train() client id: f_00004-3-2 loss: 0.657941  [   96/  306]
train() client id: f_00004-3-3 loss: 0.581760  [  128/  306]
train() client id: f_00004-3-4 loss: 0.562908  [  160/  306]
train() client id: f_00004-3-5 loss: 0.596518  [  192/  306]
train() client id: f_00004-3-6 loss: 0.683845  [  224/  306]
train() client id: f_00004-3-7 loss: 0.778135  [  256/  306]
train() client id: f_00004-3-8 loss: 0.599905  [  288/  306]
train() client id: f_00004-4-0 loss: 0.556678  [   32/  306]
train() client id: f_00004-4-1 loss: 0.551143  [   64/  306]
train() client id: f_00004-4-2 loss: 0.620734  [   96/  306]
train() client id: f_00004-4-3 loss: 0.584502  [  128/  306]
train() client id: f_00004-4-4 loss: 0.796383  [  160/  306]
train() client id: f_00004-4-5 loss: 0.546046  [  192/  306]
train() client id: f_00004-4-6 loss: 0.490440  [  224/  306]
train() client id: f_00004-4-7 loss: 0.650933  [  256/  306]
train() client id: f_00004-4-8 loss: 0.686555  [  288/  306]
train() client id: f_00004-5-0 loss: 0.601273  [   32/  306]
train() client id: f_00004-5-1 loss: 0.529831  [   64/  306]
train() client id: f_00004-5-2 loss: 0.649553  [   96/  306]
train() client id: f_00004-5-3 loss: 0.509789  [  128/  306]
train() client id: f_00004-5-4 loss: 0.620445  [  160/  306]
train() client id: f_00004-5-5 loss: 0.690782  [  192/  306]
train() client id: f_00004-5-6 loss: 0.582015  [  224/  306]
train() client id: f_00004-5-7 loss: 0.579203  [  256/  306]
train() client id: f_00004-5-8 loss: 0.657375  [  288/  306]
train() client id: f_00004-6-0 loss: 0.629262  [   32/  306]
train() client id: f_00004-6-1 loss: 0.555871  [   64/  306]
train() client id: f_00004-6-2 loss: 0.574235  [   96/  306]
train() client id: f_00004-6-3 loss: 0.611843  [  128/  306]
train() client id: f_00004-6-4 loss: 0.664378  [  160/  306]
train() client id: f_00004-6-5 loss: 0.684261  [  192/  306]
train() client id: f_00004-6-6 loss: 0.587225  [  224/  306]
train() client id: f_00004-6-7 loss: 0.642876  [  256/  306]
train() client id: f_00004-6-8 loss: 0.590400  [  288/  306]
train() client id: f_00005-0-0 loss: 0.562136  [   32/  146]
train() client id: f_00005-0-1 loss: 0.670024  [   64/  146]
train() client id: f_00005-0-2 loss: 0.708746  [   96/  146]
train() client id: f_00005-0-3 loss: 0.737765  [  128/  146]
train() client id: f_00005-1-0 loss: 0.658223  [   32/  146]
train() client id: f_00005-1-1 loss: 0.709160  [   64/  146]
train() client id: f_00005-1-2 loss: 0.583804  [   96/  146]
train() client id: f_00005-1-3 loss: 0.804609  [  128/  146]
train() client id: f_00005-2-0 loss: 0.787530  [   32/  146]
train() client id: f_00005-2-1 loss: 0.479725  [   64/  146]
train() client id: f_00005-2-2 loss: 0.927307  [   96/  146]
train() client id: f_00005-2-3 loss: 0.726158  [  128/  146]
train() client id: f_00005-3-0 loss: 0.699075  [   32/  146]
train() client id: f_00005-3-1 loss: 0.677032  [   64/  146]
train() client id: f_00005-3-2 loss: 0.516030  [   96/  146]
train() client id: f_00005-3-3 loss: 0.774600  [  128/  146]
train() client id: f_00005-4-0 loss: 0.556443  [   32/  146]
train() client id: f_00005-4-1 loss: 0.612475  [   64/  146]
train() client id: f_00005-4-2 loss: 1.055133  [   96/  146]
train() client id: f_00005-4-3 loss: 0.439940  [  128/  146]
train() client id: f_00005-5-0 loss: 0.736803  [   32/  146]
train() client id: f_00005-5-1 loss: 0.459974  [   64/  146]
train() client id: f_00005-5-2 loss: 0.846088  [   96/  146]
train() client id: f_00005-5-3 loss: 0.565403  [  128/  146]
train() client id: f_00005-6-0 loss: 0.761902  [   32/  146]
train() client id: f_00005-6-1 loss: 0.601868  [   64/  146]
train() client id: f_00005-6-2 loss: 0.439251  [   96/  146]
train() client id: f_00005-6-3 loss: 0.785734  [  128/  146]
train() client id: f_00006-0-0 loss: 0.483277  [   32/   54]
train() client id: f_00006-1-0 loss: 0.509815  [   32/   54]
train() client id: f_00006-2-0 loss: 0.503712  [   32/   54]
train() client id: f_00006-3-0 loss: 0.395855  [   32/   54]
train() client id: f_00006-4-0 loss: 0.388369  [   32/   54]
train() client id: f_00006-5-0 loss: 0.513245  [   32/   54]
train() client id: f_00006-6-0 loss: 0.457514  [   32/   54]
train() client id: f_00007-0-0 loss: 0.578022  [   32/  179]
train() client id: f_00007-0-1 loss: 0.745065  [   64/  179]
train() client id: f_00007-0-2 loss: 0.572351  [   96/  179]
train() client id: f_00007-0-3 loss: 0.804720  [  128/  179]
train() client id: f_00007-0-4 loss: 0.701249  [  160/  179]
train() client id: f_00007-1-0 loss: 0.702692  [   32/  179]
train() client id: f_00007-1-1 loss: 0.760341  [   64/  179]
train() client id: f_00007-1-2 loss: 0.914197  [   96/  179]
train() client id: f_00007-1-3 loss: 0.511825  [  128/  179]
train() client id: f_00007-1-4 loss: 0.535982  [  160/  179]
train() client id: f_00007-2-0 loss: 0.529989  [   32/  179]
train() client id: f_00007-2-1 loss: 0.526536  [   64/  179]
train() client id: f_00007-2-2 loss: 0.502142  [   96/  179]
train() client id: f_00007-2-3 loss: 1.030444  [  128/  179]
train() client id: f_00007-2-4 loss: 0.754312  [  160/  179]
train() client id: f_00007-3-0 loss: 0.663936  [   32/  179]
train() client id: f_00007-3-1 loss: 0.504061  [   64/  179]
train() client id: f_00007-3-2 loss: 0.839689  [   96/  179]
train() client id: f_00007-3-3 loss: 0.747646  [  128/  179]
train() client id: f_00007-3-4 loss: 0.642456  [  160/  179]
train() client id: f_00007-4-0 loss: 0.755031  [   32/  179]
train() client id: f_00007-4-1 loss: 0.575490  [   64/  179]
train() client id: f_00007-4-2 loss: 0.493281  [   96/  179]
train() client id: f_00007-4-3 loss: 0.886676  [  128/  179]
train() client id: f_00007-4-4 loss: 0.590254  [  160/  179]
train() client id: f_00007-5-0 loss: 0.754261  [   32/  179]
train() client id: f_00007-5-1 loss: 0.554676  [   64/  179]
train() client id: f_00007-5-2 loss: 0.731740  [   96/  179]
train() client id: f_00007-5-3 loss: 0.704618  [  128/  179]
train() client id: f_00007-5-4 loss: 0.511441  [  160/  179]
train() client id: f_00007-6-0 loss: 0.716249  [   32/  179]
train() client id: f_00007-6-1 loss: 0.661259  [   64/  179]
train() client id: f_00007-6-2 loss: 0.573685  [   96/  179]
train() client id: f_00007-6-3 loss: 0.636365  [  128/  179]
train() client id: f_00007-6-4 loss: 0.604688  [  160/  179]
train() client id: f_00008-0-0 loss: 0.765620  [   32/  130]
train() client id: f_00008-0-1 loss: 0.759363  [   64/  130]
train() client id: f_00008-0-2 loss: 0.701682  [   96/  130]
train() client id: f_00008-0-3 loss: 0.880139  [  128/  130]
train() client id: f_00008-1-0 loss: 0.680959  [   32/  130]
train() client id: f_00008-1-1 loss: 0.817921  [   64/  130]
train() client id: f_00008-1-2 loss: 0.770041  [   96/  130]
train() client id: f_00008-1-3 loss: 0.830984  [  128/  130]
train() client id: f_00008-2-0 loss: 0.845449  [   32/  130]
train() client id: f_00008-2-1 loss: 0.714891  [   64/  130]
train() client id: f_00008-2-2 loss: 0.771974  [   96/  130]
train() client id: f_00008-2-3 loss: 0.769082  [  128/  130]
train() client id: f_00008-3-0 loss: 0.794936  [   32/  130]
train() client id: f_00008-3-1 loss: 0.746957  [   64/  130]
train() client id: f_00008-3-2 loss: 0.796727  [   96/  130]
train() client id: f_00008-3-3 loss: 0.753513  [  128/  130]
train() client id: f_00008-4-0 loss: 0.797924  [   32/  130]
train() client id: f_00008-4-1 loss: 0.738625  [   64/  130]
train() client id: f_00008-4-2 loss: 0.790778  [   96/  130]
train() client id: f_00008-4-3 loss: 0.772774  [  128/  130]
train() client id: f_00008-5-0 loss: 0.769434  [   32/  130]
train() client id: f_00008-5-1 loss: 0.743528  [   64/  130]
train() client id: f_00008-5-2 loss: 0.759764  [   96/  130]
train() client id: f_00008-5-3 loss: 0.788504  [  128/  130]
train() client id: f_00008-6-0 loss: 0.851496  [   32/  130]
train() client id: f_00008-6-1 loss: 0.821507  [   64/  130]
train() client id: f_00008-6-2 loss: 0.714160  [   96/  130]
train() client id: f_00008-6-3 loss: 0.711727  [  128/  130]
train() client id: f_00009-0-0 loss: 1.080686  [   32/  118]
train() client id: f_00009-0-1 loss: 1.049864  [   64/  118]
train() client id: f_00009-0-2 loss: 1.049504  [   96/  118]
train() client id: f_00009-1-0 loss: 0.967821  [   32/  118]
train() client id: f_00009-1-1 loss: 0.997319  [   64/  118]
train() client id: f_00009-1-2 loss: 1.120556  [   96/  118]
train() client id: f_00009-2-0 loss: 0.957740  [   32/  118]
train() client id: f_00009-2-1 loss: 0.953027  [   64/  118]
train() client id: f_00009-2-2 loss: 1.028165  [   96/  118]
train() client id: f_00009-3-0 loss: 0.913143  [   32/  118]
train() client id: f_00009-3-1 loss: 0.929106  [   64/  118]
train() client id: f_00009-3-2 loss: 0.974738  [   96/  118]
train() client id: f_00009-4-0 loss: 0.946989  [   32/  118]
train() client id: f_00009-4-1 loss: 0.927775  [   64/  118]
train() client id: f_00009-4-2 loss: 1.069811  [   96/  118]
train() client id: f_00009-5-0 loss: 0.772124  [   32/  118]
train() client id: f_00009-5-1 loss: 0.911607  [   64/  118]
train() client id: f_00009-5-2 loss: 0.850969  [   96/  118]
train() client id: f_00009-6-0 loss: 0.899125  [   32/  118]
train() client id: f_00009-6-1 loss: 0.962440  [   64/  118]
train() client id: f_00009-6-2 loss: 0.947407  [   96/  118]
At round 65 accuracy: 0.649867374005305
At round 65 training accuracy: 0.590878604963112
At round 65 training loss: 0.8134843092075289
update_location
xs = -4.528292 226.001589 235.045120 -235.943528 124.896481 -160.217951 -277.215960 298.375741 -1.680116 219.695607 
ys = 312.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -242.154970 4.001482 
xs mean: 42.442869159412865
ys mean: 9.371751218646875
dists_uav = 328.225133 247.626135 255.436787 257.250107 159.998604 188.885445 294.717583 315.271098 261.995901 241.417008 
uav_gains = -117.854560 -111.095521 -111.765147 -111.924372 -105.113460 -106.986271 -115.283232 -116.938836 -112.346439 -110.585187 
uav_gains_db_mean: -111.98930250085641
dists_bs = 219.068596 431.537617 445.310599 164.184939 347.549646 172.819199 205.369399 511.653743 451.727851 430.145459 
bs_gains = -105.103303 -113.347582 -113.729624 -101.596392 -110.715531 -102.219635 -104.318060 -115.418391 -113.903611 -113.308289 
bs_gains_db_mean: -109.36604174118355
Round 66
-------------------------------
ene_coms = [0.02138148 0.01375892 0.01213049 0.01227826 0.01123272 0.00694112
 0.01635674 0.01933009 0.01442709 0.01371378]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.26017613 4.57780467 2.17412007 0.79937964 5.25985734 2.51851161
 1.00915128 3.14869092 2.28894625 2.08114117]
obj_prev = 26.1177790800194
eta_min = 2.852423550326523e-41	eta_max = 0.9551604716300203
af = 5.443341958957153	bf = 0.7890851876917137	zeta = 5.987676154852869	eta = 0.9090909090909091
af = 5.443341958957153	bf = 0.7890851876917137	zeta = 14.386801973899805	eta = 0.3783566333110259
af = 5.443341958957153	bf = 0.7890851876917137	zeta = 9.779831711097449	eta = 0.5565885098800258
af = 5.443341958957153	bf = 0.7890851876917137	zeta = 8.972472719243006	eta = 0.6066713301098117
af = 5.443341958957153	bf = 0.7890851876917137	zeta = 8.922547803332343	eta = 0.6100658779238151
af = 5.443341958957153	bf = 0.7890851876917137	zeta = 8.922330898348836	eta = 0.6100807088385947
eta = 0.6100807088385947
ene_coms = [0.02138148 0.01375892 0.01213049 0.01227826 0.01123272 0.00694112
 0.01635674 0.01933009 0.01442709 0.01371378]
ene_comp = [0.04077723 0.08576164 0.04012997 0.01391604 0.09903042 0.04724981
 0.01747596 0.05792956 0.04207175 0.0381882 ]
ene_total = [0.88866733 1.42282007 0.7471545  0.37449316 1.57640396 0.77475389
 0.48369749 1.10456154 0.80774951 0.74202944]
ti_comp = [1.65372367 1.7299493  1.74623363 1.74475595 1.75521135 1.79812731
 1.70397108 1.6742376  1.72326758 1.73040068]
ti_coms = [0.21381483 0.1375892  0.12130487 0.12278255 0.11232715 0.06941119
 0.16356742 0.1933009  0.14427092 0.13713782]
t_total = [26.69972305 26.69972305 26.69972305 26.69972305 26.69972305 26.69972305
 26.69972305 26.69972305 26.69972305 26.69972305]
ene_coms = [0.02138148 0.01375892 0.01213049 0.01227826 0.01123272 0.00694112
 0.01635674 0.01933009 0.01442709 0.01371378]
ene_comp = [1.54955614e-06 1.31732348e-05 1.32459137e-06 5.53296030e-08
 1.97027572e-05 2.03910343e-06 1.14888988e-07 4.33457452e-06
 1.56727852e-06 1.16244986e-06]
ene_total = [0.30570779 0.19689611 0.17344542 0.17553988 0.16087296 0.09926456
 0.23384983 0.27641935 0.20628287 0.19607907]
optimize_network iter = 0 obj = 2.024357845389127
eta = 0.6100807088385947
freqs = [12328913.05334086 24787326.29412057 11490436.3931538   3987961.47216329
 28210396.7529691  13138615.53686248  5128008.39483921 17300279.75374795
 12206968.37917026 11034496.37904773]
eta_min = 0.6100807088385951	eta_max = 0.7955336103150649
af = 0.0005079051261941307	bf = 0.7890851876917137	zeta = 0.0005586956388135438	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02138148 0.01375892 0.01213049 0.01227826 0.01123272 0.00694112
 0.01635674 0.01933009 0.01442709 0.01371378]
ene_comp = [2.98865292e-07 2.54074219e-06 2.55475988e-07 1.06715062e-08
 3.80010128e-06 3.93285035e-07 2.21588171e-08 8.36016098e-07
 3.02283435e-07 2.24203505e-07]
ene_total = [1.3111383  0.84385752 0.74386132 0.75290751 0.689027   0.42565593
 1.00300247 1.18537908 0.88469279 0.84094762]
ti_comp = [0.76548746 0.84171309 0.85799742 0.85651974 0.86697514 0.9098911
 0.81573487 0.78600139 0.83503137 0.84216447]
ti_coms = [0.21381483 0.1375892  0.12130487 0.12278255 0.11232715 0.06941119
 0.16356742 0.1933009  0.14427092 0.13713782]
t_total = [26.69972305 26.69972305 26.69972305 26.69972305 26.69972305 26.69972305
 26.69972305 26.69972305 26.69972305 26.69972305]
ene_coms = [0.02138148 0.01375892 0.01213049 0.01227826 0.01123272 0.00694112
 0.01635674 0.01933009 0.01442709 0.01371378]
ene_comp = [7.17269645e-07 5.51894960e-06 5.44176850e-07 2.27707632e-08
 8.00936275e-06 7.89817414e-07 4.97198949e-08 1.95056055e-06
 6.62020507e-07 4.86742384e-07]
ene_total = [0.58296488 0.37527401 0.33074074 0.33475529 0.3064674  0.18926438
 0.44595201 0.52706926 0.39335865 0.37390618]
optimize_network iter = 1 obj = 3.8597527934853337
eta = 0.7955336103150649
freqs = [12328913.05334085 23581645.88957494 10825002.99420387  3760307.8593283
 26436691.61561212 12018652.01651613  4958348.60832903 17057761.56581249
 11660921.76296525 10494878.50677151]
eta_min = 0.7955336103150681	eta_max = 0.7955336103150038
af = 0.0004569820392108726	bf = 0.7890851876917137	zeta = 0.0005026802431319599	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02138148 0.01375892 0.01213049 0.01227826 0.01123272 0.00694112
 0.01635674 0.01933009 0.01442709 0.01371378]
ene_comp = [2.98865292e-07 2.29958495e-06 2.26742584e-07 9.48791133e-09
 3.33726730e-06 3.29093826e-07 2.07168267e-08 8.12741558e-07
 2.75844591e-07 2.02811322e-07]
ene_total = [1.3111383  0.84384273 0.74385956 0.75290744 0.68899862 0.425652
 1.00300239 1.18537765 0.88469117 0.84094631]
ti_comp = [0.76548746 0.84171309 0.85799742 0.85651974 0.86697514 0.9098911
 0.81573487 0.78600139 0.83503137 0.84216447]
ti_coms = [0.21381483 0.1375892  0.12130487 0.12278255 0.11232715 0.06941119
 0.16356742 0.1933009  0.14427092 0.13713782]
t_total = [26.69972305 26.69972305 26.69972305 26.69972305 26.69972305 26.69972305
 26.69972305 26.69972305 26.69972305 26.69972305]
ene_coms = [0.02138148 0.01375892 0.01213049 0.01227826 0.01123272 0.00694112
 0.01635674 0.01933009 0.01442709 0.01371378]
ene_comp = [7.17269645e-07 5.51894960e-06 5.44176850e-07 2.27707632e-08
 8.00936275e-06 7.89817414e-07 4.97198949e-08 1.95056055e-06
 6.62020507e-07 4.86742384e-07]
ene_total = [0.58296488 0.37527401 0.33074074 0.33475529 0.3064674  0.18926438
 0.44595201 0.52706926 0.39335865 0.37390618]
optimize_network iter = 2 obj = 3.859752793484181
eta = 0.7955336103150038
freqs = [12328913.05334028 23581645.88957467 10825002.99420382  3760307.85932828
 26436691.61561208 12018652.01651631  4958348.60832892 17057761.56581187
 11660921.76296509 10494878.50677139]
Done!
ene_coms = [0.02138148 0.01375892 0.01213049 0.01227826 0.01123272 0.00694112
 0.01635674 0.01933009 0.01442709 0.01371378]
ene_comp = [6.70329248e-07 5.15777206e-06 5.08564194e-07 2.12805724e-08
 7.48520469e-06 7.38129261e-07 4.64660675e-08 1.82290969e-06
 6.18695788e-07 4.54888421e-07]
ene_total = [0.02138215 0.01376408 0.012131   0.01227828 0.0112402  0.00694186
 0.01635679 0.01933191 0.01442771 0.01371424]
At round 66 energy consumption: 0.14156820940954085
At round 66 eta: 0.7955336103150038
At round 66 a_n: 5.574576956279433
At round 66 local rounds: 7.490181173391706
At round 66 global rounds: 27.264025959805448
gradient difference: 0.6335347890853882
train() client id: f_00000-0-0 loss: 0.981942  [   32/  126]
train() client id: f_00000-0-1 loss: 1.173194  [   64/  126]
train() client id: f_00000-0-2 loss: 0.898981  [   96/  126]
train() client id: f_00000-1-0 loss: 0.845971  [   32/  126]
train() client id: f_00000-1-1 loss: 1.118443  [   64/  126]
train() client id: f_00000-1-2 loss: 0.978266  [   96/  126]
train() client id: f_00000-2-0 loss: 0.928651  [   32/  126]
train() client id: f_00000-2-1 loss: 1.048264  [   64/  126]
train() client id: f_00000-2-2 loss: 0.925916  [   96/  126]
train() client id: f_00000-3-0 loss: 0.987389  [   32/  126]
train() client id: f_00000-3-1 loss: 0.992019  [   64/  126]
train() client id: f_00000-3-2 loss: 0.798840  [   96/  126]
train() client id: f_00000-4-0 loss: 1.060450  [   32/  126]
train() client id: f_00000-4-1 loss: 0.896828  [   64/  126]
train() client id: f_00000-4-2 loss: 0.941058  [   96/  126]
train() client id: f_00000-5-0 loss: 0.832888  [   32/  126]
train() client id: f_00000-5-1 loss: 1.038847  [   64/  126]
train() client id: f_00000-5-2 loss: 0.887672  [   96/  126]
train() client id: f_00000-6-0 loss: 0.974761  [   32/  126]
train() client id: f_00000-6-1 loss: 0.899053  [   64/  126]
train() client id: f_00000-6-2 loss: 0.898031  [   96/  126]
train() client id: f_00001-0-0 loss: 0.449285  [   32/  265]
train() client id: f_00001-0-1 loss: 0.424272  [   64/  265]
train() client id: f_00001-0-2 loss: 0.505735  [   96/  265]
train() client id: f_00001-0-3 loss: 0.481653  [  128/  265]
train() client id: f_00001-0-4 loss: 0.447571  [  160/  265]
train() client id: f_00001-0-5 loss: 0.631265  [  192/  265]
train() client id: f_00001-0-6 loss: 0.381314  [  224/  265]
train() client id: f_00001-0-7 loss: 0.551030  [  256/  265]
train() client id: f_00001-1-0 loss: 0.571831  [   32/  265]
train() client id: f_00001-1-1 loss: 0.506043  [   64/  265]
train() client id: f_00001-1-2 loss: 0.442146  [   96/  265]
train() client id: f_00001-1-3 loss: 0.469755  [  128/  265]
train() client id: f_00001-1-4 loss: 0.524900  [  160/  265]
train() client id: f_00001-1-5 loss: 0.392070  [  192/  265]
train() client id: f_00001-1-6 loss: 0.471251  [  224/  265]
train() client id: f_00001-1-7 loss: 0.448400  [  256/  265]
train() client id: f_00001-2-0 loss: 0.574158  [   32/  265]
train() client id: f_00001-2-1 loss: 0.383831  [   64/  265]
train() client id: f_00001-2-2 loss: 0.493409  [   96/  265]
train() client id: f_00001-2-3 loss: 0.506006  [  128/  265]
train() client id: f_00001-2-4 loss: 0.459049  [  160/  265]
train() client id: f_00001-2-5 loss: 0.521950  [  192/  265]
train() client id: f_00001-2-6 loss: 0.460689  [  224/  265]
train() client id: f_00001-2-7 loss: 0.412717  [  256/  265]
train() client id: f_00001-3-0 loss: 0.384335  [   32/  265]
train() client id: f_00001-3-1 loss: 0.469208  [   64/  265]
train() client id: f_00001-3-2 loss: 0.502548  [   96/  265]
train() client id: f_00001-3-3 loss: 0.526623  [  128/  265]
train() client id: f_00001-3-4 loss: 0.508442  [  160/  265]
train() client id: f_00001-3-5 loss: 0.565031  [  192/  265]
train() client id: f_00001-3-6 loss: 0.436835  [  224/  265]
train() client id: f_00001-3-7 loss: 0.403177  [  256/  265]
train() client id: f_00001-4-0 loss: 0.511499  [   32/  265]
train() client id: f_00001-4-1 loss: 0.528927  [   64/  265]
train() client id: f_00001-4-2 loss: 0.435215  [   96/  265]
train() client id: f_00001-4-3 loss: 0.535798  [  128/  265]
train() client id: f_00001-4-4 loss: 0.551459  [  160/  265]
train() client id: f_00001-4-5 loss: 0.451874  [  192/  265]
train() client id: f_00001-4-6 loss: 0.380429  [  224/  265]
train() client id: f_00001-4-7 loss: 0.396547  [  256/  265]
train() client id: f_00001-5-0 loss: 0.512988  [   32/  265]
train() client id: f_00001-5-1 loss: 0.476400  [   64/  265]
train() client id: f_00001-5-2 loss: 0.405778  [   96/  265]
train() client id: f_00001-5-3 loss: 0.348937  [  128/  265]
train() client id: f_00001-5-4 loss: 0.627516  [  160/  265]
train() client id: f_00001-5-5 loss: 0.379822  [  192/  265]
train() client id: f_00001-5-6 loss: 0.554818  [  224/  265]
train() client id: f_00001-5-7 loss: 0.470882  [  256/  265]
train() client id: f_00001-6-0 loss: 0.438123  [   32/  265]
train() client id: f_00001-6-1 loss: 0.409714  [   64/  265]
train() client id: f_00001-6-2 loss: 0.601216  [   96/  265]
train() client id: f_00001-6-3 loss: 0.616188  [  128/  265]
train() client id: f_00001-6-4 loss: 0.391331  [  160/  265]
train() client id: f_00001-6-5 loss: 0.381397  [  192/  265]
train() client id: f_00001-6-6 loss: 0.477241  [  224/  265]
train() client id: f_00001-6-7 loss: 0.464551  [  256/  265]
train() client id: f_00002-0-0 loss: 0.839247  [   32/  124]
train() client id: f_00002-0-1 loss: 1.041955  [   64/  124]
train() client id: f_00002-0-2 loss: 0.935152  [   96/  124]
train() client id: f_00002-1-0 loss: 1.072884  [   32/  124]
train() client id: f_00002-1-1 loss: 0.868437  [   64/  124]
train() client id: f_00002-1-2 loss: 0.888519  [   96/  124]
train() client id: f_00002-2-0 loss: 1.020518  [   32/  124]
train() client id: f_00002-2-1 loss: 0.909326  [   64/  124]
train() client id: f_00002-2-2 loss: 0.856743  [   96/  124]
train() client id: f_00002-3-0 loss: 0.885122  [   32/  124]
train() client id: f_00002-3-1 loss: 0.743552  [   64/  124]
train() client id: f_00002-3-2 loss: 1.025350  [   96/  124]
train() client id: f_00002-4-0 loss: 0.943211  [   32/  124]
train() client id: f_00002-4-1 loss: 0.924724  [   64/  124]
train() client id: f_00002-4-2 loss: 0.801862  [   96/  124]
train() client id: f_00002-5-0 loss: 0.977657  [   32/  124]
train() client id: f_00002-5-1 loss: 0.823823  [   64/  124]
train() client id: f_00002-5-2 loss: 0.742748  [   96/  124]
train() client id: f_00002-6-0 loss: 0.838494  [   32/  124]
train() client id: f_00002-6-1 loss: 0.927786  [   64/  124]
train() client id: f_00002-6-2 loss: 0.778756  [   96/  124]
train() client id: f_00003-0-0 loss: 0.817191  [   32/   43]
train() client id: f_00003-1-0 loss: 0.749897  [   32/   43]
train() client id: f_00003-2-0 loss: 0.761916  [   32/   43]
train() client id: f_00003-3-0 loss: 0.744879  [   32/   43]
train() client id: f_00003-4-0 loss: 0.654570  [   32/   43]
train() client id: f_00003-5-0 loss: 0.748749  [   32/   43]
train() client id: f_00003-6-0 loss: 0.686220  [   32/   43]
train() client id: f_00004-0-0 loss: 1.040140  [   32/  306]
train() client id: f_00004-0-1 loss: 0.869802  [   64/  306]
train() client id: f_00004-0-2 loss: 0.717709  [   96/  306]
train() client id: f_00004-0-3 loss: 1.016744  [  128/  306]
train() client id: f_00004-0-4 loss: 0.781320  [  160/  306]
train() client id: f_00004-0-5 loss: 0.979181  [  192/  306]
train() client id: f_00004-0-6 loss: 0.979785  [  224/  306]
train() client id: f_00004-0-7 loss: 0.833992  [  256/  306]
train() client id: f_00004-0-8 loss: 1.052441  [  288/  306]
train() client id: f_00004-1-0 loss: 0.907572  [   32/  306]
train() client id: f_00004-1-1 loss: 0.870136  [   64/  306]
train() client id: f_00004-1-2 loss: 1.000951  [   96/  306]
train() client id: f_00004-1-3 loss: 0.999436  [  128/  306]
train() client id: f_00004-1-4 loss: 0.893589  [  160/  306]
train() client id: f_00004-1-5 loss: 0.955217  [  192/  306]
train() client id: f_00004-1-6 loss: 0.791591  [  224/  306]
train() client id: f_00004-1-7 loss: 0.820860  [  256/  306]
train() client id: f_00004-1-8 loss: 1.014762  [  288/  306]
train() client id: f_00004-2-0 loss: 0.848396  [   32/  306]
train() client id: f_00004-2-1 loss: 0.818176  [   64/  306]
train() client id: f_00004-2-2 loss: 1.011482  [   96/  306]
train() client id: f_00004-2-3 loss: 0.830467  [  128/  306]
train() client id: f_00004-2-4 loss: 1.023410  [  160/  306]
train() client id: f_00004-2-5 loss: 0.913525  [  192/  306]
train() client id: f_00004-2-6 loss: 1.064726  [  224/  306]
train() client id: f_00004-2-7 loss: 0.678421  [  256/  306]
train() client id: f_00004-2-8 loss: 0.978631  [  288/  306]
train() client id: f_00004-3-0 loss: 1.087259  [   32/  306]
train() client id: f_00004-3-1 loss: 0.921426  [   64/  306]
train() client id: f_00004-3-2 loss: 0.814476  [   96/  306]
train() client id: f_00004-3-3 loss: 0.847753  [  128/  306]
train() client id: f_00004-3-4 loss: 0.954905  [  160/  306]
train() client id: f_00004-3-5 loss: 0.700299  [  192/  306]
train() client id: f_00004-3-6 loss: 0.974396  [  224/  306]
train() client id: f_00004-3-7 loss: 0.847016  [  256/  306]
train() client id: f_00004-3-8 loss: 0.924545  [  288/  306]
train() client id: f_00004-4-0 loss: 0.888279  [   32/  306]
train() client id: f_00004-4-1 loss: 0.840077  [   64/  306]
train() client id: f_00004-4-2 loss: 0.947448  [   96/  306]
train() client id: f_00004-4-3 loss: 0.934113  [  128/  306]
train() client id: f_00004-4-4 loss: 0.900667  [  160/  306]
train() client id: f_00004-4-5 loss: 0.928067  [  192/  306]
train() client id: f_00004-4-6 loss: 0.932383  [  224/  306]
train() client id: f_00004-4-7 loss: 0.917343  [  256/  306]
train() client id: f_00004-4-8 loss: 0.838829  [  288/  306]
train() client id: f_00004-5-0 loss: 0.884372  [   32/  306]
train() client id: f_00004-5-1 loss: 0.802267  [   64/  306]
train() client id: f_00004-5-2 loss: 0.908983  [   96/  306]
train() client id: f_00004-5-3 loss: 0.824165  [  128/  306]
train() client id: f_00004-5-4 loss: 0.895590  [  160/  306]
train() client id: f_00004-5-5 loss: 1.045825  [  192/  306]
train() client id: f_00004-5-6 loss: 0.869635  [  224/  306]
train() client id: f_00004-5-7 loss: 0.860604  [  256/  306]
train() client id: f_00004-5-8 loss: 0.912260  [  288/  306]
train() client id: f_00004-6-0 loss: 0.962982  [   32/  306]
train() client id: f_00004-6-1 loss: 0.971614  [   64/  306]
train() client id: f_00004-6-2 loss: 0.833837  [   96/  306]
train() client id: f_00004-6-3 loss: 0.813045  [  128/  306]
train() client id: f_00004-6-4 loss: 0.863087  [  160/  306]
train() client id: f_00004-6-5 loss: 0.945406  [  192/  306]
train() client id: f_00004-6-6 loss: 0.847751  [  224/  306]
train() client id: f_00004-6-7 loss: 0.828556  [  256/  306]
train() client id: f_00004-6-8 loss: 0.917209  [  288/  306]
train() client id: f_00005-0-0 loss: 0.238820  [   32/  146]
train() client id: f_00005-0-1 loss: 0.687194  [   64/  146]
train() client id: f_00005-0-2 loss: 0.551420  [   96/  146]
train() client id: f_00005-0-3 loss: 0.902538  [  128/  146]
train() client id: f_00005-1-0 loss: 0.461473  [   32/  146]
train() client id: f_00005-1-1 loss: 0.657851  [   64/  146]
train() client id: f_00005-1-2 loss: 0.423459  [   96/  146]
train() client id: f_00005-1-3 loss: 0.394516  [  128/  146]
train() client id: f_00005-2-0 loss: 0.775660  [   32/  146]
train() client id: f_00005-2-1 loss: 0.716055  [   64/  146]
train() client id: f_00005-2-2 loss: 0.393459  [   96/  146]
train() client id: f_00005-2-3 loss: 0.355340  [  128/  146]
train() client id: f_00005-3-0 loss: 0.836335  [   32/  146]
train() client id: f_00005-3-1 loss: 0.350540  [   64/  146]
train() client id: f_00005-3-2 loss: 0.446117  [   96/  146]
train() client id: f_00005-3-3 loss: 0.769843  [  128/  146]
train() client id: f_00005-4-0 loss: 0.444532  [   32/  146]
train() client id: f_00005-4-1 loss: 0.781301  [   64/  146]
train() client id: f_00005-4-2 loss: 0.560277  [   96/  146]
train() client id: f_00005-4-3 loss: 0.478746  [  128/  146]
train() client id: f_00005-5-0 loss: 0.510093  [   32/  146]
train() client id: f_00005-5-1 loss: 0.492272  [   64/  146]
train() client id: f_00005-5-2 loss: 0.404169  [   96/  146]
train() client id: f_00005-5-3 loss: 0.672170  [  128/  146]
train() client id: f_00005-6-0 loss: 0.506135  [   32/  146]
train() client id: f_00005-6-1 loss: 0.365140  [   64/  146]
train() client id: f_00005-6-2 loss: 0.629706  [   96/  146]
train() client id: f_00005-6-3 loss: 0.724602  [  128/  146]
train() client id: f_00006-0-0 loss: 0.508452  [   32/   54]
train() client id: f_00006-1-0 loss: 0.511977  [   32/   54]
train() client id: f_00006-2-0 loss: 0.499805  [   32/   54]
train() client id: f_00006-3-0 loss: 0.573821  [   32/   54]
train() client id: f_00006-4-0 loss: 0.521315  [   32/   54]
train() client id: f_00006-5-0 loss: 0.495894  [   32/   54]
train() client id: f_00006-6-0 loss: 0.536532  [   32/   54]
train() client id: f_00007-0-0 loss: 0.558598  [   32/  179]
train() client id: f_00007-0-1 loss: 0.764150  [   64/  179]
train() client id: f_00007-0-2 loss: 0.747300  [   96/  179]
train() client id: f_00007-0-3 loss: 0.499550  [  128/  179]
train() client id: f_00007-0-4 loss: 0.719202  [  160/  179]
train() client id: f_00007-1-0 loss: 0.599784  [   32/  179]
train() client id: f_00007-1-1 loss: 0.618616  [   64/  179]
train() client id: f_00007-1-2 loss: 0.625134  [   96/  179]
train() client id: f_00007-1-3 loss: 0.816130  [  128/  179]
train() client id: f_00007-1-4 loss: 0.657885  [  160/  179]
train() client id: f_00007-2-0 loss: 0.578416  [   32/  179]
train() client id: f_00007-2-1 loss: 0.651038  [   64/  179]
train() client id: f_00007-2-2 loss: 0.642787  [   96/  179]
train() client id: f_00007-2-3 loss: 0.487622  [  128/  179]
train() client id: f_00007-2-4 loss: 0.744015  [  160/  179]
train() client id: f_00007-3-0 loss: 0.626098  [   32/  179]
train() client id: f_00007-3-1 loss: 0.661467  [   64/  179]
train() client id: f_00007-3-2 loss: 0.631852  [   96/  179]
train() client id: f_00007-3-3 loss: 0.783263  [  128/  179]
train() client id: f_00007-3-4 loss: 0.476935  [  160/  179]
train() client id: f_00007-4-0 loss: 0.499302  [   32/  179]
train() client id: f_00007-4-1 loss: 0.570267  [   64/  179]
train() client id: f_00007-4-2 loss: 0.684488  [   96/  179]
train() client id: f_00007-4-3 loss: 0.717620  [  128/  179]
train() client id: f_00007-4-4 loss: 0.613393  [  160/  179]
train() client id: f_00007-5-0 loss: 0.563282  [   32/  179]
train() client id: f_00007-5-1 loss: 0.564235  [   64/  179]
train() client id: f_00007-5-2 loss: 0.679858  [   96/  179]
train() client id: f_00007-5-3 loss: 0.598476  [  128/  179]
train() client id: f_00007-5-4 loss: 0.580571  [  160/  179]
train() client id: f_00007-6-0 loss: 0.457534  [   32/  179]
train() client id: f_00007-6-1 loss: 0.658445  [   64/  179]
train() client id: f_00007-6-2 loss: 0.673227  [   96/  179]
train() client id: f_00007-6-3 loss: 0.671348  [  128/  179]
train() client id: f_00007-6-4 loss: 0.633681  [  160/  179]
train() client id: f_00008-0-0 loss: 0.725262  [   32/  130]
train() client id: f_00008-0-1 loss: 0.798843  [   64/  130]
train() client id: f_00008-0-2 loss: 0.841981  [   96/  130]
train() client id: f_00008-0-3 loss: 0.741647  [  128/  130]
train() client id: f_00008-1-0 loss: 0.789341  [   32/  130]
train() client id: f_00008-1-1 loss: 0.630230  [   64/  130]
train() client id: f_00008-1-2 loss: 0.814067  [   96/  130]
train() client id: f_00008-1-3 loss: 0.856301  [  128/  130]
train() client id: f_00008-2-0 loss: 0.786234  [   32/  130]
train() client id: f_00008-2-1 loss: 0.803047  [   64/  130]
train() client id: f_00008-2-2 loss: 0.749770  [   96/  130]
train() client id: f_00008-2-3 loss: 0.734724  [  128/  130]
train() client id: f_00008-3-0 loss: 0.760538  [   32/  130]
train() client id: f_00008-3-1 loss: 0.822324  [   64/  130]
train() client id: f_00008-3-2 loss: 0.751046  [   96/  130]
train() client id: f_00008-3-3 loss: 0.708989  [  128/  130]
train() client id: f_00008-4-0 loss: 0.789101  [   32/  130]
train() client id: f_00008-4-1 loss: 0.714221  [   64/  130]
train() client id: f_00008-4-2 loss: 0.784776  [   96/  130]
train() client id: f_00008-4-3 loss: 0.811191  [  128/  130]
train() client id: f_00008-5-0 loss: 0.779696  [   32/  130]
train() client id: f_00008-5-1 loss: 0.773156  [   64/  130]
train() client id: f_00008-5-2 loss: 0.776449  [   96/  130]
train() client id: f_00008-5-3 loss: 0.759274  [  128/  130]
train() client id: f_00008-6-0 loss: 0.662961  [   32/  130]
train() client id: f_00008-6-1 loss: 0.814299  [   64/  130]
train() client id: f_00008-6-2 loss: 0.824387  [   96/  130]
train() client id: f_00008-6-3 loss: 0.763884  [  128/  130]
train() client id: f_00009-0-0 loss: 0.903533  [   32/  118]
train() client id: f_00009-0-1 loss: 1.008577  [   64/  118]
train() client id: f_00009-0-2 loss: 0.921564  [   96/  118]
train() client id: f_00009-1-0 loss: 0.904100  [   32/  118]
train() client id: f_00009-1-1 loss: 0.936986  [   64/  118]
train() client id: f_00009-1-2 loss: 0.731906  [   96/  118]
train() client id: f_00009-2-0 loss: 0.723984  [   32/  118]
train() client id: f_00009-2-1 loss: 0.928429  [   64/  118]
train() client id: f_00009-2-2 loss: 0.851024  [   96/  118]
train() client id: f_00009-3-0 loss: 0.841096  [   32/  118]
train() client id: f_00009-3-1 loss: 1.038353  [   64/  118]
train() client id: f_00009-3-2 loss: 0.620400  [   96/  118]
train() client id: f_00009-4-0 loss: 0.783093  [   32/  118]
train() client id: f_00009-4-1 loss: 0.811835  [   64/  118]
train() client id: f_00009-4-2 loss: 0.886033  [   96/  118]
train() client id: f_00009-5-0 loss: 0.677542  [   32/  118]
train() client id: f_00009-5-1 loss: 0.794455  [   64/  118]
train() client id: f_00009-5-2 loss: 0.839634  [   96/  118]
train() client id: f_00009-6-0 loss: 0.778952  [   32/  118]
train() client id: f_00009-6-1 loss: 0.825252  [   64/  118]
train() client id: f_00009-6-2 loss: 0.718194  [   96/  118]
At round 66 accuracy: 0.649867374005305
At round 66 training accuracy: 0.5969148222669349
At round 66 training loss: 0.801205514904329
update_location
xs = -4.528292 231.001589 240.045120 -240.943528 129.896481 -165.217951 -282.215960 303.375741 -1.680116 224.695607 
ys = 317.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -247.154970 4.001482 
xs mean: 43.442869159412865
ys mean: 9.371751218646875
dists_uav = 332.990416 252.197776 260.045003 261.843566 163.931443 193.144740 299.425472 320.007222 266.624084 245.975868 
uav_gains = -118.165859 -111.484059 -112.172086 -112.332786 -105.380952 -107.252346 -115.682456 -117.285535 -112.763755 -110.957850 
uav_gains_db_mean: -112.34776848158629
dists_bs = 222.242951 436.187725 449.918860 166.105778 351.873160 172.463489 207.903222 516.283168 456.349209 434.737934 
bs_gains = -105.278243 -113.477915 -113.854817 -101.737832 -110.865871 -102.194580 -104.467173 -115.527922 -114.027384 -113.437430 
bs_gains_db_mean: -109.48691673786065
Round 67
-------------------------------
ene_coms = [0.02215982 0.01391055 0.01251477 0.01267268 0.01135369 0.0069332
 0.01699981 0.02006759 0.01458367 0.01386313]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [2.12540656 4.29730971 2.04255594 0.75234397 4.93728966 2.3637126
 0.95053964 2.95910819 2.14912291 1.95404879]
obj_prev = 24.53143795232025
eta_min = 6.765416410859478e-44	eta_max = 0.9563136053989618
af = 5.108860222262455	bf = 0.7589527928534995	zeta = 5.6197462444887005	eta = 0.9090909090909091
af = 5.108860222262455	bf = 0.7589527928534995	zeta = 13.704670971557467	eta = 0.3727824062952938
af = 5.108860222262455	bf = 0.7589527928534995	zeta = 9.247466525192374	eta = 0.5524605261716452
af = 5.108860222262455	bf = 0.7589527928534995	zeta = 8.469451820699158	eta = 0.6032102585171464
af = 5.108860222262455	bf = 0.7589527928534995	zeta = 8.421154621821636	eta = 0.6066698038086045
af = 5.108860222262455	bf = 0.7589527928534995	zeta = 8.420942627121487	eta = 0.6066850765386114
eta = 0.6066850765386114
ene_coms = [0.02215982 0.01391055 0.01251477 0.01267268 0.01135369 0.0069332
 0.01699981 0.02006759 0.01458367 0.01386313]
ene_comp = [0.0412378  0.08673029 0.04058323 0.01407322 0.10014893 0.04778348
 0.01767334 0.05858385 0.04254693 0.03861952]
ene_total = [0.84334027 1.33876432 0.70633066 0.35578454 1.48325206 0.72786294
 0.46123605 1.04625269 0.75997398 0.69814511]
ti_comp = [1.78177903 1.86427175 1.87822953 1.87665044 1.8898404  1.93404527
 1.83337913 1.80270136 1.85754052 1.86474593]
ti_coms = [0.22159822 0.13910551 0.12514773 0.12672681 0.11353686 0.06933199
 0.16999813 0.2006759  0.14583674 0.13863133]
t_total = [26.64971886 26.64971886 26.64971886 26.64971886 26.64971886 26.64971886
 26.64971886 26.64971886 26.64971886 26.64971886]
ene_coms = [0.02215982 0.01391055 0.01251477 0.01267268 0.01135369 0.0069332
 0.01699981 0.02006759 0.01458367 0.01386313]
ene_comp = [1.38057075e-06 1.17320501e-05 1.18419289e-06 4.94644651e-08
 1.75779710e-05 1.82297209e-06 1.02643496e-07 3.86692940e-06
 1.39510914e-06 1.03528855e-06]
ene_total = [0.29479711 0.18519973 0.16649222 0.16857769 0.15126506 0.09225241
 0.22613962 0.26699848 0.19401637 0.18442667]
optimize_network iter = 0 obj = 1.930165368578351
eta = 0.6066850765386114
freqs = [11572084.78300679 23261170.35448799 10803585.83001997  3749557.13400735
 26496664.29999216 12353247.04965974  4819881.7423171  16248906.93503074
 11452491.06201426 10355170.31003229]
eta_min = 0.6066850765386125	eta_max = 0.8023713081132758
af = 0.0004203167033121952	bf = 0.7589527928534995	zeta = 0.0004623483736434148	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02215982 0.01391055 0.01251477 0.01267268 0.01135369 0.0069332
 0.01699981 0.02006759 0.01458367 0.01386313]
ene_comp = [2.63298943e-07 2.23750675e-06 2.25846256e-07 9.43373698e-09
 3.35242593e-06 3.47672601e-07 1.95759065e-08 7.37490942e-07
 2.66071668e-07 1.97447599e-07]
ene_total = [1.27536484 0.80071357 0.72026748 0.72934304 0.65362415 0.39904185
 0.97838018 1.15497926 0.83933988 0.79786714]
ti_comp = [0.78503743 0.86753015 0.88148793 0.87990884 0.8930988  0.93730367
 0.83663753 0.80595976 0.86079892 0.86800433]
ti_coms = [0.22159822 0.13910551 0.12514773 0.12672681 0.11353686 0.06933199
 0.16999813 0.2006759  0.14583674 0.13863133]
t_total = [26.64971886 26.64971886 26.64971886 26.64971886 26.64971886 26.64971886
 26.64971886 26.64971886 26.64971886 26.64971886]
ene_coms = [0.02215982 0.01391055 0.01251477 0.01267268 0.01135369 0.0069332
 0.01699981 0.02006759 0.01458367 0.01386313]
ene_comp = [6.08268310e-07 4.63375169e-06 4.59829259e-07 1.92439530e-08
 6.73177443e-06 6.63838172e-07 4.21571151e-08 1.65461389e-06
 5.55636689e-07 4.08664139e-07]
ene_total = [0.58667627 0.36839124 0.33132884 0.33549766 0.30075622 0.18356741
 0.45005495 0.53131411 0.38610356 0.36702404]
optimize_network iter = 1 obj = 3.840714298266194
eta = 0.8023713081132758
freqs = [11572084.7830068  22023823.90161308 10142306.7749685   3523402.03119942
 24703207.60722228 11230626.06041847  4653587.09010071 16012940.92658786
 10888625.60673473  9801477.44887773]
eta_min = 0.8023713081132768	eta_max = 0.8023713081132305
af = 0.00037491324646211796	bf = 0.7589527928534995	zeta = 0.0004124045711083298	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02215982 0.01391055 0.01251477 0.01267268 0.01135369 0.0069332
 0.01699981 0.02006759 0.01458367 0.01386313]
ene_comp = [2.63298943e-07 2.00579564e-06 1.99044658e-07 8.33006161e-09
 2.91395929e-06 2.87353272e-07 1.82484007e-08 7.16226839e-07
 2.40516480e-07 1.76896995e-07]
ene_total = [1.27536484 0.80070023 0.72026594 0.72934297 0.65359891 0.39903838
 0.9783801  1.15497803 0.83933841 0.79786596]
ti_comp = [0.78503743 0.86753015 0.88148793 0.87990884 0.8930988  0.93730367
 0.83663753 0.80595976 0.86079892 0.86800433]
ti_coms = [0.22159822 0.13910551 0.12514773 0.12672681 0.11353686 0.06933199
 0.16999813 0.2006759  0.14583674 0.13863133]
t_total = [26.64971886 26.64971886 26.64971886 26.64971886 26.64971886 26.64971886
 26.64971886 26.64971886 26.64971886 26.64971886]
ene_coms = [0.02215982 0.01391055 0.01251477 0.01267268 0.01135369 0.0069332
 0.01699981 0.02006759 0.01458367 0.01386313]
ene_comp = [6.08268310e-07 4.63375169e-06 4.59829259e-07 1.92439530e-08
 6.73177443e-06 6.63838172e-07 4.21571151e-08 1.65461389e-06
 5.55636689e-07 4.08664139e-07]
ene_total = [0.58667627 0.36839124 0.33132884 0.33549766 0.30075622 0.18356741
 0.45005495 0.53131411 0.38610356 0.36702404]
optimize_network iter = 2 obj = 3.840714298265314
eta = 0.8023713081132305
freqs = [11572084.78300637 22023823.90161287 10142306.77496845  3523402.0311994
 24703207.60722223 11230626.06041859  4653587.09010062 16012940.92658738
 10888625.6067346   9801477.44887764]
Done!
ene_coms = [0.02215982 0.01391055 0.01251477 0.01267268 0.01135369 0.0069332
 0.01699981 0.02006759 0.01458367 0.01386313]
ene_comp = [5.90556975e-07 4.49882780e-06 4.46440118e-07 1.86836146e-08
 6.53576107e-06 6.44508774e-07 4.09295996e-08 1.60643545e-06
 5.39457863e-07 3.96764806e-07]
ene_total = [0.02216041 0.01391505 0.01251522 0.0126727  0.01136022 0.00693384
 0.01699985 0.0200692  0.01458421 0.01386353]
At round 67 energy consumption: 0.14507424041700445
At round 67 eta: 0.8023713081132305
At round 67 a_n: 5.2320311093101175
At round 67 local rounds: 7.209936298176669
At round 67 global rounds: 26.474046148662403
gradient difference: 0.5721770524978638
train() client id: f_00000-0-0 loss: 1.045171  [   32/  126]
train() client id: f_00000-0-1 loss: 1.000086  [   64/  126]
train() client id: f_00000-0-2 loss: 1.273937  [   96/  126]
train() client id: f_00000-1-0 loss: 1.115020  [   32/  126]
train() client id: f_00000-1-1 loss: 0.976480  [   64/  126]
train() client id: f_00000-1-2 loss: 1.055485  [   96/  126]
train() client id: f_00000-2-0 loss: 0.965594  [   32/  126]
train() client id: f_00000-2-1 loss: 0.857320  [   64/  126]
train() client id: f_00000-2-2 loss: 1.226533  [   96/  126]
train() client id: f_00000-3-0 loss: 1.099059  [   32/  126]
train() client id: f_00000-3-1 loss: 0.830425  [   64/  126]
train() client id: f_00000-3-2 loss: 0.955199  [   96/  126]
train() client id: f_00000-4-0 loss: 0.934678  [   32/  126]
train() client id: f_00000-4-1 loss: 0.865764  [   64/  126]
train() client id: f_00000-4-2 loss: 0.961470  [   96/  126]
train() client id: f_00000-5-0 loss: 0.835175  [   32/  126]
train() client id: f_00000-5-1 loss: 1.002192  [   64/  126]
train() client id: f_00000-5-2 loss: 1.130333  [   96/  126]
train() client id: f_00000-6-0 loss: 0.933009  [   32/  126]
train() client id: f_00000-6-1 loss: 0.853902  [   64/  126]
train() client id: f_00000-6-2 loss: 0.972329  [   96/  126]
train() client id: f_00001-0-0 loss: 0.501368  [   32/  265]
train() client id: f_00001-0-1 loss: 0.521120  [   64/  265]
train() client id: f_00001-0-2 loss: 0.452725  [   96/  265]
train() client id: f_00001-0-3 loss: 0.513656  [  128/  265]
train() client id: f_00001-0-4 loss: 0.516453  [  160/  265]
train() client id: f_00001-0-5 loss: 0.516722  [  192/  265]
train() client id: f_00001-0-6 loss: 0.532896  [  224/  265]
train() client id: f_00001-0-7 loss: 0.425288  [  256/  265]
train() client id: f_00001-1-0 loss: 0.590344  [   32/  265]
train() client id: f_00001-1-1 loss: 0.567271  [   64/  265]
train() client id: f_00001-1-2 loss: 0.407528  [   96/  265]
train() client id: f_00001-1-3 loss: 0.418727  [  128/  265]
train() client id: f_00001-1-4 loss: 0.475780  [  160/  265]
train() client id: f_00001-1-5 loss: 0.559345  [  192/  265]
train() client id: f_00001-1-6 loss: 0.415913  [  224/  265]
train() client id: f_00001-1-7 loss: 0.475378  [  256/  265]
train() client id: f_00001-2-0 loss: 0.555118  [   32/  265]
train() client id: f_00001-2-1 loss: 0.432713  [   64/  265]
train() client id: f_00001-2-2 loss: 0.387003  [   96/  265]
train() client id: f_00001-2-3 loss: 0.616858  [  128/  265]
train() client id: f_00001-2-4 loss: 0.480469  [  160/  265]
train() client id: f_00001-2-5 loss: 0.514692  [  192/  265]
train() client id: f_00001-2-6 loss: 0.514715  [  224/  265]
train() client id: f_00001-2-7 loss: 0.399879  [  256/  265]
train() client id: f_00001-3-0 loss: 0.478180  [   32/  265]
train() client id: f_00001-3-1 loss: 0.520501  [   64/  265]
train() client id: f_00001-3-2 loss: 0.402216  [   96/  265]
train() client id: f_00001-3-3 loss: 0.389095  [  128/  265]
train() client id: f_00001-3-4 loss: 0.518156  [  160/  265]
train() client id: f_00001-3-5 loss: 0.472419  [  192/  265]
train() client id: f_00001-3-6 loss: 0.535852  [  224/  265]
train() client id: f_00001-3-7 loss: 0.571392  [  256/  265]
train() client id: f_00001-4-0 loss: 0.468927  [   32/  265]
train() client id: f_00001-4-1 loss: 0.407062  [   64/  265]
train() client id: f_00001-4-2 loss: 0.450271  [   96/  265]
train() client id: f_00001-4-3 loss: 0.538358  [  128/  265]
train() client id: f_00001-4-4 loss: 0.366527  [  160/  265]
train() client id: f_00001-4-5 loss: 0.607120  [  192/  265]
train() client id: f_00001-4-6 loss: 0.512465  [  224/  265]
train() client id: f_00001-4-7 loss: 0.515300  [  256/  265]
train() client id: f_00001-5-0 loss: 0.462619  [   32/  265]
train() client id: f_00001-5-1 loss: 0.516777  [   64/  265]
train() client id: f_00001-5-2 loss: 0.589567  [   96/  265]
train() client id: f_00001-5-3 loss: 0.574933  [  128/  265]
train() client id: f_00001-5-4 loss: 0.399444  [  160/  265]
train() client id: f_00001-5-5 loss: 0.405436  [  192/  265]
train() client id: f_00001-5-6 loss: 0.510260  [  224/  265]
train() client id: f_00001-5-7 loss: 0.405446  [  256/  265]
train() client id: f_00001-6-0 loss: 0.564726  [   32/  265]
train() client id: f_00001-6-1 loss: 0.476473  [   64/  265]
train() client id: f_00001-6-2 loss: 0.437174  [   96/  265]
train() client id: f_00001-6-3 loss: 0.422563  [  128/  265]
train() client id: f_00001-6-4 loss: 0.479358  [  160/  265]
train() client id: f_00001-6-5 loss: 0.535002  [  192/  265]
train() client id: f_00001-6-6 loss: 0.498973  [  224/  265]
train() client id: f_00001-6-7 loss: 0.435499  [  256/  265]
train() client id: f_00002-0-0 loss: 1.005396  [   32/  124]
train() client id: f_00002-0-1 loss: 1.295640  [   64/  124]
train() client id: f_00002-0-2 loss: 1.050372  [   96/  124]
train() client id: f_00002-1-0 loss: 1.165293  [   32/  124]
train() client id: f_00002-1-1 loss: 1.180697  [   64/  124]
train() client id: f_00002-1-2 loss: 1.163300  [   96/  124]
train() client id: f_00002-2-0 loss: 1.003310  [   32/  124]
train() client id: f_00002-2-1 loss: 1.004553  [   64/  124]
train() client id: f_00002-2-2 loss: 0.956574  [   96/  124]
train() client id: f_00002-3-0 loss: 0.994972  [   32/  124]
train() client id: f_00002-3-1 loss: 1.277237  [   64/  124]
train() client id: f_00002-3-2 loss: 0.970639  [   96/  124]
train() client id: f_00002-4-0 loss: 0.957374  [   32/  124]
train() client id: f_00002-4-1 loss: 1.112510  [   64/  124]
train() client id: f_00002-4-2 loss: 0.921115  [   96/  124]
train() client id: f_00002-5-0 loss: 1.036812  [   32/  124]
train() client id: f_00002-5-1 loss: 0.938082  [   64/  124]
train() client id: f_00002-5-2 loss: 1.046099  [   96/  124]
train() client id: f_00002-6-0 loss: 1.056837  [   32/  124]
train() client id: f_00002-6-1 loss: 0.886565  [   64/  124]
train() client id: f_00002-6-2 loss: 1.059234  [   96/  124]
train() client id: f_00003-0-0 loss: 0.536336  [   32/   43]
train() client id: f_00003-1-0 loss: 0.573932  [   32/   43]
train() client id: f_00003-2-0 loss: 0.588550  [   32/   43]
train() client id: f_00003-3-0 loss: 0.517526  [   32/   43]
train() client id: f_00003-4-0 loss: 0.506380  [   32/   43]
train() client id: f_00003-5-0 loss: 0.468688  [   32/   43]
train() client id: f_00003-6-0 loss: 0.406222  [   32/   43]
train() client id: f_00004-0-0 loss: 0.739501  [   32/  306]
train() client id: f_00004-0-1 loss: 0.770832  [   64/  306]
train() client id: f_00004-0-2 loss: 0.808360  [   96/  306]
train() client id: f_00004-0-3 loss: 0.855079  [  128/  306]
train() client id: f_00004-0-4 loss: 0.883694  [  160/  306]
train() client id: f_00004-0-5 loss: 1.045188  [  192/  306]
train() client id: f_00004-0-6 loss: 0.758230  [  224/  306]
train() client id: f_00004-0-7 loss: 0.699098  [  256/  306]
train() client id: f_00004-0-8 loss: 0.828915  [  288/  306]
train() client id: f_00004-1-0 loss: 0.922852  [   32/  306]
train() client id: f_00004-1-1 loss: 0.662641  [   64/  306]
train() client id: f_00004-1-2 loss: 0.813985  [   96/  306]
train() client id: f_00004-1-3 loss: 0.990958  [  128/  306]
train() client id: f_00004-1-4 loss: 0.893020  [  160/  306]
train() client id: f_00004-1-5 loss: 0.757414  [  192/  306]
train() client id: f_00004-1-6 loss: 0.896743  [  224/  306]
train() client id: f_00004-1-7 loss: 0.647280  [  256/  306]
train() client id: f_00004-1-8 loss: 0.789765  [  288/  306]
train() client id: f_00004-2-0 loss: 0.759824  [   32/  306]
train() client id: f_00004-2-1 loss: 0.807286  [   64/  306]
train() client id: f_00004-2-2 loss: 0.919092  [   96/  306]
train() client id: f_00004-2-3 loss: 0.770729  [  128/  306]
train() client id: f_00004-2-4 loss: 0.795473  [  160/  306]
train() client id: f_00004-2-5 loss: 0.804968  [  192/  306]
train() client id: f_00004-2-6 loss: 0.816796  [  224/  306]
train() client id: f_00004-2-7 loss: 0.923922  [  256/  306]
train() client id: f_00004-2-8 loss: 0.848574  [  288/  306]
train() client id: f_00004-3-0 loss: 0.831464  [   32/  306]
train() client id: f_00004-3-1 loss: 0.899452  [   64/  306]
train() client id: f_00004-3-2 loss: 0.824365  [   96/  306]
train() client id: f_00004-3-3 loss: 0.896761  [  128/  306]
train() client id: f_00004-3-4 loss: 0.980999  [  160/  306]
train() client id: f_00004-3-5 loss: 0.824708  [  192/  306]
train() client id: f_00004-3-6 loss: 0.722540  [  224/  306]
train() client id: f_00004-3-7 loss: 0.714894  [  256/  306]
train() client id: f_00004-3-8 loss: 0.746735  [  288/  306]
train() client id: f_00004-4-0 loss: 0.772374  [   32/  306]
train() client id: f_00004-4-1 loss: 0.853094  [   64/  306]
train() client id: f_00004-4-2 loss: 0.845111  [   96/  306]
train() client id: f_00004-4-3 loss: 0.763886  [  128/  306]
train() client id: f_00004-4-4 loss: 0.939363  [  160/  306]
train() client id: f_00004-4-5 loss: 0.846776  [  192/  306]
train() client id: f_00004-4-6 loss: 0.813211  [  224/  306]
train() client id: f_00004-4-7 loss: 0.588249  [  256/  306]
train() client id: f_00004-4-8 loss: 0.981268  [  288/  306]
train() client id: f_00004-5-0 loss: 0.781903  [   32/  306]
train() client id: f_00004-5-1 loss: 0.862240  [   64/  306]
train() client id: f_00004-5-2 loss: 0.898795  [   96/  306]
train() client id: f_00004-5-3 loss: 0.821488  [  128/  306]
train() client id: f_00004-5-4 loss: 0.849332  [  160/  306]
train() client id: f_00004-5-5 loss: 0.715497  [  192/  306]
train() client id: f_00004-5-6 loss: 0.797701  [  224/  306]
train() client id: f_00004-5-7 loss: 0.787283  [  256/  306]
train() client id: f_00004-5-8 loss: 0.892570  [  288/  306]
train() client id: f_00004-6-0 loss: 0.829961  [   32/  306]
train() client id: f_00004-6-1 loss: 0.810939  [   64/  306]
train() client id: f_00004-6-2 loss: 0.919694  [   96/  306]
train() client id: f_00004-6-3 loss: 0.867393  [  128/  306]
train() client id: f_00004-6-4 loss: 0.714387  [  160/  306]
train() client id: f_00004-6-5 loss: 0.927275  [  192/  306]
train() client id: f_00004-6-6 loss: 0.930024  [  224/  306]
train() client id: f_00004-6-7 loss: 0.704496  [  256/  306]
train() client id: f_00004-6-8 loss: 0.761536  [  288/  306]
train() client id: f_00005-0-0 loss: 0.405923  [   32/  146]
train() client id: f_00005-0-1 loss: 0.465038  [   64/  146]
train() client id: f_00005-0-2 loss: 0.668557  [   96/  146]
train() client id: f_00005-0-3 loss: 0.342205  [  128/  146]
train() client id: f_00005-1-0 loss: 0.245832  [   32/  146]
train() client id: f_00005-1-1 loss: 0.654570  [   64/  146]
train() client id: f_00005-1-2 loss: 0.586581  [   96/  146]
train() client id: f_00005-1-3 loss: 0.371817  [  128/  146]
train() client id: f_00005-2-0 loss: 0.607920  [   32/  146]
train() client id: f_00005-2-1 loss: 0.687067  [   64/  146]
train() client id: f_00005-2-2 loss: 0.360619  [   96/  146]
train() client id: f_00005-2-3 loss: 0.395891  [  128/  146]
train() client id: f_00005-3-0 loss: 0.539151  [   32/  146]
train() client id: f_00005-3-1 loss: 0.309141  [   64/  146]
train() client id: f_00005-3-2 loss: 0.223591  [   96/  146]
train() client id: f_00005-3-3 loss: 0.783557  [  128/  146]
train() client id: f_00005-4-0 loss: 0.468963  [   32/  146]
train() client id: f_00005-4-1 loss: 0.324058  [   64/  146]
train() client id: f_00005-4-2 loss: 0.860691  [   96/  146]
train() client id: f_00005-4-3 loss: 0.134642  [  128/  146]
train() client id: f_00005-5-0 loss: 0.437446  [   32/  146]
train() client id: f_00005-5-1 loss: 0.387157  [   64/  146]
train() client id: f_00005-5-2 loss: 0.565496  [   96/  146]
train() client id: f_00005-5-3 loss: 0.628841  [  128/  146]
train() client id: f_00005-6-0 loss: 0.543882  [   32/  146]
train() client id: f_00005-6-1 loss: 0.360610  [   64/  146]
train() client id: f_00005-6-2 loss: 0.472606  [   96/  146]
train() client id: f_00005-6-3 loss: 0.544408  [  128/  146]
train() client id: f_00006-0-0 loss: 0.465691  [   32/   54]
train() client id: f_00006-1-0 loss: 0.523268  [   32/   54]
train() client id: f_00006-2-0 loss: 0.451104  [   32/   54]
train() client id: f_00006-3-0 loss: 0.517853  [   32/   54]
train() client id: f_00006-4-0 loss: 0.479618  [   32/   54]
train() client id: f_00006-5-0 loss: 0.402609  [   32/   54]
train() client id: f_00006-6-0 loss: 0.469326  [   32/   54]
train() client id: f_00007-0-0 loss: 0.720190  [   32/  179]
train() client id: f_00007-0-1 loss: 0.738294  [   64/  179]
train() client id: f_00007-0-2 loss: 0.542276  [   96/  179]
train() client id: f_00007-0-3 loss: 0.775713  [  128/  179]
train() client id: f_00007-0-4 loss: 0.472582  [  160/  179]
train() client id: f_00007-1-0 loss: 0.793413  [   32/  179]
train() client id: f_00007-1-1 loss: 0.559872  [   64/  179]
train() client id: f_00007-1-2 loss: 0.673265  [   96/  179]
train() client id: f_00007-1-3 loss: 0.571380  [  128/  179]
train() client id: f_00007-1-4 loss: 0.486806  [  160/  179]
train() client id: f_00007-2-0 loss: 0.692246  [   32/  179]
train() client id: f_00007-2-1 loss: 0.768764  [   64/  179]
train() client id: f_00007-2-2 loss: 0.575576  [   96/  179]
train() client id: f_00007-2-3 loss: 0.617325  [  128/  179]
train() client id: f_00007-2-4 loss: 0.572160  [  160/  179]
train() client id: f_00007-3-0 loss: 0.568326  [   32/  179]
train() client id: f_00007-3-1 loss: 0.613051  [   64/  179]
train() client id: f_00007-3-2 loss: 0.572677  [   96/  179]
train() client id: f_00007-3-3 loss: 0.696063  [  128/  179]
train() client id: f_00007-3-4 loss: 0.639643  [  160/  179]
train() client id: f_00007-4-0 loss: 0.580995  [   32/  179]
train() client id: f_00007-4-1 loss: 0.467494  [   64/  179]
train() client id: f_00007-4-2 loss: 0.694249  [   96/  179]
train() client id: f_00007-4-3 loss: 0.734056  [  128/  179]
train() client id: f_00007-4-4 loss: 0.622327  [  160/  179]
train() client id: f_00007-5-0 loss: 0.523512  [   32/  179]
train() client id: f_00007-5-1 loss: 0.707263  [   64/  179]
train() client id: f_00007-5-2 loss: 0.672939  [   96/  179]
train() client id: f_00007-5-3 loss: 0.585840  [  128/  179]
train() client id: f_00007-5-4 loss: 0.723959  [  160/  179]
train() client id: f_00007-6-0 loss: 0.593992  [   32/  179]
train() client id: f_00007-6-1 loss: 0.625984  [   64/  179]
train() client id: f_00007-6-2 loss: 0.911889  [   96/  179]
train() client id: f_00007-6-3 loss: 0.522305  [  128/  179]
train() client id: f_00007-6-4 loss: 0.470224  [  160/  179]
train() client id: f_00008-0-0 loss: 0.732506  [   32/  130]
train() client id: f_00008-0-1 loss: 0.768990  [   64/  130]
train() client id: f_00008-0-2 loss: 0.787448  [   96/  130]
train() client id: f_00008-0-3 loss: 0.696537  [  128/  130]
train() client id: f_00008-1-0 loss: 0.636009  [   32/  130]
train() client id: f_00008-1-1 loss: 0.668790  [   64/  130]
train() client id: f_00008-1-2 loss: 0.928815  [   96/  130]
train() client id: f_00008-1-3 loss: 0.743264  [  128/  130]
train() client id: f_00008-2-0 loss: 0.719905  [   32/  130]
train() client id: f_00008-2-1 loss: 0.762496  [   64/  130]
train() client id: f_00008-2-2 loss: 0.726227  [   96/  130]
train() client id: f_00008-2-3 loss: 0.745513  [  128/  130]
train() client id: f_00008-3-0 loss: 0.649939  [   32/  130]
train() client id: f_00008-3-1 loss: 0.744548  [   64/  130]
train() client id: f_00008-3-2 loss: 0.642116  [   96/  130]
train() client id: f_00008-3-3 loss: 0.961675  [  128/  130]
train() client id: f_00008-4-0 loss: 0.794835  [   32/  130]
train() client id: f_00008-4-1 loss: 0.624517  [   64/  130]
train() client id: f_00008-4-2 loss: 0.863400  [   96/  130]
train() client id: f_00008-4-3 loss: 0.699987  [  128/  130]
train() client id: f_00008-5-0 loss: 0.718257  [   32/  130]
train() client id: f_00008-5-1 loss: 0.840919  [   64/  130]
train() client id: f_00008-5-2 loss: 0.780085  [   96/  130]
train() client id: f_00008-5-3 loss: 0.616956  [  128/  130]
train() client id: f_00008-6-0 loss: 0.749843  [   32/  130]
train() client id: f_00008-6-1 loss: 0.706852  [   64/  130]
train() client id: f_00008-6-2 loss: 0.763551  [   96/  130]
train() client id: f_00008-6-3 loss: 0.783584  [  128/  130]
train() client id: f_00009-0-0 loss: 1.109176  [   32/  118]
train() client id: f_00009-0-1 loss: 0.848434  [   64/  118]
train() client id: f_00009-0-2 loss: 1.056676  [   96/  118]
train() client id: f_00009-1-0 loss: 0.982346  [   32/  118]
train() client id: f_00009-1-1 loss: 1.045892  [   64/  118]
train() client id: f_00009-1-2 loss: 0.922966  [   96/  118]
train() client id: f_00009-2-0 loss: 0.933126  [   32/  118]
train() client id: f_00009-2-1 loss: 0.784872  [   64/  118]
train() client id: f_00009-2-2 loss: 1.070516  [   96/  118]
train() client id: f_00009-3-0 loss: 0.817531  [   32/  118]
train() client id: f_00009-3-1 loss: 1.011835  [   64/  118]
train() client id: f_00009-3-2 loss: 0.816685  [   96/  118]
train() client id: f_00009-4-0 loss: 0.899241  [   32/  118]
train() client id: f_00009-4-1 loss: 0.841184  [   64/  118]
train() client id: f_00009-4-2 loss: 1.007599  [   96/  118]
train() client id: f_00009-5-0 loss: 0.825031  [   32/  118]
train() client id: f_00009-5-1 loss: 0.919857  [   64/  118]
train() client id: f_00009-5-2 loss: 0.901988  [   96/  118]
train() client id: f_00009-6-0 loss: 0.860232  [   32/  118]
train() client id: f_00009-6-1 loss: 0.825870  [   64/  118]
train() client id: f_00009-6-2 loss: 0.756588  [   96/  118]
At round 67 accuracy: 0.6472148541114059
At round 67 training accuracy: 0.590878604963112
At round 67 training loss: 0.8179178336734756
update_location
xs = -4.528292 236.001589 245.045120 -245.943528 134.896481 -170.217951 -287.215960 308.375741 -1.680116 229.695607 
ys = 322.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -252.154970 4.001482 
xs mean: 44.442869159412865
ys mean: 9.371751218646875
dists_uav = 337.762486 256.785386 264.667442 266.451662 167.921061 197.438776 304.142685 324.751258 271.265463 250.551559 
uav_gains = -118.464295 -111.883447 -112.586786 -112.748134 -105.647132 -107.521028 -116.071285 -117.619042 -113.185591 -111.342985 
uav_gains_db_mean: -112.70697258956002
dists_bs = 225.483501 440.845491 454.535402 168.153397 356.214382 172.252241 210.525317 520.919444 460.978471 439.339308 
bs_gains = -105.454273 -113.607078 -113.978955 -101.886817 -111.014980 -102.179676 -104.619580 -115.636635 -114.150117 -113.565461 
bs_gains_db_mean: -109.6093572540482
Round 68
-------------------------------
ene_coms = [0.02294912 0.01406377 0.01292975 0.01309799 0.01147607 0.0069285
 0.0176685  0.02082128 0.01474192 0.01401408]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.9901525  4.01671768 1.91087747 0.70518786 4.61464523 2.20893495
 0.89160952 2.76909507 2.00919941 1.82686094]
obj_prev = 22.943280639719298
eta_min = 6.858620084775322e-47	eta_max = 0.9576514809001544
af = 4.774378485567752	bf = 0.7270223749625203	zeta = 5.251816334124527	eta = 0.9090909090909091
af = 4.774378485567752	bf = 0.7270223749625203	zeta = 13.002761715634712	eta = 0.36718187951002507
af = 4.774378485567752	bf = 0.7270223749625203	zeta = 8.707813991712355	eta = 0.5482866871193801
af = 4.774378485567752	bf = 0.7270223749625203	zeta = 7.9612850338593235	eta = 0.5996994788231212
af = 4.774378485567752	bf = 0.7270223749625203	zeta = 7.914776551146734	eta = 0.603223408104429
af = 4.774378485567752	bf = 0.7270223749625203	zeta = 7.914570424414939	eta = 0.6032391184289302
eta = 0.6032391184289302
ene_coms = [0.02294912 0.01406377 0.01292975 0.01309799 0.01147607 0.0069285
 0.0176685  0.02082128 0.01474192 0.01401408]
ene_comp = [0.04170783 0.08771885 0.0410458  0.01423362 0.10129045 0.04832812
 0.01787478 0.0592516  0.04303189 0.03905971]
ene_total = [0.79680035 1.25431882 0.66516808 0.33682132 1.38967885 0.68095528
 0.43801794 0.9867787  0.71197586 0.65405523]
ti_comp = [1.92896223 2.01781568 2.02915591 2.02747354 2.04369275 2.08916847
 1.98176839 1.9502406  2.01103426 2.01831264]
ti_coms = [0.22949121 0.14063775 0.12929752 0.13097989 0.11476068 0.06928496
 0.17668504 0.20821284 0.14741917 0.14014079]
t_total = [26.59971466 26.59971466 26.59971466 26.59971466 26.59971466 26.59971466
 26.59971466 26.59971466 26.59971466 26.59971466]
ene_coms = [0.02294912 0.01406377 0.01292975 0.01309799 0.01147607 0.0069285
 0.0176685  0.02082128 0.01474192 0.01401408]
ene_comp = [1.21866789e-06 1.03608602e-05 1.04967632e-06 4.38446045e-08
 1.55508542e-05 1.61634038e-06 9.08859316e-08 3.41825051e-06
 1.23143728e-06 9.14303369e-07]
ene_total = [0.28282865 0.1734427  0.15935282 0.16141369 0.14161704 0.08540329
 0.21773904 0.25663335 0.18168728 0.17271386]
optimize_network iter = 0 obj = 1.8328317170606074
eta = 0.6032391184289302
freqs = [10810950.53809552 21736090.78352923 10114008.77265149  3510187.53079383
 24781231.27789104 11566353.24154617  4509806.62496719 15190843.76201483
 10698944.65327236  9676328.97763161]
eta_min = 0.603239118428931	eta_max = 0.809932380097119
af = 0.00034336877977534893	bf = 0.7270223749625203	zeta = 0.00037770565775288383	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02294912 0.01406377 0.01292975 0.01309799 0.01147607 0.0069285
 0.0176685  0.02082128 0.01474192 0.01401408]
ene_comp = [2.29801925e-07 1.95372804e-06 1.97935501e-07 8.26769508e-09
 2.93239551e-06 3.04790282e-07 1.71381902e-08 6.44573107e-07
 2.32209825e-07 1.72408477e-07]
ene_total = [1.23431561 0.7565159  0.6954288  0.70446712 0.61739045 0.37266098
 0.95028969 1.11989362 0.79289673 0.75374723]
ti_comp = [0.80451221 0.89336567 0.90470589 0.90302352 0.91924273 0.96471845
 0.85731838 0.82579058 0.88658425 0.89386263]
ti_coms = [0.22949121 0.14063775 0.12929752 0.13097989 0.11476068 0.06928496
 0.17668504 0.20821284 0.14741917 0.14014079]
t_total = [26.59971466 26.59971466 26.59971466 26.59971466 26.59971466 26.59971466
 26.59971466 26.59971466 26.59971466 26.59971466]
ene_coms = [0.02294912 0.01406377 0.01292975 0.01309799 0.01147607 0.0069285
 0.0176685  0.02082128 0.01474192 0.01401408]
ene_comp = [5.08269775e-07 3.83466152e-06 3.83087643e-07 1.60344907e-08
 5.57637277e-06 5.49927357e-07 3.52326201e-08 1.38314329e-06
 4.59661326e-07 3.38183483e-07]
ene_total = [0.59037869 0.36188894 0.33262743 0.33694589 0.29536505 0.17824956
 0.45452277 0.53566263 0.37924731 0.36052056]
optimize_network iter = 1 obj = 3.8254088316942947
eta = 0.809932380097119
freqs = [10810950.53809553 20475880.72875209  9461069.65357097  3286967.17542197
 22978263.02317235 10446678.88220632  4347880.51731685 14962669.44779142
 10121603.42394428  9112492.90362489]
eta_min = 0.8099323800971173	eta_max = 0.8099323800971064
af = 0.0003034889111473942	bf = 0.7270223749625203	zeta = 0.00033383780226213365	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02294912 0.01406377 0.01292975 0.01309799 0.01147607 0.0069285
 0.0176685  0.02082128 0.01474192 0.01401408]
ene_comp = [2.29801925e-07 1.73374976e-06 1.73203842e-07 7.24960838e-09
 2.52122251e-06 2.48636397e-07 1.59295798e-08 6.25354892e-07
 2.07824787e-07 1.52901509e-07]
ene_total = [1.23431561 0.75650407 0.69542746 0.70446707 0.61736833 0.37265796
 0.95028963 1.11989258 0.79289542 0.75374618]
ti_comp = [0.80451221 0.89336567 0.90470589 0.90302352 0.91924273 0.96471845
 0.85731838 0.82579058 0.88658425 0.89386263]
ti_coms = [0.22949121 0.14063775 0.12929752 0.13097989 0.11476068 0.06928496
 0.17668504 0.20821284 0.14741917 0.14014079]
t_total = [26.59971466 26.59971466 26.59971466 26.59971466 26.59971466 26.59971466
 26.59971466 26.59971466 26.59971466 26.59971466]
ene_coms = [0.02294912 0.01406377 0.01292975 0.01309799 0.01147607 0.0069285
 0.0176685  0.02082128 0.01474192 0.01401408]
ene_comp = [5.08269775e-07 3.83466152e-06 3.83087643e-07 1.60344907e-08
 5.57637277e-06 5.49927357e-07 3.52326201e-08 1.38314329e-06
 4.59661326e-07 3.38183483e-07]
ene_total = [0.59037869 0.36188894 0.33262743 0.33694589 0.29536505 0.17824956
 0.45452277 0.53566263 0.37924731 0.36052056]
optimize_network iter = 2 obj = 3.8254088316940424
eta = 0.8099323800971064
freqs = [10810950.5380954  20475880.72875202  9461069.65357095  3286967.17542196
 22978263.02317233 10446678.88220635  4347880.51731682 14962669.44779128
 10121603.42394425  9112492.90362486]
Done!
ene_coms = [0.02294912 0.01406377 0.01292975 0.01309799 0.01147607 0.0069285
 0.0176685  0.02082128 0.01474192 0.01401408]
ene_comp = [4.41793743e-07 3.33313045e-06 3.32984041e-07 1.39373576e-08
 4.84704525e-06 4.78002977e-07 3.06245853e-08 1.20224353e-06
 3.99542738e-07 2.93952845e-07]
ene_total = [0.02294956 0.01406711 0.01293008 0.013098   0.01148092 0.00692897
 0.01766853 0.02082249 0.01474232 0.01401437]
At round 68 energy consumption: 0.14870235726806025
At round 68 eta: 0.8099323800971064
At round 68 a_n: 4.8894852623407985
At round 68 local rounds: 6.902810874525733
At round 68 global rounds: 25.7249775887069
gradient difference: 0.7619032263755798
train() client id: f_00000-0-0 loss: 0.845529  [   32/  126]
train() client id: f_00000-0-1 loss: 0.997611  [   64/  126]
train() client id: f_00000-0-2 loss: 0.689781  [   96/  126]
train() client id: f_00000-1-0 loss: 1.015095  [   32/  126]
train() client id: f_00000-1-1 loss: 0.730544  [   64/  126]
train() client id: f_00000-1-2 loss: 0.818387  [   96/  126]
train() client id: f_00000-2-0 loss: 0.860402  [   32/  126]
train() client id: f_00000-2-1 loss: 0.792028  [   64/  126]
train() client id: f_00000-2-2 loss: 0.717241  [   96/  126]
train() client id: f_00000-3-0 loss: 0.701596  [   32/  126]
train() client id: f_00000-3-1 loss: 0.806947  [   64/  126]
train() client id: f_00000-3-2 loss: 0.787789  [   96/  126]
train() client id: f_00000-4-0 loss: 0.808981  [   32/  126]
train() client id: f_00000-4-1 loss: 0.699609  [   64/  126]
train() client id: f_00000-4-2 loss: 0.859399  [   96/  126]
train() client id: f_00000-5-0 loss: 0.826060  [   32/  126]
train() client id: f_00000-5-1 loss: 0.862820  [   64/  126]
train() client id: f_00000-5-2 loss: 0.745608  [   96/  126]
train() client id: f_00001-0-0 loss: 0.664503  [   32/  265]
train() client id: f_00001-0-1 loss: 0.476262  [   64/  265]
train() client id: f_00001-0-2 loss: 0.524335  [   96/  265]
train() client id: f_00001-0-3 loss: 0.393643  [  128/  265]
train() client id: f_00001-0-4 loss: 0.461632  [  160/  265]
train() client id: f_00001-0-5 loss: 0.474628  [  192/  265]
train() client id: f_00001-0-6 loss: 0.499178  [  224/  265]
train() client id: f_00001-0-7 loss: 0.592079  [  256/  265]
train() client id: f_00001-1-0 loss: 0.454613  [   32/  265]
train() client id: f_00001-1-1 loss: 0.482546  [   64/  265]
train() client id: f_00001-1-2 loss: 0.434673  [   96/  265]
train() client id: f_00001-1-3 loss: 0.610707  [  128/  265]
train() client id: f_00001-1-4 loss: 0.547312  [  160/  265]
train() client id: f_00001-1-5 loss: 0.453330  [  192/  265]
train() client id: f_00001-1-6 loss: 0.575037  [  224/  265]
train() client id: f_00001-1-7 loss: 0.411962  [  256/  265]
train() client id: f_00001-2-0 loss: 0.537300  [   32/  265]
train() client id: f_00001-2-1 loss: 0.513420  [   64/  265]
train() client id: f_00001-2-2 loss: 0.463234  [   96/  265]
train() client id: f_00001-2-3 loss: 0.478415  [  128/  265]
train() client id: f_00001-2-4 loss: 0.593096  [  160/  265]
train() client id: f_00001-2-5 loss: 0.389089  [  192/  265]
train() client id: f_00001-2-6 loss: 0.465948  [  224/  265]
train() client id: f_00001-2-7 loss: 0.520438  [  256/  265]
train() client id: f_00001-3-0 loss: 0.437675  [   32/  265]
train() client id: f_00001-3-1 loss: 0.484777  [   64/  265]
train() client id: f_00001-3-2 loss: 0.440910  [   96/  265]
train() client id: f_00001-3-3 loss: 0.483094  [  128/  265]
train() client id: f_00001-3-4 loss: 0.580874  [  160/  265]
train() client id: f_00001-3-5 loss: 0.393398  [  192/  265]
train() client id: f_00001-3-6 loss: 0.711338  [  224/  265]
train() client id: f_00001-3-7 loss: 0.459749  [  256/  265]
train() client id: f_00001-4-0 loss: 0.525797  [   32/  265]
train() client id: f_00001-4-1 loss: 0.508341  [   64/  265]
train() client id: f_00001-4-2 loss: 0.474590  [   96/  265]
train() client id: f_00001-4-3 loss: 0.513752  [  128/  265]
train() client id: f_00001-4-4 loss: 0.436573  [  160/  265]
train() client id: f_00001-4-5 loss: 0.503783  [  192/  265]
train() client id: f_00001-4-6 loss: 0.527654  [  224/  265]
train() client id: f_00001-4-7 loss: 0.501797  [  256/  265]
train() client id: f_00001-5-0 loss: 0.392404  [   32/  265]
train() client id: f_00001-5-1 loss: 0.526221  [   64/  265]
train() client id: f_00001-5-2 loss: 0.539477  [   96/  265]
train() client id: f_00001-5-3 loss: 0.423969  [  128/  265]
train() client id: f_00001-5-4 loss: 0.403549  [  160/  265]
train() client id: f_00001-5-5 loss: 0.644499  [  192/  265]
train() client id: f_00001-5-6 loss: 0.476188  [  224/  265]
train() client id: f_00001-5-7 loss: 0.525271  [  256/  265]
train() client id: f_00002-0-0 loss: 0.959433  [   32/  124]
train() client id: f_00002-0-1 loss: 0.854758  [   64/  124]
train() client id: f_00002-0-2 loss: 1.064216  [   96/  124]
train() client id: f_00002-1-0 loss: 1.052969  [   32/  124]
train() client id: f_00002-1-1 loss: 0.872391  [   64/  124]
train() client id: f_00002-1-2 loss: 0.847521  [   96/  124]
train() client id: f_00002-2-0 loss: 0.761624  [   32/  124]
train() client id: f_00002-2-1 loss: 0.880935  [   64/  124]
train() client id: f_00002-2-2 loss: 1.114755  [   96/  124]
train() client id: f_00002-3-0 loss: 0.862490  [   32/  124]
train() client id: f_00002-3-1 loss: 0.830302  [   64/  124]
train() client id: f_00002-3-2 loss: 0.966516  [   96/  124]
train() client id: f_00002-4-0 loss: 0.756913  [   32/  124]
train() client id: f_00002-4-1 loss: 0.915316  [   64/  124]
train() client id: f_00002-4-2 loss: 0.898984  [   96/  124]
train() client id: f_00002-5-0 loss: 0.797501  [   32/  124]
train() client id: f_00002-5-1 loss: 0.864447  [   64/  124]
train() client id: f_00002-5-2 loss: 0.857624  [   96/  124]
train() client id: f_00003-0-0 loss: 0.837287  [   32/   43]
train() client id: f_00003-1-0 loss: 0.806918  [   32/   43]
train() client id: f_00003-2-0 loss: 0.743587  [   32/   43]
train() client id: f_00003-3-0 loss: 0.883550  [   32/   43]
train() client id: f_00003-4-0 loss: 0.771636  [   32/   43]
train() client id: f_00003-5-0 loss: 0.712379  [   32/   43]
train() client id: f_00004-0-0 loss: 0.958274  [   32/  306]
train() client id: f_00004-0-1 loss: 0.850339  [   64/  306]
train() client id: f_00004-0-2 loss: 0.825676  [   96/  306]
train() client id: f_00004-0-3 loss: 0.898050  [  128/  306]
train() client id: f_00004-0-4 loss: 0.807141  [  160/  306]
train() client id: f_00004-0-5 loss: 1.013131  [  192/  306]
train() client id: f_00004-0-6 loss: 0.903635  [  224/  306]
train() client id: f_00004-0-7 loss: 0.932204  [  256/  306]
train() client id: f_00004-0-8 loss: 0.799429  [  288/  306]
train() client id: f_00004-1-0 loss: 0.921614  [   32/  306]
train() client id: f_00004-1-1 loss: 0.898026  [   64/  306]
train() client id: f_00004-1-2 loss: 0.918399  [   96/  306]
train() client id: f_00004-1-3 loss: 0.819934  [  128/  306]
train() client id: f_00004-1-4 loss: 0.998711  [  160/  306]
train() client id: f_00004-1-5 loss: 1.064475  [  192/  306]
train() client id: f_00004-1-6 loss: 0.802583  [  224/  306]
train() client id: f_00004-1-7 loss: 0.801423  [  256/  306]
train() client id: f_00004-1-8 loss: 0.772215  [  288/  306]
train() client id: f_00004-2-0 loss: 1.101749  [   32/  306]
train() client id: f_00004-2-1 loss: 0.871717  [   64/  306]
train() client id: f_00004-2-2 loss: 0.969549  [   96/  306]
train() client id: f_00004-2-3 loss: 0.823882  [  128/  306]
train() client id: f_00004-2-4 loss: 0.902999  [  160/  306]
train() client id: f_00004-2-5 loss: 0.780855  [  192/  306]
train() client id: f_00004-2-6 loss: 0.712754  [  224/  306]
train() client id: f_00004-2-7 loss: 0.895305  [  256/  306]
train() client id: f_00004-2-8 loss: 0.938337  [  288/  306]
train() client id: f_00004-3-0 loss: 0.889309  [   32/  306]
train() client id: f_00004-3-1 loss: 0.725873  [   64/  306]
train() client id: f_00004-3-2 loss: 0.805994  [   96/  306]
train() client id: f_00004-3-3 loss: 0.928826  [  128/  306]
train() client id: f_00004-3-4 loss: 1.049479  [  160/  306]
train() client id: f_00004-3-5 loss: 0.818131  [  192/  306]
train() client id: f_00004-3-6 loss: 0.979770  [  224/  306]
train() client id: f_00004-3-7 loss: 0.837594  [  256/  306]
train() client id: f_00004-3-8 loss: 0.988842  [  288/  306]
train() client id: f_00004-4-0 loss: 0.840051  [   32/  306]
train() client id: f_00004-4-1 loss: 0.881361  [   64/  306]
train() client id: f_00004-4-2 loss: 0.951229  [   96/  306]
train() client id: f_00004-4-3 loss: 0.711571  [  128/  306]
train() client id: f_00004-4-4 loss: 0.871932  [  160/  306]
train() client id: f_00004-4-5 loss: 0.838041  [  192/  306]
train() client id: f_00004-4-6 loss: 1.024800  [  224/  306]
train() client id: f_00004-4-7 loss: 0.916408  [  256/  306]
train() client id: f_00004-4-8 loss: 0.912829  [  288/  306]
train() client id: f_00004-5-0 loss: 0.872244  [   32/  306]
train() client id: f_00004-5-1 loss: 0.900343  [   64/  306]
train() client id: f_00004-5-2 loss: 0.915745  [   96/  306]
train() client id: f_00004-5-3 loss: 0.704566  [  128/  306]
train() client id: f_00004-5-4 loss: 0.971925  [  160/  306]
train() client id: f_00004-5-5 loss: 0.762721  [  192/  306]
train() client id: f_00004-5-6 loss: 0.951660  [  224/  306]
train() client id: f_00004-5-7 loss: 0.970663  [  256/  306]
train() client id: f_00004-5-8 loss: 1.045575  [  288/  306]
train() client id: f_00005-0-0 loss: 0.616877  [   32/  146]
train() client id: f_00005-0-1 loss: 0.742518  [   64/  146]
train() client id: f_00005-0-2 loss: 0.651197  [   96/  146]
train() client id: f_00005-0-3 loss: 0.414792  [  128/  146]
train() client id: f_00005-1-0 loss: 0.456597  [   32/  146]
train() client id: f_00005-1-1 loss: 0.805211  [   64/  146]
train() client id: f_00005-1-2 loss: 0.538510  [   96/  146]
train() client id: f_00005-1-3 loss: 0.396365  [  128/  146]
train() client id: f_00005-2-0 loss: 0.466258  [   32/  146]
train() client id: f_00005-2-1 loss: 0.610254  [   64/  146]
train() client id: f_00005-2-2 loss: 0.471350  [   96/  146]
train() client id: f_00005-2-3 loss: 0.719107  [  128/  146]
train() client id: f_00005-3-0 loss: 0.777848  [   32/  146]
train() client id: f_00005-3-1 loss: 0.447470  [   64/  146]
train() client id: f_00005-3-2 loss: 0.577155  [   96/  146]
train() client id: f_00005-3-3 loss: 0.670874  [  128/  146]
train() client id: f_00005-4-0 loss: 0.454776  [   32/  146]
train() client id: f_00005-4-1 loss: 0.516042  [   64/  146]
train() client id: f_00005-4-2 loss: 0.652496  [   96/  146]
train() client id: f_00005-4-3 loss: 0.625618  [  128/  146]
train() client id: f_00005-5-0 loss: 0.463377  [   32/  146]
train() client id: f_00005-5-1 loss: 0.701169  [   64/  146]
train() client id: f_00005-5-2 loss: 0.722463  [   96/  146]
train() client id: f_00005-5-3 loss: 0.553474  [  128/  146]
train() client id: f_00006-0-0 loss: 0.580359  [   32/   54]
train() client id: f_00006-1-0 loss: 0.582455  [   32/   54]
train() client id: f_00006-2-0 loss: 0.498327  [   32/   54]
train() client id: f_00006-3-0 loss: 0.517681  [   32/   54]
train() client id: f_00006-4-0 loss: 0.518185  [   32/   54]
train() client id: f_00006-5-0 loss: 0.453409  [   32/   54]
train() client id: f_00007-0-0 loss: 0.688549  [   32/  179]
train() client id: f_00007-0-1 loss: 0.730712  [   64/  179]
train() client id: f_00007-0-2 loss: 1.047753  [   96/  179]
train() client id: f_00007-0-3 loss: 0.646396  [  128/  179]
train() client id: f_00007-0-4 loss: 0.779835  [  160/  179]
train() client id: f_00007-1-0 loss: 0.690917  [   32/  179]
train() client id: f_00007-1-1 loss: 0.982326  [   64/  179]
train() client id: f_00007-1-2 loss: 0.612810  [   96/  179]
train() client id: f_00007-1-3 loss: 0.935903  [  128/  179]
train() client id: f_00007-1-4 loss: 0.706502  [  160/  179]
train() client id: f_00007-2-0 loss: 0.870820  [   32/  179]
train() client id: f_00007-2-1 loss: 0.663268  [   64/  179]
train() client id: f_00007-2-2 loss: 0.919550  [   96/  179]
train() client id: f_00007-2-3 loss: 0.795458  [  128/  179]
train() client id: f_00007-2-4 loss: 0.672144  [  160/  179]
train() client id: f_00007-3-0 loss: 0.777220  [   32/  179]
train() client id: f_00007-3-1 loss: 0.945495  [   64/  179]
train() client id: f_00007-3-2 loss: 0.609149  [   96/  179]
train() client id: f_00007-3-3 loss: 0.808059  [  128/  179]
train() client id: f_00007-3-4 loss: 0.749276  [  160/  179]
train() client id: f_00007-4-0 loss: 0.848026  [   32/  179]
train() client id: f_00007-4-1 loss: 0.905910  [   64/  179]
train() client id: f_00007-4-2 loss: 0.824603  [   96/  179]
train() client id: f_00007-4-3 loss: 0.575309  [  128/  179]
train() client id: f_00007-4-4 loss: 0.632606  [  160/  179]
train() client id: f_00007-5-0 loss: 0.653709  [   32/  179]
train() client id: f_00007-5-1 loss: 1.020493  [   64/  179]
train() client id: f_00007-5-2 loss: 0.711390  [   96/  179]
train() client id: f_00007-5-3 loss: 0.604771  [  128/  179]
train() client id: f_00007-5-4 loss: 0.779876  [  160/  179]
train() client id: f_00008-0-0 loss: 0.766450  [   32/  130]
train() client id: f_00008-0-1 loss: 0.744606  [   64/  130]
train() client id: f_00008-0-2 loss: 0.699782  [   96/  130]
train() client id: f_00008-0-3 loss: 0.740454  [  128/  130]
train() client id: f_00008-1-0 loss: 0.852189  [   32/  130]
train() client id: f_00008-1-1 loss: 0.668564  [   64/  130]
train() client id: f_00008-1-2 loss: 0.734027  [   96/  130]
train() client id: f_00008-1-3 loss: 0.688234  [  128/  130]
train() client id: f_00008-2-0 loss: 0.724943  [   32/  130]
train() client id: f_00008-2-1 loss: 0.746838  [   64/  130]
train() client id: f_00008-2-2 loss: 0.764675  [   96/  130]
train() client id: f_00008-2-3 loss: 0.715435  [  128/  130]
train() client id: f_00008-3-0 loss: 0.783457  [   32/  130]
train() client id: f_00008-3-1 loss: 0.761037  [   64/  130]
train() client id: f_00008-3-2 loss: 0.634417  [   96/  130]
train() client id: f_00008-3-3 loss: 0.738978  [  128/  130]
train() client id: f_00008-4-0 loss: 0.758625  [   32/  130]
train() client id: f_00008-4-1 loss: 0.784032  [   64/  130]
train() client id: f_00008-4-2 loss: 0.660276  [   96/  130]
train() client id: f_00008-4-3 loss: 0.738738  [  128/  130]
train() client id: f_00008-5-0 loss: 0.638614  [   32/  130]
train() client id: f_00008-5-1 loss: 0.772683  [   64/  130]
train() client id: f_00008-5-2 loss: 0.735111  [   96/  130]
train() client id: f_00008-5-3 loss: 0.785165  [  128/  130]
train() client id: f_00009-0-0 loss: 1.010494  [   32/  118]
train() client id: f_00009-0-1 loss: 0.956263  [   64/  118]
train() client id: f_00009-0-2 loss: 1.067615  [   96/  118]
train() client id: f_00009-1-0 loss: 0.990216  [   32/  118]
train() client id: f_00009-1-1 loss: 0.841522  [   64/  118]
train() client id: f_00009-1-2 loss: 1.007369  [   96/  118]
train() client id: f_00009-2-0 loss: 0.926024  [   32/  118]
train() client id: f_00009-2-1 loss: 0.983376  [   64/  118]
train() client id: f_00009-2-2 loss: 1.070577  [   96/  118]
train() client id: f_00009-3-0 loss: 0.737983  [   32/  118]
train() client id: f_00009-3-1 loss: 0.929925  [   64/  118]
train() client id: f_00009-3-2 loss: 0.934747  [   96/  118]
train() client id: f_00009-4-0 loss: 0.822247  [   32/  118]
train() client id: f_00009-4-1 loss: 0.901747  [   64/  118]
train() client id: f_00009-4-2 loss: 1.010487  [   96/  118]
train() client id: f_00009-5-0 loss: 0.945071  [   32/  118]
train() client id: f_00009-5-1 loss: 1.022518  [   64/  118]
train() client id: f_00009-5-2 loss: 0.770780  [   96/  118]
At round 68 accuracy: 0.6472148541114059
At round 68 training accuracy: 0.590878604963112
At round 68 training loss: 0.8188017339437949
update_location
xs = -4.528292 241.001589 250.045120 -250.943528 139.896481 -175.217951 -292.215960 313.375741 -1.680116 234.695607 
ys = 327.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -257.154970 4.001482 
xs mean: 45.442869159412865
ys mean: 9.371751218646875
dists_uav = 342.541058 261.388122 269.303371 271.073650 171.963507 201.765333 308.868795 329.502864 275.919375 255.143175 
uav_gains = -118.750271 -112.292006 -113.007012 -113.168123 -105.912147 -107.793433 -116.448578 -117.939355 -113.609482 -111.739486 
uav_gains_db_mean: -113.06598947371282
dists_bs = 228.787431 445.510677 459.159975 170.323223 360.572670 172.185987 213.232429 525.562389 465.615400 443.949303 
bs_gains = -105.631160 -113.735086 -114.102052 -102.042727 -111.162858 -102.174998 -104.774950 -115.744539 -114.271825 -113.692394 
bs_gains_db_mean: -109.73325881878091
Round 69
-------------------------------
ene_coms = [0.0237478  0.01421861 0.01337629 0.0135549  0.01159987 0.00692702
 0.01836075 0.02158912 0.01490184 0.01416663]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.85439532 3.73602699 1.77905679 0.65788258 4.29192263 2.05417539
 0.83232483 2.57862535 1.86917411 1.69957601]
obj_prev = 21.35316000410055
eta_min = 2.4523694403431736e-50	eta_max = 0.9591819595456603
af = 4.439896748873053	bf = 0.6931482672838977	zeta = 4.883886423760359	eta = 0.9090909090909091
af = 4.439896748873053	bf = 0.6931482672838977	zeta = 12.279471872047884	eta = 0.3615706599711115
af = 4.439896748873053	bf = 0.6931482672838977	zeta = 8.160410409374233	eta = 0.5440776292051124
af = 4.439896748873053	bf = 0.6931482672838977	zeta = 7.447650907597386	eta = 0.5961472689789867
af = 4.439896748873053	bf = 0.6931482672838977	zeta = 7.403101519771364	eta = 0.5997346837694283
af = 4.439896748873053	bf = 0.6931482672838977	zeta = 7.402902280027038	eta = 0.5997508248693021
eta = 0.5997508248693021
ene_coms = [0.0237478  0.01421861 0.01337629 0.0135549  0.01159987 0.00692702
 0.01836075 0.02158912 0.01490184 0.01416663]
ene_comp = [0.04218638 0.08872533 0.04151676 0.01439694 0.10245264 0.04888263
 0.01807988 0.05993145 0.04352563 0.03950788]
ene_total = [0.74903023 1.16947106 0.62359996 0.31754048 1.29566752 0.63401285
 0.41397538 0.92609581 0.66375192 0.60975707]
ti_comp = [2.09958849 2.19488043 2.20330356 2.2015175  2.2210678  2.26779627
 2.15345903 2.12117528 2.18804812 2.19540015]
ti_coms = [0.237478   0.14218605 0.13376292 0.13554898 0.11599869 0.06927021
 0.18360745 0.2158912  0.14901836 0.14166633]
t_total = [26.54971046 26.54971046 26.54971046 26.54971046 26.54971046 26.54971046
 26.54971046 26.54971046 26.54971046 26.54971046]
ene_coms = [0.0237478  0.01421861 0.01337629 0.0135549  0.01159987 0.00692702
 0.01836075 0.02158912 0.01490184 0.01416663]
ene_comp = [1.06445792e-06 9.06152134e-06 9.21301143e-07 3.84809965e-08
 1.36246664e-05 1.41950106e-06 7.96513752e-08 2.99013782e-06
 1.07646904e-06 7.99657231e-07]
ene_total = [0.26979355 0.16163016 0.15196878 0.15398776 0.13193244 0.07870897
 0.20858396 0.24529225 0.16930114 0.16094589]
optimize_network iter = 0 obj = 1.7321448995836068
eta = 0.5997508248693021
freqs = [10046345.3552865  20211881.76217946  9421479.11332327  3269776.39451227
 23063825.95323444 10777562.75518682  4197869.04692037 14126943.72144785
  9946223.72665706  8997877.12371023]
eta_min = 0.5997508248693032	eta_max = 0.8182303783919983
af = 0.00027638624593701575	bf = 0.6931482672838977	zeta = 0.00030402487053071735	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0237478  0.01421861 0.01337629 0.0135549  0.01159987 0.00692702
 0.01836075 0.02158912 0.01490184 0.01416663]
ene_comp = [1.98445890e-07 1.68933091e-06 1.71757307e-07 7.17397603e-09
 2.54003375e-06 2.64636250e-07 1.48493311e-08 5.57448586e-07
 2.00685113e-07 1.49079347e-07]
ene_total = [1.18778779 0.71124701 0.66904168 0.67796667 0.58030994 0.34647745
 0.91833789 1.07983653 0.74534524 0.7085705 ]
ti_comp = [0.82388007 0.91917201 0.92759514 0.92580908 0.94535938 0.99208785
 0.87775062 0.84546686 0.91233971 0.91969174]
ti_coms = [0.237478   0.14218605 0.13376292 0.13554898 0.11599869 0.06927021
 0.18360745 0.2158912  0.14901836 0.14166633]
t_total = [26.54971046 26.54971046 26.54971046 26.54971046 26.54971046 26.54971046
 26.54971046 26.54971046 26.54971046 26.54971046]
ene_coms = [0.0237478  0.01421861 0.01337629 0.0135549  0.01159987 0.00692702
 0.01836075 0.02158912 0.01490184 0.01416663]
ene_comp = [4.17694072e-07 3.12189800e-06 3.14067352e-07 1.31473178e-08
 4.54407064e-06 4.48159879e-07 2.89676767e-08 1.13720977e-06
 3.74103331e-07 2.75318962e-07]
ene_total = [0.59405804 0.35575435 0.33461377 0.33907404 0.29028262 0.1732896
 0.45929197 0.54007701 0.37277652 0.35438305]
optimize_network iter = 1 obj = 3.8136009641180624
eta = 0.8182303783919983
freqs = [10046345.3552865  18938720.62984994  8781420.49940017  3051044.71808104
 21263073.6843142   9667279.5974068   4041328.67516715 13907785.90516966
  9360269.03655159  8428325.08987055]
eta_min = 0.8182303783919971	eta_max = 0.8182303783919934
af = 0.00024194507816120494	bf = 0.6931482672838977	zeta = 0.00026613958597732545	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0237478  0.01421861 0.01337629 0.0135549  0.01159987 0.00692702
 0.01836075 0.02158912 0.01490184 0.01416663]
ene_comp = [1.98445890e-07 1.48320953e-06 1.49212975e-07 6.24627296e-09
 2.15888183e-06 2.12920154e-07 1.37625041e-08 5.40286830e-07
 1.77735988e-07 1.30803668e-07]
ene_total = [1.18778779 0.7112367  0.66904055 0.67796662 0.58029088 0.34647486
 0.91833784 1.07983567 0.74534409 0.70856958]
ti_comp = [0.82388007 0.91917201 0.92759514 0.92580908 0.94535938 0.99208785
 0.87775062 0.84546686 0.91233971 0.91969174]
ti_coms = [0.237478   0.14218605 0.13376292 0.13554898 0.11599869 0.06927021
 0.18360745 0.2158912  0.14901836 0.14166633]
t_total = [26.54971046 26.54971046 26.54971046 26.54971046 26.54971046 26.54971046
 26.54971046 26.54971046 26.54971046 26.54971046]
ene_coms = [0.0237478  0.01421861 0.01337629 0.0135549  0.01159987 0.00692702
 0.01836075 0.02158912 0.01490184 0.01416663]
ene_comp = [4.17694072e-07 3.12189800e-06 3.14067352e-07 1.31473178e-08
 4.54407064e-06 4.48159879e-07 2.89676767e-08 1.13720977e-06
 3.74103331e-07 2.75318962e-07]
ene_total = [0.59405804 0.35575435 0.33461377 0.33907404 0.29028262 0.1732896
 0.45929197 0.54007701 0.37277652 0.35438305]
optimize_network iter = 2 obj = 3.8136009641179602
eta = 0.8182303783919934
freqs = [10046345.35528646 18938720.62984992  8781420.49940016  3051044.71808103
 21263073.6843142   9667279.59740681  4041328.67516714 13907785.90516961
  9360269.03655157  8428325.08987054]
Done!
ene_coms = [0.0237478  0.01421861 0.01337629 0.0135549  0.01159987 0.00692702
 0.01836075 0.02158912 0.01490184 0.01416663]
ene_comp = [3.81511828e-07 2.85146736e-06 2.86861647e-07 1.20084473e-08
 4.15044602e-06 4.09338571e-07 2.64583867e-08 1.03870035e-06
 3.41697082e-07 2.51469790e-07]
ene_total = [0.02374818 0.01422146 0.01337658 0.01355491 0.01160402 0.00692743
 0.01836077 0.02159016 0.01490218 0.01416688]
At round 69 energy consumption: 0.15245257005525978
At round 69 eta: 0.8182303783919934
At round 69 a_n: 4.546939415371483
At round 69 local rounds: 6.5690346911709785
At round 69 global rounds: 25.01484777900423
gradient difference: 0.8063594698905945
train() client id: f_00000-0-0 loss: 1.046304  [   32/  126]
train() client id: f_00000-0-1 loss: 0.853760  [   64/  126]
train() client id: f_00000-0-2 loss: 1.118939  [   96/  126]
train() client id: f_00000-1-0 loss: 0.870666  [   32/  126]
train() client id: f_00000-1-1 loss: 1.092700  [   64/  126]
train() client id: f_00000-1-2 loss: 1.133589  [   96/  126]
train() client id: f_00000-2-0 loss: 1.002463  [   32/  126]
train() client id: f_00000-2-1 loss: 0.813426  [   64/  126]
train() client id: f_00000-2-2 loss: 1.079083  [   96/  126]
train() client id: f_00000-3-0 loss: 0.955936  [   32/  126]
train() client id: f_00000-3-1 loss: 1.084811  [   64/  126]
train() client id: f_00000-3-2 loss: 0.954985  [   96/  126]
train() client id: f_00000-4-0 loss: 0.987727  [   32/  126]
train() client id: f_00000-4-1 loss: 0.947493  [   64/  126]
train() client id: f_00000-4-2 loss: 0.914712  [   96/  126]
train() client id: f_00000-5-0 loss: 0.997682  [   32/  126]
train() client id: f_00000-5-1 loss: 0.967509  [   64/  126]
train() client id: f_00000-5-2 loss: 0.964397  [   96/  126]
train() client id: f_00001-0-0 loss: 0.505252  [   32/  265]
train() client id: f_00001-0-1 loss: 0.484112  [   64/  265]
train() client id: f_00001-0-2 loss: 0.584132  [   96/  265]
train() client id: f_00001-0-3 loss: 0.699752  [  128/  265]
train() client id: f_00001-0-4 loss: 0.543314  [  160/  265]
train() client id: f_00001-0-5 loss: 0.513440  [  192/  265]
train() client id: f_00001-0-6 loss: 0.663447  [  224/  265]
train() client id: f_00001-0-7 loss: 0.524484  [  256/  265]
train() client id: f_00001-1-0 loss: 0.493488  [   32/  265]
train() client id: f_00001-1-1 loss: 0.553930  [   64/  265]
train() client id: f_00001-1-2 loss: 0.598649  [   96/  265]
train() client id: f_00001-1-3 loss: 0.476618  [  128/  265]
train() client id: f_00001-1-4 loss: 0.665200  [  160/  265]
train() client id: f_00001-1-5 loss: 0.543034  [  192/  265]
train() client id: f_00001-1-6 loss: 0.633985  [  224/  265]
train() client id: f_00001-1-7 loss: 0.546166  [  256/  265]
train() client id: f_00001-2-0 loss: 0.645057  [   32/  265]
train() client id: f_00001-2-1 loss: 0.604416  [   64/  265]
train() client id: f_00001-2-2 loss: 0.467373  [   96/  265]
train() client id: f_00001-2-3 loss: 0.526909  [  128/  265]
train() client id: f_00001-2-4 loss: 0.527121  [  160/  265]
train() client id: f_00001-2-5 loss: 0.664662  [  192/  265]
train() client id: f_00001-2-6 loss: 0.469091  [  224/  265]
train() client id: f_00001-2-7 loss: 0.499630  [  256/  265]
train() client id: f_00001-3-0 loss: 0.650895  [   32/  265]
train() client id: f_00001-3-1 loss: 0.500550  [   64/  265]
train() client id: f_00001-3-2 loss: 0.653414  [   96/  265]
train() client id: f_00001-3-3 loss: 0.569582  [  128/  265]
train() client id: f_00001-3-4 loss: 0.473002  [  160/  265]
train() client id: f_00001-3-5 loss: 0.488470  [  192/  265]
train() client id: f_00001-3-6 loss: 0.459674  [  224/  265]
train() client id: f_00001-3-7 loss: 0.700243  [  256/  265]
train() client id: f_00001-4-0 loss: 0.608826  [   32/  265]
train() client id: f_00001-4-1 loss: 0.644461  [   64/  265]
train() client id: f_00001-4-2 loss: 0.543642  [   96/  265]
train() client id: f_00001-4-3 loss: 0.491422  [  128/  265]
train() client id: f_00001-4-4 loss: 0.564979  [  160/  265]
train() client id: f_00001-4-5 loss: 0.616324  [  192/  265]
train() client id: f_00001-4-6 loss: 0.485598  [  224/  265]
train() client id: f_00001-4-7 loss: 0.541111  [  256/  265]
train() client id: f_00001-5-0 loss: 0.633064  [   32/  265]
train() client id: f_00001-5-1 loss: 0.566649  [   64/  265]
train() client id: f_00001-5-2 loss: 0.563969  [   96/  265]
train() client id: f_00001-5-3 loss: 0.502786  [  128/  265]
train() client id: f_00001-5-4 loss: 0.687818  [  160/  265]
train() client id: f_00001-5-5 loss: 0.567458  [  192/  265]
train() client id: f_00001-5-6 loss: 0.522785  [  224/  265]
train() client id: f_00001-5-7 loss: 0.471726  [  256/  265]
train() client id: f_00002-0-0 loss: 1.041386  [   32/  124]
train() client id: f_00002-0-1 loss: 0.547149  [   64/  124]
train() client id: f_00002-0-2 loss: 1.168331  [   96/  124]
train() client id: f_00002-1-0 loss: 1.132581  [   32/  124]
train() client id: f_00002-1-1 loss: 0.970869  [   64/  124]
train() client id: f_00002-1-2 loss: 0.868521  [   96/  124]
train() client id: f_00002-2-0 loss: 0.813918  [   32/  124]
train() client id: f_00002-2-1 loss: 0.905580  [   64/  124]
train() client id: f_00002-2-2 loss: 1.017218  [   96/  124]
train() client id: f_00002-3-0 loss: 1.013720  [   32/  124]
train() client id: f_00002-3-1 loss: 1.056301  [   64/  124]
train() client id: f_00002-3-2 loss: 0.869546  [   96/  124]
train() client id: f_00002-4-0 loss: 0.863570  [   32/  124]
train() client id: f_00002-4-1 loss: 1.036865  [   64/  124]
train() client id: f_00002-4-2 loss: 0.876036  [   96/  124]
train() client id: f_00002-5-0 loss: 1.117753  [   32/  124]
train() client id: f_00002-5-1 loss: 0.952276  [   64/  124]
train() client id: f_00002-5-2 loss: 0.956058  [   96/  124]
train() client id: f_00003-0-0 loss: 0.739547  [   32/   43]
train() client id: f_00003-1-0 loss: 1.010049  [   32/   43]
train() client id: f_00003-2-0 loss: 0.654586  [   32/   43]
train() client id: f_00003-3-0 loss: 0.750292  [   32/   43]
train() client id: f_00003-4-0 loss: 0.846919  [   32/   43]
train() client id: f_00003-5-0 loss: 0.617718  [   32/   43]
train() client id: f_00004-0-0 loss: 0.632386  [   32/  306]
train() client id: f_00004-0-1 loss: 0.680635  [   64/  306]
train() client id: f_00004-0-2 loss: 0.645510  [   96/  306]
train() client id: f_00004-0-3 loss: 0.935213  [  128/  306]
train() client id: f_00004-0-4 loss: 0.696257  [  160/  306]
train() client id: f_00004-0-5 loss: 0.584906  [  192/  306]
train() client id: f_00004-0-6 loss: 0.754598  [  224/  306]
train() client id: f_00004-0-7 loss: 0.755990  [  256/  306]
train() client id: f_00004-0-8 loss: 0.554638  [  288/  306]
train() client id: f_00004-1-0 loss: 0.480477  [   32/  306]
train() client id: f_00004-1-1 loss: 0.740533  [   64/  306]
train() client id: f_00004-1-2 loss: 0.872247  [   96/  306]
train() client id: f_00004-1-3 loss: 0.690292  [  128/  306]
train() client id: f_00004-1-4 loss: 0.784725  [  160/  306]
train() client id: f_00004-1-5 loss: 0.666413  [  192/  306]
train() client id: f_00004-1-6 loss: 0.740764  [  224/  306]
train() client id: f_00004-1-7 loss: 0.669591  [  256/  306]
train() client id: f_00004-1-8 loss: 0.717568  [  288/  306]
train() client id: f_00004-2-0 loss: 0.780372  [   32/  306]
train() client id: f_00004-2-1 loss: 0.795002  [   64/  306]
train() client id: f_00004-2-2 loss: 0.695516  [   96/  306]
train() client id: f_00004-2-3 loss: 0.755547  [  128/  306]
train() client id: f_00004-2-4 loss: 0.635663  [  160/  306]
train() client id: f_00004-2-5 loss: 0.698514  [  192/  306]
train() client id: f_00004-2-6 loss: 0.837948  [  224/  306]
train() client id: f_00004-2-7 loss: 0.730514  [  256/  306]
train() client id: f_00004-2-8 loss: 0.481244  [  288/  306]
train() client id: f_00004-3-0 loss: 0.718520  [   32/  306]
train() client id: f_00004-3-1 loss: 0.673185  [   64/  306]
train() client id: f_00004-3-2 loss: 0.801050  [   96/  306]
train() client id: f_00004-3-3 loss: 0.735394  [  128/  306]
train() client id: f_00004-3-4 loss: 0.653385  [  160/  306]
train() client id: f_00004-3-5 loss: 0.737366  [  192/  306]
train() client id: f_00004-3-6 loss: 0.709827  [  224/  306]
train() client id: f_00004-3-7 loss: 0.709607  [  256/  306]
train() client id: f_00004-3-8 loss: 0.623431  [  288/  306]
train() client id: f_00004-4-0 loss: 0.769365  [   32/  306]
train() client id: f_00004-4-1 loss: 0.583287  [   64/  306]
train() client id: f_00004-4-2 loss: 0.777752  [   96/  306]
train() client id: f_00004-4-3 loss: 0.755103  [  128/  306]
train() client id: f_00004-4-4 loss: 0.699856  [  160/  306]
train() client id: f_00004-4-5 loss: 0.750934  [  192/  306]
train() client id: f_00004-4-6 loss: 0.598606  [  224/  306]
train() client id: f_00004-4-7 loss: 0.655932  [  256/  306]
train() client id: f_00004-4-8 loss: 0.631978  [  288/  306]
train() client id: f_00004-5-0 loss: 0.623329  [   32/  306]
train() client id: f_00004-5-1 loss: 0.597709  [   64/  306]
train() client id: f_00004-5-2 loss: 0.827592  [   96/  306]
train() client id: f_00004-5-3 loss: 0.725758  [  128/  306]
train() client id: f_00004-5-4 loss: 0.669142  [  160/  306]
train() client id: f_00004-5-5 loss: 0.582354  [  192/  306]
train() client id: f_00004-5-6 loss: 0.770189  [  224/  306]
train() client id: f_00004-5-7 loss: 0.615605  [  256/  306]
train() client id: f_00004-5-8 loss: 0.850802  [  288/  306]
train() client id: f_00005-0-0 loss: 0.487905  [   32/  146]
train() client id: f_00005-0-1 loss: 1.037055  [   64/  146]
train() client id: f_00005-0-2 loss: 0.515901  [   96/  146]
train() client id: f_00005-0-3 loss: 0.599861  [  128/  146]
train() client id: f_00005-1-0 loss: 0.800875  [   32/  146]
train() client id: f_00005-1-1 loss: 0.592563  [   64/  146]
train() client id: f_00005-1-2 loss: 0.659253  [   96/  146]
train() client id: f_00005-1-3 loss: 0.423805  [  128/  146]
train() client id: f_00005-2-0 loss: 0.755317  [   32/  146]
train() client id: f_00005-2-1 loss: 0.528540  [   64/  146]
train() client id: f_00005-2-2 loss: 0.776294  [   96/  146]
train() client id: f_00005-2-3 loss: 0.614971  [  128/  146]
train() client id: f_00005-3-0 loss: 0.949757  [   32/  146]
train() client id: f_00005-3-1 loss: 0.687190  [   64/  146]
train() client id: f_00005-3-2 loss: 0.640228  [   96/  146]
train() client id: f_00005-3-3 loss: 0.486421  [  128/  146]
train() client id: f_00005-4-0 loss: 0.718325  [   32/  146]
train() client id: f_00005-4-1 loss: 0.555381  [   64/  146]
train() client id: f_00005-4-2 loss: 0.720807  [   96/  146]
train() client id: f_00005-4-3 loss: 0.781346  [  128/  146]
train() client id: f_00005-5-0 loss: 0.410475  [   32/  146]
train() client id: f_00005-5-1 loss: 0.690646  [   64/  146]
train() client id: f_00005-5-2 loss: 0.827987  [   96/  146]
train() client id: f_00005-5-3 loss: 0.721505  [  128/  146]
train() client id: f_00006-0-0 loss: 0.603389  [   32/   54]
train() client id: f_00006-1-0 loss: 0.566868  [   32/   54]
train() client id: f_00006-2-0 loss: 0.614608  [   32/   54]
train() client id: f_00006-3-0 loss: 0.489534  [   32/   54]
train() client id: f_00006-4-0 loss: 0.586205  [   32/   54]
train() client id: f_00006-5-0 loss: 0.533455  [   32/   54]
train() client id: f_00007-0-0 loss: 0.845871  [   32/  179]
train() client id: f_00007-0-1 loss: 0.529535  [   64/  179]
train() client id: f_00007-0-2 loss: 0.587738  [   96/  179]
train() client id: f_00007-0-3 loss: 0.425286  [  128/  179]
train() client id: f_00007-0-4 loss: 0.567017  [  160/  179]
train() client id: f_00007-1-0 loss: 0.509553  [   32/  179]
train() client id: f_00007-1-1 loss: 0.846837  [   64/  179]
train() client id: f_00007-1-2 loss: 0.414578  [   96/  179]
train() client id: f_00007-1-3 loss: 0.547524  [  128/  179]
train() client id: f_00007-1-4 loss: 0.557875  [  160/  179]
train() client id: f_00007-2-0 loss: 0.794137  [   32/  179]
train() client id: f_00007-2-1 loss: 0.584548  [   64/  179]
train() client id: f_00007-2-2 loss: 0.464139  [   96/  179]
train() client id: f_00007-2-3 loss: 0.643886  [  128/  179]
train() client id: f_00007-2-4 loss: 0.586473  [  160/  179]
train() client id: f_00007-3-0 loss: 0.438228  [   32/  179]
train() client id: f_00007-3-1 loss: 0.726759  [   64/  179]
train() client id: f_00007-3-2 loss: 0.724898  [   96/  179]
train() client id: f_00007-3-3 loss: 0.383363  [  128/  179]
train() client id: f_00007-3-4 loss: 0.606429  [  160/  179]
train() client id: f_00007-4-0 loss: 0.432410  [   32/  179]
train() client id: f_00007-4-1 loss: 0.846580  [   64/  179]
train() client id: f_00007-4-2 loss: 0.616894  [   96/  179]
train() client id: f_00007-4-3 loss: 0.528892  [  128/  179]
train() client id: f_00007-4-4 loss: 0.585247  [  160/  179]
train() client id: f_00007-5-0 loss: 0.539360  [   32/  179]
train() client id: f_00007-5-1 loss: 0.774412  [   64/  179]
train() client id: f_00007-5-2 loss: 0.418430  [   96/  179]
train() client id: f_00007-5-3 loss: 0.681772  [  128/  179]
train() client id: f_00007-5-4 loss: 0.574790  [  160/  179]
train() client id: f_00008-0-0 loss: 0.663246  [   32/  130]
train() client id: f_00008-0-1 loss: 0.692919  [   64/  130]
train() client id: f_00008-0-2 loss: 0.593922  [   96/  130]
train() client id: f_00008-0-3 loss: 0.727604  [  128/  130]
train() client id: f_00008-1-0 loss: 0.535381  [   32/  130]
train() client id: f_00008-1-1 loss: 0.599161  [   64/  130]
train() client id: f_00008-1-2 loss: 0.777946  [   96/  130]
train() client id: f_00008-1-3 loss: 0.737067  [  128/  130]
train() client id: f_00008-2-0 loss: 0.747874  [   32/  130]
train() client id: f_00008-2-1 loss: 0.645504  [   64/  130]
train() client id: f_00008-2-2 loss: 0.600693  [   96/  130]
train() client id: f_00008-2-3 loss: 0.675081  [  128/  130]
train() client id: f_00008-3-0 loss: 0.705610  [   32/  130]
train() client id: f_00008-3-1 loss: 0.694540  [   64/  130]
train() client id: f_00008-3-2 loss: 0.553465  [   96/  130]
train() client id: f_00008-3-3 loss: 0.700362  [  128/  130]
train() client id: f_00008-4-0 loss: 0.665323  [   32/  130]
train() client id: f_00008-4-1 loss: 0.643820  [   64/  130]
train() client id: f_00008-4-2 loss: 0.677835  [   96/  130]
train() client id: f_00008-4-3 loss: 0.675260  [  128/  130]
train() client id: f_00008-5-0 loss: 0.495500  [   32/  130]
train() client id: f_00008-5-1 loss: 0.743778  [   64/  130]
train() client id: f_00008-5-2 loss: 0.739763  [   96/  130]
train() client id: f_00008-5-3 loss: 0.599393  [  128/  130]
train() client id: f_00009-0-0 loss: 0.852430  [   32/  118]
train() client id: f_00009-0-1 loss: 1.124025  [   64/  118]
train() client id: f_00009-0-2 loss: 0.905508  [   96/  118]
train() client id: f_00009-1-0 loss: 0.946518  [   32/  118]
train() client id: f_00009-1-1 loss: 0.848445  [   64/  118]
train() client id: f_00009-1-2 loss: 0.874910  [   96/  118]
train() client id: f_00009-2-0 loss: 0.853836  [   32/  118]
train() client id: f_00009-2-1 loss: 1.148150  [   64/  118]
train() client id: f_00009-2-2 loss: 0.901790  [   96/  118]
train() client id: f_00009-3-0 loss: 0.791354  [   32/  118]
train() client id: f_00009-3-1 loss: 1.084308  [   64/  118]
train() client id: f_00009-3-2 loss: 0.926274  [   96/  118]
train() client id: f_00009-4-0 loss: 0.939450  [   32/  118]
train() client id: f_00009-4-1 loss: 1.252135  [   64/  118]
train() client id: f_00009-4-2 loss: 0.725574  [   96/  118]
train() client id: f_00009-5-0 loss: 0.755541  [   32/  118]
train() client id: f_00009-5-1 loss: 1.026800  [   64/  118]
train() client id: f_00009-5-2 loss: 0.948022  [   96/  118]
At round 69 accuracy: 0.6472148541114059
At round 69 training accuracy: 0.5922199865861838
At round 69 training loss: 0.8226062429664507
update_location
xs = -4.528292 246.001589 255.045120 -255.943528 144.896481 -180.217951 -297.215960 318.375741 -1.680116 239.695607 
ys = 332.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -262.154970 4.001482 
xs mean: 46.442869159412865
ys mean: 9.371751218646875
dists_uav = 347.325865 266.005199 273.952107 275.708830 176.055141 206.122365 313.603399 334.261716 280.585194 259.749872 
uav_gains = -119.024277 -112.707707 -113.430344 -113.590320 -106.176268 -108.070797 -116.813501 -118.246647 -114.032934 -112.145808 
uav_gains_db_mean: -113.42386024016862
dists_bs = 232.152037 450.183050 463.792339 172.610647 364.947415 172.264894 216.021361 530.211828 470.259769 448.567654 
bs_gains = -105.808689 -113.861955 -114.224119 -102.204951 -111.309507 -102.180569 -104.932967 -115.851643 -114.392518 -113.818242 
bs_gains_db_mean: -109.85851604858505
Round 70
-------------------------------
ene_coms = [0.0245545  0.01437506 0.01385486 0.01404373 0.01172509 0.00692878
 0.01907436 0.0223692  0.01506345 0.01432081]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.7181195  3.45523605 1.64706312 0.61039667 3.96912041 1.89943057
 0.77265185 2.38767628 1.72904533 1.57219241]
obj_prev = 19.76093216414036
eta_min = 2.3971198796262583e-54	eta_max = 0.9609117932171964
af = 4.105415012178352	bf = 0.6571880273072728	zeta = 4.515956513396188	eta = 0.9090909090909091
af = 4.105415012178352	bf = 0.6571880273072728	zeta = 11.533234573183027	eta = 0.3559638873316794
af = 4.105415012178352	bf = 0.6571880273072728	zeta = 7.604818752483064	eta = 0.5398438997428946
af = 4.105415012178352	bf = 0.6571880273072728	zeta = 6.928246295499431	eta = 0.5925619322807878
af = 4.105415012178352	bf = 0.6571880273072728	zeta = 6.885834807272998	eta = 0.5962116616335476
af = 4.105415012178352	bf = 0.6571880273072728	zeta = 6.885643526821261	eta = 0.5962282241575184
eta = 0.5962282241575184
ene_coms = [0.0245545  0.01437506 0.01385486 0.01404373 0.01172509 0.00692878
 0.01907436 0.0223692  0.01506345 0.01432081]
ene_comp = [0.04267247 0.08974766 0.04199513 0.01456283 0.10363315 0.04944588
 0.0182882  0.06062201 0.04402715 0.03996311]
ene_total = [0.7000208  1.08420875 0.58155467 0.29787424 1.20120197 0.58701789
 0.38904875 0.86417058 0.61529843 0.56524743]
ti_comp = [2.29937222 2.40116661 2.40636856 2.40447993 2.42766625 2.47562941
 2.35417363 2.32122519 2.39428273 2.4017091 ]
ti_coms = [0.24554497 0.14375058 0.13854863 0.14043726 0.11725094 0.06928778
 0.19074356 0.223692   0.15063446 0.14320809]
t_total = [26.49970627 26.49970627 26.49970627 26.49970627 26.49970627 26.49970627
 26.49970627 26.49970627 26.49970627 26.49970627]
ene_coms = [0.0245545  0.01437506 0.01385486 0.01404373 0.01172509 0.00692878
 0.01907436 0.0223692  0.01506345 0.01432081]
ene_comp = [9.18554440e-07 7.83618816e-06 7.99378737e-07 3.33868066e-08
 1.18031559e-05 1.23281845e-06 6.89788780e-08 2.58426098e-06
 9.30445665e-07 6.91539581e-07]
ene_total = [0.25569056 0.14976615 0.1442762  0.14623482 0.12221392 0.0721608
 0.19861812 0.23295285 0.15686229 0.14912688]
optimize_network iter = 0 obj = 1.6279025909847666
eta = 0.5962282241575184
freqs = [ 9279157.33543043 18688345.38395637  8725831.33743856  3028269.85598429
 21344192.01374261  9986527.36641453  3884208.58600142 13058191.41675124
  9194226.38929457  8319723.22030867]
eta_min = 0.5962282241575187	eta_max = 0.8272745000942728
af = 0.00021869233769604955	bf = 0.6571880273072728	zeta = 0.00024056157146565453	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.0245545  0.01437506 0.01385486 0.01404373 0.01172509 0.00692878
 0.01907436 0.0223692  0.01506345 0.01432081]
ene_comp = [1.69294551e-07 1.44425186e-06 1.47329824e-07 6.15336901e-09
 2.17538546e-06 2.27215108e-07 1.27131803e-08 4.76293275e-07
 1.71486168e-07 1.27454485e-07]
ene_total = [1.13561228 0.66488919 0.64077107 0.64949915 0.5423666  0.32045491
 0.8821577  1.03456016 0.69666713 0.6623194 ]
ti_comp = [0.84311977 0.94491416 0.95011611 0.94822747 0.9714138  1.01937696
 0.89792117 0.86497273 0.93803028 0.94545664]
ti_coms = [0.24554497 0.14375058 0.13854863 0.14043726 0.11725094 0.06928778
 0.19074356 0.223692   0.15063446 0.14320809]
t_total = [26.49970627 26.49970627 26.49970627 26.49970627 26.49970627 26.49970627
 26.49970627 26.49970627 26.49970627 26.49970627]
ene_coms = [0.0245545  0.01437506 0.01385486 0.01404373 0.01172509 0.00692878
 0.01907436 0.0223692  0.01506345 0.01432081]
ene_comp = [3.36809934e-07 2.49462178e-06 2.52792099e-07 1.05835679e-08
 3.63418929e-06 3.58460038e-07 2.33752712e-08 9.17499972e-07
 2.98846481e-07 2.19995901e-07]
ene_total = [0.59770088 0.34997084 0.33725396 0.34184528 0.28549454 0.16866541
 0.46429855 0.54452168 0.36667377 0.34859499]
optimize_network iter = 1 obj = 3.8050198919718166
eta = 0.8272745000942728
freqs = [ 9279157.33543044 17413287.99463553  8103493.16494447  2815679.2915214
 19558899.8827057   8892938.92937581  3734073.08349654 12849252.96135049
  8605057.20061076  7749392.4604279 ]
eta_min = 0.8272745000942711	eta_max = 0.8272745000942714
af = 0.00018951549454202513	bf = 0.6571880273072728	zeta = 0.00020846704399622766	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.0245545  0.01437506 0.01385486 0.01404373 0.01172509 0.00692878
 0.01907436 0.0223692  0.01506345 0.01432081]
ene_comp = [1.69294551e-07 1.25389969e-06 1.27063725e-07 5.31973730e-09
 1.82669328e-06 1.80176785e-07 1.17493744e-08 4.61173290e-07
 1.50212555e-07 1.10579005e-07]
ene_total = [1.13561228 0.66488039 0.64077013 0.64949911 0.54235047 0.32045274
 0.88215765 1.03455946 0.69666615 0.66231862]
ti_comp = [0.84311977 0.94491416 0.95011611 0.94822747 0.9714138  1.01937696
 0.89792117 0.86497273 0.93803028 0.94545664]
ti_coms = [0.24554497 0.14375058 0.13854863 0.14043726 0.11725094 0.06928778
 0.19074356 0.223692   0.15063446 0.14320809]
t_total = [26.49970627 26.49970627 26.49970627 26.49970627 26.49970627 26.49970627
 26.49970627 26.49970627 26.49970627 26.49970627]
ene_coms = [0.0245545  0.01437506 0.01385486 0.01404373 0.01172509 0.00692878
 0.01907436 0.0223692  0.01506345 0.01432081]
ene_comp = [3.36809934e-07 2.49462178e-06 2.52792099e-07 1.05835679e-08
 3.63418929e-06 3.58460038e-07 2.33752712e-08 9.17499972e-07
 2.98846481e-07 2.19995901e-07]
ene_total = [0.59770088 0.34997084 0.33725396 0.34184528 0.28549454 0.16866541
 0.46429855 0.54452168 0.36667377 0.34859499]
optimize_network iter = 2 obj = 3.8050198919717846
eta = 0.8272745000942714
freqs = [ 9279157.33543042 17413287.99463551  8103493.16494446  2815679.2915214
 19558899.88270569  8892938.92937581  3734073.08349653 12849252.96135047
  8605057.20061075  7749392.46042789]
Done!
ene_coms = [0.0245545  0.01437506 0.01385486 0.01404373 0.01172509 0.00692878
 0.01907436 0.0223692  0.01506345 0.01432081]
ene_comp = [3.25468436e-07 2.41061966e-06 2.44279757e-07 1.02271843e-08
 3.51181418e-06 3.46389509e-07 2.25881489e-08 8.86604730e-07
 2.88783337e-07 2.12587916e-07]
ene_total = [0.02455482 0.01437747 0.01385511 0.01404374 0.01172861 0.00692912
 0.01907438 0.02237009 0.01506373 0.01432102]
At round 70 energy consumption: 0.15631808540919692
At round 70 eta: 0.8272745000942714
At round 70 a_n: 4.204393568402164
At round 70 local rounds: 6.209080150493547
At round 70 global rounds: 24.341475756022525
gradient difference: 0.6621419787406921
train() client id: f_00000-0-0 loss: 1.164328  [   32/  126]
train() client id: f_00000-0-1 loss: 0.992359  [   64/  126]
train() client id: f_00000-0-2 loss: 0.980311  [   96/  126]
train() client id: f_00000-1-0 loss: 1.101327  [   32/  126]
train() client id: f_00000-1-1 loss: 0.959082  [   64/  126]
train() client id: f_00000-1-2 loss: 0.973020  [   96/  126]
train() client id: f_00000-2-0 loss: 0.954481  [   32/  126]
train() client id: f_00000-2-1 loss: 0.981889  [   64/  126]
train() client id: f_00000-2-2 loss: 0.997235  [   96/  126]
train() client id: f_00000-3-0 loss: 0.920316  [   32/  126]
train() client id: f_00000-3-1 loss: 0.852018  [   64/  126]
train() client id: f_00000-3-2 loss: 0.973928  [   96/  126]
train() client id: f_00000-4-0 loss: 0.899653  [   32/  126]
train() client id: f_00000-4-1 loss: 0.875287  [   64/  126]
train() client id: f_00000-4-2 loss: 0.889227  [   96/  126]
train() client id: f_00000-5-0 loss: 0.901333  [   32/  126]
train() client id: f_00000-5-1 loss: 0.899846  [   64/  126]
train() client id: f_00000-5-2 loss: 0.888551  [   96/  126]
train() client id: f_00001-0-0 loss: 0.697196  [   32/  265]
train() client id: f_00001-0-1 loss: 0.484935  [   64/  265]
train() client id: f_00001-0-2 loss: 0.562733  [   96/  265]
train() client id: f_00001-0-3 loss: 0.541897  [  128/  265]
train() client id: f_00001-0-4 loss: 0.560236  [  160/  265]
train() client id: f_00001-0-5 loss: 0.561405  [  192/  265]
train() client id: f_00001-0-6 loss: 0.528495  [  224/  265]
train() client id: f_00001-0-7 loss: 0.556216  [  256/  265]
train() client id: f_00001-1-0 loss: 0.581401  [   32/  265]
train() client id: f_00001-1-1 loss: 0.553991  [   64/  265]
train() client id: f_00001-1-2 loss: 0.466902  [   96/  265]
train() client id: f_00001-1-3 loss: 0.691467  [  128/  265]
train() client id: f_00001-1-4 loss: 0.515454  [  160/  265]
train() client id: f_00001-1-5 loss: 0.657200  [  192/  265]
train() client id: f_00001-1-6 loss: 0.510712  [  224/  265]
train() client id: f_00001-1-7 loss: 0.528936  [  256/  265]
train() client id: f_00001-2-0 loss: 0.504109  [   32/  265]
train() client id: f_00001-2-1 loss: 0.638811  [   64/  265]
train() client id: f_00001-2-2 loss: 0.456590  [   96/  265]
train() client id: f_00001-2-3 loss: 0.488788  [  128/  265]
train() client id: f_00001-2-4 loss: 0.569506  [  160/  265]
train() client id: f_00001-2-5 loss: 0.620750  [  192/  265]
train() client id: f_00001-2-6 loss: 0.620295  [  224/  265]
train() client id: f_00001-2-7 loss: 0.489601  [  256/  265]
train() client id: f_00001-3-0 loss: 0.610220  [   32/  265]
train() client id: f_00001-3-1 loss: 0.592445  [   64/  265]
train() client id: f_00001-3-2 loss: 0.492718  [   96/  265]
train() client id: f_00001-3-3 loss: 0.648955  [  128/  265]
train() client id: f_00001-3-4 loss: 0.526272  [  160/  265]
train() client id: f_00001-3-5 loss: 0.445248  [  192/  265]
train() client id: f_00001-3-6 loss: 0.546474  [  224/  265]
train() client id: f_00001-3-7 loss: 0.615041  [  256/  265]
train() client id: f_00001-4-0 loss: 0.602320  [   32/  265]
train() client id: f_00001-4-1 loss: 0.567484  [   64/  265]
train() client id: f_00001-4-2 loss: 0.548849  [   96/  265]
train() client id: f_00001-4-3 loss: 0.540071  [  128/  265]
train() client id: f_00001-4-4 loss: 0.481719  [  160/  265]
train() client id: f_00001-4-5 loss: 0.515826  [  192/  265]
train() client id: f_00001-4-6 loss: 0.697890  [  224/  265]
train() client id: f_00001-4-7 loss: 0.446516  [  256/  265]
train() client id: f_00001-5-0 loss: 0.518809  [   32/  265]
train() client id: f_00001-5-1 loss: 0.563834  [   64/  265]
train() client id: f_00001-5-2 loss: 0.591885  [   96/  265]
train() client id: f_00001-5-3 loss: 0.472485  [  128/  265]
train() client id: f_00001-5-4 loss: 0.529688  [  160/  265]
train() client id: f_00001-5-5 loss: 0.472798  [  192/  265]
train() client id: f_00001-5-6 loss: 0.694255  [  224/  265]
train() client id: f_00001-5-7 loss: 0.608786  [  256/  265]
train() client id: f_00002-0-0 loss: 1.163005  [   32/  124]
train() client id: f_00002-0-1 loss: 1.008643  [   64/  124]
train() client id: f_00002-0-2 loss: 0.710656  [   96/  124]
train() client id: f_00002-1-0 loss: 0.928165  [   32/  124]
train() client id: f_00002-1-1 loss: 0.892984  [   64/  124]
train() client id: f_00002-1-2 loss: 0.942911  [   96/  124]
train() client id: f_00002-2-0 loss: 1.156007  [   32/  124]
train() client id: f_00002-2-1 loss: 0.842009  [   64/  124]
train() client id: f_00002-2-2 loss: 1.062303  [   96/  124]
train() client id: f_00002-3-0 loss: 0.879729  [   32/  124]
train() client id: f_00002-3-1 loss: 0.985228  [   64/  124]
train() client id: f_00002-3-2 loss: 0.913714  [   96/  124]
train() client id: f_00002-4-0 loss: 0.948440  [   32/  124]
train() client id: f_00002-4-1 loss: 1.001920  [   64/  124]
train() client id: f_00002-4-2 loss: 0.862812  [   96/  124]
train() client id: f_00002-5-0 loss: 0.815568  [   32/  124]
train() client id: f_00002-5-1 loss: 0.870090  [   64/  124]
train() client id: f_00002-5-2 loss: 1.081394  [   96/  124]
train() client id: f_00003-0-0 loss: 0.824040  [   32/   43]
train() client id: f_00003-1-0 loss: 0.736946  [   32/   43]
train() client id: f_00003-2-0 loss: 0.761018  [   32/   43]
train() client id: f_00003-3-0 loss: 0.770980  [   32/   43]
train() client id: f_00003-4-0 loss: 0.860528  [   32/   43]
train() client id: f_00003-5-0 loss: 0.679338  [   32/   43]
train() client id: f_00004-0-0 loss: 0.577713  [   32/  306]
train() client id: f_00004-0-1 loss: 0.688754  [   64/  306]
train() client id: f_00004-0-2 loss: 0.583066  [   96/  306]
train() client id: f_00004-0-3 loss: 0.717484  [  128/  306]
train() client id: f_00004-0-4 loss: 0.616393  [  160/  306]
train() client id: f_00004-0-5 loss: 0.793869  [  192/  306]
train() client id: f_00004-0-6 loss: 0.746966  [  224/  306]
train() client id: f_00004-0-7 loss: 0.714828  [  256/  306]
train() client id: f_00004-0-8 loss: 0.596302  [  288/  306]
train() client id: f_00004-1-0 loss: 0.554577  [   32/  306]
train() client id: f_00004-1-1 loss: 0.866833  [   64/  306]
train() client id: f_00004-1-2 loss: 0.574426  [   96/  306]
train() client id: f_00004-1-3 loss: 0.598521  [  128/  306]
train() client id: f_00004-1-4 loss: 0.655818  [  160/  306]
train() client id: f_00004-1-5 loss: 0.781016  [  192/  306]
train() client id: f_00004-1-6 loss: 0.608344  [  224/  306]
train() client id: f_00004-1-7 loss: 0.697855  [  256/  306]
train() client id: f_00004-1-8 loss: 0.639263  [  288/  306]
train() client id: f_00004-2-0 loss: 0.692678  [   32/  306]
train() client id: f_00004-2-1 loss: 0.602459  [   64/  306]
train() client id: f_00004-2-2 loss: 0.776972  [   96/  306]
train() client id: f_00004-2-3 loss: 0.547888  [  128/  306]
train() client id: f_00004-2-4 loss: 0.766361  [  160/  306]
train() client id: f_00004-2-5 loss: 0.592047  [  192/  306]
train() client id: f_00004-2-6 loss: 0.637754  [  224/  306]
train() client id: f_00004-2-7 loss: 0.863574  [  256/  306]
train() client id: f_00004-2-8 loss: 0.794533  [  288/  306]
train() client id: f_00004-3-0 loss: 0.601101  [   32/  306]
train() client id: f_00004-3-1 loss: 0.724707  [   64/  306]
train() client id: f_00004-3-2 loss: 0.651873  [   96/  306]
train() client id: f_00004-3-3 loss: 0.808369  [  128/  306]
train() client id: f_00004-3-4 loss: 0.841951  [  160/  306]
train() client id: f_00004-3-5 loss: 0.751116  [  192/  306]
train() client id: f_00004-3-6 loss: 0.680360  [  224/  306]
train() client id: f_00004-3-7 loss: 0.600082  [  256/  306]
train() client id: f_00004-3-8 loss: 0.554174  [  288/  306]
train() client id: f_00004-4-0 loss: 0.984802  [   32/  306]
train() client id: f_00004-4-1 loss: 0.673642  [   64/  306]
train() client id: f_00004-4-2 loss: 0.663026  [   96/  306]
train() client id: f_00004-4-3 loss: 0.574256  [  128/  306]
train() client id: f_00004-4-4 loss: 0.543051  [  160/  306]
train() client id: f_00004-4-5 loss: 0.652645  [  192/  306]
train() client id: f_00004-4-6 loss: 0.780612  [  224/  306]
train() client id: f_00004-4-7 loss: 0.622322  [  256/  306]
train() client id: f_00004-4-8 loss: 0.734993  [  288/  306]
train() client id: f_00004-5-0 loss: 0.874448  [   32/  306]
train() client id: f_00004-5-1 loss: 0.713783  [   64/  306]
train() client id: f_00004-5-2 loss: 0.733386  [   96/  306]
train() client id: f_00004-5-3 loss: 0.566252  [  128/  306]
train() client id: f_00004-5-4 loss: 0.626991  [  160/  306]
train() client id: f_00004-5-5 loss: 0.884424  [  192/  306]
train() client id: f_00004-5-6 loss: 0.736677  [  224/  306]
train() client id: f_00004-5-7 loss: 0.483002  [  256/  306]
train() client id: f_00004-5-8 loss: 0.638891  [  288/  306]
train() client id: f_00005-0-0 loss: 0.474653  [   32/  146]
train() client id: f_00005-0-1 loss: 0.648588  [   64/  146]
train() client id: f_00005-0-2 loss: 0.411542  [   96/  146]
train() client id: f_00005-0-3 loss: 0.678600  [  128/  146]
train() client id: f_00005-1-0 loss: 0.435085  [   32/  146]
train() client id: f_00005-1-1 loss: 0.791674  [   64/  146]
train() client id: f_00005-1-2 loss: 0.492568  [   96/  146]
train() client id: f_00005-1-3 loss: 0.661822  [  128/  146]
train() client id: f_00005-2-0 loss: 0.625395  [   32/  146]
train() client id: f_00005-2-1 loss: 0.492955  [   64/  146]
train() client id: f_00005-2-2 loss: 0.546566  [   96/  146]
train() client id: f_00005-2-3 loss: 0.642587  [  128/  146]
train() client id: f_00005-3-0 loss: 0.366918  [   32/  146]
train() client id: f_00005-3-1 loss: 0.722679  [   64/  146]
train() client id: f_00005-3-2 loss: 0.480134  [   96/  146]
train() client id: f_00005-3-3 loss: 0.443611  [  128/  146]
train() client id: f_00005-4-0 loss: 1.025434  [   32/  146]
train() client id: f_00005-4-1 loss: 0.362526  [   64/  146]
train() client id: f_00005-4-2 loss: 0.297204  [   96/  146]
train() client id: f_00005-4-3 loss: 0.488791  [  128/  146]
train() client id: f_00005-5-0 loss: 0.712621  [   32/  146]
train() client id: f_00005-5-1 loss: 0.622295  [   64/  146]
train() client id: f_00005-5-2 loss: 0.635218  [   96/  146]
train() client id: f_00005-5-3 loss: 0.334431  [  128/  146]
train() client id: f_00006-0-0 loss: 0.509317  [   32/   54]
train() client id: f_00006-1-0 loss: 0.548828  [   32/   54]
train() client id: f_00006-2-0 loss: 0.573473  [   32/   54]
train() client id: f_00006-3-0 loss: 0.457728  [   32/   54]
train() client id: f_00006-4-0 loss: 0.484316  [   32/   54]
train() client id: f_00006-5-0 loss: 0.564398  [   32/   54]
train() client id: f_00007-0-0 loss: 0.646791  [   32/  179]
train() client id: f_00007-0-1 loss: 0.856126  [   64/  179]
train() client id: f_00007-0-2 loss: 0.444039  [   96/  179]
train() client id: f_00007-0-3 loss: 0.632727  [  128/  179]
train() client id: f_00007-0-4 loss: 0.527082  [  160/  179]
train() client id: f_00007-1-0 loss: 0.715871  [   32/  179]
train() client id: f_00007-1-1 loss: 0.466362  [   64/  179]
train() client id: f_00007-1-2 loss: 0.588161  [   96/  179]
train() client id: f_00007-1-3 loss: 0.647702  [  128/  179]
train() client id: f_00007-1-4 loss: 0.557150  [  160/  179]
train() client id: f_00007-2-0 loss: 0.573903  [   32/  179]
train() client id: f_00007-2-1 loss: 0.523986  [   64/  179]
train() client id: f_00007-2-2 loss: 0.552673  [   96/  179]
train() client id: f_00007-2-3 loss: 0.496156  [  128/  179]
train() client id: f_00007-2-4 loss: 0.916260  [  160/  179]
train() client id: f_00007-3-0 loss: 0.620425  [   32/  179]
train() client id: f_00007-3-1 loss: 0.538243  [   64/  179]
train() client id: f_00007-3-2 loss: 0.636731  [   96/  179]
train() client id: f_00007-3-3 loss: 0.505312  [  128/  179]
train() client id: f_00007-3-4 loss: 0.637403  [  160/  179]
train() client id: f_00007-4-0 loss: 0.595701  [   32/  179]
train() client id: f_00007-4-1 loss: 0.424673  [   64/  179]
train() client id: f_00007-4-2 loss: 0.617850  [   96/  179]
train() client id: f_00007-4-3 loss: 0.558875  [  128/  179]
train() client id: f_00007-4-4 loss: 0.689752  [  160/  179]
train() client id: f_00007-5-0 loss: 0.683142  [   32/  179]
train() client id: f_00007-5-1 loss: 0.433908  [   64/  179]
train() client id: f_00007-5-2 loss: 0.614306  [   96/  179]
train() client id: f_00007-5-3 loss: 0.697424  [  128/  179]
train() client id: f_00007-5-4 loss: 0.488408  [  160/  179]
train() client id: f_00008-0-0 loss: 0.825142  [   32/  130]
train() client id: f_00008-0-1 loss: 0.882198  [   64/  130]
train() client id: f_00008-0-2 loss: 0.770119  [   96/  130]
train() client id: f_00008-0-3 loss: 0.753829  [  128/  130]
train() client id: f_00008-1-0 loss: 0.812293  [   32/  130]
train() client id: f_00008-1-1 loss: 0.703900  [   64/  130]
train() client id: f_00008-1-2 loss: 0.895322  [   96/  130]
train() client id: f_00008-1-3 loss: 0.813331  [  128/  130]
train() client id: f_00008-2-0 loss: 0.913995  [   32/  130]
train() client id: f_00008-2-1 loss: 0.805480  [   64/  130]
train() client id: f_00008-2-2 loss: 0.799426  [   96/  130]
train() client id: f_00008-2-3 loss: 0.701163  [  128/  130]
train() client id: f_00008-3-0 loss: 0.770583  [   32/  130]
train() client id: f_00008-3-1 loss: 0.800939  [   64/  130]
train() client id: f_00008-3-2 loss: 0.850241  [   96/  130]
train() client id: f_00008-3-3 loss: 0.794138  [  128/  130]
train() client id: f_00008-4-0 loss: 0.790451  [   32/  130]
train() client id: f_00008-4-1 loss: 0.777337  [   64/  130]
train() client id: f_00008-4-2 loss: 0.894567  [   96/  130]
train() client id: f_00008-4-3 loss: 0.730500  [  128/  130]
train() client id: f_00008-5-0 loss: 0.918686  [   32/  130]
train() client id: f_00008-5-1 loss: 0.697524  [   64/  130]
train() client id: f_00008-5-2 loss: 0.810553  [   96/  130]
train() client id: f_00008-5-3 loss: 0.793351  [  128/  130]
train() client id: f_00009-0-0 loss: 0.830409  [   32/  118]
train() client id: f_00009-0-1 loss: 0.920144  [   64/  118]
train() client id: f_00009-0-2 loss: 0.919394  [   96/  118]
train() client id: f_00009-1-0 loss: 0.857285  [   32/  118]
train() client id: f_00009-1-1 loss: 0.859101  [   64/  118]
train() client id: f_00009-1-2 loss: 0.819682  [   96/  118]
train() client id: f_00009-2-0 loss: 1.036212  [   32/  118]
train() client id: f_00009-2-1 loss: 0.669963  [   64/  118]
train() client id: f_00009-2-2 loss: 0.731801  [   96/  118]
train() client id: f_00009-3-0 loss: 0.747869  [   32/  118]
train() client id: f_00009-3-1 loss: 0.759535  [   64/  118]
train() client id: f_00009-3-2 loss: 0.693197  [   96/  118]
train() client id: f_00009-4-0 loss: 0.770093  [   32/  118]
train() client id: f_00009-4-1 loss: 0.932501  [   64/  118]
train() client id: f_00009-4-2 loss: 0.808777  [   96/  118]
train() client id: f_00009-5-0 loss: 0.797320  [   32/  118]
train() client id: f_00009-5-1 loss: 0.758869  [   64/  118]
train() client id: f_00009-5-2 loss: 0.744927  [   96/  118]
At round 70 accuracy: 0.6472148541114059
At round 70 training accuracy: 0.5915492957746479
At round 70 training loss: 0.8245166890274858
update_location
xs = -4.528292 251.001589 260.045120 -260.943528 149.896481 -185.217951 -302.215960 323.375741 -1.680116 244.695607 
ys = 337.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -267.154970 4.001482 
xs mean: 47.442869159412865
ys mean: 9.371751218646875
dists_uav = 352.116651 270.635885 278.613008 280.356548 180.192612 210.507978 318.346119 339.027509 285.262337 264.370861 
uav_gains = -119.286864 -113.128264 -113.854286 -114.012255 -106.439897 -108.354446 -117.165500 -118.541233 -114.453524 -112.560025 
uav_gains_db_mean: -113.77962934382899
dists_bs = 235.574718 454.862391 468.432263 175.011059 369.338030 172.488763 218.888986 534.867591 474.911360 453.194104 
bs_gains = -105.986662 -113.987700 -114.345170 -102.372893 -111.454932 -102.196362 -105.093329 -115.957955 -114.512211 -113.943018 
bs_gains_db_mean: -109.98502309312971
Round 71
-------------------------------
ene_coms = [0.02536807 0.01453315 0.01436551 0.01456437 0.01185175 0.00693376
 0.01980712 0.0231598  0.01522676 0.01447662]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.58131224 3.17434322 1.51486342 0.56269661 3.64623711 1.74469712
 0.71255974 2.19622843 1.58881135 1.4447085 ]
obj_prev = 18.16645773693819
eta_min = 4.535618662036233e-59	eta_max = 0.9628467496194133
af = 3.7709332754836535	bf = 0.6190036659610687	zeta = 4.14802660303202	eta = 0.909090909090909
af = 3.7709332754836535	bf = 0.6190036659610687	zeta = 10.762531939252789	eta = 0.35037603574772375
af = 3.7709332754836535	bf = 0.6190036659610687	zeta = 7.0406324738862445	eta = 0.5355958132270179
af = 3.7709332754836535	bf = 0.6190036659610687	zeta = 6.402788848104007	eta = 0.5889516841712346
af = 3.7709332754836535	bf = 0.6190036659610687	zeta = 6.362701434045514	eta = 0.5926623014724786
af = 3.7709332754836535	bf = 0.6190036659610687	zeta = 6.362519229401285	eta = 0.5926792736528199
eta = 0.5926792736528199
ene_coms = [0.02536807 0.01453315 0.01436551 0.01456437 0.01185175 0.00693376
 0.01980712 0.0231598  0.01522676 0.01447662]
ene_comp = [0.04316511 0.09078377 0.04247995 0.01473095 0.10482956 0.05001672
 0.01849933 0.06132187 0.04453543 0.04042447]
ene_total = [0.6497698  0.99852    0.53895744 0.27775185 1.10626695 0.53995308
 0.36318722 0.800979   0.56661123 0.52052264]
ti_comp = [2.53604918 2.64439839 2.64607471 2.64408611 2.67121235 2.72039223
 2.59165868 2.55813181 2.63746224 2.64496363]
ti_coms = [0.25368067 0.14533146 0.14365514 0.14564374 0.1185175  0.06933762
 0.19807117 0.23159804 0.15226761 0.14476622]
t_total = [26.44970207 26.44970207 26.44970207 26.44970207 26.44970207 26.44970207
 26.44970207 26.44970207 26.44970207 26.44970207]
ene_coms = [0.02536807 0.01453315 0.01436551 0.01456437 0.01185175 0.00693376
 0.01980712 0.0231598  0.01522676 0.01447662]
ene_comp = [7.81561812e-07 6.68730133e-06 6.84270618e-07 2.85773015e-08
 1.00905419e-05 1.05672639e-06 5.89105003e-08 2.20231936e-06
 7.93641913e-07 5.90165274e-07]
ene_total = [0.24052454 0.13785358 0.13620734 0.13808653 0.11246327 0.06574969
 0.18779378 0.21960125 0.14437394 0.13725987]
optimize_network iter = 0 obj = 1.5199137880020188
eta = 0.5926792736528199
freqs = [ 8510306.83149269 17165297.49862761  8026975.34030909  2785641.386517
 19622094.27805308  9192924.10349147  3569014.40564109 11985673.91034324
  8442857.2576209   7641782.05986658]
eta_min = 0.5926792736528202	eta_max = 0.8370704565614783
af = 0.00016960778840687126	bf = 0.6190036659610687	zeta = 0.0001865685672475584	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02536807 0.01453315 0.01436551 0.01456437 0.01185175 0.00693376
 0.01980712 0.0231598  0.01522676 0.01447662]
ene_comp = [1.42402082e-07 1.21843931e-06 1.24675437e-07 5.20683991e-09
 1.83851636e-06 1.92537604e-07 1.07336077e-08 4.01266869e-07
 1.44603100e-07 1.07529261e-07]
ene_total = [1.07764977 0.61742452 0.61025701 0.61869957 0.5035443  0.29455663
 0.84141324 0.98385304 0.6468439  0.61497617]
ti_comp = [0.86221985 0.97056906 0.97224538 0.97025678 0.99738302 1.0465629
 0.91782935 0.88430248 0.96363291 0.9711343 ]
ti_coms = [0.25368067 0.14533146 0.14365514 0.14564374 0.1185175  0.06933762
 0.19807117 0.23159804 0.15226761 0.14476622]
t_total = [26.44970207 26.44970207 26.44970207 26.44970207 26.44970207 26.44970207
 26.44970207 26.44970207 26.44970207 26.44970207]
ene_coms = [0.02536807 0.01453315 0.01436551 0.01456437 0.01185175 0.00693376
 0.01980712 0.0231598  0.01522676 0.01447662]
ene_comp = [2.65719654e-07 1.95089015e-06 1.99187170e-07 8.34026933e-09
 2.84438527e-06 2.80592445e-07 1.84588857e-08 7.24278321e-07
 2.33644138e-07 1.72042408e-07]
ene_total = [0.60129449 0.34451904 0.34050423 0.34521319 0.28098427 0.16435457
 0.46947988 0.54896387 0.3609188  0.34313712]
optimize_network iter = 1 obj = 3.7993694560940154
eta = 0.8370704565614783
freqs = [ 8510306.83149269 15900549.10660501  7427428.67876559  2580919.51631745
 17867021.04767543  8124192.60506899  3426293.10220829 11788128.98764826
  7856415.07739422  7076123.51605783]
eta_min = 0.8370704565614857	eta_max = 0.8370704565614755
af = 0.00014543326325611335	bf = 0.6190036659610687	zeta = 0.00015997658958172468	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02536807 0.01453315 0.01436551 0.01456437 0.01185175 0.00693376
 0.01980712 0.0231598  0.01522676 0.01447662]
ene_comp = [1.42402082e-07 1.04550346e-06 1.06746593e-07 4.46964197e-09
 1.52433732e-06 1.50372574e-07 9.89231964e-09 3.88148710e-07
 1.25212461e-07 9.21994169e-08]
ene_total = [1.07764977 0.61741717 0.61025625 0.61869954 0.50353096 0.29455484
 0.8414132  0.98385249 0.64684307 0.61497552]
ti_comp = [0.86221985 0.97056906 0.97224538 0.97025678 0.99738302 1.0465629
 0.91782935 0.88430248 0.96363291 0.9711343 ]
ti_coms = [0.25368067 0.14533146 0.14365514 0.14564374 0.1185175  0.06933762
 0.19807117 0.23159804 0.15226761 0.14476622]
t_total = [26.44970207 26.44970207 26.44970207 26.44970207 26.44970207 26.44970207
 26.44970207 26.44970207 26.44970207 26.44970207]
ene_coms = [0.02536807 0.01453315 0.01436551 0.01456437 0.01185175 0.00693376
 0.01980712 0.0231598  0.01522676 0.01447662]
ene_comp = [2.65719654e-07 1.95089015e-06 1.99187170e-07 8.34026933e-09
 2.84438527e-06 2.80592445e-07 1.84588857e-08 7.24278321e-07
 2.33644138e-07 1.72042408e-07]
ene_total = [0.60129449 0.34451904 0.34050423 0.34521319 0.28098427 0.16435457
 0.46947988 0.54896387 0.3609188  0.34313712]
optimize_network iter = 2 obj = 3.7993694560939506
eta = 0.8370704565614755
freqs = [ 8510306.83149266 15900549.106605    7427428.67876559  2580919.51631745
 17867021.04767543  8124192.60506899  3426293.10220828 11788128.98764823
  7856415.07739421  7076123.51605783]
Done!
ene_coms = [0.02536807 0.01453315 0.01436551 0.01456437 0.01185175 0.00693376
 0.01980712 0.0231598  0.01522676 0.01447662]
ene_comp = [2.28139765e-07 1.67498194e-06 1.71016760e-07 7.16073147e-09
 2.44211287e-06 2.40909145e-07 1.58483040e-08 6.21845933e-07
 2.00600588e-07 1.47710996e-07]
ene_total = [0.02536829 0.01453482 0.01436569 0.01456438 0.01185419 0.006934
 0.01980713 0.02316043 0.01522696 0.01447677]
At round 71 energy consumption: 0.16029266752619162
At round 71 eta: 0.8370704565614755
At round 71 a_n: 3.8618477214328486
At round 71 local rounds: 5.823615482218687
At round 71 global rounds: 23.70256271472322
gradient difference: 0.7461999654769897
train() client id: f_00000-0-0 loss: 0.801737  [   32/  126]
train() client id: f_00000-0-1 loss: 1.125614  [   64/  126]
train() client id: f_00000-0-2 loss: 0.848919  [   96/  126]
train() client id: f_00000-1-0 loss: 0.914576  [   32/  126]
train() client id: f_00000-1-1 loss: 0.716323  [   64/  126]
train() client id: f_00000-1-2 loss: 0.984970  [   96/  126]
train() client id: f_00000-2-0 loss: 0.900502  [   32/  126]
train() client id: f_00000-2-1 loss: 0.916200  [   64/  126]
train() client id: f_00000-2-2 loss: 0.816557  [   96/  126]
train() client id: f_00000-3-0 loss: 0.986886  [   32/  126]
train() client id: f_00000-3-1 loss: 1.025135  [   64/  126]
train() client id: f_00000-3-2 loss: 0.877419  [   96/  126]
train() client id: f_00000-4-0 loss: 0.865826  [   32/  126]
train() client id: f_00000-4-1 loss: 0.836057  [   64/  126]
train() client id: f_00000-4-2 loss: 1.098555  [   96/  126]
train() client id: f_00001-0-0 loss: 0.462493  [   32/  265]
train() client id: f_00001-0-1 loss: 0.475301  [   64/  265]
train() client id: f_00001-0-2 loss: 0.480821  [   96/  265]
train() client id: f_00001-0-3 loss: 0.302152  [  128/  265]
train() client id: f_00001-0-4 loss: 0.338029  [  160/  265]
train() client id: f_00001-0-5 loss: 0.478104  [  192/  265]
train() client id: f_00001-0-6 loss: 0.415402  [  224/  265]
train() client id: f_00001-0-7 loss: 0.397037  [  256/  265]
train() client id: f_00001-1-0 loss: 0.463002  [   32/  265]
train() client id: f_00001-1-1 loss: 0.487255  [   64/  265]
train() client id: f_00001-1-2 loss: 0.348453  [   96/  265]
train() client id: f_00001-1-3 loss: 0.608335  [  128/  265]
train() client id: f_00001-1-4 loss: 0.348340  [  160/  265]
train() client id: f_00001-1-5 loss: 0.335673  [  192/  265]
train() client id: f_00001-1-6 loss: 0.362513  [  224/  265]
train() client id: f_00001-1-7 loss: 0.396012  [  256/  265]
train() client id: f_00001-2-0 loss: 0.372792  [   32/  265]
train() client id: f_00001-2-1 loss: 0.464768  [   64/  265]
train() client id: f_00001-2-2 loss: 0.338728  [   96/  265]
train() client id: f_00001-2-3 loss: 0.526591  [  128/  265]
train() client id: f_00001-2-4 loss: 0.392992  [  160/  265]
train() client id: f_00001-2-5 loss: 0.400325  [  192/  265]
train() client id: f_00001-2-6 loss: 0.307807  [  224/  265]
train() client id: f_00001-2-7 loss: 0.485555  [  256/  265]
train() client id: f_00001-3-0 loss: 0.323258  [   32/  265]
train() client id: f_00001-3-1 loss: 0.512158  [   64/  265]
train() client id: f_00001-3-2 loss: 0.450698  [   96/  265]
train() client id: f_00001-3-3 loss: 0.350565  [  128/  265]
train() client id: f_00001-3-4 loss: 0.317718  [  160/  265]
train() client id: f_00001-3-5 loss: 0.484146  [  192/  265]
train() client id: f_00001-3-6 loss: 0.325633  [  224/  265]
train() client id: f_00001-3-7 loss: 0.511071  [  256/  265]
train() client id: f_00001-4-0 loss: 0.327592  [   32/  265]
train() client id: f_00001-4-1 loss: 0.403219  [   64/  265]
train() client id: f_00001-4-2 loss: 0.418191  [   96/  265]
train() client id: f_00001-4-3 loss: 0.572576  [  128/  265]
train() client id: f_00001-4-4 loss: 0.429380  [  160/  265]
train() client id: f_00001-4-5 loss: 0.451593  [  192/  265]
train() client id: f_00001-4-6 loss: 0.332864  [  224/  265]
train() client id: f_00001-4-7 loss: 0.302701  [  256/  265]
train() client id: f_00002-0-0 loss: 1.012476  [   32/  124]
train() client id: f_00002-0-1 loss: 0.846507  [   64/  124]
train() client id: f_00002-0-2 loss: 0.854937  [   96/  124]
train() client id: f_00002-1-0 loss: 0.717268  [   32/  124]
train() client id: f_00002-1-1 loss: 0.926978  [   64/  124]
train() client id: f_00002-1-2 loss: 0.762380  [   96/  124]
train() client id: f_00002-2-0 loss: 0.801127  [   32/  124]
train() client id: f_00002-2-1 loss: 0.917741  [   64/  124]
train() client id: f_00002-2-2 loss: 0.855028  [   96/  124]
train() client id: f_00002-3-0 loss: 0.660045  [   32/  124]
train() client id: f_00002-3-1 loss: 0.781797  [   64/  124]
train() client id: f_00002-3-2 loss: 0.852749  [   96/  124]
train() client id: f_00002-4-0 loss: 0.609345  [   32/  124]
train() client id: f_00002-4-1 loss: 0.821640  [   64/  124]
train() client id: f_00002-4-2 loss: 0.853058  [   96/  124]
train() client id: f_00003-0-0 loss: 0.675016  [   32/   43]
train() client id: f_00003-1-0 loss: 0.903438  [   32/   43]
train() client id: f_00003-2-0 loss: 0.787300  [   32/   43]
train() client id: f_00003-3-0 loss: 0.565991  [   32/   43]
train() client id: f_00003-4-0 loss: 0.682985  [   32/   43]
train() client id: f_00004-0-0 loss: 0.994000  [   32/  306]
train() client id: f_00004-0-1 loss: 0.868147  [   64/  306]
train() client id: f_00004-0-2 loss: 0.918974  [   96/  306]
train() client id: f_00004-0-3 loss: 1.084584  [  128/  306]
train() client id: f_00004-0-4 loss: 0.996895  [  160/  306]
train() client id: f_00004-0-5 loss: 0.910477  [  192/  306]
train() client id: f_00004-0-6 loss: 0.971272  [  224/  306]
train() client id: f_00004-0-7 loss: 0.945477  [  256/  306]
train() client id: f_00004-0-8 loss: 0.867545  [  288/  306]
train() client id: f_00004-1-0 loss: 0.893212  [   32/  306]
train() client id: f_00004-1-1 loss: 0.881434  [   64/  306]
train() client id: f_00004-1-2 loss: 0.834800  [   96/  306]
train() client id: f_00004-1-3 loss: 0.835435  [  128/  306]
train() client id: f_00004-1-4 loss: 1.073746  [  160/  306]
train() client id: f_00004-1-5 loss: 1.076113  [  192/  306]
train() client id: f_00004-1-6 loss: 0.905525  [  224/  306]
train() client id: f_00004-1-7 loss: 1.013125  [  256/  306]
train() client id: f_00004-1-8 loss: 0.949032  [  288/  306]
train() client id: f_00004-2-0 loss: 0.859143  [   32/  306]
train() client id: f_00004-2-1 loss: 1.035749  [   64/  306]
train() client id: f_00004-2-2 loss: 0.933389  [   96/  306]
train() client id: f_00004-2-3 loss: 0.881557  [  128/  306]
train() client id: f_00004-2-4 loss: 1.013585  [  160/  306]
train() client id: f_00004-2-5 loss: 0.962809  [  192/  306]
train() client id: f_00004-2-6 loss: 1.006991  [  224/  306]
train() client id: f_00004-2-7 loss: 0.815135  [  256/  306]
train() client id: f_00004-2-8 loss: 0.862173  [  288/  306]
train() client id: f_00004-3-0 loss: 0.906124  [   32/  306]
train() client id: f_00004-3-1 loss: 1.058234  [   64/  306]
train() client id: f_00004-3-2 loss: 0.999109  [   96/  306]
train() client id: f_00004-3-3 loss: 0.864907  [  128/  306]
train() client id: f_00004-3-4 loss: 0.937406  [  160/  306]
train() client id: f_00004-3-5 loss: 0.966439  [  192/  306]
train() client id: f_00004-3-6 loss: 0.986466  [  224/  306]
train() client id: f_00004-3-7 loss: 0.837813  [  256/  306]
train() client id: f_00004-3-8 loss: 0.856185  [  288/  306]
train() client id: f_00004-4-0 loss: 0.875479  [   32/  306]
train() client id: f_00004-4-1 loss: 0.924213  [   64/  306]
train() client id: f_00004-4-2 loss: 0.807773  [   96/  306]
train() client id: f_00004-4-3 loss: 0.982510  [  128/  306]
train() client id: f_00004-4-4 loss: 0.997086  [  160/  306]
train() client id: f_00004-4-5 loss: 1.030788  [  192/  306]
train() client id: f_00004-4-6 loss: 0.840546  [  224/  306]
train() client id: f_00004-4-7 loss: 0.868505  [  256/  306]
train() client id: f_00004-4-8 loss: 0.947152  [  288/  306]
train() client id: f_00005-0-0 loss: 0.809368  [   32/  146]
train() client id: f_00005-0-1 loss: 0.916414  [   64/  146]
train() client id: f_00005-0-2 loss: 0.783710  [   96/  146]
train() client id: f_00005-0-3 loss: 0.725290  [  128/  146]
train() client id: f_00005-1-0 loss: 0.984337  [   32/  146]
train() client id: f_00005-1-1 loss: 0.971269  [   64/  146]
train() client id: f_00005-1-2 loss: 0.531988  [   96/  146]
train() client id: f_00005-1-3 loss: 0.776309  [  128/  146]
train() client id: f_00005-2-0 loss: 1.063131  [   32/  146]
train() client id: f_00005-2-1 loss: 0.891847  [   64/  146]
train() client id: f_00005-2-2 loss: 0.833847  [   96/  146]
train() client id: f_00005-2-3 loss: 0.573532  [  128/  146]
train() client id: f_00005-3-0 loss: 0.589642  [   32/  146]
train() client id: f_00005-3-1 loss: 0.896577  [   64/  146]
train() client id: f_00005-3-2 loss: 1.021885  [   96/  146]
train() client id: f_00005-3-3 loss: 0.805144  [  128/  146]
train() client id: f_00005-4-0 loss: 0.976464  [   32/  146]
train() client id: f_00005-4-1 loss: 0.761098  [   64/  146]
train() client id: f_00005-4-2 loss: 0.847665  [   96/  146]
train() client id: f_00005-4-3 loss: 0.745509  [  128/  146]
train() client id: f_00006-0-0 loss: 0.427826  [   32/   54]
train() client id: f_00006-1-0 loss: 0.419870  [   32/   54]
train() client id: f_00006-2-0 loss: 0.378047  [   32/   54]
train() client id: f_00006-3-0 loss: 0.445793  [   32/   54]
train() client id: f_00006-4-0 loss: 0.450507  [   32/   54]
train() client id: f_00007-0-0 loss: 0.808067  [   32/  179]
train() client id: f_00007-0-1 loss: 0.841797  [   64/  179]
train() client id: f_00007-0-2 loss: 0.652595  [   96/  179]
train() client id: f_00007-0-3 loss: 0.596282  [  128/  179]
train() client id: f_00007-0-4 loss: 0.642874  [  160/  179]
train() client id: f_00007-1-0 loss: 0.825300  [   32/  179]
train() client id: f_00007-1-1 loss: 0.649090  [   64/  179]
train() client id: f_00007-1-2 loss: 0.666605  [   96/  179]
train() client id: f_00007-1-3 loss: 0.623055  [  128/  179]
train() client id: f_00007-1-4 loss: 0.750530  [  160/  179]
train() client id: f_00007-2-0 loss: 0.648968  [   32/  179]
train() client id: f_00007-2-1 loss: 0.556376  [   64/  179]
train() client id: f_00007-2-2 loss: 0.710075  [   96/  179]
train() client id: f_00007-2-3 loss: 0.800815  [  128/  179]
train() client id: f_00007-2-4 loss: 0.656434  [  160/  179]
train() client id: f_00007-3-0 loss: 0.639617  [   32/  179]
train() client id: f_00007-3-1 loss: 0.590755  [   64/  179]
train() client id: f_00007-3-2 loss: 0.590379  [   96/  179]
train() client id: f_00007-3-3 loss: 0.625460  [  128/  179]
train() client id: f_00007-3-4 loss: 0.984161  [  160/  179]
train() client id: f_00007-4-0 loss: 0.538921  [   32/  179]
train() client id: f_00007-4-1 loss: 0.637489  [   64/  179]
train() client id: f_00007-4-2 loss: 0.657572  [   96/  179]
train() client id: f_00007-4-3 loss: 0.681986  [  128/  179]
train() client id: f_00007-4-4 loss: 0.829546  [  160/  179]
train() client id: f_00008-0-0 loss: 0.770602  [   32/  130]
train() client id: f_00008-0-1 loss: 0.697527  [   64/  130]
train() client id: f_00008-0-2 loss: 0.573761  [   96/  130]
train() client id: f_00008-0-3 loss: 0.837956  [  128/  130]
train() client id: f_00008-1-0 loss: 0.600244  [   32/  130]
train() client id: f_00008-1-1 loss: 0.795036  [   64/  130]
train() client id: f_00008-1-2 loss: 0.814211  [   96/  130]
train() client id: f_00008-1-3 loss: 0.638330  [  128/  130]
train() client id: f_00008-2-0 loss: 0.791771  [   32/  130]
train() client id: f_00008-2-1 loss: 0.737327  [   64/  130]
train() client id: f_00008-2-2 loss: 0.682033  [   96/  130]
train() client id: f_00008-2-3 loss: 0.664417  [  128/  130]
train() client id: f_00008-3-0 loss: 0.772403  [   32/  130]
train() client id: f_00008-3-1 loss: 0.570148  [   64/  130]
train() client id: f_00008-3-2 loss: 0.779403  [   96/  130]
train() client id: f_00008-3-3 loss: 0.755291  [  128/  130]
train() client id: f_00008-4-0 loss: 0.724263  [   32/  130]
train() client id: f_00008-4-1 loss: 0.685759  [   64/  130]
train() client id: f_00008-4-2 loss: 0.630497  [   96/  130]
train() client id: f_00008-4-3 loss: 0.790625  [  128/  130]
train() client id: f_00009-0-0 loss: 0.909943  [   32/  118]
train() client id: f_00009-0-1 loss: 0.962519  [   64/  118]
train() client id: f_00009-0-2 loss: 0.732979  [   96/  118]
train() client id: f_00009-1-0 loss: 0.901098  [   32/  118]
train() client id: f_00009-1-1 loss: 0.881281  [   64/  118]
train() client id: f_00009-1-2 loss: 0.821476  [   96/  118]
train() client id: f_00009-2-0 loss: 0.839454  [   32/  118]
train() client id: f_00009-2-1 loss: 0.740345  [   64/  118]
train() client id: f_00009-2-2 loss: 0.796038  [   96/  118]
train() client id: f_00009-3-0 loss: 0.808492  [   32/  118]
train() client id: f_00009-3-1 loss: 0.828883  [   64/  118]
train() client id: f_00009-3-2 loss: 0.961799  [   96/  118]
train() client id: f_00009-4-0 loss: 0.894045  [   32/  118]
train() client id: f_00009-4-1 loss: 0.865334  [   64/  118]
train() client id: f_00009-4-2 loss: 0.874579  [   96/  118]
At round 71 accuracy: 0.6472148541114059
At round 71 training accuracy: 0.5895372233400402
At round 71 training loss: 0.8344308372796841
update_location
xs = -4.528292 256.001589 265.045120 -265.943528 154.896481 -190.217951 -307.215960 328.375741 -1.680116 249.695607 
ys = 342.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -272.154970 4.001482 
xs mean: 48.442869159412865
ys mean: 9.371751218646875
dists_uav = 356.913176 275.279490 283.285473 285.016191 184.372835 214.920423 323.096597 343.799956 289.950255 269.005405 
uav_gains = -119.538623 -113.551236 -114.276375 -114.431522 -106.703576 -108.645756 -117.504282 -118.823536 -114.868988 -112.979919 
uav_gains_db_mean: -114.13238133621347
dists_bs = 239.052980 459.548486 473.079524 177.519875 373.743957 172.857030 221.832251 539.529514 479.569963 457.828409 
bs_gains = -106.164896 -114.112337 -114.465215 -102.545975 -111.599136 -102.222296 -105.255750 -116.063485 -114.630914 -114.066736 
bs_gains_db_mean: -110.11267405497861
Round 72
-------------------------------
ene_coms = [0.02618758 0.01469289 0.01490787 0.01511633 0.01197985 0.00694196
 0.02055684 0.02395942 0.0153918  0.01463409]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.44396311 2.89334687 1.38242306 0.51474754 3.32327129 1.58997165
 0.65202084 2.0042655  1.44847048 1.31712263]
obj_prev = 16.569602958654595
eta_min = 1.028367906884212e-64	eta_max = 0.9649917527340416
af = 3.436451538788952	bf = 0.5784627570442662	zeta = 3.7800966926678474	eta = 0.9090909090909091
af = 3.436451538788952	bf = 0.5784627570442662	zeta = 9.96590728204599	eta = 0.3448207415073856
af = 3.436451538788952	bf = 0.5784627570442662	zeta = 6.467478580228085	eta = 0.5313433196817423
af = 3.436451538788952	bf = 0.5784627570442662	zeta = 5.8710190142592085	eta = 0.5853245459506582
af = 3.436451538788952	bf = 0.5784627570442662	zeta = 5.833448087125147	eta = 0.5890943893669777
af = 3.436451538788952	bf = 0.5784627570442662	zeta = 5.833276109170588	eta = 0.5891117571798892
eta = 0.5891117571798892
ene_coms = [0.02618758 0.01469289 0.01490787 0.01511633 0.01197985 0.00694196
 0.02055684 0.02395942 0.0153918  0.01463409]
ene_comp = [0.04366331 0.09183157 0.04297024 0.01490097 0.10603947 0.050594
 0.01871285 0.06202963 0.04504945 0.04089104]
ene_total = [0.59828038 0.91239339 0.49573229 0.2571014  1.01084814 0.49280166
 0.33634906 0.73650539 0.51768576 0.47557864]
ti_comp = [2.82036207 2.935309   2.93315912 2.93107457 2.96243939 3.01281825
 2.8766695  2.84264368 2.92831989 2.93589701]
ti_coms = [0.26187579 0.14692885 0.14907874 0.15116328 0.11979846 0.06941961
 0.20556836 0.23959417 0.15391797 0.14634085]
t_total = [26.39969788 26.39969788 26.39969788 26.39969788 26.39969788 26.39969788
 26.39969788 26.39969788 26.39969788 26.39969788]
ene_coms = [0.02618758 0.01469289 0.01490787 0.01511633 0.01197985 0.00694196
 0.02055684 0.02395942 0.0153918  0.01463409]
ene_comp = [6.54064135e-07 5.61756879e-06 5.76384429e-07 2.40696658e-08
 8.49148725e-06 8.91725390e-07 4.94902643e-08 1.84600415e-06
 6.66363887e-07 4.95772941e-07]
ene_total = [0.22430503 0.12589426 0.12769248 0.12947318 0.1026814  0.05946628
 0.17607193 0.20523079 0.1318381  0.12534675]
optimize_network iter = 0 obj = 1.4080001927225694
eta = 0.5891117571798892
freqs = [ 7740728.26713772 15642572.96246707  7324908.50205495  2541895.71752854
 17897323.75524839  8396456.94103656  3252519.68394021 10910552.59739172
  7692030.13525189  6963977.0803761 ]
eta_min = 0.58911175717989	eta_max = 0.8476213469037024
af = 0.0001284501539355551	bf = 0.5784627570442662	zeta = 0.00014129516932911062	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02618758 0.01469289 0.01490787 0.01511633 0.01197985 0.00694196
 0.02055684 0.02395942 0.0153918  0.01463409]
ene_comp = [1.17812005e-07 1.01185343e-06 1.03820102e-07 4.33550081e-09
 1.52951229e-06 1.60620267e-07 8.91433568e-09 3.32507835e-07
 1.20027473e-07 8.93001176e-08]
ene_total = [1.01378652 0.56883486 0.57712242 0.58518832 0.46382687 0.26874565
 0.79580317 0.92753752 0.59585682 0.56652285]
ti_comp = [0.88117773 0.99612467 0.99397478 0.99189024 1.02325506 1.07363391
 0.93748516 0.90345935 0.98913555 0.99671267]
ti_coms = [0.26187579 0.14692885 0.14907874 0.15116328 0.11979846 0.06941961
 0.20556836 0.23959417 0.15391797 0.14634085]
t_total = [26.39969788 26.39969788 26.39969788 26.39969788 26.39969788 26.39969788
 26.39969788 26.39969788 26.39969788 26.39969788]
ene_coms = [0.02618758 0.01469289 0.01490787 0.01511633 0.01197985 0.00694196
 0.02055684 0.02395942 0.0153918  0.01463409]
ene_comp = [2.04352043e-07 1.48766316e-06 1.53076562e-07 6.41021374e-09
 2.17065527e-06 2.14161059e-07 1.42117268e-08 5.57360565e-07
 1.78120050e-07 1.31189912e-07]
ene_total = [0.60482694 0.33937782 0.34431232 0.34912335 0.27673388 0.16033487
 0.47477623 0.55337399 0.35548947 0.33798845]
optimize_network iter = 1 obj = 3.796337310136304
eta = 0.8476213469037024
freqs = [ 7740728.26713771 14401474.83587785  6753378.79949934  2346819.1859727
 16188712.76000503  7361586.62603525  3118201.29915052 10725540.95779744
  7114794.00328249  6408949.04123913]
eta_min = 0.8476213469037105	eta_max = 0.8476213469036944
af = 0.00010893319272291457	bf = 0.5784627570442662	zeta = 0.00011982651199520604	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02618758 0.01469289 0.01490787 0.01511633 0.01197985 0.00694196
 0.02055684 0.02395942 0.0153918  0.01463409]
ene_comp = [1.17812005e-07 8.57660030e-07 8.82509243e-08 3.69558395e-09
 1.25141518e-06 1.23467049e-07 8.19327276e-09 3.21326689e-07
 1.02688869e-07 7.56329432e-08]
ene_total = [1.01378652 0.5688289  0.57712182 0.58518829 0.4638161  0.26874421
 0.79580315 0.92753709 0.59585615 0.56652232]
ti_comp = [0.88117773 0.99612467 0.99397478 0.99189024 1.02325506 1.07363391
 0.93748516 0.90345935 0.98913555 0.99671267]
ti_coms = [0.26187579 0.14692885 0.14907874 0.15116328 0.11979846 0.06941961
 0.20556836 0.23959417 0.15391797 0.14634085]
t_total = [26.39969788 26.39969788 26.39969788 26.39969788 26.39969788 26.39969788
 26.39969788 26.39969788 26.39969788 26.39969788]
ene_coms = [0.02618758 0.01469289 0.01490787 0.01511633 0.01197985 0.00694196
 0.02055684 0.02395942 0.0153918  0.01463409]
ene_comp = [2.04352043e-07 1.48766316e-06 1.53076562e-07 6.41021374e-09
 2.17065527e-06 2.14161059e-07 1.42117268e-08 5.57360565e-07
 1.78120050e-07 1.31189912e-07]
ene_total = [0.60482694 0.33937782 0.34431232 0.34912335 0.27673388 0.16033487
 0.47477623 0.55337399 0.35548947 0.33798845]
optimize_network iter = 2 obj = 3.796337310136105
eta = 0.8476213469036944
freqs = [ 7740728.26713762 14401474.83587781  6753378.79949932  2346819.1859727
 16188712.76000501  7361586.62603526  3118201.2991505  10725540.95779734
  7114794.00328246  6408949.04123911]
Done!
ene_coms = [0.02618758 0.01469289 0.01490787 0.01511633 0.01197985 0.00694196
 0.02055684 0.02395942 0.0153918  0.01463409]
ene_comp = [1.88744453e-07 1.37404141e-06 1.41385188e-07 5.92062731e-09
 2.00486932e-06 1.97804295e-07 1.31262921e-08 5.14791599e-07
 1.64515955e-07 1.21170152e-07]
ene_total = [0.02618777 0.01469426 0.01490802 0.01511633 0.01198185 0.00694216
 0.02055685 0.02395993 0.01539196 0.01463421]
At round 72 energy consumption: 0.16437333630345796
At round 72 eta: 0.8476213469036944
At round 72 a_n: 3.5193018744635296
At round 72 local rounds: 5.413458240208035
At round 72 global rounds: 23.095767044477533
gradient difference: 0.7791508436203003
train() client id: f_00000-0-0 loss: 0.878841  [   32/  126]
train() client id: f_00000-0-1 loss: 0.816463  [   64/  126]
train() client id: f_00000-0-2 loss: 0.826785  [   96/  126]
train() client id: f_00000-1-0 loss: 0.831893  [   32/  126]
train() client id: f_00000-1-1 loss: 0.650977  [   64/  126]
train() client id: f_00000-1-2 loss: 0.802831  [   96/  126]
train() client id: f_00000-2-0 loss: 0.803388  [   32/  126]
train() client id: f_00000-2-1 loss: 0.763454  [   64/  126]
train() client id: f_00000-2-2 loss: 0.607146  [   96/  126]
train() client id: f_00000-3-0 loss: 0.716318  [   32/  126]
train() client id: f_00000-3-1 loss: 0.724660  [   64/  126]
train() client id: f_00000-3-2 loss: 0.773338  [   96/  126]
train() client id: f_00000-4-0 loss: 0.706241  [   32/  126]
train() client id: f_00000-4-1 loss: 0.830495  [   64/  126]
train() client id: f_00000-4-2 loss: 0.612461  [   96/  126]
train() client id: f_00001-0-0 loss: 0.602462  [   32/  265]
train() client id: f_00001-0-1 loss: 0.701114  [   64/  265]
train() client id: f_00001-0-2 loss: 0.545036  [   96/  265]
train() client id: f_00001-0-3 loss: 0.732162  [  128/  265]
train() client id: f_00001-0-4 loss: 0.624545  [  160/  265]
train() client id: f_00001-0-5 loss: 0.511694  [  192/  265]
train() client id: f_00001-0-6 loss: 0.499568  [  224/  265]
train() client id: f_00001-0-7 loss: 0.482407  [  256/  265]
train() client id: f_00001-1-0 loss: 0.485483  [   32/  265]
train() client id: f_00001-1-1 loss: 0.650170  [   64/  265]
train() client id: f_00001-1-2 loss: 0.581622  [   96/  265]
train() client id: f_00001-1-3 loss: 0.560330  [  128/  265]
train() client id: f_00001-1-4 loss: 0.527247  [  160/  265]
train() client id: f_00001-1-5 loss: 0.507407  [  192/  265]
train() client id: f_00001-1-6 loss: 0.577497  [  224/  265]
train() client id: f_00001-1-7 loss: 0.746834  [  256/  265]
train() client id: f_00001-2-0 loss: 0.519476  [   32/  265]
train() client id: f_00001-2-1 loss: 0.540563  [   64/  265]
train() client id: f_00001-2-2 loss: 0.602967  [   96/  265]
train() client id: f_00001-2-3 loss: 0.647579  [  128/  265]
train() client id: f_00001-2-4 loss: 0.581779  [  160/  265]
train() client id: f_00001-2-5 loss: 0.562451  [  192/  265]
train() client id: f_00001-2-6 loss: 0.686103  [  224/  265]
train() client id: f_00001-2-7 loss: 0.516090  [  256/  265]
train() client id: f_00001-3-0 loss: 0.587404  [   32/  265]
train() client id: f_00001-3-1 loss: 0.483067  [   64/  265]
train() client id: f_00001-3-2 loss: 0.528623  [   96/  265]
train() client id: f_00001-3-3 loss: 0.677430  [  128/  265]
train() client id: f_00001-3-4 loss: 0.695136  [  160/  265]
train() client id: f_00001-3-5 loss: 0.553753  [  192/  265]
train() client id: f_00001-3-6 loss: 0.547561  [  224/  265]
train() client id: f_00001-3-7 loss: 0.598546  [  256/  265]
train() client id: f_00001-4-0 loss: 0.650466  [   32/  265]
train() client id: f_00001-4-1 loss: 0.552512  [   64/  265]
train() client id: f_00001-4-2 loss: 0.516837  [   96/  265]
train() client id: f_00001-4-3 loss: 0.626397  [  128/  265]
train() client id: f_00001-4-4 loss: 0.564400  [  160/  265]
train() client id: f_00001-4-5 loss: 0.620053  [  192/  265]
train() client id: f_00001-4-6 loss: 0.533892  [  224/  265]
train() client id: f_00001-4-7 loss: 0.603137  [  256/  265]
train() client id: f_00002-0-0 loss: 0.936559  [   32/  124]
train() client id: f_00002-0-1 loss: 0.861977  [   64/  124]
train() client id: f_00002-0-2 loss: 0.991249  [   96/  124]
train() client id: f_00002-1-0 loss: 0.921788  [   32/  124]
train() client id: f_00002-1-1 loss: 0.892877  [   64/  124]
train() client id: f_00002-1-2 loss: 1.034563  [   96/  124]
train() client id: f_00002-2-0 loss: 0.961792  [   32/  124]
train() client id: f_00002-2-1 loss: 0.892200  [   64/  124]
train() client id: f_00002-2-2 loss: 0.915289  [   96/  124]
train() client id: f_00002-3-0 loss: 1.097597  [   32/  124]
train() client id: f_00002-3-1 loss: 0.796624  [   64/  124]
train() client id: f_00002-3-2 loss: 0.924200  [   96/  124]
train() client id: f_00002-4-0 loss: 0.886414  [   32/  124]
train() client id: f_00002-4-1 loss: 0.859508  [   64/  124]
train() client id: f_00002-4-2 loss: 1.048251  [   96/  124]
train() client id: f_00003-0-0 loss: 0.508207  [   32/   43]
train() client id: f_00003-1-0 loss: 0.676396  [   32/   43]
train() client id: f_00003-2-0 loss: 0.525985  [   32/   43]
train() client id: f_00003-3-0 loss: 0.510920  [   32/   43]
train() client id: f_00003-4-0 loss: 0.563737  [   32/   43]
train() client id: f_00004-0-0 loss: 0.909554  [   32/  306]
train() client id: f_00004-0-1 loss: 0.945061  [   64/  306]
train() client id: f_00004-0-2 loss: 0.798451  [   96/  306]
train() client id: f_00004-0-3 loss: 0.830519  [  128/  306]
train() client id: f_00004-0-4 loss: 0.921966  [  160/  306]
train() client id: f_00004-0-5 loss: 0.770989  [  192/  306]
train() client id: f_00004-0-6 loss: 0.839789  [  224/  306]
train() client id: f_00004-0-7 loss: 0.737346  [  256/  306]
train() client id: f_00004-0-8 loss: 0.794379  [  288/  306]
train() client id: f_00004-1-0 loss: 0.900496  [   32/  306]
train() client id: f_00004-1-1 loss: 0.777471  [   64/  306]
train() client id: f_00004-1-2 loss: 0.842131  [   96/  306]
train() client id: f_00004-1-3 loss: 0.892877  [  128/  306]
train() client id: f_00004-1-4 loss: 0.819203  [  160/  306]
train() client id: f_00004-1-5 loss: 0.728412  [  192/  306]
train() client id: f_00004-1-6 loss: 0.781250  [  224/  306]
train() client id: f_00004-1-7 loss: 0.828120  [  256/  306]
train() client id: f_00004-1-8 loss: 0.863535  [  288/  306]
train() client id: f_00004-2-0 loss: 0.786508  [   32/  306]
train() client id: f_00004-2-1 loss: 0.945695  [   64/  306]
train() client id: f_00004-2-2 loss: 0.845375  [   96/  306]
train() client id: f_00004-2-3 loss: 0.788266  [  128/  306]
train() client id: f_00004-2-4 loss: 0.856026  [  160/  306]
train() client id: f_00004-2-5 loss: 0.882109  [  192/  306]
train() client id: f_00004-2-6 loss: 0.824259  [  224/  306]
train() client id: f_00004-2-7 loss: 0.891445  [  256/  306]
train() client id: f_00004-2-8 loss: 0.729180  [  288/  306]
train() client id: f_00004-3-0 loss: 0.792004  [   32/  306]
train() client id: f_00004-3-1 loss: 0.767873  [   64/  306]
train() client id: f_00004-3-2 loss: 0.892700  [   96/  306]
train() client id: f_00004-3-3 loss: 0.930829  [  128/  306]
train() client id: f_00004-3-4 loss: 0.929886  [  160/  306]
train() client id: f_00004-3-5 loss: 0.843931  [  192/  306]
train() client id: f_00004-3-6 loss: 0.749152  [  224/  306]
train() client id: f_00004-3-7 loss: 0.828445  [  256/  306]
train() client id: f_00004-3-8 loss: 0.810325  [  288/  306]
train() client id: f_00004-4-0 loss: 0.681629  [   32/  306]
train() client id: f_00004-4-1 loss: 0.784017  [   64/  306]
train() client id: f_00004-4-2 loss: 0.808798  [   96/  306]
train() client id: f_00004-4-3 loss: 0.948464  [  128/  306]
train() client id: f_00004-4-4 loss: 0.782282  [  160/  306]
train() client id: f_00004-4-5 loss: 0.939964  [  192/  306]
train() client id: f_00004-4-6 loss: 0.917001  [  224/  306]
train() client id: f_00004-4-7 loss: 0.817328  [  256/  306]
train() client id: f_00004-4-8 loss: 0.741015  [  288/  306]
train() client id: f_00005-0-0 loss: 0.495078  [   32/  146]
train() client id: f_00005-0-1 loss: 0.737730  [   64/  146]
train() client id: f_00005-0-2 loss: 0.625971  [   96/  146]
train() client id: f_00005-0-3 loss: 0.862031  [  128/  146]
train() client id: f_00005-1-0 loss: 0.740252  [   32/  146]
train() client id: f_00005-1-1 loss: 0.782707  [   64/  146]
train() client id: f_00005-1-2 loss: 0.453414  [   96/  146]
train() client id: f_00005-1-3 loss: 0.517173  [  128/  146]
train() client id: f_00005-2-0 loss: 0.604551  [   32/  146]
train() client id: f_00005-2-1 loss: 0.580792  [   64/  146]
train() client id: f_00005-2-2 loss: 0.817424  [   96/  146]
train() client id: f_00005-2-3 loss: 0.607885  [  128/  146]
train() client id: f_00005-3-0 loss: 1.168297  [   32/  146]
train() client id: f_00005-3-1 loss: 0.435929  [   64/  146]
train() client id: f_00005-3-2 loss: 0.492968  [   96/  146]
train() client id: f_00005-3-3 loss: 0.500413  [  128/  146]
train() client id: f_00005-4-0 loss: 0.643888  [   32/  146]
train() client id: f_00005-4-1 loss: 0.923159  [   64/  146]
train() client id: f_00005-4-2 loss: 0.601028  [   96/  146]
train() client id: f_00005-4-3 loss: 0.490453  [  128/  146]
train() client id: f_00006-0-0 loss: 0.545893  [   32/   54]
train() client id: f_00006-1-0 loss: 0.558203  [   32/   54]
train() client id: f_00006-2-0 loss: 0.422614  [   32/   54]
train() client id: f_00006-3-0 loss: 0.427365  [   32/   54]
train() client id: f_00006-4-0 loss: 0.428041  [   32/   54]
train() client id: f_00007-0-0 loss: 0.398970  [   32/  179]
train() client id: f_00007-0-1 loss: 0.396475  [   64/  179]
train() client id: f_00007-0-2 loss: 0.750398  [   96/  179]
train() client id: f_00007-0-3 loss: 0.360162  [  128/  179]
train() client id: f_00007-0-4 loss: 0.565705  [  160/  179]
train() client id: f_00007-1-0 loss: 0.726083  [   32/  179]
train() client id: f_00007-1-1 loss: 0.529301  [   64/  179]
train() client id: f_00007-1-2 loss: 0.456763  [   96/  179]
train() client id: f_00007-1-3 loss: 0.491444  [  128/  179]
train() client id: f_00007-1-4 loss: 0.294175  [  160/  179]
train() client id: f_00007-2-0 loss: 0.508080  [   32/  179]
train() client id: f_00007-2-1 loss: 0.607729  [   64/  179]
train() client id: f_00007-2-2 loss: 0.337920  [   96/  179]
train() client id: f_00007-2-3 loss: 0.507639  [  128/  179]
train() client id: f_00007-2-4 loss: 0.358771  [  160/  179]
train() client id: f_00007-3-0 loss: 0.648025  [   32/  179]
train() client id: f_00007-3-1 loss: 0.335256  [   64/  179]
train() client id: f_00007-3-2 loss: 0.605300  [   96/  179]
train() client id: f_00007-3-3 loss: 0.395357  [  128/  179]
train() client id: f_00007-3-4 loss: 0.420359  [  160/  179]
train() client id: f_00007-4-0 loss: 0.477475  [   32/  179]
train() client id: f_00007-4-1 loss: 0.546191  [   64/  179]
train() client id: f_00007-4-2 loss: 0.406276  [   96/  179]
train() client id: f_00007-4-3 loss: 0.600121  [  128/  179]
train() client id: f_00007-4-4 loss: 0.475361  [  160/  179]
train() client id: f_00008-0-0 loss: 0.704781  [   32/  130]
train() client id: f_00008-0-1 loss: 0.716721  [   64/  130]
train() client id: f_00008-0-2 loss: 0.661299  [   96/  130]
train() client id: f_00008-0-3 loss: 0.576746  [  128/  130]
train() client id: f_00008-1-0 loss: 0.672277  [   32/  130]
train() client id: f_00008-1-1 loss: 0.662990  [   64/  130]
train() client id: f_00008-1-2 loss: 0.697191  [   96/  130]
train() client id: f_00008-1-3 loss: 0.621736  [  128/  130]
train() client id: f_00008-2-0 loss: 0.693580  [   32/  130]
train() client id: f_00008-2-1 loss: 0.583071  [   64/  130]
train() client id: f_00008-2-2 loss: 0.585497  [   96/  130]
train() client id: f_00008-2-3 loss: 0.762899  [  128/  130]
train() client id: f_00008-3-0 loss: 0.681978  [   32/  130]
train() client id: f_00008-3-1 loss: 0.673124  [   64/  130]
train() client id: f_00008-3-2 loss: 0.629465  [   96/  130]
train() client id: f_00008-3-3 loss: 0.691066  [  128/  130]
train() client id: f_00008-4-0 loss: 0.613517  [   32/  130]
train() client id: f_00008-4-1 loss: 0.571290  [   64/  130]
train() client id: f_00008-4-2 loss: 0.701590  [   96/  130]
train() client id: f_00008-4-3 loss: 0.787044  [  128/  130]
train() client id: f_00009-0-0 loss: 0.808622  [   32/  118]
train() client id: f_00009-0-1 loss: 0.761577  [   64/  118]
train() client id: f_00009-0-2 loss: 0.659180  [   96/  118]
train() client id: f_00009-1-0 loss: 0.686976  [   32/  118]
train() client id: f_00009-1-1 loss: 0.604086  [   64/  118]
train() client id: f_00009-1-2 loss: 0.896052  [   96/  118]
train() client id: f_00009-2-0 loss: 0.735544  [   32/  118]
train() client id: f_00009-2-1 loss: 0.759135  [   64/  118]
train() client id: f_00009-2-2 loss: 0.764266  [   96/  118]
train() client id: f_00009-3-0 loss: 0.679013  [   32/  118]
train() client id: f_00009-3-1 loss: 0.718888  [   64/  118]
train() client id: f_00009-3-2 loss: 0.857615  [   96/  118]
train() client id: f_00009-4-0 loss: 0.833755  [   32/  118]
train() client id: f_00009-4-1 loss: 0.753969  [   64/  118]
train() client id: f_00009-4-2 loss: 0.724060  [   96/  118]
At round 72 accuracy: 0.6472148541114059
At round 72 training accuracy: 0.5949027498323273
At round 72 training loss: 0.8133690747757426
update_location
xs = -4.528292 261.001589 270.045120 -270.943528 159.896481 -195.217951 -312.215960 333.375741 -1.680116 254.695607 
ys = 347.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -277.154970 4.001482 
xs mean: 49.442869159412865
ys mean: 9.371751218646875
dists_uav = 361.715213 279.935374 287.968940 289.687184 188.592966 219.358081 327.854496 348.578781 294.648435 273.652817 
uav_gains = -119.780160 -113.974139 -114.694270 -114.845870 -106.967987 -108.946103 -117.829779 -119.094061 -115.277292 -113.403078 
uav_gains_db_mean: -114.48127391980668
dists_bs = 242.584433 464.241130 477.733909 180.132566 378.164660 173.368775 224.848187 544.197441 484.235376 462.470332 
bs_gains = -106.343221 -114.235880 -114.584269 -102.723642 -111.742125 -102.258244 -105.419962 -116.168241 -114.748642 -114.189408 
bs_gains_db_mean: -110.24136338884901
Round 73
-------------------------------
ene_coms = [0.02701231 0.01485429 0.01548116 0.01569868 0.01210939 0.00695336
 0.02132141 0.02476674 0.01555857 0.01479321]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.30606359 2.61224532 1.24970661 0.46651404 3.00022146 1.43525077
 0.59101078 1.81177397 1.30802098 1.18943314]
obj_prev = 14.97024066352226
eta_min = 1.4064178468228431e-71	eta_max = 0.9673510288054504
af = 3.101969802094253	bf = 0.5354394064472002	zeta = 3.4121667823036788	eta = 0.9090909090909091
af = 3.101969802094253	bf = 0.5354394064472002	zeta = 9.141975766356284	eta = 0.3393106568396215
af = 3.101969802094253	bf = 0.5354394064472002	zeta = 5.885019947846451	eta = 0.5270958857547084
af = 3.101969802094253	bf = 0.5354394064472002	zeta = 5.332701535017204	eta = 0.5816882459528547
af = 3.101969802094253	bf = 0.5354394064472002	zeta = 5.297844566447641	eta = 0.5855154418345297
af = 3.101969802094253	bf = 0.5354394064472002	zeta = 5.29768399010512	eta = 0.5855331891989092
eta = 0.5855331891989092
ene_coms = [0.02701231 0.01485429 0.01548116 0.01569868 0.01210939 0.00695336
 0.02132141 0.02476674 0.01555857 0.01479321]
ene_comp = [0.0441661  0.09288901 0.04346505 0.01507256 0.10726052 0.05117659
 0.01892833 0.0627439  0.04556819 0.0413619 ]
ene_total = [0.54555976 0.82581808 0.45180388 0.2358517  0.91493231 0.44554751
 0.30850139 0.67074116 0.46851716 0.43041104]
ti_comp = [3.16768375 3.28926389 3.28299523 3.28082003 3.31671291 3.36827323
 3.22459273 3.1901394  3.28222113 3.28987468]
ti_coms = [0.27012306 0.14854292 0.15481158 0.15698678 0.1210939  0.06953358
 0.21321408 0.24766741 0.15558568 0.14793213]
t_total = [26.34969368 26.34969368 26.34969368 26.34969368 26.34969368 26.34969368
 26.34969368 26.34969368 26.34969368 26.34969368]
ene_coms = [0.02701231 0.01485429 0.01548116 0.01569868 0.01210939 0.00695336
 0.02132141 0.02476674 0.01555857 0.01479321]
ene_comp = [5.36615747e-07 4.62994250e-06 4.76167736e-07 1.98827276e-08
 7.01106397e-06 7.38379027e-07 4.07630761e-08 1.51696368e-06
 5.48946216e-07 4.08622922e-07]
ene_total = [0.20704483 0.1138889  0.1186618  0.12032551 0.09286836 0.05330093
 0.16342211 0.1898408  0.11925568 0.1133884 ]
optimize_network iter = 0 obj = 1.2919973078416278
eta = 0.5855331891989092
freqs = [ 6971354.97301668 14120030.18817276  6619724.31656956  2297071.51548715
 16169701.9550031   7596858.04934221  2934994.94822892  9834037.02031022
  6941670.33797435  6286242.3782323 ]
eta_min = 0.5855331891989096	eta_max = 0.8589284933098161
af = 9.453332113154263e-05	bf = 0.5354394064472002	zeta = 0.0001039866532446969	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02701231 0.01485429 0.01548116 0.01569868 0.01210939 0.00695336
 0.02132141 0.02476674 0.01555857 0.01479321]
ene_comp = [9.55565137e-08 8.24465489e-07 8.47923846e-08 3.54056723e-09
 1.24847777e-06 1.31485008e-07 7.25878333e-09 2.70129532e-07
 9.77522313e-08 7.27645099e-08]
ene_total = [0.94392989 0.51910187 0.54098146 0.54857968 0.42319799 0.24298494
 0.74506227 0.86546626 0.54368695 0.51694127]
ti_comp = [0.89999859 1.02157873 1.01531006 1.01313487 1.04902774 1.10058806
 0.95690757 0.92245424 1.01453596 1.02218951]
ti_coms = [0.27012306 0.14854292 0.15481158 0.15698678 0.1210939  0.06953358
 0.21321408 0.24766741 0.15558568 0.14793213]
t_total = [26.34969368 26.34969368 26.34969368 26.34969368 26.34969368 26.34969368
 26.34969368 26.34969368 26.34969368 26.34969368]
ene_coms = [0.02701231 0.01485429 0.01548116 0.01569868 0.01210939 0.00695336
 0.02132141 0.02476674 0.01555857 0.01479321]
ene_comp = [1.52462636e-07 1.10085314e-06 1.14183440e-07 4.78196383e-09
 1.60740672e-06 1.58615059e-07 1.06163871e-08 4.16106862e-07
 1.31774268e-07 9.70775096e-08]
ene_total = [0.60828718 0.33452508 0.34861913 0.35351493 0.27272471 0.15658461
 0.480132   0.55772577 0.3503627  0.33312707]
optimize_network iter = 1 obj = 3.795603174834432
eta = 0.8589284933098161
freqs = [ 6971354.97301669 12917025.37278439  6081510.71637528  2113438.78503992
 14525227.18600577  6605664.89139716  2810038.37525311  9662659.61609253
  6380642.1343453   5748295.25675124]
eta_min = 0.8589284933098114	eta_max = 0.8589284933098127
af = 7.925465955182195e-05	bf = 0.5354394064472002	zeta = 8.718012550700415e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02701231 0.01485429 0.01548116 0.01569868 0.01210939 0.00695336
 0.02132141 0.02476674 0.01555857 0.01479321]
ene_comp = [9.55565137e-08 6.89963722e-07 7.15648878e-08 2.99711329e-09
 1.00744803e-06 9.94125671e-08 6.65385938e-09 2.60796495e-07
 8.25900034e-08 6.08436835e-08]
ene_total = [0.94392989 0.51909717 0.540981   0.54857966 0.42318957 0.24298382
 0.74506225 0.86546593 0.54368642 0.51694085]
ti_comp = [0.89999859 1.02157873 1.01531006 1.01313487 1.04902774 1.10058806
 0.95690757 0.92245424 1.01453596 1.02218951]
ti_coms = [0.27012306 0.14854292 0.15481158 0.15698678 0.1210939  0.06953358
 0.21321408 0.24766741 0.15558568 0.14793213]
t_total = [26.34969368 26.34969368 26.34969368 26.34969368 26.34969368 26.34969368
 26.34969368 26.34969368 26.34969368 26.34969368]
ene_coms = [0.02701231 0.01485429 0.01548116 0.01569868 0.01210939 0.00695336
 0.02132141 0.02476674 0.01555857 0.01479321]
ene_comp = [1.52462636e-07 1.10085314e-06 1.14183440e-07 4.78196383e-09
 1.60740672e-06 1.58615059e-07 1.06163871e-08 4.16106862e-07
 1.31774268e-07 9.70775096e-08]
ene_total = [0.60828718 0.33452508 0.34861913 0.35351493 0.27272471 0.15658461
 0.480132   0.55772577 0.3503627  0.33312707]
optimize_network iter = 2 obj = 3.7956031748343397
eta = 0.8589284933098127
freqs = [ 6971354.97301666 12917025.37278437  6081510.71637527  2113438.78503992
 14525227.18600576  6605664.89139717  2810038.3752531   9662659.61609249
  6380642.13434529  5748295.25675123]
Done!
ene_coms = [0.02701231 0.01485429 0.01548116 0.01569868 0.01210939 0.00695336
 0.02132141 0.02476674 0.01555857 0.01479321]
ene_comp = [1.22471471e-07 8.84302586e-07 9.17222360e-08 3.84129621e-09
 1.29121122e-06 1.27413641e-07 8.52802092e-09 3.34253827e-07
 1.05852745e-07 7.79812401e-08]
ene_total = [0.02701243 0.01485518 0.01548125 0.01569868 0.01211068 0.00695349
 0.02132142 0.02476707 0.01555867 0.01479329]
At round 73 energy consumption: 0.16855215925480538
At round 73 eta: 0.8589284933098127
At round 73 a_n: 3.176756027494214
At round 73 local rounds: 4.979531459948522
At round 73 global rounds: 22.518764433917994
gradient difference: 0.6985564827919006
train() client id: f_00000-0-0 loss: 1.043303  [   32/  126]
train() client id: f_00000-0-1 loss: 1.088945  [   64/  126]
train() client id: f_00000-0-2 loss: 1.139760  [   96/  126]
train() client id: f_00000-1-0 loss: 1.021819  [   32/  126]
train() client id: f_00000-1-1 loss: 1.118361  [   64/  126]
train() client id: f_00000-1-2 loss: 1.121583  [   96/  126]
train() client id: f_00000-2-0 loss: 0.995017  [   32/  126]
train() client id: f_00000-2-1 loss: 0.988828  [   64/  126]
train() client id: f_00000-2-2 loss: 1.049337  [   96/  126]
train() client id: f_00000-3-0 loss: 0.965909  [   32/  126]
train() client id: f_00000-3-1 loss: 1.032610  [   64/  126]
train() client id: f_00000-3-2 loss: 0.953856  [   96/  126]
train() client id: f_00001-0-0 loss: 0.547273  [   32/  265]
train() client id: f_00001-0-1 loss: 0.571479  [   64/  265]
train() client id: f_00001-0-2 loss: 0.366052  [   96/  265]
train() client id: f_00001-0-3 loss: 0.429872  [  128/  265]
train() client id: f_00001-0-4 loss: 0.460646  [  160/  265]
train() client id: f_00001-0-5 loss: 0.381992  [  192/  265]
train() client id: f_00001-0-6 loss: 0.502011  [  224/  265]
train() client id: f_00001-0-7 loss: 0.472132  [  256/  265]
train() client id: f_00001-1-0 loss: 0.484008  [   32/  265]
train() client id: f_00001-1-1 loss: 0.400070  [   64/  265]
train() client id: f_00001-1-2 loss: 0.501192  [   96/  265]
train() client id: f_00001-1-3 loss: 0.608812  [  128/  265]
train() client id: f_00001-1-4 loss: 0.586927  [  160/  265]
train() client id: f_00001-1-5 loss: 0.348152  [  192/  265]
train() client id: f_00001-1-6 loss: 0.372985  [  224/  265]
train() client id: f_00001-1-7 loss: 0.388161  [  256/  265]
train() client id: f_00001-2-0 loss: 0.593437  [   32/  265]
train() client id: f_00001-2-1 loss: 0.395755  [   64/  265]
train() client id: f_00001-2-2 loss: 0.531176  [   96/  265]
train() client id: f_00001-2-3 loss: 0.432226  [  128/  265]
train() client id: f_00001-2-4 loss: 0.367684  [  160/  265]
train() client id: f_00001-2-5 loss: 0.418483  [  192/  265]
train() client id: f_00001-2-6 loss: 0.404361  [  224/  265]
train() client id: f_00001-2-7 loss: 0.490387  [  256/  265]
train() client id: f_00001-3-0 loss: 0.433126  [   32/  265]
train() client id: f_00001-3-1 loss: 0.508225  [   64/  265]
train() client id: f_00001-3-2 loss: 0.437446  [   96/  265]
train() client id: f_00001-3-3 loss: 0.427939  [  128/  265]
train() client id: f_00001-3-4 loss: 0.388312  [  160/  265]
train() client id: f_00001-3-5 loss: 0.448221  [  192/  265]
train() client id: f_00001-3-6 loss: 0.449254  [  224/  265]
train() client id: f_00001-3-7 loss: 0.528600  [  256/  265]
train() client id: f_00002-0-0 loss: 0.549942  [   32/  124]
train() client id: f_00002-0-1 loss: 0.925680  [   64/  124]
train() client id: f_00002-0-2 loss: 0.857259  [   96/  124]
train() client id: f_00002-1-0 loss: 0.923017  [   32/  124]
train() client id: f_00002-1-1 loss: 0.787295  [   64/  124]
train() client id: f_00002-1-2 loss: 0.780774  [   96/  124]
train() client id: f_00002-2-0 loss: 0.768959  [   32/  124]
train() client id: f_00002-2-1 loss: 0.809558  [   64/  124]
train() client id: f_00002-2-2 loss: 0.916176  [   96/  124]
train() client id: f_00002-3-0 loss: 0.652614  [   32/  124]
train() client id: f_00002-3-1 loss: 0.858905  [   64/  124]
train() client id: f_00002-3-2 loss: 0.721531  [   96/  124]
train() client id: f_00003-0-0 loss: 0.543172  [   32/   43]
train() client id: f_00003-1-0 loss: 0.796935  [   32/   43]
train() client id: f_00003-2-0 loss: 0.501651  [   32/   43]
train() client id: f_00003-3-0 loss: 0.680624  [   32/   43]
train() client id: f_00004-0-0 loss: 0.947929  [   32/  306]
train() client id: f_00004-0-1 loss: 0.807868  [   64/  306]
train() client id: f_00004-0-2 loss: 0.825435  [   96/  306]
train() client id: f_00004-0-3 loss: 0.973184  [  128/  306]
train() client id: f_00004-0-4 loss: 0.969098  [  160/  306]
train() client id: f_00004-0-5 loss: 1.035018  [  192/  306]
train() client id: f_00004-0-6 loss: 0.974945  [  224/  306]
train() client id: f_00004-0-7 loss: 0.935404  [  256/  306]
train() client id: f_00004-0-8 loss: 0.856851  [  288/  306]
train() client id: f_00004-1-0 loss: 0.915630  [   32/  306]
train() client id: f_00004-1-1 loss: 0.899132  [   64/  306]
train() client id: f_00004-1-2 loss: 0.954795  [   96/  306]
train() client id: f_00004-1-3 loss: 0.823379  [  128/  306]
train() client id: f_00004-1-4 loss: 0.974910  [  160/  306]
train() client id: f_00004-1-5 loss: 0.854537  [  192/  306]
train() client id: f_00004-1-6 loss: 1.090760  [  224/  306]
train() client id: f_00004-1-7 loss: 0.941326  [  256/  306]
train() client id: f_00004-1-8 loss: 0.886959  [  288/  306]
train() client id: f_00004-2-0 loss: 0.928376  [   32/  306]
train() client id: f_00004-2-1 loss: 0.910686  [   64/  306]
train() client id: f_00004-2-2 loss: 0.983451  [   96/  306]
train() client id: f_00004-2-3 loss: 0.900549  [  128/  306]
train() client id: f_00004-2-4 loss: 1.020556  [  160/  306]
train() client id: f_00004-2-5 loss: 0.813782  [  192/  306]
train() client id: f_00004-2-6 loss: 1.068001  [  224/  306]
train() client id: f_00004-2-7 loss: 0.869181  [  256/  306]
train() client id: f_00004-2-8 loss: 0.772839  [  288/  306]
train() client id: f_00004-3-0 loss: 0.860758  [   32/  306]
train() client id: f_00004-3-1 loss: 0.912071  [   64/  306]
train() client id: f_00004-3-2 loss: 0.871996  [   96/  306]
train() client id: f_00004-3-3 loss: 0.955233  [  128/  306]
train() client id: f_00004-3-4 loss: 0.945088  [  160/  306]
train() client id: f_00004-3-5 loss: 0.982215  [  192/  306]
train() client id: f_00004-3-6 loss: 0.881544  [  224/  306]
train() client id: f_00004-3-7 loss: 0.813874  [  256/  306]
train() client id: f_00004-3-8 loss: 1.037395  [  288/  306]
train() client id: f_00005-0-0 loss: 0.571545  [   32/  146]
train() client id: f_00005-0-1 loss: 0.924033  [   64/  146]
train() client id: f_00005-0-2 loss: 0.851658  [   96/  146]
train() client id: f_00005-0-3 loss: 0.866926  [  128/  146]
train() client id: f_00005-1-0 loss: 0.811373  [   32/  146]
train() client id: f_00005-1-1 loss: 0.695283  [   64/  146]
train() client id: f_00005-1-2 loss: 0.662828  [   96/  146]
train() client id: f_00005-1-3 loss: 0.848216  [  128/  146]
train() client id: f_00005-2-0 loss: 0.800694  [   32/  146]
train() client id: f_00005-2-1 loss: 0.896263  [   64/  146]
train() client id: f_00005-2-2 loss: 0.601620  [   96/  146]
train() client id: f_00005-2-3 loss: 0.708887  [  128/  146]
train() client id: f_00005-3-0 loss: 0.734694  [   32/  146]
train() client id: f_00005-3-1 loss: 0.813429  [   64/  146]
train() client id: f_00005-3-2 loss: 0.645962  [   96/  146]
train() client id: f_00005-3-3 loss: 0.690593  [  128/  146]
train() client id: f_00006-0-0 loss: 0.602079  [   32/   54]
train() client id: f_00006-1-0 loss: 0.588334  [   32/   54]
train() client id: f_00006-2-0 loss: 0.578331  [   32/   54]
train() client id: f_00006-3-0 loss: 0.605131  [   32/   54]
train() client id: f_00007-0-0 loss: 0.899148  [   32/  179]
train() client id: f_00007-0-1 loss: 0.592358  [   64/  179]
train() client id: f_00007-0-2 loss: 0.683549  [   96/  179]
train() client id: f_00007-0-3 loss: 0.600155  [  128/  179]
train() client id: f_00007-0-4 loss: 0.669032  [  160/  179]
train() client id: f_00007-1-0 loss: 0.714954  [   32/  179]
train() client id: f_00007-1-1 loss: 0.602971  [   64/  179]
train() client id: f_00007-1-2 loss: 0.862273  [   96/  179]
train() client id: f_00007-1-3 loss: 0.550948  [  128/  179]
train() client id: f_00007-1-4 loss: 0.718602  [  160/  179]
train() client id: f_00007-2-0 loss: 0.639616  [   32/  179]
train() client id: f_00007-2-1 loss: 0.753315  [   64/  179]
train() client id: f_00007-2-2 loss: 0.709422  [   96/  179]
train() client id: f_00007-2-3 loss: 0.653349  [  128/  179]
train() client id: f_00007-2-4 loss: 0.497283  [  160/  179]
train() client id: f_00007-3-0 loss: 0.923681  [   32/  179]
train() client id: f_00007-3-1 loss: 0.509041  [   64/  179]
train() client id: f_00007-3-2 loss: 0.553800  [   96/  179]
train() client id: f_00007-3-3 loss: 0.776694  [  128/  179]
train() client id: f_00007-3-4 loss: 0.660475  [  160/  179]
train() client id: f_00008-0-0 loss: 0.560538  [   32/  130]
train() client id: f_00008-0-1 loss: 0.485599  [   64/  130]
train() client id: f_00008-0-2 loss: 0.638175  [   96/  130]
train() client id: f_00008-0-3 loss: 0.740360  [  128/  130]
train() client id: f_00008-1-0 loss: 0.793391  [   32/  130]
train() client id: f_00008-1-1 loss: 0.538684  [   64/  130]
train() client id: f_00008-1-2 loss: 0.561689  [   96/  130]
train() client id: f_00008-1-3 loss: 0.571493  [  128/  130]
train() client id: f_00008-2-0 loss: 0.633734  [   32/  130]
train() client id: f_00008-2-1 loss: 0.564969  [   64/  130]
train() client id: f_00008-2-2 loss: 0.685299  [   96/  130]
train() client id: f_00008-2-3 loss: 0.599235  [  128/  130]
train() client id: f_00008-3-0 loss: 0.595187  [   32/  130]
train() client id: f_00008-3-1 loss: 0.639471  [   64/  130]
train() client id: f_00008-3-2 loss: 0.609327  [   96/  130]
train() client id: f_00008-3-3 loss: 0.633990  [  128/  130]
train() client id: f_00009-0-0 loss: 0.888429  [   32/  118]
train() client id: f_00009-0-1 loss: 0.919610  [   64/  118]
train() client id: f_00009-0-2 loss: 0.907436  [   96/  118]
train() client id: f_00009-1-0 loss: 0.927902  [   32/  118]
train() client id: f_00009-1-1 loss: 0.844198  [   64/  118]
train() client id: f_00009-1-2 loss: 0.861298  [   96/  118]
train() client id: f_00009-2-0 loss: 0.821544  [   32/  118]
train() client id: f_00009-2-1 loss: 0.924468  [   64/  118]
train() client id: f_00009-2-2 loss: 0.851136  [   96/  118]
train() client id: f_00009-3-0 loss: 0.912791  [   32/  118]
train() client id: f_00009-3-1 loss: 0.702465  [   64/  118]
train() client id: f_00009-3-2 loss: 0.922814  [   96/  118]
At round 73 accuracy: 0.6472148541114059
At round 73 training accuracy: 0.5935613682092555
At round 73 training loss: 0.800532800934854
update_location
xs = -4.528292 266.001589 275.045120 -275.943528 164.896481 -200.217951 -317.215960 338.375741 -1.680116 259.695607 
ys = 352.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -282.154970 4.001482 
xs mean: 50.442869159412865
ys mean: 9.371751218646875
dists_uav = 366.522543 284.602933 292.662881 294.368986 192.850387 223.819452 332.619498 353.363728 299.356393 278.312451 
uav_gains = -120.012091 -114.394547 -115.105833 -115.253263 -107.233958 -109.256789 -118.142110 -119.353367 -115.676676 -113.827011 
uav_gains_db_mean: -114.82556442794373
dists_bs = 246.166786 468.940127 482.395210 182.844679 382.599627 174.022733 227.933909 548.871216 488.907403 467.119647 
bs_gains = -106.521484 -114.358346 -114.702343 -102.905364 -111.883906 -102.304027 -105.585709 -116.272232 -114.865404 -114.311047 
bs_gains_db_mean: -110.37098628474928
Round 74
-------------------------------
ene_coms = [0.02784171 0.01501738 0.0160842  0.01631015 0.01224039 0.00696793
 0.02209886 0.02558069 0.01572709 0.01495402]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.16760674 2.33103689 1.11667864 0.41796093 2.67708616 1.28053109
 0.52950852 1.61874273 1.16746107 1.06163834]
obj_prev = 13.368251099525216
eta_min = 4.1908505550894265e-80	eta_max = 0.9699282501169969
af = 2.7674880653995517	bf = 0.48981505967408623	zeta = 3.044236871939507	eta = 0.9090909090909091
af = 2.7674880653995517	bf = 0.48981505967408623	zeta = 8.289433292730047	eta = 0.3338573298884833
af = 2.7674880653995517	bf = 0.48981505967408623	zeta = 5.292956852167827	eta = 0.5228623891513637
af = 2.7674880653995517	bf = 0.48981505967408623	zeta = 4.787626414843593	eta = 0.578050128727507
af = 2.7674880653995517	bf = 0.48981505967408623	zeta = 4.755684736439819	eta = 0.5819326172304974
af = 2.7674880653995517	bf = 0.48981505967408623	zeta = 4.755536750149818	eta = 0.5819507262376565
eta = 0.5819507262376565
ene_coms = [0.02784171 0.01501738 0.0160842  0.01631015 0.01224039 0.00696793
 0.02209886 0.02558069 0.01572709 0.01495402]
ene_comp = [0.04467251 0.0939541  0.04396343 0.01524538 0.10849039 0.05176339
 0.01914536 0.06346333 0.04609069 0.04183616]
ene_total = [0.49161798 0.73878388 0.40709938 0.21393411 0.81850736 0.39817529
 0.27961969 0.60368357 0.41910031 0.38501519]
ti_comp = [3.6008175  3.72906077 3.71839254 3.71613309 3.75683066 3.8095553
 3.65824592 3.62342766 3.72196365 3.72969435]
ti_coms = [0.27841706 0.15017379 0.16084202 0.16310147 0.12240391 0.06967927
 0.22098864 0.2558069  0.15727091 0.14954021]
t_total = [26.29968948 26.29968948 26.29968948 26.29968948 26.29968948 26.29968948
 26.29968948 26.29968948 26.29968948 26.29968948]
ene_coms = [0.02784171 0.01501738 0.0160842  0.01631015 0.01224039 0.00696793
 0.02209886 0.02558069 0.01572709 0.01495402]
ene_comp = [4.29733250e-07 3.72759092e-06 3.84099843e-07 1.60366035e-08
 5.65471323e-06 5.97309673e-07 3.27736664e-08 1.21677267e-06
 4.41748768e-07 3.28994836e-07]
ene_total = [0.18875877 0.10183722 0.10904719 0.11057651 0.08302339 0.04724386
 0.14982186 0.1734353  0.10662651 0.10138463]
optimize_network iter = 0 obj = 1.1717552276555596
eta = 0.5819507262376565
freqs = [ 6203107.11961344 12597554.9080264   5911617.0998665   2051242.67208801
 14439084.36725762  6793888.58708225  2616740.81897527  8757361.60037492
  6191716.62179476  5608524.37065159]
eta_min = 0.5819507262376569	eta_max = 0.870992206754704
af = 6.716720226619722e-05	bf = 0.48981505967408623	zeta = 7.388392249281694e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02784171 0.01501738 0.0160842  0.01631015 0.01224039 0.00696793
 0.02209886 0.02558069 0.01572709 0.01495402]
ene_comp = [7.56561895e-08 6.56256701e-07 6.76222529e-08 2.82330565e-09
 9.95533988e-07 1.05158663e-07 5.76992988e-09 2.14217503e-07
 7.77715676e-08 5.79208047e-08]
ene_total = [0.86800407 0.46820697 0.50144822 0.50849032 0.38164127 0.21723753
 0.68896125 0.79751826 0.49031513 0.46621304]
ti_comp = [0.91869419 1.04693746 1.03626923 1.03400978 1.07470735 1.12743199
 0.97612262 0.94130435 1.03984034 1.04757104]
ti_coms = [0.27841706 0.15017379 0.16084202 0.16310147 0.12240391 0.06967927
 0.22098864 0.2558069  0.15727091 0.14954021]
t_total = [26.29968948 26.29968948 26.29968948 26.29968948 26.29968948 26.29968948
 26.29968948 26.29968948 26.29968948 26.29968948]
ene_coms = [0.02784171 0.01501738 0.0160842  0.01631015 0.01224039 0.00696793
 0.02209886 0.02558069 0.01572709 0.01495402]
ene_comp = [1.09639947e-07 7.85407907e-07 8.21332436e-08 3.43997429e-09
 1.14758000e-06 1.13260112e-07 7.64489730e-09 2.99431915e-07
 9.39930751e-08 6.92593130e-08]
ene_total = [0.61166504 0.32993848 0.35336038 0.3583225  0.26893796 0.15308292
 0.48549661 0.5619963  0.34551516 0.32853082]
optimize_network iter = 1 obj = 3.7968461682515335
eta = 0.870992206754704
freqs = [ 6203107.11961344 11448137.62916808  5412012.47253767  1880847.20062415
 12877777.45185861  5856959.35681568  2502067.90516497  8600677.68373971
  5654398.30152087  5094578.1681318 ]
eta_min = 0.8709922067547137	eta_max = 0.8709922067547013
af = 5.564479236312803e-05	bf = 0.48981505967408623	zeta = 6.120927159944084e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02784171 0.01501738 0.0160842  0.01631015 0.01224039 0.00696793
 0.02209886 0.02558069 0.01572709 0.01495402]
ene_comp = [7.56561895e-08 5.41964593e-07 5.66754034e-08 2.37372740e-09
 7.91878615e-07 7.81542561e-08 5.27530168e-09 2.06620655e-07
 6.48591875e-08 4.77918481e-08]
ene_total = [0.86800407 0.4682034  0.50144787 0.50849031 0.38163493 0.21723669
 0.68896124 0.79751803 0.49031473 0.46621272]
ti_comp = [0.91869419 1.04693746 1.03626923 1.03400978 1.07470735 1.12743199
 0.97612262 0.94130435 1.03984034 1.04757104]
ti_coms = [0.27841706 0.15017379 0.16084202 0.16310147 0.12240391 0.06967927
 0.22098864 0.2558069  0.15727091 0.14954021]
t_total = [26.29968948 26.29968948 26.29968948 26.29968948 26.29968948 26.29968948
 26.29968948 26.29968948 26.29968948 26.29968948]
ene_coms = [0.02784171 0.01501738 0.0160842  0.01631015 0.01224039 0.00696793
 0.02209886 0.02558069 0.01572709 0.01495402]
ene_comp = [1.09639947e-07 7.85407907e-07 8.21332436e-08 3.43997429e-09
 1.14758000e-06 1.13260112e-07 7.64489730e-09 2.99431915e-07
 9.39930751e-08 6.92593130e-08]
ene_total = [0.61166504 0.32993848 0.35336038 0.3583225  0.26893796 0.15308292
 0.48549661 0.5619963  0.34551516 0.32853082]
optimize_network iter = 2 obj = 3.7968461682514545
eta = 0.8709922067547013
freqs = [ 6203107.11961341 11448137.62916807  5412012.47253766  1880847.20062415
 12877777.4518586   5856959.35681569  2502067.90516496  8600677.68373967
  5654398.30152086  5094578.16813179]
Done!
ene_coms = [0.02784171 0.01501738 0.0160842  0.01631015 0.01224039 0.00696793
 0.02209886 0.02558069 0.01572709 0.01495402]
ene_comp = [9.69659156e-08 6.94617232e-07 7.26388999e-08 3.04232413e-09
 1.01492337e-06 1.00167601e-07 6.76117131e-09 2.64818531e-07
 8.31277724e-08 6.12531550e-08]
ene_total = [0.0278418  0.01501807 0.01608427 0.01631015 0.01224141 0.00696803
 0.02209887 0.02558096 0.01572717 0.01495408]
At round 74 energy consumption: 0.1728248174293548
At round 74 eta: 0.8709922067547013
At round 74 a_n: 2.834210180524895
At round 74 local rounds: 4.522824198345825
At round 74 global rounds: 21.969294328877147
gradient difference: 0.779352605342865
train() client id: f_00000-0-0 loss: 1.355804  [   32/  126]
train() client id: f_00000-0-1 loss: 0.848518  [   64/  126]
train() client id: f_00000-0-2 loss: 0.972003  [   96/  126]
train() client id: f_00000-1-0 loss: 1.115864  [   32/  126]
train() client id: f_00000-1-1 loss: 0.968419  [   64/  126]
train() client id: f_00000-1-2 loss: 0.826260  [   96/  126]
train() client id: f_00000-2-0 loss: 0.964987  [   32/  126]
train() client id: f_00000-2-1 loss: 0.829448  [   64/  126]
train() client id: f_00000-2-2 loss: 1.014653  [   96/  126]
train() client id: f_00000-3-0 loss: 0.936346  [   32/  126]
train() client id: f_00000-3-1 loss: 0.957407  [   64/  126]
train() client id: f_00000-3-2 loss: 1.046119  [   96/  126]
train() client id: f_00001-0-0 loss: 0.436518  [   32/  265]
train() client id: f_00001-0-1 loss: 0.393687  [   64/  265]
train() client id: f_00001-0-2 loss: 0.541822  [   96/  265]
train() client id: f_00001-0-3 loss: 0.460098  [  128/  265]
train() client id: f_00001-0-4 loss: 0.419271  [  160/  265]
train() client id: f_00001-0-5 loss: 0.360764  [  192/  265]
train() client id: f_00001-0-6 loss: 0.489213  [  224/  265]
train() client id: f_00001-0-7 loss: 0.493097  [  256/  265]
train() client id: f_00001-1-0 loss: 0.427173  [   32/  265]
train() client id: f_00001-1-1 loss: 0.427553  [   64/  265]
train() client id: f_00001-1-2 loss: 0.527807  [   96/  265]
train() client id: f_00001-1-3 loss: 0.378501  [  128/  265]
train() client id: f_00001-1-4 loss: 0.433198  [  160/  265]
train() client id: f_00001-1-5 loss: 0.409599  [  192/  265]
train() client id: f_00001-1-6 loss: 0.403441  [  224/  265]
train() client id: f_00001-1-7 loss: 0.546091  [  256/  265]
train() client id: f_00001-2-0 loss: 0.524198  [   32/  265]
train() client id: f_00001-2-1 loss: 0.394888  [   64/  265]
train() client id: f_00001-2-2 loss: 0.450450  [   96/  265]
train() client id: f_00001-2-3 loss: 0.349736  [  128/  265]
train() client id: f_00001-2-4 loss: 0.511298  [  160/  265]
train() client id: f_00001-2-5 loss: 0.502335  [  192/  265]
train() client id: f_00001-2-6 loss: 0.412010  [  224/  265]
train() client id: f_00001-2-7 loss: 0.377077  [  256/  265]
train() client id: f_00001-3-0 loss: 0.601128  [   32/  265]
train() client id: f_00001-3-1 loss: 0.404181  [   64/  265]
train() client id: f_00001-3-2 loss: 0.524581  [   96/  265]
train() client id: f_00001-3-3 loss: 0.383638  [  128/  265]
train() client id: f_00001-3-4 loss: 0.401839  [  160/  265]
train() client id: f_00001-3-5 loss: 0.356801  [  192/  265]
train() client id: f_00001-3-6 loss: 0.488645  [  224/  265]
train() client id: f_00001-3-7 loss: 0.394526  [  256/  265]
train() client id: f_00002-0-0 loss: 0.880124  [   32/  124]
train() client id: f_00002-0-1 loss: 1.019340  [   64/  124]
train() client id: f_00002-0-2 loss: 1.016149  [   96/  124]
train() client id: f_00002-1-0 loss: 0.889708  [   32/  124]
train() client id: f_00002-1-1 loss: 1.077103  [   64/  124]
train() client id: f_00002-1-2 loss: 0.992949  [   96/  124]
train() client id: f_00002-2-0 loss: 1.068565  [   32/  124]
train() client id: f_00002-2-1 loss: 0.884892  [   64/  124]
train() client id: f_00002-2-2 loss: 0.941503  [   96/  124]
train() client id: f_00002-3-0 loss: 1.007342  [   32/  124]
train() client id: f_00002-3-1 loss: 0.968837  [   64/  124]
train() client id: f_00002-3-2 loss: 0.803489  [   96/  124]
train() client id: f_00003-0-0 loss: 0.520217  [   32/   43]
train() client id: f_00003-1-0 loss: 0.441598  [   32/   43]
train() client id: f_00003-2-0 loss: 0.548427  [   32/   43]
train() client id: f_00003-3-0 loss: 0.504483  [   32/   43]
train() client id: f_00004-0-0 loss: 0.531931  [   32/  306]
train() client id: f_00004-0-1 loss: 0.707605  [   64/  306]
train() client id: f_00004-0-2 loss: 0.724682  [   96/  306]
train() client id: f_00004-0-3 loss: 0.843257  [  128/  306]
train() client id: f_00004-0-4 loss: 0.538583  [  160/  306]
train() client id: f_00004-0-5 loss: 0.677660  [  192/  306]
train() client id: f_00004-0-6 loss: 0.616782  [  224/  306]
train() client id: f_00004-0-7 loss: 0.700354  [  256/  306]
train() client id: f_00004-0-8 loss: 0.574680  [  288/  306]
train() client id: f_00004-1-0 loss: 0.603664  [   32/  306]
train() client id: f_00004-1-1 loss: 0.495784  [   64/  306]
train() client id: f_00004-1-2 loss: 0.789812  [   96/  306]
train() client id: f_00004-1-3 loss: 0.609426  [  128/  306]
train() client id: f_00004-1-4 loss: 0.657341  [  160/  306]
train() client id: f_00004-1-5 loss: 0.759945  [  192/  306]
train() client id: f_00004-1-6 loss: 0.671461  [  224/  306]
train() client id: f_00004-1-7 loss: 0.772177  [  256/  306]
train() client id: f_00004-1-8 loss: 0.684685  [  288/  306]
train() client id: f_00004-2-0 loss: 0.794706  [   32/  306]
train() client id: f_00004-2-1 loss: 0.623539  [   64/  306]
train() client id: f_00004-2-2 loss: 0.625458  [   96/  306]
train() client id: f_00004-2-3 loss: 0.685707  [  128/  306]
train() client id: f_00004-2-4 loss: 0.616178  [  160/  306]
train() client id: f_00004-2-5 loss: 0.710429  [  192/  306]
train() client id: f_00004-2-6 loss: 0.754322  [  224/  306]
train() client id: f_00004-2-7 loss: 0.754782  [  256/  306]
train() client id: f_00004-2-8 loss: 0.509048  [  288/  306]
train() client id: f_00004-3-0 loss: 0.674010  [   32/  306]
train() client id: f_00004-3-1 loss: 0.698260  [   64/  306]
train() client id: f_00004-3-2 loss: 0.450926  [   96/  306]
train() client id: f_00004-3-3 loss: 0.807008  [  128/  306]
train() client id: f_00004-3-4 loss: 0.658870  [  160/  306]
train() client id: f_00004-3-5 loss: 0.818902  [  192/  306]
train() client id: f_00004-3-6 loss: 0.677811  [  224/  306]
train() client id: f_00004-3-7 loss: 0.753945  [  256/  306]
train() client id: f_00004-3-8 loss: 0.625775  [  288/  306]
train() client id: f_00005-0-0 loss: 0.858813  [   32/  146]
train() client id: f_00005-0-1 loss: 0.492759  [   64/  146]
train() client id: f_00005-0-2 loss: 0.767907  [   96/  146]
train() client id: f_00005-0-3 loss: 0.746059  [  128/  146]
train() client id: f_00005-1-0 loss: 1.188941  [   32/  146]
train() client id: f_00005-1-1 loss: 0.549321  [   64/  146]
train() client id: f_00005-1-2 loss: 0.725006  [   96/  146]
train() client id: f_00005-1-3 loss: 0.625677  [  128/  146]
train() client id: f_00005-2-0 loss: 0.697081  [   32/  146]
train() client id: f_00005-2-1 loss: 0.776975  [   64/  146]
train() client id: f_00005-2-2 loss: 0.846330  [   96/  146]
train() client id: f_00005-2-3 loss: 0.703530  [  128/  146]
train() client id: f_00005-3-0 loss: 0.988329  [   32/  146]
train() client id: f_00005-3-1 loss: 0.709515  [   64/  146]
train() client id: f_00005-3-2 loss: 0.845370  [   96/  146]
train() client id: f_00005-3-3 loss: 0.621161  [  128/  146]
train() client id: f_00006-0-0 loss: 0.436781  [   32/   54]
train() client id: f_00006-1-0 loss: 0.496691  [   32/   54]
train() client id: f_00006-2-0 loss: 0.465027  [   32/   54]
train() client id: f_00006-3-0 loss: 0.382329  [   32/   54]
train() client id: f_00007-0-0 loss: 0.722023  [   32/  179]
train() client id: f_00007-0-1 loss: 0.755464  [   64/  179]
train() client id: f_00007-0-2 loss: 0.663339  [   96/  179]
train() client id: f_00007-0-3 loss: 0.923346  [  128/  179]
train() client id: f_00007-0-4 loss: 0.540746  [  160/  179]
train() client id: f_00007-1-0 loss: 0.896880  [   32/  179]
train() client id: f_00007-1-1 loss: 0.873017  [   64/  179]
train() client id: f_00007-1-2 loss: 0.694993  [   96/  179]
train() client id: f_00007-1-3 loss: 0.520839  [  128/  179]
train() client id: f_00007-1-4 loss: 0.593891  [  160/  179]
train() client id: f_00007-2-0 loss: 0.727448  [   32/  179]
train() client id: f_00007-2-1 loss: 0.608394  [   64/  179]
train() client id: f_00007-2-2 loss: 0.585877  [   96/  179]
train() client id: f_00007-2-3 loss: 0.609897  [  128/  179]
train() client id: f_00007-2-4 loss: 0.927177  [  160/  179]
train() client id: f_00007-3-0 loss: 0.824983  [   32/  179]
train() client id: f_00007-3-1 loss: 0.545022  [   64/  179]
train() client id: f_00007-3-2 loss: 0.697917  [   96/  179]
train() client id: f_00007-3-3 loss: 0.564643  [  128/  179]
train() client id: f_00007-3-4 loss: 0.766090  [  160/  179]
train() client id: f_00008-0-0 loss: 0.622002  [   32/  130]
train() client id: f_00008-0-1 loss: 0.695940  [   64/  130]
train() client id: f_00008-0-2 loss: 0.731502  [   96/  130]
train() client id: f_00008-0-3 loss: 0.825763  [  128/  130]
train() client id: f_00008-1-0 loss: 0.787793  [   32/  130]
train() client id: f_00008-1-1 loss: 0.708484  [   64/  130]
train() client id: f_00008-1-2 loss: 0.655254  [   96/  130]
train() client id: f_00008-1-3 loss: 0.725405  [  128/  130]
train() client id: f_00008-2-0 loss: 0.623516  [   32/  130]
train() client id: f_00008-2-1 loss: 0.726436  [   64/  130]
train() client id: f_00008-2-2 loss: 0.756653  [   96/  130]
train() client id: f_00008-2-3 loss: 0.755016  [  128/  130]
train() client id: f_00008-3-0 loss: 0.627308  [   32/  130]
train() client id: f_00008-3-1 loss: 0.759774  [   64/  130]
train() client id: f_00008-3-2 loss: 0.743575  [   96/  130]
train() client id: f_00008-3-3 loss: 0.738765  [  128/  130]
train() client id: f_00009-0-0 loss: 0.657417  [   32/  118]
train() client id: f_00009-0-1 loss: 0.667657  [   64/  118]
train() client id: f_00009-0-2 loss: 0.886067  [   96/  118]
train() client id: f_00009-1-0 loss: 0.788029  [   32/  118]
train() client id: f_00009-1-1 loss: 0.783369  [   64/  118]
train() client id: f_00009-1-2 loss: 0.646877  [   96/  118]
train() client id: f_00009-2-0 loss: 0.718308  [   32/  118]
train() client id: f_00009-2-1 loss: 0.738652  [   64/  118]
train() client id: f_00009-2-2 loss: 0.724963  [   96/  118]
train() client id: f_00009-3-0 loss: 0.587280  [   32/  118]
train() client id: f_00009-3-1 loss: 0.732094  [   64/  118]
train() client id: f_00009-3-2 loss: 0.829036  [   96/  118]
At round 74 accuracy: 0.649867374005305
At round 74 training accuracy: 0.5868544600938967
At round 74 training loss: 0.8296471103921736
update_location
xs = -4.528292 271.001589 280.045120 -280.943528 169.896481 -205.217951 -322.215960 343.375741 -1.680116 264.695607 
ys = 357.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -287.154970 4.001482 
xs mean: 51.442869159412865
ys mean: 9.371751218646875
dists_uav = 371.334962 289.281603 297.366799 299.061089 197.142681 228.303147 337.391301 358.154550 304.073674 282.983703 
uav_gains = -120.235019 -114.810178 -115.509183 -115.651939 -107.502462 -109.578974 -118.441549 -119.602044 -116.065682 -114.249247 
uav_gains_db_mean: -115.16462783414744
dists_bs = 249.797851 473.645287 487.063230 185.651857 387.048369 174.817308 231.086621 553.550692 493.585857 471.776134 
bs_gains = -106.699543 -114.479749 -114.819449 -103.090639 -112.024486 -102.359423 -105.752754 -116.375466 -114.981215 -114.431666 
bs_gains_db_mean: -110.50143903792369
Round 75
-------------------------------
ene_coms = [0.02867541 0.01518216 0.0167155  0.01694915 0.01237286 0.00698563
 0.02288741 0.02640039 0.01589738 0.01511652]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [1.02858676 2.04971987 0.98330448 0.36905395 2.3538639  1.12580922
 0.46749616 1.42516271 1.026789   0.93373652]
obj_prev = 11.76352255814092
eta_min = 5.614269469063477e-91	eta_max = 0.9727266707765662
af = 2.433006328704853	bf = 0.4414791252875781	zeta = 2.6763069615753383	eta = 0.9090909090909091
af = 2.433006328704853	bf = 0.4414791252875781	zeta = 7.407063355356477	eta = 0.32847111088167014
af = 2.433006328704853	bf = 0.4414791252875781	zeta = 4.691027687383107	eta = 0.5186510272042558
af = 2.433006328704853	bf = 0.4414791252875781	zeta = 4.235609355747759	eta = 0.5744170730483541
af = 2.433006328704853	bf = 0.4414791252875781	zeta = 4.20678696795745	eta = 0.5783526352146533
af = 2.433006328704853	bf = 0.4414791252875781	zeta = 4.206652763245826	eta = 0.5783710863806979
eta = 0.5783710863806979
ene_coms = [0.02867541 0.01518216 0.0167155  0.01694915 0.01237286 0.00698563
 0.02288741 0.02640039 0.01589738 0.01511652]
ene_comp = [0.04518166 0.09502491 0.04446449 0.01541914 0.10972688 0.05235335
 0.01936357 0.06418664 0.04661599 0.04231298]
ene_total = [0.43646678 0.65128132 0.36155015 0.19128408 0.72156235 0.3506705
 0.24968699 0.53533443 0.36942994 0.33938622]
ti_comp = [4.15510677 4.29003922 4.27470584 4.27236939 4.3181323  4.37200452
 4.21298676 4.17785693 4.28288705 4.29069562]
ti_coms = [0.28675409 0.15182165 0.16715502 0.16949147 0.12372856 0.06985635
 0.2288741  0.26400393 0.15897381 0.15116524]
t_total = [26.24968529 26.24968529 26.24968529 26.24968529 26.24968529 26.24968529
 26.24968529 26.24968529 26.24968529 26.24968529]
ene_coms = [0.02867541 0.01518216 0.0167155  0.01694915 0.01237286 0.00698563
 0.02288741 0.02640039 0.01589738 0.01511652]
ene_comp = [3.33888956e-07 2.91386812e-06 3.00681966e-07 1.25522787e-08
 4.42819958e-06 4.69193685e-07 2.55655939e-08 9.46905764e-07
 3.45152965e-07 2.57184862e-07]
ene_total = [0.16946259 0.08973796 0.09878395 0.100163   0.07314498 0.04128518
 0.13525592 0.15602174 0.09394943 0.08933435]
optimize_network iter = 0 obj = 1.0471391082948764
eta = 0.5783710863806979
freqs = [ 5436882.58347691 11075063.08266643  5200882.56510688  1804518.16180668
 12705363.04680829  5987339.03136325  2298080.63163385  7681765.82991226
  5442122.67984129  4930783.07786057]
eta_min = 0.5783710863806982	eta_max = 0.8838124620350791
af = 4.565761298445469e-05	bf = 0.4414791252875781	zeta = 5.022337428290017e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02867541 0.01518216 0.0167155  0.01694915 0.01237286 0.00698563
 0.02288741 0.02640039 0.01589738 0.01511652]
ene_comp = [5.81200274e-08 5.07216822e-07 5.23396890e-08 2.18497427e-09
 7.70816393e-07 8.16725121e-08 4.45020115e-09 1.64827821e-07
 6.00807527e-08 4.47681510e-08]
ene_total = [0.78594603 0.41613134 0.45814507 0.4645475  0.33914018 0.19146666
 0.62730531 0.72359462 0.43572196 0.41431957]
ti_comp = [0.93728169 1.07221414 1.05688076 1.05454431 1.10030722 1.15417944
 0.99516168 0.96003185 1.06506197 1.07287054]
ti_coms = [0.28675409 0.15182165 0.16715502 0.16949147 0.12372856 0.06985635
 0.2288741  0.26400393 0.15897381 0.15116524]
t_total = [26.24968529 26.24968529 26.24968529 26.24968529 26.24968529 26.24968529
 26.24968529 26.24968529 26.24968529 26.24968529]
ene_coms = [0.02867541 0.01518216 0.0167155  0.01694915 0.01237286 0.00698563
 0.02288741 0.02640039 0.01589738 0.01511652]
ene_comp = [7.53164774e-08 5.35418281e-07 5.64588079e-08 2.36479954e-09
 7.82807515e-07 7.72734911e-08 5.25911446e-09 2.05828941e-07
 6.40617695e-08 4.72139146e-08]
ene_total = [0.61495134 0.32559596 0.35846841 0.36347781 0.2653551  0.14980995
 0.49082508 0.56616598 0.3409238  0.32417781]
optimize_network iter = 1 obj = 3.799751226254311
eta = 0.8838124620350791
freqs = [ 5436882.58347691  9995715.52002685  4745098.54312644  1649123.43956508
 11247526.01881779  5115982.67871345  2194571.11134913  7540791.46479441
  4936487.2053223   4448199.20421091]
eta_min = 0.8838124620350757	eta_max = 0.883812462035077
af = 3.7361798092822166e-05	bf = 0.4414791252875781	zeta = 4.109797790210439e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02867541 0.01518216 0.0167155  0.01694915 0.01237286 0.00698563
 0.02288741 0.02640039 0.01589738 0.01511652]
ene_comp = [5.81200274e-08 4.13170215e-07 4.35679890e-08 1.82486248e-09
 6.04074908e-07 5.96302107e-08 4.05834004e-09 1.58833553e-07
 4.94350230e-08 3.64339133e-08]
ene_total = [0.78594603 0.41612877 0.45814483 0.46454749 0.33913561 0.19146606
 0.6273053  0.72359445 0.43572167 0.41431934]
ti_comp = [0.93728169 1.07221414 1.05688076 1.05454431 1.10030722 1.15417944
 0.99516168 0.96003185 1.06506197 1.07287054]
ti_coms = [0.28675409 0.15182165 0.16715502 0.16949147 0.12372856 0.06985635
 0.2288741  0.26400393 0.15897381 0.15116524]
t_total = [26.24968529 26.24968529 26.24968529 26.24968529 26.24968529 26.24968529
 26.24968529 26.24968529 26.24968529 26.24968529]
ene_coms = [0.02867541 0.01518216 0.0167155  0.01694915 0.01237286 0.00698563
 0.02288741 0.02640039 0.01589738 0.01511652]
ene_comp = [7.53164774e-08 5.35418281e-07 5.64588079e-08 2.36479954e-09
 7.82807515e-07 7.72734911e-08 5.25911446e-09 2.05828941e-07
 6.40617695e-08 4.72139146e-08]
ene_total = [0.61495134 0.32559596 0.35846841 0.36347781 0.2653551  0.14980995
 0.49082508 0.56616598 0.3409238  0.32417781]
optimize_network iter = 2 obj = 3.7997512262542417
eta = 0.883812462035077
freqs = [ 5436882.58347689  9995715.52002685  4745098.54312644  1649123.43956508
 11247526.01881779  5115982.67871346  2194571.11134912  7540791.46479439
  4936487.2053223   4448199.20421091]
Done!
ene_coms = [0.02867541 0.01518216 0.0167155  0.01694915 0.01237286 0.00698563
 0.02288741 0.02640039 0.01589738 0.01511652]
ene_comp = [7.44904244e-08 5.29545942e-07 5.58395813e-08 2.33886298e-09
 7.74221870e-07 7.64259740e-08 5.20143375e-09 2.03571459e-07
 6.33591554e-08 4.66960837e-08]
ene_total = [0.02867548 0.01518269 0.01671556 0.01694915 0.01237363 0.00698571
 0.02288742 0.0264006  0.01589744 0.01511657]
At round 75 energy consumption: 0.17718425523882222
At round 75 eta: 0.883812462035077
At round 75 a_n: 2.4916643335555797
At round 75 local rounds: 4.044357541396622
At round 75 global rounds: 21.44519435731406
gradient difference: 0.8559920191764832
train() client id: f_00000-0-0 loss: 0.969383  [   32/  126]
train() client id: f_00000-0-1 loss: 0.736964  [   64/  126]
train() client id: f_00000-0-2 loss: 0.822668  [   96/  126]
train() client id: f_00000-1-0 loss: 0.910199  [   32/  126]
train() client id: f_00000-1-1 loss: 0.758829  [   64/  126]
train() client id: f_00000-1-2 loss: 0.699898  [   96/  126]
train() client id: f_00000-2-0 loss: 0.741799  [   32/  126]
train() client id: f_00000-2-1 loss: 0.843751  [   64/  126]
train() client id: f_00000-2-2 loss: 0.795908  [   96/  126]
train() client id: f_00000-3-0 loss: 0.787246  [   32/  126]
train() client id: f_00000-3-1 loss: 0.844256  [   64/  126]
train() client id: f_00000-3-2 loss: 0.868549  [   96/  126]
train() client id: f_00001-0-0 loss: 0.691575  [   32/  265]
train() client id: f_00001-0-1 loss: 0.532591  [   64/  265]
train() client id: f_00001-0-2 loss: 0.569965  [   96/  265]
train() client id: f_00001-0-3 loss: 0.512518  [  128/  265]
train() client id: f_00001-0-4 loss: 0.469185  [  160/  265]
train() client id: f_00001-0-5 loss: 0.612266  [  192/  265]
train() client id: f_00001-0-6 loss: 0.556481  [  224/  265]
train() client id: f_00001-0-7 loss: 0.458637  [  256/  265]
train() client id: f_00001-1-0 loss: 0.629767  [   32/  265]
train() client id: f_00001-1-1 loss: 0.499071  [   64/  265]
train() client id: f_00001-1-2 loss: 0.490594  [   96/  265]
train() client id: f_00001-1-3 loss: 0.522570  [  128/  265]
train() client id: f_00001-1-4 loss: 0.479670  [  160/  265]
train() client id: f_00001-1-5 loss: 0.578029  [  192/  265]
train() client id: f_00001-1-6 loss: 0.684816  [  224/  265]
train() client id: f_00001-1-7 loss: 0.516647  [  256/  265]
train() client id: f_00001-2-0 loss: 0.510275  [   32/  265]
train() client id: f_00001-2-1 loss: 0.477322  [   64/  265]
train() client id: f_00001-2-2 loss: 0.498579  [   96/  265]
train() client id: f_00001-2-3 loss: 0.537906  [  128/  265]
train() client id: f_00001-2-4 loss: 0.478679  [  160/  265]
train() client id: f_00001-2-5 loss: 0.667699  [  192/  265]
train() client id: f_00001-2-6 loss: 0.727608  [  224/  265]
train() client id: f_00001-2-7 loss: 0.490347  [  256/  265]
train() client id: f_00001-3-0 loss: 0.426578  [   32/  265]
train() client id: f_00001-3-1 loss: 0.462265  [   64/  265]
train() client id: f_00001-3-2 loss: 0.494997  [   96/  265]
train() client id: f_00001-3-3 loss: 0.549010  [  128/  265]
train() client id: f_00001-3-4 loss: 0.507184  [  160/  265]
train() client id: f_00001-3-5 loss: 0.618457  [  192/  265]
train() client id: f_00001-3-6 loss: 0.693276  [  224/  265]
train() client id: f_00001-3-7 loss: 0.610509  [  256/  265]
train() client id: f_00002-0-0 loss: 1.233236  [   32/  124]
train() client id: f_00002-0-1 loss: 1.164713  [   64/  124]
train() client id: f_00002-0-2 loss: 0.982037  [   96/  124]
train() client id: f_00002-1-0 loss: 1.029292  [   32/  124]
train() client id: f_00002-1-1 loss: 1.111877  [   64/  124]
train() client id: f_00002-1-2 loss: 0.959850  [   96/  124]
train() client id: f_00002-2-0 loss: 0.856722  [   32/  124]
train() client id: f_00002-2-1 loss: 1.129703  [   64/  124]
train() client id: f_00002-2-2 loss: 1.210157  [   96/  124]
train() client id: f_00002-3-0 loss: 1.011303  [   32/  124]
train() client id: f_00002-3-1 loss: 1.089021  [   64/  124]
train() client id: f_00002-3-2 loss: 1.050467  [   96/  124]
train() client id: f_00003-0-0 loss: 0.617903  [   32/   43]
train() client id: f_00003-1-0 loss: 0.512630  [   32/   43]
train() client id: f_00003-2-0 loss: 0.763735  [   32/   43]
train() client id: f_00003-3-0 loss: 0.625651  [   32/   43]
train() client id: f_00004-0-0 loss: 0.819981  [   32/  306]
train() client id: f_00004-0-1 loss: 0.851699  [   64/  306]
train() client id: f_00004-0-2 loss: 0.802727  [   96/  306]
train() client id: f_00004-0-3 loss: 0.918793  [  128/  306]
train() client id: f_00004-0-4 loss: 0.904658  [  160/  306]
train() client id: f_00004-0-5 loss: 0.993335  [  192/  306]
train() client id: f_00004-0-6 loss: 0.849919  [  224/  306]
train() client id: f_00004-0-7 loss: 0.863265  [  256/  306]
train() client id: f_00004-0-8 loss: 0.832978  [  288/  306]
train() client id: f_00004-1-0 loss: 0.884463  [   32/  306]
train() client id: f_00004-1-1 loss: 0.947345  [   64/  306]
train() client id: f_00004-1-2 loss: 0.743345  [   96/  306]
train() client id: f_00004-1-3 loss: 0.918686  [  128/  306]
train() client id: f_00004-1-4 loss: 0.812436  [  160/  306]
train() client id: f_00004-1-5 loss: 1.022743  [  192/  306]
train() client id: f_00004-1-6 loss: 0.861996  [  224/  306]
train() client id: f_00004-1-7 loss: 0.773258  [  256/  306]
train() client id: f_00004-1-8 loss: 0.967336  [  288/  306]
train() client id: f_00004-2-0 loss: 0.848711  [   32/  306]
train() client id: f_00004-2-1 loss: 0.840815  [   64/  306]
train() client id: f_00004-2-2 loss: 0.930067  [   96/  306]
train() client id: f_00004-2-3 loss: 0.871024  [  128/  306]
train() client id: f_00004-2-4 loss: 0.819350  [  160/  306]
train() client id: f_00004-2-5 loss: 1.052088  [  192/  306]
train() client id: f_00004-2-6 loss: 0.858561  [  224/  306]
train() client id: f_00004-2-7 loss: 0.760312  [  256/  306]
train() client id: f_00004-2-8 loss: 0.864040  [  288/  306]
train() client id: f_00004-3-0 loss: 0.821127  [   32/  306]
train() client id: f_00004-3-1 loss: 0.827849  [   64/  306]
train() client id: f_00004-3-2 loss: 0.868168  [   96/  306]
train() client id: f_00004-3-3 loss: 1.103501  [  128/  306]
train() client id: f_00004-3-4 loss: 0.946008  [  160/  306]
train() client id: f_00004-3-5 loss: 0.789749  [  192/  306]
train() client id: f_00004-3-6 loss: 0.931784  [  224/  306]
train() client id: f_00004-3-7 loss: 0.738509  [  256/  306]
train() client id: f_00004-3-8 loss: 0.892558  [  288/  306]
train() client id: f_00005-0-0 loss: 0.590476  [   32/  146]
train() client id: f_00005-0-1 loss: 0.646237  [   64/  146]
train() client id: f_00005-0-2 loss: 0.974053  [   96/  146]
train() client id: f_00005-0-3 loss: 0.713305  [  128/  146]
train() client id: f_00005-1-0 loss: 0.413322  [   32/  146]
train() client id: f_00005-1-1 loss: 0.673450  [   64/  146]
train() client id: f_00005-1-2 loss: 1.070610  [   96/  146]
train() client id: f_00005-1-3 loss: 0.771462  [  128/  146]
train() client id: f_00005-2-0 loss: 0.724691  [   32/  146]
train() client id: f_00005-2-1 loss: 0.972483  [   64/  146]
train() client id: f_00005-2-2 loss: 0.723044  [   96/  146]
train() client id: f_00005-2-3 loss: 0.707236  [  128/  146]
train() client id: f_00005-3-0 loss: 0.866066  [   32/  146]
train() client id: f_00005-3-1 loss: 0.854296  [   64/  146]
train() client id: f_00005-3-2 loss: 0.731136  [   96/  146]
train() client id: f_00005-3-3 loss: 0.665592  [  128/  146]
train() client id: f_00006-0-0 loss: 0.546291  [   32/   54]
train() client id: f_00006-1-0 loss: 0.525512  [   32/   54]
train() client id: f_00006-2-0 loss: 0.496668  [   32/   54]
train() client id: f_00006-3-0 loss: 0.563456  [   32/   54]
train() client id: f_00007-0-0 loss: 0.488965  [   32/  179]
train() client id: f_00007-0-1 loss: 0.684967  [   64/  179]
train() client id: f_00007-0-2 loss: 0.536079  [   96/  179]
train() client id: f_00007-0-3 loss: 0.733210  [  128/  179]
train() client id: f_00007-0-4 loss: 0.554212  [  160/  179]
train() client id: f_00007-1-0 loss: 0.495220  [   32/  179]
train() client id: f_00007-1-1 loss: 0.495087  [   64/  179]
train() client id: f_00007-1-2 loss: 0.864492  [   96/  179]
train() client id: f_00007-1-3 loss: 0.555451  [  128/  179]
train() client id: f_00007-1-4 loss: 0.504490  [  160/  179]
train() client id: f_00007-2-0 loss: 0.500879  [   32/  179]
train() client id: f_00007-2-1 loss: 0.532723  [   64/  179]
train() client id: f_00007-2-2 loss: 0.494579  [   96/  179]
train() client id: f_00007-2-3 loss: 0.566460  [  128/  179]
train() client id: f_00007-2-4 loss: 0.874312  [  160/  179]
train() client id: f_00007-3-0 loss: 0.679810  [   32/  179]
train() client id: f_00007-3-1 loss: 0.639971  [   64/  179]
train() client id: f_00007-3-2 loss: 0.431205  [   96/  179]
train() client id: f_00007-3-3 loss: 0.410543  [  128/  179]
train() client id: f_00007-3-4 loss: 0.757094  [  160/  179]
train() client id: f_00008-0-0 loss: 0.729691  [   32/  130]
train() client id: f_00008-0-1 loss: 0.611176  [   64/  130]
train() client id: f_00008-0-2 loss: 0.747688  [   96/  130]
train() client id: f_00008-0-3 loss: 0.683064  [  128/  130]
train() client id: f_00008-1-0 loss: 0.724232  [   32/  130]
train() client id: f_00008-1-1 loss: 0.588047  [   64/  130]
train() client id: f_00008-1-2 loss: 0.713210  [   96/  130]
train() client id: f_00008-1-3 loss: 0.716244  [  128/  130]
train() client id: f_00008-2-0 loss: 0.778268  [   32/  130]
train() client id: f_00008-2-1 loss: 0.693109  [   64/  130]
train() client id: f_00008-2-2 loss: 0.620044  [   96/  130]
train() client id: f_00008-2-3 loss: 0.657694  [  128/  130]
train() client id: f_00008-3-0 loss: 0.609051  [   32/  130]
train() client id: f_00008-3-1 loss: 0.681003  [   64/  130]
train() client id: f_00008-3-2 loss: 0.726652  [   96/  130]
train() client id: f_00008-3-3 loss: 0.756234  [  128/  130]
train() client id: f_00009-0-0 loss: 0.633206  [   32/  118]
train() client id: f_00009-0-1 loss: 0.474320  [   64/  118]
train() client id: f_00009-0-2 loss: 0.775361  [   96/  118]
train() client id: f_00009-1-0 loss: 0.519131  [   32/  118]
train() client id: f_00009-1-1 loss: 0.673820  [   64/  118]
train() client id: f_00009-1-2 loss: 0.805048  [   96/  118]
train() client id: f_00009-2-0 loss: 0.643220  [   32/  118]
train() client id: f_00009-2-1 loss: 0.554582  [   64/  118]
train() client id: f_00009-2-2 loss: 0.743755  [   96/  118]
train() client id: f_00009-3-0 loss: 0.798109  [   32/  118]
train() client id: f_00009-3-1 loss: 0.682054  [   64/  118]
train() client id: f_00009-3-2 loss: 0.595998  [   96/  118]
At round 75 accuracy: 0.649867374005305
At round 75 training accuracy: 0.5928906773977196
At round 75 training loss: 0.809325932406377
update_location
xs = -4.528292 276.001589 285.045120 -285.943528 174.896481 -210.217951 -327.215960 348.375741 -1.680116 269.695607 
ys = 362.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -292.154970 4.001482 
xs mean: 52.442869159412865
ys mean: 9.371751218646875
dists_uav = 376.152275 293.970851 302.080228 303.763017 201.467619 232.807874 342.169621 362.951015 308.799853 287.666008 
uav_gains = -120.449534 -115.218971 -115.902732 -116.040431 -107.774608 -109.913584 -118.728492 -119.840699 -116.443167 -114.667439 
uav_gains_db_mean: -115.49796550998164
dists_bs = 253.475533 478.356430 491.737777 188.549853 391.510414 175.750592 234.303619 558.235727 498.270557 476.439583 
bs_gains = -106.877268 -114.600104 -114.935599 -103.278993 -112.163872 -102.424169 -105.920871 -116.477953 -115.096085 -114.551279 
bs_gains_db_mean: -110.63261940691268
Round 76
-------------------------------
ene_coms = [0.02951319 0.01534866 0.01737328 0.01761383 0.0125068  0.00700644
 0.02368545 0.02722517 0.01606945 0.01528074]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.88899871 1.76829254 0.84955091 0.31976054 2.03055316 0.97108183
 0.40495873 1.23102646 0.88600295 0.80572596]
obj_prev = 10.155951796247649
eta_min = 2.550795668105864e-105	eta_max = 0.9757492505979475
af = 2.098524592010151	bf = 0.3903293925972013	zeta = 2.308377051211166	eta = 0.9090909090909091
af = 2.098524592010151	bf = 0.3903293925972013	zeta = 6.493741636640349	eta = 0.3231610848465877
af = 2.098524592010151	bf = 0.3903293925972013	zeta = 4.079008861311545	eta = 0.514469240779095
af = 2.098524592010151	bf = 0.3903293925972013	zeta = 3.676491643714546	eta = 0.5707954200298154
af = 2.098524592010151	bf = 0.3903293925972013	zeta = 3.650994058615688	eta = 0.5747817055626292
af = 2.098524592010151	bf = 0.3903293925972013	zeta = 3.6508748200173784	eta = 0.5748004780947712
eta = 0.5748004780947712
ene_coms = [0.02951319 0.01534866 0.01737328 0.01761383 0.0125068  0.00700644
 0.02368545 0.02722517 0.01606945 0.01528074]
ene_comp = [0.04569266 0.09609965 0.04496738 0.01559353 0.11096789 0.05294547
 0.01958257 0.06491259 0.04714322 0.04279154]
ene_total = [0.38011871 0.56330172 0.31509315 0.1678425  0.62408758 0.30301952
 0.21869288 0.46569895 0.31950067 0.29351915]
ti_comp = [4.88843144 5.03007674 5.00983062 5.00742506 5.0584954  5.11349894
 4.94670884 4.91131164 5.02286884 5.03075601]
ti_coms = [0.29513193 0.15348663 0.17373275 0.17613832 0.12506797 0.07006444
 0.23685453 0.27225173 0.16069453 0.15280736]
t_total = [26.19968109 26.19968109 26.19968109 26.19968109 26.19968109 26.19968109
 26.19968109 26.19968109 26.19968109 26.19968109]
ene_coms = [0.02951319 0.01534866 0.01737328 0.01761383 0.0125068  0.00700644
 0.02368545 0.02722517 0.01606945 0.01528074]
ene_comp = [2.49505618e-07 2.19228065e-06 2.26426222e-07 9.45114012e-09
 3.33756177e-06 3.54756188e-07 1.91803414e-08 7.08715383e-07
 2.59557829e-07 1.93502805e-07]
ene_total = [0.14917205 0.077589   0.08781222 0.08902699 0.06323093 0.035415
 0.11971529 0.13760986 0.08122238 0.07723557]
optimize_network iter = 0 obj = 0.9180293023578135
eta = 0.5748004780947712
freqs = [ 4673550.41594304  9552502.90797018  4487914.32920651  1557040.5146248
 10968468.25683863  5177029.04573599  1979353.33073786  6608478.08495442
  4692858.18432639  4252993.00417582]
eta_min = 0.5748004780947721	eta_max = 0.8973894702834002
af = 2.9306328231968833e-05	bf = 0.3903293925972013	zeta = 3.223696105516572e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.02951319 0.01534866 0.01737328 0.01761383 0.0125068  0.00700644
 0.02368545 0.02722517 0.01606945 0.01528074]
ene_comp = [4.29457079e-08 3.77342383e-07 3.89732083e-08 1.62676058e-09
 5.74471845e-07 6.10617738e-08 3.30138193e-09 1.21986367e-07
 4.46759267e-08 3.33063239e-08]
ene_total = [0.69770186 0.36285597 0.41071042 0.41639637 0.29567806 0.1656359
 0.55993137 0.64361424 0.37988781 0.36124203]
ti_comp = [0.95578242 1.09742772 1.0771816  1.07477603 1.12584638 1.18084991
 1.01405982 0.97866262 1.09021982 1.09810699]
ti_coms = [0.29513193 0.15348663 0.17373275 0.17613832 0.12506797 0.07006444
 0.23685453 0.27225173 0.16069453 0.15280736]
t_total = [26.19968109 26.19968109 26.19968109 26.19968109 26.19968109 26.19968109
 26.19968109 26.19968109 26.19968109 26.19968109]
ene_coms = [0.02951319 0.01534866 0.01737328 0.01761383 0.0125068  0.00700644
 0.02368545 0.02722517 0.01606945 0.01528074]
ene_comp = [4.87831497e-08 3.44240566e-07 3.66069446e-08 1.53336554e-09
 5.03596018e-07 4.97216396e-08 3.41138241e-09 1.33404160e-07
 4.11792271e-08 3.03551924e-08]
ene_total = [0.61813787 0.32147613 0.36387402 0.3689116  0.26195822 0.14674698
 0.49607825 0.57021838 0.33656632 0.32004687]
optimize_network iter = 1 obj = 3.8040146290229764
eta = 0.8973894702834002
freqs = [4673550.41594303 8560623.02058237 4081014.93780989 1418358.14503668
 9635576.77719844 4383223.12426489 1887841.84642243 6484185.87952213
 4227315.91923035 3809542.09832592]
eta_min = 0.8919840766645067	eta_max = 0.8973894702833173
af = 2.3678280012357867e-05	bf = 0.3903293925972013	zeta = 2.6046108013593655e-05	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.02951319 0.01534866 0.01737328 0.01761383 0.0125068  0.00700644
 0.02368545 0.02722517 0.01606945 0.01528074]
ene_comp = [4.29457079e-08 3.03048387e-07 3.22265200e-08 1.34988144e-09
 4.43335201e-07 4.37718971e-08 3.00317288e-09 1.17440881e-07
 3.62516785e-08 2.67228589e-08]
ene_total = [0.69770186 0.36285422 0.41071026 0.41639636 0.29567496 0.16563549
 0.55993136 0.64361413 0.37988761 0.36124188]
ti_comp = [0.95578242 1.09742772 1.0771816  1.07477603 1.12584638 1.18084991
 1.01405982 0.97866262 1.09021982 1.09810699]
ti_coms = [0.29513193 0.15348663 0.17373275 0.17613832 0.12506797 0.07006444
 0.23685453 0.27225173 0.16069453 0.15280736]
t_total = [26.19968109 26.19968109 26.19968109 26.19968109 26.19968109 26.19968109
 26.19968109 26.19968109 26.19968109 26.19968109]
ene_coms = [0.02951319 0.01534866 0.01737328 0.01761383 0.0125068  0.00700644
 0.02368545 0.02722517 0.01606945 0.01528074]
ene_comp = [4.87831497e-08 3.44240566e-07 3.66069446e-08 1.53336554e-09
 5.03596018e-07 4.97216396e-08 3.41138241e-09 1.33404160e-07
 4.11792271e-08 3.03551924e-08]
ene_total = [0.61813787 0.32147613 0.36387402 0.3689116  0.26195822 0.14674698
 0.49607825 0.57021838 0.33656632 0.32004687]
optimize_network iter = 2 obj = 3.804014629019901
eta = 0.8973894702833173
freqs = [4673550.41594207 8560623.02058179 4081014.93780954 1418358.14503656
 9635576.77719802 4383223.12426488 1887841.84642216 6484185.87952097
 4227315.91923004 3809542.09832566]
Done!
ene_coms = [0.02951319 0.01534866 0.01737328 0.01761383 0.0125068  0.00700644
 0.02368545 0.02722517 0.01606945 0.01528074]
ene_comp = [4.12815189e-08 2.91304959e-07 3.09777102e-08 1.29757219e-09
 4.26155520e-07 4.20756925e-08 2.88679694e-09 1.12889930e-07
 3.48468898e-08 2.56873215e-08]
ene_total = [0.02951323 0.01534895 0.01737331 0.01761383 0.01250722 0.00700649
 0.02368546 0.02722529 0.01606949 0.01528076]
At round 76 energy consumption: 0.18162403067214006
At round 76 eta: 0.8973894702833173
At round 76 a_n: 2.1491184865862607
At round 76 local rounds: 3.545156600834603
At round 76 global rounds: 20.94442444182072
gradient difference: 0.8981006741523743
train() client id: f_00000-0-0 loss: 0.731196  [   32/  126]
train() client id: f_00000-0-1 loss: 0.905392  [   64/  126]
train() client id: f_00000-0-2 loss: 0.816946  [   96/  126]
train() client id: f_00000-1-0 loss: 0.922641  [   32/  126]
train() client id: f_00000-1-1 loss: 0.915428  [   64/  126]
train() client id: f_00000-1-2 loss: 0.782492  [   96/  126]
train() client id: f_00000-2-0 loss: 0.793306  [   32/  126]
train() client id: f_00000-2-1 loss: 0.882653  [   64/  126]
train() client id: f_00000-2-2 loss: 0.723747  [   96/  126]
train() client id: f_00001-0-0 loss: 0.472953  [   32/  265]
train() client id: f_00001-0-1 loss: 0.687801  [   64/  265]
train() client id: f_00001-0-2 loss: 0.574549  [   96/  265]
train() client id: f_00001-0-3 loss: 0.485481  [  128/  265]
train() client id: f_00001-0-4 loss: 0.458484  [  160/  265]
train() client id: f_00001-0-5 loss: 0.490360  [  192/  265]
train() client id: f_00001-0-6 loss: 0.484622  [  224/  265]
train() client id: f_00001-0-7 loss: 0.456802  [  256/  265]
train() client id: f_00001-1-0 loss: 0.624959  [   32/  265]
train() client id: f_00001-1-1 loss: 0.546465  [   64/  265]
train() client id: f_00001-1-2 loss: 0.469527  [   96/  265]
train() client id: f_00001-1-3 loss: 0.579891  [  128/  265]
train() client id: f_00001-1-4 loss: 0.459800  [  160/  265]
train() client id: f_00001-1-5 loss: 0.597312  [  192/  265]
train() client id: f_00001-1-6 loss: 0.434500  [  224/  265]
train() client id: f_00001-1-7 loss: 0.396240  [  256/  265]
train() client id: f_00001-2-0 loss: 0.467834  [   32/  265]
train() client id: f_00001-2-1 loss: 0.494955  [   64/  265]
train() client id: f_00001-2-2 loss: 0.628473  [   96/  265]
train() client id: f_00001-2-3 loss: 0.436890  [  128/  265]
train() client id: f_00001-2-4 loss: 0.590584  [  160/  265]
train() client id: f_00001-2-5 loss: 0.471719  [  192/  265]
train() client id: f_00001-2-6 loss: 0.518556  [  224/  265]
train() client id: f_00001-2-7 loss: 0.422746  [  256/  265]
train() client id: f_00002-0-0 loss: 0.924399  [   32/  124]
train() client id: f_00002-0-1 loss: 1.056621  [   64/  124]
train() client id: f_00002-0-2 loss: 0.898636  [   96/  124]
train() client id: f_00002-1-0 loss: 0.912333  [   32/  124]
train() client id: f_00002-1-1 loss: 1.126351  [   64/  124]
train() client id: f_00002-1-2 loss: 0.869209  [   96/  124]
train() client id: f_00002-2-0 loss: 0.939731  [   32/  124]
train() client id: f_00002-2-1 loss: 0.869582  [   64/  124]
train() client id: f_00002-2-2 loss: 1.116069  [   96/  124]
train() client id: f_00003-0-0 loss: 0.652799  [   32/   43]
train() client id: f_00003-1-0 loss: 0.707390  [   32/   43]
train() client id: f_00003-2-0 loss: 0.753273  [   32/   43]
train() client id: f_00004-0-0 loss: 0.802668  [   32/  306]
train() client id: f_00004-0-1 loss: 0.767358  [   64/  306]
train() client id: f_00004-0-2 loss: 0.837302  [   96/  306]
train() client id: f_00004-0-3 loss: 0.816851  [  128/  306]
train() client id: f_00004-0-4 loss: 0.957676  [  160/  306]
train() client id: f_00004-0-5 loss: 0.957446  [  192/  306]
train() client id: f_00004-0-6 loss: 0.714825  [  224/  306]
train() client id: f_00004-0-7 loss: 0.954193  [  256/  306]
train() client id: f_00004-0-8 loss: 0.839930  [  288/  306]
train() client id: f_00004-1-0 loss: 0.764919  [   32/  306]
train() client id: f_00004-1-1 loss: 0.797507  [   64/  306]
train() client id: f_00004-1-2 loss: 0.928154  [   96/  306]
train() client id: f_00004-1-3 loss: 0.879564  [  128/  306]
train() client id: f_00004-1-4 loss: 0.861209  [  160/  306]
train() client id: f_00004-1-5 loss: 0.805019  [  192/  306]
train() client id: f_00004-1-6 loss: 0.841476  [  224/  306]
train() client id: f_00004-1-7 loss: 0.785543  [  256/  306]
train() client id: f_00004-1-8 loss: 0.843088  [  288/  306]
train() client id: f_00004-2-0 loss: 0.844893  [   32/  306]
train() client id: f_00004-2-1 loss: 0.815912  [   64/  306]
train() client id: f_00004-2-2 loss: 0.845895  [   96/  306]
train() client id: f_00004-2-3 loss: 1.062908  [  128/  306]
train() client id: f_00004-2-4 loss: 0.761361  [  160/  306]
train() client id: f_00004-2-5 loss: 0.759127  [  192/  306]
train() client id: f_00004-2-6 loss: 0.742976  [  224/  306]
train() client id: f_00004-2-7 loss: 0.902848  [  256/  306]
train() client id: f_00004-2-8 loss: 0.734979  [  288/  306]
train() client id: f_00005-0-0 loss: 0.819516  [   32/  146]
train() client id: f_00005-0-1 loss: 0.754496  [   64/  146]
train() client id: f_00005-0-2 loss: 0.764324  [   96/  146]
train() client id: f_00005-0-3 loss: 0.550372  [  128/  146]
train() client id: f_00005-1-0 loss: 0.680292  [   32/  146]
train() client id: f_00005-1-1 loss: 0.572084  [   64/  146]
train() client id: f_00005-1-2 loss: 0.874866  [   96/  146]
train() client id: f_00005-1-3 loss: 0.687262  [  128/  146]
train() client id: f_00005-2-0 loss: 0.525331  [   32/  146]
train() client id: f_00005-2-1 loss: 0.857293  [   64/  146]
train() client id: f_00005-2-2 loss: 0.826596  [   96/  146]
train() client id: f_00005-2-3 loss: 0.525981  [  128/  146]
train() client id: f_00006-0-0 loss: 0.479831  [   32/   54]
train() client id: f_00006-1-0 loss: 0.466335  [   32/   54]
train() client id: f_00006-2-0 loss: 0.520894  [   32/   54]
train() client id: f_00007-0-0 loss: 0.590702  [   32/  179]
train() client id: f_00007-0-1 loss: 0.272578  [   64/  179]
train() client id: f_00007-0-2 loss: 0.442478  [   96/  179]
train() client id: f_00007-0-3 loss: 0.599530  [  128/  179]
train() client id: f_00007-0-4 loss: 0.525859  [  160/  179]
train() client id: f_00007-1-0 loss: 0.499947  [   32/  179]
train() client id: f_00007-1-1 loss: 0.586390  [   64/  179]
train() client id: f_00007-1-2 loss: 0.326745  [   96/  179]
train() client id: f_00007-1-3 loss: 0.432121  [  128/  179]
train() client id: f_00007-1-4 loss: 0.595548  [  160/  179]
train() client id: f_00007-2-0 loss: 0.550717  [   32/  179]
train() client id: f_00007-2-1 loss: 0.517408  [   64/  179]
train() client id: f_00007-2-2 loss: 0.448558  [   96/  179]
train() client id: f_00007-2-3 loss: 0.482228  [  128/  179]
train() client id: f_00007-2-4 loss: 0.433693  [  160/  179]
train() client id: f_00008-0-0 loss: 0.718236  [   32/  130]
train() client id: f_00008-0-1 loss: 0.637566  [   64/  130]
train() client id: f_00008-0-2 loss: 0.659774  [   96/  130]
train() client id: f_00008-0-3 loss: 0.650525  [  128/  130]
train() client id: f_00008-1-0 loss: 0.642313  [   32/  130]
train() client id: f_00008-1-1 loss: 0.765976  [   64/  130]
train() client id: f_00008-1-2 loss: 0.617333  [   96/  130]
train() client id: f_00008-1-3 loss: 0.630373  [  128/  130]
train() client id: f_00008-2-0 loss: 0.687932  [   32/  130]
train() client id: f_00008-2-1 loss: 0.618345  [   64/  130]
train() client id: f_00008-2-2 loss: 0.724903  [   96/  130]
train() client id: f_00008-2-3 loss: 0.609987  [  128/  130]
train() client id: f_00009-0-0 loss: 0.774001  [   32/  118]
train() client id: f_00009-0-1 loss: 0.899571  [   64/  118]
train() client id: f_00009-0-2 loss: 0.967407  [   96/  118]
train() client id: f_00009-1-0 loss: 0.778389  [   32/  118]
train() client id: f_00009-1-1 loss: 0.851229  [   64/  118]
train() client id: f_00009-1-2 loss: 0.930466  [   96/  118]
train() client id: f_00009-2-0 loss: 0.928911  [   32/  118]
train() client id: f_00009-2-1 loss: 0.811742  [   64/  118]
train() client id: f_00009-2-2 loss: 0.792123  [   96/  118]
At round 76 accuracy: 0.649867374005305
At round 76 training accuracy: 0.590878604963112
At round 76 training loss: 0.8184035212795939
update_location
xs = -4.528292 281.001589 290.045120 -290.943528 179.896481 -215.217951 -332.215960 353.375741 -1.680116 274.695607 
ys = 367.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -297.154970 4.001482 
xs mean: 53.442869159412865
ys mean: 9.371751218646875
dists_uav = 380.974295 298.670182 306.802730 308.474319 205.823143 237.332437 346.954189 367.752901 313.534526 292.358835 
uav_gains = -120.656199 -115.619128 -116.285206 -116.417576 -108.051623 -110.261237 -119.003422 -120.069941 -116.808288 -115.079431 
uav_gains_db_mean: -115.8252050186713
dists_bs = 257.197833 483.073380 496.418666 191.534546 395.985314 176.820390 237.582292 562.926180 502.961328 481.109792 
bs_gains = -107.054544 -114.719426 -115.050806 -103.469978 -112.302073 -102.497965 -106.089853 -116.579700 -115.210028 -114.669897 
bs_gains_db_mean: -110.76442695944161
Round 77
-------------------------------
ene_coms = [0.03035497 0.01551689 0.01805552 0.01830217 0.01264222 0.00703031
 0.02449161 0.02805454 0.01624332 0.01544667]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.74883817 1.48675314 0.71538682 0.27005031 1.70715245 0.81634562
 0.34188386 1.03632783 0.74510111 0.67760491]
obj_prev = 8.545444232067062
eta_min = nan	eta_max = nan
af = 1.7640428553154524	bf = 0.33627222564236425	zeta = 1.9404471408469977	eta = 0.9090909090909091
af = 1.7640428553154524	bf = 0.33627222564236425	zeta = 5.54843814101516	eta = 0.31793503153171987
af = 1.7640428553154524	bf = 0.33627222564236425	zeta = 3.45671386706916	eta = 0.5103236551109535
af = 1.7640428553154524	bf = 0.33627222564236425	zeta = 3.1101394867317587	eta = 0.5671909130896149
af = 1.7640428553154524	bf = 0.33627222564236425	zeta = 3.0881726286658746	eta = 0.571225468078039
af = 1.7640428553154524	bf = 0.33627222564236425	zeta = 3.0880695242433505	eta = 0.5712445401460592
eta = 0.5712445401460592
ene_coms = [0.03035497 0.01551689 0.01805552 0.01830217 0.01264222 0.00703031
 0.02449161 0.02805454 0.01624332 0.01544667]
ene_comp = [0.04620473 0.09717662 0.04547132 0.01576828 0.11221149 0.05353882
 0.01980203 0.06564006 0.04767155 0.0432711 ]
ene_total = [0.32258634 0.47483714 0.26767208 0.14355675 0.52607449 0.25520965
 0.18663244 0.39478469 0.26930704 0.24740891]
ti_comp = [5.90257597 6.05095671 6.02557039 6.02310395 6.07970339 6.13582251
 5.96120951 5.92558024 6.04369237 6.05165888]
ti_coms = [0.30354965 0.15516891 0.18055523 0.18302167 0.12642223 0.0703031
 0.24491611 0.28054538 0.16243324 0.15446674]
t_total = [26.1496769 26.1496769 26.1496769 26.1496769 26.1496769 26.1496769
 26.1496769 26.1496769 26.1496769 26.1496769]
ene_coms = [0.03035497 0.01551689 0.01805552 0.01830217 0.01264222 0.00703031
 0.02449161 0.02805454 0.01624332 0.01544667]
ene_comp = [1.76952312e-07 1.56645277e-06 1.61843962e-07 6.75448785e-09
 2.38906118e-06 2.54765588e-07 1.36565275e-08 5.03413782e-07
 1.85375840e-07 1.38269031e-07]
ene_total = [0.12790221 0.06538744 0.07607811 0.0771167  0.05327841 0.02962348
 0.10319612 0.11821067 0.06844246 0.06508556]
optimize_network iter = 0 obj = 0.7843211489559606
eta = 0.5712445401460592
freqs = [3913946.48799029 8029855.89738658 3773196.68804044 1308983.04316144
 9228369.15013875 4362806.89615856 1660906.93326846 5538702.91571781
 3943909.36240625 3575143.6081141 ]
eta_min = 0.5712445401460594	eta_max = 0.9117241448195031
af = 1.7411308494670544e-05	bf = 0.33627222564236425	zeta = 1.91524393441376e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03035497 0.01551689 0.01805552 0.01830217 0.01264222 0.00703031
 0.02449161 0.02805454 0.01624332 0.01544667]
ene_comp = [3.01200487e-08 2.66634741e-07 2.75483715e-08 1.14971939e-09
 4.06655547e-07 4.33650843e-08 2.32455439e-09 8.56888926e-08
 3.15538648e-08 2.35355498e-08]
ene_total = [0.60322354 0.30836159 0.35880529 0.36370615 0.25123812 0.13970929
 0.48670466 0.55750986 0.32279283 0.30696139]
ti_comp = [1.01233619 1.16071693 1.13533061 1.13286417 1.18946361 1.24558273
 1.07096973 1.03534046 1.15345259 1.1614191 ]
ti_coms = [0.30354965 0.15516891 0.18055523 0.18302167 0.12642223 0.0703031
 0.24491611 0.28054538 0.16243324 0.15446674]
t_total = [26.1496769 26.1496769 26.1496769 26.1496769 26.1496769 26.1496769
 26.1496769 26.1496769 26.1496769 26.1496769]
ene_coms = [0.03035497 0.01551689 0.01805552 0.01830217 0.01264222 0.00703031
 0.02449161 0.02805454 0.01624332 0.01544667]
ene_comp = [2.96679934e-08 2.09947662e-07 2.24825942e-08 9.41620152e-10
 3.07813757e-07 3.04887396e-08 2.08666605e-09 8.13240091e-08
 2.50990116e-08 1.85137399e-08]
ene_total = [0.60322353 0.30836046 0.35880519 0.36370615 0.25123616 0.13970904
 0.48670466 0.55750978 0.3227927  0.30696129]
optimize_network iter = 1 obj = 3.6990089563688437
eta = 0.909090909090909
freqs = [3884464.31328397 7125328.74011415 3408667.89008279 1184611.55865538
 8028892.08760809 3658184.69892973 1573627.42731249 5395791.95351199
 3517458.27711868 3170869.70250878]
eta_min = 0.8966658205965521	eta_max = 0.9090909090908956
af = 1.380593044384866e-05	bf = 0.33627222564236425	zeta = 1.5186523488233529e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03035497 0.01551689 0.01805552 0.01830217 0.01264222 0.00703031
 0.02449161 0.02805454 0.01624332 0.01544667]
ene_comp = [2.96679934e-08 2.09947662e-07 2.24825942e-08 9.41620152e-10
 3.07813757e-07 3.04887396e-08 2.08666605e-09 8.13240091e-08
 2.50990116e-08 1.85137399e-08]
ene_total = [0.60322353 0.30836046 0.35880519 0.36370615 0.25123616 0.13970904
 0.48670466 0.55750978 0.3227927  0.30696129]
ti_comp = [1.01233619 1.16071693 1.13533061 1.13286417 1.18946361 1.24558273
 1.07096973 1.03534046 1.15345259 1.1614191 ]
ti_coms = [0.30354965 0.15516891 0.18055523 0.18302167 0.12642223 0.0703031
 0.24491611 0.28054538 0.16243324 0.15446674]
t_total = [26.1496769 26.1496769 26.1496769 26.1496769 26.1496769 26.1496769
 26.1496769 26.1496769 26.1496769 26.1496769]
ene_coms = [0.03035497 0.01551689 0.01805552 0.01830217 0.01264222 0.00703031
 0.02449161 0.02805454 0.01624332 0.01544667]
ene_comp = [2.96679934e-08 2.09947662e-07 2.24825942e-08 9.41620152e-10
 3.07813757e-07 3.04887396e-08 2.08666605e-09 8.13240091e-08
 2.50990116e-08 1.85137399e-08]
ene_total = [0.60322353 0.30836046 0.35880519 0.36370615 0.25123616 0.13970904
 0.48670466 0.55750978 0.3227927  0.30696129]
optimize_network iter = 2 obj = 3.6990089563683024
eta = 0.9090909090908956
freqs = [3884464.31328384 7125328.74011407 3408667.89008274 1184611.55865537
 8028892.08760804 3658184.69892973 1573627.42731245 5395791.95351182
 3517458.27711863 3170869.70250875]
Done!
ene_coms = [0.03035497 0.01551689 0.01805552 0.01830217 0.01264222 0.00703031
 0.02449161 0.02805454 0.01624332 0.01544667]
ene_comp = [2.85183291e-08 2.01811981e-07 2.16113712e-08 9.05131431e-10
 2.95885666e-07 2.93072705e-08 2.00580566e-09 7.81726226e-08
 2.41263998e-08 1.77963140e-08]
ene_total = [0.03035499 0.01551709 0.01805554 0.01830217 0.01264252 0.00703034
 0.02449161 0.02805462 0.01624335 0.01544669]
At round 77 energy consumption: 0.1861389257888043
At round 77 eta: 0.9090909090908956
At round 77 a_n: 1.8065726396169453
At round 77 local rounds: 3.120939520578239
At round 77 global rounds: 19.872299035783456
gradient difference: 0.8899499773979187
train() client id: f_00000-0-0 loss: 0.936291  [   32/  126]
train() client id: f_00000-0-1 loss: 0.946743  [   64/  126]
train() client id: f_00000-0-2 loss: 1.078524  [   96/  126]
train() client id: f_00000-1-0 loss: 1.000286  [   32/  126]
train() client id: f_00000-1-1 loss: 0.877004  [   64/  126]
train() client id: f_00000-1-2 loss: 1.241643  [   96/  126]
train() client id: f_00000-2-0 loss: 0.977154  [   32/  126]
train() client id: f_00000-2-1 loss: 1.033141  [   64/  126]
train() client id: f_00000-2-2 loss: 0.912461  [   96/  126]
train() client id: f_00001-0-0 loss: 0.604547  [   32/  265]
train() client id: f_00001-0-1 loss: 0.688265  [   64/  265]
train() client id: f_00001-0-2 loss: 0.578276  [   96/  265]
train() client id: f_00001-0-3 loss: 0.678750  [  128/  265]
train() client id: f_00001-0-4 loss: 0.547129  [  160/  265]
train() client id: f_00001-0-5 loss: 0.628660  [  192/  265]
train() client id: f_00001-0-6 loss: 0.648712  [  224/  265]
train() client id: f_00001-0-7 loss: 0.543109  [  256/  265]
train() client id: f_00001-1-0 loss: 0.514818  [   32/  265]
train() client id: f_00001-1-1 loss: 0.594870  [   64/  265]
train() client id: f_00001-1-2 loss: 0.486156  [   96/  265]
train() client id: f_00001-1-3 loss: 0.648670  [  128/  265]
train() client id: f_00001-1-4 loss: 0.714682  [  160/  265]
train() client id: f_00001-1-5 loss: 0.741520  [  192/  265]
train() client id: f_00001-1-6 loss: 0.667315  [  224/  265]
train() client id: f_00001-1-7 loss: 0.559697  [  256/  265]
train() client id: f_00001-2-0 loss: 0.597618  [   32/  265]
train() client id: f_00001-2-1 loss: 0.663164  [   64/  265]
train() client id: f_00001-2-2 loss: 0.579043  [   96/  265]
train() client id: f_00001-2-3 loss: 0.597821  [  128/  265]
train() client id: f_00001-2-4 loss: 0.618648  [  160/  265]
train() client id: f_00001-2-5 loss: 0.543343  [  192/  265]
train() client id: f_00001-2-6 loss: 0.621087  [  224/  265]
train() client id: f_00001-2-7 loss: 0.715783  [  256/  265]
train() client id: f_00002-0-0 loss: 0.760075  [   32/  124]
train() client id: f_00002-0-1 loss: 0.504332  [   64/  124]
train() client id: f_00002-0-2 loss: 1.031874  [   96/  124]
train() client id: f_00002-1-0 loss: 0.831210  [   32/  124]
train() client id: f_00002-1-1 loss: 0.612262  [   64/  124]
train() client id: f_00002-1-2 loss: 0.756587  [   96/  124]
train() client id: f_00002-2-0 loss: 0.730593  [   32/  124]
train() client id: f_00002-2-1 loss: 0.734197  [   64/  124]
train() client id: f_00002-2-2 loss: 0.598653  [   96/  124]
train() client id: f_00003-0-0 loss: 0.716725  [   32/   43]
train() client id: f_00003-1-0 loss: 0.567880  [   32/   43]
train() client id: f_00003-2-0 loss: 0.614218  [   32/   43]
train() client id: f_00004-0-0 loss: 0.914092  [   32/  306]
train() client id: f_00004-0-1 loss: 0.860985  [   64/  306]
train() client id: f_00004-0-2 loss: 0.831065  [   96/  306]
train() client id: f_00004-0-3 loss: 0.918655  [  128/  306]
train() client id: f_00004-0-4 loss: 0.863129  [  160/  306]
train() client id: f_00004-0-5 loss: 1.080627  [  192/  306]
train() client id: f_00004-0-6 loss: 0.972637  [  224/  306]
train() client id: f_00004-0-7 loss: 0.881276  [  256/  306]
train() client id: f_00004-0-8 loss: 0.945043  [  288/  306]
train() client id: f_00004-1-0 loss: 0.824232  [   32/  306]
train() client id: f_00004-1-1 loss: 0.990541  [   64/  306]
train() client id: f_00004-1-2 loss: 0.913638  [   96/  306]
train() client id: f_00004-1-3 loss: 0.991417  [  128/  306]
train() client id: f_00004-1-4 loss: 0.853118  [  160/  306]
train() client id: f_00004-1-5 loss: 0.809555  [  192/  306]
train() client id: f_00004-1-6 loss: 0.927104  [  224/  306]
train() client id: f_00004-1-7 loss: 0.949146  [  256/  306]
train() client id: f_00004-1-8 loss: 1.152256  [  288/  306]
train() client id: f_00004-2-0 loss: 0.960896  [   32/  306]
train() client id: f_00004-2-1 loss: 1.001440  [   64/  306]
train() client id: f_00004-2-2 loss: 0.908295  [   96/  306]
train() client id: f_00004-2-3 loss: 0.863301  [  128/  306]
train() client id: f_00004-2-4 loss: 1.014131  [  160/  306]
train() client id: f_00004-2-5 loss: 0.973607  [  192/  306]
train() client id: f_00004-2-6 loss: 1.006220  [  224/  306]
train() client id: f_00004-2-7 loss: 0.776687  [  256/  306]
train() client id: f_00004-2-8 loss: 0.953165  [  288/  306]
train() client id: f_00005-0-0 loss: 0.652499  [   32/  146]
train() client id: f_00005-0-1 loss: 0.685675  [   64/  146]
train() client id: f_00005-0-2 loss: 1.077775  [   96/  146]
train() client id: f_00005-0-3 loss: 0.816086  [  128/  146]
train() client id: f_00005-1-0 loss: 0.868883  [   32/  146]
train() client id: f_00005-1-1 loss: 0.870021  [   64/  146]
train() client id: f_00005-1-2 loss: 0.898589  [   96/  146]
train() client id: f_00005-1-3 loss: 0.957260  [  128/  146]
train() client id: f_00005-2-0 loss: 0.568947  [   32/  146]
train() client id: f_00005-2-1 loss: 0.953058  [   64/  146]
train() client id: f_00005-2-2 loss: 0.991435  [   96/  146]
train() client id: f_00005-2-3 loss: 0.980169  [  128/  146]
train() client id: f_00006-0-0 loss: 0.498485  [   32/   54]
train() client id: f_00006-1-0 loss: 0.363572  [   32/   54]
train() client id: f_00006-2-0 loss: 0.481560  [   32/   54]
train() client id: f_00007-0-0 loss: 0.632112  [   32/  179]
train() client id: f_00007-0-1 loss: 0.593520  [   64/  179]
train() client id: f_00007-0-2 loss: 0.516279  [   96/  179]
train() client id: f_00007-0-3 loss: 0.603468  [  128/  179]
train() client id: f_00007-0-4 loss: 0.963721  [  160/  179]
train() client id: f_00007-1-0 loss: 0.751350  [   32/  179]
train() client id: f_00007-1-1 loss: 0.458846  [   64/  179]
train() client id: f_00007-1-2 loss: 0.807416  [   96/  179]
train() client id: f_00007-1-3 loss: 0.726220  [  128/  179]
train() client id: f_00007-1-4 loss: 0.465273  [  160/  179]
train() client id: f_00007-2-0 loss: 0.694684  [   32/  179]
train() client id: f_00007-2-1 loss: 0.807722  [   64/  179]
train() client id: f_00007-2-2 loss: 0.503361  [   96/  179]
train() client id: f_00007-2-3 loss: 0.645420  [  128/  179]
train() client id: f_00007-2-4 loss: 0.651266  [  160/  179]
train() client id: f_00008-0-0 loss: 0.835981  [   32/  130]
train() client id: f_00008-0-1 loss: 0.782337  [   64/  130]
train() client id: f_00008-0-2 loss: 0.832905  [   96/  130]
train() client id: f_00008-0-3 loss: 0.661403  [  128/  130]
train() client id: f_00008-1-0 loss: 0.808058  [   32/  130]
train() client id: f_00008-1-1 loss: 0.715590  [   64/  130]
train() client id: f_00008-1-2 loss: 0.742720  [   96/  130]
train() client id: f_00008-1-3 loss: 0.790207  [  128/  130]
train() client id: f_00008-2-0 loss: 0.691900  [   32/  130]
train() client id: f_00008-2-1 loss: 0.870889  [   64/  130]
train() client id: f_00008-2-2 loss: 0.846357  [   96/  130]
train() client id: f_00008-2-3 loss: 0.729686  [  128/  130]
train() client id: f_00009-0-0 loss: 1.003488  [   32/  118]
train() client id: f_00009-0-1 loss: 0.615769  [   64/  118]
train() client id: f_00009-0-2 loss: 0.915498  [   96/  118]
train() client id: f_00009-1-0 loss: 0.733680  [   32/  118]
train() client id: f_00009-1-1 loss: 0.755599  [   64/  118]
train() client id: f_00009-1-2 loss: 0.742503  [   96/  118]
train() client id: f_00009-2-0 loss: 0.734625  [   32/  118]
train() client id: f_00009-2-1 loss: 0.851709  [   64/  118]
train() client id: f_00009-2-2 loss: 0.840594  [   96/  118]
At round 77 accuracy: 0.6472148541114059
At round 77 training accuracy: 0.5828303152246814
At round 77 training loss: 0.8341045354660988
update_location
xs = -4.528292 286.001589 295.045120 -295.943528 184.896481 -220.217951 -337.215960 358.375741 -1.680116 279.695607 
ys = 372.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -302.154970 4.001482 
xs mean: 54.442869159412865
ys mean: 9.371751218646875
dists_uav = 385.800846 303.379125 311.533893 313.194574 210.207352 241.875723 351.744749 372.560000 318.277314 297.061685 
uav_gains = -120.855553 -116.009153 -116.655634 -116.782519 -108.334830 -110.622162 -119.266887 -120.290366 -117.160491 -115.483326 
uav_gains_db_mean: -116.14609211052594
dists_bs = 260.962842 487.795968 501.105721 194.601947 400.472638 178.024239 240.920122 567.621918 507.658003 485.786567 
bs_gains = -107.231262 -114.837729 -115.165081 -103.663180 -112.439099 -102.580475 -106.259505 -116.680716 -115.323054 -114.787533 
bs_gains_db_mean: -110.89676340445494
Round 78
-------------------------------
ene_coms = [0.03120074 0.01568686 0.0187601  0.019012   0.01277914 0.00705719
 0.02530472 0.02888816 0.01641901 0.01561435]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.60810105 1.20509991 0.58078363 0.21989555 1.38366024 0.66159734
 0.2782614  0.84106155 0.60408163 0.5493716 ]
obj_prev = 6.931913904629357
eta_min = nan	eta_max = nan
af = 1.4295611186207509	bf = 0.27922252306299566	zeta = 1.572517230482826	eta = 0.909090909090909
af = 1.4295611186207509	bf = 0.27922252306299566	zeta = 4.570216753520117	eta = 0.3127994131831188
af = 1.4295611186207509	bf = 0.27922252306299566	zeta = 2.8239915602618444	eta = 0.5062200393007549
af = 1.4295611186207509	bf = 0.27922252306299566	zeta = 2.53644282115537	eta = 0.563608651729659
af = 1.4295611186207509	bf = 0.27922252306299566	zeta = 2.518212006153084	eta = 0.5676889456200325
af = 1.4295611186207509	bf = 0.27922252306299566	zeta = 2.5181261788087297	eta = 0.5677082946244755
eta = 0.5677082946244755
ene_coms = [0.03120074 0.01568686 0.0187601  0.019012   0.01277914 0.00705719
 0.02530472 0.02888816 0.01641901 0.01561435]
ene_comp = [0.04671714 0.0982543  0.0459756  0.01594315 0.11345591 0.05413256
 0.02002163 0.066368   0.04820022 0.04375097]
ene_total = [0.26388169 0.38588045 0.21923807 0.11838135 0.42751573 0.20722911
 0.15350513 0.3226006  0.21884364 0.20105041]
ti_comp = [7.39459473 7.54973352 7.51900112 7.51648215 7.5788107  7.63603029
 7.45355492 7.41772059 7.54241204 7.55045863]
ti_coms = [0.31200742 0.15686863 0.18760103 0.19012    0.12779145 0.07057185
 0.25304723 0.28888155 0.1641901  0.15614352]
t_total = [26.0996727 26.0996727 26.0996727 26.0996727 26.0996727 26.0996727
 26.0996727 26.0996727 26.0996727 26.0996727]
ene_coms = [0.03120074 0.01568686 0.0187601  0.019012   0.01277914 0.00705719
 0.02530472 0.02888816 0.01641901 0.01561435]
ene_comp = [1.16541336e-07 1.04009136e-06 1.07434030e-07 4.48304623e-09
 1.58912943e-06 1.70027979e-07 9.02924587e-09 3.32059006e-07
 1.23028764e-07 9.18113702e-08]
ene_total = [0.10566683 0.05312966 0.06353453 0.06438727 0.04328405 0.02390089
 0.08569863 0.0978356  0.05560609 0.05288088]
optimize_network iter = 0 obj = 0.6459244212398697
eta = 0.5677082946244755
freqs = [3158870.85252293 6507137.04849807 3057294.23890184 1060546.04193969
 7485073.49960502 3544548.43795658 1343092.7546009  4473611.46413923
 3195279.11055994 2897239.36562496]
eta_min = 0.567708294624476	eta_max = 0.9268184623773947
af = 9.267086667402938e-06	bf = 0.27922252306299566	zeta = 1.0193795334143232e-05	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03120074 0.01568686 0.0187601  0.019012   0.01277914 0.00705719
 0.02530472 0.02888816 0.01641901 0.01561435]
ene_comp = [1.96195772e-08 1.75097981e-07 1.80863745e-08 7.54714802e-10
 2.67527802e-07 2.86239816e-08 1.52006140e-09 5.59016869e-08
 2.07117268e-08 1.54563207e-08]
ene_total = [0.50246626 0.25262868 0.30211851 0.30617486 0.20580342 0.11365146
 0.40751473 0.46522426 0.26441691 0.25145837]
ti_comp = [1.30865795 1.46379674 1.43306434 1.43054537 1.49287392 1.55009351
 1.36761814 1.33178382 1.45647527 1.46452185]
ti_coms = [0.31200742 0.15686863 0.18760103 0.19012    0.12779145 0.07057185
 0.25304723 0.28888155 0.1641901  0.15614352]
t_total = [26.0996727 26.0996727 26.0996727 26.0996727 26.0996727 26.0996727
 26.0996727 26.0996727 26.0996727 26.0996727]
ene_coms = [0.03120074 0.01568686 0.0187601  0.019012   0.01277914 0.00705719
 0.02530472 0.02888816 0.01641901 0.01561435]
ene_comp = [1.77535580e-08 1.32008572e-07 1.41110633e-08 5.90511225e-10
 1.95408783e-07 1.96865146e-08 1.27961003e-09 4.91493400e-08
 1.57416369e-08 1.16434150e-08]
ene_total = [0.50246623 0.25262799 0.30211845 0.30617486 0.20580226 0.11365131
 0.40751473 0.46522416 0.26441683 0.25145831]
optimize_network iter = 1 obj = 3.0714551193626094
eta = 0.909090909090909
freqs = [3004898.10828131 5650026.07116915 2700482.3833819   938106.55882043
 6397107.49241864 2939546.33496403 1232293.78956882 4194736.1716387
 2785643.85676379 2514615.02077487]
eta_min = 0.9090909090909481	eta_max = 0.9090909090909065
af = 7.0255489424309185e-06	bf = 0.27922252306299566	zeta = 7.728103836674012e-06	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03120074 0.01568686 0.0187601  0.019012   0.01277914 0.00705719
 0.02530472 0.02888816 0.01641901 0.01561435]
ene_comp = [1.77535580e-08 1.32008572e-07 1.41110633e-08 5.90511225e-10
 1.95408783e-07 1.96865146e-08 1.27961003e-09 4.91493400e-08
 1.57416369e-08 1.16434150e-08]
ene_total = [0.50246623 0.25262799 0.30211845 0.30617486 0.20580226 0.11365131
 0.40751473 0.46522416 0.26441683 0.25145831]
ti_comp = [1.30865795 1.46379674 1.43306434 1.43054537 1.49287392 1.55009351
 1.36761814 1.33178382 1.45647527 1.46452185]
ti_coms = [0.31200742 0.15686863 0.18760103 0.19012    0.12779145 0.07057185
 0.25304723 0.28888155 0.1641901  0.15614352]
t_total = [26.0996727 26.0996727 26.0996727 26.0996727 26.0996727 26.0996727
 26.0996727 26.0996727 26.0996727 26.0996727]
ene_coms = [0.03120074 0.01568686 0.0187601  0.019012   0.01277914 0.00705719
 0.02530472 0.02888816 0.01641901 0.01561435]
ene_comp = [1.77535580e-08 1.32008572e-07 1.41110633e-08 5.90511225e-10
 1.95408783e-07 1.96865146e-08 1.27961003e-09 4.91493400e-08
 1.57416369e-08 1.16434150e-08]
ene_total = [0.50246623 0.25262799 0.30211845 0.30617486 0.20580226 0.11365131
 0.40751473 0.46522416 0.26441683 0.25145831]
optimize_network iter = 2 obj = 3.0714551193625264
eta = 0.9090909090909065
freqs = [3004898.1082813  5650026.07116915 2700482.3833819   938106.55882042
 6397107.49241864 2939546.33496404 1232293.78956882 4194736.17163869
 2785643.85676379 2514615.02077487]
Done!
ene_coms = [0.03120074 0.01568686 0.0187601  0.019012   0.01277914 0.00705719
 0.02530472 0.02888816 0.01641901 0.01561435]
ene_comp = [1.70655899e-08 1.26893109e-07 1.35642455e-08 5.67628326e-10
 1.87836498e-07 1.89236425e-08 1.23002387e-09 4.72447540e-08
 1.51316328e-08 1.11922210e-08]
ene_total = [0.03120076 0.01568699 0.01876012 0.019012   0.01277933 0.0070572
 0.02530472 0.0288882  0.01641903 0.01561436]
At round 78 energy consumption: 0.19072271636262375
At round 78 eta: 0.9090909090909065
At round 78 a_n: 1.4640267926476263
At round 78 local rounds: 3.120939520577849
At round 78 global rounds: 16.10429471912343
gradient difference: 0.8416800498962402
train() client id: f_00000-0-0 loss: 1.058728  [   32/  126]
train() client id: f_00000-0-1 loss: 0.882505  [   64/  126]
train() client id: f_00000-0-2 loss: 1.161825  [   96/  126]
train() client id: f_00000-1-0 loss: 1.094486  [   32/  126]
train() client id: f_00000-1-1 loss: 1.029884  [   64/  126]
train() client id: f_00000-1-2 loss: 1.011880  [   96/  126]
train() client id: f_00000-2-0 loss: 1.118453  [   32/  126]
train() client id: f_00000-2-1 loss: 1.108357  [   64/  126]
train() client id: f_00000-2-2 loss: 0.844006  [   96/  126]
train() client id: f_00001-0-0 loss: 0.412370  [   32/  265]
train() client id: f_00001-0-1 loss: 0.455261  [   64/  265]
train() client id: f_00001-0-2 loss: 0.362609  [   96/  265]
train() client id: f_00001-0-3 loss: 0.396608  [  128/  265]
train() client id: f_00001-0-4 loss: 0.570765  [  160/  265]
train() client id: f_00001-0-5 loss: 0.501229  [  192/  265]
train() client id: f_00001-0-6 loss: 0.316294  [  224/  265]
train() client id: f_00001-0-7 loss: 0.420213  [  256/  265]
train() client id: f_00001-1-0 loss: 0.512807  [   32/  265]
train() client id: f_00001-1-1 loss: 0.469476  [   64/  265]
train() client id: f_00001-1-2 loss: 0.483242  [   96/  265]
train() client id: f_00001-1-3 loss: 0.408825  [  128/  265]
train() client id: f_00001-1-4 loss: 0.495195  [  160/  265]
train() client id: f_00001-1-5 loss: 0.314616  [  192/  265]
train() client id: f_00001-1-6 loss: 0.428214  [  224/  265]
train() client id: f_00001-1-7 loss: 0.309686  [  256/  265]
train() client id: f_00001-2-0 loss: 0.447828  [   32/  265]
train() client id: f_00001-2-1 loss: 0.548053  [   64/  265]
train() client id: f_00001-2-2 loss: 0.448552  [   96/  265]
train() client id: f_00001-2-3 loss: 0.389520  [  128/  265]
train() client id: f_00001-2-4 loss: 0.368891  [  160/  265]
train() client id: f_00001-2-5 loss: 0.404295  [  192/  265]
train() client id: f_00001-2-6 loss: 0.452916  [  224/  265]
train() client id: f_00001-2-7 loss: 0.322477  [  256/  265]
train() client id: f_00002-0-0 loss: 0.864789  [   32/  124]
train() client id: f_00002-0-1 loss: 1.040358  [   64/  124]
train() client id: f_00002-0-2 loss: 1.278540  [   96/  124]
train() client id: f_00002-1-0 loss: 0.823470  [   32/  124]
train() client id: f_00002-1-1 loss: 1.106162  [   64/  124]
train() client id: f_00002-1-2 loss: 0.912640  [   96/  124]
train() client id: f_00002-2-0 loss: 0.884333  [   32/  124]
train() client id: f_00002-2-1 loss: 1.135628  [   64/  124]
train() client id: f_00002-2-2 loss: 1.013326  [   96/  124]
train() client id: f_00003-0-0 loss: 0.614526  [   32/   43]
train() client id: f_00003-1-0 loss: 0.603911  [   32/   43]
train() client id: f_00003-2-0 loss: 0.356823  [   32/   43]
train() client id: f_00004-0-0 loss: 0.682341  [   32/  306]
train() client id: f_00004-0-1 loss: 0.819483  [   64/  306]
train() client id: f_00004-0-2 loss: 0.877185  [   96/  306]
train() client id: f_00004-0-3 loss: 0.835619  [  128/  306]
train() client id: f_00004-0-4 loss: 0.870648  [  160/  306]
train() client id: f_00004-0-5 loss: 0.741407  [  192/  306]
train() client id: f_00004-0-6 loss: 1.041128  [  224/  306]
train() client id: f_00004-0-7 loss: 0.795480  [  256/  306]
train() client id: f_00004-0-8 loss: 0.844490  [  288/  306]
train() client id: f_00004-1-0 loss: 0.823646  [   32/  306]
train() client id: f_00004-1-1 loss: 0.653451  [   64/  306]
train() client id: f_00004-1-2 loss: 0.893691  [   96/  306]
train() client id: f_00004-1-3 loss: 0.931807  [  128/  306]
train() client id: f_00004-1-4 loss: 0.849193  [  160/  306]
train() client id: f_00004-1-5 loss: 0.875933  [  192/  306]
train() client id: f_00004-1-6 loss: 0.795765  [  224/  306]
train() client id: f_00004-1-7 loss: 0.908037  [  256/  306]
train() client id: f_00004-1-8 loss: 0.796863  [  288/  306]
train() client id: f_00004-2-0 loss: 0.830313  [   32/  306]
train() client id: f_00004-2-1 loss: 0.846602  [   64/  306]
train() client id: f_00004-2-2 loss: 0.823571  [   96/  306]
train() client id: f_00004-2-3 loss: 0.969287  [  128/  306]
train() client id: f_00004-2-4 loss: 0.938993  [  160/  306]
train() client id: f_00004-2-5 loss: 0.651738  [  192/  306]
train() client id: f_00004-2-6 loss: 0.740454  [  224/  306]
train() client id: f_00004-2-7 loss: 0.914792  [  256/  306]
train() client id: f_00004-2-8 loss: 1.003202  [  288/  306]
train() client id: f_00005-0-0 loss: 0.588108  [   32/  146]
train() client id: f_00005-0-1 loss: 0.651686  [   64/  146]
train() client id: f_00005-0-2 loss: 0.314996  [   96/  146]
train() client id: f_00005-0-3 loss: 0.393355  [  128/  146]
train() client id: f_00005-1-0 loss: 0.717992  [   32/  146]
train() client id: f_00005-1-1 loss: 0.275914  [   64/  146]
train() client id: f_00005-1-2 loss: 0.368065  [   96/  146]
train() client id: f_00005-1-3 loss: 0.574046  [  128/  146]
train() client id: f_00005-2-0 loss: 0.751116  [   32/  146]
train() client id: f_00005-2-1 loss: 0.382915  [   64/  146]
train() client id: f_00005-2-2 loss: 0.614815  [   96/  146]
train() client id: f_00005-2-3 loss: 0.255976  [  128/  146]
train() client id: f_00006-0-0 loss: 0.497176  [   32/   54]
train() client id: f_00006-1-0 loss: 0.437753  [   32/   54]
train() client id: f_00006-2-0 loss: 0.470658  [   32/   54]
train() client id: f_00007-0-0 loss: 0.560800  [   32/  179]
train() client id: f_00007-0-1 loss: 0.720119  [   64/  179]
train() client id: f_00007-0-2 loss: 0.659213  [   96/  179]
train() client id: f_00007-0-3 loss: 0.626768  [  128/  179]
train() client id: f_00007-0-4 loss: 1.034878  [  160/  179]
train() client id: f_00007-1-0 loss: 0.595018  [   32/  179]
train() client id: f_00007-1-1 loss: 0.932199  [   64/  179]
train() client id: f_00007-1-2 loss: 0.663177  [   96/  179]
train() client id: f_00007-1-3 loss: 0.574159  [  128/  179]
train() client id: f_00007-1-4 loss: 0.805140  [  160/  179]
train() client id: f_00007-2-0 loss: 0.751264  [   32/  179]
train() client id: f_00007-2-1 loss: 0.654429  [   64/  179]
train() client id: f_00007-2-2 loss: 0.681531  [   96/  179]
train() client id: f_00007-2-3 loss: 0.821824  [  128/  179]
train() client id: f_00007-2-4 loss: 0.652517  [  160/  179]
train() client id: f_00008-0-0 loss: 0.750284  [   32/  130]
train() client id: f_00008-0-1 loss: 0.761449  [   64/  130]
train() client id: f_00008-0-2 loss: 0.735971  [   96/  130]
train() client id: f_00008-0-3 loss: 0.766490  [  128/  130]
train() client id: f_00008-1-0 loss: 0.735836  [   32/  130]
train() client id: f_00008-1-1 loss: 0.832699  [   64/  130]
train() client id: f_00008-1-2 loss: 0.734984  [   96/  130]
train() client id: f_00008-1-3 loss: 0.714780  [  128/  130]
train() client id: f_00008-2-0 loss: 0.711308  [   32/  130]
train() client id: f_00008-2-1 loss: 0.722143  [   64/  130]
train() client id: f_00008-2-2 loss: 0.767698  [   96/  130]
train() client id: f_00008-2-3 loss: 0.814456  [  128/  130]
train() client id: f_00009-0-0 loss: 0.718571  [   32/  118]
train() client id: f_00009-0-1 loss: 0.856702  [   64/  118]
train() client id: f_00009-0-2 loss: 0.720055  [   96/  118]
train() client id: f_00009-1-0 loss: 0.660711  [   32/  118]
train() client id: f_00009-1-1 loss: 0.685885  [   64/  118]
train() client id: f_00009-1-2 loss: 0.800875  [   96/  118]
train() client id: f_00009-2-0 loss: 0.756563  [   32/  118]
train() client id: f_00009-2-1 loss: 0.599026  [   64/  118]
train() client id: f_00009-2-2 loss: 0.860362  [   96/  118]
At round 78 accuracy: 0.6472148541114059
At round 78 training accuracy: 0.5902079141515761
At round 78 training loss: 0.80478382563044
update_location
xs = -4.528292 291.001589 300.045120 -300.943528 189.896481 -225.217951 -342.215960 363.375741 -1.680116 284.695607 
ys = 377.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -307.154970 4.001482 
xs mean: 55.442869159412865
ys mean: 9.371751218646875
dists_uav = 390.631761 308.097240 316.273328 317.923381 214.618489 246.436695 356.541061 377.372113 323.027860 301.774089 
uav_gains = -121.048102 -116.387857 -117.013346 -117.134682 -108.625605 -110.996150 -119.519475 -120.502553 -117.499479 -115.877519 
uav_gains_db_mean: -116.46047668381142
dists_bs = 264.768738 492.524032 505.798769 197.748206 404.971973 179.359442 244.314684 572.322810 512.360417 490.469718 
bs_gains = -107.407327 -114.955027 -115.278437 -103.858211 -112.574958 -102.671338 -106.429647 -116.781009 -115.435175 -114.904201 
bs_gains_db_mean: -111.02953290753644
Round 79
-------------------------------
ene_coms = [0.03205063 0.0158586  0.0194848  0.01974113 0.01291757 0.00708702
 0.02612384 0.02972584 0.01659653 0.01578379]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.46678329 0.92333106 0.44571566 0.16927143 1.06007498 0.50683382
 0.21408306 0.64522301 0.46294265 0.42102425]
obj_prev = 5.3152831974382115
eta_min = 5.850922661923953e-201	eta_max = 0.9931597935406788
af = 1.0950793819260525	bf = 0.21910344452770567	zeta = 1.2045873201186579	eta = 0.909090909090909
af = 1.0950793819260525	bf = 0.21910344452770567	zeta = 3.5582322305099483	eta = 0.3077593903333035
af = 1.0950793819260525	bf = 0.21910344452770567	zeta = 2.1807237060520963	eta = 0.5021632859251779
af = 1.0950793819260525	bf = 0.21910344452770567	zeta = 1.955313626939984	eta = 0.5600530609710033
af = 1.0950793819260525	bf = 0.21910344452770567	zeta = 1.941022638022073	eta = 0.5641765121513227
af = 1.0950793819260525	bf = 0.21910344452770567	zeta = 1.9409551977565964	eta = 0.564196114980795
eta = 0.564196114980795
ene_coms = [0.03205063 0.0158586  0.0194848  0.01974113 0.01291757 0.00708702
 0.02612384 0.02972584 0.01659653 0.01578379]
ene_comp = [0.04722923 0.09933131 0.04647956 0.01611791 0.11469955 0.05472593
 0.0202411  0.06709549 0.04872857 0.04423055]
ene_total = [0.20401574 0.29642528 0.16975015 0.09227828 0.328405   0.15906706
 0.11931375 0.24915628 0.16810509 0.15443857]
ti_comp = [ 9.80231101  9.96423135  9.92796928  9.92540599  9.99364159 10.05194716
  9.86157891  9.82555893  9.95685205  9.96497946]
ti_coms = [0.32050631 0.15858596 0.19484803 0.19741133 0.12917572 0.07087016
 0.26123841 0.29725839 0.16596526 0.15783786]
t_total = [26.0496685 26.0496685 26.0496685 26.0496685 26.0496685 26.0496685
 26.0496685 26.0496685 26.0496685 26.0496685]
ene_coms = [0.03205063 0.0158586  0.0194848  0.01974113 0.01291757 0.00708702
 0.02612384 0.02972584 0.01659653 0.01578379]
ene_comp = [6.85260345e-08 6.16951376e-07 6.36715066e-08 2.65649943e-09
 9.44316677e-07 1.01381589e-07 5.32953467e-09 1.95544445e-07
 7.29435456e-08 5.44620679e-08]
ene_total = [0.08247804 0.04081149 0.05014161 0.05080108 0.03324401 0.01823771
 0.0672261  0.07649583 0.04270905 0.04061753]
optimize_network iter = 0 obj = 0.5027624490853012
eta = 0.564196114980795
freqs = [2409086.38269274 4984394.13978786 2340839.11460492  811952.23372434
 5738626.5294631  2722155.71296966 1026260.49394137 3414334.54175535
 2446986.6706681  2219299.44705426]
eta_min = 0.5641961149807955	eta_max = 0.9426757264697605
af = 4.165303428053543e-06	bf = 0.21910344452770567	zeta = 4.581833770858898e-06	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03205063 0.0158586  0.0194848  0.01974113 0.01291757 0.00708702
 0.02612384 0.02972584 0.01659653 0.01578379]
ene_comp = [1.14111824e-08 1.02736788e-07 1.06027903e-08 4.42369091e-10
 1.57250743e-07 1.68823983e-08 8.87491779e-10 3.25627093e-08
 1.21468010e-08 9.06920407e-09]
ene_total = [0.39538603 0.19563752 0.24037032 0.24353236 0.15935686 0.08742769
 0.3222713  0.36670698 0.20473972 0.19471348]
ti_comp = [1.79112269 1.95304303 1.91678096 1.91421766 1.98245327 2.04075884
 1.85039059 1.81437061 1.94566373 1.95379114]
ti_coms = [0.32050631 0.15858596 0.19484803 0.19741133 0.12917572 0.07087016
 0.26123841 0.29725839 0.16596526 0.15783786]
t_total = [26.0496685 26.0496685 26.0496685 26.0496685 26.0496685 26.0496685
 26.0496685 26.0496685 26.0496685 26.0496685]
ene_coms = [0.03205063 0.0158586  0.0194848  0.01974113 0.01291757 0.00708702
 0.02612384 0.02972584 0.01659653 0.01578379]
ene_comp = [9.47735238e-09 7.41549353e-08 7.88762299e-09 3.29798587e-10
 1.10811395e-07 1.13579841e-08 6.99005383e-10 2.64809251e-08
 8.82105547e-09 6.54207946e-09]
ene_total = [0.39538601 0.19563717 0.24037029 0.24353235 0.15935629 0.08742762
 0.32227129 0.36670691 0.20473968 0.19471345]
optimize_network iter = 1 obj = 2.4101410548299946
eta = 0.909090909090909
freqs = [2195485.44949481 4234668.46273771 2018991.78049723  701071.78468256
 4817301.40339007 2232783.02734844  910784.64812482 3079019.09450611
 2085263.40586046 1884903.95303591]
eta_min = 0.9090909090908983	eta_max = 0.9090909090908986
af = 3.0188755023881786e-06	bf = 0.21910344452770567	zeta = 3.3207630526269965e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03205063 0.0158586  0.0194848  0.01974113 0.01291757 0.00708702
 0.02612384 0.02972584 0.01659653 0.01578379]
ene_comp = [9.47735238e-09 7.41549353e-08 7.88762299e-09 3.29798587e-10
 1.10811395e-07 1.13579841e-08 6.99005383e-10 2.64809251e-08
 8.82105547e-09 6.54207946e-09]
ene_total = [0.39538601 0.19563717 0.24037029 0.24353235 0.15935629 0.08742762
 0.32227129 0.36670691 0.20473968 0.19471345]
ti_comp = [1.79112269 1.95304303 1.91678096 1.91421766 1.98245327 2.04075884
 1.85039059 1.81437061 1.94566373 1.95379114]
ti_coms = [0.32050631 0.15858596 0.19484803 0.19741133 0.12917572 0.07087016
 0.26123841 0.29725839 0.16596526 0.15783786]
t_total = [26.0496685 26.0496685 26.0496685 26.0496685 26.0496685 26.0496685
 26.0496685 26.0496685 26.0496685 26.0496685]
ene_coms = [0.03205063 0.0158586  0.0194848  0.01974113 0.01291757 0.00708702
 0.02612384 0.02972584 0.01659653 0.01578379]
ene_comp = [9.47735238e-09 7.41549353e-08 7.88762299e-09 3.29798587e-10
 1.10811395e-07 1.13579841e-08 6.99005383e-10 2.64809251e-08
 8.82105547e-09 6.54207946e-09]
ene_total = [0.39538601 0.19563717 0.24037029 0.24353235 0.15935629 0.08742762
 0.32227129 0.36670691 0.20473968 0.19471345]
optimize_network iter = 2 obj = 2.410141054829721
eta = 0.9090909090908986
freqs = [2195485.44949479 4234668.46273771 2018991.78049723  701071.78468256
 4817301.40339007 2232783.02734844  910784.64812481 3079019.09450607
 2085263.40586045 1884903.95303591]
Done!
ene_coms = [0.03205063 0.0158586  0.0194848  0.01974113 0.01291757 0.00708702
 0.02612384 0.02972584 0.01659653 0.01578379]
ene_comp = [9.11009552e-09 7.12813575e-08 7.58196973e-09 3.17018562e-10
 1.06517343e-07 1.09178509e-08 6.71918227e-10 2.54547628e-08
 8.47923077e-09 6.28856735e-09]
ene_total = [0.03205064 0.01585867 0.01948481 0.01974113 0.01291768 0.00708703
 0.02612384 0.02972586 0.01659653 0.01578379]
At round 79 energy consumption: 0.1953699899688828
At round 79 eta: 0.9090909090908986
At round 79 a_n: 1.1214809456783108
At round 79 local rounds: 3.1209395205781334
At round 79 global rounds: 12.336290402459998
gradient difference: 1.011549949645996
train() client id: f_00000-0-0 loss: 1.202445  [   32/  126]
train() client id: f_00000-0-1 loss: 1.040789  [   64/  126]
train() client id: f_00000-0-2 loss: 1.128201  [   96/  126]
train() client id: f_00000-1-0 loss: 0.943783  [   32/  126]
train() client id: f_00000-1-1 loss: 0.826773  [   64/  126]
train() client id: f_00000-1-2 loss: 1.326954  [   96/  126]
train() client id: f_00000-2-0 loss: 1.139409  [   32/  126]
train() client id: f_00000-2-1 loss: 1.060194  [   64/  126]
train() client id: f_00000-2-2 loss: 1.033491  [   96/  126]
train() client id: f_00001-0-0 loss: 0.689674  [   32/  265]
train() client id: f_00001-0-1 loss: 0.643760  [   64/  265]
train() client id: f_00001-0-2 loss: 0.530324  [   96/  265]
train() client id: f_00001-0-3 loss: 0.597342  [  128/  265]
train() client id: f_00001-0-4 loss: 0.535595  [  160/  265]
train() client id: f_00001-0-5 loss: 0.715945  [  192/  265]
train() client id: f_00001-0-6 loss: 0.557029  [  224/  265]
train() client id: f_00001-0-7 loss: 0.537885  [  256/  265]
train() client id: f_00001-1-0 loss: 0.545946  [   32/  265]
train() client id: f_00001-1-1 loss: 0.800371  [   64/  265]
train() client id: f_00001-1-2 loss: 0.568816  [   96/  265]
train() client id: f_00001-1-3 loss: 0.597290  [  128/  265]
train() client id: f_00001-1-4 loss: 0.521591  [  160/  265]
train() client id: f_00001-1-5 loss: 0.728753  [  192/  265]
train() client id: f_00001-1-6 loss: 0.560973  [  224/  265]
train() client id: f_00001-1-7 loss: 0.557022  [  256/  265]
train() client id: f_00001-2-0 loss: 0.585559  [   32/  265]
train() client id: f_00001-2-1 loss: 0.689775  [   64/  265]
train() client id: f_00001-2-2 loss: 0.562995  [   96/  265]
train() client id: f_00001-2-3 loss: 0.660940  [  128/  265]
train() client id: f_00001-2-4 loss: 0.502246  [  160/  265]
train() client id: f_00001-2-5 loss: 0.684544  [  192/  265]
train() client id: f_00001-2-6 loss: 0.524140  [  224/  265]
train() client id: f_00001-2-7 loss: 0.594193  [  256/  265]
train() client id: f_00002-0-0 loss: 0.788109  [   32/  124]
train() client id: f_00002-0-1 loss: 1.090978  [   64/  124]
train() client id: f_00002-0-2 loss: 0.915903  [   96/  124]
train() client id: f_00002-1-0 loss: 0.932967  [   32/  124]
train() client id: f_00002-1-1 loss: 1.094756  [   64/  124]
train() client id: f_00002-1-2 loss: 0.972136  [   96/  124]
train() client id: f_00002-2-0 loss: 1.203151  [   32/  124]
train() client id: f_00002-2-1 loss: 0.831837  [   64/  124]
train() client id: f_00002-2-2 loss: 0.902254  [   96/  124]
train() client id: f_00003-0-0 loss: 0.793363  [   32/   43]
train() client id: f_00003-1-0 loss: 0.817246  [   32/   43]
train() client id: f_00003-2-0 loss: 0.675780  [   32/   43]
train() client id: f_00004-0-0 loss: 0.711037  [   32/  306]
train() client id: f_00004-0-1 loss: 0.693340  [   64/  306]
train() client id: f_00004-0-2 loss: 0.752857  [   96/  306]
train() client id: f_00004-0-3 loss: 0.693371  [  128/  306]
train() client id: f_00004-0-4 loss: 0.526603  [  160/  306]
train() client id: f_00004-0-5 loss: 0.654846  [  192/  306]
train() client id: f_00004-0-6 loss: 0.674554  [  224/  306]
train() client id: f_00004-0-7 loss: 0.602841  [  256/  306]
train() client id: f_00004-0-8 loss: 0.914273  [  288/  306]
train() client id: f_00004-1-0 loss: 0.746042  [   32/  306]
train() client id: f_00004-1-1 loss: 0.599837  [   64/  306]
train() client id: f_00004-1-2 loss: 0.577903  [   96/  306]
train() client id: f_00004-1-3 loss: 0.719053  [  128/  306]
train() client id: f_00004-1-4 loss: 0.770173  [  160/  306]
train() client id: f_00004-1-5 loss: 0.663529  [  192/  306]
train() client id: f_00004-1-6 loss: 0.756465  [  224/  306]
train() client id: f_00004-1-7 loss: 0.739769  [  256/  306]
train() client id: f_00004-1-8 loss: 0.698063  [  288/  306]
train() client id: f_00004-2-0 loss: 0.658588  [   32/  306]
train() client id: f_00004-2-1 loss: 0.793569  [   64/  306]
train() client id: f_00004-2-2 loss: 0.666666  [   96/  306]
train() client id: f_00004-2-3 loss: 0.712441  [  128/  306]
train() client id: f_00004-2-4 loss: 0.609677  [  160/  306]
train() client id: f_00004-2-5 loss: 0.689355  [  192/  306]
train() client id: f_00004-2-6 loss: 0.821937  [  224/  306]
train() client id: f_00004-2-7 loss: 0.640905  [  256/  306]
train() client id: f_00004-2-8 loss: 0.660763  [  288/  306]
train() client id: f_00005-0-0 loss: 1.066730  [   32/  146]
train() client id: f_00005-0-1 loss: 1.103047  [   64/  146]
train() client id: f_00005-0-2 loss: 0.827100  [   96/  146]
train() client id: f_00005-0-3 loss: 0.519409  [  128/  146]
train() client id: f_00005-1-0 loss: 0.743027  [   32/  146]
train() client id: f_00005-1-1 loss: 0.809125  [   64/  146]
train() client id: f_00005-1-2 loss: 1.042268  [   96/  146]
train() client id: f_00005-1-3 loss: 0.735939  [  128/  146]
train() client id: f_00005-2-0 loss: 0.681206  [   32/  146]
train() client id: f_00005-2-1 loss: 1.148280  [   64/  146]
train() client id: f_00005-2-2 loss: 0.760691  [   96/  146]
train() client id: f_00005-2-3 loss: 0.777342  [  128/  146]
train() client id: f_00006-0-0 loss: 0.472533  [   32/   54]
train() client id: f_00006-1-0 loss: 0.429379  [   32/   54]
train() client id: f_00006-2-0 loss: 0.546360  [   32/   54]
train() client id: f_00007-0-0 loss: 0.715198  [   32/  179]
train() client id: f_00007-0-1 loss: 0.618853  [   64/  179]
train() client id: f_00007-0-2 loss: 0.868995  [   96/  179]
train() client id: f_00007-0-3 loss: 0.623613  [  128/  179]
train() client id: f_00007-0-4 loss: 0.701224  [  160/  179]
train() client id: f_00007-1-0 loss: 0.601006  [   32/  179]
train() client id: f_00007-1-1 loss: 0.808703  [   64/  179]
train() client id: f_00007-1-2 loss: 0.540106  [   96/  179]
train() client id: f_00007-1-3 loss: 0.826940  [  128/  179]
train() client id: f_00007-1-4 loss: 0.725574  [  160/  179]
train() client id: f_00007-2-0 loss: 0.661826  [   32/  179]
train() client id: f_00007-2-1 loss: 0.739872  [   64/  179]
train() client id: f_00007-2-2 loss: 0.623886  [   96/  179]
train() client id: f_00007-2-3 loss: 0.573263  [  128/  179]
train() client id: f_00007-2-4 loss: 0.881764  [  160/  179]
train() client id: f_00008-0-0 loss: 0.621791  [   32/  130]
train() client id: f_00008-0-1 loss: 0.618217  [   64/  130]
train() client id: f_00008-0-2 loss: 0.720681  [   96/  130]
train() client id: f_00008-0-3 loss: 0.662770  [  128/  130]
train() client id: f_00008-1-0 loss: 0.699805  [   32/  130]
train() client id: f_00008-1-1 loss: 0.625816  [   64/  130]
train() client id: f_00008-1-2 loss: 0.577392  [   96/  130]
train() client id: f_00008-1-3 loss: 0.677557  [  128/  130]
train() client id: f_00008-2-0 loss: 0.737396  [   32/  130]
train() client id: f_00008-2-1 loss: 0.589624  [   64/  130]
train() client id: f_00008-2-2 loss: 0.723581  [   96/  130]
train() client id: f_00008-2-3 loss: 0.574588  [  128/  130]
train() client id: f_00009-0-0 loss: 0.607068  [   32/  118]
train() client id: f_00009-0-1 loss: 0.578646  [   64/  118]
train() client id: f_00009-0-2 loss: 0.681363  [   96/  118]
train() client id: f_00009-1-0 loss: 0.408397  [   32/  118]
train() client id: f_00009-1-1 loss: 0.698228  [   64/  118]
train() client id: f_00009-1-2 loss: 0.657248  [   96/  118]
train() client id: f_00009-2-0 loss: 0.498792  [   32/  118]
train() client id: f_00009-2-1 loss: 0.642215  [   64/  118]
train() client id: f_00009-2-2 loss: 0.598559  [   96/  118]
At round 79 accuracy: 0.6472148541114059
At round 79 training accuracy: 0.5928906773977196
At round 79 training loss: 0.8083265126263691
update_location
xs = -4.528292 296.001589 305.045120 -305.943528 194.896481 -230.217951 -347.215960 368.375741 -1.680116 289.695607 
ys = 382.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -312.154970 4.001482 
xs mean: 56.442869159412865
ys mean: 9.371751218646875
dists_uav = 395.466879 312.824112 321.020668 322.660366 219.054926 251.014391 361.342895 382.189048 327.785826 306.495606 
uav_gains = -121.234320 -116.754357 -117.357938 -117.473749 -108.925323 -111.382519 -119.761794 -120.707058 -117.825179 -116.260710 
uav_gains_db_mean: -116.76829475768173
dists_bs = 268.613782 497.257416 510.497645 200.969620 409.482922 180.823087 247.763646 577.028731 517.068416 495.159066 
bs_gains = -107.582651 -115.071335 -115.390884 -104.054711 -112.709661 -102.770168 -106.600111 -116.880587 -115.546403 -115.019912 
bs_gains_db_mean: -111.16264238626472
Round 80
-------------------------------
ene_coms = [0.03290481 0.01603211 0.02022741 0.02048739 0.01305752 0.00711974
 0.02694822 0.03056752 0.01677589 0.01595499]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.32488078 0.64144476 0.31016027 0.11815621 0.73639512 0.35205193
 0.14934202 0.44880793 0.32168228 0.29256104]
obj_prev = 3.6954823404026045
eta_min = 6.147464199023544e-289	eta_max = 0.9960821503707278
af = 0.7605976452313506	bf = 0.15584591764641556	zeta = 0.8366574097544858	eta = 0.9090909090909091
af = 0.7605976452313506	bf = 0.15584591764641556	zeta = 2.511724775693778	eta = 0.30281886478635445
af = 0.7605976452313506	bf = 0.15584591764641556	zeta = 1.526821899425362	eta = 0.4981574114948252
af = 0.7605976452313506	bf = 0.15584591764641556	zeta = 1.3666838188472434	eta = 0.5565278777302652
af = 0.7605976452313506	bf = 0.15584591764641556	zeta = 1.3565340897098974	eta = 0.5606918771897644
af = 0.7605976452313506	bf = 0.15584591764641556	zeta = 1.3564861069268896	eta = 0.5607117104608462
eta = 0.5607117104608462
ene_coms = [0.03290481 0.01603211 0.02022741 0.02048739 0.01305752 0.00711974
 0.02694822 0.03056752 0.01677589 0.01595499]
ene_comp = [0.04774043 0.10040645 0.04698264 0.01629237 0.11594104 0.05531827
 0.02046018 0.06782172 0.049256   0.04470929]
ene_total = [0.14299814 0.20646597 0.1191752  0.06521695 0.22873703 0.11071354
 0.08406341 0.17446136 0.1170861  0.1075684 ]
ti_comp = [14.33372425 14.50245132 14.46049827 14.4578985  14.53219722 14.59157496
 14.39329017 14.35709714 14.49501349 14.50322248]
ti_coms = [0.32904815 0.16032107 0.20227412 0.2048739  0.13057517 0.07119743
 0.26948222 0.30567525 0.1677589  0.15954992]
t_total = [25.99966431 25.99966431 25.99966431 25.99966431 25.99966431 25.99966431
 25.99966431 25.99966431 25.99966431 25.99966431]
ene_coms = [0.03290481 0.01603211 0.02022741 0.02048739 0.01305752 0.00711974
 0.02694822 0.03056752 0.01677589 0.01595499]
ene_comp = [3.30994525e-08 3.00802925e-07 3.09974384e-08 1.29307075e-09
 4.61242140e-07 4.96914296e-08 2.58397545e-09 9.45917109e-08
 3.55484013e-08 2.65548755e-08]
ene_total = [0.05834606 0.02842827 0.0358668  0.03632773 0.02315408 0.01262464
 0.04778392 0.05420174 0.02974666 0.02829104]
optimize_network iter = 0 obj = 0.35477095258439006
eta = 0.5607117104608462
freqs = [1665318.29657711 3461706.2429589  1624516.70308743  563441.76770766
 3989108.94126757 1895555.21326114  710754.19638071 2361957.86387496
 1699066.91055328 1541357.04593011]
eta_min = 0.5607117104608473	eta_max = 0.9593007430365251
af = 1.3953760315203215e-06	bf = 0.15584591764641556	zeta = 1.5349136346723538e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03290481 0.01603211 0.02022741 0.02048739 0.01305752 0.00711974
 0.02694822 0.03056752 0.01677589 0.01595499]
ene_comp = [5.45281055e-09 4.95543351e-08 5.10652432e-09 2.13020740e-10
 7.59851241e-08 8.18617625e-09 4.25684642e-10 1.55830577e-08
 5.85625088e-09 4.37465560e-09]
ene_total = [0.28193791 0.13736811 0.1733143  0.17554182 0.11188119 0.06100407
 0.23090008 0.26191143 0.14374068 0.13670697]
ti_comp = [2.70535851 2.87408558 2.83213253 2.82953276 2.90383148 2.96320922
 2.76492443 2.7287314  2.86664775 2.87485674]
ti_coms = [0.32904815 0.16032107 0.20227412 0.2048739  0.13057517 0.07119743
 0.26948222 0.30567525 0.1677589  0.15954992]
t_total = [25.99966431 25.99966431 25.99966431 25.99966431 25.99966431 25.99966431
 25.99966431 25.99966431 25.99966431 25.99966431]
ene_coms = [0.03290481 0.01603211 0.02022741 0.02048739 0.01305752 0.00711974
 0.02694822 0.03056752 0.01677589 0.01595499]
ene_comp = [4.15420619e-09 3.42424076e-08 3.61296682e-09 1.50939177e-10
 5.16472881e-08 5.38716153e-09 3.13069536e-10 1.17074841e-08
 4.06356730e-09 3.02161877e-09]
ene_total = [0.2819379  0.13736798 0.17331429 0.17554182 0.11188098 0.06100404
 0.23090008 0.2619114  0.14374066 0.13670696]
optimize_network iter = 1 obj = 1.714306107744891
eta = 0.9090909090909091
freqs = [1453553.67364096 2877607.32942196 1366449.12142298  474284.66459377
 3288784.14082311 1537715.14698083  609531.06715376 2047281.65429789
 1415319.120671   1281005.96768791]
eta_min = 0.909090909090903	eta_max = 0.909090909090888
af = 9.66828252642489e-07	bf = 0.15584591764641556	zeta = 1.063511077906738e-06	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03290481 0.01603211 0.02022741 0.02048739 0.01305752 0.00711974
 0.02694822 0.03056752 0.01677589 0.01595499]
ene_comp = [4.15420619e-09 3.42424076e-08 3.61296682e-09 1.50939177e-10
 5.16472881e-08 5.38716153e-09 3.13069536e-10 1.17074841e-08
 4.06356730e-09 3.02161877e-09]
ene_total = [0.2819379  0.13736798 0.17331429 0.17554182 0.11188098 0.06100404
 0.23090008 0.2619114  0.14374066 0.13670696]
ti_comp = [2.70535851 2.87408558 2.83213253 2.82953276 2.90383148 2.96320922
 2.76492443 2.7287314  2.86664775 2.87485674]
ti_coms = [0.32904815 0.16032107 0.20227412 0.2048739  0.13057517 0.07119743
 0.26948222 0.30567525 0.1677589  0.15954992]
t_total = [25.99966431 25.99966431 25.99966431 25.99966431 25.99966431 25.99966431
 25.99966431 25.99966431 25.99966431 25.99966431]
ene_coms = [0.03290481 0.01603211 0.02022741 0.02048739 0.01305752 0.00711974
 0.02694822 0.03056752 0.01677589 0.01595499]
ene_comp = [4.15420619e-09 3.42424076e-08 3.61296682e-09 1.50939177e-10
 5.16472881e-08 5.38716153e-09 3.13069536e-10 1.17074841e-08
 4.06356730e-09 3.02161877e-09]
ene_total = [0.2819379  0.13736798 0.17331429 0.17554182 0.11188098 0.06100404
 0.23090008 0.2619114  0.14374066 0.13670696]
optimize_network iter = 2 obj = 1.7143061077444932
eta = 0.909090909090888
freqs = [1453553.67364094 2877607.32942195 1366449.12142297  474284.66459377
 3288784.14082312 1537715.14698084  609531.06715376 2047281.65429786
 1415319.120671   1281005.96768791]
Done!
ene_coms = [0.03290481 0.01603211 0.02022741 0.02048739 0.01305752 0.00711974
 0.02694822 0.03056752 0.01677589 0.01595499]
ene_comp = [3.99322655e-09 3.29154802e-08 3.47296075e-09 1.45090133e-10
 4.96459042e-08 5.17840364e-09 3.00937779e-10 1.12538074e-08
 3.90610002e-09 2.90452803e-09]
ene_total = [0.03290482 0.01603214 0.02022742 0.02048739 0.01305757 0.00711975
 0.02694822 0.03056754 0.01677589 0.01595499]
At round 80 energy consumption: 0.20007572708232088
At round 80 eta: 0.909090909090888
At round 80 a_n: 0.7789350987089918
At round 80 local rounds: 3.120939520578517
At round 80 global rounds: 8.568286085796919
gradient difference: 0.9834718704223633
train() client id: f_00000-0-0 loss: 0.928066  [   32/  126]
train() client id: f_00000-0-1 loss: 0.898573  [   64/  126]
train() client id: f_00000-0-2 loss: 0.920281  [   96/  126]
train() client id: f_00000-1-0 loss: 0.793416  [   32/  126]
train() client id: f_00000-1-1 loss: 0.973470  [   64/  126]
train() client id: f_00000-1-2 loss: 0.958021  [   96/  126]
train() client id: f_00000-2-0 loss: 0.895171  [   32/  126]
train() client id: f_00000-2-1 loss: 0.701436  [   64/  126]
train() client id: f_00000-2-2 loss: 1.082794  [   96/  126]
train() client id: f_00001-0-0 loss: 0.485297  [   32/  265]
train() client id: f_00001-0-1 loss: 0.640456  [   64/  265]
train() client id: f_00001-0-2 loss: 0.566352  [   96/  265]
train() client id: f_00001-0-3 loss: 0.493093  [  128/  265]
train() client id: f_00001-0-4 loss: 0.525919  [  160/  265]
train() client id: f_00001-0-5 loss: 0.590249  [  192/  265]
train() client id: f_00001-0-6 loss: 0.586204  [  224/  265]
train() client id: f_00001-0-7 loss: 0.583959  [  256/  265]
train() client id: f_00001-1-0 loss: 0.563493  [   32/  265]
train() client id: f_00001-1-1 loss: 0.569631  [   64/  265]
train() client id: f_00001-1-2 loss: 0.592014  [   96/  265]
train() client id: f_00001-1-3 loss: 0.548695  [  128/  265]
train() client id: f_00001-1-4 loss: 0.598643  [  160/  265]
train() client id: f_00001-1-5 loss: 0.615313  [  192/  265]
train() client id: f_00001-1-6 loss: 0.527904  [  224/  265]
train() client id: f_00001-1-7 loss: 0.539120  [  256/  265]
train() client id: f_00001-2-0 loss: 0.656586  [   32/  265]
train() client id: f_00001-2-1 loss: 0.611398  [   64/  265]
train() client id: f_00001-2-2 loss: 0.495972  [   96/  265]
train() client id: f_00001-2-3 loss: 0.475679  [  128/  265]
train() client id: f_00001-2-4 loss: 0.538301  [  160/  265]
train() client id: f_00001-2-5 loss: 0.679003  [  192/  265]
train() client id: f_00001-2-6 loss: 0.545222  [  224/  265]
train() client id: f_00001-2-7 loss: 0.551872  [  256/  265]
train() client id: f_00002-0-0 loss: 1.039902  [   32/  124]
train() client id: f_00002-0-1 loss: 1.033180  [   64/  124]
train() client id: f_00002-0-2 loss: 1.009493  [   96/  124]
train() client id: f_00002-1-0 loss: 1.105336  [   32/  124]
train() client id: f_00002-1-1 loss: 0.981497  [   64/  124]
train() client id: f_00002-1-2 loss: 1.010319  [   96/  124]
train() client id: f_00002-2-0 loss: 1.033427  [   32/  124]
train() client id: f_00002-2-1 loss: 0.967413  [   64/  124]
train() client id: f_00002-2-2 loss: 0.921557  [   96/  124]
train() client id: f_00003-0-0 loss: 0.927153  [   32/   43]
train() client id: f_00003-1-0 loss: 0.545725  [   32/   43]
train() client id: f_00003-2-0 loss: 0.805979  [   32/   43]
train() client id: f_00004-0-0 loss: 0.753797  [   32/  306]
train() client id: f_00004-0-1 loss: 0.854374  [   64/  306]
train() client id: f_00004-0-2 loss: 0.827842  [   96/  306]
train() client id: f_00004-0-3 loss: 0.884631  [  128/  306]
train() client id: f_00004-0-4 loss: 0.852096  [  160/  306]
train() client id: f_00004-0-5 loss: 0.904230  [  192/  306]
train() client id: f_00004-0-6 loss: 1.033107  [  224/  306]
train() client id: f_00004-0-7 loss: 0.971912  [  256/  306]
train() client id: f_00004-0-8 loss: 0.822438  [  288/  306]
train() client id: f_00004-1-0 loss: 0.867622  [   32/  306]
train() client id: f_00004-1-1 loss: 1.046351  [   64/  306]
train() client id: f_00004-1-2 loss: 1.056470  [   96/  306]
train() client id: f_00004-1-3 loss: 0.757059  [  128/  306]
train() client id: f_00004-1-4 loss: 0.988550  [  160/  306]
train() client id: f_00004-1-5 loss: 0.844105  [  192/  306]
train() client id: f_00004-1-6 loss: 0.832202  [  224/  306]
train() client id: f_00004-1-7 loss: 0.808694  [  256/  306]
train() client id: f_00004-1-8 loss: 0.830956  [  288/  306]
train() client id: f_00004-2-0 loss: 0.962794  [   32/  306]
train() client id: f_00004-2-1 loss: 0.990554  [   64/  306]
train() client id: f_00004-2-2 loss: 0.832689  [   96/  306]
train() client id: f_00004-2-3 loss: 0.844414  [  128/  306]
train() client id: f_00004-2-4 loss: 0.832395  [  160/  306]
train() client id: f_00004-2-5 loss: 0.985357  [  192/  306]
train() client id: f_00004-2-6 loss: 0.872773  [  224/  306]
train() client id: f_00004-2-7 loss: 0.925749  [  256/  306]
train() client id: f_00004-2-8 loss: 0.823509  [  288/  306]
train() client id: f_00005-0-0 loss: 0.487613  [   32/  146]
train() client id: f_00005-0-1 loss: 0.796676  [   64/  146]
train() client id: f_00005-0-2 loss: 0.663667  [   96/  146]
train() client id: f_00005-0-3 loss: 1.172681  [  128/  146]
train() client id: f_00005-1-0 loss: 0.770896  [   32/  146]
train() client id: f_00005-1-1 loss: 0.752346  [   64/  146]
train() client id: f_00005-1-2 loss: 0.659480  [   96/  146]
train() client id: f_00005-1-3 loss: 0.802579  [  128/  146]
train() client id: f_00005-2-0 loss: 0.651839  [   32/  146]
train() client id: f_00005-2-1 loss: 0.768272  [   64/  146]
train() client id: f_00005-2-2 loss: 0.619059  [   96/  146]
train() client id: f_00005-2-3 loss: 0.650785  [  128/  146]
train() client id: f_00006-0-0 loss: 0.443745  [   32/   54]
train() client id: f_00006-1-0 loss: 0.430800  [   32/   54]
train() client id: f_00006-2-0 loss: 0.463317  [   32/   54]
train() client id: f_00007-0-0 loss: 0.774211  [   32/  179]
train() client id: f_00007-0-1 loss: 0.342065  [   64/  179]
train() client id: f_00007-0-2 loss: 0.316829  [   96/  179]
train() client id: f_00007-0-3 loss: 0.512549  [  128/  179]
train() client id: f_00007-0-4 loss: 0.584763  [  160/  179]
train() client id: f_00007-1-0 loss: 0.671231  [   32/  179]
train() client id: f_00007-1-1 loss: 0.384192  [   64/  179]
train() client id: f_00007-1-2 loss: 0.302650  [   96/  179]
train() client id: f_00007-1-3 loss: 0.440995  [  128/  179]
train() client id: f_00007-1-4 loss: 0.464303  [  160/  179]
train() client id: f_00007-2-0 loss: 0.417787  [   32/  179]
train() client id: f_00007-2-1 loss: 0.527883  [   64/  179]
train() client id: f_00007-2-2 loss: 0.543060  [   96/  179]
train() client id: f_00007-2-3 loss: 0.460423  [  128/  179]
train() client id: f_00007-2-4 loss: 0.430663  [  160/  179]
train() client id: f_00008-0-0 loss: 0.891611  [   32/  130]
train() client id: f_00008-0-1 loss: 0.740609  [   64/  130]
train() client id: f_00008-0-2 loss: 0.765162  [   96/  130]
train() client id: f_00008-0-3 loss: 0.798794  [  128/  130]
train() client id: f_00008-1-0 loss: 0.851993  [   32/  130]
train() client id: f_00008-1-1 loss: 0.897559  [   64/  130]
train() client id: f_00008-1-2 loss: 0.731470  [   96/  130]
train() client id: f_00008-1-3 loss: 0.706260  [  128/  130]
train() client id: f_00008-2-0 loss: 0.901110  [   32/  130]
train() client id: f_00008-2-1 loss: 0.741251  [   64/  130]
train() client id: f_00008-2-2 loss: 0.661498  [   96/  130]
train() client id: f_00008-2-3 loss: 0.849790  [  128/  130]
train() client id: f_00009-0-0 loss: 0.820010  [   32/  118]
train() client id: f_00009-0-1 loss: 0.738898  [   64/  118]
train() client id: f_00009-0-2 loss: 0.768880  [   96/  118]
train() client id: f_00009-1-0 loss: 0.621550  [   32/  118]
train() client id: f_00009-1-1 loss: 0.778785  [   64/  118]
train() client id: f_00009-1-2 loss: 0.658450  [   96/  118]
train() client id: f_00009-2-0 loss: 0.792902  [   32/  118]
train() client id: f_00009-2-1 loss: 0.635233  [   64/  118]
train() client id: f_00009-2-2 loss: 0.667277  [   96/  118]
At round 80 accuracy: 0.6472148541114059
At round 80 training accuracy: 0.5922199865861838
At round 80 training loss: 0.8096772823630164
update_location
xs = -4.528292 301.001589 310.045120 -310.943528 199.896481 -235.217951 -352.215960 373.375741 -1.680116 294.695607 
ys = 387.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -317.154970 4.001482 
xs mean: 57.442869159412865
ys mean: 9.371751218646875
dists_uav = 400.306048 317.559350 325.775567 327.405172 223.515157 255.607910 366.150034 387.010628 332.550895 311.225823 
uav_gains = -121.414651 -117.108056 -117.689247 -117.799627 -109.235296 -111.780119 -119.994457 -120.904408 -118.137709 -116.631915 
uav_gains_db_mean: -117.0695484964626
dists_bs = 272.496319 501.995970 515.202191 204.262634 414.005107 182.412084 251.264769 581.739558 521.781848 499.854436 
bs_gains = -107.757157 -115.186666 -115.502435 -104.252349 -112.843219 -102.876560 -106.770744 -116.979460 -115.656750 -115.134679 
bs_gains_db_mean: -111.2960017817686
Round 81
-------------------------------
ene_coms = [0.03376353 0.01620741 0.02098578 0.02124867 0.01319899 0.00715531
 0.02777731 0.03141325 0.01695712 0.01612799]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.18238917 0.35943917 0.17409794 0.0665312  0.41261909 0.19724864
 0.08403258 0.25181217 0.18029861 0.16398015]
obj_prev = 2.072448717624668
eta_min = 0.0	eta_max = inf
af = 0.4261159085366522	bf = 0.08938795268020232	zeta = 0.4687274993903175	eta = 0.909090909090909
af = 0.4261159085366522	bf = 0.08938795268020232	zeta = 1.4300125019434509	eta = 0.29798054769279403
af = 0.4261159085366522	bf = 0.08938795268020232	zeta = 0.8622239974366661	eta = 0.4942055774409737
af = 0.4261159085366522	bf = 0.08938795268020232	zeta = 0.7705028053564088	eta = 0.5530361545400801
af = 0.4261159085366522	bf = 0.08938795268020232	zeta = 0.7646927199285072	eta = 0.5572380871842101
af = 0.4261159085366522	bf = 0.08938795268020232	zeta = 0.7646652198321808	eta = 0.5572581274589301
eta = 0.5572581274589301
ene_coms = [0.03376353 0.01620741 0.02098578 0.02124867 0.01319899 0.00715531
 0.02777731 0.03141325 0.01695712 0.01612799]
ene_comp = [0.04825025 0.1014787  0.04748437 0.01646635 0.11717917 0.05590902
 0.02067868 0.06854599 0.049782   0.04518674]
ene_total = [0.08083702 0.1159975  0.06748772 0.03717388 0.12850745 0.06215946
 0.04776073 0.09852499 0.06578152 0.06043496]
ti_comp = [25.98978006 26.16534128 26.11755761 26.11492868 26.1954255  26.25586232
 26.04964229 26.01328287 26.15784423 26.16613555]
ti_coms = [0.33763534 0.16207412 0.2098578  0.21248673 0.1319899  0.07155309
 0.27777312 0.31413254 0.16957117 0.16127985]
t_total = [25.94966011 25.94966011 25.94966011 25.94966011 25.94966011 25.94966011
 25.94966011 25.94966011 25.94966011 25.94966011]
ene_coms = [0.03376353 0.01620741 0.02098578 0.02124867 0.01319899 0.00715531
 0.02777731 0.03141325 0.01695712 0.01612799]
ene_comp = [1.03937783e-08 9.54007600e-08 9.80995360e-09 4.09161441e-10
 1.46548165e-07 1.58442985e-08 8.14413140e-10 2.97464940e-08
 1.12692024e-08 8.42236131e-09]
ene_total = [0.03327909 0.01597496 0.02068468 0.02094379 0.01300975 0.00705266
 0.02737875 0.03096255 0.01671382 0.01589658]
optimize_network iter = 0 obj = 0.20189663920930737
eta = 0.5572581274589301
freqs = [ 928254.24960696 1939181.57241512  909050.74948838  315267.07456622
 2236634.26800908 1064695.88316103  396909.04979149 1317518.9574338
  951569.26976486  863458.41376344]
eta_min = 0.557258127458931	eta_max = 0.9766999206783064
af = 2.452822315940312e-07	bf = 0.08938795268020232	zeta = 2.698104547534344e-07	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03376353 0.01620741 0.02098578 0.02124867 0.01319899 0.00715531
 0.02777731 0.03141325 0.01695712 0.01612799]
ene_comp = [1.69418095e-09 1.55502789e-08 1.59901781e-09 6.66931218e-11
 2.38872818e-08 2.58261318e-09 1.32748958e-10 4.84866449e-09
 1.83687467e-09 1.37284092e-09]
ene_total = [0.16207449 0.07780022 0.10073766 0.10199962 0.06335899 0.03434751
 0.13333893 0.15079249 0.08139895 0.07741888]
ti_comp = [5.06822651 5.24378773 5.19600406 5.19337513 5.27387195 5.33430877
 5.12808873 5.09172931 5.23629068 5.244582  ]
ti_coms = [0.33763534 0.16207412 0.2098578  0.21248673 0.1319899  0.07155309
 0.27777312 0.31413254 0.16957117 0.16127985]
t_total = [25.94966011 25.94966011 25.94966011 25.94966011 25.94966011 25.94966011
 25.94966011 25.94966011 25.94966011 25.94966011]
ene_coms = [0.03376353 0.01620741 0.02098578 0.02124867 0.01319899 0.00715531
 0.02777731 0.03141325 0.01695712 0.01612799]
ene_comp = [1.18365644e-09 1.02866385e-08 1.07337668e-09 4.48055737e-11
 1.56578158e-08 1.66237244e-09 9.10115014e-11 3.36243997e-09
 1.21789157e-09 9.07925730e-10]
ene_total = [0.16207448 0.07780019 0.10073766 0.10199962 0.06335895 0.03434751
 0.13333892 0.15079249 0.08139895 0.07741888]
optimize_network iter = 1 obj = 0.9832676498343076
eta = 0.909090909090909
freqs = [ 775889.51213629 1577197.65766984  744796.37892254  258406.90521158
 1810827.98715023  854200.9657262   328642.39060724 1097167.85722668
  774827.37741557  702192.97422812]
eta_min = 0.9090909090909225	eta_max = 0.9090909090908886
af = 1.6248582349765463e-07	bf = 0.08938795268020232	zeta = 1.787344058474201e-07	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03376353 0.01620741 0.02098578 0.02124867 0.01319899 0.00715531
 0.02777731 0.03141325 0.01695712 0.01612799]
ene_comp = [1.18365644e-09 1.02866385e-08 1.07337668e-09 4.48055737e-11
 1.56578158e-08 1.66237244e-09 9.10115014e-11 3.36243997e-09
 1.21789157e-09 9.07925730e-10]
ene_total = [0.16207448 0.07780019 0.10073766 0.10199962 0.06335895 0.03434751
 0.13333892 0.15079249 0.08139895 0.07741888]
ti_comp = [5.06822651 5.24378773 5.19600406 5.19337513 5.27387195 5.33430877
 5.12808873 5.09172931 5.23629068 5.244582  ]
ti_coms = [0.33763534 0.16207412 0.2098578  0.21248673 0.1319899  0.07155309
 0.27777312 0.31413254 0.16957117 0.16127985]
t_total = [25.94966011 25.94966011 25.94966011 25.94966011 25.94966011 25.94966011
 25.94966011 25.94966011 25.94966011 25.94966011]
ene_coms = [0.03376353 0.01620741 0.02098578 0.02124867 0.01319899 0.00715531
 0.02777731 0.03141325 0.01695712 0.01612799]
ene_comp = [1.18365644e-09 1.02866385e-08 1.07337668e-09 4.48055737e-11
 1.56578158e-08 1.66237244e-09 9.10115014e-11 3.36243997e-09
 1.21789157e-09 9.07925730e-10]
ene_total = [0.16207448 0.07780019 0.10073766 0.10199962 0.06335895 0.03434751
 0.13333892 0.15079249 0.08139895 0.07741888]
optimize_network iter = 2 obj = 0.983267649834088
eta = 0.9090909090908886
freqs = [ 775889.51213629 1577197.65766985  744796.37892254  258406.90521158
 1810827.98715025  854200.9657262   328642.39060724 1097167.85722668
  774827.37741557  702192.97422812]
Done!
ene_coms = [0.03376353 0.01620741 0.02098578 0.02124867 0.01319899 0.00715531
 0.02777731 0.03141325 0.01695712 0.01612799]
ene_comp = [1.13778857e-09 9.88802099e-09 1.03178226e-09 4.30693130e-11
 1.50510598e-08 1.59795384e-09 8.74847149e-11 3.23214207e-09
 1.17069706e-09 8.72742702e-10]
ene_total = [0.03376354 0.01620742 0.02098578 0.02124867 0.01319901 0.00715531
 0.02777731 0.03141326 0.01695712 0.01612799]
At round 81 energy consumption: 0.20483540144558715
At round 81 eta: 0.9090909090908886
At round 81 a_n: 0.43638925173967635
At round 81 local rounds: 3.1209395205784904
At round 81 global rounds: 4.80028176913536
gradient difference: 0.9037078619003296
train() client id: f_00000-0-0 loss: 0.966380  [   32/  126]
train() client id: f_00000-0-1 loss: 0.792047  [   64/  126]
train() client id: f_00000-0-2 loss: 0.655976  [   96/  126]
train() client id: f_00000-1-0 loss: 0.903883  [   32/  126]
train() client id: f_00000-1-1 loss: 0.778542  [   64/  126]
train() client id: f_00000-1-2 loss: 0.767631  [   96/  126]
train() client id: f_00000-2-0 loss: 0.748820  [   32/  126]
train() client id: f_00000-2-1 loss: 0.800043  [   64/  126]
train() client id: f_00000-2-2 loss: 0.661184  [   96/  126]
train() client id: f_00001-0-0 loss: 0.623948  [   32/  265]
train() client id: f_00001-0-1 loss: 0.626790  [   64/  265]
train() client id: f_00001-0-2 loss: 0.497086  [   96/  265]
train() client id: f_00001-0-3 loss: 0.481110  [  128/  265]
train() client id: f_00001-0-4 loss: 0.439425  [  160/  265]
train() client id: f_00001-0-5 loss: 0.539065  [  192/  265]
train() client id: f_00001-0-6 loss: 0.623968  [  224/  265]
train() client id: f_00001-0-7 loss: 0.525926  [  256/  265]
train() client id: f_00001-1-0 loss: 0.672011  [   32/  265]
train() client id: f_00001-1-1 loss: 0.534710  [   64/  265]
train() client id: f_00001-1-2 loss: 0.461715  [   96/  265]
train() client id: f_00001-1-3 loss: 0.664458  [  128/  265]
train() client id: f_00001-1-4 loss: 0.486972  [  160/  265]
train() client id: f_00001-1-5 loss: 0.482369  [  192/  265]
train() client id: f_00001-1-6 loss: 0.531963  [  224/  265]
train() client id: f_00001-1-7 loss: 0.493637  [  256/  265]
train() client id: f_00001-2-0 loss: 0.423958  [   32/  265]
train() client id: f_00001-2-1 loss: 0.514778  [   64/  265]
train() client id: f_00001-2-2 loss: 0.523993  [   96/  265]
train() client id: f_00001-2-3 loss: 0.538202  [  128/  265]
train() client id: f_00001-2-4 loss: 0.523037  [  160/  265]
train() client id: f_00001-2-5 loss: 0.558252  [  192/  265]
train() client id: f_00001-2-6 loss: 0.489179  [  224/  265]
train() client id: f_00001-2-7 loss: 0.673813  [  256/  265]
train() client id: f_00002-0-0 loss: 1.079825  [   32/  124]
train() client id: f_00002-0-1 loss: 1.250143  [   64/  124]
train() client id: f_00002-0-2 loss: 0.840964  [   96/  124]
train() client id: f_00002-1-0 loss: 0.978836  [   32/  124]
train() client id: f_00002-1-1 loss: 1.070085  [   64/  124]
train() client id: f_00002-1-2 loss: 1.153016  [   96/  124]
train() client id: f_00002-2-0 loss: 0.905341  [   32/  124]
train() client id: f_00002-2-1 loss: 0.882496  [   64/  124]
train() client id: f_00002-2-2 loss: 0.886474  [   96/  124]
train() client id: f_00003-0-0 loss: 0.561676  [   32/   43]
train() client id: f_00003-1-0 loss: 0.781914  [   32/   43]
train() client id: f_00003-2-0 loss: 0.507973  [   32/   43]
train() client id: f_00004-0-0 loss: 0.868565  [   32/  306]
train() client id: f_00004-0-1 loss: 0.717438  [   64/  306]
train() client id: f_00004-0-2 loss: 0.886014  [   96/  306]
train() client id: f_00004-0-3 loss: 0.881238  [  128/  306]
train() client id: f_00004-0-4 loss: 0.870284  [  160/  306]
train() client id: f_00004-0-5 loss: 0.854252  [  192/  306]
train() client id: f_00004-0-6 loss: 0.968022  [  224/  306]
train() client id: f_00004-0-7 loss: 0.775112  [  256/  306]
train() client id: f_00004-0-8 loss: 0.985898  [  288/  306]
train() client id: f_00004-1-0 loss: 0.855649  [   32/  306]
train() client id: f_00004-1-1 loss: 0.941000  [   64/  306]
train() client id: f_00004-1-2 loss: 0.819560  [   96/  306]
train() client id: f_00004-1-3 loss: 0.818790  [  128/  306]
train() client id: f_00004-1-4 loss: 0.775741  [  160/  306]
train() client id: f_00004-1-5 loss: 0.984524  [  192/  306]
train() client id: f_00004-1-6 loss: 0.784882  [  224/  306]
train() client id: f_00004-1-7 loss: 0.854268  [  256/  306]
train() client id: f_00004-1-8 loss: 0.936845  [  288/  306]
train() client id: f_00004-2-0 loss: 0.966753  [   32/  306]
train() client id: f_00004-2-1 loss: 0.946627  [   64/  306]
train() client id: f_00004-2-2 loss: 0.855279  [   96/  306]
train() client id: f_00004-2-3 loss: 0.791081  [  128/  306]
train() client id: f_00004-2-4 loss: 0.730847  [  160/  306]
train() client id: f_00004-2-5 loss: 0.813909  [  192/  306]
train() client id: f_00004-2-6 loss: 0.762089  [  224/  306]
train() client id: f_00004-2-7 loss: 0.955873  [  256/  306]
train() client id: f_00004-2-8 loss: 0.939891  [  288/  306]
train() client id: f_00005-0-0 loss: 0.490154  [   32/  146]
train() client id: f_00005-0-1 loss: 0.567042  [   64/  146]
train() client id: f_00005-0-2 loss: 0.726967  [   96/  146]
train() client id: f_00005-0-3 loss: 1.026398  [  128/  146]
train() client id: f_00005-1-0 loss: 0.517232  [   32/  146]
train() client id: f_00005-1-1 loss: 0.773056  [   64/  146]
train() client id: f_00005-1-2 loss: 0.822437  [   96/  146]
train() client id: f_00005-1-3 loss: 0.828584  [  128/  146]
train() client id: f_00005-2-0 loss: 0.671936  [   32/  146]
train() client id: f_00005-2-1 loss: 0.944657  [   64/  146]
train() client id: f_00005-2-2 loss: 0.429399  [   96/  146]
train() client id: f_00005-2-3 loss: 0.829559  [  128/  146]
train() client id: f_00006-0-0 loss: 0.385984  [   32/   54]
train() client id: f_00006-1-0 loss: 0.469683  [   32/   54]
train() client id: f_00006-2-0 loss: 0.420042  [   32/   54]
train() client id: f_00007-0-0 loss: 0.678673  [   32/  179]
train() client id: f_00007-0-1 loss: 0.577874  [   64/  179]
train() client id: f_00007-0-2 loss: 0.620906  [   96/  179]
train() client id: f_00007-0-3 loss: 0.898319  [  128/  179]
train() client id: f_00007-0-4 loss: 0.762514  [  160/  179]
train() client id: f_00007-1-0 loss: 0.692740  [   32/  179]
train() client id: f_00007-1-1 loss: 0.704979  [   64/  179]
train() client id: f_00007-1-2 loss: 0.734515  [   96/  179]
train() client id: f_00007-1-3 loss: 0.731939  [  128/  179]
train() client id: f_00007-1-4 loss: 0.717283  [  160/  179]
train() client id: f_00007-2-0 loss: 0.587067  [   32/  179]
train() client id: f_00007-2-1 loss: 0.582012  [   64/  179]
train() client id: f_00007-2-2 loss: 0.632461  [   96/  179]
train() client id: f_00007-2-3 loss: 0.682737  [  128/  179]
train() client id: f_00007-2-4 loss: 0.932805  [  160/  179]
train() client id: f_00008-0-0 loss: 0.648402  [   32/  130]
train() client id: f_00008-0-1 loss: 0.727252  [   64/  130]
train() client id: f_00008-0-2 loss: 0.754829  [   96/  130]
train() client id: f_00008-0-3 loss: 0.632711  [  128/  130]
train() client id: f_00008-1-0 loss: 0.657128  [   32/  130]
train() client id: f_00008-1-1 loss: 0.715148  [   64/  130]
train() client id: f_00008-1-2 loss: 0.667822  [   96/  130]
train() client id: f_00008-1-3 loss: 0.754185  [  128/  130]
train() client id: f_00008-2-0 loss: 0.618595  [   32/  130]
train() client id: f_00008-2-1 loss: 0.764386  [   64/  130]
train() client id: f_00008-2-2 loss: 0.709118  [   96/  130]
train() client id: f_00008-2-3 loss: 0.723790  [  128/  130]
train() client id: f_00009-0-0 loss: 0.801702  [   32/  118]
train() client id: f_00009-0-1 loss: 0.912117  [   64/  118]
train() client id: f_00009-0-2 loss: 0.989882  [   96/  118]
train() client id: f_00009-1-0 loss: 0.881633  [   32/  118]
train() client id: f_00009-1-1 loss: 0.847993  [   64/  118]
train() client id: f_00009-1-2 loss: 0.884835  [   96/  118]
train() client id: f_00009-2-0 loss: 0.732786  [   32/  118]
train() client id: f_00009-2-1 loss: 0.851954  [   64/  118]
train() client id: f_00009-2-2 loss: 0.852774  [   96/  118]
At round 81 accuracy: 0.6472148541114059
At round 81 training accuracy: 0.579476861167002
At round 81 training loss: 0.8385267165570904
update_location
xs = -4.528292 306.001589 315.045120 -315.943528 204.896481 -240.217951 -357.215960 378.375741 -1.680116 299.695607 
ys = 392.587959 15.555839 1.320614 22.544824 -0.649813 2.814151 -3.124922 -19.177652 -322.154970 4.001482 
xs mean: 58.442869159412865
ys mean: 9.371751218646875
dists_uav = 405.149122 322.302586 330.537700 332.157466 227.997786 260.216416 370.962272 391.836680 337.322764 315.964347 
uav_gains = -121.589505 -117.448619 -118.007316 -118.112415 -109.556696 -112.187361 -120.218071 -121.095097 -118.437341 -116.990440 
uav_gains_db_mean: -117.36428609855096
dists_bs = 276.414767 506.739548 519.912251 207.623840 418.538162 184.123186 254.815901 586.455174 526.500566 504.555659 
bs_gains = -107.930774 -115.301034 -115.613101 -104.450821 -112.975641 -102.990097 -106.941402 -117.077634 -115.766227 -115.248513 
bs_gains_db_mean: -111.4295243028397
Round 82
-------------------------------
ene_coms = [0.03462708 0.01638453 0.02175787 0.02202302 0.013342   0.00719365
 0.02861073 0.03226315 0.01714022 0.01630278]
ene_comp = [0.38000761 0.79922235 0.37397574 0.12968514 0.92287561 0.44032627
 0.1628604  0.53985207 0.39207134 0.35588014]
ene_total = [0.03930377 0.07731245 0.03751211 0.01438062 0.0887453  0.042421
 0.0181498  0.05423156 0.0387897  0.03527971]
obj_prev = 0.4461260200988016
eta_min = 0.0	eta_max = inf
af = 0.09163417184195034	bf = 0.01967380371355441	zeta = 0.10079758902614538	eta = 0.9090909090909091
af = 0.09163417184195034	bf = 0.01967380371355441	zeta = 0.3124822041883421	eta = 0.2932460492589196
af = 0.09163417184195034	bf = 0.01967380371355441	zeta = 0.1868902276616285	eta = 0.4903101301147608
af = 0.09163417184195034	bf = 0.01967380371355441	zeta = 0.16673482507300488	eta = 0.549580279955482
af = 0.09163417184195034	bf = 0.01967380371355441	zeta = 0.16545913532769616	eta = 0.5538175432892627
af = 0.09163417184195034	bf = 0.01967380371355441	zeta = 0.16545309341070988	eta = 0.5538377672666639
eta = 0.5538377672666639
ene_coms = [0.03462708 0.01638453 0.02175787 0.02202302 0.013342   0.00719365
 0.02861073 0.03226315 0.01714022 0.01630278]
ene_comp = [0.04875829 0.10254719 0.04798435 0.01663973 0.11841299 0.0564977
 0.02089641 0.06926773 0.05030617 0.04566252]
ene_total = [0.01753884 0.02501547 0.01466921 0.00813212 0.02771265 0.0133965
 0.01041307 0.02135547 0.01418632 0.01303345]
ti_comp = [122.78916254 122.97158803 122.91785459 122.91520315 123.00201327
 123.06349682 122.84932604 122.81280178 122.96403106 122.97240548]
ti_coms = [0.34627077 0.16384528 0.21757872 0.22023016 0.13342004 0.07193649
 0.28610727 0.32263153 0.17140225 0.16302783]
t_total = [25.89965591 25.89965591 25.89965591 25.89965591 25.89965591 25.89965591
 25.89965591 25.89965591 25.89965591 25.89965591]
ene_coms = [0.03462708 0.01638453 0.02175787 0.02202302 0.013342   0.00719365
 0.02861073 0.03226315 0.01714022 0.01630278]
ene_comp = [4.80513161e-10 4.45699129e-09 4.57034347e-10 1.90593413e-11
 6.85888571e-09 7.44241343e-10 3.77876122e-11 1.37716362e-09
 5.26244536e-10 3.93499706e-10]
ene_total = [0.00728328 0.00344624 0.00457644 0.0046322  0.00280629 0.00151307
 0.00601783 0.00678606 0.00360518 0.00342904]
optimize_network iter = 0 obj = 0.04409563094046713
eta = 0.5538377672666639
freqs = [198544.75344694 416954.82130163 195188.67738014  67687.85626791
 481345.71950441 229546.94227034  85048.93685412 282005.31868964
 204556.44520613 185661.66738226]
eta_min = 0.5538377672666641	eta_max = 0.9948813088173181
af = 2.438094222895456e-09	bf = 0.01967380371355441	zeta = 2.681903645185002e-09	eta = 0.909090909090909
eta = 0.909090909090909
ene_coms = [0.03462708 0.01638453 0.02175787 0.02202302 0.013342   0.00719365
 0.02861073 0.03226315 0.01714022 0.01630278]
ene_comp = [7.75073222e-11 7.18917791e-10 7.37201626e-11 3.07429354e-12
 1.10634611e-09 1.20046979e-10 6.09518505e-12 2.22138066e-10
 8.48838453e-11 6.34719524e-11]
ene_total = [0.03574475 0.01691338 0.02246016 0.02273386 0.01377265 0.00742584
 0.02953421 0.03330453 0.01769347 0.016829  ]
ti_comp = [24.74354966 24.92597515 24.8722417  24.86959027 24.95640039 25.01788394
 24.80371315 24.7671889  24.91841818 24.9267926 ]
ti_coms = [0.34627077 0.16384528 0.21757872 0.22023016 0.13342004 0.07193649
 0.28610727 0.32263153 0.17140225 0.16302783]
t_total = [25.89965591 25.89965591 25.89965591 25.89965591 25.89965591 25.89965591
 25.89965591 25.89965591 25.89965591 25.89965591]
ene_coms = [0.03462708 0.01638453 0.02175787 0.02202302 0.013342   0.00719365
 0.02861073 0.03226315 0.01714022 0.01630278]
ene_comp = [4.96607978e-11 4.55259869e-10 4.68447860e-11 1.95386327e-12
 6.99240304e-10 7.55759270e-11 3.89021802e-12 1.42112318e-10
 5.37793043e-11 4.01919691e-11]
ene_total = [0.03574475 0.01691338 0.02246016 0.02273386 0.01377265 0.00742584
 0.02953421 0.03330453 0.01769347 0.016829  ]
optimize_network iter = 1 obj = 0.216411842468235
eta = 0.909090909090909
freqs = [158925.61293832 331802.05304608 155593.73584262  53961.64469352
 382670.36849418 182132.57809468  67945.76806446 225559.78252173
 162820.18174101 147740.97467378]
eta_min = 0.9090909090909151	eta_max = 0.9729507206939951
af = 1.5443707969687761e-09	bf = 0.01967380371355441	zeta = 1.6988078766656539e-09	eta = 0.9090909090909091
eta = 0.9090909090909091
ene_coms = [0.03462708 0.01638453 0.02175787 0.02202302 0.013342   0.00719365
 0.02861073 0.03226315 0.01714022 0.01630278]
ene_comp = [4.96607978e-11 4.55259869e-10 4.68447860e-11 1.95386327e-12
 6.99240304e-10 7.55759270e-11 3.89021802e-12 1.42112318e-10
 5.37793043e-11 4.01919691e-11]
ene_total = [0.03574475 0.01691338 0.02246016 0.02273386 0.01377265 0.00742584
 0.02953421 0.03330453 0.01769347 0.016829  ]
ti_comp = [24.74354966 24.92597515 24.8722417  24.86959027 24.95640039 25.01788394
 24.80371315 24.7671889  24.91841818 24.9267926 ]
ti_coms = [0.34627077 0.16384528 0.21757872 0.22023016 0.13342004 0.07193649
 0.28610727 0.32263153 0.17140225 0.16302783]
t_total = [25.89965591 25.89965591 25.89965591 25.89965591 25.89965591 25.89965591
 25.89965591 25.89965591 25.89965591 25.89965591]
ene_coms = [0.03462708 0.01638453 0.02175787 0.02202302 0.013342   0.00719365
 0.02861073 0.03226315 0.01714022 0.01630278]
ene_comp = [4.96607978e-11 4.55259869e-10 4.68447860e-11 1.95386327e-12
 6.99240304e-10 7.55759270e-11 3.89021802e-12 1.42112318e-10
 5.37793043e-11 4.01919691e-11]
ene_total = [0.03574475 0.01691338 0.02246016 0.02273386 0.01377265 0.00742584
 0.02953421 0.03330453 0.01769347 0.016829  ]
optimize_network iter = 2 obj = 0.21641184246824954
eta = 0.9090909090909151
freqs = [158925.61293832 331802.05304608 155593.73584262  53961.64469352
 382670.36849418 182132.57809468  67945.76806446 225559.78252173
 162820.18174101 147740.97467378]
Done!
ene_coms = [0.03462708 0.01638453 0.02175787 0.02202302 0.013342   0.00719365
 0.02861073 0.03226315 0.01714022 0.01630278]
ene_comp = [4.77363923e-11 4.37618095e-10 4.50295038e-11 1.87814912e-12
 6.72144044e-10 7.26472844e-11 3.73946819e-12 1.36605324e-10
 5.16953026e-11 3.86344902e-11]
ene_total = [0.03462708 0.01638453 0.02175787 0.02202302 0.013342   0.00719365
 0.02861073 0.03226315 0.01714022 0.01630278]
At round 82 energy consumption: 0.20964503475964225
At round 82 eta: 0.9090909090909151
At round 82 a_n: 0.09384340477035735
At round 82 local rounds: 3.1209395205775388
At round 82 global rounds: 1.0322774524739986
gradient difference: 0.8701111078262329
train() client id: f_00000-0-0 loss: 1.234645  [   32/  126]
train() client id: f_00000-0-1 loss: 0.954038  [   64/  126]
train() client id: f_00000-0-2 loss: 0.909455  [   96/  126]
train() client id: f_00000-1-0 loss: 0.952323  [   32/  126]
train() client id: f_00000-1-1 loss: 1.272420  [   64/  126]
train() client id: f_00000-1-2 loss: 0.878257  [   96/  126]
train() client id: f_00000-2-0 loss: 0.928314  [   32/  126]
train() client id: f_00000-2-1 loss: 1.070960  [   64/  126]
train() client id: f_00000-2-2 loss: 1.100117  [   96/  126]
train() client id: f_00001-0-0 loss: 0.372946  [   32/  265]
train() client id: f_00001-0-1 loss: 0.447234  [   64/  265]
train() client id: f_00001-0-2 loss: 0.482811  [   96/  265]
train() client id: f_00001-0-3 loss: 0.444152  [  128/  265]
train() client id: f_00001-0-4 loss: 0.337418  [  160/  265]
train() client id: f_00001-0-5 loss: 0.451342  [  192/  265]
train() client id: f_00001-0-6 loss: 0.298216  [  224/  265]
train() client id: f_00001-0-7 loss: 0.349738  [  256/  265]
train() client id: f_00001-1-0 loss: 0.408680  [   32/  265]
train() client id: f_00001-1-1 loss: 0.489767  [   64/  265]
train() client id: f_00001-1-2 loss: 0.464959  [   96/  265]
train() client id: f_00001-1-3 loss: 0.309545  [  128/  265]
train() client id: f_00001-1-4 loss: 0.389593  [  160/  265]
train() client id: f_00001-1-5 loss: 0.353058  [  192/  265]
train() client id: f_00001-1-6 loss: 0.406447  [  224/  265]
train() client id: f_00001-1-7 loss: 0.342413  [  256/  265]
train() client id: f_00001-2-0 loss: 0.383121  [   32/  265]
train() client id: f_00001-2-1 loss: 0.417950  [   64/  265]
train() client id: f_00001-2-2 loss: 0.462423  [   96/  265]
train() client id: f_00001-2-3 loss: 0.369268  [  128/  265]
train() client id: f_00001-2-4 loss: 0.363966  [  160/  265]
train() client id: f_00001-2-5 loss: 0.399915  [  192/  265]
train() client id: f_00001-2-6 loss: 0.415319  [  224/  265]
train() client id: f_00001-2-7 loss: 0.363611  [  256/  265]
train() client id: f_00002-0-0 loss: 1.111251  [   32/  124]
train() client id: f_00002-0-1 loss: 1.118350  [   64/  124]
train() client id: f_00002-0-2 loss: 1.166913  [   96/  124]
train() client id: f_00002-1-0 loss: 1.212314  [   32/  124]
train() client id: f_00002-1-1 loss: 0.996920  [   64/  124]
train() client id: f_00002-1-2 loss: 0.896840  [   96/  124]
train() client id: f_00002-2-0 loss: 1.067071  [   32/  124]
train() client id: f_00002-2-1 loss: 1.013557  [   64/  124]
train() client id: f_00002-2-2 loss: 1.226037  [   96/  124]
train() client id: f_00003-0-0 loss: 0.500680  [   32/   43]
train() client id: f_00003-1-0 loss: 0.578827  [   32/   43]
train() client id: f_00003-2-0 loss: 0.658404  [   32/   43]
train() client id: f_00004-0-0 loss: 0.743570  [   32/  306]
train() client id: f_00004-0-1 loss: 0.885441  [   64/  306]
train() client id: f_00004-0-2 loss: 0.792621  [   96/  306]
train() client id: f_00004-0-3 loss: 0.772738  [  128/  306]
train() client id: f_00004-0-4 loss: 0.847388  [  160/  306]
train() client id: f_00004-0-5 loss: 0.827870  [  192/  306]
train() client id: f_00004-0-6 loss: 0.798346  [  224/  306]
train() client id: f_00004-0-7 loss: 0.703989  [  256/  306]
train() client id: f_00004-0-8 loss: 0.778616  [  288/  306]
train() client id: f_00004-1-0 loss: 0.771413  [   32/  306]
train() client id: f_00004-1-1 loss: 0.922369  [   64/  306]
train() client id: f_00004-1-2 loss: 0.755799  [   96/  306]
train() client id: f_00004-1-3 loss: 0.829577  [  128/  306]
train() client id: f_00004-1-4 loss: 0.742140  [  160/  306]
train() client id: f_00004-1-5 loss: 0.768250  [  192/  306]
train() client id: f_00004-1-6 loss: 0.769392  [  224/  306]
train() client id: f_00004-1-7 loss: 0.870155  [  256/  306]
train() client id: f_00004-1-8 loss: 0.842024  [  288/  306]
train() client id: f_00004-2-0 loss: 0.823286  [   32/  306]
train() client id: f_00004-2-1 loss: 0.785816  [   64/  306]
train() client id: f_00004-2-2 loss: 0.817328  [   96/  306]
train() client id: f_00004-2-3 loss: 0.807310  [  128/  306]
train() client id: f_00004-2-4 loss: 0.740149  [  160/  306]
train() client id: f_00004-2-5 loss: 0.792997  [  192/  306]
train() client id: f_00004-2-6 loss: 0.777932  [  224/  306]
train() client id: f_00004-2-7 loss: 0.989571  [  256/  306]
train() client id: f_00004-2-8 loss: 0.677050  [  288/  306]
train() client id: f_00005-0-0 loss: 0.464944  [   32/  146]
train() client id: f_00005-0-1 loss: 0.242028  [   64/  146]
train() client id: f_00005-0-2 loss: 0.672080  [   96/  146]
train() client id: f_00005-0-3 loss: 0.529169  [  128/  146]
train() client id: f_00005-1-0 loss: 0.283673  [   32/  146]
train() client id: f_00005-1-1 loss: 0.498119  [   64/  146]
train() client id: f_00005-1-2 loss: 0.369712  [   96/  146]
train() client id: f_00005-1-3 loss: 0.733370  [  128/  146]
train() client id: f_00005-2-0 loss: 0.450666  [   32/  146]
train() client id: f_00005-2-1 loss: 0.658128  [   64/  146]
train() client id: f_00005-2-2 loss: 0.386627  [   96/  146]
train() client id: f_00005-2-3 loss: 0.419863  [  128/  146]
train() client id: f_00006-0-0 loss: 0.487083  [   32/   54]
train() client id: f_00006-1-0 loss: 0.560119  [   32/   54]
train() client id: f_00006-2-0 loss: 0.557832  [   32/   54]
train() client id: f_00007-0-0 loss: 0.424469  [   32/  179]
train() client id: f_00007-0-1 loss: 0.474786  [   64/  179]
train() client id: f_00007-0-2 loss: 0.440751  [   96/  179]
train() client id: f_00007-0-3 loss: 0.543063  [  128/  179]
train() client id: f_00007-0-4 loss: 0.323386  [  160/  179]
train() client id: f_00007-1-0 loss: 0.622332  [   32/  179]
train() client id: f_00007-1-1 loss: 0.364783  [   64/  179]
train() client id: f_00007-1-2 loss: 0.474144  [   96/  179]
train() client id: f_00007-1-3 loss: 0.360357  [  128/  179]
train() client id: f_00007-1-4 loss: 0.436692  [  160/  179]
train() client id: f_00007-2-0 loss: 0.499814  [   32/  179]
train() client id: f_00007-2-1 loss: 0.516413  [   64/  179]
train() client id: f_00007-2-2 loss: 0.465855  [   96/  179]
train() client id: f_00007-2-3 loss: 0.303565  [  128/  179]
train() client id: f_00007-2-4 loss: 0.338079  [  160/  179]
train() client id: f_00008-0-0 loss: 0.831405  [   32/  130]
train() client id: f_00008-0-1 loss: 0.722405  [   64/  130]
train() client id: f_00008-0-2 loss: 0.606165  [   96/  130]
train() client id: f_00008-0-3 loss: 0.796986  [  128/  130]
train() client id: f_00008-1-0 loss: 0.703823  [   32/  130]
train() client id: f_00008-1-1 loss: 0.800663  [   64/  130]
train() client id: f_00008-1-2 loss: 0.662515  [   96/  130]
train() client id: f_00008-1-3 loss: 0.820796  [  128/  130]
train() client id: f_00008-2-0 loss: 0.800447  [   32/  130]
train() client id: f_00008-2-1 loss: 0.721877  [   64/  130]
train() client id: f_00008-2-2 loss: 0.745730  [   96/  130]
train() client id: f_00008-2-3 loss: 0.722345  [  128/  130]
train() client id: f_00009-0-0 loss: 0.778861  [   32/  118]
train() client id: f_00009-0-1 loss: 0.726930  [   64/  118]
train() client id: f_00009-0-2 loss: 0.838309  [   96/  118]
train() client id: f_00009-1-0 loss: 0.632160  [   32/  118]
train() client id: f_00009-1-1 loss: 0.797938  [   64/  118]
train() client id: f_00009-1-2 loss: 0.936861  [   96/  118]
train() client id: f_00009-2-0 loss: 0.746462  [   32/  118]
train() client id: f_00009-2-1 loss: 0.792655  [   64/  118]
train() client id: f_00009-2-2 loss: 0.795128  [   96/  118]
At round 82 accuracy: 0.649867374005305
At round 82 training accuracy: 0.5868544600938967
At round 82 training loss: 0.8159322644856427
Done!
